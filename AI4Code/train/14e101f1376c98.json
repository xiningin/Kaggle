{"cell_type":{"ec485a3d":"code","25fccc4f":"code","dafcde5f":"code","580603ca":"code","ecc5ae12":"code","9be8b457":"code","1cea2429":"code","6ecbae11":"code","520c425c":"code","6a784304":"code","733fc2fb":"code","b720e898":"code","97c8a36c":"code","5833d2f3":"code","a17451cc":"markdown","a042f04e":"markdown","2f698d7c":"markdown","ab30a97b":"markdown"},"source":{"ec485a3d":"# Importing dependencies\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndata = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\n\ndata.head(10)","25fccc4f":"data.describe()","dafcde5f":"data.shape, data.info()\n\n# Here, we have 303 unique rows for 14 features. ","580603ca":"data['target'].value_counts()","ecc5ae12":"# There are slightly more patient values w\/ heart disease than there are without. \n# Seeing the numerical values is great, but visualizing it is even better. \n\nx = ['1', '0']\ny = data['target'].value_counts()\n\nsns.barplot(x, y, color='pink')\nplt.show()","9be8b457":"def showBinaryFeatureValues(feature1, feature2, feature):\n    x = [feature1, feature2]\n    y = data[feature].value_counts()\n    \n    sns.barplot(x, y, color='pink')\n    plt.show()\n    \nshowBinaryFeatureValues('male', 'female', 'sex') \nshowBinaryFeatureValues('false', 'true', 'fbs')\nshowBinaryFeatureValues('false', 'true', 'exang')","1cea2429":"# Checking for any null numbers within the dataset.\n\ndata.isna().sum()\n\n# There are no n\/a values for any of the features within the dataset. ","6ecbae11":"# Creating a heat map of correlation between features\n\ncorrelation = data.corr()\n\nfig=plt.figure(figsize=(12,12))\nplt.title('Correlation between Features')\nsns.heatmap(correlation, annot= True, cmap='flare')","520c425c":"# Plotting different distributions \n\ndef plotHeartFeaturesDist(x, x2, x3, x4):\n    fig, axes = plt.subplots(1,4, figsize=(25, 5))\n    sns.histplot(data, x=x, hue='target', multiple=\"dodge\", kde=True, palette=\"bwr\", ax=axes[0])\n    axes[0].set_title(\"Distribution of {}\".format(x))\n    sns.histplot(data, x=x2, hue='target', multiple=\"dodge\", kde=True, palette=\"bwr\", ax=axes[1])\n    axes[1].set_title(\"Distribution of {}\".format(x2))\n    sns.histplot(data, x=x3, hue='target', multiple=\"dodge\", kde=True, palette=\"bwr\", ax=axes[2])\n    axes[2].set_title(\"Distribution of {}\".format(x3))\n    sns.histplot(data, x=x4, hue='target', multiple=\"dodge\", kde=True, palette=\"bwr\", ax=axes[3])\n    axes[3].set_title(\"Distribution of {}\".format(x4))\n    \nplotHeartFeaturesDist('age', 'trestbps', 'chol', 'thalach')","6a784304":"# Scatter plots comparing various features to each other \n# Slope + thalachm, cp + thalach, trestbps + age, and age + ca had strong correlations\n# so we'll make sure to compare those. \n\ndef plotHeartFeatureScatter(x, y, x2, y2, x3, y3, x4, y4):\n    fig, axes = plt.subplots(1,4, figsize=(25, 5))\n    sns.scatterplot(data=data, x=x, y=y, hue=\"target\", palette='bwr', ax=axes[0])\n    axes[0].set_title(\"Scatter of {} vs \".format(x, y))\n    sns.scatterplot(data=data, x=x2, y=y2, hue=\"target\", palette='bwr', ax=axes[1])\n    axes[1].set_title(\"Scatter of {} vs {}\".format(x2, y2))\n    sns.scatterplot(data=data, x=x3, y=y3, hue=\"target\", palette='bwr', ax=axes[2])\n    axes[2].set_title(\"Scatter of {} vs {}\".format(x3, y3))\n    sns.scatterplot(data=data, x=x4, y=y4, hue=\"target\", palette='bwr', ax=axes[3])\n    axes[3].set_title(\"Scatter of {} vs {}\".format(x4, y4))\n\nplotHeartFeatureScatter('slope', 'thalach', 'cp', 'thalach', 'trestbps', 'age', 'age', 'ca')","733fc2fb":"# Violin plotting visualization\n\ndef plotHeartFeatureViolin(x, y, x2, y2, x3, y3):\n    fig, axes = plt.subplots(1,3, figsize=(25, 5))\n    sns.violinplot(x=x, y=y, data=data, hue=x, palette='bwr', ax=axes[0])\n    axes[0].set_title(\"Violin plot of {} and {}\".format(x, y))\n    sns.violinplot(x=x2, y=y2, data=data, hue=x2, palette='bwr', ax=axes[1])\n    axes[1].set_title(\"Violin plot of {} and {}\".format(x2, y2))\n    sns.violinplot(x=x3, y=y3, data=data, hue=x3, palette='bwr', ax=axes[2])\n    axes[2].set_title(\"Violin plot of {} and {}\".format(x3, y3))\n    \nplotHeartFeatureViolin('sex', 'trestbps', 'target', 'thalach', 'sex', 'thalach')\n\n# First row:\n# For reference: female = 0 and male = 1. \n# In the first plot, we see that there is more range in trestbps (resting blood pressure) distribution in females than in males. \n# The middle distribution of maximum heart rate achieved is also in a higher range for patients where heart disease is present.\n# The distribution frequency for males in thalach is also higher than females.\n# In last distribution, we find that there is more range in the male distribution of maximum heart rate achieved than female\\\n\nplotHeartFeatureViolin('exang', 'thalach', 'fbs', 'oldpeak', 'thal', 'age')\n\n# Second row:\n# For reference: for exercise induced angina 1 = yes; 0 = no\n# In the first plot, there is a wider range in distribution in thalach in patients who did not have exercise induced angina. There is also a frequency of higher max \n# heart rate achieved in patients who did not experience exang. \n# In the middle plot, fbs:(fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false). If fbs is not greater than 120 mg\/dl, oldpeak is most frequent at 0 and has a wider \n# range of distribution than if fasting blood sugar is greater than 120 ml\/dl.\n# The final plot, 0 thal distribution range is very thin whereas 2 thal is wider. 1 and 3 thal violin distributions are pretty identical.","b720e898":"# Data Preprocessing\n# All of the values are already numerical so there is no need for any encoding. \n\nX = data.drop(['target'], axis=1)\nY = data['target']\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)","97c8a36c":"# For now, I've just created a baseline model using mostly sklearn's default values. \nrfc = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, max_leaf_nodes=None, min_samples_leaf=1, max_samples=None, max_features=\"auto\")\n\nrfc.fit(x_train, y_train)\n\npredictions = rfc.predict(x_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, predictions))\n\n# 78%, not the best accuracy score, let's continue hyper-parameter tuning. ","5833d2f3":"# To make the search for the optimal # of trees, we can iterate through a list of range of values and return the max accuracy and # of trees. \n\ndef ListComp():\n    return [x for x in range(1, 100)]\n\nn_estimators = ListComp()\n\naccuracies = [] \n\nfor i in n_estimators:                                         \n    rfc = RandomForestClassifier(n_estimators=i, max_depth=None, min_samples_split=2, max_leaf_nodes=None, min_samples_leaf=1, max_samples=None, max_features=\"auto\")\n    rfc.fit(x_train, y_train)\n    predictions = rfc.predict(x_test)\n    accuracies.append(metrics.accuracy_score(y_test, predictions))\n    \n#shows the maximum accuracy\nmax_acc = max(accuracies)\nprint(max_acc)\n\n# Using sklearn's classification report to see the precision, recall, and f1 score for our model. \nclassification_report(y_test, predictions)","a17451cc":"![image.png](attachment:image.png)\nPhoto by Ehimetalor Akhere Unuabona on [Unsplash](https:\/\/unsplash.com\/photos\/_fPUzCWxxGA). \n\n# Heart Disease Classification w\/ Random Forest \ud83d\udc8c\n\n> ### \ud83d\udccc Table of Contents:\ni. About the Dataset\n     * Understanding the dataset\n     * Exploratory Data Analysis\n     * Data Visualization\nii. Heart Disease Classification Model\n     * Random Forest Classifier\n     * Data Preprocessing\n     * RFC Model\n     * Performance\niii. Conclusion","a042f04e":"### Conclusion \ud83d\udc8c\n\nHeart disease is currently one of the leading causes of death around the world. In the United States, one person dies every [36 seconds](http:\/\/www.cdc.gov\/heartdisease\/facts.htm#:~:text=Heart%20disease%20is%20the%20leading,1%20in%20every%204%20deaths.) from cardiovascular disease. Estimates suggest up that [3.7 million](http:\/\/www.pacificmedicalcenters.org\/physician-articles\/undiagnosed-heart-conditions\/#:~:text=Estimates%20suggest%20that%203.7%20million,screenings%20can%20be%20life%20saving). Americans have an undiagnosed heart condition. It's a sobering statistic that creates a strong case for improved heart disease diagnostic methods. \n\nArtificial Intelligence; machine learning and big data analytics in this specific use case has a lot of potential in the medspace by aiding clinicians with more accurate and faster diagnosis of heart diseases and with the rapidly developing AI ecosystem, I'm personally hopeful to see more use of machine learning in order to save lives. \n\nCheck out some of my other work: \n\n[Understanding the Amazon with Multi-Label Classification ](https:\/\/www.kaggle.com\/tenzinmigmar\/understanding-the-amazon-from-space-w-mlc)\n\n[Lunar Rocky Landscape Segmentation with U-Net](https:\/\/www.kaggle.com\/tenzinmigmar\/lunar-rocky-landscape-segmentation-with-u-net)\n\n[Classifying Iris Flower Types with K-Means](https:\/\/www.kaggle.com\/tenzinmigmar\/classifying-iris-flower-types-with-k-means)","2f698d7c":"## About the Dataset\n\n### Understanding the Dataset\n\nThis original dataset has 76 attributes but this dataset only refers to 14 attributes with the target \nfeature being the presence of heart disease in the patient. 0 meaning no presence and 1 being presence.\nThe 14 attributes are as follows:\n\n* age of patient\n* sex of patient (1 = male; 0 = female)\n* chest pain type (4 values)\n* resting blood pressure\n* serum cholestoral in mg\/dl\n* fasting blood sugar > 120 mg\/dl (1 = true; 0 = false)\n* resting electrocardiographic results (values 0,1,2)\n* maximum heart rate achieved\n* exercise induced angina (1 = yes; 0 = no)\n* oldpeak = ST depression induced by exercise relative to rest\n* the slope of the peak exercise ST segment\n* number of major vessels (0-3) colored by flourosopy\n* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n* target -> 0, 1, 2, 3, or 4.\n\nYou can learn more about the dataset [here](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci)!\n\n### Exploratory Data Analysis","ab30a97b":"### Random Forest Classifier\n\nRandom Forest is an example of an **ensemble** learning algorithm; the building blocks to a random forest classifier are **decision trees**. (Many of these trees = Forest) It's more powerful than just a single decision tree because it operates as an ensemble of several trees; each of these decision trees make a prediction for one of the classes and these predictions are then averaged for a final prediction. \n\nRandomForestClassifier() has the following parameters\ufe61:\n\n* **n_estimators**; number of trees in the forest\n* **max_depth**; maximum depth of the trees\n* **min_samples_split**; minimum # of samples needed to split an internal node \n* **max_leaf_nodes**; sets a limit to node splitting\n* **min_samples_leaf**; minimum # of leaf samples required to be at a leaf node\n* **max_sample**; used to determine what amount of data is used to train each individual data\n* **max_features**; maximum # of features each tree is given\n\n![image.png](attachment:image.png)\nImage edited from [Alteryx](https:\/\/community.alteryx.com\/t5\/Alteryx-Designer-Knowledge-Base\/Seeing-the-Forest-for-the-Trees-An-Introduction-to-Random-Forest\/ta-p\/158062)\n\nTrue to its name, Random Forest Classifier adds an attribute of randomness into the forest. While growing trees, a random sampling of features are taken into consideration. This is meant to build unique trees so that the random forest classifier averages a diverse variety of predictions. \n\n\ufe61These are not all the parameters of RandomForestClassifier(), you can learn more [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html). "}}