{"cell_type":{"7f4f64a5":"code","fd4d4506":"code","fc3640a1":"code","61551fb9":"code","fbb61846":"code","038e1487":"code","ff8e4671":"code","c70e70e7":"code","08e74c06":"code","c4abd8de":"code","0cfb1900":"code","6e249eb7":"code","58831e9b":"code","ef4652ad":"code","ee6a4e61":"code","1df9a857":"code","0d345764":"code","c189b328":"code","c8269736":"code","9222f04d":"code","56a29cf8":"code","9846b5b9":"code","845e59d7":"code","a3717428":"code","e1c7b1ff":"code","0ced0963":"code","db4bbc50":"code","584170d3":"code","8b6aabd5":"code","b9c2cbfb":"code","6104c5cc":"code","bd6c63bf":"code","b9540a1d":"code","c728d2d3":"code","9bba1966":"code","ee46c512":"code","9e4ac850":"code","2c6e9888":"code","9d0df9b4":"code","1bf944a1":"markdown","186ca593":"markdown","d9db5642":"markdown","4d6df69e":"markdown","d4fab91c":"markdown","be0fc168":"markdown","4020788a":"markdown","e18391ef":"markdown","ee48bc9d":"markdown","c9d63c36":"markdown","456d3745":"markdown","6fd223a5":"markdown","cbe59d06":"markdown","3c153c2b":"markdown","1da50906":"markdown","7c38aa39":"markdown","65fd39ae":"markdown","6b4372c9":"markdown","7375d15d":"markdown","25160b61":"markdown","ae45f1f2":"markdown","bb11ca34":"markdown","84553b5a":"markdown","eb81d364":"markdown","aa5a9aa1":"markdown","54c8fb8a":"markdown","ea022fd0":"markdown","7abd4622":"markdown","a2f09039":"markdown","76236464":"markdown"},"source":{"7f4f64a5":"import re\nimport sys\nimport phik\nimport sklearn\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom IPython.display import display_html\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nassert sys.version_info >= (3, 7, 3)\nassert sklearn.__version__ == '0.24.1'\n\nsns.set_theme(style=\"whitegrid\")        # Set Seaborn plot theme\nplt.rcParams[\"figure.figsize\"] = (12,8) # Set plot size\nsklearn.set_config(display='diagram')   # Display estimators as diagrams","fd4d4506":"# Wrapper function to  obtain a summary statistic (e.g. mean) of a feature (e.g. Survival)\n# grouped by other features (e.g. Sex & Pclass). The summary, and sample size per group,\n# are displayed inline and coloured by scale for quick assessement,\ndef plotSummaryCount(df, groups, feature, summary='mean', \n                     fillna=True, proportion=True, axis=None):\n    \"\"\" Summarise a grouped dataframe, including sample size per group\n        and display inline \"\"\"\n    # Create local copy of the required columns\n    sub_df = df.loc[:, [*groups, feature]]\n    for group in groups:\n        # Boolean groups causes issues so map to string\n        if pd.api.types.is_bool_dtype(sub_df[group]):\n            sub_df[group] = sub_df[group].astype(str)\n        # Convert groups to categories\n        sub_df[group] = sub_df[group].astype('category')\n        # Replace missing data with string so it is included as a group\n        if fillna and sub_df[group].isnull().values.any():\n            sub_df[group] = (\n                sub_df[group].cat.add_categories('Unknown')\n                .fillna('Unknown'))\n    grouped_df = sub_df.groupby(groups)\n    countGroup = grouped_df.size().fillna(0)\n    if proportion:\n        countGroup \/= countGroup.sum()\n    summaryGroup = grouped_df[feature].agg(summary)\n    if len(groups) == 1:\n        summaryGroup = summaryGroup.to_frame()\n        countGroup = countGroup.to_frame()\n    else:\n        summaryGroup = summaryGroup.unstack()\n        countGroup = countGroup.unstack()\n    summaryGroup = (summaryGroup\n        .style.background_gradient(cmap='Blues', axis=axis)\n        .highlight_null('white').applymap(setNanWhite)\n        .set_caption(f'{feature} ({summary})'))\n    mode = 'proportion' if proportion else 'count'\n    countGroup = (countGroup\n        .style.background_gradient(cmap='Blues', axis=axis)\n        .set_caption(f'Sample size ({mode})'))\n    \n    return displayInline(summaryGroup, countGroup)\n\n\n# Display Pandas DataFrames inline in Jupyter notebook\n# Used by plotSummaryCount\ndef displayInline(*dfs, spaces=10):\n    \"\"\" Display pandas dataframes as inline \"\"\"\n    html = ''\n    seperator = '<table style=\\'display:inline\\'>' + (f'{\"&nbsp\"*spaces}')\n    inline = \"style='display:inline'\"\n    for i, df in enumerate(dfs):\n        if isinstance(df, pd.io.formats.style.Styler):\n            styler = df\n        else:\n            styler = df.style\n        html += styler.set_table_attributes(inline)._repr_html_() \n        # Don't add seperator to last element\n        if i < len(dfs) - 1:\n            html += seperator\n    return display_html(html, raw=True)\n\n\n# Mask text in NaN cells by setting to white (background)\n# Used by plotSummaryCount\ndef setNanWhite(val):\n    \"\"\" Colour NaN text white \"\"\"\n    if np.isnan(val):\n        return 'color: white'\n    \n    \n# This rounds the qcut output for better visual display\ndef roundInterval(x):\n    \"\"\" Round intervals \"\"\"\n    return pd.Interval(\n        left=int(round(x.left)), \n        right=int(round(x.right)))\n\n# Configure precision to diplay pandas\npd.options.display.precision = 2\n\n# Set maximum pandas column width\npd.options.display.max_colwidth = 25","fc3640a1":"# Define paths to datasets and target\/feature information\ntrainPath = '\/kaggle\/input\/titanic\/train.csv'\ntestPath = '\/kaggle\/input\/titanic\/test.csv'\ntarget = 'Survived'\nindex = 'PassengerId'","61551fb9":"# Explicity define datatypes for each feature\ndtypes = ({\n    'Survived': bool, \n    'Pclass':   'category', \n    'Name':     'category',\n    'Sex':      'category',\n    'Age':      float,\n    'SibSp':    int,\n    'Parch':    int,\n    'Ticket':  'category',\n    'Fare':     float,\n    'Cabin':   'category',\n    'Embarked':'category'\n})\n# Read training data\ntrain = pd.read_csv(trainPath, index_col=index, dtype=dtypes)\ntrain.head()","fbb61846":"plotSummaryCount(train, ['Sex', 'Pclass'], 'Survived', summary='mean')","038e1487":"train['FamSize'] = train['Parch'] + train['SibSp'] + 1\nplotSummaryCount(train, ['Sex', 'Pclass', 'FamSize'], 'Survived', summary='mean')","ff8e4671":"train['Alone'] = (train['FamSize'] == 1)\nplotSummaryCount(train, ['Sex', 'Pclass', 'Alone'], 'Survived', summary='mean')","c70e70e7":"# Majority (77%) of the Cabin \/ Deck data is missing. \n# Much more missing data in 3rd class.\ntrain['Deck'] = train['Cabin'].apply(lambda x: x[0])\nplotSummaryCount(train, ['Pclass', 'Deck'], 'Survived', summary='mean')","08e74c06":"train['Title'] = train['Name'].apply(lambda x: re.split(',|\\.', x)[1].strip())\n(train['Title'].value_counts().to_frame().T\n .style.background_gradient(cmap='Blues', axis=1)\n .set_caption('Sample size by Title'))","c4abd8de":"# Boolean mask of any title NOT in list\notherTitles = ~train['Title'].isin(['Mr', 'Miss', 'Mrs', 'Master'])\n# Replace non-standard titles with other to reduce cardinality.\ntrain.loc[otherTitles, 'Title'] = 'Other'\n\nplotSummaryCount(train, ['Pclass', 'Title'], 'Survived', summary='mean')","0cfb1900":"sns.displot(data=train, x='Age', hue=\"Pclass\", col=\"Sex\", kind=\"kde\")","6e249eb7":"train['ageGroup'] = pd.qcut(train['Age'], 4, precision=1).apply(roundInterval)\nplotSummaryCount(train, ['Sex', 'Pclass', 'ageGroup'], 'Survived', summary='mean')","58831e9b":"plotSummaryCount(train, ['Title','Pclass'], 'Age', summary='median')","ef4652ad":"plotSummaryCount(train, ['Sex', 'Pclass', 'Embarked'], 'Survived', summary='mean')","ee6a4e61":"plotSummaryCount(train, ['Pclass', 'FamSize'], 'Fare', summary='median', axis=1)","1df9a857":"# Extract Surname feature\ntrain['Surname'] = train['Name'].apply(lambda x: re.split(',', x)[0].strip())","0d345764":"# Boolean feature describing if family has a surviving adult male\nmaleSurviveWithFam = (\n    (train['Sex'] == 'male') & (train['Title'] != 'Master') & \n    (train['Survived'] == 1) & (train['FamSize'] > 1))\n# Retrieve surnames of families with surviving adult males\nmaleNames = train.loc[maleSurviveWithFam, 'Surname']\n# Engineer new feature with family survival\ntrain['famSurvive'] = train['Surname'].isin(maleNames)","c189b328":"# Boolean feature describing if family has a dead women\/child\nwomenChildDieWithFam = (\n    ((train['Sex'] == 'female') | (train['Title'] == 'Master')) &\n    (train['Survived'] == 0) & (train['FamSize'] > 1))\n# Retrieve surnames of families with women or child that did not survive\nwomenChildNames = train.loc[womenChildDieWithFam, 'Surname']\n# Engineer new feature with family non-survival\ntrain['famDie'] = train['Surname'].isin(womenChildNames)","c8269736":"def checkCardinality(df):\n    \"\"\" Return number of unique groups \n        for non-numeric columns \"\"\"\n    return (df.select_dtypes(exclude='number')\n            .apply(pd.Series.nunique)\n            .sort_values(ascending=False))\n\nnUnique = checkCardinality(train)\nhighCardinality = list((nUnique[nUnique > 5]).index)\nnUnique.to_frame().T.style.background_gradient(cmap='Blues', axis=None)","9222f04d":"def showMissing(df):\n    \"\"\" Show features with missing values \"\"\"\n    nullOrd = df.isnull().sum().sort_values(ascending=False)\n    nullOrd = nullOrd[nullOrd > 0]\n    nullOrd = pd.DataFrame(nullOrd, columns=['TotalNA'])\n    nullOrd['PropNA'] = nullOrd['TotalNA'] \/ len(df)\n    return nullOrd\n\nmissingVals = showMissing(train)\nhighPropNA = list(missingVals[missingVals['PropNA'] > 0.8].index)\nmissingVals.style.background_gradient(cmap='Blues')","56a29cf8":"# Interval columns should be explicity defined\nintervalCols = ['Age', 'Fare']\n# Features with high cardinality and lots of missing values are dropped for simplicity\ndropCols = highCardinality + highPropNA\nphikMatrix = train.drop(dropCols, axis=1).phik_matrix(interval_cols=intervalCols)\nsigMatrix = train.drop(dropCols, axis=1).significance_matrix(interval_cols=intervalCols)","9846b5b9":"# Wrapper function for plotting heatmap and optional sorting on a specific feature.\n# Here we sort features ranked by their correlation to the target (Survival)\ndef plotHeatmap(df, sortOn=None, mask=None):\n    \"\"\" Wrapper for plotting matrix as heatmap \"\"\"\n    fig, ax = plt.subplots()\n    if sortOn is not None:\n        # Retrieve sorted columns\n        order = df[target].sort_values(ascending=False).index\n        # Reorder rows and columns\n        df = df.reindex(order)[order]\n    # Plot heatmap\n    heatmap = sns.heatmap(df, cmap='Reds', vmin=0, vmax=1, mask=mask, ax=ax)\n    heatmap.set_facecolor('grey')\n    return fig, ax\n\nfig, ax = plotHeatmap(phikMatrix, sortOn=target, mask=(phikMatrix == 1))","845e59d7":"# Remove target (as we are only interested in co-correlation among features).\n# Compute 1 - phik so that similar features are 'closer'\nphikNoTarget = 1 - phikMatrix.drop(target, axis=0).drop(target, axis=1)\n\n# Perform AgglomerativeClustering using (1 - PhiK) as a precomputed distance matrix\nphikClusterer = AgglomerativeClustering(\n    affinity='precomputed', linkage='average', \n    n_clusters=None, distance_threshold=0.25)\n\n# Associate cluster labels with features names\nlabelledClusters = (pd.DataFrame(\n    {'cluster': phikClusterer.fit(phikNoTarget).labels_, \n     'feature': phikNoTarget.index}))\n\n# Add association with target and sort\nlabelledClusters = (\n    pd.merge(phikMatrix[target].drop(target), labelledClusters, \n             left_index=True, right_on='feature')\n    .sort_values(target, ascending=False))\n\nlabelledClusters.head()","a3717428":"# Group by cluster and print clustered features.\n# Features are ordered in a cluster according to their association with the target\n# and cluster are order by their mean association with target.\nclusterFeatures = labelledClusters.groupby('cluster')\n(pd.concat([clusterFeatures['feature'].apply(list), \n            clusterFeatures[target].mean()], axis=1)\n    .sort_values('Survived', ascending=False)\n    .style.background_gradient(cmap='Blues'))","e1c7b1ff":"class GroupImputer(BaseEstimator, TransformerMixin):\n    \"\"\" Extension of SimpleImputer to optionally impute \n        values by group and return a pandas Series. \"\"\"\n    \n    def __init__(self, variable, by=[], strategy='median'): \n        self.variable = variable\n        self.by = by\n        if strategy == 'most_frequent':\n            self.strategy = lambda x: x.mode().sample(1).values[0]\n        else:\n            self.strategy = strategy\n        self.maps = []\n\n    def fit(self, X, y=None):\n        # Store impute for ungrouped data\n        self.simpleImpute = X[self.variable].agg(self.strategy)\n        # Store maps for all grouping levels\n        for i in range(len(self.by), 0, -1):\n            subBy = self.by[:i]\n            mapper = X.groupby(subBy)[self.variable].agg(self.strategy)\n            if i == 1:\n                mapper = {(k,): v for k, v in mapper.to_dict().items()}\n            self.maps.append((subBy , mapper))\n        return self\n\n    def transform(self, X, y=None):\n        imputed = X[self.variable]\n        for (by, mapper) in self.maps:\n            fillVals = X[by].apply(tuple, axis=1).map(mapper)\n            imputed = imputed.fillna(fillVals)\n            if not imputed.isnull().values.any():\n                break\n        else:\n            # Replace remaining NaN (with ungrouped)\n            imputed = imputed.fillna(self.simpleImpute)\n        return imputed","0ced0963":"class NoTransformer(BaseEstimator, TransformerMixin):\n    \"\"\" Dummy transformer \"\"\"\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return X","db4bbc50":"class FeatureEngineer(BaseEstimator, TransformerMixin):\n    \"\"\" Custom imputation and feature engineering \n        of Titanic dataset \"\"\"\n    \n    def __init__(self):\n        self._imputes = {}\n        \n\n    def initData(self, X):\n        \"\"\" Feature engineering required for both fit & transform \"\"\"\n        X['Title'] = X['Name'].apply(lambda x: re.split(',|\\.', x)[1].strip())\n        X['Surname'] = X['Name'].apply(lambda x: re.split(',', x)[0].strip())\n        X['FamSize'] = X['Parch'] + X['SibSp'] + 1\n\n        \n    def fit(self, X, y=None):\n        X = X.copy()\n        self.initData(X)\n        # Store surnames of all adult males (with families) that survived\n        maleSurviveWithFam = (\n            (X['Sex'] == 'male') & (X['Title'] != 'Master') & \n            (y == 1) & (X['FamSize'] > 1))\n        self.maleNames = X.loc[maleSurviveWithFam, 'Surname']\n        # Store surnames of all women and children (with families) that died\n        femaleDieWithFam = (\n            ((X['Sex'] == 'female') | (X['Title'] == 'Master')) &\n            (y == 0) & (X['FamSize'] > 1))\n        # Store surnames of all females (with families) that died\n        self.femaleNames = X.loc[femaleDieWithFam, 'Surname']\n        # Store Age impuration values grouped by Title and Pclass\n        self._imputes['Age'] = GroupImputer(\n            'Age', by=['Title', 'Pclass'], strategy='median').fit(X)\n        return self\n\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        self.initData(X)\n        X['Age'] = self._imputes['Age'].transform(X)\n        X['famSurvive'] = X['Surname'].isin(self.maleNames)\n        X['famDie'] = X['Surname'].isin(self.femaleNames)\n        X['Deck'] = X['Cabin'].apply(lambda x: x[0])\n        # Boolean mask of any title NOT in list\n        otherTitles = ~X['Title'].isin(['Mr', 'Miss', 'Mrs', 'Master'])\n        # Replace non-standard titles with other to reduce cardinality.\n        X.loc[otherTitles, 'Title'] = 'Other'\n        return X  ","584170d3":"X = pd.read_csv(trainPath, index_col=index, dtype=dtypes)\ny = X.pop(target)\n\nsplit = train_test_split(X, y, random_state=0, train_size=0.8, test_size=0.2)\nX_train, X_valid, y_train, y_valid = map(lambda x: x.copy(), split)","8b6aabd5":"# Define custom per-feature transformations as Pipelines\nEmbarkedTransformer = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='constant', fill_value='Unknown')),\n    ('encode', OneHotEncoder(\n        handle_unknown='ignore', categories=[['C', 'Q', 'S']]))\n])\nDeckTransformer = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='constant', fill_value='Unknown')),\n    ('encode', OneHotEncoder(\n        handle_unknown='ignore', categories=[['C', 'E', 'G', 'D', 'A', 'B', 'F', 'T']]))\n])\nAgeDiscretizer = Pipeline(steps=[\n    ('discrete', KBinsDiscretizer(encode='ordinal', strategy='kmeans'))\n])","b9c2cbfb":"# Combine all transformations into a ColumnTransformer\ntransformers = ([\n    ('Pclass',   OneHotEncoder(handle_unknown='ignore',\n        categories=[['1', '2', '3']]),  ['Pclass']),\n    ('Title',    OneHotEncoder(handle_unknown='ignore',\n        categories=[['Mr', 'Miss', 'Mrs', 'Master', 'other']]),  ['Title']),\n    ('Embarked', EmbarkedTransformer, ['Embarked']),\n    ('Age',      AgeDiscretizer, ['Age']),\n    ('None',     NoTransformer(), ['famDie', 'famSurvive', 'FamSize']),\n])\nfeatureTransformer = ColumnTransformer(\n    transformers=transformers, remainder='drop', sparse_threshold=0)","6104c5cc":"# Define a preProcessor Pipeline encompassing the FeatureEngineering and featureTransformation steps\npreProcessor = Pipeline(steps=[\n    ('engineer',        FeatureEngineer()),\n    ('columnTransform', featureTransformer),\n])","bd6c63bf":"def getFeatureNames(columnTransformer, selector=None):\n    \"\"\" Extract feature names from column transformer object. \n        If transformers are pipelines the one-hot encoding step\n        should be last step of that pipeline.\n    \"\"\"\n    colNames = np.array([])\n    for tupleTransformer in columnTransformer.transformers_[:-1]:\n        if isinstance(tupleTransformer[1], Pipeline): \n            transformer = tupleTransformer[1].steps[-1][1]\n        else:\n            transformer = tupleTransformer[1]\n        try:\n            # One hot encoded names have x0_, x1_ etc.\n            names = transformer.get_feature_names()\n            trueNames = tupleTransformer[-1]\n            # Get dict mapping transformed name to true name\n            nameMap = {f'x{i}_' : name for i, name in enumerate(trueNames)}\n            # Swap transformed name with true name\n            for i, name in enumerate(names):\n                prefix = name[:3]\n                names[i] = f'{nameMap[prefix]}_{name[3:]}'\n        except AttributeError:\n            names = tupleTransformer[2]\n        # This is for kBinDiscretizers, which have n_bins_ method\n        if (isinstance(transformer, KBinsDiscretizer)\n                and transformer.encode != 'ordinal'):\n            if transformer.encode != 'ordinal':\n                nBins = transformer.n_bins_\n                newNames = []\n                for col, n in zip(names, nBins):\n                    newNames = [f'{col}-{i}' for i in range(n)]\n                names = newNames\n        colNames = np.append(colNames, names)\n    if selector is not None:\n        colNames = colNames[selector.get_support()]\n    return colNames","b9540a1d":"def plotFeatureImportance(X, y, prePreprocessor, estimator, vline=None):\n    \"\"\" Run decision tree ensemble method on a preProcesor \n        pipline and plot feature importance \"\"\"\n    pipeline = Pipeline(steps=[\n        ('prePreprocessor', prePreprocessor),\n        ('selector',        estimator)])\n    clf = pipeline.fit(X, y)\n    columnTransformer = (\n        clf.named_steps['prePreprocessor'].named_steps['columnTransform'])\n    try:\n        selector = clf.named_steps['prePreprocessor'].named_steps['selector']\n    except KeyError:\n        selector = None\n    featureNames = getFeatureNames(columnTransformer, selector)\n    features = (pd.DataFrame(\n        {'feature': featureNames,\n         'importance': clf.named_steps['selector'].feature_importances_})\n        .sort_values(by=['importance'], ascending=False))\n    \n    print(f'Total unfiltered features: {len(featureNames)}')\n    fig, ax = plt.subplots()\n    sns.barplot(y='feature', x='importance', data=features, ax=ax)\n    if vline is not None:\n        ax.axvline(vline)\n    ax.set_ylabel('')\n    ax.set_xlabel('Feature importance')\n    fig.tight_layout()","c728d2d3":"# Note the plotFeatureImportance function assumes the columnTransformer is named 'columnTransform'\n# Future update could loop through steps and find the ColumnTransformer object\nselectEstimator = RandomForestClassifier(\n    random_state=1, n_estimators=500, max_features='sqrt')\nplotFeatureImportance(X_train, y_train, preProcessor, selectEstimator, 0.02)","9bba1966":"featureSelector = Pipeline(steps=[\n    ('preProcess',    preProcessor),\n    ('selector',      RFECV(selectEstimator, cv=5, scoring='accuracy')),\n])\n# Fit data to pipeline\nfeatureSelector.fit(X_train, y_train)","ee46c512":"# Extract columnTransformer and selector to extract feature names\ncolumnTransformer = featureSelector.named_steps['preProcess'].named_steps['columnTransform']\nselector = featureSelector.named_steps['selector']\n\nfeatureNames = getFeatureNames(columnTransformer, selector)\n# Create dataframe of transformed data\ntransformedDF = pd.DataFrame(\n    featureSelector.transform(X_train), \n    columns=featureNames)\n\n# Symetric difference between 'with' and 'without selection'\nallFeatures = set(getFeatureNames(columnTransformer))\n\neliminatedFeatures = allFeatures ^ set(featureNames)\nprint(f'Eliminated features: {eliminatedFeatures}')\n\ntransformedDF.head()","9e4ac850":"class FeatureFilter(BaseEstimator, TransformerMixin):\n    \"\"\" Filter columns by boolean mask \"\"\"\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X[:, self.columns]","2c6e9888":"fullModel = Pipeline(steps=[\n    ('preProcess',    preProcessor),\n    ('featureFilter', FeatureFilter(selector.get_support())),\n    ('model',         RandomForestClassifier(random_state=1)),\n])\n\nparams = ({\n    'preProcess__columnTransform__Age__discrete__n_bins':  range(2, 5),\n    'model__n_estimators':      range(100, 1000, 10),\n    'model__max_depth':         range(1, 20),\n    'model__criterion':         ['gini', 'entropy'],\n    'model__max_features':      range(1, len(featureNames) + 1),\n})\n\ngridSearch = RandomizedSearchCV(\n    fullModel, params, scoring='accuracy', random_state=1,\n    cv=5, refit=True, n_jobs=-1, n_iter=100, verbose=1)\ngridSearch.fit(X_train, y_train)\n\nscore = gridSearch.score(X_valid, y_valid)\nprint(f'Best score: {score:.3f}')\ngridSearch # Display model object","9d0df9b4":"test = pd.read_csv(testPath, index_col=index, dtype=dtypes)\npredictions = gridSearch.predict(test).astype(int)\nsubmission = pd.DataFrame({'PassengerId': test.index, 'Survived': predictions})\nsubmission.to_csv('submissions.csv', header=True, index=False)\nsubmission.head()","1bf944a1":"<a id=\"sec_EDA\"><\/a>\n# 1. Exploratory Data Analysis \/ Feature Engineering\n  - In this section I perform some basic exploratory data analysis.\n  - The Titanic features are primarily categorical and so tabular visualisations of relationships are very informative.\n    - The main goal here is to study the relationship of Survival rates among different different catogerical groups.\n  - This is by no means comprehensive, but the main purpose is to illustrate usage of *plotSummaryCount()* and the PhiK correlation analysis.","186ca593":"<a id=\"subsec_cardinality\"><\/a>\n<a id=\"checkCardinality\"><\/a>\n### Check cardinality of non-numeric groups\n  - High cardinality groups (>5 unique) will be dropped for Phik analysis (see below).","d9db5642":"<a id=\"subsec_definePreProcessing\"><\/a>\n### Define pre-processing pipeline\n  - Here I define custom feature transformations for different features using ColumnTransformer.\n  - Any features not defined here are dropped.\n    - I prefer this approach as it requires the user to define features explicity.\n  - Since I have opted for a tree based method (RandomForestClassifier) minimal data transformation is required.\n    - **Embarked** and **Deck** - Missing values are filled with constants and OneHotEncoded.\n    - **Age** - Ordinally encoded and discretised using kmeans clustering.\n    - **Pclass** and **Title** - OneHotEncoded.\n    - **famDie**, **famSurvive** and **FamSize** - No transformation required.\n    - __*All other features*__ - Dropped.\n","4d6df69e":"<a id=\"plotFeatureImportance\"><\/a>","d4fab91c":"<a id=\"subsec_PhiK\"><\/a>\n### PhiK correlation analysis\n  - Instead of a typical correlation analysis, here we compute a [PhiK](https:\/\/phik.readthedocs.io\/en\/latest\/introduction.html) score.\n  - PhiK is a fanstastic tool to stud relationships between mixed variable types.\n    - '*Pearson-like characteristics and a sound statistical interpretation that works for interval, ordinal and categorical variable types alike*'.\n    - Check out the documentation here - https:\/\/phik.readthedocs.io\/en\/latest\/introduction.html\n  - Here I compute the PhiK score and visualise it has a heatmap sorted by strongested association with the target (Survival).","be0fc168":"<a id=\"subsec_embarked\"><\/a>\n### Embarked (mean survival by Sex, Pclass and Embarked)","4020788a":"<a id=\"subsec_deck\"><\/a>\n### Deck (mean Survival by Pclass and Deck)","e18391ef":"# Table Of Contents\n  - [**0. Introduction**](#sec_Introduction)\n  - [**1. Exploratory Data Analysis**](#sec_EDA)\n    - [Mean Survival by Sex and Pclass](#subsec_meanSurvival)\n    - [Family Size](#subsec_famSize)\n    - [Deck](#subsec_deck)\n    - [Title](#subsec_title)\n    - [Age](#subsec_age)\n    - [Embarked](#subsec_embarked)\n    - [Fare](#subsec_fare)\n    - [Cardinality](#subsec_cardinality)\n    - [Missing values](#subsec_missing)\n    - [PhiK correlation](#subsec_PhiK)\n    - [Clustering co-correlated features](#subsec_clustering)\n  - [**2. Building a pipeline for data preprocessing**](#sec_prePipeline)\n    - [GroupImputer Transformer](#subsec_groupImputer)\n    - [NoTransformer](#subsec_noTransformer)\n    - [FeatureEnginer Transformer](#subsec_featureEngineer)\n    - [Test Train Splt](#subsec_testTrainSplit)\n    - [Define pre-processing pipeline](#subsec_definePreProcessing)\n  - [**3. Feature Importance**](#sec_featureImportance)\n  - [**4. Feature Selection**](#sec_featureSelection)\n  - [**5. Model Training and Hypertuning**](#sec_modelTraining)\n  \n### Quick Find Custom Functions \/ Transformers\n  - [**plotSummaryCount()**](#plotSummaryCount) \n    - Quickly visualise grouped tabular data with colour scaling and sample sizes.\n  - [**checkCardinality()**](#checkCardinality) \n    - Display sorted list of number of unique values for each non-numeric feature.\n  - [**showMissing()**](#showMissing) \n    - Dispay sorted list of missing values per feature.\n  - [**plotHeatmap()**](#plotHeatmap) \n    - Wrapper to plot heatmaps sorted by a chosen feature\/target.\n  - [**groupImputer**](#groupImputer) \n    - Custom scikit-learn transformer to perform group imputation (e.g. mean Age by Title and Pclass) to avoid data leakage and use within a Pipeline.\n  - [**noTransformer**](#noTransformer)\n    - Dummy transformer for passing features through a ColumnTransformer without modification.\n  - [**Feature Engineer**](#featureEngineer)\n    - Custom scikit-learn transformer to perform all feature engineering and early pre-procrocessing steps.\n  - [**getFeatureNames()**](#getFeatureNames)\n    - Extract original feature names from a ColumnTransformer object.\n  - [**plotFeatureImportance()**](#plotFeatureImportance)\n    - Wrapper function to run a tree based estimator on a data processing pipeline and automatically plot feature importances.\n  - [**FeatureFilter**](#featureFilter)\n    - Simple Transformer to filter pre-selected features within a Pipeline object.","ee48bc9d":"<a id=\"sec_Introduction\"><\/a>\n# 0. Introduction\n  - As my first Kaggle competition I had great fun working through **Titanic: Machine Learning from Disaster** and learned a great deal from many of the great kernels out there.\n\n  - Since there are already many excellent kernels with thorough descriptions of exploratory data analysis, feature engineering and model building, I wanted the emphasis of this kernel to be about developing some good data science practices (or atleast my thoughts on what these could be!).\n\n- In this kernel we will cover:\n  - Building a complete Pipeline object encompassing a complete machine learning workflow (from raw data to model training and prediction).\n  - Generalised functions to nicely visualise tabular data.\n  - Custom scikit-learn transformers to perform grouped imputation and feature engineering.\n  - Reusable wrapper functions to compute and plot feature importance.\n  - Feature selection via crossed-validated recursive feature elimination.\n  - Parameter hypertuning via cross-validation randomised search.","c9d63c36":"<a id=\"subsec_testTrainSplit\"><\/a>\n### Test Train Split\n  - Here I read the training data and split 80% (training) 20% (validation)\n  - The map command ensures the X_train, X_valid, y_train, y_valid are true copies.","456d3745":"<a id=\"subsec_famSurvive\"><\/a>\n### Family Survival\n - Here we create 2 boolean features to quantify family survive for a given surname.\n - Since most males dies, if a male of the family survived the rest of his family may have survived to.\n - Since most women and children survived, if they died then the rest of their family may have died also.\n  - Adult male survived.\n  - Female or boy (title = 'Master') died.","6fd223a5":"<a id=\"plotHeatmap\"><\/a>","cbe59d06":"<a id=\"sec_featureImportance\"><\/a>\n# 3. Feature Importance\n  - Now we have built our preProcessing pipeline encompassing the feature engineering and feature transformation it is now helpful to consider feature importance of our transformed data.\n  - Here I introduce two custom functions (below) to help with visualisation.\n     - **getFeatureNames()**\n       - This function simply extracts the original feature names from a columnTransformer object.\n       - This is harder than it should be, especially when dealing with OneHotEncoded features but this function seems to do the job.\n       - I adapted this code from answer by [pjgao](https:\/\/github.com\/pjgao) to a similar [issue](https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/12525) raised on the scikit-learn github.\n     - **plotFeatureImportance()**\n       - This wrapper function simply takes a preProcessor pipeline object and a decision tree type estimator and plots a sort bar plot of feature importance.\n       - This function internally calls getFeatureNames() to retrieve the original feature names.","3c153c2b":"<a id=\"featureFilter\"><\/a>","1da50906":"<a id=\"subsec_fare\"><\/a>\n### Fare (median Fare by Pclass and Family Size)\n  - Ticket Fare is a tricky feature - as I understand it is the sum of Fare prices (which vary by Pclass) within a passenger group.\n  - Passenger group sizes may be larger than family size (perhaps due to maids, friends, multiple families in a group etc.) and true value cannot be determined from the training data.\n  - Studying median fare by Pclass and Family Size (our closest approximaation to group size) we see that, in general, Fares are high for larger families.\n  - There is also a clear relationship between Fare and Pclass as expected.","7c38aa39":"<a id=\"subsec_groupImputer\"><\/a>\n<a id=\"groupImputer\"><\/a>\n### GroupImputer Transformer\n  - Often we may want to perform imputation of missing values by group (instead of taking the global mean).\n  - Many kernels impute missing values for Age by group.\n    - For example by imputing age according to the median age of a specific Title-Pclass group.\n  - However, there doesn't seem to be an easy way to add this step to a Pipeline.\n  - Instead we use a custom Transformer object to do the job ...\n    - The GroupImputer object is initiliased with variable name to impute and the feature names to impute by.\n    - The fit() method stores the imputation values at each level of group.\n      - For example if we want to impute Age by 'Sex' and 'Pclass' the fit() method will store the mean age for each Sex-Pclass group, the mean age for each Sex group and the overall mean age.\n    - The transform() method fills each group with the imputed value.\n    - If an impute value does not exist for a particular grouping, it will move up a group and use this value.\n      - This ensure values will always be imputed even if a group is missing.\n  - This is still a bit experimental and I am sure there is room for improvement. If you see any bugs or have any suggestions here please do let me know.","65fd39ae":"<a id=\"subsec_noTransformer\"><\/a>\n<a id=\"noTransformer\"><\/a>\n### NoTransformer\n - Below we make use of the ColumnTransformer to explicity define custom pre-processing steps for each feature.\n   - Any features not included in the ColumnTransformer are dropped.\n   - However, since some features do not need pre-processing (or have already been processed in the FeatureEnginner class) we need a way to define them in the ColumnTransformer without performing any manipulations.\n - As such I have created a minimal 'dummy' transformer that allows me to explicity specify features that do not need to be transformed in the ColumnTransformer object. ","6b4372c9":"<a id=\"sec_modelTraining\"><\/a>\n# 5. Model Training and Hypertuning\n  - Up to this point we have built Pipeline object to cleanly perform all feature engineering and data imputation steps without data leakage.\n  - We also performed automated feature selection and pre-selected a subset of transformed features to pass to the machine learning estimator.\n  - Here we build our final Pipeline object incorporating:\n    - The original pre-processing Pipeline\n    - A FeatureFilter transformer to filter our pre-seleced features\n    - The estimator itself, in this case a RandomForestClassifier\n  - Then we perform hypertuning via 5 fold cross-validated random search with 100 iterations.\n  - Finally we read the test dataset and run predict() on our model object to generate our submission.\n    - This is the first time we have looked at our test dataset.\n    - Since the pipeline incorporates our entire processing pipeline, no modificaton of the test dataset is required.","7375d15d":"<a id=\"subsec_clustering\"><\/a>\n### Detecting co-correlated features using Agglomerative Clustering\n  - Here I use (1 - phikScore) as a distance matrix for agglomerative clustering.\n    - This section is still exploratory and there may be ways to improve upon the informativeness of this.\n  - I compute the mean PhiK score per cluster versus the target (Survival).\n  - For example, we find that Sex and Title are co-correlated and this cluster is most strong associated with Survival.","25160b61":"<a id=\"getFeatureNames\"><\/a>","ae45f1f2":"<a id=\"subsec_meanSurvival\"><\/a>\n### EDA Starting Point - Mean Survival by Sex and Pclass\n - We expect Sex and Class to be strong determinants of survival probability and a quick look confirms this.\n - When studying the contribution of other factors to survival it may be worth grouping by Sex and\/or Class to prevent imbalances among groups from misleading the data.\n - Key points:\n   - Female survival is better than Men, irrespective of Class. \n   - Male survival in Class 1 is slighty better than Class 2\/3.\n   - Female survival is worse (~92% vs 50%) in Class 3 vs Class 1\/2.\n   \n#### First use of plotSummaryCount function\n  - I make extensive use of this helper function throughout this section.\n  - The ouput yields two tables, the first is the summarised feature by group(s) and the right is the sample size per group.\n  - Both are important - if a group has a small sample size then the summary statistic for that group may be more prone to noise.\n  - Below we visualise mean Survival by Sex and Pclass.\n  - In addition to the obvious differences in Survival it is interesting to note that 3rd Class Males comprise 39% of the total training set.","bb11ca34":"<a id=\"sec_prePipeline\"><\/a>\n# 2. Building a pipeline for data processing\n  - In this section we create custom transformers to encompass all of the data proprocessing steps into a scikit-learn Pipeline object.","84553b5a":"<a id=\"subsec_age\"><\/a>\n### Age (mean survival by Sex, Pclass and Age)\n  - First we take a look at the age distributions according to Sex and Pclass.\n  - Then we group Age into 4 quantiles and investigate Survival rate by Age Group, Sex and Pclass.","eb81d364":"<a id=\"subsec_famSize\"><\/a>\n### Family Size (mean Survival by Sex, Pclass and Family Size)\n  - Here we engineer family size by summing Parch and SibSp + 1.","aa5a9aa1":"<a id=\"sec_featureSelection\"><\/a>\n# 4. Feature Selection\n  - Though you could argue feature selection may not be required here, I wanted to include it as part of this kernel for the record as I found it to be a very useful tool.\n  - We could perform feature selection using defined feature importance cut off based on the plot above.\n  - Alternatively, here I perform cross validation recursive feature elimination (RFECV) to automate the process.","54c8fb8a":"<a id=\"subsec_title\"><\/a>\n### Title (mean survival by Title and Pclass)\n  - We can extract Title from the first part of the Surname.\n  - This feature has quite high cardinality (lots of unique values) and is dominated by 4 titls (Mr, Miss, Mrs and Master).\n  - For simplicity we group all other Titles into a single 'Other' group.","ea022fd0":"<a id=\"subsec_missing\"><\/a>\n<a id=\"showMissing\"><\/a>\n### View missing data\n  - Groups with high proportion of missing data (> 80%) will be dropped for Phik analysis (see below).","7abd4622":"<a id=\"subsec_featureEngineer\"><\/a>\n<a id=\"featureEngineer\"><\/a>\n### FeatureEngineer Transformer\n  - I use a custom Transformer class to incorporate all feature engineering and group imputation steps.\n  - The fit and transform methods, help isolate the test and training datasets and so prevent data leakage.\n  - It's also much cleaner - all data engineering is summarised in one place and no preprocessing of the test datasets is required before passing to the machine learning model.","a2f09039":"<a id=\"plotSummaryCount\"><\/a>","76236464":"### Median Age by Title and Pclass\n  - We need to impute missing values for Age but a simple global average would miss patterns between passenger groups.\n  - Grouping by Title and Pclass exposes some interesting patterns.\n     - As expected the 'Master' title is informative that the passenger is young.\n     - Passengers in Pclass 1 tend to be older.\n     - Women with 'Miss' (unmarried) tend to be younger than women with 'Mrs'.\n       - All girls would likely be classified as Miss though Miss is not exclusive to girls like Master is to boys."}}