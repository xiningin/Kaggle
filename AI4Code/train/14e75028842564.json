{"cell_type":{"3109fc5c":"code","be9701eb":"code","ff7b5d91":"code","e8f57fc9":"code","0d3ab284":"code","9db781f1":"code","73809e29":"code","1d4e4672":"code","b51e0a56":"code","145169de":"code","63ef88b2":"code","53a59db8":"code","0e5843e9":"code","daf0dbdb":"code","5cd0cc8d":"code","b45e84a1":"code","a37e3360":"code","60337374":"code","d1cb59d6":"code","56777255":"code","caae0631":"code","2e96128e":"code","f86da093":"code","ec45e4af":"code","e4012e3a":"code","48a95fb9":"markdown","4ab829e2":"markdown","f9d747e5":"markdown","16b85799":"markdown","0951a02c":"markdown","ec5d51c2":"markdown","96bc2eb6":"markdown","4aeb47b6":"markdown","6f855208":"markdown","31ed35f4":"markdown","10dcba62":"markdown"},"source":{"3109fc5c":"import pandas as pd       \nimport matplotlib as mat\nimport matplotlib.pyplot as plt    \nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\n\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nfrom lightgbm import LGBMRegressor\n\nseed = 11","be9701eb":"df_train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col = 'id')\nX_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv', index_col = 'id')\nsubmission = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')","ff7b5d91":"df_train","e8f57fc9":"df_train.info()","0d3ab284":"df_train.describe()","9db781f1":"X_test","73809e29":"X_test.info()","1d4e4672":"X_test.describe()","b51e0a56":"for col in df_train.select_dtypes('object').columns:\n    print(col, '\\n')\n    print(df_train[col].value_counts(), '\\n')","145169de":"for col in X_test.select_dtypes('object').columns:\n    print(col, '\\n')\n    print(X_test[col].value_counts(), '\\n')","63ef88b2":"X_train = df_train.copy().drop('target', axis = 1)\nY_train = df_train['target'].copy()","53a59db8":"encoding_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6,\n                'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14}\n\nfor col in X_test.select_dtypes('object').columns:\n    X_train[col] = X_train[col].map(encoding_map).astype('int')\n    X_test[col] = X_test[col].map(encoding_map).astype('int')","0e5843e9":"X_train.info()","daf0dbdb":"X_test.info()","5cd0cc8d":"for col in X_train.select_dtypes('int').columns:\n    print(col, '\\n')\n    print(X_train[col].value_counts(), '\\n')","b45e84a1":"for col in X_test.select_dtypes('int').columns:\n    print(col, '\\n')\n    print(X_test[col].value_counts(), '\\n')","a37e3360":"def cv_function (X_train, Y_train, model, splits = 10, seed = seed):\n    \n    print('seed:', seed)\n    #random_state = {'random_state' : seed}\n    #model = model.set_params(**random_state)\n    #print(model.get_params())\n\n    kfold = KFold(n_splits = splits, shuffle=True, random_state = seed)\n    rmse = []\n   \n    cv_pred = np.zeros(len(X_train))\n    \n    for idx in kfold.split(X=X_train, y=Y_train):\n        train_idx, test_idx = idx[0], idx[1]\n        xtrain = X_train.iloc[train_idx]\n        ytrain = Y_train.iloc[train_idx]\n        xtest = X_train.iloc[test_idx]\n        ytest = Y_train.iloc[test_idx]\n        \n        # fit model for current fold\n        model.fit(xtrain, ytrain, \n            early_stopping_rounds = 300, eval_set = [(xtest,ytest)],\n            eval_metric=\"rmse\", verbose = 200)\n\n        #create predictions\n        preds = model.predict(xtest)\n        cv_pred[test_idx] = preds\n                              \n        # calculate and append accuracy\n        fold_rmse = mean_squared_error(ytest,preds, squared=False)\n        print(\"RMSE: {0:0.5f}\". format(fold_rmse))\n        rmse.append(fold_rmse)\n        \n    print (np.mean(rmse))\n    return cv_pred","60337374":"lgbm_model = LGBMRegressor(n_estimators = 10000, learning_rate = 0.1, random_state = seed, max_depth = 2,\n                          subsample = 0.95, colsample_bytree = 0.85, reg_alpha = 30.0, reg_lambda = 25.0\n                          , num_leaves = 4, max_bin = 512)","d1cb59d6":"#Uncomment to run\n#lgbm_cvpred = cv_function(X_train, Y_train, lgbm_model)\n#0.717510075306471","56777255":"def prediction (X_train, Y_train, model, X_test, seed = seed):\n    \n    print('seed:', seed)\n    #random_state = {'random_state' : seed}\n    #model = model.set_params(**random_state)\n        \n    kfold = KFold(n_splits = 10, shuffle=True, random_state = seed)\n    \n    y_pred = np.zeros(len(X_test))\n    train_oof = np.zeros(len(X_train))\n    \n    for idx in kfold.split(X=X_train, y=Y_train):\n        train_idx, val_idx = idx[0], idx[1]\n        xtrain = X_train.iloc[train_idx]\n        ytrain = Y_train.iloc[train_idx]\n        xval = X_train.iloc[val_idx]\n        yval = Y_train.iloc[val_idx]\n        \n        # fit model for current fold\n        model.fit(xtrain, ytrain, \n            early_stopping_rounds = 300, eval_set = [(xval,yval)], verbose = False)\n\n        #create predictions\n        y_pred += model.predict(X_test)\/kfold.n_splits\n        print(y_pred)\n               \n        val_pred = model.predict(xval)\n        # getting out-of-fold predictions on training set\n        train_oof[val_idx] = val_pred\n\n        # calculate and append rmsle\n        rmse = mean_squared_error(yval,val_pred, squared=False)\n        print('RMSE : {}'.format(rmse))\n  \n    return y_pred, train_oof","caae0631":"pred, train_oof = prediction (X_train, Y_train, lgbm_model, X_test)","2e96128e":"train_oof = pd.DataFrame(train_oof, columns = ['target'])\ntrain_oof.to_csv('train_oof.csv', index=False)\n\ntrain_oof","f86da093":"submission['target'] = pred\nsubmission.to_csv(\"submission_lgbm.csv\", index=False)\nsubmission","ec45e4af":"#Importing the submission file from the xgboost model\nsub_xgb = pd.read_csv('..\/input\/30days-xgboost-model\/submission.csv')","e4012e3a":"#Blending\nsub_blend = 0.15*submission + 0.85*sub_xgb\nsub_blend['id'] = submission['id']\n\nsub_blend.to_csv(\"submission_xgb_lgbm.csv\", index=False)\nsub_blend","48a95fb9":"All features have numerical values now, with no missing values.\n\nChecking the new values.","4ab829e2":"## Importing Packages and Datasets + First Look at the Data","f9d747e5":"## <center>If you find this notebook useful, support with an upvote!<center>","16b85799":"# <center>30 Days of Machine Learning<center>\n## <center>LightGBM Model (+ Blend with XGBoost)<center>\n---\nNotebook with a LightGBM model implemented on a 10-fold CV procedure. The categorical features were 'manually' encoded with the map function. Further blending with the submission file from a XGBoost model (CV score: 0.71678; LB score: 0.71773).\n    \nI haven\u2019t performed an EDA on this notebook. My suggestion of EDA:\n* [30 Days of ML - EDA](https:\/\/www.kaggle.com\/dwin183287\/30-days-of-ml-eda) by [Sharlto Cope](https:\/\/www.kaggle.com\/dwin183287)\n    \nMy other notebooks in this competition:\n* [30 Days of Machine Learning: Starter - Auto EDA + Base XGBoost (GPU)](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/30days-starter-auto-eda-base-xgboost-gpu)\n* [30 Days of Machine Learning: Random Forest on GPU (RAPIDS\/cuML)](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/30days-random-forest-on-gpu-rapids)\n* [30 Days of Machine Learning: Final Stacking](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/30days-final-stacking)","0951a02c":"## Making Predictions","ec5d51c2":"## Categorical Features ","96bc2eb6":"## Blending with XGBoost","4aeb47b6":"Encoding with map function.","6f855208":"## Creating the Model + K-fold CV","31ed35f4":"## <center>If you find this notebook useful, support with an upvote!<center>","10dcba62":"Taking a look at the categorical features."}}