{"cell_type":{"5d79c52f":"code","12f543fc":"code","74594dbd":"code","22732efa":"code","9110d8c5":"code","b4261f6b":"code","a9e5674d":"code","e125368a":"code","b656d113":"code","6927d0da":"code","cc89b86d":"code","d18fc633":"code","b60afe25":"code","ecb4b1c4":"code","8c619b1c":"code","fa9bbe8b":"code","2e98c5ba":"code","00c75b5e":"code","897c2877":"code","b5dfb215":"code","a6e0b7aa":"code","4e24eca3":"code","d42745f7":"code","4efd637f":"code","c47744da":"code","1ce8f1cb":"code","75f07cf6":"code","aa4a36d4":"code","7fea899e":"code","9f8ab310":"code","36e4e7ef":"code","7e473115":"code","8d8e70f4":"code","d0a21515":"code","2434afb0":"code","c95f5dc6":"code","5f74be74":"code","c2b90f85":"code","ac230b39":"code","462b0f9d":"code","e0c20846":"code","94688c0b":"code","16f6f80c":"code","4b784efa":"code","5ab00b83":"code","dc918644":"code","756ebb36":"code","91a16aa0":"code","38276ba2":"code","b9e5c03d":"code","4807ba3d":"code","cf49fcdb":"code","f831c87b":"code","9be3176e":"markdown","f2c9945f":"markdown","8a6a9c8d":"markdown","a1d9dd43":"markdown","ba5a17d1":"markdown","dda55d88":"markdown","081a6f02":"markdown","6bea1934":"markdown","50ece7f7":"markdown","c38a638d":"markdown","80aba10e":"markdown","38f9059f":"markdown","f0b8f325":"markdown","e3025fdf":"markdown","9be3e290":"markdown","99b7b0c5":"markdown","5c4f3ab1":"markdown","75446cb5":"markdown","7c654fb4":"markdown","8fd6b97b":"markdown","b1c60039":"markdown","2c8c6fc6":"markdown","8ac9f7be":"markdown","17be0a99":"markdown","f903b16b":"markdown","3f4560d2":"markdown","84d2fe6d":"markdown","94e1871a":"markdown","63f9e7ca":"markdown","a290fbab":"markdown","7e79ab1b":"markdown","57736ec8":"markdown","3b4bed6d":"markdown","dfc6167c":"markdown"},"source":{"5d79c52f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","12f543fc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots","74594dbd":"test_df = pd.read_csv('..\/input\/quora-insincere-questions-classification\/test.csv')\ntrain_df = pd.read_csv('..\/input\/quora-insincere-questions-classification\/train.csv')","22732efa":"train_df.info()","9110d8c5":"test_df.info()","b4261f6b":"sincere_question = train_df[train_df['target'] == 0].question_text #nh\u1eefng c\u00e2u h\u1ecfi kh\u00f4ng ph\u1ea3n c\u1ea3m th\u00ec s\u1ebd c\u00f3 nh\u00e3n 0\ninsincere_question = train_df[train_df['target'] == 1].question_text #nh\u1eefng c\u00e2u h\u1ecfi kh\u00f4ng ph\u1ea3n c\u1ea3m th\u00ec s\u1ebd c\u00f3 nh\u00e3n 1","a9e5674d":"sns.countplot(data=train_df, x='target')","e125368a":" print(\"Insincere questions: \", insincere_question.shape[0] \/ train_df.shape[0])\n print(\"Sincere questions: \", sincere_question.shape[0] \/ train_df.shape[0])","b656d113":"sincere = train_df[train_df['target'] == 0]#nh\u1eefng c\u00e2u h\u1ecfi kh\u00f4ng ph\u1ea3n c\u1ea3m th\u00ec s\u1ebd c\u00f3 nh\u00e3n 0\ninsincere = train_df[train_df['target'] == 1] #nh\u1eefng c\u00e2u h\u1ecfi kh\u00f4ng ph\u1ea3n c\u1ea3m th\u00ec s\u1ebd c\u00f3 nh\u00e3n 1","6927d0da":"from sklearn.utils import resample\ndf_train_sampled = pd.concat([resample(sincere, replace = True, n_samples = len(insincere)*5), insincere])\ndf_train_sampled #d\u1eef li\u1ec7u \u0111\u00e3 \u0111\u01b0\u1ee3c x\u1eed l\u00fd m\u1ea5t c\u00e2n b\u1eb1ng","cc89b86d":"sns.countplot(data=df_train_sampled, x='target')","d18fc633":"insincere_question.sample(n=5, random_state=4).values","b60afe25":"sincere_question.sample(n=5, random_state=4).values","ecb4b1c4":"contraction_dict = {\"ll\": \"will\", \"dont\": \"do not\", \"aint\": \"is not\", \"isnt\": \"is not\", \"doesnt\": \"does not\"\n, \"cant\": \"cannot\", \"mustnt\": \"must not\", \"hasnt\": \"has not\"\n, \"havent\": \"have not\", \"arent\": \"are not\", \"ain't\": \"is not\", \"aren't\": \"are not\"\n,\"can't\": \"cannot\", \"\u2018cause\": \"because\", \"could've\": \"could have\"\n, \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\"\n, \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\"\n,\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\"\n, \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\"\n, \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"Iam\": \"I am\", \"I've\": \"I have\"\n, \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\"\n,\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\"\n, \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\"\n, \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\"\n,\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\"\n, \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\"\n, \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\"\n, \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\"\n, \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\"\n, \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \n\"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\"\n, \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n\"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n\"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\"\n, \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n\"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n\"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \n\"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n\"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n\"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n\"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \n\"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \n\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \n\"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}","8c619b1c":"import re\nimport nltk\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nnltk.download('stopwords')\nlemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\nnltk_stopwords = stopwords.words('english')\ndef preprocessing(text):\n    # Data cleaning:\n    \n    text = re.sub('[0-9]{5,}','#####', text);\n    text = re.sub('[0-9]{4,}','####', text);\n    text = re.sub('[0-9]{3,}','###', text);\n    text = re.sub('[0-9]{2,}','##', text); \n    text = re.sub('[0-9]{1,}','#', text); \n    text = re.sub(re.compile('<.*?>'), '', text)\n    text = re.sub('[^A-Za-z0-9]+', ' ', text)\n    text = text.lower()\n\n    tokens = word_tokenize(text)\n    tokens = [contraction_dict.get(token) if (contraction_dict.get(token) != None) else token for token in tokens]\n    tokens = [w for w in tokens if w not in nltk_stopwords]\n    tokens = [stemmer.stem(token) for token in tokens]\n    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n\n    # n\u1ed1i l\u1ea1i c\u00e1c t\u1eeb v\u00e0o chu\u1ed7i sau khi x\u1eed l\u00fd\n    text = ' '.join(tokens) \n\n    return text\n","fa9bbe8b":"X_clean = []\nfor word in train_df.question_text:\n  X_clean.append(preprocessing(word))","2e98c5ba":"Y_clean = []\nfor word in df_train_sampled.question_text:\n  Y_clean.append(preprocessing(word))","00c75b5e":"train_df['cleaned_questions'] = X_clean\ntrain_df.to_csv('output_preprocessed.csv', index=False)","897c2877":"df_train_sampled['cleaned_questions'] = Y_clean\ndf_train_sampled.to_csv('Output_preprocessed.csv', index=False)","b5dfb215":"pre_data_before = pd.read_csv('output_preprocessed.csv')","a6e0b7aa":"preprocessed_data = pd.read_csv('Output_preprocessed.csv')","4e24eca3":"train_df ","d42745f7":"df_train_sampled","4efd637f":"from sklearn.model_selection import train_test_split","c47744da":"x, xtest, y, ytest = train_test_split(train_df['cleaned_questions'], train_df['target'], test_size = 0.2)","1ce8f1cb":"X_train, X_test, y_train, y_test = train_test_split(df_train_sampled['cleaned_questions'], df_train_sampled['target'], test_size = 0.2)","75f07cf6":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ncount_vectorizer = CountVectorizer(ngram_range=(1,2))\ntfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n","aa4a36d4":"model = LogisticRegression(C=1, random_state=0)\nvectorize_model_pipeline = Pipeline([\n    ('count_vectorizer', count_vectorizer),\n    ('model', model),\n])","7fea899e":"vectorize_model_pipeline.fit(X_train, y_train)","9f8ab310":"y_pred = vectorize_model_pipeline.predict(X_test)","36e4e7ef":"print('Accuracy :', accuracy_score(y_test, y_pred))\nprint('F1 score :', f1_score(y_test, y_pred))","7e473115":"print(classification_report(y_test, y_pred))","8d8e70f4":"vectorize_model_pipeline_ = Pipeline([\n    ('tfidf_vectorizer', tfidf_vectorizer),\n    ('model', model),\n])","d0a21515":"vectorize_model_pipeline_.fit(X_train, y_train)","2434afb0":"y_pred_ = vectorize_model_pipeline_.predict(X_test)","c95f5dc6":"print(classification_report(y_test, y_pred_))","5f74be74":"vectorize_model_pipeline.fit(x, y)","c2b90f85":"y_pred = vectorize_model_pipeline.predict(xtest)\nprint('Accuracy :', accuracy_score(ytest, y_pred))\nprint('F1 score :', f1_score(ytest, y_pred))\nprint(classification_report(ytest, y_pred))","ac230b39":"vectorize_model_pipeline_.fit(x, y)","462b0f9d":"y_pred = vectorize_model_pipeline_.predict(xtest)\nprint('Accuracy :', accuracy_score(ytest, y_pred))\nprint('F1 score :', f1_score(ytest, y_pred))\nprint(classification_report(ytest, y_pred))","e0c20846":"from sklearn.svm import LinearSVC\nmodel1 = LinearSVC(random_state=3, tol=0.01, loss='hinge', C=1, verbose=2)\nvectorize_model_pipeline1 = Pipeline([\n    ('count_vectorizer', count_vectorizer),\n    ('model1', model1),\n])\nvectorize_model_pipeline1.fit(X_train, y_train)","94688c0b":"y_pred1 = vectorize_model_pipeline1.predict(X_test)\nprint('Accuracy :', accuracy_score(y_test, y_pred1))\nprint('F1 score :', f1_score(y_test, y_pred1))\nprint(classification_report(y_test, y_pred1))","16f6f80c":"vectorize_model_pipeline1_ = Pipeline([\n    ('count_vectorizer', tfidf_vectorizer),\n    ('model1', model1),\n])\nvectorize_model_pipeline1_.fit(X_train, y_train)","4b784efa":"y_pred1 = vectorize_model_pipeline1_.predict(X_test)\nprint('Accuracy :', accuracy_score(y_test, y_pred1))\nprint('F1 score :', f1_score(y_test, y_pred1))\nprint(classification_report(y_test, y_pred1))","5ab00b83":"vectorize_model_pipeline1.fit(x, y)\ny_pred1 = vectorize_model_pipeline1.predict(xtest)\nprint('Accuracy :', accuracy_score(ytest, y_pred1))\nprint('F1 score :', f1_score(ytest, y_pred1))\nprint(classification_report(ytest, y_pred1))","dc918644":"vectorize_model_pipeline1_.fit(x, y)\ny_pred1 = vectorize_model_pipeline1_.predict(xtest)\nprint('Accuracy :', accuracy_score(ytest, y_pred1))\nprint('F1 score :', f1_score(ytest, y_pred1))\nprint(classification_report(ytest, y_pred1))","756ebb36":"# from sklearn.metrics import classification_report\n# print(classification_report(y_test, predictions1))","91a16aa0":"from sklearn.ensemble import RandomForestClassifier\ncount_vectorizer = CountVectorizer()\nmodel2 = RandomForestClassifier()\n\nvectorize_model_pipeline2 = Pipeline([\n    ('count_vectorizer', count_vectorizer),\n    ('model2', model2)])\nvectorize_model_pipeline2.fit(X_train, y_train)\npredictions2 = vectorize_model_pipeline2.predict(X_test)\n\nprint('Accuracy :', accuracy_score(y_test, predictions2))\nprint('F1 score :', accuracy_score(y_test, predictions2))","38276ba2":"print(classification_report(y_test, predictions2))","b9e5c03d":"test_df['preprocessing'] = test_df['question_text'].apply(preprocessing) #ti\u1ec1n x\u1eed l\u00fd v\u1edbi d\u1eef li\u1ec7u test","4807ba3d":"predictions = vectorize_model_pipeline.predict(test_df['preprocessing']) #d\u1ef1 \u0111o\u00e1n k\u1ebft qu\u1ea3","cf49fcdb":"test_df['prediction'] = predictions\nresults = test_df[['qid', 'prediction']]\nresults.to_csv('submission.csv', index=False) #l\u01b0u v\u00e0o file submission","f831c87b":"results.head()","9be3176e":"Train b\u1eb1ng ph\u01b0\u01a1ng ph\u00e1p CountVectorizer v\u1edbi t\u1eadp d\u1eef li\u1ec7u sau x\u1eed l\u00fd m\u1ea5t c\u00e2n b\u1eb1ng:","f2c9945f":"**1. \u0110\u1ecbnh ngh\u0129a b\u00e0i to\u00e1n**\n<p>M\u1ed9t v\u1ea5n \u0111\u1ec1 t\u1ed3n t\u1ea1i \u0111\u1ed1i v\u1edbi b\u1ea5t k\u1ef3 trang web l\u1edbn n\u00e0o hi\u1ec7n nay l\u00e0 l\u00e0m th\u1ebf n\u00e0o \u0111\u1ec3 x\u1eed l\u00fd n\u1ed9i dung \u0111\u1ed9c h\u1ea1i v\u00e0 g\u00e2y chia r\u1ebd. Quora mu\u1ed1n gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1 n\u00e0y tr\u1ef1c ti\u1ebfp \u0111\u1ec3 gi\u1eef cho n\u1ec1n t\u1ea3ng c\u1ee7a h\u1ecd tr\u1edf th\u00e0nh m\u1ed9t n\u01a1i m\u00e0 ng\u01b0\u1eddi d\u00f9ng c\u00f3 th\u1ec3 c\u1ea3m th\u1ea5y an to\u00e0n khi chia s\u1ebb ki\u1ebfn th\u1ee9c c\u1ee7a h\u1ecd v\u1edbi c\u1ed9ng \u0111\u1ed3ng.<\/p>\n<p>Quora l\u00e0 m\u1ed9t n\u1ec1n t\u1ea3ng cho ph\u00e9p m\u1ecdi ng\u01b0\u1eddi h\u1ecdc h\u1ecfi v\u00e0 chia s\u1ebb tri th\u1ee9c. T\u1ea1i Quora, m\u1ecdi ng\u01b0\u1eddi c\u00f3 th\u1ec3 \u0111\u1eb7t c\u00e2u h\u1ecfi v\u00e0 k\u1ebft n\u1ed1i v\u1edbi nh\u1eefng ng\u01b0\u1eddi kh\u00e1c. M\u1ed9t th\u00e1ch th\u1ee9c l\u1edbn, \u0111\u1ed3ng th\u1eddi c\u0169ng l\u00e0 m\u1ee5c ti\u00eau c\u1ee7a b\u00e0i to\u00e1n, l\u00e0 x\u00e1c \u0111\u1ecbnh \u0111\u01b0\u1ee3c nh\u1eefng c\u00e2u h\u1ecfi c\u00f3 n\u1ed9i dung nh\u1ea1y c\u1ea3m \u0111\u1ec3 c\u00f3 th\u1ec3 lo\u1ea1i b\u1ecf ch\u00fang.<\/p>\n<p>Input: C\u00e2u h\u1ecfi t\u1eeb Quora<\/p>\n<p>Output: gi\u00e1 tr\u1ecb 0 ho\u1eb7c 1 (0: c\u00e2u h\u1ecfi kh\u00f4ng ph\u1ea3n c\u1ea3m; 1: c\u00e2u h\u1ecfi ph\u1ea3n c\u1ea3m)<\/p>\n","8a6a9c8d":"T\u1eadp train g\u1ed3m c\u00f3 1306122 c\u00e2u h\u1ecfi, t\u1eadp test g\u1ed3m c\u00f3 375806 c\u00e2u h\u1ecfi","a1d9dd43":"**M\u00f4 h\u00ecnh Linear SVM**","ba5a17d1":"M\u1ed9t s\u1ed1 c\u00e2u h\u1ecfi ph\u1ea3n c\u1ea3m trong t\u1eadp d\u1eef li\u1ec7u:","dda55d88":"Train b\u1eb1ng ph\u01b0\u01a1ng ph\u00e1p CounerVectorizer v\u1edbi t\u1eadp d\u1eef li\u1ec7u sau x\u1eed l\u00fd m\u1ea5t c\u00e2n b\u1eb1ng:","081a6f02":"Train b\u1eb1ng ph\u01b0\u01a1ng ph\u00e1p TF_IDFVectorizer v\u1edbi t\u1eadp d\u1eef li\u1ec7u sau x\u1eed l\u00fd m\u1ea5t c\u00e2n b\u1eb1ng:","6bea1934":"Th\u1ef1c hi\u1ec7n c\u00f4ng vi\u1ec7c ti\u1ec1n x\u1eed l\u00fd v\u1edbi t\u1eadp d\u1eef li\u1ec7u train sau khi x\u1eed l\u00fd m\u1ea5t c\u00e2n b\u1eb1ng:","50ece7f7":"C\u00f3 th\u1ec3 th\u1ea5y, d\u1eef li\u1ec7u gi\u1eefa 2 l\u1edbp b\u1ecb m\u1ea5t c\u00e2n b\u1eb1ng l\u1edbn (nh\u00e3n 0 chi\u1ebfm 93,81% c\u00f2n nh\u00e3n 1 chi\u1ebfm 6.19%). Vi\u1ec7c m\u1ea5t c\u00e2n b\u1eb1ng d\u1eef li\u1ec7u s\u1ebd g\u00e2y ra kh\u00f3 kh\u0103n trong vi\u1ec7c d\u1ef1 \u0111o\u00e1n l\u1edbp thi\u1ec3u s\u1ed1. V\u00ec v\u1eady, ngo\u00e0i vi\u1ec7c hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh, em s\u1ebd gi\u1ea3i quy\u1ebft th\u00eam v\u1ea5n \u0111\u1ec1 m\u1ea5t c\u00e2n b\u1eb1ng d\u1eef li\u1ec7u \u0111\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c k\u1ebft qu\u1ea3 t\u1ed1t h\u01a1n v\u1edbi ph\u01b0\u01a1ng ph\u00e1p undersampling v\u1edbi t\u1ec9 l\u1ec7 l\u1edbp 1:l\u1edbp 0 = 1:4.","c38a638d":"T\u1eadp train, test tr\u01b0\u1edbc khi x\u1eed l\u00fd m\u1ea5t c\u00e2n b\u1eb1ng:","80aba10e":"**3. Hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh**","38f9059f":"C\u00f3 th\u1ec3 th\u1ea5y r\u1eb1ng, \u0111\u1ed9 \u0111o F1 c\u1ee7a m\u00f4 h\u00ecnh (nh\u00e3n 1) v\u1edbi t\u1eadp d\u1eef li\u1ec7u sau khi x\u1eed l\u00fd m\u1ea5t c\u00e2n b\u1eb1ng t\u0103ng kh\u00e1 nhi\u1ec1u:\nCountVectorizer: 0.74 so v\u1edbi 0.54, TF_IDFVectorizer: 0.76 so v\u1edbi 0.56","f0b8f325":"Train b\u1eb1ng ph\u01b0\u01a1ng ph\u00e1p TF_IDFVectorizer v\u1edbi t\u1eadp d\u1eef li\u1ec7u tr\u01b0\u1edbc x\u1eed l\u00fd m\u1ea5t c\u00e2n b\u1eb1ng:","e3025fdf":"**N\u1ed9p k\u1ebft qu\u1ea3**","9be3e290":"Th\u00eam tr\u01b0\u1eddng c\u00e2u h\u1ecfi \u0111\u00e3 \u0111\u01b0\u1ee3c ti\u1ec1n x\u1eed l\u00fd \u0111\u1ed1i v\u1edbi c\u1ea3 2 t\u1eadp d\u1eef li\u1ec7u:","99b7b0c5":"C\u00e1c l\u1ea7n submit cho th\u1ea5y m\u00f4 h\u00ecnh Logistic Regession v\u1edbi CountVectorizer cho k\u1ebft qu\u1ea3 t\u1ed1t nh\u1ea5t.","5c4f3ab1":"2.2. Ti\u1ec1n x\u1eed l\u00fd","75446cb5":"**M\u00f4 h\u00ecnh Logistic Regression**","7c654fb4":"M\u1ed9t s\u1ed1 c\u00e2u h\u1ecfi kh\u00f4ng ph\u1ea3n c\u1ea3m trong t\u1eadp d\u1eef li\u1ec7u:","8fd6b97b":"Train b\u1eb1ng ph\u01b0\u01a1ng ph\u00e1p TF_IDFVectorizer v\u1edbi t\u1eadp d\u1eef li\u1ec7u sau x\u1eed l\u00fd m\u1ea5t c\u00e2n b\u1eb1ng:","b1c60039":"<p>D\u1eef li\u1ec7u g\u1ed3m 3 c\u1ed9t:<\/p>\n<p>qid: m\u00e3 s\u1ed1 c\u00e2u h\u1ecfi<\/p>\n<p>question_text: c\u00e2u h\u1ecfi<\/p> \n<p>target: nh\u00e3n d\u1eef li\u1ec7u (0 ho\u1eb7c 1)<\/p>","2c8c6fc6":"C\u00f3 th\u1ec3 th\u1ea5y r\u1eb1ng, \u0111\u1ed9 \u0111o F1 c\u1ee7a m\u00f4 h\u00ecnh (nh\u00e3n 1) v\u1edbi t\u1eadp d\u1eef li\u1ec7u sau khi x\u1eed l\u00fd m\u1ea5t c\u00e2n b\u1eb1ng t\u0103ng kh\u00e1 nhi\u1ec1u:\nCountVectorizer: 0.73 so v\u1edbi 0.53, TF_IDFVectorizer: 0.74 so v\u1edbi 0.54","8ac9f7be":"2.1. Kh\u1ea3o s\u00e1t d\u1eef li\u1ec7u","17be0a99":"Train b\u1eb1ng ph\u01b0\u01a1ng ph\u00e1p TF-IDFVectorizer v\u1edbi t\u1eadp d\u1eef li\u1ec7u tr\u01b0\u1edbc x\u1eed l\u00fd m\u1ea5t c\u00e2n b\u1eb1ng:","f903b16b":"T\u1eadp train, test sau khi x\u1eed l\u00fd m\u1ea5t c\u00e2n b\u1eb1ng:","3f4560d2":"<h3>H\u1ecd t\u00ean: Ph\u01b0\u01a1ng Anh M\u1ef9<\/h3>\n<h3>MSSV: 18020918<\/h3>","84d2fe6d":"2 t\u1eadp d\u1eef li\u1ec7u \u0111\u00f3 c\u00f3 d\u1ea1ng sau:","94e1871a":"Ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u bao g\u1ed3m c\u00e1c vi\u1ec7c: lo\u1ea1i b\u1ecf t\u1eeb d\u1eebng, lo\u1ea1i b\u1ecf s\u1ed1, lo\u1ea1i b\u1ecf d\u1ea5u c\u00e2u, chuy\u1ec3n t\u1eeb v\u1ec1 d\u1ea1ng r\u00fat g\u1ecdn.","63f9e7ca":"Train b\u1eb1ng ph\u01b0\u01a1ng ph\u00e1p CountVectorizer v\u1edbi t\u1eadp d\u1eef li\u1ec7u tr\u01b0\u1edbc x\u1eed l\u00fd m\u1ea5t c\u00e2n b\u1eb1ng:","a290fbab":"Th\u1ef1c hi\u1ec7n c\u00f4ng vi\u1ec7c ti\u1ec1n x\u1eed l\u00fd v\u1edbi t\u1eadp d\u1eef li\u1ec7u train tr\u01b0\u1edbc khi x\u1eed l\u00fd m\u1ea5t c\u00e2n b\u1eb1ng:","7e79ab1b":"Train b\u1eb1ng ph\u01b0\u01a1ng ph\u00e1p CountVectorizer v\u1edbi t\u1eadp d\u1eef li\u1ec7u tr\u01b0\u1edbc x\u1eed l\u00fd m\u1ea5t c\u00e2n b\u1eb1ng:","57736ec8":"M\u00f4 h\u00ecnh s\u1eed d\u1ee5ng: Logistic Regression v\u00e0 Linear SVM. Ngo\u00e0i ra, em c\u00f3 th\u1eed s\u1eed d\u1ee5ng m\u00f4 h\u00ecnh Random Forest nh\u01b0ng th\u1eddi gian ch\u1ea1y kh\u00e1 l\u00e2u so v\u1edbi c\u00e1c m\u00f4 h\u00ecnh h\u1ecdc m\u00e1y c\u01a1 b\u1ea3n kh\u00e1c.\nPh\u01b0\u01a1ng ph\u00e1p vector h\u00f3a d\u1eef li\u1ec7u: CountVectorizer v\u00e0 TF-IDFVectorizer (n-gram range: (1,2))\nChia d\u1eef li\u1ec7u \u0111\u1ec3 hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh v\u00e0 d\u1ef1 \u0111o\u00e1n k\u1ebft qu\u1ea3 theo t\u1ec9 l\u1ec7 test data:train data = 2:8. \nEm s\u1ebd train v\u1edbi c\u1ea3 2 b\u1ed9 d\u1eef li\u1ec7u: tr\u01b0\u1edbc khi x\u1eed l\u00fd m\u1ea5t c\u00e2n b\u1eb1ng v\u00e0 sau khi x\u1eed l\u00fd m\u1ea5t c\u00e2n b\u1eb1ng (t\u01b0\u01a1ng \u0111\u01b0\u01a1ng v\u1edbi 8 l\u1ea7n train m\u00f4 h\u00ecnh).","3b4bed6d":"* Tr\u1ef1c quan h\u00f3a d\u1eef li\u1ec7u","dfc6167c":"**2. D\u1eef li\u1ec7u**"}}