{"cell_type":{"a58ea097":"code","b4758d1a":"code","966bfe2c":"code","184e1a65":"code","1a3c1b88":"code","14b57ae9":"code","357986ab":"code","046c0a0e":"code","a40482bb":"markdown","4a30580f":"markdown","9250d4da":"markdown","5189c89b":"markdown","c12fde7d":"markdown","045f7a2d":"markdown","c915718a":"markdown","4a2275fe":"markdown","8cf1bb43":"markdown","0d735fea":"markdown","cd87dbe2":"markdown","4d871b09":"markdown","0fb24a20":"markdown","b5ed3901":"markdown"},"source":{"a58ea097":"# Import required modules\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # data visualisation\nimport seaborn as sns # data visualisation\nfrom sklearn.preprocessing import StandardScaler # Scale features\nfrom sklearn.pipeline import Pipeline # Manage model transformations\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n#Import classification models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom scipy.stats import expon\n# Force plots to render inside the notebook\n%matplotlib inline","b4758d1a":"# Load training and testing data\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nX_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n# Inspect training data\nprint('Info for Training Data')\nprint(train.info(), '\\n')\n\n# Split training data into features and target variable\nX_train, y_train = train.drop('Survived', axis=1), train.Survived","966bfe2c":"def summarise_missing(df):\n    '''Returns fields which have missing values, and proportion of missing values'''\n    return df.isna().mean()[df.isna().mean() > 0]\n\nprint(\"Missing data from training set:\\n\")\nprint(summarise_missing(X_train))\n\nprint(\"\\nMissing data from testing set:\\n\")\nprint(summarise_missing(X_test))","184e1a65":"def unique_ratio(df):\n    '''Return ratio of unique values versus total valid values in each categorical (object) field'''\n    df = df.select_dtypes(include='object')\n    return df.nunique() \/ (~df.isna()).sum()\n\nprint('Ratio of unique values in training dataset categorical fields:\\n')\nprint(unique_ratio(X_train))\n","1a3c1b88":"X_train.Name.head(5)","14b57ae9":"def titles(df=None, title_regex=r',\\s(\\w+\\.)',common_titles=['Mrs.', 'Mr.', 'Miss.', 'Master.']):\n    '''Returns a new dataframe with a title field added, based on finding title_regex in name_field '''\n    tmp = df.Name.str.extract(title_regex)\n    tmp[~tmp.isin(common_titles)] = 'Other'\n    return tmp\n\ndef barplot_survival_by_cat(df, category):\n    survival_grouped = df.groupby(category, as_index=False).Survived.mean()\n    ax = sns.barplot(x=category, y='Survived', data=df)\n    plt.title('Survival Rate by {}'.format(category))\n    plt.xlabel(category)\n    plt.ylabel('Survival Rate')\n    return ax\n\n# Accesing the original train dataframe, so that we also have access to the target variable\ntrain['Title'] = titles(train)\n\n# Produce bar plot of Survival by Title Category \nbarplot_survival_by_cat(train, 'Title')","357986ab":"class DropFields(object):\n    # Transforms dataframe by dropping specified fields\n    def __init__(self, cols):\n        self.cols = cols\n    \n    def transform(self, x) :\n        tmp = x.copy()\n        return tmp.drop(self.cols, axis=1)\n\n    def fit(self, x, y=None) :\n        return self\n    \nclass AddFeatures(object):\n    # Transforms dataframe by adding features calculated using other features, which are each added \n    def __init__(self, features):\n        self.features = features\n        \n    def transform(self, x):\n        tmp = x.copy()\n        for name, func in self.features.items():\n            tmp[name] = func(tmp)\n        return tmp\n    \n    def fit(self, x, y=None):\n        return self\n    \n    \nclass GetDummies(object):\n    # Transforms dataframe by replacing categorical fields with corresponding fields\n    def __init__(self):\n        pass\n\n    def transform(self, x):\n        tmp = x.copy()\n        return pd.get_dummies(tmp)\n    \n    def fit(self, x, y=None):\n        return self\n\nmodels = {'Logistic Regression': LogisticRegression(),\n          'Standard Vector Machine': SVC(), \n          'Decision Tree': DecisionTreeClassifier(),\n          'K-Nearest Neigbours': KNeighborsClassifier(),\n          'Gradient Boosting': GradientBoostingClassifier(),\n          'Random Forest': RandomForestClassifier(),\n          'Gaussian Naive Bayes': GaussianNB()}\n\nt_steps = [('feature_adder', AddFeatures({'Title': titles})),\n           ('dropper', DropFields(['Name', 'Ticket', 'Cabin'])),\n           ('dummies', GetDummies()),\n           ('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n           ('scaler', StandardScaler())]\n\ndef model_scorer(X_train, y_train, models, t_steps, cv=5):\n    df = pd.DataFrame()\n    model_names = []\n    results = []\n    for model_name, model_type in models.items():\n        model_names.append(model_name)\n        steps = t_steps + [(model_name, model_type)]\n        model = Pipeline(steps)\n        c = cross_val_score(model, X_train, y_train, cv=cv)\n        results.append(c.mean())\n    df['classifier'] = models.keys()\n    df['accuracy'] = results\n    return df\n\nmodels_graded = model_scorer(X_train, y_train, models, t_steps, cv=5)\nprint(models_graded)\n\nsns.set()\nsns.barplot(x='accuracy', y='classifier', data=models_graded, orient=\"h\")\n\n    ","046c0a0e":"steps = t_steps + [('Standard Vector Machine', SVC())]\nmodel = Pipeline(steps).fit(X_train, y_train)\n\npred_df = pd.DataFrame()\npred_df['PassengerId'] = X_test.PassengerId\npred_df['Survived'] = model.predict(X_test)\n\npred_df.to_csv('submission6.csv', index=False)","a40482bb":"# 5 Modelling\nHere we create a transformation pipeline to:\n* Drop selected fields\n* Add features (in particular, title field)\n* Impute values\n* Scale numeric fields\n* Fit the data to various classification models\n* Perform cross validation to measure the accuracy of each model\n* Plot the accuracy of each model","4a30580f":"There is a large amount of data missing from the Cabin field, it may therefore be advisable to drop this column rather than impute. There is a smaller amount of data missing from the numeric Age field, so we can impute using the mean or median for the field. ","9250d4da":"We observe that there are a high proportion of unique values (of the total valid values) in the Name (100%), Ticket (76%) and Cabin (72%).\n\nTaking these fields in their raw format will likely to lead to overfitting so in the name of simplicity we will drop them, before doing so we will investigate if there is any new features we can pull out from the values of the Name and Cabin features.","5189c89b":"I have submitted prediction to Kaggle on 2021-02-08, it scored 78.5%.","c12fde7d":"# 5 Extracting Usable Data from Name Field\nWe have seen above that taking the name field in raw terms will not be sensible for our modelling. However we observe that it seems to follow a standard format, and therefore we should be able to pull out the passengers' titles by using regex.","045f7a2d":"# 2 Load Data\nLoad data from csv files into training \/ testing dataframes, and inspect contents.****","c915718a":"# 1 Imports\nImport required libraries, and force plots to render inside the notebook.","4a2275fe":"# Passenger Surival Prediction on the Titanic\n\n# 0 Introduction\nThis is my first notebook, it summarises my work on the [Titanic data set](https:\/\/www.kaggle.com\/c\/titanic\/).\n\nWe have been provided with two datasets - namely a training set and a testing set. Both datasets have observations correspoding to individuals who were on the Titanic, they also both have various features corresponding to the individuals - e.g. Age, Gender. The training set also contains the values for the target variable - which is a boolean value representing whether the individual survived.\n\nThe goal is produce a model, train it on the provided training data, and make predictions for the surivival outcomes for the test dataset.\n","8cf1bb43":"All the tested classification models give a mean accuracy of 0.785 - 0.825 using a 5-fold cross-validation on the training data. The best models (with no hyperparameter tuning) were Standard Vector Machine and Logistics Regression, so we will use these models (independently) to predict surival outcomes for the testing set.","0d735fea":"# 4 Categorical Fields\nThere are some categorical (object) fields - we should check the number of unique values contained within to decide whether to include them in our modelling.\n","cd87dbe2":"# 7 Improvements\nTo improve on the accuracy of the predictions I intend to:\n* Use Cabin field in the model\n* Combine features to add additional features, e.g. Age and Gender.\n* Categorise some of the numeric data - e.g. (1) young, middle aged, elderly (2) Travelled alone, travelled with others\n* Tune hyperparameters of best performing models","4d871b09":"# 3 Missing Values\nOn the whole most data seems to be present (see above), but there is some missing data. So we should look into that further before deciding how to impute.","0fb24a20":"The title of a passenger looks like it will be a good indicator of survival, it would therefore be a good idea to add it to our model. Although it looks very promising we should note that some of it's predictive power may already be covered off by use of Age and Sex in the model.","b5ed3901":"# 6 Making Predictions\nWe will now use the best model (Standard Vector Machine) to predict the target variable for the testing set."}}