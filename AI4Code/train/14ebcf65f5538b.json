{"cell_type":{"2ffdda46":"code","d157e737":"code","fb2ee8cd":"code","7b284bbd":"code","f8ff8b9a":"code","2181dc49":"code","6585c176":"code","8e061aa0":"code","a1b37dec":"code","c20ae827":"code","ee4758ee":"code","54710bf3":"code","060e41ed":"code","8b70d9c5":"code","7c3795c9":"code","58d0174a":"code","f545ae6e":"code","94c6b960":"code","dc5e484d":"code","067fa069":"code","43316106":"code","faa1639d":"code","18e260ce":"code","3dfcc8a9":"code","ee27ec3d":"code","f408e2d8":"code","b1dbe5ea":"code","09b9b22a":"code","8c906361":"code","09e90576":"code","535cff3f":"code","d306b815":"code","f8b53898":"code","d0b04032":"code","f58f76ac":"code","cba8ea50":"code","b8db93d9":"code","7b53f19a":"code","2825a7ec":"code","747d8380":"markdown","567daccd":"markdown","ecb18a3c":"markdown","cd0aba1f":"markdown","3c12172e":"markdown","cc1741a4":"markdown","d509a021":"markdown","04373b59":"markdown","e4e9ee48":"markdown","0f8e15b0":"markdown","a301e564":"markdown","0c91f632":"markdown","e703b959":"markdown","13422e6b":"markdown","70630a78":"markdown","d82a66e7":"markdown","b926d085":"markdown","94c19495":"markdown","399baf8d":"markdown","c3d58304":"markdown"},"source":{"2ffdda46":"import torch \nimport numpy as np\nimport sklearn\nfrom sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\nfrom sklearn import  ensemble, preprocessing, metrics\nfrom sklearn.base import  clone\nimport matplotlib\nimport seaborn\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport random\nfrom catboost import CatBoostClassifier\nfrom torch.utils.data import Dataset,DataLoader\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom torch import nn\nimport os\nBASEPATH=\"..\/input\/agriculture-master-competition\"","d157e737":"from sklearn.ensemble import RandomForestClassifier,BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier,IsolationForest,RandomTreesEmbedding\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier,MultiTaskElasticNet\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB\nfrom sklearn.linear_model import (LinearRegression, TheilSenRegressor, RANSACRegressor, HuberRegressor)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve,KFold\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n","fb2ee8cd":"def seed_torch(seed=31):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nrandom_state=31\nseed_torch(random_state)","7b284bbd":"def read_csvfile(filepath):\n    df = pd.read_csv(filepath)\n    return df\ninput_df = read_csvfile(os.path.join(BASEPATH,\"train_data.csv\"))\ninput_df_tmp = read_csvfile(os.path.join(BASEPATH,\"train_data.csv\"))\nlabelcolumns = list(input_df.columns[20:])\nlabelcolumns14 = list(input_df.columns[20:24])\nlabelcolumns57 = list(input_df.columns[24:27])\nlabelcolumns1011 = list(input_df.columns[29:])\nprint(labelcolumns14,labelcolumns57,labelcolumns1011)\nsingle_columns='actuator09'\nsingle2_columns='actuator08'\n#single3_columns='actuator05'\n#labelcolumns811.remove(single_columns)\nfeaturecolumns = list(input_df.columns[1:20])\nfeaturecolumns.remove(\"d.rainfall_detect\")\n#featurecolumns = [\"d.log_time\",\"d.temperature_B\",\"d.humidity_A\",\"d.soil_temperature\",\"d.soil_humidity\",\"d.soil_EC\",\"d.soil_PH\",\"d.outside_temperature\",\"d.outside_humidity\",\"d.radiometric\",\"d.rainfall\",\"d.wind_speed\"]\n#featurecolumns.remove(\"d.rainfall\")\n#featurecolumns.remove(\"d.wind_direction\")\nprint(featurecolumns)\nrandom_state = 31","f8ff8b9a":"def GetLogDate(input_df):\n    input_df[\"d.log_date\"] = [int(i.split(' ')[0].split('\/')[1].zfill(2)+i.split(' ')[0].split('\/')[2].zfill(2))  for i in input_df[\"d.log_time\"].tolist()]\n    return input_df\ndef GetLogTime(input_df):\n    input_df[\"d.log_time\"] = [int(i.split(' ')[1].replace(':','')) for i in input_df[\"d.log_time\"].tolist()]\n    return input_df\ndef AvgSameData(input_df):\n    input_df[\"d.temperature\"] = (input_df[\"d.temperature_A\"]+input_df[\"d.temperature_B\"])\/2\n    input_df[\"d.humidity\"] = (input_df[\"d.humidity_A\"]+input_df[\"d.humidity_B\"])\/2\n    return input_df\nprint(input_df[\"d.log_time\"][0])\ninput_df=GetLogDate(input_df)\ninput_df=GetLogTime(input_df)\nprint(input_df[\"d.log_date\"][0])\nprint(input_df[\"d.log_time\"][0])\nfeaturecolumns.append(\"d.log_date\")\n\n#input_df = AvgSameData(input_df)\n#featurecolumns.remove(\"d.temperature_A\")\n#featurecolumns.remove(\"d.temperature_B\")\n#featurecolumns.remove(\"d.humidity_A\")\n#featurecolumns.remove(\"d.humidity_B\")\n#featurecolumns.append(\"d.temperature\")\n#featurecolumns.append(\"d.humidity\")\n","2181dc49":"def addfeature(df):\n    df[\"d.stage\"] = df[\"d.log_date\"].map(lambda x: 0 if x<=806 else (1 if x<=830 else (2 if x<=1006 else 3)))\n    df[\"d.diff_temperature\"] = df[\"d.temperature_A\"] -  df[\"d.temperature_B\"]\n    df[\"d.diff_humidity\"] = df[\"d.humidity_A\"] -  df[\"d.humidity_B\"]\n    df[\"d.odiff_photometric\"] = df[\"d.photometric\"] - df[\"d.outside_photometric\"]\n    df[\"d.odiff_temperature\"] = ((df[\"d.temperature_A\"] + df[\"d.temperature_B\"])\/2) - df[\"d.outside_temperature\"]\n    df[\"d.odiff_humidity\"] = ((df[\"d.humidity_A\"] + df[\"d.humidity_B\"])\/2) - df[\"d.outside_humidity\"]\n    return df\n\ninput_df = addfeature(input_df)\nfeaturecolumns.append(\"d.stage\")\nfeaturecolumns.append(\"d.diff_temperature\")\nfeaturecolumns.append(\"d.diff_humidity\")\nfeaturecolumns.append(\"d.odiff_photometric\")\nfeaturecolumns.append(\"d.odiff_temperature\")\nfeaturecolumns.append(\"d.odiff_humidity\")\ninput_df[featurecolumns].describe()","6585c176":"sns.lineplot(data=input_df,x=\"d.log_date\",y=\"d.stage\")","8e061aa0":"def logData(df,columnsname):\n    df[columnsname]=df[columnsname].apply(lambda x: x if x!=0 else 000.1)\n    df[columnsname] = np.log(df[columnsname]).fillna(0)\n    return df\n#input_df = logData(input_df,\"d.wind_direction\")\n#input_df[featurecolumns].describe()","a1b37dec":"\n\n#xgbf = XGBClassifier(n_estimators=30)\n#xgbf=lgb.LGBMClassifier(n_estimators= 10,learning_rate=0.1,subsample_for_bin=10000,random_state=random_state )\n# feature_predict_list=[]\nfeature_predict_list.append(RANSACRegressor(random_state=random_state))\nfeature_predict_list.append(RANSACRegressor(random_state=random_state))\ndef replacezerovalues(df,columnsname,model_idx=0,train=True):\n    featureX = df[df[columnsname] > 0][featurecolumns].copy()\n    LabelX =featureX[columnsname].copy()\n    featureX = featureX.drop(columns=columnsname)\n    \n    \n    if train:\n        #xgbf.fit(np.array(featureX), np.array(LabelX))\n        feature_predict_list[model_idx].fit(np.array(featureX,dtype=int), np.array(LabelX,dtype=int))\n    \n    featureY = df[featurecolumns].copy()\n    featureY = featureY.drop(columns=columnsname)\n    #print(featureY.columns,len(featureY.columns))\n    \n    \n    #newLabelY = xgbf.predict(np.array(featureY))\n    newLabelY = feature_predict_list[model_idx].predict(np.array(featureY,dtype=int))\n    df[columnsname] = newLabelY\n    return df\n\ncolumnsnames=[\"d.wind_speed\"]#,\"d.wind_speed\"]\nfor idx,columnsname in enumerate(columnsnames):\n    input_df = replacezerovalues(input_df,columnsname,idx)","c20ae827":"def dataNormalized(feature_df,zeromean=True):\n    if zeromean:\n        feature_df = (feature_df - feature_df.mean())\/feature_df.std()\n    else:\n        feature_df=(feature_df-feature_df.min())\/(feature_df.max()-feature_df.min())\n    return feature_df\n#input_df[featurecolumns] = dataNormalized(input_df[featurecolumns],False)","ee4758ee":"def balanceDataset(train_df,valid_df,minnum):\n    #for lc in labelcolumns:\n    #    print(len(train_df[train_df[lc] == 0]),len(valid_df[valid_df[lc] == 0]),len(train_df[train_df[lc] == 1]),len(valid_df[valid_df[lc] == 1]))\n    #print(\"===\")\n    for lc in labelcolumns:\n        l=0 if len(valid_df[valid_df[lc] == 1]) > len(valid_df[valid_df[lc] == 0]) else 1\n        trainn = len(train_df[train_df[lc] == l])\n        validn=len(valid_df[valid_df[lc] == l])\n        if validn+1 < minnum*(trainn+validn):\n            moven = (trainn+validn)*minnum-validn\n            move_rows = train_df[train_df[lc] == l].sample(frac=moven\/trainn).copy()\n            valid_df=valid_df.append(move_rows)\n            train_df=train_df.drop(move_rows[\"index\"])\n\n            validnn=len(valid_df[valid_df[lc] == (not l)])\n            if validnn > len(move_rows):\n                move_rows = valid_df[valid_df[lc] == (not l)].sample(frac=len(move_rows)\/validnn).copy()\n                train_df=train_df.append(move_rows)\n                valid_df=valid_df.drop(move_rows[\"index\"])\n    #for lc in labelcolumns:\n    #    print(len(train_df[train_df[lc] == 0]),len(valid_df[valid_df[lc] == 0]),len(train_df[train_df[lc] == 1]),len(valid_df[valid_df[lc] == 1]))\n    return train_df,valid_df\n\n\ndef splitDataframe(df,train_sample):\n    shuffle_df = df.sample(frac=1,random_state=random.randint(1,1000000))\n    train_df,valid_df,_ = np.split(shuffle_df,[int(train_sample*len(shuffle_df)),int((train_sample+0.15)*len(shuffle_df))])\n    #print(len(train_df),len(valid_df))\n    #train_df,valid_df=balanceDataset(train_df,valid_df,1-train_sample)\n    #print(len(train_df),len(valid_df))\n    return train_df,valid_df\n            \ntrain_df,valid_df=splitDataframe(input_df,0.65)\nprint(len(train_df),len(valid_df))\n#featureX = dataNormalized(train_df[featurecolumns],True)\n#featureY = dataNormalized(valid_df[featurecolumns],True)","54710bf3":"def trainDataframeBalance(df):\n    balancecolumns = labelcolumns[:6]\n    balance_df = pd.DataFrame(columns = df.columns)\n    for cn in balancecolumns:\n        balance_df=balance_df.append(df[df[cn] == 0].copy())\n        \n    balancecolumns = labelcolumns[-2:]\n    for cn in balancecolumns:\n        balance_df=balance_df.append(df[df[cn] == 1].copy())\n        \n    #showactuatorplot(balance_df)\n    balancecolumns = labelcolumns[:6]\n    for cn in balancecolumns:\n        tmp_zero=balance_df[balance_df[cn] == 0]\n        tmp_one=balance_df[balance_df[cn] == 1]\n        if 2*len(tmp_one)<len(tmp_zero):\n            balance_df=balance_df.append(df[df[cn] == 1].sample(frac=(len(tmp_zero)-len(tmp_one))\/(len(tmp_zero)*3)))\n    #showactuatorplot(balance_df)\n    return balance_df\n#balance_df=trainDataframeBalance(train_df)\n        ","060e41ed":"def removeOutlier(input_df,times=3):\n    input_df[\"d.wind_speed\"] = input_df[\"d.wind_speed\"].apply(lambda x: 0 if x<0 else x)\n    input_df = input_df[abs(input_df[\"d.photometric\"]-input_df[\"d.photometric\"].mean())<input_df[\"d.photometric\"].std()*times]\n    input_df = input_df[abs(input_df[\"d.outside_photometric\"]-input_df[\"d.outside_photometric\"].mean())<input_df[\"d.outside_photometric\"].std()*times]\n    input_df = input_df[abs(input_df[\"d.radiometric\"]-input_df[\"d.radiometric\"].mean())<input_df[\"d.radiometric\"].std()*times]\n    \n    return input_df\ntrain_df = removeOutlier(train_df,5)\n\n","8b70d9c5":"def splitDataset(df):\n    featureX = df[featurecolumns]\n    labelX = df[labelcolumns]\n    return featureX,labelX\nfeatureX,labelX = splitDataset(train_df)\nfeatureY,labelY = splitDataset(valid_df)\nlabelX09 = train_df[single_columns]\nlabelY09 = valid_df[single_columns]\nlabelX14 = train_df[labelcolumns14]\nlabelY14 = valid_df[labelcolumns14]\nlabelX57 = train_df[labelcolumns57]\nlabelY57 = valid_df[labelcolumns57]\nlabelX1011 = train_df[labelcolumns1011]\nlabelY1011 = valid_df[labelcolumns1011]\nlabelX08 = train_df[\"actuator08\"]\nlabelY08 = valid_df[\"actuator08\"]\nlabelX10 = train_df[\"actuator10\"]\nlabelY10 = valid_df[\"actuator10\"]\nlabelX11 = train_df[\"actuator11\"]\nlabelY11 = valid_df[\"actuator11\"]","7c3795c9":"def printperformance(predictions,label):\n    accuracy = metrics.accuracy_score(label, predictions)\n    print(accuracy)\n    print(metrics.precision_recall_fscore_support(label, predictions,average='micro'))","58d0174a":"class Stacking_Model(object):\n    def __init__(self,classifiers,meta_model,random_state):\n        self.classifiers = list(classifiers.values())\n        self.meta_model=meta_model\n        self.random_state=random_state\n        self.modelsName = list(classifiers.keys())\n        \n    def train(self,trainX,trainY,testX,testY,n_splits=5,mutiple=False):\n        \n        self.save_model=[[] for i in range(len(self.modelsName))]\n        self.n_splits = n_splits\n        kfold = KFold(n_splits=n_splits,shuffle=False)\n        \n        meta_train_feature= np.zeros((trainX.shape[0], len(self.modelsName)))\n        for model_idx in range(len(self.modelsName)):\n            print(self.modelsName[model_idx])\n            k=0\n            for train_index , test_index in kfold.split(trainX):\n                k+=1\n                classifier = clone(self.classifiers[model_idx])\n\n                train_feature = trainX.iloc[train_index]\n                test_feature = trainX.iloc[test_index]\n                train_label = trainY.iloc[train_index]\n                test_label = trainY.iloc[test_index]\n\n                classifier.fit(train_feature,train_label)\n                self.save_model[model_idx].append(classifier)\n                if \"predict_proba\" in dir(classifier):\n                    test_predict = classifier.predict_proba(test_feature)[:,1]\n                elif \"predict\" in dir(classifier):\n                    test_predict = classifier.predict(test_feature)\n                #test_predict = [[i] for i in test_predict]\n                #print(np.array(test_predict).shape,np.array(test_feature).shape)\n                \n                #meta_train_feature[test_index,model_idx] = np.concatenate((np.array(test_predict),np.array(test_feature)),axis=1)\n                meta_train_feature[test_index,model_idx] = test_predict\n\n\n                predicts=classifier.predict(testX)\n                accuracy = metrics.accuracy_score(predicts.round(), testY)\n                print(f\"KFlod-{k}:\",accuracy)\n                #print(metrics.precision_recall_fscore_support(labelY09, predicts,average='micro'))\n        \n        #self.meta_train_feature = meta_train_feature\n\n        self.meta_train_feature =  np.concatenate((np.array(meta_train_feature),np.array(trainX)),axis=1)\n        self.train_meta(trainY,testX,testY)\n        \n    def train_meta(self,trainY,testX,testY):\n        \n        for k,model in self.meta_model.items():\n            self.meta_model[k].fit(self.meta_train_feature,trainY)\n            predictions_stacking=self.predict_sigle(k,testX)\n            accuracy = metrics.accuracy_score(testY, predictions_stacking.round())\n            print(f\"Stack {k}:\")\n            print(accuracy)\n            print(metrics.precision_recall_fscore_support(testY, predictions_stacking.round(),average='macro'))\n            \n        estimators=[]\n        for k,v in self.meta_model.items():\n            estimators.append ((k,v))\n            \n        self.finalvotingmodel = VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1)\n        self.finalvotingmodel.fit(self.meta_train_feature,trainY)\n        #self.finalvotingmodel = StackingClassifier(estimators=estimators,cv=5,passthrough=True, final_estimator=lgb.LGBMClassifier(n_estimators= 40,learning_rate=0.1,subsample_for_bin=10000,random_state=random_state ), n_jobs=-1)\n        #self.finalvotingmodel.fit(self.meta_train_feature,trainY)\n        \n        predictions_stacking=self.predict(testX)\n        accuracy = metrics.accuracy_score(testY, predictions_stacking.round())\n        print(\"Stacked final voted model\")\n        print(accuracy)\n        print(metrics.precision_recall_fscore_support(testY, predictions_stacking.round(),average='macro'))\n        \n    def getMetaFeature(self,features):\n        meta_feature=[]\n        for model_idx in range(len(self.modelsName)):\n            \n            predicts=[]\n            \n            for flododx in range(0,self.n_splits):\n                if \"predict_proba\" in dir(self.save_model[model_idx][flododx]):\n                    predicts_tmp=self.save_model[model_idx][flododx].predict_proba(features)[:,1]\n                elif \"predict\" in dir(self.save_model[model_idx][flododx]):\n                    predicts_tmp=self.save_model[model_idx][flododx].predict(features)\n                    \n                predicts.append(predicts_tmp)\n                \n            predict = np.array(predicts).mean(axis=0)\n            predict = np.array([[i] for i in predict])\n            #predict = np.concatenate((np.array(predict),np.array(features)),axis=1)\n            if meta_feature==[]:\n                meta_feature= predict\n            else:\n                meta_feature = np.concatenate((meta_feature,predict),axis=1)\n        meta_feature = np.concatenate((meta_feature,features),axis=1)        \n        return meta_feature\n    def predict_sigle(self,modelname,features):\n        meta_feature = self.getMetaFeature(features)\n        predictions_stacking = self.meta_model[modelname].predict(meta_feature)\n        return predictions_stacking\n    def predict(self,features):\n        meta_feature = self.getMetaFeature(features)\n        predictions_stacking = self.finalvotingmodel.predict(meta_feature)\n        return predictions_stacking","f545ae6e":"print(featurecolumns)","94c6b960":"## model = RandomForestClassifier(n_estimators = 100, random_state = 31)\n#from sklearn.grid_search import GridSearchCV\n#modelElasti=MultiTaskElasticNet(alpha=1)\n#modelElasti.fit(featureX[featurecolumns_first],labelX14)\n#predictionsElasti = modelElasti.predict(featureY[featurecolumns_first])\n#for idx,p in enumerate(predictionsElasti):\n#    predictionsElasti[idx] = [ 1 if i>=0.5 else 0 for i in p]\n#print(np.array(labelY14).shape,np.array(predictionsElasti).shape)\n#printperformance(labelY14,predictionsElasti)\n#130 65\nprint(featurecolumns)\nfeaturecolumns_first=['d.log_time', 'd.temperature_A', 'd.temperature_B',\\\n                      'd.humidity_A', 'd.humidity_B', 'd.photometric', \\\n                      'd.soil_temperature', 'd.soil_humidity', 'd.soil_EC',\\\n                      'd.soil_PH', 'd.outside_photometric', 'd.outside_temperature',\\\n                      'd.outside_humidity',\\\n                      'd.log_date']\n\nfeaturecolumns_14=['d.log_time', 'd.temperature_A', 'd.temperature_B',\\\n                      'd.soil_temperature', 'd.soil_humidity',\\\n                      'd.outside_temperature',\\\n                      'd.log_date']\nmodelExtra14 = ExtraTreesClassifier(criterion='entropy',n_estimators=14,min_impurity_decrease=0.0,min_weight_fraction_leaf=0.0,min_samples_leaf=1,min_samples_split=3,n_jobs=-1,random_state=31,max_depth=None,max_features= 'auto')\nmodelRandomforest14 = RandomForestClassifier(criterion='entropy',n_estimators=6,min_impurity_decrease=0.0,min_weight_fraction_leaf=0.0,min_samples_leaf=1,min_samples_split=3,n_jobs=-1,random_state=31,max_depth=None,max_features= 'auto')\n#cv_results=cross_val_score(model, featureX, y = labelX, scoring = \"f1_score\", cv = 10, n_jobs=-1)\n\nmodelExtra14.fit(featureX[featurecolumns_14],labelX14)\npredictions = modelExtra14.predict_proba(featureY[featurecolumns_14])\npredictions = np.array(predictions)[:,:,1]\npredictionsExtra=np.moveaxis(predictions, -1, 0)\n#for idx,p in enumerate(predictionsExtra):\n#    predictionsExtra[idx] = [ 1 if i>=0.5 else 0 for i in p]\n#printperformance(labelY14,predictionsExtra)\n\nmodelRandomforest14.fit(featureX[featurecolumns_14],labelX14)\npredictions = modelRandomforest14.predict_proba(featureY[featurecolumns_14])\npredictions = np.array(predictions)[:,:,1]\npredictionsRandomforest=np.moveaxis(predictions, -1, 0)\n#for idx,p in enumerate(predictionsRandomforest):\n#    predictionsRandomforest[idx] = [ 1 if i>=0.5 else 0 for i in p]\n#printperformance(labelY14,predictionsRandomforest)\n\npredictionsmix14 = (predictionsRandomforest*0+predictionsExtra*1)\n#135 65 0.35 0.65\nfor idx,p in enumerate(predictionsmix14):\n    predictionsmix14[idx] = [ 1 if i>=0.5 else 0 for i in p]\nprintperformance(labelY14,predictionsmix14)","dc5e484d":"##### model = RandomForestClassifier(n_estimators = 100, random_state = 31)\n#from sklearn.grid_search import GridSearchCV\n#modelElasti=MultiTaskElasticNet(alpha=0)\n#modelElasti.fit(featureX,labelX)\n#predictionsElasti = modelElasti.predict(featureY)\n#for idx,p in enumerate(predictionsElasti):\n#    predictionsElasti[idx] = [ 1 if i>=0.5 else 0 for i in p]\n#print(np.array(labelY).shape,np.array(predictionsElasti).shape)\n#printperformance(labelY,predictionsElasti)\n#130 65\n\nfeaturecolumns_all=['d.log_time', 'd.temperature_A', 'd.temperature_B',\\\n                    'd.humidity_A', 'd.humidity_B', 'd.photometric', 'd.CO2',\\\n                    'd.soil_temperature', 'd.soil_humidity', 'd.soil_EC',\\\n                    'd.soil_PH', 'd.outside_photometric', 'd.outside_temperature',\\\n                    'd.outside_humidity', 'd.wind_speed', 'd.wind_direction',\\\n                    'd.radiometric', 'd.rainfall', 'd.log_date', 'd.stage',\\\n                    'd.diff_temperature', 'd.diff_humidity', 'd.odiff_photometric',\\\n                    'd.odiff_temperature', 'd.odiff_humidity']\nfeaturecolumns_57=['d.log_time',\\\n                      'd.soil_temperature', 'd.soil_humidity',\\\n                      'd.soil_PH',  'd.outside_temperature',\\\n                      'd.outside_humidity',\\\n                      'd.log_date']\nmodelExtra57 = ExtraTreesClassifier(criterion='entropy',n_estimators=14,min_impurity_decrease=0.0,min_weight_fraction_leaf=0.0,min_samples_leaf=1,min_samples_split=3,n_jobs=-1,random_state=31,max_depth=None,max_features= 'auto')\nmodelRandomforest57 = RandomForestClassifier(criterion='entropy',n_estimators=8,min_impurity_decrease=0.0,min_weight_fraction_leaf=0.0,min_samples_leaf=1,min_samples_split=3,n_jobs=-1,random_state=31,max_depth=None,max_features= 'auto')\nmodelcat57=HistGradientBoostingClassifier(l2_regularization=0.1,random_state=random_state)\n\n#modelcat57.fit(featureX[featurecolumns_57],labelX05)\n#predictions = modelcat57.predict_proba(featureY[featurecolumns_57])\n#predictions = np.array(predictions)[:,1]\n#predictionsExtra=np.moveaxis(predictions, -1, 0)\n#for idx,p in enumerate(predictionsExtra):\n#    predictionsExtra[idx] = 1 if predictionsExtra[idx]>=0.5 else 0\n#printperformance(labelY05,predictionsExtra)            \n\n#cv_results=cross_val_score(model, featureX, y = labelX, scoring = \"f1_score\", cv = 10, n_jobs=-1)\n\nmodelExtra57.fit(featureX[featurecolumns_57],labelX57)\npredictions = modelExtra57.predict_proba(featureY[featurecolumns_57])\npredictions = np.array(predictions)[:,:,1]\npredictionsExtra=np.moveaxis(predictions, -1, 0)\n#for idx,p in enumerate(predictionsExtra):\n#    predictionsExtra[idx] = [ 1 if i>=0.5 else 0 for i in p]\n#printperformance(labelY57,predictionsExtra)\n\n#modelRandomforest57.fit(featureX[featurecolumns_57],labelX57)\n#predictions = modelRandomforest57.predict_proba(featureY[featurecolumns_57])\n#predictions = np.array(predictions)[:,:,1]\n#predictionsRandomforest=np.moveaxis(predictions, -1, 0)\n#for idx,p in enumerate(predictionsRandomforest):\n#    predictionsRandomforest[idx] = [ 1 if i>=0.5 else 0 for i in p]\n#printperformance(labelY57,predictionsRandomforest)\n\npredictionsmix57 = (predictionsExtra*1)\n\nfor idx,p in enumerate(predictionsmix57):\n    predictionsmix57[idx] = [ 1 if i>=0.5 else 0 for i in p]\nprintperformance(labelY57,predictionsmix57)\nprint(predictionsmix57.shape)","067fa069":"##### model = RandomForestClassifier(n_estimators = 100, random_state = 31)\n#from sklearn.grid_search import GridSearchCV\n#modelElasti=MultiTaskElasticNet(alpha=0)\n#modelElasti.fit(featureX,labelX)\n#predictionsElasti = modelElasti.predict(featureY)\n#for idx,p in enumerate(predictionsElasti):\n#    predictionsElasti[idx] = [ 1 if i>=0.5 else 0 for i in p]\n#print(np.array(labelY).shape,np.array(predictionsElasti).shape)\n#printperformance(labelY,predictionsElasti)\n#130 65\n\nfeaturecolumns_all=['d.log_time', 'd.temperature_A', 'd.temperature_B',\\\n                    'd.humidity_A', 'd.humidity_B', 'd.photometric', 'd.CO2',\\\n                    'd.soil_temperature', 'd.soil_humidity', 'd.soil_EC',\\\n                    'd.soil_PH', 'd.outside_photometric', 'd.outside_temperature',\\\n                    'd.outside_humidity', 'd.wind_speed', 'd.wind_direction',\\\n                    'd.radiometric', 'd.rainfall', 'd.log_date', 'd.stage',\\\n                    'd.diff_temperature', 'd.diff_humidity', 'd.odiff_photometric',\\\n                    'd.odiff_temperature', 'd.odiff_humidity']\nfeaturecolumns_1011=['d.log_time', 'd.temperature_A', 'd.temperature_B',\\\n                    'd.humidity_A', 'd.humidity_B', 'd.photometric', 'd.CO2',\\\n                    'd.soil_temperature', 'd.soil_humidity', 'd.soil_EC',\\\n                    'd.soil_PH', 'd.outside_photometric', 'd.outside_temperature',\\\n                    'd.outside_humidity', 'd.wind_speed', 'd.wind_direction',\\\n                    'd.radiometric', 'd.rainfall', 'd.log_date', 'd.stage',\\\n                    'd.diff_temperature', 'd.diff_humidity', 'd.odiff_photometric',\\\n                    'd.odiff_temperature', 'd.odiff_humidity']\nmodelExtra1011 = ExtraTreesClassifier(criterion='entropy',n_estimators=23,min_impurity_decrease=0.0,min_weight_fraction_leaf=0.0,min_samples_leaf=1,min_samples_split=3,n_jobs=-1,random_state=31,max_depth=None,max_features= 'auto')\nmodelRandomforest1011 = RandomForestClassifier(criterion='entropy',n_estimators=20,min_impurity_decrease=0.0,min_weight_fraction_leaf=0.0,min_samples_leaf=1,min_samples_split=3,n_jobs=-1,random_state=31,max_depth=None,max_features= 'auto')\n#cv_results=cross_val_score(model, featureX, y = labelX, scoring = \"f1_score\", cv = 10, n_jobs=-1)\n\n#modelExtra1011.fit(featureX[featurecolumns_1011],labelX1011)\n#predictions = modelExtra1011.predict_proba(featureY[featurecolumns_1011])\n#predictions = np.array(predictions)[:,:,1]\n#predictionsExtra=np.moveaxis(predictions, -1, 0)\n#for idx,p in enumerate(predictionsExtra):\n#    predictionsExtra[idx] = [ 1 if i>=0.5 else 0 for i in p]\n#printperformance(labelY1011,predictionsExtra)\n\nmodelRandomforest1011.fit(featureX[featurecolumns_1011],labelX1011)\npredictions = modelRandomforest1011.predict_proba(featureY[featurecolumns_1011])\npredictions = np.array(predictions)[:,:,1]\npredictionsRandomforest=np.moveaxis(predictions, -1, 0)\n#for idx,p in enumerate(predictionsRandomforest):\n#    predictionsRandomforest[idx] = [ 1 if i>=0.5 else 0 for i in p]\n#printperformance(labelY1011,predictionsRandomforest)\n\npredictionsmix1011 = (predictionsRandomforest)\n\nfor idx,p in enumerate(predictionsmix1011):\n    predictionsmix1011[idx] = [ 1 if i>=0.5 else 0 for i in p]\nprintperformance(labelY1011,predictionsmix1011)","43316106":"#model=ExtraTreesClassifier(criterion='entropy',n_estimators=150,min_impurity_decrease=0.0,min_weight_fraction_leaf=0.0,min_samples_leaf=1,min_samples_split=3,n_jobs=-1,random_state=31,max_depth=None,max_features= 'auto')\n#gb_param_grid = {\n#              'n_estimators' : [90,100,110,120,130,140],\n#              }\n\n#grid = GridSearchCV(model,param_grid = gb_param_grid,cv=3, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\n#grid.fit(featureX[featurecolumns_first],labelX)\n\n#model = grid.best_estimator_\n\n#print( grid.best_params_, grid.best_score_)\n\n\n#predictions = model.predict(featureY[featurecolumns_first])\n#printperformance(labelY,predictions)\n\n\n#estimators=[]\n#for k,v in classifiers.items():\n#    estimators.append ((k,v))\n#Stackingmodels = StackingClassifier(estimators=estimators,cv=5,passthrough=True, final_estimator=lgb.LGBMClassifier(n_estimators= 40,learning_rate=0.1,subsample_for_bin=10000,random_state=random_state ), n_jobs=-1,verbose=True)\n#Stackingmodels.fit(featureX,labelX09)\n#predictions_stacking=Stackingmodels.predict(featureY)\n#accuracy = metrics.accuracy_score(labelY09, predictions_stacking)\n#print(\"Stacked final voted model\")\n#print(accuracy)\n#print(metrics.precision_recall_fscore_support(labelY09, predictions_stacking,average='micro'))\n","faa1639d":"def modelCV(featureX,labelX,kflod=5):\n    modelsName = [\"SVC\",\"DecisionTree\",\"AdaBoost\",\"Bagging\",\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"XGBClassifier\",\"LogisticRegression\",\"KNeighbors\",\"LinearDiscriminantAnalysis\"]\n\n    kfold = StratifiedKFold(n_splits=kflod)\n    random_state = 31\n    classifiers = []\n    classifiers.append(SVC(random_state=random_state))\n    classifiers.append(DecisionTreeClassifier(random_state=random_state))\n    classifiers.append(AdaBoostClassifier(ExtraTreesClassifier(random_state=random_state),random_state=random_state))\n    classifiers.append(BaggingClassifier(random_state=random_state))\n    classifiers.append(RandomForestClassifier(random_state=random_state))\n    classifiers.append(ExtraTreesClassifier(random_state=random_state))\n    classifiers.append(GradientBoostingClassifier(random_state=random_state))\n    classifiers.append(MLPClassifier(random_state=random_state))\n    classifiers.append(XGBClassifier(random_state=random_state))\n    classifiers.append(LogisticRegression(random_state = random_state))\n    classifiers.append(KNeighborsClassifier())\n    classifiers.append(LinearDiscriminantAnalysis())\n\n    cv_results = []\n    for classifier in classifiers :\n        cv_results.append(cross_val_score(classifier, np.array(featureX), y = np.array(labelX), scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\n    cv_means = []\n    cv_std = []\n    for cv_result in cv_results:\n        cv_means.append(cv_result.mean())\n        cv_std.append(cv_result.std())\n        print(modelsName[len(cv_means)-1])\n        print(cv_means[len(cv_means)-1])\n\n    cv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":modelsName})\n\n    g = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\n    g.set_xlabel(\"Mean Accuracy\")\n    g = g.set_title(\"Cross validation scores\")\n","18e260ce":"kflod=5\nrandom_state=31","3dfcc8a9":"classifiers05 = {#\"SGDClassifier\":SGDClassifier(loss=\"log\", penalty=\"l2\", random_state=random_state),\n               #\"IsolationForest\":IsolationForest(random_state=random_state),\n               #\"MultinomialNB\":MultinomialNB(),\n               \"GaussianNB\":GaussianNB(),\n               \"LightGBM\":lgb.LGBMClassifier(n_estimators= 20,learning_rate=0.11,subsample_for_bin=10000,random_state=random_state ),\n               \"HistGradientBoost\":HistGradientBoostingClassifier(l2_regularization=0,random_state=random_state),\n               \"Liner SVC\":LinearSVC(random_state=random_state),\n               \"CatBoosting\": CatBoostClassifier(random_seed=random_state, silent=True,loss_function=\"Logloss\",eval_metric=\"AUC\",l2_leaf_reg=1),        \n               #\"AdaBoost\":AdaBoostClassifier(n_estimators=20,random_state=random_state),\n               #\"Bagging\":BaggingClassifier(n_estimators=600, random_state=random_state),\n               \"RandomForest\":RandomForestClassifier(n_estimators=20,random_state=random_state,n_jobs=-1,min_samples_leaf=7),\n               \"ExtraTrees\":ExtraTreesClassifier(criterion='entropy',n_estimators=20,min_impurity_decrease=0.0,min_weight_fraction_leaf=0.0,min_samples_leaf=3,min_samples_split=3,n_jobs=-1,random_state=random_state,max_depth=None,max_features='auto'),\n               \"GradientBoosting\":GradientBoostingClassifier(n_estimators=20, random_state=random_state),\n               \"XGBClassifier\":XGBClassifier(n_estimators=20,random_state=random_state),\n               \"LinearDiscriminantAnalysis\":LinearDiscriminantAnalysis(),\n               \"KNeighbors\":KNeighborsClassifier(),\n               \"LogisticRegression\":LogisticRegression(random_state = random_state),\n               \"DecisionTree\":DecisionTreeClassifier(random_state=random_state),\n               #\"SVC\":SVC(random_state=random_state,probability=True),\n               #\"MLP\":MLPClassifier(random_state=random_state),\n               #\"LinearReg\":LinearRegression(), \n               #\"TheilSen\":TheilSenRegressor(random_state=random_state), \n               #\"RANSAC\":RANSACRegressor(random_state=random_state,residual_threshold=0.01),\n               #\"Huber\":HuberRegressor(),\n              }\nmeta_classifiers05 = {\n               \"HistGradientBoost\":HistGradientBoostingClassifier(l2_regularization=0,random_state=random_state),\n               \"CatBoosting\": CatBoostClassifier(random_seed=random_state, silent=True,\n                           loss_function=\"Logloss\",\n                           eval_metric=\"AUC\",\n                           learning_rate=0.1,\n                           l2_leaf_reg=1),\n               \"LightGBM\":lgb.LGBMClassifier(n_estimators= 60,learning_rate=0.1,subsample_for_bin=10000,random_state=random_state ),            \n               #\"RandomForest\":RandomForestClassifier(n_estimators=100,random_state=random_state,n_jobs=-1),\n               #\"ExtraTrees\":ExtraTreesClassifier(criterion='entropy',n_estimators=110,n_jobs=-1,random_state=random_state,min_samples_leaf=2),\n               #\"GradientBoosting\":GradientBoostingClassifier(n_estimators=110, random_state=random_state),\n               #\"XGBClassifier\":XGBClassifier(n_estimators=30,random_state=random_state,reg_lambda=10),}\n               #\"SVC\":SVC(random_state=random_state,probability=True),\n            }\n\nfeaturecolumns_05 =['d.log_time',\\\n                      'd.soil_temperature', 'd.soil_humidity',\\\n                      'd.soil_PH',  'd.outside_temperature',\\\n                      'd.outside_humidity',\\\n                      'd.log_date']\n\n#Stackingmodels05=Stacking_Model(classifiers05,meta_classifiers05,random_state)\n#Stackingmodels05.train(featureX[featurecolumns_05],labelX05,featureY[featurecolumns_05],labelY05,n_splits=kflod)\n#predictions_stack05 = Stackingmodels05.predict(featureY[featurecolumns_05])\n\n#predictionsmix57 = np.array([[i,i,i]for i in predictions_stack05])","ee27ec3d":"classifiers09 = {#\"SGDClassifier\":SGDClassifier(loss=\"log\", penalty=\"l2\", random_state=random_state),\n               #\"IsolationForest\":IsolationForest(random_state=random_state),\n               #\"MultinomialNB\":MultinomialNB(),\n               #\"GaussianNB\":GaussianNB(),\n               #\"LightGBM\":lgb.LGBMClassifier(n_estimators= 65,learning_rate=0.11,subsample_for_bin=10000,random_state=random_state ),\n               \"HistGradientBoost\":HistGradientBoostingClassifier(l2_regularization=0,random_state=random_state),\n               #\"Liner SVC\":LinearSVC(random_state=random_state),\n               \"CatBoosting\": CatBoostClassifier(random_seed=random_state, silent=True,loss_function=\"Logloss\",eval_metric=\"AUC\",l2_leaf_reg=10),        \n               #\"AdaBoost\":AdaBoostClassifier(n_estimators=700,random_state=random_state),\n               #\"Bagging\":BaggingClassifier(n_estimators=600, random_state=random_state),\n               #\"RandomForest\":RandomForestClassifier(n_estimators=450,random_state=random_state,n_jobs=-1,min_samples_leaf=7),\n               #\"ExtraTrees\":ExtraTreesClassifier(criterion='entropy',n_estimators=450,min_impurity_decrease=0.0,min_weight_fraction_leaf=0.0,min_samples_leaf=3,min_samples_split=3,n_jobs=-1,random_state=random_state,max_depth=None,max_features='auto'),\n               #\"GradientBoosting\":GradientBoostingClassifier(n_estimators=350, random_state=random_state),\n               #\"XGBClassifier\":XGBClassifier(n_estimators=30,random_state=random_state),\n               \"LinearDiscriminantAnalysis\":LinearDiscriminantAnalysis(),\n               \"KNeighbors\":KNeighborsClassifier(),\n               \"LogisticRegression\":LogisticRegression(random_state = random_state),\n               #\"DecisionTree\":DecisionTreeClassifier(random_state=random_state),\n               #\"SVC\":SVC(random_state=random_state,probability=True),\n               #\"MLP\":MLPClassifier(random_state=random_state),\n               #\"LinearReg\":LinearRegression(), \n               #\"TheilSen\":TheilSenRegressor(random_state=random_state), \n               #\"RANSAC\":RANSACRegressor(random_state=random_state,residual_threshold=0.01),\n               #\"Huber\":HuberRegressor(),\n              }\nmeta_classifiers09 = {\n               \"HistGradientBoost\":HistGradientBoostingClassifier(l2_regularization=0,random_state=random_state),\n               \"CatBoosting\": CatBoostClassifier(random_seed=random_state, silent=True,\n                           loss_function=\"Logloss\",\n                           eval_metric=\"AUC\",\n                           learning_rate=0.1,\n                           l2_leaf_reg=10),\n               \"LightGBM\":lgb.LGBMClassifier(n_estimators= 40,learning_rate=0.1,subsample_for_bin=10000,random_state=random_state ),            \n               #\"RandomForest\":RandomForestClassifier(n_estimators=100,random_state=random_state,n_jobs=-1),\n               #\"ExtraTrees\":ExtraTreesClassifier(criterion='entropy',n_estimators=110,n_jobs=-1,random_state=random_state,min_samples_leaf=2),\n               #\"GradientBoosting\":GradientBoostingClassifier(n_estimators=110, random_state=random_state),\n               #\"XGBClassifier\":XGBClassifier(n_estimators=30,random_state=random_state,reg_lambda=10),}\n               #\"SVC\":SVC(random_state=random_state,probability=True),\n            }\n\nfeaturecolumns_09 =['d.log_time', 'd.temperature_A', 'd.temperature_B',\\\n                    'd.humidity_A', 'd.humidity_B', 'd.photometric',\\\n                    'd.CO2',\\\n                    'd.wind_speed',  'd.radiometric',\\\n                    'd.log_date',  'd.diff_temperature',\\\n                    'd.diff_humidity',]\n\nStackingmodels09=Stacking_Model(classifiers09,meta_classifiers09,random_state)\nStackingmodels09.train(featureX[featurecolumns_09],labelX09,featureY[featurecolumns_09],labelY09,n_splits=kflod)\npredictions_stack09 = Stackingmodels09.predict(featureY[featurecolumns_09])","f408e2d8":"classifiers08 = {#\"SGDClassifier\":SGDClassifier(loss=\"log\", penalty=\"l2\", random_state=random_state),\n               #\"IsolationForest\":IsolationForest(random_state=random_state),\n               #\"MultinomialNB\":MultinomialNB(),\n               \"GaussianNB\":GaussianNB(),\n               \"LightGBM\":lgb.LGBMClassifier(n_estimators= 14,learning_rate=0.11,subsample_for_bin=10000,random_state=random_state ),\n               \"HistGradientBoost\":HistGradientBoostingClassifier(l2_regularization=0,random_state=random_state),\n               \"Liner SVC\":LinearSVC(random_state=random_state),\n               \"CatBoosting\": CatBoostClassifier(random_seed=random_state, silent=True,loss_function=\"Logloss\",eval_metric=\"AUC\",l2_leaf_reg=1),        \n               #\"AdaBoost\":AdaBoostClassifier(n_estimators=20,random_state=random_state),\n               #\"Bagging\":BaggingClassifier(n_estimators=600, random_state=random_state),\n               \"RandomForest\":RandomForestClassifier(n_estimators=14,random_state=random_state,n_jobs=-1,min_samples_leaf=7),\n               \"ExtraTrees\":ExtraTreesClassifier(criterion='entropy',n_estimators=14,min_impurity_decrease=0.0,min_weight_fraction_leaf=0.0,min_samples_leaf=3,min_samples_split=3,n_jobs=-1,random_state=random_state,max_depth=None,max_features='auto'),\n               \"GradientBoosting\":GradientBoostingClassifier(n_estimators=35, random_state=random_state),\n               \"XGBClassifier\":XGBClassifier(n_estimators=14,random_state=random_state),\n               \"LinearDiscriminantAnalysis\":LinearDiscriminantAnalysis(),\n               \"KNeighbors\":KNeighborsClassifier(),\n               \"LogisticRegression\":LogisticRegression(random_state = random_state),\n               \"DecisionTree\":DecisionTreeClassifier(random_state=random_state),\n               #\"SVC\":SVC(random_state=random_state,probability=True),\n               #\"MLP\":MLPClassifier(random_state=random_state),\n               #\"LinearReg\":LinearRegression(), \n               #\"TheilSen\":TheilSenRegressor(random_state=random_state), \n               #\"RANSAC\":RANSACRegressor(random_state=random_state,residual_threshold=0.01),\n               #\"Huber\":HuberRegressor(),\n              }\nmeta_classifiers08 = {\n               \"HistGradientBoost\":HistGradientBoostingClassifier(random_state=random_state),\n               \"CatBoosting\": CatBoostClassifier(random_seed=random_state, silent=True,\n                           loss_function=\"Logloss\",\n                           eval_metric=\"AUC\",\n                           learning_rate=0.1,\n                           l2_leaf_reg=10),\n               \"LightGBM\":lgb.LGBMClassifier(n_estimators= 40,learning_rate=0.1,subsample_for_bin=10000,random_state=random_state ),            \n               #\"RandomForest\":RandomForestClassifier(n_estimators=100,random_state=random_state,n_jobs=-1),\n               #\"ExtraTrees\":ExtraTreesClassifier(criterion='entropy',n_estimators=110,n_jobs=-1,random_state=random_state,min_samples_leaf=2),\n               #\"GradientBoosting\":GradientBoostingClassifier(n_estimators=110, random_state=random_state),\n               #\"XGBClassifier\":XGBClassifier(n_estimators=30,random_state=random_state,reg_lambda=10),\n               #\"SVC\":SVC(random_state=random_state,probability=True),\n            }\n\n\nfeaturecolumns_08 =['d.log_time', 'd.temperature_A', 'd.temperature_B',\\\n                    'd.humidity_A', 'd.humidity_B', 'd.photometric', 'd.CO2',\\\n                    'd.soil_temperature', 'd.soil_humidity', 'd.soil_EC',\\\n                    'd.soil_PH', 'd.outside_photometric', 'd.outside_temperature',\\\n                    'd.outside_humidity', 'd.wind_speed', 'd.wind_direction',\\\n                    'd.radiometric', 'd.rainfall', 'd.log_date', 'd.stage',\\\n                    'd.diff_temperature', 'd.diff_humidity', 'd.odiff_photometric',\\\n                    'd.odiff_temperature', 'd.odiff_humidity']\n\nStackingmodels08=Stacking_Model(classifiers08,meta_classifiers08,random_state)\nStackingmodels08.train(featureX[featurecolumns_08],labelX08,featureY[featurecolumns_08],labelY08,n_splits=kflod)\npredictions_stack08 = Stackingmodels08.predict(featureY[featurecolumns_08])","b1dbe5ea":"classifiers10 = {#\"SGDClassifier\":SGDClassifier(loss=\"log\", penalty=\"l2\", random_state=random_state),\n               #\"IsolationForest\":IsolationForest(random_state=random_state),\n               #\"MultinomialNB\":MultinomialNB(),\n               \"GaussianNB\":GaussianNB(),\n               #\"LightGBM\":lgb.LGBMClassifier(n_estimators= 65,learning_rate=0.11,subsample_for_bin=10000,random_state=random_state ),\n               \"HistGradientBoost\":HistGradientBoostingClassifier(random_state=random_state),\n               #\"Liner SVC\":LinearSVC(random_state=random_state),\n               #\"CatBoosting\": CatBoostClassifier(random_seed=random_state, silent=True,loss_function=\"Logloss\",eval_metric=\"AUC\",l2_leaf_reg=10),\n               #\"AdaBoost\":AdaBoostClassifier(n_estimators=700,random_state=random_state),\n               #\"Bagging\":BaggingClassifier(n_estimators=600, random_state=random_state),\n               #\"RandomForest\":RandomForestClassifier(n_estimators=450,random_state=random_state,n_jobs=-1,min_samples_leaf=7),\n               #\"ExtraTrees\":ExtraTreesClassifier(criterion='entropy',n_estimators=450,min_impurity_decrease=0.0,min_weight_fraction_leaf=0.0,min_samples_leaf=3,min_samples_split=3,n_jobs=-1,random_state=random_state,max_depth=None,max_features='auto'),\n               #\"GradientBoosting\":GradientBoostingClassifier(n_estimators=350, random_state=random_state),\n               #\"XGBClassifier\":XGBClassifier(n_estimators=30,random_state=random_state),\n               \"LinearDiscriminantAnalysis\":LinearDiscriminantAnalysis(),\n               \"KNeighbors\":KNeighborsClassifier(),\n               \"LogisticRegression\":LogisticRegression(random_state = random_state),\n               #\"DecisionTree\":DecisionTreeClassifier(random_state=random_state),\n               #\"SVC\":SVC(random_state=random_state,probability=True),\n               #\"MLP\":MLPClassifier(random_state=random_state),\n               #\"LinearReg\":LinearRegression(), \n               #\"TheilSen\":TheilSenRegressor(random_state=random_state), \n               #\"RANSAC\":RANSACRegressor(random_state=random_state),\n               #\"Huber\":HuberRegressor(),\n              }\nmeta_classifiers10 = {\n               #\"HistGradientBoost\":HistGradientBoostingClassifier(random_state=random_state),\n               \"CatBoosting\": CatBoostClassifier(random_seed=random_state, silent=True,\n                           loss_function=\"Logloss\",\n                           eval_metric=\"AUC\",\n                           learning_rate=0.1,\n                           l2_leaf_reg=10),\n               \"LightGBM\":lgb.LGBMClassifier(n_estimators= 40,learning_rate=0.1,subsample_for_bin=10000,random_state=random_state ),            \n               #\"RandomForest\":RandomForestClassifier(n_estimators=100,random_state=random_state,n_jobs=-1),\n               #\"ExtraTrees\":ExtraTreesClassifier(criterion='entropy',n_estimators=110,n_jobs=-1,random_state=random_state,min_samples_leaf=2),\n               #\"GradientBoosting\":GradientBoostingClassifier(n_estimators=110, random_state=random_state),\n               #\"XGBClassifier\":XGBClassifier(n_estimators=30,random_state=random_state,reg_lambda=10),}\n               #\"SVC\":SVC(random_state=random_state,probability=True),\n            }\nfeaturecolumns_10 =['d.log_time', 'd.temperature_A', 'd.temperature_B',\\\n                    'd.humidity_A', 'd.humidity_B', 'd.photometric',\\\n                    'd.CO2', 'd.soil_temperature', 'd.soil_humidity',\\\n                    'd.soil_EC', 'd.soil_PH', 'd.outside_photometric',\\\n                    'd.outside_temperature', 'd.outside_humidity',\\\n                    'd.wind_speed', 'd.wind_direction', 'd.radiometric',\\\n                    'd.rainfall', 'd.log_date']\n#Stackingmodels10=Stacking_Model(classifiers10,meta_classifiers10,random_state)\n# Stackingmodels10.train(featureX[featurecolumns_10],labelX10,featureY[featurecolumns_10],labelY10,n_splits=kflod)\n#predictions_stack10 = Stackingmodels10.predict(featureY[featurecolumns_10])","09b9b22a":"classifiers11 = {#\"SGDClassifier\":SGDClassifier(loss=\"log\", penalty=\"l2\", random_state=random_state),\n               #\"IsolationForest\":IsolationForest(random_state=random_state),\n               #\"MultinomialNB\":MultinomialNB(),\n               #\"GaussianNB\":GaussianNB(),\n               \"LightGBM\":lgb.LGBMClassifier(n_estimators= 65,learning_rate=0.11,subsample_for_bin=10000,random_state=random_state ),\n               \"HistGradientBoost\":HistGradientBoostingClassifier(random_state=random_state),\n               #\"Liner SVC\":LinearSVC(random_state=random_state),\n               \"CatBoosting\": CatBoostClassifier(random_seed=random_state, silent=True,loss_function=\"Logloss\",eval_metric=\"AUC\",l2_leaf_reg=10),\n               #\"AdaBoost\":AdaBoostClassifier(n_estimators=700,random_state=random_state),\n               #\"Bagging\":BaggingClassifier(n_estimators=600, random_state=random_state),\n               #\"RandomForest\":RandomForestClassifier(n_estimators=450,random_state=random_state,n_jobs=-1,min_samples_leaf=7),\n               #\"ExtraTrees\":ExtraTreesClassifier(criterion='entropy',n_estimators=450,min_impurity_decrease=0.0,min_weight_fraction_leaf=0.0,min_samples_leaf=3,min_samples_split=3,n_jobs=-1,random_state=random_state,max_depth=None,max_features='auto'),\n               #\"GradientBoosting\":GradientBoostingClassifier(n_estimators=350, random_state=random_state),\n               #\"XGBClassifier\":XGBClassifier(n_estimators=30,random_state=random_state),\n               \"LinearDiscriminantAnalysis\":LinearDiscriminantAnalysis(),\n               #\"KNeighbors\":KNeighborsClassifier(),\n               \"LogisticRegression\":LogisticRegression(random_state = random_state),\n               #\"DecisionTree\":DecisionTreeClassifier(random_state=random_state),\n               #\"SVC\":SVC(random_state=random_state,probability=True),\n               #\"MLP\":MLPClassifier(random_state=random_state),\n               #\"LinearReg\":LinearRegression(), \n               #\"TheilSen\":TheilSenRegressor(random_state=random_state), \n               #\"RANSAC\":RANSACRegressor(random_state=random_state),\n               #\"Huber\":HuberRegressor(),\n              }\nmeta_classifiers11 = {\n               #\"HistGradientBoost\":HistGradientBoostingClassifier(random_state=random_state),\n               \"CatBoosting\": CatBoostClassifier(random_seed=random_state, silent=True,\n                           loss_function=\"Logloss\",\n                           eval_metric=\"AUC\",\n                           learning_rate=0.1,\n                           l2_leaf_reg=10),\n               \"LightGBM\":lgb.LGBMClassifier(n_estimators= 40,learning_rate=0.1,subsample_for_bin=10000,random_state=random_state ),            \n               #\"RandomForest\":RandomForestClassifier(n_estimators=100,random_state=random_state,n_jobs=-1),\n               #\"ExtraTrees\":ExtraTreesClassifier(criterion='entropy',n_estimators=110,n_jobs=-1,random_state=random_state,min_samples_leaf=2),\n               #\"GradientBoosting\":GradientBoostingClassifier(n_estimators=110, random_state=random_state),\n               #\"XGBClassifier\":XGBClassifier(n_estimators=30,random_state=random_state,reg_lambda=10),}\n               #\"SVC\":SVC(random_state=random_state,probability=True),\n            }\n\nfeaturecolumns_11 =['d.log_time', 'd.temperature_A', 'd.temperature_B',\\\n                    'd.humidity_A', 'd.humidity_B', 'd.photometric',\\\n                    'd.CO2', 'd.soil_temperature', 'd.soil_humidity',\\\n                    'd.soil_EC', 'd.soil_PH', 'd.outside_photometric',\\\n                    'd.outside_temperature', 'd.outside_humidity',\\\n                    'd.wind_speed', 'd.wind_direction', 'd.radiometric',\\\n                    'd.rainfall', 'd.log_date', 'd.stage']\n\n#Stackingmodels11=Stacking_Model(classifiers11,meta_classifiers11,random_state)\n#Stackingmodels11.train(featureX[featurecolumns_11],labelX11,featureY[featurecolumns_11],labelY11,n_splits=kflod)\n#predictions_stack11 = Stackingmodels11.predict(featureY[featurecolumns_11])","8c906361":"\n#Stackingmodels.meta_model=meta_classifiers\n\n#Stackingmodels.train_meta(labelX09,featureY,labelY09)","09e90576":"class Vote_Model(object):\n    def __init__(self,classifiers,random_state):\n        self.classifiers = classifiers\n        self.random_state=random_state\n    def train(self,trainX,trainY,testX,testY):\n        for k,classifier in self.classifiers.items():\n            classifier.fit(np.array(trainX),np.array(trainY))\n            predictions = classifier.predict(np.array(testX))\n            accuracy = metrics.accuracy_score(testY, predictions)\n            print(k)\n            print(accuracy)\n            print(metrics.precision_recall_fscore_support(testY, predictions,average='micro'))\n\n        estimators=[]\n        for k,v in self.classifiers.items():\n            estimators.append ((k,v))\n            \n        self.votingmodel = VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1)\n        self.votingmodel.fit(trainX,trainY)\n        \n        predictions = self.predict(testX)\n        accuracy = metrics.accuracy_score(testY, predictions)\n        print(\"Voteed model\")\n        print(accuracy)\n        print(metrics.precision_recall_fscore_support(testY, predictions,average='micro'))\n    def predict(self,features):\n        predictions = self.votingmodel.predict(np.array(features))\n        return predictions\n\n#votemodels = {\"AdaBoost\":AdaBoostClassifier(n_estimators=350,random_state=random_state),\n              #\"Bagging\":BaggingClassifier(n_estimators=350, random_state=random_state),\n              #\"RandomForest\":RandomForestClassifier(n_estimators=350,random_state=random_state,n_jobs=-1),\n              #\"ExtraTrees\":ExtraTreesClassifier(criterion='entropy',n_estimators=350,n_jobs=-1,max_depth=None,random_state=random_state),\n#              \"GradientBoosting\":GradientBoostingClassifier(n_estimators=350, random_state=random_state),\n#              \"XGBClassifier\":XGBClassifier(n_estimators=30,random_state=random_state),}\n              #\"LinearDiscriminantAnalysis\":LinearDiscriminantAnalysis()}\n#votemodel = Vote_Model(votemodels,random_state)\n#votemodel.train(featureX,labelX09,featureY,labelY09)","535cff3f":"#predictions_vote = votemodel.predict(featureY)\n#accuracy = metrics.accuracy_score(labelY09, predictions_vote)\n#print(\"vote model\")\n#print(accuracy)\n#print(metrics.precision_recall_fscore_support(labelY09, predictions_vote,average='micro'))\n\n#print(\"stack model\")\n#predictions_stack = Stackingmodels.predict(featureY)\n#accuracy = metrics.accuracy_score(labelY09, predictions_stack)\n#print(accuracy)\n#print(metrics.precision_recall_fscore_support(labelY09, predictions_stack,average='micro'))\n\n","d306b815":"labelY[labelcolumns14] = labelY14\nlabelY[labelcolumns57] = labelY57\nlabelY[labelcolumns1011] = labelY1011\n\nlabelY=labelY.drop(columns=single2_columns)\nlabelY[single2_columns] = labelY08\nlabelY=labelY.drop(columns=single_columns)\nlabelY[single_columns] = labelY09\n\npredictions09 = np.array([[i] for i in predictions_stack09])\npredictions08 = np.array([[i] for i in predictions_stack08])\n#predictionsmix811 = np.array([ i[1:] for i in predictionsmix811])\n\npredictions=np.concatenate ((predictionsmix14,predictionsmix57),axis=1)\npredictions=np.concatenate ((predictions,predictionsmix1011),axis=1)\npredictions=np.concatenate ((predictions,predictions08),axis=1)\npredictions=np.concatenate ((predictions,predictions09),axis=1)\nprint(labelY.columns)\nprint(predictionsmix1011.shape)\n#predictions=np.concatenate ((predictionsmix,predictions09),axis=1)\n\n\n\n\n#predictions10 = np.array([[i] for i in predictions_stack10])\n#predictions=np.concatenate ((predictions,predictions10),axis=1)\n#labelY[\"actuator10\"] = labelY10\n\n#predictions11 = np.array([[i] for i in predictions_stack11])\n#predictions=np.concatenate ((predictions,predictions11),axis=1)\n#labelY[\"actuator11\"] = labelY11","f8b53898":"def calcaulateMacroF1(allpred,allans,allpredacc,nclasses,rou=3):\n    recalls = [0 if allans[i] == 0 else 100*allpredacc[i]\/allans[i] for  i in range(0,nclasses)]\n    precisions = [0 if allpred[i] == 0 else 100*allpredacc[i]\/allpred[i] for  i in range(0,nclasses)]\n    avg_recalls = float(sum(recalls) \/ nclasses)\n    avg_precisions = float(sum(precisions) \/ nclasses)\n    beta=0.000001\n    macro_f1 =(2+beta)*(avg_recalls*avg_precisions)\/((avg_recalls+avg_precisions)+beta)\n    macro_f1 = round(macro_f1,rou)\n    precisions = [round(p,rou) for p in precisions]\n    recalls = [round(r,rou) for r in recalls]\n    return macro_f1,recalls ,precisions","d0b04032":"print(predictions.shape,labelY.shape)","f58f76ac":"if single_columns not in labelcolumns:\n    labelcolumns.append(single_columns)\n    #labelcolumns.append(\"autuator10\")\n    #labelcolumns.append(\"autuator11\")\ntotalacc=[0]*len(labelcolumns)\ntotalans=[0]*len(labelcolumns)\ntotalpred=[0]*len(labelcolumns)\nacc=0\n#predictions = [[p] for p in predictions]\nfor idx in range(len(predictions)):\n    allacc=True\n    pred = [int(p>0.5) for p in predictions[idx]]\n    if list(labelY.iloc[idx]) == list(pred):\n        acc+=1\n    for lidx in range(len(labelcolumns)):\n        if labelcolumns[lidx] in input_df.columns[-2:] and False:\n            totalans[lidx]+=not int(labelY.iloc[idx][lidx])\n            predvalue = pred[lidx]\n            totalpred[lidx]+= not predvalue\n            if labelY.iloc[idx][lidx] == predvalue:\n                totalacc[lidx]+= not predvalue\n        else:\n            totalans[lidx]+=int(labelY.iloc[idx][lidx])\n            predvalue = pred[lidx]\n            totalpred[lidx]+= predvalue\n            if labelY.iloc[idx][lidx] == predvalue:\n                totalacc[lidx]+= predvalue\n                \nprint(acc\/len(labelY))\nmacro_f1,recalls ,precisions = calcaulateMacroF1(totalpred,totalans,totalacc,len(labelcolumns))\nprint('f1-score: {} acc:{}'.format(\"%.3f\"%macro_f1,\"%.2f\"%(acc\/len(labelY))))\nprint('\\nrecall: {} \\nprecis: {}'.format(recalls,precisions))\nprint(sum(precisions)\/len(precisions),sum(recalls)\/len(recalls))\nprint(metrics.accuracy_score(labelY, predictions))\nprint(metrics.precision_recall_fscore_support(labelY, predictions,average='micro'))","cba8ea50":"test_df = read_csvfile(os.path.join(BASEPATH,\"test_data.csv\"))\ntest_df=test_df.drop(\"d.rainfall_detect\",axis=1)\nprint(featurecolumns)\ntest_df.info()","b8db93d9":"print(test_df[\"d.log_time\"][0])\nsubmit = read_csvfile(os.path.join(BASEPATH,\"submission.csv\"))\ntest_df[\"d.log_date\"] = [int(d.split(' ')[0].split('-')[1]+d.split(' ')[0].split('-')[2]) for d in test_df[\"d.log_time\"]]\nprint(test_df[\"d.log_date\"][0])\ntest_df[\"d.log_time\"] = [int(d.split(' ')[1].split(':')[0]+d.split(' ')[1].split(':')[1]) for d in test_df[\"d.log_time\"]]\nprint(test_df[\"d.log_time\"][0])\ntest_df = addfeature(test_df)\n#test_df = logData(test_df,\"d.wind_direction\")\n#test_df = AvgSameData(input_df)\n\nfor idx,columnsname in enumerate(columnsnames):\n    test_df = replacezerovalues(test_df,columnsname,idx,False)\n    #test_df = logData(test_df,columnsname)\n\n#normalized_feature = dataNormalized(test_df,True)\n\n#test_df = normalized_feature\n\n\ndisplay(test_df.describe())\ndisplay(train_df[featurecolumns].describe())\n\n\n","7b53f19a":"#14\npredictions = modelExtra14.predict_proba(test_df[featurecolumns_14])\npredictions = np.array(predictions)[:,:,1]\npredictionsExtra=np.moveaxis(predictions, -1, 0)\n\n\npredictions = modelRandomforest14.predict_proba(test_df[featurecolumns_14])\npredictions = np.array(predictions)[:,:,1]\npredictionsRandomforest=np.moveaxis(predictions, -1, 0)\n\npredictionsmix14 = (predictionsExtra)\n\n#57\n#predictionsmix57 = Stackingmodels05.predict(test_df[featurecolumns_05])\n#predictionsmix57 = np.array([[i,i,i] for i in predictionsmix57])\npredictions = modelExtra57.predict_proba(test_df[featurecolumns_57])\npredictions = np.array(predictions)[:,:,1]\npredictionsExtra=np.moveaxis(predictions, -1, 0)\nfor idx,p in enumerate(predictionsExtra):\n    predictionsExtra[idx] = [ 1 if i>=0.5 else 0 for i in p]\n\n#predictions = modelRandomforest57.predict_proba(test_df[featurecolumns_57])\n#predictions = np.array(predictions)[:,:,1]\n#predictionsRandomforest=np.moveaxis(predictions, -1, 0)\n#for idx,p in enumerate(predictionsRandomforest):\n#    predictionsRandomforest[idx] = [ 1 if i>=0.5 else 0 for i in p]\n    \npredictionsmix57 = (predictionsExtra)\n\n#1011\n#predictions = modelExtra1011.predict_proba(test_df[featurecolumns_1011])\n#predictions = np.array(predictions)[:,:,1]\n#predictionsExtra=np.moveaxis(predictions, -1, 0)\n\n\npredictions = modelRandomforest1011.predict_proba(test_df[featurecolumns_1011])\npredictions = np.array(predictions)[:,:,1]\npredictionsRandomforest=np.moveaxis(predictions, -1, 0)\n\npredictionsmix1011 = (predictionsRandomforest)\n#predictionsmix1011 = np.array([ i[1:] for i in predictionsmix1011])\n\nfor idx,p in enumerate(predictionsmix14):\n    predictionsmix14[idx] = [ 1 if i>=0.5 else 0 for i in p]\n\nfor idx,p in enumerate(predictionsmix57):\n    predictionsmix57[idx] = [ 1 if i>=0.5 else 0 for i in p]\n    \nfor idx,p in enumerate(predictionsmix1011):\n    predictionsmix1011[idx] = [ 1 if i>=0.5 else 0 for i in p]\n\npredictions_stack08 = Stackingmodels08.predict(test_df[featurecolumns_08])\npredictions08 = np.array([[i] for i in predictions_stack08])\n\npredictions_stack09 = Stackingmodels09.predict(test_df[featurecolumns_09])\npredictions09 = np.array([[i] for i in predictions_stack09])\n\n#predictions_stack10 = Stackingmodels10.predict(test_df[featurecolumns_10])\n#predictions10 = np.array([[i] for i in predictions_stack10])\n\n#predictions_stack11 = Stackingmodels11.predict(test_df[featurecolumns_11])\n#predictions11 = np.array([[i] for i in predictions_stack11])\n\n#predictions09 = np.array([[i] for i in predictions_stack09])\npredictions=np.concatenate ((predictionsmix14,predictionsmix57),axis=1)\npredictions=np.concatenate ((predictions,predictionsmix1011),axis=1)\npredictions=np.concatenate ((predictions,predictions08),axis=1)\npredictions=np.concatenate ((predictions,predictions09),axis=1)\n\n\n#predictions=np.concatenate ((predictionsmix,predictions09),axis=1)\n#predictions=np.concatenate ((predictions,predictions10),axis=1)\n#predictions=np.concatenate ((predictions,predictions11),axis=1)","2825a7ec":"labelcolumns=['actuator01', 'actuator02', 'actuator03', 'actuator04', 'actuator05',\n       'actuator06', 'actuator07', 'actuator10', 'actuator11','actuator08',\n       'actuator09']\nans_df = pd.DataFrame(predictions,columns=labelcolumns)\nans_df.to_csv(\"submission.csv\")","747d8380":"## Normalization Data","567daccd":"### Firstmodel\n","ecb18a3c":"## Balance of label between training and validation data","cd0aba1f":"0.9370299809485612\n(0.9919428271846803, 0.9976549599678077, 0.9947906938131706, None)\n\n0.9928807781008724\n(0.9995684494807473, 0.9982760298636117, 0.9989218216345185, None)\n\n\n0.992981048831846\n(0.9995823704652393, 0.9982760538316093, 0.9989287850753329, None)\n\n0.9994986463451319\n(0.9998982084690554, 0.9997455600223907, 0.9998218784192981, None)","3c12172e":"## Read Data","cc1741a4":"Stacked final voted model\n0.9703198636318059\n(0.9703198636318059, 0.9703198636318059, 0.9703198636318059, None)\n\nStacked final voted model\n0.9719241953273839\n(0.9719241953273839, 0.9719241953273839, 0.9719241953273839, None)\n\nStacked final voted model\n0.9725258197132257\n(0.9725258197132257, 0.9725258197132257, 0.9725258197132257, None)","d509a021":"## Set Seed","04373b59":"Stacked final voted model\n0.8546074400882382\n(0.8546074400882382, 0.8546074400882382, 0.8546074400882382, None)\n\nStacked final voted model\n0.8537050035094755\n(0.8537050035094755, 0.8537050035094755, 0.8537050035094755, None)\n\n\nStacked final voted model\n0.8595207059059461\n(0.8595207059059461, 0.8595207059059461, 0.8595207059059461, None)\n\nStacked final voted model\n0.8635315351448912\n(0.8663337360850437, 0.818271148516325, 0.8356124697189776, None)","e4e9ee48":"## Create Model","0f8e15b0":"0.8077810087235536\nf1-score: 89.838 acc:0.81\n\nrecall: [99.99, 100.0, 100.0, 99.99, 99.897, 99.897, 99.897, 99.891, 9.296, 24.9, 94.95] \nprecis: [99.877, 99.97, 99.97, 99.918, 99.588, 99.599, 99.611, 99.628, 78.723, 93.939, 85.051]\n95.98854545454546 84.42800000000001\n0.8077810087235536\n(0.9838318309257094, 0.9883549100094727, 0.9860881837771085, None)\n\n0.8156021257394966\nf1-score: 91.134 acc:0.82\n\nrecall: [100.0, 100.0, 100.0, 100.0, 99.885, 99.885, 99.885, 99.907, 95.024, 38.791, 35.193] \nprecis: [99.908, 99.97, 99.97, 99.939, 99.587, 99.599, 99.611, 99.613, 85.059, 74.396, 81.188]\n94.44 88.05181818181818\n0.8156021257394966\n(0.9831939096735354, 0.9903608065086664, 0.9867643449200384, None)\n\n\n0.8140980647748922\nf1-score: 91.237 acc:0.81\n\nrecall: [100.0, 100.0, 100.0, 100.0, 99.885, 99.885, 99.885, 99.876, 94.591, 42.821, 33.906] \nprecis: [99.898, 99.97, 99.97, 99.918, 99.61, 99.622, 99.622, 99.582, 85.23, 76.233, 79.0]\n94.42318181818182 88.259\n0.8140980647748922\n(0.9834121736075385, 0.9901334074485826, 0.986761345394799, None)\n\n0.8221197232527825\nf1-score: 91.501 acc:0.82\n\nrecall: [99.98, 100.0, 100.0, 99.98, 99.931, 99.931, 99.931, 99.876, 95.158, 42.065, 33.047] \nprecis: [99.939, 99.97, 99.97, 99.949, 99.668, 99.668, 99.679, 99.674, 85.534, 79.147, 82.796]\n95.09036363636363 88.17263636363637\n0.8221197232527825\n(0.984173600913689, 0.9906513719743292, 0.9874018623333942, None)\n\n0.8186102476687055\nf1-score: 89.746 acc:0.82\n\nrecall: [99.98, 100.0, 100.0, 99.98, 99.92, 99.92, 99.92, 99.845, 8.816, 22.318, 95.068] \nprecis: [99.969, 99.98, 99.98, 99.969, 99.77, 99.782, 99.782, 99.705, 76.087, 96.296, 86.052]\n96.12472727272728 84.16063636363637\n0.8186102476687055\n(0.9857545910270298, 0.9885311540841974, 0.9871409201273925, None)","a301e564":"## Feature Engineerning","0c91f632":"\n0.9962899829539758<br>\n(0.9990810934987365, 0.9967531227319607, 0.9979157504254547, None)<br>\n0.9954878171061867<br>\n(0.9994256834367103, 0.99557572752584, 0.9974969906567055, None)<br>\n0.9962899829539758<br>\n(0.9990810934987365, 0.9967531227319607, 0.9979157504254547, None)\n\n\n0.997292690263712\n(0.9991959568113944, 0.9977824507742306, 0.9984887035371989, None)","e703b959":"## Remove Outlier","13422e6b":"0.9425448711521107\n(0.9217378998018682, 0.9951107715813599, 0.9570200573065902, None)","70630a78":"### Repredict wind speed data","d82a66e7":"## Import Lib","b926d085":"## Model Search","94c19495":"0.9978","399baf8d":"## Submit","c3d58304":"Stacked final voted model\n0.9829539757344831\n(0.9829539757344831, 0.9829539757344831, 0.9829539757344831, None)\n\nStacked final voted model\n0.9824526220796149\n(0.9824526220796149, 0.9824526220796149, 0.9824526220796149, None)\n\nStacked final voted model\n0.9827534342725358\n(0.9827534342725358, 0.9827534342725358, 0.9827534342725358, None)"}}