{"cell_type":{"92921842":"code","91e8323e":"code","9cabe7d0":"code","ce68ebf7":"code","9aed196e":"code","7b9aaa16":"code","155dbf55":"code","ebeba48f":"code","69fcb1a5":"code","b76661d2":"code","e2096753":"code","6f4b9d1b":"code","f563c040":"code","a35148dc":"code","bcd9b5a5":"code","a367db36":"code","22d3adad":"code","404b356d":"code","068d7f2e":"code","b6606c99":"code","682b1806":"code","b0780d16":"code","e6361352":"code","cf4ff846":"code","75d2728b":"code","af4e7972":"code","e731fb83":"code","8fc2e37b":"code","016b415a":"code","870cf008":"code","d2cd487f":"code","b1ad5a06":"code","e3f0c4ad":"code","717d1537":"markdown","bf916246":"markdown","9cd269fc":"markdown","12a93fba":"markdown","7a0a39df":"markdown","fb58bab0":"markdown","00cd7540":"markdown","543d5e1d":"markdown","b599c2a9":"markdown","8054f89a":"markdown","fbcbe315":"markdown","caf5cbd0":"markdown","9a86b271":"markdown","b778817a":"markdown","139e8104":"markdown","91647fe3":"markdown","7bd28495":"markdown","7bfd98ef":"markdown","a1c5d715":"markdown","7ff8c9c5":"markdown","44a5e555":"markdown","d5ebe857":"markdown","2bcc6e91":"markdown"},"source":{"92921842":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","91e8323e":"#Importing all the necessary libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n%matplotlib inline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter\nfrom IPython.core.display import display, HTML\nsns.set_style('darkgrid')","9cabe7d0":"#loading dataset\nrwine = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\n\n#Shape\nprint(rwine.shape)\nprint(\"----------------------------\")\nprint(rwine.head(5))\nprint(\"----------------------------\")\nprint(rwine.info())\nprint(\"----------------------------\")\nprint(rwine.describe())","ce68ebf7":"rwine[\"quality\"].unique","9aed196e":"#Here we see that fixed acidity does not give any specification to classify the quality.\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'fixed acidity', data = rwine)\n","7b9aaa16":"\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'volatile acidity', data = rwine)\nplt.show()","155dbf55":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'citric acid', data = rwine)","ebeba48f":"fig = plt.figure(figsize = (20,20))\nsns.lineplot(x = 'alcohol', y = 'citric acid', data = rwine)","69fcb1a5":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'residual sugar', data = rwine)","b76661d2":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'chlorides', data = rwine)","e2096753":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'total sulfur dioxide', data = rwine)","6f4b9d1b":"sns.set_style(\"whitegrid\");\nsns.pairplot(rwine, hue=\"quality\",height=3);\nplt.show()","f563c040":"rwine['quality'].value_counts()","a35148dc":"bins = (2, 6, 8)\nlabels = ['bad', 'good']\nrwine['quality'] = pd.cut(x = rwine['quality'], bins = bins, labels = labels)","bcd9b5a5":"rwine[\"quality\"].value_counts()","a367db36":"from sklearn.preprocessing import LabelEncoder\nlabelencoder_y = LabelEncoder()\nrwine['quality'] = labelencoder_y.fit_transform(rwine['quality'])\nrwine['quality'] ","22d3adad":"X = rwine.drop('quality', axis = 1).values\ny = rwine['quality'].values.reshape(-1,1)","404b356d":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 42)","068d7f2e":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_scaled = sc.fit_transform(X_train)\nX_test_scaled = sc.transform(X_test)","b6606c99":"X_train_scaled","682b1806":"# Fitting Logistic Regression to the Training set\n#Taken the maximum iteration possible, assigned default penality,alowed constant adding\nfrom sklearn.linear_model import LogisticRegression\nclassifier_lr = LogisticRegression(C=1, fit_intercept=True, max_iter=1000, penalty = 'l2', solver='liblinear')\n#The numpy.ravel() functions returns contiguous flattened array.\nclassifier_lr.fit(X_train_scaled, y_train.ravel())","b0780d16":"# Predicting Cross Validation Score\n#taking 10 fold cross-validation\ncv_lr = cross_val_score(estimator = classifier_lr, X = X_train_scaled, y = y_train.ravel(), cv = 10)\nprint(\"CV: \", cv_lr.mean())\n\ny_pred_lr_train = classifier_lr.predict(X_train_scaled)\naccuracy_lr_train = accuracy_score(y_train, y_pred_lr_train)\nprint(\"Training set: \", accuracy_lr_train)\n\ny_pred_lr_test = classifier_lr.predict(X_test_scaled)\naccuracy_lr_test = accuracy_score(y_test, y_pred_lr_test)\nprint(\"Test set: \", accuracy_lr_test)","e6361352":"# Fitting classifier to the Training set\n#gini coefficient\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(criterion = 'gini', max_features=6, max_leaf_nodes=400, random_state = 33)\ndt.fit(X_train_scaled, y_train.ravel())","cf4ff846":"# Predicting Cross Validation Score\ncv_dt = cross_val_score(estimator = dt, X = X_train_scaled, y = y_train.ravel(), cv = 10)\nprint(\"CV: \", cv_dt.mean())\n\ny_pred_train = dt.predict(X_train_scaled)\naccuracy_dt_train = accuracy_score(y_train, y_pred_train)\nprint(\"Training set: \", accuracy_dt_train)\n\ny_pred_dt_test = dt.predict(X_test_scaled)\naccuracy_dt_test = accuracy_score(y_test, y_pred_dt_test)\nprint(\"Test set: \", accuracy_dt_test)","75d2728b":"# Import the model we are using\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(criterion = 'entropy', max_features = 4, n_estimators = 800, random_state=33)\nrf.fit(X_train_scaled, y_train.ravel())","af4e7972":" #Use the forest's predict method on the test data\npredictions = rf.predict(X_test_scaled)\n# Calculate the absolute errors\nerrors = abs(predictions - y_test)\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')","e731fb83":"# Predicting Cross Validation Score\ncv_rf = cross_val_score(estimator = rf, X = X_train_scaled, y = y_train.ravel(), cv = 10)\nprint(\"CV: \", cv_rf.mean())\n\ny_pred_rf_train = rf.predict(X_train_scaled)\naccuracy_rf_train = accuracy_score(y_train, y_pred_rf_train)\nprint(\"Training set: \", accuracy_rf_train)\n\ny_pred_rf_test = rf.predict(X_test_scaled)\naccuracy_rf_test = accuracy_score(y_test, y_pred_rf_test)\nprint(\"Test set: \", accuracy_rf_test)","8fc2e37b":"from sklearn.svm import SVC","016b415a":"svc = SVC(kernel = 'linear')\nsvc.fit(X_train_scaled, y_train.ravel())\npred_svc = svc.predict(X_test_scaled)\nprint(classification_report(y_test, pred_svc))","870cf008":"# Predicting Cross Validation Score\ncv_svm_linear = cross_val_score(estimator = svc, X = X_train_scaled, y = y_train.ravel(), cv = 10)\nprint(\"CV: \", cv_svm_linear.mean())\n\ny_pred_svm_linear_train = svc.predict(X_train_scaled)\naccuracy_svm_linear_train = accuracy_score(y_train, y_pred_svm_linear_train)\nprint(\"Training set: \", accuracy_svm_linear_train)\n\ny_pred_svm_linear_test = svc.predict(X_test_scaled)\naccuracy_svm_linear_test = accuracy_score(y_test, y_pred_svm_linear_test)\nprint(\"Test set: \", accuracy_svm_linear_test)","d2cd487f":"# Fitting classifier to the Training set\nsvm_kernel = SVC(kernel = 'rbf', C = 10, tol = 0.001, gamma = 'scale')\nsvm_kernel.fit(X_train_scaled, y_train.ravel())\n","b1ad5a06":"# Predicting Cross Validation Score\ncv_svm_kernel = cross_val_score(estimator = svm_kernel, X = X_train_scaled, y = y_train.ravel(), cv = 10)\nprint(\"CV: \", cv_svm_kernel.mean())\n\ny_pred_svm_kernel_train = svm_kernel.predict(X_train_scaled)\naccuracy_svm_kernel_train = accuracy_score(y_train, y_pred_svm_kernel_train)\nprint(\"Training set: \", accuracy_svm_kernel_train)\n\ny_pred_svm_kernel_test = svm_kernel.predict(X_test_scaled)\naccuracy_svm_kernel_test = accuracy_score(y_test, y_pred_svm_kernel_test)\nprint(\"Test set: \", accuracy_svm_kernel_test)","e3f0c4ad":"svc = SVC(kernel = 'rbf')\nsvc.fit(X_train_scaled, y_train.ravel())\npred_svc = svc.predict(X_test_scaled)\nprint(classification_report(y_test, y_pred_svm_kernel_test))","717d1537":"<h3 style=\"color:pink;\"> Feature Scaling:  It is the process of standerizing the data. We do this to our data so that while building a machine learning model, our model is not biased towards a particular feature of the dataset. <\/h3>","bf916246":"<h1 style=\"color:pink;\"> Encoding our target variable <\/h1>","9cd269fc":"**Also, The model which gives the best result is Rndom Forest Classifier**","12a93fba":"<h1 style=\"color:powderblue;\"> Dividing the columns into input and target variable.<\/h1>","7a0a39df":"rwine.isnull().mean()","fb58bab0":"<h3 style=\"color:pink;\">Support Vector Machine: Support Vector Machine\u201d (SVM) is a supervised machine learning algorithm that can be used for both classification or regression challenges <\/h3>","00cd7540":"<h1 style=\"color:powderblue;\"> Plotting Charts and Observing them <\/h1>","543d5e1d":"Why Encoding is Important?\nWe want to classify our data. And it only make sense if our data is catagorical.\nSo, we are splitting the data in Good or Bad.","b599c2a9":"At the very end, I would just like to mention I have take help of several notebooks to understand and write my own.","8054f89a":"<h2 style=\"color:powderblue;\"> EDA <\/h2>","fbcbe315":"<h3 style","caf5cbd0":"<h3 style=\"color:pink;\"> Random Forest :   It consists of a large number of individual decision trees that operate as an ensemble. Each tree in the random forest spits out a class prediction and the class with the most votes becomes our model\u2019s prediction. It also remove the problem created by over fitting of data<\/h3>","9a86b271":"**Gini Coefficient: It shows the information gain and help to split the tree in more efficient way.**","b778817a":"<h3 style=\"color:pink;\"> Data is clean no null values present <\/h3>","139e8104":"<h2 style=\"color:pink;\"> Cross Validation: It is a resampling procedure used to evaluate machine learning models on a limited data sample. <\/h1>","91647fe3":"<h4 style=\"color:lightgreen;\">We can see that we have everything better in RBC than we had in linear. Model accuracy, precision everything improved<\/h4>","7bd28495":"<h1 style=\"color:powderblue;\"> The Red Wine Quality Prediction <\/h1>\n<h3 style=\"color:pink;\"> Step by step explanation <\/h3>","7bfd98ef":"<h2 style=\"color:powderblue;\"> Logistic Regression (Sigmoid\/S shaped) <\/h2>","a1c5d715":"<h3 style=\"color:powder Blue;\">Linear <\/h3>","7ff8c9c5":"<h3 style=\"color:pink;\"> Radial Basis Function <\/h3>","44a5e555":"<h3 style=\"color:lightgreen;\">Logistic Regressor gave us 88% Accuracy which is good but we will try to find better if possible.<\/h3>","d5ebe857":"<h3 style=\"color:powderblue;\"> Decison Tree: It is a supervised learning algorithm used for classification. <h3>","2bcc6e91":"<h3>The Data is divided using basic 80% of traning and 20% test set <\/h3>"}}