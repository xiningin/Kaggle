{"cell_type":{"50c1170c":"code","5b223f61":"code","e41daf32":"code","1c93c182":"code","34b408a9":"code","c7eddfa2":"code","9d68ffe6":"code","efb82ca6":"code","8da97cbe":"code","b402bcd5":"code","4b1a3a31":"code","f16219ad":"code","98609b67":"code","2b8a4794":"code","3a5f01cc":"code","eba08ccd":"code","5e68c8ea":"code","d9a3f144":"code","076a5926":"code","416f13a1":"code","38d758af":"code","9f496b89":"code","69c3aba8":"code","2d84fd25":"code","54324b08":"code","6065db92":"code","b3c0c1de":"code","d9a0715d":"code","0c9517b1":"code","ebe8d8f7":"code","0d5abb34":"code","90df3eef":"code","c3c7b90b":"code","a3ac63de":"code","d24cf432":"code","03876d31":"code","ca0322b1":"code","c089e0a4":"code","c9d5962e":"code","74bbd1ce":"code","dc536a48":"code","b45a596f":"code","8f778dd9":"code","504aa113":"code","2ea8be20":"code","2ace87db":"code","0483a737":"code","e6289d97":"code","bf2ffdf6":"code","f99b8046":"code","5b9f0efa":"code","3870db7f":"code","ce57dac6":"code","8df09d3a":"code","9a5f310b":"code","ff138283":"code","7c962696":"code","e79559ec":"code","880a7b47":"code","be57b471":"code","49a90d45":"code","dfab3c97":"code","8be066ce":"code","53434b70":"code","4a2b734d":"code","86e30c82":"code","560fb7ca":"code","cdc8e045":"code","5a913923":"code","c7be2307":"code","31dcb6bd":"code","a705a685":"code","68fe743c":"code","97bf3189":"code","49969d72":"code","ce4df5bd":"code","0bf1bd7d":"code","45847aea":"code","76333d09":"code","e3cda1ec":"code","e4ecaafa":"code","0280ef84":"code","d3c221db":"code","49225ca8":"code","04294acf":"code","28becd91":"code","d0408219":"code","b2057502":"code","2a2bfbc4":"code","3a8c18f3":"code","ee4c79cc":"code","d6724193":"code","f0f53dae":"code","74997b66":"code","fab355bf":"code","51b59d6e":"code","2f8588a6":"code","bdf4039b":"code","b4582085":"code","68d40a49":"code","b3d32d96":"code","28fbd6b3":"code","b4a0a995":"code","d9c04ce3":"markdown","94444494":"markdown","3ea5d9b8":"markdown","d7a19fea":"markdown","17217091":"markdown","c850a5eb":"markdown","f75459e3":"markdown","dcb41519":"markdown","a2940c85":"markdown","95f37e68":"markdown","2884535c":"markdown","4da85335":"markdown","8b62af51":"markdown","de80d31f":"markdown","8d85b493":"markdown","fa49c61f":"markdown","12bff9c7":"markdown","79492574":"markdown","4d5f57ec":"markdown","abefbcbf":"markdown","3ed6621a":"markdown","761f5ff5":"markdown","bdc211f6":"markdown","da4fa3b9":"markdown","02ee8c5e":"markdown","7be2f56d":"markdown","5673fda6":"markdown"},"source":{"50c1170c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA, IncrementalPCA\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve ,confusion_matrix , precision_score, recall_score, f1_score\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.feature_selection import RFE\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5b223f61":"pip install -U lightautoml","e41daf32":"from lightautoml.automl.presets.tabular_presets import TabularAutoML\nfrom lightautoml.tasks import Task","1c93c182":"# Read training data\ntrain_dataset = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain_dataset.head()","34b408a9":"# Read testing data\ntest_dataset = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest_dataset.head()","c7eddfa2":"all_data = train_dataset.append(test_dataset)\nall_data.shape","9d68ffe6":"train_dataset.shape","efb82ca6":"train_dataset.info(verbose = 1)","8da97cbe":"train_dataset.describe(include = 'all')","b402bcd5":"num_col = ['Age', 'SibSp', 'Parch', 'Fare']\ncat_col = ['Survived', 'Pclass', 'Sex', 'Ticket', 'Cabin', 'Embarked']","4b1a3a31":"for col in num_col:\n    plt.hist(train_dataset[col])\n    plt.title(col)\n    plt.show()","f16219ad":"corr = train_dataset[num_col].corr()\nsns.heatmap(corr)","98609b67":"pd.pivot_table(train_dataset, index=\"Survived\", values=num_col)","2b8a4794":"for col in cat_col:\n    sns.barplot(x = train_dataset[col].value_counts().index, y = train_dataset[col].value_counts()).set_title(col)\n    plt.show()","3a5f01cc":"pd.pivot_table(train_dataset, index=\"Survived\", columns=\"Pclass\",\n               values=\"Ticket\", aggfunc=\"count\")","eba08ccd":"pd.pivot_table(train_dataset, index=\"Survived\", columns=\"Sex\",\n               values=\"Ticket\", aggfunc=\"count\")","5e68c8ea":"pd.pivot_table(train_dataset, index=\"Survived\", columns=\"Embarked\",\n               values=\"Ticket\", aggfunc=\"count\")","d9a3f144":"train_dataset['Cabin_split'] = train_dataset['Cabin'].apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\ntest_dataset['Cabin_split'] = test_dataset['Cabin'].apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n\ntrain_dataset['Cabin_split'].value_counts()","076a5926":"pd.pivot_table(train_dataset, index=\"Survived\", columns=\"Cabin_split\",\n               values=\"Ticket\", aggfunc=\"count\")","416f13a1":"train_dataset['Name'].head()","38d758af":"train_dataset['Name_title'] = train_dataset['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\ntest_dataset['Name_title'] = test_dataset['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\ntrain_dataset['Name_title'].value_counts()","9f496b89":"round(all_data.isnull().sum() * 100 \/ all_data.shape[0], 2)","69c3aba8":"# Imputing for age column with mean age\nmean_age = int(all_data.Age.mean())\nall_data.Age.fillna(mean_age, inplace = True)\n\nmean_fare = int(all_data.Age.mean())\nall_data.Age.fillna(mean_age, inplace = True)","2d84fd25":"all_data['Cabin_split'] = all_data['Cabin'].apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\nall_data['Name_title'] = all_data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\nall_data['Numeric_ticket'] = all_data['Ticket'].apply(lambda x: 1 if x.isnumeric() else 0)\n","54324b08":"# dropping na from Embarked\nall_data.dropna(subset=[\"Embarked\"], inplace=True)\nall_data.shape","6065db92":"all_data['norm_sibsp'] = np.log(all_data['SibSp'] + 1)\nall_data['norm_sibsp'].hist()","b3c0c1de":"all_data['norm_fare'] = np.log(all_data['Fare'] + 1)\nall_data['norm_fare'].hist()","d9a0715d":"round(all_data.isnull().sum() * 100 \/ all_data.shape[0], 2)","0c9517b1":"all_data['norm_fare'] = all_data['norm_fare'].fillna(all_data['norm_fare'].median())","ebe8d8f7":"round(all_data.isnull().sum() * 100 \/ all_data.shape[0], 2)","0d5abb34":"all_data.head()","90df3eef":"# Map categorical columns\nall_data.Sex = all_data.Sex.map({ 'female': 0, 'male': 1 })","c3c7b90b":"all_data.Embarked = all_data.Embarked.map({ 'S': 0, 'C': 1, 'Q': 2 })","a3ac63de":"all_data['Survived'].unique()","d24cf432":"all_data.drop(['Name', 'Ticket', 'Cabin', 'Fare'], axis = 1, inplace = True)","03876d31":"all_data = pd.get_dummies(data = all_data, columns = ['SibSp', 'Pclass', 'Parch', 'Embarked', 'Cabin_split', 'Name_title'], drop_first = True)\nall_data.head()","ca0322b1":"all_data = all_data.set_index('PassengerId')","c089e0a4":"test_dataset = all_data.loc[all_data['Survived'].isnull()]\ntrain_dataset = all_data.loc[all_data['Survived'].notnull()]","c9d5962e":"from sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(train_dataset, train_size=0.7, random_state=100)\nprint(df_train.shape)\nprint(df_test.shape)","74bbd1ce":"y_train = df_train['Survived']\nX_train = df_train.drop(['Survived'], axis = 1)","dc536a48":"y_test = df_test['Survived']\nX_test = df_test.drop(['Survived'], axis = 1)","b45a596f":"X_train.head()","8f778dd9":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train[['Age', 'norm_sibsp', 'norm_fare']] = scaler.fit_transform(X_train[['Age', 'norm_sibsp', 'norm_fare']])\nX_test[['Age', 'norm_sibsp', 'norm_fare']] = scaler.transform(X_test[['Age', 'norm_sibsp', 'norm_fare']])\ntest_dataset[['Age', 'norm_sibsp', 'norm_fare']] = scaler.transform(test_dataset[['Age', 'norm_sibsp', 'norm_fare']])\nX_train.head()","504aa113":"result_column_names = [\"Model\", \"AuC Score\"]\nmodel_results = pd.DataFrame(columns = result_column_names)","2ea8be20":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(class_weight='balanced')","2ace87db":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg, 10)             # running RFE with 15 variables as output\nrfe = rfe.fit(X_train, y_train)","0483a737":"## list columns that rfe predicted as useful\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","e6289d97":"rfe_col = X_train.columns[rfe.support_]\nrfe_col","bf2ffdf6":"import statsmodels.api as sm\nX_train_sm = sm.add_constant(X_train[rfe_col])\nlogm2 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","f99b8046":"#Drop parch_4\nrfe_col = rfe_col.drop('Parch_4')","5b9f0efa":"logmodel = sm.GLM(y_train, X_train_sm[rfe_col], family = sm.families.Binomial())\nres = logmodel.fit()\nres.summary()","3870db7f":"# Drop SibSp_8\nrfe_col = rfe_col.drop('SibSp_5')","ce57dac6":"logmodel = sm.GLM(y_train, X_train_sm[rfe_col], family = sm.families.Binomial())\nres = logmodel.fit()\nres.summary()","8df09d3a":"#Drop SibSp_5\nrfe_col = rfe_col.drop('SibSp_3')","9a5f310b":"logmodel = sm.GLM(y_train, X_train_sm[rfe_col], family = sm.families.Binomial())\nres = logmodel.fit()\nres.summary()","ff138283":"### Checking VIF values\nvif = pd.DataFrame()\nvif['Features'] = X_train[rfe_col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[rfe_col].values, i) for i in range(X_train[rfe_col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","7c962696":"log_rfe = LogisticRegression(class_weight='balanced')\nlog_rfe.fit(X_train[rfe_col], y_train)\n\ny_train_pred = log_rfe.predict(X_train[rfe_col])\n\ny_test_pred = log_rfe.predict(X_test[rfe_col])","e79559ec":"print(\"AUC score (train): \", roc_auc_score(y_train, y_train_pred))\nprint(\"AUC score (test): \", roc_auc_score(y_test, y_test_pred))","880a7b47":"model_results = model_results.append({ \n    result_column_names[0]: 'Logistic Regression using RFE', \n    result_column_names[1]: roc_auc_score(y_test, y_test_pred) }, ignore_index = True)\nmodel_results","be57b471":"##initialize and fit pca\npca = PCA(random_state = 42)\npca.fit(X_train)","49a90d45":"# plot feature variance\nfeatures = range(pca.n_components_)\ncumulative_variance = np.round(np.cumsum(pca.explained_variance_ratio_)*100, decimals=4)\nplt.figure(figsize=(175\/20,100\/20)) # 100 elements on y-axis; 175 elements on x-axis; 20 is normalising factor\nplt.plot(cumulative_variance)","dfab3c97":"var_cumu = np.cumsum(pca.explained_variance_ratio_)\nvar_cumu","8be066ce":"fig = plt.figure(figsize=[12,8])\nplt.vlines(x=6, ymax=2, ymin=0, colors=\"r\", linestyles=\"--\")\nplt.hlines(y=0.95, xmax=30, xmin=0, colors=\"g\", linestyles=\"--\")\nplt.plot(var_cumu)\nplt.ylabel(\"Cumulative variance explained\")\nplt.show()","53434b70":"pca_final = PCA(0.99, svd_solver='full')","4a2b734d":"df_train_pca = pca_final.fit_transform(X_train)","86e30c82":"df_train_pca.shape","560fb7ca":"corrmat = np.corrcoef(df_train_pca.transpose())\nplt.figure(figsize=[15,15])\nsns.heatmap(corrmat, annot=True)","cdc8e045":"df_test_pca = pca_final.transform(X_test)\ndf_test_pca.shape","5a913923":"# logistic regression - the class weight is used to handle class imbalance - \n#it adjusts the cost function\nlogistic = LogisticRegression(class_weight='balanced')\n\n# hyperparameter space\nparams = {'C': [0.1, 0.2, 0.4, 0.5, 1, 2], 'penalty': ['l1', 'l2']}\n\n# create 5 folds\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 42)\n\n# create gridsearch object\nlog_pca = GridSearchCV(estimator=logistic, cv=folds,\n                     param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)","c7be2307":"# fit model\nlog_pca.fit(df_train_pca, y_train)","31dcb6bd":"# print best hyperparameters\nprint(\"Best AUC: \", log_pca.best_score_)\nprint(\"Best hyperparameters: \", log_pca.best_params_)\nprint(log_pca.best_estimator_)","a705a685":"y_pred_prob_train = log_pca.predict(df_train_pca)\nprint(\"AUC score (train):\", roc_auc_score(y_train, y_pred_prob_train))","68fe743c":"y_pred_prob_test = log_pca.predict(df_test_pca)\nprint(\"AUC score (test):\", roc_auc_score(y_test, y_pred_prob_test))","97bf3189":"model_results = model_results.append({ \n    result_column_names[0]: 'Logistic Regression using PCA', \n    result_column_names[1]: roc_auc_score(y_test, y_pred_prob_test) }, ignore_index = True)\nmodel_results","49969d72":"dsTree = DecisionTreeClassifier(class_weight = 'balanced')\ndsTreeParams = {\n            'max_depth': [10, 15],\n            'min_samples_leaf': [10, 20, 30]}\n\ndsFolds = KFold(n_splits = 5, random_state = 42, shuffle=True)\n\ndsTreeModel = GridSearchCV(estimator = dsTree, cv = dsFolds, param_grid = dsTreeParams, \n                           scoring='roc_auc', verbose = 1, n_jobs = -1)","ce4df5bd":"# fit model\ndsTreeModel.fit(X_train, y_train)","0bf1bd7d":"# print best hyperparameters\nprint(\"Best AUC: \", dsTreeModel.best_score_)\nprint(\"Best hyperparameters: \", dsTreeModel.best_params_)","45847aea":"# check area under curve\ny_train_pred_prob_ds = dsTreeModel.predict(X_train)\nprint(\"AUC score on train data:\", roc_auc_score(y_train, y_train_pred_prob_ds))","76333d09":"# check area under curve\ny_test_pred_prob_ds = dsTreeModel.predict(X_test)\nprint(\"AUC score on test data:\", roc_auc_score(y_test, y_test_pred_prob_ds))","e3cda1ec":"model_results = model_results.append({ \n    result_column_names[0]: 'Decision Tree', \n    result_column_names[1]: roc_auc_score(y_test, y_test_pred_prob_ds) }, ignore_index = True)\nmodel_results","e4ecaafa":"#random forest - the class weight is used to handle class imbalance - it adjusts the cost function\nforest = RandomForestClassifier(class_weight='balanced', n_jobs = -1)\n\n# hyperparameter space\nparams = { \n            'max_depth': [7, 10, 12],\n            'min_samples_leaf': [3, 5, 7],\n            'n_estimators': [10, 30, 40]}\n\n# create 5 folds\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 42)\n\n# create gridsearch object\nrf_model = GridSearchCV(estimator=forest, cv=folds, param_grid=params, \n                        scoring='roc_auc', n_jobs=-1, verbose=1)","0280ef84":"# fit model\nrf_model.fit(X_train, y_train)","d3c221db":"# print best hyperparameters\nprint(\"Best AUC: \", rf_model.best_score_)\nprint(\"Best hyperparameters: \", rf_model.best_params_)","49225ca8":"# check area under curve\ny_train_pred_prob_forest = rf_model.predict(X_train)\nprint(\"AUC score:\", roc_auc_score(y_train, y_train_pred_prob_forest))","04294acf":"# check area under curve\ny_test_pred_prob_forest = rf_model.predict(X_test)\nprint(\"AUC score:\", roc_auc_score(y_test, y_test_pred_prob_forest))","28becd91":"model_results = model_results.append({ \n    result_column_names[0]: 'Random Forest', \n    result_column_names[1]: roc_auc_score(y_test, y_test_pred_prob_forest) }, ignore_index = True)\nmodel_results","d0408219":"from xgboost import XGBClassifier","b2057502":"# creating a KFold object \nfolds_xg = KFold(n_splits = 5, shuffle = True, random_state = 4)\n\n# specify range of hyperparameters\nparam_grid_xg = {\n            'learning_rate': [0.008, 0.01],\n             \"max_depth\"        : [7],\n             \"min_child_weight\" : [ 0.5, 1],\n             \"colsample_bytree\" : [ 0.1, 0.2 ],\n             'n_estimators': [15, 20]\n            }        \n\n# specify model\nxgb_model = XGBClassifier(scale_pos_weight = 1.5)\n\n# set up GridSearchCV()\nmodel_cv_xg = GridSearchCV(estimator = xgb_model, \n                        param_grid = param_grid_xg, \n                        scoring= 'roc_auc',\n                        cv = folds_xg, \n                        n_jobs = 4,\n                        verbose = 1,\n                        return_train_score=True)","2a2bfbc4":"# fit the model\nmodel_cv_xg.fit(X_train, y_train)","3a8c18f3":"# print best hyperparameters\nprint(\"Best AUC: \", model_cv_xg.best_score_)\nprint(\"Best hyperparameters: \", model_cv_xg.best_params_)","ee4c79cc":"# check area under curve\ny_train_pred_prob_xgb = model_cv_xg.predict(X_train)\nprint(\"AUC score (train):\", round(roc_auc_score(y_train, y_train_pred_prob_xgb),2))","d6724193":"# check area under curve\ny_test_pred_prob_xgb = model_cv_xg.predict(X_test)\nprint(\"AUC score (test):\", roc_auc_score(y_test, y_test_pred_prob_xgb))","f0f53dae":"model_results = model_results.append({ \n    result_column_names[0]: 'XGBoost', \n    result_column_names[1]: roc_auc_score(y_test, y_test_pred_prob_xgb) }, ignore_index = True)\nmodel_results","74997b66":"# check area under curve\ntest_dataset.drop('Survived', axis = 1, inplace = True)\ny_sub_pred_rf = model_cv_xg.predict(test_dataset)\ntest_dataset['Survived'] = y_sub_pred_rf","fab355bf":"test_dataset.head()","51b59d6e":"test_dataset['Survived'].to_csv('XGBoost.csv')","2f8588a6":"# check area under curve\ntest_dataset.drop('Survived', axis = 1, inplace = True)\ny_sub_pred_rf = log_rfe.predict(test_dataset[rfe_col])\ntest_dataset['Survived'] = y_sub_pred_rf","bdf4039b":"test_dataset['Survived'].to_csv('Logistic_RFE.csv')","b4582085":"automl = TabularAutoML(task = Task('binary', metric = lambda y_true, y_pred: roc_auc_score(y_true, y_pred)))\noof_pred = automl.fit_predict(df_train, roles = { 'target': 'Survived', 'drop': [] })\ntest_pred = automl.predict(df_test)","68d40a49":"# check area under curve\ntrain_pred = automl.predict(df_train)\nprint(\"AUC score (train):\", roc_auc_score(df_train['Survived'], train_pred.data[:, 0]))","b3d32d96":"# check area under curve\nprint(\"AUC score (test):\", roc_auc_score(df_test['Survived'], test_pred.data[:, 0]))","28fbd6b3":"model_results = model_results.append({ \n    result_column_names[0]: 'Light AutoML', \n    result_column_names[1]: roc_auc_score(df_test['Survived'], test_pred.data[:, 0]) }, ignore_index = True)\nmodel_results","b4a0a995":"# check area under curve\ntest_dataset.drop('Survived', axis = 1, inplace = True)\ntest_dataset['Survived'] = automl.predict(test_dataset).data[:, 0]\ntest_dataset['Survived'].to_csv('LightAutoML.csv')","d9c04ce3":"# Predictions using LightAutoML","94444494":"# LightAutoML","3ea5d9b8":"# Dummy Categorical Columns","d7a19fea":"# EDA","17217091":"### Clearly there is no correlation in PCA parameters","c850a5eb":"### Looks like, people that payed more fare survived, younger people survived","f75459e3":"### From the above plots we can see that only Age follows a normal distribution","dcb41519":"## Let's make final submision predictions using XGBoost","a2940c85":"### Model performs with an accuracy of 81.12% on test dataset, which is a decent performance by the model","95f37e68":"## Missing values inspection","2884535c":"# Using Statsmodel","4da85335":"#### Clearly cabin has a lot of missing values, we can drop this column","8b62af51":"# Let's try Logistic Regression with PCA","de80d31f":"# Feature scaling","8d85b493":"### First class survival ratio is way more the survival ratio of 3rd class. Clearly the rich people survived more","fa49c61f":"### Predictions using Logistic regression","12bff9c7":"### Clearly, using PCA, model performance has increased","79492574":"# Train-test split","4d5f57ec":"### PCA has chosen 24 parameters that explain 99% variance","abefbcbf":"# From above results, it is concluded that XGBoost performs the best with 82.68 accuracy.","3ed6621a":"## Let's try Random Forest","761f5ff5":"## Finally, let us try XGBoost","bdc211f6":"## Let's try Decision Tree","da4fa3b9":"### Clearly, Parch and SibSp have high correlation","02ee8c5e":"# Understanding Train dataset","7be2f56d":"# Logistic Regression using RFE","5673fda6":"### As observed, female survival ratio is much higher than male survival ratio"}}