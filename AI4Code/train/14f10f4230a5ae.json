{"cell_type":{"2a677d9a":"code","fd7683a1":"code","c087c341":"code","84d1bd99":"code","8b064ac9":"code","f104ce7f":"code","edd5a192":"code","e61a2591":"code","4d7a7300":"code","1435166a":"code","4282ee98":"code","67e4d8de":"code","b937c524":"code","a309e585":"code","b70ed2ec":"code","80b2ffbc":"code","1f00eef1":"code","380e18fa":"code","007a3b55":"code","a8e83736":"code","dc6db573":"code","79c0e5f8":"code","90bb8e12":"code","d00de472":"code","8ef6c439":"code","2b83b2dd":"code","2b71dfee":"code","fd17a169":"code","29956683":"code","2aae7047":"code","d238d5f6":"code","ade7d432":"code","9c8cc4eb":"code","388bca08":"code","7b4a4b48":"code","2d48d835":"code","366cd754":"code","3bb98e2c":"code","48389b6e":"code","bd133f39":"code","634f2e00":"code","db2f0815":"code","346b34b9":"code","cdfd0fc0":"code","3d182d72":"code","621becab":"markdown","045b77ca":"markdown","187d8b37":"markdown","20914fe9":"markdown","4e328efc":"markdown","4af8c122":"markdown","3263a0fe":"markdown","81b61a52":"markdown","0e2f2b52":"markdown","88b93a7d":"markdown","5be67439":"markdown","dfa7028b":"markdown","bf4b0824":"markdown","6aa9f0c9":"markdown","69ffdc63":"markdown","7eed55b1":"markdown","3181b463":"markdown","36acaddd":"markdown","9a5bffba":"markdown","f8af4831":"markdown","df8792b5":"markdown","c90fd4f4":"markdown","ebb7b262":"markdown","2ca47a9a":"markdown","86a56356":"markdown","b9b6ce24":"markdown","2cc5cd08":"markdown","540545c9":"markdown","dbd29778":"markdown","e5ed131d":"markdown","b99449e1":"markdown"},"source":{"2a677d9a":"import numpy as np\nimport pandas as pd\nimport matplotlib.gridspec as gs\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import cross_val_score\nimport statsmodels.graphics.mosaicplot as mosaicplt\nimport seaborn as sns\nfrom string import Template\nfrom matplotlib import rcParams\nfrom fancyimpute import MICE, KNN\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom scipy.stats import norm\nfrom sklearn.metrics import roc_curve\nfrom sklearn.pipeline import make_pipeline\nimport re\nimport string\nimport itertools\ncolor = sns.color_palette()\n%matplotlib inline","fd7683a1":"dataset_url = Template('..\/input\/$filename')\ntrain_df = pd.read_csv(dataset_url.substitute(filename='train.csv'))\ntest_df = pd.read_csv(dataset_url.substitute(filename='test.csv'))\ntrain_df.shape[0]\ntest_df.shape[0]","c087c341":"def distComparison(df1, df2):\n    a = len(df1.columns)\n    if a%2 != 0:\n        a += 1\n    \n    n = np.floor(np.sqrt(a)).astype(np.int64)\n    \n    while a%n != 0:\n        n -= 1\n    \n    m = (a\/n).astype(np.int64)\n    coords = list(itertools.product(list(range(m)), list(range(n))))\n    \n    numerics = df1.select_dtypes(include=[np.number]).columns\n    cats = df1.select_dtypes(include=['category']).columns\n    \n    fig = plt.figure(figsize=(15, 15))\n    axes = gs.GridSpec(m, n)\n    axes.update(wspace=0.25, hspace=0.25)\n    \n    for i in range(len(numerics)):\n        x, y = coords[i]\n        ax = plt.subplot(axes[x, y])\n        col = numerics[i]\n        sns.kdeplot(df1[col].dropna(), ax=ax, label='df1').set(xlabel=col)\n        sns.kdeplot(df2[col].dropna(), ax=ax, label='df2')\n        \n    for i in range(0, len(cats)):\n        x, y = coords[len(numerics)+i]\n        ax = plt.subplot(axes[x, y])\n        col = cats[i]\n\n        df1_temp = df1[col].value_counts()\n        df2_temp = df2[col].value_counts()\n        df1_temp = pd.DataFrame({col: df1_temp.index, 'value': df1_temp\/len(df1), 'Set': np.repeat('df1', len(df1_temp))})\n        df2_temp = pd.DataFrame({col: df2_temp.index, 'value': df2_temp\/len(df2), 'Set': np.repeat('df2', len(df2_temp))})\n\n        sns.barplot(x=col, y='value', hue='Set', data=pd.concat([df1_temp, df2_temp]), ax=ax).set(ylabel='Percentage')\n        \ndistComparison(train_df.drop(['Survived'], 1), test_df)","84d1bd99":"train_df.head()\ntest_df.head()","8b064ac9":"dtype_df = train_df.dtypes.reset_index()\nprint(dtype_df)\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","f104ce7f":"# Get title from passenger names\ndef get_title (name):\n    try:\n        found = re.search('\\S\\w+\\.',name).group(0).strip('.')\n    except AttributeError:\n        found = 'Unknown'\n    return found\n\ntrain_df['Title']=train_df['Name'].apply(lambda name: get_title(name))\ngrouped_df = train_df.groupby(['Sex','Title'])['Title'].aggregate('count').unstack(fill_value=0)\npd.options.display.float_format = '{:,.2g}'.format\ngrouped_df","edd5a192":"# Get the list of column headers\ntitle_list = list(grouped_df.columns.values)","e61a2591":"# Search for substrings\ndef substrings_in_string(big_string, substrings):\n    for substring in substrings:\n        if string.find(big_string, substring) != -1:\n            return substring\n    print (big_string)\n    return np.nan","4d7a7300":"#replacing all titles with mr, mrs, miss, master\ndef replace_titles(x):\n    title = x['Title']\n    if title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']:\n        return 'Mr'\n    elif title in ['Countess', 'Mme']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms']:\n        return 'Miss'\n    elif title =='Dr':\n        if x['Sex']=='Male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title\ntrain_df['Title']=train_df.apply(replace_titles, axis=1)","1435166a":"# Get the count of newly assigned titles.\ntrain_df.groupby('Title')['Survived'].aggregate('count').reset_index()","4282ee98":"# Get title from passenger names\ndef get_deck (cabin):\n    try:\n        found = re.search('^[a-zA-Z]',cabin).group(0)\n    except TypeError:\n        found = 'Unknown'\n    return found\n\ntrain_df['Deck']=train_df['Cabin'].apply(lambda cabin: get_deck(cabin))\ndecked_df = train_df.groupby(['Deck'])['Survived'].aggregate('count').reset_index()\npd.options.display.float_format = '{:,.2g}'.format\ndecked_df","67e4d8de":"cabin_list = list(decked_df['Deck'].values)\ncabin_list","b937c524":"#Creating new family_size column with the passenger inclusive\ntrain_df['Family_Size']=train_df['SibSp']+train_df['Parch'] + 1","a309e585":"# Preview overall attributes so far\ntrain_df.head()","b70ed2ec":"family_df = train_df.groupby(['Family_Size','Survived'])['Survived'].aggregate('count').unstack(fill_value=0)\npd.options.display.float_format = '{:,.2g}'.format\nfamily_df","80b2ffbc":"ax = sns.catplot(x=\"Family_Size\", hue=\"Survived\", kind=\"count\",data=train_df, aspect = 1.5)","1f00eef1":"# Function to discretize family size\ndef conv_discrete (size):\n    if (size == 1):\n        return 'singleton'\n    elif (1 < size <= 4) :\n        return 'small'\n    elif (size > 4):\n        return 'large'\n    else:\n        return 'unspecified'\n    \n# Discretize family size\ntrain_df['Family_Size_D'] = train_df['Family_Size'].apply(lambda size: conv_discrete(size))\n\n# train_df.head() - uncomment to view ","380e18fa":"##train_df.loc[:,['Survived','Family_Size_D']]","007a3b55":"# Visualize multivariate categorical data in a rigorous and informative way.\nmosaicplt.mosaic(train_df,index=['Survived','Family_Size_D'], gap=0.02,title='Family size by survival', statistic = True) ","a8e83736":"#Create a new function:\ndef count_missing(x):\n  return sum(x.isnull())\n\n#Applying per column:\nprint (\"Missing values per column:\")\nprint (train_df.apply(count_missing, axis=0)) #axis=0 defines that function is to be applied on each column\n\n#Applying per row:\nprint (\"\\nMissing values per row:\")\nprint (train_df.apply(count_missing, axis=1).head()) #axis=1 defines that function is to be applied on each row","dc6db573":"missing_embarked = train_df['Embarked'].isnull()\ntrain_df[missing_embarked]","79c0e5f8":"notnull_embarked = train_df['Embarked'].notnull() ## Exclude rows with null Embarked feature\nfig, ax = plt.subplots()\nsns.pointplot(x=\"x\",y=\"y\",kind=\"point\" , color=\"r\",\n              linestyles=\"--\",\n              markers=\"\",\n              data=pd.DataFrame({'x':[0,'S','C','Q'],'y':[80, 80,80,80]}),\n              ax=ax)\nsns.catplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", kind=\"box\", data=train_df[notnull_embarked] , ax=ax)","90bb8e12":"def repl_null_embarked(x):\n    if (x == 62 or 830):\n        return 'C'\n    else:\n        return x\n    \ntrain_df['Embarked'] = train_df['PassengerId'].apply(lambda x: repl_null_embarked(x))","d00de472":"## Uncomment to confirm Missing values have been added\n##missing_embarked_added = train_df['PassengerId'] == 830\n##train_df[missing_embarked_added]","8ef6c439":"#Applying per column: Missing Embarked value should be fixed\nprint (\"Missing values per column:\")\ntrain_df.apply(count_missing, axis=0) #axis=0 defines that function is to be applied on each column","2b83b2dd":"dtype_df_n = train_df.dtypes.reset_index()\nprint(dtype_df_n)","2b71dfee":"def prepForModel(df):\n    new_df = df.copy()\n    new_df['Sex'] = new_df['Sex'].astype('category')\n    new_df['Embarked'] = new_df['Embarked'].astype('category')\n    new_df.Pclass = new_df.Pclass.astype(\"int\")\n    new_df.SibSp = new_df.SibSp.astype(\"int\")\n    new_df.Parch = new_df.Parch.astype(\"int\")\n    new_df.Fare = new_df.Fare.astype(\"float\")\n    cat_columns = new_df.select_dtypes(['category']).columns\n    new_df[cat_columns] = new_df[cat_columns].apply(lambda x: x.cat.codes)\n    return new_df\n\ntrain_cl = prepForModel(train_df)\n\ntrain_cl.dtypes.reset_index()","fd17a169":"rf = RandomForestClassifier(n_estimators=1000,max_depth=None,min_samples_split=10)\ntrain_cl = prepForModel(train_df)\n\nXcol = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare','Embarked', 'Family_Size','Age']\nYcol = 'Survived'\n\nX = train_cl.loc[:, Xcol]\nY = train_cl.loc[:, Ycol]\n\nXmice = MICE(n_imputations=200, impute_type='col', verbose=False).complete(X)\nYmice = Y\n\n##train_df['Age'] = Xmice['Age']\nmice_err = cross_val_score(rf, Xmice, Y, cv=10, n_jobs=-1).mean()\nprint(\"[MICE] Estimated RF Test Error (n = {}, 10-fold CV): {}\".format(len(Xmice), mice_err))\n\n\nX_df = pd.DataFrame(Xmice)\nX_df.columns = Xcol\n#Applying per column:\nprint (\"Missing values per column after age insertion:\")\nX_df.apply(count_missing, axis=0)","29956683":"plt.figure(figsize=(10,4))\nsns.set_color_codes()\nsns.distplot(train_df['Age'].dropna(), color='R', label=\"before imputations\", hist=False, rug=True)\nsns.distplot(X_df['Age'], color='Y',label=\"after imputations\", hist=False) # Kernel Density Estimation\nplt.legend()\nplt.xlabel('Age distribution ', fontsize=12)\nplt.show()","2aae7047":"plt.figure(figsize=(10,4))\nsns.set_color_codes()\nsns.distplot(train_df['Age'].dropna(), color='G',kde=False, label=\"before imputations\", hist=False, fit=norm)\nsns.distplot(X_df['Age'], color='Y',kde=False,label=\"after imputations\", hist=False, fit=norm, rug=True) # Kernel Density Estimation\nplt.legend()\nplt.xlabel('Fitted curve for age distribution', fontsize=12)\nplt.show()","d238d5f6":"train_df['Age'] = X_df['Age']\n\nprint (\"Missing values per column after age insertion:\")\ntrain_df.apply(count_missing, axis=0)","ade7d432":"Xcol = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare','Embarked','Title','Deck','Family_Size_D','Age']\nYcol = 'Survived'\n\nXtrain = train_df.loc[:, Xcol]\nYtrain = train_df.loc[:, Ycol]","9c8cc4eb":"##RANDOM FOREST\n\nle = preprocessing.LabelEncoder()\ndef encode_str(df):\n    enc_df = df.copy()\n    enc_df['Sex'] = enc_df['Sex'].astype('object')\n    enc_df['Embarked'] = enc_df['Embarked'].astype('object')\n    for column_name in enc_df.columns:\n        if enc_df[column_name].dtype == object:\n            enc_df[column_name] = le.fit_transform(enc_df[column_name])\n        else:\n            pass\n    return enc_df\n\n## Split into train and test data set\nX_tr = encode_str(Xtrain)\nX_train, X_test, y_train, y_test = train_test_split(X_tr, Ytrain, test_size=0.5)\n## Create Random forest classification model\nrf.fit(X_train, y_train)\n\nypred_rf = rf.predict_proba(X_test)[:, 1]\nfpr_rf, tpr_rf, _ = roc_curve(y_test, ypred_rf)\nprint(rf.feature_importances_)","388bca08":"### DECISION TREES\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(criterion=\"entropy\")\nclf.fit(X_train, y_train)\n\nypred_clf = clf.predict_proba(X_test)[:, 1]\nfpr_clf, tpr_clf, _ = roc_curve(y_test, ypred_clf)\nprint(clf.feature_importances_)","7b4a4b48":"### LOGISTIC REGRESSION\nlr = LogisticRegression(penalty=\"l2\", n_jobs=-1)\nlr.fit(X_train, y_train)\nypred_lr = clf.predict_proba(X_test)[:, 1]\nfpr_lr, tpr_lr, _ = roc_curve(y_test, ypred_lr)\nprint(lr.coef_)","2d48d835":"### NAIVE BAYES - GUASSIAN NB\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, y_train)\nypred_nb = nb.predict_proba(X_test)[:, 1]\nfpr_nb, tpr_nb, _ = roc_curve(y_test, ypred_nb)","366cd754":"### KNN - K Nearest Neighbour\nfrom sklearn.neighbors import KNeighborsClassifier\nknc = KNeighborsClassifier(n_neighbors=3)\nknc.fit(X_train, y_train)\nypred_knc = knc.predict_proba(X_test)[:, 1]\nfpr_knc, tpr_knc, _ = roc_curve(y_test, ypred_knc)","3bb98e2c":"### SVM - Support Vector Machines\nfrom sklearn.svm import SVC\nsvc = SVC(probability=True)\nsvc.fit(X_train, y_train)\nypred_svc = svc.predict_proba(X_test)[:, 1]\nfpr_svc, tpr_svc, _ = roc_curve(y_test, ypred_svc)","48389b6e":"plt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_rf, tpr_rf, label='Random Forest')\nplt.plot(fpr_clf, tpr_clf, label='Decision Tree')\nplt.plot(fpr_lr, tpr_lr, label='Logistic Regression')\nplt.plot(fpr_nb, tpr_nb, label='GaussianNB')\nplt.plot(fpr_knc, tpr_knc, label='K-Nearest Neighbour')\nplt.plot(fpr_svc, tpr_svc, label='Support Vector Classifier')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()","bd133f39":"test_df.apply(count_missing, axis=0)","634f2e00":"## Get titles from dataset\ntest_df['Title']=test_df['Name'].apply(lambda name: get_title(name))\ntest_grouped_df = test_df.groupby(['Sex','Title'])['Title'].aggregate('count').unstack(fill_value=0)\npd.options.display.float_format = '{:,.2g}'.format\ntest_grouped_df","db2f0815":"# Replace Titles with custom title\ntest_df['Title']=test_df.apply(replace_titles, axis=1)\n## Create feature deck form cabin column\ntest_df['Deck']=test_df['Cabin'].apply(lambda cabin: get_deck(cabin))\n#Creating new family_size column with the passenger inclusive\ntest_df['Family_Size']=test_df['SibSp']+train_df['Parch'] + 1\n# Discretize family size\ntest_df['Family_Size_D'] = test_df['Family_Size'].apply(lambda size: conv_discrete(size))\ntest_cl = prepForModel(test_df)","346b34b9":"X_pred = encode_str(test_cl.loc[:, Xcol])\n\nXmice_pred = MICE(n_imputations=200, impute_type='col', verbose=False).complete(X_pred)\n\nXtest = pd.DataFrame(Xmice_pred)\nXtest.columns = Xcol\n#Applying per column:\nprint (\"Missing values per column after age insertion:\")\nXtest.apply(count_missing, axis=0)","cdfd0fc0":"Y_pred = rf.predict(Xtest)","3d182d72":"Y_out = pd.DataFrame(Y_pred)\nY_out.columns =['Survived']\nY_out['PassengerId'] = test_df['PassengerId']\n\nY_out.head()","621becab":"load datasets","045b77ca":"From the ROC curve above, Random Forest and Naive-Bayes  model seems to reamin the best model for our Titanic data set","187d8b37":"# Datasets overview","20914fe9":"Considering the size of families (the sum of siblings\/spouse(s)-SibSp and parents\/children-Parch attributes. Perhaps people traveling alone did better? Or on the other hand perhaps if you had a family, you might have risked your life looking for them, or even giving up a space up to them in a lifeboat.\nFirst we\u2019re going to make a family size variable based on number of siblings\/spouse(s) (maybe someone has more than one spouse?) and number of children\/parents.","4e328efc":"Compare ROC Curves for different classifier algorithm","4af8c122":"It can be deduced from the plot above that more people seems to survived with lower family size when compared with passengers with larger family size. Though some exception can be observed, which can be largely due to some other attributes that is likely to determine a passenger's survival like gender, age etc.","3263a0fe":"Our dataset has 7 classes of decks as can be seen (A-G, T). only 1st class passengers have cabins, the rest are \u2018Unknown\u2019. A cabin number looks like \u2018C123\u2019. The letter refers to the deck.","81b61a52":"* __Verify training and test set distribution__","0e2f2b52":"Preprocess test sets to predict passengers survival","88b93a7d":"# Feature Engineering","5be67439":"# Prediction","dfa7028b":"__Predictive imputation__","bf4b0824":"__Cabin__","6aa9f0c9":"Import modules","69ffdc63":"There\u2019s a survival penalty to singletons and those with family sizes above 4. This variable can be collapsed into three levels which will be helpful since there are comparatively fewer large families. Let\u2019s create a discretized family size variable.","7eed55b1":"The mosaic plot shows that we preserve our rule that there\u2019s a survival penalty among singletons and large families, but a benefit for passengers in small families.","3181b463":"__We can create an overview to see how new sttributes of 'Family_Size' can affect survival.__","36acaddd":"___Compute Receiver operating characteristic (ROC) for each classfication algorithm ___\n\n<code> Note ROC can only be computed for binary classification problem<\/code>\n","9a5bffba":"```a contingency table (also known as a cross tabulation or crosstab) is a type of table in a matrix format that displays the (multivariate) frequency distribution of the variables```","f8af4831":"With our test data prepared with no missing values and all string type variables encoded to integers, we can now proceed to predict whether a passenger from our test set survives or not. <br\/>\nThe __Random Forest Classification Model__ is used here since it gives us the best performance curve as shown in the ROC plot above.","df8792b5":"__Family size__","c90fd4f4":"This is just what I have been able to gather after studying other people analysis, I must recognize the works of [Megan Risdal ](https:\/\/www.kaggle.com\/mrisdal) - whose analysis in R Language served as a guide in handling ___feature engineering___ and ___Missingness___, was able to implement part of her ideas in python. <br\/>\n[ The work ](https:\/\/www.kaggle.com\/hephzaron\/investigating-imputation-methods) by [athi](https:\/\/www.kaggle.com\/athi94)  was also of tremendous benefit to me especially in analysing the distribution of training and test data set.\nI am new to Datascience , Python language, part knowledge of statistics, so criticism, corrections and contributions are highly welcome.\n","ebb7b262":"___As observed, after imputations, missing values for age seems to have been placed in regions with high density between age 20 and 40.\nThe fitted normal distribution only shows a minimal deviations in distribution before and after imputations.\nThe values of age generated from MICE can then be used to replace the missing values in the original data.___","2ca47a9a":"__Missing Data -Embarked__\n\nOur first attempt is to fix mixing data in the Embarked column.","86a56356":"**__Generate models basedd on various Supervised Classification Learning Algorithm__","b9b6ce24":"As observed, both paid $80 and are assigned to class 1","2cc5cd08":"__Title__\n\nThe passengers' name can be further broken down and passenger's title can be used in training to further aid our predictions","540545c9":"From the observations above, three of the features seems to have missing data with  cabin recording the highest number of missing data.","dbd29778":"On the boxplot, it can be seen that $80 fare falls on the middle quartile on 'C'. So, it most likely these passengers embarked from 'C'.","e5ed131d":"# Missing data\n\nAssumption: It is assumed that the type of Missingness here is Missing At Random(MAR)","b99449e1":"The datasets consists of 418 oobservations 12 features of type integer, float and object. 5 of the features are of type integer while 2 and 5 of the features are of type float and object respectively"}}