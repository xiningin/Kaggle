{"cell_type":{"ddf34815":"code","0dacd77f":"code","6a4f62d2":"code","04913c5b":"code","2d99ff3b":"code","482e49f6":"code","2959b583":"code","f6d2c080":"code","dea2b76c":"code","8d0cbccb":"code","1af11802":"code","6146c830":"code","35b79733":"code","ac7d2d67":"code","21d6d291":"code","86fe7289":"code","22bac081":"code","61654625":"code","b0103130":"code","19ab801c":"code","caceaf6a":"code","a019e099":"code","179b732c":"code","d03cbf89":"code","a33e46fd":"code","aac9b779":"code","9c7100c2":"code","7a426abe":"code","b128562c":"code","ba102f44":"code","c2026bfc":"code","109c1be8":"code","090819fc":"code","92fa7c65":"code","22aad165":"code","0c78cf15":"code","e1305da7":"code","086c60fc":"code","6bb89e0a":"code","29d0ca44":"code","6ae25f3e":"code","8d70c309":"code","a612145d":"code","4cca87df":"code","284d2c9e":"code","b09200c2":"code","10f6abeb":"code","6f2113b1":"code","065b2e1b":"code","38f4ed53":"code","6bac3170":"code","bbbeee7f":"code","d1c17b7a":"code","d01f864e":"code","c3ff50d1":"code","1210f099":"code","bc31bab2":"code","ca5ef142":"code","115bdec3":"code","d3898638":"code","9ecf05b6":"code","68c506cf":"markdown","14e7f756":"markdown","f3d035eb":"markdown","e8d315c1":"markdown","5060478b":"markdown","5a31fe86":"markdown","96041e5b":"markdown","296a48c2":"markdown","9b615ed0":"markdown","98cc6c30":"markdown","f21b7e8d":"markdown","901772a2":"markdown","4a291bf5":"markdown","5b8ac190":"markdown","f17edc35":"markdown","a43fdea5":"markdown","8978873f":"markdown","86d503a5":"markdown","45c77fa8":"markdown","049c2fb8":"markdown","5e6d57bf":"markdown"},"source":{"ddf34815":"# Setting package umum \nimport pandas as pd\nimport pandas_profiling as pp\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom tqdm import tqdm_notebook as tqdm\nimport time\nimport tensorflow as tf\n%matplotlib inline\n\nfrom matplotlib.pylab import rcParams\n# For every plotting cell use this\n# grid = gridspec.GridSpec(n_row,n_col)\n# ax = plt.subplot(grid[i])\n# fig, axes = plt.subplots()\nrcParams['figure.figsize'] = [10,5]\nplt.style.use('fivethirtyeight') \nsns.set_style('whitegrid')\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom tqdm import tqdm\n\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 50)\npd.options.display.float_format = '{:.5f}'.format\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0dacd77f":"### Install packages\n!pip install -U ppscore","6a4f62d2":"### Load dataset\ndf_train = pd.read_csv('\/kaggle\/input\/student-shopee-code-league-marketing-analytics\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/student-shopee-code-league-marketing-analytics\/test.csv')\ndf_user = pd.read_csv('\/kaggle\/input\/student-shopee-code-league-marketing-analytics\/users.csv')","04913c5b":"# Sumber : https:\/\/www.kaggle.com\/kabure\/eda-feat-engineering-encode-conquer\ndef count_pcg_plot(df, var, target, ax, bar_color, line_color, text_size, y2_label, adjust_height=1000) :\n\n    # Bikin countplotnya\n    ax = sns.countplot(data=df, x=var, color=bar_color, order=df[var].sort_values().unique())\n    ax.set_title('Information of '+var, fontsize=20, fontname='Monospace', fontweight=\"bold\")\n\n    # Buat jadi dua y-axis\n    ax2 = ax.twinx()\n\n    # Hitung persentase target tiap value di variabel\n    tmp = pd.crosstab(df[var], df[target], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'No',1:'Yes'}, inplace=True)\n\n    # Buat lineplotnya\n    ax2 = sns.pointplot(x=var, y='Yes', data=tmp\n                     ,order=list(tmp[var].values)\n                     ,color=line_color, legend=False, scale=0.5)\n    ax2.set_ylabel(y2_label, fontname='Monospace')\n\n    # Kosmetiknya\n    total = len(df)\n    height_plus = 0.01*total\n    sizes = []\n    for p in ax.patches :\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()\/2., height + adjust_height,\n              '{:1.2f}%'.format(height\/total*100),\n              ha=\"center\", fontsize=text_size, fontname='Monospace')\n    ax.set_ylim(0, max(sizes)*1.2) ;","2d99ff3b":"# Function to make a donut chart\ndef make_donut_chart(sizes, labels, colors=None, explode=None) :\n    '''\n    Make a donut chart\n\n    Args :\n    - sizes (list) : Proporsi ukuran tiap class\n    - labels (list) : Nama tiap class\n    - colors (list) : Hexcode untk tiap class\n    - explode (list) : Untuk membuat donut yang misah tiap class\n    '''\n\n    # Buat plot\n    plt.pie(sizes, colors = colors, labels=labels, autopct='%1.1f%%', startangle=90, pctdistance=0.85, explode = explode)\n\n    # Buat lingkaran dalam\n    centre_circle = plt.Circle((0,0),0.70,fc='white')\n    fig = plt.gcf()\n    fig.gca().add_artist(centre_circle)\n\n    # Kasih detail tambahan\n    plt.axis('equal')  \n    plt.tight_layout()","482e49f6":"### Overview data train\ndf_train.head(11)","2959b583":"### Overview data test\ndf_test.head(11)","f6d2c080":"### Overview data user\ndf_user.head(11)","dea2b76c":"# Percentage of open flag\nrcParams['figure.figsize'] = [7,5]\nplt.style.use('fivethirtyeight') \nsns.set_style('whitegrid')\n\n# Plot prep\nsizes = df_train['open_flag'].value_counts() \/ len(df_train) * 100\nlabels = ['Not Open','Open']\ncolors = ['#4285F4','#EA4335']\nexplode_donut = [0.05, 0.1]\n\n# Plot\nmake_donut_chart(sizes, labels, colors, explode_donut)\nplt.title('Percentage of open flag', fontsize=17, fontname='Monospace', fontweight=\"bold\") ;","8d0cbccb":"# Function to give quick general information about the dataset\ndef dataset_summary(df) :\n    '''\n    Give quick feneral information about the dataset such as dtpes, missing values, and unique values\n\n    Args :\n    - df (pd.DataFrame) : Dataset\n\n    Return :\n    - summary_df (pd.DataFrame) : Contain general information\n    '''\n\n    # Make summary dataframe\n    summary_df = pd.DataFrame()\n\n    # Input the characteristic of the dataset\n    summary_df['Var'] = df.columns\n    summary_df['Dtypes'] = df.dtypes.values\n    summary_df['Total Missing'] = df.isnull().sum().values\n    summary_df['Missing%'] = summary_df['Total Missing'] \/ len(df) * 100\n    summary_df['Total Unique'] = df.nunique().values\n    summary_df['Unique%'] = summary_df['Total Unique'] \/ len(df) * 100\n\n    # Dataset dimension\n    print('Dataset dimension :',df.shape)\n\n    return summary_df","1af11802":"### Summary of data\ndataset_summary(df_train)","6146c830":"### Are all the row with missing \"attr_1\" is the same row as missing \"age\"\ndataset_summary(df_user[df_user['attr_1'].isna()])","35b79733":"### Check wether all row in train dataset can be merge with data user\ndf_comb_train = df_train.merge(df_user, on='user_id')\n\nprint('Total row on data train :',len(df_train))\nprint('After merge :',len(df_comb_train))","ac7d2d67":"### Check wether all row in test dataset can be merge with data user\ndf_comb_test = df_test.merge(df_user, on='user_id')\n\nprint('Total row on data test :',len(df_test))\nprint('After merge :',len(df_comb_test))","21d6d291":"### Check the distribution of missing attr_1 and not\nrcParams['figure.figsize'] = [10,5]\nplt.style.use('fivethirtyeight') \nsns.set_style('whitegrid')\ngrid = gridspec.GridSpec(1,1)\n\n# Dataset prep\ndf_plot = df_comb_train.copy()\ndf_plot['attr1_is_nan'] = df_plot['attr_1'].isna()\n\n# Plot prep\nline_color = '#EA4335'\nbar_color = '#4285F4'\ntext_size = 12\ny2_label = 'open_flag_rate'\nlist_var = ['attr1_is_nan']\n\n# Plot\nfor i,var in enumerate(list_var) :\n    ax = plt.subplot(grid[i])\n    count_pcg_plot(df_plot, var, 'open_flag', ax, bar_color, line_color, text_size, y2_label)","86fe7289":"### Check the distribution of attr var\nrcParams['figure.figsize'] = [10,15]\nplt.style.use('fivethirtyeight') \nsns.set_style('whitegrid')\ngrid = gridspec.GridSpec(3,1)\n\n# Dataset prep\ndf_plot = df_comb_train.copy()\n\n# Plot prep\nline_color = '#EA4335'\nbar_color = '#4285F4'\ntext_size = 12\ny2_label = 'open_flag_rate'\nlist_var = ['attr_1','attr_2','attr_3']\n\n# Plot\nfor i,var in enumerate(list_var) :\n    ax = plt.subplot(grid[i])\n    count_pcg_plot(df_plot, var, 'open_flag', ax, bar_color, line_color, text_size, y2_label)\n    \nplt.tight_layout()","22bac081":"### Remove attr_2\ndf_comb_train.drop(columns=['attr_2'], inplace=True)\ndf_comb_test.drop(columns=['attr_2'], inplace=True)","61654625":"### Fill NaN values for attr_1\ndf_comb_train['attr_1'] = df_comb_train['attr_1'].fillna(1)\ndf_comb_test['attr_1'] = df_comb_test['attr_1'].fillna(1)","b0103130":"### Check the distribution of domain\nrcParams['figure.figsize'] = [15,5]\nplt.style.use('fivethirtyeight') \nsns.set_style('whitegrid')\ngrid = gridspec.GridSpec(1,1)\n\n# Dataset prep\ndf_plot = df_comb_train.copy()\n\n# Plot prep\nline_color = '#EA4335'\nbar_color = '#4285F4'\ntext_size = 12\ny2_label = 'open_flag_rate'\nlist_var = ['domain']\n\n# Plot\nfor i,var in enumerate(list_var) :\n    ax = plt.subplot(grid[i])\n    count_pcg_plot(df_plot, var, 'open_flag', ax, bar_color, line_color, text_size, y2_label)\n    \nplt.tight_layout()","19ab801c":"### Make 'domain_type'\nlist_low_domain = ['@163.com','@gmail.com','@yahoo.com','@ymail.com']\nlist_med_domain = ['@outlook.com','@qq.com','@rocketmail.com']\nlist_high_domain = ['@hotmail.com','@icloud.com','@live.com','other']\n\ndef make_domain_type(dom) :\n    if dom in list_low_domain :\n        res = 'low_domain'\n    elif dom in list_med_domain :\n        res = 'med_domain'\n    elif dom in list_high_domain :\n        res = 'high_domain'\n        \n    return res\n\ndf_comb_train['domain_type'] = df_comb_train.apply(lambda x : make_domain_type(x['domain']), axis=1)\ndf_comb_test['domain_type'] = df_comb_test.apply(lambda x : make_domain_type(x['domain']), axis=1)","caceaf6a":"### Check age variable\nrcParams['figure.figsize'] = [10,5]\nplt.style.use('fivethirtyeight') \nsns.set_style('whitegrid')\ngrid = gridspec.GridSpec(1,1)\n\n# Plot prep\ndf_plot = df_comb_train.copy()\nlist_var = ['age']\ndf_open = df_plot[df_plot['open_flag']==1]\ndf_not_open = df_plot[df_plot['open_flag']==0]\n\n# Prop\nfor i,var in enumerate(list_var) :\n    ax = plt.subplot(grid[i])\n    sns.distplot(df_open[var], color='#EA4335', ax=ax, label='Open', hist=False)\n    sns.distplot(df_not_open[var], color='#4285F4', ax=ax, kde_kws={'alpha':0.5}, label='Not Open', hist=False)\n    plt.legend()\n\n# Additional cosmetics\nplt.plot([30,30],[0,0.06], '--', color='#6a737b')\nplt.text(x=32,y=0.057, s='Age 30')\nplt.title('Distribution of age', fontsize=20, fontname='Monospace', fontweight=\"bold\")\nplt.tight_layout() ;","a019e099":"### Use PPS Score to see which variable have predictive power to impute \"age\"\nimport ppscore as pps\npps.predictors(df_comb_train.dropna(subset=['age']), \"age\")","179b732c":"### Make \"age_class\"\ndef make_age_class(dataset) :\n    df = dataset.copy()\n    \n    # For NaN values\n    df['age_class'] = df['age'].isna()\n    df['age_class'] = df['age_class'].map({True:'Unknown',False:'<>'})\n    \n    # Make class for '>=30' and '<30' age\n    df.loc[df['age']>=30, 'age_class'] = '>=30'\n    df.loc[df['age']<30, 'age_class'] = '<30'\n    \n    return df\n\ndf_comb_train = make_age_class(df_comb_train)\ndf_comb_test = make_age_class(df_comb_test)","d03cbf89":"### Correlation of open, login, and checkout day with target\nrcParams['figure.figsize'] = [15,10]\nplt.style.use('fivethirtyeight') \nsns.set_style('whitegrid')\n\n# Data Prep\nlist_col = pd.Series(df_comb_train.columns)\nbool_open = list_col.str.contains('open')\nbool_login = list_col.str.contains('login')\nbool_checkout = list_col.str.contains('checkout')\ncol_to_plot = list(list_col[bool_login]) + list(list_col[bool_checkout]) + list(list_col[bool_open])\n\ndf_plot = df_comb_train[col_to_plot]\n\ndf_plot = df_plot[df_plot['last_open_day'].str.isnumeric()]\ndf_plot = df_plot[df_plot['last_login_day'].str.isnumeric()]\ndf_plot = df_plot[df_plot['last_checkout_day'].str.isnumeric()]\n\ndf_plot['last_open_day'] = df_plot['last_open_day'].astype('int')\ndf_plot['last_login_day'] = df_plot['last_login_day'].astype('int')\ndf_plot['last_checkout_day'] = df_plot['last_checkout_day'].astype('int')\ncorr = df_plot.corr()\n\n# Plot Prep\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Plot\nsns.heatmap(corr, square =True, vmin=-1, vmax=1, linewidths=0.1, annot=True, fmt='.2f', mask=mask, cmap='Pastel1_r') ;\nplt.title('Correlation of open, login, and checkout') ; ","a33e46fd":"### PCA for last n days variable\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=1)\n\nlist_open_var = ['open_count_last_10_days', 'open_count_last_30_days', 'open_count_last_60_days']\nlist_login_var = ['login_count_last_10_days', 'login_count_last_30_days', 'login_count_last_60_days']\nlist_checkout_var = ['checkout_count_last_10_days', 'checkout_count_last_30_days', 'checkout_count_last_60_days']\ndict_var = {'open':list_open_var, 'login':list_login_var, 'checkout':list_checkout_var}\n\n# Do PCA\nfor name,var in dict_var.items() :\n    \n    # Fit PCA\n    pca.fit(df_comb_train[var])\n    print(name,':',pca.explained_variance_ratio_)\n    \n    # Make new var\n    df_comb_train[name+'_count'] = pca.transform(df_comb_train[var])\n    df_comb_test[name+'_count'] = pca.transform(df_comb_test[var])\n","aac9b779":"### Make new variable using division operator\ndf_comb_train['open_per_login'] = df_comb_train['open_count'] \/ df_comb_train['login_count']\ndf_comb_train.loc[df_comb_train['login_count']==0, 'open_per_login'] = 0\n\ndf_comb_train['open_per_checkout'] = df_comb_train['open_count'] \/ df_comb_train['checkout_count']\ndf_comb_train.loc[df_comb_train['checkout_count']==0, 'open_per_checkout'] = 0\n\ndf_comb_test['open_per_login'] = df_comb_test['open_count'] \/ df_comb_test['login_count']\ndf_comb_test.loc[df_comb_test['login_count']==0, 'open_per_login'] = 0\n\ndf_comb_test['open_per_checkout'] = df_comb_test['open_count'] \/ df_comb_test['checkout_count']\ndf_comb_test.loc[df_comb_test['checkout_count']==0, 'open_per_checkout'] = 0","9c7100c2":"### Check new variable disribution\nrcParams['figure.figsize'] = [10,8]\nplt.style.use('fivethirtyeight') \nsns.set_style('whitegrid')\ngrid = gridspec.GridSpec(3,2)\n\n# Plot prep\ndf_plot = df_comb_train.copy()\nvar_to_plot = ['open_count','login_count','checkout_count','open_per_login','open_per_checkout']\ndf_open = df_plot[df_plot['open_flag']==1]\ndf_not_open = df_plot[df_plot['open_flag']==0]\n\n# Prop\nfor i,var in enumerate(var_to_plot) :\n    ax = plt.subplot(grid[i])\n    sns.distplot(df_open[var], color='#EA4335', ax=ax, label='Open', hist=False)\n    sns.distplot(df_not_open[var], color='#4285F4', ax=ax, kde_kws={'alpha':0.5, 'bw': 0.1}, label='Not Open', hist=False)\n    plt.legend()\n\n# Additional cosmetics\nplt.tight_layout() ;","7a426abe":"### Make variable that represent if the user have ever open, check, checkout\ndef make_var_check(var):\n    if var.isnumeric():\n        return 'Yes'\n    else:\n        return 'No'\n    \ndf_comb_train['last_open_check'] = df_comb_train.apply(lambda x : make_var_check(x['last_open_day']), axis=1)\ndf_comb_train['last_login_check'] = df_comb_train.apply(lambda x : make_var_check(x['last_login_day']), axis=1)\ndf_comb_train['last_checkout_check'] = df_comb_train.apply(lambda x : make_var_check(x['last_checkout_day']), axis=1)\n\ndf_comb_test['last_open_check'] = df_comb_test.apply(lambda x : make_var_check(x['last_open_day']), axis=1)\ndf_comb_test['last_login_check'] = df_comb_test.apply(lambda x : make_var_check(x['last_login_day']), axis=1)\ndf_comb_test['last_checkout_check'] = df_comb_test.apply(lambda x : make_var_check(x['last_checkout_day']), axis=1)","b128562c":"### Check the distribution of country\nrcParams['figure.figsize'] = [15,5]\nplt.style.use('fivethirtyeight') \nsns.set_style('whitegrid')\ngrid = gridspec.GridSpec(1,1)\n\n# Dataset prep\ndf_plot = df_comb_train.copy()\n\n# Plot prep\nline_color = '#EA4335'\nbar_color = '#4285F4'\ntext_size = 12\ny2_label = 'open_flag_rate'\nlist_var = ['country_code']\n\n# Plot\nfor i,var in enumerate(list_var) :\n    ax = plt.subplot(grid[i])\n    count_pcg_plot(df_plot, var, 'open_flag', ax, bar_color, line_color, text_size, y2_label)\n    \nplt.tight_layout()","ba102f44":"### Get variable from date\ndf_comb_train['grass_date'] = pd.to_datetime(df_comb_train['grass_date'])\ndf_comb_test['grass_date'] = pd.to_datetime(df_comb_test['grass_date'])\n\ndf_comb_train['day'] = df_comb_train['grass_date'].dt.day.astype('category')\ndf_comb_train['dayofweek'] = df_comb_train['grass_date'].dt.dayofweek.astype('category')\ndf_comb_train['month'] = df_comb_train['grass_date'].dt.month.astype('category')\n\ndf_comb_test['day'] = df_comb_test['grass_date'].dt.day.astype('category')\ndf_comb_test['dayofweek'] = df_comb_test['grass_date'].dt.dayofweek.astype('category')\ndf_comb_test['month'] = df_comb_test['grass_date'].dt.month.astype('category')","c2026bfc":"### Check the distribution of attr var\nrcParams['figure.figsize'] = [15,15]\nplt.style.use('fivethirtyeight') \nsns.set_style('whitegrid')\ngrid = gridspec.GridSpec(3,1)\n\n# Dataset prep\ndf_plot = df_comb_train.copy()\n\n# Plot prep\nline_color = '#EA4335'\nbar_color = '#4285F4'\ntext_size = 12\ny2_label = 'open_flag_rate'\nlist_var = ['day','dayofweek','month']\n\n# Plot\nfor i,var in enumerate(list_var) :\n    ax = plt.subplot(grid[i])\n    count_pcg_plot(df_plot, var, 'open_flag', ax, bar_color, line_color, text_size, y2_label)\n    \nplt.tight_layout()","109c1be8":"### Change day into numeric\ndf_comb_train['day'] = df_comb_train['day'].astype('int')\ndf_comb_test['day'] = df_comb_test['day'].astype('int')","090819fc":"### Make mean encoding subject_line_length\ndict_encode = df_comb_train.groupby('subject_line_length').mean()['open_flag'].to_dict()\n\ndf_comb_train['subject_line_length_encoded'] = df_comb_train['subject_line_length'].map(dict_encode)\n\ndf_comb_test['subject_line_length_encoded'] = df_comb_test['subject_line_length'].map(dict_encode)\ndf_comb_test['subject_line_length_encoded'] = df_comb_test['subject_line_length_encoded'].fillna(0)","92fa7c65":"# Chi-square test dari variabel kategorikal terhadap variabel respon\ndef chi_square_test(dfa, var1, var2) :\n    # Membuat contingency table\n    df = pd.crosstab(dfa[var1], dfa[var2], margins=False)\n\n    # Menghitung nilai statistiknya\n    from scipy.stats import chi2_contingency, chi2\n    stat, p, dof, expected = chi2_contingency(observed=df\n                                         ,correction=True #Jika dof=1, maka digunakan Yates Correction (?)\n                                         ,lambda_=None #Untuk mengganti Pearson Chi-Square menjadi Cressie-Read Divergence)\n                                          )\n\n    # Interpretasi nilai statistik\n    prob = 0.95\n    critical = chi2.ppf(prob, dof)\n\n    if abs(stat) >= critical:\n        print(var1,': Dependent')\n    else:\n        print(var1,': Independent')   \n    \n# Lakukan tes chi square untuk tiap variabel kategorikal\ncat_var = ['country_code','last_open_check','last_login_check','last_checkout_check','attr_1',\n           'attr_3','domain_type','age_class','dayofweek','month']\nfor var in cat_var :\n    chi_square_test(df_comb_train, var, 'open_flag')","22aad165":"### Initialize h2o\nimport h2o\nh2o.init()","0c78cf15":"### Define predictor and response\nX_feat = ['country_code','subject_line_length_encoded','attr_1','attr_3','domain_type','age_class',\n     'open_count_last_10_days','open_count_last_30_days','open_count_last_60_days',\n     'open_count','login_count','checkout_count','open_per_login','open_per_checkout',\n     'last_open_check','last_login_check','last_checkout_check','day','dayofweek','month']\n\nX_ori = ['country_code','subject_line_length','last_open_day','last_login_day','last_checkout_day',\n     'open_count_last_10_days','open_count_last_30_days','open_count_last_60_days',\n     'login_count_last_10_days','login_count_last_30_days','login_count_last_60_days',\n     'checkout_count_last_10_days','checkout_count_last_30_days','checkout_count_last_60_days',\n     'attr_1','attr_3','age_class','domain','day','dayofweek','month']\n\nX = list(set(X_feat + X_ori))\n\nY = 'open_flag'\n\nlist_col = X + [Y]","e1305da7":"### Filter variable on dataset\ndf_comb_train = df_comb_train[list_col]\ndf_comb_test = df_comb_test[X]","086c60fc":"### Split dataframe\nfrom sklearn.model_selection import train_test_split\ntrain_data, val_data = train_test_split(df_comb_train, stratify=df_comb_train['open_flag'], test_size = 0.2, random_state=11)","6bb89e0a":"### Undersampling TomekLinks\nfrom imblearn.under_sampling import TomekLinks\ntl = TomekLinks(sampling_strategy='auto')\nvar_to_use = ['subject_line_length_encoded',\n              'open_count_last_10_days','open_count_last_30_days','open_count_last_60_days',\n              'open_count','login_count','checkout_count','open_per_login','open_per_checkout',\n              'day','dayofweek','month']\n\nx_res, y_res = tl.fit_sample(train_data[var_to_use].values, train_data[Y].values)\n\n# Ganti lagi jadi h2o frame\nprint('Target distribution before undersampling')\nprint(train_data[Y].value_counts() \/ len(train_data))\nprint('')\n\ntrain_data = train_data.iloc[tl.sample_indices_, :]\nprint('Target distribution after undersampling')\nprint(train_data[Y].value_counts() \/ len(train_data))\nprint('')\n","29d0ca44":"### Oversampling SMOTENC\nfrom imblearn.over_sampling import SMOTENC\n\ncat_idx = [0,2,3,4,5,14,15,16,18,19]\nsm = SMOTENC(categorical_features=cat_idx)\nx_res, y_res = sm.fit_resample(train_data[X], train_data[Y])\n\n# Ganti lagi jadi h2o frame\nprint('Target distribution before undersampling')\nprint(train_data[Y].value_counts() \/ len(train_data))\nprint('')\n\n# Ubah jadi dalam bentuk dataframe lagi\nx_res = pd.DataFrame(x_res, columns=X)\ny_res = pd.DataFrame(y_res, columns=[Y])\ntrain_data = pd.concat([x_res, y_res], axis=1)\nprint('Target distribution after undersampling')\nprint(train_data[Y].value_counts() \/ len(train_data))\nprint('')\n","6ae25f3e":"### Make H2O Frame\nh2o_train = h2o.H2OFrame(train_data[list_col])\nh2o_val = h2o.H2OFrame(val_data[list_col])\nh2o_test = h2o.H2OFrame(df_comb_test[X])","8d70c309":"print(X)","a612145d":"### Make categorical\nX_cat = ['country_code','attr_1','attr_3','domain_type','age_class',\n         'last_open_check','last_login_check','last_checkout_check',\n         'dayofweek','month','domain']\n\n# X_cat = ['country_code','attr_1','attr_3','domain','age_class',\n#          'dayofweek','month']\n\nfor var in X_cat :\n    h2o_train[var] = h2o_train[var].asfactor()\n    h2o_val[var] = h2o_val[var].asfactor()\n    h2o_test[var] = h2o_test[var].asfactor()\n    \nh2o_train[Y] = h2o_train[Y].asfactor()\nh2o_val[Y] = h2o_val[Y].asfactor()","4cca87df":"# Buat modelnya\nstart = time.time()\n\nfrom h2o.automl import H2OAutoML\nmodel_h2o = H2OAutoML(max_models=20\n                    ,max_runtime_secs=1800 #Waktu yang dibutuhkan sampai membuat stacked model\n                    ,nfolds=10\n                    ,balance_classes=True #Enabe pada kasus imbalance classification\n                    ,keep_cross_validation_predictions=True\n                    ,keep_cross_validation_fold_assignment=True\n                    ,stopping_metric='AUC'\n                    ,sort_metric='AUC'\n                    ,seed=11\n                    ,verbosity='info')\n\n# Train modelnya\nmodel_h2o.train(x=X, y=Y, training_frame=h2o_train, validation_frame=h2o_val)\n\n# Hitung metrics\nfrom sklearn.metrics import matthews_corrcoef\ntrain_auto = matthews_corrcoef(h2o_train[Y].as_data_frame(), model_h2o.predict(h2o_train)['predict'].as_data_frame())\nhold_auto = matthews_corrcoef(h2o_val[Y].as_data_frame(), model_h2o.predict(h2o_val)['predict'].as_data_frame())\n\n# Print result\nprint('Train metrics :',train_auto)\nprint('Holdout metrics :',hold_auto)\n\nend = time.time()\nprint('Time Used :',(end-start)\/60)","284d2c9e":"list_model = list(lb.as_data_frame()['model_id'])\nlist_model","b09200c2":"### Calculate all model Matthews Coeff\nlb = model_h2o.leaderboard\nlist_model = list(lb.as_data_frame()['model_id'])\nlist_met = []\n\nfor i in range(len(lb)) :\n    m = h2o.get_model(lb[i, 'model_id'])\n    model_name = list_model[i]\n    \n    # Hitung metrics\n    from sklearn.metrics import matthews_corrcoef\n    hold_auto = matthews_corrcoef(h2o_val[Y].as_data_frame(), m.predict(h2o_val)['predict'].as_data_frame())\n    list_met.append(hold_auto)\n    \ndf_result = pd.DataFrame({'model':list_model, 'Holdout score':list_met})\ndf_result.sort_values('Holdout score', ascending=False)","10f6abeb":"### Feature importance\nm = h2o.get_model(lb[1, 'model_id'])","6f2113b1":"# Hitung metrics\nfrom sklearn.metrics import matthews_corrcoef\ntrain_lgbm = matthews_corrcoef(h2o_train[Y].as_data_frame(), m.predict(h2o_train)['predict'].as_data_frame())\nhold_lgbm = matthews_corrcoef(h2o_val[Y].as_data_frame(), m.predict(h2o_val)['predict'].as_data_frame())\n\n# Print result\nprint('Train metrics :',train_lgbm)\nprint('Holdout metrics :',hold_lgbm)\n\nend = time.time()\nprint('Time Used :',(end-start)\/60)","065b2e1b":"### Make submission\npred = m.predict(h2o_test)['predict'].as_data_frame()\nsub = pd.read_csv('..\/input\/student-shopee-code-league-marketing-analytics\/sample_submission_0_1.csv')\nsub['open_flag'] = pred\n\nsub.to_csv('subs_final.csv', index=False)","38f4ed53":"### Initialize h2o\nimport h2o\nh2o.init()","6bac3170":"### Define predictor and response\nX = ['country_code','subject_line_length_encoded','attr_1','attr_3','domain_type','age_class',\n     'open_count_last_10_days','open_count_last_30_days','open_count_last_60_days',\n     'open_count','login_count','checkout_count','open_per_login','open_per_checkout',\n     'last_open_check','last_login_check','last_checkout_check','day','dayofweek','month']\n\n# X_ori = ['country_code','subject_line_length','last_open_day','last_login_day','last_checkout_day',\n#      'open_count_last_10_days','open_count_last_30_days','open_count_last_60_days',\n#      'login_count_last_10_days','login_count_last_30_days','login_count_last_60_days',\n#      'checkout_count_last_10_days','checkout_count_last_30_days','checkout_count_last_60_days',\n#      'attr_1','attr_3','age_class','domain','day','dayofweek','month']\n\nY = 'open_flag'\n\nlist_col = X + [Y]","bbbeee7f":"### Split dataframe\nfrom sklearn.model_selection import train_test_split\ntrain_data, val_data = train_test_split(df_comb_train, stratify=df_comb_train['open_flag'], test_size = 0.2, random_state=11)","d1c17b7a":"### Make H2O Frame\nh2o_train = h2o.H2OFrame(train_data[list_col])\nh2o_val = h2o.H2OFrame(val_data[list_col])\nh2o_test = h2o.H2OFrame(df_comb_test[X])","d01f864e":"### Make categorical\nX_cat = ['country_code','attr_1','attr_3','domain_type','age_class',\n         'last_open_check','last_login_check','last_checkout_check',\n         'dayofweek','month']\n\n# X_cat = ['country_code','attr_1','attr_3','domain','age_class',\n#          'dayofweek','month']\n\nfor var in X_cat :\n    h2o_train[var] = h2o_train[var].asfactor()\n    h2o_val[var] = h2o_val[var].asfactor()\n    h2o_test[var] = h2o_test[var].asfactor()\n    \nh2o_train[Y] = h2o_train[Y].asfactor()\nh2o_val[Y] = h2o_val[Y].asfactor()","c3ff50d1":"### Make all H2O baseline model\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nfrom h2o.estimators.random_forest import H2ORandomForestEstimator\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\nfrom h2o.estimators import H2OXGBoostEstimator\nfrom h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator\nimport time\n\ndef h2o_compare_models(df_train, df_test, X, Y) :\n    \n    start = time.time()\n    \n    # Initialize all model (Ganti family\/distributionnya)\n    glm = H2OGeneralizedLinearEstimator(family='binomial', nfolds=10, keep_cross_validation_predictions=True, fold_assignment='Modulo')\n    gbm = H2OGradientBoostingEstimator(distribution='bernoulli', nfolds=10, keep_cross_validation_predictions=True, fold_assignment='Modulo')\n    xgb = H2OXGBoostEstimator(distribution='bernoulli', nfolds=10, keep_cross_validation_predictions=True, fold_assignment='Modulo')\n    lgbm = H2OXGBoostEstimator(distribution='bernoulli', tree_method=\"hist\", grow_policy=\"lossguide\",\n                              nfolds=10, keep_cross_validation_predictions=True, fold_assignment='Modulo')\n    rf = H2ORandomForestEstimator(distribution='bernoulli', nfolds=10, keep_cross_validation_predictions=True, fold_assignment='Modulo')\n    ext = H2ORandomForestEstimator(distribution='bernoulli', histogram_type=\"Random\",\n                                  nfolds=10, keep_cross_validation_predictions=True, fold_assignment='Modulo')\n    \n    # Train model\n    glm.train(x=X, y=Y, training_frame=df_train)\n    gbm.train(x=X, y=Y, training_frame=df_train)\n    xgb.train(x=X, y=Y, training_frame=df_train)\n    lgbm.train(x=X, y=Y, training_frame=df_train)\n    rf.train(x=X, y=Y, training_frame=df_train)\n    ext.train(x=X, y=Y, training_frame=df_train)\n    \n    # Calculate train metrics (Bisa diganti)\n    from sklearn.metrics import matthews_corrcoef\n    train_glm = matthews_corrcoef(h2o_train[Y].as_data_frame(), glm.predict(h2o_train)['predict'].as_data_frame())\n    train_gbm = matthews_corrcoef(h2o_train[Y].as_data_frame(), gbm.predict(h2o_train)['predict'].as_data_frame())\n    train_xgb = matthews_corrcoef(h2o_train[Y].as_data_frame(), xgb.predict(h2o_train)['predict'].as_data_frame())\n    train_lgbm = matthews_corrcoef(h2o_train[Y].as_data_frame(), lgbm.predict(h2o_train)['predict'].as_data_frame())\n    train_rf = matthews_corrcoef(h2o_train[Y].as_data_frame(), rf.predict(h2o_train)['predict'].as_data_frame())\n    train_ext = matthews_corrcoef(h2o_train[Y].as_data_frame(), ext.predict(h2o_train)['predict'].as_data_frame())\n\n    # Calculate CV metrics for all model (Bisa diganti)\n    met_glm = matthews_corrcoef(h2o_train[Y].as_data_frame(), glm.cross_validation_holdout_predictions()['predict'].as_data_frame())\n    met_gbm = matthews_corrcoef(h2o_train[Y].as_data_frame(), gbm.cross_validation_holdout_predictions()['predict'].as_data_frame())\n    met_xgb = matthews_corrcoef(h2o_train[Y].as_data_frame(), xgb.cross_validation_holdout_predictions()['predict'].as_data_frame())\n    met_lgbm = matthews_corrcoef(h2o_train[Y].as_data_frame(), lgbm.cross_validation_holdout_predictions()['predict'].as_data_frame())\n    met_rf = matthews_corrcoef(h2o_train[Y].as_data_frame(), rf.cross_validation_holdout_predictions()['predict'].as_data_frame())\n    met_ext = matthews_corrcoef(h2o_train[Y].as_data_frame(), ext.cross_validation_holdout_predictions()['predict'].as_data_frame())\n    \n    # Calculate holdout metrics\n    from sklearn.metrics import matthews_corrcoef\n    hold_glm = matthews_corrcoef(h2o_val[Y].as_data_frame(), glm.predict(h2o_val)['predict'].as_data_frame())\n    hold_gbm = matthews_corrcoef(h2o_val[Y].as_data_frame(), gbm.predict(h2o_val)['predict'].as_data_frame())\n    hold_xgb = matthews_corrcoef(h2o_val[Y].as_data_frame(), xgb.predict(h2o_val)['predict'].as_data_frame())\n    hold_lgbm = matthews_corrcoef(h2o_val[Y].as_data_frame(), lgbm.predict(h2o_val)['predict'].as_data_frame())\n    hold_rf = matthews_corrcoef(h2o_val[Y].as_data_frame(), rf.predict(h2o_val)['predict'].as_data_frame())\n    hold_ext = matthews_corrcoef(h2o_val[Y].as_data_frame(), ext.predict(h2o_val)['predict'].as_data_frame())\n    \n    # Make result dataframe\n    result = pd.DataFrame({'Model':['GLM','GBM','XGB','LGBM','RF','ExtraTree'],\n                          'Train Metrics':[train_glm,train_gbm,train_xgb,train_lgbm,train_rf,train_ext],\n                          'CV Metrics':[met_glm,met_gbm,met_xgb,met_lgbm,met_rf,met_ext],\n                          'Holdout Metrics':[hold_glm,hold_gbm,hold_xgb,hold_lgbm,hold_rf,hold_ext]})\n    \n    end = time.time()\n    print('Time Used :',(end-start)\/60)\n    \n    return result.sort_values('Holdout Metrics') ","1210f099":"### Compare models\nres = h2o_compare_models(h2o_train, h2o_test, X, Y) \nres","bc31bab2":"### Search max depth\nfrom h2o.estimators import H2OXGBoostEstimator\nfrom h2o.grid.grid_search import H2OGridSearch\n\nstart = time.time()\nlgbm = H2OXGBoostEstimator(distribution='bernoulli', tree_method=\"hist\", grow_policy=\"lossguide\",\n                           nfolds=10, keep_cross_validation_predictions=True, fold_assignment='Modulo',\n                           ntrees=100, learn_rate=0.05,\n                           sample_rate = 0.8, col_sample_rate = 0.8, seed=11, score_tree_interval = 10,\n                           stopping_rounds = 5, stopping_metric = \"AUC\", stopping_tolerance = 1e-4)\n\n# LGBM Params\nlgbm_params = {'max_depth' : [3,5,7,9,11,13,15]}\n\n# Search criteria\nsearch_criteria = {'strategy': \"Cartesian\"}\n\n# Make grid model\nlgbm_grid = H2OGridSearch(model=lgbm,\n                          grid_id='best_lgbm_max_depths',\n                          hyper_params=lgbm_params,\n                          search_criteria=search_criteria)\n\n# Train model\nlgbm_grid.train(x=X, y=Y, training_frame=h2o_train, validation_frame=h2o_val)\n\n# Get best GLM\nlgbm_res = lgbm_grid.get_grid(sort_by='auc', decreasing=True)\nprint(lgbm_res)\n\nend = time.time()\nprint('Time Used :',(end-start)\/60)","ca5ef142":"### Tune Model - LGBM - RandomGridSearch\nfrom h2o.estimators import H2OXGBoostEstimator\nfrom h2o.grid.grid_search import H2OGridSearch\nfrom sklearn.metrics import log_loss\nstart = time.time()\nlgbm = H2OXGBoostEstimator(distribution='bernoulli', tree_method=\"hist\", grow_policy=\"lossguide\",\n                           nfolds=10, keep_cross_validation_predictions=True, fold_assignment='Modulo',\n                           ntrees=100, seed=11, score_tree_interval = 10,\n                           stopping_rounds = 5, stopping_metric = \"AUC\", stopping_tolerance = 1e-4)\n\n# LGBM Params\nlgbm_params = {'max_depth' : [7,9,11],\n                'sample_rate': [x\/100. for x in range(20,101)],\n                'col_sample_rate' : [x\/100. for x in range(20,101)],\n                'col_sample_rate_per_tree': [x\/100. for x in range(20,101)],\n                'min_split_improvement': [0,1e-8,1e-6,1e-4],\n              'reg_lambda':list(np.arange(0.5,1.05,0.05)),\n              'reg_alpha':list(np.arange(0.01,0.11,0.01)),\n             'learn_rate':list(np.arange(0.01,0.11,0.01))}\n\n# Search criteria\nsearch_criteria = {'strategy': \"RandomDiscrete\",\n                   'max_runtime_secs': 3600,  ## limit the runtime to 60 minutes\n                   'max_models': 10,  ## build no more than 100 models\n                   'seed' : 11,\n                   'stopping_rounds' : 5,\n                   'stopping_metric' : \"auc\",\n                   'stopping_tolerance': 1e-3\n                   }\n\n# Make grid model\nlgbm_grid = H2OGridSearch(model=lgbm,\n                          grid_id='best_lgbm_cmonman',\n                          hyper_params=lgbm_params,\n                          search_criteria=search_criteria)\n\n# Train model\nlgbm_grid.train(x=X, y=Y, training_frame=h2o_train, validation_frame=h2o_val)\n\n# Get best GLM\nlgbm_res = lgbm_grid.get_grid(sort_by='auc', decreasing=True)\nbest_lgbm = lgbm_res.models[0]","115bdec3":"### Feature importance\nbest_lgbm.varimp_plot()","d3898638":"# Hitung metrics\nfrom sklearn.metrics import matthews_corrcoef\ntrain_lgbm = matthews_corrcoef(h2o_train[Y].as_data_frame(), best_lgbm.predict(h2o_train)['predict'].as_data_frame())\nmet_lgbm = matthews_corrcoef(h2o_train[Y].as_data_frame(), best_lgbm.cross_validation_holdout_predictions()['predict'].as_data_frame())\nhold_lgbm = matthews_corrcoef(h2o_val[Y].as_data_frame(), best_lgbm.predict(h2o_val)['predict'].as_data_frame())\n\n# Print result\nprint('Train metrics :',train_lgbm)\nprint('CV metrics :',met_lgbm)\nprint('Holdout metrics :',hold_lgbm)\n\nend = time.time()\nprint('Time Used :',(end-start)\/60)","9ecf05b6":"### Make submission\npred = best_lgbm.predict(h2o_test)['predict'].as_data_frame()\nsub = pd.read_csv('..\/input\/student-shopee-code-league-marketing-analytics\/sample_submission_0_1.csv')\nsub['open_flag'] = pred\n\nsub.to_csv('subs_lgbm_gridsearch.csv', index=False)","68c506cf":"Seems like all categorial variable can be used for modelling","14e7f756":"We can see that countyr_code == 4 have a high open_flag_rate, beside that nothing interesting","f3d035eb":"Okay since LGBM give us the best CV score lets use it and fine tuned it. I will use guideline provided by the H2O (https:\/\/github.com\/h2oai\/h2o-3\/blob\/master\/h2o-docs\/src\/product\/tutorials\/gbm\/gbmTuning.ipynb). First lets get the appropriate max depth","e8d315c1":"I got max depth 7,9,11 with AUC of 0.88. So I will use these value to tune the LGBM model","5060478b":"There are some trend in the day variable. The trend go down untul it readh day==11 and then it goes up. Because of that I will treat the day variable as numeric","5a31fe86":"# Quick EDA","96041e5b":"Imbalance classification","296a48c2":"Next lets make some variable from date and check the distribution","9b615ed0":"We can see that :\n- The variable last 10days, 30days, and 60days have high correlation on each var type (open, login, checkout) so they basically give the same information. I will use PCA to reduce each 3 variable into 1 variable\n- Only open variable have decent correlation with target variable (around 0.5) so I will use all these varaible. Beside that maybe I will make a variable such as 'open per login' or 'open per checkout' to boost the correlation of login and checkout variable","98cc6c30":"# Modelling","f21b7e8d":"# Modelling","901772a2":"6 out of 10 of the most importance variable in the model are related to \"open\". ","4a291bf5":"Seems like younger people (age < 30) tend to not open the email and older people (age > 30) tend to open the email. Before we make variable based on this information, we need to impute the missing value on variable \"age\" first. For this i will try to use the **PPS Score** introduced by Florian Westchoreck in this amazing medium article (https:\/\/towardsdatascience.com\/rip-correlation-introducing-the-predictive-power-score-3d90808b9598).\n\nBasically this PPS Score is the cross-validated metrics using DecisionTree. The metrics used to evaluate the Decision Tree is based on the \"y\" variable","5b8ac190":"Seems there are no variable that can predict \"age\" well. So we just treat the missing value as a new class. Next I will make new variable \"age_class\"","f17edc35":"Seems like only \"open_count\" variable have distinguishable distribution based on target variable, will keep this in mind. Next I will handle the variable last open, login and checkout. Since these variable have a categorial value \"Never\" I will just make new binary variable called \"check\" that give us \"Yes\" if the user for example have login before and \"No' if its not ","a43fdea5":"We can group the domain into 3 groups based on the default rate :\n1. Low Open Domain -> open_flag_rate < 20\n2. Med Open Domain -> open_flag_rate < 27.5\n3. High Open Domain -> open_flag_rate > 27.5","8978873f":"Based on the plot above :\n- attr_1 is dominated by class 1, and there are clear difference of open_flag_rate between class 0 and 1. From here it safe to assume that NaN value from attr_1 can be filled by 1 because there are no significant difference between NaN and not NaN based on open_flag_rate\n- attr_2 is domianted also by class 1, but there are no clear difference of open_flag_rate so I will just exclude this variable\n- No interesting insight from attr_3\n","86d503a5":"So all the row with missing value on **attr_1** have missing value on **age**. Lets check the distribution of target variable based on this","45c77fa8":"I will like to calculate the significany of the categorical variable to the target variable. To do this I will use **Chi-square test**","049c2fb8":"There are no significant difference of open_flag_rate based on attr1_is_nan. Lets check it again based on categorical variable of dataset user","5e6d57bf":"Next lets check the distribution of countr code"}}