{"cell_type":{"a225f22f":"code","57fbe7a6":"code","355b35e7":"code","dbf92284":"code","0e0f434d":"code","6e922220":"code","089bf682":"code","95d895a0":"code","178cdde1":"code","540b820b":"code","f1ebe610":"code","d92e7044":"code","c683e3ea":"code","7113987f":"code","e7749209":"code","a3870037":"markdown","905e5d57":"markdown","f34d9614":"markdown","691c6cb8":"markdown","ff6bfb19":"markdown","aa6c6133":"markdown","2387a2af":"markdown","8b5291d9":"markdown","a72d11ec":"markdown","ab25891e":"markdown"},"source":{"a225f22f":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n#grafs\nimport pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import Dependencies\n%matplotlib inline\n\n# Start Python Imports\nimport math, time, random, datetime\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n# Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n\n# Machine learning\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\n\n# Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')","57fbe7a6":"import pandas as pd\n#oasis_cross_sectional = pd.read_csv(\"..\/input\/mri-and-alzheimers\/oasis_cross-sectional.csv\")\ndt = pd.read_csv(\"..\/input\/mri-and-alzheimers\/oasis_longitudinal.csv\")","355b35e7":"dt.describe()","dbf92284":"dt.head(5)","0e0f434d":"import missingno as msno\nmsno.matrix(dt)","6e922220":"dt.isnull().sum()","089bf682":"#complete missing SES and MMSE with median\ndt['SES'].fillna(dt['SES'].median(), inplace = True)\ndt['MMSE'].fillna(dt['MMSE'].median(), inplace = True)","95d895a0":"#Seeing if the missing values are gone\ndt.isnull().sum()","178cdde1":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX = dt.copy()\nX_test_full = dt.copy()\ntest=dt.copy()\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['CDR'], inplace=True)\ny = X.CDR              \nX.drop(['CDR'], axis=1, inplace=True)\n   \n    \n    # Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n# Low cardinality means that the column contains a lot of \u201crepeats\u201d in its data range.\n# Examples of categorical variables are race, sex, age group, and educational level. \n# While the latter two variables may also be considered in a numerical manner by using exact values for age \n# and highest grade completed\n# nunique() function to find the number of unique values over the column axis. So when it finds over 10 uniqe \n# values and the cname is a \n# dtype 'object' which means Data type objects are useful for creating structured arrays. \n# A structured array is the one which contains different types of data.\n\n### one line meaning of above####\n## for cname in a dataframes column shall return a value to 'low_cardinality_cols' if there are more then 10 uniqe values\n## and the dtype shall be a object which is a structured array that can have different types of data (lik; int, float string ect.)\n\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n### for cname (every value, one at the time) in dataframe for columns return a value to 'numeric_cols' if the \n### dtype= int64 or float64. \n\n\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)","540b820b":"X_train.head()","f1ebe610":"y.head()","d92e7044":"#The code below label encode the target, and convert it to int. This removes the \n#problem of -  ValueError: Unknown label type: \u2018continuous\u2019\n\n# Simple example for beginers how to adress \"\"\nimport numpy as np\nfrom sklearn                        import metrics, svm\nfrom sklearn.linear_model           import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn import utils\n\n\nlab_enc = preprocessing.LabelEncoder() #label encodint\ntraining_scores_encoded = lab_enc.fit_transform(y_train) #change target for label encoding\nprint(training_scores_encoded)\nprint(utils.multiclass.type_of_target(y_train))\nprint(utils.multiclass.type_of_target(y_train.astype('int')))  #make y_trian a int\nprint(utils.multiclass.type_of_target(training_scores_encoded)) ","c683e3ea":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n#from xgboost import XGBRegressor\n\n\nmodel2 = RandomForestClassifier(n_estimators=150, max_depth=4, random_state=1)\nmodel = GradientBoostingClassifier(random_state=1)\nmodel3 = DecisionTreeClassifier(max_depth=3, random_state=1)\n#model=SGDClassifier(random_state=1)\n#model=ExtraTreesClassifier(random_state=1)\n#model = XGBRegressor()\n# Define the models\nmodel_1 = RandomForestClassifier(n_estimators=50, random_state=0)\nmodel_2 = RandomForestClassifier(n_estimators=100, random_state=0)\nmodel_3 = RandomForestClassifier(n_estimators=200, min_samples_split=20, random_state=0)\nmodel_4 = RandomForestClassifier(n_estimators=300, max_depth=6, random_state=1)\n\n\n\nmodel.fit(X_train, training_scores_encoded)\ny_predictions = model.predict(X_valid)\n\nprint('model accuracy score',model.score(X_valid,y_predictions))","7113987f":"model2.fit(X_train, training_scores_encoded)\nprint('model2 accuracy score',model2.score(X_valid,y_predictions))\nmodel.fit(X_train, training_scores_encoded)\nprint('model accuracy score',model.score(X_valid,y_predictions))\nmodel3.fit(X_train, training_scores_encoded)\nprint('model3 accuracy score',model3.score(X_valid,y_predictions))","e7749209":"model_1.fit(X_train, training_scores_encoded)\nprint('model_1 accuracy score',model_1.score(X_valid,y_predictions))\nmodel_2.fit(X_train, training_scores_encoded)\nprint('model_2 accuracy score',model_2.score(X_valid,y_predictions))\nmodel_3.fit(X_train, training_scores_encoded)\nprint('model_3 accuracy score',model_3.score(X_valid,y_predictions))\nmodel_4.fit(X_train, training_scores_encoded)\nprint('model_4 accuracy score',model_4.score(X_valid,y_predictions))","a3870037":"# Missing values","905e5d57":"# Data visualisering","f34d9614":"# Predictions\/ more models","691c6cb8":"![image.png](attachment:image.png)","ff6bfb19":"# Model","aa6c6133":"# Disclaimer! This kernel is only for educational purposes and made for fun therefor the content of the kernel should not be taken seriously.","2387a2af":"# Data importering","8b5291d9":"## removes ValueError: Unknown label type: \u2018continuous\u2019","a72d11ec":"training_scores_encoded is the new y_train","ab25891e":"# Data cleaning\/prep"}}