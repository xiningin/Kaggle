{"cell_type":{"212aaedf":"code","41983a51":"code","bde2c8f3":"code","e7bb48b7":"code","0735d335":"code","aa49674c":"code","1180700e":"code","cb6f1f24":"code","684679c6":"code","61208810":"code","be8c2f63":"code","415a6fb0":"code","6093f992":"code","2b61dd17":"code","5ffd15aa":"code","e6722458":"code","63658f8a":"markdown","216d1614":"markdown","e50686c6":"markdown","44dc7449":"markdown","87f1b906":"markdown"},"source":{"212aaedf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', None)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","41983a51":"df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\n\ndf.sample(5)","bde2c8f3":"test = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","e7bb48b7":"# Let's find shape of our data, So we have around 42000 images with 786 pixel\ndf.shape","0735d335":"import seaborn as sns\nimport matplotlib.pyplot as plt","aa49674c":"# Lets try to find out how sparse our dataset is, So we can see that data is eually distrubuted among all classes, So\n# We don't need to balance classes\n \nsns.histplot(data = df, x = 'label')","1180700e":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(df.drop('label', axis = 1), df['label'])","cb6f1f24":"x_train = x_train\/255.0\nx_test = x_test\/255.0","684679c6":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV","61208810":"import tensorflow\nfrom tensorflow import keras\n","be8c2f63":"x_train, x_test, y_train, y_test = train_test_split(df.drop('label', axis = 1), df['label'])\nx_train_valid, x_test_valid, y_train_valid, y_test_valid  = train_test_split(x_train, y_train)","415a6fb0":"x_train = x_train\/255.0\nx_test = x_test\/255.0\nx_train_valid = x_train_valid\/255.0\nx_test_valid = x_test_valid\/255.0","6093f992":"model = keras.models.Sequential([\n keras.layers.Dense(400, activation = 'relu', input_shape = x_train.shape[1:]),\n keras.layers.BatchNormalization(),\n keras.layers.Dense(200, activation=\"elu\", kernel_initializer=\"he_normal\"),\n keras.layers.BatchNormalization(),\n keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n keras.layers.BatchNormalization(),\n keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(\n    optimizer =  keras.optimizers.SGD(),\n    loss = keras.losses.sparse_categorical_crossentropy,\n    metrics=[keras.metrics.SparseCategoricalAccuracy()]\n)\n\nhistory = model.fit(x_train, y_train, epochs = 20, validation_data=(x_train_valid, y_train_valid))","2b61dd17":"test = test\/255.0","5ffd15aa":"pred = np.argmax(model.predict(test), axis = 1)","e6722458":"sub = pd.DataFrame()\nsub['ImageId'] = list(test.index)[1:] + [28000]\nsub['Label'] = pred\nsub.to_csv(r'\/kaggle\/working\/sub_tens.csv', index = False)","63658f8a":"### Tensorflow","216d1614":"**The classsification algorithm that we can use**  \n1) Logistic Regression \\\n2) SVC \\\n3) Addaboost \\\n4) SGDClassifier \\\n3) Descision Tree \\\n4) Random Forest \\\n5) Ensemble Methods \\\n6) BernolliNB \\\n7) PassiveaggressiveClassifier \\\n8) LGBM \\\n9) BaggingClassifier \\\n10) CalibratedClassifierCV","e50686c6":"\nAs our data is in the range 0-255, To scale our data into 0-1.  \nWe have to scale our dataset, So we have multiple ways to do that,  \nI will mention most used one  \n1) MinMaxScaler  \n$ \\frac {value - min(feature)}{max(feature) - min(feature)}$  \n\n2) StandardScaler  \n$ \\frac {value - mean}{standard_deviation} $\n\nSo we don't need to use this scaling function , Just devide by 255 will work in our case","44dc7449":"## Prepare dta to feed the ML algorithms","87f1b906":"## Classification Algorithms\n"}}