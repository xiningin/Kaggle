{"cell_type":{"4c8863d7":"code","b8943efe":"code","366c6cd9":"code","4dad8f78":"code","f2ed9c5e":"code","be932360":"code","d1098ad4":"code","850b624b":"code","987c3742":"code","830272dd":"code","17b957bd":"code","c9e73af5":"code","9d758af7":"code","ababe02b":"code","a365ca02":"code","5de4d521":"code","a6d87582":"code","a577c082":"code","5faf65ec":"code","59f057ee":"code","39f22802":"code","30a9fac9":"code","9afefa10":"code","3b169f96":"code","c2d6d7f0":"code","3f906fd9":"code","2edac99f":"code","996e3925":"code","ce3fd8ec":"code","e0339cd4":"code","e96613b7":"code","c16aabd3":"code","dbe98949":"code","bf89d342":"code","6e9c5fdf":"code","9700ac15":"code","ba1177e3":"code","c8496978":"code","1ad6852c":"code","78d923d1":"code","c79c593a":"code","a54db84d":"code","582d7f03":"code","f8cb20a4":"code","965ec53c":"code","6902c39b":"code","705a243d":"code","03ec7dfe":"code","4360211a":"code","5b18e634":"code","0460071f":"code","bed83ab5":"code","5f9e255e":"code","1fe7111f":"code","d74b28e9":"code","e8018328":"code","9311a6a4":"code","b84e8849":"code","1d405f93":"code","06256b8d":"code","02183569":"code","fe1abfcc":"code","246df364":"code","9b3c68b0":"code","219fda7c":"code","93087247":"code","423b0915":"code","c50782c1":"code","d0009882":"code","b9dbb7d8":"markdown","85823842":"markdown","8200db95":"markdown","1f9e89f1":"markdown","73bd1dc4":"markdown","250f14fd":"markdown","ba8ba99d":"markdown","7052b7e6":"markdown","089832c3":"markdown"},"source":{"4c8863d7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set(style= 'darkgrid')\nimport warnings\nwarnings.filterwarnings('ignore')","b8943efe":"train= pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","366c6cd9":"train.head()","4dad8f78":"test.head()","f2ed9c5e":"train.shape","be932360":"test.shape","d1098ad4":"np.product(train.shape)","850b624b":"np.product(test.shape)","987c3742":"train.info()","830272dd":"sns.heatmap(train.isnull(), cbar= False, yticklabels= False)","17b957bd":"#checking Avg age based on passanger class\ntrain.groupby('Pclass')['Age'].mean()","c9e73af5":"#checking Avg age based on gender\ntrain.groupby('Sex')['Age'].mean()","9d758af7":"# fill null value of age based on passanger class\ndef fillage(x):\n    Age= x[0]\n    Pclass= x[1]\n    if pd.isnull(Age):\n        if Pclass== 1:\n            return 38\n        elif Pclass==2:\n            return 30\n        else:\n            return 25\n    else:\n        return Age","ababe02b":"train['Age']= train[['Age', 'Pclass']].apply(fillage, axis=1)","a365ca02":"sns.heatmap(train.isnull(), cbar= False, yticklabels= False)","5de4d521":"# for the null value of cabin we are going to drop null value\ntrain.drop('Cabin', axis=1, inplace= True)","a6d87582":"train.isnull().sum()","a577c082":"train.dropna(inplace=True)","5faf65ec":"#we can do this by using either sklearn or creating a dummy columns\na=pd.get_dummies(train['Sex'], drop_first=True)\nb=pd.get_dummies(train['Embarked'], drop_first=True)","59f057ee":"train= pd.concat([train,a,b], axis=1)","39f22802":"train.head()","30a9fac9":"# we can convert Pclass into binary classification\nc= pd.get_dummies(train['Pclass'], drop_first=True)","9afefa10":"c","3b169f96":"train= pd.concat([train, c], axis=1)","c2d6d7f0":"train.head()","3f906fd9":"# droping irrelevent columns\ntrain.drop(['PassengerId','Pclass','Name','Sex','Ticket','Fare', 'Embarked'], axis=1, inplace=True)","2edac99f":"F= train.loc[train.male == 0]['Survived']\nF_rate= sum(F)\/len(F)\nprint(\"% of Female who survived:\", F_rate*100)","996e3925":"M= train.loc[train.male == 1]['Survived']\nM_rate= sum(M)\/len(M)\nprint(\"% of Male who survived:\", M_rate*100)","ce3fd8ec":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","e0339cd4":"test.isnull().sum()","e96613b7":"test.groupby('Pclass')['Age'].mean()","c16aabd3":"def fillagetest(x):\n    Age= x[0]\n    Pclass= x[1]\n    if pd.isnull(Age):\n        if Pclass==1:\n            return 41\n        elif Pclass==2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age","dbe98949":"test['Age']= test[['Age', 'Pclass']].apply(fillagetest, axis=1)","bf89d342":"test.isnull().sum()","6e9c5fdf":"test.drop('Cabin', axis=1, inplace= True)","9700ac15":"test.head()","ba1177e3":"test.isnull().sum()","c8496978":"test.dropna(inplace=True)","1ad6852c":"test.isnull().sum()","78d923d1":"p=pd.get_dummies(test['Sex'], drop_first=True)\nq=pd.get_dummies(test['Embarked'], drop_first=True)\nr= pd.get_dummies(test['Pclass'], drop_first=True)","c79c593a":"test= pd.concat([test, p,q,r], axis=1)","a54db84d":"test.head()","582d7f03":"test1= test.copy()","f8cb20a4":"# droping irrelevent columns\ntest.drop(['PassengerId','Pclass','Name','Sex','Ticket','Fare', 'Embarked'], axis=1, inplace=True)","965ec53c":"test","6902c39b":"X= train.drop('Survived', axis=1)\ny= train['Survived']","705a243d":"X","03ec7dfe":"y","4360211a":"# splitting of data into train and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = 0)","5b18e634":"X_train","0460071f":"y_train","bed83ab5":"# Training K-NN  model on the training set\nfrom sklearn.neighbors import KNeighborsClassifier\nknn= KNeighborsClassifier(n_neighbors=5, metric= 'minkowski', p=2)\nknn.fit(X_train, y_train)","5f9e255e":"#Making the confusion matrix and accuracy\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred2 = knn.predict(X_test)\nprint(confusion_matrix(y_test, y_pred2))\nprint(accuracy_score(y_test, y_pred2))","1fe7111f":"# Training SVC model on the training set\nfrom sklearn.svm import SVC\nsv= SVC(kernel='linear', random_state= 0)\nsv.fit(X_train, y_train)","d74b28e9":"#Making the confusion matrix and accuracy\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred3 = sv.predict(X_test)\nprint(confusion_matrix(y_test, y_pred3))\nprint(accuracy_score(y_test, y_pred3))","e8018328":"# Training DT Classification model on the training set\nfrom sklearn.tree import DecisionTreeClassifier\ndtc= DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\ndtc.fit(X_train, y_train)","9311a6a4":"#Making the confusion matrix and accuracy\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred4 = dtc.predict(X_test)\nprint(confusion_matrix(y_test, y_pred4))\nprint(accuracy_score(y_test, y_pred4))","b84e8849":"# Training Kernel SVM model on the training set\nfrom sklearn.svm import SVC\nksvm= SVC(kernel='rbf', random_state= 0)\nksvm.fit(X_train, y_train)","1d405f93":"# Making the confusion matrix and accuracy\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred5 = ksvm.predict(X_test)\nprint(confusion_matrix(y_test, y_pred5))\nprint(accuracy_score(y_test, y_pred5))","06256b8d":"#training RF Classification model on the training set\nfrom sklearn.ensemble import RandomForestClassifier\nrfc= RandomForestClassifier(n_estimators= 10, criterion= 'entropy', random_state= 0)\nrfc.fit(X_train, y_train)","02183569":"#Making the confusion matrix and accuracy\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred6 = rfc.predict(X_test)\nprint(confusion_matrix(y_test, y_pred6))\nprint(accuracy_score(y_test, y_pred6))","fe1abfcc":"#training Naive Bayes model on the training set \nfrom sklearn.naive_bayes import GaussianNB\nnb= GaussianNB()\nnb.fit(X_train, y_train)","246df364":"#Making the confusion matrix and accuracy\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred7 = nb.predict(X_test)\nprint(confusion_matrix(y_test, y_pred7))\nprint(accuracy_score(y_test, y_pred7))","9b3c68b0":"final_pred= rfc.predict(test)","219fda7c":"test1['Survived']=final_pred","93087247":"test1","423b0915":"output= test1[['male', 'Survived']]","c50782c1":"output= pd.DataFrame(output)","d0009882":"output.to_csv('gender_submission.csv')","b9dbb7d8":"#### **Random Forest classification**","85823842":"#### **DT Classification**","8200db95":"#### Percentage of men and women survived","1f9e89f1":"#### **Kernel SVM**","73bd1dc4":"****K-NN****","250f14fd":"#### **SVM**","ba8ba99d":"#### **Naive Bayes**","7052b7e6":"### Converting Categorical data into binary classification","089832c3":"###### Based on accuracy score Random forest model is best"}}