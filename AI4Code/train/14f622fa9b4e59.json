{"cell_type":{"4e0ebe80":"code","5ec718f9":"code","6056c5bd":"code","4b8f7660":"code","f518a973":"code","c1173c9d":"code","5c61d5db":"code","2362b52b":"code","93debdb5":"code","30782c9e":"code","bb5839ee":"code","798ccf64":"code","2c42ba58":"code","261de69b":"code","0d4c546e":"code","1d283e10":"code","a4b3c006":"code","dcf82299":"code","659f58c7":"code","d2de8057":"code","729ff5b9":"code","feca4a2e":"code","79f09edc":"code","c7ae0848":"code","3b33579f":"code","38bf76c7":"code","5b57827e":"code","1ec2873b":"code","6569d766":"code","1c9c59c8":"code","62a1d53f":"code","36e09d39":"code","6e665647":"code","6d34fba8":"code","618a6b7c":"code","4243c212":"code","7ec4e228":"code","7acf495e":"code","43599760":"code","17cabe3e":"markdown","85beb56a":"markdown","2560c2c7":"markdown","ea723e05":"markdown","a721f940":"markdown","53a5bd1f":"markdown","c75e4e46":"markdown","404d1d33":"markdown","0deee6c8":"markdown","e60e9bf5":"markdown","16937205":"markdown","3e38b08d":"markdown","bdb375c6":"markdown","443c17c4":"markdown","acf75bf0":"markdown","24e2f157":"markdown"},"source":{"4e0ebe80":"#importing required libraries\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom prettytable import PrettyTable\nimport numpy as np # linear algebra\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/habermans.csv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5ec718f9":"#importing CSV dataset into pandas dataframe and set column names\ndf = pd.read_csv(\"..\/input\/habermans-survival-data-set\/haberman.csv\",names=[\"Age\",\"Year\",\"A-Nodes\",\"Survival\"])#importing required libraries\n","6056c5bd":"#view first five rows of dataset\ndf.head()","4b8f7660":"#count of rows and columns in the dataset\ndf.shape","f518a973":"#overview of the dataset\ndf.describe()","c1173c9d":"#Datatype of each column\ndf.info()","5c61d5db":"#checking null values in data set\ndf.isnull()","2362b52b":"#check null values in data set, here axis=0 represents columns(No null values in dataset)\ndf.isnull().sum(axis=0)","93debdb5":"#list of unique values in column 3 with their count .\n# value_count() method returns the count of unique entries in that column.\na = df.iloc[:,3:].value_counts()\nprint(a)","30782c9e":"#Plot bar graph for survived patients vs Died.\ngraph=plt.figure(figsize=(5,5))\nplt.ylabel(\"Count\")\nplt.title(\"No. of Surived patients vs Died\")\nplt.bar([\"Survived\",\"Didn't Survive\"],a)\nplt.show()","bb5839ee":"#Age distribution in dataset\nplt.hist(df[\"Age\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")\nplt.title(\"Age Distribution in the Dataset\")\nplt.show()","798ccf64":"#Age vs Number of A-Nodes scatter plot with their survival status\nfig, ax = plt.subplots()\nscatter = ax.scatter(df[\"Age\"],df[\"A-Nodes\"],c=df[\"Survival\"])\nlegend1 = ax.legend(*scatter.legend_elements(),loc=\"upper left\", title=\"Survival\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"No. of A-Nodes\")\nplt.title(\"Age vs No. of A-Nodes\")\n\nplt.show()","2c42ba58":"# Default heatmap\np1 = sns.heatmap(df)\n","261de69b":"#Finding correlation among attributes\ncorr = df.corr()\nsns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns)","0d4c546e":"#pairplot for finding correlations between attributes\nsns.set(style=\"ticks\", color_codes=False)\ng = sns.pairplot(df)\nplt.show()","1d283e10":"#Drop year coloum\ndf = df.drop(\"Year\",axis=1)","a4b3c006":"df.head()","dcf82299":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.utils import shuffle","659f58c7":"X = df.iloc[:,:2]\ny = df[\"Survival\"]","d2de8057":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)","729ff5b9":"# Training & testing the model and printing the accuracy\nclf = DecisionTreeClassifier(criterion=\"entropy\",max_depth=2) \nclf = clf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\nacc_1a = metrics.accuracy_score(y_test,y_pred)\npre_1a = metrics.precision_score(y_test,y_pred)\nprint(\"Accuracy:\", acc_1a, pre_1a)","feca4a2e":"#Using the loc() function, we can access the data values fitted in the particular row or\n#column based on the index value passed to the function.\n# randomly sample 81 rows from the dataframe\ndf_1 = df.loc[df[\"Survival\"]==1]\ndf_1 = df_1.sample(n=81)\ndf_2 = df.loc[df[\"Survival\"]==2]\nframes = [df_1,df_2]\nb_df = pd.concat(frames)\nprint(b_df.head())\nb_df = shuffle(b_df)\n","79f09edc":"b_df.head()","c7ae0848":"#reseting the indicies\nb_df = b_df.reset_index(inplace=False)\nb_df = b_df.drop(\"index\",axis=1)\nb_df.head()","3b33579f":"#splitting X(independent) and y(dependent) variables\nXb = b_df.iloc[:,:2]\nyb = b_df[\"Survival\"]\nX_trainb, X_testb, y_trainb, y_testb = train_test_split(Xb, yb, test_size=0.3, random_state=1)","38bf76c7":"# Training & testing the model and printing the accuracy\nclf = DecisionTreeClassifier(criterion=\"entropy\",max_depth=2) \nclf = clf.fit(X_trainb,y_trainb)\ny_pred = clf.predict(X_testb)\nacc_1b = metrics.accuracy_score(y_testb,y_pred)\npre_1b = metrics.precision_score(y_testb,y_pred)\nprint(\"Accuracy:\",acc_1b,pre_1b)","5b57827e":"#pretty table\nmyTable = PrettyTable([\"Algorithm\", \"Accuracy (Imbalanced Dataset)\", \"Accuracy (Balanced Dataset)\"])\nmyTable.add_row([\"Decision Tree\", acc_1a, acc_1b])\nprint(myTable)","1ec2873b":"from sklearn.ensemble import RandomForestClassifier\nclf=RandomForestClassifier(n_estimators=16) # creating a RF classifier. n_estimators : This is the number of trees you want to build\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test) # applying trained classifer to test\nacc_2a = metrics.accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\",acc_2a)","6569d766":"clf=RandomForestClassifier(n_estimators=20)\nclf.fit(X_trainb,y_trainb)\ny_pred=clf.predict(X_testb)\nacc_2b = metrics.accuracy_score(y_testb, y_pred)\nprint(\"Accuracy:\",acc_2b)","1c9c59c8":"myTable.add_row([\"Random Forest\", acc_2a, acc_2b])\nprint(myTable)","62a1d53f":"from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=7)\nmodel.fit(X_train,y_train)\npredicted= model.predict(X_test)\nacc_3a = metrics.accuracy_score(y_test, predicted)\nprint(\"Accuracy:\",acc_3a)","36e09d39":"model = KNeighborsClassifier(n_neighbors=5)\nmodel.fit(X_trainb,y_trainb)\npredicted= model.predict(X_testb)\nacc_3b = metrics.accuracy_score(y_testb, predicted)\nprint(\"Accuracy:\",acc_3b)","6e665647":"myTable.add_row([\"KNN\", acc_3a, acc_3b])\nprint(myTable)","6d34fba8":"from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(X_train,y_train)\npredicted = model.predict(X_test)\nacc_4a = metrics.accuracy_score(y_test, predicted)\nprint(\"Accuracy:\", acc_4a)","618a6b7c":"model = GaussianNB()\nmodel.fit(X_trainb,y_trainb)\npredicted = model.predict(X_testb)\nacc_4b = metrics.accuracy_score(y_testb, predicted)\nprint(\"Accuracy:\",acc_4b)","4243c212":"myTable.add_row([\"Naive Bayes\", acc_4a, acc_4b])\nprint(myTable)","7ec4e228":"from sklearn import svm\nclf = svm.SVC(kernel='poly')\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nacc_5a = metrics.accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\",acc_5a)","7acf495e":"clf = svm.SVC(kernel='poly')\nclf.fit(X_trainb, y_trainb)\ny_pred = clf.predict(X_testb)\nacc_5b = metrics.accuracy_score(y_testb, y_pred)\nprint(\"Accuracy:\",acc_5b)","43599760":"myTable.add_row([\"SVM\", acc_5a, acc_5b])\nprint(myTable)","17cabe3e":"From above scatter plot we get the idea that chances of survival increases with decrease in age and less number of A-nodes in a patient","85beb56a":"## 2a. Random Forest Algorithm on Imbalanced Dataset","2560c2c7":"### Habermans Dataset\n\nAbout dataset\nHaberman's Survival Data\nThe dataset contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago's Billings Hospital on the survival of patients who had undergone surgery for breast cancer.\n\nAttribute Information:\n1.Age of patient at time of operation (numerical)\n\n2.Patient's year of operation (year - 1900, numerical)\n\n3.Number of positive axillary nodes detected (numerical)\n\n4.Survival status (1 = the patient survived 5 years or longer, 2 = the patient died within 5 years, class attribute)\n\n#### OBJECTIVE:\nWe have to find out whether the patients will survive more than 5 years or not.\n\nClearly a problem of Binary Classification since the final outcome can be only two classes either yes or no.","ea723e05":"## 3a. KNN Algorithm on imbalanced Dataset","a721f940":"now splitting dataset into training and testing","53a5bd1f":"This bar graph shows that the dataset is imbalanced because of vast difference in ranges of survived vs died patients for binary classification","c75e4e46":"## 4b. Naive Bayes Algorithm on Balanced Dataset\u00b6","404d1d33":" entropy is a measure of the randomness in the information being processed. The higher the entropy, the harder it is to draw any conclusions from that information. ","0deee6c8":"## 3b. KNN Algorithm on Balanced Dataset","e60e9bf5":"## 4a. Naive Bayes Algorithm on Imbalanced Dataset","16937205":"## 1b. Decision Tree algorithm on balanced Dataset","3e38b08d":"## 5a. Support Vector Machine Algorithm on Imbalanced Dataset","bdb375c6":"## 1a. Decision Tree algorithm on Imbalanced Dataset","443c17c4":"preparing the balanced dataset and randomly shuffling\n","acf75bf0":"## 2b. Random Forest Algorithm on Balanced Dataset","24e2f157":"## 5b. Support Vector Machine Algorithm on Balanced Dataset"}}