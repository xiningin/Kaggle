{"cell_type":{"34c10168":"code","67498b43":"code","0cba0ac3":"code","b4ae36b9":"code","cd7cbf5d":"code","06f9d913":"code","652019be":"code","bef19501":"code","674b3418":"code","99c0ca8d":"code","d10b261f":"code","b806688a":"code","f991ecb6":"code","765808ba":"code","4b75d377":"code","78ff9e13":"code","3b6ce04d":"code","f4066db3":"code","21fc273f":"code","143d2f12":"code","90e33c4d":"code","e0ab4bb0":"code","3e11af17":"code","2026e1e1":"code","d5b7fb20":"code","9e28fcb8":"code","b1d51488":"code","2ed97d31":"code","87e9b862":"code","a87fe185":"code","bc70efea":"code","fe2907f3":"code","aac90949":"code","0be06034":"code","d28fafeb":"code","8a925621":"code","ad9c510a":"code","8aa88c5f":"markdown","3468fcfa":"markdown","8bf53d42":"markdown","ed0ac927":"markdown","16dda965":"markdown","a95cf625":"markdown","b61d40b2":"markdown","733d2d98":"markdown","76760dd8":"markdown","81794df9":"markdown"},"source":{"34c10168":"## import libraries\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","67498b43":"## load train test data sets\ndata_raw = pd.read_csv(\"\/kaggle\/input\/topicmodel\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/topicmodel\/test.csv\")","0cba0ac3":"## shape of datasets\ndata_raw.shape,test.shape","b4ae36b9":"data_raw.head(10)","cd7cbf5d":"data_raw['ABSTRACT'][10]","06f9d913":"## check for missing values\nmissing_values_check = data_raw.isnull().sum()\nprint(missing_values_check)","652019be":"# Comments with no label are considered to be clean comments.\n# Creating seperate column in dataframe to identify clean comments.\n\n# We use axis=1 to count row-wise and axis=0 to count column wise\n\nrowSums = data_raw.iloc[:,3:].sum(axis=1)\nclean_comments_count = (rowSums==0).sum(axis=0)\n\nprint(\"Total number of comments = \",len(data_raw))\nprint(\"Number of clean comments = \",clean_comments_count)\nprint(\"Number of comments with labels =\",(len(data_raw)-clean_comments_count))","bef19501":"### find different categories\ncategories = list(data_raw.columns.values)\ncategories = categories[3:]\nprint(categories)","674b3418":"# Calculating number of comments in each category\ncounts = []\nfor category in categories:\n    counts.append((category, data_raw[category].sum()))\ndf_stats = pd.DataFrame(counts, columns=['category', 'number of comments'])\ndf_stats","99c0ca8d":"### lets plot comments in each category\nsns.set(font_scale = 1)\nplt.figure(figsize=(15,8))\n\nax= sns.barplot(categories, data_raw.iloc[:,3:].sum().values)\n\nplt.title(\"Comments in each category\", fontsize=20)\nplt.ylabel('Number of comments', fontsize=15)\nplt.xlabel('Comment Type ', fontsize=15)\n\n#adding the text labels\nrects = ax.patches\nlabels = data_raw.iloc[:,3:].sum().values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom', fontsize=18)\n\nplt.show()","d10b261f":"## lets plot Comments having multiple labels\nrowSums = data_raw.iloc[:,3:].sum(axis=1)\nmultiLabel_counts = rowSums.value_counts()\nmultiLabel_counts = multiLabel_counts.iloc[1:]\n\nsns.set(font_scale = 1)\nplt.figure(figsize=(10,6))\n\nax = sns.barplot(multiLabel_counts.index, multiLabel_counts.values)\n\nplt.title(\"Comments having multiple labels \")\nplt.ylabel('Number of comments', fontsize=15)\nplt.xlabel('Number of labels', fontsize=15)\n\n#adding the text labels\nrects = ax.patches\nlabels = multiLabel_counts.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","b806688a":"#!pip install wordcloud\nfrom wordcloud import WordCloud,STOPWORDS\n\nplt.figure(figsize=(40,25))\n\n# Computer Science\nsubset = data_raw[data_raw['Computer Science']==1]\ntext = subset.TITLE.values\ncloud_Computer_Science = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\n\nplt.subplot(2, 3, 1)\nplt.axis('off')\nplt.title(\"Computer_Science\",fontsize=40)\nplt.imshow(cloud_Computer_Science)\n\n\n# Physics\nsubset = data_raw[data_raw.Physics==1]\ntext = subset.TITLE.values\ncloud_Physics = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\n\nplt.subplot(2, 3, 2)\nplt.axis('off')\nplt.title(\"Physics\",fontsize=40)\nplt.imshow(cloud_Physics)\n\n# Mathematics\nsubset = data_raw[data_raw.Mathematics==1]\ntext = subset.TITLE.values\ncloud_Mathematics = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\n\nplt.subplot(2, 3, 3)\nplt.axis('off')\nplt.title(\"Mathematics\",fontsize=40)\nplt.imshow(cloud_Mathematics)\n\n\n# Statistics\nsubset = data_raw[data_raw.Statistics==1]\ntext = subset.TITLE.values\ncloud_Statistics = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\n\nplt.subplot(2, 3, 4)\nplt.axis('off')\nplt.title(\"Statistics\",fontsize=40)\nplt.imshow(cloud_Statistics)\n\n# Quantitative Finance\nsubset = data_raw[data_raw['Quantitative Finance']==1]\ntext = subset.TITLE.values\ncloud_Quantitative_Finance = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\n\nplt.subplot(2, 3, 5)\nplt.axis('off')\nplt.title(\"Quantitative_Finance\",fontsize=40)\nplt.imshow(cloud_Quantitative_Finance)\n\n\n# Quantitative Biology\nsubset = data_raw[data_raw['Quantitative Biology']==1]\ntext = subset.TITLE.values\ncloud_Quantitative_Biology = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\n\nplt.subplot(2, 3, 6)\nplt.axis('off')\nplt.title(\"Quantitative_Biology\",fontsize=40)\nplt.imshow(cloud_Quantitative_Biology)\n\nplt.show()","f991ecb6":"#!pip install wordcloud\nfrom wordcloud import WordCloud,STOPWORDS\n\nplt.figure(figsize=(40,25))\n\n# Computer Science\nsubset = data_raw[data_raw['Computer Science']==1]\ntext = subset.ABSTRACT.values\ncloud_Computer_Science = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\n\nplt.subplot(2, 3, 1)\nplt.axis('off')\nplt.title(\"Computer_Science\",fontsize=40)\nplt.imshow(cloud_Computer_Science)\n\n\n# Physics\nsubset = data_raw[data_raw.Physics==1]\ntext = subset.ABSTRACT.values\ncloud_Physics = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\n\nplt.subplot(2, 3, 2)\nplt.axis('off')\nplt.title(\"Physics\",fontsize=40)\nplt.imshow(cloud_Physics)\n\n# Mathematics\nsubset = data_raw[data_raw.Mathematics==1]\ntext = subset.ABSTRACT.values\ncloud_Mathematics = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\n\nplt.subplot(2, 3, 3)\nplt.axis('off')\nplt.title(\"Mathematics\",fontsize=40)\nplt.imshow(cloud_Mathematics)\n\n\n# Statistics\nsubset = data_raw[data_raw.Statistics==1]\ntext = subset.ABSTRACT.values\ncloud_Statistics = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\n\nplt.subplot(2, 3, 4)\nplt.axis('off')\nplt.title(\"Statistics\",fontsize=40)\nplt.imshow(cloud_Statistics)\n\n# Quantitative Finance\nsubset = data_raw[data_raw['Quantitative Finance']==1]\ntext = subset.ABSTRACT.values\ncloud_Quantitative_Finance = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\n\nplt.subplot(2, 3, 5)\nplt.axis('off')\nplt.title(\"Quantitative_Finance\",fontsize=40)\nplt.imshow(cloud_Quantitative_Finance)\n\n\n# Quantitative Biology\nsubset = data_raw[data_raw['Quantitative Biology']==1]\ntext = subset.ABSTRACT.values\ncloud_Quantitative_Biology = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\n\nplt.subplot(2, 3, 6)\nplt.axis('off')\nplt.title(\"Quantitative_Biology\",fontsize=40)\nplt.imshow(cloud_Quantitative_Biology)\n\nplt.show()","765808ba":"### import nltk libraries for text processing\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import LancasterStemmer\nimport re\n\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","4b75d377":"### lets create functions to deal with texts cleaning\n\ndef cleanHtml(sentence):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', str(sentence))\n    return cleantext\n\n\ndef cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]',r' ',cleaned)\n    cleaned = cleaned.strip()\n    cleaned = cleaned.replace(\"\\n\",\" \")\n    return cleaned\n\n\ndef keepAlpha(sentence):\n    alpha_sent = \"\"\n    for word in sentence.split():\n        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n        alpha_sent += alpha_word\n        alpha_sent += \" \"\n    alpha_sent = alpha_sent.strip()\n    return alpha_sent","78ff9e13":"### apply data cleaning functions.\n\ndata_raw['ABSTRACT'] = data_raw['ABSTRACT'].str.lower()\ndata_raw['ABSTRACT'] = data_raw['ABSTRACT'].apply(cleanHtml)\n#data_raw['ABSTRACT'] = data_raw['ABSTRACT'].apply(cleanPunc)\ndata_raw['ABSTRACT'] = data_raw['ABSTRACT'].apply(keepAlpha)\n\n\ndata_raw['TITLE'] = data_raw['TITLE'].str.lower()\ndata_raw['TITLE'] = data_raw['TITLE'].apply(cleanHtml)\n#data_raw['TITLE'] = data_raw['TITLE'].apply(cleanPunc)\ndata_raw['TITLE'] = data_raw['TITLE'].apply(keepAlpha)\n\ndata_raw.head()","3b6ce04d":"### apply data cleaning functions\n\ntest['ABSTRACT'] = test['ABSTRACT'].str.lower()\ntest['ABSTRACT'] = test['ABSTRACT'].apply(cleanHtml)\n#test['ABSTRACT'] = test['ABSTRACT'].apply(cleanPunc)\ntest['ABSTRACT'] = test['ABSTRACT'].apply(keepAlpha)\n\n\ntest['TITLE'] = test['TITLE'].str.lower()\ntest['TITLE'] = test['TITLE'].apply(cleanHtml)\n#test['TITLE'] = test['TITLE'].apply(cleanPunc)\ntest['TITLE'] = test['TITLE'].apply(keepAlpha)\n\ntest.head()","f4066db3":"data_raw['ABSTRACT'][6]","21fc273f":"stop_words = set(stopwords.words('english'))\n\n### define few my own stop words\n\nstop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across',\n              'among','beside','however','yet','within','a', 'about', 'above', 'after', 'again', 'against', 'all', 'also', \n              'am', 'an', 'and','any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below',\n              'between', 'both', 'but', 'by', 'can', \"can't\", 'cannot', 'com', 'could', \"couldn't\", 'did',\n              \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'else', 'ever',\n              'few', 'for', 'from', 'further', 'get', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having',\n              'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how',\n              \"how's\", 'however', 'http', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it',\n              \"it's\", 'its', 'itself', 'just', 'k', \"let's\", 'like', 'me', 'more', 'most', \"mustn't\", 'my', 'myself',\n              'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'otherwise', 'ought', 'our', 'ours',\n              'ourselves', 'out', 'over', 'own', 'r', 'same', 'shall', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\",\n              'should', \"shouldn't\", 'since', 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs',\n              'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\",\n              \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\",\n              'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n              \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\",\n              'www', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves'])\n\nre_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\ndef removeStopWords(sentence):\n    global re_stop_words\n    return re_stop_words.sub(\" \", sentence)\n\n# additional cleaning\ndef clean_text(text):\n    \n    #Remove Unicode characters\n    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n    #remove double spaces \n    text = re.sub('\\s+', ' ',text) \n    return text\n\ndata_raw['ABSTRACT'] = data_raw['ABSTRACT'].apply(removeStopWords)\ndata_raw['TITLE'] = data_raw['TITLE'].apply(removeStopWords)\n\n#data_raw['TITLE'] = data_raw['TITLE'].apply(lambda x: clean_text(x))\n#data_raw['ABSTRACT'] = data_raw['ABSTRACT'].apply(lambda x: clean_text(x))\n\ndata_raw.head()","143d2f12":"## remove stop words from data\ntest['ABSTRACT'] = test['ABSTRACT'].apply(removeStopWords)\ntest['TITLE'] = test['TITLE'].apply(removeStopWords)\n\n#test['TITLE'] = test['TITLE'].apply(lambda x: clean_text(x))\n#test['ABSTRACT'] = test['ABSTRACT'].apply(lambda x: clean_text(x))\n\ntest.head()","90e33c4d":"data_raw['TITLE'][0]","e0ab4bb0":"### apply stemmer tried multiple stemmer like porter lancaster but snowball works better than others. did not try lemmatizer\n\nstemmer = SnowballStemmer(\"english\")\ndef stemming(sentence):\n    stemSentence = \"\"\n    for word in sentence.split():\n        stem = stemmer.stem(word)\n        stemSentence += stem\n        stemSentence += \" \"\n    stemSentence = stemSentence.strip()\n    return stemSentence\n\ndata_raw['ABSTRACT'] = data_raw['ABSTRACT'].apply(stemming)\ndata_raw['TITLE'] = data_raw['TITLE'].apply(stemming)\n\ndata_raw.head()","3e11af17":"## apply stemmer\ntest['ABSTRACT'] = test['ABSTRACT'].apply(stemming)\ntest['TITLE'] = test['TITLE'].apply(stemming)\ntest.head()","2026e1e1":"### just take a copy of test data\ntest_fin = test.copy()","d5b7fb20":"# from sklearn.model_selection import train_test_split\n\n# train, test = train_test_split(data_raw, random_state=42, test_size=0.30, shuffle=True)\n\n# print(train.shape)\n# print(test.shape)","9e28fcb8":"## lets merge texts from multiple columns into one column\ndata_raw['ABSTRACT'] = data_raw['TITLE'] + data_raw['ABSTRACT'] \ntest_fin['ABSTRACT'] = test_fin['TITLE'] + test_fin['ABSTRACT'] ","b1d51488":"data_raw['ABSTRACT'][10]","2ed97d31":"data_raw.head()","87e9b862":"# train_text = train['ABSTRACT']\n# test_text = test['ABSTRACT']\ntrain_text = data_raw['ABSTRACT']","a87fe185":"test_final = test_fin['ABSTRACT']","bc70efea":"from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\nvectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2',max_df=2.5)\n#vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2',sublinear_tf=True,stop_words=\"english\",token_pattern=r'\\w{1,}')\nvectorizer.fit(train_text)","fe2907f3":"x_train = vectorizer.transform(train_text)\n#y_train = train.drop(labels = ['ID','TITLE','ABSTRACT'], axis=1)\ny_train = data_raw.drop(labels = ['ID','TITLE','ABSTRACT'], axis=1)\n#x_test = vectorizer.transform(test_text)\n#y_test = test.drop(labels = ['ID','TITLE','ABSTRACT'], axis=1)","aac90949":"test_f = vectorizer.transform(test_final)","0be06034":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier","d28fafeb":"from IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown(string))\n    \n# Using pipeline for applying logistic regression and one vs rest classifier\n#LogReg_pipeline = Pipeline([\n#                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n#            ])\n\n# Using pipeline for applying logistic regression and one vs rest classifier\nLogReg_pipeline = Pipeline([\n                ('clf', OneVsRestClassifier(LinearSVC(class_weight='balanced',random_state=0,tol=1e-1,C=8.385), n_jobs=-1)),\n            ])\n\n\n# Using pipeline for applying logistic regression and one vs rest classifier\n#LogReg_pipeline = Pipeline([\n#                ('clf', OneVsRestClassifier(MultinomialNB(alpha=1e-2,fit_prior=True, class_prior=None), n_jobs=-1)),\n#            ])\n\n\nprintmd('**Processing {} comments...**'.format('Computer Science'))\n    \n    # Training logistic regression model on train data\nLogReg_pipeline.fit(x_train, data_raw['Computer Science'])\n    \n    # calculating test accuracy\npred_df = pd.DataFrame()\npred_df['Computer Science'] = LogReg_pipeline.predict(test_f)\n\nprintmd('**Processing {} comments...**'.format('Physics'))\n# Training logistic regression model on train data\nLogReg_pipeline.fit(x_train, data_raw['Physics'])\n# calculating test accuracy\n#pred_df = pd.DataFrame()\npred_df['Physics'] = LogReg_pipeline.predict(test_f)\n\nprintmd('**Processing {} comments...**'.format('Mathematics'))\n# Training logistic regression model on train data\nLogReg_pipeline.fit(x_train, data_raw['Mathematics'])\n# calculating test accuracy\n#pred_df = pd.DataFrame()\npred_df['Mathematics'] = LogReg_pipeline.predict(test_f)\n\nprintmd('**Processing {} comments...**'.format('Statistics'))\n# Training logistic regression model on train data\nLogReg_pipeline.fit(x_train, data_raw['Statistics'])\n# calculating test accuracy\n#pred_df = pd.DataFrame()\npred_df['Statistics'] = LogReg_pipeline.predict(test_f)\n\nprintmd('**Processing {} comments...**'.format('Quantitative Biology'))\n# Training logistic regression model on train data\nLogReg_pipeline.fit(x_train, data_raw['Quantitative Biology'])\n# calculating test accuracy\n#pred_df = pd.DataFrame()\npred_df['Quantitative Biology'] = LogReg_pipeline.predict(test_f)\n\nprintmd('**Processing {} comments...**'.format('Quantitative Finance'))\n# Training logistic regression model on train data\nLogReg_pipeline.fit(x_train, data_raw['Quantitative Finance'])\n# calculating test accuracy\n#pred_df = pd.DataFrame()\npred_df['Quantitative Finance'] = LogReg_pipeline.predict(test_f)\n    \n    #print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n    #print(\"\\n\")\nprint(pred_df.shape)\npred_df.head()","8a925621":"### save results in submission file\n\npred_df['ID'] = test_fin['ID'].copy()\npred_df = pred_df.set_index('ID')\n#pred_df.head()\npred_df.to_csv('submission.csv')","ad9c510a":"# # using classifier chains\n# from sklearn.multioutput import ClassifierChain\n# from sklearn.linear_model import LogisticRegression\n\n# #%%time\n\n# # initialize classifier chains multi-label classifier\n# classifier = ClassifierChain(LinearSVC(class_weight='balanced',random_state=0,tol=1e-2,C=8.385))\n\n# # Training logistic regression model on train data\n# classifier.fit(x_train, y_train)\n\n# # # predict\n# predictions = classifier.predict(test_f).astype(int)\n# predictions_df = pd.DataFrame(predictions)\n# predictions_df['ID'] = test_fin['ID'].copy()\n# predictions_df = predictions_df.set_index('ID')\n# predictions_df = predictions_df.rename(columns={0:'Computer Science',1:'Physics',2:'Mathematics',3:'Statistics',4:'Quantitative Biology',5:'Quantitative Finance'})\n# predictions_df.head()\n# predictions_df.to_csv('submission_chain.csv')","8aa88c5f":"#### What is Multilabel?\nMultilabel means each sample have multiple target labels. E.g. If you write some content on the quora, it automatically tags multiple topics to your content.\n\n#### What is text Classification?\nText Classification means a classification task with more than two classes, each label is mutually exclusive. The classification makes the assumption that each sample is assigned to one and only one label.\n\n#### Steps of the process:\n\n1. Make dataset or download the dataset\n2. Preprocess dataset\n3. Feature Extraction\n4. Train model","3468fcfa":"## Multi-Label Classification","8bf53d42":"### Multiple Binary Classifications - (One Vs Rest Classifier)","ed0ac927":"#### Stemming: \nIn linguistic morphology and information retrieval, stemming is the process of reducing inflected words to their word stem, base or root form\u2014generally a written word form. For example, if the word is \"run\", then the inverted algorithm might automatically generate the forms \"running\", \"runs\", \"runned\", and \"runly\".","16dda965":"#### Train model:\nThere are lots of algorithms in machine learning to train your model. To train our model we use OneVsRestClassifier and SVM algorithm for that.\n\n#### OneVsRestClassifier:\n\nAlso known as one-vs-all, this strategy consists of fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only n_classes classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy for multiclass classification and is a fair default choice.\n\nThis strategy can also be used for multi label learning, where a classifier is used to predict multiple labels for instance, by fitting on a 2-d matrix in which cell [i, j] is 1 if the sample I have label j and 0 otherwise.\nIn the multi label learning literature, OvR is also known as the binary relevance method.\n\n#### LinearSVC:\n\nSimilar to SVC with parameter kernel='linear', but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\nThis class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.","a95cf625":"### TF-IDF","b61d40b2":"### Classifier Chains","733d2d98":"#### Preprocess dataset:\nThere is a lot of effort needed in text classification for data preprocessing.\n\n#### Steps for preprocess:\n\n1. Tokenize content\n2. Remove stop words\n3. Remove Punctuation\n4. Apply Lemmatization\n5. Apply stemming","76760dd8":"## Topic Modeling for Research Articles - Multilabel Text Classification\n\nResearchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Tagging or topic modelling provides a way to give token of identification to research articles which facilitates recommendation and search process.\n\nGiven the abstract and title for a set of research articles, predict the topics for each article included in the test set. \n\nNote that a research article can possibly have more than 1 topic. The research article abstracts and titles are sourced from the following 6 topics: \n\n1. Computer Science\n\n2. Physics\n\n3. Mathematics\n\n4. Statistics\n\n5. Quantitative Biology\n\n6. Quantitative Finance\n\n### Please Upvote\/Comments in case you liked my notebook.","81794df9":"#### Feature Extraction:\nWhen the input data to an algorithm is too large to be processed and it is suspected to be redundant, then it can be transformed into a reduced set of features (also named a feature vector). Determining a subset of the initial features is called feature selection. The selected features are expected to contain the relevant information from the input data so that the desired task can be performed by using this reduced representation instead of the complete initial data.\n\nWe use the sklearn library for Feature Extraction. There is lots of algorithm for that. e.g. TfidfVectorizer , CountVectorizer etc.\n\nwe use TfidfVectorizer for this task.\n\n#### What is TF-IDF?\n\nTF-IDF score represents the relative importance of a term in the document and the entire corpus. TF-IDF score is composed of two terms: the first computes the normalized Term Frequency (TF), the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of documents in the corpus divided by the number of documents where the specific term appears.\n\nTF(t) = (Number of times term t appears in a document) \/ (Total number of terms in the document)\n\nIDF(t) = log_e(Total number of documents \/ Number of documents with term t in it)\n"}}