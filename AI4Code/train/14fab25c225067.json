{"cell_type":{"70c75507":"code","4ca223e3":"code","14f11ba1":"code","bd8de27a":"code","3b9806ca":"code","ea01b77a":"code","591d4504":"code","674054ab":"code","6333cbc9":"code","c760c382":"code","30dae174":"code","80a76802":"code","34333546":"code","eef9b7ed":"code","95de5a63":"code","b3530b4e":"code","4b8de99b":"code","bf1d2c0a":"code","2f795f40":"code","74611377":"code","2859d5ee":"code","26aa2127":"code","80c799d6":"code","0018251b":"markdown","917bfe2b":"markdown","5b0187f7":"markdown","6ac4e231":"markdown","94674b2b":"markdown","5aa4e688":"markdown","885eaaa4":"markdown"},"source":{"70c75507":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        break\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4ca223e3":"from PIL import Image\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom keras.preprocessing.image import ImageDataGenerator\n\nimport glob\n\nfrom keras.applications import Xception\nfrom keras.applications.xception import preprocess_input","14f11ba1":"im_frame = Image.open('..\/input\/expertclass\/Trainset\/Trainset\/Hoegaarden\/Hoegaarden_309.png')\nnp_frame = np.array(im_frame)\n\nplt.imshow(np_frame)","bd8de27a":"import fnmatch\nfrom tqdm import tqdm\n\ndef get_img_paths(root_paths, format_ = 'png'):\n    if root_paths.__class__ not in (list, tuple, set):\n        root_paths = [root_paths]\n    \n    paths = []\n    labels = []\n    for path in root_paths:\n        p, l = _get_img_paths(path, format_)\n        paths += p\n        labels += l\n    \n    paths_df = pd.DataFrame({'filename':paths, 'class':labels})\n    return paths_df\n\ndef _get_img_paths(root_path, format_ = 'png'):\n    paths = []\n    labels = []\n    for dirname, _, filenames in os.walk(root_path):\n        for filename in fnmatch.filter(filenames, f'*.{format_}'):\n            paths.append(os.path.join(dirname, filename))\n            labels.append(dirname.split('\/')[-1])\n    return paths, labels\n\ndef load_images(paths):\n    imgs = []\n    img_labels = []\n    for path in tqdm(paths):    \n        imgs.append(np.array(Image.open(path)))\n        img_labels.append(path.split('\/')[-2])\n    \n    return tuple(imgs), tuple(img_labels)","3b9806ca":"#load all data in order to make k-fold with entire data (train + test)\ntrain_path = '..\/input\/expertclass\/Trainset\/Trainset\/'\ntest_path = '..\/input\/expertclass\/Testset\/Testset\/'\nload_images_df = get_img_paths([train_path, test_path])\nload_images_df","ea01b77a":"def img_to_array(load_images_df,augmentation_factor = 2, augment = True):    \n    \n    if augment == False:\n        augmentation_factor = 1\n        train_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n    else:\n        train_datagen = ImageDataGenerator(\n            channel_shift_range = 10,\n            shear_range = 0.2,\n            zoom_range = 0.2,\n            horizontal_flip = False,\n            rotation_range = 45,\n            vertical_flip = False,\n            preprocessing_function = preprocess_input,    \n        )\n\n    batch_gen = train_datagen.flow_from_dataframe(\n        load_images_df,\n        #color_mode = 'grayscale',\n        target_size = (299,299),\n        shuffle = False,\n        class_mode = 'raw',\n        batch_size = len(load_images_df) + 1\n    )\n\n\n    X, y = [], []\n    for _ in range(augmentation_factor):\n        X_, y_ = batch_gen[0]\n        X.append(X_)\n        y.append(y_.flatten())\n\n    X, y = np.vstack(X), np.hstack(y)\n    return X, y","591d4504":"feature_extractor = Xception(\n    include_top=False,\n    weights=\"imagenet\",\n    input_tensor=None,\n    pooling='avg',\n    classifier_activation=\"softmax\",\n    input_shape = (299,299,3)\n)","674054ab":"X, y = img_to_array(load_images_df,augmentation_factor = 3, augment = False)","6333cbc9":"i = np.random.choice(range(len(X)))\nprint(y[i])\nplt.imshow(X[i])","c760c382":"X_features = feature_extractor.predict(X)\nX_features = X_features.reshape(X_features.shape[0], np.prod(X_features.shape[1:]))","30dae174":"import sklearn\nsklearn.__version__","80a76802":"from sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier, AdaBoostClassifier, BaggingClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.preprocessing import QuantileTransformer, StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom sklearn.metrics import log_loss\n\nimport matplotlib.pyplot as plt","34333546":"pca = PCA(1000).fit(X_features)","eef9b7ed":"plt.plot(np.arange(0, len(pca.explained_variance_ratio_)),pca.explained_variance_ratio_.cumsum())\nplt.title('cumulative explained variance vs n_components')\nprint(np.argmin(abs(pca.explained_variance_ratio_.cumsum() - 0.8)))\nprint(f'explained variance: {pca.explained_variance_ratio_.sum()}')\ndel pca","95de5a63":"dnn = MLPClassifier(\n    (64,), verbose = 0, validation_fraction = 0.2, tol = 1e-5, early_stopping = True, learning_rate = 'constant',n_iter_no_change = 15, learning_rate_init = 5e-3, alpha = 2e-4, max_iter = 200, batch_size = 'auto')\n\nfinal_estimator = BaggingClassifier(dnn, n_estimators = 50, max_samples = 1.0, max_features = 1.0)\n\nmodel_pipeline = Pipeline(\n    steps = [                \n        ('scaler2', StandardScaler()),        \n        ('model', final_estimator)\n    ]\n\n)","b3530b4e":"sample_weight = compute_sample_weight('balanced',y)\nmodel_pipeline.fit(X_features, y)","4b8de99b":"preds_path = '..\/input\/expertclass\/Prediction\/Prediction\/'\nX_pred, _ = img_to_array(get_img_paths(preds_path), augment = False)\nX_pred_features = feature_extractor.predict(X_pred)","bf1d2c0a":"train_proba_preds = model_pipeline.predict_proba(X_features)\ntrain_preds = model_pipeline.predict(X_features)\nlosses = np.array([log_loss(y[i:i+1],train_proba_preds[i:i+1], labels = model_pipeline.classes_) for i in range(len(y))])\n","2f795f40":"indexes = np.argsort(losses,)[::-1]","74611377":"n = 20\nfig,ax = plt.subplots(np.ceil(n\/5).astype(int),5)\nfor i in range(n):\n    ax[i\/\/5, i%5].imshow(X[indexes[i]])\n    ax[i\/\/5, i%5].set_title(y[indexes[i]] +'-' + train_preds[indexes[i]] +' '+ str(round(np.max(train_proba_preds[indexes[i]])*100, 2)) + '%')\nplt.subplots_adjust(left = -2, bottom = -3)\nprint('Greatest losses (\"True label\" - \"Predicted label\" \"predicted proba\")')","2859d5ee":"preds = model_pipeline.predict(X_pred_features)\nproba_preds = model_pipeline.predict_proba(X_pred_features)","26aa2127":"\nfig,ax = plt.subplots(8,5)\nfor i in range(X_pred.shape[0]):\n    ax[i\/\/5, i%5].imshow(X_pred[i])\n    ax[i\/\/5, i%5].set_title(preds[i] +' '+ str(round(np.max(proba_preds[i])*100, 2)) + '%')\nplt.subplots_adjust(left = -2, bottom = -3)","80c799d6":"name_mapper = {\n    'Budweiser':0,\n    'Hoegaarden':1,\n    'Leffe':2,\n    'Stella':3,\n    'Others':4    \n}\n\n#sort submissions\nids = (\n    get_img_paths(preds_path)['filename']\n    .apply(\n        lambda x: int((x.split('\/')[-1].split('.')[0])))\n    .values\n)\n\npd.DataFrame({'ID':ids,'Class':preds}).replace(name_mapper).set_index('ID').to_csv(f'submission_{pd.to_datetime(\"now\").date()}.csv')","0018251b":"# Loading functions","917bfe2b":"# Modelling","5b0187f7":"## Make predictions","6ac4e231":"# Lib Imports","94674b2b":"## Check Greatest losses","5aa4e688":"# Random Image Show","885eaaa4":"# Create Visual Features"}}