{"cell_type":{"d117bae6":"code","e9bb8885":"code","f67ef218":"code","9039166e":"code","509e8c5d":"code","0f2961c4":"code","c1db4f30":"code","ab9f77c5":"code","aea88bb0":"code","ed480980":"code","2fbf2820":"code","763da5f9":"code","d9520447":"code","042fcb89":"code","82e8ebed":"code","bf017702":"code","ae1d4864":"code","f597fc31":"code","4315eb25":"code","fd8fa8c3":"code","6e03860e":"code","ee73db5e":"code","037117f8":"code","3dc08b08":"code","611629de":"code","725d362b":"code","b3b236f0":"code","cbb5f8d7":"code","0531ac4f":"code","caf6baf7":"code","01025645":"code","c3863705":"code","1e9d9668":"code","9cec85ed":"code","802a1586":"code","4019c5d0":"code","8d6c1144":"code","fe754ef0":"code","8a580d96":"code","8dac4fd5":"code","f1e7ba4a":"markdown","479e0892":"markdown","a2f321ab":"markdown","9f97a61d":"markdown","af21a1a5":"markdown","76656565":"markdown","d5ccb9e7":"markdown","f556bd76":"markdown","a40beb88":"markdown","ada68c3d":"markdown","e4eec74d":"markdown","e9e1f430":"markdown","fc2ff3e2":"markdown","32b00fed":"markdown","2ec70647":"markdown","65ef4819":"markdown","4c710700":"markdown","5780241e":"markdown"},"source":{"d117bae6":"import numpy as np\nimport sklearn as sk","e9bb8885":"print (np.array(range(10)))\nprint (np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))","f67ef218":"import time\n\nn = 10**6\n\nstart = time.time()\nfoo = list(range(n))\nfor i in range(len(foo)):\n    foo[i] = foo[i] + 1\nend = time.time()\nprint (\"For loop:\", end - start)","9039166e":"# slightly better: list comprehensions\nstart = time.time()\nfoo = [elem + 1 for elem in foo]\nend = time.time()\nprint(end - start)","509e8c5d":"foo = np.array(range(n))\nstart = time.time()\n### vectorized code goes here\nend = time.time()\nprint(end - start) ","0f2961c4":"inds = foo < 10\nprint (foo[inds])\nprint (foo[~inds])","c1db4f30":"print (np.random.uniform(0, 1, size=5)) #10 draws from Uniform(0, 1)\nprint (np.random.normal(0, 100, size=5)) #10 draws from Normal(0, 100)\nprint (np.random.choice([\"A\", \"B\", \"C\", \"D\"], size=5)) #10 draws with replacement","ab9f77c5":"import pandas as pd\n\ndf = pd.DataFrame({\n        \"A\":list(range(10)),\n        \"B\":np.random.randn(10),\n        \"C\":7,\n        \"D\":[\"this is a string\" for i in range(10)]\n    })\n\ndf","aea88bb0":"df.iloc[3:7] #iloc for \"integer location\". Can use this for selecting rows of a dataframe.","ed480980":"df[\"B\"][[3, 4, 8]]","2fbf2820":"df[\"B\"][3]","763da5f9":"# Kaggle Competition: Predicting housing prices from data about the house\n# https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\ndata = pd.read_csv(\"..\/input\/train.csv\") \ndata.head()","d9520447":"# Features we'll look at:\n# OverallQual: Rates the overall material and finish of the house\n# OverallCond: Rates the overall condition of the house\n# GarageArea: Size of garage in square feet\n# YrSold: Year Sold (YYYY)\n# LotFrontage: Linear feet of street connected to property\n# LotArea: Lot size in square feet\n# YearBuilt: Original construction date\n\ndata_subset = data[[\"SalePrice\", \"OverallQual\", \"OverallCond\", \"GarageArea\", \n              \"YrSold\", \"LotArea\", \"LotFrontage\", \"YearBuilt\"]].copy()\ndata_subset.head()","042fcb89":"import matplotlib.pyplot as plt\n%matplotlib inline \n#^this is a magic line for IPython. Don't worry about it, it's not really that interesting.","82e8ebed":"plt.hist(data_subset[\"SalePrice\"], bins=100)\nplt.title(\"Marginal Distribution of Sale Price\")\nplt.xlabel(\"Sale Price (Dollars)\")\nplt.ylabel(\"Count\")\nplt.show()","bf017702":"x = np.log(data_subset[\"SalePrice\"])\nmu = np.mean(x)\nsigma = np.std(x)\nn = len(x)\nplt.hist(np.random.normal(loc=mu, scale=sigma, size=n), \n         alpha=0.5, \n         color = \"blue\",\n         bins=100)\nplt.hist(x, \n         alpha=0.5, \n         bins=100,\n        color=\"#229911\")\nplt.show()","ae1d4864":"data_subset[\"LogSalePrice\"] = np.log(data_subset[\"SalePrice\"])\ndata_subset.head()","f597fc31":"plt.scatter(data_subset[\"GarageArea\"], data_subset[\"LogSalePrice\"])\nplt.title(\"Garage Area X Sale Price\")\nplt.xlabel(\"Garage Area\")\nplt.show()","4315eb25":"# Quiz: \n# Add a column HasGarage which is True IFF the house has a garage\n","fd8fa8c3":"for col in data_subset.columns:\n    numnan = np.isnan(data_subset[col]).sum()\n    if numnan > 0:\n        print (col, numnan)","6e03860e":"# What to do with missing values?\n\n# 1. Ditch the feature entirely\ndata_feature_ditched = data_subset[[\"SalePrice\", \"OverallQual\", \"OverallCond\", \"GarageArea\", \n              \"YrSold\", \"LotArea\", \"YearBuilt\"]].copy()\nprint (data_feature_ditched.shape)\ndata_feature_ditched.head()","ee73db5e":"# 1 (Cont'd, Improved) \n\ndata_ditched = data_subset.drop(\"LotFrontage\", axis=1)\nprint (data_ditched.shape)\ndata_ditched.head()","037117f8":"# 2. Ditch rows where values are missing\n\n# np.isnan(data): returns a new dataframe of bools\n# np.sum(data, axis=1): returns the sum of ROWS of the input dataframe (axis = 0 would be columns)\ndata_clean = data_subset[~np.isnan(data_subset[\"LotFrontage\"])].copy()\nprint (data_clean.shape)\ndata_clean.head()","3dc08b08":"# 2 (cont'd)\n# More general and pandorable. Maybe I want to check NaNs for lots of rows at once!\n\n# data.dropna(how='any')    #to drop if any value in the row has a nan\n# data.dropna(how='all')    #to drop if all values in the row are nan\ndata_clean = data_subset.dropna(how=\"any\") #dropna returns a new dataframe\nprint (data_clean.shape)\ndata_clean.head()","611629de":"# 3. Impute missing values\n# Doing this manually is pretty awful tbh. \ndata_imputed = data_subset.copy()\ndata_imputed = data_imputed.fillna(data_imputed.mean())\ndata_imputed.head()","725d362b":"pd.isnull(data_subset[\"LotFrontage\"]).sum()","b3b236f0":"pd.isnull(data_imputed[\"LotFrontage\"]).sum()","cbb5f8d7":"df = pd.DataFrame(np.random.randn(10,3))\ndf","0531ac4f":"df2 = df\ndf2[0][0] = df[0][0] + 1\nprint (df2[0][0] == df[0][0]) #Modifying df2 modifies df","caf6baf7":"df2 = df.copy()\ndf2[0][0] = df[0][0] + 1\nprint (df2[0][0] == df[0][0]) #Fixed","01025645":"df3 = df[1]\ndf3[0] = df[1][0] + 1\ndf3[0] == df[1][0] #Oh noes","c3863705":"df3 = df[1].copy()\ndf3[0] = df[1][0] + 1\ndf3[0] == df[1][0] #fixed","1e9d9668":"from sklearn.model_selection import train_test_split\n\nfeatures = [\"OverallQual\", \"OverallCond\", \"GarageArea\", \n              \"YrSold\", \"LotArea\", \"LotFrontage\", \"YearBuilt\"]\nXTrain, XTest, yTrain, yTest = train_test_split(\n    data_imputed[features], \n    data_imputed[\"LogSalePrice\"], \n    test_size=100)\n# uppercase X denotes that X is a matrix, and lowercase y denotes a vector","9cec85ed":"#XTest, XVal, yTest, yVal = train_test_split(XTest, yTest, test_size=50)","802a1586":"from sklearn import linear_model \n\n# Initialize linear regression object\nmodel = linear_model.LinearRegression(fit_intercept=True)\n\n# fit your model to training data\nmodel.fit(XTrain, yTrain)\n\n# Make predictions using the testing set\nyPreds = model.predict(XTest)","4019c5d0":"from sklearn.metrics import r2_score\n\nprint (\"Test R-Squared:\", r2_score(y_true = yTest, y_pred = yPreds))\nprint (\"Training R-Squared:\", r2_score(y_true = yTrain, y_pred = model.predict(XTrain)))","8d6c1144":"# Root Mean Squared Error\nnp.sqrt(np.mean((yTest - yPreds)**2))","fe754ef0":"# Original basis\nnp.sqrt(np.mean((np.exp(yTest) - np.exp(yPreds))**2))","8a580d96":"for i in range(len(features)):\n    print (features[i], model.coef_[i])\n    \nprint (\"Intercept\", model.intercept_)","8dac4fd5":"plt.scatter(yPreds, yPreds - yTest)\nplt.xlabel(\"Predicted Log of Sale Price\")\nplt.ylabel(\"Residual\")\nplt.title(\"Residual Plot\")\nplt.show()","f1e7ba4a":"### Miscellaneous stuff\n\nRandom number generation is nice.","479e0892":"# QUIZ: Get a list\/array of all the odd elements of foo","a2f321ab":"So that model doesn't do very well - you can probably do better. We invite you to try other things - other variables, regression methods, etc - to improve your accuracy. Or try your hand at another competition you see here.\n\nHere are some strategies for approaching Kaggle competitions in general:\n\n* Explore your data plenty before you start to build a model on it - this can give you lots of insights that a model can\u2019t pick up. \n* Know domain knowledge about your data so you can do better feature engineering\n* Do data preprocessing - certain forms of data and how they\u2019re organized can make some data easier to learn. Things like normalizing your data will improve performance. \n* Feature Selection - repetitive data makes a model slower to train\n\nHere's an [example](https:\/\/www.kaggle.com\/zheye1218\/nyc-taxi-starter-code) of some starter code you can use for the NYC Taxi Fare competition. This competition is no longer open, but you can still learn a lot from it.","9f97a61d":"## Modeling in SciKit-Learn\n\nModel-fitting in scikit-learn has a very straightforward paradigm to it. ","af21a1a5":"### Vectorization","76656565":"## Model Evaluation","d5ccb9e7":"### Arrays","f556bd76":"### Boolean Indexing","a40beb88":"Visualizations aren't really the focus of this talk, so I won't dwell on matplotlib.","ada68c3d":"### Pandas Aliasing\n\nWe're pretty fast and loose with pointers here. Here's where we can run into trouble","e4eec74d":"## Matplotlib","e9e1f430":"[Kaggle competitions](https:\/\/www.kaggle.com\/competitions) give you experience working with real-world data to solve important problems, and help you prove your skills to peers and potential employers. There's also a large community with lots of resources, happy to help you learn.\n\n\n# Basic Data Analysis in Python\n\nThis workshop introduces the basics of using Python for data analysis, specifically packages like NumPy, Pandas, and Scikit-Learn.\n\nThroughout, we'll be working with a dataset of home prices in Iowa. This is a fairly common dataset for practicing and testing advanced regression techniques. If you're a real estate mogul or something, and you can model housing prices well, then you could recognize undervalued properties and sell them for their true market value, making a bundle in the process. ","fc2ff3e2":"To create a new column, you can use the following syntax.","32b00fed":"## Pandas","2ec70647":"## NumPy","65ef4819":"Before we can begin modeling, we ought to take a cursory glance at our data - some of our features have lots of missing values. Particularly `LotFrontage`.\n\nNumPy has a sentinel value \"NaN\" (not a number) to indicate missing values. If you want to test for missing values, use `np.isnan`, because equality testing for `np.nan` will fail.","4c710700":"So what do we do with features where we've got missing values? \n\nIdeas:\n* Ditch the feature entirely\n* Drop rows where values are missing\n* Replace the missing values\n\nFor pedagogical purposes, you'll see how each of these can be done in pandas.","5780241e":"We'll start by talking about programming stuff, like vectorization in NumPy. Then we'll load our dataset as a Pandas dataframe and play around with it, with some minor visualization. We'll spend the final 40% of the talk doing some modeling with SciKit-Learn."}}