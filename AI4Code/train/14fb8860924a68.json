{"cell_type":{"f9763390":"code","7e04536f":"code","6135a4ef":"code","6ee6e85c":"code","fd138476":"code","84880ecc":"code","7ca1541b":"code","fb0bb9e9":"code","6db5a751":"code","40c22269":"code","d286c7b8":"code","4d2d0562":"code","d60bbada":"code","a5469d1e":"code","0a590d9f":"code","b27be1df":"markdown","2d36d86f":"markdown","ccc7ee8c":"markdown","3d423d62":"markdown","85d0e35d":"markdown","bb47af64":"markdown","32dc7e5f":"markdown","7f9a4798":"markdown","bbe3d6c7":"markdown","50014524":"markdown","d2d4fcac":"markdown","04f78007":"markdown","904766bf":"markdown","8fa75100":"markdown","599ed0a4":"markdown","69242577":"markdown"},"source":{"f9763390":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7e04536f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('dark_background')\nimport seaborn as sns","6135a4ef":"diabetes = pd.read_csv(r'..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndiabetes.head()","6ee6e85c":"print(diabetes.describe(include='all'))\ndiabetes.isna().sum()","fd138476":"\ndiabetes.groupby('Outcome').hist(figsize=(9, 9),color='cyan')","84880ecc":"corr = diabetes.corr()\nplt.figure(figsize=(9,9))\nplt.title('Corelation',color='yellow',size=30)\nplt.xticks(size=20,color='yellow')\nplt.yticks(size=20,color='yellow')\nsns.heatmap(corr,annot=True,cmap='Blues_r')","7ca1541b":"#pip install pycaret","fb0bb9e9":"from pycaret.classification import *","6db5a751":"X = diabetes.drop('Outcome', axis=1)\ny = diabetes['Outcome']\nclf = setup(data = diabetes, target = 'Outcome',train_size=0.8,numeric_imputation='mean',categorical_imputation='mode',feature_selection=True)","40c22269":"compare_models()","d286c7b8":"logreg = create_model('lr')","4d2d0562":"tuned_model=tune_model(logreg)","d60bbada":"ensembled_lr = ensemble_model(tuned_model,method='Boosting')","a5469d1e":"plot_model(ensembled_lr,plot='learning')","0a590d9f":"evaluate_model(ensembled_lr)","b27be1df":"# Setting up the Environment\nSetup()-This function initializes the environment in pycaret and creates the transformation pipeline to prepare the data for modeling and deployment. setup() must called before executing any other function in pycaret. It takes two mandatory parameters: dataframe {array-like, sparse matrix} and name of the target column. All other parameters are optional.","2d36d86f":" # Evaluate the model\n \n **For intuitive understanding pycaret provides us with plots that we can choose to visualize. Click on the plot type to view the respective plot**","ccc7ee8c":"# Let's install PyCaret","3d423d62":"# First set of plots represent the outcome 0 and the next represents outcome as 1.","85d0e35d":"# Import the entire Classification sub module","bb47af64":"# Isn't it a relief when you don't find any missing values ?? xD","32dc7e5f":"# Importing the modules","7f9a4798":"# Now let's compare all our models","bbe3d6c7":"# Ensemble Model\nThis function ensembles the trained base estimator using the method defined in \u2018method\u2019 param (default = \u2018Bagging\u2019). The output prints a score grid that shows Accuracy, AUC, Recall, Precision, F1 and Kappa by fold (default = 10 Fold)","50014524":"# You can further use the predict_model() to get the predictions but since this is just a demo and hence ends here. The snippet for predictions is : predictions = predict_model(ensembled_model, data=your_test_data_goes_here).","d2d4fcac":"# Loading the data","04f78007":"# Parameters:\n\ndata: dataframe array-like, sparse matrix, shape (n_samples, n_features) where n_samples is the number of samples and n_features is the number of features.\n\ntarget: string Name of the target column to be passed in as a string. The target variable could be binary or multiclass. In case of a multiclass target, all estimators are wrapped with a OneVsRest classifier.\n\ntrain_size: float, default = 0.7 Size of the training set. By default, 70% of the data will be used for training and validation. The remaining data will be used for a test \/ hold-out set.\n\ncategorical_features: string, default = None If the inferred data types are not correct, categorical_features can be used to overwrite the inferred type. If when running setup the type of \u2018column1\u2019 is inferred as numeric instead of categorical, then this parameter can be used to overwrite the type by passing categorical_features = [\u2018column1\u2019].\n\ncategorical_imputation: string, default = \u2018constant\u2019 If missing values are found in categorical features, they will be imputed with a constant \u2018not_available\u2019 value. The other available option is \u2018mode\u2019 which imputes the missing value using most frequent value in the training dataset.\n\nnumeric_imputation: string, default = \u2018mean\u2019 If missing values are found in numeric features, they will be imputed with the mean value of the feature. The other available option is \u2018median\u2019 which imputes the value using the median value in the training dataset.\n\nfeature_selection: bool, default = False When set to True, a subset of features are selected using a combination of various permutation importance techniques including Random Forest, Adaboost and Linear correlation with target variable. The size of the subset is dependent on the feature_selection_param. Generally, this is used to constrain the feature space in order to improve efficiency in modeling. When polynomial_features and feature_interaction are used, it is highly recommended to define the feature_selection_threshold param with a lower value.","904766bf":"# Let's tune the model","8fa75100":"# Quick Introduction to PyCaret - An open source low-code ML library\n\n![image.png](attachment:image.png)\n\n\nYou can reach pycaret website and documentation from https:\/\/pycaret.org\n\nPyCaret is an open source, low-code machine learning library in Python that allows you to go from preparing your data to deploying your model within seconds in your choice of notebook environment.\n\nPyCaret being a low-code library makes you more productive. With less time spent coding, you and your team can now focus on business problems.\n\nPyCaret is simple and easy to use machine learning library that will help you to perform end-to-end ML experiments with less lines of code.\n\nPyCaret is a business ready solution. It allows you to do prototyping quickly and efficiently from your choice of notebook environment.\n\n","599ed0a4":"# As observed LogisticRegression is the best model so let's create one right away with the above parameters","69242577":"# Learning Curve"}}