{"cell_type":{"98a72009":"code","5ced177c":"code","f18b8f75":"code","0b5bcece":"code","9d3122c9":"code","61d02f26":"code","10a3139d":"code","9abb7d4d":"code","48704043":"code","f8a4623e":"code","7bbcf0e4":"code","702571d4":"code","21917b24":"code","85f626e7":"code","04834796":"code","6457dbea":"code","d31421f3":"code","6fed253f":"code","42992f9d":"markdown","adec4649":"markdown","19453e8f":"markdown","78edb37c":"markdown","a9a3a767":"markdown","f46ce35f":"markdown","7818ff04":"markdown","f3e63a8b":"markdown","37c846f5":"markdown","62bf3eec":"markdown","519f17d0":"markdown","69c95ec6":"markdown","09626e96":"markdown","b2561c45":"markdown","e7ec0466":"markdown","ccac5e3e":"markdown","04fc780e":"markdown","bbbf9512":"markdown","f7b17d0d":"markdown","0eba5669":"markdown","023c0cb1":"markdown"},"source":{"98a72009":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5ced177c":"import h2o\nfrom h2o.automl import H2OAutoML\nh2o.init(max_mem_size='16G')","f18b8f75":"data = h2o.import_file('\/kaggle\/input\/titanic\/train.csv')\n\n# If you only want to use a few features, manually listing them is better.\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\n# If you have a large number of features (uncomment below line), it is better to just pull out the list of columns. But make sure that you drop all unnecessary features if you do that.\n#features = data.columns\n\noutput = 'Survived'","0b5bcece":"aml = H2OAutoML(max_models=100, max_runtime_secs=1500, seed=1)\naml.train(x=features, y=output, training_frame=data)","9d3122c9":"lb = aml.leaderboard\nlb.head()","61d02f26":"#Read the test data\ntitanic_test = h2o.import_file('\/kaggle\/input\/titanic\/test.csv')\n\n#Make predictions on the test data. You don't need to feed in the columns you are using. H2O will automatically select them based on the columns in the training data.\npreds = aml.predict(titanic_test)\n\n\ntitanic_test['Survived'] = preds\nsub = titanic_test[['PassengerId', 'Survived']]\n\n#Converting H2o Frame to pandas DataFrame for submission.\nsubs = sub.as_data_frame(use_pandas=True)","10a3139d":"subs['Survived'] = subs['Survived'] > 0.5\nsubs['Survived'] = subs['Survived'].astype(int)\nsubs.to_csv('h2o_sub.csv', index=False)","9abb7d4d":"!pip install tpot","48704043":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","f8a4623e":"#Separate the target\ny = train['Survived']","7bbcf0e4":"combined = pd.concat([train, test])\nlen_train = len(train)\nlen_test = len(test)","702571d4":"combined.drop(['Survived', 'PassengerId', 'Name', 'Cabin', 'Ticket'], axis=1, inplace=True)","21917b24":"combined['Age'] = combined['Age'].fillna(-999)\ncombined['Fare'] = combined['Fare'].fillna(-999)\ncombined['Embarked'] = combined['Embarked'].fillna('Unkown')\n\ncombined = pd.get_dummies(combined)","85f626e7":"pd.isnull(combined).any()","04834796":"train = combined.head(len_train)\ntest = combined.tail(len_test)\nprint(\"Train shape is \" + str(train.shape))\nprint(\"Test shape is \" + str(train.shape))","6457dbea":"from tpot import TPOTClassifier\n\ntpot = TPOTClassifier(generations=5, verbosity=2, random_state=None)\n\ntpot.fit(train, y)","d31421f3":"a = tpot.predict(test)\nsub = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\nsub['Survived'] = a\nsub.to_csv('tpot_sub.csv', index=False)","6fed253f":"tpot.export('tpot_titanic_pipeline.py')","42992f9d":"# AutoML\n\nAutoML is a growing field, as many researchers are making their own libraries which can be used to make high quality machine learning models by just feeding it data. There are many AutoML libraries:\n1. H2O AutoML\n2. TPOT\n3. MLbox\n4. AutoSklearn\n5. AutoKeras\n\nIn this notebook, I am going to be trying out the H2O AutoML and TPOT library.\n\n\n# H2O AutoML\nThe H2O AutoML package is part of the h2o python module, which contains a lot of other algorithms, like gradient boosting, and random forest. The H2O automl package is quite robust, as it can do quite a bit of preprocessing itself, and even handles situations where it hasn't seen a particular value before (as we'll see in the program). For the H2O package, the primary requirement is to just tell it how many models to try, and how much time to spend. These hyperparamteres are easy to tune: As much as you can afford! In this dataset, the H2O package does the best out of the box.\n\n# TPOT Library\nThe TPOT library is another fantastic AutoML library, which focuses on using genetic based learning algorithms. At a high level, you can image that it starts off with lots of models, then selects the few which do the best, and then starts to optimize hyperparameters, and experiment with ensembling etc. This library has a lot more hyperparameters that need carefull tuning, but should only be done if one has experience with genetic algorithms. I will just explain the main ones:\n\n1. Generations: Basically the number of iterations. Try to put it as high as you can. It can't hurt.\n2. Populatiion Size: How many of the top algorithms should be transferred to the next generation. Try to put a relatively high number here.\n3. CV - By default, it does a stratified KFold validation strategy, so providing an integer will tell it how many folds to try.\n4. Early stopping - This should be an integer. When TPOT finds that there is no improvement in optimization process after n generations, then it will stop.\n\nAnother thing I like about the TPOT library is that you can export the entire pipeline to a pythong file, and fine tune it further if you like! However, as we'll see, TPOT is not that robust to noisy data, so a lot of the pre-processing has to be done by the data scientist. This is what I was talking about when mentioning that such modules should be used for the modelling stage.\n\n# When to use AutoML\n\nRight now, AutoML is not at the stage to beat humans. A deep dive analysis of the data is still crucial. AutoML (as of now), can't compete with human insight based feature engineering and data modification. However, one thing AutoML is very good at is trying out hundreds of models and ensembling them to find the best one. Hence in the machine learning pipeline, automl should be used in the modelling stage.\n\nML pipeline:\nData Preprocessing   -->    Feature Engineering    -->     Modelling (here is where AutoML comes in)     -->     Postprocessing (optional).\n\nSo now let's try out the libraries.","adec4649":"This is where we actually build the H2O automl model. There are 2 main parameters, as I mentioned.\n\nmax_models - The maximum number of models H2O tries. Make this as large as possible. But remember the law of diminishing return.\nmax_runtim_secs - The maximum time to spend. I am a bit impatient so I will only set it to 1500 secs, but make it as large as possible.\n\nAlso, when we train the model, x will refer to the columns used for training instead of the training data. Similarly, y refers to the name of the target column instead of the values.\nThen there is a separate parameter training_frame where we specify the data to train on.","19453e8f":"Awesome. So we can even see the error of the model on different metrics. So now let's make our predictions and Submit.","78edb37c":"I will be filling in null values with -999, but ideally, you should look for better missing value imputation techniques like mean and median imputation. Also, we will one hot encode all of these columns.","a9a3a767":"A key thing to note is that by default, H2O predicts probabilities. Hence, we will have to convert it to 0 or 1 for submission. I am just going to use the threshold of 0.5, but you can use whatever you like.","f46ce35f":"Let's just make sure all null values have been filled in.","7818ff04":"Let's drop these columns. PassengerId is useless for predictions.\nName, cabin and ticket can't be one-hot encoded into small columns, so let's get rid of them.","f3e63a8b":"TPOT has the following requirements for the data:\n\n1. No Null Values\n2. No categorical Values - Use one hot encoding or label encoding\n\nSo let's combine the data for one hot encoding. Also, in this tutorial, I will be removing the cabin and ticket columns, since they can't be one hot encoded. However, many other tutorials on the dataset cover these columns. Adding that pre-processing step should significantly improve performance.","37c846f5":"And Done! If you submit the H2O prediction file, you would get an accuracy 0.77511.\nConsidering that after quite a lot of work, a good score is around 79 - 80%, H2O automl is doing fantastic!\n\nNow, let's try out TPOT.","62bf3eec":"# TPOT Implementation - score: \u22480.75598","519f17d0":"# H2O Implementation - score: 0.77511\n\nThe H2O AutoML library is found under h2o.autml. We first have to initialize an h2o instance on the local server, which can be done with the init command. Also, since kaggle notebooks stop if memory exceeds 16GB, I will limit H2O to that memory. If you have a better computer, you can raise the level.","69c95ec6":"H2O is a slightly different library. Unlike other libraries, H2O doesn't straight away take in a pandas dataframe. Instead, it is better to use an H2O Frame. In practice, both of them are very similar, and most basic commands, like pulling out a column, work in h2o frame as well.\n\nAlso, we have to specify the features that the algorithms should use in the features array, and the target value in the output array.","09626e96":"Once the above code block has run, you can see which models perform the best by printing out the leaderboard. Generally, it will be an ensemble.","b2561c45":"Fortunately, tpot supports a pandas dataframe as input, so let's read the training and test data","e7ec0466":"Finally done with the pre-processing. Now we will move onto the classifier.\nSince this is a classification problem, we import TPOTClassifier. (for regression, import TPOTRegressor).\n\nParameters:\n\ngenerations - think of it as number of iterations\n\npopulation_size - number of 'best' algorithms to keep. I will use the default value of 100.\n\nverbosity - Using 2 will show a progress bar and speed of pipeline. This is adequate, but if you want more details, use 3.\n\nrandom_state - To initialise the seed. I haven't but if you want, change the value from None to an integer.\n","ccac5e3e":"# Remarks\n\nSuch AutoML libraries are currently very powerful for modelling purposes, as they efficiently find the best algorithm and hyperparameters. However, to actually win a competition, you still need to do hand-made feature engineering, and apply other tricks to optimize for the metric. \n\nAutoML can be thought of as a slight boost to your current performance due to its ability for complex models. Feel free to fork this notebook and try it on different datasets.\n\nGood luck for future competitions!","04fc780e":"Awesome, so now we have fit the model. Let's make our predictions and submit.","bbbf9512":"If you open the python file with an editor you will see the following:\n\n![Screenshot%202020-05-29%20at%201.46.04%20PM.png](attachment:Screenshot%202020-05-29%20at%201.46.04%20PM.png)\n\nThe average accuracy on the training set is written as a comment. If you run the pipeline, you would get the exact same results.\n\nThis is extremely usefull as once you find the best algorithm, you don't need to run the entire optimization process again. You can simply run it through this pipeline!","f7b17d0d":"Finally, we will split the train and test data back. I will also confirm their shape to make sure I did this step right.","0eba5669":"These are the final predictions. TPOT by default predicts 0 or 1, so we don't need to do the post-processing step. This will get you an accuracy of 0.75598. It might be different since I haven't fixed the seed. Also, TPOT can actually do a lot better. The reason is that I didn't do the missing value imputation correctly, as -999 is not a good value. Also, I have dropped some useful columns which I fed to H2O, so TPOT obviously did slightly worse. This was only a small tutorial on getting started with autoML so I didn't go into those details, as there are many other good notebooks for this.\n\nAnother thing I like about TPOT is that you can export the pipeline as a python file.","023c0cb1":"I have already briefly described the library above, but it essentially uses genetic algorithms to optimize for the best model.\nThe tpot library isn't pre-installed in the kaggle workflow, so we will have to install it first. It is available on pypi so we can donwload it with pip."}}