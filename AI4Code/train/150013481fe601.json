{"cell_type":{"ae049865":"code","2b25c4d9":"code","5c98560d":"code","a710262f":"code","22cde34e":"code","0b3885ae":"code","6aa7a2ba":"code","7a0d7c4a":"code","f9a8b896":"code","c1061619":"code","74f8830c":"code","57810561":"code","9483ac34":"code","c2975b96":"code","f8f99598":"code","35909359":"code","3737d41d":"code","073df3f3":"code","e740df7f":"code","c4e8c6fa":"code","2f8d42d8":"code","02a9d6b7":"code","410187b3":"code","61fb6ec2":"code","e4a139d3":"code","bdc13aeb":"code","0afefb84":"code","f01c91aa":"code","da1d6f4a":"code","a87047d9":"code","5f633740":"code","7964d740":"code","478c78f1":"code","f4f50fb0":"code","d004fd3f":"code","934cd9bd":"code","c399f136":"code","aeb4692c":"code","98a2649d":"code","59a0a47a":"code","d8ee9399":"code","2a99b29e":"code","2fedd967":"code","e0b42b5b":"code","76500cba":"code","cf1faa5a":"code","706a3bf9":"markdown","e17484ee":"markdown","90408eb0":"markdown","0ca6cc6c":"markdown","4dab2fbb":"markdown","da867059":"markdown","485af5b1":"markdown","5ae48ff3":"markdown","96df784c":"markdown","7d64be3a":"markdown","6dd1caee":"markdown","903f9ea0":"markdown","695263b0":"markdown","f52bd618":"markdown","f580d6df":"markdown","0e95920b":"markdown","e4fc5929":"markdown","a770a679":"markdown","beb1eb55":"markdown","0f8573ab":"markdown"},"source":{"ae049865":"# install requirements\n!pip install matplotlib seaborn scipy scikit-learn scipy torch tqdm datetime","2b25c4d9":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nfrom datetime import datetime\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# import logging\n# logging.basicConfig(\n#         format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n#         datefmt=\"%m\/%d\/%Y %H:%M:%S\",\n#         level=logging.INFO,\n# )\n\n%matplotlib inline\n%load_ext autoreload\n%autoreload 2","5c98560d":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\nset_seed(10)","a710262f":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","22cde34e":"# Load and show data\ndf_train = pd.read_csv('\/kaggle\/input\/hcg-test-data-services-dept\/bookings.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/hcg-test-data-services-dept\/LTV_class.csv')\ndf_train.head","0b3885ae":"sns.histplot(df_test['LTV_Cluster'])","6aa7a2ba":"data = pd.concat([df_train['RoomPrice'], df_train['RoomNo']], axis=1)\ndata.plot.scatter(x='RoomNo', y='RoomPrice')\n","7a0d7c4a":"data = pd.concat([df_train['RoomPrice'], df_train['RoomGroupID']], axis=1)\ndata.plot.scatter(x='RoomGroupID', y='RoomPrice')","f9a8b896":"sns.histplot(df_train['RoomPrice']) \nprint(\"Looks ok\")","c1061619":"sns.histplot(df_train['TotalPayment'])\nprint(\"Skewness: %f\" % df_train['TotalPayment'].skew())\nprint(\"Kurtosis: %f\" % df_train['TotalPayment'].kurt())","74f8830c":"data = pd.concat([df_train['Channel'], df_train['TotalPayment']], axis=1)\nfig = sns.boxplot(x='Channel', y=\"TotalPayment\", data=data)\nfig.axis(ymin=0, ymax=5000)\nprint('just another figure for the beauty of it, I throw out channels later due to insufficient data')","57810561":"#missing data\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(5)\n","9483ac34":"# convert string to category then to numerical\ndf_train[['Status','Channel', 'Country', 'RoomNo']] = df_train[['Status','Channel', 'Country', 'RoomNo']].fillna('other').astype('category').agg(lambda x: x.cat.codes)\ndf_train","c2975b96":"#correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True)","f8f99598":"# total payment correlation matrix zoom in\nk = 5 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'TotalPayment')['TotalPayment'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","35909359":"#scatterplot for closely inspect, not much difference from previous conclusions\nsns.set()\ncols = [cols]\nsns.pairplot(df_train[['TotalPayment', 'RoomPrice', 'Channel', 'Country', 'RoomNo']], height = 2.5)\nplt.show()","3737d41d":"# convert string to category then to numerical\ndf_train['Status'] = df_train['Status'].fillna('other').astype('category')\ndf_train['Status'] = df_train['Status'].cat.codes\ndf_train['Channel'] = df_train['Channel'].fillna('other').astype('category')\ndf_train['Channel'] = df_train['Channel'].cat.codes\ndf_train['Country'] = df_train['Country'].fillna('other').astype('category')\ndf_train['Country'] = df_train['Country'].cat.codes\ndf_train['RoomNo'] = df_train['RoomNo'].fillna('other').astype('category')\ndf_train['RoomNo'] = df_train['RoomNo'].cat.codes\ndf_train","073df3f3":"# helper function\ndef convert_datetime(str):\n    return datetime.strptime(str, '%Y-%m-%d')","e740df7f":"# Process string data\ndf_train['ArrivalDate'] = df_train['ArrivalDate'].map(convert_datetime)\ndf_train['DepartureDate'] = df_train['DepartureDate'].map(convert_datetime)\ndf_train['CreatedDate'] = df_train['CreatedDate'].map(convert_datetime)","c4e8c6fa":"# reduce a bit of data\ndf_train['DayStay'] = (df_train['DepartureDate'] - df_train['ArrivalDate']).dt.days\ndf_train['CreateDelay'] = (df_train['ArrivalDate'] - df_train['CreatedDate']).dt.days\ndf_train = df_train[['GuestID', 'Status', 'RoomGroupID', 'DayStay', 'CreateDelay',\n                    'RoomPrice', 'Channel', 'RoomNo', 'Country', 'Adults', 'Children', 'TotalPayment']]\ndf_train.set_index('GuestID')","2f8d42d8":"# summing\ndf_sum = df_train[[\"GuestID\", \"DayStay\", \"CreateDelay\", \"Adults\", \"Children\", \"TotalPayment\"]].groupby('GuestID').sum()\ndf_sum.set_axis([ \"DayStay_sum\", \"CreateDelay_sum\", \"Adults_sum\", \"Children_sum\", \"TotalPayment_sum\" ], axis='columns', inplace=True)\n\n# averaging\ndf_average = df_train[[\"GuestID\", \"DayStay\", \"RoomPrice\", \"Adults\", \"Children\", \"TotalPayment\"]].groupby('GuestID').mean()\ndf_average.set_axis([\"DayStay_avg\", \"RoomPrice_avg\", \"Adults_avg\", \"Children_avg\", \"TotalPayment_avg\" ], axis='columns', inplace=True)\ndf_sum","02a9d6b7":"# number of bookings (may violate those F in RFM metrics, but when I concatenate rows, this is obvious result)\nnum_booking = df_train.groupby(['GuestID']).size().to_frame('NumBooking') \n\n# group to list\n# list_agg = df_train[['GuestID','Status','Country','Channel','RoomNo']].groupby('GuestID').agg(lambda x: x.tolist())\n# list_agg\n\n# drop first (Roomno,Status doesnt correlate with price and totalpay, probably random)\ndrop_duplicate = df_train[['GuestID','Status','Country','RoomNo']].groupby(['GuestID']).first()\ndrop_duplicate","410187b3":"# Gather all values into one table\ndf_train = pd.concat([df_sum, df_average, num_booking, drop_duplicate], axis=1).reset_index()\ndf_train","61fb6ec2":"# all histograms\nfig, ax = plt.subplots(3, 5, figsize=(15,15))\nm=1\nfor i in range(3):\n    for j in range(5):\n        df_train.hist(column = df_train.columns[m], bins=30, ax=ax[i,j])\n        m+=1\n        if m > 13:\n            break","e4a139d3":"# Log transformation to remove skewness\ndf_train[['TotalPayment_sum', \"TotalPayment_avg\", \"DayStay_avg\", \"DayStay_sum\"]] = np.log(df_train[['TotalPayment_sum', \"TotalPayment_avg\", \"DayStay_avg\", \"DayStay_sum\"]])\nsns.histplot(df_train['TotalPayment_avg'])","bdc13aeb":"# move axis to 0 and std to [-1,1] using standard scaler (x-mean)\/std\nscaler = StandardScaler()\ndf_train_scaled = df_train.copy()\ndf_train_scaled[df_train.columns[1:]] = scaler.fit_transform(df_train[df_train.columns[1:]])\nsns.histplot(df_train_scaled['TotalPayment_sum'])","0afefb84":"guestid = df_train['GuestID']\nX_payment_onetime = df_train_scaled[df_train.columns[1:]].drop(['DayStay_sum', 'CreateDelay_sum', 'Adults_sum', 'Children_sum', 'TotalPayment_sum', 'NumBooking','TotalPayment_avg'], axis=1)\nprint(list(X_payment_onetime.columns))\nX_payment_onetime = X_payment_onetime.to_numpy()","f01c91aa":"from sklearn.cluster import MiniBatchKMeans, KMeans\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm \nfrom sklearn.metrics import normalized_mutual_info_score\nfrom sklearn.metrics import confusion_matrix\n\nfrom scipy.optimize import linear_sum_assignment","da1d6f4a":"# Convert data frame to tensor\nX = torch.Tensor(df_train_scaled[df_train_scaled.columns[1:]].to_numpy())\ndf_train_guestindex = df_train_scaled.set_index('GuestID')\nX_test = torch.Tensor(df_train_guestindex.loc[df_test['GuestID']].to_numpy())\ny_test = torch.LongTensor(df_test['LTV_Cluster'].to_numpy())-1 # minus 1 so label start from 0\ntest_set = list(map(tuple, zip(X_test, y_test)))\nX_onetime = torch.Tensor(X_payment_onetime)\n","a87047d9":"# Class holding model configurations\nclass Config:\n    def __init__(self, **kwargs):\n        for k,v in kwargs.items():\n            setattr(self, k, v)\n\nconf = Config(n_features = X.shape[1],\n            nodes=128,\n            hidden=10,\n            latent_size = 5,\n            batch_size=32,\n            n_clusters=len(torch.unique(y_test)),\n            n_epochs=20,\n            test_size=0.2,\n            num_workers=0,\n            device='cpu')","5f633740":"# Python data loader\ntrain_loader = torch.utils.data.DataLoader(\n                 dataset=X,\n                 batch_size=conf.batch_size,\n                 shuffle=True,\n                 drop_last=True,\n                 num_workers=conf.num_workers)\ntest_loader = torch.utils.data.DataLoader(\n                dataset=test_set,\n                batch_size=conf.batch_size,\n                shuffle=False,\n                drop_last=True,\n                num_workers=conf.num_workers)","7964d740":"# Define Auto-encoder model in pytorch\nclass LinearBlock(nn.Module):\n    '''\n    Typical Linear block\n    '''\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.block = nn.Sequential(nn.Linear(in_channels, out_channels, bias=False), # bias false cause we use batchnorm\n                                nn.BatchNorm1d(out_channels),\n                                nn.LeakyReLU(0.3),\n                                nn.Dropout(0.1)) # regularization\n        \n    def forward(self,x):\n        return(self.block(x))\n\nclass Encoder(nn.Module):\n    '''\n    Encode data to latent representation\n    '''\n    def __init__(self, config):\n        super().__init__()\n        self.input = LinearBlock(config.n_features, config.nodes)\n        self.sequence = nn.Sequential(*[LinearBlock(config.nodes, config.nodes)\n                                        for i in range(config.hidden)])\n        self.latent = nn.Linear(config.nodes, config.latent_size)\n    def forward(self, x):\n        x = self.input(x)\n        x = self.sequence(x)\n        x = self.latent(x)\n        \n        return x\n\nclass Decoder(nn.Module):\n    '''\n    Reconstruct from latent representation\n    '''\n    def __init__(self, config):\n        super().__init__()\n\n        self.input = LinearBlock(config.latent_size, config.nodes)\n        self.sequence = nn.Sequential(*[LinearBlock(config.nodes, config.nodes)\n                                        for i in range(config.hidden)])\n        self.out = nn.Linear(config.nodes, config.n_features)\n    def forward(self, x, targets=None):\n        x = self.input(x)\n        x = self.sequence(x)\n        x = self.out(x)\n\n        return x\n\nclass DEC_AE(nn.Module):\n    '''\n    Cluster centroids is init by k-mean, then update using backprop with KL divergion loss\n    Calculate t student distribution after encoding\n    '''\n    def __init__(self, config):\n        super().__init__()\n        self.encode = Encoder(config)\n        self.decode = Decoder(config)\n\n        self.alpha = 1.0\n        self.cluster_centers = nn.Parameter(torch.zeros(config.n_clusters,config.n_features))\n        self.config = config\n\n    def init_weights(self):\n        for module in self.modules():\n            if isinstance(module, nn.Linear) :\n                torch.nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None: module.bias.data.zero_()\n\n    def encode_forward(self, x):\n        return self.encode(x)\n    \n    def decode_forward(self, x):\n        return self.decode(x)\n    \n    def update_cluster_center(self, cluster_centers):\n        self.cluster_centers.data = cluster_centers\n        # self.cluster_center = self.cluster_center.to('cuda')\n\n    \n    def get_t_student_distribution(self, batch, alpha=1.0):\n        norm_squared = torch.sum((batch.unsqueeze(1) - self.cluster_centers) ** 2, 2)\n        numerator = 1.0 \/ (1.0 + (norm_squared \/ alpha))\n        power = float(alpha + 1) \/ 2\n        numerator = numerator ** power\n        return numerator \/ torch.sum(numerator, dim=1, keepdim=True)","478c78f1":"def cluster_accuracy(y_true, y_predicted, cluster_number = None):\n    \"\"\"\n    Calculate clustering accuracy after using the linear_sum_assignment function in SciPy to\n    determine reassignments.\n    Code follows Morbieu tutorial\n    https:\/\/smorbieu.gitlab.io\/accuracy-from-classification-to-clustering-evaluation\/\n    \"\"\"\n    if cluster_number is None:\n        cluster_number = (\n            max(y_predicted.max(), y_true.max()) + 1\n        )  # assume labels are 0-indexed\n    count_matrix = np.zeros((cluster_number, cluster_number), dtype=np.int64)\n    for i in range(y_predicted.size):\n        count_matrix[y_predicted[i], y_true[i]] += 1\n\n    row_ind, col_ind = linear_sum_assignment(count_matrix.max() - count_matrix)\n    reassignment = [row_ind, col_ind]\n    accuracy = count_matrix[row_ind, col_ind].sum() \/ y_predicted.size\n    return reassignment, accuracy\n\ndef validateOnCompleteTestData(test_loader, model):\n    \"\"\"\n    Does k-mean clustering then then calculate accuracy with test label\n    \"\"\"\n    to_eval = []\n    true_labels = []\n    for x,y in test_loader:\n        x = x.to(conf.device)\n        y = y.to(conf.device)\n        with torch.no_grad():\n            to_eval.append(model.encode_forward(x))\n        true_labels.append(y)\n    to_eval = torch.cat(to_eval, dim=0).detach().cpu().numpy()\n    true_labels = torch.cat(true_labels).cpu().numpy().astype(int)\n    km = KMeans(n_clusters=len(np.unique(true_labels)), n_init=20)\n    y_pred = km.fit_predict(to_eval)\n    reassignment, currentAcc = cluster_accuracy(true_labels, y_pred)\n    \n    # re-assignment\n    for i in range(len(np.unique(true_labels))):\n        y_pred[y_pred == reassignment[1][i]] = reassignment[0][i] + 999 #temporary value\n    y_pred -= 999\n    print(f\"accuracy {currentAcc:.4f} reassignment {reassignment}\")\n    \n    return currentAcc, reassignment\n# run sample\nwith torch.no_grad():\n    dec_ae = DEC_AE(conf).to(conf.device) #auto encoder\n    urrentAcc, reassignment = validateOnCompleteTestData(test_loader,dec_ae)","f4f50fb0":"# Main training\nclass DEC:\n    def __init__(self, config):\n        self.config = config\n        self.n_clusters = config.n_clusters\n        self.alpha = 1.0\n    \n    @staticmethod\n    def target_distribution(q):\n        weight = q ** 2 \/ q.sum(0)\n        distribution = (weight.T \/ weight.sum(1)).T.data\n        distribution.requires_grad = True\n        return distribution\n    @staticmethod\n    def KLD(q, p):\n        return torch.sum(p*torch.log(p\/q), dim=-1)\n\n   \n\n    def pretrain(self, train_loader, test_loader=None):\n        '''\n        Train auto encoder that compress all features in to latent space (5 features)\n        This has similar effect to PCA\n        Use K-Mean to cluster latent space into 3 clusters that equipvalent to LTV\n        Check accuracy\/corelation with original LTV clustering algorithm\n        '''\n        dec_ae = DEC_AE(self.config).to(self.config.device) #auto encoder\n        dec_ae.init_weights()\n        optimizer = torch.optim.SGD(dec_ae.parameters(),lr = 0.01, momentum=0.9)\n        best_acc = 0.0\n        for epoch in range(self.config.n_epochs):\n            pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n            for i,x in pbar:\n                optimizer.zero_grad()\n\n                x = x.to(self.config.device)\n                x_en = dec_ae.encode_forward(x)\n                x_de = dec_ae.decode_forward(x_en)\n                loss = F.mse_loss(x_de,x,reduction='mean')\n                \n                loss.backward()\n                optimizer.step()\n                pbar.set_description(f\"Epoch {epoch} loss {loss.item():.5f}\")\n            #now we evaluate the accuracy with AE\n            if test_loader is not None:\n                with torch.no_grad():\n                    currentAcc, reassignment = validateOnCompleteTestData(test_loader,dec_ae)\n                    if currentAcc > best_acc:                \n                        torch.save(dec_ae,'bestModel'.format(best_acc))\n                        best_acc = currentAcc\n        return dec_ae\n        \n    def clustering(self, minibatch_kmean, x, model):\n        with torch.no_grad():\n            y_pred_ae = model.encode_forward(x)\n            y_pred_ae = y_pred_ae.cpu().numpy()\n            y_pred = minibatch_kmean.partial_fit(y_pred_ae) #seems we can only get a centre from batch\n            cluster_centers = minibatch_kmean.cluster_centers_ #keep the cluster centers\n            cluster_centers = torch.Tensor(cluster_centers)\n            model.update_cluster_center(cluster_centers)\n\n    def train(self,train_loader, test_loader=None, model=None):\n        \"\"\"This method will start training for DEC cluster\"\"\"\n        if model is None:\n            model = torch.load(\"bestModel\").to(self.config.device)\n        optimizer = torch.optim.SGD([\n             {'params': model.parameters()},\n            ],lr = 0.01, momentum=0.9)\n        print('Initializing cluster center with pre-trained weights')\n        minibatch_kmean = MiniBatchKMeans(n_clusters=self.n_clusters, n_init=20, batch_size=self.config.batch_size)\n        got_cluster_center = False\n\n        for epoch in range(self.config.n_epochs):\n            pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n            for it,x in pbar:\n                optimizer.zero_grad()\n\n                x = x.to(self.config.device)\n                #step 1 - get cluster center from batch\n                #here we are using minibatch kmeans to be able to cope with larger dataset.\n                if not got_cluster_center:\n                    self.clustering(minibatch_kmean, x, model)\n                    got_cluster_center = True\n                else:\n                    model.train()\n                    #now we start training with acquired cluster center\n                    x_en = model.encode_forward(x)\n                    q = model.get_t_student_distribution(x_en)\n                    #get target distribution\n                    p = self.target_distribution(q)\n                    #print('q',q,'p',p)\n                    loss = self.KLD(q,p).mean()\n                    loss.backward()\n                    optimizer.step()\n                \n                    pbar.set_description(f\"Epoch {epoch} loss {loss.item():.4f}\")\n            reassignment = None\n            if test_loader is not None:\n                currentAcc, reassignment = validateOnCompleteTestData(test_loader,model)\n        return model, reassignment\n        \n","d004fd3f":"dec = DEC(conf)\ndec.pretrain(train_loader, test_loader)\nmodel, reassignment = dec.train(train_loader, test_loader)","934cd9bd":"x_enc = model.encode_forward(X).detach().cpu().numpy()\nkm = KMeans(n_clusters=conf.n_clusters, n_init=20)\ny_pred = km.fit_predict(x_enc)\n\n# re-assignment\nfor i in range(conf.n_clusters):\n    y_pred[y_pred == reassignment[1][i]] = reassignment[0][i] + 999 #temporary value\ny_pred = y_pred - 999 + 1 # back to 1, 2, 3","c399f136":"# Merge y_pred as new LTV columns\ndf_train['LTV'] = y_pred\nsns.histplot(df_train[\"LTV\"])\nplt.show()\ndf_train","aeb4692c":"data = pd.concat([df_train['TotalPayment_sum'], df_train['LTV']], axis=1)\ndata.plot.scatter(x='LTV', y='TotalPayment_sum')","98a2649d":"# save everything\nimport pickle\nsaveobject = {\"X\": X, \"model\":model, \"y_pred\": y_pred, \"config\": conf, 'test_set': test_set, 'guestid': guestid, \"table\": df_train}\nwith open(\"save_slot\",'wb') as f:\n    pickle.dump(saveobject, f)","59a0a47a":"conf = Config(n_features = X_onetime.shape[1],\n            nodes=128,\n            hidden=10,\n            latent_size = 5,\n            batch_size=32,\n            n_clusters=len(torch.unique(y_test)),\n            n_epochs=20,\n            test_size=0.2,\n            num_workers=0,\n            device='cpu')\nprint(conf.n_features)","d8ee9399":"train_loader_onetime = torch.utils.data.DataLoader(\n                 dataset=X_onetime,\n                 batch_size=conf.batch_size,\n                 shuffle=True,\n                 drop_last=True,\n                 num_workers=conf.num_workers)","2a99b29e":"dec = DEC(conf)\nmodel = dec.pretrain(train_loader_onetime)\nmodel, reassignment = dec.train(train_loader_onetime, model=model)","2fedd967":"x_enc = model.encode_forward(X_onetime).detach().cpu().numpy()\nkm = KMeans(n_clusters=conf.n_clusters, n_init=20)\ny_pred = km.fit_predict(x_enc)\n\n# # re-assignment\n# for i in range(conf.n_clusters):\n#     y_pred[y_pred == reassignment[1][i]] = reassignment[0][i] + 999 #temporary value\n# y_pred = y_pred - 999 + 1 # back to 1, 2, 3","e0b42b5b":"# Merge y_pred as new LTV columns\ndf_train['LTV'] = y_pred\nsns.histplot(df_train[\"LTV\"])\nplt.show()\ndf_train\n","76500cba":"data = pd.concat([df_train['TotalPayment_sum'], df_train['LTV']], axis=1)\ndata.plot.scatter(x='LTV', y='TotalPayment_sum')","cf1faa5a":"# save everything\nimport pickle\nsaveobject = {\"X\": X, \"model\":model, \"y_pred\": y_pred, \"config\": conf, 'test_set': test_set, 'guestid': guestid, \"table\": df_train}\nwith open(\"save_slot_vip\",'wb') as f:\n    pickle.dump(saveobject, f)","706a3bf9":"Surprisingly, no difference in room price at all (no double delux and king\/queen studio room as we used too)","e17484ee":"<a id=\"subsection-three\"><\/a>\n## LTV clustering using unsupervise Deep Embedded Clustering (DEC)","90408eb0":"Not the same as RFM cluster pipeline, but it's to be expected using totally different clustering technique","0ca6cc6c":"All the skews distributions need to be convert to log value","4dab2fbb":"**New table with single row of unique GuestID**","da867059":"<a id=\"section-two\"><\/a>\n# Model Implementation: Deep Embedded Clustering\nDeep Embedded Clustering is a unsupervise technique for clustering data come with the paper https:\/\/arxiv.org\/abs\/1511.06335\nIt significantly out perform traditional clustering method like K-mean, LDMGI and SEC. \nThe main idea is move the clusters in the way of minimize KL-divergent or cluster similarity between student t distribution and an aulixiary target distribution that they propose. (I will provide graphs and detail explaination of DEC in interview if i get there).\n\nEssentially, we need \n- An Auto Encoder to \"compress\" data\n- and another training part where we minimize the KL distance.","485af5b1":"<a id=\"section-one\"><\/a>\n# Data exploration","5ae48ff3":"<a id=\"subsection-two\"><\/a>\n## Pytorch Auto-encoder and DEC","96df784c":"<a id=\"subsection-four\"><\/a>\n## VIP Clustering (For non returning customer)\nVip clustering should be the same as normal clustering, main difference is we only have single point of data for that customer. We use our processed data for this.","7d64be3a":"**Looking at TotalPayment and our clustering, we see a clear separation, which is good indicators of our clustering algorithm running well**","6dd1caee":"# This notebook implement Deep Embedded Clustering, which is an unsupervise deep learning technique for clustering.\nThe dataset we'll be using is from Hash Consultant Group. This data shows a glimpse on Hotel operations. Our task is to do clustering customers to groups of Life-Time-Value, in other words, the importance of each customer to the bussiness. The notebook structure as follow:\n\n* [Data Exploration](#section-one)\n    - [Data processing](#subsection-one)\n* [Model Implementation](#section-two)\n    - [Pytorch Auto-encoder and DEC](#subsection-two)\n    - [Model training](#subsection-three)\n    - [Looking for VIP](#subsection-four)\n    \nReferences: \n\n- [Unsupervised Deep Embedding for Clustering Analysis](https:\/\/arxiv.org\/pdf\/1511.06335.pdf)\n- Pytorch implementation of DEC [here](https:\/\/github.com\/xiaopeng-liao\/DEC_pytorch) and [here](https:\/\/github.com\/vlukiyanov\/pt-dec\/blob\/master\/ptdec\/cluster.py)\n","903f9ea0":"Gonna do log transformation later to remove skewness and kurotorsis later","695263b0":"<a id=\"subsection-one\"><\/a>\n## Data processing","f52bd618":"Notice the light square RoomPrice x Total payment","f580d6df":"To do VIP predict, we need to remove everything that accumulate from multiple visits, keeping only averaging data with unique guestid that is equipvalent to single visit.","0e95920b":"### Inference","e4fc5929":"To start off, a very unbalance label set, with majority of label is group 1, and very few group 3. The indication is it's very hard to use as ground-truth for accuracy testing. I do use it, but it usefulness is limited.","a770a679":"We will drop Channel as there is not enough data","beb1eb55":"**Cluster with highest total pay is the VIP customers that we want**","0f8573ab":"Room group limits the price, mean is still the same around 200$"}}