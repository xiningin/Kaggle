{"cell_type":{"bd590565":"code","e191c2cd":"code","154fcb90":"code","151f8f28":"code","8eba3ee7":"code","778017ef":"code","12dd95af":"code","2025bbb2":"code","f6b891ac":"code","5bb62298":"code","5b773542":"code","a6652c8e":"code","f699ffab":"code","6fb3331e":"code","1bb06c29":"code","16cde90b":"code","76988e9f":"code","f2de8dbb":"code","f1a69389":"code","3c619286":"code","1ece0af3":"code","acfffe2a":"code","845f25c7":"code","b6580fe1":"code","bd421582":"code","3aedd4c9":"markdown","c6fe4572":"markdown"},"source":{"bd590565":"#Import some usefull libraries\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom keras import regularizers\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nimport math\nfrom sklearn.utils import shuffle\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\nfrom xgboost import XGBRegressor\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"..\/input\"))","e191c2cd":"#Importing the dataset to train our models, and the dataset to make predictions\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_pred = pd.read_csv('..\/input\/test.csv')","154fcb90":"#DATA INFORMATION - Lets see how the training data looks like\ndf_train.head(3)","151f8f28":"#DATA INFORMATION - Lets see how the prediction data looks like\ndf_pred.head(3)","8eba3ee7":"#I will define here some variables to use in the course of work.\ncopy_df_train = df_train.copy()\ncopy_df_pred = df_pred.copy()\ntarget = 'SalePrice'\nidentifier = 'Id'\ntest_size = 0.3\nfeature_transform = 1 #Flag variable\nNorm_Features = 1 #Flag variable\n#X_train : Will be the train data Matrix used to train our models\n#X_teste = Will be the test data Matrix used to test our models\n#X_pred = Will be the data Matrix used to make predicition and write them down to csv.","778017ef":"### Outliers ###\nvar = 'GrLivArea'\ndata = pd.concat([df_train[target], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y=target, ylim=(0,800000));\n\n#It looks like there are two point pretty off the curve SalePrice\/GrLivArea, lets remove them\ndf_train = df_train.drop(df_train.index[df_train.sort_values(by=['GrLivArea'],ascending=False)[:2].index])","12dd95af":"#Lets take a look at it without these outliers:\nvar = 'GrLivArea'\ndata = pd.concat([df_train[target], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y=target, ylim=(0,800000));","2025bbb2":"###Merge Datasets Before Preprocessing###\n#Define some usefull variables\ntrain_Id = df_train[identifier]\npred_Id = df_pred[identifier]\ntrain_target = df_train[target]\n# Merge Train and Prediction Dataset into df_merge. Drop 'Id' and 'SalePrice' columns before merge\ndf_merge = pd.concat([df_train.drop([target,identifier],1),df_pred.drop(identifier,1)],axis=0)\ndf_merge.info()\n","f6b891ac":"#Take a look of the missing variables values of the df_merge DataFrame\ntotal = df_merge.isnull().sum().sort_values(ascending=False)\npercent = (df_merge.isnull().sum()\/df_merge.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(40)","5bb62298":"###Dealing with missing data###\n#One approach to deal with missing data is to drop the colums with a lot of data \n#missing, wich in our case is PoolQC, MiscFeature, Alley and Fence, and then drop\n#some exmaple lines of the rest NaN's values. (which I dont like very much, we might miss a lot of information by doing that)\n\n#Another approach is to perform Imputation into the numerical features, \n#and get_dummies into the non-numeric variables.\n\n# Separate object and non-object features\nqualitative_features = [f for f in df_merge.dropna().columns if df_merge.dropna().dtypes[f] == 'object']\nquantitative_features = [f for f in df_merge.dropna().columns if df_merge.dropna().dtypes[f] != 'object']\n    \n############# NUMERIC FEATURES #############\nprint('\\nIs there any NaN value of numeric features in the dataset before Imputing?:',df_merge[quantitative_features].isnull().sum().any())\ndf_merge[quantitative_features].isnull().sum().sort_values(ascending=False)\ndf_merge['LotFrontage'] = df_merge['LotFrontage'].fillna(0)\ndf_merge['GarageYrBlt'] = df_merge['GarageYrBlt'].fillna(0)\ndf_merge['MasVnrArea'] = df_merge['MasVnrArea'].fillna(0)\n\nfor i in quantitative_features:\n    df_merge[i] = df_merge[i].fillna(df_merge[i].mean())\n    \n## Normalization of Numeric Features ###\ndf_merge[quantitative_features] = (df_merge[quantitative_features]-df_merge[quantitative_features].min())\/(\n        df_merge[quantitative_features].max()-df_merge[quantitative_features].min())\n## Normalization of Numeric Features ###\n\nprint('\\nIs there any NaN value of numeric features in the dataset after Imputing?:',df_merge[quantitative_features].isnull().sum().any())\n############# NUMERIC FEATURES #############\n\n############# NON-NUMERIC FEATURES #############\nprint('\\nIs there any NaN value of non-numeric features in the dataset before Imputing?:',df_merge[qualitative_features].isnull().sum().any())\ndf_merge[qualitative_features].isnull().sum().sort_values(ascending=False)\ndf_merge['PoolQC'] = df_merge['PoolQC'].fillna('None')\ndf_merge['MiscFeature'] = df_merge['MiscFeature'].fillna('None')\ndf_merge['Alley'] = df_merge['Alley'].fillna('None')\ndf_merge['BsmtQual'] = df_merge['BsmtQual'].fillna('None')\ndf_merge['BsmtCond'] = df_merge['BsmtCond'].fillna('None')\ndf_merge['BsmtExposure'] = df_merge['BsmtCond'].fillna('None')\ndf_merge['BsmtFinType1'] = df_merge['BsmtFinType1'].fillna('None')\ndf_merge['BsmtFinType2'] = df_merge['BsmtFinType2'].fillna('None')\ndf_merge['FireplaceQu'] = df_merge['FireplaceQu'].fillna('None')\ndf_merge['GarageType'] = df_merge['GarageType'].fillna('None')\ndf_merge['GarageFinish'] = df_merge['GarageFinish'].fillna('None')\ndf_merge['GarageCond'] = df_merge['GarageCond'].fillna('None')\ndf_merge['GarageQual'] = df_merge['GarageQual'].fillna('None')\ndf_merge['Fence'] = df_merge['Fence'].fillna('None')\ndf_merge['MasVnrType'] = df_merge['MasVnrType'].fillna('None')\ndf_merge[\"Functional\"] = df_merge[\"Functional\"].fillna(\"Typ\")\ndf_merge.drop('Utilities',axis=1,inplace=True)\nlast_qlfeat = ['MSZoning','KitchenQual','Exterior2nd','Exterior1st','Electrical','SaleType']\nqualitative_features = [f for f in df_merge.dropna().columns if df_merge.dropna().dtypes[f] == 'object'] #List of Non-Numeric Features\nquantitative_features = [f for f in df_merge.dropna().columns if df_merge.dropna().dtypes[f] != 'object'] #List of Numeric Features\n\nfor i in last_qlfeat:# Mode for the rest of non-numeric features\n    df_merge[i] =  df_merge[i].fillna(df_merge[i].mode()[0])\n\nprint('\\nIs there any NaN value  in the dataset after Imputing and get_dummies?:',df_merge.isnull().sum().any())\n############# NON-NUMERIC FEATURES #############\n\n### Labeling of Categorical Variables With Dummies\ndf_merge = pd.get_dummies(data=df_merge, columns=qualitative_features)","5b773542":"#### FEATURE TRANSFORM - LOG1\n#There are some skewed variables which we can perform some log transformation\nsns.distplot(df_train[target]) #SalePrice Distribution Plot\n#SalePrice looks like this before log transformation\n### FEATURE TRANSFORM - LOG1 of SalePrice ###\nif feature_transform:\n    train_target = np.log1p(train_target)\n\n","a6652c8e":"#SalePrice look like that after log transformation, much closer to a Normal Distr.\nsns.distplot(np.log1p(df_train[target]))","f699ffab":"# Restore datraframes df_train and df_pred\ndf_train = df_merge[:len(df_train)]\ndf_train[target] = train_target\ndf_pred = df_merge[len(df_train):]\ndf_train.info()","6fb3331e":"# Correlation Matrix to get a feeling of the correlations between variables on the dataset\ncorrmat = df_train.corr()\nk = 15 #number of variables for heatmap\ncols = corrmat.nlargest(k, target)[target].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nf, ax = plt.subplots(figsize=(12, 9))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","1bb06c29":"### Division between X_train,X_test,y_train,y_test\ndf_train = shuffle(df_train) #shuffle data before division\ntrain_target = df_train[target] # Just for code readibility\npredictors = df_train.drop([target], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(predictors, \n                                                    train_target,\n                                                    train_size=1-test_size, \n                                                    test_size=test_size, \n                                                random_state=0)\nX_pred = df_pred","16cde90b":"### Lasso Model ###\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n\nlasso = make_pipeline(RobustScaler(),LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n                          0.3, 0.6, 1, 3, 6, 10, 30, 60, 100], \n                max_iter = 50000, cv = 10))\nlasso.fit(X_train, y_train)\n\ny_train_las = lasso.predict(X_train)\ny_test_las = lasso.predict(X_test)\nlas_prediction = lasso.predict(X_pred)","76988e9f":"### Ridge Model ###\n#RobustScaler() to make the model robust against outliers\nridge = make_pipeline(RobustScaler(),RidgeCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n                          0.3, 0.6, 1, 3, 6, 10, 30, 60, 100]))\nridge.fit(X_train, y_train)\n\ny_train_rdg = ridge.predict(X_train)\ny_test_rdg = ridge.predict(X_test)\nrdg_prediction = ridge.predict(X_pred)","f2de8dbb":"### XGBoost Model through GridSearch ###\nxgb_model = XGBRegressor()\nparams = {'n_estimators':[1000],'learning_rate':[0.1,0.05],\n'max_depth':[3,4,5]}\ngrid = GridSearchCV(xgb_model, params)\ngrid.fit(X_train, y_train)\ny_train_gridXGB = grid.best_estimator_.predict(X_train)\ny_test_gridXGB = grid.best_estimator_.predict(X_test)\ngridXGB_prediction = grid.best_estimator_.predict(X_pred)","f1a69389":"### XGBoost Model without GridSearch ###\nxgb_model = XGBRegressor(n_estimators=10000, learning_rate=0.05)\n\nxgb_model.fit(X_train, y_train, early_stopping_rounds=5, \n             eval_set=[(X_test, y_test)], verbose=False)\n  \ny_train_xgb = xgb_model.predict(X_train)\ny_test_xgb = xgb_model.predict(X_test)\nxgb_prediction = xgb_model.predict(X_pred)","3c619286":"### ElasticNetRegression ###\nfrom sklearn.linear_model import ElasticNetCV, ElasticNet\n#ElasticNet Regressor\nelnr = make_pipeline(RobustScaler(),ElasticNetCV(l1_ratio=[0.2,0.65],alphas=[0.001,0.005],n_jobs=-1,max_iter=3000))\n\nelnr.fit(X_train,y_train)\n\ny_train_eln = elnr.predict(X_train)\ny_test_eln = elnr.predict(X_test)\neln_prediction = elnr.predict(X_pred)","1ece0af3":"### GradientBoost ###\nfrom sklearn.ensemble import GradientBoostingRegressor\n#With huber loss that makes it robust to outliers\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                    min_samples_split=10, \n                                   loss='huber', random_state =5)\n\nGBoost.fit(X_train,y_train)\n\nGBoost_train = GBoost.predict(X_train)\nGBoost_test = GBoost.predict(X_test)\nGBoost_prediction = GBoost.predict(X_pred)","acfffe2a":"### KernelRidge ###\nfrom sklearn.kernel_ridge import KernelRidge\nKR = KernelRidge(alpha=1.0)\nKR.fit(X_train, y_train) \n\ny_train_KR = KR.predict(X_train)\ny_test_KR = KR.predict(X_test)\nKR_prediction = KR.predict(X_pred)","845f25c7":"### LightGBM ###\nimport lightgbm as lgb\n# other scikit-learn modules\nestimator = lgb.LGBMRegressor(num_leaves=31)\n\nparam_grid = {\n    'learning_rate': [0.001,0.005, 0.01],\n    'n_estimators': [100,1000]}\n\ngbm = GridSearchCV(estimator, param_grid)\ngbm.fit(X_train, y_train)\n\ny_train_gbm = gbm.predict(X_train)\ny_test_gbm = gbm.predict(X_test)\ngbm_prediction = gbm.predict(X_pred)","b6580fe1":"### Results Compilation and Plots ###\nif feature_transform:\n    y_train = np.expm1(y_train)\n    y_test = np.expm1(y_test)\n\n    \n    y_train_las = np.expm1(y_train_las)\n    y_test_las = np.expm1(y_test_las)\n    las_prediction = np.expm1(las_prediction)\n    \n    y_train_rdg = np.expm1(y_train_rdg)\n    y_test_rdg = np.expm1(y_test_rdg)\n    rdg_prediction = np.expm1(rdg_prediction)\n    \n    y_train_xgb = np.expm1(y_train_xgb)\n    y_test_xgb = np.expm1(y_test_xgb)\n    xgb_prediction = np.expm1(xgb_prediction)\n    \n    y_train_gridXGB = np.expm1(y_train_gridXGB)\n    y_test_gridXGB = np.expm1(y_test_gridXGB)\n    gridXGB_prediction = np.expm1(gridXGB_prediction)\n\n    \n    y_train_eln = np.expm1(y_train_eln)\n    y_test_eln = np.expm1(y_test_eln)\n    eln_prediction = np.expm1(eln_prediction)\n    \n    GBoost_train = np.expm1(GBoost_train)\n    GBoost_test = np.expm1(GBoost_test)\n    GBoost_prediction = np.expm1(GBoost_prediction)\n    \n    y_train_KR = np.expm1(y_train_KR)\n    y_test_KR = np.expm1(y_test_KR)\n    KR_prediction = np.expm1(KR_prediction)\n    \n    y_train_gbm = np.expm1(y_train_gbm)\n    y_test_gbm = np.expm1(y_test_gbm)\n    gbm_prediction = np.expm1(gbm_prediction)\nelse:\n    pass\nfeature_transform=0\n\n\ndf_resultado_treinamento = pd.DataFrame(\n        {'x_train':list(X_train.values),\n         'y_train':list(y_train),\n         'y_train_lasso':y_train_las,\n         'y_train_ridge':y_train_rdg,\n         'y_train_xgb':y_train_xgb,\n         'y_train_gridXGB':y_train_gridXGB,\n         'y_train_eln':y_train_eln,\n         'GBoost_train':GBoost_train,\n         'y_train_KR':y_train_KR,\n         'y_train_gbm':y_train_gbm,\n                })\n    \ndf_resultado_val = pd.DataFrame(\n        {'x_val':list(X_test.values),\n         'y_val':list(y_test),  \n         'y_test_lasso':y_test_las,\n         'y_test_ridge':y_test_rdg,\n         'y_test_xgb':y_test_xgb,\n         'y_test_gridXGB':y_test_gridXGB,\n         'y_test_eln':y_test_eln,\n         'GBoost_test':GBoost_test,\n         'y_test_KR':y_test_KR,\n         'y_test_gbm':y_test_gbm,\n                })\n    \n\nplt.figure(1)\n\ndf_resultado_treinamento = df_resultado_treinamento.sort_values('y_train',ascending=1)\nplt.plot(list(df_resultado_treinamento['y_train']),'r-.',label='y_train')\nplt.plot(list(df_resultado_treinamento['y_train_lasso']),'g-.',label='LassoTrain')\nplt.plot(list(df_resultado_treinamento['y_train_ridge']),'y-.',label='RidgeTrain')\nplt.plot(list(df_resultado_treinamento['y_train_xgb']),'k-.',label='XGB')\nplt.legend()\n\n\nplt.figure(2)\ndf_resultado_val = df_resultado_val.sort_values('y_val',ascending=1)\nplt.plot(list(df_resultado_val['y_val']),'r-.',label='y_test')\nplt.plot(list(df_resultado_val['y_test_lasso']),'g-.',label='LassoTest')\nplt.plot(list(df_resultado_val['y_test_ridge']),'y-.',label='RidgeTest')\nplt.plot(list(df_resultado_val['y_test_xgb']),'k-.',label='XGB')\nplt.legend()\nplt.show()\n\n\n## Models Eval\n#### Root Mean Square Logarithmic Error ###\ndef logRMSE(y_test, y_pred) : \n    assert len(y_test) == len(y_pred)\n    return np.sqrt(np.mean((np.log(1+y_pred) - np.log(1+y_test))**2))\n\nprint(\"\\nLasso logRMSE on Training set :\", logRMSE(y_train,y_train_las))\nprint(\"Ridge logRMSE on Training set :\", logRMSE(y_train,y_train_rdg))\nprint(\"XGB logRMSE on Training set :\", logRMSE(y_train,y_train_xgb))\nprint(\"gridXGB logRMSE on Training set :\", logRMSE(y_train,y_train_gridXGB))\n\n\n\nprint(\"\\nLasso logRMSE on Test set :\", logRMSE(y_test,y_test_las))\nprint(\"Ridge logRMSE on Test set :\", logRMSE(y_test,y_test_rdg))\nprint(\"XGB logRMSE on Test set :\", logRMSE(y_test,y_test_xgb))\nprint(\"gridXGB logRMSE on Training set :\", logRMSE(y_test,y_test_gridXGB))\nprint(\"eln logRMSE on Test set :\", logRMSE(y_test,y_test_eln))\nprint(\"GBoost logRMSE on Test set :\", logRMSE(y_test,GBoost_test))\nprint(\"KR logRMSE on Test set :\", logRMSE(y_test,y_test_KR))\nprint(\"LightGBM logRMSE on Test set :\", logRMSE(y_test,y_test_gbm))\n\n\n\nprint(\"StackedModel logRMSE on Test set :\", logRMSE(y_test,(y_test_xgb+y_test_rdg+y_test_las + y_test_gridXGB\n                                                            +y_test_eln + GBoost_test +y_test_KR +y_test_gbm)\/(len(df_resultado_val.columns)-2)))\n######################################### Results Compilation and Plots #################################################  \n\n\n#########################################################################################################################\n# Writing dataset for submission\ndf_out = pd.DataFrame(\n        {identifier:pred_Id,\n        target: (xgb_prediction + las_prediction + rdg_prediction + gridXGB_prediction\n                   +eln_prediction + GBoost_prediction +KR_prediction +gbm_prediction )\/(len(df_resultado_val.columns)-2)\n                })","bd421582":"\ndf_out = pd.DataFrame(\n        {identifier:pred_Id,\n        target: (xgb_prediction + las_prediction + rdg_prediction + gridXGB_prediction\n                   +eln_prediction + GBoost_prediction +KR_prediction +gbm_prediction )\/(len(df_resultado_val.columns)-2)\n                })\ndf_out.to_csv('House_Prices_submition.csv', sep=',', index = False)","3aedd4c9":"**Descomplicated Stacked  Regression Kernel, Top 19%**\n\n[https:\/\/www.kaggle.com\/wesleyjr01](https:\/\/www.kaggle.com\/wesleyjr01)\n\n29 of August, 2018.\n\nThis kernel is for you if you want to quickly develop a solution for a regression problem like this one, with a reasonably good result. Many other kernels helped me build my own solution, so for deeper analysis of the topics I will create, some of these kernels can definately be usefull for you:\n1. The *Serigne's* kernel [Stacked Regressions to predict House Prices](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) gave me a view of the Stacked method. In this kernel I used 8 different regression algorithms to stack.\n2. The *DanB's* kernel [Handling Missing Values](https:\/\/www.kaggle.com\/dansbecker\/handling-missing-values)  gave me a good initial perspective of the missing values problem, as well as a good enginnering feature tip to keep track of the missing values by creating another variables.\n3. The *Alexandru Papiu's* kernel [Regularized Linear Models](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models) is a great kernel to get introducted to Lasso and Ridge regressions, as well as some insight on log transformation in some skewed variables.\n4. The *Pedro Marcelino's* kernel [COMPREHENSIVE DATA EXPLORATION WITH PYTHON](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) is a great data analisys kernel, gave me insights of the data visualization, and a clear aproach to Outliers.\n5.  At last, [Data Science Glossary on Kaggle !](https:\/\/www.kaggle.com\/shivamb\/data-science-glossary-on-kaggle\/notebook) is an amazing Data Scient glossary with many examples of ML algorithms you can see and learn from it.\n\n**Allright, lets get it started!**\n\n\n","c6fe4572":"As you can see, **the stacked result performs better than the individuals**. With this stacked submission, I scored **0.12151** on the public general score, **top 20%** of ranking. I hope this was usefull for you, enjoy!"}}