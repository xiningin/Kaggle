{"cell_type":{"015a5981":"code","c41ab2fd":"code","692efdef":"code","076fd3cb":"code","f049246c":"code","148e2d40":"code","9ad1a4a8":"code","4988e4a5":"code","5efcbb1f":"code","9b4181c2":"code","06f9876b":"code","ca9fae59":"code","788a5286":"code","d86f3bf8":"code","42cd8308":"code","ed36a54b":"code","36ba2535":"code","eb4ca5d2":"code","8e0089e7":"code","47ccc794":"code","b631099b":"code","36d3a786":"code","6befe5e7":"code","94c5c9d8":"code","609657db":"code","ec3b38e3":"code","0dbc315c":"code","82733902":"code","baa4f2b9":"code","ccedacdc":"code","0d0454e0":"code","316a1939":"code","1dd41b8a":"code","5e8fb302":"code","300f48df":"code","273a0026":"code","23a0c4ce":"code","4f78de95":"code","b85dbd69":"code","f5782251":"code","6fafd935":"code","a6983cf5":"code","7975dc04":"code","6793ec0c":"code","45672390":"code","b55fcf63":"code","de9c36a6":"code","59547c22":"code","59cf91cd":"code","b98aa95e":"code","708a6ac4":"code","053bcccc":"code","1281adad":"code","d0ed7d26":"code","6eb55557":"code","c24b1ec0":"code","7b4c9b63":"code","2e642a80":"code","11f7d040":"code","00ddf84a":"code","f1b3344e":"code","cb07df36":"code","15066489":"code","482ea52e":"code","f00657b2":"code","5399148a":"code","552e513b":"code","d8eaf69d":"code","2c8edab0":"code","ca721975":"code","ca7a691f":"code","ae757fbe":"code","b8e28d92":"code","5fa96c23":"code","79a9e2d1":"code","fd1ffd2f":"code","3bd27844":"code","0a819631":"code","76f85e5e":"code","aae2f9f2":"code","1be7beeb":"code","34bd3cc4":"code","27b78782":"code","28b1fb41":"code","ae176b4d":"code","73f47185":"code","e4a80fa8":"code","02af8a2d":"code","0a10eedf":"code","99536c40":"code","1cf0342f":"code","d3861d5c":"code","1fe5af7b":"code","a2b4f2d1":"code","f6108a41":"code","9dfecc8c":"code","2b4f9d40":"code","7f8751ee":"code","95c5e1f0":"code","8f79132a":"code","2ebb18aa":"code","6e3ec87c":"code","fc81bbed":"code","01be9238":"code","89df8288":"code","9d243341":"code","a0a981dc":"code","2b629eda":"code","2205317d":"code","cdd83cb0":"code","5ec625f5":"code","505e201c":"code","dca6d480":"code","7f21e279":"code","91d34425":"code","0afb2674":"code","81039020":"code","2d6a8d08":"code","70132a96":"code","a85d8c79":"code","c3c3a00e":"code","ca12d9d8":"markdown","10abb0d7":"markdown","1c87d482":"markdown","72f42e09":"markdown","5ab51e50":"markdown","2d9db729":"markdown","f7692e41":"markdown","62278ddb":"markdown","cb5865a1":"markdown","53c8e7aa":"markdown","71bc493f":"markdown","dd1e1cdc":"markdown","24bb4812":"markdown","0e7a1b05":"markdown","3a449f8b":"markdown","0503404a":"markdown","1ee81691":"markdown","2b61e453":"markdown","4d0d35a0":"markdown","a596514c":"markdown","2735c993":"markdown","236b4310":"markdown","10f9ef1d":"markdown","a896f062":"markdown","1af3b114":"markdown","d2729898":"markdown","e528cacc":"markdown","4031e9f6":"markdown","c49a4691":"markdown","2902a1db":"markdown","72e6f1ef":"markdown","fa4baa85":"markdown","16b92999":"markdown","65c379d8":"markdown","1dc27961":"markdown","62602449":"markdown","df09a584":"markdown","f3fe7305":"markdown","c001798c":"markdown","e9639032":"markdown","eb3e8b62":"markdown","1e6706ca":"markdown","54588420":"markdown","7d24bf80":"markdown","ea0ac3f5":"markdown","df47d549":"markdown","1dd96d75":"markdown","9ff3ce5f":"markdown","43d58c9e":"markdown","545d2ffc":"markdown","9123b4d0":"markdown","36c485dd":"markdown","66a206a8":"markdown","00428356":"markdown","60ac4a7b":"markdown","17a09114":"markdown","3512148f":"markdown","78d554ea":"markdown","ce176075":"markdown","e9d1554e":"markdown","ba83a276":"markdown","9ff8d42f":"markdown","983fd783":"markdown","5fbce00f":"markdown","be90e653":"markdown","a8b5d13b":"markdown","8aae83a9":"markdown","ccf1c619":"markdown","0246f613":"markdown","945941ed":"markdown","3b8b2a7c":"markdown","d6c95dd3":"markdown","f30ecf67":"markdown","aa6ea819":"markdown","0e598406":"markdown","e1297f52":"markdown"},"source":{"015a5981":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c41ab2fd":"import numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib\n%matplotlib inline\nmatplotlib.rcParams['figure.figsize'] = (8,5)","692efdef":"# that's it\nmssing_values=['\\t43' , '\\t?' , '\\tno' , '\\tyes' , ' yes', np.nan , 'NaN', 'ckd\\t']\n# data\ndata=pd.read_csv('..\/input\/ckdisease\/kidney_disease.csv', na_values=mssing_values)","076fd3cb":"data.shape","f049246c":"data.columns","148e2d40":"data.head()","9ad1a4a8":"data.tail()","4988e4a5":"data.info()","5efcbb1f":"data.describe()","9b4181c2":"data.duplicated().sum()","06f9876b":"cols =data.columns\ncolours = ['#000099', '#ffff00'] # specify the colours - yellow is missing. blue is not missing.\nsns.heatmap(data[cols].isnull(), cmap=sns.color_palette(colours))","ca9fae59":"data.isnull().sum()","788a5286":"# Here I calculate the percentage of missing data in each column.\n# % of missing.\nfor col in data.columns:\n    pct_missing = np.mean(data[col].isnull())\n    print('{} - {}%'.format(col, round(pct_missing*100)))","d86f3bf8":"data_2=data.copy()","42cd8308":"age_mean = data_2.age.mean()\nage_median = data_2.age.median()\ndata_2['age_mean'] = data_2.age.fillna(age_mean)\ndata_2['age_median'] = data_2.age.fillna(age_median)\ndata_2['new_age'] = data_2['age'].fillna(method=\"ffill\")\n#Now let's draw the new columns\nsns.kdeplot(data_2['age_mean'],color='red',label='Mean')\nsns.kdeplot(data_2['age_median'],color='blue',label='Median')\nsns.kdeplot(data_2['new_age'] ,color='yellow',label='ffill')\nsns.kdeplot(data_2['age'],color='black',label='Original')\nplt.legend()","ed36a54b":"data['age']=data_2['new_age']","36ba2535":"bp_mean = data_2.bp.mean()\nbp_median = data_2.bp.median()\ndata_2['bp_mean'] = data_2.bp.fillna(bp_mean)\ndata_2['bp_median'] = data_2.bp.fillna(bp_median)\ndata_2['new_bp'] = data_2['bp'].fillna(method=\"ffill\")\n#Now let's draw the new columns\nsns.kdeplot(data_2['bp_mean'],color='red',label='Mean')\nsns.kdeplot(data_2['bp_median'],color='blue',label='Median')\nsns.kdeplot(data_2['new_bp'] ,color='yellow',label='ffill')\nsns.kdeplot(data_2['bp'],color='black',label='Original')\nplt.legend()","eb4ca5d2":"data['bp']=data_2['new_bp']","8e0089e7":"sg_mean = data_2.sg.mean()\nsg_median = data_2.sg.median()\ndata_2['sg_mean'] = data_2.sg.fillna(sg_mean)\ndata_2['sg_median'] = data_2.sg.fillna(sg_median)\ndata_2['new_sg'] = data_2['sg'].fillna(method=\"ffill\")\ndata_2['sg_new'] = data_2['sg'].fillna(method=\"ffill\")\n#Now let's draw the new columns\nsns.kdeplot(data_2['sg_mean'],color='red',label='Mean')\nsns.kdeplot(data_2['sg_median'],color='blue',label='Median')\nsns.kdeplot(data_2['new_sg'] ,color='yellow',label='ffill')\nsns.kdeplot(data_2['sg_new'] ,color='green',label='ffill')\nsns.kdeplot(data_2['sg'],color='black',label='Original')\nplt.legend()","47ccc794":"data['sg']=data_2['new_sg']","b631099b":"al_mean = data_2.al.mean()\nal_median = data_2.al.median()\ndata_2['al_mean'] = data_2.al.fillna(al_mean)\ndata_2['al_median'] = data_2.al.fillna(al_median)\ndata_2['new_al'] = data_2['al'].fillna(method=\"ffill\")\ndata_2['al_new'] = data_2['al'].fillna(method=\"ffill\")\n#Now let's draw the new columns\nsns.kdeplot(data_2['al_mean'],color='red',label='Mean')\nsns.kdeplot(data_2['al_median'],color='blue',label='Median')\nsns.kdeplot(data_2['new_al'] ,color='yellow',label='ffill')\nsns.kdeplot(data_2['al_new'] ,color='green',label='ffill')\nsns.kdeplot(data_2['al'],color='black',label='Original')\nplt.legend()","36d3a786":"data['al']=data_2['new_al']","6befe5e7":"su_mean = data_2.su.mean()\nsu_median = data_2.su.median()\ndata_2['su_mean'] = data_2.su.fillna(su_mean)\ndata_2['su_median'] = data_2.su.fillna(su_median)\ndata_2['new_su'] = data_2['su'].fillna(method=\"ffill\")\ndata_2['su_new'] = data_2['su'].fillna(method=\"ffill\")\n#Now let's draw the new columns\nsns.kdeplot(data_2['su_mean'],color='red',label='Mean')\nsns.kdeplot(data_2['su_median'],color='blue',label='Median')\nsns.kdeplot(data_2['new_su'] ,color='yellow',label='ffill')\nsns.kdeplot(data_2['su_new'] ,color='green',label='ffill')\nsns.kdeplot(data_2['su'],color='black',label='Original')\nplt.legend()","94c5c9d8":"data['su']=data_2['new_su']","609657db":"bgr_mean = data_2.bgr.mean()\nbgr_median = data_2.bgr.median()\ndata_2['bgr_mean'] = data_2.bgr.fillna(bgr_mean)\ndata_2['bgr_median'] = data_2.bgr.fillna(bgr_median)\ndata_2['new_bgr'] = data_2['bgr'].fillna(method=\"ffill\")\ndata_2['bgr_new'] = data_2['bgr'].fillna(method=\"ffill\")\n#Now let's draw the new columns\nsns.kdeplot(data_2['bgr_mean'],color='red',label='Mean')\nsns.kdeplot(data_2['bgr_median'],color='blue',label='Median')\nsns.kdeplot(data_2['new_bgr'] ,color='yellow',label='ffill')\nsns.kdeplot(data_2['bgr_new'] ,color='green',label='ffill')\nsns.kdeplot(data_2['bgr'],color='black',label='Original')\nplt.legend()","ec3b38e3":"data['bgr']=data_2['bgr_new']","0dbc315c":"bu_mean = data_2.bu.mean()\nbu_median = data_2.bu.median()\ndata_2['bu_mean'] = data_2.bu.fillna(bu_mean)\ndata_2['bu_median'] = data_2.bu.fillna(bu_median)\ndata_2['new_bu'] = data_2['bu'].fillna(method=\"ffill\")\ndata_2['bu_new'] = data_2['bu'].fillna(method=\"ffill\")\n#Now let's draw the new columns\nsns.kdeplot(data_2['bu_mean'],color='red',label='Mean')\nsns.kdeplot(data_2['bu_median'],color='blue',label='Median')\nsns.kdeplot(data_2['new_bu'] ,color='yellow',label='ffill')\nsns.kdeplot(data_2['bu_new'] ,color='green',label='ffill')\nsns.kdeplot(data_2['bu'],color='black',label='Original')\nplt.legend()","82733902":"data['bu']=data_2['bu_mean']","baa4f2b9":"sc_mean = data_2.sc.mean()\nsc_median = data_2.sc.median()\ndata_2['sc_mean'] = data_2.sc.fillna(sc_mean)\ndata_2['sc_median'] = data_2.sc.fillna(sc_median)\ndata_2['new_sc'] = data_2['sc'].fillna(method=\"ffill\")\ndata_2['sc_new'] = data_2['sc'].fillna(method=\"ffill\")\n#Now let's draw the new columns\nsns.kdeplot(data_2['sc_mean'],color='red',label='Mean')\nsns.kdeplot(data_2['sc_median'],color='blue',label='Median')\nsns.kdeplot(data_2['new_sc'] ,color='yellow',label='ffill')\nsns.kdeplot(data_2['sc_new'] ,color='green',label='ffill')\nsns.kdeplot(data_2['sc'],color='black',label='Original')\nplt.legend()","ccedacdc":"data['sc']=data_2['sc_mean']","0d0454e0":"sod_mean = data_2.sod.mean()\nsod_median = data_2.sod.median()\ndata_2['sod_mean'] = data_2.sod.fillna(sod_mean)\ndata_2['sod_median'] = data_2.sod.fillna(sod_median)\ndata_2['new_sod'] = data_2['sod'].fillna(method=\"ffill\")\ndata_2['sod_new'] = data_2['sod'].fillna(method=\"ffill\")\n#Now let's draw the new columns\nsns.kdeplot(data_2['sod_mean'],color='red',label='Mean')\nsns.kdeplot(data_2['sod_median'],color='blue',label='Median')\nsns.kdeplot(data_2['new_sod'] ,color='yellow',label='ffill')\nsns.kdeplot(data_2['sod_new'] ,color='green',label='ffill')\nsns.kdeplot(data_2['sod'],color='black',label='Original')\nplt.legend()","316a1939":"data['sod']=data_2['sod_new']","1dd41b8a":"data['sod'].head(10)","5e8fb302":"data_2['sod']=data['sod']\nsod_mean = data_2.sod.mean()\ndata_2['sod_mean'] = data_2.sod.fillna(sod_mean)","300f48df":"data['sod']=data_2['sod_mean']\nprint(data['sod'].isnull().sum())","273a0026":"pot_mean = data_2.pot.mean()\npot_median = data_2.pot.median()\ndata_2['pot_mean'] = data_2.pot.fillna(pot_mean)\ndata_2['pot_median'] = data_2.pot.fillna(pot_median)\ndata_2['new_pot'] = data_2['pot'].fillna(method=\"ffill\")\ndata_2['pot_new'] = data_2['pot'].fillna(method=\"ffill\")\n#Now let's draw the new columns\nsns.kdeplot(data_2['pot_mean'],color='red',label='Mean')\nsns.kdeplot(data_2['pot_median'],color='blue',label='Median')\nsns.kdeplot(data_2['new_pot'] ,color='yellow',label='ffill')\nsns.kdeplot(data_2['pot_new'] ,color='green',label='ffill')\nsns.kdeplot(data_2['pot'],color='black',label='Original')\nplt.legend()","23a0c4ce":"data['pot']=data_2['pot_new']","4f78de95":"data['pot'].head()","b85dbd69":"data_2['pot']=data['pot']\npot_mean = data_2.pot.mean()\ndata_2['pot_mean'] = data_2.pot.fillna(pot_mean)","f5782251":"data['pot']=data_2['pot_mean']\nprint(data['pot'].isnull().sum())","6fafd935":"hemo_mean = data_2.hemo.mean()\nhemo_median = data_2.hemo.median()\ndata_2['hemo_mean'] = data_2.hemo.fillna(hemo_mean)\ndata_2['hemo_median'] = data_2.hemo.fillna(hemo_median)\ndata_2['new_hemo'] = data_2['hemo'].fillna(method=\"ffill\")\ndata_2['hemo_new'] = data_2['hemo'].fillna(method=\"ffill\")\n#Now let's draw the new columns\nsns.kdeplot(data_2['hemo_mean'],color='red',label='Mean')\nsns.kdeplot(data_2['hemo_median'],color='blue',label='Median')\nsns.kdeplot(data_2['new_hemo'] ,color='yellow',label='ffill')\nsns.kdeplot(data_2['hemo_new'] ,color='green',label='ffill')\nsns.kdeplot(data_2['hemo'],color='black',label='Original')\nplt.legend()","a6983cf5":"data['hemo']=data_2['hemo_new']","7975dc04":"categorical_data = data.select_dtypes(exclude=[np.number])\ncategorical_cols = categorical_data.columns.values\nprint(categorical_cols)","6793ec0c":"data['rbc'].isnull().sum()","45672390":"data['rbc'].value_counts()","b55fcf63":"data['rbc'].value_counts().plot.bar(color='green')","de9c36a6":"data['rbc'].fillna(method='ffill', inplace=True)","59547c22":"data['rbc'].isnull().sum()","59cf91cd":"data['rbc'].fillna('normal', inplace=True)","b98aa95e":"data['rbc'].value_counts()","708a6ac4":"data['pc'].isnull().sum()","053bcccc":"data['pc'].value_counts()","1281adad":"data['pc'].fillna(method='ffill', inplace=True)","d0ed7d26":"data['pc'].value_counts().plot.bar(color='green')","6eb55557":"data['pc'].value_counts()","c24b1ec0":"data['pcc'].isnull().sum()","7b4c9b63":"data['pcc'].value_counts()","2e642a80":"data['pcc'].fillna(method='ffill', inplace=True)","11f7d040":"data['pcc'].value_counts().plot.bar(color='green')","00ddf84a":"data['ba'].value_counts()","f1b3344e":"data['ba'].fillna(method='ffill', inplace=True)","cb07df36":"data['ba'].value_counts().plot.bar(color='green')","15066489":"data['htn'].isnull().sum()","482ea52e":"data['htn'].value_counts()","f00657b2":"data['htn'].fillna(method='ffill', inplace=True)","5399148a":"data['dm'].isnull().sum()","552e513b":"data['dm'].value_counts()","d8eaf69d":"data['dm'].fillna(method='ffill', inplace=True)","2c8edab0":"data['cad'].isnull().sum()","ca721975":"data['cad'].value_counts()","ca7a691f":"data['cad'].fillna(method='ffill', inplace=True)","ae757fbe":"data['appet'].value_counts()","b8e28d92":"data['appet'].fillna(method='ffill', inplace=True)","5fa96c23":"data['pe'].value_counts()","79a9e2d1":"data['pe'].fillna(method='ffill', inplace=True)","fd1ffd2f":"data['ane'].value_counts()","3bd27844":"data['ane'].fillna(method='ffill', inplace=True)","0a819631":"data['pcv'].isnull().sum()","76f85e5e":"data['pcv'].value_counts()","aae2f9f2":"pcv_mean = data_2.pcv.mean()\npcv_median = data_2.pcv.median()\ndata_2['pcv_mean'] = data_2.pcv.fillna(pcv_mean)\ndata_2['pcv_median'] = data_2.pcv.fillna(pcv_median)\ndata_2['new_pcv'] = data_2['pcv'].fillna(method=\"ffill\")\ndata_2['pcv_new'] = data_2['pcv'].fillna(method=\"ffill\")\n#Now let's draw the new columns\nsns.kdeplot(data_2['pcv_mean'],color='red',label='Mean')\nsns.kdeplot(data_2['pcv_median'],color='blue',label='Median')\nsns.kdeplot(data_2['new_pcv'] ,color='yellow',label='ffill')\nsns.kdeplot(data_2['pcv_new'] ,color='green',label='ffill')\nsns.kdeplot(data_2['pcv'],color='black',label='Original')\nplt.legend()","1be7beeb":"data['pcv']=data_2['pcv_new']","34bd3cc4":"wc_mean = data_2.wc.mean()\nwc_median = data_2.wc.median()\ndata_2['wc_mean'] = data_2.wc.fillna(wc_mean)\ndata_2['wc_median'] = data_2.wc.fillna(wc_median)\ndata_2['new_wc'] = data_2['wc'].fillna(method=\"ffill\")\ndata_2['wc_new'] = data_2['wc'].fillna(method=\"ffill\")\n#Now let's draw the new columns\nsns.kdeplot(data_2['wc_mean'],color='red',label='Mean')\nsns.kdeplot(data_2['wc_median'],color='blue',label='Median')\nsns.kdeplot(data_2['new_wc'] ,color='yellow',label='ffill')\nsns.kdeplot(data_2['wc_new'] ,color='green',label='ffill')\nsns.kdeplot(data_2['wc'],color='black',label='Original')\nplt.legend()","27b78782":"data['wc']=data_2['wc_new']","28b1fb41":"rc_mean = data_2.rc.mean()\nrc_median = data_2.rc.median()\ndata_2['rc_mean'] = data_2.rc.fillna(rc_mean)\ndata_2['rc_median'] = data_2.rc.fillna(rc_median)\ndata_2['new_rc'] = data_2['rc'].fillna(method=\"ffill\")\ndata_2['rc_new'] = data_2['rc'].fillna(method=\"ffill\")\n#Now let's draw the new columns\nsns.kdeplot(data_2['rc_mean'],color='red',label='Mean')\nsns.kdeplot(data_2['rc_median'],color='blue',label='Median')\nsns.kdeplot(data_2['new_rc'] ,color='yellow',label='ffill')\nsns.kdeplot(data_2['rc_new'] ,color='green',label='ffill')\nsns.kdeplot(data_2['rc'],color='black',label='Original')\nplt.legend()","ae176b4d":"data['rc']=data_2['rc_new']","73f47185":"data=data.drop('id' , axis=1)","e4a80fa8":"data['classification'].fillna(method='ffill', inplace=True)","02af8a2d":"data['classification'].value_counts()","0a10eedf":"ax = data[\"classification\"].value_counts().plot(kind='bar', figsize=(5, 6), fontsize=8, color='blue')\nax.set_title('target column ', size=20, pad=30)\nax.set_ylabel('Number of transactions', fontsize=14)\n\nfor i in ax.patches:\n    ax.text(i.get_x() + 0.19, i.get_height() + 70, str(round(i.get_height(), 2)), fontsize=15)","99536c40":"# select numeric columns\ndf_numeric = data.select_dtypes(include=[np.number])\nnumeric_cols = df_numeric.columns.values\nprint(numeric_cols)","1cf0342f":"# dealing with outliers values\nfor i in df_numeric.columns:\n    sns.boxplot(df_numeric[i])\n    plt.title(i)\n    plt.show()","d3861d5c":"# Here we perform an initial separation of the data, specifically the target column from the rest of the columns.\ntarget=data[\"classification\"]\nfeatures=data.drop([\"classification\"],axis=1)","1fe5af7b":"from sklearn.preprocessing import LabelEncoder\n# Here I made a transformation of the target column.\nobject_1=LabelEncoder()\ny=object_1.fit_transform(target)","a2b4f2d1":"# Here I do a conversion of categorical columns from the data\nobject_1=LabelEncoder()\n# During the conversion process, we used the first projection.\nfor i in features.select_dtypes(exclude=[np.number]).columns:\n    features[i] = object_1.fit_transform(data[i])","f6108a41":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(features)","9dfecc8c":"new_features = scaler.transform(features)\nprint(new_features)","2b4f9d40":"print(new_features.shape)","7f8751ee":"# Complete the data separation. \nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(new_features,y,test_size=0.1,random_state=0)","95c5e1f0":"print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)","8f79132a":"pip install lazypredict","2ebb18aa":"from lazypredict.Supervised import LazyClassifier","6e3ec87c":"model = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\nmodels,predictions = model.fit(x_train, x_test, y_train, y_test)","fc81bbed":"print(predictions)","01be9238":"#Import Libraries\nfrom sklearn.svm import SVC\n\nSVCModel = SVC(kernel= 'rbf',# it can be also linear,poly,sigmoid,precomputed\n               max_iter=100,C=1.0,gamma='auto')\nSVCModel.fit(x_train, y_train)","89df8288":"#Calculating Details\nprint('SVCModel Train Score is : ' , SVCModel.score(x_train, y_train))\nprint('SVCModel Test Score is : ' , SVCModel.score(x_test, y_test))","9d243341":"#Calculating Prediction\ny_pred = SVCModel.predict(x_test)\nprint('Predicted Value for SVCModel   is : ' , y_pred[:10])\nprint(\"The values we want to classify is : \" , y_test[:10])","a0a981dc":"#Calculating Confusion Matrix\nfrom sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix\nconfusion_matrix(y_test,y_pred)","2b629eda":"plot_confusion_matrix(SVCModel,x_test,y_test);","2205317d":"from sklearn.ensemble import ExtraTreesClassifier","cdd83cb0":"model = ExtraTreesClassifier(n_estimators=10, random_state=0, min_samples_leaf=1,max_features='log2',bootstrap=True, oob_score=True)","5ec625f5":"model.fit(x_train, y_train)","505e201c":"print('ExtraTreesClassifiermodel Train Score is : ' , model.score(x_train, y_train))","dca6d480":"print('ExtraTreesClassifiermodel Test Score is : ' , model.score(x_test, y_test))\nprint('ExtraTreesClassifiermodel features importances are : ' , model.feature_importances_)","7f21e279":"#Calculating Prediction\ny_predict = model.predict(x_test)\ny_predict_prob = model.predict_proba(x_test)\nprint('Predicted Value for ExtraTreesClassifiermodel is : ' , y_predict[:10])\nprint('The real values we want to classify ..........is : ' , y_test[:10])\nprint('Prediction Probabilities Value for ExtraTreesClassifiermodel is : ' , y_predict_prob[:10])","91d34425":"#Calculating Confusion Matrix\nfrom sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix\nconfusion_matrix=confusion_matrix(y_test,y_pred)\nconfusion_matrix","0afb2674":"total=sum(sum(confusion_matrix))\n\nsensitivity = confusion_matrix[0,0]\/(confusion_matrix[0,0]+confusion_matrix[1,0])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = confusion_matrix[1,1]\/(confusion_matrix[1,1]+confusion_matrix[0,1])\nprint('Specificity : ', specificity)","81039020":"plot_confusion_matrix(model,x_test,y_test);","2d6a8d08":"feat_importances = pd.Series(model.feature_importances_, index=features.columns)\nfeat_importances.nlargest(25).plot(kind='barh', color='red')","70132a96":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, precision_recall_curve, roc_curve, auc, average_precision_score","a85d8c79":"def beautiful(techniques):\n    print(techniques)\n    model_name = [\"RF\", \"ET\", \"DT\", 'GB', 'AB']\n    RFC = RandomForestClassifier(random_state = 0)\n    ETC = ExtraTreesClassifier(random_state = 0)\n    DTC = DecisionTreeClassifier(random_state = 0)\n    GBM = GradientBoostingClassifier(random_state = 0)\n    ABC = AdaBoostClassifier(random_state = 0)\n\n    for clf,i in zip([RFC, ETC, DTC, GBM, ABC], model_name):\n        model = clf.fit(x_train, y_train)\n        y_pred = model.predict(x_test)\n        y_pred_prob = model.predict_proba(x_test)[:,1]\n        print(\"#\"*25,i,\"#\"*25)\n        print(\"Training Accuracy = {:.3f}\".format(model.score(x_train, y_train)))\n        print(\"Test Accuracy = {:.3f}\".format(model.score(x_test, y_test)))\n        print(\"ROC_AUC_score : %.6f\" % (roc_auc_score(y_test, y_pred)))\n        #Confusion Matrix\n        #print(confusion_matrix =confusion_matrix(y_test, y_pred))\n        \n        print(\"-\"*15,\"CLASSIFICATION REPORT\",\"-\"*15)\n        print(classification_report(y_test, y_pred))\n        \n","c3c3a00e":"beautiful(\"techniques......\")","ca12d9d8":"# The columns that most affect the performance of the model.","10abb0d7":"# pc column","1c87d482":"**Once again, we note that the last technique is the most appropriate to use in order to compensate for the missing values.**","72f42e09":"**It seems that this column has an error in the data, I will fix it. Update We fixed it by adding the above code.**","5ab51e50":"**I've finished building some machine learning algorithms on this data.**","2d9db729":"# Target column check","f7692e41":"# cad column","62278ddb":"# Quick measurement of the performance of classification algorithms","cb5865a1":"# Data cleaning","53c8e7aa":"**From the figure, we notice that there is a convergence in the distribution between the first, third and fourth techniques, but at the end of the figure we notice that the most appropriate are the third and fourth, so we will use any of them.**","71bc493f":"# data splitting","dd1e1cdc":"**Here we have used many techniques in order to compensate for the missing values, and we note that the third and fourth technique is the most suitable in the distribution for the original column distribution and therefore we will use any of them.**","24bb4812":"# pot column","0e7a1b05":"# I'm back to finish what we started","3a449f8b":"# Data Description:\n**We use the following representation to collect the dataset**\n\n1. age - age\n2. bp - blood pressure\n3. sg - specific gravity\n4. al - albumin\n5. su - sugar\n6. rbc - red blood cells\n7. pc - pus cell\n8. pcc - pus cell clumps\n9. ba - bacteria\n10. bgr - blood glucose random\n11. bu - blood urea\n12. sc - serum creatinine\n13. sod - sodium\n14. pot - potassium\n15. hemo - hemoglobin\n16. pcv - packed cell volume\n17. wc - white blood cell count\n18. rc - red blood cell count\n19. htn - hypertension\n20. dm - diabetes mellitus\n21. cad - coronary artery disease\n22. appet - appetite\n23. pe - pedal edema\n24. ane - anemia\n25. class - class","0503404a":"# hemo column","1ee81691":"**I will isolate the numeric columns from the non-numeric columns in order to look for outliers.**","2b61e453":"# sod column","4d0d35a0":"# su column","a596514c":"**Now let's go back to processing the rest of the numeric columns. There are 3 columns left. We will work on them quickly.**","2735c993":"# data transformation","236b4310":"**Through this report, we can see that there are many powerful algorithms, and the results are very good.**","10f9ef1d":"**The data volume is small and therefore it is easy to fall into the problem of overfitting**","a896f062":"# ane column","1af3b114":"**We must deal with categorical data with a lot of sensitivity, and in the real problem, we will review the party responsible for collecting that data in order to review the missing values, so that we can obtain a way that we have done the compensation process more efficiently.**","d2729898":"**By looking at the results of the measurements, there is no need to draw the curve, as everyone knows the shape of the curve.**","e528cacc":"# pe column","4031e9f6":"# pcc column","c49a4691":"# rbc column","2902a1db":"**Now that we are done with the substitution phase for the empty values, let's move on to the next step.**","72e6f1ef":"# bp column","fa4baa85":"# rc column","16b92999":"# Duplicate data","65c379d8":"# Now I will start dealing with categorical data.","1dc27961":"**We have a lot of missing values, so I'm going to process those missing values in each column.**","62602449":"# al column","df09a584":"**From these results I think we will have a problem with outliers, especially since the data size is very small, but don't worry about the problem coast.**","f3fe7305":"# dm column","c001798c":"# The results we obtained.","e9639032":"**This is convenient, we do not have duplicate data, I will move to the next stage.**","eb3e8b62":"**But we know very well that the decision to get rid of outliers is a decision that must be a well-thought-out decision, and in most cases we consult specialists in the case of these data, we have to consult a specialist in the medical field, specifically Chronic KIdney Disease.**\n\n**But that process is not currently available to us because we are working on ready data, so we will leave that step as it is and move on to the next step.**","1e6706ca":"**Now I will implement some different algorithms based on decision trees, along with some new algorithms to see the results of each and put them in the context of comparison.**\n\n**All of this we will implement in one beautiful function.\nDuring which we will get many measurements and also we will draw two ROC_AUC_curve curves.**\n\n**Let's continue.....**","54588420":"**This is enough for now.**\n\n**Thank you, my friend, we have come a long way from the long journey, but we will not stop here, we will continue to progress, but we will take a rest, catch your breath my friend, we will continue the journey in a little while so be prepared.**","7d24bf80":"# reading data","ea0ac3f5":"**We note that the most appropriate distribution is the distribution for the second and third techniques, so I will use any of them.**","df47d549":"**I will work here to flush and get rid of duplicate data.**","1dd96d75":"**It appears that the data has a relatively good balance between the categories of the target column, and therefore we will not manipulate this natural balance.**","9ff3ce5f":"# htn column","43d58c9e":"# bgr column","545d2ffc":"**Through the schematics that we fortified, we notice the presence of extreme values,\nOutliers work to affect the algorithms that we will build negatively because the difference between the lowest value and the largest value in each column is a large difference and thus a dispersion of the model occurs.**","9123b4d0":"# missing values","36c485dd":"# data scalling","66a206a8":"**We have used 3 techniques to compensate for the missing values and we noticed that the last technique called ffill has the best congruent distribution curve on the original distribution curve.**\n\n**Therefore, it is best suited for this column and therefore we will use it in the original data.**","00428356":"# sg column","60ac4a7b":"**From observing the percentages, we note that there are 4 columns, the percentage of data loss in them exceeded the 20% barrier, and through prior data processing, we note that one of those 4 columns has a high percentage of impact on the results of the algorithms, so we will not risk those columns and we will work to compensate for Those missing values.**","17a09114":"**To make it easier, I will separate the categorical columns from the numerical columns.**","3512148f":"# appet column","78d554ea":"<h1 align=\"center\"> Chronic KIdney Disease classification<\/h1>\n<center><img src=\"https:\/\/2.bp.blogspot.com\/-EEEzJTbjNVo\/Wg6vBZW4NuI\/AAAAAAAAQy4\/j_lpCqAyk9wViYPnSMQ4VYdjQlIgcjflQCLcBGAs\/s1600\/chronic-kidney-disease.jpg\" width=\"60%\" >","ce176075":"# Let's start building machine learning models","e9d1554e":"# Get rid of outliers","ba83a276":"# wc column","9ff8d42f":"# Data balance check","983fd783":"# sc column","5fbce00f":"# General measurements for different models.","be90e653":"**You will know the importance of that little instruction if you delete it and continue running the code while activating the code that I put as comments.**","a8b5d13b":"# Chronic KIdney Disease","8aae83a9":"# ba column","ccf1c619":"# Import some important libraries","0246f613":"# SVC Algorithm","945941ed":"**I will do a lot at that stage my friend, I will clean and clean the data and prepare it in order to enter it into the algorithms that I will build.**","3b8b2a7c":"# bu column","d6c95dd3":"# Take a look at the data","f30ecf67":"# age column","aa6ea819":"# ExtraTreesClassifier Algorithm","0e598406":"**Now that we've looked at the data, let's take a step forward.**","e1297f52":"# pcv column"}}