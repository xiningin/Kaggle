{"cell_type":{"3e8a51a6":"code","d0540cc2":"code","5e815bf3":"code","df423e32":"code","46708db5":"code","c7f2285b":"code","afe4bb2f":"code","e804174b":"code","485ade5f":"code","d5e84459":"code","26a13704":"code","8e5b4b48":"code","02473b65":"code","f7135c53":"code","ad407301":"code","d0c19f88":"code","9bfec99c":"code","79e7eda0":"code","8b8cbb19":"code","4549e548":"code","6a596b44":"code","ac311e1b":"code","1d1889b9":"code","c2d97cfc":"code","d84d6224":"code","a16bf948":"code","35e4b147":"code","16577df9":"code","d168ec83":"code","79f9e3b4":"code","59ee8802":"code","6ce48500":"code","ebc42d40":"code","9a94b455":"code","a9901a8f":"code","41cba531":"code","23bda0ce":"code","dccb64de":"code","f83e13e3":"code","84e8ffdf":"code","50c0ffe7":"code","47d45f6e":"code","863f20fc":"code","653412e9":"code","51007822":"code","bbfb55f1":"code","2a0f7bbe":"code","90242198":"code","dfd19058":"code","4239982f":"markdown","f7693cf1":"markdown","03cdaaf2":"markdown","3557ddcf":"markdown","d5a98313":"markdown","cfbae567":"markdown","e7ee88b0":"markdown","8717ccc5":"markdown","7529c829":"markdown","33f2d155":"markdown","25596868":"markdown","f5ee8200":"markdown","47c7c756":"markdown","4c88a9ff":"markdown"},"source":{"3e8a51a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d0540cc2":"import pandas as pd\nimport numpy as np","5e815bf3":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno as msno","df423e32":"import plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n","46708db5":"data=pd.read_csv('..\/input\/police-violence-in-the-us\/juvenile_arrests.csv')","c7f2285b":"data.head()","afe4bb2f":"data.info()\ndata.describe().T","e804174b":"data.columns","485ade5f":"data.select_dtypes(include='object')","d5e84459":"data.isnull().sum()","26a13704":"data.shape","8e5b4b48":"data['Offense'].value_counts","02473b65":"labels = data['Offense'].value_counts().index\nvalues = data['1980'].value_counts().values\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label',\n                             insidetextorientation='radial')])\nfig.show()","f7135c53":"labels = data['Offense'].value_counts().index\nvalues = data['1985'].value_counts().values\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label',\n                             insidetextorientation='radial')])\nfig.show()","ad407301":"labels = data['Offense'].value_counts().index\nvalues = data['1990'].value_counts().values\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label',\n                             insidetextorientation='radial')])\nfig.show()","d0c19f88":"sns.set_context(\"talk\")\nplt.style.use('fivethirtyeight')\nplt.figure(figsize = (12,12))\nax = sns.barplot(y = 'Offense' , x = '2000', data = data, palette = 'mako', edgecolor = 'black')\nax.set_title('Minimum Wages (Yearly)' , size = 20, pad = 20)\nfor p in ax.patches:\n        ax.annotate(\"%.f\" % p.get_width(), xy=(p.get_width(), p.get_y()+p.get_height()\/2),\n            xytext=(5, 0), textcoords='offset points', ha=\"left\", va=\"center\")","9bfec99c":"sns.set_context(\"talk\")\nplt.style.use('fivethirtyeight')\nplt.figure(figsize = (12,12))\nax = sns.barplot(y = 'Offense' , x = '2005', data = data, palette = 'mako', edgecolor = 'black')\nax.set_title('Minimum Wages (Yearly)' , size = 20, pad = 20)\nfor p in ax.patches:\n        ax.annotate(\"%.f\" % p.get_width(), xy=(p.get_width(), p.get_y()+p.get_height()\/2),\n            xytext=(5, 0), textcoords='offset points', ha=\"left\", va=\"center\")","79e7eda0":"sns.set_context(\"talk\")\nplt.style.use('fivethirtyeight')\nplt.figure(figsize = (12,12))\nax = sns.barplot(y = 'Offense' , x = '2010', data = data, palette = 'mako', edgecolor = 'black')\nax.set_title('Minimum Wages (Yearly)' , size = 20, pad = 20)\nfor p in ax.patches:\n        ax.annotate(\"%.f\" % p.get_width(), xy=(p.get_width(), p.get_y()+p.get_height()\/2),\n            xytext=(5, 0), textcoords='offset points', ha=\"left\", va=\"center\")","8b8cbb19":"sns.set_context(\"talk\")\nplt.style.use('fivethirtyeight')\nplt.figure(figsize = (12,12))\nax = sns.barplot(y = 'Offense' , x = '2008', data = data, palette = 'mako', edgecolor = 'black')\nax.set_title('Minimum Wages (Yearly)' , size = 20, pad = 20)\nfor p in ax.patches:\n        ax.annotate(\"%.f\" % p.get_width(), xy=(p.get_width(), p.get_y()+p.get_height()\/2),\n            xytext=(5, 0), textcoords='offset points', ha=\"left\", va=\"center\")","4549e548":"# Missing values\ndef missing_values_table(data):\n        # Total missing values\n        mis_val = data.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * data.isnull().sum() \/ len(data)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing==>descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(data.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns\n\n\nmissing_values= missing_values_table(data)\nmissing_values.style.background_gradient(cmap='Reds')  ","6a596b44":"data['Category'].value_counts","ac311e1b":"import plotly.express as px","1d1889b9":"missing_value = pd.DataFrame(data.isnull().sum(),columns =[\"counts\"])\nmissing_value = missing_value[missing_value.counts > 0]\npx.bar(data_frame = missing_value,x = missing_value.index ,y = \"counts\")","c2d97cfc":"median_value=data['2010'].median()\ndata['2010']=data['2010'].fillna(median_value)","d84d6224":"data['2010'].isnull().sum()","a16bf948":"median_value=data['2011'].median()\ndata['2011']=data['2011'].fillna(median_value)\n\nmedian_value=data['2012'].median()\ndata['2012']=data['2012'].fillna(median_value)\n\nmedian_value=data['2013'].median()\ndata['2013']=data['2013'].fillna(median_value)\n\nmedian_value=data['2014'].median()\ndata['2014']=data['2014'].fillna(median_value)\n\nmedian_value=data['2015'].median()\ndata['2015']=data['2015'].fillna(median_value)\n\nmedian_value=data['2016'].median()\ndata['2016']=data['2016'].fillna(median_value)\n\nmedian_value=data['2017'].median()\ndata['2017']=data['2017'].fillna(median_value)\n\nmedian_value=data['2018'].median()\ndata['2018']=data['2018'].fillna(median_value)","35e4b147":"plt.figure(figsize=(15,6))\nsns.heatmap(data.isnull(),yticklabels=False,cbar=False,cmap='viridis')","16577df9":"Offense = data['Offense'].value_counts()\nprint(f'Total number of crimes in the dataset : {len(Offense)}')\nprint(Offense.index)","d168ec83":"data['Decade1'] = data['1980']+data['1981']+data['1982']+data['1983']+data['1984']+data['1985']+data['1986']+data['1987']+data['1988']+data['1989']+data['1990']","79f9e3b4":"data.drop(['1980','1981','1982','1983','1984','1985','1986','1987','1988','1989','1990',],inplace=True,axis=1)","59ee8802":"data.head()","6ce48500":"data['Decade2'] = data['1991']+data['1992']+data['1993']+data['1994']+data['1995']+data['1996']+data['1997']+data['1998']+data['1999']+data['2000']\n","ebc42d40":"data.drop(['1991','1992','1993','1994','1995','1996','1997','1998','1999','2000',],inplace=True,axis=1)","9a94b455":"data.head()","a9901a8f":"data['Decade3'] = data['2001']+data['2002']+data['2003']+data['2004']+data['2005']+data['2006']+data['2007']+data['2008']+data['2009']+data['2010']\n","41cba531":"data.drop(['2001','2002','2003','2004','2005','2006','2007','2008','2009','2010',],inplace=True,axis=1)","23bda0ce":"data.head()","dccb64de":"data['Decade4'] = data['2011']+data['2012']+data['2013']+data['2014']+data['2015']+data['2016']+data['2017']+data['2018']\n","f83e13e3":"data.drop(['2011','2012','2013','2014','2015','2016','2017','2018'],inplace=True,axis=1)","84e8ffdf":"data.head()","50c0ffe7":"data.drop(['Category'],inplace=True,axis=1)\ndata.head()","47d45f6e":"data[data.Offense =='Drunkenness'].Decade1.mean()","863f20fc":"data[data.Offense =='Drunkenness'].Decade2.mean()","653412e9":"data[data.Offense =='Drunkenness'].Decade3.mean()","51007822":"data[data.Offense =='Drunkenness'].Decade4.mean()","bbfb55f1":"data.groupby('Offense').Decade1.agg(['count','max','min','mean'])\n","2a0f7bbe":"data.groupby('Offense').Decade2.agg(['count','max','min','mean'])","90242198":"data.groupby('Offense').Decade3.agg(['count','max','min','mean'])","dfd19058":"data.groupby('Offense').Decade4.agg(['count','max','min','mean'])","4239982f":" We have dropped the columns from 1980 to 1990 and clubbed them under Decade 1 for our convenience","f7693cf1":"# Let us look at the different types of crimes for which there have been juvenile arrests","03cdaaf2":"data=pd.read_csv('..\/input\/police-violence-in-the-us')","3557ddcf":" Since the time frame oscillaytes from 1980 to 2018, i.e 38 columns. \n The number of features are more, which leads to over fitting.\n \nHence, we decide to categorise it into 4 decades (Decade1,2,3,4).\nDecade is a 10 year time period\n\nWe shall henceforth, perform Decade-wise Analysis","d5a98313":"# we have missing values from 2010 to 2018.\n# 7 missing values\/column from 2010 to 2012 (9.5%)\n# 14 missing values\/column from 2013 to 2018 (4.8%)\n\n","cfbae567":"# It would have been more helpful if they had specified 'Gender' as an attribute ","e7ee88b0":"> We should impute the missing values with median","8717ccc5":"# From the above charts, it is visible that the cases of 'Drunkenness' are steadily increasing with 4.76% in 1980, 8.7% in 1985 and 9.09% in 1990.\n\n# The rate of Forcible Rape and Drug abuse violations cases are steady at 4.55% for the above years.\n\n","7529c829":"# The year of 2008 witnessed a massive global housing crisis and the results are visible. Surprisingly drunkenness have now taken a back seat|","33f2d155":"# **We shall now perform the analysis on the newly formed dataset","25596868":"# We have filled the null values with median and rechecked for 2010","f5ee8200":"# We analysed Drunkenness for all the 4 decades and to our surprise, the mean juvenile arrests declined 7x","47c7c756":"# For the years 2000, 2005 and 2010 'Property Crimes' saw a steady rate and fail to deplete","4c88a9ff":"# The columns have now been clubbed under 4 decades. We shall delete the category column now"}}