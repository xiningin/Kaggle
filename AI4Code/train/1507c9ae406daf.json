{"cell_type":{"bdd3d4d0":"code","3e54aeea":"code","0ee68f93":"code","67ee1a1f":"code","dbbd84be":"code","21d1aa62":"code","0e66abc6":"code","e688bf86":"code","805df640":"code","c36e944e":"code","cab053ef":"code","144dc505":"code","0c180e46":"code","b1f4d4d6":"code","bfe62996":"code","87ea0311":"code","44098566":"code","843263eb":"code","da78a7b2":"code","8c40752e":"code","ef27e127":"code","a6dad355":"code","73d25770":"code","b3b7a414":"code","0a122738":"code","5f1d122f":"code","a093960b":"code","a060f37f":"code","d05b9951":"code","1ee8e5bc":"code","f107e92b":"code","75d497a7":"code","d10a9049":"code","de014b21":"code","5570a56e":"code","bf82ba53":"code","ad7f36f5":"code","e621242f":"code","3d628554":"code","b4438489":"code","9504e82a":"code","a3b9a49d":"code","6e294044":"code","3aeef734":"code","ef6838d2":"code","84636d39":"markdown","debab9f6":"markdown","de1d6174":"markdown","1f6ce7da":"markdown","f407c96b":"markdown","66351c0a":"markdown","b5aa8cb6":"markdown","7b3dc1ef":"markdown","c1e8934e":"markdown","3c8a30a3":"markdown","b1d4a321":"markdown","bf32f643":"markdown","3b008fd7":"markdown","10ded2ba":"markdown","4f321853":"markdown","f29321f6":"markdown"},"source":{"bdd3d4d0":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport seaborn as sns\nfrom shapely.geometry import Point","3e54aeea":"schools = pd.read_csv('Data\/2016 School Explorer.csv')\nschools['School Name'] = schools['School Name'].str.lower()\npercentages = [c for c in schools.columns if '%' in c or 'Percent' in c or 'Rate' in c]\nfor c in percentages:\n    schools[c] = schools[c].apply(lambda x: x if pd.isnull(x) else int(x[:-1]))\nschools['School Income Estimate'] = schools['School Income Estimate'].apply(lambda x: x if pd.isnull(x) else float(x[1:].replace(',','')))\nschools['Community School?'] = schools['Community School?'].apply(lambda x: 1 if x == 'Yes' else 0)\nschools['Location'] = schools.apply(lambda x: Point(x['Longitude'], x['Latitude']), axis=1)","0ee68f93":"geo_schools = gpd.GeoDataFrame(schools).set_geometry('Location')\nschool_types = geo_schools.groupby('Community School?')\nreg_schools = school_types.get_group(0)\ncom_schools = school_types.get_group(1)\nnyc = gpd.read_file('Data\/nynta_18b\/nynta.shp').set_geometry('geometry').to_crs({'init' :'epsg:4326'})\n","67ee1a1f":"fig, ax = plt.subplots(figsize=(10,10))\nnyc.plot(ax=ax, color='white',linewidth=1,edgecolor='black',alpha=.5)\nreg_schools.plot(ax=ax, color='blue', markersize=2, label='Regular Schools')\ncom_schools.plot(ax=ax, color='red', markersize=2, label='Community Schools')\nfig.suptitle('Public and Community Elementary and Middle Schools in NYC', size=16)\nplt.legend(loc='upper left',prop={'size': 16});","dbbd84be":"xaxis_labels = ['Percent Asian','Percent White','Percent Hispanic', 'Percent Black',\n                'Percent Black \/ Hispanic', 'Percent ELL']\nf, axs = plt.subplots(2,3,figsize=(24, 18))\nfor i in range(1, 7):\n    plt.subplot(2, 3, i)\n    plt.hist(schools[xaxis_labels[i-1]], bins=30)\n    plt.xlabel(xaxis_labels[i-1],size=14)\n    plt.ylabel('Number of Schools');","21d1aa62":"def point_to_nta(pt, ntas):\n    for ix, row in ntas.iterrows():\n        if pt.within(row['geometry']):\n            return row['NTACode']\nschools['nta'] = schools.apply(lambda x: point_to_nta(Point(x['Longitude'], x['Latitude']), nyc), axis=1)\nnta_school_demogs = schools.groupby('nta').mean()[xaxis_labels]\nnyc = nyc.set_index('NTACode')\nnta_school_demogs['geometry'] = nyc['geometry']\nnta_school = gpd.GeoDataFrame(nta_school_demogs).set_geometry('geometry')","0e66abc6":"schools[xaxis_labels].describe()","e688bf86":"fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(nrows=3, ncols=2, figsize=(24, 20))\nfig.subplots_adjust(hspace=.05, wspace=.05)\nax1 = nta_school.plot(ax=ax1,cmap='gist_gray_r', linewidth=2,edgecolor='black')\nnta_school.plot(ax=ax1, column= 'Percent Black', cmap='Reds', vmin=0, vmax=100)\nax1.set_axis_off()\nax1.set_title('Percent Black',size=16)\nax1 = nta_school.plot(ax=ax2,cmap='gist_gray_r', linewidth=2,edgecolor='black')\nnta_school.plot(ax=ax2, column= 'Percent Hispanic', cmap='Reds', vmin=0, vmax=100)\nax1.set_title('Percent Hispanic',size=16)\nax2.set_axis_off()\nax1 = nta_school.plot(ax=ax3,cmap='gist_gray_r', linewidth=2,edgecolor='black')\nnta_school.plot(ax=ax3, column= 'Percent Black \/ Hispanic', cmap='Reds', vmin=0, vmax=100)\nax1.set_title('Percent Black\/Hispanic',size=16)\nax3.set_axis_off()\nax1 = nta_school.plot(ax=ax4,cmap='gist_gray_r', linewidth=2,edgecolor='black')\nnta_school.plot(ax=ax4, column= 'Percent ELL', cmap='Reds', vmin=0, vmax=100)\nax1.set_title('Percent ELL',size=16)\nax4.set_axis_off()\nax1 = nta_school.plot(ax=ax5,cmap='gist_gray_r', linewidth=2,edgecolor='black')\nnta_school.plot(ax=ax5, column= 'Percent White', cmap='Reds', vmin=0, vmax=100)\nax1.set_title('Percent White',size=16)\nax5.set_axis_off()\nax1 = nta_school.plot(ax=ax6,cmap='gist_gray_r', linewidth=2,edgecolor='black')\nnta_school.plot(ax=ax6, column= 'Percent Asian', cmap='Reds', vmin=0, vmax=100)\nax1.set_title('Percent Asian',size=16)\nax6.set_axis_off()\nfig.subplots_adjust(right=0.8)\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nsm = plt.cm.ScalarMappable(cmap='Reds', norm=plt.Normalize(vmin=0, vmax=100))\nsm._A = []\nfig.colorbar(sm, cax=cbar_ax);","805df640":"econ_labels = ['Economic Need Index', 'School Income Estimate']\nnta_school_econs = schools.groupby('nta').mean()[econ_labels]\nnta_school_econs['geometry'] = nyc['geometry']\nnta_school_econ = gpd.GeoDataFrame(nta_school_econs).set_geometry('geometry')","c36e944e":"f, axs = plt.subplots(1,2,figsize=(24, 18))\nfor i in range(1, 3):\n    plt.subplot(1, 2, i)\n    plt.hist(schools[econ_labels[i-1]].dropna(), bins=30)\n    plt.xlabel(econ_labels[i-1],size=14)\n    plt.ylabel('Number of Schools');","cab053ef":"schools[econ_labels].describe()","144dc505":"fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(24, 16))\nfig.subplots_adjust(hspace=.05, wspace=.05)\nax1 = nta_school_econ.plot(ax=ax1,cmap='gist_gray_r', linewidth=2,edgecolor='black')\nnta_school_econ.plot(ax=ax1, column= 'Economic Need Index', cmap='Reds', vmin=0., vmax=1.)\nax1.set_axis_off()\nax1.set_title('Economic Need Index',size=16)\nsm = plt.cm.ScalarMappable(cmap='Reds', norm=plt.Normalize(vmin=0, vmax=1.))\nsm._A = []\nfig.colorbar(sm, ax=ax1, orientation=\"horizontal\", pad=0.05)\nax2 = nta_school_econ.plot(ax=ax2,cmap='gist_gray_r', linewidth=2,edgecolor='black')\nnta_school_econ.plot(ax=ax2, column= 'School Income Estimate', cmap='Greens', vmin=0, vmax=100000)\nax2.set_title('School Income Estimate',size=16)\nax2.set_axis_off()\nsm = plt.cm.ScalarMappable(cmap='Greens', norm=plt.Normalize(vmin=0, vmax=100000))\nsm._A = []\nfig.colorbar(sm, ax=ax2, orientation=\"horizontal\", pad=0.05);","0c180e46":"proficiency = ['Average ELA Proficiency', 'Average Math Proficiency']\nnta_school_perf = schools.groupby('nta').mean()[proficiency]\nnta_school_perf['geometry'] = nyc['geometry']\nnta_school_performance = gpd.GeoDataFrame(nta_school_perf).set_geometry('geometry')","b1f4d4d6":"f, axs = plt.subplots(1,2,figsize=(24, 18))\nfor i in range(1, 3):\n    plt.subplot(1, 2, i)\n    plt.hist(schools[proficiency[i-1]].dropna(), bins=30)\n    plt.xlabel(proficiency[i-1],size=14)\n    plt.ylabel('Number of Schools');","bfe62996":"schools[proficiency].describe()","87ea0311":"fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(24, 16))\nfig.subplots_adjust(hspace=.05, wspace=.05)\nax1 = nta_school_performance.plot(ax=ax1,cmap='gist_gray_r', linewidth=2,edgecolor='black')\nnta_school_performance.plot(ax=ax1, column= 'Average ELA Proficiency', cmap='Blues', vmin=1.5, vmax=4)\nax1.set_axis_off()\nax1.set_title('Average ELA Proficiency',size=16)\nsm = plt.cm.ScalarMappable(cmap='Blues', norm=plt.Normalize(vmin=1.5, vmax=4))\nsm._A = []\nfig.colorbar(sm, ax=ax1, orientation=\"horizontal\", pad=0.05)\nax2 = nta_school_performance.plot(ax=ax2,cmap='gist_gray_r', linewidth=2,edgecolor='black')\nnta_school_performance.plot(ax=ax2, column= 'Average Math Proficiency', cmap='Blues', vmin=1.5, vmax=4)\nax2.set_title('Average Math Proficiency',size=16)\nax2.set_axis_off()\nsm = plt.cm.ScalarMappable(cmap='Blues', norm=plt.Normalize(vmin=1.5, vmax=4))\nsm._A = []\nfig.colorbar(sm, ax=ax2, orientation=\"horizontal\", pad=0.05);","44098566":"def plot_corr(df,size=10):\n    '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n\n    Input:\n        df: pandas DataFrame\n        size: vertical and horizontal size of the plot'''\n\n    corr = df.corr()\n    fig, ax = plt.subplots(figsize=(size, size))\n    cax = ax.matshow(corr, cmap='RdYlGn', vmin=-1, vmax =1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90);\n    plt.yticks(range(len(corr.columns)), corr.columns);\n    fig.colorbar(cax);","843263eb":"other_metrics = ['Student Attendance Rate', 'Percent of Students Chronically Absent',\n                 'Rigorous Instruction %','Collaborative Teachers %','Supportive Environment %',\n                 'Effective School Leadership %','Strong Family-Community Ties %','Trust %',\n                 'Community School?']\ncorrelates = other_metrics + proficiency + econ_labels + xaxis_labels\nplot_corr(schools[correlates])","da78a7b2":"shsat = pd.read_csv('Data\/SHSAT_reg.csv')\nshsat['School name'] = shsat['School name'].str.lower()\nshsat['take_reg_ratio'] = shsat.apply(lambda x: \n                                      x['Number of students who took the SHSAT']\/\n                                      (x['Number of students who registered for the SHSAT']+.000001),axis=1)\nshsat['take_ratio'] = shsat.apply(lambda x: \n                                   x['Number of students who took the SHSAT']\/\n                                   float(x['Enrollment on 10\/31']), axis=1)\nshsat['reg_ratio'] = shsat.apply(lambda x:\n                                x['Number of students who registered for the SHSAT']\/\n                                   float(x['Enrollment on 10\/31']), axis=1)\nshsat['charter'] = shsat['DBN'].apply(lambda x: 'charter' if x[:2] == '84' else 'public')\nratios = ['take_reg_ratio','take_ratio','reg_ratio']\nshsat[ratios].corr()","8c40752e":"color_map = {'charter':'red', 'public':'blue'}\nfig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(10,5))\nplt.subplot(1, 2, 1)\nschool_types = shsat.groupby('charter')\nfor name, group in school_types:\n    plt.plot(group['reg_ratio'], group['take_ratio'], marker='o',\n             linestyle='', label=name, color=color_map[name])\nplt.xlabel('Ratio of students Registered')\nplt.ylabel('Ratio of students that take test')\nplt.legend()\nplt.subplot(1, 2, 2)\nfor name, group in school_types:\n    plt.plot(group['reg_ratio'], group['take_reg_ratio'], marker='o', \n             linestyle='', label=name, color=color_map[name])\nplt.xlabel('Ratio of students Registered')\nplt.ylabel('Ratio of students that take test of those registered');\n","ef27e127":"shsat_schools = shsat.groupby('DBN').mean()\nshsat_schools['charter'] = shsat_schools.apply(lambda x: 'charter' if x.name[:2] == '84' else 'public',axis=1)\ncolor_map = {'charter':'red', 'public':'blue'}\nfig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(10,5))\nplt.subplot(1, 2, 1)\n#plt.scatter(shsat['reg_ratio'], shsat['take_ratio'])\nschool_types = shsat_schools.groupby('charter')\nfor name, group in school_types:\n    plt.plot(group['reg_ratio'], group['take_ratio'], marker='o',\n             linestyle='', label=name, color=color_map[name])\nplt.xlabel('Ratio of students Registered')\nplt.ylabel('Ratio of students that take test')\nplt.legend()\nplt.subplot(1, 2, 2)\nfor name, group in school_types:\n    plt.plot(group['reg_ratio'], group['take_reg_ratio'], marker='o', \n             linestyle='', label=name, color=color_map[name])\nplt.xlabel('Ratio of students Registered')\nplt.ylabel('Ratio of students that take test of those registered');","a6dad355":"shsat_schools = shsat.groupby('DBN')\navg_particip = shsat_schools.apply(lambda x: sum(x['take_ratio'])) \/shsat_schools.size()\nmost_partic = avg_particip[np.argsort(avg_particip.values)[-3:]][::-1]\nleast_partic = avg_particip[np.argsort(avg_particip.values)[:3]]","73d25770":"most_partic","b3b7a414":"least_partic","0a122738":"shsat.corr()\nfrom scipy.stats import linregress\nshsat['years_from_start'] = shsat.apply(lambda x: x['Year of SHST'] - 2013, axis=1)\nshsat['take%'] = shsat.apply(lambda x: x['take_ratio']*100, axis=1)\ntake_trend = shsat_schools.apply(lambda x: linregress(list(x['take%']),list(x['years_from_start'])).slope)\n#Indexing to get rid of outliers or schools with only one year\ntrending_up = take_trend[np.argsort(take_trend.values)[-5:-2]][::-1]\ntrending_down = take_trend[np.argsort(take_trend.values)[2:5]]","5f1d122f":"trending_up","a093960b":"trending_down","a060f37f":"shsat_stats = shsat_schools.mean().drop(columns=['Year of SHST', 'Grade level',\n                                   'years_from_start', 'take%'])\nshsat_stats['trend'] = take_trend.fillna(0)\nshsat_stats['trend'] = shsat_stats['trend'].apply(lambda x: x if (x>-.5) and (x<.5) else 0)","d05b9951":"dbn_to_name = {}\nfor x in shsat['DBN'].unique():\n    for index, row in shsat.iterrows():\n        if row['DBN'] == x:\n            dbn_to_name[x] = row['School name']\n            break\nshsat_stats['name'] = shsat_stats.apply(lambda x: dbn_to_name[x.name], axis=1)","1ee8e5bc":"shsat_stats","f107e92b":"shsat_school_stats = shsat_stats.merge(schools, left_on='DBN', right_on='Location Code')","75d497a7":"pd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 100)\nshsat_school_stats","d10a9049":"nat_am = [x for x in shsat_school_stats.columns if 'Indian' in x]\ncorrs = shsat_school_stats.drop(columns=['SED Code','District','Latitude','Longitude','Zip'] + nat_am)\nplot_corr(corrs.fillna(0), size=25)","de014b21":"from pprint import pprint\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\ndef create_hyperparam_grid():\n    # Number of trees in random forest\n    n_estimators = [int(x) for x in np.linspace(start = 250, stop = 2250, num = 250)]\n    # Number of features to consider at every split\n    max_features = [.1,.2,.3,.4,.5,.6,.7,.8,.9,.95]\n    # Maximum number of levels in tree\n    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n    max_depth.append(None)\n    # Minimum number of samples required to split a node\n    min_samples_split = [2, 5, 8, 10]\n    # Minimum number of samples required at each leaf node\n    min_samples_leaf = [1, 2, 3, 4]\n    # Method of selecting samples for training each tree\n    bootstrap = [True, False]\n    # Create the random grid\n    random_grid = {'n_estimators': n_estimators,\n                   'max_features': max_features,\n                   'max_depth': max_depth,\n                   'min_samples_split': min_samples_split,\n                   'min_samples_leaf': min_samples_leaf,\n                   'bootstrap': bootstrap}\n    return random_grid","5570a56e":"def rf_classifier(labels, features, pct_train=.8, num_iter=50):\n    rfclf = RandomForestClassifier()\n    random_grid = create_hyperparam_grid()\n    rf_random = RandomizedSearchCV(estimator = rfclf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n    rf_random.fit(features, labels)\n    best_params = rf_random.best_params_ \n    rfclf = RandomForestClassifier(**best_params)\n    i = 0\n    cumu_acc = 0\n    while(i < num_iter):\n        msk = np.random.rand(len(labels)) < pct_train\n        train_data = features[msk]\n        train_labels = labels[msk]\n        test_data = features[~msk]\n        test_labels = labels[~msk]\n        rfclf.fit(train_data, train_labels) \n        cumu_acc += rfclf.score(test_data, test_labels)\n        i += 1\n    print cumu_acc\/num_iter\n    return rfclf","bf82ba53":"take_labels = (shsat_school_stats['take_ratio'] > .135).astype(np.int32)\nreg_labels = (shsat_school_stats['reg_ratio'] > .28).astype(np.int32)\ntrend_labels = (shsat_school_stats['trend'] > 0).astype(np.int32)\nlabels = ['take_ratio', 'reg_ratio','trend']\nnon_numerics = ['Adjusted Grade','New?','Other Location Code in LCGMS',\n                'School Name','Location Code','Address (Full)','City',\n                'Grades','Grade High', 'Location', 'nta', 'name', 'Grade Low']\nratings = [c for c in corrs.columns if 'Rating' in c]\ncheats = ['Enrollment on 10\/31', 'Number of students who registered for the SHSAT',\n         'Number of students who took the SHSAT']\nfeatures = corrs.drop(columns=labels+non_numerics+ratings+cheats)\nfeatures.fillna((features['School Income Estimate'].mean()), inplace=True)","ad7f36f5":"take_clf = rf_classifier(take_labels, features,pct_train=.6, num_iter=20)","e621242f":"important_features = np.argsort(-take_clf.feature_importances_)[:10]\nfeatures.columns[important_features]","3d628554":"reg_clf = rf_classifier(reg_labels, features,pct_train=.6, num_iter=20)\nimportant_reg_features = np.argsort(-reg_clf.feature_importances_)[:10]\nfeatures.columns[important_reg_features]","b4438489":"trend_clf = rf_classifier(trend_labels, features,pct_train=.6, num_iter=20)\nimportant_trend_features = np.argsort(-trend_clf.feature_importances_)[:10]\nfeatures.columns[important_trend_features]","9504e82a":"special_hs = {\n    'Stuyvesant High School': Point(-74.013762, 40.718222),\n    'The Bronx High School of Science': Point(-73.890843, 40.878435),\n    'Brooklyn Tech HS': Point(-73.976526, 40.688987),\n    'James Madison HS': Point(-73.948227, 40.610011),\n    'Hillcrest High School': Point(-73.802482, 40.709588),\n    'Long Island City High School': Point(-73.933048, 40.765480),\n    'Staten Island Technical High School': Point(-74.118124, 40.567703)\n}\nspechs = gpd.GeoSeries(special_hs)\nspec_hs_distr = gpd.read_file('Data\/special_hs\/spec_hs_distr.shp')","a3b9a49d":"fig, ax = plt.subplots()\nfig.set_size_inches(14,10)\nspec_hs_distr.plot(ax=ax, column='speciali_1',cmap='Reds')\nspechs.plot(ax=ax, color='blue')\n#stations.plot(ax=ax, color='green', markersize=1)\nsm = plt.cm.ScalarMappable(cmap='Reds', norm=plt.Normalize(vmin=0, vmax=32))\nsm._A = []\nplt.colorbar(sm)\nax.legend(['Specialized High School'], loc='upper left')\nplt.suptitle('Percent of NTA enrolled in specialized HS', size=18);","6e294044":"from geopy.distance import great_circle\nspechs_lat_lon = [(x.y, x.x) for x in special_hs.values()]\ndef closest_spec_hs(pt, specs):\n    return min([great_circle((pt.y, pt.x), hs).meters for hs in specs])\nspecial_hs_map = {\n    'M': Point(-74.013762, 40.718222),\n    'X': Point(-73.890843, 40.878435),\n    'B1': Point(-73.976526, 40.688987),\n    'B2': Point(-73.948227, 40.610011),\n    'Q1': Point(-73.802482, 40.709588),\n    'Q2': Point(-73.933048, 40.765480),\n    'R': Point(-74.118124, 40.567703)\n}\n\ndef dist_to_test(row, spec_hs_locs, spec_hs_map):\n    district = row['District']\n    bor = row['Location Code'][2]\n    pt = None\n    if bor == 'M' or bor == 'X' or bor == 'R':\n        pt = special_hs_map[bor]\n    elif district in [13, 14, 15, 16, 19, 20, 32]:\n        pt = special_hs_map['B1']\n    elif district in [17, 18, 21, 22, 23]:\n        pt = special_hs_map['B2']\n    elif district in [26, 27, 28, 29]:\n        pt = special_hs_map['Q1']\n    elif district in [24, 25, 30]:\n        pt = special_hs_map['Q2']\n    return great_circle()\nschools['dist_to_spec'] = schools['Location'].apply(lambda x: closest_spec_hs(x, spechs_lat_lon))","3aeef734":"schools","ef6838d2":"district_map = {}","84636d39":"## Schools","debab9f6":"# Recommendations","de1d6174":"## School Performance","1f6ce7da":"## Correlates","f407c96b":"Due to the lack of granular SHSAT data, it is perhaps more appropriate to identify areas of a nieghborhood granularity that could benefit from additional resources and then drill down to find the schools most in need. Fortunately we have access to The New School Center for NYC Affairs dataset for enrollment in specialized schools by neigborhood tabulation area (NTA) which is a proxy for SHSAT participation and performance. https:\/\/nimader.carto.com\/datasets","66351c0a":"Charters appear to perform marginally better than regular public schools in this district. \n\nLets identify the high\/low performing schools and the schools with strong up or down trends in participation.","b5aa8cb6":"# Overview\n## Objective\n\"[Identify] which schools have students that would benefit from outreach services and lead to a more diverse group of students taking the SHSAT and being accepted into New York City's Specialized High Schools.\" - Chris Crawford\n\n\n\"Part of this challenge is to assess the needs of students by using publicly available data to quantify the challenges they face in taking the SHSAT. The best solutions will enable PASSNYC to identify the schools where minority and underserved students stand to gain the most from services like after school programs, test preparation, mentoring, or resources for parents.\" -Problem Statement\n\n## Methodology\nTo determine how to increase participation, we want to identify schools with existing high and low participation. Then identify what traits groups of schools with high or low participation have in common in attempt to find causal features. <br> <br>\nAdditionally, we want to identify schools with notable increases or decreases in participation over time, and determine other features that trend in a similar fashion in an attempt to establish a casual link. <br> <br>\nHowever, since granular school based data is only available for one school district, we use The New School Center for NYC Affairs' dataset that quantifies specialized highschool attendence by neighbourhood tabulation area (NTA). We can use this to identify correlates of participation on an NTA level and identify neighbourhoods that need more resources.","7b3dc1ef":"## Socio-Economics","c1e8934e":"# SHSAT EDA","3c8a30a3":"## School Demographics","b1d4a321":"# Conclusions","bf32f643":"# Exploratory Data Analysis\nStart analysis on the PASSNYC Explorer data set to examine middle and elementary public school characteristics","3b008fd7":"This includes every data point, one for every school for every year. Let us average these schools over the years data is available to get a more concise view of performance.","10ded2ba":"Notice how similar the Economic Need index looks to the combinded Black\/Hispanic graph","4f321853":"The low accuracy indicates a lack of predictive power in the data given (or a methodological short coming) but I believe this has to do with the incredibly small size of available data. The little bit of signal that is evident shows that performance and participation in the middle school ELA and Math exams are indicators of SHSAT registration and participation. Notably, school and community factors (the scores) didn't seem to carry any predictive power. ","f29321f6":"The two things to note here are: <br>\n1.) The strong positive relationship between the ratio of students that register and the ratio of students that take the test <br>\n2.) The near statistically insignifcant relationship between the ratio of students that register and the ratio of students that take the test of those that register <br>\nLets take closer look at these relationships"}}