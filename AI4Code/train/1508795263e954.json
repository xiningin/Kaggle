{"cell_type":{"d783a43f":"code","5dc9188b":"code","332a5c07":"code","1c24e099":"code","0b03c4ff":"code","d1569459":"code","5b0031b2":"code","879b2e64":"code","e1b62cd7":"code","dec620a4":"code","7e94c923":"markdown","1ef789d7":"markdown","b7c00336":"markdown","bae2b22c":"markdown","dc205a0d":"markdown","bbb4be60":"markdown","cb2b085f":"markdown","04f4cc58":"markdown"},"source":{"d783a43f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5dc9188b":"heart = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\n\nheart.head()","332a5c07":"heart.describe()","1c24e099":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ncorr = heart.corr()\n\nf, ax = plt.subplots(figsize=(15, 15))\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\nsns.heatmap(corr, annot=True, mask = mask, cmap=cmap)","0b03c4ff":"# Prediction Target:\ny = heart.HeartDisease\n\n# Features:\nheart_features = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak']\n\nX = heart[heart_features]","d1569459":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, accuracy_score","5b0031b2":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)","879b2e64":"heart_model = RandomForestRegressor(random_state=1)\nheart_model.fit(train_X, train_y)","e1b62cd7":"heart_preds = heart_model.predict(val_X)\nprint(mean_absolute_error(val_y, heart_preds))","dec620a4":"from sklearn.ensemble import RandomForestClassifier\n\nheart_model_cl = RandomForestClassifier(random_state=1)\nheart_model_cl.fit(train_X, train_y)\n\nheart_preds_cl = heart_model_cl.predict(val_X)\nprint(mean_absolute_error(val_y, heart_preds_cl))","7e94c923":"### Since the value for heart disease is either 0 or 1 (present or not), a 0.3168 Mean Absolute Error means the model has roughly a 31.7% error when prediction heart disease based on the features we provided.\n\n### Seems like there's lots of room to improve here!","1ef789d7":"### Pick our features and target:","b7c00336":"#### Update 12\/2\/21:\n#### User [@Mishal Assif](https:\/\/www.kaggle.com\/mishalassif) suggested using Random Forest Classifer instead of Random Forest Regressor. Let's give it a try. ","bae2b22c":"### Build a correlation heatmap to help visualize the relationship between the features and our target:","dc205a0d":"### Import our ML libraries","bbb4be60":"### Fit the model","cb2b085f":"### Split the data into train and test sets","04f4cc58":"#### Significant improvement using the RF Classifier, reducing the error from 31.7% to 23.9% "}}