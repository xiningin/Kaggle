{"cell_type":{"314c7694":"code","a7a6bfe6":"code","77c19bdb":"code","44b687fb":"code","d94ddd82":"code","e87c9a9b":"code","62511b90":"code","96ec0eb5":"code","3f2928ef":"code","7b627790":"code","41e25e21":"code","00a5c373":"code","67b5b951":"code","090d3ed2":"code","d0694ef4":"code","47754721":"code","3e2b8105":"code","f00d4629":"code","bfcf3cb9":"code","c6dd6f07":"code","c211221c":"code","a1fe4ba8":"code","2ef7265d":"code","05b92ffc":"code","b6d8d029":"code","21ca375c":"code","4b1e7e4e":"code","8e979361":"code","e0661585":"code","567f171d":"code","7af4e2e5":"code","e33aef4e":"code","8737342a":"markdown","e1a2613a":"markdown","9937c7d4":"markdown","5bae8331":"markdown","718c11ec":"markdown","cff6c5f6":"markdown","e7f9ee66":"markdown","973d5c2a":"markdown","8f9fe7b3":"markdown","3a8fc29c":"markdown","4b8ef37d":"markdown","8147a82a":"markdown","878c72a6":"markdown","4dcc5ffa":"markdown","9ce97d2f":"markdown","2a5f9324":"markdown","d490144a":"markdown","4d05358c":"markdown","7f31ccc1":"markdown","0533dd03":"markdown"},"source":{"314c7694":"# Some typical imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nfrom sklearn.preprocessing import OneHotEncoder, QuantileTransformer\nfrom sklearn.model_selection import train_test_split\nfrom numba import jit # Compile some functions when performance is critical\nimport keras\nfrom keras.initializers import RandomNormal\nfrom keras.models import Model, load_model, save_model\nfrom keras.layers import Embedding, Input, Dense, Concatenate, Multiply, Flatten\nfrom keras.optimizers import Adam\nimport tensorflow as tf\nif tf.test.gpu_device_name():\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\nelse:\n    print(\"No GPU\")","a7a6bfe6":"# Check our data structure\nanime = pd.read_csv(\"..\/input\/anime-recommendations-database\/anime.csv\")\nanime.head(10)","77c19bdb":"anime = anime[anime['type'] == 'TV']","44b687fb":"# Copy the column\nanime['features_genre'] = anime['genre']\n\n# Cast None to an empty string\nanime['features_genre'] = anime['features_genre'].fillna('') \n# Split genres into a list of strings\nanime['features_genre'] = anime['features_genre'].map(lambda x: x.split(', '))\n\n# Create a set of all genres\nall_genres = set()\nfor row in anime['features_genre']:\n    # Union of sets is declared with the | operator\n    all_genres = all_genres | set(row)\nall_genres.remove('') # Drop the empty genre\n\ndef invert_dict(d):\n    return {value: key for key, value in d.items()}\n\nall_genres = sorted(list(all_genres)) # We convert it to a list to enforce alphabetic ordering\nngenres = len(all_genres)\n\nidx2genre = dict(enumerate(all_genres)) # Create a mapping dictionary from index to dict\ngenre2idx = invert_dict(idx2genre) # Inverse dict\n\ngenre2idx","d94ddd82":"def encode_genres(genres):\n    out = np.zeros(ngenres)\n    for genre in genres:\n        if genre == '':\n            pass\n        else:\n            out[genre2idx[genre]] = 1\n    return out.tolist()\nanime['features_genre'] = anime['features_genre'].map(encode_genres)\nanime['features_genre'] # See how the encoded features look","e87c9a9b":"anime['features_episodes'] = anime['episodes'].replace({'Unknown' : 1}).astype(np.int32)\nsb.distplot(anime['features_episodes']);\n# This feature is heavily unbalanced! Let's apply a quantile transformation to it","62511b90":"ep_discretizer = QuantileTransformer(n_quantiles = 100)\nfeats_ep = anime['features_episodes'].apply(np.log).to_numpy().reshape(-1, 1)\nfeats_ep = ep_discretizer.fit_transform(feats_ep).flatten().tolist()\nanime['features_episodes'] = feats_ep\nsb.distplot(anime['features_episodes']);","96ec0eb5":"# Check our data structure\nrating = pd.read_csv(\"..\/input\/anime-recommendations-database\/rating.csv\")\nrating.head(10)","3f2928ef":"rating = rating[rating['user_id'] <= 10000] # Can comment this line\nrating = rating[rating['anime_id'].isin(anime['anime_id'])] # Don't comment this one though!","7b627790":"print(rating['rating'].replace({-1: np.nan}).dropna().describe())\nsb.distplot(rating['rating'], kde = False);","41e25e21":"user_median = rating.groupby('user_id').median()['rating']\nsb.distplot(user_median, kde = False);\noverall_median = user_median.median()\nprint(\"Median of all users' medians: \", overall_median)\nuser_median = dict(user_median.replace({-1 : overall_median}))","00a5c373":"user_medians = rating['user_id'].apply(lambda x: user_median[x])\nrating['rating'] = rating['rating'].replace({-1 : np.nan}).fillna(user_medians)\nrating['rating'] = rating['rating'] \/ rating['rating'].max() # Divide by the max to normalize!","67b5b951":"# Resulting histogram\nsb.distplot(rating['rating'], kde = False);","090d3ed2":"num_neg = 4\nuser2n_anime = dict(rating.groupby('user_id').count()['anime_id'])","d0694ef4":"all_users = np.sort(rating['user_id'].unique())\nall_anime = np.sort(rating['anime_id'].unique())\nn_anime = len(all_anime)\nn_users = len(all_users)\n\n@jit\ndef choice_w_exclusions(array, exclude, samples):\n    max_samples = len(array)-len(exclude)\n    final_samples = min(samples, max_samples)\n    possible = np.array(list(set(array) - set(exclude)))\n    return np.random.choice(possible, size = final_samples, replace = False)\n@jit\ndef flat(l):\n    return [item for sublist in l for item in sublist]","47754721":"%%time\n#This part takes about 10 minutes with a full dataset. Time for coffee!\nneg_user_id = []\nneg_anime_id = []\nneg_rating = []\n\nfor user in all_users:\n    exclude = list(rating[rating['user_id'] == user]['anime_id'])\n    sampled_anime_id = choice_w_exclusions(all_anime, exclude, len(exclude) * num_neg)\n    \n    neg_user_id.append([user] * len(sampled_anime_id))\n    neg_anime_id.append(sampled_anime_id)\n    neg_rating.append([0.] * len(sampled_anime_id))\n    \nneg_user_id = flat(neg_user_id)\nneg_anime_id = flat(neg_anime_id)\nneg_rating = flat(neg_rating)","3e2b8105":"negatives = pd.DataFrame({'user_id': neg_user_id,\n                          'anime_id': neg_anime_id,\n                          'rating': neg_rating})\ndata = pd.concat([rating, negatives], ignore_index = True)","f00d4629":"anime['features'] = anime['features_genre'] + anime['features_episodes'].apply(lambda x: [x])\nanime['features'] = anime['features'].apply(np.array)\nn_feats = len(anime['features'].iloc[0])\ndata = data.join(anime['features'], on = 'anime_id').dropna()","bfcf3cb9":"anime2item_dict = dict(zip(np.sort(all_anime), list(range(n_anime))))\nitem2anime_dict = {v: k for k, v in anime2item_dict.items()}\n\ndef anime2item(a_id):\n    return anime2item_dict[a_id]\n\ndef item2anime(i_id):\n    return item2anime_dict[i_id]\n                       \ndata['item_id'] = data['anime_id'].apply(anime2item)","c6dd6f07":"x0 = data['user_id'].to_numpy()\nx1 =data['item_id'].to_numpy()\nx2 = np.stack(data['features'].to_numpy())\ny = data['rating'].to_numpy()\n\n(x0_train, x0_val,\n x1_train, x1_val,\n x2_train, x2_val,\n y_train, y_val) = train_test_split(x0, x1, x2, y,\n                                    test_size = 0.1,\n                                    random_state = 42)\n\nx_train = [x0_train, x1_train, x2_train]\nx_val = [x0_val, x1_val, x2_val]","c211221c":"def get_model(num_users, num_items, num_item_feats, mf_dim, layers = [64, 32, 16, 8]):\n    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n    feats_input = Input(shape=(num_item_feats,), dtype='float32', name = 'feats_input')\n\n    # User&Item Embeddings for Matrix Factorization\n    MF_Embedding_User = Embedding(input_dim = num_users + 1, output_dim = mf_dim,\n                                  name = 'user_embedding',\n                                  embeddings_initializer = RandomNormal(stddev=0.001),\n                                  input_length = 1)\n    MF_Embedding_Item = Embedding(input_dim = num_items + 1, output_dim = mf_dim,\n                                  name = 'item_embedding',\n                                  embeddings_initializer = RandomNormal(stddev=0.001),\n                                  input_length = 1)\n    \n    # User&Item Embeddings for MLP part\n    MLP_Embedding_User = Embedding(input_dim = num_users + 1, output_dim = int(layers[0] \/ 2),\n                                   name = 'mlp_embedding_user',\n                                   embeddings_initializer = RandomNormal(stddev=0.001),\n                                   input_length = 1)\n    MLP_Embedding_Item = Embedding(input_dim = num_items + 1, output_dim = int(layers[0] \/ 2),\n                                   name = 'mlp_embedding_item',\n                                   embeddings_initializer = RandomNormal(stddev=0.001),\n                                   input_length = 1) \n    \n    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n    mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n    mf_vector = Multiply()([mf_user_latent, mf_item_latent])\n\n    # MLP part with item features\n    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n    mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n    \n    mlp_vector = Concatenate()([mlp_user_latent, mlp_item_latent, feats_input])\n    for l in layers:\n        layer = Dense(l, activation='relu')\n        mlp_vector = layer(mlp_vector)\n\n    # Concatenate MF and MLP parts\n    predict_vector = Concatenate()([mf_vector, mlp_vector])\n    \n    # Final prediction layer\n    prediction = Dense(1, activation = 'sigmoid',\n                       kernel_initializer = 'lecun_uniform',\n                       name = 'prediction')(predict_vector)\n    \n    model = Model(input = [user_input, item_input, feats_input], output = prediction)\n    return model","a1fe4ba8":"learning_rate = 0.001\nbatch_size = 256\nn_epochs = 3\nmf_dim = 15\nlayers = [128, 64, 32, 16, 8]","2ef7265d":"model = get_model(n_users, n_anime, n_feats, mf_dim, layers)\nmodel.compile(optimizer = Adam(lr = learning_rate), loss = 'mean_squared_logarithmic_error')","05b92ffc":"hist = model.fit(x = x_train, y = y_train, validation_data = (x_val, y_val),\n                 batch_size = batch_size, epochs = n_epochs, verbose = True, shuffle = True)","b6d8d029":"plt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Eval'], loc = 'upper right')\nplt.show()","21ca375c":"indexed_anime = anime.set_index('anime_id')\n\ndef explore(user_id, top = 5):\n    sub = rating[rating['user_id'] == user_id]\n    watched_animes = sub['anime_id']\n    ratings = sub['rating']\n    names = indexed_anime.loc[watched_animes]['name']\n    genres = indexed_anime.loc[watched_animes]['genre']\n    rating_info = pd.DataFrame(zip(watched_animes, names,\n                                   genres, ratings * 10),\n                               columns = ['anime_id', 'name',\n                                          'genre', 'rating']).set_index('anime_id')\n    return rating_info.sort_values(by = 'rating', ascending = False).iloc[:top]\n\ndef recommend(user_id, recommendations = 5):\n    watched_animes = rating[rating['user_id'] == user_id]['anime_id']\n    \n    test_anime = np.array(list(set(all_anime) - set(watched_animes)))\n    test_user = np.array([user_id] * len(test_anime))\n    test_items = np.array([anime2item(a) for a in test_anime])\n    sub_anime = indexed_anime.loc[test_anime]\n    test_features = np.stack(sub_anime['features'].to_numpy())\n    test = [test_user, test_items, test_features]\n    preds = model.predict(test).flatten()\n    results = pd.DataFrame(zip(sub_anime['name'], test_anime,  sub_anime['genre'], preds * 10),\n                           columns = ['name', 'anime_id',\n                                      'genre', 'score']).set_index('anime_id')\n    return results.sort_values(by = 'score', ascending = False).iloc[:recommendations]","4b1e7e4e":"explore(444) # Action Sports study","8e979361":"recommend(444)","e0661585":"explore(999) # Action Fantasy case study","567f171d":"recommend(999)","7af4e2e5":"explore(111) # Techno study","e33aef4e":"recommend(111)","8737342a":"###\u00a0Split into a 90\/10 train\/test scheme.\nNote: We can't separate users between train and test sets (like train users versus test users), since we need to feed all users and anime shows to the embeddings.","e1a2613a":"### Sample negative entries","9937c7d4":"Now, we need to do some more detailed feature engineering for the remaining features.\n\nThe number of episodes contains 'Unknown' labels among numeric values. We impute them by filling missed episodes with 1 episode.","5bae8331":"## Construct training and testing sets\nOur current dataset is incomplete, since we need to generate rows including anime that users' havent watched (_negative intances_). The following accounts for that factor. We need to emphasize that we don't want every user to have a row for every anime, to not fill up our entire RAM memory.\n\nAllow us to set that every rating will trigger 4 negative entries (we picked 4 just as a fiducial value from the original repo). To generate these records, we simply sample 4 unwatched animes for each user rating.","718c11ec":"## Apparently, the recommender works like a charm.\nBut... There's still plenty of job to do, like observing extreme cases such as users with few watched anime, niche clusters, and so on.","cff6c5f6":"For simplicity, let's use TV anime only. This prevents future complications with the episode number.","e7f9ee66":"### Alright, let's visualize some recommendations!","973d5c2a":"We group by user and replace missing ratings with the median of the user.\nMany users don't leave reviews. To not lose this information, we impute them with the median of all users: 8","8f9fe7b3":"## Our goal is to reach as many users as possible; we need to impute missing reviews\n\nMany users don't review the shows they've watched. We could drop these records or impute them with the median of the users' ratings, for example.\nInspecting the distribution of ratings, we see that most of them are positive (with a median of 8). Therefore, we should consider imputing unrated shows, since most seen shows are positive signals.\n\nAn important thing to stress is that our algorithm is not trying to recommend masterpieces only, but rather a varied mix of shows that the user might enjoy more or less. This helps to reach _hardcore otakus_ as well as casual viewers.","3a8fc29c":"### Join both tables' information and drop unindexed anime","4b8ef37d":"Here, we impose a uniform-like distribution, using Scikit-Learn's QuantileTransformer. This is an easier-to-handle representation.","8147a82a":"## Model implementation\n> [Heavily based on the [Neural Collaborative Filtering paper repo](https:\/\/github.com\/hexiangnan\/neural_collaborative_filtering)]\n\nHowever, our model improved the reference model by including information of anime features!","878c72a6":"Set hyperparameters, which are very similar to the default values from the NeuralMF model repo, except for the number of epochs and layers.","4dcc5ffa":"## Collaborative-filtering feature engineering","9ce97d2f":"## Content-based feature engineering","2a5f9324":"The content-based part of our model requires features for each anime. Therefore, we are going to include as many relevant features as possible, to not waste any information. So, we are using the anime genres and number of episodes. We dropped the title here, due to a lack of ways of handling it. Rating and members are not content-related features, since they are dynamic and bounded to users' activity. So, they are going to be leveraged through the collaborative-filtering part.\n\nLet's start by one-hot encoding genres:","d490144a":"# Anime NeuralMF Hybrid Recommender\n### In this notebook, we implement a recommender model with the MyAnimeList Anime Recommendations dataset.\n\n> Based on the Neural Collaborative Filtering paper: Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu and Tat-Seng Chua (2017). Neural Collaborative Filtering. In Proceedings of WWW '17, Perth, Australia, April 03-07, 2017.\n\n#### The following is a little motivation for Hybrid recommender systems.\n\n## Why Hybrid?\nWell, there are two main kinds of recommender systems: Content-based and Collaborative filtering-based.\n* Content-based recommenders suggest similar picks to a certain _item_ (an anime movie\/series in our case), letting the users know about similar items to the ones they have watched\/rated positively. These method typically use _item features_ together with unsupervised methods in an effort to generate a product-space and compute similarities between items. However, this method may end suggesting a limited mix of items, providing a low _surprise factor_ for the user.\n* On the other hand, collaborative filtering recommenders rely on past users' history of watched\/rated items, increasing the chances of recommending a serendipitous item to a target user. Classic methods rely solely on a user-item matrix, which maps the interactions that all users have with every item. These matrix methods are heavily memory-intensive and newer neural network-based are more common. Nonetheless, these methods could miss on similar -but typically overseen- items, in comparison to the ones watched\/reviewed by the target user.\n\nIn order to get more robust recommendations, a hybrid model can combine both item features and user-item features.\n\n## And... why NeuralMF?\nThe NeuralMF is a mix of General Matrix Factorization (GMF) and Multi Layer Perceptron (MLP) recommenders, resembling a Wide&Deep model, having higih generalization power. Plus, neural nets make easier to handle large volumes of data, and it better leverages the power of GPUs! For more info, refer to the [article](https:\/\/arxiv.org\/abs\/1708.05031).","4d05358c":"Create model and train!","7f31ccc1":"## We lower the number of users for quicker exploration & training, although you can comment the next line.\n### Everything fits in memory, so it won't crash!","0533dd03":"Embeddings need a compressed index representation of animes: Let's make a quick mapping"}}