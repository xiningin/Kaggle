{"cell_type":{"0f87a872":"code","039b3175":"code","2ead6f34":"code","24e88731":"code","47d0beb4":"code","489539cb":"code","e2f3f923":"code","3010c0a9":"code","d62af797":"code","7e72433a":"code","81abe48d":"code","5f3a3e8b":"code","3dc581a2":"code","eddeb9b8":"code","02cf8d51":"code","760e84af":"code","c2455077":"code","61d3f8d7":"code","4568c420":"code","e82c4f15":"code","04a02b3a":"code","da4fe11e":"code","b0a109a1":"code","e88dba00":"code","ac7ac1cc":"code","5251bc61":"code","864e3957":"code","47204159":"code","a9fb5373":"code","594b1b8c":"code","75226559":"code","fbf18e65":"code","fa3ef193":"code","23a0721c":"code","91c3dca4":"markdown","c5ae1f87":"markdown","2c3c3aa1":"markdown","b71ac48d":"markdown","acbd6e4d":"markdown","c5166ee1":"markdown","a0abdf81":"markdown","6b98b0bd":"markdown","4d45565c":"markdown","899da27c":"markdown","0123e923":"markdown"},"source":{"0f87a872":"#This project was largely a learning opportunity to understand compromises between Domain-based inferences and model building\n#The flow of the project will explain the train of thought to justify use of specific features for further model building,\n#and defend the use of new features made on the way.","039b3175":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n#ensemble\nfrom xgboost import XGBClassifier\n#metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report,confusion_matrix\n\n#warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2ead6f34":"df = pd.read_csv('\/kaggle\/input\/lt-vehicle-loan-default-prediction\/train.csv')\n\ndf=df.drop(['UniqueID', 'branch_id','supplier_id', 'Current_pincode_ID','State_ID', 'Employee_code_ID', 'MobileNo_Avl_Flag'],axis=1)","24e88731":"def credit_risk(df):\n    d1=[]\n    d2=[]\n    for i in df:\n        p = i.split(\"-\")\n        if len(p) == 1:\n            d1.append(p[0])\n            d2.append('unknown')\n        else:\n            d1.append(p[1])\n            d2.append(p[0])\n\n    return d2\n\nsub_risk = {'unknown':-1, 'A':13, 'B':12, 'C':11,'D':10,'E':9,'F':8,'G':7,'H':6,'I':5,'J':4,'K':3, 'L':2,'M':1}\nemployment_map = {'Self employed':0, 'Salaried':1,np.nan:2}\n\ndf.loc[:,'credit_risk_grade']  = credit_risk(df[\"PERFORM_CNS.SCORE.DESCRIPTION\"])\ndf.loc[:,'Credit Risk'] = df['credit_risk_grade'].apply(lambda x: sub_risk[x])\n\ndf.loc[:,'Employment Type'] = df['Employment.Type'].apply(lambda x: employment_map[x])\n\ndf=df.drop(['PERFORM_CNS.SCORE.DESCRIPTION', 'credit_risk_grade','Employment.Type'],axis=1)","47d0beb4":"def age(dur):\n    yr = int(dur.split('-')[2])\n    if yr >=0 and yr<=19:\n        return yr+2000\n    else:\n         return yr+1900\n\ndf['Date.of.Birth'] = df['Date.of.Birth'].apply(age)\ndf['DisbursalDate'] = df['DisbursalDate'].apply(age)\ndf['Age']=df['DisbursalDate']-df['Date.of.Birth']\ndf=df.drop(['DisbursalDate','Date.of.Birth'],axis=1)","489539cb":"numerical=['disbursed_amount','asset_cost','PRI.NO.OF.ACCTS',\n       'PRI.ACTIVE.ACCTS', 'PRI.OVERDUE.ACCTS', 'PRI.CURRENT.BALANCE',\n       'PRI.SANCTIONED.AMOUNT', 'PRI.DISBURSED.AMOUNT', 'SEC.NO.OF.ACCTS',\n       'SEC.ACTIVE.ACCTS', 'SEC.OVERDUE.ACCTS', 'SEC.CURRENT.BALANCE',\n       'SEC.SANCTIONED.AMOUNT', 'SEC.DISBURSED.AMOUNT', 'PRIMARY.INSTAL.AMT',\n       'SEC.INSTAL.AMT', 'NEW.ACCTS.IN.LAST.SIX.MONTHS',\n       'DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS','NO.OF_INQUIRIES','Age','NEW.ACCTS.IN.LAST.SIX.MONTHS', \n        'DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS']\ncategorical=['manufacturer_id', 'Aadhar_flag', 'PAN_flag',\n       'VoterID_flag', 'Driving_flag', 'Passport_flag', 'PERFORM_CNS.SCORE',\n       'NEW.ACCTS.IN.LAST.SIX.MONTHS', 'DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS',\n       'AVERAGE.ACCT.AGE', 'NO.OF_INQUIRIES', 'Credit Risk','AVERAGE.ACCT.AGE','CREDIT.HISTORY.LENGTH',\n       'Employment Type']","e2f3f923":"#T Test for numerical columns\np=[]\nfrom scipy.stats import ttest_ind\n\nfor i in numerical:\n    df1=df.groupby('loan_default').get_group(0)\n    df2=df.groupby('loan_default').get_group(1)\n    t,pvalue=ttest_ind(df1[i],df2[i])\n    p.append(1-pvalue)\nplt.figure(figsize=(7,7))\nsns.barplot(x=p, y=numerical)\nplt.title('Best Numerical Features')\nplt.axvline(x=(1-0.05),color='r')\nplt.xlabel('1-p value')\nplt.show()","3010c0a9":"for i in numerical:\n    df1=df.groupby('loan_default').get_group(0)\n    df2=df.groupby('loan_default').get_group(1)\n    print(np.std(df1[i],ddof=1),np.std(df2[i],ddof=1))","d62af797":"from sklearn.feature_selection import SelectKBest,f_classif\nn = SelectKBest(score_func=f_classif, k=10)\nnumcols=n.fit(df[numerical],df['loan_default'])\nplt.figure(figsize=(7,7))\nsns.barplot(x=numcols.scores_,y=numerical)\nplt.title('Best Numerical Features')\nplt.show()","7e72433a":"df.loc[:,'No of Accounts'] = df['PRI.NO.OF.ACCTS'] + df['SEC.NO.OF.ACCTS']\ndf.loc[:,'PRI Inactive accounts'] = df['PRI.NO.OF.ACCTS'] - df['PRI.ACTIVE.ACCTS']\ndf.loc[:,'SEC Inactive accounts'] = df['SEC.NO.OF.ACCTS'] - df['SEC.ACTIVE.ACCTS']\ndf.loc[:,'Total Inactive accounts'] = df['PRI Inactive accounts'] + df['SEC Inactive accounts']\ndf.loc[:,'Total Overdue Accounts'] = df['PRI.OVERDUE.ACCTS'] + df['SEC.OVERDUE.ACCTS']\ndf.loc[:,'Total Current Balance'] = df['PRI.CURRENT.BALANCE'] + df['SEC.CURRENT.BALANCE']\ndf.loc[:,'Total Sanctioned Amount'] = df['PRI.SANCTIONED.AMOUNT'] + df['SEC.SANCTIONED.AMOUNT']\ndf.loc[:,'Total Disbursed Amount'] = df['PRI.DISBURSED.AMOUNT'] + df['SEC.DISBURSED.AMOUNT']\ndf.loc[:,'Total Installment'] = df['PRIMARY.INSTAL.AMT'] + df['SEC.INSTAL.AMT']","81abe48d":"#Chi Square test for Categorical Columns\nfrom scipy.stats import chi2_contingency\nl=[]\nfor i in categorical:\n    pvalue  = chi2_contingency(pd.crosstab(df['loan_default'],df[i]))[1]\n    l.append(1-pvalue)\nplt.figure(figsize=(7,7))\nsns.barplot(x=l, y=categorical)\nplt.title('Best Categorical Features')\nplt.axvline(x=(1-0.05),color='r')\nplt.show()","5f3a3e8b":"def duration(dur):\n    yrs = int(dur.split(' ')[0].replace('yrs',''))\n    mon = int(dur.split(' ')[1].replace('mon',''))\n    return yrs*12+mon","3dc581a2":"df['AVERAGE.ACCT.AGE'] = df['AVERAGE.ACCT.AGE'].apply(duration)\ndf['CREDIT.HISTORY.LENGTH'] = df['CREDIT.HISTORY.LENGTH'].apply(duration)\n#df.drop(['AVERAGE.ACCT.AGE','CREDIT.HISTORY.LENGTH'],axis=1,inplace=True)","eddeb9b8":"df=df.drop(['PRI.NO.OF.ACCTS','SEC.NO.OF.ACCTS','PRI.CURRENT.BALANCE','PRI Inactive accounts','SEC Inactive accounts',\n            'PRI.SANCTIONED.AMOUNT','SEC.NO.OF.ACCTS','PRI.NO.OF.ACCTS','PRI.DISBURSED.AMOUNT','PRI.ACTIVE.ACCTS', \n            'PRI.OVERDUE.ACCTS','SEC.CURRENT.BALANCE','SEC.SANCTIONED.AMOUNT', 'SEC.OVERDUE.ACCTS',\n            'SEC.DISBURSED.AMOUNT','PRIMARY.INSTAL.AMT','SEC.INSTAL.AMT','disbursed_amount','SEC.ACTIVE.ACCTS'],axis=1)","02cf8d51":"nums=['asset_cost', 'ltv','PERFORM_CNS.SCORE',\n       'NEW.ACCTS.IN.LAST.SIX.MONTHS', 'DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS',\n       'AVERAGE.ACCT.AGE', 'CREDIT.HISTORY.LENGTH', 'NO.OF_INQUIRIES','No of Accounts', 'Total Inactive accounts',\n       'Total Overdue Accounts', 'Total Current Balance', 'Total Sanctioned Amount',\n       'Total Disbursed Amount', 'Total Installment','Age']","760e84af":"len(nums)","c2455077":"y=df.loan_default\nX=df.drop(\"loan_default\",axis=1)\nfrom sklearn.model_selection import train_test_split,KFold,cross_val_score\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\nlr=LogisticRegression()\nlr.fit(X_train,y_train)\ny_pred=lr.predict(X_test)\nprint('train accuracy :',lr.score(X_train,y_train))\nprint('test accuracy :',lr.score(X_test,y_test))\nprint(\"precision :\",precision_score(y_test,y_pred),\"\\n\")\nprint(\"recall :\",recall_score(y_test,y_pred),\"\\n\")\nprint(\"f1 score:\",f1_score(y_test,y_pred),\"\\n\")\nprint(classification_report(y_test,y_pred))","61d3f8d7":"sns.countplot(df['loan_default'])","4568c420":"n=['PERFORM_CNS.SCORE','NO.OF_INQUIRIES','No of Accounts', 'Total Inactive accounts',\n       'Total Overdue Accounts', 'Total Current Balance', 'Total Sanctioned Amount',\n       'Total Disbursed Amount', 'Total Installment']\ndata=df[n]\nfig, axes = plt.subplots(nrows=3, ncols=3,figsize=(20,10))\nfig.subplots_adjust(hspace=0.5)\nfig.suptitle('Distributions of Credit History Features')\n\nfor ax, feature, name in zip(axes.flatten(), data.values.T, data.columns):\n    sns.distplot(feature, ax=ax)\n    ax.set(title=str(name))\nplt.show()","e82c4f15":"from sklearn.preprocessing import  RobustScaler\nrob_scaler = RobustScaler()\n\ndf[nums] = rob_scaler.fit_transform(df[nums])","04a02b3a":"df['Missing Features'] = (df == 0).astype(int).sum(axis=1)","da4fe11e":"y=df.loan_default\nX=df.drop(\"loan_default\",axis=1)\nfrom sklearn.model_selection import train_test_split,KFold,cross_val_score\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)","b0a109a1":"from sklearn.model_selection import GridSearchCV","e88dba00":"param_test1 = {\n 'max_depth':range(3,10,2),\n 'min_child_weight':range(1,6,2)\n}\ngsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=3)\ngsearch1.fit(X_train,y_train)\ngsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_","ac7ac1cc":"param_test2b = {\n 'max_depth':range(7,10,2)\n}\ngsearch2b = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=4,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test2b, scoring='roc_auc',n_jobs=4,iid=False, cv=3)\ngsearch2b.fit(X_train,y_train)","5251bc61":"gsearch2b.cv_results_, gsearch2b.best_params_, gsearch2b.best_score_","864e3957":"param_test3 = {\n 'gamma':[i\/10.0 for i in range(0,5)]\n}\ngsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=9,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch3.fit(X_train,y_train)\ngsearch3.best_params_, gsearch3.best_score_","47204159":"xgb4 = XGBClassifier(\n learning_rate =0.01,\n n_estimators=5000,\n max_depth=9,\n min_child_weight=1,\n gamma=0.4,\n subsample=0.8,\n colsample_bytree=0.8,\n reg_alpha=0.005,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)","a9fb5373":"xgb4.fit(X_train,y_train)\ny_pred=xgb4.predict(X_test)\nprint(\"accuracy train:\",xgb4.score(X_train,y_train),\"\\n\")\nprint(\"accuracy test:\",xgb4.score(X_test,y_test),\"\\n\")\nprint(\"precision :\",precision_score(y_test,y_pred),\"\\n\")\nprint(\"Recall :\",recall_score(y_test,y_pred),\"\\n\")\nprint(\"f1 score:\",f1_score(y_test,y_pred),\"\\n\")\nprint(\"Confusion Matrix \\n\",confusion_matrix(y_test,y_pred))","594b1b8c":"import sklearn.metrics as metrics","75226559":"probs = xgb4.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)","fbf18e65":"print(fpr, tpr)","fa3ef193":"roc_auc = metrics.auc(fpr, tpr)\n\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC-AUC Curve')\nplt.show()","23a0721c":"plt.figure(figsize=(7,10))\nsns.barplot(x=xgb4.feature_importances_,y=X.columns)\nplt.title('Significant Features of the Final Model')\nplt.show()","91c3dca4":"We'll use SelectKBest library to narrow down choices of features. This will make use of Annova test.","c5ae1f87":"Here too, the graph shows acceptance of Alternate Hypothesis. Again, we can observe some multicollinearity between credit score and PAN Card,\nsince PAN Card is mandatory to obtain credit score. Hence, the Chi Square test deemed it surplus to our research. But for argument sake, we'll\nkeep it.","2c3c3aa1":"Voila! we smashed the record set on all these scores.","b71ac48d":"Now this is a bad start to model building. The model isn't able to predict the 1s here. Maybe class-imbalance here. Plus, there is huge difference between macro and weighted scores. So, lets see the distribution of the target variable.","acbd6e4d":"The above observations are Standard Deviations of Columns (i.e., Defaulters vs Non-defaulters). T Test for independence runs on the assumption that samples under observations should have equal Standard Deviations. Although the difference observed above seems less, it still doesn't seem acceptable. Hence, we'll go with a non-parametric test.","c5166ee1":"Let's deal with outliers. Instead of using popular methods, we'll try to preserve the outlier records instead of removing them. We'll use RobustScaler to bring the observations closer to the median value (here, zero in those credit history columns).","a0abdf81":"Now, for the zero observations, we'll make a new feature that counts the features having zero. This will act as yardstick between people who have a credit history and those who dont. Of course, people with no credit history will likely have more than 9 features as zero, unlikely in case of customers with credit history.","6b98b0bd":"For visual purpose, I redid the graph to show acceptance of 'Alternative Hypothesis' for T Test. So, the ones crossing the red lines show\nstatistical significance. But for vehicle loans and educational loans, it is observed that Banks don't want to compromise of loosing any \ndetails of Secondary Account holder, since them also being the guarantor for the loan in case of a default.\nBut, there is a problem in this T Test, as will be observed in the next cell\n","4d45565c":"Maximum number of customers are applying for loan for the first time, which explains why all the above columns have so many zero values. Also, there are many notable outliers, which makes the problem more complicated.","899da27c":"The graph shows that Secondary Account informations are insignificant. Again of course, banks can't afford to drop Secondary Account informations, so we'll have to combine the Primary and Secondary Account informations. ","0123e923":"Now let's try on our base model. We'll use XGBoost to reduce bias and variance errors, and do some hyperparameter tuning."}}