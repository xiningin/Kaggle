{"cell_type":{"b2ccdc50":"code","b8a001dc":"code","d94b11a0":"code","5f74d2cd":"code","e6021188":"code","0eac5b09":"code","92b63523":"code","b3b52c1a":"code","1bdb48b8":"code","81139d83":"markdown","9a8f7080":"markdown","338e9a53":"markdown","bac7ae8e":"markdown","4d06b34e":"markdown","06c6e858":"markdown","c99eff3d":"markdown","03d499c6":"markdown","c0b6ea59":"markdown","04b8a61a":"markdown"},"source":{"b2ccdc50":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\n\nwcrms = df_train['wheezy-copper-turtle-magic'].unique()","b8a001dc":"import numpy as np\nfrom scipy import stats\n\nclass SSGaussianMixture(object):\n    def __init__(self, n_features, n_categories):\n        self.n_features = n_features\n        self.n_categories = n_categories\n        \n        self.mus = np.array([np.random.randn(n_features)]*n_categories)\n        self.sigmas = np.array([np.eye(n_features)]*n_categories)\n        self.pis = np.array([1\/n_categories]*n_categories)\n        \n        \n    def fit(self, X_train, y_train, X_test, threshold=0.00001, max_iter=100):\n        Z_train = np.eye(self.n_categories)[y_train] \n        \n        for i in range(max_iter):\n        # EM algorithm\n            # M step\n            Z_test = np.array([self.gamma(X_test, k) for k in range(self.n_categories)]).T\n            Z_test \/= Z_test.sum(axis=1, keepdims=True)\n        \n            # E step\n            datas = [X_train, Z_train, X_test, Z_test]\n            mus = np.array([self._est_mu(k, *datas) for k in range(self.n_categories)])\n            sigmas = np.array([self._est_sigma(k, *datas) for k in range(self.n_categories)])\n            pis = np.array([self._est_pi(k, *datas) for k in range(self.n_categories)])\n            \n            diff = max(np.max(np.abs(mus-self.mus)), \n                       np.max(np.abs(sigmas-self.sigmas)), \n                       np.max(np.abs(pis-self.pis)))\n            #print(diff)\n            self.mus = mus\n            self.sigmas = sigmas\n            self.pis = pis\n            if diff<threshold:\n                break\n                \n                \n    def predict_proba(self, X):\n        Z_pred = np.array([self.gamma(X, k) for k in range(self.n_categories)]).T\n        Z_pred \/= Z_pred.sum(axis=1, keepdims=True)\n        return Z_pred\n\n\n    def gamma(self, X, k):\n        # X is input vectors, k is feature index\n        return stats.multivariate_normal.pdf(X, mean=self.mus[k], cov=self.sigmas[k])\n        \n    def _est_mu(self, k, X_train, Z_train, X_test, Z_test):\n        mu = (Z_train[:,k]@X_train + Z_test[:,k]@X_test).T \/ \\\n                 (Z_train[:,k].sum() + Z_test[:,k].sum())\n        return mu\n    \n    def _est_sigma(self, k, X_train, Z_train, X_test, Z_test):\n        cmp1 = (X_train-self.mus[k]).T@np.diag(Z_train[:,k])@(X_train-self.mus[k])\n        cmp2 = (X_test-self.mus[k]).T@np.diag(Z_test[:,k])@(X_test-self.mus[k])\n        sigma = (cmp1+cmp2) \/ (Z_train[:,k].sum() + Z_test[:k].sum())\n        return sigma\n        \n    def _est_pi(self, k, X_train, Z_train, X_test, Z_test):\n        pi = (Z_train[:,k].sum() + Z_test[:,k].sum()) \/ \\\n                 (Z_train.sum() + Z_test.sum())\n        return pi\n        ","d94b11a0":"# Below is just a lapper object.\n\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\n\nclass BaseClassifier(object):\n    def __init__(self):\n        self.preprocess = Pipeline([('vt', VarianceThreshold(threshold=2)), ('scaler', StandardScaler())])\n \n\n    def fit(self, X_train, y_train, X_test, cv_qda=2, cv_meta=2):\n        X_train_org = X_train\n        self.preprocess_tune(np.vstack([X_train, X_test]))\n        X_train = self.preprocess.transform(X_train)\n        X_test = self.preprocess.transform(X_test)\n        \n        self.cgm = SSGaussianMixture(n_features=X_train.shape[1], n_categories=2)\n        self.validation(X_train_org, y_train)\n        self.cgm.fit(X_train, y_train, X_test)\n\n    \n    def predict(self, X):\n        X = self.preprocess.transform(X)\n        return self.cgm.predict_proba(X)[:,1]\n    \n    \n    def preprocess_tune(self, X):\n        self.preprocess.fit(X)\n                \n        \n    def validation(self, X, y):\n        X = self.preprocess.transform(X)\n        kf = KFold(n_splits=3, shuffle=True)\n        scores = []\n        for train_index, test_index in kf.split(X):\n            X_train, X_test = X[train_index], X[test_index]\n            y_train, y_test = y[train_index], y[test_index]\n            self.cgm.fit(X_train, y_train, X_test)\n            y_pred = self.cgm.predict_proba(X_test)[:,1]\n            scores.append(roc_auc_score(y_test, y_pred))\n        self.score = np.array(scores).mean()\n        print('validation score = ', self.score)","5f74d2cd":"df_train_sample = df_train[df_train['wheezy-copper-turtle-magic']==wcrms[3]]\nX_train_sample = df_train_sample.drop(['id', 'target', 'wheezy-copper-turtle-magic'], axis=1).values\ny_train_sample = df_train_sample['target'].values\n\ndf_test_sample = df_test[df_test['wheezy-copper-turtle-magic']==wcrms[3]]\nX_test_sample = df_test_sample.drop(['id', 'wheezy-copper-turtle-magic'], axis=1).values","e6021188":"bc = BaseClassifier()\nbc.fit(X_train_sample, y_train_sample, X_test_sample)","0eac5b09":"class ConsolEstimator(object):\n    def __init__(self, ids):\n        self.clfs = {}\n        self.id_column = 'wheezy-copper-turtle-magic'\n        self.ids = ids\n        \n        \n    def predict(self, df_X):\n        y_pred = np.zeros(shape=(len(df_X)))\n        for id in df_X[self.id_column].unique():\n            id_rows = (df_X[self.id_column]==id)\n            X = df_X.drop(['id', self.id_column], axis=1).values[id_rows]\n            y_pred[id_rows] = self.clfs[id].predict(X)\n        return y_pred\n            \n        \n    def fit(self, df_train, df_test):\n        for i, id in enumerate(self.ids):\n            print(i, 'th training...')\n            df_train_id = df_train[df_train[self.id_column]==id]\n            df_test_id = df_test[df_test[self.id_column]==id]\n            if len(df_train_id)==0 or len(df_test_id)==0:\n                continue\n            \n            X_train = df_train_id.drop(['id', 'target', self.id_column], axis=1).values\n            y_train = df_train_id['target'].values\n            X_test = df_test_id.drop(['id', self.id_column], axis=1).values\n            \n            self.clfs[id] = BaseClassifier()\n            self.clfs[id].fit(X_train, y_train, X_test)\n            \n        print('mean score = ', np.array([clf.score for clf in self.clfs.values()]).mean())","92b63523":"ce = ConsolEstimator(ids=wcrms)\nce.fit(df_train, df_test)","b3b52c1a":"y_pred = ce.predict(df_test)","1bdb48b8":"df_submission = pd.concat([df_test['id'], pd.Series(y_pred, name='target')], axis=1)\ndf_submission.to_csv('submission.csv', index=False)","81139d83":"---\n# Load Data","9a8f7080":"Then I finally get\n$$\n\\begin{align}\n\\mu_s &=\\frac{\\Sigma_{n=1}^{N_{train}}z_{ns}x_n+\\Sigma_{n=1}^{N_{test}}\\gamma(z_{ns}x_n)}\n{\\Sigma_{n=1}^{N_{train}}z_{ns}+\\Sigma_{n=1}^{N_{test}}\\gamma(z_{ns})} \\\\\n\\Sigma_s &=\\frac{\\Sigma_{n=1}^{N_{train}}z_{ns}(x_n-\\mu_s)(x_n-\\mu_s)^T+\\Sigma_{n=1}^{N_{test}}\\gamma(z_{ns}(x_n-\\mu_s)(x_n-\\mu_s)^T)}\n{\\Sigma_{n=1}^{N_{train}}z_{ns}+\\Sigma_{n=1}^{N_{test}}\\gamma(z_{ns})} \\\\\n\\pi_s &=\\frac{\\Sigma_{n=1}^{N_{train}}z_{ns}+\\Sigma_{n=1}^{N_{test}}\\gamma(z_{ns})}\n{\\Sigma_t(\\Sigma_{n=1}^{N_{train}}z_{nt}+\\Sigma_{n=1}^{N_{test}}\\gamma(z_{nt}))}\n\\end{align}\n$$\n(Sorry, I deduct other than $\\mu_s$ intuitively, then they may be wrong).","338e9a53":"> ---\n# Theory","bac7ae8e":"In Gaussian Mixture model, we maximize likelihood function $P(X_{train}|\\pi,\\mu,\\Sigma)$ about $\\pi, \\mu, \\Sigma$ by using EM algorithm (here, $\\pi$ means distribution parameter of label Y, $\\mu$ and $\\Sigma$ are set of mean vector and covariance matrix for each categories).\n\nIn this competiton, we have $X_{train}, Y_{tain}, X_{test}$. Gaussian Mixture model can treat $X_{train}, X_{test}$, and QDA (and other normal supervised model) can treat $X_{train}, Y_{train}$. Neither of them can treat $X_{train}, Y_{tain}, X_{test}$ at the same time.\n\nThen I tried to modify EM algorithm for Gaussian Mixture as to treat these at the same time. Equations are slightly complicated, but result is not so difficult. Please see chapter 9 of PRML book for detail (I used notation Z for representation of label-variable instead of Y, because I deduct formulas based on PRML).","4d06b34e":"$$\n\\begin{align}\nP(X_{train},Y_{train},X_{test}|\\pi,\\mu,\\Sigma)\n&= P(X_{train},Y_{train}|\\pi,\\mu,\\Sigma)P(X_{test}|\\pi,\\mu,\\Sigma) \\\\\n&= \\Pi_{n=1}^{N_{train}}(\\Pi_{k=1}^K \\pi_k^{z_{nk}} N(x_n|\\mu_k,\\Sigma_k)^{z_{nk}})\n\\Pi_{n=1}^{N_{test}}\\Sigma_{k=1}^K \\pi_k N(x_n|\\mu_k,\\Sigma_k)\n\\end{align}\n$$\nand take log of this becomes below (log-likelihood).\n$$\n\\log P = \\Sigma_{n=1}^{N_{train}}\\Sigma_{k=1}^K z_{nk}(\\log \\pi_k + \\log N(x_n|\\mu_k,\\Sigma_k))\n+\\Sigma_{n=1}^{N_{test}}\\log\\Sigma_{k=1}^K \\pi_k N(x_n|\\mu_k,\\Sigma_k)\n$$","06c6e858":"---\n# Submission","c99eff3d":"I tried hyper parameter tuning and bugging, but LB score didn't improve. \nWhen I used BayesianGaussianMixture instead of GaussianMixture in stacking, my CV score improved, then if we can treat above model in Bayesian, score may improve.\nBut for me, treating Gauss-Wishart distribution is hard (sklean implementation of GayesianGaussianMixture can be used as a refference).","03d499c6":"Let's compare this to normal Gaussian Mixture model.\n$$\nNormal Gaussian Mixture \\\\\n\\mu_s = \\frac{\\Sigma_{n=1}^{N_{test}}\\gamma(z_{ns}x_n)}\n{\\Sigma_{n=1}^{N_{test}}\\gamma(z_{ns})}.\n$$\nThe meaning is very simple. We just use actual $z_{nk}$ instead of $\\gamma(z_{ns})$ for taking weighted average step.","c0b6ea59":"---\n# Modeling","04b8a61a":"take derivatives of this is\n$$\n\\frac{\\partial\\log P}{\\partial\\mu_s}\n=\\Sigma_{n=1}^{N_{train}}z_{ns}\\Sigma^{-1}_s(x_n-\\mu_s)\n+\\Sigma_{n=1}^{N_{test}}\\frac{\\pi_s N(x_n|\\mu_s,\\Sigma_s)}{\\Sigma_{k=1}^K\\pi_kN(x_n|\\mu_k,\\Sigma_k)}\n\\Sigma^{-1}_s(x_n-\\mu_s)=0\n$$\ntherefore, if we assume $\\Sigma_s$ is non-singular and define as below,\n$$\n\\gamma(z_{ns}):=\\frac{\\pi_s N(x_n|\\mu_s,\\Sigma_s)}{\\Sigma_{k=1}^K\\pi_k N(x_n|\\mu_k,\\Sigma_k)}\n$$\nwe get\n$$\n\\mu_s = \\frac{\\Sigma_{n=1}^{N_{train}}z_{ns}x_n+\\Sigma_{n=1}^{N_{test}}\\gamma(z_{ns}x_n)}\n{\\Sigma_{n=1}^{N_{train}}z_{ns}+\\Sigma_{n=1}^{N_{test}}\\gamma(z_{ns})}.\n$$"}}