{"cell_type":{"0cf944e6":"code","72f7682d":"code","cdf0eabb":"code","d8a678fa":"code","96167d9f":"code","fbe9ed01":"code","049482e4":"code","df725400":"code","c8af9b6d":"code","19eee667":"code","fab849f2":"code","3ed29805":"code","715284ef":"code","d50c677f":"code","be5af5ed":"code","5c1bd516":"code","62e39d57":"code","32e6c2be":"code","bbdd1c83":"code","4fad6cd7":"code","3a033984":"code","c3d58ef9":"code","5bb8b4f8":"code","79cd64e1":"markdown","e67ff9d4":"markdown","ff8724fb":"markdown","8fded535":"markdown","10c453c1":"markdown","dee22181":"markdown","0284e7ae":"markdown","3e68234e":"markdown","f33abbcc":"markdown","7de72788":"markdown","ba27b975":"markdown"},"source":{"0cf944e6":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nfrom sklearn import preprocessing","72f7682d":"def cut_to_n(cut):\n    return ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal'].index(cut)\n\ndef color_to_n(color):\n    return ['D', 'E', 'F', 'G', 'H', 'I','J'].index(color)\n\ndef clarity_to_n(clarity):\n    return ['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF'].index(clarity)","cdf0eabb":"data_train = pd.read_csv(\"..\/input\/diamonds-ds-ft-2109\/diamonds_train.csv\")\ndata_train = data_train.iloc[:,1:]\ndata_train['n_cut'] = data_train['cut'].map(cut_to_n)\ndata_train['n_color'] = data_train['color'].map(color_to_n)\ndata_train['n_clarity'] = data_train['clarity'].map(clarity_to_n)","d8a678fa":"fig = px.imshow(data_train.corr())\nfig.show()","96167d9f":"data_train = data_train.drop(labels=['cut','color','clarity'],axis=1)","fbe9ed01":"x = data_train.drop(labels=['price'],axis=1)\ny = data_train['price']","049482e4":"x.head()","df725400":"data_train.plot.scatter(x=\"y\", y=\"price\", alpha=0.5)\ndata_train.plot.scatter(x=\"carat\", y=\"price\", alpha=0.5)\ndata_train.plot.scatter(x=\"x\", y=\"price\", alpha=0.5)","c8af9b6d":"from sklearn.model_selection import train_test_split\n(\n    X_train,\n    X_test,\n    y_train,\n    y_test,\n    indices_train,\n    indices_test,\n) = train_test_split(x, y,list(data_train.index), test_size=0.25,random_state=21)","19eee667":"scaler = preprocessing.StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","fab849f2":"intp = keras.Input(shape=[X_train.shape[1]])\nx = layers.Dense(128,activation='relu')(intp)\nx = layers.BatchNormalization()(x)\nx = layers.Dropout(.3)(x)\nx = layers.Dense(128,activation='relu')(x)\nx = layers.BatchNormalization()(x)\nx = layers.Dropout(.2)(x)\nx = layers.Dense(32,activation='relu')(x)\nx = layers.BatchNormalization()(x)\nx = layers.Dropout(.1)(x)\nx = layers.Dense(16,activation='relu')(x)\noutp = layers.Dense(1)(x)\nmodel = keras.Model(intp, outp, name=\"regressor\")\nmodel.summary()","3ed29805":"def get_lr_callback(epoch,lr):\n    lr_start   = 0.001\n    lr_max     = 0.5#0.00000125 * 1 * batch_size\n    lr_min     = 0.0001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 5\n    lr_decay   = 0.95\n    \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        return lr\n    \n    return lrfn(epoch)\n\nlr_callback = tf.keras.callbacks.LearningRateScheduler(get_lr_callback, verbose=True)\nx = [x for x in range(150)]\nplt.plot(x,[get_lr_callback(x,.1) for x in x])\nplt.show()","715284ef":"\nearlystop = tf.keras.callbacks.EarlyStopping(\n    monitor='val_root_mean_squared_error', patience=20, verbose=2,\n    mode='auto', restore_best_weights=True\n)","d50c677f":"model.compile(\n    optimizer=tf.optimizers.Adam(),\n    loss='mse',\n    metrics=[tf.keras.metrics.RootMeanSquaredError()])","be5af5ed":"hist = model.fit(X_train,y_train,epochs=300,validation_data=(X_test,y_test),batch_size=128,callbacks=[lr_callback,earlystop])","5c1bd516":"def plot_loss(history):\n    plt.plot(history.history['root_mean_squared_error'][1:], label='rmse train')\n    plt.plot(history.history['val_root_mean_squared_error'][1:], label='rmse test')\n    plt.xlabel('Epoch')\n    plt.ylabel('Error [RMSE]')\n    plt.legend()\n    plt.grid(True)\n    \nplot_loss(hist)","62e39d57":"pred = model.predict(scaler.transform(data_train.drop([\"price\"],axis=1)))\npred = pred.reshape(pred.shape[0])","32e6c2be":"import seaborn as sns\n\ndata_train[\"residual\"] = (pred - data_train[\"price\"])\n\ndata_train[\"in_training\"] = [x in indices_train for x in range(len(data_train))]\n\nsns.scatterplot(x=data_train[\"price\"],y=data_train[\"residual\"],hue=data_train[\"in_training\"])","bbdd1c83":"data_train[data_train[\"in_training\"]][\"residual\"].describe()","4fad6cd7":"#data_train[data_train[\"in_training\"]][data_train[\"residual\"]<-4000].index  #","3a033984":"data_test = pd.read_csv(\"..\/input\/diamonds-ds-ft-2109\/diamonds_test.csv\")\ndata_test = data_test.iloc[:,1:]\ndata_test['n_cut'] = data_test['cut'].map(cut_to_n)\ndata_test['n_color'] = data_test['color'].map(color_to_n)\ndata_test['n_clarity'] = data_test['clarity'].map(clarity_to_n)\ndata_test = data_test.drop(labels=['cut','color','clarity'],axis=1)\ndata_test.head()","c3d58ef9":"x = scaler.transform(data_test)\n\n\ndata_test[\"price\"] = model.predict(x)\n\nresult = data_test[\"price\"]\nresult.index.names = [\"id\"]\nresult.rename_axis(\"price\")\nresult.head()","5bb8b4f8":"result.to_csv('submission.csv')","79cd64e1":"# **Read & Convert data to number :**","e67ff9d4":"# **Make my model** ","ff8724fb":"# **My final data form :**","8fded535":"# **Scaling data to normal distribution** (gives same weigth to all coloms)","10c453c1":"# **Learing : plots**","dee22181":"# **Submit**","0284e7ae":"# **Pls Upvote if you enjoyed =D**","3e68234e":"# **Training**","f33abbcc":"# **Coorelations :**","7de72788":"If there is a hudge missed value it will affect all predictions","ba27b975":"# **Split train and test 75% \/ 25%**"}}