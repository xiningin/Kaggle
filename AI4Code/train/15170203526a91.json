{"cell_type":{"9d8a449f":"code","01ae82f2":"code","dae754b9":"code","bf27a7c9":"code","6f5061f6":"code","86ef6f67":"code","1cc55d60":"code","e8574760":"code","92768f9c":"code","b35507b5":"code","cdd64517":"code","232fbf56":"code","8de2a830":"code","e6873cf4":"code","549a808c":"code","9f8a34f2":"code","4be7de18":"markdown","2d658a32":"markdown","c688c211":"markdown","c94059bc":"markdown","3ebc869f":"markdown","6315824d":"markdown","11829be9":"markdown","fbf4f294":"markdown","ca3813b1":"markdown","35646165":"markdown","b87e150d":"markdown","1d1ab15a":"markdown","fdbf8638":"markdown","86a32c09":"markdown","19ba2f4c":"markdown","dcb2939d":"markdown","851b4877":"markdown"},"source":{"9d8a449f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\n\ndata = pd.read_csv('..\/input\/CreditCard.csv')","01ae82f2":"# checking the head\ndata.head()","dae754b9":"X = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values  # although the dependent variable is stored here, we won't be using it in the model","bf27a7c9":"from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()\nX = sc.fit_transform(X)","6f5061f6":"!pip install minisom\nfrom minisom import MiniSom","86ef6f67":"som = MiniSom(x=10, y=10, input_len=15, sigma=1.0, learning_rate=0.5)","1cc55d60":"som.random_weights_init(X)","e8574760":"som.train_random(data=X, num_iteration=100)","92768f9c":"# the library used for this specific purpose is 'pylab'\nfrom pylab import bone, pcolor, colorbar, plot, show","b35507b5":"plt.figure(figsize = (20,10))\n# Step 4.1 - initializing the figure (the window having the map)\nbone()\n\n# Step 4.2 - putting different WNs on the map \n# this is done by using different colors corresponding to the different range of values of the MIDs\n# i.e we add the info of the MIDs for all the WNs that the SOM identified \n# done by the distance_map() method which returns the matrix of all the MIDs in it for all the WNs\npcolor(som.distance_map().T)  # .T means taking transpose of this matrix to get the values in order for this function","cdd64517":"# we need to run all the lines corresponding to the visualization together hence we write the lines again\nplt.figure(figsize = (20,10))\nbone()\npcolor(som.distance_map().T)\ncolorbar()  # legend","232fbf56":"plt.figure(figsize = (20,10))\nbone()\npcolor(som.distance_map().T)\ncolorbar() \n\n# 'red circle' marker will represent customer who didn't get approval and 'green square' marker will represnt who did\nmarkers = ['o', 's']\ncolors = ['red', 'green']\n\n# creating the loop to apply the logic above\nfor i, j in enumerate(X):          \n    wn = som.winner(j)             \n    plot(wn[0] + 0.5, wn[1] + 0.5, markers[y[i]], markeredgecolor = colors[y[i]], markerfacecolor = 'None', markersize=15,\n                markeredgewidth = 2)  \n    \nshow()","8de2a830":"mappings = som.win_map(X) ","e6873cf4":"frauds = mappings[(7,6)]","549a808c":"frauds = sc.inverse_transform(frauds)\ndf_frauds = pd.DataFrame(frauds)","9f8a34f2":"df_frauds","4be7de18":"**How does this work?**\n* We get a key, in this case let's say (0,0) i.e the co-ordinates for lower left WN in SOM (the 1st WN in the SOM)\n* Then we get the list of all the customers asociated to that specific WN\n\n### Step - 7\n**Catching the Cheaters :**\n* The outlying winning node we recognized from the map was at the position (7,6)","2d658a32":"* Hence we can conclude that highest MID is white color and lowest one is represented by dark color\n* We get a range of values from 0 to 1 as we normalized the data\n* Thus the outlying WNs\/outliers\/frauds, which are far from the general rules are represented by white\n* We can also see a majority of WNs having high MIDs are close to each other\n* To conclude we can say that the WNs with the highest MIDs are the fraud applications\n\n### Step - 5 :\n* Now we need to create a list of customers by inverse maping the WNs to see which customers are associated to the WNs indicating the fraud application\n* We can also put markers to see which customers got approval and which didn't\n* **Because the customer who cheated and yet got approval are more relevant target to fraud detection than the ones who cheated cheat and didn't get approval**\n* Let's see where the customers are on the SOM\n\n**Understanding the for loop below :**\n* `i` is the index of all customers and `j` is the vector of customers at diff. iterations\n* Then we get the winning node for every customer\n* Then we place the color marker for the specific WN at the centre of the square\n* Then we decide where we need to put a red circle or a green square by knowing whether that specific customer got the approval or not\n* And we do this by taking help of the 'dependent variable vector' we seperated above which has the actual values \n* Then we associate it with markers and colors we defined above","c688c211":"## Idealogoy of the SOM :\n* Every customer is an input to the neural network and these points are going to be mapped to a new output space\n* In between this input and output space, we have a neural network composed of neurons\n* Each neuron is being initialized as a vector of weights that is the same size as of the vector of customers, i.e a vector of 15 elements(columns in dataset)\n* Hence for each one of these obseravtion point(customer), the output of this customer will be the neuron closest to the customer\n* In short we pick the neuron in the network closest to the customer and this neuron is called the **Winning Node (WN)** or **Best Matching Unit (BMU)**\n* For each customer the WN is the most similar neuron to the customer\n* Then we use different functions to update the weights of the winning node to move them closer to the actual points \n* This is done for every customer and then we repeat this agin and again\n* Hence this reduces the dimensions every time we iterate through the data again and again\n* Hence the Self-Organzied Map is used for **Dimensionality Reduction**\n* Each time this is repeated the output space decreases and loses dimensions\n* After this we get a SOM in 2D with all the WNs that were eventually identified and in turn we get closer towards identifying frauds\n\n**How to detect frauds?**\n* Frauds would be nothing but the outliers in our SOM, i.e something far from the general rule\n* Hence the outlying neurons would be considered as the outliers in this 2D SOM\n\n**What do we need?**\n* We need something called as **Mean Inter-neuron Distance (MID)**\n* For each neuron in the network we are going to compute the mean Euclidean Distance between a nueron and the neurons in its repsective neighbourhood and by doing this, we can detect outliers\n* Because the outliers will be far from all the neurons in its neighbourhood\n* This neighbourhood needs to be defined manually\n* All of this info will be mapped on the SOM to detect frauds (outliers)\n\n**How to conclude?**\n* In the end we will use an invserse mapping function to identify which customers orignally in the input space are associated to this WN, which is an outlier\n\n","c94059bc":"* **Here we get a list of all the customers associated to this WN, i.e all the cheaters**\n* The values are scaled hence its hard to recognize them\n\n\n**Inverse scaling the scaled values**","3ebc869f":"* We can see that the data is already labelled by the `Class` variable as `0`(application rejected) and `1`(application approved)\n* Hence we need to get rid of this column in order to achieve the goal of performing Unsupervised Deep Learning\n* On the SOM we would be able to distinguish who got approval and who didn't as we can know by prioritizing who cheated and yet got the approval\n\n### Step - 2 :\n**Creating the subset of the independent and dependent variable**","6315824d":"### Step - 3 :\n* Using a class MiniSom","11829be9":"### Step - 3 :\n**Feature Scaling : Normalization**\n* There are high computations required and the data has many dimensions with lots of non linear relationships\n* Thus we scale the data to crush the impact of variable with larger weight in the analysis\n* This method equalizes range and variability in the dataset","fbf4f294":"### Step - 3.2 :\n* Initializing the weights","ca3813b1":"# Credit Card Fraud Detection using Self-Organzied Map\n\n### **Objective :**\n* Suppose we are Deep Learning Scientists working at a bank and we are given a datatset regarding the customers of this bank who are applying for an advanced Credit Card\n* This data would be info that a customer provides while applying for the card\n* And our job is to detect potential frauds among these applications\n* In the end we should be able to give a list to the Manager of the customers who cheated on their application\n\n### **Approach :**\n* Unlike the Machine Learning Models, where we predict whether each customer might be a potential fradulent by already training our machine based on previously labelled fraudster as YES or NO, here our approach would be to not consider the dependent variable and plot a map which shows us what a fraudster would look like\n* We will use an **Unsupervised Deep Learning Algorithm** called **Self-Organzing Map**, which means we will be identifying patterns in high dimensional datasets full of non linear relationships and one of these patterns(in this case the customers) will be the potential frauds or the customers who cheated\n* In more simple terms I will be doing Customer Segmentation to identify segments of customers and one of the segments will contain the customers who cheated\n\n### Goals of this project :\n* Understanding the idealogy of Self Organizing Map\n* Implementing concepts like - Winning Node, Dimensionality Reduction, Mean Inter-neuron Distance \n* Interpreting outliers in the SOM \n* Understanding the need of Normalization\n* Interpreting the SOM in detail and making it more interactive by mapping the info from the dataset\n* Inverse mapping, inverse scaling and getting the list of potential fraudsters\n\n### **DataSet Information :**\n* This file concerns credit card applications. All attribute names and values have been changed to meaningless symbols to protect confidentiality of the data\n* This dataset is interesting because there is a good mix of attributes -- continuous, nominal with small numbers of values, and nominal with larger numbers of values\n* The dataset consists of 14 feature variables and 1 class label that quantifies the approval decision. Not much is known about the 14 features themselves for the sake of confidentiality and for convenience of processing statistical algorithms\n* This means that even the feature names are not present, but whether they are continuous or categorical is known\n\n**Variables :**\n\n    * A1: 0,1 CATEGORICAL (formerly: a,b)\n    * A2: continuous.\n    * A3: continuous.\n    * A4: 1,2,3 CATEGORICAL (formerly: p,g,gg)\n    * A5: 1, 2,3,4,5, 6,7,8,9,10,11,12,13,14 CATEGORICAL (formerly: ff,d,i,k,j,aa,m,c,w, e, q, r,cc, x)\n    * A6: 1, 2,3, 4,5,6,7,8,9 CATEGORICAL (formerly: ff,dd,j,bb,v,n,o,h,z)\n    * A7: continuous.\n    * A8: 1, 0 CATEGORICAL (formerly: t, f)\n    * A9: 1, 0 CATEGORICAL (formerly: t, f)\n    * A10: continuous.\n    * A11: 1, 0 CATEGORICAL (formerly t, f)\n    * A12: 1, 2, 3 CATEGORICAL (formerly: s, g, p)\n    * A13: continuous.\n    * A14: continuous.\n    * A15: 1,2 class attribute (formerly: +,-)\n    \n\n### Step - 1 :\n* Importing the Libraries and the dataset\n","35646165":"**Argument used :**\n* `num_iteration` is the no. of times we want to repeat the process discussed above","b87e150d":"**Arguments used :**\n* `x` and `y` are the dimensions of the grid of our map\n* `sigma` is the radius of different neigbourhoods in the grid\n* `learning_rate` decides how much the weights are updated during each iteration","1d1ab15a":"### Step - 4 :\n* Visualizing the results to identify the outlying neurons inside the map\n\n**What to look for in SOM?**\n* We will be seeing a 2D grid that will contain all the final WNs and for each of these WNs we will get the MID (Mean Inter-neuron Distance)\n* Higher the MID, then more the WN is far away from its neighbours\n* Hence, higher the MID, the more it is an outlier\n* Since majority of the WNs represent the rules that are respected, while an outlying nueron from this majority of neurons is therefore far from the general rules\n* This is how we detect the fraud (outliers), i.e considering the WNs having the highest MID\n* All of this will be distinguished by colors and not values, i.e the closer the color will be to white if the WN has a higher MID\n","fdbf8638":"### Step - 3.1 :\n* Initializing the SOM","86a32c09":"* So what we did here is created the SOM with all different colors corresponding to the MIDs\n* But we need to see if white color corresponds to a higher or lower MID and same for the dark colors\n* Hence we add a legend","19ba2f4c":"### Conclusion :\n* We got the Customer IDs of the cheaters\/fraudsters and a list of 20 such people\n* The bank's analyst will investigate the list\n* What this analyst will probably do is get the actual values of all the cheaters\n* He\/She will prioritize the cheaters that are approved to revise the appication and then further investigate how did the customer managed to cheat","dcb2939d":"**Take aways from the SOM above :**\n* Now we not only have the MIDs, but also we plotted the customers who got approval and the ones who didn't for each of the WN\n* Now if you observe at the upper right section which didn't get approval, we can see that the customers associated to that WN didn't get approval either\n* This also indicates that there is a high risk of fraud for the customers associated to that particular WN\n\n**Note :**\n* We also see some cheaters who got approval and got away with it (the highest MID having both the markers in it) at the co-ordinate(x=7, y=6) - check from the lower left corner\n* Hence there is a need to catch this potential cheaters in the WNs\n\n### Step - 6 :\n* Making a list of the cheaters by mapping\n* We use the method win_map that returns the dictionary of all the mappings from the WNs to the customers\n","851b4877":"### Step - 3.3 :\n* Training the SOM on X "}}