{"cell_type":{"28001ea1":"code","d159c42b":"code","51548d6d":"code","8bd5db69":"code","4d138d3c":"code","c97c2d52":"code","c358660c":"code","77a7d5e1":"code","c904b098":"code","e12eeb65":"code","e4004660":"code","96bd77d2":"code","bfcb52ce":"code","0b233f83":"code","e63c69fc":"code","452fb327":"code","a10a19a8":"code","67681071":"code","cfa03ebd":"code","b986083c":"code","8170fdb3":"code","b5d08888":"code","3580a611":"code","afe48f0c":"code","2628adb8":"code","f8ab61f7":"code","54178820":"code","7aff6d50":"code","c445d388":"code","81b188b8":"code","b65a31c0":"code","784c5d31":"code","c640e837":"code","90bdaf55":"code","d39c8758":"code","b7b9cc50":"code","2ef22e4e":"markdown","d0c142c0":"markdown","9f3e8ae6":"markdown","f05d5c8d":"markdown","205997de":"markdown","958c302e":"markdown","726c8f70":"markdown","39c62e6e":"markdown","1e20d6d5":"markdown","194aa27f":"markdown","6a7aa531":"markdown","d06ff913":"markdown","c79c95d1":"markdown","6660f1bd":"markdown","a3375634":"markdown","ee01ede4":"markdown"},"source":{"28001ea1":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob\n\n%matplotlib inline\npd.options.display.max_columns = None\npd.options.display.max_rows = None\n# %load_ext rpy2.ipython\n\nCOMPETITION_NAME = 'cap-4611-spring-21-assignment-1'\nDATA_DIR = '.\/data'\nRANDOM_STATE = 1337\nnp.random.seed(RANDOM_STATE)\n\n# create data directory if not exists\nif not os.path.exists(DATA_DIR):\n    os.makedirs(DATA_DIR)\n\n!kaggle competitions download -c '{COMPETITION_NAME}' -p '{DATA_DIR}\/'\n!unzip -nq '{DATA_DIR}\/{COMPETITION_NAME}' -d '{DATA_DIR}\/'\n!rm '{DATA_DIR}\/{COMPETITION_NAME}.zip'","d159c42b":"paths = dict()\n\nfor path in glob(f'{DATA_DIR}\/*'):\n    filename = os.path.basename(path)\n    fn = os.path.splitext(filename)[0]\n    paths[fn] = path\n\npaths","51548d6d":"train_df = pd.read_csv(paths['train'], index_col=0)\ntest_df = pd.read_csv(paths['test'], index_col=0)","8bd5db69":"pd.DataFrame(dict(train_df.isna().sum()), index=['n_na'])","4d138d3c":"len(train_df) - train_df.loc[:,train_df.columns[0]].sum(), train_df.loc[:,'Bankrupt'].sum()","c97c2d52":"train_df.describe()","c358660c":"# plt.figure(figsize=(20,30))\n# sns.heatmap(train_df.corr())\n# plt.show()","77a7d5e1":"## https:\/\/machinelearningmastery.com\/a-gentle-introduction-to-normality-tests-in-python\/\n\nfrom scipy.stats import shapiro\n\n\nlooks_gaussian = []\nnot_looks_gaussian = []\ncolumns = train_df.select_dtypes(include=['float64']).columns\n\nfor col in columns:\n    data = train_df.loc[:,col].to_numpy()\n    stat, p = shapiro(data)\n\n    alpha = 0.05\n    if p > alpha:\n        looks_gaussian.append((col.strip(), stat, p))\n    else:\n        not_looks_gaussian.append((col.strip(), stat, p))\n\nprint(len(looks_gaussian), len(not_looks_gaussian))","c904b098":"## https:\/\/machinelearningmastery.com\/how-to-use-statistics-to-identify-outliers-in-data\/\n\nbad_rows = set()\noutlier_factors = dict()\n\nfor col in columns:\n    data = train_df.loc[:,col].to_numpy()\n    q25, q75 = np.percentile(data, 25), np.percentile(data, 75)\n    iqr = q75 - q25\n\n    cut_off = iqr * 1.5\n    lower, upper = q25 - cut_off, q75 + cut_off\n\n    outlier_factors[col] = dict(lower=lower, upper=upper)\n\n    for i, x in enumerate(data):\n        if x > lower and x < upper:\n            bad_rows.add(i)\n\nprint(len(bad_rows), len(train_df))","e12eeb65":"patched_df = train_df.copy()\n\nfor col in columns:\n    lower, upper = outlier_factors[col].values()\n    condition = (patched_df[col] > lower) & (patched_df[col] < upper)\n    median = patched_df[col].median()\n    patched_df[col] = np.where(condition, median, patched_df[col])\n\n    print('{:4d} {}'.format(condition.sum(), col.strip()))","e4004660":"patched_df.describe()","96bd77d2":"patched_df.loc[:,patched_df.columns[0]].sum()","bfcb52ce":"from sklearn.preprocessing import Normalizer\n\n# X = train_df.loc[:,train_df.columns.difference(['Bankrupt','one if net income was negative for the last two year zero otherwise','one if total liabilities exceeds total assets zero otherwise'])]\nX = train_df.loc[:,patched_df.columns.difference(['Bankrupt'])]\ny = train_df.loc[:,['Bankrupt']]\n\n# transformer = Normalizer().fit(X)\n# X_normal = transformer.transform(X)\n# X_normal = pd.DataFrame(X_normal, columns=X.columns)\n# X_normal['one if net income was negative for the last two year zero otherwise'] = patched_df.loc[:,['one if net income was negative for the last two year zero otherwise']]\n# X_normal['one if total liabilities exceeds total assets zero otherwise'] = patched_df.loc[:,['one if total liabilities exceeds total assets zero otherwise']]","0b233f83":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)","e63c69fc":"from sklearn.metrics import classification_report, roc_curve, roc_auc_score\n\ndef report(clf, X=None, y=None, only_best=True):\n    scores = clf.cv_results_['mean_test_score']\n\n    if not only_best:\n        for score, p in zip(scores, clf.cv_results_['params']):\n            print('Accuracy: %0.4f | %r' % (score, p))\n\n    print('Best params:', clf.best_params_)\n    print('Best accuracy:', clf.best_score_)\n    print('\\nClassification report:')\n    y_pred = clf.predict(X)\n    print(classification_report(y, y_pred))\n\n## Source: https:\/\/stackabuse.com\/understanding-roc-curves-with-python\/\n\ndef plot_roc_curve(model, X=None, y=None, title='ROC Curve'):\n    scores = model.predict_proba(X)[:,1]\n    fpr, tpr, _ = roc_curve(y, scores)\n    AUC = roc_auc_score(y_test, scores)\n    plt.plot(fpr, tpr, label='AUC: {:.4f}'.format(AUC))\n    plt.plot([0, 1], [0, 1], linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(title)\n    plt.legend()","452fb327":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier as DTC\n\nparam_grid = {\n    'criterion': ['gini','entropy'],\n    'max_features': ['auto'],\n    'max_depth': [1, 4, 8, None],\n    'class_weight': ['balanced']\n}\n\ndtc_clf = GridSearchCV(DTC(random_state=RANDOM_STATE), param_grid=param_grid, verbose=3, n_jobs=-1)\ndtc_clf.fit(X_train, y_train)","a10a19a8":"report(dtc_clf, X=X_test, y=y_test)","67681071":"try:\n    params = {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': None, 'max_features': 'log2'}\n    params = dtc_clf.best_params_\nexcept:\n    pass\n\ndtc_model = DTC(**params, random_state=RANDOM_STATE)\ndtc_model.fit(X_train, y_train)","cfa03ebd":"plot_roc_curve(dtc_model, X=X_test, y=y_test, title='ROC Curve for Decision Tree')","b986083c":"importances = dtc_model.feature_importances_\nindices = np.argsort(importances)[::-1]\n\nfig, ax = plt.subplots(figsize=(20,5))\nplt.title('Feature importances')\nplt.bar(range(X_train.shape[1]), importances[indices], align='center')\nplt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\nplt.xlim([-1, X_train.shape[1]])\nplt.title('Feature Importance for Decision Tree Classifier')\nplt.show()","8170fdb3":"from sklearn.ensemble import RandomForestClassifier as RFC\n\nparam_grid = {\n    'n_estimators': [10000,15000],\n    'max_features': ['auto'],\n    'criterion': ['entropy'],\n    'max_features': ['auto'],\n    'max_depth': [None],\n    'class_weight': ['balanced','balanced_subsample']\n}\n\nrfc_clf = GridSearchCV(RFC(n_jobs=-1, random_state=RANDOM_STATE), param_grid=param_grid, verbose=3, n_jobs=-1)\nrfc_clf.fit(X_train, y_train)","b5d08888":"report(rfc_clf, X=X_test, y=y_test)","3580a611":"# if dtc_clf has not run, then just use these default hyper parameters\ntry:\n    params = {'class_weight': 'balanced_subsample', 'max_depth': None, 'max_features': 'auto', 'n_estimators': 5000}\n    params = dtc_clf.best_params_\nexcept:\n    pass\n\nrfc_model = RFC(**params, random_state=RANDOM_STATE)\nrfc_model.fit(X_train, y_train)","afe48f0c":"plot_roc_curve(rfc_model, X=X_test, y=y_test, title='ROC Curve for Random Forest')","2628adb8":"importances = rfc_model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rfc_model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\n\nfig, ax = plt.subplots(figsize=(20,5))\nplt.title('Feature importances')\nplt.bar(range(X_train.shape[1]), importances[indices], yerr=std[indices], align='center')\nplt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\nplt.xlim([-1, X_train.shape[1]])\nplt.title('Feature Importance for Random Forest Classifier')\nplt.show()","f8ab61f7":"from sklearn.preprocessing import Normalizer\n\nskip_features = [' ROA(C) before interest and depreciation before interest','long-term liability to current assets', 'total expense \/assets',\n       ' operating gross margin', ' cash reinvestment %',\n       ' net worth turnover rate (times)', 'Gross profit to Sales',\n       ' realized sales gross margin', ' inventory turnover rate (times)',\n       'current assets\/total assets',\n       ' inventory and accounts receivable\/net value',\n       'Quick asset\/Total asset', ' research and development expense rate',\n       ' contingent liabilities\/net worth',\n       'one if total liabilities exceeds total assets zero otherwise',\n       'one if net income was negative for the last two year zero otherwise']\n\n# X = train_df.loc[:,features]\nX = train_df.loc[:,train_df.columns.difference(['Bankrupt',*skip_features])]\ny = train_df.loc[:,['Bankrupt']]\n\n# X_val = test_df.loc[:,features]\nX_val = test_df.loc[:,test_df.columns.difference(skip_features)]","54178820":"rfc_model = RFC(n_estimators=15000, class_weight='balanced_subsample', criterion='entropy', max_depth=None, n_jobs=-1, random_state=RANDOM_STATE)\nrfc_model.fit(X, y)","7aff6d50":"importances = rfc_model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rfc_model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\n\nfig, ax = plt.subplots(figsize=(20,5))\nplt.title('Feature importances')\nplt.bar(range(X.shape[1]), importances[indices], yerr=std[indices], align='center')\nplt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)\nplt.xlim([-1, X.shape[1]])\nplt.title('Feature Importance for Random Forest Classifier')\nplt.show()","c445d388":"X.columns[indices]","81b188b8":"y_pred = rfc_model.predict_proba(X_val)\nout_df = pd.DataFrame(y_pred[:,1], columns=['Bankrupt'], index=range(len(y_pred)))\nout_df.index.name = 'id'","b65a31c0":"out_df.head()","784c5d31":"out_df.to_csv('.\/submission.csv')","c640e837":"!kaggle competitions submit -c cap-4611-spring-21-assignment-1 -f .\/submission.csv -m 'Standardized and no handling of outliers'","90bdaf55":"from sklearn.utils import resample\n\nparams = {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'log2'}\nmodels = []\nn = 10000\n\nfor i in range(n):\n    print('\\r{:03d}\/{}'.format(i+1,n), end='')\n\n    vX, vy = resample(X_train, y_train)\n    dtc_model = DTC(**params)\n    models.append(dtc_model.fit(vX, vy))","d39c8758":"from sklearn.metrics import accuracy_score\n\nscores = []\nfor i, m in enumerate(models):\n    print('\\r{:03d}\/{}'.format(i+1,n), end='')\n    y_pred = m.predict(X_test)\n    scores.append(accuracy_score(y_test, y_pred))","b7b9cc50":"a = .95\np = (1 - a)\/2 * 100\nlower = max(0, np.percentile(scores, p))\n\np = (a + (1 - a)\/2) * 100\nupper = min(1, np.percentile(scores, p))\nprint('{:.2f} confidence interval {:.2f}% and {:.2f}%'.format(a*100, lower*100, upper*100))\n\nplt.hist(scores)\nplt.show()","2ef22e4e":"# Decision Tree Classifier","d0c142c0":"# Random Forest Classifier Findings\n\nThe Random Forest classifier performed significantly better than the plain Decision Tree, with an AUC of .8944 it definitely showed improvement.","9f3e8ae6":"I'm going to be replacing every outlier with the median of that feature. This works because the median is not affected by outliers.","f05d5c8d":"# Actual Conclusion\n\nI found that a reduced model performed better than a full model. These features have been picked due to their low feature importance as provided by the RFC.","205997de":"# To Normalize or to Standardize?\nMy dataset is does not represent a Gaussian distribution (as shown above using the Shapiro-Wilk test for normality). Therefore, I will only be normalizing by data and not stardadizing. Another reason not to standardize can be seen in the difference of mean and standard deviation show in the latter dataframe `patched_df.describe()`.\n\nNote: So after several dozen retries at this I noticed that I actually made the dataset worse by the preprocessing I did. Therefore, I'm leaving the dataset untouched.","958c302e":"# Loading the dataset\nLoads the dataset into their appropriate `train` and `test` dataframes.","726c8f70":"# Check for outliers\nTo start off I'm going to check what distribution this data follows. This way I can decide how to remove outliers from the data. Since I like stats and stuff I'll be using the Shapiro-Wilk test as shown below. We get that the data is does not represent a Gaussian distribution.","39c62e6e":"# Create train test split for dataset\nI will be using a test size of 20%.","1e20d6d5":"# Parsing data folder\nGoes through the folder and grabs all the directories within it.","194aa27f":"# Model Selection\nThis `report` function will return some descriptive stats about the model passed.","6a7aa531":"# Decision Tree Classifier Findings\n\nSo, this is bad. An AUC of .6352 means that I'm barely above randomly guessing. This is due to the fact that this dataset is heavily unbalanced. I had add weights to the rows that pertained to the target of \"Bankrupt\" and this is what is causing a horrible AUC. Although, I believe this is favorable over not even taking into account the target of \"Bankrupt.\"\n\nIn conclusion, this model likes to say everyone is financially stable. Also, most features are really not that important in predicting if someone will go bankrupt.","d06ff913":"# Other stuff\nThis is just me attempting to figure out why I was getting an out of training AUC of ~.4","c79c95d1":"# Check for missing values in train data\nI wont be checking the testing data since it's supposed to work as expected.","6660f1bd":"# Random Forest Classifier\n\nI tried rerunning this notebook clean, but made the mistake of adding another zero to the 15000 below. My computer did not like this.","a3375634":"There's several different ways to look for outliers. I'll be using the interquartile Range method (IQR) since the data does not follow a Gaussian distribution. According to our finding below, every single row countains at least one outlier.","ee01ede4":"# Fetching data\nI'll be loading all data into a folder called data. The only requirement to run this notebook is to have your kaggle cli setup."}}