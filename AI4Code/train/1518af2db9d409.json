{"cell_type":{"90c7c9a2":"code","e6aafdb1":"code","b536dc24":"code","9e1f3fd5":"code","6e4e0200":"code","50b8cdce":"code","4392a90f":"code","f9d86fc5":"code","484be247":"code","64a67d81":"code","d4a66433":"code","e0140b98":"code","34a044a0":"code","e23ce2d5":"code","8ee92ffc":"code","96f3e7a6":"code","d2709caa":"code","1f21abe2":"code","5bd7fbf2":"code","0ccf2eeb":"code","5946172e":"code","23e074b4":"code","7db29e60":"code","f2ebae0a":"code","37c3db7a":"code","ab840702":"code","26ca9c2c":"code","81a91efe":"code","d051f423":"code","3e25ea92":"code","51f2eefa":"code","ae663cdb":"code","c3baec6d":"code","1feac26f":"code","76c36900":"code","daa33f79":"code","54fdf1b4":"code","aa27406b":"code","51d15788":"code","1b70203f":"code","f07a30dc":"code","3e69b120":"code","aef20ffe":"code","a94309b8":"code","1ce89f12":"code","b67da133":"code","dc163b90":"code","1516e58a":"code","0155c32a":"code","4aef403c":"markdown","fc8febdc":"markdown","0793e21f":"markdown","438cc9da":"markdown","e1ad2700":"markdown","aa660f9a":"markdown","b630cf5e":"markdown","e7804b14":"markdown","49351c35":"markdown","7adffa53":"markdown","905ff6ed":"markdown","3f8bf014":"markdown","b29689fc":"markdown","57ffc5f7":"markdown","b1cf5cfa":"markdown","b3c832a8":"markdown","8219ce78":"markdown","3d51aa32":"markdown","752f42b6":"markdown","133fcf96":"markdown","ed7aaf3e":"markdown","2e08545c":"markdown","184ed8c0":"markdown","47c0bfdf":"markdown","78c8efa0":"markdown","77dc3a1d":"markdown","d933cdb4":"markdown","5fc07808":"markdown","f95a8954":"markdown","95005f50":"markdown","54d50bc5":"markdown","aa651958":"markdown","a945182d":"markdown","b039e881":"markdown","280ffe0d":"markdown","5df15414":"markdown","4c3040a2":"markdown","4264bed7":"markdown","03c84d2d":"markdown","e6f0bb3f":"markdown","ae91799c":"markdown","179236c5":"markdown","cc6bd10a":"markdown","6ba3c7f5":"markdown","91de5b5d":"markdown","e0c3bdf6":"markdown","6d80e738":"markdown","8e4accab":"markdown"},"source":{"90c7c9a2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBRFRegressor\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split","e6aafdb1":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","b536dc24":"train.head()","9e1f3fd5":"train.shape","6e4e0200":"train.duplicated().sum()","50b8cdce":"test.head()","4392a90f":"test.shape","f9d86fc5":"corr_df = train.corr(method='pearson')\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr_df, annot=False)\nplt.show()","484be247":"corr = train.corr()[['SalePrice']].sort_values(by='SalePrice',ascending=False)\ncorr[corr.SalePrice>0.5]","64a67d81":"figure, ax = plt.subplots(1,3, figsize = (20,6))\nx = train.OverallQual\ny = train.SalePrice\nsns.countplot(x=x, ax = ax[0])\nsns.barplot(x=x, y=y, ax = ax[1])\nsns.boxplot(x=x, y=y, ax = ax[2])\nplt.show()","d4a66433":"plt.figure(figsize=(6,4))\nsns.distplot(x=train.GrLivArea)\nplt.show()","e0140b98":"plt.figure(figsize=(6,4))\nplt.scatter(train.GrLivArea, train.SalePrice, c = \"green\")\nplt.title(\"GrLivArea vs SalePrice\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()","34a044a0":"figure, ax = plt.subplots(1,3, figsize = (20,6))\nx = train.GarageCars\ny = train.SalePrice\nsns.countplot(x=x, ax = ax[0])\nsns.barplot(x=x, y=y, ax = ax[1])\nsns.boxplot(x=x, y=y, ax = ax[2])\nplt.show()","e23ce2d5":"train.SalePrice.describe()","8ee92ffc":"sns.distplot(train.SalePrice)\nplt.show()","96f3e7a6":"from scipy.stats import skew\nfrom scipy.stats import kurtosis\nprint( 'excess kurtosis of normal distribution (should be 0): {}'.format( kurtosis(train.SalePrice) ))\nprint( 'skewness of normal distribution (should be 0): {}'.format( skew(train.SalePrice) ))","d2709caa":"y = np.log1p(train.SalePrice)","1f21abe2":"ntrain = train.shape[0]\nntest = test.shape[0]\ntest_id = test.Id\ny_train = train.SalePrice.values\ndata = pd.concat((train, test)).reset_index(drop=True)\ndata.drop(['SalePrice'], axis=1, inplace=True)\ndata.shape","5bd7fbf2":"data.drop(['Id'], axis=1, inplace=True)","0ccf2eeb":"percentage = 100 * data.isnull().sum() \/ len(data)\ndata_types = data.dtypes\nskew = data.skew()\nmissingData = pd.concat([percentage, data_types, skew], axis=1)\nmissingData = missingData.rename(columns = {0 : 'Percentage', 1 : 'Data Types', 2 : 'Skew'})\nmissingData = missingData.sort_values(by ='Percentage',ascending=False)\nmissingData = missingData[missingData.Percentage>0]\nmissingData.shape","5946172e":"missingData","23e074b4":"print(len(data[data.GarageType.isnull() & data.GarageArea >=1]))\nprint(len(data[data.MasVnrType.isnull() & data.MasVnrArea >=1]))\nprint(len(data[data.FireplaceQu.isnull() & data.Fireplaces >=1]))","7db29e60":"for col in ('Utilities', 'MiscFeature', 'Alley', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond',\n            'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MSSubClass', 'PoolQC'):\n    data[col] = data[col].fillna('None')\n\ndata.Functional = data.Functional.fillna(\"Typ\")\n\ndata.MSZoning = data.MSZoning.fillna(data.MSZoning.mode()[0])\ndata.Utilities = data.Utilities.fillna(data['Utilities'].mode()[0])\ndata.Electrical = data.Electrical.fillna(data['Electrical'].mode()[0])\ndata.KitchenQual = data.KitchenQual.fillna(data['KitchenQual'].mode()[0])\ndata.Exterior1st = data.Exterior1st.fillna(data['Exterior1st'].mode()[0])\ndataExterior2nd = data.Exterior2nd.fillna(data['Exterior2nd'].mode()[0])\ndata.SaleType = data.SaleType.fillna(data['SaleType'].mode()[0])","f2ebae0a":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    data[col] = data[col].fillna(0)","37c3db7a":"data['YrSold'] = data['YrSold'].apply(str)\ndata['MoSold'] = data['MoSold'].apply(str)","ab840702":"numeric_feats = data.dtypes[data.dtypes != object].index\ndatanum = data[numeric_feats]\ndatanum = datanum.skew(axis = 0, skipna = True).sort_values()\ndatanum = datanum[datanum>1] #feature is skewed when the value is more than 1 or less than -1\nskewness = pd.DataFrame({'Skew' :datanum})\nskewed_features = skewness.index","26ca9c2c":"skewness","81a91efe":"for c in skewed_features:\n    data[c] = data[c].apply('log1p')","d051f423":"ordinal_variable = ['OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',\n    'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual',\n    'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'Street', 'Alley',\n    'LandSlope', 'Functional', 'GarageFinish', 'MoSold', 'YrSold', 'PavedDrive', 'CentralAir', 'LotShape' ]","3e25ea92":"for c in ordinal_variable:\n  data[c] = data[c].astype(str)\n\nfor c in ordinal_variable:\n    lbl = LabelEncoder() \n    lbl.fit(list(data[c].values)) \n    data[c] = lbl.transform(list(data[c].values))","51f2eefa":"data.shape","ae663cdb":"nominal_variable = ['MSSubClass', 'MSZoning', 'LandContour', 'Utilities',\n    'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n    'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', \n    'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', 'Electrical', \n    'GarageType', 'MiscFeature', 'SaleType', 'SaleCondition', 'YrSold', 'MoSold']","c3baec6d":"for c in nominal_variable:\n  data[c] = data[c].astype(str)","1feac26f":"data = pd.get_dummies(data)\ndata.shape","76c36900":"data[\"SqFtPerRoom\"] = data[\"GrLivArea\"] \/ (data[\"TotRmsAbvGrd\"] + data[\"FullBath\"] + data[\"HalfBath\"] + data[\"KitchenAbvGr\"])","daa33f79":"data['Total_Home_Quality'] = data['OverallQual'] + data['OverallCond']","54fdf1b4":"data['Total_Bathrooms'] = (data['FullBath'] + (0.5 * data['HalfBath']) + data['BsmtFullBath'] + (0.5 * data['BsmtHalfBath']))","aa27406b":"data[\"HighQualSF\"] = data['TotalBsmtSF'] + data[\"1stFlrSF\"] + data[\"2ndFlrSF\"]","51d15788":"train = data[:ntrain] #Rebuild the initial training data\ntest = data[ntrain:]","1b70203f":"X = train\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.25,random_state=0)\nprint(X_train.shape)\nprint(X_val.shape)","f07a30dc":"def hyperParameterTuning(X_train, y_train):\n    param_tuning = {\n        \"max_depth\": [2, 3, 4, 5,7,10],\n        \"n_estimators\": [200, 300, 400, 500, 600, 700, 1000],\n        \"learning_rate\": [0.01, 0.05, 0.1],\n        'min_child_weight': [1, 3, 5, 10],\n        'colsample_bytree': [0.3, 0.4, 0.6, 0.8],\n        'objective': ['reg:squarederror']\n    }\n\n    xgb_model = XGBRegressor()\n\n    gsearch = GridSearchCV(estimator = xgb_model,\n                           param_grid = param_tuning,                        \n                           cv = 5,\n                           n_jobs = -1,\n                           verbose = 1)\n\n    gsearch.fit(X_train,y_train)\n\n    return gsearch.best_params_","3e69b120":"##hyperParameterTuning(X_train, y_train)","aef20ffe":"xgb_model = XGBRegressor(objective = 'reg:squarederror',learning_rate=0.05, subsample=1,\n                         min_child_weight = 1,n_estimators=1000,max_depth=6, colsample_bytree=0.3,\n                         colsample_bylevel=0.7, colsample_bynode=0.7,\n                         gamma = 0.01)\n\nxgb_reg = xgb_model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_val, y_val)], verbose=False)","a94309b8":"y_pred_xgb = xgb_model.predict(X_val)\n\nmae_xgb = mean_absolute_error(y_val, y_pred_xgb)","1ce89f12":"mae_xgb","b67da133":"plt.figure(figsize=(8, 6))\nplt.scatter(y_val, y_pred_xgb)\nplt.show()","dc163b90":"plt.figure(figsize=(8, 6))\nplt.scatter(X_val['GrLivArea'], y_val, color='blue', label='Real',    alpha=0.5)\nplt.scatter(X_val['GrLivArea'], y_pred_xgb,  color='red' , label='Predict', alpha=0.5)\nplt.title(\"Real vs Predict\")\nplt.legend(loc='best')\nplt.show()","1516e58a":"plt.figure(figsize=(1, 5))\nxgb.plot_importance(xgb_reg, max_num_features=10)\nplt.show()","0155c32a":"y_pred = xgb_model.predict(test)\ny_pred = np.expm1(y_pred)\nsubmission = pd.DataFrame({'Id':test_id,'SalePrice':y_pred})\n\nsubmission.to_csv(\"submission.csv\",index=False)","4aef403c":"# Load all librairies","fc8febdc":"We can see that when the type of garage is missing, there isn't the size of the garage either. We will impute the categorial data with 'none' instead of putting the mode (most frequent value of the entire feature column). It's the same for the 2 other examples.","0793e21f":"## Cleaning ","438cc9da":"For the other variable with NaN values : *Utilities, MSZoning, Electrical, Exterior1st, Exterior2nd, SaleType* and *LotFrontage*, there isn't indication in the data description. NaN values will be replaced by the mode of the column.","e1ad2700":"## Target variable : SalePrice","aa660f9a":"### One hot encoding\nThis method doesn't keep the order between values, it's for categorical dummy variables. It simply creates additional features based on the number of unique values in the categorical feature. Every unique value in the category will be added as a feature. Contrary to the previous method, new features will be added.","b630cf5e":"### Feature engineering\nTo improve the accuracy of the model, we can add new features","e7804b14":"**Plot Real vs Predict** in function of one of the most correlated feature : GrLivArea","49351c35":"### Label encoder \nMachines understand numbers, not text. We need to convert each text category to numbers in order for the machine to process them using mathematical equations. This method keeep the order between values, it's for ordinal categorical variables. Label encoding uses alphabetical ordering.","7adffa53":"## K fold cross validation to find the best hyperparameters","905ff6ed":"We can see some outliers, the points with a grade living area greater than 4,000 square feet. In order to have better performance, we need to remove them.","3f8bf014":"## Transformation\n","b29689fc":"Regarding the whole dataset, there are 34 features with missing values. The main features with missing values are PoolQC, MiscFeature, Alley, Fence. They have more than 80% of missing. To handle this,there are 3 ways : drop columns with missing values, imputation and an extension To imputation (imputation + add a new column that shows the location of the imputed entries). In this study, i will use the second method.","57ffc5f7":"### Asymetric data (skewed)","b1cf5cfa":"**Mean absolute error of the prediction for the valuation sample:**","b3c832a8":"# Prediction of the test sample","8219ce78":"**There are 4 variables which evoke the number of bathrooms :**","3d51aa32":"The sample is quite small. Thus, to have a better prediction, we will preprocess together the train sample and test sample. ","752f42b6":"## Analysis of the 3 most revelant variables \nOverallQual, GrLivArea and GarageCars","133fcf96":"**Total living area suare feet :**","ed7aaf3e":"*   *PoolQC* (Pool quality) -> NaN means no pool \n* *MiscFeature* (Miscellaneous feature not covered in other categories) -> NaN means no additional feature\n* *Alley* (Type of alley access) -> NaN means no alley \n* *Fence* (Fence quality) -> NaN means no fence\n* *FireplaceQu* (Fireplace quality) -> NaN means no fire place\n* *GarageType*, *GarageFinish*, *GarageQual* and *GarageCond* -> NaN means no garage\n*  *BsmtQual*, *BsmtCond*, *BsmtExposure*, *BsmtFinType1* and *BsmtFinType2* -> NaN means no basement\n* *MSSubClass* (Identifies the type of dwelling involved in the sale) -> NaN means no identifiable sub class\n\n--> All the missing values of these columns will be replaced by the value None","2e08545c":"**Best hyperparameters** :\n{objective = 'reg:squarederror',learning_rate=0.05, subsample=1,\n                         min_child_weight = 1,n_estimators=1000,max_depth=6, colsample_bytree=0.3,\n                         colsample_bylevel=0.7, colsample_bynode=0.7,\n                         gamma = 0.01}","184ed8c0":"**Total rates of the overall material and the overall condition of the house :**","47c0bfdf":"**Evaluation :** the target validation set in function of the xgb prediction ","78c8efa0":"# Data processing : modelling & validation","77dc3a1d":"## Train \/ test and train \/ validation split","d933cdb4":"There isn't high negative correlation between variables, the maximum is above -0.4. The majority of the correlation is positive and less than 0.6. Thus, the dataset is not composed by a lot of corrolated variables.","5fc07808":"**OverallQual** is the rates the overall material and finish of the house. This variable is ordinal categorial. The values are between 0 and 10, with  0 corresponds to very poor and 10 very excellent. The majority of house have a overall quality between 5 and 7. \n\nAs we can see in the figure 3, the extent of the price depends on the quality of the houses. The more a house has an important quality, the more its price has very broad values.","f95a8954":"**Numercial data**\n\n*   *GarageYrBlt, GarageArea* and *GarageCars* -> NaN means no garage \n*   *BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath* and *BsmtHalfBath* -> NaN means no basement\n\n-> All the missing values of these columns will be replaced by 0 \n\n","95005f50":"Transformation of the sale's year and month (currently continous variables) into a string","54d50bc5":"* *Functional* (Home functionality, assume typical unless deductions are warranted) -> NaN means typical\n\n--> All the missing values of these column will be replaced by the value 'typ'","aa651958":"**GrLiveArea** is the above grade (ground) living area square feet. It's a numerical variable. The above grade living area doesn't follow a normal law, we can see a positive skweness : the mass of the distribution is concentrated on the left of the figure. Thus, the mode and the median is before the mean. The majority of the houses have a grade living area between 1,129 and 1,776 square feet. The maximum is aroung 5,500 square feet.","a945182d":"There isn't missing value for the sale price. The median sale price is 163,000 dollars and the mean is 180,921 dollars, with a standard deviation of 79,442. Three quarters of the sale price are under 214,000 dollars.","b039e881":"# House Prices - Advanced Regression Techniques\nVariable to explain : sale price of home \n\nExplanatory variables:  79 explanatory variables describing aspect of residential homes in Ames, Iowa\n\nIn this study, the machine learning model used is the XGboost. XGBoost (like eXtreme Gradient Boosting) is an optimized open source implementation of the gradient boosting tree algorithm.","280ffe0d":"**Categorical data** \n\n\n\n","5df15414":"**The 10 most important features for the prediction**","4c3040a2":"Drop the Id features, it's not a useful information for the prediction","4264bed7":"As the grade living area, we can see a positive skweness. To mesure the skweness, we will use the function skew in python and also the kurtosis function.","03c84d2d":"# Data acquisition\n","e6f0bb3f":"# Data exploration","ae91799c":"**GarageCars** is the size of garage in car capacity. It's a nominal categorical. The category with the most value is the 2 and with the less is the category 4. It's for the category 3 that the price of house are more important than the other categories. The standard deviation is more important for the category 3.","179236c5":"# Data preprocessing","cc6bd10a":"The target variable has a highly skewed distribution : the skewness value is  greater than +1. The kurtosis value is high (>3) and a high kurtosis in a data set is an indicator that data has heavy outliers. In order to have a better prediction, we need to normalize the sale price using the log function.","6ba3c7f5":"### Features","91de5b5d":"**Mean size per room above grade :**","e0c3bdf6":"## Training the model","6d80e738":"## Correlation ","8e4accab":"### Missing data"}}