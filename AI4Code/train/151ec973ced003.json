{"cell_type":{"06f17acc":"code","3f962703":"code","b3f51031":"code","34bfa107":"code","b1d8f707":"code","bd1b6557":"code","3f8ac9f5":"code","ef5e8083":"code","edbaef73":"code","ed967b07":"code","f19efb62":"code","63685f81":"code","44fe7522":"code","c5aebe23":"code","21cd7814":"code","54e7e7df":"code","d5fc6312":"code","5c1dc176":"code","4c1e05de":"code","48c002da":"code","92296299":"code","937eb005":"code","35ef2396":"code","cbd66730":"code","d286e96b":"markdown","6661145c":"markdown","88d32c55":"markdown","df715ede":"markdown","d92e949f":"markdown","dab5ad99":"markdown","51303e49":"markdown","3fac25cc":"markdown","424fc4c7":"markdown","1a6ed11e":"markdown","c556b436":"markdown","444ff49d":"markdown","c9a89e5b":"markdown","5972b652":"markdown","d959cc49":"markdown","23add1ea":"markdown","68c0e34d":"markdown","96473366":"markdown","02cbca90":"markdown","e385a649":"markdown","19970a7f":"markdown","6df394dc":"markdown"},"source":{"06f17acc":"!pip install bunch\n#Import\nimport sys\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport scipy.io as sio\nfrom bunch import Bunch\nimport torch.optim as optim\nfrom pandas import DataFrame\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.utils import weight_norm\nfrom os.path import dirname, join as pjoin\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.arima_process import ArmaProcess\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n\nprint(\"Done\")","3f962703":"# The EGK signal contains 100 data points\nEKG_sig = [41.985077,48.505554,17.868908,-17.902781,-25.676805,2.4074743,40.308300,50.589020,22.663403,-16.953960,-32.144905,-31.155527,-23.732307,6.5244060,44.417355,58.700817,60.295696,58.277584,51.427200,24.971256,18.454710,42.659885,28.012609,13.741727,14.357091,15.576780,11.927580,-14.444111,-25.185453,-19.081909,6.6035562,17.284658,17.544495,16.928368,-6.3161540,-14.990187,10.754345,21.378378,-3.1653652,-13.225622,15.521652,54.976814,64.869339,35.326790,-5.0551395,-18.008286,-15.629535,-11.389389,15.224802,52.611454,69.125626,71.429947,69.159081,63.929958,35.759071,-1.0452937,-12.551209,9.9472542,23.483477,29.982676,53.127209,38.299618,26.093702,25.454752,0.92754990,-9.3693218,19.490158,56.732342,40.379166,1.2176796,15.432666,53.198875,38.633102,1.4428799,12.662603,25.239782,24.835445,19.891754,-6.5586896,3.0409441,15.515370,13.531504,6.8291478,-15.963812,0.38266551,33.888634,14.340966,-27.637432,-37.876808,-12.587003,-1.4763706,-1.0964720,20.268143,5.8535609,-29.653194,-16.162226,18.147284,6.5141749,-4.5157704,13.640582]\nprint(\"Done!\")","b3f51031":"'''\nAccording to the documentation for the the ARIMA method. The order takes in values for p,q,d wich corresspond to \nautoregressive, differences, and moving average components respectively. Therefore, by just changing these values we can\naccomplish what the problem is saying. \n'''\n#Here we want to include a non-zero value only for q\norder1 = (0,0,2)\nmodel_1 = ARIMA(EKG_sig, order= order1 )# put the order here\nmodel_fit_1 = model_1.fit()\nresiduals_1 = DataFrame(model_fit_1.resid)\n\n# line plot of the results\nplt.plot(EKG_sig, label='EKG')\nplt.plot(residuals_1, label='MA')\nplt.legend()\nplt.show()","34bfa107":"#Here we only need to modify the p value.\norder2 = (3,0,0)\nmodel_2 = ARIMA(EKG_sig, order=order2)# put the order here\nmodel_fit_2 = model_2.fit()\nresiduals_2 = DataFrame(model_fit_2.resid)\n\n# line plot of the results\nplt.plot(EKG_sig, label='EKG')\nplt.plot(residuals_2, label='AR')\nplt.legend()\nplt.show()","b1d8f707":"order3 = (5,0,1)\nmodel_3 = ARIMA(EKG_sig, order=order3)# Arbritrarily set d\nmodel_fit_3 = model_3.fit()\nresiduals_3 = DataFrame(model_fit_3.resid)\n\n# line plot of the results\nplt.plot(EKG_sig, label='EKG')\nplt.plot(residuals_3, label='ARMA')\nplt.legend()\nplt.show()","bd1b6557":"#Define a function that can be called to calculate the RMSE\ndef findRMSE(predictions,labels):\n    '''\n    Finds the RMSE given predictions and the truth labels\n    Input:\n        - predictions: The Predictions made by a model (np.array)\n        - labels: The ground truth labels (np.array)\n    Output:\n        - RMSE: The calculated RMSE\n    '''\n    return np.sqrt(((predictions - labels) ** 2).mean())\n    \n#Now we can run it on our models\n\n#Model 1\nprint(\"Results for Model 1 {} = {}\".format(order1,findRMSE(np.array(residuals_1),np.array(EKG_sig))))\n\n#Model 2\nprint(\"Results for Model 2 {} = {}\".format(order2,findRMSE(np.array(residuals_2),np.array(EKG_sig))))\n\n#Model 3\nprint(\"Results for Model 3 {} = {}\".format(order3,findRMSE(np.array(residuals_3),np.array(EKG_sig))))","3f8ac9f5":"# data generator\ndef data_generator(N, seq_length):\n    \"\"\"\n    Args:\n        seq_length: Length of the adding problem data\n        N: # of data in the set\n    \"\"\"\n    X_num = torch.rand([N, 1, seq_length])\n    X_mask = torch.zeros([N, 1, seq_length])\n    Y = torch.zeros([N, 1])\n    for i in tqdm(range(N)):\n        positions = np.random.choice(seq_length, size=2, replace=False)\n        X_mask[i, 0, positions[0]] = 1\n        X_mask[i, 0, positions[1]] = 1\n        Y[i,0] = X_num[i, 0, positions[0]] + X_num[i, 0, positions[1]]\n    X = torch.cat((X_num, X_mask), dim=1)\n    return Variable(X), Variable(Y)\nprint(\"Done\")","ef5e8083":"# generate data\nX_train, Y_train = data_generator(5000, 100)\nX_test, Y_test = data_generator(100, 100)\n\n","edbaef73":"# define several classes for training.\nclass TCN(nn.Module):\n    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n        super(TCN, self).__init__()\n        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n        self.linear = nn.Linear(num_channels[-1], output_size)\n        self.init_weights()\n\n    def init_weights(self):\n        self.linear.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        y1 = self.tcn(x)\n        return self.linear(y1[:, :, -1])\n\n\nclass Chomp1d(nn.Module):\n    def __init__(self, chomp_size):\n        super(Chomp1d, self).__init__()\n        self.chomp_size = chomp_size\n\n    def forward(self, x):\n        return x[:, :, :-self.chomp_size].contiguous()\n\nclass TemporalConvNet(nn.Module):\n    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n        super(TemporalConvNet, self).__init__()\n        layers = []\n        num_levels = len(num_channels)\n        for i in range(num_levels):\n            dilation_size = 2 ** i\n            in_channels = num_inputs if i == 0 else num_channels[i-1]\n            out_channels = num_channels[i]\n            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.network(x)\n\nprint(\"Done!\")","ed967b07":"#Import stuff (I know we have already imported stuff above, just gives me some peace of mind to see it closer by)\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils import weight_norm as weightNorm\n\nclass TemporalBlock(nn.Module):\n    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n        super(TemporalBlock, self).__init__()\n        # Your Code:\n        \n        #TODO: Need to define the following\n        '''\n        We need to define the following, according to the example\n            - Conv1 and Conv2\n            - Call in Chomp1 and Chomp2\n            - Relu1 and Relu2\n            - dropout1 and dropout2\n            - net, which holds all of the components in our network above.\n        '''\n        #We don't want to just send our data through Conv1d, we also want to weight \n                #normalize in order to train faster and avoid exploiding gradients\n        self.conv1 = weightNorm(nn.Conv1d(n_inputs,n_outputs,kernel_size,stride=stride,padding=padding,dilation=dilation))\n        \n        #Now we want to create our chomp1 after this to cut out all the padded data at the right end\n        self.chomp1 = Chomp1d(padding) #we want to cut out the same amount we added for padding\n        #Now we want to run this through a relu\n        self.relu1 = nn.ReLU()\n        #Now we are sending it over to the next level, so we need to handle the dropout (Only in training)\n        self.dropout1 = nn.Dropout(dropout)\n        \n        #Now we basically need to do the same thing for the second layer\n        self.conv2 = weightNorm(nn.Conv1d(n_outputs,n_outputs,kernel_size,stride=stride,padding=padding,dilation=dilation))\n        self.chomp2 = Chomp1d(padding)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(dropout)\n        \n        #we need another relu, to use for the output\n        self.relu = nn.ReLU()\n        \n        #Now we need to make our net variable which will be a sequential model holding all of the above.\n        self.net = nn.Sequential(self.conv1,self.chomp1,self.relu1,self.dropout1,self.conv2,self.chomp2,self.relu2,self.dropout2)\n        \n        #We also need to implement a downsample (since it is used below). \n        #The idea with this is to allow the model to adjust the output is the correct size \n        \n        #We want to check if num_inputs = num_outputs\n        if n_inputs == n_outputs:\n            #f this is the case, we want to just return None\n            self.downsample = None\n        else:\n            #We want to set up a conv1d that gets us the correct input and output\n            self.downsample = nn.Conv1d(n_inputs,n_outputs,1)\n        \n        #Since we have an init weights function we can run that now\n        self.init_weights()\n\n    def init_weights(self):\n        self.conv1.weight.data.normal_(0, 0.01)\n        self.conv2.weight.data.normal_(0, 0.01)\n        if self.downsample is not None:\n            self.downsample.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.net(x)\n        res = x if self.downsample is None else self.downsample(x)\n        return self.relu(out + res)\n\nprint(\"Done!\")","f19efb62":"# initialize training parameters.\nargs = Bunch()\n# adjustable parameters\nargs.epochs = 35\nargs.ksize = 4\nargs.batch_size = 32\nargs.lr = 0.001\n\n# fix parameters\nargs.cuda = True\nargs.dropout = False\nargs.seq_len = 100\nargs.clip = -1\nargs.levels = 8\nargs.log_interval = 100\nargs.optim = 'Adam'\nargs.nhid = 30\nargs.seed = 112\n\nprint(\"Done!\")","63685f81":"# train the model with two conv layers\ntorch.manual_seed(args.seed)\nif torch.cuda.is_available():\n    if not args.cuda:\n        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n\ninput_channels = 2\nn_classes = 1\nbatch_size = args.batch_size\nseq_length = args.seq_len\nepochs = args.epochs\n\n\n# Note: We use a very simple setting here (assuming all levels have the same # of channels.\nchannel_sizes = [args.nhid]*args.levels\nkernel_size = args.ksize\ndropout = args.dropout\nmodel = TCN(input_channels, n_classes, channel_sizes, kernel_size=kernel_size, dropout=dropout)\n\nif args.cuda:\n    model.cuda()\n    X_train = X_train.cuda()\n    Y_train = Y_train.cuda()\n    X_test = X_test.cuda()\n    Y_test = Y_test.cuda()\n\nlr = args.lr\noptimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)\n\n\ndef train(epoch):\n    global lr\n    model.train()\n    batch_idx = 1\n    total_loss = 0\n    for i in range(0, X_train.size(0), batch_size):\n        if i + batch_size > X_train.size(0):\n            x, y = X_train[i:], Y_train[i:]\n        else:\n            x, y = X_train[i:(i+batch_size)], Y_train[i:(i+batch_size)]\n        optimizer.zero_grad()\n        output = model(x)\n        loss = F.mse_loss(output, y)\n        loss.backward()\n        if args.clip > 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n        optimizer.step()\n        batch_idx += 1\n        total_loss += loss.item()\n\n        if batch_idx % args.log_interval == 0:\n            cur_loss = total_loss \/ args.log_interval\n            processed = min(i+batch_size, X_train.size(0))\n            print('Train Epoch: {:2d} [{:6d}\/{:6d} ({:.0f}%)]\\tLearning rate: {:.4f}\\tLoss: {:.6f}'.format(\n                epoch, processed, X_train.size(0), 100.*processed\/X_train.size(0), lr, cur_loss))\n            total_loss = 0\n\n\ndef evaluate():\n    model.eval()\n    with torch.no_grad():\n        output = model(X_test)\n        test_loss = F.mse_loss(output, Y_test)\n        print('\\nValidation set: Average loss: {:.6f}\\n'.format(test_loss.item()))\n        return test_loss.item()\n\n\nfor ep in range(1, epochs+1):\n    train(ep)\n    tloss = evaluate()","44fe7522":"# show the results. You will get full credit if the average differences is less than 0.02\npreds = model(X_test)\n\ntotal_diff = 0\nfor i,pred in enumerate(preds):\n    total_diff += np.abs(pred.data.item() - Y_test[i].item())\n\n\nprint('Average Difference:', total_diff\/len(Y_test))","c5aebe23":"class TemporalBlock(nn.Module):\n    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n        super(TemporalBlock, self).__init__()\n\n        #We don't want to just send our data through Conv1d, we also want to weight \n                #normalize in order to train faster and avoid exploiding gradients\n        self.conv1 = weightNorm(nn.Conv1d(n_inputs,n_outputs,kernel_size,stride=stride,padding=padding,dilation=dilation))\n        \n        #Now we want to create our chomp1 after this to cut out all the padded data at the right end\n        self.chomp1 = Chomp1d(padding) #we want to cut out the same amount we added for padding\n        #Now we want to run this through a relu\n        self.relu1 = nn.ReLU()\n        #Now we are sending it over to the next level, so we need to handle the dropout (Only in training)\n        self.dropout1 = nn.Dropout(dropout)\n        \n        #we need another relu, to use for the output\n        self.relu = nn.ReLU()\n        \n        #Now we need to make our net variable which will be a sequential model holding all of the above.\n        self.net = nn.Sequential(self.conv1,self.chomp1,self.relu1,self.dropout1)\n        \n        #We also need to implement a downsample (since it is used below). \n        #The idea with this is to allow the model to adjust the output is the correct size \n        \n        #We want to check if num_inputs = num_outputs\n        if n_inputs == n_outputs:\n            #f this is the case, we want to just return None\n            self.downsample = None\n        else:\n            #We want to set up a conv1d that gets us the correct input and output\n            self.downsample = nn.Conv1d(n_inputs,n_outputs,1)\n        \n        #Since we have an init weights function we can run that now\n        self.init_weights()\n\n\n    def init_weights(self):\n        self.conv1.weight.data.normal_(0, 0.01)\n        if self.downsample is not None:\n            self.downsample.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.net(x)\n        res = x if self.downsample is None else self.downsample(x)\n        return self.relu(out + res)\n\n\n# train the model with two conv layers\ntorch.manual_seed(args.seed)\nif torch.cuda.is_available():\n    if not args.cuda:\n        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n\ninput_channels = 2\nn_classes = 1\nbatch_size = args.batch_size\nseq_length = args.seq_len\nepochs = args.epochs\n\n\n# Note: We use a very simple setting here (assuming all levels have the same # of channels.\nchannel_sizes = [args.nhid]*args.levels\nkernel_size = args.ksize\ndropout = args.dropout\nmodel = TCN(input_channels, n_classes, channel_sizes, kernel_size=kernel_size, dropout=dropout)\n\nif args.cuda:\n    model.cuda()\n    X_train = X_train.cuda()\n    Y_train = Y_train.cuda()\n    X_test = X_test.cuda()\n    Y_test = Y_test.cuda()\n\nlr = args.lr\noptimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)\n\n\nfor ep in range(1, epochs+1):\n    train(ep)\n    tloss = evaluate()\n\n# show the result\npreds = model(X_test)\n\ntotal_diff = 0\nfor i,pred in enumerate(preds):\n    total_diff += np.abs(pred.data.item() - Y_test[i].item())\n\n\nprint('Average Difference:', total_diff\/len(Y_test))","21cd7814":"# Import data and save as np.array\nData = sio.loadmat('..\/input\/eeg-data\/X.mat')\nLabel = sio.loadmat('..\/input\/eeg-data\/Y.mat')\nDataset = Data['Data']\nLabels = Label['Labels']\nprint(Dataset.shape, Labels.shape)\n\n","54e7e7df":"# define the EEGNet\nclass EEGNet(nn.Module):\n    def __init__(self):\n        super(EEGNet, self).__init__()\n        self.T = 120\n        \n        # Layer 1\n        self.conv1 = nn.Conv2d(1, 16, (1, 64), padding = 0)\n        self.batchnorm1 = nn.BatchNorm2d(16, False)\n        \n        # Layer 2\n        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n        self.batchnorm2 = nn.BatchNorm2d(4, False)\n        self.pooling2 = nn.MaxPool2d(2, 4)\n        \n        # Layer 3\n        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n        self.batchnorm3 = nn.BatchNorm2d(4, False)\n        self.pooling3 = nn.MaxPool2d((2, 4))\n\n        # FC Layer\n        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n        # here we have 120 timepoints\n        self.fc1 = nn.Linear(4*2*7, 1)\n        \n\n    def forward(self, x):\n        # Layer 1\n        x = F.elu(self.conv1(x))\n        x = self.batchnorm1(x)\n        x = F.dropout(x, 0.25)\n        x = x.permute(0, 3, 1, 2)\n        \n        # Layer 2\n        x = self.padding1(x)\n        x = F.elu(self.conv2(x))\n        x = self.batchnorm2(x)\n        x = F.dropout(x, 0.25)\n        x = self.pooling2(x)\n        \n        # Layer 3\n        x = self.padding2(x)\n        x = F.elu(self.conv3(x))\n        x = self.batchnorm3(x)\n        x = F.dropout(x, 0.25)\n        x = self.pooling3(x)\n        \n        # FC Layer\n        x = x.view(-1, 4*2*7)\n        x = torch.sigmoid(self.fc1(x))\n        return x\n\n\nnet = EEGNet()\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(net.parameters())","d5fc6312":"# Define the evaluation function\ndef evaluate(model, X, Y, params = [\"acc\"]):\n    results = []\n    batch_size = 50\n    \n    predicted = []\n    \n    for i in range(int(len(X)\/batch_size)):\n        s = i*batch_size\n        e = i*batch_size+batch_size\n        \n        inputs = Variable(torch.from_numpy(X[s:e]))\n        pred = model(inputs)\n        \n        predicted.append(pred.data.cpu().numpy())\n        \n        \n    inputs = Variable(torch.from_numpy(X))\n    predicted = model(inputs)\n    \n    predicted = predicted.data.cpu().numpy()\n    \n    for param in params:\n        if param == 'acc':\n            results.append(accuracy_score(Y, np.round(predicted)))\n    return results","5c1dc176":" # Your coode\nX = Dataset.reshape(300,1,120,64) #This will get it into the form the neural net wants it\nY = Labels.transpose()\nprint(X.shape, Y.shape)\n\n","4c1e05de":"from sklearn.model_selection import train_test_split\n# Please name the parameters as names given below in the print function.\nX_train,X_test,y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n#Now split the test set into test and validation\nX_val, X_test,y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n\n\n\nprint(X_train.shape, X_val.shape,X_test.shape)\nprint(y_train.shape, y_val.shape, y_test.shape)\n","48c002da":"# train and test on your dataset\nbatch_size = 16\n\nfor epoch in range(40):  # loop over the dataset multiple times\n    print(\"\\nEpoch \", epoch)\n    \n    running_loss = 0.0\n    for i in range(int(len(X_train)\/batch_size-1)):\n        s = i*batch_size\n        e = i*batch_size+batch_size\n        \n        inputs = torch.from_numpy(X_train[s:e])\n        labels = torch.FloatTensor(np.array([y_train[s:e]]).T*1.0)\n        \n        # wrap them in Variable\n        inputs, labels = Variable(inputs), Variable(labels)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)   \n        loss = criterion(outputs, labels.reshape(batch_size,1)) #Doing this seems to work because of the way the model outputs the data\n        loss.backward()\n        \n        \n        optimizer.step()\n        \n        running_loss += loss.data\n    \n    # Validation accuracy\n    params = [\"acc\"]\n    print(\"Training Loss \", running_loss)\n    print(\"Train - \", evaluate(net, X_train, y_train, params))\n    print(\"Validation - \", evaluate(net, X_val, y_val, params))\n\n","92296299":"# print test accuracy\nprint(\"Test - \", evaluate(net, X_test, y_test, params))","937eb005":"from sklearn.model_selection import train_test_split\n# Please name the parameters as names given below in the print function.\nX_train,X_test,y_train, y_test = train_test_split(X, Y, test_size=0.5, random_state=42)\n\n#Now split the test set into test and validation\nX_val, X_test,y_val, y_test = train_test_split(X_test, y_test, test_size=0.2, random_state=42)\n\n\n\nprint(X_train.shape, X_val.shape,X_test.shape)\nprint(y_train.shape, y_val.shape, y_test.shape)\n","35ef2396":"# train and test on the dataset with 50% training, 40% validation, and 10% test data\nbatch_size = 16\n\nfor epoch in range(40):  # loop over the dataset multiple times\n    print(\"\\nEpoch \", epoch)\n    \n    running_loss = 0.0\n    for i in range(int(len(X_train)\/batch_size-1)):\n        s = i*batch_size\n        e = i*batch_size+batch_size\n        \n        inputs = torch.from_numpy(X_train[s:e])\n        labels = torch.FloatTensor(np.array([y_train[s:e]]).T*1.0)\n        \n        # wrap them in Variable\n        inputs, labels = Variable(inputs), Variable(labels)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)   \n        loss = criterion(outputs, labels.reshape(batch_size,1))\n        loss.backward()\n        \n        \n        optimizer.step()\n        \n        running_loss += loss.data\n    \n    # Validation accuracy\n    params = [\"acc\", \"auc\", \"fmeasure\"]\n    print(params)\n    print(\"Training Loss \", running_loss)\n    print(\"Train - \", evaluate(net, X_train, y_train, params))\n    print(\"Validation - \", evaluate(net, X_val, y_val, params))\n","cbd66730":"print(\"Test - \", evaluate(net, X_test, y_test, params))","d286e96b":"Your Answers:\n\n# Second model does better\n\nHere we see that the second model barely beats out the first model under the parameters that were used. Since the first model has more than one layer (2 to be exact), it tries to predict the sum based on some 2nd order relations which are not really necessary for the problem at hand. The other model does a little bit better because it can instead focus on the first oder relationships between the data (as it should). I expect that as we increase the number of layers, the average difference will also become worse as the model is forced to look for higher order relationships that don't really exist.\n\n## Suggestions to improve the model\n\nMy biggest suggestion would be to remove the extra convolutional layer. As mentioned above, this would greatly help the model focus on the first order relations through bigger changes to the weights during back-propogation (with deeper networks, the previous layers begin to have smaller and smaller changes because of the nature of the chain rule) rather then non-existant higher order relationships.\n\n","6661145c":"**Problem 1: ARMA Model** In this question, you need to apply three different Autoregressive Moving Average Model (ARMA) to the given EKG signal. Compare the input and output signals and answering the following questions. ","88d32c55":"**3(a)** Now, reshape the data so you can train it using EEGNet. To train the network, you also need to modify the network parameters to match the dimension of your training data. Please see the image [here](https:\/\/github.com\/aliasvishnu\/EEGNet\/blob\/master\/EEGNet.png) for more information. ","df715ede":"**1(d)** What are the differences between those three models? Which model works best for this EKG data? What are the pros and cons of each model? Please list at least one pro and con for each model. ","d92e949f":"Your Answers:\n\n# Model with optimal training data does better\n\nFrom the results above it is clear that the model with 50% training data did much better than the model with only 80% training data. This is because of the way that the data is used. The model can only learn and improve by using the training data since it uses the validation and test data to be evaluated. As a result of this, having lower amounts of training data will reduce the amount of training that the model can actually do which makes it harder for the model to overfitt to the training dataset (since it is forced to learn on a very small dataset). This can be seen clearly seen in how accurate the 80% model is on the training data, but how badly it does on the test dataset.\n\n## I learned that it is good practice put an emphasis on giving the model the right amount of training data\n\nIt is clear from the example that there is indeed an optimal amount of training data to give a model so that it doesn't overfit and become biased to the training set. This optimal number depends on the problem that you are trying to solve.","dab5ad99":"### Derive the RMSE for each of these models\n\nDoing this will allow us to numerically compare the models instead of just looking at the graphs.","51303e49":"**Problem 3: EEGNet** In this question, we will use EEGNet, a compact convolutional network for EEG-based brain-computer interfaces. Your task is to process the given EEG data so that you can train and test the network on the dataset. For more detail about the EEGNet, please see the GitHub repo here:\nhttps:\/\/github.com\/aliasvishnu\/EEGNet.\n\nThe EEG data given in this question are generated. Feel free to print or plot the data if you want to know what the dataset looks like. \n\n","3fac25cc":"Train your model. You need to use GPU for training here. To start a GPU on your Colab, follow the instruction on the slides for this assignment. Please re-run the first block after you set up the GPU. ","424fc4c7":"**1(b)** Autoregressive model only","1a6ed11e":"# **BMEN 4470 - Deep Learning for Biomedical Signal Processing**\n# **Homework 2: Basics of Modeling Sequences and Temporal Convolution Networks**\n\nDue 11:59pm on October 18th, 2021\n\n","c556b436":"**2(b)** Set and adjust the training parameters to make the Average Difference you got from your network smaller than 0.008. ","444ff49d":"**2(a)** Build your TCN model below. Your model should contain two convolutional layers.","c9a89e5b":"# Sharable Link to Kaggle Notebook\n\n## TODO: ADD THE LINK","5972b652":"**3(b)** Now split the training, evaluation, and testing data. Please make sure you have about 80% for training, about 10% for evaluation, and about 10% for testing. Then train your network on the dataset and test it.","d959cc49":"**1(a)** Moving Average model only","23add1ea":"**3(c)** Now, split the data by 50% training, 40% validation, 10% testing, and retrain your model.","68c0e34d":"Your Answers:\n\n# Results were generally the same\n\nAccording to the RMSE calculations above, it is fairly clear that the results are pretty similar. However based on the graphs, we can see a couple of places where the models do well and a couple of places where the models don't do very well.\n\n| Model | Pros | Cons |\n|------| ------|------|\n|Moving Average model| Smooths the time series data|Is not able to reach the peaks that exist in the ECG data |\n|Autoregressive model| The Values are closely related to the previous ones. This means the model can look back far enough to where it isn't influenced by noise.|Because of the reliance on previous data points, the model will not be good at predicting sequences where future values are not closely related to previous values.|\n|Mixed (p=5,q=1,d=0) | Gives a good balance between the MA and AR sides of the model and should be better able to predict the values of a sequence|Since multiple factors are being used, optimizing the p,d, and q values requires additional effort (programming and computational) |\n\n## The Autoregressive model should perform the best on ECG data\n\nThe autogressive model should perform best (and performs a little better than the others in our example) because the values of the ECG should be relatively close to the previous values. For example, there cannot be huge jumps in the electrical activity in the ECG measurements unless there is some sort of issue with aliasing where the sampling frequency is too low.\n","96473366":"**2(d)** Compare the result you get from 2(c) with the average difference you get from 2(b). Which network gives you a better result? Why that network gives a better result? Do you have any suggestions to improve the performance of the network that has poorer performance? ","02cbca90":"**3(d)** Compare the result you get from 3(d) with the test accuracy in 3(c). Which data split method gives you a better result? Why that method gives a better result? What did you learn from this?","e385a649":"**2(c)** Now build a TCN model with only one convolutional layer. Use the same parameter you set in 2(b) and train your network.","19970a7f":"**Problem 2 Temporal Convolutional Networks** In this question, we will generate a dataset and build a simple TCN to train and test on the dataset. \n\nThis dataset contains some data sequences: a pair of input sequences and an output. The first sequence within the input pair is composed of numbers randomly sampled from the range [0, 1]. The second input sequence within the input pair is composed of only integers: 0 or 1. This sequence must contain only two integer 1; the rest of the integers are all 0s. The output is the sum of the two values from the first input sequence corresponding to 1s in the second input sequence. \n\nFor example, one input sequence pair can be [[0, 0.1, 0.2, 0.3, 0.4, 0.5], [0, 0, 0, 1, 1, 0]]. In this case, the final output value should be 0.3 + 0.4 = 0.7. In this example, the length of the data sequence is 6.\n\nFor the detail of TCN, see the GitHub repo [here](https:\/\/github.com\/locuslab\/TCN\/tree\/master\/TCN\/adding_problem).","6df394dc":"**1(c)** ARMA model with p = 5, q = 1"}}