{"cell_type":{"8513b179":"code","bea2b663":"code","4d136842":"code","89fc5780":"code","5322b92a":"code","6f9753a0":"code","4b8366a4":"code","a0e8e2a3":"code","9f5c99cb":"code","3b5f9a57":"code","579a8bc3":"code","0efffd92":"code","c59414f3":"code","2d513cb8":"code","0878d493":"markdown","a3cb6ad8":"markdown","599aae93":"markdown","ede30942":"markdown","40023937":"markdown","5e5fe155":"markdown","75288e77":"markdown","8d4eabe0":"markdown","54697489":"markdown","b535a35e":"markdown","9ce43aa9":"markdown","edc328cc":"markdown","8abff9c4":"markdown"},"source":{"8513b179":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bea2b663":"import random\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom IPython.display import Image, display\nfrom tensorflow.keras.preprocessing.image import load_img\nimport PIL\nfrom PIL import ImageOps\n\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras import layers","4d136842":"input_dir = \"..\/input\/pennfudan-database-for-pedestrian-detection-zip\/PennFudanPed\/PNGImages\"\ntarget_dir = \"..\/input\/pennfudan-database-for-pedestrian-detection-zip\/PennFudanPed\/PedMasks\"\nimg_size = (160, 160)\nnum_classes = 3\nbatch_size = 16\n\ninput_img_paths = sorted(\n    [\n        os.path.join(input_dir, fname)\n        for fname in os.listdir(input_dir)\n        if fname.endswith(\".png\")\n    ]\n)\ntarget_img_paths = sorted(\n    [\n        os.path.join(target_dir, fname)\n        for fname in os.listdir(target_dir)\n        if fname.endswith(\".png\") and not fname.startswith(\".\")\n    ]\n)\n\nprint(\"Number of samples:\", len(input_img_paths))","89fc5780":"# Display image #21 It was FudanPed00021.png  but only 21 is required though the image below is FudanPed00022.png  \ni = 21\nfigure, ax = plt.subplots(nrows=1,ncols=2,figsize=(8,8))\nax.ravel()[0].imshow(mpimg.imread(input_img_paths[i]))\nax.ravel()[0].set_title(\"Orginal image\")\nax.ravel()[0].set_axis_off()\nax.ravel()[1].imshow(mpimg.imread(target_img_paths[i]))\nax.ravel()[1].set_title(\"Mask\")\nax.ravel()[1].set_axis_off()\n#ax.ravel()[2].imshow(PIL.ImageOps.autocontrast(load_img(target_img_paths[i])))\n#ax.ravel()[2].set_title(\"Contrast of mask\")\n#ax.ravel()[2].set_axis_off()\nplt.tight_layout()","5322b92a":"#That's FudanPed00021.png\n\ni = 20\nfigure, ax = plt.subplots(nrows=1,ncols=2,figsize=(8,8))\nax.ravel()[0].imshow(mpimg.imread(input_img_paths[i]))\nax.ravel()[0].set_title(\"Orginal image\")\nax.ravel()[0].set_axis_off()\nax.ravel()[1].imshow(mpimg.imread(target_img_paths[i]))\nax.ravel()[1].set_title(\"Mask\")\nax.ravel()[1].set_axis_off()\n#ax.ravel()[2].imshow(PIL.ImageOps.autocontrast(load_img(target_img_paths[i])))\n#ax.ravel()[2].set_title(\"Contrast of mask\")\n#ax.ravel()[2].set_axis_off()\nplt.tight_layout()","6f9753a0":"class PetsDataset(keras.utils.Sequence):\n    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n\n    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.input_img_paths = input_img_paths\n        self.target_img_paths = target_img_paths\n\n    def __len__(self):\n        return len(self.target_img_paths) \/\/ self.batch_size\n\n    def __getitem__(self, idx):\n        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n        i = idx * self.batch_size\n        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n        for j, path in enumerate(batch_input_img_paths):\n            img = load_img(path, target_size=self.img_size)\n            x[j] = img\n        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n        for j, path in enumerate(batch_target_img_paths):\n            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n            y[j] = np.expand_dims(img, 2)\n            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n            y[j] -= 1\n        return x, y","4b8366a4":"def get_model(img_size, num_classes):\n    inputs = keras.Input(shape=img_size + (3,))\n\n    ### [First half of the network: downsampling inputs] ###\n\n    # Entry block\n    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    previous_block_activation = x  # Set aside residual\n\n    # Blocks 1, 2, 3 are identical apart from the feature depth.\n    for filters in [64, 128, 256]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n\n        # Project residual\n        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n            previous_block_activation\n        )\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    ### [Second half of the network: upsampling inputs] ###\n\n    for filters in [256, 128, 64, 32]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.UpSampling2D(2)(x)\n\n        # Project residual\n        residual = layers.UpSampling2D(2)(previous_block_activation)\n        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    # Add a per-pixel classification layer\n    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n\n    # Define the model\n    model = keras.Model(inputs, outputs)\n    return model\n\n\n# Free up RAM in case the model definition cells were run multiple times\nkeras.backend.clear_session()\n\n# Build model\nmodel = get_model(img_size, num_classes)\nmodel.summary()","a0e8e2a3":"val_samples = 26 # 85% Training -- 15%(1108) Validation\nrandom.Random(43).shuffle(input_img_paths) #Original 7390(samples) Here 1822(24,65%)\nrandom.Random(43).shuffle(target_img_paths)\ntrain_input_img_paths = input_img_paths[:-val_samples]\ntrain_target_img_paths = target_img_paths[:-val_samples]\nval_input_img_paths = input_img_paths[-val_samples:]\nval_target_img_paths = target_img_paths[-val_samples:]\n\n# Instantiate data Sequences for each split\ntrain_gen = PetsDataset(\n    batch_size, img_size, train_input_img_paths, train_target_img_paths\n)\nval_gen = PetsDataset(batch_size, img_size, val_input_img_paths, val_target_img_paths)","9f5c99cb":"# We use the \"sparse\" version of categorical_crossentropy\n# because our target data is integers.\nmodel.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"pets_segmentation.h5\", save_best_only=True)\n]\n\nepochs = 30\nmodelunet=model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)","3b5f9a57":"# summarize history for accuracy\nplt.plot(modelunet.history['accuracy'])\nplt.plot(modelunet.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.grid(True)\nplt.show()\n# summarize history for loss\nplt.plot(modelunet.history['loss'])\nplt.plot(modelunet.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.grid(True)\nplt.show()","579a8bc3":"# Generate predictions for all images in the validation set\nval_gen = PetsDataset(batch_size, img_size, val_input_img_paths, val_target_img_paths)\nval_preds = model.predict(val_gen)","0efffd92":"def display_mask(i):\n    \"\"\"Quick utility to display a model's prediction.\"\"\"\n    mask = np.argmax(val_preds[i], axis=-1)\n    mask = np.expand_dims(mask, axis=-1)\n    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n    return img","c59414f3":"# Display image #10\ni = 10\nfigure, ax = plt.subplots(nrows=1,ncols=3,figsize=(8,5))\nax.ravel()[0].imshow(mpimg.imread(val_input_img_paths[i]))\nax.ravel()[0].set_title(\"Orginal image\")\nax.ravel()[0].set_axis_off()\nax.ravel()[1].imshow(mpimg.imread(val_target_img_paths[i]))\nax.ravel()[1].set_title(\"Mask\")\nax.ravel()[1].set_axis_off()\nax.ravel()[2].imshow(display_mask(i))\nax.ravel()[2].set_title(\"Predicted mask \")\nax.ravel()[2].set_axis_off()\nplt.tight_layout()","2d513cb8":"# Display image #11\ni = 11\nfigure, ax = plt.subplots(nrows=1,ncols=3,figsize=(8,5))\nax.ravel()[0].imshow(mpimg.imread(val_input_img_paths[i]))\nax.ravel()[0].set_title(\"Orginal image\")\nax.ravel()[0].set_axis_off()\nax.ravel()[1].imshow(mpimg.imread(val_target_img_paths[i]))\nax.ravel()[1].set_title(\"Mask\")\nax.ravel()[1].set_axis_off()\nax.ravel()[2].imshow(display_mask(i))\nax.ravel()[2].set_title(\"Predicted mask \")\nax.ravel()[2].set_axis_off()\nplt.tight_layout()","0878d493":"#I got Error \"index 20 is out of bounds for axis 0 with size 16\"  20 is the number of the image. I changed to 10 and worked","a3cb6ad8":"#Display sample of Image Dataset","599aae93":"#Codes by Ammar Alhaj Ali https:\/\/www.kaggle.com\/ammarnassanalhajali\/image-segmentation-with-a-u-net-and-keras","ede30942":"#TRAINING","40023937":"#Build the U-Net Model Architecture","5e5fe155":"#Inference","75288e77":"#History for Accuracy","8d4eabe0":"The predicted Mask is Not correct. The image is Not eleven or any number below 16. Not a clue what is the real image number.\n\n#Worst, No clue about the Loss\/Accuracy charts meaning. Feel free to say anything. Meanwhile, I'll search it. ","54697489":"U-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg.[1] The network is based on the fully convolutional network[2] and its architecture was modified and extended to work with fewer training images and to yield more precise segmentations. Segmentation of a 512 \u00d7 512 image takes less than a second on a modern GPU.\n\nhttps:\/\/en.wikipedia.org\/wiki\/U-Net","b535a35e":"#I changed `sparse_categorical_crossentropy` to `binary_crossentropy`  I think it worked.\n\nSince I got an error: \"Received a label value of 255 which is outside the valid range of 0, 3\"\n\nAccording to Stack Overflow:\n\nRange 0, 3 means every number between 0 and 3, excluding 3. So 3 is not a value in the range 0, 3).\n\nI am not 100% sure, but the issue could be due to your choice of loss function. For a binary classification, `binary_crossentropy` should be a better choice\n\nhttps:\/\/stackoverflow.com\/questions\/44151760\/received-a-label-value-of-1-which-is-outside-the-valid-range-of-0-1-python","9ce43aa9":"![](https:\/\/slideplayer.com\/slide\/14906132\/91\/images\/8\/IEEE+International+Symposium+on+Biomedical+Imaging+%28ISBI+2015%29.jpg)slideplayer.com","edc328cc":"#Though the Predicted Mask is NOT correct, Lift your head\n\n\"Lift your head weary sinner, the river's just ahead\n\nDown the path of forgiveness, salvation's waiting there\n\nYou built a mighty fortress 10, 000 burdens high\n\nLove is here to lift you up, here to lift you high\"  Song by  David Crowder","8abff9c4":"#Pedestrian Dataset (Penn-Fudan Database)"}}