{"cell_type":{"6b0b0bbb":"code","6b3aef59":"code","18aa8201":"code","64d36665":"code","39a65a6b":"code","2ca8fd2c":"code","de1b7b57":"code","7ff5bf2a":"code","ef085bfb":"code","8418544f":"code","3cd5fb2a":"code","4ecf1e1b":"code","76216594":"code","f38dcfe8":"code","d24160a2":"code","8ee88fcd":"code","0ffe53bb":"code","6a9491d9":"code","d996818d":"code","f0c2889d":"code","04846f28":"code","4c009588":"code","ace98fbf":"code","ef2adb50":"code","86543cb9":"code","901f7871":"code","05bc03d8":"code","17c8dfdc":"code","05e5c46d":"code","5e36cc60":"code","ad809b0b":"code","75dcc7db":"code","1d0e2908":"code","b1a01616":"code","c6c23134":"markdown","882c32a5":"markdown","e49af171":"markdown","28b14592":"markdown","7d1f3acd":"markdown","f48d34fb":"markdown","fce24f12":"markdown","bc97b1c7":"markdown","e95d6c6b":"markdown","a8b82f8c":"markdown","1057dd31":"markdown","33ee97f7":"markdown","564e63fe":"markdown"},"source":{"6b0b0bbb":"import numpy as np\nimport pandas as pd\nimport os, gc\nfrom tqdm import tqdm\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nimport shap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams[\"figure.figsize\"] = [10, 7] # Make plots bigger\n\n# Pandas show more columns\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","6b3aef59":"tr = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv')\nte = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv')","18aa8201":"target_map = {'Class_1':0, 'Class_2':1, 'Class_3':2, 'Class_4':3}\nreverse_target_map = {0:'Class_1', 1:'Class_2', 2:'Class_3', 3:'Class_4'}\n\ntr['target'] = pd.to_numeric(tr['target'].map(target_map))","64d36665":"feature_cols = [c for c in tr.columns if 'feature_' in c]","39a65a6b":"# Concatenate the test to train\nte['target'] = -1\n\ntr_te = pd.concat([tr, te],axis=0,sort=True).reset_index(drop=True)\ntr_te['is_train'] = tr_te['target'] != -1","2ca8fd2c":"for col in feature_cols:\n    test_vals_not_in_train = set(te[col]) - set(tr[col])\n    train_vals_not_in_test = set(tr[col]) - set(te[col])\n    print(col, 'Test\\Train:', test_vals_not_in_train, 'Train\\Test:', train_vals_not_in_test)\n    if len(test_vals_not_in_train) > 0:\n        for val in test_vals_not_in_train:\n            print(val, 'Value has number of rows:', tr_te.loc[tr_te[col]==val].shape[0])\n            \n    if len(train_vals_not_in_test) > 0:\n        for val in train_vals_not_in_test:\n            print(val, 'Value has number of rows:', tr_te.loc[tr_te[col]==val].shape[0])","de1b7b57":"for col in tqdm(feature_cols):\n    tr_te[col+'_modified'] = tr_te[col].copy()\n    for val in tr_te[col].unique():\n        val_mask = tr_te[col] == val\n        if tr_te.loc[val_mask].shape[0] < 20: # If fewer than 20 rows\n            tr_te.loc[val_mask, col+'_modified'] = -99 # Overwrite with -99 ","7ff5bf2a":"for col in tqdm(feature_cols):\n    col = col + '_modified'\n    tr_te[col] = tr_te[col].astype('category').cat.codes","ef085bfb":"skf = StratifiedKFold(n_splits = 5, shuffle = True,  random_state = 2021)","8418544f":"tr = tr_te.loc[tr_te['target']!=-1].copy()\nte = tr_te.loc[tr_te['target']==-1].copy()","3cd5fb2a":"cols_to_use = [f'feature_{x}_modified' for x in range(50)]\ntarget_col = 'target'","4ecf1e1b":"# Label the fold numbers in the train set\ntr['fold_number'] = -1 # Initialize fold number\nfor fold_number, (train_index, valid_index) in enumerate(skf.split(tr[cols_to_use], tr[target_col])):\n    tr.loc[valid_index, 'fold_number'] = fold_number","76216594":"# Double check the folds are distributed evenly\ntr['fold_number'].value_counts()","f38dcfe8":"tr.groupby('fold_number')['target'].value_counts()","d24160a2":"tr['oof_predictions_class0'] = 0\ntr['oof_predictions_class1'] = 0\ntr['oof_predictions_class2'] = 0\ntr['oof_predictions_class3'] = 0\n\nte['predictions_class0'] = 0\nte['predictions_class1'] = 0\nte['predictions_class2'] = 0\nte['predictions_class3'] = 0\n\ngain_imps = {}\nfor col in cols_to_use:\n    gain_imps[col] = 0 # Initialize to 0\n\nfor fold_number in [0,1,2,3,4]:\n    train_mask = tr['fold_number'] != fold_number\n    valid_mask = tr['fold_number'] == fold_number\n    \n    # Create the LightGBM Datasets\n    dtrain = lgb.Dataset(data = tr.loc[train_mask, cols_to_use], \n                         label = tr.loc[train_mask, target_col],\n                         categorical_feature = list(range(50))) # Pass the indices of the categorical features\n\n    dvalid = lgb.Dataset(data = tr.loc[valid_mask, cols_to_use], \n                         label = tr.loc[valid_mask, target_col],\n                         categorical_feature = list(range(50))) # Pass the indices of the categorical features\n\n    np.random.seed(fold_number)\n    # Define parameters\n    # Note a different seed for each fold so the models can be different \n    # (Model Diversity = more robust predictions)\n    params = {\n        'objective': 'multiclass',\n        'num_class': 4, # Only used in multiclass\n        'metric': ['multi_logloss','multi_error'], \n        'boosting_type': 'gbdt',\n        'num_leaves': 31, \n        'max_depth': 11, \n        'learning_rate': 0.05, \n        'feature_fraction': 0.5, \n        'seed': 2021 + fold_number,\n        'cat_l2': 10,\n        'cat_smooth': 10,\n        'verbose':-1\n    }\n    \n    print(f'=========== Fold {fold_number} ===========')\n    # Train the model\n    model = lgb.train(\n        params=params,\n        train_set = dtrain,\n        num_boost_round=1000,\n        valid_sets=[dtrain, dvalid],\n        verbose_eval = 50,\n        early_stopping_rounds=100,\n        categorical_feature=list(range(50))\n    )\n    \n    \n    # Make your predictions\n    oof_preds = model.predict(tr.loc[valid_mask, cols_to_use])\n    test_preds = model.predict(te[cols_to_use])\n    for class_num in range(4):\n        tr.loc[valid_mask, f'oof_predictions_class{class_num}'] = oof_preds[:,class_num]\n        te[f'predictions_class{class_num}'] += test_preds[:,class_num]\n        \n    \n    # If it's the first fold, make SHAP predictions so we can explain model\n    if fold_number == 0:\n        print(f'=========== Performing SHAP Explainer ===========')\n        # Commented this out because it takes too long...\n        # Maybe SHAP is slow with categorical values?\n        # shap_values = shap.Explainer(model).shap_values(tr[cols_to_use])\n        \n    # Save out the gain importances instead\n    for col, imp in zip(cols_to_use, model.feature_importance(importance_type='gain')):\n        gain_imps[col] += imp\n    \nfor class_num in range(4):\n    te[f'predictions_class{class_num}'] \/= 5\n    \nfor col in cols_to_use:\n    gain_imps[col] \/= 5","8ee88fcd":"# https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.from_dict.html\ngain_imps_df = pd.DataFrame.from_dict(gain_imps, orient='index').reset_index(drop=False)\ngain_imps_df.columns = ['feat','imp']\ngain_imps_df = gain_imps_df.sort_values('imp',ascending=False)\n\n# Also attach the nunique for each\ngain_imps_df['nunique'] = gain_imps_df['feat'].apply(lambda x: tr_te[x].nunique())\ngain_imps_df","0ffe53bb":"from sklearn.metrics import log_loss","6a9491d9":"log_loss(y_true = tr['target'], \n         y_pred=tr[[f'oof_predictions_class{class_num}' for class_num in range(4)]])","d996818d":"for col_num in tqdm(range(50)):\n    col = f'feature_{col_num}'\n    normalized_frequency = tr_te[col].value_counts(normalize=True).to_dict()\n    tr_te[col+'_count'] = tr_te[col].map(normalized_frequency)","f0c2889d":"count_cols = [f'feature_{col_num}_count' for col_num in range(50)]\ntr_te['avg_count'] = tr_te[count_cols].mean(axis=1)\ntr_te['min_count'] = tr_te[count_cols].min(axis=1)\ntr_te['max_count'] = tr_te[count_cols].max(axis=1)\ntr_te['std_count'] = tr_te[count_cols].std(axis=1)\ntr_te['max_minus_min_count'] = tr_te['max_count'] - tr_te['min_count']","04846f28":"for col_num in tqdm(range(50)):\n    col = f'feature_{col_num}'\n    print(col, tr_te[col].value_counts().index[0])","4c009588":"for col_num in tqdm(range(50)):\n    col = f'feature_{col_num}'\n    tr_te[col+'_uncommon'] = pd.to_numeric(tr_te[col]!=0) # Since 0 is the most common class","ace98fbf":"uncommon_cols = [f'feature_{col_num}_uncommon' for col_num in range(50)]\ntr_te['avg_uncommonness'] = tr_te[uncommon_cols].mean(axis=1)","ef2adb50":"tr = tr_te.loc[tr_te['target']!=-1].copy()\nte = tr_te.loc[tr_te['target']==-1].copy()","86543cb9":"tr_te","901f7871":"cols_to_use = [f'feature_{x}_modified' for x in range(50)]+count_cols+['avg_count','min_count','max_count','std_count','max_minus_min_count']+uncommon_cols+['avg_uncommonness']\ntarget_col = 'target'","05bc03d8":"# Label the fold numbers in the train set\ntr['fold_number'] = -1 # Initialize fold number\nfor fold_number, (train_index, valid_index) in enumerate(skf.split(tr[cols_to_use], tr[target_col])):\n    tr.loc[valid_index, 'fold_number'] = fold_number","17c8dfdc":"tr.groupby('fold_number')['target'].value_counts()","05e5c46d":"tr['oof_predictions_class0'] = 0\ntr['oof_predictions_class1'] = 0\ntr['oof_predictions_class2'] = 0\ntr['oof_predictions_class3'] = 0\n\nte['predictions_class0'] = 0\nte['predictions_class1'] = 0\nte['predictions_class2'] = 0\nte['predictions_class3'] = 0\n\ngain_imps = {}\nfor col in cols_to_use:\n    gain_imps[col] = 0 # Initialize to 0\n\nfor fold_number in [0,1,2,3,4]:\n    train_mask = tr['fold_number'] != fold_number\n    valid_mask = tr['fold_number'] == fold_number\n    \n    # Create the LightGBM Datasets\n    dtrain = lgb.Dataset(data = tr.loc[train_mask, cols_to_use], \n                         label = tr.loc[train_mask, target_col],\n                         categorical_feature = list(range(50))) # Pass the indices of the categorical features\n\n    dvalid = lgb.Dataset(data = tr.loc[valid_mask, cols_to_use], \n                         label = tr.loc[valid_mask, target_col],\n                         categorical_feature = list(range(50))) # Pass the indices of the categorical features\n\n    np.random.seed(fold_number)\n    # Define parameters\n    # Note a different seed for each fold so the models can be different \n    # (Model Diversity = more robust predictions)\n    params = {\n        'objective': 'multiclass',\n        'num_class': 4, # Only used in multiclass\n        'metric': ['multi_logloss','multi_error'], \n        'boosting_type': 'gbdt',\n        'num_leaves': 31, \n        'max_depth': 11, \n        'learning_rate': 0.05, \n        'feature_fraction': 0.5, \n        'seed': 2021 + fold_number,\n        'cat_l2': 10,\n        'cat_smooth': 10,\n        'verbose':-1\n    }\n    \n    print(f'=========== Fold {fold_number} ===========')\n    # Train the model\n    model = lgb.train(\n        params=params,\n        train_set = dtrain,\n        num_boost_round=1000,\n        valid_sets=[dtrain, dvalid],\n        verbose_eval = 50,\n        early_stopping_rounds=100,\n        categorical_feature=list(range(50))\n    )\n    \n    \n    # Make your predictions\n    oof_preds = model.predict(tr.loc[valid_mask, cols_to_use])\n    test_preds = model.predict(te[cols_to_use])\n    for class_num in range(4):\n        tr.loc[valid_mask, f'oof_predictions_class{class_num}'] = oof_preds[:,class_num]\n        te[f'predictions_class{class_num}'] += test_preds[:,class_num]\n        \n    \n    # If it's the first fold, make SHAP predictions so we can explain model\n    if fold_number == 0:\n        print(f'=========== Performing SHAP Explainer ===========')\n        # Commented this out because it takes too long...\n        # Maybe SHAP is slow with categorical values?\n        # shap_values = shap.Explainer(model).shap_values(tr[cols_to_use])\n        \n    # Save out the gain importances instead\n    for col, imp in zip(cols_to_use, model.feature_importance(importance_type='gain')):\n        gain_imps[col] += imp\n    \nfor class_num in range(4):\n    te[f'predictions_class{class_num}'] \/= 5\n    \nfor col in cols_to_use:\n    gain_imps[col] \/= 5","5e36cc60":"# https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.from_dict.html\ngain_imps_df = pd.DataFrame.from_dict(gain_imps, orient='index').reset_index(drop=False)\ngain_imps_df.columns = ['feat','imp']\ngain_imps_df = gain_imps_df.sort_values('imp',ascending=False)\n\n# Also attach the nunique for each\ngain_imps_df['nunique'] = gain_imps_df['feat'].apply(lambda x: tr_te[x].nunique())\ngain_imps_df.head(50)","ad809b0b":"# Least important:\ngain_imps_df.tail(50)","75dcc7db":"log_loss(y_true = tr['target'], \n         y_pred=tr[[f'oof_predictions_class{class_num}' for class_num in range(4)]])","1d0e2908":"saveout = te[['id','predictions_class0','predictions_class1','predictions_class2','predictions_class3']]\nsaveout.columns = ['id','Class_1','Class_2','Class_3','Class_4']\nsaveout.to_csv('submission.csv', index=False)","b1a01616":"# View predictions\nsaveout","c6c23134":"# Step 1: Any categories in test that are not in train? Any categories in train that are not in test?","882c32a5":"# So we see that the maximum number of rows for a missing value is 11. In order to increase regularization and robustness, let's overwrite any values that have fewer than 20 samples with -99, just to represent the idea of \"rare value\".","e49af171":"# Can we improve this at all?\nI remember in one competition that taking the counts of each feature ended up improving the cross-validation. Maybe we can do a similar thing?","28b14592":"# And study the gain importances:","7d1f3acd":"# Okay, now let's perform LightGBM.","f48d34fb":"# Get our new log loss:","fce24f12":"# Before we begin, for my sanity, let's just convert the target column into integers. \nRight now it's \"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\" so the column is an object. Let's convert this to a numeric columns.","bc97b1c7":" Following https:\/\/www.kaggle.com\/returnofsputnik\/may-tabular-coreys-eda I hypothesize that every feature is actually a category, not a numeric feature. If that's the case, we have to take a few steps.\n 1. Check if there are any categories in the test that are not in the train\n 2. Run LightGBM, tune the categorical hyperparameters (e.g. cat_l2)\n 3. Run SHAP, see which features are most important, may be helpful for feature engineering inspiration\n 3. Feature engineer and re-run, does it have any impact?\n 4. Run SHAP, see if anything else interesting comes out\n 5. Rerun blindly on full set with same hyperparameters\n 6. Potentially pseudolabel and rerun","e95d6c6b":"# Additionally, because I believe these to all be categories, let's Label Encode them.","a8b82f8c":"# Aight, let's split back out and retry","1057dd31":"# Let's look at the log loss?","33ee97f7":"# Loop through all 5 folds","564e63fe":"# Save out predictions"}}