{"cell_type":{"3506799d":"code","0ded95d4":"code","f4f559c1":"code","536e2ea1":"code","23e0632b":"code","ac5550e2":"code","6545946b":"code","57fa23b0":"code","5464cce2":"code","588f13d7":"code","fa365e7c":"code","49d59a98":"code","fd441b30":"code","be3de712":"code","550023df":"code","2af4b8cf":"code","73a1cb05":"code","928c36f3":"code","f6b05ef2":"code","01fda6f4":"code","50bb498c":"markdown","a13e5b51":"markdown","e1e69919":"markdown","e90950ab":"markdown","326e73be":"markdown","5eda7951":"markdown","8ac8cc2e":"markdown","666f81fc":"markdown","d8bfd06f":"markdown","5a64d300":"markdown","562a0df0":"markdown","69ae7686":"markdown","bc678b17":"markdown","aa18c651":"markdown","faec6ed7":"markdown","66db5c53":"markdown","1101fe49":"markdown","3d73cfd0":"markdown","f66f971a":"markdown","ab995cc4":"markdown","8c8d9482":"markdown","5a7fde53":"markdown","b0fda55f":"markdown","5f05328f":"markdown","e6bf3f52":"markdown"},"source":{"3506799d":"#>sudo pip install xgboost","0ded95d4":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom pandas import get_dummies\nimport plotly.graph_objs as go\nfrom sklearn import datasets\nimport plotly.plotly as py\nimport seaborn as sns\nimport xgboost as xgb\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport scipy\nimport numpy\nimport json\nimport sys\nimport csv\nimport os","f4f559c1":"print('matplotlib: {}'.format(matplotlib.__version__))\nprint('sklearn: {}'.format(sklearn.__version__))\nprint('scipy: {}'.format(scipy.__version__))\nprint('seaborn: {}'.format(sns.__version__))\nprint('pandas: {}'.format(pd.__version__))\nprint('numpy: {}'.format(np.__version__))\nprint('Python: {}'.format(sys.version))","536e2ea1":"warnings.filterwarnings('ignore')\nsns.set(color_codes=True)\nplt.style.available\n%matplotlib inline\n%precision 2","23e0632b":"# import Dataset to play with it\ndataset = pd.read_csv('..\/input\/iris-dataset\/Iris.csv')","ac5550e2":"\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","6545946b":"from sklearn.ensemble import RandomForestClassifier\nModel=RandomForestClassifier(max_depth=2)\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","57fa23b0":"from sklearn.ensemble import BaggingClassifier\nModel=BaggingClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","5464cce2":"from sklearn.ensemble import AdaBoostClassifier\nModel=AdaBoostClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","588f13d7":"from sklearn.ensemble import GradientBoostingClassifier\nModel=GradientBoostingClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","fa365e7c":"from sklearn import datasets\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target","49d59a98":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","fd441b30":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)","be3de712":"from sklearn.datasets import dump_svmlight_file\n\ndump_svmlight_file(X_train, y_train, 'dtrain.svm', zero_based=True)\ndump_svmlight_file(X_test, y_test, 'dtest.svm', zero_based=True)\ndtrain_svm = xgb.DMatrix('dtrain.svm')\ndtest_svm = xgb.DMatrix('dtest.svm')","550023df":"param = {\n    'max_depth': 3,  # the maximum depth of each tree\n    'eta': 0.3,  # the training step for each iteration\n    'silent': 1,  # logging mode - quiet\n    'objective': 'multi:softprob',  # error evaluation for multiclass training\n    'num_class': 3}  # the number of classes that exist in this datset\nnum_round = 20  # the number of training iterations","2af4b8cf":"bst = xgb.train(param, dtrain, num_round)","73a1cb05":"bst.dump_model('dump.raw.txt')","928c36f3":"preds = bst.predict(dtest)","f6b05ef2":"best_preds = np.asarray([np.argmax(line) for line in preds])","01fda6f4":"from sklearn.metrics import precision_score\n\nprint (precision_score(y_test, best_preds, average='macro'))","50bb498c":"###### <a id=\"14\"><\/a> <br>\n## 7- Import packages","a13e5b51":"There is a comprehensive installation guide on the [XGBoost documentation website](http:\/\/xgboost.readthedocs.io\/en\/latest\/build.html).\n\n\n### 4-1 XGBoost in Python\nInstallation instructions are available on the Python section of the XGBoost installation guide.\n\nThe official Python Package Introduction is the best place to start when working with XGBoost in Python.\n\nTo get started quickly, you can type:\n<br>\n[go to top](#top)","e1e69919":"<a id=\"2\"><\/a> <br>\n# 2- Ensemble Techniques\n1. Bagging based Ensemble learning\n1. Boosting-based Ensemble learning\n1. Voting based Ensemble learning","e90950ab":"<a id=\"27\"><\/a> <br>\n# 9-Conclusion\n* That XGBoost is a library for developing fast and high performance gradient boosting tree models.\n* That XGBoost is achieving the best performance on a range of difficult machine learning tasks.\n* That you can use this library from the command line, Python and R and how to get started.\n<br>\n[go to top](#top)","326e73be":"<a id=\"18\"><\/a> <br>\n## 8-2 Prepare Features & Targets\nFirst of all seperating the data into dependent(Feature) and independent(Target) variables.\n\n**<< Note 4 >>**\n* X==>>Feature\n* y==>>Target","5eda7951":"<a id=\"\"><\/a> <br>\n## 5- Problem Definition\nI think one of the important things when you start a new machine learning project is Defining your problem. that means you should understand business problem.( **Problem Formalization**)\n\nProblem Definition has four steps that have illustrated in the picture below:\n<a id=\"8\"><\/a> <br>\n### 5-1 Problem Feature\nwe will use the classic Iris data set. This dataset contains information about three different types of Iris flowers:\n\n* Iris Versicolor\n* Iris Virginica\n* Iris Setosa\n\nThe data set contains measurements of four variables :\n\n* sepal length \n* sepal width\n* petal length \n* petal width\n \nThe Iris data set has a number of interesting features:\n\n1. One of the classes (Iris Setosa) is linearly separable from the other two. However, the other two classes are not linearly separable.\n\n2. There is some overlap between the Versicolor and Virginica classes, so it is unlikely to achieve a perfect classification rate.\n\n3. There is some redundancy in the four input variables, so it is possible to achieve a good solution with only three of them, or even (with difficulty) from two, but the precise choice of best variables is not obvious.\n\n**Why am I  using iris dataset:**\n\n1- This is a good project because it is so well understood.\n\n2- Attributes are numeric so you have to figure out how to load and handle data.\n\n3- It is a classification problem, allowing you to practice with perhaps an easier type of supervised learning algorithm.\n\n4- It is a multi-class classification problem (multi-nominal) that may require some specialized handling.\n\n5- It only has 4 attributes and 150 rows, meaning it is small and easily fits into memory (and a screen or A4 page).\n\n6- All of the numeric attributes are in the same units and the same scale, not requiring any special scaling or transforms to get started.[5]\n\n7- we can define problem as clustering(unsupervised algorithm) project too.\n<a id=\"9\"><\/a> <br>\n### 5-2 Aim\nThe aim is to classify iris flowers among three species (setosa, versicolor or virginica) from measurements of length and width of sepals and petals\n<a id=\"10\"><\/a> <br>\n### 5-3 Variables\nThe variables are :\n**sepal_length**: Sepal length, in centimeters, used as input.\n**sepal_width**: Sepal width, in centimeters, used as input.\n**petal_length**: Petal length, in centimeters, used as input.\n**petal_width**: Petal width, in centimeters, used as input.\n**setosa**: Iris setosa, true or false, used as target.\n**versicolour**: Iris versicolour, true or false, used as target.\n**virginica**: Iris virginica, true or false, used as target.\n\n**<< Note >>**\n> You must answer the following question:\nHow does your company expact to use and benfit from your model.\n<br>\n[go to top](#top)","8ac8cc2e":"<a id=\"21\"><\/a> <br>\n### 2-1- what-is-the-difference-between-bagging-and-boosting?\n**Bagging**: It is the method to decrease the variance of model by generating additional data for training from your original data set using combinations with repetitions to produce multisets of the same size as your original data.\n\n**Boosting**: It helps to calculate the predict the target variables using different models and then average the result( may be using a weighted average approach).\n<img src='https:\/\/cdn-images-1.medium.com\/max\/800\/0*j-igefe7bfcBLXlU.png'>\n<br>\n[go to top](#top)","666f81fc":"<a id=\"20\"><\/a> <br>\n## 8-4 Bagging classifier \nA Bagging classifier is an ensemble **meta-estimator** that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n\nThis algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting . If samples are drawn with replacement, then the method is known as Bagging . When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces . Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches .[http:\/\/scikit-learn.org]\n<br>\n[go to top](#top)","d8bfd06f":"Determine the precision of this prediction:","5a64d300":"<a id=\"3\"><\/a> <br>\n## 3-XGBoost?\n* **XGBoost** is an algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data.\n* **XGBoost** is an implementation of gradient boosted decision trees designed for speed and performance.\n* **XGBoost** is short for e**X**treme **G**radient **Boost**ing package.","562a0df0":"<a id=\"top\"><\/a> <br>\n## Notebook  Content\n1. Introduction\n    1. Why Ensemble Learning?\n1. Ensemble Techniques\n    1. what-is-the-difference-between-bagging-and-boosting?\n1. XGBoost\n    1. Installing XGBoost \n    1. Matrix Multiplication\n    1. Vector-Vector Products\n    1. Outer Product of Two Vectors\n    1. Matrix-Vector Products\n    1. Matrix-Matrix Products\n1. Random Forest\n1. AdaBoost\n1. GBM\n1. XGB\n1. Light GBM\n1. Conclusion","69ae7686":"<a id=\"22\"><\/a> <br>\n##  8-5 AdaBoost classifier\n\nAn AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\nThis class implements the algorithm known as **AdaBoost-SAMME** .","bc678b17":"<a id=\"1\"><\/a> <br>\n#  1- Introduction\n**Ensemble modeling** is a powerful way to improve the performance of your model. It usually pays off to apply ensemble learning over and above various models you might be building. Time and again, people have used ensemble models in competitions like Kaggle and benefited from it.\n\nEnsemble learning is a broad topic and is only confined by your own imagination. For the purpose of this Notebook, I will cover the basic concepts and ideas of ensemble modeling. This should be enough for you to start building ensembles at your own end. As usual, we have tried to keep things as simple as possible.[6]\n<a id=\"11\"><\/a> <br>\n## 1-1 Why Ensemble Learning?\n1. Difference in population\n1. Difference in hypothesis\n1. Difference in modeling technique\n1. Difference in initial seed\n<br>\n[go to top](#top)","aa18c651":" # Ensemble Learning \n- In this simple tutorial you can learn all of the thing you need for  <b>using ensemble learning as a method<\/b>","faec6ed7":"<a id=\"11\"><\/a> <br>\n## 6- Inputs & Outputs\n<a id=\"12\"><\/a> <br>\n### 6-1 Inputs\n**Iris** is a very popular **classification** and **clustering** problem in machine learning and it is such as \"Hello world\" program when you start learning a new programming language. then I decided to apply Iris on  20 machine learning method on it.\nThe Iris flower data set or Fisher's Iris data set is a **multivariate data set** introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers in three related species. Two of the three species were collected in the Gasp\u00e9 Peninsula \"all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus\".\nThe data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.\n\nAs a result, **iris dataset is used as the input of all algorithms**.\n<a id=\"13\"><\/a> <br>\n### 6-2 Outputs\nthe outputs for our algorithms totally depend on the type of classification or clustering algorithms.\nthe outputs can be the number of clusters or predict for new input.\n\n**setosa**: Iris setosa, true or false, used as target.\n**versicolour**: Iris versicolour, true or false, used as target.\n**virginica**: Iris virginica, true or false, used as a target.\n<br>\n[go to top](#top)","66db5c53":"<a id=\"26\"><\/a> <br>\n##  8-7 XGBoost\nFinally see how to perform XGBoost","1101fe49":"<a id=\"16\"><\/a> <br>\n## 8- Model Deployment\nIn this section have been applied more than **20 learning algorithms** that play an important rule in your experiences and improve your knowledge in case of ML technique.\n\n> **<< Note 3 >>** : The results shown here may be slightly different for your analysis because, for example, the neural network algorithms use random number generators for fixing the initial value of the weights (starting points) of the neural networks, which often result in obtaining slightly different (local minima) solutions each time you run the analysis. Also note that changing the seed for the random number generator used to create the train, test, and validation samples can change your results.\n<br>\n[go to top](#top)","3d73cfd0":"<a id=\"17\"><\/a> <br>\n## 8-1 Families of ML algorithms\nThere are several categories for machine learning algorithms, below are some of these categories:\n* Linear\n    * Linear Regression\n    * Logistic Regression\n    * Support Vector Machines\n* Tree-Based\n    * Decision Tree\n    * Random Forest\n    * GBDT\n* KNN\n* Neural Networks\n\n-----------------------------\nAnd if we  want to categorize ML algorithms with the type of learning, there are below type:\n* Classification\n\n    * k-Nearest \tNeighbors\n    * LinearRegression\n    * SVM\n    * DT \n    * NN\n    \n* clustering\n\n    * K-means\n    * HCA\n    * Expectation Maximization\n    \n* Visualization \tand\tdimensionality \treduction:\n\n    * Principal \tComponent \tAnalysis(PCA)\n    * Kernel PCA\n    * Locally -Linear\tEmbedding \t(LLE)\n    * t-distributed\tStochastic\tNeighbor\tEmbedding \t(t-SNE)\n    \n* Association \trule\tlearning\n\n    * Apriori\n    * Eclat\n* Semisupervised learning\n* Reinforcement Learning\n    * Q-learning\n* Batch learning & Online learning\n* Ensemble  Learning\n\n**<< Note >>**\n> Here is no method which outperforms all others for all tasks\n<br>\n[go to top](#top)","f66f971a":"<a id=\"19\"><\/a> <br>\n## 8-3 RandomForest\nA random forest is a meta estimator that **fits a number of decision tree classifiers** on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. \n\nThe sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).","ab995cc4":"After loading the data via **pandas**, we should checkout what the content is, description and via the following:\n<br>\n[go to top](#top)","8c8d9482":"**<< Note 1 >>**\n\n* Each row is an observation (also known as : sample, example, instance, record)\n* Each column is a feature (also known as: Predictor, attribute, Independent Variable, input, regressor, Covariate)","5a7fde53":"<a id=\"15\"><\/a> <br>\n## 7-1 Data Collection\n**Data collection** is the process of gathering and measuring data, information or any variables of interest in a standardized and established manner that enables the collector to answer or test hypothesis and evaluate outcomes of the particular collection.[techopedia]\n\n**Iris dataset**  consists of 3 different types of irises\u2019 (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray\n\nThe rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.[6]\n<br>\n[go to top](#top)","b0fda55f":"<a id=\"4\"><\/a> <br>\n## 4- Installing XGBoost","5f05328f":"* Speed and performance : Originally written in C++, it is comparatively faster than other ensemble classifiers.\n\n* Core algorithm is parallelizable : Because the core XGBoost algorithm is parallelizable it can harness the power of multi-core computers. It is also parallelizable onto GPU\u2019s and across networks of computers making it feasible to train on very large datasets as well.\n\n* Consistently outperforms other algorithm methods : It has shown better performance on a variety of machine learning benchmark datasets.\n\n* Wide variety of tuning parameters : XGBoost internally has parameters for cross-validation, regularization, user-defined objective functions, missing values, tree parameters, scikit-learn compatible API etc.\n* Win competition On Kaggle : there are a lot of winners on Kaggle that use XGBoost\n<br>\n[go to top](#top)","e6bf3f52":"<a id=\"23\"><\/a> <br>\n## 8-6 Gradient Boosting Classifier\nGB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions."}}