{"cell_type":{"801df8b1":"code","77e10e0e":"code","b2dcb7ec":"code","9402786a":"code","c9f95547":"code","87948a1f":"code","9fc33c6b":"code","62a9a2da":"code","e380ac6d":"code","79d6b6af":"code","5d4917f2":"code","91a52f86":"code","1513778d":"code","b04cd0ad":"code","97613d69":"code","ce554df3":"code","e2e4af58":"code","e8e1cd32":"code","5189b263":"code","525f91e1":"code","e3453941":"code","9761c9db":"code","6c7a0e58":"code","5db58282":"code","82507d95":"code","738a2087":"markdown","2bdcadcb":"markdown","a70a49c4":"markdown","28ccc0d9":"markdown","1702c991":"markdown","f4f21879":"markdown","24415f68":"markdown","00c02f68":"markdown","40b408c8":"markdown","ed72bdf8":"markdown","00c2aa4a":"markdown","d917f593":"markdown","deb6a548":"markdown","4128f82d":"markdown","ce46350e":"markdown"},"source":{"801df8b1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","77e10e0e":"test = pd.read_csv(\"\/kaggle\/input\/santander-product-recommendation\/test_ver2.csv.zip\",encoding=\"latin1\", compression=\"zip\")\ntest.head(2)","b2dcb7ec":"sample = pd.read_csv(\"\/kaggle\/input\/santander-product-recommendation\/sample_submission.csv.zip\",encoding=\"latin1\", compression=\"zip\")\nsample.head(100)","9402786a":"df = pd.read_csv(\"\/kaggle\/input\/santander-product-recommendation\/train_ver2.csv.zip\",encoding=\"latin1\", compression=\"zip\",nrows=10000)","c9f95547":"for ft in df.columns:\n    print(ft,\" : \",df[ft].unique(),\" : \",len(df[ft].unique()))","87948a1f":"def processData(df,getDummies=True):\n    data = df.copy()\n    data.drop(columns=[\"ult_fec_cli_1t\",\"canal_entrada\"],inplace=True)\n    data[\"conyuemp\"] = data[\"conyuemp\"].fillna('N')\n    data[\"renta\"] = data[\"renta\"].fillna(data[\"renta\"].mean())\n    #Filling nan vlaues \n    def is_float(string):\n      try:\n        return float(string) or float(string)==0  \n      except:  # String is not a number\n        return False\n\n    data[\"ind_empleado\"] = data[\"ind_empleado\"].fillna('N')\n\n    data[\"age\"] = data[\"age\"].replace(' ', '', regex=True)\n    data[\"age\"] = data[\"age\"].replace('.', '')\n\n    data[\"age\"] = data[\"age\"].replace('NA',np.nan)\n    data[\"age\"] = data[\"age\"].astype(float)\n\n    data[\"ind_nuevo\"] = data[\"ind_nuevo\"].fillna(1)\n\n\n    data[\"antiguedad\"] = data[\"antiguedad\"].replace(' ', '', regex=True)\n    data[\"antiguedad\"] = data.loc[:,\"antiguedad\"].replace(\"NA\",1)\n    data[\"antiguedad\"] = data[\"antiguedad\"].astype(int)\n    data.loc[data.antiguedad<0,\"antiguedad\"] = 1    \n    data[\"indfall\"] = data[\"indfall\"].fillna('S')\n    data[\"tipodom\"] = data[\"tipodom\"].fillna(0)\n    data[\"ind_actividad_cliente\"] = data[\"ind_actividad_cliente\"].fillna(0)\n    data.drop(columns=[\"indrel\",\"fecha_alta\",\"nomprov\"],inplace=True)\n    data = data.dropna()    \n    if(getDummies):\n        data = pd.get_dummies(data, columns=['indresi','indext','conyuemp','indfall','sexo',\n                                         'pais_residencia','ind_empleado','tiprel_1mes',\n                                        \"segmento\"],drop_first=True)\n    return data\ndef dataBin(df, getDummies=True):\n    binned = df.copy()\n    binned[\"renta\"] = pd.qcut(binned[\"renta\"], 3, labels=[\"low\", \"mid\", \"high\"])\n    binned[\"age\"] = pd.cut(binned[\"age\"], [0, 40, 80,200], labels=[\"low\",\"mid\",\"high\"])\n    binned[\"antiguedad\"] = pd.cut(binned[\"antiguedad\"], [0, 50, 150,250], labels=[\"low\",\"mid\",\"high\"])\n    if(getDummies):\n        binned = pd.get_dummies(binned, columns=[\"renta\",\"age\",\"antiguedad\",\"cod_prov\"],drop_first=True)\n    return binned\n","9fc33c6b":"import seaborn as sns\nimport matplotlib.pyplot as plt\ndata = processData(df)\nsns.set_style('whitegrid')\n#Binning on renta, age, antiguedad\ndata['age'].plot(kind='hist')\nplt.show()\ndata['renta'].plot(kind='hist')\nplt.show()\n\ndata['antiguedad'].plot(kind='hist')\nplt.show()\n\n#age can be binned: 0-40, 40-80, 80+\n","62a9a2da":"data['antiguedad'].unique()","e380ac6d":"binned= dataBin(data)","79d6b6af":"ItemNames = ['ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1',\n       'ind_cder_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1',\n       'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1',\n       'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n       'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1',\n       'ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1',\n       'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1',\n       'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1',]","5d4917f2":"itemcols = ItemNames\nitemcols.append(\"ncodpers\") \n\n#binned\nUserItemMatrix = df[itemcols]\nUserItemMatrix = UserItemMatrix.groupby(\"ncodpers\").sum() \nUserItemMatrix","91a52f86":"ratings_dict = {\n    \"item\": [],\n    \"user\": [],\n    \"rating\": [],\n}\nUserItemMatrix \n\nindexes = UserItemMatrix.index\nitems = UserItemMatrix.columns\nfor i in range(len(UserItemMatrix)):\n    for j in range(len(items)):\n        rating = UserItemMatrix.iloc[i,j]\n        \n        if(rating>0):\n            ratings_dict[ \"item\" ].append(items[j])\n            ratings_dict[ \"user\" ].append(indexes[i])\n            ratings_dict[\"rating\"].append(rating)\n        else:\n            ratings_dict[ \"item\" ].append(items[j])\n            ratings_dict[ \"user\" ].append(indexes[i])\n            ratings_dict[\"rating\"].append(0)#UserItemMatrixCrop.iloc[:,j].mean())","1513778d":"from surprise import Reader, Dataset\n\nmax_rating = UserItemMatrix.max().values.max()\ndfRatings = pd.DataFrame(ratings_dict)\nreader = Reader(rating_scale=(0, max_rating))\n\ndata = Dataset.load_from_df(dfRatings[[\"user\", \"item\", \"rating\"]], reader)\n\nfrom surprise import SVD\nfrom surprise.model_selection import GridSearchCV\n\nparam_grid = {\n    \"n_epochs\": [1, 30],\n    \"lr_all\": [0.002, 0.003],\n    \"reg_all\": [0.4]\n}\ngs = GridSearchCV(SVD, param_grid, measures=[\"rmse\", \"mae\"], cv=3,refit=True)\ngs.fit(data)\n\nprint(gs.best_score[\"rmse\"])\nprint(gs.best_params[\"rmse\"])","b04cd0ad":"from surprise.model_selection import KFold\nfrom surprise import accuracy\nfrom surprise import KNNBasic\n\nkf = KFold(n_splits=5)\nsim_options = {'name': 'cosine',\n               'user_based': True  # compute  similarities between items\n               }\nalgo = SVD()\n\nfor trainset, testset in kf.split(data):\n\n    # train and test algorithm.\n    algo.fit(trainset)\n    predictions = algo.test(testset)\n    # Compute and print Root Mean Squared Error\n    accuracy.mae(predictions, verbose=True)\n    accuracy.rmse(predictions, verbose=True)","97613d69":"testPersons = test[\"ncodpers\"]\nsubmission = {\"ncodpers\":[],\"added_products\":[]}\nfor personid in testPersons.values:\n    preds = \"\"\n    for itemName in ItemNames:\n        pred= gs.predict(personid,itemName)\n        prob = pred[3]\/max_rating\n        if(prob>0.05):\n            preds+= \" \" + itemName if preds != \"\" else itemName\n    submission[\"ncodpers\"].append(personid)\n    submission[\"added_products\"].append(preds)        \nSubmissiondf = pd.DataFrame(data=submission)  ","ce554df3":"Submissiondf.set_index(\"ncodpers\")\nSubmissiondf.to_csv(\"sub7MData5e-2.csv\",index=False)","e2e4af58":"featuresNotUserInfo = ['ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1',\n       'ind_cder_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1',\n       'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1',\n       'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n       'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1',\n       'ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1',\n       'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1',\n       'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1',\"fecha_dato\"]\ndata = processData(df.drop(columns=featuresNotUserInfo),getDummies = False)\ndata = dataBin(data,getDummies= False)\nuserInfo = data.groupby(\"ncodpers\").last()\nuserInfo = userInfo.reset_index()\n\nuserInfo.rename(columns={\"ncodpers\":\"user_id\"},inplace=True)\nuserInfo = userInfo.to_dict()","e8e1cd32":"!pip install turicreate\nimport turicreate as tc\n\nSF_userInfo = tc.SFrame(userInfo)\n#!pip install turicreate\nturiDict = {}\nturiDict[\"item_id\"] = ratings_dict[\"item\"]\nturiDict[\"user_id\"] = ratings_dict[\"user\"]\nturiDict[\"rating\"] = ratings_dict[\"rating\"]\nactions  = tc.SFrame(turiDict)\ntraining_data, validation_data = tc.recommender.util.random_split_by_user(actions)\nmodel = tc.recommender.create(training_data,target='rating',user_data=SF_userInfo)\n\n","5189b263":"model","525f91e1":"model.save(\"recommendations.model\")\n","e3453941":"ItemNames = ['ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1',\n       'ind_cder_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1',\n       'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1',\n       'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n       'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1',\n       'ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1',\n       'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1',\n       'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1',]","9761c9db":"model.get_similar_items(ItemNames, k=24)","6c7a0e58":"testPersons = test[\"ncodpers\"]\n#Recommendation skorlar\u0131\nrecommendations = model.recommend(testPersons.values)\nsubmission = {\"ncodpers\":[],\"added_products\":[]}\n\n#0.9 thresholddakilerin \u00fcst\u00fc al\u0131n\u0131r\nusers = recommendations[recommendations[\"score\"]>0.9]\n\n#Test dosyas\u0131ndak her user i\u00e7in, \u00f6neri skorlar\u0131nda o user id'ye denk gelen itemleri ekle\nfor id in testPersons.values:\n    submission[\"ncodpers\"].append(id)\n    itemsOfUser = users[users[\"user_id\"]==id][\"item_id\"]\n    \n    #userlar\u0131n alaca\u011f\u0131 t\u00fcm itemleri bo\u015fluk ile ay\u0131r\u0131p string haline getir\n    itemString = \"\"\n    for item in itemsOfUser:\n        itemString += \" \"+item\n        \n    print(id,\" : \",itemString)    \n    submission[\"added_products\"].append(itemString)  \nSubmissiondf = pd.DataFrame(data=submission)  ","5db58282":"Submissiondf.head(5)","82507d95":"Submissiondf.set_index(\"ncodpers\")\nSubmissiondf.to_csv(\"turicreate.csv\",index=False)","738a2087":"Create and Train Recommender System","2bdcadcb":"### My Data Processing and Binning Functions","a70a49c4":"Check Item Similarity","28ccc0d9":"## Handling NANs\n* ind_empleado          -> fill with 'N' or delete\n* pais_residencia       -> drop nan\n* sexo                  -> drop nan\n* age                   -> fill with most popular value \n* fecha_alta            -> delete, we mustn't use that feature anyway, we have antiguedad\n* ind_nuevo             -> fill NAN with 1(new customer)\n* antiguedad            -> fill 'NA' and negative numbers to 1\n* indrel                -> fill NAN to 99 OR DELETE FEATURE   \n* indresi               -> drop nan\n* indext                -> drop nan\n* indfall               -> fill nan as dead ('S')\n* tipodom               -> fill nan to 0\n* ind_actividad_cliente -> fill nan to 0 (not active) or drop  \n* segmento              -> drop nan\n* ind_nomina_ult1       -> drop nan\n* ind_nom_pens_ult1     -> drop nan","1702c991":"Save recommender Model","f4f21879":"ItemNames","24415f68":"Binning to renta, age and antiguedad","00c02f68":"1. ult_fec_cli_1t, : Last date as primary customer (if he isn't at the end of the month) -> deleting feature\n1. conyuemp, : Spouse index. 1 if the customer is spouse of an employee                  -> filling with 0\n1. renta, : Gross income of the household                                                -> (filling with AVG value)\n1. canal_entrada : channel used by the customer to join                                  -> can't fill, delete feature or nans \n1. nomprov and cod_prov is null at same way                                              -> drop nan   \n1. tiprel_1mes and indrel_1mes at same way                                               -> drop nan \n\nfeatures contains a lot of nan variables. We can drop these features or fill nan values with any logic.    ","40b408c8":"Fine Tuning and Using SVD Collabrative Filtering algorithm using Scikit-Suprise ","ed72bdf8":"## TURICREATE AND INCLUDING USER INFORMATIONS","00c2aa4a":"Get Ratings By Sale Count","d917f593":"Set Ratings Dictionary ","deb6a548":"* ncodpers              -> customer id, REMAIN SAME\n* ind_empleado          -> 5 classes and a NAN ,ohe\n* pais_residencia       -> 118 classes and a NAN, ohe\n* sexo                  -> 2 classes and a NAN, ohe\n* age                   -> have value 'NA' divide by 100 OR apply binning \n* fecha_alta            -> have NAN, date of beginning the journey of customer in bank\n* ind_nuevo             -> 1 if customer is new and a NAN -> remain same\n* antiguedad            -> the time customer is a customer, have NA and extreme numbers -99999 -> min max scale OR binning\n* indrel                -> 2 classes 1, 9 and a NAN  \n* ult_fec_cli_1t        -> date, have NAN\n* indrel_1mes           -> 5 classes, 1,2,3,4,5,P , different type of values [1.0 nan 3.0 '1.0' '1' 'P' .. etc]  \n* tiprel_1mes           -> 4 classes, ohe, have NAN and 'N'\n* indresi               -> 2 classes, ohe, have NAN\n* indext                -> 2 classes, ohe, have NAN\n* conyuemp              -> 2 classes, ohe, have NAN (it means if it is a spurse of an employee)\n* canal_entrada         -> 162 classes,have NAN, ohe or DIFFERENT ENCODING\n* indfall               -> is dead, 2 classes (have NAN values), ohe\n* tipodom               -> 2 classes, 1 or NAN, convert na to 0\n* cod_prov              -> 53 classes with a NAN, [\"52.\",\"4.\"...]\n* nomprov               -> province name, 53 classes, ohe OR DIFFERENT ENCODING\n* ind_actividad_cliente -> 0 or 1 or NAN, delete NAN?\n* renta                 -> income, scale OR binning\n* segmento              -> 3 classes and NAN, ohe, delete NAN?\n* ind_nomina_ult1       -> 0, 1 or NAN\n* ind_nom_pens_ult1     -> 0, 1 or NAN\n* others                -> REMAIN SAME  ","4128f82d":"Drop date and items to get user information","ce46350e":"Check all features' values to see exceptions and errors on data"}}