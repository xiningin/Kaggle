{"cell_type":{"ba75db4d":"code","b71db4f3":"code","20ad2031":"code","dabeaa27":"code","8acd74de":"code","3cc1b986":"code","9e33106c":"code","9c50945a":"code","90e708f1":"code","fb1edfee":"code","87759082":"code","3fd5968f":"code","393b2323":"code","0af18a84":"markdown","3ce18e30":"markdown","7959c615":"markdown","bc465288":"markdown","270a4f4a":"markdown","dc341717":"markdown","f41c159a":"markdown","1e53b0bb":"markdown","a65ef2bd":"markdown","d3fb2a33":"markdown","8fc6aea5":"markdown","f3eb7763":"markdown","2e807b04":"markdown","e2dfbd8c":"markdown"},"source":{"ba75db4d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b71db4f3":"dataset = pd.read_csv('\/kaggle\/input\/heart-failure-prediction\/heart.csv')\n\ndataset","20ad2031":"dataset.isnull().sum()","dabeaa27":"category_cols = [ col  for col, dt in dataset.dtypes.items() if dt == object]\n\nprint(category_cols)","8acd74de":"list_of_cat = []\n\nfor i in category_cols:\n    list_of_cat.append(dataset.columns.tolist().index(i))\n    \nprint(list_of_cat)","3cc1b986":"dataset_1 = dataset\n\nX = dataset_1.iloc[:, :-1].values\ny = dataset_1.iloc[:, -1].values","9e33106c":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(),list_of_cat)], remainder='passthrough')\nX = ct.fit_transform(X)","9c50945a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 1)","90e708f1":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train[:, 0:] = sc.fit_transform(X_train[:, 0:])\nX_test[:, 0:] = sc.transform(X_test[:, 0:])","fb1edfee":"from sklearn.linear_model import LogisticRegression\nclassifier_LR= LogisticRegression(random_state = 3)\nclassifier_LR.fit(X_train, y_train)\ny_pred_LR = classifier_LR.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ncm_LR = confusion_matrix(y_test, y_pred_LR)\nprint(cm_LR)\nprint('accuracy score of', classifier_LR,  'model is', accuracy_score(y_test, y_pred_LR)*100)","87759082":"from sklearn.svm import SVC\nclassifier_SVC = SVC(kernel = 'rbf', random_state = 9)\nclassifier_SVC.fit(X_train, y_train)\ny_pred_SVC = classifier_SVC.predict(X_test)\ncm_SVC = confusion_matrix(y_test, y_pred_SVC)\nprint(cm_SVC)\nprint('accuracy score of', classifier_SVC,  'model is', accuracy_score(y_test, y_pred_SVC)*100)","3fd5968f":"from sklearn.naive_bayes import GaussianNB\nclassifier_NB = GaussianNB()\nclassifier_NB.fit(X_train, y_train)\ny_pred_NB = classifier_NB.predict(X_test)\ncm_NB = confusion_matrix(y_test, y_pred_NB)\nprint(cm_NB)\nprint('accuracy score of', classifier_NB,  'model is', accuracy_score(y_test, y_pred_NB)*100)","393b2323":"from sklearn.ensemble import RandomForestClassifier\n\n\nn_estimators = []\n\nAcc_Score_rand = []\nAcc_Score_nest = []\n\nfor i in range(1,100):\n    #print(i)\n    classifier_RF = RandomForestClassifier(n_estimators = i, criterion = 'entropy', random_state = 0)\n    classifier_RF.fit(X_train, y_train)\n    y_pred_RF = classifier_RF.predict(X_test)\n    #print(y_pred_RF)\n    cm_RF = confusion_matrix(y_test, y_pred_RF)\n    Acc_Score_nest.append(accuracy_score(y_test, y_pred_RF)*100)\n    #print(accuracy_score(y_test, y_pred_RF)*100)\n\n\n\nfor i in range(1,100):\n    #print(i)\n    classifier_RF = RandomForestClassifier(n_estimators = Acc_Score_nest.index(max(Acc_Score_nest))+1, criterion = 'entropy', random_state = i)\n    classifier_RF.fit(X_train, y_train)\n    y_pred_RF = classifier_RF.predict(X_test)\n    #print(y_pred_RF)\n    cm_RF = confusion_matrix(y_test, y_pred_RF)\n    Acc_Score_rand.append(accuracy_score(y_test, y_pred_RF)*100)\n    #print(accuracy_score(y_test, y_pred_RF)*100)\n    \n    \n    \nprint('Best n_estimator is', Acc_Score_nest.index(max(Acc_Score_nest))+1)\n\nprint('Best random_state is', Acc_Score_rand.index(max(Acc_Score_rand))+1)\n\nprint('accuracy score of RandomForestClassifier model is', max(Acc_Score_rand))\n","0af18a84":"## Feature Scaling\n","3ce18e30":"### Identifying Column Index numbers of categorical variables","7959c615":"### Splitting Dataset into Independent Variables and Dependent Variable","bc465288":"## Reading the Dataset","270a4f4a":"## Training the Logistic Regression model on the Training set\n","dc341717":"## Training the SVM model on the Training set\n","f41c159a":"### Identifying Categorical Data columns","1e53b0bb":"## Splitting the dataset into the Training set and Test set\n","a65ef2bd":"## Conclusion\n\n1. Data available in CSV file is an aggregarion of 5 independent studies and well documented, with no null values\n2. 4 classification models are tested with 60% is data used for training and 40% used for testing\n3. It is seen that iterating through random_state parameter for Log Regression, SupportVectorClassifier and NaiveBayesClassiifer has no effect on accuracy score. \n4. However, for RandomForestClassifier, the accuracy score is found to be sensitive to n_estimator and random_state parameters.\n5. RandomForestClassifier is trained after iterating through available parameters for n_estimators and random_state\n6. Best accuracy is given by Random Forest Classificatio model of 89% \n","d3fb2a33":"#### No null values, hence no need for imputation","8fc6aea5":"## Training the NaiveBayes model on the Training set","f3eb7763":"## Checking for null values","2e807b04":"### Encoding Categorical Variables with OneHotEncoder","e2dfbd8c":"## Random Forest Regressor - Identifying optimum parameters"}}