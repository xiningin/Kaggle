{"cell_type":{"6a132b02":"code","0f757176":"code","a212807a":"code","4494e927":"code","33249f93":"code","dd041b84":"code","5c10de4f":"code","12214a8e":"code","96f29987":"code","7f4a2a50":"code","d9b3988a":"code","5f0defd7":"code","61a3397c":"code","c4f4435f":"code","eb219edb":"code","46518f3d":"code","6bc3ada5":"code","781b3d4a":"code","132ba8d8":"code","0d9520e5":"code","19079e79":"code","825e3e28":"code","3bd60b17":"code","bfcfa272":"code","9ca554a9":"code","d20627a0":"code","845b18b1":"code","8c1eb617":"code","487ce1b3":"code","08432a2b":"code","7c0b787d":"code","9a16b06a":"code","4d2f7928":"code","c0ea2c90":"code","d88a031b":"code","d075b12d":"code","90d88b0d":"code","8e0a5d41":"code","5c08b4b6":"code","64e75c5a":"code","9ed982b6":"code","e157cbc0":"code","c3cd3237":"code","56fc6fa3":"code","5f8a5b06":"code","43ec7ffa":"code","10b075c6":"code","e9faf27a":"code","77b7cdd8":"code","b3f1a301":"code","6bbc9201":"code","a020ada4":"code","a2760e3f":"code","a2e3919e":"markdown","78f7800f":"markdown","5aec4bdb":"markdown","8d2a1c43":"markdown","030d6b13":"markdown","b3d93d4f":"markdown","20847e68":"markdown","98cd31f7":"markdown","9c6db34f":"markdown","2f30b9fc":"markdown","a42fd8a6":"markdown","bbf4dbda":"markdown","d721ff4a":"markdown","03f41b69":"markdown","5db191a2":"markdown","b82a9339":"markdown","9bf9391d":"markdown","5e9196aa":"markdown","63e40514":"markdown","4f00982a":"markdown","544a00fe":"markdown","2fbb6d17":"markdown","e9fffe69":"markdown","0e9647a2":"markdown","f4b3e29e":"markdown","d9b2a1b7":"markdown","bfff1ed5":"markdown","525505f1":"markdown","143332a1":"markdown","8a8c74af":"markdown","5b1baf6b":"markdown","aadefd19":"markdown","d6cb1c47":"markdown"},"source":{"6a132b02":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 28, 18\n\n%matplotlib inline\nplt.style.use('ggplot')\nimport dask.dataframe as dd\nimport gc\n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)    #THIS LINE IS MOST IMPORTANT AS THIS WILL DISPLAY PLOT ON \n#NOTEBOOK WHILE KERNEL IS RUNNING\nimport plotly.graph_objs as go              \nimport plotly.express as px                 \nimport plotly.offline as py     # \u7ed8\u56fe\u7684\u51fd\u6570","0f757176":"building = pd.read_csv('..\/input\/ashrae-energy-prediction\/building_metadata.csv')\nweather_train = pd.read_csv('..\/input\/ashrae-energy-prediction\/weather_train.csv')\nweather_test = pd.read_csv('..\/input\/ashrae-energy-prediction\/weather_test.csv')\ntrain_df = pd.read_csv('..\/input\/ashrae-energy-prediction\/train.csv')\ntest_df = pd.read_csv('..\/input\/ashrae-energy-prediction\/test.csv')","a212807a":"train_df = train_df.merge(building, on='building_id', how='left')\ntest_df = test_df.merge(building, on='building_id', how='left')\n\ntrain_df = train_df.merge(weather_train, on=['site_id', 'timestamp'], how='left')\ntest_df = test_df.merge(weather_test, on=['site_id', 'timestamp'], how='left')\n\ntrain_df['timestamp'] = pd.to_datetime(train_df[\"timestamp\"], format='%Y-%m-%d %H:%M:%S')\ntest_df['timestamp'] = pd.to_datetime(test_df[\"timestamp\"], format='%Y-%m-%d %H:%M:%S')\n\n\ndel weather_train, weather_test,building\ngc.collect();","4494e927":"def reduce_mem_usage(df, verbose=True):\n\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\n    # \u8ba1\u7b97\u5f53\u524d\u5360\u7528\u7684\u5185\u5b58 \n    start_mem = df.memory_usage(deep=True).sum() \/ 1024**2\n\n    # \u5faa\u73af\u6bcf\u4e00\u5217\n    for col in df.columns:\n\n        # \u83b7\u53d6\u6bcf\u4e00\u5217\u7684\u6570\u636e\u7c7b\u578b\n        col_type = df[col].dtypes\n\n        # \u5982\u679c\u6570\u636e\u7c7b\u578b\u5c5e\u4e8e\u4e0a\u9762\u5b9a\u4e49\u7684\u7c7b\u578b\u4e4b\n        if col_type in numerics:\n\n            # \u8ba1\u7b97\u8be5\u5217\u6570\u636e\u7684\u6700\u5c0f\u503c\u548c\u6700\u5927\u503c \u7528\u4e8e\u6211\u4eec\u6307\u5b9a\u76f8\u5e94\u7684\u6570\u636e\u7c7b\u578b \n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            # \u5982\u679c \u8be5\u5217\u7684\u6570\u636e\u7c7b\u578b\u5c5e\u4e8e int \u7c7b\u578b\uff0c\u7136\u540e\u8fdb\u884c\u5224\u65ad\n            if str(col_type)[:3] == 'int':\n                # \u5982\u679c \u8be5\u5217\u6700\u5c0f\u7684\u503c \u5927\u4e8eint8\u7c7b\u578b\u7684\u6700\u5c0f\u503c\uff0c\u5e76\u4e14\u6700\u5927\u503c\u5c0f\u4e8eint8\u7c7b\u578b\u7684\u6700\u5927\u503c\uff0c\u5219\u91c7\u7528int8\u7c7b\u578b \n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n\n                # \u540c\u4e0a\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n\n                # \u540c\u4e0a\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n\n                # \u540c\u4e0a\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n\n            # \u5426\u5219 \u5219\u91c7\u7528 float \u7684\u5904\u7406\u65b9\u6cd5       \n            else:\n\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage(deep=True).sum() \/ 1024**2\n\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","33249f93":"train_df['hour'] = train_df.timestamp.dt.hour\ntrain_df['day'] = train_df.timestamp.dt.day\ntrain_df['week'] = ((train_df.timestamp.dt.dayofweek) \/\/ 5 == 1).astype(float) \ntrain_df['month'] = train_df.timestamp.dt.month\n\ntest_df['hour'] = test_df.timestamp.dt.hour\ntest_df['day'] = test_df.timestamp.dt.day\ntest_df['week'] = ((test_df.timestamp.dt.dayofweek) \/\/ 5 == 1).astype(float) \ntest_df['month'] = test_df.timestamp.dt.month","dd041b84":"train_bd = np.round(train_df.memory_usage().sum()\/(1024*1024),1)\ntest_bd = np.round(test_df.memory_usage().sum()\/(1024*1024),1)","5c10de4f":"%%time\n\ntrain_df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)","12214a8e":"train_ad = np.round(train_df.memory_usage().sum()\/(1024*1024),1)\ntest_ad = np.round(test_df.memory_usage().sum()\/(1024*1024),1)","96f29987":"memory = pd.DataFrame({\n    'DataFrame':['train','test'],\n    'Before memory reducing':[train_bd,test_bd],\n    'After memory reducing':[train_ad,test_ad],\n\n})\n\nmemory = pd.melt(memory,id_vars='DataFrame',var_name='Status',value_name='Memory(MB)')\nmemory.sort_values('Memory(MB)',inplace=True)\nfig = px.bar(memory,x='DataFrame',y='Memory(MB)',color='Status',barmode='group',text='Memory(MB)')\nfig.update_traces(textposition='outside')\nfig.update_layout(template='seaborn',title='Effect of Downcasting')\nfig.show()","7f4a2a50":"train_df.head()","d9b3988a":"%%time\n\nstats = []\nfor col in train_df.columns:\n    stats.append((col, train_df[col].nunique(), train_df[col].isnull().sum() * 100 \/ train_df.shape[0], train_df[col].value_counts(normalize=True, dropna=False).values[0] * 100, train_df[col].dtype))\n\n# \u5217\u5206\u522b\u4e3a \u7279\u5f81\u540d\u3001\u8be5\u7279\u5f81\u72ec\u7acb\u503c\u6570\u91cf\u3001\u8be5\u7279\u5f81\u7f3a\u5931\u503c\u6bd4\u4f8b\u3001\u8be5\u7279\u5f81\u4e0b\u51fa\u73b0\u6700\u591a\u7684\u503c\u5360\u6240\u6709\u503c\u7684\u6bd4\u4f8b\u3001\u6570\u636e\u7c7b\u578b\nstats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values', 'Percentage of missing values', 'Percentage of values in the biggest category', 'type'])\nstats_df.sort_values('Percentage of missing values', ascending=False)","5f0defd7":"%%time\n\nstats = []\nfor col in test_df.columns:\n    stats.append((col, test_df[col].nunique(), test_df[col].isnull().sum() * 100 \/ test_df.shape[0], test_df[col].value_counts(normalize=True, dropna=False).values[0] * 100, test_df[col].dtype))\n\n# \u5217\u5206\u522b\u4e3a \u7279\u5f81\u540d\u3001\u8be5\u7279\u5f81\u72ec\u7acb\u503c\u6570\u91cf\u3001\u8be5\u7279\u5f81\u7f3a\u5931\u503c\u6bd4\u4f8b\u3001\u8be5\u7279\u5f81\u4e0b\u51fa\u73b0\u6700\u591a\u7684\u503c\u5360\u6240\u6709\u503c\u7684\u6bd4\u4f8b\u3001\u6570\u636e\u7c7b\u578b\nstats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values', 'Percentage of missing values', 'Percentage of values in the biggest category', 'type'])\nstats_df.sort_values('Percentage of missing values', ascending=False)","61a3397c":"train_df.drop(columns='floor_count',inplace=True)\ntest_df.drop(columns='floor_count',inplace=True)","c4f4435f":"category_features=['hour','day','week','month','primary_use','meter','site_id'] \ntrain_df[category_features] = train_df[category_features].astype('category')\ntest_df[category_features] = test_df[category_features].astype('category')\n\ntrain_df.info()","eb219edb":"# train_df.info()","46518f3d":"%%time\n\nstats = []\nfor col in train_df.columns:\n    stats.append((col, train_df[col].nunique(), train_df[col].isnull().sum() * 100 \/ train_df.shape[0], train_df[col].value_counts(normalize=True, dropna=False).values[0] * 100, train_df[col].dtype))\n\n# \u5217\u5206\u522b\u4e3a \u7279\u5f81\u540d\u3001\u8be5\u7279\u5f81\u72ec\u7acb\u503c\u6570\u91cf\u3001\u8be5\u7279\u5f81\u7f3a\u5931\u503c\u6bd4\u4f8b\u3001\u8be5\u7279\u5f81\u4e0b\u51fa\u73b0\u6700\u591a\u7684\u503c\u5360\u6240\u6709\u503c\u7684\u6bd4\u4f8b\u3001\u6570\u636e\u7c7b\u578b\nstats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values', 'Percentage of missing values', 'Percentage of values in the biggest category', 'type'])\nstats_df.sort_values('Percentage of missing values', ascending=False)","6bc3ada5":"test_df.head()","781b3d4a":"print('B Training Features shape: ', train_df.shape)\nprint('B Testing Features shape: ', test_df.shape)\n\ntrain_labels = train_df['meter_reading']\n\n# Align the training and testing data, keep only columns present in both dataframes \n# \u9009\u51fa\u4e24\u4e2a\u6570\u636e\u4e2d\u5171\u540c\u62e5\u6709\u7684\u5217\uff0c\u4e0d\u5171\u6709\u7684\u5217\u8fc7\u6ee4\u51fa\u53bb\ntrain_df, test_df = train_df.align(test_df, join = 'inner', axis = 1)\n\n# Add the target back in\ntrain_df['meter_reading'] = train_labels\n\nprint('A Training Features shape: ', train_df.shape)\nprint('A Testing Features shape: ', test_df.shape)","132ba8d8":"# Density plots of features function\ndef plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n    \n    failed_features = []\n    \n    for feature in features:\n        try:\n            i += 1\n            plt.subplot(5,2,i)\n            sns.distplot(df1[feature], hist=False,label=label1)\n            sns.distplot(df2[feature], hist=False,label=label2)\n            plt.xlabel(feature, fontsize=9)\n            locs, labels = plt.xticks()\n            plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n            plt.tick_params(axis='y', which='major', labelsize=6)\n        except:\n            print(feature + 'KDE failed')\n            failed_features.append(feature)\n            continue\n    plt.show();\n    \n    return failed_features","0d9520e5":"sns.set_style('darkgrid')","19079e79":"# plotly func \n\n# def plot_categorical_feature(col,top_n=10000,train=train_df):\n    \n#     top_n = top_n if train[col].nunique() > top_n else train[col].nunique()\n#     print(f\"{col} has {train[col].nunique()} unique values and type: {train[col].dtype}.\")\n#     print(train[col].value_counts(normalize=True, dropna=False).head())\n    \n#     top_cat = list(train[col].value_counts(dropna=False).index[:top_n])\n    \n#     df = train.groupby([col]).agg({'meter_reading': ['count','mean']})\n#     df = df.sort_values(('meter_reading', 'count'), ascending=False).head(top_n).sort_index()\n    \n#     data = [go.Bar(x=df.index, y=df['meter_reading']['count'], name='counts'),\n#             go.Bar(x=df.index, y=df['meter_reading']['count'], name='counts'),\n#             go.Scatter(x=df.index, y=df['meter_reading']['mean'], name='meter_reading mean', yaxis='y2')]\n\n    \n    \n#     layout = go.Layout(dict(title = f\"Counts of {col} by top-{top_n} categories\",\n#                             xaxis = dict(title = f'{col}',\n#                                     showgrid=False,\n#                                     zeroline=False,\n#                                     showline=False,),\n#                             yaxis = dict(title = 'Counts',\n#                                     showgrid=False,\n#                                     zeroline=False,\n#                                     showline=False,),\n#                             yaxis2=dict(title='meter_reading mean', \n#                                         overlaying='y', \n#                                         side='right')\n#                             ),\n#                     legend=dict(orientation=\"v\"), barmode='group')\n\n#     py.iplot(dict(data=data, layout=layout))","825e3e28":"[col for col in train_df.columns if col != 'meter_reading' and col not in category_features and col != 'timestamp']","3bd60b17":"features = [col for col in train_df.columns if col != 'meter_reading' and col not in category_features and col != 'timestamp']\nplot_feature_distribution(train_df, test_df, 'train', 'test', features)","bfcfa272":"train_df['precip_depth_1_hr'].hist(figsize=(12,8))","9ca554a9":"fig, ax = plt.subplots(figsize = (16, 6))\nplt.subplot(1, 2, 1)\nplt.hist(train_df['meter_reading']);\nplt.title('Distribution of meter_reading');\nplt.subplot(1, 2, 2)\nplt.hist(np.log1p(train_df['meter_reading']));\nplt.title('Distribution of log of meter_reading');","d20627a0":"# plot_categorical_feature('meter')","845b18b1":"def Classification_features_and_continuous_targets_func(col,target='meter_reading',rotation=0,loc=2):\n    \n    # \u8bad\u7ec3\u6570\u636e\n    train_data = train_df[col].value_counts(dropna=False, normalize=True).sort_index().values\n    ind = np.arange(len(train_data))\n    width = 0.35\n\n    fig, axes = plt.subplots(1,1,figsize=(14, 6), dpi=100)\n    tr = axes.bar(ind, train_data, width, color='royalblue')\n    \n    # \u6d4b\u8bd5\u6570\u636e\n    test_data = test_df[col].value_counts(dropna=False, normalize=True).sort_index().values\n    tt = axes.bar(ind+width, test_data, width, color='seagreen')\n\n    axes.set_ylabel('Normalized number of observations');\n    axes.set_xlabel(col);\n    axes.set_xticks(ind + width \/ 2)\n    axes.set_xticklabels(train_df[col].value_counts().sort_index().index, rotation=rotation)\n    axes2 = axes.twinx()\n    mr = axes2.plot(ind, train_df[[col, target]].groupby(col)[target].mean().sort_index().values, 'D-', color='tab:orange', label='Mean {}'.format(target));\n    axes2.grid(False);\n    axes2.tick_params(axis='y', labelcolor='tab:orange');\n    axes2.set_ylabel('Mean {} by {}'.format(target,col), color='tab:orange');\n    axes.legend([tr, tt], ['Train', 'Test'], facecolor='white');\n    axes2.legend(loc=loc, facecolor='white');","8c1eb617":"Classification_features_and_continuous_targets_func('meter','meter_reading')","487ce1b3":"plt.figure(figsize=(13,6)) #figure size\n\n#It's another way to plot our data. using a variable that contains the plot parameters\ng1 = sns.boxplot(x='meter', y='meter_reading', \n                   data=train_df[(train_df['meter'].isin((train_df['meter'].value_counts()[:10].index.values))) &\n                                  train_df['meter_reading'] > 0]\n                   ,showfliers=False)\ng1.set_title('Meter by meter_reading', fontsize=20) # title and fontsize\ng1.set_xticklabels(g1.get_xticklabels(),rotation=45) # It's the way to rotate the xticks when we use variable to our graphs\ng1.set_xlabel('Meter', fontsize=18) # Xlabel\ng1.set_ylabel('Trans meter_reading(log) Dist', fontsize=18) #Ylabel\n\nplt.show()","08432a2b":"Classification_features_and_continuous_targets_func('site_id',rotation=0)","7c0b787d":"train_data = train_df['cloud_coverage'].value_counts(normalize=True).sort_index().values\nind = np.arange(len(train_data))\nwidth = 0.35\n\nfig, axes = plt.subplots(1,1,figsize=(14, 6), dpi=100)\ntr = axes.bar(ind, train_data, width, color='royalblue')\n\n# \u6d4b\u8bd5\u6570\u636e\ntest_data = test_df['cloud_coverage'].value_counts(normalize=True).sort_index().values\ntt = axes.bar(ind+width, test_data, width, color='seagreen')\n\naxes.set_ylabel('Normalized number of observations');\naxes.set_xlabel('cloud_coverage');\naxes.set_xticks(ind + width \/ 2)\naxes.set_xticklabels(train_df['cloud_coverage'].value_counts().sort_index().index, rotation=0)\naxes2 = axes.twinx()\nmr = axes2.plot(ind, train_df[['cloud_coverage', 'meter_reading']].groupby('cloud_coverage')['meter_reading'].mean().sort_index().values, 'D-', color='tab:orange', label='Mean {}'.format('meter_reading'));\naxes2.grid(False);\naxes2.tick_params(axis='y', labelcolor='tab:orange');\naxes2.set_ylabel('Mean {} by {}'.format('meter_reading','cloud_coverage'), color='tab:orange');\naxes.legend([tr, tt], ['Train', 'Test'], facecolor='white');\naxes2.legend(loc=2, facecolor='white');","9a16b06a":"# plot_categorical_feature('primary_use')","4d2f7928":"Classification_features_and_continuous_targets_func('primary_use',rotation=90,loc=5)","c0ea2c90":"Classification_features_and_continuous_targets_func('week',rotation=0)","d88a031b":"Classification_features_and_continuous_targets_func('month',rotation=0)","d075b12d":"# plot_categorical_feature('hour')","90d88b0d":"# plot_categorical_feature('day')","8e0a5d41":"# plot_categorical_feature('week')","5c08b4b6":"# plot_categorical_feature('month')","64e75c5a":"# Calling the function to transform the date column in datetime pandas object\n\n# \u8bbe\u7f6e\u4e00\u4e9b\u9759\u6001\u989c\u8272\u9009\u9879\ncolor_op = ['#5527A0', '#BB93D7', '#834CF7', '#6C941E', '#93EAEA', '#7425FF', '#F2098A', '#7E87AC', \n        '#EBE36F', '#7FD394', '#49C35D', '#3058EE', '#44FDCF', '#A38F85', '#C4CEE0', '#B63A05', \n        '#4856BF', '#F0DB1B', '#9FDBD9', '#B123AC']\n\ndates_temp = train_df.groupby(train_df.timestamp.dt.date)['meter_reading'].mean().reset_index()\n\n# \u4f7f\u7528\u7b2c\u4e00\u4e2a\u5fc5\u8981\u7684 trace \u53c2\u6570\uff0copacity \u8868\u793a\u900f\u660e\u5ea6\ntrace = go.Scatter(x=dates_temp['timestamp'], y=dates_temp.meter_reading,\n                opacity = 0.8, line = dict(color = color_op[7]), name= 'Meter_reading mean')\n\n# \u4e0b\u9762\u6211\u4eec\u5c06\u5f97\u5230\u9500\u552e\u603b\u989d (\u5177\u4f53\u6848\u4f8b\u5177\u4f53\u5206\u6790)\ndates_temp_sum = train_df.groupby(train_df.timestamp.dt.date)['meter_reading'].sum().reset_index()\n\n# \u4f7f\u7528\u65b0\u7684 dates_temp_sum \uff0c\u6211\u4eec\u5c06\u521b\u5efa\u7b2c\u4e8c\u4e2a trace \ntrace1 = go.Scatter(x=dates_temp_sum.timestamp, line = dict(color = color_op[1]), name=\"Total Amount\",\n                        y=dates_temp_sum['meter_reading'], opacity = 0.8, yaxis='y2')\n\n# \u521b\u5efa\u5e03\u5c40\u5c06\u5141\u8bb8\u6211\u4eec\u7ed9\u51fa\u6807\u9898\u548c\u7ed9\u6211\u4eec\u4e00\u4e9b\u6709\u8da3\u7684\u9009\u9879\u6765\u5904\u7406\u56fe\u7684\u8f93\u51fa\nlayout = dict(\n        title= \"Total meter_reading Informations by Date\",\n        xaxis=dict(\n                rangeselector=dict(\n                buttons=list([\n                        dict(count=1, label='1m', step='month', stepmode='backward'),\n                        dict(count=3, label='3m', step='month', stepmode='backward'),\n                        dict(count=6, label='6m', step='month', stepmode='backward'),\n                        dict(step='all')\n                ])\n                ),\n                rangeslider=dict(visible = True), # \u63a7\u5236\u6700\u4e0b\u9762\u7684\u56fe\n                type='date' ),\n        yaxis=dict(title='Total meter_reading'), # \u5de6\u4fa7\u7684y\u8f74\n        yaxis2=dict(overlaying='y',             # \u53f3\u4fa7\u7684y\u8f74 side ='right'\n                        anchor='x', side='right',\n                        zeroline=False, showgrid=False,\n                        title='Total meter_reading Amount')\n        )\n\n# \u521b\u5efa\u65e2\u6709 trace \u53c8\u6709 layout \u7684\u56fe\u5f62\nfig = dict(data= [trace, trace1,], layout=layout)\n\n#\u7ed8\u5236\u56fe\u5f62\niplot(fig) #it's an equivalent to plt.show()","9ed982b6":"meter_reading_sum = train_df[['timestamp','meter_reading']].groupby(['timestamp']).sum()\nmeter_reading_sum.head()\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nweeks_per_year = 365\n\n\ntime_series = meter_reading_sum['meter_reading']\nsj_sc = seasonal_decompose(time_series, freq= weeks_per_year)\nsj_sc.plot()\n\nplt.show()","e157cbc0":"# fig, axes = plt.subplots(8,2,figsize=(14, 30), dpi=100)\n# for i in range(train_df['site_id'].nunique()):\n#     train_df[train_df['site_id'] == i][['timestamp', 'sea_level_pressure']].set_index('timestamp').resample('H').mean()['sea_level_pressure'].plot(ax=axes[i%8][i\/\/8], alpha=0.8, label='By hour', color='tab:blue').set_ylabel('Mean sea_level_pressure', fontsize=13);\n#     test_df[test_df['site_id'] == i][['timestamp', 'sea_level_pressure']].set_index('timestamp').resample('H').mean()['sea_level_pressure'].plot(ax=axes[i%8][i\/\/8], alpha=0.8, color='tab:blue', label='').set_xlabel('')\n#     train_df[train_df['site_id'] == i][['timestamp', 'sea_level_pressure']].set_index('timestamp').resample('D').mean()['sea_level_pressure'].plot(ax=axes[i%8][i\/\/8], alpha=1, label='By day', color='tab:orange')\n#     test_df[test_df['site_id'] == i][['timestamp', 'sea_level_pressure']].set_index('timestamp').resample('D').mean()['sea_level_pressure'].plot(ax=axes[i%8][i\/\/8], alpha=1, color='tab:orange', label='').set_xlabel('')\n#     axes[i%8][i\/\/8].legend();\n#     axes[i%8][i\/\/8].set_title('site_id {}'.format(i), fontsize=13);\n#     axes[i%8][i\/\/8].axvspan(test_df['timestamp'].min(), test_df['timestamp'].max(), facecolor='green', alpha=0.2);\n#     plt.subplots_adjust(hspace=0.45)","c3cd3237":"fig, axes = plt.subplots(1,1,figsize=(14, 6))\ntrain_df[['timestamp', 'precip_depth_1_hr']].set_index('timestamp').resample('M').mean()['precip_depth_1_hr'].plot(ax=axes, alpha=0.8, label='By month', color='tab:blue').set_ylabel('Mean precip_depth_1_hr', fontsize=14);\ntest_df[['timestamp', 'precip_depth_1_hr']].set_index('timestamp').resample('M').mean()['precip_depth_1_hr'].plot(ax=axes, alpha=0.8, color='tab:blue', label='');\naxes.legend();","56fc6fa3":"fig, axes = plt.subplots(1,1,figsize=(14, 6))\ntrain_df.groupby('precip_depth_1_hr')['meter_reading'].mean().plot().set_ylabel('Mean meter reading');\naxes.set_title('Mean meter reading by precip_depth_1_hr of the building', fontsize=16);","5f8a5b06":"fig, axes = plt.subplots(2, 2, figsize=(10, 8))\nsns.kdeplot(train_df['year_built'], ax=axes[0][0], label='Train');\nsns.kdeplot(test_df['year_built'], ax=axes[0][0], label='Test');\nsns.violinplot(x=train_df['year_built'], ax=axes[1][0]);\nsns.violinplot(x=test_df['year_built'], ax=axes[1][1]);\npd.DataFrame({'train': [train_df['year_built'].isnull().sum()], 'test': [test_df['year_built'].isnull().sum()]},index=['year_built']).plot(kind='bar', rot=0, ax=axes[0][1]);\naxes[0][0].legend();\naxes[0][0].set_title('Train\/Test KDE distribution',fontsize=10);\naxes[0][1].set_title('Number of NaNs',fontsize=10);\naxes[1][0].set_title('violinplot for train',fontsize=10);\naxes[1][1].set_title('violinplot for test',fontsize=10);\ngc.collect();","43ec7ffa":"fig, axes = plt.subplots(1,1,figsize=(14, 6))\ntrain_df.groupby('year_built')['meter_reading'].mean().plot().set_ylabel('Mean meter reading');\naxes.set_title('Mean meter reading by year_built of the building', fontsize=16);","10b075c6":"fig, axes = plt.subplots(1,1,figsize=(14, 6))\ntrain_df.groupby('building_id')['meter_reading'].mean().plot().set_ylabel('Mean meter reading');\naxes.set_title('Mean meter reading by building_id of the building', fontsize=16);","e9faf27a":"train_df[(train_df['meter_reading'] > 2500000)]['building_id'].unique()","77b7cdd8":"fig, axes = plt.subplots(1,1,figsize=(14, 6))\ntrain_df[(train_df['primary_use'] == 'Education')].groupby('building_id')['meter_reading'].mean().plot().set_ylabel('Mean meter reading');\naxes.set_title('Mean meter reading by primary_use == Education  of the building', fontsize=16);","b3f1a301":"fig, axes = plt.subplots(1,1,figsize=(14, 6))\ntrain_df[train_df['building_id'] != 1099].groupby('building_id')['meter_reading'].mean().plot();\naxes.set_title('Mean meter reading by building_id', fontsize=14);\naxes.set_ylabel('Mean meter reading', fontsize=14)","6bbc9201":"fig, axes = plt.subplots(1,1,figsize=(14, 6))\ntrain_df[train_df['building_id'] != 1099].groupby('square_feet')['meter_reading'].mean().plot();\naxes.set_title('Mean meter reading by square_feet of the building', fontsize=16);","a020ada4":"fig, axes = plt.subplots(1,1,figsize=(14, 6))\ntrain_df.groupby('wind_speed')['meter_reading'].mean().plot().set_ylabel('Mean meter reading');\naxes.set_title('Mean meter reading by wind_speed of the building', fontsize=16);","a2760e3f":"fig, axes = plt.subplots(1,1,figsize=(14, 6))\ntrain_df.groupby('air_temperature')['meter_reading'].mean().plot().set_ylabel('Mean meter reading');\naxes.set_title('Mean meter reading by air_temperature of the building', fontsize=16);","a2e3919e":"### air_temperature:","78f7800f":"There is an obvious outlier in data. building id == 1099\n\n\u80fd\u770b\u5230\u5728 primary_use == Education \u7684\u6570\u636e\u4e2d\uff0cbuilding_id = 1099 \u7684\u6570\u636e\u597d\u50cf\u5b58\u5728\u5f02\u5e38","5aec4bdb":"Mean meter reading by building_id WITHOUT building_id 1099 \n\n\u8fd9\u91cc\u6392\u9664  building_id 1099 \u7684\u6570\u636e\u518d\u6765\u770b\u4e00\u4e0b\uff1a","8d2a1c43":"`floor_count` have a lot of missing values for count \uff0cthat seem to say nothing","030d6b13":"### meter:","b3d93d4f":"### timestamp:","20847e68":"we can see that the distribution of training data and test data is basically the same","98cd31f7":"### <center>Continuous data\uff1a","9c6db34f":"### meter_reading","2f30b9fc":"education and services meter_reading is very big","a42fd8a6":"### year_built:","bbf4dbda":"### site_id:","d721ff4a":"\u5728\u56fe\u4e2d\u6211\u4eec\u4f3c\u4e4e\u6bd4\u8f83\u96be\u76f4\u63a5\u53d1\u73b0 square_feet \u548c meter_reading\u4e4b\u95f4\u7684\u5173\u7cfb","03f41b69":"### look at train and test dist","5db191a2":"### <center> Data Merge","b82a9339":"### <center> Import","9bf9391d":"### wind_speed:","5e9196aa":"### square_feet:","63e40514":"### building_id","4f00982a":"### <center> Data info","544a00fe":"### \u7b5b\u9009\u51fa train \u548c test \u5171\u540c\u5217","2fbb6d17":"### precip_depth_1_hr","e9fffe69":"As shown in the figure \uff1a may be have outliers in the data\uff0cLet's explore further","0e9647a2":"### cloud_coverage:","f4b3e29e":"### <center>Reduce Memory","d9b2a1b7":"## <center> Visualization and Data Processing","bfff1ed5":"## <center> Import and Glimpse of Data","525505f1":"### <center> Plot Memory Reduce ","143332a1":"### <center>Classified data\uff1a","8a8c74af":"### timestamp\uff1a","5b1baf6b":"### <center> Read Data","aadefd19":"### year_built\uff1a","d6cb1c47":"### primary_use:"}}