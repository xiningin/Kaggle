{"cell_type":{"d045ede8":"code","540bf0f2":"code","0aa26ae5":"code","d29475a0":"code","53a4e5c0":"code","b0be6918":"code","359adb70":"code","17374fd1":"code","45861433":"code","b3cf90d7":"code","84524039":"code","e85d46f0":"code","7fdd4c84":"code","9a50deaf":"code","1965d154":"code","2e8baaf2":"code","442363a6":"code","49e1b918":"code","0887e4e8":"code","014b145f":"code","7951ee12":"code","75de5ca2":"code","251199d0":"code","ff1b97f5":"code","9fabb47b":"code","7aac2593":"code","ce28ca67":"code","fdd198c0":"code","2f11d31f":"code","1bf34c69":"code","df3187c1":"code","3344557f":"code","85d3ac21":"code","59f78684":"code","dd237550":"code","c7fa4a84":"code","684f32b3":"code","678854ff":"code","fa5c9661":"markdown","c5071d9d":"markdown","2ce5a454":"markdown","df61adb5":"markdown","5c3f84e8":"markdown","3bac2877":"markdown","06b613c6":"markdown","7f0e9b19":"markdown","178d246e":"markdown","7c2b4f2e":"markdown","63e3d6a1":"markdown","52e759aa":"markdown","55118501":"markdown","271b1944":"markdown","20aad04b":"markdown","4ad5bf93":"markdown","4e08b8ba":"markdown","703da038":"markdown","6d8b02d8":"markdown","31d14026":"markdown","3460671c":"markdown","cd882187":"markdown","7a85e21b":"markdown","b3f680ee":"markdown","0ffac1de":"markdown","bfa02002":"markdown","64ea40f1":"markdown","f4eb351d":"markdown","dd2259bd":"markdown"},"source":{"d045ede8":"import math\nimport os\nimport shutil\nimport sys\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport glob\nimport pydicom\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm","540bf0f2":"random_stat = 123\nnp.random.seed(random_stat)","0aa26ae5":"!git clone https:\/\/github.com\/pjreddie\/darknet.git\n\n# Build gpu version darknet\n!cd darknet && sed '1 s\/^.*$\/GPU=1\/; 2 s\/^.*$\/CUDNN=1\/' -i Makefile\n\n# -j <The # of cpu cores to use>. Chang 999 to fit your environment. Actually i used '-j 50'.\n!cd darknet && make -j 999 -s\n!cp darknet\/darknet darknet_gpu","d29475a0":"DATA_DIR = \"..\/input\"\n\ntrain_dcm_dir = os.path.join(DATA_DIR, \"stage_1_train_images\")\ntest_dcm_dir = os.path.join(DATA_DIR, \"stage_1_test_images\")\n\nimg_dir = os.path.join(os.getcwd(), \"images\")  # .jpg\nlabel_dir = os.path.join(os.getcwd(), \"labels\")  # .txt\nmetadata_dir = os.path.join(os.getcwd(), \"metadata\") # .txt\n\n# YOLOv3 config file directory\ncfg_dir = os.path.join(os.getcwd(), \"cfg\")\n# YOLOv3 training checkpoints will be saved here\nbackup_dir = os.path.join(os.getcwd(), \"backup\")\n\nfor directory in [img_dir, label_dir, metadata_dir, cfg_dir, backup_dir]:\n    if os.path.isdir(directory):\n        continue\n    os.mkdir(directory)","53a4e5c0":"!ls -shtl","b0be6918":"annots = pd.read_csv(os.path.join(DATA_DIR, \"stage_1_train_labels.csv\"))\nannots.head()","359adb70":"def save_img_from_dcm(dcm_dir, img_dir, patient_id):\n    img_fp = os.path.join(img_dir, \"{}.jpg\".format(patient_id))\n    if os.path.exists(img_fp):\n        return\n    dcm_fp = os.path.join(dcm_dir, \"{}.dcm\".format(patient_id))\n    img_1ch = pydicom.read_file(dcm_fp).pixel_array\n    img_3ch = np.stack([img_1ch]*3, -1)\n\n    img_fp = os.path.join(img_dir, \"{}.jpg\".format(patient_id))\n    cv2.imwrite(img_fp, img_3ch)\n    \ndef save_label_from_dcm(label_dir, patient_id, row=None):\n    # rsna defualt image size\n    img_size = 1024\n    label_fp = os.path.join(label_dir, \"{}.txt\".format(patient_id))\n    \n    f = open(label_fp, \"a\")\n    if row is None:\n        f.close()\n        return\n\n    top_left_x = row[1]\n    top_left_y = row[2]\n    w = row[3]\n    h = row[4]\n    \n    # 'r' means relative. 'c' means center.\n    rx = top_left_x\/img_size\n    ry = top_left_y\/img_size\n    rw = w\/img_size\n    rh = h\/img_size\n    rcx = rx+rw\/2\n    rcy = ry+rh\/2\n    \n    line = \"{} {} {} {} {}\\n\".format(0, rcx, rcy, rw, rh)\n    \n    f.write(line)\n    f.close()\n        \ndef save_yolov3_data_from_rsna(dcm_dir, img_dir, label_dir, annots):\n    for row in tqdm(annots.values):\n        patient_id = row[0]\n\n        img_fp = os.path.join(img_dir, \"{}.jpg\".format(patient_id))\n        if os.path.exists(img_fp):\n            save_label_from_dcm(label_dir, patient_id, row)\n            continue\n\n        target = row[5]\n        # Since kaggle kernel have samll volume (5GB ?), I didn't contain files with no bbox here.\n        if target == 0:\n            continue\n        save_label_from_dcm(label_dir, patient_id, row)\n        save_img_from_dcm(dcm_dir, img_dir, patient_id)","17374fd1":"save_yolov3_data_from_rsna(train_dcm_dir, img_dir, label_dir, annots)","45861433":"!du -sh images labels","b3cf90d7":"ex_patient_id = annots[annots.Target == 1].patientId.values[0]\nex_img_path = os.path.join(img_dir, \"{}.jpg\".format(ex_patient_id))\nex_label_path = os.path.join(label_dir, \"{}.txt\".format(ex_patient_id))\n\nplt.imshow(cv2.imread(ex_img_path))\n\nimg_size = 1014\nwith open(ex_label_path, \"r\") as f:\n    for line in f:\n        print(line)\n        class_id, rcx, rcy, rw, rh = list(map(float, line.strip().split()))\n        x = (rcx-rw\/2)*img_size\n        y = (rcy-rh\/2)*img_size\n        w = rw*img_size\n        h = rh*img_size\n        plt.plot([x, x, x+w, x+w, x], [y, y+h, y+h, y, y])","84524039":"def write_train_list(metadata_dir, img_dir, name, series):\n    list_fp = os.path.join(metadata_dir, name)\n    with open(list_fp, \"w\") as f:\n        for patient_id in series:\n            line = \"{}\\n\".format(os.path.join(img_dir, \"{}.jpg\".format(patient_id)))\n            f.write(line)","e85d46f0":"# Following lines do not contain data with no bbox\npatient_id_series = annots[annots.Target == 1].patientId.drop_duplicates()\n\ntr_series, val_series = train_test_split(patient_id_series, test_size=0.1, random_state=random_stat)\nprint(\"The # of train set: {}, The # of validation set: {}\".format(tr_series.shape[0], val_series.shape[0]))\n\n# train image path list\nwrite_train_list(metadata_dir, img_dir, \"tr_list.txt\", tr_series)\n# validation image path list\nwrite_train_list(metadata_dir, img_dir, \"val_list.txt\", val_series)","7fdd4c84":"def save_yolov3_test_data(test_dcm_dir, img_dir, metadata_dir, name, series):\n    list_fp = os.path.join(metadata_dir, name)\n    with open(list_fp, \"w\") as f:\n        for patient_id in series:\n            save_img_from_dcm(test_dcm_dir, img_dir, patient_id)\n            line = \"{}\\n\".format(os.path.join(img_dir, \"{}.jpg\".format(patient_id)))\n            f.write(line)","9a50deaf":"test_dcm_fps = list(set(glob.glob(os.path.join(test_dcm_dir, '*.dcm'))))\ntest_dcm_fps = pd.Series(test_dcm_fps).apply(lambda dcm_fp: dcm_fp.strip().split(\"\/\")[-1].replace(\".dcm\",\"\"))\n\nsave_yolov3_test_data(test_dcm_dir, img_dir, metadata_dir, \"te_list.txt\", test_dcm_fps)","1965d154":"ex_patient_id = test_dcm_fps[0]\nex_img_path = os.path.join(img_dir, \"{}.jpg\".format(ex_patient_id))\n\nplt.imshow(cv2.imread(ex_img_path))","2e8baaf2":"data_extention_file_path = os.path.join(cfg_dir, 'rsna.data')\nwith open(data_extention_file_path, 'w') as f:\n    contents = \"\"\"classes= 1\ntrain  = {}\nvalid  = {}\nnames  = {}\nbackup = {}\n    \"\"\".format(os.path.join(metadata_dir, \"tr_list.txt\"),\n               os.path.join(metadata_dir, \"val_list.txt\"),\n               os.path.join(cfg_dir, 'rsna.names'),\n               backup_dir)\n    f.write(contents)","442363a6":"!cat cfg\/rsna.data","49e1b918":"# Label list of bounding box.\n!echo \"pneumonia\" > cfg\/rsna.names","0887e4e8":"!wget -q https:\/\/pjreddie.com\/media\/files\/darknet53.conv.74","014b145f":"!wget --no-check-certificate -q \"https:\/\/docs.google.com\/uc?export=download&id=18ptTK4Vbeokqpux8Onr0OmwUP9ipmcYO\" -O cfg\/rsna_yolov3.cfg_train","7951ee12":"# !.\/darknet_gpu detector train cfg\/rsna.data cfg\/rsna_yolov3.cfg_train darknet53.conv.74 -i 0 | tee train_log.txt","75de5ca2":"# !.\/darknet_gpu detector train cfg\/rsna.data cfg\/rsna_yolov3.cfg_train backup\/rsna_yolov3_1000.weights -gpus 0,1,2,3 | tee train_log.txt","251199d0":"!wget --no-check-certificate -q \"https:\/\/docs.google.com\/uc?export=download&id=1OhnlV3s7r6xsEme6DKkNYjcYjsl-C_Av\" -O train_log.txt","ff1b97f5":"iters = []\nlosses = []\ntotal_losses = []\nwith open(\"train_log.txt\", 'r') as f:\n    for i,line in enumerate(f):\n        if \"images\" in line:\n            iters.append(int(line.strip().split()[0].split(\":\")[0]))\n            losses.append(float(line.strip().split()[2]))        \n            total_losses.append(float(line.strip().split()[1].split(',')[0]))\n\nplt.figure(figsize=(20, 5))\nplt.subplot(1,2,1)\nsns.lineplot(iters, total_losses, label=\"totla loss\")\nsns.lineplot(iters, losses, label=\"avg loss\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\n\nplt.subplot(1,2,2)\nsns.lineplot(iters, total_losses, label=\"totla loss\")\nsns.lineplot(iters, losses, label=\"avg loss\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.ylim([0, 4.05])","9fabb47b":"ex_patient_id = annots[annots.Target == 1].patientId.values[2]\nshutil.copy(ex_img_path, \"test.jpg\")\nprint(ex_patient_id)","7aac2593":"!wget --load-cookies \/tmp\/cookies.txt -q \"https:\/\/docs.google.com\/uc?export=download&confirm=$(wget --quiet --save-cookies \/tmp\/cookies.txt --keep-session-cookies --no-check-certificate 'https:\/\/docs.google.com\/uc?export=download&id=1FDzMN-kGVYCvBeDKwemAazldSVkAEFyd' -O- | sed -rn 's\/.*confirm=([0-9A-Za-z_]+).*\/\\1\\n\/p')&id=1FDzMN-kGVYCvBeDKwemAazldSVkAEFyd\" -O backup\/rsna_yolov3_15300.weights && rm -rf \/tmp\/cookies.txt","ce28ca67":"!ls -alsth backup","fdd198c0":"!wget --no-check-certificate -q \"https:\/\/docs.google.com\/uc?export=download&id=10Yk6ZMAKGz5LeBbikciALy82aK3lX-57\" -O cfg\/rsna_yolov3.cfg_test","2f11d31f":"!cd darknet && .\/darknet detector test ..\/cfg\/rsna.data ..\/cfg\/rsna_yolov3.cfg_test ..\/backup\/rsna_yolov3_15300.weights ..\/test.jpg -thresh 0.005","1bf34c69":"# ![](predictions.jpg)\nplt.imshow(cv2.imread(\".\/darknet\/predictions.jpg\"))","df3187c1":"!wget --no-check-certificate -q \"https:\/\/docs.google.com\/uc?export=download&id=1-KTV7K9G1bl3SmnLnzmpkDyNt6tDmH7j\" -O darknet.py","3344557f":"from darknet import *","85d3ac21":"threshold = 0.2","59f78684":"submit_file_path = \"submission.csv\"\ncfg_path = os.path.join(cfg_dir, \"rsna_yolov3.cfg_test\")\nweight_path = os.path.join(backup_dir, \"rsna_yolov3_15300.weights\")\n\ntest_img_list_path = os.path.join(metadata_dir, \"te_list.txt\")","dd237550":"gpu_index = 0\nnet = load_net(cfg_path.encode(),\n               weight_path.encode(), \n               gpu_index)\nmeta = load_meta(data_extention_file_path.encode())","c7fa4a84":"submit_dict = {\"patientId\": [], \"PredictionString\": []}\n\nwith open(test_img_list_path, \"r\") as test_img_list_f:\n    # tqdm run up to 1000(The # of test set)\n    for line in tqdm(test_img_list_f):\n        patient_id = line.strip().split('\/')[-1].strip().split('.')[0]\n\n        infer_result = detect(net, meta, line.strip().encode(), thresh=threshold)\n\n        submit_line = \"\"\n        for e in infer_result:\n            confi = e[1]\n            w = e[2][2]\n            h = e[2][3]\n            x = e[2][0]-w\/2\n            y = e[2][1]-h\/2\n            submit_line += \"{} {} {} {} {} \".format(confi, x, y, w, h)\n\n        submit_dict[\"patientId\"].append(patient_id)\n        submit_dict[\"PredictionString\"].append(submit_line)\n\npd.DataFrame(submit_dict).to_csv(submit_file_path, index=False)","684f32b3":"# !ls -lsht\n!rm -rf darknet images labels metadata backup cfg\n!rm -rf train_log.txt darknet53.conv.74 darknet.py darknet_gpu\n!rm -rf test.jpg\n!rm -rf __pycache__ .ipynb_checkpoints","678854ff":"!ls -alsht","fa5c9661":"## 1. Clone and Build YOLOv3","c5071d9d":"## Contents\n### YOLO v3 for image detection\n0. <a href='#0.-Introduction'>Introduction<\/a>\n1. <a href='#1.-Clone-and-Build-YOLOv3'>Clone and Build YOLOv3<\/a>\n2. <a href='#2.-Data-Migration-for-YOLOv3'>Data Migration<\/a>\n3. <a href='#3.-Prepare-Configuration-Files-for-Using-YOLOv3'>Prepare Configuration Files for training<\/a>\n4. <a href='#4.-Training-YOLOv3'>Training model<\/a>\n5. <a href='#5.-How-to-use-trainined-YOLOv3-for-test-images-(command-line)'>How to use trained model for test images (command line)<\/a>\n6. <a href='#6.-Generate-Submission-Files-with-YOLOv3-Python-Wrapper'>Generate Submission Files Using YOLOv3 Python Wrapper<\/a>\n7. <a href='#7.-Future-works-&-Etc'>Future works & Etc<\/a>","2ce5a454":"### 5.2. cfg file for test (not for training)","df61adb5":"### - cfg\/rsna.names","5c3f84e8":"## 4. Training YOLOv3","3bac2877":"### 2.0. Make subdirectories","06b613c6":"### 2.3. Plot a sample train image and label","7f0e9b19":"### 2.4. Generate train\/val file path list (.txt)\n* We should give the list of image paths to YOLO. two seperate list textfiles for training images and validation images.","178d246e":"## 5. How to use trainined YOLOv3 for test images (command line)","7c2b4f2e":"### - cfg\/rsna_yolov3.cfg_train\n* Basically, you can use darknet\/cfg\/yolov3.cfg files. However it won't work for RSNA. you need to edit for RSNA.\n* You can just download a cfg file I edited for RSNA with following wget command.\n\n\n* I refer to the following articles for editing cfg files.\n  * [YOLOv3 blog](https:\/\/pjreddie.com\/darknet\/yolo\/)\n  * [YOLOv3 paper](https:\/\/pjreddie.com\/media\/files\/papers\/YOLOv3.pdf)\n  * [how to train yolov2 blog](https:\/\/medium.com\/@manivannan_data\/how-to-train-yolov2-to-detect-custom-objects-9010df784f36)\n  * [darknet github issues\/236](https:\/\/github.com\/pjreddie\/darknet\/issues\/236)","63e3d6a1":"### - cfg\/rsna.data\nThis file point to RSNA data path\n  * train: Path to training image list textfile\n  * val: Path to validation image list textfile\n  * names: RSNA class name list (see <a href='#3.1.-cfg\/rsna.names'>3.1<\/a>)\n  * backup: A directory where trained weights(checkpoints) will be stored as training progresses.","52e759aa":"## 3. Prepare Configuration Files for Using YOLOv3\nWe should prepare and modify config files, and bring pre-trained weights necessary for training. This proceeds with following four steps.\n```\n cfg\/rsna.data\n cfg\/rsna.names\n darknet53.conv.74\n cfg\/rsna_yolov3.cfg_train\n```","55118501":"## 0. Introduction\n* I'll introduce super easy and quick way to train [YOLOv3](https:\/\/pjreddie.com\/darknet\/yolo\/) on RSNA and to generate submission file (to be honest, not super easy ...!).\n\n\n* The purpose of this competition is 'object detection'. Generally, object detection algorithms with deep learning take a long time to train model and require a lot of gpu resources. Most individual participants use one or two gpu (... or zero). Therefore, there is a need for **algorithms that works quickly with less gpu resources.**\n\n\n* I tried to use Mask R-CNN, UNet, Fast R-CNN and FCN algorithms, But eventually switched to YOLOv3.\n\n\n* In comparison to YOLOv3, Other algorithms(Mask R-CNN, UNet, FCN, ...) which contain instance\/sementic segmentation tasks are very slow, and require more gpu resources, redundant parameter tunning and post-processes. Therefore, if you try to use these algorithms, you may experience difficulties in terms of training time and gpu resources. (Please see [YOLOv3 paper](https:\/\/pjreddie.com\/media\/files\/papers\/YOLOv3.pdf) for details)\n\n\n* In addition, **YOLOv3 was able to obtain high score (LB: 0.141) without additional processes(data augmentation, parameter tunning, etc...)** compared to other algorithms. So I think YOLOv3 has sufficient potential for this competition.\n\n\n* In this notebook, I'll introduce how to simply apply YOLOv3 on RSNA data. I hope this notebook would be helpful for everyone.","271b1944":"### 2.5. Create test image and labels for YOLOv3","20aad04b":"### 5.0. Copy sample test image","4ad5bf93":"### 2.6. Plot a sample test Image","4e08b8ba":"### 6.2. Generate submission files\n* When making submission files, be aware of label format which is different in yolo.","703da038":"### 6.1. Load darknet python wrapper module","6d8b02d8":"### 2.2. Generate images and labels for training YOLOv3\n* YOLOv3 needs .txt file for each image, which contains ground truth object in the image that looks like:\n```\n<object-class_1> <x_1> <y_1> <width_1> <height_1>\n<object-class_2> <x_2> <y_2> <width_2> <height_2>\n```\n* <object-class\\>: Since RSNA task is binary classification basically, <object-class\\> is 0.\n* <x\\>, <y\\>: Those are float values of bbox center coordinate, divided by image width and height respectively.\n* <w\\>, <h\\>: Those are width and height of bbox, divided by image width and height respectively.\n\n* So it is different from the format of label data provided by kaggle. We should change it.","31d14026":"### 4.1. Command for training with Multi-gpu after 1000 iteration\n\nIf you are trying to train with multi-gpu, there are three things to watch out.\n* (The # of gpus)x('learning rate' in 'cfg\/rsna_yolov3.cfg_train') is the real learning rate for training\n* I don't recommend you to use multi-gpu for first 1000 iterations. with multi-gpu, training would not be stable. Use single gpu before 1000 and after 1000, continue with more gpus.\n* By the way, If the # of gpus is over 5, training is not stable.\n\n```\nAbove things will depend on your environment. The best way to find the most appropriate method is to just give it a try :)\n```","3460671c":"### 2.1. Load stage_1_train_labels.csv","cd882187":"### 6.0. Download darknet python wrapper (darknet.py)\n* Basically, you can use darknet\/python\/darknet.py files. However it'll show error.\n* So, I edited the darknet.py. There are two main modifications.\n  * Change print statement to print function for python3\n  * Edit dynamic library('libdarknet.so') file path\n* I leaved '# ===' marks where i edited in darknet.py. For example,\n```\n# ==============================================================================\n#lib = CDLL(\"\/home\/pjreddie\/documents\/darknet\/libdarknet.so\", RTLD_GLOBAL)\ndarknet_lib_path = os.path.join(os.getcwd(), \"darknet\", \"libdarknet.so\")\nlib = CDLL(darknet_lib_path, RTLD_GLOBAL)\n# ==============================================================================\n```","7a85e21b":"## 2. Data Migration for YOLOv3\nIt might take a while.","b3f680ee":"### 4.0. Command for training with Pre-trained CNN Weights (darknet53.conv.74)\n* I didn't run following command on kaggle kernel becuase of the long output.\n* If you crash with  'CUDA Error: out of memory', Solve it by Editing 'batch' and 'subdivisions' in 'cfg\/rsna_yolov3.cfg_train'\n* If 'batch' and 'subdivisions' are 64 and 64 respectively, for every iteration only one image will be loaded on GPU memory. So it will use less GPU memory.","0ffac1de":"### 4.2. My Plot of Training Loss\nIt's a loss graph up to about 2000 iteration. Since it tooks too long on kaggle kernel, I brought it. When learning, don't be surprised of big loss values at the beginning. Stay calm and It'll go down. Please See the following loss graph.","bfa02002":"## 7. Future works & Etc\n\n### Future works (Things to try)\n* Image augmentation\n* More training\n* Utilizing the not labeled images because we got rid of not labeled images above\n\n### ETC\n* For a private matter, i can not proceed RSNA task after 09\/27. If you have any ideas, questions and problems with this kernel after 09\/27, Please leave those things anyway~! Collaborator '@John Byun' will reply to your comments.","64ea40f1":"### - darknet53.conv.74  (Download Pre-trained Model)\nFor training, we would download the pre-trained model weights(darknet53.conv.74) using following wget command. I recommend you to use this pre-trained weight too. Author of darknet also uses this pre-trained weights in different fields of image recognition.","f4eb351d":"### 5.1. Load trained model (at 15300 iteration)\nSince i uploaded the weights file (large big file) on my google drive, the command is very very long ...\n* It's a weight file at 15300 iteration, which I made submission file with. If you use this weight, you'll get a score of 0.141LB.\n  * Up to 15300 iteration, It takes about 8 hours.\n    * In .cfg file, I set 'batch' and 'subdivisions' as 64 and 8 respectively.\n    * Up to 1000 iteration from 0, it takes about 1h with **one** Tesla P100 GPU.      **(1000 iter\/h)**\n    * Up to 15300 iteration from 1000, it takes about 7h with **four** Tesla P100 GPU. **(2043 iter\/h)**","dd2259bd":"## 6. Generate Submission Files with YOLOv3 Python Wrapper"}}