{"cell_type":{"c21b11ee":"code","5415963f":"code","d9579c5d":"code","eb7b5abf":"code","a5021a4c":"code","f4b1288a":"code","11e44958":"code","28eb2fc6":"code","669f40ff":"code","41c6f0dd":"code","c6964421":"code","2142bf5b":"code","a0a56310":"code","d9f1cb39":"code","53d0e337":"code","6198ea79":"code","84b90576":"code","6cc64015":"code","703f4026":"code","3eff9f34":"code","239fd00d":"code","fd96f578":"code","b4946983":"code","1fe91db6":"markdown","35d37fc3":"markdown","4a03bd4f":"markdown","38506227":"markdown","34e61cff":"markdown","a9630703":"markdown","760a77f5":"markdown","c2971049":"markdown","ba443b08":"markdown","f9c365c0":"markdown","ad30a57e":"markdown","907a3f28":"markdown","cc205958":"markdown","633ad68b":"markdown","520f6b89":"markdown","55cadf78":"markdown","69be6f2f":"markdown","b8fbf963":"markdown","6cef2abd":"markdown"},"source":{"c21b11ee":"from pathlib import Path\nfrom typing import List, Tuple, Optional\n\nimport numpy as np\nimport pandas as pd\nimport category_encoders as ce\nfrom sklearn.model_selection import StratifiedKFold\n\nimport lightgbm as lgb","5415963f":"DATASET_NAME = \"iris\"\nROOT = Path.cwd().parent\nINPUT_ROOT = ROOT \/ \"input\"\nRAW_DATA = INPUT_ROOT \/ DATASET_NAME\n\nID = \"Id\"\nTARGET = \"Species\"\nRANDOM_SEED = 0\nNUM_THREADS = 4\nN_FOLD = 5","d9579c5d":"train_all = pd.read_csv(RAW_DATA \/ \"Iris.csv\")\ntrain_all.head()","eb7b5abf":"class MultiLoglossForLGBM:\n    \"\"\"Self-made multi-class logloss for LightGBM.\"\"\"\n    \n    def __init__(self, n_class: int=3, use_softmax: bool=True, epsilon: float=1e-32) -> None:\n        \"\"\"Initialize.\"\"\"\n        self.name = \"my_mlnloss\"\n        self.n_class = n_class\n        self.prob_func = self._get_prob_value if use_softmax else lambda x: x\n        self.epsilon = epsilon\n    \n    def __call__(self, preds: np.ndarray, labels: np.ndarray, weight: Optional[np.ndarray]=None) -> float:\n        \"\"\"Calc loss.\"\"\"\n        # # get prob value by softmax\n        prob = self.prob_func(preds)           # <= logits \u3092\u78ba\u7387\u5024\u306b\u76f4\u3059\n        # # convert labels to 1-hot\n        labels = self._get_1hot_label(labels)  # <= labels (1D-array) \u3092 1hot \u306b\u5909\u63db\n\n        loss_by_sample = np.sum(- np.log(prob) * labels, axis=1)\n        loss = np.average(loss_by_sample, weight)\n        \n        return loss\n    \n    def _calc_grad_and_hess(\n        self, preds: np.ndarray, labels: np.ndarray, weight: Optional[np.ndarray]=None\n    ) -> Tuple[np.ndarray]:\n        \"\"\"Calc Grad and Hess\"\"\"\n        # # get prob value by softmax\n        prob = self.prob_func(preds)           # <= margin \u3092\u78ba\u7387\u5024\u306b\u76f4\u3059\n        # # convert labels to 1-hot\n        labels = self._get_1hot_label(labels)  # <= labels (1D-array) \u3092 1hot \u306b\u5909\u63db\n\n        grad = prob - labels\n        hess = prob * (1 - prob)        \n        if weight is not None:\n            grad = grad * weight[:, None]\n            hess = hess * weight[:, None]\n        return grad, hess\n    \n    def return_loss(self, preds: np.ndarray, data: lgb.Dataset) -> Tuple[str, float, bool]:\n        \"\"\"Return Loss for lightgbm\"\"\"\n        labels = data.get_label()\n        weight = data.get_weight()\n        n_example = len(labels)\n        \n        # # reshape preds: (n_class * n_example,) => (n_class, n_example) =>  (n_example, n_class)\n        preds = preds.reshape(self.n_class, n_example).T  # <= preds (1D-array) \u3092 2D-array \u306b\u76f4\u3059\n        # # calc loss\n        loss = self(preds, labels, weight)\n        \n        return self.name, loss, False\n    \n    def return_grad_and_hess(self, preds: np.ndarray, data: lgb.Dataset) -> Tuple[np.ndarray]:\n        \"\"\"Return Grad and Hess for lightgbm\"\"\"\n        labels = data.get_label()\n        weight = data.get_weight()\n        n_example = len(labels)\n        \n        # # reshape preds: (n_class * n_example,) => (n_class, n_example) =>  (n_example, n_class)\n        preds = preds.reshape(self.n_class, n_example).T  # <= preds (1D-array) \u3092 2D-array \u306b\u76f4\u3059\n        # # calc grad and hess.\n        grad, hess =  self._calc_grad_and_hess(preds, labels, weight)\n\n        # # reshape grad, hess: (n_example, n_class) => (n_class, n_example) => (n_class * n_example,) \n        grad = grad.T.reshape(n_example * self.n_class)   # <= 1D-array \u306b\u623b\u3059\n        hess = hess.T.reshape(n_example * self.n_class)   # <= 1D-array \u306b\u623b\u3059\n        \n        return grad, hess\n    \n    def _get_prob_value(self, preds: np.ndarray) -> np.ndarray:\n        \"\"\"Convert Margin(Logit) to Prob by Softmax.\"\"\"\n        prob = np.exp(preds - preds.max(axis=1)[:, None])\n        prob = prob \/ prob.sum(axis=1)[:, None]\n        prob = np.clip(prob, self.epsilon, 1 - self. epsilon)\n        \n        return prob\n    \n    def _get_1hot_label(self, labels: np.ndarray) -> np.ndarray:\n        \"\"\"Convert labels to 1hot array.\"\"\"\n        n_example = len(labels)\n        ohot = np.zeros((n_example, self.n_class), dtype=int)\n        ohot[np.arange(n_example), labels.astype(int)] = 1\n        \n        return ohot","a5021a4c":"def multi_class_accuracy_for_lgbm(\n    preds: np.ndarray, data: lgb.Dataset, n_class: int=3,\n):\n    labels = data.get_label()  # (n_example,)\n    weight = data.get_weight()  # (n_example,)\n    \n    n_example = len(labels)\n    # # reshape: (n_example * n_class) => (n_class, n_example) => (n_example, n_class)\n    preds = preds.reshape(n_class, n_example).T\n    labels_pred = preds.argmax(axis=1)\n    \n    score = np.average(labels_pred == labels, weights=weight)\n    return \"my_macc\", score, True","f4b1288a":"ord_enc = ce.OrdinalEncoder(\n    mapping=[{\n        \"col\": TARGET,\n        \"mapping\": {c: i for i, c in enumerate(train_all.Species.unique())}}],\n    cols=[TARGET])\ntrain_all_multi_class = ord_enc.fit_transform(train_all)","11e44958":"ord_enc.category_mapping","28eb2fc6":"train_all_multi_class.head()","669f40ff":"X_0 = train_all_multi_class.iloc[:,1:5].values\ny_0 = train_all_multi_class.iloc[:,-1].values\n\nkf = StratifiedKFold(n_splits=N_FOLD, random_state=RANDOM_SEED, shuffle=True)\ntrain_val_splits = list(kf.split(X_0, y_0))\n\n# # use fold 0\ntrain_index, valid_index = train_val_splits[0]\nX_0_tr, y_0_tr = X_0[train_index], y_0[train_index]\nX_0_val, y_0_val =  X_0[valid_index], y_0[valid_index]","41c6f0dd":"MODEL_PARAMS_LGB = {\n    'num_class': 3,  # <= class \u6570\u3092\u6307\u5b9a\n    \"metric\": \"None\",\n    \"first_metric_only\": True,\n    \"eta\": 0.01,\n    \"max_depth\": -1,\n    \"seed\": RANDOM_SEED,\n    \"num_threads\": NUM_THREADS,\n    \"verbose\": -1\n}\nFIT_PARAMS_LGB = {\n    \"num_boost_round\": 10000,\n    \"early_stopping_rounds\": 100,\n    \"verbose_eval\":100}\n\nmy_mlnloss = MultiLoglossForLGBM(n_class=3, use_softmax=True)  # <= \u81ea\u4f5c class \u3092\u521d\u671f\u5316 \n\nlgb_tr = lgb.Dataset(X_0_tr, y_0_tr)\nlgb_val = lgb.Dataset(X_0_val, y_0_val)\n\nmodel_my_mlnloss = lgb.train(\n    params=MODEL_PARAMS_LGB, train_set=lgb_tr, **FIT_PARAMS_LGB,\n    valid_names=['train', 'valid'], valid_sets=[lgb_tr, lgb_val],\n    fobj=my_mlnloss.return_grad_and_hess,           # <= gradient \u3068 hessian \u3092\u8fd4\u3059\u95a2\u6570\n    feval=lambda preds, data: [\n        my_mlnloss.return_loss(preds, data),        # <= loss \u3092\u8fd4\u3059\u95a2\u6570\n        multi_class_accuracy_for_lgbm(preds, data)  # <= multi-class accuracy (\u81ea\u4f5c) \n    ]\n)","c6964421":"MODEL_PARAMS_LGB = {\n    \"objective\": \"multiclass\",  # <= set implemented multi logloss\n    'num_class': 3,\n    \"first_metric_only\": True,\n    \"eta\": 0.01,\n    \"max_depth\": -1,\n    \"seed\": RANDOM_SEED,\n    \"num_threads\": NUM_THREADS,\n    \"verbose\": -1\n}\nFIT_PARAMS_LGB = {\n    \"num_boost_round\": 10000,\n    \"early_stopping_rounds\": 100,\n    \"verbose_eval\":100}\n\nmy_mlnloss = MultiLoglossForLGBM(n_class=3, use_softmax=False)   # <= objective \u304c\u7d44\u307f\u8fbc\u307f\u306a\u306e\u3067 softmax \u306f\u4f7f\u7528\u3057\u306a\u3044.\nlgb_tr = lgb.Dataset(X_0_tr, y_0_tr)\nlgb_val = lgb.Dataset(X_0_val, y_0_val)\n\nmodel_mlnloss = lgb.train(\n    params=MODEL_PARAMS_LGB, train_set=lgb_tr, **FIT_PARAMS_LGB,\n    valid_names=['train', 'valid'], valid_sets=[lgb_tr, lgb_val],\n    feval=lambda preds, data: [\n        my_mlnloss.return_loss(preds, data),        # <= multi-class logloss  (\u81ea\u4f5c)\n        multi_class_accuracy_for_lgbm(preds, data)  # <= multi-class accuracy (\u81ea\u4f5c) \n    ]\n)","2142bf5b":"class MultiLabelDatasetForLGBM(lgb.Dataset):\n    \"\"\"\n    Makeshift Class for storing multi label.\n    \n    label: numpy.ndarray (n_example, n_target)\n    \"\"\"\n\n    def __init__(\n        self, data, label=None, reference=None, weight=None, group=None, init_score=None, silent=False,\n        feature_name='auto', categorical_feature='auto', params=None,  free_raw_data=True\n    ):\n        \"\"\"Initialize.\"\"\"\n        if label is not None:\n            # # make dummy 1D-array\n            dummy_label = np.arange(len(data))\n\n        super(MultiLabelDatasetForLGBM, self).__init__(\n            data, dummy_label, reference, weight, group, init_score, silent,\n            feature_name, categorical_feature, params, free_raw_data)\n        \n        self.mult_label = label\n        \n    def get_multi_label(self):\n        \"\"\"Get 2D-array label\"\"\"\n        return self.mult_label\n    \n    def set_multi_label(self, multi_label: np.ndarray):\n        \"\"\"Set 2D-array label\"\"\"\n        self.mult_label = multi_label\n        return self","a0a56310":"class MultiMSEForLGBM:\n    \"\"\"Self-made multi-task(?) mse for LightGBM.\"\"\"\n    \n    def __init__(self, n_target: int=3) -> None:\n        \"\"\"Initialize.\"\"\"\n        self.name = \"my_mmse\"\n        self.n_target = n_target\n    \n    def __call__(self, preds: np.ndarray, labels: np.ndarray, weight: Optional[np.ndarray]=None) -> float:\n        \"\"\"Calc loss.\"\"\"\n        loss_by_sample = np.sum((preds - labels) ** 2, axis=1)\n        loss = np.average(loss_by_sample, weights=weight)\n        \n        return loss\n    \n    def _calc_grad_and_hess(\n        self, preds: np.ndarray, labels: np.ndarray, weight: Optional[np.ndarray]=None\n    ) -> Tuple[np.ndarray]:\n        \"\"\"Calc Grad and Hess\"\"\"\n        grad = preds - labels\n        hess = np.ones_like(preds)     \n        if weight is not None:\n            grad = grad * weight[:, None]\n            hess = hess * weight[:, None]\n\n        return grad, hess\n    \n    def return_loss(self, preds: np.ndarray, data: lgb.Dataset) -> Tuple[str, float, bool]:\n        \"\"\"Return Loss for lightgbm\"\"\"\n        labels = data.get_multi_label()  # <= \u6539\u9020\u3057\u305f Dataset \u304b\u3089 multi-label \u3092\u53d7\u3051\u53d6\u308b\n        weight = data.get_weight()\n        n_example = len(labels)\n        \n        # # reshape preds: (n_target * n_example,) => (n_target, n_example) =>  (n_example, n_target)\n        preds = preds.reshape(self.n_target, n_example).T  # <= preds (1D-array) \u3092 2D-array \u306b\u76f4\u3059\n        # # calc loss\n        loss = self(preds, labels, weight)\n        \n        return self.name, loss, False\n    \n    def return_grad_and_hess(self, preds: np.ndarray, data: lgb.Dataset) -> Tuple[np.ndarray]:\n        \"\"\"Return Grad and Hess for lightgbm\"\"\"\n        labels = data.get_multi_label()  # <= \u6539\u9020\u3057\u305f Dataset \u304b\u3089 multi-label \u3092\u53d7\u3051\u53d6\u308b\n        weight = data.get_weight()\n        n_example = len(labels)\n        \n        # # reshape preds: (n_target * n_example,) => (n_target, n_example) =>  (n_example, n_target)\n        preds = preds.reshape(self.n_target, n_example).T  # <= preds (1D-array) \u3092 2D-array \u306b\u76f4\u3059\n        # # calc grad and hess.\n        grad, hess =  self._calc_grad_and_hess(preds, labels, weight)\n\n        # # reshape grad, hess: (n_example, n_target) => (n_class, n_target) => (n_target * n_example,) \n        grad = grad.T.reshape(n_example * self.n_target)   # <= 1D-array \u306b\u623b\u3059\n        hess = hess.T.reshape(n_example * self.n_target)   # <= 1D-array \u306b\u623b\u3059\n        \n        return grad, hess","d9f1cb39":"def multi_class_accuracy_for_lgbm_altered(\n    preds: np.ndarray, data: lgb.Dataset, n_class: int=3,\n):\n    labels = data.get_multi_label()  # (n_example, n_class)\n    weight = data.get_weight()  # (n_example,)\n    \n    n_example = len(labels)\n    # # reshape: (n_example * n_class) => (n_class, n_example) => (n_example, n_class)\n    preds = preds.reshape(n_class, n_example).T\n    labels_true = labels.argmax(axis=1)\n    labels_pred = preds.argmax(axis=1)\n\n    score = np.average(labels_pred == labels_true, weights=weight)\n    return \"my_macc\", score, True","53d0e337":"ohot_enc = ce.OneHotEncoder(cols=[TARGET], use_cat_names=True)\ntrain_all_multi_reg = ohot_enc.fit_transform(train_all)\n\ntrain_all_multi_reg.head()","6198ea79":"X_1 = train_all_multi_reg.iloc[:, 1:5].values\ny_1 = train_all_multi_reg.iloc[:, 5:8].values\n\n# # use the same split as multi-class classification\ntrain_index, valid_index = train_val_splits[1]\nX_1_tr, y_1_tr = X_1[train_index], y_1[train_index]\nX_1_val, y_1_val =  X_1[valid_index], y_1[valid_index]","84b90576":"MODEL_PARAMS_LGB = {\n    'num_class': 3,  # <= class \u6570\u3092\u6307\u5b9a\n    \"eta\": 0.01,\n    \"metric\": \"None\",\n    \"first_metric_only\": True,\n    \"max_depth\": -1,\n    \"seed\": RANDOM_SEED,\n    \"num_threads\": NUM_THREADS,\n    \"verbose\": -1\n}\nFIT_PARAMS_LGB = {\n    \"num_boost_round\": 10000,\n    \"early_stopping_rounds\": 100,\n    \"verbose_eval\":50}\n\nmy_mmse = MultiMSEForLGBM(n_target=3)  # <= \u81ea\u4f5c class \u3092\u521d\u671f\u5316 \n\nlgb_tr = MultiLabelDatasetForLGBM(X_1_tr, y_1_tr)     # <= \u6539\u9020 Dataset\nlgb_val = MultiLabelDatasetForLGBM(X_1_val, y_1_val)  # <= \u6539\u9020 Dataset\n\nmodel_my_mmse = lgb.train(\n    MODEL_PARAMS_LGB, lgb_tr, **FIT_PARAMS_LGB,\n    valid_names=['train', 'valid'], valid_sets=[lgb_tr, lgb_val],\n    fobj=my_mmse.return_grad_and_hess,                       # <= gradient \u3068 hessian \u3092\u8fd4\u3059\u95a2\u6570\n    feval=lambda preds, data: [\n        my_mmse.return_loss(preds, data),                    # <= loss \u3092\u8fd4\u3059\u95a2\u6570\n        multi_class_accuracy_for_lgbm_altered(preds, data),  # <= multi-class accuracy (\u81ea\u4f5c) \n    ]\n)","6cc64015":"class MultiMSEAlphaForLGBM:\n    \"\"\"Self-made multi-task(?) mse for LightGBM.\"\"\"\n    \n    def __init__(self, n_target: int=3) -> None:\n        \"\"\"Initialize.\"\"\"\n        self.name = \"my_mmse_2\"\n        self.n_target = n_target\n    \n    def __call__(self, preds: np.ndarray, labels: np.ndarray, weight: Optional[np.ndarray]=None) -> float:\n        \"\"\"Calc loss.\"\"\"\n        loss_by_sample = np.sum((preds - labels) ** 2, axis=1) + (1 - np.sum(preds, axis=1)) ** 2\n        loss = np.average(loss_by_sample, weights=weight)\n        \n        return loss\n    \n    def _calc_grad_and_hess(\n        self, preds: np.ndarray, labels: np.ndarray, weight: Optional[np.ndarray]=None\n    ) -> Tuple[np.ndarray]:\n        \"\"\"Calc Grad and Hess\"\"\"\n        grad = preds - labels - 1 + np.sum(preds, axis=1)[:, None]\n        hess = np.ones_like(preds) * 2\n        if weight is not None:\n            grad = grad * weight[:, None]\n            hess = hess * weight[:, None]\n\n        return grad, hess\n    \n    def return_loss(self, preds: np.ndarray, data: lgb.Dataset) -> Tuple[str, float, bool]:\n        \"\"\"Return Loss for lightgbm\"\"\"\n        labels = data.get_multi_label()  # <= \u6539\u9020\u3057\u305f Dataset \u304b\u3089 multi-label \u3092\u53d7\u3051\u53d6\u308b\n        weight = data.get_weight()\n        n_example = len(labels)\n        \n        # # reshape preds: (n_target * n_example,) => (n_target, n_example) =>  (n_example, n_target)\n        preds = preds.reshape(self.n_target, n_example).T  # <= preds (1D-array) \u3092 2D-array \u306b\u76f4\u3059\n        # # calc loss\n        loss = self(preds, labels, weight)\n        \n        return self.name, loss, False\n    \n    def return_grad_and_hess(self, preds: np.ndarray, data: lgb.Dataset) -> Tuple[np.ndarray]:\n        \"\"\"Return Grad and Hess for lightgbm\"\"\"\n        labels = data.get_multi_label()  # <= \u6539\u9020\u3057\u305f Dataset \u304b\u3089 multi-label \u3092\u53d7\u3051\u53d6\u308b\n        weight = data.get_weight()\n        n_example = len(labels)\n        \n        # # reshape preds: (n_target * n_example,) => (n_target, n_example) =>  (n_example, n_target)\n        preds = preds.reshape(self.n_target, n_example).T  # <= preds (1D-array) \u3092 2D-array \u306b\u76f4\u3059\n        # # calc grad and hess.\n        grad, hess =  self._calc_grad_and_hess(preds, labels, weight)\n\n        # # reshape grad, hess: (n_example, n_target) => (n_class, n_target) => (n_target * n_example,) \n        grad = grad.T.reshape(n_example * self.n_target)   # <= 1D-array \u306b\u623b\u3059\n        hess = hess.T.reshape(n_example * self.n_target)   # <= 1D-array \u306b\u623b\u3059\n        \n        return grad, hess","703f4026":"MODEL_PARAMS_LGB = {\n    'num_class': 3,  # <= class \u6570\u3092\u6307\u5b9a\n    \"eta\": 0.01,\n    \"metric\": \"None\",\n    \"first_metric_only\": True,\n    \"max_depth\": -1,\n    \"seed\": RANDOM_SEED,\n    \"num_threads\": NUM_THREADS,\n    \"verbose\": -1\n}\nFIT_PARAMS_LGB = {\n    \"num_boost_round\": 10000,\n    \"early_stopping_rounds\": 100,\n    \"verbose_eval\":50}\n\n\nmy_mmse_2 = MultiMSEAlphaForLGBM(n_target=3)  # <= \u81ea\u4f5c class \u3092\u521d\u671f\u5316 \nmy_mmse = MultiMSEForLGBM(n_target=3)          # <= \u81ea\u4f5c class \u3092\u521d\u671f\u5316 (loss \u306e\u6bd4\u8f03\u7528)\n\nlgb_tr = MultiLabelDatasetForLGBM(X_1_tr, y_1_tr)     # <= \u6539\u9020 Dataset\nlgb_val = MultiLabelDatasetForLGBM(X_1_val, y_1_val)  # <= \u6539\u9020 Dataset\n\nmodel_my_mmse_2 = lgb.train(\n    MODEL_PARAMS_LGB, lgb_tr, **FIT_PARAMS_LGB,\n    valid_names=['train', 'valid'], valid_sets=[lgb_tr, lgb_val],\n    fobj=my_mmse_2.return_grad_and_hess,                       # <= gradient \u3068 hessian \u3092\u8fd4\u3059\u95a2\u6570\n    feval=lambda preds, data: [\n        my_mmse_2.return_loss(preds, data),                 # <= loss \u3092\u8fd4\u3059\u95a2\u6570\n        my_mmse.return_loss(preds, data),                    # <= loss \u3092\u8fd4\u3059\u95a2\u6570\n        multi_class_accuracy_for_lgbm_altered(preds, data),  # <= multi-class accuracy (\u81ea\u4f5c) \n    ]\n)","3eff9f34":"# # prediction of classification\ny_pred_val_mlnloss = model_my_mlnloss.predict(X_0_val)\ny_pred_val_mlnloss = my_mlnloss._get_prob_value(y_pred_val_mlnloss) # <= softmax \u3092\u9069\u7528\n\n# # prediction of regression\ny_pred_val_mmse = model_my_mmse.predict(X_1_val)\ny_pred_val_mmse_2 = model_my_mmse_2.predict(X_1_val)","239fd00d":"y_pred_val_mlnloss[:5]","fd96f578":"y_pred_val_mmse[:5]","b4946983":"y_pred_val_mmse_2[:5]","1fe91db6":"### split data","35d37fc3":"### definition of Custom Dataset","4a03bd4f":"## Compare Prediciton","38506227":"#### use self-made `MultiLoglossForLGBM`","34e61cff":"### split data","a9630703":"## As Multi-Class Classification","760a77f5":"### definition of self-made multi-class logloss\n$\nN: \\verb|number of examples|\n\\\\\nK: \\verb|number of classes|\n\\\\\nY: \\verb|matrix of predicted values|\n\\\\\nT: \\verb|matrix representing labels|\n\\\\\n\\displaystyle Y = \\left ( \\vec{y}_1, \\vec{y}_2, ... , \\vec{y}_N\\right )^\\mathsf{T}\n\\verb|, where |\n\\ \\vec{y}_i = \\left( y_{i 1}, y_{i 2}, ... , y_{i K} \\right)^\\mathsf{T} \n\\\\\nT = \\left ( \\vec{t}_1, \\vec{t}_2, ... , \\vec{t}_N\\right )^\\mathsf{T}\n\\verb|, where | \\displaystyle \\vec{t}_i = \\left( t_{i 1}, t_{i 2}, ... , t_{i K}\\right)^\\mathsf{T} \\verb| (1-hot vector)|\n$\n\n$\n\\displaystyle p_{ij} = \\mathrm{softmax}_j \\left( \\vec{y}_i \\right) = \\frac{\\exp \\left( y_{ij} \\right)}{\\sum_{j'=1}^K \\exp \\left(y_{ij'} \\right)}\n\\\\\n\\displaystyle\\frac{\\partial p_{ij}}{\\partial y_{ij}} =\n\\frac{\n\\exp\\left(y_{ij} \\right) \\sum_{j'=1}^K \\exp\\left(y_{ij'} \\right) - \\exp\\left(y_{ij} \\right) \\exp\\left(y_{ij} \\right)\n}{\\left \\{ \\sum_{j'=1}^K \\exp\\left(y_{ij'} \\right) \\right \\}^2}\n= p_{ij} \\left( 1 - p_{ij} \\right)\n\\\\\n\\displaystyle\\frac{\\partial p_{ik}}{\\partial y_{ij}} =\n\\frac{\n-\\exp\\left(y_{ik} \\right) \\exp\\left(y_{ij} \\right)\n}{\\left \\{ \\sum_{j'=1}^K \\exp\\left(y_{ij'} \\right) \\right \\}^2}\n= - p_{ik} \\ p_{ij} \\ \\left(k \\neq j \\right)\n$\n\n$\n\\displaystyle \\mathrm{MultiLogLoss}\\left(Y, T \\right)\n= \\frac{1}{N} \\sum_{i = 1}^N \\mathrm{loss} \\left( \\vec{y}_i, \\vec{t}_i \\right)\n\\\\\n\\displaystyle \\mathrm{loss} \\left( \\vec{y}_i, \\vec{t}_i \\right)\n= - \\sum_{j = 1}^K t_{ij} \\log \\left(p_{ij} \\right)\n\\\\\n\\displaystyle \\frac{\\partial}{\\partial y_{ij}} \\mathrm{loss} \\left( \\vec{y}_i, \\vec{t}_i \\right)\n= -t_{ij}\\frac{1}{p_{ij}} \\frac{d p_{ij}}{d y_{ij}} - \\sum_{k \\neq j} t_{ik} \\frac{1}{p_{ik}} \\frac{d p_{ik}}{d y_{ij}}\n\\\\\n\\displaystyle\n\\hspace{3.0cm} = -t_{ij} \\left( 1 - p_{ij}\\right) - \\sum_{k \\neq j} t_{ik} \\left(- p_{ij} \\right)\n= p_{ij} \\sum_{k = 1}^K t_{ik} - t_{ij}\n\\\\\n\\hspace{3.0cm}\\displaystyle = p_{ij} - t_{ij}\n\\\\\n\\displaystyle \\frac{\\partial^2}{\\partial y_{ij}^2} \\mathrm{loss} \\left( \\vec{y}_i, \\vec{t}_i \\right)\n= p_{ij} \\left( 1 - p_{ij} \\right)\n$","c2971049":"### definition of self-made multi-task(???) MSE\n\n$\nN: \\verb|number of examples|\n\\\\\nM: \\verb|number of targets (tasks)|\n\\\\\nY: \\verb|matrix of predicted values|\n\\\\\nT: \\verb|matrix representing target values|\n\\\\\n\\displaystyle Y = \\left ( \\vec{y}_1, \\vec{y}_2, ... , \\vec{y}_N\\right )^\\mathsf{T}\n\\verb|, where |\n\\ \\vec{y}_i = \\left( y_{i 1}, y_{i 2}, ... , y_{i M} \\right)^\\mathsf{T} \n\\\\\nT = \\left ( \\vec{t}_1, \\vec{t}_2, ... , \\vec{t}_N\\right )^\\mathsf{T}\n\\verb|, where | \\displaystyle \\vec{t}_i = \\left( t_{i 1}, t_{i 2}, ... , t_{M}\\right)^\\mathsf{T}\n$\n\n$\n\\displaystyle \\mathrm{MultiTaskMSE} \\left(Y, T \\right) = \\frac{1}{N} \\sum_{i=1}^N 2 \\ \\mathrm{loss} \\left(\\vec{y}_i, \\vec{t}_i \\right)\n\\\\\n\\displaystyle \\mathrm{loss} \\left(\\vec{y}_i, \\vec{t}_i \\right) = \\sum_{j=1}^M \\frac{1}{2} \\left( y_{ij} - t_{ij} \\right)^2\n\\\\\n\\displaystyle \\frac{\\partial}{\\partial y_{ij}} \\mathrm{loss} \\left(\\vec{y}_i, \\vec{t}_i \\right) = y_{ij} - t_{ij}\n\\\\\n\\displaystyle \\frac{\\partial^2}{\\partial y_{ij}^2} \\mathrm{loss} \\left(\\vec{y}_i, \\vec{t}_i \\right) = 1\n$\n","ba443b08":"### Training","f9c365c0":"### Read Data","ad30a57e":"### Training","907a3f28":"#### training by inplemented `multiclass`","cc205958":"### encode target","633ad68b":"## Preparation","520f6b89":"### encoding target","55cadf78":"### import","69be6f2f":"### definition of self-made multi-task(???) MSE + $\\alpha$\n\n$\nN: \\verb|number of examples|\n\\\\\nM: \\verb|number of targets (tasks)|\n\\\\\nY: \\verb|matrix of predicted values|\n\\\\\nT: \\verb|matrix representing target values|\n\\\\\n\\displaystyle Y = \\left ( \\vec{y}_1, \\vec{y}_2, ... , \\vec{y}_N\\right )^\\mathsf{T}\n\\verb|, where |\n\\ \\vec{y}_i = \\left( y_{i 1}, y_{i 2}, ... , y_{i M} \\right)^\\mathsf{T} \n\\\\\nT = \\left ( \\vec{t}_1, \\vec{t}_2, ... , \\vec{t}_N\\right )^\\mathsf{T}\n\\verb|, where | \\displaystyle \\vec{t}_i = \\left( t_{i 1}, t_{i 2}, ... , t_{M}\\right)^\\mathsf{T}\n$\n\n$\n\\displaystyle \\mathrm{MultiTaskMSE}_{\\alpha} \\left(Y, T \\right) = \\frac{1}{N} \\sum_{i=1}^N 2 \\ \\mathrm{loss} \\left(\\vec{y}_i, \\vec{t}_i \\right)\n\\\\\n\\displaystyle \\mathrm{loss} \\left(\\vec{y}_i, \\vec{t}_i \\right)\n= \\sum_{j=1}^M \\frac{1}{2} \\left( y_{ij} - t_{ij} \\right)^2 + \\frac{1}{2} \\left( 1 - \\sum_{j=1}^M y_{ij} \\right)^2\n\\\\\n\\displaystyle\\frac{d}{d y_{ij}} \\mathrm{loss}(\\vec{y}_i, \\vec{t}_i)\n= \\left( y_{ij} - t_{ij} \\right) - \\left( 1 - \\sum_{k=1}^M y_{ik} \\right) = 2 y_{ij} - t_{ij} + \\sum_{k \\neq j} y_{ik}  - 1\n\\\\\n\\displaystyle \\frac{\\partial^2}{\\partial y_{ij}^2} \\mathrm{loss} \\left(\\vec{y}_i, \\vec{t}_i \\right) = 2\n$\n","b8fbf963":"### Training","6cef2abd":"## As Multi-Task(?) Regression"}}