{"cell_type":{"8952b7b4":"code","ff41676b":"code","63718361":"code","17bfe708":"code","1b69a556":"code","f024618a":"code","1e84c943":"code","89b2f8fa":"code","3c4d13fb":"code","bf5a8e26":"code","d2556414":"code","ae98c49f":"code","aa97e324":"code","21c2b57a":"code","4e8469bf":"code","c4a0068a":"code","4356da38":"code","039015e4":"code","34c6a403":"code","36ba07eb":"code","c4c2c038":"code","392db0f4":"code","f168d0bf":"code","99fd4c3d":"code","5952ec74":"code","bf456a50":"code","42e20593":"code","b7fc304a":"code","edb30092":"markdown","685e1353":"markdown","e671e326":"markdown","0c3b6f54":"markdown","26a78348":"markdown","ee498682":"markdown"},"source":{"8952b7b4":"%%capture\n!pip install ..\/input\/sacremoses\/sacremoses-master\/ > \/dev\/null\n\nimport os\nimport sys\nimport glob\nimport torch\n\nsys.path.insert(0, \"..\/input\/transformers\/transformers-master\/\")\n!pip install \/kaggle\/input\/transformers\/transformers-2.2.1-py3-none-any.whl","ff41676b":"!mkdir -p .\/xlnet-base-cased\n!cp \/kaggle\/input\/xlnetbasecased\/xlnet-base-cased-config.json .\/xlnet-base-cased\/config.json\n!cp \/kaggle\/input\/xlnetbasecased\/xlnet-base-cased-pytorch_model.bin .\/xlnet-base-cased\/pytorch_model.bin\n!cp \/kaggle\/input\/xlnetbasecased\/xlnet-base-cased-spiece.model .\/xlnet-base-cased\/spiece.model","63718361":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, DataLoader\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nimport os\nimport math\nfrom torch.nn import BCEWithLogitsLoss\nfrom torch import nn\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import AdamW, XLNetTokenizer, XLNetModel, XLNetLMHeadModel, XLNetConfig\nfrom tqdm.notebook import tqdm\nfrom tqdm import trange\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","17bfe708":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)\ndf_test  = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/test.csv')\ndf_test['host_topic'] = df_test.host.apply(lambda x: x.split('.')[0])\ndf_train = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/train.csv')\ndf_train['host_topic'] = df_train.host.apply(lambda x: x.split('.')[0])\ndf_submit = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/sample_submission.csv')","1b69a556":"target_cols = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']","f024618a":"plt.figure()\nfig, ax = plt.subplots(figsize=(20, 10));\ndf_train[target_cols].hist(ax=ax);\nplt.tight_layout()\nplt.show()","1e84c943":"def tokenize_inputs(text_list, tokenizer, num_embeddings=512, cut_class = True):\n    \"\"\"\n    Tokenizes the input text input into ids. Appends the appropriate special\n    characters to the end of the text to denote end of sentence. Truncate or pad\n    the appropriate sequence length.\n    original author: Josh Xin Jie Lee @ Medium\n    \"\"\"\n    # tokenize the text, then truncate sequence to the desired length minus 2 for\n    cut = 1\n    if not cut_class:\n        cut += 1\n    # the 2 special characters\n    tokenized_texts = [tokenizer.tokenize(t)[:num_embeddings-cut] for t in text_list]\n    # convert tokenized text into numeric ids for the appropriate LM\n    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n    # append special token \"<s>\" and <\/s> to end of sentence\n    if cut_class:\n        input_ids = [tokenizer.build_inputs_with_special_tokens(x)[:-1] for x in input_ids]\n    else:\n        input_ids = [tokenizer.build_inputs_with_special_tokens(x) for x in input_ids]\n    # print(input_ids)\n    # input()\n    # pad sequences\n    input_ids = pad_sequences(input_ids, maxlen=num_embeddings, dtype=\"long\", truncating=\"post\", padding=\"post\")\n    return input_ids","89b2f8fa":"tokenizer = XLNetTokenizer.from_pretrained('.\/xlnet-base-cased\/', do_lower_case=True)","3c4d13fb":"def tokenize_inputs(text_list, tokenizer, num_embeddings=512, cut_class = True):\n    \"\"\"\n    Tokenizes the input text input into ids. Appends the appropriate special\n    characters to the end of the text to denote end of sentence. Truncate or pad\n    the appropriate sequence length.\n    original author: Josh Xin Jie Lee @ Medium\n    \"\"\"\n    # tokenize the text, then truncate sequence to the desired length minus 2 for\n    cut = 1\n    if not cut_class:\n        cut += 1\n    # the 2 special characters\n    tokenized_texts = [tokenizer.tokenize(t)[:num_embeddings-cut] for t in text_list]\n    # convert tokenized text into numeric ids for the appropriate LM\n    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n    # append special token \"<s>\" and <\/s> to end of sentence\n    if cut_class:\n        input_ids = [tokenizer.build_inputs_with_special_tokens(x)[:-1] for x in input_ids]\n    else:\n        input_ids = [tokenizer.build_inputs_with_special_tokens(x) for x in input_ids]\n    # print(input_ids)\n    # input()\n    # pad sequences\n    input_ids = pad_sequences(input_ids, maxlen=num_embeddings, dtype=\"long\", truncating=\"post\", padding=\"post\")\n    return input_ids","bf5a8e26":"%%time\nT = tokenize_inputs(df_train.question_title.values, tokenizer, 64)\nQ = tokenize_inputs(df_train.question_body.values, tokenizer, 224)\nA = tokenize_inputs(df_train.answer.values, tokenizer,224, cut_class=False)\ntT = tokenize_inputs(df_test.question_title.values, tokenizer, 64)\ntQ = tokenize_inputs(df_test.question_body.values, tokenizer, 224)\ntA = tokenize_inputs(df_test.answer.values, tokenizer, 224, cut_class=False)","d2556414":"X_train = np.concatenate([T, Q, A], axis=1)\nX_test = np.concatenate([tT, tQ, tA], axis=1)","ae98c49f":"y_train = df_train[target_cols].values","aa97e324":"def create_attn_masks(input_ids):\n    \"\"\"\n    Create attention masks to tell model whether attention should be applied to\n    the input id tokens. Do not want to perform attention on padding tokens.\n    original author: Josh Xin Jie Lee @ Medium\n    \"\"\"\n    # Create attention masks\n    attention_masks = []\n\n    # Create a mask of 1s for each token followed by 0s for padding\n    for seq in input_ids:\n        seq_mask = [float(i>0) for i in seq]\n        attention_masks.append(seq_mask)\n    return attention_masks\n    \nX_train_masks = create_attn_masks(X_train)\nX_test_masks = create_attn_masks(X_test)","21c2b57a":"X_train, X_val, X_train_masks, X_val_masks, y_train, y_val = train_test_split(X_train, X_train_masks, y_train,\n                                                                              test_size=0.15, random_state=46)","4e8469bf":"X_train = torch.tensor(X_train)\nX_val = torch.tensor(X_val)\n\ny_train = torch.tensor(y_train, dtype=torch.float32)\ny_val = torch.tensor(y_val, dtype=torch.float32)\n\nX_train_masks = torch.tensor(X_train_masks, dtype=torch.long)\nX_val_masks = torch.tensor(X_val_masks, dtype=torch.long)","c4a0068a":"# Select a batch size for training\nbatch_size = 8\n\n# Create an iterator of our data with torch DataLoader. This helps save on \n# memory during training because, unlike a for loop, \n# with an iterator the entire dataset does not need to be loaded into memory\n\ntrain_data = TensorDataset(X_train, X_train_masks, y_train)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data,\\\n                              sampler=train_sampler,\\\n                              batch_size=batch_size)\n\nvalidation_data = TensorDataset(X_val, X_val_masks, y_val)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data,\\\n                                   sampler=validation_sampler,\\\n                                   batch_size=batch_size)","4356da38":"class XLNetForMultiLabelSequenceClassification(torch.nn.Module):\n  \n  def __init__(self, num_labels=2):\n    super(XLNetForMultiLabelSequenceClassification, self).__init__()\n    self.num_labels = num_labels\n    self.xlnet = XLNetModel.from_pretrained('.\/xlnet-base-cased')\n    self.classifier = torch.nn.Linear(768, num_labels)\n    self.loss_fct = BCEWithLogitsLoss()\n    self.dropout = torch.nn.Dropout(0.2)\n    self.activation = torch.nn.ELU()\n    \n#     self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n#         self.activation = nn.Tanh()\n\n    torch.nn.init.xavier_normal_(self.classifier.weight)\n\n  def forward(self, input_ids, token_type_ids=None,\\\n              attention_mask=None, labels=None):\n    # last hidden layer\n    last_hidden_state = self.xlnet(input_ids=input_ids,\\\n                                   attention_mask=attention_mask,\\\n                                   token_type_ids=token_type_ids)\n    # pool the outputs into a mean vector\n    mean_last_hidden_state = self.pool_hidden_state(last_hidden_state)\n    mean_last_hidden_state = self.activation(mean_last_hidden_state)\n    mean_last_hidden_state = self.dropout(mean_last_hidden_state)\n    logits = self.classifier(mean_last_hidden_state)\n        \n    if labels is not None:\n      loss = self.loss_fct(logits.view(-1, self.num_labels),\\\n                      labels.view(-1, self.num_labels))\n      return loss\n    else:\n      return logits\n    \n  def freeze_xlnet_decoder(self):\n    \"\"\"\n    Freeze XLNet weight parameters. They will not be updated during training.\n    \"\"\"\n    for param in self.xlnet.parameters():\n      param.requires_grad = False\n    \n  def unfreeze_xlnet_decoder(self):\n    \"\"\"\n    Unfreeze XLNet weight parameters. They will be updated during training.\n    \"\"\"\n    for param in self.xlnet.parameters():\n      param.requires_grad = True\n    \n  def pool_hidden_state(self, last_hidden_state):\n    \"\"\"\n    Pool the output vectors into a single mean vector \n    \"\"\"\n    last_hidden_state = last_hidden_state[0]\n    mean_last_hidden_state = torch.mean(last_hidden_state[-128:], 1)\n#     mean_last_hidden_state = last_hidden_state[-1, :]\n    return mean_last_hidden_state\n\n# len(Y_train[0]) = 6\nmodel = XLNetForMultiLabelSequenceClassification(num_labels=len(y_train[0]))\n# model.freeze_xlnet_decoder()","039015e4":"# optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01, correct_bias=False)\n","34c6a403":"from scipy.stats import spearmanr\nrho_bar = []\ndef train(model, num_epochs,\\\n#           optimizer,\\\n          train_dataloader, valid_dataloader,\\\n          model_save_path,\\\n          train_loss_set=[], valid_loss_set = [],\\\n          lowest_eval_loss=None, start_epoch=0,\\\n          device=\"cpu\"\n          ):\n    \"\"\"\n    Train the model and save the model with the lowest validation loss\n    \"\"\"\n    crit_function = nn.BCEWithLogitsLoss()\n    model.to(device)\n    optimizer = AdamW(model.parameters(), lr=3.5e-5, weight_decay=0.01, correct_bias=False)\n#     optimizer = torch.optim.Adamax(model.parameters(), lr=3e-5)\n    # trange is a tqdm wrapper around the normal python range\n    for i in trange(num_epochs, desc=\"Epoch\"):\n        # if continue training from saved model\n        actual_epoch = start_epoch + i\n\n        # Training\n\n        # Set our model to training mode (as opposed to evaluation mode)\n        model.train()\n\n        # Tracking variables\n        tr_loss = 0\n        num_train_samples = 0\n\n        t = tqdm(total=len(train_data), desc=\"Training: \", position=0)\n        # Train the data for one epoch\n        for step, batch in enumerate(train_dataloader):\n            # Add batch to GPU\n            batch = tuple(t.to(device) for t in batch)\n            # Unpack the inputs from our dataloader\n            b_input_ids, b_input_mask, b_labels = batch\n            # Clear out the gradients (by default they accumulate)\n            optimizer.zero_grad()\n            # Forward pass\n            loss = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n            # store train loss\n            tr_loss += loss.item()\n            num_train_samples += b_labels.size(0)\n            # Backward pass\n            loss.backward()\n            # Update parameters and take a step using the computed gradient\n            optimizer.step()\n            #scheduler.step()\n            t.update(n=b_input_ids.shape[0])\n        t.close()\n        # Update tracking variables\n        epoch_train_loss = tr_loss\/num_train_samples\n        train_loss_set.append(epoch_train_loss)\n\n        print(\"Train loss: {}\".format(epoch_train_loss))\n\n        # Validation\n\n        # Put model in evaluation mode to evaluate loss on the validation set\n        model.eval()\n\n        # Tracking variables \n        eval_loss = 0\n        num_eval_samples = 0\n\n        v_preds = []\n        v_labels = []\n\n        # Evaluate data for one epoch\n        t = tqdm(total=len(validation_data), desc=\"Validating: \", position=0)\n        for batch in valid_dataloader:\n            # Add batch to GPU\n            batch = tuple(t.to(device) for t in batch)\n            # Unpack the inputs from our dataloader\n            b_input_ids, b_input_mask, b_labels = batch\n            # Telling the model not to compute or store gradients,\n            # saving memory and speeding up validation\n            with torch.no_grad():\n                # Forward pass, calculate validation loss\n                preds = model(b_input_ids, attention_mask=b_input_mask)\n                loss = crit_function(preds, b_labels)\n                v_labels.append(b_labels.cpu().numpy())\n                v_preds.append(preds.cpu().numpy())\n                # store valid loss\n                eval_loss += loss.item()\n                num_eval_samples += b_labels.size(0)\n            t.update(n=b_labels.shape[0])\n        t.close()\n\n        v_labels = np.vstack(v_labels)\n        v_preds = np.vstack(v_preds)\n        print(v_labels.shape)\n        print(v_preds.shape)\n        rho_val = np.mean([spearmanr(v_labels[:, ind] + np.random.normal(0, 1e-7, v_preds.shape[0]),\n                                            v_preds[:, ind] + np.random.normal(0, 1e-7, v_preds.shape[0])).correlation for ind in range(v_preds.shape[1])]\n                                )\n        rho_bar.append([spearmanr(v_labels[:, ind] + np.random.normal(0, 1e-7, v_preds.shape[0]),\n                                            v_preds[:, ind] + np.random.normal(0, 1e-7, v_preds.shape[0])).correlation for ind in range(v_preds.shape[1])])\n        epoch_eval_loss = eval_loss\/num_eval_samples\n        valid_loss_set.append(epoch_eval_loss)\n\n        print(\"Epoch #{}, training BCE loss: {}, validation BCE loss: ~{}, validation spearmanr: {}\"\\\n                .format(0, epoch_train_loss, epoch_eval_loss, rho_val))\n\n        if lowest_eval_loss == None:\n            lowest_eval_loss = epoch_eval_loss\n            # save model\n        #   save_model(model, model_save_path, actual_epoch,\\\n        #              lowest_eval_loss, train_loss_set, valid_loss_set)\n        else:\n            if epoch_eval_loss < lowest_eval_loss:\n                lowest_eval_loss = epoch_eval_loss\n            # save model\n            # save_model(model, model_save_path, actual_epoch,\\\n            #            lowest_eval_loss, train_loss_set, valid_loss_set)\n        print(\"\\n\")\n\n    return model, train_loss_set, valid_loss_set\n\n\ndef save_model(model, save_path, epochs, lowest_eval_loss, train_loss_hist, valid_loss_hist):\n  \"\"\"\n  Save the model to the path directory provided\n  Not used here!\n  \"\"\"\n  model_to_save = model.module if hasattr(model, 'module') else model\n  checkpoint = {'epochs': epochs, \\\n                'lowest_eval_loss': lowest_eval_loss,\\\n                'state_dict': model_to_save.state_dict(),\\\n                'train_loss_hist': train_loss_hist,\\\n                'valid_loss_hist': valid_loss_hist\n               }\n  torch.save(checkpoint, save_path)\n  print(\"Saving model at epoch {} with validation loss of {}\".format(epochs,\\\n                                                                     lowest_eval_loss))\n  return\n  \ndef load_model(save_path):\n  \"\"\"\n  Load the model from the path directory provided\n  Not used here!\n  \"\"\"\n  checkpoint = torch.load(save_path)\n  model_state_dict = checkpoint['state_dict']\n  model = XLNetForMultiLabelSequenceClassification(num_labels=model_state_dict[\"classifier.weight\"].size()[0])\n  model.load_state_dict(model_state_dict)\n\n  epochs = checkpoint[\"epochs\"]\n  lowest_eval_loss = checkpoint[\"lowest_eval_loss\"]\n  train_loss_hist = checkpoint[\"train_loss_hist\"]\n  valid_loss_hist = checkpoint[\"valid_loss_hist\"]\n  \n  return model, epochs, lowest_eval_loss, train_loss_hist, valid_loss_hist","36ba07eb":"cwd = os.getcwd()\nmodel_save_path = \".\/\"\nmodel, train_loss_set, valid_loss_set = train(model=model,\\\n                                              num_epochs = 4,\n#                                               optimizer = optimizer,\n                                              train_dataloader = train_dataloader,\n                                              valid_dataloader = validation_dataloader,\n                                              model_save_path = model_save_path,\n                                              device='cuda'\n                                              )","c4c2c038":"type(X_test)","392db0f4":"rho_bar\nfig = plt.figure()\nplt.figure(figsize=(100,10))\nfig.tight_layout()\nplt.bar(x = target_cols, height = np.mean(np.array(rho_bar), 0))\nplt.show()","f168d0bf":"test_data = TensorDataset(torch.tensor(X_test), torch.tensor(X_test_masks, dtype=torch.long))\ntest_dataloader = DataLoader(test_data,\n                                   shuffle=False,\n                                   batch_size=batch_size)","99fd4c3d":"\ndef generate_predictions(model, dataloader, num_labels, device=\"cpu\", batch_size=8):\n\n    pred_probs = np.array([]).reshape(0, num_labels)\n\n    model.to(device)\n    model.eval()\n\n    for X, masks in dataloader:\n        X = X.to(device)\n        masks = masks.to(device)\n        with torch.no_grad():\n          logits = model(input_ids=X, attention_mask=masks)\n          logits = logits.sigmoid().detach().cpu().numpy()\n          pred_probs = np.vstack([pred_probs, logits])\n    return pred_probs\nnum_labels = len(target_cols)\npred_probs = generate_predictions(model, test_dataloader, num_labels=30, device=\"cuda\", batch_size=8)","5952ec74":"df_submit[target_cols] = pred_probs\n","bf456a50":"df_submit.question_not_really_a_question = np.random.normal(0, 1e-10, df_submit.question_not_really_a_question.values.shape)\ndf_submit.question_type_spelling = np.random.normal(0, 1e-10, df_submit.question_type_spelling.values.shape)\n# Fix these value because in EDA, we saw these were almost all 0s.","42e20593":"df_submit.to_csv(\"submission.csv\", index = False)\ndf_submit","b7fc304a":"del model\ntorch.cuda.empty_cache()","edb30092":"# Conclusion:\n- As you can see, this model as of now does not perform very well. But I am pretty sure someone would figure out flaws in how this is implemented here. XLNet used to be one of the popular architectures in the Toxic comments classification competition.","685e1353":"#### Create Tokenizer for making inputs!","e671e326":"# Here, I attempt to use huggingface@github's implementation of XLNet!\n- Also, credit to [Josh Xin Jie Lee@Medium](https:\/\/towardsdatascience.com\/multi-label-text-classification-with-xlnet-b5f5755302df) for XLNetClassifier and other utility codes ","0c3b6f54":"## Utility Codes","26a78348":"## EDA: distribution of the outputs?","ee498682":"## SUBMIT!"}}