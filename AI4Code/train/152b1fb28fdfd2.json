{"cell_type":{"7582e2a6":"code","6fe02ca4":"code","ac89bdfa":"code","bcf6de4b":"code","301f7d84":"code","2d3d80c8":"code","9be1cfa3":"code","2ef262ae":"code","3bdb224c":"code","461be28a":"code","38b68289":"code","85a8839d":"code","ff9c2625":"code","94d18ddd":"code","65b819b1":"code","87298ddc":"code","d420368c":"code","0f2fc45b":"code","ab1b9e09":"code","c7edb617":"code","3527f26d":"code","f61a2d84":"code","5769534d":"code","591d22f8":"code","df50554b":"code","9dd13925":"code","a6b5b300":"code","cb9094f1":"code","739f4e5d":"code","da6ce7ad":"code","d8b24250":"code","d550bf9e":"code","39298c9b":"code","f81c69f9":"markdown","725b2cc8":"markdown","10b2119c":"markdown","f1f3925e":"markdown","cd1f65a9":"markdown","45bbdaf8":"markdown","3db63d61":"markdown","52cd7b5f":"markdown","e5829c8d":"markdown","c5a7e3c8":"markdown","c6be4718":"markdown","6c4913a7":"markdown","0198f420":"markdown","6e47fabc":"markdown","d786eccd":"markdown","f94426c2":"markdown","01883aae":"markdown","a21a5050":"markdown","0db1ad6a":"markdown","a7520ae8":"markdown","0cb9da6d":"markdown","442685b1":"markdown","695158d9":"markdown","2729b04e":"markdown","3359bf3a":"markdown","ee59ab06":"markdown","1e638669":"markdown","19dda1f7":"markdown","b4666034":"markdown","247a8494":"markdown","344a27d7":"markdown","9cd5b977":"markdown","aa25688c":"markdown","47648ce1":"markdown","3a6ed009":"markdown"},"source":{"7582e2a6":"!pip install texthero","6fe02ca4":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom PIL import Image\n\n#plotly\n!pip install chart_studio\nimport plotly.express as px\nimport chart_studio.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='white')\n\n\nimport re                                  \nimport string                              \nimport nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords        \nfrom nltk.stem import PorterStemmer        \nfrom nltk.tokenize import TweetTokenizer\nfrom textblob import TextBlob\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\nfrom datetime import datetime\nimport os\nimport glob\n\nfrom IPython.display import Markdown\ndef bold(string):\n    display(Markdown(string))","ac89bdfa":"# reading and concating subtitle dataset\npath = '..\/input\/chai-time-data-science\/Cleaned Subtitles' # use path\nall_files = glob.glob(path + \"\/*.csv\")\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, header=0)\n    li.append(df)\n\ndf_sub = pd.concat(li, axis=0, ignore_index=True)","bcf6de4b":"# reading episodes and desription dataset\ndf_eps = pd.read_csv('..\/input\/chai-time-data-science\/Episodes.csv')\ndf_desc = pd.read_csv('..\/input\/chai-time-data-science\/Description.csv')","301f7d84":"bold('**Preview of Sutitles Dataset**')\ndisplay(df_sub.head())\nbold('**Preview of Episodes Dataset**')\ndisplay(df_eps.head())\nbold('**Preview of Description Dataset**')\ndisplay(df_desc.head())","2d3d80c8":"def clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\ndef text_preprocessing(text):\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    #remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(tokenized_text)\n    return combined_text","9be1cfa3":"# applying the fuction\ndf_sub['clean_text'] = df_sub['Text'].apply(str).apply(lambda x: text_preprocessing(x))\ndf_desc['clean_description'] = df_desc['description'].apply(str).apply(lambda x: text_preprocessing(x))","2ef262ae":"# extracting Kaggle speakers\n[list(df_eps[df_eps['category']=='Kaggle']['heroes'])]","3bdb224c":"# creating new dataframe of kaggle speakers and subltite\ndf_sub.set_index('Speaker', inplace=True)\nkg_heroes_df = df_sub.loc[[\n  'Sanyam Bhutani',\n  'Abhishek Thakur',\n  'Ryan Chesler',\n  'Shivam Bansal',\n  'Andrew Lukyanenko',\n  'Dr. Vlamidir Iglovikov',\n  'Dr. Yury Kashnitsky',\n  'Robbert Bracco',\n  'Dr. Boris Dorado',\n  'Andres Torrubia',\n  'Philipp Singer',\n  'CPMP',\n  'Eugene Khvedchenya',\n  'Dr. Olivier Grellier',\n  'Gilberto Titericz',\n  'Dmitry Gordeev',\n  'Philipp Singer',\n  'Rohan Rao',\n  'Anthony Goldbloom',\n  'Anokas',\n  'Marios',\n  'John Miller',\n  'Christof',\n  'Inversion',\n  'Russ Wolfinger',\n  'Dmytro',\n  'Mark Landry',\n  'Dmitry Danevskiy',\n  'Yauhen Babakhin',\n  'Martin Henze',\n  'Max J',\n  'Dmitry Larko'\n  ],'clean_text'].to_frame('clean_text')\n\nkg_heroes_df.reset_index(inplace=True)\nkg_heroes_df","461be28a":"from wordcloud import WordCloud, STOPWORDS\n\nplt.figure(figsize=(20,50), dpi=100)\nfont = '..\/input\/boldfonts\/ColorTube-Regular.otf'\n\n\nplt.subplot(11,3,1)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Sanyam Bhutani']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, # Maximum numbers of words we want to see \n                       min_word_length=3, # Minimum numbers of letters of each word to be part of the cloud\n                       max_font_size=150, min_font_size=20,  # Font size range\n                       background_color=\"white\").generate(\" \".join(txt))\n\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words Used By Sanyam Bhutani\")\n\nplt.subplot(11,3,2)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Abhishek Thakur']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500,\n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20,  \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Abhishek Thakur\")\n\nplt.subplot(11,3,3)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Ryan Chesler']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Ryan Chesler\")\n\n\nplt.subplot(11,3,4)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Shivam Bansal']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Shivam Bansal\")\n\nplt.subplot(11,3,5)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Andrew Lukyanenko']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Andrew Lukyanenko\")\n\nplt.subplot(11,3,6)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Dr. Vlamidir Iglovikov']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Dr. Vlamidir Iglovikov\")\n\nplt.subplot(11,3,7)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Dr. Yury Kashnitsky']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Dr. Yury Kashnitsky\")\n\n\nplt.subplot(11,3,8)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Robbert Bracco']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Robbert Bracco\")\n\n\nplt.subplot(11,3,9)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Dr. Boris Dorado']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Dr. Boris Dorado\")\n\nplt.subplot(11,3,10)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Andres Torrubia']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Andres Torrubia\")\n\nplt.subplot(11,3,11)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Philipp Singer']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Philipp Singer\")\n\nplt.subplot(11,3,12)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='CPMP']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By CPMP\")\n\nplt.subplot(11,3,13)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Eugene Khvedchenya']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Eugene Khvedchenya\")\n\nplt.subplot(11,3,14)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Dr. Olivier Grellier']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Dr. Olivier Grellier\")\n\nplt.subplot(11,3,15)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Gilberto Titericz']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Gilberto Titericz\")\n\nplt.subplot(11,3,16)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Dmitry Gordeev']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Dmitry Gordeev\")\n\nplt.subplot(11,3,17)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Philipp Singer']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Philipp Singer\")\n\nplt.subplot(11,3,18)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Rohan Rao']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Rohan Rao\")\n\nplt.subplot(11,3,19)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Anthony Goldbloom']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Anthony Goldbloom\")\n\nplt.subplot(11,3,20)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Anokas']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Anokas\")\n\nplt.subplot(11,3,21)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Marios']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Marios\")\n\nplt.subplot(11,3,22)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='John Miller']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By John Miller\")\n\nplt.subplot(11,3,23)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Christof']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Christof\")\n\nplt.subplot(11,3,24)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Inversion']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Inversion\")\n\nplt.subplot(11,3,25)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Russ Wolfinger']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Russ Wolfinger\")\n\nplt.subplot(11,3,26)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Dmytro']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Dmytro\")\n\nplt.subplot(11,3,27)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Dmitry Danevskiy']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Dmitry Danevskiy\")\n\nplt.subplot(11,3,28)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Mark Landry']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Mark Landry\")\n\nplt.subplot(11,3,29)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Yauhen Babakhin']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Yauhen Babakhin\")\n\nplt.subplot(11,3,30)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Martin Henze']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Martin Henze\")\n\nplt.subplot(11,3,31)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Max J']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Max J\")\n\nplt.subplot(11,3,32)\ntxt = kg_heroes_df[kg_heroes_df['Speaker'] =='Dmitry Larko']['clean_text']\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='GnBu', \n                       margin=0,\n                       stopwords=STOPWORDS,\n                       max_words=500, \n                       min_word_length=3, \n                       max_font_size=150, min_font_size=20, \n                       background_color=\"white\").generate(\" \".join(txt))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words  Used By Dmitry Larko\")\n\nplt.show()","38b68289":"font = '..\/input\/boldfonts\/FFF_Tusj.ttf'\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='cividis', \n                       margin=0,\n                       max_words=500, # Maximum numbers of words we want to see \n                       min_word_length=3, # Minimum numbers of letters of each word to be part of the cloud\n                       max_font_size=150, min_font_size=20,  # Font size range\n                       background_color=\"white\").generate(\" \".join(df_desc['clean_description']))\n\nplt.figure(figsize=(10, 16))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words in Decription\", fontsize=15)\nplt.show()","85a8839d":"font = '..\/input\/boldfonts\/FFF_Tusj.ttf'\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='cividis', \n                       margin=0,\n                       max_words=500, # Maximum numbers of words we want to see \n                       min_word_length=3, # Minimum numbers of letters of each word to be part of the cloud\n                       max_font_size=150, min_font_size=20,  # Font size range\n                       background_color=\"white\").generate(\" \".join(df_eps['episode_name']))\n\nplt.figure(figsize=(10, 16))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.title(\"Frequent Words in Episode Name\", fontsize=15)\nplt.show()","ff9c2625":"# Extracting polarity, text length and Word Count\nkg_heroes_df['polarity'] = kg_heroes_df['clean_text'].map(lambda text: TextBlob(text).sentiment.polarity)\nkg_heroes_df['text_len'] = kg_heroes_df['clean_text'].astype(str).apply(len)\nkg_heroes_df['word_count'] = kg_heroes_df['clean_text'].apply(lambda x: len(str(x).split()))","94d18ddd":"kg_heroes_df['polarity'].iplot(\n    kind='hist',\n    bins=50,\n    xTitle='polarity',\n    linecolor='black',\n    color='dodgerblue',\n    yTitle='count',\n    title='Sentiment Polarity Distribution')","65b819b1":"kg_heroes_df['text_len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='review length',\n    linecolor='black',\n    color='dodgerblue',\n    yTitle='count',\n    title='Text Length Distribution')","87298ddc":"kg_heroes_df['word_count'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='word count',\n    linecolor='black',\n    color='dodgerblue',\n    yTitle='count',\n    title='Text Word Count Distribution')","d420368c":"def get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_bigram(kg_heroes_df['clean_text'], 20)\n\ndf_bi = pd.DataFrame(common_words, columns = ['Text' , 'count'])\ndf_bi.groupby('Text').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', color='dodgerblue', title='Top 20 bigrams in Text')","0f2fc45b":"def get_top_n_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_trigram(kg_heroes_df['clean_text'], 20)\n\ndf_tri = pd.DataFrame(common_words, columns = ['Text' , 'count'])\ndf_tri.groupby('Text').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', color='dodgerblue', title='Top 20 trigrams in Text')","ab1b9e09":"import texthero as hero\n\ntemp = kg_heroes_df.set_index('Speaker')\ntemp = temp.drop(index='Sanyam Bhutani')\ntemp.reset_index(inplace=True)\n\ntemp['pca'] = (\n   temp['clean_text']\n   .pipe(hero.tfidf)\n   .pipe(hero.pca)\n)\nhero.scatterplot(temp, 'pca', color='Speaker', title=\"PCA Speaker's Subtitle\")","c7edb617":"# Let's see the episode data\ndf_eps.info()","3527f26d":"# plot bar plot\nfig = go.Figure(data=[\n    go.Bar(x=df_eps.episode_id, \n           y=df_eps.youtube_impressions,\n           name='Youtube Impressions',\n           marker_color='#000000'),\n    go.Bar(x=df_eps.episode_id, \n           y=df_eps.youtube_impression_views,\n           name='Youtube Impression Views', \n           marker_color='#FF0000')\n])\n# Change the bar mode\nfig.update_layout(barmode='stack', template = 'plotly_white', width=700, height=700, title_text = 'Total Youtube Impressions And Impression Views',\n                  font=dict(family=\"Arial, Balto, Courier New, Droid Sans\",color='black'))\nfig.show()\n\n\n# plot of growth rate of confirmed cases\nfig1 = px.scatter(df_eps, \n                 x='episode_id', \n                  y=\"youtube_ctr\", \n                  text='youtube_ctr',)\nfig1.update_traces(marker=dict(size=3,\n                              line=dict(width=2,\n                                        color='DarkSlateGrey')),\n                  marker_color='#4169e1',\n                  mode='text+lines+markers',textposition='top center', )\n\nfig1.update_layout(template = 'plotly_white', width=700, height=700, title_text = 'Click Through Rate',\n                  font=dict(family=\"Arial, Balto, Courier New, Droid Sans\",color='black'))\nfig1.show()","f61a2d84":"# sorting \ncat_ctr = df_eps.sort_values(by = 'youtube_ctr',ascending = False)\n\n# plot\nfig = px.bar(cat_ctr, \n                 x='episode_id', \n                  y=\"youtube_ctr\", \n                 color='category')\n\nfig.update_layout(template = 'plotly_white',width=700, height=500, title_text = '<b>Category Wise Click Through Rate',\n                  font=dict(family=\"Arial, Balto, Courier New, Droid Sans\",color='black'))\nfig.show()","5769534d":"# plot bar plot\nfig = go.Figure(data=[\n    go.Bar(x=df_eps.episode_id, \n           y=df_eps.youtube_views,\n           name='Views',\n           marker_color='#008080'),\n    go.Bar(x=df_eps.episode_id, \n           y=df_eps.youtube_likes,\n           name='Likes', \n           marker_color='#ADFF2F'),\n    go.Bar(x=df_eps.episode_id, \n           y=df_eps.youtube_comments,\n           name='Comment', \n           marker_color='#FF4500')\n    \n])\n# Change the bar mode\nfig.update_layout(barmode='stack', template = 'plotly_white', width=700, height=700, title_text = 'Total Youtube View, Like, Comment',\n                  font=dict(family=\"Arial, Balto, Courier New, Droid Sans\",color='black'))\nfig.show()","591d22f8":"# sorting \ncat_views = df_eps.sort_values(by = 'youtube_views',ascending = False)\n\n# plot\nfig = px.bar(cat_views, \n            x='episode_id', \n            y='youtube_views', \n            color='category')\n\nfig.update_layout(template = 'plotly_white',width=700, height=500, title_text = '<b>Category Wise Views Distribution',\n                  font=dict(family=\"Arial, Balto, Courier New, Droid Sans\",color='black'))\nfig.show()","df50554b":"df_eps['recording_date'] = df_eps['recording_date'].apply(pd.to_datetime)\nfig = px.scatter(df_eps, \n            x='recording_date', \n            y='youtube_views')\nfig.update_traces(marker=dict(size=4.5),\n                  mode='markers',\n                  marker_color='#800080')\nfig.update_layout(template = 'plotly_white',width=700, height=500, title_text = '<b>Trend of Views over the year',\n                  font=dict(family=\"Arial, Balto, Courier New, Droid Sans\",color='black'))\nfig.show()","9dd13925":"fig = go.Figure(data=[\n    go.Scatter(x=df_eps.episode_id, \n           y=df_eps.episode_duration,\n           name='Episode Duration',\n           mode='lines', \n           line_color='indigo',\n           fill='tonexty'),\n    go.Scatter(x=df_eps.episode_id, \n           y=df_eps.youtube_avg_watch_duration,\n           name='Avg Watch Duration', \n           mode='lines', \n           line_color='blue',\n               fill='tonexty'),\n\n])\n\n\nfig.update_layout(template = 'plotly_white', width=700, height=700, title_text = 'Total Episodes Duration Vs Avg Watch Duration(In Sec)',\n                  font=dict(family=\"Arial, Balto, Courier New, Droid Sans\",color='black'))\nfig.show()\n\ndf_eps['youtube_watch_hours'].iplot(kind='area',\n                                        fill=True,\n                                        opacity=1,\n                                        color = 'blue',\n                                        xTitle='Episode',\n                                        yTitle='Duration(Hrs)',\n                                        title='Total watch hours on YouTube')","a6b5b300":"fig = px.bar(df_eps.sort_values('youtube_subscribers', ascending= False).sort_values('youtube_subscribers', ascending=True), \n             x=\"youtube_subscribers\", y=\"episode_id\", \n             title='New subscribers to YouTube channel', \n             text='youtube_subscribers', \n             orientation='h', \n             width=700, height=2000)\nfig.update_traces(marker_color='#FFA500', opacity=0.8, textposition='inside')\n\nfig.update_layout(template = 'plotly_white')\nfig.show()","cb9094f1":"# let's see the episode 27 video\nprint(df_eps[df_eps['episode_id'] == 'E27']['episode_name'])\nfrom IPython.display import IFrame, YouTubeVideo\nYouTubeVideo('205j37G1cxw',width=400, height=200)","739f4e5d":"# fit the regression\nfrom sklearn.linear_model import LinearRegression\nX = df_eps['youtube_watch_hours']\nY = df_eps['youtube_subscribers']\n\ntemp_df = pd.DataFrame({'youtube_watch_hours': X, 'youtube_subscribers':Y})\nreg = LinearRegression().fit(np.vstack(temp_df['youtube_watch_hours']), Y)\ntemp_df['bestfit'] = reg.predict(np.vstack(temp_df['youtube_watch_hours']))\n\n# plot\nfig = go.Figure(data=[\n    go.Scatter(x=temp_df['youtube_watch_hours'], \n               y=temp_df['youtube_subscribers'].values, \n               mode='markers',\n               name='Watch hours vs New subscribers', \n               marker_color='black'),\n    \n    go.Scatter(x=X, \n               y=temp_df['bestfit'],\n               name='line of best fit', \n               mode='lines', \n               marker_color='red'),\n])\n    \nfig.update_layout(template = 'plotly_white', width=700, height=500, title_text = 'Relationship Between Watch Hours and New Subcribers',\n                  xaxis_title = 'youtube_watch_hours', yaxis_title = 'youtube_subscribers',\n                  font=dict(family=\"Arial, Balto, Courier New, Droid Sans\",color='black'))\nfig.show()","da6ce7ad":"from sklearn.linear_model import LinearRegression\nX = df_eps['youtube_views']\nY = df_eps['youtube_likes']\n\ntemp_df = pd.DataFrame({'youtube_views': X, 'youtube_likes':Y})\nreg = LinearRegression().fit(np.vstack(temp_df['youtube_views']), Y)\ntemp_df['bestfit'] = reg.predict(np.vstack(temp_df['youtube_likes']))\n\n# plot\nfig = go.Figure(data=[\n    go.Scatter(x=temp_df['youtube_views'], \n               y=temp_df['youtube_likes'].values, \n               mode='markers',\n               name='Watch hours vs New subscribers', \n               marker_color='black'),\n    \n    go.Scatter(x=X, \n               y=temp_df['bestfit'],\n               name='line of best fit', \n               mode='lines', \n               marker_color='red'),\n])\n    \nfig.update_layout(template = 'plotly_white', width=700, height=500, title_text = 'Relationship Between Watch Hours and New Subcribers',\n                  xaxis_title = 'youtube_views', yaxis_title = 'youtube_likes',\n                  font=dict(family=\"Arial, Balto, Courier New, Droid Sans\",color='black'))\nfig.show()","d8b24250":"trace1 = go.Scatter(\n                x=df_eps['episode_id'],\n                y=df_eps['anchor_plays'],\n                name=\"Total Anchor Play\",\n                mode='lines+markers',\n                line_color='orange')\ntrace2 = go.Scatter(\n                x=df_eps['episode_id'],\n                y=df_eps['spotify_listeners'],\n                name=\"Spotify listeners\",\n                mode='lines+markers',\n                line_color='red')\n\ntrace3 = go.Scatter(\n                x=df_eps['episode_id'],\n                y=df_eps['apple_listeners'],\n                name=\"Apple listeners\",\n                mode='lines+markers',\n                line_color='green')\n\n\nlayout = go.Layout(template=\"plotly_white\", width=700, height=500, title_text = 'Number of unique listeners on spotify and Apple podcasts.<\/b>',\n                  font=dict(family=\"Arial, Balto, Courier New, Droid Sans\",color='black'))\nfig = go.Figure(data = [trace1,trace2,trace3], layout = layout)\nfig.show()","d550bf9e":"fig = go.Figure(data=[\n    go.Scatter(x=df_eps.episode_id, \n           y=df_eps.spotify_starts,\n           name='Spotify Starts(>= sec)',\n           mode='lines', \n           line_color='red',\n           fill='tonexty'),\n    go.Scatter(x=df_eps.episode_id, \n           y=df_eps.spotify_streams,\n           name='Spotify Streams(>=60 sec)', \n           mode='lines', \n           line_color='deeppink',\n               fill='tonexty'),\n\n])\n\n\nfig.update_layout(template = 'plotly_white', width=700, height=700, title_text = 'Spotify Starts(>= sec) Vs Spotify Streams(>=60 sec)',\n                  font=dict(family=\"Arial, Balto, Courier New, Droid Sans\",color='black'))\nfig.show()","39298c9b":"fig = go.Figure(data=[\n    go.Scatter(x=df_eps.episode_id, \n           y=df_eps.apple_listened_hours * 3600,\n           name='Total Lintened',\n           mode='lines', \n           line_color='orange',\n           fill='tonexty'),\n    go.Scatter(x=df_eps.episode_id, \n           y=df_eps.apple_avg_listen_duration,\n           name='Total Avg Listened', \n           mode='lines', \n           line_color='green',\n               fill='tonexty'),\n\n])\n\n\nfig.update_layout(template = 'plotly_white', width=700, height=700, title_text = 'Total Listen Vs Avg Listen',\n                  font=dict(family=\"Arial, Balto, Courier New, Droid Sans\",color='black'))\nfig.show()","f81c69f9":"**We can obverse that slightly positive slope so we can conclude that as views increases, likes also increases but not in the some proposition.**","725b2cc8":"## <font color='dodgerblue'><u>World Cloud<\/u><\/font>\nWord frequency can be used to list the most frequently occurring words or concepts in a given text. Word cloud tools, for example, are used to perform very basic text analysis techniques, like detecting keywords and phrases that appear most often in your data. <font color='maroon'>***Let's see the most commonly used to highlight popular or trending terms frequency used by kaggel speakers***<font\/>","10b2119c":"## <font color='dodgerblue'><u>Which Episode is Have Highest Click Through Rate?<\/u><\/font>\nOnce your video is published, YouTube starts surfacing it to relevant audiences, based on their behavior on YouTube (like what they watch, don\u2019t watch, what they search for, etc.). When a viewer comes across your video thumbnails on YouTube, it\u2019s called impressions, that is, how many times your video thumbnails are shown on YouTube. Each impression can be a potential chance to earn a youtube view. Clicking on a link, embedding video on blogs, youtube notifications etc don't count as a impression.\n\nclick-through rate shows you what percentage of your impressions on YouTube turned into views.\n\nYouTube's official Help Center answer is this: half of all channels and videos on YouTube have an <font color='maroon'>impressions click through rate that can range between 2% and 10%.<font\/> ","f1f3925e":"<font color='dodgerblue'>***Most Frequent Word used in episodes Title***<font\/>","cd1f65a9":"## <font color='dodgerblue'><u>Is there any relationship between Watch hours and New subcribers?<\/u><\/font>\n\nWe have fit regression model to find out the relationship between watch hours and new subcribers. X as watch hour and Y as new subcribers. ","45bbdaf8":"## <font color='dodgerblue'><u>Distribution <\/u><\/font>\n\n* Using TextBlob to calculate sentiment polarity which lies in the range of [-1,1] where 1 means positive sentiment and -1 means a negative sentiment.\n* Create new feature for the length of the text.\n* Create new feature for the word count of the text.","3db63d61":"## <font color='dodgerblue'><u>Is there any relationship between Views and Likes?<\/u><\/font>\n\nWe have fit regression model to find out the relationship between Views and Likes. X as views and Y as Likes. ","52cd7b5f":"## <font color='dodgerblue'><u>Which Episode is Have Highest View, Like, Comment?<\/u><\/font>\nLet's see which episode get highest view, like and coment.","e5829c8d":"## <font color='dodgerblue'><u>Episodes Duration<\/u><\/font>\n\nWatch time is important for the youtube creaters. youtube avg watch duration suggests amount of average time a user spends watching a youtube video. The average watch time indicator for quality of content","c5a7e3c8":"* Episode 27 have hisghest no. of listerns followed by episode one and seven.\n* spotify has more unique listeners than apple.","c6be4718":"<h1 style=\"background-color:powderblue;\">Exploration of Text data<\/h1>\n\nWe\u2019ll start the analysis with very simple text data like text length and word frequencies. Before we start with any NLP project we need to pre-process the data to get it all in a consistent format.We need to clean, tokenize and convert our data into a matrix. Let's create a function which will perform the following tasks on the text columns: Make text lowercase, removes hyperlinks, remove punctuation removes numbers tokenizes removes stopwords\n\n<font color='dodgerblue'>***For text analysis I have chooses only kaggle speaker***<font\/>\n","6c4913a7":"* Episode 23 is the longest interview.\n* But episode 27 get longest averge watch and total watch hours.","0198f420":"* Episode 27 have highest Youtube Impressions and Impressions View\n* But Episode 19 have highest click through rate.","6e47fabc":"We can see that industry category have highest CTR followed by the Kaggle and Others.","d786eccd":"We can see that industry category have highest Nuber of View followed by the Kaggle and Others.","f94426c2":"![](https:\/\/chaitimedatascience.com\/content\/images\/2020\/07\/ctds-1.png)\n\n**[Chai Time Data science show](https:\/\/www.youtube.com\/channel\/UCRjtBP-o5FbgRzX2BHQEFtQ) is a podcast series hosted by Sanyam Bhutani.. This show features interviews researchers, practitioners and Kagglers in Data science community.**\n\nIn this notebook we will be analyse the Chai time data science(CTDS) show dataset. We using visualiztion tools and methods for analyses and try to insights.","01883aae":"There were quite number of speakers like to speak less words.","a21a5050":"**Episode 27 Have add highest new subscriber to channel**","0db1ad6a":"Figure show that there is no clear distinguise cluters between the words of speakers.","a7520ae8":"<font color='dodgerblue'>***Most Frequent Word used in episodes Decription***<font\/>","0cb9da6d":"**Episode 27 has highest highest view, like and coment**","442685b1":"## <font color='dodgerblue'><u>Principal component analysis clustering<\/u><\/font>","695158d9":"## <font color='dodgerblue'><u>Which Episode have Highest Watch time on Apple Podcast?<\/u><\/font>","2729b04e":"<h1 style=\"background-color:powderblue;\">Reading Datasets<\/h1>","3359bf3a":"Trend of view seems like constant over the year","ee59ab06":"This is Data Science show so it is obvious that <font color='maroon'>Data Science and Machine learing<font\/> term used in the interviews.","1e638669":"<h1 style=\"background-color:powderblue;\">Importing Library<\/h1>","19dda1f7":"Vast majority of the sentiment polarity scores are greater than or equal to  zero, means most of them are <font color='maroon'>pretty positive.<font>","b4666034":"**We can obverse that as watch time increases new subscribers also increases so, we can say that there is positive relationship between watch time and new subscribers.**","247a8494":"## <font color='dodgerblue'><u>Ngram Analysis<\/u><\/font>\nNow we come to \u201cText\u201d feature, before explore this feature, we need to extract N-Gram features. N-grams are used to describe the number of words used as observation points, e.g., unigram means singly-worded, bigram means 2-worded phrase, and trigram means 3-worded phrase. In order to do this, we use scikit-learn\u2019s CountVectorizer function.","344a27d7":"## <font color='dodgerblue'><u>Which Episode have Great Start on Spotify Streams?<\/u><\/font>","9cd5b977":"## <font color='dodgerblue'><u>Audio Streams<\/u><\/font>\n\nLet's compare the no. of unique listeners on spotify and Apple podcasts.","aa25688c":"## <font color='dodgerblue'><u>Which Episodes Add Highest New subscribers to YouTube channel?<\/u><\/font>","47648ce1":"## <font color ='red'>Give me your feedback and if you find my kernel helpful please UPVOTE will be appreciated.<\/font>","3a6ed009":"<h1 style=\"background-color:powderblue;\">Exploration of Episodes Dataset<\/h1>"}}