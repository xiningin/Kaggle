{"cell_type":{"310a09e3":"code","8bfda0b8":"code","8a93a9d3":"code","51bdd7b9":"code","70b7376a":"code","6f4a0608":"code","2e8d580e":"code","48e277c9":"code","4d2c12fe":"code","79b77f64":"code","8622e2fb":"code","2adcfe6c":"code","718187f6":"code","12774347":"code","dc0866c4":"code","e3a2e833":"code","e571c8e5":"code","260e045c":"code","c73650e8":"code","c2e3227f":"code","a36dcf12":"code","5fd2dbea":"code","54065711":"code","04b6a20c":"code","e73e216b":"code","a9a6c227":"code","cfbbb7dd":"code","9e8de94d":"code","f206075e":"code","8f89836e":"code","a2867a6c":"code","237ed633":"code","53fa45dd":"code","d3cb46c4":"code","97acf851":"code","b83d4fac":"code","452ee8f5":"code","1cd98a82":"markdown","b640ad4a":"markdown","96a7450f":"markdown","c646e450":"markdown","1cbcc968":"markdown","652edcf8":"markdown","c964580d":"markdown"},"source":{"310a09e3":"SAMPLE = False # set True for debugging","8bfda0b8":"!pip install seqeval -qq # evaluation metrics for training (not the competition metric)\n!pip install --upgrade wandb -qq # experiment tracking","8a93a9d3":"# setup wandb for experiment tracking\n# source: https:\/\/www.kaggle.com\/debarshichanda\/pytorch-w-b-jigsaw-starter\n\nimport wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    wandb.init(project=\"feedback_prize\", entity=\"darek\")\n    anony = None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')","51bdd7b9":"# CONFIG\n\nEXP_NUM = 4\ntask = \"ner\"\nmodel_checkpoint = \"allenai\/longformer-base-4096\"\nmax_length = 1024\nstride = 128\nmin_tokens = 6\nmodel_path = f'{model_checkpoint.split(\"\/\")[-1]}-{EXP_NUM}'\n\n# TRAINING HYPERPARAMS\nBS = 4\nGRAD_ACC = 8\nLR = 5e-5\nWD = 0.01\nWARMUP = 0.1\nN_EPOCHS = 5","70b7376a":"import pandas as pd\n\n# read train data\ntrain = pd.read_csv('..\/input\/feedback-prize-2021\/train.csv')\ntrain.head(1)","6f4a0608":"# check unique classes\nclasses = train.discourse_type.unique().tolist()\nclasses","2e8d580e":"# setup label indices\n\nfrom collections import defaultdict\ntags = defaultdict()\n\nfor i, c in enumerate(classes):\n    tags[f'B-{c}'] = i\n    tags[f'I-{c}'] = i + len(classes)\ntags[f'O'] = len(classes) * 2\ntags[f'Special'] = -100\n    \nl2i = dict(tags)\n\ni2l = defaultdict()\nfor k, v in l2i.items(): \n    i2l[v] = k\ni2l[-100] = 'Special'\n\ni2l = dict(i2l)\n\nN_LABELS = len(i2l) - 1 # not accounting for -100","48e277c9":"# some helper functions\n\nfrom pathlib import Path\n\npath = Path('..\/input\/feedback-prize-2021\/train')\n\ndef get_raw_text(ids):\n    with open(path\/f'{ids}.txt', 'r') as file: data = file.read()\n    return data","4d2c12fe":"# group training labels by text file\n\ndf1 = train.groupby('id')['discourse_type'].apply(list).reset_index(name='classlist')\ndf2 = train.groupby('id')['discourse_start'].apply(list).reset_index(name='starts')\ndf3 = train.groupby('id')['discourse_end'].apply(list).reset_index(name='ends')\ndf4 = train.groupby('id')['predictionstring'].apply(list).reset_index(name='predictionstrings')\n\ndf = pd.merge(df1, df2, how='inner', on='id')\ndf = pd.merge(df, df3, how='inner', on='id')\ndf = pd.merge(df, df4, how='inner', on='id')\ndf['text'] = df['id'].apply(get_raw_text)\n\ndf.head()","79b77f64":"# debugging\nif SAMPLE: df = df.sample(n=100).reset_index(drop=True)","8622e2fb":"# we will use HuggingFace datasets\nfrom datasets import Dataset, load_metric\n\nds = Dataset.from_pandas(df)\ndatasets = ds.train_test_split(test_size=0.1, shuffle=True, seed=42)\ndatasets","2adcfe6c":"from transformers import AutoTokenizer\n    \ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)","718187f6":"# Not sure if this is needed, but in case we create a span with certain class without starting token of that class,\n# let's convert the first token to be the starting token.\n\ne = [0,7,7,7,1,1,8,8,8,9,9,9,14,4,4,4]\n\ndef fix_beginnings(labels):\n    for i in range(1,len(labels)):\n        curr_lab = labels[i]\n        prev_lab = labels[i-1]\n        if curr_lab in range(7,14):\n            if prev_lab != curr_lab and prev_lab != curr_lab - 7:\n                labels[i] = curr_lab -7\n    return labels\n\nfix_beginnings(e)","12774347":"# tokenize and add labels\ndef tokenize_and_align_labels(examples):\n\n    o = tokenizer(examples['text'], truncation=True, padding=True, return_offsets_mapping=True, max_length=max_length, stride=stride, return_overflowing_tokens=True)\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = o[\"overflow_to_sample_mapping\"]\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = o[\"offset_mapping\"]\n    \n    o[\"labels\"] = []\n\n    for i in range(len(offset_mapping)):\n                   \n        sample_index = sample_mapping[i]\n\n        labels = [l2i['O'] for i in range(len(o['input_ids'][i]))]\n\n        for label_start, label_end, label in \\\n        list(zip(examples['starts'][sample_index], examples['ends'][sample_index], examples['classlist'][sample_index])):\n            for j in range(len(labels)):\n                token_start = offset_mapping[i][j][0]\n                token_end = offset_mapping[i][j][1]\n                if token_start == label_start: \n                    labels[j] = l2i[f'B-{label}']    \n                if token_start > label_start and token_end <= label_end: \n                    labels[j] = l2i[f'I-{label}']\n\n        for k, input_id in enumerate(o['input_ids'][i]):\n            if input_id in [0,1,2]:\n                labels[k] = -100\n\n        labels = fix_beginnings(labels)\n                   \n        o[\"labels\"].append(labels)\n        \n    return o","dc0866c4":"tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True, \\\n                                  batch_size=20000, remove_columns=datasets[\"train\"].column_names)","e3a2e833":"tokenized_datasets","e571c8e5":"# we will use auto model for token classification\n\nfrom transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n\nmodel = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=N_LABELS)","260e045c":"model_name = model_checkpoint.split(\"\/\")[-1]\nargs = TrainingArguments(\n    f\"{model_name}-finetuned-{task}\",\n    evaluation_strategy = \"epoch\",\n    logging_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=LR,\n    per_device_train_batch_size=BS,\n    per_device_eval_batch_size=BS,\n    num_train_epochs=N_EPOCHS,\n    weight_decay=WD,\n    report_to='wandb', \n    gradient_accumulation_steps=GRAD_ACC,\n    warmup_ratio=WARMUP\n)","c73650e8":"from transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(tokenizer)","c2e3227f":"# this is not the competition metric, but for now this will be better than nothing...\n\nmetric = load_metric(\"seqeval\")","a36dcf12":"import numpy as np\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [i2l[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [i2l[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }","5fd2dbea":"trainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics, \n)","54065711":"trainer.train()\nwandb.finish()","04b6a20c":"trainer.save_model(model_path)","e73e216b":"def tokenize_for_validation(examples):\n\n    o = tokenizer(examples['text'], truncation=True, return_offsets_mapping=True, max_length=4096)\n\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = o[\"offset_mapping\"]\n    \n    o[\"labels\"] = []\n\n    for i in range(len(offset_mapping)):\n                   \n        labels = [l2i['O'] for i in range(len(o['input_ids'][i]))]\n\n        for label_start, label_end, label in \\\n        list(zip(examples['starts'][i], examples['ends'][i], examples['classlist'][i])):\n            for j in range(len(labels)):\n                token_start = offset_mapping[i][j][0]\n                token_end = offset_mapping[i][j][1]\n                if token_start == label_start: \n                    labels[j] = l2i[f'B-{label}']    \n                if token_start > label_start and token_end <= label_end: \n                    labels[j] = l2i[f'I-{label}']\n\n        for k, input_id in enumerate(o['input_ids'][i]):\n            if input_id in [0,1,2]:\n                labels[k] = -100\n\n        labels = fix_beginnings(labels)\n                   \n        o[\"labels\"].append(labels)\n        \n    return o","a9a6c227":"tokenized_val = datasets.map(tokenize_for_validation, batched=True)\ntokenized_val","cfbbb7dd":"# ground truth for validation\n\nl = []\nfor example in tokenized_val['test']:\n    for c, p in list(zip(example['classlist'], example['predictionstrings'])):\n        l.append({\n            'id': example['id'],\n            'discourse_type': c,\n            'predictionstring': p,\n        })\n    \ngt_df = pd.DataFrame(l)\ngt_df","9e8de94d":"# visualization with displacy\n\nimport pandas as pd\nimport os\nfrom pathlib import Path\nimport spacy\nfrom spacy import displacy\nfrom pylab import cm, matplotlib","f206075e":"path = Path('..\/input\/feedback-prize-2021\/train')\n\ncolors = {\n            'Lead': '#8000ff',\n            'Position': '#2b7ff6',\n            'Evidence': '#2adddd',\n            'Claim': '#80ffb4',\n            'Concluding Statement': 'd4dd80',\n            'Counterclaim': '#ff8042',\n            'Rebuttal': '#ff0000',\n            'Other': '#007f00',\n         }\n\ndef visualize(df, text):\n    ents = []\n    example = df['id'].loc[0]\n\n    for i, row in df.iterrows():\n        ents.append({\n                        'start': int(row['discourse_start']), \n                         'end': int(row['discourse_end']), \n                         'label': row['discourse_type']\n                    })\n\n    doc2 = {\n        \"text\": text,\n        \"ents\": ents,\n        \"title\": example\n    }\n\n    options = {\"ents\": train.discourse_type.unique().tolist() + ['Other'], \"colors\": colors}\n    displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)","8f89836e":"predictions, labels, _ = trainer.predict(tokenized_val['test'])","a2867a6c":"preds = np.argmax(predictions, axis=-1)\npreds.shape","237ed633":"# code that will convert our predictions into prediction strings, and visualize it at the same time\n# this most likely requires some refactoring\n\ndef get_class(c):\n    if c == 14: return 'Other'\n    else: return i2l[c][2:]\n\ndef pred2span(pred, example, viz=False, test=False):\n    example_id = example['id']\n    n_tokens = len(example['input_ids'])\n    classes = []\n    all_span = []\n    for i, c in enumerate(pred.tolist()):\n        if i == n_tokens-1:\n            break\n        if i == 0:\n            cur_span = example['offset_mapping'][i]\n            classes.append(get_class(c))\n        elif i > 0 and (c == pred[i-1] or (c-7) == pred[i-1]):\n            cur_span[1] = example['offset_mapping'][i][1]\n        else:\n            all_span.append(cur_span)\n            cur_span = example['offset_mapping'][i]\n            classes.append(get_class(c))\n    all_span.append(cur_span)\n    \n    if test: text = get_test_text(example_id)\n    else: text = get_raw_text(example_id)\n    \n    # abra ka dabra se soli fanta ko pelo\n    \n    # map token ids to word (whitespace) token ids\n    predstrings = []\n    for span in all_span:\n        span_start = span[0]\n        span_end = span[1]\n        before = text[:span_start]\n        token_start = len(before.split())\n        if len(before) == 0: token_start = 0\n        elif before[-1] != ' ': token_start -= 1\n        num_tkns = len(text[span_start:span_end+1].split())\n        tkns = [str(x) for x in range(token_start, token_start+num_tkns)]\n        predstring = ' '.join(tkns)\n        predstrings.append(predstring)\n                    \n    rows = []\n    for c, span, predstring in zip(classes, all_span, predstrings):\n        e = {\n            'id': example_id,\n            'discourse_type': c,\n            'predictionstring': predstring,\n            'discourse_start': span[0],\n            'discourse_end': span[1],\n            'discourse': text[span[0]:span[1]+1]\n        }\n        rows.append(e)\n\n\n    df = pd.DataFrame(rows)\n    df['length'] = df['discourse'].apply(lambda t: len(t.split()))\n    \n    # short spans are likely to be false positives, we can choose a min number of tokens based on validation\n    df = df[df.length > min_tokens].reset_index(drop=True)\n    if viz: visualize(df, text)\n\n    return df","53fa45dd":"pred2span(preds[0], tokenized_val['test'][0], viz=True)","d3cb46c4":"pred2span(preds[1], tokenized_val['test'][1], viz=True)","97acf851":"dfs = []\nfor i in range(len(tokenized_val['test'])):\n    dfs.append(pred2span(preds[i], tokenized_val['test'][i]))\n\npred_df = pd.concat(dfs, axis=0)\npred_df['class'] = pred_df['discourse_type']\npred_df","b83d4fac":"# source: https:\/\/www.kaggle.com\/robikscube\/student-writing-competition-twitch#Competition-Metric-Code\n\ndef calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(\" \"))\n    set_gt = set(row.predictionstring_gt.split(\" \"))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter \/ len_gt\n    overlap_2 = inter \/ len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp_micro(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n\n    Uses the steps in the evaluation page here:\n        https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/overview\/evaluation\n    \"\"\"\n    gt_df = (\n        gt_df[[\"id\", \"discourse_type\", \"predictionstring\"]]\n        .reset_index(drop=True)\n        .copy()\n    )\n    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n    pred_df[\"pred_id\"] = pred_df.index\n    gt_df[\"gt_id\"] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(\n        gt_df,\n        left_on=[\"id\", \"class\"],\n        right_on=[\"id\", \"discourse_type\"],\n        how=\"outer\",\n        suffixes=(\"_pred\", \"_gt\"),\n    )\n    joined[\"predictionstring_gt\"] = joined[\"predictionstring_gt\"].fillna(\" \")\n    joined[\"predictionstring_pred\"] = joined[\"predictionstring_pred\"].fillna(\" \")\n\n    joined[\"overlaps\"] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined[\"overlap1\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[0])\n    joined[\"overlap2\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[1])\n\n    joined[\"potential_TP\"] = (joined[\"overlap1\"] >= 0.5) & (joined[\"overlap2\"] >= 0.5)\n    joined[\"max_overlap\"] = joined[[\"overlap1\", \"overlap2\"]].max(axis=1)\n    tp_pred_ids = (\n        joined.query(\"potential_TP\")\n        .sort_values(\"max_overlap\", ascending=False)\n        .groupby([\"id\", \"predictionstring_gt\"])\n        .first()[\"pred_id\"]\n        .values\n    )\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined[\"pred_id\"].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query(\"potential_TP\")[\"gt_id\"].unique()\n    unmatched_gt_ids = [c for c in joined[\"gt_id\"].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    # calc microf1\n    my_f1_score = TP \/ (TP + 0.5 * (FP + FN))\n    return my_f1_score\n\n\ndef score_feedback_comp(pred_df, gt_df, return_class_scores=False):\n    class_scores = {}\n    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n    for discourse_type, gt_subset in gt_df.groupby(\"discourse_type\"):\n        pred_subset = (\n            pred_df.loc[pred_df[\"class\"] == discourse_type]\n            .reset_index(drop=True)\n            .copy()\n        )\n        class_score = score_feedback_comp_micro(pred_subset, gt_subset)\n        class_scores[discourse_type] = class_score\n    f1 = np.mean([v for v in class_scores.values()])\n    if return_class_scores:\n        return f1, class_scores\n    return f1","452ee8f5":"score_feedback_comp(pred_df, gt_df, return_class_scores=True)","1cd98a82":"## End\n\nI'll appreciate every upvote or comment!","b640ad4a":"## Validation","96a7450f":"## Data Preprocessing","c646e450":"## Setup","1cbcc968":"## Model and Training","652edcf8":"# HuggingFace Training Baseline\n\nI wanted to create my own baseline for this competition, and I tried to do so \"without peeking\" at the kernels published by others. Ideally this can be used for training on a Kaggle kernel. Let's see how good we can get. \n\nThis baseline is based on the following notebook by Sylvain Gugger: https:\/\/github.com\/huggingface\/notebooks\/blob\/master\/examples\/token_classification.ipynb\n\nI initially started building with Roberta - thanks to Chris Deotte for pointing to Longformer :) The evaluation code is from Rob Mulla.\n\nThe notebook requires a couple of hours to run, so we'll use W&B to be able to monitor it along the way and keep the record of our experiments. ","c964580d":"## CV Score"}}