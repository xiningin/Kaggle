{"cell_type":{"7444fb3f":"code","897eda7a":"code","8742f019":"code","4e70425a":"code","9d5d30f6":"code","78e18fd8":"code","6504943f":"code","5403f127":"code","975c7a8e":"code","e688efbb":"code","140a7d3d":"code","99e9c8cb":"code","fe14c9ea":"code","3b433946":"code","4f9cff27":"code","40441918":"code","62932721":"code","c328ecdf":"code","55063feb":"code","1e4b7ab3":"code","39cccf00":"code","d1596d1e":"code","f23a0d86":"code","0206cec5":"code","a4e8e47b":"code","51a2ac71":"code","52af4ceb":"code","ad910b61":"code","ad754a2b":"code","1b50dd7a":"code","89106826":"code","035dbe5d":"code","5757c0c9":"code","c0a85f03":"code","488b3d23":"code","5dbcf0b5":"code","8733efe5":"code","918fa72e":"code","d3c50d50":"code","90e12a68":"code","49a4e996":"code","b3185598":"code","86a90501":"code","411decaf":"markdown","034de214":"markdown","1a44db31":"markdown","37e5f2f0":"markdown","0fcdc4a1":"markdown","6039c1e2":"markdown","eaf6c935":"markdown","911164dd":"markdown","af66fb49":"markdown","ab0775aa":"markdown","5be8fdd7":"markdown"},"source":{"7444fb3f":"# Import libraries \nimport librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport glob \nfrom sklearn.metrics import confusion_matrix\nimport IPython.display as ipd  # To play sound in the notebook\nimport os\nimport sys\nimport warnings\n# ignore warnings \nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","897eda7a":"#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\nTESS = \"\/kaggle\/input\/toronto-emotional-speech-set-tess\/tess toronto emotional speech set data\/TESS Toronto emotional speech set data\/\"\nRAV = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/\"\nSAVEE = \"\/kaggle\/input\/surrey-audiovisual-expressed-emotion-savee\/ALL\/\"\nCREMA = \"\/kaggle\/input\/cremad\/AudioWAV\/\"\n\n# Run one example \ndir_list = os.listdir(SAVEE)\ndir_list[0:5]","8742f019":"# Get the data location for SAVEE\ndir_list = os.listdir(SAVEE)\n\n# parse the filename to get the emotions\nemotion=[]\npath = []\nfor i in dir_list:\n    if i[-8:-6]=='_a':\n        emotion.append('male_angry')\n    elif i[-8:-6]=='_d':\n        emotion.append('male_disgust')\n    elif i[-8:-6]=='_f':\n        emotion.append('male_fear')\n    elif i[-8:-6]=='_h':\n        emotion.append('male_happy')\n    elif i[-8:-6]=='_n':\n        emotion.append('male_neutral')\n    elif i[-8:-6]=='sa':\n        emotion.append('male_sad')\n    elif i[-8:-6]=='su':\n        emotion.append('male_surprise')\n    else:\n        emotion.append('male_error') \n    path.append(SAVEE + i)\n    \n# Now check out the label count distribution \nSAVEE_df = pd.DataFrame(emotion, columns = ['labels'])\nSAVEE_df['source'] = 'SAVEE'\nSAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])], axis = 1)\nSAVEE_df.labels.value_counts()","4e70425a":"dir_list = os.listdir(RAV)\ndir_list.sort()\n\nemotion = []\ngender = []\npath = []\nfor i in dir_list:\n    fname = os.listdir(RAV + i)\n    for f in fname:\n        part = f.split('.')[0].split('-')\n        emotion.append(int(part[2]))\n        temp = int(part[6])\n        if temp%2 == 0:\n            temp = \"female\"\n        else:\n            temp = \"male\"\n        gender.append(temp)\n        path.append(RAV + i + '\/' + f)\n\n        \nRAV_df = pd.DataFrame(emotion)\nRAV_df = RAV_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\nRAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\nRAV_df.columns = ['gender','emotion']\nRAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\nRAV_df['source'] = 'RAVDESS'  \nRAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nRAV_df = RAV_df.drop(['gender', 'emotion'], axis=1)\nRAV_df.labels.value_counts()","9d5d30f6":"dir_list = os.listdir(TESS)\ndir_list.sort()\ndir_list","78e18fd8":"path = []\nemotion = []\n\nfor i in dir_list:\n    fname = os.listdir(TESS + i)\n    for f in fname:\n        if i == 'OAF_angry' or i == 'YAF_angry':\n            emotion.append('female_angry')\n        elif i == 'OAF_disgust' or i == 'YAF_disgust':\n            emotion.append('female_disgust')\n        elif i == 'OAF_Fear' or i == 'YAF_fear':\n            emotion.append('female_fear')\n        elif i == 'OAF_happy' or i == 'YAF_happy':\n            emotion.append('female_happy')\n        elif i == 'OAF_neutral' or i == 'YAF_neutral':\n            emotion.append('female_neutral')                                \n        elif i == 'OAF_Pleasant_surprise' or i == 'YAF_pleasant_surprised':\n            emotion.append('female_surprise')               \n        elif i == 'OAF_Sad' or i == 'YAF_sad':\n            emotion.append('female_sad')\n        else:\n            emotion.append('Unknown')\n        path.append(TESS + i + \"\/\" + f)\n\nTESS_df = pd.DataFrame(emotion, columns = ['labels'])\nTESS_df['source'] = 'TESS'\nTESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nTESS_df.labels.value_counts()","6504943f":"dir_list = os.listdir(CREMA)\ndir_list.sort()\nprint(dir_list[0:10])","5403f127":"gender = []\nemotion = []\npath = []\nfemale = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,1024,1025,1028,1029,1030,1037,1043,1046,1047,1049,\n          1052,1053,1054,1055,1056,1058,1060,1061,1063,1072,1073,1074,1075,1076,1078,1079,1082,1084,1089,1091]\n\nfor i in dir_list: \n    part = i.split('_')\n    if int(part[0]) in female:\n        temp = 'female'\n    else:\n        temp = 'male'\n    gender.append(temp)\n    if part[2] == 'SAD' and temp == 'male':\n        emotion.append('male_sad')\n    elif part[2] == 'ANG' and temp == 'male':\n        emotion.append('male_angry')\n    elif part[2] == 'DIS' and temp == 'male':\n        emotion.append('male_disgust')\n    elif part[2] == 'FEA' and temp == 'male':\n        emotion.append('male_fear')\n    elif part[2] == 'HAP' and temp == 'male':\n        emotion.append('male_happy')\n    elif part[2] == 'NEU' and temp == 'male':\n        emotion.append('male_neutral')\n    elif part[2] == 'SAD' and temp == 'female':\n        emotion.append('female_sad')\n    elif part[2] == 'ANG' and temp == 'female':\n        emotion.append('female_angry')\n    elif part[2] == 'DIS' and temp == 'female':\n        emotion.append('female_disgust')\n    elif part[2] == 'FEA' and temp == 'female':\n        emotion.append('female_fear')\n    elif part[2] == 'HAP' and temp == 'female':\n        emotion.append('female_happy')\n    elif part[2] == 'NEU' and temp == 'female':\n        emotion.append('female_neutral')\n    else:\n        emotion.append('Unknown')\n    path.append(CREMA + i)\n    \nCREMA_df = pd.DataFrame(emotion, columns = ['labels'])\nCREMA_df['source'] = 'CREMA'\nCREMA_df = pd.concat([CREMA_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nCREMA_df.labels.value_counts()","975c7a8e":"df = pd.concat([SAVEE_df, RAV_df, TESS_df, CREMA_df], axis = 0)\nprint(df.labels.value_counts())\ndf.head()\ndf.to_csv(\"Data_path.csv\",index=False)","e688efbb":"ref = pd.read_csv(\"Data_path.csv\")\nref.head()","140a7d3d":"df = pd.DataFrame(columns=['feature'])\n\n# loop feature extraction over the entire dataset\ncounter=0\nfor index,path in enumerate(ref.path):\n    X, sample_rate = librosa.load(path\n                                  , res_type='kaiser_fast'\n                                  ,duration=3\n                                  ,sr=44100\n                                  ,offset=0.5\n                                 )\n    sample_rate = np.array(sample_rate)\n    \n    # mean as the feature. Could do min and max etc as well. \n    mfccs = np.mean(librosa.feature.mfcc(y=X, \n                                        sr=sample_rate, \n                                        n_mfcc=13),\n                    axis=0)\n    df.loc[counter] = [mfccs]\n    counter=counter+1   \n\n# Check a few records to make sure its processed successfully\nprint(len(df))\ndf.head()","99e9c8cb":"df = pd.concat([ref,pd.DataFrame(df['feature'].values.tolist())],axis=1)\ndf.head()","fe14c9ea":"df.isnull().sum()","3b433946":"df=df.fillna(0)\nprint(df.shape)","4f9cff27":"import keras\nfrom keras import regularizers\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint\n\n# sklearn\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Other  \nimport librosa\nimport librosa.display\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport seaborn as sns\nimport glob \nimport os\nimport pickle\nimport IPython.display as ipd  ","40441918":"def noise(data):\n    \"\"\"\n    Adding White Noise.\n    \"\"\"\n    # you can take any distribution from https:\/\/docs.scipy.org\/doc\/numpy-1.13.0\/reference\/routines.random.html\n    noise_amp = 0.05*np.random.uniform()*np.amax(data)   # more noise reduce the value to 0.5\n    data = data.astype('float64') + noise_amp * np.random.normal(size=data.shape[0])\n    return data\n    \ndef shift(data):\n    \"\"\"\n    Random Shifting.\n    \"\"\"\n    s_range = int(np.random.uniform(low=-5, high = 5)*1000)  #default at 500\n    return np.roll(data, s_range)\n    \ndef stretch(data, rate=0.8):\n    \"\"\"\n    Streching the Sound. Note that this expands the dataset slightly\n    \"\"\"\n    data = librosa.effects.time_stretch(data, rate)\n    return data\n    \ndef pitch(data, sample_rate):\n    \"\"\"\n    Pitch Tuning.\n    \"\"\"\n    bins_per_octave = 12\n    pitch_pm = 2\n    pitch_change =  pitch_pm * 2*(np.random.uniform())   \n    data = librosa.effects.pitch_shift(data.astype('float64'), \n                                      sample_rate, n_steps=pitch_change, \n                                      bins_per_octave=bins_per_octave)\n    return data\n    \ndef dyn_change(data):\n    \"\"\"\n    Random Value Change.\n    \"\"\"\n    dyn_change = np.random.uniform(low=-0.5 ,high=7)  # default low = 1.5, high = 3\n    return (data * dyn_change)\n    \ndef speedNpitch(data):\n    \"\"\"\n    peed and Pitch Tuning.\n    \"\"\"\n    # you can change low and high here\n    length_change = np.random.uniform(low=0.8, high = 1)\n    speed_fac = 1.2  \/ length_change # try changing 1.0 to 2.0 ... =D\n    tmp = np.interp(np.arange(0,len(data),speed_fac),np.arange(0,len(data)),data)\n    minlen = min(data.shape[0], tmp.shape[0])\n    data *= 0\n    data[0:minlen] = tmp[0:minlen]\n    return data\n","62932721":"ref = pd.read_csv(\"Data_path.csv\")\nref.head()","c328ecdf":"from tqdm import tqdm","55063feb":"df = pd.DataFrame(columns=['feature'])\ndf_noise = pd.DataFrame(columns=['feature'])\ndf_speedpitch = pd.DataFrame(columns=['feature'])\ncnt = 0\n\n# loop feature extraction over the entire dataset\nfor i in tqdm(ref.path):\n    \n    # first load the audio \n    X, sample_rate = librosa.load(i\n                                  , res_type='kaiser_fast'\n                                  ,duration=3\n                                  ,sr=44100\n                                  ,offset=0.5\n                                 )\n\n    # take mfcc and mean as the feature. Could do min and max etc as well. \n    mfccs = np.mean(librosa.feature.mfcc(y=X, \n                                        sr=np.array(sample_rate), \n                                        n_mfcc=13),\n                    axis=0)\n    \n    df.loc[cnt] = [mfccs]   \n\n    # random shifting (omit for now)\n    # Stretch\n    # pitch (omit for now)\n    # dyn change\n    \n    # noise \n    aug = noise(X)\n    aug = np.mean(librosa.feature.mfcc(y=aug, \n                                    sr=np.array(sample_rate), \n                                    n_mfcc=13),    \n                  axis=0)\n    df_noise.loc[cnt] = [aug]\n\n    # speed pitch\n    aug = speedNpitch(X)\n    aug = np.mean(librosa.feature.mfcc(y=aug, \n                                    sr=np.array(sample_rate), \n                                    n_mfcc=13),    \n                  axis=0)\n    df_speedpitch.loc[cnt] = [aug]   \n\n    cnt += 1\n\ndf.head()","1e4b7ab3":"df = pd.concat([ref,pd.DataFrame(df['feature'].values.tolist())],axis=1)\ndf_noise = pd.concat([ref,pd.DataFrame(df_noise['feature'].values.tolist())],axis=1)\ndf_speedpitch = pd.concat([ref,pd.DataFrame(df_speedpitch['feature'].values.tolist())],axis=1)\nprint(df.shape,df_noise.shape,df_speedpitch.shape)","39cccf00":"df = pd.concat([df,df_noise,df_speedpitch],axis=0,sort=False)\ndf=df.fillna(0)\ndel df_noise, df_speedpitch\n\ndf.head()","d1596d1e":"def r(x):\n    if x=='male_surprise':\n        return 'surprise'\n    if x=='male_disgust':\n        return 'disgust'\n    if x=='male_neutral':\n        return 'neutral'\n    if x=='male_angry':\n        return 'angry'\n    if x=='male_fear':\n        return 'fear'\n    if x=='male_sad':\n        return 'sad'\n    if x=='male_happy':\n        return 'happy'\n    if x=='female_disgust':\n        return 'disgust'\n    if x=='female_surprise':\n        return 'surprise'\n    if x=='female_happy':\n        return 'happy'\n    if x=='female_angry':\n        return 'angry'\n    if x=='female_neutral':\n        return 'neutral'\n    if x=='female_fear':\n        return 'fear'\n    if x=='female_sad':\n        return 'sad'","f23a0d86":"y = df.labels.apply(r)","0206cec5":"y.unique()","a4e8e47b":"X_train, X_test, y_train, y_test = train_test_split(df.drop(['path','labels','source'],axis=1)\n                                                    , y\n                                                    , test_size=0.20\n                                                    , shuffle=True\n                                                    , random_state=42\n                                                   )","51a2ac71":"mean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0)\n\nX_train = (X_train - mean)\/std\nX_test = (X_test - mean)\/std","52af4ceb":"X_train = np.array(X_train)\ny_train = np.array(y_train)\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n\n# one hot encode the target \nlb = LabelEncoder()\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))\n\nprint(X_train.shape)\nprint(lb.classes_)\n#print(y_train[0:10])\n#print(y_test[0:10])\n\n# Pickel the lb object for future use \nfilename = 'labels'\noutfile = open(filename,'wb')\npickle.dump(lb,outfile)\noutfile.close()","ad910b61":"X_train = np.expand_dims(X_train, axis=2)\nX_test = np.expand_dims(X_test, axis=2)\nX_train.shape","ad754a2b":"y_train.shape","1b50dd7a":"model = Sequential()\nmodel.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(256, 8, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D(pool_size=(8)))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D(pool_size=(8)))\nmodel.add(Conv1D(64, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(64, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(7)) # Target class number\nmodel.add(Activation('softmax'))\n# opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n# opt = keras.optimizers.Adam(lr=0.0001)\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nmodel.summary()","89106826":"model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\nmodel_history=model.fit(X_train, y_train, batch_size=64, epochs=100, validation_data=(X_test, y_test))","035dbe5d":"model_name = 'Emotion_Model.h5'\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\n\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Save model and weights at %s ' % model_path)\n\n# Save the model to disk\nmodel_json = model.to_json()\nwith open(\"model_json.json\", \"w\") as json_file:\n    json_file.write(model_json)","5757c0c9":"json_file = open('model_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"saved_models\/Emotion_Model.h5\")\nprint(\"Loaded model from disk\")\n \n# Keras optimiser\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nscore = loaded_model.evaluate(X_test, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))","c0a85f03":"preds = loaded_model.predict(X_test, \n                         batch_size=16, \n                         verbose=1)\n\npreds=preds.argmax(axis=1)\npreds","488b3d23":"preds = preds.astype(int).flatten()\npreds = (lb.inverse_transform((preds)))\npreds = pd.DataFrame({'predictedvalues': preds})\n\n# Actual labels\nactual=y_test.argmax(axis=1)\nactual = actual.astype(int).flatten()\nactual = (lb.inverse_transform((actual)))\nactual = pd.DataFrame({'actualvalues': actual})\n\n# Lets combined both of them into a single dataframe\nfinaldf = actual.join(preds)","5dbcf0b5":"classes = finaldf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(finaldf.actualvalues, finaldf.predictedvalues, target_names=classes))","8733efe5":"from keras.models import Sequential, Model, model_from_json\nimport matplotlib.pyplot as plt\nimport keras \nimport pickle\nimport wave  # !pip install wave\nimport os\nimport pandas as pd\nimport numpy as np\nimport sys\nimport warnings\nimport librosa\nimport librosa.display\nimport IPython.display as ipd  # To play sound in the notebook\n\n# ignore warnings \nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","918fa72e":"CHUNK =1024\nFORMAT = pyaudio.paInt16 \nCHANNELS = 2 \nRATE = 44100 \nRECORD_SECONDS = 4\nWAVE_OUTPUT_FILENAME = \"test audio\\\\testing.wav\"\n\np = pyaudio.PyAudio()\n\nstream = p.open(format=FORMAT,\n                channels=CHANNELS,\n                rate=RATE,\n                input=True,\n                frames_per_buffer=CHUNK) #buffer\n\nprint(\"* recording\")\n\nframes = []\n\nfor i in range(0, int(RATE \/ CHUNK * RECORD_SECONDS)):\n    data = stream.read(CHUNK)\n    frames.append(data) # 2 bytes(16 bits) per channel\n\nprint(\"* done recording\")\n\nstream.stop_stream()\nstream.close()\np.terminate()\n\nwf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\nwf.setnchannels(CHANNELS)\nwf.setsampwidth(p.get_sample_size(FORMAT))\nwf.setframerate(RATE)\nwf.writeframes(b''.join(frames))\nwf.close()\n","d3c50d50":"json_file = open('model_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"saved_models\/Emotion_Model.h5\")\nprint(\"Loaded model from disk\")\n\n# the optimiser\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])","90e12a68":"data, sampling_rate = librosa.load('\/kaggle\/input\/abcdefghi\/akshat.wav')\nipd.Audio('\/kaggle\/input\/abcdefghi\/akshat.wav')","49a4e996":"X, sample_rate = librosa.load('\/kaggle\/input\/abcdefghi\/akshat.wav'\n                              ,res_type='kaiser_fast'\n\n                              ,sr=44100\n                              ,offset=0.5\n                             )\n\nsample_rate = np.array(sample_rate)\nmfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\nnewdf = pd.DataFrame(data=mfccs).T\nnewdf","b3185598":"newdf= np.expand_dims(newdf, axis=2)\nnewpred = loaded_model.predict(newdf, \n                         batch_size=16, \n                         verbose=1)\n\nnewpred","86a90501":"final = newpred.argmax(axis=1)\nfinal = final.astype(int).flatten()\nfinal = (lb.inverse_transform((final)))\nprint(final) #emo(final) #gender(final)","411decaf":"<a id=\"final\"><\/a>\n##  <center> 5. Final thoughts<center>\nAll 4 dataset are good datasets. Having listen to them and doing some really rough inspections, I feel we can combine all of them. We need to anyway or else we will run into problems with overfitting. One of the issues that I see many other people before me who have made an attempt on an emotion classifier, they tend to stick to just one dataset. And whilst their hold-out set accuracy is high, they don't work well on new unseen dataset. \n    \nThis is because, the classifier is trained on the same dataset and given the similar circumstances that the dataset was obtained or produced, (eg. audio quality, speaker repetition, duration and sentence uttered). To enable it to do well on new datasets, it needs to be given noise, make it work hard to find the real distinguishing characteristics of the emotion.\n\nBefore we end it, final steps are to combine all the meta-data together as one. Remember we saved the paths for all the audio files. So this will be handy when we need to read all 4 data sources in different folder structures. \n\nUpvote this notebook if you like, and be sure to check out the other parts which are now available:\n* [Part 2 | Feature Extract](https:\/\/www.kaggle.com\/ejlok1\/audio-emotion-part-2-feature-extract)\n* [Part 3 | Baseline model](https:\/\/www.kaggle.com\/ejlok1\/audio-emotion-part-3-baseline-model)\n* [Part 4 | Apply to new audio data](https:\/\/www.kaggle.com\/ejlok1\/audio-emotion-part-4-apply-to-new-audio-data)","034de214":"<a id=\"crema\"><\/a>\n##  <center> 4. CREMA-D dataset <center>\nLast but not least, CREMA dataset. Not much is known about this dataset and I don't see much usage of this in general in the wild. But its a very large dataset which we need. And it has a good variety of different speakers, apparently taken from movies. And the speakers are of different ethnicities. This is good. Means better generalisation when we do transfer learning. Very important\n\nWhat we are missing from this dataset is the \"surprise\" emotion but no biggie, we can use the rest. But we have the rest. What's extra here is that it has different level of intensity of the emotion like RAVDESS. But we won't be using that for now","1a44db31":"<a id=\"ravdess\"><\/a>\n## <center>2. RAVDESS dataset<\/center>\n\nRAVDESS is one of the more common dataset used for this excercise by others. It's well liked because of its quality of speakers, recording and it has 24 actors of different genders. And there's more! You can get it in song format as well. There's something for everyone and their research project. So for convenience, here's the filename identifiers as per the official RAVDESS website:\n\n- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n- Vocal channel (01 = speech, 02 = song).\n- Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n- Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n\nSo, here's an example of an audio filename. \n_02-01-06-01-02-01-12.mp4_\n\nThis means the meta data for the audio file is:\n- Video-only (02)\n- Speech (01)\n- Fearful (06)\n- Normal intensity (01)\n- Statement \"dogs\" (02)\n- 1st Repetition (01)\n- 12th Actor (12) - Female (as the actor ID number is even)\n\nAt my early beginings embarking on this journey, I learnt through the hard way that male and female speakers have to be trained seperately or the model will struggle to get a good accuracy. From reading a few blogs and articles, it seems female has a higher pitch that male. So if we don't tag the gender label to the audio file, it won't be able to detect anger or fear if it was a male speaker. It will just get bucketed into neutral \n\nLets specifically model the 2 speakers seperately. Note that there's a 'calm' emotion and a 'neutral' emotion as seperate. I don't really know the difference but for now, I'll just combined them into the same category.","37e5f2f0":"<a id=\"tess_load\"><\/a>\n###  Load the dataset \nThe speakers and the emotions are organised in seperate folders which is very convenient","0fcdc4a1":"<a id=\"savee_load\"><\/a>\n###  Load the dataset \nI'm not going to be reading the entire audio to memory. Rather I'm just going to read the meta-data associated with it. Cause at this point I just want a high level snapshot of some statistics. And then I might just load 1 or 2 audio files and expand on it. \n\nSo lets take 2 different emotions and play it just to get a feel for what we are dealing with. Ie. whether the data (audio) quality is good. It gives us an early insight as to how likely our classifier is going to be successful.  ","6039c1e2":"<a id=\"savee\"><\/a>\n##  <center> 1. SAVEE dataset <center>\nThe audio files are named in such a way that the prefix letters describes the emotion classes as follows:\n- 'a' = 'anger'\n- 'd' = 'disgust'\n- 'f' = 'fear'\n- 'h' = 'happiness'\n- 'n' = 'neutral'\n- 'sa' = 'sadness'\n- 'su' = 'surprise' \n\nThe original source has 4 folders each representing a speaker, but i've bundled all of them into one single folder and thus the first 2 letter prefix of the filename represents the speaker initials. Eg. 'DC_d03.wav' is the 3rd disgust sentence uttered by the speaker DC. It's  worth nothing that they are all male speakers only. This isn't an issue as we'll balance it out with the TESS dataset which is just female only. So lets check out the distribution of the emotions...","eaf6c935":"The 4 sources of the datasets are all on Kaggle so I've just imported them into the workspace. The directory path to the 4 sources in this environment are below:","911164dd":"So we've already seen the shape of an MFCC output for each file, and it's a 2D matrix of the number of bands by time. In order to optimise space and memory, we're going to read each audio file, extract its mean across all MFCC bands by time, and just keep the extracted features, dropping the entire audio file data.","af66fb49":"<a id=\"crema_load\"><\/a>\n###  Load the dataset \nThe speakers and the emotions like all previous datasets, are tagged in the audio filename itself. However, what we are missing is the Gender, which is kept as a seperate csv file that maps the actors. Instead of reading it and doing some matching, I'm just going to hardcode it here instead. Not the best practice but can do for now. ","ab0775aa":"<a id=\"tess\"><\/a>\n##  <center> 3. TESS dataset <center>\nNow on to the TESS dataset, its worth nothing that it's only based on 2 speakers, a young female and an older female. This should hopefully balance out the male dominant speakers that we have on SAVEE. \n\nIts got the same 7 key emotions we're interested in. But what is slightly different about this dataset compared to the previous two above, is that the addition of 'pleasant surprise' emotion. I haven't really checked to see for the RADVESS and SAVEE dataset, if the surpises are unpleasant. But I'm going to work with the assumption for now that its a pleasant surprise. If we find out from post modelling, surpise is highly inaccurate, we can come back and modify our assumption here. ","5be8fdd7":"<a id=\"ravdess_load\"><\/a>\n###  Load the dataset \nBecause of the way the entire data was packaged for us, and the format of the audio filename, there's a few more parsing steps required for the RAVDESS dataset compared to SAVEE "}}