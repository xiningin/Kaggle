{"cell_type":{"1a9df17f":"code","1183425a":"code","1989817b":"code","39a2159c":"code","aac2a57d":"code","5be51679":"code","6d42d8f1":"code","367ac898":"code","6dbd7f6a":"code","80a08d4b":"code","3152c839":"code","225c0784":"code","a4af0be4":"code","edf2e90a":"code","76dbfac6":"code","c07fb65d":"code","512f3ada":"code","35014454":"code","147d1368":"code","8d4247ba":"code","d95f5168":"code","92390211":"code","ec2a3ea3":"code","822e2a3e":"code","93b3a79a":"code","b7ffad0f":"code","49f11386":"code","5c66f1f3":"code","b3216eec":"code","3a5cd2ab":"code","de14b19b":"code","3a40f231":"code","df098be6":"code","4fb484b6":"code","32c8ee6b":"code","b0be236f":"code","b29653e6":"code","fb2dcc5a":"code","ce24958c":"code","7d43ce55":"code","d47a2709":"code","6084dc7c":"code","7d647208":"code","9a2c187c":"code","b74b5b1a":"code","35c5bb65":"code","6670cf45":"code","36b2ad14":"code","75c51056":"code","4e0ffe64":"code","336153a0":"code","e4b8a2d2":"code","4198f933":"code","d3992e5e":"code","0e844ae4":"code","0898c49c":"code","a53d99e3":"code","d50bcd98":"code","49eda1db":"code","bdaf96f3":"code","6ad40671":"code","0f7476ec":"code","5e917a7f":"code","4640f64f":"code","ba2ce389":"code","4a3bbef5":"code","a4706a49":"code","4f0598cc":"code","0e17e0a9":"code","5318304f":"code","1e6a68dc":"code","14c2696d":"code","25f020ef":"code","d1e506cf":"code","d01b67cd":"code","188ca89d":"code","f4f125a1":"code","3228b51d":"code","a5a1e0ca":"code","82964293":"code","2670a27e":"code","f8ad5810":"code","3d7fe4e8":"code","85a14b89":"code","7c1eac83":"code","cb1d77e9":"code","f3e82406":"code","378d6325":"code","5486d158":"code","8f68bdd6":"code","060f3d9e":"code","13125810":"code","16dd29ad":"code","005f7cc9":"code","232d8659":"code","f32585eb":"code","be790d7d":"code","714a454f":"code","1cb489cf":"code","fa87710f":"code","082e2a3d":"code","73e2b68e":"code","797072ec":"code","c315093a":"code","9705385a":"code","73cc4baa":"code","eafec084":"code","658d97e3":"code","ac7cbb65":"code","e8bb74b2":"code","ce23da56":"code","c82d03b2":"code","1941752e":"code","bd790d63":"code","325980f0":"code","cbe07fb4":"code","bcd11db9":"code","b70f3882":"code","bad856db":"code","36179487":"code","ac5d8d42":"code","57261be0":"code","e7d0b854":"markdown","1a7dc4fc":"markdown","0c67dcd4":"markdown","b6abc3ed":"markdown","34ab081e":"markdown","0042bd44":"markdown","4d6ce68a":"markdown","ecc70d72":"markdown","f7cb10f2":"markdown","3ddbbc17":"markdown","ece5f68f":"markdown","58dc5db7":"markdown","651572b0":"markdown","a000ef0f":"markdown","9ebb336f":"markdown","3a669ff4":"markdown","b661373f":"markdown","07fbdc41":"markdown","1d564d05":"markdown","d5b7ab2f":"markdown","78a79bfd":"markdown","ca705179":"markdown","2eea28a2":"markdown","e2651540":"markdown","b465c2d7":"markdown","67a79580":"markdown","ace61741":"markdown","7c6328b4":"markdown","5c80fd2a":"markdown","159edacd":"markdown","b8580f83":"markdown","e40df2cd":"markdown","651f9914":"markdown","75038e9c":"markdown","895570ba":"markdown","df21b378":"markdown","4aea267b":"markdown","a6039ac4":"markdown","7f73f9d4":"markdown","8eb45261":"markdown","65a41429":"markdown","f21b7170":"markdown","1a25d564":"markdown","841513d9":"markdown","ff92dfbe":"markdown","4ac955bf":"markdown","b08ca107":"markdown","5c8e91fd":"markdown","4d8dada7":"markdown","05751d47":"markdown","d5451d00":"markdown","338c277d":"markdown"},"source":{"1a9df17f":"\n# Import our libraries\nimport pandas as pd\nimport numpy as np\n\n# Import sklearn libraries\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, precision_recall_curve, auc, make_scorer, confusion_matrix, f1_score, fbeta_score\n\n# Import the Naive Bayes, logistic regression, Bagging, RandomForest, AdaBoost, GradientBoost, Decision Trees and SVM Classifier\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm\nfrom xgboost import XGBClassifier\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n#from matplotlib import style\n#plt.style.use('bmh')\n#plt.style.use('ggplot')\nplt.style.use('seaborn-notebook')\n\nfrom matplotlib.ticker import StrMethodFormatter\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelBinarizer\n","1183425a":"#titanic_features = pd.read_csv('train.csv')\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv(\"..\/input\/test.csv\")","1989817b":"train_df.info()","39a2159c":"train_df.shape","aac2a57d":"test_df.shape","5be51679":"test_df.info()","6d42d8f1":"train_df.head()","367ac898":"train_df.describe()","6dbd7f6a":"total = train_df.isnull().sum().sort_values(ascending=False)\npercent_1 = train_df.isnull().sum()\/train_df.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","80a08d4b":"train_df.columns.values","3152c839":"from IPython.display import HTML\nHTML('''<script>\ncode_show_err=false; \nfunction code_toggle_err() {\n if (code_show_err){\n $('div.output_stderr').hide();\n } else {\n $('div.output_stderr').show();\n }\n code_show_err = !code_show_err\n} \n$( document ).ready(code_toggle_err);\n<\/script>\nTo toggle on\/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here<\/a>.''')","225c0784":"train_df['Embarked'].value_counts()\/len(train_df)","a4af0be4":"sns.set(style=\"darkgrid\")\nsns.countplot( x='Embarked', data=train_df, hue=\"Embarked\", palette=\"Set1\");","edf2e90a":"sns.set(style=\"darkgrid\")\nsns.countplot( x='Survived', data=train_df, hue=\"Embarked\", palette=\"Set1\");","76dbfac6":"train_df.groupby('Embarked').mean()","c07fb65d":"train_df.groupby('Sex').mean()","512f3ada":"FacetGrid = sns.FacetGrid(train_df, row='Embarked', size=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', order=None, hue_order=None )\nFacetGrid.add_legend();","35014454":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(16, 8))\nwomen = train_df[train_df['Sex']=='female']\nmen = train_df[train_df['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False, color=\"green\")\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False, color=\"red\")\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False, color=\"green\")\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False, color=\"red\")\nax.legend()\n_ = ax.set_title('Male');","147d1368":"sns.barplot(x='Pclass', y='Survived', data=train_df);","8d4247ba":"grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=3.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","d95f5168":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'travelled_alone'] = 'No'\n    dataset.loc[dataset['relatives'] == 0, 'travelled_alone'] = 'Yes'\n    #dataset['travelled_alone'] = dataset['travelled_alone'].astype(int)\ntrain_df['travelled_alone'].value_counts()","92390211":"test_df['travelled_alone'].value_counts()","ec2a3ea3":"train_df['relatives'].value_counts()","822e2a3e":"axes = sns.factorplot('relatives','Survived', \n                      data=train_df, aspect = 2.5, );","93b3a79a":"# Drop 'PassengerId' from the train set, because it does not contribute to a persons survival probability.\ntrain_df = train_df.drop(['PassengerId'], axis=1)","b7ffad0f":"train_df['Cabin'].describe()","49f11386":"import re\ndeck = {\"A\": \"A\", \"B\": \"B\", \"C\": \"C\", \"D\": \"D\", \"E\": \"E\", \"F\": \"F\", \"G\": \"G\", \"U\": \"U\"}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(\"U\")\n    #dataset['Deck'] = dataset['Deck'].astype(int)\n# we can now drop the cabin feature\ntrain_df = train_df.drop(['Cabin'], axis=1)\ntest_df = test_df.drop(['Cabin'], axis=1)","5c66f1f3":"train_df['Deck'].value_counts()","b3216eec":"train_df.groupby('Deck').mean()","3a5cd2ab":"test_df['Deck'].value_counts()","de14b19b":"data = [train_df, test_df]\n\nfor dataset in data:\n    mean = train_df[\"Age\"].mean()\n    std = test_df[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train_df[\"Age\"].astype(int)","3a40f231":"train_df[\"Age\"].isnull().sum()","df098be6":"test_df[\"Age\"].isnull().sum()","4fb484b6":"train_df[\"Age\"].describe()","32c8ee6b":"#train_df.groupby('Age').mean()","b0be236f":"train_df['Embarked'].describe()","b29653e6":"train_df['Embarked'].mode()","fb2dcc5a":"#common_value = train_df['Embarked'].mode()\n#common_value","ce24958c":"common_value = 'S'\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)","7d43ce55":"test_df['Embarked'].describe()","d47a2709":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","6084dc7c":"train_df['Fare'].describe()","7d647208":"train_df['Fare'].isnull().sum()","9a2c187c":"test_df['Fare'].describe()","b74b5b1a":"test_df['Fare'].isnull().sum()","35c5bb65":"train_df['Fare'] = train_df['Fare'].astype(int)","6670cf45":"test_df['Fare'] = test_df['Fare'].astype(int)","36b2ad14":"train_titles = train_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntype(train_titles)","75c51056":"train_titles.value_counts()","4e0ffe64":"data = [train_df, test_df]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    #dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(\"NA\")\ntrain_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)","336153a0":"train_df.groupby(['Title']).mean()","e4b8a2d2":"test_df.groupby(['Title']).mean()","4198f933":"train_df['Sex'].value_counts()","d3992e5e":"'''\ngenders = {\"male\": 0, \"female\": 1}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)\n\n'''","0e844ae4":"train_df['Ticket'].describe()","0898c49c":"test_df['Ticket'].describe()","a53d99e3":"train_df = train_df.drop(['Ticket'], axis=1)\ntest_df = test_df.drop(['Ticket'], axis=1)","d50bcd98":"'''\nports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(ports)\n    \n'''    ","49eda1db":"\ndata = [train_df, test_df]\nfor dataset in data:\n    dataset['Age_Class']= dataset['Age']* dataset['Pclass']\n    \n    ","bdaf96f3":"\nfor dataset in data:\n    dataset['Fare_Per_Person'] = dataset['Fare']\/(dataset['relatives']+1)\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n    ","6ad40671":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 7\n    \n    dataset['Age'] = dataset['Age'].astype(str)\n    dataset.loc[ dataset['Age'] == '0', 'Age'] = \"Children\"\n    dataset.loc[ dataset['Age'] == '1', 'Age'] = \"Teens\"\n    dataset.loc[ dataset['Age'] == '2', 'Age'] = \"Youngsters\"\n    dataset.loc[ dataset['Age'] == '3', 'Age'] = \"Young Adults\"\n    dataset.loc[ dataset['Age'] == '4', 'Age'] = \"Adults\"\n    dataset.loc[ dataset['Age'] == '5', 'Age'] = \"Middle Age\"\n    dataset.loc[ dataset['Age'] == '6', 'Age'] = \"Senior\"\n    dataset.loc[ dataset['Age'] == '7', 'Age'] = \"Retired\"\n\n# let's see how it's distributed \ntrain_df['Age'].value_counts()","0f7476ec":"test_df['Age'].value_counts()","5e917a7f":"train_df.info()","4640f64f":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    dataset['Fare'] = dataset['Fare'].astype(str)\n    dataset.loc[ dataset['Fare'] == '0', 'Fare'] = \"Extremely Low\"\n    dataset.loc[ dataset['Fare'] == '1', 'Fare'] = \"Very Low\"\n    dataset.loc[ dataset['Fare'] == '2', 'Fare'] = \"Low\"\n    dataset.loc[ dataset['Fare'] == '3', 'Fare'] = \"High\"\n    dataset.loc[ dataset['Fare'] == '4', 'Fare'] = \"Very High\"\n    dataset.loc[ dataset['Fare'] == '5', 'Fare'] = \"Extremely High\"\n    ","ba2ce389":"train_df['Fare'].value_counts()","4a3bbef5":"test_df['Fare'].value_counts()","a4706a49":"train_df.info()","4f0598cc":"test_df.info()","0e17e0a9":"# Let's take a last look at the training set\ntrain_df.head(10)","5318304f":"train_df.info()","1e6a68dc":"test_df.info()","14c2696d":"train_df['Pclass'].value_counts()","25f020ef":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Pclass'] = dataset['Pclass'].astype(str)\n    dataset.loc[ dataset['Pclass'] == '1', 'Pclass'] = \"Class1\"\n    dataset.loc[ dataset['Pclass'] == '2', 'Pclass'] = \"Class2\"\n    dataset.loc[ dataset['Pclass'] == '3', 'Pclass'] = \"Class3\"\n    ","d1e506cf":"train_df.info()","d01b67cd":"test_df.info()","188ca89d":"train_df['Pclass'].value_counts()","f4f125a1":"# Capture all the numerical features so that we can scale them later\n#data = [train_df, test_df]\ntrain_numerical_features = list(train_df.select_dtypes(include=['int64', 'float64', 'int32']).columns)\ntrain_numerical_features","3228b51d":"type(train_numerical_features)","a5a1e0ca":"del train_numerical_features[0]\ntrain_numerical_features","82964293":"# Feature scaling - Standard scaler\nss_scaler = StandardScaler()\ntrain_df_ss = pd.DataFrame(data = train_df)\ntrain_df_ss[train_numerical_features] = ss_scaler.fit_transform(train_df_ss[train_numerical_features])","2670a27e":"train_df_ss.shape","f8ad5810":"train_df_ss.head()","3d7fe4e8":"test_numerical_features = list(test_df.select_dtypes(include=['int64', 'float64', 'int32']).columns)\ntest_numerical_features","85a14b89":"del test_numerical_features[0]\ntest_numerical_features","7c1eac83":"# Feature scaling - Standard scaler\ntest_ss_scaler = StandardScaler()\ntest_df_ss = pd.DataFrame(data = test_df)\ntest_df_ss[test_numerical_features] = test_ss_scaler.fit_transform(test_df_ss[test_numerical_features])","cb1d77e9":"test_df.shape","f3e82406":"test_df.head()","378d6325":"# One-Hot encoding \/ Dummy variables\nencode_col_list = list(train_df.select_dtypes(include=['object']).columns)\nfor i in encode_col_list:\n    train_df_ss = pd.concat([train_df_ss,pd.get_dummies(train_df_ss[i], prefix=i)],axis=1)\n    train_df_ss.drop(i, axis = 1, inplace=True)","5486d158":"train_df_ss.shape","8f68bdd6":"train_df_ss.head()","060f3d9e":"# One-Hot encoding \/ Dummy variables\ntest_encode_col_list = list(test_df.select_dtypes(include=['object']).columns)\nfor i in test_encode_col_list:\n    test_df_ss = pd.concat([test_df_ss,pd.get_dummies(test_df_ss[i], prefix=i)],axis=1)\n    test_df_ss.drop(i, axis = 1, inplace=True)","13125810":"test_df_ss.shape","16dd29ad":"test_df_ss.head()","005f7cc9":"X_train = train_df_ss.drop(\"Survived\", axis=1)\nY_train = train_df_ss[\"Survived\"]\nX_test  = test_df_ss.drop(\"PassengerId\", axis=1).copy()","232d8659":"X_train.shape","f32585eb":"Y_train.shape","be790d7d":"X_test.shape","714a454f":"X_train.info()","1cb489cf":"# Instantiate our model\nlogreg = LogisticRegression()\n\n# Fit our model to the training data\nlogreg.fit(X_train, Y_train)\n\n# Predict on the test data\nlogreg_predictions = logreg.predict(X_test)\n\nlogreg_data = pd.read_csv('..\/\/input\/test.csv')\nlogreg_data.insert((logreg_data.shape[1]),'Survived',logreg_predictions)\n\nlogreg_data.to_csv('LogisticRegression_SS_OH_FE2.csv')","fa87710f":"answer = logreg_data[['PassengerId', 'Survived']]\nanswer.to_csv('LogisticRegression_two_col.csv', index=False)","082e2a3d":"answer.head()","73e2b68e":"# Instantiate our model\nadaboost = AdaBoostClassifier()\n\n# Fit our model to the training data\nadaboost.fit(X_train, Y_train)\n\n# Predict on the test data\nadaboost_predictions = adaboost.predict(X_test)\n\nadaboost_data = pd.read_csv('..\/\/input\/test.csv')\nadaboost_data.insert((adaboost_data.shape[1]),'Survived',adaboost_predictions)\n\nadaboost_data.to_csv('AdaptiveBoosting_SS_OH_FE.csv')","797072ec":"answer = adaboost_data[['PassengerId', 'Survived']]\nanswer.to_csv('Adaptive_Boosting.csv', index=False)","c315093a":"# Instantiate our model\nbag = BaggingClassifier()\n\n# Fit our model to the training data\nbag.fit(X_train, Y_train)\n\n# Predict on the test data\nbag_predictions = bag.predict(X_test)\n\nbag_data = pd.read_csv('..\/\/input\/test.csv')\nbag_data.insert((bag_data.shape[1]),'Survived',bag_predictions)\n\nbag_data.to_csv('Bagging.csv')","9705385a":"answer = bag_data[['PassengerId', 'Survived']]\nanswer.to_csv('Bagging_Classifier.csv', index=False)","73cc4baa":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nrandom_forest_predictions = random_forest.predict(X_test)\n\nrf_data = pd.read_csv('..\/\/input\/test.csv')\nrf_data.insert((rf_data.shape[1]),'Survived',random_forest_predictions)\n\nrf_data.to_csv('RandomForest_SS_OH.csv')","eafec084":"answer = rf_data[['PassengerId', 'Survived']]\nanswer.to_csv('Random_Forest.csv', index=False)","658d97e3":"# Instantiate our model\ndt = DecisionTreeClassifier()\ndt.fit(X_train, Y_train)\n\ndt_predictions = dt.predict(X_test)\n\ndt_data = pd.read_csv('..\/\/input\/test.csv')\ndt_data.insert((dt_data.shape[1]),'Survived',dt_predictions)\n\ndt_data.to_csv('DecisionTrees.csv')","ac7cbb65":"answer = dt_data[['PassengerId', 'Survived']]\nanswer.to_csv('Decision_Trees.csv', index=False)","e8bb74b2":"# Instantiate our model\ngb = GradientBoostingClassifier()\ngb.fit(X_train, Y_train)\ndt_data\ngb_predictions = gb.predict(X_test)\n\ngb_data = pd.read_csv('..\/\/input\/test.csv')\ngb_data.insert((gb_data.shape[1]),'Survived',gb_predictions)\n\ngb_data.to_csv('GradientBoost_SS_OH_FE.csv')","ce23da56":"answer = gb_data[['PassengerId', 'Survived']]\nanswer.to_csv('Gradient_Boost.csv', index=False)","c82d03b2":"# Instantiate our model\nxg = XGBClassifier(learning_rate=0.02, n_estimators=750,\n                   max_depth= 3, min_child_weight= 1, \n                   colsample_bytree= 0.6, gamma= 0.0, \n                   reg_alpha= 0.001, subsample= 0.8\n                  )\nxg.fit(X_train, Y_train)\n\nxg_predictions = xg.predict(X_test)\n\nxg_data = pd.read_csv('..\/\/input\/test.csv')\nxg_data.insert((xg_data.shape[1]),'Survived',xg_predictions)\n\nxg_data.to_csv('XGBoost_SS_OH_FE_GSCV.csv')","1941752e":"answer = xg_data[['PassengerId', 'Survived']]\nanswer.to_csv('XGBoost.csv', index=False)","bd790d63":"\nparam_test1 = {\n    #'n_estimators': [100,200,500,750,1000],\n    #'max_depth': [3,5,7,9],\n    #'min_child_weight': [1,3,5],\n    'gamma':[i\/10.0 for i in range(0,5)],\n    'subsample':[i\/10.0 for i in range(6,10)],\n    'colsample_bytree':[i\/10.0 for i in range(6,10)],\n    'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05, 0.1, 1]\n    #'learning_rate': [0.01, 0.02, 0.05, 0.1]\n}\n\nscoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n\n'''\nfit_params={\"early_stopping_rounds\":42, \n            \"eval_metric\" : \"mae\", \n            \"eval_set\" : [[test_features, test_labels]]}\n            \n'''\n\ngsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate=0.02, n_estimators=750,\n                   max_depth= 3, min_child_weight= 1), \n                       param_grid = param_test1, #fit_params=fit_params,\n                       scoring=scoring, iid=False, cv=3, verbose = 5, refit='Accuracy')\ngsearch1.fit(X_train, Y_train)\n","325980f0":"#gsearch1.grid_scores_, \ngsearch1.best_params_, gsearch1.best_score_","cbe07fb4":"results = gsearch1.cv_results_","bcd11db9":"results","b70f3882":"# Instantiate our model\nxg = XGBClassifier(learning_rate=0.02, n_estimators=750,\n                   max_depth= 3, min_child_weight= 1, \n                   colsample_bytree= 0.6, gamma= 0.0, \n                   reg_alpha= 0.001, subsample= 0.8\n                  )\nxg.fit(X_train, Y_train)\nxg_predictions = xg.predict(X_test)\nxg_data = pd.read_csv('..\/\/input\/test.csv')\nxg_data.insert((xg_data.shape[1]),'Survived',xg_predictions)\nxg_data.to_csv('XGBoost_SS_OH_FE_GSCV.csv')","bad856db":"xg_data.head()","36179487":"answer = xg_data[['PassengerId', 'Survived']]\nanswer.to_csv('XGBoost_2.csv', index=False)","ac5d8d42":"test = pd.read_csv(\"..\/\/input\/test.csv\")","57261be0":"test['Age'].fillna(test['Age'].median(),inplace=True) # Age\ntest['Fare'].fillna(test['Fare'].median(),inplace=True) # Fare\nd = {1:'1st',2:'2nd',3:'3rd'} #Pclass\ntest['Pclass'] = test['Pclass'].map(d)\ntest['Embarked'].fillna(test['Embarked'].value_counts().index[0], inplace=True) # Embarked\nids = test[['PassengerId']]# Passenger Ids\ntest.drop(['PassengerId','Name','Ticket','Cabin'],1,inplace=True)# Drop Unnecessary Columns\ncategorical_vars = test[['Pclass','Sex','Embarked']]# Get Dummies of Categorical Variables\ndummies = pd.get_dummies(categorical_vars,drop_first=True)\ntest = test.drop(['Pclass','Sex','Embarked'],axis=1)#Drop the Original Categorical Variables\ntest = pd.concat([test,dummies],axis=1)#Instead, concat the new dummy variables\ntest.head()","e7d0b854":"#### Pclass","1a7dc4fc":"## Features:\n* `survival`: Survival \n* `PassengerId`: Unique Id of a passenger\n* `pclass`: Ticket class     \n* `sex`: Sex     \n* `Age`: Age in years     \n* `sibsp`: # of siblings \/ spouses aboard the Titanic     \n* `parch`: # of parents \/ children aboard the Titanic     \n* `ticket`: Ticket number     \n* `fare`: Passenger fare     \n* `cabin`: Cabin number     \n* `embarked`: Port of Embarkation","0c67dcd4":"* ##### Embarked seems to be correlated with survival, depending on the gender.\n\n* ##### Women on port Q and on port S have a higher chance of survival. The inverse is true, if they are at port C. Men have a high survival probability if they are on port C, but a low probability if they are on port Q or S.\n\n* ##### Pclass also seems to be correlated with survival. We will generate another plot of it below.","b6abc3ed":"### Creating categories:","34ab081e":"##### The `Embarked` feature has only 2 missing values, which can easily be filled. It will be much more tricky, to deal with the `Age` feature, which has 177 missing values. The `Cabin` feature needs further investigation, but it looks like that we might want to drop it from the dataset, since 77 % of it are missing.","0042bd44":"#### From the table above, we can note a few things:\n* ##### We have a few categorical variabes that need to be either converted to numerical or one-hot encoded, so that the machine learning algorithms can process them. \n* ##### The features have widely different ranges, and we will need to convert into roughly the same scale. \n* ##### Some features contain missing values (NaN = not a number), that we need to deal with.","4d6ce68a":"## Adaptive Boosting","ecc70d72":"#### Age: Convert from float to int and create a new feature \"AgeGroup\" using bins ","f7cb10f2":"## Random Forest","3ddbbc17":"## Decision Trees","ece5f68f":"#### Embarked:","58dc5db7":"* ##### You can see that men have a high probability of survival when they are between 18 and 30 years old, which is also a little bit true for women but not fully. For women the survival chances are higher between 14 and 40.\n\n* ##### For men the probability of survival is very low between the age of 5 and 18, but that isn\u2019t true for women. Another thing to note is that infants also have a little bit higher probability of survival.\n\n* ##### Since there seem to be certain ages, which have increased odds of survival and because I want every feature to be roughly on the same scale, I will create age groups later on.","651572b0":"#### Fare:","a000ef0f":"#### Fare: Convert from float to int","9ebb336f":"## ML Modelling","3a669ff4":"#### Sex: Convert to numeric","b661373f":"### Read the training and testing data","07fbdc41":"### Q2: Does survival depend upon gender?","1d564d05":"### Hyperparameter tuning - using GridSearchCV to find the best set of parameters","d5b7ab2f":"#### Ticket:","78a79bfd":"### Create the Prediction File for the Kaggle Competition","ca705179":"## One-hot encoding","2eea28a2":"# Titanic - Machine Learning from Disasters\n## Kaggle Competition","e2651540":"### Create new features","b465c2d7":"## Gradient Boost","67a79580":"#### Fare per Person:","ace61741":"### Missing data ","7c6328b4":"#### Suppress warnings from output","5c80fd2a":"### Q3: Do the passengers have better chance at survival while travelling alone?","159edacd":"### Exploratory Analysis","b8580f83":"## Bagging Classifier","e40df2cd":"##### Here we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person is in class 1. We will create another pclass plot below.","651f9914":"#### Age:\n##### Fill null values with random numbers, which are computed based on the mean age value in regards to the standard deviation.","75038e9c":"#### Name: Extract titles from name and build a new feature from that","895570ba":"`Age` and `Embarked`","df21b378":"#### RMS Titanic was a British passenger liner that sank in the North Atlantic Ocean in the early hours of 15 April 1912, after colliding with an iceberg during her maiden voyage from Southampton to New York City. There were an estimated 2,224 passengers and crew aboard, and more than 1,500 died, making it one of the deadliest commercial peacetime maritime disasters in modern history. RMS Titanic was the largest ship afloat at the time she entered service and was the second of three Olympic-class ocean liners operated by the White Star Line.\n\n### In this exercise, we will try to predict whether a passenger on the Titanic would have survived or not.","4aea267b":"### Q1: How many people Embarked from different ports? Is there a correlation between port of embarkment and survival? ","a6039ac4":"### Converting Features","7f73f9d4":"#### Age times class","8eb45261":"#### Embarked: Convert to Numeric","65a41429":"##### SibSp and Parch would make more sense as a combined feature, that shows the total number of relatives, a person has on the Titanic. I will create it below and also a feature that sows if someone is not alone.","f21b7170":"## XGBoost","1a25d564":"### Data Preprocessing","841513d9":"#### Cabin:\n##### Cabin number looks like \u2018C23\u2019 and the letter refers to the deck. We will extract these and create a new feature, to represent a persons deck. ","ff92dfbe":"### Import the necessary libraries","4ac955bf":"## Scaling the numerical data","b08ca107":"## Logistic Regression","5c8e91fd":"#### Missing Data","4d8dada7":"#### `SibSp` and `Parch`","05751d47":"##### Since the Ticket attribute has too many unique values, it will be a bit tricky to convert them into useful categories. So we will drop it from the dataset.","d5451d00":"##### Here we can see that you had a high probabilty of survival with 1 to 3 realitves, but a lower one if you had less than 1 or more than 3 (except for some cases with 6 relatives).","338c277d":"##### The plot above confirms our assumption about pclass 1, but we can also spot a high probability that a person in pclass 3 will not survive."}}