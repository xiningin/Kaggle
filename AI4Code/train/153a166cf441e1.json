{"cell_type":{"af12b8d8":"code","5e35fd34":"code","e981915f":"code","2fe84806":"code","7bd72622":"code","2076c6bf":"code","b759098f":"code","41dd3511":"code","4da863fd":"code","f1b9c778":"code","84d723c2":"code","bca3f28f":"code","2b72e967":"code","abca2279":"code","b79d0143":"code","e903b801":"code","6a99902c":"code","6365eafb":"code","425c0bb7":"code","40b3c792":"code","ca2a6256":"code","aba48625":"code","8f35291d":"code","577e69d4":"code","1b80aa39":"code","3629657b":"code","7154c2ef":"code","8fcf6106":"code","db265fff":"code","3398e54f":"markdown","d1ea5ee1":"markdown","c4b26d58":"markdown","361553af":"markdown","f31afc9f":"markdown","3a6ac512":"markdown","68faae00":"markdown","578df646":"markdown","0b49852c":"markdown","bbc5e2b3":"markdown","a9380d69":"markdown","7808ffed":"markdown","1ddeef64":"markdown","d96c439c":"markdown","2bdeaa2d":"markdown","fbf629d2":"markdown"},"source":{"af12b8d8":"#Initial\nimport os\nimport numpy as np\nimport pandas as pd\nimport math\n\n#Spark\nimport pyspark as spark\nfrom pyspark import SparkConf, SparkContext\n\n\nsc = SparkContext.getOrCreate()\n\n\nfrom pyspark.sql import SparkSession, SQLContext\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import GBTRegressor, GeneralizedLinearRegression, AFTSurvivalRegression\nfrom pyspark.ml.feature import VectorIndexer\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.sql.types import DoubleType\n \nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder  \n    \n#Plots    \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n","5e35fd34":"dataPath = \"..\/input\"\n', '.join(os.listdir(dataPath))","e981915f":"pdtrainset = pd.read_csv(\"..\/input\/train.csv\")","2fe84806":"pdtrainset.head(10)","7bd72622":"pdtrainset.describe()","2076c6bf":"pdtrainset[\"SalePrice\"] = np.log(pdtrainset[\"SalePrice\"])\npdtrainset.head(7)","b759098f":"pdtrainset.info()","41dd3511":"correl = pdtrainset[1:].corr()\ncorrel.drop([\"SalePrice\", \"Id\"], axis=1, inplace=True)","4da863fd":"correl.iloc[-1].apply(lambda x: abs(x)).plot(kind='bar', figsize=(10, 6), title=\"Correlation with SalePrice\", grid=True)","f1b9c778":"salecorrel = pd.DataFrame(correl.transpose()[\"SalePrice\"])\ntop_features = salecorrel[salecorrel[\"SalePrice\"] >= 0.45].sort_values(by=[\"SalePrice\"], \n                                                                      ascending=False)\ntopcolumns = top_features.transpose().columns.values\ntopcorrelVal = top_features.values\n\n[\"{0}: {1:.4f}\".format(col, topcorrelVal[i][0]) for i,col in enumerate(topcolumns)]\n","84d723c2":"# [sns.lmplot(x=\"SalePrice\",y=x,data=pdtrainset,\n#            scatter_kws={'alpha':0.07}, aspect=2, height=4) for x in topcolumns]\n\ntopItemsCorrelation = pdtrainset[list(topcolumns)].corr().abs()\n\ntopItemsCorrelation","bca3f28f":"sns.heatmap(topItemsCorrelation, annot=True, fmt=\".2f\")\nplt.show()","2b72e967":"utcItems = pd.DataFrame(topItemsCorrelation.unstack(), columns=[\"c\"])\nutcItems[(utcItems[\"c\"] > 0.8) & (utcItems[\"c\"] < 1)]","abca2279":"topcolumns = list(set(topcolumns) - set([\"TotRmsAbvGrd\", \"GarageArea\", \"1stFlrSF\", \"GarageYrBlt\"]))\ntopcolumns","b79d0143":"getDecades = lambda col: col.apply(lambda x: math.ceil(float(x) \/ 10)*10)\n\npdtrainset[\"YearBuilt\"] = getDecades(pdtrainset[\"YearBuilt\"])\npdtrainset[\"YearRemodAdd\"] = getDecades(pdtrainset[\"YearRemodAdd\"])\npdtrainset = pdtrainset[topcolumns+[\"SalePrice\"]]\npdtrainset.head()","e903b801":"valuableColumns = list(pdtrainset.columns.values)","6a99902c":"[sns.lmplot(x=col,y=\"SalePrice\",data=pdtrainset, \n            scatter_kws={'alpha':0.07}, aspect=3, height=5) for col in topcolumns]\n","6365eafb":"sqlContext = SQLContext(sc)\n\nsc.setLogLevel(\"ERROR\")","425c0bb7":"df = sqlContext.createDataFrame(pdtrainset)","40b3c792":"df.describe()","ca2a6256":"# #sptrain = sqlContext.read.csv(\"..\/input\/train.csv\", header=True)\n\nsptrain = df.withColumn(\"label\", df.SalePrice.cast(\"double\")).cache()\n\nsptest = sqlContext.read.csv(\"..\/input\/test.csv\", header=True)","aba48625":"for col in valuableColumns[:-1]:\n    # Of cause we can't change immutable values, but we can owerwrite them\n    sptrain = sptrain.withColumn(col+\"_d\", sptrain[col].cast(\"double\"))\n    sptest = sptest.withColumn(col+\"_d\", sptest[col].cast(\"double\"))\n    \nsptrain = sptrain.fillna(-1., subset=valuableColumns)\nsptest = sptest.fillna(-1., subset=valuableColumns)\n# sptrain = sptrain.fillna(\"no\", subset=catColumn)\n# sptest = sptest.fillna(\"no\", subset=catColumn)    \nsptrain","8f35291d":"#let add few train model \ngbt = GBTRegressor(maxIter=100, \n                   maxDepth=5)\\\n                  .setLabelCol(\"label\")\\\n                  .setFeaturesCol(\"features\")\n\n# Than let split our features on to group categorical (string) and continious (numerical)\n# indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\")\\\n#             .setHandleInvalid(\"keep\")\\\n#             .fit(sptrain) for column in catColumn]\n\n#indexers = indexers + ([OneHotEncoder(inputCol= col+\"_index\", outputCol= col+\"_oht\") for col in catColumn])\n#TODO: change all columns to a valuable only\nindexers = []\nindexers.append(VectorAssembler(\n    inputCols=[\"{0}_d\".format(col) for col in valuableColumns[:-1]], \n        outputCol=\"features\"))\nindexers.append(gbt)","577e69d4":"(trainingData, subtestData) = sptrain.randomSplit([0.7, 0.3])\n\nmodelGBT = Pipeline(stages=tuple(indexers)).fit(trainingData)","1b80aa39":"predictions = modelGBT.transform(subtestData)\n\npredictions.select([\"prediction\", \"label\", \"features\"]).show(7)","3629657b":"evaluator = RegressionEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").setMetricName(\"rmse\")\nrmse = evaluator.evaluate(predictions)\nprint(\"Root Mean Squared Error (RMSE) on test data = {0}\".format(str(rmse)))","7154c2ef":"testResult = modelGBT.transform(sptest)\nsubmit = testResult.select([\"Id\",\"prediction\"])\\\n          .withColumn(\"SalePrice\", testResult[\"prediction\"])\\\n          .drop(\"prediction\")","8fcf6106":"#submit.write.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").save(\"..\/input\/output.csv\")\n\njs  = submit.dropna()","db265fff":"#submit.coalesce(1).write.csv('..\/input\/submission_.csv', encoding=\"utf-8\", emptyValue=-1)\n# submit.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"..\/input\/submit__.csv\")\nsubmit = submit.fillna(-1.0)\n#submit.show()\nsubmit.dropna().coalesce(1).write.csv(os.path.join(\"submission.csv\",dataPath), encoding=\"utf-8\", emptyValue=\"na\")","3398e54f":"At this section, I'll use tipical python datascience tools because this is more reliable and suitable tools for EDA part. (I mean using pandas in place of spark's dataframes and etc.)","d1ea5ee1":"Let's list a pars that have correlation more then 0.8","c4b26d58":"### Spark ML","361553af":"### Dataset Exploration:","f31afc9f":"Before train our models, let split features on to two groups: continious and categorical","3a6ac512":"Then prepare spark's dataframes ","68faae00":"For some reasons I'm going to use apache spark ml lib.\nSo, before we begin let's import all nesessary *stuff*, that we need for our research.","578df646":"Let's find a list of most valuable features, then sort it by descending.","0b49852c":"### Data Exploration ","bbc5e2b3":"Here we interrested in features which have more significant effect on sale price.","a9380d69":"Now compare valuable feature's correlation eachother.\n\n(Here I should note, that In my city, there are many paradoxes when, for excample, age of a building has a strong positive correlation with the quality of supporting structures...)\n \nI believe that is not everywhere, so I will solely rely  on digits.)","7808ffed":"Let's take a look on what do we have here","1ddeef64":"**GrLivArea** has higher correlation with SalePrice then TotRmsAbvGrd patam\nas **GarageCars** and **TotalBsmtSF**, so I'll exclude from dataset \"TotRmsAbvGrd\", \"GarageArea\" and \"1stFlrSF\" from our features research.","d96c439c":"## A Bit Different View on Real Estate","2bdeaa2d":"I won't deskribe all valuable features here, but ","fbf629d2":"Totally we have seven features, two of them is year of **YearBuilt** and **YearRemodAdd** (Remodel date). \nI guess, it will good idea to use them as categorical variable splited by decades."}}