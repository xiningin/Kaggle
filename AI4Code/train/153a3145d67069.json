{"cell_type":{"7a2ebe44":"code","e7fd1c20":"code","29c66931":"code","2031bf6c":"code","504e7442":"code","175e74a4":"code","c8eddab2":"code","39a4d9f5":"code","6614d3b5":"code","ac9ece33":"code","bc33dae6":"code","0a08370d":"code","5ec01196":"code","e721b0ec":"code","ea48b5c0":"code","90369d74":"code","746b00ff":"code","6a2f9bf3":"code","731f3e59":"code","257d007f":"code","3c8e6ad8":"code","a078ba29":"code","9471f7b3":"code","5327c8f2":"code","26736aa4":"code","f280eeb7":"code","7a5f8c6d":"code","ea3cb66f":"code","532b399c":"code","def9e11b":"code","c03c24e7":"code","27bdf0fd":"code","c044b743":"code","17ee441e":"code","814f07ed":"code","4b766a66":"code","3ee9f7ac":"code","2eba7458":"code","f81088d5":"code","4ac87679":"code","bf4ae26f":"code","f53ae206":"code","245bdab9":"code","58cf9217":"code","a657e599":"code","d7e49b21":"code","32387dad":"code","7ca13635":"code","ac2dc878":"code","3f302c00":"code","7b1ae454":"code","cda3d249":"code","f0473ab5":"code","48d67030":"code","da0531e3":"code","46bb6a94":"code","9350f979":"markdown","bbcd48ca":"markdown","839a474e":"markdown","2d6808ed":"markdown","f28cad07":"markdown","3b5dd7f2":"markdown","f39a8402":"markdown"},"source":{"7a2ebe44":"# !pip install -U yellowbrick","e7fd1c20":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import r2_score\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nfrom sklearn import svm\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport seaborn as sn\nimport warnings\nfrom sklearn.exceptions import DataConversionWarning\nfrom yellowbrick.regressor import ManualAlphaSelection\nfrom yellowbrick.regressor import AlphaSelection\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn import preprocessing\n\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\n%matplotlib inline","29c66931":"df = pd.read_csv(\"..\/input\/seoul-bike-sharing-demand-prediction\/SeoulBikeData.csv\", encoding = 'unicode_escape')","2031bf6c":"df","504e7442":"#df = pd.get_dummies(df, columns=['Seasons','Holiday', 'Functioning Day'], drop_first=True)","175e74a4":"df['Date'] = pd.to_datetime(df['Date'], errors='coerce')","c8eddab2":"df['WeekDay']=df['Date'].dt.day_name()","39a4d9f5":"df['Month']=df['Date'].dt.month","6614d3b5":"mapping_dictDay={'Monday':1,'Tuesday':2,'Wednesday':3,'Thursday':4,'Friday':5,'Saturday':6,'Sunday':7}\n\ndf['WeekDayEncoding']=df['WeekDay'].map(mapping_dictDay)","ac9ece33":"df","bc33dae6":"plt.title(\"Hours\")\ndf['Hour'].value_counts().plot(kind='pie')\nplt.show()","0a08370d":"df['Temperature(\u00b0C)'].plot(kind='hist')\nplt.show()","5ec01196":"df['Humidity(%)'].plot(kind='hist')\nplt.show()","e721b0ec":"df['Wind speed (m\/s)'].plot(kind='hist')\nplt.show()","ea48b5c0":"df['Visibility (10m)'].plot(kind='hist')\nplt.show()","90369d74":"df['Solar Radiation (MJ\/m2)'].plot(kind='hist')\nplt.show()","746b00ff":"df['Rainfall(mm)'].plot(kind='hist')\nplt.show()","6a2f9bf3":"df['Snowfall (cm)'].plot(kind='hist')\nplt.show()","731f3e59":"plt.title(\"Months\")\ndf['Month'].value_counts().plot(kind='pie')\nplt.show()","257d007f":"df.skew().sort_values(ascending=True) # Snowfall and Rainfall are highly skewed","3c8e6ad8":"corrMatrix = df.corr()\n\nfig, ax = plt.subplots(figsize=(12,12)) # Sample figsize in inches\nsn.heatmap(corrMatrix, annot=True, linewidths=.5, ax=ax)\nplt.show()","a078ba29":"plt.figure(figsize=(5, 10))\nheatmap = sn.heatmap(corrMatrix[['Rented Bike Count']].sort_values(by='Rented Bike Count', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Features Correlating with Rented Bike Count', fontdict={'fontsize':18}, pad=16);","9471f7b3":"rented_bike_corr = pd.DataFrame(corrMatrix.iloc[:, 0])","5327c8f2":"significant = rented_bike_corr[abs(rented_bike_corr) >= 0.05]\nsignificant['Use'] = significant['Rented Bike Count'].notna()","26736aa4":"significant_col = significant.index[significant['Use']].tolist()\nsignificant_col.remove('Rented Bike Count')\nsignificant_col.remove('Dew point temperature(\u00b0C)') # Due to high correlation with regular temperature\nsignificant_col.remove('Rainfall(mm)') # Highly skewed towards 0\nsignificant_col.remove('Snowfall (cm)') # Highly skewed towards 0\nsignificant_col","f280eeb7":"df[significant_col]","7a5f8c6d":"X = df[significant_col]\ny = df['Rented Bike Count']","ea3cb66f":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state= 0)","532b399c":"linreg = LinearRegression()\nlinreg.fit(X_train, y_train)\nprint(linreg.intercept_, linreg.coef_)","def9e11b":"y_pred = linreg.predict(X_test)","c03c24e7":"print('Variance score: %.3f' % linreg.score(X_test, y_test)) # Also the r^2","27bdf0fd":"print('R^2: %.3f' % r2_score(y_test, y_pred))\nprint('Mean Absolute Error: %.3f' % metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error: %.3f' % metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error: %.3f' % np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","c044b743":"alphas = 10**np.linspace(10,-2,100)*0.5","17ee441e":"lasso = Lasso(max_iter = 10000)\ncoefs = []\n\nfor a in alphas:\n    lasso.set_params(alpha=a)\n    lasso.fit(X_train, y_train)\n    coefs.append(lasso.coef_)\n\nplt.figure(figsize=(16, 12))    \nax = plt.gca()\nax.plot(alphas*2, coefs)\nax.set_xscale('log')\nplt.axis('tight')\nplt.xlabel('alpha')\nplt.ylabel('weights')\n","814f07ed":"lassocv = LassoCV(alphas = alphas, cv = 10, max_iter = 100000, random_state=0)\nlassocv.fit(X, y)","4b766a66":"print(\"Alpha: %.3f\" % lassocv.alpha_)\nprint('R^2: %.3f' % r2_score(y, lassocv.predict(X)))\nprint('Mean Squared Error: %.3f' % mean_squared_error(y, lassocv.predict(X)))","3ee9f7ac":"pd.Series(lassocv.coef_, index=X.columns)","2eba7458":"# Instantiate the linear model and visualizer\nmodel = LassoCV(alphas = alphas, cv = 10, max_iter = 100000, random_state=0)\nvisualizer = AlphaSelection(model)\nplt.figure(figsize=(20, 10)) \nvisualizer.ax.set_xscale('log')\nvisualizer.fit(X, y)\nvisualizer.show()","f81088d5":"lasso.set_params(alpha=lassocv.alpha_)\nlasso.fit(X_train, y_train)\nprint('Mean Squared Error: %.3f' % mean_squared_error(y_test, lasso.predict(X_test)))\nprint('R^2: %.3f' % r2_score(y_test,lasso.predict(X_test)))","4ac87679":"lasso_cross_val_scores = cross_val_score(lasso,X,y, scoring='neg_mean_squared_error', cv=lassocv.cv)\ncross_val_scores = np.sqrt(np.abs(lasso_cross_val_scores)) \nprint(cross_val_scores)\nprint(\"mean:\", np.mean(cross_val_scores))","bf4ae26f":"pd.Series(lasso.coef_, index=X.columns)","f53ae206":"alphas = 10**np.linspace(10,-2,100)*0.5","245bdab9":"ridge = Ridge()\ncoefs = []\n\nfor a in alphas:\n  ridge.set_params(alpha = a)\n  ridge.fit(X, y)\n  coefs.append(ridge.coef_)","58cf9217":"plt.figure(figsize=(16, 12)) \nax = plt.gca()\nax.plot(alphas, coefs)\nax.set_xscale('log')\nplt.axis('tight')\nplt.xlabel('alpha')\nplt.ylabel('weights')","a657e599":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state= 0)","d7e49b21":"ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', cv = KFold(n_splits=10, random_state=0, shuffle=False))\nridgecv.fit(X, y)\nprint('Alpha: %.3f' % ridgecv.alpha_)\nprint('Mean Squared Error: %.3f' % mean_squared_error(y, ridgecv.predict(X)))\nprint('R^2: %.3f' % r2_score(y, ridgecv.predict(X)))","32387dad":"pd.Series(ridgecv.coef_, index=X.columns)","7ca13635":"ridgecv1 = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', store_cv_values=True) # This method uses Leave-One-Out Cross validation\nridgecv1.fit(X, y)\nprint('Alpha: %.3f' % ridgecv1.alpha_)\nprint('Mean Squared Error: %.3f' % mean_squared_error(y, ridgecv1.predict(X)))\nprint('R^2: %.3f' % r2_score(y, ridgecv1.predict(X)))","ac2dc878":"# Instantiate the linear model and visualizer\nmodel = RidgeCV(alphas=alphas, scoring = 'neg_mean_squared_error', store_cv_values=True)\nvisualizer = AlphaSelection(model)\nplt.figure(figsize=(20, 10)) \nvisualizer.ax.set_xscale('log')\nvisualizer.fit(X, y)\nvisualizer.show()","3f302c00":"ridge = Ridge(alpha = ridgecv1.alpha_)\nridge.fit(X_train, y_train)\nprint('Mean Squared Error: %.3f' % mean_squared_error(y_test, ridge.predict(X_test)))\nprint('R^2: %.3f' % r2_score(y_test, ridge.predict(X_test)))","7b1ae454":"ridge_cross_val_scores = cross_val_score(ridge, X, y, scoring='neg_mean_squared_error', cv=ridgecv.cv)\ncross_val_scores = np.sqrt(np.abs(ridge_cross_val_scores)) \nprint(cross_val_scores)\nprint(\"mean: %.3f\" % np.mean(cross_val_scores))","cda3d249":"pd.Series(ridge.coef_, index = X.columns)","f0473ab5":"# calculate distortion for a range of number of cluster\ndistortions = []\nfigure(figsize=(16, 8), dpi=80)\n\nfor i in range(1, 16):\n    km = KMeans(\n        n_clusters=i, init='random',\n        n_init=10, max_iter=300,\n        tol=1e-04, random_state=0\n    )\n    km.fit(X)\n    distortions.append(km.inertia_)\n\n# plot\nplt.plot(range(1, 16), distortions, marker='o')\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Distortion')\nplt.show()","48d67030":"kmeans = KMeans(n_clusters=4, random_state=0,).fit(X) # By using an Elbow Method for K-means clustering we find the optimal number of clusters to be = 4\nX['cluster'] = kmeans.labels_\nX.cluster.value_counts()","da0531e3":"centroids = np.array(kmeans.cluster_centers_)\ncentroids = PCA(n_components=2).fit_transform(centroids)","46bb6a94":"# Run PCA on the data and reduce the dimensions in pca_num_components dimensions\nreduced_data = PCA(n_components=2).fit_transform(X)\nresults = pd.DataFrame(reduced_data,columns=['pca1','pca2'])\ncentroids_x = centroids[:,0]\ncentroids_y = centroids[:,1]\nax = sn.set(rc={\"figure.figsize\":(24, 12)})\nax = sn.scatterplot(x=\"pca1\", y=\"pca2\", hue=X['cluster'], data=results, palette=\"bright\", alpha=0.6, s=50, legend='full')\nax = sn.scatterplot(centroids_x, centroids_y, hue=range(kmeans.n_clusters),s=100, palette=\"bright\", ec='black', legend=False, marker='X',ax=ax)\n#ax.set_xscale('log')\nplt.title('K-means Clustering with 2 dimensions')\nplt.show()","9350f979":"# **Regular Linear Regression**","bbcd48ca":"# **Lasso Regression**","839a474e":"# Optional Clustering","2d6808ed":"# Exploratory Data Analysis (EDA)","f28cad07":"# **Ridge Regression**","3b5dd7f2":"# **Instructions**\nThe attached dataset   contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour, and date information. Use the Regression to predict the count of bike count required at each hour for the stable supply of rental bikes.","f39a8402":"# **Conclusion**\n\nI tested a wide range of alphas for both Ridge and Lasso regressions, using cross-validation for both regression types. Ridge regression with an alpha=50 turned out to have the smallest MSE and highest R^2. Lasso regression also performed well with an alpha=7. Normalization of the data could've proven to help create a better model but wasn't part of our task. \n\n\n"}}