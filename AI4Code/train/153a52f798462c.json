{"cell_type":{"b7a1efe1":"code","a32461c2":"code","409fe9c1":"code","59cbb901":"code","5369f527":"code","23c769be":"code","8816d8e9":"code","585a590d":"code","e646f9cb":"code","91bea319":"code","28a9daf2":"code","a04c0202":"code","29516beb":"code","3fbfe219":"code","a25bc7fb":"code","d8f31ee3":"code","f2bb4668":"code","8c2fa28c":"code","0f23cef1":"code","ad83a76e":"code","09541f2e":"code","14372aab":"code","2dfeebcc":"code","3addbc94":"code","daf6df9e":"code","9e63c126":"code","29547f4f":"code","44bba044":"code","e71ff7c0":"markdown","a3ea446b":"markdown","472b6edf":"markdown","26f0d1ae":"markdown","f89eb1ce":"markdown","792a9719":"markdown","db84564a":"markdown","648b1dfd":"markdown","2f642575":"markdown","89128a6a":"markdown","6253f195":"markdown","cecf0132":"markdown","775bb828":"markdown","41e2d5ee":"markdown","179b9011":"markdown","d8b27d1d":"markdown","235ee178":"markdown","6bb21df8":"markdown","a86b7ce9":"markdown","bc1966b2":"markdown","97dccaf4":"markdown","7d94146f":"markdown","a2d22ee3":"markdown","dd4b8919":"markdown","10aca1eb":"markdown","ebf88341":"markdown","8e56b96d":"markdown","1bb51556":"markdown","4778926b":"markdown","ab5d5613":"markdown","84f7b765":"markdown","8e385eba":"markdown","121689e0":"markdown","3f681284":"markdown","56f7aeaa":"markdown"},"source":{"b7a1efe1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a32461c2":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","409fe9c1":"df_train= pd.read_csv(r'\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n#checking top 5 row using head function\ndf_train.head()\n","59cbb901":"#analyzing the salesprice\ndf_train['SalePrice'].describe()","5369f527":"sns.distplot(df_train['SalePrice']);","23c769be":"#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"kurtosis: %f\" % df_train['SalePrice'].kurt())","8816d8e9":"data = pd.concat([df_train['SalePrice'], df_train['GrLivArea']], axis=1)\nprint(data)\ndata.plot.scatter(x='GrLivArea', y='SalePrice',c='black',xlim=(0,6000), ylim=(0,800000));","585a590d":"correlation = df_train['SalePrice'].corr(df_train['GrLivArea'])\nprint(correlation)\n","e646f9cb":"data = pd.concat([df_train['SalePrice'], df_train['TotalBsmtSF']], axis=1)\nprint(data)\ndata.plot.scatter(x='TotalBsmtSF', y='SalePrice',c='green', ylim=(0,800000));","91bea319":"#Box plot is awesome to identify the relationship in categorical vs numerical data.\n#Analyzing the saleprice agist the overallqual.\n\ndata = pd.concat([df_train['SalePrice'], df_train['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(10, 8))\ngraph=sns.boxplot(x=\"OverallQual\", y=\"SalePrice\", data=data)","28a9daf2":"data = pd.concat([df_train['SalePrice'], df_train['YearBuilt']], axis=1)\nf, ax = plt.subplots(figsize=(30, 12))\nfig = sns.boxplot(x='YearBuilt', y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);\n#plt.xticks is used to rotate the the data of x axis. We perfom this operation here becuase the data were ovelapping earlier.","a04c0202":"#correlation matrix\ncorrmatrix = df_train.corr()\nf, ax = plt.subplots(figsize=(25, 12))\nsns.heatmap(corrmatrix,vmax=.8, annot=True,fmt='.1g', cmap= 'coolwarm', linewidths=1, linecolor='black');\n","29516beb":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmatrix.nlargest(k, 'SalePrice')['SalePrice'].index\nf, ax = plt.subplots(figsize=(8, 8))\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","3fbfe219":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols], size = 2.5)\nplt.show();","a25bc7fb":"total_missing_data = df_train.isnull().sum().sort_values(ascending=False)\n\ncount_miss_data=df_train.isnull().count().sort_values(ascending=False)\n\n\npercentage=(total_missing_data\/count_miss_data)\nmiss_data=pd.concat([total_missing_data,percentage], axis=1, keys=['Total', 'Percent'])\nmiss_data.head(25)","d8f31ee3":"#deleting missing variable\n\ndf_train = df_train.drop((miss_data[miss_data['Total'] > 1]).index,axis=1)\ndf_train = df_train.drop(df_train.loc[df_train['Electrical'].isnull()].index)\ndf_train.isnull().sum().max()","f2bb4668":"#standardizing and fit_transform the data\n\nsaleprice_scaled = StandardScaler().fit_transform(df_train['SalePrice'][:,np.newaxis]);\n\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\n\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)","8c2fa28c":"#bivariate analysis saleprice\/grlivarea\ndata=pd.concat([df_train['SalePrice'],df_train['GrLivArea']], axis=1)\ndata.plot.scatter(x='GrLivArea', y='SalePrice',ylim=(0,800000));","0f23cef1":"#deleting the two points.\ndf_train.sort_values(by = 'GrLivArea', ascending = False)[:2]\n#first we sort the data in descending order then pick the first two value using slicing [:2] it will take 0th and 1st position values\ndf_train = df_train.drop(df_train[df_train['Id']==1299].index)\ndf_train = df_train.drop(df_train[df_train['Id'] == 524].index)","ad83a76e":"data = pd.concat([df_train['SalePrice'], df_train['TotalBsmtSF']], axis=1)\ndata.plot.scatter(x='TotalBsmtSF', y='SalePrice', ylim=(0,800000));","09541f2e":"#histogram and normal probability plot\nsns.distplot(df_train['SalePrice'],fit=norm)\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","14372aab":"#fixing with log transformation\ndf_train['SalePrice'] = np.log(df_train['SalePrice'])\n\n#checking the graph again\nsns.distplot(df_train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","2dfeebcc":"#histogram and normal probability plot\nsns.distplot(df_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['GrLivArea'], plot=plt)","3addbc94":"#fixing the same issue again with log transformation\n\n\ndf_train['GrLivArea'] = np.log(df_train['GrLivArea'])\nsns.distplot(df_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['GrLivArea'], plot=plt)","daf6df9e":"sns.distplot(df_train['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['TotalBsmtSF'], plot=plt)","9e63c126":"plt.scatter(df_train['GrLivArea'],df_train['SalePrice']);","29547f4f":"#TotalBsmtSf vs SalePrice\nplt.scatter(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], df_train[df_train['TotalBsmtSF']>0]['SalePrice']);","44bba044":"my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","e71ff7c0":"To analyze the ouliar we will standarize the data in to noraml distribution N(0,1).\nwhere mean =0, and standard deviation =1.","a3ea446b":"Out of these many variables. Now, I am going to analyze the only those which are making sense.","472b6edf":"**Univariate analysis**","26f0d1ae":"Kurtosis= 4th moment about the mean divide by sqaure of 2nd moment about the mean.\nif the value of Kurtosis is greaotor than 3 then it is known as leptokurtic distribution.\n\nSkewness describes how the sample differs in shape from a symmetrical distribution.\n1. normal distribution has a skewness of 0, \n2. right skewed is greater then 0\n3. left skewed is less than 0.\n\nHere we get skewness of 1.882876. So it is postive.","f89eb1ce":"Previous scatter plot between these two variable has a shape of cone. Now it doesnt.\nnormalizing the data fixed the homoscedasticity issue.","792a9719":"checking TotalBsmtSF for normality","db84564a":"AS the overalqual increases the median sales prices in each category increases.","648b1dfd":"It is really hard to say anything in this case. ","2f642575":"Outliars can affect whole analysis process in a very bad way. We can not neglect them becasue some time there is fruitful meaning in outliars and sometime they are irrelevant. So before making a quick conclusion lets analyze the outliar situation.","89128a6a":"# Dealing with outliars","6253f195":"we can easily see the two points one betweeb 4k-5k on x axis and another highger than than thr 5k on x axis. Despite of having larger area there sales price are low. It may becuase they are in village or tier 3 cities. we can delete these values becuase it is not represeting our case in\\\\\\","cecf0132":"It looks more like linear relationship.","775bb828":"There are two data point in the top, which we said earlier have value higher than 7. we will considerd these values becasue they are following the trend.","41e2d5ee":"**1. overallqual vs salesprice**","179b9011":"# **Loading the data set.**","d8b27d1d":"# Relationship with numerical variables","235ee178":"# Getting into details","6bb21df8":"1. **analyzing the correlation between GrLivArea varibale with SalePrice using scatterplot**","a86b7ce9":"Untill now we explored the things randomly. But thats not how things work. We need a set of rules so that we can ease out work. So from now onward we will explore thing in systematic way.","bc1966b2":"**homoscedasticity**","97dccaf4":"Now we will test 4 statistical assumption, So that we can apply multivariate technique\n1. Normality \n2. Homoscedasticity \n3. Linearity\n4. Absence of correlated errors","7d94146f":"we will delete those variale whoes values missing is more than 15%. Those varible also not providing us any significant information and it is better to remove them.","a2d22ee3":"# Relationship with categorical features","dd4b8919":"**2. YearBuilt vs salesprice**","10aca1eb":"Few data are far far away but they are following the trend so we will not delete those data points.","ebf88341":"**Bivariate analysis**","8e56b96d":"**normality**\n\nOur data should follow normal distribution so that we can perform various statistics test such as t Test, chi square test etc.","1bb51556":"Correlation matrix gives the correlation coeffiecents between two variable.\nIt is superb technique to analyze the multiple variable at once. I have use the Anot=true to assign the value in each box for more easier visualization.","4778926b":"There is strong linear relationship between these two variable.","ab5d5613":"checking GrLivArea for normality","84f7b765":"# Handling missing data","8e385eba":"2. **analyzing the correlation between TotalBsmtSF varibale with SalePrice using scatterplot**","121689e0":"It follows normal distribution now. and ready for statistical test.","3f681284":"we can see our salesprice is not normal. we can fix this issue with log transformation","56f7aeaa":"Correlation is a statistic that measures the degree to which two variables move in relation to each other.\n"}}