{"cell_type":{"072da124":"code","284fcfc1":"code","7b994c8c":"code","e98fc0f5":"code","5d8168ef":"code","ac90a90f":"code","71e68687":"code","a8fdb66c":"code","3bea5961":"code","9812bd37":"code","312bb2e4":"code","8d45229e":"code","dbbbb807":"code","c4df88cb":"code","a915f3b6":"code","03b49f0b":"code","6f267179":"markdown","09fa9490":"markdown","fd951690":"markdown","3daa425d":"markdown","5592e94d":"markdown","c55aa4ec":"markdown","4e78b42f":"markdown","ded51f24":"markdown","be612626":"markdown","8576f983":"markdown","db97ff3a":"markdown","5383c985":"markdown","bfe9e293":"markdown"},"source":{"072da124":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","284fcfc1":"from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\nimport pandas, xgboost, numpy, textblob, string\nfrom keras.preprocessing import text, sequence\n\nfrom keras import layers, models, optimizers","7b994c8c":"# load the dataset\ndata = open('..\/input\/corpus.txt').read()\nlabels, texts = [], []\nfor i, line in enumerate(data.split(\"\\n\")):\n    content = line.split()\n    labels.append(content[0])\n    texts.append(\" \".join(content[1:]))\n\n# create a dataframe using texts and lables\ntrainDF = pandas.DataFrame()\ntrainDF['text'] = texts\ntrainDF['label'] = labels","e98fc0f5":"# split the dataset into training and validation datasets \ntrain_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n\n# label encode the target variable \nencoder = preprocessing.LabelEncoder()\ntrain_y = encoder.fit_transform(train_y)\nvalid_y = encoder.fit_transform(valid_y)","5d8168ef":"# create a count vectorizer object \ncount_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\ncount_vect.fit(trainDF['text'])\n\n# transform the training and validation data using count vectorizer object\nxtrain_count =  count_vect.transform(train_x)\nxvalid_count =  count_vect.transform(valid_x)","ac90a90f":"# word level tf-idf\n#  Word Level TF-IDF : Matrix representing tf-idf scores of every term in different documents\n\ntfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(trainDF['text'])\nxtrain_tfidf =  tfidf_vect.transform(train_x)\nxvalid_tfidf =  tfidf_vect.transform(valid_x)\n# N-gram Level TF-IDF : N-grams are the combination of N terms together. This Matrix representing tf-idf scores of N-grams\n\n# ngram level tf-idf \ntfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\ntfidf_vect_ngram.fit(trainDF['text'])\nxtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\nxvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n# Character Level TF-IDF : Matrix representing tf-idf scores of character level n-grams in the corpus\n# characters level tf-idf\ntfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\ntfidf_vect_ngram_chars.fit(trainDF['text'])\nxtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \nxvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) ","71e68687":"# load the pre-trained word-embedding vectors \nembeddings_index = {}\nfor i, line in enumerate(open('..\/input\/corpus.txt')):\n    values = line.split()\n    #embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n\n# create a tokenizer \ntoken = text.Tokenizer()\ntoken.fit_on_texts(trainDF['text'])\nword_index = token.word_index\n\n# convert text to sequence of tokens and pad them to ensure equal length vectors \ntrain_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\nvalid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n\n# create token-embedding mapping\nembedding_matrix = numpy.zeros((len(word_index) + 1, 300))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","a8fdb66c":"trainDF['char_count'] = trainDF['text'].apply(len)\ntrainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\ntrainDF['word_density'] = trainDF['char_count'] \/ (trainDF['word_count']+1)\ntrainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \ntrainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\ntrainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))","3bea5961":"# train a LDA Model\nlda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\nX_topics = lda_model.fit_transform(xtrain_count)\ntopic_word = lda_model.components_ \nvocab = count_vect.get_feature_names()\n\n# view the topic models\nn_top_words = 10\ntopic_summaries = []\nfor i, topic_dist in enumerate(topic_word):\n    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n    topic_summaries.append(' '.join(topic_words))","9812bd37":"def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n    \n    # predict the labels on validation dataset\n    predictions = classifier.predict(feature_vector_valid)\n    \n    if is_neural_net:\n        predictions = predictions.argmax(axis=-1)\n    \n    return metrics.accuracy_score(predictions, valid_y)","312bb2e4":"# Naive Bayes on Count Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\nprint ('NB, Count Vectors:', accuracy)\n\n# Naive Bayes on Word Level TF IDF Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\nprint ('NB, WordLevel TF-IDF:', accuracy)\n\n# Naive Bayes on Ngram Level TF IDF Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\nprint ('NB, N-Gram Vectors: ', accuracy)\n\n# Naive Bayes on Character Level TF IDF Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\nprint ('NB, CharLevel Vectors: ', accuracy)\n","8d45229e":"# Linear Classifier on Count Vectors\naccuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\nprint ('LR, Count Vectors:  ', accuracy)\n\n# Linear Classifier on Word Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\nprint ('LR, WordLevel TF-IDF: ', accuracy)\n\n# Linear Classifier on Ngram Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\nprint ('LR, N-Gram Vectors: ', accuracy)\n\n# Linear Classifier on Character Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\nprint ('LR, CharLevel Vectors:', accuracy)\n","dbbbb807":"# SVM on Ngram Level TF IDF Vectors\naccuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\nprint ('SVM, N-Gram Vectors:', accuracy)\n","c4df88cb":"# RF on Count Vectors\naccuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\nprint ('RF, Count Vectors: ', accuracy)\n\n# RF on Word Level TF IDF Vectors\naccuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\nprint ('RF, WordLevel TF-IDF:', accuracy)\n","a915f3b6":"# Extereme Gradient Boosting on Count Vectors\naccuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\nprint ('Xgb, Count Vectors:', accuracy)\n\n# Extereme Gradient Boosting on Word Level TF IDF Vectors\naccuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\nprint ('Xgb, WordLevel TF-IDF:', accuracy)\n\n# Extereme Gradient Boosting on Character Level TF IDF Vectors\naccuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\nprint ('Xgb, CharLevel Vectors:', accuracy)\n","03b49f0b":"def create_cnn():\n    # Add an Input Layer\n    input_layer = layers.Input((70, ))\n\n    # Add the word embedding Layer\n    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n    # Add the convolutional Layer\n    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n\n    # Add the pooling Layer\n    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n\n    # Add the output Layers\n    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n    output_layer1 = layers.Dropout(0.25)(output_layer1)\n    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n    # Compile the model\n    model = models.Model(inputs=input_layer, outputs=output_layer2)\n    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n    \n    return model\n\nclassifier = create_cnn()\naccuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint (\"CNN, Word Embeddings\",  accuracy)","6f267179":"## 3-Bagging Model\n","09fa9490":"# Model Building \n## 1- Naive Bayes","fd951690":"## 2- Linear Classifier","3daa425d":"# Feature Engineering\n\n## 1-Count Vectors as features","5592e94d":"## 5- Topic Models as features\n","c55aa4ec":"\n\n## 3- Word Embeddings as features\n","4e78b42f":"##  4- Text \/ NLP based features\n","ded51f24":"## 5.1-Deep Neural Networks","be612626":"## 4- Boosting Model","8576f983":"## 3- Implementing a SVM Model","db97ff3a":"## 2- TF-IDF Vectors as features\n### 2.1Word level\n### 2.2N-Gram level\n### 2.3Character level\n","5383c985":"* How do we transform raw text data into flat features?\u00b6\n* Just explain and use one existing methods.\n* Suggested Methods you can start with:\n* \u2022Count Vectors as features\n* \u2022TF-IDF Vectors as features\n* \u2022Word level\n* \u2022N-Gram level\n* \u2022Character level\n* \u2022Word Embeddings as features\n* \u2022Text \/ NLP based features\n* \u2022Topic Models as features\n* libraries for dataset preparation, feature engineering, model training","bfe9e293":"# Model Building"}}