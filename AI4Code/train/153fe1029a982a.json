{"cell_type":{"b847d6f0":"code","7f1307be":"code","624adbcc":"code","0045bc0f":"code","93c72aed":"code","6681d71b":"code","37bd4cba":"code","a6043b04":"code","59b08ef0":"code","ae9dcc79":"code","87822f04":"code","ba3aca2c":"code","81b75012":"code","4ff9d35c":"code","7b3cbbf2":"code","0768d59f":"code","4384f055":"code","67f08bf6":"code","aa7a70b1":"code","e2c1b409":"code","1c3f3dd5":"code","b62001df":"code","9c281c89":"code","f29c9e9c":"code","a07de90a":"code","28f526d1":"code","7b9e8da5":"code","00fb99a0":"code","0a3d0b24":"code","6702f979":"code","d3a8fe12":"code","7a62fd3c":"code","0faf6a74":"code","c576f11d":"code","3a4bca04":"code","2ec43b4c":"code","02323262":"code","f3f47dd8":"code","33d7677c":"code","eb842129":"code","1df07c23":"code","11b0202c":"code","f8701c87":"code","78771090":"code","bf9006e3":"code","dea0edcd":"code","53452da9":"code","05aa11da":"code","7d359381":"markdown","c28125f8":"markdown","057d7c01":"markdown","287c1465":"markdown","167227de":"markdown","6b2612df":"markdown","f01afd99":"markdown","1c6d2ca0":"markdown","f3f675bd":"markdown","cb3cecac":"markdown","dd769fb1":"markdown","f957263b":"markdown","70d257fd":"markdown","5ca884d3":"markdown","fd2d4a66":"markdown","68b18e07":"markdown","2f40e523":"markdown"},"source":{"b847d6f0":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import r2_score\nimport os\nimport glob\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\nimport gc\n\nfrom sklearn.model_selection import train_test_split, KFold\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor","7f1307be":"class Config:\n    data_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n    seed = 42","624adbcc":"train = pd.read_csv(Config.data_dir + 'train.csv')\ntrain.head()","0045bc0f":"train.stock_id.unique()","93c72aed":"test = pd.read_csv(Config.data_dir + 'test.csv')\ntest.head()","6681d71b":"display(train.groupby('stock_id').size())\n\nprint(\"\\nUnique size values\")\ndisplay(train.groupby('stock_id').size().unique())","37bd4cba":"def get_trade_and_book_by_stock_and_time_id(stock_id, time_id=None, dataType = 'train'):\n    book_example = pd.read_parquet(f'{Config.data_dir}book_{dataType}.parquet\/stock_id={stock_id}')\n    trade_example =  pd.read_parquet(f'{Config.data_dir}trade_{dataType}.parquet\/stock_id={stock_id}')\n    if time_id:\n        book_example = book_example[book_example['time_id']==time_id]\n        trade_example = trade_example[trade_example['time_id']==time_id]\n    book_example.loc[:,'stock_id'] = stock_id\n    trade_example.loc[:,'stock_id'] = stock_id\n    return book_example, trade_example","a6043b04":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\ndef calculate_wap1(df):\n    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n    b1 = df['bid_size1'] + df['ask_size1']\n    a2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n    b2 = df['bid_size2'] + df['ask_size2']\n    \n    x = (a1\/b1 + a2\/b2)\/ 2\n    \n    return x\n\n\ndef calculate_wap2(df):\n        \n    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n    a2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n    b = df['bid_size1'] + df['ask_size1'] + df['bid_size2']+ df['ask_size2']\n    \n    x = (a1 + a2)\/ b\n    return x\n\ndef realized_volatility_per_time_id(file_path, prediction_column_name):\n\n    stock_id = file_path.split('=')[1]\n\n    df_book = pd.read_parquet(file_path)\n    df_book['wap1'] = calculate_wap1(df_book)\n    df_book['wap2'] = calculate_wap2(df_book)\n\n    df_book['log_return1'] = df_book.groupby(['time_id'])['wap1'].apply(log_return)\n    df_book['log_return2'] = df_book.groupby(['time_id'])['wap2'].apply(log_return)\n    df_book = df_book[~df_book['log_return1'].isnull()]\n\n    df_rvps =  pd.DataFrame(df_book.groupby(['time_id'])[['log_return1', 'log_return2']].agg(realized_volatility)).reset_index()\n    df_rvps[prediction_column_name] = 0.6 * df_rvps['log_return1'] + 0.4 * df_rvps['log_return2']\n\n    df_rvps['row_id'] = df_rvps['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    \n    return df_rvps[['row_id',prediction_column_name]]","59b08ef0":"def get_agg_info(df):\n    agg_df = df.groupby(['stock_id', 'time_id']).agg(mean_sec_in_bucket = ('seconds_in_bucket', 'mean'), \n                                                     mean_price = ('price', 'mean'),\n                                                     mean_size = ('size', 'mean'),\n                                                     mean_order = ('order_count', 'mean'),\n                                                     max_sec_in_bucket = ('seconds_in_bucket', 'max'), \n                                                     max_price = ('price', 'max'),\n                                                     max_size = ('size', 'max'),\n                                                     max_order = ('order_count', 'max'),\n                                                     min_sec_in_bucket = ('seconds_in_bucket', 'min'), \n                                                     min_price = ('price', 'min'),\n                                                     #min_size = ('size', 'min'),\n                                                     #min_order = ('order_count', 'min'),\n                                                     median_sec_in_bucket = ('seconds_in_bucket', 'median'), \n                                                     median_price = ('price', 'median'),\n                                                     median_size = ('size', 'median'),\n                                                     median_order = ('order_count', 'median')\n                                                    ).reset_index()\n    \n    return agg_df","ae9dcc79":"def get_stock_stat(stock_id : int, dataType = 'train'):\n    \n    book_subset, trade_subset = get_trade_and_book_by_stock_and_time_id(stock_id, dataType=dataType)\n    book_subset.sort_values(by=['time_id', 'seconds_in_bucket'])\n\n    ## book data processing\n    \n    book_subset['bas'] = (book_subset[['ask_price1', 'ask_price2']].min(axis = 1)\n                                \/ book_subset[['bid_price1', 'bid_price2']].max(axis = 1)\n                                - 1)                               \n\n    \n    book_subset['wap1'] = calculate_wap1(book_subset)\n    book_subset['wap2'] = calculate_wap2(book_subset)\n    \n    book_subset['log_return_bid_price1'] = np.log(book_subset['bid_price1'].pct_change() + 1)\n    book_subset['log_return_ask_price1'] = np.log(book_subset['ask_price1'].pct_change() + 1)\n    # book_subset['log_return_bid_price2'] = np.log(book_subset['bid_price2'].pct_change() + 1)\n    # book_subset['log_return_ask_price2'] = np.log(book_subset['ask_price2'].pct_change() + 1)\n    book_subset['log_return_bid_size1'] = np.log(book_subset['bid_size1'].pct_change() + 1)\n    book_subset['log_return_ask_size1'] = np.log(book_subset['ask_size1'].pct_change() + 1)\n    # book_subset['log_return_bid_size2'] = np.log(book_subset['bid_size2'].pct_change() + 1)\n    # book_subset['log_return_ask_size2'] = np.log(book_subset['ask_size2'].pct_change() + 1)\n    book_subset['log_ask_1_div_bid_1'] = np.log(book_subset['ask_price1'] \/ book_subset['bid_price1'])\n    book_subset['log_ask_1_div_bid_1_size'] = np.log(book_subset['ask_size1'] \/ book_subset['bid_size1'])\n    \n\n    book_subset['log_return1'] = (book_subset.groupby(by = ['time_id'])['wap1'].\n                                  apply(log_return).\n                                  reset_index(drop = True).\n                                  fillna(0)\n                                 )\n    book_subset['log_return2'] = (book_subset.groupby(by = ['time_id'])['wap2'].\n                                  apply(log_return).\n                                  reset_index(drop = True).\n                                  fillna(0)\n                                 )\n    \n    stock_stat = pd.merge(\n        book_subset.groupby(by = ['time_id'])['log_return1'].agg(realized_volatility).reset_index(),\n        book_subset.groupby(by = ['time_id'], as_index = False)['bas'].mean(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    \n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return2'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    \n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return_bid_price1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return_ask_price1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return_bid_size1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return_ask_size1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    \n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_ask_1_div_bid_1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_ask_1_div_bid_1_size'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    \n    \n    stock_stat['stock_id'] = stock_id\n    \n    # Additional features that can be added. Referenced from https:\/\/www.kaggle.com\/yus002\/realized-volatility-prediction-lgbm-train\/data\n    \n    # trade_subset_agg = get_agg_info(trade_subset)\n    \n    #     stock_stat = pd.merge(\n    #         stock_stat,\n    #         trade_subset_agg,\n    #         on = ['stock_id', 'time_id'],\n    #         how = 'left'\n    #     )\n    \n    ## trade data processing \n    \n    return stock_stat\n\ndef get_data_set(stock_ids : list, dataType = 'train'):\n\n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat)(stock_id, dataType) \n        for stock_id in stock_ids\n    )\n    \n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n\n    return stock_stat_df","87822f04":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))","ba3aca2c":"def plot_feature_importance(df, model):\n    feature_importances_df = pd.DataFrame({\n        'feature': df.columns,\n        'importance_score': model.feature_importances_\n    })\n    plt.rcParams[\"figure.figsize\"] = [10, 5]\n    ax = sns.barplot(x = \"feature\", y = \"importance_score\", data = feature_importances_df)\n    ax.set(xlabel=\"Features\", ylabel = \"Importance Score\")\n    plt.xticks(rotation=45)\n    plt.show()\n    return feature_importances_df","81b75012":"book_stock_1, trade_stock_1 = get_trade_and_book_by_stock_and_time_id(1, 5)\ndisplay(book_stock_1.shape)\ndisplay(trade_stock_1.shape)","4ff9d35c":"book_stock_1.head()","7b3cbbf2":"trade_stock_1.head()","0768d59f":"%%time\ntrain_stock_stat_df = get_data_set(train.stock_id.unique(), dataType = 'train')\ntrain_stock_stat_df.head()","4384f055":"train_data_set = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\ntrain_data_set.head()","67f08bf6":"train_data_set.info()","aa7a70b1":"%%time\ntest_stock_stat_df = get_data_set(test['stock_id'].unique(), dataType = 'test')\ntest_stock_stat_df","e2c1b409":"test_data_set = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\ntest_data_set.fillna(-999, inplace=True)\ntest_data_set","1c3f3dd5":"train_data_set.to_pickle('train_features_df.pickle')\ntest_data_set.to_pickle('test_features_df.pickle')","b62001df":"x = gc.collect()","9c281c89":"X_display = train_data_set.drop(['stock_id', 'time_id', 'target'], axis = 1)\nX = X_display.values\ny = train_data_set['target'].values\n\nX.shape, y.shape","f29c9e9c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=Config.seed, shuffle=False)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","a07de90a":"rs = Config.seed","28f526d1":"import optuna\nfrom optuna.samplers import TPESampler\n\ndef objective(trial, data=X, target=y):\n    \n    def rmspe(y_true, y_pred):\n        return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=rs, shuffle=False)\n    \n    param = {\n        'tree_method':'gpu_hist', \n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': trial.suggest_int('n_estimators', 500, 3000),\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300)}\n    \n    model = XGBRegressor(**param)\n    \n    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-rmse\")\n    model.fit(X_train ,y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)\n    \n    preds = model.predict(X_test)\n    \n    rmspe = rmspe(y_test, preds)\n    \n    return rmspe","7b9e8da5":"study = optuna.create_study(sampler=TPESampler(), direction='minimize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\nstudy.optimize(objective, n_trials=1000, gc_after_trial=True)","00fb99a0":"print('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","0a3d0b24":"optuna.visualization.plot_optimization_history(study)","6702f979":"optuna.visualization.plot_param_importances(study)","d3a8fe12":"best_xgbparams = study.best_params\nbest_xgbparams","7a62fd3c":"xgb = XGBRegressor(**best_xgbparams, tree_method='gpu_hist')","0faf6a74":"%%time\nxgb.fit(X_train ,y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)\n\npreds = xgb.predict(X_test)\nR2 = round(r2_score(y_true = y_test, y_pred = preds), 5)\nRMSPE = round(rmspe(y_true = y_test, y_pred = preds), 5)\nprint(f'Performance of the Tuned XGB prediction: R2 score: {R2}, RMSPE: {RMSPE}')","c576f11d":"def objective(trial):\n    \n    def rmspe(y_true, y_pred):\n        return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=rs, shuffle=False)\n    valid = [(X_test, y_test)]\n    \n    param = {\n        \"device\": \"gpu\",\n        \"metric\": \"rmse\",\n        \"verbosity\": -1,\n        'learning_rate':trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 500),\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 4000),\n#         \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 100000, 700000),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100)}\n\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"rmse\")\n    model = LGBMRegressor(**param)\n    \n    model.fit(X_train, y_train, eval_set=valid, verbose=False, callbacks=[pruning_callback], early_stopping_rounds=100)\n\n    preds = model.predict(X_test)\n    \n    rmspe = rmspe(y_test, preds)\n    return rmspe","3a4bca04":"study = optuna.create_study(sampler=TPESampler(), direction='minimize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\nstudy.optimize(objective, n_trials=1000, gc_after_trial=True)","2ec43b4c":"print('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","02323262":"optuna.visualization.plot_optimization_history(study)","f3f47dd8":"optuna.visualization.plot_param_importances(study)","33d7677c":"best_lgbmparams = study.best_params\nbest_lgbmparams","eb842129":"lgbm = LGBMRegressor(**best_lgbmparams, device='gpu')","1df07c23":"%%time\nlgbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False, early_stopping_rounds=100)\n\npreds = xgb.predict(X_test)\nR2 = round(r2_score(y_true = y_test, y_pred = preds), 6)\nRMSPE = round(rmspe(y_true = y_test, y_pred = preds), 6)\nprint(f'Performance of the Tuned LIGHTGBM prediction: R2 score: {R2}, RMSPE: {RMSPE}')","11b0202c":"def_xgb = XGBRegressor(tree_method='gpu_hist', random_state = rs, n_jobs= - 1)\n\ndef_lgbm = LGBMRegressor(device='gpu', random_state=rs)","f8701c87":"from sklearn.ensemble import StackingRegressor\n\n\nestimators = [('def_xgb', def_xgb),\n              ('def_lgbm', def_lgbm),\n              ('tuned_xgb', xgb)]\n\nclf = StackingRegressor(estimators=estimators, final_estimator=lgbm, verbose=1)","78771090":"%%time\nclf.fit(X_train, y_train)","bf9006e3":"preds = clf.predict(X_test)\nR2 = round(r2_score(y_true = y_test, y_pred = preds),6)\nRMSPE = round(rmspe(y_true = y_test, y_pred = preds), 6)\nprint(f'Performance of the STACK prediction: R2 score: {R2}, RMSPE: {RMSPE}')","dea0edcd":"test_data_set_final = test_data_set.drop(['stock_id', 'time_id'], axis = 1)\n\ny_pred = test_data_set_final[['row_id']]\nX_test = test_data_set_final.drop(['row_id'], axis = 1)","53452da9":"X_test","05aa11da":"y_pred = y_pred.assign(target = clf.predict(X_test))\ny_pred.to_csv('submission.csv',index = False)","7d359381":"# Stacking Regressor","c28125f8":"### Config","057d7c01":"#### Plotting","287c1465":"#### Storing for later usages. Processing time for features took 25 mins\nYou can directly use this from the notebook output and build on that","167227de":"# Optuna Tuned XGBoost","6b2612df":"#### Metric","f01afd99":"#### File reading","1c6d2ca0":"#### Most of the feature engineering code","f3f675bd":"#### Feature engineering","cb3cecac":"### Preparing Train and Test set for training and prediction with the desired features\nThe following cell takes around 25 mins for execution. You can also use the pickled data from the notebook output and build on that","dd769fb1":"### Helper Functions","f957263b":"# Optuna Tuned LGBM","70d257fd":"# Submission","5ca884d3":"### Example of book and trade data","fd2d4a66":"Wouldnt be possible without these amazing notebooks\n\n* https:\/\/www.kaggle.com\/abhishek1aa\/feature-engineering-xgboost-lgbm-baseline\/notebook\n* https:\/\/www.kaggle.com\/yus002\/realized-volatility-prediction-lgbm-train\/data\n* https:\/\/www.kaggle.com\/konradb\/we-need-to-go-deeper","68b18e07":"THIS WORK IS COPIED FROM https:\/\/www.kaggle.com\/munumbutt\/feature-engineering-tuned-xgboost-lgbm","2f40e523":"### Imports"}}