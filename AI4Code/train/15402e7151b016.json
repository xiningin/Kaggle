{"cell_type":{"383b2188":"code","a5d5bdcf":"code","683daa3d":"code","b8292513":"code","e21551a8":"code","0fee295f":"code","d72e05c2":"code","56617cc8":"code","f93f8537":"code","66224671":"code","46a8c9f0":"code","9228b534":"code","f8fbd89b":"code","e26032c8":"code","67786b51":"code","60d1f3a8":"code","2f662853":"code","67c7a3ba":"code","de2e6ab1":"code","7574f088":"code","c75a2717":"code","4d9dc84d":"code","9b109f09":"code","f2310bfe":"code","05585418":"code","dcf84468":"code","a028130b":"code","e1b8ed63":"code","80431be7":"code","bcc2da1a":"code","1681e76a":"code","78bc7de0":"code","0d2712b1":"code","d8733243":"code","eaf1129a":"code","5196dea3":"code","faabb9c8":"code","da8f85d4":"code","71dba2fc":"code","683a752e":"code","9d86ff73":"code","9b199eb9":"code","94c4daeb":"code","b479c7f3":"code","49f6ab5b":"code","9ebce4e6":"code","aebc81fd":"code","e4b0a1ed":"code","780a9ee0":"code","feb8932d":"code","15ee70dd":"code","376149a7":"code","c0d8f3ec":"code","7e1741d5":"code","79e3e137":"code","ec6ece3b":"code","431fe9e4":"code","42723cda":"code","2e6b3268":"code","e49c6863":"code","0b7a2b34":"code","ca54603a":"code","1fb07693":"code","0b405c3a":"code","ad1fdd97":"code","56dea4e6":"code","66a8bcfe":"code","38600cdc":"code","8e676e76":"code","599df995":"code","4cd8a269":"code","d3ab4ad5":"code","4b78d2d4":"code","1378489b":"code","347438b8":"code","54f35c40":"code","532eb711":"code","69ca3957":"code","2c2d96d3":"code","c9cb5e07":"code","129c6067":"code","a415c784":"code","713d4d5a":"code","7de898f1":"markdown","69d60e75":"markdown","516d0e2d":"markdown","bafab6f4":"markdown","9de7bcd4":"markdown","675872f4":"markdown","cce05450":"markdown","4e366e2d":"markdown","ffbe4441":"markdown","aa673ee9":"markdown","0486859c":"markdown","9285d552":"markdown","a21bcb1c":"markdown","8bf2f53e":"markdown","4a4c423d":"markdown","6f3032e0":"markdown","bfe69162":"markdown","fb4f63af":"markdown","6ffc9c19":"markdown","be1293b7":"markdown","9f1b943b":"markdown","550d57f2":"markdown","8fd6848c":"markdown","0cf32b4e":"markdown","d42e518c":"markdown","927589d3":"markdown","c3351093":"markdown","c58fe95e":"markdown","5f3fb3dc":"markdown","cd7f5d92":"markdown","0ed6acc1":"markdown","1a98ecf5":"markdown","15abd475":"markdown","0e68dd13":"markdown","950973dd":"markdown","13b0708a":"markdown","9e29d2a2":"markdown","2a05b8c7":"markdown","a839e1af":"markdown","f72b7dcd":"markdown","149a31c3":"markdown","3dcdecc8":"markdown","bf5d997c":"markdown","8336e943":"markdown","35c23165":"markdown","b0aa54d4":"markdown","7bb0021c":"markdown","9f5ed985":"markdown","d8a39b6b":"markdown","a227ec9f":"markdown","281bd8c8":"markdown","6d7aca16":"markdown","afdfcb6e":"markdown","d31cfb2c":"markdown","39d07cd0":"markdown","eee4a680":"markdown","d33f7483":"markdown","fd6f0056":"markdown","f2287f7f":"markdown","3c1714f5":"markdown","5781476f":"markdown"},"source":{"383b2188":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a5d5bdcf":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# allow multi-outputs\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","683daa3d":"input = pd.read_csv(\"..\/input\/insurance\/insurance.csv\")","b8292513":"input.head(5)\ninput.info()\ninput.describe()","e21551a8":"plt.style.use('ggplot')","0fee295f":"for col in input.loc[:,['age','bmi','children','charges']].columns:\n    sns.distplot(a=input[col]);\n    plt.show();","d72e05c2":"input.groupby('children').charges.agg(['median','mean']).plot(kind='bar', title='Charges by number of children - mean vs median');","56617cc8":"plt.figure(figsize=(8,5))\nplt.title('Charges by smoker status')\nsns.violinplot(data=input, x='smoker', y='charges');\nplt.show()\n\nplt.figure(figsize=(8,5))\nsns.scatterplot(x=input['bmi'], y=input['charges'], hue=input['smoker']);\nplt.title('Charges by BMI');\nplt.show();","f93f8537":"plt.figure(figsize=(8,8))\nplt.title('Charges by region')\nsns.swarmplot(x=input['region'], y=input['charges']);\nplt.show();\n\nplt.figure(figsize=(8,8))\nplt.title('Charges by region')\nsns.boxplot(data=input, x='region', y='charges');\nsns.swarmplot(data=input, x='region', y='charges', size=2, color=\".3\")\nplt.show();\n\ninput.groupby('region').charges.agg(['mean','median']).sort_values(by='region', ascending=False).plot(kind=\"bar\", title='Mean and median charges by region');","66224671":"cnt_smoker_byRegion = input.groupby(['region', 'smoker']).agg({'smoker':'count'})\ncnt_byRegion = input.groupby('region').agg({'smoker':'count'})\ncnt_smoker_byRegion.div(cnt_byRegion, level='region')","46a8c9f0":"plt.figure(figsize=(8,8))\nsns.boxplot(data=input, x='region', y='bmi', hue='smoker');\nplt.title(\"BMI by region and smoker status\");","9228b534":"sns.boxplot(data=input.loc[(((input.region=='southeast') & (input.charges<42000)) | ((input.region=='northeast') & (input.charges<35000)))],\n            x='smoker', y='charges', hue='region');\nplt.show();\n\nsns.boxplot(data=input.loc[(((input.region=='southeast') & (input.charges<42000)) | ((input.region=='northeast') & (input.charges<35000)))],\n            x='sex', y='charges', hue='region');\nplt.show();\n\nsns.boxplot(data=input.loc[(((input.region=='southeast') & (input.charges<42000)) | ((input.region=='northeast') & (input.charges<35000)))],\n            x='children', y='charges', hue='region')\nplt.show();","f8fbd89b":"input.loc[(((input.region=='southeast') & (input.charges<42000)) | ((input.region=='northeast') & (input.charges<35000)))].groupby(['region','children']).charges.median().plot(kind=\"bar\");","e26032c8":"from scipy.stats import kruskal","67786b51":"sw = input.loc[input.region=='southwest','charges']\nse = input.loc[input.region=='southeast','charges']\nne = input.loc[input.region=='northeast','charges']\nnw = input.loc[input.region=='northwest','charges']\n\nkruskal(sw, se, ne, nw)","60d1f3a8":"fg = sns.FacetGrid(data=input, row='region', col='children');\nfg.map(plt.scatter, 'bmi', 'charges');\nfg.add_legend();","2f662853":"input.groupby('sex').charges.agg(['mean','median'])","67c7a3ba":"plt.figure(figsize=(8,8))\nplt.title('Charges by gender')\nsns.boxplot(data=input, x='sex', y='charges');\nplt.show();","de2e6ab1":"plt.figure(figsize=(8,8))\nplt.title('Charges by gender')\nsns.boxplot(data=input.loc[(((input.sex == 'female') & (input.charges < 30000)) | ((input.sex == 'male') & (input.charges < 40000))),:], x='sex', y='charges');\nplt.show();","7574f088":"input.loc[(((input.sex == 'female') & (input.charges < 30000)) | ((input.sex == 'male') & (input.charges < 40000))),:].groupby('sex').charges.agg(['mean','median'])","c75a2717":"sns.boxplot(data=input, x='smoker', y='charges', hue='sex')\nplt.title(\"Charges by gender and smoker status\")\nplt.show();\n\ninput.groupby(['sex','smoker']).charges.median().sort_values().plot(title='Median charges by gender and smoker status');","4d9dc84d":"input.groupby(['sex', 'smoker']).bmi.mean().sort_values().plot(title='Average BMI by gender and smoker status');\n\nsns.lmplot(data=input, x='bmi', y='charges', hue='sex');\nplt.title('Regression line for BMI\/Charges, gender-wise')\nplt.show();","9b109f09":"from scipy.stats import mannwhitneyu","f2310bfe":"mannwhitneyu(input.loc[input.sex=='female','charges'].values,input.loc[input.sex=='male','charges'].values)","05585418":"plt.figure(figsize=(8,8))\nplt.title('Charges by number of children')\nsns.boxplot(data=input, x='children', y='charges');\nplt.show();","dcf84468":"# split the charges by number of children\nchildren = []\nfor i in range(0,6):\n    children.append(input.loc[input.children==i,'charges'])","a028130b":"kruskal(children[0], children[1], children[2], children[3], children[4], children[5])\nkruskal(children[0], children[4])\nkruskal(children[2], children[5])\nkruskal(children[1], children[5])","e1b8ed63":"X = input.iloc[:,0:6]\ny = input.iloc[:,6]","80431be7":"X.head()","bcc2da1a":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder","1681e76a":"LabelEncoder_X1 = LabelEncoder()\nLabelEncoder_X4 = LabelEncoder()\nLabelEncoder_X5 = LabelEncoder()\nX.iloc[:,1] = LabelEncoder_X1.fit_transform(X.iloc[:,1])\nX.iloc[:,4] = LabelEncoder_X4.fit_transform(X.iloc[:,4])\nX.iloc[:,5] = LabelEncoder_X5.fit_transform(X.iloc[:,5])","78bc7de0":"from sklearn.compose import ColumnTransformer\n\nct = ColumnTransformer([('one_hot_encoder', OneHotEncoder(categories='auto'), [1,4,5])], remainder='passthrough')\nX = ct.fit_transform(X)","0d2712b1":"X = X[:,[1,2,4,5,6,8,9,10]]","d8733243":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","eaf1129a":"from sklearn.metrics import mean_absolute_error","5196dea3":"from sklearn.linear_model import LinearRegression\n\nregressor1 = LinearRegression()\nregressor1.fit(X_train, y_train)\n\ny_pred1 = regressor1.predict(X_test)","faabb9c8":"MAE1 = mean_absolute_error(y_test, y_pred1)\nMAE1","da8f85d4":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression","71dba2fc":"regressor2 = PolynomialFeatures(degree=3)\nX_poly = regressor2.fit_transform(X_train)\nregressor2.fit(X_poly, y_train)\n\nlinreg = LinearRegression()\nlinreg.fit(X_poly,y_train)\n\ny_pred2 = linreg.predict(regressor2.fit_transform(X_test))","683a752e":"MAE2 = mean_absolute_error(y_test, y_pred2)\nMAE2","9d86ff73":"from sklearn.tree import DecisionTreeRegressor\n\nregressor3 = DecisionTreeRegressor(random_state = 0)\nregressor3.fit(X_train, y_train)\n\ny_pred3 = regressor3.predict(X_test)\n\nMAE3 = mean_absolute_error(y_test, y_pred3)\nMAE3","9b199eb9":"from sklearn.ensemble import RandomForestRegressor","94c4daeb":"from sklearn.model_selection import GridSearchCV","b479c7f3":"parameters = {'n_estimators': [2, 3, 5, 10, 15, 20, 30, 50, 75, 100, 500, 1000],\n              'max_leaf_nodes': [5, 10, 20, 35, 50, 100],\n              'random_state': [0]}\ngrid_search = GridSearchCV(estimator = RandomForestRegressor(),\n                           param_grid = parameters,\n                           scoring = 'neg_mean_absolute_error',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nprint(f\"Best MAE: {grid_search.best_score_ * (-1)}\")\nprint(f\"Best parameters: {grid_search.best_params_}\")","49f6ab5b":"regressor4 = RandomForestRegressor(n_estimators=100, max_leaf_nodes=35, random_state=0)\nregressor4.fit(X_train, y_train)\n\ny_pred4 = regressor4.predict(X_test)\n\nMAE4 = mean_absolute_error(y_test, y_pred4)\nMAE4","9ebce4e6":"from sklearn.svm import SVR\n\nregressor5 = SVR(kernel = 'rbf')\nregressor5.fit(X_train, y_train)\n\ny_pred5 = regressor5.predict(X_test)\n\nMAE5 = mean_absolute_error(y_test, y_pred5)\nMAE5","aebc81fd":"from sklearn.preprocessing import StandardScaler\n\nsc_X = StandardScaler()\nsc_y = StandardScaler()\nsc_X_train = sc_X.fit_transform(X_train)\nsc_y_train = sc_y.fit_transform(y_train.values.reshape(-1,1))\nsc_X_test = sc_X.fit_transform(X_test)","e4b0a1ed":"parameters = {'C': [1, 5, 10, 20, 50, 100],\n              'kernel': ['rbf', 'linear', 'poly'],\n              'degree': [2, 3, 4]}\ngrid_search = GridSearchCV(estimator = SVR(),\n                           param_grid = parameters,\n                           scoring = 'neg_mean_absolute_error',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(sc_X_train, sc_y_train)\nprint(f\"Best MAE: {grid_search.best_score_ * (-1)}\")\nprint(f\"Best parameters: {grid_search.best_params_}\")","780a9ee0":"regressor5 = SVR(kernel = 'rbf', C = 1)\nregressor5.fit(sc_X_train, sc_y_train)\n\ny_pred5 = regressor5.predict(sc_X_test)\ny_pred5 = sc_y.inverse_transform(y_pred5)\n\nMAE5 = mean_absolute_error(y_test, y_pred5)\nMAE5","feb8932d":"from xgboost import XGBRegressor","15ee70dd":"parameters = {'base_score': [0.1, 0.3, 0.5, 0.7, 1, 1.5, 2, 5, 10, 20],\n              'learning_rate': [0.001, 0.005, 0.01, 0.03, 0.05, 0.07, 0.1, 0.3, 0.5],\n              #'booster': ['gbtree', 'linear', 'dart'],\n              'n_estimators': [50, 100, 150, 200, 250, 300, 500, 750, 1000]}\n              #'max_depth': [3, 5]}\ngrid_search = GridSearchCV(estimator = XGBRegressor(),\n                           param_grid = parameters,\n                           scoring = 'neg_mean_absolute_error',\n                           cv = 2,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nprint(f\"Best MAE: {grid_search.best_score_ * (-1)}\")\nprint(f\"Best parameters: {grid_search.best_params_}\")","376149a7":"regressor6 = XGBRegressor(learning_rate=0.01, n_estimators=300)\nregressor6.fit(X_train, y_train)\ny_pred6 = regressor6.predict(X_test)\nMAE6 = mean_absolute_error(y_test, y_pred6)\nMAE6","c0d8f3ec":"summary = {'Multiple Linear': MAE1, 'Polynomial': MAE2, 'Decision Tree': MAE3,\n           'Random Forest': MAE4, 'SVR': MAE5, 'XGB': MAE6}\n\nfrom sklearn.metrics import r2_score\n\nsummary_R2 = {'Multiple Linear': r2_score(y_test,y_pred1), 'Polynomial': r2_score(y_test,y_pred2),\n             'Decision Tree': r2_score(y_test,y_pred3), 'Random Forest': r2_score(y_test,y_pred4),\n             'SVR ': r2_score(y_test,y_pred5), 'XGBoost': r2_score(y_test,y_pred6)}","7e1741d5":"f = plt.figure(figsize=(15,5))\n\nax = f.add_subplot(121)\nplt.bar(summary.keys(), summary.values(), color='green');\nplt.title(\"Mean absolute error by model (the lower the better)\")\n\nax=f.add_subplot(122)\nplt.plot(summary_R2.keys(), summary_R2.values(), color='cyan');\nplt.title(\"R-Squared coefficient by model (the higher the better)\")\naxes = plt.gca()\naxes.set_ylim([0.5,1])\nplt.show();","79e3e137":"# compare MAE to the average value of the dependent variable\nround(100*MAE6\/np.mean(y_test),2)\nround(100*MAE6\/input.charges.mean(),2)","ec6ece3b":"from sklearn.cluster import KMeans\ninertia = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 0)\n    kmeans.fit(X)\n    inertia.append(kmeans.inertia_)\nplt.plot(range(1, 11), inertia);\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show();","431fe9e4":"kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 0)\ny_kmeans = kmeans.fit_predict(X)","42723cda":"input.head()","2e6b3268":"XClass = input.iloc[:,[0,1,2,3,4,6]]\nyClass = input.iloc[:,5]","e49c6863":"LabelEncoder_XClass1 = LabelEncoder()\nLabelEncoder_XClass4 = LabelEncoder()\nLabelEncoder_yClass = LabelEncoder()\nXClass.iloc[:,1] = LabelEncoder_XClass1.fit_transform(XClass.iloc[:,1])\nXClass.iloc[:,4] = LabelEncoder_XClass4.fit_transform(XClass.iloc[:,4])\nyClass = LabelEncoder_yClass.fit_transform(yClass)","0b7a2b34":"ct_XClass = ColumnTransformer([('one_hot_encoder', OneHotEncoder(categories='auto'), [1,4])], remainder='passthrough')\nXClass = ct_XClass.fit_transform(XClass)","ca54603a":"XClass = XClass[:,[0,1,3,5,6,7]]","1fb07693":"XClass_train, XClass_test, yClass_train, yClass_test = train_test_split(XClass, yClass, test_size=0.2, random_state=1)","0b405c3a":"sc_XClass = StandardScaler()\nXClass_train = sc_XClass.fit_transform(XClass_train)\nXClass_test = sc_XClass.transform(XClass_test)","ad1fdd97":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import cohen_kappa_score","56dea4e6":"from sklearn.tree import DecisionTreeClassifier\n\nclassifier1 = DecisionTreeClassifier(criterion = 'entropy', random_state = 1)\nclassifier1.fit(XClass_train, yClass_train)\nyClass_pred1 = classifier1.predict(XClass_test)","66a8bcfe":"kappa1 = cohen_kappa_score(yClass_test, yClass_pred1)\nkappa1\nacc1 = accuracy_score(yClass_test, yClass_pred1)\nacc1","38600cdc":"from sklearn.ensemble import RandomForestClassifier","8e676e76":"parameters = {'n_estimators': [50, 100, 150, 200, 250, 300, 500, 750, 1000], \n              'max_leaf_nodes': [5, 10, 20, 30, 50, 100, 300, 600, 800, 1000],\n              'criterion': ['gini', 'entropy'],\n              'max_depth': [3, 4, 6, 8, 9],\n              'random_state': [0]}\ngrid_search = GridSearchCV(estimator = RandomForestClassifier(),\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 2,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(XClass_train, yClass_train)\nprint(f\"Best Accuracy: {grid_search.best_score_}\")\nprint(f\"Best parameters: {grid_search.best_params_}\")","599df995":"classifier2 = RandomForestClassifier(n_estimators = 100)\nclassifier2.fit(XClass_train, yClass_train)\nyClass_pred2 = classifier2.predict(XClass_test)\nkappa2 = cohen_kappa_score(yClass_test, yClass_pred2)\nkappa2\nacc2 = accuracy_score(yClass_test, yClass_pred2)\nacc2","4cd8a269":"from sklearn.svm import SVC","d3ab4ad5":"parameters = {'kernel': ['rbf'], \n              'C': [1, 3, 5, 9, 10, 20, 25, 30, 40, 50, 75, 100, 200, 500, 1000],\n              'random_state': [0]}\ngrid_search = GridSearchCV(estimator = SVC(),\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(XClass_train, yClass_train)\nprint(f\"Best Accuracy: {grid_search.best_score_}\")\nprint(f\"Best parameters: {grid_search.best_params_}\")","4b78d2d4":"classifier3 = SVC(kernel = 'poly', C = 1, random_state = 0)\nclassifier3.fit(XClass_train, yClass_train)\nyClass_pred3 = classifier3.predict(XClass_test)\nkappa3 = cohen_kappa_score(yClass_test, yClass_pred3)\nkappa3\nacc3 = accuracy_score(yClass_test, yClass_pred3)\nacc3","1378489b":"from sklearn.linear_model import LogisticRegression\nclassifier4 = LogisticRegression(random_state = 0)\nclassifier4.fit(XClass_train, yClass_train)\n\n# Predicting the Test set results\nyClass_pred4 = classifier4.predict(XClass_test)\nkappa4 = cohen_kappa_score(yClass_test, yClass_pred4)\nkappa4\nacc4 = accuracy_score(yClass_test, yClass_pred4)\nacc4","347438b8":"from sklearn.naive_bayes import GaussianNB\n\nclassifier5 = GaussianNB()\nclassifier5.fit(XClass_train, yClass_train)\nyClass_pred5 = classifier5.predict(XClass_test)\nkappa5 = cohen_kappa_score(yClass_test, yClass_pred5)\nkappa5\nacc5 = accuracy_score(yClass_test, yClass_pred5)\nacc5","54f35c40":"from sklearn.neighbors import KNeighborsClassifier","532eb711":"parameters = {'n_neighbors': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \n              'p': [1, 2, 3, 5, 10, 20, 30, 50, 70, 90, 120, 150, 200]}\ngrid_search = GridSearchCV(estimator = KNeighborsClassifier(),\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(XClass_train, yClass_train)\nprint(f\"Best Accuracy: {grid_search.best_score_}\")\nprint(f\"Best parameters: {grid_search.best_params_}\")","69ca3957":"classifier6 = KNeighborsClassifier(n_neighbors = 4, metric = 'minkowski', p = 120)\nclassifier6.fit(XClass_train, yClass_train)\nyClass_pred6 = classifier6.predict(XClass_test)\nkappa6 = cohen_kappa_score(yClass_test, yClass_pred6)\nkappa6\nacc6 = accuracy_score(yClass_test, yClass_pred6)\nacc6","2c2d96d3":"from xgboost import XGBClassifier","c9cb5e07":"classifier7 = XGBClassifier(base_score=0.1, n_estimators=2600, max_depth=2)\nclassifier7.fit(XClass_train, yClass_train)\nyClass_pred7 = classifier7.predict(XClass_test)\nkappa7 = cohen_kappa_score(yClass_test, yClass_pred7)\nkappa7\nacc7 = accuracy_score(yClass_test, yClass_pred7)\nacc7","129c6067":"summaryClass = {'Decision Tree': kappa1, 'Random Forest': kappa2, 'Kernel SVM': kappa3,\n               'Logistic Regression': kappa4, 'Naive Bayes': kappa5, 'KNN': kappa6, 'XGBoost': kappa7}\n\nclassmodels = []\nfor key in summaryClass.keys():\n    classmodels.append(key)","a415c784":"accuracies = [acc1, acc2, acc3, acc4, acc5, acc6, acc7]","713d4d5a":"plt.figure(figsize=(10,5))\nsns.barplot(x=classmodels, y=accuracies);\nplt.title('Model Accuracy (the higher the better)')\nplt.show();","7de898f1":"The distribution looks normal for 'BMI', but non-normal for 'age', 'children' and 'charges'.\n\nThis means that **for 'age' and 'charges' we should look at the median instead of the mean values**. Their mean values would be highly affected by the number of outliers and will significantly vary from their median. This is well illustrated by the example below.","69d60e75":"##### Split into test and training set","516d0e2d":"# Machine Learning Models","bafab6f4":"The optimal number of clusters is 3.","9de7bcd4":"# Data analysis\n\n### Let's peek into the data","675872f4":"The average charges increase with the increase of the number of children, but decrease for 5 children. However, for people having no children the costs are high - they are comparable to the costs for people having 4 children.\n\nWe'll now run a Kruskal-Wallis test to compare the median charges.","cce05450":"We'll try below to analyze the charges by gender. We'll also have a look at the gender characteristics - to make sure that the charges have not been affected by the gender alone!\n\nWe'll first notice that there is some difference in charges, which is very slightly higher for females than for males (just to remind - we are working with median):","4e366e2d":"This is my first data science project. Some of the approaches might not be the best ones, but I am still learning and always open to any feedback.\n\nWe will first start with analysis of the features in the dataset and the relationship between them, to get better idea of which features should and which shouldn't be taken into account to determine the dependent variable. We will also run statistical tests to compare different sets of data.\nThen, we will split the data into training set and test set, will apply machine learning models on the training data and will try to:\n* predict the insurance charges on the test data;\n* identify clusters;\n* predict the region of the customer (this non-binary variable was chosen just for training purposes)","ffbe4441":"## K-Nearest Neighbors","aa673ee9":"Although we can see some differences, it seems that they are not statistically significant, because the P-value is greater than 0.05.","0486859c":"Both the boxplot and the mean\/median table show that when we exclude the special outlying cases, male customers have been charged significantly more than females.\nHowever, does this have anything to do with smoking and BMI? Shouldn't we have a look at these two features and how they are distributed gender-wise?","9285d552":"##### Escape the dummy variable trap","a21bcb1c":"## Decision Tree Regression","8bf2f53e":"# Clustering","4a4c423d":"### Decision Tree","6f3032e0":"It's not a surprise that smokers pay more than non-smokers. \n##### However, the charges go up for smokers with the increase in their BMI. Interestingly, for non-smokers such tendency is not observed.","bfe69162":"The P-Value is greater than 0.05, meaning that there is no significant difference between males' and females' charges. Hence, there is no gender inequality.","fb4f63af":"##### The proportion of smokers by region is highest (25%) for Southeast. Smokers are significantly less proportionally in the other regions, where they are between 17.9% and 20.7%. \nThis is a good explanation why the charges in Southeast are high. However, this doesn't explain anything about the Northeast. As we said, charges are non-normally distributed, so we should be more interested in median than in mean.\n\nWe can additionally explore the relation between region and BMI:","6ffc9c19":"##### Encode categorical variables","be1293b7":"We have 7 columns and 1338 observations. Three of the columns are categorical - sex, smoker and region, + children column which takes a value between 0 and 5.\n\nOne of our first tasks would be to observe the distribution of each numeric variable.","9f1b943b":"## Naive Bayes","550d57f2":"#### Most of the classification model measurement tools are designed for binary classifications. One of the options for non-binary data is the Cohen Kappa score.","8fd6848c":"## XGBoost","0cf32b4e":"##### Here's the explanation: Female smokers have been charged significantly less than male smokers. This might be due to the fact that female smokers have significantly lower BMI than male smokers.\n\nThe steeper line for males on the second chart shows how much quicker their charges go up with the increase of BMI, compared to females.\n\nThe last part here will be to run a Mann-Whitney test to compare the median charges for males and females.","d42e518c":"The Kruskal-Wallis test for charges by number of children confirms that the children do matter - there is significant difference in the median charges. It also confirms that the charges for 'no children' are similar to those for 4 children; charges for 2 children are similar to 5 children.\n\nHowever, from the boxplots above we can see that there are so many outliers for 0-3 children, therefore any conclusions here might be inappropriate.","927589d3":"#### Summarize mean absolute error and R-squared","c3351093":"### Polynomial Regression","c58fe95e":"## Summary","5f3fb3dc":"#### The best perfomed model is:\n## XGBoost Classification\n##### It received the highest accuracy, much higher than the other models. However, it is still too low - only 44%. Therefore, none of the models is good enough to make reliable predictions for the region of the customers. This section was created just for training purposes as this my first data science project.","cd7f5d92":"## SVM (Kernel)","0ed6acc1":"## Gender (in)equality?","1a98ecf5":"## Train Classification models","15abd475":"### XGBoost","0e68dd13":"### Multiple Linear Regression","950973dd":"## How the charges relate to the other categorical variables?","13b0708a":"From the charts above it seems that in Southeast region the charges are a bit higher than in the other regions... But wait, this is only if we talk about the mean. If we have a look at the median values, the picture is different - the charges are higher in the Northeast. Two conclusions:\n* The charges are 'normally' highest in the Northeast region\n* If there are specific, outlying cases, the charges in the Southeast tend to go up much higher than in the other regions\n\nThis could be a result of specific demographic characteristics in these regions. For example, let's observe the smoker status by region.","9e29d2a2":"Let's run the Kruskal-Wallis test to compare the median charges by region.","2a05b8c7":"#### This MAE is too high to be true. We have forgotten that SVR model requires feature scaling before fitting!","a839e1af":"No other findings here. We can humbly leave this with the conclusion that:\n##### \u0422he charges are just naturally high in Northeast, although for some specific cases (e.g. smoking, high BMI, many children) the charges tend to go higher in Southeast. However, according to Kruskal-Wallis test, these differences are not statistically significant.","f72b7dcd":"## Data Preprocessing","149a31c3":"*Note: 'Children' is a numeric variable in the source data, but we will treat it as a categorical one, due to it being discrete rather than continuous (it is countable, accepting only integer values between 0 and 5).*","3dcdecc8":"## Train regression models","bf5d997c":"##### Encode categorical variables","8336e943":"## Does the number of children matter?","35c23165":"## Logistic Regression","b0aa54d4":"##### Measure the result","7bb0021c":"## Distributions","9f5ed985":"## Random Forest","d8a39b6b":"#### Conclusion: The best regression model that we've created for this dataset is:\n### XGBoost\n##### It returned mean absolute error of 2219, which is a deviation of about 16% of the average dependent variable value. It also has a high R-squared value of 0.90.\n##### Support Vector Machine also did a very good job, but was just narrowly outperformed. Random Forest Regression also did a relatively good job and completes the top 3.","a227ec9f":"# Classification","281bd8c8":"##### Measure the result","6d7aca16":"We will need to have a look at the data and split into test and training set again, but this time the the dependent variable will be a categorical one - we will try to predict the region of the customer.","afdfcb6e":"#### Apply Grid Search","d31cfb2c":"Now, that's an interesting finding:\n* On the one hand, female non-smokers have been charged about 9% more (on median) than male non-smokers\n* On the other hand, female smokers have been charged about 20% less (on median) than male smokers\n\nHow is that possible? Well, let's see how the BMI will fit into the picture.","39d07cd0":"## Random Forest Regression","eee4a680":"# Intro","d33f7483":"## Regression Models Summary","fd6f0056":"Although the average charges are very similar for both genders, it's mainly affected by outliers. It seems that there are more outliers for females, which have dragged their average charges up. We will try below to re-create the boxplot, but by excluding the outliers (females' charges >30k and males' charges > 40k):","f2287f7f":"The boxplot for BMI gives one more finding:\n##### the Southeast region has the highest average BMI for both smokers and non-smokers amongst all regions! It still doesn't say anything about the Northeast, though!\n\nThis gives us no other chance but to see how this competition would look like if we exclude the outliers from the picture.","3c1714f5":"## Support Vector Regression","5781476f":"## Region-wise analysis"}}