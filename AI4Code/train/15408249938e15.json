{"cell_type":{"95ab9ead":"code","0984be6c":"code","e7f76ef4":"code","7da409b8":"markdown","e98c701e":"markdown","bc9bbe7a":"markdown","ca2b3672":"markdown","307e2502":"markdown","1ce85779":"markdown","954c0feb":"markdown","aa6e4f39":"markdown","c4f77d8c":"markdown","356a243e":"markdown","bcff0981":"markdown","cc4bdf8a":"markdown","c0e5fed1":"markdown","fd63cf25":"markdown","84f51060":"markdown","8a623fc0":"markdown","30b0907d":"markdown","e2846cf1":"markdown","d297ce36":"markdown","ce195852":"markdown","73284f3b":"markdown","4670bd33":"markdown","be9512c8":"markdown","d0bbff98":"markdown","9c68c4ae":"markdown","250ce816":"markdown","d846002c":"markdown","6361c65b":"markdown","b2bb4d64":"markdown","532b1274":"markdown","af9c5820":"markdown","109eb7b9":"markdown"},"source":{"95ab9ead":"# !pip install 'kaggle-environments>=0.1.6'","0984be6c":"# import numpy as np\n# import pandas as pd\n# from kaggle_environments import evaluate, make, utils\n# from copy import deepcopy\n\n# env = make(\"connectx\", debug=True)\n# env.render()","e7f76ef4":"# trainer = env.train([None, \"random\"])\n# observation = trainer.reset()\n\n# while not env.done:\n#     my_action = agent\n#     observation, reward, done, info = trainer.step(my_action)\n#     if reward==0:\n#     env.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\n# env.render()","7da409b8":"Optimal Value Function:\n\n![image.png](attachment:image.png)","e98c701e":"![image.png](attachment:image.png)","bc9bbe7a":"Policy:\n\n![image.png](attachment:image.png)","ca2b3672":"## Conclusion","307e2502":"State-Value Function:\n\n![image.png](attachment:image.png)","1ce85779":"**Observation** is the data about environment at the exact state.\n\nFor example, if the environment is the Chess game, then an observation is the board (8\u04458 array) at the exact state, and the state is the game at the exact point of the time.","954c0feb":"A Markov decision process (MDP) is a Markov reward process with\ndecisions. It is an environment in which all states are Markov:\n\n![image.png](attachment:image.png)","aa6e4f39":"In this notebook we defined basic and the most important RL-definitions: action, state, reward, observation, state-transition matrix, value function, policy, model. We also considered Markov reward processes (MRPs) and Markov decision processes (MDPs), Bellman Equation and Optimal Value Function.","c4f77d8c":"![image.png](attachment:image.png)","356a243e":"## Practice (to be finished..)\nLet's model out environment.","bcff0981":"**Reward** is:\n![image.png](attachment:image.png)","cc4bdf8a":"History is the sequence of **observations, actions, rewards**:\n\n![image.png](attachment:image.png)","c0e5fed1":"A Markov reward process is a Markov chain with values (at each state):\n\n![image.png](attachment:image.png)","fd63cf25":"### Markov Decision Process","84f51060":"### Markov Process (Markov Chain)","8a623fc0":"## Definitions","30b0907d":"Value Function:\n\n![image.png](attachment:image.png)","e2846cf1":"where q_star(s,a) is:\n\n![image.png](attachment:image.png)\n\nIf we know q_star(s,a), we immediately have the optimal policy.","d297ce36":"### Markov Reward Process","ce195852":"Bellman Equation:\n\n![image.png](attachment:image.png)","73284f3b":"When the state is Markov:\n\n![image.png](attachment:image.png)","4670bd33":"Return:\n\n![image.png](attachment:image.png)","be9512c8":"State Transition Matrix:\n\n![image.png](attachment:image.png)","d0bbff98":"**Action** is the change in the environment (transition from the one state to other), that was provoked by an agent.","9c68c4ae":"Model:\n\n![image.png](attachment:image.png)","250ce816":"![image.png](attachment:image.png)","d846002c":"### The Main Point\n#### \"To train the reinforcement learnig agent\" in the context of MDP means to achieve the maximum (optimal) value function with the optimal policy. And in order to do that we need to solve this:\n\n![image.png](attachment:image.png)","6361c65b":"Markov Process:\n\n![image.png](attachment:image.png)","b2bb4d64":"In Practice block we implemented a simple Connect4 environment with rewards and actions and have tested it with \"manual input\" and random agent.","532b1274":"**State** is a function of the history:\n\n![image.png](attachment:image.png)","af9c5820":"# Markov Reward and Decision Processes\n### Notebook Series about Reinforcement Learning\n##### Created with the help of [RL Course by David Silverman](http:\/\/www0.cs.ucl.ac.uk\/staff\/d.silver\/web\/Teaching.html)","109eb7b9":"Action-Value Function:\n\n![image.png](attachment:image.png)"}}