{"cell_type":{"e51c850a":"code","52881cc8":"code","ea09651b":"code","d47ffa24":"code","6eebc58b":"code","7c5beee0":"code","0e34f785":"code","17bfe383":"code","17f04012":"code","a2b01e9d":"code","012a1bd8":"code","9320a44a":"code","7a724bb6":"code","0eed95d0":"code","170e6ee1":"code","eb24400a":"code","36254795":"code","6a8afc35":"code","48284b2e":"code","de7a6164":"code","2faaa302":"code","c1145cdd":"code","54a593c7":"code","c206905e":"code","37bcd9a8":"code","243394bb":"code","9fc66f2c":"code","3b0e554d":"code","6a374bef":"code","8f5bfc7a":"code","1ffd2571":"code","7c9f245f":"code","0b8424df":"code","23414638":"code","b5b2f030":"code","697ac1ed":"code","bead96aa":"code","5bd142d1":"code","b4a28c48":"code","edf6abff":"code","a623adad":"code","c31dfebc":"code","0e39b4c1":"code","c9be1ae9":"code","149152a1":"code","c918cfa6":"code","0e994a48":"code","7810e74e":"code","3f165410":"markdown","0de6f3d1":"markdown","2313c6a2":"markdown","48c534e3":"markdown","000b5364":"markdown","6ee534e7":"markdown","bc2885bb":"markdown","a7289741":"markdown","8a930658":"markdown","a6cf97d7":"markdown","d4d3f983":"markdown","79ea29a0":"markdown","9b602396":"markdown","b5cb1cff":"markdown","859a0ffa":"markdown","97fc7456":"markdown","632e58e0":"markdown","74b86596":"markdown","7880bd84":"markdown","83766b23":"markdown","30c094ef":"markdown","4602090e":"markdown","b0753edd":"markdown","05562d80":"markdown","bb4fe114":"markdown","7ce5e0e0":"markdown","174a8ac4":"markdown","ddb5d3e4":"markdown","cf89022a":"markdown","f641ba30":"markdown","29a7bdac":"markdown","e26ab53b":"markdown","21abd70d":"markdown","88c8255e":"markdown"},"source":{"e51c850a":"import warnings\nwarnings.filterwarnings(\"ignore\") #Para n\u00e3o poluir o notebook com avisos \n\nimport pandas as pd #Manipula\u00e7\u00e3o de Dados\nimport numpy as np #\u00c1lgebra Linear\nimport matplotlib.pyplot as plt #Visualiza\u00e7\u00e3o de Dados\nimport seaborn as sns #Visualiza\u00e7\u00e3o de Dados\nimport pandas_profiling #relat\u00f3rio pronto para entender os dados\n\n# machine learning\n\nfrom sklearn.linear_model import RidgeCV\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n","52881cc8":"test = pd.read_csv(\"..\/input\/titanic\/test (1).csv\")\ntrain = pd.read_csv(\"..\/input\/titanic\/train (1).csv\")\ncombine = [train, test]\n","ea09651b":"print(train.shape)\ntest.shape","d47ffa24":"train.dtypes","6eebc58b":"test.dtypes","7c5beee0":"profile_report = pandas_profiling.ProfileReport(train)\nprofile_report","0e34f785":"train.describe(include=[np.number]).T","17bfe383":"train.describe(include='O').T\n","17f04012":"pd.options.display.max_rows = 100 ##Caso a visualiza\u00e7\u00e3o abaixo fique truncada, \u00e9 s\u00f3 rodar essa linha de c\u00f3digo\n","a2b01e9d":"def missingValuesInfo(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = round(df.isnull().sum().sort_values(ascending=False)\/len(df)*100, 2)\n    temp = pd.concat([total, percent], axis = 1, keys = ['Total', 'Percent'])\n    return temp.loc[temp['Total'] > 0]","012a1bd8":"missingValuesInfo(train)","9320a44a":"#Cabin\ntrain[['Cabin', 'Survived']].groupby(['Cabin'], as_index=False).mean().sort_values(by='Survived', ascending=False)","7a724bb6":"plt.figure(figsize=(3,3))\n\ntrain.Cabin.value_counts().plot(kind='bar')\nplt.ylabel('Counts')\nplt.xlabel('Cabin')\n#plt.title('Gender Distribution')\nplt.show()","0eed95d0":"#PCLASS\ntrain[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","170e6ee1":"plt.figure(figsize=(3,3))\n\ntrain.Pclass.value_counts().plot(kind='bar')\nplt.ylabel('Counts')\nplt.xlabel('Pclass')\n#plt.title('Gender Distribution')\nplt.show()","eb24400a":"train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","36254795":"train[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","6a8afc35":"train[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","48284b2e":"#Embarked\ntrain[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","de7a6164":"Target = 'Survived'\ntrain.dropna(axis=0, subset =[Target], inplace=True)","2faaa302":"def dropValues(df):\n  return df.drop(['Name', 'Cabin', 'Name','Ticket'],axis = 1)","c1145cdd":"train = dropValues(train)","54a593c7":"train.shape","c206905e":"def HandleMissingValues(df):\n    # for Object columns fill using 'UNKOWN'\n    # for Numeric columns fill using median\n    num_cols = [cname for cname in df.columns if df[cname].dtype in ['int64', 'float64']]\n    cat_cols = [cname for cname in df.columns if df[cname].dtype == \"object\"]\n    values = {}\n    for a in cat_cols:\n        values[a] = 'S' #Somente 2 valores nulos e vou colocar o que \u00e9 mais frequente. \n    for a in num_cols:\n        values[a] = df[a].median()\n\n    df.fillna(value=values,inplace=True)   ","37bcd9a8":"missingValuesInfo(train)","243394bb":"HandleMissingValues(train)","9fc66f2c":"missingValuesInfo(train)","3b0e554d":"train.head(3)","6a374bef":"train.dtypes.T","8f5bfc7a":"train.describe(include='O')","1ffd2571":"def getObjectColumnsList(df):\n    return [cname for cname in df.columns if df[cname].dtype == \"object\"]\n\ndef PerformOneHotEncoding(df, columnsToEncode):\n    return pd.get_dummies(df, columns = columnsToEncode)","7c9f245f":"cat_cols = getObjectColumnsList(train)","0b8424df":"print(cat_cols)","23414638":"train = PerformOneHotEncoding(train, cat_cols)","b5b2f030":"train.dtypes.T","697ac1ed":"train.head()","bead96aa":"train['Age'] = round(train['Age']).values.astype(np.int64)","5bd142d1":"train.dtypes","b4a28c48":"train.head()","edf6abff":"HandleMissingValues(test)\n\ntest = dropValues(test)\n\ncat_colsTest = getObjectColumnsList(test)\n\ntest = PerformOneHotEncoding(test, cat_colsTest)\n\ntest['Age'] = round(test['Age']).values.astype(np.int64)\n#test['Fare'] = round(test['Fare']).values.astype(np.int64)\n#test['Sex_female'] = round(test['Sex_female']).values.astype(np.int64)\n#test['Sex_male'] = round(test['Sex_male']).values.astype(np.int64)","a623adad":"test.head()","c31dfebc":"train.head()","0e39b4c1":"print(train.shape)\ntest.shape","c9be1ae9":"train_df = train.drop(['PassengerId'], axis=1)\ntest_df = test\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","149152a1":"X_train = train_df.drop(\"Survived\", axis=1) #features\nY_train = train_df[\"Survived\"] # label\n\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy() #test\n\nX_train.shape, Y_train.shape, X_test.shape","c918cfa6":"from sklearn.linear_model import RidgeClassifier\nclf = RidgeClassifier().fit(X_train, Y_train)\nclf.score(X_train, Y_train)\n","0e994a48":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": clf\n    })\n","7810e74e":"submission.to_csv('submission.csv', index=False)","3f165410":"Fizemos todas as altera\u00e7\u00f5es no dataset de train, agora vamos fazer todas as altera\u00e7\u00f5es no dataset de test para deixar tudo igual.","0de6f3d1":"Sibs tem muitos zeros, mas eles representam viajantes solit\u00e1rios, o que \u00e9 interessante, uma vez que 0, 1 e 2 tem boa rela\u00e7\u00e3o com a label Survived. Vou manter","2313c6a2":"# **Olhando novamente as colunas**","48c534e3":"Nessa fase, temos os conjuntos de dados prontos, separados e preparados para rodar nos modelos de Machine Learning. Agora, vamos rodar em v\u00e1rios modelos e ver qual vai gerar uma resposta melhor.","000b5364":"Vou manter porque vejo rela\u00e7\u00e3o de 60% Parch 3. N\u00e3o entendo bem essa feature, mas vou manter.","6ee534e7":"# **Aplicando modelo de Machine Learning**","bc2885bb":"Depois da an\u00e1lise, sabendo as features que queremos no nosso modelo, vamos deixar tudo direito. \n\n*   Remover features que n\u00e3o iremos usar;\n*   Criar features necess\u00e1rias;\n*   Preencher nulos;\n*   Lidar com Strings, Floats, Formatos moeda, data, etc;\n","a7289741":"# **Remover as colunas que identificamos que n\u00e3o ir\u00e3o participar do modelo**","8a930658":"# **Transformar String em Int**\n\n\n","a6cf97d7":"# **PREDI\u00c7\u00c3O DOS DADOS**","d4d3f983":"No primeiro notebook vimos alguns conceitos de manipula\u00e7\u00e3o de dados, como remover linhas, substituir nulos, plotar gr\u00e1ficos, entre outros. Todos esses conceitos s\u00e3o essenciais para termos um conjunto de dados limpo e pronto para colocar dentro de um modelo de machine learning com o objetivo de predizer algo. \n\nPor\u00e9m, s\u00f3 com os conceitos aprendidos de limpeza, n\u00e3o conseguimos atingir o objetivo da predi\u00e7\u00e3o de uma vari\u00e1vel. Dessa forma, esse notebook aborda o in\u00edcio, que \u00e9 a an\u00e1lise dos dados, o meio, que \u00e9 a modelagem dos dados(limpeza, transforma\u00e7\u00e3o e cria\u00e7\u00e3o de features) e o fim que \u00e9 a predi\u00e7\u00e3o dos dados.\n\nA ideia desse notebook \u00e9 vir como um tutorial que possa dar clareza sobre os processos. \n\nDepois desse notebook ter\u00e1 mais um tutorial da serie Titanic, por\u00e9m, com an\u00e1lises e racioc\u00ednios mais complexos. Lembrando que o objetivo \u00e9 sempre ter um modelo de predi\u00e7\u00e3o com alt\u00edssima taxa de acerto.","79ea29a0":"# **Vamos fazer no dataset TEST agora**","9b602396":"Modelo rodou, gostei do resultado, vou modelar o output: 'PassengerId' e 'Survived' e depois vou exportar para csv e assim submeter no kaggle;","b5cb1cff":"# **Vamos focar no dataset de Treino e depois replicamos as mudan\u00e7as no Teste**","859a0ffa":"# **MODELAGEM DE DADOS**","97fc7456":"**Vamos ver as colunas que possuem nulo**","632e58e0":"# **AN\u00c1LISE DE DADOS**","74b86596":"## **Lendo Arquivos**","7880bd84":"**Elimino as linhas em que forem nulas na feature principal de predi\u00e7\u00e3o do treino,\nporque isso pode impactar negativamente no modelo de ia! (nem preciso ver se tem nulo ou n\u00e3o)**","83766b23":"Tenho aqui uma vis\u00e3o de que quem ficou na primeira classe teve uma sobreviv\u00eancia mais alta, o que j\u00e1 me diz algo com a label Survived. Manter","30c094ef":"# **Vamos ver os datasets em mais detalhe**\n","4602090e":"**Validando que lidamos com os dados nulos**","b0753edd":"Olhando Cabin, n\u00e3o vejo nada que posso associar com a label Survived. Al\u00e9m de ter muitos nulos e os valores restantes s\u00e3o majoritariamente \u00fanicos. Remover","05562d80":"# **Transformar Float em INT**","bb4fe114":"# **Lidando com Missing Data - Nulos!**\n","7ce5e0e0":"# **Olhada Final antes de ir para Machine Learning**","174a8ac4":"## **Importando Libraries Necess\u00e1rias para o desenvolvimento da an\u00e1lise**","ddb5d3e4":"# **Vamos modelar os datasets para aplicar Machine Learning**","cf89022a":"Vamos aqui entender os dados que temos no nosso conjunto e assim escolher quais as features(atributos ou colunas) iremos manter. Essa escolha de features \u00e9 essencial para o modelo, uma vez que se eu escolho features que n\u00e3o tem nenhuma liga\u00e7\u00e3o com a minha label (vari\u00e1vel que irei prever), ent\u00e3o n\u00e3o importa o modelo que eu use, sempre terei uma taxa p\u00edfia de acerto.","f641ba30":"S\u00e3o v\u00e1rios modelos que voc\u00ea pode escolher e ai fica como li\u00e7\u00e3o de casa qual seria o melhor modelo e como seria adaptar outros modelos aqui ;p\nVou deixar o Ridge como exemplo, porque rodou bem mais r\u00e1pido que os outros modelos. Importante saber que outros modelos chegam a fornecer acur\u00e1cia de 99% sem stress.","29a7bdac":"Sexo tem um padr\u00e3o bem forte de sobreviv\u00eancia feminina, o que me diz bastante coisa relacionada \u00e0 label survived. Vamos manter.","e26ab53b":"**Vis\u00f5es**\n\nFare - manter\n\nSibSp - manter ( agrupar)\n\nParch - manter\n\nTicket has a high cardinality: 681 distinct values Warning - remover\n\nCabin - 687 de 891 desconhecido  - remover\n\nEmbarked tem alta cardinalidade com 644 de 891 sendo S, por\u00e9m, a maioria dos que sobrevivem s\u00e3o do C. Achei interessante, ent\u00e3o vou manter.\n\nName - 891 de 891 \u00fanicos, n\u00e3o me dando nenhuma caracter\u00edstica, entao eu removo\n\nPassengerId      Manter (Importante para o modelo ter como \u00edndice) \n\nSex             Manter\n\nAge            Manter\n","21abd70d":"# **Vamos dar aquela olhada final nas colunas restantes!**","88c8255e":"Por mais que a maioria dos viajantes tenham embarcado pelo S, a maior taxa de sobreviv\u00eancia est\u00e1 no C, ent\u00e3o achei interessante e vou manter."}}