{"cell_type":{"b6aae14a":"code","ff7a448d":"code","9b3c6e52":"code","020370cf":"code","c35ad52d":"code","2b72689b":"code","90dfc354":"code","1be67beb":"code","cd68fd27":"code","f382c8ae":"code","9443a6a6":"code","01596408":"code","d1ce2727":"code","435a391c":"code","7522b73c":"code","ab2c22a7":"code","3b464175":"code","d7f3a78a":"code","d48025e1":"code","e3c5153b":"code","1b124a5c":"code","65a6756b":"code","5ac0782f":"code","4dc79b7b":"code","c13f7629":"code","e1e64931":"code","ec2d116c":"code","b63e2416":"code","1e525349":"code","e9556d51":"code","ab392528":"code","a55f0a54":"code","bb497ef3":"code","410a6abc":"code","f6b2f546":"code","a230dbe0":"code","d3c7f582":"code","a24d8a04":"code","15c27074":"code","6cfe9352":"code","7384f647":"code","06a2d36d":"code","e5e58988":"code","ef0aad62":"markdown","29fd16ea":"markdown","df6225ea":"markdown","a18015a2":"markdown","2841bd28":"markdown","a5e69172":"markdown","ae97e87c":"markdown","da907621":"markdown","958690f3":"markdown","33910770":"markdown","a6edecd2":"markdown","1bdc5623":"markdown"},"source":{"b6aae14a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score\nimport os,re\nimport warnings\nwarnings.filterwarnings('ignore')\nimport nltk \nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom wordcloud import WordCloud\nimport tensorflow as tf\nfrom tqdm import tqdm, trange\nimport torch\nfrom torch.nn import BCEWithLogitsLoss\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler\nfrom transformers import BertTokenizer, BertForSequenceClassification","ff7a448d":"train_df = pd.read_csv('..\/input\/janatahack-independence-day-2020-ml-hackathon\/train.csv')\ntest_df = pd.read_csv('..\/input\/janatahack-independence-day-2020-ml-hackathon\/test.csv')","9b3c6e52":"train_df.isnull().sum()","020370cf":"test_df.isnull().sum()","c35ad52d":"cols = train_df.columns\nlabel_cols = list(cols[3:9])\nlabel_cols","2b72689b":"# plotting number of classes and their count of articles\nsns.barplot(['CS','Physics','Math','Stats','Bio','Fin'], train_df.iloc[:,3:9].sum().values)\nplt.title('Count of articles per class')\nplt.xlabel('Classes', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.show()","90dfc354":"# stopwords list and stemmer \nstop_words = stopwords.words('english')\nstemmer = PorterStemmer()","1be67beb":"# plotting wordcloud \nwordcloud = WordCloud(width = 400, height = 400, background_color ='white', stopwords = stop_words, \n                min_font_size = 10).generate(train_df['ABSTRACT'][0]) \nplt.figure(figsize = (10, 10), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.show()","cd68fd27":"def clean_text(df):\n  for i in range(0,len(df)):\n    text = re.sub('[^a-zA-Z]', ' ', df['ABSTRACT'][i])\n    text = text.lower()\n    text = text.split()\n    text = [stemmer.stem(word) for word in text if not word in stop_words]\n    text = ' '.join(text)\n    df['ABSTRACT'][i] = text\n  return df","f382c8ae":"# cleaning train data \ntrain_df = clean_text(train_df)","9443a6a6":"# cleaning train data \ntest_df = clean_text(test_df)","01596408":"train_df['one_hot_labels'] = list(train_df[label_cols].values)\nlabels = list(train_df['one_hot_labels'].values)\nabstract = list(train_df['ABSTRACT'].values)","d1ce2727":"# average abstract length\ntrain_df.insert(10,'Length',0)\nfor i in range(0,len(train_df)):\n  train_df['Length'][i] = len(train_df['ABSTRACT'][i])\ntrain_df['Length'].mean()","435a391c":"max_length = 200\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\nencodings = tokenizer.batch_encode_plus(abstract,max_length=max_length,pad_to_max_length=True,truncation=True)","7522b73c":"# loading necessary encoded values \ninput_ids = encodings['input_ids'] \ntoken_type_ids = encodings['token_type_ids'] \nattention_masks = encodings['attention_mask']","ab2c22a7":"# splitting data into training and validation\ntrain_inputs, validation_inputs, train_labels, validation_labels, train_token_types, validation_token_types, train_masks, validation_masks = train_test_split(input_ids, labels,\ntoken_type_ids,attention_masks, random_state=200, test_size=0.20)","3b464175":"# converting all numpy arrays to tensors\ntrain_inputs = torch.tensor(train_inputs)\ntrain_labels = torch.tensor(train_labels)\ntrain_masks = torch.tensor(train_masks)\ntrain_token_types = torch.tensor(train_token_types)\n\nvalidation_inputs = torch.tensor(validation_inputs)\nvalidation_labels = torch.tensor(validation_labels)\nvalidation_masks = torch.tensor(validation_masks)\nvalidation_token_types = torch.tensor(validation_token_types)","d7f3a78a":"# initialise batch size \nbatch_size = 32","d48025e1":"# creating data loader and loading data into it using random sampler\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels, train_token_types)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels, validation_token_types)\nvalidation_sampler = RandomSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","e3c5153b":"# saving data loaders\ntorch.save(validation_dataloader,'validation_data_loader')\ntorch.save(train_dataloader,'train_data_loader')","1b124a5c":"# loading pretrained BERT model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)","65a6756b":"# enabling GPU\nmodel.cuda()","5ac0782f":"# initialising optimizer\noptimizer = AdamW(model.parameters(),lr=3e-5,correct_bias=True)","4dc79b7b":"# device name of GPU\ndevice = 'cuda'\ntorch.cuda.get_device_name(0)","c13f7629":"epochs = 4\nval_acc_list = []\nfor _ in trange(epochs, desc=\"Epoch\"):\n\n  # set model to training mode \n  model.train()\n\n  # train the data for one epoch\n  for step, batch in enumerate(train_dataloader):\n    batch = tuple(t.to(device) for t in batch)\n    # unpack the inputs from dataloader\n    b_input_ids, b_input_mask, b_labels, b_token_types = batch\n    # clearing the gradients\n    optimizer.zero_grad()\n\n    # forward pass\n    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n    logits = outputs[0]\n    loss_func = BCEWithLogitsLoss() \n    # converting logits and labels to appropriate shape \n    loss = loss_func(logits.view(-1,6),b_labels.type_as(logits).view(-1,6)) \n\n    # backward pass\n    loss.backward()\n    # update parameters and take a step using the computed gradient\n    optimizer.step()\n\n  # set model to evaluation mode\n  model.eval()\n\n  # variables to gather outputs\n  logit_preds,true_labels,pred_labels = [],[],[]\n\n\n  # predict\n  for i, batch in enumerate(validation_dataloader):\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels, b_token_types = batch\n    with torch.no_grad():\n      # forward pass\n      outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n      b_logit_pred = outs[0]\n      pred_label = torch.sigmoid(b_logit_pred)\n\n      # converting tensors to numpy arrays\n      b_logit_pred = b_logit_pred.detach().cpu().numpy()\n      pred_label = pred_label.to('cpu').numpy()\n      b_labels = b_labels.to('cpu').numpy()\n\n    logit_preds.append(b_logit_pred)\n    true_labels.append(b_labels)\n    pred_labels.append(pred_label)\n\n  # flatten outputs\n  pred_labels = [item for sublist in pred_labels for item in sublist]\n  true_labels = [item for sublist in true_labels for item in sublist]\n\n  # calculate accuracy\n  threshold = 0.40\n  pred_bools = [pl>threshold for pl in pred_labels]\n  true_bools = [tl==1 for tl in true_labels]\n  val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\n  val_acc_list.append(val_f1_accuracy)\n  print('F1 Validation Accuracy: ', val_f1_accuracy)","e1e64931":"# plotting validation accuracy vs epochs\nepoch_list = [ i for i in range(1,epochs+1) ]\nplt.title('Validation Accuracy vs Epochs')\nplt.plot(epoch_list,val_acc_list,'red')\nplt.xlabel('No of Epochs')\nplt.ylabel('Validation Accuracy')\nplt.show()","ec2d116c":"# save model\ntorch.save(model.state_dict(), 'bert_model_topic')","b63e2416":"# preparing test data \ntest_df.insert(3, \"Computer Science\", 0) \ntest_df.insert(4, \"Physics\", 0)\ntest_df.insert(5, \"Mathematics\", 0)\ntest_df.insert(6, \"Statistics\", 0)\ntest_df.insert(7, \"Quantitative Biology\", 0)\ntest_df.insert(8, \"Quantitative Finance\", 0)","1e525349":"# one hot labels for test data\ntest_label_cols = list(test_df.columns[4:])\ntest_df['one_hot_labels'] = list(test_df[test_label_cols].values)","e9556d51":"test_labels = list(test_df['one_hot_labels'].values)\ntest_abstract = list(test_df['ABSTRACT'].values)","ab392528":"# encoding test data\ntest_encodings = tokenizer.batch_encode_plus(test_abstract,max_length=max_length,pad_to_max_length=True,truncation=True)\ntest_input_ids = test_encodings['input_ids']\ntest_token_type_ids = test_encodings['token_type_ids']\ntest_attention_masks = test_encodings['attention_mask']","a55f0a54":"# converting test data into tensors\ntest_inputs = torch.tensor(test_input_ids)\ntest_labels = torch.tensor(test_labels)\ntest_masks = torch.tensor(test_attention_masks)\ntest_token_types = torch.tensor(test_token_type_ids)","bb497ef3":"# create test dataloader \ntest_data = TensorDataset(test_inputs, test_masks, test_labels, test_token_types)\ntest_sampler = RandomSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)","410a6abc":"# save test dataloader\ntorch.save(test_dataloader,'test_data_loader')","f6b2f546":"# testing \n\n# put model in evaluation mode \nmodel.eval()\n\nlogit_preds,pred_labels = [],[]\n\n# predict\n# reading input from each batch\nfor i, batch in enumerate(test_dataloader):\n  batch = tuple(t.to(device) for t in batch)\n  # unpack the inputs from test dataloader\n  b_input_ids, b_input_mask, b_labels, b_token_types = batch\n  with torch.no_grad():\n    # forward pass\n    outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n    b_logit_pred = outs[0]\n    pred_label = torch.sigmoid(b_logit_pred)\n\n    # converting into numpy arrays \n    b_logit_pred = b_logit_pred.detach().cpu().numpy()\n    pred_label = pred_label.to('cpu').numpy()\n    b_labels = b_labels.to('cpu').numpy()\n\n  # appending output variables \n  logit_preds.append(b_logit_pred)\n  pred_labels.append(pred_label)\n\n# flatten output variables\npred_labels = [item for sublist in pred_labels for item in sublist]\n\n# converting flattened binary values to boolean values\npred_bools = [pl>0.4 for pl in pred_labels]","a230dbe0":"# dictonary of classes and respective encoded ids\nidx2label = dict(zip(range(6),label_cols))\nprint(idx2label)","d3c7f582":"# appending ids to list\npred_label_ids=[]\nfor vals in pred_bools:\n  pred_label_ids.append(np.where(vals)[0].flatten().tolist())","a24d8a04":"# converting ids to texts\npred_label_texts = []\nfor vals in pred_label_ids:\n  if vals:\n    pred_label_texts.append([idx2label[val] for val in vals])\n  else:\n    pred_label_texts.append(vals)","15c27074":"# output dataframe\noutput_df = pd.DataFrame({'pred_labels':pred_label_texts})\noutput_df.head()","6cfe9352":"output_df.insert(0,'ID','0')\noutput_df['ID'] = test_df['ID']\noutput_df.insert(2, \"Computer Science\", 0) \noutput_df.insert(3, \"Physics\", 0)\noutput_df.insert(4, \"Mathematics\", 0)\noutput_df.insert(5, \"Statistics\", 0)\noutput_df.insert(6, \"Quantitative Biology\", 0)\noutput_df.insert(7, \"Quantitative Finance\", 0)","7384f647":"# converting encoded ids to textual labels\nfor i in range(0,len(output_df)):\n  for j in range(0,len(output_df.iloc[:,1][i])):\n    if output_df.iloc[:,1][i][j] == 'Computer Science':\n      output_df.iloc[:,2][i] = 1\n    elif output_df.iloc[:,1][i][j] =='Phyiscs':\n        output_df.iloc[:,3][i] = 1\n    elif output_df.iloc[:,1][i][j] =='Mathematics':\n        output_df.iloc[:,4][i] = 1\n    elif output_df.iloc[:,1][i][j] =='Statistics':\n        output_df.iloc[:,5][i] = 1\n    elif output_df.iloc[:,1][i][j] =='Quantitative Biology':\n        output_df.iloc[:,6][i] = 1\n    elif output_df.iloc[:,1][i][j] =='Quantitative Finance':\n        output_df.iloc[:,7][i] = 1","06a2d36d":"output_df.drop(['pred_labels'],inplace=True,axis=1)","e5e58988":"# saving final output dataframe into csv file\noutput_df.to_csv('bert_submission.csv',index=False)","ef0aad62":"number of classes","29fd16ea":"function for cleaning data","df6225ea":"# Preparation of output dataframe","a18015a2":"# Importing libraries","2841bd28":"initalise max length , bert tokenizer and encoding train data\n\nless length in order to avoid memory error ","a5e69172":"# Training and Validation","ae97e87c":"converting train data classes into one hot label list","da907621":"reading dataset","958690f3":"# Testing","33910770":"# Preparation of test data","a6edecd2":"checking for null values","1bdc5623":"# Data visualization and preprocessing"}}