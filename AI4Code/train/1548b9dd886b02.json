{"cell_type":{"93e58a16":"code","bd06a023":"code","3cceddc1":"code","72bd5ca2":"code","0cca3a66":"code","cdfda608":"code","34c1e175":"code","6338f6ad":"code","f3c58371":"code","467177de":"code","6f93c410":"code","6206c4a4":"code","207515c4":"code","7762376a":"code","aa34f19d":"code","9d308963":"code","298e3256":"code","0a15482d":"code","a60b4fc1":"code","317a7ce0":"code","e3a5f343":"code","165f669b":"code","7967e8cd":"markdown","9072be49":"markdown","a4c57ea7":"markdown","91e42f98":"markdown","d972cd0e":"markdown","25b7bc23":"markdown","9d5d8961":"markdown","327f63ba":"markdown","6e3c18d4":"markdown","2d98a014":"markdown","9701ed56":"markdown","b015049f":"markdown","ce354586":"markdown","a84d8871":"markdown","73743ac5":"markdown","b4a626aa":"markdown","05478f65":"markdown","ab758d6b":"markdown","be12038c":"markdown","c44d0831":"markdown","481d7cdc":"markdown","c02ff1b3":"markdown","fc37f1e1":"markdown"},"source":{"93e58a16":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport time\nimport warnings\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom scipy.sparse import hstack\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n#from sklearn.cross_validation import StratifiedKFold \nfrom collections import Counter, defaultdict\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom sklearn.metrics import normalized_mutual_info_score\nfrom sklearn.ensemble import RandomForestClassifier\nwarnings.filterwarnings(\"ignore\")\n\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\n","bd06a023":"data = pd.read_csv('\/kaggle\/input\/msk-redefining-cancer-treatment\/training_variants.zip')\nprint('Number of data points : ', data.shape[0])\nprint('Number of features : ', data.shape[1])\nprint('Features : ', data.columns.values)\n#y_true=data['Class'].values\n#data=data.drop('Class', axis=1)\n\ndata.head()\ntest_variant = pd.read_csv('\/kaggle\/input\/msk-redefining-cancer-treatment\/test_variants.zip')\ntest_text = pd.read_csv('\/kaggle\/input\/msk-redefining-cancer-treatment\/test_text.zip', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"TEXT\"])","3cceddc1":"# note the seprator in this file\ndata_text =pd.read_csv('\/kaggle\/input\/msk-redefining-cancer-treatment\/training_text.zip',sep=\"\\|\\|\",engine=\"python\",names=[\"ID\",\"TEXT\"],skiprows=1)\nprint('Number of data points : ', data_text.shape[0])\nprint('Number of features : ', data_text.shape[1])\nprint('Features : ', data_text.columns.values)\ndata_text.head()","72bd5ca2":"# loading stop words from nltk library\n#import nltk\n#nltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\n\ndef nlp_preprocessing(total_text, index, column):\n    if type(total_text) is not int:\n        string = \"\"\n        # replace every special char with space\n        total_text = re.sub('[^a-zA-Z0-9\\n]', ' ', total_text)\n        # replace multiple spaces with single space\n        total_text = re.sub('\\s+',' ', total_text)\n        # converting all the chars into lower-case.\n        total_text = total_text.lower()\n        \n        for word in total_text.split():\n        # if the word is a not a stop word then retain that word from the data\n            if not word in stop_words:\n                string += word + \" \"\n        \n        return string\n","0cca3a66":"#start_time = time.clock()\nfor index, row in data_text.iterrows():\n    if type(row['TEXT']) is str:\n        data_text['TEXT'][index]=nlp_preprocessing(row['TEXT'], index, 'TEXT')\n    else:\n        print(\"there is no text description for id:\",index)\n#print('Time took for preprocessing the text :',time.clock() - start_time, \"seconds\")\n#start_time = time.clock()\nfor index1, row1 in test_text.iterrows():\n    if type(row1['TEXT']) is str:\n        test_text['TEXT'][index1]=nlp_preprocessing(row1['TEXT'], index1, 'TEXT')\n    else:\n        print(\"there is no text description for id:\",index1)\n#print('Time took for preprocessing the text :',time.clock() - start_time, \"seconds\")","cdfda608":"#merging both gene_variations and text data based on ID\nd=pd.merge(test_variant,test_text,on='ID',how='left')\ntest_index = d['ID'].values\nprint(d.shape)\nresult = pd.merge(data, data_text,on='ID', how='left')\nsecond=d.shape[0]\n\nakku_data=result.copy()\n#akku_data['Class']=y_true\ny_true=result['Class'].values\nresult=result.drop('Class', axis=1)\nprint(akku_data.shape)\nfirst=result.shape[0]\nprint(first)\nprint(second)\nresult = np.concatenate((result, d), axis=0)\nresult = pd.DataFrame(result)\nresult.columns= [\"ID\", \"Gene\", \"Variation\", \"TEXT\"]\nprint(result.head())\nprint(result.shape)\n#for i in range(result['TEXT'].shape[0]):\n #   nlp_preprocessing(result['TEXT'][i],i,'TEXT')","34c1e175":"result[result.isnull().any(axis=1)]\nresult.loc[result['TEXT'].isnull(),'TEXT'] = result['Gene'] +' '+result['Variation']\n\nakku_data[akku_data.isnull().any(axis=1)]\nakku_data.loc[akku_data['TEXT'].isnull(),'TEXT'] = akku_data['Gene'] +' '+akku_data['Variation']","6338f6ad":"y_true1 = akku_data['Class'].values\n#result.drop('Class', axis=1)\nresult.Gene = result.Gene.str.replace('\\s+', '_')\nresult.Variation = result.Variation.str.replace('\\s+', '_')\n\n# split the data into test and train by maintaining same distribution of output varaible 'y_true' [stratify=y_true]\nX_train, test_df, y_train, y_test = train_test_split(akku_data, y_true1, test_size=0.2)\n# split the train data into train and cross validation by maintaining same distribution of output varaible 'y_train' [stratify=y_train]\ntrain_df, cv_df, y_train, y_cv = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)\n\nprint('Number of data points in train data:', train_df.shape[0])\nprint('Number of data points in test data:', test_df.shape[0])\nprint('Number of data points in cross validation data:', cv_df.shape[0])","f3c58371":"# it returns a dict, keys as class labels and values as the number of data points in that class\ntrain_class_distribution = train_df['Class'].value_counts().sort_values()\ntest_class_distribution = test_df['Class'].value_counts().sort_values()\ncv_class_distribution = cv_df['Class'].value_counts().sort_values()\n\nmy_colors = 'rgbkymc'\ntrain_class_distribution.plot(kind='bar')\nplt.xlabel('Class')\nplt.ylabel('Data points per Class')\nplt.title('Distribution of yi in train data')\nplt.grid()\nplt.show()\n\n# ref: argsort https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.argsort.html\n# -(train_class_distribution.values): the minus sign will give us in decreasing order\nsorted_yi = np.argsort(-train_class_distribution.values)\nfor i in sorted_yi:\n    print('Number of data points in class', i+1, ':',train_class_distribution.values[i], '(', np.round((train_class_distribution.values[i]\/train_df.shape[0]*100), 3), '%)')\n\n    \nprint('-'*80)\nmy_colors = 'rgbkymc'\ntest_class_distribution.plot(kind='bar')\nplt.xlabel('Class')\nplt.ylabel('Data points per Class')\nplt.title('Distribution of yi in test data')\nplt.grid()\nplt.show()\n\n# ref: argsort https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.argsort.html\n# -(train_class_distribution.values): the minus sign will give us in decreasing order\nsorted_yi = np.argsort(-test_class_distribution.values)\nfor i in sorted_yi:\n    print('Number of data points in class', i+1, ':',test_class_distribution.values[i], '(', np.round((test_class_distribution.values[i]\/test_df.shape[0]*100), 3), '%)')\n\nprint('-'*80)\nmy_colors = 'rgbkymc'\ncv_class_distribution.plot(kind='bar')\nplt.xlabel('Class')\nplt.ylabel('Data points per Class')\nplt.title('Distribution of yi in cross validation data')\nplt.grid()\nplt.show()\n\n# ref: argsort https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.argsort.html\n# -(train_class_distribution.values): the minus sign will give us in decreasing order\nsorted_yi = np.argsort(-train_class_distribution.values)\nfor i in sorted_yi:\n    print('Number of data points in class', i+1, ':',cv_class_distribution.values[i], '(', np.round((cv_class_distribution.values[i]\/cv_df.shape[0]*100), 3), '%)')\n","467177de":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n    \n    A =(((C.T)\/(C.sum(axis=1))).T)\n\n    B =(C\/C.sum(axis=0))\n    \n    \n    labels = [1,2,3,4,5,6,7,8,9]\n \n    print(\"-\"*20, \"Confusion matrix\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(C, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n\n    print(\"-\"*20, \"Precision matrix (Columm Sum=1)\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(B, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    \n    # representing B in heatmap format\n    print(\"-\"*20, \"Recall matrix (Row sum=1)\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(A, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()","6f93c410":"unique_genes = train_df['Gene'].value_counts()\nprint('Number of Unique Genes :', unique_genes.shape[0])\n# the top 10 genes that occured most\nprint(unique_genes.head(10))\ns = sum(unique_genes.values);\nh = unique_genes.values\/s;\nplt.plot(h, label=\"Histrogram of Genes\")\nplt.xlabel('Index of a Gene')\nplt.ylabel('Number of Occurances')\nplt.legend()\nplt.grid()\nplt.show()\nc = np.cumsum(h)\nplt.plot(c,label='Cumulative distribution of Genes')\nplt.grid()\nplt.legend()\nplt.show()","6206c4a4":"\n# one-hot encoding of Gene feature.\nprint(first)\ngene_vectorizer = CountVectorizer()\ntrain_gene_feature_onehotCoding = gene_vectorizer.fit_transform(result['Gene'][0:first])\nprint(len(result['Gene'][0:first]))\ntest_gene_feature_onehotCoding = gene_vectorizer.transform(result['Gene'][first:])\n#cv_gene_feature_onehotCoding = gene_vectorizer.transform(cv_df['Gene'])","207515c4":"alpha = [10 ** x for x in range(-5, 1)] \n\n\ncv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_gene_feature_onehotCoding[0:len(y_train)], y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_gene_feature_onehotCoding[0:len(y_train)], y_train)\n    predict_y = sig_clf.predict_proba(test_gene_feature_onehotCoding[0:len(y_cv)])\n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_gene_feature_onehotCoding[0:len(y_train)], y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_gene_feature_onehotCoding[0:len(y_train)], y_train)\n\npredict_y = sig_clf.predict_proba(train_gene_feature_onehotCoding[0:len(y_train)])\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n#predict_y = sig_clf.predict_proba(cv_gene_feature_onehotCoding[0:len(y_cv)])\n#print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_gene_feature_onehotCoding[0:len(y_test)])\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n#plot_confusion_matrix(y_test, predict_y)","7762376a":"unique_variations = train_df['Variation'].value_counts()\nprint('Number of Unique Variations :', unique_variations.shape[0])\n# the top 10 variations that occured most\nprint(unique_variations.head(10))\ns = sum(unique_variations.values);\nh = unique_variations.values\/s;\nplt.plot(h, label=\"Histrogram of Variations\")\nplt.xlabel('Index of a Variation')\nplt.ylabel('Number of Occurances')\nplt.legend()\nplt.grid()\nplt.show()\nc = np.cumsum(h)\nprint(c)\nplt.plot(c,label='Cumulative distribution of Variations')\nplt.grid()\nplt.legend()\nplt.show()","aa34f19d":"# one-hot encoding of variation feature.\nvariation_vectorizer = CountVectorizer()\ntrain_variation_feature_onehotCoding = variation_vectorizer.fit_transform(result['Variation'][0:first])\ntest_variation_feature_onehotCoding = variation_vectorizer.transform(result['Variation'][first:])\n#cv_variation_feature_onehotCoding = variation_vectorizer.transform(cv_df['Variation'])","9d308963":"alpha = [10 ** x for x in range(-5, 1)]\n\n# read more about SGDClassifier() at http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=\u2019hinge\u2019, penalty=\u2019l2\u2019, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=\u2019optimal\u2019, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, \u2026])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: \n#------------------------------\n\n\ncv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_variation_feature_onehotCoding[0:len(y_train)], y_train)\n    \n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_variation_feature_onehotCoding[0:len(y_train)], y_train)\n    predict_y = sig_clf.predict_proba(test_variation_feature_onehotCoding[0:len(y_cv)])\n    \n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_variation_feature_onehotCoding[0:len(y_train)], y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_variation_feature_onehotCoding[0:len(y_train)], y_train)\n\npredict_y = sig_clf.predict_proba(train_variation_feature_onehotCoding[0:len(y_train)])\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n#predict_y = sig_clf.predict_proba(cv_variation_feature_onehotCoding)\n#print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_variation_feature_onehotCoding[0:len(y_test)])\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n","298e3256":"# cls_text is a data frame\n# for every row in data fram consider the 'TEXT'\n# split the words by space\n# make a dict with those words\n# increment its count whenever we see that word\n\ndef extract_dictionary_paddle(cls_text):\n    dictionary = defaultdict(int)\n    for index, row in cls_text.iterrows():\n        for word in row['TEXT'].split():\n            dictionary[word] +=1\n    return dictionary\n\n##########response coding function\nimport math\n#https:\/\/stackoverflow.com\/a\/1602964\ndef get_text_responsecoding(df):\n    text_feature_responseCoding = np.zeros((df.shape[0],9))\n    for i in range(0,9):\n        row_index = 0\n        for index, row in df.iterrows():\n            sum_prob = 0\n            for word in row['TEXT'].split():\n                sum_prob += math.log(((dict_list[i].get(word,0)+10 )\/(total_dict.get(word,0)+90)))\n            text_feature_responseCoding[row_index][i] = math.exp(sum_prob\/len(row['TEXT'].split()))\n            row_index += 1\n    return text_feature_responseCoding","0a15482d":"text_vectorizer = CountVectorizer(min_df=3)\n#print(result['TEXT'].shape[0])\ntrain_text_feature_onehotCoding = text_vectorizer.fit_transform(result['TEXT'][0:first])\n# getting all the feature names (words)\ntrain_text_features= text_vectorizer.get_feature_names()\n\n# train_text_feature_onehotCoding.sum(axis=0).A1 will sum every row and returns (1*number of features) vector\ntrain_text_fea_counts = train_text_feature_onehotCoding.sum(axis=0).A1\n\n# zip(list(text_features),text_fea_counts) will zip a word with its number of times it occured\ntext_fea_dict = dict(zip(list(train_text_features),train_text_fea_counts))\n\n\nprint(\"Total number of unique words in train data :\", len(train_text_features))\n\n######################                          Normalization                   ################\n# don't forget to normalize every feature\ntrain_text_feature_onehotCoding = normalize(train_text_feature_onehotCoding, axis=0)\n\n# we use the same vectorizer that was trained on train data\ntest_text_feature_onehotCoding = text_vectorizer.transform(result['TEXT'][first:])\n# don't forget to normalize every feature\ntest_text_feature_onehotCoding = normalize(test_text_feature_onehotCoding, axis=0)\n\n# we use the same vectorizer that was trained on train data\n#cv_text_feature_onehotCoding = text_vectorizer.transform(cv_df['TEXT'])\n# don't forget to normalize every feature\n#cv_text_feature_onehotCoding = normalize(cv_text_feature_onehotCoding, axis=0)","a60b4fc1":"# Train a Logistic regression+Calibration model using text features whicha re on-hot encoded\nalpha = [10 ** x for x in range(-5, 1)]\n\n\ncv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_text_feature_onehotCoding[0:len(y_train)], y_train)\n    \n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_text_feature_onehotCoding[0:len(y_train)], y_train)\n    predict_y = sig_clf.predict_proba(test_text_feature_onehotCoding[0:len(y_cv)])\n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_text_feature_onehotCoding[0:len(y_train)], y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_text_feature_onehotCoding[0:len(y_train)], y_train)\n\npredict_y = sig_clf.predict_proba(train_text_feature_onehotCoding[0:len(y_train)])\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n#predict_y = sig_clf.predict_proba(cv_text_feature_onehotCoding)\n#print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_text_feature_onehotCoding[0:len(y_test)])\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n","317a7ce0":"train_gene_var_onehotCoding = hstack((train_gene_feature_onehotCoding,train_variation_feature_onehotCoding))\ntest_gene_var_onehotCoding = hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\n# = hstack((cv_gene_feature_onehotCoding,cv_variation_feature_onehotCoding))\n\ntrain_x_onehotCoding = hstack((train_gene_var_onehotCoding, train_text_feature_onehotCoding)).tocsr()\nprint(train_x_onehotCoding.shape)\ntrain_y = np.array(list(y_true))\nprint(train_x_onehotCoding.shape)\ntest_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_text_feature_onehotCoding)).tocsr()\nprint(test_x_onehotCoding.shape)\ntest_y = np.array(list(y_true))\n\n#cv_x_onehotCoding = hstack((cv_gene_var_onehotCoding, cv_text_feature_onehotCoding)).tocsr()\n#cv_y = np.array(list(cv_df['Class']))","e3a5f343":"def predict_and_plot_confusion_matrix(train_x, train_y,test_x, test_y, clf):\n    clf.fit(train_x, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x, train_y)\n    pred_y = sig_clf.predict(test_x)\n\n    # for calculating log_loss we willl provide the array of probabilities belongs to each class\n    print(\"Log loss :\",log_loss(test_y, sig_clf.predict_proba(test_x)))\n    # calculating the number of data points that are misclassified\n    print(\"Number of mis-classified points :\", np.count_nonzero((pred_y- test_y))\/test_y.shape[0])\n    plot_confusion_matrix(test_y, pred_y)","165f669b":"alpha = [100,200,500,1000,2000]\nmax_depth = [5, 10]\ncv_log_error_array = []\n\nfor i in alpha:\n    for j in max_depth:\n        print(\"for n_estimators =\", i,\"and max depth = \", j)\n        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n        clf.fit(train_x_onehotCoding, train_y)\n        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        sig_clf.fit(train_x_onehotCoding, train_y)\n        sig_clf_probs = sig_clf.predict_proba(test_x_onehotCoding[0:len(train_y)])\n        cv_log_error_array.append(log_loss(train_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n        print(\"Log Loss :\",log_loss(train_y, sig_clf_probs)) \n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\npredict_y = sig_clf.predict_proba(test_x_onehotCoding[0:len(y_train)])\n\nprint(len(test_index))\n\n\nprint('For values of best estimator = ', alpha[int(best_alpha\/2)], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n#predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n#print('For values of best estimator = ', alpha[int(best_alpha\/2)], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nprint(len(predict_y))\nsubmission = pd.DataFrame(predict_y)\nsubmission['id'] = test_index\nsubmission.columns = ['class1', 'class2', 'class3', 'class4', 'class5', 'class6', 'class7', 'class8', 'class9', 'id']\nsubmission.to_csv(\"submission_all.csv\",index=False)\nsubmission.head()\n#('For values of best estimator = ', alpha[int(best_alpha\/2)], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\n\n\n\n############             best aplha  -hyper parameter\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\n\n#confusion matrix\n#predict_and_plot_confusion_matrix(train_y,test_x_onehotCoding,test_y,pre clf)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\npred_y = sig_clf.predict(test_x_onehotCoding)\n\n    # for calculating log_loss we willl provide the array of probabilities belongs to each class\n#print(\"Log loss :\",log_loss(train_y[0:len(sig_clf.predict_proba(test_x_onehotCoding))], sig_clf.predict_proba(test_x_onehotCoding)))\n    # calculating the number of data points that are misclassified\n#print(\"Number of mis-classified points :\", np.count_nonzero((pred_y- train_y))\/train_y.shape[0])\nprint(pred_y.shape)\nprint(train_y.shape)\nplot_confusion_matrix(train_y, pred_y[0:first])","7967e8cd":"<h2>3.2.3 logistic regression model with hyper parameter -to check the feature<\/h2>","9072be49":"<h2>3.3.3 logistic regression on text feature<\/h2>\n","a4c57ea7":"# Reading Text Data","91e42f98":"# Univariate Analysis\n","d972cd0e":"# Stacking all features (one hot encoding)","25b7bc23":"# Exploratory Data Analysis","9d5d8961":"# Conclusion\n<h4>log loss is minimum in random forest model around 1.1, therefore RFM is best suited model<\/h4> ","327f63ba":"<h2>3.3 logistic regression model with hyper parameter -to check the feature<\/h2>","6e3c18d4":"<h3>3.2.1 Univariate Analysis on Variation Feature  histogram and pdf<\/h3>\n","2d98a014":"<h6> Problem statement : <\/h6>\n<p> Classify the given genetic variations\/mutations based on evidence from text-based clinical literature. <\/p>","9701ed56":"# Confusion ,Recall,Precision function","b015049f":"<h6>training_variants<\/h6>\n<hr>\nID,Gene,Variation,Class<br>\n0,FAM58A,Truncating Mutations,1 <br>\n1,CBL,W802*,2 <br>\n2,CBL,Q249E,2 <br>\n...\n\n<h6> training_text<\/h6>\n<hr>\nID,Text <br>\n0||Cyclin-dependent kinases (CDKs) regulate a variety of fundamental cellular processes. CDK10 stands out as one of the last orphan CDKs for which no activating cyclin has been identified and no kinase activity revealed. Previous work has shown that CDK10 silencing increases ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2)-driven activation of the MAPK pathway, which confers tamoxifen resistance to breast cancer cells. The precise mechanisms by which CDK10 modulates ETS2 activity, and more generally the functions of CDK10, remain elusive. Here we demonstrate that CDK10 is a cyclin-dependent kinase by identifying cyclin M as an activating cyclin. Cyclin M, an orphan cyclin, is the product of FAM58A, whose mutations cause STAR syndrome, a human developmental anomaly whose features include toe syndactyly, telecanthus, and anogenital and renal malformations. We show that STAR syndrome-associated cyclin M mutants are unable to interact with CDK10. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and in conferring tamoxifen resistance to breast cancer cells. CDK10\/cyclin M phosphorylates ETS2 in vitro, and in cells it positively controls ETS2 degradation by the proteasome. ETS2 protein levels are increased in cells derived from a STAR patient, and this increase is attributable to decreased cyclin M levels. Altogether, our results reveal an additional regulatory mechanism for ETS2, which plays key roles in cancer and development. They also shed light on the molecular mechanisms underlying STAR syndrome.Cyclin-dependent kinases (CDKs) play a pivotal role in the control of a number of fundamental cellular processes (1). The human genome contains 21 genes encoding proteins that can be considered as members of the CDK family owing to their sequence similarity with bona fide CDKs, those known to be activated by cyclins (2). Although discovered almost 20 y ago (3, 4), CDK10 remains one of the two CDKs without an identified cyclin partner. This knowledge gap has largely impeded the exploration of its biological functions. CDK10 can act as a positive cell cycle regulator in some cells (5, 6) or as a tumor suppressor in others (7, 8). CDK10 interacts with the ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2) transcription factor and inhibits its transcriptional activity through an unknown mechanism (9). CDK10 knockdown derepresses ETS2, which increases the expression of the c-Raf protein kinase, activates the MAPK pathway, and induces resistance of MCF7 cells to tamoxifen (6). ... ","ce354586":"<h3>3.3.2 one hot encoding of text feature<\/h3>","a84d8871":"# Personalized cancer diagnosis","73743ac5":"<h2>3.2.2 one hot encoding  on Variation feature<\/h2>","b4a626aa":"# Random forest model ","05478f65":"# Test, Train and Cross Validation Split","ab758d6b":"<h2>3.3 Univariate analysis of text feature<\/h2>","be12038c":"<h4> Distribution of y_i's in Train, Test and Cross Validation datasets<\/h4>","c44d0831":"# Reading Gene and Variation Data","481d7cdc":"# Preprocessing of text","c02ff1b3":"<h3>3.1 Univariate Analysis on Gene Feature  histogram and pdf<\/h3>\n","fc37f1e1":"<h2>3.2one hot encoding response encoding on gene feature<\/h2>"}}