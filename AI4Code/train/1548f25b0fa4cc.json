{"cell_type":{"48d88f0a":"code","ff381b02":"code","46a80b5e":"code","f6719643":"code","7c0d1203":"code","da009b5f":"code","af258e07":"code","a9fefae4":"code","b89dbe71":"code","1dc798ca":"code","07ba54cc":"code","0f0004a0":"code","8892aff3":"code","6697da6c":"code","8ecbac2b":"code","288563a5":"code","277f367b":"code","0ba67b4b":"code","5be9a409":"code","52e6f8b6":"code","4e1cbdf6":"code","cddeffab":"code","f6662a93":"code","3888ca4b":"markdown","4f8d0c56":"markdown"},"source":{"48d88f0a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ff381b02":"!pip install tensorflow_probability==0.12.2","46a80b5e":"import tensorflow_probability as tfp","f6719643":"#title Import and set ups\n%matplotlib inline\nimport matplotlib as mpl\nfrom matplotlib import pylab as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport pandas as pd\n\nimport collections\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nimport tensorflow_probability as tfp\n\nfrom tensorflow_probability import distributions as tfd\nfrom tensorflow_probability import sts\n\ntf.enable_v2_behavior()","7c0d1203":"#tfp.sts.build_factored_surrogate_posterior(model=demand_model)","da009b5f":"df = pd.read_csv('..\/input\/household-energy-consumption\/Household_Energy_Consumption.csv')","af258e07":"df.head()","a9fefae4":"df.shape","b89dbe71":"new = df[\"Sub_metering_1\"].str.split(\".\", n = 1, expand = True)\ndf[\"Sub_metering_1\"] = new[0]\nnew1 = df[\"Sub_metering_2\"].str.split(\".\", n = 1, expand = True)\ndf[\"Sub_metering_2\"] = new1[0]","1dc798ca":"df[\"Sub_metering_1\"] = df['Sub_metering_1'].str.replace('?', '0').astype(float)\ndf[\"Sub_metering_2\"] = df['Sub_metering_2'].str.replace('?', '0').astype(float)","07ba54cc":"df.info()","0f0004a0":"df.fillna(0, inplace=True)\ndf['Meter'] = df.apply(lambda row: row['Sub_metering_1'] + row['Sub_metering_2'] + row['Sub_metering_3'], axis=1)","8892aff3":"df.isna().sum()","6697da6c":"df['Date1'] = pd.to_datetime(df['Date1'])\ndf.set_index('Date1', inplace=True)\ndf.head()","8ecbac2b":"df1 = df['Meter']\ndf1_hourly = df1.resample('H').sum()\ndf1_Daily = df1.resample('D').sum()\ndf1_weekly = df1.resample('W').sum()\ndf1_monthly = df1.resample('M').sum()","288563a5":"num_forecast_steps = 24 * 7 * 2 # Two weeks.\ndemand_training_data = df1_hourly[:-num_forecast_steps]","277f367b":"demand_training_data.shape","0ba67b4b":"def build_model(observed_time_series):\n    \n    hour_of_day_effect = sts.Seasonal(num_seasons=24, observed_time_series=observed_time_series,\n      name='hour_of_day_effect')\n    day_of_week_effect = sts.Seasonal(num_seasons = 7, num_steps_per_season=24, observed_time_series=observed_time_series,\n      name='day_of_week_effect')\n    week_of_year_effect = sts.Seasonal(num_seasons = 52, num_steps_per_season=168, observed_time_series=observed_time_series,\n      name='week_of_year_effect')    \n    month_of_year_effect = sts.Seasonal(num_seasons = 12, num_steps_per_season=728, observed_time_series=observed_time_series,\n      name='month_of_year_effect')    \n    autoregressive = sts.Autoregressive(order=1, observed_time_series=observed_time_series, name='autoregressive')\n    model = sts.Sum([hour_of_day_effect, day_of_week_effect, week_of_year_effect, month_of_year_effect, autoregressive],\n                   observed_time_series=observed_time_series)\n    return model","5be9a409":"trainingdata = demand_training_data.to_numpy().reshape(-1, 1)","52e6f8b6":"# detect and init the TPU\n#tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\n#tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\n#with tpu_strategy.scope():\n#    demand_model = build_model(trainingdata)\n#    variational_posteriors = tfp.sts.build_factored_surrogate_posterior(model=demand_model)\n\n# train model normally\n#model.fit(training_dataset, epochs=EPOCHS, steps_per_epoch=\u2026)","4e1cbdf6":"demand_model = build_model(trainingdata)\n%time\n# Build the variational surrogate posteriors `qs`. Using GPU\nvariational_posteriors = tfp.sts.build_factored_surrogate_posterior(model=demand_model)","cddeffab":"demand_model.components","f6662a93":"variational_posteriors","3888ca4b":"Flip to TPU usage for model training","4f8d0c56":"Importing the dataset"}}