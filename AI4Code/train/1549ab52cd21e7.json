{"cell_type":{"53aaca68":"code","b732c02e":"code","c4b2a2af":"code","d70bbfab":"code","2c9dbe35":"code","3eed90db":"code","b5f4e9bb":"code","4a85f077":"code","e5e5700c":"code","d03af993":"code","48d74d01":"code","cad94178":"code","9afd01e0":"code","eb624655":"code","7df9617c":"code","33ade5eb":"code","0e27eca7":"code","20874334":"code","6838baf9":"code","f7bc6fab":"code","497f78a0":"code","aa8adeb6":"code","f54e20e9":"code","858206ee":"code","f2fd329f":"code","1137b6c4":"code","921b2804":"code","0dfef14f":"code","bb0d7416":"code","9bbe3983":"code","494aed49":"markdown","fa2d2946":"markdown","3ded3575":"markdown","020c1fdc":"markdown","8e49d3a7":"markdown","314fcb31":"markdown","2ff74a6e":"markdown","361e4b1c":"markdown","84cf14c1":"markdown","c2ab3b96":"markdown","adaa1717":"markdown","e20fb7f1":"markdown","3f490302":"markdown","4211fc68":"markdown","d0cf7318":"markdown","d3ad6c92":"markdown","062ca17f":"markdown","28818c36":"markdown","603c2e08":"markdown"},"source":{"53aaca68":"import matplotlib.pyplot as plt\nimport numpy as np\n\nx = [1, 2, 2.5, 3, 4]\ny = [1, 4, 7, 9, 15]\nplt.plot(x, y, 'ro')\nplt.axis([0, 6, 0, 20])","b732c02e":"plt.plot(x, y, 'ro')\nplt.axis([0, 6, 0, 20])\nplt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\nplt.show()","c4b2a2af":"from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom six.moves import urllib\n\nimport tensorflow.compat.v2.feature_column as fc\n\nimport tensorflow as tf","d70bbfab":"# Load dataset.\ndftrain = pd.read_csv('https:\/\/storage.googleapis.com\/tf-datasets\/titanic\/train.csv') # training data\ndfeval = pd.read_csv('https:\/\/storage.googleapis.com\/tf-datasets\/titanic\/eval.csv') # testing data\ny_train = dftrain.pop('survived')\ny_eval = dfeval.pop('survived')","2c9dbe35":"dftrain.head()","3eed90db":"dftrain.describe()","b5f4e9bb":"dftrain.shape[0], dfeval.shape[0]","4a85f077":"y_train.head()","e5e5700c":"dftrain.age.hist(bins=20)","d03af993":"dftrain.sex.value_counts().plot(kind='barh')","48d74d01":"dftrain['class'].value_counts().plot(kind='barh')","cad94178":"pd.concat([dftrain, y_train], axis=1).groupby('sex').survived.mean().plot(kind='barh').set_xlabel('% survive')","9afd01e0":"CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\n                       'embark_town', 'alone']\nNUMERIC_COLUMNS = ['age', 'fare']\n\nfeature_columns = []\nfor feature_name in CATEGORICAL_COLUMNS:\n  vocabulary = dftrain[feature_name].unique()  # gets a list of all unique values from given feature column\n  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n\nfor feature_name in NUMERIC_COLUMNS:\n  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n\nprint(feature_columns)","eb624655":"#input Function\ndef make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n  def input_function():  # inner function, this will be returned\n    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  # create tf.data.Dataset object with data and its label\n    if shuffle:\n      ds = ds.shuffle(1000)  # randomize order of data\n    ds = ds.batch(batch_size).repeat(num_epochs)  # split dataset into batches of 32 and repeat process for number of epochs\n    return ds  # return a batch of the dataset\n  return input_function  # return a function object for use\n\ntrain_input_fn = make_input_fn(dftrain, y_train)  # here we will call the input_function that was returned to us to get a dataset object we can feed to the model\neval_input_fn = make_input_fn(dfeval, y_eval, num_epochs=1, shuffle=False)","7df9617c":"#Creating the Model\nlinear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)\n# We create a linear estimtor by passing the feature columns we created earlie","33ade5eb":"linear_est.train(train_input_fn)  # train\nresult = linear_est.evaluate(eval_input_fn)  # get model metrics\/stats by testing on tetsing data\n\nclear_output()  # clears consoke output\nprint('Accuracy: ',result['accuracy'])  # the result variable is simply a dict of stats about our model","0e27eca7":"pred_dicts = list(linear_est.predict(eval_input_fn))\nprobs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\n\nprobs.plot(kind='hist', bins=20, title='predicted probabilities')","20874334":"CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\nSPECIES = ['Setosa', 'Versicolor', 'Virginica']\n# Lets define some constants to help us later on","6838baf9":"train_path = tf.keras.utils.get_file(\n    \"iris_training.csv\", \"https:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/iris_training.csv\")\ntest_path = tf.keras.utils.get_file(\n    \"iris_test.csv\", \"https:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/iris_test.csv\")\n\ntrain = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\ntest = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n# Here we use keras (a module inside of TensorFlow) to grab our datasets and read them into a pandas dataframe","f7bc6fab":"train.head()","497f78a0":"train_y = train.pop('Species')\ntest_y = test.pop('Species')\ntrain.head() # the species column is now gone","aa8adeb6":"train.shape","f54e20e9":"def input_fn(features, labels, training=True, batch_size=256):\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n\n    # Shuffle and repeat if you are in training mode.\n    if training:\n        dataset = dataset.shuffle(1000).repeat()\n    \n    return dataset.batch(batch_size)","858206ee":"# Feature columns describe how to use the input.\nmy_feature_columns = []\nfor key in train.keys():\n    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\nprint(my_feature_columns)","f2fd329f":"# Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each.\nclassifier = tf.estimator.DNNClassifier(\n    feature_columns=my_feature_columns,\n    # Two hidden layers of 30 and 10 nodes respectively.\n    hidden_units=[30, 10],\n    # The model must choose between 3 classes.\n    n_classes=3)","1137b6c4":"classifier.train(\n    input_fn=lambda: input_fn(train, train_y, training=True),\n    steps=5000)\n# We include a lambda to avoid creating an inner function previously","921b2804":"eval_result = classifier.evaluate(\n    input_fn=lambda: input_fn(test, test_y, training=False))\n\nprint('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))","0dfef14f":"def input_fn(features, batch_size=256):\n    # Convert the inputs to a Dataset without labels.\n    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n\nfeatures = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\npredict = {}\n\nprint(\"Please type numeric values as prompted.\")\nfor feature in features:\n  valid = True\n  while valid: \n    val = input(feature + \": \")\n    if not val.isdigit(): valid = False\n\n  predict[feature] = [float(val)]\n\npredictions = classifier.predict(input_fn=lambda: input_fn(predict))\nfor pred_dict in predictions:\n    class_id = pred_dict['class_ids'][0]\n    probability = pred_dict['probabilities'][class_id]\n\n    print('Prediction is \"{}\" ({:.1f}%)'.format(\n        SPECIES[class_id], 100 * probability))","bb0d7416":"# Here is some example input and expected classes you can try above\nexpected = ['Setosa', 'Versicolor', 'Virginica']\npredict_x = {\n    'SepalLength': [5.1, 5.9, 6.9],\n    'SepalWidth': [3.3, 3.0, 3.1],\n    'PetalLength': [1.7, 4.2, 5.4],\n    'PetalWidth': [0.5, 1.5, 2.1],\n}\ndef input_fn(features, batch_size=256):\n    \"\"\"An input function for prediction.\"\"\"\n    # Convert the inputs to a Dataset without labels.\n    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n\npredictions = classifier.predict(\n    input_fn=lambda: input_fn(predict_x))","9bbe3983":"for pred_dict, expec in zip(predictions, expected):\n    class_id = pred_dict['class_ids'][0]\n    probability = pred_dict['probabilities'][class_id]\n\n    print('Prediction is \"{}\" ({:.1f}%), expected \"{}\"'.format(\n        SPECIES[class_id], 100 * probability, expec))","494aed49":"**Building the Model**\n\nAnd now we are ready to choose a model. For classification tasks there are variety of different estimators\/models that we can pick from. Some options are listed below.\n\n* DNNClassifier (Deep Neural Network)\n* LinearClassifier\n\nWe can choose either model but the DNN seems to be the best choice. This is because we may not be able to find a linear coorespondence in our data.\n\nSo let's build a model!","fa2d2946":"# **Clustering**\n\nBasic Algorithm for K-Means.\n* Step 1: Randomly pick K points to place K centroids\n* Step 2: Assign all the data points to the centroids by distance. The closest centroid to a point is the one it is assigned to.\n* Step 3: Average all the points belonging to each centroid to find the middle of those clusters (center of mass). Place the corresponding centroids into that position.\n* Step 4: Reassign every point once again to the closest centroid.\n* Step 5: Repeat steps 3-4 until no point changes which centroid it belongs to.","3ded3575":"# **Classification**\nNow that we've covered linear regression it is time to talk about classification. Where regression was used to predict a numeric value, classification is used to seperate data points into classes of different labels. In this example we will use a TensorFlow estimator to classify flowers.\n\nSince we've touched on how estimators work earlier, I'll go a bit quicker through this example.\n\nThis section is based on the following guide from the TensorFlow website. https:\/\/www.tensorflow.org\/tutorials\/estimator\/premade","020c1fdc":"**Feature Columns**\nIn our dataset we have two different kinds of information: Categorical and Numeric\n\nOur categorical data is anything that is not numeric! For example, the sex column does not use numbers, it uses the words \"male\" and \"female\".\n\nBefore we continue and create\/train a model we must convet our categorical data into numeric data. We can do this by encoding each category with an integer (ex. male = 1, female = 2).\n\nFortunately for us TensorFlow has some tools to help!","8e49d3a7":"**Predictions**\n\nNow that we have a trained model it's time to use it to make predictions. I've written a little script below that allows you to type the features of a flower and see a prediction for its class.","314fcb31":"**Feature Columns**\n\nAnd you didn't think we forgot about the feature columns, did you?","2ff74a6e":"What we've just done is created a deep neural network that has two hidden layers. These layers have 30 and 10 neurons respectively. This is the number of neurons the TensorFlow official tutorial uses so we'll stick with it. However, it is worth mentioning that the number of hidden neurons is an arbitrary number and many experiments and tests are usually done to determine the best choice for these values. Try playing around with the number of hidden neurons and see if your results change.","361e4b1c":"Now we can pop the species column off and use that as our label.","84cf14c1":"**Training**\n\nNow it's time to train the model!","c2ab3b96":"Training the model is as easy as passing the input functions that we created earlier.","adaa1717":"# **Linear Regression**\n\nhttps:\/\/www.tensorflow.org\/tutorials\/estimator\/linear","e20fb7f1":"**Dataset**\n\nThis specific dataset seperates flowers into 3 different classes of species.\n\n* Setosa\n* Versicolor\n* Virginica\n\nThe information about each flower is the following.\n\n* sepal length\n* sepal width\n* petal length\n* petal width","3f490302":"**Input Function**\n\nRemember that nasty input function we created earlier. Well we need to make another one here! Fortunatly for us this one is a little easier to digest.","4211fc68":"In this notebook we will walk through 4 fundemental machine learning algorithms. We will apply each of these algorithms to unique problems and datasets before highlighting the use cases of each.\n\nThe algorithms we will focus on include:\n\n* Linear Regression\n* Classification\n* Clustering\n* Hidden Markov Models\n\nIt is worth noting that there are many tools within TensorFlow that could be used to solve the problems we will see below. I have chosen the tools that I belive give the most variety and are easiest to use.","d0cf7318":"**Data**\n\nYou will use the Titanic dataset with the (rather morbid) goal of predicting passenger survival, given characteristics such as gender, age, class, etc.","d3ad6c92":"Let's have a look at our data.","062ca17f":"**Evaluation**\n\nNow let's see how this trained model does!","28818c36":"**The Training Process**","603c2e08":"**Imports**"}}