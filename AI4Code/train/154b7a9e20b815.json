{"cell_type":{"a4cbfcff":"code","04533f35":"code","3369fb5d":"code","f5eb5ba6":"code","d07f557e":"code","d78c47c5":"code","beb6eea3":"code","d8aa63e5":"code","eaa34fe8":"code","eae39696":"code","e92ca3bf":"code","ace0625c":"code","ded197b1":"code","ff2308be":"code","99e47b15":"code","976181a5":"code","47f9a8d5":"code","b1c4cb8c":"code","051dd8df":"code","56f6c6af":"code","fe3c9718":"code","ff4eb2cd":"code","05c88cb0":"code","f6447637":"code","a9a45e32":"code","b4fd15be":"code","d4ee0ef6":"code","492250b6":"code","66699ef7":"code","83501d3c":"code","4dfc40e7":"code","e5a672c0":"code","d56856f9":"code","bfc2f1ce":"code","78b47da5":"code","7ef66803":"code","d510a3b1":"code","08efc634":"code","71b59332":"code","038ca55b":"code","6461d6ae":"code","37d3baad":"code","00d2da69":"code","4367f622":"code","77c2ead8":"code","da167f3b":"code","bf56b8c9":"code","88eb814e":"code","bfc323bf":"code","59525ddf":"code","12913085":"code","9066565c":"code","db51d84c":"code","fcb8ee22":"code","477a48aa":"code","c89b5e92":"code","5e3dafba":"code","6bf5285e":"code","32326ed5":"code","4ab0d0b7":"code","cfab073c":"code","325c67fc":"code","fd945667":"code","f96a9308":"code","2efe5f71":"code","88eac760":"code","e7248017":"code","d93c3f21":"code","e150cc03":"code","733467f6":"code","40ad388f":"code","a0390e7d":"code","51191d9f":"code","cfccc29e":"code","191c4a8c":"code","8252a2eb":"code","c44c8b9d":"code","00deface":"code","3fb756a0":"markdown","5d3914c4":"markdown","3ae65dc3":"markdown","1350ecc3":"markdown","d45783b6":"markdown","25d97555":"markdown","662deac3":"markdown","a440a2f9":"markdown","a540f729":"markdown"},"source":{"a4cbfcff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","04533f35":"ptbdb_abnormal_data = pd.read_csv('\/kaggle\/input\/heartbeat\/ptbdb_abnormal.csv', header=None)\nptbdb_normal_data = pd.read_csv('\/kaggle\/input\/heartbeat\/ptbdb_normal.csv', header=None)","3369fb5d":"frames = [ptbdb_abnormal_data, ptbdb_normal_data]\n\nptb_data = pd.concat(frames,keys=[\"abnormal\",\"normal\"])","f5eb5ba6":"ptb_data.loc['normal']","d07f557e":"ptb_data.loc['abnormal']","d78c47c5":"msk = np.random.rand(len(ptb_data)) < 0.8\n\ntrain_ptb = ptb_data[msk]\n\ntest_ptb = ptb_data[~msk]","beb6eea3":"from keras.utils import to_categorical\nptb_train_X = train_ptb.loc[:, train_ptb.columns != 187]\nptb_train_y = train_ptb.loc[:, train_ptb.columns == 187]\nptb_train_y = to_categorical(ptb_train_y)\nptb_testX = test_ptb.loc[:, test_ptb.columns != 187]\nptb_testy = test_ptb.loc[:, test_ptb.columns == 187]\nptb_testy = to_categorical(ptb_testy)","d8aa63e5":"print(\"ptb_train_X shape=\" +str(ptb_train_X.shape))\nprint(\"ptb_train_y shape=\" +str(ptb_train_y.shape))\nprint(\"ptb_testX shape=\" +str(ptb_testX.shape))\nprint(\"ptb_testy shape=\" +str(ptb_testy.shape))","eaa34fe8":"from keras import layers\nfrom keras.layers import Input, Dense, Dropout, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Embedding, Add\nfrom keras.layers import Conv1D, GlobalAveragePooling1D, AveragePooling2D, MaxPooling2D, MaxPool1D, ZeroPadding1D, GlobalMaxPooling2D, GlobalAveragePooling2D, LSTM, SpatialDropout1D\nfrom keras.models import Sequential, Model\nfrom keras.preprocessing import image\nfrom keras.utils import plot_model\nfrom keras.applications.inception_v3 import InceptionV3\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nfrom keras.layers.merge import concatenate","eae39696":"ann_model = Sequential()\nann_model.add(Dense(50, activation='relu', input_shape=(187,)))\nann_model.add(Dense(50, activation='relu'))\nann_model.add(Dense(50, activation='relu'))\nann_model.add(Dense(50, activation='relu'))\nann_model.add(Dense(2, activation='softmax'))","e92ca3bf":"ann_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","ace0625c":"ann_model.summary()","ded197b1":"plot_model(ann_model)","ff2308be":"ann_model_history = ann_model.fit(ptb_train_X, ptb_train_y, epochs=100, batch_size = 100, validation_data = (ptb_testX, ptb_testy))","99e47b15":"plt.plot(ann_model_history.history['accuracy'])\nplt.plot(ann_model_history.history['val_accuracy'])\nplt.legend([\"accuracy\",\"val_accuracy\"])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","976181a5":"plt.plot(ann_model_history.history['loss'])\nplt.plot(ann_model_history.history['val_loss'])\nplt.legend([\"loss\",\"val_loss\"])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","47f9a8d5":"y_true=[]\nfor element in ptb_testy:\n    y_true.append(np.argmax(element))\nprediction_proba=ann_model.predict(ptb_testX)\nprediction=np.argmax(prediction_proba,axis=1)","b1c4cb8c":"ann_model_cf_matrix = confusion_matrix(y_true, prediction)\nsns.heatmap(ann_model_cf_matrix\/np.sum(ann_model_cf_matrix), annot=True,fmt='.3%', cmap='Blues')","051dd8df":"from sklearn.utils import shuffle\ntrain_ptb[187] = train_ptb[187].astype('float')\ntest_ptb[187] = test_ptb[187].astype('float')\nX_train = np.array(train_ptb.iloc[:, :187])\nX_test = np.array(test_ptb.iloc[:, :187])\ny_train = np.array(train_ptb[187])\ny_test = np.array(test_ptb[187])\nX_train, y_train = shuffle(X_train, y_train, random_state = 101)\nX_test, y_test = shuffle(X_test, y_test, random_state = 101)\nX_train = np.expand_dims(X_train, 2)\nX_test = np.expand_dims(X_test, 2)\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","56f6c6af":"lenet_5_model=Sequential()\n\nlenet_5_model.add(Conv1D(filters=6, kernel_size=3, padding='same', activation='relu', input_shape=(187,1)))\nlenet_5_model.add(BatchNormalization())\nlenet_5_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nlenet_5_model.add(Conv1D(filters=16, strides=1, kernel_size=5, activation='relu'))\nlenet_5_model.add(BatchNormalization())\nlenet_5_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nlenet_5_model.add(Flatten())\n\nlenet_5_model.add(Dense(64, activation='relu'))\n\nlenet_5_model.add(Dense(32, activation='relu'))\n\nlenet_5_model.add(Dense(2, activation = 'sigmoid'))\n","fe3c9718":"lenet_5_model.summary()","ff4eb2cd":"plot_model(lenet_5_model)","05c88cb0":"lenet_5_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","f6447637":"lenet_5_model_history = lenet_5_model.fit(X_train, y_train, epochs = 100, batch_size = 100, validation_data = (X_test, y_test))","a9a45e32":"plt.plot(lenet_5_model_history.history['accuracy'])\nplt.plot(lenet_5_model_history.history['val_accuracy'])\nplt.legend([\"accuracy\",\"val_accuracy\"])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","b4fd15be":"plt.plot(lenet_5_model_history.history['loss'])\nplt.plot(lenet_5_model_history.history['val_loss'])\nplt.legend([\"loss\",\"val_loss\"])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","d4ee0ef6":"y_true=[]\nfor element in y_test:\n    y_true.append(np.argmax(element))\nprediction_proba=lenet_5_model.predict(X_test)\nprediction=np.argmax(prediction_proba,axis=1)","492250b6":"lenet_5_model_cf_matrix = confusion_matrix(y_true, prediction)\nsns.heatmap(lenet_5_model_cf_matrix\/np.sum(lenet_5_model_cf_matrix), annot=True,fmt='.3%', cmap='Blues')","66699ef7":"alexNet_model=Sequential()\n\nalexNet_model.add(Conv1D(filters=96, activation='relu', kernel_size=11, strides=4, input_shape=(187,1)))\nalexNet_model.add(BatchNormalization())\nalexNet_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nalexNet_model.add(Conv1D(filters=256, kernel_size=5, padding='same', activation='relu'))\nalexNet_model.add(BatchNormalization())\nalexNet_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nalexNet_model.add(Conv1D(filters=384, padding='same', kernel_size=3, activation='relu'))\nalexNet_model.add(Conv1D(filters=384, kernel_size=3, activation='relu'))\nalexNet_model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\nalexNet_model.add(BatchNormalization())\nalexNet_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nalexNet_model.add(Flatten())\nalexNet_model.add(Dense(4096, activation='relu'))\nalexNet_model.add(Dropout(0.4))\nalexNet_model.add(Dense(4096, activation='relu'))\nalexNet_model.add(Dropout(0.4))\nalexNet_model.add(Dense(2, activation='softmax'))","83501d3c":"alexNet_model.summary()","4dfc40e7":"plot_model(alexNet_model)","e5a672c0":"alexNet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","d56856f9":"alexNet_model_history = alexNet_model.fit(X_train, y_train, epochs = 100, batch_size = 100, validation_data = (X_test, y_test))","bfc2f1ce":"plt.plot(alexNet_model_history.history['accuracy'])\nplt.plot(alexNet_model_history.history['val_accuracy'])\nplt.legend([\"accuracy\",\"val_accuracy\"])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","78b47da5":"plt.plot(alexNet_model_history.history['loss'])\nplt.plot(alexNet_model_history.history['val_loss'])\nplt.legend([\"loss\",\"val_loss\"])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","7ef66803":"y_true=[]\nfor element in y_test:\n    y_true.append(np.argmax(element))\nprediction_proba=alexNet_model.predict(X_test)\nprediction=np.argmax(prediction_proba,axis=1)","d510a3b1":"alexNet_model_cf_matrix = confusion_matrix(y_true, prediction)\nsns.heatmap(alexNet_model_cf_matrix\/np.sum(alexNet_model_cf_matrix), annot=True,fmt='.3%', cmap='Blues')","08efc634":"vgg_16_model=Sequential()\n\nvgg_16_model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu',  input_shape=(187,1)))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nvgg_16_model.add(Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nvgg_16_model.add(Conv1D(filters=256, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Conv1D(filters=256, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Conv1D(filters=256, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nvgg_16_model.add(Conv1D(filters=512, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Conv1D(filters=512, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Conv1D(filters=512, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nvgg_16_model.add(Conv1D(filters=512, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Conv1D(filters=512, kernel_size=1, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Conv1D(filters=512, kernel_size=1, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nvgg_16_model.add(Flatten())\nvgg_16_model.add(Dense(4096, activation='relu'))\nvgg_16_model.add(Dropout(0.4))\nvgg_16_model.add(Dense(4096, activation='relu'))\nvgg_16_model.add(Dropout(0.4))\nvgg_16_model.add(Dense(2, activation='softmax'))","71b59332":"vgg_16_model.summary()","038ca55b":"plot_model(vgg_16_model)","6461d6ae":"vgg_16_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","37d3baad":"vgg_16_model_history = vgg_16_model.fit(X_train, y_train, epochs = 100, batch_size = 100, validation_data = (X_test, y_test))","00d2da69":"plt.plot(vgg_16_model_history.history['accuracy'])\nplt.plot(vgg_16_model_history.history['val_accuracy'])\nplt.legend([\"accuracy\",\"val_accuracy\"])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","4367f622":"plt.plot(vgg_16_model_history.history['loss'])\nplt.plot(vgg_16_model_history.history['val_loss'])\nplt.legend([\"loss\",\"val_loss\"])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","77c2ead8":"y_true=[]\nfor element in y_test:\n    y_true.append(np.argmax(element))\nprediction_proba=vgg_16_model.predict(X_test)\nprediction=np.argmax(prediction_proba,axis=1)","da167f3b":"vgg_16_model_cf_matrix = confusion_matrix(y_true, prediction)\nsns.heatmap(vgg_16_model_cf_matrix\/np.sum(vgg_16_model_cf_matrix), annot=True,fmt='.3%', cmap='Blues')","bf56b8c9":"def identity_block(X, f, filters):\n    F1, F2, F3 = filters\n    \n    X_shortcut = X\n    \n    X = Conv1D(filters = F1, kernel_size = 1, activation='relu', strides = 1, padding = 'valid')(X)\n    X = BatchNormalization()(X)\n    \n    X = Conv1D(filters = F2, kernel_size = f, activation='relu', strides = 1, padding = 'same')(X)\n    X = BatchNormalization()(X)\n\n    X = Conv1D(filters = F3, kernel_size = 1, activation='relu', strides = 1, padding = 'valid')(X)\n    X = BatchNormalization()(X)\n\n    X = Add()([X,X_shortcut])\n    X = Activation('relu')(X)\n    \n    return X","88eb814e":"def convolutional_block(X, f, filters, s = 2):\n    F1, F2, F3 = filters\n    \n    X_shortcut = X\n\n    X = Conv1D(F1, 1, activation='relu', strides = s)(X)\n    X = BatchNormalization()(X)\n    \n    X = Conv1D(F2, f, activation='relu', strides = 1,padding = 'same')(X)\n    X = BatchNormalization()(X)\n\n    X = Conv1D(F3, 1, strides = 1)(X)\n    X = BatchNormalization()(X)\n\n    X_shortcut = Conv1D(F3, 1, strides = s)(X_shortcut)\n    X_shortcut = BatchNormalization()(X_shortcut)\n    \n    X = Add()([X,X_shortcut])\n    X = Activation('relu')(X)\n    \n    return X","bfc323bf":"def ResNet50(input_shape = (187,1)):\n    \n    X_input = Input(input_shape)\n\n    X = ZeroPadding1D(3)(X_input)\n    \n    X = Conv1D(64, 7, activation='relu', strides = 2)(X)\n    X = BatchNormalization()(X)\n    X = MaxPool1D(pool_size=2, strides=2, padding='same')(X)\n\n    X = convolutional_block(X, f = 3, filters = [64, 64, 256], s = 1)\n    X = identity_block(X, 3, [64, 64, 256])\n    X = identity_block(X, 3, [64, 64, 256])\n\n    X = convolutional_block(X, f = 3, filters = [128,128,512], s = 2)\n    X = identity_block(X, 3, [128,128,512])\n    X = identity_block(X, 3, [128,128,512])\n    X = identity_block(X, 3, [128,128,512])\n\n    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], s = 2)\n    X = identity_block(X, 3, [256, 256, 1024])\n    X = identity_block(X, 3, [256, 256, 1024])\n    X = identity_block(X, 3, [256, 256, 1024])\n    X = identity_block(X, 3, [256, 256, 1024])\n    X = identity_block(X, 3, [256, 256, 1024])\n\n    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], s = 2)\n    X = identity_block(X, 3, [512, 512, 2048])\n    X = identity_block(X, 3, [512, 512, 2048])\n\n    X = MaxPool1D(pool_size=2, strides=2, padding='same')(X)\n    \n    X = Flatten()(X)\n    X = Dense(2,activation='softmax')(X)\n    \n    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n\n    return model","59525ddf":"resNet50_model = ResNet50(input_shape = (187,1))","12913085":"resNet50_model.summary()","9066565c":"plot_model(resNet50_model)","db51d84c":"resNet50_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","fcb8ee22":"resNet50_model_history = resNet50_model.fit(X_train, y_train, epochs = 100, batch_size = 100, validation_data = (X_test, y_test))","477a48aa":"plt.plot(resNet50_model_history.history['accuracy'])\nplt.plot(resNet50_model_history.history['val_accuracy'])\nplt.legend([\"accuracy\",\"val_accuracy\"])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","c89b5e92":"plt.plot(resNet50_model_history.history['loss'])\nplt.plot(resNet50_model_history.history['val_loss'])\nplt.legend([\"loss\",\"val_loss\"])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","5e3dafba":"y_true=[]\nfor element in y_test:\n    y_true.append(np.argmax(element))\nprediction_proba=resNet50_model.predict(X_test)\nprediction=np.argmax(prediction_proba,axis=1)","6bf5285e":"resNet50_model_cf_matrix = confusion_matrix(y_true, prediction)\nsns.heatmap(resNet50_model_cf_matrix\/np.sum(resNet50_model_cf_matrix), annot=True,fmt='.3%', cmap='Blues')","32326ed5":"def inception_block(prev_layer):\n    \n    conv1=Conv1D(filters = 64, kernel_size = 1, activation='relu', padding = 'same')(prev_layer)\n    \n    conv3=Conv1D(filters = 64, kernel_size = 1, activation='relu', padding = 'same')(prev_layer)\n    conv3=Conv1D(filters = 64, kernel_size = 3, activation='relu', padding = 'same')(conv3)\n    \n    conv5=Conv1D(filters = 64, kernel_size = 1, activation='relu', padding = 'same')(prev_layer)\n    conv5=Conv1D(filters = 64, kernel_size = 5, activation='relu', padding = 'same')(conv5)\n    \n    pool= MaxPool1D(pool_size=3, strides=1, padding='same')(prev_layer)\n    convmax=Conv1D(filters = 64, kernel_size = 1, activation='relu', padding = 'same')(pool)\n    \n    layer_out = concatenate([conv1, conv3, conv5, convmax], axis=1)\n    return layer_out","4ab0d0b7":"def inception_model(input_shape):\n    X_input=Input(input_shape)\n    \n    X = Conv1D(filters = 64, kernel_size = 7, activation='relu', padding = 'same')(X_input)\n    X = MaxPool1D(pool_size=3, strides=2, padding='same')(X)\n    \n    X = Conv1D(filters = 64, kernel_size = 1, activation='relu', padding = 'same')(X)\n    \n    X = inception_block(X)\n    X = inception_block(X)\n    X = inception_block(X)\n    X = inception_block(X)\n    \n    X = MaxPool1D(pool_size=7, strides=2, padding='same')(X)\n    \n    X = Flatten()(X)\n    X = Dense(2,activation='softmax')(X)\n    \n    model = Model(inputs = X_input, outputs = X, name='Inception')\n    \n    return model","cfab073c":"inception_model = inception_model(input_shape = (187,1))","325c67fc":"inception_model.summary()","fd945667":"plot_model(inception_model)","f96a9308":"inception_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","2efe5f71":"inception_model_history = inception_model.fit(X_train, y_train, epochs = 100, batch_size = 100, validation_data = (X_test, y_test))","88eac760":"plt.plot(inception_model_history.history['accuracy'])\nplt.plot(inception_model_history.history['val_accuracy'])\nplt.legend([\"accuracy\",\"val_accuracy\"])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","e7248017":"plt.plot(inception_model_history.history['loss'])\nplt.plot(inception_model_history.history['val_loss'])\nplt.legend([\"loss\",\"val_loss\"])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","d93c3f21":"y_true=[]\nfor element in y_test:\n    y_true.append(np.argmax(element))\nprediction_proba=inception_model.predict(X_test)\nprediction=np.argmax(prediction_proba,axis=1)","e150cc03":"inception_model_cf_matrix = confusion_matrix(y_true, prediction)\nsns.heatmap(inception_model_cf_matrix\/np.sum(inception_model_cf_matrix), annot=True,fmt='.3%', cmap='Blues')","733467f6":"lstm_model = Sequential()\nlstm_model.add(LSTM(64, input_shape=(187,1)))\nlstm_model.add(Dense(128, activation = 'relu'))\nlstm_model.add(Dropout(0.3))\nlstm_model.add(Dense(2, activation = 'softmax'))","40ad388f":"lstm_model.summary()","a0390e7d":"plot_model(lstm_model)","51191d9f":"lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","cfccc29e":"lstm_model_history = lstm_model.fit(X_train, y_train, epochs = 100, batch_size = 100, validation_data = (X_test, y_test))","191c4a8c":"plt.plot(lstm_model_history.history['accuracy'])\nplt.plot(lstm_model_history.history['val_accuracy'])\nplt.legend([\"accuracy\",\"val_accuracy\"])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","8252a2eb":"plt.plot(lstm_model_history.history['loss'])\nplt.plot(lstm_model_history.history['val_loss'])\nplt.legend([\"loss\",\"val_loss\"])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","c44c8b9d":"y_true=[]\nfor element in y_test:\n    y_true.append(np.argmax(element))\nprediction_proba=lstm_model.predict(X_test)\nprediction=np.argmax(prediction_proba,axis=1)","00deface":"lstm_model_cf_matrix = confusion_matrix(y_true, prediction)\nsns.heatmap(lstm_model_cf_matrix\/np.sum(lstm_model_cf_matrix), annot=True,fmt='.3%', cmap='Blues')","3fb756a0":"# VGG-16","5d3914c4":"# RNN","3ae65dc3":"# Simple ANN","1350ecc3":"# LENET-5","d45783b6":"# CNN","25d97555":"# AlexNet","662deac3":"# LSTM","a440a2f9":"# Inception","a540f729":"# ResNet50"}}