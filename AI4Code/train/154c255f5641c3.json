{"cell_type":{"9ab32876":"code","b549a93a":"code","05d9b884":"code","deca3985":"code","89d2706c":"code","f1835f41":"code","bcfeabb2":"code","c74f36c4":"code","200b0623":"code","162eae96":"code","d3555a65":"code","8b115470":"code","89e78b2b":"code","177a8df4":"code","e111deb8":"code","4d1ee21b":"code","c104ff4a":"code","a0650263":"code","78803210":"code","54b02347":"code","fe5e812d":"code","598e6b6d":"code","897b68cb":"markdown","646ce5a9":"markdown","786d6e41":"markdown","009a88a9":"markdown","fa3011f4":"markdown","a5e21398":"markdown","3ae42df6":"markdown","8df8b2df":"markdown"},"source":{"9ab32876":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b549a93a":"# train and test csv file \ntrain_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntrain_df.head(3)","05d9b884":"print('Length of Data')\nprint('Training data',len(train_df))\nprint('Testing data',len(test_df))","deca3985":"# utility --  check null values\n# print the column-name , dtype and number of missing vals\ndef check_null_values(df):\n    s = df.isna().sum()\n    cols, na_vals = s.index, s.values\n    missing_cols=[]\n    for c, n in zip(cols, na_vals):\n        if n>0:\n            print(f'{c:15}  {train_df[c].dtype !s:10} {n :5} missing values ({n\/len(df) :.2f}% missing)')\n            missing_cols.append({'column':c, 'missing':n, 'missing_percentage':n\/len(df)})\n    return missing_cols","89d2706c":"# check for null values in training data\ntrain_missing_cols = check_null_values(train_df)","f1835f41":"# check for null values in testing data\ntest_missing_cols = check_null_values(test_df)","bcfeabb2":"set1 = set([obj['column'] for obj in train_missing_cols])\nset2 = set([obj['column'] for obj in test_missing_cols])\n\nintersection = set1.intersection(set2)\nprint('Common Missing values \\n')\nintersection","c74f36c4":"# explore the training data\n# check number of unique vals\n# check dtype\n# check precentage of missing\nu = train_df.nunique()\nm = train_df.isna().sum()\n\nprint('Total number of records',len(train_df))\nfor cols, unique, missing in zip(u.index, u.values, m.values):\n    print(f'{cols :15}{unique !s:5}unique {train_df[cols].dtype !s:10} ({missing\/len(train_df) :.3f}% missing)')\n","200b0623":"# Category columns\ndef get_cat_cols(type='O', threshold=15):\n    cat = []\n\n    # Feature Selection\n    for col in train_df.columns:\n        if train_df[col].dtype == type:\n            if train_df[col].nunique()<=threshold:\n                cat.append(col)\n    return cat","162eae96":"# Numerical columns  (int64\/float64)\ndef get_numerical_cols(threshold=15):\n    num_cols =[]\n    num_df = train_df.select_dtypes(exclude='O')\n    for num_col in num_df.columns:\n        if num_col != 'SalePrice':\n            if train_df[num_col].nunique() > 15:\n                num_cols.append(num_col)\n    return num_cols","d3555a65":"# if a column has many missing fields then it has no use \n# eg 'MiscFeature' has 97% missing\nfield_missing = train_df['MiscFeature'].isna().sum()\nprint(f'Missing field in MiscFeature column {field_missing} ({field_missing\/len(train_df):.2f}%)')","8b115470":"def feature_selection_util(type='O', cat_threshold=15, missing_threshold=100, is_cat=True):\n    features=[]\n    columns = get_cat_cols(type=type, threshold=cat_threshold) if is_cat else get_numerical_cols(threshold=cat_threshold)\n    for feature in columns:\n        if train_df[feature].isna().sum() <100:\n            features.append(feature)\n    return features","89e78b2b":"# Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\n# cross validation\nfrom sklearn.model_selection import cross_val_score\n\n# Pipeline\nfrom sklearn.pipeline import Pipeline\n\n# column transformer\nfrom sklearn.compose import ColumnTransformer\n\n# Models\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\n","177a8df4":"# string category \nstr_cat_features = feature_selection_util()\nstr_cat_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),\n                                      ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n\n# integer category \nint_cat_features = feature_selection_util(type='int64')\nint_cat_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),\n                                      ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n\n# numerical columns \nnumerical_features = feature_selection_util(is_cat=False)\nnumerical_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),\n                                        ('scale',StandardScaler())])\n\n\n# transformation\npreprocessing = ColumnTransformer(transformers=[('str_cat',str_cat_transformer,str_cat_features),\n                                                ('int_cat',int_cat_transformer, int_cat_features),\n                                                ('num_col', numerical_transformer, numerical_features)])\n\n\n# pipeline combining preprocessing step and modeling\npipe = Pipeline(steps=[('preprocessing', preprocessing),\n                       ('model', RandomForestRegressor(random_state=42))])","e111deb8":"# training features and lables\nX = train_df.drop(columns=['SalePrice'])\ny = train_df['SalePrice'] \n# cross validation score\ncv_score = cross_val_score(pipe, X, y, scoring='neg_root_mean_squared_error')","4d1ee21b":"print('Cross validation score\\n',cv_score)\nprint('Mean cross validation score',cv_score.mean())","c104ff4a":"def build_model_pipeline(model):\n    # string category \n    str_cat_features = feature_selection_util()\n    str_cat_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),\n                                          ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n\n    # integer category \n    int_cat_features = feature_selection_util(type='int64')\n    int_cat_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),\n                                          ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n\n    # numerical columns \n    numerical_features = feature_selection_util(is_cat=False)\n    numerical_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),\n                                            ('scale',StandardScaler())])\n\n\n    # transformation\n    preprocessing = ColumnTransformer(transformers=[('str_cat',str_cat_transformer,str_cat_features),\n                                                    ('int_cat',int_cat_transformer, int_cat_features),\n                                                    ('num_col', numerical_transformer, numerical_features)])\n\n\n    # pipeline combining preprocessing step and modeling\n    pipe = Pipeline(steps=[('preprocessing', preprocessing),\n                           ('model', model)])\n    \n    return pipe\n\ndef cross_validate(model,scoring='neg_root_mean_squared_error'):\n    # training features and lables\n    X = train_df.drop(columns=['SalePrice'])\n    y = train_df['SalePrice'] \n    # cross validation score\n    cv_score = cross_val_score(model, X, y, scoring=scoring)\n    return cv_score.mean()","a0650263":"model_list = [('RandomForestRegressor',RandomForestRegressor(random_state=42)),\n              ('Lasso',Lasso(tol=1e-2)),\n              ('Ridge',Ridge()),\n              ('ElasticNet',ElasticNet()),\n              ('XGBRegressor',XGBRegressor())\n             ]\n\nmodel_mean_rmse = {}\nfor name, model in model_list:\n    pipeline = build_model_pipeline(model)\n    mean_cv_score = cross_validate(pipeline)\n    model_mean_rmse[name]=mean_cv_score","78803210":"for k,v in model_mean_rmse.items():\n    print(f'{k :25} RMSE {-v}')","54b02347":"xgb_pipe = build_model_pipeline(XGBRegressor(n_estimators=350, learning_rate=0.05, max_depth=5, subsample = 0.7, colsample_bytree = 0.5))\ncv_mean_score = cross_validate(xgb_pipe)\nprint('cv mean score',cv_mean_score)","fe5e812d":"final_model = XGBRegressor(n_estimators=350, learning_rate=0.05, max_depth=5, subsample = 0.7, colsample_bytree = 0.5)\nmodel_pipe = build_model_pipeline(final_model)\n# training features and lables\nX = train_df.drop(columns=['SalePrice'])\ny = train_df['SalePrice'] \n# train the model\nmodel_pipe.fit(X,y)\n# make predictions\npredictions = model_pipe.predict(test_df)","598e6b6d":"output = pd.DataFrame({'Id':test_df['Id'], 'SalePrice':predictions})\noutput = output.to_csv('submission.csv',index=False)\nprint('Submission saved successfully')","897b68cb":"## Make predicitons on test set","646ce5a9":"## Preprocessing \n\n* Integer category \n> Impute missing values then Onehot encoding\n\n* String category\n> Impute missing values then Onehot encoding\n\n* Numerical columns\n> Impute missing values then Scale (0 mean unit variance)\n","786d6e41":"### Utility for selecting Category columns","009a88a9":"### Create a utility for training and comparing  different models","fa3011f4":"Utility to for dropping columns that has too many missing feilds","a5e21398":"## Model Pipeline","3ae42df6":"## Explore the dataframe","8df8b2df":"## Model Improvement"}}