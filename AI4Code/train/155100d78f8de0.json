{"cell_type":{"a2f07b03":"code","92f045b1":"code","72992545":"code","d348ba47":"code","1e7d215f":"code","d88d81bb":"code","01a6149f":"code","ddbef9c1":"code","52e0c604":"code","394c5366":"code","4ca6b871":"code","ce3f95a3":"code","76465697":"code","3b425baa":"code","dcce45f4":"code","7a7520b0":"code","bbaf1ba9":"code","c21b8224":"markdown","6f99eb82":"markdown","0d5290fa":"markdown","90f13df7":"markdown","b2497b1e":"markdown","4558f382":"markdown","31996014":"markdown","769a1f6e":"markdown","08ab7cf5":"markdown","adf18ada":"markdown","1b37d344":"markdown","96c3a063":"markdown","da9f5e4e":"markdown","072bdd16":"markdown","599dd038":"markdown"},"source":{"a2f07b03":"import os\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Conv2D,MaxPool2D,Flatten,Dense\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils import shuffle","92f045b1":"img=150\nepochs=30\nbatch_size=25\nnames=['O','R']\nencode_name={name:i for i,name in enumerate(names)}","72992545":"def load_data():\n    datasets=['\/kaggle\/input\/waste-classification-data\/DATASET\/TRAIN','\/kaggle\/input\/waste-classification-data\/DATASET\/TEST']\n    output=[]\n    for dataset in tqdm(datasets):\n        images=[]\n        labels=[]\n        for folder in os.listdir(dataset):\n            label=encode_name[folder]\n            if dataset=='\/kaggle\/input\/waste-classification-data\/DATASET\/TRAIN':\n                img_set=random.sample(os.listdir(os.path.join(dataset,folder)),7000)\n            else:\n                img_set=random.sample(os.listdir(os.path.join(dataset,folder)),1000)\n            for file in img_set:\n                img_path=os.path.join(os.path.join(dataset,folder),file)\n                image=cv2.imread(img_path)\n                image=cv2.resize(image,(img,img))\n                images.append(image)\n                labels.append(label)\n        images=np.array(images,dtype=np.float32)\n        labels=np.array(labels,dtype=np.int32)\n        output.append((images,labels))\n    return output","d348ba47":"(train_images,train_labels),(test_images,test_labels)=load_data()","1e7d215f":"train_images=np.array(train_images)\ntrain_labels=np.array(train_labels)","d88d81bb":"test_images=np.array(test_images)\ntest_labels=np.array(test_labels)","01a6149f":"print(f'Training images size:{train_images.shape}, Training labels size:{train_labels.shape}')\nprint(f'Testing images size:{test_images.shape}, Testing labels size:{test_labels.shape}')","ddbef9c1":"train_images,test_images=train_images\/255,test_images\/255","52e0c604":"train_images,train_labels=shuffle(train_images,train_labels)","394c5366":"model=Sequential([\n    Conv2D(filters=32,activation='relu',input_shape=(img,img,3),padding='same',kernel_size=(3,3)),\n    Conv2D(filters=32,activation='relu',padding='same',kernel_size=(3,3)),\n    MaxPool2D(pool_size=(2,2)),\n    Conv2D(filters=64,activation='relu',padding='same',kernel_size=(3,3)),\n    Conv2D(filters=64,activation='relu',padding='same',kernel_size=(3,3)),\n    MaxPool2D(pool_size=(2,2)),\n    Conv2D(filters=128,activation='relu',padding='same',kernel_size=(3,3)),\n    Conv2D(filters=128,activation='relu',padding='same',kernel_size=(3,3)),\n    MaxPool2D(pool_size=(2,2)),\n    Conv2D(filters=256,activation='relu',padding='same',kernel_size=(3,3)),\n    Conv2D(filters=256,activation='relu',padding='same',kernel_size=(3,3)),\n    MaxPool2D(pool_size=(2,2)),\n    Conv2D(filters=256,activation='relu',padding='same',kernel_size=(3,3)),\n    Conv2D(filters=256,activation='relu',padding='same',kernel_size=(3,3)),\n    MaxPool2D(pool_size=(2,2)),\n    Conv2D(filters=512,activation='relu',padding='same',kernel_size=(3,3)),\n    Conv2D(filters=512,activation='relu',padding='same',kernel_size=(3,3)),\n    MaxPool2D(pool_size=(2,2)),\n    Conv2D(filters=512,activation='relu',padding='same',kernel_size=(3,3)),\n    Conv2D(filters=512,activation='relu',padding='same',kernel_size=(3,3)),\n    Flatten(),\n    Dense(units=4096,activation='relu'),\n    Dense(units=4096,activation='relu'),\n    Dense(units=1,activation='sigmoid'),\n])","4ca6b871":"model.summary()","ce3f95a3":"model.save('model.h5')","76465697":"model.compile(optimizer=Adam(lr=0.0001),loss='binary_crossentropy',metrics=['accuracy'])","3b425baa":"model.load_weights('model.h5')","dcce45f4":"model.fit(x=train_images,y=train_labels,validation_split=0.3,epochs=epochs,batch_size=batch_size,steps_per_epoch=100,verbose=2)","7a7520b0":"p=model.predict(test_images)","bbaf1ba9":"cm=confusion_matrix(y_true=test_labels,y_pred=np.round(p))\nprint(f'Accuracy:{(cm.trace()\/cm.sum())*100}')","c21b8224":"Importing the relevant files","6f99eb82":"Loading the training and testing data","0d5290fa":"Using the model to make predictions on the testing data","90f13df7":"Printing the size of the images and labels","b2497b1e":"Saving the weights of the model","4558f382":"Printing the model summary","31996014":"Making a function to load the images and labels. I have used openCV for this purpose","769a1f6e":"Scaling the values of the images pixels to 0-1 to make the computation easier for our model","08ab7cf5":"Setting the image size, epochs,classes and batch size","adf18ada":"Converting the training and testing images and labels to numpy arrays","1b37d344":"Randomizing the training data","96c3a063":"Using the confusion matrix to print the accuracy of those predictions","da9f5e4e":"Making the model architecture","072bdd16":"Training the model on the training data","599dd038":"Compiling the model. Specifying the optimizer to act on the data. The loss function(which could also have been sparse_categorical_crossentropy),since there are only two classes, I have used binary_crossentropy"}}