{"cell_type":{"ada5ae20":"code","51311eee":"code","23539318":"code","8934215a":"code","7ee11c94":"code","7a656934":"code","0f56f094":"code","b8d35352":"code","197f79fb":"code","96328730":"markdown","ae875460":"markdown","8e8aed6b":"markdown","6cea9a83":"markdown"},"source":{"ada5ae20":"import numpy as np\nfrom numpy import array\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing.text import Tokenizer","51311eee":"# define documents\ndocs = ['Well done!',\n\t\t'Good work',\n\t\t'Great effort',\n\t\t'nice work',\n\t\t'Excellent!',\n\t\t'Weak',\n\t\t'Poor effort!',\n\t\t'not good',\n\t\t'poor work',\n\t\t'Could have done better.']\n# define class labels\nlabels = np.array([1,1,1,1,1,0,0,0,0,0])","23539318":"# integer encode the documents\nvocab_size = 50\nencoded_docs = [one_hot(d, vocab_size) for d in docs]\nprint(encoded_docs)\n# pad documents to a max length of 4 words\nmax_length = 6\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)","8934215a":"# define the model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 8, input_length=max_length))\nmodel.add(Flatten())\nmodel.add(Dense(2, activation='softmax'))\n# compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n# summarize the model\nprint(model.summary())\n# fit the model\nmodel.fit(padded_docs, labels, epochs=100, verbose=0)\n# evaluate the model\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\nprint('Accuracy: %f' % (accuracy*100))","7ee11c94":"model.predict(padded_docs)","7a656934":"labels","0f56f094":"t = Tokenizer()\nt.fit_on_texts(docs)\nvocab_size = len(t.word_index) + 1\n# integer encode the documents\nencoded_docs = t.texts_to_sequences(docs)\nprint(encoded_docs)\n# pad documents to a max length of 4 words\nmax_length = 4\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)","b8d35352":"t.word_docs, t.word_index","197f79fb":"# define the model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 8, input_length=max_length))\nmodel.add(Flatten())\nmodel.add(Dense(2, activation='softmax'))\n# compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n# summarize the model\nprint(model.summary())\n# fit the model\nmodel.fit(padded_docs, labels, epochs=100, verbose=0)\n# evaluate the model\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\nprint('Accuracy: %f' % (accuracy*100))","96328730":"# Processing","ae875460":"# Imports","8e8aed6b":"The nb is based on the wonderful blog, \nhttps:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/","6cea9a83":"# Tokenizer"}}