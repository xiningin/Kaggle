{"cell_type":{"c427dbab":"code","899f5763":"code","88f54675":"code","8d35a125":"code","c9fb401a":"code","bea8f962":"code","0ac59b89":"code","af5867ae":"code","7cb82148":"code","6a4043d9":"code","74d563bd":"code","ae594de9":"code","13fe263e":"code","f29b57ae":"markdown","c22d303d":"markdown","31866470":"markdown","aadb2d2c":"markdown","fc10f488":"markdown","0dbeb381":"markdown","8598587d":"markdown","d9f1a943":"markdown","5efc51f1":"markdown","1ce438b3":"markdown"},"source":{"c427dbab":"import os\nimport pandas as pd\nimport random\nimport numpy as np\n\n# tqdm: utility for progession bar\nfrom tqdm.notebook import tqdm\n\n# pytorch: helps us make model\nimport torch\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom torch.utils.data import TensorDataset\n\n# ml_things: help to genrate predicitve report\n!pip install git+https:\/\/github.com\/gmihaila\/ml_things\nfrom ml_things import plot_confusion_matrix\n\n# sklearn: help to genrate predicitve report\nfrom sklearn.metrics import classification_report, f1_score\nfrom sklearn.model_selection import train_test_split\n\n# huggingface: Stores pre-trained models\nfrom transformers import BertTokenizer, BertForSequenceClassification, set_seed\nfrom transformers import pipeline, AdamW, get_linear_schedule_with_warmup\n\n# wandb: kind of a VCS for reports\nimport wandb\n\n# json: helps pasring JSON\nimport json","899f5763":"# init wandb project\nwandb.init(project=\"stumbleupon\")\n\n# logs config\nconfig = wandb.config\n\n# Lets set a seed\n## Ensures easy reproducibility\nconfig.seed = seed = 69\nset_seed(config.seed)\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\n# Number of epochs to train for\nconfig.epoch = epoch = 3 \n\n# Batch size for training\nconfig.batch = batch = 8\n\n# learniing rate\nconfig.lr = lr = 1e-5\n\n# scheduler warm steps\nconfig.warup_steps = warup_steps = 0\n\n# Each model has number of words they can process\n# Setting sequence length\nconfig.maxLength = maxLength = 512\n\n# This chooses GPU for training if it's available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Model which we choose from huggingface\/tranformers library\nconfig.modelName = modelName = 'bert-base-uncased'\n","88f54675":"df = pd.read_table('\/kaggle\/input\/stumbleupon\/train.tsv')\ndfTest = pd.read_table('\/kaggle\/input\/stumbleupon\/test.tsv')\ndfSub = pd.read_csv('\/kaggle\/input\/stumbleupon\/sampleSubmission.csv')","8d35a125":"def boilerplatePreproc(data):\n    '''\n        This function helps us preproc the data stored in the boilerplate column. This function only takes 'body' of the article in account as\n        article body contains the context needed to classify article as evergreen or ephemeral. Furthermore, we filter out last 512 words from the\n        body as they are generally concluding statement hence has article summary.\n    '''\n    jsonData = json.loads(data)\n    \n    # if JSON has no body, we will substitute the body with title\n    try:\n        x = jsonData['body']\n        x = ' '.join(x.split(' ')[-400:])\n    except:\n        x = jsonData['title']\n        \n    return x\n\n# applies above func and stores result in a new column\ndf['body'] = df.boilerplate.map(boilerplatePreproc)\ndf = df.dropna()\ndfTest['body'] = dfTest.boilerplate.map(boilerplatePreproc)","c9fb401a":"df.label.value_counts()","bea8f962":"# split dataset into train and val for validating model performance\nx_train, x_val, y_train, y_val = train_test_split(df.body.values, df.label.values, test_size=0.15, random_state=seed, stratify=df.label.values)\n\n# init tokenizer\ntokenizer = BertTokenizer.from_pretrained(modelName, do_lower_case=True)\n\n# get embedings\nencoded_data_train = tokenizer.batch_encode_plus(\n    list(x_train), \n    add_special_tokens=True, \n    return_attention_mask=True, \n    #pad_to_max_length=True, \n    padding='max_length',\n    max_length=maxLength, \n    return_tensors='pt',\n    truncation=True\n)\n\nencoded_data_val = tokenizer.batch_encode_plus(\n    list(x_val), \n    add_special_tokens=True, \n    return_attention_mask=True, \n    #pad_to_max_length=True,\n    padding='max_length', \n    max_length=maxLength, \n    return_tensors='pt',\n    truncation=True\n)\n\ninput_ids_train = encoded_data_train['input_ids']\nattention_masks_train = encoded_data_train['attention_mask']\nlabels_train = torch.tensor(list(y_train))#.float()\n\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nlabels_val = torch.tensor(list(y_val))#.float()\n\n# making dataset\ndataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ndataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n\n# making dataloader\ndataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch)\ndataloader_val = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch)","0ac59b89":"# helper that init model obj\ndef initHelper():\n    # get pretrained model\n    # model = BertForSequenceClassification.from_pretrained(modelName, num_labels=2, output_attentions=False, output_hidden_states=False)\n    model = BertForSequenceClassification.from_pretrained(modelName)\n    model = model.to(device)\n\n    # get optimizer\n    optimizer = AdamW(model.parameters(), lr=lr, eps=2e-8)\n\n    # init scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warup_steps, num_training_steps=len(dataloader_train) * epoch)\n    \n    return model, optimizer, scheduler","af5867ae":"# helper that validates the model\ndef evaluate():\n\n    model.eval()\n    \n    loss_val_total = 0\n    predictions, true_vals = [], []\n    \n    for batch in dataloader_val:\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {\n            'input_ids':      batch[0],\n            'attention_mask': batch[1],\n            'labels':         batch[2],\n        }\n\n        with torch.no_grad():        \n            outputs = model(**inputs)\n            \n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n    \n    loss_val_avg = loss_val_total\/len(dataloader_val) \n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n            \n    return loss_val_avg, predictions, true_vals","7cb82148":"# Helper that calcs f1 score\ndef f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average='weighted')\n\n# Helper that trains the model\ndef trainHelper(epoch):\n    \n    loss_train_total = 0\n    \n    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n    for batch in progress_bar:\n        # zero previous step's grad\n        model.zero_grad()\n        batch = tuple(b.to(device) for b in batch)\n        inputs = {\n            'input_ids':      batch[0],\n            'attention_mask': batch[1],\n            'labels':         batch[2],\n        }       \n\n        outputs = model(**inputs)\n        \n        # calc loss\n        loss = outputs[0]\n        loss_train_total += loss.item()\n        loss.float().backward()\n\n        # clips from to 1, prevents exploding gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # perform optimizer and scheduler steps\n        optimizer.step()\n        scheduler.step()\n        \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()\/len(batch))})\n         \n    # save model    \n    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.pth')\n    wandb.save(f'finetuned_BERT_epoch_{epoch}.pth')\n    \n    tqdm.write(f'\\nEpoch {epoch}')\n    \n    # log loss\n    loss_train_avg = loss_train_total\/len(dataloader_train)            \n    tqdm.write(f'Training loss: {loss_train_avg}')\n    \n    # log val report\n    val_loss, predictions, true_vals = evaluate()\n    val_f1 = f1_score_func(predictions, true_vals)\n    tqdm.write(f'Validation loss: {val_loss}')\n    tqdm.write(f'F1 Score (Weighted): {val_f1}')\n    \n    # log to wandb\n    wandb.log({\n        'epoch': epoch,\n        'loss': loss_train_avg,\n        'validation loss': val_loss,\n        'f1 score': val_f1,\n    })","6a4043d9":"# get model obj\nmodel, optimizer, scheduler = initHelper()\n\n# log model in wandb\nwandb.watch(model)\n\n# train model\nfor i in tqdm(range(1, epoch+1)):\n    trainHelper(i)","74d563bd":"# Helper that predicts the class of the article\ndef predLabels(data):\n    encoded_data = tokenizer.batch_encode_plus(\n        data, \n        add_special_tokens=True, \n        return_attention_mask=True, \n        #pad_to_max_length=True,\n        padding='max_length', \n        max_length=maxLength, \n        return_tensors='pt',\n        truncation=True\n    )\n    \n    input_ids = encoded_data['input_ids']\n    attention_masks = encoded_data['attention_mask']\n    \n    dataset = TensorDataset(input_ids, attention_masks)\n    dataloader = DataLoader(dataset, sampler=SequentialSampler(dataset), batch_size=1)\n    \n    model.eval()\n    \n    predictions = []\n    \n    for batch in dataloader:\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {\n            'input_ids':      batch[0],\n            'attention_mask': batch[1],\n#             'labels':         batch[2],\n        }\n\n        with torch.no_grad():        \n            outputs = model(**inputs)\n        \n#         print(outputs[0])\n        logits = outputs[0]\n        logits = logits.detach().cpu().numpy()\n        predictions.append(logits)\n    \n    predictions = np.concatenate(predictions, axis=0)\n            \n    return np.argmax(predictions, axis=1).flatten()","ae594de9":"# Validation data list\nx_val_list = list(x_val)\ny_val_list = list(y_val)\n\n# Make predictions\npred = predLabels(x_val_list)\n    \n# plot conf matrix\nplot_confusion_matrix(y_val_list, pred)\n\n# wandb\nwandb.log({\n    \"conf_mat\" : wandb.plot.confusion_matrix(\n                    probs=None,\n                    y_true=y_val_list,\n                    preds=pred,\n                    class_names=['ephemeral', 'evergreen']\n                ),\n})","13fe263e":"preds = predLabels(dfTest.body.to_list())\n\nsubDict = {\n    'urlid': dfTest.urlid.to_list(),\n    'label': preds,\n}\n\ndfSub = pd.DataFrame.from_dict(subDict)\ndfSub.to_csv('submission.csv', index = False)\nwandb.save(f'submission.csv')\n# dfSub.head()","f29b57ae":"Let's check for class imbalance","c22d303d":"# Train Model","31866470":"# Creating Pipeline","aadb2d2c":"Perfectly balanced, as all things should be.\n\nNow let's move onto embeddings.","fc10f488":"# Model Helper Functions","0dbeb381":"BERT could only process 512 token, so I need to process the boilerplate text to include most crucial data.","8598587d":"# Generating Submission File","d9f1a943":"# Environment Variables","5efc51f1":"# Libraries","1ce438b3":"# Data Preprocesing"}}