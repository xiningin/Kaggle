{"cell_type":{"16093c2f":"code","855afcb8":"code","b64f1b62":"code","57866137":"code","0b2daeea":"code","9d665ab6":"code","7781773f":"code","3273ec31":"code","36dddfe0":"code","e08a243f":"code","73656fac":"code","fde6556b":"code","8f74b2dc":"code","cadb867f":"code","f26719fd":"code","e9033231":"code","5e6b86b2":"code","a78cd692":"code","6d9d4c9e":"code","d6d53038":"code","9fbe980c":"code","0c016f78":"code","6e9e37ca":"code","8b9727ca":"code","f43831c7":"code","2d4cf6f2":"code","6ea222fa":"code","fe59e5fa":"code","5e31e974":"code","602d0290":"code","5061bf42":"code","1e88b332":"code","c7607f54":"code","6c8f6f51":"code","20980a66":"code","274d31d7":"code","e5f4a752":"code","e3c3a4ce":"code","1d85bf59":"code","92e377cf":"code","e0386d88":"code","2e689516":"markdown","9a757c96":"markdown","ab2a1267":"markdown","8a1a66fe":"markdown","3ec27536":"markdown","efe026a6":"markdown"},"source":{"16093c2f":"import warnings\n# warnings.filterwarnings('ignore')\n%config Completer.use_jedi = False\n\nimport pandas as pd\nimport numpy as np\n\n","855afcb8":"#\u89e3\u538b\u7f29\n!unzip \/kaggle\/input\/word2vec-nlp-tutorial\/labeledTrainData.tsv.zip\n!unzip \/kaggle\/input\/word2vec-nlp-tutorial\/unlabeledTrainData.tsv.zip\n!unzip \/kaggle\/input\/word2vec-nlp-tutorial\/testData.tsv.zip","b64f1b62":"train=pd.read_csv('\/kaggle\/working\/labeledTrainData.tsv',delimiter='\\t',quoting=3)\ntest=pd.read_csv('\/kaggle\/working\/testData.tsv',delimiter='\\t',quoting=3)\nunlabeled_train=pd.read_csv(\"\/kaggle\/working\/unlabeledTrainData.tsv\",\n                            header=0,    #\u8bbe\u7f6e\u5bfc\u5165dataframe\u7684\u5217\u540d\u79f0\uff0c\u9ed8\u8ba4\u4e3ainfer\n                            delimiter='\\t',\n                            quoting=3)   #\u53ef\u4ee5\u5982\u5b9e\u6253\u5370csv\u4e2d\u5185\u5bb9\uff0c\n                                         #\u4e0d\u8bbe\u7f6e\u8be5\u53c2\u6570\u5f15\u53f7\u5185\u5185\u5bb9\u4f1a\u8bfb\u53d6\u9519\u8bef","57866137":"print(train.shape)\nprint(test.shape)\nprint(unlabeled_train.shape)\nprint(train.info)\n\nprint(train['review'].size)\nprint(test['review'].size)\nprint(unlabeled_train['review'].size) \n# pd.isnull(train)","0b2daeea":"#bs4\u5e93 \u662f\u89e3\u6790\u3001\u904d\u5386\u3001\u7ef4\u62a4\u3001\u201c\u6807\u7b7e\u6811\u201c\u7684\u529f\u80fd\u5e93(bs4\u5e93\u628ahtml\u6e90\u4ee3\u7801\u91cd\u65b0\u8fdb\u884c\u4e86\u683c\u5f0f\u5316,\n#                                      \u4ece\u800c\u65b9\u4fbf\u6211\u4eec\u5bf9\u5176\u4e2d\u7684\u8282\u70b9\u3001\u6807\u7b7e\u3001\u5c5e\u6027\u7b49\u8fdb\u884c\u64cd\u4f5c)\n!pip3 install Beautifulsoup4","9d665ab6":"%%writefile Word2VecUtil.py\nimport re\nimport nltk\n#nltk\u662f\u4e00\u4e2anlp\u7684\u5de5\u5177\u5305\uff0c\u5185\u542b\u6570\u636e\u96c6\u3001python\u6a21\u5757\u7b49\u3002\n#\u53ef\u641c\u7d22\u6587\u672c\uff0c\u8ba1\u6570\u8bcd\u6c47\n\nimport pandas as pd\nimport numpy as np\n\nfrom bs4 import BeautifulSoup      #\u6700\u4e3b\u8981\u529f\u80fd\u5c31\u662f\u4ece\u7f51\u9875\u6293\u53d6\u6570\u636e\nfrom nltk.corpus import stopwords  #\u5bfc\u5165\u82f1\u8bed\u505c\u7528\u8bcd\uff0c\u5982at of you a that\u7b49\nfrom nltk.stem.snowball import SnowballStemmer #\u4e00\u4e2a\u5c0f\u7684\u8bed\u8a00\u8f6c\u6362\u5e93 \u652f\u630115\u79cd\u8bed\u8a00\n\nfrom multiprocessing import Pool\n\nclass KaggleWord2VecUtility(object):\n\n    @staticmethod\n    #\u5220\u9664\u505c\u7528\u8bcd\uff0c\u8fd4\u56delist\n    def review_to_wordlist(review, remove_stopwords=False): \n        # 1. HTML \u5220\u9664\n        review_text = BeautifulSoup(review, \"html.parser\").get_text() #\u521b\u5efaBS\u5bf9\u8c61\u5e76\u83b7\u53d6\u5176\u6587\u672c\n        # 2. \u5c06\u7279\u6b8a\u5b57\u7b26\u8f6c\u4e3a\u7a7a\u683c sub(pattern,repl,string)\n        # pattern\uff1a\u6b63\u5219\u4e2d\u7684\u6a21\u5f0f\u5b57\u7b26\u4e32 string\uff1a\u88ab\u5904\u7406\u7684\u5b57\u7b26\u4e32\n        review_text = re.sub('[^a-zA-Z]', ' ', review_text)\n        # 3. \u8f6c\u5316\u4e3a\u5c0f\u5199+\u5206\u5272\n        words = review_text.lower().split()\n        # 4. \u5220\u9664\u505c\u7528\u8bcd\n        if remove_stopwords:\n            #\u521b\u5efa\u4e00\u4e2a\u65e0\u5e8f\u4e0d\u91cd\u590d\u5143\u7d20\u96c6\uff08\u5373\u505c\u7528\u8bcd\u96c6\u5408\uff09\n            stops = set(stopwords.words('english')) \n            words = [w for w in words if not w in stops]\n        # 5. \u63d0\u53d6\u8bcd\u5e72\n        stemmer = SnowballStemmer('english')  #\u9009\u62e9\u8bed\u8a00\u4e3aenglish\n        words = [stemmer.stem(w) for w in words] #\u63d0\u53d6\u6bcf\u4e2a\u8bcd\u8bed\u7684\u4e3b\u5e72\n        # 6. \u8fd4\u56de\u4e00\u4e2alist\n        return(words)\n\n    @staticmethod\n    #\u7528join\u5c06word\u7528\u201c \u201d\u8fde\u63a5\u8d77\u6765\n    def review_to_join_words( review, remove_stopwords=False ):\n        words = KaggleWord2VecUtility.review_to_wordlist(\\\n            review, remove_stopwords=False)\n        join_words = ' '.join(words)\n        return join_words\n\n    @staticmethod\n    def review_to_sentences( review, remove_stopwords=False ):\n        #\u4e0b\u884c\u662f\u4e00\u4e2a\u5c06\u82f1\u6587\u6587\u672c\u5212\u5206\u6210\u53e5\u5b50\u7684\u6a21\u578b\uff0c\u5212\u5206\u4f9d\u636e\u662f\u6bcf\u4e2a\u53e5\u5b50\u7ed3\u675f\u540e\u7559\u6709\u7a7a\u683c\n        tokenizer = nltk.data.load('tokenizers\/punkt\/english.pickle')\n        # 1. nltk tokenize\u5206\u5272\u53e5\u5b50\u4e3a\u5355\u8bcd\uff0c\u5220\u9664\u7a7a\u683cstrip()\n        raw_sentences = tokenizer.tokenize(review.strip())\n        # 2. \u904d\u5386\u6bcf\u4e2a\u53e5\u5b50\n        sentences = []\n        for raw_sentence in raw_sentences:\n            # \u4e3a\u7a7a\u5219skip\n            if len(raw_sentence) > 0:\n                # \u5220\u9664\u6807\u7b7e\uff0c\u7528\u7a7a\u683c\u66ff\u6362\u975e\u5b57\u6bcd\u5b57\u7b26\uff0c\u5220\u9664\u505c\u7528\u8bcd\n                sentences.append(\\\n                    KaggleWord2VecUtility.review_to_wordlist(\\\n                    raw_sentence, remove_stopwords))\n        return sentences\n\n\n    # \u53c2\u8003 : https:\/\/gist.github.com\/yong27\/7869662\n    # http:\/\/www.racketracer.com\/2016\/07\/06\/pandas-in-parallel\/\n    # \u4e0e\u591a\u4e2a\u7ebf\u7a0b\u4e00\u8d77\u4f7f\u7528\u4ee5\u63d0\u9ad8\u901f\u5ea6\n    @staticmethod\n    def _apply_df(args):\n        df, func, kwargs = args\n        return df.apply(func, **kwargs)\n\n    @staticmethod\n    def apply_by_multiprocessing(df, func, **kwargs):\n        # \u83b7\u53d6\u5173\u952e\u5b57\u9879\u4e2d\u7684worker\u53c2\u6570\n        workers = kwargs.pop('workers')\n        # \u5b9a\u4e49Pool\u8fdb\u7a0b\u6c60\n        pool = Pool(processes=workers)\n        #map()\u6839\u636e\u63d0\u4f9b\u7684\u51fd\u6570\u5bf9\u6307\u5b9a\u5e8f\u5217\u505a\u6620\u5c04 \n        result = pool.map(KaggleWord2VecUtility._apply_df, [(d, func, kwargs)\n                for d in np.array_split(df, workers)])\n        pool.close()\n        # \uc791\uc5c5 \uacb0\uacfc\ub97c \ud569\uccd0\uc11c \ubc18\ud658\n        return pd.concat(result)","7781773f":"# \u4fee\u6539word2vecutil\u6587\u4ef6\n# %load Word2VecUtil.py","3273ec31":"from Word2VecUtil import KaggleWord2VecUtility","36dddfe0":"print(train['review'][0][:50])\nKaggleWord2VecUtility.review_to_wordlist(train['review'][0][:50])\n#\u63d0\u53d6\u8bcd\u5e72\u4e86 \u5982going-go","e08a243f":"import nltk\nnltk.download('punkt')","73656fac":"sentences = []\nfor review in train['review']:\n    sentences += KaggleWord2VecUtility.review_to_sentences(review,\n                                                           remove_stopwords=False)\n","fde6556b":"for review in unlabeled_train['review']:\n    sentences += KaggleWord2VecUtility.review_to_sentences(\n        review,remove_stopwords=False)","8f74b2dc":"# len(sentences)","cadb867f":"print(sentences[0][:10])\nprint(sentences[1][:10])","f26719fd":"import logging\nlogging.basicConfig(  # \u4e3a\u65e5\u5fd7\u914d\u7f6e\u57fa\u672c\u4fe1\u606f\n    format='%(asctime)s : %(levelname)s : %(message)s', #\u65e5\u5fd7\u8f93\u51fa\u683c\u5f0f\n    level=logging.INFO) # \u8bbe\u7f6e\u65e5\u5fd7\u7ea7\u522b \u4f4e\u4e8e\u8be5\u7ea7\u522b\u7684\u65e5\u5fd7\u6d88\u606f\u5c06\u88ab\u5ffd\u7565\n                        # CRITICAL>ERROR>WARNNING>INFO>DEBUG>NOTSET\n    ","e9033231":"num_features = 300 # \u5b57\u7b26\u6570\u77e2\u91cf\u7ef4\nmin_word_count = 40 # \u6700\u5c0f\u5b57\u7b26\u6570\nnum_workers = 4 # \u5e76\u884c\u5904\u7406\u7ebf\u7a0b\u6570\ncontext = 10 # \u5b57\u7b26\u4e32\u7a97\u53e3\u5927\u5c0f\ndownsampling = 1e-3\n\n#\u521d\u59cb\u5316\u548c\u6a21\u578b\u8bad\u7ec3\nfrom gensim.models import word2vec\n#gensim\u662f\u4e2anlp\u5de5\u5177\u5305\uff0c\u5185\u542b\u591a\u4e2a\u5e38\u89c1\u6a21\u578b LSI LDA HDP DTM DIM TF-IDF word2vec paragraph2vec\n\nmodel = word2vec.Word2Vec(sentences,\n                         workers=num_workers,\n#                          size=num_features,\n                         min_count=min_word_count,\n                         window=context,\n                         sample=downsampling) #\u9ad8\u9891\u8bcd\u968f\u673a\u4e0b\u91c7\u6837\u7684\u914d\u7f6e\u9608\u503c\uff0c\u8303\u56f4\uff080\uff0c1e-5\uff09\nmodel","5e6b86b2":"#  \u8bad\u7ec3\u5b8c\u540e\u5220\u9664\u4e0d\u5fc5\u8981\u7684\u5185\u5b58\nmodel.init_sims(replace=True)\n\nmodel_name = '300features_40minwords_10text'\n\nmodel.save(model_name)","a78cd692":"#\u63d0\u53d6\u6ca1\u6709\u76f8\u4f3c\u6027\u7684\u5355\u8bcd\nmodel.wv.doesnt_match('man woman child kitchen'.split())","6d9d4c9e":"model.wv.doesnt_match('france england germany berlin'.split())","d6d53038":"#\u63d0\u53d6\u76f8\u4f3c\u7684\u8bcd\nmodel.wv.most_similar('man')","9fbe980c":"model.wv.most_similar('queen')","0c016f78":"# model.wv.most_similar('awful')","6e9e37ca":"model.wv.most_similar('film')","8b9727ca":"# model.wv.most_similar('happy')","f43831c7":"model.wv.most_similar('happi')","2d4cf6f2":"from sklearn.manifold import TSNE  #\u6570\u636e\u964d\u7ef4\u548c\u53ef\u89c6\u5316\n#TSNE\u4f7f\u9ad8\u4e8e\u4e8c\u7ef4\u7684\u805a\u7c7b\u7ed3\u679c\u4ee5\u4e8c\u7ef4\u65b9\u5f0f\u5c55\u73b0\u51fa\u6765\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport gensim\n#gensim\u4ece\u539f\u59cb\u7684\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\uff0c\u65e0\u76d1\u7763\u5b66\u4e60\u5230\u6587\u672c\u9690\u5c42\u7684\u4e3b\u9898\u5411\u91cf\u8868\u8fbe\n#\u652f\u6301TF-IDF\uff0cLSA\uff0cLDA\uff0cword2vec\nimport gensim.models as g","6ea222fa":"# \u7528\u6765\u663e\u793a\u4e2d\u6587\u6807\u7b7e\n# plt.rcParams['font.sans-serif']=['SimHei']\n#\u7528\u4e8e\u663e\u793a\u8d1f\u53f7\nmpl.rcParams['axes.unicode_minus'] = False","fe59e5fa":"model_name = '300features_40minwords_10text'\nmodel = g.Doc2Vec.load(model_name)\n\n#model.wv.vocab \u76f4\u63a5\u8c03\u7528\u751f\u6210\u7684\u8bcd\u5411\u91cf\nvocab = list(model.wv.key_to_index)  \n\nX = model[vocab]\n\n\n# rock_idx = model.wv.key_to_index[\"rock\"]\n# vocab = (model.wv.get_vecattr(,))\n\n# \n\nprint(len(X))\n# print(X[0][:10])\ntsne = TSNE(n_components=2)\nX_tsne = tsne.fit_transform(X[:100, :])\n","5e31e974":"df = pd.DataFrame(X_tsne, index=vocab[:100], columns=['x', 'y'])\ndf.shape \ndf.head(10)","602d0290":"fig = plt.figure()\nfig.set_size_inches(40,20)\nax = fig.add_subplot(111)\nax.scatter(df['x'],df['y'])\nfor word,pos in df.iterrows():\n    ax.annotate(word,pos,fontsize=30)\nplt.show()","5061bf42":"import numpy as np\n\ndef makeFeatureVec(words, model, num_features):\n    '''\n    \u5728IMDB\u4e2d\u6bcf\u6bb5\u8bc4\u8bba\u957f\u5ea6\u4e0d\u4e00\uff0c\u56e0\u6b64\u9700\u8981\u5148\u5c06\u6bcf\u4e2a\u72ec\u7acb\u7684\u5355\u8bcd\u5411\u91cf\u8f6c\u6362\u6210\u7b49\u957f\u7684\u7279\u5f81\u96c6\u5408\n    \u5bf9\u6240\u6709\u5355\u8bcd\u5411\u91cf\u6c42\u5e73\u5747\n    '''\n    # \u521d\u59cb\u5316\u4e00\u4e2a\u6570\u7ec4\uff08\u4e3a\u4e86\u63d0\u9ad8\u901f\u5ea6\uff09\n    featureVec = np.zeros((num_features,), dtype='float32')\n    \n    nwords = 0.\n    # Index2word\u662f\u6a21\u578b\u8bcd\u6c47\u8868\u4e2d\u5355\u8bcd\u540d\u79f0\u7684\u5217\u8868\uff08\u4e3a\u4e86\u63d0\u901f\uff09\n    index2word_set = set(model.wv.index2word)\n    \n    # \u5faa\u73af\u904d\u5386\u8bc4\u8bba\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd\uff0c\u5982\u679c\u5b83\u5728\u6a21\u578b\u7684\u8bcd\u6c47\u8868\u4e2d\uff0c\n    # \u5219\u5c06\u5176\u53ca\u5176\u7279\u5f81\u5411\u91cf\u5faa\u73af\u5230\u603b\u548c\n    for word in words:\n        if word in index2word_set:\n            nwords = nwords + 1.\n            featureVec = np.add(featureVec, model[word])\n    \n    # \u505a\u5e73\u5747\u8fd0\u7b97\n    featureVec = np.divide(featureVec, nwords)\n    return featureVec\n\ndef getAvgFeatureVecs(reviews, model, num_features):\n    # \u7ed9\u5b9a\u4e00\u7ec4\u8bc4\u8bba\uff08\u6bcf\u4e2a\u8bc4\u8bba\u4e00\u4e2a\u5355\u8bcd\u5217\u8868\uff09\uff0c\u8ba1\u7b97\u6bcf\u4e2a\u7279\u5f81\u5411\u91cf\u7684\u5e73\u5747\u7279\u5f81\u5411\u91cf\uff0c\n    #\u5e76\u8fd4\u56de\u4e00\u4e2a2D numpy\u6570\u7ec4\n     \n    counter = 0\n\n    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n       \n    for review in reviews:\n       if counter%1000 == 0:\n           print \"Review %d of %d\" % (counter, len(reviews))\n       \n       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n           num_features)\n\n       counter = counter + 1\n    return reviewFeatureVecs\n\ndef getCleanReviews(reviews):\n    clean_reviews = []\n    clean_reviews = KaggleWord2VecUtility.apply_by_multiprocessing(\\\n            reviews['review'], KaggleWord2VecUtility.review_to_wordlist,\\\n            workers=4)\n    return clean_reviews","1e88b332":"%time\ntrainDataVecs = getAvgFeatureVecs(\\\n    getCleanReviews(train), model, num_features)","c7607f54":"%time\ntestDataVecs = getAvgFeatureVecs(\\\n    getCleanReviews(test), model, num_features)","6c8f6f51":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(\n    n_estimators=100, n_jobs=-1, random_state=2018)","20980a66":"%time\nforest = forest.fit(trainDataVecs, train['sentiment'])","274d31d7":"from sklearn.model_selection import cross_val_score\n%time\nscore = np.mean(cross_val_score(\\\n        forest, trainDataVecs, \\\n        train['sentiment'], cv=10, scoring='roc_auc'))","e5f4a752":"score","e3c3a4ce":"result = forest.predict(testDataVecs)","1d85bf59":"output = pd.DataFrame(data={'id':test['id'], 'sentiment':result})\noutput.to_csv('Word2Vec_AverageVEctors_{0:.5f}.csv'.format(score),\n             index=False, quoting=3)","92e377cf":"output_sentiment = output['sentiment'].value_counts()\nprint(output_sentiment[0] - output_sentiment[1])\noutput_sentiment","e0386d88":"import seaborn as sns\n%matplotlib inline\n\nfig, axes = plt.subplots(ncols=2)\nfig.set_size_inches(12,5)\nsns.countplot(train['sentiment'], ax=axes[0])\nsns.countplot(output['sentiment'], ax=axes[1])","2e689516":"# **RandomForest**","9a757c96":"\u4f53\u7cfb\u7ed3\u6784\uff1a\u201c\u4f53\u7cfb\u7ed3\u6784\u201d\u9009\u9879\u662fskip-gram\uff08\u9ed8\u8ba4\uff09\u6216CBOW\u6a21\u578b\u3002 skip-gram\u901f\u5ea6\u8f83\u6162\uff0c\u4f46\u6548\u679c\u66f4\u597d\n\u5b66\u4e60\u7b97\u6cd5\uff1a\u5206\u5c42softmax\uff08\u9ed8\u8ba4\uff09\u6216\u8d1f\u91c7\u6837\u3002\u9ed8\u8ba4\u503c\u5728\u8fd9\u91cc\u5de5\u4f5c\u6b63\u5e38\u3002\n\u5bf9\u7ecf\u5e38\u51fa\u73b0\u7684\u5355\u8bcd\u8fdb\u884c\u4e0b\u91c7\u6837\uff1aGoogle\u6587\u6863\u5efa\u8bae\u57280.00001\u548c0.001\u4e4b\u95f4\u7684\u503c\u3002\u6b64\u5904\uff0c\u663e\u793a\u63a5\u8fd10.001\u7684\u503c\u53ef\u63d0\u9ad8\u6700\u7ec8\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002\n\u5b57\u5411\u91cf\u7ef4\uff1a\u4f7f\u7528\u8bb8\u591a\u529f\u80fd\u5e76\u4e0d\u603b\u662f\u5f88\u597d\uff0c\u4f46\u901a\u5e38\u662f\u66f4\u597d\u7684\u6a21\u578b\u3002\u5408\u7406\u7684\u503c\u53ef\u4ee5\u4ece\u6570\u5341\u5230\u6570\u767e\uff0c\u6b64\u5904\u6307\u5b9a\u4e3a300\u3002\n\u4e0a\u4e0b\u6587\/\u7a97\u53e3\u5927\u5c0f\uff1a\u4e0a\u4e0b\u6587\u4e2d\u5b66\u4e60\u7b97\u6cd5\u5e94\u8003\u8651\u591a\u5c11\u4e2a\u5355\u8bcd\uff1f\u8f83\u5927\u7684\u6570\u5b57\u5bf9\u5206\u5c42softmax\u6709\u76ca\uff0c\u4f4610\u5219\u5f88\u597d\n\u8f85\u52a9\u7ebf\u7a0b\uff1a\u8981\u8fd0\u884c\u7684\u5e76\u884c\u8fdb\u7a0b\u6570\uff0c\u8fd9\u5728\u8ba1\u7b97\u673a\u4e4b\u95f4\u662f\u4e0d\u540c\u7684\uff0c\u4f46\u5728\u5927\u591a\u6570\u7cfb\u7edf\u4e0a\u4f7f\u7528\u7684\u503c\u4e3a4\u52306\u4e4b\u95f4\u3002\n\u6700\u5c0f\u5355\u8bcd\u6570\uff1a\u6709\u52a9\u4e8e\u5c06\u8bcd\u6c47\u8868\u7684\u5927\u5c0f\u9650\u5236\u4e3a\u6709\u610f\u4e49\u7684\u5355\u8bcd\u6570\u3002\u5728\u6240\u6709\u6587\u6863\u4e2d\u6ca1\u6709\u591a\u6b21\u51fa\u73b0\u7684\u5355\u8bcd\u5c06\u88ab\u5ffd\u7565\u3002\u9002\u5f53\u7684\u8303\u56f4\u662f10\u5230100\uff0c\n\u56e0\u4e3a\u6bcf\u4e2a\u7535\u5f71\u90fd\u670930\u6761\u8bc4\u8bba\uff0c\u56e0\u6b64\u6700\u5c0f\u5b57\u6570\u8bbe\u7f6e\u4e3a40\uff0c\u4ee5\u907f\u514d\u8fc7\u4e8e\u91cd\u89c6\u5355\u4e2a\u7535\u5f71\u6807\u9898\u3002\u7ed3\u679c\uff0c\u603b\u8bcd\u6c47\u91cf\u7ea6\u4e3a15,000\u4e2a\u5355\u8bcd\u3002\u8f83\u9ad8\u7684\u503c\u6709\u52a9\u4e8e\u6709\u9650\u7684\u6267\u884c\u65f6\u95f4\u3002","ab2a1267":"# **\u52a0\u8f7d\u6570\u636e**","8a1a66fe":"\u901a\u8fc7t-SNE\u53ef\u89c6\u5316Word2Vec\u77e2\u91cf\u5316\u7684\u5355\u8bcd","3ec27536":"# **\u63a2\u7d22\u6a21\u578b\u7ed3\u679c**","efe026a6":"# **Word2Vec\u7b97\u6cd5** \n# \u901a\u8fc7\u9884\u5904\u7406\u89e3\u6790\u51fa\u7684\u53e5\u5b50\u5217\u8868\u6765\u8bad\u7ec3\u6a21\u578b"}}