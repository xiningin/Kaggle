{"cell_type":{"69a2c29a":"code","674b097e":"code","1509be26":"code","5f2db70d":"code","21daf392":"code","ba855d05":"code","c5a3642c":"code","f61dcf8a":"code","985a2c0e":"code","2d0f4212":"code","36f74f3c":"code","6bd320b8":"code","dedfe105":"code","f9cefc66":"code","7d3525a1":"code","8e9ff497":"code","590a28fc":"code","ce5af2e6":"code","a35976a1":"code","4ec848e9":"code","0c598eed":"code","4db0f94d":"code","a86146a4":"code","503e6937":"code","1c493fda":"code","c012b29e":"code","cc9d17ab":"code","7cc7a919":"code","579d7fe3":"code","2d288ece":"code","0210d738":"code","0c3d6149":"code","4abe68ea":"code","62c24116":"code","5c241787":"code","029547a1":"code","1c5d3c81":"code","0ef275b7":"code","70674445":"code","8882a667":"code","bf9a94c4":"code","f9d8f9c5":"code","4a824b1c":"code","5019ba42":"code","103639ff":"code","84998ecf":"code","dd0d20f6":"code","46b8b4a0":"code","f5502a06":"code","2f0237bc":"code","d53f65d5":"code","a4cc52e5":"code","e72cf939":"code","62c5dc02":"code","c7541185":"code","f166bd23":"code","b6d9392b":"code","5785def9":"code","f8cbd092":"code","4be6c250":"code","97ac0bf0":"code","93b96dee":"code","137ddafc":"code","e3800050":"code","0a27f444":"code","d8d5cf9e":"code","6a77cdd3":"code","cb0a98c5":"code","772c5ca7":"code","b713cd93":"code","939c7b25":"code","ad328b9b":"code","41db406f":"code","e5115283":"code","75c48b9a":"markdown","4db0c429":"markdown","b318c6be":"markdown","28fafffa":"markdown","6d01249c":"markdown","91682974":"markdown","4d2dae1d":"markdown","28b77c6e":"markdown","55e1064d":"markdown","f561cd8b":"markdown"},"source":{"69a2c29a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","674b097e":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest= pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain.head()","1509be26":"train.info()","5f2db70d":"#dealing with null values in the train and test dataset\nnull = train.isna().sum().sort_values(ascending = True)\nnull_2 = test.isna().sum().sort_values(ascending = True)\nnull_values = pd.concat([null, null_2], keys = ['train null', 'test null'], axis = 1)\nnull_values.head(40)","21daf392":"#Lot Frontage\ndata = [train, test]\nfor dataset in data:\n    x = dataset.iloc[:, 3].values\n    x = x.reshape(-1,1)\n    imputer  = SimpleImputer(strategy = 'mean', missing_values = np.nan)\n    imputer = imputer.fit(x)\n    x = imputer.transform(x)\n    dataset.iloc[:, 3] = x","ba855d05":"#Alley\ndata = [train, test]\nfor dataset in data:\n    x = dataset.iloc[:,6].values\n    x = x.reshape(-1,1)\n    impute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\n    impute = impute.fit(x)\n    x = impute.transform(x)\n    dataset.iloc[:, 6] = x    ","c5a3642c":"# BsmtCond\ndata = [train, test]\nfor dataset in data:\n    x = dataset.iloc[:,31].values\n    x = x.reshape(-1,1)\n    impute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\n    impute = impute.fit(x)\n    x = impute.transform(x)\n    dataset.iloc[:, 31] = x ","f61dcf8a":"#BsmtExposure\ndata = [train, test]\nfor dataset in data:\n    x = dataset.iloc[:,32].values\n    x = x.reshape(-1,1)\n    impute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\n    impute = impute.fit(x)\n    x = impute.transform(x)\n    dataset.iloc[:, 32] = x ","985a2c0e":"#BsmtFinType1\ndata = [train, test]\nfor dataset in data:\n    x = dataset.iloc[:,33].values\n    x = x.reshape(-1,1)\n    impute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\n    impute = impute.fit(x)\n    x = impute.transform(x)\n    dataset.iloc[:, 33] = x ","2d0f4212":"#BsmtFinType2\ndata = [train, test]\nfor dataset in data:\n    x = dataset.iloc[:,35].values\n    x = x.reshape(-1,1)\n    impute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\n    impute = impute.fit(x)\n    x = impute.transform(x)\n    dataset.iloc[:, 35] = x ","36f74f3c":"#BsmtQual\ndata = [train, test]\nfor dataset in data:\n    x = dataset.iloc[:,30].values\n    x = x.reshape(-1,1)\n    impute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\n    impute = impute.fit(x)\n    x = impute.transform(x)\n    dataset.iloc[:, 30] = x ","6bd320b8":"#Fence\ndata = [train, test]\nfor dataset in data:\n    x = dataset.iloc[:,73].values\n    x = x.reshape(-1,1)\n    impute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\n    impute = impute.fit(x)\n    x = impute.transform(x)\n    dataset.iloc[:, 73] = x ","dedfe105":"#FireplaceQu\ndata = [train, test]\nfor dataset in data:\n    x = dataset.iloc[:,57].values\n    x = x.reshape(-1,1)\n    impute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\n    impute = impute.fit(x)\n    x = impute.transform(x)\n    dataset.iloc[:, 57] = x ","f9cefc66":"#GarageCond\ndata = [train, test]\nfor dataset in data:\n    x = dataset.iloc[:,64].values\n    x = x.reshape(-1,1)\n    impute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\n    impute = impute.fit(x)\n    x = impute.transform(x)\n    dataset.iloc[:, 64] = x ","7d3525a1":"#GarageFinish\ndata = [train, test]\nfor dataset in data:\n    x = dataset.iloc[:,60].values\n    x = x.reshape(-1,1)\n    impute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\n    impute = impute.fit(x)\n    x = impute.transform(x)\n    dataset.iloc[:, 60] = x ","8e9ff497":"#GarageQual\ndata = [train, test]\nfor dataset in data:\n    x = dataset.iloc[:,63].values\n    x = x.reshape(-1,1)\n    impute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\n    impute = impute.fit(x)\n    x = impute.transform(x)\n    dataset.iloc[:, 63] = x ","590a28fc":"#GarageType\ndata = [train, test]\nfor dataset in data:\n    x = dataset.iloc[:,58].values\n    x = x.reshape(-1,1)\n    impute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\n    impute = impute.fit(x)\n    x = impute.transform(x)\n    dataset.iloc[:, 58] = x ","ce5af2e6":"#GarageYrBit\ndata = [train, test]\nfor dataset in data:\n    x = dataset.iloc[:,59].values\n    x = x.reshape(-1,1)\n    impute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\n    impute = impute.fit(x)\n    x = impute.transform(x)\n    dataset.iloc[:, 59] = x ","a35976a1":"null_values.tail(40)","4ec848e9":"#MiscFeature\ndata = [train, test]\nfor dataset in data:\n    x = dataset.iloc[:,74].values\n    x = x.reshape(-1,1)\n    impute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\n    impute = impute.fit(x)\n    x = impute.transform(x)\n    dataset.iloc[:, 74] = x ","0c598eed":"#PoolQC\ndata = [train, test]\nfor dataset in data:\n    x = dataset.iloc[:,72].values\n    x = x.reshape(-1,1)\n    impute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\n    impute = impute.fit(x)\n    x = impute.transform(x)\n    dataset.iloc[:, 72] = x ","4db0f94d":"#MasVnrType\ndata = [train, test]\nfor dataset in data:\n    x = dataset.iloc[:,25].values\n    x = x.reshape(-1,1)\n    impute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\n    impute = impute.fit(x)\n    x = impute.transform(x)\n    dataset.iloc[:, 25] = x ","a86146a4":"#MasVnrArea\ndata = [train, test]\nfor dataset in data:\n    x = dataset.iloc[:,26].values\n    x = x.reshape(-1,1)\n    impute = SimpleImputer(strategy = 'mean', missing_values = np.nan)\n    impute = impute.fit(x)\n    x = impute.transform(x)\n    dataset.iloc[:, 26] = x ","503e6937":"#Remaining null values \nnull_3 = train.isna().sum().sort_values(ascending = True)\nnull_4 = test.isna().sum().sort_values(ascending = True)\nnull_values_1 = pd.concat([null_3, null_4], keys = ['train null', 'test null'], axis = 1)\nnull_values_1.head(40)","1c493fda":"null_values_1.tail(40)","c012b29e":"#Drop remaining null rows in train set\ntrain = train.dropna(axis = 0)\n# test = test.fillna(0, axis = 0)","cc9d17ab":"test.columns","7cc7a919":"#imputing remaining test set null values\nx = test.iloc[:,39].values\nx = x.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(x)\nx = impute.transform(x)\ndataset.iloc[:, 39] = x ","579d7fe3":"# #imputing remaining test set null values\n# b = test.iloc[:,80].values\n# b = b.reshape(-1,1)\n# impute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\n# impute = impute.fit(b)\n# b = impute.transform(b)\n# dataset.iloc[:, 80] = b ","2d288ece":"#imputing remaining test set null values\nc = test.iloc[:,55].values\nc = c.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(c)\nc = impute.transform(c)\ndataset.iloc[:, 55] = c ","0210d738":"#imputing remaining test set null values\nd = test.iloc[:,2].values\nd = d.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(d)\nd = impute.transform(d)\ndataset.iloc[:, 2] = d ","0c3d6149":"#imputing remaining test set null values\ne = test.iloc[:,-18].values\ne = e.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(e)\ne = impute.transform(e)\ndataset.iloc[:, -18] = e ","4abe68ea":"\n#imputing remaining test set null values\nf = test.iloc[:,-19].values\nf = f.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(f)\nf = impute.transform(f)\ndataset.iloc[:, -19] = f \n","62c24116":"#imputing remaining test set null values\ng = test.iloc[:,23].values\ng = g.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(g)\ng = impute.transform(g)\ndataset.iloc[:, 23] = g \n","5c241787":"\n#imputing remaining test set null values\nh = test.iloc[:,24].values\nh = h.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(h)\nh = impute.transform(h)\ndataset.iloc[:, 24] = h \n","029547a1":"#imputing remaining test set null values\ni = test.iloc[:,57].values\ni = i.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(i)\ni = impute.transform(i)\ndataset.iloc[:, 57] = i ","1c5d3c81":"#imputing remaining test set null values\nj = test.iloc[:,38].values\nj = j.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(j)\nj = impute.transform(j)\ndataset.iloc[:, 38] = j","0ef275b7":"#imputing remaining test set null values\nk = test.iloc[:,37].values\nk = k.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(k)\nk = impute.transform(k)\ndataset.iloc[:, 37] = k ","70674445":"#imputing remaining test set null values\na = test.iloc[:,9].values\na = a.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(a)\na = impute.transform(a)\ndataset.iloc[:, 9] = a \n","8882a667":"#imputing remaining test set null values\nn = test.iloc[:,35].values\nn = n.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(n)\nn = impute.transform(n)\ndataset.iloc[:, 35] = n ","bf9a94c4":"#imputing remaining test set null values\nl = test.iloc[:,49].values\nl = l.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(l)\nl = impute.transform(l)\ndataset.iloc[:, 49] = l \n\n","f9d8f9c5":"#imputing remaining test set null values\nm = test.iloc[:,48].values\nm = m.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(m)\nm = impute.transform(m)\ndataset.iloc[:, 48] = m ","4a824b1c":"#imputing remaining test set null values\nl = test.iloc[:,-2].values\nl = l.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(l)\nl = impute.transform(l)\ndataset.iloc[:, -2] = l ","5019ba42":"#imputing remaining test set null values\nl = test.iloc[:,-27].values\nl = l.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(l)\nl = impute.transform(l)\ndataset.iloc[:, -27] = l ","103639ff":"#imputing remaining test set null values\nl = test.iloc[:,34].values\nl = l.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(l)\nl = impute.transform(l)\ndataset.iloc[:, 34] = l ","84998ecf":"#imputing remaining test set null values\nl = test.iloc[:,36].values\nl = l.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(l)\nl = impute.transform(l)\ndataset.iloc[:, 36] = l ","dd0d20f6":"#imputing remaining test set null values\nl = test.iloc[:,47].values\nl = l.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(l)\nl = impute.transform(l)\ndataset.iloc[:, 47] = l ","46b8b4a0":"#imputing remaining test set null values\nl = test.iloc[:,49].values\nl = l.reshape(-1,1)\nimpute = SimpleImputer(strategy = 'most_frequent', missing_values = np.nan)\nimpute = impute.fit(l)\nl = impute.transform(l)\ndataset.iloc[:, 49] = l ","f5502a06":"#Remaining null values \ntest.isna().any()","2f0237bc":"#Drop the ID colum\ntrain = train.drop('Id', axis = 1)\ntest = test.drop('Id', axis = 1)","d53f65d5":"#correlation plot\n\n# Compute correlations\ncorr = train.corr()\n\nsns.set_style(style = 'white')\nf, ax = plt.subplots(figsize=(16, 11))\nsns.heatmap(corr, vmax=0.9, cmap=\"Reds\", square=True)","a4cc52e5":"#check for skewness\nf, ax = plt.subplots(figsize=(9, 8))\nsns.distplot(train['SalePrice'], bins = 20, color = 'Magenta')\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\n","e72cf939":"# log transformation\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","62c5dc02":"#check for skewness after transformation\nf, ax = plt.subplots(figsize=(9, 8))\nsns.distplot(train['SalePrice'], bins = 20, color = 'Magenta')\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\n","c7541185":"#Extracting the independent variables\ntrain_features = train.drop('SalePrice', axis = 1)\ntrain_dependent = train['SalePrice'].reset_index(drop=True)\ntest_features = test","f166bd23":"test_features.shape","b6d9392b":"#Joining the features tables\nall_variables = pd.concat([train_features, test_features]).reset_index(drop=True)","5785def9":"# Fetch all numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in all_variables.columns:\n    if all_variables[i].dtype in numeric_dtypes:\n        numeric.append(i)","f8cbd092":"# Find skewed numerical features\nskew_features = all_variables[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features\n","4be6c250":"# Normalize skewed features\nfor i in skew_index:\n    all_variables[i] = boxcox1p(all_variables[i], boxcox_normmax(all_variables[i] + 1))","97ac0bf0":"def logs(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n        res.columns.values[m] = l + '_log'\n        m += 1\n    return res\n\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd']\n\nall_variables = logs(all_variables, log_features)","93b96dee":"def squares(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   \n        res.columns.values[m] = l + '_sq'\n        m += 1\n    return res \n\nsquared_features = ['YearRemodAdd', 'LotFrontage_log', \n              'TotalBsmtSF_log', '1stFlrSF_log', '2ndFlrSF_log', 'GrLivArea_log',\n              'GarageCars_log', 'GarageArea_log']\nall_variables = squares(all_variables, squared_features)\n","137ddafc":"all_variables = pd.get_dummies(all_variables).reset_index(drop=True)\nall_variables.shape\n# one_hot_encoded_test_predictors = pd.get_dummies(test)\n# final_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,join='left',axis=1)","e3800050":"all_variables.head()","0a27f444":"#Restracture back to original 2 datasets\nX = all_variables.iloc[:len(train_dependent), :]\nX_test = all_variables.iloc[len(train_dependent):, :]\nX.shape, train_dependent.shape, X_test.shape","d8d5cf9e":"# Fitting Random Forest Regressor\nregressor = RandomForestRegressor(n_estimators=300, random_state=0)\nregressor.fit(X, train_dependent.ravel())","6a77cdd3":"#cross validation score\nfrom sklearn.model_selection import cross_val_score\nregressor = RandomForestRegressor(n_estimators=300, random_state=0)\n# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(regressor, X, train_dependent,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n\nprint(\"Average MAE score:\", scores.mean())","cb0a98c5":"# XGBRegressor\nfrom xgboost import XGBRegressor\nregressor_2 = XGBRegressor(n_estimators = 500, learning_rate = 0.05)\n# Fit the model\nregressor_2 = regressor_2.fit(X, train_dependent, verbose = False)\n\n# Get predictions\n# pred_2 = regressor_2.predict(X_test) ","772c5ca7":"#cross validation score\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score\nregressor_2 = XGBRegressor(n_estimators = 500, learning_rate = 0.05)\nscores = -1 * cross_val_score(regressor_2, X, train_dependent,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n\nprint(\"Average MAE score:\", scores.mean())","b713cd93":"# Predicting results (with XGB)\nregressor_2 = regressor_2.fit(X, train_dependent, verbose = False)\nY_pred = regressor_2.predict(X_test)","939c7b25":"Y_pred.shape","ad328b9b":"submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(Y_pred))","41db406f":"# Fix outlier predictions\nq1 = submission['SalePrice'].quantile(0.05)\nq2 = submission['SalePrice'].quantile(0.95)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\"submission_1.csv\", index=False)","e5115283":"# Scale predictions\nsubmission['SalePrice'] *= 1.001619\nsubmission.to_csv(\"submission_2.csv\", index=False)","75c48b9a":"## Null Values","4db0c429":"## Feature Transformations \n(Borrowed from Lavanya Shukla)\nCalculating log and square transformations.","b318c6be":"## Remapping Categorical variables","28fafffa":"## Submission","6d01249c":"Better now!","91682974":"### Skewness in other variables","4d2dae1d":"Right tailed","28b77c6e":"## Visualization 1","55e1064d":"### Train and Test sets","f561cd8b":"## Model Fitting"}}