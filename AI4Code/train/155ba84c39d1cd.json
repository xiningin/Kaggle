{"cell_type":{"30cf759c":"code","10485afe":"code","1f2bf088":"code","00cc6d32":"code","6a9c9941":"code","c88a9d02":"code","625f1b54":"code","4983cb0d":"code","36b04a7e":"code","baee7934":"markdown","5e3bad97":"markdown","a27146c1":"markdown","86050a15":"markdown","53ad6446":"markdown","01308674":"markdown","d247ec7c":"markdown"},"source":{"30cf759c":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import backend as K","10485afe":"df = pd.read_csv('\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv', delimiter=',', encoding='latin-1')\nprint(df)\n\nY = df['v1']\nX = df['v2']\n\nle = LabelEncoder()\nY = le.fit_transform(Y)\nY = Y.reshape(-1,1)\n","1f2bf088":"X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0)\nX_train = X_train.tolist()\nX_test = X_test.tolist()\n\nX_train = [text.lower() for text in X_train]\nX_test = [text.lower() for text in X_test]\n\nlabel2idx = {\n    'ham':0,\n    'spam':1\n}","00cc6d32":"maxlen = 150\nvocab_size = 10000\ntokenizer = Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(X_train)\ntrain_seq = tokenizer.texts_to_sequences(X_train)\ntrain_pad = pad_sequences(train_seq, maxlen=maxlen, truncating='post')\ntest_seq = tokenizer.texts_to_sequences(X_test)\ntest_pad = pad_sequences(test_seq, maxlen=maxlen, truncating='post')\n","6a9c9941":"model = keras.Sequential([\n    keras.layers.Embedding(vocab_size, 128, input_length=maxlen),\n    keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True)),\n    keras.layers.Bidirectional(keras.layers.LSTM(32)),\n    keras.layers.Dense(256, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(train_pad, y_train, epochs=10, validation_data=(test_pad, y_test))","c88a9d02":"model.summary()\nmodel.evaluate(test_pad, y_test)\n\n","625f1b54":"def plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel('epochs')\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n\nplot_graphs(history, 'loss')\nplot_graphs(history, 'accuracy')","4983cb0d":"tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5, max_df=0.5)\nX_train_tfidf = tfidf.fit_transform(X_train)\nX_test_tfidf = tfidf.transform(X_test)","36b04a7e":"clf = LogisticRegression(penalty='l2', C=10).fit(X_train_tfidf, y_train)\ny_pred = clf.predict(X_test_tfidf)\nprint(y_pred[:1000])\ny_pred_labels = []\n\n#Converting output of classifier back in terms of input ham\/spam labels.\nfor i in y_pred:\n    for label, idx in label2idx.items():\n        if i==idx:\n            y_pred_labels = np.append(y_pred_labels, label)\nprint(y_pred_labels[:1000])\n\n        \n    \n\n#Final accuracy on testing.\nprint(accuracy_score(y_test, y_pred))","baee7934":"# Training using logistic regression classifier(due to large no. of features)\n","5e3bad97":"#  **Performing an alternate training using tfidf vectorization for text.**","a27146c1":"# Summary of training and accuracy on testing.","86050a15":"# Importing requisite libraries.","53ad6446":"# Tokenizing and Padding of text","01308674":"# Constructing the RNN architecture \n","d247ec7c":"# Plotting accuracy and loss vs no. of epochs"}}