{"cell_type":{"c7c9fd27":"code","a42fa590":"code","6b3e170f":"code","1ec51e5c":"code","393e3126":"code","9342692e":"code","0c906bcf":"code","63f24883":"code","ccc0dc55":"code","5f9f9620":"code","c4f58c8d":"code","62e42b90":"code","c0700c13":"code","abadf278":"code","8be7ad5b":"code","1a8b7a11":"code","3f1b304a":"code","6ecfa289":"code","d4a83bbf":"code","dec6aa8b":"code","8c7cd498":"code","5ddbf361":"code","e2f95228":"code","c400cb27":"code","f6a1bc2e":"code","5f8b13f6":"code","0f76d6ad":"code","1b508249":"code","ea2e0942":"code","6f5fb706":"code","c34a9de2":"code","e1833352":"code","db911345":"code","af6a48da":"code","0b221e55":"code","78e914ca":"code","5c2ccc3b":"code","10a13634":"code","b4510bee":"code","6f575968":"code","98c825ff":"markdown","3a2f84eb":"markdown","d783a9ca":"markdown","e11b446e":"markdown","57281366":"markdown","702ba313":"markdown","906ce059":"markdown","78066119":"markdown","3c69eb93":"markdown","6a117959":"markdown","27114774":"markdown","1e1056c5":"markdown","9b678e37":"markdown","057b7183":"markdown","072cdd24":"markdown","47482679":"markdown","46c0d85b":"markdown","92d737fa":"markdown","d14f86d2":"markdown","49b92f51":"markdown","e1e82048":"markdown","aa6c3ae3":"markdown","151c3c3f":"markdown","37c50884":"markdown","c385f010":"markdown","1aecc4b1":"markdown","f1d8cc90":"markdown","1bd685dd":"markdown","f21f39d4":"markdown","248cd3fd":"markdown"},"source":{"c7c9fd27":"import pandas as pd\nimport numpy as np\nimport string\nimport spacy\nfrom wordcloud import WordCloud\nimport re\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, roc_curve, roc_auc_score","a42fa590":"# reading dataset\norig_df = pd.read_csv(r\"..\/input\/amazon-fine-food-reviews\/Reviews.csv\")\norig_df.head()","6b3e170f":"# plotting histogram of scores\norig_df['Score'].hist()\nplt.title(\"Histogram of Scores in original data\")\nplt.xlabel(\"Scores\")\nplt.ylabel(\"Number of data rows\")","1ec51e5c":"# making new dataframe of required data\nnew_df = orig_df[['Text','Score']]\nnew_df.head()","393e3126":"# setting values of scores > 3 as positive(1) and rest as negative sentiment(0) \nnew_df['Score'] = np.where(new_df['Score'] > 3, 1, 0)\n\n# dropping duplicates\nnew_df.drop_duplicates(inplace = True)\nnew_df.head()","9342692e":"#sampling 20,000 rows and keeping score classes size equal (i.e. equal number of 0's and 1's)\nnew_df = new_df.groupby(\"Score\").sample(n=10000, random_state=1) ","0c906bcf":"#shuffling the rows\nnew_df = new_df.sample(frac = 1).reset_index(drop=True) ","63f24883":"# convert strings to lowercase\nnew_df['Text'] = new_df['Text'].str.lower()","ccc0dc55":"new_df.head()","5f9f9620":"# Loading model\nnlp = spacy.load('en_core_web_lg')\n\n# removing negative stopwords from the inbuilt list of stopwords\nremove_stopwords = [word for word in nlp.Defaults.stop_words if \"n't\" in word or 'no' in word or 'n\u2018t' in word or 'n\u2019t' in word or 'nt' in word]\nremove_stopwords_extra = ['serious','against','out','well','top','full','empty','neither','never','nevertheless']\nremove_stopwords.extend(remove_stopwords_extra)\nfor i in remove_stopwords:\n    nlp.Defaults.stop_words.remove(i)\n    \n# Lemmatization with stopwords removal\nnew_df['Lemmatized_Text'] = new_df['Text'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))","c4f58c8d":"new_df.head()","62e42b90":"# HTML tag removal\nnew_df['Lemmatized_Text'] = new_df['Lemmatized_Text'].replace(to_replace ='<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', value = ' ', regex = True)","c0700c13":"# Punctuation removal\nnew_df['Lemmatized_Text'] = new_df['Lemmatized_Text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))","abadf278":"# Punctuation removal\nnew_df['Lemmatized_Text'] = new_df['Lemmatized_Text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))","8be7ad5b":"# Number removal\nnew_df['Lemmatized_Text'] = new_df['Lemmatized_Text'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))","1a8b7a11":"new_df['Text'] = [i for i in new_df['Lemmatized_Text'].str.split()]\nnew_df['Text'] = new_df['Text'].apply(' '.join)\nnew_df.drop('Lemmatized_Text', axis=1, inplace=True)\nnew_df.head()","3f1b304a":"tfidf=TfidfVectorizer(analyzer='word',ngram_range=(2,3))\ndata_p=tfidf.fit_transform(new_df[new_df['Score'] == 1]['Text'])\ndtm_p = pd.DataFrame(data_p.toarray(), columns=tfidf.get_feature_names())\ndtm_p.index=new_df[new_df['Score'] == 1].index\ndtm_p","6ecfa289":"# Top 10 positive words\ndtm_p.sum().nlargest(10).plot.bar()\nplt.title(\"Top 10 most frequent words in postive reviews\")\nplt.xlabel(\"Words\")\nplt.ylabel(\"Occurence\")\nprint(list(dtm_p.sum().nlargest(10).index))","d4a83bbf":"tfidf=TfidfVectorizer(analyzer='word',ngram_range=(4,4))\ndata_n=tfidf.fit_transform(new_df[new_df['Score'] == 0]['Text'])\ndtm_n = pd.DataFrame(data_n.toarray(), columns=tfidf.get_feature_names())\ndtm_n.index=new_df[new_df['Score'] == 0].index\ndtm_n","dec6aa8b":"dtm_n.sum().nlargest(10).plot.bar()\nplt.title(\"Top 10 most frequent words in negative reviews\")\nplt.xlabel(\"Words\")\nplt.ylabel(\"Occurence\")\nprint(list(dtm_n.sum().nlargest(10).index))","8c7cd498":"top_postive_words = list(dtm_p.sum().nlargest(10).index)\nfor top_words in top_postive_words:\n    pos_sent_with_top_words = []\n    for pos_sent in new_df[new_df['Score'] == 1]['Text']:\n        if top_words in pos_sent:\n            pos_sent_with_top_words.append(pos_sent.replace(top_words,''))\n    df = pd.DataFrame(pos_sent_with_top_words,columns=['Txt'])\n    tfidf=TfidfVectorizer(analyzer='word',ngram_range=(2,3))\n    data_pcw=tfidf.fit_transform(df['Txt'])\n    dtm_pcw = pd.DataFrame(data_pcw.toarray(), columns=tfidf.get_feature_names())\n    dtm_pcw.index=df.index\n    lst = list(dtm_pcw.sum().nlargest(15).index)\n    print(top_words,': ',lst,'\\n')\n            ","5ddbf361":"top_negative_words = list(dtm_n.sum().nlargest(4).index)\nfor top_words in top_negative_words:\n    neg_sent_with_top_words = []\n    for neg_sent in new_df[new_df['Score'] == 0]['Text']:\n        if top_words in neg_sent:\n            neg_sent_with_top_words.append(neg_sent.replace(top_words,''))\n    df = pd.DataFrame(neg_sent_with_top_words,columns=['Txt'])\n    tfidf=TfidfVectorizer(analyzer='word',ngram_range=(4,4))\n    data_ncw=tfidf.fit_transform(df['Txt'])\n    dtm_ncw = pd.DataFrame(data_ncw.toarray(), columns=tfidf.get_feature_names())\n    dtm_ncw.index=df.index\n    lst = list(dtm_ncw.sum().nlargest(15).index)\n    print(top_words,': ',lst,'\\n')\n            ","e2f95228":"arr_len = [i for i in range(20000)]","c400cb27":"char_count = [len(i) for i in new_df['Text']]\nplt.scatter(arr_len,char_count)\n","f6a1bc2e":"np.percentile(char_count, 98.48)","5f8b13f6":"word_count = [len(i.split()) for i in new_df['Text']]\nplt.scatter(arr_len,word_count)","0f76d6ad":"np.percentile(word_count, 99.27)","1b508249":"# function to generate word cloud\ndef generate_wcloud(text):\n    stopwords = nlp.Defaults.stop_words\n    \n    wordcloud = WordCloud(stopwords=stopwords, background_color='white',collocation_threshold = 3 )\n    wordcloud.generate(text),\n    \n    plt.figure(figsize=(15,7))\n    plt.axis('off')\n    plt.imshow(wordcloud, interpolation='bilinear')\n    return plt.show()\n\n# word cloud for postive sentences\npos = new_df.loc[new_df.Score==1].Text\ntext = \" \".join(review for review in pos.astype(str))\n\ngenerate_wcloud(text)","ea2e0942":"# word cloud for negative sentences\nneg = new_df.loc[new_df.Score==0].Text\ntext = \" \".join(review for review in neg.astype(str))\n\ngenerate_wcloud(text)","6f5fb706":"# function to convert sentence into vector using spaCy's word2vec vectorizer\ndef get_vec(x):\n    doc = nlp(x)\n    vec = doc.vector\n    return vec    ","c34a9de2":"# vectorize the text\nnew_df['vec'] = new_df['Text'].apply(lambda x: get_vec(x))\nnew_df.head()","e1833352":"# reshaping the vector in appropriate shape\nX = new_df[\"vec\"].to_numpy()\nX = X.reshape(-1,1)\nX.shape","db911345":"X = np.concatenate(np.concatenate(X,axis=0),axis=0).reshape(-1,300)\nX.shape","af6a48da":"y = new_df.Score\n\n# splitting the data into training and testing with 75:25 split ratio\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)","0b221e55":"print(\"Shape of training data X: \",X_train.shape)\nprint(\"Shape of testing data X: \",X_test.shape)","78e914ca":"# Load model and set parameters\nclf = LogisticRegression(C=1, solver=\"liblinear\", random_state=1)\n# fitting the model\nclf.fit(X_train,y_train)","5c2ccc3b":"print(\"Training Accuracy: \",clf.score(X_train,y_train))\nprint(\"Testing Accuracy: \",clf.score(X_test, y_test))","10a13634":"y_pred = clf.predict(X_test)\nprint(\"Precision score on test data: \",precision_score(y_test,y_pred))\nprint(\"Recall score on test data: \",recall_score(y_test,y_pred))\nprint(\"F1 score on test data: \",f1_score(y_test,y_pred))","b4510bee":"fpr, tpr, _ = roc_curve(y_test,  y_pred)\nauc = roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","6f575968":"# Confusion matrix\nconfusion_matrix(y_test, y_pred)","98c825ff":"Majority of the sentences have a character count less than 1000 characters (98.48% of time). A very few sentences haev more than 2500 characters","3a2f84eb":"Some dominant words in negative sentences are: taste, waste, good, smell like, taste, bad, ......","d783a9ca":"## Lemmatization and stopwords removal","e11b446e":"## Split the data","57281366":"Here, we are creating a document term matrix, which holds the frequency of words present in the corpus. We are using TF-IDF vectorizer to make this matrix. Also, there may be some positive words which are more than 1 word length, eg. 'Tastes good','highly recommended' etc. To include these words, we need to use n-grams. In this case, the best results for positive reviews are with 2-gram and 3-gram words. ","702ba313":"AUC score is 0.79 which is good. (Closer to 1, the better the classifier is)","906ce059":"Some dominant words in postive sentences are: Highly recommend, like, taste, good, ......","78066119":"To find top 15 co-occuring words with top positve words, we create a list of all the strings that contains top positive words. Then we create a document term matrix for all the postive sentences in the list and filter out top 15 co-occuring words related to a top positve word","3c69eb93":"## Top 15 co-occuring words with top positive review words","6a117959":"## Character count distribution in Text column","27114774":"## Word count distribution in Text column","1e1056c5":"## Word cloud for positive sentences","9b678e37":"# Training and Testing","057b7183":"To find top 15 co-occuring words with top negative words, we create a list of all the strings that contains top negative words. Then we create a document term matrix for all the negative sentences in the list and filter out top 15 co-occuring words related to a top negative word","072cdd24":"Precision of 81% means that if classifer classifies that a sentence is positive, 81% of the time, it is correct\n\nRecall of 79.3% means that 79.3% of the actual postive sentences were identified correctly","47482679":"## Document term matrix for negative reviews - Top 10 most frequent words","46c0d85b":"## Word cloud for negative sentences","92d737fa":"# EDA for Text column","d14f86d2":"# Sampling 20k rows and keeping size of the classes in the target equal ","49b92f51":"## Removing HTML tags, punctuations and numerical vals","e1e82048":"## Top 15 co-occuring words with top negative review words","aa6c3ae3":"## Document term matrix for positive reviews - Top 10 most frequent words","151c3c3f":"Majority of the sentences have a character count less than 200 characters (99.27% of times). A very few sentences have more than 400 characters(0.04%)","37c50884":"# Reading the data and preparing dataframe as per requirement","c385f010":"## Word2Vec vectorization for converting text to numerical vector","1aecc4b1":"There are some positive words in the negative word cloud. The most reasonable explanation for this is the misclassification of the 1-5 ranges scores into positves and negatives. There are some sentences rated 3 which sounds like a positive sentence rather than a negative. But the classification threshold suggests to classify it as a negative.","f1d8cc90":"There are some negative words which are more than 1 word length, eg. 'not good', \"don't like\", etc. To include these words, we need to use n-grams. In this case, the best results for positive reviews are with 4-gram words. ","1bd685dd":"# Adding libraries","f21f39d4":"# Processing data before EDA","248cd3fd":"## Training a Logistic regression model"}}