{"cell_type":{"8683942d":"code","014cf079":"code","73f74848":"code","eb17ca3e":"code","d2d6dff9":"code","516673b2":"code","dca71c50":"code","29a2bafa":"code","4e6d09c7":"code","dad29254":"code","202af193":"code","6f940c64":"code","5d378133":"code","ff435456":"code","a19c2205":"code","822e639f":"code","35d515c2":"code","956f16a9":"code","ee17c3c7":"code","5000b61e":"code","781c6f4e":"markdown","58b37e61":"markdown","b35f64bd":"markdown","2ef42478":"markdown","586fea25":"markdown","cf58ddfb":"markdown","45921013":"markdown","126e0fa5":"markdown","cf29e329":"markdown","7dfa5c51":"markdown","7a24b9aa":"markdown","afa0f0a7":"markdown","c0e894cd":"markdown"},"source":{"8683942d":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nfrom numpy import percentile\nfrom numpy import nanmedian\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import skew\nfrom scipy.stats import mode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, RobustScaler, StandardScaler\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.scorer import make_scorer\nfrom sklearn.model_selection import KFold\n\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom catboost import Pool, CatBoostRegressor\n\n## Hyperopt modules\nimport gc\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nfrom functools import partial\n\nfrom sklearn.externals import joblib \npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)","014cf079":"df = pd.read_csv('..\/input\/TTiDS20\/train.csv').drop(columns=['Unnamed: 0'])\ntest = pd.read_csv('..\/input\/TTiDS20\/test_no_target.csv').drop(columns=['Unnamed: 0'])\nzipcodes = pd.read_csv('..\/input\/TTiDS20\/zipcodes.csv').drop(columns=['Unnamed: 0'])\nsample_submission = pd.read_csv('..\/input\/TTiDS20\/sample_submission.csv')","73f74848":"#skew<1.88 - norm\nprint(df.price.skew())\n\nplt.figure(figsize=(8,15))\n\nplt.subplot(3,1,1)\nplt.title('Car Price Distribution Plot')\nsns.distplot(df.price)\n\nplt.subplot(3,1,2)\nplt.title('Car Price Spread')\nsns.boxplot(y=df.price)\n\nplt.subplot(3,1,3)\n_ = stats.probplot(df['price'], plot=plt)\nplt.title(\"Probability plot: SalePrice\")\nplt.show()\n","eb17ca3e":"df['price'] = np.log1p(df['price'])","d2d6dff9":"print(df.price.skew())\n\nplt.figure(figsize=(8,15))\n\nplt.subplot(3,1,1)\nplt.title('Car Price Distribution Plot')\nsns.distplot(np.log1p(df.price))\n\nplt.subplot(3,1,2)\nplt.title('Car Price Spread')\nsns.boxplot(np.log1p(df.price))\n\nplt.subplot(3,1,3)\n_ = stats.probplot(df['price'], plot=plt)\nplt.title(\"Probability plot: SalePrice\")\nplt.show()","516673b2":"df_zip = zipcodes.groupby('zipcode').agg({'city': ['count'],\n                                          'latitude': ['mean'],\n                                          'longitude': ['mean']})\n\ndf_zip.columns = df_zip.columns.map('_'.join)\ndf_zip = df_zip.fillna(0).reset_index()\ndf_zip['zip_size'] = np.where(df_zip.city_count>20, 'L', np.where(df_zip.city_count>5, 'M', 'S'))\n\ndf = df.merge(df_zip, on ='zipcode', how = 'left')\ncol_na = ['city_count', 'latitude_mean', 'longitude_mean']\nfor i in col_na:\n    df[i].fillna(df[i].mean(), inplace = True)\n    \n    \ntest = test.merge(df_zip, on ='zipcode', how = 'left')\nfor i in col_na:\n    test[i].fillna(test[i].mean(), inplace = True)    \n\ndf.drop('zipcode', axis = 1, inplace = True)\ntest.drop('zipcode', axis = 1, inplace = True)","dca71c50":"def reg_year_transform(my_df, reg_col):\n    my_df[reg_col] = my_df[reg_col].apply(lambda x: '19' + str(x) if len(str(x))==2 else \n                                                    '200' + str(x) if len(str(x) ) == 1 else x)\n    my_df[reg_col] = my_df[reg_col].astype('int')\n    my_df['IsOld'] = np.where(my_df[reg_col] < 2000, 1, 0)\n    my_df['YearCar'] = my_df[reg_col].max() - my_df[reg_col]\n    my_df['reg_year_str'] = my_df[reg_col].astype('str')\n    return my_df\n    \ndf = reg_year_transform(df, 'registration_year')  \ntest = reg_year_transform(test, 'registration_year') ","29a2bafa":"def fill_missings(my_df):    \n    my_df['zip_size'] = my_df['zip_size'].fillna(my_df['zip_size'].mode()[0])\n    my_df['gearbox'] = my_df['gearbox'].fillna(my_df['gearbox'].mode()[0])\n    my_df['type'] = my_df['type'].fillna('other')\n    my_df['fuel'] = my_df['fuel'].fillna(my_df['fuel'].mode()[0])\n    my_df['damage'] = my_df['damage'].fillna(0.0)\n    my_df['model'] = my_df['model'].fillna(my_df.groupby(['brand'])['model']\\\n                          .transform(lambda x: 'other' if mode(x)[0][0] == 0 else mode(x)[0][0]))\n    return my_df\n    \ndf = fill_missings(df)\ntest = fill_missings(test) ","4e6d09c7":"%%time\ncats = ['insurance_price', 'engine_capacity']\nfor cat in cats:\n    print(\"Category: {}\".format(cat))\n    tmp = df.groupby(['model', 'type', 'registration_year']).agg({cat: ['mean', 'median']}) \n    tmp.columns = tmp.columns.map('_'.join)\n    \n    df = df.merge(tmp, how='left', on=['model', 'type', 'registration_year'])\n    df[cat + '_mean'].fillna(np.nanmean(df[cat + '_mean']), inplace = True)\n    df[cat + '_median'].fillna(np.nanmedian(df[cat + '_median']), inplace = True)\n    df[cat].fillna(np.nanmedian(df[cat + '_median']), inplace = True)\n    \n    test = test.merge(tmp, how='left', on=['model', 'type', 'registration_year'])\n    test[cat + '_mean'].fillna(np.nanmean(df[cat + '_mean']), inplace = True)\n    test[cat + '_median'].fillna(np.nanmedian(df[cat + '_median']), inplace = True)\n    test[cat].fillna(np.nanmedian(test[cat + '_median']), inplace = True)\n","dad29254":"def addlogs(my_df, ls):\n    m = my_df.shape[1]\n    for l in ls:\n        my_df = my_df.assign(newcol=pd.Series(np.log(1.01+my_df[l])).values)   \n        my_df.columns.values[m] = l + '_log'\n        m += 1\n    return my_df\n\ndef addSquared(my_df, ls):\n    m = my_df.shape[1]\n    for l in ls:\n        my_df = my_df.assign(newcol=pd.Series(my_df[l]*my_df[l]).values)   \n        my_df.columns.values[m] = l + '_sq'\n        m += 1\n    return my_df \n\nnum_col_transf_list = ['engine_capacity','insurance_price',\n                        'city_count', 'latitude_mean', 'longitude_mean', 'YearCar',\n                        'insurance_price_mean', 'insurance_price_median',\n                        'engine_capacity_mean', 'engine_capacity_median']\n\n\ndf = addlogs(df, num_col_transf_list)\ndf = addSquared(df, num_col_transf_list)\ntest = addlogs(test, num_col_transf_list)\ntest = addSquared(test, num_col_transf_list)","202af193":"cat_col = ['type',  'gearbox', 'model', 'fuel', 'brand', 'damage', 'reg_year_str', 'zip_size', 'IsOld']\n\nLE_mapper = {}\nfor category in cat_col:\n    LE_mapper[category] = LabelEncoder()\n    df[category] = LE_mapper[category].fit_transform(df[category])\n\nfor item in LE_mapper.items():  \n    category = item[0]\n    le = item[1]\n    le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n    test[category] = test[category].apply(lambda x: le_dict.get(x, -99999999))","6f940c64":"# prep for modeling\n\nX = df[df.columns.difference(['price'])]\ny = df['price']\ncategorical_features = [i for i, e in enumerate(X.columns) if e in cat_col]","5d378133":"# create custom scoring\n\ndef mape(y_true, y_pred):\n    y_pred[y_pred < 0 ] = 0\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\nmape_scorer = make_scorer(mape, greater_is_better = False)","ff435456":"# set diapason of parameters\n\nlgbm_space = {\n        'boosting_type': hp.choice('boosting_type', ['dart','gbdt','goss']),  \n        'learning_rate': hp.choice('learning_rate', np.arange(0.005, 0.1005, 0.005)),\n        'n_estimators': hp.choice('n_estimators', np.arange(100, 7001, 25, dtype=int)),\n        'max_depth': hp.choice('max_depth', np.arange(5, 70, 2, dtype=int)),\n        'num_leaves': hp.choice('num_leaves', [3,5,7,15,31,50,75,100]),\n    \n        'lambda_l1':  hp.loguniform('lambda_l1', -3, 2),\n        'lambda_l2':  hp.loguniform('lambda_l2', -3, 2),\n    \n#         'num_leaves': hp.choice('num_leaves', np.arange(5, 31, 1, dtype=int)),    \n#         'bagging_fraction': hp.uniform('bagging_fraction', 0, 1), \n#         'feature_fraction': hp.uniform('feature_fraction', 0, 1),   \n#         'min_data_in_leaf': hp.choice('min_data_in_leaf', np.arange(2, 31, 1, dtype=int)),\n#         'max_bin': hp.choice('max_bin', np.arange(200, 2000, 10, dtype=int)),\n#         'bagging_freq': hp.choice('bagging_freq', np.arange(0, 11, 1, dtype=int)),\n#         'min_child_weight': hp.uniform('min_child_weight', 0.1, 10),\n#         'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.),\n#         'subsample': hp.uniform('subsample', 0.5, 1.),\n#         'bagging_fraction' : hp.uniform('bagging_fraction', 0.01, 1.0),\n#         'feature_fraction' :  hp.uniform('feature_fraction', 0.5, 1.0),\n#         'min_gain_to_split' : hp.uniform('min_gain_to_split', 0.0, 1.0),\n    }","a19c2205":"lgbm_model = lgb.LGBMRegressor(objective='regression', verbosity=-1, nthread=-1, random_state=42)\n\n# model and fit params\nparams = dict(\n        objective='regression',\n        verbosity=-1,\n        nthread=-1,\n        random_state=42,\n        categorical_feature=categorical_features,\n)\n\nfit_params = {\n        'categorical_feature':categorical_features,\n         }","822e639f":"def lgbm_objective(params):\n    est = lgb.LGBMRegressor(verbosity=-1,nthread=-1,random_state=42,**params) \n\n    score = cross_val_score(\n                est,\n                X.values, y.values,\n                scoring = mape_scorer,\n                cv = KFold(5),\n                n_jobs= -1,\n                fit_params = fit_params,\n    )\n    print(abs(score))\n    return abs(score.mean())\ntrials = Trials()","35d515c2":"#  model_params = lgbm_model.get_params()\n#  hp_lgbm_best = fmin(\n#      fn=lgbm_objective,\n#      space=lgbm_space,\n#      algo=tpe.suggest,\n#      max_evals=25,\n#      trials=trials\n#  )\n#\n#  best_params_lgbm = space_eval(lgbm_space, hp_lgbm_best)","956f16a9":"filename = '..\/input\/carsdata\/model_lgbm.joblib'\nmodel_lgbm = joblib.load(filename)","ee17c3c7":"model_lgbm","5000b61e":"test['pred'] = model_lgbm.predict(test)\ntest['pred'] = np.exp(test['pred']) - 1 \n\nsample_submission['Predicted'] = test['pred']\nsample_submission.to_csv('sample_submission.csv', index=False)","781c6f4e":"__Missings__","58b37e61":"# Data loading","b35f64bd":"__Hyperopt for tunning param__","2ef42478":"__Zipcode in process__","586fea25":"__Add transformations for some columns__","cf58ddfb":"__Registration_year__","45921013":"For some of the predictors added their squares (i.e. we have predictor X and we add predictor X^2) and Log transformations.  Adding squares or log - is motivated by non-linearities in\"predictor vs. log of prices\" - I assume that similar non-linearities will also hold when we add predictor to multi-dimensional model.","126e0fa5":"Missings in categorical predictors I filled in with modes or other typical values. \nI also have noticed that 'NA' values have similar avg prices with mode values of these columns.\n\nPay attention that __model__ is inputted of mode groupby __brand__. But some brands don't have any model, so I just called these models - 'other'. The best option is for every brand input different model, but 'other' models ~ .001% of the data, so I didn`t care.  ","cf29e329":"Log-transform of the dependent variable. Many features from the dataset have distribution close to log-normal, i.e. heavy-tailed. Usually it is better to predict smth which is not heavy tailed (I honestly tried to predict price per se, and results were worse).","7dfa5c51":"__engine_capacity__ , __insurance_price__ are highly correlated with the target. So try to input values and create some new columns.","7a24b9aa":"# Modeling","afa0f0a7":"# Preprocessing","c0e894cd":"__LabelEncoding__"}}