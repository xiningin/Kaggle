{"cell_type":{"045f537c":"code","67838350":"code","9c2e9e13":"code","a0b799d1":"code","be89480d":"code","e14d4398":"code","8e112b2a":"code","4bdb72ca":"code","08e1a021":"code","2b1a6221":"code","8a9e66fa":"code","1933416b":"code","68defec2":"code","63902ac7":"code","e425804b":"code","c5f46609":"code","594e99fc":"code","bdbdf33b":"code","16837890":"code","cf9fb46c":"code","453cd79e":"code","7ce345e6":"code","78ff3f04":"code","f8788ed2":"code","1bb62147":"code","50fc2871":"code","9daa0f4a":"code","da70b33d":"code","130014f1":"code","050053b1":"code","d549e526":"code","9cc474df":"code","7e283503":"code","78f49f2b":"code","20f03db3":"code","a9be921a":"code","52e9f796":"code","a2719207":"code","09923dbd":"code","4e639957":"markdown","3065cb0a":"markdown","d7cafd8e":"markdown","4dd5c4d7":"markdown","38425faf":"markdown","0ff22deb":"markdown","6078be6f":"markdown","08bf2300":"markdown","5dbcdaaf":"markdown","16d195cc":"markdown","4a134617":"markdown","1f104033":"markdown","0e266f06":"markdown","2a749439":"markdown","fbf109be":"markdown","68295fde":"markdown","d8e148fa":"markdown","883020bb":"markdown","91fb2e52":"markdown","669f7828":"markdown","2969e112":"markdown","0dc639c8":"markdown","4ce17744":"markdown","547d595f":"markdown","64e17b2f":"markdown","09a79ca6":"markdown","f5b09e16":"markdown","746442d2":"markdown","e2377dfd":"markdown","25da6236":"markdown","561ad782":"markdown","a887c1c7":"markdown","a797d457":"markdown","010ed884":"markdown","456995ac":"markdown"},"source":{"045f537c":"#pandas\r\nimport pandas as pd \r\n\r\n#numpy\r\nimport numpy as np \r\n\r\n#matplotlib\r\nimport matplotlib.pyplot as plt \r\n\r\n#seaborn\r\nimport seaborn as sns\r\nsns.set_theme(style=\"darkgrid\")\r\n\r\n#sklearn\r\nfrom sklearn.metrics import f1_score\r\nfrom sklearn.preprocessing import LabelEncoder\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.metrics import classification_report\r\nfrom sklearn.metrics import roc_auc_score\r\n","67838350":"stroke_df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\r\nstroke_df.head()","9c2e9e13":"categorical_vars = list()\r\nnumerical_vars = list()\r\n\r\nfor i in stroke_df.columns:\r\n    if stroke_df[i].dtype =='object':\r\n        categorical_vars.append(i)\r\n    else:\r\n        numerical_vars.append(i)\r\n","a0b799d1":"print(categorical_vars)","be89480d":"print(numerical_vars)","e14d4398":"stroke_df.drop('id',axis=1,inplace=True)\r\nstroke_df.head()","8e112b2a":"stroke_df.isnull().sum()","4bdb72ca":"print(\"Total Rows In BMI column :\",len(stroke_df.bmi))\r\nprint(\"Total null values present in bmi column :\",stroke_df.bmi.isnull().sum())","08e1a021":"stroke_df['bmi'] = stroke_df.bmi.replace(np.NAN,stroke_df.bmi.mean())","2b1a6221":"stroke_df.isnull().sum()","8a9e66fa":"stroke_df.head()","1933416b":"sns.countplot(x='stroke',data=stroke_df)\r\nplt.title(\"Countplot for Stroke\",{'fontsize':20});","68defec2":"print(\"Total Observations :\",stroke_df.shape[0])\r\nprint(\"Patients does not have stroke :\",stroke_df.stroke.value_counts()[0])\r\nprint(\"Patients have stroke :\",stroke_df.stroke.value_counts()[1])\r\nprint(\"Event Rate :\",(stroke_df.stroke.value_counts()[1] \/ stroke_df.shape[0])*100)","63902ac7":"fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharey=True)\r\nfig.suptitle('Distribution CountPlot Some Features')\r\n\r\nsns.countplot(ax=axes[0][0], x=stroke_df['hypertension'],palette=\"viridis\")\r\n\r\nsns.countplot(ax=axes[0][1], x=stroke_df['work_type'],palette=\"rocket\");\r\n\r\nsns.countplot(ax=axes[1][0], x=stroke_df['ever_married'],palette=\"husl\");\r\n\r\nsns.countplot(ax=axes[1][1], x=stroke_df['work_type'],palette=\"husl\");\r\n","e425804b":"stroke_df.work_type[stroke_df.stroke == 1].value_counts()","c5f46609":"stroke_df.smoking_status.unique()","594e99fc":"plt.figure(figsize=(7,8))\r\nlabels = [ \"formely smoked\" , \"neber smoked\",\"smokes\",\"unknown\"]\r\nplt.pie(x=stroke_df.smoking_status[stroke_df.stroke == 1].value_counts(),\r\n        # explode = (0, 0, 0, 0.2),\r\n        autopct='%1.1f%%',\r\n        shadow=True, colors=['plum','lightpink','lawngreen','cyan']);\r\nplt.legend(labels,bbox_to_anchor=(1.05,1.025), loc=\"upper left\");\r\nplt.title(\"Patients have stroke based on work type\",{'fontsize':20});","bdbdf33b":"plt.figure(figsize=(7,8))\r\nlabels = [ \"Private\" , \"Self-employed\",\"Govt_job\",\"children\"]\r\nplt.pie(x=stroke_df.work_type[stroke_df.stroke == 1].value_counts(),\r\n        explode = (0, 0, 0, 0.2),\r\n        autopct='%1.1f%%',\r\n        shadow=True, colors=['royalblue','darkorange','springgreen','lightcyan','lavender']);\r\nplt.legend(labels,bbox_to_anchor=(1.05,1.025), loc=\"upper left\");\r\nplt.title(\"Patients have stroke based on work type\",{'fontsize':20});","16837890":"stroke_df.head()","cf9fb46c":"X = stroke_df.drop('stroke',axis=1)\r\ny = stroke_df.stroke","453cd79e":"X.age = round(X.age)","7ce345e6":"encoder = LabelEncoder()","78ff3f04":"objList = X.select_dtypes(include = \"object\").columns\r\nfor feat in objList:\r\n    X[feat] = encoder.fit_transform(X[feat])","f8788ed2":"from imblearn.over_sampling import SMOTE\r\nsmote = SMOTE()\r\n\r\nx_smote, y_smote = smote.fit_resample(X, y)","1bb62147":"print('Original dataset shape', X.shape)\r\nprint('Resample dataset shape', x_smote.shape)","50fc2871":"X_train,X_test,y_train,y_test = train_test_split(x_smote,y_smote,test_size=0.28)","9daa0f4a":"scalar = StandardScaler()\r\nX_train_scaled = scalar.fit_transform(X_train)\r\nX_test_scaled = scalar.fit_transform(X_test)","da70b33d":"log_reg = LogisticRegression()\r\nlog_reg.fit(X_train_scaled,y_train)\r\nlog_reg.score(X_test_scaled,y_test)","130014f1":"rf = RandomForestClassifier()\r\nrf.fit(X_train_scaled,y_train)\r\nrf.score(X_test_scaled,y_test)","050053b1":"rf_pred = rf.predict(X_test_scaled)\r\nlog_pred = log_reg.predict(X_test_scaled)\r\n\r\nprint(\"Classifiaction Report for Random Forest\")\r\nprint(classification_report(y_test,rf_pred))\r\nprint(\"******************************************************\")\r\nprint(\"Classification Report for Logistic Regression\")\r\nprint(classification_report(y_test,log_pred))","d549e526":"class_names = [0,1]\r\nfig,ax = plt.subplots()\r\ntick_marks = np.arange(len(class_names))\r\nplt.xticks(tick_marks,class_names)\r\nplt.yticks(tick_marks,class_names)\r\n\r\ncnf_matrix = confusion_matrix(y_test,rf_pred)\r\nsns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap=\"Blues\",\r\n            fmt = 'g')\r\nax.xaxis.set_label_position('top')\r\nplt.tight_layout()\r\nplt.title(f'Heat Map for Random Forest', {'fontsize':20})\r\nplt.ylabel('Actual label')\r\nplt.xlabel('Predicted label')\r\nplt.show()","9cc474df":"class_names = [0,1]\r\nfig,ax = plt.subplots()\r\ntick_marks = np.arange(len(class_names))\r\nplt.xticks(tick_marks,class_names)\r\nplt.yticks(tick_marks,class_names)\r\n\r\ncnf_matrix = confusion_matrix(y_test,log_pred)\r\nsns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Blues',\r\n            fmt = 'g')\r\nax.xaxis.set_label_position('top')\r\nplt.tight_layout()\r\nplt.title(f'Heat Map for Logistic Regression', {'fontsize':20})\r\nplt.ylabel('Actual label')\r\nplt.xlabel('Predicted label')\r\nplt.show()","7e283503":"pred_prob1 = log_reg.predict_proba(X_test_scaled)\r\npred_prob2 = rf.predict_proba(X_test_scaled)\r\nfrom sklearn.metrics import roc_curve\r\n\r\n# roc curve for models\r\nfpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:,1], pos_label=1)\r\nfpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob2[:,1], pos_label=1)\r\n\r\n# roc curve for tpr = fpr \r\nrandom_probs = [0 for i in range(len(y_test))]\r\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)","78f49f2b":"auc_score1 = roc_auc_score(y_test, pred_prob1[:,1])\r\nauc_score2 = roc_auc_score(y_test, pred_prob2[:,1])\r\n\r\nprint(auc_score1, auc_score2)","20f03db3":"plt.style.use('seaborn')\r\n# plot roc curves\r\nplt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Logistic Regression')\r\nplt.plot(fpr2, tpr2, linestyle='--',color='green', label='Random Forest')\r\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\r\n# title\r\nplt.title('ROC curve')\r\n# x label\r\nplt.xlabel('False Positive Rate')\r\n# y label\r\nplt.ylabel('True Positive rate')\r\n\r\nplt.legend(loc='best')\r\nplt.savefig('ROC',dpi=300)\r\nplt.show();","a9be921a":"print(f1_score(y_test,log_pred))","52e9f796":"print(f1_score(y_test,rf_pred))","a2719207":"plt.figure(figsize=(9,7))\r\nfeature_imp1 = rf.feature_importances_\r\nsns.barplot(x=feature_imp1, y=X.columns)\r\n# Add labels to your graph\r\nplt.xlabel('Feature Importance Score')\r\nplt.ylabel('Features')\r\nplt.title(\"Visualizing Important Features For Random Forest \",{'fontsize':25})\r\nplt.show();\r\nfeature_dict = {k:v for (k,v) in zip(X.columns,feature_imp1)}","09923dbd":"import pickle\r\nwith open('stroke.sav','wb') as f:\r\n    pickle.dump(rf,f)","4e639957":"## **Inference :**\r\n## Based on distribution the people whos work type is private having stroke as compared to gov job.","3065cb0a":"## SMOTE algorithm works in 4 simple steps:\r\n## <ul>\r\n## <li>Choose a minority class as the input vector\r\n## <li>Find its k nearest neighbors (k_neighbors is specified as an argument in the SMOTE() function)\r\n## <li>Choose one of these neighbors and place a synthetic point anywhere on the line joining the point under consideration and its chosen neighbor\r\n## <li> Repeat the steps until data is balanced\r\n## <\/ul>","d7cafd8e":"# **Spliting Data**\r\n## To get a good prediction, divide the data into training and testing data, it is because as the name suggests you will train few data points and test few data points, and keep on doing that unless you get good results.","4dd5c4d7":"## **Logistic Regression**","38425faf":"### **Categorical Variables**","0ff22deb":"## **Distribution based on Stroke Patients**","6078be6f":"## **Clasification Report**","08bf2300":"# **Conclusion :**","5dbcdaaf":"## **Random Forest**","16d195cc":"## **Inference :**\r\n## After plotting AUC-ROC curve we can observe Random Forest curve is higher than that for the Logistic Regression ROC curve. Therefore, we can say that Random Forest did a better job of classifying the positive class in the dataset.","4a134617":"# **Handling Missing Values**\r\n## There are many ways to handle missing values.\r\n## One could be delete rows in which we have null values present.\r\n## But because of this we can can loss lot of information\r\n## Another way is replace null vvalues with mean\/median.\r\n## The secod method is effective when dataset is numeric and continous & good news is our bmi column fit perfectly in this condition.\r\n## So we use second method.","1f104033":"## **Logistic Regression**","0e266f06":"## **Random Forest**","2a749439":"### **Numerical Variables**","fbf109be":"## **Feature Importance For Random Forest Model**","68295fde":"# **Handling Imbalance Data**","d8e148fa":"# **Save Model**","883020bb":"## If you observe we dont requiredd id column for our prediction.So we drop it.","91fb2e52":"## **Confusion Metrices**","669f7828":"## **Identify Categorical and Numerical Fetures**","2969e112":"## **Random Forest**","0dc639c8":"## Read Data From CSV.","4ce17744":"# **EDA**","547d595f":"## **Inference :**\r\n## In our dataset there is no null values present except bmi column.","64e17b2f":"## **Logistic Regression**","09a79ca6":"## We start with reading data and then categorised categorical features and numerical features.After that we deal with missing values   in **BMI** feature.\n## Then we perform EDA on features.We conclude that we have imbalance data ie negative  examples is greater that positive class.\n## After visulization we handle imbalance data.\n## After that we move to most important part model building. Before starting to train model we split our data into train data(testing ## purpose) and test data(validation purpose) and perform **feature scaling**.\n## Random Forest and Logistic Regression models were tried.\n## To check which model perform best plot roc-auc curves along with **classifiaction report** and **confusion matrices**.\n## While **Random Forest**  win the race.\n## **I therefore selected the Random Forest as my model.**odel.**","f5b09e16":"# F1 Score\r\n## The F1 Score is the 2*((precision*recall)\/(precision+recall))","746442d2":"## **Roc-Auc Curves**","e2377dfd":"## **Count Plot for Stroke Feature**","25da6236":"## **Inference :**\r\n## Feature importance hepls to understand how model is work.\r\n## In this case we can see that Age,Average Glucoge and BMI are the most important features for our model.\r\n## Age is most signifiacant feature for our model.","561ad782":"## **Inference :**\r\n## Based on distribution of stroke feature we can say that dataset is imbalance.\r\n## We have more records of patients had no stroke as compare to patients had stroke.\r\n## Lets handle the imbalance data later.","a887c1c7":"# **Models**","a797d457":"## **1**:Patient had stroke.\r\n## **0**:Patient had no stroke.","010ed884":"## Feature Scaling ","456995ac":"## **Check for NULL Values**"}}