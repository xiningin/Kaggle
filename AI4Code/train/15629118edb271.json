{"cell_type":{"f0c4327b":"code","4781057f":"code","6b05573d":"code","c82310c0":"code","84293e17":"code","528f9998":"code","48ca646e":"code","bd813b42":"code","6ac833ce":"code","9724652e":"code","3204cc62":"code","3b9bd9f4":"code","cf334da6":"code","5c1cc55d":"code","75dd6135":"code","7d17fb64":"code","cc0f561f":"code","1371e7e2":"code","50dd7c69":"code","f0e7ad78":"code","b18d81ed":"code","cc112490":"code","7417ad7b":"code","24a074ec":"code","8ba74199":"code","a47b25e7":"code","e475c3ca":"code","9f910aa5":"code","759f277b":"code","611f2647":"markdown","85c0c5bc":"markdown","14fcccd2":"markdown","c8148ffc":"markdown","c05a4614":"markdown","8b91b460":"markdown","fb9cf019":"markdown","36357f3b":"markdown"},"source":{"f0c4327b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport random\nimport warnings\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom nltk import FreqDist\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\n#from ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import Model, optimizers\nfrom tensorflow.keras.layers import Lambda, Input, Dense, Dropout, Concatenate, BatchNormalization, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n#from googleqa_utilityscript import *\nimport seaborn as sns\nSEED = 0\n#seed_everything(SEED)\nwarnings.filterwarnings(\"ignore\")\nsns.set(font_scale=1.5)\nplt.rcParams.update({'font.size': 16})\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom scipy.sparse import hstack\nfrom sklearn.metrics import roc_auc_score, accuracy_score, log_loss\nfrom tqdm import tqdm_notebook, tqdm\nfrom scipy import stats\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport gc\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n#\/kaggle\/input\/google-quest-challenge\/sample_submission.c","4781057f":"train = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/test.csv')\n\ntrain['set'] = 'train'\ntest['set'] = 'test'\ncomplete_set = train.append(test)\n\nprint('Train samples: %s' % len(train))\nprint('Test samples: %s' % len(test))\ndisplay(train.head())","6b05573d":"samp_id = 9\nprint('Question Title: %s \\n' % train['question_title'].values[samp_id])\nprint('Question Body: %s \\n' % train['question_body'].values[samp_id])\nprint('Answer: %s' % train['answer'].values[samp_id])","c82310c0":"question_target_cols = ['question_asker_intent_understanding','question_body_critical', 'question_conversational', \n                        'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n                        'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', \n                        'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n                        'question_type_compare', 'question_type_consequence', 'question_type_definition', \n                        'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n                        'question_type_reason_explanation', 'question_type_spelling', 'question_well_written']\nanswer_target_cols = ['answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n                      'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', \n                      'answer_type_reason_explanation', 'answer_well_written']\ntarget_cols = question_target_cols + answer_target_cols\n\nprint('Question labels')\ndisplay(train.iloc[[samp_id]][question_target_cols])\nprint('Answer labels')\ndisplay(train.iloc[[samp_id]][answer_target_cols])","84293e17":"train_users = set(train['question_user_page'].unique())\ntest_users = set(test['question_user_page'].unique())\n\nprint('Unique users in train set: %s' % len(train_users))\nprint('Unique users in test set: %s' % len(test_users))\nprint('Users in both sets: %s' % len(train_users & test_users))\nprint('What users are in both sets? %s' % list(train_users & test_users))","528f9998":"train_users = set(train['answer_user_page'].unique())\ntest_users = set(test['answer_user_page'].unique())\n\nprint('Unique users in train set: %s' % len(train_users))\nprint('Unique users in test set: %s' % len(test_users))\nprint('Users in both sets: %s' % len(train_users & test_users))","48ca646e":"question_gp = complete_set[['qa_id', 'question_user_name', 'question_user_page']].groupby(['question_user_name', 'question_user_page'], as_index=False).count()\nquestion_gp.columns = ['question_user_name', 'question_user_page', 'count']\ndisplay(question_gp.sort_values('count', ascending=False).head())\n\ntrain_question_gp = train[['qa_id', 'question_user_page']].groupby('question_user_page', as_index=False).count()\ntest_question_gp = test[['qa_id', 'question_user_page']].groupby('question_user_page', as_index=False).count()\ntrain_question_gp.columns = ['question_user_page', 'Question count']\ntest_question_gp.columns = ['question_user_page', 'Question count']\n\nsns.set(style=\"darkgrid\")\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\nsns.countplot(x=\"Question count\", data=train_question_gp, palette=\"Set3\", ax=ax1).set_title(\"Train\")\nsns.countplot(x=\"Question count\", data=test_question_gp, palette=\"Set3\", ax=ax2).set_title(\"Test\")\nplt.show()","bd813b42":"answer_gp = complete_set[['qa_id', 'answer_user_name', 'answer_user_page']].groupby(['answer_user_name', 'answer_user_page'], as_index=False).count()\nanswer_gp.columns = ['answer_user_name', 'answer_user_page', 'count']\ndisplay(answer_gp.sort_values('count', ascending=False).head())\n\ntrain_answer_gp = train[['qa_id', 'answer_user_page']].groupby('answer_user_page', as_index=False).count()\ntest_answer_gp = test[['qa_id', 'answer_user_page']].groupby('answer_user_page', as_index=False).count()\ntrain_answer_gp.columns = ['answer_user_page', 'Answer count']\ntest_answer_gp.columns = ['answer_user_page', 'Answer count']\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\nsns.countplot(x=\"Answer count\", data=train_answer_gp, palette=\"Set3\", ax=ax1).set_title(\"Train\")\nsns.countplot(x=\"Answer count\", data=test_answer_gp, palette=\"Set3\", ax=ax2).set_title(\"Test\")\nplt.show()","6ac833ce":"question_title_gp = complete_set[['qa_id', 'question_title']].groupby('question_title', as_index=False).count()\nquestion_title_gp.columns = ['question_title', 'count']\ndisplay(question_title_gp.sort_values('count', ascending=False).head())\n\ntrain_question_title_gp = train[['qa_id', 'question_title']].groupby('question_title', as_index=False).count()\ntest_question_title_gp = test[['qa_id', 'question_title']].groupby('question_title', as_index=False).count()\ntrain_question_title_gp.columns = ['question_title', 'Question title count']\ntest_question_title_gp.columns = ['question_title', 'Question title count']\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\nsns.countplot(x=\"Question title count\", data=train_question_title_gp, palette=\"Set3\", ax=ax1).set_title(\"Train\")\nsns.countplot(x=\"Question title count\", data=test_question_title_gp, palette=\"Set3\", ax=ax2).set_title(\"Test\")\nplt.show()","9724652e":"question_body_gp = complete_set[['qa_id', 'question_body']].groupby('question_body', as_index=False).count()\nquestion_body_gp.columns = ['question_body', 'count']\ndisplay(question_body_gp.sort_values('count', ascending=False).head())\n\ntrain_question_body_gp = train[['qa_id', 'question_body']].groupby('question_body', as_index=False).count()\ntest_question_body_gp = test[['qa_id', 'question_body']].groupby('question_body', as_index=False).count()\ntrain_question_body_gp.columns = ['question_body', 'Question body count']\ntest_question_body_gp.columns = ['question_body', 'Question body count']\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\nsns.countplot(x=\"Question body count\", data=train_question_body_gp, palette=\"Set3\", ax=ax1).set_title(\"Train\")\nsns.countplot(x=\"Question body count\", data=test_question_body_gp, palette=\"Set3\", ax=ax2).set_title(\"Test\")\nplt.show()","3204cc62":"complete_set['question_title_len'] = complete_set['question_title'].apply(lambda x : len(x))\ncomplete_set['question_body_len'] = complete_set['question_body'].apply(lambda x : len(x))\ncomplete_set['answer_len'] = complete_set['answer'].apply(lambda x : len(x))\ncomplete_set['question_title_wordCnt'] = complete_set['question_title'].apply(lambda x : len(x.split(' ')))\ncomplete_set['question_body_wordCnt'] = complete_set['question_body'].apply(lambda x : len(x.split(' ')))\ncomplete_set['answer_wordCnt'] = complete_set['answer'].apply(lambda x : len(x.split(' ')))\n\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(24, 7), sharex=True)\nsns.distplot(complete_set[complete_set['set'] == 'train']['question_title_len'], ax=ax1).set_title(\"Train\")\nsns.distplot(complete_set[complete_set['set'] == 'test']['question_title_len'], ax=ax2).set_title(\"Test\")\nplt.show()\n\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(24, 7), sharex=True)\nsns.distplot(complete_set[complete_set['set'] == 'train']['question_title_wordCnt'], ax=ax1).set_title(\"Train\")\nsns.distplot(complete_set[complete_set['set'] == 'test']['question_title_wordCnt'], ax=ax2).set_title(\"Test\")\nplt.show()","3b9bd9f4":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24, 7), sharex=True)\nsns.distplot(complete_set[complete_set['set'] == 'train']['question_body_len'], ax=ax1).set_title(\"Train\")\nsns.distplot(complete_set[complete_set['set'] == 'test']['question_body_len'], ax=ax2).set_title(\"Test\")\nplt.show()\n\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(24, 7), sharex=True)\nsns.distplot(complete_set[complete_set['set'] == 'train']['question_body_wordCnt'], ax=ax1).set_title(\"Train\")\nsns.distplot(complete_set[complete_set['set'] == 'test']['question_body_wordCnt'], ax=ax2).set_title(\"Test\")\nplt.show()","cf334da6":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24, 7), sharex=True)\nsns.distplot(complete_set[complete_set['set'] == 'train']['answer_len'], ax=ax1).set_title(\"Train\")\nsns.distplot(complete_set[complete_set['set'] == 'test']['answer_len'], ax=ax2).set_title(\"Test\")\nplt.show()\n\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(24, 7), sharex=True)\nsns.distplot(complete_set[complete_set['set'] == 'train']['answer_wordCnt'], ax=ax1).set_title(\"Train\")\nsns.distplot(complete_set[complete_set['set'] == 'test']['answer_wordCnt'], ax=ax2).set_title(\"Test\")\nplt.show()","5c1cc55d":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 7), sharex=True)\nsns.countplot(complete_set[complete_set['set'] == 'train']['category'], ax=ax1).set_title(\"Train\")\nsns.countplot(complete_set[complete_set['set'] == 'test']['category'], ax=ax2).set_title(\"Test\")\nplt.show()\n","75dd6135":"complete_set['host_first'] = complete_set['host'].apply(lambda x : x.split('.')[0])\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 12), sharex=True)\nsns.countplot(y=complete_set[complete_set['set'] == 'train']['host_first'], ax=ax1, palette=\"muted\").set_title(\"Train\")\nsns.countplot(y=complete_set[complete_set['set'] == 'test']['host_first'], ax=ax2, palette=\"muted\").set_title(\"Test\")\nplt.show()","7d17fb64":"f = plt.subplots(figsize=(24, 7))\nfor col in question_target_cols[:5]:\n    sns.distplot(train[col], label=col, rug=True, hist=False)\nplt.show()\n\nf = plt.subplots(figsize=(24, 7))\nfor col in question_target_cols[5:10]:\n    sns.distplot(train[col], label=col, rug=True, hist=False)\nplt.show()\n\nf = plt.subplots(figsize=(24, 7))\nfor col in question_target_cols[10:15]:\n    sns.distplot(train[col], label=col, rug=True, hist=False)\nplt.show()\n\nf = plt.subplots(figsize=(24, 7))\nfor col in question_target_cols[15:]:\n    sns.distplot(train[col], label=col, rug=True, hist=False)\nplt.show()","cc0f561f":"f = plt.subplots(figsize=(24, 7))\nfor col in answer_target_cols[:5]:\n    sns.distplot(train[col], label=col, rug=True, hist=False)\nplt.show()\n\nf = plt.subplots(figsize=(24, 7))\nfor col in answer_target_cols[5:]:\n    sns.distplot(train[col], label=col, rug=True, hist=False)\nplt.show()","1371e7e2":"eng_stopwords = stopwords.words('english')\n\ncomplete_set['question_title'] = complete_set['question_title'].str.replace('[^a-z ]','')\ncomplete_set['question_body'] = complete_set['question_body'].str.replace('[^a-z ]','')\ncomplete_set['answer'] = complete_set['answer'].str.replace('[^a-z ]','')\ncomplete_set['question_title'] = complete_set['question_title'].apply(lambda x: x.lower())\ncomplete_set['question_body'] = complete_set['question_body'].apply(lambda x: x.lower())\ncomplete_set['answer'] = complete_set['answer'].apply(lambda x: x.lower())\n\nfreq_dist = FreqDist([word for comment in complete_set['question_title'] for word in comment.split() if word not in eng_stopwords])\nplt.figure(figsize=(20, 6))\nplt.title('Word frequency on question title').set_fontsize(20)\nfreq_dist.plot(60, marker='.', markersize=10)\nplt.show()\n\nfreq_dist = FreqDist([word for comment in complete_set['question_body'] for word in comment.split() if word not in eng_stopwords])\nplt.figure(figsize=(20, 6))\nplt.title('Word frequency on question body').set_fontsize(20)\nfreq_dist.plot(60, marker='.', markersize=10)\nplt.show()\n\nfreq_dist = FreqDist([word for comment in complete_set['answer'] for word in comment.split() if word not in eng_stopwords])\nplt.figure(figsize=(20, 6))\nplt.title('Word frequency on answer').set_fontsize(20)\nfreq_dist.plot(60, marker='.', markersize=10)\nplt.show()\n","50dd7c69":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow_hub as hub\nimport tensorflow as tf\nfrom bert_tokenization import tokenization\nimport tensorflow.keras.backend as K\nimport gc\nimport os\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\n\nnp.set_printoptions(suppress=True)","f0e7ad78":"pip install bert","b18d81ed":"PATH = '..\/input\/google-quest-challenge\/'\nBERT_PATH = '..\/input\/bert-base-from-tfhub\/bert_en_uncased_L-12_H-768_A-12'\ntokenizer = tokenization.FullTokenizer(BERT_PATH+'\/assets\/vocab.txt', True)\nMAX_SEQUENCE_LENGTH = 512\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\noutput_categories = list(df_train.columns[11:])\ninput_categories = list(df_train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","cc112490":"def _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef _trim_input(title, question, answer, max_sequence_length, \n                t_max_len=30, q_max_len=239, a_max_len=239):\n\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)\/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)\/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n        \n        t = t[:t_new_len]\n        q = q[:q_new_len]\n        a = a[:a_new_len]\n    \n    return t, q, a\n\ndef _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        t, q, a = _trim_input(t, q, a, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","7417ad7b":"def compute_spearmanr(trues, preds):\n    rhos = []\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        rhos.append(\n            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n    return np.mean(rhos)\n\n\nclass CustomCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n        self.test_inputs = test_data\n        \n        self.batch_size = batch_size\n        self.fold = fold\n        \n    def on_train_begin(self, logs={}):\n        self.valid_predictions = []\n        self.test_predictions = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        self.valid_predictions.append(\n            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n        \n        rho_val = compute_spearmanr(\n            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n        \n        print(\"\\nvalidation rho: %.4f\" % rho_val)\n        \n        if self.fold is not None:\n            self.model.save_weights(f'bert-base-{fold}-{epoch}.h5py')\n        \n        self.test_predictions.append(\n            self.model.predict(self.test_inputs, batch_size=self.batch_size)\n        )\n\ndef bert_model():\n    \n    input_word_ids = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_word_ids')\n    input_masks = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n    input_segments = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n    \n    bert_layer = hub.KerasLayer(BERT_PATH, trainable=True)\n    \n    _, sequence_output = bert_layer([input_word_ids, input_masks, input_segments])\n    \n    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    out = tf.keras.layers.Dense(30, activation=\"sigmoid\", name=\"dense_output\")(x)\n\n    model = tf.keras.models.Model(\n        inputs=[input_word_ids, input_masks, input_segments], outputs=out)\n    \n    return model    \n        \ndef train_and_predict(model, train_data, valid_data, test_data, \n                      learning_rate, epochs, batch_size, loss_function, fold):\n        \n    custom_callback = CustomCallback(\n        valid_data=(valid_data[0], valid_data[1]), \n        test_data=test_data,\n        batch_size=batch_size,\n        fold=None)\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(loss=loss_function, optimizer=optimizer)\n    model.fit(train_data[0], train_data[1], epochs=epochs, \n              batch_size=batch_size, callbacks=[custom_callback])\n    \n    return custom_callback\n","24a074ec":"gkf = GroupKFold(n_splits=5).split(X=df_train.question_body, groups=df_train.question_body)\n\noutputs = compute_output_arrays(df_train, output_categories)\ninputs = compute_input_arays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)","8ba74199":"\nhistories = []\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n    \n    # will actually only do 3 folds (out of 5) to manage < 2h\n    if fold < 3:\n        K.clear_session()\n        model = bert_model()\n\n        train_inputs = [inputs[i][train_idx] for i in range(3)]\n        train_outputs = outputs[train_idx]\n\n        valid_inputs = [inputs[i][valid_idx] for i in range(3)]\n        valid_outputs = outputs[valid_idx]\n\n        # history contains two lists of valid and test preds respectively:\n        #  [valid_predictions_{fold}, test_predictions_{fold}]\n        history = train_and_predict(model, \n                          train_data=(train_inputs, train_outputs), \n                          valid_data=(valid_inputs, valid_outputs),\n                          test_data=test_inputs, \n                          learning_rate=3e-5, epochs=4, batch_size=8,\n                          loss_function='binary_crossentropy', fold=fold)\n\n        histories.append(history)\n","a47b25e7":"test_predictions = [histories[i].test_predictions for i in range(len(histories))]\ntest_predictions = [np.average(test_predictions[i], axis=0) for i in range(len(test_predictions))]\ntest_predictions = np.mean(test_predictions, axis=0)\n\ndf_sub.iloc[:, 1:] = test_predictions\n\ndf_sub.to_csv('submission.csv', index=False)","e475c3ca":"submission.to_csv('submit.csv', index=False)","9f910aa5":"submission","759f277b":"import pandas as pd\nsubmission = pd.read_csv(\"..\/input\/abbbbbb\/submission.csv\")","611f2647":"Heres the revised code.","85c0c5bc":"#### 6. Process and submit test predictions\n\nFirst the test predictions are read from the list of lists of `histories`. Then each test prediction list (in lists) is averaged. Then a mean of the averages is computed to get a single prediction for each data point. Finally, this is saved to `submission.csv`","14fcccd2":"#### 4. Obtain inputs and targets, as well as the indices of the train\/validation splits","c8148ffc":"#### 3. Create model\n\n`compute_spearmanr()` is used to compute the competition metric for the validation set\n<br><br>\n`CustomCallback()` is a class which inherits from `tf.keras.callbacks.Callback` and will compute and append validation score and validation\/test predictions respectively, after each epoch.\n<br><br>\n`bert_model()` contains the actual architecture that will be used to finetune BERT to our dataset. It's simple, just taking the sequence_output of the bert_layer and pass it to an AveragePooling layer and finally to an output layer of 30 units (30 classes that we have to predict)\n<br><br>\n`train_and_predict()` this function will be run to train and obtain predictions","c05a4614":"### Bert-base TensorFlow 2.0\n\nThis kernel does not explore the data. For that you could check out some of the great EDA kernels: [introduction](https:\/\/www.kaggle.com\/corochann\/google-quest-first-data-introduction), [getting started](https:\/\/www.kaggle.com\/phoenix9032\/get-started-with-your-questions-eda-model-nn) & [another getting started](https:\/\/www.kaggle.com\/hamditarek\/get-started-with-nlp-lda-lsa). This kernel is an example of a TensorFlow 2.0 Bert-base implementation, using TensorFow Hub. <br><br>\n\n---\n**Update 1 (Commit 7):**\n* removing penultimate dense layer; now there's only one dense layer (output layer) for fine-tuning\n* using BERT's sequence_output instead of pooled_output as input for the dense layer\n---\n\n**Update 2 (Commit 8):**\n* adjusting `_trim_input()` --- now have a q_max_len and a_max_len, instead of 'keeping the ratio the same' while trimming.\n* **importantly:** now also includes question_title for the input sequence\n---\n\n**Update 3 (Commit 9)**\n<br><br>*A lot of experiments can be made with the title + body + answer sequence. Feel free to look into e.g. (1) inventing new tokens (add it to '..\/input\/path-to-bert-folder\/assets\/vocab.txt'), (2) keeping [SEP] between title and body but modify `_get_segments()`, (3) using the [PAD] token, or (4) merging title and body without any kind of separation. In this commit I'm doing (2). I also tried (3) offline, and they both perform better than in commit 8, in terms of validation rho.*\n* Ignoring first [SEP] token in `_get_segments()`\n---","8b91b460":"#### 1. Read data and tokenizer\n\nRead tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)","fb9cf019":"#### 2. Preprocessing functions\n\nThese are some functions that will be used to preprocess the raw text data into useable Bert inputs.","36357f3b":"#### 5. Training, validation and testing\n\nLoops over the folds in gkf and trains each fold for 5 epochs --- with a learning rate of 1e-5 and batch_size of 8. A simple binary crossentropy is used as the objective-\/loss-function. "}}