{"cell_type":{"1b017fe7":"code","38225262":"code","c811d40a":"code","9dc27ad4":"code","339258ee":"code","6fd4f838":"code","d6ed132c":"code","bf3ec687":"code","7a425249":"code","1edf531f":"code","069750d4":"code","c5afcd7d":"code","3ae1f64d":"code","1b228d0d":"code","4ab253c1":"code","1375cd01":"code","32e17f64":"code","4473a2da":"markdown","60159f74":"markdown","6876055f":"markdown","c139f80f":"markdown","280c67d5":"markdown","58c6a1e5":"markdown","0ea59c56":"markdown","b6881391":"markdown","d1eb312c":"markdown","1cf3c4fd":"markdown","e39a062d":"markdown","952a6058":"markdown","426ad7f7":"markdown","45d16a9e":"markdown","cb287997":"markdown","315f2d72":"markdown","1b6307a7":"markdown","baf65b59":"markdown"},"source":{"1b017fe7":"import pandas as pd\nimport sqlite3\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom nltk.corpus import stopwords\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\n","38225262":"# Importing the data\ndf = pd.read_csv('..\/input\/Reviews.csv')\ndata = df[df.Score!=3][0:5000]\n\n# Performing Basic Checks\nprint('The Shape of the data'+str(data.shape)+'\\n')\nprint('Data Types of all columns\\n',data.dtypes,'\\n')\nprint('Checking for Na Values\\n',data.isna().sum())\ndata.head(5)","c811d40a":"# Converting score to 1(positive) or 0(negative)\ndef score(x):\n    '''This function converts the score into positive or negative'''\n    if x<3:\n        return 0\n    elif x>3:\n        return 1\npositivenegative = data['Score'].map(score)\ndata['score'] = positivenegative\ndata.drop('Score',axis=1,inplace=True)\ndata.head(5)    \n\n","9dc27ad4":"# Checkes: 1. Numerator<=Denominator\nprint(data[data.HelpfulnessNumerator>data.HelpfulnessDenominator])\n\n# 2. Duplication\ndata[data.duplicated(subset={'ProfileName', 'HelpfulnessNumerator',\n       'HelpfulnessDenominator', 'Time', 'Summary', 'Text', 'score'})]\n","339258ee":"data[data.Text==data['Text'].iloc[2309]]","6fd4f838":"# Deleting the values\ndata.drop_duplicates(keep='first',subset={'ProfileName', 'HelpfulnessNumerator',\n       'HelpfulnessDenominator', 'Time', 'Summary', 'Text', 'score'},inplace=True)\nprint(data[data.duplicated(subset={'ProfileName','HelpfulnessNumerator',\n       'HelpfulnessDenominator', 'Time', 'Summary', 'Text', 'score'})])\nprint(data.shape)\n# All the duplicate values have been removed","d6ed132c":"# Distribution of scores\nprint(data.score.value_counts())\nc=[]\nfor i in data.score.value_counts():c.append(i)\nplt.bar(['1','0'],c)\nplt.show()\n","bf3ec687":"from nltk.stem import SnowballStemmer\nimport nltk.stem\nimport re\nimport string\n\nstop = set(stopwords.words('english'))\nsno = nltk.SnowballStemmer('english')\n\n # Defining functions that will make our cleaning easier\ndef cleanhtml(sent):\n    '''This function cleans the html tags ina  sentence'''\n    cleanr = re.compile('<.*?>')\n    clean_sentence = re.sub(cleanr,'',sent)\n    return clean_sentence\n\ndef cleanpunc(word):\n    '''This function cleans the punctuations in a word'''\n    clean_word = re.sub(r'[?|!|,|.|\\|\/|\\'|\"|:|;|#]',r'',word)\n    return clean_word","7a425249":"cleaned_text =[] # List of the cleaned reviews\nfor sentence in data['Text'].values:\n    clean_sentence = []\n    sentence = cleanhtml(sentence)\n    for word in sentence.split():\n        cleaned_word = cleanpunc(word)\n        if (cleaned_word.lower() not in stop) & (cleaned_word.isalpha()):\n            if len(cleaned_word)>2:\n                stemmed_word = sno.stem(cleaned_word.lower()).encode('utf8')\n                clean_sentence.append(stemmed_word)\n            \n\n    str1=b' '.join(clean_sentence)\n    #print(str1)\n    cleaned_text.append(str1)\nprint('Cleaned Text: ',cleaned_text[0])\nprint('\\nActual Text: ',data['Text'].values[0])\ndata['cleanedText'] = cleaned_text\ndata.head(5)","1edf531f":"from sklearn.model_selection import train_test_split\ny = data['score']\ndata.drop('score',axis=1,inplace = True)\nX = data\nprint(X.shape)\nprint(y.shape)\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.33,shuffle =False) # 33% data in test\nX_train,X_cv,y_train,y_cv = train_test_split(X_train,y_train,test_size = 0.33,shuffle =False) # 33% data in cross validation\nprint('Train shape ',X_train.shape,y_train.shape)\nprint('CV shape',X_cv.shape,y_cv.shape)\nprint('Test shape',X_test.shape,y_cv.shape)\n","069750d4":"# Converting cleaned text into bow vectors\n\nbow = CountVectorizer(max_features = 500) #ngram = 1\n\n\nXtrain_bow = bow.fit_transform(X_train['cleanedText'])\nprint(bow.get_feature_names()[0:10])\n#print(bow.vocabulary_)\n\nXcross_bow = bow.transform(X_cv['cleanedText'])\nXtest_bow = bow.transform(X_test['cleanedText'])\n\n# Checking their dimension\nprint(Xtrain_bow.shape)\nprint(Xcross_bow.shape)\nprint(Xtest_bow.shape)\n# Reduced the reviews bow vectors with 500 features\n\n","c5afcd7d":"# Standardizing\nfrom sklearn.preprocessing import StandardScaler\nstd = StandardScaler()\n\nXtrain_bow_std = std.fit_transform(Xtrain_bow.toarray()) # converting sparse matrix into dense matrix using toarray()\nXtest_bow_std = std.fit_transform(Xtest_bow.toarray())\nXcross_bow_std = std.fit_transform(Xcross_bow.toarray())\n","3ae1f64d":"# cheking if train and test have same distribution ~ Not taking cross validation as if train and test have same distribution we can assume CV will also have similar distribution.\n\n# 1. Creating the new dataset train labels = 1, test labels = 0\nTrain_label = np.ones(len(X_train))\nTest_label = np.zeros(len(X_test))\nlabels = np.hstack((Train_label.T,Test_label.T))\n#Train\nnew_data_train = (np.vstack((Xtrain_bow_std.T,y_train)).T)\nprint('Dimensions of train dataset incusing original labels',new_data_train.shape)\n#print(new_data_train[0:10])\n\n# Test\nnew_data_test = (np.vstack((Xtest_bow_std.T,y_test)).T)\nprint('\\nDimensions of train dataset incusing original labels',new_data_test.shape)\n#print(new_data_test[0:10])\n\n# 2. Combine the train and test data\ndist_data = np.vstack((new_data_train,new_data_test))\nprint('\\nThe shape of combined new data',dist_data.shape)\n\n# 3. Random splitting into train and test for modeling\nx_train,x_test,Y_train,Y_test = train_test_split(dist_data,labels,test_size=0.33,shuffle=True)\nprint('\\nDimension of train data with label=1',x_train.shape,Y_train.shape)\nprint('\\nDimension of test data with label=0',x_test.shape,Y_test.shape)\n\n# 4. Modelling using KNN\nfrom sklearn.metrics import accuracy_score\nknn = KNeighborsClassifier(n_neighbors = 20)\nknn.fit(x_train,Y_train)\ntest_predict = knn.predict(x_test)\n\n#5. Inspect Accuracy\naccuracy = accuracy_score(test_predict,Y_test)\nprint('\\nAccuracy of mdoel',accuracy)","1b228d0d":"from imblearn.over_sampling import SMOTE\nprint('Number of positive and negative reviews:\\n',y_train.value_counts())\nsm = SMOTE(random_state=0,ratio=1.0)\nXtrain_res,ytrain_res = sm.fit_sample(Xtrain_bow_std,y_train)\nprint(np.bincount(ytrain_res),Xtrain_res.shape) # equal 1s and 0s","4ab253c1":"import scipy\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\nneighbors = [1,2,5,10,20,25,30,40,50]\nauc_scores_cv = []\nauc_scores_train = []\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(Xtrain_res,ytrain_res) # fitting the model\n    \n    cv_predict = knn.predict_proba(Xcross_bow_std) # predicting the probabilistic values cross validation set\n    cv_auc = roc_auc_score(y_cv,cv_predict[:,1])\n    auc_scores_cv.append(cv_auc) #auc value for CV set\n    \n    train_predict = knn.predict_proba(Xtrain_bow_std) # predicting on train itself\n    train_auc = roc_auc_score(y_train,train_predict[:,1]) # auc value for train\n    auc_scores_train.append(train_auc)\n\nprint('Train AUC Scores ',auc_scores_train)\nprint('CV AUC Scores',auc_scores_cv)\nerror_cv = [1-  x for x in auc_scores_cv]\nerror_train = [1 - x for x in auc_scores_train]\n# Visualising Train error and Cross Validation Error\nplt.figure(figsize=(7,7))\nplt.plot(neighbors,error_cv,color = 'r',label='Cross-Validation error') \nplt.plot(neighbors,error_train,color='b',label='Train Error')\nplt.xlabel('K Neighbors')\nplt.ylabel('error')\nplt.title('K: Hyperparameter')\nplt.legend(loc='lower right')\nplt.grid()\nplt.show()\n\n","1375cd01":"# The optimal value of K\nbest_k =10","32e17f64":"from sklearn.metrics import roc_curve,auc\nfrom sklearn.metrics import confusion_matrix\n# Final Prediction using Test Data\nknn = KNeighborsClassifier(n_neighbors = best_k)\nknn.fit(Xtrain_res,ytrain_res) # fitting the model in train set\n\n# predicted values determination\npredicted_values_train = knn.predict_proba(Xtrain_bow_std)\npredicted_values_test = knn.predict_proba(Xtest_bow_std)\npredicted_values_cv = knn.predict_proba(Xcross_bow_std)\n\n# False Positive Rate and True Positive Rate\ntrain_fpr,train_tpr,thresholds = roc_curve(y_train,predicted_values_train[:,1],pos_label=1)\ncv_fpr,cv_tpr,thresholds = roc_curve(y_cv,predicted_values_cv[:,1],pos_label=1)\ntest_fpr,test_tpr,thresholds = roc_curve(y_test,predicted_values_test[:,1],pos_label=1)\n\n# Visualising ROC\nplt.figure(figsize=(7,7))\nplt.plot(train_fpr,train_tpr,color='g',label='Train AUC = '+str(auc(train_fpr,train_tpr)))\nplt.plot(cv_fpr,cv_tpr,color='b',label = 'CV AUC = '+str(auc(cv_fpr,cv_tpr)))\nplt.plot(test_fpr,test_tpr,color='r',label = 'Test AUC = '+str(auc(test_fpr,test_tpr)))\nplt.legend(loc = 'lower right')\nplt.show()\n\n# Confucion Matrix\n\nprint('Train:\\n',confusion_matrix(y_train,knn.predict(Xtrain_bow_std)).T)\nprint('\\nCV:\\n',confusion_matrix(y_cv,knn.predict(Xcross_bow_std)).T)\nprint('\\nTest:\\n',confusion_matrix(y_test,knn.predict(Xtest_bow_std)).T)\ncm = confusion_matrix(y_test,knn.predict(Xtest_bow_std)).T\nprint('#'*50)\nprint('TNR for Test = ',(cm[0][0])\/(cm[0][0] + cm[1][0]) )\nprint('FPR for Test = ',cm[1][0]\/(cm[1][0]+cm[0][0]) )\n","4473a2da":"1. The Test AUC is 65%. The model seems fine.\n2. TNR = 92.4% which is very good.\n3. FPR = 0.07% which again is very good.\n\n**Conclusion: Overall, this model is a fairly good model. Using this model we are easily able to predict if a review is negative or not. For an internet company like amazon identifying negative comments are more important for which our True negative rate must be high and our False positive rate must be low. We are able to easily achieve that.** \n\n\n","60159f74":"1. Dataset is imbalanced. We have 4182 positive reviews and 811 negative reviews.\n2. We will have to oversample or undersample the data points while modelling","6876055f":"1. There are 10 features.\n2. No Na values present in the text data. For our objective the text is all we need.","c139f80f":"## [3.2] Bag of Words\n\n- BOW is a technique used to convert textual data into numeric data. \n- It creates a sparse matrix where the features are the vocabulary built using the train data and the values are the frequency of those terms in the document.","280c67d5":"# [3.2.5] Prediction & Performance Metrics\n\n\n\n- The performance metric we are using is ROC and AUC.\n- The dataset is highly imbalanced and binary hence ROC and AUC seem to be the optimal metric.\n- We will also take a look at the confusion matrix to better understand the working of the model","58c6a1e5":"1. Initial model is overfitting as it should be. Train error is low and Validation error is high.\n\n2. The train error keeps increasing and peeks at 50. Validation error decreases till k =10 and then increases gradually. \n   The model starts to underfit.\n\n3. The optimal value of k is for which Crosh Validation error is low and train error close to CV error\n This can be seen for K=10.","0ea59c56":"## [3.2.2] Train and Test Distribution\n\n- Since we have done a time based split we must ensure that the distribution of train and test are not too dissimilar.\n- As mentioned before, the data in internet companies keeps chaning over time, this may lead to 2 completely different distribution in 2 points of time. The train and test dat\nmay be completely different and if that is the case then our model will perform horribly.\n\n#### Process\n\n- To ensure that train and test are of similar distribution we are applying KNN classification.\n- A new dataset has been created where label=1 is given to train and label = 0 is given to test.\n- Then, we are modelling a classifier to distinguish between the train and test.\n- If the accuracy is high then that means that the distribution is completely different and classifier is easily able to classify train and test.\n- If the accuracy is low to medium then we can say that the distribution of train and test set is not same.","b6881391":"# [2] Text Pre processing\n\n- All the punctuations, html tags and the stopwords need to be removed before analysing the text.\n- Stopwords are frequently occuting words which do not offer any real meaning to the text. Eg: the, is, and, no etc.\n- Stopwords may make sense when used along with a non-stopword like 'not good' where not is a stop word. \n\n*For now, we will remove all the stopwords and make a new feature called cleanedText*","d1eb312c":"# [3] Data Modelling","1cf3c4fd":"* There are some duplicated values present. Lets print one out.","e39a062d":"## [3.2.1] Standardizing","952a6058":"# [3.2.3] Oversampling\n\n- As we saw before, the dataset we have is highly imbalanced. \n- The positive points are much higher than negative points, this wiil make even a dunb model give a high accuracy.\n\nThere are  ways to tackle this problem:\n1. Undersampling: Here we reduce the majority class data to match that of minority class.\n2. Oversampling: We increase the minority class data to match that of majority class.\n\n- We have used oversampling as in undersampling we are deleting a lot of data hence, we are losing a lot of information.\n- Oversampling does not lose information.\n\n- As for oversampling, there are 3 types of oversampling. We have usend SMOTE technique, which is a state of the art technique for resampling.\n\nFor more info: https:\/\/medium.com\/anomaly-detection-with-python-and-r\/sampling-techniques-for-extremely-imbalanced-data-part-ii-over-sampling-d61b43bc4879","426ad7f7":"- We are getting an accuracy in a range of 45-55% (for different k values). Medium accuracy.\n- The classifier is not able to distiguin train and test points that well. \n \n**Conclusion:** The distribution is somewhat changing through time but 50% accuracy indicates that the distribution of train and test data is **not similar.**","45d16a9e":"# [1] Data Cleaning","cb287997":"# [0] Loading the Data\n\n","315f2d72":"## [3.1] Splitting the Data\n\n- Splitting of the data can be done mainly in 2 was.\n1. Random Splitting: The data is randomly split into train and test set where each datapoint as equal probability of going into the train or test set respectively.\n2. Time Based Splitting: Train, Cross Validation and Test set is formed in ascending order of time.\n\n\n- We have used time based splitting as the data in internet companies keep changing over time. If we use random splitting then we might get a good accuracy on train or cross validation data but\non futue unseen data we will get a lower accuracy.\n","1b6307a7":"# Amazon Fine Food Reviews\n\nThe Amazon Fine Food Reviews dataset consists of reviews of fine foods from Amazon.\n\nNumber of reviews: 568,454\nNumber of users: 256,059\nNumber of products: 74,258\nTimespan: Oct 1999 - Oct 2012\nNumber of Attributes\/Columns in data: 10\n\n<br>\n#### Attribute Information:\n\n1. Id\n2. ProductId - unique identifier for the product\n3. UserId - unqiue identifier for the user\n4. ProfileName\n5. HelpfulnessNumerator - number of users who found the review helpful\n6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n7. Score - rating between 1 and 5\n8. Time - timestamp for the review\n9. Summary - brief summary of the review\n10. Text - text of the review\n\n#### Objective:\nGiven a review, determine whether the review is positive (rating of 4 or 5) or negative (rating of 1 or 2).\n\nNote: I have taken only 5000 reviews for this purpose because of computational constraints. ","baf65b59":"# [3.2.4] Hyperparameter Tuning ~ KNN\n\n- For hyper parameter tuning, cross-validation set is being used so as to avoid any data leakage to the test data.\n- The hyper parameter here is K which determines the optimal number of neighborhood points for the algorithm to be able to obtain the highest test accuracy.\n- We plot the Train error vs K and Cross-Validation Error vs K plots and identify best value of K using it."}}