{"cell_type":{"90025656":"code","6477bc32":"code","d7d092f5":"code","81410132":"code","2ff564f3":"code","9b1d1f5f":"code","3b68b445":"code","e1973bf7":"code","397dcc74":"code","000ed1d6":"code","157ce234":"code","8bc4ee31":"code","0d770c10":"code","7446c37c":"code","30c6adef":"code","d3aa1ad9":"code","61923433":"code","45ddd2d6":"code","2a089253":"code","e72dd0f5":"code","14a2bac8":"code","bdb8a317":"code","6d7e4e57":"code","28f64bfc":"markdown","089c0fa9":"markdown","27863dd6":"markdown"},"source":{"90025656":"import numpy as np\nimport pandas as pd","6477bc32":"# Only need the tweets and associated sentiment\ntweets_df = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv', encoding='latin-1')\ntweets_df = tweets_df[['OriginalTweet', 'Sentiment']]\n\ntest_df = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv')\ntest_df = test_df[['OriginalTweet', 'Sentiment']]","d7d092f5":"test_df","81410132":"# Check for null values\nfor column in tweets_df.columns:\n    print(tweets_df[column].isnull().value_counts())\nprint()  \nfor column in test_df.columns:\n    print(test_df[column].isnull().value_counts())","2ff564f3":"# Check for duplicate values\nprint(tweets_df.duplicated().value_counts())\nprint()\nprint(test_df.duplicated().value_counts())","9b1d1f5f":"import nltk\nfrom nltk.corpus import stopwords\n#from nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport string\n\nnltk.download('stopwords')\n#ps = PorterStemmer()\nlemmatizer = WordNetLemmatizer()","3b68b445":"!pip install pyspellchecker \nfrom spellchecker import SpellChecker\nspell = SpellChecker()","e1973bf7":"'''def textClean(text):\n    nopunc = [char.lower() for char in text if char not in string.punctuation]\n    nopunc = ''.join(nopunc)\n    #nodigit = ''.join([char for char in nopunc if not char.isdigit()])\n    tokens = word_tokenize(nopunc)\n    nohttp = [word for word in tokens if word[0:4]!='http']\n    nostop = [word for word in nohttp if word not in stopwords.words('english')]\n    #stemmed = [ps.stem(word) for word in nostop ]\n    lemmatized = [lemmatizer.lemmatize(word) for word in nostop]\n    return lemmatized'''","397dcc74":"puncs_ = string.punctuation.replace('@','')\npuncs = puncs_.replace('#','')\npuncs","000ed1d6":"s = '           @ the springs theatre httpstcoaertookvav'\nmytext = \" \".join(s.split(\"  \"))\nmytext","157ce234":"def textClean(text):\n    # convert to lowercase\n    lower = [char.lower() for char in text if char not in puncs]\n    lower = ''.join(lower)\n    lower = ' '.join(lower.split())\n    \n    # delete @mentions and #tags\n    for char in lower:\n        if lower.find('@')==-1 and lower.find('#')==-1: # break loop once @ and # is over\n            break\n        if (char=='@' or char=='#'):\n            try:\n                char_index = lower.index(char)\n            except ValueError:\n                #print(lower)\n                break\n            del_word = ''\n            while char not in string.whitespace:\n                del_word = del_word+lower[char_index]\n                char_index = char_index + 1\n                try:\n                    char = lower[char_index] #trying for indexerror incase it is the last character of string\n                except IndexError:\n                    char = ' '\n                except:\n                    print(\"Something else went wrong\")\n            lower = lower.replace(del_word,'',1)\n    lower = [char for char in lower if char not in string.punctuation and char not in string.digits]\n    lower = ''.join(lower)\n    \n    tokens = word_tokenize(lower)\n    nohttp = [word for word in tokens if word[0:4]!='http']\n    nostop = [word for word in nohttp if word not in stopwords.words('english')]\n    return nostop","8bc4ee31":"#print(tweets_df.OriginalTweet[0:10])\n#tweets_df.OriginalTweet[0:10].apply(textClean)\ntemp_list = tweets_df.OriginalTweet[0:10].apply(textClean)\nfor each_list in temp_list:\n    print(each_list)#for word in each_list:","0d770c10":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(analyzer=textClean)\nmessage = vectorizer.fit_transform(tweets_df['OriginalTweet'])\nmessage.shape","7446c37c":"#split the data into 80% training and 20% testing\nfrom sklearn.model_selection import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(message,tweets_df.Sentiment,test_size=0.20,random_state=0)","30c6adef":"# create and train the Naive Bayes Classifier\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB().fit(xtrain, ytrain)","d3aa1ad9":"print(classifier.predict(xtest))\nprint(ytest.values)","61923433":"# Evaluating the model on the training data set\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\npred = classifier.predict(xtrain)\nprint(classification_report(ytrain, pred))\nprint()\nprint(\"Confusion Matrix: \\n\", confusion_matrix(ytrain, pred))\nprint(\"Accuracy: \\n\", accuracy_score(ytrain, pred))","45ddd2d6":"# Evaluating the model on the testing data set\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\npred = classifier.predict(xtest)\nprint(classification_report(ytest, pred))\nprint()\nprint(\"Confusion Matrix: \\n\", confusion_matrix(ytest, pred))\nprint(\"Accuracy: \\n\", accuracy_score(ytest, pred))","2a089253":"test_df.shape","e72dd0f5":"message2 = vectorizer.transform(test_df['OriginalTweet'])\nmessage2.shape","14a2bac8":"message2","bdb8a317":"print(classifier.predict(message2))\nprint(test_df.Sentiment)","6d7e4e57":"pred = classifier.predict(message2)\nprint(classification_report(test_df.Sentiment, pred))\nprint()\nprint(\"Confusion Matrix: \\n\", confusion_matrix(test_df.Sentiment, pred))\nprint(\"Accuracy: \\n\", accuracy_score(test_df.Sentiment, pred))","28f64bfc":"# Test Data","089c0fa9":"# Model Training","27863dd6":"# Pre-processing"}}