{"cell_type":{"12459e23":"code","85c47cb6":"code","e03c1312":"code","4f96b614":"code","d7b36154":"code","008768dd":"code","fea7f08f":"code","356c49f3":"code","84346329":"code","b84270bf":"code","6a1deb16":"code","952c5648":"code","f395a0f8":"code","e69699c2":"code","620fcc69":"code","7d6abb61":"code","d380f8f5":"code","f2642684":"code","de98ea85":"code","6d33bdcf":"code","89ef8736":"code","4941eddc":"code","f1ff3b9a":"code","78750dec":"code","4b819430":"markdown","38a781d7":"markdown","f7d94229":"markdown","fe56b3cb":"markdown","3078590c":"markdown","b9f65d44":"markdown","159f407e":"markdown","7ef04bbb":"markdown","627b7949":"markdown","12c93a6d":"markdown","d0c56302":"markdown","cc55f78f":"markdown","6ea391e1":"markdown","5789619a":"markdown","708ab74a":"markdown","3f4ac446":"markdown","e91d2b94":"markdown"},"source":{"12459e23":"import os, cv2\nimport numpy as np\nimport pandas as pd\nimport random, tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport albumentations as album","85c47cb6":"!pip install -q -U segmentation-models-pytorch albumentations > \/dev\/null\nimport segmentation_models_pytorch as smp","e03c1312":"DATA_DIR = '..\/input\/massachusetts-buildings-dataset\/tiff\/'\n\nx_train_dir = os.path.join(DATA_DIR, 'train')\ny_train_dir = os.path.join(DATA_DIR, 'train_labels')\n\nx_valid_dir = os.path.join(DATA_DIR, 'val')\ny_valid_dir = os.path.join(DATA_DIR, 'val_labels')\n\nx_test_dir = os.path.join(DATA_DIR, 'test')\ny_test_dir = os.path.join(DATA_DIR, 'test_labels')","4f96b614":"class_dict = pd.read_csv(\"..\/input\/massachusetts-buildings-dataset\/label_class_dict.csv\")\n# Get class names\nclass_names = class_dict['name'].tolist()\n# Get class RGB values\nclass_rgb_values = class_dict[['r','g','b']].values.tolist()\n\nprint('All dataset classes and their corresponding RGB values in labels:')\nprint('Class Names: ', class_names)\nprint('Class RGB values: ', class_rgb_values)","d7b36154":"# Useful to shortlist specific classes in datasets with large number of classes\nselect_classes = ['background', 'building']\n\n# Get RGB values of required classes\nselect_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\nselect_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n\nprint('Selected classes and their corresponding RGB values in labels:')\nprint('Class Names: ', class_names)\nprint('Class RGB values: ', class_rgb_values)","008768dd":"# helper function for data visualization\ndef visualize(**images):\n    \"\"\"\n    Plot images in one row\n    \"\"\"\n    n_images = len(images)\n    plt.figure(figsize=(20,8))\n    for idx, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n_images, idx + 1)\n        plt.xticks([]); \n        plt.yticks([])\n        # get title from the parameter names\n        plt.title(name.replace('_',' ').title(), fontsize=20)\n        plt.imshow(image)\n    plt.show()\n\n# Perform one hot encoding on label\ndef one_hot_encode(label, label_values):\n    \"\"\"\n    Convert a segmentation image label array to one-hot format\n    by replacing each pixel value with a vector of length num_classes\n    # Arguments\n        label: The 2D array segmentation image label\n        label_values\n        \n    # Returns\n        A 2D array with the same width and hieght as the input, but\n        with a depth size of num_classes\n    \"\"\"\n    semantic_map = []\n    for colour in label_values:\n        equality = np.equal(label, colour)\n        class_map = np.all(equality, axis = -1)\n        semantic_map.append(class_map)\n    semantic_map = np.stack(semantic_map, axis=-1)\n\n    return semantic_map\n    \n# Perform reverse one-hot-encoding on labels \/ preds\ndef reverse_one_hot(image):\n    \"\"\"\n    Transform a 2D array in one-hot format (depth is num_classes),\n    to a 2D array with only 1 channel, where each pixel value is\n    the classified class key.\n    # Arguments\n        image: The one-hot format image \n        \n    # Returns\n        A 2D array with the same width and hieght as the input, but\n        with a depth size of 1, where each pixel value is the classified \n        class key.\n    \"\"\"\n    x = np.argmax(image, axis = -1)\n    return x\n\n# Perform colour coding on the reverse-one-hot outputs\ndef colour_code_segmentation(image, label_values):\n    \"\"\"\n    Given a 1-channel array of class keys, colour code the segmentation results.\n    # Arguments\n        image: single channel array where each value represents the class key.\n        label_values\n\n    # Returns\n        Colour coded image for segmentation visualization\n    \"\"\"\n    colour_codes = np.array(label_values)\n    x = colour_codes[image.astype(int)]\n\n    return x","fea7f08f":"class BuildingsDataset(torch.utils.data.Dataset):\n\n    \"\"\"Massachusetts Buildings Dataset. Read images, apply augmentation and preprocessing transformations.\n    \n    Args:\n        images_dir (str): path to images folder\n        masks_dir (str): path to segmentation masks folder\n        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n        augmentation (albumentations.Compose): data transfromation pipeline \n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing \n            (e.g. noralization, shape manipulation, etc.)\n    \n    \"\"\"\n    \n    def __init__(\n            self, \n            images_dir, \n            masks_dir, \n            class_rgb_values=None, \n            augmentation=None, \n            preprocessing=None,\n    ):\n        \n        self.image_paths = [os.path.join(images_dir, image_id) for image_id in sorted(os.listdir(images_dir))]\n        self.mask_paths = [os.path.join(masks_dir, image_id) for image_id in sorted(os.listdir(masks_dir))]\n\n        self.class_rgb_values = class_rgb_values\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n    \n    def __getitem__(self, i):\n        \n        # read images and masks\n        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n        \n        # one-hot-encode the mask\n        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            \n        return image, mask\n        \n    def __len__(self):\n        # return length of \n        return len(self.image_paths)","356c49f3":"dataset = BuildingsDataset(x_train_dir, y_train_dir, class_rgb_values=select_class_rgb_values)\nrandom_idx = random.randint(0, len(dataset)-1)\nimage, mask = dataset[2]\n\nvisualize(\n    original_image = image,\n    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n    one_hot_encoded_mask = reverse_one_hot(mask)\n)","84346329":"def get_training_augmentation():\n    train_transform = [    \n        album.RandomCrop(height=256, width=256, always_apply=True),\n        album.OneOf(\n            [\n                album.HorizontalFlip(p=1),\n                album.VerticalFlip(p=1),\n                album.RandomRotate90(p=1),\n            ],\n            p=0.75,\n        ),\n    ]\n    return album.Compose(train_transform)\n\n\ndef get_validation_augmentation():   \n    # Add sufficient padding to ensure image is divisible by 32\n    test_transform = [\n        album.PadIfNeeded(min_height=1536, min_width=1536, always_apply=True, border_mode=0),\n    ]\n    return album.Compose(test_transform)\n\n\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\n\ndef get_preprocessing(preprocessing_fn=None):\n    \"\"\"Construct preprocessing transform    \n    Args:\n        preprocessing_fn (callable): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \"\"\"   \n    _transform = []\n    if preprocessing_fn:\n        _transform.append(album.Lambda(image=preprocessing_fn))\n    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n        \n    return album.Compose(_transform)","b84270bf":"augmented_dataset = BuildingsDataset(\n    x_train_dir, y_train_dir, \n    augmentation=get_training_augmentation(),\n    class_rgb_values=select_class_rgb_values,\n)\n\nrandom_idx = random.randint(0, len(augmented_dataset)-1)\n\n# Different augmentations on a random image\/mask pair (256*256 crop)\nfor i in range(3):\n    image, mask = augmented_dataset[random_idx]\n    visualize(\n        original_image = image,\n        ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n        one_hot_encoded_mask = reverse_one_hot(mask)\n    )","6a1deb16":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n    \n    \nclass DownBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DownBlock, self).__init__()\n        self.double_conv = DoubleConv(in_channels, out_channels)\n        self.down_sample = nn.MaxPool2d(2)\n\n    def forward(self, x):\n        skip_out = self.double_conv(x)\n        down_out = self.down_sample(skip_out)\n        return (down_out, skip_out)\n\n    \nclass UpBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, up_sample_mode):\n        super(UpBlock, self).__init__()\n        if up_sample_mode == 'conv_transpose':\n            self.up_sample = nn.ConvTranspose2d(in_channels-out_channels, in_channels-out_channels, kernel_size=2, stride=2)        \n        elif up_sample_mode == 'bilinear':\n            self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        else:\n            raise ValueError(\"Unsupported `up_sample_mode` (can take one of `conv_transpose` or `bilinear`)\")\n        self.double_conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, down_input, skip_input):\n        x = self.up_sample(down_input)\n        x = torch.cat([x, skip_input], dim=1)\n        return self.double_conv(x)\n\n    \nclass UNet(nn.Module):\n    def __init__(self, out_classes=2, up_sample_mode='conv_transpose'):\n        super(UNet, self).__init__()\n        self.up_sample_mode = up_sample_mode\n        # Downsampling Path\n        self.down_conv1 = DownBlock(3, 64)\n        self.down_conv2 = DownBlock(64, 128)\n        self.down_conv3 = DownBlock(128, 256)\n        self.down_conv4 = DownBlock(256, 512)\n        # Bottleneck\n        self.double_conv = DoubleConv(512, 1024)\n        # Upsampling Path\n        self.up_conv4 = UpBlock(512 + 1024, 512, self.up_sample_mode)\n        self.up_conv3 = UpBlock(256 + 512, 256, self.up_sample_mode)\n        self.up_conv2 = UpBlock(128 + 256, 128, self.up_sample_mode)\n        self.up_conv1 = UpBlock(128 + 64, 64, self.up_sample_mode)\n        # Final Convolution\n        self.conv_last = nn.Conv2d(64, out_classes, kernel_size=1)\n\n    def forward(self, x):\n        x, skip1_out = self.down_conv1(x)\n        x, skip2_out = self.down_conv2(x)\n        x, skip3_out = self.down_conv3(x)\n        x, skip4_out = self.down_conv4(x)\n        x = self.double_conv(x)\n        x = self.up_conv4(x, skip4_out)\n        x = self.up_conv3(x, skip3_out)\n        x = self.up_conv2(x, skip2_out)\n        x = self.up_conv1(x, skip1_out)\n        x = self.conv_last(x)\n        return x\n    \n\n# Get UNet model\nmodel = UNet()","952c5648":"# Get train and val dataset instances\ntrain_dataset = BuildingsDataset(\n    x_train_dir, y_train_dir, \n    augmentation=get_training_augmentation(),\n    preprocessing=get_preprocessing(preprocessing_fn=None),\n    class_rgb_values=select_class_rgb_values,\n)\n\nvalid_dataset = BuildingsDataset(\n    x_valid_dir, y_valid_dir, \n    augmentation=get_validation_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn=None),\n    class_rgb_values=select_class_rgb_values,\n)\n\n# Get train and val data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=12)\nvalid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)","f395a0f8":"# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\nTRAINING = True\n\n# Set num of epochs\nEPOCHS = 12\n\n# Set device: `cuda` or `cpu`\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# define loss function\nloss = smp.utils.losses.DiceLoss()\n\n# define metrics\nmetrics = [\n    smp.utils.metrics.IoU(threshold=0.5),\n]\n\n# define optimizer\noptimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=0.00008),\n])\n\n# define learning rate scheduler (not used in this NB)\nlr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n)\n\n# load best saved model checkpoint from previous commit (if present)\nif os.path.exists('..\/input\/unet-for-building-segmentation-pytorch\/best_model.pth'):\n    model = torch.load('..\/input\/unet-for-building-segmentation-pytorch\/best_model.pth', map_location=DEVICE)","e69699c2":"train_epoch = smp.utils.train.TrainEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=DEVICE,\n    verbose=True,\n)\n\nvalid_epoch = smp.utils.train.ValidEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)","620fcc69":"%%time\n\nif TRAINING:\n\n    best_iou_score = 0.0\n    train_logs_list, valid_logs_list = [], []\n\n    for i in range(0, EPOCHS):\n\n        # Perform training & validation\n        print('\\nEpoch: {}'.format(i))\n        train_logs = train_epoch.run(train_loader)\n        valid_logs = valid_epoch.run(valid_loader)\n        train_logs_list.append(train_logs)\n        valid_logs_list.append(valid_logs)\n\n        # Save model if a better val IoU score is obtained\n        if best_iou_score < valid_logs['iou_score']:\n            best_iou_score = valid_logs['iou_score']\n            torch.save(model, '.\/best_model.pth')\n            print('Model saved!')","7d6abb61":"# load best saved model checkpoint from the current run\nif os.path.exists('.\/best_model.pth'):\n    best_model = torch.load('.\/best_model.pth', map_location=DEVICE)\n    print('Loaded UNet model from this run.')\n\n# load best saved model checkpoint from previous commit (if present)\nelif os.path.exists('..\/input\/unet-for-building-segmentation-pytorch\/best_model.pth'):\n    best_model = torch.load('..\/input\/unet-for-building-segmentation-pytorch\/best_model.pth', map_location=DEVICE)\n    print('Loaded UNet model from a previous commit.')","d380f8f5":"# create test dataloader to be used with UNet model (with preprocessing operation: to_tensor(...))\ntest_dataset = BuildingsDataset(\n    x_test_dir, \n    y_test_dir, \n    augmentation=get_validation_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn=None),\n    class_rgb_values=select_class_rgb_values,\n)\n\ntest_dataloader = DataLoader(test_dataset)\n\n# test dataset for visualization (without preprocessing transformations)\ntest_dataset_vis = BuildingsDataset(\n    x_test_dir, y_test_dir, \n    augmentation=get_validation_augmentation(),\n    class_rgb_values=select_class_rgb_values,\n)\n\n# get a random test image\/mask index\nrandom_idx = random.randint(0, len(test_dataset_vis)-1)\nimage, mask = test_dataset_vis[random_idx]\n\nvisualize(\n    original_image = image,\n    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n    one_hot_encoded_mask = reverse_one_hot(mask)\n)\n\n# Notice the images \/ masks are 1536*1536 because of 18px padding on all sides. \n# This is to ensure the input image dimensions to UNet model are a multiple of 2 (to account for pooling & transpose conv. operations).","f2642684":"# Center crop padded image \/ mask to original image dims\ndef crop_image(image, target_image_dims=[1500,1500,3]):\n   \n    target_size = target_image_dims[0]\n    image_size = len(image)\n    padding = (image_size - target_size) \/\/ 2\n\n    return image[\n        padding:image_size - padding,\n        padding:image_size - padding,\n        :,\n    ]","de98ea85":"sample_preds_folder = 'sample_predictions\/'\nif not os.path.exists(sample_preds_folder):\n    os.makedirs(sample_preds_folder)","6d33bdcf":"for idx in range(len(test_dataset)):\n\n    random_idx = random.randint(0, len(test_dataset)-1)\n    image, gt_mask = test_dataset[random_idx]\n    image_vis = crop_image(test_dataset_vis[random_idx][0].astype('uint8'))\n    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n    # Predict test image\n    pred_mask = best_model(x_tensor)\n    pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n    # Convert pred_mask from `CHW` format to `HWC` format\n    pred_mask = np.transpose(pred_mask,(1,2,0))\n    # Get prediction channel corresponding to building\n    pred_building_heatmap = pred_mask[:,:,select_classes.index('building')]\n    pred_mask = crop_image(colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values))\n    # Convert gt_mask from `CHW` format to `HWC` format\n    gt_mask = np.transpose(gt_mask,(1,2,0))\n    gt_mask = crop_image(colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values))\n    cv2.imwrite(os.path.join(sample_preds_folder, f\"sample_pred_{idx}.png\"), np.hstack([image_vis, gt_mask, pred_mask])[:,:,::-1])\n    \n    visualize(\n        original_image = image_vis,\n        ground_truth_mask = gt_mask,\n        predicted_mask = pred_mask,\n        predicted_building_heatmap = pred_building_heatmap\n    )","89ef8736":"test_epoch = smp.utils.train.ValidEpoch(\n    model,\n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)\n\nvalid_logs = test_epoch.run(test_dataloader)\nprint(\"Evaluation on Test Data: \")\nprint(f\"Mean IoU Score: {valid_logs['iou_score']:.4f}\")\nprint(f\"Mean Dice Loss: {valid_logs['dice_loss']:.4f}\")","4941eddc":"train_logs_df = pd.DataFrame(train_logs_list)\nvalid_logs_df = pd.DataFrame(valid_logs_list)\ntrain_logs_df.T","f1ff3b9a":"plt.figure(figsize=(20,8))\nplt.plot(train_logs_df.index.tolist(), train_logs_df.iou_score.tolist(), lw=3, label = 'Train')\nplt.plot(valid_logs_df.index.tolist(), valid_logs_df.iou_score.tolist(), lw=3, label = 'Valid')\nplt.xlabel('Epochs', fontsize=21)\nplt.ylabel('IoU Score', fontsize=21)\nplt.title('IoU Score Plot', fontsize=21)\nplt.legend(loc='best', fontsize=16)\nplt.grid()\nplt.savefig('iou_score_plot.png')\nplt.show()","78750dec":"plt.figure(figsize=(20,8))\nplt.plot(train_logs_df.index.tolist(), train_logs_df.dice_loss.tolist(), lw=3, label = 'Train')\nplt.plot(valid_logs_df.index.tolist(), valid_logs_df.dice_loss.tolist(), lw=3, label = 'Valid')\nplt.xlabel('Epochs', fontsize=21)\nplt.ylabel('Dice Loss', fontsize=21)\nplt.title('Dice Loss Plot', fontsize=21)\nplt.legend(loc='best', fontsize=16)\nplt.grid()\nplt.savefig('dice_loss_plot.png')\nplt.show()","4b819430":"### Defining train \/ val \/ test directories \ud83d\udcc1","38a781d7":"#### Set Hyperparams","f7d94229":"### Model Definition","fe56b3cb":"<h3><center>UNet Model Architecture<\/center><\/h3>\n<img src=\"https:\/\/miro.medium.com\/max\/2824\/1*f7YOaE4TWubwaFF7Z1fzNw.png\" width=\"750\" height=\"750\"\/>\n<h4><center><a href=\"https:\/\/arxiv.org\/abs\/1505.04597\">Image Courtesy: UNet [Ronneberger et al.]<\/a><\/center><\/h4>","3078590c":"## Training UNet","b9f65d44":"#### Get Train \/ Val DataLoaders","159f407e":"## Introduction\n\n### In this notebook we use [UNet](https:\/\/arxiv.org\/abs\/1505.04597) segmentation model for performing building segmentation on [Massachusetts Buildings Dataset](https:\/\/www.cs.toronto.edu\/~vmnih\/docs\/Mnih_Volodymyr_PhD_Thesis.pdf).","7ef04bbb":"### Prediction on Test Data","627b7949":"### Defining Augmentations \ud83d\ude43","12c93a6d":"#### Visualize Augmented Images & Masks","d0c56302":"### Libraries \ud83d\udcda\u2b07","cc55f78f":"### Plot Dice Loss & IoU Metric for Train vs. Val","6ea391e1":"#### Visualize Sample Image and Mask \ud83d\udcc8","5789619a":"### Model Evaluation on Test Dataset","708ab74a":"### Helper functions for viz. & one-hot encoding\/decoding","3f4ac446":"#### Shortlist specific classes to segment","e91d2b94":"### Training UNet"}}