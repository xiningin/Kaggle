{"cell_type":{"7797ac80":"code","6f60344a":"code","35347d46":"code","12240fb2":"code","b4c3912e":"code","08e4b063":"code","d6609744":"code","4fd51317":"code","9e9c6b3f":"code","fe3f7d17":"code","86e62b28":"code","0e6cee37":"code","ee1bc82f":"code","20405840":"code","548cad1a":"code","23133e79":"code","09880a18":"code","f7cf5213":"code","3f8287c0":"code","d1ab8a4a":"code","471793a1":"code","4e4d06be":"code","a0b13f59":"code","277783fc":"code","554708fd":"code","31019904":"code","f05ba0af":"code","ba87fda1":"code","5ab7f87a":"code","61b2e469":"code","43801a57":"code","f081ff86":"code","28d0f7d0":"code","472fff00":"code","40c2572f":"code","053a943d":"code","2acafb1e":"code","a0e0451f":"code","d01427cc":"code","f7b4bbf2":"code","a3ef4a5f":"code","a7db502a":"code","5271559c":"code","4b6eeab6":"markdown","bdece86b":"markdown","c65c83bb":"markdown","2b5cd3a7":"markdown","e1bb46ae":"markdown","4be6048b":"markdown","ca4d8ec6":"markdown","e0658ebf":"markdown","a298d2c9":"markdown","180c4064":"markdown","d952de82":"markdown","98681c92":"markdown","1d24982b":"markdown","212ced1f":"markdown","9de1edc3":"markdown","0636cf77":"markdown","1aaad083":"markdown","d7d9b5cd":"markdown","73bb083b":"markdown","9634eed4":"markdown","5d0cc035":"markdown","409ec3ea":"markdown","69d21449":"markdown","b3cd542e":"markdown","381b4112":"markdown","4fbbd337":"markdown","9e2a2768":"markdown","59905cbb":"markdown","673e95d5":"markdown","8864fbb4":"markdown","7bdda3d6":"markdown","d9de1eeb":"markdown","38a530b9":"markdown","6ecdad2d":"markdown","bda78fd7":"markdown","131e5b6f":"markdown","84975499":"markdown","af8573b5":"markdown"},"source":{"7797ac80":"import numpy as np \nimport pandas as pd\nimport plotly as py\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected = True)\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.model_selection import KFold\nimport optuna\n\npd.set_option('display.max_columns', None)","6f60344a":"train = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\nID = test['PassengerId']","35347d46":"train.head()","12240fb2":"train.info()","b4c3912e":"test.info()","08e4b063":"all_data = pd.concat([train, test])","d6609744":"emb = pd.DataFrame(all_data.groupby(['Pclass', 'Sex', 'SibSp'])['Embarked'].apply(pd.Series.mode).reset_index()).drop('level_3', axis = 1)\n\nfor i in range(len(train.index)):\n    if pd.isna(train.iloc[i,11]) == True:\n        if train.iloc[i,2] == 1 and train.iloc[i,4] == 'female' and train.iloc[i,6] <= 2:\n            train.iloc[i,11] = 'C'\n        else:\n            train.iloc[i,11] = 'S'\n            \nfor i in range(len(test.index)):\n    if pd.isna(test.iloc[i,10]) == True:\n        if test.iloc[i,1] == 1 and test.iloc[i,3] == 'female' and test.iloc[i,5] <= 2:\n            test.iloc[i,10] = 'C'\n        else:\n            test.iloc[i,10] = 'S'","4fd51317":"ages = all_data.groupby(['Pclass', 'Sex', 'SibSp', 'Embarked']).agg({'Age': 'mean'}).reset_index()\n\nfor i in range(len(train.index)):\n    if pd.isna(train.iloc[i,5]) == True:\n        for k in range(len(ages.index)):\n            if train.iloc[i,2] == ages.iloc[k,0] and train.iloc[i,4] == ages.iloc[k,1] and train.iloc[i,6] == ages.iloc[k,2] and train.iloc[i,11] == ages.iloc[k,3]:\n                train.iloc[i,5] = ages.iloc[k,4]\n                \nfor i in range(len(test.index)):\n    if pd.isna(test.iloc[i,4]) == True:\n        for k in range(len(ages.index)):\n            if test.iloc[i,1] == ages.iloc[k,0] and test.iloc[i,3] == ages.iloc[k,1] and test.iloc[i,5] == ages.iloc[k,2] and test.iloc[i,10] == ages.iloc[k,3]:\n                test.iloc[i,4] = ages.iloc[k,4]","9e9c6b3f":"fare = all_data.groupby(['Pclass', 'Sex', 'Embarked']).agg({'Fare': 'mean'}).reset_index()\n\nfor i in range(len(train.index)):\n    if pd.isna(train.iloc[i,9]) == True:\n        for k in range(len(fare.index)):\n            if train.iloc[i,2] == fare.iloc[k,0] and train.iloc[i,4] == fare.iloc[k,1] and train.iloc[i,11] == fare.iloc[k,2]:\n                train.iloc[i,9] = fare.iloc[k,3]\n                \nfor i in range(len(test.index)):\n    if pd.isna(test.iloc[i,8]) == True:\n        for k in range(len(fare.index)):\n            if test.iloc[i,1] == fare.iloc[k,0] and test.iloc[i,3] == fare.iloc[k,1] and test.iloc[i,10] == fare.iloc[k,2]:\n                test.iloc[i,8] = fare.iloc[k,3]","fe3f7d17":"train = train.drop(['PassengerId', 'Ticket', 'Cabin', 'Name'], axis = 1)\ntest = test.drop(['PassengerId', 'Ticket', 'Cabin', 'Name'], axis = 1)","86e62b28":"fig = px.pie(train['Survived'].value_counts().reset_index(), values = 'Survived', names = ['Not survived', 'Survived'],\n                 width = 600, height = 600)\nfig.update_traces(textposition = 'inside', \n                  textinfo = 'percent + label', \n                  hole = 0.78, \n                  marker = dict(colors = ['#A01D26','#ACBEBE'], line = dict(color = 'white', width = 2)))\n\nfig.update_layout(title_text = 'Survivors', title_x = 0.5, title_y = 0.53, title_font_size = 32, title_font_family = 'Calibri Black', title_font_color = 'black',\n                  showlegend = False)\n                  \nfig.show()","0e6cee37":"pclass = train.groupby(['Pclass', 'Survived']).agg({'Survived': 'count'}).rename(columns = {'Survived': 'count'}).reset_index()\nsex = train.groupby(['Sex', 'Survived']).agg({'Survived': 'count'}).rename(columns = {'Survived': 'count'}).reset_index()\nembarked = train.groupby(['Embarked', 'Survived']).agg({'Survived': 'count'}).rename(columns = {'Survived': 'count'}).reset_index()\n\ndef percent(data):\n    data['percent'] = 0\n    for i in range(len(data.index)):\n        if data.index[i] % 2 == 0:\n            data.iloc[i, 3] = round((data.iloc[i, 2] \/ (data.iloc[i, 2] + data.iloc[i+1, 2])) * 100, 1)\n        else:\n            data.iloc[i, 3] = 100 - data.iloc[i-1, 3]\n            \npercent(pclass)\npercent(sex)\npercent(embarked)\n\npclass.iloc[[0,2,4], 1] = 'Not survived'\npclass.iloc[[1,3,5], 1] = 'Survived'","ee1bc82f":"fig = plt.figure(figsize = (18, 18))\nfig.patch.set_facecolor('#fafafa')\n\nplt.subplot(321)\nsns.set_style('white')\nplt.title('Class', size = 20, x = 1.1, y = 1.03)\nplt.grid(color = 'gray', linestyle = ':', axis = 'y', zorder = 0,  dashes = (1,7))\na = sns.barplot(data = pclass, x = pclass['Pclass'], y = pclass['count'], hue = pclass['Survived'], palette = ['#A01D26','#ACBEBE'])\nfor p in a.patches:\n    height = p.get_height()\n    a.annotate(f'{height:g}', (p.get_x() + p.get_width() \/ 2, p.get_height()), \n                   ha = 'center', va = 'center', \n                   size = 10,\n                   xytext = (0, 5), \n                   textcoords = 'offset points')\nplt.ylabel('')\nplt.xlabel('')\nplt.legend(loc = 'upper left')\n\n\nplt.subplot(322)\nplt.grid(color = 'gray', linestyle = ':', axis = 'y', zorder = 0,  dashes = (1,7))\naa = sns.barplot(data = pclass, x = pclass['Pclass'], y = pclass['percent'], hue = pclass['Survived'], palette = ['#A01D26','#ACBEBE'])\nfor p in aa.patches:\n    height = p.get_height()\n    aa.annotate(f'{height:g}%', (p.get_x() + p.get_width() \/ 2, p.get_height()), \n                   ha = 'center', va = 'center', \n                   size = 10,\n                   xytext = (0, 5), \n                   textcoords = 'offset points')\nplt.ylabel('')\nplt.xlabel('')\nplt.legend('').set_visible(False)\n\nplt.subplot(323)\nplt.title('Gender', size = 20, x = 1.1, y = 1.03)\na2 = sns.barplot(data = sex, x = sex['Sex'], y = sex['count'], hue = sex['Survived'], palette = ['#A01D26','#ACBEBE'])\nfor p in a2.patches:\n    height = p.get_height()\n    a2.annotate(f'{height:g}', (p.get_x() + p.get_width() \/ 2, p.get_height()), \n                   ha = 'center', va = 'center', \n                   size = 10,\n                   xytext = (0, 5), \n                   textcoords = 'offset points')\nplt.grid(color = 'gray', linestyle = ':', axis = 'y', zorder = 0,  dashes = (1,7))\nplt.ylabel('')\nplt.xlabel('')\nplt.legend('').set_visible(False)\n\nplt.subplot(324)\nplt.grid(color = 'gray', linestyle = ':', axis = 'y', zorder = 0,  dashes = (1,7))\naa2 = sns.barplot(data = sex, x = sex['Sex'], y = sex['percent'], hue = sex['Survived'], palette = ['#A01D26','#ACBEBE'])\nfor p in aa2.patches:\n    height = p.get_height()\n    aa2.annotate(f'{height:g}%', (p.get_x() + p.get_width() \/ 2, p.get_height()), \n                   ha = 'center', va = 'center', \n                   size = 10,\n                   xytext = (0, 5), \n                   textcoords = 'offset points')\nplt.ylabel('')\nplt.xlabel('')\nplt.legend('').set_visible(False)\n\nplt.subplot(325)\nplt.title('Embarked', size = 20, x = 1.1, y = 1.03)\na3 = sns.barplot(data = embarked, x = embarked['Embarked'], y = embarked['count'], hue = embarked['Survived'], palette = ['#A01D26','#ACBEBE'])\nfor p in a3.patches:\n    height = p.get_height()\n    a3.annotate(f'{height:g}', (p.get_x() + p.get_width() \/ 2, p.get_height()), \n                   ha = 'center', va = 'center', \n                   size = 10,\n                   xytext = (0, 5), \n                   textcoords = 'offset points')\nplt.grid(color = 'gray', linestyle = ':', axis = 'y', zorder = 0,  dashes = (1,7))\nplt.ylabel('')\nplt.xlabel('')\nplt.legend('').set_visible(False)\n\nplt.subplot(326)\nplt.grid(color = 'gray', linestyle = ':', axis = 'y', zorder = 0,  dashes = (1,7))\naa3 = sns.barplot(data = embarked, x = embarked['Embarked'], y = embarked['percent'], hue = embarked['Survived'], palette = ['#A01D26','#ACBEBE'])\nfor p in aa3.patches:\n    height = p.get_height()\n    aa3.annotate(f'{height:g}%', (p.get_x() + p.get_width() \/ 2, p.get_height()), \n                   ha = 'center', va = 'center', \n                   size = 10,\n                   xytext = (0, 5), \n                   textcoords = 'offset points')\nplt.ylabel('')\nplt.xlabel('')\nplt.legend('').set_visible(False)\n\n\nplt.show()","20405840":"fig = plt.figure(figsize = (18, 18))\nfig.patch.set_facecolor('#fafafa')\n\nplt.subplot(221)\nsns.set_style('white')\nplt.title('Age', size = 14)\nsns.kdeplot(train.query('Survived == 0')['Age'], color = '#A01D26', shade = True, label = 'Not survived', alpha = 0.7)\nsns.kdeplot(train.query('Survived == 1')['Age'], color = '#ACBEBE', shade = True, label = 'Survived', alpha = 0.7)\nplt.grid(color = 'gray', linestyle = ':', axis = 'x', zorder = 0,  dashes = (1,7))\nplt.ylabel('')\nplt.xlabel('')\nplt.yticks([])\nplt.legend(loc = 'upper left')\n\nplt.subplot(222)\nplt.title('SibSp', size = 14)\nsns.kdeplot(train.query('Survived == 0')['SibSp'], color = '#A01D26', shade = True, label = 'Not survived', alpha = 0.7)\nsns.kdeplot(train.query('Survived == 1')['SibSp'], color = '#ACBEBE', shade = True, label = 'Survived', alpha = 0.7)\nplt.grid(color = 'gray', linestyle = ':', axis = 'x', zorder = 0,  dashes = (1,7))\nplt.ylabel('')\nplt.xlabel('')\nplt.yticks([])\n\nplt.subplot(223)\nplt.title('Parch', size = 14)\nsns.kdeplot(train.query('Survived == 0')['Parch'], color = '#A01D26', shade = True, label = 'Not survived', alpha = 0.7)\nsns.kdeplot(train.query('Survived == 1')['Parch'], color = '#ACBEBE', shade = True, label = 'Survived', alpha = 0.7)\nplt.grid(color = 'gray', linestyle = ':', axis = 'x', zorder = 0,  dashes = (1,7))\nplt.ylabel('')\nplt.xlabel('')\nplt.yticks([])\n\nplt.subplot(224)\nplt.title('Fare', size = 14)\nsns.kdeplot(train.query('Survived == 0')['Fare'], color = '#A01D26', shade = True, label = 'Not survived', alpha = 0.7)\nsns.kdeplot(train.query('Survived == 1')['Fare'], color = '#ACBEBE', shade = True, label = 'Survived', alpha = 0.7)\nplt.grid(color = 'gray', linestyle = ':', axis = 'x', zorder = 0,  dashes = (1,7))\nplt.ylabel('')\nplt.xlabel('')\nplt.yticks([])\n\nplt.show()","548cad1a":"suv = train.groupby(['Survived', 'Sex', 'Pclass']).agg({'Survived': 'count'}).rename(columns = {'Survived': 'count'}).reset_index()\nsuv.iloc[0:6, 0] = 'Not survived'\nsuv.iloc[6:, 0] = 'Survived'\nfor i in range(len(suv.index)):\n    suv.iloc[i,2] = str(suv.iloc[i,2]) + ' class'\n\nfig = px.sunburst(suv, path = ['Survived', 'Sex', 'Pclass'], values = 'count', color = 'Survived',\n                 color_discrete_map = {'Not survived': '#A01D26', 'Survived': '#ACBEBE'},\n                 width = 700, height = 700)\n\nfig.update_layout(annotations = [dict(text = 'Distribution of male and female survival rates by class', \n                                      x = 0.5, y = 1.1, font_size = 24, showarrow = False, \n                                      font_family = 'Calibri Black',\n                                      font_color = 'black')])\n\nfig.update_traces(textinfo = 'label + percent parent')\n                  \nfig.show()","23133e79":"matrix = np.triu(train.corr())\npalette = ['#ACBEBE', '#A01D26']\nplt.figure(figsize=(13, 10))\nsns.heatmap(train.corr(), annot = True, cmap = palette, fmt=\".2f\", mask = matrix,\n            vmin = -1, vmax = 1, linewidths = 0.1, linecolor = 'white', cbar = False)\nplt.show()","09880a18":"X = train.drop(['Survived'], axis = 1)\ny = train['Survived']\n\nnum_cols = ['Age', 'SibSp', 'Parch', 'Fare']\ncat_cols = ['Sex', 'Embarked']\n\ndef label_encoder(df):\n    for i in cat_cols:\n        le = LabelEncoder()\n        df[i] = le.fit_transform(df[i])\n    return df\n\nsc = StandardScaler()\nX[num_cols] = sc.fit_transform(X[num_cols])\ntest[num_cols] = sc.fit_transform(test[num_cols])\n\nX = label_encoder(X)\ntest = label_encoder(test)\n\nfor i in ['Pclass', 'Sex', 'Embarked']:\n    X[i] = X[i].astype('category')\n    test[i] = test[i].astype('category')\n\nX.head()","f7cf5213":"def objective(trial, data = X, target = y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 2021)\n\n    params = {\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 10, 500),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.6),\n        'cat_feature': ['Pclass', 'Sex', 'Embarked'],\n        'cat_smooth' : trial.suggest_int('cat_smooth', 10, 100),\n        'cat_l2': trial.suggest_int('cat_l2', 1, 20),\n        'min_data_per_group': trial.suggest_int('min_data_per_group', 50, 200),\n        'n_estimators': 10000,\n        'random_state': 2021,\n        'metric': 'binary_logloss'\n    }\n    \n    model = LGBMClassifier(**params)  \n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 100, verbose = False)\n    y_pred = model.predict(X_val)\n    \n    accuracy = accuracy_score(y_val, y_pred)\n    \n    return accuracy\n\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 100)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)\n\nparamsLGBM = study.best_trial.params\nparams = {'n_estimators': 10000, 'random_state': 2021, 'metric': 'binary_logloss', 'cat_feature': ['Pclass', 'Sex', 'Embarked']}\nparamsLGBM.update(params)\n\nfolds = KFold(n_splits = 10, shuffle = True, random_state = 2021)\npredictions = np.zeros(len(test))\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = LGBMClassifier(**paramsLGBM)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False, early_stopping_rounds = 100)\n    \n    predictions += model.predict_proba(test)[:,1] \/ folds.n_splits ","3f8287c0":"sns.histplot(predictions)\nplt.show()","d1ab8a4a":"submission = pd.DataFrame({'PassengerId': ID, 'Survived': np.where(predictions > 0.5, 1, 0)})\nsubmission.to_csv('submissionLGBM.csv', index = False)\nsubmission.head()","471793a1":"submission = pd.DataFrame({'PassengerId': ID, 'Survived': np.where(predictions > 0.45, 1, 0)})\nsubmission.to_csv('submissionLGBM2.csv', index = False)\nsubmission.head()","4e4d06be":"pin_params = {'n_estimators': 10000, 'learning_rate': 0.05, 'metric': 'binary_logloss', 'cat_feature': ['Pclass', 'Sex', 'Embarked']}\n\ndef objective(trial):\n\n    params = {\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 10, 500),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.6),\n        'cat_smooth' : trial.suggest_int('cat_smooth', 10, 100),\n        'cat_l2': trial.suggest_int('cat_l2', 1, 20),\n        'min_data_per_group': trial.suggest_int('min_data_per_group', 50, 200)\n    }\n\n    params.update(pin_params)\n    \n    model = LGBMClassifier(**params) \n    scores = []\n    k = KFold(n_splits = 10, shuffle = True, random_state = 2021)\n    for i, (trn_idx, val_idx) in enumerate(k.split(X)):\n        print(f\"\\n----- FOLD {i} -----\")\n        \n        X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n        model.fit(X, y, eval_set = [(X_val, y_val)], early_stopping_rounds = 25, verbose = 1000)\n        \n        tr_preds = model.predict(X_train)\n        tr_score = accuracy_score(y_train, tr_preds)\n        \n        val_preds = model.predict(X_val)\n        val_score = accuracy_score(y_val, val_preds)\n\n        scores.append((tr_score, val_score))\n        \n        print(f\"Fold {i} | Accuracy: {val_score}\")\n        \n        \n    scores = pd.DataFrame(scores, columns = ['train score', 'validation score'])\n    \n    return scores['validation score'].mean()\n\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, timeout = 1800)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","a0b13f59":"pin_params.update(study.best_params)\n\nmodel = LGBMClassifier(**pin_params) \npredictions = np.zeros(len(test))\nk = KFold(n_splits = 10, shuffle = True, random_state = 2021)\nfor i, (trn_idx, val_idx) in enumerate(k.split(X, y)):\n    print(f\"\\n----- FOLD {i} -----\")\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model.fit(X, y, eval_set = [(X_val, y_val)], early_stopping_rounds = 25, verbose = 1000)\n    predictions += model.predict_proba(test)[:,1] \/ k.n_splits","277783fc":"submission = pd.DataFrame({'PassengerId': ID, 'Survived': np.where(predictions > 0.5, 1, 0)})\nsubmission.to_csv('submissionLGBM3.csv', index = False)\nsubmission.head()","554708fd":"for i in ['Pclass', 'Sex', 'Embarked']:\n    X[i] = X[i].astype('int')\n    test[i] = test[i].astype('int')","31019904":"def objective(trial, data = X, target = y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 2021)\n    \n    params = {\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n        'gamma': trial.suggest_float('gamma', 0.0001, 1.0, log = True),\n        'alpha': trial.suggest_float('alpha', 0.0001, 10.0, log = True),\n        'lambda': trial.suggest_float('lambda', 0.0001, 10.0, log = True),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.6),\n        'subsample': trial.suggest_float('subsample', 0.1, 0.6),\n        'max_bin': trial.suggest_int('max_bin', 50, 500),\n        'n_estimators': 10000,\n        'random_state': 2021,\n        'objective': 'binary:logistic',\n        'eval_metric': 'logloss'\n\n    }\n    \n    model = XGBClassifier(**params)  \n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 100, verbose = False)\n    y_pred = model.predict(X_val)\n    \n    accuracy = accuracy_score(y_val, y_pred)\n    \n    return accuracy\n\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 100)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","f05ba0af":"paramsXGB = study.best_trial.params\nparams = {'n_estimators': 10000, 'random_state': 2021}\nparamsXGB.update(params)\n\nfolds = KFold(n_splits = 10, shuffle = True, random_state = 2021)\npredictions = np.zeros(len(test))\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = XGBClassifier(**paramsXGB)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], eval_metric = 'logloss', verbose = False, early_stopping_rounds = 100)\n    \n    predictions += model.predict_proba(test)[:,1] \/ folds.n_splits ","ba87fda1":"sns.histplot(predictions)\nplt.show()","5ab7f87a":"submission = pd.DataFrame({'PassengerId': ID, 'Survived': np.where(predictions > 0.5, 1, 0)})\nsubmission.to_csv('submissionXGB.csv', index = False)\nsubmission.head()","61b2e469":"submission = pd.DataFrame({'PassengerId': ID, 'Survived': np.where(predictions > 0.45, 1, 0)})\nsubmission.to_csv('submissionXGB2.csv', index = False)\nsubmission.head()","43801a57":"rf = RandomForestClassifier(random_state = 2021)\n\nparams = { \n    'n_estimators': [200, 500, 1000, 2000, 5000],\n    'max_depth' : range(3,8)\n}\n\nCV_rf = GridSearchCV(estimator = rf, param_grid = params, cv = 5, scoring = 'accuracy')\nCV_rf.fit(X, y)","f081ff86":"best_rf = CV_rf.best_estimator_\nbest_rf","28d0f7d0":"predictions = best_rf.predict_proba(test)[:,1]\nsubmission = pd.DataFrame({'PassengerId': ID, 'Survived': np.where(predictions > 0.5, 1, 0)})\nsubmission.to_csv('submissionRF.csv', index = False)\nsubmission.head()","472fff00":"submission = pd.DataFrame({'PassengerId': ID, 'Survived': np.where(predictions > 0.45, 1, 0)})\nsubmission.to_csv('submissionRF2.csv', index = False)\n\nsubmission = pd.DataFrame({'PassengerId': ID, 'Survived': np.where(predictions > 0.55, 1, 0)})\nsubmission.to_csv('submissionRF3.csv', index = False)","40c2572f":"train['Cabin'] = train['Cabin'].map(lambda x: str(x)[0].strip())\ntest['Cabin'] = test['Cabin'].map(lambda x: str(x)[0].strip())\n\ntrain['FamilySize'] = train['SibSp'] + train['Parch'] + 1\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1\n\ntrain['is_alone'] = train['FamilySize'].apply(lambda x: 1 if x == 1 else 0)\ntest['is_alone'] = test['FamilySize'].apply(lambda x: 1 if x == 1 else 0)\n\ntrain['ticket_type'] = 0\nfor i in range(len(train)):\n    if pd.isna(train.iloc[i,8]) == True:\n        train.iloc[i,14] = 'Na'\n    elif train.iloc[i,8].isdigit() == True:\n        train.iloc[i,14] = 'N'\n    else:\n        train.iloc[i,14] = train.iloc[i,8].split(' ')[0]\n        \ntest['ticket_type'] = 0\nfor i in range(len(test)):\n    if pd.isna(test.iloc[i,7]) == True:\n        test.iloc[i,13] = 'Na'\n    elif test.iloc[i,7].isdigit() == True:\n        test.iloc[i,13] = 'N'\n    else:\n        test.iloc[i,13] = test.iloc[i,7].split(' ')[0]","053a943d":"train = train.drop(['PassengerId', 'Name', 'Ticket'], axis = 1)\ntest = test.drop(['PassengerId', 'Name', 'Ticket'], axis = 1)","2acafb1e":"!pip install -U lightautoml","a0e0451f":"from lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.dataset.roles import DatetimeRole\nfrom lightautoml.tasks import Task\nfrom lightautoml.utils.profiler import Profiler\nimport torch","d01427cc":"N_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 10 # folds cnt for AutoML\nRANDOM_STATE = 2021 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 300 # Time in seconds for automl run\n\nnp.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","f7b4bbf2":"# Task\ndef acc_score(y_true, y_pred, **kwargs):\n    return accuracy_score(y_true, (y_pred > 0.5).astype(int), **kwargs)\n\ntask = Task('binary', metric = 'logloss')\n\n# Column role\nroles = {'target': 'Survived'}","a3ef4a5f":"%%time\nautoml = TabularUtilizedAutoML(task = task, \n                       timeout = TIMEOUT,\n                       cpu_limit = N_THREADS,\n                       general_params = {'use_algos': [['linear_l2', 'lgb', 'lgb_tuned']]},\n                       reader_params = {'n_jobs': N_THREADS})\noof_pred = automl.fit_predict(train, roles = roles)\nprint(f'oof_pred:\\n{oof_pred[:10]}\\nShape = {oof_pred.shape}')","a7db502a":"predictions = automl.predict(test)\nprint(f'Prediction for test data:\\n{predictions[:10]}\\nShape = {predictions.shape}')\n\nprint('Check scores...')\nprint('OOF score: {}'.format(acc_score(train['Survived'].values, oof_pred.data[:, 0])))","5271559c":"submission = pd.DataFrame({'PassengerId': ID, 'Survived': (predictions.data[:, 0] > 0.5).astype(int)})\nsubmission.to_csv('submissionAutoML.csv', index = False)\nsubmission.head()","4b6eeab6":"**Imputing fare**","bdece86b":"Great, we have missing values! Let's impute them.","c65c83bb":"**Imputing embarked**","2b5cd3a7":"**Distribution of survivors**","e1bb46ae":"**Affect of class, gender and embarked on survival**","4be6048b":"# Basic information","ca4d8ec6":"result - 0.77387. It's sad :(\n\n(from past notebook version)","e0658ebf":"For modeling 1 I will use LGBM tuned with Optuna (100 trials)","a298d2c9":"**Delete unnecessary columns**","180c4064":"# Modeling 2","d952de82":"**Imputing age**","98681c92":"First, let's try to create new features. For this I repeat my preproceesing without deleting columns (except PassengerId and Name).","1d24982b":"# EDA","212ced1f":"Result - 0.79288 (from past notebook version)","9de1edc3":"For modeling 2 I try to use tuned with Optuna (30 minutes) LGBM with cross-validation (10 folds)","0636cf77":"# Modeling 3","1aaad083":"# Modeling 1","d7d9b5cd":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/26478\/logos\/header.png?t=2021-03-29-17-07-0)","73bb083b":"Let's try to change threshold","9634eed4":"# Preprocessing","5d0cc035":"According to recent observations, I have noticed that many masters successfully use AutoML, so I'll try to keep up.","409ec3ea":"**I will use all information to imputing missing values**","69d21449":"Result - 0.78945 (from past notebook version)","b3cd542e":"**Distribution of male and female survival rates by class**","381b4112":"# Random Forest (why not?)","4fbbd337":"Let's change threshold","9e2a2768":"Result - 0.79450 (from past notebook version)","59905cbb":"# Prepare for modeling","673e95d5":"1. As in the original Titanic dataset, the most important factors affecting survival are the class and gender of the passenger. \n2. Also a great importance have a port of embarkation. \n3. Age also affects survival, but not much. \n4. Count of of siblings \/ spouses \/ parents \/ children have almost no effect on survival.","8864fbb4":"For modeling 3 I try to use tuned XGB with Optuna (100 trials)","7bdda3d6":"**Affect of age, SibSp, parch and fare on survival**","d9de1eeb":"Result - 0.78961","38a530b9":"Result - 0.79151","6ecdad2d":"Results - 0.79401 and 0.77871","bda78fd7":"Let's try to change threshold","131e5b6f":"# Conclusions of EDA","84975499":"Result - 0.79591 (from past notebook version)","af8573b5":"# AutoML"}}