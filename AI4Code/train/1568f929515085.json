{"cell_type":{"2f7a9b47":"code","a9c39600":"code","a6f9679a":"code","35c84a8c":"code","b9782a2d":"code","3e43bb37":"code","dc7d0f5f":"code","3fdece68":"code","f1c3017d":"code","1b2c28c1":"code","d3b482b7":"code","17a97546":"code","378d25c1":"code","81e2d25b":"code","54116278":"code","51250e83":"code","42347c64":"code","62ac6e11":"code","b9af4f0e":"code","3ee1ecbd":"code","ae8758ce":"code","86173bb6":"code","f43b5634":"code","a11cf85f":"code","6c529973":"markdown","b026389c":"markdown","2ccc04eb":"markdown","325a2d6e":"markdown","f491e0f5":"markdown","b09743bd":"markdown","5378f757":"markdown","a638a3b8":"markdown","cd8ea0e7":"markdown","11d684ff":"markdown","dff281a6":"markdown","0e6dabad":"markdown","6018d65e":"markdown","bc87b7d0":"markdown","d9acc20f":"markdown","b58a8487":"markdown","7d4e6f07":"markdown","72a9ee71":"markdown","b657f183":"markdown","e509635d":"markdown","45b28bf1":"markdown","dabcab81":"markdown","c31d3682":"markdown","0ad49e2f":"markdown","34c2b04f":"markdown","149ce9cd":"markdown","431e8e96":"markdown","bc682579":"markdown","4e1a0273":"markdown","d1335220":"markdown","047fa2b0":"markdown","b7c726e4":"markdown","fb1f4a14":"markdown","819faac1":"markdown","82a042a1":"markdown"},"source":{"2f7a9b47":"import numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\n\n%matplotlib inline","a9c39600":"def load_dataset():\n    train_dataset = h5py.File('..\/input\/train_catvnoncat.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n\n    test_dataset = h5py.File('..\/input\/test_catvnoncat.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n    \n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    \n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes","a6f9679a":"# Carga de datos (gato\/no-gato)\ntrain_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\nclases=[\"no-gato\", \"gato\"]","35c84a8c":"train_set_x_orig[0].shape","b9782a2d":"# Ejemplo de una imagen\nindice = 0\nplt.imshow(train_set_x_orig[indice])\nprint (\"La imagen #\" + str(indice) + \", es un '\" + str(clases[np.squeeze(train_set_y[:, indice])]) + \"'\" )","3e43bb37":"### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 3 l\u00edneas de c\u00f3digo)\nm_train = \nm_test =\nnum_px = \n### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n\nprint (\"N\u00famero de ejemplos de entrenamiento: m_train = \" + str(m_train))\nprint (\"N\u00famero de ejemplos de prueba: m_test = \" + str(m_test))\nprint (\"Altura\/Ancho de cada imagen: num_px = \" + str(num_px))\n\nprint (\"Cada imagen es de tama\u00f1o: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\nprint (\"Dimensi\u00f3n del train_set_x: \" + str(train_set_x_orig.shape))\nprint (\"Dimensi\u00f3n del train_set_y: \" + str(train_set_y.shape))\nprint (\"Dimensi\u00f3n del test_set_x: \" + str(test_set_x_orig.shape))\nprint (\"Dimensi\u00f3n del test_set_y: \" + str(test_set_y.shape))","dc7d0f5f":"# Re-dimensione los ejemplos de entrenamiento y prueba\n\n### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 2 l\u00edneas de c\u00f3digo)\ntrain_set_x_flatten = \ntest_set_x_flatten = \n### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n\nprint (\"Dimensi\u00f3n del train_set_x_flatten: \" + str(train_set_x_flatten.shape))\nprint (\"Dimensi\u00f3n del train_set_y: \" + str(train_set_y.shape))\nprint (\"Dimensi\u00f3n del test_set_x_flatten: \" + str(test_set_x_flatten.shape))\nprint (\"Dimensi\u00f3n del test_set_y: \" + str(test_set_y.shape))\nprint (\"Chequeo luego del re-dimensionamiento: \" + str(train_set_x_flatten[0:5,0]))","3fdece68":"train_set_x = train_set_x_flatten\/255.\ntest_set_x = test_set_x_flatten\/255.","f1c3017d":"# FUNCI\u00d3N A CALIFICAR: sigmoid\n\ndef sigmoid(z):\n    \"\"\"\n    Calcule el sigmoide de z\n    Input:\n    z: Un escalar o arreglo numpy de cualquier tama\u00f1o\n    Output:\n    s: sigmoid(z)\n    \"\"\"\n\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ###  (\u2248 1 l\u00ednea de c\u00f3digo)\n    s = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    return s","1b2c28c1":"print (\"sigmoide([0, 2]) = \" + str(sigmoid(np.array([0,2]))))","d3b482b7":"# FUNCI\u00d3N A CALIFICAR: initialize_with_zeros\n\ndef initialize_with_zeros(dim):\n    \"\"\"\n    Esta funci\u00f3n crea un vector de ceros de dimensi\u00f3n (dim, 1) para w e inicializa b a 0.\n    Input:\n    dim: tama\u00f1o del vector w (n\u00famero de par\u00e1metros para este caso)\n    Output:\n    w: vector inicializado de tama\u00f1o (dim, 1)\n    b: escalar inicializado (corresponde con el sesgo)\n    \"\"\"\n    \n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 1 l\u00ednea de c\u00f3digo)\n    w =    \n    b = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n\n    assert(w.shape == (dim, 1))\n    assert(isinstance(b, float) or isinstance(b, int))\n    \n    return w, b","17a97546":"dim = 2\nw, b = initialize_with_zeros(dim)\nprint (\"w = \" + str(w))\nprint (\"b = \" + str(b))","378d25c1":"# FUNCI\u00d3N A CALIFICAR: propagate\n\ndef propagate(w, b, X, Y):\n    \"\"\"\n    Implemente la funci\u00f3n de coste y su gradiente para la propagaci\u00f3n\n    Input:\n    w: pesos, un arreglo numpy de tama\u00f1o (num_px * num_px * 3, 1)\n    b: sesgo, un escalar\n    X: datos de tama\u00f1o (num_px * num_px * 3, n\u00famero de ejemplos)\n    Y: vector de etiquetas observadas (0 si es no-gato, 1 si es gato) de tama\u00f1o (1, n\u00famero de ejemplos)\n    Output:\n    coste: coste negativo de log-verosimilitud para la regresi\u00f3n log\u00edstica\n    dw: gradiente de la p\u00e9rdida con respecto a w, con las mismas dimensiones que w\n    db: gradiente de la p\u00e9rdida con respecto a b, con las mismas dimensiones que b\n    \n    (Sugerencia: escriba su c\u00f3digo paso a paso para la propagaci\u00f3n. np.log(), np.dot()\n    \"\"\"\n    \n    m = X.shape[1]\n    \n    # PROPAGACI\u00d3N HACIA DELANTE \n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 2 l\u00edneas de c\u00f3digo)\n    A =                                                      # compute la activaci\u00f3n\n    cost =                                                   # compute el coste\n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    # RETRO-PROPAGACI\u00d3N (PROPAGACI\u00d3N HACIA ATR\u00c1S)\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 2 l\u00edneas de c\u00f3digo)\n    dw = \n    db = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost","81e2d25b":"w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"coste = \" + str(cost))","54116278":"# FUNCI\u00d3N A CALIFICAR: optimize\n\ndef optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    Esta funci\u00f3n optimiza w y b implementando el algoritmo de GD\n    Input:\n    w: pesos, un arreglo numpy de tama\u00f1o (num_px * num_px * 3, 1)\n    b: sesgo, un escalar\n    X: datos de tama\u00f1o (num_px * num_px * 3, n\u00famero de ejemplos)\n    Y: vector de etiquetas observadas (0 si es no-gato, 1 si es gato) de tama\u00f1o (1, n\u00famero de ejemplos)\n    num_iterations: n\u00famero de iteracionespara el bucle de optimizaci\u00f3n\n    learning_rate: tasa de aprendizaje para la regla de actualizaci\u00f3n del GD\n    print_cost: True para imprimir la p\u00e9rdida cada 100 iteraciones\n    Output:\n    params: diccionario con los pesos w y el sesgo b\n    grads: diccionario con los gradientes de los pesos y el sesgo con respecto a la funci\u00f3n de p\u00e9rdida\n    costs: lista de todos los costes calculados durante la optimizaci\u00f3n, usados para graficar la curva de aprendizaje.\n    \n    Sugerencia: puede escribir dos pasos e iterar sobre ellos:\n        1) Calcule el coste y el gradiente de los par\u00e1metros actuales. Use propagate().\n        2) Actualize los par\u00e1metros usando la regla del GD para w y b.\n    \"\"\"\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        \n        # Computaci\u00f3n del coste y el gradiente (\u2248 1-4 l\u00edneas de c\u00f3digo)\n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### \n        grads, cost = \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n        \n        # Recupere las derivadas de grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        # Actualize la regla (\u2248 2 l\u00edneas de c\u00f3digo)\n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ###\n        w = \n        b = \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n        \n        # Guarde los costes\n        if i % 100 == 0:\n            costs.append(cost)\n        \n        # Se muestra el coste cada 100 iteraciones de entrenamiento\n        if print_cost and i % 100 == 0:\n            print (\"Coste tras la iteraci\u00f3n %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs","51250e83":"params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint (\"w = \" + str(params[\"w\"]))\nprint (\"b = \" + str(params[\"b\"]))\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))","42347c64":"# FUNCI\u00d3N A CALIFICAR: predict\n\ndef predict(w, b, X):\n    '''\n    Prediga si una etiqueta es 0 o 1 usando los par\u00e1metros de regresi\u00f3n log\u00edstica aprendidos (w, b)\n    Input:\n    w: pesos, un arreglo numpy de tama\u00f1o (num_px * num_px * 3, 1)\n    b: sesgo, un escalar\n    X: datos de tama\u00f1o (num_px * num_px * 3, n\u00famero de ejemplos)\n    Output:\n    Y_prediction: vector con todas las predicciones (0\/1) para los ejemplos en X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0], 1)\n    \n    # Compute el vector \"A\" prediciendo las probabilidades de que la imagen contenga un gato\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 1 l\u00ednea de c\u00f3digo)\n    A = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    for i in range(A.shape[1]):\n        \n        # Convierta las probabilidades A[0,i] a predicciones p[0,i]\n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 1-4 l\u00edneas de c\u00f3digo)\n        Y_prediction[0,i] =\n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    assert(Y_prediction.shape == (1, m))\n    \n    return Y_prediction","62ac6e11":"w = np.array([[0.1124579],[0.23106775]])\nb = -0.3\nX = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\nprint (\"predicciones = \" + str(predict(w, b, X)))","b9af4f0e":"# FUNCI\u00d3N A CALIFICAR: model\n\ndef model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Construye el modelo de regresi\u00f3n log\u00edstica llamando las funciones implementadas anteriormente\n    Input:\n    X_train: conjunto de entrenamiento con dimensiones (num_px * num_px * 3, m_train)\n    Y_train: vector con las etiquetas de entrenamiento con dimensiones (1, m_train)\n    X_test: conjunto de prueba con dimensiones (num_px * num_px * 3, m_test)\n    Y_test: vector con las etiquetas de prueba con dimensiones (1, m_test)\n    num_iterations: (hiper-par\u00e1metro) n\u00famero de iteracionespara para optimizar los par\u00e1metros\n    learning_rate: (hiper-par\u00e1metro) tasa de aprendizaje para la regla de optimize()\n    print_cost: True para imprimir la p\u00e9rdida cada 100 iteraciones\n    Output:\n    d: diccionario con la informaci\u00f3n sobre el modelo.\n    \"\"\"\n    \n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ###\n    \n    # Inicialize los parametros con ceros (\u2248 1 l\u00ednea de c\u00f3digo)\n    w, b = \n\n    # Descenso en la direcci\u00f3n del gradiente (GD) (\u2248 1 l\u00ednea de c\u00f3digo)\n    parameters, grads, costs = \n    \n    # Recupere los par\u00e1metros w y b del diccionario \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    \n    # Prediga los ejemplos de prueba y entrenamiento (\u2248 2 l\u00edneas de c\u00f3digo)\n    Y_prediction_test = \n    Y_prediction_train = \n\n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n\n    # Imprima los errores de entrenamiento y prueba\n    print(\"Precisi\u00f3n de entrenamiento: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"Precisi\u00f3n de prueba: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {\"Costes\": costs,\n         \"Prediccion_prueba\": Y_prediction_test, \n         \"Prediccion_entrenamiento\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"Tasa de aprendizaje\" : learning_rate,\n         \"Num_iteraciones\": num_iterations}\n    \n    return d","3ee1ecbd":"d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)","ae8758ce":"# Ejemplo de una imagen mal clasificada.\nindex = 6\nplt.imshow(test_set_x_orig[:,index].reshape((num_px, num_px, 3)))\nclase=clases[int(d[\"Prediccion_prueba\"][0,index])]\nprint (\"Para y = \" + str(test_set_y[0,index]) + \", el modelo dice que es una imagen de un \\\"\" + clase +  \"\\\".\")","86173bb6":"# Gr\u00e1fica de la curva de aprendizaje (con costes)\ncostes = np.squeeze(d['Costes'])\nplt.plot(costes)\nplt.ylabel('coste')\nplt.xlabel('iteraciones (en cientos)')\nplt.title(\"Tasa de aprendizaje =\" + str(d[\"Tasa de aprendizaje\"]))\nplt.show()","f43b5634":"learning_rates = [0.01, 0.001, 0.0001]\nmodels = {}\nfor i in learning_rates:\n    print (\"La tasa de aprendizaje es: \" + str(i))\n    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n\nfor i in learning_rates:\n    plt.plot(np.squeeze(models[str(i)][\"Costes\"]), label= str(models[str(i)][\"Tasa de aprendizaje\"]))\n\nplt.ylabel('coste')\nplt.xlabel('iteraciones (en cientos)')\n\nlegend = plt.legend(loc='upper center', shadow=True)\nframe = legend.get_frame()\nframe.set_facecolor('0.90')\nplt.show()","a11cf85f":"### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (INTRODUZCA EL NOMBRE DE SU IMAGEN) \nmy_image = \" \"   # el nombre debe coincidir con el de su imagen\n### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n\n# Pre-procesamos la imagen\nfname = \"images\/\" + my_image\nimage = np.array(ndimage.imread(fname, flatten=False))\nmy_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T\nmy_predicted_image = predict(d[\"w\"], d[\"b\"], my_image)\n\nplt.imshow(image)\nprint(\"y = \" + str(int(np.squeeze(my_predicted_image))) + \", el algoritmo predice que es una imagen de un \\\"\" + clases[int(np.squeeze(my_predicted_image))]+  \"\\\".\")","6c529973":"## 7 - Pruebe con otras imagenes ##\n\nPuede utilizar imagenes propias para ver los resultados de su modelo. Para ello, agregue su(s) imagen(es) al directorio de este cuaderno en la carpeta \"images\", cambie el nombre de la(s) imagen(es) en el siguiente c\u00f3digo, y compruebe si el algoritmo acierta (1=gato, 0=no-gato). ","b026389c":"Grafiquemos la funci\u00f3n de p\u00e9rdida y los gradientes.","2ccc04eb":"Comprobamos las dimensiones de una observaci\u00f3n","325a2d6e":"**Ejercicio:** La funci\u00f3n anterior aprende los par\u00e1metros w y b, que se pueden usar para predecir las etiquetas para el conjunto de datos X. Ahora implemente la funci\u00f3n `predict()`. Hay dos pasos para calcular las predicciones:\n\n1. Calcule $\\hat{Y} = A = \\sigma(w^T X + b)$\n\n2. Convierta a 0 las entradas de a (si la activaci\u00f3n es <= 0.5) o 1 (si la activaci\u00f3n es > 0.5), guarde las predicciones en un vector `Y_prediction`. Si lo desea, puede usar un `if`\/`else` en un bucle `for`, aunque tambi\u00e9n hay una manera de vectorizarlo. ","f491e0f5":"**Salida esperada**:  \n\n<table style=\"width:40%\">\n    <tr>\n       <td> **w** <\/td>\n       <td>[[ 0.19033591]\n [ 0.12259159]] <\/td>\n    <\/tr>\n    \n    <tr>\n       <td> **b** <\/td>\n       <td> 1.92535983008 <\/td>\n    <\/tr>\n    <tr>\n       <td> **dw** <\/td>\n       <td> [[ 0.67752042]\n [ 1.41625495]] <\/td>\n    <\/tr>\n    <tr>\n       <td> **db** <\/td>\n       <td> 0.219194504541 <\/td>\n    <\/tr>\n\n<\/table>","b09743bd":"A\u00f1adimos \"_orig\" al final de los datos de entrenamiento y prueba porque vamos a pre-procesarlos. Luego de esto, vamos a obtener un train_set_x y un test_set_x (n\u00f3tese que las etiquetas de train_set_y y test_set_y no necesitan ning\u00fan pre-procesamiento).\n\nCada observaci\u00f3n (l\u00ednea) del train_set_x_orig y del test_set_x_orig es un arreglo representando una imagen. Se puede visualizar cada observaci\u00f3n mediante el siguiente c\u00f3digo. Puede cambiar el valor del `indice` para visualizar imagenes distintas. ","5378f757":"### 4.4 - Optimizaci\u00f3n\n- Se tienen los par\u00e1metros inicializados.\n- Tambi\u00e9n se tiene el c\u00f3digo para calcular la funci\u00f3n de coste y su gradiente.\n- Ahora se quieren actualizar los par\u00e1metros utilizando el descenso en la direcci\u00f3n del gradiente (GD).\n\n**Ejercicio:** Escriba la funci\u00f3n de optimizaci\u00f3n. EL objetivo es el de aprender $w$ y $b$ minimizando la funci\u00f3n de coste $J$. Para un par\u00e1metro $\\theta$, la regla de actualizaci\u00f3n es $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, donde $\\alpha$ es la tasa de aprendizaje.","a638a3b8":"#### Selecci\u00f3n de la tasa de aprendizaje ####\n\n**Nota**:\nPara que el m\u00e9todo del GD funcione de manera adecuada, se debe elegir la tasa de aprendiazaje de manera acertada. Esta tasa $\\alpha$  determina qu\u00e9 tan r\u00e1pido se actualizan los par\u00e1metros. Si la tasa es muy grande se puede \"sobrepasar\" el valor \u00f3ptimo. Y de manera similar, si es muy peque\u00f1a se van a necesitar muchas iteraciones para converger a los mejores valores. Por ello la importancia de tener una tase de aprensizaje bien afinada.  \n\nAhora, comparemos la curva de aprendizaje de nuestro modelo con distintas elecciones para $\\alpha$. Ejecute el c\u00f3digo abajo. Tambi\u00e9n puede intentar con valores distintos a los tres que estamos utilizando abajo para `learning_rates` y analize los resultados. ","cd8ea0e7":"Run the following cell to train your model.","11d684ff":"Algunas referencias:\n- http:\/\/www.wildml.com\/2015\/09\/implementing-a-neural-network-from-scratch\/\n- https:\/\/stats.stackexchange.com\/questions\/211436\/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c\n\n\nEspero te haya gustado este Notebook. Por favor compartelo para que entre todos aprendamos juntos.\n\nPuedes Seguirme a mi cuenta en Twitter **[@andres_jejen](https:\/\/twitter.com\/andres_jejen)** Constantemente comparto noticias y contenido educativo sibre Machine Learning, Big Data y Data Science.","dff281a6":"## 1 - Paquetes ##\n\nPrimero, importamos los paquetes que vamos a necesitar a lo largo de este taller. \n- [numpy](www.numpy.org) paquete b\u00e1sico para ciencias computacionales con Python.\n- [h5py](http:\/\/www.h5py.org) paquete para interactuar con un conjunto de datos guardado en un archivo de tipo H5.\n- [matplotlib](http:\/\/matplotlib.org) librer\u00eda para graficar en Python.\n- [PIL](http:\/\/www.pythonware.com\/products\/pil\/) y [scipy](https:\/\/www.scipy.org\/) usados para probar el modelo con sus propias imagenes al final del taller","0e6dabad":"**Salida esperada**: \n\n<table style=\"width:50%\">\n    <tr>\n        <td>  ** dw **  <\/td>\n      <td> [[ 0.99845601]\n     [ 2.39507239]]<\/td>\n    <\/tr>\n    <tr>\n        <td>  ** db **  <\/td>\n        <td> 0.00145557813678 <\/td>\n    <\/tr>\n    <tr>\n        <td>  ** cost **  <\/td>\n        <td> 5.801545319394553 <\/td>\n    <\/tr>\n\n<\/table>","6018d65e":"**Discusi\u00f3n**: \n- Tasas diferentes obtienen costes diferentes y por lo tanto, predicciones diferentes.\n- Si la tasa es muy grande (0.01), el coste puede oscilar arriba y abajo. Hasta puede diverger, aunque en este ejemplo $\\alpha=0.01$ aun consigue un buen valor para el coste.  \n- Un coste m\u00e1s bajo no implica un mejor modelo. Se debe chequear si hay una posibilidad de sobre-ajuste. Esto ocurre cuando la precisi\u00f3n de entrenamiento es mucho mayor que la precisi\u00f3n de prueba.\n- En deep learning, es recomendable que se elija la tasa de aprendizaje que minimize la funci\u00f3n de coste. Y si el modelo sobre-ajusta, se pueden probar otras t\u00e9cnicas (que veremos m\u00e1s adelante) para reducir dicho sobre-ajuste. \n","bc87b7d0":"**Salida esperada**: \n\n<table style=\"width:30%\">\n    <tr>\n         <td>\n             **predicciones**\n         <\/td>\n          <td>\n            [[ 1.  1.  0.]]\n         <\/td>  \n   <\/tr>\n\n<\/table>\n","d9acc20f":"### 4.2 - Inicializaci\u00f3n de par\u00e1mteros\n\n**Ejercicio:** Implemente la inicializaci\u00f3n de par\u00e1metros. Se tiene un vector w de ceros. Si no sabe qu\u00e9 funci\u00f3n de numpy puede utilizar, puede buscar np.zeros() en la documentaci\u00f3n de la biblioteca Numpy.","b58a8487":"**Nota**: La precisi\u00f3n de entrenamiento es cercana al 100%. Esto es una buena se\u00f1al de que el modelo est\u00e1 aprendiendo, ya que muestra capacidad suficiente para ajustarse a los datos de entrenamiento. Por el otro lado, el error de prueba es del 70%. Este resultado no est\u00e1 mal tomando en cuenta que es un modelo bastante simple, dado el conjunto de datos que se ha usado, el cual es relativamente peque\u00f1o, y que el modelo de regresi\u00f3n log\u00edstica es un calsificador lineal. La pr\u00f3xima semana veremos un clasificador m\u00e1s complejo, y que permitir\u00e1 obtener mejores resultados. \n\nN\u00f3tese tambi\u00e9n que el modelo se est\u00e1 sobre-ajustando a los datos de entrenamiento. M\u00e1s adelante veremos c\u00f3mo se peude reducir este sobre-ajuste (\"overfitting\"), por ejemplo mediante regularizaci\u00f3n. A continuaci\u00f3n puede examinar las predicciones de las imagenes de prueba.","7d4e6f07":"## 5 - Fusione todas las funciones ##\n\nAhora debe construir el modelo global, estructurando todos los bloques que ha programado arriba.\n\n**Ejercicio:** Implemente la funci\u00f3n madre. Use la siguiente notaci\u00f3n:\n    - Y_prediction_test para las predicciones sobr el conjunto de prueba\n    - Y_prediction_train para las predicciones sobre el conjunto de entrenamiento\n    - w, costs, grads para las salidas de optimize()","72a9ee71":"**Salida esperada**: \n\n<table style=\"width:50%\"> \n\n    <tr>\n        <td> **Coste tras la iteraci\u00f3n 0 **  <\/td> \n        <td> 0.693147 <\/td>\n    <\/tr>\n      <tr>\n        <td> <center> $\\vdots$ <\/center> <\/td> \n        <td> <center> $\\vdots$ <\/center> <\/td> \n    <\/tr>  \n    <tr>\n        <td> **Precisi\u00f3n de entrenamiento**  <\/td> \n        <td> 74.64114832535886 % <\/td>\n    <\/tr>\n\n    <tr>\n        <td>**Precisi\u00f3n de prueba** <\/td> \n        <td> 34.0 % <\/td>\n    <\/tr>\n<\/table> \n\n\n","b657f183":"<font color='blue'>\n**Recapitulemos:**\n\nPasos comunes para el pre-procesamiento de un nuevo conjunto de datos:\n- Examinar las dimensiones del problema (m_train, m_test, num_px, ...)\n- Re-dimensionar los conjuntos de datos para que cada ejemplo sea un vector de tama\u00f1o (num_px \\* num_px \\* 3, 1)\n- Normalizar o estandarizar los datos","e509635d":"## 2 - Enunciado del problema ##\n\n**Enunciado**: El siguiente conjunto de datos est\u00e1 disponible (\"data.h5\") con la siguiente informaci\u00f3n:\n    - un conjunto de entrenamiento m_train con imagenes etiquetadas como gato (y=1) o no-gato (y=0)\n    - un conjunto de prueba m_test con imagenes etiquetadas como cat\/gato o non-cat\/no-gato\n    - cada imagen tiene dimensiones (num_px, num_px, 3) donde 3 es para los canales RGB (n\u00f3tese que cada imagen es cuadrada (altura = num_px) y (ancho = num_px).\n\nUsted debe construir un algoritmo simple de reconocimiento de imagenes que pueda clasificar correctamente las imagenes como gato o no-gato.\n\nPrimero, examinemos los datos. Cargue el archivo con el siguiente c\u00f3digo.","45b28bf1":"**Interpretaci\u00f3n**:\nSe puede ver el coste decreciendo, demostrando que los par\u00e1metros est\u00e1n siendo aprendidos. Sin embargo, el modelo se podr\u00eda entrenar aun m\u00e1s sobre el conjunto de entrenamiento. Intente aumentar el n\u00famero de iteraciones arriba y vuelva a ejecutar el c\u00f3digo. Podr\u00e1 ver precisi\u00f3n del conjunto de entrnamiento aumenta, pero la del conjunto de prueba decrece. Este es evidencia del sobre-ajuste.","dabcab81":"**Salida esperada para m_train, m_test y num_px**: \n<table style=\"width:15%\">\n  <tr>\n    <td>**m_train**<\/td>\n    <td> 209 <\/td> \n  <\/tr>\n  \n  <tr>\n    <td>**m_test**<\/td>\n    <td> 50 <\/td> \n  <\/tr>\n  \n  <tr>\n    <td>**num_px**<\/td>\n    <td> 64 <\/td> \n  <\/tr>\n  \n<\/table>\n","c31d3682":"Las imagenes a color son com\u00fanmente representadas mediante los tres canales rojo, verde y azul (RGB) para cada pixel, de tal manera que a cada pixel le corresponde un vector de tres n\u00fameros en el rango de 0 a 255.\n\nUn paso muy com\u00fan en el pre-procesamiento de datos en machine learning es el de estandarizar el conjunto de datos multivariado, es decir, restando la media de cada vector a cada ejemplo, y dividiendo por la desviaci\u00f3n estandar del vector. En este caso del tratamiento de imagenes, es m\u00e1s simple y conveniente tan solo dividir todas las filas del conjunto de datos por 255 (el valor m\u00e1ximo de un canal RGB).\n\n<!-- Durante el entrenamiento del modelo, se multiplican pesos y se suman sesgos a algunos inputs iniciales para observar activaciones neuronales. Luego se retro-propaga a partir de los gradientes para entrenar el modelo. Pero es importante que cada patr\u00f3n del input tenga un rango similar para que los gradientes no exploten. M\u00e1s adelante se profundizar\u00e1 en esto. !--> \n\nNormalizemos los datos.","0ad49e2f":"**Salida esperada**: \n\n\n<table style=\"width:25%\">\n    <tr>\n        <td>  ** w **  <\/td>\n        <td> [[ 0.]\n [ 0.]] <\/td>\n    <\/tr>\n    <tr>\n        <td>  ** b **  <\/td>\n        <td> 0 <\/td>\n    <\/tr>\n<\/table>\n\nPara inputs de imagen, w ser\u00e1 de tama\u00f1o (num_px $\\times$ num_px $\\times$ 3, 1).","34c2b04f":"Ahora puede intentar desarrollar su propio c\u00f3digo modificado y compara los resultados. Intente mejorar los resultados obtenidos. Puede jugar con la tasa de aprendizaje, el n\u00famero de iteraciones o distintos m\u00e9todos de inicializaci\u00f3n. Tambi\u00e9n puede probar otras t\u00e9cnicas de pre-procesamiento, como el de estandarizar los datos, etc. \n\n<font color='blue'>\n**Comentarios finales:**\n1. Recuerde la importancia del pre-procesamiento de los datos.\n2. Hemos implementado cada funci\u00f3n de manera separada: initialize(), propagate(), optimize(). Y luego se construye el modelo: model().\n3. La selecci\u00f3n adecuada de la tasa de aprendizaje, al cual nos referimos como un \"hiper-par\u00e1metro\", puede hacer una gran diferencia en el algoritmo. Seguiremos viendo m\u00e1s ejemplos de esto en actividades futuras.","149ce9cd":"Es recomendable ahora re-dimensionar las imagenes de tama\u00f1o (num_px, num_px, 3) en un arreglo numpy de dimensi\u00f3n (num_px $*$ num_px $*$ 3, 1). Luego, los conjuntos de entrenamiento y prueba ser\u00e1n un arreglo numpy donde cada columna representa una imagen (aplanada). Deber\u00edan haber m_train y m_test columnas.\n\n**Ejercicio:** Re-dimensione los conjuntos de datos de entrenamiento y prueba para que las imagenes de tama\u00f1o (num_px, num_px, 3) sean aplanadas en vectores de dimensi\u00f3n (num\\_px $*$ num\\_px $*$ 3, 1).\n\nAyuda. Cuando se quiere aplanar una matriz X de dimensi\u00f3n (a,b,c,d) en una matriz X_flatten de dimensi\u00f3n (b$*$c$*$d, a) se puede usar: \n```python\nX_flatten = X.reshape(X.shape[0], -1).T      # X.T es la transpuesta de X\n```","431e8e96":"### 4.3 - Propagaci\u00f3n hacia delante y hacia atr\u00e1s\n\nUna vez los par\u00e1metros est\u00e1n inicializados, se pueden implementar los pasos de propagaci\u00f3n hacia \"delante\" y hacia \"atr\u00e1s\" para el aprendizaje de los par\u00e1metros.\n\n**Ejercicio:** Implemente la funci\u00f3n `propagate()` que calcula la funci\u00f3n de coste y su gradiente.\n\n**Ayuda**:\n\nPropagaci\u00f3n hacia delante:\n- Se tiene X\n- Se calcula $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n- Se calcula la funci\u00f3n de coste\/p\u00e9rdida: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n\nSe pueden usar las siguientes f\u00f3rmulas: \n\n$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$","bc682579":"## 4 - Construyendo las partes del algoritmo ## \n\nLos pasos principales para construir una red neuronal son: \n1. Definir la estructura del modelo (tal como el n\u00famero de patrones en el input) \n2. Inicializar los par\u00e1metros del modelo\n3. Bucle:\n    - Calcular la p\u00e9rdida actual (propagaci\u00f3n hacia delante)\n    - Calcular el gradiente actual (retro-propagaci\u00f3n)\n    - Actualizar los par\u00e1metros (descenso en la direcci\u00f3n del gradiente)\n\nSe suele construir 1-3 de manera separada e integrarlos en una funci\u00f3n que llamaremos `model()`.\n\n### 4.1 - Funciones de ayuda\n\n**Ejercicio**: Utilizando su c\u00f3digo del Taller_1 \"IntroPython_numpy\", implemente `sigmoid()`. Como se puede ver en la figura arriba, se debe computar $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$ para predecir. Para ello puede utilizar np.exp().","4e1a0273":"# Regresi\u00f3n log\u00edstica con una perspectiva de Redes Neuronales\n\nEn este ejercicio construir\u00e1 un clasificador de regresi\u00f3n log\u00edstica. En este cuaderno encontrar\u00e1 la gu\u00eda para hacerlo desde una perspectiva de redes neuronales, ganando una intuici\u00f3n sobre lo que es el aprendizaje computacional y el deep learning.\n\n**Instrucciones:**\n- No utilize bucles-for\/while en su c\u00f3digo, a menos que se le pida hacerlo expl\u00edcitamente.\n\n**Tras este taller usted va a ser capaz de:**\n- Construir la arquitectura general de un algoritmo de aprendizaje, incluyendo:\n    - Inicializaci\u00f3n de par\u00e1metros\n    - Calcular la funci\u00f3n de coste y su gradiente\n    - Utilizar un algoritmo de optimizaci\u00f3n (descenso en la direcci\u00f3n del gradiente, GD) \n- Reunir todas las tres funciones en un modelo principal en el orden adecuado.\n\nManos a la obra!!","d1335220":"**Salida esperada**: \n\n<table style=\"width:55%\">\n  <tr>\n    <td>**Dimensi\u00f3n train_set_x_flatten**<\/td>\n    <td> (12288, 209)<\/td> \n  <\/tr>\n  <tr>\n    <td>**Dimensi\u00f3n train_set_y**<\/td>\n    <td>(1, 209)<\/td> \n  <\/tr>\n  <tr>\n    <td>**Dimensi\u00f3n test_set_x_flatten**<\/td>\n    <td>(12288, 50)<\/td> \n  <\/tr>\n  <tr>\n    <td>**Dimensi\u00f3n test_set_y**<\/td>\n    <td>(1, 50)<\/td> \n  <\/tr>\n  <tr>\n  <td>**Chequeo tras el re-dimensionamiento**<\/td>\n  <td>[17 31 56 22 33]<\/td> \n  <\/tr>\n<\/table>","047fa2b0":"<font color='blue'>\n**Recapitulemos:**\nSe han implementado varias funciones:\n- Inicializaci\u00f3n de (w,b)\n- Optimizaci\u00f3n iterativa de la p\u00e9rdida para aprender los parametros (w,b):\n    - computando el coste y su gradiente \n    - actualizando los parametros usando el GD\n- Se utilizan los par\u00e1metros aprendidos (w,b) para predecir las etiquetas para un conjunto dado de ejemplos","b7c726e4":"## 3 - Arquitectura general de un algoritmo de aprendizaje ##\n\nLleg\u00f3 el momento de dise\u00f1ar un algoritmo simple para distinguir imagenes de gatos y de aquello que no son gatos.\n\nDebe constuir un modelos de regresi\u00f3n log\u00edstica, desde una perspectiva de Redes Neuronales.\n\n**Formulaci\u00f3n del algoritmo**:\n\nPara un ejemplo $x^{(i)}$:\n$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n$$\\hat{y}^{(i)} = a^{(i)} = sigmoide(z^{(i)})\\tag{2}$$ \n$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n\nEl coste se calcula sumando sobre todos los ejemplos de entrenamiento:\n$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n\n**Paso a paso**:\nEn este ejercicio, se deben dar los siguientes pasos: \n    - Inicializar los par\u00e1metros del modelo\n    - Aprender los par\u00e1metros del modelo a partir de la minimizaci\u00f3n del coste  \n    - Utilizar los par\u00e1metros aprendidos para hacer predicciones (sobr el conjunto de prueba)\n    - Analizar los resultados y concluir","fb1f4a14":"**Salida esperada**: \n\n<table>\n  <tr>\n    <td>**sigmoid([0, 2])**<\/td>\n    <td> [ 0.5         0.88079708]<\/td> \n  <\/tr>\n<\/table>","819faac1":"## 6 - Profundizando en el an\u00e1lisis ##\n\nYa tienes un primer modelo de clasificaci\u00f3n de imagenes. Analiz\u00e9moslo un poco m\u00e1s, como por ejemplo examinando distintos valores para la tasa de aprendizaje $\\alpha$. ","82a042a1":"Muchos fallos\/bugs del c\u00f3digo en deep learning ocurren por tener dimensiones de la matriz\/vector que no encajan. Si puede mantener las dimensiones correctas podr\u00e1 evitar tener que dedicar tiempo a corregir estos fallos. \n\n**Ejercicio:** Encuentre los valores para:\n    - m_train (n\u00famero de ejemplos de entrenamiento)\n    - m_test (n\u00famero de ejemplos de prueba)\n    - num_px (= altura = ancho de la imagen)\nRecuerde que `train_set_x_orig` es un arreglo numpy de dimensiones (m_train, num_px, num_px, 3). De esta manera, puede acceder a `m_train` escribiendo `train_set_x_orig.shape[0]`."}}