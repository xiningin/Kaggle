{"cell_type":{"de0706e9":"code","0d06f1df":"code","15b91a02":"code","53c8374e":"code","22e10d27":"code","e6e8b433":"code","c4508180":"code","87f99615":"code","642451fe":"code","f33ab41f":"code","c86aa57d":"code","b6f377b4":"code","9425e776":"code","8ac61f99":"code","1538bcc5":"code","64aaf09e":"code","c798c415":"code","fef536d6":"code","b2be5b76":"code","fab11168":"code","1598da45":"code","9726aed2":"code","0cccf04b":"code","a81b9ad7":"code","2ab31d4d":"code","19d0d498":"code","c0c53400":"code","9c34d61a":"code","4c33b392":"code","342419ac":"code","c647b299":"code","d4c4f2b3":"code","eb4cfbc2":"code","126fdc5d":"code","8948afc7":"code","732165f2":"code","29bd600f":"code","3bc3b0b5":"code","5fbf9637":"code","a2ffea3f":"code","3a942abd":"code","b6eafe1b":"code","e4f98ea2":"code","f965a3a5":"code","2eab0c29":"code","0040a00e":"code","0cdee626":"code","0902bcb4":"code","0fcb3564":"code","d2fb5af2":"code","5856c35b":"code","428ad6db":"code","965b9195":"code","d7b248be":"code","aa1a0f42":"code","2228a9a1":"code","003a0c66":"code","69d8832d":"code","74eddac3":"code","def3c66a":"code","69de2496":"code","e09f0c1d":"code","d39fc33c":"code","9dda77c6":"code","af75b3b8":"code","50375143":"code","cf569660":"markdown","1bbc5feb":"markdown","9e772b84":"markdown"},"source":{"de0706e9":"# Mounting Google drive to access data\n#from google.colab import drive\n#drive.mount('\/content\/drive')","0d06f1df":"#reading  train data\nimport pandas as pd\nimport numpy as np\ndf=pd.read_csv(r\"..\/input\/iba-ml1-final-project\/train.csv\")","15b91a02":"# reading test data\ntest= pd.read_csv(r\"..\/input\/iba-ml1-final-project\/test.csv\")","53c8374e":"test.head()","22e10d27":"test.isnull().sum()","e6e8b433":"# creating new variable by joining two columns \"Review_Title\" and Review\ntest['Final_review']= test['Review_Title']+\" \"+test['Review']","c4508180":"# filling NA cases with word \"Gorgeous\"\ntest.fillna(\"Gorgeous\", inplace=True) ","87f99615":"df.head()","642451fe":"# creating new variable by joining two columns \"Review_Title\" and Review\ndf['Final_review']= df['Review_Title']+\" \"+df['Review']","f33ab41f":"#df.fillna(\"Unknown\", inplace=True) ","c86aa57d":"# dropping missing rows by Final_review column\nmain_col='Final_review'\ndf.dropna(subset=[main_col],inplace=True)","b6f377b4":"df.isnull().sum()","9425e776":"df.groupby(by=['Rating', 'Recommended']).agg({\"Age\":\"count\"})","8ac61f99":"df.Recommended.sum()","1538bcc5":"import re\nimport string\nfrom string import punctuation\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')","64aaf09e":"# removing stopwords and punctuation from reviews\ndef text_processing(text):\n    Stopwords = stopwords.words('english')\n    no_punctuation = [char for char in text if char not in string.punctuation]\n    no_punctuation = ''.join(no_punctuation)\n    return ' '.join([word for word in text.split() if word.lower() not in Stopwords])\n    \ndf['Final_review2'] = df['Final_review'].apply(text_processing)\ndf.head()","c798c415":"# removing stopwords and punctuation from reviews\ntest['Final_review2'] = test['Final_review'].apply(text_processing)","fef536d6":"from nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import wordnet","b2be5b76":"# Stemmer function\nfrom nltk.stem import PorterStemmer\nps =PorterStemmer()\ndef stemming(w):\n  stems= [ps.stem(i) for i in w.split(\" \")]\n  return \" \".join(stems)","fab11168":"# Creating new variable by appling stemmization to \"Final_review2\" column\ndf['Final_review3'] = df['Final_review2'].apply(stemming)","1598da45":"# Creating new variable by appling stemmization to \"Final_review2\" column\ntest['Final_review3'] = test['Final_review2'].apply(stemming)","9726aed2":"# Creating new variable by appling stemmization to \"Review_Title\" column\ndf['review3'] = df['Review_Title'].apply(stemming)\ntest['review3'] = test['Review_Title'].apply(stemming)","0cccf04b":"# Main column that is used as input to model\nmain_col2=\"Final_review3\"","a81b9ad7":"df.head()","2ab31d4d":"df.isnull().sum()","19d0d498":"# Splitting df data to train and test data (90%,10%)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df, df.Recommended,test_size = 0.1)","c0c53400":"X_train['Rating'].value_counts()","9c34d61a":"X_test['Rating'].value_counts()","4c33b392":"X_train.shape","342419ac":"X_train1 = list(X_train[main_col2])\nX_test1 = list(X_test[main_col2])\ny_train1 = np.array(y_train)\ny_test1 = np.array(y_test)\nval_test = list(test[main_col2])\n\ny_train2 = np.array(X_train['Rating'])\ny_test2 = np.array(X_test['Rating'])\n\nprint(y_train2.shape)\nprint(y_test2.shape)","c647b299":"all_txt=X_train1+X_test1+val_test","d4c4f2b3":"# Bag of Words model - Using Tfidf vectorizer\n\n#Vectorization\n#bow = CountVectorizer( ngram_range=(1, 2))\n#X_train1 = bow.fit_transform(X_train1)\n#X_test1 = bow.transform(X_test1)\n#Term Frequency, Inverse Document Frequency\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer( max_features=5000)\nvectorizer.fit(all_txt)\nX_train1 = vectorizer.transform(X_train1)\nX_test1 = vectorizer.transform(X_test1)\nX_train1=X_train1.toarray()\nX_test1=X_test1.toarray()\n\n#val_test = bow.transform(val_test)\nval_test = vectorizer.transform(val_test)\nval_test=val_test.toarray()","eb4cfbc2":"X_train1.shape","126fdc5d":"X_test1.shape","8948afc7":"# Coverting target variable (Recommended) to to_categorical form\nfrom tensorflow.keras.utils import to_categorical\ny_train11 = to_categorical(y_train1,2)\ny_test11 = to_categorical(y_test1,2)","732165f2":"# Coverting target variable (Rating) to to_categorical form\ny_train22 = to_categorical(y_train2,6)\ny_test22 = to_categorical(y_test2,6)","29bd600f":"# Trainig model with Bag of Words - Tfidf\n# Target variable is \"Recommended\"\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout\nmodel = Sequential()\nmodel.add(Dense(units=5000,activation='relu'))\nmodel.add(Dense(units=1000,activation='relu'))\nmodel.add(Dense(units=200,activation='relu'))\nmodel.add(Dense(units=2, activation='softmax'))\nopt=tf.keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\nmodel.fit(x=X_train1, y=y_train11, batch_size=10, epochs=5, validation_data=(X_test1, y_test11), verbose=1 ,callbacks=early_stop)","3bc3b0b5":"X_train1.shape","5fbf9637":"# calculate the spearman's correlation between two variables\nfrom numpy.random import rand\nfrom numpy.random import seed\nfrom scipy.stats import spearmanr\n# calculate spearman's correlation\npred2= np.argmax(model.predict(X_test1), axis=-1)\ncoef, p = spearmanr(y_test1, pred2)\nprint('Spearmans correlation coefficient: %.3f' % coef)\n","a2ffea3f":"# Trainig model with Bag of Words - Tfidf\n# Target variable is \"Rating\"\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout\nmodel = Sequential()\nmodel.add(Dense(units=1000,activation='relu'))\nmodel.add(Dense(units=500,activation='relu'))\nmodel.add(Dense(units=200,activation='relu'))\nmodel.add(Dense(units=6, activation='softmax'))\nopt=tf.keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\nmodel.fit(x=X_train1, y=y_train22, batch_size=5, epochs=15, validation_data=(X_test1, y_test22), verbose=1)","3a942abd":"# calculate the spearman's correlation between two variables\nfrom numpy.random import rand\nfrom numpy.random import seed\nfrom scipy.stats import spearmanr\n# calculate spearman's correlation\npred2= np.argmax(model.predict(X_test1), axis=-1)\ncoef, p = spearmanr(y_test2, pred2)\nprint('Spearmans correlation coefficient: %.3f' % coef)\n","b6eafe1b":"# Coverting train,test data to numpy array\nxtr = np.array(X_train[main_col])\nxte = np.array(X_test[main_col])\nytr = np.array(y_train)\nyte = np.array(y_test)\n\nytee = np.array(test[main_col])\n\nytr_r = np.array(X_train['Rating'])\nyte_r = np.array(X_test['Rating'])\n\nprint(xtr.shape)\nprint(xte.shape)\nprint(ytee.shape)\nprint(ytr_r.shape)\nprint(yte_r.shape)","e4f98ea2":"#Method 1 -  Defining keras word tokenizer \nimport tensorflow as tf\n\ntok = tf.keras.preprocessing.text.Tokenizer( \n    num_words=1000, filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,\n    split=' ', char_level=False, oov_token = 'unknown')","f965a3a5":"# Fitting tokenizer with train data and coverting train, validation and test review texts to sequences\ntok.fit_on_texts(xtr)\nxtr1 = tok.texts_to_sequences(xtr)\nxte1 = tok.texts_to_sequences(xte)\nxtee1 = tok.texts_to_sequences(ytee)","2eab0c29":"# Padding sequences with parameters  maxlen=100, padding, truncating as 'pre'\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nxtr2 = pad_sequences(xtr1, 1000, padding='pre', truncating='pre')\nxte2 = pad_sequences(xte1, 1000, padding='pre', truncating='pre')\nxtee2 = pad_sequences(xtee1, 1000, padding='pre', truncating='pre')","0040a00e":"from tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.losses import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.activations import *\nfrom tensorflow.keras.metrics import *","0cdee626":"# Coverting target variables (Recommended, Rating) to to_categorical form\nytr1 = to_categorical(ytr,2)\nyte1 = to_categorical(yte,2)\n\nytr1_r = to_categorical(ytr_r,6)\nyte1_r = to_categorical(yte_r,6)","0902bcb4":"xtr2.shape","0fcb3564":"# RNN model 1 - Bidirectional LSTM used\nembedding_vecor_length = 200\nmax_review_length = 1000\nNUM_WORDS = 1000\n\nmodel = Sequential()\nmodel.add(Embedding(NUM_WORDS, embedding_vecor_length, input_length=max_review_length))\n#model.add(Conv1D(filters=100, kernel_size=3, padding='same', activation='relu'))\n#model.add(MaxPooling1D())\nmodel.add(Bidirectional(LSTM(100, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(50)))\n#model.add(Dropout(0.5))\nmodel.add(Dense(200,activation='relu'))\nmodel.add(Dense(2, activation='softmax'))","d2fb5af2":"model.summary()","5856c35b":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","428ad6db":"model.fit(xtr2, ytr1, epochs=1, batch_size=100, validation_data=(xte2, yte1))","965b9195":"# Method 2  -Defining keras word tokenizer\nfrom keras.preprocessing.text import Tokenizer\nnum_words = 6000\ntokenizer = Tokenizer(num_words=num_words)\n# Fitting tokenizer with train data and coverting train review texts to sequences\ntokenizer.fit_on_texts(xtr)\nX_train_seq = tokenizer.texts_to_sequences(xtr)\nX_train_pad = pad_sequences(X_train_seq, maxlen=512)","d7b248be":"# RNN model 2 - Unidirectional LSTM used\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, GlobalMaxPool1D, Dropout\n\nmodel = Sequential()\n\nmodel.add(Embedding(input_dim=num_words, output_dim=64))\nmodel.add(LSTM(32, return_sequences=True))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()","aa1a0f42":"#  coverting validation review texts to sequences\nX_test_seq = tokenizer.texts_to_sequences(xte)\nX_test_pad = pad_sequences(X_test_seq, maxlen=512)","2228a9a1":"X_test_pad.shape","003a0c66":"# Compiling and training RNN model 2 for target variable \"Recommended\"\nmodel.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n\nbatch_size = 32\nepochs = 2\nvalidation_split = 0.01\nmodel.fit(x=X_train_pad, y=y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split,validation_data=(X_test_pad,y_test))","69d8832d":"# coverting validation review texts to sequences and padding\nX_test_seqe = tokenizer.texts_to_sequences(ytee)\nX_test_pade = pad_sequences(X_test_seqe, maxlen=512)\n\n#pred1= np.argmax(model.predict(X_test_pade), axis=-1)\npred = model.predict(x=X_test_pade)\ny_pred = (pred >= 0.5) * 1\ntest['Recommended']=y_pred","74eddac3":"test.head()","def3c66a":"# RNN model 2 for target variable \"Rating\"\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, GlobalMaxPool1D, Dropout\n\nmodel2 = Sequential()\n\nmodel2.add(Embedding(input_dim=num_words, output_dim=256))\nmodel2.add(LSTM(256, return_sequences=True))\nmodel2.add(LSTM(128, return_sequences=True))\nmodel2.add(LSTM(64, return_sequences=True))\nmodel2.add(GlobalMaxPool1D())\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(600,activation='relu'))\nmodel2.add(Dense(6, activation='softmax'))\nmodel2.summary()","69de2496":"# Compiling and training RNN model 2 for target variable \"Rating\"\nmodel2.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n\nbatch_size = 32\nepochs = 2\nvalidation_split = 0.01\nmodel2.fit(x=X_train_pad, y=ytr1_r, batch_size=batch_size, epochs=epochs, validation_split=validation_split,validation_data=(X_test_pad,yte1_r))","e09f0c1d":"# calculate the spearman's correlation between two variables\nfrom numpy.random import rand\nfrom numpy.random import seed\nfrom scipy.stats import spearmanr\n# calculate spearman's correlation\npred2= np.argmax(model2.predict(X_test_pad), axis=-1)\ncoef, p = spearmanr(yte_r, pred2)\nprint('Spearmans correlation coefficient: %.3f' % coef)\n","d39fc33c":"X_test_seqe2 = tokenizer.texts_to_sequences(ytee)\nX_test_pade2 = pad_sequences(X_test_seqe, maxlen=512)\n\npred1= np.argmax(model2.predict(X_test_pade), axis=-1)\n#pred2 = model2.predict(x=X_test_pade2)\ntest['Rating']=pred1","9dda77c6":"test.head()","af75b3b8":"test[['Id','Rating','Recommended']].to_csv(\"submission_100.csv\", index=False)","50375143":"test","cf569660":"**Main Model for Submit**","1bbc5feb":"Main Model for Submit\n\n","9e772b84":"# Train a model to classify the polarity of comments"}}