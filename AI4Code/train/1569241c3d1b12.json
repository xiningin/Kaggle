{"cell_type":{"38f7e980":"code","d44da851":"code","e3618c53":"code","9e03942f":"code","4dde637d":"code","9f439fed":"code","25347a73":"code","fa84f623":"code","5e25d678":"code","1bc53766":"code","2edd5431":"code","7426fdd0":"code","e2256e82":"code","49fe62cf":"code","d0a4eb64":"code","72273ba3":"code","aaa9217f":"code","9cf9eebd":"code","4de3148d":"code","99d56875":"code","5cba9688":"code","beac4c68":"code","182e759c":"code","b2f21e19":"code","4e41cd59":"code","5b0edef1":"code","0177206e":"code","884e204e":"code","2a63f2ae":"code","a5db2aae":"code","cdf34b3f":"code","3016811e":"code","62b16389":"code","50454289":"code","0848a29b":"code","b81506c4":"code","9b52e85b":"code","e6ac6834":"code","88c8be02":"code","2890c140":"code","21b71297":"code","c4c0c3d2":"code","234e3258":"code","3a027093":"code","62901bca":"code","3efa05fa":"code","ec2475a2":"code","5215be2f":"code","5da45593":"code","bdcebf40":"code","039318e5":"code","2330a6a6":"code","b4e728a6":"markdown","e2c0dadf":"markdown","02450ffa":"markdown","c440b5d1":"markdown","2782d75c":"markdown","6c489743":"markdown","bc49aac6":"markdown","b66f0af9":"markdown","21f70fe2":"markdown","b9e037af":"markdown","f134d62a":"markdown","f6f39d5e":"markdown","cd367c97":"markdown","7072c2ff":"markdown","235b3cc3":"markdown","ae97a2ba":"markdown","5abe50b5":"markdown","ec41fd46":"markdown","ff9b4c20":"markdown","1d8e2189":"markdown","61acc960":"markdown","467a488e":"markdown","fb6d76a4":"markdown","c26b9da3":"markdown","fc47d6aa":"markdown"},"source":{"38f7e980":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pylab as plt\nfrom IPython.display import HTML\nimport warnings\npd.set_option('max_columns', 100)\nwarnings.filterwarnings(\"ignore\")\nsns.set_style(\"whitegrid\")\nmy_pal = sns.color_palette(n_colors=10)","d44da851":"!ls -GFlash ..\/input\/data-science-bowl-2019\/","e3618c53":"# Read in the data CSV files\ntrain = pd.read_csv('..\/input\/data-science-bowl-2019\/train.csv')\ntrain_labels = pd.read_csv('..\/input\/data-science-bowl-2019\/train_labels.csv')\ntest = pd.read_csv('..\/input\/data-science-bowl-2019\/test.csv')\nspecs = pd.read_csv('..\/input\/data-science-bowl-2019\/specs.csv')\nss = pd.read_csv('..\/input\/data-science-bowl-2019\/sample_submission.csv')","9e03942f":"train_ = train.sample(1000000) #sample 1M observations","4dde637d":"train_labels.head()","9f439fed":"train_labels.groupby('accuracy_group')['game_session'].count() \\\n    .plot(kind='barh', figsize=(15, 5), title='Target (accuracy group)')\nplt.show()","25347a73":"sns.pairplot(train_labels, hue='accuracy_group')\nplt.show()","fa84f623":"train.head()","5e25d678":"train['event_id_as_int'] = train['event_id'].apply(lambda x: int(x, 16))\ntrain['game_session_as_int'] = train['game_session'].apply(lambda x: int(x, 16))","1bc53766":"#code by Isaac & Jack\ndef extract_time_features(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['date'] = df['timestamp'].dt.date\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['year'] = df['timestamp'].dt.year\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    df['weekofyear'] = df['timestamp'].dt.weekofyear\n    df['dayofyear'] = df['timestamp'].dt.dayofyear\n    df['quarter'] = df['timestamp'].dt.quarter\n    df['is_month_start'] = df['timestamp'].dt.is_month_start    \n    \n    return df","2edd5431":"print(f'Train data has shape: {train.shape}')\nprint(f'Test data has shape: {test.shape}')","7426fdd0":"train.groupby('date')['event_id'] \\\n    .agg('count') \\\n    .plot(figsize=(15, 3),\n         title='Numer of Event Observations by Date',\n         color=my_pal[2])\nplt.show()\ntrain.groupby('hour')['event_id'] \\\n    .agg('count') \\\n    .plot(figsize=(15, 3),\n         title='Numer of Event Observations by Hour',\n         color=my_pal[1])\nplt.show()\ntrain.groupby('weekday_name')['event_id'] \\\n    .agg('count').T[['Monday','Tuesday','Wednesday',\n                     'Thursday','Friday','Saturday',\n                     'Sunday']].T.plot(figsize=(15, 3),\n                                       title='Numer of Event Observations by Day of Week',\n                                       color=my_pal[3])\nplt.show()","e2256e82":"print(train['event_data'][4])\nprint(train['event_data'][5])","49fe62cf":"train['installation_id'].nunique()","d0a4eb64":"train.groupby('installation_id') \\\n    .count()['event_id'] \\\n    .plot(kind='hist',\n          bins=40,\n          color=my_pal[4],\n          figsize=(15, 5),\n         title='Count of Observations by installation_id')\nplt.show()","72273ba3":"train.groupby('installation_id') \\\n    .count()['event_id'] \\\n    .apply(np.log1p) \\\n    .plot(kind='hist',\n          bins=40,\n          color=my_pal[6],\n         figsize=(15, 5),\n         title='Log(Count) of Observations by installation_id')\nplt.show()","aaa9217f":"train.groupby('installation_id') \\\n    .count()['event_id'].sort_values(ascending=False).head(5)","9cf9eebd":"train.query('installation_id == \"f1c21eda\"') \\\n    .set_index('timestamp')['event_code'] \\\n    .plot(figsize=(15, 5),\n          title='installation_id #f1c21eda event Id - event code vs time',\n         style='.',\n         color=my_pal[8])\nplt.show()","4de3148d":"train.groupby('event_code') \\\n    .count()['event_id'] \\\n    .sort_values() \\\n    .plot(kind='bar',\n         figsize=(15, 5),\n         title='Count of different event codes.')\nplt.show()","99d56875":"train['game_time'].apply(np.log1p) \\\n    .plot(kind='hist',\n          figsize=(15, 5),\n          bins=100,\n          title='Log Transform of game_time',\n          color=my_pal[1])\nplt.show()","5cba9688":"train.groupby('title')['event_id'] \\\n    .count() \\\n    .sort_values() \\\n    .plot(kind='barh',\n          title='Count of Observation by Game\/Video title',\n         figsize=(15, 15))\nplt.show()","beac4c68":"# Chow Time Video\nHTML('<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/tvRtFqOqa-Y\" frameborder=\"0\" allow=\"accelerometer; \\\n        autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","182e759c":"# Scrub-a-Dub\nHTML('<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/kkNzO2QzWaQ\" frameborder=\"0\" allow=\"accelerometer; \\\n    autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","b2f21e19":"train.groupby('type')['event_id'] \\\n    .count() \\\n    .sort_values() \\\n    .plot(kind='bar',\n          figsize=(15, 4),\n          title='Count by Type',\n          color=my_pal[2])\nplt.show()","4e41cd59":"train.groupby('world')['event_id'] \\\n    .count() \\\n    .sort_values() \\\n    .plot(kind='bar',\n          figsize=(15, 4),\n          title='Count by World',\n          color=my_pal[3])\nplt.show()","5b0edef1":"train['log1p_game_time'] = train['game_time'].apply(np.log1p)","0177206e":"fig, ax = plt.subplots(figsize=(15, 5))\nsns.catplot(x=\"type\", y=\"log1p_game_time\",\n            data=train.sample(10000), alpha=0.5, ax=ax);\nax.set_title('Distribution of log1p(game_time) by Type')\nplt.close()\nplt.show()\nfig, ax = plt.subplots(figsize=(15, 5))\nsns.catplot(x=\"world\", y=\"log1p_game_time\",\n            data=train.sample(10000), alpha=0.5, ax=ax);\nax.set_title('Distribution of log1p(game_time) by World')\nplt.close()\nplt.show()","884e204e":"specs.head()","2a63f2ae":"specs.describe()","a5db2aae":"# First Attempt... still working to fully understand the problem\nfrom sklearn.model_selection import train_test_split\n\n# Define cleared or not cleared\n# \ntrain['cleared'] = True\ntrain.loc[train['event_data'].str.contains('false') & train['event_code'].isin([4100, 4110]), 'cleared'] = False\n\ntest['cleared'] = True\ntest.loc[test['event_data'].str.contains('false') & test['event_code'].isin([4100, 4110]), 'cleared'] = False\n\naggs = {'hour': ['max','min','mean'],\n        'cleared': ['mean']}\n\ntrain_aggs = train.groupby('installation_id').agg(aggs)\ntest_aggs = test.groupby('installation_id').agg(aggs)\ntrain_aggs = train_aggs.reset_index()\ntest_aggs = test_aggs.reset_index()\ntrain_aggs.columns = ['_'.join(col).strip() for col in train_aggs.columns.values]\ntest_aggs.columns = ['_'.join(col).strip() for col in test_aggs.columns.values]\ntrain_aggs = train_aggs.rename(columns={'installation_id_' : 'installation_id'})","cdf34b3f":"# Hmmm... not 1:1\ntrain_aggs.merge(train_labels[['installation_id','accuracy_group']],\n                 how='left')","3016811e":"import numpy as np\nimport pandas as pd\nimport datetime\nfrom catboost import CatBoostClassifier\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom scipy import stats","62b16389":"from sklearn.metrics import confusion_matrix\n# this function is the quadratic weighted kappa (the metric used for the competition submission)\ndef qwk(act,pred,n=4,hist_range=(0,3)):\n    \n    # Calculate the percent each class was tagged each label\n    O = confusion_matrix(act,pred)\n    # normalize to sum 1\n    O = np.divide(O,np.sum(O))\n    \n    # create a new matrix of zeroes that match the size of the confusion matrix\n    # this matriz looks as a weight matrix that give more weight to the corrects\n    W = np.zeros((n,n))\n    for i in range(n):\n        for j in range(n):\n            # makes a weird matrix that is bigger in the corners top-right and botton-left (= 1)\n            W[i][j] = ((i-j)**2)\/((n-1)**2)\n            \n    # make two histograms of the categories real X prediction\n    act_hist = np.histogram(act,bins=n,range=hist_range)[0]\n    prd_hist = np.histogram(pred,bins=n,range=hist_range)[0]\n    \n    # multiply the two histograms using outer product\n    E = np.outer(act_hist,prd_hist)\n    E = np.divide(E,np.sum(E)) # normalize to sum 1\n    \n    # apply the weights to the confusion matrix\n    num = np.sum(np.multiply(W,O))\n    # apply the weights to the histograms\n    den = np.sum(np.multiply(W,E))\n    \n    return 1-np.divide(num,den)\n    ","50454289":"train = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train.csv')\ntrain_labels = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train_labels.csv')\nspecs = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/specs.csv')\ntest = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/sample_submission.csv')","0848a29b":"\n# make a list with all the unique 'titles' from the train and test set\nlist_of_user_activities = list(set(train['title'].value_counts().index).union(set(test['title'].value_counts().index)))\n# make a list with all the unique 'event_code' from the train and test set\nlist_of_event_code = list(set(train['event_code'].value_counts().index).union(set(test['event_code'].value_counts().index)))\n# create a dictionary numerating the titles\nactivities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\nactivities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n\n# replace the text titles withing the number titles from the dict\ntrain['title'] = train['title'].map(activities_map)\ntest['title'] = test['title'].map(activities_map)\ntrain_labels['title'] = train_labels['title'].map(activities_map)","b81506c4":"# I didnt undestud why, but this one makes a dict where the value of each element is 4100 \nwin_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n# then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\nwin_code[activities_map['Bird Measurer (Assessment)']] = 4110","9b52e85b":"# convert text into datetime\ntrain['timestamp'] = pd.to_datetime(train['timestamp'])\ntest['timestamp'] = pd.to_datetime(test['timestamp'])","e6ac6834":"train.head()","88c8be02":"\ndef get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_activity = 0\n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    # news features: time spent in each activity\n    time_spent_each_act = {actv: 0 for actv in list_of_user_activities}\n    event_code_count = {eve: 0 for eve in list_of_event_code}\n    last_session_time_sec = 0\n    \n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy=0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0 \n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        \n        # get current session time in seconds\n        if session_type != 'Assessment':\n            time_spent = int(session['game_time'].iloc[-1] \/ 1000)\n            time_spent_each_act[activities_labels[session_title]] += time_spent\n        \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(time_spent_each_act.copy())\n            features.update(event_code_count.copy())\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0] \n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy\/counter if counter > 0 else 0\n            accuracy = true_attempts\/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group\/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n        # this piece counts how many actions was made in each event_code so far\n        n_of_event_codes = Counter(session['event_code'])\n        \n        for key in n_of_event_codes.keys():\n            event_code_count[key] += n_of_event_codes[key]\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type\n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n                return all_assessments[-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments","2890c140":"# here the get_data function is applyed to each installation_id and added to the compile_data list\ncompiled_data = []\n# tqdm is the library that draws the status bar below\nfor i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort=False)), total=17000):\n    # user_sample is a DataFrame that contains only one installation_id\n    compiled_data += get_data(user_sample)","21b71297":"# the compiled_data is converted to DataFrame and deleted to save memmory\nnew_train = pd.DataFrame(compiled_data)\ndel compiled_data\nnew_train.shape","c4c0c3d2":"pd.set_option('display.max_columns', None)\nnew_train[:10]","234e3258":"# this list comprehension create the list of features that will be used on the input dataset X\n# all but accuracy_group, that is the label y\nall_features = [x for x in new_train.columns if x not in ['accuracy_group']]\n# this cat_feature must be declared to pass later as parameter to fit the model\ncat_features = ['session_title']\n# here the dataset select the features and split the input ant the labels\nX, y = new_train[all_features], new_train['accuracy_group']\ndel train\nX.shape","3a027093":"\n# for configure others parameter consult the documentation below:\n# https:\/\/catboost.ai\/docs\/concepts\/python-reference_catboostclassifier.html\ndef make_classifier(iterations=6000):\n    clf = CatBoostClassifier(\n                               loss_function='MultiClass',\n                                eval_metric=\"WKappa\",\n                               task_type=\"CPU\",\n                               #learning_rate=0.01,\n                               iterations=iterations,\n                               od_type=\"Iter\",\n                                #depth=4,\n                                #changed stopping rounds to 800 to detect overfitting\n                               early_stopping_rounds=800,\n                                #l2_leaf_reg=10,\n                                #border_count=96,\n                               random_seed=37,\n                                #use_best_model=use_best_model\n                              )\n        \n    return clf","62901bca":"\nfrom sklearn.model_selection import KFold\n# oof is an zeroed array of the same size of the input dataset\noof = np.zeros(len(X))\nNFOLDS = 10\n# here the KFold class is used to split the dataset in 5 diferents training and validation sets\n# this technique is used to assure that the model isn't overfitting and can performs aswell in \n# unseen data. More the number of splits\/folds, less the test will be impacted by randomness\nfolds = KFold(n_splits=NFOLDS, shuffle=True, random_state=2019)\ntraining_start_time = time()\nmodels = []\nfor fold, (trn_idx, test_idx) in enumerate(folds.split(X, y)):\n    # each iteration of folds.split returns an array of indexes of the new training data and validation data\n    start_time = time()\n    print(f'Training on fold {fold+1}')\n    # creates the model\n    clf = make_classifier()\n    # fits the model using .loc at the full dataset to select the splits indexes and features used\n    clf.fit(X.loc[trn_idx, all_features], y.loc[trn_idx], eval_set=(X.loc[test_idx, all_features], y.loc[test_idx]),\n                          use_best_model=True, verbose=500, cat_features=cat_features)\n    \n    # then, the predictions of each split is inserted into the oof array\n    oof[test_idx] = clf.predict(X.loc[test_idx, all_features]).reshape(len(test_idx))\n    models.append(clf)\n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n    print('____________________________________________________________________________________________\\n')\n    #break\n    \nprint('-' * 30)\n# and here, the complete oof is tested against the real data using que metric (quadratic weighted kappa)\nprint('OOF QWK:', qwk(y, oof))\nprint('-' * 30)","3efa05fa":"# train model on all data once\n#clf = make_classifier()\n#clf.fit(X, y, verbose=500, cat_features=cat_features)\n\ndel X, y","ec2475a2":"# process test set, the same that was done with the train set\nnew_test = []\nfor ins_id, user_sample in tqdm(test.groupby('installation_id', sort=True), total=888):\n    a = get_data(user_sample, test_set=True)\n    new_test.append(a)\n    \nX_test = pd.DataFrame(new_test)\ndel test","5215be2f":"# make predictions on test set once\npredictions = []\nfor model in models:\n    predictions.append(model.predict(X_test))\npredictions = np.concatenate(predictions, axis=1)\nprint(predictions.shape)\npredictions = stats.mode(predictions, axis=1)[0].reshape(-1)\nprint(predictions.shape)\n#del X_test","5da45593":"submission['accuracy_group'] = np.round(predictions).astype('int')\nsubmission.to_csv('submission.csv', index=None)\nsubmission.head()","bdcebf40":"submission['accuracy_group'].plot(kind='hist')","039318e5":"train_labels['accuracy_group'].plot(kind='hist')","2330a6a6":"pd.Series(oof).plot(kind='hist')","b4e728a6":"Wow, 50000+ events for a single `installation_id`. Lets take a closer look at the id with the most observations. Not exactly sure what I'm looking at here. But it looks like this `installation_id` spans a long duration (over one month). Could this be installed by a bot? The use history does not look natural.","e2c0dadf":"# Baseline Model\n\nWe are told in the data description that:\n- The file train_labels.csv has been provided to show how these groups would be computed on the assessments in the training set.\n- Assessment attempts are captured in event_code 4100 for all assessments except for Bird Measurer, which uses event_code 4110. \n- If the attempt was correct, it contains \"correct\":true.\n\nWe also know:\n- The intent of the competition is to **use the gameplay data to forecast how many attempts a child will take to pass a given assessment** (an incorrect answer is counted as an attempt). \n- Each application install is represented by an installation_id. This will typically correspond to one child, but you should expect noise from issues such as shared devices.\n- **In the training set, you are provided the full history of gameplay data.**\n- In the test set, we have truncated the history after the start event of a single assessment, chosen randomly, for which you must predict the number of attempts.\n- Note that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment.","02450ffa":"**#Credit: [Rob Mulla](http:\/\/www.kaggle.com\/robikscube)\n**\n\n# 2019 Data Science Bowl\n## A Simple Introduction\n\ntl;dr\n\n*In this challenge, you\u2019ll use anonymous gameplay data, including knowledge of videos watched and games played, from the PBS KIDS Measure Up! app, a game-based learning tool developed as a part of the CPB-PBS Ready To Learn Initiative with funding from the U.S. Department of Education. Competitors will be challenged to predict scores on in-game assessments and create an algorithm that will lead to better-designed games and improved learning outcomes. *\n\n**\nNote that this is a synchronous rerun code competition and the private test set has approximately 8MM rows. You should be mindful of memory in your notebooks to avoid submission errors.** \ud83d\ude05","c440b5d1":"# event_data\nThis looks to have most of the interesting data about the event. It is in JSON format which isn't easy to wrangle in a tabular way. We need to be clever when parsing this data. They have already parsed some of this data for us like `event_count` and `event_code`.","2782d75c":"Lets looks at some of the installation_ids with the highest counts. We see some installation_ids have tens of thousands of observations!","6c489743":"## Game\/Video type\n- Media type of the game or video. Possible values are: 'Game', 'Assessment', 'Activity', 'Clip'.\n- Most are games, next are activities\n- Clips are the least common","bc49aac6":"## log(game_time) vs game\/video categories ","b66f0af9":"## timestamp\nLets see how many observations we have over time. Are they all in the same\/similar time zone?\n- Looks like number of observations rises over time. Steep pickup and dropoff at the start\/end\n- Much less use during the middle of the night hours. Use increases during the day with a slow reduction in use around midnight. We don't know how the timestamp relates to time zones for different users.\n- More users on Thursday and Friday. ","21f70fe2":"## game_time\n- Time in milliseconds since the start of the game session. Extracted from event_data.\n- The `log1p` transform shows a somewhat normal distribution with a peak at zero.","b9e037af":"# train.csv \/ test.csv\nThe data provided in these files are as follows:\n- `event_id` - Randomly generated unique identifier for the event type. Maps to event_id column in specs table.\n- `game_session` - Randomly generated unique identifier grouping events within a single game or video play session.\n- `timestamp` - Client-generated datetime\n- `event_data` - Semi-structured JSON formatted string containing the events parameters. Default fields are: event_count, event_code, and game_time; otherwise - fields are determined by the event type.\n- `installation_id` - Randomly generated unique identifier grouping game sessions within a single installed application instance.\n- `event_count` - Incremental counter of events within a game session (offset at 1). Extracted from event_data.\n- `event_code` - Identifier of the event 'class'. Unique per game, but may be duplicated across games. E.g. event code '2000' always identifies the 'Start Game' event for all games. Extracted from event_data.\n- `game_time` - Time in milliseconds since the start of the game session. Extracted from event_data.\n- `title` - Title of the game or video.\n- `type` - Media type of the game or video. Possible values are: 'Game', 'Assessment', 'Activity', 'Clip'.\n- `world` - The section of the application the game or video belongs to. Helpful to identify the educational curriculum goals of the media. Possible values are: 'NONE' (at the app's start screen), TREETOPCITY' (Length\/Height), 'MAGMAPEAK' (Capacity\/Displacement), 'CRYSTALCAVES' (Weight).","f134d62a":"Because the training data is so large, we will take a random sample of it for plotting. Since we are doing this at random it will speed up the time it takes to plot, and should still give us a a good view of the data's format.","f6f39d5e":"# specs.csv\nThe `specs.csv` gives us more information about what the event ids represent.\n- There are 386 unique event_ids\n- 168 unique info\n- 191 unique args\n- info and args columns","cd367c97":"## The target.\nFirst we will look at the target we intend to predict.\n\nWe are told: *The intent of the competition is to use the gameplay data to forecast how many attempts a child will take to pass a given assessment (an incorrect answer is counted as an attempt).*\nThe outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):\n- 3: the assessment was solved on the first attempt\n- 2: the assessment was solved on the second attempt\n- 1: the assessment was solved after 3 or more attempts\n- 0: the assessment was never solved\n","7072c2ff":"## Model - Jack & Isaac","235b3cc3":"## Game\/Video titles\n- Chow Time is very popular, along with Sandcastle Builder, Scrub-A-Dub, and Bottle Filler\n- After that there is a steep dropoff\n- Assessment's are in the 200000 count range.\n- Games with levels are less frequent\n- Some games or titles (maybe videos?) at the bottom are very infrequently seen.\n\nSome examples of the top games:\nChow Time:\nhttps:\/\/www.youtube.com\/watch?v=tvRtFqOqa-Y\n\n","ae97a2ba":"## Video Examples of the Gameplay\nIts helpful to see what the games actually look like. Here are a few youtube videos showing gameplay of the more popular titles.","5abe50b5":"**Unit Model**","ec41fd46":"## event_id & game_session\nThey say it's randomly generated, but is that true? Looks to be hex, lets convert it to an integer. Plotting shows nothign really interesting.","ff9b4c20":"## installation_id *important - predictions are grouped by these*\n- Randomly generated unique identifier grouping game sessions within a single installed application instance.\n- We will be predicting based off of these IDs\n- The training set has exactly 17000 unique `installation_ids`","1d8e2189":"First we will see what files we are given to work with. Note the `train.csv` file is quite large at 3.7G.\nFrom the data description we know:\n- `train.csv` & `test.csv` : These are the main data files which contain the gameplay events.\n- `specs.csv` : This file gives the specification of the various event types.\n- `train_labels.csv` : This file demonstrates how to compute the ground truth for the assessments in the training set.\n- `sample_submission.csv` : A sample submission in the correct format.","61acc960":"Thngs to note about the taget:\n- Accuracy of 100% goes to group 3\n- Accuracy of ~50% goes to group 2\n- Not finishing goes to group 0\n- Group 1 looks to have the most variation","467a488e":"## World\n- The section of the application the game or video belongs to. Helpful to identify the educational curriculum goals of the media.\n- Possible values are: 'NONE' (at the app's start screen), TREETOPCITY' (Length\/Height), 'MAGMAPEAK' (Capacity\/Displacement), 'CRYSTALCAVES' (Weight).","fb6d76a4":"## event_code\n- Identifier of the event 'class'. Unique per game, but may be duplicated across games. E.g. event code '2000' always identifies the 'Start Game' event for all games. Extracted from event_data.","c26b9da3":"Lets take a log transform of this count to we can more easily see what the distribution of counts by `insallation_id` looks like","fc47d6aa":"lets take a closer look at the event codes `4070` and `4030`\n- We notice that event 4070 and 4030 always comes with coordinates (x, y) and stage_width.\n- Possibly they could be marking acheivements or something related to position on the screen.\nThese events look like this:\n```\n{\"size\":0,\"coordinates\":{\"x\":782,\"y\":207,\"stage_width\":1015,\"stage_height\":762},\"event_count\":55,\"game_time\":34324,\"event_code\":4030}\n```"}}