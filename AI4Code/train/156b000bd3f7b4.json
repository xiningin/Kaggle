{"cell_type":{"d61e3205":"code","08950756":"code","116a4087":"code","e93bd5ad":"code","9496df89":"code","ed764f6b":"code","c5b2aa2a":"code","e7b1f1b5":"code","3b4e44e5":"code","6318e8e7":"code","b2e10a47":"code","a3937699":"code","ec420155":"code","42aacf4e":"code","517b52ce":"code","6d3f4c2e":"code","b29f1214":"code","49f41205":"code","be357112":"code","5705b9c5":"code","1c246a6b":"code","d4f8b83f":"code","63f70a02":"code","c60ea383":"code","50ef3496":"code","3c1d5ac2":"code","d4efb4e1":"code","35134762":"code","300be861":"code","6dd92e4b":"code","a9f2c62c":"code","194a6ba5":"code","da02a60f":"code","7937f319":"code","70fad3f3":"markdown","ff8e50cf":"markdown","b2358774":"markdown","288d8304":"markdown","7a92fae4":"markdown","fef02366":"markdown","a5a833e4":"markdown","7b982f1a":"markdown","4455437e":"markdown","cc16e874":"markdown","87127d38":"markdown","49d9994f":"markdown","6c0536e6":"markdown","9da5f682":"markdown","5538d365":"markdown","b68f94fe":"markdown","1192e96b":"markdown","3a651363":"markdown","e9e260b9":"markdown","42fa4b8b":"markdown","9a3b09a1":"markdown","138e7b36":"markdown","7b3d4382":"markdown","26d2ebd2":"markdown","f41b85bb":"markdown","3ed5f689":"markdown"},"source":{"d61e3205":"import os\nfolder_path = '..\/input\/ieee-fraud-detection\/'\nprint(os.listdir(folder_path))","08950756":"import numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\nfrom sklearn import linear_model\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nimport gc\nfrom numba import jit\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\nfrom itertools import product\nfrom IPython.display import HTML\nfrom joblib import Parallel, delayed\nimport matplotlib.pyplot as plt\nimport warnings\nfrom scipy import stats\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\n\npd.options.display.precision = 15\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\n        \ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","116a4087":"gc.collect()","e93bd5ad":"%%time \ntrain_transaction = pd.read_csv(f'{folder_path}train_transaction.csv')\ntest_transaction = pd.read_csv(f'{folder_path}test_transaction.csv')\n\ntrain_identity = pd.read_csv(f'{folder_path}train_identity.csv')\ntest_identity = pd.read_csv(f'{folder_path}test_identity.csv')\n\nsample_submission = pd.read_csv(f'{folder_path}sample_submission.csv', index_col='TransactionID')\n\ndf_train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True, on='TransactionID')\ndf_test = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True, on='TransactionID')\n\nprint(df_train.shape)\nprint(df_test.shape)\n\ndel train_transaction, train_identity, test_transaction, test_identity","9496df89":"%%time \ndf_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)","ed764f6b":"%%time \nemails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\nus_emails = ['gmail', 'net', 'edu']\n\n# https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100499#latest-579654\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    df_train[c + '_bin'] = df_train[c].map(emails)\n    df_test[c + '_bin'] = df_test[c].map(emails)\n    \n    df_train[c + '_suffix'] = df_train[c].map(lambda x: str(x).split('.')[-1])\n    df_test[c + '_suffix'] = df_test[c].map(lambda x: str(x).split('.')[-1])\n    \n    df_train[c + '_suffix'] = df_train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    df_test[c + '_suffix'] = df_test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","c5b2aa2a":"%%time \n\n# Label Encoding\nfor f in df_train.drop('isFraud', axis=1).columns:\n    if df_train[f].dtype=='object' or df_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df_train[f].values) + list(df_test[f].values))\n        df_train[f] = lbl.transform(list(df_train[f].values))\n        df_test[f] = lbl.transform(list(df_test[f].values))   ","e7b1f1b5":"%%time \ndf_train = df_train.fillna(-999)\ndf_test = df_test.fillna(-999)","3b4e44e5":"%%time \ny_train_full = df_train['isFraud'].copy()\nX_train_full = df_train.drop('isFraud', axis=1)\nX_test = df_test.copy()","6318e8e7":"%%time \nX_train, X_val, y_train, Y_val = train_test_split(X_train_full, y_train_full, \n                                                    test_size=0.15, \n                                                    random_state=42)","b2e10a47":"%%time \nclf = RandomForestClassifier(n_estimators=1000, max_depth=2, random_state=0, n_jobs=4)\nclf.fit(X_train, y_train)\n","a3937699":"%%time \nsample_submission['isFraud'] = clf.predict_proba(X_test)[:,1]\nsample_submission.to_csv('RandomForest.csv')","ec420155":"del X_train, X_val, y_train, Y_val\ngc.collect()","42aacf4e":"%%time \ndf_train['Trans_min_mean'] = df_train['TransactionAmt'] - df_train['TransactionAmt'].mean()\ndf_train['Trans_min_std'] = df_train['Trans_min_mean'] \/ df_train['TransactionAmt'].std()\ndf_test['Trans_min_mean'] = df_test['TransactionAmt'] - df_test['TransactionAmt'].mean()\ndf_test['Trans_min_std'] = df_test['Trans_min_mean'] \/ df_test['TransactionAmt'].std()\n\ndf_train['TransactionAmt_to_mean_card1'] = df_train['TransactionAmt'] \/ df_train.groupby(['card1'])['TransactionAmt'].transform('mean')\ndf_train['TransactionAmt_to_mean_card4'] = df_train['TransactionAmt'] \/ df_train.groupby(['card4'])['TransactionAmt'].transform('mean')\ndf_train['TransactionAmt_to_std_card1'] = df_train['TransactionAmt'] \/ df_train.groupby(['card1'])['TransactionAmt'].transform('std')\ndf_train['TransactionAmt_to_std_card4'] = df_train['TransactionAmt'] \/ df_train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ndf_test['TransactionAmt_to_mean_card1'] = df_test['TransactionAmt'] \/ df_test.groupby(['card1'])['TransactionAmt'].transform('mean')\ndf_test['TransactionAmt_to_mean_card4'] = df_test['TransactionAmt'] \/ df_test.groupby(['card4'])['TransactionAmt'].transform('mean')\ndf_test['TransactionAmt_to_std_card1'] = df_test['TransactionAmt'] \/ df_test.groupby(['card1'])['TransactionAmt'].transform('std')\ndf_test['TransactionAmt_to_std_card4'] = df_test['TransactionAmt'] \/ df_test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ndf_train['TransactionAmt'] = np.log(df_train['TransactionAmt'])\ndf_test['TransactionAmt'] = np.log(df_test['TransactionAmt'])","517b52ce":"%%time \ndf_test['isFraud'] = 'test'\ndf = pd.concat([df_train, df_test], axis=0, sort=False )\ndf = df.reset_index()\ndf = df.drop('index', axis=1)","6d3f4c2e":"def PCA_change(df, cols, n_components, prefix='PCA_', rand_seed=4):\n    pca = PCA(n_components=n_components, random_state=rand_seed)\n\n    principalComponents = pca.fit_transform(df[cols])\n\n    principalDf = pd.DataFrame(principalComponents)\n\n    df.drop(cols, axis=1, inplace=True)\n\n    principalDf.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n\n    df = pd.concat([df, principalDf], axis=1)\n    \n    return df","b29f1214":"mas_v = df_train.columns[55:394]","49f41205":"%%time \n\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.decomposition import PCA\n# from sklearn.cluster import KMeans\n\nfor col in mas_v:\n    df[col] = df[col].fillna((df[col].min() - 2))\n    df[col] = (minmax_scale(df[col], feature_range=(0,1)))\n\n    \ndf = PCA_change(df, mas_v, prefix='PCA_V_', n_components=30)\n","be357112":"%%time \n\ndf = reduce_mem_usage(df)","5705b9c5":"%%time \ndf_train, df_test = df[df['isFraud'] != 'test'], df[df['isFraud'] == 'test'].drop('isFraud', axis=1)","1c246a6b":"df_train.shape","d4f8b83f":"%%time \n\nX_train = df_train.sort_values('TransactionDT').drop(['isFraud', \n                                                      'TransactionDT', \n                                                      #'Card_ID'\n                                                     ],\n                                                     axis=1)\ny_train = df_train.sort_values('TransactionDT')['isFraud'].astype(bool)\n\nX_test = df_test.sort_values('TransactionDT').drop(['TransactionDT',\n                                                    #'Card_ID'\n                                                   ], \n                                                   axis=1)\ndel df_train\ndf_test = df_test[[\"TransactionDT\"]]","63f70a02":"from sklearn.model_selection import KFold,TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import plot_importance\nfrom sklearn.metrics import make_scorer\n\nimport time\ndef objective(params):\n    time1 = time.time()\n    params = {\n        'max_depth': int(params['max_depth']),\n        'gamma': \"{:.3f}\".format(params['gamma']),\n        'subsample': \"{:.2f}\".format(params['subsample']),\n        'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n        'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n        'num_leaves': '{:.3f}'.format(params['num_leaves']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n        'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n        'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n        'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n    }\n\n    print(\"\\n############## New Run ################\")\n    print(f\"params = {params}\")\n    FOLDS = 7\n    count=1\n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\n    tss = TimeSeriesSplit(n_splits=FOLDS)\n    y_preds = np.zeros(sample_submission.shape[0])\n    y_oof = np.zeros(X_train.shape[0])\n    score_mean = 0\n    for tr_idx, val_idx in tss.split(X_train, y_train):\n        clf = xgb.XGBClassifier(\n            n_estimators=600, random_state=4, verbose=True, \n            tree_method='gpu_hist', \n            **params\n        )\n\n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        \n        clf.fit(X_tr, y_tr)\n        #y_pred_train = clf.predict_proba(X_vl)[:,1]\n        #print(y_pred_train)\n        score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n        # plt.show()\n        score_mean += score\n        print(f'{count} CV - score: {round(score, 4)}')\n        count += 1\n    time2 = time.time() - time1\n    print(f\"Total Time Run: {round(time2 \/ 60,2)}\")\n    print(f'Mean ROC_AUC: {score_mean \/ FOLDS}')\n    del X_tr, X_vl, y_tr, y_vl, clf, score\n    gc.collect()\n    return -(score_mean \/ FOLDS)\n\n\nspace = {\n    # The maximum depth of a tree, same as GBM.\n    # Used to control over-fitting as higher depth will allow model \n    # to learn relations very specific to a particular sample.\n    # Should be tuned using CV.\n    # Typical values: 3-10\n    'max_depth': hp.quniform('max_depth', 7, 23, 1),\n    \n    # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n    # (meaning pulling weights to 0). It can be more useful when the objective\n    # is logistic regression since you might need help with feature selection.\n    'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n    \n    # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n    # approach can be more useful in tree-models where zeroing \n    # features might not make much sense.\n    'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n    \n    # eta: Analogous to learning rate in GBM\n    # Makes the model more robust by shrinking the weights on each step\n    # Typical final values to be used: 0.01-0.2\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n    \n    # colsample_bytree: Similar to max_features in GBM. Denotes the \n    # fraction of columns to be randomly samples for each tree.\n    # Typical values: 0.5-1\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, .9),\n    \n    # A node is split only when the resulting split gives a positive\n    # reduction in the loss function. Gamma specifies the \n    # minimum loss reduction required to make a split.\n    # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n    'gamma': hp.uniform('gamma', 0.01, .7),\n    \n    # more increases accuracy, but may lead to overfitting.\n    # num_leaves: the number of leaf nodes to use. Having a large number \n    # of leaves will improve accuracy, but will also lead to overfitting.\n    'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n    \n    # specifies the minimum samples per leaf node.\n    # the minimum number of samples (data) to group into a leaf. \n    # The parameter can greatly assist with overfitting: larger sample\n    # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n    'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n    \n    # subsample: represents a fraction of the rows (observations) to be \n    # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n    # in their paper A Scalable Tree Boosting System recommend \n    'subsample': hp.choice('subsample', [0.2, 0.4, 0.5, 0.6, 0.7, .8, .9]),\n    \n    # randomly select a fraction of the features.\n    # feature_fraction: controls the subsampling of features used\n    # for training (as opposed to subsampling the actual training data in \n    # the case of bagging). Smaller fractions reduce overfitting.\n    'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n    \n    # randomly bag or subsample training data.\n    'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)\n    \n    # bagging_fraction and bagging_freq: enables bagging (subsampling) \n    # of the training data. Both values need to be set for bagging to be used.\n    # The frequency controls how often (iteration) bagging is used. Smaller\n    # fractions and frequencies reduce overfitting.\n}\n","c60ea383":"gc.collect()","50ef3496":"%%time \n\n# Set algoritm parameters\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=5)\n\n# Print best parameters\nbest_params = space_eval(space, best)","3c1d5ac2":"%%time \n\nprint(\"BEST PARAMS: \", best_params)\n\nbest_params['max_depth'] = int(best_params['max_depth'])","d4efb4e1":"%%time \n\nclf = xgb.XGBClassifier(\n    n_estimators=300,\n    **best_params,\n    tree_method='gpu_hist'\n)","35134762":"%%time \n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(clf, X_train, y_train, cv=kfold)\nprint(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","300be861":"%%time \n\nclf.fit(X_train, y_train)\n\ny_preds = clf.predict_proba(X_test)[:,1] ","6dd92e4b":"%%time \n\nfeature_important = clf.get_booster().get_score(importance_type=\"weight\")\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\n\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n\n# Top 20 features\ndata.head(20)","a9f2c62c":"%%time \n\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\n\npyplot.rcParams[\"figure.figsize\"] = (14, 10)\nplot_importance(clf, max_num_features=15)\npyplot.show()","194a6ba5":"\nsample_submission['isFraud'] = y_preds\nsample_submission.to_csv('XGB_hypopt_model.csv')","da02a60f":"# predict with lightgbm, LeaveOneGroupOut \n# def pred_lgb_LOGO(X_train, Y_train, X_test, groups, params):\n#     feature_importances = pd.DataFrame()\n#     feature_importances['feature'] = X_train.columns\n\n#     oof_preds = np.zeros(X_train.shape[0])\n#     sub_preds = np.zeros(X_test.shape[0])\n\n#     nfolds = groups.nunique()\n#     print('groups:', nfolds)\n\n#     kf = LeaveOneGroupOut()\n#     pred_count = 0\n#     for fold, (tt_index, out_index) in enumerate(kf.split(X_train, Y_train, groups)):\n#         train_index = tt_index[tt_index < out_index[0]]\n#         test_index = tt_index[tt_index > out_index[len(out_index) - 1]]\n#         print(f'fold[{fold+1}] - {len(tt_index)} (train:{len(train_index)}, valid:{len(test_index)})')\n#         if len(train_index) == 0 or len(test_index) == 0: # or len(train_index) < len(test_index)\n#             print('- skip')\n#             continue\n        \n#         pred_count += 1\n        \n#         dtrain = lgb.Dataset(X_train.iloc[train_index], label=Y_train.iloc[train_index])\n#         dvalid = lgb.Dataset(X_train.iloc[test_index], label=Y_train.iloc[test_index])\n\n#         clf = lgb.train(params, dtrain, 5000, \n#                         valid_sets=[dtrain, dvalid], valid_names=['train', 'valid'],\n#                         verbose_eval=500, early_stopping_rounds=200)\n#         feature_importances['fold_{}'.format(pred_count)] = clf.feature_importance()\n#         oof_preds += clf.predict(X_train)\n#         sub_preds += clf.predict(X_test) \n\n#     oof_preds = oof_preds \/ pred_count\n#     sub_preds = sub_preds \/ pred_count \n        \n#     return oof_preds, sub_preds, feature_importances","7937f319":"# %%time\n\n# params={\n#         #'boosting':'dart', # dart (drop out trees)\n#         'learning_rate': 0.01,\n#         'objective': 'binary',\n#         'boost_from_average': False,\n#         'is_unbalance': False,\n#         'metric': 'auc',\n#         'num_threads': -1,\n#         'num_leaves': 128,\n#         'max_bin': 256,\n#         'verbose': 1,\n#         'random_state': 42,\n#         'bagging_fraction': 0.85,\n#         'bagging_freq': 1,\n#         'feature_fraction': 0.60\n#     }\n\n# oof_preds, sub_preds, feature_importances = pred_lgb_LOGO(X_train, Y_train, X_test, train_group, params)","70fad3f3":"## Baseline Model \n\n### RandomForest ","ff8e50cf":"For EDA please check [this](http:\/\/https:\/\/www.kaggle.com\/muke5hy\/eda-ieee-cis-fraud-detection) my another Kernel which only for the EDA. ","b2358774":"### Running the optimizer","288d8304":"## Mapping emails","7a92fae4":"### Encoding categorical features","fef02366":"### Delete BaseLine varaibles ","a5a833e4":"\n## Seting train and test back\n","7b982f1a":"### Predicting X test","4455437e":"### Getting PCA ","cc16e874":"### Top 20 Feature importance","87127d38":"## PCA ","49d9994f":"## LGBM","6c0536e6":"## Seting X and y","9da5f682":"# Modelling \nTo start simple, I will start using as base the kernels below: <br>\nhttps:\/\/www.kaggle.com\/artkulak\/ieee-fraud-simple-baseline-0-9383-lb - (@artkulak - Art) <br>\nhttps:\/\/www.kaggle.com\/artgor\/eda-and-models - (@artgor - Andrew Lukyanenko)\n\n","5538d365":"## XGBoost","b68f94fe":"### Concating dfs to get PCA of V features","1192e96b":"### Seting y_pred to csv","3a651363":"### KFold Cross validation","e9e260b9":"## Data loading and overview\n\nData is separated into two datasets: information about the identity of the customer and transaction information. Not all transactions belong to identities, which are available. Maybe it would be possible to use additional transactions to generate new features.","42fa4b8b":"#### Reduce the size for memory","9a3b09a1":"### Defining the HyperOpt function with parameters space and model","138e7b36":"## Imports and Functions used in this kernel\nThey are in the hidden cell below.","7b3d4382":"### Fit the model and predict ","26d2ebd2":"# Feature engineering","f41b85bb":"# Training\n","3ed5f689":"### Best parameters"}}