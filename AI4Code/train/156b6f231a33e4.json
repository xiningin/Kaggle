{"cell_type":{"de717fad":"code","1c8bd438":"code","054f5416":"code","5bd1ce8c":"code","9593faae":"code","c87153ce":"code","82bc0af2":"code","57dd293c":"code","d7c8a8d0":"code","33dcf73f":"code","ca8893b8":"code","2b321144":"code","cfde58ed":"code","c965525c":"code","b78103c0":"code","161f5a6d":"code","c5afd55b":"code","5e83612c":"code","5220ad9f":"code","24aa3936":"code","03fc4743":"code","f30662e3":"code","0b294736":"code","99e6068a":"code","c90fc5d7":"code","445b934c":"code","63f39d40":"code","c0d56e3e":"code","fc86e686":"code","f7a2ef14":"code","2cc6be29":"code","7528100f":"code","c85270e8":"code","f709ca27":"code","6a4a3918":"code","22b540ae":"code","26426c5e":"code","89a64cdf":"code","4e65f1d2":"code","a8042c7f":"code","7b79ec51":"code","b70e42b8":"code","d78b1d54":"code","8991d6d4":"code","10c1bd4a":"code","138c437a":"code","9400edc9":"code","63a760fe":"code","d2e5954f":"code","e678d8f7":"code","c1272add":"code","662acb2c":"code","f536b6e1":"code","bea449a4":"code","3f0b1771":"code","a655d5e1":"code","58b195e1":"code","4903d2e1":"code","e17b6c8f":"code","694df26b":"code","64b8b7ca":"code","9b0dba9e":"markdown","250feb15":"markdown"},"source":{"de717fad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom scipy.cluster.hierarchy import linkage,fcluster,dendrogram\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c8bd438":"df = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')","054f5416":"prev = pd.read_csv('..\/input\/home-credit-default-risk\/previous_application.csv')","5bd1ce8c":"des = pd.read_csv('..\/input\/home-credit-default-risk\/HomeCredit_columns_description.csv')","9593faae":"des[des['Row']=='OWN_CAR_AGE']['Description']","c87153ce":"df.isna().any().sum()","82bc0af2":"df.loc[:,((df.isna().sum()\/len(df)) > .25)].isna().sum().sort_values(ascending=False)","57dd293c":"df.loc[:,((df.isna().sum()\/len(df)) <= .4) & ((df.isna().sum()\/len(df)) > 0)].isna().sum().sort_values(ascending=False)","d7c8a8d0":"df.loc[:,((df.isna().sum()\/len(df)) <= .25) & ((df.isna().sum()\/len(df)) > 0)].isna().sum().sort_values(ascending=False)","33dcf73f":"import re\n\nflag1 = re.findall(r'(FLAG_DOCUMENT_\\d+)',str(df.columns.to_list()))\nflag2 = re.findall(r'(\\w+_CNT_SOCIAL_CIRCLE)',str(df.columns.to_list()))\nflag3 = re.findall(r\"'(\\w+_(?:MODE|MEDI|AVG))'\",str(df.columns.to_list()))\nyears = re.findall(r\"'(YEARS_BUILD.+?)'\",str(df.columns.to_list()))\nareas = re.findall(r\"'(COMMONAREA.+?)'\",str(df.columns.to_list()))","ca8893b8":"df[flag1].describe()","2b321144":"df[flag3].hist(figsize=(25,25))\nplt.show()","cfde58ed":"des[des['Row'].isin(years)]['Description'][47]","c965525c":"sns.scatterplot(years[0],areas[0],data=df)","b78103c0":"plt.figure(figsize=(40,30))\nsns.heatmap(df.isna().transpose(),\n            cmap=\"YlGnBu\",\n            cbar_kws={'label': 'Missing Data'})\n\nplt.show()","161f5a6d":"des[des['Row'] == 'EXT_SOURCE_2']['Description']","c5afd55b":"des[des['Row'] == 'FLAG_DOCUMENT_21']['Description']","5e83612c":"outlier = df[df['OBS_30_CNT_SOCIAL_CIRCLE']>=300].index","5220ad9f":"sns.scatterplot('OBS_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE',hue='TARGET',data=df.drop(outlier,axis=0))\nplt.show()","24aa3936":"sns.scatterplot('DEF_30_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE',hue='TARGET',data=df.drop(outlier,axis=0))\nplt.show()","03fc4743":"sns.scatterplot('OBS_30_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE',hue='TARGET',data=df.drop(outlier,axis=0))\nplt.show()","f30662e3":"plt.figure(figsize=(12,12))\nsns.heatmap(df[['OBS_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE']].corr())\n# sns.heatmap(df['Target'])\nplt.show()","0b294736":"df[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']].hist()","99e6068a":"df['OWN_CAR_AGE'].plot(kind='box')","c90fc5d7":"len(df[(df['OWN_CAR_AGE'] > df['OWN_CAR_AGE'].quantile(.25)) & (df['OWN_CAR_AGE'] < df['OWN_CAR_AGE'].quantile(.75))]['OWN_CAR_AGE'].notnull())\/len(df['OWN_CAR_AGE'].notnull())","445b934c":"df.FLAG_OWN_CAR.value_counts()","63f39d40":"df.OWN_CAR_AGE.isna().sum()","c0d56e3e":"book = linkage(df.select_dtypes([int,float]).isna().transpose())\nplt.figure(figsize=(26,6))\ndendrogram(book)\n\nplt.show()","fc86e686":"des[des['Row'] == 'AMT_REQ_CREDIT_BUREAU_QRT']['Description'][120]","f7a2ef14":"df[['AMT_REQ_CREDIT_BUREAU_YEAR','AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT']].hist()","2cc6be29":"todrop = re.findall(r\"\\s'(\\w+?_(?:AVG|MODE))'\",str(df.columns.to_list())) + ['AMT_REQ_CREDIT_BUREAU_YEAR','AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_HOUR','OBS_30_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','OWN_CAR_AGE']","7528100f":"df_raw = df.copy()","c85270e8":"df = df_raw.drop(todrop,axis=1).copy()","f709ca27":"obj_col = df.select_dtypes('object').columns\nnum_col = ['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT']","6a4a3918":"df[obj_col] = df[obj_col].fillna('None')\ndf[num_col] = df[num_col].fillna(-1)","22b540ae":"!git clone https:\/\/github.com\/DmitryUlyanov\/Multicore-TSNE.git\n!pip install .\/Multicore-TSNE\/.","26426c5e":"from MulticoreTSNE import MulticoreTSNE as TSNE\n\ntsne = TSNE(n_jobs=-1)\n\nfeatures_tsne = tsne.fit_transform(df.select_dtypes('int','float'))","89a64cdf":"feature = pd.DataFrame(features_tsne,columns=['x','y'])","4e65f1d2":"df_tsne = pd.concat([df.select_dtypes('object').copy(),feature],axis=1)","a8042c7f":"for cat in df_tsne.select_dtypes('object').columns:\n    plt.figure(figsize=(12,12))\n    plt.legend(loc='upper left')\n    sns.scatterplot(x='x',y='y',hue=cat,data=df_tsne)\n    plt.show()","7b79ec51":"# df = df.join(prev,on='SK_ID_CURR',rsuffix='_PREV')","b70e42b8":"df_clean = pd.concat([df,pd.get_dummies(df.select_dtypes('object'),drop_first=True)],axis=1).select_dtypes(['int','float']).drop('SK_ID_CURR',axis=1)","d78b1d54":"from sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.impute import SimpleImputer\n# from sklearn.decomposition import PCA\n# from sklearn.pipeline import Pipeline","8991d6d4":"from sklearn.model_selection import train_test_split\n\nX = df_clean.copy()\ny = X.pop('TARGET')\n\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.3,stratify=y,random_state=42)","10c1bd4a":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\nimpute = SimpleImputer(strategy='median')\nscale =  StandardScaler()\n\nX_train = impute.fit_transform(X_train)\nX_test = impute.transform(X_test)\n\nX_train = scale.fit_transform(X_train)\nX_test = scale.transform(X_test)","138c437a":"# baseline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import cross_validate\n\ndef compare_model(classifier,name='',graph=False):\n    \n    scoring = {\n           'Precision': 'precision_macro',\n           'Recall': 'recall_macro',\n           'F1':'f1_macro',\n           'ROC_AUC'  : 'roc_auc'\n    }\n    score_dict = cross_validate(classifier, X_train, y_train,\n                              cv=10,\n                              scoring=scoring)\n    \n    scores = pd.DataFrame(columns=['Model','Precision','Recall','F1','ROC AUC'])\n    scores = scores.append(pd.Series({ \n                                      'Model' : name,\n                                      # 'Accuracy' : score_dict['test_Accuracy'].mean(),\n                                      'Precision' : score_dict['test_Precision'].mean(),\n                                      'Recall' : score_dict['test_Recall'].mean(),\n                                      'F1' : score_dict['test_F1'].mean(),\n                                      'ROC AUC' : score_dict['test_ROC_AUC'].mean()\n    }),ignore_index=True)\n \n    return scores","9400edc9":"from sklearn.metrics import classification_report, accuracy_score, make_scorer\n\n\nmodels =  { \"Decision Tree\" : DecisionTreeClassifier(),\n            \"Random Forest\" : RandomForestClassifier(),\n            \"XGBoost\" : XGBClassifier(), \n            \"LogisticRegression\" : LogisticRegression(max_iter=1200), \n            \"CatBoost\" :CatBoostClassifier(verbose=False) ,\n            'LGBM' : LGBMClassifier(),\n            }\n\nscores = pd.DataFrame(columns=['Model','Precision','Recall','F1','ROC AUC'])\n# probability = {}\n\nfor model in models:\n  scores = scores.append(compare_model(models[model],name=model))","63a760fe":"scores.sort_values('ROC AUC',ascending=False).reset_index(drop=True)","d2e5954f":"# space = {'max_depth': hp.quniform('max_depth', 2, 10, 2),'learning_rate': hp.uniform('learning_rate', .001,.9)}\n\n# # Set up objective function\n# def objective(params):\n#     params = {'max_depth': int(params['max_depth']),'learning_rate': params['learning_rate']}\n#     gbm_clf = GradientBoostingClassifier(n_estimators=100, **params) \n#     best_score = cross_val_score(gbm_clf, X_train, y_train, scoring='accuracy', cv=2, n_jobs=4).mean()\n#     loss = 1 - best_score\n#     return loss\n\n# # Run the algorithm\n# best = fmin(fn=objective,space=space, max_evals=20, rstate=np.random.RandomState(42), algo=tpe.suggest)\n# print(best)","e678d8f7":"# from tpot import TPOTClassifier\n\n# # Assign the values outlined to the inputs\n# number_generations = 5\n# population_size = 5\n# offspring_size = 5\n# scoring_function = 'roc_auc'\n\n# # Create the tpot classifier\n# tpot_clf = TPOTClassifier(generations=number_generations, population_size=population_size,\n#                           offspring_size=offspring_size, scoring=scoring_function,\n#                           verbosity=2, random_state=42, cv=2)\n\n# # Fit the classifier to the training data\n# tpot_clf.fit(X_train, y_train)\n\n# # Score on the test set\n# print(tpot_clf.score(X_test, y_test))","c1272add":"!pip install optuna","662acb2c":"puter = SimpleImputer()\nscaler1 = StandardScaler()\nX_change = puter.fit_transform(X)\nX_change = scaler1.fit_transform(X)\n\nX = pd.DataFrame(X_change, columns=X.columns)","f536b6e1":"from optuna.integration import LightGBMPruningCallback\nimport optuna  # pip install optuna\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\n\ndef objective(trial, X, y):\n    param_grid = {\n        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n        \"bagging_fraction\": trial.suggest_float(\n            \"bagging_fraction\", 0.2, 0.95, step=0.1\n        ),\n        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n        \"feature_fraction\": trial.suggest_float(\n            \"feature_fraction\", 0.2, 0.95, step=0.1\n        ),\n    }\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1121218)\n\n    cv_scores = np.empty(5)\n    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        model = LGBMClassifier(objective=\"binary\", **param_grid)\n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_test, y_test)],\n            eval_metric=\"binary_logloss\",\n            early_stopping_rounds=100,\n            callbacks=[\n                LightGBMPruningCallback(trial, \"binary_logloss\")\n            ],  # Add a pruning callback\n        )\n        preds = model.predict_proba(X_test)\n        cv_scores[idx] = log_loss(y_test, preds)\n\n    return np.mean(cv_scores)","bea449a4":"# study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Classifier\")\n# func = lambda trial: objective(trial, X, y)\n# study.optimize(func, n_trials=20)","3f0b1771":"# print(study.best_params)","a655d5e1":"model = LGBMClassifier(n_estimators= 10000,\n                       learning_rate= 0.13072382959785664, \n                       num_leaves= 2260, max_depth= 6,\n                       min_data_in_leaf= 3500, \n                       lambda_l1= 20, \n                       lambda_l2= 0,\n                       min_gain_to_split= 0.485502034262153, \n                       bagging_fraction= 0.7, \n                       bagging_freq= 1, \n                       feature_fraction= 0.5)","58b195e1":"model.fit(X_train, y_train)","4903d2e1":"scoring = {\n       'Precision': 'precision_macro',\n       'Recall': 'recall_macro',\n       'F1':'f1_macro',\n       'ROC_AUC'  : 'roc_auc'\n}\nscore_dict = cross_validate(model, X_train, y_train,\n                          cv=10,\n                          scoring=scoring)\n\nscores = pd.DataFrame(columns=['Name','Precision','Recall','F1','ROC AUC'])\nscores = scores.append(pd.Series({ \n                                    'Name':'LGBMTuned',\n                                  'Precision' : score_dict['test_Precision'].mean(),\n                                  'Recall' : score_dict['test_Recall'].mean(),\n                                  'F1' : score_dict['test_F1'].mean(),\n                                  'ROC AUC' : score_dict['test_ROC_AUC'].mean()\n}),ignore_index=True)","e17b6c8f":"print(scores)","694df26b":"from sklearn.metrics import f1_score,classification_report\n\nprint(classification_report(y_test,model.predict(X_test)))","64b8b7ca":"from sklearn import metrics\nmetrics.plot_roc_curve(model, X_test, y_test)","9b0dba9e":"15% data are inside the box","250feb15":"-fill median\n\n-drop car age\n\n-fill na category\n\n-amt req qrt dan month\n\n-fillna EXT SOURCE "}}