{"cell_type":{"fb4832bf":"code","732b1a3f":"code","dbcbd2c6":"code","c5b7c9b4":"code","bde46303":"code","d953e369":"code","997dc4be":"code","571da32e":"code","dab0124e":"code","ff7a509b":"code","ca9e7861":"code","505cc51c":"code","47a20b12":"code","cdb4ae55":"code","ee9d7a54":"code","63d375f1":"code","b846e61a":"code","063bb161":"code","b94b0ce7":"code","200a053c":"code","a3145bff":"code","8d3da55b":"code","7520fa13":"code","8e5c76aa":"code","bf9fb7a9":"code","0479aceb":"code","3f0a5d69":"code","f8fcbf7d":"code","07d23e09":"code","a219d03c":"code","9f7330b6":"code","103b9feb":"code","2a3cac12":"code","d7b7ee52":"code","ed49094b":"code","7b5c5b86":"code","d96e1374":"code","53c2f4bd":"markdown","19b665fb":"markdown","18bd0eae":"markdown","eb69fde1":"markdown","1388ac81":"markdown","5afd48b3":"markdown","bf2a9f5b":"markdown","2befa929":"markdown","a6cb08ac":"markdown","74701083":"markdown","2a475873":"markdown","4d6b73c1":"markdown","8c0a9672":"markdown","8b8c553e":"markdown","61815d85":"markdown"},"source":{"fb4832bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","732b1a3f":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n%matplotlib inline\n\n#to ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","dbcbd2c6":"#reading the data file and put it into credit\ncredit = pd.read_csv(\"..\/input\/credit-card-default-predictions\/credit-card-default.csv\")\ncredit.head()","c5b7c9b4":"#let's take a look at type of columns\ncredit.info()","bde46303":"#importing test_train split from sklearn library\nfrom sklearn.model_selection import train_test_split","d953e369":"# Putting feature variable to X\nX = credit.drop('defaulted',axis=1)\n\n# Putting response variable to y\ny = credit['defaulted']\n\n# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)","997dc4be":"#importing random forest classifier from sklearn library\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Running random forest with default parameters\nrfc = RandomForestClassifier()","571da32e":"#fit\nrfc.fit(X_train,y_train)","dab0124e":"#Making prediction\npredictions = rfc.predict(X_test)","ff7a509b":"#importing classification report and confusion matrix from sklearn metrics\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score","ca9e7861":"#check the report of our default model\nprint(classification_report(y_test,predictions))","505cc51c":"#check confusion matrix\nprint(confusion_matrix(y_test,predictions))","47a20b12":"#check accuracy score\nprint(accuracy_score(y_test,predictions))","cdb4ae55":"# GridSearchCV to find optimal n_estimators\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(2, 20, 5)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                  return_train_score=True,\n                   scoring=\"accuracy\")\nrf.fit(X_train, y_train)","ee9d7a54":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","63d375f1":"# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","b846e61a":"# GridSearchCV to find optimal n_estimators\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'n_estimators': range(100, 1500, 400)}\n\n# instantiate the model (note we are specifying a max_depth)\nrf = RandomForestClassifier(max_depth=4)\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                  return_train_score=True,\n                   scoring=\"accuracy\")\nrf.fit(X_train, y_train)","063bb161":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","b94b0ce7":"# plotting accuracies with n_estimators\nplt.figure()\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","200a053c":"# GridSearchCV to find optimal max_features\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_features': [4, 8, 14, 20, 24]}\n\n# instantiate the model\nrf = RandomForestClassifier(max_depth=4)\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                  return_train_score=True,\n                   scoring=\"accuracy\")\nrf.fit(X_train, y_train)","a3145bff":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","8d3da55b":"# plotting accuracies with max_features\nplt.figure()\nplt.plot(scores[\"param_max_features\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_features\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_features\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","7520fa13":"# GridSearchCV to find optimal min_samples_leaf\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_leaf': range(100, 400, 50)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                  return_train_score=True,\n                   scoring=\"accuracy\")\nrf.fit(X_train, y_train)","8e5c76aa":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","bf9fb7a9":"# plotting accuracies with min_samples_leaf\nplt.figure()\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","0479aceb":"# GridSearchCV to find optimal min_samples_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_split': range(200, 500, 50)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                  return_train_score=True,\n                   scoring=\"accuracy\")\nrf.fit(X_train, y_train)","3f0a5d69":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","f8fcbf7d":"# plotting accuracies with min_samples_split\nplt.figure()\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_split\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","07d23e09":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [4,8,10],\n    'min_samples_leaf': range(100, 400, 200),\n    'min_samples_split': range(200, 500, 200),\n    'n_estimators': [100,200, 300], \n    'max_features': [5, 10]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1,verbose = 1)","a219d03c":"# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)","9f7330b6":"# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","103b9feb":"# model with the best hyperparameters\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=10,\n                             min_samples_leaf=100, \n                             min_samples_split=200,\n                             max_features=10,\n                             n_estimators=100)","2a3cac12":"# fit\nrfc.fit(X_train,y_train)","d7b7ee52":"# predict\npredictions = rfc.predict(X_test)","ed49094b":"# evaluation metrics\nfrom sklearn.metrics import classification_report,confusion_matrix","7b5c5b86":"print(classification_report(y_test,predictions))","d96e1374":"print(confusion_matrix(y_test,predictions))","53c2f4bd":"Apparently, the training and test scores both seem to increase as we increase max_features, and the model doesn't seem to overfit more with increasing max_features. Think about why that might be the case.\n","19b665fb":"Fitting the final model with the best parameters obtained from grid search.","18bd0eae":"**Tuning min_samples_split\n**\n\nLet's now look at the performance of the ensemble as we vary min_samples_split.\n","eb69fde1":"**Tuning n_estimators**\n\nLet's try to find the optimum values for n_estimators and understand how the value of n_estimators impacts the overall accuracy. Notice that we'll specify an appropriately low value of max_depth, so that the trees do not overfit. ","1388ac81":"**Tuning min_samples_leaf\n**\n\nThe hyperparameter min_samples_leaf is the minimum number of samples required to be at a leaf node:\n\n   If int, then consider min_samples_leaf as the minimum number.\n   If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n\nLet's now check the optimum value for min samples leaf in our case.\n","5afd48b3":"You can see that as we increase the value of max_depth, both train and test scores increase till a point, but after that test score starts to decrease. The ensemble tries to overfit as we increase the max_depth.\n\nThus, controlling the depth of the constituent trees will help reduce overfitting in the forest.\n","bf2a9f5b":"Default Hyperparameters\n\nLet's first fit a random forest model with default hyperparameters.","2befa929":"**Tuning max_features\n***\n\n\nLet's see how the model performance varies with max_features, which is the maximum numbre of features considered for splitting at a node.\n","a6cb08ac":"**Tuning max_depth****\nLet's try to find the optimum values for max_depth and understand how the value of max_depth impacts the overall accuracy of the ensemble.\n","74701083":" let's now look at the list of hyperparameters which we can tune to improve model performance","2a475873":"In this case, we know that there are no major data quality issues, so we'll go ahead and build the model.\n","4d6b73c1":"**Grid Search to Find Optimal Hyperparameters\u00b6\n**\n\nWe can now find the optimal hyperparameters using GridSearchCV.\n","8c0a9672":"Data Preparation and Model Building","8b8c553e":"**Hyperparameter Tuning**","61815d85":"You can see that the model starts of overfit as you decrease the value of min_samples_leaf. "}}