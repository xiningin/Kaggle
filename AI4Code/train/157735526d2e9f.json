{"cell_type":{"f06cfdba":"code","aed7d2d0":"code","5a516b22":"code","bc7da6bf":"code","0dbcf76c":"code","79d219aa":"code","5df5d025":"code","65243ac6":"code","c3062db5":"code","481d9d4d":"code","1117bcbb":"code","c15856e0":"code","7f74f871":"code","d491bcbd":"code","fc0adb6a":"code","541ee5f9":"code","03e81e70":"code","a7c78e5c":"code","454adc1b":"code","f94c3471":"code","7e931715":"code","bd0f66d4":"code","4d50ebcd":"code","e66d0e0e":"code","142737bf":"code","c1b77817":"code","e7001123":"code","30e694a9":"code","9c64601c":"code","3c526b61":"markdown","4022f3ef":"markdown","68a51195":"markdown","31ff27c5":"markdown","b9d082a9":"markdown","68787573":"markdown","9edc8fcd":"markdown","db7ab1f4":"markdown","b38a420c":"markdown","b036e939":"markdown"},"source":{"f06cfdba":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom scipy.stats import chisquare,chi2_contingency\n\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import metrics\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import linear_model\n\nfrom statsmodels.formula.api import ols   \nfrom sklearn.linear_model import SGDClassifier\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.metrics import roc_curve,roc_auc_score\n\nfrom sklearn import svm","aed7d2d0":"data=pd.read_csv('..\/input\/personal-loan\/Bank_Personal_Loan_Modelling-1.xlsx')\ndata.head()","5a516b22":"data.info()","bc7da6bf":"data.describe()","0dbcf76c":"#Analysing the columns we see that the experinece column the data is entered wrongly like -3,-2,-1 \n\ndata['Experience'].replace({-3:3,-2:2,-1:1},inplace=True)\n\n#Replacing 0 experience\n#From the data we see that the people with 0 exp is between age 24 and 30\n\nexp_mean=data['Experience'].loc[(data['Age']>=24) & (data['Age']<=30) ].mean()\nexp_std=data['Experience'].loc[(data['Age']>=24) & (data['Age']<=30) ].std()\nexp_zero_count=data['Experience'].loc[data['Experience']==0].value_counts()\n\n\nblank_exp=np.random.randint(exp_mean-exp_std,exp_mean+exp_std,size=exp_zero_count)\n\ndata['Experience'].loc[data['Experience']==0]=blank_exp\n\n#Thus Experience column is preprocessed","79d219aa":"#Checking the normality of the columns\n\nplt.figure(figsize=(20,15))\nplt.subplot(3,3,1)\nsns.distplot(data['Age'])\nplt.subplot(3,3,2)\nsns.distplot(data['Experience'])\nplt.subplot(3,3,3)\nsns.distplot(data['Income'])\nplt.subplot(3,3,4)\nsns.distplot(data['CCAvg'])\nplt.subplot(3,3,5)\nsns.distplot(data['Mortgage'].loc[data['Mortgage']!=0])","5df5d025":"#Checking the outliers of the columns using boxplot\n\nplt.figure(figsize=(20,15))\nplt.subplot(3,3,1)\nsns.boxplot(x='Personal Loan',y='Age',data=data)\nplt.subplot(3,3,2)\nsns.boxplot(x='Personal Loan',y='Experience',data=data)\nplt.subplot(3,3,3)\nsns.boxplot(x='Personal Loan',y='Income',data=data)\nplt.subplot(3,3,4)\nsns.boxplot(x='Personal Loan',y='CCAvg',data=data)\nplt.subplot(3,3,5)\nsns.boxplot(x='Personal Loan',y=data['Mortgage'].loc[data['Mortgage']!=0],data=data)","65243ac6":"sns.pairplot(data)","c3062db5":"linear_corr=data.corr()\nfig, ax = plt.subplots(figsize=(15,10)) \nsns.heatmap(linear_corr,annot=True,ax=ax)\n\n#From figure,we see that the Experience and Age are linearily dependent, So we can drop one of them","481d9d4d":"fig, ax = plt.subplots(figsize=(20,10)) \nsns.countplot(x='ZIP Code',hue='Personal Loan',data=data,ax=ax)","1117bcbb":"pd.crosstab(data['ZIP Code'],data['Personal Loan'])","c15856e0":"sns.jointplot(x='ID',y='Personal Loan',data=data)","7f74f871":"#From the above 3 cells, we can see thet the columns ID,ZIP Code can be dropped,Since they are not providing necessary info about the Personal Loan\n#Since the Age and Experience are also Linearily dependent we can drop one of them. We can drop Experience column\n\ndrop_cols=['Experience','ID','ZIP Code']\ndata.drop(columns=drop_cols,inplace=True)\ndata.head()","d491bcbd":"print(data[['Personal Loan','Family']].groupby(['Family']).mean())\nprint(data[['Personal Loan','Education']].groupby(['Education']).mean())\nprint(data[['Personal Loan','Securities Account']].groupby(['Securities Account']).mean())","fc0adb6a":"#From above cell,we see that the Family count has some dependency on the Personal Loan, Its better we can \n\nplt.figure(figsize=(20,10))\nplt.subplot(1,2,1)\nsns.countplot(x='Family',hue='Personal Loan',data=data)\nplt.subplot(1,2,2)\nsns.countplot(x='Education',hue='Personal Loan',data=data)\n\n#We see that if the family size is greater than 2, it makes people to apply loan\n#So we can make a category column, Big_family - 0 means less than or equal to 2 ; 1 means greater than 2\ndata['Big_family']=data['Family'].replace({1:0,2:0,3:1,4:1})\n\n#We also see that the people with more than 1 degree has same characteristics, so we can group em together,\ndata['Is_Educated']=data['Education'].replace({1:0,2:1,3:1})\n\n\n#We see that most of people dont take Mortgage, So its better to convert wether Mortgage is taken or not,\ndata['IsMortgage']=data['Mortgage']\ndata[data['IsMortgage']!=0] = 1\n\n#Thus we can drop Family,Education and Mortrage columns,\ndrop_cols=['Mortgage','Family','Education']\ndata.drop(columns=drop_cols,inplace=True)","541ee5f9":"plt.figure(figsize=(20,10))\nplt.subplot(2,2,1)\nsns.countplot(x='Securities Account',hue='Personal Loan',data=data)\nplt.subplot(2,2,2)\nsns.countplot(x='CD Account',hue='Personal Loan',data=data)\nplt.subplot(2,2,3)\nsns.countplot(x='Online',hue='Personal Loan',data=data)\nplt.subplot(2,2,4)\nsns.countplot(x='CreditCard',hue='Personal Loan',data=data)","03e81e70":"#Check dependency among the categorical variables\n\ncont1=pd.crosstab(data['Securities Account'],data['CD Account'])\nprint(chi2_contingency(cont1))\n\ncont2=pd.crosstab(data['CreditCard'],data['CD Account'])\nprint(chi2_contingency(cont2))\n\ncont3=pd.crosstab(data['CreditCard'],data['Online'])\nprint(chi2_contingency(cont3))","a7c78e5c":"#Done for OLS method- formula\n\ndata.rename(index=str,columns={\"Personal Loan\":\"Personal_Loan\" , \"Securities Account\":\"Securities_Account\" , \"CD Account\" : \"CD_Account\"},inplace=True)\ndata.head()","454adc1b":"#Splitting of Independent and Dependent variables\n\ny=data['Personal_Loan']\nX=data.drop(columns='Personal_Loan')","f94c3471":"#Standardization of Data\n\ndef standardization(X_train,X_test):\n    scaler=preprocessing.StandardScaler()\n    X_train=scaler.fit_transform(X_train)\n    X_test=scaler.transform(X_test)\n    return X_train,X_test","7e931715":"#Linear regression method,\n\ndef linear_reg(X,y):\n    \n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=1)\n    X_train,X_test=standardization(X_train,X_test)\n    \n    linear_reg=LinearRegression()\n    linear_reg.fit(X_train,y_train)\n    score=linear_reg.score(X_test,y_test)\n    print(\"The linear model prediction is \" + str(score*100) + \"%\")\n    \n    \n    # make predictions\n    expected = y_test\n    predicted = linear_reg.predict(X_test).round()\n    print(\"The confusion matrix is \")\n    print(metrics.confusion_matrix(expected, predicted))\n    \n    roc=roc_auc_score(y_test, predicted)\n    print(\"ROC value for linear model is \"+ str(roc*100) + \"%\")\n    \n    \n#OlS Linear Regression method\n\ndef linear_reg_ols(formula,data):\n    model=ols(formula,data).fit()\n    print(model.summary())\n    \n\n#Polynomial Regression model\n\ndef polynomial_reg(X,y):\n    X_poly_train,X_poly_test,y_poly_train,y_poly_test=train_test_split(X,y,test_size=0.25,random_state=1)\n    X_poly_train,X_poly_test=standardization(X_poly_train,X_poly_test)\n    \n    poly = PolynomialFeatures(degree=2, interaction_only=True)\n\n    X1_poly_train=poly.fit_transform(X_poly_train)\n    X1_poly_test=poly.fit_transform(X_poly_test)\n\n    lin=linear_model.LinearRegression()\n    lin.fit(X1_poly_train,y_poly_train)\n\n    y_pred=lin.predict(X1_poly_test)\n\n    poly_score=lin.score(X1_poly_test,y_poly_test)\n    print(\"The polynomial model prediction is \" + str(poly_score*100) + \"%\")\n    \n    # make predictions\n    expected = y_poly_test\n    predicted = lin.predict(X1_poly_test).round()\n    print(\"The confusion matrix is \")\n    print(metrics.confusion_matrix(expected, predicted))\n    \n    roc=roc_auc_score(expected, predicted)\n    print(\"ROC value for linear model is \"+ str(roc*100) + \"%\")\n    \n    \n#Gradient Descent\n\ndef gradient_descent(X_train,y_train):\n    gradient=SGDClassifier(max_iter=1000,tol=1e-3)\n    gradient.fit(X_train,y_train)\n    y_pred=gradient.predict(X_test)\n    y_pred=y_pred.reshape(1250,1)\n    grad_score=gradient.score(X_test,y_test)\n    \n    print(y_pred)\n    print(y_test)\n    print(\"The Gradient Descent model prediction is \" + str(grad_score*100) + \"%\")\n    \n#Logistic regression\n\ndef logistic_reg(X,y):\n    \n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=1)\n    X_train,X_test=standardization(X_train,X_test)\n    \n    logistic_reg=LogisticRegression()\n    logistic_reg.fit(X_train,y_train)\n    log_pred=logistic_reg.predict(X_test)\n    log_score=logistic_reg.score(X_test,y_test)\n    print(\"The Logistic model prediction is \" + str(log_score*100) + \"%\")\n    print(\"The confusion matrix is \")\n    print(metrics.confusion_matrix(y_test, log_pred))\n    print(\"the Classification report is\")\n    print(metrics.classification_report(y_test, log_pred))\n    roc=roc_auc_score(y_test, log_pred)\n    print(\"ROC value for logistic model is \"+ str(roc*100) + \"%\")\n    \n    \n#Naive Bayes\n\ndef naive_bayes(X,y):\n    \n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=1)\n    X_train,X_test=standardization(X_train,X_test)\n    \n    naive_model=GaussianNB()\n    naive_model.fit(X_train,y_train)\n    naive_pred=naive_model.predict(X_test)\n    naive_score=naive_model.score(X_test,y_test)\n    print(\"The Naive Bayes model prediction is \" + str(naive_score*100) + \"%\")\n    print(\"The confusion matrix is \")\n    print(metrics.confusion_matrix(y_test, naive_pred))\n    print(\"the Classification report is\")\n    print(metrics.classification_report(y_test, naive_pred))\n    roc=roc_auc_score(y_test, naive_pred)\n    print(\"ROC value for linear model is \"+ str(roc*100) + \"%\")\n    \n    \n#KNN\n\ndef knn(X,y,n):\n    \n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=1)\n    X_train,X_test=standardization(X_train,X_test)\n    \n    knn_model = KNeighborsClassifier(n_neighbors= n , weights = 'distance' )\n    knn_model.fit(X_train, y_train)\n    knn_predict=knn_model.predict(X_test)\n    knn_score=knn_model.score(X_test,y_test)\n    print(\"The KNN model prediction is \" + str(knn_score*100) + \"%\")\n    print(\"The confusion matrix is \")\n    print(metrics.confusion_matrix(y_test,knn_predict))\n    print(\"the Classification report is\")\n    print(metrics.classification_report(y_test,knn_predict))\n    roc=roc_auc_score(y_test, knn_predict)\n    print(\"ROC value for linear model is \"+ str(roc*100) + \"%\")\n    \n    \n#SVM\n\ndef svm_fun(X,y):\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=1)\n    X_train,X_test=standardization(X_train,X_test)\n    \n    clf = svm.SVC(gamma=0.025,C=3)\n    #when C increases Marigin shrinks\n    # gamma is a measure of influence of a data point. It is inverse of distance of influence. C is complexity of the model\n    # lower C value creates simple hyper surface while higher C creates complex surface\n\n    clf.fit(X_train,y_train)\n    svm_pred=clf.predict(X_test)\n    svm_score=clf.score(X_test,y_test)\n    print(\"The KNN model prediction is \" + str(svm_score*100) + \"%\")\n    \n    print(\"The confusion matrix is \")\n    print(metrics.confusion_matrix(y_test,svm_pred))\n    print(\"the Classification report is\")\n    print(metrics.classification_report(y_test,svm_pred))\n    roc=roc_auc_score(y_test, svm_pred)\n    print(\"ROC value for svm model is \"+ str(roc*100) + \"%\")","bd0f66d4":"#Linear\nlinear_reg(X,y)","4d50ebcd":"#OLS linear\nformula= ' Personal_Loan ~ Age + Income + CCAvg + Securities_Account + CD_Account + Online + CreditCard + Big_family + Is_Educated + IsMortgage '\nlinear_reg_ols(formula,data)","e66d0e0e":"#Polynomial\npolynomial_reg(X,y)","142737bf":"#Gradient Descent\n#gradient_descent(X_train,y_train)","c1b77817":"#SVM\n\nsvm_fun(X,y)","e7001123":"#Logistic Regression\nlogistic_reg(X,y)\n\n#Since in the input data, we have more value for personal Loan as 0 than 1,\n   #we must consider the 0 class level f1 score - Here it is 96% good that it would predict who would get the Personal loan","30e694a9":"#Naive Bayes\nnaive_bayes(X,y)\n\n#Since in the input data, we have more value for personal Loan as 0 than 1,\n   #we must consider the 0 class level f1 score - Here it is 91% good that it would predict who would get the Personal loan","9c64601c":"#KNN\nknn(X,y,3)\n\n#Since in the input data, we have more value for personal Loan as 0 than 1,\n   #we must consider the 0 class level f1 score - Here it is 96% good that it would predict who would get the Personal loan","3c526b61":"# DATA PREPROCESSING","4022f3ef":"# Models","68a51195":"Here,\n    3 predicted people by us dont get loan\n    93 who we predict dont get loan actually get loan\n    \nOut of the Actual who get loan,we predicted,  \nPrecession=461\/(93+461)=0.83\n","31ff27c5":"Inference:\n    From the above figure,we see that the CCAvg,Income and Mortgage column has outliers and thus thet are Right Skewed","b9d082a9":"Inference:\n    From the above,we see that Age and Experience are linearly correlated\n    Income and CCAvg are partially correlated","68787573":"Here,\n    10 predicted people by us dont get loan\n    32 who we predict dont get loan actually get loan\n    \nOut of the Actual who get loan,we predicted,  \nRecall=522\/(32+522)=0.94\n","9edc8fcd":"# In KNN, we see that the Precession value,ROC and f1-score of 1 is higher than compared to Logistic and Naive bayes, So i recommend to follow KNN Algorithm","db7ab1f4":"#Since the p<0.05 for Securities Account ,CD Account and \n#CreditCard, CD Account \n#we can just keep the CD Account column and delte other 2\n\ndrop_cols=['Securities Account','CreditCard']\ndata.drop(columns=drop_cols,inplace=True)\ndata.head()","b38a420c":"Inference:\n        From the above figures, we see that the Age,Experience column are normally distributed\n        Income,CCAvg,Mortgage columns are right Skewed","b036e939":"Here,\n    5 predicted people by us dont get loan\n    32 who we predict dont get loan actually get loan\n  \nOut of the Actual who get loan,we predicted,  \nPrecession=518\/(36+518)=0.94\n\n"}}