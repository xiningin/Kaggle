{"cell_type":{"b13ecd57":"code","c462fd41":"code","e597860d":"code","1c1f6849":"code","3b78a0d4":"code","9dddbe15":"code","d4494045":"code","71d15205":"code","e09ca2b8":"code","447c368d":"code","f3408567":"code","42a8f025":"code","04ec52d6":"code","52c39dcb":"code","286cdcd3":"code","7377d7fe":"code","257dc8a5":"code","4ff3fa91":"code","3edf0bfb":"code","c8ecc243":"code","5ed123b0":"code","10191336":"code","5defa788":"code","b87a2897":"code","573b7f24":"code","4a09c097":"code","c4bad17f":"code","f9faa27a":"code","9b157f84":"code","a8a72d3c":"code","e6420da7":"code","94968a41":"code","e624f861":"code","04890406":"code","f4943cad":"code","23bef515":"code","983e77c6":"code","6ac6d64a":"code","babd0cdc":"code","49de0809":"code","c55e88b5":"code","a6276afa":"code","82978b2e":"code","cfd966ca":"code","c8f3afc5":"code","8078eb50":"code","f7dc3ce5":"code","b2f639b9":"code","c6d28c69":"code","5bb3d699":"code","672e6d51":"code","b7240364":"code","cf9822b5":"code","12615191":"code","66f3d04b":"code","e80114b4":"code","3ed34d0f":"code","86f643a9":"code","6cd2603f":"code","db659b4d":"code","951b7672":"code","cf2701d9":"code","52a4ffeb":"code","3fb48151":"code","7c732840":"code","02d0ae81":"code","f70702eb":"code","10cea8b8":"code","78edd07d":"code","058cebaa":"code","f6188359":"code","5a12d3a0":"code","2c835c89":"code","460d43a2":"code","7220ea2c":"code","b300ea60":"code","dbf2b778":"code","3cc5b76b":"code","18d97645":"code","a27c2fca":"code","1467808b":"code","3cce9937":"code","a0e07b36":"code","91f3728a":"code","00cb642a":"code","754e7714":"code","ec3ad6ea":"code","eae323d6":"code","3bd85518":"markdown","94ff138d":"markdown","d77ff2f3":"markdown","dda2218f":"markdown","7572f562":"markdown","582f3c1c":"markdown","af571d9a":"markdown","ad4d5614":"markdown","30f5cc9d":"markdown","c9593cd1":"markdown","fc28fefb":"markdown","8ab733c2":"markdown","f8655e2e":"markdown","87236717":"markdown","81d754ff":"markdown","f600046d":"markdown","5de78f35":"markdown","0c34c7fb":"markdown","368d9222":"markdown","d4f89848":"markdown","d4f4f664":"markdown","9261516f":"markdown","3e00b341":"markdown","ce620ae4":"markdown","1ea3a654":"markdown","24f3aea8":"markdown","99dfd0e1":"markdown","7a9dd7a9":"markdown","52f57b0d":"markdown","e1b0cce3":"markdown","6309ac6c":"markdown","ed070b9e":"markdown","7384520f":"markdown","7c5de08d":"markdown","f7140859":"markdown","70c11d2c":"markdown","eb02cd04":"markdown","30e88786":"markdown","af55563a":"markdown","4c58cdf1":"markdown","7eb87a6e":"markdown","ae946845":"markdown","76150d36":"markdown","7ebe05c6":"markdown","9db1c63e":"markdown","20d72c80":"markdown","7e32aeea":"markdown","0c89aee6":"markdown","2590ff19":"markdown","262e52e6":"markdown","35a69395":"markdown","2f827588":"markdown","7f715286":"markdown","2b9d91f4":"markdown","fbc7dfaa":"markdown","997cc7fd":"markdown","a836ab36":"markdown","b2c550f4":"markdown","95ee945f":"markdown","010b9c2a":"markdown"},"source":{"b13ecd57":"import sklearn\nsklearn.__version__","c462fd41":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport random\n\n# pd.options.plotting.backend = 'plotly'\n# plt.style.use('ggplot')\n%matplotlib inline","e597860d":"# data = pd.read_csv(\"data.csv\", index_col='ID')\ndata = pd.read_csv(\"..\/input\/fifa19\/data.csv\", index_col='ID')","1c1f6849":"data.head()","3b78a0d4":"pd.set_option('display.max_columns', None)","9dddbe15":"data.head()","d4494045":"print(f\"Total number of players in dataset {data.shape[0]}\")","71d15205":"# \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c % \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439\nfrom tabulate import tabulate\n\ntop = 10\nprint(tabulate(\n    sorted(list(zip(data.columns, data.isnull().sum(), data.isnull().sum() \/ \n                    data.shape[0] * 100)), key=lambda x: -x[2])[:top], \n    headers=['col_name', 'null_cnt', 'null_perc']))","e09ca2b8":"print(f\"Weight column type is '{data['Weight'].dtype}'\")","447c368d":"data['Weight_float'] = data['Weight'].str.extract(r'([0-9]+)lbs').astype(float)\ndata['Weight_float'] = data['Weight_float'].fillna(data['Weight_float'].median())\nPOUND_TO_KILO = 0.454\ndata['Weight_kg'] = data.apply(lambda row: row['Weight_float'] * POUND_TO_KILO, axis=1)\n# data.hist(column='Weight_kg', bins=30)\n# plt.show()","f3408567":"fig = px.histogram(data, x=\"Weight_kg\", nbins=60)\nfig.show()","42a8f025":"data.shape","04ec52d6":"# datka = data.copy()","52c39dcb":"data = data.dropna(subset=['Height'])","286cdcd3":"FEET_TO_METER = 0.305\nINCH_TO_METER = 0.025\n\n\ndef to_meters(row):\n    return (float(row['Height_m'][0]) * FEET_TO_METER + \n            float(row['Height_m'][1]) * INCH_TO_METER)","7377d7fe":"# Your code here\ndata['Height_m'] = data['Height'].str.split(\"'\")\ndata['Height_m'] = data.apply(lambda row: to_meters(row), axis=1)\n# datka.hist(column='Height_m', bins=19)\n# plt.show()","257dc8a5":"fig = px.histogram(data['Height_m'], x=\"Height_m\", nbins=24)\nfig.show()","4ff3fa91":"data.shape","3edf0bfb":"# data.plot.scatter(x='Weight_kg', y='Strength')\n# plt.figure(figsize=(10, 6))\n# plt.scatter(x=data[\"Weight_kg\"], y=data[\"Strength\"], alpha=0.4)\n# plt.title('Dependence of strength on weight')\n# plt.xlabel(\"Weight_kg\")\n# plt.ylabel(\"Strength\")\n# plt.show();","c8ecc243":"fig = px.scatter(data, x=\"Weight_kg\", y=\"Strength\")\nfig.show()","5ed123b0":"sns.pairplot(data[['ShortPassing', 'Dribbling', 'BallControl', 'Strength']]);","10191336":"def to_age_group(x):\n    if x['Age'] < 20:\n        return 'young'\n    elif x['Age'] <= 30:\n        return 'mature'\n    else:\n        return 'masters'","5defa788":"# data['age_group'] = data.apply(lambda x: 'young' if x['Age'] < 20 else 'mature' if x['Age'] <= 30 else 'masters', axis=1)\ndata['age_group'] = data.apply(lambda x: to_age_group(x), axis=1)\ndistr = data.groupby('age_group').count().max(axis=1)[['young', 'mature', 'masters']]\n\n# plt.bar(distr.index, distr.values)\n# plt.ylabel('Number of players')\n# plt.title('Distribution of players across age groups')\n# plt.show()","b87a2897":"fig = px.bar(data, x=distr.index, y=distr.values)\nfig.update_layout(\n    title_text='Distribution of players across age groups',\n    yaxis=dict(\n        title='Number of players',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),\n    xaxis=dict(title='')\n)\nfig.show()","573b7f24":"# sns.boxplot(x='age_group', y='SprintSpeed', data=data);","4a09c097":"fig = px.box(data, x=\"age_group\", y=\"SprintSpeed\")\nfig.show()","c4bad17f":"from sklearn.model_selection import train_test_split\n\ndata.fillna({'BallControl': data['BallControl'].mean(), 'Dribbling': data['Dribbling'].mean()}, inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(data['Dribbling'].values, data['BallControl'].values, train_size=0.8)\nX_train = X_train.reshape(-1, 1)\nX_test = X_test.reshape(-1, 1)","f9faa27a":"from sklearn.linear_model import Ridge\n\nlr = Ridge(alpha=0)\nlr.fit(X=X_train, y=y_train)","9b157f84":"print(f'w_0 = {lr.intercept_}, w_1 = {lr.coef_[0]}')","a8a72d3c":"y_pred_train = lr.predict(X_train)\ny_pred_test = lr.predict(X_test)","e6420da7":"data['predicted_BallControl'] = lr.predict(data['Dribbling'].values.reshape(-1, 1))\ndata[['Name', 'Dribbling', 'BallControl', 'predicted_BallControl']].head()","94968a41":"def mse(y_true, y_pred):\n    # your code here\n    error = np.mean((y_true - y_pred)**2)\n    return error","e624f861":"from sklearn.metrics import mean_squared_error\n\nassert mean_squared_error(y_train, y_pred_train) == mse(y_train, y_pred_train)\nassert mean_squared_error(y_test, y_pred_test) == mse(y_test, y_pred_test)\n\n# assert round(mean_squared_error(y_train, y_pred_train), 9) == round(mse(y_train, y_pred_train), 9)\n# assert round(mean_squared_error(y_test, y_pred_test), 9) == round(mse(y_test, y_pred_test), 9)","04890406":"print(f'Train MSE {mse(y_train, y_pred_train)}, test MSE {mse(y_test, y_pred_test)}')","f4943cad":"#your code here\nplt.figure(figsize=(10, 6))\nplt.scatter(x=data[\"Dribbling\"], y=data[\"BallControl\"], alpha=0.4, label=\"Real\")\nplt.scatter(x=X_test, y=y_pred_test, color=\"orange\", label=\"Prediction\")\nplt.title(\"Dependence of predicted Ball control and real on Dribbling\")\nplt.xlabel(\"Dribbling\")\nplt.ylabel(\"BallControl\")\nplt.legend()\nplt.show();","23bef515":"# interactive plot with plotly\nfig = px.scatter(data, x=\"Dribbling\", y=\"BallControl\", opacity=0.4,\n                 trendline='ols', trendline_color_override='orange')\nfig.show()","983e77c6":"def compute_residuals(w, X, y):\n    \"\"\"\n    Compute residuals when predicting y_hat as matrix product of X and transposed w\n    :param w: linear regression weights, numpy.ndarrya: float64[num_features]\n    :param X: training features, numpy.ndarray: float64[num_samples, num_features]\n    :param y: training target, numpy.ndarray: float64[num_samples]\n    :returns: vector of residuals (y_i_hat - y_i) for each sample_i in X\n    \"\"\"\n    residuals = (X @ w) - y\n\n    return residuals","6ac6d64a":"from sklearn.base import BaseEstimator\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom scipy.optimize import least_squares\n\nclass LinearRegression(BaseEstimator):\n    def __init__(self, fit_intercept=True):\n        self.fit_intercept = fit_intercept\n    \n    def fit(self, X, y):\n        \"\"\"\n        fit model weights given input features and target\n        :param X: training features, numpy.ndarray: numeric[num_samples, num_features]\n        :param y: training target, numpy.ndarray: numeric[num_samples]\n        :returns: linear predictor with fitted weights so that train MSE is the lowest possible\n        :note: weights: numpy.ndarray: float64[num_features] stored as class field\n\n        \"\"\"\n        \n        # Check that X and y have correct shape\n        X, y = check_X_y(X, y)\n        \n        # Save train data information. Necessary for following the uniform API\n        self.X_ = X\n        self.y_ = y\n        self.n_features_in_ = X.shape[1]\n        \n        # Copy arrays and cast them to uniform type\n        X_train = X.astype('float64')\n        y_train = y.astype('float64')\n        \n        # Add dummy column of ones to X_train if we want to train an intercept - last component of future weight vector\n        if self.fit_intercept:\n            X_train = np.column_stack((X_train, np.ones(X_train.shape[0])))\n        \n        # Your code here.\n        # Just follow the suggested steps: create initial weights vector,\n        # apply least_squares optimizer passing the parameters described above\n        # and finally extract optimized weights.\n        # Remember: you need to distinguish coefficients from intercept when fit_intercept=True\n\n        self.W_ = np.zeros(X_train.shape[1]) \n        least_sq = least_squares(fun=compute_residuals, x0=self.W_, args=(X_train, y_train))\n        \n        self.coef_ = least_sq.x[:-1] if self.fit_intercept else least_sq.x\n        self.intercept_ = least_sq.x[-1] if self.fit_intercept else 0\n        \n        # Return the classifier\n        return self\n        \n    def predict(self, X):\n        # Check is fit had been called\n        check_is_fitted(self)\n        \n        # Input validation\n        X = check_array(X)\n        \n        return X.dot(self.coef_) + self.intercept_","babd0cdc":"#Testing area\nfrom sklearn.utils.estimator_checks import check_estimator\nfrom sklearn.linear_model import Ridge\n\nlr = LinearRegression()\nridge = Ridge(alpha=0)\nlr_no_intercept = LinearRegression(fit_intercept=False)\nridge_no_intercept = Ridge(alpha=0, fit_intercept=False)\n\n#Check compatibility with Sklearn framework and apply some spesific internal tests\ncheck_estimator(lr)\ncheck_estimator(lr_no_intercept)\n\n#Compare model accuracy with Ridge(0) from Sklearn\ndata.fillna({'BallControl': data['BallControl'].mean()\n             , 'Dribbling': data['Dribbling'].mean()\n             , 'Strength': data['Strength'].mean()}, inplace=True)\nX_sample, y_sample = data[['Dribbling', 'Strength']], data['BallControl']\nlr.fit(X_sample, y_sample)\nridge.fit(X_sample, y_sample)\nassert np.allclose(lr.predict(X_sample), ridge.predict(X_sample), rtol=1e-03), \"Your model with intercept not accurate enough!\"\nlr_no_intercept.fit(X_sample, y_sample)\nridge_no_intercept.fit(X_sample, y_sample)\nassert np.allclose(lr_no_intercept.predict(X_sample), ridge_no_intercept.predict(X_sample), rtol=1e-03), \"Your model without intercept not accurate enough!\"","49de0809":"features = ['BallControl', 'ShortPassing', 'Strength', 'Weight_float', 'Weight_kg']\ntarget = 'Dribbling'\nfor feat in features:\n    data.fillna({feat: data[feat].mean()}, inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(data[features].values, data[target].values, train_size=0.8, random_state=2)","c55e88b5":"lr = Ridge(0)\nlr.fit(X=X_train, y=y_train)\n\ny_pred_train = lr.predict(X_train)\ny_pred_test = lr.predict(X_test)\n\nprint(f'Train MSE {mean_squared_error(y_train, y_pred_train)}, test MSE {mean_squared_error(y_test, y_pred_test)}')","a6276afa":"print(f'w_0 = {lr.intercept_}, w_1, w_2, w_3, w_4, w_5 = {lr.coef_}')","82978b2e":"player = data[features + [target]].iloc[0:2]\nplayer['Predicted_dribbling'] = lr.predict(player[features].values)\nplayer.head()","cfd966ca":"player['Weight_kg'] = player['Weight_kg'] + [-0.001, 0.001]\nplayer['Predicted_dribbling_with_error'] = lr.predict(player[features].values)\nplayer.head()","c8f3afc5":"from seaborn import heatmap\n\nheatmap(data[features].corr(method='pearson'), center=0, square=True)\nplt.show()","8078eb50":"features = ['BallControl', 'ShortPassing', 'Strength', 'Weight_kg']\nX_train, X_test, y_train, y_test = train_test_split(data[features].values, data[target].values, train_size=0.8, random_state=2)\n\nlr = Ridge(alpha=0)\nlr.fit(X=X_train, y=y_train)\n\nplayer['Predicted_dribbling_with_error'] = lr.predict(player[features].values)\nplayer.head()","f7dc3ce5":"# Your code and a bunch of cool ideas here","b2f639b9":"len(pd.unique(data['Value']))","c6d28c69":"# pd.unique(data['Value'])","5bb3d699":"data['Value'].isna().value_counts()","672e6d51":"def euro_to_float(x):\n    if x['Value'] == '\u20ac0':\n        return float(0)\n    elif \"K\" in x['Value']:\n        return float(x['Value'][1:-1])  \/ 1000\n#     else:\n    elif \"M\" in x['Value']:\n        return float(x['Value'][1:-1])","b7240364":"data['Value_M_euro'] = data.apply(lambda row: euro_to_float(row), axis=1)\n\nprint(len(pd.unique(data['Value'])))\n# pd.unique(data['Value_M_euro'])","cf9822b5":"data['Value_M_euro'] = data['Value_M_euro'].fillna(data['Value_M_euro'].median())\n\n# data = data.dropna(subset=['Value_M_euro'])","12615191":"# data.columns","66f3d04b":"# simple encoding age_group column\ndata['age_group'] = data['age_group'].map({\"young\": 1, \"mature\": 2, \"masters\": 3})","e80114b4":"# simple encoding 'Work Rate' column\ndata['Work Rate'] = data['Work Rate'].map({\n    \"Medium\/ Medium\": 1, \"High\/ Medium\": 2, \"Medium\/ High\": 3,\n    \"High\/ High\": 4, \"Medium\/ Low\": 5, \"High\/ Low\": 6,\n    \"Low\/ Medium\": 7, \"Low\/ High\": 8, \"Low\/ Low\": 9,\n})","3ed34d0f":"# simple encoding 'Body Type' column\ndata['Body Type'] = data['Body Type'].fillna(\"-\").map({\n    \"Normal\": 1, \"Lean\": 2, \"Stocky\": 3,\n    \"-\": 4, \"C. Ronaldo\": 5, \"Akinfenwa\": 6,\n    \"Messi\": 7, \"Shaqiri\": 8, \"Neymar\": 9,\n    \"PLAYER_BODY_TYPE_25\": 10, \"Courtois\": 11,\n})","86f643a9":"# simple encoding 'Preferred Foot' column\ndata['Preferred Foot'] = data['Preferred Foot'].fillna(\"-\").map({\n    \"-\": 0, \"Right\": 1, \"Left\": 2\n})","6cd2603f":"data['Preferred Foot'].fillna('-').value_counts()","db659b4d":"data[[\"Name\", \"Overall\", \"Potential\", \"Club\", \"Value_M_euro\"]]\\\n    .sort_values(by=['Value_M_euro'], ascending=False).head(5)","951b7672":"data[[\"Name\", \"Overall\", \"Potential\", \"Club\", \"Value_M_euro\"]]\\\n    .sort_values(by=['Value_M_euro'], ascending=False).tail(5)","cf2701d9":"fig = px.box(data, y=\"Value_M_euro\")\nfig.show()","52a4ffeb":"# drop players with value less than \u20ac20K\nprint(f\"Count of players with value < \u20ac20K: {data[data['Value_M_euro'] < 0.02].shape[0]}\")\ndata = data.drop(data[data[\"Value_M_euro\"] < 0.02].index)","3fb48151":"# drop players with value more than \u20ac20 million \nprint(f\"Count of players with value > \u20ac20M: {data[data['Value_M_euro'] > 20].shape[0]}\")\ndata = data.drop(data[data[\"Value_M_euro\"] > 20].index)","7c732840":"fig = px.box(data, y=\"Value_M_euro\")\nfig.show()","02d0ae81":"data.shape","f70702eb":"features = [\n    'Value_M_euro',\n    'Overall',\n    'Potential',\n    'International Reputation',\n    'Weak Foot',\n    'Skill Moves',\n    'Volleys', \n    'Body Type',\n    'Special',\n    'Work Rate',\n    'Jersey Number',\n    'Weight_kg',\n    'Height_m',\n    'age_group',\n    'Age',\n    'Stamina',\n    'Strength',\n    'Crossing',\n    'Finishing',\n    'HeadingAccuracy',\n    'ShortPassing', \n    'Dribbling',\n    'BallControl', \n    'Curve',\n    'Composure',\n    'Preferred Foot',\n    'Unnamed: 0', \n    'Reactions',\n]","10cea8b8":"data.head(2)","78edd07d":"data_task = data[features].copy()\ndata_task.head()","058cebaa":"corr_matr = data_task.corr(method='pearson').abs()\n# corr_matr","f6188359":"cmap = cmap=sns.diverging_palette(220, 10, as_cmap=True)\n\ndef plot_corr_matr(c_matr):\n    f, ax = plt.subplots(figsize=(16, 14))\n    matrix_tr = np.triu(c_matr)\n\n    sns.heatmap(\n        c_matr, \n        mask=matrix_tr, \n        cmap=cmap,\n        linewidths=0.5,\n        square=True, \n        ax=ax,\n        annot=True\n    );","5a12d3a0":"plot_corr_matr(corr_matr)","2c835c89":"upper_tri = corr_matr.where(np.triu(np.ones(corr_matr.shape), k=1).astype(np.bool))\n# print(upper_tri)","460d43a2":"to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\nprint(to_drop)","7220ea2c":"data_task = data_task.drop(data_task[to_drop], axis=1)\ndata_task.head()","b300ea60":"corr_matr2 = data_task.corr(method='pearson').abs()","dbf2b778":"plot_corr_matr(corr_matr2)","3cc5b76b":"target = 'Value_M_euro'\nfeatures = data_task.columns.tolist()\nfeatures.remove(target)\nfeatures","18d97645":"for feat in features:\n    data_task.fillna({feat: data_task[feat].median()}, inplace=True)\n    \nX_train, X_test, y_train, y_test = train_test_split(\n    data_task[features].values, data_task[target].values, train_size=0.8, random_state=17\n)","a27c2fca":"# model = Ridge(alpha=0)\nmodel = LinearRegression()\nmodel.fit(X=X_train, y=y_train)","1467808b":"y_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)","3cce9937":"player = data[features + [target]].iloc[0:5]\nplayer['Predicted_Value'] = model.predict(player[features].values)\nplayer.head()","a0e07b36":"from sklearn.metrics import r2_score","91f3728a":"print(f\"\"\"\nTrain. \nMSE: {mean_squared_error(y_train, y_pred_train)}\nr2_score: {r2_score(y_train, y_pred_train)}\n\nTest.\nMSE: {mean_squared_error(y_test, y_pred_test)}\nr2_score: {r2_score(y_test, y_pred_test)}\n\"\"\")","00cb642a":"print(f\"\"\"w_0 = {model.intercept_}\nw_1, w_2, w_3, w_4 = {model.coef_}\"\"\")","754e7714":"importance = model.coef_\n\nfeat_imp_dict = dict(zip(importance, features))\nfeat_imp_dict = dict(sorted(feat_imp_dict.items(), key=lambda item: item[0]))\n\nfor score, feature in feat_imp_dict.items():\n    print(f'Feature: {feature}, Score: {round(score, 5)}')","ec3ad6ea":"fig = px.bar(x=feat_imp_dict.keys(), y=[x for x in feat_imp_dict.values()])\nfig.update_layout(\n    title_text='Feature importance',\n    xaxis=dict(title='Features score'),\n    yaxis=dict(title='Features'),\n)\nfig.show()","eae323d6":"# Bonus =)\n\nfig = px.parallel_categories(\n    corr_matr2,\n    dimensions=[\"age_group\", \"Value_M_euro\", \"Body Type\"], \n    color=\"Value_M_euro\", \n    color_continuous_scale=px.colors.sequential.Inferno\n)\nfig.update_layout(title='Value dependence')\nfig.show()","3bd85518":"Just to understand the basic principles, let's try to predict _BallControl_ score based on the _Dribbling_ score for every player. Simple Linear Regression with one feature.\n$$BallControl = w_0 + w_1 * Dribbling$$","94ff138d":"**Example 5.**\nIllustrate pairwise dependencies between _ShortPassing_, _Dribbling_, _BallControl_ and _Strength_ features of footbal players.","d77ff2f3":"So we are selecting the columns which are having absolute correlation greater than 0.9 and making a list of those columns named 'to_drop'.","dda2218f":"Effective way to visualize the relationship between two features is to draw a simple _scatter plot_. The position of each dot on the horizontal and vertical axis indicates values for an individual data point.","7572f562":"## Part 1. Initial data analysis with Pandas","582f3c1c":"\u0421hoose at least 10 features that I expect to influence on player Value","af571d9a":"Number of unique records in column Value","ad4d5614":"But why it is so bad? The main reason is that Linear Regression tries to capture the contribution of each feature to target _independently_, which obviously is not possible in terms of feature multicolliearity.<br><br>\nThere are a whole bunch of really interesting thoughts that can help to capture the intuition behind it [here](https:\/\/stats.stackexchange.com\/questions\/1149\/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-r). I'd citate one of the examples provided.<br><br>\n_Assume that two people collaborated and accomplished scientific discovery. It is easy to tell their unique contributions (who did what) when two are totally different persons (one is theory guy and the other is good at experiment), while it is difficult to distinguish their unique influences (coefficients in regression) when they are twins acting similarly._","30f5cc9d":"### Part 2.3 Putting all together","c9593cd1":"Importing libraries.","fc28fefb":"Find missing values","8ab733c2":"https:\/\/github.com\/rolling-scopes-school\/ml-intro\/blob\/2021\/2_linear_regression\/seminar_and_homework.ipynb","f8655e2e":"We are going to do real data science, aren't we? So let us split the available data into train and test samples. We let our model see only the train data, then we can measure it's quality on test sample.","87236717":"## Part 2. Minimizing Mean Squared Error. Linear Regression","81d754ff":"Drop some outliers","f600046d":"Extract float number from Value field","5de78f35":"Unfortunately the number of columns exceeds the maximum visible default value in Pandas. Use the magic line above to remove this restriction.","0c34c7fb":"Histograms and scatter plots are good for continuous (numerical) features. Distribution of data by categorical features (that have a fixed number of possible values) can be represented with **bar charts**.","368d9222":"That is not ok, two last weight components are too large, and they vary depending on the run! Although the result seems better our model would behave unexpectadly to the patterns in data it has never seen! Large weights and weights instability are the sign of [**overfitting**](https:\/\/en.wikipedia.org\/wiki\/Overfitting).  <br><br>\nAccording to the definition it is \"_the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably_\". But what does it actually mean?","d4f89848":"**Task 5 (up to 5 points).** Build a Linear Regression model for _Value_ prediction for every football player and validate it. You **have to** use either your custom Linear Regression class or `sklearn.linear_model.Ridge` with regularization param alpha=0. Steps you need to follow:\n- Extract float number from _Value_ field in DataFrame (**0.5 points**)\n- \u0421hoose more features that you expect to influence on player _Value_ (at least 10)\n- Plot feature correlation matrix. (**0.5 points**)\n- Drop features that are highly correlated with each other (_abs_(corr) > 0.9) one by one until no correlated pairs left. _Hint_: you may reuse code from Task_9 in HW_1 for automatic correlated pairs selection. (**1.5 points**)\n- Split data into train\/test with some proportion (**0.5 points**)\n- Train a model on train dataset, make predictions both for train and test. (**0.5 points**)\n- Measure the model quality in terms of MSE in train and test samples,  (**0.5 points**)\n- Write a short report about the work done. Why did you take these particular features? Can you find a logical explanation for high correlation of some of your features? Are you satisfied with the quality of predictions? etc. (**1 point**)","d4f4f664":"Let's add more features in order to predict Dribbling score more accurately.","9261516f":"This assignment is dedicated to Linear regression. By focusing on prediction different features of football players you understand the mathematics behind it and see the usefulness of main data analysis libraries.","3e00b341":"In this task you will work with _optimize_ module of [_scipy_](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/) open-source library for mathematics, science, and engineering. You will need a function [_least_squares_](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.optimize.least_squares.html) that finds a coefficients for linear regression by minimizing the sum of the squares of the residuals (which is equivalent to MSE minimizing). More information about least squares approach [here](https:\/\/en.wikipedia.org\/wiki\/Least_squares). <br><br>\nEven though this function has many parameters, you need only a few of them to complete the task (the rest will be filled in with default values automatically).\n- **fun** computes a vector of residuals given weights, features and target, we provide you a function template _compute_residuals_\n- **x0** this is an initial weights vector. You can either pass a vector of zeros[n_features] or fill in randomly.\n- **args** are fixed arguments to _fun_ function (which we are not going to optimize). In that particular case you will need to pass X and y.\n\n\nYou can access optimized weights by accessing the field **.x** of object which returns by this function.","ce620ae4":"Summarize feature importance","1ea3a654":"# Initial data analysis and Linear Regression","24f3aea8":"**Materials**\n- [Documentation](http:\/\/docs.scipy.org\/doc\/) libraries Numpy and SciPy\n- [Documentation](http:\/\/matplotlib.org\/) library Matplotlib \n- [Documentation](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/tutorials.html) library Pandas\n- [Pandas Cheat Sheet](http:\/\/www.analyticsvidhya.com\/blog\/2015\/07\/11-steps-perform-data-analysis-pandas-python\/)\n- [Documentation](http:\/\/stanford.edu\/~mwaskom\/software\/seaborn\/) library Seaborn \n\n**Resources**\n- In this notebook we will use *FIFA 19 complete player dataset* which is taken from [here](https:\/\/www.kaggle.com\/karangadiya\/fifa19)","99dfd0e1":"Drop features that are highly correlated with each other (abs(corr) > 0.9) one by one until no correlated pairs left.","7a9dd7a9":"**Task 3 (1.5 points).** Visualize the dependence of **test** _BallControl_ predictions and real _BallControl_ score on _Dribbling_ score. Don't forget to add axis and plot names!","52f57b0d":"Write a short report about the work done. Why did you take these particular features? Can you find a logical explanation for high correlation of some of your features? Are you satisfied with the quality of predictions? etc. ","e1b0cce3":"**Task 4 (5 points).** Implement your own Linear Regression class for any number of input features and settable boolean parameter *fit_intercept*.","6309ac6c":"There are a few approaches how to prevent overfitting and overcome multicollinearity.\n- Drop features\n- Combine features\n- Regularization\n\n\nRegularization is something we are going to speak about in the next modules. Combining features is problem-specific and could easily trigger a _holy_war_ due to ambiguity of approaches. Let's focus on simpliest - drop one of the features from the correlated pair.<br>\nAt first we need to define those pairs of features, **correlation matrix** comes to rescue! Each cell in the table shows the correlation between two variables. We use dataframe in-built method _corr_ in combination with seaborn _heatmap_.","ed070b9e":"Predicted dribbling value has changed significantly! Look at how this tiny **1g** error leads to extremly big or small dribbling!","7384520f":"Split data into train\/test with some proportion","7c5de08d":"The reason behind it is strange unstable behaviour is **collinearity** between Weight and Weight_kg features, what means that Weight_kg can be linearly predicted from Weight. As a matter of fact they represent the same essense but in different scales. <br><br>\n**Multicollinearity** describes a more general case, when one feature can be predicted by linear combination of some other features.<br><br>\nCollinearity is really close related to **correlation** - degree to which a pair of variables are linearly related. Collinearity origins from Linear Algebra and Geometry whereas Correlation is a term from Statistics. Anyway all of this three terms refer to **linearly dependent features**, which is really bad for Linear Models.","f7140859":"Measure the model quality in terms of MSE in train and test samples","70c11d2c":"Much better now.\n\n**Example 2** Print total player number and top-10 columns containing the most number of null values.","eb02cd04":"**Task 2 (0.5 point).** Write your own function for MSE calculation using the formula above. Calculate train and test MSE, compare to built-in method (_sklearn.metrics.mean_squared_error_)","30e88786":"To illustrate the approach, let's use Ridge model from sklearn with _regularization_ param alpha=0. What does it mean and what it if for we will find out later on in this course. But for now I require avoiding regularization by setting regularization param to zero.","af55563a":"**Example 4.** Visualize the dependence of _Strength_  on _Weight_kg_.","4c58cdf1":"Predictions are pretty good if the data is _pure_. Let's add some noise to _Weight_kg_ feature:","7eb87a6e":"One more effective way of initial data analysis is to plot pairwise feature dependencies. That simply combines already considered Scatter plot and a histogram. We create $m \\times m$ plots (_m_ is number of features) where pictures on diagonal represent **histograms** and outside the diagonal **scatter_matrix**. That can be done with the help of _scatter_matrix_ Pandas DataFrame method or _pairplot_ in Seaborn.","ae946845":"Load the data. Table *data.csv* should be in the same directory as this notebook.","76150d36":"Right now we have predictions for train and test samples. How about measure the quality of the model?","7ebe05c6":"The first thing you need to do with a dataframe after loading is to look at first few records. This way you can make sure that you have parsed it correctly. Moreover, you can get acquainted with the data, look at the features and their type (categorical, numerical, text ...).\n\nThey you may check whether the data has missing values inside. Depending on the problem type and percentage of missing values you can either fill them with some value or drop columns\/rows having null values.\n\nAfter that you may want to look closer at some features. You can draw a histogram for defining a feature distribution (normal, power or some other). Also with the help of histogram you can find values which are really differ from the rest, we call them **outliers**. Histograms can be plotted by *hist* method of Pandas DataFrame.\n\n**Example 1** Let's look at first 5 rows of data using method *head* for DataFrame data.","9db1c63e":" ### Part 2.1 Linear Regression with one variable","20d72c80":"\n!!! IMPORTANT\nPlease complete this assignment without any cycles. You may use the standard operations of matrix \\ vector multiplication ans different statistic calculation with NumPy. Otherwise, your solution may not go through asserts.","7e32aeea":"Plot feature correlation matrix","0c89aee6":"**Example 7.** Show _SprintSpeed_ statistics across different age groups.","2590ff19":"Assume that we have a player whose weight in kg was calculated with some tiny error, let's say +=1g.","262e52e6":"Really often it is necessary to explore the distribution of some numerical feature based on the value of categorical one. Here comes the _boxplot_ of Seaborn library, which can show statistics of numerical features (mean, quantiles) by different value of categorical feature. Boxplot can also help to detect **outliers** - values that significantly differ from the rest. More detailed explanation [here](https:\/\/towardsdatascience.com\/understanding-boxplots-5e2df7bcbd51).","35a69395":"### Part 2.2 Linear regression with many variables","2f827588":"Train a model on train dataset, make predictions both for train and test.","7f715286":"All unique entries in Value","2b9d91f4":"**Task 1 (1 point)**. Built a histogram of the height distribution in *meters* from footbal player data. Remember that height is in format *feet* '*inches*.  Instead of filling null values with some constant just drop them. Use *.dropna* for specified column.","fbc7dfaa":"Note that Correlation matrix will be mirror image about the diagonal and all the diagonal elements will be 1. So, It does not matter that we select the upper triangular or lower triangular part of the correlation matrix but we should not include the diagonal elements. So we are selecting the upper triangular.","997cc7fd":"**Penalties**\n- **-1 point** if used a different model besides custom Linear Regression or `sklearn.linear_model.Ridge` with regularization param alpha=0\n- **-0.5 points** if number of selected features BEFORE removal of linearly dependent ones is less than 10.\n- **-0.5 points** if did not remove linearly dependent features before training the model.","a836ab36":"We are going to predict target numerical variable $y$ for _n_ samples with the help of $x_1, x_2, ..., x_m$ _m_ features under the assumption of _liner dependence_ existence between features and target, i.e.\n$$\\hat{y} = w_0 + w_1 * x_1 + w_2 * x_2 + ... + w_m * x_m$$\nso that Mean Squared Error between $y$ and $\\hat{y}$ was the lowest possible\n$$MSE = \\frac{1}{n}\\sum_{i=1}^n {(y_i - \\hat{y})}^2 -> min_{w_0, w_1, w_2, ...w_m}$$\nwhere $w_0$ is \"free\" weight component called **intercept** and $(w_1, w_2, ... w_n)$ is a **vector of coefficients**.","b2c550f4":"I did a lot of fun experiments with features, but none of the feature set affected the MSE error as much as statistical outliers related to the cost of players, you can see this in the boxplots above in the code.\n\nThese are players with very low or zero value, or players with very high value, such as Messi. I came to the conclusion that it is better to remove such outliers from the dataframe. I removed players with a value of less than 10 thousand euros and with a value of over 20 million euros, which gave an interesting result. During the experiments, there were attempts to remove players with a value of more than 5 million euros, which gave an MSE error of about 0.5, but in the final version I decided not to leave this scheme. \n\nAre you satisfied with the quality of the prediction? It's hard to say, probably not, but on the other hand, I am very satisfied with the work done as a whole, and I am especially happy with the knowledge and skills that I received in the process of doing the work.\n\nThanks!","95ee945f":"**Example 6.** Show distribution of players by age groups (under 20 yo. _young_, between 20-30 _mature_, over 30 yo. _masters_)","010b9c2a":"**Example 3**. Let's built a histogram of weight distribution in kgs from footbal players data. Follow steps:\n- Extract weight value from string.\n- Convert *Weight* column to float type.\n- Get rid of null values in weight column, use median column value instead of them.\n- Convert pounds to kilograms\n- Finally use method *hist* for DataFrame *data* with arguments *column=Weight* (we look at this feature distribution)"}}