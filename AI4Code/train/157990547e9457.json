{"cell_type":{"c7f8e021":"code","7c9c6a87":"code","51758080":"code","16cc0ce9":"code","73368765":"code","bc162538":"code","6bc88cdb":"code","24a03545":"code","36d5d876":"code","d5bfe258":"code","7720c61f":"code","9a53cc9b":"code","2cb279b8":"code","61a34b01":"code","17c74a19":"code","96f9a2ae":"code","d88afb28":"code","caeceaae":"code","41fc3a97":"code","b31ac44d":"code","33106c2b":"code","c7592c46":"code","f209c5d3":"code","1657ac51":"code","f325f025":"code","5c66d542":"code","9c0b4509":"code","4cd32b84":"code","70b11fc9":"code","e853c8cf":"code","ee0c7410":"code","9b35635a":"code","eee6a20d":"code","95a15395":"code","ed24878f":"code","1436c925":"code","fc9e7fec":"code","d13ff9bb":"code","234cc30c":"code","9792609f":"code","99771726":"code","cab582cf":"code","aa44616d":"code","5575ba09":"code","826b857b":"code","8dc1b1f8":"code","7a4efcae":"code","e08fcba3":"code","989ab377":"code","78139cfd":"code","7ff0259a":"code","8dbddb6f":"code","9445047e":"code","a70339fa":"code","f921317b":"code","b0672125":"code","8d5e996e":"code","f54e2ba1":"code","7036683d":"code","880259a3":"code","84eb8f62":"code","eee79b41":"code","7802d423":"code","784b805a":"code","0aaeaff9":"code","2ba42a30":"markdown","b61cf153":"markdown","88b61d4d":"markdown","4614c7d1":"markdown","e3e738f3":"markdown","74ceb973":"markdown","274a41bc":"markdown","3d8497ea":"markdown","b0f10ba5":"markdown","e667953e":"markdown","90e85bf0":"markdown","f4617dcf":"markdown","3809d918":"markdown","2d8cd834":"markdown","ee564b7c":"markdown","890a5b2f":"markdown","32f14613":"markdown","0801517d":"markdown","476ea7ad":"markdown","5733f162":"markdown","c044ec2d":"markdown","60970843":"markdown","5b66501f":"markdown","38fe44a1":"markdown","5e16818f":"markdown","996c3ff5":"markdown","5418e40b":"markdown","da6cf0ec":"markdown","bb04540f":"markdown","b88a5a39":"markdown","d6f7e188":"markdown","912ecc40":"markdown","9662b134":"markdown","ebd13c94":"markdown","1018b40d":"markdown","308fc5ba":"markdown","ce98196a":"markdown","eca604c1":"markdown","163ca75f":"markdown","4e132ea2":"markdown","4e23eb78":"markdown"},"source":{"c7f8e021":"import random\nfrom tensorflow.keras import regularizers\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression \nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout, Bidirectional, GRU\nimport warnings\nwarnings.filterwarnings(\"ignore\")","7c9c6a87":"tf.random.set_seed(1234)","51758080":"def downcastMemoryUsage(dataFrame):\n    startMemoryOptimization = dataFrame.memory_usage().sum() \/ 1024 ** 2\n    print('Memory usage of dataframe is: {:.2f} MB'.format(startMemoryOptimization))\n    subTypeInt = ['uint8','uint16','uint32','uint64','int8','int16','int32','int64']\n    subTypeFloat = ['float16','float32','float64']\n    for column in dataFrame.columns:\n        columnType = str(dataFrame[column].dtypes)\n        maximumColumn = dataFrame[column].max()\n        minimumColumn = dataFrame[column].min()\n        if 'int' in columnType:\n            for element in subTypeInt:\n                if minimumColumn > np.iinfo(element).min and maximumColumn < np.iinfo(element).max:\n                    dataFrame[column] = dataFrame[column].astype(element)\n                    break\n        elif 'float' in columnType:\n            for element in subTypeFloat:\n                if minimumColumn > np.finfo(element).min and maximumColumn < np.finfo(element).max:\n                    dataFrame[column] = dataFrame[column].astype(element)\n                    break\n        elif 'object' in columnType:\n            if column =='date':\n                dataFrame['date'] = pd.to_datetime(dataFrame['date'],format='%Y-%m-%d')\n            else:\n                numberOfUnique = len(dataFrame[column].unique())\n                numberOfTotal = len(dataFrame[column])\n                if numberOfUnique \/ numberOfTotal < 0.5:\n                    dataFrame[column] = dataFrame[column].astype('category')\n    endMemoryOptimization = dataFrame.memory_usage().sum() \/ 1024 ** 2\n    print('Memory usage after optimization is: {:.2f} MB'.format(endMemoryOptimization))\n    print('Compressed by: {:.2f} %'.format(100*(startMemoryOptimization - endMemoryOptimization) \/ startMemoryOptimization))\n    \n    return dataFrame","16cc0ce9":"def basicSummary(dataFrameForSummary):\n    print(f'Shape : {dataFrameForSummary.shape}')\n    summary = pd.DataFrame(dataFrameForSummary.dtypes, columns=['Data Type'])\n    summary = summary.reset_index()\n    summary = summary.rename(columns={'index': 'Feature'})\n    summary['Num of Nulls'] = dataFrameForSummary.isnull().sum().values\n    summary['Num of Unique'] = dataFrameForSummary.nunique().values\n    summary['First Value'] = dataFrameForSummary.loc[0].values\n    summary['Second Value'] = dataFrameForSummary.loc[1].values\n    summary['Third Value'] = dataFrameForSummary.loc[2].values\n    summary['Fourth Value'] = dataFrameForSummary.loc[3].values\n    summary['Fifth Value'] = dataFrameForSummary.loc[4].values\n    return summary","73368765":"assetDetailsData = pd.read_csv('..\/input\/g-research-crypto-forecasting\/asset_details.csv')\ndowncastMemoryUsage(assetDetailsData)","bc162538":"assetDetailsData.sort_values(by=['Weight'],ascending=False,inplace=True)\n# df[percent] = (df['column_name'] \/ df['column_name'].sum()) * 100\nassetDetailsData['coinWeightPercent'] = (assetDetailsData['Weight'] \/ assetDetailsData['Weight'].sum()) * 100\nassetDetailsData","6bc88cdb":"fig = plt.figure()\nax = plt.gca()\nbox = ax.get_position()\nax.set_position([box.x0, box.y0, box.width * 5, box.height * 3])\ncolors = sns.color_palette('colorblind')[0:14]\nlabels = ['Bitcoin', 'Ethereum', 'Cardano', 'Binance Coin', 'Dogecoin', 'Bitcoin Cash', 'Litecoin', 'Ethereum Classic',\n          'Stellar', 'TRON', 'Monero', 'EOS.IO', 'IOTA', 'Maker']\nexplode = (0.3, 0.3, 0.2, 0.2, 0.1, 0.1, 0.1, 0.0, 0.1, 0.0, 0.1, 0.0, 0.1, 0.0)\nplt.pie(assetDetailsData['coinWeightPercent'], colors=colors, autopct='%.0f%%', labels=labels, explode=explode,\n        startangle=30, shadow=True, textprops={'fontweight': 'semibold', 'fontsize': 15},\n        wedgeprops={'linewidth': 2, 'edgecolor': 'k'}, labeldistance=1.1)\nplt.title(\"Amount of Weight Each Crypto, Received in the Metric.\", fontweight=\"bold\", fontsize=22, pad=21)\nplt.axis('equal')\nplt.show()","24a03545":"trainData = pd.read_csv('..\/input\/g-research-crypto-forecasting\/train.csv')\ntrainData['DateAndTime'] = pd.to_datetime(trainData['timestamp'], unit='s',utc = True,infer_datetime_format = True,).apply(lambda x:x.tz_convert('Europe\/London'))\ntrainData['Date'] = trainData.DateAndTime.dt.date\ntrainData['Date'] = trainData['Date'].astype('datetime64[ns]')\ntrainData.set_index(['DateAndTime'], inplace=True)\ndowncastMemoryUsage(trainData)","36d5d876":"assetsNamesDictionary = {row[\"Asset_Name\"]:row[\"Asset_ID\"] for x, row in assetDetailsData.iterrows()}\n\nassetNames = ['Bitcoin', 'Ethereum', 'Cardano', 'Binance Coin', 'Dogecoin', 'Bitcoin Cash', 'Litecoin', 'Ethereum Classic',\n          'Stellar', 'TRON', 'Monero', 'EOS.IO', 'IOTA', 'Maker']\n\nfromToListTimes = []\nfor crypto in assetNames:\n    cryptoDataFrame = trainData[trainData[\"Asset_ID\"] == assetsNamesDictionary[crypto]]\n    fromTime = cryptoDataFrame.index[0]\n    toTime = cryptoDataFrame.index[-1]\n    fromToListTimes.append([crypto, fromTime, toTime])\nfromToDataframe = pd.DataFrame(fromToListTimes)\nfromToDataframe.columns = [\"AssetName\", \"StartsFrom\", \"EndsTo\"]\nfromToDataframe","d5bfe258":"trainData.isnull().sum()","7720c61f":"def replace_missing (attribute):\n    return attribute.interpolate(inplace=True)\n\n\nreplace_missing(trainData['VWAP'])\nreplace_missing(trainData['Target'])\n\ntrainData.isnull().sum()","9a53cc9b":"startDate = '2021-01-01'\nendDate = '2021-09-21'\nmask = (trainData['Date'] > startDate) & (trainData['Date'] <= endDate) & (trainData['Asset_ID'] == 1)\nnewTrainData = trainData.loc[mask]\nnewTrainData.head()","2cb279b8":"data_training = newTrainData[newTrainData['Date'] >= '2021-01-01'].copy()","61a34b01":"# Outlier detection\nupperFence = data_training['Close'].mean() + 2*data_training['Close'].std()\nlowwerFence = data_training['Close'].mean() - 2*data_training['Close'].std()\n\n# Replace outlier by interpolation for base consumption\ndata_training.loc[data_training['Close'] > upperFence, 'Close'] = np.nan\ndata_training.loc[data_training['Close'] < lowwerFence, 'Close'] = np.nan\ndata_training['Close'].interpolate(inplace=True)","17c74a19":"# Split train data and test data\ntrain_size = int(len(data_training)*0.8)\ntrain_dataset, test_dataset = data_training.iloc[:train_size],data_training.iloc[train_size:]","96f9a2ae":"fig, ax = plt.subplots(figsize = (20,10))\nax.plot(train_dataset.Close,color=\"#004C99\")\nax.plot(test_dataset.Close,color=\"#D96552\")\nax.set_facecolor(\"#D3D3D3\")\nplt.grid(b=True,axis = 'y')\nax.grid(b=True,axis = 'y')\nplt.ylabel('USD')\nplt.xlabel('Time')\nplt.legend(['Train set', 'Test set'], loc='upper right',prop={'size': 15})\nprint('Dimension of train data: ',train_dataset.shape)\nprint('Dimension of test data: ', test_dataset.shape)","d88afb28":"# Split train data to X and y\nX_train = train_dataset.drop(['timestamp','Asset_ID','Count','VWAP','Target','Date'], axis = 1)\ny_train = train_dataset.loc[:,['Close']]\n\n# Split test data to X and y\nX_test = test_dataset.drop(['timestamp','Asset_ID','Count','VWAP','Target','Date'], axis = 1)\ny_test = test_dataset.loc[:,['Close']]","caeceaae":"print(\"X_train Dimensions:\", X_train.shape)\nprint(\"y_train Dimensions:\", y_train.shape)\nprint(\"X_test Dimensions:\", X_test.shape)\nprint(\"y_test Dimensions:\", y_test.shape)","41fc3a97":"# MinMaxScaler is used to normalize the data\nscaler = MinMaxScaler()\n\n# Apply the scaler to training data\nX_train = scaler.fit_transform(X_train)\ny_train = scaler.fit_transform(y_train)\n\n# Apply the scaler to test data\nX_test = scaler.fit_transform(X_test)\ny_test = scaler.fit_transform(y_test)","b31ac44d":"# Create a 3D input for Scikit-Learn\ndef create_dataset (X, y, time_steps = 1):\n    Xs, ys = [], []\n    for i in range(len(X)-time_steps):\n        v = X[i:i+time_steps, :]\n        Xs.append(v)\n        ys.append(y[i+time_steps])\n    return np.array(Xs), np.array(ys)\nTIME_STEPS = 30\nX_test, y_test = create_dataset(X_test, y_test, TIME_STEPS)\nX_train, y_train = create_dataset(X_train, y_train,TIME_STEPS)\n\nprint(\"X_train Dimensions:\", X_train.shape)\nprint(\"y_train Dimensions:\", y_train.shape)\nprint(\"X_test Dimensions:\", X_test.shape)\nprint(\"y_test Dimensions:\", y_test.shape)","33106c2b":"# Create BiLSTM model\ndef create_model_bilstm(units):\n    model = Sequential()\n    model.add(Bidirectional(LSTM(units = units,return_sequences=True),input_shape=(X_train.shape[1], X_train.shape[2])))\n    model.add(Dropout(0.5))\n    model.add(Bidirectional(LSTM(units = units)))\n    model.add(Dense(1,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n    #Compile model\n    model.compile(loss='mse', optimizer='adam')\n    return model\n\n# Create LSTM or GRU model\ndef create_model(units, m):\n    model = Sequential()\n    model.add(m (units = units, return_sequences = True,input_shape = [X_train.shape[1], X_train.shape[2]]))\n    model.add(Dropout(0.5))\n    model.add(m (units = units))\n    model.add(Dropout(0.5))\n    model.add(Dense(units = 1,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n    #Compile model\n    model.compile(loss='mse', optimizer='adam')\n    return model\n\n# BiLSTM\nmodel_bilstm = create_model_bilstm(4)\n\n# GRU and LSTM\nmodel_gru = create_model(16, GRU)\nmodel_lstm = create_model(32, LSTM)","c7592c46":"# Fit BiLSTM, LSTM and GRU\ndef fit_model(model):\n    early_stop = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',\n                                               patience = 10)\n    history = model.fit(X_train, y_train, epochs = 100,  \n                        validation_split = 0.2, batch_size = 1024, \n                        shuffle = False, callbacks = [early_stop])\n    return history","f209c5d3":"history_bilstm = fit_model(model_bilstm)","1657ac51":"# history_lstm = fit_model(model_lstm)","f325f025":"# history_gru = fit_model(model_gru)","5c66d542":"# # Plot train loss and validation loss\n# def plot_loss (history):\n#     fig, ax = plt.subplots(figsize = (20,10))\n#     ax.plot(history.history['loss'],color=\"#004C99\")\n#     ax.plot(history.history['val_loss'],color=\"#D96552\")\n#     ax.set_facecolor(\"#D3D3D3\")\n#     plt.grid(b=True,axis = 'y')\n#     ax.grid(b=True,axis = 'y')\n#     plt.ylabel('Loss')\n#     plt.xlabel('epoch')\n#     plt.legend(['Train loss', 'Validation loss'], loc='upper right',prop={'size': 15})","9c0b4509":"# plot_loss (history_bilstm)","4cd32b84":"# plot_loss (history_lstm)","70b11fc9":"# plot_loss (history_gru)","e853c8cf":"y_test = scaler.inverse_transform(y_test)\ny_train = scaler.inverse_transform(y_train)","ee0c7410":"\nimport gresearch_crypto\nenv = gresearch_crypto.make_env()\niter_test = env.iter_test()\ndef prediction(model):\n    for (test_df, sample_prediction_df) in iter_test:\n        sample_prediction_df['Target'] = model.predict(X_test)\n        env.predict(sample_prediction_df)\n\n\n# # Make prediction\n# def prediction(model):\n#     prediction = model.predict(X_test)\n#     prediction = scaler.inverse_transform(prediction)\n#     return prediction\n\n\n# prediction_bilstm = prediction(model_bilstm)\n# prediction_lstm = prediction(model_lstm)\n# prediction_gru = prediction(model_gru)","9b35635a":"# # Plot true future vs prediction\n# def plot_future(prediction, y_test):\n#     fig, ax = plt.subplots(figsize = (20,10))\n#     range_future = len(prediction)\n#     ax.plot(np.arange(range_future), np.array(y_test),label='Actual',color=\"#004C99\")\n#     ax.plot(np.arange(range_future),np.array(prediction),label='Prediction',color=\"#D96552\")\n#     ax.set_facecolor(\"#D3D3D3\")\n#     plt.grid(b=True,axis = 'y')\n#     ax.grid(b=True,axis = 'y')\n#     plt.ylabel('USD')\n#     plt.legend(loc='upper left',prop={'size': 15})\n#     plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)    ","eee6a20d":"# plot_future(prediction_bilstm, y_test)","95a15395":"# plot_future(prediction_lstm, y_test)","ed24878f":"# plot_future(prediction_gru, y_test)","1436c925":"# # Define a function to calculate MAE and RMSE\n# def evaluate_prediction(predictions, actual, model_name):\n#     errors = predictions - actual\n#     mse = np.square(errors).mean()\n#     rmse = np.sqrt(mse)\n#     mae = np.abs(errors).mean()\n#     print(model_name + ':')\n#     print('Mean Absolute Error: {:.4f}'.format(mae))\n#     print('Root Mean Square Error: {:.4f}'.format(rmse))\n#     print('')\n    \n    \n# evaluate_prediction(prediction_bilstm, y_test, 'Bidirectional LSTM')\n# evaluate_prediction(prediction_lstm, y_test, 'LSTM')\n# evaluate_prediction(prediction_gru, y_test, 'GRU')","fc9e7fec":"# # Import new CRYPTO data \n# newinput = pd.read_csv('new.csv', parse_dates=['Date'], index_col = 'Date')\n\n# # Order of the variable are important\n# X_new = newinput.loc['2022-01-01':'2032-01-01',:] \n# X_new","d13ff9bb":"## Plot histoy and future data\n# def plot_history_future(y_train, prediction):\n#     fig, ax = plt.subplots(figsize = (20,10))\n#     range_history = len(y_train)\n#     range_future = list(range(range_history, range_history + len(prediction)))\n#     ax.plt.plot(np.arange(range_history), np.array(y_train),label='History',color=\"#004C99\")\n#     ax.plt.plot(range_future, np.array(prediction),label='Prediction',color=\"#D96552\")\n#     ax.set_facecolor(\"#D3D3D3\")\n#     plt.grid(b=True,axis = 'y')\n#     ax.grid(b=True,axis = 'y')\n#     plt.ylabel('USD')\n#     plt.legend(loc='upper left',prop={'size': 15})\n#     plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)","234cc30c":"# # Multi-step forecasting \n# def forecast(X_input, time_steps):\n#     X = X_train.transform(X_input)\n#     Xs = []\n#     for i in range(len(X) - time_steps):\n#         v = X[i:i+time_steps, :]\n#         Xs.append(v)\n        \n#     X_transformed = np.array(Xs)\n    \n#     prediction = model_bilstm.predict(X_transformed)\n#     prediction_actual = scaler.inverse_transform(prediction)\n#     return prediction_actual\n\n# prediction = forecast(X_new, TIME_STEPS)\n# plot_history_future(y_train, prediction)","9792609f":"trainData.head()","99771726":"# startDateEthereum = '2021-01-01'\n# endDateEthereum = '2021-09-21'\n# maskEthereum  = (trainData['Date'] > startDateEthereum) & (trainData['Date'] <= endDateEthereum) & (trainData['Asset_ID'] == 6)\n# newTrainDataEthereum  = trainData.loc[maskEthereum]\n# newTrainDataEthereum .head()","cab582cf":"# dataTrainingEthereum = newTrainDataEthereum[newTrainDataEthereum['Date'] >= '2021-01-01'].copy()","aa44616d":"# # Outlier detection\n# upperFenceEthereum = dataTrainingEthereum['Close'].mean() + 2*dataTrainingEthereum['Close'].std()\n# lowwerFenceEthereum = dataTrainingEthereum['Close'].mean() - 2*dataTrainingEthereum['Close'].std()\n\n# # Replace outlier by interpolation for base consumption\n# dataTrainingEthereum.loc[dataTrainingEthereum['Close'] > upperFenceEthereum, 'Close'] = np.nan\n# dataTrainingEthereum.loc[dataTrainingEthereum['Close'] < lowwerFenceEthereum, 'Close'] = np.nan\n# dataTrainingEthereum['Close'].interpolate(inplace=True)","5575ba09":"# # Split train data and test data\n# train_size_ethereum = int(len(dataTrainingEthereum)*0.8)\n# train_dataset_ethereum, test_dataset_ethereum = dataTrainingEthereum.iloc[:train_size_ethereum],dataTrainingEthereum.iloc[train_size_ethereum:]","826b857b":"# fig, ax = plt.subplots(figsize = (20,10))\n# ax.plot(train_dataset_ethereum.Close,color=\"#004C99\")\n# ax.plot(test_dataset_ethereum.Close,color=\"#D96552\")\n# ax.set_facecolor(\"#D3D3D3\")\n# plt.grid(b=True,axis = 'y')\n# ax.grid(b=True,axis = 'y')\n# plt.ylabel('USD')\n# plt.xlabel('Time')\n# plt.legend(['Train set', 'Test set'], loc='upper right',prop={'size': 15})\n# print('Dimension of train data: ',train_dataset.shape)\n# print('Dimension of test data: ', test_dataset.shape)","8dc1b1f8":"# # Split train data to X and y\n# xTrainEthereum = train_dataset_ethereum.drop(['timestamp','Asset_ID','Count','VWAP','Target','Date'], axis = 1)\n# yTrainEthereum = train_dataset_ethereum.loc[:,['Close']]\n\n# # Split test data to X and y\n# xTestEthereum = test_dataset_ethereum.drop(['timestamp','Asset_ID','Count','VWAP','Target','Date'], axis = 1)\n# yTestEthereum = test_dataset_ethereum.loc[:,['Close']]\n\n# # #output\n# # yEthereumOutput= test_dataset_ethereum[['Close']]\n \n# # #input\n# # xEthereumInput=test_dataset_ethereum.drop(['timestamp','Asset_ID','Count','VWAP','Target','Date'],axis=1)\n\n# # #splitting\n# # xTrainEthereum,xTestEthereum,yTrainEthereum,yTestEthereum=train_test_split(xEthereumInput,yEthereumOutput,test_size=0.2)","7a4efcae":"# #printing shapes of testing and training sets :\n# print(\"shape of original dataset :\", dataTrainingEthereum.shape)\n# print(\"shape of input - training set\", xTrainEthereum.shape)\n# print(\"shape of output - training set\", yTrainEthereum.shape)\n# print(\"shape of input - testing set\", xTestEthereum.shape)\n# print(\"shape of output - testing set\", yTestEthereum.shape)","e08fcba3":"# # MinMaxScaler is used to normalize the data\n# scaler = MinMaxScaler()\n\n# # Apply the scaler to training data\n# xTrainEthereum = scaler.fit_transform(xTrainEthereum)\n# yTrainEthereum = scaler.fit_transform(yTrainEthereum)\n\n# # Apply the scaler to test data\n# xTestEthereum = scaler.fit_transform(xTestEthereum)\n# yTestEthereum = scaler.fit_transform(yTestEthereum)","989ab377":"# # Create a 3D input for Scikit-Learn\n# def create_dataset_ethereum (X, y, time_steps = 1):\n#     Xs, ys = [], []\n#     for i in range(len(X)-time_steps):\n#         v = X[i:i+time_steps, :]\n#         Xs.append(v)\n#         ys.append(y[i+time_steps])\n#     return np.array(Xs), np.array(ys)\n# TIME_STEPS = 30\n# xTestEthereum, yTestEthereum = create_dataset_ethereum(xTestEthereum, yTestEthereum, TIME_STEPS)\n# xTrainEthereum, yTrainEthereum = create_dataset_ethereum(xTrainEthereum, yTrainEthereum,TIME_STEPS)\n\n# print(\"X_train Dimensions:\", xTrainEthereum.shape)\n# print(\"y_train Dimensions:\", yTrainEthereum.shape)\n# print(\"X_test Dimensions:\", xTestEthereum.shape)\n# print(\"y_test Dimensions:\", yTestEthereum.shape)","78139cfd":"# # Create BiLSTM model\n# def create_model_bilstm_ethereum(units):\n#     model = Sequential()\n#     model.add(Bidirectional(LSTM(units = units,return_sequences=True),input_shape=(xTrainEthereum.shape[1], xTrainEthereum.shape[2])))\n# #     model.add(Dropout(0.5))\n#     model.add(Bidirectional(LSTM(units = units)))\n#     model.add(Dense(1))\n#     #Compile model\n#     model.compile(loss='mse', optimizer='adam')\n#     return model\n\n# # Create LSTM or GRU model\n# def create_model_ethereum(units, m):\n#     model = Sequential()\n#     model.add(m (units = units, return_sequences = True,input_shape = [xTrainEthereum.shape[1], xTrainEthereum.shape[2]]))\n#     model.add(Dropout(0.5))\n#     model.add(m (units = units))\n#     model.add(Dropout(0.5))\n#     model.add(Dense(units = 1,activation='relu',kernel_regularizer=keras.regularizers.l2(0.01)))\n#     #Compile model\n#     model.compile(loss='mse', optimizer='adam')\n#     return model\n\n# # BiLSTM\n# model_bilstm_ethereum = create_model_bilstm_ethereum(4)\n\n# # GRU and LSTM\n# model_gru_ethereum = create_model_ethereum(16, GRU)\n# model_lstm_ethereum = create_model_ethereum(16, LSTM)","7ff0259a":"# # Fit BiLSTM, LSTM and GRU\n# def fit_model_ethereum(model):\n#     early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss',\n#                                                patience = 10)\n#     historyEthereum = model.fit(xTrainEthereum, yTrainEthereum, epochs = 100,  \n#                         validation_split = 0.2, batch_size = 1024, \n#                         shuffle = False, callbacks = [early_stop])\n#     return historyEthereum","8dbddb6f":"# history_bilstm_ethereum = fit_model_ethereum(model_bilstm_ethereum)","9445047e":"# history_lstm_ethereum = fit_model_ethereum(model_lstm_ethereum)","a70339fa":"# history_gru_ethereum = fit_model_ethereum(model_gru_ethereum)","f921317b":"# # Plot train loss and validation loss\n# def plot_loss_ethereum (historyEthereum):\n#     fig, ax = plt.subplots(figsize = (20,10))\n#     ax.plot(historyEthereum.history['loss'],color=\"#004C99\")\n#     ax.plot(historyEthereum.history['val_loss'],color=\"#D96552\")\n#     ax.set_facecolor(\"#D3D3D3\")\n#     plt.grid(b=True,axis = 'y')\n#     ax.grid(b=True,axis = 'y')\n#     plt.ylabel('Loss')\n#     plt.xlabel('epoch')\n#     plt.legend(['Train loss', 'Validation loss'], loc='upper right',prop={'size': 15})","b0672125":"# plot_loss_ethereum (history_bilstm_ethereum)","8d5e996e":"# plot_loss_ethereum (history_lstm_ethereum)","f54e2ba1":"# plot_loss_ethereum (history_gru_ethereum)","7036683d":"# yTestEthereum = scaler.inverse_transform(yTestEthereum)\n# yTrainEthereum = scaler.inverse_transform(yTrainEthereum)","880259a3":"# # Make prediction\n# def predictionEthereum(model):\n#     predictionEthereum = model.predict(xTestEthereum)\n#     predictionEthereum = scaler.inverse_transform(predictionEthereum)\n#     return predictionEthereum\n\n\n# prediction_bilstm_ethereum = predictionEthereum(model_bilstm_ethereum)\n# prediction_lstm_ethereum = predictionEthereum(model_lstm_ethereum)\n# prediction_gru_ethereum = predictionEthereum(model_gru_ethereum)","84eb8f62":"# # Plot true future vs prediction\n# def plot_future_ethereum(predictionEthereum, yTestEthereum):\n#     fig, ax = plt.subplots(figsize = (20,10))\n#     range_future_ethereum = len(predictionEthereum)\n#     ax.plot(np.arange(range_future_ethereum), np.array(yTestEthereum),label='Actual',color=\"#004C99\")\n#     ax.plot(np.arange(range_future_ethereum),np.array(predictionEthereum),label='Prediction',color=\"#D96552\")\n#     ax.set_facecolor(\"#D3D3D3\")\n#     plt.grid(b=True,axis = 'y')\n#     ax.grid(b=True,axis = 'y')\n#     plt.ylabel('USD')\n#     plt.legend(loc='upper left',prop={'size': 15})\n#     plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)    ","eee79b41":"# plot_future_ethereum(prediction_bilstm_ethereum, yTestEthereum)","7802d423":"# plot_future_ethereum(prediction_lstm_ethereum, yTestEthereum)","784b805a":"# plot_future_ethereum(prediction_gru_ethereum, yTestEthereum)","0aaeaff9":"# # Define a function to calculate MAE and RMSE\n# def evaluate_prediction_ethereum(predictions, actual, model_name):\n#     errors = predictions - actual\n#     mse = np.square(errors).mean()\n#     rmse = np.sqrt(mse)\n#     mae = np.abs(errors).mean()\n#     print(model_name + ':')\n#     print('Mean Absolute Error: {:.4f}'.format(mae))\n#     print('Root Mean Square Error: {:.4f}'.format(rmse))\n#     print('')\n    \n    \n# evaluate_prediction_ethereum(prediction_bilstm_ethereum, yTestEthereum, 'Bidirectional LSTM')\n# evaluate_prediction_ethereum(prediction_lstm_ethereum, yTestEthereum, 'LSTM')\n# evaluate_prediction_ethereum(prediction_gru_ethereum, yTestEthereum, 'GRU')","2ba42a30":"<a id=\"19\"><\/a> <br>\n# Bi-LSTM, LSTM and GRUs models","b61cf153":"<a id=\"32\"><\/a> <br>\n# Actual vs Prediction Plot for Long Short-Term Memory","88b61d4d":"**<p id=\"24\"><li>Train and Validation Loss Plot Function<\/p>**","4614c7d1":"<a id=\"20\"><\/a> <br>\n# Fit the Models","e3e738f3":"**<p id=\"7\">Import, Read, and Convert Train Data into the Proper Format.<\/p>**\n<p><li>Convert \"timestamp\" to DateTime[s]<\/li><\/p>","74ceb973":"**<p id=\"34\">Calculate RMSE and MAE for Performance<\/p>**","274a41bc":"# **<center>Predictive Analytics: Bidirectional Long Short-Term Memory(Bi-LSTM), Long Short-Term Memory (LSTM), Gated Recurrent Units (GRUs)<\/center>**","3d8497ea":"<a id=\"11\"><\/a> <br>\n# Data Preprocessing","b0f10ba5":"**<p id=\"23\">Gated Recurrent Units Model History<\/p>**","e667953e":"<a id=\"16\"><\/a> <br>\n# Data Transformation","90e85bf0":"**<p id=\"12\">Replace Outliers<\/p>**\n<p><li>Outliers are detected using statistical approaches. The statistical approaches presume that the data points are distributed in a normal manner. Outliers are values that fall outside of a low probability area.<\/li><\/p>\n<p><li>In statistical methods, I use the concept of maximum likelihood, which means that results beyond the range of \u03bc\u00b12\u03c3  are labeled as outliers. Under the assumption of normal distribution, number \u03bc\u00b12\u03c3  contains 95% of the data.<\/li><\/p>","f4617dcf":"<a id=\"2\"><\/a> <br>\n# Universal Downcasting Function for Dataframes\n\n<p>For Memory Optimizaton and Utilization.<\/p>","3809d918":"**<p id=\"29\">BiLSTM, LSTM and GRU Models Predictions<\/p>**","2d8cd834":"<a id=\"6\"><\/a> <br>\n# Plotting the Amount of Weight Each Crypto, Received in the Metric.","ee564b7c":"**<p id=\"17\">Normalization<\/p>**","890a5b2f":"<a id=\"3\"><\/a> <br>\n# Universal Basic Summary Function for Dataframes","32f14613":"**<p id=\"4\">Importing and Reading Asset Data<\/p>**\n<p><li>Provides the real name and of the cryptoasset for each Asset_ID and the weight each cryptoasset receives in the metric.<\/li><\/p>","0801517d":"**<p id=\"5\"><li>Calculate Percentage Weight for Each Coin.<\/li><\/p>**","476ea7ad":"<a id=\"14\"><\/a> <br>\n# Train and Test Data Set Plot","5733f162":"<a id=\"35\"><\/a> <br>\n# Future Forecasting","c044ec2d":"**<p id=\"10\">Handling the Null Values<\/p>**","60970843":"**<p id=\"9\">Checking Null Values<\/p>**","5b66501f":"**<p id=\"21\">Bidirectional Long Short-Term Memory Model History<\/p>**","38fe44a1":"<a id=\"TrainData\"><\/a> <br>\n# <center>Train Data<\/center>","5e16818f":"**<p id=\"1\">Set Seed<\/p>**\n<p><li>Set global seed but the operation seed is not set here, we get different results for every call to the random op, but the same sequence for every re-run of the program.<\/li><\/p>","996c3ff5":"**<p id=\"18\">Create a 3D input dataset for Sk-Learn<\/p>**","5418e40b":"**<p id=\"13\">Spliting the dataset into train and test data.<\/p>**\n<p><li>I configured the first 80% of the 2021 data as train data and the remaining 20% as test data. I have used train data to train the model and test data to validate its performance.<\/li><\/p>","da6cf0ec":"<a id=\"31\"><\/a> <br>\n# Actual vs Prediction Plot for Bidirectional Long Short-Term Memory","bb04540f":"<a id=\"AssetData\"><\/a> <br>\n# <center>Assets Data<\/center>","b88a5a39":"<a id=\"8\"><\/a> <br>\n# Histroy of CRYPTO Data from 2018-2021(Sep)","d6f7e188":"**<p id=\"22\">Long Short-Term Memory Model History<\/p>**","912ecc40":"<a id=\"33\"><\/a> <br>\n# Actual vs Prediction Plot for Gated Recurrent Units","9662b134":"<a id=\"0\"><\/a> <br>\n# Importing Libraries","ebd13c94":"**<p id=\"28\">Inverse and transform the target variable<\/p>**","1018b40d":"**<p id=\"15\">Separating the target and dependent variables.<\/p>**","308fc5ba":"<a id=\"26\"><\/a> <br>\n# Train\/Validation loss Plot for Long Short-Term Memory","ce98196a":"<a id=\"TOC\"><\/a> <br>\n# <center>Table of Contents<\/center>\n\n* [Importing Libraries](#0)\n* [Set Seed](#1)\n* [Universal Downcasting Function for Dataframes](#2)\n* [Universal Basic Summary Function for Dataframes](#3)\n\n<h4>Asset Details<\/h4>\n\n* [Importing and Reading Asset Data](#4)\n* [Calculate Percentage Weight for Each Coin.](#5)\n* [Plotting the Amount of Weight Each Crypto, Received in the Metric.](#6)\n\n<h4>Train Data<\/h4>\n\n* [Import, read, and convert Train Data into the proper format.](#7)\n* [Histroy of CRYPTO Data from 2018-2021(Sep)](#8)\n* [Checking Null Values](#9)\n* [Handling the Null values](#10)\n* [Data Preprocessing](#11)\n* [Replace Otliers](#12)\n* [Spliting the dataset into train and test data](#13)\n* [Train and Test Data Set Plot](#14)\n* [Separating the target and dependent variables](#15)\n* [Data Transformation](#16)\n* [Normalization](#17)\n* [Create a 3D input dataset for Sk-Learn](#18)\n* [Bi-LSTM, LSTM and GRUs models](#19)\n* [Fit the Models](#20)\n* [Bidirectional Long Short-Term Memory Model History](#21)\n* [Long Short-Term Memory Model History](#22)\n* [Gated Recurrent Units Model History](#23)\n* [Train and Validation Loss Plot Function](#24)\n* [Train\/Validation loss Plot for Bidirectional Long Short-Term Memory ](#25)\n* [Train\/Validation loss Plot for Long Short-Term Memory](#26)\n* [Train\/Validation loss Plot for Gated Recurrent Units](#27)\n* [Inverse and transform the target variable](#28)\n* [BiLSTM, LSTM and GRU Models Predictions](#29)\n* [Actual vs Prediction Plot Function](#30)\n* [Actual vs Prediction Plot for Bidirectional Long Short-Term Memory](#31)\n* [Actual vs Prediction Plot for Long Short-Term Memory](#32)\n* [Actual vs Prediction Plot for Gated Recurrent Units](#33)\n* [Calculate RMSE and MAE for Performance](#34)\n* [Future Forecasting](#35)","eca604c1":"**<p>A copy of an existing list. The copy() method is added to the end of a list object and so it does not accept any parameters.<\/p>**","163ca75f":"**<p id=\"30\">Actual vs Prediction Plot Function<\/p>**","4e132ea2":"<a id=\"27\"><\/a> <br>\n# Train\/Validation loss Plot for Gated Recurrent Units","4e23eb78":"<a id=\"25\"><\/a> <br>\n# Train\/Validation loss Plot for Bidirectional Long Short-Term Memory (Good Fit)"}}