{"cell_type":{"8355f6ee":"code","1fec05fd":"code","091f0951":"code","45a0c379":"code","bf7ef73a":"code","110a248d":"code","d3baeef8":"code","aa51a4c6":"code","3cfad892":"code","dc086733":"code","84312f34":"code","6ed3f26a":"code","3f5891b4":"code","24f1e4ca":"code","af385ebf":"code","4781ed85":"code","74353adf":"code","f81dbde9":"code","bae06c33":"code","8fd75f7a":"code","a3944f39":"code","6bfcfa5d":"code","b0ad37fc":"code","b9231769":"code","c07b53d2":"code","896981a1":"code","4c9571f5":"code","79d929b9":"code","337f2086":"code","7e2ee1ed":"code","0cf36faa":"code","f017f7f2":"code","92273edf":"code","6e7f4777":"code","5ad51672":"code","0138483e":"code","88b8b2a2":"code","c12f02b4":"code","d500a5b0":"code","619e56eb":"code","716c167c":"code","3d77fd9d":"code","4ccdd9d8":"code","0153200f":"code","396148b1":"code","9382f186":"code","f84e7f11":"code","b8b76f84":"code","363b3776":"code","10f248c8":"code","d3fd7c67":"code","fce5be42":"code","e249e732":"code","748ee3df":"code","df37622c":"code","6e09741e":"code","12d00f66":"code","493d57c9":"code","b3a0ab6f":"code","d2dc6d80":"code","01b0e79f":"code","0958ba0d":"code","b2896c6c":"code","0eb95aa7":"code","0bec6586":"code","8d7ee054":"code","d980845e":"code","a2e112b8":"code","ba48e946":"code","73e267d3":"code","d2209455":"code","8c6747e2":"code","9eef49fb":"code","3f99cf3a":"code","7404198a":"code","2457c2e6":"code","b1f45a90":"code","12ba34d7":"code","d16342d0":"code","cd9934fc":"code","9dfeec46":"code","9d003171":"code","885fa597":"code","318bbc4b":"code","9c10f465":"code","a04119d6":"code","dc8a8e37":"code","f1423059":"markdown","567d1240":"markdown","732c05d0":"markdown","b867ab2b":"markdown","70e2db2c":"markdown","d0915e2b":"markdown","0591ed40":"markdown","a42c366c":"markdown","3956d264":"markdown","a2780005":"markdown","a267d534":"markdown","12a08b49":"markdown","82f9e334":"markdown","f1a16003":"markdown","ed9f3cfe":"markdown","cf9ef054":"markdown","60ccb354":"markdown","053c8462":"markdown","39a6625e":"markdown","8ba0f190":"markdown","0f84210b":"markdown","c66246c7":"markdown","e62a9f27":"markdown","3afafc35":"markdown","548d1f32":"markdown","9fe621b2":"markdown","5a589b40":"markdown","38d23015":"markdown","7d6a173a":"markdown"},"source":{"8355f6ee":"# Data Wrangling \nimport numpy as np\nimport pandas as pd \n\n# Data Visualisation \n%matplotlib inline\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n# Machine Learning \nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron \nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error","1fec05fd":"train_data = pd.read_csv('..\/input\/train (1).csv')\ntest_data = pd.read_csv('..\/input\/test (1).csv')\ncombine = [train_data, test_data]","091f0951":"train_data.head()","45a0c379":"test_data.head()","bf7ef73a":"train_data.describe()","110a248d":"train_data.describe(percentiles = [.08, .07, .06])","d3baeef8":"plt.figure(figsize = (15, 6))\nsns.heatmap(train_data.corr(), annot = True)\nplt.show()","aa51a4c6":"train_data.isnull().sum()","3cfad892":"test_data.isnull().sum()","dc086733":"for dataset in combine: \n    dataset['age'] = dataset['age_in_days']\/\/365\n    dataset.drop(['age_in_days'], axis = 1, inplace = True)\ntrain_data.head()","84312f34":"train_data[['sourcing_channel', 'target']].groupby('sourcing_channel', as_index = False).mean()","6ed3f26a":"train_data['IncomeBands'] = pd.cut(train_data['Income'], 5)\ntrain_data[['IncomeBands', 'target']].groupby('IncomeBands', as_index = False).count()","3f5891b4":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nscaler = MinMaxScaler()\nscaler = scaler.fit(train_data[['Income']])\nx_scaled = scaler.transform(train_data[['Income']])\nx_scaled","24f1e4ca":"# print(scaler.mean_)\nprint(scaler.scale_)","af385ebf":"train_data['scaled_income'] = x_scaled\ntrain_data.head()","4781ed85":"train_data['IncomeBands'] = pd.cut(train_data['scaled_income'], 5)\ntrain_data[['IncomeBands', 'target']].groupby('IncomeBands', as_index = False).count()","74353adf":"print(train_data['Income'].mean())\nprint(train_data['Income'].median())","f81dbde9":"plt.hist(train_data['Income'])\nplt.show()","bae06c33":"upper_bound = 0.95\nlower_bound = 0.1\nres = train_data['Income'].quantile([lower_bound, upper_bound])\nprint(res)","8fd75f7a":"true_index = (train_data['Income'] < res.loc[upper_bound])\ntrue_index","a3944f39":"false_index = ~true_index","6bfcfa5d":"no_outlier_data = train_data[true_index].copy()\nno_outlier_data.head()","b0ad37fc":"no_outlier_data['IncomeBands'] = pd.cut(no_outlier_data['Income'], 5)\nno_outlier_data[['IncomeBands', 'target']].groupby('IncomeBands', as_index = False).count()","b9231769":"combine = [train_data, test_data]\nfor dataset in combine: \n    dataset.loc[ dataset['Income'] <= 23603.99, 'Income'] = 0\n    dataset.loc[(dataset['Income'] > 23603.99) & (dataset['Income'] <= 109232.0), 'Income'] = 1\n    dataset.loc[(dataset['Income'] > 109232.0) & (dataset['Income'] <= 194434.0), 'Income'] = 2\n    dataset.loc[(dataset['Income'] > 194434.0) & (dataset['Income'] <= 279636.0), 'Income'] = 3\n    dataset.loc[(dataset['Income'] > 279636.0) & (dataset['Income'] <= 364838.0), 'Income'] = 4\n    dataset.loc[(dataset['Income'] > 364838.0) & (dataset['Income'] <= 450040.0), 'Income'] = 5\n    dataset.loc[ dataset['Income'] > 450040.0, 'Income'] = 6\n    \ntrain_data.head()","c07b53d2":"train_data.loc[false_index, 'Income'] = 5\ntrain_data.head()","896981a1":"train_data.drop(['IncomeBands', 'scaled_income'], axis = 1, inplace = True)\ntrain_data.head()","4c9571f5":"train_data['AgeBands'] = pd.cut(train_data['age'], 5)\ntrain_data[['AgeBands', 'target']].groupby('AgeBands', as_index = False).count()","79d929b9":"for dataset in combine:    \n    dataset.loc[ dataset['age'] <= 37.4, 'age'] = 0\n    dataset.loc[(dataset['age'] > 37.4) & (dataset['age'] <= 53.8), 'age'] = 1\n    dataset.loc[(dataset['age'] > 53.8) & (dataset['age'] <= 70.2), 'age'] = 2\n    dataset.loc[(dataset['age'] > 70.2) & (dataset['age'] <= 86.6), 'age'] = 3\n    dataset.loc[ dataset['age'] > 86.6, 'age'] = 4\ntrain_data.drop('AgeBands', axis = 1, inplace = True)\ncombine = [train_data, test_data]\ntrain_data.head()","337f2086":"train_data[['age', 'application_underwriting_score']].groupby('age').mean()","7e2ee1ed":"train_data['PremBand'] = pd.cut(train_data['no_of_premiums_paid'], 5)\ntrain_data[['PremBand', 'application_underwriting_score']].groupby('PremBand').count()","0cf36faa":"print(train_data['application_underwriting_score'].mean())\nprint(train_data['application_underwriting_score'].std())","f017f7f2":"print(train_data[train_data['sourcing_channel'] == 'A']['application_underwriting_score'].std())\ntrain_data[['sourcing_channel', 'target']].groupby('sourcing_channel', as_index = False).mean()","92273edf":"# print(train_data[train_data['sourcing_channel'] == 'C']['application_underwriting_score'].std())\ntrain_data[['sourcing_channel', 'application_underwriting_score']].groupby('sourcing_channel', as_index = False).mean()","6e7f4777":"train_data[['residence_area_type', 'application_underwriting_score']].groupby('residence_area_type', as_index = False).mean()","5ad51672":"train_data.dtypes","0138483e":"combine = [train_data, test_data]\nfor dataset in combine: \n    mask1 = dataset['application_underwriting_score'].isnull()\n    for source in ['A', 'B', 'C', 'D', 'E']:\n        mask2 = (dataset['sourcing_channel'] == source)\n        source_mean = dataset[dataset['sourcing_channel'] == source]['application_underwriting_score'].mean()\n        dataset.loc[mask1 & mask2, 'application_underwriting_score'] = source_mean\ntrain_data.head()","88b8b2a2":"dataset['application_underwriting_score'].isnull()","c12f02b4":"test_data[test_data['Count_3-6_months_late'].isnull()]","d500a5b0":"sns.countplot(x = 'Count_3-6_months_late', data = train_data, hue = 'target')","619e56eb":"sns.countplot(x = 'Count_6-12_months_late', data = train_data, hue = 'target')","716c167c":"combine = [train_data, test_data]\nfor dataset in combine: \n    dataset['late_premium'] = 0.0\ntrain_data.head()","3d77fd9d":"combine = [train_data, test_data]\nfor dataset in combine:\n        dataset.loc[(dataset['Count_3-6_months_late'].isnull()),  'late_premium'] = np.NaN\n        dataset.loc[(dataset['Count_3-6_months_late'].notnull()), 'late_premium'] = dataset['Count_3-6_months_late'] + dataset['Count_6-12_months_late'] + dataset['Count_more_than_12_months_late']\ntrain_data.head() ","4ccdd9d8":"train_data['target'].corr(train_data['late_premium'])","0153200f":"plt.figure(figsize = (15, 6))\nsns.heatmap(test_data.corr(), annot = True)","396148b1":"sns.regplot(x = 'perc_premium_paid_by_cash_credit', y = 'late_premium', data = train_data)","9382f186":"sns.countplot(x = 'late_premium', data = train_data, hue = 'target')","f84e7f11":"train_data[['late_premium', 'target']].groupby('late_premium').mean()","b8b76f84":"# for dataset in [train_data]:\ntrain_data.loc[(train_data['target'] == 0) & (train_data['late_premium'].isnull()),'late_premium'] = 7\ntrain_data.loc[(train_data['target'] == 1) & (train_data['late_premium'].isnull()),'late_premium'] = 2\ntrain_data.head()","363b3776":"print(train_data.isnull().sum())\nprint('\\n')\nprint(test_data.isnull().sum())","10f248c8":"guess_prem = np.zeros(5)\nfor dataset in [test_data]:\n    for i in range(1, 6):\n        guess_df = dataset[(dataset['Income'] == i)]['late_premium'].dropna()\n\n        # age_mean = guess_df.mean()\n        # age_std = guess_df.std()\n        # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n        premium_guess = guess_df.median()\n        guess_prem[i - 1] = int(premium_guess) \n\n    for j in range(1, 6):\n        dataset.loc[(dataset.late_premium.isnull()) & (dataset.Income == j), 'late_premium'] = guess_prem[j - 1] + 1\n\n    dataset['late_premium'] = dataset['late_premium'].astype(int)\n\ntest_data.head(10)","d3fd7c67":"train_data.drop(['Count_3-6_months_late', 'Count_6-12_months_late', 'Count_more_than_12_months_late'], axis = 1, inplace = True)\ntest_data.drop(['Count_3-6_months_late', 'Count_6-12_months_late', 'Count_more_than_12_months_late'], axis = 1, inplace = True)","fce5be42":"# Converting Area Type and sourcing channel to Ordinal Variables\ncombine = [train_data, test_data]\nfor dataset in combine: \n    dataset['residence_area_type'] = dataset['residence_area_type'].map( {'Urban' : 1, 'Rural' : 0} )\n    dataset['sourcing_channel'] = dataset['sourcing_channel'].map( {'A' : 0, 'B' : 1, 'C' : 2, 'D' : 3, 'E' : 4} )\ntrain_data.head()\n","e249e732":"train_data['application_underwriting_score'] = train_data['application_underwriting_score']\/100\ntrain_data.head()","748ee3df":"upper_bound = 0.95\nres = train_data['no_of_premiums_paid'].quantile([upper_bound])\nprint(res)\n","df37622c":"true_index = train_data['no_of_premiums_paid'] < res.loc[upper_bound]\nfalse_index = ~true_index\ntrue_index","6e09741e":"train_data['PremBand'] = pd.cut(train_data[true_index]['no_of_premiums_paid'], 4)\ntrain_data[['PremBand', 'application_underwriting_score']].groupby('PremBand').count()","12d00f66":"# combine = [train_data, test_data]\n# for dataset in combine: \n#     dataset.loc[ dataset['no_of_premiums_paid'] <= 6.25, 'no_of_premiums_paid'] = 0\n#     dataset.loc[(dataset['no_of_premiums_paid'] > 6.25) & (dataset['no_of_premiums_paid'] <= 10.5), 'no_of_premiums_paid'] = 1\n#     dataset.loc[(dataset['no_of_premiums_paid'] > 10.50) & (dataset['no_of_premiums_paid'] <= 14.75), 'no_of_premiums_paid'] = 2\n#     dataset.loc[(dataset['no_of_premiums_paid'] > 14.75) & (dataset['no_of_premiums_paid'] <= 19.0), 'no_of_premiums_paid'] = 3\n#     dataset.loc[ dataset['no_of_premiums_paid'] > 19.0, 'no_of_premiums_paid'] = 4\n    \n# train_data.drop('PremBand', axis = 1, inplace = True)\n# train_data.head()","493d57c9":"upper_bound = 0.90\nres = train_data['premium'].quantile([upper_bound])\nprint(res)\ntrue_index = train_data['premium'] < res.loc[upper_bound]\nfalse_index = ~true_index\ntrue_index","b3a0ab6f":"train_data['PremBand'] = pd.cut(train_data[true_index]['premium'], 4)\ntrain_data[['PremBand', 'target']].groupby('PremBand').count()","d2dc6d80":"test_data.head()","01b0e79f":"combine = [train_data]\nfor dataset in combine: \n    dataset.loc[ dataset['premium'] <= 5925.0, 'premium'] = 0\n    dataset.loc[(dataset['premium'] > 5925.00) & (dataset['premium'] <= 10650.0), 'premium'] = 1\n    dataset.loc[(dataset['premium'] > 10650.0) & (dataset['premium'] <= 15375.0), 'premium'] = 2\n    dataset.loc[(dataset['premium'] > 15375.0) & (dataset['premium'] <= 201200.0), 'premium'] = 3\n    dataset.loc[ dataset['premium'] > 201200.0, 'premium'] = 4\ntrain_data.drop('PremBand', axis = 1, inplace = True)\ntrain_data.head()\ncombine = [train_data, test_data]","0958ba0d":"train_data['PremBand'] = pd.cut(train_data['perc_premium_paid_by_cash_credit'], 4)\ntrain_data[['PremBand', 'target']].groupby('PremBand').mean()","b2896c6c":"combine = [train_data, test_data]\nfor dataset in combine: \n    dataset.loc[ dataset['perc_premium_paid_by_cash_credit'] <= 0.25, 'perc_premium_paid_by_cash_credit'] = 0\n    dataset.loc[(dataset['perc_premium_paid_by_cash_credit'] > 0.25) & (dataset['perc_premium_paid_by_cash_credit'] <= 0.5), 'perc_premium_paid_by_cash_credit'] = 1\n    dataset.loc[(dataset['perc_premium_paid_by_cash_credit'] > 0.5) & (dataset['perc_premium_paid_by_cash_credit'] <= 0.75), 'perc_premium_paid_by_cash_credit'] = 2\n    dataset.loc[ dataset['perc_premium_paid_by_cash_credit'] > 0.75, 'perc_premium_paid_by_cash_credit'] = 3\ntrain_data.drop('PremBand', axis = 1, inplace = True)\ntrain_data.head()","0eb95aa7":"test_data.head()","0bec6586":"train_data[['perc_premium_paid_by_cash_credit', 'late_premium']] = train_data[['perc_premium_paid_by_cash_credit', 'late_premium']].astype(int)\ntest_data[['perc_premium_paid_by_cash_credit']] = test_data[['perc_premium_paid_by_cash_credit']].astype(int)\ntest_data.head()","8d7ee054":"X_train = train_data.drop(['id', 'target', 'premium', 'perc_premium_paid_by_cash_credit'], axis = 1).copy()\ny_train = train_data['target']\nX_test = test_data.drop(['id', 'perc_premium_paid_by_cash_credit'], axis = 1).copy()\nprint(X_train.shape, y_train.shape, X_test.shape)","d980845e":"X_train.head()","a2e112b8":"X_test.head()","ba48e946":"from imblearn.over_sampling import SMOTE\nprint('Number of positive and negative reviews:\\n',y_train.value_counts())\nsm = SMOTE(random_state=0,ratio=1.0)\nX_train_res,y_train_res = sm.fit_sample(X_train,y_train)\nprint('Shape after oversampling\\n',X_train_res.shape) \nprint('Equal 1s and 0s \\n', np.bincount(y_train_res))","73e267d3":"# Without Oversampling\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)\nacc_log\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\ncm = confusion_matrix(logreg.predict(X_train),y_train)\nprint(cm)\n\nprint(classification_report(logreg.predict(X_train),y_train))\n\ntnr = np.round(cm[0][0]\/(cm[0][0] + cm[1][0]) * 100,3)\ntpr = np.round(cm[1][1]\/(cm[1][1] + cm[0][1]) * 100,3)\nfpr = np.round(cm[1][0] \/ (cm[1][0] + cm[0][0]) * 100,3)\nprint('TPR = ',tpr,'%')\nprint('TNR = ',tnr,'%')\nprint('FPR = ',fpr,'%')","d2209455":"# With Oversampling\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train_res, y_train_res)\ny_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train_res, y_train_res) * 100, 2)\nacc_log\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\ncm = confusion_matrix(logreg.predict(X_train_res),y_train_res)\nprint(cm)\n\nprint(classification_report(logreg.predict(X_train_res),y_train_res))\n\ntnr = np.round(cm[0][0]\/(cm[0][0] + cm[1][0]) * 100,3)\ntpr = np.round(cm[1][1]\/(cm[1][1] + cm[0][1]) * 100,3)\nfpr = np.round(cm[1][0] \/ (cm[1][0] + cm[0][0]) * 100,3)\nprint('TPR = ',tpr,'%')\nprint('TNR = ',tnr,'%')\nprint('FPR = ',fpr,'%')\n","8c6747e2":"coeff_data = pd.DataFrame(train_data.columns.delete(0))\ncoeff_data.columns = ['Feature']\ncoeff_data['Correlation'] = pd.Series(logreg.coef_[0])\ncoeff_data.sort_values(by = 'Correlation', ascending = False)","9eef49fb":"# Gaussian Naive Bayes ~ Without oversampling\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ny_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)\nacc_gaussian","3f99cf3a":"# Gaussian Naive Bayes ~ With oversampling\n\ngaussian = GaussianNB()\ngaussian.fit(X_train_res, y_train_res)\ny_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train_res, y_train_res) * 100, 2)\nprint(acc_gaussian)\n\ncm = confusion_matrix(gaussian.predict(X_train_res),y_train_res)\nprint(cm)\n\nprint(classification_report(gaussian.predict(X_train_res),y_train_res))\n\ntnr = np.round(cm[0][0]\/(cm[0][0] + cm[1][0]) * 100,3)\ntpr = np.round(cm[1][1]\/(cm[1][1] + cm[0][1]) * 100,3)\nfpr = np.round(cm[1][0] \/ (cm[1][0] + cm[0][0]) * 100,3)\nprint('TPR = ',tpr,'%')\nprint('TNR = ',tnr,'%')\nprint('FPR = ',fpr,'%')","7404198a":"# without oversampling\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nacc_knn","2457c2e6":"# with oversampling\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train_res, y_train_res)\ny_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train_res, y_train_res) * 100, 2)\n\ncm = confusion_matrix(knn.predict(X_train_res),y_train_res)\nprint(cm)\n\nprint(classification_report(knn.predict(X_train_res),y_train_res))\n\ntnr = np.round(cm[0][0]\/(cm[0][0] + cm[1][0]) * 100,3)\ntpr = np.round(cm[1][1]\/(cm[1][1] + cm[0][1]) * 100,3)\nfpr = np.round(cm[1][0] \/ (cm[1][0] + cm[0][0]) * 100,3)\nprint('TPR = ',tpr,'%')\nprint('TNR = ',tnr,'%')\nprint('FPR = ',fpr,'%')\n","b1f45a90":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, y_train)\ny_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, y_train) * 100, 2)\nacc_perceptron","12ba34d7":"# Perceptron - with oversampling\n\nperceptron = Perceptron()\nperceptron.fit(X_train_res, y_train_res)\ny_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train_res, y_train_res) * 100, 2)\nacc_perceptron\n\ncm = confusion_matrix(perceptron.predict(X_train_res),y_train_res)\nprint(cm)\n\nprint(classification_report(perceptron.predict(X_train_res),y_train_res))\n\ntnr = np.round(cm[0][0]\/(cm[0][0] + cm[1][0]) * 100,3)\ntpr = np.round(cm[1][1]\/(cm[1][1] + cm[0][1]) * 100,3)\nfpr = np.round(cm[1][0] \/ (cm[1][0] + cm[0][0]) * 100,3)\nprint('TPR = ',tpr,'%')\nprint('TNR = ',tnr,'%')\nprint('FPR = ',fpr,'%')","d16342d0":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, y_train) * 100, 2)\nacc_sgd","cd9934fc":"# Stochastic Gradient Descent - with oversampling\n\nsgd = SGDClassifier()\nsgd.fit(X_train_res, y_train_res)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train_res, y_train_res) * 100, 2)\nprint(acc_sgd)\n\ncm = confusion_matrix(sgd.predict(X_train_res),y_train_res)\nprint(cm)\n\nprint(classification_report(sgd.predict(X_train_res),y_train_res))\n\ntnr = np.round(cm[0][0]\/(cm[0][0] + cm[1][0]) * 100,3)\ntpr = np.round(cm[1][1]\/(cm[1][1] + cm[0][1]) * 100,3)\nfpr = np.round(cm[1][0] \/ (cm[1][0] + cm[0][0]) * 100,3)\nprint('TPR = ',tpr,'%')\nprint('TNR = ',tnr,'%')\nprint('FPR = ',fpr,'%')","9dfeec46":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier(max_depth = 7)\ndecision_tree.fit(X_train, y_train)\ny_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nacc_decision_tree","9d003171":"# Decision Tree - oversampling\n\ndecision_tree = DecisionTreeClassifier(max_depth = 7)\ndecision_tree.fit(X_train_res, y_train_res)\ny_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train_res, y_train_res) * 100, 2)\nprint(acc_decision_tree)\n\n\ncm = confusion_matrix(decision_tree.predict(X_train_res),y_train_res)\nprint(cm)\n\nprint(classification_report(decision_tree.predict(X_train_res),y_train_res))\n\ntnr = np.round(cm[0][0]\/(cm[0][0] + cm[1][0]) * 100,3)\ntpr = np.round(cm[1][1]\/(cm[1][1] + cm[0][1]) * 100,3)\nfpr = np.round(cm[1][0] \/ (cm[1][0] + cm[0][0]) * 100,3)\nprint('TPR = ',tpr,'%')\nprint('TNR = ',tnr,'%')\nprint('FPR = ',fpr,'%')\n","885fa597":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators = 10)\nrandom_forest.fit(X_train, y_train)\ny_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nacc_random_forest","318bbc4b":"# Random Forest - oversampling\n\nrandom_forest = RandomForestClassifier(n_estimators = 10)\nrandom_forest.fit(X_train_res, y_train_res)\ny_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train_res, y_train_res)\nacc_random_forest = round(random_forest.score(X_train_res, y_train_res) * 100, 2)\nprint(acc_random_forest)\n\ncm = confusion_matrix(random_forest.predict(X_train_res),y_train_res)\nprint(cm)\n\nprint(classification_report(random_forest.predict(X_train_res),y_train_res))\n\ntnr = np.round(cm[0][0]\/(cm[0][0] + cm[1][0]) * 100,3)\ntpr = np.round(cm[1][1]\/(cm[1][1] + cm[0][1]) * 100,3)\nfpr = np.round(cm[1][0] \/ (cm[1][0] + cm[0][0]) * 100,3)\nprint('TPR = ',tpr,'%')\nprint('TNR = ',tnr,'%')\nprint('FPR = ',fpr,'%')","9c10f465":"pred_values = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\npred_values.sort_values(by='Score', ascending=False)","a04119d6":"submission = pd.DataFrame({\n        \"id\": test_data[\"id\"],\n        \"target\": y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)","dc8a8e37":"submission.describe()","f1423059":"# Hyperparameter Tuning ~ KNN","567d1240":"If it's more than 7 then the loan is never sanctioned. So, let's set those values first. ","732c05d0":"### Let's try and fill the missing values\n\n#### Application Under writing score","b867ab2b":"# Predicting whether a person will default on their premium\n\nImporting necessary libraries","70e2db2c":"Let's make groups for the new income range","d0915e2b":"Finally convert percentage premium paid","0591ed40":"## Building our models","a42c366c":"Further conversions","3956d264":"k - Nearest Neighbours","a2780005":"### Let's also work on no of premiums paid","a267d534":"### Logistic Regression ","12a08b49":"\n#### Run it when you have time\n\nfrom sklearn.model_selection import GridSearchCV\nknn = KNeighborsClassifier()\np = list(range(1,100,3))\nparameters = {'n_neighbors':p}\nclf = GridSearchCV(knn,param_grid = parameters, scoring = 'roc_auc', cv=10, return_train_score = True)\nclf.fit(X_train_res,y_train_res)\n\ntrain_auc_error = [1 - x for x in clf.cv_reslts_['mean_train_score']]\ntrain_auc_std = np.std(train_auc_error)\n\ntest_auc_error = [1 - x for x in clf.cv_results_['mean_test_score']]\ntest_auc_std = np.std(test_auc_error)\n\nplt.plot(p,train_auc_error,label = 'Train AUC',color = 'orange')\nplt.gca().fill_between(parameters,train_auc_error-train_auc_std,train_auc_error+train_auc_std,color= 'orange')\n\nplt.plot(p,test_auc_error,label = 'Test AUC', color = 'darkblue')\nplt.gca().fill_between(parameters,test_auc_error-test_auc_std,test_auc_std+test_auc_std,color= 'darkbue')\n\nplt.xlabel('K: Hyperparameter')\nplt.ylabel('Errors')\nplt.legend(loc = 'lower right')\nplt.show()\n\noptimal_k = clf.best_params_.get('n_neighbors')\n\nprint('optimal k is ', optimal_k)\n\n","82f9e334":"Let's standardize our data by using a standard scaler","f1a16003":"### Replacing the late_premium value in the test data ","ed9f3cfe":"**Perceptron Algorithm**","cf9ef054":"So, we can collect all the values in this range and let go of the other ones. ","60ccb354":"We can set the values of underwriting score on the basis of the sourcing channel","053c8462":"We might need to make income groups to understand the relations better ","39a6625e":"**Inference**\n\n* 93% of the people have paid their premiums. \n* The age of people is very varied between 21 and 103","8ba0f190":"## Conversion to numerical data","0f84210b":" Add  a new variable 'late premium' for late premiums","c66246c7":"Read data into dataframes ","e62a9f27":"### Let's try and deal with outlier values","3afafc35":"## EDA ","548d1f32":"## Data Wrangling ","9fe621b2":"**Let's also make groups for Age**","5a589b40":"Let's make the data splits","38d23015":"# Oversampling\n","7d6a173a":"### We also need to convert the premium column"}}