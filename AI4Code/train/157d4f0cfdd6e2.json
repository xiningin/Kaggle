{"cell_type":{"8d4d2318":"code","5e5f6d59":"code","d7612d62":"code","1461bb8c":"code","2650c783":"code","605996da":"code","ae5f694c":"code","21129fbe":"code","6d0c75b8":"code","c88f7674":"code","299daf53":"code","b51639ae":"code","c4c13b4a":"code","2d7b0a00":"code","c6e1778d":"code","09103ea6":"code","abca9ca5":"code","e3c079ae":"code","96a12a55":"code","235fea3c":"code","947f9975":"code","3f8cb4d2":"code","f679965e":"code","1a9c9196":"code","7d12a840":"code","8349b265":"markdown","aac4958f":"markdown","bf69f67d":"markdown","45a5c651":"markdown","e124e295":"markdown","3c0ffd76":"markdown","cd8b8a2f":"markdown","008da8ef":"markdown","0bb5bf16":"markdown","d4bd5494":"markdown","06105348":"markdown","d958cae2":"markdown","57bb467a":"markdown","2505f7ed":"markdown","641b6ce0":"markdown","db1734b6":"markdown","6642b11e":"markdown","391c2eec":"markdown","8938d69a":"markdown","803a0e18":"markdown","534521ad":"markdown","0da84c55":"markdown","b28c91e4":"markdown","fc7fbc8d":"markdown","9094800a":"markdown","ffba195a":"markdown","ba6e1452":"markdown","538349dd":"markdown","5707e374":"markdown","0f8ea776":"markdown","4ee9d185":"markdown","0f503062":"markdown","05bceeda":"markdown","25f83148":"markdown","656921ab":"markdown","01abf3c0":"markdown","595ba4c5":"markdown","a595caa8":"markdown"},"source":{"8d4d2318":"import matplotlib.pyplot as plt\nimport sklearn.model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd","5e5f6d59":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","d7612d62":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","1461bb8c":"all_data = pd.concat([train_data, test_data])\nall_data.head()","2650c783":"print(f\"The training set has {len(train_data)} datapoints\")\nprint(f\"The test set has {len(test_data)} datapoints\")\nprint(f\"Overall the whole dataset has {len(all_data)} datapoints\")","605996da":"all_data.info()","ae5f694c":"all_data.drop(columns=[\"Survived\"]).describe()","21129fbe":"# Since the target is a categorical value with 0 or 1 values computing the mean gives us the percentage of people who survived\nsurvival_rate = train_data[\"Survived\"].mean()\n\nprint(f\"Survival rate: {survival_rate}\")\nprint(f\"Death rate: {1-survival_rate}\")","6d0c75b8":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)","c88f7674":"corr = train_data.corr()\nfig, ax = plt.subplots(figsize=(10,10)) \nax = sns.heatmap(corr, annot=True, linewidth=0.5)\nax.set_title(\"Correlation matrix\")\nplt.show()","299daf53":"ax = sns.pairplot(train_data.drop(columns=[\"PassengerId\"]).replace({\"Survived\": {0:\"No\", 1:\"Yes\"}}),\n                                                                    hue=\"Survived\",\n                                                                    plot_kws={\"alpha\":0.5},\n                                                                    diag_kind=\"hist\")\nax.fig.suptitle(\"Pairplots\", y=1.08)","b51639ae":"to_show_features = [\"Sex\", \"Embarked\"]\nfig, axs = plt.subplots(1, 2, figsize=(20, 10))\n\nsns.countplot(x=train_data[to_show_features[0]], hue='Survived', data=train_data.replace({\"Survived\": {0:\"No\", 1:\"Yes\"}}), ax=axs[0])\nsns.countplot(x=train_data[to_show_features[1]], hue='Survived', data=train_data.replace({\"Survived\": {0:\"No\", 1:\"Yes\"}}), ax=axs[1])\n\nplt.show()","c4c13b4a":"train_data.loc[train_data[\"Sex\"] == \"male\"][\"Embarked\"]","2d7b0a00":"fig, axs = plt.subplots(1, 2, figsize=(20,10))\nsns.countplot(x=train_data.loc[train_data[\"Sex\"] == \"male\"][\"Embarked\"], hue='Survived', data=train_data.replace({\"Survived\": {0:\"No\", 1:\"Yes\"}}), ax=axs[0])\naxs[0].set_title(\"Male Passengers\")\nsns.countplot(x=train_data.loc[train_data[\"Sex\"] == \"female\"][\"Embarked\"], hue='Survived', data=train_data.replace({\"Survived\": {0:\"No\", 1:\"Yes\"}}), ax=axs[1])\naxs[1].set_title(\"Female Passengers\")\nplt.show()","c6e1778d":"fig, ax = plt.subplots(1, 1, figsize=(10,10))\nsns.countplot(x=train_data.loc[(train_data[\"Sex\"] == \"male\") & (train_data[\"Embarked\"] == \"S\")][\"Pclass\"],\n              hue='Survived', data=train_data.replace({\"Survived\": {0:\"No\", 1:\"Yes\"}}), ax=ax)\nax.set_title(\"Class distribution of Male Passenger from Southampton\")","09103ea6":"features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age\", \"Embarked\"]\n\nX_train = pd.get_dummies(train_data[features])\n\n# Fill missing values\nimputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\nX_train = imputer.fit_transform(X_train)\n\n# Standardize data\nscaler = MinMaxScaler()\n\nX_train = scaler.fit_transform(X_train)\n\ny_train = np.array(train_data[\"Survived\"])\n\nprint(f\"Shape of the training set: {X_train.shape}\")","abca9ca5":"def trainClassifier(X_train, y_train, model_name, classifier, params, score, verbose=False, num_folds=10):\n\n    kf = sklearn.model_selection.StratifiedKFold(num_folds)\n    \n    \n    train_scores = []\n\n    best_score = 0\n        \n    for config in sklearn.model_selection.ParameterGrid(params):\n        train_scores_run = []\n        counts = []\n        for train_indices, valid_indices in kf.split(X_train, y_train):\n            counts.append(len(train_indices))\n            X_train_kf = X_train[train_indices]\n            y_train_kf = y_train[train_indices]\n            X_valid_kf = X_train[valid_indices]\n            y_valid_kf = y_train[valid_indices]\n\n            model = classifier(**config)\n            model.fit(X_train_kf, y_train_kf)\n            y_hat = model.predict(X_valid_kf)\n            train_score = score(y_valid_kf, y_hat)\n            train_scores_run.append(train_score)\n\n        if np.average(train_scores_run, weights=counts) > best_score:\n            best_score = np.average(train_scores_run, weights=counts)\n            best_config = config\n            if(verbose):\n                print(\"New best score obtained\")\n                print(f\"Training with: {config}\")\n                print(f\"Total Score obtained with cross validation: {best_score}\\n\")\n\n        train_scores.append(np.average(train_scores_run, weights=counts))\n\n    output_df = pd.DataFrame(data = [[model_name, best_config ,best_score]], \\\n        columns=[\"model_name\", \"parameters\", \"training_score\"])\n\n    return output_df","e3c079ae":"results = pd.DataFrame()","96a12a55":"from sklearn.neighbors import KNeighborsClassifier\n\nparams = {\n    \"n_neighbors\": [1, 3, 5, 7, 9, 11, 13, 15]\n}\nclassifier = KNeighborsClassifier\n\nclassifier_df = trainClassifier(X_train, y_train, \"k-NN\", classifier, params, accuracy_score)\nresults = results.append(classifier_df)","235fea3c":"from sklearn.svm import LinearSVC\nparams = {\n    \"C\": [1e-3, 1e-2, 1e-1, 1],\n    \"max_iter\": [30000]\n}\nclassifier = LinearSVC\nclassifier_df = trainClassifier(X_train, y_train, \"LinearSVC\", classifier, params, accuracy_score)\nresults = results.append(classifier_df)","947f9975":"from sklearn.svm import SVC\nparams = {\n    \"kernel\" : [\"rbf\"],\n    \"C\": [1e-4, 1e-3, 1e-2, 1e-1, 1, 10],\n    \"gamma\": [1e-3, 1e-2, 1e-1, 1, 10]\n}\nclassifier = SVC\nclassifier_df = trainClassifier(X_train, y_train, \"SVC\", classifier, params, accuracy_score)\nresults = results.append(classifier_df)","3f8cb4d2":"from sklearn.linear_model import LogisticRegression\nparams = {\n    \"C\": [1e-3, 1e-2, 1e-1, 1, 10]\n}\nclassifier = LogisticRegression\nclassifier_df = trainClassifier(X_train, y_train, \"LogisticRegression\", classifier, params, accuracy_score)\nresults = results.append(classifier_df)","f679965e":"from sklearn.ensemble import RandomForestClassifier\nparams = {\"max_depth\": [3, 5, 7, 10, None],\n          \"n_estimators\":[3, 5,10, 25, 50],\n          \"max_features\": [1, 2, \"auto\"]}\nclassifier = RandomForestClassifier\nclassifier_df = trainClassifier(X_train, y_train, \"RandomForests\", classifier, params, accuracy_score)\nresults = results.append(classifier_df)","1a9c9196":"results = results.set_index(\"model_name\")\nresults","7d12a840":"classifier = RandomForestClassifier\n\n\nbest_params = results.loc[\"RandomForests\"][\"parameters\"]\nsubmission_model = classifier(**best_params)\nsubmission_model.fit(X_train, y_train)\n\nX_test = pd.get_dummies(test_data[features])\n\n# Fill missing values\nX_test = imputer.transform(X_test)\n\n# Standardize data\nX_test = scaler.transform(X_test)\n\ny_hat = submission_model.predict(X_test)\n\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': y_hat})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n","8349b265":"Let's count how many datapoints are present in each set","aac4958f":"It seems that the rate of survivability for men embarked in Southampton is very low, maybe the majority of them stayed in the third class:","bf69f67d":"#### Random Forests","45a5c651":"#### SVM","e124e295":"Let's load train and test set into a pandas dataframe","3c0ffd76":"We can then apply this function to different classification approaches","cd8b8a2f":"In the Titanic movie women and children had the priority to get on the lifeboats, so it make sense to imagine that a very simple classifier would consists in just assigning the survived status to all women. Let's see if this makes sense:","008da8ef":"# Exploratory Data Analysis","0bb5bf16":"### Comparing models","d4bd5494":"Let's analyze the target distribution of the training set to see if dataset is balanced or not:","06105348":"### Code for the submission","d958cae2":"## Using different classifiers","57bb467a":"As we can see there is a negative correlation between the class of the passenger and the fare, this makes a lot of sense: passengers of higher classes (where 1st class means highest) pay more. This may be an indication that the Fare feature is rendundant and can be discarded. ","2505f7ed":"## Pairplots","641b6ce0":"A lot of other relationship can be found from examining the data, here I gave only a brief analysis. We can deduce that for each feature there is an element that allows to discriminate well between classes, with some spikes in the data distribution. This may be an indication that a decision tree based approach (e.g. random forests) may be the a good choice.","db1734b6":"With the info method we can find out if there are columns with missing data","6642b11e":"Let's see if there is high correlation among features:","391c2eec":"A lot of interesting properties of the data can be extracted form these pairplots. First of all, note how a lot of passengers in the lowest class (third) died, while the majority of first class passenger survived. Also the age of passenger has a certain impact on the survival rate, with quite a skewed distribution.","8938d69a":"Can we do better than this?","803a0e18":"Random Forests seems to be the best approach among the ones that I tried","534521ad":"Clearly, also in this case for each feature there is one category that is very important to determine the survaval rate. For the Sex category we can see that a male is much more likely to have died than a women. Furthermore for the Embarked feature it seems that people that Embarked in Southampton (S) have a definitely lower survival rate. Probably there is a relation between variables here, e.g. in Southampton the majority of people that embarked where males. Let's see if this is true:","0da84c55":"A good idea is to concat the two datasets in order to do EDA on all the available data","b28c91e4":"## Correlation","fc7fbc8d":"Now I define a function for performing cross validation and hyperparameter search with a generic sklearn model","9094800a":"#### K-NN","ffba195a":"With describe we can get some basic statistics on the quantitative variables","ba6e1452":"Hi guys, I am quite new to Kaggle competitions and I am getting started with Titanic. Have fun!","538349dd":"A synthetic way to see the relationship between variables and the various data distributions is through pairplots. I show here the pairplots for the train data (the passenger id feature is dropped because not useful in this context):","5707e374":"So only $38\\%$ of people in the training set survived. while the rest died. Data is a bit unbalanced, but not so much.","0f8ea776":"Let's fit then a random forest classifier with the best hyperparameters found with CV","4ee9d185":"# Import stuff","0f503062":"Some of the categorical features where notshown in pairlots, since they are not encoded numerically. We can see their distribution with histograms:","05bceeda":"From this very simple analysis we can already get quite a few things:\n* We have $12$ features in the dataset, one of them is the target variable: survived (note how the test set does not have this features, hence survived is present only in $891$ entries).\n* The dataset contains both categorical an numerical features, for example Fare is a number, while Embarked is not. This means that we will probably need to use a feature encoder.\n* Some of the features have missing values: Age, Cabin and Embarked. While the number of missing values for Age and Embarked is quite low, only 295 points out of 1309 are present for the Cabin feature ","25f83148":"#### Logistic Regression","656921ab":"# Training","01abf3c0":"Lets split train_data in train and test set. I will use only 6 features and I will encode the categorical ones with dummy encoding. \nSince there are missing values in the dataset I will use a KNN Imputer approach to fill them, then I will rescale the dataset","595ba4c5":"# Loading data","a595caa8":"# Data preparation"}}