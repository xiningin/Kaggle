{"cell_type":{"798d0082":"code","bb789c6e":"code","3c80a417":"code","2f31d71e":"code","341173d9":"code","4c007d54":"code","7681aba6":"code","099ac97b":"code","dcc1a956":"code","cac25a2b":"code","5b280110":"code","45fc7db0":"code","1465aa83":"code","70a5196f":"code","d06076a0":"code","a8a82311":"code","a12a671d":"code","5a04314c":"code","99ca9bf9":"code","937ebbad":"code","3ea41abe":"code","230d9feb":"code","d677340e":"code","69795101":"code","cd761b55":"code","43b6cb88":"code","df514815":"code","5cd859f0":"code","b2c22005":"code","60b3c90a":"code","f0346cab":"code","3f2c9bbe":"code","541952a4":"markdown","eea18d69":"markdown","6dd4fed2":"markdown","aad9f669":"markdown","d772c612":"markdown","82fdd6fa":"markdown","1b2aba43":"markdown","7c136e1f":"markdown","54a68068":"markdown","418f9613":"markdown","789aafcd":"markdown","331ea1df":"markdown","36424e16":"markdown","5f33425c":"markdown","d45a36de":"markdown"},"source":{"798d0082":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bb789c6e":"# Additional packages\nimport category_encoders as encoders\nfrom sklearn.preprocessing import StandardScaler\nfrom pandas.api.types import is_numeric_dtype\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn import model_selection, metrics\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.metrics import roc_auc_score\nimport os, psutil\n\n#Lgbm\nimport lightgbm as lgb\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\n","3c80a417":"p_corr = 0.75\np_feature = 0.01\np_score = 15\n# No. of rows to be read from the training data set\np_nrows = 500000\n\n#This parameter defines the number of HP points to be tested\nn_HP_points_to_test = 50","2f31d71e":"def cpu_stats():\n    pid = os.getpid()\n    py = psutil.Process(pid)\n    memory_use = py.memory_info()[0] \/ 2. ** 30\n    return 'memory GB:' + str(np.round(memory_use, 2))","341173d9":"df_test=pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')\ndf_train=pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv',nrows=p_nrows)\nprint(\"Data imported\")\n\n## from: https:\/\/www.kaggle.com\/bextuychiev\/how-to-work-w-million-row-datasets-like-a-pro\ndef reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df\n\ndf_train = reduce_memory_usage(df_train, verbose=True)\ndf_test = reduce_memory_usage(df_test, verbose=True)\nprint(cpu_stats())\nprint('Memory reduced')","4c007d54":"features=[]\ncategorical=[]\nnumerical=[]\nfor feature in df_train.columns:\n    if feature not in ['id', 'target']:\n        features.append(feature)\n        if df_train.dtypes[feature]=='int8':\n            categorical.append(feature)\n        if df_train.dtypes[feature]=='float16':\n            numerical.append(feature)\n        #print(test.dtypes[feature])\nprint('features obtained')\n\nprint ('Size of the base {}'.format(df_train.shape))\nprint ('Total No. of variables {}'.format (len(categorical) + len(numerical)))\nprint ('Total No. of categorical variables {}'.format (len(categorical)))\nprint ('Total No. of numberical variables {}'.format (len(numerical)))\n","7681aba6":"df_train['target'].value_counts()","099ac97b":"standardEncoder = StandardScaler()\ndf_num = pd.DataFrame(standardEncoder.fit_transform(df_train[numerical]), columns =numerical)\nprint(df_num.shape)\n","dcc1a956":"# Code segment for correlation analysis\ndef get_corr(df, threshold):\n\n    tmp = df.copy()\n    \n    print('Before the Correlation analysis = {}'.format(tmp.shape))\n\n    # Create correlation matrix\n    corr_matrix = tmp.corr().abs()\n#    print(corr_matrix)\n    \n    # Select upper triangle of correlation matrix\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n\n    # Find features with correlation greater than threshold\n    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\n    print('Highly correlated numerical variables to drop:'.format(to_drop))\n\n    # Drop features \n    tmp.drop(to_drop, axis=1, inplace=True)\n    print('After the Correlation analysis = {}'.format(tmp.shape))\n    # corr_matrix['Optout_Mobile'].sort_values(ascending=False)\n    \n    return tmp\n","cac25a2b":"df_num_corr = get_corr(df_num, p_corr)","5b280110":"del df_num","45fc7db0":"def get_feature (training, target,  threshold, t_score):\n\n    fs = SelectKBest(score_func=f_classif, k=len(training.columns))\n    # apply feature selection\n    X_selected = fs.fit_transform(training, target.values.ravel())\n    print('Befoe the SelectKBest = {}'.format(training.shape))\n\n    new_features = [] # The list of features with z p-values\n\n    for i in range(len(training.columns)):\n#        print('Feature {}: {:.3f} with p-value {:.3f}'.format(training.columns[i], fs.scores_[i], fs.pvalues_[i]))\n        if fs.pvalues_[i] <= threshold and fs.scores_[i] >= t_score:\n            new_features.append(training.columns[i])\n       \n    X_selected_final =  pd.DataFrame(X_selected)\n    X_selected_final.columns = training.columns\n#    print(X_selected_final.shape)\n    X_selected_final = X_selected_final[new_features]\n#    print(X_selected_final.shape)\n\n    print('=' * 30)\n    print('After the SelectKBest = {}'.format(X_selected_final.shape))\n\n    return X_selected_final\n","1465aa83":"df_train_num = get_feature(df_num_corr, df_train['target'], p_feature, p_score)","70a5196f":"#df_train_num.hist(figsize=(16,20),color = 'g',xlabelsize=0,ylabelsize=0)","d06076a0":"del df_num_corr","a8a82311":"df_cat_corr = get_corr(df_train[categorical], p_corr)","a12a671d":"df_train_cat = get_feature(df_cat_corr, df_train['target'], p_feature, p_score)\ndf_train_cat.shape","5a04314c":"#df_train_cat.hist(figsize=(16,20),color = 'g',xlabelsize=0,ylabelsize=0)","99ca9bf9":"del df_cat_corr","937ebbad":"X = pd.concat([df_train_num,df_train_cat], axis=1)\nY = df_train['target']\nprint(X.shape)\nprint('='  *  30)\nprint(Y.shape)\n","3ea41abe":"X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.3, random_state=0)","230d9feb":"print('X_train = {}, Y_Train = {}'.format(X_train.shape[0], Y_test.shape[0]))","d677340e":"def learning_rate_010_decay_power_099(current_iter):\n    base_learning_rate = 0.1\n    lr = base_learning_rate  * np.power(.99, current_iter)\n    return lr if lr > 1e-3 else 1e-3\n\ndef learning_rate_010_decay_power_0995(current_iter):\n    base_learning_rate = 0.1\n    lr = base_learning_rate  * np.power(.995, current_iter)\n    return lr if lr > 1e-3 else 1e-3\n\ndef learning_rate_005_decay_power_099(current_iter):\n    base_learning_rate = 0.05\n    lr = base_learning_rate  * np.power(.99, current_iter)\n    return lr if lr > 1e-3 else 1e-3\n\nfit_params={\"early_stopping_rounds\":30, \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(X_test,Y_test)],\n            'eval_names': ['valid'],\n            'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n            'verbose': 250,\n            'categorical_feature': 'auto'}\n\n\n\nparam_test ={'num_leaves': sp_randint(100, 1000), \n             'min_child_samples': sp_randint(500, 1000), \n             'min_child_weight': [1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.2, scale=0.8),\n#             'max_depth' : sp_randint(5, 15),            \n             'reg_alpha': [1e-1, 1, 2, 5, 10, 50, 100, 200],\n             'reg_lambda': [1e-1, 1, 2, 5, 10, 50, 100, 200]}\n\n#first round of LGBM training\n# param_test ={'num_leaves': [50, 55, 60, 65, 70], \n#              'min_child_samples': [400, 425, 450, 475, 500], \n#              'min_child_weight': [0.01, 0.05, 0.1, 0.5, 1],\n#              'max_depth' : [6, 8, 10, 12, 14, 16, 18, 20], \n#              'subsample': [0.3, 0], \n#              'colsample_bytree': [0.2,0.4,0.5,0.6,0.8],\n#              'reg_alpha': [0.1, 1,  5, 10, 50, 100],\n#              'reg_lambda': [0.1, 1, 5, 10, 50, 100]}","69795101":"clf = lgb.LGBMClassifier(max_depth=-1, random_state=1234, silent=True, metric='auc', n_estimators=1000,  class_weight='balanced', n_jobs = -1)\n","cd761b55":"gs = RandomizedSearchCV(\n    estimator=clf, param_distributions=param_test, \n    n_iter=n_HP_points_to_test,\n    scoring='roc_auc',\n    #scoring='auc',\n    n_jobs = -1,\n    cv=3,\n    refit=True,\n    verbose=500,\n    random_state=4563)","43b6cb88":"gs.fit(X_train, Y_train, **fit_params)","df514815":"print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))","5cd859f0":"#opt_parameters = {'colsample_bytree': 0.2461334734170988, 'min_child_samples': 476, 'min_child_weight': 0.1, 'num_leaves': 60, 'reg_alpha': 0.1, 'reg_lambda': 50, 'subsample': 0.3356546639847473} \nopt_parameters = {'objective': 'binary', 'learning_rate': 0.01, 'random_state': 234123\n                    , 'colsample_bytree': 0.442550753153399, 'max_depth': -1\n                    , 'min_child_samples': 238, 'min_child_weight': 100.0, 'num_leaves': 79\n                    , 'reg_alpha': 0.1, 'reg_lambda': 1, 'subsample': 0.7737071420860528} \n\n\n#clf_final = lgb.LGBMClassifier(**clf.get_params())\n#set optimal parameters\n#clf_final.set_params(**opt_parameters)\n\n\n#Configure from the HP optimisation\nclf_final = lgb.LGBMClassifier(**gs.best_estimator_.get_params())\n\n#Configure locally from hardcoded values\n#clf_final = lgb.LGBMClassifier(**clf.get_params())\n\n\nclf_final = lgb.LGBMClassifier(max_depth=-1, random_state=3453456, silent=True, metric='auc', n_estimators=5000,  class_weight='balanced', n_jobs = -1)\n\n#Train the final model with learning rate decay\n#clf_final.fit(X_train, Y_train, **fit_params )  #, callbacks=[lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_0995)])","b2c22005":"preds = np.zeros(df_test.shape[0])\n\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nkf = StratifiedKFold(n_splits = 5, random_state=434512,shuffle=True)\n\nauc = []\nmodel_lst = []\nn = 0\n\nfor train_idx, test_idx in kf.split(X,Y):\n    x_train, x_val = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[test_idx]\n    #model = lgb.LGBMClassifier(**opt_parameters)\n    clf_final.fit(x_train, y_train, eval_set = [(x_val,y_val)], early_stopping_rounds = 30, eval_metric = \"auc\", verbose = 100)\n#    preds += model.predict_proba(df_test_sub)[:,1]\/kf.n_splits\n    model_lst.append(clf_final)\n    auc.append(roc_auc_score(y_val, clf_final.predict_proba(x_val)[:, 1]))\n#    gc.collect()\n    print(f\"fold: {n+1}, auc: {auc[n]}\")\n    n+=1       \n\n","60b3c90a":"print(model_lst)","f0346cab":"# Read testing data\ndf_test_sub = df_test.copy()\n\n\nprint(\"Testing Data imported\")\ndf_id = df_test_sub.iloc[:,0:1]\n\n# Standardization, correlation and KBest feature selection for numerical variables\ndf_num_t = pd.DataFrame(standardEncoder.transform(df_test_sub[numerical]), columns =numerical)\ndf_test_num = df_num_t[df_train_num.columns].copy()\nprint(df_test_num.shape)\n\n# Correlation and KBest feature selection for categorical variables\ndf_test_cat = df_test_sub[df_train_cat.columns].copy()\nprint(df_test_cat.shape)\n\n# Prepare the dataset\ndf_test_sub.drop(categorical, axis = 1, inplace=True)\ndf_test_sub.drop(numerical, axis = 1, inplace=True)\n\n\ndf_test_sub = pd.concat([df_test_num, df_test_cat], axis=1)  \nprint(df_test_sub.shape)","3f2c9bbe":"# Prediction\nfor i in range(len(model_lst)):\n    preds += model_lst[i].predict_proba(df_test_sub)[:,1]\/kf.n_splits\ndf_rst = pd.concat([df_id, pd.DataFrame(preds, columns = ['target'])], axis = 1)\ndf_rst.to_csv(\".\/submission.csv\",index=False)\nprint('Done!')","541952a4":"# Correlation analysis for categorical variables","eea18d69":"### Find the hyperparameters","6dd4fed2":"# LGBM Classification","aad9f669":"# Top most relevant features for categorical variables","d772c612":"# Memory Optimization ","82fdd6fa":"This notebook is for submission to the Playground of Oct 2021. In addition to the LGBM Classifier which has been used in the notebook.  Continuous work will be done to use other models for the compeition. Some codes are referred and copied from other notebooks in Kaggle.  Detailsl will be given in the Reference below.\n\n## Summary of ML pipeline\n\n* Read the training and testing data sets with memory  optimiztaion \n* Identify categorical and numerical features\n* For numerical features, standardize feature's values\n* Analyze if there are highly correlated features and automatically remove these features (p.s. no features have been removed by correlation analysis)\n* Select the best K featuers which are relevant to the target based on p-value <= 0.01 and score >= 10\n* Apply correlation analysis and the best K feature selection for categorical variables \n* Optimize the hyperparameters for LGBM\n* Get the optimal hyperparameters for cross-validation and data submission\n\n## Reference\n\n* \"TPS - Oct 2021 Model with Memory reduced\" by S T MOHAMMED @stmohd (URL: https:\/\/www.kaggle.com\/stmohd\/tps-oct-2021-model-with-memory-reduced)\n* \"TPS 2021 Oct - LigtGBM Classif - Begginers\" by B\u00c1RBARA SULPIS @brbarasulpis  (URL: https:\/\/www.kaggle.com\/brbarasulpis\/tps-2021-oct-ligtgbm-classif-begginers)\n* \"LightGBM hyperparameter optimisation\" by Misha Lisovyi @mlisovyi (https:\/\/www.kaggle.com\/mlisovyi\/lightgbm-hyperparameter-optimisation-lb-0-761?kernelSessionId=5289497)\n","1b2aba43":"# Data Submission","7c136e1f":"# Top most relevant features for numerical variables","54a68068":"### Based on the optimal model hyperparameters, use stratifiedKFold for cross-validation","418f9613":"# Correlation analysis for numerical variables","789aafcd":"# About this notebook","331ea1df":"# Identify categorical and numerical variables","36424e16":"## Standaridize the numerical variables","5f33425c":"## Parameter Setting","d45a36de":"<div style=\"color:#D81F26;\n           display:fill;\n           border-style: solid;\n           border-color:#C1C1C1;\n           font-size:14px;\n           font-family:Calibri;\n           background-color:#373737;\">\n<h2 style=\"text-align: center;\n           padding: 10px;\n           color:#FFFFFF;\">\n======= Playground Oct 2021 =======\n<\/h2>\n<\/div>"}}