{"cell_type":{"00c7d249":"code","627bfe07":"code","84ed1594":"code","0c9d07aa":"code","5b279180":"code","c0da6dfc":"code","47a117d0":"code","b7e42f60":"code","7f8392aa":"code","0df6461e":"code","6aaf2083":"code","f42d047b":"code","0f90e464":"code","c4c22e50":"code","1c58c283":"code","60e100bf":"markdown","65062bd2":"markdown","db91f001":"markdown","940321b0":"markdown","4f2e4fae":"markdown","4697e8f8":"markdown","097c9e69":"markdown","c2da3fd7":"markdown"},"source":{"00c7d249":"import warnings\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")","627bfe07":"tf.__version__","84ed1594":"mnist_data = tf.keras.datasets.mnist\n(training_images, training_labels), (testing_images, testing_labels) = mnist_data.load_data()\n\nprint('Shape of data')\nprint(f'Training set: {training_images.shape}')\nprint(f'Testing set: {testing_images.shape}')","0c9d07aa":"training_images = training_images\/255.0\ntesting_images = testing_images\/255.0","5b279180":"# Plotting sample images of hand written characters using plt.imshow()\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(5, 5))\nax1.imshow(training_images[0]);\nax2.imshow(training_images[1000]);\nax3.imshow(training_images[45000]);","c0da6dfc":"from tensorflow.nn import softmax\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.initializers import HeNormal\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.metrics import SparseCategoricalAccuracy\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy","47a117d0":"feature_layer = Flatten(input_shape=(28, 28))  # Flatten will convert a 2D image to 1D image so that we asign it to 1 row of neurons\n\n# Creating Sequential Model\nmodel = Sequential([\n                    feature_layer,  # Input layer \n                    Dense(128, activation = 'relu'),  # 1st hidden layer \n                    Dense(10, activation = softmax)  # Output layer\n                    ])\n\n# Compiling the Model\nmodel.compile(\n              optimizer = Adam(), \n              loss = SparseCategoricalCrossentropy(), \n              metrics = [SparseCategoricalAccuracy()]\n              )\n\n# Fitting the Model\nmodel.fit(training_images, training_labels, validation_data=(testing_images, testing_labels), epochs=25, verbose=0);","b7e42f60":"plot_model(model)","7f8392aa":"model_history = pd.DataFrame(model.history.history)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\nax1.plot(model_history[['loss', 'val_loss']]);\nax1.title.set_text('Train loss V\/S Test loss')\nax1.set_xlabel('Epochs');\nax1.set_ylabel('Loss');\nax1.legend('best', labels=['Train', 'Test']);\n\nax2.plot(model_history[['sparse_categorical_accuracy', 'val_sparse_categorical_accuracy']]);\nax2.title.set_text('Train accuracy V\/S Test accuracy')\nax2.set_xlabel('Epochs');\nax2.set_ylabel('Accuracy');\nax2.legend('best', labels=['Train', 'Test']);","0df6461e":"prediction = model.predict(testing_images)\n\ndef predict_handwritten_char(index):\n  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 2))\n  x, y = np.arange(0, 10), prediction[index]\n\n  ax1.imshow(testing_images[index]);\n  ax2.bar(x, y);\n\n  ax2.set_xticks(x);\n  ax2.set_ylabel('Probability Score');\n  ax2.set_xlabel('Hand Written Characters');","6aaf2083":"# Randomly predicting 5 hand written characters\n\nindex = [522, 648, 8, 8796, 48]\n\nfor i in index:\n  predict_handwritten_char(i)","f42d047b":"feature_layer = Flatten(input_shape=(28, 28))\n\n# Creating Sequential Model\nmodel = Sequential([\n                    feature_layer,  # Input layer \n                    Dense(128, activation = 'relu'),  # 1st hidden layer \n                    Dropout(.40),  # Dropout\n                    Dense(64, activation = 'relu'),  # 2nd hidden layer  \n                    Dropout(.40),  # Dropout\n                    Dense(32, activation = 'relu'),  # 3rd hidden layer\n                    Dense(10, activation = softmax)  # Output layer\n                    ])\n\n# Compiling the Model\nmodel.compile(\n              optimizer = Adam(), \n              loss = SparseCategoricalCrossentropy(), \n              metrics = [SparseCategoricalAccuracy()]\n              )\n\n# Fitting the Model\nmodel.fit(training_images, training_labels, validation_data=(testing_images, testing_labels), epochs=25, verbose=0);","0f90e464":"plot_model(model)","c4c22e50":"model_history = pd.DataFrame(model.history.history)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\nax1.plot(model_history[['loss', 'val_loss']]);\nax1.title.set_text('Train loss V\/S Test loss')\nax1.set_xlabel('Epochs');\nax1.set_ylabel('Loss');\nax1.legend('best', labels=['Train', 'Test']);\n\nax2.plot(model_history[['sparse_categorical_accuracy', 'val_sparse_categorical_accuracy']]);\nax2.title.set_text('Train accuracy V\/S Test accuracy')\nax2.set_xlabel('Epochs');\nax2.set_ylabel('Accuracy');\nax2.legend('best', labels=['Train', 'Test']);","1c58c283":"# Randomly predicting 5 hand written characters\n\nindex = [522, 648, 8, 8796, 48]\n\nfor i in index:\n  predict_handwritten_char(i)","60e100bf":"#### Predictions","65062bd2":"## Model 2\n- Used 3 hidden layers\n- Added 2 dropouts to avoid overfitting","db91f001":"## Notebook is also available on [GitHub](https:\/\/github.com\/siddheshshankar\/Deep-Learning\/blob\/main\/Basic%20Implementation\/MNIST.ipynb)\n## Check out --> [Simplest Implementation of Neural Network for Regression ](https:\/\/siddheshshankar.medium.com\/beginners-guide-simplest-implementation-of-neural-network-using-tensorflow-2-0-49ff95c36984)\n## *Please upvote if you liked my work*","940321b0":"## Model 1","4f2e4fae":"In the training data, we have collection of 60,000 images. Each image is 28X28 pixel represented in the form of an array. Pixel intensity ranges from 0 to 255. We need to normalize the data by simply dividing each element of the array by 255, so that all the values of the array gets scaled between 0 to 1.","4697e8f8":"#### Predictions","097c9e69":"# Load MNIST Data","c2da3fd7":"# Building Neural Network"}}