{"cell_type":{"9f96caa4":"code","2a4c2276":"code","fef9cf4a":"code","2fc3516b":"code","42ff6946":"code","8fb9a2b1":"code","0fe87fa2":"code","566b3e5f":"code","70e7a49f":"code","9b0a7b19":"markdown","77f99c1e":"markdown","6efd33e4":"markdown","eeb244df":"markdown","683fb362":"markdown","65275702":"markdown"},"source":{"9f96caa4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2a4c2276":"import os\nimport re\nimport json\nimport string\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer, TFBertModel, BertConfig\n\nmax_len = 384\nconfiguration = BertConfig()  # default parameters and configuration for BERT\n\n## U would get this below warning message\n## wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n# If you don't want your script to sync to the cloud\nos.environ['WANDB_MODE'] = 'dryrun'","fef9cf4a":"# Save the slow pretrained tokenizer\nslow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nsave_path = \"bert_base_uncased\/\"\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\nslow_tokenizer.save_pretrained(save_path)\n\n# Load the fast tokenizer from saved file\ntokenizer = BertWordPieceTokenizer(\"bert_base_uncased\/vocab.txt\", lowercase=True)","2fc3516b":"train_data_url = \"https:\/\/rajpurkar.github.io\/SQuAD-explorer\/dataset\/train-v1.1.json\"\ntrain_path = keras.utils.get_file(\"train.json\", train_data_url)\neval_data_url = \"https:\/\/rajpurkar.github.io\/SQuAD-explorer\/dataset\/dev-v1.1.json\"\neval_path = keras.utils.get_file(\"eval.json\", eval_data_url)","42ff6946":"\nclass SquadExample:\n    def __init__(self, question, context, start_char_idx, answer_text, all_answers):\n        self.question = question\n        self.context = context\n        self.start_char_idx = start_char_idx\n        self.answer_text = answer_text\n        self.all_answers = all_answers\n        self.skip = False\n\n    def preprocess(self):\n        context = self.context\n        question = self.question\n        answer_text = self.answer_text\n        start_char_idx = self.start_char_idx\n\n        # Clean context, answer and question\n        context = \" \".join(str(context).split())\n        question = \" \".join(str(question).split())\n        answer = \" \".join(str(answer_text).split())\n\n        # Find end character index of answer in context\n        end_char_idx = start_char_idx + len(answer)\n        if end_char_idx >= len(context):\n            self.skip = True\n            return\n\n        # Mark the character indexes in context that are in answer\n        is_char_in_ans = [0] * len(context)\n        for idx in range(start_char_idx, end_char_idx):\n            is_char_in_ans[idx] = 1\n\n        # Tokenize context\n        tokenized_context = tokenizer.encode(context)\n\n        # Find tokens that were created from answer characters\n        ans_token_idx = []\n        for idx, (start, end) in enumerate(tokenized_context.offsets):\n            if sum(is_char_in_ans[start:end]) > 0:\n                ans_token_idx.append(idx)\n\n        if len(ans_token_idx) == 0:\n            self.skip = True\n            return\n\n        # Find start and end token index for tokens from answer\n        start_token_idx = ans_token_idx[0]\n        end_token_idx = ans_token_idx[-1]\n\n        # Tokenize question\n        tokenized_question = tokenizer.encode(question)\n\n        # Create inputs\n        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n            tokenized_question.ids[1:]\n        )\n        attention_mask = [1] * len(input_ids)\n\n        # Pad and create attention masks.\n        # Skip if truncation is needed\n        padding_length = max_len - len(input_ids)\n        if padding_length > 0:  # pad\n            input_ids = input_ids + ([0] * padding_length)\n            attention_mask = attention_mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n        elif padding_length < 0:  # skip\n            self.skip = True\n            return\n\n        self.input_ids = input_ids\n        self.token_type_ids = token_type_ids\n        self.attention_mask = attention_mask\n        self.start_token_idx = start_token_idx\n        self.end_token_idx = end_token_idx\n        self.context_token_to_char = tokenized_context.offsets\n\n\nwith open(train_path) as f:\n    raw_train_data = json.load(f)\n\nwith open(eval_path) as f:\n    raw_eval_data = json.load(f)\n\n\ndef create_squad_examples(raw_data):\n    squad_examples = []\n    for item in raw_data[\"data\"]:\n        for para in item[\"paragraphs\"]:\n            context = para[\"context\"]\n            for qa in para[\"qas\"]:\n                question = qa[\"question\"]\n                answer_text = qa[\"answers\"][0][\"text\"]\n                all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n                start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n                squad_eg = SquadExample(\n                    question, context, start_char_idx, answer_text, all_answers\n                )\n                squad_eg.preprocess()\n                squad_examples.append(squad_eg)\n    return squad_examples\n\n\ndef create_inputs_targets(squad_examples):\n    dataset_dict = {\n        \"input_ids\": [],\n        \"token_type_ids\": [],\n        \"attention_mask\": [],\n        \"start_token_idx\": [],\n        \"end_token_idx\": [],\n    }\n    for item in squad_examples:\n        if item.skip == False:\n            for key in dataset_dict:\n                dataset_dict[key].append(getattr(item, key))\n    for key in dataset_dict:\n        dataset_dict[key] = np.array(dataset_dict[key])\n\n    x = [\n        dataset_dict[\"input_ids\"],\n        dataset_dict[\"token_type_ids\"],\n        dataset_dict[\"attention_mask\"],\n    ]\n    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n    return x, y\n\n\ntrain_squad_examples = create_squad_examples(raw_train_data)\nx_train, y_train = create_inputs_targets(train_squad_examples)\nprint(f\"{len(train_squad_examples)} training points created.\")\n\neval_squad_examples = create_squad_examples(raw_eval_data)\nx_eval, y_eval = create_inputs_targets(eval_squad_examples)\nprint(f\"{len(eval_squad_examples)} evaluation points created.\")\n","8fb9a2b1":"\ndef create_model():\n    ## BERT encoder\n    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\n\n    ## QA Model\n    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n    embedding = encoder(\n        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n    )[0]\n\n    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n    start_logits = layers.Flatten()(start_logits)\n\n    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n    end_logits = layers.Flatten()(end_logits)\n\n    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n\n    model = keras.Model(\n        inputs=[input_ids, token_type_ids, attention_mask],\n        outputs=[start_probs, end_probs],\n    )\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n    optimizer = keras.optimizers.Adam(lr=5e-5)\n    model.compile(optimizer=optimizer, loss=[loss, loss])\n    return model\n\n","0fe87fa2":"use_tpu = True\nif use_tpu:\n    # Create distribution strategy\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    # Create model\n    with strategy.scope():\n        model = create_model()\nelse:\n    model = create_model()\n\nmodel.summary()\n","566b3e5f":"\ndef normalize_text(text):\n    text = text.lower()\n\n    # Remove punctuations\n    exclude = set(string.punctuation)\n    text = \"\".join(ch for ch in text if ch not in exclude)\n\n    # Remove articles\n    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n    text = re.sub(regex, \" \", text)\n\n    # Remove extra white space\n    text = \" \".join(text.split())\n    return text\n\n\nclass ExactMatch(keras.callbacks.Callback):\n    \"\"\"\n    Each `SquadExample` object contains the character level offsets for each token\n    in its input paragraph. We use them to get back the span of text corresponding\n    to the tokens between our predicted start and end tokens.\n    All the ground-truth answers are also present in each `SquadExample` object.\n    We calculate the percentage of data points where the span of text obtained\n    from model predictions matches one of the ground-truth answers.\n    \"\"\"\n\n    def __init__(self, x_eval, y_eval):\n        self.x_eval = x_eval\n        self.y_eval = y_eval\n\n    def on_epoch_end(self, epoch, logs=None):\n        pred_start, pred_end = self.model.predict(self.x_eval)\n        count = 0\n        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n            squad_eg = eval_examples_no_skip[idx]\n            offsets = squad_eg.context_token_to_char\n            start = np.argmax(start)\n            end = np.argmax(end)\n            if start >= len(offsets):\n                continue\n            pred_char_start = offsets[start][0]\n            if end < len(offsets):\n                pred_char_end = offsets[end][1]\n                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n            else:\n                pred_ans = squad_eg.context[pred_char_start:]\n\n            normalized_pred_ans = normalize_text(pred_ans)\n            normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n            if normalized_pred_ans in normalized_true_ans:\n                count += 1\n        acc = count \/ len(self.y_eval[0])\n        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")\n\n","70e7a49f":"exact_match_callback = ExactMatch(x_eval, y_eval)\nmodel.fit(\n    x_train,\n    y_train,\n    epochs=1,  # For demonstration, 3 epochs are recommended\n    verbose=2,\n    batch_size=64,\n    callbacks=[exact_match_callback],\n)\n","9b0a7b19":"# BERT (from HuggingFace Transformers) for Text Extraction\n<hr>\n( This Notebook is for the <b>Brave-heart<\/b> .  NLP ML Engineers, Students doing their final Year Project in NLP and PHD Scholars; would definetly find this a very useful resource. Do upvote , if you like my work !!)\n<hr>\n\n**Author:** [Rocky Jagtiani](https:\/\/www.linkedin.com\/in\/rocky-jagtiani-3b390649\/)<br> and the data Science team at Suven ( https:\/\/datascience.suvenconsultants.com )\n\n**Description:** Fine tune pretrained BERT from HuggingFace Transformers on SQuAD.\n\n<hr>\n\n## Introduction\n\nThis demonstration uses SQuAD (Stanford Question-Answering Dataset).\nIn SQuAD, an input consists of a question, and a paragraph for context.\nThe goal is to find the span of text in the paragraph that answers the question.\nWe evaluate our performance on this data with the \"Exact Match\" metric,\nwhich measures the percentage of predictions that exactly match any one of the\nground-truth answers.\n\nWe fine-tune a BERT model to perform this task as follows:\n\n1. Feed the context and the question as inputs to BERT.\n2. Take two vectors S and T with dimensions equal to that of\n   hidden states in BERT.\n3. Compute the probability of each token being the start and end of\n   the answer span. The probability of a token being the start of\n   the answer is given by a dot product between S and the representation\n   of the token in the last layer of BERT, followed by a softmax over all tokens.\n   The probability of a token being the end of the answer is computed\n   similarly with the vector T.\n4. Fine-tune BERT and learn S and T along the way.\n\n**References:** ( Make Sure to Scan all the below resources before moving ahead in this Notebook)\n\n- [First See this video -> How Transformers Work ? ](https:\/\/drive.google.com\/file\/d\/17f8o_CcbF9d3LxW1dS1K0Hy0jkvvM1Yu\/view?usp=sharing)\n\n> Its difficult to retain & reproduce (complex) concepts after watching a video. The Video has been created primarily from this resource. Make sure you spend a good time to read it once, twice, .. , till your mind can hold the blocks together.  \n- [A high level look into Transformers - Excellent Read !!](http:\/\/jalammar.github.io\/illustrated-transformer\/)\n\n> Now , you know all the blocks of a Transformer , time that you understand all blocks of a BERT\n- [See this Video on High level Explaination on BERT](https:\/\/drive.google.com\/file\/d\/1h1Zpxcrzlj_tkp18VVbxkPD5DEBu86t0\/view?usp=sharing)\n\n> Some Resources refered in the Videos. Only if you are a PHD Scholar or Researcher, you may be interested to view them too\n- [pre-trained word vectors](https:\/\/nlp.stanford.edu\/projects\/glove\/)\n- [BERT](https:\/\/arxiv.org\/pdf\/1810.04805.pdf)\n- [SQuAD](https:\/\/rajpurkar.github.io\/SQuAD-explorer\/)  -> **Have used SQuAD dataset in this NB** \n- [Follow Google AI blog to be up-to-date on BERT](https:\/\/ai.googleblog.com\/2018\/11\/open-sourcing-bert-state-of-art-pre.html)\n","77f99c1e":"## Preprocess the data\n\n1. Go through the JSON file and store every record as a `SquadExample` object.\n2. Go through each `SquadExample` and create `x_train, y_train, x_eval, y_eval`.\n","6efd33e4":"This code should preferably be run on **`Google Colab TPU runtime`**.\n\n> With Colab TPUs, each epoch will take 5-6 minutes.\n","eeb244df":"## Create the Question-Answering Model using BERT and Functional API","683fb362":"## Create evaluation Callback\n\nThis callback will compute the exact match score using the validation data\nafter every epoch.\n","65275702":"## Train and Evaluate"}}