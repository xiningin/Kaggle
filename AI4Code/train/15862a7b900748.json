{"cell_type":{"116f9752":"code","eb5e1874":"code","c7078e24":"code","aa59a90b":"code","1c4f7b02":"code","7f3bb993":"code","d6a4f13c":"code","20ea8030":"code","f3052d5f":"code","e332a809":"code","1bbe07ac":"code","853e9282":"code","f5648fa5":"code","abe062a1":"code","7e191e35":"code","b10f74d6":"code","23672cdc":"code","038928ab":"code","086759f1":"code","5d748a19":"code","53596b6a":"code","1d77af11":"code","5711d51d":"code","36ad22eb":"code","76eab3e7":"code","f26cbe7a":"code","a473a41d":"code","5546c125":"code","f692f484":"code","d452653e":"code","0155e5c3":"code","781dc13b":"code","5b815083":"markdown","7a78c56c":"markdown","cbda11af":"markdown","92011486":"markdown","8a3295d1":"markdown","da1a0576":"markdown","53685e51":"markdown","98d8b6d1":"markdown","2fe96289":"markdown","fd1d103d":"markdown","a54d793b":"markdown","d2df7cf0":"markdown","8e4d90fd":"markdown","4194b5a9":"markdown","b0933bd2":"markdown","e77df135":"markdown","78efb9d9":"markdown","dbab2ff7":"markdown","9bbd6a5e":"markdown"},"source":{"116f9752":"import os #to access files\nimport pandas as pd #to work with dataframes\nimport numpy as np #just a tradition\nfrom sklearn.model_selection import StratifiedKFold #for cross-validation\nfrom sklearn.metrics import roc_auc_score #this is we are trying to increase\nimport matplotlib.pyplot as plt #we will plot something at the end)\nimport seaborn as sns #same reason\nimport lightgbm as lgb #the model we gonna use","eb5e1874":"%%time\nPATH_TO_DATA = '..\/input\/'\n\ndf_train_features = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_features.csv'), index_col='match_id_hash')#.sample(frac=0.01)\ndf_train_targets = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_targets.csv'), index_col='match_id_hash')#.sample(frac=0.01)\ndf_test_features = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_features.csv'), index_col='match_id_hash')#.sample(frac=0.01)","c7078e24":"df_train_features.head(2)","aa59a90b":"df_train_targets.head(2)","1c4f7b02":"df_test_features.head(2)","7f3bb993":"#turn to X and y notations for train data and target\nX = df_train_features.values\ny = df_train_targets['radiant_win'].values #extract the colomn we need","d6a4f13c":"#this is to make sure we have \"ujson\" and \"tqdm\"\ntry:\n    import ujson as json\nexcept ModuleNotFoundError:\n    import json\n    print ('Please install ujson to read JSON oblects faster')\n    \ntry:\n    from tqdm import tqdm_notebook\nexcept ModuleNotFoundError:\n    tqdm_notebook = lambda x: x\n    print ('Please install tqdm to track progress with Python loops')","20ea8030":"#a helper function, we will use it in next cell\ndef read_matches(matches_file):\n    \n    MATCHES_COUNT = {\n        'test_matches.jsonl': 10000,\n        'train_matches.jsonl': 39675,\n    }\n    _, filename = os.path.split(matches_file)\n    total_matches = MATCHES_COUNT.get(filename)\n    \n    with open(matches_file) as fin:\n        for line in tqdm_notebook(fin, total=total_matches):\n            yield json.loads(line)","f3052d5f":"def add_new_features(df_features, matches_file):\n    \n    # Process raw data and add new features\n    for match in read_matches(matches_file):\n        match_id_hash = match['match_id_hash']\n\n        # Counting ruined towers for both teams\n        radiant_tower_kills = 0\n        dire_tower_kills = 0\n        for objective in match['objectives']:\n            if objective['type'] == 'CHAT_MESSAGE_TOWER_KILL':\n                if objective['team'] == 2:\n                    radiant_tower_kills += 1\n                if objective['team'] == 3:\n                    dire_tower_kills += 1\n\n        # Write new features\n        df_features.loc[match_id_hash, 'radiant_tower_kills'] = radiant_tower_kills\n        df_features.loc[match_id_hash, 'dire_tower_kills'] = dire_tower_kills\n        df_features.loc[match_id_hash, 'diff_tower_kills'] = radiant_tower_kills - dire_tower_kills\n        \n        # ... here you can add more features ...\n        ","e332a809":"%%time\n# copy the dataframe with features\ndf_train_features_extended = df_train_features.copy()\ndf_test_features_extended = df_test_features.copy()\n\n# add new features\nadd_new_features(df_train_features_extended, os.path.join(PATH_TO_DATA, 'train_matches.jsonl'))\nadd_new_features(df_test_features_extended, os.path.join(PATH_TO_DATA, 'test_matches.jsonl'))","1bbe07ac":"#Just a shorter names for data\nnewtrain=df_train_features_extended\nnewtest=df_test_features_extended\ntarget=pd.DataFrame(y)","853e9282":"#lastly, check the shapes, Andrew Ng approved)\nnewtrain.shape,target.shape, newtest.shape","f5648fa5":"features=newtrain.columns","abe062a1":"param = {\n        'bagging_freq': 5,\n        'bagging_fraction': 0.5,\n        'boost_from_average':'false',\n        'boost': 'gbdt',\n        'feature_fraction': 0.05,\n        'learning_rate': 0.01,\n        'max_depth': -1,  \n        'metric':'auc',\n        'min_data_in_leaf': 50,\n        'min_sum_hessian_in_leaf': 10.0,\n        'num_leaves': 10,\n        'num_threads': 5,\n        'tree_learner': 'serial',\n        'objective': 'binary', \n        'verbosity': 1\n    }","7e191e35":"%%time\n#divide training data into train and validaton folds\nfolds = StratifiedKFold(n_splits=5, shuffle=False, random_state=17)\n\n#placeholder for out-of-fold, i.e. validation scores\noof = np.zeros(len(newtrain))\n\n#for predictions\npredictions = np.zeros(len(newtest))\n\n#and for feature importance\nfeature_importance_df = pd.DataFrame()\n\n#RUN THE LOOP OVER FOLDS\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(newtrain.values, target.values)):\n    \n    X_train, y_train = newtrain.iloc[trn_idx], target.iloc[trn_idx]\n    X_valid, y_valid = newtrain.iloc[val_idx], target.iloc[val_idx]\n    \n    print(\"Computing Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(X_train, label=y_train)\n    val_data = lgb.Dataset(X_valid, label=y_valid)\n\n    \n    num_round = 5000 \n    verbose=1000 \n    stop=500 \n    \n    #TRAIN THE MODEL\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=verbose, early_stopping_rounds = stop)\n    \n    #CALCULATE PREDICTION FOR VALIDATION SET\n    oof[val_idx] = clf.predict(newtrain.iloc[val_idx], num_iteration=clf.best_iteration)\n    \n    #FEATURE IMPORTANCE\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    #CALCULATE PREDICTIONS FOR TEST DATA, using best_iteration on the fold\n    predictions += clf.predict(newtest, num_iteration=clf.best_iteration) \/ folds.n_splits\n\n#print overall cross-validatino score\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","b10f74d6":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:150].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()\nplt.savefig('FI.png')","23672cdc":"df_submission = pd.DataFrame({'radiant_win_prob': predictions}, \n                                 index=df_test_features.index)\nimport datetime\nsubmission_filename = 'submission_{}.csv'.format(\n    datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\ndf_submission.to_csv(submission_filename)\nprint('Submission saved to {}'.format(submission_filename))","038928ab":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\n\ngkf = KFold(n_splits=5, shuffle=False, random_state=42).split(X=newtrain.values, y=target.values.ravel())\n\nparam_grid = {\n    'num_leaves': [10, 31, 127],\n    'max_depth': [3, 4, 5, 6, -1],\n    #'reg_alpha': [0.1, 0.5],\n    #'min_data_in_leaf': [30, 400],\n    #'lambda_l1': [0, 1, 1.5],\n    #'lambda_l2': [0, 1],\n    #'num_boost_round' : [500,1000]\n    }\n\nlgb_estimator = lgb.LGBMClassifier(boosting_type='gbdt',  objective='binary', learning_rate=0.1, metric='auc', verbose=1, n_estimators=500)\n\ngsearch = GridSearchCV(estimator=lgb_estimator, param_grid=param_grid, cv=gkf)\nlgb_model = gsearch.fit(X=newtrain.values, y=target.values.ravel())\n\nprint(lgb_model.best_params_, lgb_model.best_score_)","086759f1":"from lightgbm import LGBMClassifier\nlgb_clf = LGBMClassifier(random_state=17, n_estimators=500)","5d748a19":"%%time\nlgb_clf.fit(X_train, y_train)","53596b6a":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_valid, lgb_clf.predict(X_valid))","1d77af11":"param_grid = {'num_leaves': [7, 15, 31, 63], \n              'max_depth': [3, 4, 5, 6, -1]}","5711d51d":"grid_searcher = GridSearchCV(estimator=lgb_clf, param_grid=param_grid, cv=5, verbose=1, n_jobs=4)\ngrid_searcher.fit(X_train, y_train)","36ad22eb":"grid_searcher.best_params_, grid_searcher.best_score_","76eab3e7":"roc_auc_score(y_valid, grid_searcher.predict(X_valid))","f26cbe7a":"num_iterations = 1500\n\nlgb_clf2 = LGBMClassifier(random_state=17, max_depth=4, num_leaves=15, n_estimators=num_iterations, n_jobs=1)\n\nparam_grid2 = {'learning_rate': np.logspace(-3, 0, 10)}\ngrid_searcher2 = GridSearchCV(estimator=lgb_clf2, param_grid=param_grid2, cv=5, verbose=1, n_jobs=4)\ngrid_searcher2.fit(X_train, y_train)","a473a41d":"print(grid_searcher2.best_params_, grid_searcher2.best_score_)\nprint(roc_auc_score(y_valid, grid_searcher2.predict(X_valid)))","5546c125":"final_lgb = LGBMClassifier(n_estimators=1500, num_leaves=31,learning_rate=0.046, max_depth=-1, n_jobs=4)","f692f484":"%%time\nfinal_lgb.fit(newtrain.values, target.values.ravel())","d452653e":"%%time\nlgb_final_pred = final_lgb.predict_proba(newtest.values)","0155e5c3":"predictions = lgb_final_pred[:,1]\npredictions.shape","781dc13b":"df_submission = pd.DataFrame({'radiant_win_prob': predictions}, index=df_test_features.index)\nimport datetime\nsubmission_filename = 'submission_opt_{}.csv'.format(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\ndf_submission.to_csv(submission_filename)\nprint('Submission saved to {}'.format(submission_filename))","5b815083":"# Finally, let's run the model","7a78c56c":"## Prepare submission file","cbda11af":"### Now, let's import all required packages ","92011486":"After running the LightGBM model, we will visualize something called \"feature importance\", which  kind of shows which features and how much they affected the final result. For this reason we need to store feature names:","8a3295d1":"From these feature importance chart, we can see that \"x_gold\" features play significant role in making the prediction. But still need more investigation of dota2 features...","da1a0576":"## Let's read the data: train, target and test","53685e51":"# What's next?\n\n* try to tune parameters, it will definitely improve your LB score\n* try to come up with good features\n* read other kernels\n* try other models as well","98d8b6d1":"## Lets have a look what are these data look like:","2fe96289":"# 1'st stage of hyper-param tuning: tuning model complexity","fd1d103d":"I have no idea what these features mean...I prefer FIFA)","a54d793b":"### Hopefully, this kernel was usefull. Feel free to fork, comment and upvote!","d2df7cf0":"Now, we define a function which adds some new features:\n\nPS: all of these are from \"how to start\" kernel by @yorko","8e4d90fd":"* https:\/\/stackoverflow.com\/questions\/50686645\/grid-search-with-lightgbm-example\n* https:\/\/www.kaggle.com\/kashnitsky\/topic-10-practice-with-logit-rf-and-lightgbm","4194b5a9":"# here, imagine some cool picture about dota2)","b0933bd2":"## Feature Importance","e77df135":"Not much, just how to implement LightGBM )\n\nMotivation to write this kernel was the tabular data structure, which means data is in the form of a table(pandas dataframe). This data structure and the task of binary classifcation is similar to other competition, \"Santander Customer Transaction Prediction\", where top kernels use LightGBM. Sooo, I tried it here as well)\n\nPS: no EDA here","78efb9d9":"## What you will learn?","dbab2ff7":"## Noow, let's define LightGBM parameters. \n\nPersonally, I understand only some of these parameters. So, these are some random set up. Maybe it is better to look up the official documentation. Tuning these parameters probably will increase your score.\n\nInvestigation in process...","9bbd6a5e":"# 2'nd stage of hyper-param tuning: convergence:"}}