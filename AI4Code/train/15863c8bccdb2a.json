{"cell_type":{"b5f6c762":"code","23771a23":"code","568bc34e":"markdown","b145f827":"markdown","ae8994cf":"markdown","351b3af9":"markdown","ab79bd1d":"markdown","c22e20ca":"markdown","ac80827f":"markdown","170042cd":"markdown","46214910":"markdown"},"source":{"b5f6c762":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef sigmoid(x):\n    return 1\/(1 + np.exp(-(x)))\n\ndef relu(x):\n    import copy\n    \n    x_deep = copy.deepcopy(x) \n    neg_ele_idxs = np.where(x<0)\n    x_deep[neg_ele_idxs] = 0\n    return x_deep\n\n# gen data\nxs = np.linspace(-50,50,100)\n\n# plot\nfig, axarr = plt.subplots(1, 2)\nfig.set_size_inches(10,4)\n\n\naxarr[0].plot(xs, sigmoid(xs))\naxarr[0].set_title(\"Saturating sigmoid\")\n\naxarr[1].plot(xs, relu(xs))\naxarr[1].set_title(\"Non-Saturating ReLU\")\n\nplt.show()","23771a23":"xs = np.random.normal(0, 0.01, 99999)\n\nfrom datetime import datetime\nnp.random.seed(123)\n\nbeg = datetime.now()\nsigmoid(xs)\nend = datetime.now()\nprint('Time for sat func: ', end-beg)\n\nbeg = datetime.now()\nsigmoid(xs)\nend = datetime.now()\nprint('Time for non-sat func: ', end-beg)\n","568bc34e":"**Simple speed test**\n\n> *Note as we are perform deep copy on ReLU, it should be significantly slower. Overall time isn't less. Because it is extremely faster which compensates for deepcopy*","b145f827":"# 02. Non-Saturating Non-Liniearites","ae8994cf":"# Saturating and Non-Saturating Non-Linearities","351b3af9":"> ReLU is significantly faster","ab79bd1d":"> ### **Saturation:** \n> - *State in which a neuron predominantly outputs **values close to the asymptotic ends** of the bounded activation function*\n> - *Saturation damages both the **information capacity and the learning ability** of a neural network. Causes **Vanishing gradient** problem due to multiplication w\/ near-zero values*","c22e20ca":"> Squash input into some boundaries\n\nExample, sigmoid $[0, 1]$ tanH $[-1, 1]$ <i><a href=\"https:\/\/www.kaggle.com\/l0new0lf\/sat-nonlin\" >Even their derivatives are saturating<\/a><\/i>\n\n- Contibutes to [vanishing gradient problem](link)\n- $\\exp$ function is time consuming. Hence, **slow** (Proved in AlexNet paper on CIFAR-10)","ac80827f":"**Mathematically**\n\n- $\ud835\udc53$ is non-saturating iff &nbsp; $(|\\lim_{z \\to -\\infty} \ud835\udc53(\ud835\udc67)|=+\u221e) \u2228 (|\\lim_{z \\to \\infty} \ud835\udc53(\ud835\udc67)|=+\u221e)$\n- \ud835\udc53 is saturating iff \ud835\udc53 is not non-saturating.\n\n","170042cd":"> Do not sqaush.\n>\n> $x \\rightarrow \\infty \\implies y\\rightarrow \\infty$ either at positive or negative side\n\nExample ReLU $[0, \\infty]$\n\n- Do not need normalized input\n  - Even if little positive input $\\implies$ Neuron Learns\n  - Hence, do not contribute to [vanishing gradient problem](link)\n- Faster to compute (upto 7x as in [Alexnet](paper))","46214910":"# 01. Saturating Non-Liniearites"}}