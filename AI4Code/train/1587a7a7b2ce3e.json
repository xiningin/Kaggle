{"cell_type":{"ab461fea":"code","8ad83ca5":"code","ca55877e":"code","35606d69":"code","24870715":"code","d108f886":"code","268f5c57":"code","9482558e":"code","4e4d54e3":"code","819150c9":"code","a9b493f0":"code","c9d8d78a":"code","8591349c":"code","5f8da596":"code","a64c25a8":"code","eec2db99":"code","40fd40e9":"code","8828a095":"code","939c6cac":"code","f40ced2d":"code","32ae3ced":"code","94f95998":"code","feb3fe75":"code","4761b0dc":"code","1c347aa1":"code","f1e9a720":"markdown","f3e7d533":"markdown","b270258a":"markdown","d81e58c5":"markdown","93ee50e9":"markdown","12a1d489":"markdown","ad2b2ce3":"markdown","5a9a79f3":"markdown","1232d99a":"markdown","f11f8a23":"markdown","c669c014":"markdown","3b8fb9fe":"markdown","cc1a047d":"markdown","e9285f42":"markdown","14ef3f5e":"markdown","b43946e2":"markdown","23aa40a9":"markdown","2df3ead3":"markdown","8653efff":"markdown","742f253b":"markdown"},"source":{"ab461fea":"# import libraries  \nimport numpy as np\nimport pandas as pd\nimport nltk\nimport re, random, os\nimport string, pprint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# spacy for basic preprocessing, optional, can use nltk as well (lemmatisation etc.)\nimport spacy\n\n# gensim for LDA \nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim  # don't skip this\n#from pyLDAvis import gensim_models as pg\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","8ad83ca5":"df = pd.read_csv('..\/input\/amazon-product-reviews-dataset\/7817_1.csv')\nprint(df.shape)\ndf.head()","ca55877e":"# filter for product id = amazon echo\ndf = df[df['asins']==\"B01BH83OOM\"]\nprint(df.shape)\ndf.head()","35606d69":"# tokenize using gensim simple_preprocess\ndef sent_to_words(sentences, deacc=True): # deacc=True removes punctuations\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence)))  \n\n\n# convert to list\ndata = df['reviews.text'].values.tolist()\ndata_words = list(sent_to_words(data))\n\n# sample\nprint(data_words[3])","24870715":"# create list of stop words\n# string.punctuation (from the 'string' module) contains a list of punctuations\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english') + list(string.punctuation)\n","d108f886":"# functions for removing stopwords and lemmatization\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","268f5c57":"# call functions\n\n# remove stop words\ndata_words_nostops = remove_stopwords(data_words)\n\n# initialize spacy 'en' model, use only tagger since we don't need parsing or NER \n# python3 -m spacy download en\nnlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n\n# lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[3])","9482558e":"# compare the nostop, lemmatised version with the original one\n# note that speakers is lemmatised to speaker; \nprint(' '.join(data_words[3]), '\\n')\nprint(' '.join(data_lemmatized[3]))","4e4d54e3":"# create dictionary and corpus\n# create dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create corpus\ncorpus = [id2word.doc2bow(text) for text in data_lemmatized]\n\n# sample\nprint(corpus[2])","819150c9":"# human-readable format of corpus (term-frequency)\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","a9b493f0":"# help(gensim.models.ldamodel.LdaModel)","c9d8d78a":"# Build LDA model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=10, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","8591349c":"# print the 10 topics\npprint.pprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","5f8da596":"# coherence score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","a64c25a8":"# visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","eec2db99":"# compute coherence value at various values of alpha and num_topics\ndef compute_coherence_values(dictionary, corpus, texts, num_topics_range, alpha_range):\n    \n    coherence_values = []\n    model_list = []\n    for alpha in alpha_range:\n        for num_topics in num_topics_range:\n            lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                               id2word=dictionary,\n                                               num_topics=num_topics, \n                                               alpha=alpha,\n                                               per_word_topics=True)\n            model_list.append(lda_model)\n            coherencemodel = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n            coherence_values.append((alpha, num_topics, coherencemodel.get_coherence()))\n        \n\n    return model_list, coherence_values","40fd40e9":"# build models across a range of num_topics and alpha\nnum_topics_range = [2, 6, 10, 15, 20]\nalpha_range = [0.01, 0.1, 1]\nmodel_list, coherence_values = compute_coherence_values(dictionary=id2word, \n                                                        corpus=corpus, \n                                                        texts=data_lemmatized, \n                                                        num_topics_range=num_topics_range, \n                                                        alpha_range=alpha_range)","8828a095":"coherence_df = pd.DataFrame(coherence_values, columns=['alpha', 'num_topics', 'coherence_value'])\ncoherence_df","939c6cac":"coherence_df.sort_values('coherence_value')","f40ced2d":"# plot\ndef plot_coherence(coherence_df, alpha_range, num_topics_range):\n    plt.figure(figsize=(16,6))\n\n    for i, val in enumerate(alpha_range):\n\n        # subplot 1\/3\/i\n        plt.subplot(1, 3, i+1)\n        alpha_subset = coherence_df[coherence_df['alpha']==val]\n\n        plt.plot(alpha_subset[\"num_topics\"], alpha_subset[\"coherence_value\"])\n        plt.xlabel('num_topics')\n        plt.ylabel('Coherence Value')\n        plt.title(\"alpha={0}\".format(val))\n        plt.ylim([0.30, 1])\n        plt.legend('coherence value', loc='upper left')\n        plt.xticks(num_topics_range)\n\nplot_coherence(coherence_df, alpha_range, num_topics_range)","32ae3ced":"#Finally building the LDA Model by selecting \n\nFinal_LDA_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                                  id2word=id2word,\n                                                  num_topics=6,\n                                                  random_state=100,\n                                                  update_every=1,\n                                                  chunksize=100,\n                                                  passes=10,\n                                                  alpha=1,\n                                                  per_word_topics=True)","94f95998":"# print the 10 topics\npprint.pprint(Final_LDA_model.print_topics())\ndoc_lda = Final_LDA_model[corpus]","feb3fe75":"# coherence score\ncoherence_model_lda = CoherenceModel(model=Final_LDA_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","4761b0dc":"# visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(Final_LDA_model, corpus, id2word)\nvis","1c347aa1":"#<<<------------------------------------------THE END---------------------------------------------->>","f1e9a720":"### Preprocessing\n\nLet's first do some preprocessing. For tokenisation, though one can use NLTK as well, let's try using gensim's ```simple_preprocess``` this time. The preprocessing pipeline is mentioned below.<br>\n\n1. Tokenize each review (using gensim)\n2. Remove stop words (including punctuations)\n3. Lemmatize (using spacy)\n\nThough you can build topic models without lemmatisation, it is actually quite important (and highly recommended) because otherwise you may end up getting topics having similar words for e.g. *speaker, speakers* etc. (which are basically referring to the same thing - speaker).\n\nNote that lemmatization uses POS tags of words, so we need to specify a list of POS tags - here we've used ```['NOUN', 'ADJ', 'VERB', 'ADV']``` .","f3e7d533":"**Important Note:** All models are not automatically downloaded with spacy, so you will need to do a ```python -m spacy download en``` to use its preprocessing methods.","b270258a":"---","d81e58c5":"---","93ee50e9":"#### Importing the necessary libraries","12a1d489":"The (3, 7) above represents the fact that the word with id=3 appears 7 times in the second document (review), word id 12 appears twice and so on. The nested list below shows the frequencies of words in the first document.","ad2b2ce3":"Let's now build the topic model. We'll define 10 topics to start with. The hyperparameter `alpha` affects sparsity of the document-topic\n(theta) distributions, whose default value is 1. Similarly, the hyperparameter `eta` can also be specified, which affects the topic-word distribution's sparsity.\n\n","5a9a79f3":"---","1232d99a":"### Building the Topic Model","f11f8a23":"<center><img src=\"https:\/\/raw.githubusercontent.com\/Masterx-AI\/Project_Amazon_Product_Ratings_Topic_Modelling\/main\/Amazon.png\" style=\"width: 800px;\"\/>","c669c014":"Now lets visualise the topics. The `pyLDAvis` library comes with excellent interactive visualisation capabilities.","3b8fb9fe":"### Creating Dictionary and Corpus\n\nGensim's LDA requires the data in a certain format. Firstly, it needs the corpus as a dicionary of id-word mapping, where each word has a unique numeric ID. This is for computationally efficiency purposes. Secondly, it needs the corpus as a term-document frequency matrix which contains the frequency of each word in each document.","cc1a047d":"### Description:\n\nThe dataset is samples of Amazon Ratings for select produts. The reviews are picked randomly and the corpus has nearly 1.6k reviews of different customers.\\\nAmazon aims to understand what are the main topics of these reviews to classify them for easier search.\\\nCan you build a strong model that differentiates the topics based on the reviews corpus? \n\n#### Acknowledgements\nThe dataset is referred from Kaggle.\n\n### Objective:\n- Understand the Dataset & perform the necessary cleanup.\n- Build a strong Topic Modelling Algorithm to classify the topics.","e9285f42":"---","14ef3f5e":"# <center> \u2605 AI & ML Project - Amazon Product Reviews Topic Modelling \u2605\n### <center> ***Domain: E-Commerce***","b43946e2":"The code below creates a list of stop words. The 'string' module in python comes with a list of punctuation characters, which we'll append to the builtin stopwords of NLTK.","23aa40a9":"Let's now evaluate the model using coherence score.","2df3ead3":"## Hyperparameter Tuning - Number of Topics and Alpha\n\nLet's now tune the two main hyperparameters - number of topics and alpha. The strategy typically used is to tune these parameters such that the coherence score is maximised.","8653efff":"Let's now print the topics found in the dataset.","742f253b":"Let's now filter the dataframe to only one product - Amazon Echo. If you are not aware of Echo, <a href=\"https:\/\/www.amazon.in\/Amazon-Echo-control-weather-Powered\/dp\/B0749YXL1J?tag=googinhydr18418-21\">here's the amazon page<\/a>."}}