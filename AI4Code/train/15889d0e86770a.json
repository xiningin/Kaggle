{"cell_type":{"e58f2be3":"code","5f49a16f":"code","16540b88":"code","47d0f1ea":"code","84bfd870":"code","bdc33cb7":"code","91b6a6fd":"code","1dd9ad7d":"code","342b23c9":"code","d74dbab2":"code","4bff6378":"code","d3e6665b":"code","60e96ba7":"code","3c24f3d9":"code","3f7c4658":"code","61f2cd9d":"code","715c1532":"code","9cf6c990":"code","74a04088":"code","59acfd9d":"code","2e781a77":"code","97dcbe59":"code","7c4d1497":"code","f18070cc":"code","b2cbc449":"code","5fc53719":"code","f4dff6bb":"code","1809c57c":"code","d6b91b28":"code","2f9a66b8":"code","db3da90d":"code","3648997b":"code","7e1133aa":"code","1a822849":"code","02957962":"code","a7f20b54":"code","48f0babe":"code","56de9a2d":"code","4b576260":"code","66405f68":"code","73ae6493":"code","7b93cf2d":"code","6f7e5275":"code","9c848d08":"code","5d22e213":"code","fd82774a":"markdown","efab1210":"markdown","97c034f3":"markdown","227b9c94":"markdown","7d3afbe1":"markdown","85b9a4ce":"markdown","5efca568":"markdown","4f35ea72":"markdown","5da9d747":"markdown","83751ead":"markdown","322c87cb":"markdown","2b94b760":"markdown","24897da2":"markdown","4a7fb7da":"markdown","f0416718":"markdown","b7806ab6":"markdown","9e3a2320":"markdown","9211cefd":"markdown","032ed1f6":"markdown","3f123cc6":"markdown","eb956ec8":"markdown","e9d131fc":"markdown","045e878b":"markdown","88621bb6":"markdown","b829a8d6":"markdown","877313da":"markdown","5baf8e53":"markdown","6ac6bba6":"markdown","6d64a0fc":"markdown","699e0865":"markdown","7ce0bbc7":"markdown","8768b5c1":"markdown","afa2e054":"markdown","1144bcef":"markdown","0503434e":"markdown","e5b006fd":"markdown","3583c827":"markdown","05f73626":"markdown","41a7578f":"markdown","41973e74":"markdown","55fe1c5e":"markdown","f2c1da2f":"markdown","6a8e7eb2":"markdown","7190cef5":"markdown","54eb3738":"markdown","7b1c1763":"markdown","e9bbcba2":"markdown"},"source":{"e58f2be3":"#importing  python  libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n","5f49a16f":"#Import data set\ndf = pd.read_csv('..\/input\/heart.csv')\n","16540b88":"df.head(5)# this command shows first 5 rows of data frame","47d0f1ea":"df.info()  ","84bfd870":"df['target'].value_counts()","bdc33cb7":"#checking for Missing Data \n\nmissing_data=df.isnull()\nfor column in missing_data.columns.values.tolist():\n    print(column)\n    print (missing_data[column].value_counts())\n    print(\"\")    ","91b6a6fd":"#Correct data format\n\ndf.dtypes\n#Conclusion:All the dtypes are correct format","1dd9ad7d":"%%capture\n! pip install seaborn","342b23c9":"#correlation of independent variable and dependent variable \ndf.corr()","d74dbab2":"df[['cp','target']].corr()","4bff6378":"df.describe() # this will shows the descriptive statistics of data frame ","d3e6665b":"#Heat map\nplt.figure(figsize=(15,7))\ncorr = df.corr()\nsns.heatmap(corr, annot=True )","60e96ba7":"sns.distplot(df['age'],color='Red',hist_kws={'alpha':1,\"linewidth\": 2}, kde_kws={\"color\": \"k\", \"lw\": 3, \"label\": \"KDE\"})\n#Most people age is from 40 to 60","3c24f3d9":"fig,ax=plt.subplots(figsize=(16,6))\nsns.pointplot(x='age',y='cp',data=df,color='Lime',hue='target',linestyles=[\"-\", \"--\"])\nplt.title('Age vs Cp')\n#People with heart disease tend to have higher 'cphest pain' at all ages only exceptions at age 45 and 49","3f7c4658":"sns.countplot(x='ca',data=df,hue='target',palette='YlOrRd',linewidth=3)\n# People with 'ca' as 0 have highest chance of heart disease","61f2cd9d":"\npearson_coef, p_value = stats.pearsonr(df['age'], df['target'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value) ","715c1532":"pearson_coef, p_value = stats.pearsonr(df['sex'], df['target'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value) ","9cf6c990":"pearson_coef, p_value = stats.pearsonr(df['cp'], df['target'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value) ","74a04088":"pearson_coef, p_value = stats.pearsonr(df['trestbps'], df['target'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value) ","59acfd9d":"pearson_coef, p_value = stats.pearsonr(df['chol'], df['target'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value) ","2e781a77":"pearson_coef, p_value = stats.pearsonr(df['fbs'], df['target'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value) ","97dcbe59":"pearson_coef, p_value = stats.pearsonr(df['restecg'], df['target'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value) ","7c4d1497":"pearson_coef, p_value = stats.pearsonr(df['thalach'], df['target'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value) ","f18070cc":"pearson_coef, p_value = stats.pearsonr(df['exang'], df['target'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value) ","b2cbc449":"pearson_coef, p_value = stats.pearsonr(df['oldpeak'], df['target'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value) ","5fc53719":"pearson_coef, p_value = stats.pearsonr(df['slope'], df['target'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value) ","f4dff6bb":"pearson_coef, p_value = stats.pearsonr(df['ca'], df['target'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value) ","1809c57c":"pearson_coef, p_value = stats.pearsonr(df['thal'], df['target'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value) ","d6b91b28":"\nx=df.drop('target',axis=1)\nx.head()\ny=df['target']","2f9a66b8":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state =0)","db3da90d":"#First we import the library of Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n\nlr=LogisticRegression() #create an object\n\n# now we train the model using fit\nlr.fit(x_train,y_train) \n\n# in this line model makes predictions\npredictions=lr.predict(x_test)\n\n#we find the accuracy of model and saved in accuracy varaible\naccuracy=accuracy_score(y_test,predictions)\n\n\nprint(predictions)\nprint(f\"LogisticRegression  Accuracy Score is  {accuracy}\")","3648997b":"#First we import the library of Support vector machine\n\nfrom sklearn.svm import SVC\n\nsvc=SVC()\n\n# now we train the model using fit\nsvc.fit(x_train,y_train) \n\n# in this line model is tested\nsv_predictions=svc.predict(x_test)\n\n#we find the accuracy of model and saved in sv_accuracy varaible\nsv_accuracy=accuracy_score(y_test,sv_predictions)\n\n\nprint(sv_predictions)\nprint(f\"Support vectoe Machine   Accuracy Score is  {sv_accuracy}\")\n","7e1133aa":"from sklearn.naive_bayes import GaussianNB\n\nnb=GaussianNB()\n\n# now we train the model using fit\nnb.fit(x_train,y_train) \n\n# in this line model is tested\nnb_predictions=nb.predict(x_test)\n\n#we find the accuracy of model and saved in nb_accuracy varaible\nnb_accuracy=accuracy_score(y_test,nb_predictions)\n\n\nprint(nb_predictions)\nprint(f\" Gaussian Naive Bayes Algorithm Accuracy Score is  {nb_accuracy}\")\n","1a822849":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=50)\n\n# now we train the model using fit\nrf.fit(x_train,y_train) \n\n# in this line model is tested\nrf_predictions=rf.predict(x_test)\n\n#we find the accuracy of model and saved in rf_accuracy varaible\nrf_accuracy=accuracy_score(y_test,rf_predictions)\n\n\nprint(rf_predictions)\nprint(f\" RAndom Forest Algorithm Accuracy Score is  {rf_accuracy}\")\n","02957962":"from sklearn.neighbors import KNeighborsClassifier\n\nknn=KNeighborsClassifier(3)\n\n# now we train the model using fit\nknn.fit(x_train,y_train) \n\n# in thisn line model is tested\nknn_predictions=knn.predict(x_test)\n\n#we find the accuracy of model and saved in sv_accuracy varaible\nknn_accuracy=accuracy_score(y_test,knn_predictions)\n\n\nprint(knn_predictions)\nprint(f\" K Nearest  neighbour Cklassification Accuracy Score is  {knn_accuracy}\")\n","a7f20b54":"from sklearn.model_selection import cross_val_score","48f0babe":"cross_val_score(LogisticRegression(), x, y)","56de9a2d":"cross_val_score(SVC(), x, y)","4b576260":"cross_val_score(GaussianNB(),x,y)","66405f68":"cross_val_score(KNeighborsClassifier(),x,y)","73ae6493":"cross_val_score(RandomForestClassifier(n_estimators=30),x,y)","7b93cf2d":"scores1 = cross_val_score(RandomForestClassifier(n_estimators=5),x, y, cv=10) \nnp.average(scores1)","6f7e5275":"scores2 = cross_val_score(RandomForestClassifier(n_estimators=20),x, y, cv=10)\nnp.average(scores2)","9c848d08":"scores3 = cross_val_score(RandomForestClassifier(n_estimators=28),x, y, cv=10)\nnp.average(scores3)","5d22e213":"scores4 = cross_val_score(RandomForestClassifier(n_estimators=50),x, y, cv=10)\nnp.average(scores4)","fd82774a":"# Naive Bayes","efab1210":"## Conclusion\nSince the p-value is  >0.1, the correlation between chol and target is statistically unsignificant, and there is negative  linear relationship which is very strong (-0.854).","97c034f3":"## trestbps vs Target\nlet's  calculate the pearsonr coefficient and p-value ","227b9c94":"# Data Modelling ","7d3afbe1":"## slope vs target\nlet's  calculate the pearsonr coefficient and p-value ","85b9a4ce":"#### _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _","5efca568":"## Conclusion\nSince the p-value is  <  0.001, the correlation between trestbps and target is statistically significant, and there is negative  linear relationship which is very weak (~0.144).","4f35ea72":"## restecg vs target\nlet's  calculate the pearsonr coefficient and p-value ","5da9d747":"## KNN performance using KFold","83751ead":"# Support Vector Machine Algorithm","322c87cb":"## Conclusion\nSince the p-value is  <  0.001, the correlation between slope  and target is statistically significant, and there is linear relationship which is stong (~0.34).","2b94b760":"# Data Cleaning","24897da2":"## thalach vs target\nlet's  calculate the pearsonr coefficient and p-value ","4a7fb7da":"## Conclusion\nSince the p-value is  >0.1, the correlation between fbs and target is statistically unsignificant, and there is negative  linear relationship which is very weak (~0.2).","f0416718":"# Random Forest","b7806ab6":"## Conclusion\nSince the p-value is  >  0.001, the correlation between restecg and target is statistically unsignificant, and there is linear relationship which is very weak (~0.137).","9e3a2320":"## Conclusion\nSince the p-value is  <  0.001, the correlation between thal and target is statistically significant, and there is negative  linear relationship which is very weak (~0.344).","9211cefd":"## cp vs target\nlet's  calculate the pearsonr coefficient and p-value ","032ed1f6":"## Conclusion\nSince the p-value is  <  0.001, the correlation between cp  and target  is statistically significant, and the linear relationship is quite strong (~0.4332).","3f123cc6":"## Parameter tunning using k fold cross validation","eb956ec8":"## Conclusion\nSince the p-value is  <  0.001, the correlation between exang  and target is statistically significant, and there is negative  linear relationship which is strong (~0.43).","e9d131fc":"### _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ __ _ _ _ _ _  _ _ _ _ _ _ _ _ _ _ _ _   _ _ _ ___ _ __ __ _ _ _ _ _ _ _  _ _ _ _ _   ","045e878b":"## Conclusion\nSince the p-value is  <  0.001, the correlation between sex and target is statistically significant, and there is negative  linear relationship which  is very weak (-0.28).","88621bb6":"# Conclusion : There is no missing values in data frame","b829a8d6":"## Conclusion\nSince the p-value is  <  0.001, the correlation between oldpeak and target is statistically significant, and there is negative  linear relationship which is strong (~0.43).","877313da":"# K-Fold cross Validation","5baf8e53":"### _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ __ _ _ _ _ _  _ _ _ _ _ _ _ _ _ _ _ _   _ _ _ ___ _ __ __ _ _ _ _ _ _ _  _ _ _ _ _   ","6ac6bba6":"## Random Forest using KFold","6d64a0fc":"## Chol vs target\nlet's  calculate the pearsonr coefficient and p-value ","699e0865":"## oldpeak vs target\nlet's  calculate the pearsonr coefficient and p-value ","7ce0bbc7":"## SVC model performance using cross_val_score","8768b5c1":"## Conclusion\nSince the p-value is $<$ 0.001, the correlation between age and target is statistically significant, and there is negative  linear relationship  (-0.225).","afa2e054":"# Logistic Regression ","1144bcef":"## KNN Classifier","0503434e":"## exang vs target\nlet's  calculate the pearsonr coefficient and p-value ","e5b006fd":"## ca vs target\nlet's  calculate the pearsonr coefficient and p-value ","3583c827":"Here we used cross_val_score to fine tune our random forest classifier and figured that having around 50 trees in random forest gives best result. ","05f73626":"## sex vs target\nlet's  calculate the pearsonr coefficient and p-value ","41a7578f":"## Age vs target\nlet's  calculate the pearsonr coefficient and p-value ","41973e74":"## Conclusion\nSince the p-value is  <  0.001, the correlation between thalach and target is statistically significant, and there is  linear relationship which is stong (~0.42).","55fe1c5e":"## Conclusion\nSince the p-value is  <  0.001, the correlation between ca and target is statistically significant, and there is negative  linear relationship which is weak (~0.39).","f2c1da2f":"## thal vs target\nlet's  calculate the pearsonr coefficient and p-value ","6a8e7eb2":"## fbs vs target\nlet's  calculate the pearsonr coefficient and p-value ","7190cef5":"## Naive Bayes Performance using KFold ","54eb3738":"# Data Visualization","7b1c1763":"## Logistic regression model performance using cross_val_score","e9bbcba2":"-------------------------Details of  Attributes of data frame-------------------------\nage- in years\nsex-(1 = male; 0 = female)\ncp- chest pain type\ntrestbps- resting blood pressure (in mm Hg on admission to the hospital)\nchol- serum cholestoral in mg\/dl\nfbs-(fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\nrestecg-resting electrocardiographic results\nthalach-maximum heart rate achieved\nexang-exercise induced angina (1 = yes; 0 = no)\noldpeak-ST depression induced by exercise relative to rest\nslope-the slope of the peak exercise ST segment\nca-number of major vessels (0-3) colored by flourosopy\nthal- 3 = normal; 6 = fixed defect; 7 = reversable defect\ntarget- 1 or 0 . (0 refers no ,1 refers yes)"}}