{"cell_type":{"64000742":"code","e653e533":"code","a3315988":"code","ca8d4c48":"code","3872f4a9":"code","9aa32d72":"code","81a93ea8":"code","307894cd":"code","8cf6d9bc":"code","b0fc811b":"code","82fad240":"code","67374a81":"code","6deba6dc":"code","ec7e7a43":"code","87aed6a2":"code","d03bf5d1":"code","d966ea94":"code","1a91fbcb":"code","0adffb58":"markdown","8632b4e7":"markdown","1687da64":"markdown","66fc1625":"markdown","6254aa26":"markdown","79ffbc59":"markdown","2145abfb":"markdown","3f4ee93e":"markdown","a6c3d152":"markdown","96b36e31":"markdown","09d4d1c7":"markdown","e3b3ff83":"markdown","e9f3f0b4":"markdown","11ce59d8":"markdown","3deeffc4":"markdown","78c60e2d":"markdown","9c0a7d21":"markdown","8c63d36c":"markdown","a0579b52":"markdown","abbccbff":"markdown"},"source":{"64000742":"import numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nTRAIN_LABELS_PATH = \"..\/input\/bms-molecular-translation\/train_labels.csv\"\ndf_train_labels = pd.read_csv(TRAIN_LABELS_PATH)\ndf_train_labels.head()","e653e533":"df_train_labels.tail()","a3315988":"fully_qualified_path = \"..\/input\/bms-molecular-translation\/train\/{}\/{}\/{}\/{}.png\"\nconvert_image_id_to_path = lambda image_id_details :fully_qualified_path.format(image_id_details[0], image_id_details[1], image_id_details[2], image_id_details) ","ca8d4c48":"df_train_labels['image_path']=df_train_labels['image_id'].apply(convert_image_id_to_path)","3872f4a9":"df_train_labels.head()","9aa32d72":"def visualize_train_batch(image_ids, labels):\n    plt.figure(figsize=(16, 12))\n    \n    for ind, (image_id, label) in enumerate(zip(image_ids, labels)):\n        plt.subplot(3, 3, ind + 1)\n        image = cv2.imread(convert_image_id_to_path(image_id))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        plt.imshow(image)\n        plt.title(f\"{label[:30]}...\", fontsize=10)\n        plt.axis(\"off\")\n    \n    plt.show()\ntmp_df = df_train_labels[:9]\nimage_ids = tmp_df['image_id']\nlabels = tmp_df[\"InChI\"].values\nvisualize_train_batch(image_ids, labels)","81a93ea8":"print('Length of training-data:',len(df_train_labels))\nprint('Number of unique chemical identifier:',len(df_train_labels['InChI'].value_counts().index))\nprint('Max count of any chemical identifier in trainign data:',max(df_train_labels['InChI'].value_counts().values))","307894cd":"h_shape=[]\nw_shape=[]\naspect_ratio=[]\nfor idx,image_id in enumerate(df_train_labels.image_id.values[:1000]):\n    image = cv2.imread(df_train_labels['image_path'][idx])\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    h_shape.append(image.shape[0])\n    w_shape.append(image.shape[1])\n    aspect_ratio.append(1.0 * (image.shape[1] \/ image.shape[0]))\nplt.figure(figsize=(12, 12))\nplt.subplots_adjust(top = 0.5, bottom=0.01, hspace=1, wspace=0.4)\nplt.subplot(2, 2, 1)\nplt.hist(np.array(h_shape) * np.array(w_shape), bins=50)\nplt.xticks(rotation=45)\nplt.title(\"Area Image Distribution\", fontsize=14)\nplt.subplot(2, 2, 2)\nplt.hist(h_shape, bins=50)\nplt.title(\"Height Image Distribution\", fontsize=14)\nprint()\nplt.subplot(2, 2, 3)\nplt.hist(w_shape, bins=50)\nplt.title(\"Width Image Distribution\", fontsize=14)\nplt.subplot(2, 2, 4)\nplt.hist(aspect_ratio, bins=50)\nplt.title(\"Aspect Ratio Distribution\", fontsize=14);","8cf6d9bc":"# tensorflow version\nimport tensorflow\nprint('tensorflow: %s' % tensorflow.__version__)\n# keras version\nimport keras\nprint('keras: %s' % keras.__version__)","b0fc811b":"from pickle import dump\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Model\n \n# extract features from each image\ndef extract_features():\n    \n # load the model\n    model = VGG16()\n    # re-structure the model\n    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n    # summarize\n    print(model.summary())\n # extract features from each image\n    features = dict()\n    for idx,name in enumerate(df_train_labels['image_path'].values[:100]):\n        filename = name\n        image = load_img(filename, target_size=(224, 224))\n         # convert the image pixels to a numpy array\n        image = img_to_array(image)\n         # reshape data for the model\n        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n         # prepare the image for the VGG model\n        image = preprocess_input(image)\n         # get features\n        feature = model.predict(image, verbose=0)\n         # store feature\n        features[df_train_labels['image_id'][idx]] = feature\n        #print('>%s' % name)\n    return features\n\nfeatures = extract_features()\nprint('Extracted Features: %d' % len(features))\n# save to file\ndump(features, open('features.pkl', 'wb'))","82fad240":"# extract texts for images\ndef load_text():\n    mapping = dict()\n    for idx,text in enumerate(df_train_labels['InChI'].values[:100]):\n        mapping[df_train_labels['image_id'][idx]]=text\n    return mapping\n\ndef to_vocabulary(descriptions):\n    all_desc = set()\n    for key,value in descriptions.items():\n        all_desc.update([value])\n    return all_desc\ntexts = load_text()\nvocabulary  = to_vocabulary(texts)\nprint('Loaded: %d ' % len(texts))\nprint('Vocabulary Size: %d' % len(vocabulary))","67374a81":"from tqdm.auto import tqdm\ntqdm.pandas()\nimport Levenshtein","6deba6dc":"test = pd.read_csv('..\/input\/bms-molecular-translation\/sample_submission.csv')","ec7e7a43":"train=df_train_labels\ntrain['InChI_list'] = train['InChI'].progress_apply(lambda x: x.split('\/'))\ntrain['InChI_length'] = train['InChI_list'].progress_apply(len)\nInChI_df = train['InChI_list'].progress_apply(pd.Series)\ntrain = pd.concat([train, InChI_df.add_prefix('InChI_')], axis=1)\ndisplay(train)","87aed6a2":"#train.to_pickle('train.pkl')\n#test.to_pickle('test.pkl')","d03bf5d1":"def get_score(y_true, y_pred):\n    scores = []\n    for true, pred in zip(y_true, y_pred):\n        score = Levenshtein.distance(true, pred)\n        scores.append(score)\n    avg_score = np.mean(scores)\n    return avg_score","d966ea94":"mode_concat_string = ''\nfor i in range(11):\n    mode_string = train[f'InChI_{i}'].fillna('nan').mode()[0]\n    if mode_string != 'nan':\n        if i == 0:\n            mode_concat_string += mode_string\n        else:\n            mode_concat_string += '\/' + mode_string\nprint(mode_concat_string)\n\ny_true = train['InChI'].values\ny_pred = [mode_concat_string] * len(train)\nscore = get_score(y_true, y_pred)\nprint(score)\n","1a91fbcb":"test['InChI'] = mode_concat_string\noutput_cols = ['image_id', 'InChI']\ndisplay(test[output_cols])\ntest[output_cols].to_csv('submission.csv', index=False)","0adffb58":"### Let us now get into Model Development - which involves feature extraction and using them for training with labeled texts to help the model predict texts for newer images with their feature-set","8632b4e7":"![image.png](attachment:image.png)","1687da64":"#### With this details about competition - let us get an overview of the data-files in the next step","66fc1625":"### If we carefully watch the image-id or image-name,first 3 characters in the id represents the folder structure, so using this lets construct the fully qualified image path for each image using below code","6254aa26":"<a id=\"1\"><\/a>\n<h3 style='background:skyblue; border:0; color:white'><center>Aim of the Competition<center><h3>","79ffbc59":"### In this notebook, we are going to cover the following topics\n\n<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:skyblue; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick Navigation<\/center><\/h2>\n\n* [Aim of the Competition](#1)\n* [Data Files Overview\/ Quick EDA](#2)\n* [Baseline Text Generation Model Creatio to Predict Text for a Given Image](#3)","2145abfb":"<a id=\"2\"><\/a>\n<h3 style='background:skyblue; border:0; color:white'><center>Data Files Overview\/ Quick EDA<center><h3>","3f4ee93e":"* This competition aims at annotating or predicting the right text-string for scanned images.\n    * The scanned images are nothing but chemical structures.\n    * Annotation\/ text that must be predicted for each of the images are the corresponding chemical idenifier (InChI) - https:\/\/en.wikipedia.org\/wiki\/International_Chemical_Identifier.\n\n* As you might have guessed, We are provided with training data - which comprises of scanned images of chemical structures and the corresponding text\/annotations which shall be used to develop a system that can help us predict the right text for given a new chemical structure image present in test-data.\n* Few considerations are - Images presented both in Training and Test-Data may be very much augmented - having rotated at different angles, presented at various resolutions and even with noise added.","a6c3d152":"### Let's create a new column in the above dataframe involving the path of image","96b36e31":"As part of input information provided in competition we have the following data-set\n\n**Train Folder**\n\n**Test Folder**\n\n**train_labels.csv**\n\n**sample_submission.csv**\n\n\n\n","09d4d1c7":"### Now lets understand how the train folder, This is arranged in a 3-Level folder structure for each image-id, let us convert each of it in to a fully qualified path","e3b3ff83":"![image.png](attachment:image.png)","e9f3f0b4":"### Here is a snippet of the folder structure for first few and last few images","11ce59d8":"<a id=\"3\"><\/a>\n<h3 style='background:skyblue; border:0; color:white'><center>Baseline Text Generation Model Creatio to Predict Text for a Given Image<center><h3>","3deeffc4":"### Let us first read through train_labels.csv","78c60e2d":"### Quick EDA","9c0a7d21":"#### Similar Approach goes with test file image-id and images present in test-folder structure","8c63d36c":"### Work in progress....","a0579b52":"## Bristol-Myers Squibb \u2013 Molecular Translation\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/22422\/logos\/header.png)","abbccbff":"> We observe that we have two columns in the above file, one with the training data(training image id) and the other wirh corresponding label or text associated with it"}}