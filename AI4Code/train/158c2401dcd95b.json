{"cell_type":{"84f93291":"code","28dfb632":"code","5c45c448":"code","dd25cadb":"code","a04d26bb":"code","e14c9e2b":"code","2fd99324":"code","fcec6357":"code","45d3ec0d":"code","b0f3eb53":"code","2da56e7b":"code","08f84117":"code","82b6419d":"code","ed605f3f":"code","4864cc00":"code","fc8bd1f1":"code","14368ff8":"code","7c1b3b06":"code","51af276f":"code","47d3ceb5":"code","06617f1d":"code","279344dd":"code","03da418a":"code","203d6528":"code","a9e8351e":"code","0326a1a4":"code","348ff0fe":"code","2b5751ea":"code","a03d86bd":"code","d8134f31":"code","d582af8b":"code","7c67178a":"code","7501f1ca":"code","5cdc6f07":"code","31d0c488":"code","528cd5f7":"markdown","65f0e49a":"markdown","9103d737":"markdown","cc76fd80":"markdown","74fce85b":"markdown","153fce4a":"markdown","a6051024":"markdown","8517a557":"markdown","ae9a3add":"markdown","26577916":"markdown","2b2e5518":"markdown","4a978212":"markdown","2282c115":"markdown","3bf07292":"markdown","5d7e23f1":"markdown","3b13e042":"markdown","41c84b6d":"markdown","a9378997":"markdown","fbd8757d":"markdown","98d293c7":"markdown","71c10bb9":"markdown","aabc8501":"markdown","9f9d7036":"markdown","10efbc99":"markdown","6a9a4d67":"markdown","783672ac":"markdown","f4951145":"markdown","ab4805f8":"markdown","89fd1412":"markdown","80125b49":"markdown"},"source":{"84f93291":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np","28dfb632":"df = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","5c45c448":"df.head(10)","dd25cadb":"df.target.value_counts()","a04d26bb":"sns.countplot(x='target', data=df, palette='bwr')\nplt.show()","e14c9e2b":"countNoDisease = len(df[df.target==0])\ncountHaveDisease = len(df[df.target==1])\nprint(\"Percentage of Patients Haven't Heart Disease: {:.2f}%\".format((countNoDisease \/ (len(df.target))*100)))\nprint(\"Percentage of Patients Hav Heart Disease: {:.2f}%\".format((countHaveDisease \/ (len(df.target))*100)))","2fd99324":"df.describe()","fcec6357":"df.info()","45d3ec0d":"df.groupby('target').mean()","b0f3eb53":"# MALE vs FEMALE\npd.crosstab(df.sex, df.target).plot(kind='bar', figsize=(15,6),color=['green', 'red'])\nplt.title('Heart Disease Frequency for Sex')\nplt.xlabel('Sex(0 = Female, 1 = Male)')\nplt.xticks(rotation=0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency')\nplt.show()","2da56e7b":"plt.scatter(x=df.age[df.target==1], y=df.thalach[(df.target==1)], c='red')\nplt.scatter(x=df.age[df.target==0], y=df.thalach[(df.target==0)])\nplt.legend(['Disease', 'Not Disease'])\nplt.xlabel('Age')\nplt.ylabel('Maximum Heart Age')\nplt.show()","08f84117":"a = pd.get_dummies(df['cp'], prefix='cp')\nb = pd.get_dummies(df['thal'], prefix='thal')\nc = pd.get_dummies(df['slope'], prefix='slope')","82b6419d":"frames = [df,a,b,c]\ndf = pd.concat(frames, axis=1)\ndf.head()","ed605f3f":"df = df.drop(columns = ['cp', 'thal', 'slope'])\ndf.head()","4864cc00":"X = df.drop(['target'], axis=1)\ny = df['target']","fc8bd1f1":"X.columns","14368ff8":"y","7c1b3b06":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","51af276f":"print(X.shape, X_train.shape, X_test.shape)","47d3ceb5":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver='liblinear')\nlr.fit(X_train, y_train)\nacc_lr = round(lr.score(X_train, y_train)*100, 2)\nprint(str(acc_lr)+ ' Percentage')","06617f1d":"from sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(X_train, y_train)\nacc_svc = round(svc.score(X_train, y_train)*100, 2)\nprint(str(acc_svc)+' Percentage')","279344dd":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\nacc_knn = round(knn.score(X_train, y_train)*100, 2)\nprint(str(acc_knn)+' Percentage')","03da418a":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\nacc_dt = round(dt.score(X_train, y_train)*100, 2)\nprint(str(acc_dt)+' Percentage')","203d6528":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100)\nrf.fit(X_train, y_train)\nacc_rf = round(rf.score(X_train, y_train)*100, 2)\nprint(str(acc_rf)+' Percentage')","a9e8351e":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, y_train)\nacc_nb = round(nb.score(X_train, y_train)*100, 2)\nprint(str(acc_nb)+' Percentage')","0326a1a4":"models = pd.DataFrame({\n    'Models':['Logistic Regression', 'Support Vector', 'KNN', 'Decision Tree', 'Random Forest', 'Naive Bayes'],\n    'Score':[acc_lr, acc_svc, acc_knn, acc_dt, acc_rf, acc_nb]\n})\n\nmodels.sort_values(by='Score', ascending=False)","348ff0fe":"from sklearn.metrics import accuracy_score","2b5751ea":"lr_pred = lr.predict(X_test) #Logistic Regression\nsvm_pred = svc.predict(X_test) #Support Vector\nknn_pred = knn.predict(X_test) #K-Nearest\ndt_pred = dt.predict(X_test) #Decision Tree\nrf_pred = rf.predict(X_test) #Random Forest\nnb_pred = rf.predict(X_test) #Naive Bayes","a03d86bd":"test_lr = round(accuracy_score(lr_pred, y_test)*100,2)\ntest_svm = round(accuracy_score(svm_pred, y_test)*100,2)\ntest_knn = round(accuracy_score(knn_pred, y_test)*100,2)\ntest_dt = round(accuracy_score(dt_pred, y_test)*100,2)\ntest_rf = round(accuracy_score(rf_pred, y_test)*100,2)\ntest_nb = round(accuracy_score(nb_pred, y_test)*100,2)\n\ntest_models = pd.DataFrame({\n    'Models':['Logistic Regression', 'Support Vector', 'KNN', 'Decision Tree', 'Random Forest', 'Naive Bayes'],\n    'Score(Test Data)':[test_lr, test_svm, test_knn, test_dt, test_rf, test_nb]\n})\n\ntest_models.sort_values(by='Score(Test Data)', ascending=False)","d8134f31":"from sklearn.metrics import classification_report, confusion_matrix","d582af8b":"print(classification_report(y_test, lr_pred))","7c67178a":"cm = confusion_matrix(y_test, lr_pred)","7501f1ca":"cm","5cdc6f07":"plt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True)\nplt.title('Confusion Matrix')","31d0c488":"input_data = (34, 1, 140,230,0,1,170,1, 3.2, 1, 1,0,0,0,0,1,0,0,0,0,1)\n\n# change the input data to a numpy array\ninput_data_as_numpy_array= np.asarray(input_data)\n\n# reshape the numpy array as we are predicting for only on instance\ninput_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n\nprediction = lr.predict(input_data_reshaped)\nprint(prediction)\n\nif (prediction[0]== 0):\n  print('The Person does not have a Heart Disease')\nelse:\n  print('The Person has Heart Disease')","528cd5f7":"### Data Exploration","65f0e49a":"5. Random Forest Classifier","9103d737":"# Machine Learning Model","cc76fd80":"\n**We have a data which classified if patients have heart disease or not according to features in it. We will try to use this data to create a model which tries predict if a patient has this disease or not. We will use Machine Learning Algorithms.**","74fce85b":"3. K-Nearest Classifier","153fce4a":"2. Support Vector Classifier","a6051024":"### READ DATA","8517a557":"From Above two tables, we can see that **Logistic Regression** has better score on both test and train scores. \nSo will evaluate our model on **Logistic Regression**.","ae9a3add":"**Creating Dummy Variable**\n\nSince 'cp', 'thal'and 'slope' are categorical variables we'll turn them into dummy variables.","26577916":"# HEART DISEASE CLASSIFICATION","2b2e5518":"From above table we can see that Decision Tree and RandomForest have 100% accuracy, but above scores are based on Train datasets.","4a978212":"# GREAT JOB !\n\n**We are done with this Project**","2282c115":"Time to check accuracy_score on test","3bf07292":"6. Naive Bayes","5d7e23f1":"# Model Evaluations","3b13e042":"### Importing Datasets and Libraries","41c84b6d":"**Let's play with differet Machine Learning Algorithms,  from data we can say that this is a classification problem**","a9378997":"# Splitting the Data","fbd8757d":"4. Decision Tree Classifier","98d293c7":"Applying predict method to all algorithms","71c10bb9":"**Confusion Matrix**","aabc8501":"Data contains;\n\n- age - age in years\n\n- sex - (1 = male; 0 = female)\n\n- cp - chest pain type\n\n- trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n\n- chol - serum cholestoral in mg\/dl\n\n- fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\n- restecg - resting electrocardiographic results\n\n- thalach - maximum heart rate achieved\n\n- exang - exercise induced angina (1 = yes; 0 = no)\n\n- oldpeak - ST depression induced by exercise relative to rest\n\n- slope - the slope of the peak exercise ST segment\n\n- ca - number of major vessels (0-3) colored by flourosopy\n\n- thal - 3 = normal; 6 = fixed defect; 7 = reversable defect\n\n- target - have disease or not (1=yes, 0=no)","9f9d7036":"# Comparing Modela\nLet's compare the accuracy score of all the models used above","10efbc99":"**Classification Report**","6a9a4d67":"# Visualize","783672ac":"So, our task is now to check accuracy_score on TEST data.\n\nSo let's check","f4951145":"**DATA FOR PERSON DOES NOT HAVE A HEART DISEASE**\n\n\n34, 1, 140,230,0,1,170,1, 3.2, 1, 1,0,0,0,0,1,0,0,0,0,1","ab4805f8":"**DATA FOR PERSON HAS A HEART DISEASE**\n\n54, 0,132,200,1,0,220,0,4.2,0,0,1,1,1,1,0,1,1,1,1,0","89fd1412":"1. Logistic Regression","80125b49":"# Building Predictive Model"}}