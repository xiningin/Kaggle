{"cell_type":{"4a01f7d7":"code","be14eb16":"code","03fdaa45":"code","a5a26fac":"code","66d603df":"code","05d3f1a4":"code","b7594d46":"code","c5969adf":"code","cbd5eec0":"code","189bb6c1":"code","ed3c53c1":"code","f9bfc80a":"code","1bd9e4fd":"code","fc4ec507":"markdown","88340f1f":"markdown","360e1d4b":"markdown","52d50e11":"markdown","207b9d07":"markdown","98907a4f":"markdown","30727483":"markdown","528c7120":"markdown","1ec089ac":"markdown","fe2e226b":"markdown","b6d0a7e8":"markdown","9e413f9f":"markdown","85ecc8df":"markdown"},"source":{"4a01f7d7":"# Data manipulation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n\n# Data Visualization\nimport matplotlib.pyplot as plt \nfrom mpl_toolkits import mplot3d \nimport matplotlib.ticker as mticker\n\n# Data Classification\nfrom sklearn.naive_bayes import MultinomialNB \nfrom sklearn.model_selection import train_test_split  \nfrom sklearn.preprocessing import StandardScaler, Normalizer\nfrom sklearn.metrics import confusion_matrix, classification_report","be14eb16":"# Loading the data \ndf = pd.read_csv('..\/input\/clothessizeprediction\/final_test.csv')\n\n# Cleaning data \ndf.dropna(inplace=True)\nprint(df.isna().sum())\n\n# Removing age outliers\ndf = df[df['age'] > 9]\ndf = df[df['age'] < 90]\n\n# Summary of the data\nprint(df.describe())","03fdaa45":"# Overview of data\nsns.pairplot(data=df, hue='size', height=7)","a5a26fac":"# getting the unique sizes present in the dataset and turning the string labels into int labels \nSizeChart = [];\nfor label in df['size']:\n    if label == 'XXS':\n        SizeChart.append(1)\n    if label == 'S':\n        SizeChart.append(2)\n    if label == 'M':\n        SizeChart.append(3)\n    if label == 'L':\n        SizeChart.append(4)\n    if label == 'XL':\n        SizeChart.append(5)\n    if label == \"XXL\":\n        SizeChart.append(6)\n    if label == \"XXXL\":\n        SizeChart.append(7)\n        \ndf['SizeLabels'] = SizeChart\nSizeTicks = ['XXS','S', 'M', 'L', 'XL', 'XXL', 'XXXL']\n\n\n# Declaring a plot\nfig = plt.figure(figsize=(8,10))\nax = fig.add_subplot(projection='3d')\n\n# 3d Scatter plot of age, height, weight\nax.scatter(xs=df['SizeLabels'], ys=df['weight'], zs=df['age'], c=df['height'], s=10)\nax.set_facecolor('white')\nax.set_zlabel(\"Age\")\nax.set_ylabel(\"Weight\")\nax.set_xlabel('Size')\nax.set_title('Size Distrubtion')\nax.view_init(15, 300)","66d603df":"# dropping the size column with strings for ease of Visualizarion\nplt_df = df.drop('size',axis=1)\nplt_colnames = ['Weight (kg)', 'Age (Years)','Height (cm)','Size Labels']\nplt_df.columns = plt_colnames\n\n\n# Plot of all 4 meterics to understand the data\nfig, ax = plt.subplots(2,2, figsize=(15,18))\n\nit = 0\n\nfor x in range(2):\n    for y in range(2):\n        # Defaulting the bin value to 20\n        num_bin = 40\n        \n        # Setting up the iteration loop so that it is always +1\n        colname = plt_colnames[it]\n        \n        # grabbing the min and max values\n        min_val = plt_df[colname].min()\n        max_val = plt_df[colname].max()\n        val_width = max_val - min_val\n        \n        # Changing the number of bins for the size plot\n        if it == 3:\n        \n            sns.countplot(x=colname,\n                          data=plt_df,\n                          ax = ax[x,y])\n            ax[x,y].set(title=colname)\n            pass    \n                          \n        # setting the bin width to the number of bins present in the data\n        bin_width = val_width\/num_bin\n        \n    \n        # plotting the graph\n        if it < 3:\n            if it  == 2:\n                num_bin = 10\n            sns.histplot(plt_df[colname],\n                         bins = num_bin,\n                         binrange=(min_val, max_val),\n                         ax = ax[x,y])\n            \n            ax[x,y].set(title=colname)\n        \n        # counter\n        it+=1","05d3f1a4":"# Prepping Dataframe\npre_df = df.drop(\"size\", axis=1)\n# Looking at skew measurement\nfor columns in pre_df:\n    print( \"Before: \" +pre_df[columns].skew().astype(str)) ","b7594d46":"# addressing this through log transfrom\nfor columns in pre_df:\n    # only applying the transform to the first two columns\n    if columns == \"SizeLabels\" or columns == \"height\":\n        pass\n    else:\n        pre_df[columns] = np.log(pre_df[columns])\n        print( \"After: \"+pre_df[columns].skew().astype(str))","c5969adf":"# Addressing outliers through std method (95%)\nfor column in pre_df:\n    if column == \"SizeLabels\":\n        pass\n    else: \n        pre_df = pre_df[pre_df[column] < ( pre_df[column].mean() + (3*pre_df[column].std()))]\n        pre_df = pre_df[pre_df[column] > ( pre_df[column].mean() - 3*pre_df[column].std())]   \n        # Checking\n        print(pre_df[pre_df[column] > ( pre_df[column].mean() + 3*pre_df[column].std())])\n        print(pre_df[pre_df[column] < ( pre_df[column].mean() - 3*pre_df[column].std())])\n        \nprint(\"\\n\", pre_df)","cbd5eec0":"# Preping the data\nFeatures = pre_df.drop('SizeLabels', axis = 1)\n\n# seperating out the labels\nlabels = pre_df['SizeLabels']\n\n# Generating test set and training set (trainf)\ntrainingFeatures, testingFeatures, trainingLabels, testingLabels = train_test_split(Features, labels, test_size = .2, random_state=42)\n\n# now we gonna set up a scale for the values!\nscaler = StandardScaler()\n\n# Scaling the features after splitting them\ntrainingFeatures = scaler.fit_transform(trainingFeatures)\ntestingFeatures = scaler.transform(testingFeatures)\n","189bb6c1":"# KNeighboors Classifer\nfrom sklearn.neighbors import KNeighborsClassifier \n# These are used to keep track of the best score obtained by the model\nBestN = 0\nprev = 0\naccuracy = []\n\nfor x in range(80,150,5):\n    # Declare a model\n    model = KNeighborsClassifier(n_neighbors=x, metric='manhattan', weights='uniform') \n\n    # Fit the model\n    model.fit(trainingFeatures, trainingLabels)\n\n    # Trying to predict the model\n    score = model.score(testingFeatures,testingLabels) * 100\n    # Iteritive testing for nieghbors score\n    if score > prev:\n        prev = score\n        BestN = x\n    # Creating meterics to identify optimal neighbors\n    accuracy.append(score)\n\n# For model comparision\nclusterScore = (np.mean(accuracy))\n\n# Printing best score\nprint(BestN, \" is the best number of neighbors with a score of: \", prev)","ed3c53c1":"# Graphing model preformace\nfig = plt.figure(figsize=(8,10))\nax = fig.add_subplot()\nax.plot(range(80,150,5),accuracy)\nax.set_title('Model Accuracy')\nax.set_xlabel('Number of clusters')\nax.set_xticks(range(80,150,5))\nax.set_ylabel('Accuracy (%)')","f9bfc80a":"# Multiple Linear Regression\nfrom sklearn.linear_model import LinearRegression\n\n# Declare Model\nmodel = LinearRegression()\n\n# Fit Model\nmodel.fit(trainingFeatures, trainingLabels)\n\n# Scoring the model --- 69%\nregressionScore = (model.score(testingFeatures,testingLabels)) * 100\n","1bd9e4fd":"# Graph comparing the model effectness (95% reflection on the graph!)\nY = [clusterScore*.95, regressionScore*.95]\nX = [\"Neighbors\" , \"Regression\"]\nsns.barplot(y=Y, x=X)\nplt.show()\nprint(\"Neighbors method averaged a score of: \" + clusterScore.astype(str) + \"\\n\\n\")\nprint(\"Regression method average a score of: \" + regressionScore.astype(str))","fc4ec507":"Height was used as the hue for this graph!\n\n**Key take aways for me**\n- Weight seems to have the largest effect on size difference\n- Age has little effect on size in most cases\n- height and weight seem to be strongly correlated","88340f1f":"**Key take aways for me**\n- Shows the skewness of the data pretty well, strong right skew on age and weight\n- Size \"6\" or XXL has a extremely low repesentation in this dataset and can be removed (though you can keep it in!)\n- This shows a justification for some pre-processing in order to have a more repersentive model","360e1d4b":"# Model Time!","52d50e11":"# ---- Getting a grasp on the data ----","207b9d07":"This is more managable degree of skew if we want to address outliers.\n\n**Key Take Away**\n- The less skewed your data, the more accurate outlier corrects will become","98907a4f":"I have reduced my dataset to only repersent 95% of the overall population so any my models accuracy on the whole data set is its score * 95% for the population as a whole!\n\n**Key take away**\n- When you reduce the size of the data set you should also reduce the accuracy score of model accordingly","30727483":"# Neighboors","528c7120":"A strong measurement of skew is noted to be > 1 || < -1 \n\nTo address this I go for a **log transform** which is the a strong corrector of right skew\n\n[See here for more information on transforms](http:\/\/towardsdatascience.com\/top-3-methods-for-handling-skewed-data-1334e0debf45)\n","1ec089ac":"# --- Data Prep aka the fun stuff! ---","fe2e226b":"This will be the base for which the rest of the analysis is done. There is a couple of thing to notice right off the bat:\n- The data is skewed right\n- We will need to need to handle outliers","b6d0a7e8":"# Final Breakdown\n\n**My conclusions**\n- When looking at the two models its clear that there just isn't enough parameters\n- Regression handles predicting better than clustering with this dataset\n- More context to the data would help us better understand how good our model preforms\n- A ~65% chance to guess the cloth size of an individual with 7 options is a huge improvement on just guessing!\n","9e413f9f":"# 3-D Visualization","85ecc8df":"# Graphing the preformace of difference number of clusters"}}