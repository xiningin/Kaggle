{"cell_type":{"bfdb4907":"code","3eee648e":"code","194c0723":"code","d84ef542":"code","54e60b3b":"code","c120a428":"code","35cd5341":"code","4449dd8a":"code","60846691":"code","c7e5d8ad":"code","f66d3c23":"code","474280cb":"code","7cd76cd7":"code","4e72d103":"code","ac10368f":"code","b8245c65":"code","326de93c":"code","b9a966a4":"code","00b856af":"code","7ecf2df3":"code","dcf973cd":"code","c7613227":"code","65617561":"code","88604868":"code","89db285d":"code","a58f2de6":"code","ecfda430":"code","83266800":"code","33932798":"code","a9fff268":"code","14364a9d":"code","925d9908":"code","d40caa5c":"code","2a5436eb":"code","009031a3":"code","69477f4e":"code","45ef2f18":"code","85bb3272":"code","95f4bb37":"code","f7c80c0d":"code","0ba2af74":"markdown","cfa170a8":"markdown","26f9210e":"markdown","97c1c08b":"markdown","e34d048c":"markdown","0a6336b2":"markdown","6472e649":"markdown","0b56859b":"markdown","60136be9":"markdown","5e8353de":"markdown","f4cb4890":"markdown","0efb74a2":"markdown","bdb4674f":"markdown","d89bdbd6":"markdown","d03983bf":"markdown","d04ff257":"markdown","c731b5f5":"markdown","958351b0":"markdown","5c319f48":"markdown","2a0fd958":"markdown","57f8edcb":"markdown","351d781d":"markdown","3394dec6":"markdown","4a4d10a2":"markdown","a0722f8a":"markdown","58a7fcf1":"markdown","54f9e0be":"markdown","f875939e":"markdown","c5eab827":"markdown","e5c2a3d4":"markdown","50ef1dca":"markdown","8cc1edfd":"markdown","7c843488":"markdown","488b822b":"markdown","8c8da32b":"markdown","c7745281":"markdown","eb563a77":"markdown","66247595":"markdown","f88b221f":"markdown","4a00c6f1":"markdown","d795adf4":"markdown","ff87991c":"markdown","7c0735d8":"markdown","bcf61f9d":"markdown","0ac7b7c5":"markdown","1a178c61":"markdown","f45acb0f":"markdown","9d90b6cb":"markdown","9d74db3b":"markdown","a7b0ea98":"markdown","faac52ec":"markdown","55ce681b":"markdown","0ee8d78c":"markdown","4635b8a7":"markdown","6c7adf4e":"markdown","e4e75200":"markdown","b1a057d5":"markdown","78aab92d":"markdown","d0b2bb4d":"markdown","afa83e50":"markdown","412c963e":"markdown","d011eb2f":"markdown","f76a21db":"markdown","39684bb5":"markdown","11546ac2":"markdown","775c1dcd":"markdown","4db89243":"markdown","964480bf":"markdown","810c1fd3":"markdown","ace4cf23":"markdown"},"source":{"bfdb4907":"import numpy as np\nimport random\nimport pandas as pd\nfrom pandas.tools import plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected=True)  \nimport plotly.figure_factory as ff\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import  accuracy_score\n\n\nimport xgboost as xgb\nimport lightgbm as  lgb\nfrom xgboost.sklearn import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.preprocessing import StandardScaler, LabelBinarizer\n# auxiliary function\nfrom sklearn.preprocessing import LabelEncoder\ndef random_colors(number_of_colors):\n    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n                 for i in range(number_of_colors)]\n    return color\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","3eee648e":"df = pd.read_csv('..\/input\/Iris.csv')\ntable = ff.create_table(df.head())\n","194c0723":"py.iplot(table,filename='jupyter-table1')\n","d84ef542":"py.iplot(ff.create_table(df.describe()),filename='jupyter-table1')\n","54e60b3b":"df.info()\n","c120a428":"Species = df['Species'].unique()\nSpecies\n","35cd5341":"species_count = df['Species'].value_counts()\ndata = [go.Bar(\n    x = species_count.index,\n    y = species_count.values,\n    marker = dict(color = random_colors(3))\n)]\npy.iplot(data)\n","4449dd8a":"corelation = df.corr()\ndata = [go.Heatmap(z = np.array(corelation.values),\n                   x = np.array(corelation.columns),\n                   y = np.array(corelation.columns),\n                     colorscale='Blackbody',)\n       ]\npy.iplot(data)\n","60846691":"setosa = go.Scatter(x = df['SepalLengthCm'][df.Species =='Iris-setosa'], y = df['SepalWidthCm'][df.Species =='Iris-setosa']\n                   , mode = 'markers', name = 'setosa')\nversicolor = go.Scatter(x = df['SepalLengthCm'][df.Species =='Iris-versicolor'], y = df['SepalWidthCm'][df.Species =='Iris-versicolor']\n                   , mode = 'markers', name = 'versicolor')\nvirginica = go.Scatter(x = df['SepalLengthCm'][df.Species =='Iris-virginica'], y = df['SepalWidthCm'][df.Species =='Iris-virginica']\n                   , mode = 'markers', name = 'virginica')\ndata = [setosa, versicolor, virginica]\n\nfig = dict(data=data)\npy.iplot(fig, filename='styled-scatter')","c7e5d8ad":"setosa = go.Scatter(x = df['PetalLengthCm'][df.Species =='Iris-setosa'], y = df['PetalWidthCm'][df.Species =='Iris-setosa']\n                   , mode = 'markers', name = 'setosa')\nversicolor = go.Scatter(x = df['PetalLengthCm'][df.Species =='Iris-versicolor'], y = df['PetalWidthCm'][df.Species =='Iris-versicolor']\n                   , mode = 'markers', name = 'versicolor')\nvirginica = go.Scatter(x = df['PetalLengthCm'][df.Species =='Iris-virginica'], y = df['PetalWidthCm'][df.Species =='Iris-virginica']\n                   , mode = 'markers', name = 'virginica')\ndata = [setosa, versicolor, virginica]\n\nfig = dict(data=data)\npy.iplot(fig, filename='styled-scatter')","f66d3c23":"trace0 = go.Box(y=df['PetalWidthCm'][df['Species'] == 'Iris-setosa'],\n                boxmean=True, name = 'setosa')\n\ntrace1 = go.Box(y=df['PetalWidthCm'][df['Species'] == 'Iris-versicolor'],\n                boxmean=True, name = 'versicolor')\n\ntrace2 = go.Box(y=df['PetalWidthCm'][df['Species'] == 'Iris-virginica'],\n                boxmean=True, name = 'virginica')\n\ndata = [trace0, trace1, trace2]\npy.iplot(data)","474280cb":"trace0 = go.Box(y=df['PetalLengthCm'][df['Species'] == 'Iris-setosa'],name = 'setosa')\n\ntrace1 = go.Box(y=df['PetalLengthCm'][df['Species'] == 'Iris-versicolor'], name = 'versicolor')\n\ntrace2 = go.Box(y=df['PetalLengthCm'][df['Species'] == 'Iris-virginica'], name = 'virginica')\n\ndata = [trace0, trace1, trace2]\npy.iplot(data)","7cd76cd7":"trace0 = go.Box(y=df['SepalLengthCm'][df['Species'] == 'Iris-setosa'], name = 'setosa')\n\ntrace1 = go.Box(y=df['SepalLengthCm'][df['Species'] == 'Iris-versicolor'], name = 'versicolor')\n\ntrace2 = go.Box(y=df['SepalLengthCm'][df['Species'] == 'Iris-virginica'], name = 'virginica')\n\ndata = [trace0, trace1, trace2]\npy.iplot(data)","4e72d103":"setosa = go.Box(y=df['SepalWidthCm'][df['Species'] == 'Iris-setosa'])\n\nversicolor = go.Box(y=df['SepalWidthCm'][df['Species'] == 'Iris-versicolor'])\n\nvirginica = go.Box(y=df['SepalWidthCm'][df['Species'] == 'Iris-virginica'])\n\ndata = [trace0, trace1, trace2]\npy.iplot(data)","ac10368f":"plt.subplots(figsize = (10,8))\nplotting.andrews_curves(df.drop(\"Id\", axis=1), \"Species\")","b8245c65":"g = sns.lmplot(x=\"SepalWidthCm\", y=\"SepalLengthCm\", hue=\"Species\", data=df)","326de93c":"g = sns.lmplot(x=\"PetalWidthCm\", y=\"PetalLengthCm\", hue=\"Species\", data=df)","b9a966a4":"x = df[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\ny = df['Species']","00b856af":"encoder = LabelEncoder()\ny = encoder.fit_transform(y)","7ecf2df3":"y","dcf973cd":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 101)","c7613227":"lr_model = LogisticRegression()\nlr_model.fit(x_train,y_train)\nlr_predict = lr_model.predict(x_test)\n\nprint('Logistic Regression - ',accuracy_score(lr_predict,y_test))","65617561":"svm_model = SVC(kernel='linear')\nsvm_model.fit(x_train,y_train)\nsvc_predict = svm_model.predict(x_test)\n\nprint('SVM - ',accuracy_score(svc_predict,y_test))","88604868":"nb_model = GaussianNB()\nnb_model.fit(x_train,y_train)\nnb_predict = nb_model.predict(x_test)\n\nprint('Naive bayes - ',accuracy_score(nb_predict,y_test))","89db285d":"dt_model = DecisionTreeClassifier(max_leaf_nodes=3)\ndt_model.fit(x_train,y_train)\ndt_predict = dt_model.predict(x_test)\n\nprint('Decision Tree - ',accuracy_score(dt_predict,y_test))","a58f2de6":"rfc_model = RandomForestClassifier(max_depth=3)\nrfc_model.fit(x_train,y_train)\nrfc_predict = rfc_model.predict(x_test)\n\nprint('Random Forest - ',accuracy_score(rfc_predict,y_test))","ecfda430":"etc_model = ExtraTreesClassifier()\netc_model.fit(x_train,y_train)\netc_predict = etc_model.predict(x_test)\n\nprint('Extra Tree Classifier - ',accuracy_score(etc_predict,y_test))\n","83266800":"knn_model = KNeighborsClassifier(n_neighbors=3)\nknn_model.fit(x_train,y_train)\nknn_predict = knn_model.predict(x_test)\n\nprint('knn - ',accuracy_score(knn_predict,y_test))","33932798":"xg_model = xgb.XGBClassifier()\nxg_model = xg_model.fit(x_train,y_train)\nxg_model.score(x_test, y_test)","a9fff268":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense","14364a9d":"from sklearn.preprocessing import StandardScaler, LabelBinarizer\nX = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\ny = df['Species']\n\nX = StandardScaler().fit_transform(X)\ny = LabelBinarizer().fit_transform(y)","925d9908":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)","d40caa5c":"shallow_model = Sequential()\nshallow_model.add(Dense( 4, input_dim=4, activation = 'relu'))\nshallow_model.add(Dense( units = 10, activation= 'relu'))\nshallow_model.add(Dense( units = 3, activation= 'softmax'))\nshallow_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","2a5436eb":"shallow_history = shallow_model.fit(x_train, y_train, epochs = 150, validation_data = (x_test, y_test))","009031a3":"plt.plot(shallow_history.history['acc'])\nplt.plot(shallow_history.history['val_acc'])\nplt.title(\"Accuracy\")\nplt.legend(['train', 'test'])\nplt.show()","69477f4e":"plt.plot(shallow_history.history['loss'])\nplt.plot(shallow_history.history['val_loss'])\nplt.plot('Loss')\nplt.legend(['Train','Test'])\nplt.show()","45ef2f18":"deep_model = Sequential()\ndeep_model.add(Dense( 4, input_dim=4, activation = 'relu'))\ndeep_model.add(Dense( units = 10, activation= 'relu'))\ndeep_model.add(Dense( units = 10, activation= 'relu'))\ndeep_model.add(Dense( units = 10, activation= 'relu'))\ndeep_model.add(Dense( units = 10, activation= 'relu'))\ndeep_model.add(Dense( units = 10, activation= 'relu'))\ndeep_model.add(Dense( units = 10, activation= 'relu'))\ndeep_model.add(Dense( units = 10, activation= 'relu'))\ndeep_model.add(Dense( units = 10, activation= 'relu'))\ndeep_model.add(Dense( units = 3, activation= 'softmax'))\ndeep_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","85bb3272":"deep_history = deep_model.fit(x_train, y_train, epochs = 150, validation_data = (x_test, y_test))","95f4bb37":"plt.plot(deep_history.history['acc'])\nplt.plot(deep_history.history['val_acc'])\nplt.title(\"Accuracy\")\nplt.legend(['train', 'test'])\nplt.show()","f7c80c0d":"plt.plot(deep_history.history['loss'])\nplt.plot(deep_history.history['val_loss'])\nplt.plot('Loss')\nplt.legend(['Train','Test'])\nplt.show()","0ba2af74":"# <a id='load_data'>Load data set<\/a>","cfa170a8":"# <a id='dl'>Deep Learning<\/a><br>","26f9210e":"<a href='#'>Preface<\/a><br>\n<a href='#desc'>description<\/a><br>\n<a href='#about'>About notebook<\/a><br>\n<a href='#load_lib'>Load libraries<\/a><br>\n<a href='#load_data'>Load Dataset<\/a><br>\n<a href='#visual'>Let's Visualize the dataset<\/a><br>\n- <a href='#types_species'> Types of species<\/a>\n- <a href='#corelation'>Corelation between features<\/a><br>\n- <a href='#visual_sepal'>Visualizing species based on sepal length and width<\/a><br>\n- <a href='#visual_petal'>Visualizing species based on petal length and width<\/a><br>\n- <a href='#value_petal_width'>Values distribution based on petal width<\/a><br>\n- <a href='#value_petal_length'>Values distribution based on petal length<\/a><br>\n- <a href='#value_sepal_length'>Values distribution based on sepal length<\/a><br>\n- <a href='#value_sepal_width'>Values distribution based on sepal width<\/a><br>\n- <a href='#andrew'>Andrew curves<\/a><br>\n- <a href='#lin_sepal'>Linear regression based on sepal<\/a><br>\n- <a href='#lin_petal'>Linear regression based on petal<\/a><br>\n\n<a href='#ml'>Machine Learning<\/a><br>\n- <a href='#list'> List of algorithms<\/a>\n- <a href='#logistic'>Logistic regression<\/a><br>\n- <a href='#decision'>Decision tree<\/a><br>\n- <a href='#knn'>KNN<\/a><br>\n- <a href='#svm'>SVM<\/a><br>\n- <a href='#nbc'>Naive Bayes Classification<\/a><br>\n- <a href='#random'>Random forest<\/a><br>\n- <a href='#etc'>Extra Tree Classifier<\/a><br>\n- <a href='#xgboost'>XGBoost<\/a><br>\n- <a href='#lbgm'>LigthGBM<\/a><br>\n\n<a href='#dl'>Deep Learning<\/a><br>\n- <a href='#shallow'>Shallow Deep learning<\/a><br>\n- <a href='#deep'>Deep Deep learning<\/a><br>\n","97c1c08b":"<img src=\"https:\/\/i.imgur.com\/e7MIgXk.png\">","e34d048c":"<img src = \"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*TudH6YvvH7-h5ZyF2dJV2w.jpeg\">","0a6336b2":"**\u201cSupport Vector Machine\u201d (SVM)** is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes very well.\n\nSupport Vectors are simply the co-ordinates of individual observation. Support Vector Machine is a frontier which best segregates the two classes (hyper-plane\/ line).\n(https:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/understaing-support-vector-machine-example-code\/)\n","6472e649":"# <a id='lin_sepal'>Linear regression based on sepal<\/a><br>","0b56859b":"# <a id='logistic'>Logistic regression<\/a><br>","60136be9":"\n(https:\/\/machinelearningmastery.com\/ensemble-machine-learning-algorithms-python-scikit-learn\/)","5e8353de":"# <a id='load_lib'>Let's load the required libraries<\/a>\n","f4cb4890":"# <a id='value_sepal_width'>Values distribution based on sepal width<\/a><br>","0efb74a2":"# <a id='value_petal_width'>Values distribution based on petal width<\/a><br>","bdb4674f":"# <a id='lin_petal'>Linear regression based on petal<\/a><br>","d89bdbd6":"So our shallow model is  good accurate.","d03983bf":"# <a id='corelation'>Corelation between features<\/a><br>","d04ff257":"The Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the UCI Machine Learning Repository.\n\nIt includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n\nThe columns in this dataset are:\n\n- Id\n- SepalLengthCm\n- SepalWidthCm\n- PetalLengthCm\n- PetalWidthCm\n- Species","c731b5f5":"# <a id='xgboost'>XGBoost<\/a><br>","958351b0":"# <a id='svm'>SVM<\/a><br>","5c319f48":"The process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide. The primary aim is to allow the computers learn automatically without human intervention or assistance and adjust actions accordingly.\n\nPlease go through the blog for in-depth description of machine learning\nhttps:\/\/www.expertsystem.com\/machine-learning-definition\/","2a0fd958":"- From the above four graph you can see that the distribution of setosa < vericolor < virginica\n- There are few outliers which can be explained by the scatter plot graph.","57f8edcb":"# <a id='visual'>Now lets start visualizing the data set<\/a>","351d781d":"So by the defination we see that we need data and we do have the data (Iris dataset).\nBut how will we test the dataset ?","3394dec6":"# <a id='random'>Random forest<\/a><br>","4a4d10a2":"# <a id='list'> List of algorithms<\/a>","a0722f8a":"# <a id='decision'>Decision tree<\/a><br>","58a7fcf1":"so there is no null values available in the data set\n","54f9e0be":"<img src = \"https:\/\/annalyzin.files.wordpress.com\/2016\/07\/decision-trees-titanic-tutorial.png\">","f875939e":"Again based on petal we can easily classify setosa and for versicolor and virginica also we can classify but there is a thin line which should be taken care of","c5eab827":"As you can see Iris-setosa Iris-versicolor Iris-virginica are converted to 0, 1, 2 respectively","e5c2a3d4":"In this notebook we will look into famous dataset which is iris, we will analyse the dataset with plotly library which is very interactive library in python then later we will apply different macine learning algorithms and see the best accuracy.\n\n","50ef1dca":"# <a href='etc'>Extra Tree Classifier<\/a><br>","8cc1edfd":"**Logistic regression** is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes).\n(https:\/\/www.medcalc.org\/manual\/logistic_regression.php)","7c843488":"Lets create a regression plot for both petal and sepal","488b822b":"So our deep model is more accurate than the shallow model.","8c8da32b":"Andrews curves are a method for visualizing multidimensional data by mapping each observation onto a function.\n\nSource - https:\/\/dzone.com\/articles\/andrews-curves","c7745281":"# <a id='about'>About the notebook<\/a>\n","eb563a77":"given the coloums are<br>\nSepalLengthCm<br>\nSepalWidthCm<br>\nPetalLengthCm<br>\nPetalWidthCm<br>\nSpecies<br>","66247595":"# <a id='value_sepal_length'>Values distribution based on sepal length<\/a><br>","f88b221f":"<img src=\"https:\/\/i1.wp.com\/dataaspirant.com\/wp-content\/uploads\/2017\/04\/Random-Forest-Introduction.jpg?resize=690%2C345\">","4a00c6f1":"# <a id='desc'> Description<\/a>\n","d795adf4":"**Decision tree** is a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used in classification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter \/ differentiator in input variables.\n(https:\/\/www.analyticsvidhya.com\/blog\/2016\/04\/complete-tutorial-tree-based-modeling-scratch-in-python\/)","ff87991c":"**K nearest neighbors** is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). KNN has been used in statistical estimation and pattern recognition already in the beginning of 1970's as a non-parametric technique.\n(https:\/\/www.analyticsvidhya.com\/blog\/2018\/03\/introduction-k-neighbours-algorithm-clustering\/)","7c0735d8":"**Random Forest** is considered to be a panacea of all data science problems. On a funny note, when you can\u2019t think of any algorithm (irrespective of situation), use random forest!\n\nRandom Forest is a versatile machine learning method capable of performing both regression and classification tasks. It also undertakes dimensional reduction methods, treats missing values, outlier values and other essential steps of data exploration, and does a fairly good job. It is a type of ensemble learning method, where a group of weak models combine to form a powerful model.\n\n(https:\/\/www.analyticsvidhya.com\/blog\/2016\/04\/complete-tutorial-tree-based-modeling-scratch-in-python\/#nine)","bcf61f9d":"**Naive Bayes** is a simple, yet effective and commonly-used, machine learning classifier. It is a probabilistic classifier that makes classifications using the Maximum A Posteriori decision rule in a Bayesian setting. It can also be represented using a very simple Bayesian network. Naive Bayes classifiers have been especially popular for text classification, and are a traditional solution for problems such as spam detection.\n(https:\/\/towardsdatascience.com\/introduction-to-naive-bayes-classification-4cffabb1ae54)","0ac7b7c5":"# <a id='shallow'>Shallow Deep learning<\/a>","1a178c61":"Spliting the data into train - 70% and test - 30%","f45acb0f":"# <a id='types_species'> Types of Species<\/a>","9d90b6cb":"#  <a id='visual_petal'>Visualizing species based on petal length and width<\/a><br>","9d74db3b":"Since it is a classification problem we will be using<br>\nLogistic regression<br>\nDecision tree<br>\nKNN<br>\nSVM<br>\nNaive Bayes Classification<br>\nRandom forest<br>\nXGBoost<br>\nLightGBM<br>","a7b0ea98":"<img src=\"http:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1531424125\/Knn_k1_z96jba.png\">","faac52ec":"# <a id='nbc'>Naive Bayes Classification<\/a><br>","55ce681b":"# <a id='visual_sepal'>Visualizing species based on Sepal length and width<\/a><br>","0ee8d78c":"<img src = \"https:\/\/helloacm.com\/wp-content\/uploads\/2016\/03\/Bayes_rule.png\">","4635b8a7":"First we are splitting the data set into training data and testing data which is 7:3 ratio ","6c7adf4e":"# <a id='ml'>What is machine learning ?<\/a><br> ","e4e75200":"We can easily differentiate setosa based on Sepal but for versicolor and virginica its difficult because the data is scattred.","b1a057d5":"# Stay tune as the algorithms are learning. Hold tight \n\n# Upvote if you like ","78aab92d":"<img src = \"https:\/\/image.slidesharecdn.com\/logitregression-161121215510\/95\/intro-to-logistic-regression-4-638.jpg?cb=1479765630\">","d0b2bb4d":"<img src=\"https:\/\/static.vecteezy.com\/system\/resources\/previews\/000\/145\/921\/non_2x\/vector-iris-flower-banner-line-art.jpg\">\n","afa83e50":"Best place to understand deep learning.\nPlease follow the blog\n\nhttps:\/\/machinelearningmastery.com\/what-is-deep-learning\/","412c963e":"So there are three types of species \n\nIris-setosa<br\/>\nIris-versicolor<br\/>\nIris-virginica<br\/>","d011eb2f":"# <a id='knn'>KNN<\/a><br>","f76a21db":"# <a id='value_petal_length'>Values distribution based on petal length<\/a><br>","39684bb5":"For that we will split out data set into three parts train, test, validation sets.<br>\nwe are going to use the scikit-learn library which has all the required functions and machine learning algorithms required for this notebook","11546ac2":"We have seen the visualization part\n<br>\nNow lets see the how to apply machine learning to the dataset","775c1dcd":"Before we split our data lets look at the output we want to predict.<br> \nWe want to predict the given sepal and petal dimensions follows to which type of species.<br>\nwe have 3 type of species  Iris-setosa Iris-versicolor Iris-virginica.<br>\nWe will convert those species names to a categorical values using label encoding.<br>","4db89243":"The beauty of this powerful algorithm lies in its scalability, which drives fast learning through parallel and distributed computing and offers efficient memory usage.\n\nIt\u2019s no wonder then that CERN recognized it as the best approach to classify signals from the Large Hadron Collider. This particular challenge posed by CERN required a solution that would be scalable to process data being generated at the rate of 3 petabytes per year and effectively distinguish an extremely rare signal from background noises in a complex physical process. XGBoost emerged as the most useful, straightforward and robust solution.\n\n(https:\/\/www.analyticsvidhya.com\/blog\/2018\/09\/an-end-to-end-guide-to-understand-the-math-behind-xgboost\/)","964480bf":"# <a id='deep'>Deep Deep learning<\/a>","810c1fd3":"# <a id='andrew'>Andrew curves<\/a><br>","ace4cf23":"So we have equally distributed species all are of 50"}}