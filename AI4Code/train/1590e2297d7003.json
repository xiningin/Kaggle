{"cell_type":{"c21487c5":"code","2e1c9616":"code","ac4447bd":"code","d2ecd483":"code","5338bf08":"code","a9abaaa7":"code","f54cf6c6":"code","b8f754c5":"code","c3767110":"code","49ff9aad":"code","8e74efb0":"code","e52c61e0":"code","3c0a6273":"code","6c87c6cd":"code","89ac6a18":"code","e836361f":"code","a3c6e464":"code","f54b2119":"code","752654a7":"code","bef18a7c":"markdown","83b12cf2":"markdown","4634e572":"markdown","795d1d92":"markdown","a8253ee9":"markdown","9739e98d":"markdown","030c928b":"markdown","4a2118e2":"markdown","82f9a1e6":"markdown","545ce3bf":"markdown","b2559546":"markdown","52edc972":"markdown","174fe985":"markdown","17d5e9d5":"markdown","2e82829b":"markdown","14dc67b0":"markdown","86f2aba0":"markdown","97570423":"markdown","1adbae43":"markdown","20b14f1b":"markdown","a8080894":"markdown","677a8b51":"markdown","964aad6d":"markdown","36d4e0e3":"markdown","dc77e87c":"markdown","a57d6b9a":"markdown","184eb9cb":"markdown"},"source":{"c21487c5":"import torch\n\nconfig = {\n    'batch_size': 8,\n    'best_pretrained_roberta_folder': '..\/input\/pretrain-roberta-large-on-clrp-data\/clrp_roberta_large\/best_model\/',\n    'num_of_folds': 5,\n    'num_of_models': 5,\n    'seed': 2021,\n    'sentence_max_length': 256\n}\n\nfor (k, v) in config.items():\n    print(f\"The value for {k}: {v}\")\n    \ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nprint(f\"Device: {device}\")","2e1c9616":"import torch\n\nclass ReadabilityDataset(torch.utils.data.Dataset):\n    \"\"\"Custom dataset for the Readability task\"\"\"\n    def __init__(self, encodings):\n        self.encodings = encodings\n    \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        return item\n        \n    def __len__(self):\n        return len(self.encodings['input_ids'])\n    \n\nprint(ReadabilityDataset.__doc__)","ac4447bd":"from torch import nn\n\nclass AttentionHead(nn.Module):\n    \"\"\"Class implementing the attention head of the model.\"\"\"\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n       \n    \n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        \n        return torch.sum(context_vector, dim=1)\n\n\nprint(AttentionHead.__doc__)","d2ecd483":"from torch import nn\nfrom transformers import RobertaModel\nfrom transformers import RobertaConfig\n\nclass ReadabilityRobertaModel(nn.Module):\n    \"\"\"Custom model for the Readability task containing a Roberta layer and a custom NN head.\"\"\"\n        \n    def __init__(self):\n        super(ReadabilityRobertaModel, self).__init__()\n        \n        self.model_config = RobertaConfig.from_pretrained(config['best_pretrained_roberta_folder'])\n        self.model_config.update({\n            \"output_hidden_states\": True,\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })\n        \n        self.roberta = RobertaModel.from_pretrained(config['best_pretrained_roberta_folder'],\n                                                    config=self.model_config)\n        self.attention_head = AttentionHead(self.model_config.hidden_size, \n                                            self.model_config.hidden_size)\n        self.dropout = nn.Dropout(0.1)\n        self.regressor = nn.Linear(self.model_config.hidden_size, 1)\n        \n        \n    def forward(self, tokens, attention_mask):\n        x = self.roberta(input_ids=tokens, attention_mask=attention_mask)[0]\n        x = self.attention_head(x)\n\n        return x\n    \n    \n    def freeze_roberta(self):\n        \"\"\"\n        Freezes the parameters of the Roberta model so when ReadabilityRobertaModel is \n        trained only the wieghts of the custom regressor are modified.\n        \"\"\"\n        for param in self.roberta.named_parameters():\n            param[1].requires_grad=False\n    \n    def unfreeze_roberta(self):\n        \"\"\"\n        Unfreezes the parameters of the Roberta model so when ReadabilityRobertaModel is \n        trained both the wieghts of the custom regressor and of the underlying Roberta\n        model are modified.\n        \"\"\"\n        for param in self.roberta.named_parameters():\n            param[1].requires_grad=True\n\n    \nprint(ReadabilityRobertaModel.__doc__)","5338bf08":"from torch import nn\nfrom transformers import RobertaModel\nfrom transformers import RobertaConfig\n\nclass ReadabilityRobertaModelWithCustomNNHead(nn.Module):\n    \"\"\"Custom model for the Readability task containing a Roberta layer and a custom NN head.\"\"\"\n        \n    def __init__(self):\n        super(ReadabilityRobertaModelWithCustomNNHead, self).__init__()\n        \n        self.model_config = RobertaConfig.from_pretrained(config['best_pretrained_roberta_folder'])\n        self.model_config.update({\n            \"output_hidden_states\": True,\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })\n        \n        self.roberta = RobertaModel.from_pretrained(config['best_pretrained_roberta_folder'],\n                                                    config=self.model_config)\n        self.attention_head = AttentionHead(self.model_config.hidden_size, \n                                            self.model_config.hidden_size)\n        self.dropout = nn.Dropout(0.1)\n        self.regressor = nn.Linear(self.model_config.hidden_size, 1)\n        \n        \n    def forward(self, tokens, attention_mask):\n        x = self.roberta(input_ids=tokens, attention_mask=attention_mask)[0]\n        x = self.attention_head(x)\n        x = self.dropout(x)\n        x = self.regressor(x)\n        return x\n    \n    \n    def freeze_roberta(self):\n        \"\"\"\n        Freezes the parameters of the Roberta model so when ReadabilityRobertaModel is \n        trained only the wieghts of the custom regressor are modified.\n        \"\"\"\n        for param in self.roberta.named_parameters():\n            param[1].requires_grad=False\n    \n    def unfreeze_roberta(self):\n        \"\"\"\n        Unfreezes the parameters of the Roberta model so when ReadabilityRobertaModel is \n        trained both the wieghts of the custom regressor and of the underlying Roberta\n        model are modified.\n        \"\"\"\n        for param in self.roberta.named_parameters():\n            param[1].requires_grad=True\n\n    \nprint(ReadabilityRobertaModelWithCustomNNHead.__doc__)","a9abaaa7":"import pandas as pd\n\ntrain_csv_path = '\/kaggle\/input\/commonlitreadabilityprize\/train.csv'\ntrain_data = pd.read_csv(train_csv_path)\n\nprint('The total # of samples is {}.'.format(len(train_data)))","f54cf6c6":"import numpy as np\n\n# create & fill bins column (needed for kfold)\nnum_of_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:,'bin'] = pd.cut(train_data['target'], bins=num_of_bins, labels=False)\nbins = train_data['bin'].to_numpy()\n\ntarget = train_data['target'].to_numpy()","b8f754c5":"import pandas as pd\n\ntest_csv_path = '\/kaggle\/input\/commonlitreadabilityprize\/test.csv'\ntest_data = pd.read_csv(test_csv_path)\n\nprint('The total # of samples is {}.'.format(test_data.shape[0]))","c3767110":"import numpy as np\n\nfrom sklearn.metrics import mean_squared_error\n\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","49ff9aad":"from torch.utils.data import DataLoader\n\ndef get_dataloader_from_dataframes(df, tokenizer):\n    \"\"\"Converts a complete dataframe (with all columns included) into a dataloader.\"\"\"\n    texts = df['excerpt'].values.tolist()\n    data_encodings = tokenizer(texts, max_length=config['sentence_max_length'],\n                              truncation=True, padding=True)\n    dataset = ReadabilityDataset(data_encodings)\n    dataloader = DataLoader(dataset, batch_size=config['batch_size'])\n    \n    return dataloader\n\nprint(get_dataloader_from_dataframes.__doc__)","8e74efb0":"import numpy as np\nimport torch\n\nfrom tqdm.auto import tqdm\n\ntqdm.pandas()\n\ndef get_embeddings_from_model(model_path, data, tokenizer):\n    \"\"\"Get embeddings (which can then be fed to regressors) using the provided model.\"\"\"\n    \n    # Setup model\n    model = ReadabilityRobertaModel()\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.to(device)\n    model.eval()\n    \n    # create dataloader from data\n    dataloader = get_dataloader_from_dataframes(data, tokenizer)\n    \n    # compute embeddings\n    embeddings = list()\n    with torch.no_grad():\n        print('Getting embeddings:')\n        for i, batch in enumerate(tqdm(dataloader)):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n        \n            output = model(tokens=input_ids, attention_mask=attention_mask)\n            output = output.cpu().detach().numpy()\n            embeddings.extend(output)\n            \n    return np.array(embeddings)\n        \n    \nprint(get_embeddings_from_model.__doc__)","e52c61e0":"import numpy as np\n\nfrom sklearn.model_selection import StratifiedKFold\n\n\ndef get_predictions_with_regressor(X, y, X_test, bins, regressor):\n    \"\"\"This method uses SVM on top of ROberta to predict the readibility score of a text.\"\"\"\n    scores = list()\n    preds = np.zeros(len(X_test))\n    kfold = StratifiedKFold(n_splits=config['num_of_folds'], shuffle=True,\n                            random_state=config['seed'])\n    \n    print('Getting predictions:')\n    for k, (train_idx, test_idx) in enumerate(kfold.split(X, bins)):\n                \n        X_train, y_train = X[train_idx], y[train_idx]\n        X_valid, y_valid = X[test_idx], y[test_idx]\n        \n        regressor.fit(X_train, y_train)\n        prediction = regressor.predict(X_valid)\n        score = rmse_score(y_valid, prediction)\n        print(f'Fold {k} rmse_score: {score}')\n        scores.append(score)\n        preds += regressor.predict(X_test)\n        \n        \n    print(f'Mean rmse: {np.mean(scores)}')\n    return np.array(preds) \/ config['num_of_folds']\n\n\nprint(get_predictions_with_regressor.__doc__)","3c0a6273":"import torch\n\nfrom tqdm.auto import tqdm\n\ntqdm.pandas()\n\n\ndef get_predictions_with_custom_NN_head(model_path, data, tokenizer):\n    \n    # setup model\n    model = ReadabilityRobertaModelWithCustomNNHead()\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.to(device)\n    model.eval()\n    \n    # convert data into dataloader\n    dataloader = get_dataloader_from_dataframes(data, tokenizer)\n    \n    # iteration for predictions\n    predictions = list()\n    for i, batch in enumerate(tqdm(dataloader)):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        \n        output = torch.flatten(model(tokens=input_ids, attention_mask=attention_mask))\n        output = output.cpu().detach().numpy().tolist()\n        \n        predictions.extend(output)\n        \n    torch.cuda.empty_cache()\n    return predictions","6c87c6cd":"from sklearn.svm import SVR\n\nsvr_regressor = SVR(C=10, kernel='rbf', gamma='auto')\n\nprint('SVR regressor has been initialized.')","89ac6a18":"from sklearn.linear_model import Ridge\n\nridge_regressor = Ridge(alpha=50.0)\n\nprint('Ridge regressor has been initialized')","e836361f":"import numpy as np\nfrom tqdm.auto import tqdm\n\nfrom transformers import RobertaTokenizer\n\ntqdm.pandas()\n\n\ndef run_all_models_with_regressor(regressor):\n    predictions = np.zeros(test_data.shape[0])\n\n    for i in tqdm(range(config['num_of_models'])):\n        print(f'Model # {i}:')\n        model_path = f'..\/input\/roberta-large-k-fold-models\/model{i}\/model{i}.bin'\n        tokenizer_path = f'..\/input\/roberta-large-k-fold-models\/model{i}\/'\n        tokenizer = RobertaTokenizer.from_pretrained(tokenizer_path)\n\n        train_embeddings = get_embeddings_from_model(model_path, train_data, tokenizer)\n        test_embeddings = get_embeddings_from_model(model_path, test_data, tokenizer)\n\n        preds = get_predictions_with_regressor(train_embeddings, target, test_embeddings,\n                                               bins, regressor)\n\n        predictions = predictions + preds\n        print(f'Predictions for model {i}:')\n        print(preds)\n\n\n    predictions = predictions \/ config['num_of_models']\n    print('Final predictions:')\n    print(predictions)\n    \n    return predictions","a3c6e464":"import numpy as np\nfrom tqdm.auto import tqdm\n\nfrom transformers import RobertaTokenizer\n\ntqdm.pandas()\n\n\ndef run_all_models_with_custom_NN_head():\n    predictions = np.zeros(test_data.shape[0])\n\n    for i in tqdm(range(config['num_of_models'])):\n        print(f'Model # {i}:')\n        model_path = f'..\/input\/roberta-large-k-fold-models\/model{i}\/model{i}.bin'\n        tokenizer_path = f'..\/input\/roberta-large-k-fold-models\/model{i}\/'\n        tokenizer = RobertaTokenizer.from_pretrained(tokenizer_path)\n\n        train_embeddings = get_embeddings_from_model(model_path, train_data, tokenizer)\n        test_embeddings = get_embeddings_from_model(model_path, test_data, tokenizer)\n\n        preds = get_predictions_with_custom_NN_head(model_path, test_data, tokenizer)\n\n        predictions = predictions + preds\n        print(f'Predictions for model {i}:')\n        print(preds)\n\n\n    predictions = predictions \/ config['num_of_models']\n    print('Final predictions:')\n    print(predictions)\n    \n    return predictions","f54b2119":"svr_predictions = run_all_models_with_regressor(svr_regressor) \nridge_predictions =  run_all_models_with_regressor(ridge_regressor)\ncustom_NN_head_predictions = run_all_models_with_custom_NN_head()\n\npredictions = (svr_predictions + ridge_predictions + custom_NN_head_predictions) \/ 3\n\nprint(predictions)","752654a7":"submission = pd.DataFrame()\nsubmission['id'] = test_data['id']\nsubmission['target'] = predictions\nsubmission.to_csv('submission.csv', index=False)\n\nprint('Saved predictions.')","bef18a7c":"<a id=classes_model><\/a>\n## Model\n[[back to top]](#toc)","83b12cf2":"<a id='introduction'><\/a>\n# Introduction\n\nThis offline notebook uses [my other notebook](https:\/\/www.kaggle.com\/angyalfold\/roberta-large-k-fold-models\/)'s model and tokenizer to make predictions. The idea here is to use \nRoberta large + SVM regressor, Roberta large + ridge regressor, Roberta large + custom NN regressor and combine the results (ensemble). Each prediction would use k=5 pre-trained model for making predictions. The pre-trained models were created in [my second notebook](https:\/\/www.kaggle.com\/angyalfold\/roberta-large-k-fold-models) \n\nThis notebook is part of a series:\n1. Pretrain roberta large on the CommonLit dataset [here](https:\/\/www.kaggle.com\/angyalfold\/pretrain-roberta-large-on-clrp-data\/).\n2. Produce k models which can later be used for determining the readability of texts [here](https:\/\/www.kaggle.com\/angyalfold\/roberta-large-k-fold-models).\n3. Make predictions with a custom NN regressor [here](https:\/\/www.kaggle.com\/angyalfold\/roberta-large-with-custom-regressor-pytorch\/).\n4. Ensemble (Roberta large + SVR, Roberta large + Ridge, Roberta large + custom NN head) (this notebook)\n\nI use Maunish' [notebook](https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm\/) as reference.","4634e572":"<a id='make_predictions_run_models_with_nn'><\/a>\n## Run models with NN regressor\n[[back to top]](#toc)","795d1d92":"<a id='setup_prediction_with_nn'><\/a>\n## Get predictions with custom NN regressor\n[[back to top]](#toc)","a8253ee9":"Note, that the concept of attention is awesomely explained in [Lena Voita](https:\/\/lena-voita.github.io\/)'s excellent notebook [here](https:\/\/lena-voita.github.io\/nlp_course\/seq2seq_and_attention.html).","9739e98d":"<a id='setup_prediction_with_regressor'><\/a>\n## Get predictions with a given regressor\n[[back to top]](#toc)","030c928b":"<a id='setup_rmse_score'><\/a>\n## RMSE score\n[[back to top]](#toc)","4a2118e2":"<a id='setup_dataframe_to_dataloader'><\/a>\n## Convert pandas' dataframes to dataloader\n[[back to top]](#toc)","82f9a1e6":"Add values to the *bin* column as described [here](https:\/\/www.kaggle.com\/angyalfold\/roberta-large-k-fold-models\/).","545ce3bf":"<a id='read_data_train_data'><\/a>\n## Read train data\n[[back to top]](#toc)","b2559546":"<a id='setup_ridge'><\/a>\n## Ridge regressor\n[[back to top]](#toc)\n\nRidge regressor: alpha = 50 is taken from [this notebook](https:\/\/www.kaggle.com\/solorzano\/clrp-roberta-ridge).","52edc972":"<a id='setup_svr'><\/a>\n## SVR regressor\n[[back to top]](#toc)","174fe985":"<a id='classes'><\/a>\n# Classes & configs\n[[back to top]](#toc)","17d5e9d5":"<a id='save_results'><\/a>\n# Save results\n[[back to top]](#toc)","2e82829b":"<a id='NN'><\/a>\n## Get predictions\n[[back to top]](#toc)","14dc67b0":"<a id=\"toc\"><\/a>\n# Table of contents\n* [Introduction](#introduction)\n* [Classes & configs](#classes)\n    * [Configs](#classes_config)\n    * [Data set](#classes_data_set)\n    * [Model](#classes_model)\n* [Read data](#read_data)\n    * [Read train data](#read_data_train_data)\n    * [Read test data](#read_data_test_data)\n* [Setup](#setup)\n    * [RMSE score](#setup_rmse_score)\n    * [Convert pandas' dataframes to dataloader](#setup_dataframe_to_dataloader)\n    * [Get embeddings from model](#setup_embeddings_from_model)\n    * [Get predictions with a given regressor](#setup_prediction_with_regressor)\n    * [Get predictions with custom NN regressor](#setup_prediction_with_nn)\n    * [SVR](#setup_svr)\n    * [Ridge regressor](#setup_ridge)\n* [Make predictions](#make_predictions)\n    * [Run models with regressor](#make_predictions_run_models_with_regressor)\n    * [Run models with NN](#make_predictions_run_models_with_nn)\n    * [Get predictions](#make_predictions_get_predictions)\n* [Save results](#save_results)","86f2aba0":"<a id='classes_config'><\/a>\n## Config\n[[back to top]](#toc)","97570423":"<a id='make_predictions_run_models_with_regressor'><\/a>\n## Run models with regressor\n[[back to top]](#toc)","1adbae43":"<a id='read_data'><\/a>\n# Read data\n[[back to top]](#toc)","20b14f1b":"<a id='read_data_test_data'><\/a>\n## Read test data\n[[back to top]](#toc)","a8080894":"<a id='make_predictions'><\/a>\n# Make predictions\n[[back to top]](#toc)","677a8b51":"On top of *ReadabilityRobertaModel* a regressor, such as SVM or Ridge could be placed. That way a regressor could assign a continous value to the output of *ReadabilityRobertaModel*.","964aad6d":"<a id='setup'><\/a>\n# Setup\n[[back to top]](#toc)\n\nIn the following I collected some helper methods and variables which will be needed for making the actual predictions.","36d4e0e3":"<a id=classes_data_set><\/a>\n## Data set\n[[back to top]](#toc)","dc77e87c":"Unlike *ReadabilityRobertaModel*, *ReadabilityRobertaModelWithCustomNNHead* returns with the actual predicted value without the need for a further regressor (i.e.: the last layer of this NN functions as a regressor).","a57d6b9a":"<a id='setup_embeddings_from_model'><\/a>\n## Get embeddings from model\n[[back to top]](#toc)","184eb9cb":"## Model with custom NN regressor"}}