{"cell_type":{"82cdeb11":"code","523e564e":"code","62cc3d19":"code","b8b19e05":"code","8a3259b9":"code","9f7bf21f":"code","a6430908":"code","d5749d54":"code","052dad28":"code","3fb5bed1":"code","07965671":"code","1954f626":"markdown","d64e377d":"markdown","e2a77690":"markdown","85f197a4":"markdown","5e7a3199":"markdown","3e9e0c61":"markdown","36f83f71":"markdown"},"source":{"82cdeb11":"import numpy as np # linear algebra\nimport matplotlib.pyplot as plt\nimport os.path\nimport gc\nimport pickle, gzip\n\n\ndef get_MNIST_data():\n    \"\"\"\n    Reads mnist dataset from file\n\n    Returns:\n        train_x - 2D Numpy array (n, d) where each row is an image\n        train_y - 1D Numpy array (n, ) where each row is a label\n        test_x  - 2D Numpy array (n, d) where each row is an image\n        test_y  - 1D Numpy array (n, ) where each row is a label\n\n    \"\"\"\n    f = gzip.open(r\"..\/input\/mnist.pkl.gz\", 'rb')\n    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n    train_x, train_y = train_set\n    valid_x, valid_y = valid_set\n    train_x = np.vstack((train_x, valid_x))\n    train_y = np.append(train_y, valid_y)\n    test_x, test_y = test_set\n    return (train_x, train_y, test_x, test_y)\n\ntrain_x, train_y, test_x, test_y = get_MNIST_data()","523e564e":"# PCA utilities\ndef project_onto_PC(X, pcs, n_components):\n    \"\"\"\n    Given principal component vectors pcs = principal_components(X)\n    this function returns a new data array in which each sample in X\n    has been projected onto the first n_components principcal components.\n    \"\"\"\n    # TODO: first center data using the centerData() function.\n    # TODO: Return the projection of the centered dataset\n    #       on the first n_components principal components.\n    #       This should be an array with dimensions: n x n_components.\n    # Hint: these principal components = first n_components columns\n    #       of the eigenvectors returned by principal_components().\n    #       Note that each eigenvector is already be a unit-vector,\n    #       so the projection may be done using matrix multiplication.\n    return (pcs[:,:n_components].T@center_data(X).T).T\n\ndef center_data(X):\n    \"\"\"\n    Returns a centered version of the data, where each feature now has mean = 0\n\n    Args:\n        X - n x d NumPy array of n data points, each with d features\n\n    Returns:\n        n x d NumPy array X' where for each i = 1, ..., n and j = 1, ..., d:\n        X'[i][j] = X[i][j] - means[j]\n    \"\"\"\n    feature_means = X.mean(axis=0)\n    return(X - feature_means)\n\n\ndef principal_components(X):\n    \"\"\"\n    Returns the principal component vectors of the data, sorted in decreasing order\n    of eigenvalue magnitude. This function first caluclates the covariance matrix\n    and then finds its eigenvectors.\n\n    Args:\n        X - n x d NumPy array of n data points, each with d features\n\n    Returns:\n        d x d NumPy array whose columns are the principal component directions sorted\n        in descending order by the amount of variation each direction (these are\n        equivalent to the d eigenvectors of the covariance matrix sorted in descending\n        order of eigenvalues, so the first column corresponds to the eigenvector with\n        the largest eigenvalue\n    \"\"\"\n    centered_data = center_data(X)  # first center data\n    scatter_matrix = np.dot(centered_data.transpose(), centered_data)\n    eigen_values, eigen_vectors = np.linalg.eig(scatter_matrix)\n    # Re-order eigenvectors by eigenvalue magnitude:\n    idx = eigen_values.argsort()[::-1]\n    eigen_values = eigen_values[idx]\n    eigen_vectors = eigen_vectors[:, idx]\n    return eigen_vectors\n\n\ndef plot_PC(X, pcs, labels):\n    \"\"\"\n    Given the principal component vectors as the columns of matrix pcs,\n    this function projects each sample in X onto the first two principal components\n    and produces a scatterplot where points are marked with the digit depicted in\n    the corresponding image.\n    labels = a numpy array containing the digits corresponding to each image in X.\n    \"\"\"\n    pc_data = project_onto_PC(X, pcs, n_components=2)\n    text_labels = [str(z) for z in labels.tolist()]\n    fig, ax = plt.subplots()\n    ax.scatter(pc_data[:, 0], pc_data[:, 1], alpha=0, marker=\".\")\n    for i, txt in enumerate(text_labels):\n        ax.annotate(txt, (pc_data[i, 0], pc_data[i, 1]))\n    ax.set_xlabel('PC 1')\n    ax.set_ylabel('PC 2')\n    plt.show()\n\n\ndef reconstruct_PC(x_pca, pcs, n_components, X):\n    \"\"\"\n    Given the principal component vectors as the columns of matrix pcs,\n    this function reconstructs a single image from its principal component\n    representation, x_pca.\n    X = the original data to which PCA was applied to get pcs.\n    \"\"\"\n    feature_means = X - center_data(X)\n    feature_means = feature_means[0, :]\n    x_reconstructed = np.dot(x_pca, pcs[:, range(n_components)].T) + feature_means\n    return x_reconstructed\n\ndef augment_feature_vector(X):\n    \"\"\"\n    Adds the x[i][0] = 1 feature for each data point x[i].\n\n    Args:\n        X - a NumPy matrix of n data points, each with d - 1 features\n\n    Returns: X_augment, an (n, d) NumPy array with the added feature for each datapoint\n    \"\"\"\n    column_of_ones = np.zeros([len(X), 1]) + 1\n    return np.hstack((column_of_ones, X))\n","62cc3d19":"def polynomial_kernel(X, Y, c, p):\n    \"\"\"\n        Compute the polynomial kernel between two matrices X and Y::\n            K(x, y) = (<x, y> + c)^p\n        for each pair of rows x in X and y in Y.\n\n        Args:\n            X - (n, d) NumPy array (n datapoints each with d features)\n            Y - (m, d) NumPy array (m datapoints each with d features)\n            c - a coefficient to trade off high-order and low-order terms (scalar)\n            p - the degree of the polynomial kernel\n\n        Returns:\n            kernel_matrix - (n, m) Numpy array containing the kernel matrix\n    \"\"\"\n#     return (X @ Y.T + c).astype(np.float32) ** p\n    return (X @ Y.T + c) ** p\n\ndef rbf_kernel(X, Y, gamma):\n    \"\"\"\n        Compute the Gaussian RBF kernel between two matrices X and Y::\n            K(x, y) = exp(-gamma ||x-y||^2)\n        for each pair of rows x in X and y in Y.\n\n        Args:\n            X - (n, d) NumPy array (n datapoints each with d features)\n            Y - (m, d) NumPy array (m datapoints each with d features)\n            gamma - the gamma parameter of gaussian function (scalar)\n\n        Returns:\n            kernel_matrix - (n, m) Numpy array containing the kernel matrix\n    \"\"\"\n    return np.exp(-gamma * ((X ** 2).sum(axis=1)[None].T + (Y ** 2).sum(axis=1) - 2 * X @ Y.T))\n","b8b19e05":"kernel = lambda X, Y: polynomial_kernel(X, Y, 1, 3)\n# # kernel = lambda X, Y: X@Y.T\nalpha = 1\ntemp_parameter = 10\nlambda_factor = 1e-4","8a3259b9":"import time\nstart = time.time()\nn_size = 25000\nn_components = 50\npcs = principal_components(train_x)\ntrain_x = project_onto_PC(train_x, pcs, n_components)[:n_size]\ntest_x = project_onto_PC(test_x, pcs, n_components)\ntrain_y = train_y[:n_size]\nprint(time.time()-start)\ntrain_x = augment_feature_vector(train_x)\ntest_x = augment_feature_vector(test_x)\nK = kernel(train_x, train_x)  # full\nprint(time.time()-start)\nKtest = kernel(train_x, test_x)\nprint(time.time()-start)","9f7bf21f":"use_ym = True\n# use_ym = False\n\ndef compute_probabilities_kernel(a, temp_parameter, K, YM):\n    \"\"\"\n    Computes, for each datapoint X[i], the probability that X[i] is labeled as j\n    for j = 0, 1, ..., k-1\n\n    Args:\n        a - (k, n) NumPy array, where row j represents the parameters of kernel coeffection for label j\n        temp_parameter - the temperature parameter of softmax function (scalar)\n        K - Kernel matrix, training time: train_x against train_x; test time: train_x against test_x\n        YM - (k, n) Label matrix, -1: false, +1: true\n    Returns:\n        H - (k, n) NumPy array, where each entry H[j][i] is the probability that X[i] is labeled as j\n    \"\"\"\n    if use_ym:\n        w = (a * YM) @ K \/ temp_parameter\n    else:\n        w = a @ K \/ temp_parameter\n    w = np.exp(w - w.max(axis=0))\n    return w \/ w.sum(axis=0)\n\n\ndef compute_log_probabilities_kernel(a, temp_parameter, K, YM):\n    \"\"\"\n    Computes, for each datapoint X[i], the probability that X[i] is labeled as j\n    for j = 0, 1, ..., k-1\n\n    Args:\n        a - (k, n) NumPy array, where row j represents the parameters of kernel coeffection for label j\n        temp_parameter - the temperature parameter of softmax function (scalar)\n        K - Kernel matrix, training time: train_x against train_x; test time: train_x against test_x\n        YM - (k, n) Label matrix, -1: false, +1: true\n    Returns:\n        H - (k, n) NumPy array, where each entry H[j][i] is the log probability that X[i] is labeled as j\n    \"\"\"\n    if use_ym:\n        w = (a * YM) @ K \/ temp_parameter\n    else:\n        w = a @ K \/ temp_parameter\n    w = w - w.max(axis=0)\n    return w - np.log(np.exp(w).sum(axis=0))\n\n\ndef compute_cost_function_kernel(A, a, lambda_factor, temp_parameter, K, YM):\n    \"\"\"\n    Computes the total cost over every datapoint.\n\n    Args:\n        A - (n, k) NumPy array containing the onehot for labels (a number from 0-9) for each\n            data point\n        a - (n,) NumPy array, where row j represents the parameters of kernel perceptron\n        lambda_factor - the regularization constant (scalar)\n        temp_parameter - the temperature parameter of softmax function (scalar)\n        K - Kernel matrix, training time: train_x against train_x; test time: train_x against test_x\n        YM - (k, n) Label matrix, -1: false, +1: true\n    Returns\n        c - the cost value (scalar)\n    \"\"\"\n    w = compute_log_probabilities_kernel(a, temp_parameter, K, YM)\n    return -(A * w).sum() \/ A.shape[0]\n\n\n\ndef run_gradient_descent_iteration_kernel(A, a, alpha, lambda_factor, temp_parameter, K, YM):\n    \"\"\"\n    Runs one step of batch gradient descent\n\n    Args:\n        A - (n, k) NumPy array containing the onehot for labels (a number from 0-9) for each\n            data point\n        a - (n,) NumPy array, where row j represents the parameters of kernel perceptron\n        alpha - the learning rate (scalar)\n        lambda_factor - the regularization constant (scalar)\n        temp_parameter - the temperature parameter of softmax function (scalar)\n        K - Kernel matrix, training time: train_x against train_x; test time: train_x against test_x\n        YM - (k, n) Label matrix, -1: false, +1: true\n    Returns:\n        theta - (k, d) NumPy array that is the final value of parameters theta\n    \"\"\"\n\n    if use_ym:\n        return (1 - lambda_factor * alpha) * a + YM * (\n                K @ (A - compute_probabilities_kernel(a, temp_parameter, K, YM)).T).T \\\n               \/ (temp_parameter * A.shape[0] \/ alpha)\n    else:\n        return (1 - lambda_factor * alpha) * a + (\n                K @ (A - compute_probabilities_kernel(a, temp_parameter, K, YM)).T).T \\\n               \/ (temp_parameter * A.shape[0] \/ alpha)\n\n\n\ndef get_classification_kernel(a, temp_parameter, K, YM):\n    \"\"\"\n    Makes predictions by classifying a given dataset\n\n    Args:\n        temp_parameter - the temperature parameter of softmax function (scalar)\n\n    Returns:\n        Y - (n, ) NumPy array, containing the predicted label (a number between 0-9) for\n            each data point\n    \"\"\"\n    probabilities = compute_probabilities_kernel(a, temp_parameter, K, YM)\n    return np.argmax(probabilities, axis=0)\n\n\ndef compute_test_error_kernel_inside(test_y, a, temp_parameter, K, YM):\n    assigned_labels = get_classification_kernel(a, temp_parameter, K, YM)\n    return 1 - np.mean(assigned_labels == test_y)\n\ndef compute_test_error_kernel(train_y, a, temp_parameter, Ktest, test_y):\n    # label matrix with -1 and 1\n    YM = -1 * np.ones(a.shape)\n    YM[train_y, np.arange(train_y.shape[0])] = 1\n    return compute_test_error_kernel_inside(test_y, a, temp_parameter, Ktest, YM)\n\ndef plot_cost_function_over_time(cost_function_history):\n    plt.plot(range(len(cost_function_history)), cost_function_history)\n    plt.ylabel('Cost Function')\n    plt.xlabel('Iteration number')\n    plt.show()","a6430908":"def softmax_regression_kernel(K, train_y, Ktest, test_y, temp_parameter, alpha, lambda_factor, k, num_iterations):\n    \"\"\"\n    Runs batch gradient descent for a specified number of iterations on a dataset\n    with theta initialized to the all-zeros array. Here, theta is a k by d NumPy array\n    where row j represents the parameters of our model for label j for\n    j = 0, 1, ..., k-1\n\n    Args:\n        train_y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n            data point\n        temp_parameter - the temperature parameter of softmax function (scalar)\n        alpha - the learning rate (scalar)\n        lambda_factor - the regularization constant (scalar)\n        k - the number of labels (scalar)\n        num_iterations - the number of iterations to run gradient descent (scalar)\n\n    Returns:\n        theta - (k, d) NumPy array that is the final value of parameters theta\n        cost_function_progression - a Python list containing the cost calculated at each step of gradient descent\n    \"\"\"\n    a = np.zeros((k, train_y.shape[0]))  # (k,n) k column for each label\n\n    \n    cost_function_progression = []\n    test_error_progression = []\n\n    # label onehot matrix\n    A = np.zeros(a.shape)\n    A[train_y, np.arange(train_y.shape[0])] = 1\n\n    # label matrix with -1 and 1\n    YM = -1 * np.ones(a.shape)\n    YM[train_y, np.arange(train_y.shape[0])] = 1\n\n    for i in range(1, num_iterations + 1):\n        loss = compute_cost_function_kernel(A, a, lambda_factor, temp_parameter, K, YM)\n        cost_function_progression.append(loss)\n        a = run_gradient_descent_iteration_kernel(A, a, alpha, lambda_factor, temp_parameter, K, YM)\n        test_error = compute_test_error_kernel_inside(test_y, a, temp_parameter, Ktest, YM)\n        test_error_progression.append(test_error)\n    return a, cost_function_progression, test_error_progression\n","d5749d54":"\na, cost_function_history, test_error_hist = softmax_regression_kernel(K, train_y, Ktest, test_y, temp_parameter, alpha=alpha,\n                                                     lambda_factor=lambda_factor, k=10, num_iterations=400)\n","052dad28":"test_error = compute_test_error_kernel(train_y, a, temp_parameter, Ktest, test_y)","3fb5bed1":"test_error","07965671":"plot_cost_function_over_time(cost_function_history)\nplot_cost_function_over_time(test_error_hist )","1954f626":"### Kernel functions","d64e377d":"## Load MNIST data","e2a77690":"# Softmax regression using kernel","85f197a4":"## Run regression, takes a long time. Due to numpy in this notebook is not using Intel MKL.","5e7a3199":"## Utilities functions","3e9e0c61":"## Define kernel and hyperparameter","36f83f71":"With kernel trick, define $\\theta$ as below:\n$\\alpha$, kernel coefficient, $ym$:label matrix, (-1,1)\n\n$\\theta=\\begin{bmatrix} \n\\theta_0= \\sum _{i=1}^{n} \\alpha^{(0,i)} ym^{(0,i)} \\phi (x^{(i)})\\\\\n\\theta_1= \\sum _{i=1}^{n} \\alpha^{(1,i)} ym^{(1,i)} \\phi (x^{(i)})\\\\\n...\\\\\n\\theta_k= \\sum _{i=1}^{n} \\alpha^{(k,i)} ym^{(k,i)} \\phi (x^{(i)})\\\\\n\\end{bmatrix}$\n\n$\\theta = (\\alpha \\circ ym ) \\phi(x)$\n\nThen convert cost function $J(\\theta)$ using above defination.\n\n$J(\\theta ) = -\\frac{1}{n}\\Bigg[\\sum _{i=1}^ n \\sum _{j={0}}^{k-1} [[y^{(i)} == j]] \\log {\\frac{e^{\\theta _ j \\cdot x^{(i)} \/ \\tau }}{\\sum _{l={0} }^{{k-1} } e^{\\theta _ l \\cdot x^{(i)} \/ \\tau }}}\\Bigg] + \\frac{\\lambda }{2}\\sum _{j=0}^{k- }\\sum _{i=0}^{d-1} \\theta _{ji}^2$\n\nGradient become:\n\n$\\nabla _{\\alpha_j} J(\\alpha ) = -\\frac{1}{\\tau n} \\sum _{i = 1} ^{n} [([[y^{(i)} == j]] - p(y^{(i)} = j | x^{(i)}, \\alpha_j ))]K(x^{(j)},x^{(i)})\\circ ym^{(j)} + \\lambda \\alpha_j$"}}