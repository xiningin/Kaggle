{"cell_type":{"8eb9ca1b":"code","b7baab81":"code","296a480f":"code","159741ad":"code","8df0eb83":"code","1377dcba":"code","1fa0aa41":"code","8ff50243":"code","b95724ac":"code","7e0cda4b":"code","5dd97748":"code","427a0abc":"code","3f008d71":"code","57d874bb":"code","bad8cf45":"code","e2712151":"code","d4ebc2ce":"code","ebb8316a":"code","75f11212":"code","4610ef3b":"code","4c9c9341":"code","ed9269b1":"code","8f115bc6":"code","edd561cf":"code","f4c5f992":"code","b2760879":"code","be17dd4c":"code","86484e94":"markdown","8d2f2beb":"markdown","518c2c2f":"markdown","909f4ced":"markdown","e9754ec8":"markdown","960bc96a":"markdown"},"source":{"8eb9ca1b":"#import the usual suspects\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#Ignore warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","b7baab81":"#importing the data set\npd.set_option('display.max_columns', None)\ndata = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndata.head()","296a480f":"#Distribution of the target variable\nplt.figure(figsize = (7,4))\nsns.set(style=\"darkgrid\")\n\npercentage = lambda i: len(i) \/ float(len(data['Class'])) * 100\nax = sns.barplot(x= data['Class'], y=data['Class'],  estimator=percentage)\nax.set(ylabel=\"Percent\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2., height + 1,'{:1.2f}'.format(height),ha=\"center\")\nplt.show()","159741ad":"#Summary Statistics for Amount and Class\ndata[['Amount','Class']].describe()","8df0eb83":"#Any null values in the dataset ?\ndata.isnull().sum().sum()","1377dcba":"#Distribution of Time\nplt.figure(figsize = (10,4))\nsns.distplot(data['Time'], color='g', bins = 100)\nplt.show()","1fa0aa41":"#Box plot for Amount\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\ns = sns.boxplot(ax = ax1, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data, palette=\"PRGn\",showfliers=True)\ns = sns.boxplot(ax = ax2, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data, palette=\"PRGn\",showfliers=False)\nplt.show()","8ff50243":"#Splitting data set\nX = data.drop('Class', axis = 1)\ny = data['Class']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","b95724ac":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\n\n#Fitting the model\ndtree.fit(X_train, y_train)\n\n#Prediction\ny_score = dtree.predict_proba(X_test)\npred = dtree.predict(X_test)","7e0cda4b":"#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, pred))\n\n#Recall Score\nfrom sklearn.metrics import recall_score\nprint('Recall Score: ',recall_score(y_test, pred))","5dd97748":"#Precision-Recall curve\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(y_test, y_score[:,-1])\n\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\nprecision, recall, _ = precision_recall_curve(y_test, y_score[:,-1])\n\nplt.step(recall, precision, color='b', alpha=0.2, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\nplt.show()","427a0abc":"#Cross validation score\n#Stratified because we need balanced samples\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nSKfold = StratifiedKFold(n_splits=5, random_state=42)\nscores = cross_val_score(dtree, X, y, cv=SKfold, scoring='recall')\nscores.mean()","3f008d71":"%%time\n\n#Hyperparameter tuning\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {\n    'max_depth':[1,2,3,4,5],\n    'min_samples_leaf':[1,2,3,4,5],\n    'min_samples_split':[2,3,4,5],\n    'criterion':['gini','entropy']\n}\n\nrandom_search = RandomizedSearchCV(estimator=dtree, param_distributions=param_dist, scoring='recall', n_jobs=-1, cv=SKfold, n_iter=100)\nrandom_search.fit(X_train, y_train)\n\nprint('Best Score: ', random_search.best_score_)\nprint('Best Params:', random_search.best_params_)","57d874bb":"#Checking the SD of the best score\nstd_dev = pd.DataFrame(random_search.cv_results_)\nstd_dev = std_dev[std_dev['rank_test_score'] == 1]\nprint(std_dev['std_test_score'].unique())","bad8cf45":"#Prediction and evaluation using Decision Tree\npred = random_search.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix, recall_score\nprint(confusion_matrix(y_test, pred))\nprint('Recall Score: ',recall_score(y_test, pred))","e2712151":"%%time\n\n#SMOTE within CV\nfrom imblearn.pipeline import Pipeline, make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {\n    'max_depth':[1,2,3,4,5],\n    'min_samples_leaf':[1,2,3,4,5],\n    'min_samples_split':[2,3,4,5],\n    'criterion':['entropy']\n}\n\nparam_dist = {'decisiontreeclassifier__' + key: param_dist[key] for key in param_dist}\nimba_pipeline = make_pipeline(SMOTE(random_state=42), dtree)\nrandom_search = RandomizedSearchCV(imba_pipeline, param_distributions=param_dist, cv=SKfold, scoring='recall', n_iter=50)\nrandom_search.fit(X_train, y_train)\n\nprint('Best Score: ', random_search.best_score_)\nprint('Best Params:', random_search.best_params_)","d4ebc2ce":"#Checking the SD of the best score\nstd_dev = pd.DataFrame(random_search.cv_results_)\nstd_dev = std_dev[std_dev['rank_test_score'] == 1]\nprint(std_dev['std_test_score'].unique())","ebb8316a":"#Prediction and evaluation using Decision Tree with SMOTE\npred = random_search.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix, recall_score\nprint(confusion_matrix(y_test, pred))\nprint('Recall Score: ',recall_score(y_test, pred))","75f11212":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))","4610ef3b":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrfm = RandomForestClassifier()\n\n#Cross validation score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nSKfold = StratifiedKFold(n_splits=5, random_state=42)\nscores = cross_val_score(rfm, X, y, cv=SKfold, scoring='recall')\nprint('Mean score: ', scores.mean())\nprint('Standard deviation: ', scores.std())","4c9c9341":"%%time\n\n#Hyperparameter tuning\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {'n_estimators': [5, 25, 50],\n              'max_features': ['log2', 'sqrt'],\n              'max_depth': [int(x) for x in np.linspace(10, 50, num = 5)],\n              'min_samples_split': [2, 5, 10],\n              'min_samples_leaf': [1, 3, 5]\n}\n\nrandom_search = RandomizedSearchCV(estimator=rfm, param_distributions=param_dist, scoring='recall', n_jobs=-1, cv=SKfold, n_iter=50)\nrandom_search.fit(X_train, y_train)\n\nprint('Best Score: ', random_search.best_score_)\nprint('Best Params:', random_search.best_params_)","ed9269b1":"#Checking the SD of the best score\nstd_dev = pd.DataFrame(random_search.cv_results_)\nstd_dev = std_dev[std_dev['rank_test_score'] == 1]\nstd_dev['std_test_score']","8f115bc6":"#Prediction and evaluation using Random Forest\npred = random_search.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix, recall_score\nprint(confusion_matrix(y_test, pred))\nprint('Recall Score: ',recall_score(y_test, pred))","edd561cf":"%%time\n\n#Balanced Random Forest\nfrom imblearn.ensemble import BalancedRandomForestClassifier\nbrfm = BalancedRandomForestClassifier(sampling_strategy=0.1)\n\n#Cross validation score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nSKfold = StratifiedKFold(n_splits=5, random_state=42)\nscores = cross_val_score(brfm, X, y, cv=SKfold, scoring='recall')\nprint('Mean score: ', scores.mean())\nprint('Standard deviation: ', scores.std())","f4c5f992":"%%time\n\n#Hyperparameter tuning\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {'sampling_strategy': [0.1, 0.15, 0.2],\n              'max_features': ['log2', 'sqrt'],\n              'max_depth': [int(x) for x in np.linspace(10, 50, num = 5)],\n              'min_samples_split': [2, 5, 10, 25],\n              'min_samples_leaf': [1, 3, 5]\n}\n\nrandom_search = RandomizedSearchCV(estimator=brfm, param_distributions=param_dist, scoring='recall', n_jobs=-1, cv=SKfold, n_iter=100)\nrandom_search.fit(X_train, y_train)\n\nprint('Best Score: ', random_search.best_score_)\nprint('Best Params:', random_search.best_params_)","b2760879":"#Checking the SD of the best score\nstd_dev = pd.DataFrame(random_search.cv_results_)\nstd_dev = std_dev[std_dev['rank_test_score'] == 1]\nprint(std_dev['std_test_score'].unique())","be17dd4c":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))","86484e94":"**Balanced Random Forest**","8d2f2beb":"The data is imbalanced. Only 0.17 percent of the target variable has 1 (Credit Card fraud) thus sampling is required for this dataset.","518c2c2f":"**Random Forest**","909f4ced":"**Decision Tree**","e9754ec8":"Reviewing both precision and recall is useful in cases where there is an imbalance in the observations between the two classes. Specifically, there are many examples of no event (class 0) and only a few examples of an event (class 1).\n\nThe reason for this is that typically the large number of class 0 examples means we are less interested in the skill of the model at predicting class 0 correctly, e.g. high true negatives.\n\nKey to the calculation of precision and recall is that the calculations do not make use of the true negatives. It is only concerned with the correct prediction of the minority class, class 1. A precision-recall curve is a plot of the precision (y-axis) and the recall (x-axis) for different thresholds, much like the ROC curve.","960bc96a":"sampling_strategy (when str) corresponds to the desired ratio of the number of samples in the minority class over the number of samples in the majority class after resampling. Therefore, the ratio is expressed as \u03b1_us = N_{m} \/ N_{rM} where N_{m} is the number of samples in the minority class and N_{rM} is the number of samples in the majority class after resampling."}}