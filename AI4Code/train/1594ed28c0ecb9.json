{"cell_type":{"63192f8e":"code","9388e3f4":"code","b8159262":"code","4465ac4b":"code","60628fa1":"code","b62fb33a":"code","8fe51940":"code","b3700544":"code","eea26b65":"code","cfb3a12b":"code","1e57b40a":"code","1bbd9dcf":"code","94642878":"code","851c3601":"code","b0ab7b2e":"code","e2ccbf6f":"code","4b417fbb":"code","66c575f0":"code","14958ced":"code","d91f25f4":"code","6c4b2025":"code","ffec4648":"code","f4a4afae":"markdown","a46c5590":"markdown","06c431b2":"markdown","35ef5e54":"markdown","776a2693":"markdown","3ca1bdd7":"markdown","575e34bb":"markdown","2d5fb607":"markdown","0bb05662":"markdown","8bc916ac":"markdown","d453fe84":"markdown","dffc946c":"markdown","f4f9bf79":"markdown","30622b51":"markdown","6d30fc99":"markdown","916be9db":"markdown","21b52d30":"markdown","cba8f514":"markdown","f1602c3b":"markdown","14339aa4":"markdown","f0c953a8":"markdown","7dd724d6":"markdown","c4ca96b1":"markdown"},"source":{"63192f8e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n!pip install tensorflow==2.0.0-rc1\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n\n\nprint(tf.__version__)\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9388e3f4":"imdb = keras.datasets.imdb","b8159262":"# Prepare Data \nNUM_WORDS = 10000\n\n(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=NUM_WORDS)\n\ndef multi_hot_sequences(sequences, dimension):\n    # Create an all-zero matrix of shape (len(sequences), dimension)\n    results = np.zeros((len(sequences), dimension))\n    for i, word_indices in enumerate(sequences):\n        results[i, word_indices] = 1.0  # set specific indices of results[i] to 1s\n    return results\n\n\ntrain_zeros = multi_hot_sequences(train_data, dimension=NUM_WORDS)\ntest_zeros = multi_hot_sequences(test_data, dimension=NUM_WORDS)","4465ac4b":"# input shape is the vocabulary count used for the movie reviews (10000 words)\n\nwithout_embedding= keras.Sequential([\n    # `input_shape` is only required here so that `.summary` works.\n    keras.layers.Dense(64, activation=tf.nn.relu,\n                       input_shape=(NUM_WORDS,)),\n    keras.layers.Dense(64, activation=tf.nn.relu),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\n\nwithout_embedding.compile(optimizer='adam',\n                       loss='binary_crossentropy',\n                       metrics=['accuracy', 'binary_crossentropy'])\n\nwithout_embedding.summary()\nhistory_without_embedding = without_embedding.fit(train_zeros,\n                                      train_labels,\n                                      epochs=20,\n                                      batch_size=512,\n                                      validation_data=(test_zeros, test_labels),\n                                      verbose=2)\nwithout_embedding_result = without_embedding.evaluate(test_zeros, test_labels)\n","60628fa1":"print(without_embedding_result)","b62fb33a":"word_index = imdb.get_word_index()\nword_index = {k:(v+3) for k,v in word_index.items()}\nword_index[\"<PAD>\"] = 0         # PAD words  int = 0   \nword_index[\"<START>\"] = 1       # the start of text  =  int =1 \nword_index[\"<UNK>\"] = 2         # unknown words  int =2 \nword_index[\"<UNUSED>\"] = 3\n\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n\n\ntrain_data = keras.preprocessing.sequence.pad_sequences(train_data,\n                                                        value=word_index[\"<PAD>\"],\n                                                        padding='post',\n                                                        maxlen=256)\n\ntest_data = keras.preprocessing.sequence.pad_sequences(test_data,\n                                                       value=word_index[\"<PAD>\"],\n                                                       padding='post',\n                                                       maxlen=256)","8fe51940":"decode_review(train_data[0])","b3700544":"embedding_model = keras.Sequential()\nembedding_model.add(keras.layers.Embedding(10000, 64))\nembedding_model.add(keras.layers.GlobalAveragePooling1D())\nembedding_model.add(keras.layers.Dense(64, activation=tf.nn.relu))\nembedding_model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nembedding_model.compile(optimizer='adam',\n                       loss='binary_crossentropy',\n                       metrics=['accuracy', 'binary_crossentropy'])\n\nembedding_model.summary()\nhistory_embedding_model = embedding_model.fit(train_data,\n                                      train_labels,\n                                      epochs=20,\n                                      batch_size=512,\n                                      validation_data=(test_data, test_labels),\n                                      verbose=2)\nembedding_model_result = embedding_model.evaluate(test_data, test_labels)","eea26b65":"print(embedding_model_result)","cfb3a12b":"# embe\nembedding_small = keras.Sequential()\nembedding_small.add(keras.layers.Embedding(10000, 16))\nembedding_small.add(keras.layers.GlobalAveragePooling1D())\nembedding_small.add(keras.layers.Dense(16, activation=tf.nn.relu))\nembedding_small.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nembedding_small.compile(optimizer='adam',\n                       loss='binary_crossentropy',\n                       metrics=['accuracy', 'binary_crossentropy'])\n\nembedding_small.summary()\nhistory_embedding_small = embedding_small.fit(train_data,\n                                      train_labels,\n                                      epochs=20,\n                                      batch_size=512,\n                                      validation_data=(test_data, test_labels),\n                                      verbose=2)\nembedding_small_result = embedding_small.evaluate(test_data, test_labels)","1e57b40a":"print(embedding_small_result)","1bbd9dcf":"(sm_train_data, sm_train_labels), (sm_test_data, sm_test_labels) = keras.datasets.imdb.load_data(num_words=1000)\nsm_train_data = keras.preprocessing.sequence.pad_sequences(sm_train_data,\n                                                        value=word_index[\"<PAD>\"],\n                                                        padding='post',\n                                                        maxlen=256)\n\nsm_test_data = keras.preprocessing.sequence.pad_sequences(sm_test_data,\n                                                       value=word_index[\"<PAD>\"],\n                                                       padding='post',\n                                                       maxlen=256)\n","94642878":"\nsmall_vocab = keras.Sequential()\nsmall_vocab.add(keras.layers.Embedding(1000, 16))              # put vocabulary size 1000 instead of 10000\nsmall_vocab.add(keras.layers.GlobalAveragePooling1D())\nsmall_vocab.add(keras.layers.Dense(16, activation=tf.nn.relu))\nsmall_vocab.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nsmall_vocab.compile(optimizer='adam',\n                       loss='binary_crossentropy',\n                       metrics=['accuracy', 'binary_crossentropy'])\n\nsmall_vocab.summary()\nhistory_small_vocab = small_vocab.fit(sm_train_data,\n                                      sm_train_labels,\n                                      epochs=20,\n                                      batch_size=512,\n                                      validation_data=(sm_test_data, sm_test_labels),\n                                      verbose=2)\nembedding_small_vocab = small_vocab.evaluate(sm_test_data, sm_test_labels)","851c3601":"print(embedding_small_vocab)","b0ab7b2e":"import matplotlib.pyplot as plt\n\ndef plot_history(histories, key='binary_crossentropy'):\n  plt.figure(figsize=(16,10))\n\n  for name, history in histories:\n    val = plt.plot(history.epoch, history.history['val_'+key],\n                   '--', label=name.title()+' Val')\n    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n             label=name.title()+' Train')\n\n  plt.xlabel('Epochs')\n  plt.ylabel(key.replace('_',' ').title())\n  plt.legend()\n\n  plt.xlim([0,max(history.epoch)])","e2ccbf6f":"def plot_history_acc(histories, key='accuracy'):\n  plt.figure(figsize=(16,10))\n\n  for name, history in histories:\n    val = plt.plot(history.epoch, history.history['val_'+key],\n                   '--', label=name.title()+' Val')\n    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n             label=name.title()+' Train')\n\n  plt.xlabel('Epochs')\n  plt.ylabel(key.replace('_',' ').title())\n  plt.legend()\n\n  plt.xlim([0,max(history.epoch)])","4b417fbb":"plot_history([('Without word Embedding ', history_without_embedding),\n              ('with word Embedding ', history_embedding_model)])","66c575f0":"plot_history_acc([('Without word Embedding ', history_without_embedding),\n              ('with word Embedding ', history_embedding_model)])","14958ced":"plot_history([('word Embedding small = 16', history_embedding_small),\n              ('word Embedding big = 64 ', history_embedding_model)])","d91f25f4":"plot_history_acc([('word Embedding small = 16', history_embedding_small),\n              ('word Embedding big = 64 ', history_embedding_model)])","6c4b2025":"plot_history([('Vocabulary size = 10000', history_embedding_small),\n              ('Vocabulary size = 1000 ', history_small_vocab)])","ffec4648":"plot_history_acc([('Vocabulary size = 10000', history_embedding_small),\n              ('Vocabulary size = 1000 ', history_small_vocab)])","f4a4afae":"# Vocabulary Size : 10000 VS 1000","a46c5590":"## Build Model","06c431b2":"### Prepare Data with zeros","35ef5e54":"# Model without word Embedding vs word Embedding model","776a2693":"## accuracy","3ca1bdd7":"### the model with bigest vocabulary size is more accurate ","575e34bb":"## binary cross entropy","2d5fb607":"## accuracy ","0bb05662":"# 2- Model with Embedding Layer","8bc916ac":"# 1-Model without Embedding Layer","d453fe84":"# Word Embedding small size VS Big Size","dffc946c":"### model with small size embedding is more accurate","f4f9bf79":"# Now let change the word embedding size","30622b51":"### word embedding with small size better than big size ","6d30fc99":"## Small word embedding size","916be9db":"## binary cross entropy","21b52d30":"## Text Classification with Word Embedding  :\n- befor see this notebook you can see basic text classification : \nhttps:\/\/www.kaggle.com\/salahuddinemr\/basic-text-classification\n\n### in this notebook we will see :\n- model without word embedding  vs model with word embedding \n- for word embedding larg size and small size for perceptrons and vocabulary ","cba8f514":"# Plot Results ","f1602c3b":"### Prepare Data with pad sequence","14339aa4":"# Small Vocabulary size","f0c953a8":"## accuracy","7dd724d6":"## binary cross entropy","c4ca96b1":"### the model with word embedding is more effictive than model without embedding"}}