{"cell_type":{"1b50e8f6":"code","03a203d6":"code","ff5596ea":"code","908f3df3":"code","4c7441c7":"code","7b27b80a":"code","b5e6ffb0":"code","34c5d21b":"code","a987340f":"code","3bc6e168":"code","702ce6d7":"code","96285020":"code","88d4c94c":"code","d5813f6f":"code","6946c202":"code","9d79fb3c":"code","51b82038":"code","a3d85699":"code","45f4c7a2":"code","fe8a7e29":"code","32ba6e62":"markdown","0399a943":"markdown","9934dfde":"markdown","4fb97d0d":"markdown","9c3384b4":"markdown","1311910c":"markdown","d1cbbca9":"markdown","29b6d145":"markdown","e296f605":"markdown","2dcb1361":"markdown","14465eac":"markdown","b49c42db":"markdown","befca59a":"markdown","df76a187":"markdown","ab5eea4e":"markdown","086b20d8":"markdown"},"source":{"1b50e8f6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","03a203d6":"df1 = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")\ndf2 = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_3C_weka.csv\")\n","ff5596ea":"df1.head()","908f3df3":"df1.info()","4c7441c7":"In 'column_2C_weka.csv'\n* there are 7 features and for each of them 310 samples.\n* Six feature are in float type and one of them is object.","7b27b80a":"df1.describe()","b5e6ffb0":"df2.head()","34c5d21b":"df2.info()","a987340f":"df2.describe()","3bc6e168":"df1.corr()","702ce6d7":"sns.countplot(x=\"class\", data=df1)\ndf1.loc[:,'class'].value_counts()","96285020":"data = df1[df1['class'] =='Abnormal']\npelvic_incidence = np.array(data.loc[:,'pelvic_incidence']).reshape(-1,1)\nsacral_slope = np.array(data.loc[:,'sacral_slope']).reshape(-1,1)\n# Scatter\nplt.figure(figsize=[10,10])\nplt.scatter(pelvic_incidence,sacral_slope)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()","88d4c94c":"from sklearn.linear_model import LinearRegression\nreg = LinearRegression()\n# Predict space\npredict_space = np.linspace(min(pelvic_incidence), max(pelvic_incidence)).reshape(-1,1)\n# Fit\nreg.fit(pelvic_incidence,sacral_slope)\n# Predict\npredicted = reg.predict(predict_space)\n# R^2 \nprint('R^2 score: ',reg.score(pelvic_incidence, sacral_slope))\n# Plot regression line and scatter\nplt.figure(figsize=(10,10))\nplt.plot(predict_space, predicted, color='green', linewidth=2)\nplt.scatter(pelvic_incidence,sacral_slope)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()","d5813f6f":"x = (df1.iloc[:,[0,2]]).values # [pelvic_incidence,lumbar_lordosis_angle]\ny = df1.sacral_slope.values.reshape(-1,1)","6946c202":"multiple_linear_regression = LinearRegression()\nmultiple_linear_regression.fit(x,y)\n\n\nprint(\"b0: \",multiple_linear_regression.intercept_)\nprint(\"b1,b2:\",multiple_linear_regression.coef_)\n\nmultiple_linear_regression.predict(np.array([[63.0278175 , 39.60911701],[40.47523153, 39.60911701]]))","9d79fb3c":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope']).reshape(-1,1)\n\nlr = LinearRegression()\n\nlr.fit(x,y)\n# predict\ny_head = lr.predict(x)\n\nplt.plot(x,y_head,color=\"purple\",label = \"linear\")\n\n# polynomial regression = y = b0 + b1*x + b2*x^2 + b3*x^3 + ... + bn*x^n\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npolynomial_regression = PolynomialFeatures(degree = 4) # takes polynomial until fourth degree\n\nx_polynomial = polynomial_regression.fit_transform(x) #transform func makes x values polynomial\n# fit\nlinear_regression2 = LinearRegression()\nlinear_regression2.fit(x_polynomial,y) \n\n# visualize\n\ny_head2 = linear_regression2.predict(x_polynomial)\n\n\nplt.plot(x,y_head2,color = \"green\", label = \"poly\")\nplt.legend()\nplt.show()","51b82038":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope']).reshape(-1,1)\n\n\nfrom sklearn.tree import DecisionTreeRegressor # random state = 0\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(x,y)\n\n\ntree_reg.predict([[5.5]])\n\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\n\ny_head = tree_reg.predict(x_)\n#%% visualize\nplt.figure(figsize = (10,10))\nplt.scatter(x,y,color = \"red\")\nplt.plot(x_,y_head,color=\"green\")\nplt.xlabel(\"Pelvic Incidence\")\nplt.ylabel(\"Sacral Slope\")\nplt.show()","a3d85699":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope'])\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators = 40,random_state = 42)\n\nrf.fit(x,y)\n\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_head = rf.predict(x_)\n\n#visualize\nplt.figure(figsize = (10,10))\nplt.scatter(x,y,color=\"blue\")\nplt.plot(x_,y_head,color = \"red\")\nplt.xlabel(\"Pelvic Incidence\")\nplt.ylabel(\"Sacral Slope\")\nplt.show()","45f4c7a2":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope'])\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators = 100,random_state = 42)\n\nrf.fit(x,y)\n\ny_head = rf.predict(x)\n\n\nfrom sklearn.metrics import r2_score\n\nprint(\"r_score\", r2_score(y,y_head))","fe8a7e29":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope'])\n\nplt.figure(figsize=(10,10))\nplt.scatter(pelvic_incidence,sacral_slope)\nplt.xlabel(\"Pelvic Incidence\")\nplt.ylabel(\"Sacral Slope\")\n\nfrom sklearn.linear_model import LinearRegression\n\n#linear regression model\nlinear_reg = LinearRegression()\n\n\nlinear_reg.fit(x,y)\n\ny_head = linear_reg.predict(x)\nplt.plot(x, y_head , color = \"red\")\n\nfrom sklearn.metrics import r2_score\n\nprint(\"r_square score: \", r2_score(y, y_head))","32ba6e62":"<a id=\"9\"><\/a> <br>\n# R Square with Linear Regression","0399a943":"<a id=\"1\"><\/a> <br>\n# Reading Data","9934dfde":"<a id=\"7\"><\/a> <br>\n# Random Forest Regression","4fb97d0d":"* **CART** : Classification and Regression Tree\n* One of the most important things is \"**split**\" in decision tree regression.\n* the areas that are splitted are called **terminal leaves**.","9c3384b4":"* We can say also \"line fit\"\n* **y = b0 + b1*x** where \n * b0 = constant (*the point where line intersects the y axis*)\n * b1 = coefficient (*slope of the line*)\n* The aim is to draw the line closest to the points. But the line may not pass exactly from the center of the dots. So there is the term \"residual\".\n  * **residual = y - y_head**\n      * y is where the point is\n      * y_head is where it hits when drawn upright to the line from the point\n* We can square the residual to reduce the error and get positive residual. By adding the squares of residual we can see how fit the line.\n* Mean Squared Error  **MSE = (sum(residual^2))\/n**\n* The smaller value of the MSE, the better the line is fit.\n","1311910c":"<a id=\"3\"><\/a> <br>\n# **Linear Regression**","d1cbbca9":"<a id=\"6\"><\/a> <br>\n# Decision Tree Regression","29b6d145":"<a id=\"4\"><\/a> <br>\n# Multiple Linear Regression","e296f605":"<a id=\"5\"><\/a> <br>\n# Polinomial Linear Regression","2dcb1361":"* **random_state** allows selection of the same random values.\nIf we dont assign random state there would be different results every time the code executed.","14465eac":"* **y = b0 + b1*x1 + b2*x2 + ... + bn*xn**","b49c42db":"In 'column_3C_weka.csv'\n* there are 7 features and for each of them 310 samples.\n* Six feature are in float type and one of them is object.","befca59a":"<a id=\"8\"><\/a> <br>\n# R Square with Random Forest Regression","df76a187":"### **Correlation Between Features**","ab5eea4e":"* **y = b0 + b1*x1 + b2*x2**\n* The aim is minimum MSE","086b20d8":"# **Regression**\n* [Reading Data](#1)\n* [Linear Regression](#3)\n* [Multiple Linear Regression](#4)\n* [Polynomial Linear Regression](#5)\n* [Decision Tree Regression](#6)\n* [Random Forest Regression](#7)\n* [R Square with Random Forest Regression](#8)\n* [R Square with Linear Regression](#9)"}}