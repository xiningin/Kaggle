{"cell_type":{"5b187c89":"code","9af64b69":"code","1908bd03":"code","83117f74":"code","b2a298a8":"code","defd979f":"code","61906698":"code","cdf8af60":"code","9c91644f":"code","e480cb82":"code","b6603bce":"code","0b47aee5":"code","efea0e70":"code","31218472":"code","947ce264":"code","b23c30ba":"code","1a550d04":"code","fdf8b240":"code","a2d4ff0e":"code","4ebeb19c":"code","8af81c89":"code","9f39f8cd":"code","272b7aed":"code","51c4f63a":"code","d8a4a1de":"code","15ce14fe":"code","ab06621a":"code","a7089332":"code","3f798165":"code","dbe29334":"code","9767b294":"code","1a3e18f4":"code","be39c0ea":"code","2011c0cf":"code","469bb1f1":"code","919fcd0f":"code","63e42cc4":"code","927c3b10":"code","a879b96c":"code","33522de1":"code","c6cb9839":"code","23e557a2":"code","26d0cd66":"code","80379590":"code","247a4a18":"code","55264026":"code","3ffba1d9":"code","8f8316ad":"code","b2c39008":"code","debcd37d":"code","583370d8":"code","33363d7e":"code","f866d09e":"code","a7ab3b89":"code","15c4934c":"code","066fcc13":"code","04ad5a97":"code","f073f1a8":"code","9bce6643":"code","27348cce":"code","55562061":"code","0a0629a4":"code","a35a7a18":"code","2c51200c":"code","db5dac19":"code","94939cca":"code","faa41972":"code","5cdd8328":"code","aa4d0bbb":"code","26b29159":"code","3f5482c4":"code","1c1eb861":"code","8af450a4":"code","3c7a608d":"markdown","f3fb03dd":"markdown","b816fff6":"markdown","273c5c0a":"markdown","62d52d63":"markdown","933c9a9d":"markdown","4f4da82b":"markdown","9acd3655":"markdown","2c311192":"markdown","afc3309e":"markdown","588fd18e":"markdown","74f71643":"markdown","3ceeb7b8":"markdown","f2c41ee2":"markdown","e6460e64":"markdown","7b4415b5":"markdown","21064ee6":"markdown","1f6ae28b":"markdown","360e04cb":"markdown","a65c0dcd":"markdown","39cf7517":"markdown","71cf5702":"markdown","fe6a6cc5":"markdown","9670e115":"markdown","fdc750ad":"markdown","ff9d767d":"markdown","15604d1c":"markdown","411ead53":"markdown","e6f1b805":"markdown","99f3b688":"markdown","f62dab54":"markdown","c38f2996":"markdown"},"source":{"5b187c89":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","9af64b69":"df_card = pd.read_csv('..\/input\/creditcard.csv')","1908bd03":"df_card.head()","83117f74":"df_card[['Time', 'Amount']].describe()","b2a298a8":"df_card.info()","defd979f":"df_card['Class'].value_counts()","61906698":"print(\"Fraudulent transactions account for {:.2f}% of the dataset\"\n      .format(df_card['Class'].value_counts()[1]\/len(df_card)*100))","cdf8af60":"df_card[['Amount', 'Class']].groupby('Class').mean()","9c91644f":"df_card[['Amount', 'Class']].groupby('Class').max()","e480cb82":"df_card['Amount'].value_counts()","b6603bce":"df_card[df_card['Class'] == 1][['Amount', 'Class']].sort_values(by='Amount', ascending=False).head(10)","0b47aee5":"df_card[df_card['Class'] == 1][['Amount', 'Class']]['Amount'].value_counts()","efea0e70":"def get_transactions_average():\n    ## Fraudulent transactions mean\n    fraudulent_transactions_mean = df_card[df_card['Class'] == 1]['Amount'].mean()\n    ## Regular transactions mean\n    normal_transactions_mean = df_card[df_card['Class'] == 0]['Amount'].mean()\n    ## Creating an array with the mean values\n    return [fraudulent_transactions_mean, normal_transactions_mean]","31218472":"# Get the mean values for each transaction type\nmean_arr = get_transactions_average()\n# Calculate the overall mean\noverall_mean = df_card['Amount'].mean()","947ce264":"fig = plt.figure(figsize=(10, 8))\n## Labels to replace the elements' indexes in the x-axis\nxticks_labels = ['Fraudulent transactions', 'Regular transactions']\n## X-axis elements\nxticks_elements = [item for item in range(0,len(mean_arr))]\nax = plt.gca()\n## Plot the bar char custom bar colors\nplt.bar(xticks_elements, mean_arr, color='#2F4F4F')\n## Map the xticks to their string descriptions, then rotate them to make them more readable\nplt.xticks(xticks_elements, xticks_labels, rotation=70)\n## Draw a horizontal line to show the overall mean to compare with each category's mean\nplt.axhline(overall_mean, color='#e50000', animated=True, linestyle='--')\n## Annotate the line to explain its purpose\nax.annotate('Overall Mean', xy=(0.5, overall_mean), xytext=(0.5, 110),\n            arrowprops=dict(facecolor='#e50000', shrink=0.05))\n## Set the x-axis label\nplt.xlabel('Transactions')\n## Set the y-axis label\nplt.ylabel('Average amount in $ Dollar')\n## Show the plot\nplt.show()","b23c30ba":"# Describing the amount values for the fraulent transactions\ndescribe_arr = df_card[df_card['Class'] == 1]['Amount'].describe()\ndescribe_arr","1a550d04":"## Creates a new figure\nplt.figure(figsize=(10, 8))\n## Filter out the fraudulent transactions from dataframe\ndf_fraudulent = df_card[df_card['Class']==1]\n## Creates a boxplot from fraudulent transactions data\nsns.boxplot(x=\"Class\", y=\"Amount\", \n                 data=df_fraudulent, palette='muted')\n## Most values are clustered around small values, but the max transactions amount is smaller \n## than those from regular transactions","fdf8b240":"## Creates a new figure\nplt.figure(figsize=(10, 8))\n## Filter out the normal transactions from dataframe\ndf_regular = df_card[df_card['Class']==0]\n## Creates a boxplot from the regular transactions data\nsns.boxplot(x=\"Class\", y=\"Amount\", \n                 data=df_regular, palette='muted')\n\n## Most transactions are grouped around small amounts ","a2d4ff0e":"## Creates a new figure \nplt.figure(figsize=(10, 8))\n## Draw a distribution plot (histogram) from amount values\nsns.distplot(df_card['Amount'], kde=True, hist=True, norm_hist=True)\n## Check that most of the transactions are clustered around small values","4ebeb19c":"## Creates a new figure \nplt.figure(figsize=(10, 8))\n## Draw a distribution plot (histogram) from fraudulent transactions data\nsns.distplot(df_fraudulent['Amount'], kde=True, hist=True, norm_hist=True)\n## Check that most transactions are clustered around $0 and $500.","8af81c89":"df_card.head()","9f39f8cd":"## Dataset split import\nfrom sklearn.model_selection import train_test_split","272b7aed":"## Scale the amount feature before fitting the models\nsc= StandardScaler()\ndf_card[\"scaled_amount\"]=  sc.fit_transform(df_card.iloc[:,29].values.reshape(-1,1))\n## Drops the old amount, once the scaled one has been added to the dataframe\ndf_card.drop('Amount', axis=1, inplace=True)","51c4f63a":"## Set the features to the X variable\nX = df_card.drop(['Time', 'Class'], axis=1)\n## Set the target column to the y_target variable\ny_target = df_card['Class']","d8a4a1de":"## Models and evaluation metrics imports\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, average_precision_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV","15ce14fe":"## Split the data into train and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y_target, random_state=42)","ab06621a":"## This is a generic function to calculate the auc score which is used several times in this notebook\ndef evaluate_model_auc(model, X_test_parameter, y_test_parameter):\n    ## The predictions\n    y_pred = model.predict(X_test_parameter)\n    ## False positive rate, true positive rate and treshold\n    fp_rate, tp_rate, treshold = roc_curve(y_test_parameter, y_pred)\n    ## Calculate the auc score\n    auc_score = auc(fp_rate, tp_rate)\n    ## Returns the score to the model\n    return (auc_score)","a7089332":"## This is a generic function to plot the area under the curve (AUC) for a model\ndef plot_auc(model, X_test, y_test):\n    ## Predictions\n    y_pred = model.predict(X_test)\n    \n    ## Calculates auc score\n    fp_rate, tp_rate, treshold = roc_curve(y_test, y_pred)\n    auc_score = auc(fp_rate, tp_rate)\n    \n    ## Creates a new figure and adds its parameters\n    plt.figure()\n    plt.title('ROC Curve')\n    ## Plot the data - false positive rate and true positive rate\n    plt.plot(fp_rate, tp_rate, 'b', label = 'AUC = %0.2f' % auc_score)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')","3f798165":"## This is a generic utility function to calculate a model's score\ndef evaluate_model_score(model, X_test, y_test):\n    ## Return the score value to the model\n    return model.score(X_test, y_test)","dbe29334":"## This is a generic function to create a classification report and return it to the model. The target\n## variables have been mapped to the transaction types\ndef evaluate_classification_report(model, y_test):\n    return classification_report(y_test, model.predict(X_test), target_names=['Regular transaction',\n                                                                      'Fraudulent transaction'])","9767b294":"## This utility function evaluates a model using some common metrics such as accurary and auc. Also, it\n## prints out the classification report for the specific model\ndef evaluate_model(model_param, X_test_param, y_test_param):\n    print(\"Model evaluation\")\n    print(\"Accuracy: {:.5f}\".format(evaluate_model_score(model_param, X_test_param, y_test_param)))\n    print(\"AUC: {:.5f}\".format(evaluate_model_auc(model_param, X_test_param, y_test_param)))\n    print(\"\\n#### Classification Report ####\\n\")\n    print(evaluate_classification_report(model_param, y_test_param))\n    plot_auc(model_param, X_test_param, y_test_param)","1a3e18f4":"## This is a shared function used to print out the results of a gridsearch process\ndef gridsearch_results(gridsearch_model):\n    print('Best score: {} '.format(gridsearch_model.best_score_))\n    print('\\n#### Best params ####\\n')\n    print(gridsearch_model.best_params_)","be39c0ea":"# Returns the Random Forest model which the n_estimators returns the highest score in order to improve \n# the results of the default classifier\n# min_estimator - min number of estimators to run\n# max_estimator - max number of estimators to run\n# X_train, y_train, X_test, y_test - splitted dataset\n# scoring function: accuracy or auc\ndef model_selection(min_estimator, max_estimator, X_train_param, y_train_param,\n                   X_test_param, y_test_param, scoring='accuracy'):\n    scores = [] \n    ## Returns the classifier with highest accuracy score\n    if (scoring == 'accuracy'):\n        for n in range(min_estimator, max_estimator):\n            rfc_selection = RandomForestClassifier(n_estimators=n, random_state=42).fit(X_train_param, y_train_param)\n            score = evaluate_model_score(rfc_selection, X_test_param, y_test_param)\n            print('Number of estimators: {} - Score: {:.5f}'.format(n, score))\n            scores.append((rfc_selection, score))\n            \n    ## Returns the classifier with highest auc score\n    elif (scoring == 'auc'):\n         for n in range(min_estimator, max_estimator):\n            rfc_selection = RandomForestClassifier(n_estimators=n, random_state=42).fit(X_train_param, y_train_param)\n            score = evaluate_model_auc(rfc_selection, X_test_param, y_test_param)\n            print('Number of estimators: {} - AUC: {:.5f}'.format(n, score))\n            scores.append((rfc_selection, score))\n    return sorted(scores, key=lambda x: x[1], reverse=True)[0][0]","2011c0cf":"## Importing SMOTE \nfrom imblearn.over_sampling import SMOTE\n## Importing resample\nfrom sklearn.utils import resample","469bb1f1":"## Making a copy of the dataset (could've been done using df.copy())\ndataset = df_card[df_card.columns[1:]]\n## Defines the features to the dataset_features variable\ndataset_features = dataset.drop(['Class'], axis=1)\n## Defines the target feature to the dataset_target variable\ndataset_target = dataset['Class']","919fcd0f":"## Split the data once again\nX_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(dataset_features,\n                                                   dataset_target,\n                                                   random_state=42)","63e42cc4":"## This function generates a balanced X_train and y_train from the original dataset to fit the model\ndef get_balanced_train_data(df):\n    sm = SMOTE(random_state=42, ratio = 1.0)\n    X_train_res, y_train_res = sm.fit_sample(X_train_2, y_train_2)\n    ## Returns balanced X_train & y_train\n    return (X_train_res, y_train_res)","927c3b10":"## Calling the function to get scalled training data\n(X_train_resampled, y_train_resampled) = get_balanced_train_data(df_card)","a879b96c":"## Creating a SVC model with default parameters\nsvc = svm.SVC()\nsvc.fit(X_train_2, y_train_2)","33522de1":"## Evaluating the model\nevaluate_model(svc, X_test_2, y_test_2)","c6cb9839":"## Parameters grid to be tested on the model\nparameters = {\n    'C': [1, 5, 10, 15],\n    'degree':[1, 2, 3, 5],\n    'kernel': ['linear'],\n    'class_weight': ['balanced', {0:1, 1:10}, {0:1, 1:15}, {0:1, 1:20}],\n    'gamma': [0.01, 0.001, 0.0001, 0.00001]\n    }","23e557a2":"## Creates a gridsearch to find the best parameters for this dataset.\nclf = GridSearchCV(estimator=svm.SVC(random_state=42),\n                   ## Passes the parameter grid as argument (these parameters will be tested\n                   ## when this model is created)\n                   param_grid=parameters,\n                   ## Run the processes in all CPU cores\n                   n_jobs=-1,\n                   ## Set the scoring method to 'roc_auc'\n                   scoring='roc_auc')","26d0cd66":"## Fit the gridsearch model to the data\n# clf.fit(X_train_2[:5000], y_train_2[:5000])","80379590":"## Find the model with the best score achieved and the best parameters to use\n# gridsearch_results(clf)","247a4a18":"## Creates a SVC model with the optimal parameters found in the previous step\nsvc_grid_search = svm.SVC(C=1,\n                          kernel='linear',\n                          degree=1,\n                          class_weight={0:1, 1:10},\n                          gamma=0.01,\n                          random_state=42)\nsvc_grid_search.fit(X_train_2[:5000], y_train_2[:5000])","55264026":"## Evaluate the model\nevaluate_model(svc_grid_search, X_test_2, y_test_2)","3ffba1d9":"## Creates a Random Forest Classifier with default parameters\nmodel_rfc = RandomForestClassifier().fit(X_train_2, y_train_2)","8f8316ad":"## Evaluate the model\nevaluate_model(model_rfc, X_test, y_test)","b2c39008":"## Creating a model selecting the best number of estimators\nrfc_model = model_selection(5, 15, X_train, y_train, X_test, y_test, scoring='auc')","debcd37d":"## Evaluate the model\nevaluate_model(rfc_model, X_test, y_test)","583370d8":"## Select the model with the best number of estimators using the balanced dataset\nrfc_smote = model_selection(5, 15, X_train_resampled, y_train_resampled,\n                     X_test_2, y_test_2, scoring='auc')","33363d7e":"## Evaluate the model with AUC metric\nevaluate_model(rfc_smote, X_test_2, y_test_2)","f866d09e":"## Show the most important features from the dataset\nsorted(rfc_smote.feature_importances_, reverse=True)[:5]","a7ab3b89":"## Itemgetter import\nfrom operator import itemgetter","15c4934c":"## Loading features and importance\nfeatures = [i for i in X.columns.values]\nimportance = [float(i) for i in rfc_smote.feature_importances_]\nfeature_importance = []\n\n## Creating a list of tuples concatenating feature names and its importance\nfor item in range(0, len(features)):\n    feature_importance.append((features[item], importance[item]))\n\n## Sorting the list\nfeature_importance.sort(key=itemgetter(1), reverse=True)\n\n## Printing the top 5 most important features\nfeature_importance[:5]","066fcc13":"## Parameters to use with the RFC model\nparameters_rfc = { \n    'n_estimators': [5, 6, 7, 8, 9, 10, 13, 15],\n#     'class_weight': ['balanced'],\n    'max_depth': [None, 5, 10, 15, 20, 25, 30, 35, 40],\n    'min_samples_leaf': [1, 2, 3, 4, 5]\n}","04ad5a97":"## Gridsearch to get the best parameters for RFC\nrfc_grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42,\n                                                               n_jobs=-1),\n                               param_grid=parameters_rfc,\n                               cv=10, \n                               scoring='roc_auc',\n                               return_train_score=True)","f073f1a8":"## Train the gridsearch model\n## Using only part of the dataset because the entire data takes too long to train; the same\n## applies to the other models\n## Takes too long\nrfc_grid_search.fit(X_train_2[:10000], y_train_2[:10000])","9bce6643":"## Check the results of cross validation\ncv_results = pd.DataFrame(rfc_grid_search.cv_results_)\n## Sort the values to get the best result\ncv_results.sort_values(by='rank_test_score').head()","27348cce":"## Model with the best score and the best parameters\n#\ngridsearch_results(rfc_grid_search)","55562061":"## RFC model using the parameters found by gridsearch \nrfc = RandomForestClassifier(random_state=42,\n                            n_estimators=7, min_samples_leaf=1, max_depth=5)\n## Fit the data\nrfc.fit(X_train_2, y_train_2)","0a0629a4":"## Evaluate the model\nevaluate_model(rfc, X_test_2, y_test_2)","a35a7a18":"## Running gridsearch again to find the best results for the scalled dataset\nrfc_grid_search_balanced = GridSearchCV(estimator=RandomForestClassifier(random_state=42,\n                                                               n_jobs=-1),\n                               param_grid=parameters_rfc,\n                               cv=10,\n                               scoring='roc_auc',\n                               return_train_score=True)","2c51200c":"## Fitting the data\n## Takes too long\n# rfc_grid_search_balanced.fit(X_train_resampled[:5000], y_train_resampled[:5000])","db5dac19":"## Best score and best parameters\n#gridsearch_results(rfc_grid_search_balanced)\n\n#13 4 None","94939cca":"## Creating a new model with the selected parameters\nrfc_balanced = RandomForestClassifier(random_state=42, \n                            n_estimators=13, min_samples_leaf=4, max_depth=None)\nrfc_balanced.fit(X_train_resampled, y_train_resampled)","faa41972":"## Evaluate the model\nevaluate_model(rfc_balanced, X_test_2, y_test_2)","5cdd8328":"## Parameters grid for Logistic Regression model\nparam_grid_lreg = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 15],\n    'class_weight': ['balanced', {0:1, 0:10}, {0:1, 1:15}, {0:1, 1:20}],\n    'penalty': ['l1', 'l2']\n}","aa4d0bbb":"## Running gridsearch to find best parameters for Logistic Regression model\nlreg_grid_search = GridSearchCV(estimator=LogisticRegression(random_state=42),\n                               param_grid=param_grid_lreg, cv=10, scoring='roc_auc')","26b29159":"## Fitting the data (it takes a long time)\n# lreg_grid_search.fit(X_train_2[:2000], y_train_2[:2000])","3f5482c4":"## Printing the best results for this model\n# gridsearch_results(lreg_grid_search)","1c1eb861":"## Creating a model with gridsearch parameters\nlreg = LogisticRegression(C=1, penalty='l1', random_state=42,\n                         class_weight={0:1, 1:10})\n## Fitting the model\nlreg.fit(X_train_2, y_train_2)","8af450a4":"## Evaluate the model\nevaluate_model(lreg, X_test_2, y_test_2)","3c7a608d":"### Parameter tuning ","f3fb03dd":"### Even though regular transactions have a higher transaction amount","b816fff6":"## Model utility functions","273c5c0a":"### Cross validation with parameter tuning\nSetting parameters","62d52d63":"## **SVM** \nWith default parameters","933c9a9d":"## Final words\nAs you can see, we can get good results with some models out of the box such as Random Forest Classifier. However, the imbalanced nature of this dataset might impact the overall result. For this reason, I've tried several classifier algorithms along with different parameter tuning and a varied set of evaluation metrics in order to achieve stable results.\n\nThis is just an example of how to use some basic machine learning techniques such as: data manipulation, EDA, data scaling, balancing (SMOTE), gridsearch, cross validation, and model evaluation. ","4f4da82b":"### SMOTE\nModels like RFC and SVC have a parameter that penalizes imbalanced datasets in order to get more accurate results. However, we are going to balance the data using a technique called SMOTE to create synthetic data points from the monirity class using KNearest Neighbors.","9acd3655":"### Head of dataset","2c311192":"#### Checking the data types of dataframe columns","afc3309e":"### Training model with optimal parameters","588fd18e":"\n### **Dealing with imbalanced classes**\n","74f71643":"### Using the optimal parameters","3ceeb7b8":"### Very imbalanced classes. There are a lot more regular transactions than fraudulent\n\n1. 0    284315\n2. 1       492","f2c41ee2":"### Fraudulent transactions have a slightly higher mean value","e6460e64":"### Describing the dataset","7b4415b5":"It takes too long to run the gridsearch process on this model for this dataset. For this reason, I decided to run on my local machine and wait until the process finishes. After completed, I obtained the following parameters:\n1. **n_estimators**: 13\n2. **min_samples_leaf**: 4\n3. **max_depth**: default (None)\n\nNote that a lower **max_depth** parameter will lower the precision for the 'balanced' dataset.","21064ee6":"### Training with balanced dataset","1f6ae28b":"### Models","360e04cb":"### With balanced dataset","a65c0dcd":"#### Plotting the mean values in a bar plot. We can check the regular transactions are aroud the overall mean value, but the fraudulent ones are slightly above the mean","39cf7517":"### Selecting a model with best # of estimators","71cf5702":"### Now we can check that the most repeated amount for fraudulent transactions is $1.00! This could indicate that this is just a \"checking\" amount, a value used to test if the transaction is approved.\n","fe6a6cc5":"### The transaction amount that repeated the most is a very small value: **$1.00**. Would it be fraudulent or not?","9670e115":"### **Grid search random forest**","fdc750ad":"### Training model with optimal parameters","ff9d767d":"### Sorting the fraudulent transactions by Amount","15604d1c":"### **Exploratory Data Analysis (EDA)**","411ead53":"## **Logistic regression**","e6f1b805":"### Preparing data for model ","99f3b688":"### **Loading dataset into pandas dataframe**","f62dab54":"## **Random Forest Classifier**\nWith default parameters","c38f2996":"### Random Forest Classifier presentes a good performance right out of the box, but can we improve it? Let's test using the balanced dataset using parameter tuning"}}