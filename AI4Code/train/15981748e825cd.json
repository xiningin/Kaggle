{"cell_type":{"e6f117f8":"code","e2d6c792":"code","38372f69":"code","c5c2d7ff":"code","e4444751":"code","84a84c0b":"code","c97c22a7":"code","698ed08a":"code","9d0b6b8b":"code","5d977cf5":"code","171e9af6":"code","105fa928":"code","8ec65891":"code","c6517c42":"code","43c2c507":"code","a0a19727":"code","ea1a8408":"code","eba39f55":"code","9e1ae6ef":"code","0c549705":"code","27e88692":"code","0c1efb34":"code","2935eec0":"code","3a0857d0":"code","d31058d0":"code","80d6b47e":"code","29edd50c":"code","f02b2f23":"code","e0d44e6a":"code","aea888aa":"code","f8e123f0":"code","3550303a":"code","fce0c2a5":"code","8aca7546":"markdown","4c436ef3":"markdown","7de7bc75":"markdown","0da107eb":"markdown","1809e3ce":"markdown"},"source":{"e6f117f8":"def fff(x):\n  with open(x) as file: \n    line = []\n    for lines in file.readlines():\n      line.append(lines)\n    return line\nline = fff('..\/input\/emotions-dataset-for-nlp\/train.txt')","e2d6c792":"line[0:5]","38372f69":"import pandas as pd","c5c2d7ff":"import re","e4444751":"def csv(line):\n  list1,list2 = [],[]\n  for lines in line:\n    x,y = lines.split(';')\n    y = y.replace('\\n','')\n    list1.append(x)\n    list2.append(y)\n  df = pd.DataFrame(list(list1),columns=['sentence'])\n  df['emotion'] = list2\n  return df","84a84c0b":"df = csv(line)","c97c22a7":"df","698ed08a":"df.emotion.value_counts()","9d0b6b8b":"df.isnull().sum()","5d977cf5":"import nltk\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import stopwords","171e9af6":"wn = WordNetLemmatizer()","105fa928":"def lem(x):\n  corpus = []\n  i=1\n  for words in x:\n    words = words.split()\n    y = [wn.lemmatize(word) for word in words if not word in stopwords.words('english')]\n    y =  ' '.join(y)\n    corpus.append(y)\n  return corpus\nx = lem(df['sentence'])","8ec65891":"x[:5]","c6517c42":"test_line = fff('..\/input\/emotions-dataset-for-nlp\/train.txt') ","43c2c507":"test_df = csv(test_line)","a0a19727":"test_df[:5]","ea1a8408":"x_test = lem(test_df['sentence'])","eba39f55":"all = x + x_test","9e1ae6ef":"len(all)","0c549705":"y = df.iloc[:,1].values","27e88692":"y.shape","0c1efb34":"y_test = test_df.iloc[:,1].values","2935eec0":"y_test.shape","3a0857d0":"from tensorflow.keras.layers import Embedding,LSTM,Dense\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential","d31058d0":"y_train = pd.DataFrame(y)","80d6b47e":"tokenizer = Tokenizer(nb_words=10000, split=' ')\ntokenizer.fit_on_texts(all)\nX1 = tokenizer.texts_to_sequences(all)\nX1 = pad_sequences(X1,maxlen=20,padding='post',truncating='post')\nY1 = pd.get_dummies(y_train).values","29edd50c":"X_train = X1[:16000]\nX_test = X1[16000:]","f02b2f23":"Y_train = Y1","e0d44e6a":"Y_test = pd.get_dummies(y_test).values","aea888aa":"model = Sequential()\nmodel.add(Embedding(input_dim=10000,output_dim = 64,input_length=20))\nmodel.add(LSTM(64))\nmodel.add(Dense(6,activation='softmax'))","f8e123f0":"model.summary()","3550303a":"model.compile(optimizer='rmsprop',loss='mse',metrics=['accuracy'])","fce0c2a5":"model.fit(X_train,Y_train,batch_size=32,epochs=10,verbose=2,validation_split=0.2)","8aca7546":"##handling text data","4c436ef3":"##text preprocessing\n####stopwords\n####lematization\n####count vectorizer\n","7de7bc75":"###building and training the model","0da107eb":"building an lstm model","1809e3ce":"labelling test and train data"}}