{"cell_type":{"e1a2856c":"code","d06fbf0c":"code","201ddf8c":"code","9b8cbe4f":"code","15300131":"code","9fc800d0":"code","971b49b8":"code","9fc1e4dd":"code","ff5bb81d":"code","84fe431d":"code","a95d6ec6":"code","dfaef6d1":"code","17a8ce0b":"code","02e4c438":"code","b90a6181":"code","cc57c174":"markdown","bb97d9bb":"markdown","13f5d090":"markdown","4802d88f":"markdown","a36a0f91":"markdown","364a045d":"markdown","80e6abfc":"markdown","15130ea9":"markdown","db3b9d75":"markdown","a5927e3e":"markdown","3c46f4ad":"markdown","79a4ff1d":"markdown","b5753018":"markdown"},"source":{"e1a2856c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d06fbf0c":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import *\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"\/kaggle\/input\/fieldgoals\/4.csv\")\ndata_filter = data[[\"Good?\",\"Dist\"]]\ndata_filter.Dist=data_filter.Dist.astype(str)\n\nexamples = data_filter.iloc[480:]\n\ndata_filter = data_filter.drop([x for x in range(479,500,1)])\n\nplt.figure(0)\nplt.subplot(211)\ndata_filter['Good?'].value_counts().plot.bar()\nplt.subplot(212)\ndata_filter['Good?'].value_counts().plot.pie()\nplt.show()","201ddf8c":"data = data.replace(to_replace = ['Y','N'],value = ['1','0'])\ndata","9b8cbe4f":"data2 = data[[\"Dist\", \"Good?\", \"Blk?\"]]\n#data2\ndata2[\"Good?\"] = pd.to_numeric(data2[\"Good?\"], downcast=\"float\")\ndata2[\"Blk?\"] = pd.to_numeric(data2[\"Blk?\"], downcast=\"float\")","15300131":"data2.describe()","9fc800d0":"all_features = data2[['Dist', 'Blk?']].values\nall_classes = data2['Good?'].values\nfeature_names = ['Dist', 'Blk?']\n\nfrom sklearn import preprocessing\n\nscaler = preprocessing.StandardScaler()\nall_features_scaled = scaler.fit_transform(all_features)\n#all_features_scaled","971b49b8":"from tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\n\ndef create_model():\n    model = Sequential() \n    model.add(Dense(2, input_dim=2, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","9fc1e4dd":"from sklearn.model_selection import cross_val_score\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n\nestimator = KerasClassifier(build_fn=create_model, epochs=20, verbose=1)\n\ncv_scores = cross_val_score(estimator, all_features_scaled, all_classes, cv=10)","ff5bb81d":"cv_scores.mean()","84fe431d":"vectorizer = CountVectorizer()\ncounts = vectorizer.fit_transform(data_filter['Dist'].values)\n\nclassifier = ComplementNB()\ntargets = data_filter['Good?'].values\nclassifier.fit(counts, targets)","a95d6ec6":"test = examples.drop(columns= [\"Good?\"])\ntest1 = [x for x in test.values]\npredict = []\nfor y in test1:\n    example_counts = vectorizer.transform(y)\n    predictions = classifier.predict(example_counts)\n    predict.append(predictions)\n    print(predictions)\n","dfaef6d1":"examples.head()","17a8ce0b":"test2 = examples.drop(columns= [\"Dist\"])\ntest3 = [x for x in test2.values]\n\ncorrect = [i for i, j in zip(predict, test3) if i == j]\n\nprint(\"The accuracy is:\")\nprint(len(correct)\/len(test3))","02e4c438":"classifier2 = MultinomialNB()\nclassifier2.fit(counts, targets)\n\npredict2 = []\nfor y in test1:\n    example_counts = vectorizer.transform(y)\n    predictions2 = classifier2.predict(example_counts)\n    predict2.append(predictions2)\n\ncorrect2 = [i for i, j in zip(predict2, test3) if i == j]\n\nprint(\"The accuracy is:\")\nprint(len(correct2)\/len(test3))    \n","b90a6181":"examples2 = ['55', '49', '33', '22', '13', '38', '57', '45', '24']\nexample_counts2 = vectorizer.transform(examples2)\npredictions3 = classifier.predict(example_counts2) ###Complement \npredictions4 = classifier2.predict(example_counts2) ###Bernoulli\nprint(predictions3)\nprint(predictions4)\n","cc57c174":"The model isn't great; but we get an accuracy of 65%. Pretty good for just a few lines of code! Next, I want to compare these results with a Bernoulli classifier. ","bb97d9bb":"We'll start with the Neural net and then compare to the Naive-Bayes. Here I chose also to include whether or not the field goal was blocked within the model (for the Neural net - for the NB model I justed used distance from the goal posts). ","13f5d090":"In this notebook I'm attempting to use a Naive-Bayes classifier to determine whether or not a field goal will be made based on the distance from the goal posts. I'm using 500 field goals from the 2019 NFL regular season. ","4802d88f":"I picked 20 field goals from the original dataset to use for testing. I'm also printing the predictions of the classifier. (click 'output' to reveal)","a36a0f91":"Calculate the accuracy:","364a045d":"I'm now trying to reproduce the results of the Neural network with a simplier Naive-Bayes Model. I'll use 2 different models (ComplementNB and BournoulliNB) and see how close I can get to 80% accuracy. \n\nThe charts above show that there are many more field goals that are made compared to ones that aren't.  I choose to use the complement naive bayes model because it is good for imbalanced data sets such as this. ","80e6abfc":"Scaling the features of the model.","15130ea9":"Here are the actual results.","db3b9d75":"See the results (click 'output' to reveal)","a5927e3e":"if you look at the code below you can see the keras classifier does on average 7% - sometimes up to 10% better than the Naive Bayes classifier!","3c46f4ad":"Field goals of yardage greater than 40 would seem to be harder to make; that seems to be captured by both models! ","79a4ff1d":"Wow! that did much better. Now I'm making up some of my own examples","b5753018":"Simple sequential model"}}