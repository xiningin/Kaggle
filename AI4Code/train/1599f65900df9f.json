{"cell_type":{"1d384793":"code","d7890e11":"code","43e201c4":"code","2d9a3391":"code","6efbab7c":"markdown"},"source":{"1d384793":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nfrom sklearn.metrics import cohen_kappa_score\nimport gc","d7890e11":"def read_data():\n    print('Reading train.csv file....')\n    train = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train.csv')\n    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n\n    print('Reading test.csv file....')\n    test = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/test.csv')\n    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n\n    print('Reading train_labels.csv file....')\n    train_labels = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train_labels.csv')\n    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n\n    print('Reading specs.csv file....')\n    specs = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/specs.csv')\n    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n\n    print('Reading sample_submission.csv file....')\n    sample_submission = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/sample_submission.csv')\n    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n    return train, test, train_labels, specs, sample_submission\n\ndef get_time(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['date'] = df['timestamp'].dt.date\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    return df\n\ndef get_object_columns(df, columns):\n    df = df.groupby(['installation_id', columns])['event_id'].count().reset_index()\n    df = df.pivot_table(index = 'installation_id', columns = [columns], values = 'event_id')\n    df.columns = list(df.columns)\n    df.fillna(0, inplace = True)\n    return df\n\ndef get_numeric_columns(df, column):\n    df = df.groupby('installation_id').agg({f'{column}': ['mean', 'sum', 'std']})\n    df.fillna(0, inplace = True)\n    df.columns = [f'{column}_mean', f'{column}_sum', f'{column}_std']\n    return df\n\ndef get_numeric_columns_2(df, agg_column, column):\n    df = df.groupby(['installation_id', agg_column]).agg({f'{column}': ['mean', 'sum', 'std']}).reset_index()\n    df = df.pivot_table(index = 'installation_id', columns = [agg_column], values = [col for col in df.columns if col not in ['installation_id', 'type']])\n    df.fillna(0, inplace = True)\n    df.columns = list(df.columns)\n    return df\n\ndef get_correct_incorrect(df):\n    df = df.groupby(['title'])['num_correct', 'num_incorrect'].agg({'num_correct': ['mean', 'std'], 'num_incorrect': ['mean', 'std']}).reset_index()\n    df.columns = ['title', 'num_correct_mean', 'num_correct_std', 'num_incorrect_mean', 'num_incorrect_std']\n    return df\n\ndef preprocess(train, test, train_labels):\n    # columns for feature engineering\n    numerical_columns = ['game_time']\n    categorical_columns = ['type', 'world']\n\n    reduce_train = pd.DataFrame({'installation_id': train['installation_id'].unique()})\n    reduce_train.set_index('installation_id', inplace = True)\n    reduce_test = pd.DataFrame({'installation_id': test['installation_id'].unique()})\n    reduce_test.set_index('installation_id', inplace = True)\n    \n    for i in numerical_columns:\n        reduce_train = reduce_train.merge(get_numeric_columns(train, i), left_index = True, right_index = True)\n        reduce_test = reduce_test.merge(get_numeric_columns(test, i), left_index = True, right_index = True)\n    \n    for i in categorical_columns:\n        reduce_train = reduce_train.merge(get_object_columns(train, i), left_index = True, right_index = True)\n        reduce_test = reduce_test.merge(get_object_columns(test, i), left_index = True, right_index = True)\n\n    for i in categorical_columns:\n        for j in numerical_columns:\n            reduce_train = reduce_train.merge(get_numeric_columns_2(train, i, j), left_index = True, right_index = True)\n            reduce_test = reduce_test.merge(get_numeric_columns_2(test, i, j), left_index = True, right_index = True)\n            \n    reduce_train.reset_index(inplace = True)\n    reduce_test.reset_index(inplace = True)\n    \n    print('Our training set have {} rows and {} columns'.format(reduce_train.shape[0], reduce_train.shape[1]))\n    \n    # get the mode of the title\n    labels_map = dict(train_labels.groupby('title')['accuracy_group'].agg(lambda x:x.value_counts().index[0]))\n    # merge target\n    labels = train_labels[['installation_id', 'title', 'accuracy_group']]\n    # merge with correct incorrect\n    corr_inc = get_correct_incorrect(train_labels)\n    labels = labels.merge(corr_inc, how = 'left', on = 'title')\n    # replace title with the mode\n    labels['title'] = labels['title'].map(labels_map)\n    # get title from the test set\n    reduce_test['title'] = test.groupby('installation_id').last()['title'].reset_index(drop = True)\n    # merge with correct incorrect\n    reduce_test = reduce_test.merge(corr_inc, how = 'left', on = 'title')\n    # map title\n    reduce_test['title'] = reduce_test['title'].map(labels_map)\n    # join train with labels\n    reduce_train = labels.merge(reduce_train, on = 'installation_id', how = 'left')\n    print('We have {} training rows'.format(reduce_train.shape[0]))\n    # align datasets\n    categoricals = ['title']\n    reduce_train = reduce_train[[col for col in reduce_test.columns] + ['accuracy_group']]\n    return reduce_train, reduce_test, categoricals\n\n# function to perform features selection, not calling it to optimize time, best features are in a list in the next cell\ndef run_feature_selection(reduce_train, reduce_test):\n    kf = KFold(n_splits=10, random_state = 42)\n    all_columns = ['num_correct_mean', 'num_correct_std', 'num_incorrect_mean', 'num_incorrect_std', \n                     'game_time_mean', 'game_time_sum', 'game_time_std', 'Activity', 'Assessment', 'Clip', 'Game', 'CRYSTALCAVES', \n                     'MAGMAPEAK', 'NONE', 'TREETOPCITY', ('game_time', 'mean', 'Activity'), \n                     ('game_time', 'mean', 'Assessment'), ('game_time', 'mean', 'Clip'), ('game_time', 'mean', 'Game'),  \n                     ('game_time', 'std', 'Activity'), ('game_time', 'std', 'Assessment'), ('game_time', 'std', 'Clip'), \n                     ('game_time', 'std', 'Game'), ('game_time', 'sum', 'Activity'), ('game_time', 'sum', 'Assessment'), \n                     ('game_time', 'sum', 'Clip'), ('game_time', 'sum', 'Game'), ('game_time', 'mean', 'CRYSTALCAVES'), \n                     ('game_time', 'mean', 'MAGMAPEAK'), ('game_time', 'mean', 'NONE'), ('game_time', 'mean', 'TREETOPCITY'), \n                     ('game_time', 'std', 'CRYSTALCAVES'), ('game_time', 'std', 'MAGMAPEAK'), ('game_time', 'std', 'NONE'), \n                     ('game_time', 'std', 'TREETOPCITY'), ('game_time', 'sum', 'CRYSTALCAVES'), \n                     ('game_time', 'sum', 'MAGMAPEAK'), ('game_time', 'sum', 'NONE'), ('game_time', 'sum', 'TREETOPCITY')]\n    target = 'accuracy_group'\n    oof_pred = np.zeros((len(reduce_train), 4))\n    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train)):\n        print('Fold {}'.format(fold + 1))\n        x_train, x_val = reduce_train[all_columns].iloc[tr_ind], reduce_train[all_columns].iloc[val_ind]\n        y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n        train_set = lgb.Dataset(x_train, y_train, categorical_feature = categoricals)\n        val_set = lgb.Dataset(x_val, y_val, categorical_feature = categoricals)\n\n        params = {\n                'learning_rate': 0.01,\n                'metric': 'multiclass',\n                'objective': 'multiclass',\n                'num_classes': 4,\n                'feature_fraction': 0.75,\n                'subsample': 0.75,\n                'n_jobs': -1,\n            }\n\n        model = lgb.train(params, train_set, num_boost_round = 100000, early_stopping_rounds = 100, \n                          valid_sets=[train_set, val_set], verbose_eval = 100)\n        oof_pred[val_ind] = model.predict(x_val)\n    # using cohen_kappa because it's the evaluation metric of the competition\n    loss_score = cohen_kappa_score(reduce_train[target], np.argmax(oof_pred, axis = 1), weights = 'quadratic')\n    score = loss_score\n    best_features = all_columns.copy()\n    for i in all_columns:\n        oof_pred = np.zeros((len(reduce_train), 4))\n        features = [x for x in best_features if x not in [i]]\n        print('Evaluating {} column'.format(i))\n        for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train)):\n            print('Fold {}'.format(fold + 1))\n            x_train, x_val = reduce_train[features].iloc[tr_ind], reduce_train[features].iloc[val_ind]\n            y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n            train_set = lgb.Dataset(x_train, y_train, categorical_feature = categoricals)\n            val_set = lgb.Dataset(x_val, y_val, categorical_feature = categoricals)\n\n            model = lgb.train(params, train_set, num_boost_round = 100000, early_stopping_rounds = 100, \n                              valid_sets=[train_set, val_set], verbose_eval = 100)\n            oof_pred[val_ind] = model.predict(x_val)\n        loss_score = cohen_kappa_score(reduce_train[target], np.argmax(oof_pred, axis = 1), weights = 'quadratic')\n        if loss_score > score:\n            print('Feature {} is useless'.format(i))\n            best_features.remove(i)\n            score = loss_score\n        else:\n            print('Feature {} is usefull'.format(i))\n        gc.collect()\n    print('The best features are: ', best_features + 'title')\n\n    return best_features + 'title'\n\ntrain, test, train_labels, specs, sample_submission = read_data()\nreduce_train, reduce_test, categoricals = preprocess(train, test, train_labels)\n#best_features = run_feature_selection(reduce_train, reduce_test)","43e201c4":"# best features extracted from run_feature_selection function\nusefull_features = ['num_correct_mean', 'num_correct_std', 'num_incorrect_mean', 'num_incorrect_std',\n'game_time_mean', 'game_time_sum', 'Activity', 'Clip', 'Game', 'CRYSTALCAVES', 'NONE',\n'TREETOPCITY', ('game_time', 'mean', 'Clip'), ('game_time', 'mean', 'Game'), \n('game_time', 'std', 'Assessment'), ('game_time', 'std', 'Clip'), ('game_time', 'std', 'Game'),\n('game_time', 'sum', 'Activity'), ('game_time', 'sum', 'Clip'), ('game_time', 'sum', 'Game'),\n('game_time', 'mean', 'NONE'), ('game_time', 'mean', 'TREETOPCITY'), ('game_time', 'std', 'CRYSTALCAVES'),\n('game_time', 'std', 'MAGMAPEAK'), ('game_time', 'std', 'NONE'), ('game_time', 'std', 'TREETOPCITY'),\n('game_time', 'sum', 'CRYSTALCAVES'), ('game_time', 'sum', 'MAGMAPEAK'), \n('game_time', 'sum', 'NONE'), 'title']","2d9a3391":"def run_lgb(reduce_train, reduce_test, usefull_features):\n    kf = KFold(n_splits=10)\n    target = 'accuracy_group'\n    oof_pred = np.zeros((len(reduce_train), 4))\n    y_pred = np.zeros((len(reduce_test), 4))\n    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train)):\n        print('Fold {}'.format(fold + 1))\n        x_train, x_val = reduce_train[usefull_features].iloc[tr_ind], reduce_train[usefull_features].iloc[val_ind]\n        y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n        train_set = lgb.Dataset(x_train, y_train, categorical_feature=categoricals)\n        val_set = lgb.Dataset(x_val, y_val, categorical_feature=categoricals)\n\n        params = {\n            'learning_rate': 0.01,\n            'metric': 'multiclass',\n            'objective': 'multiclass',\n            'num_classes': 4,\n            'feature_fraction': 0.75,\n            'subsample': 0.75,\n            'n_jobs': -1\n        }\n\n        model = lgb.train(params, train_set, num_boost_round = 100000, early_stopping_rounds = 100, \n                          valid_sets=[train_set, val_set], verbose_eval = 100)\n        oof_pred[val_ind] = model.predict(x_val)\n        y_pred += model.predict(reduce_test[usefull_features]) \/ 10\n    loss_score = cohen_kappa_score(reduce_train[target], np.argmax(oof_pred, axis = 1), weights = 'quadratic')\n    print('Our oof cohen kappa score is: ', loss_score)\n    return y_pred\n\ndef predict(reduce_test, sample_submission, y_pred):\n    reduce_test = reduce_test.reset_index()\n    reduce_test = reduce_test[['installation_id']]\n    reduce_test['accuracy_group'] = y_pred.argmax(axis = 1)\n    sample_submission.drop('accuracy_group', inplace = True, axis = 1)\n    sample_submission = sample_submission.merge(reduce_test, on = 'installation_id')\n    sample_submission.to_csv('submission.csv', index = False)\n    print(sample_submission['accuracy_group'].value_counts(normalize = True))\ny_pred = run_lgb(reduce_train, reduce_test, usefull_features)\npredict(reduce_test, sample_submission, y_pred)","6efbab7c":"# Objective\n\n* Title feature is very predictive, using only this feature give 0.385 cohen_kappa_score\n* In this notebook i will apply feature selection to the baseline model lb(0.399). \n* Let's check if this feature selection technique work's!!\n\nLink for the past notebook is here: https:\/\/www.kaggle.com\/ragnar123\/simple-exploratory-data-analysis-and-model"}}