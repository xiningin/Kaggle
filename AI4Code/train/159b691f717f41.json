{"cell_type":{"227e7fe5":"code","dcc626b6":"code","7a8f1312":"code","5f2081d9":"code","327a82f8":"code","d840a161":"code","32a4df83":"code","1e9a265c":"code","d8d6e6be":"code","23968ad0":"code","c3b9a62d":"code","2ccbda40":"markdown","7cad6d77":"markdown","f4e7b88c":"markdown","751c82c1":"markdown","10a4f291":"markdown","fc679fec":"markdown","447ac8a5":"markdown","9597b801":"markdown","4b2dae65":"markdown","82301825":"markdown","1dfa9733":"markdown","56ae4de5":"markdown","9da6f74e":"markdown","fc9342fe":"markdown","e3a3ca28":"markdown"},"source":{"227e7fe5":"from mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport matplotlib.colors\n\nfrom matplotlib import animation,rc\nfrom IPython.display import HTML\nimport numpy as np\nimport seaborn as sns\nfrom PIL import Image, ImageDraw\n\nsns.set()","dcc626b6":"class SN:\n  \n  def __init__(self, w_init, b_init, algo):\n    self.w = w_init\n    self.b = b_init\n    self.w_h = []\n    self.b_h = []\n    self.e_h = []\n    self.algo = algo\n    \n  def sigmoid(self, x, w=None, b=None):\n    if w is None:\n      w = self.w\n    if b is None:\n      b = self.b\n    return 1. \/ (1. + np.exp(-(w*x + b)))\n  \n  def error(self, X, Y, w=None, b=None):\n    if w is None:\n      w = self.w\n    if b is None:\n      b = self.b\n    err = 0\n    for x, y in zip(X, Y):\n      err += 0.5 * (self.sigmoid(x, w, b) - y) ** 2\n    return err\n  \n  def grad_w(self, x, y, w=None, b=None):\n    if w is None:\n      w = self.w \n    if b is None:\n      b = self.b\n    y_pred = self.sigmoid(x, w, b)\n    return (y_pred - y) * y_pred * (1 - y_pred) * x\n  \n  def grad_b(self, x, y, w=None, b=None):\n    if w is None:\n      w = self.w\n    if b is None:\n      b = self.b\n    y_pred = self.sigmoid(x, w, b)\n    return (y_pred - y) * y_pred * (1 - y_pred)\n  \n  def fit(self, X, Y, \n          epochs=5000, eta=0.01, gamma=0.9, mini_batch_size=100, eps=1e-8,  \n          beta=0.9, beta1=0.9, beta2=0.9\n         ):\n    self.w_h = []\n    self.b_h = []\n    self.e_h = []\n    self.X = X\n    self.Y = Y\n    \n    if self.algo == 'GD':\n      for i in range(epochs):\n        dw, db = 0, 0\n        for x, y in zip(X, Y):\n          dw += self.grad_w(x, y)\n          db += self.grad_b(x, y)\n        self.w -= eta * dw \/ X.shape[0]\n        self.b -= eta * db \/ X.shape[0]\n        self.append_log()\n        \n    elif self.algo == 'MiniBatch':\n      for i in range(epochs):\n        dw, db = 0, 0\n        points_seen = 0\n        for x, y in zip(X, Y):\n          dw += self.grad_w(x, y)\n          db += self.grad_b(x, y)\n          points_seen += 1\n          if points_seen % mini_batch_size == 0:\n            self.w -= eta * dw \/ mini_batch_size\n            self.b -= eta * db \/ mini_batch_size\n            self.append_log()\n            dw, db = 0, 0\n        \n    elif self.algo == 'Momentum':\n      v_w, v_b = 0, 0\n      for i in range(epochs):\n        dw, db = 0, 0\n        for x, y in zip(X, Y):\n          dw += self.grad_w(x, y)\n          db += self.grad_b(x, y)\n        v_w = gamma * v_w + eta * dw \n        v_b = gamma * v_b + eta * db\n        self.w = self.w - v_w\n        self.b = self.b - v_b\n        self.append_log()\n        \n    elif self.algo == 'NAG':\n      v_w, v_b = 0, 0\n      for i in range(epochs):\n        dw, db = 0, 0\n        v_w = gamma * v_w\n        v_b = gamma * v_b\n        for x, y in zip(X, Y):\n          dw += self.grad_w(x, y, self.w - v_w, self.b - v_b)\n          db += self.grad_b(x, y, self.w - v_w, self.b - v_b)\n        v_w = v_w + eta * dw\n        v_b = v_b + eta * db\n        self.w = self.w - v_w\n        self.b = self.b - v_b\n        self.append_log()\n       \n            \n    elif self.algo == 'AdaGrad':\n      v_w, v_b = 0, 0\n      for i in range(epochs):\n        dw, db = 0, 0\n        for x, y in zip(X, Y):\n          dw += self.grad_w(x, y)\n          db += self.grad_b(x, y)\n        v_w += dw**2\n        v_b += db**2\n        self.w -= (eta \/ np.sqrt(v_w) + eps) * dw\n        self.b -= (eta \/ np.sqrt(v_b) + eps) * db\n        self.append_log()\n        \n    elif self.algo == 'RMSProp':\n      v_w, v_b = 0, 0\n      for i in range(epochs):\n        dw, db = 0, 0\n        for x, y in zip(X, Y):\n          dw += self.grad_w(x, y)\n          db += self.grad_b(x, y)\n        v_w = beta * v_w + (1 - beta) * dw**2\n        v_b = beta * v_b + (1 - beta) * db**2\n        self.w -= (eta \/ np.sqrt(v_w) + eps) * dw\n        self.b -= (eta \/ np.sqrt(v_b) + eps) * db\n        self.append_log()\n        \n    elif self.algo == 'Adam':\n      v_w, v_b = 0, 0\n      m_w, m_b = 0, 0\n      num_updates = 0\n      for i in range(epochs):\n        dw, db = 0, 0\n        for x, y in zip(X, Y):\n          dw = self.grad_w(x, y)\n          db = self.grad_b(x, y)\n          num_updates += 1\n          m_w = beta1 * m_w + (1-beta1) * dw\n          m_b = beta1 * m_b + (1-beta1) * db\n          v_w = beta2 * v_w + (1-beta2) * dw**2\n          v_b = beta2 * v_b + (1-beta2) * db**2\n          m_w_c = m_w \/ (1 - np.power(beta1, num_updates))\n          m_b_c = m_b \/ (1 - np.power(beta1, num_updates))\n          v_w_c = v_w \/ (1 - np.power(beta2, num_updates))\n          v_b_c = v_b \/ (1 - np.power(beta2, num_updates))\n          self.w -= (eta \/ np.sqrt(v_w_c) + eps) * m_w_c\n          self.b -= (eta \/ np.sqrt(v_b_c) + eps) * m_b_c\n          self.append_log()\n        \n  def append_log(self):\n    self.w_h.append(self.w)\n    self.b_h.append(self.b)\n    self.e_h.append(self.error(self.X, self.Y))","7a8f1312":"X = np.asarray([3.5, 0.35, 3.2, -2.0, 1.5, -0.5])\nY = np.asarray([0.5, 0.50, 0.5,  0.5, 0.1,  0.3])\n\nalgo = 'Adam'\n\nw_init = -6\nb_init = 4.0\n\nw_min = -7\nw_max = 5\n\nb_min = -7\nb_max = 5\n\nepochs = 200\ngamma = 0.9\neta = 0.5\neps = 1e-8\n\nanimation_frames = 20\n\nplot_2d = True\nplot_3d = False","5f2081d9":"sn = SN(w_init, b_init, algo)\nsn.fit(X, Y, epochs=epochs, eta=eta, gamma=gamma)\nplt.plot(sn.e_h, 'r')\nplt.plot(sn.w_h, 'b')\nplt.plot(sn.b_h, 'g')\n# w_diff = [t - s for t, s in zip(sn.w_h, sn.w_h[1:])]\n# b_diff = [t - s for t, s in zip(sn.b_h, sn.b_h[1:])]\n# plt.plot(w_diff, 'b--')\n# plt.plot(b_diff, 'g--')\nplt.show()\n","327a82f8":"def plot_animate_3d(i):\n  i = int(i*(epochs\/animation_frames))\n  line1.set_data(sn.w_h[:i+1], sn.b_h[:i+1])\n  line1.set_3d_properties(sn.e_h[:i+1])\n  line2.set_data(sn.w_h[:i+1], sn.b_h[:i+1])\n  line2.set_3d_properties(np.zeros(i+1) - 1)\n  title.set_text('Epoch: {: d}, Error: {:.4f}'.format(i, sn.e_h[i]))\n  return line1, line2, title","d840a161":"if plot_3d: \n  W = np.linspace(w_min, w_max, 256)\n  b = np.linspace(b_min, b_max, 256)\n  WW, BB = np.meshgrid(W, b)\n  Z = sn.error(X, Y, WW, BB)\n\n  fig = plt.figure(dpi=100)\n  ax = fig.gca(projection='3d')\n  surf = ax.plot_surface(WW, BB, Z, rstride=3, cstride=3, alpha=0.5, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n  cset = ax.contourf(WW, BB, Z, 25, zdir='z', offset=-1, alpha=0.6, cmap=cm.coolwarm)\n  ax.set_xlabel('w')\n  ax.set_xlim(w_min - 1, w_max + 1)\n  ax.set_ylabel('b')\n  ax.set_ylim(b_min - 1, b_max + 1)\n  ax.set_zlabel('error')\n  ax.set_zlim(-1, np.max(Z))\n  ax.view_init (elev=25, azim=-75) # azim = -20\n  ax.dist=12  \n  title = ax.set_title('Epoch 0')","32a4df83":"if plot_3d: \n  i = 0\n  line1, = ax.plot(sn.w_h[:i+1], sn.b_h[:i+1], sn.e_h[:i+1], color='black',marker='.')\n  line2, = ax.plot(sn.w_h[:i+1], sn.b_h[:i+1], np.zeros(i+1) - 1, color='red', marker='.')\n  anim = animation.FuncAnimation(fig, func=plot_animate_3d, frames=animation_frames)\n  rc('animation', html='jshtml')\n  anim","1e9a265c":"if plot_2d: \n  W = np.linspace(w_min, w_max, 256)\n  b = np.linspace(b_min, b_max, 256)\n  WW, BB = np.meshgrid(W, b)\n  Z = sn.error(X, Y, WW, BB)\n\n  fig = plt.figure(dpi=100)\n  ax = plt.subplot(111)\n  ax.set_xlabel('w')\n  ax.set_xlim(w_min - 1, w_max + 1)\n  ax.set_ylabel('b')\n  ax.set_ylim(b_min - 1, b_max + 1)\n  title = ax.set_title('Epoch 0')\n  cset = plt.contourf(WW, BB, Z, 25, alpha=0.6, cmap=cm.bwr)\n  plt.show()","d8d6e6be":"def plot_animate_2d(i):\n  i = int(i*(epochs\/animation_frames))\n  line.set_data(sn.w_h[:i+1], sn.b_h[:i+1])\n  title.set_text('Epoch: {: d}, Error: {:.4f}'.format(i, sn.e_h[i]))\n  return line, title","23968ad0":"if plot_2d: \n  i = 0\n  line, = ax.plot(sn.w_h[:i+1], sn.b_h[:i+1], color='black',marker='.')\n  anim = animation.FuncAnimation(fig, func=plot_animate_2d, frames=animation_frames)\n  rc('animation', html='jshtml')\n  anim","c3b9a62d":"if algo == 'GD':\n  print('algo = {}, eta = {}'.format(algo, eta))\nelif algo == 'Momentum' or algo == 'NAG':\n  print('algo = {}, eta = {}, gamma = {}'.format(algo, eta, gamma))\nelif algo == 'MiniBatch':\n  print('algo = {}, eta = {}, batch size = {}'.format(algo, eta, mini_batch_size))\nelif algo == 'AdaGrad' or algo == 'RMSProp':\n  print('algo = {}, eta = {}, eps = {}'.format(algo, eta, eps))\nanim\n\n","2ccbda40":"<h2>What are optimization algorithums ? \ud83e\udd14<\/h2>\n<ul>\n<li>The process of minimizing (or maximizing) any mathematical expression is called optimization.<\/li>\n<li>Optimizers are algorithms or methods used to change the attributes of the neural network such as weights and learning rate to reduce the losses.<\/li>\n<\/ul>","7cad6d77":"<Center><h2>Small preview<\/h2><\/Center>","f4e7b88c":"   <Center ><h1>Motivation for <b>Gradient descent<\/b> to <b>ADAM<\/b> algorithum \ud83e\udd17\ud83e\udd17<\/h1><\/Center>\n   <ul>\n   <li><h4><a href=\"https:\/\/lucid.app\/lucidchart\/87d0520b-d43f-4788-b111-ebecb0211d76\/edit?invitationId=inv_48c1ab02-7d86-46af-b7ac-54e5b7b046c6\">https:\/\/lucid.app\/lucidchart\/87d0520b-d43f-4788-b111-ebecb0211d76\/edit?invitationId=inv_48c1ab02-7d86-46af-b7ac-54e5b7b046c6<\/a><\/h4><\/li>\n   <li><h4>To view the  <b>mind map<\/b> please paste the url in you browser<\/h2><\/li>\n   <li><h4>To Visualize in you hands try to change the<b> algo== <\/b>option in the next block<\/h2><\/li>\n   <\/ul>\n","751c82c1":"<img src=\"\" width=3000><\/img>","10a4f291":"<center><h1>Optimization Algorithms in Deep Learning \ud83d\ude0d\ud83d\ude0d<\/h1><\/center>","fc679fec":"<Center ><h1>Motivation for <b>Gradient descent<\/b> to <b>ADAM<\/b> algorithum \ud83e\udd17\ud83e\udd17<\/h1><\/Center>\n   <ul>\n    <li><h4><a href=\"https:\/\/lucid.app\/lucidchart\/87d0520b-d43f-4788-b111-ebecb0211d76\/edit?invitationId=inv_48c1ab02-7d86-46af-b7ac-54e5b7b046c6\">https:\/\/lucid.app\/lucidchart\/87d0520b-d43f-4788-b111-ebecb0211d76\/edit?invitationId=inv_48c1ab02-7d86-46af-b7ac-54e5b7b046c6<\/a><\/h4><\/li>\n   <li><h4>To view the  <b>mind map<\/b> of optimization algorithums please paste the url in you browser(Zoom it)<\/h2><\/li>","447ac8a5":"<img src=\"https:\/\/res.cloudinary.com\/qna\/image\/upload\/v1631126772\/photo_2021-09-08_23-13-55_evssvk.jpg\" width=2000><\/img>","9597b801":"<center><img src=\"https:\/\/static.wixstatic.com\/media\/3eee0b_33163162ddd94900b7d9f5b049e9b7e3~mv2.gif\"><\/img><\/center>","4b2dae65":"<center><img src=\"https:\/\/c.tenor.com\/GYSBtk98T8sAAAAC\/conspiracy-big-bang-theory.gif\" width=500><\/img><center>","82301825":"<Center><h3>This is the plot showing the movement of<B> w,b and the loss <\/B>towards their optimum values  \ud83e\udd78 <\/h3><Center>","1dfa9733":"<h4>This is an interactive implentation of All optmization algorithums with a Sigmoid neuron because it is very handy for explanation \ud83d\ude05<\/h4>\n<center><img src=\"https:\/\/machinelearningknowledge.ai\/wp-content\/uploads\/2019\/06\/Sigmoid_Neuron_Working.gif\" width=500><\/img><center>","56ae4de5":"<Center><h3>This is the Contour plot of the Error surface in the 2D dimension  \ud83e\udd78 <\/h3><Center>","9da6f74e":" <center><h1>Properties and functions of the class<\/h1><center>\n <ul>\n \n <li><b>__init__<\/b> is basically a constructor for initializing all features of the class w_h reffer to weights history, b_h reffers to bias history<\/li>\n  <li><b>Sigmoid<\/b> function is basically the calculation of sigmoif function<b>(1\/1+e^-x)<\/b><\/li>\n\n <li>\n <b>Error<\/b> function is used to calculate the loss of the model<\/b><\/li>\n  <li><b>Grad_w<\/b> func is used to calculate the gradients of the weight<\/b><\/li>\n  <li><b>Grad_b<\/b> func is used to calculate the gradients of the bias<\/b><\/li>\n  <li><b>Grad_b<\/b> func is used to calculate the gradients of the bias<\/b><\/li>\n  <li><b>if self.algo == 'GD'<\/b> func is used to train the model using gradient descent as the optimization algorithum <\/b><\/li>\n  <li><b>if self.algo == 'MiniBatch'<\/b> func is used to train the model using Mini batch gradient descent as the optimization algorithum <\/b><\/li>\n  <li><b>if self.algo == 'Momentum'<\/b> func is used to train the model using Momentum based gradient descent as the optimization algorithum <\/b><\/li>\n  <li><b>if self.algo == 'NAG'<\/b> func is used to train the model using Nesterov accelerated gradient descent as the optimization algorithum <\/b><\/li>\n  <li><b>if self.algo == 'Adagrad'<\/b> func is used to train the model using <b>Adaptive learning rate<\/b> as the optimization algorithum <\/b><\/li>\n <li> <b>if self.algo == 'RMSprop'<\/b> func is used to train the model using RMSPROP as the optimization algorithum and it is a modified version of Adagrad with decaying history <\/b><\/li>\n  <li><b>if self.algo == 'Adam'<\/b> func is used to train the model using Adam as the optimization algorithum.It is the combination of both RMSprop and Momentum based gradient descent <\/b><\/li>\n\n  <\/ul>\n\n\n  \n  \n  \n ","fc9342fe":"<h2>Optimization algorithums and their differnces that will be discussed in this notebook<\/h2>\n<ul>\n<li>Gradient descent<\/li>\n<li>Gradient descent with momentum<\/li>\n<li>NAG<\/li>\n<li>Adagrad<\/li>\n<li>RMSprop<\/li>\n<li>Adam<\/li>\n<li>MiniBatchGradient descent<\/li>\n\n<\/ul>","e3a3ca28":"<Center><h3>This is an interseting visualization of the hardwork made my the optimization algorithums \ud83e\udd17  \ud83e\udd78 <\/h3><Center>"}}