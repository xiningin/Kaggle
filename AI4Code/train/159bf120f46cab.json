{"cell_type":{"4289defb":"code","da2926ca":"code","d44a92a0":"code","585172bc":"code","c521d661":"code","038d7498":"code","7c41e987":"code","ed09261e":"code","0c3f4fc8":"code","3652d515":"code","12587b64":"code","bd7310d4":"code","7c5122fb":"code","9cda3a77":"code","28624d7b":"code","5f157fa7":"code","8c24e8ae":"code","ba0df73c":"code","2f4fb9ca":"code","a89cc68b":"code","80f539c7":"code","0b7b85f7":"code","fafb9808":"code","3a45da83":"code","e1a2d963":"code","4e85063a":"code","acc4350c":"code","7d72f443":"code","9caa06c0":"code","0ee3688c":"code","e6c7796e":"code","6ce22cd5":"code","44f6d3b8":"code","74933787":"code","778fbc7c":"code","5ca3985d":"code","1f1a27b9":"code","d6a99fa0":"code","1d6c473f":"code","b49e7567":"code","856af9fe":"code","d124a8b6":"code","4471bf66":"code","6c29e36f":"code","9dd7bfe5":"code","edec138b":"code","bf50f8ad":"code","7bcc0ba9":"code","290b43f4":"code","e3e53607":"code","5dd7519d":"code","4066532d":"markdown","77190030":"markdown","5b6739af":"markdown","e2049145":"markdown","e3252514":"markdown","53466c88":"markdown","20d916b2":"markdown","384831c2":"markdown","ff10db7e":"markdown","b282eeb8":"markdown","16e65e69":"markdown","8adfa3ac":"markdown","cdb88f04":"markdown","881b18a2":"markdown","671de700":"markdown","7395bb6a":"markdown"},"source":{"4289defb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","da2926ca":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nfrom keras.models import Sequential\nfrom keras.layers import Dense","d44a92a0":"df = pd.read_csv(\"..\/input\/dataisbeautiful\/r_dataisbeautiful_posts.csv\")","585172bc":"df.head()","c521d661":"df.isna().sum() # Checking for 'Nan' values if any","038d7498":"del df['author_flair_text']\ndel df['removed_by']\ndel df['total_awards_received']\ndel df['awarders']\ndel df['id']\ndel df['created_utc']\ndel df['full_link']","7c41e987":"df.head()","ed09261e":"df.title.fillna(\" \",inplace = True)","0c3f4fc8":"df.title.value_counts()","3652d515":"df['text'] = df['title'] + ' ' + df['author']\ndel df['title']\ndel df['author']","12587b64":"df.head()","bd7310d4":"df.over_18.replace(True,1,inplace = True)\ndf.over_18.replace(False,0,inplace = True)","7c5122fb":"df.over_18.value_counts()","9cda3a77":"x = df[:100000]\ntrain_false = x[x.over_18 == 0.0].text\ntrain_true = x[x.over_18 == 1.0].text\ntrain_text = df.text.values[:100000]\ntest_text = df.text.values[100000:]\ntrain_category = df.over_18[:100000]\ntest_category = df.over_18[100000:]","28624d7b":"plt.figure(figsize = (20,20))\nwc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(\" \".join(train_true)))\nplt.imshow(wc,interpolation = 'bilinear')","5f157fa7":"text_true = wc.process_text(str(\" \".join(train_true))) # Getting the most frequently used words from wordcloud \nlist(text_true.keys())[:10]","8c24e8ae":"len(text_true.keys())","ba0df73c":" text_true = sorted(text_true.items(),key = \n             lambda kv:(kv[1], kv[0]))","2f4fb9ca":"ans_true = []\nfor i in text_true:\n    ans_true.append(i[0])\nans_true [:5] ","a89cc68b":"predictions = []\nfor i in test_text:\n    x = i.split()\n    for j in x:\n        if j in ans_true:\n            predictions.append(1)\n            break\n        else:\n            predictions.append(0)\n            break\nlen(predictions)","80f539c7":"len(test_category)","0b7b85f7":"count = 0\nfor i in range(len(predictions)):\n    test_category = list(test_category)\n    if(predictions[i] == int(test_category[i])):\n        count += 1\nprint(count)","fafb9808":"accuracy = (count\/len(predictions))*100\naccuracy","3a45da83":"print(\"Accuracy using WordCloud is : \", accuracy , \"%\")","e1a2d963":"stop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","4e85063a":"def get_simple_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","acc4350c":"lemmatizer = WordNetLemmatizer()\ndef lemmatize_words(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            pos = pos_tag([i.strip()])\n            word = lemmatizer.lemmatize(i.strip(),get_simple_pos(pos[0][1]))\n            final_text.append(word.lower())\n    return final_text        ","7d72f443":"df.text = df.text.apply(lemmatize_words)","9caa06c0":"def join_text(text):\n    string = ''\n    for i in text:\n        string += i.strip() +' '\n    return string    ","0ee3688c":"df.text = df.text.apply(join_text)","e6c7796e":"train_message = df.text[:150000]\ntest_message = df.text[150000:]\ntrain_category = df.over_18[:150000]\ntest_category = df.over_18[150000:]","6ce22cd5":"cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,2))\n#transformed train reviews\ncv_train_reviews=cv.fit_transform(train_message)\n#transformed test reviews\ncv_test_reviews=cv.transform(test_message)\n\nprint('BOW_cv_train:',cv_train_reviews.shape)\nprint('BOW_cv_test:',cv_test_reviews.shape)","44f6d3b8":"tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,2))\n#transformed train reviews\ntv_train_reviews=tv.fit_transform(train_message)\n#transformed test reviews\ntv_test_reviews=tv.transform(test_message)\nprint('Tfidf_train:',tv_train_reviews.shape)\nprint('Tfidf_test:',tv_test_reviews.shape)","74933787":"lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n#Fitting the model for Bag of words\nlr_bow=lr.fit(cv_train_reviews,train_category)\nprint(lr_bow)\n#Fitting the model for tfidf features\nlr_tfidf=lr.fit(tv_train_reviews,train_category)\nprint(lr_tfidf)","778fbc7c":"#Predicting the model for bag of words\nlr_bow_predict=lr.predict(cv_test_reviews)\n##Predicting the model for tfidf features\nlr_tfidf_predict=lr.predict(tv_test_reviews)","5ca3985d":"#Accuracy score for bag of words\nlr_bow_score=accuracy_score(test_category,lr_bow_predict)\nprint(\"lr_bow_score :\",lr_bow_score)\n#Accuracy score for tfidf features\nlr_tfidf_score=accuracy_score(test_category,lr_tfidf_predict)\nprint(\"lr_tfidf_score :\",lr_tfidf_score)","1f1a27b9":"#Classification report for bag of words\nlr_bow_report=classification_report(test_category,lr_bow_predict,target_names=['0','1'])\nprint(lr_bow_report)\n\n#Classification report for tfidf features\nlr_tfidf_report=classification_report(test_category,lr_tfidf_predict,target_names=['0','1'])\nprint(lr_tfidf_report)","d6a99fa0":"plot_confusion_matrix(lr_bow, cv_test_reviews, test_category,display_labels=['0','1'],cmap=\"Blues\",values_format = '')\nplot_confusion_matrix(lr_tfidf, tv_test_reviews, test_category,display_labels=['0','1'],cmap=\"Blues\",values_format = '')","1d6c473f":"#training the model\nmnb=MultinomialNB()\n#fitting the nb for bag of words\nmnb_bow=mnb.fit(cv_train_reviews,train_category)\nprint(mnb_bow)\n#fitting the nb for tfidf features\nmnb_tfidf=mnb.fit(tv_train_reviews,train_category)\nprint(mnb_tfidf)","b49e7567":"#Predicting the model for bag of words\nmnb_bow_predict=mnb.predict(cv_test_reviews)\n#Predicting the model for tfidf features\nmnb_tfidf_predict=mnb.predict(tv_test_reviews)","856af9fe":"#Accuracy score for bag of words\nmnb_bow_score=accuracy_score(test_category,mnb_bow_predict)\nprint(\"mnb_bow_score :\",mnb_bow_score)\n#Accuracy score for tfidf features\nmnb_tfidf_score=accuracy_score(test_category,mnb_tfidf_predict)\nprint(\"mnb_tfidf_score :\",mnb_tfidf_score)","d124a8b6":"mnb_bow_report = classification_report(test_category,mnb_bow_predict,target_names = ['0','1'])\nprint(mnb_bow_report)\nmnb_tfidf_report = classification_report(test_category,mnb_tfidf_predict,target_names = ['0','1'])\nprint(mnb_tfidf_report)","4471bf66":"plot_confusion_matrix(mnb_bow, cv_test_reviews, test_category,display_labels=['0','1'],cmap=\"Blues\",values_format = '')\nplot_confusion_matrix(mnb_tfidf, tv_test_reviews, test_category,display_labels=['0','1'],cmap=\"Blues\",values_format = '')","6c29e36f":"model = Sequential()\nmodel.add(Dense(units = 128 , activation = 'relu' , input_dim = cv_train_reviews.shape[1]))\nmodel.add(Dense(units = 64 , activation = 'relu'))\nmodel.add(Dense(units = 32 , activation = 'relu'))\nmodel.add(Dense(units = 32 , activation = 'relu'))\nmodel.add(Dense(units = 16 , activation = 'relu'))\nmodel.add(Dense(units = 1 , activation = 'sigmoid'))\nmodel.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\nmodel.summary()","9dd7bfe5":"history = model.fit(cv_train_reviews,train_category , epochs = 3 , batch_size = 128 , validation_data = (cv_test_reviews,test_category))","edec138b":"epochs = [i for i in range(3)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Validation Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'r-o' , label = 'Testing Loss')\nax[1].set_title('Testing Accuracy & Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","bf50f8ad":"predictions = model.predict_classes(cv_test_reviews)\npredictions[:5]","7bcc0ba9":"print(classification_report(test_category, predictions, target_names = ['0','1']))","290b43f4":"cm = confusion_matrix(test_category,predictions)\ncm","e3e53607":"cm = pd.DataFrame(cm , index = ['0','1'] , columns = ['0','1'])","5dd7519d":"plt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')","4066532d":"**WHAT ARE STOPWORDS?**\n\n**Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc. Such words are already captured this in corpus named corpus. We first download it to our python environment.**","77190030":"**Wow! Using just WordCloud, we have got an 89 accuracy! Now we will compare this result by testing this dataset on different classifiers.**","5b6739af":"**Deleting columns that are of no use**","e2049145":"**WHAT DOES COUNT VECTORIZER AND TFIDF VECTORIZER DO?**\n\n**CountVectorizer just counts the word frequencies. Simple as that.**\n\n**With the TFIDFVectorizer the value increases proportionally to count, but is offset by the frequency of the word in the corpus. - This is the IDF (inverse document frequency part).**\n\n**This helps to adjust for the fact that some words appear more frequently.\nHow often do you really use crepuscular, and petrichor?\nSure, they are important words, and should be used when appropriate, but how often do we really use them?\nWithout the inverse document frequency part common, less meaning giving words such as \u201cthe\u201d (assuming you don\u2019t filter stopwords) would bear a higher weight than these less frequent words.\nInverse document frequency is defined as:**\n\n**idf(t,D)=logN|d\u2208D:t\u2208d|**\n\n**Where N is the total number of documents in the corpus, and |d\u2208D:t\u2208d| is the number of documents where t appears.**","e3252514":"**Now for each word in every test data point , we will just check that if any word of that test data point is present in our dictionary ans_true which contains the most frequent 3000 words of label 1. If the word is present , then we will simply predict 1, otherwise 0.**","53466c88":"**We will now hypothesise importance of some words (most frequently used 3000 words) using WordCloud and based on that we will test and check if our hypothesis is correct**","20d916b2":"# Description of Dataset\n**Data is Beautiful, is a place for visual representations of data: Graphs, charts, maps, etc. DataIsBeautiful is for visualizations that effectively convey information. Aesthetics are an important part of information visualization, but pretty pictures are not the aim of this subreddit.This dataset contains a couple of fields with the information based on Reddit post submission, such as : id,title,score,author,authorfalirtext,removed_by,totalawardsreceived,awarders,created_utc,full_link,num_commnets,over_18**\n![image.png](attachment:image.png)","384831c2":"**WORDCLOUD FOR OVER 18 TEXT (LABEL 1) **","ff10db7e":"# Importing The Necessary Libraries","b282eeb8":"# Data Visualization and Preprocessing ","16e65e69":"**WHAT IS LEMMATIZATION AND STEMMING?**\n\n**For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\nThe goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:**\n* am, are, is $\\Rightarrow$ be\n* car, cars, car's, cars' $\\Rightarrow$ car\n* The result of this mapping of text will be something like:\n* the boy's cars are different colors $\\Rightarrow$\n* the boy car be differ color\n\n**However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source.**","8adfa3ac":"**Splitting the data into 2 parts - training and testing data**","cdb88f04":"# TRAINING WITH DIFFERENT CLASSIFIERS AND ANALYSIS AFTER TESTING","881b18a2":"**Sorting dictionary based on key values**","671de700":"# Loading the Dataset","7395bb6a":"**Splitting the data into 2 parts - training and testing data**"}}