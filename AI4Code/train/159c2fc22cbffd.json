{"cell_type":{"5df18321":"code","b8dfcc57":"code","f3bb97ec":"code","ce2b9ccd":"code","732a954e":"code","4fe03360":"code","0059f0f0":"code","26223786":"code","6f3751ca":"code","767d390f":"code","13d84b9d":"code","bcae99ed":"code","f52f8bb1":"code","cd0e31bc":"code","a1c8c529":"code","a9e877b5":"code","b797a6c7":"code","cc8174bb":"code","e9357533":"code","52a1a23f":"code","110b8feb":"code","c3340c85":"code","27580ea9":"code","25695c01":"code","94308ca6":"code","5eb6bcc8":"code","bfe991da":"code","051c843d":"code","418eee2b":"code","f0ef0183":"code","3cf54c32":"markdown","b72d9dc4":"markdown","91c68fdf":"markdown","d7dc5d31":"markdown","31a31687":"markdown"},"source":{"5df18321":"import os\nimport sys\nimport datetime\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\nimport optuna\nfrom optuna.trial import TrialState","b8dfcc57":"train_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv\")\npd.set_option(\"display.max_columns\", 77)","f3bb97ec":"train_df.head()","ce2b9ccd":"train_df[\"target\"].value_counts()","732a954e":"encoder = LabelEncoder()\ntrain_df['target'] = encoder.fit_transform(train_df['target'])","4fe03360":"feature_cols = list(train_df.columns)[1:-1]","0059f0f0":"scaler = StandardScaler()\ntrain_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\ntest_df[feature_cols] = scaler.transform(test_df[feature_cols])","26223786":"train_df.head()","6f3751ca":"X = train_df.drop(['target','id'],axis=1)\ny = train_df['target']","767d390f":"X.head()","13d84b9d":"y.head()","bcae99ed":"sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=0)\n\nfor train_index, val_index in sss.split(X, y):    \n    print(\"TRAIN:\\t\", train_index, \"Size:\\t\", len(train_index))\n    print(\"VAL:\\t\", val_index, \"Size:\\t\", len(val_index))\n    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[val_index]","f52f8bb1":"y_train.value_counts()\/y.value_counts()","cd0e31bc":"y_val.value_counts()\/y.value_counts()","a1c8c529":"CLASSES = 9\nNUM_FEATURES = 75\nDEVICE = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda\")\n\nBATCHSIZE = 1000\nEPOCHS = 10\nN_TRAIN_EXAMPLES = BATCHSIZE * 14\nN_VALID_EXAMPLES = BATCHSIZE * 6\n\ncriterion = nn.CrossEntropyLoss()\n\ndef define_model(trial):\n    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n    n_layers = trial.suggest_int(\"n_layers\", 1, 10)\n    layers = []\n\n    in_features = NUM_FEATURES\n    for i in range(n_layers):\n        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 32, 1024)\n        layers.append(nn.Linear(in_features, out_features))\n        layers.append(nn.ReLU(inplace=True))\n        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.1, 0.5)\n        layers.append(nn.Dropout(p))\n        layers.append(nn.BatchNorm1d(out_features))\n\n        in_features = out_features\n        \n    layers.append(nn.Linear(in_features, CLASSES))\n\n    return nn.Sequential(*layers)\n\ndef get_data(X_train, y_train, X_val, y_val):\n    train = TensorDataset(torch.Tensor(np.array(X_train)), torch.Tensor(np.array(y_train)))\n    train_loader = DataLoader(train, batch_size = 10000, shuffle = True)\n\n    val = TensorDataset(torch.Tensor(np.array(X_val)), torch.Tensor(np.array(y_val)))\n    val_loader = DataLoader(val, batch_size = 10000, shuffle = True)\n\n    return train_loader, val_loader\n\ndef objective(trial):\n    model = define_model(trial).to(DEVICE)\n    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n    \n    train_loader, valid_loader = get_data(X_train, y_train, X_val, y_val)\n    \n    for epoch in range(EPOCHS):\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n\n            optimizer.zero_grad()\n            output = model(data)\n            \n            loss = criterion(output, target.long())\n            loss.backward()\n            optimizer.step()\n\n        # Validation of the model.\n        model.eval()\n        val_loss = []\n        with torch.no_grad():\n            for batch_idx, (data, target) in enumerate(valid_loader):\n                data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n                output = model(data)\n                \n                loss = criterion(output, target.long())\n                val_loss.append(loss.item())\n        \n        avg_val_loss = np.mean(val_loss)\n\n        trial.report(avg_val_loss, epoch)\n\n        # Handle pruning based on the intermediate value.\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n    \n    return avg_val_loss","a9e877b5":"DEVICE","b797a6c7":"study = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=50, timeout=600)","cc8174bb":"pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n\nprint(\"Study statistics: \")\nprint(\"  Number of finished trials: \", len(study.trials))\nprint(\"  Number of pruned trials: \", len(pruned_trials))\nprint(\"  Number of complete trials: \", len(complete_trials))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: \", trial.value)\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","e9357533":"optuna.visualization.plot_intermediate_values(study).show()","52a1a23f":"batch_size = 1000\n\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=0)\n\nfor train_index, val_index in sss.split(X, y):    \n    print(\"TRAIN:\\t\", train_index, \"Size:\\t\", len(train_index))\n    print(\"VAL:\\t\", val_index, \"Size:\\t\", len(val_index))\n    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n\ntrain = TensorDataset(torch.Tensor(np.array(X_train)), torch.Tensor(np.array(y_train)))\ntrain_loader = DataLoader(train, batch_size = batch_size, shuffle = True)\n\nval = TensorDataset(torch.Tensor(np.array(X_val)), torch.Tensor(np.array(y_val)))\nval_loader = DataLoader(val, batch_size = batch_size, shuffle = True)\n\nphases = [\"train\", \"val\"]\nloaders = {\"train\": train_loader, \"val\": val_loader}","110b8feb":"y_train.value_counts()\/y.value_counts()","c3340c85":"y_val.value_counts()\/y.value_counts()","27580ea9":"class classification_model(nn.Module):\n    def __init__(self, n_in, n_out, layers, p=None):\n        super(classification_model, self).__init__()\n\n        all_layers = []\n        self.n_in = n_in\n        self.n_out = n_out\n\n        for i in range(len(layers)):\n            all_layers.append(nn.Linear(self.n_in, layers[i]))\n            all_layers.append(nn.ReLU(inplace=True))\n            if p:\n                all_layers.append(nn.Dropout(p[i]))\n            all_layers.append(nn.BatchNorm1d(layers[i]))\n            self.n_in = layers[i]\n\n        all_layers.append(nn.Linear(layers[-1], self.n_out))\n\n        self.layers = nn.Sequential(*all_layers)\n\n    def forward(self, x):\n        x = self.layers(x)\n        return x\n\n    net_name = \"classification_model\"","25695c01":"\"\"\"\nBest trial:\n  Value:  1.7740078568458557\n  Params: \n    n_layers: 2\n    n_units_l0: 582\n    dropout_l0: 0.32774551169492927\n    n_units_l1: 73\n    dropout_l1: 0.20755288576753822\n    optimizer: RMSprop\n    lr: 0.0009641167645278553\n\"\"\"\n\nlayers_ = []\ndropout_ = []\nl_rate = trial.params[\"lr\"]\n\nfor i in range(trial.params[\"n_layers\"]):\n    layers_.append(trial.params[f\"n_units_l{i}\"])\n    dropout_.append(trial.params[f\"dropout_l{i}\"])\n\ndevice = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda\")\n#model = classification_model(75, 9, [1007, 552, 607, 564, 595], p=[0.4458008129590088, 0.41239986533350254, 0.4446225146055265, 0.42198656364503173, 0.10940623981859748])\nmodel = classification_model(75, 9, layers_, p=dropout_)\nmodel.to(device)\n\n#l_rate = 0.0009624002880303564\n\ncriterion = nn.CrossEntropyLoss()\nif trial.params[\"optimizer\"] == \"RMSprop\":\n    optimizer = optim.RMSprop(model.parameters(), lr=l_rate)\nelif trial.params[\"optimizer\"] == \"Adam\":\n    optimizer = optim.Adam(model.parameters(), lr=l_rate)\nelif trial.params[\"optimizer\"] == \"SGD\":\n    optimizer = optim.SGD(model.parameters(), lr=l_rate)\n\nprint(device)\nprint(model)","94308ca6":"loss_train = []\nloss_valid = []\nbest_validation_loss = 1000\nbest_epoch = 1\ncorrect = 0\ntotal = 0\n\nn_epochs = 50\n\nnow = datetime.datetime.now()\nweights_path = \".\/output\/{:%Y%m%dT%H%M}\".format(now)\nos.makedirs(weights_path, exist_ok=True)\n\nbreak_st = False\n\nfor epoch in range(1, n_epochs + 1):\n    for phase in phases:\n        if phase == \"train\":\n            model.train()\n        elif phase == \"val\":\n            model.eval()\n            \n        for _, data in enumerate(loaders[phase], 0):\n            features, y_true = data[0], data[1]\n            features = features.to(device, dtype=torch.float)\n            y_true = y_true.to(device, dtype=torch.float)\n\n            optimizer.zero_grad()\n            \n            with torch.set_grad_enabled(phase == \"train\"):\n                y_pred = model(features)\n                \n                sm = nn.Softmax(dim=1)\n                pred_percentage = sm(y_pred)\n                \n                if break_st:\n                    break\n                    \n                y_true = y_true.long()\n                \n                _, preds = torch.max(pred_percentage, 1)\n                total += y_true.size(0)\n                correct += (preds == y_true).sum().item()\n                    \n                loss = criterion(y_pred, y_true)\n                \n                if phase == \"val\":\n                    loss_valid.append(loss.item())\n\n                if phase == \"train\":\n                    loss_train.append(loss.item())\n                    loss.backward()\n                        \n                    optimizer.step()\n        \n        if break_st:\n            break\n        \n        if phase == \"train\":\n            mean_train_loss = np.mean(loss_train)\n            acc_train = 100 * correct \/ total\n            loss_train = []\n            correct = 0\n            total = 0\n            \n        if phase == \"val\":\n            validation_loss = np.mean(loss_valid)\n            acc_valid = 100 * correct \/ total\n            loss_valid = []\n            correct = 0\n            total = 0\n    \n    if break_st:\n        break\n        \n    if validation_loss < best_validation_loss:\n        print(\"saving weights...\")\n        best_epoch = epoch\n        best_validation_loss = validation_loss\n        torch.save(model.state_dict(),\n            os.path.join(weights_path, \"model.pt\"),\n            )\n        \n    print(f\"Epoch={epoch}\/{n_epochs}\\tloss={mean_train_loss:.4f}\\tval_loss={validation_loss:.4f}\\tacc={acc_train:.4f}\\tval_acc={acc_valid:.4f}\")\n    ","5eb6bcc8":"test_df.head()","bfe991da":"X_test = test_df.drop(['id'],axis=1)\n\ntest = TensorDataset(torch.Tensor(np.array(X_test)))\ntest_loader = DataLoader(test, batch_size = 100000, shuffle = False)","051c843d":"if not weights_path:\n    print(\"Choose weights path\")\n    sys.exit()\n\ndevice = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda\")\n\nmodel_name = weights_path + \"\/model.pt\"\nprint(model_name)\nstate_dict = torch.load(model_name, map_location=device)\nmodel.load_state_dict(state_dict)\nmodel.eval()\n\nfor _, data in enumerate(test_loader, 0):\n    features = data[0]\n    print(features.size())\n    features = features.to(device, dtype=torch.float)\n    \n    with torch.set_grad_enabled(False):\n        y_pred = model(features)\n        \n        sm = nn.Softmax(dim=1)\n        pred_percentage = sm(y_pred)\n        \n        print(pred_percentage.size())\n\nprint(pred_percentage.detach().cpu().numpy())\nprint(\"DONE!\")","418eee2b":"sub = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")\npred_array = pred_percentage.detach().cpu().numpy()\nsub.loc[:,\"Class_1\":\"Class_9\"] = pred_array\nsub = sub.set_index(\"id\")\nsub.head()","f0ef0183":"sub.to_csv(\".\/output\/submission.csv\")","3cf54c32":"## Optuna","b72d9dc4":"### Submission","91c68fdf":"## Training with the best parameters","d7dc5d31":"### Stratified Split","31a31687":"### Testing"}}