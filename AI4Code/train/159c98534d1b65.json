{"cell_type":{"4e572bd2":"code","902c707f":"code","897714e5":"code","78346f6e":"code","a22230ae":"code","34da9c42":"code","13339a68":"code","d53edd94":"code","f22cb443":"code","7b472327":"code","601a451f":"code","48b52756":"code","c7d772fd":"code","420019d1":"code","88934d07":"code","976f17a3":"code","2d1922ff":"code","420b07f8":"code","1dcc5de9":"code","4ce90500":"code","1d03a4f3":"code","95e3ea9b":"code","4e24cb50":"code","9db933af":"code","b3e70fef":"code","4234c8a2":"code","c443a918":"code","5e6fd962":"code","81064372":"code","8d781432":"code","ca57c882":"code","c66a45d2":"code","8d22f4db":"code","2334be3c":"code","a0ad4244":"code","c44d0bee":"code","06a05a2a":"code","44695287":"code","a56abee9":"code","42e387c5":"code","ce53ffab":"code","989d1e25":"code","e9d874ec":"code","015cd416":"code","bc15d13c":"markdown","8bb4673e":"markdown","03b27eac":"markdown","6cdff62e":"markdown","aae81ac3":"markdown","a57e39d5":"markdown","32181fb1":"markdown","6bbd6063":"markdown","5a0a0309":"markdown"},"source":{"4e572bd2":"import numpy as np\nimport pandas as pd","902c707f":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","897714e5":"train","78346f6e":"test","a22230ae":"import pandas_profiling","34da9c42":"train.profile_report()","13339a68":"# Divide the training data into features and objective variables\ntrain_x = train.drop(['Survived'], axis=1)\ntrain_y = train['Survived']","d53edd94":"# The test data is only features, so you can leave it as it is.\ntest_x = test.copy()","f22cb443":"# remove the variable PassengerId\ntrain_x = train_x.drop(['PassengerId'], axis=1)\ntest_x = test_x.drop(['PassengerId'], axis=1)\n# remove variables Name, Ticket, and Cabin\ntrain_x = train_x.drop(['Name', 'Ticket', 'Cabin'], axis=1)\ntest_x = test_x.drop(['Name', 'Ticket', 'Cabin'], axis=1)","7b472327":"from sklearn.preprocessing import LabelEncoder\n\n# Apply label encoding to each categorical variable\nle = LabelEncoder()\nfor c in ['Sex', 'Embarked']:\n    le.fit(train_x[c].fillna('NA'))    \n    train_x[c] = le.transform(train_x[c].fillna('NA'))\n    test_x[c] = le.transform(test_x[c].fillna('NA'))","601a451f":"train_x.head()","48b52756":"from xgboost import XGBClassifier","c7d772fd":"# Create model and learn with training data\nmodel = XGBClassifier(n_estimators=20, random_state=71)\nmodel.fit(train_x, train_y)","420019d1":"# Output the predicted value of test data with probability\npred = model.predict_proba(test_x)[:, 1]","88934d07":"# Convert the predicted value of test data to binary\npred_label = np.where(pred > 0.5, 1, 0)","976f17a3":"# submission\nsubmission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': pred_label})\nsubmission.to_csv('submission_00.csv', index=False)\nsubmission","2d1922ff":"from sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.model_selection import KFold","420b07f8":"# cross validation\nscores_accuracy = []\nscores_logloss = []\nkf = KFold(n_splits=4, shuffle=True, random_state=71)\nfor tr_idx, va_idx in kf.split(train_x):\n    # divide into training data and validation data\n    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n    # train\n    model = XGBClassifier(n_estimators=20, random_state=71)\n    model.fit(tr_x, tr_y)\n    # predict\n    va_pred = model.predict_proba(va_x)[:, 1]\n    # Calculate the score in the validation data\n    logloss = log_loss(va_y, va_pred)\n    accuracy = accuracy_score(va_y, va_pred > 0.5)\n    # Save the scores\n    scores_logloss.append(logloss)\n    scores_accuracy.append(accuracy)","1dcc5de9":"print(scores_logloss)\nprint(scores_accuracy)","4ce90500":"# Output the average score of each fold\nlogloss = np.mean(scores_logloss)\naccuracy = np.mean(scores_accuracy)\nprint(f'logloss: {logloss:.4f}, accuracy: {accuracy:.4f}')","1d03a4f3":"import itertools","95e3ea9b":"# Prepare tuning candidate parameters\nparam_space = {\n    'max_depth': [3, 5, 7],\n    'min_child_weight': [1.0, 2.0, 4.0]\n}","4e24cb50":"# Combination of hyperparameters to search\nparam_combinations = itertools.product(param_space['max_depth'], param_space['min_child_weight'])\n\nparams = []\nscores = []\n# Cross-validation for each combination of parameters\nfor max_depth, min_child_weight in param_combinations:\n    score_folds = []\n    # cross validation\n    kf = KFold(n_splits=4, shuffle=True, random_state=123456)\n    for tr_idx, va_idx in kf.split(train_x):\n        # Divide into training data and validation data\n        tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n        # train\n        model = XGBClassifier(n_estimators=20, random_state=71,\n                              max_depth=max_depth, min_child_weight=min_child_weight)\n        model.fit(tr_x, tr_y)\n        # predict\n        va_pred = model.predict_proba(va_x)[:, 1]\n        logloss = log_loss(va_y, va_pred)\n        score_folds.append(logloss)\n    # Average the score for each fold\n    score_mean = np.mean(score_folds)\n    # Save the combination of parameters and the score of it\n    params.append((max_depth, min_child_weight))\n    scores.append(score_mean)","9db933af":"# Choose the parameter with the best score\nbest_idx = np.argsort(scores)[0]\nbest_param = params[best_idx]\nprint(f'max_depth: {best_param[0]}, min_child_weight: {best_param[1]}')","b3e70fef":"from sklearn.preprocessing import OneHotEncoder","4234c8a2":"train_x2 = train.drop(['Survived'], axis=1)\ntest_x2 = test.copy()\n\ntrain_x2 = train_x2.drop(['PassengerId'], axis=1)\ntest_x2 = test_x2.drop(['PassengerId'], axis=1)\ntrain_x2 = train_x2.drop(['Name', 'Ticket', 'Cabin'], axis=1)\ntest_x2 = test_x2.drop(['Name', 'Ticket', 'Cabin'], axis=1)","c443a918":"# one-hot encoding\ncat_cols = ['Sex', 'Embarked', 'Pclass']\nohe = OneHotEncoder(categories='auto', sparse=False)\nohe.fit(train_x2[cat_cols].fillna('NA'))","5e6fd962":"# Create a column of one-hot encoding\nohe_columns = []\nfor i, c in enumerate(cat_cols):\n    ohe_columns += [f'{c}_{v}' for v in ohe.categories_[i]]\n\nohe_columns","81064372":"# one-hot encoding\nohe_train_x2 = pd.DataFrame(ohe.transform(train_x2[cat_cols].fillna('NA')), columns=ohe_columns)\nohe_test_x2 = pd.DataFrame(ohe.transform(test_x2[cat_cols].fillna('NA')), columns=ohe_columns)\n# remove unnecessary old variables\ntrain_x2 = train_x2.drop(cat_cols, axis=1)\ntest_x2 = test_x2.drop(cat_cols, axis=1)\n# Join columns of one-hot encoding\ntrain_x2 = pd.concat([train_x2, ohe_train_x2], axis=1)\ntest_x2 = pd.concat([test_x2, ohe_test_x2], axis=1)","8d781432":"train_x2.head()","ca57c882":"test_x2.head()","c66a45d2":"train_x2.isnull().sum()","8d22f4db":"# replace missing values in numeric variables with mean values\n# num_cols = ['Age', 'SibSp', 'Parch', 'Fare']\nnum_cols = ['Age', 'Fare']\nfor col in num_cols:\n    train_x2[col].fillna(train_x2[col].mean(), inplace=True)\n    test_x2[col].fillna(train_x2[col].mean(), inplace=True)\n    \ntrain_x2.isnull().sum()","2334be3c":"test_x2.head()","a0ad4244":"!pip install ptitprince","c44d0bee":"from ptitprince import RainCloud\nimport matplotlib.pyplot as plt","06a05a2a":"fig, ax = plt.subplots(figsize=(10, 10))\nRainCloud(data=train_x2, y='Fare', orient='h')\nax.grid()","44695287":"# change the variable Fare x to log(x+1)\ntrain_x2['Fare'] = np.log1p(train_x2['Fare'])\ntest_x2['Fare'] = np.log1p(test_x2['Fare'])","a56abee9":"fig, ax = plt.subplots(figsize=(10, 10))\nRainCloud(data=train_x2, y='Fare', orient='h')\nax.grid()","42e387c5":"from sklearn.linear_model import LogisticRegression","ce53ffab":"# xgboost\nmodel_xgb = XGBClassifier(n_estimators=20, random_state=71)\nmodel_xgb.fit(train_x, train_y)\npred_xgb = model_xgb.predict_proba(test_x)[:, 1]\n# logistic regression model\n# Since the features are different from the xgboost, we created train_x2 and test_x2.\nmodel_lr = LogisticRegression(solver='lbfgs', max_iter=300)\nmodel_lr.fit(train_x2, train_y)\npred_lr = model_lr.predict_proba(test_x2)[:, 1]","989d1e25":"# Take a weighted average of the predicted values\npred = pred_xgb * 0.8 + pred_lr * 0.2\npred_label = np.where(pred > 0.5, 1, 0)","e9d874ec":"# submission\nsubmission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': pred_label})\nsubmission","015cd416":"# submission\nsubmission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': pred_label})\nsubmission.to_csv('submission_01.csv', index=False)\nsubmission","bc15d13c":"# Profiling\nBefore start programming, let's check the data you are going to deal with.  \nIn order to create a report, pandas_profiling is very helpful!","8bb4673e":"# Model tuning","03b27eac":"# Ensemble","6cdff62e":"# Introduction\n\nThis notebook is for beginners.  \nIf you are just getting started with kaggle, I think this notebook will be very useful\u263a\ufe0f","aae81ac3":"# Validation","a57e39d5":"# Create model","32181fb1":"# Read training data and test data\nFirst of all, read the input data.","6bbd6063":"# Create features for logistic regression","5a0a0309":"# Create feature"}}