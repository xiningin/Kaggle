{"cell_type":{"4e6fe793":"code","60ab790e":"code","cb395137":"code","e63403bf":"code","0cc606ab":"code","51adf60e":"code","e3a90bac":"code","22a14a42":"code","958e88a6":"code","41c9cfae":"code","468263f2":"code","52a699b2":"code","7b7f4472":"code","bf6ee55d":"code","8510f834":"code","db8c876a":"code","137dffea":"code","9bfdebbe":"code","7d2587f4":"code","e284ed70":"code","31960bc3":"code","06b07bee":"code","cf015e75":"code","f2d8af3d":"code","fa15ba61":"code","c15df44f":"code","b053fea1":"markdown","4e585a48":"markdown","38d2abcd":"markdown","dea59096":"markdown","687e0e5e":"markdown","ec4952d5":"markdown","cc22cc2b":"markdown","4b2de457":"markdown","1b5470fa":"markdown","ee0fbe9c":"markdown","d687fffa":"markdown","806da62f":"markdown","5a5a19b0":"markdown","ffdbd7b0":"markdown","d2dd8f42":"markdown","748e4595":"markdown","c0446839":"markdown","a6c2769c":"markdown","839bd088":"markdown","aee9036b":"markdown","f6c7bad0":"markdown","aab8adbe":"markdown","95cdcebe":"markdown","6c3775e9":"markdown","854924d5":"markdown","090bf30a":"markdown","d0709371":"markdown","4087160b":"markdown","7c3ce870":"markdown","8a1ed3c7":"markdown","c6599759":"markdown","448c29b3":"markdown","4c3ed119":"markdown","f25a9135":"markdown","a90f9d9a":"markdown","dad52d01":"markdown","59f829b2":"markdown","011ad7e0":"markdown","ff0b37ff":"markdown","ced8988f":"markdown","7cbf98e0":"markdown","40381bf2":"markdown","1fa3cd1d":"markdown"},"source":{"4e6fe793":"import math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# We can change this any time we want, to change the number of points we generate.\nN = 100\n\ndef generateSinPairs(N, valueRange):\n    X = np.zeros((N, 1))\n    Y = np.zeros((N, 1))\n    for i in range(N):\n#         Set X to a random value between -4pi and 4pi.\n        X[i][0] = (np.random.ranf() *(valueRange) - (0.5*valueRange)) * math.pi\n        Y[i][0] = math.sin(X[i][0])\n        pass\n    return (X,Y)\n\n(train_X, train_Y) = generateSinPairs(N, 2)\n\nplt.subplot(111)\nplt.scatter(train_X, train_Y, )\nplt.show()","60ab790e":"# Notice I've added all the complex stuff here,\n# but I'm going to set them to zero or one to cancel them out properly\n# I'm doing this so I can reuse my code later on.\n# Don't worry, I'll be generating the exact function you see above for now.\ndef generateRadialPairs(N, mu, lambda_val, w):\n    X = np.zeros((N, 1))\n    Y = np.zeros((N, 1))\n    max_val = 8\n    init_val = - max_val\n    step = 2 * max_val \/ N\n    for i in range(N):\n        X[i][0] = init_val\n        x = X[i][0] - mu\n        x_squared = x * x\n        lambda_x_squared = lambda_val * x_squared\n        Y[i][0] = w * math.exp(-lambda_x_squared)\n        init_val += step\n        pass\n    return (X,Y)\n\n(exp_X, exp_Y) = generateRadialPairs(100, 0, 1, 1)\n\n\nplt.subplot(111)\nplt.plot(exp_X, exp_Y)\nplt.show()","cb395137":"plt.subplot(111)\ncol = [0,0,0.5]\nfor i in range(5):\n    (mu_X, mu_Y) = generateRadialPairs(100, i*2 - 5, 1, 1)\n    plt.plot(mu_X, mu_Y, c=col.copy(), label=\"mu={}\".format(i*2-5))\n    col[0] += 0.1\n    col[2] -= 0.1\n    pass\nplt.legend()\nplt.show()\n    ","e63403bf":"plt.subplot(111)\ncol = [0,0,0.5]\nfor i in range(5):\n    (mu_X, mu_Y) = generateRadialPairs(100, 0, (i+1)*0.08, 1)\n    plt.plot(mu_X, mu_Y, c=col.copy(), label=\"mu={}, l={}\".format(0, (i+1)*.08))\n    col[0] += 0.1\n    col[2] -= 0.1\n    pass\nplt.legend(loc=\"best\")\nplt.show()","0cc606ab":"plt.subplot(111)\ncol = [0,0,0.5]\nfor i in range(5):\n    (mu_X, mu_Y) = generateRadialPairs(100, 0, 0.3, i+0.3)\n    plt.plot(mu_X, mu_Y, c=col.copy(), label=\"mu={}, l={}, w={}\".format(0, 0.3, i+0.3))\n    col[0] += 0.1\n    col[2] -= 0.1\n    pass\nplt.legend(loc=\"best\")\nplt.show()","51adf60e":"def generateRBFPairs(max_val, mu, lambda_val, w):\n    X = np.zeros((N, 1))\n    Y = np.zeros((N, 1))\n    max_val = 8\n    init_val = - max_val\n    step = 2 * max_val \/ N\n    for i in range(N):\n        sum = 0\n        for j in range(mu.shape[0]):\n            X[i][0] = init_val\n            x = X[i][0] - mu[j]\n            x_squared = x * x\n            lambda_x_squared = lambda_val[j] * x_squared\n            sum += w[j] * math.exp(-lambda_x_squared)\n            pass\n        Y[i][0] = float(sum)\n        init_val += step\n        pass\n    return (X,Y)\n\nmu = np.array([-6,-3,0,3,6])\nlambda_val = np.array([.1,2,.4,.03,.8])\nw = np.array([.3,-1,.5,.01,.7])\n\n(rbf_X, rbf_Y) = generateRBFPairs(100, mu, lambda_val, w)\n\nplt.subplot(111)\nplt.plot(rbf_X, rbf_Y)\nplt.show()\n","e3a90bac":"def _predict(x, mu, lambda_val, w):\n    x = float(x)\n    sum = 0\n    for j in range(mu.shape[0]):\n        dist = np.linalg.norm(x - mu[j])\n        x_squared = dist * dist\n        lambda_x_squared = lambda_val[j] * x_squared\n        sum += w[j] * math.exp(-lambda_x_squared)\n        pass\n    return sum","22a14a42":"def predict (x, mu, precision_rate, w):\n    A = np.zeros((1, mu.shape[0]))\n    for i in range(mu.shape[0]):\n        #   distance between point x and mu\n        dist = np.linalg.norm(x - mu[i])\n        #   square distance, multiply by lambda\n        exponent = (- precision_rate[i]) * dist * dist\n        A[0][i] = math.exp(exponent)\n        pass\n    return np.dot(A, w)","958e88a6":"def get_loss(X, Y, precision_rate, mu, w):\n    mse = 0\n    count = 0\n    for i in range(X.shape[0]):\n        predicted = predict(X[i], mu, precision_rate, w)\n        err = Y[i] - predicted\n        mse += err * err\n        count += 1\n        pass\n    res = mse \/ count\n    return res","41c9cfae":"#  Lloyd's algorithm: Implmentation.\n# Assume feature engineering has been done.\nclass KMeans:\n\n    def __init__ (self, k, X):\n        self.pointMatrix = X\n        self.k = int(k)\n        pass\n\n    def assign_means(self, num_trials, num_iterations, verbose= True):\n        if verbose:\n            print(\"evaluting means for k = {}...\".format(self.k))\n            plt.subplot(111)\n            pass\n        best_error = float(\"inf\") # Best error is positive infinity\n        best_mu = []\n        best_meanSet = []\n        pointMatrix = self.pointMatrix\n        k = self.k\n        col = [0,0,1]\n\n        for trial in range(num_trials):\n            self.mu = self.init_mu()\n            errors = []\n\n            for i in range(num_iterations):\n                #iteratively update the clusters and cluster means nunm_iterations times\n                self.meanSet = self.update_clusters()\n                self.mu = self.update_cluster_points()\n                errors.append(self.calculate_total_error())\n                pass\n\n            #calculate error of current mu model\n            model_error = self.calculate_total_error()\n            \n            if verbose:\n                print(\"MSE for trial {} : {}\".format(trial, model_error))\n    #             plot convergence over iterations\n                plt.plot(np.arange(0,num_iterations, 1), errors.copy(), c=col.copy())\n    #             reset errors, iterate color\n                col[1] += (1\/num_trials) * 0.9\n                col[2] -= (1\/num_trials) * 0.9\n                pass\n\n            #always take the best model w\/ minimum error\n            if(model_error < best_error):\n                best_error = model_error\n                best_mu = self.mu\n                best_meanSet = self.meanSet\n            pass\n        pass\n        if verbose:\n            plt.show()\n            pass\n        print(\"Saved best mu with mean squared error: {}\".format(best_error))\n        return best_mu\n\n\n    def init_mu(self):\n        k = self.k\n        X = self.pointMatrix\n        mu = np.zeros((k, X.shape[1]))\n#         print(\"generating mu of {} from x of {}\".format(mu.shape, X.shape))\n        for i in range(X.shape[1]):\n            for j in range(k):\n                index = int(np.random.ranf() * X.shape[0])\n                mu[j] = X[index]\n                pass\n            pass\n        return mu\n\n    def update_clusters(self):\n        pointMatrix = self.pointMatrix\n        mu = self.mu\n        k = self.k\n        meanSet = [[] for i in range(k)]\n        #   iterate over points\n        for i in range(pointMatrix.shape[0]):\n            minIndex = 0\n            minDistance = np.linalg.norm(pointMatrix[i] - mu[minIndex])\n        #     iterate over mu (mean points)\n            for j in range(k):\n                dist = np.linalg.norm(pointMatrix[i] - mu[j])\n                #       pick j with the minimum distance from i\n                if(dist < minDistance):\n                    minDistance = dist\n                    minIndex = j\n                    pass\n                pass\n            #   Add point i to mu[j]'s cluster'\n            meanSet[minIndex].append(pointMatrix[i])\n            pass\n        return meanSet\n\n    def update_cluster_points(self):\n        meanSet = self.meanSet\n        mu = self.mu\n        k = self.k\n        #   iterate over mu\n        for i in range(k):\n            set_sum = np.zeros(mu[i].shape)\n            #     iterate over mu[i]'s cluster'\n            for j in range(len(meanSet[i])):\n                # sum up all the positions of each point\n                set_sum += meanSet[i][j]\n                pass\n            # update mu to the average of each point in mu's cluster\n            if len(meanSet[i]) != 0:\n                mu[i] = set_sum \/ len(meanSet[i])\n                pass\n            pass\n        return mu\n\n    def calculate_total_error(self):\n        meanSet = self.meanSet\n        mu = self.mu\n        mserror = 0\n        N = 0\n        for i in range(self.k):\n            for j in range(len(meanSet[i])):\n                error = np.linalg.norm(meanSet[i][j] - mu[i])\n                mserror += error * error\n                N += 1\n                pass\n            pass\n        return mserror \/ N\n\n    pass\n\n","468263f2":"# Let's generate a large N\n\nN = 1000\n\n(train_X, train_Y) = generateSinPairs(N, 4)\n\nK = int(2 * math.log(N))\n","52a699b2":"Lloyd = KMeans(K, train_X)\n\nmu = Lloyd.assign_means(10,30)","7b7f4472":"plt.scatter(mu, [1 for i in range(mu.shape[0])])\nplt.show()","bf6ee55d":"def regress_w(X, Y, mu, lambda_val):\n    A = np.zeros((X.shape[0], mu.shape[0]))\n    #   iterate over X\n    for i in range(X.shape[0]):\n        #     iterate over mu\n        for j in range(mu.shape[0]):\n            #     create vectors for x and mu\n            _x = np.transpose([X[i]])\n            _mu = np.transpose([mu[j]])\n            #     Take the distance between point x and mu\n            dist = np.linalg.norm(_x - _mu)\n            #   square distance, multiply by lambda\n            exponent = (- lambda_val[j]) * dist * dist\n            A[i][j] = math.exp(exponent)\n            pass\n        pass\n\n    #   Get ATA\n    transpose = np.transpose(A)\n    ATA = np.dot(transpose, A)\n\n    #   Invert ATA\n    pseudoInv = np.linalg.inv(ATA)\n\n    #   Take(ATA)^-1 ATy\n    res = np.dot(transpose, Y)\n    res = np.dot(pseudoInv, res)\n    return res","8510f834":"# Train on 1000 values\ntrain_N = 1000\n# Error calculated from 100 values\ntest_N = 200\n# Set K proportional to log size of training set\nK = int(2 * math.log(train_N))\n\n# Generate relevant (x, y) pairs\n(train_X, train_Y) = generateSinPairs(train_N, 8)\n(test_X, test_Y) = generateSinPairs(test_N, 8)\n\n# Run K-means on training set.\nLloyd = KMeans(K, train_X)\nmu = Lloyd.assign_means(5,20)\n\n# Set lambda to 1 for all values\nlambda_val = np.ones(mu.shape)\n\nprint(\"Training for w...\")\n#   Train for given precision rate\nw = regress_w(train_X, train_Y, mu, lambda_val)\n#   Get the error within the training set\nE_in = get_loss(train_X, train_Y, lambda_val, mu, w)\n# Get the error outside the training set\nE_out = get_loss(test_X, test_Y, lambda_val, mu, w)\nprint('MSE for lambda = {} was {}, In-sample error was {}'.format(1, E_out, E_in))\n","db8c876a":"plot_X = np.arange(-4*math.pi, 4*math.pi, 0.1)\nplot_Y = np.zeros(plot_X.shape)\nplot_sin = np.zeros(plot_X.shape)\nplt.subplot(111)\nfor i in range(plot_X.shape[0]):\n    plot_Y[i] = predict(plot_X[i], mu, lambda_val, w)\n    plot_sin[i] = math.sin(plot_X[i])\n    pass\n\nplt.plot(plot_X, plot_Y, c=[0,0,1], label=\"f(x)\")\nplt.plot(plot_X, plot_sin, c=[1,0,0], label=\"sin(x)\")\nplt.legend(loc=\"best\")\nplt.show()\n","137dffea":"plt.subplot(111)\nx = np. arange(-3,7,0.1)\ny = lambda x : x ** 4 - 7 *x ** 3 - 9*x\ngrad = lambda x : 4 * x ** 3 - 21 *x ** 2 - 9\ntangent_line = lambda s : grad(s) * x + (y(s) - grad(s)*s)\nplt.plot(x, y(x), c=[1,0,0], label=\"f(x)\")\nplt.plot(x,tangent_line(3), c=[0,1,0], label=\"tangent at x=3\")\nplt.legend(loc='best')\nplt.show()","9bfdebbe":"def descend_lambda (x, y, mu, lambda_val, w, eta):\n#     Predict the value of f(x)\n#     'eta' is the learning rate, by the way\n    f = predict(x, mu, lambda_val, w)\n    coefficient =  - eta * (y - f)\n    learning_vector = np.ones(mu.shape)\n    for i in range(mu.shape[0]):\n        dist = (x - mu[i]) **2\n        exponent = -lambda_val[i] * dist\n        learning_vector[i] = dist * w[i] * math.exp(exponent)\n        pass\n    new_lambda = lambda_val.copy()\n    new_lambda += coefficient * learning_vector\n    return new_lambda        ","7d2587f4":"# This block initializes our dataset and uses K-means to find the vector mu.\n\n# Train on 1000 values\ntrain_N = 1000\n# Error calculated from 100 values\ntest_N = 200\n# Set K proportional to log size of training set\nK = int(2 * math.log(train_N))\n\n# Generate relevant (x, y) pairs\n(train_X, train_Y) = generateSinPairs(train_N, 8)\n(test_X, test_Y) = generateSinPairs(test_N, 8)\n\n# Run K-means on training set.\nprint(\"Running K-means...\")\nLloyd = KMeans(K, train_X)\nmu = Lloyd.assign_means(10,15, verbose=False)","e284ed70":"# Start off lambda as 1 for all values\nlambda_val = np.ones(mu.shape) * 1\n\n# Set the learning rate and number of epochs\neta = 0.005\nnum_epochs = 30\n\n# Set up\nE_in_vector = []\nE_out_vector = []\nepoch_X = np.arange(0,num_epochs,1)\n\nprint(\"Training model...\")\nfor epoch in range(num_epochs):\n#     Regress to find w\n    w = regress_w(train_X, train_Y, mu, lambda_val)\n#     use gradient dexcent through one epoch to find lambda\n    for i in range(train_X.shape[0]):\n        lambda_val = descend_lambda(train_X[i], train_Y[i], mu, lambda_val, w, eta)\n        pass\n    \n    #   Get the error within the training set\n    E_in = get_loss(train_X, train_Y, lambda_val, mu, w)\n    E_in_vector.append(float(E_in))\n    \n    # Get the error outside the training set\n    E_out = get_loss(test_X, test_Y, lambda_val, mu, w)\n    E_out_vector.append(float(E_out))\n    \n    print('MSE for epoch {} was {}, In-sample error was {}'.format(epoch, E_out, E_in))\n    pass\n\nplt.subplot(111)\nplt.plot(epoch_X, E_in_vector, c=[0,0,1], label =\"in-sample error\")\nplt.plot(epoch_X, E_out_vector, c=[0,1,0], label =\"out-of-sample error\")\nplt.legend(loc='best')\nplt.show()\n","31960bc3":"plot_X = np.arange(-4*math.pi, 4*math.pi, 0.1)\nplot_Y = np.zeros(plot_X.shape)\nplot_sin = np.zeros(plot_X.shape)\nplt.subplot(111)\nfor i in range(plot_X.shape[0]):\n    plot_Y[i] = predict(plot_X[i], mu, lambda_val, w)\n    plot_sin[i] = math.sin(plot_X[i])\n    pass\n\nplt.plot(plot_X, plot_Y, c=[0,0,1], label=\"f(x)\")\nplt.plot(plot_X, plot_sin, c=[1,0,0], label=\"sin(x)\")\nplt.legend(loc=\"best\")\nplt.show()","06b07bee":"plot_X = np.arange(-10*math.pi, 10*math.pi, 0.1)\nplot_Y = np.zeros(plot_X.shape)\nplot_sin = np.zeros(plot_X.shape)\nplt.subplot(111)\nfor i in range(plot_X.shape[0]):\n    plot_Y[i] = predict(plot_X[i], mu, lambda_val , w)\n    plot_sin[i] = math.sin(plot_X[i])\n    pass\n\nplt.plot(plot_X, plot_Y, c=[0,0,1], label=\"Predicted values\")\nplt.plot(plot_X, plot_sin, c=[1,0,0], label=\"Actual values\")\nplt.legend(loc=\"best\")\nplt.show()","cf015e75":"def generateNoisySinPairs(N, valueRange):\n    X = np.zeros((N, 1))\n    Y = np.zeros((N, 1))\n    for i in range(N):\n#         Set X to a random value between -4pi and 4pi.\n        X[i][0] = (np.random.ranf() *(valueRange) - (0.5*valueRange)) * math.pi\n        Y[i][0] = math.sin(X[i][0]) + np.random.ranf()*2 - 1\n        pass\n    return (X,Y)\n\n(train_X, train_Y) = generateNoisySinPairs(1000, 8)\n\nplt.subplot(111)\nplt.scatter(train_X, train_Y)\nplt.show()","f2d8af3d":"# Train on 10000 values\ntrain_N = 1000\n# Error calculated from 100 values\ntest_N = 200\n# Set K proportional to log size of training set\nK = int(2 * math.log(train_N))\n\n# Generate relevant (x, y) pairs\n(train_X, train_Y) = generateNoisySinPairs(train_N, 8)\n(test_X, test_Y) = generateNoisySinPairs(test_N, 8)\n\n# Run K-means on training set.\nprint(\"Running K-means for K={}...\".format(K))\nLloyd = KMeans(K, train_X)\nmu = Lloyd.assign_means(10,15, verbose=False)","fa15ba61":"# Start off lambda as 1 for all values\nlambda_val = np.ones(mu.shape) * 1\n\n# Set the learning rate and number of epochs\neta = 0.005\nnum_epochs = 60\n\n# Set up error vectors to plot data\nE_in_vector = []\nE_out_vector = []\nepoch_X = np.arange(0,num_epochs,1)\n\nprint(\"Training model...\")\nfor epoch in range(num_epochs):\n#     Regress to find w\n    w = regress_w(train_X, train_Y, mu, lambda_val)\n#     use gradient dexcent through one epoch to find lambda\n    for i in range(train_X.shape[0]):\n        lambda_val = descend_lambda(train_X[i], train_Y[i], mu, lambda_val, w, eta)\n        pass\n    \n    #   Get the error within the training set\n    E_in = get_loss(train_X, train_Y, lambda_val, mu, w)\n    E_in_vector.append(float(E_in))\n    \n    # Get the error outside the training set\n    E_out = get_loss(test_X, test_Y, lambda_val, mu, w)\n    E_out_vector.append(float(E_out))\n    \n    if epoch % 10 == 0:\n        print('MSE for epoch {} was {}, In-sample error was {}'.format(epoch, E_out, E_in))\n    pass\n\nplt.subplot(111)\nplt.plot(epoch_X, E_in_vector, c=[0,0,1], label =\"in-sample error\")\nplt.plot(epoch_X, E_out_vector, c=[0,1,0], label =\"out-of-sample error\")\nplt.legend(loc='best')\nplt.show()","c15df44f":"plot_X = np.arange(-7*math.pi, 7*math.pi, 0.1)\nplot_Y = np.zeros(plot_X.shape)\nplot_sin = np.zeros(plot_X.shape)\nplt.subplot(111)\nfor i in range(plot_X.shape[0]):\n    plot_Y[i] = predict(plot_X[i], mu, lambda_val, w)\n    plot_sin[i] = math.sin(plot_X[i])\n    pass\n\nplt.plot(plot_X, plot_Y, c=[0,0,1], label=\"f(x)\")\nplt.plot(plot_X, plot_sin, c=[1,0,0], label=\"sin(x)\")\nplt.legend(loc=\"best\")\nplt.show()","b053fea1":"We need to choose a number of representative points for the entire dataset. We need $K$ values that speak for the data, so we can talk about a meaningful number of $\\mu$s that can still give a good idea of what kinds of points around which we want to place our radial distributions. Can we pick random points in our dataset? Yes, but it's definitely not ideal. We need ","4e585a48":"### Gradients\n\nWhat is a gradient? In one dimension, it's just the slope of a curve. But even in one dimension you can consider it useful when it comes to finding the minimum value of a function","38d2abcd":"\nI'm going to try building a Radial Basis Function Model and train it to learn the value of $\\sin(x)$ between the ranges $ [-4\\pi, 4\\pi]$. There's a few reasons for my choice of model and challenge:\n - I'm a more math inclined person, and the idea of training simple programs to understand complex mathematical relations is ridiculously interesting to me.\n - I don't really remember all the formulas from Dr. Mostafa;s course too well, the RBF equation really stuck with me though, so my goal here is to derive every step by applying the general intuition I've been given by the course to a just the equation. I'm going to do a lot of the math on my own and this means the reader should really look out for mistakes I almost certainly not catch.\n - ...And I can't just use the internet to look up the equations for other models because I'm on a 10 hour flight from San Francisco to Frakfurt. I hope my battery makes it, because my plan right now is to try to finish and upload this as soon as I reach Frankfurt. \n - I _thought_ I had the MNISt dataset with me before I got on the plane, but apparently the file is no longer encoded in utf-8 and I don't have an editor or reader of any kind that understands the values. So I don't have access to any data I could use with the network. Luckily, Python knows the sin function. That's where I start.","dea59096":"Tell me that doesn't look crazy! The implications of this are enormous. Imagine if we could come up with a correct set of values for all these parameters. Doesn't the $sin$ function look trivial? I could probably write down a decent answer on the spot. But that's not what we're going to do. Even though the analysis of this one little function was cool, the interesting math lies entirely in a function that trains itself to find the right datapoints.\n\nNotice that before we introduced that magical $\\sum$ to our function, there were 3 parameters to play around with. In my code above, I used a sum up til $K=5$ and chose them specifically to come up with something interesting, so that no one value just dominated the graph and you could see the complexity. But that's 15 parameters. 15 numbers to guess the best way to apply this function to a given problem is not trivial, so I'm going to go over the process I can use to figure it out.","687e0e5e":"I'm writing thie notebook to give myself (and others, if they are interested) a record of some of the experiments with Machine Learning I'm going to be conducting to improve my understanding of the field. I'm at a very low level when it comes to experience in statistics, data science, machine learning and even programming in general.\n\nHowever, after watching Dr. Yasser Abu-Mostafa's brilliant course on Machine Learniing on YouTube, I feel like I should, at this point, have at least enough experience to go ahead and test out different implementations of machine learning models.","ec4952d5":"Now, we begin the _real_ work.","cc22cc2b":"Well, what does this really mean? Well, let's pick the number of points we have and take a look at what choosing $K = N$ does to our model.\n\nMost machine learning problems involve tons of data. By tons, a standard sample starts at $10^6$, and since most machine learning problems involve people (Advertising, financial transactions, stock markets etc.), we could probably put a safe-ish bound near the order of the total human population - 10 billion or $10^{10}$. That means at $K = 10^10$, every time our final model is given a value to predict, it's going to find the distance between two points, get its exponential, multiply it by a scalar value, and then add it to a sum, 10 billion times. That's just a litlle bit unreasonable, even for a company the size of Google.","4b2de457":"We're going to ue a very simple definition of our pronblem. WE are currently predicting $y = sin{x}$, and then trying to find the right model to get that to happen. What if instead, we framed our problem in a different way.\nInstead of fitting $f(x)$ to be a certain value at different times, let's take a function whose value we value to shift towards only one thing. For example, let's look at the error between the predicted value of a input within our sample, and its actual value.\n\n\\begin{equation}\nE_{in} = y - f(\\mathbf{x})\n\\end{equation}\n\nEven better, let's take the squared distance.\n\n\\begin{equation}\nE_{in} = (y - f(\\mathbf{x}))^2\n\\end{equation}\n\nNow, our goal isn't to make $f(x)$ equal to 1 at $\\frac{\\pi}{2}$, or $0$ at multiples of $\\pi$. When $f(x)$ deviates _in either direction_ of the correct value, higher or lower, $E_{in}$ goes up. We have a much sinpler goal all of sudden - minimize $E_{in}$. We're going to have to look at a concept straight out of multivariable calculus to solve this: gradients.","1b5470fa":"### Selecting K\n\nThe information we actually have at our disposal, is really two vectors. They probably look like this:\n\n\\begin{equation}\nX = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix}, \nY = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix} = \n\\begin{bmatrix} \\sin{x_1} \\\\ \\sin{x_2} \\\\ \\vdots \\\\ \\sin{x_N} \\end{bmatrix}\n\\end{equation}\n\nHere, $N$ is the number of data points we have.\n\nWhat our goal is, however, to gleam some deeper trend from these vectors and write a function that'll give a close approximation to $f(x) \\approx \\sin{x}$","ee0fbe9c":"So, for any given $|lambda$, we can now tell you what direction would most likely increase the value $E_{in}$. But, wait a minute. We don't actually want that. Our goal is to **decrease** $E_{in}$. So what would happen if we just went in the _negative_ of the gradient we got? It turns out, that would work perfectly.\n\nSo, the following algorithm for updating $\\lambda_k$ would genuinely decrease the value of $E_{in}$ for a single value of x:\n\n\\begin{equation}\n\\lambda_k = \\lambda_k + 2 \\dot (y - f(\\mathbf{x})) \\mathbf{\\| x - \\mu_k\\|^2} w_k e^{-\\lambda_k \\mathbf{\\| x - \\mu_k \\|^2}}\n\\end{equation}\n\nHere's the problem with that: It might not actually decrease in-sample error overall- the reason for this is moving each $\\lambda$ in one direction for a single x might move it in the wrong direction for a different x. So how do we make sure $|lambda_k$ is always moving is the right direction? Well, suppose we moved it a _tiny_ bit in the right direction for every value of x, so that overall the error will slowly be pushed down for the entire set? Thisi s where we'll introduce a _learning rate_, or $\\eta$, that need to be _very_ small, like between $0$ and $.001$, that we multiply by the value for $\\frac{\\partial E}{\\partial \\lambda_k}$ by \\eta before adding it to $\\lambda_k$\n\n\\begin{equation}\n\\lambda_k = \\lambda_k + \\eta (y - f(\\mathbf{x})) \\mathbf{\\| x - \\mu_k\\|^2} w_k e^{-\\lambda_k \\mathbf{\\| x - \\mu_k \\|^2}}\n\\end{equation}\n\nNow, it's true that this already looks quite like a monster, but we need to add one more arbitration onto it before we're ready to see if it works, and that's making the whole thing apply to a vector. We shouldn't have to calculate anything more times than is necessary, so the only term on the right side that expands is the one concerning $\\mu$, $\\lambda$ and $w@. Out full equation for stochastic gradient descent looks like this:\n\n\\begin{equation}\n\\Large\n\\vec{\\lambda} = \\vec{\\lambda}\n+\n\\eta\n \\dot (y - f(\\mathbf{x}))\n\\cdot\n\\begin{bmatrix}\n\\mathbf{\\| x - \\mu_1\\|^2} w_1 e^{-\\lambda_1 \\mathbf{\\| x - \\mu_1 \\|^2}} \\\\\n\\mathbf{\\| x - \\mu_2\\|^2} w_2 e^{-\\lambda_2 \\mathbf{\\| x - \\mu_2 \\|^2}} \\\\\n\\vdots \\\\\n\\mathbf{\\| x - \\mu_K\\|^2} w_K e^{-\\lambda_K \\mathbf{\\| x - \\mu_K \\|^2}}\n\\end{bmatrix}\n\\end{equation}\n\nNow, all that's left to do is try and code this into a new function that pushes $\\lambda$ in the right direction for each x value.","d687fffa":"### Testing\n\nIt's now time to test everything we've done so far. I'm going to set all the $\\lambda$s to 1 and see what we get.","806da62f":"We will also want a function that can calculate how well our model is doing. Given a set of $X$s and $Y$s, we should define a functio nto calculate the mean squared error between any given predicted value and the actual value. We'll call this the **loss** of our model.","5a5a19b0":"### Matrix operations\n\nLet's assume that by some magic, we have found ourswelf a **very** promising set of values for the parameter $\\vec{\\lambda}$. What if we built two vectors of values where one is built using our \"known\" $\\mu$s and $\\lambda$, and the other with the value we don't, $\\mathbf{w}$. It would look like the following:\n\n\\begin{equation}\n\\large\n\\mathbf{\\vec{v}} =\n\\begin{bmatrix}\ne^{-\\lambda_1\\mathbf{\\| x - \\mu_1\\|^2}} \\\\\ne^{-\\lambda_2\\mathbf{\\| x - \\mu_2\\|^2}} \\\\\n\\vdots \\\\\ne^{-\\lambda_K\\mathbf{\\| x - \\mu_k\\|^2}}\n\\end{bmatrix},\n\\mathbf{w} = \\begin{bmatrix} w_1 \\\\w_1\\\\\\vdots\\\\w_K\\end{bmatrix}\n\\end{equation}\n\nUsing these vectors, we can use the Linear Algebra concept of the dot product to derive a new eqation for $f(\\mathbf{x})$.\n\\begin{equation}\n\\large\n\\mathbf{\\vec{v}} \\cdot \\mathbf{w} = \nw_1e^{-\\lambda_1\\mathbf{\\| x - \\mu_1\\|^2}} +\nw_2e^{-\\lambda_2\\mathbf{\\| x - \\mu_2\\|^2}} +\n\\cdots +\nw_Ke^{-\\lambda_K\\mathbf{\\| x - \\mu_K\\|^2}}  \\\\ =\n\\large\n\\sum_{n = 1}^N w_ne^{-\\lambda_n\\mathbf{\\| x - x_n\\|^2}} \\\\ =\n\\large\nf(\\mathbf{x})\n\\end{equation}\n\nIn fact, we can actually take the transpose of $\\mathbf{\\vec{v}}$, $\\mathbf{\\vec{v}}^T$, to get:\n\n\n\\begin{equation}\nf(\\mathbf{x}) =\n\\begin{bmatrix}\ne^{-\\lambda_1\\mathbf{\\| x - \\mu_1\\|^2}} &\ne^{-\\lambda_2\\mathbf{\\| x - \\mu_2\\|^2}} &\n\\cdots &\ne^{-\\lambda_K\\mathbf{\\| x - \\mu_k\\|^2}}\n\\end{bmatrix} \\cdot\n\\begin{bmatrix} w_1 \\\\w_2\\\\\\vdots\\\\w_K\\end{bmatrix}\n\\end{equation}\n\nNow, $f(\\mathbf{x})$ is just a matrix operation on $\\mathbf{w}$!\n\n\n\\begin{equation}\n\\mathbf{A} = \\begin{bmatrix}\ne^{-\\lambda_1\\mathbf{\\| x - \\mu_1\\|^2}} &\ne^{-\\lambda_2\\mathbf{\\| x - \\mu_2\\|^2}} &\n\\cdots &\ne^{-\\lambda_K\\mathbf{\\| x - \\mu_k\\|^2}}\n\\end{bmatrix}, \\\\\nf(\\mathbf{x}) =\n\\mathbf{Aw}\n\\end{equation}\n","ffdbd7b0":"Anyeways, I'm going to stop elaborating and start writing the code. The following block is going to import all the Python libraries I'll need to pull this off, and generate $N$ of x, y pairs between my given range. $N$ will be arbitrary, I'll change that as we move forward.","d2dd8f42":"### Descent\n\nIf you looked at and played around with gradients for a while, not just in one dimension but in multiple, you'd come across the true definition of gradient: the direction in which $f(\\mathbf{x})$ increases the most rapidly. You might notice that, in our previous diagram, the gradient at $x=3$ was negative, and if you pushed $x$ in the negative direction, $f(x)$ would got up. The same would concept would apply in areas where the gradient is positive. So, if we have a defined funciton, and we want to know what direction we need to move our parameters to increase a fcuntion's value, why not take a look at our error equation again?\n\n\\begin{equation}\n\\large\nE_{in} = (y - f(\\mathbf{x}))^2\n\\end{equation}\n\nAnd what is the significance, if we take the partial differential of this equation with respect to $\\lambda$?","748e4595":"Now that we have a reasonable intuition for the power that this function wields, I'm going to take myself (and you) through building a process that will, theoretically, will tell _us_ how to approximate $sin{x}$ using this function.\n\nI'm going to begin by writing a function to actually predict the value, once we've given given the appropriate value from $\\mu$, $\\lambda$ and $w$ before I start.","c0446839":"## Step 1: Finding $\\mu$ and $K$\n\nThis may sound weird, but in order to find the optimal values for $\\mu$, we actually have to decide on $K$ first. Even though $\\mu$ was the first thing we dealt with in defining the Radial Basis Function and $K$ was the last, the two are intimately related. Let's understand why by looking at what information we have, and how to make the first step towards building our model.","a6c2769c":"Here's the radial basis function mathematically. It's going to look really complex at first, but it's really not. hopefully, with the help of all this text detracting the code, by the end of this experiment the reader will find every bit of this code completely intuitive, with every statement an obvious consequence of the statement preciding it.\n\n\\begin{equation}\n\\large\nf(\\mathbf{x}) = \\sum_{k = 1}^K w_ke^{-\\lambda_k\\mathbf{\\| x - \\mu_k\\|^2}}\n\\end{equation}","839bd088":"The third parameter we have here is $w$, the weight of the distribution. This is probably the most obvious piece of the puzzle, since it just modulates your value from $[0,1]$ to the value you're actually going for for a given $x$\n\n\\begin{equation}\nf(x) = we^{-\\lambda\\|x - \\mu\\|^2}\n\\end{equation}\n\nI shouldn't have to put this up, but it's really just copying code at this point and it's nice to have it for completeness' sake","aee9036b":"I wonder, though, how well the model does when Iadd noise to the model. Heres some code that adds some random error for every datapoint.","f6c7bad0":"I really enjoyed the math here, but the main reason for this is that when these things were first explained to me I had JUST learned about the concepts that this model, and most machine learning really, builds heavily on. To readers that are not me, I doubt any of this math will make sense unless you have some basic familiarity with:\n- Linear Algebra\n- Multivariable Calculus\n\nEven in general, I think all the math in those two areas can be considered some of the most exciting stuff in Math, especially right now. The applications and value of these are so clear and visible now, and it kind of feels like all of my middle and high school math has been building itself towards a full grasp of this beautiful math.","aab8adbe":"That's a good looking convergence! Let's plot the means themselves.","95cdcebe":"Now it's time to test it out using the full code.\n\nWe're going to add yet another parameter to the mix; this one will be the number of 'epochs'. This denotes the number of times we're going to run the combination of linear regression on $w$ and stochastic gradient descent on $\\lambda$ to find the right pair of vectors.\n\nThis time we're also going to plot the Mean squared error as the number of epochs goes on.","6c3775e9":"## Stochastic Gradient Descent\n\nSince $\\lambda$ does not affect $f(x)$ linearly, there's no one \"best answer\" that we can immediately solve for. Linear Algebra will do nothing for us here. We're going to have to switch to calculus to solve this problem.","854924d5":"This function is going to be iterated over throughout this notebook. Notice that its $y$ value is 1 when $x$ is zero, and it tapers off as you move further away from it. Why is this useful? Well, it's useful simply to specify to a model that y should be 1 when x is close to 0 and taper off when it gets far away. It's not useful. Now we will introduce mu and make it useful.\n\n\\begin{equation}\nf(x) = e^{-\\|x - \\mu\\|^2}\n\\end{equation}\n\nI'm not only to going to plot this once, I'll be plotting it for more than one mu, so you can see what happens in different situations.","090bf30a":"# Radial Basis Functions: Learning the value of sin (x).","d0709371":"Well, what if our $\\mu$s are just all our datapoints? Let's just define our function for every value of x we have available, and go from there?\n\n\\begin{equation}\n\\large\nf(\\mathbf{x}) = \\sum_{n = 1}^N w_ne^{-\\lambda_n\\mathbf{\\| x - x_n\\|^2}}\n\\end{equation}","4087160b":"So, we 've established that we want $K$ to be small ,but still represent our dataset. I don't know of any established heuristics when it comes to deciding this value, but my gut tells me that since it can't grow to the order of $N$, we should do the next best thing, and set $K \\propto \\log{N}$. We have to use some heuristic to find this number, because it wouldn't be the same for different problems. Our problem here isn't a practical one (we know how to calculate $sin{x}$ pretty well already) so we have some amoynt of leeway, but for most cases it's a bit harder to find the right trade-off between accuracy and speed.","7c3ce870":"# Radial Basis Model: Implementation","8a1ed3c7":"The model seems to be doing well! the function hugs the sin curve within its range.","c6599759":"I'm going to iteratively build this model until the reader can theoretically understand every part of it, and what it means. Let's start with the simplest possible form.\n\\begin{equation}\nf(x) = e^{-x^2}\n\\end{equation}\nI'm going to write a short script to show you what this graph actually looks like.","448c29b3":"Very interesting. The means seem to have converged to an almost regular distribution across our input space. I have a feeling this is going to be a **very** successful experiment.","4c3ed119":"This is looking really impressive, but it could be much better. It's now time to find the best $\\lambda$, and finish the basic structure of our model.","f25a9135":"This looks really good! You wouldn't think a function with some additions and exponentials would be able to plot the $sin$ function so well, but it seems to be pretty close - and we haven't even corrrected for $lambda$ yet. Of course, you could say that I happen to be cheating a little bit. For example, here's the same code, plotting over a larger range.","a90f9d9a":"### Selecting $\\mu$: K-means clustering and Lloyd's algorithm\n\nLuckily, there are well-established algorithms within computer science that do this. Lloyd's algorithm works as follows:\n - Step 1: Pick a set of $\\mu$ with RANDOM values in the space of your dataset.\n     - A good way to to do this is to pick out K randomly selected _actual_ points from your dataset.\n     - Another way, assuming you know the distribution, the mean and the standard deviation of the set, simulate that distribution and pick out a random member of it. For example, if your data is roughly normally distributed, and you pick out values of the form $\\mathtt{mean}_{dataset} + \\alpha \\cdot\\mathtt{(standardDeviation)}$, where $\\alpha \\in [-3, 3]$, you know that about 97% of your datapoints are covered by the range of these values.\n - Step 2: For each $\\mu$ pick its cluster such that for each cluster of a given $\\mu$ contains all the points closer to that $\\mu$ than any other $\\mu$.\n - Step 3: Update each $\\mu$ to be the mean value of each datapoint in its cluster. You can actually just plug in this formula:\n \\begin{equation}\n \\mu_{new} = \\frac{1}{\\|S_{\\mu}\\|} \\sum_{\\mathbf{x} \\in S_{\\mu}}(\\mathbf{x})\n \\end{equation}\n where $S_{\\mu}$ is the cluster of $\\mu$.\n - Step 4: Repeat steps 2 and 3 iteratively, assuming the value converges eventually. I'm going to be plotting the mean squared error of this process (square distance of eadch cluster point to its mean) and see what number of iterations is required to make it coverge reasonably.\n - Step 5: Repeat steps 1 through 4 qa couple times, and pick out the converged $\\mu$ with the lowest final error. The reason for this is that initial positions affect the quiality of the means by a lot and we don't want to subject our model to the luck of random chance.\n \n","dad52d01":"### Lease Squares Regression\n\nWhat is the value of defining $f(\\mathbf{x})$ in terms of matrix multiplication? Well, this is function that builds the matrix $\\mathbf{A}$ out of a given unkown $x$, and gives a single value $y$ after $\\mathbf{A}$ is applied to $\\mathbf{w}$. Well, let's assume, just for a moment, that the multiplication $\\mathbf{Aw}$ gave you the best _possible_ approximaton for $y$ given what you had to work with. That would mean that, for every $n$, the following is true:\n\n\\begin{equation}\n\\begin{bmatrix}\ne^{-\\lambda_1\\mathbf{\\| x_n - \\mu_1\\|^2}} &\ne^{-\\lambda_2\\mathbf{\\| x_n - \\mu_2\\|^2}} &\n\\cdots &\ne^{-\\lambda_K\\mathbf{\\| x_n - \\mu_k\\|^2}}\n\\end{bmatrix} \\cdot \\begin{bmatrix} w_1 \\\\w_1\\\\\\vdots\\\\w_K\\end{bmatrix} \\approx y_n\n\\end{equation}\n\nBut, since we have $N$ $n$s, couldn't we just extend our single row matrix, and output a column of $y_n$s?\n\n\\begin{equation}\n\\large\n\\begin{bmatrix}\ne^{-\\lambda_1\\mathbf{\\| x_1 - \\mu_1\\|^2}} &\ne^{-\\lambda_2\\mathbf{\\| x_1 - \\mu_2\\|^2}} &\n\\cdots &\ne^{-\\lambda_K\\mathbf{\\| x_1 - \\mu_k\\|^2}} \\\\\ne^{-\\lambda_1\\mathbf{\\| x_2 - \\mu_1\\|^2}} &\ne^{-\\lambda_2\\mathbf{\\| x_2 - \\mu_2\\|^2}} &\n\\cdots &\ne^{-\\lambda_K\\mathbf{\\| x_2 - \\mu_k\\|^2}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ne^{-\\lambda_1\\mathbf{\\| x_N - \\mu_1\\|^2}} &\ne^{-\\lambda_2\\mathbf{\\| x_N - \\mu_2\\|^2}} &\n\\cdots &\ne^{-\\lambda_K\\mathbf{\\| x_N - \\mu_k\\|^2}}\n\\end{bmatrix} \\cdot\n\\begin{bmatrix} w_1 \\\\w_2\\\\\\vdots\\\\w_K\\end{bmatrix} \\approx\n\\begin{bmatrix} y_1 \\\\y_2\\\\\\vdots\\\\y_K\\end{bmatrix}\n\\end{equation}\n\nSince we've already decided that this new matrix (we're going to call this one $\\mathbf{A}$ instead now) is filled with constant values, given we know $\\mu$s and assuming we know $\\lambda$s, we can use the concept of least squares regression to give the best possible approximation for the entire vector $\\mathbf{w}$\n\n\\begin{equation}\n\\mathbf{Aw = y} \\\\\n\\mathbf{w = (A^TA)^{-1}A^Ty}\n\\end{equation}\n\nThe matrix $\\mathbf{(A^TA)^{-1}A^T}$ is called the **pseudo-inverse** of $\\mathbf{A}$. This one definitely looks like magic, I know, but trust me, it works 100% of the time.\n\nNow, we're just going to translate all this to code.","59f829b2":"That looks _very_ messay. I wonder how well we'd do if we tried to train a Radial Basis Model on these points.","011ad7e0":"### Derivative of $E_{in}$\n\nAll of the following almost certainly wouldn't make sense to anyone who has no familiarity with multivariable calculus.\n\n\n\\begin{equation}\n\\large\nf(\\mathbf{x}) = \\sum_{k=1}^K w_k e^{-\\lambda_k \\mathbf{\\| x - \\mu_k \\|^2}}\n\\end{equation}\n\n\\begin{equation}\n\\large\n\\frac{\\partial E_{in}}{\\partial \\lambda_k} = \\frac{\\partial E_{in}}{\\partial F} \\cdot \\frac{\\partial F}{\\partial \\lambda_k}\n\\end{equation}\nSince\n\n\\begin{equation}\n\\frac{\\partial E_{in}}{\\partial F} = 2\\cdot (y - f(\\mathbf{x}))\n\\end{equation}\n\n\\begin{equation}\n\\frac{\\partial F}{\\partial \\lambda_k} = - \\mathbf{\\| x - \\mu_k\\|^2} w_k e^{-\\lambda_k \\mathbf{\\| x - \\mu_k \\|^2}}\n\\end{equation}\n\nWe get the following:\n\\begin{equation}\n\\Large\n\\frac{\\partial E_{in}}{\\partial \\lambda_k} = - 2 \\dot (y - f(\\mathbf{x})) \\mathbf{\\| x - \\mu_k\\|^2} w_k e^{-\\lambda_k \\mathbf{\\| x - \\mu_k \\|^2}}\n\\end{equation}\n","ff0b37ff":"Now, I haven't gotten into any real math yet, now have done anything interesting in terms of programming or machine learning, AND as of right now all these equations haven't seemed interesting at ALL yet. I sympathize, but now we're going to bring in the big guns, and introduce the reason that this equation is about to become ridiculously powerful. I now re-introduce to you the full radial basic function, and the functions I can build by sending in different parameters are ridiculously powerful.\n\n\\begin{equation}\n\\large\nf(\\mathbf{x}) = \\sum_{k = 1}^K w_ke^{-\\lambda_k\\mathbf{\\| x - \\mu_k\\|^2}}\n\\end{equation}","ced8988f":"## Step 2: Linear Regression\n\nThis is the first interesting piece of math we're going to see. It involves a specific reframing of our equation. The current model looks like the following:\n\n\\begin{equation}\n\\large\nf(\\mathbf{x}) = \\sum_{k = 1}^K w_ke^{-\\lambda_k\\mathbf{\\| x - \\mu_k\\|^2}}\n\\end{equation}\n\nThe summation by itself looks pretty interesting, but the general idea of summations in and of themselves can be extremely nasty, We're going to use a much prettier notation to write the exact same functon.","7cbf98e0":"Brilliant! Let's now actually plot the function for a bunch of values.","40381bf2":"So now, we know that having a specific value for $\\mu$ tells us where the function reaches its peak of 1. But that's not all we can do. Suppose we want some points to be close to 1 even when we're a bit further from mu, suppose we want a function that tapers off quickly in some places and slowly in others? Now we add $\\lambda$ to our equation:\n\n\\begin{equation}\nf(x) = e^{-\\lambda\\|x - \\mu\\|^2}\n\\end{equation}\nWhat would this look like? Let's find out.","1fa3cd1d":"# The Radial Basis Function: Introduction"}}