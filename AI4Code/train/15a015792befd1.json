{"cell_type":{"c39ec30b":"code","83fd9290":"code","c047b620":"code","355453b8":"code","b3977f04":"code","04e525ad":"code","bf82db69":"code","e8f63c20":"code","251eb7e4":"code","5115ba06":"code","9141ed29":"code","4f2b90e9":"code","3cf0198d":"code","f95283b1":"code","65db0bc2":"code","4033b662":"code","49d735e4":"code","6b30b0b7":"code","3378cd51":"code","b28a3cff":"code","0cd6a934":"code","96c59100":"markdown","ae646f2c":"markdown","6b20430a":"markdown","a2f97446":"markdown","4fdf5b4b":"markdown","555a9518":"markdown","726b95bf":"markdown","7358f1c1":"markdown","38fbdaa7":"markdown"},"source":{"c39ec30b":"# 1. importing libraries\nimport re\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom PIL import Image\nfrom nltk.corpus import stopwords\nfrom nltk import download","83fd9290":"# reading our data as text file\nwith open(\"..\/input\/whatsapp-chat\/whatsapp.txt\", 'r', encoding = 'UTF-8') as text:\n    text_str = text.read()","c047b620":"regex = r'\"(.*:\\d\\d) - (.*): (.*)\"'\nmatches = re.findall(regex, text_str)","355453b8":"df = []\nfor i in range(len(matches)):\n    d = {\n        'Time' : matches[i][0],   # this columns about writing times\n        'Name' : matches[i][1],   # this columns about names\n        'Content' : matches[i][2] # this columns about contents\n    }\n    df.append(d)\n\ndf = pd.DataFrame(df)","b3977f04":"# first touch to data\ndf.head()","04e525ad":"df.info()","bf82db69":"df.shape","e8f63c20":"df['Time'] = pd.to_datetime(df['Time'])\ndf['Year'] = df['Time'].apply(lambda x : x.year)\ndf['Month'] = df['Time'].apply(lambda x : x.month)\ndf['Day_Period'] = pd.cut(df['Time'].apply(lambda x : x.hour), 3 , labels=['Night','Noon','Evening'])","251eb7e4":"df['Name'] = df['Name'].apply(lambda x : max(x.split(), key = len))","5115ba06":"df['Content'] = df['Content'].str.strip('\\\\n') # cleaning extra chars\ndf['Total_Words'] = df['Content'].apply(lambda x : len(x.split()))\ndf['Content_Long'] = df['Content'].apply(lambda x : len(x))\n","9141ed29":"df.head()","4f2b90e9":"# most talkers\ndf.groupby('Name')['Time'].count() ","3cf0198d":"# talkers basis on day periods\ndf.groupby(['Name', 'Day_Period'])['Time'].count() ","f95283b1":"# users who sent media\ndf[df['Content'].str.contains('Medya')].groupby('Name')['Time'].count() ","65db0bc2":"# longest sentences\nindex_df=df['Content_Long'].sort_values(ascending=False)\ndf[df.index.isin(index_df)][['Name', 'Content', 'Content_Long']]\\\n    .sort_values(by='Content_Long', ascending=False)  ","4033b662":"# word count per message\ndf.groupby('Name')['Total_Words'].sum() \/ df.groupby('Name')['Total_Words'].count() ","49d735e4":"# user's messages per year\nmessages = df.groupby(['Name', 'Year'])['Time'].count().reset_index() \nmessages.head()","6b30b0b7":"# messages per year\nsns.lineplot(x='Year', y='Time', hue='Name', data=messages)\nplt.show();","3378cd51":"# preprocessing phase\ntext = df['Content'].apply(lambda x: x.strip().lower()) # lowering the chars\ntext = text.str.replace(\"[^\\w\\s]\", \"\") # dropping the punctuation marks\ntext.str.replace(\"\\d\", \"\")  # dropping the numbers\n\ndownload('stopwords') # downloading stopwords\nsw = stopwords.words('english') \ntext = text.apply(lambda i : ' '.join(i for i in i.split() if i not in sw)) # dropping the stopwords\n\nlink_index = text[text.apply(lambda x : x.startswith('http'))].index.to_list() # selecting the links\nspace_index = text[text==''].index.to_list() # selecting the empty messages\nind=space_index + link_index\ntext = text[~text.index.isin(ind)] # dropping the unnecessary","b28a3cff":"# counting the most used words\nsorted_text =pd.Series(' '.join(text).split()).value_counts() \nsorted_text = sorted_text[sorted_text>=10] # 10 is the word frequency limit","0cd6a934":"mask=np.array(Image.open('..\/input\/ovalll\/phploeBuh.png'))\ntext=' '.join(i for i in sorted_text.index)\nwc = WordCloud(max_font_size=50, mask=mask, contour_width=60, contour_color='white', background_color='gray')\nwc.generate(text)\nplt.figure(figsize=[30,30])\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.show()","96c59100":"After preprocessing, we can create our word cloud!! ","ae646f2c":"Besides these analyses, we can make the word cloud from the most used words. After the preprocessing phase, we can use the \u2018word cloud\u2019 library for this issue. We will get exactly like this schema:","6b20430a":"Realize that all variables is object. Next step is changing the time object to datetime and parsing as a \u2018Year\u2019, \u2018Month\u2019 and \u2018Dy_Period\u2019.","a2f97446":"Hi, everyone. Today we will extract our data from Whatsapp and make a \u2018word cloud\u2019 from the most used words. Exporting the chat option is only available on Whatsapp App. Whatsapp Web is only a replica of your Whatsapp Account. So, you have access to Whatsapp from your device and hit somebody or a group account. Only we have to do:\n1. Tap three-dot at the top of the right corner\n2. Then hit \u2018More\u2019\n3. And \u2018Export chat\u2019\n\nNow we have a text file. The file content involves -timestamp, -person -message content and the other mass staff. We will use regex library to parse this content. Let's impor tour necessary libraries.","4fdf5b4b":"Now turn to focus on messages. Let\u2019 count the letters and words!","555a9518":"Now it's time to regulate the \u2018Name\u2019 variable. We chose the longest name. So denotation is dropped","726b95bf":"We can reach everything from this structure. For example number of messages per person; per day of periods, longest and shortest messages; average word and letter count; message traffic on a time basis etc. Let's glance our data!","7358f1c1":"Soon we have a DataFrame object. We can do whatever we want as as feature engineering.","38fbdaa7":"Note that this regex pattern only captures the texts, not the media. Captured only times, names, and messages. The next step is creating the Pandas DataFrame for detailed research."}}