{"cell_type":{"b1ce6402":"code","06f35b52":"code","7a8ff5a9":"code","5c6a7149":"code","a3203211":"code","58aa0732":"code","6557fdaa":"code","eeb149cd":"code","bd0225f5":"code","c3522823":"code","66ce7f38":"code","d83f4d08":"code","855188a5":"code","9001f700":"code","93a97929":"code","8fdd1e35":"code","be140496":"code","83906052":"code","a4397513":"code","a2156251":"code","ef250f0c":"code","41507e8d":"code","f9c1ad99":"code","27339313":"code","efd24f98":"code","f241784e":"code","0a4fdd70":"code","07626f9b":"code","8356a409":"code","c5e0a7c6":"code","9214caa0":"code","f9a09cfe":"code","8adc8fa4":"code","5d88867c":"code","d3d70e9a":"code","6b387076":"code","33a2e8c7":"code","5279c8ad":"code","9a703efc":"code","63fcd7a2":"code","dd73d8ad":"code","61ccb74f":"code","0a6b4b7c":"code","66f3eba5":"code","666bd72b":"code","502dd4cc":"code","9841eef6":"code","e9e84f36":"markdown","5bd4a608":"markdown","afd75681":"markdown","dcfacabf":"markdown","9c3ff8ca":"markdown","3c3edf35":"markdown","17dfbb11":"markdown","51c3ca77":"markdown","b9ee465a":"markdown","348a1683":"markdown","4dc27455":"markdown","3d25464a":"markdown","7004db3f":"markdown","4d0bbfbd":"markdown","266d56d0":"markdown","bc74848e":"markdown","56b4142c":"markdown","638208bb":"markdown","1a825afd":"markdown","bcfaa2f5":"markdown","f6940ac5":"markdown","fefe2f62":"markdown","afce3f2d":"markdown","6ff2be90":"markdown","cd21c58c":"markdown","94972920":"markdown","c63c43e6":"markdown","5ccc73a7":"markdown","f7f3333b":"markdown","324f18b2":"markdown","97d4485d":"markdown","f68f7ab9":"markdown","c672e8fd":"markdown","08a4ff8c":"markdown","7741ccb9":"markdown","19393cef":"markdown","719bcbdb":"markdown","2c99b5e5":"markdown","e77a288d":"markdown","dae66ef3":"markdown","04c7f433":"markdown","ad4a49e3":"markdown","244ddbda":"markdown","3af3ea41":"markdown","8f682430":"markdown","4a084c2c":"markdown","bd809850":"markdown","f1f6b45c":"markdown","a00a37e1":"markdown","5f2d640a":"markdown","c6a19d60":"markdown","2731304b":"markdown","0dac17b8":"markdown","83289e1d":"markdown","f8612d5d":"markdown","93850ff1":"markdown","836ab5ad":"markdown","b67dce30":"markdown","c330cbb6":"markdown","4b645ddf":"markdown","c2b2938c":"markdown","c56b23b9":"markdown","82b34375":"markdown","3fc00bb4":"markdown","025d8bf4":"markdown","259dfa86":"markdown","a1879713":"markdown","04cdce2c":"markdown","2573de6d":"markdown","bdda31d8":"markdown","398c308d":"markdown","3b2e85b8":"markdown","e95a4d14":"markdown","7b58b042":"markdown","c0e87502":"markdown","0461a411":"markdown","58b023a3":"markdown","97d2cd81":"markdown","51993311":"markdown","bedce35e":"markdown"},"source":{"b1ce6402":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\ncolor=sns.color_palette()","06f35b52":"data=pd.read_csv('..\/input\/BlackFriday.csv')\ndata.head()","7a8ff5a9":"fig = plt.figure(figsize=(8,5))\nsns.distplot(data.Purchase)\nplt.title('Purchase Distribution')","5c6a7149":"print(\"skew\",data.Purchase.skew(),\"kurt\",data.Purchase.kurt())","a3203211":"data.describe(include = ['object', 'integer', 'float'])","58aa0732":"sns.heatmap(data.isnull(), cmap= 'Blues')","6557fdaa":"data.User_ID.plot.hist(bins=70)","eeb149cd":"df = pd.DataFrame(data = data.User_ID.value_counts())\ndf=df.reset_index()\ndf.columns = ['users', 'Purchase_History']\ndata = data.merge(df, left_on = 'User_ID', right_on = 'users')","bd0225f5":"data.head()","c3522823":"del data['User_ID']\ndel data['users']","66ce7f38":"del data['Product_ID']","d83f4d08":"fig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.boxplot(x='Gender',y='Purchase',data=data,palette='Set3',ax=ax[0])\nax[0].set_title(\"F -> Female , M -> Male\",size=12)\ndata.Gender.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True, explode=[0.1,0],cmap='Blues')\nax[1].set_title(\"Total\")","855188a5":"fig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.boxplot(x='Age',data=data,y='Purchase',palette='Set2',ax=ax[0])\nax[0].set_title('Purchase v\/s Age',size=12)\ndata.Age.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True,cmap='Oranges')","9001f700":"fig,ax=plt.subplots(2,1,figsize=(15,12))\nsns.boxplot(x='Occupation',data=data,y='Purchase',palette='Set1',ax=ax[0])\nax[0].set_title('Purchase v\/s Occupation')\ndata.Occupation.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True,cmap='Reds')","93a97929":"fig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.boxplot(x='City_Category',data=data,y='Purchase',palette='Set3',ax=ax[0])\nax[0].set_title('Purchase v\/s City Category', size=12)\ndata.City_Category.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True,cmap='Greens', \n                                           explode=[0.05,0.05,0.05])","8fdd1e35":"fig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.boxplot(x='Stay_In_Current_City_Years',data=data,y='Purchase',palette='Set3',ax=ax[0])\nax[0].set_title('Purchase v\/s No. of years stayed')\ndata.Stay_In_Current_City_Years.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True,cmap='Greys')","be140496":"fig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.boxplot(x='Marital_Status',data=data,y='Purchase',palette='Set3',ax=ax[0])\ndata.Marital_Status.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',explode=[0.05,0.05],shadow=True, cmap='Blues')","83906052":"fig,ax=plt.subplots(1,2,figsize=(15,6))\nsns.boxplot(y='Purchase',data=data,x='Product_Category_1',palette='Set2',ax=ax[0])\nsns.countplot(x= 'Product_Category_1',ax=ax[1], data= data)","a4397513":"fig,ax=plt.subplots(1,2,figsize=(15,6))\nsns.boxplot(y='Purchase',data=data,x='Product_Category_2',palette='Set2',ax=ax[0])\nsns.countplot(x= 'Product_Category_2',ax=ax[1], data= data)","a2156251":"fig,ax=plt.subplots(1,2,figsize=(15,6))\nsns.boxplot(y='Purchase',data=data,x='Product_Category_3',palette='Set2',ax=ax[0])\nsns.countplot(x= 'Product_Category_3',ax=ax[1], data= data)\n","ef250f0c":"del data['Product_Category_3']\ndata.Product_Category_2.fillna(0, inplace=True)","41507e8d":"def outliers(df):\n    q1= pd.DataFrame(df.quantile(0.25))\n    q3= pd.DataFrame(df.quantile(0.75))\n    iqr = pd.DataFrame(q3[0.75] - q1[0.25])\n    iqr['lower'] = q1[0.25] - 1.5 * iqr [0]\n    iqr['upper'] = q3[0.75] + 1.5 * iqr [0]\n    return(np.where(df > iqr['upper']) or (df < iqr['lower']))","f9c1ad99":"x = outliers(pd.DataFrame(data.Purchase))\ndata = data.drop(x[0])","27339313":"fig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.boxplot(x='Gender',y='Purchase',data=data,palette='Set3',ax=ax[0])\nax[0].set_title(\"F -> Female , M -> Male\",size=12)\nsns.boxplot(x='Age',data=data,y='Purchase',palette='Set2',ax=ax[1])\nax[1].set_title('Purchase v\/s Age',size=12)","efd24f98":"fig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.boxplot(x='City_Category',data=data,y='Purchase',palette='Set1',ax=ax[0])\nsns.boxplot(x='Stay_In_Current_City_Years',data=data,y='Purchase',palette='Set2',ax=ax[1])","f241784e":"fig=plt.figure(figsize=(12,8))\nsns.heatmap(data.corr(), annot= True, cmap='Blues')","0a4fdd70":"data.info()","07626f9b":"data.Product_Category_1=data.Product_Category_1.astype('category')\ndata.Marital_Status=data.Marital_Status.astype('category')\ndata.Occupation=data.Occupation.astype('category')\ndata.Product_Category_2=data.Product_Category_2.astype('category')","8356a409":"data_label=data['Purchase']\ndel data['Purchase']\ndata_label=pd.DataFrame(data_label)","c5e0a7c6":"data=pd.get_dummies(data,drop_first=True)\ndata.head()","9214caa0":"from sklearn.preprocessing import MinMaxScaler\ndata_scaled=MinMaxScaler().fit_transform(data)\ndata_scaled=pd.DataFrame(data=data_scaled, columns=data.columns)","f9a09cfe":"data_scaled.head()","8adc8fa4":"data_scaled.shape","5d88867c":"from sklearn.decomposition import PCA\nvariance_ratio = []\nfor i in range(5,65,5):\n    pca=PCA(n_components = i)\n    pca.fit_transform(data_scaled)\n    variance_ratio = np.append(variance_ratio,np.sum(pca.explained_variance_ratio_))","d3d70e9a":"df =pd.Series(data = variance_ratio, index = range(5,65,5))\ndf.plot.bar(figsize=(8,6))","6b387076":"pca=PCA(n_components = 40, whiten = False, random_state=876)\ndata_scaled = pd.DataFrame(pca.fit_transform(data_scaled), index= data_scaled.index)","33a2e8c7":"data_scaled.head()","5279c8ad":"from sklearn.model_selection import train_test_split\nXtrain,Xtest,Ytrain,Ytest = train_test_split(data_scaled, data_label, test_size=0.30,random_state=54368)","9a703efc":"from sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor","63fcd7a2":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score","dd73d8ad":"def CrossVal(dataX,dataY,mode,cv=3):\n    score=cross_val_score(mode,dataX , dataY, cv=cv, scoring='neg_mean_squared_error')\n    return(np.sqrt(np.mean((-score))))","61ccb74f":"sgd=SGDRegressor(random_state=324,penalty= \"l1\", alpha=0.4)\nscore_sgd=CrossVal(Xtrain,Ytrain,sgd)\nprint(\"RMSE is : \",score_sgd)","0a6b4b7c":"lr=LinearRegression(n_jobs=-1)\nscore_lr=CrossVal(Xtrain,Ytrain,lr)\nprint(\"RMSE is : \",score_sgd)","66f3eba5":"dtc=DecisionTreeRegressor(random_state=42234)\nscore_dtc=CrossVal(Xtrain,Ytrain,dtc)\nprint(\"RMSE is : \",score_dtc)","666bd72b":"rf=RandomForestRegressor(n_estimators=10, n_jobs=-1, random_state=487987)\nscore_rf= CrossVal(Xtrain,Ytrain,rf)\nprint('RMSE is:',score_rf)","502dd4cc":"etc=ExtraTreesRegressor(n_estimators=10, n_jobs=-1, random_state=3141)\nscore_etc= CrossVal(Xtrain,Ytrain,etc)\nprint('RMSE is:',score_etc)","9841eef6":"model_accuracy = pd.Series(data=[score_sgd, score_lr, score_dtc, score_rf, score_etc], \n                           index=['Stochastic GD','linear Regression','decision tree', 'Random Forest',\n                            'Extra Tree'])\nfig= plt.figure(figsize=(8,8))\nmodel_accuracy.sort_values(ascending= False).plot.barh()\nplt.title('MODEL RMSE SCORE')","e9e84f36":"Here also we are seeing lots of outliers. Will treat all in **Data Cleaning** section later in notebook","5bd4a608":"### a) Handling missing values","afd75681":"### Model accuracy plot","dcfacabf":"Product_Category_3 contains most of highest numbers of missing values. Filling this feature will not be easy therefore let's delete it.","9c3ff8ca":"### e) Dimension Reduction","3c3edf35":"With above description we can say : Total entries present in dataset is 537577. Other than that we can conclude Uniquely identified Gender is 2, Uniquely identified Age is 7, City_Category is 3, Stayed_In_Current_City is 5.","17dfbb11":"Since we know Product_Category_2 and Produuct_Category_3 contain missing values. therefore we will delete Produuct_Category_3 as most of the rows are filled with missing values. For Produuct_Category_2 we will fill all category as 0 so that other feature will be utilize to make prediction","51c3ca77":"**Product_Category_2 and Product_Category_3** contain missing value. Most of the values are missing specially from **Product_Category_3**. We have to take care these columns.","b9ee465a":"### Feel free to ask any doubt\/question\/ or to give any suggestion :)","348a1683":"### e)  City_Category","4dc27455":"#### Now we can see the difference","3d25464a":"People from City C spend more than other two cities. People from City B are more active than other two cities","7004db3f":"## Part3: Predictive Modeling","4d0bbfbd":"Most of the purchase made from Product_category 5. Whereas product Category 17 contain least number.","266d56d0":"Let's explore more feature to get more insight from dataset","bc74848e":"### e) Extra Trees Classifier","56b4142c":"### d) Occupation","638208bb":"This insight will be very usefull for our model","1a825afd":"**Will add bagging , boosting , stacking , Hyper param tuning in next update. Keep checking**","bcfaa2f5":"#### Let's check Target columns (Purchase) detail ?","f6940ac5":"### Importing ML libraries","fefe2f62":"#### Splittting data into test and train set","afce3f2d":"### Objective : \nMain obejctive behind this notebook is to give an idea along with workflow of Machine Learning Processes.\n\nStarting from **Getting data informaion to Exploratory Data Analysis, Data Manipulation, Building and then Validation of Model.**\n\nI am trying to keep it as **simple** as i can so that newbie can also understand the workflow.\n\nIf you learn anything useful from this notebook then **Give Upvote :)\n** All **QUESTION\/DOUBTS\/SUGGESTIONS** are welcomed here\n","6ff2be90":"User_ID columns seems to be important feature. We can't use it as raw. But we can extract some information.","cd21c58c":"#### Normalization (To bring each feature at same scale)","94972920":"## 1. Feature analysis","c63c43e6":"## 2. Data Cleaning","5ccc73a7":"Initially there was lots of Outliers but now we can see most of the outliers are deleted from dataset. Also conserved **Variance of about ~ 95%**","f7f3333b":"Let's fit and see output","324f18b2":"### c) Age","97d4485d":"#### Let's use some statistic to handle Outliers","f68f7ab9":"Product Category 7  have very less entries.","c672e8fd":"Since Product_ID seems to be no use therefore deleting it here itself","08a4ff8c":"**Will add Chi-squared test** in next version to improve this part","7741ccb9":"# Begineer Pack + Intermediate (ALL IN ONE)","19393cef":"### h) Product_Category_1, Product_Category_2, Product_Category_3","719bcbdb":"Little bit of outliers we can expect that is above 23000. Will dig more let's move to next feature ","2c99b5e5":"Since we know that all predictors are Categorical variables. therefore correlation can't determine relationship between predictor and target.","e77a288d":"Here we can see if we set compenents near ~ 30-40 then we will be able to capture around 90~95% of variance.","dae66ef3":"### b)-> Gender","04c7f433":"Here also we can easily see lots of outliers are present. With pie plot we can see mostly youngster are pretty active in online purchase. ","ad4a49e3":"From above distribution graph we can conclude that Purchase ranges from around 100+ to 24000+ and distribution seems to be very little positive skewed. We need to dig more to get info related to distibution.","244ddbda":"### c) Decision Tree Classifier","3af3ea41":"#### Let's check some of the plot after removing outliers","8f682430":"## Contents of the Notebook:\n\n#### Part1: Exploratory Data Analysis(EDA):\na) Analysis of the features.\n\nb) Finding any relations or trends considering multiple features.\n\n#### Part2: Data Cleaning:\na) Handling Missing Values\n\nb) Outliers handling Using Statistics\n\nc) Correlation\n\nd) Converting features into suitable form\n\ne) Dimension Reduction\n \n\n#### Part3: Predictive Modeling\na) Fitting ML models\n\nb) Cross Validation\n\nc) Model Comparison \n\nd) Ensemble model (Will update in enxt Version)","4a084c2c":"### d) Random Forest Classifier","bd809850":"### b) Handling Outliers","f1f6b45c":"### f) Stayed_In_Current_City_Years","a00a37e1":"### a)->User_ID","5f2d640a":"It will be easy for our model to quickly do prediction. So lets **take dimension as 40**","c6a19d60":"**What if we get number of times user make purchase using Columns User_ID.** ","2731304b":"Main problem is **number of principal components**. ","0dac17b8":"Without any Hyper parameter tuning, Random forest is doing better than other models.","83289e1d":"### Attribute Information:\n    1. User ID                          2. Product ID\n    3. Gender                           4. Age\n    5. Occupation                       6. City Category\n    7. Stayed in current city           8. Marital Status\n    9. Product Category 1               10. Product Category 2\n    11.Product Category 3               12. Purchase","f8612d5d":"Now next","93850ff1":"### Cross validation helper function","836ab5ad":"### g) Marital_status","b67dce30":"Most of the people who are in occupation 17 spend more than others. Most of people belong to occupation 4.  ","c330cbb6":"**NOTE: Normalisation is should be done before dimension reduction**. As we already done Normalisation in our last section so we can proceed further","4b645ddf":"### a)Stochastic Gradient Descent ","c2b2938c":"![image.png](attachment:image.png)","c56b23b9":"### a) Fitting ML models","82b34375":"### Stay tuned for more updates. And don't forget to give an upvote if you like it ","3fc00bb4":"### d) Feature conversion","025d8bf4":"Here we have added one more column which show's **How many times order placed using same User_ID**.\nNow we can delete **User_ID** as we extracted information","259dfa86":"Let's explore other feature","a1879713":"### c) Correlation between different features","04cdce2c":"let's move to next feature","2573de6d":"Male are more active in shopping than Female. **This is interesting**. With plot we can expect median of around 9000. ","bdda31d8":"**We can use this attribute to train our model**. Lets start ","398c308d":"#### Let's check for Missing Values","3b2e85b8":"## Part1: Exploratory Data Analysis(EDA)","e95a4d14":"**Married people spend more than unmarried ones.** This isn't surprise me :P","7b58b042":"### b) Linear Regression","c0e87502":"#### As we seen in EDA part that Purchase (target) column contains lots of outliers.","0461a411":"### Evalutation metrics to check model performance","58b023a3":"There are very small small things which can be infer from above description","97d2cd81":"Will set upper and lower limit on Purchase column. And any entries outside bounded region wil be deleted","51993311":"**Stayed_In_Current_City_Years feature seems to identical for all the years**. May be this feature don't have much impact on purchase. ","bedce35e":"Let''s reduce dimension of data so that it will be easy for ML to process. It may result in reduction of accuracy but it will result in fasten training process"}}