{"cell_type":{"829fea48":"code","b1100b46":"code","89925469":"code","4d8059c9":"code","702b8663":"code","16c981fc":"code","d3a87d3b":"code","5b8cbfd5":"code","9598c9cc":"code","4cfd1d2c":"code","09599557":"code","3a1f27cc":"code","e1f21c7d":"code","0209c896":"code","9f00fe33":"code","19414d8c":"code","029434ac":"code","180579a0":"code","d7bbab91":"code","a7ee17e7":"code","acd9c584":"code","1eab1dc0":"markdown","8c4ee2ab":"markdown","71fe1d54":"markdown","44834714":"markdown","82064bf7":"markdown","0dc90aaf":"markdown","18e816dd":"markdown","ebb2962c":"markdown","cdc9a3f3":"markdown","e21d28c3":"markdown","f7232bc3":"markdown","e35be975":"markdown","dffb03c6":"markdown","5ceba33b":"markdown","95f93dd4":"markdown","d6fc85eb":"markdown"},"source":{"829fea48":"!pip install efficientnet-pytorch -qqq","b1100b46":"from fastai.vision.all import *\n\n\nimport albumentations # Data Augmentation\nfrom efficientnet_pytorch import EfficientNet # The Model\n","89925469":"class AlbumentationsTransform(DisplayedTransform):\n    def __init__(self, aug): self.aug = aug\n    def encodes(self, img: PILImage):\n        aug_img = self.aug(image=np.array(img))['image']\n        return PILImage.create(aug_img)","4d8059c9":"class AlbumentationsTransform(RandTransform):\n    \"A transform handler for multiple `Albumentation` transforms\"\n    split_idx,order=None,2\n    def __init__(self, train_aug, valid_aug): store_attr()\n    \n    def before_call(self, b, split_idx):\n        self.idx = split_idx\n    \n    def encodes(self, img: PILImage):\n        if self.idx == 0:\n            aug_img = self.train_aug(image=np.array(img))['image']\n        else:\n            aug_img = self.valid_aug(image=np.array(img))['image']\n        return PILImage.create(aug_img)","702b8663":"def get_train_aug(): return albumentations.Compose([\n            albumentations.RandomResizedCrop(256,256),\n            albumentations.Transpose(p=0.5),\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.ShiftScaleRotate(p=0.5),\n            albumentations.HueSaturationValue(\n                hue_shift_limit=0.2, \n                sat_shift_limit=0.2, \n                val_shift_limit=0.2, \n                p=0.5\n            ),\n            albumentations.RandomBrightnessContrast(\n                brightness_limit=(-0.1,0.1), \n                contrast_limit=(-0.1, 0.1), \n                p=0.5\n            ),\n            albumentations.CoarseDropout(p=0.5),\n            albumentations.Cutout(p=0.5)\n])","16c981fc":"def get_valid_aug(): return albumentations.Compose([\n    albumentations.CenterCrop(256,256, p=1.),\n    albumentations.Resize(256,256)\n], p=1.)","d3a87d3b":"item_tfms = AlbumentationsTransform(get_train_aug(), get_valid_aug())","5b8cbfd5":"set_seed(999)","9598c9cc":"path = Path(\"..\/input\")\ndata_path = path\/'cassava-leaf-disease-classification'\ndf = pd.read_csv(data_path\/'train.csv')\ndf['image_id'] = df['image_id'].apply(lambda x: f'train_images\/{x}')","4cfd1d2c":"df.head()","09599557":"idx2lbl = {0:\"Cassava Bacterial Blight (CBB)\",\n          1:\"Cassava Brown Streak Disease (CBSD)\",\n          2:\"Cassava Green Mottle (CGM)\",\n          3:\"Cassava Mosaic Disease (CMD)\",\n          4:\"Healthy\"}\n\ndf['label'].replace(idx2lbl, inplace=True)","3a1f27cc":"df.head()","e1f21c7d":"blocks = (ImageBlock, CategoryBlock)\nsplitter = RandomSplitter(valid_pct=0.2, seed=999)\ndef get_x(row): return data_path\/row['image_id']\ndef get_y(row): return row['label']","0209c896":"block = DataBlock(blocks=blocks,\n                 get_x=get_x,\n                 get_y=get_y,\n                 splitter=splitter,\n                 item_tfms=item_tfms,\n                 batch_tfms=[Normalize.from_stats(*imagenet_stats)])","9f00fe33":"dls = block.dataloaders(df, bs=32, val_bs=64)","19414d8c":"dls.show_batch(figsize=(12,12))","029434ac":"class LeafModel(Module):\n    def __init__(self, num_classes):\n\n        self.effnet = EfficientNet.from_pretrained(\"efficientnet-b3\")\n        self.dropout = nn.Dropout(0.1)\n        self.out = nn.Linear(1536, num_classes)\n\n    def forward(self, image):\n        batch_size, _, _, _ = image.shape\n\n        x = self.effnet.extract_features(image)\n        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)\n        outputs = self.out(self.dropout(x))\n        return outputs","180579a0":"net = LeafModel(dls.c)","d7bbab91":"learn = Learner(dls, net, loss_func=CrossEntropyLossFlat(),\n               metrics=[accuracy]).to_native_fp16()","a7ee17e7":"learn.fit_flat_cos(10, 3e-4, pct_start=0.0,\n                  cbs=[EarlyStoppingCallback(patience=3),\n                      SaveModelCallback()])","acd9c584":"learn.export('baseline')","1eab1dc0":"## Building the Model\n\nNext we'll want to build our model. I've recreated it in raw PyTorch below:","8c4ee2ab":"## Setting up our transforms\n\nNow we need to use `albumentations` for everything, as that's what he does!\n\nSo, how can we manage this?\n\nfastai has a helpful [tutorial](https:\/\/docs.fast.ai\/tutorial.albumentations.html) on how to get started with albumentations, and we'll take it a step further.\n\nFirst here is what we currently have:\n","71fe1d54":"While this transform *does* work, it assumes that everything is applied to both the training and the validation set, and doesn't let us put in custom pipelines for both. Let's change that:","44834714":"And now we'll train our model. To be equivalent we'll use `fit_flat_cos` with a `pct_start` of zero, along with `EarlyStopping`:","82064bf7":"## Training\n\nAll that's left is to train our model.\n\nWe're not going to follow the freeze + unfreeze transfer learning methodology of fastai *only because I want this as close to a 1:1 as possible*. \n\nLet's build our `Learner`:","0dc90aaf":"And now we can build it:","18e816dd":"I can now safely say we have *equivalent* data to Abhishek's. Let's look at a batch:","ebb2962c":"Since albumentations are operating on `PILImage`'s, these need to be in the item transforms\n> Warning: do not include `Normalize` in here! Our transform expects to be able to generate a PILImage at the end, and adjusting the datatypes will cause issues!","cdc9a3f3":"## Importing from the libraries\n\nNext we'll want to bring in everything we want.","e21d28c3":"## A few key differences between his and what other fastai-kernels are doing\n\nThere are a few pieces that differ in most other fastai kernels that don't quite breach the 89% threshold with a single model. I believe it comes down to two factors:\n\n1. Data Augmentation\n2. How they are performing TTA\n3. The scheduling being used\n\n**Data Augmentation**\n\nIn his notebook there are a few transforms that fastai does not have equivalents of, such as `HueSaturationValue`. We have `Hue` and `Saturation`, and `Value` has not been implemented yet (and I don't know quite enough yet about those transforms to work out the logic). \n\n**TTA**\n\nIn most of the fastai kernels I've seen, they follow a standard TTA regiment. Abhishek performs TTA 15 times, which I saw had a significant boost in accuracy compared to the standard 3+1 that fastai will do\n\n**Scheduling**\n\nMore PyTorch related kernels are now fitting with a `CosineAnnealingWarmRestarts` scheduler, but fastai doesn't have an equivalent, right?\n\nIt actually does. So `WarmRestarts` `T_0` value simply dictates how long we should be calling a `fit` for, and as we will see later the proper scheduler to use is `fit_flat_cos` with a `start_pct` of 0 (thanks to @tanlikesmath for pointing this out to me!)","f7232bc3":"We've achieved roughly 84-85% accuracy, so now we will export our model away and do inference with it in a seperate kernel [here](https:\/\/www.kaggle.com\/muellerzr\/fastai-abhishek-inference)","e35be975":"And build the `DataLoaders`:","dffb03c6":"What we have done here is allow for a `train_aug` and `valid_aug` transform pipeline (these can be `albumentation.Compose`'d transforms, as we will see later) and we've ensured that some transforms will only occur on the training set (when split_idx is 0) and on the validation set (1). Let's now recreate his data augmentation:","5ceba33b":"# Recreating Abhishek's 89.3% Start Kernel with fastai\n\n\nI saw Abhishek's [notebook](https:\/\/www.kaggle.com\/abhishek\/tez-faster-and-easier-training-for-leaf-detection?scriptVersionId=47408263) a number of days ago and wanted to try and recreate it in a 1:1 fashion with fastai to ensure that it could perform similar.\n\nThis notebook will be the result of that process, and by the end of it we will have trained an equivalent model that can achieve 89.4% on the public LB.","95f93dd4":"## Building our `DataBlock`\n\nWe now have everything setup to build our data! We'll follow my similar notebook's data approach from [here](https:\/\/www.kaggle.com\/muellerzr\/cassava-fastai-starter)","d6fc85eb":"## Installing what we need\n\nWe'll be installing the `efficientnet-pytorch` library:"}}