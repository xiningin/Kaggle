{"cell_type":{"a9a588e1":"code","847e208d":"code","bd6b7748":"code","fa65761e":"code","b94c0e30":"code","21908c0b":"code","9c537987":"code","70d00253":"code","5c6c30ac":"code","4e459887":"code","2ba5c933":"code","3087e928":"code","42bdbe75":"code","2c89baa8":"code","57ba88e8":"code","2664cd28":"code","3215fb22":"code","cb06cb06":"code","509cec72":"code","7956b8c6":"code","8e9b4d20":"code","15cc6a63":"code","c964fa6e":"code","31e9fe7a":"code","2404206e":"code","6bc26522":"code","fd0a8e42":"code","bbba23f3":"code","ed7965a8":"code","2c4bd3df":"code","1379e284":"code","949a4aa9":"code","7fc3169c":"code","1bdb63e2":"code","d8bac1da":"code","6c8f7b3f":"code","e7908e46":"code","021cb3bd":"code","b06f87be":"code","2769bdbf":"code","b1439ca1":"code","f8fd0e52":"code","b0ff1eb7":"code","5baab85a":"code","46f3f05c":"code","09045770":"code","a2cbe3db":"code","c3beced0":"code","bc2fa072":"code","16504d9b":"code","fc878681":"code","370d9d7a":"code","809ba354":"code","853c20dd":"code","9688f503":"code","9c0bf003":"code","66fdfde9":"code","fc221c11":"code","1f80d284":"code","79a94d23":"code","a72ad825":"code","24199c9b":"markdown","fd316a67":"markdown","3fd2c98a":"markdown","81c33a6e":"markdown","ab8343cf":"markdown","82ce1b90":"markdown","cb0b0a00":"markdown","109536b6":"markdown","20aec42d":"markdown","fb039cf6":"markdown","1d2f6b5a":"markdown","45fc88bf":"markdown","c18b2e56":"markdown","9c44f5cd":"markdown","4495f8e1":"markdown","dbc4a0bd":"markdown","0bd16bc1":"markdown","1c47786b":"markdown","f2da470b":"markdown","fe762a75":"markdown","cc47e109":"markdown","b33606e4":"markdown","6e8b62f8":"markdown","cd5fb1a3":"markdown","13a93971":"markdown","3241bd89":"markdown","c5375a13":"markdown","12304aab":"markdown","4b520529":"markdown","187ab04c":"markdown","8c54b587":"markdown","968059fd":"markdown","2808403a":"markdown","d0005073":"markdown","25bdd689":"markdown","e155c905":"markdown","8435b22b":"markdown"},"source":{"a9a588e1":"#Upgrade pip and install tqdm\n!pip install --upgrade pip\n!pip install tqdm\n!pip install pyLDAvis","847e208d":"!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.3\/en_core_sci_md-0.2.3.tar.gz","bd6b7748":"#import packages\nimport json\nfrom pprint import pprint\nimport numpy as np\nimport pandas as pd\nimport os\nimport zipfile\nfrom tqdm import tqdm\nfrom copy import deepcopy\nimport spacy\nimport en_core_sci_md\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import PCA, LatentDirichletAllocation\nfrom scipy.spatial.distance import jensenshannon\nimport joblib\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML, display\nfrom ipywidgets import interact, Layout, HBox, VBox, Box\nimport ipywidgets as widgets\nfrom IPython.display import clear_output\nimport pyLDAvis\nimport pyLDAvis.sklearn\npyLDAvis.enable_notebook()\nplt.style.use(\"dark_background\")","fa65761e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b94c0e30":"df= pd.read_csv('\/kaggle\/input\/all_papers_filtered.csv')\ndf.shape","21908c0b":"df.head()","9c537987":"all_texts = df.text","70d00253":"# example snippet\nall_texts[42][0:500]","5c6c30ac":"# medium model\nnlp = en_core_sci_md.load(disable=[\"tagger\", \"parser\", \"ner\"])\nnlp.max_length = 1700000","4e459887":"!python -m spacy download fr_core_news_sm\n!python -m spacy download es_core_news_sm\n!python -m spacy download it_core_news_sm\n!python -m spacy download de_core_news_sm\n!python -m spacy download en_core_web_sm\n!python -m spacy download xx_ent_wiki_sm","2ba5c933":"import fr_core_news_sm\nimport es_core_news_sm\nimport it_core_news_sm\nimport de_core_news_sm\nimport en_core_web_sm\nimport xx_ent_wiki_sm\n\nnlp_fr = fr_core_news_sm.load()\nnlp_es = es_core_news_sm.load()\nnlp_it = it_core_news_sm.load()\nnlp_de = de_core_news_sm.load()\nnlp_en = en_core_web_sm.load()\nnlp_xx = xx_ent_wiki_sm.load()\n\nspacy_stopwords_fr = list(spacy.lang.fr.stop_words.STOP_WORDS)\nspacy_stopwords_es = list(spacy.lang.es.stop_words.STOP_WORDS)\nspacy_stopwords_it = list(spacy.lang.it.stop_words.STOP_WORDS)\nspacy_stopwords_de = list(spacy.lang.de.stop_words.STOP_WORDS)\nspacy_stopwords_en = list(spacy.lang.en.stop_words.STOP_WORDS)\n#spacy_stopwords_xx = list(spacy.lang.xx.stop_words.STOP_WORDS)","3087e928":"spacy_stopwords_en[0:10]","42bdbe75":"def spacy_tokenizer(sentence):\n    return [word.lemma_ for word in nlp(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1)]","2c89baa8":"# New stop words list \nstop_words = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'fig', 'fig.', 'al.', \n    'di', 'la', 'il', 'del', 'le', 'della', 'dei', 'delle', 'una', 'da',  'dell',  'non', 'si'\n]\n\ncustomize_stop_words = stop_words + spacy_stopwords_fr + spacy_stopwords_es + spacy_stopwords_it + spacy_stopwords_de + spacy_stopwords_en\n\n# Mark them as stop words\nfor w in customize_stop_words:\n    nlp.vocab[w].is_stop = True","57ba88e8":"print(customize_stop_words[1:10])\nprint(len(customize_stop_words))","2664cd28":"sent = \"Je suis un champion 9 123 *.\"\nspacy_tokenizer(sent)","3215fb22":"# vectorizer = CountVectorizer(ngram_range=(1,2), tokenizer = spacy_tokenizer, max_features=800000, analyzer='word')\n# data_vectorized = vectorizer.fit_transform(tqdm(all_texts))\n\n# joblib.dump(vectorizer, '\/kaggle\/working\/bigram_vectorizer.csv')\n# joblib.dump(data_vectorized, '\/kaggle\/working\/bigram_data_vectorized.csv')","cb06cb06":"# Load trained vectorizer and vectorized data\nvectorizer = joblib.load('\/kaggle\/input\/bigram_vectorizer.csv')\ndata_vectorized = joblib.load('\/kaggle\/input\/bigram_data_vectorized.csv')","509cec72":"print(data_vectorized.shape)\nprint(data_vectorized)","7956b8c6":"# most frequent words\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(data_vectorized.sum(axis=0))[0]})\n\nword_count.sort_values('count', ascending=False).set_index('word')[:20].sort_values('count', ascending=True).plot(kind='barh')\nplt.show()","8e9b4d20":"# Define Search Param\n# from sklearn.model_selection import GridSearchCV\n\n# search_params = {'n_components': [10, 20, 30, 40, 50], 'learning_decay': [.5, .7, .9]}\n\n# # Init the Model\n# lda = LatentDirichletAllocation()\n\n# # Init Grid Search Class\n# model = GridSearchCV(lda, param_grid=search_params,n_jobs= -1)\n\n# # Do the Grid Search\n# model.fit(data_vectorized)","15cc6a63":"# Best Model\n# best_lda_model = model.best_estimator_\n\n# # Model Parameters\n# print(\"Best Model's Params: \", model.best_params_)\n\n# # Log Likelihood Score\n# print(\"Best Log Likelihood Score: \", model.best_score_)\n\n# # Perplexity\n# print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))","c964fa6e":"# lda = LatentDirichletAllocation(n_components=10, learning_decay=0.9,  random_state=0,verbose=1)\n# lda.fit(data_vectorized)\n# joblib.dump(lda, '\/kaggle\/working\/lda_10.csv')","31e9fe7a":"lda = joblib.load('\/kaggle\/input\/lda_10.csv')","2404206e":"print(lda)","6bc26522":"# Visualize topic model \n#pyLDAvis.sklearn.prepare(lda, data_vectorized, vectorizer)","fd0a8e42":"# Log Likelyhood: Higher the better\nprint(\"Log Likelihood: \", lda.score(data_vectorized))\n\n# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\nprint(\"Perplexity: \", lda.perplexity(data_vectorized))\n\n# # See model parameters\n# pprint(lda.get_params())","bbba23f3":"def print_top_words(model, vectorizer, n_top_words):\n    feature_names = vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"\\nTopic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\n    print()","ed7965a8":"print_top_words(lda, vectorizer, n_top_words=25)","2c4bd3df":"# doc_topic_dist = pd.DataFrame(lda.transform(data_vectorized))\n# doc_topic_dist.to_csv('\/kaggle\/working\/doc_topic_dist_10.csv', index=False)","1379e284":"doc_topic_dist = pd.read_csv('\/kaggle\/input\/doc_topic_dist_10.csv')","949a4aa9":"print(doc_topic_dist.shape)","7fc3169c":"doc_topic_dist.head()","1bdb63e2":"pca = PCA (n_components=10)\npca_topics = pca.fit(doc_topic_dist).transform(doc_topic_dist)","d8bac1da":"print ('First component explain {} variance of data and second component explain {} variance of data'.format(pca.explained_variance_ratio_[0],pca.explained_variance_ratio_[2]))","6c8f7b3f":"plt.scatter(pca_topics[:,0],pca_topics[:,1])\nplt.title(\"PCA\")\nplt.show()","e7908e46":"df.index = range(len(df.index))\ndf.tail()","021cb3bd":"is_covid19_article = df.text.str.contains('COVID-19|SARS-CoV-2|2019-nCov|SARS Coronavirus 2|2019 Novel Coronavirus')\nprint(is_covid19_article.shape)\nprint(is_covid19_article)","b06f87be":"doc_topic_dist[is_covid19_article]","2769bdbf":"def get_k_nearest_docs(doc_dist, k=5, lower=1950, upper=2020, only_covid19=False, get_dist=False):\n    '''\n    doc_dist: topic distribution (sums to 1) of one article\n    \n    Returns the index of the k nearest articles (as by Jensen\u2013Shannon divergence in topic space). \n    '''\n    \n    relevant_time = df.publish_year.between(lower, upper)\n    \n    if only_covid19:\n        temp = doc_topic_dist[relevant_time & is_covid19_article]\n        \n    else:\n        temp = doc_topic_dist[relevant_time]\n         \n    distances = temp.apply(lambda x: jensenshannon(x, doc_dist), axis=1)\n    k_nearest = distances[distances != 0].nsmallest(n=k).index\n    \n    if get_dist:\n        k_distances = distances[distances != 0].nsmallest(n=k)\n        return k_nearest, k_distances\n    else:\n        return k_nearest","b1439ca1":"task5 = [\"Effectiveness of drugs being developed and tried to treat COVID-19 patients. Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.\",\n\"Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\",\n\"Exploration of use of best animal models and their predictive value for a human vaccine.\",\n\"Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\",\n\"Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.\",\n\"Efforts targeted at a universal coronavirus vaccine.\",\n\"Efforts to develop animal models and standardize challenge studies\",\n\"Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers\",\n\"Approaches to evaluate risk for enhanced disease after vaccination\",\n\"Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics]\"]\n\ntask5b = [\n\"Exploration of use of best animal models and their predictive value for a human vaccine.\"]\n\ntasks={'What do we know about vaccines and therapeutics?': task5, \n       'Exploration of use of best animal models and their predictive value for a human vaccine.': task5b}","f8fd0e52":"def relevant_articles(tasks, k=3, lower=1950, upper=2020, only_covid19=False):\n    tasks = [tasks] if type(tasks) is str else tasks \n    \n    tasks_vectorized = vectorizer.transform(tasks)\n    tasks_topic_dist = pd.DataFrame(lda.transform(tasks_vectorized))\n\n    for index, bullet in enumerate(tasks):\n        print(bullet)\n        recommended = get_k_nearest_docs(tasks_topic_dist.iloc[index], k, lower, upper, only_covid19)\n        recommended = df.iloc[recommended].copy()\n\n    return recommended[['paper_id','title','authors','affiliations','abstract','text','source_x','journal','pubmed_id','publish_time']]","b0ff1eb7":"relevant_articles(task5b, 10, only_covid19=True)","5baab85a":"recommended = relevant_articles(task5b, 10, only_covid19=True)","46f3f05c":"title = recommended.iloc[3,].title\nabstract = recommended.iloc[3,].abstract\ntext = recommended.iloc[3,].text\nprint(\"Title: {0}  \\nAbstract: {1}\".format(title,abstract))","09045770":"title = recommended.iloc[4,].title\nabstract = recommended.iloc[4,].abstract\ntext = recommended.iloc[4,].text[0:500]\nprint(\"Title: {0}  \\nAbstract: {1}\".format(title,abstract))","a2cbe3db":"title = recommended.iloc[6,].title\nabstract = recommended.iloc[6,].abstract\ntext = recommended.iloc[6,].text[0:500]\nprint(\"Title: {0}  \\nAbstract: {1}\".format(title,abstract))","c3beced0":"title = recommended.iloc[8,].title\nabstract = recommended.iloc[8,].abstract\ntext = recommended.iloc[8,].text[0:500]\nprint(\"Title: {0}  \\nAbstract: {1}\".format(title,abstract))","bc2fa072":"title = recommended.iloc[3,].title\nabstract = recommended.iloc[3,].abstract\ntext = recommended.iloc[3,].text\nprint(\"Title: {0}  \\nAbstract: {1}\".format(title,abstract))","16504d9b":"import logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.CRITICAL)\nfrom collections import defaultdict\nfrom gensim import corpora\nfrom gensim import models\nfrom gensim import similarities\nimport nltk\nfrom nltk import tokenize","fc878681":"nltk.download('punkt')","370d9d7a":"documents = tokenize.sent_tokenize(text)\nprint(documents[0])","809ba354":"documents = tokenize.sent_tokenize(text)\n\n# remove common words and tokenize\nstoplist = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'fig', 'fig.', 'al.', \n    'di', 'la', 'il', 'del', 'le', 'della', 'dei', 'delle', 'una', 'da',  'dell',  'non', 'si'\n]\ntexts = [\n    [word for word in document.lower().split() if word not in stoplist]\n    for document in documents\n]\n\n# remove words that appear only once\nfrequency = defaultdict(int)\nfor text in texts:\n    for token in text:\n        frequency[token] += 1\n\ntexts = [\n    [token for token in text if frequency[token] > 1]\n    for text in texts\n]\n\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]","853c20dd":"print(dictionary)","9688f503":"# Similarity interface\nlsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n\ndoc = \"Exploration of use of best animal models and their predictive value for a human vaccine.\"\nvec_bow = dictionary.doc2bow(doc.lower().split())\nvec_lsi = lsi[vec_bow]  # convert the query to LSI space\nprint(vec_lsi)","9c0bf003":"index = similarities.MatrixSimilarity(lsi[corpus])  # transform corpus to LSI space and index it\n\nsims = index[vec_lsi]  # perform a similarity query against the corpus\n#print(list(enumerate(sims)))  # print (document_number, document_similarity) 2-tuples\n\nsims = sorted(enumerate(sims), key=lambda item: -item[1])\nfor i, s in enumerate(sims):\n    print(s, documents[i])","66fdfde9":"import logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.CRITICAL)\npd.set_option('display.max_colwidth', -1)","fc221c11":"#pd.reset_option('^display.', silent=True)","1f80d284":"# Refresh corpus\ndf= pd.read_csv('\/kaggle\/input\/all_papers_filtered.csv')","79a94d23":"#Implement function for sentance matching\ndef sim_sentence(title,text, stopwords, search):\n    \"\"\"\"\n    title: str title of paper with text\n    text: str body of text of paper you want to search\n    stopwords: list of stopwords to remove from text\n    search: str search of sentence you want to return a match for.\n    \n    returns dataframe of document number and similarity score to search string\n    \"\"\"\n    \n    documents = tokenize.sent_tokenize(text)\n\n    # remove common words and tokenize\n    stoplist = stopwords \n    \n    texts = [\n        [word for word in document.lower().split() if word not in stoplist]\n        for document in documents\n    ]\n\n    # remove words that appear only once\n    frequency = defaultdict(int)\n    for text in texts:\n        for token in text:\n            frequency[token] += 1\n\n    texts = [\n        [token for token in text if frequency[token] > 1]\n        for text in texts\n    ]\n\n    dictionary = corpora.Dictionary(texts)\n    corpus = [dictionary.doc2bow(text) for text in texts]\n    lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n    \n    doc = search\n    vec_bow = dictionary.doc2bow(doc.lower().split())\n    vec_lsi = lsi[vec_bow]  # convert the query to LSI space\n    #print(vec_lsi)\n    \n    index = similarities.MatrixSimilarity(lsi[corpus])  # transform corpus to LSI space and index it\n    sims = index[vec_lsi]  # perform a similarity query against the corpus\n    #print(list(enumerate(sims)))  # print (document_number, document_similarity) 2-tuples\n\n    sims = sorted(enumerate(sims), key=lambda item: -item[1])\n    top10_sentences = sims[0:10]\n    df=pd.DataFrame()\n    for i, s in enumerate(top10_sentences):\n        index = top10_sentences[i][0]\n        sim_score = top10_sentences[i][1]\n        matched_sentence = documents[i]\n        d= {\"title\": title,\"Index\": index,\"sim_score\":sim_score,\"matched_sentence\":matched_sentence}\n        df= df.append(d,ignore_index=True)\n    return df[['Index','title','matched_sentence','sim_score']]\n\n\ndef relevant_sentences(task, search, stopwords, k=3,lower=1950, upper=2020, only_covid19=False):\n    \"\"\"\"\n    task: list of task that you want to return relevant sentences for\n    search: search str of task \n    stopwords: list of stopwords\n    k = number of recommended articles to return\n    lower= beginning year range\n    upper= end year range\n    only_covid19= boolean indicating whether to only return covid-19 articles\n    \n    Returns: printed dataframes showing matched sentances and similarity scores to task and search string\n    \"\"\"\n    # Return recommended articles\n    recommended = relevant_articles(task, k, only_covid19)\n    stoplist= stopwords\n    doc = search\n    \n    # Run for all papers in the recommended papers\n    for index,row in recommended.iterrows():\n        print(row['title'])\n        df = sim_sentence(row['title'],row['text'],stoplist,doc)\n        print(df[['matched_sentence','sim_score']].head(10))","a72ad825":"#Test out function\nsearch = \"Exploration of use of best animal models and their predictive value for a human vaccine.\"\nrelevant_sentences(task5b, search, customize_stop_words,k=10,only_covid19= True)","24199c9b":"### The count vectorizer counts the number of times a word appears in a text. You can see below that some sentences have some words that occur more than once","fd316a67":"## We fit an LDA topic model on the vectorized data with 10 topics and a learning decay rate of 0.9 which were the best parameters from the grid search output","3fd2c98a":"## Use PyLDA Davis to visualize the topics. Due to memory issues this is not displaying","81c33a6e":"## What do we know about virus genetics, origin and evolution>","ab8343cf":"## After finding the most relevant papers using LDA topic modeling for a task we were curious to see if we could find the most relevant sentences within the recommended papers that is similar to the task query.Gensim has a LSI algorithm that can return semantically similar sentance with a score.\n## The downside to this method is that it only returns a sentance that is semantically similar by the words and does not return sentances that are similarly meaning or can exactly answer the task query.\n## However the idea is that if a sentance is semantically similar to a task query then maybe it can help answer that question.\n\n## This is a work in progress","82ce1b90":"## Latent Dirichlet Allocation (LDA) topic model","cb0b0a00":"### Function definition","109536b6":"## Install scispacy language model en_core_sci_md","20aec42d":"## We print the parameters of the best model from the grid search","fb039cf6":"## We transform the vectorized data into a document topic distribution using the LDA topic model we trained","1d2f6b5a":"## Kaggle competition tasks. We focused on the tasks specifically related to Vaccines and therapeutics with particular interest in \"Exploration of use of best animal models and their predictive value for a human vaccine\"","45fc88bf":"## The shape of the document topic distribution matrix is 27171 rows for all the papers in the corpus and 10 columns for the topics from the topic model","c18b2e56":"Reference: https:\/\/www.kaggle.com\/danielwolffram\/topic-modeling-finding-related-articles\/data","9c44f5cd":"### Sample a few paper abstract results","4495f8e1":"## We are using the same function that Daniel Wolframm constructed. The function uses the Jensenshannon distance to return the most similar articles in a topic space to a given article","dbc4a0bd":"## Install spacy language models for different languages","0bd16bc1":"### Example of spacy tokenizer\nThe tokenizer removes any punctuation, numbers, stop words and any foreign language words like \"suis\" and \"un\".","1c47786b":"### Reference https:\/\/scikit-learn.org\/stable\/auto_examples\/applications\/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py","f2da470b":"We want to give credit to @xhulu for this kernel https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv. We used this kernel to help clean the papers dataset we used for this competition and this kernel saved us a lot of time.\n\nWe also want to give credit to Daniel Wolffram and his kernel https:\/\/www.kaggle.com\/danielwolffram\/topic-modeling-finding-related-articles. Our kernel follows a similar approach to his Kernel with some edits and additions.","fe762a75":"## As you can see below the document topic distribution shows the distribution that each paper corpus has. For example you can see that paper 1 is mainly topic 4 (0.81).","cc47e109":"## Create stop words list with stopwords from several foreign languages","b33606e4":"## We print the top 25 words in each of the 10 topics from the LDA model.","6e8b62f8":"### Exploration of use of best animal models and their predictive value for a human vaccine.","cd5fb1a3":"## Gensim sentence similarity implementation using LSI (Latent Semantic Indexing)","13a93971":"## Discovered Topics ","3241bd89":"## Import necessary packages","c5375a13":"## Topic modeling ","12304aab":"### Function that returns a dataframe of the most relevant articles for a task query","4b520529":"# Read in cleaned papers dataset","187ab04c":"## We run PCA (Principle component analysis) on the topics to see how much variation of the data can be explained by the topics","8c54b587":"## We use scikit-learn's gridsearch to find the ideal number of topics and learning decay parameters. In the Gensim package you can use the coherence scores to find the ideal number of topics. ","968059fd":"#### We use a countvectorizer and use the spacy tokenizer to fit the text. We use an ngram_range of (1,2) which means we are considering using unigrams and bigrams in the text. For example for this sentence \"Data Science is a fascinating field\". A unigram would be \"Data\" and a bigram would be \"Data Science\".","2808403a":"1. This Kaggle Kernel is the work of the Central Data Science (CDS) team at the Janssen Pharmaceutical companies of Johnson & Johnson. \n* Notable contributors of this project were Geoffrey Kip, Yu Shih-Huah, Levon Demirdjian, Peter Shen, Asha Mahesh, Bethany Hyde, Yi Zhang, Matthew Richtmyer and Kristopher Standish.\n* The goal of the project was to use topic modeling to return relevant papers for specifically the subtasks focussing on \"What do we know about vaccines and therapeutics\". ","d0005073":"## Reference https:\/\/radimrehurek.com\/gensim\/auto_examples\/core\/run_similarity_queries.html","25bdd689":"### Topic model evaluation measures","e155c905":"![PyLDA%20davis%20topic%20models.JPG](attachment:PyLDA%20davis%20topic%20models.JPG)","8435b22b":"## Give the function a search string and see what articles it recommends and what the most semantically similar sentences are to the search string. For some recommended articles it returns some sentances with high similarity scores while other recommended articles have no semantically similar sentences to the task query."}}