{"cell_type":{"a4ad5512":"code","3aa03b0f":"code","13b3f37a":"code","914e0720":"code","233463e3":"code","0459f3a1":"code","c02b8307":"code","17524951":"code","9dbdeb24":"code","50a9deb7":"code","900d6c47":"code","08560664":"code","4951c657":"code","d6ee0cc2":"code","dff570b8":"code","12523b13":"code","7008b795":"code","b6485113":"code","c5302a3e":"code","c14a0fa1":"code","00b058c7":"code","223096fc":"code","2a1ec956":"code","2a123d85":"code","7a0c0afc":"code","17724158":"code","9057b133":"code","238457a2":"code","d7a2108c":"code","6346ec1a":"code","b52e4fa0":"code","5ea26b4a":"code","af772f6f":"code","06e0a41e":"code","7fbb77d9":"code","d91ce281":"code","2059121d":"code","4b3b9c0f":"code","95b56e5f":"code","4caef226":"code","12bd6cbd":"code","100e833e":"code","3ab0b192":"code","44a07113":"code","78e9194d":"code","bcce0571":"code","a356de22":"code","8224b420":"code","007b22e2":"code","bc7ed6b3":"code","925c7f9c":"code","1936ba63":"code","0434e638":"code","7ce1fe39":"code","9697fb86":"code","46432d38":"code","12df3d85":"code","1bb0ced9":"code","646faf6d":"code","f7db6f0f":"code","d6ace83a":"code","847ad535":"markdown","3ca539a0":"markdown","0a7d4af7":"markdown","4385c680":"markdown","17149742":"markdown","df1a8cb8":"markdown","d9e967aa":"markdown","b4160d57":"markdown","2e961c6d":"markdown","d37fe114":"markdown","01b6c4a0":"markdown","772cfbc6":"markdown","c1d782a7":"markdown","2d3748c7":"markdown","b1f4944b":"markdown","37f9d6a5":"markdown","b716c15b":"markdown","1eae4a9b":"markdown","0a4a0d28":"markdown","0f39766d":"markdown","c379c21b":"markdown","2b0e3000":"markdown","08c6b8e9":"markdown","a3cf26ca":"markdown","aefb2e4a":"markdown","7de7104f":"markdown","c702aa1e":"markdown","1f3c716f":"markdown","80c5755d":"markdown","503b73b1":"markdown","aa5b6c7c":"markdown","5c899de6":"markdown"},"source":{"a4ad5512":"!git clone https:\/\/github.com\/ayulockin\/SwAV-TF\n    \nimport sys\nsys.path.append('SwAV-TF\/utils')","3aa03b0f":"# Import SwAV methods\nimport multicrop_dataset\nimport architecture","13b3f37a":"import tensorflow as tf\nimport tensorflow_datasets as tfds\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport os\nimport gc\n\nfrom itertools import groupby\nfrom tqdm import tqdm","914e0720":"tf.random.set_seed(666)\nnp.random.seed(666)","233463e3":"gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Currently, memory growth needs to be the same across GPUs\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        # Memory growth must be set before GPUs have been initialized\n        print(e)","0459f3a1":"# Train Dataset\nTRAIN_PATH = \"..\/input\/unlabeled-stanford-dags-dataset\/Unlabeled_Stanford_Dogs_Dataset\/train\/\"\ntrain_images = os.listdir(TRAIN_PATH)\ntrain_df = pd.DataFrame(columns=[\"image_id\", \"image\"])\ntrain_df[\"image_id\"] = [file.split(\".\")[0] for file in train_images]\ntrain_df[\"image\"] = train_images\ntrain_df.head()","c02b8307":"# Test Dataset\nTEST_PATH = \"..\/input\/unlabeled-stanford-dags-dataset\/Unlabeled_Stanford_Dogs_Dataset\/test\/\"\ntest_images = os.listdir(TEST_PATH)\ntest_df = pd.DataFrame(columns=[\"image_id\", \"image\"])\ntest_df[\"image_id\"] = [file.split(\".\")[0] for file in test_images]\ntest_df[\"image\"] = test_images\ntest_df = test_df.iloc[:736]\ntest_df.head()","17524951":"print(f\"Nombre d'enregistrements de Train : {len(train_df)}\")\nprint(f\"Nombre d'enregistrements de Test : {len(test_df)}\")","9dbdeb24":"AUTO = tf.data.experimental.AUTOTUNE\n\n@tf.function\ndef parse_data(df_dict):\n    img = tf.io.read_file(TRAIN_PATH+df_dict['image'])\n    img = tf.image.decode_jpeg(img, channels=3)\n    image = {'image': img}\n    return image\n\n# Create Tensorflow Dataset\nvizloader = tf.data.Dataset.from_tensor_slices(dict(train_df))\n\nvizloader = (\n    vizloader\n    .map(parse_data, num_parallel_calls=AUTO)\n    .prefetch(AUTO)\n)","50a9deb7":"#Visualization\ndef show_batch(image_batch):\n    plt.figure(figsize=(20,20))\n    plt.suptitle(\"Visualisation des images du dataset Train\", \n             fontsize=22, color=\"#44546a\", y=0.92)\n    for n in range(25):\n        ax = plt.subplot(5,5,n+1)\n        plt.imshow(image_batch[n])\n        plt.axis('off')\n    \nimage_batch = []\nfor img in vizloader.take(25):\n    image_batch.append(img['image'])\n\nshow_batch(image_batch)","900d6c47":"# Configs\nBS = 16\n# Image sizes used to train the model.\nSIZE_CROPS = [224, 96]\n# Number of different augmentations of the same image\nNUM_CROPS = [2, 2]\n# Parameters for Random Resize Crop.\nMIN_SCALE = [0.5, 0.14] \nMAX_SCALE = [1., 0.5]\n\n# Experimental options\noptions = tf.data.Options()\noptions.experimental_optimization.noop_elimination = True\noptions.experimental_optimization.map_vectorization.enabled = True\noptions.experimental_optimization.apply_default_optimizations = True\noptions.experimental_deterministic = False\noptions.experimental_threading.max_intra_op_parallelism = 1","08560664":"# Get multiple data loaders\ntrainloaders = multicrop_dataset.get_multires_dataset(\n    vizloader,\n    size_crops=SIZE_CROPS,\n    num_crops=NUM_CROPS,\n    min_scale=MIN_SCALE,\n    max_scale=MAX_SCALE,\n    options=options)","4951c657":"# Prepare the final data loader\n# Zipping \ntrainloaders_zipped = tf.data.Dataset.zip(trainloaders)\n\n# Final trainloader\ntrainloaders_zipped = (\n    trainloaders_zipped\n    .batch(BS)\n    .prefetch(AUTO)\n)\n\nim1, im2, im3, im4 = next(iter(trainloaders_zipped))\nprint(im1.shape, im2.shape, im3.shape, im4.shape)","d6ee0cc2":"plt.figure(figsize=(20, 7))\nfor n in range(16):\n    ax = plt.subplot(2, 8, n+1)\n    plt.imshow(im1[n])\n    plt.axis('off')\nplt.suptitle(\"Some im1 224 x 224 px images created\", y=0.95,\n             fontsize=22, color=\"#44546a\")\nplt.tight_layout()\nplt.show()","dff570b8":"plt.figure(figsize=(20, 7))\nfor n in range(16):\n    ax = plt.subplot(2, 8, n+1)\n    plt.imshow(im2[n])\n    plt.axis('off')\nplt.suptitle(\"Some im2 224 x 224 px images created\", y=0.95,\n             fontsize=22, color=\"#44546a\")\nplt.tight_layout()\nplt.show()","12523b13":"plt.figure(figsize=(20, 7))\nfor n in range(16):\n    ax = plt.subplot(2, 8, n+1)\n    plt.imshow(im3[n])\n    plt.axis('off')\n    plt.suptitle(\"im3 random crop 96 x 96 px images created\", y=0.95,\n                 fontsize=22, color=\"#44546a\")\nplt.tight_layout()\nplt.show()","7008b795":"tf.keras.backend.clear_session()\nfeature_backbone = architecture.get_resnet_backbone()\nfeature_backbone.summary()\n\ndel feature_backbone\n_ = gc.collect()","b6485113":"tf.keras.backend.clear_session()\nprojection_prototype = architecture.get_projection_prototype(2048, 128, 256)\nprojection_prototype.summary()\n\ndel projection_prototype\n_ = gc.collect()","c5302a3e":"def sinkhorn(sample_prototype_batch):\n    Q = tf.transpose(tf.exp(sample_prototype_batch\/0.01))\n    Q \/= tf.keras.backend.sum(Q)\n    K, B = Q.shape\n\n    u = tf.zeros_like(K, dtype=tf.float32)\n    r = tf.ones_like(K, dtype=tf.float32) \/ K\n    c = tf.ones_like(B, dtype=tf.float32) \/ B\n\n    for _ in range(3):\n        u = tf.keras.backend.sum(Q, axis=1)\n        Q *= tf.expand_dims((r \/ u), axis=1)\n        Q *= tf.expand_dims(c \/ tf.keras.backend.sum(Q, axis=0), 0)\n\n    final_quantity = Q \/ tf.keras.backend.sum(Q, axis=0, keepdims=True)\n    final_quantity = tf.transpose(final_quantity)\n\n    return final_quantity","c14a0fa1":"# @tf.function\ndef train_step(input_views, feature_backbone, projection_prototype, \n               optimizer, crops_for_assign, temperature):\n    # ============ retrieve input data ... ============\n    im1, im2, im3, im4 = input_views \n    inputs = [im1, im2, im3, im4]\n    batch_size = inputs[0].shape[0]\n\n    # ============ create crop entries with same shape ... ============\n    crop_sizes = [inp.shape[1] for inp in inputs] # list of crop size of views\n    unique_consecutive_count = [len([elem for elem in g]) for _, g in groupby(crop_sizes)] # equivalent to torch.unique_consecutive\n    idx_crops = tf.cumsum(unique_consecutive_count)\n    \n    # ============ multi-res forward passes ... ============\n    start_idx = 0\n    with tf.GradientTape() as tape:\n        for end_idx in idx_crops:\n            concat_input = tf.stop_gradient(tf.concat(inputs[start_idx:end_idx], axis=0))\n            _embedding = feature_backbone(concat_input) # get embedding of same dim views together\n            if start_idx == 0:\n                embeddings = _embedding # for first iter\n            else:\n                embeddings = tf.concat((embeddings, _embedding), axis=0) # concat all the embeddings from all the views\n            start_idx = end_idx\n        \n        projection, prototype = projection_prototype(embeddings) # get normalized projection and prototype\n        projection = tf.stop_gradient(projection)\n\n        # ============ swav loss ... ============\n        # https:\/\/github.com\/facebookresearch\/swav\/issues\/19\n        loss = 0\n        for i, crop_id in enumerate(crops_for_assign): # crops_for_assign = [0,1]\n            with tape.stop_recording():\n                out = prototype[batch_size * crop_id: batch_size * (crop_id + 1)]\n                \n                # get assignments\n                q = sinkhorn(out) # sinkhorn is used for cluster assignment\n            \n            # cluster assignment prediction\n            subloss = 0\n            for v in np.delete(np.arange(np.sum(NUM_CROPS)), crop_id): # (for rest of the portions compute p and take cross entropy with q)\n                p = tf.nn.softmax(prototype[batch_size * v: batch_size * (v + 1)] \/ temperature) \n                subloss -= tf.math.reduce_mean(tf.math.reduce_sum(q * tf.math.log(p), axis=1))\n            loss += subloss \/ tf.cast((tf.reduce_sum(NUM_CROPS) - 1), tf.float32)\n        \n        loss \/= len(crops_for_assign)\n\n    # ============ backprop ... ============\n    variables = feature_backbone.trainable_variables + projection_prototype.trainable_variables\n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))\n\n    return loss","00b058c7":"# ============ Train loop ============\ndef train_swav(feature_backbone, \n               projection_prototype, \n               dataloader, \n               optimizer, \n               crops_for_assign,\n               temperature, \n               epochs=50):\n  \n    step_wise_loss = []\n    epoch_wise_loss = []\n    \n    for epoch in range(epochs):\n        w = projection_prototype.get_layer('prototype').get_weights()\n        w = tf.transpose(w)\n        w = tf.math.l2_normalize(w, axis=1)\n        projection_prototype.get_layer('prototype').set_weights(tf.transpose(w))\n\n        for step, inputs in enumerate(dataloader):\n            loss = train_step(inputs, feature_backbone, projection_prototype, \n                              optimizer, crops_for_assign, temperature)\n\n            if step%500==0:\n                print(f'[{step}|{len(trainloaders_zipped)}] loss: {loss}')\n            \n            step_wise_loss.append(loss)\n        epoch_wise_loss.append(np.mean(step_wise_loss))\n\n        print(\"epoch: {} epoch-wise loss: {:.3f}\".format(epoch + 1, np.mean(step_wise_loss)))\n\n    return epoch_wise_loss, [feature_backbone, projection_prototype]","223096fc":"# ============ re-initialize the networks and the optimizer ... ============\ntf.keras.backend.clear_session()\nfeature_backbone = architecture.get_resnet_backbone()\nprojection_prototype = architecture.get_projection_prototype(dense_2=128, prototype_dimension=256)\n\ndecay_steps = 1000\nlr_decayed_fn = tf.keras.experimental.CosineDecay(\n    initial_learning_rate=0.1, decay_steps=decay_steps)\nopt = tf.keras.optimizers.SGD(learning_rate=lr_decayed_fn)\n\n# ======================= train  ===========================\nepoch_wise_loss, models = train_swav(feature_backbone, \n    projection_prototype, \n    trainloaders_zipped, \n    opt,\n    crops_for_assign=[0, 1],\n    temperature=0.1, \n    epochs=10\n)","2a1ec956":"plt.style.use(\"seaborn-whitegrid\")\nfig = plt.figure(figsize=(10,6))\nplt.plot(epoch_wise_loss, color=\"#fe7a00\",\n         marker=\"^\")\nplt.title(\"Train loss of SwAV over epochs\",\n          fontsize=18, color=\"#44546a\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.grid(linestyle='--')\nplt.show()","2a123d85":"# Serialize the models\nself_feature_backbone, self_projection_prototype = models\nself_feature_backbone.save('self_feature_backbone_10_epochs.h5')\nself_projection_prototype.save('self_projection_prototype_10_epochs.h5')\nself_feature_backbone.save_weights('self_feature_backbone_weights.h5')\nself_projection_prototype.save_weights('self_projection_prototype_weights.h5')","7a0c0afc":"# Load base ResNet model from Keras Application\nbase_model = tf.keras.applications.ResNet50(\n    include_top=False,\n    weights=\"imagenet\",\n    input_shape=(224,224,3),\n    pooling=\"avg\"\n)\n\n# Don't retrain the layers\nfor layer in base_model.layers:\n    layer.trainable = False","17724158":"def get_model(base_model):\n    base_output = base_model.output\n    base_output = tf.keras.layers.Dense(128, activation='relu')(base_output)\n    base_output = tf.keras.layers.Dropout(0.2)(base_output)\n    predictions = tf.keras.layers.Dense(15, activation='softmax')(base_output)\n    my_model = tf.keras.models.Model(\n        inputs=base_model.input,\n        outputs=predictions)\n    my_model.compile(optimizer=\"adam\",\n                     loss=\"categorical_crossentropy\",\n                     metrics=[\"accuracy\"])\n    return my_model","9057b133":"# Define base ResNet Model with Classifier\nbase_resnet_model = get_model(base_model)","238457a2":"train_df[\"cat_id\"] = train_df[\"image_id\"].apply(lambda x: x.split(\"_\")[0])\nbreeds_df = pd.read_csv(\n    '..\/input\/unlabeled-stanford-dags-dataset\/Unlabeled_Stanford_Dogs_Dataset\/list_breeds.csv', \n    sep=';')\ntrain_df = pd.merge(train_df, breeds_df, how=\"left\", left_on=\"cat_id\", right_on=\"Id\")\ntrain_df.drop([\"cat_id\",\"Id\"], axis=1, inplace=True)\ntrain_df.head()","d7a2108c":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.resnet.preprocess_input,\n    validation_split=0.2)\ntest_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.resnet.preprocess_input)","6346ec1a":"train_generator = datagen.flow_from_dataframe(\n    dataframe=train_df,\n    directory=TRAIN_PATH,\n    x_col=\"image\",\n    y_col=\"Breed\",\n    subset=\"training\",\n    batch_size=32,\n    seed=42,\n    shuffle=True,\n    class_mode=\"categorical\")\n\nvalid_generator=datagen.flow_from_dataframe(\n    dataframe=train_df,\n    directory=TRAIN_PATH,\n    x_col=\"image\",\n    y_col=\"Breed\",\n    subset=\"validation\",\n    batch_size=32,\n    seed=42,\n    shuffle=True,\n    class_mode=\"categorical\")\n\ntest_generator=test_datagen.flow_from_dataframe(\n    dataframe=test_df,\n    directory=TEST_PATH,\n    x_col=\"image\",\n    y_col=None,\n    batch_size=32,\n    shuffle=False,\n    seed=42,\n    class_mode=None)","b52e4fa0":"tf.keras.backend.clear_session()\n\nSTEP_SIZE_TRAIN = train_generator.n\/\/train_generator.batch_size\nSTEP_SIZE_VALID = valid_generator.n\/\/valid_generator.batch_size\nSTEP_SIZE_TEST = test_generator.n\/\/test_generator.batch_size\n\nbase_history = base_resnet_model.fit(\n    train_generator,\n    steps_per_epoch=STEP_SIZE_TRAIN,\n    validation_data=valid_generator,\n    validation_steps=STEP_SIZE_VALID,\n    epochs=10\n)","5ea26b4a":"fig, ax = plt.subplots(1, 2, figsize=(20, 7))\nax = ax.ravel()\n\nfor i, metric in enumerate([\"accuracy\",\"loss\"]):\n    ax[i].plot(base_history.history[metric], \n               color=\"green\", linestyle=\"-.\")\n    ax[i].plot(base_history.history[\"val_\" + metric], \n               color=\"green\")\n    ax[i].set_title(\"Base resNet Model {}\".format(metric))\n    ax[i].set_xlabel(\"epochs\")\n    ax[i].set_ylabel(metric)\n    ax[i].grid(linestyle='--')\n    ax[i].legend([\"train\", \"val\"])\n\nplt.suptitle(\"Metrics of base ResNet on 15 dogs breeds\", \n             fontsize=18, color=\"#44546a\", y=0.99)\nplt.show()","af772f6f":"base_resnet_model.evaluate(valid_generator,\nsteps=STEP_SIZE_TEST)","06e0a41e":"test_generator.reset()\npred = base_resnet_model.predict(\n    test_generator,\n    steps=STEP_SIZE_TEST,\n    verbose=1)","7fbb77d9":"predicted_class_indices = np.argmax(pred,axis=1)\nlabels = (train_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]\ntest_df[\"ResNet_predict\"] = predictions\ntest_df.head()","d91ce281":"train_df = train_df.sample(frac=1)\ntrain_ds = train_df.sample(frac=0.1)\nextra_train_ds = train_df.drop(train_ds.index, axis=0)\n\nprint(f\"Size of labelized data: {len(train_ds)}\")\nprint(f\"Size of non-labelized data: {len(extra_train_ds)}\")","2059121d":"train_ds.groupby(\"Breed\").agg({\"image\":\"count\"})","4b3b9c0f":"swav_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1.\/255,\n    validation_split=0.2)\nswav_test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1.\/255)\n\nswav_train_generator = swav_datagen.flow_from_dataframe(\n    dataframe=train_ds,\n    directory=TRAIN_PATH,\n    x_col=\"image\",\n    y_col=\"Breed\",\n    subset=\"training\",\n    batch_size=16,\n    seed=42,\n    shuffle=True,\n    class_mode=\"categorical\",\n    target_size=(224,224))\n\nswav_valid_generator = swav_datagen.flow_from_dataframe(\n    dataframe=extra_train_ds,\n    directory=TRAIN_PATH,\n    x_col=\"image\",\n    y_col=\"Breed\",\n    subset=\"validation\",\n    batch_size=16,\n    seed=42,\n    shuffle=True,\n    class_mode=\"categorical\",\n    target_size=(224,224))\n\nswav_test_generator=swav_test_datagen.flow_from_dataframe(\n    dataframe=test_df,\n    directory=TEST_PATH,\n    x_col=\"image\",\n    y_col=None,\n    batch_size=16,\n    shuffle=False,\n    seed=42,\n    class_mode=None,\n    target_size=(224,224))","95b56e5f":"def get_linear_classifier(\n    model_weights_path='.\/self_feature_backbone_weights.h5'):\n    # input placeholder\n    inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n    # get swav baseline model architecture\n    feature_backbone = architecture.get_resnet_backbone()\n    # load trained weights\n    feature_backbone.load_weights(model_weights_path)\n    feature_backbone.trainable = False\n\n    x = feature_backbone(inputs, training=False)\n    outputs = tf.keras.layers.Dense(15, activation=\"softmax\",\n                                    kernel_regularizer=tf.keras.regularizers.L2(1e-6),\n                                    name=\"last_dense\")(x)\n    linear_model = tf.keras.models.Model(inputs, outputs)\n\n    return linear_model","4caef226":"swav_model = get_linear_classifier()\nswav_model.summary()","12bd6cbd":"# Early Stopping to prevent overfitting\nearly_stopper = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\", \n    patience=15, \n    verbose=2, \n    restore_best_weights=True)","100e833e":"swav_model.compile(\n    loss=\"categorical_crossentropy\", \n    metrics=[\"accuracy\"],\n    optimizer=\"adam\")","3ab0b192":"STEP_SIZE_TRAIN = swav_train_generator.n\/\/swav_train_generator.batch_size\nSTEP_SIZE_VALID = swav_valid_generator.n\/\/swav_valid_generator.batch_size\nSTEP_SIZE_TEST = swav_test_generator.n\/\/swav_test_generator.batch_size\n\nswav_history = swav_model.fit(\n    swav_train_generator,\n    steps_per_epoch=STEP_SIZE_TRAIN,\n    validation_data=swav_valid_generator,\n    validation_steps=STEP_SIZE_VALID,\n    epochs=100,\n    callbacks=[early_stopper]\n)","44a07113":"# Save the model for Fine-tuning\nswav_model.save('warmup.h5')","78e9194d":"fig, ax = plt.subplots(1, 2, figsize=(20, 7))\nax = ax.ravel()\n\nfor i, metric in enumerate([\"accuracy\",\"loss\"]):\n    ax[i].plot(swav_history.history[metric], \n               color=\"#fe7a00\", linestyle=\"-.\")\n    ax[i].plot(swav_history.history[\"val_\" + metric], \n               color=\"#fe7a00\")\n    ax[i].set_title(\"SwAV pre-trained Model {}\".format(metric))\n    ax[i].set_xlabel(\"epochs\")\n    ax[i].set_ylabel(metric)\n    ax[i].grid(linestyle='--')\n    ax[i].legend([\"train\", \"val\"])\n\nplt.suptitle(\"Metrics of SwAV pre-trained (10 epochs) on 15 dogs breeds\", \n             fontsize=18, color=\"#44546a\", y=0.99)\nplt.show()","bcce0571":"fig = plt.figure(figsize=(12,7))\nplt.plot(swav_history.history[\"accuracy\"],\n         color=\"#fe7a00\", linestyle=\"-.\",\n         label=\"SwAV Accuracy\"\n        )\nplt.plot(swav_history.history[\"val_accuracy\"],\n         color=\"#fe7a00\", marker=\"^\",\n         label=\"Val SwAV Accuracy\")\nplt.plot(base_history.history[\"accuracy\"],\n         color=\"green\", linestyle=\"-.\",\n         label=\"ResNet50 Accuracy\")\nplt.plot(base_history.history[\"val_accuracy\"],\n         color=\"green\", marker=\"o\",\n         label=\"Val ResNet50 Accuracy\")\nplt.title(\"Accuracy models compare on 15 dogs breeds\",\n          fontsize=18, color=\"#44546a\")\nplt.legend()\nplt.show()","a356de22":"tf.keras.backend.clear_session()\nswav_50_model = get_linear_classifier(\n    model_weights_path='..\/input\/unlabeled-stanford-dags-dataset\/self_feature_backbone_50_weights.h5')\nswav_50_model.summary()","8224b420":"swav_50_model.compile(\n    loss=\"categorical_crossentropy\", \n    metrics=[\"accuracy\"],\n    optimizer=\"adam\")","007b22e2":"swav_50_history = swav_50_model.fit(\n    swav_train_generator,\n    steps_per_epoch=STEP_SIZE_TRAIN,\n    validation_data=swav_valid_generator,\n    validation_steps=STEP_SIZE_VALID,\n    epochs=100,\n    callbacks=[early_stopper]\n)","bc7ed6b3":"fig, ax = plt.subplots(1, 2, figsize=(20, 7))\nax = ax.ravel()\n\nfor i, metric in enumerate([\"accuracy\",\"loss\"]):\n    ax[i].plot(swav_history.history[metric], \n               color=\"#fe7a00\", linestyle=\"-.\",\n               label=\"Train 10 epochs SwAV\")\n    ax[i].plot(swav_history.history[\"val_\" + metric], \n               color=\"#fe7a00\",\n               label=\"Val 10 epochs SwAV\")\n    ax[i].plot(swav_50_history.history[metric], \n               color=\"red\", linestyle=\"-.\",\n               marker=\"^\",\n               label=\"Train 50 epochs SwAV\")\n    ax[i].plot(swav_50_history.history[\"val_\" + metric], \n               color=\"red\", marker=\"^\",\n               label=\"Val 50 epochs SwAV\")\n    ax[i].set_title(\"SwAV pre-trained Model {}\".format(metric))\n    ax[i].set_xlabel(\"epochs\")\n    ax[i].set_ylabel(metric)\n    ax[i].grid(linestyle='--')\n    ax[i].legend()\n\nplt.suptitle(\"Metrics of SwAV pre-trained (10 and 50 epochs) on 15 dogs breeds\", \n             fontsize=18, color=\"#44546a\", y=0.99)\nplt.show()","925c7f9c":"def get_fine_classifier():\n    # input placeholder\n    inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n    # get swav baseline model architecture\n    feature_backbone = architecture.get_resnet_backbone()\n    # load trained weights\n    feature_backbone.load_weights('.\/self_feature_backbone_weights.h5')\n    feature_backbone.trainable = True\n\n    # load warmup model\n    warmup_model = tf.keras.models.load_model('.\/warmup.h5')\n    # get trained output layer\n    last_layer = warmup_model.get_layer('last_dense')\n\n    \n    x = feature_backbone(inputs, training=False)\n    outputs = last_layer(x)\n    linear_model = tf.keras.models.Model(inputs, outputs)\n\n    return linear_model","1936ba63":"# get model and compile\ntf.keras.backend.clear_session()\nfull_trainable_swav = get_fine_classifier()\nfull_trainable_swav.summary()","0434e638":"full_trainable_swav.compile(loss=\"categorical_crossentropy\", \n                            metrics=[\"accuracy\"],\n                            optimizer='adam')","7ce1fe39":"swav_fine_history = full_trainable_swav.fit(\n    swav_train_generator,\n    steps_per_epoch=STEP_SIZE_TRAIN,\n    validation_data=swav_valid_generator,\n    validation_steps=STEP_SIZE_VALID,\n    epochs=100,\n    callbacks=[early_stopper])","9697fb86":"fig, ax = plt.subplots(1, 2, figsize=(20, 7))\nax = ax.ravel()\n\nfor i, metric in enumerate([\"accuracy\",\"loss\"]):\n    ax[i].plot(swav_history.history[metric], \n               color=\"#fe7a00\", linestyle=\"-.\",\n               label=\"Train 10 epochs SwAV\")\n    ax[i].plot(swav_history.history[\"val_\" + metric], \n               color=\"#fe7a00\",\n               label=\"Val 10 epochs SwAV\")\n    ax[i].plot(swav_50_history.history[metric], \n               color=\"red\", linestyle=\"-.\",\n               label=\"Train 50 epochs SwAV\")\n    ax[i].plot(swav_50_history.history[\"val_\" + metric], \n               color=\"red\",\n               label=\"Val 50 epochs SwAV\")\n    ax[i].plot(swav_fine_history.history[metric], \n               color=\"blue\", linestyle=\"-.\",\n               marker=\"^\",\n               label=\"Train Fine-tuned SwAV\")\n    ax[i].plot(swav_fine_history.history[\"val_\" + metric], \n               color=\"blue\", marker=\"^\",\n               label=\"Val Fine-tuned SwAV\")\n    ax[i].set_title(\"SwAV pre-trained Model {}\".format(metric))\n    ax[i].set_xlabel(\"epochs\")\n    ax[i].set_ylabel(metric)\n    ax[i].grid(linestyle='--')\n    ax[i].legend()\n\nplt.suptitle(\"Metrics of SwAV pre-trained (10 and 50 epochs) and Fine-tuning\", \n             fontsize=18, color=\"#44546a\", y=0.99)\nplt.show()","46432d38":"fig = plt.figure(figsize=(12,7))\nplt.plot(swav_fine_history.history[\"accuracy\"],\n         color=\"blue\", linestyle=\"-.\",\n         label=\"SwAV Fine-tuned Accuracy\"\n        )\nplt.plot(swav_fine_history.history[\"val_accuracy\"],\n         color=\"blue\", marker=\"^\",\n         label=\"Val SwAV Fine-tuned Accuracy\")\nplt.plot(base_history.history[\"accuracy\"],\n         color=\"green\", linestyle=\"-.\",\n         label=\"ResNet50 Accuracy\")\nplt.plot(base_history.history[\"val_accuracy\"],\n         color=\"green\", marker=\"o\",\n         label=\"Val ResNet50 Accuracy\")\nplt.title(\"Accuracy models compare on 15 dogs breeds\",\n          fontsize=18, color=\"#44546a\")\nplt.legend()\nplt.show()","12df3d85":"full_trainable_swav.evaluate(swav_valid_generator,\nsteps=STEP_SIZE_TEST)","1bb0ced9":"swav_test_generator.reset()\nswav_pred = full_trainable_swav.predict(\n    swav_test_generator,\n    steps=STEP_SIZE_TEST,\n    verbose=1)","646faf6d":"swav_predicted_class_indices = np.argmax(swav_pred,axis=1)\nswav_labels = (swav_train_generator.class_indices)\nswav_labels = dict((v,k) for k,v in swav_labels.items())\nswav_predictions = [swav_labels[k] for k in swav_predicted_class_indices]\ntest_df[\"SwAV_predict\"] = swav_predictions\ntest_df.head()","f7db6f0f":"test_df[\"cat_id\"] = test_df[\"image_id\"].apply(lambda x: x.split(\"_\")[0])\ntest_df = pd.merge(test_df, breeds_df, how=\"left\", left_on=\"cat_id\", right_on=\"Id\")\ntest_df.drop([\"cat_id\",\"Id\"], axis=1, inplace=True)\ntest_df.head()","d6ace83a":"conf_base = pd.crosstab(test_df[\"Breed\"], test_df[\"ResNet_predict\"])\nconf_swav = pd.crosstab(test_df[\"Breed\"], test_df[\"SwAV_predict\"])\n\nfig, ax = plt.subplots(1, 2, figsize=(25, 8))\nax = plt.subplot(1,2,1)\nax = sns.heatmap(conf_base, annot=True, cmap=\"Greens\")\nax.set_title(\"Base ResNet50\")\n\nax2 = plt.subplot(1,2,2)\nax2 = sns.heatmap(conf_swav, annot=True, cmap=\"Oranges\")\nax2.set_title(\"Base ResNet50 SwAV\")\n\nplt.suptitle(\"Confusion Matrix for ResNet50 predictions\",\n          fontsize=18, color=\"#44546a\", y=0.96)\nplt.show()","847ad535":"### Test avec 50 \u00e9poques de SwAV","3ca539a0":"Les pr\u00e9dictions du mod\u00e8le ResNet50 classique sont ici bien meilleures que les pr\u00e9dictions SwAV Fine-tuned avec 10% de donn\u00e9es lab\u00e9lis\u00e9es. \nCela peut s'expliquer par la faible volum\u00e9trie du jeu d'entra\u00eenement utilis\u00e9 pour le clustering du SwAV. Le papier mentionne l'efficacit\u00e9 de l'algorithme sur de faibles volumes de donn\u00e9es mais en utilisant leur mod\u00e8le pr\u00e9-entra\u00een\u00e9 (800 \u00e9poques sur plus de 20 000 images ImageNet).\nIci, on s'apper\u00e7ois donc de la limite de l'utilisation du Self-supervised Learning sur un petit jeu de donn\u00e9es.","0a7d4af7":"# <span style=\"color:#44546a; font-variant:small-caps;\" id=\"section_6\">Analyse des r\u00e9sultats sur le jeu de test<\/span>\n\nNous allons affecter les valeurs r\u00e9\u00e9lles des races de chien sur notre jeu de test, puis les comparer avec les r\u00e9sulats obtenus des diff\u00e9rentes mod\u00e9lisations : ResNet50 et SwAV_ResNet50 avec les poids d'entra\u00eenement de notre jeu de donn\u00e9es initial.","4385c680":"## <span id=\"section_2_2\">Cr\u00e9ation des Datasets de Train et Test<\/span>\nIci, nous allons charger le dataset Stanford Dogs*[2]* bas\u00e9 sur les **15 races** pr\u00e9f\u00e9r\u00e9es des Fran\u00e7ais en mode **non-supervis\u00e9**.","17149742":"# <span style=\"color:#44546a; font-variant:small-caps;\" id=\"section_5\">Etude comparative avec une m\u00e9thode supervis\u00e9e<\/span>\n\nL'\u00e9tude comparative vise \u00e0 analyser un mod\u00e8le ResNet-50 entrain\u00e9 \u00e0 l'aide de la m\u00e9thode de classification d'images conventionnelle et un mod\u00e8le ResNet-50 pr\u00e9-entrain\u00e9 \u00e0 l'aide de la m\u00e9thode SwAV.\n\n## <span id=\"section_5_1\">Mod\u00e8le ResNet50 supervis\u00e9 *(baseline)*<\/span>\n","df1a8cb8":"### Visualisation d'exemples d'images de Train","d9e967aa":"## <span id=\"section_5_2\">Mod\u00e8le ResNet50 SwAV pr\u00e9-entra\u00een\u00e9<\/span>\n\nDans cette partie, nous allons utiliser, comme le pr\u00e9conise le papier, **10% de donn\u00e9es lab\u00e9lis\u00e9es**.\nNous allons donc modifier les datasets en ce sens :","b4160d57":"### G\u00e9n\u00e9rateurs Keras","2e961c6d":"### Pr\u00e9dictions sur le jeu de test avec SwAV","d37fe114":"### Entra\u00eenement du mod\u00e8le ResNet50","01b6c4a0":"Dans ce Notebook, nous allons impl\u00e9menter la m\u00e9hode SwAV sur la classification de races de chien avec des donn\u00e9es non-lab\u00e9lis\u00e9es, simplement r\u00e9parties dans les dossiers test et train.\nAfin de pouvoir comparer les r\u00e9sultats et analyser la performance de l'algorithme, nous utiliserons un mod\u00e8le pr\u00e9-entrain\u00e9 sur ces m\u00eames donn\u00e9es d'un pr\u00e9c\u00e9dent Notebook : [computer-vision-cnn-stanford-dogs-dataset](https:\/\/www.kaggle.com\/michaelfumery\/computer-vision-cnn-stanford-dogs-dataset). Nous avions alors utiliser le transfert-learning d'un mod\u00e8le Xception avec Fine-tuning et optimisation des hyperparam\u00e8tres du classifier.\n\n# <span style=\"color:#44546a; font-variant:small-caps;\">Sommaire<\/span>\n1. [Unsupervised Learning of Visual Features by Contrasting Cluster Assignments](#section_1)      \n    1.1. [Contexte](#section_1_1)       \n    1.2. [Qu'est ce que la m\u00e9thode SwAV ?](#section_1_2)       \n2. [Importations et configurations de base pour notre cas](#section_2)      \n    2.1. [Chargement des librairies externes](#section_2_1)       \n    2.2. [Cr\u00e9ation des Datasets de Train et Test](#section_2_2)      \n3. [Preprocessing et Data augmentation : Multi-crop](#section_3)      \n4. [Architecture du mod\u00e8le ResNet - SwAV](#section_4)       \n    4.1. [Chargement des mod\u00e8les de base](#section_4_1)       \n    4.2. [Sinkhorn Knopp pour l'affectation de cluster](#section_4_2)       \n    4.3. [Les \u00e9tapes d'entrainement du mod\u00e8le](#section_4_3)       \n5. [Etude comparative avec une m\u00e9thode supervis\u00e9e](#section_5)      \n    5.1. [Mod\u00e8le ResNet50 supervis\u00e9](#section_5_1)      \n    5.2. [Mod\u00e8le ResNet50 SwAV pr\u00e9-entra\u00een\u00e9](#section_5_2)      \n6. [Analyse des r\u00e9sultats sur le jeu de test](#section_6)","772cfbc6":"On remarque ici que la m\u00e9trique Loss ne s'am\u00e9liore plus vraiment apr\u00e8s 20 \u00e9poques.","c1d782a7":"On remarque ici que le SwAV entra\u00een\u00e9 sur 50 \u00e9poques n'offre pas de meilleurs r\u00e9sultats que celui entra\u00een\u00e9 sur 10 \u00e9poques seulement *(pour notre jeu de donn\u00e9es)*. Nous utiliserons donc le SwAV_10 pour la suite des tests.","2d3748c7":"## <span id=\"section_4_4\">Sauvegarde des poids entra\u00een\u00e9s avec TensorFlow<\/span>","b1f4944b":"Nous avons \u00e9galement r\u00e9alis\u00e9 l'entrainement du SwAV sur le Stanford Dogs Dataset *(15 races de chiens)* sur **50 \u00e9poques**. Les poids obtenus sont disponible dans les data \u00e0 disposition du Notebook :\n\n| ![loss_50_epochs](http:\/\/www.mf-data-science.fr\/images\/projects\/loss_50_epochs.png) | \n|:--:| \n| *Figure 6 : R\u00e9sultats de la m\u00e9trique Loss du SwAV sur 50 \u00e9poques.* |","37f9d6a5":"[1] : *Unsupervised Learning of Visual Features by Contrasting Cluster Assignments* : Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin | [arXiv:2006.09882](https:\/\/arxiv.org\/abs\/2006.09882)       \n[2] : @inproceedings{KhoslaYaoJayadevaprakashFeiFei_FGVC2011,\nauthor = \"Aditya Khosla and Nityananda Jayadevaprakash and Bangpeng Yao and\n          Li Fei-Fei\",\ntitle = \"Novel Dataset for Fine-Grained Image Categorization\",\nbooktitle = \"First Workshop on Fine-Grained Visual Categorization,\n             IEEE Conference on Computer Vision and Pattern Recognition\",\nyear = \"2011\",\nmonth = \"June\",\naddress = \"Colorado Springs, CO\",\n}","b716c15b":"La r\u00e9partition est plut\u00f4t \u00e9quitable, nous pouvons donc l'utiliser dans les g\u00e9n\u00e9rateurs Keras.","1eae4a9b":"# <span style=\"color:#44546a; font-variant:small-caps;\" id=\"section_2\">Importations et configurations de base pour notre cas<\/span>\n\n## <span id=\"section_2_1\">Chargement des librairies externes<\/span>","0a4a0d28":"On remarque donc ici que nos diff\u00e9rents crops sont bien cr\u00e9\u00e9s avec les tailles d\u00e9sir\u00e9es. Nous pouvons \u00e0 pr\u00e9sent **visualiser quelques r\u00e9sultats** :","0f39766d":"### Pr\u00e9dictions sur le jeu de Test avec ResNet50","c379c21b":"## <span id=\"section_1_2\">Qu'est ce que la m\u00e9thode SwAV ?<\/span>\n\n| ![Swav_2](http:\/\/www.mf-data-science.fr\/images\/projects\/Swav-architecture.png) | \n|:--:| \n| *Figure 2 : Aper\u00e7u g\u00e9n\u00e9ral de SwAV* |\n\nLes auteurs de cet article se sont pench\u00e9s sur une question :\n\n> *Pouvons-nous apprendre une m\u00e9trique significative qui refl\u00e8te la similitude apparente entre les instances via un apprentissage discriminant pur\u00a0?*\n\nPour r\u00e9pondre \u00e0 cela, ils ont con\u00e7u un nouvel algorithme d'apprentissage de caract\u00e9ristiques non supervis\u00e9 appel\u00e9 instance-level discrimination. Ici, chaque image et ses transformations *(vues)* sont trait\u00e9es comme deux instances distinctes. Chaque instance d'image est trait\u00e9e comme une classe distincte. L'objectif est d'apprendre une int\u00e9gration, un mapping $x$ *(image)* vers $v$ *(feature)* de telle sorte que les instances *(images)* s\u00e9mantiquement similaires soient plus proches dans l'espace d'int\u00e9gration.\n\n| ![SWaV-anim.gif](http:\/\/www.mf-data-science.fr\/images\/projects\/SWaV-anim.gif) | \n|:--:| \n| *Figure 3 : Representation of SwAV method* |\n\nDans ce Notebook, nous allons impl\u00e9menter la m\u00e9hode SwAV sur la classification de races de chien avec des donn\u00e9es non-lab\u00e9lis\u00e9es, simplement r\u00e9parties dans les dossiers test et train.\nAfin de pouvoir comparer les r\u00e9sultats et analyser la performance de l'algorithme, nous utiliserons un mod\u00e8le pr\u00e9-entrain\u00e9 sur ces m\u00eames donn\u00e9es d'un pr\u00e9c\u00e9dent Notebook : [computer-vision-cnn-stanford-dogs-dataset](https:\/\/www.kaggle.com\/michaelfumery\/computer-vision-cnn-stanford-dogs-dataset). Nous avions alors utiliser le transfert-learning d'un mod\u00e8le Xception avec Fine-tuning et optimisation des hyperparam\u00e8tres du classifier.","2b0e3000":"### Fine-Tuning du mod\u00e8le SwAV","08c6b8e9":"### Evaluation du mod\u00e8le SwAV","a3cf26ca":"# <span style=\"color:#44546a; font-variant:small-caps;\" id=\"section_4\">Architecture du mod\u00e8le ResNet - SwAV<\/span>\n\nDans ce papier, le mod\u00e8le de base utilis\u00e9 est un ResNet50. 2 approches sont \u00e9tudi\u00e9es :\n\n- Une classification lin\u00e9aire sur les features d\u00e9tect\u00e9es,\n- et une approche semi-supervis\u00e9e par fine-tuning en utilisant quelques labels (de 1% \u00e0 10% de donn\u00e9es lab\u00e9lis\u00e9es).\n\n## <span id=\"section_4_1\">Chargement des mod\u00e8les de base<\/span>","aefb2e4a":"<h1 style=\"text-align:center !important; font-size:32px; color:#44546a; font-variant:small-caps;\">Application de l'algorithme SwAV et du Self-supervised Learning pour la classififcation de races de chien sur des images non-labelis\u00e9es.<\/h1>\n\n**SwAV *(Swapping Assignments between Views)*** est une m\u00e9thode r\u00e9cente pour le pr\u00e9-entrainement des r\u00e9seaux de neurones \u00e0 convolution **sans utiliser d'annotations**. Cette m\u00e9thode, expos\u00e9e dans le papier [Unsupervised Learning of Visual Features by Contrasting Cluster Assignments](https:\/\/arxiv.org\/abs\/2006.09882)[1], est publi\u00e9e en Juin 2020 par une \u00e9quipe d\u00e9di\u00e9e Facebook AI Research.\n\nTout comme les approches contrastives *(comme SimCLR)*, SwAV apprend les repr\u00e9sentations en comparant les transformations d'une image, mais contrairement aux m\u00e9thodes contrastives, il ne n\u00e9cessite pas de calculer des comparaisons par paires de caract\u00e9ristiques. \n\nCela rend ce framework plus efficace car il ne n\u00e9cessite pas une grande banque de m\u00e9moire ou un r\u00e9seau de dynamique auxiliaire. Plus pr\u00e9cis\u00e9ment, **SwAV regroupe simultan\u00e9ment les donn\u00e9es tout en assurant la coh\u00e9rence entre les affectations de cluster produites pour diff\u00e9rentes augmentations (ou \u00ab\u00a0vues\u00a0\u00bb) de la m\u00eame image**, au lieu de comparer directement les caract\u00e9ristiques. \n\nEn d'autres termes, un m\u00e9canisme de pr\u00e9diction \u00ab\u00a0interverti\u00a0\u00bb est utilis\u00e9 o\u00f9 la pr\u00e9diction de l'affectation de cluster d'une vue est r\u00e9alis\u00e9e \u00e0 partir de la repr\u00e9sentation d'une autre vue.","7de7104f":"## <span id=\"section_4_2\">Sinkhorn Knopp pour l'affectation de cluster<\/span>\n\nAffectation de cluster en ligne et configuration du probl\u00e8me de swapped prediction. L'id\u00e9e de pr\u00e9diction permut\u00e9e *(swapped prediction)* est que **le code d'une vue d'une image est pr\u00e9dit \u00e0 partir de la repr\u00e9sentation d'une autre vue de la m\u00eame image**.\n\n| ![data-augmentation](http:\/\/www.mf-data-science.fr\/images\/projects\/Swav-swapped-Prediction.png) | \n|:--:| \n| *Figure 5 : Affectation de cluster suivie d'une pr\u00e9diction permut\u00e9e.* |","c702aa1e":"### Cr\u00e9ation des datasets supervis\u00e9s","1f3c716f":"Nous allons \u00e9galement v\u00e9rifier la r\u00e9partition des diff\u00e9rentes races dans le jeu de donn\u00e9es 10% lab\u00e9lis\u00e9 pour contr\u00f4ler qu'il n'y a pas de race trop pr\u00e9pond\u00e9rante :","80c5755d":"### Evaluation du mod\u00e8le baseline ResNet50","503b73b1":"# <span style=\"color:#44546a; font-variant:small-caps;\" id=\"section_3\">Preprocessing et Data augmentation : Multi-crop<\/span>\n\nDans le papier de l'\u00e9quipe Facebook AI Research, le type de preprocessesing et de data augmentation a utiliser est tr\u00e8s important pour que l'algorithme donne les r\u00e9sultats attendus. Il s'agit d'augmentation multi-crop que nous allons d\u00e9tailler ci-dessous :\n\n| ![data-augmentation](http:\/\/www.mf-data-science.fr\/images\/projects\/data-augmentation.jpg) | \n|:--:| \n| *Figure 4 : Multi-crop = l'image $x_n$ est transform\u00e9e en $V+2$ vues : deux vues globales et $V$ vues de petite r\u00e9solution zoom\u00e9es.* |\n\n- Nous obtenons deux vues diff\u00e9rentes \u00e0 partir d'une image en effectuant des **recadrages de tailles et de proportions al\u00e9atoires**. pour cela, la m\u00e9thode ***RandomResizedCrop*** du module `torchvision.transforms` de PyTorch est utilis\u00e9e avec les param\u00e8tres de mise \u00e0 l'\u00e9chelle suivants\u00a0: `s=(0.14, 1)`. \n\n- Ces deux vues sont redimensionn\u00e9es en pleine r\u00e9solution \u00e0 224 x 224 pixels.\n\n- Cr\u00e9ation de $V$ vues suppl\u00e9mentaires en recadrant de petites parties dans l'image. Pour ce faire, nous utilisons les param\u00e8tres RandomResizedCrop suivants\u00a0: `s=(0.5,0,14)`. Nous redimensionnons les recadrages r\u00e9sultants \u00e0 une r\u00e9solution de 96 x 96. \n\n- Enfin, nous appliquons au hasard des retournements horizontaux, distorsion des couleurs et flou gaussien \u00e0 chaque recadrage r\u00e9sultant.","aa5b6c7c":"# <span style=\"color:#44546a; font-variant:small-caps;\" id=\"section_1\">Unsupervised Learning of Visual Features by Contrasting Cluster Assignments<\/span>\n\n[Paper](https:\/\/arxiv.org\/abs\/2006.09882) | [Official PyTorch Implementation](https:\/\/github.com\/facebookresearch\/swav) | [Minimal TensorFlow Implementation](https:\/\/github.com\/ayulockin\/SwAV-TF)\n\n## <span id=\"section_1_1\">Contexte<\/span>\nL'apprentissage non supervis\u00e9 des repr\u00e9sentations visuelles progresse \u00e0 un rythme exceptionnellement rapide. La plupart des frameworks modernes *(SimCLR, BYOL, MoCo (V2))* dans ce domaine utilisent un mod\u00e8le auto-supervis\u00e9 pr\u00e9-entrain\u00e9 avec un objectif d'apprentissage contrast\u00e9. Dire que ces frameworks fonctionnent tr\u00e8s bien par rapport aux m\u00e9thodes supervis\u00e9es serait un euph\u00e9misme, comme le montre la figure ci-dessous ici du papier source :\n\n| ![swav_1](http:\/\/www.mf-data-science.fr\/images\/projects\/Swav-vs-supervised.png) | \n|:--:| \n| *Figure 1 :  Top-1 Accuracy des classificateurs lin\u00e9aires entra\u00een\u00e9s avec les caract\u00e9ristiques fig\u00e9es de diff\u00e9rentes m\u00e9thodes auto-supervis\u00e9es par rapport aux m\u00e9thodes enti\u00e8rement supervis\u00e9es.* |\n\nDe plus, lorsque les features apprises \u00e0 l'aide de ces diff\u00e9rentes m\u00e9thodes d'auto-surveillance sont **affin\u00e9es avec seulement 1\u00a0% et 10\u00a0% des donn\u00e9es d'entra\u00eenement \u00e9tiquet\u00e9es**, elles montrent des performances extraordinaires.","5c899de6":"## <span id=\"section_4_3\">Les \u00e9tapes d'entrainement du mod\u00e8le<\/span>\n\nLes \u00e9tapes suivantes sont calqu\u00e9es sur l'impl\u00e9mentation PyTorch li\u00e9e au papier : [ https:\/\/github.com\/facebookresearch\/swav\/blob\/master\/main_swav.py]( https:\/\/github.com\/facebookresearch\/swav\/blob\/master\/main_swav.py)"}}