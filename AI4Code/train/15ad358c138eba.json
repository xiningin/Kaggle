{"cell_type":{"314c9211":"code","3f4208a9":"code","d7c1183a":"code","083c9161":"code","f47aba35":"code","d0852e09":"code","ee952068":"code","9de00d46":"code","831bb3a5":"code","3f83272e":"code","ef6914d0":"code","3a7f8e66":"code","542fa6bd":"code","5d517343":"code","8ebec914":"code","41769349":"code","a83f7f24":"code","e1ec4a38":"code","a814993f":"code","bb3f6040":"code","f0ca59c6":"code","310d737e":"code","bd316995":"code","84d9cabb":"code","c1897499":"code","ee1b73c6":"code","14c26aaa":"code","22f25dea":"code","db395dfc":"code","acab2681":"code","b0d09e49":"code","f24eed19":"code","08547ac4":"code","b6539ac0":"code","31b66b3c":"code","28464cd1":"code","a5fdde15":"code","7480ab87":"code","b8c15cd4":"code","bb12b4ad":"code","d7a4b676":"code","8846f670":"code","06feaef3":"code","c28b3aff":"code","818efc07":"code","6c2d367c":"code","c9def163":"code","b4c6361c":"code","7e383608":"code","73b04963":"code","b9d4193c":"code","cf8e4496":"code","798d45ea":"code","7e153b6c":"code","77a81107":"code","ba0afa8e":"code","ee64e3ce":"code","d5d43065":"code","aa123611":"code","1e4c49ec":"code","c285702d":"code","ab7a1f1e":"code","f85c2f0b":"markdown","901d214c":"markdown","61b409fc":"markdown","c6d2a3a6":"markdown","1246284d":"markdown","e4a4ee11":"markdown","2125de5b":"markdown","90f911a9":"markdown","ffb03b71":"markdown","e71a7adf":"markdown"},"source":{"314c9211":"import warnings\nwarnings.filterwarnings('ignore')","3f4208a9":"#importing the libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_recall_curve, roc_auc_score,auc, classification_report, confusion_matrix, f1_score, roc_curve\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n","d7c1183a":"#reading the dataset\n\ndf = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ndf.head(10)","083c9161":"#checking for null values\nprint(\"Missing Values:- \\n\", df.isnull().sum())\nprint(\"_________________________________________________________\")\nprint()\nprint(\"Target Value Count\\n\", df['Class'].value_counts())\nprint()\n","f47aba35":"# Checking summary of the data\n\ndf.describe()","d0852e09":"# Visualizing different features using histogram\n\ndf.hist(figsize=(10, 10))\nplt.tight_layout()\nplt.show()","ee952068":"# Arranging the columns \n\ncols = ['Time', 'Amount' ,'V1','V2','V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Class']\ndf = df[cols]\n","9de00d46":"# Visualizing the features based on the class 0(No Fraud) or 1(Fraud)\n\nplt.figure(figsize = (10, 80))\ncount1 = 1\nfor i in (df.columns):\n    \n    ax = plt.subplot(16, 2, count1)\n    \n    sns.distplot(df[i][df[\"Class\"] == 1], bins = 100, ax = ax, color='blue' )\n    sns.distplot(df[i][df[\"Class\"] == 0], bins = 100 , ax = ax, color = 'orange')\n    \n    ax.set_xlabel('')\n    ax.set_title('Histogram for feature: ' + str(i))\n    count1 = count1+1\n\nplt.show()","831bb3a5":"# Checking the summary of data based on the class 0 (No Fraud)\n\ndf[df[\"Class\"] == 0].describe()","3f83272e":"# Checking the summary of data based on the class 1 (Fraud)\n\ndf[df['Class'] == 1].describe()","ef6914d0":"# From the above plots I saw that lot of features are skewed  \n#So, I tried to convert those features into boolean features to see if I can improve my accuracy. \n\n# Created a new dataframe to check the accuracy with new boolean features\nnew_df = df.copy()\n\n#Converting features to boolean features by using the 75 percentile as the threshold value for 0 and 1\nnew_df['V1'] = new_df['V1'].map(lambda x: 1 if (x<-0.41) else 0)\nnew_df['V2'] = new_df['V2'].map(lambda x: 1 if (x<4.97) else 0)\nnew_df['V3'] = new_df['V3'].map(lambda x: 1 if (x<-2.27) else 0)\nnew_df['V4'] = new_df['V4'].map(lambda x: 1 if (x<6.34) else 0)\nnew_df['V5'] = new_df['V5'].map(lambda x: 1 if (x<0.21) else 0)\nnew_df['V6'] = new_df['V6'].map(lambda x: 1 if (x<-0.41) else 0)\nnew_df['V7'] = new_df['V7'].map(lambda x: 1 if (x<-0.945) else 0)\nnew_df['V8'] = new_df['V8'].map(lambda x: 1 if (x<1.76) else 0)\nnew_df['V9'] = new_df['V9'].map(lambda x: 1 if (x<-0.787) else 0)\nnew_df['V10'] = new_df['V10'].map(lambda x: 1 if (x<-2.61) else 0)\nnew_df['V11'] = new_df['V11'].map(lambda x: 1 if (x<5.30) else 0)\nnew_df['V12'] = new_df['V12'].map(lambda x: 1 if (x<-2.97) else 0)\nnew_df['V13'] = new_df['V13'].map(lambda x: 1 if (x<0.67) else 0)\nnew_df['V14'] = new_df['V14'].map(lambda x: 1 if (x<-4.28) else 0)\nnew_df['V15'] = new_df['V15'].map(lambda x: 1 if (x<0.609) else 0)\nnew_df['V16'] = new_df['V16'].map(lambda x: 1 if (x<-1.22) else 0)\nnew_df['V17'] = new_df['V17'].map(lambda x: 1 if (x<-1.34) else 0)\nnew_df['V18'] = new_df['V18'].map(lambda x: 1 if (x<0.092) else 0)\nnew_df['V19'] = new_df['V19'].map(lambda x: 1 if (x<1.649) else 0)\nnew_df['V20'] = new_df['V20'].map(lambda x: 1 if (x<0.822) else 0)\nnew_df['V21'] = new_df['V21'].map(lambda x: 1 if (x<1.244) else 0)\nnew_df['V22'] = new_df['V22'].map(lambda x: 1 if (x<0.617) else 0)\nnew_df['V23'] = new_df['V23'].map(lambda x: 1 if (x<0.308) else 0)\nnew_df['V24'] = new_df['V24'].map(lambda x: 1 if (x<0.285) else 0)\nnew_df['V25'] = new_df['V25'].map(lambda x: 1 if (x<0.456) else 0)\nnew_df['V26'] = new_df['V26'].map(lambda x: 1 if (x<0.396) else 0)\nnew_df['V27'] = new_df['V27'].map(lambda x: 1 if (x<0.827) else 0)\nnew_df['V28'] = new_df['V28'].map(lambda x: 1 if (x<0.381) else 0)","3a7f8e66":"# Using standard Scaler to Standardize the Time and Amount Feature\nsc = StandardScaler()\n\ndf[\"Time\"] = sc.fit_transform(df['Time'].values.reshape(-1,1))\ndf['Amount'] = sc.fit_transform(df['Amount'].values.reshape(-1,1))\n\ndf.head()\n","542fa6bd":"#Visualizing the boxplot to see if there are any outliers\n\nplt.figure(figsize = (10,10))\nsns.set(style= 'whitegrid')\nsns.boxplot(data = df, palette= 'Set3')\nplt.xticks(rotation = 90)\nplt.show()","5d517343":"df.shape","8ebec914":"# Checking the interquartile. This will help in removeing the outliers\n\nfrom scipy import stats\n\nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","41769349":"# Removing the outliers and creating a new dataframe. \n\ndf_out = df[~((df < (Q1 - 3 * IQR)) |(df > (Q3 + 3 * IQR))).any(axis=1)]\ndf_out.shape","a83f7f24":"# Checking the counts of two classes after removing the outliers. But it seems that all are class 1 value have been removed \n# and therefore I will not remove outliers from my data. \ndf_out['Class'].value_counts()","e1ec4a38":"# Setting the width and height of the figure\nplt.figure(figsize=(8,4))\n\n# Adding title\nplt.title(\"Fraud vs Non - Fraud Cases\")\n\n# count plot with fraud and non fraud cases\nsns.countplot('Class', data=df)\n\n# Add label for vertical axis\nplt.ylabel(\"Count\")","a814993f":"#Creating X data frame without the target variable\n\nX = df.drop(\"Class\", axis = 1)\n\n# Creating X_processed dataframe with V1, V2...V28 as boolean features\nX_processed = new_df.drop(\"Class\", axis = 1)\n\n# Creating y which has target values\ny = df[\"Class\"]\n\n#Created y_processed which has target Values\ny_processed = new_df[\"Class\"]","bb3f6040":"#Spliting the data set into training and test set\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = 0)\n\n\nX_train_p, X_test_p, y_train_p, y_test_p = train_test_split(X_processed, y_processed, test_size = 0.3, stratify = y_processed, random_state = 0)\n","f0ca59c6":"# Creating a modelling() function to run different models\n\nfrom sklearn.metrics import recall_score\n\n# Function definition\ndef modelling(model_list, X1, X2, y1, y2):\n    \n    \n    X_train = X1\n    X_test  = X2 \n    y_train = y1 \n    y_test  = y2\n    \n    matrix_data = {}    # dictionary to store confusion matrix data\n    auc_data = {}       # dictionary to store the ROC curve data\n    precision_recall_data = {}  # dictionary to store the Precision Recall Curve Data\n     \n    # For loop to access different models that are stored in dictionary\n\n    for name, models in model_list.items():\n        model = models\n        model.fit(X_train, y_train)     #training a model\n        \n        y_pred = model.predict(X_test)  # predicting the results \n        \n        y_probs = model.predict_proba(X_test)    # predicting the probabilities of both the classes\n        y_probs = y_probs[:,1]                   # selecting the probability of class 1\n        \n        matrix_data[name] = confusion_matrix(y_test, y_pred, normalize = None)   # storing the confusion matrix data \n        \n        fpr, tpr, threshold = roc_curve(y_test, y_probs)    # calculating the fpr and tpr values\n        \n        auc_score = roc_auc_score(y_test, y_probs)    # calculating the auc score \n        \n        auc_data[name] = [fpr, tpr, auc_score]  # storing the fpr, tpr and auc_score in auc_data \n        \n        model_precision, model_recall, _ = precision_recall_curve(y_test, y_probs)   #calculating the precision and recall\n\n        model_f1 = f1_score(y_test, y_pred)             #calculating the f1 score of a model\n\n        model_auc = auc(model_recall, model_precision)   # calculating the auc of precision recall curve\n        \n        # storing precision, recall, f1 and auc values in precision_recall_data dictionary\n        precision_recall_data[name] = [model_precision, model_recall, model_f1, model_auc]  \n\n        # Printing the different scores of the models. Summarizing the result of each model\n        print(\"********************************************************************************\")\n        print('Model: {} \\t F1 Score = {:.3f} auc(precision_recall_curve) = {:.3f}\\n'.format(name, model_f1, model_auc))\n        print('Model: {} \\t Recall Score = {:.3f} \\n'.format(name, recall_score(y_test, y_pred)))\n        print(\"Model: {} \\t Accuracy of Tranining Data is: {}\\n\".format(name, accuracy_score(y_train, model.predict(X_train))))\n        print(\"Model: {} \\t Accuracy of Test Data is: {}\\n\".format(name, accuracy_score(y_test, y_pred)))\n        print(\"Model: {} \\t Classification report is:\\n\".format(name))\n        print((classification_report(y_test, y_pred)))\n        print(\"*********************************************************************************\")\n    \n    return matrix_data, auc_data, precision_recall_data","310d737e":"# Plotting the confusion matrix \n\n# function definition \ndef conf_matrix(data, Labels = ['No-Fraud', 'Fraud']):\n    \n    count = 1\n    # using for loop to access the data (its a dictionary)\n    for name, data in matrix_data.items():\n        \n        # setting the figure size\n        plt.figure(figsize=(8,16), facecolor='white')\n        \n        # assigning the number of subplots and with each count increment the axis\n        ax = plt.subplot(3, 1,count)\n        \n        # using seaborn heatmap to plot confusion matrix\n        sns.heatmap(data, xticklabels= Labels, yticklabels= Labels, annot = True, ax = ax, fmt= 'g')\n        \n        # setting the title name\n        ax.set_title(\"Confusion Matrix of {}\".format(name))\n        \n        plt.xlabel(\"Predicted Class\") #setting the x label\n        plt.ylabel(\"True Class\")   # setting the y label\n        plt.show()\n        count = count +1   # incrementor to increase the value of axis with each loop \n","bd316995":"from sklearn.metrics import roc_curve\n\n# function definition for roc curve plot \ndef roc_curve_plot(data):\n    \n    \n    # Setting the figure size\n    plt.figure(figsize=(8,6))\n    \n    # using the for loop to access the data dictionary to plot roc curve\n    for name, data in data.items():\n        plt.plot(data[0], data[1],label=\"{}, AUC={:.3f}\".format(name, data[2]))\n    \n    # plotting a linear line\n    plt.plot([0,1], [0,1], color='orange', linestyle='--')\n\n    # setting the x ticks and x label\n    plt.xticks(np.arange(0.0, 1.1, step=0.1))\n    plt.xlabel(\"Flase Positive Rate\", fontsize=15)\n    \n    # setting the y ticks and y label\n    plt.yticks(np.arange(0.0, 1.1, step=0.1))\n    plt.ylabel(\"True Positive Rate\", fontsize=15)\n    \n    # setting the title of the plots and plotting the legent\n    plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n    plt.legend(prop={'size':13}, loc='lower right')\n\n    plt.show()\n        \n","84d9cabb":"# function definition to plot the precision recall curve\n\ndef precision_recall_curve_plot(data):\n    # setting the figure size \n    plt.figure(figsize=(8,6))\n    \n    # using for loop to access the precision recall data dictionary \n    for name, data in data.items():\n        # plotting the precision recall curve\n        plt.plot(data[1], data[0], marker='.', label='{}, AUC = {:.3f}'.format(name, data[3]))\n    \n    # plotting a flat line\n    single_line = len(y_test[y_test==1]) \/ len(y_test)\n    plt.plot([0, 1], [single_line,single_line], linestyle='--')\n\n\n    # axis labels\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    # show the legend\n    plt.legend()\n    # show the plot\n    plt.show()\n    \n\n","c1897499":"df[\"Class\"].value_counts()","ee1b73c6":"492\/284315","14c26aaa":"# Creating a baseline model without weight adjustments\nmodel_list1 = {\"Logistic\": LogisticRegression(max_iter = 2000), \"RandomForest\": RandomForestClassifier(), \"XGBoost\": XGBClassifier()}\n\n\n# Creating a dictionary of different models with weight adjustments\nmodel_list__with_class_weights = {\"Logistic\": LogisticRegression(max_iter = 2000, class_weight= {0:0.01, 1:1}), \"RandomForest\": RandomForestClassifier(class_weight={0:0.01, 1:1}), \"XGBoost\": XGBClassifier(class_weight= {0:0.01, 1:1})}\n","22f25dea":"\n# running the models (without weight adjustments) using modeeling function \nmatrix_data, auc_data, precision_recall_data = modelling(model_list1, X_train, X_test, y_train , y_test)\n","db395dfc":"# Building the confusion matrix of the three models\n\nconf_matrix(matrix_data)\n","acab2681":"# plotting the ROC curve of the three models (without weight adjustments)\n\nroc_curve_plot(auc_data)","b0d09e49":"# plotting the precision recall curve with wright adjustments\n\nprecision_recall_curve_plot(precision_recall_data)","f24eed19":"y_test.value_counts()","08547ac4":"# Running the different models with weight adjustments using the modelling() function \n\nmatrix_data, auc_data, precision_recall_data = modelling(model_list__with_class_weights, X_train, X_test,y_train, y_test)\n","b6539ac0":"# plotting the confusion matrix \n\nconf_matrix(matrix_data)\n","31b66b3c":"# Plotting the ROC curve \n\nroc_curve_plot(auc_data)\n","28464cd1":"# Plotting the precision recall curve (with weight adjusted models)\n\nprecision_recall_curve_plot(precision_recall_data)","a5fdde15":"#let us try undersampling the negative class (i.e., low rating)\nfrom imblearn.under_sampling import NearMiss \nnm = NearMiss() \n\nX_resampled, y_resampled = nm.fit_sample(X_train, y_train) \n\nmatrix_data, auc_data, precision_recall_data = modelling(model_list1, X_resampled, X_test, y_resampled, y_test)\n","7480ab87":"# Plotting the the confusion matrix for undersampled data\nconf_matrix(matrix_data)\n","b8c15cd4":"# Plotting the roc curve for undersampled data \n\nroc_curve_plot(auc_data)","bb12b4ad":"# plotting the precision recall curve for undersampled data \n\nprecision_recall_curve_plot(precision_recall_data)","d7a4b676":"#let us try undersampling the negative class (i.e., low rating)\n\n# Using RandomUnderSampler this time to see if we can improve an accuracy a little bit\nfrom imblearn.under_sampling import RandomUnderSampler \n\nrus = RandomUnderSampler() \n\nX_resampled, y_resampled = rus.fit_sample(X_train, y_train) \n\nmatrix_data, auc_data, precision_recall_data = modelling(model_list1, X_resampled, X_test, y_resampled, y_test)\n","8846f670":"# plotting the confusion matrix \n\nconf_matrix(matrix_data)\n","06feaef3":"# plotting the Roc Curve for undersample data\n\nroc_curve_plot(auc_data)","c28b3aff":"# plottiing the precision recall curve for under sampled data \n\nprecision_recall_curve_plot(precision_recall_data)","818efc07":"# Using SMOTE to oversample the data \n\nfrom imblearn.over_sampling import SMOTE\n\nsmt = SMOTE()\n\n# creating oversampled data from training dataset \nX_oversampled, y_oversampled = smt.fit_sample(X_train, y_train)\n\n# running the oversampled data using modelling function\nmatrix_data, auc_data, precision_recall_data = modelling(model_list1, X_oversampled, X_test, y_oversampled, y_test)\n","6c2d367c":"# plotting confusion matrix of oversampled data \n\nconf_matrix(matrix_data)\n","c9def163":"# plotting ROC curve for undersampled data \n\nroc_curve_plot(auc_data)","b4c6361c":"# plotting the precision recall curve for oversampled data \n\nprecision_recall_curve_plot(precision_recall_data)","7e383608":"# Using another over sampling algorithm ADASYN\n\nfrom imblearn.over_sampling import ADASYN \nada = ADASYN() \n\n# creating the oversampled data from training data set \nX_resampled, y_resampled = ada.fit_sample(X_train, y_train) \n\n# running the models using over sampled data \nmatrix_data, auc_data, precision_recall_data = modelling(model_list1, X_resampled, X_test, y_resampled, y_test)\n","73b04963":"# plotting a confusion matrix of oversampled datga  \n\nconf_matrix(matrix_data)\n","b9d4193c":"# plotting the ROC curve for oversampled data \n\nroc_curve_plot(auc_data)","cf8e4496":"# plotting the precision recall curve for oversampled data \n\nprecision_recall_curve_plot(precision_recall_data)\n","798d45ea":"# importin the keras libraries to create a deep learning model\n\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\n\nn_inputs = X_train.shape[1]\n\n# creating a deep learning model \n\ndeep_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])","7e153b6c":"# compiling a model\ndeep_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","77a81107":"# fitting my training data to the deep learning model\ndeep_model.fit(X_train, y_train, validation_split=0.2, batch_size=25, epochs=20, shuffle=True, verbose=2)","ba0afa8e":"# predicting the values\ndeep_model_pred = deep_model.predict_classes(X_test, batch_size=200, verbose=0)","ee64e3ce":"# creating a confusion matrix \n\ndeep_model_cm = confusion_matrix(y_test, deep_model_pred)\nlabels = ['No Fraud', 'Fraud']\n","d5d43065":"#function to display confusion matrix\ndef draw_matrix(conf_matrix, LABELS = [\"Low Rating\", \"High Rating\"]):\n    plt.figure(figsize=(6, 6))\n    sns.heatmap(conf_matrix, xticklabels=LABELS,\n                yticklabels=LABELS, annot=True, fmt=\"g\");\n    plt.title(\"Confusion matrix\")\n    plt.ylabel('True class')\n    plt.xlabel('Predicted class')\n    plt.show()\n    \ndraw_matrix(deep_model_cm)","aa123611":"print(\"Accuracy of a deep learning model is: {}\".format(accuracy_score(y_test, deep_model_pred)))\nprint(\"Recall score of a deep learning model is: {}\".format(recall_score(y_test, deep_model_pred)))\nprint(\"F1 Score is: {}\".format(f1_score(y_test, deep_model_pred)))\nprint(\"\\n Classification report is: \")\nprint(classification_report(y_test, deep_model_pred))","1e4c49ec":"y_probs =   deep_model.predict(X_test, batch_size=200, verbose=0)[:,1]\n\nfpr, tpr, threshold = roc_curve(y_test, y_probs)    # calculating the fpr and tpr values\n        \nauc_score = roc_auc_score(y_test, y_probs)    # calculating the auc score \n        \n#calculating the precision and recall        \ndeep_precision, deep_recall, _ = precision_recall_curve(y_test, y_probs)  \n\ndeep_f1 = f1_score(y_test, deep_model_pred)             #calculating the f1 score of a model\n\ndeep_auc = auc(deep_recall, deep_precision)   # calculating the auc of precision recall curve\n        ","c285702d":"# Setting the figure size\nplt.figure(figsize=(8,6))\n    \n# using the for loop to access the data dictionary to plot roc curve\nplt.plot(fpr, tpr,label=\"Deep Learning Model, AUC={:.3f}\".format(auc_score))\n    \n# plotting a linear line\nplt.plot([0,1], [0,1], color='orange', linestyle='--')\n\n# setting the x ticks and x label\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"Flase Positive Rate\", fontsize=15)\n    \n# setting the y ticks and y label\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n# setting the title of the plots and plotting the legent\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\n\nplt.show()\n        \n    \n   \n    \n","ab7a1f1e":"# setting the figure size \nplt.figure(figsize=(8,6))\n\n# plotting the precision recall curve\nplt.plot(deep_recall, deep_precision, marker='.', label='Deep Learning Model, AUC = {:.3f}'.format(deep_auc))\n    \n# plotting a flat line\nsingle_line = len(y_test[y_test==1]) \/ len(y_test)\nplt.plot([0, 1], [single_line,single_line], linestyle='--')\n\n\n# axis labels\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","f85c2f0b":"### Oversampling of  data using SMOTE and ADASYN","901d214c":"#### Trying Deep Learning using Keras and Tensorflow to see if recall accuracy is improved","61b409fc":"### Trying the same three machine learning models by adjusting the class_weights","c6d2a3a6":"### Summary","1246284d":"### Trying Under Sampling (Near Miss) with all the above algorithms","e4a4ee11":"<table border=0 cellpadding=0 cellspacing=0 width=898 style='border-collapse:\n collapse;table-layout:fixed;width:674pt'>\n <col width=290 style='mso-width-source:userset;mso-width-alt:10325;width:218pt'>\n <col width=154 style='mso-width-source:userset;mso-width-alt:5489;width:116pt'>\n <col width=169 style='mso-width-source:userset;mso-width-alt:6001;width:127pt'>\n <col width=118 style='mso-width-source:userset;mso-width-alt:4181;width:88pt'>\n <col width=86 style='mso-width-source:userset;mso-width-alt:3043;width:64pt'>\n <col width=81 style='mso-width-source:userset;mso-width-alt:2872;width:61pt'>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl65 width=290 style='height:14.4pt;width:218pt'>Model\n  Name<\/td>\n  <td class=xl65 width=154 style='border-left:none;width:116pt'>Accuracy Score\n  (Training)<\/td>\n  <td class=xl65 width=169 style='border-left:none;width:127pt'>Accuracy Score\n  (Testing)<\/td>\n  <td class=xl65 width=118 style='border-left:none;width:88pt'>F1 Score<\/td>\n  <td class=xl65 width=86 style='border-left:none;width:64pt'>Recall Score<\/td>\n  <td class=xl65 width=81 style='border-left:none;width:61pt'>AUC Score<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl65 style='height:14.4pt;border-top:none'>Logistic\n  Regression<span style='mso-spacerun:yes'>  <\/span>(No weights adjusted)<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>99.926<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>99.92<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>73.2<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>62.8<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>74<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl65 style='height:14.4pt;border-top:none'>Logistic\n  Regression (Weights Adjusted)<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>99.57<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>99.5<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>42.1<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>85.1<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>72<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl65 style='height:14.4pt;border-top:none'>Logistic\n  Regression (Near Miss)<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>97.09<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>53.5<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>0.7<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>93.2<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>3<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl65 style='height:14.4pt;border-top:none'>Logistic\n  Regression (Random UnderSampler)<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>94.9<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>97.8<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>12.8<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>89.9<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>70<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl65 style='height:14.4pt;border-top:none'>Logistic\n  Regression (SMOTE)<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>95<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>97.6<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>11.8<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>91.2<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>74<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl65 style='height:14.4pt;border-top:none'>Logistic\n  Regression (ADASYN)<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>88.95<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>91.5<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>3.6<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>92.6<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>76<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl65 style='height:14.4pt;border-top:none'>&nbsp;<\/td>\n  <td class=xl65 style='border-top:none;border-left:none'>&nbsp;<\/td>\n  <td class=xl65 style='border-top:none;border-left:none'>&nbsp;<\/td>\n  <td class=xl65 style='border-top:none;border-left:none'>&nbsp;<\/td>\n  <td class=xl65 style='border-top:none;border-left:none'>&nbsp;<\/td>\n  <td class=xl65 style='border-top:none;border-left:none'>&nbsp;<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl65 style='height:14.4pt;border-top:none'>Random Forest\n  Classifier (No weights adjusted)<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>1<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>99.94<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>82.2<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>71.6<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>83<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl65 style='height:14.4pt;border-top:none'>Random Forest\n  Classifier (Weights Adjusted)<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>1<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>99.94<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>82.3<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>72.3<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>84.7<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl65 style='height:14.4pt;border-top:none'>Random Forest\n  Classifier (Near Miss)<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>1<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>3.95<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>0.3<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>96.6<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>61<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl65 style='height:14.4pt;border-top:none'>Random Forest\n  Classifier (Random UnderSampler<span style='display:none'>)<span\n  style='mso-spacerun:yes'> <\/span><\/span><\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>1<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>98.32<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>15.4<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>87.8<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>75<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl66 style='height:14.4pt;border-top:none'>Random Forest\n  Classifier (SMOTE)<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>1<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>99.95<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>84.9<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>79.7<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>84<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl65 style='height:14.4pt;border-top:none'>Random Forest\n  Classifier (ADASYN)<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>1<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>99.94<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>83.1<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>76.4<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>83<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl65 style='height:14.4pt;border-top:none'>&nbsp;<\/td>\n  <td class=xl65 style='border-top:none;border-left:none'>&nbsp;<\/td>\n  <td class=xl65 style='border-top:none;border-left:none'>&nbsp;<\/td>\n  <td class=xl65 style='border-top:none;border-left:none'>&nbsp;<\/td>\n  <td class=xl65 style='border-top:none;border-left:none'>&nbsp;<\/td>\n  <td class=xl65 style='border-top:none;border-left:none'>&nbsp;<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl66 style='height:14.4pt;border-top:none'>XGBoost\n  Classifier (No weights adjusted)<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>1<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>99.95<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>84.1<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>75<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>84.6<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl66 style='height:14.4pt;border-top:none'>XGBoost\n  Classifier (Weights Adjusted)<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>1<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>99.95<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>84.1<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>75<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>84.6<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl65 style='height:14.4pt;border-top:none'>XGBoost\n  Classifier (Near Miss)<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>1<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>10.46<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>0.4<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>95.9<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>13.2<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl65 style='height:14.4pt;border-top:none'>XGBoost\n  Classifier (Random Under Sampler)<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>1<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>97.74<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>12<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>89.2<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>71.1<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl66 style='height:14.4pt;border-top:none'>XGBoost\n  Classifier (SMOTE)<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>1<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>99.93<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>81.4<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>81.1<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>83.5<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl66 style='height:14.4pt;border-top:none'>XGBoost\n  Classifier (ADASYN)<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>1<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>99.92<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>78.6<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>81.8<\/td>\n  <td class=xl66 align=right style='border-top:none;border-left:none'>82.8<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl65 style='height:14.4pt;border-top:none'>&nbsp;<\/td>\n  <td class=xl65 style='border-top:none;border-left:none'>&nbsp;<\/td>\n  <td class=xl65 style='border-top:none;border-left:none'>&nbsp;<\/td>\n  <td class=xl65 style='border-top:none;border-left:none'>&nbsp;<\/td>\n  <td class=xl65 style='border-top:none;border-left:none'>&nbsp;<\/td>\n  <td class=xl65 style='border-top:none;border-left:none'>&nbsp;<\/td>\n <\/tr>\n <tr height=19 style='height:14.4pt'>\n  <td height=19 class=xl65 style='height:14.4pt;border-top:none'>Deep Learning\n  Model (Keras )<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>99.96<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>99.94<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>81.7<\/td>\n  <td class=xl65 align=right style='border-top:none;border-left:none'>74.3<\/td>\n  <td class=xl65 style='border-top:none;border-left:none'>&nbsp;<\/td>\n <\/tr>\n <tr height=0 style='display:none'>\n  <td width=290 style='width:218pt'><\/td>\n  <td width=154 style='width:116pt'><\/td>\n  <td width=169 style='width:127pt'><\/td>\n  <td width=118 style='width:88pt'><\/td>\n  <td width=86 style='width:64pt'><\/td>\n  <td width=81 style='width:61pt'><\/td>\n <\/tr>\n<\/table>\n","2125de5b":"##### Time is the seconds elapsed between each transaction and the first transaction in the dataset and is represented as numbers. There is a lot of variation in the first value and the last value so I applied standard scaler in order to standardize the results. \n##### Amount feature also needs to be scaled because there is also a huge difference in the values of amount. \n\n##### Also, the mean of both these columns varies a lot as compared to other features and Time and Amount will dominate other features. Therefore, I scaled both of these features","90f911a9":"Implement SMOTE() for oversampling our imbalanced dataset actually helped in improving the recall score of the model along with maintaining the precision score of the model. Out of the other models I have used the best performance was by the XGBoost followed by the Random Forest Classifier. The highest recall accuracy I got was 96.6 from the Random Forest Classifier when I used the under sampling technique called Near Miss. But with undersampling, there was a trade off with the precision score and recall score. I got the highest recall score but my precision for class 1 was very low which means that I was misclassifying the No-Fraud as Fraud classes. \nI also tried deep learning model with Keras and I recieved the recall score of 74.3. \n\nThe best model which I have got is the Random Forest Classifier and XGBoost when I have used SMOTE and ADASYN for oversampling with recall, f1 and AUC scores more than 80. These are the best models since they are correctly identifying the fraud and no fraud.\n\n","ffb03b71":"##### As I can see, this is a highly imbalanced data with very few non fraud cases. I will try using techniques like oversampling, under sampling and random sample to determine the better results.\n\n##### But first, I will  split my data into training and testing dataset so that I can keep some data aside for final testing.","e71a7adf":"##### First I will create simple Logistic Regression, Random Forest Classifier and XGBoost without setting the class weight.\n\n#### And to easily access the models, I have created different functions: \n####  1. modelling() - This function will run different models and will return the accuracy scores, recall scores, confusion matrix data to plot confusion matrix, roc curve data (fpr, tpr and auc) to plot roc curve and precision recall data to plot the precision recall curve.\n####  2. conf_matrix() - This function will plot the confusion matrix from the data that I have got from the 1st function.\n####  3. roc_curve_plot() - This function will plot the ROC curve.\n####  4. precision_recall_curve_plot() - This function will plot the Precision Recall Curve."}}