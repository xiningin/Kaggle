{"cell_type":{"592372a3":"code","4d921872":"code","3c2d15d6":"code","678c75fb":"code","4fffbc50":"code","a16a8b16":"code","8ef11745":"code","4a8cc46d":"code","c5f203f8":"code","22d833f2":"code","be396cff":"code","8d896b97":"code","1e577518":"code","04b3c278":"code","f4b7e365":"code","7433763f":"code","b13913cb":"code","f34fdbdb":"code","ea6afc1c":"code","115200d2":"code","0d25c6b2":"code","b17c9685":"code","39327389":"code","bfa52c62":"code","95395ef7":"code","fe834fd7":"code","f058084c":"code","4fea7510":"code","60519446":"code","af65e844":"code","c5226611":"code","76ad9e92":"code","a223eba0":"code","d4156bcb":"code","8622bc74":"code","90af3abc":"code","07be1e14":"code","abe27f29":"code","ec6c5519":"code","7470eca4":"code","f5fee706":"code","0cad3a24":"code","6b42268e":"code","9dc103e8":"code","146ef07e":"code","c1921926":"markdown","e181ce1b":"markdown","31de4731":"markdown","59f28206":"markdown","d40c372f":"markdown","fbe49937":"markdown","2c4c55af":"markdown","09ca5e89":"markdown","5c47b2aa":"markdown","cd9b7c8e":"markdown","64146181":"markdown","2b867dd1":"markdown","a81c1ed3":"markdown","63ac213d":"markdown","af07ec58":"markdown","6e87c1e4":"markdown","45941ca0":"markdown"},"source":{"592372a3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os","4d921872":"dataset_path = '\/kaggle\/input\/dry-beans-classification-iti-ai-pro-intake01\/'\n\ntrain_df = pd.read_csv(os.path.join(dataset_path, 'train.csv'))\n\ntest_df = pd.read_csv(os.path.join(dataset_path, 'test.csv'))","3c2d15d6":"train_df.head()","678c75fb":"train_df.info()","4fffbc50":"train_df.iloc[:,1:].describe()","a16a8b16":"train_df['y'].value_counts()","8ef11745":"train_df['y'] = train_df['y'].map({'DERMASON': 0, 'SIRA': 1, 'SEKER': 2, 'HOROZ': 3, 'CALI': 4,\\\n                                   'BARBUNYA': 5, 'BOMBAY': 6})\n\ntrain_df['y'] = train_df['y'].astype('int') \ntrain_df['y'].value_counts()","4a8cc46d":"# #distributions of the data\n# for i in train_df.drop('ID', axis=1):\n#     sns.displot(train_df[i])\n#     plt.show()","c5f203f8":"train_df.columns","22d833f2":"#sns.pairplot(train_df,vars=train_df.drop(['ID'], axis=1).columns, palette='husl',plot_kws={'alpha': 0.5})","be396cff":"# sns.boxplot(x='Area', data=train_df)","8d896b97":"fig, ax = plt.subplots(figsize=(12,7))\nsns.heatmap(train_df.corr(), linewidths=.5, annot=True, fmt='.2f', ax=ax)","1e577518":"#features = train_df.drop([\"ID\", \"y\", \"Perimeter\", \"MajorAxisLength\", \"MinorAxisLength\", \"EquivDiameter\",\\\n#                         \"Compactness\", \"ShapeFactor3\"], axis=1)\nfeatures = train_df.drop([\"ID\", \"y\", \"Perimeter\", \"ConvexArea\", \"ShapeFactor3\", \"EquivDiameter\",\n                          \"MajorAxisLength\", \"MinorAxisLength\", \"ShapeFactor2\"],axis=1)\n\nlabels = train_df[\"y\"].astype('category')\n\nmy_test_df = test_df.drop([\"ID\", \"Perimeter\", \"ConvexArea\", \"ShapeFactor3\", \"EquivDiameter\",\n                          \"MajorAxisLength\", \"MinorAxisLength\", \"ShapeFactor2\"],axis=1)","04b3c278":"features.head()","f4b7e365":"my_df = features.copy()\nmy_df['y'] = train_df['y']\nmy_df.head()","7433763f":"# for i in range(7):\n#     sns.displot(my_df[my_df['y'] == i].Area)\n#     plt.title('y ={}'.format(i))","b13913cb":"for i in range(7):\n    print(i)\n    print(my_df[my_df['y'] == i].Area.describe())","f34fdbdb":"bins = [0,28000, 35000, 42000, 47000, 58000, 62000, 69000, 76000, 81000, 157000,187000]\n# Use .cut() method to make bins from the Area column\n\ncategorized_area = pd.cut(my_df['Area'], bins)\nmy_df['area_cat'] = categorized_area\nmy_df = my_df.drop([\"Area\"], axis=1)\nmy_df\n\ncategorized_area = pd.cut(my_test_df['Area'], bins)\nmy_test_df['area_cat'] = categorized_area\nmy_test_df = my_test_df.drop([\"Area\"], axis=1)","ea6afc1c":"def bin_col(df, col, bins):\n    # Use .cut() method to make bins from the Area column\n    categorized = pd.cut(df[col], bins)\n    df[col] = categorized\n    return df ","115200d2":"for i in range(7):\n    print(i)\n    print(my_df[my_df['y'] == i]['AspectRation'].describe())","0d25c6b2":"bins = [0, 1.1, 1.2, 1.4, 1.5, 1.6, 1.7, 1.9, 2.1,5]\nbin_col(my_df, 'AspectRation', bins)\n\nbin_col(my_test_df, 'AspectRation', bins)\n","b17c9685":"for i in range(7):\n    print(i)\n    print(my_df[my_df['y'] == i]['Eccentricity'].describe())","39327389":"bins = [0, 0.5, 0.6, 0.7, 0.8, 0.85, 0.88, 10]\nbin_col(my_df, 'Eccentricity', bins)\n\nbin_col(my_test_df, 'Eccentricity', bins)\n","bfa52c62":"for i in range(7):\n    print(i)\n    print(my_df[my_df['y'] == i]['Compactness'].describe())","95395ef7":"bins = [0, 0.6, 0.7, 0.8, 0.83, 0.91, 10]\nbin_col(my_df, 'Compactness', bins)\n\nbin_col(my_test_df, 'Compactness', bins)\n","fe834fd7":"onehot_area = pd.get_dummies(my_df[['area_cat']])\n\n# Use pandas method .drop() to remove the old columns and method .concat() to add the new columns\nmy_df = pd.concat([my_df, onehot_area], axis=1)\n\n# drop columns\nmy_df.drop(['area_cat'], axis=1, inplace=True)\n\n#AspectRation\nonehot_aspectration = pd.get_dummies(my_df[['AspectRation']])\n\n# Use pandas method .drop() to remove the old columns and method .concat() to add the new columns\nmy_df = pd.concat([my_df, onehot_aspectration], axis=1)\n\n# drop columns\nmy_df.drop(['AspectRation'], axis=1, inplace=True)\n\n#Eccentricity\nonehot_eccentricity = pd.get_dummies(my_df[['Eccentricity']])\n\n# Use pandas method .drop() to remove the old columns and method .concat() to add the new columns\nmy_df = pd.concat([my_df, onehot_eccentricity], axis=1)\n\n# drop columns\nmy_df.drop(['Eccentricity'], axis=1, inplace=True)\n\n#Compactness\nonehot_compactness = pd.get_dummies(my_df[['Compactness']])\n\n# Use pandas method .drop() to remove the old columns and method .concat() to add the new columns\nmy_df = pd.concat([my_df, onehot_compactness], axis=1)\n\n# drop columns\nmy_df.drop(['Compactness'], axis=1, inplace=True)\n\nmy_df.head()\n\n#################################\n\nonehot_area = pd.get_dummies(my_test_df[['area_cat']])\n\n# Use pandas method .drop() to remove the old columns and method .concat() to add the new columns\nmy_test_df = pd.concat([my_test_df, onehot_area], axis=1)\n\n# drop columns\nmy_test_df.drop(['area_cat'], axis=1, inplace=True)\n\n#AspectRation\nonehot_aspectration = pd.get_dummies(my_test_df[['AspectRation']])\n\n# Use pandas method .drop() to remove the old columns and method .concat() to add the new columns\nmy_test_df = pd.concat([my_test_df, onehot_aspectration], axis=1)\n\n# drop columns\nmy_test_df.drop(['AspectRation'], axis=1, inplace=True)\n\n#Eccentricity\nonehot_eccentricity = pd.get_dummies(my_test_df[['Eccentricity']])\n\n# Use pandas method .drop() to remove the old columns and method .concat() to add the new columns\nmy_test_df = pd.concat([my_test_df, onehot_eccentricity], axis=1)\n\n# drop columns\nmy_test_df.drop(['Eccentricity'], axis=1, inplace=True)\n\n#Compactness\nonehot_compactness = pd.get_dummies(my_test_df[['Compactness']])\n\n# Use pandas method .drop() to remove the old columns and method .concat() to add the new columns\nmy_test_df = pd.concat([my_test_df, onehot_compactness], axis=1)\n\n# drop columns\nmy_test_df.drop(['Compactness'], axis=1, inplace=True)\n\nmy_test_df.head()\n\n","f058084c":"my_df.columns","4fea7510":"# # import modules\n# import numpy as np\n# from scipy import stats\n\n# train_data_boxcox = pd.DataFrame()\n# for col in features.columns:\n#     train_data_boxcox[col], fitted_lambda = stats.boxcox(features[col])\n#     # visualize distribution\n#     fig, ax = plt.subplots(1, 2)\n#     sns.distplot(train_data_boxcox[col], ax=ax[0])\n#     sns.distplot(features[col], ax=ax[1])\n#     plt.show()\n    \n# #show data after normalization with boxcox method    \n# print(train_data_boxcox.head())","60519446":"# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n\n# features = scaler.fit_transform(features)","af65e844":"labels = my_df['y']\nfeatures = my_df.drop(['y'], axis=1)","c5226611":"features.columns","76ad9e92":"features.rename(columns= { 'area_cat_(0, 28000]' : 'area_1',\n                          'area_cat_(28000, 35000]': 'area_2',\n                          'area_cat_(35000, 42000]': 'area_3',\n                          'area_cat_(42000, 47000]': 'area_4',\n                          'area_cat_(47000, 58000]': 'area_5',\n                          'area_cat_(58000, 62000]': 'area_6',\n                          'area_cat_(62000, 69000]': 'area_7',\n                          'area_cat_(69000, 76000]': 'area_8',\n                          'area_cat_(76000, 81000]': 'area_9',\n                          'area_cat_(81000, 157000]': 'area_10',\n                          'area_cat_(157000, 187000]': 'area_11',\n                          'AspectRation_(0.0, 1.1]': 'aspect_1',\n                          'AspectRation_(1.1, 1.2]': 'aspect_2',\n                          'AspectRation_(1.2, 1.4]': 'aspect_3',\n                          'AspectRation_(1.4, 1.5]': 'aspect_4',\n                          'AspectRation_(1.5, 1.6]': 'aspect_5',\n                          'AspectRation_(1.6, 1.7]': 'aspect_6',\n                          'AspectRation_(1.7, 1.9]': 'aspect_7',\n                          'AspectRation_(1.9, 2.1]': 'aspect_8',\n                          'AspectRation_(2.1, 5.0]': 'aspect_9',\n                          'Eccentricity_(0.0, 0.5]': 'eccentricity_1',\n                          'Eccentricity_(0.5, 0.6]': 'eccentricity_2',\n                          'Eccentricity_(0.6, 0.7]': 'eccentricity_3',\n                          'Eccentricity_(0.7, 0.8]': 'eccentricity_4',\n                          'Eccentricity_(0.8, 0.85]': 'eccentricity_5',\n                          'Eccentricity_(0.85, 0.88]': 'eccentricity_6',\n                          'Eccentricity_(0.88, 10.0]': 'eccentricity_7',\n                          'Compactness_(0.0, 0.6]': 'compactness_1',\n                          'Compactness_(0.6, 0.7]': 'compactness_2',\n                          'Compactness_(0.7, 0.8]': 'compactness_3',\n                          'Compactness_(0.8, 0.83]': 'compactness_4',\n                          'Compactness_(0.83, 0.91]': 'compactness_5',\n                          'Compactness_(0.91, 10.0]': 'compactness_6',\n                         }, inplace=True)\nfeatures.columns\n\nmy_test_df.rename(columns= { 'area_cat_(0, 28000]' : 'area_1',\n                          'area_cat_(28000, 35000]': 'area_2',\n                          'area_cat_(35000, 42000]': 'area_3',\n                          'area_cat_(42000, 47000]': 'area_4',\n                          'area_cat_(47000, 58000]': 'area_5',\n                          'area_cat_(58000, 62000]': 'area_6',\n                          'area_cat_(62000, 69000]': 'area_7',\n                          'area_cat_(69000, 76000]': 'area_8',\n                          'area_cat_(76000, 81000]': 'area_9',\n                          'area_cat_(81000, 157000]': 'area_10',\n                          'area_cat_(157000, 187000]': 'area_11',\n                          'AspectRation_(0.0, 1.1]': 'aspect_1',\n                          'AspectRation_(1.1, 1.2]': 'aspect_2',\n                          'AspectRation_(1.2, 1.4]': 'aspect_3',\n                          'AspectRation_(1.4, 1.5]': 'aspect_4',\n                          'AspectRation_(1.5, 1.6]': 'aspect_5',\n                          'AspectRation_(1.6, 1.7]': 'aspect_6',\n                          'AspectRation_(1.7, 1.9]': 'aspect_7',\n                          'AspectRation_(1.9, 2.1]': 'aspect_8',\n                          'AspectRation_(2.1, 5.0]': 'aspect_9',\n                          'Eccentricity_(0.0, 0.5]': 'eccentricity_1',\n                          'Eccentricity_(0.5, 0.6]': 'eccentricity_2',\n                          'Eccentricity_(0.6, 0.7]': 'eccentricity_3',\n                          'Eccentricity_(0.7, 0.8]': 'eccentricity_4',\n                          'Eccentricity_(0.8, 0.85]': 'eccentricity_5',\n                          'Eccentricity_(0.85, 0.88]': 'eccentricity_6',\n                          'Eccentricity_(0.88, 10.0]': 'eccentricity_7',\n                          'Compactness_(0.0, 0.6]': 'compactness_1',\n                          'Compactness_(0.6, 0.7]': 'compactness_2',\n                          'Compactness_(0.7, 0.8]': 'compactness_3',\n                          'Compactness_(0.8, 0.83]': 'compactness_4',\n                          'Compactness_(0.83, 0.91]': 'compactness_5',\n                          'Compactness_(0.91, 10.0]': 'compactness_6',\n                         }, inplace=True)\nmy_test_df.columns","a223eba0":"fig, ax = plt.subplots(figsize=(20,20))\nsns.heatmap(features.corr(), linewidths=.5, annot=True, fmt='.2f', ax=ax)","d4156bcb":"from sklearn.model_selection import train_test_split\nX_train, X_vald, y_train, y_vald = train_test_split(features, labels, test_size=0.2, random_state=100, stratify=labels )","8622bc74":"# Decisin Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt_model = DecisionTreeClassifier().fit(X_train, y_train)\n#SVM\nfrom sklearn.svm import SVC\nsvm_model = SVC(gamma='auto').fit(X_train, y_train)\n#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier().fit(X_train, y_train)\n#Gradient Boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\ngb_model = GradientBoostingClassifier().fit(X_train, y_train)\n#Ada Boosting\nfrom sklearn.ensemble import AdaBoostClassifier\nab_model = AdaBoostClassifier().fit(X_train, y_train)\n# XG Boosting\nfrom xgboost import XGBClassifier\nxgb_model = XGBClassifier(n_estimators=500)\nxgb_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_vald, y_vald)],\n             verbose=False)","90af3abc":"# print accuracy of each model on validation data\nprint(\"Scores of the models\")\nprint(\"Decision tree:\", dt_model.score(X_vald, y_vald))\nprint(\"SVM:\", svm_model.score(X_vald, y_vald))\nprint(\"Random forest:\", rf_model.score(X_vald, y_vald))\nprint(\"Gradient boosting:\", gb_model.score(X_vald, y_vald))\nprint(\"AdaBoost:\", ab_model.score(X_vald, y_vald))\nprint(\"XGBoost:\", xgb_model.score(X_vald, y_vald))","07be1e14":"feature_imp = pd.Series(rf_model.feature_importances_,index=X_train.columns).sort_values(ascending=False)\nfeature_imp\n\n# Creating a bar plot\nsns.set(rc={'figure.figsize':(10,10)})\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","abe27f29":"features = features.drop(['aspect_1', 'area_9', 'aspect_2', 'eccentricity_6', 'eccentricity_7'],axis=1)\n\nX_train, X_vald, y_train, y_vald = train_test_split(features, labels, test_size=0.2,\n                                                    random_state=100, stratify=labels )\n\n# XG Boosting\nfrom xgboost import XGBClassifier\nxgb_model = XGBClassifier(n_estimators=500)\nxgb_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_vald, y_vald)],\n             verbose=False)\n\nprint(\"XGBoost:\", xgb_model.score(X_vald, y_vald))","ec6c5519":"#from sklearn.model_selection import GridSearchCV\n# svm_parameters = {'kernel': ['rbf'],\n#                   'C': [0.01, 0.1, 1 , 10, 100],\n#                   'gamma': [0.01, 0.1, 1, 10, 100]\n#                 }\n# # use gridsearch to find the best hyperparameters \n# svm = SVC()\n# svm_gs = GridSearchCV(estimator = svm, param_grid = svm_parameters)\n# svm_gs.fit(X_train, y_train)\n\n# svm_winner = svm_gs.best_estimator_\n# svm_winner\n\n# svm_winner.score(X_vald, y_vald)","7470eca4":"#features = features.drop(['aspect_1', 'area_9'],axis=1)\n\n# X_train, X_vald, y_train, y_vald = train_test_split(features, labels, test_size=0.2,\n#                                                     random_state=100, stratify=labels )\n\nfrom catboost import CatBoostClassifier\n\ncat_model = CatBoostClassifier(\n    iterations=1000,\n    random_seed=42,\n    learning_rate=0.1,\n    custom_loss=['AUC', 'Accuracy']\n)\n\ncat_model.fit(\n    X_train, y_train,\n    eval_set=(X_vald, y_vald),\n    verbose=False,\n    plot=True\n)","f5fee706":"print(\"CatBoost:\", cat_model.score(X_vald, y_vald))","0cad3a24":"len(features.columns)","6b42268e":"my_test_df.shape","9dc103e8":"X_test = my_test_df.copy()\n\nX_test = X_test.drop(columns=['aspect_1','area_9'], axis=1)\n\ny_test_predicted = cat_model.predict(X_test)\n\ntest_df['y'] = y_test_predicted.astype(int)\n\n#mapping back to string classes\ntest_df['y'] = test_df['y'].map({0: 'DERMASON', 1: 'SIRA', 2: 'SEKER', 3: 'HOROZ', 4: 'CALI',\\\n                                   5: 'BARBUNYA', 6: 'BOMBAY'})\n\ntest_df['y'] = test_df['y'].astype('object') \n\ntest_df.head()","146ef07e":"test_df[['ID', 'y']].to_csv('\/kaggle\/working\/submission.csv', index=False)","c1921926":"#### Area highly correlated with Periemeter & MajorAxisLength & MinorAxisLength & EquivDiameter\n#### AspectRation highly correlated with Compactness & ShapeFactor3\n### Drop Periemeter & MajorAxisLength & MinorAxisLength & EquivDiameter & Compactness & ShapeFactor3","e181ce1b":"# Show distribution each feature with respect to each label","31de4731":"# Dry Beans Classification","59f28206":"# Standard Scaling","d40c372f":"# XGBoost","fbe49937":"# Features & labels","2c4c55af":"# Training Different models","09ca5e89":"# Import libraries","5c47b2aa":"ToDO\n\n1- stratify data in split to solve imbalanced labels\n\n2- feature importance as many features are correlated\n\n3- transform distribution into normal\n\n4- PCA\n\n5- Oversampling & Undersampling\n\n6- Hyperparameter Tuning","cd9b7c8e":"# There are many features highly correlated with each other , confirm this with corr matrix and remove one of the correlated features to rempve redundancy & overfitting","64146181":"# Test Data with XGB","2b867dd1":"## It is clear that:\n### y = 0 => Area 28000:35000\n### y = 1 => Area 41000:47000\n### y = 2 => Area 36000:42000\n### y = 3 => Area 33000:58000\n### y = 4 => Area 69000:81000\n### y = 5 => Area 62000:76000\n### y = 6 => Area 157000:187000","a81c1ed3":"# Bining Area","63ac213d":"# Read data","af07ec58":"# Evaluating Models","6e87c1e4":"# EDA","45941ca0":"# one hot encoding"}}