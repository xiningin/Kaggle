{"cell_type":{"962668be":"code","cef8f18f":"code","63fb6f4b":"code","f160b97e":"code","935a7171":"code","77529891":"code","3590aa97":"code","c3bd8092":"code","2d287b92":"code","d1749e6d":"code","f2c80050":"code","024a0f20":"code","36c3ca1b":"code","cc1e4518":"code","e629deea":"code","21ad766f":"code","2b444efc":"code","f891c5a6":"code","1311cd69":"code","17b59ff4":"code","6f26408d":"code","865f4246":"code","a76488c3":"code","bd0238da":"code","68096dfe":"code","5a6432b1":"code","f4201576":"code","612b4ff1":"code","46f6bd0b":"code","3c9a0980":"markdown"},"source":{"962668be":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport cv2\nfrom tqdm import tqdm\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, log_loss, accuracy_score\nfrom sklearn.model_selection import train_test_split","cef8f18f":"directory = '..\/input\/biggest-genderface-recognition-dataset\/faces'","63fb6f4b":"Name=[]\nfor file in os.listdir(directory):\n    Name+=[file]\nprint(Name)\nprint(len(Name))","f160b97e":"N=list(range(len(Name)))\nnormal_mapping=dict(zip(Name,N)) \nreverse_mapping=dict(zip(N,Name)) ","935a7171":"!pip install mtcnn\nfrom mtcnn.mtcnn import MTCNN","77529891":"detector = MTCNN()","3590aa97":"def mtcnn_detector(image): \n    face_location = detector.detect_faces(image)\n    for face in zip(face_location): \n        x_coordinate,y_coordinate,width,height=face[0]['box']\n        image=image[(y_coordinate):(y_coordinate+height),(x_coordinate):(x_coordinate+width)]\n    return image","c3bd8092":"path0='..\/input\/biggest-genderface-recognition-dataset\/faces\/woman\/woman_1016.jpg'\nimage=cv2.imread(path0)\nprint(image.shape)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))","2d287b92":"image2=mtcnn_detector(image) \nprint(image2.shape)\nplt.imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB))","d1749e6d":"image3=cv2.resize(image2,(60,60))\nprint(image3.shape)\nplt.imshow(cv2.cvtColor(image3, cv2.COLOR_BGR2RGB))","f2c80050":"dataset=[]\ndatacount=[]\ncount=0\nfor file in os.listdir(directory):\n    path=os.path.join(directory,file)\n    for im in tqdm(os.listdir(path)):\n        image=cv2.imread(os.path.join(path,im))\n        image2=mtcnn_detector(image)\n        if image2.shape[0]>10 and image2.shape[1]>10:\n            image3=cv2.resize(image2,(60,60))\n            dataset+=[image3]\n            datacount+=[count]\n    count+=1","024a0f20":"n=len(dataset)\nprint(n)\nD=list(range(n))\nrandom.seed(2021)\nrandom.shuffle(D)","36c3ca1b":"trainY=np.array(datacount)[D[0:(n\/\/4)*3]]\ntestY=np.array(datacount)[D[(n\/\/4)*3:]]\ntrainX=np.array(dataset)[D[0:(n\/\/4)*3]]\ntestX=np.array(dataset)[D[(n\/\/4)*3:]]","cc1e4518":"print(trainX.shape)\nprint(testX.shape)\nprint(trainY.shape)\nprint(testY.shape)","e629deea":"labels1=to_categorical(trainY)\ntrainY2=np.array(labels1)","21ad766f":"tlabels1=to_categorical(testY)\ntestY2=np.array(tlabels1)","2b444efc":"trainx,testx,trainy,testy=train_test_split(trainX,trainY2,test_size=0.2,random_state=44)","f891c5a6":"print(trainx.shape)\nprint(testx.shape)\nprint(trainy.shape)\nprint(testy.shape)","1311cd69":"datagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=True,rotation_range=20,zoom_range=0.2,\n                        width_shift_range=0.2,height_shift_range=0.2,shear_range=0.1,fill_mode=\"nearest\")","17b59ff4":"pretrained_model3 = tf.keras.applications.DenseNet201(input_shape=(60,60,3),include_top=False,weights='imagenet',pooling='avg')\npretrained_model3.trainable = False","6f26408d":"inputs3 = pretrained_model3.input\nx3 = tf.keras.layers.Dense(128, activation='relu')(pretrained_model3.output)\noutputs3 = tf.keras.layers.Dense(len(Name), activation='softmax')(x3)\nmodel = tf.keras.Model(inputs=inputs3, outputs=outputs3)","865f4246":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","a76488c3":"his=model.fit(datagen.flow(trainx,trainy,batch_size=32),validation_data=(testx,testy),epochs=30)","bd0238da":"y_pred=model.predict(testX)\npred=np.argmax(y_pred,axis=1)\nground=testY\nprint(classification_report(ground,pred))","68096dfe":"get_acc = his.history['accuracy']\nvalue_acc = his.history['val_accuracy']\nget_loss = his.history['loss']\nvalidation_loss = his.history['val_loss']\n\nepochs = range(len(get_acc))\nplt.plot(epochs, get_acc, 'r', label='Accuracy of Training data')\nplt.plot(epochs, value_acc, 'b', label='Accuracy of Validation data')\nplt.title('Training vs validation accuracy')\nplt.legend(loc=0)\nplt.figure()\nplt.show()","5a6432b1":"epochs = range(len(get_loss))\nplt.plot(epochs, get_loss, 'r', label='Loss of Training data')\nplt.plot(epochs, validation_loss, 'b', label='Loss of Validation data')\nplt.title('Training vs validation loss')\nplt.legend(loc=0)\nplt.figure()\nplt.show()","f4201576":"pred2=model.predict(testX)\n\nPRED=[]\nfor item in pred2:\n    value2=np.argmax(item)      \n    PRED+=[value2]","612b4ff1":"ANS=testY","46f6bd0b":"accuracy=accuracy_score(ANS,PRED)\nprint(accuracy)","3c9a0980":"# Face Detect and Extract"}}