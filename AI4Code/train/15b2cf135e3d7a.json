{"cell_type":{"f333dd88":"code","b9ea5468":"code","4810f592":"code","2f4a7a61":"code","2d53ea5d":"code","3adc939c":"code","8751a4fd":"code","4d784fb9":"code","493425a4":"code","48805a04":"code","31c277ca":"code","6bcf285d":"code","a28e6e9d":"code","7b0b986c":"code","5c041a77":"code","d8eb1677":"code","37859c46":"markdown","0b777061":"markdown","424c2a02":"markdown","7bdc4736":"markdown","148bfd11":"markdown"},"source":{"f333dd88":"from PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Conv2D, Activation, MaxPool2D, BatchNormalization, Flatten, Dense, Dropout\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import SGD\n\nimport glob\nimport cv2\nimport os","b9ea5468":"path = '..\/input\/fruits\/fruits-360\/'\ntraining_dir = path + 'Training\/'\nvalidation_dir = path + 'Test\/'\ntest_dir = path + 'test-multiple_fruits\/'","4810f592":"plt.figure(figsize=(20,10))\nplt.axis('off')\nplt.imshow(np.array(Image.open(\"..\/input\/fruits360\/vgg-16.png\")))","2f4a7a61":"def create_model():\n    # Instantiate an empty sequential model\n    model = Sequential()\n\n    # block\n    model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same', input_shape=(64,64, 3)))\n    model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n    model.add(MaxPool2D((2,2), strides=(2,2)))\n\n    # block\n    model.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n    model.add(MaxPool2D((2,2), strides=(2,2)))\n\n    # block\n    model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n    model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n    model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n    model.add(MaxPool2D((2,2), strides=(2,2)))\n\n    # block\n    model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n    model.add(MaxPool2D((2,2), strides=(2,2)))\n\n    # block\n    model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n    model.add(MaxPool2D((2,2), strides=(2,2)))\n\n    # block #6 (classifier)\n    model.add(Flatten())\n    model.add(Dense(4096, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(4096, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(131, activation='softmax'))\n\n    # reduce learning rate by 0.1 when the validation error plateaus\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1))\n\n    # set the SGD optimizer with lr of 0.01 and momentum of 0.9\n    optimizer = SGD(lr = 0.01, momentum = 0.9)\n\n    # compile the model\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    \n    return model","2d53ea5d":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import preprocess_input\n\ntraining_datagen = ImageDataGenerator(\n                                    rescale=1.\/255,\n                                    shear_range=0.2, \n                                    zoom_range=0.2,\n                                    horizontal_flip=True,\n                                    preprocessing_function=preprocess_input)\n\nvalidation_datagen = ImageDataGenerator(rescale = 1.\/255, preprocessing_function=preprocess_input)","3adc939c":"IMAGE_SIZE = [64, 64]","8751a4fd":"training_generator = training_datagen.flow_from_directory(training_dir, target_size = IMAGE_SIZE, batch_size = 200, class_mode = 'categorical')\nvalidation_generator = validation_datagen.flow_from_directory(validation_dir, target_size = IMAGE_SIZE, batch_size = 200, class_mode = 'categorical')","4d784fb9":"training_images = 37836\nvalidation_images = 12709","493425a4":"checkpoint_path = '..\/input\/fruits360\/fruits.ckpt'\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# Create a callback that saves the model's weights\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_weights_only=True,\n                                                 save_best_only=True,\n                                                 verbose=1)","48805a04":"def train(model):\n    model.fit(training_generator,\n               steps_per_epoch = training_images,\n               epochs = 2,\n               validation_data = validation_generator,\n               validation_steps = validation_images,\n               callbacks=[cp_callback])\n    return model","31c277ca":"def load(model):\n    model.load_weights(checkpoint_path)\n    return model","6bcf285d":"# train or load model\nmodel = create_model()\n\n#model = train(model)\nmodel = load(model)\n\nloss,acc = model.evaluate(validation_generator, verbose=2)\nprint(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))","a28e6e9d":"idx_to_name = {x:i for (x,i) in enumerate(training_generator.class_indices)}\n\ndef predict(img):\n    to_predict = np.zeros(shape=training_generator[0][0].shape)\n    to_predict[0] = img\n    \n    return idx_to_name[np.argmax(model(to_predict)[0])]","7b0b986c":"img = cv2.imread('..\/input\/fruits\/fruits-360\/Training\/Banana\/0_100.jpg')\nresized = cv2.resize(img, (64,64), interpolation = cv2.INTER_AREA) ","5c041a77":"predict(resized)","d8eb1677":"plt.imshow(resized)","37859c46":"<h1 id=\"predict\" style=\"color:orange; background:green; border:0.5px dashed white;\"> \n    <center>Predict\n        <a class=\"anchor-link\" href=\"#predict\">\u00b6<\/a>\n    <\/center>\n<\/h1>","0b777061":"<div>\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/5857\/8713\/df2a86f4b6ecfff7996be179a4e8ebdf\/dataset-cover.jpg\"\/>\n<\/div>","424c2a02":"<h1 id=\"dataset\" style=\"color:orange; background:green; border:0.5px dashed white;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\">\u00b6<\/a>\n    <\/center>\n<\/h1>\n\nGoal of this notebook is to show the VGG-16 layers implementation with Tensorflow Keras.<br>\nUse VGG-16 layers to train and detect Fruits.","7bdc4736":"<h1 id=\"implementation\" style=\"color:orange; background:green; border:0.5px dashed white;\"> \n    <center>Implementation\n        <a class=\"anchor-link\" href=\"#implementation\">\u00b6<\/a>\n    <\/center>\n<\/h1>","148bfd11":"<h1 id=\"architecture\" style=\"color:orange; background:green; border:0.5px dashed white;\"> \n    <center>Architecture\n        <a class=\"anchor-link\" href=\"#architecture\">\u00b6<\/a>\n    <\/center>\n<\/h1>"}}