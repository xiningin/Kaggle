{"cell_type":{"f630c3be":"code","08f1b76b":"code","e8217550":"code","49e802e8":"code","92052d07":"code","da98ef39":"code","e079d3d1":"code","6c1a8b9a":"code","681d0f10":"code","bf1079c9":"code","962d4590":"code","b5406cb0":"code","42fc1055":"code","a16e8b47":"code","b0aa2cd2":"code","f576f073":"code","e2a21a38":"code","29091050":"code","e0cef0b5":"code","cd1f56f0":"code","ff4cf299":"code","c86d4582":"code","e8e8e91c":"code","db76e181":"code","73e2d7d1":"code","46b2e597":"markdown","260b25d1":"markdown","e4e66d89":"markdown","faadd31c":"markdown","7da1d7ed":"markdown","521da90b":"markdown","81406e22":"markdown","daaabb5c":"markdown","d63ebc0d":"markdown","8e05b1fe":"markdown","2cdd1dd7":"markdown","4f3d1808":"markdown","7aa17836":"markdown","32ff2956":"markdown","ae83b742":"markdown","de1a8861":"markdown","01c61930":"markdown","fcb3dbdf":"markdown","89934067":"markdown","4852e374":"markdown","705f157e":"markdown","e0453820":"markdown","0c06ac5c":"markdown","c07f321e":"markdown","14af0681":"markdown","2c96b1b8":"markdown","bed078d2":"markdown","0bb89885":"markdown","036f19b1":"markdown"},"source":{"f630c3be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","08f1b76b":"# Data\nx_data = [1, 2, 3, 4, 5]\ny_data = [1, 2, 3, 4, 5]\n\nW = tf.Variable(2.9)\nb = tf.Variable(0.5)\n\n# \uac00\uc124 = W * x + b\nh = W * x_data + b","e8217550":"# \uac00\uc124\uc5d0\uc11c y_data\ub97c \ube80 \uac12\uc744 \uc81c\uacf1(tf.square)\ud55c \ub4a4, \uc774\uc758 \ud3c9\uade0(tf.reduce_meam)\uc744 \uad6c\ud55c\ub2e4.\ncost = tf.reduce_mean(tf.square(h - y_data))","49e802e8":"# Learning_rate \uac12 \uc124\uc815 -> \uad6c\ud55c \uae30\uc6b8\uae30 \uac12(W_grad \ub610\ub294 b_grad)\uc744 \uc5bc\ub9cc\ud07c \ubc18\uc601\ud560\uc9c0 \uacb0\uc815\ud558\ub294 \uac12\uc774\ub2e4.\nlearning_rate = 0.01\n\n# Gradient descent\nwith tf.GradientTape() as tape:\n    h = W * x_data + b\n    cost = tf.reduce_mean(tf.square(h - y_data))\n\nW_grad, b_grad = tape.gradient(cost, [W, b])\n\n# A.assign_sub(B)\ub294 A = A - B\uc640 \uac19\uc740 \uae30\ub2a5\uc744 \ud55c\ub2e4. \nW.assign_sub(learning_rate * W_grad)\nb.assign_sub(learning_rate * b_grad)","92052d07":"x_data = [1, 2, 3, 4, 5]\ny_data = [1, 2, 3, 4, 5]\n\nW = tf.Variable(2.9)\nb = tf.Variable(0.5)\n\n# Learning_rate \uac12 \uc124\uc815\n# \uad6c\ud55c \uae30\uc6b8\uae30 \uac12(W_grad \ub610\ub294 b_grad)\uc744 \uc5bc\ub9cc\ud07c \ubc18\uc601\ud560\uc9c0 \uacb0\uc815\ud558\ub294 \uac12\uc774\ub2e4.\nlearning_rate = 0.01\n\nfor i in range(100+1): # W, b  update\n    # Gradient descent\n    with tf.GradientTape() as tape:\n        h = W * x_data + b\n        cost = tf.reduce_mean(tf.square(h - y_data))\n\n    W_grad, b_grad = tape.gradient(cost, [W, b])\n\n    # A.assign_sub(B)\ub294 A = A - B\uc640 \uac19\uc740 \uae30\ub2a5\uc744 \ud55c\ub2e4. \n    W.assign_sub(learning_rate * W_grad)\n    b.assign_sub(learning_rate * b_grad)\n    \n    # \uc2dc\ub3c4 10\ubc88 \uc911 \ud55c \ubc88\uc529 W \uac12\uacfc b \uac12\uc744 \ud45c\uc2dc\ud55c\ub2e4.\n    if i % 10 == 0:\n        print(\"{:5}|{:10.4f}|{:10.4}|{:10.6f}\".format(i, W.numpy(), b.numpy(), cost))","da98ef39":"X = np.array([1, 2, 3])\nY = np.array([1, 2, 3])\n\n# cost(W) = (Wxi - yi)^2\uc758 \ud3c9\uade0\uac12\uc744 \uad6c\ud558\ub294 \ud568\uc218\ndef cost_func(W, X, Y):\n    c = 0\n    for i in range(len(X)):\n        c += (W * X[i] - Y[i]) ** 2\n    return c \/ len(X)\n\n# W\uc5d0 -3 ~ 5 \uc0ac\uc774\uc758 \uac12\uc744 \uc785\ub825\ud55c cost(W)\uc758 \uac12\uc744 15\uad6c\uac04\uc73c\ub85c \ucd9c\ub825  \nfor feed_W in np.linspace(-3, 5, num=15):\n    curr_cost = cost_func(feed_W, X, Y)\n    print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))\n    \n","e079d3d1":"X = np.array([1, 2, 3])\nY = np.array([1, 2, 3])\n\n# cost(W) = (Wxi - yi)^2\uc758 \ud3c9\uade0\uac12\uc744 \uad6c\ud558\ub294 \ud568\uc218\ndef cost_func(W, X, Y):\n    hypothesis = X * W\n    return tf.reduce_mean(tf.square(hypothesis - Y))\n\n\n# W\uc5d0 -3 ~ 5 \uc0ac\uc774\uc758 \uac12\uc744 \uc785\ub825\ud55c cost(W)\uc758 \uac12\uc744 15\uad6c\uac04\uc73c\ub85c \ucd9c\ub825  \nW_values = np.linspace(-3, 5, num=15)\ncost_values = []\n\nfor feed_W in W_values:\n    curr_cost = cost_func(feed_W, X, Y)\n    cost_values.append(curr_cost)\n    print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))\n","6c1a8b9a":"# learning rate = 0.01\nalpha = 0.01\n\n# \ube44\uc6a9 \ud568\uc218\uc758 \ud604\uc7ac \uae30\uc6b8\uae30\ngradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))\n\n# \uacbd\uc0ac\ud558\uac15 \ud6c4 \uae30\uc6b8\uae30\ndescent = W - tf.multiply(alpha, gradient)\n\n# W\uc758 \uac12 \uc5c5\ub370\uc774\ud2b8\nW.assign(descent)","681d0f10":"tf.random.set_seed(0) # \ub098\uc911\uc5d0 \ubcf8 \ucf54\ub4dc\ub97c \ub2e4\uc2dc \uc218\ud589\ud588\uc744 \ub54c\uc5d0\ub3c4 \ub3d9\uc77c\ud558\uace0 \ub611\uac19\uc774 \uc9c4\ud589\ub420 \uc218 \uc788\ub3c4\ub85d \uc124\uc815\n\nx_data = [1., 2., 3., 4.]\ny_data = [1., 3., 5., 7.]\n\nW = tf.Variable(tf.random.normal([1], -100., 100.))\n\nfor step in range(300):\n    h = W * x_data\n    cost = tf.reduce_mean(tf.square(h - y_data))\n    \n    alpha = 0.01\n    grad = tf.reduce_mean(tf.multiply(tf.multiply(W, x_data) - y_data, x_data))\n    descent = W - tf.multiply(alpha, grad)\n    W.assign(descent)\n    \n    if step % 10 == 0:\n        print('{:5} | {:10.4f} | {:10.6f}'.format(step, cost.numpy(), W.numpy()[0]))\n","bf1079c9":"# \ub370\uc774\ud130 \ubc0f \uc2e4\uc81c \uacb0\uacfc\uac12\nx1 = [73., 93., 89., 96., 73.]\nx2 = [80., 88., 91., 98., 66.]\nx3 = [75., 93., 90., 100., 70.]\nY = [152., 185., 180., 196., 142.]\n\n# \uac00\uc911\uce58(Weights)\nw1 = tf.Variable(10.)\nw2 = tf.Variable(10.)\nw3 = tf.Variable(10.)\nb = tf.Variable(10.)\n\nh = w1 * x1 + w2 * x2 + w3 * x3 + b","962d4590":"# \ub370\uc774\ud130 \ubc0f \uc2e4\uc81c \uacb0\uacfc\uac12(Label)\ndata = np.array([\n    # X1,  X2,   X3,   y\n    [ 73., 80.,  75., 152. ],\n    [ 93., 88.,  93., 185. ],\n    [ 89., 91.,  90., 180. ],\n    [ 96., 98., 100., 196. ],\n    [ 73., 66.,  70., 142. ],\n], dtype=np.float32)\n\nX = data[:, :-1]\ny = data[:, [-1]]\n\n\n# \uac00\uc911\uce58(Weights)\nW = tf.Variable(tf.random.normal([3, 1]))\nb = tf.Variable(tf.random.normal([1]))\n\nlearning_rate = 0.000001\n\n# \uac00\uc124, \uc608\uc0c1 \ud568\uc218\ndef predict(X):\n    return tf.matmul(X, W) + b\n\nn_epochs = 2000\n\nfor i in range(n_epochs + 1):\n    # tf.GradientTape(): \ube44\uc6a9 \ud568\uc218\uc758 \uae30\uc6b8\uae30\ub97c \uae30\ub85d\n    with tf.GradientTape() as tape:\n        cost = tf.reduce_mean(tf.square(predict(X) - y))\n        \n    # \ube44\uc6a9\uc758 \uae30\uc6b8\uae30 \uacc4\uc0b0\n    W_grad, b_grad = tape.gradient(cost, [W, b])\n    \n    # W, b\uc758 \uac12 \uc5c5\ub370\uc774\ud2b8\n    W.assign_sub(learning_rate * W_grad)\n    b.assign_sub(learning_rate * b_grad)\n    \n    if i % 100 == 0:\n        print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))","b5406cb0":"# \ub370\uc774\ud130 \uc120\uc5b8\nx_train = [[1., 2.], [2., 3.], [3., 1.], [4., 3.], [5., 3.], [6., 2.]]\ny_train = [[0.], [0.], [0.], [1.], [1.], [1.]]\n\n# \ud14c\uc2a4\ud2b8\uc6a9 \ub370\uc774\ud130 \uc120\uc5b8\nx_test = [[5., 2.]]\ny_test = [[1.]]\n\nx1 = [x[0] for x in x_train]\nx2 = [x[1] for x in x_train]\n\ncolors = [int(y[0] % 3) for y in y_train]\nplt.scatter(x1, x2, c=colors, marker='^')\nplt.scatter(x_test[0][0], x_test[0][1], c=\"red\")\n\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.show()\n\n","42fc1055":"# \ub370\uc774\ud130 \uc120\uc5b8\nx_train = [[1., 2.], [2., 3.], [3., 1.], [4., 3.], [5., 3.], [6., 2.]]\ny_train = [[0.], [0.], [0.], [1.], [1.], [1.]]\n\n# \ud14c\uc2a4\ud2b8\uc6a9 \ub370\uc774\ud130 \uc120\uc5b8\nx_test = [[5., 2.]]\ny_test = [[1.]]\n\n# \ub370\uc774\ud130 \uc14b \uc124\uc815\ndataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n\n# W, b \uac12 \uc124\uc815\nW = tf.Variable(tf.zeros([2, 1]), name = 'weight')\nb = tf.Variable(tf.zeros([1]), name = 'bias')\n\n# \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0 \ud568\uc218 \uad6c\ud604\ndef logistic_regression(features):\n    h = tf.divide(1., 1. + tf.exp(-tf.matmul(features, W) + b))\n    return h\n\n# \ube44\uc6a9 \ud568\uc218 \uad6c\ud604\ndef loss_fn(h, labels):\n    cost = -tf.reduce_mean(labels * tf.math.log(h) + (1 - labels) * tf.math.log(1 - h))\n    return cost\n\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n\n# \uc608\uc0c1 \uacb0\uacfc\uac12(predicted)\uacfc \uc2e4\uc81c \uac12(labels)\uc758 \uc77c\uce58 \uc815\ub3c4(accuracy)\ub97c \uacc4\uc0b0\ndef accuracy_fn(h, labels):\n    predicted = tf.cast(h > 0.5, dtype=tf.float32)\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\n    return accuracy\n\ndef grad(features, labels):\n    with tf.GradientTape() as tape:\n        h = logistic_regression(features)\n        loss_value = loss_fn(h, labels)\n    return tape.gradient(loss_value, [W, b])\n\nEPOCHS = 1001\nfor step in range(EPOCHS):\n    for features, labels in iter(dataset.batch(len(x_train))):\n        h = logistic_regression(features)\n        grads = grad(features, labels)\n        optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b]))\n        if step % 100 == 0:\n            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(h, features)))\n\ntest_acc = accuracy_fn(logistic_regression(x_test), y_test)\nprint(\"Test Result = {}\".format(tf.cast(logistic_regression(x_test) > 0.5, dtype=tf.int32)))\nprint(\"Test set Accuracy: {:.4f}\".format(test_acc))","a16e8b47":"x_data = [[1, 2, 1, 1],\n          [2, 1, 3, 2],\n          [3, 1, 3, 4],\n          [4, 1, 5, 5],\n          [1, 7, 5, 5],\n          [1, 2, 5, 6],\n          [1, 6, 6, 6],\n          [1, 7, 7, 7]]\n\ny_data = [[0, 0, 1],\n          [0, 0, 1],\n          [0, 0, 1],\n          [0, 1, 0],\n          [0, 1, 0],\n          [0, 1, 0],\n          [1, 0, 0],\n          [1, 0, 0]]\n\n# numpy\uc640 float \ud3ec\ub9f7\uc73c\ub85c \ubcc0\ud658\nx_data = np.asarray(x_data, dtype=np.float32)\ny_data = np.asarray(y_data, dtype=np.float32)\n\n# \ub808\uc774\ube14\uc758 \ud074\ub798\uc2a4 \uc218 \nnb_classes = 3","b0aa2cd2":"# w, b \uac12 \uc124\uc815\nW = tf.Variable(tf.random.normal([4, nb_classes]), name=\"weight\")\nb = tf.Variable(tf.random.normal([nb_classes]), name=\"bias\")\nvariables = [W, b]\n\n# \uac00\uc124 \uc815\uc758\ndef hypothesis(X):\n    return tf.nn.softmax(tf.matmul(X,W)+b)\n","f576f073":"# \ube44\uc6a9 \ud568\uc218 \uad6c\ud604\ndef cost_fn(X, Y):\n    logits = hypothesis(X)\n    cost = -tf.reduce_sum(Y * tf.log(logits), axis=1)\n    cost_mean = tf.reduce_mean(cost)\n    return cost_mean\n\n# \uacbd\uc0ac\ud558\uac15 \ud568\uc218 \uad6c\ud604\ndef grad_fn(X, Y):\n    with tf.GradientTape() as tape:\n        cost = cost_fn(X, Y)\n        grads = tape.gradient(cost, variables)\n        return grads","e2a21a38":"# \ud2b8\ub808\uc774\ub2dd \ud568\uc218 \uad6c\ud604\ndef fit(X, Y, epochs=2000, verbose=100):\n    optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n    for i in range(epochs):\n        grads = grad_fn(X, Y)\n        optimizer.apply_gradients(zip(grads, variables))\n        if (i==0) | ((i+1)%verbose==0):\n            print('Loss at epoch %d: %f' %(i+1, cost_fn(X,Y).numpy()))","29091050":"# \uc0d8\ud50c \ub370\uc774\ud130\uc14b \uad6c\uc131\nx_data = [[1, 2, 1, 1],\n          [2, 1, 3, 2],\n          [3, 1, 3, 4],\n          [4, 1, 5, 5],\n          [1, 7, 5, 5],\n          [1, 2, 5, 6],\n          [1, 6, 6, 6],\n          [1, 7, 7, 7]]\n\ny_data = [[0, 0, 1],\n          [0, 0, 1],\n          [0, 0, 1],\n          [0, 1, 0],\n          [0, 1, 0],\n          [0, 1, 0],\n          [1, 0, 0],\n          [1, 0, 0]]\n\n# numpy\uc640 float \ud3ec\ub9f7\uc73c\ub85c \ubcc0\ud658\nx_data = np.asarray(x_data, dtype=np.float32)\ny_data = np.asarray(y_data, dtype=np.float32)\n\n# \ub808\uc774\ube14\uc758 \ud074\ub798\uc2a4 \uc218 \nnb_classes = 3\n\n\n# w, b \uac12 \uc124\uc815\nW = tf.Variable(tf.random.normal([4, nb_classes]), name=\"weight\")\nb = tf.Variable(tf.random.normal([nb_classes]), name=\"bias\")\nvariables = [W, b]\n\n# \uac00\uc124 \uc815\uc758\ndef hypothesis(X):\n    return tf.nn.softmax(tf.matmul(X,W)+b)\n\n# \ube44\uc6a9 \ud568\uc218 \uad6c\ud604\ndef cost_fn(X, Y):\n    logits = hypothesis(X)\n    cost = -tf.reduce_sum(Y * tf.math.log(logits), axis=1)\n    cost_mean = tf.reduce_mean(cost)\n    return cost_mean\n\n# \uacbd\uc0ac\ud558\uac15 \ud568\uc218 \uad6c\ud604\ndef grad_fn(X, Y):\n    with tf.GradientTape() as tape:\n        cost = cost_fn(X, Y)\n        grads = tape.gradient(cost, variables)\n        return grads\n\n# \ud2b8\ub808\uc774\ub2dd \ud568\uc218 \uad6c\ud604\ndef fit(X, Y, epochs=2000, verbose=100):\n    optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n    for i in range(epochs):\n        grads = grad_fn(X, Y)\n        optimizer.apply_gradients(zip(grads, variables))\n        if (i==0) | ((i+1)%verbose==0):\n            print('Loss at epoch %d: %f' %(i+1, cost_fn(X,Y).numpy()))\n\n# \ud2b8\ub808\uc774\ub2dd \uc2e4\ud589\nfit(x_data, y_data)","e0cef0b5":"# \ub370\uc774\ud130\uc14b \ub85c\ub4dc\nxy = np.loadtxt('\/kaggle\/input\/data04zoo\/data-04-zoo.csv', delimiter=',', dtype=np.float32)\nx_data = xy[:, :-1]\ny_data = xy[:, [-1]]\n\n# \ub77c\ubca8 \ud074\ub798\uc2a4 \uac2f\uc218\nnb_classes = 7\n\n# y_data\ub97c one-hot \ud615\ud0dc\ub85c \ubcc0\uacbd\ny_data = tf.dtypes.cast(y_data, tf.int32)\nY_one_hot = tf.one_hot(list(y_data), nb_classes)\nY_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n\nprint(x_data.shape, Y_one_hot.shape)\n\n# W\uc640 b \uac12 \uc124\uc815\nW = tf.Variable(tf.random.normal([16, nb_classes]), name='weight')\nb = tf.Variable(tf.random.normal([nb_classes]), name='bias')\nvariables = [W, b]\n\n# softmax_cross_entropy_with_logits \uc0ac\uc6a9\ud558\uc5ec \ube44\uc6a9 \ud568\uc218 \uad6c\ud604\ndef logit_fn(X):\n    return tf.matmul(X, W) + b\n\ndef hypothesis(X):\n    return tf.nn.softmax(logit_fn(X))\n\ndef cost_fn(X, Y):\n    logits = logit_fn(X)\n    cost_i = tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y)\n    cost = tf.reduce_mean(cost_i)\n    return cost\n\n# \ube44\uc6a9 \ud568\uc218\uc758 \uae30\uc6b8\uae30 \uae30\ub85d \ud568\uc218 \uad6c\ud604\ndef grad_fn(X, Y):\n    with tf.GradientTape() as tape:\n        loss = cost_fn(X, Y)\n        grads = tape.gradient(loss, variables)\n        return grads\n\n# \uc608\uc0c1 \uac12\uc758 \uc815\ud655\ub3c4 \uacc4\uc0b0 \ud568\uc218 \uad6c\ud604\ndef prediction(X, Y):\n    pred = tf.argmax(hypothesis(X), 1)\n    correct_prediction = tf.equal(pred, tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    return accuracy\n\n# softmax \uae30\ub2a5\uc744 \ud559\uc2b5\ud558\ub294 \ud568\uc218 \uad6c\ud604\ndef fit(X, Y, epochs=1000, verbose=50):\n    optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n    \n    for i in range(epochs):\n        grads = grad_fn(X, Y)\n        optimizer.apply_gradients(zip(grads, variables))\n        if (i==0) | ((i+1)%verbose==0):\n            # \uc815\ud655\ub3c4 \ucd9c\ub825\n            acc = prediction(X, Y).numpy()\n            # \ube44\uc6a9 \ucd9c\ub825\n            loss = cost_fn(X, Y).numpy()\n            print(\"Steps: {} Loss: {}, Acc: {}\".format(i+1, loss, acc))\n\nfit(x_data, Y_one_hot)","cd1f56f0":"# Exponential Decay(\uc9c0\uc218\uc801 \uac10\uc18c) \n# \ud150\uc11c\ud50c\ub85c\uc6b0 \ucf54\ub4dc\nlearning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 1000, 0.96, staircase=true)\n\n# \ud30c\uc774\uc36c \ucf54\ub4dc\ndef exponential_decay(epoch):\n    starter_rate = 0.01\n    k = 0.96\n    exp_rate = starter_rate * exp(-k*t)\n    return exp_rate\n","ff4cf299":"# \ud45c\uc900\ud654 \uae30\ubc95(Standardization: Mean Distance) \ud30c\uc774\uc36c, numpy \uc0ac\uc6a9\nStandardization = (data - np.mean(data)) \/ sqrt(np.sum((data - np.mean(data))^2) \/ np.count(data))\n\n# \uc815\uaddc\ud654 \uae30\ubc95(Normalization: 0 ~ 1) \ud30c\uc774\uc36c, numpy \uc0ac\uc6a9\nNormalization = (data - np.min(data, 0)) \/ (np.max(data, 0)) - np.min(data, 0)\n","c86d4582":"# \ud150\uc11c\ud50c\ub85c\uc6b0\ub85c Regularization \uad6c\ud604\nL2_loss = tf.nn.l2_loss(w)","e8e8e91c":"xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n               [816, 820.958984, 1008100, 815.48999, 819.23999],\n               [819.359985, 823, 1188100, 818.469971, 818.97998],\n               [819, 823, 1198100, 816, 820.450012],\n               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n\n\n\n# \uc815\uaddc\ud654 \ud568\uc218 \uad6c\ud604 (\ub370\uc774\ud130 \uc804\ucc98\ub9ac)\ndef normalization(data):\n    numerator = data - np.min(data, 0)\n    denominator = np.max(data, 0) - np.min(data, 0)\n    return numerator \/ denominator\n\nxy = normalization(xy)\nx_train = xy[:, 0:-1]\ny_train = xy[:, [-1]]\nprint(xy)\n\n# \ub370\uc774\ud130\uc14b \uc120\uc5b8\ndataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))\n\n# W, b \uc120\uc5b8\nW = tf.Variable(tf.random.normal([4, 1]), dtype=tf.float32)\nb = tf.Variable(tf.random.normal([1]), dtype=tf.float32)\n\n# \uc120\ud615 \ud68c\uadc0 \uac00\uc124 \ud568\uc218 \uad6c\ud604\ndef linearReg_fn(features):\n    h = tf.matmul(features, W) + b\n    return h\n\n# L2 Regularization \ud568\uc218 \uad6c\ud604 (\uc624\ubc84\ud53c\ud305 \ubb38\uc81c \ud574\uacb0)\ndef l2_loss(loss, beta = 0.01):\n    W_reg = tf.nn.l2_loss(W)\n    loss = tf.reduce_mean(loss + W_reg * beta)\n    return loss\n\n# \ube44\uc6a9 \ud568\uc218 \uad6c\ud604\ndef loss_fn(h, labels, flag = False):\n    cost = tf.reduce_mean(tf.square(h - labels))\n    if(flag):\n        cost = l2_loss(cost)\n    return cost\n\n# \ucd08\uae30 \ud559\uc2b5\ub960 \uc124\uc815\nstarter_learning_rate = 0.1\n\n# \ucd08\uae30 Exponential Decay \uc801\uc6a9 \uc5ec\ubd80 \uc124\uc815 (\ud559\uc2b5\ub960 \uc870\uc808 \uae30\ubc95)\nis_decay = True\n\n# Exponential Decay \uc801\uc6a9 \uc870\uac74\ubb38 \uad6c\ud604 (\ud559\uc2b5\ub960 \uc870\uc808 \uae30\ubc95)\nif(is_decay):\n    global_step = tf.Variable(0, trainable=False)\n    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=starter_learning_rate, \n                                                                   decay_steps=50,\n                                                                   decay_rate=0.96,\n                                                                   staircase=True)\n    optimizer = tf.keras.optimizers.SGD(learning_rate)\n\nelse:\n    optimizer = tf.keras.optimizers.SGD(learning_rate=starter_learning_rate)\n\n# \uac00\uc124\uacfc \uc2e4\uc81c \uac12\uc744 \ube44\uad50\ud558\uba70 \ube44\uc6a9 \ud568\uc218\uc758 \uae30\uc6b8\uae30 \uac12\uc744 \uacc4\uc0b0\ud558\uace0 L2 Regularization\uc744 \uc801\uc6a9\ud560\uc9c0\uc758 \uc5ec\ubd80\ub97c \uacb0\uc815\ud558\ub294 \ud568\uc218 \uad6c\ud604 \ndef grad(features, labels, l2_flag):\n    with tf.GradientTape() as tape:\n        loss_value = loss_fn(linearReg_fn(features), labels, l2_flag)\n    return tape.gradient(loss_value, [W, b]), loss_value\n\nEPOCHS = 101\n\n# \ubaa8\ub378 \ud559\uc2b5 \uacfc\uc815 \uad6c\ud604\nfor step in range(EPOCHS):\n    for features, labels in dataset:\n        features = tf.cast(features, tf.float32)\n        labels = tf.cast(labels, tf.float32)\n        grads, loss_value = grad(features, labels, False)\n        optimizer.apply_gradients(grads_and_vars=zip(grads, [W,b]))\n        if step % 10 == 0:\n            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_value))\n\n","db76e181":"# keras \ub0b4 fashion_mnist \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c fashion_mnist\ub85c \uc120\uc5b8 \ud6c4 \ub370\uc774\ud130 \uac00\uc838\uc624\uae30\nfashion_mnist = keras.datasets.fashion_mnist\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\n# \ud074\ub798\uc2a4 10\uac1c \uc815\uc758\nclass_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\ntrain_images = train_images \/ 255.0\ntest_images = test_images \/ 255.0\n\n# \ubaa8\ub378 \uc120\uc5b8\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28, 28)),\n    keras.layers.Dense(128, activation=tf.nn.relu),\n    keras.layers.Dense(10, activation=tf.nn.softmax)\n])\n\n# \ubaa8\ub378\uc758 optimizer \uc120\uc5b8, \ube44\uc6a9 \uac12 \uc815\uc758, \uc815\ud655\ub3c4 \uce21\uc815 \ubc29\uc2dd \uacb0\uc815\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# \ubaa8\ub378 \ud6c8\ub828\nmodel.fit(train_images, train_labels, epochs=5)\ntest_loss, test_acc = model.evaluate(test_images, test_labels)\npredictions = model.predict(test_images)\nnp.argmax(predictions[0])\n","73e2d7d1":"# keras \ub0b4 imdb \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c imdb\ub85c \uc120\uc5b8 \ud6c4 \ub370\uc774\ud130 \uac00\uc838\uc624\uae30\nimdb = keras.datasets.imdb\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n# print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))\n# print(train_data[0])\n\n# \uc790\uc5f0\uc5b4 \uc804\ucc98\ub9ac \uc900\ube44\nword_index = imdb.get_word_index()\n\nword_index = {k:(v+3) for k, v in word_index.items()}\nword_index[\"<PAD>\"] = 0\nword_index[\"<START>\"] = 1\nword_index[\"<UNK>\"] = 2 # Unknown\nword_index[\"<UNUSED>\"] = 3\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return ''.join([reverse_word_index.get(i, '?') for i in text])\n\ndecode_review(train_data[4])\n\n# \ud559\uc2b5\ud560 \ub370\uc774\ud130 \uc804\ucc98\ub9ac \uacfc\uc815\ntrain_data = keras.preprocessing.sequence.pad_sequences(train_data,\n                                                       value=word_index[\"<PAD>\"],\n                                                       padding='post',\n                                                       maxlen=256)\n\ntest_data = keras.preprocessing.sequence.pad_sequences(test_data,\n                                                       value=word_index[\"<PAD>\"],\n                                                       padding='post',\n                                                       maxlen=256)\n\n# print(len(train_data[0]), len(test_data[0]))\n# print(train_data[0])\n\n# \uc785\ub825\ud560 \ub2e8\uc5b4\ub7c9 \uc815\uc758\nvocab_size = 10000\n\n# \ubaa8\ub378 \uad6c\uc131 \uacfc\uc815\nmodel = keras.Sequential()\nmodel.add(keras.layers.Embedding(vocab_size, 16))\nmodel.add(keras.layers.GlobalAveragePooling1D())\nmodel.add(keras.layers.Dense(16, activation=tf.nn.relu))\nmodel.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nmodel.summary()\n\n# \ubaa8\ub378\uc5d0 \uc0ac\uc6a9\ud560 \uc190\uc2e4 \ud568\uc218\uc640 \uc635\ud2f0\ub9c8\uc774\uc800 \uc124\uc815\nmodel.compile(optimizer='adam', \n              loss='binary_crossentropy', \n              metrics=['accuracy'])\n\n# \ubaa8\ub378 \ud3c9\uac00\ud560 Test \ub370\uc774\ud130 \uc815\uc758\nx_val = train_data[:10000]\npartial_x_train = train_data[10000:]\n\ny_val = train_labels[:10000]\npartial_y_train = train_labels[10000:]\n\n# \ubaa8\ub378 \ud6c8\ub828\nhistory = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=40,\n                    batch_size=512,\n                    validation_data=(x_val, y_val),\n                    verbose=1)\n\n# \ubaa8\ub378 \ud3c9\uac00\nresults = model.evaluate(test_data, test_labels)\nprint(results)","46b2e597":"## Softmax classifier \uc2e4\uc81c \uad6c\ud604","260b25d1":"## \uacbd\uc0ac\ud558\uac15\ubc95 \uc2e4\uc81c \uad6c\ud604","e4e66d89":"## \ud589\ub82c\uc744 \uc774\uc6a9\ud55c \uac00\uc124 \uc2e4\uc81c \uad6c\ud604","faadd31c":"# Lec 07: Application & Tips","7da1d7ed":"## \ud559\uc2b5\ub960 \uc870\uc808, \ub370\uc774\ud130 \uc804\ucc98\ub9ac, \uc624\ubc84\ud53c\ud305 \ubb38\uc81c \ud574\uacb0 \uc2e4\uc2b5 \uc608\uc81c \uad6c\ud604","521da90b":"## Softmax Classifier (\ub3d9\ubb3c \ubd84\ub958) \uc608\uc81c \uad6c\ud604","81406e22":"## Softmax \ud568\uc218","daaabb5c":"## Fashion MNIST Image Classification \uc608\uc81c \uad6c\ud604 (Keras)","d63ebc0d":"## \ub370\uc774\ud130 \uc804\ucc98\ub9ac - Feature Scaling\n\n![image.png](attachment:image.png)\n","8e05b1fe":"# Lec 03: Linear Regression and How to minimize cost\n","2cdd1dd7":"## \ud2b8\ub808\uc774\ub2dd \uacfc\uc815","4f3d1808":"# Lec 02: Simple Linear Regression","7aa17836":"## \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0 \uc2e4\uc81c \uad6c\ud604","32ff2956":"## \uac00\uc124(hypothesis)\uacfc \ube44\uc6a9(cost) \uc815\uc758","ae83b742":"## \uacbd\uc0ac \ud558\uac15\ubc95(Gradient descent)","de1a8861":"## TensorFlow\ub85c \ube44\uc6a9 \ud568\uc218 \uad6c\ud604 \uc608\uc2dc","01c61930":"# Lab 05: Logistic Regression\/Classification","fcb3dbdf":"## \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0\/\ubd84\ub958(Classification)","89934067":"## Overfitting \ubb38\uc81c \ud574\uacb0 \ubc29\ubc95\n### Overfitting\uc758 \uc815\uc758\n- \ubaa8\ub378\uc744 \ud559\uc2b5 \uc2dc\ud0a4\ub294 \uacfc\uc815\uc5d0\uc11c \ubaa8\ub378\uc774 \ud559\uc2b5\uc6a9 \ub370\uc774\ud130\uc5d0 \uacfc\ub3c4\ud558\uac8c \ub9de\ucdb0\uc9c0\ub294 \ud604\uc0c1 \n\n### \ud574\uacb0 \ubc29\ubc95\n1. Feature \uc124\uc815 \uad00\ub828\n\n    - \ub370\uc774\ud130\uc758 \uc591\uc744 \uc99d\uac00 (High variance \ubb38\uc81c \ud574\uacb0)\n    - feature(\ub370\uc774\ud130\uc758 \ud2b9\uc131)\uc758 \uc218\ub97c \uac10\uc18c (High variance \ubb38\uc81c \ud574\uacb0)\n    - \ubcf4\ub2e4 \ub354 \uc758\ubbf8\uc788\ub294 feature\ub97c \ucd94\uac00 (High bias \ubb38\uc81c \ud574\uacb0) \n\n\n2. \uc815\uaddc\ud654 \uae30\ubc95\n\n    - \uc120\ud615 \ud68c\uadc0 \uac00\uc124\uc758 \ube44\uc6a9 \ud568\uc218 \ub4a4\uc5d0 \ub78c\ub2e4\ud56d\uc744 \ucd94\uac00\n    \n        ![image.png](attachment:image.png)","4852e374":"## TensorFlow\ub85c \uacbd\uc0ac\ud558\uac15\ubc95 \uad6c\ud604 \uc608\uc2dc","705f157e":"## Python\uc73c\ub85c \ube44\uc6a9 \ud568\uc218 \uad6c\ud604 \uc608\uc2dc","e0453820":"# Lab 04: Multi-variable Linear Regression","0c06ac5c":"## \uc2e4\uc81c \uad6c\ud604","c07f321e":"## \ube44\uc6a9 \ud568\uc218(Cross entropy)\ubc0f \uacbd\uc0ac\ud558\uac15 \ud568\uc218","14af0681":"## IMDB Text Classification \uc608\uc81c \uad6c\ud604 (Keras)","2c96b1b8":"## \uc0d8\ud50c \ub370\uc774\ud130\uc14b \uad6c\uc131\ud558\uae30","bed078d2":"# Lab 06: Softmax classifier","0bb89885":"## \ud559\uc2b5\ub960(Learning rate) \uc870\uc808 \uae30\ubc95","036f19b1":"## \ud589\ub82c\uc744 \uc774\uc6a9\ud55c \uac00\uc124 \uc124\uc815"}}