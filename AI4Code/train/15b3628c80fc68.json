{"cell_type":{"332a4b00":"code","14c1736c":"code","a12c76db":"code","f018676b":"code","03afaac0":"code","c5fd9ced":"code","f2c01cbd":"code","0a20cf8d":"code","4c08f4d1":"code","6a27f85f":"code","a484d7c1":"code","3bc10402":"code","66efb338":"code","15d2d03d":"code","270153d1":"code","cd9163d1":"code","65d52d42":"code","7ab4e03c":"code","cb810804":"code","69837843":"code","ca4d9950":"code","98a2ca35":"code","21734b75":"code","a9b87707":"code","26829e92":"code","3d27dc9f":"code","ca195d62":"code","82851aa2":"code","e58fb096":"code","750817f1":"code","5a87a047":"code","33e02baa":"code","105f29dc":"code","0e049e4f":"code","bfdf4207":"code","152483e7":"code","a16fe8e3":"code","ad8597a2":"code","f14f91e0":"code","c2581601":"code","22d1eed8":"code","f7990789":"code","f5d4bc1a":"code","bc6b6a2d":"code","85dab7e6":"code","87636046":"code","e0ffe55b":"code","50e890b8":"code","ae3bd3f7":"code","1e0f6d18":"code","fea096fd":"code","94b1f8f4":"code","cb62c52a":"code","5430ddbb":"code","dcf3c057":"code","ce22ffc9":"code","2bda0a31":"code","46d9ed4c":"code","436478ec":"code","2ce39d3b":"code","893e4a7a":"code","e8318640":"code","4b9560d4":"code","78074def":"code","b147dac7":"code","8fdf958d":"code","90a8217c":"code","f96a33df":"code","e4932c68":"code","a752b8c3":"code","cc063cce":"code","849a078e":"code","fe760230":"code","2b04bdaf":"code","960fdfdb":"code","dd7d0ca0":"code","0a9aad12":"code","531180c6":"code","ac2ab42b":"code","5927729e":"code","57d408b2":"code","fa5ddafc":"code","708927ff":"code","654f3f99":"code","32461a10":"code","983fc713":"code","cab85bec":"code","ee9d26ec":"code","645dc0a4":"code","99102dc8":"code","6d53b9a0":"code","2c69c94f":"code","9fe7967d":"code","b431f9b5":"code","bfcfbf6a":"code","49332881":"code","06aabf92":"code","6f2dc6be":"code","00121643":"code","c0219eb0":"code","5e6a841f":"code","b3239a4d":"code","d92b247c":"code","031dd796":"code","d1b7b540":"code","42d45bc6":"code","f1a362a9":"code","a518516d":"code","f4ab1a5e":"code","3cb7add0":"code","564d318d":"code","a9328ad5":"code","ec58867b":"code","56040032":"code","cd601fb8":"code","65cef9c9":"code","c0eec013":"code","7fdf9633":"code","67478b21":"code","a33e1469":"code","785595a2":"code","914d4bcf":"code","28b204f3":"code","a0ef7098":"code","3a826e27":"code","ba608146":"code","8e0796f0":"code","b34f8f37":"code","c8739aea":"code","2178f704":"code","476602be":"code","85d7ac3d":"markdown","d160d48a":"markdown","e0dbc410":"markdown","056fbd86":"markdown","b0c25e44":"markdown","65adda8f":"markdown","526b7be6":"markdown","90637e04":"markdown","70e2fe15":"markdown","c9187866":"markdown","eca34992":"markdown","88a933dd":"markdown","5be99230":"markdown","0798bc24":"markdown","ce772594":"markdown","73069988":"markdown","510936ae":"markdown","4e25609d":"markdown","bb63f06e":"markdown","bf235367":"markdown","b9fb9258":"markdown","02864665":"markdown","7870caa9":"markdown","1c6baf50":"markdown","809b328f":"markdown","389e61fe":"markdown","bf5802f0":"markdown","8e6f85a3":"markdown","d4bd0ab7":"markdown","f53610a2":"markdown","217cd613":"markdown","d5aa920c":"markdown","0bdf51a9":"markdown","82be7cea":"markdown","3c3c9447":"markdown","4318fa4b":"markdown","4c6794a0":"markdown","20d0211b":"markdown","4e31fd53":"markdown","21fd0210":"markdown","173da1eb":"markdown","17d687ac":"markdown","916d5059":"markdown","50026bfb":"markdown","a53694da":"markdown","58a5c193":"markdown","efb0763d":"markdown","4ad17083":"markdown","8fc6fcb8":"markdown","bb3015c5":"markdown","a5d9a0f8":"markdown","23db6bec":"markdown","9244cb29":"markdown","96715a80":"markdown","768e654e":"markdown","c371723a":"markdown","849dca27":"markdown","45e2348a":"markdown","5fa8d477":"markdown","1041d8a6":"markdown","838450dc":"markdown","f4e14447":"markdown","5311a1a2":"markdown","c564c689":"markdown","d0e658c2":"markdown","8b82e3f7":"markdown","c4a7eac7":"markdown","d9d42c98":"markdown","d755b558":"markdown","1a1c0760":"markdown","6ebd55f9":"markdown","167d0774":"markdown","605a1708":"markdown","aeb773df":"markdown","73890123":"markdown","d0fbe63a":"markdown","22ddf428":"markdown","315bb921":"markdown","b438a91f":"markdown","d1b327d3":"markdown","d512e0b5":"markdown","7b28a4ba":"markdown","b575a5ab":"markdown","a389d946":"markdown","cfa41640":"markdown","d7d01503":"markdown","148f662c":"markdown","cbf8b2d3":"markdown","a7223e11":"markdown","82afcd58":"markdown","2e0dde1f":"markdown","d5dafde5":"markdown","7af5d454":"markdown","f6646cd0":"markdown","23d04407":"markdown","8966b45d":"markdown","f0412231":"markdown","c0de79ad":"markdown","d8901c86":"markdown","a90f22ae":"markdown","02e98fd4":"markdown","b5bffd0a":"markdown","75aeb873":"markdown","862d3370":"markdown","eb7a3bcb":"markdown","80bbe68b":"markdown","9a5b66cf":"markdown","1267c0d6":"markdown","cc39d965":"markdown","b19e1579":"markdown","c6f68ebd":"markdown","cff3ca84":"markdown","ba22b791":"markdown","49a1b0fd":"markdown","1616d6a8":"markdown","65feb75b":"markdown","7267c94c":"markdown","8247a42c":"markdown","4de0591e":"markdown"},"source":{"332a4b00":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime as dt\nfrom matplotlib.ticker import PercentFormatter\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom itertools import combinations\nfrom datetime import datetime\nimport statsmodels.api as sm\nimport warnings\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom fbprophet import Prophet\nfrom sklearn.metrics import r2_score, mean_squared_error\n\npd.options.mode.chained_assignment = None\n\nplt.rcParams[\"axes.facecolor\"] = \"#A2A2A2\"\nplt.rcParams[\"axes.grid\"] = 1","14c1736c":"df = pd.read_csv(\"..\/input\/ecommerce-data\/data.csv\")\ndisplay(df.head())\nprint(df.shape)","a12c76db":"df.info()","f018676b":"df.isnull().sum()","03afaac0":"df[df.Description.isnull()]","c5fd9ced":"df[df.Description.isnull()].CustomerID.nunique()","f2c01cbd":"df[df.Description.isnull()].UnitPrice.value_counts()","0a20cf8d":"df = df[df.Description.notnull()]","4c08f4d1":"df[df.CustomerID.isnull()]","6a27f85f":"print(\"We have {} observations.\".format(df.shape[0]))\n\ndf = df[df.CustomerID.notnull()]\n\nprint(\"We have {} observations after removing unknown customers.\".format(df.shape[0]))","a484d7c1":"df.isnull().sum()","3bc10402":"df[df.Description.str.len() < 5]","66efb338":"df.InvoiceNo.value_counts()","15d2d03d":"df[df[\"InvoiceNo\"].str.startswith(\"C\")]","270153d1":"df[\"Cancelled\"] = df[\"InvoiceNo\"].apply(lambda x: 1 if x.startswith(\"C\") else 0)","cd9163d1":"cancelled_invoiceNo = df[df.Cancelled == 1].InvoiceNo.tolist()\ncancelled_invoiceNo = [x[1:] for x in cancelled_invoiceNo]\n\ncancelled_invoiceNo[:5]","65d52d42":"df[df[\"InvoiceNo\"].isin(cancelled_invoiceNo)]","7ab4e03c":"df[df.InvoiceNo.str.len() != 6]","cb810804":"df = df[df.Cancelled == 0]","69837843":"df[df.StockCode.str.contains(\"^[a-zA-Z]\")].StockCode.value_counts()","ca4d9950":"df[df.StockCode.str.contains(\"^[a-zA-Z]\")].Description.value_counts()","98a2ca35":"df[df.StockCode.str.len() > 5].StockCode.value_counts()","21734b75":"df[df.StockCode.str.len() > 5].Description.value_counts()","a9b87707":"df =  df[~ df.StockCode.str.contains(\"^[a-zA-Z]\")]","26829e92":"df[\"Description\"] = df[\"Description\"].str.lower()","3d27dc9f":"df.groupby(\"StockCode\")[\"Description\"].nunique()[df.groupby(\"StockCode\")[\"Description\"].nunique() != 1]","ca195d62":"df[df.StockCode == \"16156L\"].Description.value_counts()","82851aa2":"df[df.StockCode == \"17107D\"].Description.value_counts()","e58fb096":"df[df.StockCode == \"90014C\"].Description.value_counts()","750817f1":"df.CustomerID.value_counts()","5a87a047":"customer_counts = df.CustomerID.value_counts().sort_values(ascending=False).head(25)\n\nfig, ax = plt.subplots(figsize = (10, 8))\n\nsns.barplot(y = customer_counts.index, x = customer_counts.values, orient = \"h\", \n            ax = ax, order = customer_counts.index, palette = \"Reds_r\")\n\nplt.title(\"Customers that have most transactions\")\nplt.ylabel(\"Customers\")\nplt.xlabel(\"Transaction Count\")\n\nplt.show()","33e02baa":"df.Country.value_counts()","105f29dc":"country_counts = df.Country.value_counts().sort_values(ascending=False).head(25)\n\nfig, ax = plt.subplots(figsize = (18, 10))\n\nsns.barplot(x = country_counts.values, y = country_counts.index, orient = \"h\", \n            ax = ax, order = country_counts.index, palette = \"Blues_r\")\nplt.title(\"Countries that have most transactions\")\nplt.xscale(\"log\")\nplt.show()","0e049e4f":"df[\"UnitPrice\"].describe()","bfdf4207":"df[df.UnitPrice == 0].head()","152483e7":"print(\"We have {} observations.\".format(df.shape[0]))\n\ndf = df[df.UnitPrice > 0]\n\nprint(\"We have {} observations after removing records that have 0 unit price.\".format(df.shape[0]))","a16fe8e3":"fig, axes = plt.subplots(1, 3, figsize = (18, 6))\n\nsns.kdeplot(df[\"UnitPrice\"], ax = axes[0], color = \"#195190\").set_title(\"Distribution of Unit Price\")\nsns.boxplot(y = df[\"UnitPrice\"], ax = axes[1], color = \"#195190\").set_title(\"Boxplot for Unit Price\")\nsns.kdeplot(np.log(df[\"UnitPrice\"]), ax = axes[2], color = \"#195190\").set_title(\"Log Unit Price Distribution\")\n\nplt.show()","ad8597a2":"print(\"Lower limit for UnitPrice: \" + str(np.exp(-2)))\nprint(\"Upper limit for UnitPrice: \" + str(np.exp(3)))","f14f91e0":"np.quantile(df.UnitPrice, 0.99)","c2581601":"print(\"We have {} observations.\".format(df.shape[0]))\n\ndf = df[(df.UnitPrice > 0.1) & (df.UnitPrice < 20)]\n\nprint(\"We have {} observations after removing unit prices smaller than 0.1 and greater than 20.\".format(df.shape[0]))","22d1eed8":"fig, axes = plt.subplots(1, 3, figsize = (18, 6))\n\nsns.kdeplot(df[\"UnitPrice\"], ax = axes[0], color = \"#195190\").set_title(\"Distribution of Unit Price\")\nsns.boxplot(y = df[\"UnitPrice\"], ax = axes[1], color = \"#195190\").set_title(\"Boxplot for Unit Price\")\nsns.kdeplot(np.log(df[\"UnitPrice\"]), ax = axes[2], color = \"#195190\").set_title(\"Log Unit Price Distribution\")\n\nfig.suptitle(\"Distribution of Unit Price (After Removing Outliers)\")\nplt.show()","f7990789":"df[\"Quantity\"].describe()","f5d4bc1a":"fig, axes = plt.subplots(1, 3, figsize = (18, 6))\n\nsns.kdeplot(df[\"Quantity\"], ax = axes[0], color = \"#195190\").set_title(\"Distribution of Quantity\")\nsns.boxplot(y = df[\"Quantity\"], ax = axes[1], color = \"#195190\").set_title(\"Boxplot for Quantity\")\nsns.kdeplot(np.log(df[\"Quantity\"]), ax = axes[2], color = \"#195190\").set_title(\"Log Quantity\")\nplt.show()","bc6b6a2d":"print(\"Upper limit for Quantity: \" + str(np.exp(5)))","85dab7e6":"np.quantile(df.Quantity, 0.99)","87636046":"print(\"We have {} observations.\".format(df.shape[0]))\n\ndf = df[(df.Quantity < 150)]\n\nprint(\"We have {} observations after removing quantities greater than 150.\".format(df.shape[0]))","e0ffe55b":"fig, axes = plt.subplots(1, 3, figsize = (18, 6))\n\nsns.kdeplot(df[\"Quantity\"], ax = axes[0], color = \"#195190\").set_title(\"Distribution of Quantity\")\nsns.boxplot(y = df[\"Quantity\"], ax = axes[1], color = \"#195190\").set_title(\"Boxplot for Quantity\")\nsns.kdeplot(np.log(df[\"Quantity\"]), ax = axes[2], color = \"#195190\").set_title(\"Log Quantity\")\n\nfig.suptitle(\"Distribution of Quantity (After Removing Outliers)\")\nplt.show()","50e890b8":"df[\"TotalPrice\"] = df[\"Quantity\"] * df[\"UnitPrice\"]\ndf['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])","ae3bd3f7":"df.drop(\"Cancelled\", axis = 1, inplace = True)\ndf.to_csv(\"online_retail_final.csv\", index = False)","1e0f6d18":"print(\"Min date: {} \\nMax date: {}\".format(df.InvoiceDate.min(), df.InvoiceDate.max()))\nprint(\"Time difference is: {}\".format(df.InvoiceDate.max() - df.InvoiceDate.min()))","fea096fd":"def get_month(x): return dt.datetime(x.year, x.month, 1) \n\ndef get_dates(df, col):\n    \n    year = df[col].dt.year\n    month = df[col].dt.month\n    day = df[col].dt.day\n    \n    return year, month, day","94b1f8f4":"df[\"InvoiceMonth\"] = df[\"InvoiceDate\"].apply(get_month)\n\ndf[\"CohortMonth\"] = df.groupby(\"CustomerID\")[\"InvoiceMonth\"].transform(\"min\")","cb62c52a":"df.head()","5430ddbb":"invoice_year, invoice_month, invoice_day = get_dates(df, \"InvoiceMonth\")\ncohort_year, cohort_month, cohort_day = get_dates(df, \"CohortMonth\")\n\nyear_diff = invoice_year - cohort_year\nmonth_diff = invoice_month - cohort_month\n\ndf[\"CohortIndex\"] = 12 * year_diff + month_diff + 1","dcf3c057":"cohort_data = df.groupby([\"CohortIndex\", \"CohortMonth\"])[\"CustomerID\"].nunique().reset_index()\ncohort_pivot = cohort_data.pivot(index = \"CohortMonth\", columns = \"CohortIndex\", values = \"CustomerID\")\n\ncohort_pivot","ce22ffc9":"cohort_sizes = cohort_pivot.iloc[:, 0]\n\nretention = cohort_pivot.divide(cohort_sizes, axis = 0)\nretention.index = retention.index.strftime(\"%Y-%m\")\n\nretention","2bda0a31":"plt.rcParams[\"axes.facecolor\"] = \"white\"\nfig, ax = plt.subplots(figsize = (14, 10))\n\nsns.heatmap(retention, cmap = \"Blues\", annot = True, fmt = \".2%\", annot_kws = {\"fontsize\": 12}, cbar = False, ax = ax)\n\nplt.title(\"Retention Rate Percentages - Monthly Cohorts\")\nplt.yticks(rotation = 0)\nplt.show()","46d9ed4c":"customer_per_month = df.groupby(\"CohortMonth\")[\"CustomerID\"].nunique().values\ncustomers = customer_per_month.cumsum()\ncustomers = customers[::-1]\ncustomers","436478ec":"customer_in_month = df.groupby(\"CohortIndex\")[\"CustomerID\"].nunique()\ncustomer_in_month","2ce39d3b":"plt.rcParams[\"axes.facecolor\"] = \"#A2A2A2\"\nfig, ax = plt.subplots(figsize = (14, 8), facecolor = \"#A2A2A2\")\nax.grid(False)\nx = customer_in_month.index\ny = 100*(customer_in_month \/ customers)\n\nsns.lineplot(x = x, y = y,  color = \"#101820\", marker = \"o\", markerfacecolor = \"#0EB8F1\", markeredgecolor = \"#000000\")\n\nfor x, y in zip(x, y):\n    plt.text(x, y + 2, s = str(round(y, 2)) + \"%\")\n\nplt.xlabel(\"Cohort Index\")\nplt.ylabel(\"Retention Rate %\")\nplt.title(\"Monthly Retention Rates for All Customers\")\nsns.despine()\nplt.show()","893e4a7a":"monthly_customer_price_df = df.groupby(\"InvoiceMonth\").agg({\"TotalPrice\": \"sum\",\n                                                            \"CustomerID\": \"nunique\"})\n\nmonthly_customer_price_df","e8318640":"fig, ax = plt.subplots(figsize = (16, 8), facecolor = \"#A2A2A2\")\nax.set_facecolor(\"#A2A2A2\")\n\nsns.barplot(x = np.arange(len(monthly_customer_price_df.index)), y = monthly_customer_price_df.TotalPrice, ax = ax,\n            color = \"#101820\")\nax2 = ax.twinx()\nsns.lineplot(x = np.arange(len(monthly_customer_price_df.index)), y = monthly_customer_price_df.CustomerID, ax = ax2,\n            color = \"#F1480F\", marker = \"o\", markerfacecolor = \"#0EB8F1\", markeredgecolor = \"#000000\")\n\nax.set_yticks([])\nax2.set_yticks([])\nax2.set_ylabel(\"Total Customer\", fontname = \"Times New Roman\")\nax.set_ylabel(\"Total Price\", fontname = \"Times New Roman\")\nplt.title(\"Revenue & Customer Count per Month\", fontname = \"Times New Roman\")\nax.text(-0.75, 1000000, \"Bars represent revenue \\nLine represents unique customer count\", \n        fontsize = 7, alpha = 0.8, fontname = \"Times New Roman\")\nfor x, y in zip(np.arange(len(monthly_customer_price_df.index)),  monthly_customer_price_df.CustomerID):\n    ax2.text(x -0.1, y + 20 , y, color = \"white\")\n    \nsns.despine(left = True, right = True, bottom = True, top = True)\nplt.show()","4b9560d4":"def prepare_pareto_data(df, col, price):\n    \n    df_price = pd.DataFrame(df.groupby(col)[price].sum())\n    df_price = df_price.sort_values(price, ascending = False)\n\n    df_price[\"CumulativePercentage\"] = (df_price[price].cumsum() \/ df_price[price].sum() * 100).round(2)\n    \n    return df_price","78074def":"def create_pareto_plot(df, col, price, log = True):\n    \n    plt.rcParams[\"axes.facecolor\"] = \"#A2A2A2\"\n    fig, ax = plt.subplots(figsize = (15, 5), dpi = 150, facecolor = \"#A2A2A2\")\n    plt.rcParams[\"axes.grid\"] = False\n    \n    if log == True:\n        sns.barplot(x = np.arange(len(df)), y = np.log(df[price]),  ax = ax, color = \"#101820\")\n        ax.set_ylabel(\"Total Price (Log - Scale)\")\n    else:\n        sns.barplot(x = np.arange(len(df)), y = df[price],  ax = ax, color = \"#101820\")\n\n    ax2 = ax.twinx()\n    \n    sns.lineplot(x = np.arange(len(df)), y = df.CumulativePercentage, ax = ax2, color = \"#0019AA\")\n    ax2.axhline(80, color = \"#008878\", linestyle = \"dashed\", alpha = 1)\n    ax2.axhline(90, color = \"#008878\", linestyle = \"dashed\", alpha = 0.75)\n\n    vlines = [int(len(df) * x \/ 10) for x in range(1, 10)]\n    for vline in vlines: ax2.axvline(vline, color = \"#008878\", linestyle = \"dashed\", alpha = 0.1)\n        \n    interaction_80 = (df.shape[0] - df[df.CumulativePercentage >= 80].shape[0])\n    ax2.axvline(interaction_80, color = \"#008878\", linestyle = \"dashed\", alpha = 1)\n\n    interaction_80_percentage = round((interaction_80 \/ df.shape[0]) * 100)\n    plt.text(interaction_80 + 25, 95, str(interaction_80_percentage) + \"%\")   \n    \n    prop = dict(arrowstyle= \"-|>\", color = \"#000000\", lw = 1.5, ls = \"--\")\n    plt.annotate(\"\", xy = (interaction_80 - 10, 80),  xytext = (interaction_80 + 120 , 73), arrowprops = prop)\n    \n    interaction_90 = (df.shape[0] - df[df.CumulativePercentage >= 90].shape[0])\n    ax2.axvline(interaction_90, color = \"#008878\", linestyle = \"dashed\", alpha = 0.8)\n    interaction_90_percentage = round((interaction_90 \/ df.shape[0]) * 100)\n    plt.text(interaction_90 + 25, 95, str(interaction_90_percentage) + \"%\")   \n    plt.annotate(\"\", xy = (interaction_90 - 10, 90),  xytext = (interaction_90 + 120 , 83), arrowprops = prop)\n\n    ax2.yaxis.set_major_formatter(PercentFormatter())\n    ax.set_yticks([])\n    plt.xticks([])\n    ax.set_ylabel(\"Revenue\", fontname = \"Times New Roman\")\n    ax2.set_ylabel(\"Cumulative Percentage\", fontname = \"Times New Roman\")\n    subject = \"Customers\" if col == \"CustomerID\" else \"Products\"\n    plt.title(\"Pareto Chart for \" + subject, fontname = \"Times New Roman\")\n    ax.set_xlabel(subject, fontname = \"Times New Roman\")\n    sns.despine(left = True, right = True, bottom = True, top = True)\n    plt.show()","b147dac7":"customer_price = prepare_pareto_data(df, \"CustomerID\", \"TotalPrice\")\n\ncustomer_price.head(10)","8fdf958d":"create_pareto_plot(customer_price, \"CustomerID\", \"TotalPrice\", log = False)","90a8217c":"create_pareto_plot(customer_price, \"CustomerID\", \"TotalPrice\", log = True)","f96a33df":"item_price = prepare_pareto_data(df, \"StockCode\", \"TotalPrice\")\n\nitem_price.head(10)","e4932c68":"create_pareto_plot(item_price, \"StockCode\", \"TotalPrice\", log = False)","a752b8c3":"create_pareto_plot(item_price, \"StockCode\", \"TotalPrice\")","cc063cce":"top_customers = customer_price[customer_price.CumulativePercentage <= 80].index.tolist()\n\nproducts_for_top_customers = df[df.CustomerID.isin(top_customers)].Description.drop_duplicates().values.tolist()\n\nproducts_for_other_customers = df[~df.CustomerID.isin(top_customers)].Description.drop_duplicates().values.tolist()","849a078e":"print(\"Min date: {} \\nMax date: {}\".format(df.InvoiceDate.min(), df.InvoiceDate.max()))","fe760230":"last_day = df.InvoiceDate.max() + dt.timedelta(days = 1)","2b04bdaf":"rfm_table = df.groupby(\"CustomerID\").agg({\"InvoiceDate\": lambda x: (last_day - x.max()).days,\n                                         \"InvoiceNo\": \"nunique\",\n                                         \"TotalPrice\": \"sum\"})\n\nrfm_table.rename(columns = {\"InvoiceDate\": \"Recency\",\n                            \"InvoiceNo\": \"Frequency\",\n                            \"TotalPrice\": \"Monetary\"}, inplace = True)\n\nrfm_table.head()","960fdfdb":"r_labels = range(5, 0, -1)\nfm_labels = range(1, 6)\n\nrfm_table[\"R\"] = pd.qcut(rfm_table[\"Recency\"], 5, labels = r_labels)\nrfm_table[\"F\"] = pd.qcut(rfm_table[\"Frequency\"].rank(method = 'first'), 5, labels = fm_labels)\nrfm_table[\"M\"] = pd.qcut(rfm_table[\"Monetary\"], 5, labels = fm_labels)\n\nrfm_table.head()","dd7d0ca0":"rfm_table[\"RFM_Segment\"] = rfm_table[\"R\"].astype(str) + rfm_table[\"F\"].astype(str) + rfm_table[\"M\"].astype(str)\nrfm_table[\"RFM_Score\"] = rfm_table[[\"R\", \"F\", \"M\"]].sum(axis = 1)\n\nrfm_table.head()","0a9aad12":"segt_map = {\n    r'[1-2][1-2]': 'Hibernating',\n    r'[1-2][3-4]': 'At-Risk',\n    r'[1-2]5': 'Cannot lose them',\n    r'3[1-2]': 'About To Sleep',\n    r'33': 'Need Attention',\n    r'[3-4][4-5]': 'Loyal Customers',\n    r'41': 'Promising',\n    r'51': 'New Customers',\n    r'[4-5][2-3]': 'Potential Loyalists',\n    r'5[4-5]': 'Champions'\n}\nrfm_table['Segment'] = rfm_table['R'].astype(str) + rfm_table['F'].astype(str)\nrfm_table['Segment'] = rfm_table['Segment'].replace(segt_map, regex=True)\nrfm_table.head()","531180c6":"rfm_coordinates = {\"Champions\": [3, 5, 0.8, 1],\n                   \"Loyal Customers\": [3, 5, 0.4, 0.8],\n                   \"Cannot lose them\": [4, 5, 0, 0.4],\n                   \"At-Risk\": [2, 4, 0, 0.4],\n                   \"Hibernating\": [0, 2, 0, 0.4],\n                   \"About To Sleep\": [0, 2, 0.4, 0.6],\n                   \"Promising\": [0, 1, 0.6, 0.8],\n                   \"New Customers\": [0, 1, 0.8, 1],\n                   \"Potential Loyalists\": [1, 3, 0.6, 1],\n                   \"Need Attention\": [2, 3, 0.4, 0.6]}","ac2ab42b":"fig, ax = plt.subplots(figsize = (19, 15))\n\nax.set_xlim([0, 5])\nax.set_ylim([0, 5])\n\nplt.rcParams[\"axes.facecolor\"] = \"white\"\npalette = [\"#282828\", \"#04621B\", \"#971194\", \"#F1480F\",  \"#4C00FF\", \n           \"#FF007B\", \"#9736FF\", \"#8992F3\", \"#B29800\", \"#80004C\"]\n\nfor key, color in zip(rfm_coordinates.keys(), palette[:10]):\n    \n    coordinates = rfm_coordinates[key]\n    ymin, ymax, xmin, xmax = coordinates[0], coordinates[1], coordinates[2], coordinates[3]\n    \n    ax.axhspan(ymin = ymin, ymax = ymax, xmin = xmin, xmax = xmax, facecolor = color)\n    \n    users = rfm_table[rfm_table.Segment == key].shape[0]\n    users_percentage = (rfm_table[rfm_table.Segment == key].shape[0] \/ rfm_table.shape[0]) * 100\n    avg_monetary = rfm_table[rfm_table.Segment == key][\"Monetary\"].mean()\n    \n    user_txt = \"\\n\\nTotal Users: \" + str(users) + \"(\" +  str(round(users_percentage, 2)) + \"%)\"\n    monetary_txt = \"\\n\\n\\n\\nAverage Monetary: \" + str(round(avg_monetary, 2))\n    \n    x = 5 * (xmin + xmax) \/ 2\n    y = (ymin + ymax) \/ 2\n    \n    plt.text(x = x, y = y, s = key, ha = \"center\", va = \"center\", fontsize = 18, color = \"white\", fontweight = \"bold\")\n    plt.text(x = x, y = y, s = user_txt, ha = \"center\", va = \"center\", fontsize = 14, color = \"white\")    \n    plt.text(x = x, y = y, s = monetary_txt, ha = \"center\", va = \"center\", fontsize = 14, color = \"white\")    \n    \n    ax.set_xlabel(\"Recency Score\")\n    ax.set_ylabel(\"Frequency Score\")\n    \nsns.despine(left = True, bottom = True)\nplt.show()","5927729e":"rfm_table2 = rfm_table.reset_index()\n\nrfm_monetary_size = rfm_table2.groupby(\"Segment\").agg({\"Monetary\": \"mean\",\n                                                       \"CustomerID\": \"nunique\"})\n\nrfm_monetary_size.rename(columns = {\"Monetary\": \"MeanMonetary\", \"CustomerID\": \"CustomerCount\"}, inplace = True)\nrfm_monetary_size = rfm_monetary_size.sort_values(\"MeanMonetary\", ascending = False)","57d408b2":"plt.rcParams[\"axes.facecolor\"] = \"#A2A2A2\"\nfig, ax = plt.subplots(figsize = (16, 10), facecolor = \"#A2A2A2\")\n\nsns.barplot(x = rfm_monetary_size.MeanMonetary, y = rfm_monetary_size.index, ax = ax, color = \"#101820\")\nax2 = ax.twiny()\nsns.lineplot(x = rfm_monetary_size.CustomerCount, y = rfm_monetary_size.index, ax = ax2, marker = \"o\", linewidth = 0,\n             color = \"#F1480F\", markeredgecolor = \"#F1480F\")\n\n\nax2.axis(\"off\")\n\nfor y, x in list(enumerate(rfm_monetary_size.CustomerCount)):\n    ax2.text(x + 10, y + 0.05, str(x) + \" Customer\", color = \"white\", fontweight = \"normal\")\n\nplt.title(\"RFM Segments Details\")\nsns.despine(left = True, right = True, bottom = True, top = True)\nplt.show()","fa5ddafc":"rfm = rfm_table2.groupby(\"Segment\").agg({\"CustomerID\": \"nunique\", \n                                        \"Recency\": \"mean\",\n                                        \"Frequency\": \"mean\",\n                                        \"Monetary\": \"mean\"})\nrfm.rename(columns = {\"CustomerID\": \"Segment Size\"}, inplace = True)\n\ncm = sns.light_palette(\"#A2A2A2\", as_cmap = True)\n\nrfm.T.style.background_gradient(cmap = cm, axis = 1)\\\n.set_precision(2)\\\n.highlight_min(axis = 1, color = \"#195190\")\\\n.highlight_max(axis = 1, color = \"#D60000\")","708927ff":"plt.rcParams[\"axes.facecolor\"] = \"#A2A2A2\"\nplt.rcParams[\"axes.grid\"] = False\n\nsns.relplot(x = \"Recency\", y = \"Frequency\", hue = \"Segment\", size = \"Monetary\", data = rfm_table2, palette = palette,\n            height = 10, aspect = 2, sizes = (50, 1000))\n\nplt.show()","654f3f99":"monetary_per_segment = (rfm_table2.groupby(\"Segment\")[\"Monetary\"].sum() \/\\\n                        rfm_table2.groupby(\"Segment\")[\"Monetary\"].sum().sum()).sort_values(ascending = False)","32461a10":"fig, ax = plt.subplots(figsize = (10, 10), facecolor = \"#A2A2A2\")\n\nwedges, texts = ax.pie(monetary_per_segment.values, wedgeprops=dict(width=0.5), \n                       startangle=-40, normalize=False, colors = palette)\n\nbbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\nkw = dict(arrowprops=dict(arrowstyle=\"-\"),\n          bbox=bbox_props, zorder=0, va=\"center\")\n\nfor i, p in enumerate(wedges):\n    ang = (p.theta2 - p.theta1)\/2. + p.theta1\n    y = np.sin(np.deg2rad(ang))\n    x = np.cos(np.deg2rad(ang))\n    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n    connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n    kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n    ax.annotate(monetary_per_segment.index[i] + \" \" + str(round(monetary_per_segment[i] * 100, 2)) + \"%\", xy=(x, y), \n                xytext=(1.35*np.sign(x), 1.4*y),horizontalalignment=horizontalalignment, **kw)\nplt.show()","983fc713":"rfm_clustering = rfm_table2[[\"Recency\", \"Frequency\", \"Monetary\", \"Segment\"]]\n\nfor col in [\"Recency\", \"Frequency\", \"Monetary\"]:\n    \n    scaler = StandardScaler()\n    rfm_clustering[col] = np.log(rfm_clustering[col])\n    rfm_clustering[col] = scaler.fit_transform(rfm_clustering[col].values.reshape(-1, 1))\n    \nrfm_melted = pd.melt(rfm_clustering, id_vars = \"Segment\", value_vars = [\"Recency\", \"Frequency\", \"Monetary\"],\n                     var_name = \"RFM\", value_name = \"Value\")","cab85bec":"fig, ax = plt.subplots(figsize = (15, 12), facecolor = \"#A2A2A2\")\nax.set_facecolor(\"#A2A2A2\")\n\nsns.lineplot(x = \"RFM\", y = \"Value\", hue = \"Segment\", data = rfm_melted, palette = palette)\nax.legend(bbox_to_anchor = (1.05, 1), loc = 2, borderaxespad = 0.)\nax.set_yticks([])\nax.set_title(\"Snake Plot for RFM Segments\")\nplt.show()","ee9d26ec":"features = [\"Recency\", \"Frequency\", \"Monetary\"]\n\nkmeans_ = KMeans(init = \"k-means++\", random_state = 42)\n\nfig, ax = plt.subplots(figsize = (9, 6), facecolor = \"#A2A2A2\")\nax.set_facecolor(\"#A2A2A2\")\nax.grid(False)\n\nvisualizer = KElbowVisualizer(kmeans_, k = (2, 21), timings = False)\n\nvisualizer.fit(rfm_clustering[features]) \nvisualizer.show()","645dc0a4":"kmeans = KMeans(n_clusters = 6, random_state = 42) \nkmeans.fit(rfm_clustering[features])\ncluster = kmeans.labels_\n\nfig, axes = plt.subplots(1, 3, figsize = (24, 8))\n\nfor i, feature in list(enumerate(combinations([\"Recency\", \"Frequency\", \"Monetary\"], 2))):\n\n    sns.scatterplot(x = rfm_clustering[feature[0]], y = rfm_clustering[feature[1]], hue = cluster, \n                    palette = palette[: len(set(cluster))], ax = axes[i]).set_title(feature[0] + \" - \" + feature[1])\n\n    sns.scatterplot(x = kmeans.cluster_centers_[:, 0], y = kmeans.cluster_centers_[:, 1],\n                    s = 250, color = '#C0EB00', label = 'Centroids', marker = \"X\", ax = axes[i], edgecolor = \"black\")\n\nplt.suptitle(\"Segmentation with KMeans - 6 Clusters\")\nfor ax in axes:\n    ax.set_facecolor(\"#A2A2A2\")\n    ax.grid(False)\n    \nplt.show()","99102dc8":"fig, axes = plt.subplots(1, 3, figsize = (18, 6))\n\nfor ax in axes:\n    ax.set_facecolor(\"#A2A2A2\")\n    ax.set_xlabel(\"Clusters\")\n    \nsns.boxplot(x = cluster, y = \"Recency\", data = rfm_clustering, ax = axes[0]).set_title(\"Boxplot for Recency\")\nsns.boxplot(x = cluster, y = \"Frequency\", data = rfm_clustering, ax = axes[1]).set_title(\"Boxplot for Frequency\")\nsns.boxplot(x = cluster, y = \"Monetary\", data = rfm_clustering, ax = axes[2]).set_title(\"Boxplot for Monetary\")\n\nplt.show()","6d53b9a0":"plt.rcdefaults()","2c69c94f":"df = pd.read_csv(\"..\/input\/online-retail-final\/online_retail_final.csv\")\ndisplay(df.head())\n\nprint(df.shape)","9fe7967d":"df = df[df.Country == \"United Kingdom\"]","b431f9b5":"df.loc[:, \"InvoiceDate\"] = pd.to_datetime(df.loc[:, \"InvoiceDate\"])\n\nprint(f\"First observation on data is {df.InvoiceDate.min()}\")\nprint(f\"Last observation on data is {df.InvoiceDate.max()}\")\n\nprint(f\"We have {(df.InvoiceDate.max() - df.InvoiceDate.min()).days} days.\")","bfcfbf6a":"df.set_index(\"InvoiceDate\", inplace = True)\ndf.head()","49332881":"prices_df = pd.DataFrame(df.groupby(\"InvoiceDate\").TotalPrice.sum())\nprices_df = prices_df.iloc[:-1]\nprices_df","06aabf92":"weekly_prices = prices_df.resample(\"W\").sum()\nweekly_prices.head()","6f2dc6be":"fig, ax = plt.subplots(figsize = (12, 5), facecolor = \"#e5e5e5\")\nax.set_facecolor(\"#101820\")\n\nweekly_prices.plot(ax = ax).set_title(\"Weekly Revenue for United Kingdom\")\nax.legend(facecolor = \"#101820\", labelcolor = \"#e5e5e5\")\n\nsns.despine()\nplt.show()","00121643":"daily_sales = pd.DataFrame(df.groupby(\"InvoiceDate\").sum().resample(\"D\").sum()[\"TotalPrice\"])\ndaily_sales","c0219eb0":"daily_sales.loc[\"2010-12-23\" : \"2011-1-4\"]","5e6a841f":"daily_sales.reset_index(inplace = True)\ndaily_sales[\"Closed\"] = np.where((daily_sales.TotalPrice == 0), 1, 0)\ndaily_sales.set_index(\"InvoiceDate\", inplace = True)","b3239a4d":"daily_sales[\"weekday\"] = daily_sales.index.day_name()\ndaily_sales","d92b247c":"daily_sales[daily_sales.TotalPrice == 0].weekday.value_counts()","031dd796":"daily_sales.groupby(\"weekday\").TotalPrice.sum()","d1b7b540":"daily_sales_workdays = daily_sales[daily_sales.TotalPrice > 0].copy()\ndaily_sales.drop(\"weekday\", axis = 1, inplace = True)\ndaily_sales_workdays.drop(\"weekday\", axis = 1, inplace = True)","42d45bc6":"fig, axes = plt.subplots(2, 1, sharex = True, figsize = (14, 10), facecolor = \"#e5e5e5\")\n\nfor ax in axes: ax.set_facecolor(\"#101820\")\n\ndaily_sales.TotalPrice.plot(ax = axes[0]).set_title(\"Daily Revenue\")\ndaily_sales_workdays.TotalPrice.plot(ax = axes[1]).set_title(\"Daily Revenue for Work Days\")\n\nsns.despine()\nplt.show()","f1a362a9":"fig, axes = plt.subplots(2, 1, sharex = True, figsize = (14, 10), facecolor = \"#e5e5e5\")\n\nfor ax in axes: ax.set_facecolor(\"#101820\")\n    \nsns.lineplot(x = daily_sales.index, y = daily_sales.TotalPrice, \n             ax = axes[0], label = \"Daily Sales\").set_title(\"For Daily Sales \\nIncludes Vacation Days\")\n\nsns.lineplot(x = daily_sales.index, y = daily_sales.TotalPrice.rolling(7).mean(), \n             ax = axes[0], label = \"Rolling Mean for 7 Days\")\nsns.lineplot(x = daily_sales.index, y = daily_sales.TotalPrice.rolling(35).mean(), \n             ax = axes[0], label = \"Rolling Mean for 35 Days\")\nsns.lineplot(x = daily_sales.index, y = daily_sales.TotalPrice.rolling(70).mean(), \n             ax = axes[0], label = \"Rolling Mean for 70 Days\")\nsns.lineplot(x = daily_sales.index, y = daily_sales.TotalPrice.rolling(140).mean(), \n             ax = axes[0], label = \"Rolling Mean for 140 Days\")\n\n\nsns.lineplot(x = daily_sales_workdays.index, y = daily_sales_workdays.TotalPrice, \n             ax = axes[1], label = \"Daily Sales\").set_title(\"Daily Sales \\nJust Work Days\")\n\nsns.lineplot(x = daily_sales_workdays.index, y = daily_sales_workdays.TotalPrice.rolling(6).mean(), \n             ax = axes[1], label = \"Rolling Mean for 7 Days\")\nsns.lineplot(x = daily_sales_workdays.index, y = daily_sales_workdays.TotalPrice.rolling(30).mean(),\n             ax = axes[1], label = \"Rolling Mean for 30 Days\")\nsns.lineplot(x = daily_sales_workdays.index, y = daily_sales_workdays.TotalPrice.rolling(60).mean(), \n             ax = axes[1], label = \"Rolling Mean for 60 Days\")\nsns.lineplot(x = daily_sales_workdays.index, y = daily_sales_workdays.TotalPrice.rolling(120).mean(), \n             ax = axes[1], label = \"Rolling Mean for 120 Days\")\n\nfor ax in axes: ax.legend(facecolor = \"#101820\", labelcolor = \"#e5e5e5\")\n    \nsns.despine()\nplt.tight_layout()\nplt.show()","a518516d":"def check_adf(series, check = 0.05):\n    \n    adf = adfuller(series, autolag = \"AIC\")\n    \n    print(f\"H0: {series.name} is non-stationary.\")\n    print(f\"H1: {series.name} is stationary.\\n\")\n    \n    test_stat = adf[0]; print(f\"ADF test statistic: {adf[0]}\")\n    pvalue = adf[1]; print(f\"p-value: {adf[1]}\")\n    print(f\"Number of lags: {adf[2]}\")    \n    print(\"\\nCritical Values : \\n\")\n    for key, item in adf[4].items(): print(\"\\t\", key, \"\\t\", item)\n    \n    print(f\"\\nFor {check} significance level: \\n\")\n    if pvalue < check:\n        print(\"We can reject null hypothesis. This series is stationary.\")\n    else:\n        print(\"We can not reject null hypothesis. This series is non-stationary.\")","f4ab1a5e":"print(\"Performing Augmented Dickey-Fuller test for Total Price \\n\")\n\ncheck_adf(daily_sales.TotalPrice)\n\nprint(\"\\nPerforming Augmented Dickey-Fuller test for Total Price (for workdays)\\n\")\n\ncheck_adf(daily_sales_workdays.TotalPrice)","3cb7add0":"fig, axes = plt.subplots(2, 1, figsize = (14, 10), facecolor = \"#e5e5e5\")\n\nfor ax in axes: ax.set_facecolor(\"#101820\")\n\nsns.lineplot(x = daily_sales_workdays.index, y = daily_sales_workdays.TotalPrice, \n             ax = axes[0], label = \"Daily Sales\").set_title(\"Daily Sales for Work Days\")\nsns.lineplot(x = daily_sales_workdays.index, y = daily_sales_workdays.TotalPrice.rolling(60).mean(), \n             ax = axes[0], label = \"Rolling Mean - 60 Days\")\n\nsns.lineplot(x = daily_sales_workdays.index, y = daily_sales_workdays.TotalPrice.diff(7), \n             ax = axes[1], label = \"Daily Sales - First Difference\").set_title(\"1st Difference of Work Days Daily Sales\")\nsns.lineplot(x = daily_sales_workdays.index, y = daily_sales_workdays.TotalPrice.diff(7).rolling(60).mean(), \n             ax = axes[1], label = \"Rolling Mean - 60 Days\")\n\nfor ax in axes: ax.legend(facecolor = \"#101820\", labelcolor = \"#e5e5e5\")\n\nsns.despine()\nplt.tight_layout()\nplt.show()","564d318d":"print(\"Performing Augmented Dickey-Fuller test for 1st Difference Revenue (All Days)\")\ncheck_adf(daily_sales[\"TotalPrice\"].diff(7).dropna())\n\nprint(\"\\nPerforming Augmented Dickey-Fuller test for 1st Difference of Work Days Sales\")\ncheck_adf(daily_sales_workdays[\"TotalPrice\"].diff(6).dropna())","a9328ad5":"decompose = seasonal_decompose(daily_sales[\"TotalPrice\"], period = 7)","ec58867b":"fig, axes = plt.subplots(4, 1, sharex = True, figsize = (15, 10), facecolor = \"#e5e5e5\")\n\nfor ax in axes: ax.set_facecolor(\"#101820\")\n\ndecompose.observed.plot(ax = axes[0]).set_title(\"Observed\")\ndecompose.trend.plot(ax = axes[1]).set_title(\"Trend\")\ndecompose.seasonal.plot(ax = axes[2]).set_title(\"Seasonal\")\ndecompose.resid.plot(ax = axes[3]).set_title(\"Residual\")\n\nsns.despine()\nplt.tight_layout()\nplt.show()","56040032":"fig, axes = plt.subplots(1, 2, figsize = (14, 4), facecolor = \"#e5e5e5\")\n\nfor ax in axes: ax.set_facecolor(\"#e5e5e5\")\n    \nsns.kdeplot(decompose.resid, ax = axes[0]).set_title(\"Density for Residual\")\n\nwith warnings.catch_warnings(): \n    warnings.simplefilter(\"ignore\")\n    \n    sm.qqplot(decompose.resid.dropna(), ax = axes[1], marker = \"x\", line = \"45\", fit = True)\n    axes[1].set_title(\"QQ Plot for Residual\")\n    \nplt.show()","cd601fb8":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nfig, axes = plt.subplots(2, 1, sharex = True, figsize = (15, 10), facecolor = \"#e5e5e5\")\n\nfor ax in axes: ax.set_facecolor(\"#e5e5e5\")\n    \nplot_acf(daily_sales[\"TotalPrice\"].diff(7).dropna(), lags = 60, ax = axes[0])\nplot_pacf(daily_sales[\"TotalPrice\"].diff(7).dropna(), lags = 60, ax = axes[1])\n\nsns.despine()\nplt.show()","65cef9c9":"daily_data = daily_sales[[\"TotalPrice\"]].dropna().reset_index()\n\ndaily_data.columns = [\"ds\", \"y\"]\n\ntrain_size = int(0.85 * len(daily_data))\n\ntrain = daily_data.iloc[:train_size]\nval = pd.DataFrame(daily_data.iloc[train_size:])\n\nprint(f\"Training Days:\\t\\t{len(train)} \\nValidation Days:\\t {len(val)}\")","c0eec013":"def scores(y_true, y_pred):\n    print(f\"R2: {r2_score(y_true, y_pred)}\")\n    print(f\"MSE: {mean_squared_error(y_true, y_pred)}\")\n    print(f\"Correlation: {np.corrcoef(y_true, y_pred)[0][1]}\")","7fdf9633":"from fbprophet import Prophet\n\nmodel = Prophet()\n\nmodel.fit(train)\n\nval_pred = model.predict(val)\ntrain_pred = model.predict(train)","67478b21":"print(\"For Training set: \\n\")\nscores(train.y, train_pred.yhat)\n\nprint(\"\\nFor Validation set: \\n\")\nscores(val.y, val_pred.yhat)","a33e1469":"fig, ax = plt.subplots(figsize = (14, 5), facecolor = \"#e5e5e5\")\nax.set_facecolor(\"#101820\")\n\nsns.lineplot(x = daily_data.ds, y = daily_data.y, ax = ax)\nsns.lineplot(x = val_pred.ds, y = val_pred.yhat, ax = ax)\n\nsns.despine()\nplt.show()","785595a2":"black_friday = pd.DataFrame(\n    {\n        \"holiday\": \"black friday\",\n        \"ds\": pd.to_datetime([\"2011-11-24\", \"2012-11-23\", \"2013-11-22\"]),\n        \"lower_window\": 0,\n        \"upper_window\": 1\n    }\n)","914d4bcf":"def is_saturday(ds):\n    date = pd.to_datetime(ds)\n    return date.day_name() == \"Saturday\"\n\ndaily_data[\"is_saturday\"] = daily_data[\"ds\"].apply(is_saturday)\n\ntrain = daily_data.iloc[:train_size]\nval = pd.DataFrame(daily_data.iloc[train_size:])","28b204f3":"model = Prophet(\n    holidays = black_friday,\n    daily_seasonality = True, \n    weekly_seasonality = True,\n    holidays_prior_scale = 1,\n    seasonality_prior_scale = 5,\n    changepoint_prior_scale = 1,\n)\n\nmodel.add_country_holidays(country_name = \"UK\")\n\nmodel.add_regressor(\"is_saturday\")\n\nmodel.fit(train)\n\nval_pred = model.predict(val)\ntrain_pred = model.predict(train)","a0ef7098":"print(\"For Training set: \\n\")\nscores(train.y, train_pred.yhat)\n\nprint(\"\\nFor Validation set: \\n\")\nscores(val.y, val_pred.yhat)","3a826e27":"fig, ax = plt.subplots(figsize = (14, 5), facecolor = \"#e5e5e5\")\nax.set_facecolor(\"#101820\")\n\nsns.lineplot(x = daily_data.ds, y = daily_data.y, ax = ax, label = \"Original Data\")\nsns.lineplot(x = train_pred.ds, y = train_pred.yhat, alpha = 0.8, ax = ax, label = \"Train Predictions\")\nsns.lineplot(x = val_pred.ds, y = val_pred.yhat, ax = ax, alpha = 0.8, label = \"Validation Predictions\")\n\nax.legend(labelcolor = \"#e5e5e5\", facecolor = \"#101820\")\nsns.despine()\nplt.show()","ba608146":"train_pred[\"yhat\"] = np.where((train_pred.is_saturday == 0), train_pred.yhat, 0)\nval_pred[\"yhat\"] = np.where((val_pred.is_saturday == 0), val_pred.yhat, 0)\n\nprint(\"For Training set: \\n\")\nscores(train.y, train_pred.yhat)\n\nprint(\"\\nFor Validation set: \\n\")\nscores(val.y, val_pred.yhat)","8e0796f0":"fig, ax = plt.subplots(figsize = (14, 5), facecolor = \"#e5e5e5\")\nax.set_facecolor(\"#101820\")\n\nsns.lineplot(x = daily_data.ds, y = daily_data.y, ax = ax, label = \"Original Data\")\nsns.lineplot(x = train_pred.ds, y = train_pred.yhat, alpha = 0.8, ax = ax, label = \"Train Predictions\")\nsns.lineplot(x = val_pred.ds, y = val_pred.yhat, ax = ax, alpha = 0.8, label = \"Validation Predictions\")\n\nax.legend(labelcolor = \"#e5e5e5\", facecolor = \"#101820\")\nsns.despine()\nplt.show()","b34f8f37":"model = Prophet(\n    holidays = black_friday,\n    daily_seasonality = True, \n    weekly_seasonality = True,\n    yearly_seasonality = True,\n    holidays_prior_scale = 1,\n    seasonality_prior_scale = 5,\n    changepoint_prior_scale = 1,\n)\n\nmodel.add_country_holidays(country_name = \"UK\")\n\nmodel.add_regressor(\"is_saturday\")\n\nmodel.fit(daily_data)","c8739aea":"future = model.make_future_dataframe(periods = 365)\n\nfuture[\"is_saturday\"] = future[\"ds\"].apply(is_saturday)\nfuture.tail()","2178f704":"forecast = model.predict(future)\nforecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]].tail()","476602be":"forecast[\"yhat\"] = np.where((forecast.is_saturday == 0), forecast.yhat, 0)\nforecast[\"yhat_lower\"] = np.where((forecast.is_saturday == 0), forecast.yhat_lower, 0)\nforecast[\"yhat_upper\"] = np.where((forecast.is_saturday == 0), forecast.yhat_upper, 0)\n\nfig, ax = plt.subplots(figsize = (9, 3), facecolor = \"#e5e5e5\", dpi = 300)\n\nmodel.plot(forecast, ax = ax)\n\nax.set_title(\"Daily Revenue Forecast\")\nax.grid(False)\nax.set_facecolor(\"#e5e5e5\")\nax.set_ylim(0, 70000)\n\nsns.despine()\nplt.show()","85d7ac3d":"<a id=\"section-four-two\"><\/a>\n\n\n# 4.2) RFM Segments","d160d48a":"Our dataset contains invoice records for more than one year. Let's apply cohort analysis. We can create monthly cohorts.\n\nWe will group customers for first invoice record. Cohort index will be number of months since first transaction.","e0dbc410":"We make the series stationary. Let's decompose and examine its components.","056fbd86":"[take me to the top](#section-top)","b0c25e44":"<a id=\"section-four-four\"><\/a>\n\n# 4.4) Visualizing RFM Segments","65adda8f":"<a id=\"section-two-one\"><\/a>\n\n# 2.1) Retention Rate\n\n![](https:\/\/images.ctfassets.net\/vrkkgjbn4fsk\/7KUaWGQ9IJvf85OheLucgk\/e03a27e5c68cc317b38dda354fee504d\/how_to_calculate_user_retention_rate)\n\n*https:\/\/amplitude.com\/blog\/how-to-calculate-retention-rate-b2b-saas","526b7be6":"<a id=\"section-five\"><\/a>\n\n# 5) Forecasting with Prophet","90637e04":"<a id=\"section-five-two\"><\/a>\n\n# 5.2) Stationarity","70e2fe15":"[take me to the top](#section-top)","c9187866":"<a id=\"section-five-one-one\"><\/a>\n\n# 5.1.1) Weekly","eca34992":"<a id=\"section-five-three\"><\/a>\n\n# 5.3) Seasonality, Decomposition","88a933dd":"I just convert data to daily form with using resample method. Let's dive in.","5be99230":"<a id=\"section-four-three\"><\/a>\n\n# 4.3) Visualizing RFM Grid\n\n![](https:\/\/www.wigzo.com\/blog\/wp-content\/uploads\/2021\/06\/RFM-Analysis.jpg)\n\n*https:\/\/www.wigzo.com\/blog\/wp-content\/uploads\/2021\/06\/RFM-Analysis.jpg","0798bc24":"<a id=\"section-three-one\"><\/a>\n\n# 3.1) Pareto Chart for Customers","ce772594":"I will use daily sales data for model training. We have 0 values on saturdays, it is fixed. We can get rid of these records with subsetting the data, just like stock prices. \n\nFor training set, I just get first 85% records of all days, and remaining part is validation set.\n\nFirst of all, I want to say again, my main goal is getting more detailed project about this dataset. As we talk theoretical part, I think this dataset is not fully convenient for forecasting. We have one year data and after then major peak, we don't have a lot observations to predicting the behaivor of data.\n\nThis part would be a simple introduction of Facebook's Prophet, and I probably prepare more detailed notebooks about these concepts.","73069988":"[take me to the top](#section-top)","510936ae":"68% R2 for validation set, good increasing.","4e25609d":"0 unit price?","bb63f06e":"**Sample Pareto Chart**\n\n![](https:\/\/www.cec.health.nsw.gov.au\/__data\/assets\/image\/0005\/341285\/Pareto-1.png)\n\n*https:\/\/www.cec.health.nsw.gov.au\/Quality-Improvement-Academy\/quality-improvement-tools\/pareto-charts","bf235367":"# 4) RFM Analysis","b9fb9258":"[take me to the top](#section-top)","02864665":"We can saw trend, seasonal component and residual. We know we have weekly seasonality.\n\nWe need normal distributed residual. Let's look at it.","7870caa9":"Some stock codes have a letter at the end of their codes. I don't know what they refers, so I will keep them.","1c6baf50":"<a id=\"section-conc\"><\/a>\n\n# Conclusion\n\nI probably update this notebook with related content.\n\n**Feel free to make comments, ask questions, criticise me.**","809b328f":"Cohort analysis is a subset of behavioral analytics that takes the data from a given eCommerce platform, web application, or online game and rather than looking at all users as one unit, it breaks them into related groups for analysis. These related groups, or cohorts, usually share common characteristics or experiences within a defined time-span.\n\nCohort analysis is a tool to measure user engagement over time. It helps to know whether user engagement is actually getting better over time or is only appearing to improve because of growth.\n\nCohort analysis proves to be valuable because it helps to separate growth metrics from engagement metrics as growth can easily mask engagement problems. In reality, the lack of activity of the old users is being hidden by the impressive growth numbers of new users, which results in concealing the lack of engagement from a small number of people.\n\n*https:\/\/clevertap.com\/blog\/cohort-analysis\/","389e61fe":"No, we only have proper invoices and cancellations for InvoiceNo. We don't have any different pattern.","bf5802f0":"[take me to the top](#section-top)","8e6f85a3":"<a id=\"section-three\"><\/a>\n\n# 3) Pareto Principle","d4bd0ab7":"Also you can use add_country_holidays(country) method for adding all holidays for a country.","f53610a2":"We have a problem, we get negative valued forecasts. Actually, using \"floor\" and \"cap\" didn't work for me. http:\/\/facebook.github.io\/prophet\/docs\/saturating_forecasts.html#saturating-minimum\n\nTo get rid of this, I will manually set them zero.","217cd613":"In saturdays, we don't have any sale record. I will simply add \"is_saturday\" column as an extra regressor.","d5aa920c":"Here is the results of this year's predictions and next year's forecast. Black dots represent actual revenue, and blue lines represent forecasts.\n\nIn the last months of the year, our predictions on the border of upper confidence interval, or sometimes outside of it.\n\nIf we look at next year's forecast, we can see minor peaks in the middle of the year, and also we have positive trend late of the year.","0bdf51a9":"I will use only UK sales for this analysis.","82be7cea":"The Pareto principle states that for many outcomes, roughly 80% of consequences come from 20% of causes (the \u201cvital few\u201d).\n\nOther names for this principle are the 80\/20 rule, the law of the vital few, or the principle of factor sparsity.\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/eb\/Pareto_principle.png)\n\n*https:\/\/en.wikipedia.org\/wiki\/Pareto_principle","3c3c9447":"[take me to the top](#section-top)","4318fa4b":"For this problem our metrics are R2, mean squared error and correlation. Actually, I don't like to use infinite intervaled metrics like MSE, but I just add it for variety.","4c6794a0":"**Nice!**","20d0211b":"<a id=\"section-five-four\"><\/a>\n\n# 5.4) Modelling","4e31fd53":"Well, lets interpret these two charts. Actually, we don't need to write a lot of things. \n\nWe can see that 80% of company's revenue comes from top 30% of customers.\n\nAlso, 90% of company's revenue comes from top 48% of customers.","21fd0210":"I didn't find any pattern. So, I remove them.","173da1eb":"Above data is our cohort table. Its interpretation is simple. For example, We have 874 unique customer with their first transaction is in 2010-12. Its cohort month is 2010-12 and cohort index is 1. Go on the one right cell, it is 319.\nIts mean, 319 unique customer retain their customership for next month.","17d687ac":"<a id=\"section-five-four-two\"><\/a>\n\n# 5.4.2) Prophet's parameters","916d5059":"As we can see above daily revenue graphs, we have an increasing trend for end of the year.","50026bfb":"As we can see kde plot and QQ plot, residual has normal distribution.","a53694da":"Great!\n\n**Retention rate increases significantly for last months of the year.**\n\nProbably, Thanksgiving, Black Friday and Christmas causes it. There are lots of special day at the end of year.","58a5c193":"<a id=\"section-five-one-two\"><\/a>\n\n# 5.1.2) Daily Data","efb0763d":"For ADF test, this series is not stationary. To make it stationary, we have two main option.\n\n1- Take difference until it become stationary.\n\n2- Apply transformations: Log, square root, etc.","4ad17083":"Okey, we don't have any record at 69 days. 53 of 69 is saturday. This retailer could be closed on saturdays. Let's look at total revenues per weekday.","8fc6fcb8":"We have 373 days sales record. Daily data can be more suitable.","bb3015c5":"[take me to the top](#section-top)","a5d9a0f8":"We can also plot it in log scale. It helps us for better visualization.","23db6bec":"I don't know is it right source of below segmentation map dict, but I found that code here.\nhttps:\/\/guillaume-martin.github.io\/rfm-segmentation-with-python.html","9244cb29":"**Lets implement Pareto's 80-20 rule to our dataset**. We have two hypothesis:\n\n    1) 80% of company's revenue comes from 20% of total customers.\n\n    2) 80% of company's revenue comes from 20% of total products.","96715a80":"When description is null, we have no available customer id and zero unit price for all data. Let's drop nan values.","768e654e":"Cohort analysis is a better way of looking at data. Its application is not limited to a single industry or function. For example, eCommerce companies can use cohort analysis to spot products that have more potential for sales growth. In digital marketing, it can help identify web pages that perform well based on time spent on websites, conversions or sign-ups. In product marketing, this analysis can be used to identify the success of feature adoption rate and also to reduce churn rates.\n\n*https:\/\/www.moengage.com\/blog\/growth-tactic-1-how-to-use-cohort-analysis-to-measure-customer-retention\/","c371723a":"Can we have both cancellation record, and record before cancellation. I mean, for example, we have C536379, have we 536379 ?","849dca27":"Stock Codes generally contains 5 digit numerical codes.","45e2348a":"It seems, the model does not capture huge peaks.\n\nLet's add some parameters.","5fa8d477":"[take me to the top](#section-top)","1041d8a6":"<a id=\"section-four\"><\/a>\n\n\nRecency, frequency, monetary value is a marketing analysis tool used to identify a company's or an organization's best customers by using certain measures. The RFM model is based on three quantitative factors:\n\n**Recency**: How recently a customer has made a purchase\n\n**Frequency**: How often a customer makes a purchase\n\n**Monetary** Value: How much money a customer spends on purchases\n\nRFM analysis numerically ranks a customer in each of these three categories, generally on a scale of 1 to 5 (the higher the number, the better the result). The \"best\" customer would receive a top score in every category.\n\n*https:\/\/www.investopedia.com\/terms\/r\/rfm-recency-frequency-monetary-value.asp\n\n![](https:\/\/d35fo82fjcw0y8.cloudfront.net\/2018\/03\/01013508\/Incontent_image.png)\n\n*https:\/\/clevertap.com\/blog\/rfm-analysis\/","838450dc":"A cohort simply means that a group of people they have same characteristics.\n\nGenerally, we have three type of cohort analysis:\n\n   - **Time cohorts** or **Acquisition cohorts**: Groups are divided by first activity.\n   - **Behavior cohorts** or **Segment-Based cohorts**: Groups are divided by their behaivors and actions about your service.\n   - **Size cohorts**: Size-based cohorts refer to the various sizes of customers who purchase a company\u2019s products or services.\n","f4e14447":"I define below functions for calculation and visualization.\n\n**prepare_pareto_data** finds individual revenue per customer\/product and calculates cumulative percentage of them.\n\n**create_pareto_plot** takes output from these data and visualize it.","5311a1a2":"It looks like data contains more than customer transactions. I will drop them.","c564c689":"**To check these hypothesis, we need only two things.** \n\n1) Individual sale records for customer\/product\n\n2) Calculating cumulative sum for them","d0e658c2":"Well, maybe we have different pattern about InvoiceNo. Let's check it","8b82e3f7":"<a id=\"section-five-two-one\"><\/a>\n\n# 5.2.1) Augmented Dickey-Fuller Test","c4a7eac7":"We can show table of descriptive statistics for RFM segments, but it is not best way. Using data visualization skills and creating great plots as important as finding great results.","d9d42c98":"[take me to the top](#section-top)","d755b558":"Well, I'm surprised noone plot the above graph.\n\nI wrote these codes for visualizing above RFM grid.","1a1c0760":"<a id=\"section-top\"><\/a>\n# Table of Contents\n* [Introduction](#section-intro)\n\n* [1. General Infos & Playing with Features](#section-one)\n\n\n* [2. Cohort Analysis](#section-two)\n     - [2.1 Retention Rate](#section-two-one)\n\n\n* [3. Pareto Principle](#section-three)\n     - [3.1 Pareto Chart for Customers](#section-three-one)\n     - [3.2 Pareto Chart for Products](#section-three-two)\n\n\n* [4. RFM Analysis](#section-four)\n    - [4.1 Preparing RFM Table](#section-four-one)\n    - [4.2 RFM Segments](#section-four-two)\n    - [4.3 Visualizing RFM Grid](#section-four-three)\n    - [4.4 Visualizing RFM Segments](#section-four-four)\n    - [4.4 Customer Segmentation with using RFM Metrics](#section-four-five)\n    \n    \n* [5. Forecasting with Prophet](#section-five)\n     - [5.1 Data Preparation for Time Series](#section-five-one)\n         - [5.1.1 Weekly](#section-five-one-one)\n         - [5.1.2 Daily](#section-five-one-two)\n     - [5.2 Stationarity](#section-five-two)\n         - [5.2.1 Augmented Dickey-Fuller Test](#section-five-two-one)\n     - [5.3. Seasonality, Decomposition](#section-five-three)\n     - [5.4. Modelling](#section-five-four)\n         - [5.4.1 Simple Model](#section-five-four-one)\n         - [5.4.2 Prophet's parameters](#section-five-four-two)\n         - [5.4.3 Forecasting Future](#section-five-four-three)\n         \n\n\n* [Resources, Readings](#section-zero)\n\n* [Conclusion](#section-conc)\n\n\n**First version: 14.8.2021**\n\n**Last Edit: 29.10.2021**","6ebd55f9":"I just standardize descriptions with converting them to all lowercase characters.","167d0774":"At first look, we can see records that have missing customer id, there is no specific characteristics.\n\nStockCode contains non-numeric records i.e. DOT. It is a cue for examining stock codes.","605a1708":"We get 48% R2 for validation. Let's plot that results.","aeb773df":"Let's look at above retention rate chart and interpret it.\n\n40.05% of customers that made their first shopping in January 2011, use this company after five months.","73890123":"Here, we have revenue per week graph. Let's interpret it.\n\nEarlier January, this retailer was closed. Revenue is 0.\n\nWe see first peak in the middle of May.\n\nSecond peak is in the beginning of October.\n\nThen the revenue top out in the middle of November.","d0fbe63a":"Lastly, I want to quote from [@carlmcbrideellis](https:\/\/www.kaggle.com\/carlmcbrideellis)\n\n> It should go without saying, but:\n> \n> * If you like a notebook: give it an upvote\n> * If you learnt something from a notebook: upvote\n> * If you fork and adapt a notebook: upvote\n> * If you fork, run and submit, just for the score: \u00a1upvote!\n> \n\n*https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/250798#1375745","22ddf428":"Prophet gives lots of options in model building step.\n\nholidays: You can use this for adding special days to model.\n\nseasonality: If the data has seasonality, you can set daily_seasonality, weekly_seasonality, and yearly_seasonality parameters to True.\n\n_prior_scale: This parameter controls the flexibility of components' affects.\n\nIn this model, I just set them with simple numbers. You can turn them with [0.01-10] range.","315bb921":"[take me to the top](#section-top)","b438a91f":"47.5% of total revenue comes from \"Champions\" segment, and 28% of total revenue comes from \"Loyal Customers\" segment. These two segments have 75% of company's total revenue.","d1b327d3":"<a id=\"section-intro\"><\/a>\n\n# Introduction\n\nThis is Online Retail dataset from UCI Machine Learning Repository. It contains transactions from 2010 and 2011.\n\nFor other datasets from UCI ML https:\/\/archive.ics.uci.edu\/ml\/datasets.php","d512e0b5":"[take me to the top](#section-top)","7b28a4ba":"<a id=\"section-five-four-three\"><\/a>\n\n# 5.4.3) Forecasting Future","b575a5ab":"<a id=\"section-four-five\"><\/a>\n\n# 4.5) Customer Segmentation with using RFM Metrics","a389d946":"This is my first work about marketing. I have finished Datacamp's marketing course for a long time ago, but didn't practise it.\n\nI prepare this notebook for practising all these stuff.\n\nThis is my full work, I also separate it into chapters. You can take a look to chapters individually.\n\nhttps:\/\/www.kaggle.com\/mustafacicek\/online-retail-part-i-preparing-getting-insights\n\nhttps:\/\/www.kaggle.com\/mustafacicek\/marketing-analytics-cohort-analysis\n\nhttps:\/\/www.kaggle.com\/mustafacicek\/marketing-analytics-pareto-principle\n\nhttps:\/\/www.kaggle.com\/mustafacicek\/marketing-analytics-rfm-analysis\n\nhttps:\/\/www.kaggle.com\/mustafacicek\/marketing-analytics-forecasting\n\nYou can reach prepared data from here https:\/\/www.kaggle.com\/mustafacicek\/online-retail-final","cfa41640":"First model is simple prophet model with default parameters. We don't have good results, we have 53% R2 for training and 48% R2 for validation set.","d7d01503":"InvoiceNo has coded with 6 digit numeric characters. We can see that some InvoiceNo records starts with the letter C. This means cancellation.","148f662c":"<a id=\"section-five-four-one\"><\/a>\n\n# 5.4.1) Simple Model","cbf8b2d3":"[take me to the top](#section-top)","a7223e11":"[take me to the top](#section-top)","82afcd58":"When description is null, we have 0 unit price and missing customer ids. Let's check for whole data.","2e0dde1f":"Our model can not capture sudden peaks well. Actually, we have to investigate the reasons underlying this peaks. Is it yearly seasonal, or not? We can not detect it since we have only one year data.","d5dafde5":"213 Stock codes have more than one description. Let's check some of them.","7af5d454":"We have missing values for column Description and CustomerID. Go deeper","f6646cd0":"We can see that 80% of company's revenue comes from top 23% of products that have most revenue.\n\nAlso, 90% of company's revenue comes from 36% of products that have most revenue.\n\nMaybe, if the company reduce by half its variety of items, revenue couldn't decrease significantly.","23d04407":"<a id=\"section-one\"><\/a>\n\n# 1) General Infos & Playing with Features","8966b45d":"[take me to the top](#section-top)","f0412231":"Nothing, we have just cancellation.","c0de79ad":"We are done with systematically missing values. But lets go deeper.","d8901c86":"Retention tables show a group of people that visited your site or used your app for the first time during a certain time frame. They also display the progressive \u2018drop-off\u2019 or decline in activity over time for that particular group (a cohort).\n\nMarketers can use Retention tables to analyze the quality of users brought by a marketing campaign and compare it to other sources of traffic.\n\n*https:\/\/www.smartlook.com\/blog\/retention-tables-introduction\/","a90f22ae":"<a id=\"section-two\"><\/a>\n\n# 2) Cohort Analysis","02e98fd4":"Stock Codes - Description","b5bffd0a":"**Champions**: Bought recently, buy often and spend the most\n\n**Loyal customers**: Buy on a regular basis. Responsive to promotions.\n\n**Potential loyalist**: Recent customers with average frequency.\n\n**Recent customers**: Bought most recently, but not often.\n\n**Promising**: Recent shoppers, but haven\u2019t spent much.\n\n**Needs attention**: Above average recency, frequency and monetary values. May not have bought very recently though.\n\n**About to sleep**: Below average recency and frequency. Will lose them if not reactivated.\n\n**At risk**: Some time since they\u2019ve purchased. Need to bring them back!\n\n**Can\u2019t lose them**: Used to purchase frequently but haven\u2019t returned for a long time.\n\n**Hibernating**: Last purchase was long back and low number of orders. May be lost.\n\n*https:\/\/futurice.com\/blog\/know-your-customers-with-rfm","75aeb873":"Let's check stationarity. Stationarity simply means that, our series has constant mean, variance on different times.\n\nIn above rolling mean plots, we saw that our mean values increase over time.\n\nBut, let's check it with Augmented Dickey Fuller test.","862d3370":"Nice, we have a constant mean over time. Let's look at ADF test results of 1st differenced series.","eb7a3bcb":"We can take difference with using .diff() method.\n\nIn this problem, our series has seasonal behaivor. Our revenue is increasing from saturday to thursday, then it decrease. For taking difference of seasonal series, we need set period in diff method.","80bbe68b":"<a id=\"section-zero\"><\/a>\n\n# Resources, Readings\n\nDatacamp's https:\/\/www.datacamp.com\/courses\/customer-segmentation-in-python course\n\nhttps:\/\/www.kaggle.com\/allunia\/e-commerce-sales-forecast I inspired some parts in analysing features.\n\nhttps:\/\/www.kaggle.com\/fabiendaniel\/customer-segmentation It is also exciting notebook, probably you have already looked.\n\nBasic readings about concept:\n\nhttps:\/\/clevertap.com\/blog\/cohort-analysis\/\n\nhttps:\/\/en.wikipedia.org\/wiki\/Pareto_principle\n\nhttps:\/\/www.investopedia.com\/terms\/p\/paretoprinciple.asp\n\nhttps:\/\/clevertap.com\/blog\/rfm-analysis\/\n\nhttps:\/\/www.wigzo.com\/blog\/rfm-analysis-for-ecommerce\/\n\nhttps:\/\/www.investopedia.com\/terms\/r\/rfm-recency-frequency-monetary-value.asp\n\nhttps:\/\/futurice.com\/blog\/know-your-customers-with-rfm","9a5b66cf":"Let's perform RFM Analysis on our data.","1267c0d6":"<a id=\"section-three-two\"><\/a>\n\n# 3.2) Pareto Chart for Products","cc39d965":"Now, I will build a new model for forecasting next year. I just add yearly seasonality to last model that we used.","b19e1579":"Cancelled invoices have negative quantity.","c6f68ebd":"[take me to the top](#section-top)","cff3ca84":"75%          12.000000\n\nmax       80995.000000\n\nLet's look at these outliers.","ba22b791":"<a id=\"section-five-one\"><\/a>\n\n# 5.1) Data Preparation for Time Series","49a1b0fd":"We can add holidays to prophet model with below format.\n\nI just add \"Black Friday\" as a holiday for three years.","1616d6a8":"Seems we have just a litle differences between them, i.e. \",\" or \"\/\"","65feb75b":"<a id=\"section-four-one\"><\/a>\n\n# 4.1) Preparing RFM Table","7267c94c":"Yeap, this retailer closed on saturdays.","8247a42c":"Sometimes, missing values are filled with some denotations. \"NAN\", \"na\", \"?\", \"Unknown\", and so on. Let's check them.","4de0591e":"We saw that there is a zero revenue week in earlier January."}}