{"cell_type":{"9fdbc8c8":"code","85222889":"code","299bd0a1":"code","527e6b20":"code","77005b67":"code","701155bf":"code","bea64337":"code","d69d4a64":"code","3a4d52c6":"code","0105cbc6":"code","0b495a45":"code","f47003f6":"code","a8a8d599":"code","be161e34":"code","b465c2db":"code","1e312bd4":"code","e7aa6a83":"code","9b778ca4":"code","ec8a93bc":"code","211e7ca0":"code","f8474fe0":"code","842d486e":"code","a8960e24":"code","50a41e12":"markdown","b25778cf":"markdown","d0c0ad15":"markdown","232ea4b7":"markdown","bcfba670":"markdown","025ad476":"markdown","c60b14f2":"markdown","62a8ea93":"markdown","bddfcc59":"markdown","24da202f":"markdown","3a3fb0b7":"markdown","37e26bb5":"markdown","d2b37575":"markdown","21133097":"markdown","87d51f24":"markdown"},"source":{"9fdbc8c8":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.utils import shuffle\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nfrom IPython.display import clear_output\nimport tensorflow as tf","85222889":"path = '..\/input\/pima-indians-diabetes-database\/diabetes.csv'\ndf = pd.read_csv(path)\ndf.fillna(df.mean(), inplace=True)\ndf = shuffle(df)\ndf.head()","299bd0a1":"features = [\"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\",\"Age\"]","527e6b20":"for feat in features:\n    df[feat] \/= df[feat].max()\ndf.head()","77005b67":"_ = plt.figure(figsize=(14,8))\n_ = sns.heatmap(df.corr(), \n        xticklabels=df.corr().columns,\n        yticklabels=df.corr().columns)\nplt.show()","701155bf":"features = df.drop('Outcome', axis=1)\nlabels = df['Outcome']","bea64337":"X_train, X_test, y_train, y_test = train_test_split(\n                                        features, labels, test_size=0.20, random_state=42)","d69d4a64":"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(16, activation='sigmoid', input_shape=(8, )),\n    tf.keras.layers.Dense(32, activation='sigmoid'),\n    tf.keras.layers.Dense(64, activation='sigmoid'),\n    tf.keras.layers.Dense(16, activation='sigmoid'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n  ])","3a4d52c6":"loss_object = tf.keras.losses.BinaryCrossentropy(\n                    from_logits=False, label_smoothing=0, \n                    name='binary_crossentropy'\n                )","0105cbc6":"def loss(model, x, y, training):\n    y_ = model(x, training=training)\n\n    return loss_object(y_true=y, y_pred=y_)","0b495a45":"def grad(model, inputs, targets):\n    with tf.GradientTape() as tape:\n        loss_value = loss(model, inputs, targets, training=True)\n    return loss_value, tape.gradient(loss_value, model.trainable_variables)","f47003f6":"optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01, rho=0.1, \n                                        momentum=0.1, epsilon=1e-03)","a8a8d599":"y_temp = y_train.values\nX, y = X_train.values, y_temp.reshape(y_temp.shape[0], 1)\n\ny_temp = y_test.values\nX_val, y_val = X_test.values, y_temp.reshape(y_temp.shape[0], 1)","be161e34":"num_epochs = 3001\n\ntrain_loss_results = []\ntrain_accuracy_results = []\n\ntest_loss_results = []\ntest_accuracy_results = []\n\nloss_fn = tf.keras.metrics.Mean()\nacc_fn = tf.keras.metrics.BinaryAccuracy()\n\nfor epoch in range(num_epochs):\n    epoch_loss_avg = tf.keras.metrics.Mean()\n    epoch_accuracy = tf.keras.metrics.BinaryAccuracy()\n\n    batches = np.array_split(np.arange(len(X)), len(X) \/\/ 8)\n    batches = [b.tolist() for b in batches]\n\n    for batch in batches:\n        X_b, y_b = X[batch], y[batch]\n        # Optimize the model\n        loss_value, grads = grad(model, X_b, y_b)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n        # Track progress\n        epoch_loss_avg.update_state(loss_value)\n        epoch_accuracy.update_state(y_b, model(X_b, training=True))\n\n    # End epoch\n    train_loss_results.append(epoch_loss_avg.result())\n    train_accuracy_results.append(epoch_accuracy.result())\n        \n    test_loss_results.append(loss_fn(y_val, model(X_val)).numpy())\n    test_accuracy_results.append(acc_fn(y_val, model(X_val)).numpy())\n\n    if epoch % 300 == 0:\n        print(\"Epoch {:3d}: Train_Loss:{:3.3f}, Train_Accuracy:{:3.3f}, Test_Loss:{:3.3f}, Test_Accuracy:{:3.3f}\"\n              .format(epoch, epoch_loss_avg.result(), epoch_accuracy.result(),\n                      test_loss_results[-1], test_accuracy_results[-1]))","b465c2db":"fig, axes = plt.subplots(2, sharex=True, figsize=(14, 8))\nfig.suptitle('Training Metrics')\n\naxes[0].set_ylabel(\"Loss\", fontsize=14)\naxes[0].plot(train_loss_results)\n\naxes[1].set_ylabel(\"Accuracy\", fontsize=14)\naxes[1].set_xlabel(\"Epoch\", fontsize=14)\naxes[1].plot(train_accuracy_results)\nplt.show()","1e312bd4":"fig, axes = plt.subplots(2, sharex=True, figsize=(14, 8))\nfig.suptitle('Testing Metrics')\n\naxes[0].set_ylabel(\"Loss\", fontsize=14)\naxes[0].plot(test_loss_results)\n\naxes[1].set_ylabel(\"Accuracy\", fontsize=14)\naxes[1].set_xlabel(\"Epoch\", fontsize=14)\naxes[1].plot(test_accuracy_results)\nplt.show()","e7aa6a83":"fc = tf.feature_column\nNUMERIC_COLUMNS = list(df.drop('Outcome', axis=1).columns)\n\ndef one_hot_cat_column(feature_name, vocab):\n    return fc.indicator_column(\n      fc.categorical_column_with_vocabulary_list(feature_name,\n                                                 vocab))\nfeature_columns = []\n\nfor feature_name in NUMERIC_COLUMNS:\n    feature_columns.append(fc.numeric_column(feature_name,\n                                           dtype=tf.float32))","9b778ca4":"n_batches = 4\n\nest = tf.estimator.BoostedTreesClassifier(feature_columns,\n                                          n_batches_per_layer=n_batches)","ec8a93bc":"NUM_EXAMPLES = len(y_train)\n\ndef make_input_fn(X, y, n_epochs=None, shuffle=True):\n    def input_fn():\n        dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n        if shuffle:\n            dataset = dataset.shuffle(NUM_EXAMPLES)\n        # For training, cycle thru dataset as many times as need (n_epochs=None).\n        dataset = dataset.repeat(n_epochs)\n        # In memory training doesn't use batching.\n        dataset = dataset.batch(NUM_EXAMPLES)\n        return dataset\n    return input_fn","211e7ca0":"train_input_fn = make_input_fn(X_train, y_train)\neval_input_fn = make_input_fn(X_test, y_test, shuffle=False, n_epochs=1)","f8474fe0":"NUM_EXAMPLES = len(y_train)\n\nest.train(train_input_fn, max_steps=300)","842d486e":"result = est.evaluate(eval_input_fn)\nclear_output()\n\nprint(pd.Series(result))","a8960e24":"pred_dicts = list(est.predict(eval_input_fn))\nprobs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\n\nprobs.plot(kind='hist', bins=20, title='predicted probabilities')\nplt.show()","50a41e12":"## Model","b25778cf":"<h1 id=\"boosted\" style=\"color:#301202; background:#d26231; border:0.5px dotted;\"> \n    <center>Boosted Trees\n        <a class=\"anchor-link\" href=\"#boosted\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","d0c0ad15":"<div width=\"100%\">\n    <img width=\"100%\" src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/228\/482\/a520351269b547c89afe790820a1087e\/dataset-cover.jpeg\"\/>\n<\/div>","232ea4b7":"<h1 id=\"ann\" style=\"color:#301202; background:#d26231; border:0.5px dotted;\"> \n    <center>Artificial neural network\n        <a class=\"anchor-link\" href=\"#ann\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","bcfba670":"## Optimizer","025ad476":"## Create dataset","c60b14f2":"<h1 id=\"dataset\" style=\"color:#301202; background:#d26231; border:0.5px dotted;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","62a8ea93":"## Analysis","bddfcc59":"## Train Boosted Trees","24da202f":"## Training","3a3fb0b7":"## Analysis","37e26bb5":"## Gradient function","d2b37575":"## Feature columns","21133097":"## Loss Function","87d51f24":"## Boosted Trees Classifier"}}