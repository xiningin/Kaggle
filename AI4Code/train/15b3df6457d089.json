{"cell_type":{"560937f6":"code","d7246fb1":"code","9b777576":"code","bd2b9f61":"code","dbf7420e":"code","d612e8dc":"code","bbd996ac":"code","4efc0142":"code","3ba2e7d5":"code","dbc6b0aa":"code","a6d0133a":"code","1c2c9286":"code","da13fff6":"markdown","a04b285e":"markdown","4cc06257":"markdown","e805f2dc":"markdown","099cc42f":"markdown","d2658635":"markdown","0a7a39a1":"markdown","8e92eee2":"markdown","d67595f0":"markdown","29f67864":"markdown"},"source":{"560937f6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport xgboost as xgb\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d7246fb1":"df = pd.read_csv('\/kaggle\/input\/heart-failure-prediction\/heart.csv')\n\ndf.head()","9b777576":"df = pd.get_dummies(df)\ndf.head()","bd2b9f61":"successCorr = df.corrwith(df['HeartDisease'])\n\nSuccessCorr = successCorr.sort_values().tail(30)\n\nprint(SuccessCorr)","dbf7420e":"SuccessCorr.plot.bar(x=None, y=None)","d612e8dc":"from sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\n\ny = df.HeartDisease\nX = df.drop(['HeartDisease'], axis=1).select_dtypes(exclude=['object'])\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=1)\n\nmy_imputer = SimpleImputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntest_X = my_imputer.transform(test_X)","bbd996ac":"from xgboost import XGBRegressor\n\nmy_model = XGBRegressor()\nmy_model.fit(train_X, train_y, verbose=False)\n\nprint('train_X size =', train_X.shape)\nprint('test_X size =', test_X.shape)\npredictions = my_model.predict(train_X)\npredictions2 = my_model.predict(test_X)","4efc0142":"eval_set = [(train_X, train_y), (test_X, test_y)]\neval_metric = [\"auc\",\"error\"]\nmy_model.fit(train_X, train_y, eval_metric=eval_metric, eval_set=eval_set, verbose=False)\n\n# Evaluation results\nevaluation_results = my_model.evals_result_\nerr_train = evaluation_results['validation_0']['error'] # Train \u2018error\u2019 metric\nerr_test = evaluation_results['validation_1']['error'] # Test \u2018error\u2019 metric\n# Plotting \u2018XGBOOST Classification Error\u2019 \nplt.plot(err_train)\nplt.plot(err_test)\nplt.ylabel('Classification Error')\nplt.legend(['Train', 'Test'])\nplt.title('XGBOOST Classification Error')\nplt.show()\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolute Error on Train Data: \" + str(mean_absolute_error(predictions, train_y)))\nprint(\"Mean Absolute Error on Test Data: \" + str(mean_absolute_error(predictions2, test_y)))\n\nfrom sklearn.metrics import accuracy_score\n\npredictions5 = [round(value) for value in predictions2]\naccuracy0 = accuracy_score(test_y, predictions5)\n\nprint('Accuracy: ', (round(accuracy0,3) * 100.0), '%',sep='')","3ba2e7d5":"#n_estimators = 500\n#Early Stopping Rounds = 100\n#Learning Rate = 0.0001\n#Max Depth = 5\n#Subsample = .9999\n#Colsample_bytree = 0.4\nmy_model2 = XGBRegressor(scale_pos_weight=1, learning_rate=0.0001,  colsample_bytree = 0.4,subsample = .9999,objective='binary:logistic', n_estimators=500, reg_alpha = 0.3,max_depth=5, gamma=5)\nmy_model2.fit(train_X, train_y, early_stopping_rounds=100, eval_metric=eval_metric, eval_set=eval_set, verbose=False)\n\npredictions7 = my_model2.predict(test_X)\npredictions5 = [round(value) for value in predictions7]\naccuracy1 = accuracy_score(test_y, predictions5)\n\nprint('Accuracy: ', (round(accuracy1,5) * 100.0), '%',sep='')","dbc6b0aa":"print('Accuracy BEFORE Tuning: ', (round(accuracy0,5) * 100.0), '%',sep='')\nprint('Accuracy AFTER Tuning: ', (round(accuracy1,5) * 100.0), '%',sep='')\nprint('Improvment: ', round((round(accuracy1,5) * 100.0) - (round(accuracy0,5) * 100.0),4), '%',sep='')\nprint()\nprint(\"Mean Absolute Error on Test Data BEFORE Tuning: \" + str(mean_absolute_error(predictions2, test_y)))\nprint(\"Mean Absolute Error on Test Data AFTER Tuning: \" + str(mean_absolute_error(predictions7, test_y)))\nprint('Improvment:', round((mean_absolute_error(predictions7, test_y)) - (mean_absolute_error(predictions2, test_y)),5))","a6d0133a":"evaluation_results = my_model.evals_result_\nerr_train = evaluation_results['validation_0']['error'] # Train \u2018error\u2019 metric\nerr_test = evaluation_results['validation_1']['error'] # Test \u2018error\u2019 metric\n# Plotting \u2018XGBOOST Classification Error\u2019 \nplt.plot(err_train)\nplt.plot(err_test)\nplt.legend(['Train', 'Test'])\nplt.title('XGBOOST Classification Error BEFORE Tuning')\nplt.show()\n\n# Evaluation results\nevaluation_results = my_model2.evals_result_\nerr_train = evaluation_results['validation_0']['error'] # Train \u2018error\u2019 metric\nerr_test = evaluation_results['validation_1']['error'] # Test \u2018error\u2019 metric\n# Plotting \u2018XGBOOST Classification Error\u2019 \nplt.plot(err_train)\nplt.plot(err_test)\nplt.legend(['Train', 'Test'])\nplt.title('XGBOOST Classification Error AFTER Tuning')\nplt.show()","1c2c9286":"evaluation_results = my_model2.evals_result_\nerr_train = evaluation_results['validation_0']['error'] # Train \u2018error\u2019 metric\nerr_test = evaluation_results['validation_1']['error'] # Test \u2018error\u2019 metric\n# Plotting \u2018XGBOOST Classification Error\u2019 \nplt.plot(err_train)\nplt.plot(err_test)\nplt.legend(['Train', 'Test'])\nplt.title('XGBOOST Classification Error')\nplt.show()","da13fff6":"# Tuning Results","a04b285e":"# Creating the Machine Learning Model","4cc06257":"# Finding and Graphing Correlations in the Data","e805f2dc":"# Initial Model Evaluation","099cc42f":"**Model Statistics**\n* Accuracy = 91.304%\n* Mean Absolute Error = 0.49973078331221704\n","d2658635":"# Loading Heart Failure Data","0a7a39a1":"# Encoding The Data","8e92eee2":"# Tuning the Model","d67595f0":"This notebook was my first real attempt at using a Machine Learning Algorithm to predict the result of an event using other attributes in the data. Even with a \nlimited knowledge of Data Science and Machine Learning, I was able to use a sophisticated engine to come up with a program that has a rather high success rate. As a beginner, I wanted to recollect the method I used to evaluate and improve the prediction outcomes in my notebook for this dataset.\n\nI started off by learning about Machine Learning and how to prepare a dataset for prediction through reading a sample EDA that I found on Kaggle. This beginners EDA helped me understand encoding, which was a crucial part in making the data used in this notebook suitable to be analyzed. After I set up the data in the proper form, I used correlation functions to map out the correlations in the data, a process I learned to do in the sample EDA.\n\nFollowing this, I learned about Machine Learning Algorithms such as scikit and XGBoost, and chose to try out XGBoost as the algorithm for this dataset.\nUsing a Kaggle Learn Course on XGBoost, I learned about fitting my data and inputting it into the XGBoost Machine Learning system.\n\nI also learned the basics of tuning from this course. A majority of my time afterwards was spent researching how to accurately tune my model. Sources I found the most helpful in this process were **https:\/\/towardsdatascience.com\/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e** and **https:\/\/towardsdatascience.com\/selecting-optimal-parameters-for-xgboost-model-training-c7cd9ed5e45e**. After tuning and refining my data, presenting my data was the last task, in which I used the matplotlib library to graph my data.\n\nWorking on this notebook was a great experience, as it introduced me to the world of data science and Machine Learning. Putting your mind to something and actually completing it is the greatest feeling in the world, and the completion of this notebook further cemented that truth in me. Anyone can accomplish anything; you just need to put your mind to it and do it.","29f67864":"# Conclusions"}}