{"cell_type":{"432dc899":"code","ff31936c":"code","0e604261":"code","04aad9c6":"code","f47bd2ad":"code","86ac2e51":"code","bd707129":"code","b1e11f83":"code","51017c5e":"code","6b034977":"code","3567b6e4":"code","4764e2f3":"code","dc2ed0f7":"code","10236356":"code","fb70cab9":"code","354ba6b6":"code","8b0e0578":"code","d4073a04":"code","1e0c5865":"code","f1b68e7d":"code","fefabb53":"code","9943917b":"code","6fe1a72c":"code","cab01ef9":"code","c6d5447e":"code","e14d6e17":"code","397251c0":"code","4e37a550":"code","f9df66f1":"code","923b4792":"code","056ddf0c":"code","c1f640e4":"code","842a5ab0":"code","d7d26fc9":"code","5fa3245a":"code","2b2d8cca":"code","48373ca2":"code","5270981e":"code","7fb012e0":"code","5eb370ae":"code","555a809e":"code","862f51c4":"code","304c867d":"code","c101f392":"code","ed327400":"code","cdcaa19b":"code","db586805":"code","86f28428":"code","106b500a":"code","3d7c9b8d":"code","dd7cd52c":"code","12f58d73":"code","d18ad352":"code","6ee6b3ae":"code","8f292308":"code","b7fa81fd":"code","5d3ac512":"code","fc699b51":"code","893f6d1b":"code","0812863b":"code","e1115c78":"code","e859683c":"code","f732aee0":"code","5812c3b4":"code","53f8b738":"code","3f67fc8b":"code","e6180808":"code","270e048b":"code","db5bb04f":"code","34fe14d5":"code","71920dd8":"code","5ad73e6d":"code","80f5bb82":"code","3a3f02db":"code","37478e21":"code","108cf5f8":"code","0bb902cf":"code","40b17f88":"code","051733f8":"code","51215c40":"code","26510732":"code","fd92ad46":"code","35656db6":"code","fc0a17d7":"code","7c24c399":"code","f146e632":"code","a19ac8b5":"code","8e5c1688":"code","34664c7f":"code","4778fc92":"code","2f11057f":"code","5c2a7bb1":"code","3e69faa5":"code","e4cae0c9":"code","bf3c0dd2":"code","ffcf8cda":"code","c0f69ca7":"code","5e15bcc8":"code","7d8c8586":"code","f311bbbf":"code","23ebdadc":"code","72ed7861":"code","a2900c1c":"code","b594f653":"code","d21eab36":"code","5b9e87c2":"code","c053b89b":"code","4ffbff10":"code","7220aabf":"code","57c1b450":"code","42d3a9bc":"code","e3fd1e0a":"code","c35bc519":"code","0f9ad656":"code","6276d5c1":"code","5c3b464c":"code","e09a2f66":"code","3b12503b":"code","25459c8e":"code","b8b75625":"code","36a94094":"code","2194a7e8":"code","10893cc3":"code","6c029b56":"code","49cbefdc":"code","6018ed05":"code","eddcc00a":"code","8b1c3bb9":"code","da973c72":"code","4b3eed53":"code","89a6e1b4":"code","168eb361":"code","5157c087":"code","4725997e":"code","04b517c4":"code","55191bbf":"code","204d22da":"code","745d04ef":"code","0bcf6244":"code","d45041d9":"code","4b6170d6":"code","61f34625":"code","d8b0ae57":"code","84f866d1":"code","acfd5d93":"code","eae8556d":"code","7368e501":"code","7a09f46c":"code","1bd0c120":"code","fad9e35b":"code","a35bfe15":"code","5165dc5d":"code","691c8eef":"code","d3deb994":"code","bf291be4":"code","4e839565":"code","761dcc32":"code","f57378bf":"code","7437c4a6":"code","65acf105":"code","b921da9b":"code","e4dee8b4":"code","95953a7e":"code","71ee700d":"code","e875d43e":"code","d01853a3":"code","d7615aaa":"code","1faccb56":"code","01b52dab":"code","547cd4d0":"code","32bce538":"code","c1e01d2b":"code","8f213395":"code","362b7646":"code","e03d75a3":"code","2e008d22":"code","ef6a9236":"code","a1daeed8":"code","267c660a":"code","cb2de364":"code","109c37ec":"code","6160aa0e":"code","1e332661":"code","88ae0b7e":"code","a54e2368":"code","b8acd3ce":"code","b088dc59":"code","594de0b9":"code","f2b37154":"code","c7a99a0f":"code","3a076b97":"code","cb7b16b3":"code","7ba5e8f8":"code","0abfcbb9":"code","9a892fcf":"code","c677a566":"code","83d698b7":"code","97967eab":"code","95ed06df":"code","3e46e56f":"code","116fd642":"code","0e6c9c53":"code","f2854267":"code","b4f22cd1":"code","729417bf":"code","656217fb":"code","f49734e4":"code","cdc73c82":"code","65343b49":"code","df8b82ec":"code","a8ea9f1f":"code","7c6d330c":"code","bc37e257":"code","6d0144cc":"code","683b5a7c":"code","ee379828":"code","df523f79":"code","42344643":"code","ebd3d5fb":"code","2d59e606":"code","f635753d":"code","044fd314":"code","b29e06d7":"code","dbec2801":"code","4e8b0fef":"code","fed5880b":"code","c10cc577":"code","906bd844":"code","b2f6113f":"code","97e287d2":"code","2903ff1e":"code","d8df1861":"code","cb884561":"code","bc03cb4f":"code","b6e730ad":"code","2dbd451b":"code","c3f9e73d":"code","0500f810":"code","a44e7c1d":"code","c5f11d37":"code","6c931988":"code","afcfdaa0":"code","32bb3b5a":"code","5d3872d7":"markdown","8304cc6a":"markdown","f8b03400":"markdown","66604ab9":"markdown","beb86eec":"markdown","4e1f61c8":"markdown","a4dbd09f":"markdown","8b444d17":"markdown","c734a279":"markdown","2e4d26fe":"markdown","8e248d23":"markdown","4fd97dc2":"markdown","0e0be91e":"markdown","6cdeac74":"markdown","b64eb770":"markdown","436dd967":"markdown","caeb10cb":"markdown","955bf986":"markdown","f9ee4044":"markdown","119c11ea":"markdown","df290c3d":"markdown","9da37fb6":"markdown","1ee63d4e":"markdown","94217d8e":"markdown","82b4f634":"markdown","c4f7913a":"markdown","d0fe9060":"markdown","ba2404df":"markdown","d628ef94":"markdown","26401c0d":"markdown","8cc71841":"markdown","6a39eb3a":"markdown","66730912":"markdown","01e8aea8":"markdown","1cd724bf":"markdown","effc071c":"markdown","42d564cc":"markdown","ffe90aba":"markdown","eafdc396":"markdown","9cd87d43":"markdown","d76e83cf":"markdown","7ddb29ed":"markdown","ed145918":"markdown","3f25a3c9":"markdown","06fcebda":"markdown","2b3aee80":"markdown","9c944839":"markdown","7ed139db":"markdown","f422069b":"markdown","7be30deb":"markdown","0a4e3628":"markdown","9bf75f87":"markdown","cb209ee0":"markdown","549f6e82":"markdown","93f3188d":"markdown","fab6a8bb":"markdown","dc1b77fd":"markdown","035f92f0":"markdown","064cf6c9":"markdown","aa5d57f9":"markdown","d3075da3":"markdown","6ee63fcb":"markdown","1c63ccae":"markdown","f628e74b":"markdown","da14ba36":"markdown","c028b24d":"markdown","37a7a67c":"markdown","fe6eaa94":"markdown","ee34a601":"markdown","f5863ec4":"markdown","465ea61f":"markdown","da712020":"markdown","c7fef65e":"markdown","1659af88":"markdown","1dcee845":"markdown","84823231":"markdown","dee5a2dc":"markdown","72723320":"markdown","2c9b47f7":"markdown","8e0bd29d":"markdown","a5e24e00":"markdown","9e59483e":"markdown","58780ef0":"markdown","4557306e":"markdown","07de44da":"markdown","44d5b5c8":"markdown","500079af":"markdown","b060a2b7":"markdown","43b5ce03":"markdown","c40694e4":"markdown","aa51e935":"markdown","425200f4":"markdown","6787c482":"markdown","e612eb4d":"markdown","3440399b":"markdown","faa76ea9":"markdown","358eb6f1":"markdown","ab70fbae":"markdown","f24bc6da":"markdown","cf394bf5":"markdown","c7bad67e":"markdown","04f007e9":"markdown","2ffc823a":"markdown","31378ea0":"markdown","7254e03e":"markdown","9537dbb2":"markdown","b2a7fd67":"markdown","0e7264ce":"markdown","e01f4299":"markdown","c17c75ff":"markdown","7dff3fe6":"markdown","a9618053":"markdown","12b6fafb":"markdown","3c8e1517":"markdown","c49e2750":"markdown","c79fb422":"markdown","b03ba459":"markdown","92300671":"markdown","40a89ed5":"markdown","bf800a8d":"markdown","ebe9bc70":"markdown","32959244":"markdown","6df1b7ff":"markdown","46a89dca":"markdown","6916a6e6":"markdown","39021c87":"markdown","d9b38445":"markdown"},"source":{"432dc899":"!pip3 install pyforest","ff31936c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport scipy.stats as stats\nimport pyforest\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, StandardScaler, PowerTransformer, MinMaxScaler, LabelEncoder, RobustScaler\nfrom sklearn.model_selection import RepeatedStratifiedKFold, KFold, cross_val_predict, train_test_split, GridSearchCV, cross_val_score, cross_validate\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge,ElasticNet\nfrom sklearn.metrics import plot_confusion_matrix, r2_score, mean_absolute_error, mean_squared_error, classification_report, confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import make_scorer, precision_score, precision_recall_curve, plot_precision_recall_curve, plot_roc_curve, roc_auc_score, roc_curve, f1_score, accuracy_score, recall_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostClassifier\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif, f_regression, mutual_info_regression\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom xgboost import plot_importance\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import plot_tree\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n#importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.warn(\"this will not show\")\nplt.rcParams[\"figure.figsize\"] = (10,6)\npd.set_option('max_colwidth',200)\n\n# pd.set_option('display.max_rows', 100) # if you wish to see more rows rather than default, just uncomment this line.\npd.set_option('display.max_columns', 200)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nimport colorama\nfrom colorama import Fore, Style  # maakes strings colored\n# !pip3 install termcolor\nfrom termcolor import colored","0e604261":"# Function for determining the number and percentages of missing values\n\ndef missing (df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values","04aad9c6":"# Function for insighting summary information about the column\n\ndef first_looking(col):\n    print(\"column name    : \", col)\n    print(\"--------------------------------\")\n    print(\"per_of_nulls   : \", \"%\", round(df[col].isnull().sum()\/df.shape[0]*100, 2))\n    print(\"num_of_nulls   : \", df[col].isnull().sum())\n    print(\"num_of_uniques : \", df[col].nunique())\n    print(df[col].value_counts(dropna = False))","f47bd2ad":"# Function for examining scores\n\ndef train_val(y_train, y_train_pred, y_test, y_pred):\n    \n    scores = {\"train_set\": {\"Accuracy\" : accuracy_score(y_train, y_train_pred),\n                            \"Precision\" : precision_score(y_train, y_train_pred),\n                            \"Recall\" : recall_score(y_train, y_train_pred),                          \n                            \"f1\" : f1_score(y_train, y_train_pred)},\n    \n              \"test_set\": {\"Accuracy\" : accuracy_score(y_test, y_pred),\n                           \"Precision\" : precision_score(y_test, y_pred),\n                           \"Recall\" : recall_score(y_test, y_pred),                          \n                           \"f1\" : f1_score(y_test, y_pred)}}\n    \n    return pd.DataFrame(scores)","86ac2e51":"df0 = pd.read_csv(\"..\/input\/heart-failure-prediction\/heart.csv\")","bd707129":"df = df0","b1e11f83":"df","51017c5e":"df.head()","6b034977":"df.tail()","3567b6e4":"df.sample(10)","4764e2f3":"df.columns","dc2ed0f7":"print(\"There is\", df.shape[0], \"observation and\", df.shape[1], \"columns in the dataset\")","10236356":"df.info()","fb70cab9":"df.describe().T","354ba6b6":"df.describe(include=object).T","8b0e0578":"df.nunique()","d4073a04":"# to find how many unique values object features have\n\nfor col in df.select_dtypes(include=[np.number]).columns:\n  print(f\"{col} has {df[col].nunique()} unique value\")","1e0c5865":"df.duplicated().value_counts()","f1b68e7d":"missing (df)","fefabb53":"first_looking(\"HeartDisease\")","9943917b":"print(df[\"HeartDisease\"].value_counts())\ndf[\"HeartDisease\"].value_counts().plot(kind=\"pie\", autopct='%1.1f%%', figsize=(10,10));","6fe1a72c":"y = df['HeartDisease']\nprint(f'Percentage of Heart Disease: % {round(y.value_counts(normalize=True)[1]*100,2)} --> \\\n({y.value_counts()[1]} cases for Heart Disease)\\nPercentage of NOT Heart Disease: % {round(y.value_counts(normalize=True)[0]*100,2)} --> ({y.value_counts()[0]} cases for NOT Heart Disease)')","cab01ef9":"df['HeartDisease'].describe()","c6d5447e":"df[df['HeartDisease']==0].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')","e14d6e17":"df[df['HeartDisease']==1].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')","397251c0":"print( f\"Skewness: {df['HeartDisease'].skew()}\")","4e37a550":"print( f\"Kurtosis: {df['HeartDisease'].kurtosis()}\")","f9df66f1":"df['HeartDisease'].iplot(kind='hist')","923b4792":"numerical= df.drop(['HeartDisease'], axis=1).select_dtypes('number').columns\n\ncategorical = df.select_dtypes('object').columns\n\nprint(f'Numerical Columns:  {df[numerical].columns}')\nprint('\\n')\nprint(f'Categorical Columns: {df[categorical].columns}')","056ddf0c":"df[numerical].head().T","c1f640e4":"df[numerical].describe().T","842a5ab0":"df[numerical].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')","d7d26fc9":"df[numerical].iplot(kind='hist');","5fa3245a":"df[numerical].iplot(kind='histogram', subplots=True,bins=50)","2b2d8cca":"for i in numerical:\n    df[i].iplot(kind=\"box\", title=i, boxpoints=\"all\", color='lightseagreen')","48373ca2":"index = 0\nplt.figure(figsize=(20,20))\nfor feature in numerical:\n    if feature != \"HeartDisease\":\n        index += 1\n        plt.subplot(4, 3, index)\n        sns.boxplot(x='HeartDisease', y=feature, data=df)","5270981e":"fig = px.scatter_3d(df, \n                    x='RestingBP',\n                    y='Age',\n                    z='Sex',\n                    color='HeartDisease')\nfig.show();","7fb012e0":"sns.pairplot(df, hue=\"HeartDisease\", palette=\"inferno\", corner=True);","5eb370ae":"skew_vals = df.skew().sort_values(ascending=False)\nskew_vals","555a809e":"skew_limit = 0.5 # This is our threshold-limit to evaluate skewness. Overall below abs(1) seems acceptable for the linear models. \nskew_vals = df.skew()\nskew_cols = skew_vals[abs(skew_vals)> skew_limit].sort_values(ascending=False)\nskew_cols ","862f51c4":"#Interpreting Skewness \n\nfor skew in skew_vals:\n    if -0.5 < skew < 0.5:\n        print (\"A skewness value of\", '\\033[1m', Fore.GREEN, skew, '\\033[0m', \"means that the distribution is approx.\", '\\033[1m', Fore.GREEN, \"symmetric\", '\\033[0m')\n    elif  -0.5 < skew < -1.0 or 0.5 < skew < 1.0:\n        print (\"A skewness value of\", '\\033[1m', Fore.YELLOW, skew, '\\033[0m', \"means that the distribution is approx.\", '\\033[1m', Fore.YELLOW, \"moderately skewed\", '\\033[0m')\n    else:\n        print (\"A skewness value of\", '\\033[1m', Fore.RED, skew, '\\033[0m', \"means that the distribution is approx.\", '\\033[1m', Fore.RED, \"highly skewed\", '\\033[0m')","304c867d":"kurtosis_vals = df.kurtosis().sort_values(ascending=False)\nkurtosis_vals","c101f392":"#Calculating Kurtosis \n\nkurtosis_limit = 7 # This is our threshold-limit to evaluate skewness. Overall below abs(1) seems acceptable for the linear models.\nkurtosis_vals = df.kurtosis()\nkurtosis_cols = kurtosis_vals[abs(kurtosis_vals) > kurtosis_limit].sort_values(ascending=False)\nkurtosis_cols","ed327400":"plt.figure(figsize=(12, 8))\nsns.heatmap(df.corr(), annot=True)\nplt.xticks(rotation=45);","cdcaa19b":"df_temp = df.corr()\n\ncount = \"Done\"\nfeature =[]\ncollinear=[]\nfor col in df_temp.columns:\n    for i in df_temp.index:\n        if (df_temp[col][i]> .9 and df_temp[col][i] < 1) or (df_temp[col][i]< -.9 and df_temp[col][i] > -1) :\n                feature.append(col)\n                collinear.append(i)\n                print(Fore.RED + f\"\\033[1mmulticolinearity alert in between\\033[0m {col} - {i}\")\n        else:\n            print(f\"For {col} and {i}, there is NO multicollinearity problem\") \n\nprint(\"\\033[1mThe number of strong corelated features:\\033[0m\", count) ","db586805":"df[categorical].head().T","86f28428":"df[categorical].describe()","106b500a":"for i in categorical:\n    df[i].iplot(kind=\"box\", title=i, boxpoints=\"all\", color='lightseagreen')","3d7c9b8d":"df[categorical].iplot(kind='hist');","dd7cd52c":"df[categorical].iplot(kind='histogram',subplots=True,bins=50)","12f58d73":"df[\"Sex\"].value_counts()","d18ad352":"df['Sex'].iplot(kind='hist', )","6ee6b3ae":"sns.swarmplot(y=\"Age\", x=\"Sex\", hue=\"HeartDisease\", data=df, palette=\"husl\");","8f292308":"df[\"ChestPainType\"].value_counts()","b7fa81fd":"df['ChestPainType'].iplot(kind='hist', )","5d3ac512":"sns.swarmplot(y=\"Age\", x=\"ChestPainType\", hue=\"HeartDisease\", data=df, palette=\"husl\");","fc699b51":"df[\"RestingECG\"].value_counts()","893f6d1b":"df['RestingECG'].iplot(kind='hist')","0812863b":"sns.swarmplot(y=\"Age\", x=\"RestingECG\", hue=\"HeartDisease\", data=df, palette=\"husl\");","e1115c78":"df[\"ExerciseAngina\"].value_counts()","e859683c":"df['ExerciseAngina'].iplot(kind='hist')","f732aee0":"sns.swarmplot(y=\"Age\", x=\"ExerciseAngina\", hue=\"HeartDisease\", data=df, palette=\"husl\");","5812c3b4":"df[\"ST_Slope\"].value_counts()","53f8b738":"df['ST_Slope'].iplot(kind='hist')","3f67fc8b":"sns.swarmplot(y=\"Age\", x=\"ST_Slope\", hue=\"HeartDisease\", data=df, palette=\"husl\");","e6180808":"df.shape","270e048b":"df.head()","db5bb04f":"df[categorical].value_counts()","34fe14d5":"df = pd.get_dummies(df, drop_first=True)","71920dd8":"df.shape","5ad73e6d":"df.head()","80f5bb82":"X = df.drop([\"HeartDisease\"], axis=1)\ny = df[\"HeartDisease\"]","3a3f02db":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify = y, random_state = 101)","37478e21":"missing(df)","108cf5f8":"scaler = MinMaxScaler()\nscaler","0bb902cf":"X_train_scaled = scaler.fit_transform(X_train)","40b17f88":"X_test_scaled = scaler.transform(X_test)","051733f8":"# General Insights\n\ndef model_first_insight(X_train, y_train, class_weight, solver='liblinear'):\n    # Logistic Regression\n    log = LogisticRegression(random_state=101, class_weight=class_weight)\n    log.fit(X_train, y_train)\n    \n    # Decision Tree\n    decision_tree = DecisionTreeClassifier(criterion = 'entropy', random_state=101, class_weight=class_weight)\n    decision_tree.fit(X_train, y_train)\n   \n    # Random Forest\n    random_forest = RandomForestClassifier(n_estimators=10, criterion = 'entropy', random_state=101, class_weight=class_weight)\n    random_forest.fit(X_train, y_train)\n    \n    # KNN\n    knn = KNeighborsClassifier(n_neighbors = 3)\n    knn.fit(X_train, y_train) \n   \n    # SVC\n    svc = SVC(random_state=101, class_weight=class_weight)\n    svc.fit(X_train, y_train) \n    \n    # XGB\n    xgb = XGBClassifier(random_state=101, class_weight=class_weight)\n    xgb.fit(X_train, y_train)\n    \n    # AdaBoosting\n    ab = AdaBoostClassifier(n_estimators=50, random_state=101)\n    ab.fit(X_train, y_train)\n    \n    # GB GradientBoosting\n    gb = GradientBoostingClassifier(random_state=101)\n    gb.fit(X_train, y_train)\n    \n    # Model Accuracy on Training Data\n    print(f\"\\033[1m1) Logistic Regression Training Accuracy:\\033[0m {log.score(X_train, y_train)}\")\n    print(f\"\\033[1m2) SVC Training Accuracy:\\033[0m {svc.score(X_train, y_train)}\")    \n    print(f\"\\033[1m3) Decision Tree Training Accuracy:\\033[0m {decision_tree.score(X_train, y_train)}\")\n    print(f\"\\033[1m4) Random Forest Training Accuracy:\\033[0m {random_forest.score(X_train, y_train)}\")\n    print(f\"\\033[1m5) KNN Training Accuracy:\\033[0m {knn.score(X_train, y_train)}\")\n    print(f\"\\033[1m6) GradiendBoosting Training Accuracy:\\033[0m {gb.score(X_train, y_train)}\")\n    print(f\"\\033[1m7) AdaBoosting Training Accuracy:\\033[0m {ab.score(X_train, y_train)}\")\n    print(f\"\\033[1m8) XGBoosting Training Accuracy:\\033[0m {xgb.score(X_train, y_train)}\")\n    \n    return log, svc, decision_tree, random_forest, knn, gb, ab, xgb","51215c40":"def models(X_train, y_train, class_weight):\n    \n    # Logistic Regression\n    log = LogisticRegression(random_state=101, class_weight=class_weight, solver='liblinear')\n    log.fit(X_train, y_train)\n    \n    # Decision Tree\n    decision_tree = DecisionTreeClassifier(criterion = 'entropy', random_state=101, class_weight=class_weight)\n    decision_tree.fit(X_train, y_train)\n    \n    # Random Forest\n    random_forest = RandomForestClassifier(n_estimators=10, criterion = 'entropy', random_state=101, class_weight=class_weight)\n    random_forest.fit(X_train, y_train)\n    # KNN\n    knn = KNeighborsClassifier(n_neighbors = 3)\n    knn.fit(X_train, y_train) \n   \n    # SVC\n    svc = SVC(random_state=101, class_weight=class_weight)\n    svc.fit(X_train, y_train) \n    \n    # XGB\n    xgb = XGBClassifier(random_state=101, class_weight=class_weight)\n    xgb.fit(X_train, y_train)\n    \n    # AdaBoosting\n    ab = AdaBoostClassifier(n_estimators=50, random_state=101)\n    ab.fit(X_train, y_train)\n    \n    # GB GradientBoosting\n    gb = GradientBoostingClassifier(random_state=101)\n    gb.fit(X_train, y_train)\n    \n    # Model Accuracy on Training Data\n    print(f\"\\033[1m1) Logistic Regression Training Accuracy:\\033[0m {log}\")\n    print(f\"\\033[1m2) SVC Training Accuracy:\\033[0m {svc}\")    \n    print(f\"\\033[1m3) Decision Tree Training Accuracy:\\033[0m {decision_tree}\")\n    print(f\"\\033[1m4) Random Forest Training Accuracy:\\033[0m {random_forest}\")\n    print(f\"\\033[1m5) KNN Training Accuracy:\\033[0m {knn}\")\n    print(f\"\\033[1m6) GradiendBoosting Training Accuracy:\\033[0m {gb}\")\n    print(f\"\\033[1m7) AdaBoosting Training Accuracy:\\033[0m {ab}\")\n    print(f\"\\033[1m8) XGBoosting Training Accuracy:\\033[0m {xgb}\")\n  \n    return log.score(X_train, y_train), svc.score(X_train, y_train),decision_tree.score(X_train, y_train),random_forest.score(X_train, y_train),knn.score(X_train, y_train),gb.score(X_train, y_train),ab.score(X_train, y_train),xgb.score(X_train, y_train)","26510732":"def models_accuracy(X_Set, y_Set):    \n    Scores = pd.DataFrame(columns = [\"LR_Acc\", \"SVC_Acc\", \"DT_Acc\", \"RF_Acc\", \"KNN_Acc\", \"GB_Acc\", \"AB_Acc\", \"XGB_Acc\"])\n\n    print(\"\\033[1mBASIC ACCURACY\\033[0m\")\n    Basic = [log_acc, svc_acc, decision_tree_acc, random_forest_acc, knn_acc, gb_acc, ab_acc, xgb_acc] = models(X_train, y_train, None)\n    Scores.loc[0] = Basic\n\n    print(\"\\n\\033[1mSCALED ACCURACY WITHOUT BALANCED\\033[0m\")    \n    Scaled = [log_acc, svc_acc, decision_tree_acc, random_forest_acc, knn_acc, gb_acc, ab_acc, xgb_acc] = models(X_train_scaled, y_train, None)\n    Scores.loc[1] = Scaled\n\n    \n    print(\"\\n\\033[1mBASIC ACCURACY WITH BALANCED\\033[0m\")\n    Balanced = [log_acc, svc_acc, decision_tree_acc, random_forest_acc, knn_acc, gb_acc, ab_acc, xgb_acc] = models(X_train, y_train, \"balanced\")\n    Scores.loc[2] = Balanced\n\n    print(\"\\n\\033[1mSCALED ACCURACY WITH BALANCED\\033[0m\")    \n    Scaled_Balanced = [log_acc, svc_acc, decision_tree_acc, random_forest_acc, knn_acc, gb_acc, ab_acc, xgb_acc] = models(X_train_scaled, y_train, \"balanced\")\n    Scores.loc[3] = Scaled_Balanced\n\n    Scores.set_axis(['Basic', 'Scaled', 'Balanced', 'Scaled_Balanced'], axis='index', inplace=True)\n    #Scores.style.background_gradient(cmap='RdPu')\n\n    return Scores.style.applymap(lambda x: \"background-color: pink\" if x<0.6 or x == 1 else \"background-color: lightgreen\")\\\n                       .applymap(lambda x: 'opacity: 40%;' if (x < 0.8) else None)\\\n                       .applymap(lambda x: 'color: red' if x == 1 or x <=0.8 else 'color: darkblue')\n\n# https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/style.html","fd92ad46":"models_accuracy(X_train, y_train)","35656db6":"Scores = pd.DataFrame(columns = [\"LR_Acc\", \"SVC_Acc\", \"DT_Acc\", \"RF_Acc\", \"KNN_Acc\", \"GB_Acc\", \"AB_Acc\", \"XGB_Acc\"])\n\nprint(\"\\033[1mBASIC ACCURACY\\033[0m\")\nBasic = [log_acc, svc_acc, decision_tree_acc, random_forest_acc, knn_acc, gb_acc, ab_acc, xgb_acc] = models(X_train, y_train, None)\nScores.loc[0] = Basic\n\nprint(\"\\n\\033[1mSCALED ACCURACY WITHOUT BALANCED\\033[0m\")    \nScaled = [log_acc, svc_acc, decision_tree_acc, random_forest_acc, knn_acc, gb_acc, ab_acc, xgb_acc] = models(X_train_scaled, y_train, None)\nScores.loc[1] = Scaled\n\nprint(\"\\n\\033[1mBASIC ACCURACY WITH BALANCED\\033[0m\")\nBalanced = [log_acc, svc_acc, decision_tree_acc, random_forest_acc, knn_acc, gb_acc, ab_acc, xgb_acc] = models(X_train, y_train, \"balanced\")\nScores.loc[2] = Balanced\n\nprint(\"\\n\\033[1mSCALED ACCURACY WITH BALANCED\\033[0m\")    \nScaled_Balanced = [log_acc, svc_acc, decision_tree_acc, random_forest_acc, knn_acc, gb_acc, ab_acc, xgb_acc] = models(X_train_scaled, y_train, \"balanced\")\nScores.loc[3] = Scaled_Balanced\n\nScores.set_axis(['Basic', 'Scaled', 'Balanced', 'Scaled_Balanced'], axis='index', inplace=True)","fc0a17d7":"accuracy_scores = Scores.style.applymap(lambda x: \"background-color: pink\" if x<0.6 or x == 1 else \"background-color: lightgreen\")\\\n                              .applymap(lambda x: 'opacity: 40%;' if (x < 0.8) else None)\\\n                              .applymap(lambda x: 'color: red' if x == 1 or x <=0.8 else 'color: darkblue')\n\naccuracy_scores","7c24c399":"accuracy_scores","f146e632":"operations = [(\"scaler\", MinMaxScaler()), (\"power\", PowerTransformer()), (\"log\", LogisticRegression(random_state=101))]","a19ac8b5":"# Defining the pipeline object for LogisticClassifier\n\npipe_log_model = Pipeline(steps=operations)","8e5c1688":"# Another step by step way for defining the pipeline object for LogisticClassifier\n\n# scaler = MinMaxScaler()\n# power = PowerTransformer(method='yeo-johnson')\n# pipe_model = LogisticRegression(random_state=101)\n# pipe_log_model = Pipeline(steps=[('s', scaler),('p', power), ('m', pipe_model)])","34664c7f":"pipe_log_model.get_params()","4778fc92":"pipe_log_model.fit(X_train, y_train)\ny_pred = pipe_log_model.predict(X_test)\ny_train_pred = pipe_log_model.predict(X_train)","2f11057f":"print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","5c2a7bb1":"pipe_scores = cross_validate(pipe_log_model, X_train, y_train, scoring = ['accuracy', 'precision','recall','f1'], cv = 10)\ndf_pipe_scores = pd.DataFrame(pipe_scores, index = range(1, 11))\n\ndf_pipe_scores","3e69faa5":"df_pipe_scores.mean()[2:]","e4cae0c9":"# evaluate the pipeline\n\n# from sklearn.model_selection import RepeatedStratifiedKFold\n\ncv = RepeatedStratifiedKFold(n_splits=10, random_state=101)\nn_scores = cross_val_score(pipe_log_model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n\nprint(f'Accuracy: Results Mean : %{round(n_scores.mean()*100,3)}, Results Standard Deviation : {round(n_scores.std()*100,3)}')","bf3c0dd2":"print('Accuracy: %.3f (%.3f)' % (n_scores.mean(), n_scores.std()))","ffcf8cda":"accuracy_scores","c0f69ca7":"LR_model = LogisticRegression() # Since Basic accuracy outcome gives the best model accuracy results, we will implement it \nLR_model.fit(X_train_scaled, y_train)\ny_pred = LR_model.predict(X_test_scaled)\ny_train_pred = LR_model.predict(X_train_scaled)\n\nlog_f1 = f1_score(y_test, y_pred)\nlog_acc = accuracy_score(y_test, y_pred)\nlog_recall = recall_score(y_test, y_pred)\nlog_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(LR_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","5e15bcc8":"y_pred_proba = LR_model.predict_proba(X_test_scaled)\ny_pred_proba","7d8c8586":"test_data = pd.concat([X_test.set_index(y_test.index), y_test], axis=1)\ntest_data[\"pred\"] = y_pred\ntest_data[\"pred_proba\"] = y_pred_proba[:, 1]\ntest_data.sample(10)","f311bbbf":"log_xvalid_model = LogisticRegression()\n\nlog_xvalid_model_scores = cross_validate(log_xvalid_model, X_train_scaled, y_train, scoring = ['accuracy', 'precision','recall',\n                                                                          'f1'], cv = 10)\nlog_xvalid_model_scores = pd.DataFrame(log_xvalid_model_scores, index = range(1, 11))\n\nlog_xvalid_model_scores","23ebdadc":"log_xvalid_model_scores.mean()[2:]","72ed7861":"penalty = [\"l1\", \"l2\", \"elasticnet\"]\nl1_ratio = np.linspace(0, 1, 20)\nC = np.logspace(0, 10, 20)\n\nparam_grid = {\"penalty\" : penalty,\n             \"l1_ratio\" : l1_ratio,\n             \"C\" : C}","a2900c1c":"LR_grid_model = LogisticRegression(solver='saga', max_iter=5000, class_weight = \"balanced\")\n\nLR_grid_model = GridSearchCV(LR_grid_model, param_grid = param_grid)","b594f653":"LR_grid_model.fit(X_train_scaled, y_train)","d21eab36":"print(colored('\\033[1mBest Parameters of GridSearchCV for LR Model:\\033[0m', 'blue'), colored(LR_grid_model.best_params_, 'cyan'))\nprint(\"--------------------------------------------------------------------------------------------------------------------\")\nprint(colored('\\033[1mBest Estimator of GridSearchCV for LR Model:\\033[0m', 'blue'), colored(LR_grid_model.best_estimator_, 'cyan'))","5b9e87c2":"y_pred = LR_grid_model.predict(X_test_scaled)\ny_train_pred = LR_grid_model.predict(X_train_scaled)\n\nlog_grid_f1 = f1_score(y_test, y_pred)\nlog_grid_acc = accuracy_score(y_test, y_pred)\nlog_grid_recall = recall_score(y_test, y_pred)\nlog_grid_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(LR_grid_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","c053b89b":"plot_roc_curve(LR_model, X_test_scaled, y_test, response_method='auto');","4ffbff10":"plot_precision_recall_curve(LR_model, X_test_scaled, y_test);","7220aabf":"fp_rate, tp_rate, thresholds = roc_curve(y_test, y_pred_proba[:, 1])","57c1b450":"optimal_idx = np.argmax(tp_rate - fp_rate)\noptimal_threshold = thresholds[optimal_idx]\noptimal_threshold","42d3a9bc":"roc_curve = {\"fp_rate\":fp_rate, \"tp_rate\":tp_rate, \"thresholds\":thresholds}\ndf_roc_curve = pd.DataFrame(roc_curve)\ndf_roc_curve","e3fd1e0a":"df_roc_curve.iloc[optimal_idx]","c35bc519":"accuracy_scores","0f9ad656":"SVM_model = SVC(random_state=42)\nSVM_model.fit(X_train_scaled, y_train)\ny_pred = SVM_model.predict(X_test_scaled)\ny_train_pred = SVM_model.predict(X_train_scaled)\n\nsvm_f1 = f1_score(y_test, y_pred)\nsvm_acc = accuracy_score(y_test, y_pred)\nsvm_recall = recall_score(y_test, y_pred)\nsvm_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(SVM_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","6276d5c1":"y_train_pred = SVM_model.predict(X_train_scaled)\n\nprint(confusion_matrix(y_train, y_train_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_train, y_train_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(SVM_model, X_train_scaled, y_train);","5c3b464c":"from yellowbrick.classifier import ClassPredictionError\n\nvisualizer = ClassPredictionError(SVM_model)\n\n# Fit the training data to the visualizer\nvisualizer.fit(X_train_scaled, y_train)\n\n# Evaluate the model on the test data\nvisualizer.score(X_test_scaled, y_test)\n\n# Draw visualization\nvisualizer.poof();","e09a2f66":"svm_xvalid_model = SVC()\n\nsvm_xvalid_model_scores = cross_validate(svm_xvalid_model, X_train_scaled, y_train, scoring = ['accuracy', 'precision','recall',\n                                                                   'f1'], cv = 10)\nsvm_xvalid_model_scores = pd.DataFrame(svm_xvalid_model_scores, index = range(1, 11))\n\nsvm_xvalid_model_scores","3b12503b":"svm_xvalid_model_scores.mean()[2:]","25459c8e":"param_grid = {'C': [0.1,1, 10, 100, 1000],\n              'gamma': [\"scale\", \"auto\", 1,0.1,0.01,0.001,0.0001],\n              'kernel': ['rbf', 'linear']}","b8b75625":"SVM_grid_model = SVC(random_state=42)\n\nSVM_grid_model = GridSearchCV(SVM_grid_model, param_grid, verbose=3, refit=True)","36a94094":"SVM_grid_model.fit(X_train_scaled, y_train)","2194a7e8":"print(colored('\\033[1mBest Parameters of GridSearchCV for SVM Model:\\033[0m', 'blue'), colored(SVM_grid_model.best_params_, 'cyan'))\nprint(\"--------------------------------------------------------------------------------------------------------------------\")\nprint(colored('\\033[1mBest Estimator of GridSearchCV for SVM Model:\\033[0m', 'blue'), colored(SVM_grid_model.best_estimator_, 'cyan'))","10893cc3":"y_pred = SVM_grid_model.predict(X_test_scaled)\ny_train_pred = SVM_grid_model.predict(X_train_scaled)\n\nsvm_grid_f1 = f1_score(y_test, y_pred)\nsvm_grid_acc = accuracy_score(y_test, y_pred)\nsvm_grid_recall = recall_score(y_test, y_pred)\nsvm_grid_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(SVM_grid_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","6c029b56":"plot_roc_curve(SVM_grid_model, X_test_scaled, y_test);","49cbefdc":"plot_precision_recall_curve(SVM_grid_model, X_test_scaled, y_test);","6018ed05":"accuracy_scores","eddcc00a":"DT_model = DecisionTreeClassifier(class_weight=\"balanced\", random_state=42)\nDT_model.fit(X_train_scaled, y_train)\ny_pred = DT_model.predict(X_test_scaled)\ny_train_pred = DT_model.predict(X_train_scaled)\n\ndt_f1 = f1_score(y_test, y_pred)\ndt_acc = accuracy_score(y_test, y_pred)\ndt_recall = recall_score(y_test, y_pred)\ndt_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(DT_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","8b1c3bb9":"from yellowbrick.classifier import ClassPredictionError\n\nvisualizer = ClassPredictionError(DT_model)\n\n# Fit the training data to the visualizer\nvisualizer.fit(X_train_scaled, y_train)\n\n# Evaluate the model on the test data\nvisualizer.score(X_test_scaled, y_test)\n\n# Draw visualization\nvisualizer.poof();","da973c72":"dt_xvalid_model = DecisionTreeClassifier(max_depth=None, random_state=42)\n\ndt_xvalid_model_scores = cross_validate(dt_xvalid_model, X_train_scaled, y_train, scoring = [\"accuracy\", \"precision\", \"recall\", \"f1\"], cv = 10)\ndt_xvalid_model_scores = pd.DataFrame(dt_xvalid_model_scores, index = range(1, 11))\n\ndt_xvalid_model_scores","4b3eed53":"dt_xvalid_model_scores.mean()[2:]","89a6e1b4":"param_grid = {\"splitter\":[\"best\", \"random\"],\n              \"max_features\":[None, 3, 5, 7],\n              \"max_depth\": [None, 4, 5, 6, 7, 8, 9, 10],\n              \"min_samples_leaf\": [2, 3, 5],\n              \"min_samples_split\": [2, 3, 5, 7, 9, 15]}","168eb361":"DT_grid_model = DecisionTreeClassifier(class_weight = \"balanced\", random_state=42)\n\nDT_grid_model = GridSearchCV(estimator=DT_grid_model,\n                            param_grid=param_grid,\n                            scoring='recall',\n                            n_jobs = -1, verbose = 2).fit(X_train_scaled, y_train)","5157c087":"print(colored('\\033[1mBest Parameters of GridSearchCV for Decision Tree Model:\\033[0m', 'blue'), colored(DT_grid_model.best_params_, 'cyan'))\nprint(\"--------------------------------------------------------------------------------------------------------------------\")\nprint(colored('\\033[1mBest Estimator of GridSearchCV for Decision Tree Model:\\033[0m', 'blue'), colored(DT_grid_model.best_estimator_, 'cyan'))","4725997e":"DT_grid_model.fit(X_train_scaled, y_train)\ny_pred = DT_grid_model.predict(X_test_scaled)\n\ny_train_pred = DT_grid_model.predict(X_train_scaled)\n\ndt_grid_f1 = f1_score(y_test, y_pred)\ndt_grid_acc = accuracy_score(y_test, y_pred)\ndt_grid_recall = recall_score(y_test, y_pred)\ndt_grid_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(DT_grid_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","04b517c4":"DT_model.feature_importances_","55191bbf":"DT_feature_imp = pd.DataFrame(index=X.columns, data = DT_model.feature_importances_, \n                      columns = [\"Feature Importance\"]).sort_values(\"Feature Importance\")\nDT_feature_imp","204d22da":"sns.barplot(x=DT_feature_imp[\"Feature Importance\"], y=DT_feature_imp.index)\nplt.title(\"Feature Importance\")\nplt.show()","745d04ef":"X1 = X.drop(columns = [\"ST_Slope_Up\"])\ny1 = df[\"HeartDisease\"]","0bcf6244":"X1.columns","d45041d9":"X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.15, random_state=42)","4b6170d6":"operations = [(\"scaler\", MinMaxScaler()), (\"dt\", DecisionTreeClassifier(class_weight=\"balanced\", random_state=42))]\n\nDT_pipe_model = Pipeline(steps=operations)\nDT_pipe_model.get_params()\nDT_pipe_model.fit(X1_train, y1_train)\n\ny1_pred = DT_pipe_model.predict(X1_test)\ny1_train_pred = DT_pipe_model.predict(X1_train)\n\nprint(confusion_matrix(y1_test, y1_pred))\nprint(classification_report(y1_test, y1_pred))\n\nrf_pipe_f1 = f1_score(y1_test, y1_pred)\nrf_pipe_acc = accuracy_score(y1_test, y1_pred)\nrf_pipe_recall = recall_score(y1_test, y1_pred)\nrf_pipe_auc = roc_auc_score(y1_test, y_pred)\n\nprint(confusion_matrix(y1_test, y1_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y1_test, y1_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(DT_pipe_model, X1_test, y1_test)\n\ntrain_val(y1_train, y1_train_pred, y1_test, y1_pred)","61f34625":"plot_roc_curve(DT_grid_model, X_test_scaled, y_test);","d8b0ae57":"plot_precision_recall_curve(DT_grid_model, X_test_scaled, y_test);","84f866d1":"accuracy_scores","acfd5d93":"RF_model = RandomForestClassifier(class_weight=\"balanced\", random_state=101)\nRF_model.fit(X_train_scaled, y_train)\ny_pred = RF_model.predict(X_test_scaled)\ny_train_pred = RF_model.predict(X_train_scaled)\n\nrf_f1 = f1_score(y_test, y_pred)\nrf_acc = accuracy_score(y_test, y_pred)\nrf_recall = recall_score(y_test, y_pred)\nrf_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(RF_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","eae8556d":"from yellowbrick.classifier import ClassPredictionError\n\nvisualizer = ClassPredictionError(RF_model)\n\n# Fit the training data to the visualizer\nvisualizer.fit(X_train_scaled, y_train)\n\n# Evaluate the model on the test data\nvisualizer.score(X_test_scaled, y_test)\n\n# Draw visualization\nvisualizer.poof();","7368e501":"rf_xvalid_model = RandomForestClassifier(max_depth=None, random_state=101)\n\nrf_xvalid_model_scores = cross_validate(rf_xvalid_model, X_train_scaled, y_train, scoring = [\"accuracy\", \"precision\", \"recall\", \"f1\"], cv = 10)\nrf_xvalid_model_scores = pd.DataFrame(rf_xvalid_model_scores, index = range(1, 11))\n\nrf_xvalid_model_scores","7a09f46c":"rf_xvalid_model_scores.mean()[2:]","1bd0c120":"param_grid = {'n_estimators':[50, 100, 300],\n             'max_features':[2, 3, 4],\n             'max_depth':[3, 5, 7, 9],\n             'min_samples_split':[2, 5, 8]}","fad9e35b":"RF_grid_model = RandomForestClassifier(random_state=101)\n\nRF_grid_model = GridSearchCV(estimator=RF_grid_model, \n                             param_grid=param_grid, \n                             scoring = \"recall\", \n                             n_jobs = -1, verbose = 2).fit(X_train_scaled, y_train)  # Whatch out, fit() can also be used here","a35bfe15":"print(colored('\\033[1mBest Parameters of GridSearchCV for Random Forest Model:\\033[0m', 'blue'), colored(RF_grid_model.best_params_, 'cyan'))\nprint(\"--------------------------------------------------------------------------------------------------------------------\")\nprint(colored('\\033[1mBest Estimator of GridSearchCV for Random Forest Model:\\033[0m', 'blue'), colored(RF_grid_model.best_estimator_, 'cyan'))","5165dc5d":"y_pred = RF_grid_model.predict(X_test_scaled)\ny_train_pred = RF_grid_model.predict(X_train_scaled)\n\nrf_grid_f1 = f1_score(y_test, y_pred)\nrf_grid_acc = accuracy_score(y_test, y_pred)\nrf_grid_recall = recall_score(y_test, y_pred)\nrf_grid_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(RF_grid_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","691c8eef":"RF_model.feature_importances_","d3deb994":"RF_feature_imp = pd.DataFrame(index = X.columns, data = RF_model.feature_importances_,\n                              columns = [\"Feature Importance\"]).sort_values(\"Feature Importance\", ascending = False)\nRF_feature_imp","bf291be4":"sns.barplot(x=RF_feature_imp[\"Feature Importance\"], y=RF_feature_imp.index)\nplt.title(\"Feature Importance\")\nplt.show()","4e839565":"sns.barplot(x=DT_feature_imp[\"Feature Importance\"], y=DT_feature_imp.index)\nplt.title(\"Feature Importance\")\nplt.show()","761dcc32":"plot_roc_curve(RF_grid_model, X_test_scaled, y_test);","f57378bf":"plot_precision_recall_curve(RF_grid_model, X_test_scaled, y_test);","7437c4a6":"features = list(X.columns)\ntargets = str(df.HeartDisease.unique())\n\nfig, axes = plt.subplots(nrows = 1,ncols = 1, figsize = (20,10), dpi=150)\nplot_tree(RF_model.estimators_[0],\n          feature_names = features,\n          class_names=targets,\n          filled = True);","65acf105":"accuracy_scores","b921da9b":"KNN_model = KNeighborsClassifier(n_neighbors=5, algorithm=\"kd_tree\")\nKNN_model.fit(X_train_scaled, y_train)\ny_pred = KNN_model.predict(X_test_scaled)\ny_train_pred = KNN_model.predict(X_train_scaled)\n\nknn_f1 = f1_score(y_test, y_pred)\nknn_acc = accuracy_score(y_test, y_pred)\nknn_recall = recall_score(y_test, y_pred)\nknn_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(KNN_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","e4dee8b4":"y_pred_proba = KNN_model.predict_proba(X_test_scaled)","95953a7e":"pd.DataFrame(y_pred_proba)","71ee700d":"my_dict = {\"Actual\": y_test, \"Pred\": y_pred, \"Proba_1\": y_pred_proba[:,1], \"Proba_0\":y_pred_proba[:,0]}","e875d43e":"pd.DataFrame.from_dict(my_dict).sample(10)","d01853a3":"knn_xvalid_model = KNeighborsClassifier(n_neighbors=5)\n\nknn_xvalid_model_scores = cross_validate(knn_xvalid_model, X_train_scaled, y_train, scoring = [\"accuracy\", \"precision\", \"recall\", \"f1\"], cv = 10)\nknn_xvalid_model_scores = pd.DataFrame(knn_xvalid_model_scores, index = range(1, 11))\n\nknn_xvalid_model_scores","d7615aaa":"knn_xvalid_model_scores.mean()[2:]","1faccb56":"test_error_rates = []\n\n\nfor k in range(1, 30):\n    KNN_model = KNeighborsClassifier(n_neighbors=k)\n    KNN_model.fit(X_train_scaled, y_train) \n   \n    y_test_pred = KNN_model.predict(X_test_scaled)\n    \n    test_error = 1 - accuracy_score(y_test, y_test_pred)\n    test_error_rates.append(test_error)","01b52dab":"test_error_rates","547cd4d0":"plt.figure(figsize=(15, 8))\nplt.plot(range(1, 30), test_error_rates, color='blue', linestyle='--', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K_values')\nplt.ylabel('Error Rate')\nplt.hlines(y=0.14492753623188404, xmin=0, xmax=30, colors='r', linestyles=\"--\")\nplt.hlines(y=0.13043478260869568, xmin=0, xmax=30, colors='r', linestyles=\"--\");","32bce538":"k_values= range(1, 30)\nparam_grid = {\"n_neighbors\": k_values, \"p\": [1, 2], \"weights\": ['uniform', \"distance\"]}","c1e01d2b":"KNN_grid = KNeighborsClassifier()","8f213395":"KNN_grid_model = GridSearchCV(KNN_grid, param_grid, cv=10, scoring='accuracy')","362b7646":"KNN_grid_model.fit(X_train_scaled, y_train)","e03d75a3":"print(colored('\\033[1mBest Parameters of GridSearchCV for KNN Model:\\033[0m', 'blue'), colored(KNN_grid_model.best_params_, 'cyan'))\nprint(\"--------------------------------------------------------------------------------------------------------------------\")\nprint(colored('\\033[1mBest Estimator of GridSearchCV for KNN Model:\\033[0m', 'blue'), colored(KNN_grid_model.best_estimator_, 'cyan'))","2e008d22":"# NOW WITH K=26\n\nKNN_model = KNeighborsClassifier(n_neighbors=26, p=2)\nKNN_model.fit(X_train_scaled, y_train)\npred = KNN_model.predict(X_test_scaled)\ny_train_pred = KNN_model.predict(X_train_scaled)\n\nknn26_f1 = f1_score(y_test, y_pred)\nknn26_acc = accuracy_score(y_test, y_pred)\nknn26_recall = recall_score(y_test, y_pred)\nknn26_auc = roc_auc_score(y_test, y_pred)\n\nprint('WITH K=26')\nprint('-------------------')\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(KNN_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","ef6a9236":"# NOW WITH K=13\n\nKNN_model = KNeighborsClassifier(n_neighbors=13, p=2)\nKNN_model.fit(X_train_scaled, y_train)\npred = KNN_model.predict(X_test_scaled)\ny_train_pred = KNN_model.predict(X_train_scaled)\n\nknn13_f1 = f1_score(y_test, y_pred)\nknn13_acc = accuracy_score(y_test, y_pred)\nknn13_recall = recall_score(y_test, y_pred)\nknn13_auc = roc_auc_score(y_test, y_pred)\n\nprint('WITH K=13')\nprint('-------------------')\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(KNN_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","a1daeed8":"plot_roc_curve(KNN_model, X_test_scaled, y_test);","267c660a":"plot_precision_recall_curve(KNN_model, X_test_scaled, y_test);","cb2de364":"accuracy_scores","109c37ec":"GB_model = GradientBoostingClassifier(random_state=42)\nGB_model.fit(X_train_scaled, y_train)\ny_pred = GB_model.predict(X_test_scaled)\ny_train_pred = GB_model.predict(X_train_scaled)\n\ngb_f1 = f1_score(y_test, y_pred)\ngb_acc = accuracy_score(y_test, y_pred)\ngb_recall = recall_score(y_test, y_pred)\ngb_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(GB_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","6160aa0e":"y_train_pred = GB_model.predict(X_train_scaled)\n\nprint(confusion_matrix(y_train, y_train_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_train, y_train_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(GB_model, X_train_scaled, y_train);","1e332661":"from yellowbrick.classifier import ClassPredictionError\n\nvisualizer = ClassPredictionError(GB_model)\n\n# Fit the training data to the visualizer\nvisualizer.fit(X_train_scaled, y_train)\n\n# Evaluate the model on the test data\nvisualizer.score(X_test_scaled, y_test)\n\n# Draw visualization\nvisualizer.poof();","88ae0b7e":"gb_xvalid_model = GradientBoostingClassifier(random_state=42)\n\ngb_xvalid_model_scores = cross_validate(gb_xvalid_model, X_train_scaled, y_train, scoring = [\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\"], cv = 10)\ngb_xvalid_model_scores = pd.DataFrame(gb_xvalid_model_scores, index = range(1, 11))\n\ngb_xvalid_model_scores","a54e2368":"gb_xvalid_model_scores.mean()","b8acd3ce":"GB_model.feature_importances_","b088dc59":"GB_feature_imp = pd.DataFrame(index = X.columns, data = GB_model.feature_importances_,\n                              columns = [\"Feature Importance\"]).sort_values(\"Feature Importance\", ascending = False)\nGB_feature_imp","594de0b9":"sns.barplot(y=GB_feature_imp[\"Feature Importance\"], x=GB_feature_imp.index)\nplt.title(\"Feature Importance\")\nplt.xticks(rotation=45)\nplt.show()","f2b37154":"# Computing the accuracy scores on train and validation sets when training with different learning rates\n\nlearning_rates = [0.05, 0.1, 0.15, 0.25, 0.5, 0.6, 0.75, 0.85, 1]\n\nfor learning_rate in learning_rates:\n    gb = GradientBoostingClassifier(n_estimators=20, learning_rate = learning_rate, random_state=42)\n    gb.fit(X_train, y_train)\n    print(\"Learning rate: \", learning_rate)\n    print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train, y_train)))\n    print(\"Accuracy score (test): {0:.3f}\".format(gb.score(X_test, y_test)))\n    print()","c7a99a0f":"param_grid = {\"n_estimators\":[100, 200, 300],\n             \"subsample\":[0.5, 1], \"max_features\" : [None, 2, 3, 4], \"learning_rate\": [0.2, 0.5, 0.6, 0.75, 0.85, 1.0, 1.25, 1.5]}  # 'max_depth':[3,4,5,6]","3a076b97":"GB_grid_model = GradientBoostingClassifier(random_state=42)\n\nGB_grid_model = GridSearchCV(GB_grid_model, param_grid, scoring = \"f1\", verbose=2, n_jobs = -1).fit(X_train, y_train)","cb7b16b3":"print(colored('\\033[1mBest Parameters of GridSearchCV for Gradient Boosting Model:\\033[0m', 'blue'), colored(GB_grid_model.best_params_, 'cyan'))\nprint(\"--------------------------------------------------------------------------------------------------------------------\")\nprint(colored('\\033[1mBest Estimator of GridSearchCV for Gradient Boosting Model:\\033[0m', 'blue'), colored(GB_grid_model.best_estimator_, 'cyan'))","7ba5e8f8":"y_pred = GB_grid_model.predict(X_test_scaled)\ny_train_pred = GB_grid_model.predict(X_train_scaled)\n\ngb_grid_f1 = f1_score(y_test, y_pred)\ngb_grid_acc = accuracy_score(y_test, y_pred)\ngb_grid_recall = recall_score(y_test, y_pred)\ngb_grid_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(GB_grid_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","0abfcbb9":"plot_roc_curve(GB_model, X_test, y_test);","9a892fcf":"plot_precision_recall_curve(GB_model, X_test, y_test);","c677a566":"accuracy_scores","83d698b7":"AB_model = AdaBoostClassifier(n_estimators=50, random_state=101)\nAB_model.fit(X_train, y_train)\ny_pred = AB_model.predict(X_test)\ny_train_pred = AB_model.predict(X_train)\n\nab_f1 = f1_score(y_test, y_pred)\nab_acc = accuracy_score(y_test, y_pred)\nab_recall = recall_score(y_test, y_pred)\nab_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(AB_model, X_test, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","97967eab":"y_train_pred = AB_model.predict(X_train)\n\nprint(confusion_matrix(y_train, y_train_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_train, y_train_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(AB_model, X_train, y_train);","95ed06df":"from yellowbrick.classifier import ClassPredictionError\n\nvisualizer = ClassPredictionError(AB_model)\n\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n\n# Draw visualization\nvisualizer.poof();","3e46e56f":"ab_xvalid_model = AdaBoostClassifier(n_estimators=50, random_state=101)\n\nab_xvalid_model_scores = cross_validate(ab_xvalid_model, X_train, y_train, scoring = ['accuracy', 'precision','recall', 'f1'], cv = 10)\nab_xvalid_model_scores = pd.DataFrame(ab_xvalid_model_scores, index = range(1, 11))\n\nab_xvalid_model_scores","116fd642":"ab_xvalid_model_scores.mean()","0e6c9c53":"AB_model = AdaBoostClassifier(n_estimators=3, random_state=42)\nAB_model.fit(X_train, y_train)","f2854267":"df.columns","b4f22cd1":"features = list(X.columns)\ntargets = df[\"HeartDisease\"].astype(\"str\")\n\nplt.figure(figsize=(12, 8),dpi=150)\nplot_tree(AB_model.estimators_[0], filled=True, feature_names=features, class_names=targets.unique(), proportion=True);","729417bf":"error_rates = []\n\nfor n in range(1, 100):\n    \n    AB_model = AdaBoostClassifier(n_estimators=n)\n    AB_model.fit(X_train, y_train)\n    preds = AB_model.predict(X_test)\n    err = 1 - f1_score(y_test, preds)\n    \n    error_rates.append(err)","656217fb":"plt.figure(figsize=(14, 8))\nplt.plot(range(1, 100), error_rates);","f49734e4":"AB_model.feature_importances_","cdc73c82":"AB_feature_imp = pd.DataFrame(index = X.columns, data = AB_model.feature_importances_,\n                              columns = [\"Feature Importance\"]).sort_values(\"Feature Importance\", ascending = False)\nAB_feature_imp","65343b49":"imp_feats = AB_feature_imp.sort_values(\"Feature Importance\")","df8b82ec":"plt.figure(figsize=(12,6))\n\nsns.barplot(y=AB_feature_imp[\"Feature Importance\"], x=AB_feature_imp.index)\n\nplt.title(\"Feature Importance\")\nplt.xticks(rotation=45)\nplt.show()","a8ea9f1f":"# Computing the accuracy scores on train and validation sets when training with different learning rates\n\nlearning_rates = [0.05, 0.1, 0.15, 0.25, 0.5, 0.6, 0.75, 0.85, 1]\n\nfor learning_rate in learning_rates:\n    ab = AdaBoostClassifier(n_estimators=20, learning_rate = learning_rate, random_state=42)\n    ab.fit(X_train, y_train)\n    print(\"Learning rate: \", learning_rate)\n    print(\"Accuracy score (training): {0:.3f}\".format(ab.score(X_train, y_train)))\n    print(\"Accuracy score (test): {0:.3f}\".format(ab.score(X_test, y_test)))\n    print()","7c6d330c":"param_grid = {\"n_estimators\": [15, 20, 100, 500], \"learning_rate\": [0.2, 0.5, 0.6, 0.75, 0.85, 1.0, 1.25, 1.5]}","bc37e257":"AB_grid_model = AdaBoostClassifier(random_state=42)\nAB_grid_model = GridSearchCV(AB_grid_model, param_grid, cv=5, scoring= 'f1')","6d0144cc":"AB_grid_model.fit(X_train, y_train)","683b5a7c":"print(colored('\\033[1mBest Parameters of GridSearchCV for AdaBoosting Model:\\033[0m', 'blue'), colored(AB_grid_model.best_params_, 'cyan'))\nprint(\"--------------------------------------------------------------------------------------------------------------------\")\nprint(colored('\\033[1mBest Estimator of GridSearchCV for AdaBoosting Model:\\033[0m', 'blue'), colored(AB_grid_model.best_estimator_, 'cyan'))","ee379828":"y_pred = AB_grid_model.predict(X_test)\ny_train_pred = AB_grid_model.predict(X_train)\n\nab_grid_f1 = f1_score(y_test, y_pred)\nab_grid_acc = accuracy_score(y_test, y_pred)\nab_grid_recall = recall_score(y_test, y_pred)\nab_grid_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(AB_grid_model, X_test, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","df523f79":"y_train_pred = AB_grid_model.predict(X_train)\n\nprint(confusion_matrix(y_train, y_train_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_train, y_train_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(AB_model, X_train, y_train);","42344643":"plot_roc_curve(AB_grid_model, X_test, y_test);","ebd3d5fb":"plot_precision_recall_curve(AB_grid_model, X_test, y_test);","2d59e606":"accuracy_scores","f635753d":"XGB_model = XGBClassifier(random_state=101)\nXGB_model.fit(X_train_scaled, y_train)\ny_pred = XGB_model.predict(X_test_scaled)\ny_train_pred = XGB_model.predict(X_train_scaled)\n\nxgb_f1 = f1_score(y_test, y_pred)\nxgb_acc = accuracy_score(y_test, y_pred)\nxgb_recall = recall_score(y_test, y_pred)\nxgb_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(XGB_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","044fd314":"from yellowbrick.classifier import ClassPredictionError\n\nvisualizer = ClassPredictionError(XGB_model)\n\n# Fit the training data to the visualizer\nvisualizer.fit(X_train_scaled, y_train)\n\n# Evaluate the model on the test data\nvisualizer.score(X_test_scaled, y_test)\n\n# Draw visualization\nvisualizer.poof();","b29e06d7":"xgb_xvalid_model = XGBClassifier(random_state=101)\n\nxgb_xvalid_model_scores = cross_validate(xgb_xvalid_model, X_train_scaled, y_train, scoring = [\"accuracy\", \"precision\", \"recall\", \"f1\"], cv = 10)\nxgb_xvalid_model_scores = pd.DataFrame(xgb_xvalid_model_scores, index = range(1, 11))\n\nxgb_xvalid_model_scores","dbec2801":"xgb_xvalid_model_scores.mean()","4e8b0fef":"XGB_model.feature_importances_ ","fed5880b":"feats = pd.DataFrame(index=X.columns, data=XGB_model.feature_importances_, columns=[\"Feature Importance\"])\nXGB_feature_imp = feats.sort_values(\"Feature Importance\", ascending=False)\n\nXGB_feature_imp ","c10cc577":"plt.figure(figsize=(12,6))\nsns.barplot(y=XGB_feature_imp[\"Feature Importance\"], x=XGB_feature_imp.index)\n\nplt.title(\"Feature Importance\")\nplt.xticks(rotation=45)\nplt.show()","906bd844":"X2 = X.drop(columns = [\"ST_Slope_Up\"]) ","b2f6113f":"X2.columns","97e287d2":"operations = [(\"scaler\", MinMaxScaler()), (\"xgb\", XGBClassifier(random_state=101))]\n\nXGB_pipe_model = Pipeline(steps=operations)\nXGB_pipe_model.get_params()\nXGB_pipe_model.fit(X_train, y_train)\n\ny_pred = XGB_pipe_model.predict(X_test)\ny_train_pred = XGB_pipe_model.predict(X_train)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\nxgb_pipe_f1 = f1_score(y_test, y_pred)\nxgb_pipe_acc = accuracy_score(y_test, y_pred)\nxgb_pipe_recall = recall_score(y_test, y_pred)\nxgb_pipe_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(XGB_pipe_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","2903ff1e":"pipe_scores = cross_validate(XGB_pipe_model, X_train, y_train, scoring = ['accuracy', 'precision','recall','f1'], cv = 10)\ndf_pipe_scores = pd.DataFrame(pipe_scores, index = range(1, 11))\n\ndf_pipe_scores","d8df1861":"df_pipe_scores.mean()[2:]","cb884561":"plot_confusion_matrix(XGB_pipe_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","bc03cb4f":"# evaluate the pipeline\n\n# from sklearn.model_selection import RepeatedStratifiedKFold\n\ncv = RepeatedStratifiedKFold(n_splits=10, random_state=101)\nn_scores = cross_val_score(XGB_pipe_model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n\nprint(f'Accuracy: Results Mean : %{round(n_scores.mean()*100,3)}, Results Standard Deviation : {round(n_scores.std()*100,3)}')","b6e730ad":"print('Accuracy: %.3f (%.3f)' % (n_scores.mean(), n_scores.std()))","2dbd451b":"param_grid = {\"n_estimators\":[100, 300], \n              \"max_depth\":[3,5,6], \n              \"learning_rate\": [0.1, 0.3],\n              \"subsample\":[0.5, 1], \n              \"colsample_bytree\":[0.5, 1]}","c3f9e73d":"XGB_grid_model = XGBClassifier(random_state=42)\nXGB_grid_model = GridSearchCV(XGB_grid_model, param_grid, scoring = \"f1\", verbose=2, n_jobs = -1)","0500f810":"XGB_grid_model.fit(X_train_scaled, y_train)","a44e7c1d":"print(colored('\\033[1mBest Parameters of GridSearchCV for RF Model:\\033[0m', 'blue'), colored(XGB_grid_model.best_params_, 'cyan'))\nprint(\"--------------------------------------------------------------------------------------------------------------------\")\nprint(colored('\\033[1mBest Estimator of GridSearchCV for RF Model:\\033[0m', 'blue'), colored(XGB_grid_model.best_estimator_, 'cyan'))","c5f11d37":"y_pred = XGB_grid_model.predict(X_test_scaled)\ny_train_pred = XGB_grid_model.predict(X_train_scaled)\n\nxgb_grid_f1 = f1_score(y_test, y_pred)\nxgb_grid_acc = accuracy_score(y_test, y_pred)\nxgb_grid_recall = recall_score(y_test, y_pred)\nxgb_grid_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(XGB_grid_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","6c931988":"plot_roc_curve(XGB_grid_model, X_test_scaled, y_test);","afcfdaa0":"plot_precision_recall_curve(XGB_grid_model, X_test_scaled, y_test);","32bb3b5a":"compare = pd.DataFrame({\"Model\": [\"Logistic Regression\", \"SVM\", \"KNN\", \"Decision Tree\", \"Random Forest\", \"AdaBoost\", \"GradientBoost\", \"XGBoost\"],\n                        \"F1\": [log_f1, svm_grid_f1, knn_f1, dt_grid_f1, rf_grid_f1, ab_grid_f1, gb_f1, xgb_grid_f1],\n                        \"Recall\": [log_recall, svm_grid_recall, knn_recall, dt_grid_recall, rf_grid_recall, ab_grid_recall, gb_recall, xgb_grid_recall],\n                        \"Accuracy\": [log_acc, svm_grid_acc, knn_acc, dt_grid_acc, rf_grid_acc, ab_grid_acc, gb_acc, xgb_grid_acc],\n                        \"ROC_AUC\": [log_auc, svm_grid_auc, knn_auc, dt_grid_auc, rf_grid_auc, ab_grid_auc, gb_auc, xgb_grid_auc]})\n\ndef labels(ax):\n    for p in ax.patches:\n        width = p.get_width()                        # get bar length\n        ax.text(width,                               # set the text at 1 unit right of the bar\n                p.get_y() + p.get_height() \/ 2,      # get Y coordinate + X coordinate \/ 2\n                '{:1.3f}'.format(width),             # set variable to display, 2 decimals\n                ha = 'left',                         # horizontal alignment\n                va = 'center')                       # vertical alignment\n    \nplt.figure(figsize=(14,14))\nplt.subplot(411)\ncompare = compare.sort_values(by=\"F1\", ascending=False)\nax=sns.barplot(x=\"F1\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\n\nplt.subplot(412)\ncompare = compare.sort_values(by=\"Recall\", ascending=False)\nax=sns.barplot(x=\"Recall\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\n\nplt.subplot(413)\ncompare = compare.sort_values(by=\"Accuracy\", ascending=False)\nax=sns.barplot(x=\"Accuracy\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\n\nplt.subplot(414)\ncompare = compare.sort_values(by=\"ROC_AUC\", ascending=False)\nax=sns.barplot(x=\"ROC_AUC\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\n\nplt.show()","5d3872d7":"**Cross-checking the model by predictions in Train Set for consistency**","8304cc6a":"We must separate the columns (attributes or features) of the dataset into input patterns (X) and output patterns (y).","f8b03400":"<a id=\"7.2.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.2.a Modelling Support Vector Machine (SVM) with Default Parameters<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","66604ab9":"**SPECIAL NOTE: When we examine the results after handling with skewness, it's clear to assume that handling with skewness could NOT make any contribution to our model when comparing the results obtained by LogisticClassifier without using PowerTransform. So, for the next steps in this study, we will continue not handling with skewness assuming that it's useless for the results.**  ","beb86eec":"<a id=\"7.7.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.7.a Modelling AdaBoostingBoosting (AB) with Default Parameters & Model Performance<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","4e1f61c8":"<a id=\"7.8.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.8.a Modelling XGBoosting (XGB) with Default Parameters & Model Performance<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","a4dbd09f":"<a id=\"6.1\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>6.1 The Implementation of Scaling<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","8b444d17":"<a id=\"7.2.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.2.b Cross-Validating Support Vector Machine (SVM) Model<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","c734a279":"<a id=\"7.5.c\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.5.c Elbow Method for Choosing Reasonable K Values<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","2e4d26fe":"<a id=\"4.3\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>4.3 - The Examination of Numerical Features<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","8e248d23":"<a id=\"7.5.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.5.a Modelling K-Nearest Neighbor (KNN) with Default Parameters<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","4fd97dc2":"### Train \/ Test and Split","0e0be91e":"Kurtosis are of three types:\n\nMesokurtic: When the tails of the distibution is similar to the normal distribution then it is mesokurtic. The kutosis for normal distibution is 3.\n\nLeptokurtic: If the kurtosis is greater than 3 then it is leptokurtic. In this case, the tails will be heaviour than the normal distribution which means lots of outliers are present in the data. It can be recognized as thin bell shaped distribution with peak higher than normal distribution.\n\nPlatykurtic: Kurtosis will be less than 3 which implies thinner tail or lack of outliers than normal distribution.In case of platykurtic, bell shaped distribution will be broader and peak will be lower than the mesokurtic.\nHair et al. (2010) and Bryne (2010) argued that data is considered to be normal if Skewness is between \u20102 to +2 and Kurtosis is between \u20107 to +7.\n\nMulti-normality data tests are performed using leveling asymmetry tests (skewness < 3), (Kurtosis between -2 and 2) and Mardia criterion (< 3). Source Chemingui, H., & Ben lallouna, H. (2013).\n\nSkewness and kurtosis index were used to identify the normality of the data. The result suggested the deviation of data from normality was not severe as the value of skewness and kurtosis index were below 3 and 10 respectively (Kline, 2011). Source Yadav, R., & Pathak, G. S. (2016).","6cdeac74":"Feature scaling (Normalization) is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step. \n\nFor machine learning, in general, it is necessary to normalize features so that no features are arbitrarily large (centering) and all features are on the same scale (scaling).\n\nIn general, algorithms that exploit distances or similarities (e.g. in the form of scalar product) between data samples, such as K-NN and SVM, are sensitive to feature transformations. So it is generally useful, when you are solving a system of equations, least squares, etc, where you can have serious issues due to rounding errors.\n\nHowever, Graphical-model based classifiers, such as Fisher LDA or Naive Bayes, as well as Decision trees and Tree-based ensemble methods (RF, XGB) are invariant to feature scaling, but still, it might be a good idea to rescale\/standardize your data.\n\nNOTE: XGBoost actually implements a second algorithm too, based on linear boosting. Scaling will make a difference there\n\nFor a better understanding and more information please refer to [external link text](https:\/\/en.wikipedia.org\/wiki\/Feature_scaling) & [external link text](https:\/\/www.dataschool.io\/comparing-supervised-learning-algorithms\/)","b64eb770":"Before deeping into the analysis it would be benefical to examine the correlation among variables using heatmap.","436dd967":"**First let's take a close look at the models' accuracy scores for comparing the results given by Scaled, Not Scaled, Balanced and Not Balanced models.**","caeb10cb":"<a id=\"7.3.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.3.b Cross-Validating Decision Tree (DT)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","955bf986":"**RestingECG and HeartDisease**","f9ee4044":"<a id=\"4\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>4) EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","119c11ea":"**Let's look at the best parameters & estimator found by GridSearchCV.**","df290c3d":"After determining related Classifiers from the scikit-learn framework, we can create and and fit them to our training dataset. Models are fit using the scikit-learn API and the model.fit() function.\n\nThen we can make predictions using the fit model on the test dataset. To make predictions we use the scikit-learn function model.predict().","9da37fb6":"<a id=\"3.1\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>3.1 Reading the Data<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n\nHow to read and assign the dataset as df. [external link text](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_csv.html) (You can define it as what you want instead of df)","1ee63d4e":"<a id=\"7.8.c\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.8.c Feature Importance for XGBoosting (XGB) Model<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","94217d8e":"**In our dataset, there have been no missing values so there is no need to handle with them.**\n\nFor a better understanding and more information how to handle with missing values, please refer to [external link text](https:\/\/machinelearningmastery.com\/handle-missing-data-python\/) & [external link text](https:\/\/www.kaggle.com\/kaanboke\/the-most-used-methods-to-deal-with-missing-values)","82b4f634":"<a id=\"7.4\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>7.4 The Implementation of Random Forest (RF)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","c4f7913a":"<a id=\"11\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>11) FURTHER READINGS<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","d0fe9060":"<a id=\"7.8\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>7.8 The Implementation of XGBoosting (XGB)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","ba2404df":"<a id=\"1\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>1) LIBRARIES NEEDED IN THE STUDY<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","d628ef94":"![image.png](attachment:image.png)\n\nImage credit: https:\/\/www.webmd.com\/heart-disease\/ss\/slideshow-heart-disease-surprising-causes ","26401c0d":"<a id=\"7.6.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.6.b Cross-Validating GradientBoosting (GB)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","8cc71841":"<a id=\"7.1.c\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.1.c Modelling Logistic Regression (LR) with Best Parameters Using GridSeachCV<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","6a39eb3a":"**ExerciseAngina and HeartDisease**","66730912":"<a id=\"8\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>8) THE COMPARISON OF MODELS<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","01e8aea8":"**Let's look at the best parameters & estimator found by GridSearchCV.**","1cd724bf":"<a id=\"5\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>5) TRAIN | TEST SPLIT & HANDLING WITH MISSING VALUES<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","effc071c":"<a id=\"7.6\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>7.6 The Implementation of GradientBoosting (GB)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","42d564cc":"<a id=\"7.4.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.4.a Modelling Random Forest (RF) with Default Parameters<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","ffe90aba":"**Have fun while...**","eafdc396":"**Let's look at the best parameters & estimator found by GridSearchCV.**","9cd87d43":"<a id=\"7.3.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.3.a Modelling Decision Tree (DT) with Default Parameters<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","d76e83cf":"<a id=\"7.7.d\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.7.d Analyzing Performance While Weak Learners Are Added<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","7ddb29ed":"- Kline, R.B. (2011). Principles and practice of structural equation modeling (5th ed., pp. 3-427). New York:The Guilford Press.\n- Edwards, A. (1976). An introduction to linear regression and correlation. W. H. Freeman\n- Everitt, B. S.; Skrondal, A. (2010), The Cambridge Dictionary of Statistics, Cambridge University Press.\n- https:\/\/www.amazon.com\/Python-Feature-Engineering-Cookbook-transforming\/dp\/1789806313\/ref=sr_1_1?dchild=1&keywords=feature+engineering+cookbook&qid=1627628487&s=books&sr=1-1\n- https:\/\/www.amazon.com\/Feature-Engineering-Made-Easy-Identify-ebook\/dp\/B077N6MK5W\n- https:\/\/www.amazon.com\/Feature-Engineering-Selection-Chapman-Science\/dp\/1032090855\/ref=sr_1_1?crid=19T9G95E1W7VJ&dchild=1&keywords=feature+engineering+and+selection+kuhn&qid=1628050948&sprefix=feature+engineering+and+%2Cdigital-text%2C293&sr=8-1\n- https:\/\/www.amazon.com\/Introduction-Machine-Learning-Python-Scientists\/dp\/1449369413\n- Neural Networks from Scratch in Python (by Kinsley \u00a7 Kukiela) [external link text](https:\/\/nnfs.io\/)\n- Practical Statistics for Data Scientists (by Bruce & Gedeck) [external link text](https:\/\/www.amazon.com\/Practical-Statistics-Data-Scientists-Essential\/dp\/149207294X\/ref=sr_1_1?dchild=1&keywords=Practical+Statistics+for+Data+Scientists&qid=1627662007&sr=8-1)\n- Applications of Deep Neural Networks(by Jeff Heaton) [external link text](https:\/\/arxiv.org\/abs\/2009.05673)\n- Applied Predictive Modeling (by Kuhn & Johnson) [external link text](https:\/\/www.amazon.com\/Applied-Predictive-Modeling-Max-Kuhn\/dp\/1461468485\/ref=pd_sbs_3\/141-4288971-3747365?pd_rd_w=AOIS7&pf_rd_p=3676f086-9496-4fd7-8490-77cf7f43f846&pf_rd_r=MCCHJXWK39VD6VW7RVAR&pd_rd_r=4ffcd1ea-44b9-4f33-b9b3-dc02ee159662&pd_rd_wg=nU1Ex&pd_rd_i=1461468485&psc=1:)\n- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (by Aur\u00e9lien G\u00e9ron) [external link text](https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1492032646\/ref=sr_1_1?crid=2GV554Q2EKD1E&dchild=1&keywords=hands-on+machine+learning+with+scikit-learn%2C+keras%2C+and+tensorflow&qid=1627628294&s=books&sprefix=hands%2Cstripbooks-intl-ship%2C309&sr=1-1)\n- Master Machine Learning Algorithms (by Brownlee, ML algorithms are very well explained ) [external link text](https:\/\/machinelearningmastery.com\/master-machine-learning-algorithms\/)\n- Python Feature Engineering Cookbook (by Galli) [external link text](https:\/\/www.amazon.com\/Python-Feature-Engineering-Cookbook-transforming\/dp\/1789806313\/ref=sr_1_1?dchild=1&keywords=feature+engineering+cookbook&qid=1627628487&s=books&sr=1-1)\n- Feature Engineering Made Easy (by Ozdemir & Susarla) [external link text](https:\/\/www.amazon.com\/Feature-Engineering-Made-Easy-Identify-ebook\/dp\/B077N6MK5W)\n- Feature Engineering and Selection (by Kuhn & Johnson) [external link text](https:\/\/www.amazon.com\/Feature-Engineering-Selection-Chapman-Science\/dp\/1032090855\/ref=sr_1_1?crid=19T9G95E1W7VJ&dchild=1&keywords=feature+engineering+and+selection+kuhn&qid=1628050948&sprefix=feature+engineering+and+%2Cdigital-text%2C293&sr=8-1)\n- Imbalanced Classification with Python(by Brownlee) [external link text](https:\/\/machinelearningmastery.com\/imbalanced-classification-with-python\/)","ed145918":"<a id=\"4.1\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>4.1 - A General Looking at the Data<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","3f25a3c9":"<a id=\"2.4\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>2.4 Target Variable<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n\nTarget variable, in the machine learning context, is the variable that is or should be the output. For example it could be binary 0 or 1 if you are classifying or it could be a continuous variable if you are doing a regression. In statistics you also refer to it as the response variable.\n\nIn our study our target variable is **HeartDisease** in the contex of determining whether anybody is likely to get hearth disease based on the input parameters like gender, age and various test results or not.","06fcebda":"<a id=\"7.7.g\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.7.g ROC (Receiver Operating Curve) and AUC (Area Under Curve)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","2b3aee80":"<a id=\"7.3.d\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.3.d Feature Importance for Decision Tree (DT) Model<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","9c944839":"<a id=\"7\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>7) MODELLING & MODEL PERFORMANCE<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","7ed139db":"<a id=\"7.4.c\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.4.c Modelling Random Forest (RF) with Best Parameters Using GridSeachCV<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","f422069b":"**Let's compare the results with the ones found via Decision Tree.**","7be30deb":"<a id=\"7.1\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>7.1 The Implementation of Logistic Regression (LR)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","0a4e3628":"<a id=\"2\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>2) Data<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","9bf75f87":"<a id=\"7.7\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>7.7 The Implementation of AdaBoosting (AB)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","cb209ee0":"<a id=\"toc\"><\/a>\n\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Contents<\/h3>\n\n* [   PREFACE](#0)\n* [1) LIBRARIES NEEDED IN THE STUDY](#1)\n    * [1.1 User Defined Functions](#1.1)\n* [2) DATA](#2)\n    * [2.1 Context](#2.1)\n    * [2.2 About the Features](#2.2) \n    * [2.3 What the Problem is](#2.3) \n    * [2.4 Target Variable](#2.3) \n* [3) ANALYSIS](#3)\n    * [3.1) Reading the Data](#3)\n* [4) EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION](#4)\n    * [4.1 A General Looking at the Data](#4.1)\n    * [4.2 - The Examination of Target Variable](#4.2)\n    * [4.3 - The Examination of Numerical Features](#4.3)\n    * [4.4 - The Examination of Skewness & Kurtosis](#4.4)\n    * [4.5 - The Examination of Numerical Features](#4.5)\n    * [4.6 - Dummy Variables Operation](#4.6)    \n* [5) TRAIN | TEST SPLIT & HANDLING WITH MISSING VALUES](#5)    \n    * [5.1 Train | Test Split](#5.1)\n    * [5.2 Handling with Missing Values](#5.2) \n* [6) FEATURE SCALLING](#6)\n    * [6.1 The Implementation of Scaling](#6.1)\n    * [6.2 General Insights Before Going Further](#6.2)    \n    * [6.3 Handling with Skewness with PowerTransform & Checking Model Accuracy Scores](#6.3)\n* [7) MODELLING](#7)    \n    * [7.1 The Implementation of Logistic Regression (LR)](#7.1)\n        * [7.1.a Modelling Logistic Regression (LR) with Default Parameters](#7.1.a)\n        * [7.1.b Cross-Validating Logistic Regression (LR) Model](#7.1.b)\n        * [7.1.c Modelling Logistic Regression (LR) with Best Parameters Using GridSearchCV](#7.1.c)\n        * [7.1.d ROC (Receiver Operating Curve) and AUC (Area Under Curve)](#7.1.d)\n        * [7.1.e The Determination of The Optimal Treshold](#7.1.e)\n    * [7.2 The Implementation of Support Vector Machine (SVM)](#7.2)\n        * [7.2.a Modelling Support Vector Machine (SVM) with Default Parameters](#7.2.a)\n        * [7.2.b Cross-Validating Support Vector Machine (SVM)](#7.2.b)\n        * [7.2.c Modelling Support Vector Machine (SVM) with Best Parameters Using GridSearchCV](#7.2.c)\n        * [7.2.d ROC (Receiver Operating Curve) and AUC (Area Under Curve)](#7.2.d)     \n    * [7.3 The Implementation of Decision Tree (DT)](#7.3)\n        * [7.3.a Modelling Decision Tree (DT) with Default Parameters](#7.3.a)\n        * [7.3.b Cross-Validating Decision Tree (DT)](#7.3.b)\n        * [7.3.c Modelling Decision Tree (DT) with Best Parameters Using GridSeachCV](#7.3.c)\n        * [7.3.d Feature Importance for Decision Tree (DT) Model](#7.3.d)\n        * [7.3.e ROC (Receiver Operating Curve) and AUC (Area Under Curve)](#7.3.e)\n        * [7.3.f The Visualization of the Tree](#7.3.f)\n    * [7.4 The Implementation of Random Forest (RF)](#7.4)\n        * [7.4.a Modelling Random Forest (RF) with Default Parameters](#7.4.a)\n        * [7.4.b Cross-Validating Random Forest (RF)](#7.4.b)\n        * [7.4.c Modelling Random Forest (RF) with Best Parameters Using GridSeachCV](#7.4.c)\n        * [7.4.d Feature Importance for Random Forest (RF) Model](#7.4.d)\n        * [7.4.e ROC (Receiver Operating Curve) and AUC (Area Under Curve)](#7.4.e)\n        * [7.4.f The Visualization of the Tree](#7.4.f)    \n    * [7.5 The Implementation of K-Nearest Neighbor (KNN)](#7.5)\n        * [7.5.a Modelling K-Nearest Neighbor (KNN) with Default Parameters](#7.5.a)\n        * [7.5.b Cross-Validating K-Nearest Neighbor (KNN)](#7.5.b)\n        * [7.5.c Elbow Method for Choosing Reasonable K Values](#7.5.c) \n        * [7.5.d GridsearchCV for Choosing Reasonable K Values](#7.5.d)   \n        * [7.5.e ROC (Receiver Operating Curve) and AUC (Area Under Curve)](#7.5.e)\n    * [7.6 The Implementation of GradientBoosting (GB)](#7.6)\n        * [7.6.a Modelling GradientBoosting (GB) with Default Parameters](#7.6.a)\n        * [7.6.b Cross-Validating GradientBoosting (GB)](#7.6.b)\n        * [7.6.c Feature Importance for GradientBoosting (GB) Model](#7.6.c)\n        * [7.6.d Modelling GradientBoosting (GB) with Best Parameters Using GridSearchCV](#7.6.d)        \n        * [7.6.e ROC (Receiver Operating Curve) and AUC (Area Under Curve)](#7.6.e)       \n    * [7.7 The Implementation of AdaBoosting (AB)](#7.7)\n        * [7.7.a Modelling AdaBoosting (AB) with Default Parameters & Model Performance](#7.7.a)\n        * [7.7.b Cross-Validating AdaBoosting (AB)](#7.7.b)\n        * [7.7.c The Visualization of the Tree](#7.7.c)     \n        * [7.7.d Analyzing Performance While Weak Learners Are Added](#7.7.d)         \n        * [7.7.e Feature Importance for AdaBoosting (AB) Model](#7.7.e)\n        * [7.7.f Modelling AdaBoosting (AB) with Best Parameters Using GridSearchCV](#7.7.f)\n        * [7.7.g ROC (Receiver Operating Curve) and AUC (Area Under Curve)](#7.7.g)       \n    * [7.8 The Implementation of XGBoosting (XGB)](#7.8)\n        * [7.8.a Modelling XGBoosting (XGB) with Default Parameters](#7.8.a)    \n        * [7.8.b Cross-Validating XGBoosting (XGB)](#7.8.b)\n        * [7.8.c Feature Importance for XGBoosting (XGB) Model](#7.8.c)           \n        * [7.8.d Modelling XGBoosting (XGB) with Best Parameters Using GridSearchCV](#7.8.d)     \n        * [7.8.e ROC (Receiver Operating Curve) and AUC (Area Under Curve)](#7.8.e)     \n* [8) THE COMPARISON OF MODELS](#8) \n* [9) CONLUSION](#9)\n* [10) REFERENCES](#10)\n* [11) FURTHER READINGS](#11)","549f6e82":"<a id=\"7.7.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.7.b Cross-Validating AdaBoostingBoosting (AB)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","93f3188d":"**First let's take a close look at the models' accuracy scores for comparing the results given by Scaled, Not Scaled, Balanced and Not Balanced models.**","fab6a8bb":"We are now ready to train our models.","dc1b77fd":"- https:\/\/www.kaggle.com\/andrewmvd\/heart-failure-clinical-data\n- https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_csv.html\n- https:\/\/www.researchgate.net\/publication\/49814836_Problematic_standard_errors_and_confidence_intervals_for_skewness_and_kurtosis\n- https:\/\/www.researchgate.net\/publication\/304577646_Young_consumers'_intention_towards_buying_green_products_in_a_developing_nation_Extending_the_theory_of_planned_behavior\n- https:\/\/www.researchgate.net\/publication\/314032599_TO_DETERMINE_SKEWNESS_MEAN_AND_DEVIATION_WITH_A_NEW_APPROACH_ON_CONTINUOUS_DATA\n- https:\/\/imaging.mrc-cbu.cam.ac.uk\/statswiki\/FAQ\/Simon\n- https:\/\/www.researchgate.net\/publication\/263372601_Resistance_motivations_trust_and_intention_to_use_mobile_financial_services\n- https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html\n- https:\/\/machinelearningmastery.com\/power-transforms-with-scikit-learn\/\n- https:\/\/en.wikipedia.org\/wiki\/Dummy_variable_(statistics) \n- https:\/\/www.displayr.com\/what-are-dummy-variables\/ \n- https:\/\/stattrek.com\/multiple-regression\/dummy-variables.aspx \n- https:\/\/www.statisticshowto.com\/dummy-variables\/\n- https:\/\/en.wikipedia.org\/wiki\/Feature_scaling\n- https:\/\/www.dataschool.io\/comparing-supervised-learning-algorithms\/\n- https:\/\/machinelearningmastery.com\/handle-missing-data-python\/\n- https:\/\/www.kaggle.com\/kaanboke\/the-most-used-methods-to-deal-with-missing-values\n- https:\/\/www.kaggle.com\/karnikakapoor\/fetal-health-classification\n- https:\/\/www.kaggle.com\/karnikakapoor\/heart-failure-prediction-ann\n- https:\/\/www.kaggle.com\/kaanboke\/feature-selection-the-most-common-methods-to-know\n- https:\/\/www.kaggle.com\/kaanboke\/the-most-common-evaluation-metrics-a-gentle-intro\n- https:\/\/www.kaggle.com\/kaanboke\/beginner-friendly-end-to-end-ml-project-enjoy","035f92f0":"**In general, droping the feature that weighs too much on the estimation did NOT make any sense. While True Positive predictions increased, False Negative ones decreased. This situation should be evaluated in accordiance with what we would assume and DOMAIN KNOWLEDGE.** ","064cf6c9":"<a id=\"7.1.d\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.1.d ROC (Receiver Operating Curve) and AUC (Area Under Curve)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","aa5d57f9":"<a id=\"4.2\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>4.2 - The Examination of Target Variable<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","d3075da3":"- In this study respectively,\n\n- We have tried to a predict classification problem in Heart Disease Dataset by a variety of models to classifiy Heart Disease predictions in the contex of determining whether anybody is likely to get hearth disease based on the input parameters like gender, age and various test results or not.\n\n- We have made the detailed exploratory analysis (EDA).\n\n- There have been NO missing values in the Dataset.\n\n- We have decided which metrics will be used.\n\n- We have analyzed both target and features in detail.\n\n- We have transformed categorical variables into dummies so we can use them in the models.\n\n- We have handled with skewness problem for make them closer to normal distribution; however, having examined the results, it's clear to assume that handling with skewness could NOT make any contribution to our models when comparing the results obtained by LogisticClassifier without using PowerTransform. Therefore, in this study we have continue not handling with skewness assuming that it's useless for the results.\n\n- We have cross-checked the models obtained from train sets by applying cross validation for each model performance.\n\n- We have examined the feature importance of some models.\n\n- Lastly we have examined the results of all models visually with respect to select the best one for the problem in hand.\n\n- Any contribution will be appriciated.\n\n- By the way, if you enjoy reading this analysis, you can show it by supporting \ud83d\udc4d","6ee63fcb":"<a id=\"2.3\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>2.3 What the Problem is<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n\n- In the given study, we have a binary classification problem.\n- We will make a prection on the target variable **HeartDisease**\n- Lastly we will build a variety of Classification models and compare the models giving the best prediction on Heart Disease.","1c63ccae":"**Let's look at the best parameters & estimator found by GridSearchCV.**","f628e74b":"<a id=\"6.2\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>6.2 General Insights Before Going Further<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","da14ba36":"**Cross-checking the model by predictions in Train Set for consistency**","c028b24d":"<a id=\"7.4.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.4.b Cross-Validating Random Forest (RF)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","37a7a67c":"**GridSearchCV made a little contribution to True Positive predictions by increasing 69 to 70 while False Negative predictions stayed same.**","fe6eaa94":"<a id=\"7.1.e\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.1.e The Determination of The Optimal Treshold<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","ee34a601":"<a id=\"7.3\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>7.3 The Implementation of Decision Tree (DT)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","f5863ec4":"<a id=\"1.1\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>1.1 User Defined Functions<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","465ea61f":"<a id=\"5.1\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>5.1 Train | Test Split<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","da712020":"<a id=\"7.6.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.6.a Modelling GradientBoosting (GB) with Default Parameters<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","c7fef65e":"<a id=\"2.2\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>2.2 About the Features<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Age:** age of the patient [years]\n\n**Sex:** sex of the patient [M: Male, F: Female]\n\n**ChestPainType:** chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n\n**RestingBP:** resting blood pressure [mm Hg]\n\n**Cholesterol:** serum cholesterol [mm\/dl]\n\n**FastingBS:** fasting blood sugar [1: if FastingBS > 120 mg\/dl, 0: otherwise]\n\n**RestingECG:** resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n\n**MaxHR:** maximum heart rate achieved [Numeric value between 60 and 202]\n\n**ExerciseAngina:** exercise-induced angina [Y: Yes, N: No]\n\n**Oldpeak:** oldpeak = ST [Numeric value measured in depression]\n\n**ST_Slope:** the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n\n**HeartDisease:** output class [1: heart disease, 0: Normal]","1659af88":"<a id=\"5.2\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>5.2 Handling with Missing Values<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","1dcee845":"<a id=\"7.4.d\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.4.d Feature Importance for Random Forest (RF) Model<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","84823231":"**First let's take a close look at the models' accuracy scores for comparing the results given by Scaled, Not Scaled, Balanced and Not Balanced models.**","dee5a2dc":"<a id=\"9\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>9) CONCLUSION<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","72723320":"<a id=\"7.3.c\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.3.c Modelling Decision Tree (DT) with Best Parameters Using GridSeachCV<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","2c9b47f7":"**In general, droping the feature that weighs too much on the estimation did NOT make any sense. Both True Positive predictions and False Negative ones increadably decreased.** ","8e0bd29d":"<a id=\"7.6.e\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.6.e ROC (Receiver Operating Curve) and AUC (Area Under Curve)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","a5e24e00":"<a id=\"7.5.e\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.5.e ROC (Receiver Operating Curve) and AUC (Area Under Curve)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","9e59483e":"For preventing data leakage, we need to handle with kurtosis and skewness issue after splitting our data into train and test sets.\n\n**For this purpose, we will use pipeline() since The pipeline can be used as any other estimator and avoids leaking the test set into the train set**\n\nFor a better understanding and more information, please refer to [external link text](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html) & [external link text](https:\/\/machinelearningmastery.com\/power-transforms-with-scikit-learn\/)","58780ef0":"<a id=\"7.8.e\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.8.e ROC (Receiver Operating Curve) and AUC (Area Under Curve)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","4557306e":"<a id=\"7.2\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>7.2 The Implementation of Support Vector Machine (SVM)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","07de44da":"**Let's look at the best parameters & estimator found by GridSearchCV.**","44d5b5c8":"<a id=\"7.7.c\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.7.c The Visualization of the Tree<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","500079af":"<a id=\"4.5\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>4.5 - The Examination of Categorical Features<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","b060a2b7":"<a id=\"4.4\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>4.4 - The Examination of Skewness & Kurtosis<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","43b5ce03":"<a id=\"0\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>PREFACE<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n\nIn this Exploratory Data Analysis (EDA) and a variety of Model Classifications including Logistic Regression (LR), Support Vector Machine (SVM), AdaBoosting (AB), GradientBoosting (GB), K-Nearest Neighbors (KNN), Random Forest (RF), Desicion Tree (DT), XGBoost (XGB), this study will examine the dataset named as \"Heart Failure Prediction\" under the 'heart_failure_clinical_records' \"csv\" file at Kaggle website [external link text](https:\/\/www.kaggle.com\/andrewmvd\/heart-failure-clinical-data).\n\nThis study, in general, will cover what any beginner in Machine Learning can do as much as possible for a better understanding with the given dataset not only by examining its various aspects but also visualising it. Later S\/he will be familiar with eight (8) Classification Algorithms in Machine Learning.","c40694e4":"<a id=\"6.3\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>6.3 Handling with Skewness with PowerTransform & Checking Model Accuracy Scores<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","aa51e935":"<a id=\"7.3.e\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.3.e ROC (Receiver Operating Curve) and AUC (Area Under Curve)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","425200f4":"<a id=\"7.8.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.8.b Cross-Validating XGBoosting (XGB)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","6787c482":"**First let's take a close look at the models' accuracy scores for comparing the results given by Scaled, Not Scaled, Balanced and Not Balanced models.**","e612eb4d":"<a id=\"7.7.f\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.7.f Modelling AdaBoosting (AB) with Best Parameters Using GridSearchCV<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","3440399b":"<a id=\"7.2.c\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.2.c Modelling Support Vector Machine (SVM)  with Best Parameters Using GridSeachCV<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","faa76ea9":"<a id=\"2.1\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>2.1 Context<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n\nCardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of 5CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.","358eb6f1":"<a id=\"7.4.e\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.4.e ROC (Receiver Operating Curve) and AUC (Area Under Curve)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","ab70fbae":"**Let's look at the best parameters & estimator found by GridSearchCV.**","f24bc6da":"<a id=\"7.1.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.1.b Cross-Validating Logistic Regression (LR) Model<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","cf394bf5":"<a id=\"7.4.f\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.4.f The Visualization of the Tree<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","c7bad67e":"<a id=\"7.6.d\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.6.d Modelling GradientBoosting (GB) Model with Best Parameters Using GridSeachCV<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","04f007e9":"**First let's take a close look at the models' accuracy scores for comparing the results given by Scaled, Not Scaled, Balanced and Not Balanced models.**","2ffc823a":"<a id=\"6\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>6) FEATURE SCALING<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","31378ea0":"**ST_Slope and HeartDisease**","7254e03e":"Finally, we can split the X and Y data into a training and test dataset. The training set will be used to prepare the models used in this study and the test set will be used to make new predictions, from which we can evaluate the performance of the model.\n\nFor this we will use the train_test_split() function from the scikit-learn library. We also specify a seed for the random number generator so that we always get the same split of data each time this example is executed.","9537dbb2":"<a id=\"3\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>3) ANALYSIS<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","b2a7fd67":"**Spliting Dataset into numeric & categoric features**","0e7264ce":"<a id=\"10\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>10) REFERENCES<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","e01f4299":"**First let's take a close look at the models' accuracy scores for comparing the results given by Scaled, Not Scaled, Balanced and Not Balanced models.**","c17c75ff":"<a id=\"7.6.c\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.6.c Feature Importance for GradientBoosting (GB) Model<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","7dff3fe6":"<a id=\"7.2.d\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.2.d ROC (Receiver Operating Curve) and AUC (Area Under Curve)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","a9618053":"**First let's take a close look at the models' accuracy scores for comparing the results given by Scaled, Not Scaled, Balanced and Not Balanced models.**","12b6fafb":"**Sex and HeartDisease**","3c8e1517":"**ChestPainType and HeartDisease**","c49e2750":"<a id=\"7.1.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.1.a Modelling Logistic Regression (LR) with Default Parameters<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","c79fb422":"<a id=\"7.8.d\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.8.d Modelling XGBoosting (XGB) with Best Parameters Using GridSearchCV<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","b03ba459":"**First let's take a close look at the models' accuracy scores for comparing the results given by Scaled, Not Scaled, Balanced and Not Balanced models.**","92300671":"**Let's look at the best parameters & estimator found by GridSearchCV.**","40a89ed5":"**The feature that weighs too much on the estimation can SOMETIMES cause overfitting. We are curious about what happens to our model if we drop the feature with contribution. For this reason, the most important feature will be dropped and the scores will be checked again.**","bf800a8d":"<a id=\"7.7.e\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.7.e Feature Importance for AdaBoostingBoosting (AB) Model<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","ebe9bc70":"<a id=\"7.5.d\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.5.d GridsearchCV for Choosing Reasonable K Values<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","32959244":"<a id=\"7.5.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.5.b Cross-Validating K-Nearest Neighbor (KNN)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","6df1b7ff":"**Cross-checking the model by predictions in Train Set for consistency**","46a89dca":"<a id=\"7.5\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>7.5 The Implementation of K-Nearest Neighbor (KNN)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","6916a6e6":"<a id=\"4.6\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>4.6 - Dummy Variables Operation<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n\nA dummy variable is a variable that takes values of 0 and 1, where the values indicate the presence or absence of something (e.g., a 0 may indicate a placebo and 1 may indicate a drug). Where a categorical variable has more than two categories, it can be represented by a set of dummy variables, with one variable for each category. Numeric variables can also be dummy coded to explore nonlinear effects. Dummy variables are also known as indicator variables, design variables, contrasts, one-hot coding, and binary basis variables.\n\nFor a better understanding and more information, please refer to [external link text](https:\/\/en.wikipedia.org\/wiki\/Dummy_variable_(statistics)), [external link text](https:\/\/www.displayr.com\/what-are-dummy-variables\/), [external link text](https:\/\/stattrek.com\/multiple-regression\/dummy-variables.aspx) & [external link text](https:\/\/www.statisticshowto.com\/dummy-variables\/)","39021c87":"**Let's look at the best parameters & estimator found by GridSearchCV.**","d9b38445":"**The feature that weighs too much on the estimation can SOMETIMES cause overfitting. We are curious about what happens to our model if we drop the feature with contribution. For this reason, the most important feature will be dropped and the scores will be checked again.**"}}