{"cell_type":{"1bec780a":"code","2da95cef":"code","1a54078e":"code","9a97e7dd":"code","de313301":"code","2324c269":"code","0209a018":"code","e7932163":"code","f5ba860b":"code","ab09fc28":"code","caa7b8fc":"code","d94d9608":"code","c7980027":"code","16bd5738":"code","22bf7f97":"code","5d19edb2":"markdown","7628e98c":"markdown","a8546469":"markdown","4bd69f78":"markdown","b98899df":"markdown","14ea6d01":"markdown","d8b00b83":"markdown","249eb377":"markdown","7cd97160":"markdown","e9e69b07":"markdown","16db2632":"markdown","a2d1f25d":"markdown","e96c56f8":"markdown","b3b4274d":"markdown"},"source":{"1bec780a":"import numpy as np \nimport pandas as pd \nimport sklearn as sl\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(\"whitegrid\")\n%matplotlib inline","2da95cef":"full_data = pd.read_csv('..\/input\/heart-disease-dataset\/heart.csv')\n\nnum_features = ['age','trestbps','chol','restecg','thalach','oldpeak']\ncat_features = ['sex','cp','fbs','exang','ca','thal','slope','restecg']\nfull_data = full_data[['sex','cp','fbs','exang','slope','restecg','ca','thal','age','trestbps','chol','thalach','oldpeak','target']]\n","1a54078e":"MF_pairplot = sns.pairplot(full_data[['age','trestbps','chol','thalach','oldpeak','target']], hue='target', kind ='reg', height = 4)\nMF_pairplot","9a97e7dd":"\nfull_data = full_data.dropna()\ny = full_data['target']\nX = full_data.drop('target', axis = 1)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state =42) \n\nprint(y_train.shape)\nprint(y_test.shape)\n","de313301":"from sklearn.compose import ColumnTransformer \nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\ncat_column_tran = ColumnTransformer([('ohe',OneHotEncoder(sparse=False), slice(0,8,1))], remainder='passthrough')\nnum_column_tran = ColumnTransformer([('ss',StandardScaler(),slice(9,13,1))], remainder='passthrough')\npreprocess = Pipeline([('Cat_tran',cat_column_tran),('Num_tran',num_column_tran),('PCA',PCA(0.95))])\n\nprocessed_data = preprocess.fit_transform(X_train)","2324c269":"from sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier()","0209a018":"from sklearn.model_selection import GridSearchCV\nparams_KNN = dict(n_neighbors = range(1,10))\ngrid_search_KNN = GridSearchCV(KNN, param_grid = params_KNN, cv =4, scoring='recall')\ngrid_search_KNN.fit(X_train,y_train)","e7932163":"KNN_best_k = grid_search_KNN.best_params_['n_neighbors']\nprint(\"For a k-Nearest Neighbors model, the optimal value of k is \"+str(KNN_best_k))\nKNN_df = pd.DataFrame(grid_search_KNN.cv_results_)\nfig_KNN = plt.figure(figsize=(12,9))\nplt.plot(KNN_df['param_n_neighbors'],KNN_df['mean_test_score'],'b-o')\nplt.xlim(0,10)\nplt.ylim(0.5,1.0)\nplt.xlabel('k')\nplt.ylabel('Mean recall over 4 cross-validation sets')","f5ba860b":"from sklearn.tree import DecisionTreeClassifier\nDT = DecisionTreeClassifier(max_features = 1)","ab09fc28":"params_DT = dict(max_depth = range(2,30))\ngrid_search_DT = GridSearchCV(DT, param_grid = params_DT, cv = 4, scoring='recall')\ngrid_search_DT.fit(X_train,y_train)","caa7b8fc":"DT_best_layers = grid_search_DT.best_params_['max_depth']\nprint(\"For a Decision Tree model, the optimal number of layers is \"+str(DT_best_layers))\nDT_df = pd.DataFrame(grid_search_DT.cv_results_)\nfig = plt.figure(figsize=(12,9))\nplt.plot(DT_df['param_max_depth'],DT_df['mean_test_score'],'g-o')\nplt.xlim(0,30)\nplt.ylim(0.65,1.0)\nplt.xlabel('Maximum layers')\nplt.ylabel('Mean recall over 4 cross-validation sets')","d94d9608":"from sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier(max_depth = DT_best_layers, max_features = 1)","c7980027":"params_RF = dict(n_estimators = range(1,50))\ngrid_search_RF = GridSearchCV(RF, param_grid = params_RF, cv = 4, scoring='recall')\ngrid_search_RF.fit(X_train,y_train)","16bd5738":"RF_best_estimators = grid_search_RF.best_params_['n_estimators']\nprint(\"For a Random Forest, the optimal number of estimators is \"+str(RF_best_estimators))\nRF_df = pd.DataFrame(grid_search_RF.cv_results_)\nfig = plt.figure(figsize=(30,9))\nplt.plot(RF_df['param_n_estimators'],RF_df['mean_test_score'],'r-o')\nplt.xlim(0,50)\nplt.ylim(0.65,1.0)\nplt.xlabel('Number of estimators')\nplt.ylabel('Mean recall over 4 cross-validation sets')","22bf7f97":"from sklearn.metrics import recall_score, precision_score, accuracy_score\n\nKNN_final = grid_search_KNN.best_estimator_\nKNN_final.fit(processed_data, y_train)\n\nDT_final = grid_search_DT.best_estimator_\nDT_final.fit(processed_data, y_train)\n\nRF_final = grid_search_RF.best_estimator_\nRF_final.fit(processed_data, y_train)\n\npipelines = [KNN_final, DT_final, RF_final]\n\nbest_recall = 0.0\nbest_classifier = 0.0\nbest_pipeline = \"\"\n\npipe_dict = {0:'k-Nearest Neighbours',1:'Decision Tree',2:'Random Forest'}\n\nfor i,model in enumerate(pipelines):\n    X_test_trans = preprocess.transform(X_test)\n    y_pred = model.predict(X_test_trans)\n    print(\"{} test recall: {}\".format(pipe_dict[i],recall_score(y_pred, y_test) ))\n    print(\"{} test precision: {}\".format(pipe_dict[i],precision_score(y_pred, y_test) ))\n    print(\"{} test accuracy: {}\".format(pipe_dict[i],accuracy_score(y_pred, y_test) ))\n    if recall_score(y_pred,y_test)>best_recall:\n        best_recall = recall_score(y_pred,y_test)\n        best_pipeline = model \n        best_classifer = i\n\nprint(\"Classifier with best recall: {}\".format(pipe_dict[best_classifier]))\n","5d19edb2":"We begin by removing NaN elements from the dataset and splitting into training and test sets.","7628e98c":"## Evaluation of the models on the test set ","a8546469":"### Random Forest","4bd69f78":"There is a lot of overlap between the classes for many of the numerical features, suggesting that the use of non-linear classification algorithms such as k-Nearest Neighbours, Decision Trees and Random Forests would be more suitable for performing classification. ","b98899df":"The grid search is used to find the optimal number of layers, with maximum number of features at each node set to 1 to prevent overfitting. ","14ea6d01":"This seems a little too good to be true, even despite my attempts to perform regularisation when training the algorithms. I am aware that the dataset is very small, particularly for these algorithms - this analysis is one of my first independent projects so I'd be particularly grateful for any advice and feedback from the community, particularly regarding parameter selection and coding style. Let me know your thoughts in the comments!","d8b00b83":"This analysis uses data taken from the 1988 heart disease dataset, as described [here](https:\/\/www.kaggle.com\/johnsmith88\/heart-disease-dataset).","249eb377":"## Model Training","7cd97160":"Performing a grid search to obtain the best value for k:","e9e69b07":"### Decision Tree","16db2632":"For the purposes of this investigation, numerical features will be scaled using StandardScaler and catagorical features will be transformed using a One-Hot Encoder, before applying a Principle Component Analysis (PCA). ","a2d1f25d":"### k-Nearest Neighbours ","e96c56f8":"We use the best number of layers from the single Decision Tree model to train the forest, using a grid search to find the optimal number of trees in the forest. ","b3b4274d":"# Predicting Heart Disease with Supervised Learning techniques "}}