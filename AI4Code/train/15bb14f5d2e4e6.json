{"cell_type":{"a13bb885":"code","59c6fdb3":"code","f3ceadd5":"code","31a72432":"code","09655a67":"code","349528c9":"code","4566cf80":"code","01e66e7b":"code","c4ed1854":"code","32c21b65":"code","4a231e47":"code","e3bdf35b":"code","12587cc5":"code","f5248382":"code","862967cd":"code","3e137801":"code","d51023aa":"code","b59ae035":"code","a8b6a359":"code","4d4b5786":"markdown","0e855933":"markdown","1a9ca89f":"markdown","735bbaa4":"markdown","4657c7a9":"markdown","98273f49":"markdown","d896d4c8":"markdown","ac410d79":"markdown","8e262fad":"markdown"},"source":{"a13bb885":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","59c6fdb3":"# Import libraries necessary for this project\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display # Allows the use of display() for DataFrames\n\n# Pretty display for notebooks\n%matplotlib inline\n\n# Set a random seed\nimport random\nrandom.seed(42)\n\n# Load the dataset\ntrain_file = '\/kaggle\/input\/titanic\/train.csv'\ntest_file = '\/kaggle\/input\/titanic\/test.csv'\ntrain_data = pd.read_csv(train_file)\ntest = pd.read_csv(test_file)\n\n# Print the first few entries of the  Titanic data\ntrain_data.head()","f3ceadd5":"import matplotlib.pyplot as plt\nimport seaborn as sns\ncountfeature = [\"Survived\", \"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\"]\ncountlist = list(enumerate(countfeature))\n\nplt.figure(figsize = (15,15))\nplt.suptitle(\"Countplot of Categorical Features\", fontsize=18)\nfor i in countlist: \n    plt.subplot(3,2,i[0]+1)\n    sns.countplot(data = train_data, x = i[1], hue = \"Survived\", palette=\"rainbow\")\n    plt.ylabel(\"\")\n    plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 10})\nplt.tight_layout()\nplt.show()","31a72432":"train_data.Embarked.unique()","09655a67":"# selecting features\ntrain_data=train_data[['Pclass','Sex','SibSp','Parch','Fare','Embarked','Survived']]\n#Change male and Female to 0 ,1\ntrain_data[\"Sex\"].replace({\"male\": 0, \"female\": 1}, inplace=True)\ntrain_data[\"Embarked\"].replace({\"nan\": 0, \"S\": 1,\"C\":2,\"Q\":3}, inplace=True)\ntrain_data.head()","349528c9":"train_data.describe()","4566cf80":"#Age Clumn conatin null \ntrain_data.info()\n","01e66e7b":"#Age Clumn conatin null substitue with mean\ntrain_data.dropna(inplace=True)","c4ed1854":"train_data.info()","32c21b65":"from sklearn.model_selection import train_test_split\nTarget=train_data['Survived']\ntrain_data=train_data.drop(columns=['Survived'])\nX_train, X_test, y_train, y_test = train_test_split(train_data, Target, test_size=0.2, random_state=42)","4a231e47":"#import logistic regression model\nfrom sklearn.linear_model import LogisticRegression \nlogreg = LogisticRegression(solver='liblinear') \nlogreg.fit(X_train, y_train)","e3bdf35b":"# Making predictions\ny_train_p= logreg.predict(X_train)\ny_test_p = logreg.predict(X_test)\n\n# Calculate the accuracy\n# this model seems to overfit the data\nfrom sklearn.metrics import accuracy_score\ntrn_accuracy = accuracy_score(y_train, y_train_p)\ntst_accuracy = accuracy_score(y_test, y_test_p)\nprint('The training accuracy is', trn_accuracy)\nprint('The test accuracy is', tst_accuracy)","12587cc5":"# Import the classifier from sklearn\nfrom sklearn.tree import DecisionTreeClassifier\n\n#  Define the classifier, and fit it to the data\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train,y_train)","f5248382":"# Making predictions\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n# Calculate the accuracy\n# this model seems to overfit the data\nfrom sklearn.metrics import accuracy_score\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprint('The training accuracy is', train_accuracy)\nprint('The test accuracy is', test_accuracy)","862967cd":"#  Train the model\nmodel2 = DecisionTreeClassifier(max_depth=10,min_samples_leaf=6,min_samples_split=3)\nmodel2.fit(X_train,y_train)\n#  Make predictions\ny_train_pred = model2.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n#  Calculate the accuracy\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprint('The training accuracy is', train_accuracy)\nprint('The test accuracy is', test_accuracy)","3e137801":"#Age Clumn conatin null substitue with mean\n# selecting features\ntest_data=test[['Pclass','Sex','SibSp','Parch','Fare','Embarked']]\n#Change male and Female to 0 ,1\ntest_data[\"Sex\"].replace({\"male\": 0, \"female\": 1}, inplace=True)\ntest_data[\"Embarked\"].replace({\"nan\": 0, \"S\": 1,\"C\":2,\"Q\":3}, inplace=True)\n\ntest_data.fillna(0,inplace=True)","d51023aa":"test_data.info()","b59ae035":"y_pred=logreg.predict(test_data)","a8b6a359":"output = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': y_pred})\noutput.to_csv('my_submission_01.csv', index=False)\nprint(\"Your submission was successfully saved!\")","4d4b5786":"## Training the model\n\nNow ready to train a model in sklearn. First, split the data into training and testing sets. Then  train the model on the training set.","0e855933":"## Testing the model\nNow, let's see how our model does, let's calculate the accuracy over both the training and the testing set.","1a9ca89f":"# Improving the model\n\nOk, high training accuracy and a lower testing accuracy. We may be overfitting a bit.\n\nSo now Train a new model, and try to specify some parameters in order to improve the testing accuracy, such as:\n- `max_depth`\n- `min_samples_leaf`\n- `min_samples_split`\n\ncan use trial and error\n","735bbaa4":"\n## Preprocessing the data\n\n","4657c7a9":"# Reading Data","98273f49":"# EDA","d896d4c8":"Recall that these are the various features present for each passenger on the ship:\n- **Survived**: Outcome of survival (0 = No; 1 = Yes)\n- **Pclass**: Socio-economic class (1 = Upper class; 2 = Middle class; 3 = Lower class)\n- **Name**: Name of passenger\n- **Sex**: Sex of the passenger\n- **Age**: Age of the passenger (Some entries contain `NaN`)\n- **SibSp**: Number of siblings and spouses of the passenger aboard\n- **Parch**: Number of parents and children of the passenger aboard\n- **Ticket**: Ticket number of the passenger\n- **Fare**: Fare paid by the passenger\n- **Cabin** Cabin number of the passenger (Some entries contain `NaN`)\n- **Embarked**: Port of embarkation of the passenger (C = Cherbourg; Q = Queenstown; S = Southampton)\n\n","ac410d79":"# Titanic Survival Exploration with Decision Trees And logistic regression \n","8e262fad":"# Prepare test Data"}}