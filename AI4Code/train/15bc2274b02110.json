{"cell_type":{"f69a233c":"code","35ae0c21":"code","bd3f04ef":"code","5374fcea":"code","45be4fc2":"code","f1c20fac":"code","5081621b":"code","ee221792":"code","5fb26bba":"code","a167bf54":"code","b919efaf":"code","acdcc7fe":"code","d4c3c892":"code","ca8bcf9e":"code","4d2ab3fe":"code","a6fd3963":"code","67eb087e":"code","f9d0e08e":"code","761757a2":"code","7a0d73d3":"code","d6e8c42b":"code","600cda59":"code","7436f587":"code","c604b1a7":"code","a8a243d0":"code","dd4bc068":"code","48f7b6b4":"code","5cb3971c":"code","46e0d4e2":"code","e34b1bb8":"code","a1aa7e32":"code","a2067c27":"code","de3a6cb6":"code","47f66dfc":"code","e4e3970f":"code","f9b94d73":"code","69df9521":"code","0e08a8be":"code","1ad9e997":"code","8cd5f39f":"code","3376b804":"code","851fd602":"code","a60a48db":"code","a67bcfee":"code","8cf6699a":"code","703d94c4":"code","0c57699c":"code","5187e14b":"code","800c60be":"code","30458232":"code","f8214594":"code","f937b032":"code","9e80d531":"code","3e0bbe08":"code","9f0fbf36":"code","23387962":"code","1dcdb0a9":"code","ea0ce1a5":"code","8586eb08":"code","37203b5d":"code","a50b3af3":"code","3f8ecb25":"code","6c8d9ab6":"markdown","9ab7ef7c":"markdown","add1c8d7":"markdown","f3045f75":"markdown","606d58de":"markdown","3d6b7b38":"markdown","4aa84d2b":"markdown","4939f1dd":"markdown","8eb7795f":"markdown","63b09289":"markdown","7bade0f6":"markdown","4ab204db":"markdown"},"source":{"f69a233c":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\n%matplotlib inline","35ae0c21":"import seaborn as sns\nsns.set(style = 'darkgrid', palette = 'colorblind')","bd3f04ef":"pd.options.display.max_rows = 100\npd.options.display.max_columns = 100","5374fcea":"import warnings\nwarnings.filterwarnings('ignore')","45be4fc2":"df = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","f1c20fac":"df.head()","5081621b":"df[(df['Class'] == 0)]['Amount']","ee221792":"df[(df['Class'] == 1)]['Amount']","5fb26bba":"sns.countplot(df['Class'])","a167bf54":"df['Class'].value_counts(normalize = True)","b919efaf":"df.columns","acdcc7fe":"df['amount_log'] = np.log(df.Amount + 0.01)\ndf['amount_log']","d4c3c892":"from sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\ndf['amount_scaled'] = ss.fit_transform(df['Amount'].values.reshape(-1,1))","ca8bcf9e":"from sklearn.preprocessing import MinMaxScaler\n\nmm = MinMaxScaler()\n\ndf['amount_minmax'] = mm.fit_transform(df['Amount'].values.reshape(-1,1))","4d2ab3fe":"df\n","a6fd3963":"fig, axs = plt.subplots(nrows = 1, ncols = 4, figsize = (16,4))\n\n\nsns.boxplot(x = 'Class', y = 'Amount', data = df, ax = axs[0])\naxs[0].set_title(\"Class vs Amount\")\n\nsns.boxplot(x = 'Class', y = 'amount_log',data = df, ax = axs[1])\naxs[1].set_title(\"Class vs Amount_Log\")\n\nsns.boxplot(x = \"Class\", y = \"amount_scaled\", data = df, ax = axs[2])\naxs[2].set_title('Class vs Scaled Amt')\n\nsns.boxplot(x = 'Class', y = \"amount_minmax\", data = df, ax = axs[3])\naxs[3].set_title(\"Class vs MinMaxAmt\")","67eb087e":"import pickle","f9d0e08e":"dataCleaned = df\n\nwith open('CreditCardDataCleaned.pkl', 'wb') as fileWriteStream:\n    pickle.dump(dataCleaned, fileWriteStream)\n    \n    fileWriteStream.close()\n    \n    \nprint(\"Pickle file is saved...\")","761757a2":"with open(\"CreditCardDataCleaned.pkl\", 'rb') as fileReadStream:\n    df = pickle.load(fileReadStream)\n    fileReadStream.close()\n    \ndf.head()","7a0d73d3":"df.shape","d6e8c42b":"X = df.drop(['Time', 'Class', 'Amount'], axis = 1)\nprint(X.head())","600cda59":"y = df['Class']\ny","7436f587":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, shuffle = True, random_state = 101)","c604b1a7":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)","a8a243d0":"y_pred = logreg.predict(X_test)\n\ny_pred","dd4bc068":"from sklearn import metrics\n\n\n\nprint(metrics.classification_report(y_test, y_pred))","48f7b6b4":"cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n\ncnf_matrix","5cb3971c":"TN = cnf_matrix[0,0]\nFP = cnf_matrix[0,1]\nFN = cnf_matrix[1,0]\nTP = cnf_matrix[1,1]\n\n\nrecall = TP\/(TP + FN)\nprecision = TP\/(TP + FP)","46e0d4e2":"print('recall = ', round(recall, 3), 'precision = ', round(precision, 3))","e34b1bb8":"F1 = 2 * recall * precision \/ (recall + precision)\n\nprint('F1 = ', round(F1,3))","a1aa7e32":"from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler","a2067c27":"from collections import Counter \n\nfrom sklearn.datasets import make_classification","de3a6cb6":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, shuffle = True, random_state = 0)\n\n\nprint(\"X_train -> \", X_train.shape)\nprint(\"y_train -> \", y_train.shape)\nprint(\"X_test -> \", X_test.shape)\nprint(\"y_test -> \", y_test.shape)","47f66dfc":"## Undersampling only on train\n\n\nprint(\"Original dataset shape -> \", Counter(y_train))\n\nrus = RandomUnderSampler(random_state = 21)\nX_res, y_res = rus.fit_resample(X_train, y_train)","e4e3970f":"print(\"Resampled Dataset -> \", Counter(y_res))","f9b94d73":"X_train = X_res\n\ny_train = y_res","69df9521":"print(\"X Train --> \", X_train.shape)","0e08a8be":"print('y_train --> ', y_train.shape)","1ad9e997":"print(\"X Test ---> \", X_test.shape)","8cd5f39f":"print(\"y_test --> \", y_test.shape)","3376b804":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n\ny_pred = logreg.predict(X_test)","851fd602":"conf_mx = metrics.confusion_matrix(y_test, y_pred)\n\nTN = conf_mx[0,0]\nFP = conf_mx[0,1]\nFN = conf_mx[1,0]\nTP = conf_mx[1,1]\n\n\nprint(\"TN : \", TN)\nprint(\"FP : \", FP)\nprint(\"FN : \", FN)\nprint(\"TP : \", TP)\n\n\nrecall = TP \/ (TP + FN)\nprecision = TP \/ (TP + FP)\n\n\nprint(\"Recall --> \", round(recall, 3), 'precision --> ', round(precision,3))\n\nF1 = 2 * recall * precision \/ (recall + precision)\n\n\nprint('F1 - ', round(F1, 3))","a60a48db":"from imblearn.over_sampling import RandomOverSampler\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, shuffle = True, random_state = 21)\n\nprint(\"X_train -> \", X_train.shape)\nprint(\"y_train -> \", y_train.shape)\nprint(\"X_test -> \", X_test.shape)\nprint(\"y_test -> \", y_test.shape)","a67bcfee":"## Over sampling only on train\n\n\nprint(\"Original dataset shape --> \", Counter(y_train))\nrandom_state = 21\n\n\nros = RandomOverSampler(random_state = random_state)\nX_res, y_res = ros.fit_resample(X_train, y_train)\n\n\nprint(\"Resampled dataset shape --> \", Counter(y_res))\n\n\nX_train = X_res\ny_train = y_res\n\n\nprint(\"X_train  --- > \", X_train.shape)\nprint(\"y_train --- > \", y_train.shape)\nprint(\"X_test  --- > \", X_test.shape)\nprint(\"y_test ---> \", y_test.shape)","8cf6699a":"### oversampling with Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n\ny_pred = logreg.predict(X_test)","703d94c4":"print(\"Accuracy : \", metrics.accuracy_score(y_test, y_pred))\nprint(\"AUC : \", metrics.roc_auc_score(y_test, y_pred))\nprint(\"Precision : \", metrics.precision_score(y_test, y_pred))\nprint(\"Recall : \", metrics.recall_score(y_test, y_pred))\nprint(\"F1 : \", metrics.f1_score(y_test, y_pred))","0c57699c":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, shuffle = True, random_state = 0)\n\nprint(\"X_train -- > \", X_train.shape)\nprint(\"y_train -- > \", y_train.shape)\nprint(\"X_test --> \", X_test.shape)\nprint(\"y_test --> \", y_test.shape)","5187e14b":"## Oversampling only on train\n\nprint(\"Original dataset shape --> \", Counter(y_train))\n\nrandom_state = 21\n\nsmote = SMOTE(random_state = 21)\nX_res, y_res = smote.fit_resample(X_train, y_train)\n\nprint(\"After Resampling ---> \", Counter(y_res))\nX_train = X_res\ny_train = y_res\n\n\nprint(\"X_train  --- > \", X_train.shape)\nprint(\"y_train ----> \", y_train.shape)\nprint(\"X_test ---> \", X_test.shape)\nprint(\"y_test ---> \", y_test.shape)","800c60be":"### SMOTE Sampling with Logistic Regression\n\nlogreg = LogisticRegression(max_iter = 1000)\nlogreg.fit(X_train, y_train)\n\n\ny_pred = logreg.predict(X_test)","30458232":"print(\"Accuracy  -- > \", metrics.accuracy_score(y_test, y_pred))\nprint(\"AUC  --> \", metrics.roc_auc_score(y_test, y_pred))\nprint(\"Precision ---> \", metrics.precision_score(y_test, y_pred))\nprint(\"Recall ---> \", metrics.recall_score(y_test, y_pred))\nprint(\"F1 ---> \", metrics.f1_score(y_test, y_pred))","f8214594":"conf_mx = metrics.confusion_matrix(y_test, y_pred)\n\nTN = conf_mx[0,0]\nFP = conf_mx[0,1]\nFN = conf_mx[1,0]\nTP = conf_mx[1,1]\n\n\nrecall = TP \/ (TP + FN)\nprecision = TP \/ (TP + FP)\n\n\nprint(\"Recall --> \", round(recall, 3), 'precision --> ', round(precision,3))\n\nF1 = 2 * recall * precision \/ (recall + precision)\n\nprint(\"F1 --> \", round(F1, 3))","f937b032":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, shuffle = True, random_state= 0)\n\n\nprint(\"X_train --> \", X_train.shape)\nprint(\"y_train --> \", y_train.shape)\nprint(\"X_test  --> \", X_test.shape)\nprint(\"y_test  --> \", y_test.shape)","9e80d531":"# Oversampling only on train\n\nprint(\"Original dataset shape --> \", Counter(y_train))\n\n\n\nadasyn = ADASYN(random_state = 21)\nX_res, y_res = adasyn.fit_resample(X_train, y_train)\n\n\nprint(\"Resampled Dataset shape ---> \", Counter(y_res))","3e0bbe08":"X_train = X_res\ny_train = y_res","9f0fbf36":"print(\"X_train.shape --> \", X_train.shape)\nprint(\"y_train.shape --> \", y_train.shape)\nprint(\"X_test.shape --> \", X_test.shape)\nprint(\"y_test.shape --> \", y_test.shape)","23387962":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n\ny_pred = logreg.predict(X_test)","1dcdb0a9":"print(\"Accuracy : \", metrics.accuracy_score(y_pred, y_test))\nprint(\"AUC  : \", metrics.roc_auc_score(y_test, y_pred))\nprint(\"Precision : \", metrics.precision_score(y_test, y_pred))\nprint(\"Recall : \", metrics.recall_score(y_test, y_pred))\nprint(\"F1 : \", metrics.f1_score(y_test, y_pred))","ea0ce1a5":"from sklearn.decomposition import PCA\n\n\n\nX_reduced_pca_im = PCA(n_components = 2, random_state = 42).fit_transform(X)","8586eb08":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, shuffle = True, random_state = 0)\n\nprint(\"X_train --> \", X_train.shape)\nprint(\"y_train --> \", y_train.shape)\nprint(\"X_test  --> \", X_test.shape)\nprint(\"y_test  --> \", y_test.shape)\n\nprint(\"-\"*50)\n\n\n# Oversample on train dataset\n\nrandom_state = 21\n\nrus = RandomUnderSampler(random_state = random_state)\nX_res, y_res = rus.fit_resample(X_train, y_train)\n\n\n\nX_train = X_res\ny_train = y_res","37203b5d":"from imblearn.pipeline import Pipeline","a50b3af3":"## CROSS FOLD CROSS VALIDATION\n\nfrom sklearn import model_selection\nkf = model_selection.StratifiedKFold(n_splits = 5)","3f8ecb25":"import xgboost as xgb","6c8d9ab6":"#### Applying PCA to the dataset to reduce dimensionality","9ab7ef7c":"Import Imbalance technique algorithms","add1c8d7":"\n### Baseline Modelling","f3045f75":"1. Classification Models\n    1. LR\n    2. Decision Trees\n    3. Random Forest\n    4. Naive Bayes Classifier\n    \n    \n2. Class Imbalance Handler\n    1. Under Sampling\n    2. Over Sampling\n    3. SMOTE\n    4. ADASYN\n    \n\n3. Metrics \n    1. Accuracy Score\n    2. Confusion Matrix\n    3. Precision Score\n    4. Recall Score\n    5. ROC_AUC\n    6. F1 Score","606d58de":"#### Train Test Distribution","3d6b7b38":"##### Load the preprocessed file","4aa84d2b":"SMOTE Oversampling","4939f1dd":"#### Notebook in making....","8eb7795f":"#### Necessary Data Preprocessing","63b09289":"There's a huge difference between the samples\/class. <br>\nLet's explore the details in terms of percentage.","7bade0f6":"### Pipeline Method","4ab204db":"#### Model Evalution"}}