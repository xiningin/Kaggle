{"cell_type":{"5ca07b74":"code","68cb1bc1":"code","6eb9f127":"code","8f991f46":"code","a2376700":"code","7a3d4781":"code","1974af6f":"code","03d8660b":"code","c4de43eb":"code","28616494":"code","bab9d722":"code","7240e81b":"code","5a681820":"code","93600dfb":"code","48e5d7e1":"code","17b65704":"code","4b7db57e":"code","e533d596":"code","120940b1":"code","f647ca64":"code","89166fab":"code","72507fba":"code","75ca83d7":"code","41063185":"code","64654b0d":"code","556dc40e":"code","15506158":"code","72ff4cbc":"code","9517dede":"code","2793d720":"code","8773b2ce":"code","e8dfbc46":"code","2d11a963":"code","d9fb4077":"code","8139bb1e":"code","f32ad462":"code","ee0237dd":"code","f05f0916":"code","185311b2":"code","a58620aa":"code","db0df714":"code","5959bc8a":"code","bffd7126":"code","b72144f7":"code","5f152afc":"code","004dc087":"code","51ea2194":"code","116b8756":"code","c5590c26":"code","c1416f2c":"code","e341af45":"code","c4876ae1":"code","37ba3db1":"code","aba7a999":"code","04adcbfe":"code","dd565790":"code","bb9af515":"code","960b590c":"code","fc903636":"code","92b058b3":"code","683f92c0":"code","1796cc9e":"code","d8bbf558":"code","f08543ef":"code","960782db":"code","6d44b8c7":"code","44f5e294":"code","f080c2e6":"code","39d43e0c":"code","62aa61dc":"code","8ae5431a":"code","49071016":"code","97dfdf70":"code","366df5eb":"code","3da65749":"code","aa0e9a66":"code","9bcb9dce":"code","74d1a395":"code","739595e6":"code","6c8f59a1":"code","e82aecc5":"code","798bfbfc":"code","04856e27":"code","78a50e81":"code","87d5a2fe":"code","12600d70":"code","16166a9c":"code","801059a7":"code","449e662d":"code","b742abc0":"code","fa1098b8":"code","2421bcff":"code","730aaa9f":"code","c33da075":"code","10094870":"code","6cbc5c71":"code","2b268aeb":"markdown","6b134d49":"markdown","f55e804e":"markdown","4b84d3fc":"markdown","a4c62896":"markdown","d8b0b469":"markdown","84f37257":"markdown","a6907f85":"markdown","d17cf798":"markdown","90907978":"markdown","2c3be4fc":"markdown","8d4d5b11":"markdown","785d3f12":"markdown","261e452e":"markdown","d2712dcb":"markdown","8f2e6926":"markdown","fd7949f7":"markdown","850a4b83":"markdown","d1bba56d":"markdown","1a0ca9e1":"markdown","74886d34":"markdown","df1d3200":"markdown","cfcc4bc8":"markdown","11d84577":"markdown","23d14cc5":"markdown","044e9eaf":"markdown","45816625":"markdown","080005c8":"markdown","1dca97fe":"markdown","d376a64a":"markdown","2fac3bc7":"markdown","defa173e":"markdown","1c0a97f9":"markdown","088ec9fd":"markdown","71156cb4":"markdown","a5db7d4b":"markdown","1f0b20a5":"markdown","62625e11":"markdown","2e4d3a52":"markdown","0700f895":"markdown","89aa61e1":"markdown","af4f20bc":"markdown","c8e4cd7e":"markdown","dd2428d8":"markdown"},"source":{"5ca07b74":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","68cb1bc1":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\nfrom scipy import stats\nfrom cycler import cycler\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_validate, ShuffleSplit\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn import svm, tree, linear_model, neighbors, ensemble\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","6eb9f127":"#plots confusion matrix and values of precison, recall, f1, and accuracy.\n#Credit: http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n    title = 'Confusion Matrix'\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes #[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix')#, without normalization')\n\n    print(cm)\n    tn = cm[0,0]\n    fp = cm[0,1]\n    fn = cm[1,0]\n    tp = cm[1,1]\n\n    precision = tp\/(tp+fp)\n    accuracy = (tp+tn)\/(tp+tn+fp+fn)\n    recall = tp\/(tp+fn)\n    f1score = 2 * precision * recall \/ (precision + recall)\n    print(\"Precision:\\t\", round(precision, 2) * 100, \"%\")\n    print(\"Recall:\\t\\t\", round(recall, 2) * 100, \"%\")\n    print(\"Accuracy:\\t\", round(accuracy, 2) * 100, \"%\")\n    print(\"F1 Score:\\t\", round(f1score, 2) * 100, \"%\")\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\n","8f991f46":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head(n=10)","a2376700":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","7a3d4781":"simple_train = train_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Parch', 'Survived']]\nsimple_train['GroupSize'] = simple_train['SibSp'] + simple_train['Parch'] + 1\nsimple_train = simple_train.drop(columns = ['SibSp', 'Parch'])\nsimple_train['Age'] = simple_train['Age'].fillna(simple_train['Age'].median())\nsimple_train['Sex'] = np.where(simple_train['Sex'] == 'male', 0, 1)\nsimple_train['Fare'] = simple_train['Fare'].fillna(simple_train['Fare'].median())\nprint(simple_train.head())\nprint('='*60)\n\nsimple_test = test_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Parch']]\nsimple_test['GroupSize'] = simple_test['SibSp'] + simple_test['Parch'] + 1\nsimple_test = simple_test.drop(columns = ['SibSp', 'Parch'])\nsimple_test['Age'] = simple_test['Age'].fillna(simple_train['Age'].median())\nsimple_test['Sex'] = np.where(simple_test['Sex'] == 'male', 0, 1)\nsimple_test['Fare'] = simple_test['Fare'].fillna(simple_train['Fare'].median())\nprint(simple_test.head())\nprint('='*60)\nprint(simple_train.isnull().sum())\nprint('='*60)\nprint(simple_test.isnull().sum())","1974af6f":"y = simple_train['Survived']\nX = simple_train.drop(columns = 'Survived')\n\n\nclf = DecisionTreeClassifier(max_depth = 4)#, min_samples_split = 200, min_samples_leaf = 200)\nclf.fit(X, y)\nprint('Training Score:', clf.score(X,y))\n#print(clf.score(X_test,y_test))\n\n","03d8660b":"featuresRank = pd.DataFrame()\nfeaturesRank['features'] = X.columns\nfeaturesRank['scores'] = clf.feature_importances_\nfeaturesRank = featuresRank.sort_values(by=['scores'], ascending=False).reset_index()\nfeaturesRank.plot(x = 'features', y = 'scores', kind=\"bar\", fontsize=12)","c4de43eb":"f, ax = plt.subplots(1, 1, figsize=(30, 10))\ntree.plot_tree(clf, feature_names = X.columns, fontsize = 12, filled = True)\nplt.show()","28616494":"# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\nclf_params = {\n    'criterion': [\"gini\", \"entropy\"], \n    'max_depth': range(2,18,2), \n    'min_samples_split': range(2, 20, 2), \n    'min_samples_leaf' : range(2, 20, 2), \n    'n_estimators': [100,200], \n    'max_features': ['auto', 'log2', None]\n}\n\ncv_split = ShuffleSplit(n_splits = 5, test_size = 0.3, train_size = 0.7, random_state = 42) \n\nclf = RandomForestClassifier()\n\nclf_grid = GridSearchCV(clf, clf_params, scoring = 'roc_auc', verbose = True, n_jobs = -1, cv = cv_split)\n\nclf_grid.fit(X, y)\n\n# print(clf_grid.score(X_val, y_val))\n\nprint('Best params: ', clf_grid.best_params_)\nprint('Mean cross-validated score of the best_estimator', clf_grid.best_score_)\n\n\n# A lot of useful information\n# print(clf_grid.cv_results_)\n\n# Create submission file\npredictions = clf_grid.predict(simple_test)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('rf_submission.csv', index=False)\n","bab9d722":"print(train_data.info())\nprint('='*50)\nprint(test_data.info())","7240e81b":"train_data.describe()","5a681820":"train_data.describe(include=['O'])","93600dfb":"print('% null values in training', 100.0*train_data['Sex'].isnull().sum()\/train_data.shape[0])\nprint('% null values in testing', 100.0*test_data['Sex'].isnull().sum()\/test_data.shape[0])","48e5d7e1":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"Total # of men in train.csv:\", len(men))\nprint(\"Total # of men who survived:\", sum(men))\nprint(\"% of men who survived:\", rate_men)","17b65704":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"Total # of women in train.csv:\", len(women))\nprint(\"Total # of women who survived:\", sum(women))\nprint(\"% of women who survived:\", rate_women)","4b7db57e":"train_data[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e533d596":"g = sns.FacetGrid(train_data, col='Sex')\ng.map(plt.hist, 'Survived', bins=2)","120940b1":"# Dumb Model based on Gender alone\npredictions = np.where(test_data['Sex'] == 'male', 0, 1)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission_gender.csv', index=False)\noutput.head()","f647ca64":"print('Parch')\nprint('% null values in training', 100.0*train_data['Parch'].isnull().sum()\/train_data.shape[0])\nprint('% null values in testing', 100.0*test_data['Parch'].isnull().sum()\/test_data.shape[0])\nprint('SibSp')\nprint('% null values in training', 100.0*train_data['SibSp'].isnull().sum()\/train_data.shape[0])\nprint('% null values in testing', 100.0*test_data['SibSp'].isnull().sum()\/test_data.shape[0])","89166fab":"g = sns.FacetGrid(train_data, col='Survived')\ng.map(plt.hist, 'Parch', bins=20)","72507fba":"train_data[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","75ca83d7":"g = sns.FacetGrid(train_data, col='Survived')\ng.map(plt.hist, 'SibSp', bins=20)","41063185":"train_data['FamilySize'] = train_data['SibSp'] + train_data['Parch']\ntest_data['FamilySize'] = test_data['SibSp'] + test_data['Parch']\ntrain_data[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","64654b0d":"train_data['SmallFamily'] = np.where(train_data['FamilySize'].isin([1,2,3]), 1, 0)\ntest_data['SmallFamily'] = np.where(test_data['FamilySize'].isin([1,2,3]), 1, 0)","556dc40e":"train_data[['SmallFamily', 'Survived']].groupby(['SmallFamily'], as_index=False).mean().sort_values(by='Survived', ascending=False)","15506158":"train_data['isAlone'] = np.where(train_data['FamilySize'] == 0, 1, 0)\ntest_data['isAlone'] = np.where(test_data['FamilySize'] == 0, 1, 0)\ntrain_data[['isAlone', 'Survived']].groupby(['isAlone'], as_index=False).mean().sort_values(by='Survived', ascending=False)","72ff4cbc":"col = 'Age'\nplot_data = train_data.groupby([col, 'Survived']).size().unstack('Survived').fillna(0).reset_index()\nplot_data.plot(x=col, y=[0, 1], kind=\"bar\", figsize=(25,5), fontsize=14)\nplt.title('Distribution of variable: ' + col);","9517dede":"#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid(train_data, hue = 'Survived', aspect=4)\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , train_data['Age'].max()))\na.add_legend()","2793d720":"g = sns.FacetGrid(train_data, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","8773b2ce":"print('% null values in training', 100.0*train_data['Age'].isnull().sum()\/train_data.shape[0])\nprint('% null values in testing', 100.0*test_data['Age'].isnull().sum()\/test_data.shape[0])","e8dfbc46":"print('Overall Median Age', train_data['Age'].median())\nprint('Overall Alone Passengers Median Age', train_data[train_data['isAlone'] == 1]['Age'].median())\nprint('Overall Accompanied Passengers Median Age', train_data[train_data['isAlone'] == 0]['Age'].median())\nprint('='*60)\nprint('Survived Median Age', train_data[train_data['Survived'] == 1]['Age'].median())\nprint('='*60)\nprint('Deceased Median Age', train_data[train_data['Survived'] == 0]['Age'].median())\nprint('='*60)\nprint('With SibSp Passengers Median Age', train_data[train_data['SibSp'] > 0]['Age'].median())\nprint('With Parch Passengers Median Age', train_data[train_data['Parch'] > 0]['Age'].median())\nprint('='*60)\nprint('No SibSp No Parch Median Age', train_data[(train_data['SibSp'] == 0) & (train_data['Parch'] == 0)]['Age'].median())\nprint('No SibSp With Parch Median Age', train_data[(train_data['SibSp'] == 0) & (train_data['Parch'] > 0)]['Age'].median())\nprint('With SibSp No Parch Median Age', train_data[(train_data['SibSp'] > 0) & (train_data['Parch'] == 0)]['Age'].median())\nprint('With SibSp With Parch Median Age', train_data[(train_data['SibSp'] > 0) & (train_data['Parch'] > 0)]['Age'].median())\n","2d11a963":"train_data['Age'] = train_data['Age'].fillna(train_data['Age'].median())\ntest_data['Age'] = test_data['Age'].fillna(train_data['Age'].median())","d9fb4077":"bins = 5\ntrain_data['AgeBin'], train_bins = pd.cut(x = train_data['Age'].astype(int), bins = bins, labels = np.arange(0,bins,1), retbins = True)\ntest_data['AgeBin'], test_bins = pd.cut(x = test_data['Age'].astype(int), bins = bins, labels = np.arange(0,bins,1), retbins = True)\nplot_data = train_data.groupby(['AgeBin', 'Survived']).size().unstack('Survived').fillna(0).reset_index()\nplot_data.plot(x='AgeBin', y=[0, 1], kind=\"bar\", figsize=(10,4), fontsize=14)\nplt.title('Distribution of variable: AgeBin in training');","8139bb1e":"#Manual Bins \n# train_data['AgeBin'], train_bins = pd.cut(x = train_data['Age'].astype(int), bins = [0, 1, 19, 60, 140], retbins = True)\n# plot_data = train_data.groupby(['AgeBin', 'Survived']).size().unstack('Survived').fillna(0).reset_index()\n# plot_data.plot(x='AgeBin', y=[0, 1], kind=\"bar\", figsize=(10,4), fontsize=14)\n# plt.title('Distribution of variable: AgeBin in training');","f32ad462":"print('Bins starting points')\nprint('Train:', train_bins)\n\nprint('Training Age Bins')\nfor i in np.arange(0, len(train_bins) - 1, 1):\n    print('(', train_bins[i], ',', train_bins[i+1], ']')\nprint('='*60)\n\nprint('Test:', test_bins)\nprint('Testing Age Bins')\nfor i in np.arange(0, len(test_bins) - 1, 1):\n    print('(', test_bins[i], ',', test_bins[i+1], ']')\n\n#Note to self, what is the impact of different bin ranges in train and test data?","ee0237dd":"train_data['Embarked'].value_counts()","f05f0916":"g = sns.FacetGrid(train_data, col='Survived')\ng.map(plt.hist, 'Embarked', bins=10)","185311b2":"train_data['Embarked'].mode()","a58620aa":"train_data['Embarked'] = train_data['Embarked'].fillna('S')\ntest_data['Embarked'] = test_data['Embarked'].fillna('S')","db0df714":"feats = ['Pclass', 'Fare']\n\nfor f in feats:\n    print(f)\n    print('% null values in training', 100.0*train_data[f].isnull().sum()\/train_data.shape[0])\n    print('% null values in testing', 100.0*test_data[f].isnull().sum()\/test_data.shape[0])\n    print('='*60)\n","5959bc8a":"train_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","bffd7126":"col = 'Pclass'\nplot_data = train_data.groupby([col, 'Survived']).size().unstack('Survived').fillna(0).reset_index()\nplot_data.plot(x=col, y=[0, 1], kind=\"bar\", figsize=(10,4), fontsize=14)\nplt.title('Distribution of variable = ' + col);","b72144f7":"print('Median Fare', train_data['Fare'].median())\nprint('Class 1 Median Fare', train_data[train_data['Pclass'] == 1]['Fare'].median())\nprint('Class 2 Median Fare', train_data[train_data['Pclass'] == 2]['Fare'].median())\nprint('Class 3 Median Fare', train_data[train_data['Pclass'] == 3]['Fare'].median())\n","5f152afc":"train_data.loc[(train_data['Pclass'] == 1) & (train_data['Fare'].isnull()), ['Fare']] = train_data[train_data['Pclass'] == 1]['Fare'].median()\ntrain_data.loc[(train_data['Pclass'] == 2) & (train_data['Fare'].isnull()), ['Fare']] = train_data[train_data['Pclass'] == 2]['Fare'].median()\ntrain_data.loc[(train_data['Pclass'] == 3) & (train_data['Fare'].isnull()), ['Fare']] = train_data[train_data['Pclass'] == 3]['Fare'].median()\ntest_data.loc[(test_data['Pclass'] == 1) & (test_data['Fare'].isnull()), ['Fare']] = train_data[train_data['Pclass'] == 1]['Fare'].median()\ntest_data.loc[(test_data['Pclass'] == 2) & (test_data['Fare'].isnull()), ['Fare']] = train_data[train_data['Pclass'] == 2]['Fare'].median()\ntest_data.loc[(test_data['Pclass'] == 3) & (test_data['Fare'].isnull()), ['Fare']] = train_data[train_data['Pclass'] == 3]['Fare'].median()","004dc087":"cols = ['Pclass', 'Min', 'Max', 'Avg', 'Std', 'Median']\nfare_stats = pd.DataFrame(columns = cols)\n\nfor pclass in [1,2,3]:\n    #set name and parameters\n    fare_stats.loc[pclass - 1, 'Pclass'] = pclass\n    fare_stats.loc[pclass - 1, 'Min'] = train_data[train_data['Pclass'] == pclass]['Fare'].min()\n    fare_stats.loc[pclass - 1, 'Max'] = train_data[train_data['Pclass'] == pclass]['Fare'].max()\n    fare_stats.loc[pclass - 1, 'Avg'] = train_data[train_data['Pclass'] == pclass]['Fare'].mean()\n    fare_stats.loc[pclass - 1, 'Std'] = train_data[train_data['Pclass'] == pclass]['Fare'].std()\n    fare_stats.loc[pclass - 1, 'Median'] = train_data[train_data['Pclass'] == pclass]['Fare'].median()\n\nfare_stats.head()","51ea2194":"#plot distributions of fare of passengers\na = sns.FacetGrid(train_data, row = 'Survived', hue = 'Pclass', aspect=4)\na.map(sns.kdeplot, 'Fare', shade= True )\na.set(xlim=(0 , train_data['Fare'].max()))\na.add_legend()","116b8756":"g = sns.FacetGrid(train_data, row = 'Survived', col='Pclass')\ng.map(sns.kdeplot, 'Fare', shade = 'True')","c5590c26":"train_data[train_data['Fare'] == 0]\n\n#Note to self - May be zero fare values are errors and need to be cleaned?","c1416f2c":"test_data[test_data['Fare'] == 0]","e341af45":"col = 'Fare'\nplot_data = train_data.groupby([col, 'Survived']).size().unstack('Survived').fillna(0).reset_index()\nplot_data.plot(x=col, y=[0, 1], kind=\"bar\", figsize=(35,7), fontsize=14)\nplt.title('Distribution of variable: ' + col);","c4876ae1":"grid = sns.FacetGrid(train_data, col='Survived', size=4)#, aspect=1.6)\ngrid.map(plt.hist, 'Fare', bins=10)\ngrid.add_legend();","37ba3db1":"#Binning for age vs fare (using raw data)\ntrain_raw = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nf, ax = plt.subplots(2, 1, figsize=(6,8))\nsns.distplot(a = train_raw['Age'], ax = ax[0])\nax[0].set_title('Distribution of Age')\nsns.distplot(a = train_raw['Fare'], ax = ax[1])\nax[1].set_title('Distribution of Fare')\nf.set_tight_layout(True)","aba7a999":"#Equal bin sizes for Fare\nbins = 30\ntrain_data['FareBin'], train_bins = pd.cut(x = train_data['Fare'].astype(int), bins = bins, retbins = True)\ntest_data['FareBin'], test_bins = pd.cut(x = test_data['Fare'].astype(int), bins = bins, retbins = True)\nplot_data = train_data.groupby(['FareBin', 'Survived']).size().unstack('Survived').fillna(0).reset_index()\nplot_data.plot(x='FareBin', y=[0, 1], kind=\"bar\", figsize=(15,4), fontsize=14)\nplt.title('Distribution of variable: FareBin in training');","04adcbfe":"#Equal samples per bin\nbins = 3\ntrain_data['FareBin'], train_bins = pd.qcut(x = train_data['Fare'].astype(int), q = bins, retbins = True, labels = np.arange(0,bins))\ntest_data['FareBin'], test_bins = pd.qcut(x = test_data['Fare'].astype(int), q = bins, retbins = True, labels = np.arange(0,bins))\nplot_data = train_data.groupby(['FareBin', 'Survived']).size().unstack('Survived').fillna(0).reset_index()\nplot_data.plot(x='FareBin', y=[0, 1], kind=\"bar\", figsize=(10,4), fontsize=14)\nplt.title('Distribution of variable: FareBin in training')\nplt.show()\nprint(train_data[['FareBin', 'Survived']].groupby(['FareBin'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('='*60)\nprint(train_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False))","dd565790":"train_data['AgeBin'] = train_data['AgeBin'].astype(int)\ntest_data['AgeBin'] = test_data['AgeBin'].astype(int)\ntrain_data['FareBin'] = train_data['FareBin'].astype(int)\ntest_data['FareBin'] = test_data['FareBin'].astype(int)","bb9af515":"print('Training data nulls')\nprint(train_data.isnull().sum())\nprint('='*60)\nprint('Testing data nulls')\nprint(test_data.isnull().sum())","960b590c":"train_data['Name']","fc903636":"train_data['Title'] = train_data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\nprint('Training Data')\nprint(train_data.groupby(['Title', 'Sex']).size())\n\nprint('='*60)\nprint('Testing Data')\ntest_data['Title'] = test_data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\nprint(test_data.groupby(['Title', 'Sex']).size())","92b058b3":"# example: Braund, Mr. Owen Harris\n# split on comma and then .\n\ntrain_data['Title'] = train_data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n#print(train_data['Title'].value_counts())\ntrain_data.loc[train_data['Title'].isin(['Capt', 'Rev', 'Major', 'Col', 'Sir']), ['Title']] = 'Mr'\ntrain_data.loc[train_data['Title'].isin(['Mlle', 'Ms']), ['Title']] = 'Miss'\ntrain_data.loc[train_data['Title'].isin(['Mme']), ['Title']] = 'Mrs'\n\ntest_data['Title'] = test_data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\ntest_data.loc[test_data['Title'].isin(['Capt', 'Rev', 'Major', 'Col', 'Sir']), ['Title']] = 'Mr'\ntest_data.loc[test_data['Title'].isin(['Mlle', 'Ms']), ['Title']] = 'Miss'\ntest_data.loc[test_data['Title'].isin(['Mme']), ['Title']] = 'Mrs'\n\nprint('='*40)\nprint('After Cleaning')\nprint(train_data['Title'].value_counts())\nprint('='*40)\nprint('Test After Cleaning')\nprint(test_data['Title'].value_counts())\n","683f92c0":"title_counts = (train_data['Title'].value_counts() < 10)\ntrain_data['Title'] = np.where(train_data['Title'].isin(title_counts.index[title_counts]), 'Misc', train_data['Title'])\n\ntitle_counts = (test_data['Title'].value_counts() < 10)\ntest_data['Title'] = np.where(test_data['Title'].isin(title_counts.index[title_counts]), 'Misc', test_data['Title'])","1796cc9e":"print('Training Data')\nprint(train_data.groupby(['Title', 'Sex']).size())\n\nprint('='*60)\nprint('Testing Data')\nprint(test_data.groupby(['Title', 'Sex']).size())","d8bbf558":"train_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean().sort_values(by='Survived', ascending=False)","f08543ef":"print(train_data.info())\nprint('='*60)\nprint(test_data.info())","960782db":"train = train_data.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin', 'Age', 'Fare', 'SibSp', 'Parch'])\ntest = test_data.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin', 'Age', 'Fare', 'SibSp', 'Parch'])","6d44b8c7":"train.columns","44f5e294":"features = [x for x in train.columns if x != 'Survived'] + ['Survived']\nprint(features)","f080c2e6":"#train = train[features]\n#test = test[features[:-1]]","39d43e0c":"for feature in features[:-1]:\n    print(train_data[[feature, 'Survived']].groupby([feature], as_index=False).mean().sort_values(by='Survived', ascending=False))\n    print('='*40)","62aa61dc":"train_data[(train_data['Title'] == 'Misc')]['Name']","8ae5431a":"print('Null values in training')\nprint(train.isnull().sum())\nprint('='*60)\nprint('Null values in test')\nprint(test.isnull().sum())\n","49071016":"train.head()","97dfdf70":"test.head()","366df5eb":"le = LabelEncoder()\ntrain['Sex'] = le.fit_transform(train['Sex'])\ntest['Sex'] = le.transform(test['Sex'])\n\ny = train[\"Survived\"]\nX = pd.get_dummies(train[features[:-1]], columns = ['Embarked', 'Title'])\nX_test = pd.get_dummies(test[features[:-1]], columns = ['Embarked', 'Title'])\nX.head()\n","3da65749":"corr = pd.get_dummies(train, columns = ['Embarked', 'Title']).corr()\nf, ax = plt.subplots(1, 1, figsize=(40,10))\nsns.heatmap(corr, cmap = 'RdBu', ax = ax, annot = True, annot_kws={'fontsize':14 })\n# sns.heatmap(corr[['Survived']], cmap = 'RdBu', ax=ax[0], annot = True, annot_kws={'fontsize':11 })","aa0e9a66":"models = [\n    RandomForestClassifier(),\n    LogisticRegressionCV(),\n    KNeighborsClassifier(),\n    SVC(probability=True),\n    DecisionTreeClassifier(max_depth = 4),\n    ]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\ncv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .7, random_state = 42) \n# run model 10 times with 70\/30 split\n\n\n\nresult_columns = ['Model Name', 'Parameters', 'Time', 'Avg. Train Accuracy', 'Avg. Test Accuracy', \n                  'Avg. Train Precision', 'Avg. Test Precision', 'Avg. Train Recall', \n                  'Avg. Test Recall', 'Avg. Train F1 Score', 'Avg. Test F1 Score',\n                  'Avg. Train ROC AUC', 'Avg. Test ROC AUC']\nresults = pd.DataFrame(columns = result_columns)\n\nmax_test = 0.0\nrow_index = 0\n\nall_predictions = pd.DataFrame()\nall_predictions['Survived'] = y_train\n\nall_probs = pd.DataFrame()\n#all_probs['Survived'] = y\n\nfor model in models:\n\n    model_name = model.__class__.__name__\n    print('Fitting', model_name)\n    results.loc[row_index, 'Model Name'] = model_name\n    results.loc[row_index, 'Parameters'] = str(model.get_params())\n    \n    # use sorted(sklearn.metrics.SCORERS.keys()) to get list of metrics\n    cv_results = cross_validate(model, X_train, y_train, cv  = cv_split, return_train_score = True, \n                                scoring = ['precision', 'recall', 'accuracy', 'roc_auc'])\n    #print(cv_results)\n    results.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n    results.loc[row_index, 'Avg. Train Accuracy'] = cv_results['train_accuracy'].mean()\n    results.loc[row_index, 'Avg. Test Accuracy'] = cv_results['test_accuracy'].mean()\n    results.loc[row_index, 'Avg. Train Precision'] = cv_results['train_precision'].mean()\n    results.loc[row_index, 'Avg. Test Precision'] = cv_results['test_precision'].mean()\n    results.loc[row_index, 'Avg. Train Recall'] = cv_results['train_recall'].mean()\n    results.loc[row_index, 'Avg. Test Recall'] = cv_results['test_recall'].mean()\n    results.loc[row_index, 'Avg. Train ROC AUC'] = cv_results['train_roc_auc'].mean()\n    results.loc[row_index, 'Avg. Test ROC AUC'] = cv_results['test_roc_auc'].mean()\n    \n    model.fit(X, y)\n    all_predictions[model_name] = model.predict(X_train)\n    all_probs[model_name] = model.predict_proba(X_val)[:,1]\n    \n    if cv_results['train_accuracy'].mean() > max_test:\n        max_test = cv_results['train_accuracy'].mean()\n        best_clf = model\n    \n    row_index = row_index + 1\n\n\nresults['Avg. Train F1 Score'] = 2 * (results['Avg. Train Precision'] * results['Avg. Train Recall'])\/(results['Avg. Train Precision'] + results['Avg. Train Recall'])\nresults['Avg. Test F1 Score'] = 2 * (results['Avg. Test Precision'] * results['Avg. Test Recall'])\/(results['Avg. Test Precision'] + results['Avg. Test Recall'])\nresults = results.sort_values(by = ['Avg. Test Accuracy'], ascending = False).reset_index(drop = True)\nresults.head(n=10)","9bcb9dce":"#all_probs = all_probs.drop(columns = 'Survived')\n\nf, ax = plt.subplots(1, 1, figsize=(15,10))\nax.set_prop_cycle(cycler('color', ['r', 'c', 'm', 'y', 'g', 'b']))\n\nfor model in all_probs.columns:\n    y_score = all_probs[model].values\n    fpr, tpr, thresholds = roc_curve(y_val, y_score)\n    ax.plot(fpr, tpr, label = model)\n\n#Gender Classifier\ny_score = np.where(X_val['Sex'] == 'male', 0.0, 1.0)\nfpr, tpr, thresholds = roc_curve(y_val, y_score)\nax.plot(fpr, tpr, label = 'Gender-based')\n\n#Perfect Classifier\ny_score = np.where(X_val['Sex'] == 'male', 0.0, 1.0)\nfpr, tpr, thresholds = roc_curve(y_val, y_val)\nax.plot(fpr, tpr, 'r--', label = 'Perfect Classifier')\n\n#Worst Classifier\ny_score = 1 - y_val\nfpr, tpr, thresholds = roc_curve(y_val, y_score)\nax.plot(fpr, tpr, 'b--', label = 'Worst Classifier')\n    \nax.plot(np.linspace(0.0,1.0,100), np.linspace(0.0,1.0,100), 'k--')\nax.set_ylim(-0.05, 1.05)\nax.set_xlim(-0.05, 1.05)\nplt.legend()","74d1a395":"print(best_clf.__class__.__name__)\nprint(str(best_clf.get_params()))\nbest_clf.fit(X, y)\npredictions = best_clf.predict(X_test)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission_untuned.csv', index=False)\n","739595e6":"# clf = DecisionTreeClassifier(max_depth = 5)\n# clf.fit(X, y)\n# predictions = clf.predict(X_test)\n# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission_dt.csv', index=False)\n","6c8f59a1":"# For visualization only\nclf = DecisionTreeClassifier(max_depth = 3)#, min_samples_split = 200, min_samples_leaf = 200)\nclf.fit(X, y)\nfeaturesRank = pd.DataFrame()\nfeaturesRank['features'] = X.columns\nfeaturesRank['scores'] = clf.feature_importances_\nfeaturesRank = featuresRank.sort_values(by=['scores'], ascending=False).reset_index()\nfeaturesRank.plot(x = 'features', y = 'scores', kind=\"bar\", figsize=(15,4), fontsize=12)\n","e82aecc5":"f, ax = plt.subplots(1, 1, figsize=(20,10))\ntree.plot_tree(clf, ax = ax, feature_names = X.columns, fontsize = 12, filled = True)\nplt.show()","798bfbfc":"all_predictions.head()","04856e27":"f, ax = plt.subplots(1, 1, figsize=(10,8))\nsns.heatmap(all_predictions.drop(columns='Survived').corr(), cmap = 'RdBu', ax = ax, annot = True, annot_kws={'fontsize':14 })","78a50e81":"results.head()","87d5a2fe":"models = [\n    ('rfc', RandomForestClassifier()),\n    ('knn', KNeighborsClassifier()),\n    ('svc', SVC(probability=True)),\n    ('dtc', DecisionTreeClassifier()),\n    ]\n\n# run model 10 times with 70\/30 split\ncv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .7, random_state = 42) \n\nvoting_results = pd.DataFrame(columns = results.columns)\n#Ensemble\n\n#vote_models = []\n\nfor row_index, voting in enumerate(['hard', 'soft']):\n    vote_clf = VotingClassifier(estimators = models , voting = voting)\n    vote_cv = cross_validate(vote_clf, X, y, cv  = cv_split, verbose = True, n_jobs = -1, \n                               return_train_score = True, scoring = ['precision', 'recall', 'accuracy'])\n    voting_results.loc[row_index, 'Model Name'] = vote_clf.__class__.__name__ + '_' + voting\n    voting_results.loc[row_index, 'Parameters'] = str(vote_clf.get_params())\n    voting_results.loc[row_index, 'Time'] = vote_cv['fit_time'].mean()\n    voting_results.loc[row_index, 'Avg. Train Accuracy'] = vote_cv['train_accuracy'].mean()\n    voting_results.loc[row_index, 'Avg. Test Accuracy'] = vote_cv['test_accuracy'].mean()\n    voting_results.loc[row_index, 'Avg. Train Precision'] = vote_cv['train_precision'].mean()\n    voting_results.loc[row_index, 'Avg. Test Precision'] = vote_cv['test_precision'].mean()\n    voting_results.loc[row_index, 'Avg. Train Recall'] = vote_cv['train_recall'].mean()\n    voting_results.loc[row_index, 'Avg. Test Recall'] = vote_cv['test_recall'].mean()\n#     voting_results.loc[row_index, 'Avg. Train AUC ROC'] = vote_cv['train_roc_auc'].mean()\n#     voting_results.loc[row_index, 'Avg. Test AUC ROC'] = vote_cv['test_roc_auc'].mean()\n\nvoting_results['Avg. Train F1 Score'] = 2 * (voting_results['Avg. Train Precision'] * voting_results['Avg. Train Recall'])\/(voting_results['Avg. Train Precision'] + voting_results['Avg. Train Recall'])\nvoting_results['Avg. Test F1 Score'] = 2 * (voting_results['Avg. Test Precision'] * voting_results['Avg. Test Recall'])\/(voting_results['Avg. Test Precision'] + voting_results['Avg. Test Recall'])\n","12600d70":"pd.concat([results, voting_results]).head(n = 10)","16166a9c":"vote_clf = VotingClassifier(estimators = models , voting = 'soft')\nvote_clf.fit(X, y)\npredictions = vote_clf.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission_untuned_ensemble.csv', index=False)\n","801059a7":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)","449e662d":"train_score = []\ntest_score = []\ndepths = np.arange(1,16)\n\nfor depth in depths:\n    clf = DecisionTreeClassifier(max_depth = depth)#, min_samples_split = 10)#, min_samples_leaf = 200)\n    clf.fit(X_train, y_train)\n    train_score.append(clf.score(X_train, y_train))\n    test_score.append(clf.score(X_val, y_val))\nplt.plot(depths, train_score, 'b', label = 'Train Score')\nplt.plot(depths, test_score, 'r', label = 'Test Score')\nplt.ylim(0.7,1)\nplt.xlim(0,16)\n\ntrain_score = []\ntest_score = []\nfor depth in depths:\n    clf = DecisionTreeClassifier(max_depth = depth, min_samples_split = 25)#, min_samples_leaf = 200)\n    clf.fit(X_train, y_train)\n    train_score.append(clf.score(X_train, y_train))\n    test_score.append(clf.score(X_val, y_val))\nplt.plot(depths, train_score, 'b--', label = 'Train Score (Pruned)')\nplt.plot(depths, test_score, 'r--', label = 'Test Score (Pruned)')\n\n\n\nplt.legend()\nplt.show()","b742abc0":"clf_params = {'criterion': [\"gini\"], \n              'max_depth': list(range(2,10,2)) + ['None'], \n              'min_samples_split': list(range(2, 10, 2)), \n              'min_samples_leaf' : list(range(2, 10, 2)), \n              'n_estimators': [100,200], \n              'max_features': ['auto', 'log2', None]}\n\nclf = RandomForestClassifier(random_state=42)\nclf_grid = GridSearchCV(clf, clf_params, scoring = 'roc_auc', verbose = True, n_jobs = -1, cv = 3)\nclf_grid.fit(X_train, y_train)\n","fa1098b8":"print(\"Classifier: \", clf_grid.best_estimator_.__class__.__name__)\nprint(\"Parameters: \", str(clf_grid.best_params_))\nprint(\"Mean CV score: \", clf_grid.best_score_)\nprint(\"Validation score: \", clf_grid.score(X_val, y_val))","2421bcff":"plot_confusion_matrix(y_val, clf_grid.predict(X_val), classes = [0,1])","730aaa9f":"predictions = clf_grid.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission_tuned_rf.csv', index=False)\n","c33da075":"models = [\n    ('rfc', RandomForestClassifier()),\n    ('lr', LogisticRegressionCV()),\n    ('knn', KNeighborsClassifier()),\n    ('svc', SVC()),\n    ]\n\nfor model in models:\n    print(model[0])","10094870":"models = [\n    ('rfc', RandomForestClassifier()),\n    ('lr', LogisticRegressionCV()),\n    ('knn', KNeighborsClassifier()),\n    ('svc', SVC(probability=True)),\n    ]\n\nparams = [\n        {\n            # Random Forest\n            'criterion': [\"gini\", \"entropy\"], \n            'max_depth': list(range(2,18,2)) + ['None'], \n            'min_samples_split': list(range(2, 18, 2)), \n            'min_samples_leaf' : list(range(2, 18, 2)), \n            'n_estimators': [100,200], \n            'max_features': ['auto', 'log2', None]\n        },\n    \n        {\n            #Logistic Regression \n            'fit_intercept': [True, False],\n            'penalty': ['l1','l2'],\n            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \n        },\n    \n        {\n            #K Nearest Neighbors\n            'n_neighbors': [1,2,3,4,5,6,7],\n            'weights': ['uniform', 'distance'],\n            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n        },\n\n        {\n            #SVC\n            #http:\/\/blog.hackerearth.com\/simple-tutorial-svm-parameter-tuning-python-r\n            'kernel': ['linear', 'poly', 'rbf'],\n            'C': [1,2,3,4,5],\n            'gamma': [.1, .25, .5, .75, 1.0],\n            'probability': [True],\n         },\n]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\ncv_split = ShuffleSplit(n_splits = 5, test_size = 0.2, train_size = 0.8, random_state = 42) \n# run model 10 times with 80\/20 split\n\nfor i, model in enumerate(models):\n\n    print('Model:', model[1].__class__.__name__)\n    best_clf = GridSearchCV(estimator = model[1], param_grid = params[i], cv = cv_split, scoring = 'roc_auc', verbose = True, n_jobs = -1)\n    best_clf.fit(X_train, y_train)\n    best_params = best_clf.best_params_\n    model[1].set_params(**best_params) \n    print('Parameters:', best_params)\n    print('CV Score:', best_clf.best_score_)\n    print(\"Validation score: \", best_clf.score(X_val, y_val))\n    print('='*60)\n    predictions = best_clf.predict(X_test)\n    output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n    output.to_csv('my_submission_tuned_model_' + model[0] + '.csv', index=False)\n         \n","6cbc5c71":"for voting in ['hard', 'soft']:    \n    vote_clf = VotingClassifier(estimators = models, voting = voting)\n    vote_cv = cross_validate(vote_clf, X_train, y_train, cv  = cv_split, verbose = True, n_jobs = -1, return_train_score = True)\n    vote_clf.fit(X_train, y_train)\n    vote_clf.score(X_val, y_val)\n    print('Voting (' + voting + ') with tuned models')\n    print('Training Score:', vote_cv['train_score'].mean())\n    print('Testing Score:', vote_cv['test_score'].mean())\n    print('Validation Score:', vote_clf.score(X_val, y_val))\n    predictions = vote_clf.predict(X_test)\n    output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n    output.to_csv('my_submission_tuned_voting_' + voting + '.csv', index=False)","2b268aeb":"Pandas cut vs qcut (Quantile-based discretization function)\n\nhttps:\/\/pbpython.com\/pandas-qcut-cut.html#:~:text=The%20major%20distinction%20is%20that,specifically%20define%20the%20bin%20edges.\n\nhttps:\/\/stackoverflow.com\/questions\/30211923\/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n\ncut gives you equal sized bins, with no guarantees on how many samples will be in each bin (that depends on data distribution). Some bins may even be empty.\nqcut gives you unequal ranged bins where the samples in each bin are nearly equal. A simple example is when you use 2 bins. The bins will be (min value, median] and (median, max value]. Since by definition, median divides your data into two equal segments.\n\nExamples:\nA grader gives 4 letter grades (A, B, C, F) to the class on an exam score of 0-100.\n\nScheme 1: Absoluate Grading: Everyone gets grade based on their score.\n\n(-1, 25] -> F,\n(25, 50] -> C,\n(50, 75] -> B,\n(75, 100] -> A\n\nScheme 2: Equal number of students get each grade. Since there are 4 groups, so we are looking for quartiles.\nTop 25% will get A, Bottom 25% will get F, and so on. \n\n\n","6b134d49":"### Overfitting vs Underfitting","f55e804e":"Now question is whether one should use cut or qcut for age (or fare)?\n\nI haven't been able to find a concrete answer. Different posts suggest to try both for feature engineering and pick up resulting feature that give higher correlation with dependent variable.\n\n","4b84d3fc":"### Combining Everything Together\n\n**Tuning + Ensemble**","a4c62896":"### Feature Analysis - Pclass & Fare","d8b0b469":"We can see that passengers traveling such that Parch > 0 have lower median value. There can be multiple ways of slicing and dicing the age. Let's try simple median first.","84f37257":"## Feature Analysis - Gender","a6907f85":"### Looking at Number of records, data types, and nulls (missing data)","d17cf798":"We can use the above bins by building a single feature such that:\nIf the fare is in the first bin, feature is 1, else 0.\n\nAnother approach is to use quantiles-based bins creating our own fare classes.\n\nSince there are three original passenger classes, we can use three bins or even four bins. However, for right answer, you can try both options and see which one gives better results.","90907978":"### Ready For Modeling","2c3be4fc":"Pandas has useful functions for starting the data analysis.\nThese tools are:\n* info()\n* describe()\n* plot()\n* rolling()\n* diff()\n* mean()\n* median()","8d4d5b11":"Smoothing using Bins here again, just like Age.","785d3f12":"> After leaving Southampton on 10 April 1912, Titanic called at Cherbourg in France and Queenstown (now Cobh) in Ireland, before heading west to New York.\n\n* Southampton \u2013 179 First Class, 247 Second Class, and 494 Third Class.\n* Cherbourg - 274 additional passengers were taken aboard \u2013 142 First Class, 30 Second Class, and 102 Third Class. Twenty-four passengers left.\n* Queenstown - 123 passengers boarded Titanic at Queenstown \u2013 three First Class, seven Second Class and 113 Third Class. 7 passengers left. \n\nTrivia: Last-ever picture of Titanic was taken by one of the departing passengers at Queenstown, Francis Browne.\n","261e452e":"There can be multiple ways of completing missing age values. Simplest will be to just ","d2712dcb":"### Read Training Data","8f2e6926":"### Model Fitting","fd7949f7":"Higher class had a higher rate of survival, whereas lower class has lowest rate of survival, making Pclass a desirable feature.","850a4b83":"# Exploratory Data Analysis","d1bba56d":"Data description:\n\n1. Survival - Survival (0 = No; 1 = Yes). Not included in test.csv file.\n2. Pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n3. Name - Name\n4. Sex - Sex\n5. Age - Age\n6. Sibsp - Number of Siblings\/Spouses Aboard\n7. Parch - Number of Parents\/Children Aboard\n8. Ticket - Ticket Number\n9. Fare - Passenger Fare\n10. Cabin - Cabin\n11. Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n","1a0ca9e1":"Plotting feature importances","74886d34":"Credits for this notebook:\n\nPrimary: https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n\nSecondary: https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions","df1d3200":"### Simple Modeling For Illustrative Purposes","cfcc4bc8":"# Data Input","11d84577":"74.2% of female passengers survived, whereas only 18.8% of men survived in the training data sample. This means that gender can be a useful feature for modeling.","23d14cc5":"People with zero Parch or SibSp values had a lower likelihood of surviving.","044e9eaf":"As we have seen with earlier with Pclass, lower class had high likelihood of perishing in the disaster. Same thing can be seen here based on classes created fare values.\n\nIn the end, which scheme to chose is just one of the things that need to be tried to see its effect on final results.\n","45816625":"### Ensembling of Models","080005c8":"# Helper Functions","1dca97fe":"The distribution of fare is more skewed than the distribution of age. Equal sized bins may have fewer bins with high number of samples and very low number of samples in other bins. ","d376a64a":"### Feature Analysis - Embarked","2fac3bc7":"Plotting decision tree","defa173e":"### Read Test Data","1c0a97f9":"### Feature Analysis - Name","088ec9fd":"**min_samples_split vs min_samples_leaf**\n1. \nFrom StackOverFlow:\nhttps:\/\/stackoverflow.com\/questions\/46480457\/difference-between-min-samples-split-and-min-samples-leaf-in-sklearn-decisiontre\n\nThe main difference between the two is that min_samples_leaf guarantees a minimum number of samples in a leaf, while min_samples_split can create arbitrary small leaves, though min_samples_split is more common in the literature.\n\nLet's first understand the distinction between a leaf (also called external node) and an internal node. An internal node will have further splits (also called children), while a leaf is by definition a node without any children (without any further splits).\n\nmin_samples_split specifies the minimum number of samples required to split an internal node, while min_samples_leaf specifies the minimum number of samples required to be at a leaf node.\n\nFor instance, if min_samples_split = 5, and there are 7 samples at an internal node, then the split is allowed. But let's say the split results in two leaves, one with 1 sample, and another with 6 samples. If min_samples_leaf = 2, then the split won't be allowed (even if the internal node has 7 samples) because one of the leaves resulted will have less then the minimum number of samples required to be at a leaf node.\n\nmin_samples_leaf guarantees a minimum number of samples in every leaf, no matter the value of min_samples_split.","71156cb4":"50% of passengers traveling alone didn't survive, as opposed to 30% of passengers traveling accompanied.","a5db7d4b":"We will use median age for each Passenger Class for data cleaning.","1f0b20a5":"### Simple Statistics using Pandas describe()","62625e11":"The plot shows that there is high survival rate for younger passengers, lower for adults in 15-35 years and 60+. Let's try putting this into bins.","2e4d3a52":"## Feature Analysis - Age","0700f895":"### Tuning - Grid Search and Cross Validation","89aa61e1":"### Coverting Categorical to Numeric \/ One Hot Encoding","af4f20bc":"Grid Search with Decision Tree's most powerful cousin (which trains multiple decision trees)","c8e4cd7e":"The age distribution among survivors and deceased is bit noisy with a peak in the middle. We can smoothen it by discretizing it into bins, and see if some patterns emerge.","dd2428d8":"## Feature Analysis - SibSp & Parch"}}