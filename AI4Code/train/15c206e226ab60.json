{"cell_type":{"4d8a8487":"code","ba64c736":"code","0ba776aa":"code","661f0a8d":"code","135adebc":"code","21a281a1":"code","c7918d05":"code","139bcb88":"code","4567784f":"code","3775aa6f":"code","e864b049":"code","640cc06b":"code","3e9ce3fb":"code","7b7e8055":"code","271eb1db":"code","8e49757f":"code","53bcd652":"markdown"},"source":{"4d8a8487":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ba64c736":"#the data comes from this website https:\/\/archive.ics.uci.edu\/ml\/datasets\/Airfoil+Self-Noise\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost.sklearn import XGBRegressor\n\nfrom sklearn.metrics import r2_score, explained_variance_score, mean_squared_error","0ba776aa":"cols = 'frequency attack_angle chord_length velocity thickness sound_level'.split()\ndf = pd.read_table('..\/input\/airfoil_self_noise.dat', header = None, names=cols)","661f0a8d":"df.info()","135adebc":"df.describe()","21a281a1":"X = df.iloc[:,0:5]\ny = df.iloc[:,5:6]","c7918d05":"X_train, X_test, y_train, y_test = train_test_split(X, y)","139bcb88":"#the pair plot shows that linear or ridge regression may not be the best choices here\nsns.pairplot(X)","4567784f":"#because I was learning and I wanted to show that how bad they performed, I ran the following two models\nreg = Ridge(alpha = 0.01).fit(X_train, y_train)\n\nprint(f'The training set score of the model is {round(reg.score(X_train, y_train), 2)}')\nprint(f'The test set score of the model is {round(reg.score(X_test, y_test), 2)}')","3775aa6f":"reg = LinearRegression().fit(X_train, y_train)\n\nprint(f'The training set score of the model is {round(reg.score(X_train, y_train), 2)}')\nprint(f'The test set score of the model is {round(reg.score(X_test, y_test), 2)}')","e864b049":"reg = RandomForestRegressor(n_estimators = 1000, max_features = 'log2').fit(X_train, y_train.values.ravel())\n\nprint(f'The training set score of the model is {round(reg.score(X_train, y_train), 2)}')\nprint(f'The test set score of the model is {round(reg.score(X_test, y_test), 2)}')","640cc06b":"reg = GradientBoostingRegressor(learning_rate = 0.1, n_estimators = 10000).fit(X_train, y_train.values.ravel())\n    \nprint(f'The training set score of the model is {round(reg.score(X_train, y_train), 2)}')\nprint(f'The test set score of the model is {round(reg.score(X_test, y_test), 2)}')","3e9ce3fb":"#this was much faster than GradientBoostingRegressor from SKLearn\nxgb_model = XGBRegressor(eta = 0.1, n_estimators = 1000).fit(X_train, y_train)\n\nprint(f'The training set score of the model is {round(xgb_model.score(X_train, y_train), 2)}')\nprint(f'The test set score of the model is {round(xgb_model.score(X_test, y_test), 2)}')","7b7e8055":"parameters_grid = {\n    'n_estimators': [100, 1000, 10000],\n    'learning_rate': [0.001, 0.1, 0.2]\n}\n\nreg = GridSearchCV(estimator = XGBRegressor(),\n                   param_grid = parameters_grid,\n                   cv = 10,\n                   n_jobs = -1)\n\nreg.fit(X_train, y_train)\n\n#best score of accuracy\nprint('Best score:', reg.best_score_)\n\n# View the best parameters for the model found using grid search\nprint('Best Number of Trees:',reg.best_estimator_.n_estimators) \nprint('Best Learning Rate:',reg.best_estimator_.learning_rate)","271eb1db":"xgb_model = XGBRegressor(learning_rate = reg.best_estimator_.learning_rate,\n                         n_estimators = reg.best_estimator_.n_estimators).fit(X_train, y_train)\n\ny_pred = xgb_model.predict(X_test)\n\nprint(f'The R2 Score is {r2_score(y_test, y_pred)}') #one of the best\nprint(f'The Explained Variance Score is {explained_variance_score(y_test, y_pred)}') #very good as well\nprint(f'The Mean Squared Error is {mean_squared_error(y_test, y_pred)}')\nprint(f'The Root Mean Squared Error is {np.sqrt(mean_squared_error(y_test, y_pred))}')","8e49757f":"g = sns.distplot(y_pred, hist = False)\ng = sns.distplot(y_test, hist = False)\nplt.show()","53bcd652":"After finishing [this book](https:\/\/www.amazon.com\/Introduction-Machine-Learning-Python-Scientists\/dp\/1449369413) called Introduction to Machine Learning with Python, I decided to use my knowledge for a Regression Problem. I carried out the steps as written in the textbook and also on my [Medium Blog](https:\/\/towardsdatascience.com\/9-steps-for-solving-data-science-problems-dc3c238cb58c).\n\nAfter going through the suggested workflow, I landed on XGBoost as the best algorithm for the present case. This is a very simple notebook and I am sure, there can be many better ways of doing the same thing."}}