{"cell_type":{"c6778414":"code","be8d5717":"code","aa084ed3":"code","ac7fbbe4":"code","9861ddfc":"code","55f5a66c":"code","9a0c4662":"code","9b1c0d24":"code","f6d86e44":"code","23ba6d32":"code","a73fb7c9":"code","ed11608e":"code","fc35c060":"code","936a3d90":"markdown","efbabdcc":"markdown","af971b89":"markdown","2191f9d6":"markdown","8a0232e6":"markdown","0902fb86":"markdown","54953c48":"markdown","83381fd2":"markdown","394c1d2f":"markdown","27269163":"markdown","62ec1542":"markdown","586e7192":"markdown","23bae4ee":"markdown"},"source":{"c6778414":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","be8d5717":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","aa084ed3":"test_data = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_data.head()","ac7fbbe4":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\n#\"Rate\" simply means the number of things per some other number. \nrate_women = sum(women)\/len(women)#\n\nprint(\"% of women who survived:\", rate_women*100)#A percentage is a rate per 100.","9861ddfc":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\",rate_men*100)","55f5a66c":"from sklearn.ensemble import RandomForestClassifier\n\n#Target Feature or Variable\ny = train_data[\"Survived\"]\n\n#independent features or variables\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n\n#Convert categorical variable into dummy\/indicator variables.\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\n","9a0c4662":"#Random Forest Classifier model from sklearn library\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","9b1c0d24":"len(model.estimators_)","f6d86e44":"# install dtreeviz library to this notebook \n! pip install dtreeviz","23ba6d32":"from sklearn import tree\nfrom matplotlib import pyplot as plt\nfrom dtreeviz.trees import *","a73fb7c9":"plt.figure(figsize=(20,20))\n_ = tree.plot_tree(model.estimators_[0], feature_names=X.columns, filled=True)","ed11608e":"#To check the maximum depth of the tree\nmodel.estimators_[0].tree_.max_depth","fc35c060":"viz = dtreeviz(model.estimators_[0], X, y, feature_names=X.columns, target_name=\"Target\",orientation='TD')\nviz","936a3d90":"Random Forests grows many classification trees\/Decision trees. To classify a data, put the data down each of the trees in the forest. Each tree gives a classification, and we say the tree \"votes\" for that class. The forest chooses the classification having the most votes (over all the trees in the forest).\n\nreference: \n1. [Decision trees example](http:\/\/www.butleranalytics.com\/decision-trees-simple-effective\/)\n2. [Random Forest](https:\/\/www.stat.berkeley.edu\/~breiman\/RandomForests\/cc_home.htm)\n\nBelow is the example of decision tree,random forest consists of multiple decision trees and each predict the result. Consider the scenario that you ask this question \"what kind of pet is right for you?\" to 10 of your friends each friend has different decision tree in their mind based on the information\/facts they know about you.Now you got answers from 10 friends,if most vote goes to fish and you propably buy it.\n\nHere we have hundereds of random decision trees with different combination of tress,now for each new data it pass the value down to these hundereds of decision tree and gets the vote to give the accurate value.","efbabdcc":"## Model-Random Forest","af971b89":"Decision Trees are stored in a list in the estimators_ attribute in the model. We can check the length of the list, which should be equal to n_estiamtors value.","2191f9d6":"### ******There are 100 trees in our random forest model because default value is 100 in sklearn!!!******","8a0232e6":"![Decision tree example](http:\/\/www.butleranalytics.com\/wp-content\/uploads\/DT-decision-tree.jpg)","0902fb86":"## Lets check how many trees our model has","54953c48":"# visulaizing model Decision Tree","83381fd2":"Here the single tree is displayed to get the idea how the tress are made and to get clear view of a tree rather than displaying 100 trees.Don't bother now about what are the values in the tree that are used for quality of the split. you can dig deeper in sklearn library documentation or some google searches","394c1d2f":"upto this step i followed the code given in the Titanic Tutorial for 30 Days of ML, Now we are going to explore the model here using dtreeviz library to better understand the working behind the model","27269163":"## Random Forest ","62ec1542":"##  visualize the single Decision Tree using dtreeviz with distribution","586e7192":"## Load Data","23bae4ee":"## Explore a pattern"}}