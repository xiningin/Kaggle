{"cell_type":{"149b2c29":"code","3c4d75a2":"code","2c8baf35":"code","f9575bc0":"code","a595f3c2":"code","55953ee1":"code","cb818698":"code","66d29d04":"code","1afb9c53":"code","b589386f":"code","9cf35e6a":"code","58905c3c":"code","c784ea95":"code","fda1f4c0":"markdown","a71314e4":"markdown","67c4a31b":"markdown","28f4f02d":"markdown","ab8230c4":"markdown"},"source":{"149b2c29":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport plotly.offline as po\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_absolute_error\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3c4d75a2":"train = pd.read_csv(\"..\/input\/learn-together\/train.csv\")\ntest = pd.read_csv(\"..\/input\/learn-together\/test.csv\")","2c8baf35":"train.head(10)","f9575bc0":"train.describe()","a595f3c2":"train.info()","55953ee1":"train['Cover_Type'].unique()","cb818698":"train.groupby('Cover_Type').size()","66d29d04":"array = train.values\ny = array[:,-1]\nX = array[:,0:55]\n\nvalidation_size = 0.40\nseed = 7\nX_train, X_validation, y_train, y_validation = model_selection.train_test_split(X, y, test_size=validation_size, random_state=seed)\narray.shape, y.shape, X.shape\nX_train.shape, X_validation.shape, y_train.shape, y_validation.shape","1afb9c53":"model = KNeighborsClassifier()\nmodel.fit(X_train, y_train)\npredicted_values = model.predict(X_validation)\nmae = mean_absolute_error(y_validation, predicted_values)\nmae\n\n\n","b589386f":"model = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\npredicted_values = model.predict(X_validation)\nmae = mean_absolute_error(y_validation, predicted_values)\nmae\n","9cf35e6a":"model = RandomForestClassifier()\nmodel.fit(X_train, y_train)\npredicted_values = model.predict(X_validation)\nmae = mean_absolute_error(y_validation, predicted_values)\nmae","58905c3c":"accuracy = []\nfor i in range(50, 300, 50):\n    model = RandomForestClassifier(n_estimators = i)\n    model.fit(X_train, y_train)\n    predicted_values = model.predict(X_validation)\n    mae = mean_absolute_error(y_validation, predicted_values)\n    accuracy.append(mae)\naccuracy\n    \n    \n","c784ea95":"def submission_file(test_data, test_preds):\n    output = pd.DataFrame({'Id': test_data.index+15121,\n                      'Cover_Type': test_preds})\n\n    output.to_csv('submission.csv', index=False)\n    \ndef feature_importances(model, X, y, figsize=(18, 6)):\n    model = model.fit(X, y)\n    \n    importances = pd.DataFrame({'Features': X.columns, \n                                'Importances': model.feature_importances_})\n    \n    importances.sort_values(by=['Importances'], axis='index', ascending=False, inplace=True)\n\n    fig = plt.figure(figsize=figsize)\n    sns.barplot(x='Features', y='Importances', data=importances)\n    print(model.feature_importances_)\n    plt.xticks(rotation='vertical')\n    plt.show()\n    return importances\n\n# create a dict that map soil type with rockness\n# 0=unknow 1=complex 2=rubbly, 3=stony, \n# 4=very stony, 5=extremely stony 6=extremely bouldery\nsoils = [\n    [7, 15, 8, 14, 16, 17,\n     19, 20, 21, 23], #unknow and complex \n    [3, 4, 5, 10, 11, 13],   # rubbly\n    [6, 12],    # stony\n    [2, 9, 18, 26],      # very stony\n    [1, 24, 25, 27, 28, 29, 30,\n     31, 32, 33, 34, 36, 37, 38, \n     39, 40, 22, 35], # extremely stony and bouldery\n]\n\nsoil_dict = dict()\nfor index, values in enumerate(soils):\n    for v in values:\n        soil_dict[v] = index\n        \n        \ndef soil(df, soil_dict=soil_dict):\n    df['Rocky'] =  sum(i * df['Soil_Type'+ str(i)] for i in range(1, 41))\n    df['Rocky'] = df['Rocky'].map(soil_dict) \n\n    return df\n\n\ndef select(importances, edge):\n    c = importances.Importances >= edge\n    cols = importances[c].Features.values\n    return cols\n\nX=train.copy()\nTARGET='Cover_Type'\ny=train[TARGET]\n\nX = soil(X)\ntest=soil(test)\n\n# drop label \nif TARGET in X.columns:\n    X.drop(TARGET, axis=1, inplace=True)\n    \nX.drop('Id', axis=1, inplace=True)\n\nmodel=RandomForestClassifier()\n\nimportances = feature_importances(model, X, y)   \ncol = select(importances, 0.003)\n\nX = X[col]\ntest=test[col]\n\n\nmodel = model.fit(X, y)\ny_pred = model.predict(test)\npredictions = [round(value) for value in y_pred]\n\nsubmission_file(test, predictions)","fda1f4c0":"<font color = blue> <h2> Generate train\/test split from the train csv<\/h2> <\/font>","a71314e4":"<font color = red><h3> Next steps<\/h3><\/font>\n* Work on feature selection\n* Drill down on Random Forest\n\n","67c4a31b":"<h2>My very first submission.  Comments, suggestions welcomed<\/h2>","28f4f02d":" <font color = blue> <h2>Run a few preliminary models for baseline thinking<\/h2>","ab8230c4":"<font color = blue> <h2>Import CSV and create the dataframe<\/h2><\/font>"}}