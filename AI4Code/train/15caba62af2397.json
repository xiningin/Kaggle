{"cell_type":{"6eb7838f":"code","c37dcc9b":"code","b5a9a244":"code","18e24889":"code","c63a778e":"code","1bb79237":"code","52dee7aa":"code","1ac38d55":"code","9e027999":"code","d6a991d4":"code","8535e6b5":"code","6f2b661c":"code","1dc63677":"code","b36d15e4":"code","d3d5f964":"code","9c25e655":"code","691bb2d6":"code","952a5aec":"code","67fa0335":"code","967dfa8b":"code","7b1c60a8":"code","6ac037df":"code","45a57037":"code","97609c4a":"code","73a8127b":"code","0a85d3ea":"code","aea355cd":"code","0f9fcaab":"code","eb29e4d5":"code","91e3090a":"markdown","6a2c8879":"markdown","93170e7b":"markdown","a9115534":"markdown","b2c82ad5":"markdown","bd8cca46":"markdown","0dc82939":"markdown","0299a6cc":"markdown","554e7f6b":"markdown","62a279a9":"markdown","98a360cd":"markdown","f7ce8437":"markdown","8bd1b97a":"markdown","d1e22ad2":"markdown","01c4833f":"markdown","61082b8e":"markdown","148f97a3":"markdown","15803993":"markdown","4562abc6":"markdown"},"source":{"6eb7838f":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\n\n%matplotlib inline","c37dcc9b":"import torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader","b5a9a244":"np.random.seed(1)\ntorch.manual_seed(1)","18e24889":"train_csv = '..\/input\/digit-recognizer\/train.csv'\ntest_csv = '..\/input\/digit-recognizer\/test.csv'\n\ntrain_df = pd.read_csv(train_csv)\ntest_df = pd.read_csv(test_csv)","c63a778e":"test_df[\"label\"] = [-1]*len(test_df)","1bb79237":"train_df.head()","52dee7aa":"test_df.head()","1ac38d55":"from sklearn.model_selection import KFold","9e027999":"train_df['fold'] = -1\n\nFOLDS = 5\n\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=1)\n\nfor i, (_, val_idx) in enumerate(kf.split(train_df)):\n    train_df.loc[val_idx, 'fold'] = i\n\ntrain_df.head(3)","d6a991d4":"def to_tensor(data):\n    # Utility function to convert data into PyTorch tensors\n    return [torch.FloatTensor(point) for point in data]\n\nclass MNISTDataset(Dataset):\n    def __init__(self, df, X_col, y_col):\n        \"\"\"\n            df: the pandas DataFrame to be referred\n            X_col: the columns representing the pixel values of the image.\n            y_col: the column representing the target label.\n        \"\"\"\n        self.features = (df[X_col].values\/255).reshape((-1,1,28,28)) # Reshaping images to form (batch_size, channels, height, width)\n        self.targets = df[y_col].values.reshape((-1,1)) \n        \n    def __len__(self):\n        return len(self.targets)\n    \n    def __getitem__(self, idx):\n        return to_tensor([self.features[idx], self.targets[idx]])","8535e6b5":"class ConvNet(torch.nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.relu_layer = torch.nn.ReLU()\n        self.conv1 = torch.nn.Conv2d(1, 32, 5)\n        self.conv2 = torch.nn.Conv2d(32, 64, 5)\n        self.max = torch.nn.MaxPool2d(2)\n        self.fc1 = torch.nn.Linear(4*4*64, 256)\n        self.fc2 = torch.nn.Linear(256, 128)\n        self.fc3 = torch.nn.Linear(128, 10)\n    \n    def forward(self, x, train=True):\n        x = self.conv1(x)\n        x = self.relu_layer(x)\n        x = self.max(x) # First Conv Block\n        \n        x = self.conv2(x)\n        x = self.relu_layer(x)\n        x = self.max(x) # Second Conv Block\n        \n        x = x.view(-1, 4*4*64) # Flattening\n        \n        x = self.fc1(x)\n        x = self.relu_layer(x)\n                \n        x = self.fc2(x)\n        x = self.relu_layer(x)\n        \n        x = self.fc3(x)\n        \n        if train:\n            return x\n        else:\n            return F.log_softmax(x, dim=1)","6f2b661c":"def cel(y_pred, y_true):\n    y_true = y_true.long().squeeze()\n    return torch.nn.CrossEntropyLoss()(y_pred, y_true)\n\ndef acc(y_pred, y_true):\n    y_true = y_true.long().squeeze()\n    y_pred = torch.argmax(y_pred, axis=1)\n    return (y_true == y_pred).float().sum()\/len(y_true)","1dc63677":"EPOCHS = 10\n\nall_train_losses = np.zeros((EPOCHS))\nall_valid_losses = np.zeros((EPOCHS))\n\nall_train_accs = np.zeros((EPOCHS))\nall_valid_accs = np.zeros((EPOCHS))","b36d15e4":"def train(fold):\n    print(\"Starting Training for FOLD {} ....\".format(fold))\n    \n    train_bs = 64\n    valid_bs = 32\n    \n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    \n    train_fold = train_df[train_df.fold != fold].reset_index(drop=True)\n    valid_fold = train_df[train_df.fold == fold].reset_index(drop=True)\n    \n    model = ConvNet()\n    model.to(device)\n    \n    X_col = list(train_df.columns[1:-1])\n    y_col = \"label\"\n        \n    train_set = MNISTDataset(train_fold, X_col, y_col)\n    valid_set = MNISTDataset(valid_fold, X_col, y_col)\n    \n    train_loader = DataLoader(train_set, batch_size=train_bs, shuffle=True)\n    valid_loader = DataLoader(valid_set, batch_size=valid_bs, shuffle=True)\n    \n    optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-2)\n    \n    train_losses, valid_losses = [], []\n    train_accs, valid_accs = [], []\n    \n    for epoch in range(EPOCHS):\n        model = model.train()\n        print(\"Epoch: {}\".format(epoch+1))\n        \n        batch = 0\n        batch_train_losses, batch_train_accs = [], []\n        for train_batch in train_loader:\n            train_X, train_y = train_batch\n            \n            train_X = train_X.to(device)\n            train_y = train_y.to(device)\n            \n            train_preds = model.forward(train_X, train=True)\n            train_loss = cel(train_preds, train_y)\n            train_acc = acc(train_preds, train_y)\n            \n            optimizer.zero_grad()\n            train_loss.backward()\n            \n            optimizer.step()\n            train_loss = np.round(train_loss.item(), 3)\n            train_acc = np.round(train_acc.item(), 3)\n            \n            batch += 1\n            log = batch % 10 == 0\n            \n            batch_train_losses.append(train_loss)\n            batch_train_accs.append(train_acc)\n            if log:\n                print(\">\", end=\"\")\n        \n        logs = \"\\nTrain Loss: {} || Train Acc: {}\"\n        print(logs.format(np.round(np.mean(batch_train_losses), 3), np.round(np.mean(batch_train_accs), 3)))\n        \n        train_losses.append(np.mean(batch_train_losses))\n        train_accs.append(np.mean(batch_train_accs))\n        \n        total_valid_points = 0\n        total_valid_loss = 0\n        total_valid_acc = 0\n        \n        with torch.no_grad():\n            for valid_batch in valid_loader:\n                valid_X, valid_y = valid_batch\n                \n                valid_X = valid_X.to(device)\n                valid_y = valid_y.to(device)\n                \n                valid_preds = model.forward(valid_X, train=True) # To use Loss function, we need raw outputs, hence train=True\n                valid_loss = cel(valid_preds, valid_y)\n                valid_acc = acc(valid_preds, valid_y)\n                \n                total_valid_points += 1\n                total_valid_loss += valid_loss.item()\n                total_valid_acc += valid_acc.item()\n        \n        valid_loss = np.round(total_valid_loss \/ total_valid_points, 3)\n        valid_acc = np.round(total_valid_acc \/ total_valid_points, 3)\n        \n        valid_losses.append(valid_loss)\n        valid_accs.append(valid_acc)\n        \n        logs = \"Valid Loss: {} || Valid Acc: {}\"\n        print(logs.format(valid_loss, valid_acc) + \"\\n\")\n    \n    MODEL_PATH = \"model_fold_{}.pt\"\n    torch.save(model.state_dict(), MODEL_PATH.format(fold))\n    \n    print(\"Ending Training for FOLD {}.... \\n\".format(fold))\n    \n    return train_losses, valid_losses, train_accs, valid_accs","d3d5f964":"for fold in range(FOLDS):\n    train_losses, valid_losses, train_accs, valid_accs = train(fold)\n    \n    all_train_losses += train_losses\n    all_valid_losses += valid_losses\n    \n    all_train_accs += train_accs\n    all_valid_accs += valid_accs","9c25e655":"all_train_losses \/= FOLDS\nall_valid_losses \/= FOLDS\n\nall_train_accs \/= FOLDS\nall_valid_accs \/= FOLDS","691bb2d6":"def modify(train_vals, valid_vals):\n    train_hue = np.zeros(len(train_vals)).reshape(-1,1)\n    valid_hue = np.ones(len(valid_vals)).reshape(-1,1)\n    \n    epo = np.arange(1, EPOCHS+1).reshape(-1,1)\n\n    train_vals = np.round(train_vals.reshape(-1,1), 3)\n    valid_vals = np.round(valid_vals.reshape(-1,1), 3)\n    \n    train = np.concatenate((train_vals, train_hue, epo), axis=1)\n    valid = np.concatenate((valid_vals, valid_hue, epo), axis=1)\n    \n    return np.concatenate((train, valid), axis=0)","952a5aec":"losses = modify(all_train_losses, all_valid_losses)\naccs = modify(all_train_accs, all_valid_accs)","67fa0335":"loss_df = pd.DataFrame(data=losses, columns=['loss', 'is_valid', 'epoch'])\nacc_df = pd.DataFrame(data=accs, columns=['acc', 'is_valid', 'epoch'])\n\nloss_df.to_csv('loss.csv', index=False)\nacc_df.to_csv('acc.csv', index=False)","967dfa8b":"plt.figure(figsize=(16,9))\nsns.lineplot(x='epoch', y='loss', data=loss_df, hue='is_valid', style='is_valid')\nplt.show()","7b1c60a8":"plt.figure(figsize=(16,9))\nsns.lineplot(x='epoch', y='acc', hue='is_valid', data=acc_df, style='is_valid')\nplt.show()","6ac037df":"def predict(fold):\n    \n    bs = 64\n    \n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    \n    MODEL_PATH = \"model_fold_{}.pt\".format(fold)\n    \n    model = ConvNet()\n    model.load_state_dict(torch.load(MODEL_PATH))\n    model.to(device)\n    \n    X_col = list(test_df.columns[:-1])\n    y_col = \"label\"\n    \n    test_set = MNISTDataset(test_df, X_col, y_col)\n    test_loader = tqdm(DataLoader(test_set, batch_size=bs, shuffle=False))\n    \n    test_preds = []\n    \n    with torch.no_grad():\n        for test_X, _ in test_loader:\n            test_X = test_X.to(device)\n            batch_test_preds = model.forward(test_X, train=False) # Outputs softmax values\n            test_preds.append(batch_test_preds.cpu().detach().numpy())\n            \n    test_preds = np.concatenate(test_preds, axis=0)\n    \n    return test_preds","45a57037":"preds = np.empty(shape=(FOLDS, test_df.shape[0], 10))","97609c4a":"for fold in range(FOLDS):\n    preds[fold] = predict(fold)","73a8127b":"subs = np.zeros((test_df.shape[0], 10))","0a85d3ea":"for fold in range(FOLDS):\n    subs += preds[fold]\n\nsubs \/= 5","aea355cd":"X_col = list(test_df.columns[:-1])\ny_col = \"label\"\n    \ntest_set = MNISTDataset(test_df, X_col, y_col)\ntest_loader = tqdm(DataLoader(test_set, batch_size=12, shuffle=False))\n\ntest_batch = next(iter(test_loader))[0]\ntest_X = test_batch.reshape(-1, 28, 28)[:12]\nfig, ax = plt.subplots(nrows=2, ncols=6, figsize=(12, 6))\n\nfor i, image in enumerate(test_X):\n    ax[i\/\/6][i%6].axis('off'); ax[i\/\/6][i%6].imshow(image, cmap='gray')\n    ax[i\/\/6][i%6].set_title(np.argmax(subs[i], axis=0), fontsize=20, color=\"red\")","0f9fcaab":"submission = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")\nsubmission[\"Label\"] = np.argmax(subs, axis=1)\n\nsubmission.head()","eb29e4d5":"submission.to_csv(\"submission.csv\", index=False)","91e3090a":"## Loading the data\n\nWe load the data given as .csv files by mapping them into pandas DataFrames. The data consists of the 784 (28\\*28) pixel values for each image, along with its target labels in the training data.","6a2c8879":"## Setting random seed values\nThis allows us our final outputs to be deterministic, as the random operations in the following sections will give the same outputs.","93170e7b":"## Defining PyTorch Datasets\n\nFor our PyTorch models to use the given data, we have to define a custom Dataset by extending the `Dataset` class from `torch.utils.data`. For more information on data loading with PyTorch, make sure you check out their [official docs](https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html).\n\n* The `len` function returns the length of the dataset.\n* The `getitem` function returns the data at a given `idx`, which consists of the 28x28 image and its label","a9115534":"# \ud83d\udd8a MNIST Digit Recognizer - PyTorch CNN","b2c82ad5":"## Defining ConvNet\n\nFor our PyTorch ConvNet, we make our custom model class, which extends the `torch.nn.Module` class. In the `init` function, we define the different components to be used in the ConvNet such as, convolutional layers, pooling layers, full-connected layers etc.\n\nFor this notebook, we use a basic ConvNet architecture, comprising of 2 Conv blocks, each comprising of a Convolutional Layer, a ReLU activation layer and MaxPooling layer. The 2 Conv blocks are then followed by 2 Fully Connected Layers, each followed with a ReLU layer.\n\n`Conv1` -> `ReLU` -> `MaxPool` -> `Conv2` -> `ReLU` -> `MaxPool` -> `FLATTEN` -> `Fc1` -> `ReLU` -> `Fc2` -> `ReLU`\n\n*Note: We output raw values since that is how the loss function `CrossEntropyLoss` expects its inputs to be.*","bd8cca46":"We take the mean of the predictions as the final prediction.","0dc82939":"### Training Loop\n\nWe loop through the different folds, keeping each of them as the validation fold.","0299a6cc":"This is a utility function I made to manipulate the training and validation data into a single numpy array, which I then use to define a pandas DataFrame and plot graphs with Seaborn.","554e7f6b":"## Visualizing Predictions","62a279a9":"## Further ideas\n\n* Data augmentation\n* Transfer Learning","98a360cd":"## Defining Loss and Metric Functions\n\nWe use the `CrossEntropyLoss` function for calculating loss, and use basic **Accuracy** for metrics.","f7ce8437":"## Cross Validation Setup\n\nFor better generalization of the model, and better predictions, we generally use some sort of cross validation to test the model before allowing it to make the final test predictions.\n\nThis method gives us a better idea of how our model is performing on unseen data and helps evaluate if our model generalizes good enough.\n\nIn this notebook, we'll be using a simple implementation of K-Fold Cross Valdation, inspired from Abhishek Thakur's [notebook](https:\/\/www.kaggle.com\/abhishek\/melanoma-detection-with-pytorch), using the sklearn library.\n\nTo understand Cross Validation, I would recommend this video by [StatQuest](https:\/\/www.youtube.com\/watch?v=fSytzGwwBVw) on Youtube. If you have further doubts regarding the implementation shown below, feel free to ask in the comments section.","8bd1b97a":"## Introduction\nThis notebook has been made as a guide for beginners starting off with Convolutional Neural Networks aka CNNs. In the following sections, I will demonstrate how to make a basic CNN using PyTorch, which can recognize hand-written digits taken from the well-known MNIST dataset. The dataset consists of 28x28 grayscale images, each with a label as the digit (0-9).\n\n### References\nI would like to thank [Abhishek Thakur](https:\/\/www.kaggle.com\/abhishek) for his [notebook](https:\/\/www.kaggle.com\/abhishek\/melanoma-detection-with-pytorch) on SIIM Melanoma Classification, allowing me implement a modular way of Cross Validation. I also would lke to thank [Tarun Paparaju](https:\/\/www.kaggle.com\/tarunpaparaju) for his [notebook](https:\/\/www.kaggle.com\/tarunpaparaju\/mnist-competition-pytorch-nn) on the same topic, using PyTorch and implementing a Vanilla Neural Network.","d1e22ad2":"## Submission\n\nWe use the sample submission csv to replace our final predictions in the `Label` column of the DataFrame, which we then save as the submission.csv. ","01c4833f":"## Training\n\nWe define a modular function for training, which takes in the fold number which is to be taken as the **validation fold**. The functionality of the function is as follows:\n\n* We define variables for training and validation batch sizes, `train_bs` and `valid_bs` respectively.\n* We define the device on which our model will be training, either CPU or GPU.\n* We then initate an object of our `ConvNet` class and move the model to the device.\n* We define the `X_col` and `y_col` variables to the needed column names, which are then used to to define our training and validation datasets.\n* We then use the datasets defined to initiate DataLoaders, which act as iterables, providing data in batches and shuffled.\n* We define the optimizer, which in this case is the `Adam` optimizer, along with a learning rate.\n* We then define some empty lists to store intermediate values of losses and accuracies throughout the training epochs.\n* We loop through the number of epochs and for each epoch, we loop through batches of data provided by the DataLoaders.\n* For each batch of data, we calculate the loss and accuracy.\n* We then reset the gradients, using the `zero_grad()` function. If we don't do this, the gradients keep accumulating, giving us wrong results.\n* After resetting the gradients, we populate the gradient values using the `backward()` function on the loss. This allows us to calculate the gradients for the loss, wrt to the variables used to calculate it. For more information, check out the official PyTorch `autograd` [docs](https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/autograd_tutorial.html).\n* We do a similar loop for validation, the **only difference** is that, it will be done in the scope of `torch.no_grad()` as it prevents the gradients being calculated, which is what we need to prevent while validating out model.\n* We conclude the function by saving the model, which is then used for predictions later.","61082b8e":"## Inference\n\nWe define a similar function to training for predictions, where the parameter `fold` tells which model to use to predict from the saved models.","148f97a3":"We basically take the mean of the losses and accuracies from different folds of validation and training.","15803993":"## Importing libraries","4562abc6":"In the following cell, we define global variables and lists to store the number of epochs, and the losses and accuracies for each fold of training."}}