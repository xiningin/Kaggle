{"cell_type":{"a62c4ab4":"code","a7642563":"code","971bf34b":"code","b4eaba92":"code","e18379f8":"code","0418b6cc":"code","c57a2f92":"code","88c12048":"code","6e9bcb3a":"code","768791a9":"code","c9fd91f2":"code","af70edad":"code","3aff8951":"code","8b5b5aa6":"code","91cdcd89":"code","962073ae":"code","be521da2":"code","a87f9835":"code","cd8ca8d0":"code","6ab79c68":"code","964618cc":"code","b12af4a2":"code","cc0a2788":"code","a770058b":"code","6910b2d0":"code","f2087126":"code","2f50e8ae":"code","f97a179d":"code","ac1f02f4":"code","a45fbaa1":"code","f2e0bef9":"code","a3215c84":"code","c8df6fed":"code","d80001cf":"code","ee37be85":"code","85d9289d":"code","f095a48d":"code","f6ead553":"code","7c961957":"code","06b69785":"code","fdde73fa":"code","b1ab445d":"code","5d70338f":"code","02c08323":"code","4aed2924":"code","2609ef63":"code","9ccde5d9":"code","8d739a17":"code","222a4673":"code","63b8d110":"markdown","61434358":"markdown","5d0bd86c":"markdown","7da90ef3":"markdown","5e117793":"markdown","aed79f6f":"markdown","d8633484":"markdown","5ea4dd01":"markdown","1d6908a4":"markdown","11972b48":"markdown","ea17c77a":"markdown","f09f399f":"markdown","dcc55be2":"markdown","b377aaff":"markdown","84b9ee1a":"markdown","80edd273":"markdown","ad53a98e":"markdown","de4d138e":"markdown","35cb66a6":"markdown","7e8f5d34":"markdown","cce9d3c2":"markdown","83559562":"markdown","364dc08f":"markdown","7f1d10af":"markdown"},"source":{"a62c4ab4":"import json\nimport pandas as pd\nimport numpy as np\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport re, string\nfrom nltk.corpus import stopwords\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau \n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","a7642563":"df_News = pd.read_json('..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json', lines=True)\ndf_2 = pd.read_json('..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json', lines=True)\ndf_News.head()","971bf34b":"df_News = pd.concat([df_News, df_2], ignore_index=True)\ndf_News.shape","b4eaba92":"df_News.info()","e18379f8":"# looking at some sarcastic news\ndf_News[df_News.is_sarcastic == 1].head(5)","0418b6cc":"# looking at some legitimate news\ndf_News[df_News.is_sarcastic == 0].head(5)","c57a2f92":"df_News.is_sarcastic.value_counts()","88c12048":"sns.countplot(df_News.is_sarcastic)","6e9bcb3a":"wordcloud = WordCloud(background_color='black',\n                    stopwords = STOPWORDS,\n                    max_words = 100,\n                    random_state = 101, \n                    width=1800, \n                    height=1000)\nwordcloud.generate(str(df_News['headline']))\nplt.imshow(wordcloud)","768791a9":"df_News['headline_len'] = df_News.headline.apply(lambda x: len(x.split()))","c9fd91f2":"sarcastic = df_News[df_News.is_sarcastic == 1]\nlegit = df_News[df_News.is_sarcastic == 0]","af70edad":"plt.figure(figsize=(8,5))\nsns.distplot(sarcastic.headline_len, hist= True, label= 'Sarcastic')\nsns.distplot(legit.headline_len, hist= True, label= 'legitimate')\nplt.legend()\nplt.title('News Headline Length Distribution by Class', fontsize = 10)\nplt.show()","3aff8951":"df_News = df_News.drop(columns=['article_link'])","8b5b5aa6":"lem = WordNetLemmatizer()\nstop_words = set(stopwords.words(\"english\"))\npunctuations = string.punctuation","91cdcd89":"def clean_text(news):\n    \"\"\"\n    This function receives headlines sentence and returns clean sentence\n    \"\"\"\n    news = news.lower()\n    news = re.sub(\"\\\\n\", \"\", news)\n    #news = re.sub(\"\\W+\", \" \", news)\n    \n    #Split the sentences into words\n    words = list(news.split())\n    \n    words = [lem.lemmatize(word, \"v\") for word in words]\n    words = [w for w in words if w not in punctuations]\n    #words = [w for w in words if w not in stop_words]\n    #words = [''.join(x for x in w if x.isalpha()) for w in words]\n\n    clean_sen = \" \".join(words)\n    \n    return clean_sen","962073ae":"df_News['news_headline'] = df_News.headline.apply(lambda news: clean_text(news)) \ndf_News.head()","be521da2":"df_News.groupby(['is_sarcastic']).headline_len.mean()","a87f9835":"df_News.groupby(['is_sarcastic']).headline_len.max()","cd8ca8d0":"headlines = df_News['news_headline']\nlabels = df_News['is_sarcastic'] ","6ab79c68":"train_sentences, test_sentences, train_labels, test_labels = train_test_split(headlines, labels, test_size=0.2, stratify=labels, random_state=42)","964618cc":"train_labels.value_counts()","b12af4a2":"#Defining Hyperparameters to be used\n\nmax_words = 30000     # how many unique words to use (i.e num rows in embedding vector)\nmax_len = 70       # max number of words in a headline to use\noov_token = '00_V'    # for the words which are not in training samples\npadding_type = 'post'   # padding type\ntrunc_type = 'post'    # truncation for headlines longer than max length\nembed_size = 100    # how big is each word vector","cc0a2788":"tokenizer = Tokenizer(num_words=max_words, oov_token=oov_token)\ntokenizer.fit_on_texts(train_sentences)\n\nword_index = tokenizer.word_index","a770058b":"train_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_sequences = pad_sequences(train_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)\ntest_sequences = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)","6910b2d0":"train_sequences","f2087126":"from tensorflow.keras.callbacks import ReduceLROnPlateau \n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(max_words, embed_size, input_length=max_len),\n    tf.keras.layers.GlobalMaxPooling1D(),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n\nrlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","2f50e8ae":"history = model.fit(train_sequences, train_labels, batch_size=32, epochs=5, \n                    validation_data=(test_sequences, test_labels), \n                    callbacks=[rlrp] ,verbose=1)","f97a179d":"score = model.evaluate(test_sequences, test_labels)\nprint('Test Loss: ', score[0])\nprint('Test Accuracy', score[1])\n\n\n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","ac1f02f4":"model_rnn = tf.keras.Sequential([\n    tf.keras.layers.Embedding(max_words, embed_size, input_length=max_len),\n    tf.keras.layers.SpatialDropout1D(0.2),\n    tf.keras.layers.SimpleRNN(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),\n    tf.keras.layers.GlobalMaxPooling1D(),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n\nrlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\nmodel_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel_rnn.summary()","a45fbaa1":"history_rnn = model_rnn.fit(train_sequences, train_labels, batch_size=32, epochs=5, \n                    validation_data=(test_sequences, test_labels), \n                    callbacks=[rlrp] ,verbose=1)","f2e0bef9":"score = model_rnn.evaluate(test_sequences, test_labels)\nprint('Test Loss: ', score[0])\nprint('Test Accuracy', score[1])\n\n\n# list all data in history\nprint(history_rnn.history.keys())\n# summarize history for accuracy\nplt.plot(history_rnn.history['accuracy'])\nplt.plot(history_rnn.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_rnn.history['loss'])\nplt.plot(history_rnn.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","a3215c84":"from tensorflow.keras.callbacks import ReduceLROnPlateau \n\nmodel_lstm = tf.keras.Sequential([\n    tf.keras.layers.Embedding(max_words, embed_size, input_length=max_len),\n    tf.keras.layers.SpatialDropout1D(0.2),\n    tf.keras.layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),\n    tf.keras.layers.GlobalMaxPooling1D(),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n\nrlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\nmodel_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel_lstm.summary()","c8df6fed":"history_lstm = model_lstm.fit(train_sequences, train_labels, batch_size=32, epochs=5, \n                    validation_data=(test_sequences, test_labels), \n                    callbacks=[rlrp] ,verbose=1)","d80001cf":"score = model_lstm.evaluate(test_sequences, test_labels)\nprint('Test Loss: ', score[0])\nprint('Test Accuracy', score[1])\n\n\n# list all data in history\nprint(history_lstm.history.keys())\n# summarize history for accuracy\nplt.plot(history_lstm.history['accuracy'])\nplt.plot(history_lstm.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_lstm.history['loss'])\nplt.plot(history_lstm.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","ee37be85":"from tensorflow.keras.callbacks import ReduceLROnPlateau \n\nmodel_lstm_avg = tf.keras.Sequential([\n    tf.keras.layers.Embedding(max_words, embed_size, input_length=max_len),\n    tf.keras.layers.SpatialDropout1D(0.2),\n    tf.keras.layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n\nrlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\nmodel_lstm_avg.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel_lstm_avg.summary()","85d9289d":"history_lstm_avg = model_lstm_avg.fit(train_sequences, train_labels, batch_size=32, epochs=5, \n                    validation_data=(test_sequences, test_labels), \n                    callbacks=[rlrp] ,verbose=1)","f095a48d":"score = model_lstm_avg.evaluate(test_sequences, test_labels)\nprint('Test Loss: ', score[0])\nprint('Test Accuracy', score[1])\n\n\n# list all data in history\nprint(history_lstm_avg.history.keys())\n# summarize history for accuracy\nplt.plot(history_lstm_avg.history['accuracy'])\nplt.plot(history_lstm_avg.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_lstm_avg.history['loss'])\nplt.plot(history_lstm_avg.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","f6ead553":"from tensorflow.keras.callbacks import ReduceLROnPlateau \n\nmodel_lstm1 = tf.keras.Sequential([\n    tf.keras.layers.Embedding(max_words, embed_size, input_length=max_len),\n    tf.keras.layers.SpatialDropout1D(0.2),\n    tf.keras.layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),\n    tf.keras.layers.GlobalMaxPooling1D(),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n\nrlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\nmodel_lstm1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel_lstm1.summary()","7c961957":"history_lstm1 = model_lstm1.fit(train_sequences, train_labels, batch_size=32, epochs=5, \n                    validation_data=(test_sequences, test_labels), \n                    callbacks=[rlrp] ,verbose=1)","06b69785":"score = model_lstm1.evaluate(test_sequences, test_labels)\nprint('Test Loss: ', score[0])\nprint('Test Accuracy', score[1])\n\n\n# list all data in history\nprint(history_lstm1.history.keys())\n# summarize history for accuracy\nplt.plot(history_lstm1.history['accuracy'])\nplt.plot(history_lstm1.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_lstm1.history['loss'])\nplt.plot(history_lstm1.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","fdde73fa":"from tensorflow.keras.callbacks import ReduceLROnPlateau \n\nmodel_st_lstm = tf.keras.Sequential([\n    tf.keras.layers.Embedding(max_words, embed_size, input_length=max_len),\n    tf.keras.layers.SpatialDropout1D(0.2),\n    tf.keras.layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),\n    tf.keras.layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),\n    tf.keras.layers.GlobalMaxPooling1D(),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n\nrlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\nmodel_st_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel_st_lstm.summary()","b1ab445d":"history_st_lstm = model_st_lstm.fit(train_sequences, train_labels, batch_size=32, epochs=5, \n                    validation_data=(test_sequences, test_labels), \n                    callbacks=[rlrp] ,verbose=1)","5d70338f":"score = model_st_lstm.evaluate(test_sequences, test_labels)\nprint('Test Loss: ', score[0])\nprint('Test Accuracy', score[1])\n\n\n# list all data in history\nprint(history_st_lstm.history.keys())\n# summarize history for accuracy\nplt.plot(history_st_lstm.history['accuracy'])\nplt.plot(history_st_lstm.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_st_lstm.history['loss'])\nplt.plot(history_st_lstm.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","02c08323":"model_gru = tf.keras.Sequential([\n    tf.keras.layers.Embedding(max_words, embed_size, input_length=max_len),\n    tf.keras.layers.GRU(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),\n    tf.keras.layers.GlobalMaxPooling1D(),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n\nrlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\nmodel_gru.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel_gru.summary()","4aed2924":"history_gru = model_gru.fit(train_sequences, train_labels, batch_size=32, epochs=5, \n                    validation_data=(test_sequences, test_labels), \n                    callbacks=[rlrp] ,verbose=1)","2609ef63":"score = model_gru.evaluate(test_sequences, test_labels)\nprint('Test Loss: ', score[0])\nprint('Test Accuracy', score[1])\n\n\n# list all data in history\nprint(history_gru.history.keys())\n# summarize history for accuracy\nplt.plot(history_gru.history['accuracy'])\nplt.plot(history_gru.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_gru.history['loss'])\nplt.plot(history_gru.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","9ccde5d9":"from tensorflow.keras.callbacks import ReduceLROnPlateau \n\nmodel_bidir = tf.keras.Sequential([\n    tf.keras.layers.Embedding(max_words, embed_size, input_length=max_len),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n    tf.keras.layers.GlobalMaxPool1D(),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n\nrlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\nmodel_bidir.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel_bidir.summary()","8d739a17":"history_bidir = model_bidir.fit(train_sequences, train_labels, batch_size=32, epochs=5, \n                    validation_data=(test_sequences, test_labels), \n                    callbacks=[rlrp] ,verbose=1)","222a4673":"score = model_bidir.evaluate(test_sequences, test_labels)\nprint('Test Loss: ', score[0])\nprint('Test Accuracy', score[1])\n\n\n# list all data in history\nprint(history_bidir.history.keys())\n# summarize history for accuracy\nplt.plot(history_bidir.history['accuracy'])\nplt.plot(history_bidir.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_bidir.history['loss'])\nplt.plot(history_bidir.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","63b8d110":"### Please Upvote the Kernel if you liked it. ","61434358":"### News Headline length Distribution","5d0bd86c":"### Target Column Distribution","7da90ef3":"### 3. LSTM (Long Short Term Memory) with GlobalMaxPooling & SpatialDropout","5e117793":"### Converting News Headlines into Sequences of tokens","aed79f6f":"## Abstract\n\n### Can you identify Sarcastic sentences?\n### Can you distinguish between Fake news and Legitimate news?\n\nSince news headlines are written by professionals in a formal manner, there are no spelling mistakes and informal usage. This reduces the sparsity and also increases the chance of finding pre-trained embeddings.\n\nFurthermore, since the sole purpose of TheOnion is to publish sarcastic news, we get high-quality labels with much less noise as compared to Twitter datasets.\n\n### Dataset\n\nEach record consists of three attributes:\n\nis_sarcastic: 1 if the record is sarcastic otherwise 0\n\nheadline: the headline of the news article\n\narticle_link: link to the original news article. Useful in collecting supplementary data\n\n### Deep Learning Framework\nI will be using Tensorflow to implement the Deep Learning Models for this Project.","d8633484":"### Data Cleaning & Pre-processing","5ea4dd01":"### Stratified Split \n\nI am using Stratified split to sample approx equal number of instances for training for both the categories of our Target.","1d6908a4":"I will be doing just some basic Pre-processing, as these are News Headlines which are written by Professionals in a formal way, so to not remove anything which can help with context, I will only remove punctuations and apply lemmatization","11972b48":"### Joining the two Datasets","ea17c77a":"### Conclusion\n\nThe best model is using LSTM and Bidirectional along with GlobalMaxPooling and Spatial Dropout. \n\nBest test Accuracy - 96.71 %\n","f09f399f":"### Stacked LSTM","dcc55be2":"### Predictive Modeling\nOur aim is to build a binary Classification model which given a sequence of text, can classify it as Sarcastic or not, or Fake news or real news.\n\n### A. I will try below different models and see which works best.\n1. Neural Network with Embedding\n2. RNN(Recurrent Neural Network)\n3. LSTM(Long-short term Memory) with GlobalAveragePooling\n4. LSTM with GlobalMaxPooling\n5. Stacked LSTM\n6. Bidirectional LSTM\n7. GRU(Gated Recurrent Unit)\n8. Stacked Bidirectional LSTM\n9. Best Model from above with pre-trained Embeddings","b377aaff":"### 7. GRU (Gated Recurrent Unit","84b9ee1a":"### 5. LSTM with only one FC Dense Layer","80edd273":"### Exploratory Data Analysis","ad53a98e":"On an Average most of the headlines have same length, in some cases, Sarcastic headlines are longer. ","de4d138e":"### 2. RNN (Recurrent Neural Network)","35cb66a6":"### Bidirectional LSTM","7e8f5d34":"### 4. LSTM with GlobalAveragePool","cce9d3c2":"### 1. Neural Network with Embedding","83559562":"### Import the libraries","364dc08f":"### Loading the Dataset","7f1d10af":"The dataset appears to be balanced for Sarcastic and legitimate news"}}