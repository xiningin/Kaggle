{"cell_type":{"9b7b7758":"code","58246cb5":"code","2787d0fb":"code","27451728":"code","bbeec655":"code","82e0ea97":"code","a9b40dc2":"code","45322724":"code","6d07d2fb":"code","1b0c2b45":"code","6736b980":"code","408af5e3":"code","cd4403bd":"code","24bba822":"code","1ef30f29":"code","fbd6b931":"code","5823e0f6":"code","3f056a5b":"code","f250f258":"code","0b777317":"code","1dbb8a72":"code","ba069ce0":"code","285c5b27":"code","9fcb604d":"code","360d4b5e":"code","e41971ff":"code","aa2b59ac":"code","207f1222":"code","161ff77c":"code","22d57d82":"code","a81a1092":"code","035987d8":"code","f5e1ed3e":"code","090f6acf":"code","85c321ff":"code","8590d1c9":"code","b470a219":"code","812e01fc":"code","5440ff45":"code","0c9f80d3":"code","6d7e6d76":"code","23597202":"code","e0ddc4a5":"code","e327b616":"code","d29772f9":"code","6690b321":"code","518eb9e2":"code","14f32fa7":"code","154d254b":"code","5a9ddfed":"code","f66931bb":"code","3f58fc3a":"code","b75c6106":"code","233e22af":"code","69e2fcc4":"code","cc7fc84b":"code","93df9fb1":"code","0faa7cf6":"code","6b843490":"code","10c78c4c":"code","78938455":"markdown","f66e8bf5":"markdown","f8c9a135":"markdown","70d390ac":"markdown","81d7371f":"markdown","1d5740b3":"markdown","8a28e915":"markdown","b25a5cc6":"markdown","a4f2171c":"markdown","93752386":"markdown","160401aa":"markdown","99fd94fc":"markdown","dae14917":"markdown","290ad7b2":"markdown","9a8a110c":"markdown","0737b457":"markdown","a6e94ed9":"markdown","b2401836":"markdown","edfd260a":"markdown","dbdca854":"markdown","3723c33a":"markdown","c1f68000":"markdown","bb1c46c7":"markdown","6e0eee14":"markdown","7feb63d7":"markdown","c89f04a0":"markdown","ea7febca":"markdown","57fbbe7f":"markdown","bd60d097":"markdown","d626d3fa":"markdown","192e8d82":"markdown","2da5c415":"markdown","f6e3bc43":"markdown","9ba0e593":"markdown","31f1466a":"markdown","a2073711":"markdown","2f82c89b":"markdown","b51f52b2":"markdown","118aa08b":"markdown","71d37358":"markdown","5c2c0d6d":"markdown","73b3c039":"markdown","7923ce0f":"markdown","eb88b4c9":"markdown","87a0b773":"markdown"},"source":{"9b7b7758":"!cp -r ..\/input\/tweetcovid19\/* .\/","58246cb5":"#Import the necessary libraries\nimport pandas as pd\npd.options.display.max_columns = 500\npd.options.display.max_rows = 500\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\nimport warnings \nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport keras\nfrom keras.models import Model\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","2787d0fb":"df = pd.read_csv('data\/covid_19_tweets.CSV')","27451728":"df.head()","bbeec655":"df.drop(['status_id', 'user_id', 'screen_name'], axis = 1, inplace = True)","82e0ea97":"num_tweets = len(df)","a9b40dc2":"df.isnull().sum()\/num_tweets * 100","45322724":"missing_cols = list(df.columns[(df.isnull().sum()\/num_tweets * 100) > 85.0])\n\ndf.drop(missing_cols, axis = 1, inplace = True)","6d07d2fb":"\nfrom iso639 import languages\n\ndef get_language(x):\n    try:\n        return languages.get(alpha2=x).name \n    except KeyError:\n        return x","1b0c2b45":"df['language'] = df['lang'].apply(lambda x: get_language(x))","6736b980":"df['language'].value_counts()[:10]","408af5e3":"df['language'] = df['language'].str.replace('und','Undefined')","cd4403bd":"plt.figure(figsize = (15,7))\nsns.barplot(x = df['language'].value_counts()[:10].index , y = df['language'].value_counts()[:10]\/num_tweets*100)\nplt.xlabel('Language', fontsize = 20)\nplt.ylabel('Percentage of Tweets', fontsize = 20)\nplt.xticks(fontsize = 15)\nplt.title('Top Ten Languages with most Tweets', fontsize=20)\nplt.show()","24bba822":"df_eng = df[df['language'] == 'English']","1ef30f29":"df_eng['language'].value_counts()","fbd6b931":"df_eng['text'][1128]","5823e0f6":"from clean_text import CleanText \nclean = CleanText()","3f056a5b":"df_eng['text_clean'] = clean.clean(df_eng['text']) #clean() removes urls, emoticons and hashtags","f250f258":"df_eng['text_clean'][1128]","0b777317":"df_eng['text_clean'] = df_eng['text_clean'].apply(lambda x: clean.tokenize(x)) #remove punctuations, stopwords, lemmatize and splits the sentences into tokens","1dbb8a72":"df_eng['text_clean'][1128]","ba069ce0":"#Saving the dataframe as a pickle file to resume where I left off incase the kernel crashes or if I have to continue some other day\ndf_eng.to_pickle('pickle_files\/tweets_eng.pkl') #Also reading and writing pickle files are much faster than csv","285c5b27":"df_eng = pd.read_pickle('pickle_files\/tweets_eng.pkl')","9fcb604d":"docs = df_eng['text_clean']\n\n#tokenizer\nt = Tokenizer()\nt.fit_on_texts(docs)\nvocab_size = len(t.word_index) + 1\n\n#encode the documents\nencoded_docs = t.texts_to_sequences(docs)\n\n#pad docs to max length\npadded_docs = pad_sequences(encoded_docs, maxlen = 22, padding = 'post') ","360d4b5e":"# Loading the classifier \nclassifier = keras.models.load_model('Models\/sentiment_classifier4.h5') #Negative: 0, Neutral: 1, Postive: 2","e41971ff":"labels_categorical = classifier.predict(padded_docs) # Predicting the Sentiments of the Covid-19 tweets","aa2b59ac":"labels_categorical[:10] #Output of each class by the softmax function","207f1222":"np.argmax(labels_categorical[:10], axis = 1) #np.argmax to get labels of the classes Negative: 0, Neutral: 1, Postive: 2","161ff77c":"df_eng['labels'] = np.argmax(labels_categorical, axis = 1)","22d57d82":"df_eng.to_pickle('pickle_files\/final_df.pkl') ","a81a1092":"df_eng = pd.read_pickle('pickle_files\/final_df.pkl')","035987d8":"def label_to_sentiment(label):\n    if label == 0:\n        return 'Negative'\n    elif label == 1:\n        return 'Neutral'\n    else:\n        return 'Positive'","f5e1ed3e":"df_eng['sentiment'] = df_eng['labels'].apply(lambda x: label_to_sentiment(x))","090f6acf":"pd.set_option('max_colwidth', 200)\ndf_eng[['text','sentiment']].iloc[368:373] #Let's check some random tweets to see if the predicted sentiments make sense","85c321ff":"plt.figure(figsize = (15,7))\nsns.barplot(x = df_eng['sentiment'].value_counts().index, y = df_eng['sentiment'].value_counts()\/len(df_eng)*100)\nplt.xlabel('Sentiment', fontsize = 20)\nplt.ylabel('Percentage of Tweets(%)', fontsize = 20)\nplt.xticks(fontsize = 15)\nplt.title('Distribution of Tweets based on Sentiment', fontsize = 20)\nplt.show()","8590d1c9":"from wordcloud import WordCloud\ndef plot_wordcloud(data):\n    words = []\n    for sent in data:\n        for word in sent:\n            words.append(word) \n    words = pd.Series(words).str.cat(sep=' ')\n    wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(words)\n    plt.figure(figsize=(12,10))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()","b470a219":"plot_wordcloud(df_eng['text_clean'][df_eng['sentiment'] == 'Positive'])","812e01fc":"plot_wordcloud(df_eng['text_clean'][df_eng['sentiment'] == 'Negative'])","5440ff45":"import nltk","0c9f80d3":"import re\ndef extract_hashtag(text):\n    hashtags=[]\n    for i in text:\n        ht=re.findall(r'#(\\w+)',i)\n        hashtags.append(ht)\n    return hashtags","6d7e6d76":"all_hashtags=extract_hashtag(df_eng.text)\ndef df_hashtag(sentiment_label):\n    hashtags=extract_hashtag(df_eng.text[df_eng['sentiment']==sentiment_label])\n    ht_fredist=nltk.FreqDist(sum(hashtags,[]))\n    df_ht=pd.DataFrame({'Hashtag':list(ht_fredist.keys()),'Count':list(ht_fredist.values())})\n    return df_ht","23597202":"#Hashtags dataframes\nht_neg_df=df_hashtag('Negative')\nht_neu_df=df_hashtag('Neutral')\nht_pos_df=df_hashtag('Positive')","e0ddc4a5":"ht_neg_df.to_pickle('ht_neg_df.pkl')\nht_neu_df.to_pickle('ht_neu_df.pkl')\nht_pos_df.to_pickle('ht_pos_df.pkl')","e327b616":"ht_neg_df = pd.read_pickle('pickle_files\/ht_neg_df.pkl')\nht_neu_df = pd.read_pickle('pickle_files\/ht_neu_df.pkl')\nht_pos_df = pd.read_pickle('pickle_files\/ht_pos_df.pkl')","d29772f9":"def plot_hashtag(df,title):\n    data=df.nlargest(columns=\"Count\",n=20)\n    plt.figure(figsize=(16,5))\n    ax=sns.barplot(data=data,x='Hashtag',y='Count')\n    plt.suptitle(title, fontsize=20)\n    plt.xlabel('Hashtag', fontsize=15)\n    plt.ylabel('Count', fontsize=15)\n    plt.xticks(rotation=90)\n    plt.tick_params(labelsize=15)\n    plt.show()","6690b321":"plot_hashtag(ht_pos_df,'Positive sentiments')","518eb9e2":"plot_hashtag(ht_neg_df,'Negative sentiments')","14f32fa7":"plot_hashtag(ht_neu_df,'Neutral sentiments')","154d254b":"plt.figure(figsize = (15,8))\ndf_eng.groupby(['sentiment'])['favourites_count'].mean().plot(color='red', linestyle='dashed', marker='o',\n                                                            markerfacecolor='red', markersize=10)\nplt.title('Sentiment-wise Likes ratio', fontsize = 20)\nplt.xlabel('Sentiment', fontsize = 20)\nplt.ylabel('Average Likes', fontsize = 20)\nplt.xticks(fontsize = 15)\nplt.show()","5a9ddfed":"plt.figure(figsize = (15,8))\ndf_eng.groupby(['sentiment'])['retweet_count'].mean().plot(color='green', linestyle='dashed', marker='o',\n                                                            markerfacecolor='g', markersize=10)\nplt.title('Sentiment-wise Retweets ratio', fontsize = 20)\nplt.xlabel('Sentiment', fontsize = 20)\nplt.ylabel('Average Retweets', fontsize = 20)\nplt.xticks(fontsize = 15)\nplt.show()","f66931bb":"df_eng.sort_values(by = 'favourites_count', ascending = False).iloc[:3][['text','favourites_count']]","3f58fc3a":"df_eng.sort_values(by = 'retweet_count', ascending = False).iloc[:3][['text','retweet_count']]","b75c6106":"df_eng['time'] = pd.to_datetime(df_eng['created_at'])","233e22af":"df_eng.groupby(['time'])['text'].count().plot(marker='.', alpha=0.5, figsize=(15, 5))\nplt.xlabel('Time', fontsize = 20)\nplt.ylabel('Tweet Count', fontsize = 20)\nplt.xticks(fontsize = 12)\nplt.title('Rate of Overall Tweets', fontsize = 20)\nplt.show()","69e2fcc4":"df_eng[df_eng['sentiment'] == 'Positive'].groupby(['time'])['text'].count().plot(marker='.', alpha=0.5, figsize=(15, 5),\n                                                                                 color = 'g',markerfacecolor='g')\nplt.xlabel('Time', fontsize = 20)\nplt.ylabel('Tweet Count', fontsize = 20)\nplt.xticks(fontsize = 12)\nplt.title('Rate of Positive Tweets', fontsize = 20)\nplt.show()","cc7fc84b":"df_eng[df_eng['sentiment'] == 'Negative'].groupby(['time'])['text'].count().plot(marker='.', alpha=0.5, figsize=(15, 5),\n                                                                                 color = 'r',markerfacecolor='r')\nplt.xlabel('Time', fontsize = 20)\nplt.ylabel('Tweet Count', fontsize = 20)\nplt.xticks(fontsize = 12)\nplt.title('Rate of Negative Tweets', fontsize = 20)\nplt.show()","93df9fb1":"# Importing Gensim\nimport gensim\nfrom gensim import corpora\n\n# Creating the term dictionary of our corpus, where every unique term is assigned an index. \ndictionary = corpora.Dictionary(docs)\n\n# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\ndoc_term_matrix = [dictionary.doc2bow(doc) for doc in docs]","0faa7cf6":"# Creating the object for LDA model using gensim library\nLda = gensim.models.ldamodel.LdaModel","6b843490":"# Running and Trainign LDA model on the document term matrix.\nldamodel2 = Lda(doc_term_matrix, num_topics=5, id2word = dictionary, passes=10, iterations=10) ","10c78c4c":"for idx, topic in ldamodel2.show_topics(formatted=False, num_words= 30):\n    print('Topic: {} \\nWords: {}'.format(idx+1, '|'.join([w[0] for w in topic])))","78938455":"## Top 3 Most Liked Tweets","f66e8bf5":"Before we can start with the classification we have to find a way to represent the words.\nNow there two popular ways to do this: <b> Word Vectors<\/b> and <b>Word Embeddings<\/b>\n\n<b>Word Vectors<\/b> are high dimensional sparse (mostly 0s) vectors where each vector represents a word which is simply one hot encoded.\n<b>Word Embeddings<\/b> unlike word vectors represent the words in dense vectors. The words are mapped into a meaningful space where the distance between words is related to their semantic similarity.\n","f8c9a135":"Therefore the above analysis of the tweets is one way through which people and government can understand the mental and emotional state of a given population and aid them through these times.","70d390ac":"Check the amount of missing values present","81d7371f":"# Sentiment Classification","1d5740b3":"75% of the tweets are Negative whereas only 10% of the tweets are Positive","8a28e915":"On average Positive and Neutral tweets gets more likes than Negative tweets.","b25a5cc6":"# Hashtags Visualization","a4f2171c":"## Top 3 Most Retweeted Tweets","93752386":"I have carried out some analysis on the same below. The sentiments for the tweets have not been provided in the dataset. So I have presented my way on how one could go about this problem so hope this helps. Also I would appreciate any kind of feedback.","160401aa":"We see that 56% of all tweets are in English which is no surprise followed by Spanish and French at 15% and 5% respectively.","99fd94fc":"All the tweets have been cleaned and tokenized. This will make it easier to vectorize the words and help the model to predict the sentiment of each tweet.","dae14917":"On average people are active between 8:00am to 9:00pm","290ad7b2":"As you can see the tweets are not clean. They contain urls, hashtags, stop words, special chracters and some also contains emoticons. So i've made a function to clean and tokenize them which will be in my repo.","9a8a110c":"Positive tweets seem to occur more frequently between 3:00pm to 8:00pm","0737b457":"As the sentiments for the tweets have not been provided the classifier can be trained on any public data which contains labels. So I trained my model on an <b>Airline tweets sentiment<\/b> dataset(availble in kaggle) in which the tweets were separated into three sentiments <b>'Positive', 'Negative'<\/b> and <b>'Neutral'<\/b>\n\nThis dataset was trained on a 6 layer <b>LSTM<\/b> network with pretained <b>GloVe embeddings<\/b>. The model is available in my repository as 'sentiment_classifier.py'. The final model which I've used below had an accuracy of:<b> Train set 87%, Dev set 81%, Test set 80%<\/b>","a6e94ed9":"Thank You for taking some time and reading this notebook. Stay Home Stay Safe.","b2401836":"We can see that the classifier has managed to classify the sentiments quite well.","edfd260a":"<b>Negative tweets:<\/b> These tweets mostly contain words like 'business', 'work', 'government' which are related to how businesses are facing heavy losses during this lockdown and how this has caused many employees to be laid off as their companies are not able to make enough profits.","dbdca854":"The first two columns and also the 'screen_name' column can be dropped as they wont be necessary during Visualization","3723c33a":"# Topic Modelling using LDA","c1f68000":"There some words which are common in both the positive and negative tweets like Coronavirus, Pandemic, Lockdown because they are the main subject of these tweets and are therefore in most of them.","bb1c46c7":"I'll be using pretrained <b>GloVe<\/b> embeddings to represent the words","6e0eee14":"# Sentiment-wise Retweets ratio","7feb63d7":"# Conclusion:","c89f04a0":"Negative tweets are retweeted the most on average followed by Positive tweets.","ea7febca":"Tweets are a good way for people to express their thoughts in front of a large audience. Therefore analyzing tweets and predicting their sentiments helps us to better understand what is going through everyones mind, how people are coping up being quarantined for months, how it has affected the life of so many people and also some positive effects (yes there are a few) that this pandemic has caused.","57fbbe7f":"### Word Embeddings","bd60d097":"<b>Topic Modelling<\/b> helps us to understand and summarise large collections of text. So far I have used <b>WordClouds<\/b> to manually look through the words and get a basic understanding of the different topics the tweets were related to. But there are much better statistical methods like <b>LDA<\/b> which can be used to discover <b>abstract topics<\/b> in a collection of documents.\n\n<b>LDA<\/b> or <b>Latent Dirichlet Allocation<\/b> is used to classify text in a document to a particular topic. It backtracks and tries to figure out which topics would create the given documents. Then it assigns words to a given topic with some probability.","d626d3fa":"# Covid-19 Tweets - Sentiment Analysis","192e8d82":"Hashtags help to categorize any content. Therefore in a way they give a good idea about the theme and contents of a particular tweet.","2da5c415":"There are 7 columns with more than 85% of it's values missing so I will be dropping these as they won't provide much info","f6e3bc43":"Negative tweets are more frequent between 7:00am to 11:00pm and they seem to occur at amuch higher rate than Positive tweets.","9ba0e593":"For now I'll be only focusing on the English tweets","31f1466a":"# Time Series Analysis","a2073711":"## Sentiment-wise Likes ratio","2f82c89b":"<b>Covid-19<\/b> is an infectious disease caused by the newly discovered coronavirus.It was first identified in December 2019 in <b>Wuhan, China<\/b>, and has resulted in an ongoing <b>pandemic<\/b>.\nIt spreads primarily through droplets of saliva or discharge from the nose when an infected person coughs or sneezes. Most infected with the virus have faced mild respiratory illness with around <b>80%<\/b> cases being <b>asymptomatic<\/b>. People with underlying medical problems are found to be more likely to develop serious illness.","b51f52b2":"Let's look at the top five rows of our dataset to get a basic overview","118aa08b":"The LDA model has managed to classify the words into 5 abstract topics. On reading the individual words of a topic we can see that the words are not randomly assigned and that these words together have some meaning. \n\nThe <b>first topic<\/b> contains words like <b>'worker', 'business', 'health', 'open'<\/b> which are related the losses faced by <b>businesses<\/b> and also the reaction to people regarding the <b>government's decisions<\/b>. The <b>second topic<\/b> is related to the <b>daily updates<\/b> on the number of cases that have increased along with the death rate and recovery rate. Some topics are not as easily interpretable like the <b>third topic<\/b>, it may seem a little vague at first compared to the other four topics but it seems like it is linked with how people are questioning Trump about the <b>current affairs<\/b>. The <b>fourth topic<\/b> is associated with <b>news<\/b> and the <b>fifth topic<\/b> with something more <b>positive<\/b> than the other topics for instance it seems to be linked with <b>spreading awareness<\/b> about social distancing, helping and supporting others.","71d37358":"This pandemic has been a psychological crisis. It has the potential to drastically affect the mental well-being of many. Therefore even in this time of social distancing it is important to understand what one is going through and provide them with all the necessary help and support they need. ","5c2c0d6d":"Adding a 'Language' column using the given language codes","73b3c039":" ![](img\/covid19.jpg)","7923ce0f":"Before I can continue with any visualization I have to first Classify the sentiment of each tweet","eb88b4c9":"<b>Positive tweets:<\/b> These mostly contain words like 'help', 'social distancing',  'self reporting' which are related to spreading awareness about how the spread of virus can be slowed down by staying at home and containing the pandemic. Also people have shared different ways through which one can take care of their mental health like self reporting.","87a0b773":"Let's take a look at one of the tweets"}}