{"cell_type":{"8d251b31":"code","fafb4ae9":"code","01b13b2c":"code","7d07f48f":"code","1b1fa021":"code","a845a947":"code","d651b89f":"code","2e751fa9":"code","f51b378d":"code","3a0132b6":"code","0f987cf8":"code","4b56bfb8":"code","e9a07660":"code","08103ec5":"code","cd47bd8e":"code","aba48b4d":"code","08ea5280":"code","bed7ddd6":"code","ca237fab":"code","74b6d5f2":"code","1d32974b":"code","d4432116":"code","f22e1177":"code","1020aedd":"code","14236859":"code","ffbc3b7e":"code","8439df5f":"code","f5874c11":"code","621be5c2":"code","bfd78c39":"markdown","5d40690c":"markdown","07130100":"markdown","87224ffa":"markdown","de0cc4e7":"markdown","aa6a8823":"markdown","5641eca4":"markdown","0932852d":"markdown","12528ca4":"markdown","a7cc703c":"markdown","069870c5":"markdown","fda6e2fa":"markdown","48bfe357":"markdown"},"source":{"8d251b31":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fafb4ae9":"# Importing libraries\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno as msno\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nplt.style.use('ggplot')","01b13b2c":"# Load the data:-\n\ndf = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf","7d07f48f":"# View raw data:-\n\npd.set_option('max_columns',33)\ndf.head()","1b1fa021":"# Dimension of the data:-\n\ndf.shape","a845a947":"# Data types:-\n\ndf.info()","d651b89f":"# Checking Null values using heatmap:-\n\nsns.heatmap(df.isnull())","2e751fa9":"# Count the null values:-\n\ndf.isnull().sum()","f51b378d":"# Droping Unnamed: 32 Column\n\ndf.drop('Unnamed: 32', axis = 1, inplace = True)","3a0132b6":"# Find unique values in 'diagnosis' column:-\n\ndf.diagnosis.unique()","0f987cf8":"# Replace 'M' with 1 and 'B' with 0 in column 'diagnosis':-\n\ndf['diagnosis'] = df['diagnosis'].apply(lambda val: 1 if val == 'M' else 0)","4b56bfb8":"# Again view raw data:-\n\ndf.head()","e9a07660":"# Describing the data \/ Statistical Data analysis:-\n\npd.set_option('precision',3)\ndf.describe()","08103ec5":"# Diagnosis Pie chart:-\n\nprint(df.diagnosis.value_counts())\ndf.diagnosis.value_counts().plot.pie();","cd47bd8e":"# Heatmap:-\n\nplt.figure(figsize=(30,30))\nsns.heatmap(df.corr(),annot = True, cmap = 'Blues');","aba48b4d":"# EDA:-\n\nplt.figure(figsize = (20, 15))\nplotnumber = 1\n\nfor column in df:\n    if plotnumber <= 30:\n        ax = plt.subplot(5, 6, plotnumber)\n        sns.distplot(df[column])\n        plt.xlabel(column)\n        \n    plotnumber += 1\n\nplt.tight_layout()\nplt.show()","08ea5280":"# Getting Mean Columns\nm_col = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n\n# Getting Se Columns\ns_col = ['diagnosis','radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se']\n\n# Getting Worst column\nw_col = ['diagnosis','radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']","bed7ddd6":"# Heatmap For Mean Columns:-\n\nplt.figure(figsize=(15,15))\nsns.heatmap(df[m_col].corr(),annot = True, cmap = 'Blues');","ca237fab":"# pairplot for mean columns\n\nsns.pairplot(df[m_col],hue = 'diagnosis', palette='Blues');","74b6d5f2":"# Heatmap for se columns\n\nplt.figure(figsize=(15,15))\nsns.heatmap(df[s_col].corr(),annot = True, cmap = 'Reds');","1d32974b":"# pairplot for se columns\n\nsns.pairplot(df[m_col],hue = 'diagnosis', palette='Reds');","d4432116":"# Heatmap for Worst columns:-\n\nplt.figure(figsize=(15,15))\nsns.heatmap(df[w_col].corr(),annot = True, cmap = 'Greens');","f22e1177":"# pairplot for worst columns:-\n\nsns.pairplot(df[w_col],hue = 'diagnosis', palette='Greens');","1020aedd":"# removing highly correlated features\n\ncorr_matrix = df.corr().abs() \n\nmask = np.triu(np.ones_like(corr_matrix, dtype = bool))\ntri_df = corr_matrix.mask(mask)\n\nto_drop = [x for x in tri_df.columns if any(tri_df[x] > 0.92)]\n\ndf = df.drop(to_drop, axis = 1)\n\nprint(f\"The reduced dataframe has {df.shape[1]} columns.\")","14236859":"# Getting Features:-\n\nx = df.drop(columns = 'diagnosis')\n\n# Getting Predicting Value:-\n\ny = df['diagnosis']","ffbc3b7e":"# Splitting data into training and testing data:-\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=1\/9, random_state=252)","8439df5f":"# Scaling data:-\n\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nx_train = scale.fit_transform(x_train)\nx_test = scale.fit_transform(x_test)","f5874c11":"# Defining Models:-\n\ndef Classification_Models(x,y,xt,yt):\n    # Importing All LIberaries\n    from sklearn.metrics import accuracy_score\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn import svm\n    from sklearn.neighbors import KNeighborsClassifier\n\n    # Initializing models:-\n    \n    logisreg = LogisticRegression()\n    lda = LinearDiscriminantAnalysis()\n    gnb = GaussianNB()\n    dtc = DecisionTreeClassifier()\n    rfc = RandomForestClassifier()\n    svmodel = svm.SVC()\n    knnmodel = KNeighborsClassifier()\n    \n    # Fitting Models\n    logisreg.fit(x,y)\n    lda.fit(x,y)\n    gnb.fit(x,y)\n    dtc.fit(x,y)\n    rfc.fit(x,y)\n    svmodel.fit(x,y)\n    knnmodel.fit(x,y)\n    \n    # Getting Predicting Values:-\n    \n    logi_pred = logisreg.predict(xt)\n    lda_pred = lda.predict(xt)\n    gnb_pred = gnb.predict(xt)\n    dtc_pred = dtc.predict(xt)\n    rfc_pred = rfc.predict(xt)\n    svm_pred = svmodel.predict(xt)\n    knn_pred = knnmodel.predict(xt)\n    \n    # Getting Accuracy Score\n    acc_logisreg = accuracy_score(yt, logi_pred)\n    acc_lda = accuracy_score(yt, lda_pred)\n    acc_ganb = accuracy_score(yt, gnb_pred)\n    acc_dtree = accuracy_score(yt, dtc_pred)\n    acc_rf = accuracy_score(yt, rfc_pred)\n    acc_svc = accuracy_score(yt, svm_pred)\n    acc_knn = accuracy_score(yt, knn_pred)\n    \n    # MOdel Selection\n    models = pd.DataFrame({\n    'Model': ['Logistic Regression','Linear Discriminant Analysis','Naive Bayes', 'Decision Tree', 'Random Forest', 'Support Vector Machines', \n              'K - Nearest Neighbors'],\n    'Score': [acc_logisreg, acc_lda, acc_ganb, acc_dtree, acc_rf, acc_svc, acc_knn]})\n\n    print(models.sort_values(by='Score', ascending=False))\n    sns.barplot(x = models['Score'], y = models['Model'], palette='viridis');","621be5c2":"Classification_Models(x_train,y_train,x_test,y_test)","bfd78c39":"> We can see that there are many columns which are very highly correlated which causes multicollinearity so we have to remove highly correlated features.","5d40690c":"# Creating Classification Models:-","07130100":"> Logistic Regression, SVM, Random Forest and KNN were the best here with the accuracy of 100%.","87224ffa":"> No. of rows and columns in the data is 569 and 33 respectvely.","de0cc4e7":"# Breast Cancer Classification","aa6a8823":"# Data Modelling:-","5641eca4":"***Ten real-valued features are computed for each cell nucleus:***\n* radius (mean of distances from center to points on the perimeter)\n* texture (standard deviation of gray-scale values)\n* perimeter\n* area\n* smoothness (local variation in radius lengths)\n* compactness (perimeter^2 \/ area - 1.0)\n* concavity (severity of concave portions of the contour)\n* concave points (number of concave portions of the contour)\n* symmetry\n* fractal dimension (\"coastline approximation\" - 1)","0932852d":"***Attribute Information:***\n\n* ID number\n \n* Diagnosis (M = malignant, B = benign)","12528ca4":">As we can see that the last column Unnamed: 32 has all NaN value so we will drop this column.","a7cc703c":"# Data preprocessing:-","069870c5":"# Please leave your feedbacks in the comment section. Thank you.....","fda6e2fa":"# If you like my work, please do a upvote :)","48bfe357":"# Data Visualization:-"}}