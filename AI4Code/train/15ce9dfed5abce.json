{"cell_type":{"e5ccdb9e":"code","9248373a":"code","a5b9a277":"code","aed21f59":"code","9f0e7b7e":"code","b2cd5b37":"code","995442f9":"code","14004248":"code","946d1992":"code","0cedc9eb":"code","dac2a0b1":"code","8613b847":"code","43b29d47":"code","f25e5cc1":"code","1dd515b6":"code","2b3aab78":"code","ba4caea0":"code","4fbcff3e":"code","a106496f":"code","9969e4da":"code","3532921a":"code","9c15720d":"code","43c34509":"code","a2dbf2f9":"code","98c29ea0":"code","427863a6":"code","3b57e704":"code","79b8cd15":"code","489c3a34":"code","1eb5010f":"code","622532a7":"code","f0e02bb4":"code","0360d511":"code","fc251a46":"code","f4b03f0e":"code","8dff02bd":"code","f29547c3":"code","71f6940e":"code","1c8efddf":"code","c063b07c":"code","61aaebaa":"code","9857d935":"code","019f82bc":"code","2aed5910":"code","19c664a8":"code","41279f8e":"code","466b082b":"code","92fb8bdd":"code","291cc3d4":"code","914c21d6":"code","42bdebc4":"code","38f8667c":"code","0008caef":"code","684dad42":"code","10738faa":"code","a3ecc52a":"code","8a6223e7":"code","2ce29fb5":"code","5f9e1948":"code","e766ff85":"code","361c6fd0":"code","9d87ef16":"code","e9866c3f":"code","f100b31e":"code","0858a9fe":"code","b5f10f25":"code","28f4c899":"code","d6b40cc9":"code","c7ee06aa":"code","8a7bb6e4":"code","3af02258":"code","297c84d7":"code","ea56585c":"code","1eff20ee":"code","1ce0584d":"code","eb8c3af0":"code","90d050aa":"code","328c0bbb":"code","9101d343":"code","10345a7e":"code","4bec1b46":"code","76e7e42e":"code","42f773a1":"code","19351e5b":"code","d89dd86a":"markdown","353dd620":"markdown","9ca25676":"markdown","bce911c6":"markdown","bda77c63":"markdown","3077801a":"markdown","97c611a0":"markdown","48b0c487":"markdown","3ce5cb41":"markdown","6c9abea0":"markdown","bdf7dd6e":"markdown","b60614ba":"markdown","d44bade0":"markdown","b573b79a":"markdown","c147966b":"markdown","5a666af2":"markdown","c2221446":"markdown","e73ff7eb":"markdown","2a50eb74":"markdown","71b90360":"markdown","ba90cfd0":"markdown","fcd62345":"markdown","158a1147":"markdown","c2ee8550":"markdown","01ab1a47":"markdown","7d5d0181":"markdown","aba39e41":"markdown","9ec8bf09":"markdown","f4accff3":"markdown","955cc24f":"markdown","38995458":"markdown","1069e024":"markdown","664660be":"markdown","ee9137b5":"markdown","03753d20":"markdown","929dee1c":"markdown","c7233a32":"markdown","fcc846ee":"markdown","6240d13b":"markdown","06dccc53":"markdown","d721fc3a":"markdown","acb5ce15":"markdown","4c0d4636":"markdown","1237e250":"markdown","2c0912c9":"markdown","45726d26":"markdown","2206cd7d":"markdown","63af4088":"markdown","dda95b40":"markdown","d3dadb52":"markdown","17f17d97":"markdown","ce926c3e":"markdown","ddcb3310":"markdown","df841d48":"markdown","9dba3cc1":"markdown","9fe63b04":"markdown","9535a714":"markdown","b7150040":"markdown","868cd77e":"markdown"},"source":{"e5ccdb9e":"# import all libraries and dependencies for dataframe\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom datetime import datetime, timedelta\n\n# import all libraries and dependencies for data visualization\npd.options.display.float_format='{:.4f}'.format\nplt.rcParams['figure.figsize'] = [8,8]\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', -1) \nsns.set(style='darkgrid')\nimport matplotlib.ticker as plticker\n%matplotlib inline\n\n# import all libraries and dependencies for machine learning\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nfrom math import isnan\n\n# import all libraries and dependencies for clustering\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree","9248373a":"# Reading the country file on which analysis needs to be done\n\ndf_country = pd.read_csv('..\/input\/pca-kmeans-hierarchical-clustering\/Country-data.csv')\n\ndf_country.head()","a5b9a277":"# Reading the data dictionary file\n\ndf_structure = pd.read_csv('..\/input\/pca-kmeans-hierarchical-clustering\/data-dictionary.csv')\ndf_structure.head(10)","aed21f59":"df_country.shape","9f0e7b7e":"df_country.describe()","b2cd5b37":"df_country.info()","995442f9":"# Calculating the Missing Values % contribution in DF\n\ndf_null = df_country.isna().mean()*100\ndf_null","14004248":"# Datatype check for the dataframe\n\ndf_country.dtypes","946d1992":"# Duplicates check\n\ndf_country.loc[df_country.duplicated()]","0cedc9eb":"# Segregation of Numerical and Categorical Variables\/Columns\n\ncat_col = df_country.select_dtypes(include = ['object']).columns\nnum_col = df_country.select_dtypes(exclude = ['object']).columns","dac2a0b1":"# Heatmap to understand the attributes dependency\n\nplt.figure(figsize = (15,10))        \nax = sns.heatmap(df_country.corr(),annot = True)\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","8613b847":"# Pairplot of all numeric columns\n\nsns.pairplot(df_country)","43b29d47":"# Converting exports,imports and health spending percentages to absolute values.\n\ndf_country['exports'] = df_country['exports'] * df_country['gdpp']\/100\ndf_country['imports'] = df_country['imports'] * df_country['gdpp']\/100\ndf_country['health'] = df_country['health'] * df_country['gdpp']\/100","f25e5cc1":"df_country.head(5)","1dd515b6":"# Dropping Country field as final dataframe will only contain data columns\n\ndf_country_drop = df_country.copy()\ncountry = df_country_drop.pop('country')","2b3aab78":"df_country_drop.head()","ba4caea0":"# Standarisation technique for scaling\n\nwarnings.filterwarnings(\"ignore\")\nscaler = StandardScaler()\ndf_country_scaled = scaler.fit_transform(df_country_drop)","4fbcff3e":"df_country_scaled","a106496f":"pca = PCA(svd_solver='randomized', random_state=42)","9969e4da":"# Lets apply PCA on the scaled data\n\npca.fit(df_country_scaled)","3532921a":"# PCA components created \n\npca.components_","9c15720d":"# Variance Ratio\n\npca.explained_variance_ratio_","43c34509":"# Variance Ratio bar plot for each PCA components.\n\nax = plt.bar(range(1,len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)\nplt.xlabel(\"PCA Components\",fontweight = 'bold')\nplt.ylabel(\"Variance Ratio\",fontweight = 'bold')","a2dbf2f9":"# Scree plot to visualize the Cumulative variance against the Number of components\n\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.vlines(x=3, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\nplt.hlines(y=0.93, xmax=8, xmin=0, colors=\"g\", linestyles=\"--\")\nplt.xlabel('Number of PCA components')\nplt.ylabel('Cumulative Explained Variance')","98c29ea0":"# Checking which attributes are well explained by the pca components\n\norg_col = list(df_country.drop(['country'],axis=1).columns)\nattributes_pca = pd.DataFrame({'Attribute':org_col,'PC_1':pca.components_[0],'PC_2':pca.components_[1],'PC_3':pca.components_[2]})","427863a6":"attributes_pca","3b57e704":"# Plotting the above dataframe for better visualization with PC1 and PC2\n\nsns.pairplot(data=attributes_pca, x_vars=[\"PC_1\"], y_vars=[\"PC_2\"], hue = \"Attribute\" ,height=8)\nplt.xlabel(\"Principal Component 1\",fontweight = 'bold')\nplt.ylabel(\"Principal Component 2\",fontweight = 'bold')\n\nfor i,txt in enumerate(attributes_pca.Attribute):\n    plt.annotate(txt, (attributes_pca.PC_1[i],attributes_pca.PC_2[i]))","79b8cd15":"# Plotting the above dataframe with PC1 and PC3 to understand the components which explains inflation.\n\nsns.pairplot(data=attributes_pca, x_vars=[\"PC_1\"], y_vars=[\"PC_3\"], hue = \"Attribute\" ,height=8)\nplt.xlabel(\"Principal Component 1\",fontweight = 'bold')\nplt.ylabel(\"Principal Component 3\",fontweight = 'bold')\n\nfor i,txt in enumerate(attributes_pca.Attribute):\n    plt.annotate(txt, (attributes_pca.PC_1[i],attributes_pca.PC_3[i]))","489c3a34":"# Building the dataframe using Incremental PCA for better efficiency.\n\ninc_pca = IncrementalPCA(n_components=3)","1eb5010f":"# Fitting the scaled df on incremental pca\n\ndf_inc_pca = inc_pca.fit_transform(df_country_scaled)\ndf_inc_pca","622532a7":"# Creating new dataframe with Principal components\n\ndf_pca = pd.DataFrame(df_inc_pca, columns=[\"PC_1\", \"PC_2\",\"PC_3\"])\ndf_pca_final = pd.concat([country, df_pca], axis=1)\ndf_pca_final.head()","f0e02bb4":"# Plotting Heatmap to check is there still dependency in the dataset.\n\nplt.figure(figsize = (8,6))        \nax = sns.heatmap(df_pca_final.corr(),annot = True)\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","0360d511":"# Scatter Plot to visualize the spread of data across PCA components\n\nplt.figure(figsize=(20, 8))\nplt.subplot(1,3,1)\nsns.scatterplot(data=df_pca_final, x='PC_1', y='PC_2')\nplt.subplot(1,3,2)\nsns.scatterplot(data=df_pca_final, x='PC_1', y='PC_3')\nplt.subplot(1,3,3)\nsns.scatterplot(data=df_pca_final, x='PC_3', y='PC_2')","fc251a46":"# Outlier Analysis \n\noutliers = ['PC_1','PC_2','PC_3']\nplt.rcParams['figure.figsize'] = [10,8]\nsns.boxplot(data = df_pca_final[outliers], orient=\"v\", palette=\"Set2\" ,whis=1.5,saturation=1, width=0.7)\nplt.title(\"Outliers Variable Distribution\", fontsize = 14, fontweight = 'bold')\nplt.ylabel(\"Range\", fontweight = 'bold')\nplt.xlabel(\"PC Components\", fontweight = 'bold')","f4b03f0e":"# Statstical Outlier treatment for PC_1\n\nQ1 = df_pca_final.PC_1.quantile(0.05)\nQ3 = df_pca_final.PC_1.quantile(0.95)\nIQR = Q3 - Q1\ndf_pca_final = df_pca_final[(df_pca_final.PC_1 >= Q1) & (df_pca_final.PC_1 <= Q3)]\n\n# Statstical Outlier treatment for PC_2\n\nQ1 = df_pca_final.PC_2.quantile(0.05)\nQ3 = df_pca_final.PC_2.quantile(0.95)\nIQR = Q3 - Q1\ndf_pca_final = df_pca_final[(df_pca_final.PC_2 >= Q1) & (df_pca_final.PC_2 <= Q3)]\n\n# Statstical Outlier treatment for PC_3\n\nQ1 = df_pca_final.PC_3.quantile(0.05)\nQ3 = df_pca_final.PC_3.quantile(0.95)\nIQR = Q3 - Q1\ndf_pca_final = df_pca_final[(df_pca_final.PC_3 >= Q1) & (df_pca_final.PC_3 <= Q3)]","8dff02bd":"# Plot after Outlier removal \n\noutliers = ['PC_1','PC_2','PC_3']\nplt.rcParams['figure.figsize'] = [10,8]\nsns.boxplot(data = df_pca_final[outliers], orient=\"v\", palette=\"Set2\" ,whis=1.5,saturation=1, width=0.7)\nplt.title(\"Outliers Variable Distribution\", fontsize = 14, fontweight = 'bold')\nplt.ylabel(\"Range\", fontweight = 'bold')\nplt.xlabel(\"PC Components\", fontweight = 'bold')","f29547c3":"# Reindexing the df after outlier removal\n\ndf_pca_final = df_pca_final.reset_index(drop=True)\ndf_pca_final_data = df_pca_final.drop(['country'],axis=1)\ndf_pca_final.head()","71f6940e":"# Calculating Hopkins score to know whether the data is good for clustering or not.\n\ndef hopkins(X):\n    d = X.shape[1]\n    n = len(X)\n    m = int(0.1 * n) \n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    HS = sum(ujd) \/ (sum(ujd) + sum(wjd))\n    if isnan(HS):\n        print(ujd, wjd)\n        HS = 0\n \n    return HS","1c8efddf":"# Hopkins score\n\n#hopkins(df_pca_final_data)","c063b07c":"# Elbow curve method to find the ideal number of clusters.\nssd = []\nfor num_clusters in list(range(1,8)):\n    model_clus = KMeans(n_clusters = num_clusters, max_iter=50,random_state= 100)\n    model_clus.fit(df_pca_final_data)\n    ssd.append(model_clus.inertia_)\n\nplt.plot(ssd)","61aaebaa":"# Silhouette score analysis to find the ideal number of clusters for K-means clustering\n\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50,random_state= 100)\n    kmeans.fit(df_pca_final_data)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(df_pca_final_data, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))\n","9857d935":"#K-means with k=4 clusters\n\ncluster4 = KMeans(n_clusters=4, max_iter=50, random_state= 100)\ncluster4.fit(df_pca_final_data)","019f82bc":"# Cluster labels\n\ncluster4.labels_","2aed5910":"# Assign the label\n\ndf_pca_final['Cluster_Id4'] = cluster4.labels_\ndf_pca_final.head()","19c664a8":"# Number of countries in each cluster\n\ndf_pca_final['Cluster_Id4'].value_counts()","41279f8e":"# Scatter plot on Principal components to visualize the spread of the data\n\nfig, axes = plt.subplots(1,2, figsize=(15,7))\n\nsns.scatterplot(x='PC_1',y='PC_2',hue='Cluster_Id4',legend='full',palette=\"Set1\",data=df_pca_final,ax=axes[0])\nsns.scatterplot(x='PC_1',y='PC_3',hue='Cluster_Id4',legend='full',palette=\"Set1\",data=df_pca_final,ax=axes[1])","466b082b":"# Lets drop the Cluster Id created with 4 clusters and proceed with 5 clusters.\n\ndf_pca_final = df_pca_final.drop('Cluster_Id4',axis=1)","92fb8bdd":"#K-means with k=5 clusters\n\ncluster5 = KMeans(n_clusters=5, max_iter=50,random_state=100)\ncluster5.fit(df_pca_final_data)","291cc3d4":"# Cluster labels\n\ncluster5.labels_","914c21d6":"# assign the label\n\ndf_pca_final['Cluster_Id'] = cluster5.labels_\ndf_pca_final.head()","42bdebc4":"# Number of countries in each cluster\n\ndf_pca_final['Cluster_Id'].value_counts()","38f8667c":"# Scatter plot on Principal components to visualize the spread of the data\nfig, axes = plt.subplots(1,2, figsize=(15,7))\n\nsns.scatterplot(x='PC_1',y='PC_2',hue='Cluster_Id',legend='full',palette=\"Set1\",data=df_pca_final,ax=axes[0])\nsns.scatterplot(x='PC_1',y='PC_3',hue='Cluster_Id',legend='full',palette=\"Set1\",data=df_pca_final,ax=axes[1])","0008caef":"# Merging the df with PCA with original df\n\ndf_merge = pd.merge(df_country,df_pca_final,on='country')\ndf_merge_col = df_merge[['country','child_mort','exports','imports','health','income','inflation','life_expec','total_fer','gdpp','Cluster_Id']]\n\n# Creating df with mean values\ncluster_child = pd.DataFrame(df_merge_col.groupby([\"Cluster_Id\"]).child_mort.mean())\ncluster_export = pd.DataFrame(df_merge_col.groupby([\"Cluster_Id\"]).exports.mean())\ncluster_import = pd.DataFrame(df_merge_col.groupby([\"Cluster_Id\"]).imports.mean())\ncluster_health = pd.DataFrame(df_merge_col.groupby([\"Cluster_Id\"]).health.mean())\ncluster_income = pd.DataFrame(df_merge_col.groupby([\"Cluster_Id\"]).income.mean())\ncluster_inflation = pd.DataFrame(df_merge_col.groupby([\"Cluster_Id\"]).inflation.mean())         \ncluster_lifeexpec = pd.DataFrame(df_merge_col.groupby([\"Cluster_Id\"]).life_expec.mean())\ncluster_totalfer = pd.DataFrame(df_merge_col.groupby([\"Cluster_Id\"]).total_fer.mean())\ncluster_gdpp = pd.DataFrame(df_merge_col.groupby([\"Cluster_Id\"]).gdpp.mean())\n\ndf_concat = pd.concat([pd.Series([0,1,2,3,4]),cluster_child,cluster_export,cluster_import,cluster_health,cluster_income\n                       ,cluster_inflation,cluster_lifeexpec,cluster_totalfer,cluster_gdpp], axis=1)\ndf_concat.columns = [\"Cluster_Id\", \"Child_Mortality\", \"Exports\", \"Imports\",\"Health_Spending\",\"Income\",\"Inflation\",\"Life_Expectancy\",\"Total_Fertility\",\"GDPpcapita\"]\ndf_concat.head()","684dad42":"df_merge_col.head(5)","10738faa":"# Scatter plot on Original attributes to visualize the spread of the data\n\nfig, axes = plt.subplots(2,2, figsize=(15,12))\n\nsns.scatterplot(x = 'income', y = 'child_mort',hue='Cluster_Id',data = df_merge_col,legend='full',palette=\"Set1\",ax=axes[0][0])\nsns.scatterplot(x = 'gdpp', y = 'income',hue='Cluster_Id', data = df_merge_col,legend='full',palette=\"Set1\",ax=axes[0][1])\nsns.scatterplot(x = 'child_mort', y = 'gdpp',hue='Cluster_Id', data=df_merge_col,legend='full',palette=\"Set1\",ax=axes[1][0])","a3ecc52a":"# Box plot on Original attributes to visualize the spread of the data\n\nfig, axes = plt.subplots(2,2, figsize=(15,12))\n\nsns.boxplot(x = 'Cluster_Id', y = 'child_mort', data = df_merge_col,ax=axes[0][0])\nsns.boxplot(x = 'Cluster_Id', y = 'income', data = df_merge_col,ax=axes[0][1])\nsns.boxplot(x = 'Cluster_Id', y = 'inflation', data=df_merge_col,ax=axes[1][0])\nsns.boxplot(x = 'Cluster_Id', y = 'gdpp', data=df_merge_col,ax=axes[1][1])","8a6223e7":"# Box plot to visualise the mean value of few original attributes.\n\nfig, axes = plt.subplots(2,2, figsize=(15,12))\n\nsns.boxplot(x = 'Cluster_Id', y = 'Child_Mortality', data = df_concat,ax=axes[0][0])\nsns.boxplot(x = 'Cluster_Id', y = 'Income', data = df_concat,ax=axes[0][1])\nsns.boxplot(x = 'Cluster_Id', y = 'Inflation', data=df_concat,ax=axes[1][0])\nsns.boxplot(x = 'Cluster_Id', y = 'GDPpcapita', data=df_concat,ax=axes[1][1])","2ce29fb5":"# List of countries in Cluster 0\n\ndf_merge_col[df_merge_col['Cluster_Id']==0]","5f9e1948":"# List of countries in Cluster 3\n\ndf_merge_col[df_merge_col['Cluster_Id']==3]","e766ff85":"df_pca_final_data.head()","361c6fd0":"# Single linkage\n\nmergings = linkage(df_pca_final_data, method='single',metric='euclidean')\ndendrogram(mergings)\nplt.show()","9d87ef16":"# Complete Linkage\n\nmergings = linkage(df_pca_final_data, method='complete',metric='euclidean')\ndendrogram(mergings)\nplt.show()","e9866c3f":"df_pca_hc = df_pca_final.copy()\ndf_pca_hc = df_pca_hc.drop('Cluster_Id',axis=1)\ndf_pca_hc.head()","f100b31e":"# Let cut the tree at height of approx 3 to get 4 clusters and see if it get any better cluster formation.\n\nclusterCut = pd.Series(cut_tree(mergings, n_clusters = 4).reshape(-1,))\ndf_hc = pd.concat([df_pca_hc, clusterCut], axis=1)\ndf_hc.columns = ['country', 'PC_1', 'PC_2','PC_3','Cluster_Id']","0858a9fe":"df_hc.head()","b5f10f25":"# Scatter plot on Principal components to visualize the spread of the data\n\nfig, axes = plt.subplots(1,2, figsize=(15,8))\n\nsns.scatterplot(x='PC_1',y='PC_2',hue='Cluster_Id',legend='full',palette=\"Set1\",data=df_hc,ax=axes[0])\nsns.scatterplot(x='PC_1',y='PC_3',hue='Cluster_Id',legend='full',palette=\"Set1\",data=df_hc,ax=axes[1])","28f4c899":"# Merging the df with PCA with original df\n\ndf_merge_hc = pd.merge(df_country,df_hc,on='country')\ndf_merge_col_hc = df_merge[['country','child_mort','exports','imports','health','income','inflation','life_expec','total_fer','gdpp','Cluster_Id']]","d6b40cc9":"df_merge_col_hc.head()","c7ee06aa":"# Scatter plot on Original attributes to visualize the spread of the data\n\nfig, axes = plt.subplots(2,2, figsize=(15,12))\n\nsns.scatterplot(x = 'income', y = 'child_mort',hue='Cluster_Id',data = df_merge_col_hc,legend='full',palette=\"Set1\",ax=axes[0][0])\nsns.scatterplot(x = 'gdpp', y = 'income',hue='Cluster_Id', data = df_merge_col_hc,legend='full',palette=\"Set1\",ax=axes[0][1])\nsns.scatterplot(x = 'child_mort', y = 'gdpp',hue='Cluster_Id', data=df_merge_col_hc,legend='full',palette=\"Set1\",ax=axes[1][0])","8a7bb6e4":"df_clus0 = df_merge_col[df_merge_col['Cluster_Id'] ==0]","3af02258":"df_clus3 = df_merge_col[df_merge_col['Cluster_Id'] ==3]","297c84d7":"# List of countries which need help\n\ndf_append= df_clus0.append(df_clus3)","ea56585c":"df_append.head()","1eff20ee":"df_append.describe()","1ce0584d":"# Based on final clusters information we are going to deduce the final list.\n# We observed that mean child mortality is 53 for the selected clusters and hence \n# let's take all the countries with more than this child mortality .\n\ndf_final_list = df_country[df_country['child_mort']>53]\ndf_final_list.shape","eb8c3af0":"# Let's check the demographic of the resultant data again\n\ndf_final_list.describe()","90d050aa":"# We observed that mean income is 3695 for the selected clusters and hence \n# let's take all the countries with less than this income .\n\ndf_final_list1 = df_final_list[df_final_list['income']<=3695]\ndf_final_list1.shape","328c0bbb":"# Let's check the demographic of the resultant data again\n\ndf_final_list1.describe()","9101d343":"# We observed that mean gdpp is 831 for the selected clusters and hence \n# let's take all the countries with less than this gdpp .\n\ndf_final_list2 = df_final_list1[df_final_list1['gdpp']<=831]\ndf_final_list2.shape","10345a7e":"df_final_list2['country']","4bec1b46":"# BarPlot for Child Mortality of countries which are in need of aid\n\ndf_list_cm = pd.DataFrame(df_final_list2.groupby(['country'])['child_mort'].mean().sort_values(ascending = False))\ndf_list_cm.plot.bar()\nplt.title('Country and Child Mortality')\nplt.xlabel(\"Country\",fontweight = 'bold')\nplt.ylabel(\"Child Mortality\", fontsize = 12, fontweight = 'bold')\nplt.show()","76e7e42e":"# BarPlot for Per Capita Income of countries which are in need of aid\n\ndf_list_in = pd.DataFrame(df_final_list2.groupby(['country'])['income'].mean().sort_values(ascending = False))\ndf_list_in.plot.bar()\nplt.title('Country and Per Capita Income')\nplt.xlabel(\"Country\",fontweight = 'bold')\nplt.ylabel(\"Per Capita Income\", fontsize = 12, fontweight = 'bold')\nplt.show()","42f773a1":"# BarPlot for Per Capita Income of countries which are in need of aid\n\ndf_list_gdp = pd.DataFrame(df_final_list2.groupby(['country'])['gdpp'].mean().sort_values(ascending = False))\ndf_list_gdp.plot.bar()\nplt.title('Country and GDP per capita')\nplt.xlabel(\"Country\",fontweight = 'bold')\nplt.ylabel(\"GDP per capita\", fontsize = 12, fontweight = 'bold')\nplt.show()","19351e5b":"# Final countries list\ndf_final_list2.reset_index(drop=True).country","d89dd86a":"From the business understanding we have learnt that `Child_Mortality`, `Income`, `Gdpp` are some important factors which decides the development of any country.\nWe have also cross checked with Principal components and found that these variables have good score in PCA.\nHence, we will proceed with analyzing these 3 components to build some meaningful clusters.","353dd620":"The `Hopkins statistic` (introduced by Brian Hopkins and John Gordon Skellam) is a way of `measuring the cluster tendency` of a data set.It acts as a statistical hypothesis test where the null hypothesis is that the data is generated by a Poisson point process and are thus uniformly randomly distributed. A value close to `1` tends to indicate the data is `highly clustered`, `random data` will tend to result in values around `0.5`, and uniformly distributed data will tend to result in values close to `0`.","9ca25676":"### If this Kernel helped you in any way, some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated.","bce911c6":"<a id=\"4\"><\/a> <br>\n## Step 4 : Data Preparation","bda77c63":"#### Inference:\n- None of the columns have null values hence no imputation or drop required.","3077801a":"Since 90% variance is explained by 3 principal components, lets build the dataframe using those 3 components only.","97c611a0":"## Business Goal","48b0c487":"**Complete Linkage<br>**\n\nIn complete linkage hierarchical clustering, the distance between two clusters is defined as the longest distance between two points in each cluster. For example, the distance between clusters \u201cr\u201d and \u201cs\u201d to the left is equal to the length of the arrow between their two furthest points. \n![](https:\/\/www.saedsayad.com\/images\/Clustering_complete.png)","3ce5cb41":"#### Inference:\nCluster `4` seems to be not properly formed in Plot 1.","6c9abea0":"<a id=\"7\"><\/a> <br>\n## Step 7 : Model Building","bdf7dd6e":"#### Inference:\n- Here also we got the same issue as with 4 clusters but we got a new segment, so lets proceed with K means using 5 clusters.","b60614ba":"#### Inference:\n- With first component variance explained is almost 60%.\n- For second component variance explained is almost 20%.","d44bade0":"We have removed few countries during outlier treatment but we might have dropped some countries which might be in need of help.\nLet's iterate our final list based on the information from the clusters which were in need of aid.ie, `Cluster 3` and `Cluster 4`","b573b79a":"#### Inference:\nThe mean values suggests the same story as above","c147966b":"![](https:\/\/www.iagua.es\/sites\/default\/files\/styles\/share-fb-830x436\/public\/images\/blogs\/redaccion\/consuelo_mora_agua_potable_pobreza.JPG?itok=KkAP7WxH)","5a666af2":"#### Outlier Analysis and Treatment\n\nThere are 2 types of outliers and we will treat outliers as it can skew our dataset\u00b6\n- Statistical\n- Domain specific","c2221446":"We have visualized the data on the principal components and saw some good clusters were formed but some were not so good hence let's now visualize the data on the original attributes.","e73ff7eb":"#### Inference:\n- `child_mortality` and `life_expentency` are highly correlated with correlation of `-0.89`\n- `child_mortality` and `total_fertility` are highly correlated with correlation of `0.85`\n- `imports` and `exports` are highly correlated with correlation of `0.74`\n- `life_expentency` and `total_fertility` are highly correlated with correlation of `-0.76`","2a50eb74":"#### Inference:\n0.83 is a good Hopkins score for Clustering.","71b90360":"### Closing Statement","ba90cfd0":"#### Inference:\n- None of the columns have inconsistent datatype, hence no conversion is required.","fcd62345":"#### Inference:\nIt is evident from the above Scree plot that more than 90% variance is explained by the first 3 principal components.\nHence, we will use these components only going forward for Clustering process.","158a1147":"HELP International is an international humanitarian NGO that is committed to fighting poverty and providing the people of backward countries with basic amenities and relief during the time of disasters and natural calamities. It runs a lot of operational projects from time to time along with advocacy drives to raise awareness as well as for funding purposes.\n\n \n\nAfter the recent funding programmes, they have been able to raise around $ 10 million. Now the CEO of the NGO needs to decide how to use this money strategically and effectively. The significant issues that come while making this decision are mostly related to choosing the countries that are in the direst need of aid.","c2ee8550":"### K- means Clustering\n\nK-means clustering is one of the simplest and popular unsupervised machine learning algorithms.\n\nThe algorithm works as follows:\n\nFirst we initialize k points, called means, randomly.\nWe categorize each item to its closest mean and we update the mean\u2019s coordinates, which are the averages of the items categorized in that mean so far.\nWe repeat the process for a given number of iterations and at the end, we have our clusters.","01ab1a47":"<a id=\"6\"><\/a> <br>\n## Step 6 : Hopkins Statistics Test","7d5d0181":"**Single Linkage:<br>**\n\nIn single linkage hierarchical clustering, the distance between two clusters is defined as the shortest distance between two points in each cluster. For example, the distance between clusters \u201cr\u201d and \u201cs\u201d to the left is equal to the length of the arrow between their two closest points.\n![](https:\/\/www.saedsayad.com\/images\/Clustering_single.png)","aba39e41":"Hierarchical clustering involves creating clusters that have a predetermined ordering from top to bottom. For example, all files and folders on the hard disk are organized in a hierarchy. There are two types of hierarchical clustering,\n\n- Divisive\n- Agglomerative.","9ec8bf09":"#### Elbow Curve to get the right number of Clusters\nA fundamental step for any unsupervised algorithm is to determine the optimal number of clusters into which the data may be clustered. The Elbow Method is one of the most popular methods to determine this optimal value of k.","f4accff3":"*The dataset seems to be almost clean and hence no such cleansing activities are required.*","955cc24f":"### Hierarchical Clustering","38995458":"**We have analyzed both K-means and Hierarchial clustering and found clusters formed are not identical.\nThe clusters formed in both the cases are not that great but its better in K-means as compared to Hierarchial.\nSo, we will proceed with the clusters formed by K-means and based on the information provided by the final clusters we will deduce the final list of countries which are in need of aid.**","1069e024":"<a id=\"8\"><\/a> <br>\n## Step 8 : Final Analysis","664660be":"### Silhouette Analysis\n\n$$\\text{silhouette score}=\\frac{p-q}{max(p,q)}$$\n\n$p$ is the mean distance to the points in the nearest cluster that the data point is not a part of\n\n$q$ is the mean intra-cluster distance to all the points in its own cluster.\n\n* The value of the silhouette score range lies between -1 to 1. \n\n* A score closer to 1 indicates that the data point is very similar to other data points in the cluster, \n\n* A score closer to -1 indicates that the data point is not similar to the data points in its cluster.","ee9137b5":"# Assignment : Clustering and PCA","03753d20":"#### Inference:\nAs we can see from above heatmap that the correlation among the attributes is almost `0`, we can proceed with this dataframe.","929dee1c":"#### Inference:\n- `inflation` is well explained by PC3.","c7233a32":"## Problem Statement","fcc846ee":"<a id=\"1\"><\/a> <br>\n## Step 1: Reading and Understanding the Data","6240d13b":"#### Final List of countries which are in need of the aid based on socio-economic factors.","06dccc53":"We got Cluster `0` and Cluster `3` which are in need of aid.","d721fc3a":"#### Below are the steps which we will be basically following:\n\n1. [Step 1: Reading and Understanding the Data](#1)\n1.  [Step 2: Data Cleansing](#2)\n    - Missing Value check\n    - Data type check\n    - Duplicate check\n1. [Step 3: Data Visualization](#3)\n    - Heatmap\n    - Pairplot\n1. [Step 4: Data Preparation](#4) \n   - Rescaling\n1. [Step 5: PCA Application](#5)\n   - Principal Components Selection\n   - Outlier Analysis and Treatment\n1. [Step 6: Hopkins Statistics Test](#6)\n   - Hopkins Score Calculation\n1. [Step 7: Model Building](#7)\n   - K-means Clustering\n   - Elbow Curve\n   - Silhouette Analysis\n   - Hierarchial Clustering\n1. [Step 8: Final Analysis](#8)\n   - Final Country list Preparation       ","acb5ce15":"#### Inference:\nIt seems there are good number of countries in each clusters.","4c0d4636":"#### Inference:\n\nLooking at the above elbow curve it looks good to proceed with either 4 or 5 clusters.","1237e250":"**Finding the Optimal Number of Clusters**","2c0912c9":"#### If this Kernel helped you in any way, some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated.","45726d26":"Our job is to categorise the countries using some socio-economic and health factors that determine the overall development of the country. Then you need to suggest the countries which the CEO needs to focus on the most.","2206cd7d":"#### Inference:\nIt seems there are good number of countries in each clusters.","63af4088":"#### Inference:\n- `life expectency`, `income`, `gdpp` and `health` are very well explained by PC1.\n- `imports` and `exports` are well explained by both the components PC1 and PC2.\n- `child mortality` and `total fertility` are well explained by PC2.\n- `inflation` is neither explained by PC1 nor with PC2","dda95b40":"We need to do some basic cleansing check in order to feed our model the correct data.","d3dadb52":"#### Rescaling the Features\nMost software packages use SVD to compute the principal components and assume that the data is scaled and centred, so it is important to do standardisation\/normalisation.\nThere are two common ways of rescaling:\n\n1. Min-Max scaling<br>\n2. Standardisation (mean-0, sigma-1)\n\nHere, we will use Standardisation Scaling.","17f17d97":"<a id=\"2\"><\/a> <br>\n## Step 2 : Data Cleansing","ce926c3e":"#### Inference:\n- Child Mortality is highest for Cluster `0`  and Cluster `3`.These clusters need some aid.\n- Income and Gdpp are measures of development. Higher the per capita income and gdpp better is the country's development.\n  Income per capita and gdpp seems lowest for countries in clusters `0` and `3`. Hence, these countries need some help.","ddcb3310":"<a id=\"3\"><\/a> <br>\n## Step 3 : Data Visualization","df841d48":"**Derived Metrices**<br>\n\nDeducing imports,exports and health spending from percentage values to actual values of their GDP per capita .Because the percentage values don't give a clear picture of that country. For example Austria and Belarus have almost same exports % but their gdpp has a huge gap which doesn't give an accurate idea of which country is more developed than the other.","9dba3cc1":"#### This kernel is based on the assignment by IIITB collaborated with upgrad.","9fe63b04":"Now lets apply Hierarchial Clustering to see if we get any better clusters or not.","9535a714":"#### Inference:\n- In plot 1, it seems lot of intra-distance between the cluster elements, which is not a good sign.","b7150040":"<a id=\"5\"><\/a> <br>\n## Step 5 : PCA Application\n\nWe are doing PCA because we want to remove the redundancies in the data and find the most important directions where the data was aligned. A somewhat similar heuristic is also used by the United Nations to calculate the Human Development Index(HDI) to rank countries on the basis of their development. \n\nPrincipal component analysis (PCA) is one of the most commonly used `dimensionality reduction` techniques in the industry. By converting large data sets into smaller ones containing fewer variables, it helps in improving model performance, visualising complex data sets, and in many more areas.\n\nLet's use PCA for dimensionality reduction as from the heatmap it is evident that correlation exists between the attributes.","868cd77e":"We have used PCA above to reduce the variables involved and then done the clustering of countries based on those Principal components\nand then later we identified few factors like `child mortality`, `income` etc which plays a vital role in deciding the development status of the country and builded clusters of countries based on that.\nBased on those clusters we have identified the below list of countries which are in dire need of aid.\nThe list of countries are subject to change as it is based on the few factors like `Number of components chosen`,\n`Number of Clusters chosen`, `Clustering method used` etc.which we have used to build the model. "}}