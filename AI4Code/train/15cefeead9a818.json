{"cell_type":{"e7c51cd7":"code","4b0d6e57":"code","d7344955":"code","e5b5ada4":"code","bf7ca329":"code","2c6f5436":"code","d72b9745":"code","b70e9c54":"code","7ce87bb0":"code","22921611":"code","977af463":"code","4b2ed9d8":"code","e15e9058":"code","4835be2b":"code","3228e78e":"code","56521d86":"code","5a67c793":"code","41379709":"code","fe9d7d4b":"code","b2e201a8":"code","9318f942":"code","7c2e70e1":"code","7f2ba337":"code","8d5a179c":"code","7b44b78c":"code","b6df36d7":"code","8c0fa13c":"code","f1f12726":"code","0c3367df":"code","eddba2f1":"code","abbddaef":"code","b3f16360":"code","31e1cf47":"code","0110f148":"code","a904a19a":"code","a036c803":"code","ffdbfa19":"code","f72d6b89":"code","2464c22c":"code","a91e7893":"code","6db1885c":"code","38260aa6":"code","aa03fe95":"code","eb5cbd25":"code","39e335c2":"code","1b9da325":"code","9ea0725d":"markdown","82fa4301":"markdown"},"source":{"e7c51cd7":"import keras \n\nimport pandas as pd\nimport numpy as np\nfrom numpy.random import seed\nseed(101)\n\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nimport os\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n\n#Menentukan letak path data\nprint('List folder pada path input')\nos.listdir('..\/input')","4b0d6e57":"df_data = pd.read_csv('..\/input\/metadata\/metadata.csv')\n\nprint('Metadata dataset')\ndf_data.head()","d7344955":"# Class yang terdapat pada dataset\nlesion_type_dict = {\n    'nv': 'Melanocytic nevi',\n    'mel': 'Melanoma',\n    'bkl': 'Benign keratosis ',\n    'bcc': 'Basal cell carcinoma',\n    'akiec': 'Actinic keratoses',\n    'vasc': 'Vascular lesions',\n    'df': 'Dermatofibroma'\n}\ndf_data['lesion']= df_data.dx.map(lesion_type_dict)","e5b5ada4":"print('Kelas yang terdapat pada dataset awal')\nprint(df_data['dx'].unique())\nprint(df_data.lesion.value_counts())","bf7ca329":"import matplotlib.pyplot as plt\n\ndf_data['dx'].value_counts().plot.bar(rot=0)\nplt.title('Jumlah citra untuk tipe class penyakit yang berbeda')\nplt.xlabel('Class Penyakit')\nplt.ylabel('Jumlah')\nplt.grid(axis='y')","2c6f5436":"# Memberi informasi berapa banyak citra yang dikaitkan dengan setiap lesion_id\ndf = df_data.groupby('lesion_id').count()\n\n# Memfilter lesion_id yang hanya memiliki satu citra yang terkait dengannya\ndf = df[df['image_id'] == 1]\n\ndf.reset_index(inplace=True)\n\ndf.head(20)","d72b9745":"# identifikasi lesion_id yg mempunyai duplikat citra atau tidak.\n\ndef identify_duplicates(x):\n    \n    unique_list = list(df['lesion_id'])\n    \n    if x in unique_list:\n        return 'no_duplicates'\n    else:\n        return 'has_duplicates'\n    \n# buat kolom baru yang merupakan salinan dari kolom lesi _id\ndf_data['duplicates'] = df_data['lesion_id']\n# terapkan fungsi ke kolom baru ini\ndf_data['duplicates'] = df_data['duplicates'].apply(identify_duplicates)\n\ndf_data.head(30)","b70e9c54":"df_data['duplicates'].value_counts()","7ce87bb0":"# filter citra yang tidak memiliki duplikat\ndf = df_data[df_data['duplicates'] == 'no_duplicates']\n\nprint('Citra yang tidak memiliki duplikat berjumlah')\ndf.shape","22921611":"# df yang telah dipastikan tidak memiliki duplikat displit kemudian dijadikan set val (validasi)\ny = df['dx']\n\nimport tensorflow\nfrom sklearn.model_selection import train_test_split\n_, df_val = train_test_split(df, test_size=0.17, random_state=101, stratify=y)\n\n#train_size -> If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split\n#randostate -> If int, random_state is the seed used by the random number generator\n#stratif -> If not None, data is split in a stratified fashion\n\nprint('Jumlah citra yang akan dijadikan dataset validasi')\ndf_val.shape","977af463":"#Membuat set train yg tidak termasuk images yg ada di set val\n\n#Fungsi ini mengidentifikasi apakah gambar adalah bagian dari set train atau set val\ndef identify_val_rows(x):\n    # create a list of all the lesion_id's in the val set\n    val_list = list(df_val['image_id'])\n    \n    if str(x) in val_list:\n        return 'val'\n    else:\n        return 'train'\n# buat kolom baru yang merupakan salinan dari kolom image_id\ndf_data['train_or_val'] = df_data['image_id']\n# terapkan fungsi ke kolom baru ini\ndf_data['train_or_val'] = df_data['train_or_val'].apply(identify_val_rows)\n# filter baris set train\ndf_train = df_data[df_data['train_or_val'] == 'train']\n\nprint('Jumlah citra yang akan dijadikan set train:')\nprint(len(df_train))\nprint('Jumlah citra yang akan dijadikan set validasi:')\nprint(len(df_val))","4b2ed9d8":"print('Jumlah citra tiap class yang akan dijadikan set train sebelum augmanted')\nprint(df_train['dx'].value_counts())","e15e9058":"print('Jumlah citra tiap class yang akan dijadikan set validas')\nprint(df_val['dx'].value_counts())","4835be2b":"# cek berapa banyak image di set train setiap class \nprint('Jumlah data citra setelah dilakukan Augmanted')\nprint(len(os.listdir('..\/input\/basedir\/base_dir\/base_dir\/train_dir\/nv')))\nprint(len(os.listdir('..\/input\/basedir\/base_dir\/base_dir\/train_dir\/mel')))\nprint(len(os.listdir('..\/input\/basedir\/base_dir\/base_dir\/train_dir\/bkl')))\nprint(len(os.listdir('..\/input\/basedir\/base_dir\/base_dir\/train_dir\/bcc')))\nprint(len(os.listdir('..\/input\/basedir\/base_dir\/base_dir\/train_dir\/akiec')))\nprint(len(os.listdir('..\/input\/basedir\/base_dir\/base_dir\/train_dir\/vasc')))\nprint(len(os.listdir('..\/input\/basedir\/base_dir\/base_dir\/train_dir\/df')))","3228e78e":"#Mulai membuat model\n\ntrain_path = '..\/input\/basedir\/base_dir\/base_dir\/train_dir'\nvalid_path = '..\/input\/basedir\/base_dir\/base_dir\/val_dir'\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\nimage_size = 224\n\ntrain_steps = 3857\nval_steps = np.ceil(num_val_samples \/ val_batch_size)","56521d86":"print(train_steps)\nprint(val_steps)","5a67c793":"datagen = ImageDataGenerator(\n    preprocessing_function= \\\n    keras.applications.mobilenet.preprocess_input)\n\ntrain_batches = datagen.flow_from_directory(train_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=train_batch_size)\n\nvalid_batches = datagen.flow_from_directory(valid_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=val_batch_size)\n\n# Note: shuffle=False causes the test dataset to not be shuffled\ntest_batches = datagen.flow_from_directory(valid_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=1,\n                                            shuffle=False)","41379709":"#Model MobileNet\nmobile = keras.applications.mobilenet.MobileNet()","fe9d7d4b":"mobile.summary()","b2e201a8":"type(mobile.layers)","9318f942":"# layer yang dimiliki MobileNet\nprint('Jumlah Layer pada pretained MobileNet')\nlen(mobile.layers)","7c2e70e1":"# MEMBUAT MODEL ARCHITECTURE\n\n# Exclude 5 layer terakhir dari model di atas.\n# Mencakup semua layer sampai layer global_average_pooling2d_1\nx = mobile.layers[-6].output\n\n# Membuat layer Dense baru untuk prediksi\n# 7 corresponds sesuai dengan classs yg dimiliki\nx = Dropout(0.25)(x)\npredictions = Dense(7, activation='softmax')(x)\n\n# input = mobile.input memilih layer input \n# output = prediksi mengacu pada Dense layer yang dibuat di atas.\n\nmodel = Model(inputs=mobile.input, outputs=predictions)","7f2ba337":"model.summary()","8d5a179c":"# layer yang dimiliki model\nlen(model.layers)","7b44b78c":"# memilih berapa banyak layer yang sebenarnya ingin di-train\n\n# Di sini kita mem-freez weight semua lapisan kecuali 23 lapisan terakhir dalam model baru\n# 23 lapisan terakhir dari model akan dilatih.\n\nfor layer in model.layers[:-23]:\n    layer.trainable = False","b6df36d7":"#TRAIN MODEL\n# Mendifinisikan Top2 dan Top3 Accuracy\n\nfrom keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n\ndef top_3_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n\ndef top_2_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=2)","8c0fa13c":"model.compile(Adam(lr=0.01), loss='categorical_crossentropy', \n              metrics=[categorical_accuracy, top_2_accuracy, top_3_accuracy])","f1f12726":"# Mendapatkan labels yang terkait dengan setiap indeks\nprint(valid_batches.class_indices)","0c3367df":"# Tambahkan bobot untuk mencoba membuat model lebih sensitif terhadap melanoma\n\nclass_weights={\n    0: 1.0, # akiec\n    1: 1.0, # bcc\n    2: 1.0, # bkl\n    3: 1.0, # df\n    4: 3.0, # mel # Try to make the model more sensitive to Melanoma.\n    5: 1.0, # nv\n    6: 1.0, # vasc\n}","eddba2f1":"filepath = \"model04-v5.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_top_3_accuracy', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_top_3_accuracy', factor=0.5, patience=5, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(train_batches, steps_per_epoch=train_steps, \n                              class_weight=class_weights,\n                    validation_data=valid_batches,\n                    validation_steps=val_steps,\n                    epochs=30, verbose=1,\n                   callbacks=callbacks_list)","abbddaef":"# get the metric names so we can use evaulate_generator\nmodel.metrics_names","b3f16360":"# Here the the last epoch will be used.\n\nval_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\nmodel.evaluate_generator(test_batches, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_cat_acc:', val_cat_acc)\nprint('val_top_2_acc:', val_top_2_acc)\nprint('val_top_3_acc:', val_top_3_acc)","31e1cf47":"# Here the best epoch will be used.\n\nmodel.load_weights('model04-v5.h5')\n\nval_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\nmodel.evaluate_generator(test_batches, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_cat_acc:', val_cat_acc)\nprint('val_top_2_acc:', val_top_2_acc)\nprint('val_top_3_acc:', val_top_3_acc)","0110f148":"import matplotlib.pyplot as plt\nacc = history.history['categorical_accuracy']\nval_acc = history.history['val_categorical_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\ntrain_top2_acc = history.history['top_2_accuracy']\nval_top2_acc = history.history['val_top_2_accuracy']\ntrain_top3_acc = history.history['top_3_accuracy']\nval_top3_acc = history.history['val_top_3_accuracy']\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, '', label='Training loss')\nplt.plot(epochs, val_loss, '', label='Validation loss')\nplt.title('MobileNet -- Training dan Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc, '', label='Training cat accuracy')\nplt.plot(epochs, val_acc, '', label='Validation cat accuracy')\nplt.title('MobileNet -- Training dan Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, train_top2_acc, '', label='Training top2 acc')\nplt.plot(epochs, val_top2_acc, '', label='Validation top2 acc')\nplt.title('TOP 2 - Training dan Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, train_top3_acc, '', label='Training top3 acc')\nplt.plot(epochs, val_top3_acc, '', label='Validation top3 acc')\nplt.title('TOP 3 - Training dan Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()","a904a19a":"test_labels = test_batches.classes","a036c803":"test_labels","ffdbfa19":"test_batches.class_indices","f72d6b89":"predictions = model.predict_generator(test_batches, steps=len(df_val), verbose=1)","2464c22c":"predictions.shape","a91e7893":"# Source: Scikit Learn website\n# http:\/\/scikit-learn.org\/stable\/auto_examples\/\n# model_selection\/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n# selection-plot-confusion-matrix-py\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","6db1885c":"test_labels.shape","38260aa6":"# argmax returns the index of the max value in a row\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))","aa03fe95":"test_batches.class_indices","eb5cbd25":"# Define the labels of the class indices. These need to match the \n# order shown above.\ncm_plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'mel','nv', 'vasc']\n\nplot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')","39e335c2":"# Get the index of the class with the highest probability score\ny_pred = np.argmax(predictions, axis=1)\n\n# Get the labels of the test images.\ny_true = test_batches.classes","1b9da325":"from sklearn.metrics import classification_report\n\n# Generate a classification report\nreport = classification_report(y_true, y_pred, target_names=cm_plot_labels)\n\nprint(report)","9ea0725d":"**Fungsi-fungsi *libraries* yang digunakan**\n1. keras = menggunakan *backend* keras. <br>\n2. pandas = *toolkit* analisis data,  pada fitur dataframe dapat membaca sebuah file berformat csv dan menjadikannya tabel.\n3. numpy = untuk operasi vektor dan matriks. Fiturnya hampir sama dengan MATLAB dalam mengelola array dan array multidimensi. Numpy merupakan salah satu library yang digunakan oleh library lain seperti Scikit-Learn untuk keperluan analisis data.\n4. [Dense pada keras](https:\/\/keras.io\/layers\/core\/#dense) (Fully Connected Layers) = Regular layer neuron dalam Neural Network. Setiap neuron menerima input dari semua neuron di lapisan sebelumnya, sehingga disebut *'densely connected'*.\n5. [Dropout ](http:\/\/jmlr.org\/papers\/v15\/srivastava14a.html)= teknik untuk mengatasi masalah *overfitting*. Main ide-nya adalah untuk secara acak *dropout* unit (bersama dengan koneksi mereka) dari neural network selama pelatihan. Ini mencegah unit terlalu banyak melakukan adaptasi. \n6. [Adam](https:\/\/arxiv.org\/abs\/1412.6980) = *optimizer* yang digunakan, algoritma optimisasi yang dapat digunakan sebagai ganti dari prosedur penurunan gradien stokastik klasik untuk memperbarui *network weight* yang berulang berdasarkan data pelatihan.\n7. [Categorical Crossentropy](https:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/loss_functions.html#id11) = mengukur kinerja model klasifikasi yang outputnya merupakan nilai probabilitas antara 0 dan 1. *Cross-entropy loss* meningkat karena probabilitas yang diprediksi menyimpang dari label aktual. Jadi memprediksi probabilitas 0,012 ketika label observasi aktual adalah 1 maka menghasilkan *loss value* yang tinggi.\n8. Image Data Generator = berfungsi untuk *Augmanted* (menambahkan) data\n9. Model = Susunan layer pada CNN. Dalam proses training menggunakan *pretrained model* yaitu mobilenet v1.\n10. [Early Stopping, ReduceLROnPlateau](https:\/\/keras.io\/callbacks\/#callback) = Terlalu banyak epoch dapat menyebabkan *overfitting* dari dataset train, sedangkan terlalu sedikit dapat menghasilkan model yg *underfitting*.  \n11. \n12. ModelCheckpoint \n13. os\n14. Sklearn confusion_matrix\n15. Sklearn train_test_split\n16. itertools\n17. shutil\n18. matplotlib = ","82fa4301":"# Klasifikasi Penyakit Kulit Menggunakan CNN (MobileNet)\n> Kanker kulit merupakan salah satu penyakit pada kelenjar kulit yang paling mematikan. Kanker kulit dapat didiagnosis secara visual, melalui cara skrining klinis awal disertai oleh analisis dermoskopik, biopsi, dan pemeriksaan histopatologis. Dalam program ini akan dijelaskan klasifikasi lesi kulit secara otomatis menggunakan Convolutional Neural Network (CNN). <br>\n\n> Dataset yang digunakan pada program ini adalah HAM10000, terdiri dari 10015 citra dermatoskopik dengan ukuran (600 x450) pxl yang dirilis oleh [Harvard Dataserve](https:\/\/dataverse.harvard.edu\/dataset.xhtml?persistentId=doi:10.7910\/DVN\/DBW86T) sebagai set pelatihan untuk tujuan pembelajaran mesin dan akademik. Dataset ini tersedia untuk umum melalui arsip [International Skin Imaging Collaboration (ISIC)](https:\/\/challenge2018.isic-archive.com\/).\n\n**HAM10000 memiliki 7 *class* kanker kulit yang tercantum di bawah ini:**\n1. Melanocytic Nevi (nv)\n2. Melanoma (mel)\n3. Benign keratosis-like lesions (bkl) \n4. Basal cell carcinoma (bcc)\n5. Actinic keratoses (akiec)\n6. Vascular lesions (vasc)\n7. Dermatofibroma (df)\n\n**Program ini akan mencoba untuk mendeteksi 7 kelas kanker kulit yang berbeda menggunakan MobileNet dengan backend keras native dan kemudian menganalisis hasilnya untuk melihat bagaimana model dapat berguna dalam skenario praktis.**\n\n**Catatan penting pada versi ini:**\n1. Train steps (iterasi) dalam versi ini dirubah dari semula berukuran 907 yang sesuai dengan jumlah citra sebelum augmented yaitu 9.078 diganti dengan ukuran 3.857 sesuai dengan jumlah citra setelah data augmanted menggunakan rumus *np.ceil(num_train_samples \/ train_batch_size)* \n"}}