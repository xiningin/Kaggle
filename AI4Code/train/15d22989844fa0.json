{"cell_type":{"564b11da":"code","93e78736":"code","ca9e9715":"code","00e5e367":"code","421e6a6c":"code","7e7a090b":"code","e51cbcdb":"code","8c03406f":"code","a74b4dbb":"code","3704939f":"code","1655155a":"code","19b6cd90":"code","0065ae1b":"code","510499ff":"code","5f3239b4":"code","b5e18ff9":"code","a46bd00a":"code","c73e9cd9":"code","e96a462a":"code","f4d69d9f":"code","28a046f8":"code","387e2c1e":"code","e9965d6b":"code","5cd11ce8":"code","4915dccc":"code","8ae076b0":"code","38453f0d":"code","a42d6e6c":"code","21c5f074":"code","4b23925a":"code","7c1cf261":"code","6059cdeb":"code","b7be698c":"code","db1e7857":"code","98b2bc5b":"code","e257badd":"code","226f1d72":"code","868f85d0":"code","7f15fcee":"code","b16aa372":"code","da10f7dc":"code","f582308c":"code","88f80f39":"code","984f20e6":"code","59331739":"code","e939074c":"code","8ebacb9b":"code","34e217d5":"code","fd9e8c45":"code","2311e2e8":"code","d549fb67":"code","4b0fad10":"code","3bdb2da1":"code","4840e4b4":"code","6996445b":"code","88af2119":"code","6d1b6068":"markdown","bb152aac":"markdown","d73455e9":"markdown","4989ea42":"markdown","52e975bd":"markdown","372cf070":"markdown","ed8f7383":"markdown","52c5ff74":"markdown","dc7e063b":"markdown"},"source":{"564b11da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","93e78736":"import nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\n\n# Loading some sklearn packaces for modelling.\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\nfrom sklearn.metrics import f1_score, accuracy_score\n\n# Some packages for word clouds and NER.\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter, defaultdict\nfrom PIL import Image\nimport spacy\n!pip install https:\/\/github.com\/explosion\/spacy-models\/releases\/download\/en_core_web_sm-2.2.5\/en_core_web_sm-2.2.5.tar.gz\nimport en_core_web_sm\n\n# Core packages for general use throughout the notebook.\n\nimport random\nimport warnings\nimport time\nimport datetime\n\n# For customizing our plots.\n\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\n\n# Loading pytorch packages.\n\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n\n# Setting some options for general use.\n\nstop = set(stopwords.words('english'))\n# plt.style.use('fivethirtyeight')\nsns.set(font_scale=1.5)\npd.options.display.max_columns = 250\npd.options.display.max_rows = 250\nwarnings.filterwarnings('ignore')\n\n\n#Setting seeds for consistent results.\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport torch\nimport transformers as ppb\nimport warnings\nwarnings.filterwarnings('ignore')","ca9e9715":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown(string))\n#printmd('**bold**')","00e5e367":"# Loading the train and test data for visualization & exploration.\n\ntrainv = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntestv = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","421e6a6c":"# Taking general look at the both datasets.\n\ndisplay(trainv.sample(5))\ndisplay(testv.sample(5))","7e7a090b":"# Checking observation and feature numbers for train and test data.\n\nprint(trainv.shape)\nprint(testv.shape)","e51cbcdb":"import re\nimport string","8c03406f":"# Some basic helper functions to clean text by removing urls, emojis, html tags and punctuations.\n\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n# Applying helper functions\n\ntrainv['text_clean'] = trainv['text'].apply(lambda x: remove_URL(x))\ntrainv['text_clean'] = trainv['text_clean'].apply(lambda x: remove_emoji(x))\ntrainv['text_clean'] = trainv['text_clean'].apply(lambda x: remove_html(x))\ntrainv['text_clean'] = trainv['text_clean'].apply(lambda x: remove_punct(x))\n","a74b4dbb":"# Tokenizing the tweet base texts.\n\ntrainv['tokenized'] = trainv['text_clean'].apply(word_tokenize)\n\ntrainv.head()","3704939f":"# Lower casing clean text.\n\ntrainv['lower'] = trainv['tokenized'].apply(\n    lambda x: [word.lower() for word in x])\n\ntrainv.head()","1655155a":"# Removing stopwords.\n\ntrainv['stopwords_removed'] = trainv['lower'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ntrainv.head()\n","19b6cd90":"# Applying part of speech tags.\n\ntrainv['pos_tags'] = trainv['stopwords_removed'].apply(nltk.tag.pos_tag)\n\ntrainv.head()","0065ae1b":"# Converting part of speeches to wordnet format.\n\ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\n\ntrainv['wordnet_pos'] = trainv['pos_tags'].apply(\n    lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n\ntrainv.head()\n","510499ff":"# Applying word lemmatizer.\n\nwnl = WordNetLemmatizer()\n\ntrainv['lemmatized'] = trainv['wordnet_pos'].apply(\n    lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n\ntrainv['lemmatized'] = trainv['lemmatized'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ntrainv['lemma_str'] = [' '.join(map(str, l)) for l in trainv['lemmatized']]\n\ntrainv.head()","5f3239b4":"# Displaying target distribution.\n\nfig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), dpi=100)\nsns.countplot(trainv['target'], ax=axes[0])\naxes[1].pie(trainv['target'].value_counts(),\n            labels=['Not Disaster', 'Disaster'],\n            autopct='%1.2f%%',\n            shadow=True,\n            explode=(0.05, 0),\n            startangle=60)\nfig.suptitle('Distribution of the Tweets', fontsize=24)\nplt.show()","b5e18ff9":"# Creating a new feature for the visualization.\n\ntrainv['Character Count'] = trainv['text_clean'].apply(lambda x: len(str(x)))\n\n\ndef plot_dist3(df, feature, title):\n    # Creating a customized chart. and giving in figsize and everything.\n    fig = plt.figure(constrained_layout=True, figsize=(18, 8))\n    # Creating a grid of 3 cols and 3 rows.\n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    # Customizing the histogram grid.\n    ax1 = fig.add_subplot(grid[0, :2])\n    # Set the title.\n    ax1.set_title('Histogram')\n    # plot the histogram.\n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 ax=ax1,\n                 color='#e74c3c')\n    ax1.set(ylabel='Frequency')\n    ax1.xaxis.set_major_locator(MaxNLocator(nbins=20))\n\n    # Customizing the ecdf_plot.\n    ax2 = fig.add_subplot(grid[1, :2])\n    # Set the title.\n    ax2.set_title('Empirical CDF')\n    # Plotting the ecdf_Plot.\n    sns.distplot(df.loc[:, feature],\n                 ax=ax2,\n                 kde_kws={'cumulative': True},\n                 hist_kws={'cumulative': True},\n                 color='#e74c3c')\n    ax2.xaxis.set_major_locator(MaxNLocator(nbins=20))\n    ax2.set(ylabel='Cumulative Probability')\n\n    # Customizing the Box Plot.\n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    ax3.set_title('Box Plot')\n    # Plotting the box plot.\n    sns.boxplot(x=feature, data=df, orient='v', ax=ax3, color='#e74c3c')\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=25))\n\n    plt.suptitle(f'{title}', fontsize=24)\n","a46bd00a":"plot_dist3(trainv[trainv['target'] == 0], 'Character Count',\n           'Characters Per \"Non Disaster\" Tweet')","c73e9cd9":"plot_dist3(trainv[trainv['target'] == 1], 'Character Count',\n           'Characters Per \"Disaster\" Tweet')","e96a462a":"def plot_word_number_histogram(textno, textye):\n    \n    \"\"\"A function for comparing word counts\"\"\"\n\n    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), sharey=True)\n    sns.distplot(textno.str.split().map(lambda x: len(x)), ax=axes[0], color='#e74c3c')\n    sns.distplot(textye.str.split().map(lambda x: len(x)), ax=axes[1], color='#e74c3c')\n    \n    axes[0].set_xlabel('Word Count')\n    axes[0].set_ylabel('Frequency')\n    axes[0].set_title('Non Disaster Tweets')\n    axes[1].set_xlabel('Word Count')\n    axes[1].set_title('Disaster Tweets')\n    \n    fig.suptitle('Words Per Tweet', fontsize=24, va='baseline')\n    \n    fig.tight_layout()","f4d69d9f":"plot_word_number_histogram(trainv[trainv['target'] == 0]['text'],\n                           trainv[trainv['target'] == 1]['text'])","28a046f8":"def plot_word_len_histogram(textno, textye):\n    \n    \"\"\"A function for comparing average word length\"\"\"\n    \n    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), sharey=True)\n    sns.distplot(textno.str.split().apply(lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x)),\n                 ax=axes[0], color='#e74c3c')\n    sns.distplot(textye.str.split().apply(lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x)),\n                 ax=axes[1], color='#e74c3c')\n    \n    axes[0].set_xlabel('Word Length')\n    axes[0].set_ylabel('Frequency')\n    axes[0].set_title('Non Disaster Tweets')\n    axes[1].set_xlabel('Word Length')\n    axes[1].set_title('Disaster Tweets')\n    \n    fig.suptitle('Mean Word Lengths', fontsize=24, va='baseline')\n    fig.tight_layout()","387e2c1e":"plot_word_len_histogram(trainv[trainv['target'] == 0]['text'],\n                        trainv[trainv['target'] == 1]['text'])","e9965d6b":"lis = [\n    trainv[trainv['target'] == 0]['lemma_str'],\n    trainv[trainv['target'] == 1]['lemma_str']\n]","5cd11ce8":"fig, axes = plt.subplots(1, 2, figsize=(18, 8))\naxes = axes.flatten()\n\nfor i, j in zip(lis, axes):\n    try:\n        new = i.str.split()\n        new = new.values.tolist()\n        corpus = [word.lower() for i in new for word in i]\n        dic = defaultdict(int)\n        for word in corpus:\n            if word in stop:\n                dic[word] += 1\n\n        top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:15]\n        x, y = zip(*top)\n        df = pd.DataFrame([x, y]).T\n        df = df.rename(columns={0: 'Stopword', 1: 'Count'})\n        sns.barplot(x='Count', y='Stopword', data=df, palette='plasma', ax=j)\n        plt.tight_layout()\n    except:\n        plt.close()\n        print('No stopwords left in texts.')\n        break","4915dccc":"# Displaying most common words.\n\nfig, axes = plt.subplots(1, 2, figsize=(18, 8))\naxes = axes.flatten()\n\nfor i, j in zip(lis, axes):\n\n    new = i.str.split()\n    new = new.values.tolist()\n    corpus = [word for i in new for word in i]\n\n    counter = Counter(corpus)\n    most = counter.most_common()\n    x, y = [], []\n    for word, count in most[:30]:\n        if (word not in stop):\n            x.append(word)\n            y.append(count)\n\n    sns.barplot(x=y, y=x, palette='plasma', ax=j)\naxes[0].set_title('Non Disaster Tweets')\n\naxes[1].set_title('Disaster Tweets')\naxes[0].set_xlabel('Count')\naxes[0].set_ylabel('Word')\naxes[1].set_xlabel('Count')\naxes[1].set_ylabel('Word')\n\nfig.suptitle('Most Common Unigrams', fontsize=24, va='baseline')\nplt.tight_layout()","8ae076b0":"train_df = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\nsample = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","38453f0d":"# For DistilBERT:\n#model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n\n## Want BERT instead of distilBERT? Uncomment the following line:\nmodel_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n\n# Load pretrained model\/tokenizer\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\nmodel = model_class.from_pretrained(pretrained_weights)","a42d6e6c":"train_df.head()","21c5f074":"# If there's a GPU available...\n\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.  \n    \n    device = torch.device('cuda')    \n\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device('cpu')","4b23925a":"print(f'Number of training tweets: {train_df.shape[0]}\\n')\nprint(f'Number of training tweets: {test_df.shape[0]}\\n')","7c1cf261":"# Setting target variables, creating combined data and saving index for dividing combined data later.\n\nlabels = train_df['target'].values\nidx = len(labels)\ncombined = pd.concat([train_df, test_df])\ncombined = combined.text.values","6059cdeb":"# Tokenizing the combined text data using bert tokenizer.\n\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)","b7be698c":"# Print the original tweet.\n\nprint(\"Origina:\",combined[0])\n\n# Print the tweet split into tokens\n\nprint(\"Tokenized:\", tokenizer.tokenize(combined[0]))\n\n#print the sentence mapped to token ID's\n\nprint(\"Token IDs: \", tokenizer.convert_tokens_to_ids(tokenizer.tokenize(combined[0])))","db1e7857":"max_len = 0\n\n# For every sentence...\n\nfor text in combined:\n\n    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n    \n    input_ids = tokenizer.encode(text, add_special_tokens=True)\n\n    # Update the maximum sentence length.\n    \n    max_len = max(max_len, len(input_ids))\n\nprint('Max sentence length: ', max_len)","98b2bc5b":"# Making list of sentence lenghts:\n\ntoken_lens = []\n\nfor text in combined:\n    tokens = tokenizer.encode(text, max_length = 512)\n    token_lens.append(len(tokens))","e257badd":"# Displaying sentence length dist.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfig, axes = plt.subplots(figsize=(14, 6))\nsns.distplot(token_lens, color='#e74c3c')\nplt.show()","226f1d72":"# Splitting the train test data after tokenizing.\n\ntrain= combined[:idx]\ntest = combined[idx:]\ntrain.shape","868f85d0":"def tokenize_map(sentence,labs='None'):\n    \n    \"\"\"A function for tokenize all of the sentences and map the tokens to their word IDs.\"\"\"\n    \n    global labels\n    \n    input_ids = []\n    attention_masks = []\n\n    # For every sentence...\n    \n    for text in sentence:\n        #   \"encode_plus\" will:\n        \n        #   (1) Tokenize the sentence.\n        #   (2) Prepend the `[CLS]` token to the start.\n        #   (3) Append the `[SEP]` token to the end.\n        #   (4) Map tokens to their IDs.\n        #   (5) Pad or truncate the sentence to `max_length`\n        #   (6) Create attention masks for [PAD] tokens.\n        \n        encoded_dict = tokenizer.encode_plus(\n                            text,                      # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            truncation='longest_first', # Activate and control truncation\n                            max_length = 84,           # Max length according to our text data.\n                            pad_to_max_length = True, # Pad & truncate all sentences.\n                            return_attention_mask = True,   # Construct attn. masks.\n                            return_tensors = 'pt',     # Return pytorch tensors.\n                       )\n\n        # Add the encoded sentence to the id list. \n        \n        input_ids.append(encoded_dict['input_ids'])\n\n        # And its attention mask (simply differentiates padding from non-padding).\n        \n        attention_masks.append(encoded_dict['attention_mask'])\n\n    # Convert the lists into tensors.\n    \n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    \n    if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n        labels = torch.tensor(labels)\n        return input_ids, attention_masks, labels\n    else:\n        return input_ids, attention_masks","7f15fcee":"# Tokenizing all of the train test sentences and mapping the tokens to their word IDs.\n\ninput_ids, attention_masks, labels = tokenize_map(train, labels)\ntest_input_ids, test_attention_masks= tokenize_map(test)","b16aa372":"# Combine the training inputs into a TensorDataset.\n\ndataset = TensorDataset(input_ids, attention_masks, labels)\n\n# Create a 80-20 train-validation split.\n\n# Calculate the number of samples to include in each set.\n\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\n\n# Divide the dataset by randomly selecting samples.\n\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))","da10f7dc":"# The DataLoader needs to know our batch size for training, so we specify it here. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n\nbatch_size = 32\n\n# Create the DataLoaders for our training and validation sets.\n# We'll take training samples in random order. \n\ntrain_dataloader = DataLoader(\n            train_dataset,  # The training samples.\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = batch_size # Trains with this batch size.\n        )\n\n# For validation the order doesn't matter, so we'll just read them sequentially.\n\nvalidation_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )","f582308c":"prediction_data = TensorDataset(test_input_ids, test_attention_masks)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)","88f80f39":"# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-large-uncased', # Use the 124-layer, 1024-hidden, 16-heads, 340M parameters BERT model with an uncased vocab.\n    num_labels = 2, # The number of output labels--2 for binary classification. You can increase this for multi-class tasks.   \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\n# Tell pytorch to run this model on the device which we set GPU in our case.\n\nmodel.to(device)","984f20e6":"# Get all of the model's parameters as a list of tuples:\n\nparams = list(model.named_parameters())\n\nprint('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n\nprint('==== Embedding Layer ====\\n')\n\nfor p in params[0:5]:\n    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== First Transformer ====\\n')\n\nfor p in params[5:21]:\n    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== Output Layer ====\\n')\n\nfor p in params[-4:]:\n    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))","59331739":"# Note: AdamW is a class from the huggingface library (as opposed to pytorch).\n\n# The 'W' stands for 'Weight Decay fix' probably...\n\noptimizer = AdamW(model.parameters(),\n                  lr = 6e-6, # args.learning_rate\n                  eps = 1e-8 # args.adam_epsilon\n                )","e939074c":"# Number of training epochs. The BERT authors recommend between 2 and 4. \n\n# We chose to run for 3, but we'll see later that this may be over-fitting the training data.\n\nepochs = 3\n\n# Total number of training steps is [number of batches] x [number of epochs] (Note that this is not the same as the number of training samples).\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\n\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","8ebacb9b":"def flat_accuracy(preds, labels):\n    \n    \"\"\"A function for calculating accuracy scores\"\"\"\n    \n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    \n    return accuracy_score(labels_flat, pred_flat)\n\ndef flat_f1(preds, labels):\n    \n    \"\"\"A function for calculating f1 scores\"\"\"\n    \n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    \n    return f1_score(labels_flat, pred_flat)","34e217d5":"def format_time(elapsed):    \n    \n    \"\"\"A function that takes a time in seconds and returns a string hh:mm:ss\"\"\"\n    \n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","fd9e8c45":"# This training code is based on the `run_glue.py` script here:\n\n# https:\/\/github.com\/huggingface\/transformers\/blob\/5bfcd0485ece086ebcbed2d008813037968a9e58\/examples\/run_glue.py#L128\n\n\n# We'll store a number of quantities such as training and validation loss, validation accuracy, f1 score and timings.\n\ntraining_stats = []\n\n# Measure the total training time for the whole run.\n\ntotal_t0 = time.time()\n\n# For each epoch...\n\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n\n    print('')\n    print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes:\n    \n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    \n    total_train_loss = 0\n\n    # Put the model into training mode. Don't be mislead--the call to `train` just changes the *mode*, it doesn't *perform* the training.\n    \n    # `dropout` and `batchnorm` layers behave differently during training vs. test ,\n    # source: https:\/\/stackoverflow.com\/questions\/51433378\/what-does-model-train-do-in-pytorch\n    \n    model.train()\n\n    # For each batch of training data...\n    \n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the device(gpu in our case) using the `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        \n        b_input_ids = batch[0].to(device).to(torch.int64)\n        b_input_mask = batch[1].to(device).to(torch.int64)\n        b_labels = batch[2].to(device).to(torch.int64)\n\n        # Always clear any previously calculated gradients before performing a backward pass. PyTorch doesn't do this automatically because accumulating the gradients is 'convenient while training RNNs'. \n        # Source: https:\/\/stackoverflow.com\/questions\/48001598\/why-do-we-need-to-call-zero-grad-in-pytorch\n        \n        model.zero_grad()        \n\n        # Perform a forward pass (evaluate the model on this training batch).\n        # The documentation for this `model` function is down here: \n        # https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers BertForSequenceClassification.\n        \n        # It returns different numbers of parameters depending on what arguments given and what flags are set. For our useage here, it returns the loss (because we provided labels),\n        # And the 'logits' (the model outputs prior to activation.)\n        \n        loss, logits = model(b_input_ids, \n                             token_type_ids=None, \n                             attention_mask=b_input_mask, \n                             labels=b_labels)\n\n        # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end, \n        # `loss` is a tensor containing a single value; the `.item()` function just returns the Python value from the tensor.\n        \n        total_train_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        \n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0 This is to help prevent the 'exploding gradients' problem.\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        \n        # The optimizer dictates the 'update rule'(How the parameters are modified based on their gradients, the learning rate, etc.)\n        \n        optimizer.step()\n\n        # Update the learning rate.\n        \n        scheduler.step()\n\n    # Calculate the average loss over all of the batches.\n    \n    avg_train_loss = total_train_loss \/ len(train_dataloader)            \n    \n    # Measure how long this epoch took.\n    \n    training_time = format_time(time.time() - t0)\n\n    print('')\n    print('  Average training loss: {0:.2f}'.format(avg_train_loss))\n    print('  Training epcoh took: {:}'.format(training_time))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on our validation set.\n\n    print('')\n    print('Running Validation...')\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n    \n    model.eval()\n\n    # Tracking variables:\n    \n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    total_eval_f1 = 0\n    nb_eval_steps = 0\n\n    # Evaluate data for one epoch.\n    \n    for batch in validation_dataloader:\n        \n        # Unpack this training batch from our dataloader. \n        \n        # As we unpack the batch, we'll also copy each tensor to the GPU using the `to` method.\n        \n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        # Tell pytorch not to bother with constructing the compute graph during the forward pass, since this is only needed for backprop (training part).\n        \n        with torch.no_grad():        \n\n            # Forward pass, calculate logit predictions.\n            # token_type_ids is the same as the 'segment ids', which differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is down here: \n            # https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers BertForSequenceClassification.\n            # Get the 'logits' output by the model. The 'logits' are the output values prior to applying an activation function like the softmax.\n            \n            (loss, logits) = model(b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=b_input_mask,\n                                   labels=b_labels)\n            \n        # Accumulate the validation loss.\n        \n        total_eval_loss += loss.item()\n\n        # Move logits and labels to CPU:\n        \n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches:\n        \n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n        total_eval_f1 += flat_f1(logits, label_ids)\n        \n\n    # Report the final accuracy for this validation run.\n    \n    avg_val_accuracy = total_eval_accuracy \/ len(validation_dataloader)\n    print('  Accuracy: {0:.2f}'.format(avg_val_accuracy))\n    \n    # Report the final f1 score for this validation run.\n    \n    avg_val_f1 = total_eval_f1 \/ len(validation_dataloader)\n    print('  F1: {0:.2f}'.format(avg_val_f1))\n\n    # Calculate the average loss over all of the batches.\n    \n    avg_val_loss = total_eval_loss \/ len(validation_dataloader)\n    \n    \n    \n    # Measure how long the validation run took:\n    \n    validation_time = format_time(time.time() - t0)\n    \n    print('  Validation Loss: {0:.2f}'.format(avg_val_loss))\n    print('  Validation took: {:}'.format(validation_time))\n\n    # Record all statistics from this epoch.\n    \n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Val_F1' : avg_val_f1,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint('')\nprint('Training complete!')\n\nprint('Total training took {:} (h:mm:ss)'.format(format_time(time.time()-total_t0)))\n","2311e2e8":"# Display floats with two decimal places.\n\npd.set_option('precision', 2)\n\n# Create a DataFrame from our training statistics.\n\ndf_stats = pd.DataFrame(data=training_stats)\n\n# Use the 'epoch' as the row index.\n\ndf_stats = df_stats.set_index('epoch')\n\n# Display the table.\n\ndisplay(df_stats)","d549fb67":"# Increase the plot size and font size:\n\nfig, axes = plt.subplots(figsize=(12,8))\n\n# Plot the learning curve:\n\nplt.plot(df_stats['Training Loss'], 'b-o', label='Training')\nplt.plot(df_stats['Valid. Loss'], 'g-o', label='Validation')\n\n# Label the plot:\n\nplt.title('Training & Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n\nplt.legend()\nplt.xticks([1, 2, 3])\n\nplt.show()","4b0fad10":"# Prediction on test set:\n\nprint('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n\n# Put model in evaluation mode:\n\nmodel.eval()\n\n# Tracking variables :\n\npredictions = []\n\n# Predict:\n\nfor batch in prediction_dataloader:\n    \n  # Add batch to GPU\n\n  batch = tuple(t.to(device) for t in batch)\n  \n  # Unpack the inputs from our dataloader:\n    \n  b_input_ids, b_input_mask, = batch\n  \n  # Telling the model not to compute or store gradients, saving memory and speeding up prediction:\n\n  with torch.no_grad():\n      # Forward pass, calculate logit predictions:\n    \n      outputs = model(b_input_ids, token_type_ids=None, \n                      attention_mask=b_input_mask)\n\n  logits = outputs[0]\n\n  # Move logits and labels to CPU:\n    \n  logits = logits.detach().cpu().numpy()\n \n  \n  # Store predictions and true labels:\n    \n  predictions.append(logits)\n\n\nprint('    DONE.')","3bdb2da1":"# Getting list of predictions and then choosing the target value with using argmax on probabilities.\n\nflat_predictions = [item for sublist in predictions for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()","4840e4b4":"len(flat_predictions)","6996445b":"\nsubmission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\nsubmission['target'] = flat_predictions\nsubmission.head(10)","88af2119":"submission.to_csv(\"submission.csv\")","6d1b6068":"> Tokenization and Formatting the Inputs\nFor feeding our text to BERT we have to tokenize our text first and then these tokens must be mapped. For this job we gonna download and use BERT's own tokenizer. Thanks to Transformers library it's like one line of code, we also convert our tokens to lowercase for uncased model. You can see how the tokenizer works below there on first row of tweets for example.\nWe set our max len according to our tokenized sentences for padding and truncation, then we use tokenizer.encode_plus it'll split the sentences into tokens, then adds special tokens for classificication [CLS]:\nThe first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. (from the BERT paper)\n\nThen it adds [SEP] tokens for making BERT decide if sentences are related. In our case it shouldn't be that important I think.\nThen our tokenizer map's our tokens to their IDs first and pads or truncates all sentences to same length according to our max length. If sentence is longer than our limit it gets truncated, if it's shorter than our defined length then it adds [PAD] tokens to get them in same length.\nFinally tokenizer create attention masks which is consisting of 1's and 0's for differentiating [PAD] tokens from the actual tokens.\nWe do these steps for each train and test set and then get our converted data for our BERT model. We also split train test on our train data for checking our models accuracy.\nLastly we define how to load the data into our model for training, since we can't use it all at once because of memory restrictions. On the official BERT paper batch size of 16 or 32 is recommended so we went with 32 since Kaggle offers us decent GPU's thanks to them!\n","bb152aac":"> Visualizing the Data\nWell... Our text is ready for inspection, clean and in order. We can start visualizing the data to see if we can find some visible relations between tweet classes.\n\n\n> Target Distribution\nWhen we check our target variables and look at how they disturbuted we can say it not bad. There is no huge difference between classes we can say it's good sign for modelling.\n","d73455e9":"Reference : https:\/\/huggingface.co\/transformers\/quickstart.html\n            https:\/\/arxiv.org\/pdf\/1810.04805.pdf\n            https:\/\/github.com\/amueller\/word_cloud\n            https:\/\/neptune.ai\/blog\/exploratory-data-analysis-natural-language-processing-tools\n            https:\/\/www.kaggle.com\/datafan07\/disaster-tweets-nlp-eda-bert-with-transformers","4989ea42":"> Setting the Bert Classification Model\nIt's time to load our model, exciting right?! Thanks to Transformers library we have exact tools we need for classification task. We set bert-large-uncased for the more accurate results and assign 2 labels for classification.\nYou can see the model parameters down there, it's pretty straightforward with the transformers.\nThen we choose our optimizer and fine-tune our model. Again these hyperparameters (learning_rate, epsilon, epochs etc.) are recommended on the official BERT paper.","52e975bd":"So basically what we did are:\u00b6\n- Removed urls, emojis, html tags and punctuations,\n- Tokenized the tweet base texts,\n- Lower cased clean text,\n- Removed stopwords,\n- Applied part of speech tags,\n- Converted part of speeches to wordnet format,\n- Applying word lemmatizer,\n- Converted tokenized text to string again.","372cf070":"> Predicting and Submission\nOk we trained our model and it's ready to make predictions on our test data. Then we save them to csv file for submission, this part is pretty straightforward for the most of classification tasks.\u00b6","ed8f7383":"> Training and Evaluating\nTime to train our model! First we set some helper functions to calculate our metrics and time spent on the process. Then it moves like this, directly from the original notebook, it's pretty good at explaining I shouldn't confuse you with my own way of telling I guess:\nTraining:\n\n- Unpack our data inputs and labels\n- Load data onto the GPU for acceleration,\n- Clear out the gradients calculated in the previous pass,\n- In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out,\n- Forward pass (feed input data through the network),\n- Backward pass (backpropagation),\n- Tell the network to update parameters with optimizer.step(),\n- Track variables for monitoring progress.\n\n>> Evalution:\n- Unpack our data inputs and labels,\n- Load data onto the GPU for acceleration,\n- Forward pass (feed input data through the network),\n- Compute loss on our validation data and track variables for monitoring progress.\n\n>> Pytorch hides all of the detailed calculations from us, but we've commented the code to point out which of the above steps are happening on each line.\n> The code below trains according to our data and saves the learning progress on the way so we can summarize at the end and see our results. We can also turn these to dataframe and plot it to see our eavluation better. So we can decide if the model performs well and not overfitting...","52c5ff74":"Word Counts\nOk let's check number of words per tweet now, they both look somewhat normally distributed, again disaster tweets seems to have slightly more words than non disaster ones. We might dig this deeper to get some more info in next part..","dc7e063b":"Tweet Lengths\n> Let's start with the number of characters per tweet and compare if it's disaster related or not. It seems disaster tweets are longer than non disaster tweets in general. We can assume longer tweets are more likely for disasters but this is only an assumption and might be not true..."}}