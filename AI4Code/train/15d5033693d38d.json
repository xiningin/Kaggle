{"cell_type":{"d6cfba11":"code","3c796230":"code","cc248233":"code","249e26e7":"code","5068bb3d":"code","ac3c338b":"code","d08e58ad":"code","cea706c4":"code","4952a664":"code","832be255":"code","80aed219":"code","925339f4":"code","e4d56faa":"code","c6205d9f":"code","56367571":"code","545c4807":"code","7dc1dc09":"code","08bc195f":"code","a75f3f62":"code","f0855ab6":"code","3a5d8fac":"code","039678aa":"code","91fa5363":"code","914d1501":"code","060ee933":"code","a1242989":"code","ea9ee62b":"code","8ef7ba81":"code","399b2c6d":"code","4a18936f":"code","2003224b":"code","9f4513b3":"code","609a70d5":"code","7fd1c53c":"code","c5f325b8":"code","b7c3dccb":"code","51536be5":"code","9558ec74":"code","3604ae9e":"code","d1d6fb5e":"code","2812a18a":"code","4c06fe12":"code","ea42fa6a":"code","19b25e17":"code","0b7255ed":"code","e0b661b7":"code","390e8629":"code","43e9819a":"code","935dbc4a":"code","7945dc85":"code","262b9d88":"code","d7576040":"code","320081a6":"code","a53672a3":"code","791d49fc":"code","32140b95":"code","8d7bc3a8":"code","9a2f3692":"code","09da0e06":"code","d59ece04":"code","3c670720":"code","a5f6c4d9":"code","9dcadd8a":"code","12ee66c6":"code","795dac1b":"code","fafd401d":"code","8f73482b":"code","9ac88361":"code","80e1bfa3":"code","79dc9b1b":"code","fb4bd4d8":"code","c9f3820f":"code","a0bc4a69":"code","c14523d6":"markdown","e2aa4a39":"markdown","12f92515":"markdown","8232e487":"markdown","671990b5":"markdown","554124a3":"markdown","ef367466":"markdown","e2e2a95e":"markdown","3df7db23":"markdown","9f6c8f05":"markdown","00f7f516":"markdown","ac753bf6":"markdown","593e72b4":"markdown","3a1c6c1c":"markdown","638e74b6":"markdown","47b03973":"markdown","30ef9eed":"markdown","8341fe8c":"markdown","83bd690f":"markdown","c2d43ead":"markdown","f1fabca3":"markdown","f25e2591":"markdown","41d74b04":"markdown","e2cba97a":"markdown","6468d0af":"markdown","12836abd":"markdown","bfd343bf":"markdown","2f30a141":"markdown","b9837de3":"markdown","0b360a13":"markdown","be614474":"markdown","56904734":"markdown","e5bf14f2":"markdown","b013600a":"markdown","f9de77dd":"markdown","9bb52ccc":"markdown","a740d6d6":"markdown","91f156b1":"markdown","45f06a58":"markdown","01086f09":"markdown","2019c0c0":"markdown","bbb5a22e":"markdown","0714f65b":"markdown","88a4bde7":"markdown","0cc649d8":"markdown","2398fc35":"markdown","31f9efee":"markdown","98af7c71":"markdown","9cd585d6":"markdown","828e1c35":"markdown","19b64ef3":"markdown","7a775d32":"markdown","b72270f4":"markdown","f27ecf94":"markdown","5c3f9e13":"markdown","d30868f0":"markdown"},"source":{"d6cfba11":"import pandas as pd\n\n# Not limiting the column number when displaying dataframe\npd.set_option(\"display.max_columns\", None)","3c796230":"df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf.head()","cc248233":"df.tail()","249e26e7":"print(df.columns)","5068bb3d":"rows = df.shape[0]\ncols = df.shape[1]\n\nprint(\"Before cleaning, there are \" + str(rows) + \" rows and \" + str(cols) + \" columns in this dataframe.\")","ac3c338b":"dupRows = df.duplicated().sum()\nprint(\"There are \" + str(dupRows) + \" duplicated rows in the dataframe.\")","d08e58ad":"df.isnull().sum()","cea706c4":"df.nunique()","4952a664":"df.info()","832be255":"df.describe()","80aed219":"df.memory_usage()","925339f4":"df.corr()","e4d56faa":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncorrelations = df.corr()\n\nplt.figure(figsize = (16, 12))\nplt.title(\"Heatmap displaying the correlations between all columns\", fontsize = 20)\nsns.heatmap(correlations, annot = True, cmap = \"mako\")","c6205d9f":"survivedCorr = df.corr()[\"Survived\"]\nsurvivedCorr = pd.DataFrame(survivedCorr)\nsurvivedCorr","56367571":"%matplotlib inline\n\nplt.figure(figsize = (16, 12))\nplt.title(\"Correlations between input columns and target column 'Survived'\", fontsize = 20)\nplt.xlabel(\"Columns\", fontsize = 16)\nplt.ylabel(\"Correlation factor\", fontsize = 16)\nplt.plot(survivedCorr, color = \"red\", linestyle = \"\", marker = \"o\")\nplt.show()","545c4807":"no = df.value_counts([\"Survived\"])[0]\nyes = df.value_counts([\"Survived\"])[1]\n\nprint(str(no) + \" passengers on the Titanic died and \" + str(yes) + \" people luckily survived.\")","7dc1dc09":"Category = [\"Deaths\", \"Survivals\"]\n\nplt.figure(figsize = (16, 12))\n\nplt.title(\"Number of Deaths vs. Number of Survivals\", fontsize = 20)\n\nplt.bar(Category, df.value_counts([\"Survived\"]), width = 0.5, edgecolor = \"k\", linewidth = 1, color = \"orange\")\n\nplt.xlabel(\"Category\", fontsize = 16)\nplt.ylabel(\"Numbers\", fontsize = 16)\n\nplt.yticks(ticks=[x * 100 for x in range(11)])\n\nplt.show()","08bc195f":"men = df.value_counts([\"Sex\"])[0]\nwomen = df.value_counts([\"Sex\"])[1]\n\nprint(\"When the Titanic initially took off, there were \" + str(men) + \" men on board, compared to \" + str(women) + \" women.\")","a75f3f62":"pd.crosstab(df[\"Sex\"], df[\"Survived\"]) ","f0855ab6":"ct = pd.crosstab(df[\"Sex\"], df[\"Survived\"]) \n\nplt.figure(figsize = (16, 12))\nplt.title(\"Crosstab showing how many Victims were Men\/Women\", fontsize = 20)\nsns.heatmap(ct, cmap = \"YlGnBu\", annot = True, cbar = True, fmt = \"g\")","3a5d8fac":"percentage_of_dead_men = (468 * 100) \/ 577\npercentage_of_dead_women = (81 * 100) \/ 314\n\npercentage_of_dead_men = str(int(percentage_of_dead_men))\npercentage_of_dead_women = str(int(percentage_of_dead_women))\n\nprint(\"Of all men on board, \" + percentage_of_dead_men + \" % died, while only \" + percentage_of_dead_women + \" % of women did so.\")","039678aa":"pd.crosstab(df[\"Pclass\"], df[\"Survived\"]) ","91fa5363":"ct = pd.crosstab(df[\"Pclass\"], df[\"Survived\"]) \n\nplt.figure(figsize = (16, 12))\nplt.title(\"Crosstab showing how many Victims each Ship Class counted\", fontsize = 20)\nsns.heatmap(ct, cmap = \"RdPu\", annot = True, cbar = True, fmt = \"g\")","914d1501":"percOf1stClassDeaths = (80 * 100) \/ 216\npercOf3rdClassDeaths = (372 * 100) \/ 491\n\nprint(percOf1stClassDeaths)\nprint(percOf3rdClassDeaths)","060ee933":"sibsur = df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index = False).mean().sort_values(by = \"Survived\", ascending = False)\nsibsur = pd.DataFrame(sibsur)\nsibsur","a1242989":"plt.figure(figsize = (16, 12))\n\nplt.title(\" Number of Siblings\/Spouses - Survival Chances\", fontsize = 20)\n\nplt.bar(sibsur[\"SibSp\"], sibsur[\"Survived\"], width = 0.5, edgecolor = \"k\", linewidth = 2, color = \"green\")\n\nplt.xlabel(\"Number of Siblings\/Spouses\", fontsize = 16)\nplt.ylabel(\"Survival Rate Percentage\", fontsize = 16)\n\nplt.show()","ea9ee62b":"par = df[[\"Parch\", \"Survived\"]].groupby([\"Parch\"], as_index = False).mean().sort_values(by = \"Survived\", ascending = False)\npar = pd.DataFrame(par)\npar","8ef7ba81":"plt.figure(figsize = (16, 12))\n\nplt.title(\" Number of Parents\/Children - Survival Chances\", fontsize = 20)\n\nplt.bar(par[\"Parch\"], par[\"Survived\"], width = 0.5, edgecolor = \"k\", linewidth = 2, color = \"blue\")\n\nplt.xlabel(\"Number of Parents\/Children\", fontsize = 16)\nplt.ylabel(\"Survival Rate Percentage\", fontsize = 16)\n\nplt.show()","399b2c6d":"pip install pandas-profiling[notebook]","4a18936f":"import pandas_profiling\nfrom pandas_profiling import ProfileReport\n\nprofile = ProfileReport(df, title = \"Pandas Profiling Report\", explorative = True)\n\nprofile.to_widgets()","2003224b":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\", skipinitialspace = True)\ntrain.head()","9f4513b3":"del train[\"Ticket\"]\ndel train[\"Name\"]\n\ntrain.head()","609a70d5":"train = train.fillna(0)\ntrain.head()","7fd1c53c":"train = train.replace(\"male\", 1)\ntrain = train.replace(\"female\", 2)\ntrain.head()","c5f325b8":"train[\"Cabin\"] = train[\"Cabin\"].astype(str)\ntrain[\"Embarked\"] = train[\"Embarked\"].astype(str)\n\nfrom sklearn import preprocessing\n\nnumber = preprocessing.LabelEncoder()\n\ntrain[\"Cabin\"] = number.fit_transform(train[\"Cabin\"])\ntrain[\"Embarked\"] = number.fit_transform(train[\"Embarked\"])\n\ntrain.head()","b7c3dccb":"from sklearn.model_selection import train_test_split\n\nX = train.drop([\"Survived\"], axis = 1)\ny = train[\"Survived\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.1)","51536be5":"test = pd.read_csv(\"..\/input\/titanic\/test.csv\", skipinitialspace = True)\ntest.head()","9558ec74":"test.shape","3604ae9e":"del test[\"Ticket\"]\ndel test[\"Name\"]\n\ntest.head()","d1d6fb5e":"test = test.fillna(0)\ntest.head()","2812a18a":"test = test.replace(\"male\", 1)\ntest = test.replace(\"female\", 2)\ntest.head()","4c06fe12":"test[\"Cabin\"] = test[\"Cabin\"].astype(str)\ntest[\"Embarked\"] = test[\"Embarked\"].astype(str)\n\nfrom sklearn import preprocessing\n\nnumber = preprocessing.LabelEncoder()\n\ntest[\"Cabin\"] = number.fit_transform(test[\"Cabin\"])\ntest[\"Embarked\"] = number.fit_transform(test[\"Embarked\"])\n\ntest.head()","ea42fa6a":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(solver = \"saga\", max_iter = 10000)\nmodel.fit(X_train, y_train)","19b25e17":"model.score(X_test, y_test)","0b7255ed":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(criterion = \"entropy\")\nmodel.fit(X_train, y_train)","e0b661b7":"model.score(X_test, y_test)","390e8629":"from sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier(criterion = \"entropy\")\nmodel.fit(X_train, y_train)","43e9819a":"print(model.score(X_test, y_test))","935dbc4a":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","7945dc85":"from sklearn.svm import SVC\n\nmodel = SVC(kernel = \"rbf\", gamma = 0.015, C = 10)\nmodel.fit(X_train, y_train)","262b9d88":"model.score(X_test, y_test)","d7576040":"from sklearn.naive_bayes import GaussianNB\n\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)","320081a6":"model.score(X_test, y_test)","a53672a3":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors = 13)\nmodel.fit(X_train, y_train)","791d49fc":"model.score(X_test, y_test)","32140b95":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(solver = \"sag\", max_iter = 10000)\nmodel.fit(X_train, y_train)","8d7bc3a8":"model.score(X_test, y_test)","9a2f3692":"from sklearn.model_selection import validation_curve\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\nparam_range = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n\ntrain_scores, test_scores = validation_curve(DecisionTreeClassifier(), X_train, y_train, param_name = \"max_depth\", param_range = param_range)\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize = (16, 12))\n\nplt.plot(param_range, np.mean(train_scores, axis = 1))\nplt.plot(param_range, np.mean(test_scores, axis = 1))\n\nplt.title(\"How does the tree depth influence the accuracy?\", fontsize = 20)\nplt.xlabel(\"depth levels of model\", fontsize = 15)\nplt.ylabel(\"model accuracy\", fontsize = 15)\n\n# Adding a legend\nplt.legend([\"train\", \"test\"], loc = \"upper left\", fontsize = 12)\n\nplt.show()","09da0e06":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)","d59ece04":"model.score(X_test, y_test)","3c670720":"y_test_pred = model.predict(X_test)","a5f6c4d9":"# Predicted y values\nprint(y_test_pred)","9dcadd8a":"# Real y values\ny_test = list(y_test)\nprint(y_test)","12ee66c6":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_test_pred)","795dac1b":"from sklearn.model_selection import learning_curve\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.utils import shuffle\n\nX_train, y_train = shuffle(X_train, y_train)\n\nimport numpy as np\n\ntrain_sizes_abs, train_scores, test_scores = learning_curve(KNeighborsClassifier(), X_train, y_train)\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize = (16, 12))\n\nplt.plot(train_sizes_abs, np.mean(train_scores, axis = 1))\nplt.plot(train_sizes_abs, np.mean(test_scores, axis = 1))\n\nplt.title(\"Learning Curve KNN\", fontsize = 20)\nplt.xlabel(\"Number of neighbors\", fontsize = 15)\nplt.ylabel(\"Model Accuracy\", fontsize = 15)\n\n# Adding a legend\nplt.legend([\"train\", \"test\"], loc = \"upper left\", fontsize = 12)\n\nplt.show()","fafd401d":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)","8f73482b":"model.score(X_test, y_test)","9ac88361":"model.predict(X_test)","80e1bfa3":"model.predict_proba(X_test)[:, 1] # Only probability that model predicts Yes\/True","79dc9b1b":"y_test_pred = model.predict_proba(X_test)[:, 1]\n\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\nfpr, tpr, thresholds = roc_curve(y_test, y_test_pred)\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize = (16, 12))\n\nplt.plot(fpr, tpr)\n\nplt.title(\"ROC Curve for Logistic Regression Algorithm Performance\", fontsize = 20)\nplt.xlabel(\"False-Positive-Rate (FPR))\", fontsize = 15)\nplt.ylabel(\"True-Positive-Rate (TPR))\", fontsize = 15)\n\nplt.show()","fb4bd4d8":"# Area under the curve\nroc_auc_score(y_test, y_test_pred)","c9f3820f":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors = 13)\nmodel.fit(X_train, y_train)","a0bc4a69":"pred = model.predict(test)\n\n# Append new column named 'Survived' as stated in the Submission Rules\ntest[\"Survived\"] = pred\n\nsubmission = test[[\"PassengerId\", \"Survived\"]]\n\nsubmission.to_csv(\"seschlbeck_prediction_titanic.csv\", index = False)","c14523d6":"#### Loading train data","e2aa4a39":"### **Machine Learning Models**","12f92515":"#### Replacing categorical values with numeric values","8232e487":"#### Sum of unique values per column","671990b5":"#### Dropping less relevant column","554124a3":"#### How much memory does each column need (in bytes)?","ef367466":"As shown above, people with one spouse and\/or sibling had the highest chances of survival with percentages decreasing to both sides (more\/less siblings and spouses). People with four siblings and\/or spouses were less than 20 % likely to survive...","e2e2a95e":"## **Exploratory Data Analysis (EDA)**","3df7db23":"#### Loading and Displaying data","9f6c8f05":"#### How much do the columns correlate with each other?","00f7f516":"#### Data types of the columns","ac753bf6":"#### Interactively playing with the data via Profiling","593e72b4":"#### Learning Curve","3a1c6c1c":"## **Predictions**","638e74b6":"|                   | Model: Not True | Model: True    |\n|-------------------|-----------------|----------------|\n| Reality: Not True | True negative   | False positive |\n| Reality: True     | False negative  | True positive  |","47b03973":"#### Shape of the train dataset","30ef9eed":"#### Dropping less relevant column","8341fe8c":"#### Validation Curve","83bd690f":"#### **Gaussian Naive Bayes**","c2d43ead":"#### Like this...","f1fabca3":"Condlusion: There is no simple answer to the question who survived and there were certainly special cases but generally speaking, coming from a mid-sized family might have been of great advantage compared to single people or big families.","f25e2591":"### **But Sven, how could I submit a result then?**","41d74b04":"#### 1. Run model","e2cba97a":"#### How was the Male-Female-Ratio on the Titanic?","6468d0af":"## Final Notice:\n\nThe code in this example is not exactly the code used for my submissions to contests (even though it is similar) and I finally want to make one thing clear:\nNormally, the words train data and test data mean the data used for training and testing. In this case, I validated my results with the validation data I \"lazily\/wrongly\" called test data (still from the train data set). I just used that name since it would be annoying to write X_validation instead of X_test. Nevertheless, the correct order would be train > validate > test. Testing would then be what you do, when you submit a result for a competition since you cannot see the correct label\/y column from the beginning.","12836abd":"#### Number of columns","bfd343bf":"#### Label Encoding","2f30a141":"#### **Decision Tree**","b9837de3":"#### **Logistic Regression**","0b360a13":"#### Replacing categorical values with numeric values","be614474":"#### Any null values? If yes, how many in which column?","56904734":"#### **KNN**","e5bf14f2":"Again, people with 1, 2 or 3 parents\/children were likely to survive, while children with no parents, couples with no children and people with many children were likely to die.","b013600a":"The **corr()**-function is used to find the pairwise correlation of all columns in the dataframe. Any missing values are automatically excluded. For any non-numeric data type columns in the dataframe, it is ignored. This function comes in handy while we doing the Feature Selection by observing the correlation between features and target variable or between variables.","f9de77dd":"#### ROC Curve","9bb52ccc":"#### Were people from bigger families more likely to survive than single people or couples?","a740d6d6":"#### Replacing NAs with 0","91f156b1":"#### Any duplicates?","45f06a58":"#### **SVM with RBF-Kernel**","01086f09":"There are multiple null values in the columns **Age**, **Cabin** and **Embarked**.","2019c0c0":"#### **Random Forest**","bbb5a22e":"# **Titanic EDA & Predictions**","0714f65b":"#### How much are the columns correlation with our target\/pred column **Survived**?","88a4bde7":"#### How many people of what class had to die?","0cc649d8":"#### Replacing NAs with 0","2398fc35":"#### Visualizing the correlation table above","31f9efee":"#### Loading test data","98af7c71":"It is not much of a surprise that the 1st class suffered only a death rate of little over 37 % and the 3rd class a shocking 76 %.","9cd585d6":"### **Metrics for Machine Learning**","828e1c35":"#### How many people of what sex had to die?","19b64ef3":"#### Confusion Matrix","7a775d32":"#### How many people died and how many survived?","b72270f4":"#### Statistics for each column","f27ecf94":"#### **Logistic Regression with scaled data**","5c3f9e13":"#### Choosing X and y","d30868f0":"#### Label Encoding"}}