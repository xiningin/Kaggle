{"cell_type":{"f33b309e":"code","a3904703":"code","39aa1925":"code","778a3735":"code","db089636":"code","fece82ac":"code","9324f16b":"code","91dfc7dd":"code","26224e29":"code","c6ad19ef":"code","4660519a":"code","7a5ba132":"code","db6c7ab0":"code","b176b9e1":"code","444a3789":"code","b2c2101e":"code","b7100e00":"code","cdb5e23b":"code","d698c348":"code","302c5929":"code","b4ad7198":"code","d6623be8":"code","026ae5e8":"code","edcfbdd4":"markdown","7b913bad":"markdown","96500ac1":"markdown","34b81e58":"markdown","f5c3f9d1":"markdown","bb9218c9":"markdown","227b6062":"markdown","9b588eb0":"markdown","9898f93b":"markdown","9a0049ef":"markdown","c7697e69":"markdown","2f421b7f":"markdown","a21671c2":"markdown","c353c52d":"markdown","a9a24355":"markdown"},"source":{"f33b309e":"# Importing the required libraries\n\n# Libraries for reading and handling the data\nimport numpy as np, pandas as pd\n\n# Libraries for data visvalization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Libraries for data preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Libraries for creating ML model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Library for Analysing the ML model\nfrom sklearn import metrics\n\n%matplotlib inline","a3904703":"# Reading the data\ndf=pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndf.head()","39aa1925":"# df.info() is shows the basic information about the data like column names, data types, number of rows, memory usages, etc.\ndf.info()","778a3735":"# Checking for missing values\ndf.isnull().sum()","db089636":"# Chaning the data type for the categorical variables\ncategorical_var=['sex','cp','fbs','restecg','exng','slp','caa','thall']\ndf[categorical_var]=df[categorical_var].astype('category')\n\nnumeric_var=[i for i in df.columns if i not in categorical_var][:-1] # Storing all the numeric columns in one list","fece82ac":"# Visvalization the data\nsns.pairplot(df)","9324f16b":"# Visvalization of categorical columns\nfig, ax=plt.subplots(2,4, figsize=(10,5)) # Creates a grid of 2 rows and 4 colums as we have 8 columns.\nfor axis, cat_var in zip(ax.ravel(), categorical_var): # ax.ravel() kind of flattens the 2d grid we created, for iteration\n    sns.countplot(x=cat_var,data=df, hue='output', ax=axis) # plots the count of each column\n\nplt.tight_layout() # makes the layout of the plot tight, i.e. to avoid overlapping of plots","91dfc7dd":"# Visvalization of numeric columns\n\nfig, ax=plt.subplots(1,5, figsize=(15,5))\nfor axis, num_var in zip(ax, numeric_var):\n    sns.boxplot(y=num_var,data=df, x='output', ax=axis)\n\nplt.tight_layout()","26224e29":"# Removing outliers\n\n# I have considered more than 95 percetile and less than 5 percentile as outliers, but it totally depends on you data and decision.\ndf=df[df['trtbps']<df['trtbps'].quantile(0.95)]\ndf=df[df['chol']<df['chol'].quantile(0.95)]\ndf=df[df['thalachh']>df['thalachh'].quantile(0.05)]\ndf=df[df['oldpeak']<df['oldpeak'].quantile(0.95)]","c6ad19ef":"# y is the target column and X contains the features using which we have to predict y.\ny=df['output']\nX=df.iloc[:,:-1]","4660519a":"X.head()","7a5ba132":"# One hot encoding\ntemp = pd.get_dummies(X[categorical_var], drop_first=True)\n\n# Concanating Data Frames\nX_modified = pd.concat([X, temp], axis=1)\n\n#Removing the old columns\nX_modified.drop(categorical_var, axis=1, inplace=True)","db6c7ab0":"X_modified.head()","b176b9e1":"X_train, X_test, y_train, y_test = train_test_split(X_modified, y, train_size=0.8) # 80% training and 20% testing","444a3789":"scaler=StandardScaler()\nX_train[numeric_var] = scaler.fit_transform(X_train[numeric_var]) # Use fit_transform on training set\nX_test[numeric_var] = scaler.transform(X_test[numeric_var]) # Use transform on test set","b2c2101e":"X_train.head()","b7100e00":"log_reg=LogisticRegression().fit(X_train, y_train) # Just one line code for creating model!","cdb5e23b":"print('Train accuracy score is', log_reg.score(X_train, y_train))\nprint('Test accuracy score is', log_reg.score(X_test, y_test))","d698c348":"# max_depth - maximum depth to which tree can grow, if we increase it very much then, model can overfit\n# min_samples_leaf - minimum number of samples a leaf can have\n# min_samples_split - minimum number of samples a node should have to further split\n\ntree=DecisionTreeClassifier(max_depth=5, min_samples_leaf=20, min_samples_split=40).fit(X_train, y_train)","302c5929":"print('Train accuracy score is', tree.score(X_train, y_train))\nprint('Test accuracy score is', tree.score(X_test, y_test))","b4ad7198":"def plot_auc_roc(model):\n    probs = model.predict_proba(X_test)\n    preds = probs[:,1]\n    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n    roc_auc = metrics.auc(fpr, tpr)\n\n    # method I: plt\n    import matplotlib.pyplot as plt\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","d6623be8":"# for Logistic Regression\nplot_auc_roc(log_reg)","026ae5e8":"# for Decision Tree\nplot_auc_roc(tree)","edcfbdd4":"**PLEASE UPVOTE GUYS AND RECOMMEND THAT SHOULD IMPLEMENT**","7b913bad":"#### Scaling your data is a good practice. This reduces the training time, as Gradient Descent converges quickly for scaled data compared to non-scaled data. You can do [MinMaxScaling](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html) (squeezes data into 0 and 1) and [StandardScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html) (squeezes data such that the standared deviation is 1)\n\n![scaling](https:\/\/miro.medium.com\/max\/1200\/1*yi0VULDJmBfb1NaEikEciA.png)","96500ac1":"#### For identifying the outliers from numeric columns and understand the spread of the data, we will use a box plot which is commonly used.\n![Box plot](http:\/\/www.simplypsychology.org\/boxplot.jpg)","34b81e58":"## 1. Data handling and visualizing\n\n#### Believe it or not, but these steps generally take 70% to 80% of the whole time. So, if you become good at this, you can save a lot of time and effort. I have discussed many methods which you can directly apply to your dataset and you can get an edge over others.\n\n#### This step includes handling the missing values, correcting the data types, removing the outliers, visualizing the data, and preprocessing the data to feed it into the model. I know that many of you want to make their Machine Learning model directly without doing all of this work, but please be a little patient as these steps are also important. If you feed trash into your ML model it will produce nothing but thrash! And so, almost all of the time, you have to perform these steps to clean your data before making your machine learning model. But don't worry, these steps are very simple and easy to understand and once you practice them, you will become very good at this.","f5c3f9d1":"## Decision Tree\n\n![decision tree](https:\/\/www.explorium.ai\/wp-content\/uploads\/2019\/12\/Decision-Trees-2.png)","bb9218c9":"#### I have just scratched the surface for visualizing the data, I highly recommend you to go through the [seaborn](https:\/\/seaborn.pydata.org\/) and [matplotlib](https:\/\/matplotlib.org\/) libraries for more details.\n\n#### Tip: \n#### Use 'pairplot' for getting a detailed view of the data. \n#### If both columns are numeric => scatterplot, relplot, regplot, lmplot. \n#### If both columns are categorical or one categorical and one numeric => catplot, barplot, countplot.","227b6062":"#### The categorical features have to be converted into something which can be understood by our ML model, so we are using one-hot encoding. Eg. If we have a categorical feature, say 'color' having 3 values, say 'red, yellow and green them it will create 3 columns for 'red, yellow and green, one for each. These columns will have 1 if the value is present at the original column and 0 otherwise. You can check this out for more details: [One hot encoding](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.get_dummies.html)  \n\n![One hot encoding](https:\/\/i.imgur.com\/mtimFxh.png)","9b588eb0":"## 3. Analysing the Model\n\n#### We can use confusion matrix, AUC ROC curve, MSE, F1 score, etc. for analyzing the model. Each one of them is suited for a specific purpose. We have to decide which one of them to be used, but generally, we can use AUC ROC curve for most of the cases unless explicitly mentioned. \n\n#### For now, you can understand about AUC ROC curve that, more the area under the curve better is the model. You can read more about that [here](https:\/\/www.analyticsvidhya.com\/blog\/2020\/06\/auc-roc-curve-machine-learning\/).\n\n![AUC ROC curve](https:\/\/glassboxmedicine.files.wordpress.com\/2019\/02\/roc-curve-v2.png?w=576)\n","9898f93b":"## Logistic Regression\n\n![log_reg](https:\/\/www.equiskill.com\/wp-content\/uploads\/2018\/07\/WhatsApp-Image-2020-02-11-at-8.30.11-PM.jpeg)","9a0049ef":"# Introduction\n\n#### If you are new to data science, this notebook will surely help you. After going to this notebook, I am sure you will be more comfortable with handling data, visualization, creating models, and analyzing them. \n\n#### I have divided the notebook into 3 major parts -\n\n#### 1. Data handling and visvalisation\n#### 2. Creating models\n#### 3. Analysing the models for better understanding\n\n#### So, let's get started!","c7697e69":"**PLEASE UPVOTE GUYS AND RECOMMEND THAT SHOULD IMPLEMENT**","2f421b7f":"## Creating Machine Learning Model\n\n### Now finally we will create ML model, and this is the quickest step! Just a one line code!\n\n#### We will use logistic regression and decision trees for this classification problem as they take very little time for training and are light to deploy online. Once you understand these, you can also try [SVM](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html), [Random Forest](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html), [XGBoost](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html), [CastBoost](https:\/\/catboost.ai\/docs\/concepts\/python-reference_catboostclassifier.html), etc.","a21671c2":"#### Now we will split the data into train and test sets, for training and evaluation purposes respectively. We will be dividing the data into 80% training and 20% testing. We are not using a validation set for this dataset, but you can very well use it.\n\n![train_test_split](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/bb\/ML_dataset_training_validation_test_sets.png)","c353c52d":"#### As there are no missing values in our data we shall proceed with further steps, but if your data have some missing values you can either remove them by using [df.dropna](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.dropna.html) or substitute the missing values with mean or mode with [df.fillna](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.fillna.html) . Some models can give you an error if missing values are present and some will show incorrect results, so it is important to remove them.","a9a24355":"#### The data type decides what actions are to be performed on the data. To change the datatypes [df.astype](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.astype.html)(data_type_to_convert_to) like 'int', 'float', 'category', etc. In our case, there some columns like 'sex' 'cp' 'fbs', etc are categorical variables as they have discrete and few values like 0 and 1 for 'sex', but they are mentioned as 'int' so we will change their data type."}}