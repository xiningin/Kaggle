{"cell_type":{"b5296912":"code","ad0f1bfe":"code","9ffc5a10":"code","3afbd483":"code","f689d940":"code","15af5cde":"code","e8c01524":"code","16954168":"code","5180fb10":"code","21006a21":"code","140c0943":"code","d731a806":"code","a8a43090":"code","bbbc7b25":"code","ba09633e":"code","7ad979b5":"code","ba3880ea":"code","c6fe75b5":"code","7af86d0d":"code","f9db6ad2":"code","f683429c":"markdown","1693e40d":"markdown","74065734":"markdown","35df5012":"markdown","1ff23994":"markdown","1ccc5887":"markdown"},"source":{"b5296912":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn.feature_extraction import FeatureHasher\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\nfrom fastai.tabular import *\nfrom sklearn.preprocessing import PowerTransformer\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nfrom matplotlib_venn import venn2\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nimport seaborn as sns\nsns.set_context(\"talk\")\n# sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\nstyle.use('fivethirtyeight')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","ad0f1bfe":"## \u8a13\u7df4\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\ntrain = pd.read_csv(\"..\/input\/ykc-cup-1st\/train.csv\")\ntrain.head()","9ffc5a10":"train[\"store_id\"].value_counts()","3afbd483":"train[\"area_name\"].value_counts()","f689d940":"train[\"genre_name\"].value_counts()","15af5cde":"## \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\ntest = pd.read_csv(\"..\/input\/ykc-cup-1st\/test.csv\")\ntest.head()","e8c01524":"test.tail()","16954168":"## submission\u30d5\u30a1\u30a4\u30eb\u8aad\u307f\u8fbc\u307f\nsample_submission = pd.read_csv(\"..\/input\/ykc-cup-1st\/sample_submission.csv\")\nsample_submission.head()","5180fb10":"venn2([set(train[\"id\"].values.tolist()), set(test[\"id\"].values.tolist())])","21006a21":"venn2([set(train[\"store_id\"].values.tolist()), set(test[\"store_id\"].values.tolist())])","140c0943":"venn2([set(train[\"area_name\"].values.tolist()), set(test[\"area_name\"].values.tolist())])","d731a806":"def feature_engineering(train, test):\n        \n    # area name split\n    train[\"prefecture\"] = train[\"area_name\"].apply(lambda x : x.split()[0])\n    train[\"city\"] = train[\"area_name\"].apply(lambda x : x.split()[1])\n    train[\"town\"] = train[\"area_name\"].apply(lambda x : x.split()[2])\n    test[\"prefecture\"] = test[\"area_name\"].apply(lambda x : x.split()[0])\n    test[\"city\"] = test[\"area_name\"].apply(lambda x : x.split()[1])\n    test[\"town\"] = test[\"area_name\"].apply(lambda x : x.split()[2])\n        \n    # latitude, longitude in each prefecture\n    train[\"position\"] = np.sqrt(train[\"latitude\"] ** 2 + train[\"longitude\"] ** 2)\n    test[\"position\"] = np.sqrt(test[\"latitude\"] ** 2 + test[\"longitude\"] ** 2)\n    for l in [\"latitude\", \"longitude\", \"position\"]:\n        train[f\"mean_to_{l}_abs\"] = 0\n        test[f\"mean_to_{l}_abs\"] = 0\n        train[f\"mean_to_{l}\"] = 0\n        test[f\"mean_to_{l}\"] = 0\n    for p in train[\"prefecture\"].unique():\n        train_idx = train[\"prefecture\"] == p\n        test_idx = test[\"prefecture\"] == p\n        for l in [\"latitude\", \"longitude\", \"position\"]:\n            center = train.loc[train_idx, l].mean()\n            train.loc[train_idx, f\"mean_to_{l}_abs\"] = np.abs(train.loc[train_idx, l] - center)\n            test.loc[test_idx, f\"mean_to_{l}_abs\"] = np.abs(test.loc[test_idx, l] - center)\n            train.loc[train_idx, f\"mean_to_{l}\"] = train.loc[train_idx, l] - center\n            test.loc[test_idx, f\"mean_to_{l}\"] = test.loc[test_idx, l] - center\n            \n    # categorical encoding\n    for c in [\"area_name\", \"genre_name\", \"prefecture\", \"city\"]:\n        # fillna\n        train[c] = train[c].fillna(\"na\")\n        test[c] = test[c].fillna(\"na\")\n        \n        # frequency encoding\n        freq = train[c].value_counts()\n        train[f\"{c}_freq\"] = train[c].map(freq)\n        test[f\"{c}_freq\"] = test[c].map(freq)\n        \n        # diff to mean\n        center = train[f\"{c}_freq\"].mean()\n        train[f\"{c}_freq_tomean\"] = train[f\"{c}_freq\"] - center\n        test[f\"{c}_freq_tomean\"] = test[f\"{c}_freq\"] - center\n        \n#         # feature hashing\n#         fh = FeatureHasher(n_features=10, input_type='string')\n#         hash_tr = fh.transform(train[c].astype(str).values)\n#         hash_te = fh.transform(test[c].astype(str).values)\n#         hash_tr = pd.DataFrame(hash_tr.todense(), columns=[f'{c}_{i}' for i in range(10)])\n#         hash_te = pd.DataFrame(hash_te.todense(), columns=[f'{c}_{i}' for i in range(10)])\n#         train = pd.concat([train, hash_tr], axis=1)\n#         test = pd.concat([test, hash_te], axis=1)\n    \n        \n    ## label encode all categorical features\n    cat = [\"store_id\", \"genre_name\", \"area_name\", \"day_of_week\", \"prefecture\", \"city\", \"town\"]\n    for c in cat:\n        le = LabelEncoder()\n        train[c] = le.fit_transform(train[c].fillna(\"na\"))\n        test[c] = le.transform(test[c].fillna(\"na\"))\n        \n#     ## categorical interactions\n#     train[\"store_genre\"] = train[\"store_id\"].astype(str) + train[\"genre_name\"].astype(str)\n#     test[\"store_genre\"] = test[\"store_id\"].astype(str) + test[\"genre_name\"].astype(str)\n#     train[\"store_dayofweek\"] = train[\"store_id\"].astype(str) + train[\"day_of_week\"].astype(str)\n#     test[\"store_dayofweek\"] = test[\"store_id\"].astype(str) + test[\"day_of_week\"].astype(str)\n\n    return train, test","a8a43090":"train, test = feature_engineering(train, test)\nprint(train.shape)\ntrain.head()","bbbc7b25":"print(test.shape)\ntest.head()","ba09633e":"features = test.columns.values.tolist()\nprint(features)","7ad979b5":"cat_features = [\"store_id\",\"genre_name\", \"area_name\", 'city', 'town', \"day_of_week\", \"prefecture\"]\n\n## for embedding\nemb = {\"store_id\": 12, \"genre_name\": 8, \"area_name\": 8, 'city': 4, 'town': 4, \"day_of_week\": 4, \"prefecture\": 8}\n\n## for target encoding\n# te = [\"day_of_week_te\",\"prefecture_te\"]\n# train[\"day_of_week_te\"] = train[\"day_of_week\"].values\n# test[\"day_of_week_te\"] = test[\"day_of_week\"].values\n# train[\"prefecture_te\"] = train[\"prefecture\"].values\n# test[\"prefecture_te\"] = test[\"prefecture\"].values\n\nfor c in cat_features:\n    train[c] = train[c].astype(int)\n    test[c] = test[c].astype(int)\ngroup = \"id\"\ndropcols = [group]\nfeatures = [f for f in features if f not in dropcols]\ncat_features = [c for c in cat_features if c in features]\nnum_features = [f for f in features if f not in cat_features]\n    \n# to normal\npt = PowerTransformer(method=\"yeo-johnson\")\ntrain[num_features] = pt.fit_transform(train[num_features])\ntest[num_features] = pt.transform(test[num_features])\n    \n# group = \"store_id\"\ntarget = \"log_visitors\" ## \u4e88\u6e2c\u5bfe\u8c61\nn_split = 5 ## cross validation\u306efold\u6570","ba3880ea":"## cross validation\u3092\u884c\u3044\uff0c\u5404fold\u3067\u8a13\u7df4\u3057\u305f\u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u306e\u5e73\u5747\u5024\u3092submit\u3059\u308b\ny_pred = np.zeros(len(test))\npath = Path('..\/input')\nprocs = [FillMissing, Categorify, Normalize]\noof_pred = np.zeros(len(train))\nkf = KFold(n_splits=n_split, shuffle = True, random_state=42)\nfor i_fold, (train_idx, valid_idx) in enumerate(kf.split(train, train[target])):\n    print(f\"--------fold {i_fold}-------\")\n    \n    ## train data\n    x_tr = train.loc[train_idx, features]\n    y_tr = train.loc[train_idx, target]\n\n    ## valid data\n    x_va = train.loc[valid_idx, features]\n    y_va = train.loc[valid_idx, target]\n\n#     ## target encoding\n#     train_te = train.copy()\n#     test_te = test.copy()\n#     x_tr_te = x_tr.copy()\n#     for c in te:\n#         data_tmp = pd.DataFrame({c: x_tr_te[c], 'target': y_tr})\n#         target_mean = data_tmp.groupby(c)['target'].mean()\n#         x_va.loc[:, c] = x_va[c].map(target_mean)\n#         test_te.loc[:, c] = test_te[c].map(target_mean)\n        \n#         tmp = np.repeat(np.nan, x_tr.shape[0])\n#         kf_encoding = KFold(n_splits=n_split, shuffle=True, random_state=43)\n#         for idx_1, idx_2 in kf_encoding.split(x_tr_te):\n#             target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n#             tmp[idx_2] = x_tr_te[c].iloc[idx_2].map(target_mean)\n#         x_tr_te.loc[:, c] = tmp\n#         train_te.loc[train_idx, c] = x_tr_te[c].values\n#         train_te.loc[valid_idx, c] = x_va[c].values\n        \n    ## NN training\n    data = TabularDataBunch.from_df(path, train[features + [target]], dep_var=target, procs=procs, \n                                               cat_names=cat_features, valid_idx=valid_idx, test_df=test[features])\n    learn = tabular_learner(data, layers=[128, 64], emb_szs=emb, metrics=mean_squared_error, path='.')\n    learn.lr_find()\n    learn.recorder.plot()\n    learn.fit_one_cycle(5, 8e-2)\n    \n    ## run prediction on validation set\n    valid_predicts = learn.get_preds(ds_type=DatasetType.Valid)\n    oof_pred[valid_idx] = to_np(valid_predicts[0][:,0])\n    valid_score = np.sqrt(np.mean((oof_pred[valid_idx] - train.loc[valid_idx, target].values) ** 2))\n    print(f\"Partial CV = {valid_score}\")\n    \n    ## prediction\n    preds = learn.get_preds(ds_type=DatasetType.Test)\n    y_pred += to_np(preds[0][:,0]) \/ n_split","c6fe75b5":"learn.summary()","7af86d0d":"score = np.sqrt(np.mean((oof_pred - train[target].values) ** 2))\nprint(score)","f9db6ad2":"## 5fold\u3067\u8a13\u7df4\u3057\u305f\u305d\u308c\u305e\u308c\u306e\u4e88\u6e2c\u5024\u3092\u5143\u306b\uff0c\u305d\u308c\u3089\u306e\u5e73\u5747\u3092\u8a08\u7b97\u3057\u3066\u6700\u7d42\u7684\u306a\u4e88\u6e2c\u5024\u3068\u3059\u308b\uff0e\nsample_submission[\"log_visitors\"] = y_pred\nsample_submission.to_csv(\"submission.csv\", index = False)","f683429c":"## load","1693e40d":"# Feature engineering","74065734":"# CV","35df5012":"## Train, evaluate, and pred","1ff23994":"## Submission","1ccc5887":"**1\u4f4d\u76f8\u5f53\u306e\u30b9\u30b3\u30a2\u3042\u308b\u308f\u3068\u304b\u8a00\u3063\u3066\u307e\u3057\u305f\u304c\u3042\u308a\u307e\u305b\u3093\u3067\u3057\u305f**\u3063\u3059\u307f\u307e\u305b\u3093\u3093\uff01\uff08public\u3068private\u8aad\u307f\u9593\u9055\u3048\u3066\u305f\uff09\u3002\u4ee5\u4e0b\u96d1\u3067\u3059\u304cFast.ai\u306etabular_learner\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002LGB, CatB\u3088\u308a\u3082\u30b9\u30b3\u30a2\u306f\u826f\u304b\u3063\u305f\u3067\u3059\u3002"}}