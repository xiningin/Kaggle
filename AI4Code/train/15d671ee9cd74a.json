{"cell_type":{"f38a9cd3":"code","e1110564":"code","2314d0ef":"code","b50b5711":"code","c5a87240":"code","dcdfd5e4":"code","af621c92":"code","1f43db32":"code","67577828":"code","523ac285":"code","ed6ec612":"code","f921dacf":"code","fdbf394e":"code","4a2ba021":"code","36bcf838":"code","725df624":"code","fbb0667e":"code","02d93e2b":"code","18bfc449":"code","cb7bb285":"code","4dd0c01c":"code","0419bb5c":"code","18ad3a77":"code","a92ab39c":"code","e50c5669":"code","292a8a9f":"code","d0f0f8f9":"code","6e479eaf":"code","a9424844":"code","44e57410":"code","de9cd176":"code","85a1d37c":"code","5f7a7523":"code","59309ea3":"code","76c13ef6":"code","885778f5":"code","a95929cf":"code","a1d45aeb":"code","fec16ada":"code","4bf6b3f2":"code","3a675119":"code","568aa990":"code","bf49cd3e":"code","730d2a37":"code","91276b78":"code","92fc8eca":"code","e25df5d5":"code","948398ec":"code","78d1e7e6":"code","948db328":"code","85c73e93":"code","72a2f178":"code","fbf4dd80":"code","8ac2212b":"code","f37a7772":"code","19a75ae2":"code","9dbe2c61":"code","09d48a47":"code","66974214":"code","4988ab7e":"code","896cfb88":"code","3a019e8a":"code","7fb8286a":"code","46a39fdb":"code","0ea566c0":"code","c013aba2":"code","acd130a2":"code","0dea8746":"code","9950a528":"markdown","1c07378f":"markdown","1cd8d2f5":"markdown","b4699971":"markdown","744caad9":"markdown","bba813c3":"markdown","4e1f99cd":"markdown"},"source":{"f38a9cd3":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom contextlib import contextmanager\nfrom time import time\nfrom tqdm import tqdm\nimport lightgbm as lgbm\nimport category_encoders as ce\n\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import classification_report, log_loss, accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\n# Pandas Profiling to get the overview of the data\nimport pandas_profiling as pp\n# Seaborn for plot graphically depicting groups of numerical data through their quartiles\nimport seaborn as sns","e1110564":"# Load data file - diabetes _ 012 _ health _ indicators _ BRFSS2015.csv\n# is a clean dataset of 253,680 survey responses to the CDC's BRFSS2015. \n# The target variable Diabetes_012 has 3 classes.\n#    0 is for no diabetes or only during pregnancy\n#    1 is for prediabetes\n#    2 is for diabetes\n# There is class imbalance in this dataset. This dataset has 21 feature variables\n# Info about original data collected by BRFSS - https:\/\/www.kaggle.com\/cdc\/behavioral-risk-factor-surveillance-system\n\ndata0 = pd.read_csv(\"..\/input\/diabetes-health-indicators-dataset\/diabetes_012_health_indicators_BRFSS2015.csv\")\ndata0","2314d0ef":"# This is a QA check, showing that approximatly 2000 people hadn't had a cholesterol check in the past 5 years, \n# yet they indicats for a high cholesterol","b50b5711":"data0[(data0['CholCheck'] == 0.0) & (data0['HighChol'] != 0.0)]","c5a87240":"# Comparing non-smokers to smokers in each diabetic status shows for no significant difference in-between\n# the different diabetic groups, thus the Smoking indicator doesn't provide business insight to this questionnaire","dcdfd5e4":"Target_Smoker = data0[[\"Diabetes_012\", \"Smoker\"]]\nTarget_Smoker\nTarget_NonSmoker_0 = Target_Smoker[(Target_Smoker.Diabetes_012 == 0) & (Target_Smoker.Smoker== 0)]\nTarget_Smoker_0 = Target_Smoker[(Target_Smoker.Diabetes_012 == 0) & (Target_Smoker.Smoker== 1)]\nHealthy_NonSmoker = Target_NonSmoker_0.shape[0]\nprint(Healthy_NonSmoker)\nHealthy_Smoker = Target_Smoker_0.shape[0]\nHealthy_Smoker","af621c92":"#group diabetes status & Smokers\ndiabetes_Smokers = data0.groupby(['Diabetes_012', 'Smoker']).size().reset_index(name = 'Count')\nprint(diabetes_Smokers)","1f43db32":"Total_Stat = diabetes_Smokers.groupby(['Diabetes_012']).sum().Count\nTotal_Stat\n","67577828":"Temp = diabetes_Smokers.set_index('Diabetes_012').join(Total_Stat,rsuffix='_')\nTemp['Percentage'] = Temp['Count']\/Temp['Count_']\nTemp = Temp.reset_index()\nTemp","523ac285":"Temp['Diabetes_012'] = Temp['Diabetes_012'].replace({0.0:'Healthy', 1.0:'Pre-diabetic', 2.0:'Diabetic'})\nTemp['Smoker'] = Temp['Smoker'].replace({0.0:'Non-Smoker', 1.0:'Smoker'})\n","ed6ec612":"#visualize diabetes status ~ Smokers\nplt.figure(figsize = (8,6))\nsns.barplot(x = 'Diabetes_012', y = 'Percentage', hue = 'Smoker', data = Temp, palette = 'Set2')\nplt.title(\"Diabetes Status ~ Smokers\")\nplt.show()","f921dacf":"#Comparing normalized ....","fdbf394e":"diabetes_Mental_temp = data0['MentHlth']\n\ndata0.loc[diabetes_Mental_temp < 6,'MentHlth_group'] = '0-5'\ndata0.loc[((diabetes_Mental_temp >= 6) & (diabetes_Mental_temp <= 24)),'MentHlth_group'] = '6-24'\ndata0.loc[((diabetes_Mental_temp > 24) & (diabetes_Mental_temp <= 30)),'MentHlth_group'] = '25-30'\ndata0","4a2ba021":"#group diabetes status & Mental Health\ndiabetes_Mental = data0.groupby(['Diabetes_012', 'MentHlth_group']).size().reset_index(name = 'Count')\nprint(diabetes_Mental)\n\n# Check the missing values in the dataset\ndata0.isnull().values.any()","36bcf838":"#calculating percentage\nTotal_Stat_Mnth = diabetes_Mental.groupby(['Diabetes_012']).sum()\nTotal_Stat_Mnth\n# Total_Stat_Mnth.set_index('Diabetes_012')\n# diabetes_Mental.append(Count)","725df624":"Temp1 = diabetes_Mental.set_index('Diabetes_012').join(Total_Stat_Mnth,rsuffix='_')\nTemp1['Percentage'] = Temp1['Count']\/Temp1['Count_']\nTemp1\nTemp1 = Temp1.reset_index()","fbb0667e":"Temp1['Diabetes_012'] = Temp1['Diabetes_012'].replace({0.0:'Healthy', 1.0:'Pre-diabetic', 2.0:'Diabetic'})\nTemp1['MentHlth_group'] = Temp1['MentHlth_group'].replace({'0-5':'Not so sad','6-24':'Sad','25-30':'Very sad'})","02d93e2b":"#visualize diabetes status ~ Smokers\nplt.figure(figsize = (8,6))\nsns.barplot(x = 'Diabetes_012', y = 'Percentage', hue = 'MentHlth_group', data = Temp1, palette = 'Set3')\nplt.title(\"Diabetes Status ~ Mental Health\")\nplt.show()","18bfc449":"#Conclusion....","cb7bb285":"print(data0.columns.to_list())","4dd0c01c":"#Top 5 Dataset\ndata0.head()","0419bb5c":"#Display the last 5 dataset\ndata0.tail()","18ad3a77":"data0.shape","a92ab39c":"target=['Diabetes_012']\ndataY=data0['Diabetes_012']\ndataX=data0.drop('Diabetes_012',axis=1)","e50c5669":"#Info of the data\ndata0.info()","292a8a9f":"#Five Point summary of the data \ndata0.describe()","d0f0f8f9":"pp.ProfileReport(data0)","6e479eaf":"data0.BMI.value_counts().head()","a9424844":"# BMI different values\ndata0.BMI.value_counts().hist()","44e57410":"# data of Diabetes_012 = 0 without column Diabetes_012\ndf_Diabetes_012_val_0 = data0[data0.Diabetes_012 == 0].drop('Diabetes_012', axis=1)\ndf_Diabetes_012_val_1 = data0[data0.Diabetes_012 == 1].drop('Diabetes_012', axis=1)\ndf_Diabetes_012_val_2 = data0[data0.Diabetes_012 == 2].drop('Diabetes_012', axis=1)\ndf_Diabetes_012_val_0.info()\n# df.set_index(df.iloc[0].values)","de9cd176":"df_Diabetes_012_val_2.info()","85a1d37c":"sns.boxplot(x=df_Diabetes_012_val_0['BMI'])","5f7a7523":"sns.boxplot(x=df_Diabetes_012_val_1['BMI'])","59309ea3":"sns.boxplot(x=df_Diabetes_012_val_2['BMI'])","76c13ef6":"df_Diabetes_012_val_2['BMI'].describe()","885778f5":"df_Diabetes_012_val_2['BMI'].hist()","a95929cf":"df_DiabetesVal2_BMI_gr_60 = df_Diabetes_012_val_2[df_Diabetes_012_val_2.BMI > 60]","a1d45aeb":"df_DiabetesVal2_BMI_gr_60.info()","fec16ada":"df_DiabetesVal2_BMI_gr_60.describe()","4bf6b3f2":"x = df_DiabetesVal2_BMI_gr_60.reset_index(drop=True)\npp.ProfileReport(x)","3a675119":"# Pandas Profiling to get the overview of the data\n# do not profile it with index\ndf_Diabetes_012_val_0.reset_index(drop=True, inplace=True)\npp.ProfileReport(df_Diabetes_012_val_0)","568aa990":"(data0.BMI>=100).value_counts()","bf49cd3e":"# read data file diabetes _ binary _ 5050split _ health _ indicators _ BRFSS2015.csv\n# is a clean dataset of 70,692 survey responses to the CDC's BRFSS2015. \n# It has an equal 50-50 split of respondents with no diabetes and with either prediabetes or diabetes.\n# The target variable Diabetes_binary has 2 classes:\n#   0 is for no diabetes, \n#   1 is for prediabetes or diabetes.\n# This dataset has 21 feature variables and is balanced.\n\nb_data0 = pd.read_csv(\"..\/input\/diabetes-health-indicators-dataset\/diabetes_binary_health_indicators_BRFSS2015.csv\")\ndata0","730d2a37":"# Check the missing values in the dataset\nb_data0.isnull().values.any()","91276b78":"print(b_data0.columns.to_list())","92fc8eca":"#Top 5 Dataset\nb_data0.head()","e25df5d5":"#Display the last 5 dataset\nb_data0.tail()","948398ec":"b_data0.shape","78d1e7e6":"target=['Diabetes_binary']\ndataY=b_data0['Diabetes_binary']\ndataX=b_data0.drop('Diabetes_binary',axis=1)","948db328":"#Info of the data\nb_data0.info()","85c73e93":"#Five Point summary of the data \nb_data0.describe()","72a2f178":"# Pandas Profiling to get the overview of the data\npp.ProfileReport(b_data0)","fbf4dd80":"ballanced_data0 = pd.read_csv(\"..\/input\/diabetes-health-indicators-dataset\/diabetes_binary_5050split_health_indicators_BRFSS2015.csv\")\nballanced_data0","8ac2212b":"# Pandas Profiling to get the overview of the data\nimport pandas_profiling as pp\n\npp.ProfileReport(ballanced_data0)","f37a7772":"n=len(dataX)\nN=[]\nfor i in range(n):\n    N+=[i]\nrandom.seed(2021)\nrandom.shuffle(N)","19a75ae2":"trainX=dataX.loc[N[0:(n\/\/4)*3]]\ntrainY=dataY.loc[N[0:(n\/\/4)*3]]\ntestX=dataX.loc[N[(n\/\/4)*3:]]\ntestY=dataY.loc[N[(n\/\/4)*3:]]","9dbe2c61":"df_columns = list(dataX.columns)\nprint(df_columns)","09d48a47":"def create_numeric_feature(input_df):\n    use_columns = df_columns \n    return input_df[use_columns].copy()","66974214":"from contextlib import contextmanager\nfrom time import time\n\nclass Timer:\n    def __init__(self, logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None, sep=' '):\n\n        if prefix: format_str = str(prefix) + sep + format_str\n        if suffix: format_str = format_str + sep + str(suffix)\n        self.format_str = format_str\n        self.logger = logger\n        self.start = None\n        self.end = None\n\n    @property\n    def duration(self):\n        if self.end is None:\n            return 0\n        return self.end - self.start\n\n    def __enter__(self):\n        self.start = time()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.end = time()\n        out_str = self.format_str.format(self.duration)\n        if self.logger:\n            self.logger.info(out_str)\n        else:\n            print(out_str)","4988ab7e":"from tqdm import tqdm\n\ndef to_feature(input_df):\n\n    processors = [\n        create_numeric_feature,\n    ]\n    \n    out_df = pd.DataFrame()\n    \n    for func in tqdm(processors, total=len(processors)):\n        with Timer(prefix='create' + func.__name__ + ' '):\n            _df = func(input_df)\n\n        assert len(_df) == len(input_df), func.__name__\n        out_df = pd.concat([out_df, _df], axis=1)\n        \n    return out_df","896cfb88":"train_feat_df = to_feature(trainX)\ntest_feat_df = to_feature(testX)","3a019e8a":"import lightgbm as lgbm\nfrom sklearn.metrics import mean_squared_error\n\ndef fit_lgbm(X, y, cv, \n             params: dict=None, \n             verbose: int=50):\n\n    if params is None:\n        params = {}\n\n    models = []\n    oof_pred = np.zeros_like(y, dtype=np.float)\n\n    for i, (idx_train, idx_valid) in enumerate(cv): \n        x_train, y_train = X[idx_train], y[idx_train]\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n\n        clf = lgbm.LGBMRegressor(**params)\n        \n        with Timer(prefix='fit fold={} '.format(i)):\n            clf.fit(x_train, y_train, \n                    eval_set=[(x_valid, y_valid)],  \n                    early_stopping_rounds=100,\n                    verbose=verbose)\n\n        pred_i = clf.predict(x_valid)\n        oof_pred[idx_valid] = pred_i\n        models.append(clf)\n        print(f'Fold {i} RMSLE: {mean_squared_error(y_valid, pred_i) ** .5:.4f}')\n        print()\n\n    score = mean_squared_error(y, oof_pred) ** .5\n    print('-' * 50)\n    print('FINISHED | Whole RMSLE: {:.4f}'.format(score))\n    return oof_pred, models","7fb8286a":"params = {\n    'objective': 'rmse', \n    'learning_rate': .1,\n    'reg_lambda': 1.,\n    'reg_alpha': .1,\n    'max_depth': 5, \n    'n_estimators': 10000, \n    'colsample_bytree': .5, \n    'min_child_samples': 10,\n    'subsample_freq': 3,\n    'subsample': .9,\n    'importance_type': 'gain', \n    'random_state': 71,\n    'num_leaves': 62\n}","46a39fdb":"y = trainY","0ea566c0":"ydf=pd.DataFrame(y)\nydf","c013aba2":"from sklearn.model_selection import KFold\n\nfor i in range(1):\n    fold = KFold(n_splits=5, shuffle=True, random_state=71)\n    ydfi=ydf.iloc[:,i]\n    y=np.array(ydfi)\n    cv = list(fold.split(train_feat_df, y))\n    oof, models = fit_lgbm(train_feat_df.values, y, cv, params=params, verbose=500)\n    \n    fig,ax = plt.subplots(figsize=(6,6))\n    ax.set_title(target[i],fontsize=20)\n    ax.set_xlabel('Train Predicted '+target[i],fontsize=12)\n    ax.set_ylabel('Train Actual '+target[i],fontsize=12)\n    ax.scatter(oof,y)\n","acd130a2":"def visualize_importance(models, feat_train_df):\n\n    feature_importance_df = pd.DataFrame()\n    for i, model in enumerate(models):\n        _df = pd.DataFrame()\n        _df['feature_importance'] = model.feature_importances_\n        _df['column'] = feat_train_df.columns\n        _df['fold'] = i + 1\n        feature_importance_df = pd.concat([feature_importance_df, _df], \n                                          axis=0, ignore_index=True)\n\n    order = feature_importance_df.groupby('column')\\\n        .sum()[['feature_importance']]\\\n        .sort_values('feature_importance', ascending=False).index[:50]\n    \n    order0=order[0:5]\n    print(order0.tolist())\n\n    fig, ax = plt.subplots(figsize=(8, max(6, len(order) * .25)))\n    sns.boxenplot(data=feature_importance_df, \n                  x='feature_importance', \n                  y='column', \n                  order=order, \n                  ax=ax, \n                  palette='viridis', \n                  orient='h')\n    \n    ax.tick_params(axis='x', rotation=0)\n    #ax.set_title('Importance')\n    ax.grid()\n    fig.tight_layout()\n    \n    return fig,ax\n\n#fig, ax = visualize_importance(models, train_feat_df)","0dea8746":"for i in range(1):\n    fold = KFold(n_splits=5, shuffle=True, random_state=71)\n    ydfi=ydf.iloc[:,i]\n    y=np.array(ydfi)\n    cv = list(fold.split(train_feat_df, y))\n    oof, models = fit_lgbm(train_feat_df.values, y, cv, params=params, verbose=500)\n    fig, ax = visualize_importance(models, train_feat_df)\n    ax.set_title(target[i]+' Imortance',fontsize=20)\n","9950a528":"# Model","1c07378f":"# Data QA: Cholesterol check:","1cd8d2f5":"# RDA - Data preparation ","b4699971":"# Target setting","744caad9":"# Visualize Importance","bba813c3":"# Smokers vs. Diabetes","4e1f99cd":"# Diabetes Status & Mental Health"}}