{"cell_type":{"50b734f1":"code","82b99348":"code","3f27883b":"code","71d98077":"code","b34a1959":"code","e0422360":"code","1e737960":"code","3e4c6bb9":"code","27583d1d":"code","b6c5ec88":"code","8c26e2b6":"code","2d0ececb":"code","ad330e65":"code","f987489f":"markdown","bdcce898":"markdown","1fa2e3b3":"markdown","95310c69":"markdown","2ba5e5cf":"markdown","30f977ec":"markdown","84cd2f6e":"markdown","cbd35112":"markdown"},"source":{"50b734f1":"import pandas as pd\nimport os\nimport numpy as np\nimport pandas as pd\nimport zipfile\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport sys\nimport datetime","82b99348":"#downloading weights and cofiguration file for the model\n!wget https:\/\/storage.googleapis.com\/bert_models\/2018_10_18\/uncased_L-12_H-768_A-12.zip","3f27883b":"repo = 'model_repo'\nwith zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(repo)","71d98077":"!ls 'model_repo\/uncased_L-12_H-768_A-12'","b34a1959":"!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/modeling.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/optimization.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/run_classifier.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/tokenization.py ","e0422360":"# Available pretrained model checkpoints:\n#   uncased_L-12_H-768_A-12: uncased BERT base model\n#   uncased_L-24_H-1024_A-16: uncased BERT large model\n#   cased_L-12_H-768_A-12: cased BERT large model\n#We will use the most basic of all of them\nBERT_MODEL = 'uncased_L-12_H-768_A-12'\nBERT_PRETRAINED_DIR = f'{repo}\/uncased_L-12_H-768_A-12'\nOUTPUT_DIR = f'{repo}\/outputs'\nprint(f'***** Model output directory: {OUTPUT_DIR} *****')\nprint(f'***** BERT pretrained directory: {BERT_PRETRAINED_DIR} *****')\n","1e737960":"from sklearn.model_selection import train_test_split\n\ntrain_df =  pd.read_csv('..\/input\/train.csv')\ntrain_df = train_df.sample(2000)\n\ntrain, test = train_test_split(train_df, test_size = 0.1, random_state=42)\n\ntrain_lines, train_labels = train.question_text.values, train.target.values\ntest_lines, test_labels = test.question_text.values, test.target.values","3e4c6bb9":"import modeling\nimport optimization\nimport run_classifier\nimport tokenization\nimport tensorflow as tf\n\n\ndef create_examples(lines, set_type, labels=None):\n#Generate data for the BERT model\n    guid = f'{set_type}'\n    examples = []\n    if guid == 'train':\n        for line, label in zip(lines, labels):\n            text_a = line\n            label = str(label)\n            examples.append(\n              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    else:\n        for line in lines:\n            text_a = line\n            label = '0'\n            examples.append(\n              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    return examples\n\n# Model Hyper Parameters\nTRAIN_BATCH_SIZE = 32\nEVAL_BATCH_SIZE = 8\nLEARNING_RATE = 2e-5\nNUM_TRAIN_EPOCHS = 3.0\nWARMUP_PROPORTION = 0.1\nMAX_SEQ_LENGTH = 128\n# Model configs\nSAVE_CHECKPOINTS_STEPS = 1000 #if you wish to finetune a model on a larger dataset, use larger interval\n# each checpoint weights about 1,5gb\nITERATIONS_PER_LOOP = 1000\nNUM_TPU_CORES = 8\nVOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\nCONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\nINIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\nDO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n\nlabel_list = ['0', '1']\ntokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\ntrain_examples = create_examples(train_lines, 'train', labels=train_labels)\n\ntpu_cluster_resolver = None #Since training will happen on GPU, we won't need a cluster resolver\n#TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.\nrun_config = tf.contrib.tpu.RunConfig(\n    cluster=tpu_cluster_resolver,\n    model_dir=OUTPUT_DIR,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n    tpu_config=tf.contrib.tpu.TPUConfig(\n        iterations_per_loop=ITERATIONS_PER_LOOP,\n        num_shards=NUM_TPU_CORES,\n        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n\nnum_train_steps = int(\n    len(train_examples) \/ TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\nnum_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n\nmodel_fn = run_classifier.model_fn_builder(\n    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n    num_labels=len(label_list),\n    init_checkpoint=INIT_CHECKPOINT,\n    learning_rate=LEARNING_RATE,\n    num_train_steps=num_train_steps,\n    num_warmup_steps=num_warmup_steps,\n    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available  \n    use_one_hot_embeddings=True)\n\nestimator = tf.contrib.tpu.TPUEstimator(\n    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available \n    model_fn=model_fn,\n    config=run_config,\n    train_batch_size=TRAIN_BATCH_SIZE,\n    eval_batch_size=EVAL_BATCH_SIZE)","27583d1d":"\"\"\"\nNote: You might see a message 'Running train on CPU'. \nThis really just means that it's running on something other than a Cloud TPU, which includes a GPU.\n\"\"\"\n\n# Train the model.\nprint('Please wait...')\ntrain_features = run_classifier.convert_examples_to_features(\n    train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\nprint('***** Started training at {} *****'.format(datetime.datetime.now()))\nprint('  Num examples = {}'.format(len(train_examples)))\nprint('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\ntf.logging.info(\"  Num steps = %d\", num_train_steps)\ntrain_input_fn = run_classifier.input_fn_builder(\n    features=train_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=True,\n    drop_remainder=True)\nestimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\nprint('***** Finished training at {} *****'.format(datetime.datetime.now()))","b6c5ec88":"\"\"\"\nThere is a weird bug in original code.\nWhen predicting, estimator returns an empty dict {}, without batch_size.\nI redefine input_fn_builder and hardcode batch_size, irnoring 'params' for now.\n\"\"\"\n\ndef input_fn_builder(features, seq_length, is_training, drop_remainder):\n  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n\n  all_input_ids = []\n  all_input_mask = []\n  all_segment_ids = []\n  all_label_ids = []\n\n  for feature in features:\n    all_input_ids.append(feature.input_ids)\n    all_input_mask.append(feature.input_mask)\n    all_segment_ids.append(feature.segment_ids)\n    all_label_ids.append(feature.label_id)\n\n  def input_fn(params):\n    \"\"\"The actual input function.\"\"\"\n    print(params)\n    batch_size = 32\n\n    num_examples = len(features)\n\n    d = tf.data.Dataset.from_tensor_slices({\n        \"input_ids\":\n            tf.constant(\n                all_input_ids, shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        \"input_mask\":\n            tf.constant(\n                all_input_mask,\n                shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        \"segment_ids\":\n            tf.constant(\n                all_segment_ids,\n                shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        \"label_ids\":\n            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n    })\n\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n    return d\n\n  return input_fn","8c26e2b6":"predict_examples = create_examples(test_lines, 'test')\n\npredict_features = run_classifier.convert_examples_to_features(\n    predict_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n\npredict_input_fn = input_fn_builder(\n    features=predict_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=False,\n    drop_remainder=False)\n\nresult = estimator.predict(input_fn=predict_input_fn)","2d0ececb":"from tqdm import tqdm\npreds = []\nfor prediction in tqdm(result):\n    for class_probability in prediction:\n      preds.append(float(class_probability))\n\nresults = []\nfor i in tqdm(range(0,len(preds),2)):\n  if preds[i] < 0.9:\n    results.append(1)\n  else:\n    results.append(0)","ad330e65":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nprint(accuracy_score(np.array(results), test_labels))\nprint(f1_score(np.array(results), test_labels))","f987489f":"# Downloading all necessary dependencies\nYou will have to turn on internet for that.\n\nThis code is slightly modefied version of this colab notebook https:\/\/colab.research.google.com\/github\/tensorflow\/tpu\/blob\/master\/tools\/colab\/bert_finetuning_with_cloud_tpus.ipynb","bdcce898":"I've run a test with 30% of Quora data on free colab cloud TPU and achieved f1 score of 0.684.(11th place at the moment)\n\n**You can't use BERT in the competition, the notebook will fail when it comes to real testing.\nI apologies for littering the leaderboard, but I couldn't believe that local test where so high, I had to check.**\n\nTraining took about 30-40 minutes.\nResults are really amazing, espetially because it's a raw model on random 30% sample with no optimization or ensamble, using the simlest of 3 released models.\nI didn't even have to preprocess anything, model does it for you.\n","1fa2e3b3":"Example below is done on preprocessing code, similar to **CoLa**:\n\nThe Corpus of Linguistic Acceptability is\na binary single-sentence classification task, where \nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not\n\nYou can use pretrained BERT model for wide variety of tasks, including classification.\nThe task of CoLa is close to the task of Quora competition, so I thought it woud be interesting to use that example.\nObviously, outside sources aren't allowed in Quora competition, so you won't be able to use BERT to submit a prediction.\n\n\n\n","95310c69":"<img src=\"https:\/\/image.ibb.co\/gGCZ0A\/image.jpg\" alt=\"image\" border=\"0\">","2ba5e5cf":"There are several downsides for BERT at this moment:\n\n- Training is expensive. All results on the paper were fine-tuned on a single Cloud TPU, which has 64GB of RAM. It is currently not possible to re-produce most of the BERT-Large results on the paper using a GPU with 12GB - 16GB of RAM, because the maximum batch size that can fit in memory is too small. \n\n- At the moment BERT supports only English, though addition of other languages is expected.\n\n","30f977ec":"![](https:\/\/www.lyrn.ai\/wp-content\/uploads\/2018\/11\/transformer.png)","84cd2f6e":"# Competition test\n","cbd35112":"#                                                                                 BERT\n\nBERT, or Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.\n\nAcademic paper which describes BERT in detail and provides full results on a number of tasks can be found here: https:\/\/arxiv.org\/abs\/1810.04805.\n\nGithub account for the paper can be found here: https:\/\/github.com\/google-research\/bert\n\nBERT is a method of pre-training language representations, meaning training of a general-purpose \"language understanding\" model on a large text corpus (like Wikipedia), and then using that model for downstream NLP tasks (like question answering). BERT outperforms previous methods because it is the first *unsupervised, deeply bidirectional *system for pre-training NLP."}}