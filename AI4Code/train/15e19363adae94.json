{"cell_type":{"e32cda05":"code","af06bf1a":"code","955ae9e0":"code","b0a6cb3a":"code","6a6bd830":"code","864c1d31":"code","351f33de":"code","30b81e99":"code","84ec37ff":"code","ef3fd266":"code","908eeb0c":"code","4f893e18":"code","9cb1a4d1":"code","bc29c13a":"code","e1fba1fb":"code","0968e690":"code","762e1fe3":"code","abc594fd":"code","53f94db1":"code","423e3bc6":"code","e82983de":"code","7b87a00d":"code","feb4bf1c":"code","e3f04e0b":"code","cf3167df":"code","64e81565":"code","ec855e7d":"code","392db58f":"code","3764f7ed":"code","24a0e91d":"markdown","637662a9":"markdown","b048ca5a":"markdown","583c28a8":"markdown","75c8d5e8":"markdown","0faffb6f":"markdown","99e070fa":"markdown","14a824cb":"markdown","a610db12":"markdown","c8e87881":"markdown","45025d80":"markdown","412906fd":"markdown","b35bcb9b":"markdown","46f4b4d9":"markdown","6f5dcff9":"markdown","d0b54ac8":"markdown","5b7653e7":"markdown","f7e11552":"markdown","2a8abc06":"markdown","00cac0c9":"markdown","b9b4b30a":"markdown","119ae838":"markdown","59847741":"markdown","45175400":"markdown","5671232c":"markdown"},"source":{"e32cda05":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","af06bf1a":"metaData1 = pd.read_csv(\"..\/input\/csgo-matchmaking-damage\/esea_meta_demos.part1.csv\")\nmetaData2 = pd.read_csv(\"..\/input\/csgo-matchmaking-damage\/esea_meta_demos.part2.csv\")\nmetaData = pd.concat([metaData1, metaData2]) # combine the data which was seperate","955ae9e0":"metaData.head() # so you can see what the data looks like\n# ignore the winner_team as the majority of the rounds it is simply Team 1 or Team 2","b0a6cb3a":"metaData.shape","6a6bd830":"# picks out the relavent data and groups them by game and map\nmapPrevalence = metaData.groupby(['file','map']).count().reset_index()[['file','map']]\n# renames the column and counts the number of times a map is seen\nmapPrevalence = mapPrevalence['map'].value_counts().reset_index().rename(columns = {'index':'Map', 'map':'Times Played'})\n\n# plots the data\nfig, ax = plt.subplots(figsize = (12,8))\nsns.barplot(x='Map', y='Times Played', data = mapPrevalence)","864c1d31":"mapPrevalence","351f33de":"# finding the number of rounds in each game and renaming the columns\ngameRounds = metaData[['file','round']].groupby('file').max().rename(columns = {'round':'Number of Rounds'}).reset_index()","30b81e99":"# some of the data for the games do not begin at round 1 as can be seen below\nmetaData[['file','round']].groupby('file').min()['round'].value_counts()","84ec37ff":"# it was going to be a little tedious to filter the start time of each round because of the above. So I just found an average\n# start time and used that. The average game times are 2268 second which is small compared to the std of the start times.\nstartTime = metaData[['file','start_seconds']].groupby('file').min().mean()\nstartTimeStd = metaData[['file','start_seconds']].groupby('file').min().std()\nprint(startTime)\nprint(startTimeStd)","ef3fd266":"# finding the total game time\ngameTime = metaData[['file','end_seconds']].groupby('file').max()\n# renaming the columns and subtracting the average start time\ngameTime = gameTime.rename(columns = {'end_seconds':'Game Time \/min'})-startTime.iloc[0]\nprint(gameTime.mean())\n# convert from seconds to minutes\ngameTime \/= 60","908eeb0c":"# plot the results for the number of rounds and minutes per game\n\nfig, ax = plt.subplots(figsize=(18,4))\nsns.violinplot(x = 'Number of Rounds', data = gameRounds, ax = ax)\n\nfig2, ax2 = plt.subplots(figsize=(18,4))\nsns.violinplot(x = 'Game Time \/min', data = gameTime, ax = ax2)","4f893e18":"# create a column with the round duration\nmetaData['Round Duration \/s'] = metaData['end_seconds']-metaData['start_seconds']\n# remove rounds that lasted more than 180 seconds as they must be erroneous\nduration = metaData[metaData['Round Duration \/s'] < 180]\n#calculate the mean and std of the duration and present the data\ndurationMean = duration[['map','Round Duration \/s']].groupby('map').mean()\ndurationStd = duration[['map','Round Duration \/s']].groupby('map').std().rename(columns = {'Round Duration \/s':'Sigma \/s'})\npd.concat([durationMean, durationStd], axis = 1)","9cb1a4d1":"# plotting the data\nfig, ax1 = plt.subplots(figsize = (8, 12))\nsns.set(font_scale = 1.3)\nsns.violinplot(y = 'map', x = 'Round Duration \/s', data = duration)","bc29c13a":"# extracting the relavent columns and renaming them\nmetaDataFormatted = metaData[['round_type','Round Duration \/s']].rename(columns = {'round_type':'Round Type'})\n# renaming the round types so they are more presentable\nmetaDataFormatted['Round Type'] = metaDataFormatted['Round Type'].map({'PISTOL_ROUND':'Pistol Round','ECO':'Eco','SEMI_ECO':'Semi-eco',\n                                                         'NORMAL':'Normal','FORCE_BUY':'Force Buy'})\n\n# plotting the data\nfig, ax = plt.subplots(figsize=(12,8))\nsns.violinplot(y='Round Type', x='Round Duration \/s', data=metaDataFormatted, ax=ax)\nax.set_xlim(0,200)","e1fba1fb":"# define an empty data frame to hold the data about who won the rounds\nwins = pd.DataFrame()\n# extract a dataframe with the relavent information\nwinner = metaData[['map','winner_side']]\nfor mapName in metaData['map'].unique(): # loop through the different maps that are played\n    # calculate the number of times Ts and CTs one per map type\n    tWins = winner[(winner['map']==mapName) & (winner['winner_side']=='Terrorist')].shape[0]\n    ctWins = winner[(winner['map']==mapName) & (winner['winner_side']=='CounterTerrorist')].shape[0]\n    # calculate it as a percentage and append all the data to the wins dataframe\n    total = tWins + ctWins\n    dataToAdd = pd.DataFrame({'Map':[mapName, mapName], 'Wins':[ctWins, tWins], 'Winning Side':['CT','T'], \n                              'Winning Percentage':[ctWins\/total*100, tWins\/total*100],\n                               'Sidedness':[(ctWins-tWins)\/total*100, (tWins-ctWins)\/total*100]})\n    wins = wins.append(dataToAdd, ignore_index = True)\nwins","0968e690":"# plot the data from above\n\nfig1, ax1 = plt.subplots(figsize=(12,8))\nsns.set(font_scale = 1.3)\nsns.barplot(x='Map',y='Winning Percentage',hue='Winning Side',data=wins, ax= ax1)\nax1.legend(loc = 7)\n\nfig2, ax2 = plt.subplots(figsize=(12,8))\nfilteredWins = wins[wins['Winning Side'] == 'T'][['Map','Sidedness']].rename(columns={'Sidedness':'T-Sidedness'})\nsns.barplot(x='Map', y = 'T-Sidedness', palette = ['sandybrown'], data = filteredWins, ax = ax2)","762e1fe3":"killsData1 = pd.read_csv(\"..\/input\/csgo-matchmaking-damage\/esea_master_kills_demos.part1.csv\")\nkillsData2 = pd.read_csv(\"..\/input\/csgo-matchmaking-damage\/esea_master_kills_demos.part2.csv\")\nkillsData = pd.concat([killsData1, killsData2])","abc594fd":"killsData.head()","53f94db1":"# extract the data related to the weapons used on each kill\nweaponData = killsData.groupby(['wp_type','wp']).count().reset_index()[['wp_type','wp','file']]\nweaponData = weaponData.rename(columns = {'wp_type':'Weapon Type', 'wp':'Weapon', 'file':'Kills'})\n\n# create a subplot for each weapon type\nfig, axs = plt.subplots(ncols=1, nrows=8, figsize = (15,25)) # create 8 subplots\nfor ax_num in range(len(axs)): # loop through the subplots and plot the relavent data\n    weaponType = weaponData['Weapon Type'].unique()[ax_num]\n    dataToPlot = weaponData[weaponData['Weapon Type']==weaponType]\n    sns.barplot(x='Weapon', y='Kills', data=dataToPlot, ax = axs[ax_num])","423e3bc6":"# extracting all the raw columns that don't need additional processing\ndataProcessing1 = metaData[['round','ct_eq_val','t_eq_val']]\n\n# converting the map data into a binary input for each map so it can be processed by the machine\nmaps = metaData['map'].unique() # order: overpass, cache, inferno, muirage, train, dust2, cobble, nuke\nmapsBin = pd.DataFrame()\nfor mapp in maps:\n    mapsBin[mapp] = metaData['map'].apply(lambda x: 1 if x == mapp else 0)\n    \n# creating the binary column with who won the previous round\n# first create a series with a single value (to shift the series down)\ns = pd.Series([0])\n# creating a series with 1 if the it is not the first round and 0 if it is\nisNotFirstRound = metaData['round'].apply(lambda x: 0 if x == 1 else 1).rename('isNotFirstRound').reset_index(drop=True)\n# append the series from above, shifting the series down (now when the series are put together match up with the previous\n#   match)\nshiftedWinnerSide = s.append(metaData['winner_side'])\n# create the two series with the binary output whether Ts or CTs won (cutting off the last row as it not meaningful)\ntWinPrev = shiftedWinnerSide.apply(lambda x: 1 if x=='Terrorist' else 0).iloc[:-1]\nctWinPrev = shiftedWinnerSide.apply(lambda x: 1 if x=='CounterTerrorist' else 0).iloc[:-1]\n# resetting the index for the next part\ntWinPrev = tWinPrev.reset_index(drop=True)\nctWinPrev = ctWinPrev.reset_index(drop=True)\n# multiply each series by the \"isNotFirstRound\" from above. This ensures that on pistol round nobody has won previously,\n#    without this first rounds would have a previous winner and the machine may attempt to find a pattern in that\ntWinPrev = (tWinPrev * isNotFirstRound).rename('tWinPrev')\nctWinPrev = (ctWinPrev * isNotFirstRound).rename('tWinPrev')\n# combine the two series into a single dataframe\nprevWinDF = pd.concat([tWinPrev,ctWinPrev], axis = 1)\n\n# convert the round winner to a binary number. This is the data the machine will attempt to predict\ndataToPredict = metaData['winner_side'].apply(lambda x: 1 if x=='Terrorist' else 0)","e82983de":"# creating a list of the different data categories we will use, you can add and remove these as you please\nprocessedCols = [dataProcessing1, mapsBin, tWinPrev, ctWinPrev, dataToPredict]\n# concatenate them into a single dataframe\nprocessedData = pd.concat(processedCols, axis = 'columns', join = 'inner')\n# take a peek at the feature list\nprint('Feature list:')\nprocessedData.columns","7b87a00d":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, f1_score, confusion_matrix","feb4bf1c":"# separtate the data into X and y sets. The datasets are shortened to cut down on processing time! Feel free to crank the set size up or down depending\n# on your resources. You can change this and then run an individual classifier.\nsubset_size = 5000\nX = processedData.drop('winner_side', axis = 1)[:subset_size]\ny = processedData['winner_side'][:subset_size]\n# split them into testing and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n\n# scale the data\nss = StandardScaler()\nX_train = ss.fit_transform(X_train)\nX_test = ss.transform(X_test)","e3f04e0b":"# instantiate lists to hold the values\nKs = []\nKscores = []\n# loop through different K values, testing each one, to find the optimal one\nfor K in range(15,100):\n    kClassifier = KNeighborsClassifier(n_neighbors=K)\n    kClassifier.fit(X_train, y_train)\n    y_pred = kClassifier.predict(X_test)\n    Kscores.append(f1_score(y_test, y_pred))\n    Ks.append(K)\n\n# extract and print the best K value\nbestK = Ks[Kscores.index(max(Kscores))]\nprint(f'Optimal K-value: {bestK}')\n\nkClassifier = KNeighborsClassifier(n_neighbors=bestK)\nkClassifier.fit(X_train, y_train)\ny_pred = kClassifier.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","cf3167df":"from sklearn.svm import SVC\n\n# instantiate hyper variable and scores lists\ngammas = []\nCs = []\nsvmScores = []\n\n# loop through the data, changing gamma and C to find an optimal value. This takes a while depending on the number of cases you test so be careful.\nfor C in np.linspace(0, 10, 20):\n    for gamma in np.linspace(0.01, 1, 20):\n        SVM = SVC(C = C, gamma = gamma)\n        SVM.fit(X_train, y_train)\n        score = SVM.score(X_test, y_test)\n        svmScores.append(score)\n        gammas.append(gamma)\n        Cs.append(C)\n        \n# extract the best C and gamma values\nindex = svmScores.index(max(svmScores))\nbestC = Cs[index]\nbestGamma = gammas[index]\n\nprint(f'Best C value: {bestC}')\nprint(f'Best gamma value: {bestGamma}')\n\nSVM = SVC(C=bestC, gamma=bestGamma)\nSVM.fit(X_train, y_train)\ny_pred = SVM.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","64e81565":"depths = []\nRFCscores = []\nfor depth in range(2, 16):\n    forest = RandomForestClassifier(n_estimators=200, max_depth=depth, random_state=42)\n    forest.fit(X_train, y_train)\n    y_pred = forest.predict(X_test)\n    depths.append(depth)\n    RFCscores.append(f1_score(y_test, y_pred))\n    #print(confusion_matrix(y_test, y_pred))\n    #print(classification_report(y_test, y_pred))\n    \nbestDepth = depths[RFCscores.index(max(RFCscores))]\nprint(f'Best depth setting: {bestDepth}')\n\nforest = RandomForestClassifier(n_estimators=500, max_depth=bestDepth, random_state=42)\nforest.fit(X_train, y_train)\ny_pred = forest.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","ec855e7d":"# feel free to mess around with all the values here to try and get better scores, it's pretty barebones right now.\nMLPC = MLPClassifier(hidden_layer_sizes=(11,11), activation='relu',\n                     batch_size=500, max_iter=200, random_state=42)\nMLPC.fit(X_train, y_train)\ny_pred = MLPC.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","392db58f":"# creates a table indicating whether the bomb was planted for each round. \n# requires somebody to die after the bomb was planted to work and so is not entirely accurate !\nkillsData['binBombPlant'] = killsData['is_bomb_planted'].map({False:0, True:1})\nbombPlantData = killsData[['file','round','binBombPlant']].groupby(['file','round']).max().reset_index()\nbombPlantData.head()","3764f7ed":"# gives how many of each team remained alive after each round\nkillsData.groupby(['file','round'])[['ct_alive', 't_alive']].min()","24a0e91d":"The data above shows the weapons used to kill other players. The sniper and rifle weapon types are dominated by the AWP and AK47 repsectively (as was expected based on experience).\n\nI believe some of the data is erroneous. For example, 12000 players were killed using incendiaries versus 5 with molotovs (the T equivalent). Further inspection shows that many of these incendiary kills were done by Ts, many on other Ts, which is unlikely. It seems also that the number of players killed by the bomb is relatively low (given I would think it should be death by \"world\"). I believe that players killed by the bomb where logged as being killed by an incendiary, and by the bomb planter (explaining the high number of team-kills).\n\n1 person was killed with a decoy, 7 with smoke grenades, and 10 with flashbangs. This isn't very relavent but is kind of funny, poor guys got bonked with tactical grenades.\n\nFeel free to look at the source dataframe to see exactly how many kills each weapon got.","637662a9":"### K-Nearest Neighbours Classifier","b048ca5a":"### Game Durations","583c28a8":"### Quick Side Notes","75c8d5e8":"It can be seen that Mirage and Cache are by far the most picked maps, and Nuke by far the least. A possible reason for this will be shown later when looking at map balance.","0faffb6f":"So as I'm sure you can see, the classifiers are all only slightly better than random at predicting the outcome. I was unable to bring the classifiers' accuracies above 70% (the best I did was 69% at some pouint). So that's pretty poor honestly. It could be that I'm using the machine learning badly (of course a possibility). Even more likely than that is I simply am not analysing the most important features that truly impact the result. This is somewhat expected, the most important determinant in whether a team wins or loses a match is ultimately down primarily to the players' skills, which are not at all implemented here. \n\nTo make a more accurate model I suggest finding information about the teams (or even individual players) to add as features here. It would also be useful to include how many of each kind of weapon each team has etc. I don't know if this data even exists, but it's a thought.\n\nCheers if you actually went through this whole thing.\n","99e070fa":"The top graph shows that games generally have between 20 and 30 rounds which indicates generally balanced matchups. The numbers cuts off abruptly at 30 as that is the maximum number of rounds without overtime. Three further bulges can be seen afterwards (at around 35, 42, and 47) showing the data for further overtimes (which are progressively less common).\n\nThe bottom graph shows that games generally last between 30 and 45 minutes.","14a824cb":"The different types of rounds are very similar, however, \"normal\" rounds are generally a little bit longer. Pistol rounds (first rounds) are often slightly shorter which much less spread in the durations. I speculate that this is due to teams quickly rushinf in to finish the pistol round, knowing their own standard strategy, as well has knowing pretty much what the enemy has.","a610db12":"### Conclusion","c8e87881":"The data shows that on average, rounds in CSGO all last about the same amount of time, regardless of map. In general they last between 80 and 90 seconds. I was expecting more unbalanced maps to have shorter rounds as the side towards which the map is unbalanced may win more quickly. The statistics do not support this theory, however.","45025d80":"In this section I will attempt to use the Scikit-learn library to create a number of supervised machine learning models to predict the outcome of individual rounds. I have attempted to use a neural network (Scikit) as they excel at big data streams like this one. I am currently learning to use PyTorch and may come back to apply what I learn using a PyTorch NN.","412906fd":"### Map Prevalence","b35bcb9b":"#  CSGO Data Analysis","46f4b4d9":"## Machine Learning to Predict Round Outcome","6f5dcff9":"So basically I planned to add the data for the number of people alive in the previous round and the data for whether the bomb was planted. These are in a different file do not contain the same number of games and so would require a good bit more processing to work out and implement. After doing all the above classifiers and finding the scores to be only around 68%, I figured it would be a lot of work and is unlikely to change the results. If you guys are interested I have included some code below to create columns for those two features, but the files need to be matched up.","d0b54ac8":"## Sidedness of the Maps","5b7653e7":"The data shows the number of matches won by Ts and CTs respectively. The data shows that the only truly balanced map is Mirage. The most unbalanced is Nuke (which is heavily CT-sded). It should be noted that Nuke was also the least played map and thus the statistic is based on much less data (Nuke was played just 63 times compared to Mirage's 5300).","f7e11552":"### Multi-Layered Perceptron Classifier","2a8abc06":"### Random Forest Classifier","00cac0c9":"The machine will take into account the following features:\n- The T and CT economy (how much equipment they buy)\n- The map\n- Who won the previous round\n- What round number it is\n- How many players were alive at the end of last round\n- Whether it is the first round\n- Whether the bomb was planted last round\n\nThe different models that will be tested are:\n- SVM\n- K nearest neighbours\n- Random forest classifier\n- Multi-Layered Perceptron\n\nFeel free to take this code and edit the classifier parameters as you please to try and get a better score!","b9b4b30a":"Plot the results","119ae838":"### Map Round Durations","59847741":"## Game Time","45175400":"### SVM Classifier","5671232c":"## Kills Analysis"}}