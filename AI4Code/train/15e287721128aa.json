{"cell_type":{"a06b95c8":"code","4c1b070b":"code","375c300f":"code","731c29a3":"code","78d450f2":"code","34f3a496":"code","0df251af":"code","cf69833d":"code","e117d153":"code","8ebab72d":"code","eaaac1c6":"code","b9c975ce":"code","3e0ef6ce":"code","86e9b96c":"code","8efe4edf":"code","78e1b11c":"code","7f0b5388":"code","6f0c5616":"code","b6cbbf94":"code","643d70cf":"code","be6cff75":"code","12addcf5":"code","c9281b96":"code","a11125c9":"code","fdd7b5f4":"code","e213013b":"code","70c536af":"code","aa937754":"code","6293db9d":"code","e87e85e3":"code","8e44b1a6":"code","0318b530":"code","b82254a5":"markdown","11aba8eb":"markdown","be243d44":"markdown","4668cabe":"markdown","8edfcca7":"markdown","55fb5d72":"markdown","4e7eb0fb":"markdown","50e7ad2c":"markdown","12eb82cf":"markdown","9290ca66":"markdown","b86c9650":"markdown","f3e8e5ac":"markdown","394898d0":"markdown"},"source":{"a06b95c8":"#importing library\nimport pandas as pd \nimport numpy as np\nimport seaborn as sns\nimport missingno as msno\nimport pandas_profiling as pp\nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n%matplotlib inline\n","4c1b070b":"#reading the data\ndata=pd.read_csv(r'\/kaggle\/input\/heart-disease-prediction-using-logistic-regression\/framingham.csv')\ndata.head()","375c300f":"# Quick undersatning of the data\n# the shape of th data\nprint('The data consists of {columns} columns and {rows} rows'.format(columns=data.shape[1],rows=data.shape[0]))","731c29a3":"# Data Types\ndata.dtypes","78d450f2":"# checking the missing values\ndata.isnull().sum()","34f3a496":"# visualize the missing data\nmsno.bar(data,color='blue')\nplt.show()","0df251af":"# Extracting the columns that have missing values\nmissing={}\nfor i in range(0,len(data.isnull().sum().index)):\n    if data.isnull().sum()[i]!=0:\n        missing[data.isnull().sum().index[i]]=data.isnull().sum()[i]\nmissing_data = pd.Series(missing).to_frame() #creating a data frame to be easy to manipulate with\nmissing_data=missing_data.rename(columns={0:'missing_value'}) # naming the column of th missing values\nmissing_data\n","cf69833d":"# visualize the distribution of the missing values\nmsno.matrix(data)","e117d153":"# handling the missing values\n# filling the missing values in glucose column with the mode\ndata['glucose'].fillna(data['glucose'].mode()[0],inplace=True)","8ebab72d":"# droping all the missing values\ndata.dropna(inplace=True)\ndata.isnull().sum()","eaaac1c6":"data.describe()","b9c975ce":"sns.pairplot(data,hue='TenYearCHD')\nplt.show()            ","3e0ef6ce":"#droping the columns that have lower correlation with the label\n# droping columns that has Pearson correlation coefficient less than 0.05 and -0.05\nX=data.drop(['TenYearCHD','prevalentStroke','heartRate','currentSmoker','education'],axis=1)\ny=data['TenYearCHD']","86e9b96c":"# dealing with labels\ndata['TenYearCHD'].value_counts()\nprint('{} have no risk of coronary heart disease and {} have'.format(data['TenYearCHD'].value_counts()[0],\n                                                                    data['TenYearCHD'].value_counts()[1]))","8efe4edf":"# The correlation between TenYearCHD(label) and the other columns(features)\ncorrelation=data.corr()\ncorrelation['TenYearCHD']\ncorrelation_data = pd.Series(correlation['TenYearCHD']).to_frame()\ncorrelation_data.sort_values(by='TenYearCHD',ascending=False)","78e1b11c":"#Checking the features\n# finding the correlation between the features to reduce the number of features if we could\ncorr=X.corr()\nplt.figure(figsize=(15,8))\nmask=np.triu(np.ones_like(corr,dtype='bool'))\nsns.heatmap(corr,mask=mask,annot=True)","7f0b5388":"X=X.drop(['sysBP'],axis=1)\n","6f0c5616":"# counting the values in TenYearCHD column\ny.value_counts()","b6cbbf94":"X.isnull().sum()","643d70cf":"#scaling the datat\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nX_scaled=scaler.fit_transform(X)","be6cff75":"#spliting the data\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X_scaled,y,test_size=0.3,random_state=1111)","12addcf5":"def evaluation(model,y_pred):\n    train_score=model.score(X_train,y_train)\n    test_score=model.score(X_test,y_test)\n    print('the training score is {}'.format(round(train_score,2)))\n    print('the test score is {}'.format(round(test_score,2)))\n    print('_________________________________')\n    from sklearn.metrics import confusion_matrix,recall_score,precision_score,accuracy_score\n    print('The confusion matrix is')\n    print(confusion_matrix(y_test,y_pred))\n    print('_________________________________')\n    recall=recall_score(y_test,y_pred)\n    precision=precision_score(y_test,y_pred)\n    accuracy=accuracy_score(y_test,y_pred)\n    print('recall score is {}'.format(round(recall,2)))\n    print('precision score is {}'.format(round(precision,2)))\n    print('Accuracy score is {}'.format(round(accuracy,2)))\n","c9281b96":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(X_train,y_train)\ny_pred_lr=lr.predict(X_test)\nevaluation(lr,y_pred_lr)","a11125c9":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(n_estimators=500,max_depth=10,n_jobs=-1,)\nrf_clf.fit(X_train, y_train)\ny_pred_rf=rf_clf.predict(X_test)\nevaluation(rf_clf,y_pred_rf)","fdd7b5f4":"#AdaBoost\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nada=AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=500,algorithm='SAMME.R')\nada.fit(X_train,y_train)\ny_pred_ada=ada.predict(X_test)\nevaluation(ada,y_pred_ada)","e213013b":"positive_label=data[data['TenYearCHD']==1]\nextened_data=pd.concat([data,positive_label],ignore_index=1)\nextened_data=extened_data.sample(frac=1).reset_index()\nextened_data.head()","70c536af":"extened_data['TenYearCHD'].value_counts()","aa937754":"X=extened_data.drop(['TenYearCHD','prevalentStroke','heartRate','currentSmoker','education','sysBP'],axis=1)\ny=extened_data['TenYearCHD']\nscaler=StandardScaler()\nX_scaled=scaler.fit_transform(X)\nX_train,X_test,y_train,y_test=train_test_split(X_scaled,y,test_size=0.2,random_state=1111)","6293db9d":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(X_train,y_train)\ny_pred_lr=lr.predict(X_test)\nevaluation(lr,y_pred_lr)","e87e85e3":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(n_estimators=500,max_depth=8,n_jobs=-1,)\nrf_clf.fit(X_train, y_train)\ny_pred_rf=rf_clf.predict(X_test)\nevaluation(rf_clf,y_pred_rf)","8e44b1a6":"#AdaBoost\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nada=AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=500,algorithm='SAMME.R',learning_rate=1)\nada.fit(X_train,y_train)\ny_pred_ada=ada.predict(X_test)\nevaluation(ada,y_pred_ada)","0318b530":"from sklearn.ensemble import VotingClassifier\nvoting=VotingClassifier(estimators=[('LogisticRegression',lr),('RandomForestClassifier',rf_clf),('AdaBoostClassifier',ada)],\n                       voting='soft')\nvoting.fit(X_train,y_train)\ny_pred=voting.predict(X_test)\nevaluation(voting,y_pred)","b82254a5":"<a name='summary' ><\/a>\n# Summary Statistics","11aba8eb":"<a name='miss' ><\/a>\n# Missing Values","be243d44":"<a name='spliting' ><\/a>\n# Spliting The Data","4668cabe":"## Table of contents\n- [Importing Libraries And Packages](#lib)\n- [Reading the dataset](#data)\n- [Understanding the data](#under)\n- [Missing values](#miss)\n- [Summary statistics](#summary)\n- [Feature Engineering](#feature)\n- [Scaling the data](#scaling)\n- [Spliting the data](#spliting)\n- [Modeling](#model)\n","8edfcca7":"<a name='scaling' ><\/a>\n# Scaling The Data","55fb5d72":"<a name='lib'> <\/a>\n# Importing Libraries And Packages","4e7eb0fb":"<a name='under' ><\/a>\n# Understanding the data","50e7ad2c":"> the number of people that have the risk of coronary heart disease is very lower compared to the number of people that they haven't <br> which means that it could affect the model","12eb82cf":"<a name='feature' ><\/a>\n# Feature Engineering","9290ca66":"<a name='data' ><\/a>\n# Reading The Dataset","b86c9650":"> the two columns [sysBP , diaBP]  have a strong correlation <br> so we could drop one of them to avoid overfitting problems","f3e8e5ac":"<a name='model' ><\/a>\n# Modeling The Data","394898d0":"> so because of the poor preformance of each model and that's because the number of positive labels are so small compared to the negative labels So we will increasing the positive labels by repeating them again in the dataset\n"}}