{"cell_type":{"0d1cf607":"code","fbed9f72":"code","2d89b1f4":"code","ef79768a":"code","f9ce780e":"code","701699a5":"code","f2d49447":"code","c2999152":"code","e6f31d65":"code","82a88467":"code","17247179":"code","4292cddd":"code","00ebb485":"code","727754ad":"code","dfaf86cb":"markdown","ce3f8caf":"markdown","6732d118":"markdown","6ca5d789":"markdown","ddb93a1b":"markdown","b6c3d577":"markdown"},"source":{"0d1cf607":"import tensorflow as tf\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndata = pd.read_csv('..\/input\/ner-dataset\/ner_datasetreference.csv', encoding= 'unicode_escape')\nprint(data.shape)\ndata.head()","fbed9f72":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize = (16,8))\nsns.set_style('darkgrid')\nsns.histplot(data['Tag'])\nplt.tight_layout(pad=2)\nplt.show()","2d89b1f4":"from itertools import chain\ndef get_dict_map(data, token_or_tag):\n    tok2idx = {}\n    idx2tok = {}\n    \n    if token_or_tag == 'token':\n        vocab = list(set(data['Word'].to_list()))\n    else:\n        vocab = list(set(data['Tag'].to_list()))\n    \n    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n    return tok2idx, idx2tok\ntoken2idx, idx2token = get_dict_map(data, 'token')\ntag2idx, idx2tag = get_dict_map(data, 'tag')\n#Code language: Python (python)","ef79768a":"data['Word_idx'] = data['Word'].map(token2idx)\ndata['Tag_idx'] = data['Tag'].map(tag2idx)\ndata_fillna = data.fillna(method='ffill', axis=0)\n# Groupby and collect columns\ndata_group = data_fillna.groupby(\n['Sentence #'],as_index=False\n)['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))","f9ce780e":"import numpy as np \nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\n\ndef get_pad_train_test_val(data_group, data):\n\n    #get max token and tag length\n    n_token = len(list(set(data['Word'].to_list())))\n    n_tag = len(list(set(data['Tag'].to_list())))\n\n    #Pad tokens (X var)    \n    tokens = data_group['Word_idx'].tolist()\n    maxlen = max([len(s) for s in tokens])\n    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n\n    #Pad Tags (y var) and convert it into one hot encoding\n    tags = data_group['Tag_idx'].tolist()\n    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx[\"O\"])\n    n_tags = len(tag2idx)\n    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n    \n    #Split train, test and validation set\n    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.25,train_size =0.75, random_state=2020)\n\n    print(\n        'train_tokens length:', len(train_tokens),\n        '\\ntrain_tokens length:', len(train_tokens),\n        '\\ntest_tokens length:', len(test_tokens),\n        '\\ntest_tags:', len(test_tags),\n        '\\nval_tokens:', len(val_tokens),\n        '\\nval_tags:', len(val_tags),\n    )\n    \n    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags\n\ntrain_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, data)\ntrain_tags = np.array(train_tags)\nval_tags = np.array(val_tags)\ntest_tags = np.array(test_tags)\nprint('train_tags: ',train_tags.shape,'val_tags: ',val_tags.shape,'test_tags: ',test_tags.shape)","701699a5":"from sklearn.utils import class_weight\n\ntrain_temp_tags = np.ravel(np.argmax(train_tags, axis=-1))\nprint(len(train_temp_tags))\n\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(train_temp_tags),\n                                                 train_temp_tags)\nclass_weight_dict = dict(enumerate(class_weights))\nprint(class_weight_dict)\n\ntrain_label = np.argmax(train_tags, axis=-1)\nprint(train_tokens.shape)\nprint(train_label.shape)","f2d49447":"import numpy as np\nimport tensorflow\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Input\nfrom tensorflow.keras.utils import plot_model\nfrom numpy.random import seed\nseed(1)\ntensorflow.random.set_seed(2)","c2999152":"input_dim = len(list(set(data['Word'].to_list())))+1\noutput_dim = 64\ninput_length = max([len(s) for s in data_group['Word_idx'].tolist()])\nn_tags = len(tag2idx)","e6f31d65":"def get_bilstm_lstm_model():\n    model = Sequential()\n    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n    model.add((Dense(n_tags, activation=\"softmax\")))\n\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.summary()\n    \n    return model","82a88467":"maj_index = tag2idx['O']\n\n''' Change it More for Better Fine-Tuning '''\n\nclass_weight_dict[maj_index] = 28.5277954105731576 \nprint(class_weight_dict)\nsample_weights = np.ones(shape=(len(train_label), train_label.shape[-1]))\nfor i in range(17):\n    sample_weights[train_label == i] = class_weight_dict.get(i)\nprint(sample_weights.shape)","17247179":"nlp_model = get_bilstm_lstm_model()\nplot_model(nlp_model)\nhis = nlp_model.fit(train_tokens, train_label , batch_size = 64, epochs=5, validation_split=0.2, sample_weight = sample_weights) \ntf.keras.models.save_model(nlp_model, filepath  = \".\/nlp_model.h5\")","4292cddd":"nlp_model = tf.keras.models.load_model('.\/nlp_model.h5')\n\ny_test = np.argmax(test_tags, axis=-1)\nprint(test_tokens.shape,y_test.shape)\nnlp_model.evaluate(test_tokens, y_test)","00ebb485":"def predict(seed):\n    query  = test_tokens[seed]\n    query_text = []\n    for i in query.tolist():\n        query_text.append(idx2token.get(i))\n    print('Query_Text: ',' '.join(query_text[:10]))\n\n    ans = y_test[seed]\n    ans_text = []\n    for i in ans.tolist():\n        ans_text.append(idx2tag.get(i))\n    print('Tag_Text: ',' '.join(ans_text[:10]))\n\n    query = query.reshape(1,-1)\n    pred = nlp_model.predict(query)\n    pred = np.ravel(np.argmax(pred, axis=-1))\n    print('Query: ',query.shape,'Prediction: ',pred.shape)\n\n    pred_list = []\n    for i in pred.tolist():\n        pred_list.append(idx2tag.get(i))\n    print('Prediction_Text: ',' '.join(pred_list[:10])) \n    print()\n    print('--- Better-Representation---')\n    print()\n    rep_qr = []\n    for q, r_tag in zip(query_text[:10], ans_text[:10]):\n        rep_qr.append(q)\n        rep_qr.append('['+r_tag+']')\n    print('Actual_NER: ',' '.join(rep_qr),'....')\n    print()\n    rep_qp = []\n    for q, r_tag in zip(query_text[:10], pred_list[:10]):\n        rep_qp.append(q)\n        rep_qp.append('['+r_tag+']')\n    print('--'*70)\n    print()\n    print('Predicted_NER: ',' '.join(rep_qp),'....')\n\nseed = 1\npredict(seed)","727754ad":"import spacy\nfrom spacy import displacy\nnlp = spacy.load('en_core_web_sm')\ntext = nlp('A spokesman says he expects the Tibetan leader to return')\ndisplacy.render(text, style = 'ent', jupyter=True)","dfaf86cb":"<img style=\"height:70px; width:70px; float:left;\" src = \"https:\/\/64.media.tumblr.com\/037cef717d8faf83379d7b6a36c4ed3a\/tumblr_ont3zyBF7r1uf5j8co1_400.gifv\"><img style=\"height:70px; width:70px; float:right;\" src = \"https:\/\/64.media.tumblr.com\/037cef717d8faf83379d7b6a36c4ed3a\/tumblr_ont3zyBF7r1uf5j8co1_400.gifv\"><h2 style = \"background-color: black; color:white;text-align:center;\"><b><i> Inference <\/i><\/b><h2>","ce3f8caf":"#### **Model -  Evaluation**","6732d118":"<img  style = \"width:100%; height:80%;\" src = \"https:\/\/www.cloud-trade.com\/hubfs\/Imported_Blog_Media\/Hero-Image-Template-2-Mar-10-2021-01-33-58-89-PM.jpg\">\n<img style=\"height:70px; width:70px; float:left;\" src = \"http:\/\/25.media.tumblr.com\/2e07f1879192037cffcffc1f2ca0cf8c\/tumblr_mld1zftHtz1qkjjfoo1_500.gif\"><img style=\"height:70px; width:70px; float:right;\" src = \"http:\/\/25.media.tumblr.com\/2e07f1879192037cffcffc1f2ca0cf8c\/tumblr_mld1zftHtz1qkjjfoo1_500.gif\"><h1 style = \"background-color: black; color:white;text-align:center;\"><b>Named<\/b><i> Entitiy <\/i><b>Recognition<\/b>","6ca5d789":"<img style=\"height:70px; width:70px; float:left;\" src = \"https:\/\/64.media.tumblr.com\/1a4eb159a9792663977408170e86a33a\/tumblr_o1q075uyTb1uf5j8co1_500.gifv\"><img style=\"height:70px; width:70px; float:right;\" src = \"https:\/\/64.media.tumblr.com\/1a4eb159a9792663977408170e86a33a\/tumblr_o1q075uyTb1uf5j8co1_500.gifv\"><h2 style = \"background-color: black; color:white;text-align:center;\"><b><i>Model-Creation & Training<\/i><\/b><h2>","ddb93a1b":"#### **SpaCy Implementation**","b6c3d577":"<img style=\"height:70px; width:70px; float:left;\" src = \"https:\/\/i.pinimg.com\/originals\/e8\/1d\/8d\/e81d8d8032101ffab2ddc3fd1e2d8ab1.gif\"><img style=\"height:70px; width:70px; float:right;\" src = \"https:\/\/i.pinimg.com\/originals\/e8\/1d\/8d\/e81d8d8032101ffab2ddc3fd1e2d8ab1.gif\"><h2 style = \"background-color: black; color:white;text-align:center;\"><b><i>Data Preprocessing<\/i><\/b><h2>"}}