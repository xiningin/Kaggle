{"cell_type":{"c05416c6":"code","c5bfefea":"code","ce7bc865":"code","2f6216d4":"code","4c2564ba":"code","6084758f":"code","a4ed1957":"code","8f1b4cbc":"code","47d30c42":"code","6155ae1d":"code","8b86c167":"code","ee4d3f7b":"code","68f86352":"code","1882f365":"code","f76d4435":"code","6d9fc5cd":"code","7767689e":"code","0af357f2":"code","eaa7c146":"code","9b21c3a3":"code","57426c25":"code","c9ecfbf5":"code","4c468b95":"code","d6a095a8":"code","83ca335d":"markdown","cdc6bec0":"markdown","4ec42d59":"markdown","7064aeea":"markdown","5921749f":"markdown","22959a36":"markdown","1f222377":"markdown","ea90cee3":"markdown","541531f2":"markdown","db5d1c46":"markdown","e0594782":"markdown","4482b714":"markdown","34462665":"markdown","44ace5b5":"markdown","52cb94f4":"markdown","bec0e072":"markdown","86d45370":"markdown","f6b568bf":"markdown","440bdc5b":"markdown"},"source":{"c05416c6":"# importing libraries\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom mlxtend.frequent_patterns import association_rules, apriori\n\nimport warnings\nwarnings.filterwarnings('ignore')","c5bfefea":"# reading the data\ndata = pd.read_csv('\/kaggle\/input\/the-bread-basket\/bread basket.csv')\n\n# looking top 10 rows\ndata.head(10)","ce7bc865":"# looking the bigger picture\ndata.info()","2f6216d4":"# Converting the 'date_time' column into the right format\ndata['date_time'] = pd.to_datetime(data['date_time'])","4c2564ba":"data.head()","6084758f":"# Count of unique customers\ndata.Transaction.nunique()","a4ed1957":"# Extracting date\ndata['date'] = data['date_time'].dt.date\ndata['date'] = pd.to_datetime(data['date'], format = '%Y-%m-%d')\n\n# Extracting time\ndata['time'] = data['date_time'].dt.time\n\n# Extracting month and replacing it with text\ndata['month'] = data['date_time'].dt.month\ndata['month'] = data['month'].replace((1,2,3,4,5,6,7,8,9,10,11,12), \n                                          ('January','February','March','April','May','June','July','August',\n                                          'September','October','November','December'))\n\n# Extracting hour\ndata['hour'] = data['date_time'].dt.hour\n# Replacing hours with text\nhour_in_num = (1,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23)\nhour_in_obj = ('1-2','7-8','8-9','9-10','10-11','11-12','12-13','13-14','14-15',\n               '15-16','16-17','17-18','18-19','19-20','20-21','21-22','22-23','23-24')\ndata['hour'] = data['hour'].replace(hour_in_num, hour_in_obj)\n\n# Extracting weekday and replacing it with text\ndata['weekday'] = data['date_time'].dt.weekday\ndata['weekday'] = data['weekday'].replace((0,1,2,3,4,5,6), \n                                          ('Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'))\n\n# dropping date_time column\ndata.drop('date_time', axis = 1, inplace = True)\n\ndata.head()","8f1b4cbc":"# cleaning the item column\ndata['Item'] = data['Item'].str.strip()\ndata['Item'] = data['Item'].str.lower()","47d30c42":"# looking 10 rows of data\ndata.head(10)","6155ae1d":"all_headlines = ' '.join(data['Item'])\nwordcloud = WordCloud(width = 3000, height = 2000, background_color = 'white', \n                      collocations = False).generate((all_headlines))\nplt.figure(figsize = (15, 5))\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","8b86c167":"plt.figure(figsize=(15,5))\nsns.barplot(x = data.Item.value_counts().head(20).index, y = data.Item.value_counts().head(20).values, color='pink')\nplt.xlabel('Items', size = 15)\nplt.xticks(rotation=45)\nplt.ylabel('Count of Items', size = 15)\nplt.title('Top 20 Items purchased by customers', color = 'green', size = 20)\nplt.show()","ee4d3f7b":"monthTran = data.groupby('month')['Transaction'].count().reset_index()\nmonthTran.loc[:,\"monthorder\"] = [4,8,12,2,1,7,6,3,5,11,10,9]\nmonthTran.sort_values(\"monthorder\",inplace=True)\n\n\nplt.figure(figsize=(12,5))\nsns.barplot(data = monthTran, x = \"month\", y = \"Transaction\")\nplt.xlabel('Months', size = 15)\nplt.ylabel('Orders per month', size = 15)\nplt.title('Number of orders received each month', color = 'green', size = 20)\nplt.show()","68f86352":"plt.figure(figsize=(10,5))\nsns.barplot(x = data.period_day.value_counts().index, y = data.period_day.value_counts().values, color='pink')\nplt.xlabel('Period', size = 15)\nplt.ylabel('Orders per period', size = 15)\nplt.title('Number of orders received in each period of a day', color = 'green', size = 20)\nplt.show()","1882f365":"hourTran = data.groupby('hour')['Transaction'].count().reset_index()\nhourTran.loc[:,\"hourorder\"] = [1,10,11,12,13,14,15,16,17,18,19,20,21,22,23,7,8,9]\nhourTran.sort_values(\"hourorder\",inplace=True)\n\nplt.figure(figsize=(12,5))\nsns.barplot(data = hourTran, x = \"hour\", y = \"Transaction\")\nplt.xlabel('Hours', size = 15)\nplt.ylabel('Orders each hour', size = 15)\nplt.title('Count of orders received each hour', color = 'green', size = 20)\nplt.show()","f76d4435":"weekTran = data.groupby('weekday')['Transaction'].count().reset_index()\nweekTran.loc[:,\"weekorder\"] = [5,1,6,7,4,2,3]\nweekTran.sort_values(\"weekorder\",inplace=True)\n\n\nplt.figure(figsize=(10,5))\nsns.barplot(data = weekTran, x = \"weekday\", y = \"Transaction\", color='pink')\nplt.xlabel('Weekdays', size = 15)\nplt.ylabel('Orders per weekday', size = 15)\nplt.title('Number of orders received each of the weekday', color = 'green', size = 20)\nplt.show()","6d9fc5cd":"data.groupby('date')['Transaction'].count().plot(kind=\"line\",figsize=(15,7),color='purple')\nplt.xlabel('Date', size = 15)\nplt.ylabel('Count of Transaction', size = 15)\nplt.hlines(y = 129, color='red', xmin=data['date'].min(), xmax=data['date'].max(),\n           linestyles='dashed', label='Mean:129')\nplt.title('Transactions per day',  color = 'green', size = 20)\nplt.legend(fontsize='large')\nplt.show()","7767689e":"# getting dates where number of transactions are more than 200\ndates = data.groupby('date')['Transaction'].count().reset_index()\ndates = dates[dates['Transaction']>=200].sort_values('date').reset_index(drop=True)\n\ndates = pd.merge(dates,data[['date','weekday']],on='date', how='inner')\ndates.drop_duplicates(inplace=True)\ndates","0af357f2":"df = data.groupby(['period_day','Item'])['Transaction'].count().reset_index().sort_values(['period_day','Transaction'],ascending=False)\nday = ['morning','afternoon','evening','night']\n\nplt.figure(figsize=(15,8))\nfor i,j in enumerate(day):\n    plt.subplot(2,2,i+1)\n    df1 = df[df.period_day==j].head(10)\n    sns.barplot(data=df1, y=df1.Item, x=df1.Transaction, color='pink')\n    plt.xlabel('')\n    plt.ylabel('')\n    plt.title('Top 10 items people like to order in \"{}\"'.format(j), size=13)\n\nplt.show()","eaa7c146":"# grouping the data with respect to transaction and item and look at the count of each item in each transaction\n\ndf = data.groupby(['Transaction','Item'])['Item'].count().reset_index(name='Count')\ndf","9b21c3a3":"# making a mxn matrice where m=transaction and n=items and each row represents whether the item was in the transaction or not\n\nmy_basket = df.pivot_table(index='Transaction', columns='Item', values='Count', aggfunc='sum').fillna(0)\n# df.groupby(['Transaction','Item'])['Count'].sum().unstack().reset_index().fillna(0).set_index('Transaction')\n\nmy_basket.head()","57426c25":"# making a function which returns 0 or 1\n# 0 means item was not in that transaction, 1 means item present in that transaction\n\ndef encode(x):\n    if x<=0:\n        return 0\n    if x>=1:\n        return 1\n\n# applying the function to the dataset\n\nmy_basket_sets = my_basket.applymap(encode)\nmy_basket_sets.head()","c9ecfbf5":"# using the 'apriori algorithm' with min_support=0.01 (1% of 9465)\n# It means the item should be present in atleast 94 transaction out of 9465 transactions only when we considered that item in\n# frequent itemset\n\nfrequent_itemsets = apriori(my_basket_sets, min_support = 0.01, use_colnames = True)\nfrequent_itemsets","4c468b95":"# now making the rules from frequent itemset generated above\n\nrules = association_rules(frequent_itemsets, metric = \"lift\", min_threshold = 1)\nrules.sort_values('confidence', ascending = False, inplace = True)\nrules","d6a095a8":"# arranging the data from highest to lowest with respect to 'confidence'\n\nrules.sort_values('confidence', ascending=False)","83ca335d":"#### Observation:\nMost orders are from the `winter months` i.e. people order more in winters.","cdc6bec0":"#### Observation\nThe above word cloud is formed based upon the item list in the dataset. Larger the frequency of the item, larger the size of the word.<br>\nWe seeing that **coffee**, **bread**, **tea**, **cake**, **pastry**, **sandwich** are bigger in sizes. As the frequency of item is lower the size of word is also smaller.","4ec42d59":"***Thank you!!***","7064aeea":"---\n# <font color = 'orange'>Exploratory Data Analysis","5921749f":"#### Obervation:\n1. The mean number of orders per day was around 129. We can see a lot of fluctuation in the data. It is also noted that on `1st Jan'2017 the store was closed` as there was no order on that day.\n2. The above table shows days where the number of transactions are more than 200 and it is been observed that 80% of them are weekends.","22959a36":"#### Observations:\nFriday, Saturday and Sunday was the days where orders are higher than rest of the days. Together these three days account for 48% of total order received in a week. ","1f222377":"#### Observation:\n1. Around 48% of transactions contains `cofffee`, 33% contains `bread`, 14% contains `tea` and 10% contains `cake`.\n2. Together coffee and bread 9%, coffee and tea 5%, coffee and cake 5%, bread and tea 3%, bread and cake 2%, tea and cake 2%.","ea90cee3":"### <font color = 'blue'>The analysis of the association rules depend on five measures (but only first three are widely used)<\/font>\n1. `Support`: Support of the item x is nothing but the ratio of the number of transactions in which the item x appears to the total number of transactions. \n    - **Support(Item A) = (Transactions containing Item A) \/ (Total transactions), range:[0,1]**\n2. `Confidence`: Confidence refers to the likelihood that an item B is also bought if item A is bought. It can be calculated by finding the number of transactions where A and B are bought together, divided by total number of transactions where A is bought. A confidence of 0.5 would mean that in 50% of the cases where A were purchased, the purchase also included B. For product recommendation, a 50% confidence may be perfectly acceptable but in a medical situation, this level may not be high enough.\n    - **Confidence(A \u2192 B) = (Transactions containing both (A and B)) \/ (Transactions containing A), range:[0,1]**\n3. `Lift`: Lift(A \u2192 B) refers to the increase in the ratio of sale of B when A is sold. Lift (A \u2192 B) is nothing but the \u2018interestingness\u2019 or the likelihood of the item B being purchased when the item A is sold. A Lift of 1 means there is no association between products A and B. Lift of greater than 1 means products A and B are more likely to be bought together. Finally, Lift of less than 1 refers to the case where two products are unlikely to be bought together.\n    - **Lift(A \u2192 B) = (Confidence (A \u2192 B)) \/ (Support (B)), range:[0,\u221e]**\n4. `Leverage`: Leverage computes the difference between the observed frequency of A and C appearing together and the frequency that would be expected if A and C were independent. An leverage value of 0 indicates independence.\n    - **levarage(A \u2192 C) = support(A \u2192 C) \u2212 support(A)\u00d7support(C), range:[\u22121,1]**\n5. `Conviction`: A high conviction value means that the consequent is highly depending on the antecedent. For instance, in the case of a perfect confidence score, the denominator becomes 0 (due to 1 - 1) for which the conviction score is defined as 'inf'.Similar to lift, if items are independent, the conviction is 1.\n    - **conviction(A \u2192 C) = (1\u2212support(C)) \/ (1\u2212confidence(A \u2192 C)), range:[0,\u221e]**","541531f2":"#### Observation:\nMore than 50% of orders came in afternoon i.e. between 12 noon to 5 pm and specifically between 12-2 it was 48%.","db5d1c46":"---\n# <font color = 'orange'>Reading and Cleaning","e0594782":"### <font color = 'blue'>Which factor can determine the importance of a rule in Apriori Association Rule, Confidence or Support or both?<\/font>\nOne way in which you can relate these concepts is to think in the following way:<br>\n- Support is similar to Recall metric, a rule with high support means it has high presence on the dataset.<br>\n- Confidence is similar to Precision metric, a rule with high confidence means it has high precision whenever the rule appears.\n<br><br>While mining association rules,`you prefer the ones with high confidence` (precision) over ones with high support (recall). That is why usually the minSupport may be on the lower side, even lower than 50%, while minConfidence is usually set higher, above 50% for example.","4482b714":"---\n# <font color = 'orange'>Final Comment<\/font>\nOn the basis of high confidence (considered greater than or equal to 0.55), we have the following rules:\n- toast \u2192 coffee\n- spanish brunch \u2192 coffee \n- medialuna \u2192 coffee\n- pastry \u2192 coffee\n<br> Also note that these rules have *lift > 1* which means those pairs have positive correlation between them.\n<br><br> It is been observed that (coffee, tea) \u2192 (cake) has a highest lift of 1.94 which indicates that they have high correlation between them. Here lift of 1.94 means the likelihood of a customer buying all coffee, tea and cake together is 1.94 times more than the chance of purchasing cake alone.","34462665":"#### <font color = 'red'>How to read the tabel ?<\/font>\nLet us consider the first row of the table. \n>It can be read as `Toast \u2192 Coffee` {antecedents \u2192 consequents}\n>>It stated that **if Toast then Coffee** which means when Toast is ordered people also ordered Coffee and to support this statement we have *confidence* and *lift* which are 0.70 and 1.47 respectively which are very good.\n\n- *antecedents*: It is an item (here toast) who support the other item (coffee).\n- *consequents*: It is an item (here coffee) who is supported by the an item (toast).\n- *antecedent support*: Support of an antecedent (toast). It states that 3.36% of transactions contain toast.\n- *consequent support*: Support of consequent (coffee). It states that 47.84% of transactions contain coffee.\n- *support*: Support of an both antecedent and consequent. It states that 2.37% of transactions contain both toast and coffee.\n- *confidence*: A confidence of 0.7 would mean that in 70% of the cases where toast were purchased, the purchase also included coffee.\n- *lift*: Lift of greater than 1 means products A and B are more likely to be bought together. Here lift of 1.47 means the likelihood of a customer buying both toast and coffee together is 1.47 times more than the chance of purchasing coffee alone.","44ace5b5":"----\n# <font color = 'orange'>Market Basket Analysis\nAlso known as **Association Rule Analysis**<br>\nTechnique to solve MBA is **Apriori Algorithm**","52cb94f4":"![image.png](attachment:image.png)","bec0e072":"### <font color = 'blue'>Apriori Algorithm<\/font>\n1. Apriori algorithm is given by R. Agrawal and R. Srikant in 1994 for finding frequent itemsets in a dataset for boolean association rule. Name of the algorithm is Apriori because it uses prior knowledge of frequent itemset properties.\n2. Apriori algorithm assumes that any subset of a frequent itemset must be frequent.The value of \u201cfrequent itemset\u201d > than a threshold value(i.e. support). Its the algorithm behind Market Basket Analysis.\n3. Apriori assumes that: **All subsets of a frequent itemset must be frequent(Apriori propertry).If an itemset is infrequent, all its supersets will be infrequent.** Say, a transaction containing {Grapes, Apple, Mango} also contains {Grapes, Mango}. So, according to the principle of Apriori, if {Grapes, Apple, Mango} is frequent, then {Grapes, Mango} must also be frequent.","86d45370":"- There is no missing values in the data\n- Converting 'date_time' column into datetime\n- Create some extra features from date_time column for analysis purpose","f6b568bf":"### <font color = 'blue'>What is Market Basket Analysis?<\/font>\n- MARKET Basket Analysis (MBA) is an association analysis and is a popular data mining technique. It\u2019s a kind of knowledge discovery in data (KDD) and this technique can be applied in various fields of work.\n- In market basket analysis (also called **association analysis** or **frequent itemset mining**), you analyze purchases that commonly happen together. For example, people who buy bread and peanut butter also buy jelly. Or people who buy shampoo might also buy conditioner. What relationships there are between items is the target of the analysis. Knowing what your customers tend to buy together can help with marketing efforts and store\/website layout.\n- Market Basket Analysis is one of the key techniques used by large retailers to uncover associations between items. It works by looking for combinations of items that occur together frequently in transactions. To put it another way, `it allows retailers to identify relationships between the items that people buy`.\n- The discovery of these associations can help retailers develop marketing strategies by gaining insight into which items are frequently purchased together by customers. The strategies may include:\n    1. Changing the store layout according to trends\n    2. Customer behavior analysis\n    3. Catalog design\n    4. Cross marketing on online stores\n    5. What are the trending items customers buy\n    6. Customized emails with add-on sales etc..","440bdc5b":"#### Observation:\nMaxium number of orders came in morning 11-12. But from previous graph the percentage of order was maxium in afternoon and we can also see in this figure that between 12-5 the bars are high."}}