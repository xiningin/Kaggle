{"cell_type":{"aeffabc8":"code","a7c468e5":"code","0aba7d22":"code","01492a55":"code","e8e4650d":"code","0630b25a":"code","07cc14c7":"code","9db3add0":"code","aa0835be":"code","6f94739f":"code","3e5d3f6f":"code","3f434887":"code","35160869":"code","c6c82d99":"code","99f40bf2":"code","8da835ed":"code","cf3e0ae8":"code","1839deb7":"code","d1cdb7e6":"code","d7a5187f":"code","43fa7822":"code","43ba5f1a":"code","c6f2de40":"code","875ed25d":"code","febfcb95":"code","12ddbe2f":"code","5224ea38":"code","2f242b98":"code","60bb3c94":"code","54353f5f":"code","e2a6f649":"code","dc5f4c79":"code","dff14b5e":"code","8455587e":"code","2453cf52":"code","675dc4fb":"code","4094ca87":"code","044141a8":"code","b13708ad":"code","c339307c":"code","035efd1d":"code","5a36c3fc":"code","9bed6505":"code","a455caf4":"code","53e2c2a2":"code","63808572":"markdown","f15c370b":"markdown","85db0648":"markdown","4181bad3":"markdown","75c92552":"markdown","9142f1ac":"markdown","4c738aac":"markdown","3681d741":"markdown","b54e3a11":"markdown","d61b79a4":"markdown","8bf7840b":"markdown","d8bcb5ba":"markdown","09a8cc28":"markdown","ac7894d7":"markdown","bab0c7cd":"markdown","bcf64056":"markdown","7a6de30e":"markdown","a00a6b18":"markdown","7e57ea42":"markdown","0f62dbe5":"markdown","8a603c48":"markdown","95c89b22":"markdown","d8d7e0fa":"markdown","6c2cd4fb":"markdown","9aad8a45":"markdown","3d780262":"markdown","8e1bb6de":"markdown","e1ecb635":"markdown","357fdf67":"markdown","1aca7a12":"markdown","c8ac9580":"markdown","5f30f04d":"markdown","fc355f82":"markdown","2e835d19":"markdown","8400d6a9":"markdown","74619e1a":"markdown","d722fd8f":"markdown","7e8fbfde":"markdown","c0135293":"markdown","6a9f5aa0":"markdown","5c586175":"markdown","7833955f":"markdown","19c83901":"markdown","107363d3":"markdown","6559ec27":"markdown"},"source":{"aeffabc8":"# data processing\nimport pandas as pd\n\n## linear algebra\nimport numpy as np\n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n \nfrom sklearn.metrics import accuracy_score  #for accuracy_score\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.metrics import confusion_matrix #for confusion matrix","a7c468e5":"titanic = pd.read_csv('..\/input\/titanic\/train.csv')\n# Print the first 5 rows of the dataframe.\ntitanic.head(5)\n","0aba7d22":"titanic_test = pd.read_csv('..\/input\/titanic\/test.csv')\n# Print the last 5 rows of the dataframe.\ntitanic_test.tail(5)","01492a55":"#shape command will give number of rows\/samples\/examples and number of columns\/features\/predictors in dataset\n#(rows,columns)\ntitanic.shape","e8e4650d":"# Describe gives statistical information about numerical columns in the dataset\ntitanic.describe()\n#you can check from count if there are missing values in columns, here we can see there are some missing values in column \"Age\"","0630b25a":"# info method provides information about dataset like.\n# total values in each column, null\/not null, datatype, memory occupied etc.\ntitanic.info()","07cc14c7":"# Let's write a function to print the total percentage of the missing values.\n# (this can be a good exercise for beginners to try to write simple functions like this.)\n\n#This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\ndef missing_data(df):\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = round(df.isnull().sum().sort_values(ascending = False) * 100 \/len(df),2)\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])","9db3add0":"# lets check missing values by columns\nmissing_data(titanic)","aa0835be":"# Check missing values in test data set\nmissing_data(titanic_test)","6f94739f":"drop_column = ['Cabin']\ntitanic.drop(drop_column, axis= 1, inplace = True)\ntitanic_test.drop(drop_column,axis = 1,inplace = True)","3e5d3f6f":"#COMPLETING: complete or delete missing values in train and test\/validation dataset\ndataset = [titanic, titanic_test]\n\n# def missing_data(x):\nfor data in dataset:\n    #complete missing age with median\n    data['Age'].fillna(data['Age'].median(), inplace = True)\n\n    #complete missing Embarked with Mode\n    data['Embarked'].fillna(data['Embarked'].mode()[0], inplace = True)\n\n    #complete missing Fare with median\n    data['Fare'].fillna(data['Fare'].median(), inplace = True)\n      \nmissing_data(titanic)","3f434887":"def draw(graph):\n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()\/2., height + 5,height ,ha= \"center\")","35160869":"sns.set(style=\"darkgrid\")\nplt.figure(figsize = (8, 5))\ngraph= sns.countplot(x='Survived', hue=\"Survived\", data=titanic)\ndraw(graph)","c6c82d99":"plt.figure(figsize = (8, 5))\ngraph  = sns.countplot(x =\"Sex\", hue =\"Survived\", data = titanic)\ndraw(graph)","99f40bf2":" \nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,4)) \nx = sns.countplot(titanic['Pclass'], ax=ax[0])\ny = sns.countplot(titanic['Embarked'], ax=ax[1])\ndraw(x)\ndraw(y)\nfig.show()","8da835ed":"FacetGrid = sns.FacetGrid(titanic, col='Pclass', height=4, aspect=1)\nFacetGrid.map(sns.pointplot, 'Embarked', 'Survived', 'Sex', palette=None,  order=None, hue_order=None)\nFacetGrid.add_legend()","cf3e0ae8":"drop_column = ['Embarked']\ntitanic.drop(drop_column, axis=1, inplace = True)\ntitanic_test.drop(drop_column,axis=1,inplace=True)","1839deb7":"plt.figure(figsize = (8, 5))\npclass= sns.countplot(x='Pclass', hue='Survived', data=titanic)\ndraw(pclass)","d1cdb7e6":"plt.figure(figsize = (8, 5))\nsns.barplot(x='Pclass', y='Survived', data=titanic)","d7a5187f":"# combine test and train as single to apply some function, we will use it again in Data Preprocessing\nall_data=[titanic,titanic_test]\n\nfor dataset in all_data:\n    dataset['Family'] = dataset['SibSp'] + dataset['Parch'] + 1","43fa7822":"axes = sns.factorplot('Family','Survived', \n                      data=titanic, aspect = 2.5, )","43ba5f1a":"axes = sns.factorplot('Family','Age','Survived',\n                      data=titanic, aspect = 2.5, )","c6f2de40":"# create bin for age features. \nfor dataset in all_data:\n    dataset['Age_bin'] = pd.cut(dataset['Age'], bins=[0,12,20,40,120], labels=['Children','Teenage','Adult','Elder'])\n    \nplt.figure(figsize = (8, 5))\nsns.barplot(x='Age_bin', y='Survived', data=titanic)","875ed25d":"plt.figure(figsize = (8, 5))\nag = sns.countplot(x='Age_bin', hue='Survived', data=titanic)\ndraw(ag)","febfcb95":"AAS = titanic[['Sex','Age_bin','Survived']].groupby(['Sex','Age_bin'],as_index=False).mean()\nsns.factorplot('Age_bin','Survived','Sex', data=AAS\n                ,aspect=3,kind='bar')\nplt.suptitle('Age , Sex vs Survived')","12ddbe2f":"# create bin for fare features\nfor dataset in all_data:\n    dataset['Fare_bin'] = pd.cut(dataset['Fare'], bins=[0,10,50,100,550], labels=['Low_fare','median_fare','Average_fare','high_fare'])\nplt.figure(figsize = (8, 5))\nag = sns.countplot(x='Pclass', hue='Fare_bin', data=titanic)","5224ea38":"sns.barplot(x='Fare_bin', y='Survived', data=titanic)","2f242b98":"pd.DataFrame(abs(titanic.corr()['Survived']).sort_values(ascending = False))","60bb3c94":"# Generate a mask for the upper triangle (taken from seaborn example gallery)\ncorr=titanic.corr()#['Survived']\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.subplots(figsize = (12,8))\nsns.heatmap(corr, \n            annot=True,\n            mask = mask,\n            cmap = 'RdBu',\n            linewidths=.9, \n            linecolor='white',\n            vmax = 0.3,\n            fmt='.2f',\n            center = 0,\n            square=True)\nplt.title(\"Correlations Matrix\", y = 1,fontsize = 20, pad = 20);","54353f5f":"titanic.info()","e2a6f649":"# Convert \u2018Sex\u2019 feature into numeric.\ngenders = {\"male\": 0, \"female\": 1}\n\nfor dataset in all_data:\n    dataset['Sex'] = dataset['Sex'].map(genders)\ntitanic['Sex'].value_counts()","dc5f4c79":"for dataset in all_data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 15, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 15) & (dataset['Age'] <= 20), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 20) & (dataset['Age'] <= 26), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 28), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 28) & (dataset['Age'] <= 35), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 35) & (dataset['Age'] <= 45), 'Age'] = 5\n    dataset.loc[ dataset['Age'] > 45, 'Age'] = 6\ntitanic['Age'].value_counts()","dff14b5e":"# As we created new fetures form existing one, so we remove that one.\n\n# Removing SibSp & Parch because we have family now. same way Age.\n# We also going to remove some other features like passenger id in list, Ticket number and Name.\n\nfor dataset in all_data:\n    drop_column = ['Age_bin','Fare','Name','Ticket', 'PassengerId','SibSp','Parch','Fare_bin']\n    dataset.drop(drop_column, axis=1, inplace = True)\n","8455587e":"all_features = titanic.drop(\"Survived\",axis=1)\nTargete = titanic[\"Survived\"]\nX_train,X_test,y_train,y_test = train_test_split(all_features,Targete,test_size=0.3,random_state=0)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","2453cf52":"model = LogisticRegression()\nmodel.fit(X_train,y_train)\nprediction_lr=model.predict(X_test)\nLog_acc = round(accuracy_score(prediction_lr,y_test)*100,2)\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nLog_cv_acc=cross_val_score(model,all_features,Targete,cv=10,scoring='accuracy')\n\nprint('The accuracy of the Logistic Regression is',Log_acc)\nprint('The cross validated score for Logistic REgression is:',round(Log_cv_acc.mean()*100,2))","675dc4fb":"knn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(X_train, y_train)  \nY_pred = knn.predict(X_test)  \nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nkfold = KFold(n_splits=10, random_state=22) \nresult_knn=cross_val_score(model,all_features,Targete,cv=10,scoring='accuracy')\n\nprint('The accuracy of the K Nearst Neighbors Classifier is',round(accuracy_score(Y_pred,y_test)*100,2))\nprint('The cross validated score for K Nearest Neighbors Classifier is:',round(result_knn.mean()*100,2))","4094ca87":"from sklearn.naive_bayes import GaussianNB\nmodel= GaussianNB()\nmodel.fit(X_train,y_train)\nprediction_gnb=model.predict(X_test) \nnb_acc = round(accuracy_score(prediction_gnb,y_test)*100,2)\nkfold = KFold(n_splits=12, random_state=22)\nresult_gnb=cross_val_score(model,all_features,Targete,cv=12,scoring='accuracy')\n\nprint('The accuracy of the Gaussian Naive Bayes Classifier is',nb_acc)\nprint('The cross validated score for Gaussian Naive Bayes classifier is:',round(result_gnb.mean()*100,2))","044141a8":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\n\nY_pred = linear_svc.predict(X_test)\n\nacc_linear_svc = round(linear_svc.score(X_train, y_train) * 100, 2)\nkfold = KFold(n_splits=5, random_state=22)\nresult_svm=cross_val_score(model,all_features,Targete,cv=10,scoring='accuracy')\n\nprint('The accuracy of the Support Vector Machines Classifier is',acc_linear_svc)\nprint('The cross validated score for Support Vector Machines Classifier is:',round(result_svm.mean()*100,2))","b13708ad":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\n\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_rm=cross_val_score(model,all_features,Targete,cv=10,scoring='accuracy')\n\nprint('The accuracy of the Random Forest Classifier is',acc_random_forest)\nprint('The cross validated score for Random Forest Classifier is:',round(result_rm.mean()*100,2))","c339307c":"decision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, y_train)\nY_pred = decision_tree.predict(X_test) \nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\n\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_rm=cross_val_score(model,all_features,Targete,cv=10,scoring='accuracy')\n\nprint('The accuracy of the Random Forest Classifier is',acc_decision_tree)\nprint('The cross validated score for Random Forest Classifier is:',round(result_rm.mean()*100,2))","035efd1d":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes',  \n              'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn, Log_acc, \n              acc_random_forest, nb_acc, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Model')\nresult_df.head(9)","5a36c3fc":"predictions = cross_val_predict(random_forest, X_train, y_train, cv=3)\nconfusion_matrix(y_train, predictions)","9bed6505":"from sklearn.metrics import precision_score, recall_score\nprint(\"Precision:\", precision_score(y_train, predictions))\nprint(\"Recall:\",recall_score(y_train, predictions))","a455caf4":"from sklearn.metrics import f1_score\nf1_score(y_train, predictions)","53e2c2a2":"from sklearn.metrics import roc_curve\n# compute true positive rate and false positive rate\ny_scores = random_forest.predict_proba(X_train)\ny_scores = y_scores[:,1]\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, y_scores)\n# plotting them against each other\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([-0.05, 1.05, -0.05, 1.05])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n\nplt.figure(figsize=(14, 7))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","63808572":"### F-Score\nThe F-score is computed with the harmonic mean of precision and recall. Note that it assigns much more weight to low values. As a result of that, the classifier will only get a high F-score, if both recall and precision are high.","f15c370b":"Age, Fare and cabin has missing values. we will see how to work with these missing values next.\n#### Column \"Cabin\" has more than 75% of missing values in Train and test both dataset, so we are remove it.  \n#### Suggestion: Not to impute missing data in columns, which have more than 40% of missing data. ","85db0648":"High fare for passenger class one, and survived count are higher for Pclass-1 and High fare. \n\nLow fare for passenger class three, and survived count are lower for Pclass-3 and low fare.\n\nso they are co-related with each other. \n\nNow we drop fare form our data set but before that we check correlation.","4181bad3":"Graph of survival vs family represent that \n\n- If you are alone then your survival chances is around 25%\n- If you have family of 4 members then survival chances is highest around 70%\n- If you have more family members then your survival chances is reducing. \n\n### 6. Age vs Survived\nHere another feature is important the age value & sex of all family members.\n\nBut as we already seen that for sex Female have more probability of survive as compare to Male.\n\nSo lets check Combination of Family and Age for survival.","75c92552":"### About Titanic\n\nThe RMS Titanic was a British passenger liner that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after it collided with an iceberg during its maiden voyage from Southampton to New York City. There were an estimated 2,224 passengers and crew aboard the ship, and more than 1,500 passengers and crew died, That's why the name DieTanic. The Titanic was the largest ship afloat at the time it entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. The Titanic was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, her architect, died in the unforgettable disaster.\n\n<img src=\"https:\/\/i.ibb.co\/7jqRQ7c\/Titanic-side-plan.png\" alt=\"Titanic\" style=\"width:850px;height:300px;\">\n<ul>\n\n<br>","9142f1ac":"### ROC AUC Curve\nAnother way to evaluate and compare your binary classifier is provided by the ROC AUC Curve.\n\nThis curve plots the true positive rate (also called recall) against the false positive rate (ratio of incorrectly classified negative instances), instead of plotting the precision versus the recall.","4c738aac":"## Lets check missing data\n----","3681d741":"We can see that Age, Embarked and cabin has missing values.\nnow, lets check missing values for test data.","b54e3a11":"### 2. Sex vs Survived\nLets check How many male and female survival.","d61b79a4":"First Lets check how many person in different Embarked and Passenger Class. ","8bf7840b":"### 5. SibSp & Parch vs Survived\n\n**SibSp** and **Parch** would make more sense, Parents not let child die, Bond of Blood relation always help each other first, rather than helping others they think about them self and them family member. \n\nCreate new feature Family Size as a combination of SibSp and Parch,","d8bcb5ba":"Here is only age no relation with family passenger is calculated alone with his\/her age.\n\nSo children who have age less than 12 years have high chances to survival.\n\nnow think whats logic behind that?\n\nYes, Parents and elder siblings. who think to save them younger once first. \n\nBut here display probability\/chances to survival only not displayed count how many Children, Teenager, Adults and Elders on Titanic. So lets check Count of survived for each age.","09a8cc28":"we can start evaluating it\u2019s performace in a more accurate way. Previously we only used accuracy.\n\nThe problem is just, that it\u2019s more complicated to evaluate a classification model than a regression model. We will talk about this in the following section.\n## Further Evaluation\n----\n### Confusion Matrix:","ac7894d7":"### Precision and Recall:","bab0c7cd":"## Predictive Modeling\nWe have gained some insights from the EDA part. But with that, we cannot accurately predict or tell whether a passenger will survive or die. So now we will predict the whether the Passenger will survive or not using some great Classification Algorithms.","bcf64056":"### 5. Random Forest:","7a6de30e":"### 2. K Nearest Neighbor:","a00a6b18":"## Conclusion\nThis was my very first Kaggle competition and climbing up the leaderboard one step at a time was definitely a really nice journey.\n\nFrom the community, I learned that more feature engineering could be done here and for this reason, I will probably try a ticket grouping approach soon.\n\nThank you for taking the time to read through my first exploration of a Kaggle dataset.\n\nFor the moment, let me know if you found this notebook useful or you just liked it: I would really appreciate it!\n\n## Plz Upvote!","7e57ea42":"### 3. Gaussian Naive Bayes","0f62dbe5":"## Getting  the data\n----------------","8a603c48":"### 1.  Survived Count\nLets visualize how many passenger & crew survived and how many not. for that create frequency diagram or count plot. ","95c89b22":"lets check which model performed best here.","d8d7e0fa":"Here we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person is in class 1. We will create another pclass plot below.","6c2cd4fb":"## Data Exploration\/Analysis\n--------","9aad8a45":"### 3. Embarked & PClass vs Survived\n\nAs per the graph we can say that Gender is matter to survival if gender is Female then higher survival chances.\nNow we will check from which location(Embarked & PClass) gender have more chances to survive.\n\n* **Embarked:** From which location passenger go on board to Titanic.\n * C = Cherbourg\n * Q = Queenstown \n * S = Southampton\n \n* **PClass:** Passenger belongs to which class.\n * 1st = Upper\n * 2nd = Middle\n * 3rd = Lower","3d780262":"### Feature engineering\n\nFeature engineering is the art of converting raw data into useful features. There are several feature engineering techniques that you can apply to be an artist.","8e1bb6de":"What do you think Embarked is important feature to predict high survival rate?  \n\n**From which location passenger go on board to Titanic does it matter or its more important that passenger is on Titanic, no matter from where you go on board to Titanic.** as we know that At **2:20 a.m. on April 15, 1912**, the British ocean liner Titanic sinks into the North Atlantic Ocean.\n\nIts night time, high cold weather (The temperature of the water was -2.2 degrees Celsius when Titanic was sinking), and Job-Location\/Rest-room(Passenger class) allocated to everyone on Titanic.\n\nWe can use **Embarked** as feature here for getting high accuracy but logically its doesn't matter. so we drop it out.\n\nAs a part of data science you have to thing 360 degree angle, some features are important but in calculation(mathematically) its not, so that why you must have domain knowledge for feature selection.","e1ecb635":"Here we see age is important feature so lets check relation of age and sex for survived.\n\nThis different combination give us idea for selection of correct feature by programmatically and logically.\n\nSo lets plot the graph of Age & Sex vs Survived","357fdf67":"### 4. Passenger Class Vs Survived\nLets check Pclass impact on survival, as we already know the location of different passenger class.\n\n* 1st = Upper\n* 2nd = Middle\n* 3rd = Lower","1aca7a12":"### 4. Linear Support Vector Machine:","c8ac9580":"### 7. Fare Vs Survived\n\nBefore we go ahead lets understand the feature \"Fare\"\n\n**What is \"Fare\"?**\n\nA fare is the fee paid by a passenger for travel in Titanic in other words we can say Fare is the Ticket price.\n\nLets check Fare value is related to Passenger class? Logically YES.\n\nso, lets plot graph of it.","5f30f04d":"## Which is the best Model ?","fc355f82":"Now you get more confident that person who have age around 30 have more chances to survive.\n\nAnd in family if all members age near to 30 then survival probability is very high. But in real life(practically) very less chances. If all young siblings travel in Titanic then its work here.\n\nBut we can see that if you are alone and your age is around 30 then your survival chances are 50%\n\nWhile you with family with 5 members and all are around 30 then your survival chance very high.\n\nSo family & age features are very important.","2e835d19":"So we can say that fare is correlated with Passenger class.\n\nHigh fare then upper class, Low fare then lower class.\n\nNow check For survived relation with Fare_bin and compare result of it with Pclass vs Survived.","8400d6a9":"Looks like very unfair with male passenger,\n\nin all age band Female have more chances to survived.\n\n**Think on this:** Male are good person to help them or Female are smart person to survive. ","74619e1a":"### 6. Decision tree","d722fd8f":"## Data Exploration\/ Analysis\/ Visualizing \n------\n#### Let's do analysis of data and Check which features could contribute to a high survival rate ?","7e8fbfde":"The first row is about the not-survived-predictions: 331 passengers were **correctly classified as not survived (called true negatives)** and 50 where **wrongly classified as not survived (false positives)**.\n\nThe second row is about the survived-predictions: 79 passengers where **wrongly classified as survived (false negatives)** and 247 where **correctly classified as survived (true positives).**","c0135293":"Here we can say random forest give best result here, so lets Understand little bit more about Random Forest.\n\n### What is Random Forest ?\nRandom Forest is a supervised learning algorithm. Like you can already see from it\u2019s name, it creates a forest and makes it somehow random. The \u201eforest\u201c it builds, is an ensemble of Decision Trees, most of the time trained with the \u201cbagging\u201d method. The general idea of the bagging method is that a combination of learning models increases the overall result.\n\nTo say it in simple words: \n\n**Random forest builds multiple decision trees and merges them together to get a more accurate and stable prediction.**\n\nOne big advantage of random forest is, that it can be used for both classification and regression problems, which form the majority of current machine learning systems. With a few exceptions a random-forest classifier has all the hyperparameters of a decision-tree classifier and also all the hyperparameters of a bagging classifier, to control the ensemble itself.\n\nThe random-forest algorithm brings extra randomness into the model, when it is growing the trees. Instead of searching for the best feature while splitting a node, it searches for the best feature among a random subset of features. This process creates a wide diversity, which generally results in a better model. Therefore when you are growing a tree in random forest, only a random subset of the features is considered for splitting a node. You can even make trees more random, by using random thresholds on top of it, for each feature rather than searching for the best possible thresholds (like a normal decision tree does).\n\nBelow you can see how a random forest would look like with two trees:\n\n<img src=\"https:\/\/i.ibb.co\/TBbZnGC\/two-tree-random-forest.png\" alt=\"Random Forest\" style=\"width:450px;height:300px;\">\n<ul>\n\n<br>","6a9f5aa0":"# Titanic Simply understand Best Model\n\n\n\n### **Introduction**\nThis is my first stab at a Kaggle script. I have chosen to work with the Titanic dataset after spending some time poking around on the site and looking at other scripts made by other Kagglers for inspiration. \n\nWe will have a detailed statistical analysis of Titanic data set along with Machine learning model implementation.\n\n<h2 style=\"color:blue\"><center> Don't forget to upvote\ud83d\udcc8 if you like\ud83d\udc4d\ud83c\udffb it! It's free! \ud83c\udf97\ufe0f\ud83c\udfaf ","5c586175":"## Import Necessary Libraries and Data Sets.\n-----------------------------","7833955f":"## Correlation & Correlation Matrix\n----\n\nThe first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\n\n**POSITIVE CORRELATION:** If an increase in feature A leads to increase in feature B, then they are positively correlated. A value 1 means perfect positive correlation.\n\n**NEGATIVE CORRELATION:** If an increase in feature A leads to decrease in feature B, then they are negatively correlated. A value -1 means perfect negative correlation.","19c83901":"### 1. Logistic Regression:","107363d3":"## Table of content:\n* About Titanic\n* Import Necessary Libraries\n* Handle Missing Values\n* Data Exploration\/ Analysis\/ Visualizing\n* Correlation & Correlation Matrix\n* Feature Engineering\n* Predictive Modeling \n>  1. Logistic Regression\n>  2. KNN Classifier\n>  3. Gaussian Naive Bayes\n>  4. Support Vector Machine(SVM)\n>  5. Decision Tree\n>  6. Random Forest\n* Confusion Matrix\n* Precision and Recall\n* F-Score\n* ROC AUC Curve\n","6559ec27":"Note: There is no  survived column in test data, which is our target variable and we will going to predict it."}}