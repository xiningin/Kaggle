{"cell_type":{"f514ac26":"code","e9650813":"code","d48bd796":"code","f7356c26":"code","9ef226bd":"code","1fcaa392":"code","7a6b9f23":"code","a2e1f315":"code","39736d92":"code","52d720b9":"code","386ec357":"code","94915f16":"code","ea42a932":"code","2c0bc090":"code","8e4df117":"code","c3f3eb64":"code","95df5915":"code","b905ed2d":"code","3015dc48":"code","e6f837d9":"code","ed1f2abc":"code","c20b93f2":"code","b0c220f6":"code","62857001":"code","34c8dfdf":"code","22b3d741":"code","bd061696":"code","3c5fb1b3":"code","8a89b610":"code","5b72a290":"code","a18ec245":"code","d9daa038":"code","36d89218":"markdown","a29dc8bc":"markdown","c148eecf":"markdown","5a360e3c":"markdown","063417a1":"markdown","d1c59193":"markdown","7456cb76":"markdown","3295f7ab":"markdown","2324db25":"markdown","7eb9da82":"markdown"},"source":{"f514ac26":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n","e9650813":"train = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/test.csv\")","d48bd796":"cont_features = [col for col in train.columns if col.startswith(\"cont\")]\nlen(cont_features)","f7356c26":"y = train[\"target\"]\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\noof = np.zeros(len(train))\nscore_list = []\nfold = 1\ntest_preds = []\nseed_list = [None,2,3]# Use more. Original list: [None,2,3,4,5]\n\n\nfor train_index, test_index in kf.split(train):\n    X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n    \n\n    X_train = X_train.abs() # Taking aboslute was also a bit improving.\n\n    y_pred_list = []\n    for seed in seed_list:\n        dtrain = lgbm.Dataset(X_train[cont_features], y_train)\n        dvalid = lgbm.Dataset(X_val[cont_features], y_val)\n        print(seed)\n        params = {\"objective\": \"regression\",\n              \"metric\": \"rmse\",\n              \"verbosity\": -1,\n              \"boosting_type\": \"gbdt\",\n              \"feature_fraction\":0.5,\n              \"num_leaves\": 250,\n              \"lambda_l1\":7,\n              \"lambda_l2\":2,\n              \"learning_rate\":0.01,\n              'min_child_samples': 35,\n              \"bagging_fraction\":0.75,\n              \"bagging_freq\":1,\n             }\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                        dtrain,\n                        valid_sets=[dtrain, dvalid],\n                        verbose_eval=100,\n                        num_boost_round=100000,\n                        early_stopping_rounds=100\n                    )\n\n        dtrain = lgbm.Dataset(X_train[cont_features], y_train)\n        dvalid = lgbm.Dataset(X_val[cont_features], y_val)\n        params = {\"objective\": \"regression\",\n                  \"metric\": \"rmse\",\n                  \"verbosity\": -1,\n                  \"boosting_type\": \"gbdt\",\n                  \"feature_fraction\":0.5,\n                  \"num_leaves\": 350,\n                  \"lambda_l1\":7,\n                  \"lambda_l2\":1,\n                  \"learning_rate\":0.003,\n                  'min_child_samples': 35,\n                  \"bagging_fraction\":0.8,\n                  \"bagging_freq\":1,\n                 }\n        \n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                            dtrain,\n                            valid_sets=[dtrain, dvalid],\n                            verbose_eval=100,\n                            num_boost_round=100000,\n                            early_stopping_rounds=200,\n                           init_model = model\n                        )\n\n    \n    \n        y_pred_list.append(model.predict(X_val[cont_features]))\n        print(np.sqrt(mean_squared_error(y_val,   np.mean(y_pred_list,axis=0)       )))\n        test_preds.append(model.predict(test[cont_features]))\n        \n    \n   \n    \n    oof[test_index] = np.mean(y_pred_list,axis=0)    \n    score = np.sqrt(mean_squared_error(y_val, oof[test_index]))\n    score_list.append(score)\n    print(f\"RMSE Fold-{fold} : {score}\")\n    fold+=1\n\nnp.mean(score_list)","9ef226bd":"print(score_list)\nnp.mean(score_list)","1fcaa392":"train[\"1_preds\"] = oof\ntest[\"1_preds\"] = np.mean(test_preds,axis=0)","7a6b9f23":"threshold = 8\ntrain[\"target_class\"] = train[\"target\"].apply(lambda x: 1 if x > threshold  else 0)\ntrain.groupby(\"target_class\")[\"target\"].mean()","a2e1f315":"train[train.target_class == 0].target.hist(bins=100)\ntrain[train.target_class == 1].target.hist(bins=100)","39736d92":"#X = X.abs()\nfrom sklearn.metrics import roc_auc_score\n\ny = train[\"target_class\"]\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\noof = np.zeros(len(train))\nscore_list = []\nfold = 1\ntest_preds = []\n\nfor train_index, test_index in kf.split(train):\n    X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n   \n\n    #X_train = X_train.abs()\n    dtrain_0 = lgbm.Dataset(X_train[cont_features], y_train)\n    dvalid = lgbm.Dataset(X_val[cont_features], y_val)\n    \n    params = {\"objective\": \"binary\",\n              \"metric\": \"auc\",\n              \"verbosity\": -1,\n              \"boosting_type\": \"gbdt\",\n              \"feature_fraction\":0.5,\n              \"num_leaves\": 200,\n              \"lambda_l1\":7,\n              \"lambda_l2\":0,\n              \"learning_rate\":0.01,\n              'min_child_samples': 0,\n              \"bagging_fraction\":0.75,\n              \"bagging_freq\":1,\n             }\n    model_0 = lgbm.train(params,\n                    dtrain_0,\n                    valid_sets=[dtrain_0, dvalid],\n                    verbose_eval=100,\n                    num_boost_round=100000,\n                    early_stopping_rounds=100\n                )\n    \n    y_pred_0 = model_0.predict(X_val[cont_features])    \n    score = roc_auc_score(y_val,y_pred_0)\n\n    print(f\"AUC Fold-{fold} : {score}\")\n    \n    oof[test_index] = y_pred_0\n    \n    \n    score_list.append(score)\n    test_preds.append(model_0.predict(test[cont_features]))\n    fold+=1\n    \nnp.mean(score_list)","52d720b9":"# Here I get classification predictions for both train and test data.\ntrain[\"class_preds\"] = oof\ntest[\"class_preds\"] = np.mean(test_preds,axis=0)\nroc_auc_score(train[\"target_class\"],train[\"class_preds\"])","386ec357":"#X = X.abs()\ny = train[\"target\"]\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\noof_2 = np.zeros(len(train))\nscore_list = []\nfold = 1\ntest_preds_2 = []\n\nfor train_index, test_index in kf.split(train):\n    X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n    \n\n    # First model with only X_train having class = 0\n    #X_train = X_train.abs()\n    X_train_0 = X_train[X_train.target_class==0]\n    y_train_0 = y_train[X_train.target_class==0]\n    \n    X_train_1 = X_train[X_train.target_class==1]\n    y_train_1 = y_train[X_train.target_class==1]\n    \n    dtrain_0 = lgbm.Dataset(X_train_0[cont_features], y_train_0)\n    dvalid = lgbm.Dataset(X_val[cont_features], y_val)\n    \n    params = {\"objective\": \"regression\",\n              \"metric\": \"rmse\",\n              \"verbosity\": -1,\n              \"boosting_type\": \"gbdt\",\n              \"feature_fraction\":0.5,\n              \"num_leaves\": 250,\n              \"lambda_l1\":4,\n              \"lambda_l2\":2,\n              \"learning_rate\":0.01,\n              'min_child_samples': 35,\n              \"bagging_fraction\":0.75,\n              \"bagging_freq\":1,\n             }\n    model_0 = lgbm.train(params,\n                    dtrain_0,\n                    valid_sets=[dtrain_0, dvalid],\n                    verbose_eval=100,\n                    num_boost_round=100000,\n                    early_stopping_rounds=100\n                )\n    \n    y_pred_0 = model_0.predict(X_val[cont_features])    \n    score = np.sqrt(mean_squared_error(y_val, y_pred_0))\n    print(f\"RMSE Fold-{fold} : {score}\")\n    \n    # Second model with only X_train having class = 1\n    dtrain_1 = lgbm.Dataset(X_train_1[cont_features], y_train_1)\n    dvalid = lgbm.Dataset(X_val[cont_features], y_val)\n    params = {\"objective\": \"regression\",\n              \"metric\": \"rmse\",\n              \"verbosity\": -1,\n              \"boosting_type\": \"gbdt\",\n              \"feature_fraction\":0.5,\n              \"num_leaves\": 250,\n              \"lambda_l1\":4,\n              \"lambda_l2\":2,\n              \"learning_rate\":0.01,\n              'min_child_samples': 35,\n              \"bagging_fraction\":0.75,\n              \"bagging_freq\":1,\n             }\n    model_1 = lgbm.train(params,\n                    dtrain_1,\n                    valid_sets=[dtrain_1, dvalid],\n                    verbose_eval=100,\n                    num_boost_round=100000,\n                    early_stopping_rounds=100\n                )\n    \n    y_pred_1 = model_1.predict(X_val[cont_features])    \n    score = np.sqrt(mean_squared_error(y_val, y_pred_1))\n    print(f\"RMSE Fold-{fold} : {score}\")\n    \n    # Then we are getting final prediction as following. Multiply first model output with the probability of being class 0, then multiply second model\n    # output with the probability of being class 1 and then finally sum them up.\n    oof_2[test_index] = y_pred_0*(1-X_val[\"class_preds\"]) + y_pred_1*(X_val[\"class_preds\"])\n    score = np.sqrt(mean_squared_error(y_val,oof_2[test_index] ))\n    print(f\"RMSE Fold-Final-{fold} : {score}\")\n    \n\n    score_list.append(score)\n    test_preds_2.append(model_0.predict(test[cont_features])*(1-test[\"class_preds\"])  + model_1.predict(test[cont_features])*(test[\"class_preds\"]))\n    fold+=1\n    \n\nnp.mean(score_list)","94915f16":"print(np.mean(score_list))\nscore_list","ea42a932":"train[\"2_preds\"] = oof_2\ntest[\"2_preds\"] = np.mean(test_preds_2,axis=0)\nnp.sqrt(mean_squared_error(train[\"target\"],train[\"2_preds\"] ))","2c0bc090":"from tensorflow.keras.layers import Input,Dense,Dropout\nfrom tensorflow.keras import Model\nfrom  tensorflow.keras.regularizers import l2\nimport tensorflow as tf","8e4df117":"def get_DAE():\n    # denoising autoencoder\n    inputs = Input((14,))\n    x = Dense(500, activation='relu')(inputs) # 1500 original\n    x = Dense(500, activation='relu', name=\"feature\")(x) # 1500 original\n    x = Dense(500, activation='relu')(x) # 1500 original\n    outputs = Dense(14, activation='relu')(x)\n    model = Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer='adam', loss='mse')\n\n    return model","c3f3eb64":"alldata = pd.concat([train[cont_features],test[cont_features]],axis=0)\nprint(alldata.shape)\nautoencoder = get_DAE()\nautoencoder.fit(alldata[cont_features], alldata[cont_features],\n                    epochs=15,\n                    batch_size=256,\n                    shuffle=True\n                    )","95df5915":"# Getting denoised datasets.\ntest_denoised = test.copy()\ntest_denoised[cont_features] = autoencoder.predict(test_denoised[cont_features])\ntrain_denoised = train.copy()\ntrain_denoised[cont_features] = autoencoder.predict(train_denoised[cont_features])","b905ed2d":"train_denoised.head()","3015dc48":"#X = X.abs()\ny = train[\"target\"]\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\noof_3 = np.zeros(len(train))\nscore_list = []\nfold = 1\ntest_preds_3 = []\nseed_list = [None,2,3]# Use more. Original list: [None,2,3,4,5]\n\nfor train_index, test_index in kf.split(train):\n    X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n    X_train_denoised, X_val_denoised = train_denoised.iloc[train_index], train_denoised.iloc[test_index]\n    \n    # Augmenting only training.\n    print(X_train_denoised.shape)\n    X_train_denoised = pd.concat([X_train_denoised,X_train],axis=0)\n    y_train = pd.concat([y_train,y_train],axis=0)\n    print(X_train_denoised.shape)\n    \n    y_pred_list = []\n    \n    for seed in seed_list:\n        \n        dtrain = lgbm.Dataset(X_train_denoised[cont_features], y_train)\n        dvalid = lgbm.Dataset(X_val[cont_features], y_val)\n\n        params = {\"objective\": \"regression\",\n              \"metric\": \"rmse\",\n              \"verbosity\": -1,\n              \"boosting_type\": \"gbdt\",\n              \"feature_fraction\":0.5,\n              \"num_leaves\": 250,\n              \"lambda_l1\":7,\n              \"lambda_l2\":2,\n              \"learning_rate\":0.01,\n              'min_child_samples': 35,\n              \"bagging_fraction\":0.75,\n              \"bagging_freq\":1,\n             }\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                        dtrain,\n                        valid_sets=[dtrain, dvalid],\n                        verbose_eval=100,\n                        num_boost_round=100000,\n                        early_stopping_rounds=100\n                    )\n        \n        \n        dtrain = lgbm.Dataset(X_train_denoised[cont_features], y_train)\n        dvalid = lgbm.Dataset(X_val[cont_features], y_val)\n        \n        params = {\"objective\": \"regression\",\n                  \"metric\": \"rmse\",\n                  \"verbosity\": -1,\n                  \"boosting_type\": \"gbdt\",\n                  \"feature_fraction\":0.5,\n                  \"num_leaves\": 350,\n                  \"lambda_l1\":7,\n                  \"lambda_l2\":1,\n                  \"learning_rate\":0.003,\n                  'min_child_samples': 35,\n                  \"bagging_fraction\":0.8,\n                  \"bagging_freq\":1,\n                 }\n        \n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                            dtrain,\n                            valid_sets=[dtrain, dvalid],\n                            verbose_eval=100,\n                            num_boost_round=100000,\n                            early_stopping_rounds=200,\n                           init_model = model\n                        )\n        \n        \n\n        y_pred_list.append(model.predict(X_val[cont_features]))\n        print(np.sqrt(mean_squared_error(y_val, np.mean(y_pred_list,axis=0))))\n        \n        test_preds_3.append(model.predict(test[cont_features]))\n        \n    oof_3[test_index] = np.mean(y_pred_list,axis=0)    \n    score = np.sqrt(mean_squared_error(y_val, oof_3[test_index]))\n    score_list.append(score)\n    print(f\"RMSE Fold-{fold} : {score}\")\n    \n    fold+=1\n\nnp.mean(score_list)","e6f837d9":"print(np.mean(score_list))\nscore_list","ed1f2abc":"train[\"3_preds\"] = oof_3\ntest[\"3_preds\"] = np.mean(test_preds_3,axis=0)","c20b93f2":"train[[\"id\",\"1_preds\",\"2_preds\",\"3_preds\"]].to_csv(\"train_preds.csv\",index=False)\ntest[[\"id\",\"1_preds\",\"2_preds\",\"3_preds\"]].to_csv(\"test_preds.csv\",index=False)","b0c220f6":"train[\"4_preds\"] = pd.read_csv(\"..\/input\/nn-with-categorical-embedding\/nn_preds_train.csv\")[\"4_preds\"]\ntest[\"4_preds\"] = pd.read_csv(\"..\/input\/nn-with-categorical-embedding\/nn_preds_test.csv\")[\"4_preds\"]","62857001":"print(\"1st model score: \",np.sqrt(mean_squared_error(train[\"target\"], train[\"1_preds\"])))\nprint(\"2nd model score: \",np.sqrt(mean_squared_error(train[\"target\"], train[\"2_preds\"])))\nprint(\"3rd model score: \",np.sqrt(mean_squared_error(train[\"target\"], train[\"3_preds\"])))\nprint(\"4th model score: \",np.sqrt(mean_squared_error(train[\"target\"], train[\"4_preds\"])))\nprint(\"Blending model score: \",np.sqrt(mean_squared_error(train[\"target\"], train[\"4_preds\"]*0.1+train[\"3_preds\"]*0.3+train[\"2_preds\"]*0.3+train[\"1_preds\"]*0.3)))","34c8dfdf":"input_df = train[[\"1_preds\",\"2_preds\",\"3_preds\",\"4_preds\"]].copy()\nsub_input_df = test[[\"1_preds\",\"2_preds\",\"3_preds\",\"4_preds\"]].copy()","22b3d741":"y = train[\"target\"]\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\noof_stack = np.zeros(len(train))\n\nscore_list= []\nfold = 1\ntest_preds_stack = []\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge,Lasso\n\nfor train_index, test_index in kf.split(input_df):\n    X_train, X_val = input_df.iloc[train_index], input_df.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]    \n    \n    reg = LinearRegression(fit_intercept=True).fit(X_train, y_train)\n    y_stack = reg.predict(X_val)\n    \n    oof_stack[test_index] = y_stack*1\n    score = np.sqrt(mean_squared_error(y_val, oof_stack[test_index]))\n    score_list.append(score)\n    \n    test_preds_stack.append(reg.predict(sub_input_df.values).ravel())\n    \n\nnp.sqrt(mean_squared_error(y, oof_stack))   ","bd061696":"print(np.mean(score_list))\nscore_list","3c5fb1b3":"def final_features(df):\n    combinations_list = [(\"1_preds\",\"2_preds\"),\n                        (\"1_preds\",\"3_preds\"),\n                         (\"1_preds\",\"4_preds\"),\n                        (\"2_preds\",\"3_preds\"),\n                        (\"2_preds\",\"4_preds\"),\n                        (\"3_preds\",\"4_preds\")]\n    for i in range(len(combinations_list)):\n        col1 = combinations_list[i][0]\n        col2 = combinations_list[i][1]\n        \n        df.loc[:,f\"{col1}_{col2}_multiply\"] = df[col1] * df[col2]\n        df.loc[:,f\"{col1}_{col2}_divide\"] = df[col1] \/ df[col2]\n        \n    combinations_list = [(\"1_preds\",\"2_preds\",\"3_preds\"),\n                        (\"1_preds\",\"2_preds\",\"4_preds\"),\n                        (\"1_preds\",\"3_preds\",\"4_preds\"),\n                        (\"2_preds\",\"3_preds\",\"4_preds\")]\n    for i in range(len(combinations_list)):\n        col1 = combinations_list[i][0]\n        col2 = combinations_list[i][1]\n        col3 = combinations_list[i][2]\n        \n        df.loc[:,f\"{col1}_{col2}_{col3}_multiply\"] = df[col1] * df[col2]* df[col3]        \n    return df\n\ninput_df = train[[\"1_preds\",\"2_preds\",\"3_preds\",\"4_preds\"]].copy()\nsub_input_df = test[[\"1_preds\",\"2_preds\",\"3_preds\",\"4_preds\"]].copy()\ninput_df = final_features(input_df)\nsub_input_df = final_features(sub_input_df)\ninput_df.head()","8a89b610":"y = train[\"target\"]\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\noof_stack = np.zeros(len(train))\n\nscore_list= []\nfold = 1\ntest_preds_stack = []\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge,Lasso\n\nfor train_index, test_index in kf.split(input_df):\n    X_train, X_val = input_df.iloc[train_index], input_df.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n    \n    reg = LinearRegression(fit_intercept=True).fit(X_train, y_train)\n    y_stack = reg.predict(X_val)\n    \n    oof_stack[test_index] = y_stack*1\n    score = np.sqrt(mean_squared_error(y_val, oof_stack[test_index]))\n    score_list.append(score)\n    \n    test_preds_stack.append(reg.predict(sub_input_df.values).ravel())\n    \n\nnp.sqrt(mean_squared_error(y, oof_stack))   ","5b72a290":"print(np.mean(score_list))\nscore_list","a18ec245":"train[\"stacked_preds\"] = oof_stack\ntest[\"target\"] = np.mean(test_preds_stack,axis=0)","d9daa038":"train[[\"id\",\"stacked_preds\"]].to_csv(\"tarin_stacked_oof.csv\",index=False)\ntest[[\"id\",\"target\"]].to_csv(\"submission.csv\",index=False)","36d89218":"## 1st type of modeling\n\nMy plain best lgbm model. You can get 6952-6953 cv score with one seed modeling and 0.001 learning rate. But since it takes more time, I prefer to run a fast model with a higher learning rate for a couple of seeds and then average their predictions. Here I also used init_model parameter of lightgbm model to squeeze more from the data.[You can also check this notebook](https:\/\/www.kaggle.com\/fatihozturk\/lgbm-model-initialisation-trick). ","a29dc8bc":"## 3rd type of modeling: augmenting data with dae output.\n\nEven though I failed with dae (given the 1st place's amazing solution :)), it added diverstiy to my final blending in the end. First I run a simple MLP model with a linear output and then run it for a couple epochs. Later on, I used the MLP output for augmenting the original training data during my CV.\n","c148eecf":"**Important note:** To be able to have a safer evaluation and less leaking during stacking, it's always good practice to use same KFold scheme at every model, which I also did here.","5a360e3c":"## 2nd type of modeling\nSecond model is a 2-stage modeling. As a first thing I turn target column into a binary column and run 1st model to predict the probability of being class_1 for each row. In 2nd stage models, I do separete modeling again. I only use training data with class 1 and get real target predictions for validation and test set. Then doing the same by only using training data with class 0. At then end I calcualte final prediction as:\n\nfinal_prediction = probability_of_being_0 * (predictions_from_training_0) + probability_of_being_1 * (predictions_from_training_1)","063417a1":"By using mainly these 4 type of models and this stacking method at the end I reached to 0.694X CV score. If you want to get same score, you need to run samed models for seed_list = [None,2,3,4,5] and for the 2nd stage model, you need to generate more models with a threshold other than 8 to have more diverse models.","d1c59193":"With this method, you can just change follwoing \"**threshold**\" parameter and get more results to use for final stacking. **This is what I did to increase diversity.**","7456cb76":"### And now one more final step to squeeze more. Since linear regression model can only create linear relation between the features, I also wanted to create some non-linear interactions between input predictions. This method also gave some boost at 4t and 5th decimals, which was worth for this competition :)","3295f7ab":"I also have another modeling with full NN method. ","2324db25":"## In this notebook I want to share my main tree models that helped for adding diversity for my final stacking.\n\n1 - First model is my main best single model.\n\n2 - Second model is a 2-stage lgbm. As a first thing I turn target column into a binary column and run 1st model to predict the probability of being class 1 for each row. In 2nd stage models, I only use training data with class 1 and get real target predictions for validation and test set. Then doing the same by only using training data with class 0. At then end I calcualte final prediction as like this:\n\nfinal_prediction = probability_of_being_0 * (predictions_from_training_0) + probability_of_being_1 * (predictions_from_training_1)\n\n3 - As a third model, I tried to apply dae. Even though I failed with it (given the 1st place's solution :)), it added diverstiy to my final blending in the end. First I run a simple MLP model with a linear output and then run it for a couple epochs. Later on, I used the MLP output for augmenting the original training data during my CV.","7eb9da82":"Now I'll also read my NN predictions from another notebook and I'll apply stacking with all of my predictions.\n\nYou'll see that, stacking with LR is betten than dealing with manual weights."}}