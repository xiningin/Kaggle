{"cell_type":{"b316ded4":"code","32631801":"code","d9ddcc42":"code","1f699e37":"code","89da21bc":"code","d392b9c0":"code","ff77d386":"code","9d6d195c":"code","cd41e8cd":"code","5df4a8a6":"code","b6991650":"code","bb02a6f0":"code","c0fe91e4":"code","feb78708":"code","e3f6a21d":"code","f774580f":"code","6f178729":"code","211b4631":"code","8c1bab69":"code","284e108c":"code","691421ab":"code","94e245be":"code","dad2315c":"code","f60a268b":"code","9f96f734":"code","6d032c8e":"code","083d4228":"markdown","125c9798":"markdown","708586fd":"markdown","65bc12aa":"markdown","496291c2":"markdown","2a0fba88":"markdown","241ebaed":"markdown","9a3f4f10":"markdown","6a300ac8":"markdown"},"source":{"b316ded4":"import pandas as pd # For data processing \nimport numpy as np # For array operations\nimport matplotlib.pyplot as plt # For visualizing the data\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OrdinalEncoder\n\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","32631801":"dataset = pd.read_csv(\"..\/input\/tabular-playground-series-apr-2021\/train.csv\")\ntestset = pd.read_csv(\"..\/input\/tabular-playground-series-apr-2021\/test.csv\")","d9ddcc42":"dataset.head() # Viewing the top of the dataset","1f699e37":"dataset.isna() #Finding the missing data points","89da21bc":"dataset.Age = dataset.Age.fillna(np.mean(dataset.Age)) #Filling the missing datapoints","d392b9c0":"dataset.head() # Viewing the data after filling the NaN values","ff77d386":"trainlen = len(dataset)\nprint(\"Length of the training set is: \", trainlen) # Looking at the number of data points in the dataset","9d6d195c":"encoder = OrdinalEncoder() #Here we are using Ordinal Data encoding\ngender = encoder.fit_transform(np.array(dataset.Sex).reshape(-1,1)) #Transforming the categorical data","cd41e8cd":"gender[:3] # Viewing a sample of the encoded data","5df4a8a6":"feature1 = np.array(dataset[\"PassengerId\"]).reshape(-1,1)\nfeature2 = np.array(dataset[\"Pclass\"]).reshape(-1,1)\nfeature3 = np.array(dataset[\"Age\"]).reshape(-1,1)\nfeature4 = np.array(dataset[\"Parch\"]).reshape(-1,1)\nfeature5 = np.array(dataset[\"SibSp\"]).reshape(-1,1)\nfeature6 = gender\n\nfeaturevector = np.concatenate((feature1,feature2,feature3,feature4,feature5,feature6) , axis = 1)","b6991650":"print(\"The shape of the final feature vector is: \", featurevector.shape) # Viewing the shape of the featurevector","bb02a6f0":"featurevector[:3] # Viewing at the final featurevector","c0fe91e4":"target = np.array(dataset[\"Survived\"]).reshape(-1,1) # Forming the targetvector","feb78708":"x_train , x_test , y_train , y_test = train_test_split(featurevector , target , test_size = 0.10 , random_state = 6) #Splitting the data","e3f6a21d":"#Viewing the train data after splitting\nprint(\"The shape of the final target vector is: \", y_train.shape) \nprint(\"The shape of the final feature vector is: \", x_train.shape)","f774580f":"classifier = RandomForestClassifier(n_estimators = 600 , ccp_alpha = 0.002) # Instantiating the classifier with 600 estimators and ccp_alpha as 0.001\nclassifier.fit(x_train , y_train) #Training the classifier","6f178729":"plt.plot(y_train[:50])\nplt.plot(classifier.predict(x_train[:50]))\nplt.grid(\"on\")\nplt.title(\"Plot to look at the data match\")\nplt.xlabel(\"Samples --->\")\nplt.ylabel(\"Prediction --->\")","211b4631":"plot_confusion_matrix(classifier , x_test , y_test) # Evaluating the model","8c1bab69":"testset.Age = testset.Age.fillna(np.mean(testset.Age)) #Filling the NaN datapoints in the testset using the mean of the column","284e108c":"feature1 = np.array(testset[\"PassengerId\"]).reshape(-1,1)\nfeature2 = np.array(testset[\"Pclass\"]).reshape(-1,1)\nfeature3 = np.array(testset[\"Age\"]).reshape(-1,1)\nfeature4 = np.array(testset[\"Parch\"]).reshape(-1,1)\nfeature5 = np.array(testset[\"SibSp\"]).reshape(-1,1)\nfeature6 = encoder.fit_transform(np.array(testset.Sex).reshape(-1,1))\n\nfeaturevector = np.concatenate((feature1,feature2,feature3,feature4,feature5,feature6) , axis = 1) # Engineering the feature vector for testing the model","691421ab":"predictions = classifier.predict(featurevector) # Inference\nid_pass = testset[\"PassengerId\"]","94e245be":"print(\"Shape of predictions is: \", len(predictions))\nprint(\"Shape of passenger id is: \", len(id_pass))","dad2315c":"data = {'PassengerId': id_pass , 'Survived' : predictions} # Making a dictionary to store the data","f60a268b":"submission = pd.DataFrame(data = data) # Creating the dataframe to store the data","9f96f734":"submission.head() #Viewing the submission data","6d032c8e":"submission.to_csv(\"submission.csv\") #Saving the dataframe to a csv file","083d4228":"### Engineering the featurevector (Input for the model)","125c9798":"### Viewing the top of the dataset","708586fd":"# Synthetic Titanic Data Classification\n## Predictive Theory\n\nIn this theory, we will be looking at possible relationships for predicting the survival of passengers on the titanic. \n\n<b>Some known facts about the titanic are:<\/b> \n\n1) Most of the women and kids were rescued\n\n2) 1st class passengers were rescued (both men and women)\n\n3) Most of the titanic\u2019s engineers were rescued\n          \n        \nOur predictive theory tries to use age, gender, passenger class,  are their children onboard the ship or not, and sibling or spouse on board or not to predict if the passenger survived.\n\nLooking at the relationship between survival and gender we can say that as most of the women were evacuated, the chance of a survivor being a woman is much greater. \n\n Also, after looking at the survivor list on the real titanic we can see that most of the males who survived are from the first class. So, to model this we would need to know the passenger class. \n \nThe survival of women with kids is much greater than normal women and this could significantly impact their probability of survival.\n\n## Problems\n\n1) Missing data in almost all the columns\n\n2) Some columns where the mean of the data is not at an appropriate value to fill in.\n\n\n## Possible Solution\n\n1) Some of the columns can be filled with fair information as first-class tickets cost more.\n\n2) Using the other column information try to predict the missing data\n\n3) Look at the possibility of replacing by mean for the columns where it is possible. \n\n","65bc12aa":"### Reading the csv files","496291c2":"### Handling the missing datapoints","2a0fba88":"### Plan of Action\n\n1) Using the predictive theory try to model a RandomForestClassifier to predict the survival of the passenger.","241ebaed":"## Conclusion\nUsing a simple predictive theory we have structurally engineered a supervised learning based classifier and the performance of the final model is quite good. For improving the performance of the model we can try to adjust the ","9a3f4f10":"### Importing the required libraries","6a300ac8":"### Encoding the Categorical Variables"}}