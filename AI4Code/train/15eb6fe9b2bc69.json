{"cell_type":{"e601e2ac":"code","f5807f01":"code","4c77e315":"code","112eba07":"code","bb68ea0a":"code","6c0242fc":"code","1267ec16":"code","f5d2408a":"code","3ecad024":"code","3f6d39f4":"code","dbc0a8b7":"code","b41dcd5b":"code","fbe28aba":"code","634d758a":"code","0c17d7bf":"code","e2ebf58e":"code","204bba5a":"code","1905b7de":"code","dace7951":"code","b5547650":"code","ff693a79":"code","54b59bac":"code","a26382cc":"code","c9fdea7a":"code","f27d3c1e":"code","e0e24f10":"code","a373f9e2":"code","2c58f002":"code","525e049c":"code","ef15eb7f":"code","8be62c7f":"code","21af9117":"code","c21e579c":"code","893e51a4":"code","9e560ec4":"code","146cce2b":"code","97a658da":"markdown","3ec55550":"markdown","b4298d07":"markdown","3f78577b":"markdown","ec5c3411":"markdown","c961a85c":"markdown","24b2472e":"markdown","aaf39b16":"markdown","fc060414":"markdown","f2499c87":"markdown","3d102a9a":"markdown","30c24120":"markdown","9198c016":"markdown","fc4d8f0a":"markdown","1ae7824e":"markdown","3f3ddfb3":"markdown","a9b422f2":"markdown","78abfbca":"markdown","ba038817":"markdown","faf914c4":"markdown","497c2b03":"markdown","5eb9f7cf":"markdown","27db22c6":"markdown","5d131b16":"markdown","f3aa4042":"markdown","2c7ed35a":"markdown","7008ebb4":"markdown","8f91f19b":"markdown","78fc51ad":"markdown","0265399f":"markdown"},"source":{"e601e2ac":"#!pip install googletrans","f5807f01":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n\n## Data Visualisation\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.offline as py\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport plotly.offline as pyo\n\n## Data Preprocessing\nimport re\nimport nltk\nfrom gensim.models import word2vec\n\n## Visializing similarity of words\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n##Translation\n#from googletrans import Translator\n\n\n## Models\nimport xgboost as xgb\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import ensemble, metrics, model_selection, naive_bayes\nfrom sklearn.preprocessing import LabelEncoder","4c77e315":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","112eba07":"train_df = pd.read_csv(\"\/kaggle\/input\/contradictory-my-dear-watson\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/contradictory-my-dear-watson\/test.csv\")\nprint(\"Number of rows and columns in train data : \",train_df.shape)\nprint(\"Number of rows and columns in test data : \",test_df.shape)","bb68ea0a":"train_df.head()","6c0242fc":"test_df.head()","1267ec16":"Accuracy=pd.DataFrame()\nAccuracy['Type']=train_df.label.value_counts().index\nAccuracy['Count']=train_df.label.value_counts().values\nAccuracy['Type']=Accuracy['Type'].replace(0,'Entailment')\nAccuracy['Type']=Accuracy['Type'].replace(1,'Neutral')\nAccuracy['Type']=Accuracy['Type'].replace(2,'Contradiction')\nAccuracy","f5d2408a":"py.init_notebook_mode(connected=True)\nfig = go.Figure(data=[go.Pie(labels=Accuracy['Type'], values=Accuracy['Count'],hole=0.2)])\nfig.update_layout( title={\n                    'text': \"Percentage distribution of the 3 classes\",\n                    'y':0.9,\n                    'x':0.5,\n                    'xanchor': 'center',\n                    'yanchor': 'top'})\nfig.show()\n\nfig = px.bar(Accuracy, x='Type', y='Count',\n             hover_data=['Count'], color='Count',\n             labels={'pop':'Total Number of game titles'}, height=400)\n\nfig.update_layout( title={\n                    'text': \"Count of each of the target classes\",\n                    'y':0.9,\n                    'x':0.5,\n                    'xanchor': 'center',\n                    'yanchor': 'top'})\nfig.show()","3ecad024":"Languages=pd.DataFrame()\nLanguages['Type']=train_df.language.value_counts().index\nLanguages['Count']=train_df.language.value_counts().values","3f6d39f4":"py.init_notebook_mode(connected=True)\nfig = go.Figure(data=[go.Pie(labels=Languages['Type'], values=Languages['Count'],hole=0.2)])\nfig.update_layout( title={\n                    'text': \"Percentage distribution of different Languages\",\n                    'y':0.9,\n                    'x':0.5,\n                    'xanchor': 'center',\n                    'yanchor': 'top'})\nfig.show()","dbc0a8b7":"Languages_test=pd.DataFrame()\nLanguages_test['Type']=test_df.language.value_counts().index\nLanguages_test['Count']=test_df.language.value_counts().values\na = sum(Languages_test.Count)\nLanguages_test.Count = Languages_test.Count.div(a).mul(100).round(2)","b41dcd5b":"a = sum(Languages.Count)\nLanguages.Count = Languages.Count.div(a).mul(100).round(2)","fbe28aba":"fig = go.Figure(data=[\n    go.Bar(name='Train', x=Languages.Type, y=Languages.Count),\n    go.Bar(name='Test', x=Languages_test.Type, y=Languages_test.Count)\n])\n# Change the bar mode\nfig.update_layout(barmode='group',title={\n                    'text': \"Distribution across Train and Test\",\n                    'y':0.9,\n                    'x':0.5,\n                    'xanchor': 'center',\n                    'yanchor': 'top'})\nfig.show()","634d758a":"import string","0c17d7bf":"Meta_features = pd.DataFrame()\n\n## Number of words in the text ##\nMeta_features[\"premise_num_words\"] = train_df[\"premise\"].apply(lambda x: len(str(x).split()))\nMeta_features[\"hypothesis_num_words\"] = train_df[\"hypothesis\"].apply(lambda x: len(str(x).split()))\n\n## Number of characters in the text ##\nMeta_features[\"premise_num_chars\"] = train_df[\"premise\"].apply(lambda x: len(str(x)))\nMeta_features[\"hypothesis_num_chars\"] = train_df[\"hypothesis\"].apply(lambda x: len(str(x)))\n\n## Number of punctuations in the text ##\nMeta_features[\"premise_num_punctuations\"] =train_df[\"premise\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\nMeta_features[\"hypothesis_num_punctuations\"] =train_df[\"hypothesis\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Average length of the words in the text ##\nMeta_features[\"premise_mean_word_len\"] = train_df[\"premise\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\nMeta_features[\"hypothesis_mean_word_len\"] = train_df[\"hypothesis\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\nMeta_features['label'] = train_df['label']","e2ebf58e":"fig = go.Figure()\n\ncategories = [0,1,2]\nName = ['Entailment','Contradiction','Neutral']\nfor category in categories:\n    fig.add_trace(go.Violin(x=Meta_features['label'][Meta_features['label'] == category],\n                            y=Meta_features['premise_num_words'][Meta_features['label'] == category],\n                            name=Name[category],\n                            box_visible=True,\n                            meanline_visible=True))\n    \n\nfig.update_layout( title={\n                    'text': \"Number of Premise words per category\",\n                    'y':0.9,\n                    'x':0.5,\n                    'xanchor': 'center',\n                    'yanchor': 'top'})\n\nfig.show()","204bba5a":"fig = go.Figure()\nfor category in categories:\n    fig.add_trace(go.Violin(x=Meta_features['label'][Meta_features['label'] == category],\n                            y=Meta_features['premise_num_punctuations'][Meta_features['label'] == category],\n                            name=Name[category],\n                            box_visible=True,\n                            meanline_visible=True))\n    \n\nfig.update_layout( title={\n                    'text': \"Number of Punctuations in Premise per category\",\n                    'y':0.9,\n                    'x':0.5,\n                    'xanchor': 'center',\n                    'yanchor': 'top'})\n\nfig.show()","1905b7de":"fig = go.Figure()\nfig.add_trace(go.Histogram(x=Meta_features['premise_num_words']))\nfig.add_trace(go.Histogram(x=Meta_features['hypothesis_num_words']))\n\n# Overlay both histograms\nfig.update_layout(barmode='overlay',title={\n                    'text': \"Distribution of Words over Premise VS Hypothesis\",\n                    'y':0.9,\n                    'x':0.5,\n                    'xanchor': 'center',\n                    'yanchor': 'top'})\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","dace7951":"fig = go.Figure()\nfig.add_trace(go.Histogram(x=Meta_features['premise_num_punctuations']))\nfig.add_trace(go.Histogram(x=Meta_features['hypothesis_num_punctuations']))\n\n# Overlay both histograms\nfig.update_layout(barmode='overlay',title={\n                    'text': \"Distribution of Punctuations over Premise VS Hypothesis\",\n                    'y':0.9,\n                    'x':0.5,\n                    'xanchor': 'center',\n                    'yanchor': 'top'})\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","b5547650":"fig = go.Figure()\nfig.add_trace(go.Histogram(x=Meta_features['premise_num_chars']))\nfig.add_trace(go.Histogram(x=Meta_features['hypothesis_num_chars']))\n\n# Overlay both histograms\nfig.update_layout(barmode='overlay',title={\n                    'text': \"Distribution of Characters over Premise VS Hypothesis\",\n                    'y':0.9,\n                    'x':0.5,\n                    'xanchor': 'center',\n                    'yanchor': 'top'})\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","ff693a79":"#def Translation(x):\n#    translator = Translator()\n#    return translator.translate(x).text","54b59bac":"#test_df.premise[test_df.lang_abv!= 'en']=test_df.premise[test_df.lang_abv!= 'en'].apply(lambda x: Translation(x))\n#print(\"here\")\n#test_df.hypothesis[test_df.lang_abv!= 'en']=test_df.hypothesis[test_df.lang_abv!= 'en'].apply(lambda x: Translation(x))","a26382cc":"#train_df.premise[train_df.lang_abv!= 'en']=train_df.premise[train_df.lang_abv!= 'en'].apply(lambda x: Translation(x))\n#print(\"here\")\n#train_df.hypothesis[train_df.lang_abv!= 'en']=train_df.hypothesis[train_df.lang_abv!= 'en'].apply(lambda x: Translation(x))","c9fdea7a":"train_df = pd.read_csv(\"..\/input\/contradictory-my-watson-translated\/train_translated.csv\")\ntest_df = pd.read_csv(\"..\/input\/contradictory-my-watson-translated\/test_translated.csv\")","f27d3c1e":"train_df.head()","e0e24f10":"temp = pd.DataFrame()\ntemp['premise'] = train_df['premise']\ntemp['hypothesis'] = train_df['hypothesis']","a373f9e2":"STOP_WORDS = nltk.corpus.stopwords.words()\n\ndef clean_sentence(val):\n    regex = re.compile('([^\\s\\w]|_)+')\n    sentence = regex.sub('', val).lower()\n    sentence = sentence.split(\" \")\n    \n    for word in list(sentence):\n        if word in STOP_WORDS:\n            sentence.remove(word)  \n            \n    sentence = \" \".join(sentence)\n    return sentence\n\ntemp['premise'] =  temp['premise'].apply(clean_sentence)\ntemp['hypothesis'] =  temp['hypothesis'].apply(clean_sentence)","2c58f002":"def build_corpus(data):\n    corpus = []\n    for col in ['premise', 'hypothesis']:\n        for sentence in data[col].iteritems():\n            word_list = sentence[1].split(\" \")\n            corpus.append(word_list)\n            \n    return corpus\n\ncorpus = build_corpus(temp)        ","525e049c":"def tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","ef15eb7f":"# A more selective model\nmodel = word2vec.Word2Vec(corpus, size=100, window=20, min_count=150, workers=4)\ntsne_plot(model)","8be62c7f":"## Number of words in the text ##\ntrain_df[\"premise_num_words\"] = train_df[\"premise\"].apply(lambda x: len(str(x).split()))\ntrain_df[\"hypothesis_num_words\"] = train_df[\"hypothesis\"].apply(lambda x: len(str(x).split()))\ntest_df[\"premise_num_words\"] = test_df[\"premise\"].apply(lambda x: len(str(x).split()))\ntest_df[\"hypothesis_num_words\"] = test_df[\"hypothesis\"].apply(lambda x: len(str(x).split()))\n\n## Number of characters in the text ##\ntrain_df[\"premise_num_chars\"] = train_df[\"premise\"].apply(lambda x: len(str(x)))\ntrain_df[\"hypothesis_num_chars\"] = train_df[\"hypothesis\"].apply(lambda x: len(str(x)))\ntest_df[\"premise_num_chars\"] = test_df[\"premise\"].apply(lambda x: len(str(x)))\ntest_df[\"hypothesis_num_chars\"] = test_df[\"hypothesis\"].apply(lambda x: len(str(x)))\n\n## Number of punctuations in the text ##\ntrain_df[\"premise_num_punctuations\"] =train_df[\"premise\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntrain_df[\"hypothesis_num_punctuations\"] =train_df[\"hypothesis\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest_df[\"premise_num_punctuations\"] = test_df[\"premise\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest_df[\"hypothesis_num_punctuations\"] = test_df[\"hypothesis\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Average length of the words in the text ##\ntrain_df[\"premise_mean_word_len\"] = train_df[\"premise\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntrain_df[\"hypothesis_mean_word_len\"] = train_df[\"hypothesis\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df[\"premise_mean_word_len\"] = test_df[\"premise\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df[\"hypothesis_mean_word_len\"] = test_df[\"hypothesis\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n## Language Transformation\nlb_make = LabelEncoder()\ntrain_df[\"language\"] = lb_make.fit_transform(train_df[\"language\"])\ntest_df[\"language\"] = lb_make.fit_transform(test_df[\"language\"])\n                                         \n## lang_abv Transformation\nlb_make = LabelEncoder()\ntrain_df[\"lang_abv\"] = lb_make.fit_transform(train_df[\"lang_abv\"])\ntest_df[\"lang_abv\"] = lb_make.fit_transform(test_df[\"lang_abv\"])","21af9117":"from nltk.corpus import stopwords\nimport re\nimport nltk\nimport string\n\nstop_words = set(stopwords.words('english')) \ndef text_cleaner(text):\n    newString = text.lower()\n    newString = re.sub(r'\\([^)]*\\)', '', newString)\n    newString = re.sub('\"','', newString)    \n    newString = re.sub(r\"'s\\b\",\"\",newString)\n    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n    tokens = [w for w in newString.split() if not w in stop_words]\n    long_words=[]\n    for i in tokens:\n        if len(i)>=3:                  #removing short word\n            long_words.append(i)   \n    return (\" \".join(long_words)).strip()\n\ncleaned_text = []\nfor t in train_df['premise']:\n    cleaned_text.append(text_cleaner(t))\ntrain_df['premise'] = cleaned_text   \n\ncleaned_text = []\nfor t in test_df['premise']:\n    cleaned_text.append(text_cleaner(t))\ntest_df['premise'] = cleaned_text \n\ncleaned_text = []\nfor t in train_df['hypothesis']:\n    cleaned_text.append(text_cleaner(t))\ntrain_df['hypothesis'] = cleaned_text   \n\ncleaned_text = []\nfor t in test_df['hypothesis']:\n    cleaned_text.append(text_cleaner(t))\ntest_df['hypothesis'] = cleaned_text ","c21e579c":"## premise\ntfidf_vec = TfidfVectorizer(analyzer='word',max_features=1000)\ntfidf_vec.fit(train_df['premise'].values.tolist() + test_df['premise'].values.tolist())\ntrain_premise = tfidf_vec.transform(train_df['premise'].tolist())\ndf1 = pd.DataFrame(train_premise.toarray(), columns=tfidf_vec.get_feature_names()).add_suffix('_premise')\ntrain_df = pd.concat([train_df, df1], axis = 1)\n\ntest_premise = tfidf_vec.transform(test_df['premise'].tolist())\ndf1 = pd.DataFrame(test_premise.toarray(), columns=tfidf_vec.get_feature_names()).add_suffix('_premise')\ntest_df = pd.concat([test_df, df1], axis = 1)\n\n## premise\ntfidf_vec = TfidfVectorizer(analyzer='word',max_features=1000)\ntfidf_vec.fit(train_df['hypothesis'].values.tolist() + test_df['hypothesis'].values.tolist())\ntrain_premise = tfidf_vec.transform(train_df['hypothesis'].tolist())\ndf1 = pd.DataFrame(train_premise.toarray(), columns=tfidf_vec.get_feature_names()).add_suffix('_hypothesis')\ntrain_df = pd.concat([train_df, df1], axis = 1)\n\ntest_premise = tfidf_vec.transform(test_df['hypothesis'].tolist())\ndf1 = pd.DataFrame(test_premise.toarray(), columns=tfidf_vec.get_feature_names()).add_suffix('_hypothesis')\ntest_df = pd.concat([test_df, df1], axis = 1)","893e51a4":"def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n    param = {}\n    param['objective'] = 'multi:softprob'\n    param['eta'] = 0.1\n    param['max_depth'] = 3\n    param['silent'] = 1\n    param['num_class'] = 3\n    param['eval_metric'] = \"mlogloss\"\n    param['min_child_weight'] = child\n    param['subsample'] = 0.8\n    param['colsample_bytree'] = colsample\n    param['seed'] = seed_val\n    num_rounds = 2000\n\n    plst = list(param.items())\n    xgtrain = xgb.DMatrix(train_X, label=train_y)\n\n    if test_y is not None:\n        xgtest = xgb.DMatrix(test_X, label=test_y)\n        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n    else:\n        xgtest = xgb.DMatrix(test_X)\n        model = xgb.train(plst, xgtrain, num_rounds)\n\n    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n    if test_X2 is not None:\n        xgtest2 = xgb.DMatrix(test_X2)\n        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n    return pred_test_y, pred_test_y2, model","9e560ec4":"train_X = train_df.drop(list(train_df.columns[[0,1]])+['label']+['premise','hypothesis'], axis=1)\ntest_X = test_df.drop(list(test_df.columns[[0,1]])+['premise','hypothesis'], axis=1)\ntrain_y = train_df['label']\n\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\n\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val=0, colsample=0.7)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    break\nprint(\"cv scores : \", cv_scores)\n\nout_df = pd.DataFrame(pred_full_test)","146cce2b":"submission = pd.DataFrame()\nsubmission['id'] = test_df['id']\nsubmission['prediction'] = out_df.idxmax(axis=1)\n\nsubmission.to_csv(\"submission.csv\", index=False)","97a658da":"#### Simple Data Exploration ","3ec55550":"**Observation**\n\nFrom the above graph, we can see that **English** is dominating language in the given dataset.","b4298d07":"**About this kernel**\n\nThis kernel acts as a starter kit. It gives all the essential Key insights on the given text data.\n\n**Key Takeaways**\n\n* Extensive EDA\n* Effective Story Telling\n* Creative Feature Engineering\n* Modelling","3f78577b":"Being a ardent fan of the boosting algorithms, even though this competition mainly focuses on Transferm models, I apply both Transformer models and boosting algorithms in this case, for understading purpose","ec5c3411":"While dealing the text data, feature engineering can be done in two parts. They are\n\n1. Meta features - features that are extracted from the text like number of words, number of stop words, number of punctuations etc\n2. Text based features - features directly based on the text \/ words like frequency, svd, word2vec etc.\n\n**Meta Features:**\n\nWe will start with creating meta featues and see how good are they at predicting the spooky authors. The feature list is as follows:\n\n1. Number of words in the text\n2. Number of unique words in the text\n3. Number of characters in the text\n4. Number of stopwords\n5. Number of punctuations\n6. Number of upper case words\n7. Number of title case words\n8. Average length of the words\n\n**We'll try to analyse the Meta features between Premesis and Hypothesis. If possible we'll try to include them in models, in later part of this notebook**","c961a85c":"Since the Boosting algorithm didn't perform as expected, let's move on to transformers","24b2472e":"## Importing the Necessary Packages","aaf39b16":"#### Target Variable Exploration","fc060414":"#### Reference\n\n1) [Spooky Author](https:\/\/github.com\/SudalaiRajkumar\/Kaggle\/blob\/master\/SpookyAuthor\/simple_fe_notebook_spooky_author.ipynb)\n\n2) [Visualisation using T-sne](https:\/\/www.kaggle.com\/jeffd23\/visualizing-word-vectors-with-t-sne)\n\nThis notebook is highly inspired from the above sources.","f2499c87":"## Tranformer Models\n\nSince Transformer model is new to me, I'll try implementing Bert Tranformer through TPU's making it as a complete tutorial in the comin week.","3d102a9a":"## About the Comeptition","30c24120":"**Observation**\n\nThe distribution of words across the classes are almost the same for contradiction and Neutral, whereas, it is little bit less in Entailment ","9198c016":"## Modelling","fc4d8f0a":"### I will be working on this kernel, extensively on coming weeks.","1ae7824e":"## Exploratory Data Analysis","3f3ddfb3":"In this data as we obsevered earlier there are total of 15 languages, so I have decided to translate all the languages into English so that, we could perform a generalised analysis.","a9b422f2":"**Creative Feature Engineering**\n\nAs I mentioned, earlier in this notebook we will try to use the meta features derived above. To check whether they have any impact on the predicition. ","78abfbca":"### Please stay tuned for more updates, any suggestions please leave it in the comments.","ba038817":"### Please upvote the kernel, if you find it useful.","faf914c4":"Similarity of words after being converted into English","497c2b03":"**Observation**\n\nThe distribution of punctuations across the classes are almost the same for contradiction and entailment, whereas, it is little bit less in Neutral","5eb9f7cf":"#### t-SNE on word vectors\n\n* t-Distributed Stochastic Neighbor Embedding is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data.  \n* It works by taking a group of high-dimensional vocabulary word feature vectors, then compresses them down to 2-dimensional x,y coordinate pairs. \n* The idea is to keep similar words close together on the plane, while maximizing the distance between dissimilar words.","27db22c6":"**Observation:**\n\nThere are total of **12120** records, which contain\n\n1. **4176** records of Entailment\n2. **4064** records of Contradiction\n3. **3880** records of Neutral.\n\nTherefore, there is **No Class Imblanace** in the given data.","5d131b16":"**Observations:**\n\nFrom the following observations were made,\n\n1) The distrbution of the parameters of hypothesis fall within the range of permise.\n\n2) The peak of the curve represents, the most probable event in the dataset. Which is very high for Hypothesis.","f3aa4042":"For a any given two sentences, there are three ways they could be related: one could entail the other, one could contradict the other, or they could be unrelated. **Natural Language Inferencing** (NLI) is a popular NLP problem that involves determining how pairs of sentences (consisting of a premise and a hypothesis) are related.\n\nOur task is to create an NLI model that assigns labels of 0, 1, or 2 (corresponding to entailment, neutral, and contradiction) to pairs of premises and hypotheses. To make things more interesting, the train and test set include text in fifteen different languages!","2c7ed35a":"**Observation**\n\nFrom the graph, we can clearly see that the distribution of languages across train and test data are equal.","7008ebb4":"#### XGBoost on Natural Language Inference","8f91f19b":"### Visualizing Word Vectors in Hypothesis and Premise","78fc51ad":"#### Languages in Train and Test data","0265399f":"### Premise vs Hypothesis"}}