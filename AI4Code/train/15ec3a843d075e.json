{"cell_type":{"76dcc083":"code","99c5f83f":"code","e46828b4":"code","cada5244":"code","2009ec7f":"code","c34c418a":"code","29b7b177":"code","2e5edcc8":"code","e0de3184":"code","826cf0ef":"code","eda73896":"code","1a4b6235":"code","f8a911ee":"code","fbec7541":"code","bee9ba62":"code","a87487ac":"code","c18ccc65":"code","69a96e5c":"code","e1648a9f":"code","9eeef8c7":"markdown","04967822":"markdown","a0c9fc63":"markdown","9f9c4fbd":"markdown","e22e55f9":"markdown","8b2e8d18":"markdown","b2a3a811":"markdown","93465d3d":"markdown","ba639177":"markdown","ccf7efa6":"markdown","273f8ffe":"markdown","6162e879":"markdown","434e9fc7":"markdown","dbf9cf44":"markdown","f73566d5":"markdown","0d9158c9":"markdown","e24ab73b":"markdown","6d475697":"markdown","d272d4c3":"markdown","3f91630f":"markdown","a1eb914b":"markdown","d67a8b0d":"markdown"},"source":{"76dcc083":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","99c5f83f":"train = pd.read_csv('\/kaggle\/input\/text-classification-int20h\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/text-classification-int20h\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/text-classification-int20h\/submission.csv')","e46828b4":"print('train:', train.shape, 'test:', test.shape)\ntrain.head()","cada5244":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(12,6))\nsns.distplot(train[train.sentiment==1].review.str.len(), ax=ax1)\nax1.set_title('Text with Good Reviews')\nsns.distplot(train[train.sentiment==0].review.str.len(), ax=ax2)\nax2.set_title('Text with Bad Reviews')\nplt.show()","2009ec7f":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(12,6))\nsns.distplot(train[train.sentiment==1].review.str.split().map(lambda x: len(x)), ax=ax1)\nax1.set_title('Text with Good Reviews')\nsns.distplot(train[train.sentiment==0].review.str.split().map(lambda x: len(x)), ax=ax2)\nax2.set_title('Text with Bad Reviews')\nplt.show()","c34c418a":"plt.figure(figsize = (20,20))\nwc_positive = WordCloud(max_words = 1000).generate(\" \".join(train[train.sentiment == 1].review))\nplt.imshow(wc_positive , interpolation = 'bilinear')","29b7b177":"plt.figure(figsize = (20,20))\nwc_neg = WordCloud(max_words = 1000).generate(\" \".join(train[train.sentiment == 0].review))\nplt.imshow(wc_neg , interpolation = 'bilinear')","2e5edcc8":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","e0de3184":"AUTO = tf.data.experimental.AUTOTUNE\n\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\n\nEPOCHS = 1\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nMAX_LEN = 384","826cf0ef":"import transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom keras import backend as K\nfrom sklearn.metrics import f1_score\n\nseed=47","eda73896":"MODEL = 'albert-xxlarge-v2'","1a4b6235":"def encoding(texts, tokenizer, chunk_size=256, maxlen=512):\n\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_mask=True, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen,\n        truncation=True\n    )\n    \n    return {\n        \"input_ids\": np.array(enc_di['input_ids']),\n        \"attention_mask\": np.array(enc_di['attention_mask'])\n    }","f8a911ee":"tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL)\ntokenizer.save_pretrained('.')","fbec7541":"test_enc = encoding(test['review'].astype(str), tokenizer, maxlen=MAX_LEN)\n    \ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_enc)\n    .batch(BATCH_SIZE)\n    )","bee9ba62":"def f1(y_true, y_pred):\n    \n    def recall(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n    \n    def precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    \n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","a87487ac":"def build_model(transformer, max_len=512):\n\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n    sequence_output = transformer({\"input_ids\": input_word_ids, \"attention_mask\": attention_mask})[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    model = Model(inputs={\n        \"input_ids\": input_word_ids,\n        \"attention_mask\": attention_mask}, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=[f1])\n    \n    return model","c18ccc65":"with strategy.scope():\n    transformer_layer = (\n        transformers.TFAutoModel\n        .from_pretrained(MODEL)\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","69a96e5c":"kfold = StratifiedKFold(n_splits=4, random_state=seed, shuffle=True)\n\nres=[]\nlocal_probs=pd.DataFrame()\n\nX = train['review']\ny = train['sentiment']\n\nfor i, (tdx, vdx) in enumerate(kfold.split(X, y)):\n    X_train, X_valid, y_train, y_valid  = X.iloc[tdx], X.iloc[vdx], y.iloc[tdx], y.iloc[vdx]\n    \n    print(f\"Fold {i}\")\n\n    X_train_enc = encoding(X_train.astype(str), tokenizer, maxlen=MAX_LEN)\n    X_test_enc = encoding(X_valid.astype(str), tokenizer, maxlen=MAX_LEN)\n    \n    X_train_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((X_train_enc, y_train))\n        .repeat()\n        .shuffle(2048)\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n    )\n    \n    X_test_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_test_enc, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n    )\n    \n    n_steps = X_train.shape[0] \/\/ BATCH_SIZE\n\n    train_history = model.fit(\n        X_train_dataset,\n        steps_per_epoch=n_steps,\n        verbose=True,\n        validation_data=X_test_dataset,\n        epochs = EPOCHS \n        )\n    \n    preds_oof = model.predict(test_dataset, verbose=1)\n    \n    local_probs['fold_%i'%i] = preds_oof.flatten()","e1648a9f":"submission['sentiment'] = np.round(np.mean(local_probs, axis=1)).astype(int)\nsubmission.to_csv('sub_oof.csv', index=False)","9eeef8c7":"The **encoding** function encodes the text before feeding it to the model. Typically, it is used for getting tokens, token types, and attention masks. It outputs a dictionary of encoded text. ","04967822":"# Simple EDA","a0c9fc63":"# Bonus\n\nThis is our **best solo model** which gives us great result. **To improve the result, we use ensemble.**\n\n> Ensemble Learning performs a strategic combination of various experts or ML models in order to improve the effectiveness obtained using a single model.\n\n**How to estimate the number of votes that is considered to be the majority?** For this we used our validation. We used the ensemble method on each fold at different values for the majority of votes and chose the optimal one.\n\nIn total, we got 15 submissions, the optimal value of the majority turned out to be 11. This helped us improve the final result!","9f9c4fbd":"# Modeling","e22e55f9":"# Create a F1-metric","8b2e8d18":"Note that word and character distributions are quite similar, with a long right tail. But this means that there are very long reviews, so pay attention, because most models process strings of a small length.","b2a3a811":"**What is an ALBERT?**\n\n[ALBERT](https:\/\/huggingface.co\/albert-xxlarge-v2) is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.\n\n\nALBERT is particular in that it shares its layers across its Transformer. Therefore, all layers have the same weights. Using repeating layers results in a small memory footprint, however, the computational cost remains similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same number of (repeating) layers.\n","93465d3d":"Also you can check our notebook from another NLP competition [Sibur AI](https:\/\/www.kaggle.com\/imgremlin\/sibur-ai-16th-place-solution) where we gained 16th place (top 4%)","ba639177":"# Create a submission","ccf7efa6":"# Build model","273f8ffe":"# Load test into memory","6162e879":"# TPU Configs\n\nAs using NLP-model requires power computing, using TPU is highly recommended to decrease time you spent on training models. You can check if TPU is available for you and if yes, set the distribution strategy using the following code:","434e9fc7":"We first tokenize all our input datasets - train&test. We will use the tokenizer for this purpose. You create a tokenizer instance using the following statement:","dbf9cf44":"**Wordcloud for positive text**","f73566d5":"**Number of characters in texts**","0d9158c9":"**Wordcloud for negative text**","e24ab73b":"As you can see, we used big MAX_LEN cause data contains long texts.\n\nP.S. Also you can try LongBERT, but it's another story...","6d475697":"# What didn't help us\n\n\n* Data cleaning\n* Fitting on all data (leads to overfit)\n* Custom F1 Loss Function\n* Other transformers models (RoBERTa, BERT, XLNet)","d272d4c3":"**Number of words in text**","3f91630f":"# Train and validation\n\nFor training and validation, we used simple **StratifiedKFold** for Cross-Validation and got a very good correlation with public score.\n\n**Out-of-Fold Predictions** was used as a prediction method. An out-of-fold prediction is a prediction by the model during the k-fold cross-validation procedure. That is, out-of-fold predictions are those predictions made on the holdout datasets during the resampling procedure. There will be one prediction for each example in the training dataset.","a1eb914b":"The build method receives an instance of a pre-trained model in its first parameter called a transformer.","d67a8b0d":"# Create tokenizer"}}