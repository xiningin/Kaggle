{"cell_type":{"2cf38908":"code","f0b5619d":"code","ea735564":"code","0dfbb934":"markdown"},"source":{"2cf38908":"import pandas as pd\nimport numpy as np\nimport functools\nimport re\nfrom IPython.core.display import display, HTML\nimport string\n\n### BERT QA\nimport torch\n!pip install -q transformers --upgrade\nfrom transformers import *\nmodelqa = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","f0b5619d":"print ('Python packages loaded')\n# keep only documents with covid -cov-2 and cov2\ndef search_focus(df):\n    dfa = df[df['abstract'].str.contains('covid')]\n    dfb = df[df['abstract'].str.contains('-cov-2')]\n    dfc = df[df['abstract'].str.contains('cov2')]\n    dfd = df[df['abstract'].str.contains('ncov')]\n    frames=[dfa,dfb,dfc,dfd]\n    df = pd.concat(frames)\n    df=df.drop_duplicates(subset='title', keep=\"first\")\n    return df\n\n# load the meta data from the CSV file\n#usecols=['title','journal','abstract','authors','doi','publish_time','sha','full_text_file']\ndf=pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')\nprint ('ALL CORD19 articles',df.shape)\n#fill na fields\ndf=df.fillna('no data provided')\n#drop duplicate titles\ndf = df.drop_duplicates(subset='title', keep=\"first\")\n#keep only 2020 dated papers\ndf=df[df['publish_time'].str.contains('2020')]\n# convert abstracts to lowercase\ndf[\"abstract\"] = df[\"abstract\"].str.lower()+df[\"title\"].str.lower()\n#show 5 lines of the new dataframe\ndf=search_focus(df)\nprint ('Keep only COVID-19 related articles',df.shape)\n\nimport os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\nimport math\n\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\ndef format_tables(ref_entries):\n    extract=''\n    for x in ref_entries.values():\n        if 'html' in x:\n            start = '<html>'\n            end = '<\/html>'\n            x=str(x).lower()\n            dat=(x.split(start))[1].split(end)[0]\n            extract=extract+' '+dat\n    \n    return extract\n\ndf['tables']='N\/A'\nfor index, row in df.iterrows():\n    #print (row['pdf_json_files'])\n    if 'no data provided' not in row['pdf_json_files'] and os.path.exists('\/kaggle\/input\/CORD-19-research-challenge\/'+row['pdf_json_files'])==True:\n        with open('\/kaggle\/input\/CORD-19-research-challenge\/'+row['pdf_json_files']) as json_file:\n            #print ('in loop')\n            data = json.load(json_file)\n            body=format_body(data['body_text'])\n            #ref_entries=format_tables(data['ref_entries'])\n            #print (body)\n            body=body.replace(\"\\n\", \" \")\n            text=row['abstract']+' '+body.lower()\n            df.loc[index, 'abstract'] = text\n            #df.loc[index, 'tables'] = ref_entries\n\ndf=df.drop(['pdf_json_files'], axis=1)\ndf=df.drop(['sha'], axis=1)\ndf.head()","ea735564":"# extract result excerpt\ndef extract_result(text,word,focus):\n    extracted='N\/A'\n    if word in text and focus in text:\n        res = [i.start() for i in re.finditer(word, text)]\n        for result in res:\n            extracted=text[result:result+600]\n            if focus in extracted:\n                return extracted\n    return 'N\/A'\n\n# extract method excerpt\ndef extract_method(text,word,focus):\n    extracted='N\/A'\n    if word in text:\n        res = [i.start() for i in re.finditer(word, text)]\n        for result in res:\n            extracted=text[result:result+600]\n    return extracted\n\n# extract study design\ndef extract_design(text):\n    words=['retrospective','prospective cohort','retrospective cohort', 'systematic review',' meta ',' search ','case control','case series,','time series','cross-sectional','observational cohort', 'retrospective clinical','virological analysis','prevalence study','literature','two-center']\n    study_types=['retrospective','prospective cohort','retrospective cohort','systematic review','meta-analysis','literature search','case control','case series','time series analysis','cross sectional','observational cohort study', 'retrospective clinical studies','virological analysis','prevalence study','literature search','two-center']\n    extract=''\n    res=''\n    for word in words:\n        if word in text:\n            res = [i.start() for i in re.finditer(word, text)]\n        for result in res:\n            extracted=text[result-30:result+30]\n            extract=extract+' '+extracted\n    i=0\n    study=''\n    for word in words:\n        if word in extract:\n            study=study_types[i]\n        #print (extract)\n        i=i+1\n    return study\n\n# BERT pretrained question answering module\ndef answer_question(question,text,model,tokenizer):\n    input_text = \"[CLS] \" + question + \" [SEP] \" + text + \" [SEP]\"\n    input_ids = tokenizer.encode(input_text)\n    token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n    start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    #print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n    answer=(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n    # show qeustion and text\n    #tokenizer.decode(input_ids)\n    answer=answer.replace(' ##','')\n    #print (answer)\n    return answer\n\ndef process_text(df1,focus):\n    df_results = pd.DataFrame(columns=['Date', 'Study', 'Study Link','Journal', 'Study Type', 'Method', 'Result', 'Measure of Evidence', 'Added on', 'DOI', 'CORD_UID'])\n    for index, row in df1.iterrows():\n        method=extract_method(row['abstract'],'method',focus)\n        result=extract_result(row['abstract'],'conclusion',focus)\n        if result!='N\/A':\n            study_type=''\n            study_type=extract_design(row['abstract'])\n            ### get sample size\n            sample_q='how many patients cases studies were included collected or enrolled'\n            sample=row['abstract'][0:1000]\n            moe=answer_question(sample_q,sample,modelqa,tokenizer)\n            if '[SEP]' in moe or '[CLS]' in moe:\n                moe='-'\n            moe=moe.replace(' , ',',')\n            \n            link=row['doi']\n            linka='https:\/\/doi.org\/'+link\n            to_append = [row['publish_time'], row['title'], linka,row['journal'], study_type, method, result, moe, '-', row['doi'], row['cord_uid']]\n            df_length = len(df_results)\n            df_results.loc[df_length] = to_append\n    return df_results\n\nfocuses=['immune response','mutat','virus adaptations','phenotyp', 'genetic variation','transmission model', 'serial interval','assessment framework']\n\n\nfor focus in focuses:\n    df1 = df[df['abstract'].str.contains(focus)]\n    df1 = df[df['abstract'].str.contains('method')]\n    df1 = df[df['abstract'].str.contains('conclusion')]\n    df_results=process_text(df1,focus)\n    df_results=df_results.sort_values(by=['Date'], ascending=False)\n    df_table_show=HTML(df_results.to_html(escape=False,index=False))\n    display(HTML('<h1>'+focus+'<\/h1>'))\n    display(df_table_show)\n    file=focus+'.csv'\n    df_results.to_csv(file,index=False)","0dfbb934":"# Round 2 - summary tables that address models and open questions related to COVID-19\n\n![](https:\/\/sportslogohistory.com\/wp-content\/uploads\/2018\/09\/georgia_tech_yellow_jackets_1991-pres-1.png)\n\n**GOAL:** Create automated article summary tables that address model and open question studies related to COVID-19\n\n**SCENARIO:** When a new virus is discovered and causes a pandemic, it is important for scientists to get specific information coming from all scientific literature that may help them combat the pandemic.  In an effort to better understand the virus, scientists need to know what data exists for models and open questions of interest.\n\n**PROBLEM:** The challenege, however, is that the number of scientific papers created is large and the papers are published very rapidly, making it nearly impossible for scientists to digest and understand important data in this sea of data.  When the COVID-19 Challenge started in March 2020, there were about 50,000 documents in the competition corpus.  As of this writing (June 15, 2020), there are about 140,000 documents in the corpus, almost triple in a three month period. Just keeping up with the volume of papers released is a daunting task, not to mention the near impossible task of searching through them trying to isolate answer to open questions.\n\n**SOLUTION:** Create an unsupervised scientific literature understanding system that can analyze a very large corpus - made up of tens or hundreds of thousands of scientific papers and return specific text excerpts containing answers to open questions.\n"}}