{"cell_type":{"4c45adaf":"code","7013c743":"code","54f4981c":"code","726ba193":"code","40b21253":"code","233b152e":"code","c0d7902b":"code","569e1706":"code","7c682e69":"code","7dfbcd76":"code","7d1a68e7":"code","16db9047":"code","d6f1c73c":"code","ab6840ac":"code","dc7d4c84":"code","304d6f0a":"code","b648880e":"code","73ef66d9":"code","80ddb95d":"code","05365741":"code","f264cc59":"code","fab04420":"code","60380e4b":"code","5cb2a629":"code","7205a762":"code","34b3194b":"code","218704f9":"code","511186e6":"code","1c2a0bdf":"code","2096c6bb":"code","6ad74775":"code","189e03ec":"code","eef8d39e":"code","3d01ff5f":"code","4c2b72b6":"code","fee9d002":"code","df4d4d69":"code","4c0b7fa3":"code","2697ef4a":"code","acc783c8":"code","2ce77dba":"code","93e502c4":"code","358fb216":"code","7a88d6e3":"code","c9daf1a6":"code","026e6f20":"code","b7930390":"markdown","5bcfe2f0":"markdown","83fd9426":"markdown"},"source":{"4c45adaf":"#Importing the essential libraries\n#Beautiful Soup is a Python library for pulling data out of HTML and XML files\n#The Natural Language Toolkit\n\nimport requests\nimport nltk\nfrom bs4 import BeautifulSoup\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport random\nfrom wordcloud import WordCloud\nimport os\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nfrom textblob import TextBlob\nfrom pattern.en import sentiment\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","7013c743":"#we are using request package to make a GET request for the website, which means we're getting data from it.\n\nr=requests.get('https:\/\/www.newsy.com\/stories\/commercial-companies-advance-space-exploration\/')","54f4981c":"#Setting the correct text encoding of the HTML page\nr.encoding = 'utf-8'","726ba193":"#Extracting the HTML from the request object\nhtml = r.text","40b21253":"# Printing the first 500 characters in html\nprint(html[:500])","233b152e":"# Creating a BeautifulSoup object from the HTML\nsoup = BeautifulSoup(html)\n\n# Getting the text out of the soup\ntext = soup.get_text()","c0d7902b":"#total length\nlen(text)","569e1706":"#having a look at the text\n\nprint(text[100:1100])","7c682e69":"text","7dfbcd76":"clean_text= text.replace(\"\\n\", \" \")","7d1a68e7":"clean_text= clean_text.replace(\"\/\", \" \")       ","16db9047":"clean_text= ''.join([c for c in clean_text if c != \"\\'\"])","d6f1c73c":"clean_text","ab6840ac":"sentence=[]\n\n\ntokens = nlp(clean_text)\nfor sent in tokens.sents:\n    sentence.append((sent.text.strip()))","dc7d4c84":"sentence","304d6f0a":"print(len(sentence))","b648880e":"print(sentence[2])","73ef66d9":"textblob_sentiment=[]\n\nfor s in sentence:\n    txt= TextBlob(s)\n    a= txt.sentiment.polarity\n    b= txt.sentiment.subjectivity\n    \n    textblob_sentiment.append([s,a,b])\n    ","80ddb95d":"df_textblob = pd.DataFrame(textblob_sentiment, columns =['Sentence', 'Polarity', 'Subjectivity']) ","05365741":"df_textblob.head()","f264cc59":"df_textblob.info()","fab04420":"sns.displot(df_textblob[\"Polarity\"], height= 5, aspect=1.8)\nplt.xlabel(\"Sentence Polarity (Textblob)\")","60380e4b":"sns.displot(df_textblob[\"Subjectivity\"], height= 5, aspect=1.8)\nplt.xlabel(\"Sentence Subjectivity (Textblob)\")","5cb2a629":"pattern_sentiment=[]\n\nfor s in sentence:\n    res= sentiment(s)\n    c= res[0]\n    d= res[1]\n    \n    pattern_sentiment.append([s,c,d])\n    ","7205a762":"pattern_sentiment[1]","34b3194b":"df_pattern = pd.DataFrame(textblob_sentiment, columns =['Sentence', 'Polarity', 'Subjectivity']) ","218704f9":"df_pattern.head()","511186e6":"df_pattern.info()","1c2a0bdf":"sns.displot(df_pattern[\"Polarity\"], height= 5, aspect=1.8)\nplt.xlabel(\"Sentence Polarity (Pattern)\")","2096c6bb":"sns.displot(df_pattern[\"Subjectivity\"], height= 5, aspect=1.8)\nplt.xlabel(\"Sentence Subjectivity (Pattern)\")","6ad74775":"clean_text[100:300]","189e03ec":"#Creating the tokenizer\ntokenizer = nltk.tokenize.RegexpTokenizer('\\w+')","eef8d39e":"#Tokenizing the text\ntokens = tokenizer.tokenize(clean_text)","3d01ff5f":"len(tokens)","4c2b72b6":"print(tokens[0:10])","fee9d002":"#now we shall make everything lowercase for uniformity\n#to hold the new lower case words\n\nwords = []\n\n# Looping through the tokens and make them lower case\nfor word in tokens:\n    words.append(word.lower())","df4d4d69":"len(words)","4c0b7fa3":"print(words[0:10])","2697ef4a":"#Now we have to remove stopwords\n#Stop words are a set of commonly used words in any language. \n#For example, in English, \u201cthe\u201d, \u201cis\u201d and \u201cand\u201d, would easily qualify as stop words. \n#In NLP and text mining applications, stop words are used to eliminate unimportant words, \n#allowing applications to focus on the important words instead.\n\n#English stop words from nltk\n\nstopwords = nltk.corpus.stopwords.words('english')","acc783c8":"#Now we need to remove the stop words from the words variable\n\n#A new list to hold Moby Dick with No Stop words\nwords_new = []\n\n#Appending to words_new all words that are in words but not in sw\nfor word in words:\n    if word not in stopwords:\n        words_new.append(word)","2ce77dba":"len(words_new)","93e502c4":"print(words_new[0:10])","358fb216":"#The frequency distribution of the words\nfreq_dist = nltk.FreqDist(words_new)","7a88d6e3":"#Frequency Distribution Plot\nplt.subplots(figsize=(16,10))\nfreq_dist.plot(20)","c9daf1a6":"#converting into string\n\nres=' '.join([i for i in words_new if not i.isdigit()]) ","026e6f20":"plt.subplots(figsize=(16,10))\nwordcloud = WordCloud(\n                          background_color='black',\n                          max_words=100,\n                          width=1400,\n                          height=1200\n                         ).generate(res)\n\n\nplt.imshow(wordcloud)\nplt.title('NEWS ARTICLE (100 words)')\nplt.axis('off')\nplt.show()","b7930390":"# Word Frequency and Word Cloud","5bcfe2f0":"# Sentiment Analysis Textblob","83fd9426":"# Sentiment Analysis Pattern"}}