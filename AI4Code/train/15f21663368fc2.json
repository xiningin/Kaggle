{"cell_type":{"90d10f17":"code","bce0615c":"code","2beac8f5":"code","d99ae45f":"code","908e1d6e":"code","77c8ab7e":"code","ee7a9087":"code","a9f128c7":"code","8e2b66e8":"code","4a48dc9d":"code","d7222a0f":"code","c41c87ea":"code","085d4bd9":"code","803f4a8f":"code","b2e5cb58":"code","7c38af38":"code","dcee0a61":"code","a15bcb11":"code","12bb3eec":"code","97c73558":"code","2825f0ff":"code","0af003e9":"code","0ba2f58c":"code","3ac51dd4":"code","aca225bb":"code","f0ad6b99":"code","e639e60d":"code","b2e41a71":"code","3306e8bf":"code","e2eef323":"code","4d6b68d7":"code","c82554d6":"code","712fb544":"code","9a798d8a":"code","0b2785c7":"code","bcd93ccf":"code","f02c62b7":"code","d2979ac3":"code","52eb3614":"code","a919e26b":"code","0897f13d":"code","3dc7f831":"code","297ab7fb":"code","38a16abf":"code","158d3f12":"code","3d64a8cd":"code","61f896eb":"code","167031e2":"code","9d469207":"code","0a0e791c":"code","fe6b797d":"code","0987ae7b":"code","957b2026":"code","9b5c4caf":"code","e2caeaff":"code","9bd2953f":"code","9a52a671":"markdown","ace0c61c":"markdown","3efe1fb1":"markdown","c9af6088":"markdown","7cca95a0":"markdown","3fe715a8":"markdown","95c45626":"markdown","6e607b5c":"markdown","c47eff82":"markdown","d140496b":"markdown","6e7bf4e5":"markdown","29bfc1e5":"markdown","0b03380b":"markdown","2174d415":"markdown","1cf581e4":"markdown","5af5fb1c":"markdown","7a8b00bf":"markdown","e210ec53":"markdown","0474684b":"markdown","36b38ac9":"markdown","df46d7f9":"markdown","f96533f4":"markdown","192f218a":"markdown","dde9c9df":"markdown","a0df3680":"markdown","5ca6621e":"markdown","2b9d87af":"markdown","aced7433":"markdown","76cbbd5e":"markdown","1312d340":"markdown","881645f4":"markdown","7550bb91":"markdown","03d9c8bb":"markdown"},"source":{"90d10f17":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n%matplotlib inline\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bce0615c":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","2beac8f5":"train.head()","d99ae45f":"train['SalePrice'].describe()","908e1d6e":"print(train['SalePrice'].skew())\nprint(train['SalePrice'].kurt())","77c8ab7e":"sns.distplot(train['SalePrice'], fit=norm)\n#create new figure\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)","ee7a9087":"train['SalePrice'] = np.log(train['SalePrice'])\nsns.distplot(train['SalePrice'], fit=norm)\n\n#create new figure\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)","a9f128c7":"fig, ax = plt.subplots(figsize=(12,9))\nsns.heatmap(train.corr(),square=True);","8e2b66e8":"cols = train.corr().nlargest(10, 'SalePrice').index\nplt.subplots(figsize=(10,10))\nsns.set(font_scale=1.25)\nsns.heatmap(train[cols].corr(),square=True, annot=True);","4a48dc9d":"sns.pairplot(train[cols]);","d7222a0f":"var = 'GrLivArea'\nplt.scatter(x=train[var], y=train['SalePrice']);","c41c87ea":"train[train['GrLivArea'] > 4500].index","085d4bd9":"var = 'GarageArea'\nplt.scatter(x=train[var], y=train['SalePrice']);","803f4a8f":"train[train['GarageArea'] > 1220].index","b2e5cb58":"var = 'TotalBsmtSF'\nplt.scatter(x=train[var], y=train['SalePrice']);","7c38af38":"train[train['TotalBsmtSF'] > 5000].index","dcee0a61":"var = '1stFlrSF'\nplt.scatter(x=train[var], y=train['SalePrice']);","a15bcb11":"train[train['TotalBsmtSF'] > 4000].index","12bb3eec":"var = 'OverallQual'\nplt.subplots(figsize=(10,6))\nsns.boxplot(x=train[var], y=train['SalePrice']);","97c73558":"var = 'YearBuilt'\nplt.subplots(figsize=(20,10))\nsns.boxplot(x=train[var], y=train['SalePrice']);","2825f0ff":"train = train.drop([523, 581, 1061, 1190, 1298])","0af003e9":"train.reset_index(drop=True, inplace=True)","0ba2f58c":"y_train = train['SalePrice'].reset_index(drop=True)\ntrain = train.drop(['SalePrice'], axis=1)","3ac51dd4":"#Delete ID\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","aca225bb":"#concatenate train and test\nall_features = pd.concat((train,test)).reset_index(drop=True)\nall_features.shape","f0ad6b99":"# missing values\ntotal = all_features.isnull().sum().sort_values(ascending=False)\npercent = (all_features.isnull().sum()\/all_features.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total,percent], axis=1, keys=['Total','Percent'])\nmissing_data.head(40)","e639e60d":"all_features['MSSubClass'] = all_features['MSSubClass'].apply(str)\nall_features['YrSold'] = all_features['YrSold'].apply(str)\nall_features['MoSold'] = all_features['MoSold'].apply(str)","b2e41a71":"# Some features have only a few missing value. Fill up using most common value\ncommon_vars = ['Exterior1st','Exterior2nd','SaleType','Electrical','KitchenQual']\nfor var in common_vars:\n    all_features[var] = all_features[var].fillna(all_features[var].mode()[0])\n\nall_features['MSZoning'] = all_features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n# data description says NA means typical\nall_features['Functional'] = all_features['Functional'].fillna('Typ')","3306e8bf":"col_str = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','BsmtQual',\n            'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\"PoolQC\"\n           ,'Alley','Fence','MiscFeature','FireplaceQu','MasVnrType','Utilities']\nfor col in col_str:\n    all_features[col] = all_features[col].fillna('None')","e2eef323":"col_num = ['GarageYrBlt','GarageArea','GarageCars','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BsmtUnfSF','TotalBsmtSF']\nfor col in col_num:\n    all_features[col] = all_features[col].fillna(0)\n    \nall_features['LotFrontage'] = all_features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","4d6b68d7":"all_features.isnull().sum().sort_values(ascending=False).head(5)","c82554d6":"num_features = all_features.select_dtypes(exclude='object').columns","712fb544":"# Create box plots for all numeric features\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 12))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=all_features[num_features], orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","9a798d8a":"# Find skewed numerical features\nskewness = all_features[num_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skewness = skewness[abs(skewness) > 0.5]\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skewness.shape[0]))\nhigh_skewness.sort_values(ascending=False)","0b2785c7":"high_skewness.index","bcd93ccf":"from scipy.special import boxcox1p\nskewed_features = high_skewness.index\nfor feat in skewed_features:\n    all_features[feat] = boxcox1p(all_features[feat], boxcox_normmax(all_features[feat] + 1))","f02c62b7":"new_skewness = all_features[num_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nnew_high_skewness = new_skewness[abs(new_skewness) > 0.5]\nprint(\"There are {} skewed numerical features after Box Cox transform\".format(new_high_skewness.shape[0]))\nprint(\"Mean skewnees: {}\".format(np.mean(new_high_skewness)))\nnew_high_skewness.sort_values(ascending=False)","d2979ac3":"#  Adding total sqfootage feature \nall_features['TotalSF']=all_features['TotalBsmtSF'] + all_features['1stFlrSF'] + all_features['2ndFlrSF']\n#  Adding total bathrooms feature\nall_features['Total_Bathrooms'] = (all_features['FullBath'] + (0.5 * all_features['HalfBath']) +\n                               all_features['BsmtFullBath'] + (0.5 * all_features['BsmtHalfBath']))\n#  Adding total porch sqfootage feature\nall_features['Total_porch_sf'] = (all_features['OpenPorchSF'] + all_features['3SsnPorch'] +\n                              all_features['EnclosedPorch'] + all_features['ScreenPorch'] +\n                              all_features['WoodDeckSF'])\n\nall_features['haspool'] = all_features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasgarage'] = all_features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasbsmt'] = all_features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasfireplace'] = all_features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n\n# Not normaly distributed can not be normalised and has no central tendecy\nall_features = all_features.drop(['MasVnrArea', 'OpenPorchSF', 'WoodDeckSF', 'BsmtFinSF1','2ndFlrSF'], axis=1)","52eb3614":"all_features = pd.get_dummies(all_features).reset_index(drop=True)\nall_features.shape","a919e26b":"X = all_features.iloc[:len(y_train), :]\nX_test = all_features.iloc[len(y_train):, :]\nX.shape, y_train.shape, X_test.shape","0897f13d":"# Removes colums where the threshold of zero's is (> 99.95), means has only zero values \noverfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 99.95:\n        overfit.append(i)\n\noverfit = list(overfit)\noverfit.append('MSZoning_C (all)')\n\nX = X.drop(overfit, axis=1).copy()\nX_test = X_test.drop(overfit, axis=1).copy()\n\nprint(X.shape,y_train.shape,X_test.shape)","3dc7f831":"from datetime import datetime\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.linear_model import LinearRegression\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","297ab7fb":"# setup cross validation folds\nkfolds = KFold(n_splits=16, shuffle=True, random_state=42)\n\n# define error metrics\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y_train, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","38a16abf":"# LightGBM regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=4,\n                       learning_rate=0.01, \n                       n_estimators=9000,\n                       max_bin=200, \n                       bagging_fraction=0.75,\n                       bagging_freq=5, \n                       bagging_seed=7,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=7,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n# XGBoost Regressor\n\"\"\"xgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:linear',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)\"\"\"\n\n\"\"\"gbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  \n\"\"\"\n\n# setup models hyperparameters using a pipline\n# This is a range of values that the model considers each time in runs a CV\nridge_alpha = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nlasso_alpha = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\nelastic_alpha = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\n# Ridge Regression: robust to outliers using RobustScaler\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alpha, cv=kfolds))\n\n# Lasso Regression: \nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, \n                    alphas=lasso_alpha,random_state=42, cv=kfolds))\n\n# Elastic Net Regression:\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, \n                         alphas=elastic_alpha, cv=kfolds, l1_ratio=e_l1ratio))\n\n# Support Vector Regression\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Stack up all the models above\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, svr, lightgbm),\n                                meta_regressor=elasticnet,\n                                use_features_in_secondary=True)","158d3f12":"# Store scores of each model\nscores = {}\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lightgbm'] = (score.mean(), score.std())","3d64a8cd":"score = cv_rmse(ridge)\nprint(\"ridge: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['ridge'] = (score.mean(), score.std())","61f896eb":"score = cv_rmse(lasso)\nprint(\"lasso: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lasso'] = (score.mean(), score.std())","167031e2":"score = cv_rmse(elasticnet)\nprint(\"elasticnet: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['elasticnet'] = (score.mean(), score.std())","9d469207":"score = cv_rmse(svr)\nprint(\"svr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['svr'] = (score.mean(), score.std())","0a0e791c":"\"\"\"score = cv_rmse(gbr)\nprint(\"gbr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['gbr'] = (score.mean(), score.std())\"\"\"","fe6b797d":"print('----START Fit----',datetime.now())\nprint('Elasticnet')\nelastic_model = elasticnet.fit(X, y_train)\nprint('Lasso')\nlasso_model = lasso.fit(X, y_train)\nprint('Ridge')\nridge_model = ridge.fit(X, y_train)\nprint('lightgbm')\nlgb_model = lightgbm.fit(X, y_train)\nprint('svr')\nsvr_model = svr.fit(X, y_train)\n\nprint('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y_train))","0987ae7b":"def blend_predictions(X):\n    return ((0.12  * elastic_model.predict(X)) + \\\n            (0.12 * lasso_model.predict(X)) + \\\n            (0.12 * ridge_model.predict(X)) + \\\n            (0.22 * lgb_model.predict(X)) + \\\n            (0.1 * svr_model.predict(X)) + \\\n            (0.32 * stack_gen_model.predict(np.array(X))))","957b2026":"# Get final precitions from the blended model\nblended_score = rmsle(y_train, blend_predictions(X))\nscores['blended'] = (blended_score, 0)\nprint('RMSLE score on train data:')\nprint(blended_score)","9b5c4caf":"# Plot the predictions for each model\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Models', size=20)\n\nplt.show()","e2caeaff":"submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blend_predictions(X_test)))","9bd2953f":"submission.to_csv(\"submission.csv\", index=False)","9a52a671":"## \u0417\u0430\u043a\u043e\u0434\u0438\u0440\u0443\u0435\u043c \u043a\u0430\u0442-\u0444\u0438\u0447\u0438","ace0c61c":"# \u0424\u0438\u0447\u0438","3efe1fb1":"### Cross-validation and Stacking","c9af6088":"## Make the submission","7cca95a0":"### Drop ID and combine train\/test","3fe715a8":"Next step, we can use Box Cox transformation on skewed data.","95c45626":"### Heat map","6e607b5c":"Hello, dear kagglers. This notebook based on the various and awesome notebooks by our colleagues. I've just added some theoretical insights and added a translation to Russian as far as it's my native.\n\nhttps:\/\/lingtra.in\/images\/flag-ru-h.svg\n\n\u041f\u0440\u0438\u0432\u0435\u0442, \u0434\u043e\u0440\u043e\u0433\u0438\u0435 \u043a\u0435\u0433\u0433\u043b\u0435\u0440\u044b \u0438 \u043a\u0435\u0433\u0433\u043b\u0435\u0440\u0448\u0438. \u042d\u0442\u043e\u0442 \u043d\u043e\u0443\u0442\u0431\u0443\u043a \u043e\u0441\u043d\u043e\u0432\u0430\u043d \u043d\u0430 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u0437\u0430\u043c\u0435\u0447\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u043d\u0430\u0440\u0430\u0431\u043e\u0442\u043a\u0430\u0445 \u043e\u0442 \u043d\u0430\u0448\u0438\u0445 \u043a\u043e\u043b\u043b\u0435\u0433, \u044f \u0432\u0441\u0435\u0433\u043e \u043b\u0438\u0448\u044c \u0434\u043e\u0431\u0430\u0432\u0438\u043b \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u0441\u0432\u043e\u0435\u0433\u043e, \u0441\u043d\u0430\u0431\u0434\u0438\u043b \u0435\u0433\u043e \u0442\u0435\u043e\u0440\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u043c\u0438 \u0432\u044b\u043a\u043b\u0430\u0434\u043a\u0430\u043c\u0438 \u0438 \u043f\u0435\u0440\u0435\u0432\u0435\u043b \u0440\u0443\u0441\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a.\n\n* [The Competition Pipeline](https:\/\/www.kaggle.com\/averkij\/the-competition-pipeline)\n\n* [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python)\n* [How I made top 0.3% on a Kaggle competition](https:\/\/www.kaggle.com\/lavanyashukla01\/how-i-made-top-0-3-on-a-kaggle-competition)\n* [Comprehensive EDA and stacked regression model](https:\/\/www.kaggle.com\/scpitt\/comprehensive-eda-and-stacked-regression-model)","c47eff82":"Let't check the skewness after transformation.","d140496b":"### Blend the models","6e7bf4e5":"### **\u041c\u0435\u0434\u0438\u0442\u0430\u0446\u0438\u044f**","29bfc1e5":"#### String features","0b03380b":"They will be deleted later.\n","2174d415":"### \u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432","1cf581e4":"\u041f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 \u0432 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u0444\u0438\u0447\u0430\u0445 \u043c\u043e\u0436\u043d\u043e \u0437\u0430\u043f\u043e\u043b\u043d\u0438\u0442\u044c \u043d\u0443\u043b\u044f\u043c\u0438 \u0438\u043b\u0438 \u043c\u0435\u0434\u0438\u0430\u043d\u043e\u0439.","5af5fb1c":"There are still a lot of skewness. They will be dealt with later.","7a8b00bf":"## \u0422\u0440\u0435\u043d\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c","e210ec53":"We can find that there are two outliers at the bottom right. Let's locate them.","0474684b":"\u0423\u0434\u0430\u043b\u0438\u043c \u0432\u044b\u0431\u0440\u043e\u0441\u044b","36b38ac9":"\u041e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u0435\u043c \u043f\u0435\u0440\u0435\u043a\u043e\u0441 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u0444\u0438\u0447\u0430\u0445","df46d7f9":"## Set up cross validation","f96533f4":"Some of the non-numeric features are stored as numbers. They should be converted to strings.","192f218a":"### Zoomed heat map","dde9c9df":"Let's transform SalePrice distribution using log function.","a0df3680":"\u041d\u0430 \u044d\u0442\u043e\u0439 \"\u0442\u0435\u043f\u043b\u043e\u0432\u043e\u0439 \u043a\u0430\u0440\u0442\u0435\" \u0432\u0438\u0434\u043d\u043e, \u0447\u0442\u043e \u043c\u043d\u043e\u0433\u0438\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043a\u043e\u0440\u0440\u0435\u043b\u0438\u0440\u0443\u044e\u0442 \u0441 \u043d\u0430\u0448\u0438\u043c \u0442\u0430\u0440\u0433\u0435\u0442\u043e\u043c **SalePrice**. \u041f\u0440\u0438\u0447\u0435\u043c \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0441\u0438\u043b\u044c\u043d\u043e \u043a\u043e\u0440\u0440\u0435\u043b\u0438\u0440\u0443\u044e\u0442 \u0438 \u043c\u0435\u0436\u0434\u0443 \u0441\u043e\u0431\u043e\u0439. \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0441 \u0441\u0430\u043c\u043e\u0439 \u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0435\u0439.","5ca6621e":"## \u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043d\u043e\u0432\u044b\u0435 \u0444\u0438\u0447\u0438","2b9d87af":"\u0423\u0434\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u0438\u043c\u0441\u044f, \u0447\u0442\u043e \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432 \u043d\u0435 \u043e\u0441\u0442\u0430\u043b\u043e\u0441\u044c.","aced7433":"### Best model","76cbbd5e":"The rest string features can be filled with None.","1312d340":"## \u041f\u043e\u0434\u0435\u043b\u0438\u043c \u043d\u0430 train \u0438 test","881645f4":"# \u0415\u0441\u0442\u044c \u043b\u0438 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445?","7550bb91":"### \u0427\u0438\u0441\u043b\u043e\u0432\u044b\u0435 \u0444\u0438\u0447\u0438","03d9c8bb":"# \u0418\u0441\u0441\u043b\u0435\u0434\u0443\u0435\u043c \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0435 \u0444\u0438\u0447\u0438 "}}