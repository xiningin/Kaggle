{"cell_type":{"852dc1b6":"code","cac1d5d3":"code","ab23e53d":"code","df1ba340":"code","392276b2":"code","9724035a":"code","851b3a83":"code","9f6a5316":"code","7ee43828":"code","4e395ade":"code","ac85673d":"code","11d92112":"code","bf0413a7":"code","cc54acc6":"code","645b13e9":"code","5c106439":"code","1b3bb3ec":"code","5818e7a4":"code","bde2df70":"code","ccb3c025":"code","bc606b7c":"code","5ea22f3a":"code","20725c8e":"code","6f50c079":"code","bb2650c6":"code","92fccd5f":"code","0357f4bb":"code","f9f7f53c":"code","f77bf305":"code","c858cd78":"code","e992e7a3":"code","dac38427":"code","248971d0":"code","88b3f559":"code","6a26bde7":"code","3bfb5a50":"code","8ecc14d4":"code","5d2765d8":"code","76645e37":"code","69bdf221":"code","5dec3f3c":"code","176fcb73":"code","502750ad":"code","2baf4bbe":"code","262f13ed":"code","8c7ccd9b":"code","24cca848":"code","d56a1ab5":"code","35aa0905":"code","29ca1866":"code","e0c2829b":"code","c4e35fac":"code","733c1b5d":"code","be133a34":"code","71a2742a":"code","39d79d21":"code","db993cb1":"code","d51bd9b5":"code","eee9a826":"code","399ca686":"code","639c3fe7":"code","b63a0e4a":"code","21971f80":"markdown","8287d3f2":"markdown","14666481":"markdown","97c52aea":"markdown","d436f2ba":"markdown","e00b47de":"markdown","0bb5b966":"markdown","59a491a8":"markdown","8a492632":"markdown","d1ed33cc":"markdown","51a5554d":"markdown","129fafb6":"markdown","337babd4":"markdown","3dcf27b0":"markdown","f8482f13":"markdown","8f27141e":"markdown","92779b11":"markdown","5736b3dc":"markdown","572f402c":"markdown","25530bf5":"markdown"},"source":{"852dc1b6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom sklearn.preprocessing import normalize\nfrom sklearn.pipeline import Pipeline\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n","cac1d5d3":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","ab23e53d":"train.head(6)","df1ba340":"train.info()","392276b2":"for n in train.columns:\n    print('The number of different values in ', n, 'are:', len(train[n].unique()))\nprint('-------------------------------------')\n#I will search numbers of Nan values\nfor n in train.columns:\n    if train[n].isnull().values.any() == True:\n        print('There is' , train[n].isna().sum(), 'null values in', n, 'column')","9724035a":"for n in test.columns:\n    print('The number of different values in ', n, 'are:', len(test[n].unique()))\nprint('-------------------------------------')\n#I will search numbers of Nan values\nfor n in test.columns:\n    if test[n].isnull().values.any() == True:\n        print('There is' , test[n].isna().sum(), 'null values in', n, 'column')","851b3a83":"train = train.drop(['Name','Ticket','PassengerId'], axis = 1)\ntest = test.drop(['Name','Ticket','PassengerId'], axis = 1)","9f6a5316":"#First, we will create a copy:\ntrain_bf = train.copy()\ntest_bf = test.copy()","7ee43828":"print(\"The mean Age value for the train DataFrame is:\",train['Age'].mean(),\"\\nThe Standard Deviation for the Age is:\",train['Age'].std())\nprint(\"The mean Age value for the test DataFrame is:\",test['Age'].mean(),\"\\nThe Standard Deviation for the Age is:\",test['Age'].std())","4e395ade":"#Creating a function that fill NaN values of train and test\ndef fill_nan_w_mean_std(df,col='Age'):\n    nan = df[df[col].isna()]\n    min = df[col].mean() - df[col].std()\n    max = df[col].mean() + df[col].std()\n    for i in nan.index:\n        random_num = random.uniform(min,max)\n        df[col].loc[i] = random_num","ac85673d":"fill_nan_w_mean_std(train)\nfill_nan_w_mean_std(test)","11d92112":"#We will transform the values to integers\ntrain['Age'] = train['Age'].astype(int)\ntest['Age'] = test['Age'].astype(int)","bf0413a7":"plt.style.use('ggplot')\nfig = plt.figure(figsize = (25,10))\nfig.subplots_adjust(hspace=0.6, wspace=0.15)\nax = fig.add_subplot(1,2,1)\nax.set_ylim([0,0.04])\nax.set_title('Before filling NaN values')\nsns.distplot(train_bf['Age'].dropna(), bins=20)\nax_2 = fig.add_subplot(1,2,2)\nax_2.set_ylim([0,0.04])\nax_2.set_title('Data with filled values')\nsns.distplot(train['Age'],bins=20)","cc54acc6":"#First, we have to visualize the Embarked column in train data: \nsns.countplot(x='Embarked',data=train)","645b13e9":"#Now we see that most of the data embarked from S port we can fill with this class\nnan_emb = train[train['Embarked'].isna()]\nfor i in nan_emb.index:\n    train['Embarked'].loc[i] = 'S'","5c106439":"#We deal with that one nan value in fare column of test data\nnan_fare = test[test['Fare'].isna()]\nfor i in nan_fare.index:\n    test['Fare'].loc[i] = test['Fare'].mean()","1b3bb3ec":"train['Family_members'] = train['SibSp'] + train['Parch'] + 1 #him\/her\ntest['Family_members'] = test['SibSp'] + test['Parch'] + 1\n\n#Let's create the Alone feature\ntrain['not_Alone'] = 0 #0 if he\/she is alone\ntrain['not_Alone'].loc[train['Family_members'] > 1] = 1\n\ntest['not_Alone'] = 0 #1 if he\/she is alone\ntest['not_Alone'].loc[train['Family_members'] > 1] = 1","5818e7a4":"train.head()","bde2df70":"#We will create a third string in gender: child. Because in a catastrophe like this children are first.\ndef agg_child(df,col='Age'):\n    mask = (df[col] <= 15)\n    df.loc[mask,'Sex'] = 'child'\n\n#We apply the function\nagg_child(train)\nagg_child(test)","ccb3c025":"def agg_cabin_bin(df,col='Cabin'):\n    df['with_Cabin'] = 0\n    mask = (df[col].isna() == False)\n    df.loc[mask,'with_Cabin'] = 1\n#0 if the passenger had not a Cabin, 1 if he has a cabin","bc606b7c":"agg_cabin_bin(train)\nagg_cabin_bin(test)\n\ndel train['Cabin']\ndel test['Cabin']","5ea22f3a":"train.head()","20725c8e":"sns.barplot(x='Sex',y='Survived',data=train, order=['female','child','male'])\nplt.title('Sex vs Survived')","6f50c079":"gs = plt.GridSpec(2,3,wspace=0.45, hspace=0.8)\nplt.figure(figsize=(12,10))\nax1 = plt.subplot2grid((3,3),(0,0),rowspan=2,colspan=2)\nplt.title('Age vs Survived vs Sex')\nsns.swarmplot(x = 'Survived',y='Age', \n              data=train, linewidth=1,hue='Sex', palette = 'muted')\nax2 = plt.subplot2grid((3,3),(0,2))\nplt.title('Embarked vs Survived')\nsns.barplot(x='Embarked',y='Survived',\n            data=train,order=['C','Q','S'])\nax3 = plt.subplot2grid((3,3),(1,2))\nplt.title('Pclass vs Survived')\nsns.barplot(x='Pclass',y='Survived',\n            data=train, palette = 'muted')","bb2650c6":"#Now we can say that Embarked class\/Pclass\/sex have priorities for survive. So we can transform the categorical data to numbers\ncat_to_nums = {\"Embarked\":  {\"S\": 0, \"Q\": 1, \"C\":2},\n               \"Sex\": {\"male\":0,\"child\":1,\"female\":2}}\n#We will use replace to convert the values\ntrain.replace(cat_to_nums, inplace = True)","92fccd5f":"test.replace(cat_to_nums, inplace = True)","0357f4bb":"train.head()","f9f7f53c":"train.corr()","f77bf305":"def correlation_heatmap(df): #from \"A Data Science Framework: To Achieve 99% Accuracy\" kernel by LD Freedman\n    s , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    g = sns.heatmap(df.corr(), cmap = colormap,square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax, annot=True,linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 })\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)","c858cd78":"correlation_heatmap(train)","e992e7a3":"# Normalize 'Fare' & 'Age' values for test and train dataset\nX = [train['Fare'],\n     train['Age']]\nX_normalize = normalize(X)\n\nX_2 = [test['Fare'],\n      test['Age']]\nX_2_normalize = normalize(X_2)\n\ntrain = train.assign(Fare = X_normalize[0])\n\ntrain = train.assign(Age = X_normalize[1])\n\ntest = test.assign(Fare = X_2_normalize[0])\n\ntest = test.assign(Age = X_2_normalize[1])","dac38427":"#Now we have all numerical values!\ntrain.head()","248971d0":"X_train = train.loc[:,'Pclass':]\ny_train = train.loc[:,'Survived']\nX_test_final = test","88b3f559":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV #We will use gridsearchCV\n\n#Dividing the data before tuning the model\nX_train, X_test, y_train, y_test = train_test_split(X_train,y_train,test_size = 0.3, \n                                                    random_state=21)","6a26bde7":"#Logistic Regression:\n\nlogreg = LogisticRegression()\n\nparam_grid = {'C' : [x for x in range(1,5000,5)]  }\n#finding the best parameter:\nsearcher = GridSearchCV(logreg, param_grid)\n\nsearcher.fit(X_train,y_train)","3bfb5a50":"# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher.best_params_)\nprint(\"Best CV accuracy\", searcher.best_score_)\n\n# Report the test accuracy using these best parameters\nprint(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))","8ecc14d4":"pred = searcher.predict(X_test_final)","5d2765d8":"from sklearn.svm import SVC\nsvc = SVC()\nparameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1,1,10,100],\n              'C':[x for x in np.linspace(0.1,10,100)]}\nssp = GridSearchCV(svc,parameters)\n\nssp.fit(X_train,y_train)","76645e37":"# Report the best parameters and the corresponding score\nprint(\"Best CV params\", ssp.best_params_)\nprint(\"Best CV accuracy\", ssp.best_score_)\n\n# Report the test accuracy using these best parameters\nprint(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))","69bdf221":"pred23 = ssp.predict(X_test)\nlen(y_test)","5dec3f3c":"y_test","176fcb73":"from sklearn.linear_model import SGDClassifier","502750ad":"# We set random_state=0 for reproducibility \nlinear_classifier = SGDClassifier(random_state=0)\n\n# Instantiate the GridSearchCV object and run the search\nparameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n             'loss':['hinge', 'log'], 'penalty':['l1','l2']}\nsearcher_sgd = GridSearchCV(linear_classifier, parameters, cv=10)\nsearcher_sgd.fit(X_train, y_train)\n\n# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher.best_params_)\nprint(\"Best CV accuracy\", searcher.best_score_)\nprint(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))","2baf4bbe":"from sklearn.tree import DecisionTreeClassifier, export_graphviz #importing the module\nfrom sklearn.metrics import accuracy_score","262f13ed":"#Finding our optimize max_depth\nfor i in range(1,18):\n    tree_clf = DecisionTreeClassifier(max_depth=i)\n    tree_clf.fit(X_train,y_train)\n    y_pred = tree_clf.predict(X_test)\n    print(\"Test accuracy of the Decision Trees model:\", accuracy_score(y_pred,y_test), \"for max_depth: \", i)","8c7ccd9b":"#Basic Decision Tree model\ntree_clf = DecisionTreeClassifier(max_depth = 2)\ntree_clf.fit(X_train,y_train) #training the model\n\n#Predicting with the model\ny_pred = tree_clf.predict(X_test)\n\n#We will try to evaluate the accuracy score\nprint(\"Test accuracy of the Decision Trees model:\", accuracy_score(y_pred,y_test))","24cca848":"pred = tree_clf.predict(X_test_final)","d56a1ab5":"from sklearn.ensemble import RandomForestClassifier","35aa0905":"#Random Forest Classifier model\nrnd_clf = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16, n_jobs = -1)\nrnd_clf.fit(X_train,y_train)\n\n#Predicting\ny_pred = rnd_clf.predict(X_test)\n\n#Evaluate the accuracy score\nprint(\"Test accuracy of the Decision Trees model:\", accuracy_score(y_pred,y_test))","29ca1866":"pred = rnd_clf.predict(X_test_final)","e0c2829b":"from sklearn.ensemble import VotingClassifier","c4e35fac":"#We will now try an ensemble method\nvoting_clf = VotingClassifier(estimators = [('lr',searcher),('rf',rnd_clf),('svc',ssp),\n                                            ('sgd',searcher_sgd),('dt',tree_clf)],\n                             voting = 'hard')\nvoting_clf.fit(X_train,y_train)","733c1b5d":"y_pred = voting_clf.predict(X_test)","be133a34":"#Evaluate the accuracy score\nprint(\"Test accuracy of the Decision Trees model:\", accuracy_score(y_pred,y_test))\npred = voting_clf.predict(X_test_final)","71a2742a":"from sklearn.ensemble import AdaBoostClassifier","39d79d21":"adb_clf = AdaBoostClassifier(base_estimator = tree_clf,\n                             n_estimators = 100)\nadb_clf.fit(X_train,y_train)\n","db993cb1":"y_pred = adb_clf.predict(X_test_final)\ny_pred","d51bd9b5":"from keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical","eee9a826":"y_train2 = to_categorical(y_train)","399ca686":"n_cols = X_train.shape[1]\nmodel = Sequential()\nmodel.add(Dense(100,activation='relu',input_shape = (n_cols,)))\nmodel.add(Dense(100,activation='relu'))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(optimizer='sgd', \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])","639c3fe7":"model.fit(X_train, y_train2, epochs = 100)","b63a0e4a":"preds = model.predict(X_test)","21971f80":"## Normalizing data\nhttps:\/\/medium.com\/@rrfd\/standardize-or-normalize-examples-in-python-e3f174b65dfc\n","8287d3f2":"## Ensemble Method (all the models) [0.805]","14666481":"## AdaBoost","97c52aea":"### Logistic Regression  (81.7)","d436f2ba":"Now we can visualize if there is a difference on the fill values:","e00b47de":"## Data Visualization","0bb5b966":"Now lets deal with cabin values. Since there are too many NaN values we can binarize if the passengers have Cabin or not.","59a491a8":"We have seen there are two variables with family relations (SibSp & Parch) so why we can not create a variable that resumen the family member for one person? And also count the people who travel alone in the ship. ","8a492632":"## Support Vector Machines: \n#### Support Vector Classifier (81,7)","d1ed33cc":"## We can create other features from the data","51a5554d":"## Stochastic Gradient Descent Classifier (81,7)\n","129fafb6":"We can say that there is no significative difference after we fill the nan values. The distribution is almost the same. But have two peaks. Later we will deal with that.\n\nNow we have to deal with nan values in embarked column on train dataframe and with nan ones in Fare column on test dataframe. ","337babd4":"We will eliminate the irrelevant features and the one that contains too many nan values","3dcf27b0":"# Machine Learning\n\n-Logistic regression vs SVM","f8482f13":"Now we fill NaN values of the columns Age and Embarked. In the 'Age' column we will fill with random numbers beetwen (mean +- standard deviation) values. ","8f27141e":"## Decision Trees Classifier (0.79)","92779b11":"# Basic Neural Network","5736b3dc":"## RANDOM FOREST MODEL  (0.81)","572f402c":"## Finding Correlations on data:","25530bf5":"Pclass: socio-economic status 1st = Upper 2nd=Middle 3rd=Lower \n\nSibSp: The dataset defines family relations # of siblings \/ spouses aboard the Titanic\n\nParch: # of parents \/ children aboard the Titanic\n\n\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\nSome children travelled only with a nanny, therefore parch=0 for them.\n\n\nsurvival\tSurvival\t0 = No, 1 = Yes\npclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n\nembarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n\nCabin: cabin number"}}