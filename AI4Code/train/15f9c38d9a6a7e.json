{"cell_type":{"662992b8":"code","bc303a09":"code","96617ecc":"code","4a1833ed":"code","b12360af":"code","13187692":"code","6bdb9fdb":"code","c0f4c364":"code","d1815d55":"code","c9c21560":"code","68c3a747":"code","0ced7874":"code","52bcd50f":"code","d7be7d2e":"code","1ea0f7e6":"code","94c3219b":"code","f0b60fc6":"code","f5201535":"code","6cdcb693":"code","3d316478":"code","ace275c0":"code","fc8489a4":"code","fbe0f6f0":"code","39df74dc":"code","22441319":"markdown","44496508":"markdown","abffa6f9":"markdown","8c2593af":"markdown","c1e906cd":"markdown","a428f148":"markdown","5170ee43":"markdown","42e32788":"markdown","1a3729f0":"markdown","926d0631":"markdown","cb54f803":"markdown","5239df38":"markdown","230fd414":"markdown","e9a10404":"markdown","642198d5":"markdown","3ca4e5aa":"markdown","feb28a9f":"markdown","8641bc3d":"markdown","d5e17f96":"markdown","f7a3f210":"markdown"},"source":{"662992b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport json","bc303a09":"df = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")","96617ecc":"isempty = df[(df['has_full_text'] == True) & (df['sha'] == 'NaN')].empty\nprint('Is the DataFrame empty :', isempty)","4a1833ed":"df_verified = df.dropna(subset=['sha'])\nuniq_count = df_verified['sha'].drop_duplicates().count()\ntotal_count = df_verified['sha'].count()\nprint('Uniq sha: ', uniq_count)\nprint('Total sha: ', total_count)\nprint(total_count - uniq_count)\n","b12360af":"df_verified[df_verified.duplicated(subset=['sha'],keep=False)].sort_values('sha')","13187692":"import os.path\ndef full_path(sha,full_text_file):\n    data_path_pattern = \"\/kaggle\/input\/CORD-19-research-challenge\/{full_text_file}\/{full_text_file}\/{sha}.json\"\n    path = data_path_pattern.format(\n            full_text_file=full_text_file,\n            sha=sha\n            )\n    return path","6bdb9fdb":"def is_file_exists(shas, full_text_file):\n        result = all(os.path.exists(full_path(sha,full_text_file)) for sha in shas.split(\"; \"))    \n        return result\n\ndf_uniq = df_verified.drop_duplicates(subset = [\"sha\"])\ndf_uniq['is_file_exists'] = df_uniq.apply(lambda x: is_file_exists(x['sha'], x['full_text_file']), axis=1)\n    ","c0f4c364":"len(df_uniq[df_uniq['is_file_exists'] == False])","d1815d55":"# Example of wp's data\n\ndata_path_pattern = \"\/kaggle\/input\/CORD-19-research-challenge\/{full_text_file}\/{full_text_file}\/{sha}.json\"\n\npath = data_path_pattern.format(\n    full_text_file=\"custom_license\",\n    sha=\"be7e8df88e63d2579e8d61e2c3d716d57d347676\"\n)\n\nwith open(path, \"r\") as f:\n    data = json.load(f)\n","c9c21560":"def object_size(name, sha, full_text_file):\n    path = full_path(sha,full_text_file)\n    with open(path, \"r\") as f:\n        data = json.load(f)\n        return len(data[name])\n    \ndef abstract_size(sha, full_text_file):\n    return object_size('abstract', sha, full_text_file)\n\n    \ndef body_size(sha, full_text_file):\n    return object_size('body_text', sha, full_text_file)\n\ndef authors_size(sha, full_text_file):\n    path = full_path(sha,full_text_file)\n    with open(path, \"r\") as f:\n        data = json.load(f)\n        return len(data['metadata']['authors'])\n    \n","68c3a747":"from tqdm.notebook import trange, tqdm\n\ndf_uniq = df_verified.drop_duplicates(subset = [\"sha\"])[['sha','full_text_file']]\nrows = []\n\nfor row in tqdm(df_uniq.iterrows()):\n    _,(shas,full_text_file) = row\n    for sha in shas.split(\"; \"):\n        new_row = {'sha': sha, 'full_text_file': full_text_file}\n        rows.append(new_row)\n        \ndf_test = pd.DataFrame(rows)","0ced7874":"len(df_test)","52bcd50f":"df_test['is_file_exists'] = df_test.apply(lambda x: is_file_exists(x['sha'], x['full_text_file']), axis=1)","d7be7d2e":"df_test['abstarct_size'] = df_test.apply(lambda x: abstract_size(x['sha'], x['full_text_file']), axis=1)","1ea0f7e6":"df_test['authors_size'] = df_test.apply(lambda x: authors_size(x['sha'], x['full_text_file']), axis=1)","94c3219b":"df_test['body_size'] = df_test.apply(lambda x: body_size(x['sha'], x['full_text_file']), axis=1)","f0b60fc6":"df_test['bib_entries_size'] = df_test.apply(lambda x: object_size('bib_entries', x['sha'], x['full_text_file']), axis=1)","f5201535":"df_test['authors_size'].describe()","6cdcb693":"from tqdm.notebook import trange, tqdm\ndf_uniq = df_verified.drop_duplicates(subset = [\"sha\"]).head(100)[['sha','full_text_file']]\nfor row in tqdm(df_uniq.iterrows()):\n    _,(shas,full_text_file) = row\n    for sha in shas.split(\"; \"):\n        path = data_path_pattern.format(\n        full_text_file=full_text_file,\n        sha=sha\n        )\n        with open(path, \"r\") as f:\n            data = json.load(f)\n            if (len(data['body_text']) == 1):\n                print(data['body_text'])","3d316478":"duplicate_title_df = df_verified[df_verified.duplicated(subset=['title'],keep=False)].sort_values('title')\nduplicate_title_df","ace275c0":"duplicate_title_df['title'].value_counts()","fc8489a4":"df_verified['journal'].value_counts()","fbe0f6f0":"#Article should has a bib_entries > 3 items\n\nfrom tqdm.notebook import trange, tqdm\ndf_uniq = df_verified.drop_duplicates(subset = [\"sha\"]).head(10)[['sha','full_text_file']]\nfor row in tqdm(df_uniq.iterrows()):\n    _,(shas,full_text_file) = row\n    for sha in shas.split(\"; \"):\n        path = data_path_pattern.format(\n        full_text_file=full_text_file,\n        sha=sha\n        )\n        with open(path, \"r\") as f:\n            data = json.load(f)\n            if (len(data['bib_entries']) < 3):\n                print(data['bib_entries'])\n                print(sha)","39df74dc":"#Article should has a **Cred-Test-2****: article should have at least 3 authors > 3 items\n\nfrom tqdm.notebook import trange, tqdm\ni = 0\ndf_uniq = df_verified.drop_duplicates(subset = [\"sha\"])[['sha','full_text_file']]\nfor row in tqdm(df_uniq.iterrows()):\n    _,(shas,full_text_file) = row\n    for sha in shas.split(\"; \"):\n        path = data_path_pattern.format(\n        full_text_file=full_text_file,\n        sha=sha\n        )\n\n        with open(path, \"r\") as f:\n            data = json.load(f)\n            if (len(data['metadata']['authors']) < 2):\n                i += 1\n\nprint(i)","22441319":"As bonus let's define journal distribution\n\n","44496508":"[](http:\/\/)**CSV-Test-1**: Sha key should be not null if has_full_test is True","abffa6f9":"***Found all articles with body test len = 1***\nprint body\n","8c2593af":"12 sha values are not uniq","c1e906cd":"***Completeness***\n0. Abstract is presented (-) many articles have no abstract\n1. full body (?) There are articles with non clear body. len(body_text) = 1\n2. number of words in full body (30k symbols)\n3. link t external resources: (10)","a428f148":"1. I found that  titles have title \"Index' and 'Author index'","5170ee43":"***accuracy***\n1. no typo\n2. relevants links\n3. Structure and style","42e32788":"***Uniqueness***\n1. Data should contain uniq articals \n2. Sha key should be uniq : Look at test CSV-Test-2 ","1a3729f0":"As QA I want to care about data quality. Data quality is important for climbing a excellent result.\nUsually data testing should start from defining expected results with product owners and end-users. In this case I have no access to owners and i assume some expected results by myself. Anyway, I start QA from some exploratory testing session.","926d0631":"* I think I could assume that let(data['body_text']) == 1 is not good data too.\n","cb54f803":"**CSV file data**","5239df38":"**Cred-Test-2****: article should have at least 3 authors","230fd414":"> PLOS One is a peer-reviewed open access scientific journal published by the Public Library of Science since 2006. The journal covers primary research from any discipline within science and medicine. Wikipedia","e9a10404":"> **Uniq-Test-1: Article should be uniq**\n\nIdea: Data set should contain only uniq articles. \nI can check articles in Metadata","642198d5":" I'm only interested in sources with sha is not NaN","3ca4e5aa":"**CSV-Test-3:** article should be existed","feb28a9f":"**Cred-Test-1****: article should have at least 3 bib_entries","8641bc3d":"***Credibility***\nIt is the most difficult for testing. How can o define Credibility for article? It means that should i find all \"fake\" articles? I'd like to found all articles by next creteriases:\n1. Article should has a bib_entries\n2. Article should has Authors","d5e17f96":"*sha key*\n1. Should be not null if full body is true (+)\n2. Should be uniq (-) 12 sha keys are not uniq\n3. Article with sha and has_full_text==true should be existed (+)","f7a3f210":"* [](http:\/\/)**CSV-Test-2**: Sha key should be uniq"}}