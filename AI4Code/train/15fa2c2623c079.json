{"cell_type":{"827a1e5f":"code","b4e0eaee":"code","c81286d9":"code","eae6fc55":"code","d77ebe69":"code","a4dbf71b":"code","5e94a3aa":"code","ed791eaa":"code","587bd18a":"code","5b700a95":"code","4cf884ed":"code","67c5f996":"code","beee18ce":"code","e49d44cb":"code","247f45eb":"code","659e0f05":"code","a318ffda":"code","934b1471":"code","f2522dcc":"code","5065f18e":"code","7cf86cb5":"code","7e17f968":"code","265f2341":"code","5d7b8b2d":"code","a1d37f7e":"code","65c4b6b5":"code","3f42418c":"code","4dc6724d":"code","a2f6884e":"code","462e5e51":"code","f367f712":"code","a94d5a8d":"code","e95847b5":"code","e8f1f209":"markdown","bf2643ec":"markdown","2b99e3b9":"markdown","85fd034e":"markdown","35511f2c":"markdown","83fc47fc":"markdown","7c2faa22":"markdown","3e61053e":"markdown","fd36fd85":"markdown","d70d441a":"markdown","2ea6c8ed":"markdown","018ed1d0":"markdown","1a74b090":"markdown","3883a6df":"markdown","714bcc85":"markdown","e09c95bd":"markdown","e146b733":"markdown","1c279931":"markdown","6594a831":"markdown","32f9d2ab":"markdown","9e5b7dca":"markdown","80ae7529":"markdown","61d198dd":"markdown","e7804360":"markdown","69682308":"markdown","dc1398ff":"markdown","79e6de58":"markdown"},"source":{"827a1e5f":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nsns.set_style('darkgrid')\nsns.set_palette('Pastel1')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b4e0eaee":"df = pd.read_csv('\/kaggle\/input\/fraud-detection-bank-dataset-20k-records-binary\/fraud_detection_bank_dataset.csv')","c81286d9":"df.head()","eae6fc55":"df.drop('Unnamed: 0', axis=1, inplace=True)","d77ebe69":"df.shape","a4dbf71b":"df.info()","5e94a3aa":"df.isnull().sum().sum()","ed791eaa":"df.describe()","587bd18a":"sns.countplot(df['targets'])\nplt.show()\n\nprint('Ratio Target Class')\ndf['targets'].value_counts(normalize=True)","5b700a95":"corr = df.corr()","4cf884ed":"corr","67c5f996":"duplicate = np.sum(corr >= 0.8)\nduplicate = duplicate[duplicate > 1]\nduplicate = duplicate.reset_index()\nduplicate = duplicate.sort_values(0)\nduplicate.shape","beee18ce":"duplicate","e49d44cb":"corr.fillna(-999, inplace=True)","247f45eb":"null = np.sum(corr == -999)\nnull = null[null > 15]\nnull = null.reset_index()\nnull.shape","659e0f05":"null","a318ffda":"col_to_drop = null['index'].append(duplicate['index'].iloc[:-1])","934b1471":"df.drop(col_to_drop, axis=1, inplace=True)","f2522dcc":"df.head()","5065f18e":"df.shape","7cf86cb5":"X, y = df.drop('targets', axis=1), df['targets']","7e17f968":"scaler = StandardScaler()\nX = scaler.fit_transform(X)","265f2341":"pca = PCA(n_components=17)\nX = pca.fit_transform(X)","5d7b8b2d":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)","a1d37f7e":"smt = SMOTE()\nX_train, y_train = smt.fit_resample(X_train, y_train)","65c4b6b5":"def result(name, y_pred):\n    \"\"\"Returns multiple classification metrics of a machine learning algorithm\n    \n    :params name: Machine learning algorithm name\n    \n    :params y_pred: Prediction results of a machine learning algorithm test set\n    \"\"\"\n    print(f'Results of Machine Learning Modelling with {name} Algorithms\\n')\n    print('-' * 78)\n    print(f'\\nAccuracy Score :\\n{accuracy_score(y_test, y_pred) * 100 } %\\n')\n    print(f'Confusion Matrix :\\n{confusion_matrix(y_test, y_pred)}\\n')\n    conf = confusion_matrix(y_test, y_pred, normalize='all')\n    print('Confusion Matrix with Normalized Value :')\n    print('[[{:.3f}  {:.3f}]\\n [{:.3f}  {:.3f}]]\\n'.format(conf[0,0], conf[0,1], conf[1,0], conf[1,1]))\n    print(f'Classification Report :\\n{classification_report(y_test, y_pred)}\\n')\n    print(f'ROC AUC Score :\\n{roc_auc_score(y_test, y_pred,)}\\n')","3f42418c":"knn_params = {'n_neighbors':np.arange(2,50)}\nknn_grid = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5, refit=True, scoring='roc_auc')\nknn_grid.fit(X_train, y_train)\nknn_pred = knn_grid.predict(X_test)\nresult('KNN', knn_pred)","4dc6724d":"rfc_params = {'min_samples_split':[0.001, 0.0001],\n             'n_estimators':[200, 300],\n              'criterion':['entropy']\n             }\nrfc_grid = GridSearchCV(RandomForestClassifier(), rfc_params, scoring='roc_auc', cv=3, refit=True)\nrfc_grid.fit(X_train, y_train)\nrfc_pred = rfc_grid.predict(X_test)\nresult('Random Forest Classifier', rfc_pred)","a2f6884e":"svc = SVC(kernel='rbf', C=3, gamma=0.1, degree=2)\nsvc.fit(X_train, y_train)\nsvc_pred = svc.predict(X_test)\nresult('Support Vector Classifier', svc_pred)","462e5e51":"gbc = GradientBoostingClassifier(min_samples_split=0.0001, n_estimators=300, max_depth=10, learning_rate=0.1)\ngbc.fit(X_train, y_train)\ngbc_pred = gbc.predict(X_test)\nresult('Gradient Boosting Classifier', gbc_pred)","f367f712":"vc = VotingClassifier([('RFC', rfc_grid.best_estimator_),\n                      ('KNN', knn_grid.best_estimator_),\n                      ('GBC', GradientBoostingClassifier(min_samples_split=0.0001, n_estimators=300, max_depth=10, learning_rate=0.1)),\n                      ('SVC', SVC(kernel='rbf', C=3, gamma=0.1, degree=2, probability=True))],\n                      voting='soft',n_jobs=-1)\nvc.fit(X_train, y_train)\nvc_pred = vc.predict(X_test)\nresult('Voting Classifier w\/ 4', vc_pred)","a94d5a8d":"model = VotingClassifier([('RFC', rfc_grid.best_estimator_),\n                      ('KNN', knn_grid.best_estimator_),\n                      ('GBC', GradientBoostingClassifier(min_samples_split=0.0001, n_estimators=300, max_depth=10, learning_rate=0.1)),\n                      ('SVC', SVC(kernel='rbf', C=3, gamma=0.1, degree=2, probability=True))],\n                      voting='soft',n_jobs=-1)\nmodel.fit(X, y)\npred = model.predict(X)\nprint(roc_auc_score(y, pred))\nprint(confusion_matrix(y, pred))\nprint(classification_report(y, pred))","e95847b5":"import joblib\njoblib.dump(model, 'VC Model')","e8f1f209":"Here I will do an over-sampling for a class with a value of 1, after that I will train the model with several algorithm choices to see how it performs. Finally, I will use VotingClassifier as the main model to solve this case","bf2643ec":"Finally we have 73 columns to use","2b99e3b9":"# Importing the Library","85fd034e":"Whoa! I love the recall","35511f2c":"# Load Dataset & Exploring Data","83fc47fc":"# Machine Learning Modelling","7c2faa22":"Hi, friend! My name is Rafka, now I am in 11th grade of a vocational high school. Oh yeah, this is my first task that i publish on kaggle, i hope i can get some feedback from you to improve myself in the future","3e61053e":"### Dropping Multicollinearity Columns","fd36fd85":"After satisfactory results from the four previous algorithms, I will now make the main model which is a combination of the previously defined models","d70d441a":"# Fraud Detection Bank Dataset","2ea6c8ed":"I will standardize the data and do PCA to extract the features into 17 components so that the model doesn't have so many columns","018ed1d0":"#### That's all from me, thank you very much","1a74b090":"### KNeighborsClassifier","3883a6df":"## Prepare the Model for All Data","714bcc85":"In closing, I want to make the same model but with all the data we have, I know the results will be good because we are testing with the same data","e09c95bd":"# Standardize Data and Feature Extraction with PCA","e146b733":"Here we have several NaN correlations which means that there are several columns that only have the same value or can be called 0 variance. We will remove that column from our data. For ease of analysis, I filled in the value of NaN to change to -999","1c279931":"### GradientBoostingClassifier","6594a831":"Condition null > 15 is to select columns that actually have only NaN values. Since we have 15 features with 0 variance, all of our columns must have at least 15 NaN in the correlation shown. Therefore I need to select it until it finally displays a column that actually has 0 variance","32f9d2ab":"Here I found 25 features that are correlated above 0.8. I will only leave one column that has the most correlation above 0.80 of the 25 correlated columns. That's why I did sum() and did sorting.\n\nMy hypothesis of doing this is based on the probability of similar values \u200b\u200bamong these 25 columns. And right, one of the columns has a correlation above 0.8 with 17 columns that I have sorted earlier","9e5b7dca":".iloc[:-1] means that we remove all columns that have high collinearity and leave only one of them","80ae7529":"# Voting Classifier","61d198dd":"### Support Vector Classifier","e7804360":"Here I see some irregularities, such as NaN correlation and multiple multicollinearity. I want to drop the columns that have NaN correlations and those that have correlations above 0.8 and leave only one of them.","69682308":"We have 15 features with 0 variance, we will discard all those columns","dc1398ff":"Your feedback is very helpful for me, thank you!","79e6de58":"### Random Forest Classifier"}}