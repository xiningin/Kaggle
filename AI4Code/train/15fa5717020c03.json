{"cell_type":{"53366111":"code","6da2830f":"code","1e3ee798":"code","c7e4d508":"code","0f0abfc5":"code","15d5473e":"code","86de9b0f":"code","09321e86":"code","578d61e6":"code","8422b6d4":"code","860ab581":"code","11221660":"code","b4ebca26":"code","09c3e526":"code","c2f297a5":"code","2850af05":"code","a915acb1":"code","59993913":"code","1b704daf":"code","7a235da3":"code","7feff237":"code","e87e832c":"code","a8f6f539":"code","b4198b25":"code","2ea13f8f":"code","88fcc18c":"code","b1671f74":"code","30e46994":"code","df308405":"code","bd60c162":"code","610bbe25":"code","40c82e45":"code","cfe32eaa":"code","aaf1cc58":"code","7e4d031e":"code","9a01a263":"code","a15eec6c":"code","aa715e38":"code","5c609de5":"code","5ee1b6a3":"code","dede3822":"code","a39954f9":"code","3147643e":"code","4735bdc0":"code","374f1409":"code","232d53e9":"code","971180ce":"code","03c81f73":"code","f54b52b9":"code","7453e234":"code","dd4eff63":"code","960b42a2":"code","902acf31":"code","44bed4df":"code","cd7f0740":"code","c5a928c6":"code","28741dbd":"code","40a293ac":"code","d79ccfc7":"code","81087555":"code","f954beba":"code","33f88bb7":"markdown","2f898d78":"markdown","7b0f896d":"markdown","357c7e4c":"markdown","d326314e":"markdown","4215d5a8":"markdown","5c543c20":"markdown","abb7f3a0":"markdown","d93daf9e":"markdown","fe5fb976":"markdown","478fced2":"markdown","9c9c1ccb":"markdown","8fd9f744":"markdown","6ff0348f":"markdown","31509eed":"markdown","1a30938d":"markdown","28021c56":"markdown","3ce82afc":"markdown","89935f23":"markdown","06544c5a":"markdown","e7f82326":"markdown","ee318af1":"markdown","7b45410d":"markdown","bb30bbf9":"markdown","243f0ac3":"markdown","40affebe":"markdown","b83847ba":"markdown","6f7fbbf7":"markdown","92c474ba":"markdown","5cdcc62c":"markdown","2a288219":"markdown","e3bb50e0":"markdown","7437ed93":"markdown","94ccb1b3":"markdown","a9371fec":"markdown","257c9bc9":"markdown","58c0bd7a":"markdown","0b65da12":"markdown","78fa8187":"markdown","d1c1f9f3":"markdown","ce938e44":"markdown","08692aa6":"markdown","66ba9153":"markdown","1a1a8c2b":"markdown","d59884ec":"markdown","1226047e":"markdown","74cfc6ca":"markdown","1b36f48a":"markdown","42ffe9cb":"markdown","6a789f3e":"markdown","b21074f2":"markdown","18ef80fe":"markdown","22161b1a":"markdown","4fec2899":"markdown","46e91796":"markdown","df609f7b":"markdown","00f6597d":"markdown","c25ce2bc":"markdown","a6e009ef":"markdown","4b5da5bf":"markdown","21657b41":"markdown","4495b05f":"markdown","461992b3":"markdown","ec6e8bd5":"markdown","e27e7a91":"markdown","3618ac28":"markdown","a0c78b96":"markdown","7e83efd6":"markdown","b4ee57a3":"markdown","230cc92a":"markdown","bd2426f4":"markdown","3bdef1ea":"markdown","93bfd0ea":"markdown","7955c0c3":"markdown","26d7e052":"markdown","f9e79cbb":"markdown","056a0c98":"markdown","d7b60e0d":"markdown","c121d88e":"markdown","418f2a8b":"markdown","bcc3034b":"markdown","81312cae":"markdown","9e5e7b63":"markdown","f47856e7":"markdown","3c3db089":"markdown","e2ef460e":"markdown","35bc6599":"markdown","26a38c9b":"markdown","ea38288d":"markdown","43610b30":"markdown","c87e7cab":"markdown","65ddf0b1":"markdown","f91bba12":"markdown","806e8597":"markdown","5d8e6113":"markdown","b4c21ea9":"markdown","00c5264c":"markdown","5b948805":"markdown","f7c04288":"markdown","205063fa":"markdown","d9de6f4f":"markdown","87e7e4ae":"markdown","8ca74021":"markdown","2e01cb04":"markdown","744d75e0":"markdown","577edd26":"markdown","481665b8":"markdown","02ae84cf":"markdown","37f80d85":"markdown","0cecc0e1":"markdown","8c812bca":"markdown","c563fb54":"markdown","badeeba7":"markdown","1377ea93":"markdown","ffec378c":"markdown","acc5f7ef":"markdown","d82e2093":"markdown","4fe8aaea":"markdown","476db865":"markdown","c9e39200":"markdown","11a15ba6":"markdown","1eaafe6f":"markdown","73040880":"markdown","6abb65f9":"markdown","07934fa9":"markdown","6de19817":"markdown","345303c4":"markdown","bda6361c":"markdown","0fb7cb9c":"markdown","2ea7ca95":"markdown","e33b7c63":"markdown","2970f0f5":"markdown","a51bf24c":"markdown","09b476ad":"markdown","a878f9ee":"markdown","31be8e29":"markdown","2c3113e2":"markdown","f170cc6b":"markdown","322a3ebe":"markdown","2eed6d92":"markdown","383f66fa":"markdown","e1dd0d81":"markdown","aba4c6d9":"markdown","180dbf02":"markdown","b81819c1":"markdown","990fde37":"markdown","06396b06":"markdown","316e76fd":"markdown","1c067f80":"markdown","c7527293":"markdown","3a634309":"markdown","f83ba9cc":"markdown","6564480e":"markdown","70e0c97b":"markdown","371ddebe":"markdown","ca02e4ff":"markdown","9e2715ee":"markdown","990df60d":"markdown","b0ff8828":"markdown","5c4884b3":"markdown","4da81d70":"markdown","6bccadd8":"markdown","003083e8":"markdown","7132d8bb":"markdown","4df7ca11":"markdown","ad2df4bf":"markdown"},"source":{"53366111":"!python -m pip install matplotlib\n!python -m pip install nltk\n!python -m pip install numpy\n!python -m pip install pandas\n!python -m pip install seaborn\n!python -m pip install sklearn\n!python -m pip install tensorflow\n!python -m pip install transformers","6da2830f":"import re\n\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn import feature_extraction\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.metrics import (accuracy_score, confusion_matrix, precision_score,\n                             recall_score)\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom transformers import TFTrainer, TFTrainingArguments","1e3ee798":"nltk.download(\"punkt\")\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")","c7e4d508":"df = pd.read_csv(\n    \"..\/input\/sms-spam\/spamraw.csv\", encoding=\"latin-1\"\n)\ndf.head(n=10)","0f0abfc5":"df[\"type\"].replace({\"ham\": 0, \"spam\": 1}, inplace=True)\ndf.rename({\"type\": \"is_spam\", \"text\": \"content\"}, axis=1, inplace=True)\ndf.head(n=10)","15d5473e":"df.shape","86de9b0f":"df[\"nwords\"] = df[\"content\"].apply(lambda s: len(re.findall(r\"\\w+\", s)))\ndf[\"message_len\"] = df[\"content\"].apply(len)\ndf[\"nupperchars\"] = df[\"content\"].apply(\n    lambda s: sum(1 for c in s if c.isupper())\n)\ndf[\"nupperwords\"] = df[\"content\"].apply(\n    lambda s: len(re.findall(r\"\\b[A-Z][A-Z]+\\b\", s))\n)\ndf[\"is_free_or_win\"] = df[\"content\"].apply(\n    lambda s: int(\"free\" in s.lower() or \"win\" in s.lower())\n)\ndf[\"is_url\"] = df[\"content\"].apply(\n    lambda s: 1\n    if re.search(\n        r\"http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n        s,\n    )\n    else 0\n)\ndf.head(n=25)","09321e86":"n_sms = pd.value_counts(df[\"is_spam\"], sort=True)\nn_sms.plot(kind=\"pie\", labels=[\"ham\", \"spam\"], autopct=\"%1.0f%%\")\n\nplt.title(\"SMS Distribution\")\nplt.ylabel(\"\")\nplt.show()","578d61e6":"from collections import Counter\n\ndf1 = pd.DataFrame.from_dict(\n    Counter(\" \".join(df[df['is_spam'] == 0][\"content\"]).split()).most_common(20)\n)\ndf1 = df1.rename(columns={0: \"word_in_ham\", 1 : \"frequency\"})\n                 \ndf2 = pd.DataFrame.from_dict(\n    Counter(\" \".join(df[df['is_spam'] == 1][\"content\"]).split()).most_common(20)\n)\ndf2 = df2.rename(columns={0: \"word_in_spam\", 1 : \"frequency\"})","8422b6d4":"df1.plot.bar(legend=False)\nplt.xticks(np.arange(len(df1[\"word_in_ham\"])), df1[\"word_in_ham\"])\nplt.title(\"Word Frequency in Ham SMS.\")\nplt.xlabel(\"Words\")\nplt.ylabel(\"Frequency\")\nplt.show()","860ab581":"df2.plot.bar(legend=False, color=\"orange\")\nplt.xticks(np.arange(len(df2[\"word_in_spam\"])), df2[\"word_in_spam\"])\nplt.title(\"Word Frequency in Spam SMS.\")\nplt.xlabel(\"Word\")\nplt.ylabel(\"Frequency\")\nplt.show()","11221660":"_, ax = plt.subplots(figsize=(10, 4))\nsns.kdeplot(\n    df.loc[df.is_spam == 0, \"message_len\"],\n    shade=True,\n    label=\"Ham\",\n    clip=(-50, 250),\n)\nsns.kdeplot(df.loc[df.is_spam == 1, \"message_len\"], shade=True, label=\"Spam\")\nax.set(\n    xlabel=\"Length\",\n    ylabel=\"Density\",\n    title=\"Length of SMS.\",\n)\nax.legend(loc=\"upper right\")\nplt.show()","b4ebca26":"_, ax = plt.subplots(figsize=(10, 4))\nsns.kdeplot(\n    df.loc[df.is_spam == 0, \"nwords\"],\n    shade=True,\n    label=\"Ham\",\n    clip=(-10, 50),\n)\nsns.kdeplot(df.loc[df.is_spam == 1, \"nwords\"], shade=True, label=\"Spam\")\nax.set(\n    xlabel=\"Words\",\n    ylabel=\"Density\",\n    title=\"Number of Words in SMS.\",\n)\nax.legend(loc=\"upper right\")","09c3e526":"_, ax = plt.subplots(figsize=(10, 4))\nsns.kdeplot(\n    df.loc[df.is_spam == 0, \"nupperwords\"],\n    shade=True,\n    label=\"Ham\",\n    clip=(0, 35),\n)\nsns.kdeplot(df.loc[df.is_spam == 1, \"nupperwords\"], shade=True, label=\"Spam\")\nax.set(\n    xlabel=\"Uppercased Words\",\n    ylabel=\"Density\",\n    title=\"Number of Uppercased Words.\",\n)\nax.legend(loc=\"upper right\")\nplt.show()","c2f297a5":"_, ax = plt.subplots(figsize=(10, 5))\nax = sns.scatterplot(x=\"message_len\", y=\"nupperchars\", hue=\"is_spam\", data=df)\nax.set(\n    xlabel=\"Characters\",\n    ylabel=\"Uppercase Characters\",\n    title=\"Number of Uppercased Characters in SMS.\",\n)\nax.legend(loc=\"upper right\")\nplt.show()","2850af05":"_, ax = plt.subplots(figsize=(10, 4))\ngrouped_data = (\n    df.groupby(\"is_spam\")[\"is_free_or_win\"]\n    .value_counts(normalize=True)\n    .rename(\"Percentage of Group\")\n    .reset_index()\n)\nprint(grouped_data)\n\nsns.barplot(\n    x=\"is_spam\",\n    y=\"Percentage of Group\",\n    hue=\"is_free_or_win\",\n    data=grouped_data,\n)\nplt.show()","a915acb1":"_, ax = plt.subplots(figsize=(10, 4))\ngrouped_data = (\n    df.groupby(\"is_spam\")[\"is_url\"]\n    .value_counts(normalize=True)\n    .rename(\"Percentage of Group\")\n    .reset_index()\n)\nprint(grouped_data)\n\nsns.barplot(\n    x=\"is_spam\",\n    y=\"Percentage of Group\",\n    hue=\"is_url\",\n    data=grouped_data,\n)\nplt.show()","59993913":"from nltk import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\ndf[\"content\"] = df[\"content\"].apply(\n    lambda row: re.sub(r\"[^a-zA-Z]+\", \" \", row)  \n)\ndf[\"content\"] = df[\"content\"].apply(lambda row: word_tokenize(row))\ndf[\"content\"] = df[\"content\"].apply(\n    lambda row: [\n        token for token in row if token not in set(stopwords.words(\"english\"))\n    ]\n)\ndf[\"content\"] = df[\"content\"].apply(\n    lambda row: \" \".join([WordNetLemmatizer().lemmatize(word) for word in row])\n)\ndf.head(n=25)","1b704daf":"X_train, X_test, y_train, y_test = train_test_split(\n    df.drop(\"is_spam\", axis=1), df[\"is_spam\"], stratify=df[\"is_spam\"], test_size=0.2\n)","7a235da3":"print(f\"Training data: {len(X_train)} (80%)\")\nprint(f\" Testing data: {len(X_test)} (20%)\")","7feff237":"from transformers import BertTokenizerFast\n\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\ntokenizer","e87e832c":"max_len = 0\nfor row in X_train[\"content\"]:\n    max_len = max(max_len, len(tokenizer.encode(row)))\nprint(f\"Max sentence length (train): {max_len}\")\n\nmax_len = 0\nfor row in X_test[\"content\"]:\n    max_len = max(max_len, len(tokenizer.encode(row)))\nprint(f\"Max sentence length (test): {max_len}\")","a8f6f539":"train_encodings = tokenizer(\n    X_train[\"content\"].tolist(),\n    max_length=96,\n    padding=\"max_length\",\n    truncation=True,\n)\ntest_encodings = tokenizer(\n    X_test[\"content\"].tolist(),\n    max_length=96,\n    padding=\"max_length\",\n    truncation=True,\n)","b4198b25":"train_dataset = tf.data.Dataset.from_tensor_slices(\n    (dict(train_encodings), y_train)\n)\ntest_dataset = tf.data.Dataset.from_tensor_slices(\n    (dict(test_encodings), y_test)\n)","2ea13f8f":"training_args = TFTrainingArguments(\n    output_dir=\"\/kaggle\/working\/sms\/results\/bert\",\n    num_train_epochs=8,\n    per_device_train_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=\"\/kaggle\/working\/sms\/logs\/bert\",\n    logging_steps=10,\n)","88fcc18c":"from transformers import TFBertForSequenceClassification\n\nwith training_args.strategy.scope():\n    model = TFBertForSequenceClassification.from_pretrained(\n        \"bert-base-uncased\"\n    )\n\ntrainer = TFTrainer(\n    model=model, args=training_args, train_dataset=train_dataset\n)\ntrainer.train()","b1671f74":"trainer.save_model(\"\/kaggle\/working\/sms\/models\/bert\")\ntokenizer.save_pretrained(training_args.output_dir)","30e46994":"preds, label_ids, metrics = trainer.predict(test_dataset)\npreds[:5]","df308405":"print(f\"Test dataset size: {len(y_test)}\")\nprint(f\" Predictions size: {len(preds)}\")","bd60c162":"preds = preds[: len(y_test)]\nlen(preds)","610bbe25":"preds = np.argmax(preds, axis=1)\npreds","40c82e45":"plt.figure(figsize=(10, 4))\n\nheatmap = sns.heatmap(\n    data=pd.DataFrame(confusion_matrix(y_test, preds)),\n    annot=True,\n    fmt=\"d\",\n    cmap=sns.color_palette(\"Blues\", 50),\n)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14)\nheatmap.yaxis.set_ticklabels(\n    heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14\n)\n\nplt.title(\"Confusion Matrix\")\nplt.ylabel(\"Ground Truth\")\nplt.xlabel(\"Prediction\")","cfe32eaa":"print(f\"Precision: {precision_score(y_test, preds) * 100:.3f}%\")\nprint(f\"   Recall: {recall_score(y_test, preds) * 100 :.3f}%\")\nprint(f\" Accuracy: {accuracy_score(y_test, preds) * 100:.3f}%\")","aaf1cc58":"from transformers import DistilBertTokenizerFast\n\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ntokenizer","7e4d031e":"train_encodings = tokenizer(\n    X_train[\"content\"].tolist(),\n    max_length=96,\n    padding=\"max_length\",\n    truncation=True,\n)\ntest_encodings = tokenizer(\n    X_test[\"content\"].tolist(),\n    max_length=96,\n    padding=\"max_length\",\n    truncation=True,\n)","9a01a263":"train_dataset = tf.data.Dataset.from_tensor_slices(\n    (dict(train_encodings), y_train)\n)\ntest_dataset = tf.data.Dataset.from_tensor_slices(\n    (dict(test_encodings), y_test)\n)","a15eec6c":"training_args = TFTrainingArguments(\n    output_dir=\"\/kaggle\/working\/sms\/results\/distilbert\",\n    num_train_epochs=8,\n    per_device_train_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=\"\/kaggle\/working\/sms\/logs\/distilbert\",\n    logging_steps=10,\n)","aa715e38":"from transformers import TFDistilBertForSequenceClassification\n\nwith training_args.strategy.scope():\n    model = TFDistilBertForSequenceClassification.from_pretrained(\n        \"distilbert-base-uncased\"\n    )\n\ntrainer = TFTrainer(\n    model=model, args=training_args, train_dataset=train_dataset\n)\ntrainer.train()","5c609de5":"trainer.save_model(\"\/kaggle\/working\/sms\/models\/distilbert\")\ntokenizer.save_pretrained(training_args.output_dir)","5ee1b6a3":"preds, label_ids, metrics = trainer.predict(test_dataset)\npreds[:5]","dede3822":"print(f\"Test dataset size: {len(y_test)}\")\nprint(f\" Predictions size: {len(preds)}\")","a39954f9":"preds = preds[: len(y_test)]\nlen(preds)","3147643e":"preds = np.argmax(preds, axis=1)","4735bdc0":"plt.figure(figsize=(10, 4))\n\nheatmap = sns.heatmap(\n    data=pd.DataFrame(confusion_matrix(y_test, preds)),\n    annot=True,\n    fmt=\"d\",\n    cmap=sns.color_palette(\"Blues\", 50),\n)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14)\nheatmap.yaxis.set_ticklabels(\n    heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14\n)\n\nplt.title(\"Confusion Matrix\")\nplt.ylabel(\"Ground Truth\")\nplt.xlabel(\"Prediction\")","374f1409":"print(f\"Precision: {precision_score(y_test, preds) * 100:.3f}%\")\nprint(f\"   Recall: {recall_score(y_test, preds) * 100 :.3f}%\")\nprint(f\" Accuracy: {accuracy_score(y_test, preds) * 100:.3f}%\")","232d53e9":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = GridSearchCV(\n    Pipeline(\n        [\n            (\"bow\", CountVectorizer()),\n            (\"tfidf\", TfidfTransformer()),\n            (\"clf\", KNeighborsClassifier()),\n        ]\n    ),\n    {\n        \"clf__n_neighbors\": (8, 15, 20, 25, 40, 55),\n    }\n)\nknn.fit(X=X_train[\"content\"], y=y_train)","971180ce":"preds = knn.predict(X_test[\"content\"])\npreds","03c81f73":"plt.figure(figsize=(10, 4))\n\nheatmap = sns.heatmap(\n    data=pd.DataFrame(confusion_matrix(y_test, preds)),\n    annot=True,\n    fmt=\"d\",\n    cmap=sns.color_palette(\"Blues\", 50),\n)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14)\nheatmap.yaxis.set_ticklabels(\n    heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14\n)\n\nplt.title(\"Confusion Matrix\")\nplt.ylabel(\"Ground Truth\")\nplt.xlabel(\"Prediction\")","f54b52b9":"print(f\"Precision: {precision_score(y_test, preds) * 100:.3f}%\")\nprint(f\"   Recall: {recall_score(y_test, preds) * 100 :.3f}%\")\nprint(f\" Accuracy: {accuracy_score(y_test, preds) * 100:.3f}%\")","7453e234":"from sklearn.naive_bayes import MultinomialNB\n\nmnbayes = GridSearchCV(\n    Pipeline(\n        [\n            (\"bow\", CountVectorizer()),\n            (\"tfidf\", TfidfTransformer()),\n            (\"clf\", MultinomialNB()),\n        ]\n    ),\n    {\n        \"tfidf__use_idf\": (True, False),\n        \"clf__alpha\": (0.1, 1e-2, 1e-3),\n        \"clf__fit_prior\": (True, False),\n    },\n)\nmnbayes.fit(X=X_train[\"content\"], y=y_train)","dd4eff63":"mnbayes.best_params_","960b42a2":"print(f\"{mnbayes.best_score_ * 100:.3f}%\") ","902acf31":"preds = mnbayes.predict(X_test[\"content\"])\npreds","44bed4df":"plt.figure(figsize=(10, 4))\n\nheatmap = sns.heatmap(\n    data=pd.DataFrame(confusion_matrix(y_test, preds)),\n    annot=True,\n    fmt=\"d\",\n    cmap=sns.color_palette(\"Blues\", 50),\n)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14)\nheatmap.yaxis.set_ticklabels(\n    heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14\n)\n\nplt.title(\"Confusion Matrix\")\nplt.ylabel(\"Ground Truth\")\nplt.xlabel(\"Prediction\")","cd7f0740":"print(f\"Precision: {precision_score(y_test, preds) * 100:.3f}%\")\nprint(f\"   Recall: {recall_score(y_test, preds) * 100 :.3f}%\")\nprint(f\" Accuracy: {accuracy_score(y_test, preds) * 100:.3f}%\")","c5a928c6":"from sklearn.svm import SVC\n\nsvc = GridSearchCV(\n    Pipeline(\n        [\n            (\"bow\", CountVectorizer()),\n            (\"tfidf\", TfidfTransformer()),\n            (\"clf\", SVC(gamma=\"auto\", C=1000)),\n        ]\n    ),\n    dict(tfidf=[None, TfidfTransformer()], clf__C=[500, 1000, 1500]),\n)\nsvc.fit(X=X_train[\"content\"], y=y_train)","28741dbd":"svc.best_params_","40a293ac":"print(f\"{svc.best_score_ * 100:.3f}%\") ","d79ccfc7":"preds = svc.predict(X_test[\"content\"])\npreds","81087555":"plt.figure(figsize=(10, 4))\n\nheatmap = sns.heatmap(\n    data=pd.DataFrame(confusion_matrix(y_test, preds)),\n    annot=True,\n    fmt=\"d\",\n    cmap=sns.color_palette(\"Blues\", 50),\n)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14)\nheatmap.yaxis.set_ticklabels(\n    heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14\n)\n\nplt.title(\"Confusion Matrix\")\nplt.ylabel(\"Ground Truth\")\nplt.xlabel(\"Prediction\")","f954beba":"print(f\"Precision: {precision_score(y_test, preds) * 100:.3f}%\")\nprint(f\"   Recall: {recall_score(y_test, preds) * 100 :.3f}%\")\nprint(f\" Accuracy: {accuracy_score(y_test, preds) * 100:.3f}%\")","33f88bb7":"### Predictions","2f898d78":"Our DistilBERT model being trained, we can now use it to predict if the SMS in our test dataset are spams or not:","7b0f896d":"By Feature Engineering, we **refer to the creation of the features according to the raw data**. It is on the basis of these features that the training of the classification ML models will be done.\n\nAmong these features, we will create these:\n\n*   `nwords`: feature that will **contain the number of words** in an SMS.\n*   `message_len`: feature that will **contain the number of characters** in an SMS message.\n*   `nupperchars`: feature that will **contain the number of uppercase characters** in an SMS.\n*   `nupperwords`: feature that will **contain the number of uppercase words** in an SMS.\n*   `is_free_o_win`: feature that will **contain 1 if the SMS contains the words \"free\" and \"win\"; 0 otherwise**.\n*   `is_url`: feature that will **contain 1 if the SMS contains a URL; 0 otherwise**.\n\nThis translates as follows:","357c7e4c":"As seen previously, let's measure SMS predictions as spam or ham.","d326314e":"## Number of Words","4215d5a8":"Let's start by loading our dataset and looking at the columns available to us:","5c543c20":"Finally, to get a better idea on the amount of data made available, we can look at the shape of the DataFrame that defines the dataset:","abb7f3a0":"Let's look at the score obtained by the predictions:","d93daf9e":"## Number of Uppercased Words","fe5fb976":"Above, each row of the confusion matrix corresponds to a real class and each column corresponds to an estimated class.\n\nThrough the matrix of confusion, we have:\n\n*  **962 SMS being ham were well predicted**: True Negative (TN);\n*  **3 ham SMS have been detected as spam** False Negative (FN);\n*  **1 spam SMS have been detected as ham** False Positive (FP);\n*  **146 spam SMS have been detected as spam** True Positive (TP).","478fced2":"As we said before, let's use grid search techniques using cross-validation to determine the optimal value of the $\\alpha$ hyper-parameter of our model and train this model on this hyper-parameter:","9c9c1ccb":"BERT is a bidirectional transformer pretrained using a combination of Masked Language Modeling (MLM) objective and Next Sentence Prediction (NSP) on a large corpus comprising the Book Corpus and Wikipedia.","8fd9f744":"Our Multinomial Naive Bayes model being trained, we can now use it to predict if the SMS in our test dataset are spams or not:","6ff0348f":"Our dataset initially composed of 5559 lines is now split into two smaller datasets according to the following proportions:","31509eed":"Since this dataset has a lot of abbreviations, we will not apply stemming, but only lemmatization.\n\nAs a quick reminder:\n\n*   **Stemming**: NLP algorithm that **cuts the end or the beginning of a word** based on a list of common prefixes that can be found in an inflected word (e.g., `Stemming[change, changing, changes]` \u27a1\ufe0f chang).\n*   **Lemmatization**: NLP algorithm that **looks at the morphological analysis of words** based on detailed dictionaries, in order to relate the shape of a word to its lemma (e.g., `Lemmatization[change, changing, changes]` \u27a1\ufe0f change).\n\nAs a second pre-processing of these data, let's remove the stop words, punctuation and digits from each SMS, without forgetting to apply lemmatization to them:","1a30938d":"With this plot, we can notice two things:\n1.   there is **2.55% of spam** that contains a URL;\n2.   there is only **97.45% of spam** that doesn't contains a URL.","28021c56":"Here, we already have the final predictions given by the logit probabilities.","3ce82afc":"Following Fine-Tuning and our datasets, the training of the BERT model can be done as follows:","89935f23":"Let's also save our DistilBERT model and its configuration in a persistent way:","06544c5a":"With this plot, we can notice that there is a small pattern with the number of **uppercased words**. The **density is lower** which is normal due to the fact that there is **less spam** messages than **ham** messages.\n\nWe can also notice that the number of **uppercased words** is around **zero** for the **ham** messages.","e7f82326":"## Measurement of Predictions","ee318af1":"Using the confusion matrix, measures of the quality of the classification system are given: ","7b45410d":"For our training dataset, $C$ must be equal to 1000 and we shouldn't transform the count matrix to a normalized term-frequency (tf) representation or for a term-frequency times inverse document-frequency (tf-idf) representation.","bb30bbf9":"As with BERT, we will take a maximum length of 96 characters for both datasets to have a better speed for training and evaluation. \n\nBased on this pre-trained model, the encodings for our training and testing datasets are generated as follows:","243f0ac3":"## Word Frequency","40affebe":"### Scores","b83847ba":"As we said before, let's use grid search techniques using cross-validation to determine the hyper-parameters of our model and train this model on them:","6f7fbbf7":"**NOTE:** in a multi-class classification problem, these logits would be normalized with a `softmax` function.","92c474ba":"Let's see if a URL has an influence on SMS spam or ham:","5cdcc62c":"## SMS Distribution","2a288219":"Let's start by creating two DataFrames: \n\n1.   `df1`: will contain the words and their frequency in the SMS ham.\n2.   `df2`: will contain the words and their frequency in the SMS spam.\n","e3bb50e0":"Above, each row of the confusion matrix corresponds to a real class and each column corresponds to an estimated class.\n\nThrough the matrix of confusion, we have:\n\n*  **962 SMS being ham were well predicted**: True Negative (TN);\n*  **2 ham SMS have been detected as spam** False Negative (FN);\n*  **1 spam SMS have been detected as ham** False Positive (FP);\n*  **147 spam SMS have been detected as spam** True Positive (TP).","7437ed93":"Through the confusion matrix, we have:\n\n*   **959 SMS being ham were well predicted**: True Negative (TN);\n*   **12 ham SMS have been detected as spam**: False Negative (FN);\n*   **4 spam SMS have been detected as ham**: False Positive (FP);\n*   **137 spam SMS have been detected as spam**: True Positive (TP).","94ccb1b3":"# Introduction\n\nThis Notebook is **mainly for educational purposes**. According to a dataset to classify Short Message Service (SMS) as spam or not, the goal will be to evaluate Bidirectional Encoder Representations from Transformers (BERT) with other Machine Learning (ML) classification algorithms.\n\nAmong these algorithms, four ML classification algorithms will be compared:\n\n1.   **DistilBERT**;\n2.   **K-Nearest Neighbors (KNN)**;\n3.   **Multinomial Naive Bayes**;\n4.   **Support Vector Model (SVM)**.\n\nWe had the opportunity to test those algorithms with [another dataset](https:\/\/www.kaggle.com\/rememberyou\/comparison-of-bert-and-other-ml-classification-i) containing SMS samples as well.","a9371fec":"### Scores","257c9bc9":"Using the `pip` Python package manager, let's install all the necessary packages for this Notebook:","58c0bd7a":"As the features of our dataset have discrete frequency counts, we will use the Multinomial type of Naive Bayes Model.\n\nTo detect if a SMS is consider as spam or not, the Multinomial Naive Bayes classifier will use word counts in the content of the SMS with the help of the Bag-of-Words (BoW) method. This method, will elaborate a matrix of rows according to words, where each intersection corresponds to the frequency of occurrence of these words.","0b65da12":"Above, **87% of these SMS are ham and 13% of them are spam**.","78fa8187":"### Predictions","d1c1f9f3":"Before we can encode our datasets with BERT, **it is important to decide on a maximum sentence length for padding\/truncating to**. This will allow us to have a better speed for training and evaluation.\n\nTo do this, we will perform one tokenization pass of the datasets in order to measure the maximum sentence length:","ce938e44":"After loading this dataset, you can directly see some modifications to be made:\n\n*   **three** \"Unnamed\" **columns can be deleted**;\n*   **the spam column** (`v1`) a**nd the SMS content column** (`v2`) **can be renamed** to be more explicit;\n*  **the content of the spam column** (`v1`) **can be binarized** for better processing ease for ML algorithms.\n\n","08692aa6":"In ML, preprocessing data is a process of preparing raw data to make them suitable to a ML model. \n\nFrom a semantic point of view, our dataset has some drawbacks for a ML model:\n\n*   **presence of stop words** (e.g., so, is, a);\n*   **presence of punctuations and digits**;\n*   **words are not lemmatized.**","66ba9153":"Let's look at the score obtained by the predictions.\n\nAs a quick reminder:\n\n1.   **Precision:** is the ratio between the True Positives and all the Positives.\n2.   **Recall:** is the measure of our model correctly identifying True Positives.\n3.   **Accuracy:** is the ratio of the total number of correct predictions and the total number of predictions.\n\nWhich gives us:","1a1a8c2b":"# DistilBERT\n","d59884ec":"The necessary packages being installed, let's already import most of the packages for this Notebook:","1226047e":"In order to have a better ease of use, a first pre-processing of these data would be to apply the modifications mentioned above:","74cfc6ca":"# Conclusion","1b36f48a":"Here it is the case, we have 8 additional embeddings. Let's make sure to delete them:","42ffe9cb":"Out of curiosity, let us look at which were the hyper-parameters to be privileged for the training of the model with respect to our training dataset:","6a789f3e":"## Measurement of Predictions","b21074f2":"The reason for these extra embeddings after training is due to a `huggingface\/transformers` bug, which should be fixed in the next releases.","18ef80fe":"Now that the DataFrames have been created, let's sketch their corresponding graphs in order to look at their respective word frequencies:","22161b1a":"### Scores","4fec2899":"Now that we know a bit more about the organization of the dataset, it is good to know the percentage of spam SMS and ham SMS:","46e91796":"# Analyze and Understanding Data","df609f7b":"Above all, it is **important to analyze and understand the data** made available. Indeed, once a better understanding of the dataset is achieved, **we will be able to create the necessary features** for the dataset.\n\nThe dataset being loaded, we will analyze the following eight aspects:\n\n1.   **the SMS distribution**;\n2.   **the word frequency in spam and ham SMS**;\n3.   **the length of spam SMS compared to ham SMS**;\n4.   **the number of words in spam SMS compared to ham SMS**;\n5.   **the number of uppercase words in spam SMS compared to ham SMS**;\n6.   **the number of uppercase characters in spam SMS compared to ham SMS**;\n7.   **the content of the words \"free\" or \"win\" in the SMS**;\n8.   **the content of a URL in the SMS**.","00f6597d":"Here it is the case, we have 8 additional embeddings. Let's make sure to delete them:","c25ce2bc":"Following Fine-Tuning and our datasets, the training of the BERT model can be done as follows:","a6e009ef":"From this Notebook, we started by loading a dataset of Spam SMS and created our features on the raw data using Feature Engineering. \n\nOnce our features were created, we analyzed the data made available on the basis of these features, before being able to do data preprocessing which consisted in removing the presence of stop words, punctuation, digits and lemmatize the words.\n\nIn addition, we learned how to fine-tuning different Machine Learning classification algorithms. To do this, it was useful for fine-tuning some of these algorithms to use search grid techniques using cross-validation to evaluate the performance of the model.\n\nBERT and DistilBERT are to be preferred when we would like to push performance to its maximum and to optimize the avoidance of True Positive misclassification (given by the recall score).\n\nHowever, even these algorithms are the best according to the scores, we can still apply Okhalm's razor principle. Indeed, if these few percent more can be neglected, classical classification algorithms such as Multinomial Naive Bayes and SVM can still be preferred because of their simplicity of understanding and implementation.","4b5da5bf":"Similar to what we saw before, let's associate these codings to a `TensorSliceDataset` object in order to Fine-Tuning and train our model.","21657b41":"Our BERT model being trained, we can now use it to predict if the SMS in our test dataset are spams or not:","4495b05f":"## Fine-Tuning and Training","461992b3":"Here, we already have the final predictions given by the logit probabilities.","ec6e8bd5":"## Fine-Tuning and Training","e27e7a91":"With this plot we can notice two things:\n1.   **Spam** messages are clustered together based on their length. But we can also see that some **spam** messages have more uppercased characters that others.\n2.   There is a linear pattern for **ham** messages that contains more uppercased character than others. \n\n","3618ac28":"The mean cross-validated score is therefore 98.246%","a0c78b96":"With the NLTK's data downloader, we will install the following corpora and trained models:\n*   `punkt`: Punkt Tokenizer Models.\n*   `stopwords`: Stopwords Corpus.\n*   `wordnet`: WordNet-InfoContent.","7e83efd6":"### Confusion Matrix","b4ee57a3":"Here, we already have the final predictions given by the logit probabilities.","230cc92a":"Let's look at the score obtained by the predictions:","bd2426f4":"The mean cross-validated score is therefore 98.516%","3bdef1ea":"## Measurement of Predictions","93bfd0ea":"K-Nearest Neighbors (KNN) is an approach to data classification that estimates how likely a data point is to be a member of one group or the other depending on what group the data points nearest to it are in.","7955c0c3":"If we summarize the results obtained, here is what we get:","26d7e052":"Using the confusion matrix, measures of the quality of the classification system are given:","f9e79cbb":"We can see that the dataset has become much clearer.","056a0c98":"Sketch the confusion matrix will allow us to measures the quality of the classification system:","d7b60e0d":"Let's see if the number of uppercased characters has an influence on SMS spam or ham:","c121d88e":"We can see that the columns corresponding to our features have been created.","418f2a8b":"Following Fine-Tuning and our datasets, the training of the DistilBERT model can be done as follows:","bcc3034b":"Let's see if the number of words has an influence on SMS spam or ham:","81312cae":"Let's look at the score obtained by the predictions:","9e5e7b63":"As we said before, let's use grid search techniques using cross-validation to determine the hyper-parameters of our model and train this model on them:\n\nTo tune the hyper-parameters of the KNN, it is recommended to use grid search techniques using cross-validation (**SEE:** [scikit-learn's documentation](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html#cross-validation)) to evaluate the performance of the model on the data at each value.\n\nLet's use this technique to train our model according to the optimal value of the neighbors hyper-parameter:","f47856e7":"To get rid of these logits, the vector of raw (non-normalized) predictions generated by the classification model should be passed to a normalization function to convert logits to probabilities. As we use a binary classification, we should use the `sigmoid` function and then the conversion of the probabilities into final predictions is done by taking the label for which the probability is highest.\n\nWith the help of the `argmax` function from `numpy`, we can make a two-shot stone:","3c3db089":"Let's see if the number of uppercased words has an influence on SMS spam or ham:","e2ef460e":"### Normalization","35bc6599":"# Creation of Training and Testing Datasets","26a38c9b":"Tokenization will allow us to feed batches of sequences into the model at the same time, only if these two conditions are met:\n\n1.   **the SMS are padded to the same length**;\n2.   **the SMS are truncated to be not longer model's maximum input length**.\n\nTo do the tokenization of our datasets, we also need to choose a pre-trained model. \nFor this dataset, the basic model (`bert-base-uncased`) will be sufficient:\n","ea38288d":"### Confusion Matrix","43610b30":"**NOTE:** you can ignore the warnings.","c87e7cab":"## Contains \"free\" or \"win\"","65ddf0b1":"**NOTE:** you can ignore the warnings.","f91bba12":"## Fine-Tuning and Training","806e8597":"### Normalization","5d8e6113":"Let's look at the score obtained by the predictions:","b4c21ea9":"This model being trained, let's save it, as well as its configuration to be able to load it directly when needed.","00c5264c":"### Scores","5b948805":"## Loading","f7c04288":"## Measurement of Predictions","205063fa":"[BERT Fine-Tuning Tutorial with PyTorch](http:\/\/mccormickml.com\/2019\/07\/22\/BERT-fine-tuning\/)\n\n[Naive Bayes & SVM Spam Filtering](https:\/\/www.kaggle.com\/pablovargas\/naive-bayes-svm-spam-filtering)\n\n[Starter: Neural Net w\/ 0.97 ROC-AUC - 99% accuracy](https:\/\/www.kaggle.com\/mrlucasfischer\/starter-neural-net-w-0-97-roc-auc-99-accuracy)","d9de6f4f":"### Predictions","87e7e4ae":"As seen previously, let's measure SMS predictions as spam or ham.","8ca74021":"### Predictions","2e01cb04":"# BERT","744d75e0":"### Confusion Matrix","577edd26":"Since we have that the maximum length sentence is 92 for the training dataset and 93 for the testing dataset, **we will take a maximum length of 96 characters for both datasets**.","481665b8":"## Transformation of Labels and Encodings","02ae84cf":"We are now ready to Fine-Tuning and training our BERT model!","37f80d85":"With DistilBERT, we also have logits.","0cecc0e1":"### Confusion Matrix","8c812bca":"Based on this pre-trained model, the encodings for our training and testing  datasets are generated as follows:","c563fb54":"With this plot, we notice two things:\n1.   In general, **spam** messages are **longer** than **ham** messages (that is normal due to the number of words).\n2.   **Spam** messages have around **150 characters**.\n\n","badeeba7":"Let's see if the length has an influence on SMS spam or ham:","1377ea93":"Through the confusion matrix, we have:\n\n*   **962 SMS being ham were well predicted**: True Negative (TN);\n*   **59 ham SMS have been detected as spam**: False Negative (FN);\n*   **1 spam SMS have been detected as ham**: False Positive (FP);\n*   **90 spam SMS have been detected as spam**: True Positive (TP).","ffec378c":"# Objectives\n\nTo compare these algorithms, we will:\n\n*   **Do Feature Engineering**: create the features according to the raw data.\n*   **Analyze and understand the data** made available.\n*   **Pre-process these data according to the algorithm**: for instance, some of these algorithms only work with numerical values.\n*   **Do Fine-Tuning**: optimize the training parameters of the ML algorithm.\n*   **Compare the results** obtained.\n*   **Apply the Ockham's razor principle**: take the best and\/or simplest algorithm if there is no significant difference.","acc5f7ef":"## Contains URL","d82e2093":"## Measurement of Predictions","4fe8aaea":"# Dataset","476db865":"## Preprocessing","c9e39200":"Before we can Fine-Tuning and training our model, we must batched these encodings to a `TensorSliceDataset` object, so that each key in the batch encoding corresponds to a hyper-parameters named according to the model we are going to train:","11a15ba6":"We can see that BERT and DistilBERT are the ML classification algorithms that provide the best results. However, based on the scores, we can see that there is no significant difference in precision and accuracy between these algorithms, it is only a few percent!","1eaafe6f":"It should be noted above that we have what are called **logits** (i.e. \t$\\exists n, n \\in ]-\\infty, \\infty[$).\n\nAnother thing we have to be careful of, is that no additional embeddings have been generated after the predictions:","73040880":"We are now ready to fine-tuning and training our DistilBERT model!","6abb65f9":"The measurement of SMS predictions present in our test dataset as spam or ham, will allow us to make sure that the model is well trained.","07934fa9":"## Number of Uppercased Characters","6de19817":"DistilBERT is a distilled version of BERT, which is smaller, faster, cheaper and lighter. This variant should have performance close to BERT.","345303c4":"Let's see if the \"free\" and \"win\" words has an influence on SMS spam or ham:","bda6361c":"# References","0fb7cb9c":"Before being able to train our model, it is necessary to split our dataset into a training and testing dataset:","2ea7ca95":"It would still be possible to speculate on more pre-processing to be done (e.g., finding the original words based on abbreviations), but since a SMS is not a formal message, it may be wise to keep capital letters and abbreviations.","e33b7c63":"# SVM","2970f0f5":"### Predictions","a51bf24c":"So we have a **dataset that contains 5559 rows and 2 columns**.","09b476ad":"| ML Algo                  |                                                                  Accuracy |  Precision |      Recall |\n| -----------------------: | ------------------------------------------------------------------------: | ---------: | ----------: |\n| BERT                     |                                                                   99.640  |     99.320 |      97.987 |\n| DistilBERT               |                                                                 **99.730**| **99.324** |  **98.658** |\n| KNN                      |                                                                   94.604  |     98.901 |      60.403 |\n| Multinomial Naive Bayes  |                                                                   98.561  |     97.163 |      91.946 |\n| SVM                      |                                                                   97.572  |     96.212 |      85.235 |","a878f9ee":"# Feature Engineering","31be8e29":"**NOTE:** in some use-cases, it can be interesting to split again the training dataset in order to create a validation dataset.  The validation dataset could be useful when we want to stop training a model when a certain precision is reached, to avoid overlearning. In our case, it may be preferable to use the training data set to train the model and achieve better accuracy.","2c3113e2":"Through the confusion matrix, we have:\n\n*   **958 SMS being ham were well predicted**: True Negative (TN);\n*   **22 ham SMS have been detected as spam**: False Negative (FN);\n*   **5 spam SMS have been detected as ham**: False Positive (FP);\n*   **127 spam SMS have been detected as spam**: True Positive (TP).","f170cc6b":"Let's convert these logits into probabilities and the latter into final predictions by taking the label for which the probability is highest:","322a3ebe":"# Preprocessing Data","2eed6d92":"## Tokenization","383f66fa":"Let's see if we also have additional embeddings:","e1dd0d81":"# KNN","aba4c6d9":"## Transformation of Labels and Encodings","180dbf02":"### Scores","b81819c1":"# Results","990fde37":"After sketching, we can see that stop words are the most frequent words in both spam and ham SMS.","06396b06":"# Installing and Importing Packages\n","316e76fd":"# Multinomial Naive Bayes Classifier","1c067f80":"## Tokenization","c7527293":"## Length","3a634309":"Our KNN model being trained, we can now use it to predict if the SMS in our test dataset are spams or not:","f83ba9cc":"Our SVM model being trained, we can now use it to predict if the SMS in our test dataset are spams or not:","6564480e":"With this plot, we can notice that **spam** SMS have more words than **ham** SMS.\n\n**Spam** SMS seem to have around **30 words**, where **ham** SMS seem to have around **10 words** to **25 words** and more.","70e0c97b":"## Fine-Tuning and Training","371ddebe":"**Fine-tuning consists of generating embeddings specific to a task**. Since we would like to create embeddings specifically for a classification task, we will have to train our data only for this task. However, for a pre-trained BERT model that is best suited for multiple tasks, fine-tuning will not be possible. It will therefore be necessary to generate the BERT embeddings as features and pass them through an independent classifier (e.g., RandomForest).\n\nUsing the `TFTrainingArguments` class present in the `huggingface\/transformers` module, the Fine-Tuning can be done this way:","ca02e4ff":"Using the confusion matrix, measures of the quality of the classification system are given:","9e2715ee":"As for BERT, let's tokenize our dataset so that we can feed batches of sequences into the model at the same time.\n\nFor this dataset, the basic model (`distilbert-base-uncased`) will be sufficient:","990df60d":"## Fine-Tuning and Training","b0ff8828":"For this Notebook, the [SMS Spam Collection Dataset](https:\/\/www.kaggle.com\/vivekchutke\/spam-ham-sms-dataset) dataset has been chosen. The main reasons for such a choice is that it contains a lot of data, and its columns are suitable for the comparison of the ML algorithms we want to make.\n\nIn this section we will load the dataset and apply minor preprocessing to the columns and values to make it easier to use.","5c4884b3":"With this plot, we can notice two things:\n1.   There is **36.94% of spam** SMS that contains the words **\"free\"** or **\"win\"**.\n2.   There is only **2.69% of ham** SMS that contains the words **\"free\"** or **\"win\"**.","4da81d70":"For our training dataset, $\\alpha$ must be equal to $10^{-2}$.","6bccadd8":"Using the confusion matrix, measures of the quality of the classification system are given:","003083e8":"### Confusion Matrix","7132d8bb":"The fine-tuning for DistilBERT is identical to BERT:","4df7ca11":"In addition, we can get the mean cross-validated score of the estimator that was chosen by the search:","ad2df4bf":"Out of curiosity, let us look at which were the hyper-parameters to be privileged for the training of the model with respect to our training dataset:"}}