{"cell_type":{"1403d263":"code","08a0fa57":"code","528e6915":"code","f3b136f6":"code","b7ebe8e1":"code","ab986fdc":"code","b19ef9d6":"code","63992b7f":"code","c0a63692":"code","686179bd":"code","bb09659a":"code","0f45649b":"code","21b64d49":"code","db57e7ad":"code","fda4dfef":"code","6ff93373":"code","73107269":"code","79688e9a":"code","bc2fb8cf":"code","c44e706f":"code","0203f5c0":"markdown","3495df7f":"markdown","e2913596":"markdown","eaf031bd":"markdown","474f37b1":"markdown","307dbf9c":"markdown","1213f19f":"markdown","b64fc970":"markdown"},"source":{"1403d263":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nimport keras\nimport cv2\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.metrics import categorical_crossentropy\nfrom keras.optimizers import Adam\nfrom keras import models\nfrom keras.layers.core import Dense , Flatten , Dropout \nfrom keras.models import Sequential\nfrom keras.layers import Activation , Conv2D , MaxPooling2D, Lambda, ZeroPadding2D, Convolution2D\nfrom keras.preprocessing.image import ImageDataGenerator\n# from keras import backend as K\n\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames[:2]:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","08a0fa57":"submission_names = '\/kaggle\/input\/aptos2019-blindness-detection\/sample_submission.csv'\ntrain_names = \"\/kaggle\/input\/aptos2019-blindness-detection\/train.csv\"\ntest_names = \"\/kaggle\/input\/aptos2019-blindness-detection\/test.csv\"\n\ntest_images_path = \"\/kaggle\/input\/aptos2019-blindness-detection\/test_images\/\"\ntrain_images_path = \"\/kaggle\/input\/aptos2019-blindness-detection\/train_images\/\"\n\ntrain_data = pd.read_csv(train_names)\ntrain_data.head()","528e6915":"trainX , testX , trainY , testY = train_test_split(train_data['id_code'], train_data['diagnosis'], test_size=0.2)","f3b136f6":"IMAGE_SHAPE = (120, 120, 3)\nIMAGE_SIZE = (120, 120)\n\ndef preprocess_image(image):\n    \n    image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)\n    \n    height, width, _ = image.shape\n    center_x = int(width \/ 2)\n    center_y = int(height \/ 2)\n    radius = min(center_x, center_y)\n    \n    circle_mask = np.zeros((height, width), np.uint8)\n    cv2.circle(circle_mask, (center_x, center_y), radius, color=1, thickness=-1)\n    image = cv2.resize(cv2.bitwise_and(image, image, mask=circle_mask)[center_y - radius:center_y + radius, center_x - radius:center_x + radius], IMAGE_SIZE)\n    return image","b7ebe8e1":"def getting_image_from_path(img_id, common_path):\n    img_url = common_path + str(img_id + \".png\")\n    img = cv2.imread(img_url) # 1 = color && 0 = black&white\n    imgs =  preprocess_image(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n#     imgs = cv2.resize(img, (260, 260)) # (height, width) pixels for img resolution try 480, 640\n    return imgs\n\ndef data_maker(img_ids , labels, common_train_test_path, size):\n    img_lst = []\n    for i in img_ids[:int(size)]:\n        image = getting_image_from_path(str(i), common_train_test_path)\n        img_lst.append(image)\n        \n    return np.array(img_lst), np.array(labels[:int(size)])","ab986fdc":"train_imgs, train_labels = data_maker(trainX, trainY, train_images_path, len(trainY))\ntest_imgs, test_labels = data_maker(testX, testY, train_images_path, len(testY))","b19ef9d6":"# Plotting the Images Graph\n\nfor i in range(10):\n        if (i % 5) == 0:\n            fig, ax = plt.subplots(1,5,figsize=(25,5)) \n        else:\n            k = i\n            for j in range(5):\n                ax[j].imshow(train_imgs[k])\n                k = k + 1\n                \nprint(train_labels[:10])","63992b7f":"def one_hot_encoding(list_of_target):\n    one_hot_lst = []\n    lst = pd.get_dummies(np.unique(list_of_target))\n#     print(lst)\n    for i in list_of_target:\n        a = np.array(lst[i]).tolist()\n        one_hot_lst.append(a)\n    return np.array(one_hot_lst) , len(lst)\n\n# print(one_hot_encoding(train_labels))","c0a63692":"def saving_model(model_instance,model_name):\n    model_instance.save(str(model_name))\n    print(\"Model Saved\")\n    \ndef loading_model(model_name):\n    model = models.load_model(str(model_name))\n    print(\"Model Loaded\")\n    return model","686179bd":"SHAPE = (120, 120,3)\n###################################\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape= SHAPE))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(5, activation='softmax'))\n\n####################################\nmodel.summary()","bb09659a":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # try optimizer sgd with vgg16","0f45649b":"datagen = ImageDataGenerator(\n        featurewise_center=True,\n        featurewise_std_normalization=True,\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        horizontal_flip=True)\n\ndatagen.fit(train_imgs)\nhistory = model.fit_generator(datagen.flow(train_imgs, one_hot_encoding(train_labels)[0], batch_size=32),\n                        steps_per_epoch=len(train_imgs) \/ 32,\n                        validation_data=datagen.flow(test_imgs, one_hot_encoding(test_labels)[0], batch_size=32),\n                        validation_steps=len(test_imgs) \/ 32,\n                        epochs=100)","21b64d49":"print(history.history.keys())","db57e7ad":"# Accuracy Graph\nplt.plot(history.epoch, history.history['acc'] , label=\"acc\")\nplt.plot(history.epoch, history.history['val_acc'] , label = \"val_acc\")\nplt.legend()\nplt.show()","fda4dfef":"# Loss Graph\nplt.plot(history.epoch, history.history['loss'] , label = \"loss\")\nplt.plot(history.epoch, history.history['val_loss'] , label = \"val_loss\")\nplt.legend()\nplt.show()","6ff93373":"# Now Making the Prediction and Evaluating the model\n\nscore = model.evaluate(test_imgs, one_hot_encoding(test_labels)[0], verbose = 0)\nprint(\"%s: %.2f%%\" % (\"acc\", score[1]*100))","73107269":"sub_file = pd.read_csv(submission_names)\nsub_file.head()","79688e9a":"length = len(sub_file['diagnosis'])\n#####################\n\nval = data_maker(sub_file[\"id_code\"], sub_file['diagnosis'], test_images_path, length)[0]\npred = model.predict_classes(val)\nprint(pred[:10])","bc2fb8cf":"df = pd.DataFrame(data = {\"id_code\": sub_file[\"id_code\"], \"diagnosis\": pred })\ndf.to_csv(\"submission.csv\", sep = \",\", index = False)\nprint(\"Done\")","c44e706f":"# Can do TTA (test time Augmentation) and blending of model for better accuracy","0203f5c0":"## Image Preprocessing","3495df7f":"### Predicting the model and Saving in CSV","e2913596":"## Making the model","eaf031bd":"## One Hot Encoding","474f37b1":"## Code for saving & loading back the training model****","307dbf9c":"## Importing Libraries","1213f19f":"## Visualization","b64fc970":"1. 1. ## Plotting accuracy & loss graph"}}