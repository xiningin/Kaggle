{"cell_type":{"22cbd276":"code","3dc640b0":"code","c5598e73":"code","fb0dd42d":"code","5452f812":"code","44232141":"code","f6e3a00b":"code","461cc1f8":"code","2ad405f4":"code","9459383c":"code","61e40c99":"code","92e61762":"markdown","33851bda":"markdown","7e5567ea":"markdown","323d6c68":"markdown","7e92c07c":"markdown","1c464030":"markdown","bf00c0ea":"markdown","e14465d6":"markdown","84f84353":"markdown","43388bba":"markdown","e793bb2d":"markdown","74d89e28":"markdown","56ca1802":"markdown","d09ff1b0":"markdown","8eb318ed":"markdown","826637fc":"markdown","521f1ef1":"markdown","df54e4ab":"markdown"},"source":{"22cbd276":"# Check the versions of libraries\n# Python version\nimport sys\nprint('Python: {}'.format(sys.version))\n# scipy\nimport scipy\nprint('scipy: {}'.format(scipy.__version__))\n# numpy\nimport numpy\nprint('numpy: {}'.format(numpy.__version__))\n# matplotlib\nimport matplotlib\nprint('matplotlib: {}'.format(matplotlib.__version__))\n# pandas\nimport pandas\nprint('pandas: {}'.format(pandas.__version__))\n# scikit-learn\nimport sklearn\nprint('sklearn: {}'.format(sklearn.__version__))","3dc640b0":"import pandas \nfrom pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","c5598e73":"#url = \"https:\/\/raw.githubusercontent.com\/jbrownlee\/Datasets\/master\/iris.csv\"\n#names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n#dataset = pandas.read_csv(url, names=names)\ndata='..\/input\/iris-flower-dataset\/IRIS.csv'\n#reading dataset\ndataset=pandas.read_csv(data)","fb0dd42d":"print('shape:')\nprint(dataset.shape)    #outputs shape of the dataset\nprint('data:')\nprint(dataset.head())    #prints top 5 data points\nprint('description:')\nprint(dataset.describe())    #computs mean ,std ,min,max etc\nprint('number of examples:')\nprint(dataset.groupby('species').size())   #class distribution","5452f812":"#plotting box plot\ndataset.plot(kind='box',subplots=True, layout=(2,2),sharex=False, sharey=False)\nplt.show()\n","44232141":"#histogram\ndataset.hist()\nplt.show()","f6e3a00b":"# scatter plot matrix\nscatter_matrix(dataset)\nplt.show()","461cc1f8":"array=dataset.values\nX=array[:,0:4]\nY=array[:,4]\nvalidation_size=0.2\nseed=1\nX_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)\n","2ad405f4":"seed=1\nscoring='accuracy'","9459383c":"# Spot Check Algorithms\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","61e40c99":"# Make predictions on validation dataset\nknn = KNeighborsClassifier()\nknn.fit(X_train, Y_train)\npredictions = knn.predict(X_validation)\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))","92e61762":"****Make Predictions","33851bda":"Loading required libraries...","7e5567ea":"We don\u2019t know which algorithms would be good on this problem or what configurations to use. We get an idea from the plots that some of the classes are partially linearly separable in some dimensions, so we are expecting generally good results.\n\nLet\u2019s evaluate 6 different algorithms:\n\n    Logistic Regression (LR)\n    Linear Discriminant Analysis (LDA)\n    K-Nearest Neighbors (KNN).\n    Classification and Regression Trees (CART).\n    Gaussian Naive Bayes (NB).\n    Support Vector Machines (SVM).\n","323d6c68":"We will use 10-fold cross validation to estimate accuracy.\n\nThis will split our dataset into 10 parts, train on 9 and test on 1 and repeat for all combinations of train-test splits.","7e92c07c":"****Let\u2019s build and evaluate our models:","1c464030":"****Start Python and Check Versions","bf00c0ea":"****Build Models","e14465d6":"Books and courses are frustrating. They give you lots of recipes and snippets, but you never get to see how they all fit together.\n\nWhen you are applying machine learning to your own datasets, you are working on a project.\n\nA machine learning project may not be linear, but it has a number of well known steps:\n\n    Define Problem.\n    Prepare Data.\n    Evaluate Algorithms.\n    Improve Results.\n    Present Results.\n\nThe best way to really come to terms with a new platform or tool is to work through a machine learning project end-to-end and cover the key steps. Namely, from loading data, summarizing data, evaluating algorithms and making some predictions.\n\nIf you can do that, you have a template that you can use on dataset after dataset. You can fill in the gaps such as further data preparation and improving result tasks later, once you have more confidence.","84f84353":"Create a Validation Dataset\nLater, we will use statistical methods to estimate the accuracy of the models that we create on unseen data. We also want a more concrete estimate of the accuracy of the best model on unseen data by evaluating it on actual unseen data.\n\nThat is, we are going to hold back some data that the algorithms will not get to see and we will use this data to get a second and independent idea of how accurate the best model might actually be.\n\nWe will split the loaded dataset into two, 80% of which we will use to train our models and 20% that we will hold back as a validation dataset.","43388bba":"****Summary","e793bb2d":"****Hello World of Machine Learning\nThe best small project to start with on a new tool is the classification of iris flowers (e.g. the iris dataset).\n\nThis is a good project because it is so well understood.\n\n    Attributes are numeric so you have to figure out how to load and handle data.\n    It is a classification problem, allowing you to practice with perhaps an easier type of supervised learning algorithm.\n    It is a multi-class classification problem (multi-nominal) that may require some specialized handling.\n    It only has 4 attributes and 150 rows, meaning it is small and easily fits into memory (and a screen or A4 page).\n    All of the numeric attributes are in the same units and the same scale, not requiring any special scaling or transforms to get started.\n\nLet\u2019s get started with your hello world machine learning project in Python.","74d89e28":"****Beginners Need A Small End-to-End Project","56ca1802":"About the dataset","d09ff1b0":"Loading the Iris dataset","8eb318ed":"****Data Visualization","826637fc":"We can get a quick idea of how many instances (rows) and how many attributes (columns) the data contains with the shape property.","521f1ef1":"We did not cover all of the steps in a machine learning project because this is your first project and we need to focus on the key steps. Namely, loading data, looking at the data, evaluating some algorithms and making some predictions","df54e4ab":"We are using the metric of \u2018accuracy\u2018 to evaluate models. This is a ratio of the number of correctly predicted instances in divided by the total number of instances in the dataset multiplied by 100 to give a percentage (e.g. 95% accurate). We will be using the scoring variable when we run build and evaluate each model next."}}