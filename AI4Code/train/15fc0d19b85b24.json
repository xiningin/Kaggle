{"cell_type":{"a3182066":"code","577dea94":"code","87acfbee":"code","ccd7ba39":"code","c94f52f2":"code","3291f968":"code","72b38a46":"code","ab14df0c":"code","adfb140c":"code","22b5ce93":"code","9536dbf3":"markdown","55507003":"markdown","8ac4943b":"markdown","aa762cbf":"markdown","da7b726b":"markdown","619a3071":"markdown","61cd2287":"markdown"},"source":{"a3182066":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error # MSE metric\nfrom sklearn.model_selection import KFold\nfrom catboost import CatBoostRegressor\nfrom catboost import Pool\nimport shap as shap\n\nSEED = 91 # random seed","577dea94":"PATH = '\/kaggle\/input\/30-days-of-ml\/'\n\nprint('Files in directory:')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print('  '+os.path.join(dirname, filename))\nprint()\n\n# Load the training data\ntry:\n    df_train = pd.read_csv(PATH+'train.csv', index_col=0)\n    df_test = pd.read_csv(PATH+'test.csv', index_col=0)\n    submission = pd.read_csv(PATH+'sample_submission.csv', index_col=0)\n    print('All of the data has been loaded successfully!')\nexcept Exception as err:\n    print(repr(err))\nprint()","87acfbee":"# Train amount of data\nlen(df_train)","ccd7ba39":"# Test amount of data\nlen(df_test)","c94f52f2":"CAT_FEATURES = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9']\nNUM_FEATURES = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n                'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\nALL_FEATURES = CAT_FEATURES+NUM_FEATURES","3291f968":"X = df_train.copy()\ny = X.pop('target')","72b38a46":"catboost_params = {\n    'depth': 3,\n    'cat_features': CAT_FEATURES,\n    'iterations': 5000,\n    'early_stopping_rounds': 20,\n    'l2_leaf_reg': 10.87,\n    'bootstrap_type': 'Bernoulli',\n    'leaf_estimation_method': 'Newton',\n    'random_strength': 0,\n    'silent': True,\n    'thread_count': 4,\n    'random_seed': SEED\n}","ab14df0c":"N_FOLDS = 5\n\nmodel_fi = 0\nval_rmse = []\n\n# CV starts here.\nfor fold, (train_idx, valid_idx) in enumerate(KFold(n_splits=N_FOLDS, random_state=SEED, shuffle=True).split(X)):\n    # split the train data into train and validation\n    X_train = X.iloc[train_idx]\n    X_valid = X.iloc[valid_idx]\n    y_train = y.iloc[train_idx]\n    y_valid = y.iloc[valid_idx]\n\n    # Train model\n    model = CatBoostRegressor(**catboost_params)\n    model.fit(X_train, y_train,\n             eval_set=(X_valid, y_valid))\n    \n    # Predict on valid data\n    predictions = model.predict(X_valid)\n\n    # Mean of feature importance\n    model_fi += model.feature_importances_ \/ N_FOLDS \n    \n    # RMSE\n    rmse = mean_squared_error(y_valid, predictions, squared=False)\n    val_rmse.append(rmse)\n    print(f'Fold {fold} | RMSE: {rmse:.6f}')\n\nprint(f'\\nFull CrossVal RMSE: {np.mean(val_rmse):.6f} (std: {np.std(val_rmse):.6f})')","adfb140c":"feature_importance_df = pd.DataFrame(model_fi, index=X.columns)\nfeature_importance_df.sort_values(by=0, ascending=False)","22b5ce93":"train_data = Pool(data=X,\n                  label=y,\n                  cat_features=CAT_FEATURES\n                 )\n                 \nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(train_data)\nshap.summary_plot(shap_values, X, feature_names=ALL_FEATURES)","9536dbf3":"# Import libraries and load data","55507003":"# Model","8ac4943b":"# Feature importance","aa762cbf":"Let's visualize it","da7b726b":"CatBoost yperparams tuning:\nhttps:\/\/www.kaggle.com\/sergeyzemskov\/catboost-hyperparameters-tuning-hyperopt-30dml","619a3071":"# <center>Feature importance with CatBoost and SHAP<center>\n\nThis is the final competition of [30 Days of ML program](https:\/\/www.kaggle.com\/thirty-days-of-ml).    \n\nIn this notebook we'll explore feature importance using SHAP and CatBoost. SHAP values are the most mathematically consistent way for getting feature importances, and they work particulalry nicely with the tree-based models. ","61cd2287":"**Continuous features are more important than categorical and most significant are: cont10, cont12, cont9**"}}