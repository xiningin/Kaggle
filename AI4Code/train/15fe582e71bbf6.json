{"cell_type":{"887db3a6":"code","8135e53d":"code","3dc33675":"code","ad9daeaa":"code","e6206b2d":"code","050a87b2":"code","29257e0c":"code","628282c8":"code","559a46a4":"code","15ab4733":"code","18bbc9ef":"code","6431ff57":"code","9f5e0694":"code","4744fe3f":"code","0b53fe6f":"code","0d9a7a35":"code","e1ec1871":"code","57692aea":"code","ab8d5676":"code","013f26c1":"code","53fb8a61":"code","9fa29005":"code","c8951b8b":"code","582f94fd":"markdown","305bbeb7":"markdown","c9afffa5":"markdown","9fc34d77":"markdown","305b0c6a":"markdown","cda5da25":"markdown","562998ac":"markdown","b1b78288":"markdown","2ad7a116":"markdown","a812c633":"markdown","980a1470":"markdown","4dce5fd0":"markdown","7c5c1c83":"markdown","2c80e99c":"markdown","8cc24439":"markdown"},"source":{"887db3a6":"import re\nimport numpy as np\nimport pandas as pd\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\nimport matplotlib.pyplot as plt\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nimport plotly.io as pio\npio.renderers.default = 'kaggle'\n\nfrom tqdm.auto import tqdm \ntqdm.pandas()","8135e53d":"data_aday_log_df = pd.read_csv(\"\/kaggle\/input\/datathon-guess-the-last-one\/data_aday_log.csv\")\ndata_job_details_df = pd.read_csv(\"\/kaggle\/input\/datathon-guess-the-last-one\/data_job_details.csv\")\ndata_cv_details_df = pd.read_csv(\"\/kaggle\/input\/datathon-guess-the-last-one\/data_cv_details.csv\")\nson2_basvurular_test_df = pd.read_csv(\"\/kaggle\/input\/datathon-guess-the-last-one\/son2_basvurular_test.csv\")","3dc33675":"data_aday_log_df = data_aday_log_df.drop(columns=[\"Unnamed: 0\"], axis=1)\ndata_aday_log_df.head(2)","ad9daeaa":"data_job_details_df = data_job_details_df.drop(columns=[\"Unnamed: 0\"], axis=1)\ndata_job_details_df.head(2)","e6206b2d":"data_cv_details_df = data_cv_details_df.drop(columns=[\"Unnamed: 0\"], axis=1)\ndata_cv_details_df.head(2)","050a87b2":"son2_basvurular_test_df.head(2)","29257e0c":"dummy_df = pd.merge(data_aday_log_df, data_job_details_df, on=\"jobId\")\ndummy_df = pd.merge(dummy_df, data_cv_details_df, on=\"jobseekerId\")\ndummy_df = dummy_df.sort_values([\"jobseekerId\", \"applicationDate\"], ascending=[True, True]).reset_index(drop=True)\ndummy_df[\"rating\"] = 1.0","628282c8":"agg_df = pd.DataFrame(dummy_df.groupby([\"jobseekerId\"])[\"jobId\"].apply(lambda x: set(x.dropna())))\nagg_df[\"jobseekerId\"] = agg_df.index\nagg_df[\"len\"] = agg_df[\"jobId\"].apply(lambda x:len(x))\nagg_df = agg_df.sort_values(\"len\", ascending=False).reset_index(drop=True)\nagg_dict = dict(zip(agg_df[\"jobseekerId\"], agg_df[\"jobId\"]))\nagg_df","559a46a4":"agg_df[\"len\"].iplot(kind='hist')","15ab4733":"agg_df[\"len\"].mean()","18bbc9ef":"agg_df[\"len\"].quantile(0.05), agg_df[\"len\"].quantile(0.95), agg_df[\"len\"].quantile(0.99)","6431ff57":"application_count = pd.DataFrame(dummy_df[\"jobId\"].value_counts())\napplication_count.columns = [\"count\"]\napplication_count[\"jobId\"] = application_count.index\napplication_count = application_count.reset_index(drop=True)\n\napplication_count = pd.merge(application_count, data_job_details_df[[\"jobId\", \"jobCity\"]], how=\"inner\", on=[\"jobId\"])\napplication_count[\"jobCity\"] = application_count[\"jobCity\"].apply(lambda x: set(x.split(\",\")))\n\njob_city_dict = dict(zip(application_count[\"jobId\"], application_count[\"jobCity\"]))\napplication_count_dict = dict(zip(application_count[\"jobId\"], application_count[\"count\"]))\n\napplication_count","9f5e0694":"jobseekerId_city = pd.DataFrame(data_cv_details_df.groupby(\"jobseekerId\")[\"jobseekerCity\"].apply(lambda x: set(x.dropna())))\njobseekerId_city[\"jobseekerId\"] = jobseekerId_city.index\njobseekerId_city.columns = [\"jobCity\", \"jobseekerId\"]\njobseekerId_city = jobseekerId_city.reset_index(drop=True)\njobseekerId_city_dict = dict(zip(jobseekerId_city[\"jobseekerId\"], jobseekerId_city[\"jobCity\"]))\njobseekerId_city","4744fe3f":"!pip install sparse_dot_topn","0b53fe6f":"import numpy as np\nfrom scipy.sparse import rand\nfrom scipy.sparse import csr_matrix\nfrom pandas.api.types import CategoricalDtype\nfrom sparse_dot_topn import awesome_cossim_topn\n\nclass JobseekerMatcher:\n    def __init__(self, col, topk=10, lower_bound=0.01):\n        self.col = col\n        self.topk = topk\n        self.lower_bound = lower_bound\n        \n    def get_matches_df(self, sparse_matrix, names):\n        non_zeros = sparse_matrix.nonzero()\n        \n        name_indices = non_zeros[0]\n        gt_indices = non_zeros[1]\n\n        left_side = np.empty(gt_indices.size, dtype=object)\n        right_side = np.empty(gt_indices.size, dtype=object)\n        match_score = np.zeros(gt_indices.size)\n\n        for index in range(gt_indices.size):\n            left_side[index] = name_indices[index]\n            right_side[index] = gt_indices[index]\n            match_score[index] = sparse_matrix.data[index]\n\n        res_df = pd.DataFrame({self.col: left_side,\n                               self.col + \"_similar\": right_side,\n                               'match_score': match_score})\n\n        return res_df\n\n\n    def match(self, dummy_df, n_threads=2):\n\n        # Get unique jobseekers\n        jobseekers = list(np.sort(dummy_df[\"jobseekerId\"].unique())) \n\n        # Get unique jobs\n        jobs = list(dummy_df[\"jobId\"].unique()) \n\n        jobseekers_cat_type = CategoricalDtype(categories=jobseekers, ordered=False)\n        jobs_cat_type = CategoricalDtype(categories=jobs, ordered=False)\n\n        # Get ratings\n        ratings = list(dummy_df[\"rating\"].astype(\"double\")) # All of our ratings\n        \n        # Get the associated row indices\n        rows = dummy_df[\"jobseekerId\"].astype(jobseekers_cat_type).cat.codes \n\n        # Get the associated column indices\n        cols = dummy_df[\"jobId\"].astype(jobs_cat_type).cat.codes \n\n        # create sparse matrix\n        apply_sparse = csr_matrix((ratings, (rows, cols)), shape=(len(jobseekers), len(jobs)))\n        \n        # create map\n        jobseekerDict = dict(zip(dummy_df[\"jobseekerId\"].astype(jobseekers_cat_type).cat.codes , dummy_df[\"jobseekerId\"]))\n\n        # get similar seekers\n        sparse_matrix = awesome_cossim_topn(apply_sparse,\n                                            apply_sparse.T,\n                                            self.topk,\n                                            self.lower_bound,\n                                            use_threads=True,\n                                            n_jobs=2)\n        return jobseekerDict, self.get_matches_df(sparse_matrix, dummy_df[\"jobseekerId\"])","0d9a7a35":"%%time\njsm = JobseekerMatcher(col=\"jobseekerId\", topk=5, lower_bound=0.00)\njobseekerDict, res_df = jsm.match(dummy_df, n_threads=2)\n\nres_df = res_df[res_df[\"jobseekerId\"] != res_df[\"jobseekerId_similar\"]]\nres_df[\"jobseekerId\"] = res_df[\"jobseekerId\"].map(jobseekerDict)\nres_df[\"jobseekerId_similar\"] = res_df[\"jobseekerId_similar\"].map(jobseekerDict)","e1ec1871":"def merge_jobs(jobseekers, agg_dict):\n    jobs = set()\n    for seeker in jobseekers:\n        seeker_jobs = set(agg_dict[seeker])\n        jobs |= seeker_jobs\n    return jobs\n\n\ndef get_recomendations(application_count, recomendedJobs, jobseekerId):\n    \n    filtered_count = application_count[application_count[\"jobId\"].isin(list(recomendedJobs))]\n    flags = filter_city(filtered_count[\"jobId\"].to_list(), jobseekerId)\n\n    filtered_count = filtered_count[flags]\n    filtered_count = filtered_count.sort_values(\"count\", ascending=False)\n    jobIds = filtered_count.head(10)[\"jobId\"].values\n\n    del filtered_count\n    return jobIds\n\n\ndef filter_city(recomendedJobs, jobseekerId):\n    jobIds = []\n    applied_cities = jobseekerId_city_dict[jobseekerId]\n    for job in recomendedJobs:\n        job_cities = job_city_dict[job]\n        if len(set.intersection(job_cities, applied_cities)) > 0:\n            jobIds.append(True)\n        else:\n            jobIds.append(False)\n    return jobIds","57692aea":"res_df.head()","ab8d5676":"result_df = pd.DataFrame(res_df.groupby([\"jobseekerId\"])[\"jobseekerId_similar\"].apply(lambda x: merge_jobs(list(x.dropna()), agg_dict)))\nresult_df[\"jobseekerId\"] = result_df.index\nresult_df = result_df.reset_index(drop=True)","013f26c1":"result_df.head()","53fb8a61":"result_df[\"recomendedJobs\"] = result_df.apply(lambda x: x[\"jobseekerId_similar\"] - set(agg_dict[x[\"jobseekerId\"]]), axis=1)\nresult_df.head(3)","9fa29005":"result_df[\"jobId\"] = result_df.apply(lambda row: get_recomendations(application_count, row[\"recomendedJobs\"], row[\"jobseekerId\"]), axis=1)\nfinal_df = result_df[[\"jobseekerId\", \"jobId\"]].explode(\"jobId\")\nfinal_df.head(5)","c8951b8b":"final_df = pd.merge(son2_basvurular_test_df[\"jobseekerId\"], final_df, on = \"jobseekerId\", how=\"left\")\nfinal_df.to_csv(\"submission.csv\", index=False)\nfinal_df.head(2)","582f94fd":"# Kendisinin Ba\u015fvurmad\u0131\u011f\u0131 Ama Benzer Kullan\u0131c\u0131lar\u0131n Ba\u015fvurdu\u011fu \u0130lanlar","305bbeb7":"# Jobseeker Applied Cities","c9afffa5":"# Benzer Userlar\u0131n Ayn\u0131 Sat\u0131ra Getirilmesi","9fc34d77":"# Get Similar Users","305b0c6a":"# Libraries","cda5da25":"# Calculate Popular Jobs","562998ac":"# Otomatik Submission Yollama :)\n* !kaggle competitions submit -c datathon-guess-the-last-one -f submission.csv -m \"Guess The Last One\"","b1b78288":"# Read Datasets","2ad7a116":"# JobSeeker Application Histogram","a812c633":"# Uygun \u0130lanlar\u0131n Pop\u00fcleritiye G\u00f6re Re-Rank Edilmesi ve \u015eehir Filtresi Koyulmas\u0131","980a1470":"# TopN Similar JobSeeker","4dce5fd0":"# Ortalama Ba\u015fvuru Say\u0131s\u0131","7c5c1c83":"# Predictionlar\u0131n Yollanmas\u0131","2c80e99c":"# \u0130leride yap\u0131labilecek Geli\u015ftirmeler\n* Job2Job collabrative filtering benzer bir \u015fekile yap\u0131labilir\n* \u0130lan a\u00e7\u0131klamalar\u0131 kullan\u0131larak content tabanl\u0131 recomendation sistemi extend edilebilir\n* NDCG@10 metri\u011fine algoritman\u0131n e\u011fitilmesi\n* Re-ranking i\u00e7in makine \u00f6\u011frenme tabanl\u0131 y\u00f6ntemlere gidimesi","8cc24439":"# Merge Dataframes"}}