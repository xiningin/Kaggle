{"cell_type":{"bec8e4b7":"code","885fcdaf":"code","d93c52e6":"code","8249824c":"code","1fdc9b3a":"code","49073e7d":"code","b3a1856a":"code","53b53611":"code","c94b8e1d":"code","66567a35":"code","822feacd":"code","e7a9b5e3":"code","98036884":"code","4eb563d1":"code","7fb16f07":"code","3a13a228":"code","828a7e24":"code","840dad1d":"code","505b5afd":"code","0fd0b4d8":"code","5598b2ad":"code","138bb224":"code","9f6a044c":"code","fcb4abfd":"code","57a6b06c":"code","146f98ad":"code","aea8b3bd":"code","77b93006":"markdown","b86bae4a":"markdown","07aeb0cd":"markdown","a0523f07":"markdown","c7bfbbe5":"markdown","175ac5a4":"markdown","d31c8dee":"markdown","145ef4eb":"markdown","293c121e":"markdown","792f67f8":"markdown","9830addf":"markdown","f08d1802":"markdown","4d465bfc":"markdown","1133ac1b":"markdown","881cfe9b":"markdown","34516893":"markdown","f1f90310":"markdown","a7b86005":"markdown","7c2d4ba6":"markdown","f6ab54c1":"markdown","e1961b23":"markdown","3b05c1e9":"markdown","a86f9d74":"markdown","f922cba2":"markdown"},"source":{"bec8e4b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom collections import Counter\n\nimport spacy\nfrom spacy.lang.en import English\n\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nfrom plotly.offline import download_plotlyjs, plot, init_notebook_mode, iplot\ninit_notebook_mode(connected=True)# initiate notebook for offline plot\nimport plotly.graph_objs as go\nimport os\nimport plotly.express as px\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\nfrom nltk.corpus import stopwords\nimport plotly.offline as py\nimport nltk\nfrom nltk.corpus import stopwords\npy.init_notebook_mode(connected=True)\nnltk.download('stopwords')\nstop=set(stopwords.words('english'))\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","885fcdaf":"tweets_csv = pd.read_csv('\/kaggle\/input\/all-trumps-twitter-insults-20152021\/trump_insult_tweets_2014_to_2021.csv')\ntweets_csv.drop(columns=['Unnamed: 0'], inplace=True)\ntweets_csv['date']= pd.to_datetime(tweets_csv['date'])\ntweets_csv.shape","d93c52e6":"# create meta data\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n\ntweets_csv['jacard_score'] = tweets_csv.apply(lambda x: jaccard(x['tweet'], x['insult']), axis=1)\n\ntweets_csv['tweet_num_word'] = tweets_csv['tweet'].apply(lambda x : len(str(x).split()))\ntweets_csv['insult_num_word'] = tweets_csv['insult'].apply(lambda x : len(str(x).split()))\ntweets_csv.head(5)","8249824c":"tweets_csv.head(5)","1fdc9b3a":"duplicate = tweets_csv[tweets_csv.duplicated(['tweet', 'date'])] \nduplicate['tweet'] = duplicate['tweet'].str.lower()\nduplicate['insult'] = duplicate['insult'].str.lower()\nduplicate.shape, tweets_csv.shape","49073e7d":"plt.figure(figsize=(12, 6))\np1 = sns.kdeplot(tweets_csv['tweet_num_word'], shade = True, color='r').set_title('Kernel dirtribution of number of words')\np1 = sns.kdeplot(tweets_csv['insult_num_word'], shade=True, color='b')","b3a1856a":"plt.figure(figsize=(12, 6))\np1 = sns.kdeplot(tweets_csv['jacard_score'], shade = True, color='r').set_title('Kernel dirtribution of jacard score')","53b53611":"tweets_csv[tweets_csv['jacard_score'] > 0.8]","c94b8e1d":"target_full_insult = tweets_csv[tweets_csv['jacard_score'] > 0.8]\nplt.figure(figsize=(12,6))\nplt.xticks(rotation=30)\ng = sns.countplot(x='target',data=target_full_insult, order=target_full_insult.target.value_counts().iloc[:9].index)\n","66567a35":"tweets_csv.target.nunique()","822feacd":"tweets_csv.target.value_counts()","e7a9b5e3":"trace = go.Bar(x=tweets_csv.target.value_counts().index[:25], y=tweets_csv.target.value_counts(),\n              marker=dict(\n                  opacity=0.8,\n                  color=np.arange(25)\n              ))\n\nfig = go.Figure(data=[trace])\nfig.update_layout(title=\"Top 25 Targets\")\nfig.update_xaxes(title=\"Target\")\nfig.update_yaxes(title=\"Frequency\")\n\niplot(fig)","98036884":"nlp = spacy.load(\"en\")\n# Let's add some stop word to list.\nnlp.Defaults.stop_words |= {\"The\",\"&\", \"-\", \"A\"}\ndef remove_stop(row):\n    return [word for word  in row if word not in nlp.Defaults.stop_words]\n\nduplicate['tweet_removed_stop_word'] = duplicate['tweet'].apply(lambda x:str(x).split()).apply(lambda x:remove_stop(x))\nduplicate['insult_removed_stop_word'] = duplicate['insult'].apply(lambda x:str(x).split()).apply(lambda x:remove_stop(x))","4eb563d1":"top_words = Counter([item for row in duplicate['tweet_removed_stop_word'] for item in row])\ntop_wrods_df = pd.DataFrame(top_words.most_common(20))\ntop_wrods_df.columns = ['word', 'count']\ntop_wrods_df.style.background_gradient(cmap='Blues')","7fb16f07":"fig = px.bar(top_wrods_df, x=\"count\", y=\"word\", title='Most words in tweets', orientation='h', \n             width=700, height=700,color='word')\nfig.show()","3a13a228":"fig = px.treemap(top_wrods_df, path=['word'], values='count',title='Tree Of Most Common words in tweets')\nfig.show()","828a7e24":"top_words = Counter([item for row in duplicate['insult_removed_stop_word'] for item in row])\ntop_wrods_df = pd.DataFrame(top_words.most_common(20))\ntop_wrods_df.columns = ['word', 'count']\ntop_wrods_df.style.background_gradient(cmap='Reds')","840dad1d":"fig = px.bar(top_wrods_df, x=\"count\", y=\"word\", title='Most Commmon Words in insult', orientation='h', \n             width=700, height=700,color='word')\nfig.show()","505b5afd":"fig = px.treemap(top_wrods_df, path=['word'], values='count',title='Tree Of Most Common Words in insults')\nfig.show()","0fd0b4d8":"pos_mask = np.array(Image.open('\/kaggle\/input\/masksforwordclouds\/twitter_mask4.jpg'))\nstopwords = set(STOPWORDS)\nmore_stopwords = {'I', \"i\"}\nstopwords = stopwords.union(more_stopwords)\n\nwordcloud = WordCloud(background_color='white',\n                    stopwords = stopwords,\n                    max_words = 200,\n                    max_font_size = 100, \n                    random_state = 42,\n                    width=400, \n                    height=200,\n                    mask = pos_mask)\nwordcloud.generate(str(duplicate.tweet))\nplt.figure(figsize=(16.0,9.0))\nplt.imshow(wordcloud)\nplt.title('word cloud for tweets', fontdict={'size': 16,  \n                                  'verticalalignment': 'bottom'})","5598b2ad":"pos_mask = np.array(Image.open('\/kaggle\/input\/masksforwordclouds\/twitter_mask4.jpg'))\nstopwords = set(STOPWORDS)\nmore_stopwords = {'I', \"i\"}\nstopwords = stopwords.union(more_stopwords)\n\nwordcloud = WordCloud(background_color='white',\n                    stopwords = stopwords,\n                    max_words = 200,\n                    max_font_size = 100, \n                    random_state = 42,\n                    width=400, \n                    height=200,\n                    mask = pos_mask)\nwordcloud.generate(str(duplicate.insult))\nplt.figure(figsize=(16.0,9.0))\nplt.imshow(wordcloud)\nplt.title('word cloud for insult  ', fontdict={'size': 16,  \n                                  'verticalalignment': 'bottom'})","138bb224":"\ndef plot_n_first_rare_word(main_df, column, from_index, to_index):\n    '''\n    main_df : is dataframe you want to check\n    column : is the column you want to check it's rare words\n    from_index, to_index :  you will set a range to see\n    for example if you set from_index : 200 and to index : 300  you will see range 200 to 300\n    '''\n    if to_index < from_index: assert print(\"from_index must be greater than from_index\")\n    rare_words = Counter([item for row in main_df[column] for item in row])\n    rare_words_df = pd.DataFrame(rare_words.most_common()[-to_index:-from_index])\n    rare_words_df.columns = ['word', 'count']\n    fig = px.treemap(rare_words_df, path=['word'], values='count',title=f'plot for range of {from_index} to {to_index} rare words')\n    fig.show()\n\n","9f6a044c":"plot_n_first_rare_word(duplicate, \"tweet_removed_stop_word\",100, 200)","fcb4abfd":"from nltk.corpus import stopwords\nnltk.download('stopwords')\nstop=set(stopwords.words('english'))\npy.init_notebook_mode(connected=True)\n\ndef get_top_ngram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(n, n),stop_words=stop).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:20]\n","57a6b06c":"new=tweets_csv['tweet']\ntop_n_bigrams=get_top_ngram(new,2)[:20]\npd.DataFrame(top_n_bigrams, columns=['n-gram','count']).set_index('n-gram').plot(kind='bar');","146f98ad":"new=tweets_csv['tweet']\ntop_n_bigrams=get_top_ngram(new,3)[:20]\npd.DataFrame(top_n_bigrams, columns=['n-gram','count']).set_index('n-gram').plot(kind='bar');","aea8b3bd":"datecount_data = tweets_csv.date.value_counts().resample(\"M\").count()\ntrace1 = go.Scatter(x=datecount_data.index, y=datecount_data)\nfig = go.Figure(data=[trace1])\n\nfig.update_xaxes(title=\"Year\")\nfig.update_yaxes(title=\"Tweet Count\")\n\niplot(fig)","77b93006":"We expect to see jacard under 0.5 but we see a peak around 1. Weird? So let's see what are those?","b86bae4a":"this was our plots for ngrams","07aeb0cd":"## most common words in insults","a0523f07":"# Insult length","c7bfbbe5":"most of the time insults are just smething around 8 words of tweet. So he is not so Rude :)","175ac5a4":"# Let's check jacard score and same rows first","d31c8dee":"As we can see Fake, is so much repeated. As you can see he is not so rude :).\n\nhe had so much attention to election.\n\nBut my question is how insult part is selected ?","145ef4eb":"Let's see how many word of a tweet were insults.","293c121e":"Checking ngrams are so useful. Always check them","792f67f8":"So As we can remember he had a lot of conflicts with Media and democrate.","9830addf":"# a little work with time","f08d1802":"So As we can see these are the tweets which their insult part and tweet part are some how same.\n\nLet's see these tweets target.","4d465bfc":"# Check ngrams","1133ac1b":"I didn't focus on text_cleaning so much and you can see some punctutation and other things there. You can clean your data more and get better results here.\n\nThis will be useful for you to double check your text dataset by this approach","881cfe9b":"So there are 866 target. Let's see how many times trump tweeted against them.","34516893":"focuse on first and second rows. they are same tweets. but 2 parts were selected as insult. BUT second row has jacard_score 0.03 while first row has 0.0, WHY?\n\nif you look closer, insult for first row was 'fool' but we have 'fool,' in text of the tweet. So this is the reason. maybe need a better function to calculte jacard. but for now forget about this problem.\n\nLet's check how many of our rows are same Tweets with different insult part\n\n\n** TIP ** : I know trump love CAPSLOCK and tweet many times in uppercase but I will lower case all the words.","f1f90310":"As the name of dataset says ( \"all-trumps-twitter-insults\" ) this dataset just contain those tweets which Donal J. Trump wasn't so Polite :D .Let's see what was the problem and who made him angry :)\n\n<br>\n\nthis dataset is so much similar to [this comp](https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction). we can assume insult part is same as selected text in the comp. \n\nif you were in that competition feel free to show your EDA's here too.\n\n\nI inspired by many notebooks there such as :\n* https:\/\/www.kaggle.com\/tanulsingh077\/twitter-sentiment-extaction-analysis-eda-and-model\/\n* https:\/\/www.kaggle.com\/rahul253801\/eda-of-trump-s-twitter-insults\n* https:\/\/www.kaggle.com\/shahules\/complete-eda-baseline-model-0-708-lb","a7b86005":"Half of data are same in tweet and date ( means they are one tweet but different insults are there )it means we have just 4678 unique tweets.","7c2d4ba6":"# Targets\n\nLet's see who made him angry:D. first let's see how many target we have","f6ab54c1":"#  Check Rare words","e1961b23":"## Word cloud in insults and tweets","3b05c1e9":"## most common words in tweets","a86f9d74":"# Let's see Trump's vocabulary","f922cba2":"Let's show most common words, but first we need 2 steps : \n\n1. ignore duplicated rows ( we did it before )\n2. remove stop words"}}