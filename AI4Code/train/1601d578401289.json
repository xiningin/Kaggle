{"cell_type":{"cb195470":"code","a07bd167":"code","be34e6b5":"code","6963f212":"code","32e8db01":"code","2689e79e":"code","037edec0":"code","815025e4":"code","6ecf317f":"code","48d638ff":"code","33546bf0":"code","2d1cebcd":"code","2d1c77a8":"markdown","f7c82897":"markdown","c0439a5c":"markdown","df555b4f":"markdown","0861c8e0":"markdown","5e5fbbb4":"markdown","36ca94d9":"markdown","b5ad130c":"markdown","e31f7767":"markdown","f92cd04c":"markdown","32b349e6":"markdown","92590e12":"markdown","95d5fb2b":"markdown","78886de4":"markdown"},"source":{"cb195470":"!pip install git+https:\/\/github.com\/crowdai\/coco.git#subdirectory=PythonAPI","a07bd167":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Garbage collector\nimport gc\n\n# Folder manipulation\nimport os\nfrom glob import glob\n\n# Linear algebra\nimport numpy as np\n\n# Visualisation of picture and graph\nfrom matplotlib import pylab as plt\nimport cv2\n\n# Get version python\/keras\/tensorflow\/sklearn\nfrom platform import python_version\nimport sklearn\nimport keras\nimport tensorflow as tf\n\n# Keras importation\nimport keras.layers as layers\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n\n# Plotting segmentation mask\nfrom pycocotools.coco import COCO\nfrom pycocotools import mask as cocomask\nimport skimage.io as io\n\n# Others import\nimport random\nimport time","be34e6b5":"print(os.listdir(\"..\/input\"))\nprint(\"Keras version : \" + keras.__version__)\nprint(\"Tensorflow version : \" + tf.__version__)\nprint(\"Python version : \" + python_version())\nprint(\"Sklearn version : \" + sklearn.__version__)","6963f212":"IMG_ROWS = 256\nIMG_COLS = 256\nCHANNELS = 3\nMAX_IMAGES = 3000 # Max number of picture used for training\n# (more create a ressource crash on Kaggle...)\n\nMAIN_DIR = \"..\/input\/\"\nIMAGES_DIR = f\"{MAIN_DIR}train\/train\/images\/\"\nANNOTATIONS_PATH = f\"{MAIN_DIR}train\/train\/annotation-small.json\"\n\nplt.rcParams['figure.figsize'] = (8.0, 10.0) # Set images sizes for plotting result during learning","32e8db01":"def load_random_data():\n    coco = COCO(ANNOTATIONS_PATH)\n    \n    image_ids = coco.getImgIds(catIds=coco.getCatIds())\n    \n    # Select a random pictures id\n    random_image_id = random.choice(image_ids)\n    img = coco.loadImgs(random_image_id)[0]\n\n    annotation_ids = coco.getAnnIds(imgIds=img['id'])\n    annotations = coco.loadAnns(annotation_ids)\n\n    image_path = os.path.join(IMAGES_DIR, img[\"file_name\"])\n    I = io.imread(image_path) # Image en png\n\n    mask = np.zeros((300, 300))\n    for _idx, annotation in enumerate(annotations):\n        rle = cocomask.frPyObjects(annotation['segmentation'], img['height'], img['width'])\n        m = cocomask.decode(rle)\n        m = m.reshape((img['height'], img['width']))\n        mask = mask + m\n    \n    return I, mask","2689e79e":"img, mask = load_random_data()","037edec0":"def plot_pictures(img, mask):\n    fig, axs = plt.subplots(1,2, figsize=(7.5, 7.5))\n        \n    axs[0].imshow(img)\n    axs[0].set_title(\"Picture\")\n    axs[0].axis('off')\n    \n    axs[1].imshow(mask)\n    axs[1].set_title(\"Mask\")\n    axs[1].axis('off')","815025e4":"plot_pictures(img, mask)","6ecf317f":"def load_data(batch_size):\n    coco = COCO(ANNOTATIONS_PATH)\n    \n    path1 = sorted(glob(IMAGES_DIR + \"*\"))\n    path2 = coco.getImgIds(catIds=coco.getCatIds())\n    \n    i = np.random.randint(0,27)\n    batch1 = path1[i*batch_size:(i+1)*batch_size]\n    batch2 = path2[i*batch_size:(i+1)*batch_size]\n    \n    img_A, img_B = [], []\n    \n    for filename1,filename2 in zip(batch1,batch2):\n        \n        json = coco.loadImgs(filename2)[0]\n        image_path = os.path.join(IMAGES_DIR, json[\"file_name\"])\n        img = cv2.imread(image_path)\n        \n        annotation_ids = coco.getAnnIds(imgIds=json['id'])\n        annotations = coco.loadAnns(annotation_ids)\n        \n        mask = np.zeros((300, 300, 1))\n        \n        for _idx, annotation in enumerate(annotations):\n            rle = cocomask.frPyObjects(annotation['segmentation'], json['height'], json['width'])\n            m = cocomask.decode(rle)\n            mask = mask + m\n        \n        img = cv2.resize(img,(256,256),interpolation=cv2.INTER_AREA)\n        mask = cv2.resize(mask,(256,256),interpolation=cv2.INTER_AREA)\n        \n        mask = mask * 255\n        mask = np.reshape(mask, (mask.shape[0], mask.shape[1], 1))\n        \n        img_A.append(img)\n        img_B.append(mask)\n      \n    img_A = np.array(img_A) \/ 127.5 - 1\n    img_B = np.array(img_B) \/ 127.5 - 1\n    \n    return img_A, img_B ","48d638ff":"def load_batch(batch_size):\n    coco = COCO(ANNOTATIONS_PATH)\n    \n    path1 = sorted(glob(IMAGES_DIR + \"*\"))\n    path2 = coco.getImgIds(catIds=coco.getCatIds())\n    \n    n_batches=int(len(path1)\/batch_size)\n    max_batch = 0\n  \n    for i in range(n_batches):\n        batch1 = path1[i*batch_size:(i+1)*batch_size]\n        batch2 = path2[i*batch_size:(i+1)*batch_size]\n        img_A, img_B=[],[]\n        \n        for filename1,filename2 in zip(batch1,batch2):\n            json = coco.loadImgs(filename2)[0]\n            image_path = os.path.join(IMAGES_DIR, json[\"file_name\"])\n            img1 = cv2.imread(image_path)\n            \n            annotation_ids = coco.getAnnIds(imgIds=json['id'])\n            annotations = coco.loadAnns(annotation_ids)\n\n            mask = np.zeros((300, 300, 1))\n            for _idx, annotation in enumerate(annotations):\n                rle = cocomask.frPyObjects(annotation['segmentation'], json['height'], json['width'])\n                m = cocomask.decode(rle)\n                mask = mask + m\n\n            img1 = img1\n            img2 = mask\n\n            img1=cv2.resize(img1,(256,256),interpolation=cv2.INTER_AREA)\n            img2=cv2.resize(img2,(256,256),interpolation=cv2.INTER_AREA)\n\n            img2 = np.reshape(img2, (img2.shape[0], img2.shape[1], 1))\n            \n            img_A.append(img1)\n            img_B.append(img2*255)\n\n        img_A=np.array(img_A) \/ 127.5-1\n        img_B=np.array(img_B) \/ 127.5-1\n        \n        max_batch = max_batch + 1\n        \n        if(max_batch > MAX_IMAGES):\n            raise StopIteration\n        else:\n            yield img_B, img_A","33546bf0":"class Pix2pix():\n    def __init__(self):\n        self.img_rows = IMG_ROWS\n        self.img_cols = IMG_COLS\n        self.channels = CHANNELS\n        self.img_shape = (self.img_rows,self.img_cols,self.channels)\n        self.mask_shape = (self.img_rows,self.img_cols,1)\n    \n        patch = int(self.img_rows \/ (2**4)) # 16\n        self.disc_patch = (patch, patch, 1)\n\n        self.gf = 64\n        self.df = 64\n    \n        optimizer = Adam(0.0002,0.5)\n    \n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy',\n                              optimizer=optimizer)\n    \n        self.generator = self.build_generator()\n    \n        mask_input = layers.Input(shape=self.mask_shape) #img_B = layers.Input(shape=self.img_shape)\n        \n        img = self.generator(mask_input)\n    \n        self.discriminator.trainable = False\n    \n        valid = self.discriminator([img, mask_input])\n    \n        self.combined = Model(mask_input, valid)\n        self.combined.compile(loss='binary_crossentropy',\n                              optimizer=optimizer)\n    \n    def build_generator(self):\n        def conv2d(layer_input,filters,f_size=(4,4),bn=True):\n            d = layers.Conv2D(filters,kernel_size=f_size,strides=(2,2),padding='same')(layer_input)\n            d = layers.LeakyReLU(0.2)(d)\n            if bn:\n                d = layers.BatchNormalization()(d)\n            return d\n    \n        def deconv2d(layer_input,skip_input,filters,f_size=(4,4),dropout_rate=0):\n            u = layers.UpSampling2D((2,2))(layer_input)\n            u = layers.Conv2D(filters,kernel_size=f_size,strides=(1,1),padding='same',activation='relu')(u)\n            if dropout_rate:\n                u = layers.Dropout(dropout_rate)(u)\n            u = layers.BatchNormalization()(u)\n            u = layers.Concatenate()([u,skip_input])\n            return u\n    \n        d0 = layers.Input(shape=self.mask_shape)\n    \n        d1 = conv2d(d0,self.gf,bn=False) \n        d2 = conv2d(d1,self.gf*2)         \n        d3 = conv2d(d2,self.gf*4)         \n        d4 = conv2d(d3,self.gf*8)         \n        d5 = conv2d(d4,self.gf*8)         \n        d6 = conv2d(d5,self.gf*8)        \n    \n        d7 = conv2d(d6,self.gf*8)         \n    \n        u1 = deconv2d(d7,d6,self.gf*8,dropout_rate=0.5)   \n        u2 = deconv2d(u1,d5,self.gf*8,dropout_rate=0.5)   \n        u3 = deconv2d(u2,d4,self.gf*8,dropout_rate=0.5)   \n        u4 = deconv2d(u3,d3,self.gf*4)   \n        u5 = deconv2d(u4,d2,self.gf*2)   \n        u6 = deconv2d(u5,d1,self.gf)     \n        u7 = layers.UpSampling2D((2,2))(u6)\n    \n        output_img = layers.Conv2D(3,kernel_size=(4,4),strides=(1,1),padding='same',activation='tanh')(u7)\n    \n        return Model(d0,output_img)\n  \n    def build_discriminator(self):\n        def d_layer(layer_input,filters,f_size=(4,4),bn=True):\n            d = layers.Conv2D(filters,kernel_size=f_size,strides=(2,2),padding='same')(layer_input)\n            d = layers.LeakyReLU(0.2)(d)\n            if bn:\n                d=layers.BatchNormalization()(d)\n            return d\n    \n        img_input = layers.Input(shape=self.img_shape)\n        mask_input = layers.Input(shape=self.mask_shape)\n    \n        combined_imgs = layers.Concatenate(axis=-1)([img_input, mask_input])\n    \n        d1 = d_layer(combined_imgs,self.df,bn=False)\n        d2 = d_layer(d1,self.df*2)\n        d3 = d_layer(d2,self.df*4)\n        d4 = d_layer(d3,self.df*8)\n    \n        validity = layers.Conv2D(1,kernel_size=(4,4),strides=(1,1),padding='same',activation='sigmoid')(d4)\n    \n        return Model([img_input, mask_input], validity)\n  \n    def train(self,epochs,batch_size=1):\n        valid=np.ones((batch_size,)+self.disc_patch)\n        fake=np.zeros((batch_size,)+self.disc_patch)\n    \n        for epoch in range(epochs):\n            start=time.time()\n            for batch_i,(img_A,img_B) in enumerate(load_batch(1)):\n                gen_imgs=self.generator.predict(img_A)\n        \n                d_loss_real = self.discriminator.train_on_batch([img_B, img_A], valid)\n                d_loss_fake = self.discriminator.train_on_batch([gen_imgs, img_A], fake)\n                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n        \n                g_loss = self.combined.train_on_batch(img_A,valid)\n\n                if batch_i % 50 == 0:\n                    print (\"[Epoch %d] [Batch %d] [D loss: %f] [G loss: %f]\" % (epoch,\n                                                                                batch_i,\n                                                                                d_loss,\n                                                                                g_loss))\n            \n            self.sample_images(epoch)\n            print('Time for epoch {} is {} sec'.format(epoch,time.time()-start))\n      \n    def sample_images(self, epoch):\n        # Set values\n        row, col = 3, 3\n        nb_img = 3\n        \n        # Load and generate data\n        img_A, img_B =load_data(nb_img)\n        fake_A = self.generator.predict(img_B)\n        \n        # Rescale or reshape data\n        img_A = 0.5 * img_A + 0.5\n        fake_A = 0.5 * fake_A + 0.5\n        img_B = np.reshape(img_B, (3, 256, 256)) # Avoid matplotlib error\n        \n        imgs = [img_A, fake_A, img_B]\n\n        # Plot results pictures\n        titles = ['Condition', 'Generated', 'Original']\n        \n        fig, axs = plt.subplots(row, col)\n        \n        for r, img, title in zip(range(0, row), imgs, titles):\n            for c in range(0, col):\n                axs[r,c].imshow(img[c])\n                axs[r,c].set_title(title)\n                axs[r,c].axis('off')\n        \n        plt.show()","2d1cebcd":"gan = Pix2pix()\ngan.train(epochs=70, batch_size=1)","2d1c77a8":"## 5.2 Pictures <a id=\"pictures\"><\/a>","f7c82897":"## 5.1 Load data <a id=\"load_data\"><\/a>","c0439a5c":"# Table of Contents\n\n1. [Context](#context)  \n2. [Importations](#importations)  \n3. [Informations](#informations)\n4. [Set parameters](#set_parameters)\n5. [Data exploration](#data_exploration)  \n    5.1 [Load data](#load_data)  \n    5.2 [Pictures](#pictures)  \n6. [Modelisation](#modelisation)\n7. [Conclusion](#conclusion)\n8. [Additionnal informations](#additionnal_informations)  ","df555b4f":"# 2. Importations <a id=\"importations\"><\/a>","0861c8e0":"# 8. Conclusion <a id=\"conclusion\"><\/a>\n\n<p style=\"text-align:justify;\">\nDue to lack of time and performance on Kaggle for this type of network I can't explore more this implemantion on Kaggle kernel but it seems to work weel on satellite images. See addionnals informations for more details around this notebook and Pix2Pix network.<\/p>","5e5fbbb4":"# 9. Additionnal informations <a id=\"additionnal_informations\"><\/a> \n\n[[1]](https:\/\/www.aicrowd.com\/challenges\/mapping-challenge) AI crowd competition on segmentation of satellite images.  \n[[2]](https:\/\/github.com\/hanwen0529\/GAN-pix2pix-Keras) Implemenation of Pix2Pix on GitHub.  \n[[3]](https:\/\/github.com\/crowdAI\/mapping-challenge-starter-kit\/blob\/master\/Dataset%20Utils.ipynb) Visualize satellite images of AI crowd competition.    \n[[4]](https:\/\/github.com\/NVIDIA\/pix2pixHD) Work of NVidia on Pix2PixHD an amelioration of Pix2Pix.  \n[[5]](https:\/\/arxiv.org\/pdf\/1611.07004.pdf) Pix2Pix paper.","36ca94d9":"<p style=\"text-align:justify;\">\nThis notebook is an implementation of Pix2Pix network, a CGAN applied on satellite images. The goal is to generate from label a picture of satellite images, as example, when you didn't have enough training data it can be a way for increasing your database by generating new image from random segmentation label. The data come from a competition organize on AI Crowd <a href=\"https:\/\/www.aicrowd.com\/challenges\/mapping-challenge\">[1]<\/a>. The Pix2Pix implementation is adapted from this github <a href=\"https:\/\/github.com\/hanwen0529\/GAN-pix2pix-Keras\">[2]<\/a>.\n<\/p>\n\n<p style=\"text-align:justify;\">\nFor more informations and references about the construction of this notebook see the <a href=\"#additionnal_informations\">additionnal informations<\/a> part.\n<\/p>","b5ad130c":"# 4. Set parameters <a id=\"set_parameters\"><\/a>","e31f7767":"# 6. Modelisation <a id=\"modelisation\"><\/a>","f92cd04c":"# 5. Data exploration <a id=\"data_exploration\"><\/a>","32b349e6":"<p style=\"text-align:center;\">\n    <img src=\"https:\/\/miro.medium.com\/max\/1414\/1*yDTyBU-vGGQ3zqVgQr-RGg.jpeg\" style=\"height:300px; width:100%\"\/>\n<\/p>\n<p style=\"text-align:right;\">\n    Source : <a href=\"https:\/\/medium.com\/@anttilip\/seeing-earth-from-space-from-raw-satellite-data-to-beautiful-high-resolution-images-feb522adfa3f\">https:\/\/medium.com\/@anttilip\/seeing-earth-from-space-from-raw-satellite-data-to-beautiful-high-resolution-images-feb522adfa3f<\/a>\n<\/p>","92590e12":"## CGAN (Conditionnal GAN) Pix2Pix with Keras on satellite images","95d5fb2b":"# 3. Informations <a id=\"informations\"><\/a>","78886de4":"# 1. Context <a id=\"context\"><\/a>"}}