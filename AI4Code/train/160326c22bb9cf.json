{"cell_type":{"dde4f5d0":"code","9dcbb70e":"code","921cad8b":"code","e8e08fa2":"markdown"},"source":{"dde4f5d0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9dcbb70e":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\n","921cad8b":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=2500):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model) #initialize a tensor filled with scaler values of zero\n        \n        position = torch.arange(0, max_len).unsqueeze(1) #torch.unsqueeze adds an additional dimension to the tensor.\n        div_term = torch.exp(torch.arange(0, d_model, 2) *\n                             -(math.log(10000.0) \/ d_model)) #division term of the formula of PE\n        print(div_term)\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe) # buffers are used when you need to save the parameters in which will be restored from state_dict but not needed to be trained by optimizers\n        \n    def forward(self, x):\n        '''\n        Tensors and variables as same. Variables are  wrappers for the tensors so you can now easily auto compute the gradients.\n        So if a tensor was batman\u2026\n        A Variable would be batman but with his utility belt on\n        '''\n        x = x + Variable(self.pe[:, :x.size(1)], \n                         requires_grad=False)\n        return self.dropout(x)\n    \n\n%matplotlib inline\nplt.figure(figsize=(15, 5))\npe = PositionalEncoding(20,0)\ny = pe.forward(Variable(torch.zeros(1, 100, 20)))\nplt.plot(np.arange(100), y[0, :, 3:8].data.numpy())\nplt.legend([\"dim %d\"%p for p in [3,4,5,6,7]])","e8e08fa2":" If you have parameters in your model, which should be saved and restored in the state_dict, but not trained by the optimizer, you should register them as buffers.\nBuffers won\u2019t be returned in model.parameters(), so that the optimizer won\u2019t have a change to update them."}}