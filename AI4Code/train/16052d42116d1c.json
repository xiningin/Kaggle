{"cell_type":{"e5ce8c56":"code","0ee8d3b4":"code","88095323":"code","046f0354":"code","1b8b9364":"code","6ba0b78d":"code","89208672":"code","d221c1ca":"code","c78e1fc9":"code","0e83ef2b":"code","704e0dec":"code","3e16a32c":"code","fd70bd7e":"code","c454ef43":"code","49308e65":"code","33c17154":"code","593b28af":"code","1555a191":"code","f9e4756b":"code","5be19f8c":"code","c6750843":"code","5910d073":"code","222f77ed":"code","c61499eb":"code","eb11d5b2":"code","70653905":"code","a594060e":"code","94605543":"code","bc66ccd7":"code","22aea460":"code","bd28cd18":"code","feffffd0":"code","4fa3dd5f":"code","3feda484":"code","48340b82":"code","83b3f86e":"code","3f158d85":"code","6670a345":"code","45bce73b":"code","4256785a":"code","7f1b73ac":"code","145aecf1":"code","25a6bd37":"code","eaef5bd2":"code","b31f80dc":"code","d9e9e550":"code","85893a66":"code","b9dbca41":"code","e4d47109":"code","f23679b8":"code","cba0e905":"code","37c7d8d6":"code","bedc4d03":"code","46e01836":"code","1f845109":"code","94ba1b5c":"code","2872bc93":"markdown","8c9f149b":"markdown","039fd063":"markdown","7c8e8193":"markdown"},"source":{"e5ce8c56":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0ee8d3b4":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sb","88095323":"# Get the data.\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\nprint(train.info())\nprint(test.info())\nprint(train.head())","046f0354":"# EDA\n# As we can see we have three types of data and most of the integer data are booleans. So here we only \n# concentrate on exploring float data and objects data.\n\n# Float data.\nfrom collections import OrderedDict\nplt.figure(figsize = (30, 26))\n\ncolor_use = {1: 'r', 2: 'g', 3: 'c', 4: 'y'}\ncolors = OrderedDict(color_use)\npoverty_level = {1: 'extreme poverty', 2: 'moderate poverty', 3: 'vulnerable households', \n                            4: 'nonvulnerable households'}\npl_marker = OrderedDict(poverty_level)\n\nfloat_data = train.select_dtypes('float')\nfor index, columns in enumerate(float_data):\n    axis = plt.subplot(4, 2, index + 1)\n    for pl, cl in colors.items():\n        sb.kdeplot(train.loc[train['Target'] == pl, columns], ax = axis, color = cl, \n                   label = pl_marker[pl])\n        plt.xlabel('values of ' f'{columns}')\n        plt.ylabel('proportion')","1b8b9364":"# From these float distributions we may find some features that can influence the poverty level \n# significantly and do further analysis.\n\n# Object data.\nobject_data = train.select_dtypes('object')\nprint(object_data.info())\nprint(object_data.head())","6ba0b78d":"# We can ignore the \"Id\" and \"idhogar\" because they are identifying features based on the problem. For the\n# other three features we can replace \"yes\" to \"1\" and replace \"no\" to \"0\" according to the documentation \n# on the kaggle.\nreplace_yn = {'yes': 1, 'no': 0}\n\ntrain['dependency'] = train['dependency'].replace(replace_yn).astype(np.float)\ntrain['edjefe'] = train['edjefe'].replace(replace_yn).astype(np.float)\ntrain['edjefa'] = train['edjefa'].replace(replace_yn).astype(np.float)\n\ntest['dependency'] = test['dependency'].replace(replace_yn).astype(np.float)\ntest['edjefe'] = test['edjefe'].replace(replace_yn).astype(np.float)\ntest['edjefa'] = test['edjefa'].replace(replace_yn).astype(np.float)","89208672":"train['dependency'].head()","d221c1ca":"# Make plots if needed.","c78e1fc9":"# Since some of individuals have different poverty level in the same household, we only need to consider \n# the poverty level of the head of the household according to the ducumentation on kaggle. Here we use the \n# poverty level of the head of the household as the true target variable.\ntrain_num_househould = train.groupby('idhogar')['Target'].nunique()\nprint('Numbers of households: ', len(train_num_househould))\ntrain_unique = train_num_househould[train_num_househould == 1]\nprint('Numbers of households whose poverty levels of family members are the same: ', len(train_unique))\ntrain_not_unique = train_num_househould[train_num_househould != 1]\nprint('Numbers of households whose poverty levels of family members are not the same', len(train_not_unique))","0e83ef2b":"# One example of these ununique targets within each household.\nprint(train[train['idhogar'] == train_not_unique.index[0]][['idhogar', 'parentesco1', 'Target']])\n# Locate the head of household.\nhead = train[(train['idhogar'] == train_not_unique.index[0]) & (train['parentesco1'] == 1)]\nprint(head)\n# Select the target variables that need to be changed.\nnot_unique_target = train.loc[train['idhogar'] == train_not_unique.index[0], 'Target']\nprint(not_unique_target)\n# Change the target variables.\nnot_unique_target = int(head['Target'])\nprint(not_unique_target)","704e0dec":"# Set the true label for members of households whose labels are not the same.\n# Here we write a for loop to replace all the ununique target variables.\nfor unique_hhid in train_not_unique.index:\n    hh_head = train[(train['idhogar'] == unique_hhid) & (train['parentesco1'] == 1)]\n    train.loc[train['idhogar'] == unique_hhid, 'Target'] = hh_head['Target']\n\n# Let's check if it works.\ntrain_num_househould = train.groupby('idhogar')['Target'].nunique()\nprint('Numbers of households: ', len(train_num_househould))\ntrain_unique = train_num_househould[train_num_househould == 1]\nprint('Numbers of households whose poverty levels of family members are the same: ', len(train_unique))\ntrain_not_unique = train_num_househould[train_num_househould != 1]\nprint('Numbers of households whose poverty levels of family members are not the same', len(train_not_unique))","3e16a32c":"# Now let's deal with the missing values.\n# Let's check the missiong values for both train and test data.\ntrainms = pd.DataFrame(train.isnull().sum())\ntrainms['counts'] = trainms\ntrainms['ratio'] = trainms['counts']\/len(train)\nprint(trainms.sort_values(by = 'ratio', ascending = False).head(10))\n\n\ntestms = pd.DataFrame(test.isnull().sum())\ntestms['counts'] = testms\ntestms['ratio'] = testms['counts']\/len(test)\nprint(testms.sort_values(by = 'ratio', ascending = False).head(10))","fd70bd7e":"# Now we know what features have the most missing values, then we can analyze them based on their own \n# meanings and decide whether we should delete the features or replace the missing values.\nhead_train = train.loc[train['parentesco1'] == 1]\nhead_test = test.loc[test['parentesco1'] == 1]\n\n# rez_esc: years behind in school.\n# Actually I don't really know the mean of \"years behind in school\", but what I do know is that it may be \n# related to the age feature. So let's check what's the relationship between this feature and age.\nprint(train.loc[train['rez_esc'].isnull()]['age'].describe())\nprint(train.loc[train['rez_esc'].notnull()]['age'].describe())\nprint(test.loc[test['rez_esc'].isnull()]['age'].describe())\nprint(test.loc[test['rez_esc'].notnull()]['age'].describe())","c454ef43":"# Now we find that all the defined \"rez_esc\" values are between the age 7 and 17. Those who are younger \n# than 7 or older than 17 have missing values, which means we should set \"rez_esc\" values of these people \n# to 0 instead of null. And if there are some other situations, we can leave the values to be imputed and \n# add a boolean flag.\ntrain.loc[(train['age'] > 17) & (train['rez_esc'].isnull()), 'rez_esc'] = 0\ntrain.loc[(train['age'] < 7) & (train['rez_esc'].isnull()), 'rez_esc'] = 0\n\ntest.loc[(test['age'] > 17) & (test['rez_esc'].isnull()), 'rez_esc'] = 0\ntest.loc[(test['age'] < 7) & (test['rez_esc'].isnull()), 'rez_esc'] = 0","49308e65":"# v18q1: number of tablets household owns.\n# This could be compared with the feature \"v18q: owns a tablet\". Maybe the missing values in \"v18q1\" \n# indicates that the households don't even own a tablet, which means the value is 0 in \"v18q\".\n# First let's check the values of \"v18q1\".\nprint('Numbers of null values of \"v18q1\" in train: ', head_train['v18q1'].isnull().sum())\nprint('Numbers of \"0\" of \"v18q\" in train: ', head_train[head_train['v18q'] == 0]['v18q'].count())\nprint('Numbers of null values of \"v18q1\" in test: ', head_test['v18q1'].isnull().sum())\nprint('Numbers of \"0\" of \"v18q\" in test: ', head_test[head_test['v18q'] == 0]['v18q'].count())","33c17154":"# Here we can see that every household has missing value in \"v18q1\" doesn't have any tablet. Then we can \n# replace the missing values to 0.\ntrain['v18q1'] = train['v18q1'].fillna(0)\ntest['v18q1'] = test['v18q1'].fillna(0)\n\ntrain['v18q1'].value_counts().plot.bar(color = 'red')\nplt.xlabel('values of \"v18q1\"')\nplt.ylabel('counts of individuals')\nplt.title('\"v18q1\" in train')\nplt.show()\n\ntrain['v18q'].value_counts().plot.bar(color = 'red')\nplt.xlabel('values of \"v18q\"')\nplt.ylabel('counts of individuals')\nplt.title('\"v18q\" in train')\nplt.show()","593b28af":"test['v18q1'].value_counts().plot.bar(color = 'blue')\nplt.xlabel('values of \"v18q1\"')\nplt.ylabel('counts of individuals')\nplt.title('\"v18q1\" in test')\nplt.show()\n\ntest['v18q'].value_counts().plot.bar(color = 'blue')\nplt.xlabel('values of \"v18q\"')\nplt.ylabel('counts of individuals')\nplt.title('\"v18q\" in test')\nplt.show()","1555a191":"# v2a1: monthly rent payment.\n# According to the explanations of the documentation on the kaggle, this feature may be related to the \n# \"tipovivi\", which represents the status of the house. For example, if people already own this house, then \n# they don't need to pay the rent anymore.\nstatus = ['tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5']\n\ntrain.loc[train['v2a1'].isnull(), status].sum().plot.bar(color = 'green')\nplt.xticks([0, 1, 2, 3, 4], ['own and fully paid house', 'own and paying in installments', \n                            'rented', 'precarious', 'other'], rotation = 40)\nplt.title('status of the house for missing values of \"v2a1\" in train')\nplt.show()\n\ntest.loc[test['v2a1'].isnull(), status].sum().plot.bar(color = 'cyan')\nplt.xticks([0, 1, 2, 3, 4], ['own and fully paid house', 'own and paying in installments', \n                            'rented', 'precarious', 'other'], rotation = 40)\nplt.title('status of the house for missing values of \"v2a1\" in test')\nplt.show()","f9e4756b":"# We can see that most of the households who don't have monthly rent payment generally own their house. But \n# we don't know the reason of missing values for the other situations.\ntrain.loc[(train['tipovivi1'] == 1), 'v2a1'] = 0\ntest.loc[(test['tipovivi1'] == 1), 'v2a1'] = 0\n\ntrain['v2a1'].isnull().value_counts()\ntest['v2a1'].isnull().value_counts()","5be19f8c":"# Since the rest of the missing values belong to \"rez_esc\" and \"v2a1\", which only contain integer values. \n# So we simply replace the remaining missing values to the median of the column values.\ntrain.fillna(train.median(), inplace = True)\ntest.fillna(test.median(), inplace = True)\n\nprint(train.isnull().sum().sort_values(ascending = False).head())\nprint(test.isnull().sum().sort_values(ascending = False).head())","c6750843":"# ID_variables.\n# We will keep these in the data since we need them for identification.\nidv = ['Id', 'idhogar', 'Target']\n\n# Individual variables.\nib = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', \n      'estadocivil5', 'estadocivil6', 'estadocivil7', 'parentesco1', 'parentesco2',  'parentesco3', \n      'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', \n      'parentesco10', 'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n      'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9', 'mobilephone']\n\nio = ['rez_esc', 'escolari', 'age']\n\n# Househould variables.\nhb = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', 'paredpreb','pisocemento', \n      'pareddes', 'paredmad','paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n      'pisonatur', 'pisonotiene', 'pisomadera','techozinc', 'techoentrepiso', 'techocane', 'techootro', \n      'cielorazo', 'abastaguadentro', 'abastaguafuera', 'abastaguano', 'public', 'planpri', 'noelec', \n      'coopele', 'sanitario1', 'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6', 'energcocinar1', \n      'energcocinar2', 'energcocinar3', 'energcocinar4', 'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n      'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3', 'etecho1', 'etecho2', 'etecho3', \n      'eviv1', 'eviv2', 'eviv3', 'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n      'computer', 'television', 'lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6', 'area1', \n      'area2']\n\nho = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', 'r4t3', 'v18q1', \n      'tamhog','tamviv','hhsize','hogar_nin', 'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', \n      'qmobilephone']\n\nhcn = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']\n\n# Squared Variables.\nsv = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', \n      'SQBdependency', 'SQBmeaned', 'agesq']","5910d073":"# Sometimes variables are squared or transformed as part of feature engineering because it can help linear\n# relationships that are non-linear. However, here we are going to use models that are more complex, these \n# squared features may be redundant since they are highly correlated with the non-squared features. This \n# means these squared will hurt the models badly by adding irrelevant information.\n# We can take a look at the relationshop between the squared features and non-squared features.\nsb.lmplot('edjefe', 'SQBedjefe', data = train)\nplt.title('\"edjefe\" vs \"SQBedjefe\"')\nplt.show()\n\nsb.lmplot('dependency', 'SQBdependency', data = train)\nplt.title('\"dependency\" vs \"SQBdependency\"')\nplt.show()","222f77ed":"# These features are highly correlated, we don't need to keep them both in the data. So I decide to delete \n# all of the squared variables.\ntrain = train.drop(columns = sv)\ntest = test.drop(columns = sv)","c61499eb":"# Household variables.\nhhtrain = train.loc[train['parentesco1'] == 1, :]\nhhtrain = hhtrain[idv + hb + ho + hcn]\n\nhhtest = test.loc[test['parentesco1'] == 1, :]\nhhtest = hhtest[['Id', 'idhogar'] + hb + ho + hcn]","eb11d5b2":"# Then we need to check the correlations between these features. If there are some features highly \n# correlated, we have to delete one of the pair so that it will not cause data redundant.\ncor_hh = hhtrain.corr()\nuptr = np.triu(np.ones(cor_hh.shape), k = 1).astype(np.bool)\nup = cor_hh.where(uptr)\n\na = [column for column in up.columns if any(abs(up[column]) > 0.9)]\nprint(a)\n\ncor_hh.loc[cor_hh['tamviv'] > 0.9, cor_hh['tamviv'] > 0.9]","70653905":"sb.heatmap(cor_hh.loc[cor_hh['tamviv'].abs() > 0.9, cor_hh['tamviv'].abs() > 0.9], annot = True)","a594060e":"# Based on the hearmap, we drop some of the features.\nhhtrain = hhtrain.drop(columns = ['tamhog', 'r4t3', 'hogar_total', 'hhsize'])\nhhtest = hhtest.drop(columns = ['tamhog', 'r4t3', 'hogar_total', 'hhsize'])","94605543":"# Individual vriables.\nitrain = train[idv + ib + io]\n\nitest = test[['Id', 'idhogar'] + ib + io]","bc66ccd7":"# Identify redundant features.\ncor_i = itrain.corr()\nuptr = np.triu(np.ones(cor_i.shape), k = 1).astype(np.bool)\nup = cor_i.where(uptr)\n\nb = [column for column in up.columns if any(abs(up[column]) > 0.9)]\nprint(b)","22aea460":"# This is related to the \"male\" feature, so we can move one of them. Here we remove the \"male\".\nitrain = itrain.drop(columns = 'male')\nitest = itest.drop(columns = 'male')","bd28cd18":"# Finally, we just need to aggregate the individual and household data, and we can run the model.\n# Since the boolean aggregations can be the same, but this will create many redundant columns that need \n# to be deleted. We will do the aggregations and then go back to drop the redundant columns.\naggf = lambda x: x.max() - x.min()\naggf.__name__ = 'aggregation'\n\niagg_train = itrain.drop(columns = 'Target').groupby('idhogar').agg(['min', 'max', aggf])\niagg_test = itest.groupby('idhogar').agg(['min', 'max', aggf])\niagg_test.head()","feffffd0":"# Rename the columns so that we can know their exact meaning.\nrntrain = []\nfor realname in iagg_train.columns.levels[0]:\n    for aggval in iagg_train.columns.levels[1]:\n        rntrain.append(f'{realname}-{aggval}')\niagg_train.columns = rntrain\n\nrntest = []\nfor realname in iagg_test.columns.levels[0]:\n    for aggval in iagg_test.columns.levels[1]:\n        rntest.append(f'{realname}-{aggval}')\niagg_test.columns = rntest\niagg_test.head()","4fa3dd5f":"# Check the correlations so that we can delete the redundant data.\ncor_aggtrain = iagg_train.corr()\n\nuptr_agg = cor_aggtrain.where(np.triu(np.ones(cor_aggtrain.shape), k = 1).astype(np.bool))\ndeletrain = [column for column in uptr_agg.columns if any(abs(uptr_agg[column])> 0.95)]\nprint(len(deletrain))","3feda484":"cor_aggtest = iagg_test.corr()\n\nuptr_agg = cor_aggtest.where(np.triu(np.ones(cor_aggtest.shape), k = 1).astype(np.bool))\ndeletest = [column for column in uptr_agg.columns if any(abs(uptr_agg[column])> 0.95)]\nprint(len(deletest))","48340b82":"# Merge the data.\niagg_train = iagg_train.drop(columns = deletrain)\ncleaned_train = hhtrain.merge(iagg_train, on = 'idhogar', how = 'left')\ncleaned_train.shape","83b3f86e":"iagg_test = iagg_test.drop(columns = deletest)\ncleaned_test = hhtest.merge(iagg_test, on = 'idhogar', how = 'left')\ncleaned_test.shape","3f158d85":"# First we use a simple method to establish a baseline accuracy based on kfold cross validation.\nimport sklearn\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score, make_scorer\nfrom sklearn.preprocessing import Imputer, MinMaxScaler\nfrom sklearn.pipeline import Pipeline","6670a345":"# Define the target labels.\ny = np.array(cleaned_train['Target'])\n\n# Since we are going to use different models to compare their performances, scaling the features would be \n# very necessary. Some distance metric-based models such as kNN and SVM may not be applied to the data if \n# the data is not scaled.\n# Extract the training data.\ntrain_set = cleaned_train.drop(columns = ['Id', 'idhogar', 'Target'])\ntest_set = cleaned_test.drop(columns = ['Id', 'idhogar'])","45bce73b":"# Scale the data.\npipeline = Pipeline([('scaler', MinMaxScaler())])\n\ntrain_set = pipeline.fit_transform(train_set)\ntest_set = pipeline.fit_transform(test_set)","4256785a":"# kNN.\nknn_classifier = KNeighborsClassifier(n_neighbors = 5)\nacc_sco = make_scorer(accuracy_score)\nf1_sco = make_scorer(f1_score, average = 'macro')\n\nacc_knn = cross_val_score(knn_classifier, train_set, y, cv = 10, scoring = acc_sco)\nf1_knn = cross_val_score(knn_classifier, train_set, y, cv = 10, scoring = f1_sco)\n\nprint(np.mean(acc_knn))\nprint(np.mean(f1_knn))","7f1b73ac":"# The accuracy increases with the increasing of n_neighbors.\naccknn = []\nf1knn = []\nfor i in range(4, 11):\n    knn_classifier = KNeighborsClassifier(n_neighbors = i)\n    acc_knn = cross_val_score(knn_classifier, train_set, y, cv = 10, scoring = acc_sco)\n    f1_knn = cross_val_score(knn_classifier, train_set, y, cv = 10, scoring = f1_sco)\n    a = np.mean(acc_knn)\n    b = np.mean(f1_knn)\n    accknn.append(a)\n    f1knn.append(b)\n    \nprint(accknn)\nprint(f1knn)","145aecf1":"# After we get the baseline accuracy, we will use some of the advanced models to do further machine \n# learning.\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression","25a6bd37":"# Random Forest.\nrf_classifier = RandomForestClassifier()\nacc_rf = cross_val_score(rf_classifier, train_set, y, cv = 10, scoring = acc_sco)\nf1_rf = cross_val_score(rf_classifier, train_set, y, cv = 10, scoring = f1_sco)\n\nprint(np.mean(acc_rf))\nprint(np.mean(f1_rf))","eaef5bd2":"accrf = []\nf1rf = []\nfor i in range(1, 6):\n    rf_classifier = RandomForestClassifier(max_depth = i)\n    acc_rf = cross_val_score(rf_classifier, train_set, y, cv = 10, scoring = acc_sco)\n    f1_rf = cross_val_score(rf_classifier, train_set, y, cv = 10, scoring = f1_sco)\n    c = np.mean(acc_rf)\n    d = np.mean(f1_rf)\n    accrf.append(c)\n    f1rf.append(d)\n    \nprint(accrf)\nprint(f1rf)","b31f80dc":"# SVM.\nsvm_classifier = SVC(kernel = 'rbf')\nacc_svm = cross_val_score(svm_classifier, train_set, y, cv = 10, scoring = acc_sco)\nf1_svm = cross_val_score(svm_classifier, train_set, y, cv = 10, scoring = f1_sco)\n\nprint(np.mean(acc_svm))\nprint(np.mean(f1_svm))","d9e9e550":"svm_classifier = SVC(kernel = 'linear')\nacc_svm = cross_val_score(svm_classifier, train_set, y, cv = 10, scoring = acc_sco)\nf1_svm = cross_val_score(svm_classifier, train_set, y, cv = 10, scoring = f1_sco)\n\nprint(np.mean(acc_svm))\nprint(np.mean(f1_svm))","85893a66":"svm_classifier = SVC(kernel = 'poly')\nacc_svm = cross_val_score(svm_classifier, train_set, y, cv = 10, scoring = acc_sco)\nf1_svm = cross_val_score(svm_classifier, train_set, y, cv = 10, scoring = f1_sco)\n\nprint(np.mean(acc_svm))\nprint(np.mean(f1_svm))","b9dbca41":"# Naive Bayes.\nnb_classifier = GaussianNB()\nacc_nb = cross_val_score(nb_classifier, train_set, y, cv = 10, scoring = acc_sco)\nf1_nb = cross_val_score(nb_classifier, train_set, y, cv = 10, scoring = f1_sco)\n\nprint(np.mean(acc_nb))\nprint(np.mean(f1_nb))","e4d47109":"# LDA.\nlda_classifier = LinearDiscriminantAnalysis(n_components = 1)\nacc_lda = cross_val_score(lda_classifier, train_set, y, cv = 10, scoring = acc_sco)\nf1_lda = cross_val_score(lda_classifier, train_set, y, cv = 10, scoring = f1_sco)\n\nprint(np.mean(acc_lda))\nprint(np.mean(f1_lda))","f23679b8":"lda_classifier = LinearDiscriminantAnalysis(n_components = 2)\nacc_lda2 = cross_val_score(lda_classifier, train_set, y, cv = 10, scoring = acc_sco)\nf1_lda2 = cross_val_score(lda_classifier, train_set, y, cv = 10, scoring = f1_sco)\n\nprint(np.mean(acc_lda2))\nprint(np.mean(f1_lda2))","cba0e905":"# Logistic Regression.\nlr_classifier = LogisticRegression()\nacc_lr = cross_val_score(lr_classifier, train_set, y, cv = 10, scoring = acc_sco)\nf1_lr = cross_val_score(lr_classifier, train_set, y, cv = 10, scoring = f1_sco)\n\nprint(np.mean(acc_lr))\nprint(np.mean(f1_lr))","37c7d8d6":"all_model = pd.DataFrame(columns = ['model', 'avg_acc', 'avg_f1'])","bedc4d03":"all_model = all_model.append(pd.DataFrame({'model': 'kNN', 'avg_acc': np.mean(acc_knn), \n                                          'avg_f1': np.mean(f1_knn)}, index = [0]))","46e01836":"all_model = all_model.append(pd.DataFrame({'model': 'Random Forest', 'avg_acc': np.mean(acc_rf), \n                                          'avg_f1': np.mean(f1_rf)}, index = [0]))\nall_model = all_model.append(pd.DataFrame({'model': 'SVM', 'avg_acc': np.mean(acc_svm), \n                                          'avg_f1': np.mean(f1_svm)}, index = [0]))\nall_model = all_model.append(pd.DataFrame({'model': 'Naive Bayes', 'avg_acc': np.mean(acc_nb), \n                                          'avg_f1': np.mean(f1_nb)}, index = [0]))\nall_model = all_model.append(pd.DataFrame({'model': 'LDA', 'avg_acc': np.mean(acc_lda), \n                                          'avg_f1': np.mean(f1_lda)}, index = [0]))\nall_model = all_model.append(pd.DataFrame({'model': 'Logistic Regression', 'avg_acc': np.mean(acc_lr), \n                                          'avg_f1': np.mean(f1_lr)}, index = [0]))","1f845109":"print(all_model)","94ba1b5c":"all_model.set_index('model', inplace = True)\nall_model['avg_acc'].plot.bar(color = 'orange', figsize = (8, 6), yerr = list(all_model['avg_acc']))\nplt.title('Model Average Accuracy Comparision')\nplt.ylabel('Average Accuracy with F1 Score')\nall_model.reset_index(inplace = True)","2872bc93":"## Feature Engineering","8c9f149b":"#### There are several different categories of variables:\n#### 1. Individual Variables: these are characteristics of each individual rather than the household\n#### Boolean: Yes or No (0 or 1)\n#### Ordered Discrete: Integers with an ordering\n#### 2. Household variablesBoolean: Yes or No\n#### Ordered Discrete: Integers with an ordering\n#### Continuous numeric\n#### 3. Squared Variables: derived from squaring variables in the data\n#### 4. Id variables: identifies the data and should not be used as features","039fd063":"## Exploratory Data Analysis","7c8e8193":"## Model"}}