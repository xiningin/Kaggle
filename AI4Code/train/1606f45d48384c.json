{"cell_type":{"04ab9456":"code","8e109de1":"code","54417108":"code","e707b573":"code","b586c0cc":"code","9a8d697b":"code","4e7fed37":"code","c6544163":"code","45a344e7":"code","59871ae5":"code","7a0a7ab6":"code","cd31d357":"code","493c94a5":"code","4d0d01ab":"code","af587886":"code","c7275453":"code","d44f906b":"code","1a22ef98":"code","b68b4dce":"markdown","5e59948c":"markdown","2b809f99":"markdown","f8153b7b":"markdown","273bd78b":"markdown","02b2f2ef":"markdown","23ead267":"markdown","e82c64b9":"markdown","c436ecc0":"markdown","3078560e":"markdown","f366704e":"markdown","9c67e65d":"markdown","ce5d1e99":"markdown","582478c2":"markdown","ad72469f":"markdown","5cbb8da5":"markdown","3280925a":"markdown","f5444a49":"markdown","1205e76b":"markdown","c83e3574":"markdown","769ca7cb":"markdown","b8bb07b6":"markdown","51b35335":"markdown","7280a98e":"markdown","5a4d6ee7":"markdown"},"source":{"04ab9456":"import numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.simplefilter('ignore')","8e109de1":"from sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=100, n_features=2, \n                           n_redundant=0, n_informative=2, \n                           n_clusters_per_class=2, \n                           random_state=1)","54417108":"plt.scatter(X[:, 0], X[:, 1], marker='o', c=y,\n            s=25, edgecolor='k');","e707b573":"from sklearn.model_selection import train_test_split\n# default is 75% \/ 25% train-test split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)","b586c0cc":"from sklearn.linear_model import LogisticRegression\n\nLR_clf = LogisticRegression()","9a8d697b":"LR_clf.fit(X_train, y_train)","4e7fed37":"print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n     .format(LR_clf.score(X_train, y_train)))\nprint('Accuracy of Logistic regression classifier on test set: {:.2f}'\n     .format(LR_clf.score(X_valid, y_valid)))","c6544163":"def plot_decision_boundary(model, X, y):\n    x1, x2 = X[:, 0], X[:, 1]\n    x1_min, x1_max = x1.min() - 1, x1.max() + 1\n    x2_min, x2_max = x2.min() - 1, x2.max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.1),\n                         np.arange(x2_min, x2_max, 0.1))\n\n    Z = model.predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)\n\n    plt.contourf(xx1, xx2, Z, alpha=0.4)\n    plt.scatter(x1, x2, c=y, marker='o',\n                s=25, edgecolor='k');","45a344e7":"plot_decision_boundary(LR_clf, X, y)","59871ae5":"from sklearn.datasets import make_gaussian_quantiles\nX1, y1 = make_gaussian_quantiles(n_features=2, \n                                 n_classes=2, \n                                 random_state=12)\n\nplt.scatter(X1[:, 0], X1[:, 1], marker='o', c=y1,\n            s=25, edgecolor='k');\n\nX1_train, X1_valid, y1_train, y1_valid = train_test_split(X1, y1, random_state=0)","7a0a7ab6":"LR_clf2 = LogisticRegression().fit(X1_train, y1_train)\n\nprint('Accuracy of Logistic regression classifier on training set: {:.2f}'\n     .format(LR_clf2.score(X1_train, y1_train)))\nprint('Accuracy of Logistic regression classifier on test set: {:.2f}'\n     .format(LR_clf2.score(X1_valid, y1_valid)))\n\nplot_decision_boundary(LR_clf2, X1_train, y1_train)","cd31d357":"from sklearn.tree import DecisionTreeClassifier\nDT_clf = DecisionTreeClassifier().fit(X1_train, y1_train)\n\nprint('Accuracy of Decision Tree classifier on training set: {:.2f}'\n     .format(DT_clf.score(X1_train, y1_train)))\nprint('Accuracy of Decision Tree classifier on test set: {:.2f}'\n     .format(DT_clf.score(X1_valid, y1_valid)))\n\nplot_decision_boundary(DT_clf, X1, y1)","493c94a5":"from sklearn.neighbors import KNeighborsClassifier\nknn_clf = KNeighborsClassifier().fit(X1_train, y1_train)\n\nprint('Accuracy of K-NN classifier on training set: {:.2f}'\n     .format(knn_clf.score(X1_train, y1_train)))\nprint('Accuracy of K-NN classifier on test set: {:.2f}'\n     .format(knn_clf.score(X1_valid, y1_valid)))\n\nplot_decision_boundary(knn_clf, X1, y1)","4d0d01ab":"from sklearn.svm import SVC\nsvc_clf = SVC().fit(X1_train, y1_train)\n\nprint('Accuracy of Support Vector classifier on training set: {:.2f}'\n     .format(svc_clf.score(X1_train, y1_train)))\nprint('Accuracy of Support Vector classifier on test set: {:.2f}'\n     .format(svc_clf.score(X1_valid, y1_valid)))\n\nplot_decision_boundary(svc_clf, X1, y1)","af587886":"from sklearn.ensemble import RandomForestClassifier\nRF_clf = RandomForestClassifier(random_state=0).fit(X1_train, y1_train)\n\nprint('Accuracy of Random Forest classifier on training set: {:.2f}'\n     .format(RF_clf.score(X1_train, y1_train)))\nprint('Accuracy of Random Forest classifier on test set: {:.2f}'\n     .format(RF_clf.score(X1_valid, y1_valid)))\n\nplot_decision_boundary(RF_clf, X1, y1)","c7275453":"from sklearn.ensemble import VotingClassifier\n\neclf1 = VotingClassifier(estimators=[('lr', LR_clf2), \n            ('dt', DT_clf), ('knn', knn_clf),\n            ('rf', RF_clf), ('svc', svc_clf)], \n            voting='hard')\n\neclf1 = eclf1.fit(X1_train, y1_train)\n\nprint('Accuracy of Random Forest classifier on training set: {:.2f}'\n     .format(eclf1.score(X1_train, y1_train)))\nprint('Accuracy of Random Forest classifier on test set: {:.2f}'\n     .format(eclf1.score(X1_valid, y1_valid)))\n\nplot_decision_boundary(RF_clf, X1, y1)","d44f906b":"from sklearn.ensemble import VotingClassifier\n\neclf1 = VotingClassifier(estimators=[('lr', LR_clf2), \n            ('dt', DT_clf), ('knn', knn_clf),\n            ('rf', RF_clf)], \n            voting='soft', \n            flatten_transform=True)\n\neclf1 = eclf1.fit(X1_train, y1_train)\n\nprint('Accuracy of Random Forest classifier on training set: {:.2f}'\n     .format(eclf1.score(X1_train, y1_train)))\nprint('Accuracy of Random Forest classifier on test set: {:.2f}'\n     .format(eclf1.score(X1_valid, y1_valid)))\n\nplot_decision_boundary(RF_clf, X1, y1)","1a22ef98":"from sklearn.ensemble import VotingClassifier\n\neclf1 = VotingClassifier(estimators=[('lr', LR_clf2), \n            ('dt', DT_clf), ('knn', knn_clf),\n            ('rf', RF_clf)], \n            voting='soft', weights=[1, 1, 10, 10],\n            flatten_transform=True)\n\neclf1 = eclf1.fit(X1_train, y1_train)\n\nprint('Accuracy of Random Forest classifier on training set: {:.2f}'\n     .format(eclf1.score(X1_train, y1_train)))\nprint('Accuracy of Random Forest classifier on test set: {:.2f}'\n     .format(eclf1.score(X1_valid, y1_valid)))\n\nplot_decision_boundary(RF_clf, X1, y1)","b68b4dce":"[Decision Tree classifier](https:\/\/towardsdatascience.com\/decision-trees-in-machine-learning-641b9c4e8052):\n\n- Tree-like model of decisions\n- Also known as CART algorithm\n- Decision making is explicit and can be visualized\n- Sensitive to the training data used. Slight changes in the training can lead to vastly different trees. This is one of the reasons we prefer Random Forest ensemble given below.\n- Can create over-complex trees that over-fit to the training data\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/f3\/CART_tree_titanic_survivors.png\" width=\"300\" height=\"350\" \/>\n<p style=\"text-align: center;\"> Decision Tree classifier for Titanic dataset <\/p>\n","5e59948c":"Using the kernel trick to transform the feature space, SVM can learn non-linear decision boundaries. \n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/1\/1b\/Kernel_Machine.png\" width=\"350\" height=\"350\" \/>\n<p style=\"text-align: center;\"> Using Kernel trick for Support Vector Machines (SVM) classifier <\/p>","2b809f99":"<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/7\/72\/SVM_margin.png\" width=\"350\" height=\"350\" \/>\n<p style=\"text-align: center;\"> Support Vector Machines (SVM) classifier <\/p>\n","f8153b7b":"First we import python modules:","273bd78b":"Next we train the classifier using the training data:","02b2f2ef":"Now, we test the accuracy of the classifier on both training and testing dataset.","23ead267":"Logistic regression is perhaps not a good choice for this kind dataset. We first test this dataset with other classification algorithms, namely:\n- Decision Trees\n- k-Nearest Neighbors\n- Support Vector Machines (SVM)\n- Random Forests\n\nThese algorithms are briefly introduced here first and we will revisit to learn more about them in the exercise session.","e82c64b9":"We define the function to plot the decision boundaries of the classifier:","c436ecc0":"Let us split the dataset into training and testing sets:","3078560e":"The binary classification dataset containing two features (or variables) is plotted below. ","f366704e":"We create a logistic classifier using [`sklearn.linear_model.LogisticRegression`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)","9c67e65d":"The promise of neural networks is that they can learn almost any kind of decision boundary if we build our network deep enough with suitable architecture and train it long enough with sufficiently big dataset. All other methods have limitations in how complex of a boundary they can learn. However, unless we have an abundance of training examples and the computational power, simpler methods are a good answer to many problems in industry even today. ","ce5d1e99":"# Classification algorithms\n\nThis is one of the notebooks for the fourth session of the [Machine Learning workshop series at Harvey Mudd College](http:\/\/www.aashitak.com\/ML-Workshops\/). It involves a gentle introduction of logistic regression and a brief overview of the various classification algorithms with illustrative examples. \n\nTopics covered:\n- Logistic Regression\n- Decision Trees\n- k-Nearest Neighbours\n- Support Vector Machines\n- Random Forest ensemble\n- Voting Classifiers","582478c2":"Support Vector Machines (SVM):\n- An extension of linear classification\n- Separates the classes by choosing a hyperplane that maximizes the distance to the nearest data point on either side and hence known as maximum margin classifier.\n- The data points that are closest to the decision boundary on either side are called support vectors.\n- It minimize the classification error and at the same time maximizes the geometric margin ","ad72469f":"### Acknowledgements:\n\nThe credits for the images used above are as follows.\n- Image 1 and 3: https:\/\/github.com\/trekhleb\/machine-learning-octave\/tree\/master\/logistic-regression\n- Image 2: https:\/\/i.stack.imgur.com\/W7I4r.png\n- Image 6: https:\/\/commons.wikimedia.org\/wiki\/File:CART_tree_titanic_survivors.png\n- Image 7: https:\/\/www.mathworks.com\/matlabcentral\/fileexchange\/63621-knn-classifier\n- Image 8: https:\/\/commons.wikimedia.org\/wiki\/File:SVM_margin.png\n- Image 9: https:\/\/commons.wikimedia.org\/wiki\/File:Kernel_Machine.png\n\nThe inspiration for the code for the function `plot_decision_boundary` is taken from the following Github gist:\n- https:\/\/gist.github.com\/anandology\/772d44d291a9daa198d4\n\nReferences:\n- [Applied Machine Learning in Python](https:\/\/www.coursera.org\/learn\/python-machine-learning?specialization=data-science-python) course by University of Michigan in Coursera","5cbb8da5":"**Revision:**\n\nTerminology:  \n* Features: columns of our dataframe; independent (response) variables in our model\n* Target variable\n\n\nAs we learned in the previous session, the equation for simple linear regression is \n\n$$y_{pred} = w*x+b $$\n\nSimilarly, the linear regression for more than one features, say $x_1, x_2, \\dots, x_n$ is given by\n\n$$y_{pred} = w_1*x_1 + w_2*x_2 + \\cdots + w_n*x_n + b$$\n\nThe learning process for this model involves learning the weights (or parameters) $w_1, w_2, \\dots, w_n$ and $b$ by minimizing the cost function defined by mean-squared error. The weights are iteratively updated using the gradient descent algorithm to minimize the cost function in the fastest way.\n\n#### Logistic classification:\nFits a linear decision boundary to separate the classes. \n\n<img src=\"https:\/\/camo.githubusercontent.com\/f663cd4f29335972950dded4d422c07aeee8af55\/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a34473067737539327250684e2d636f397076315035414032782e706e67\" width=\"300\" height=\"250\" \/>\n<p style=\"text-align: center;\"> Logistic Regression classifier <\/p>\n\nFor binary classification, we assign the two classes the labels 0 and 1. The class labeled 1 is also called the positive class. The classifier predicts the probability ($p$) that an observation belongs to the positive class. The probability for the class labeled $0$ (or the negative class) would be $1-p$.\n\nTo build a linear classifier is same as finding a function for probability that gives a value close to 1 for points in the upper region (or the points in the positive class) and a value close to 0 for points in the lower region (or the points in the negative class). \n\nIn the context of neural networks, such a function is called an activation function. They are said to be fired or not depending on whether $f \\to 1$ or $f \\to 0$ for the input. The logistic classifiers are one of the simplest cases of the neural networks pared-down to a single layer. And they are called so, because they use one of the most commonly used activation function called logistic (or sigmoid) function. \n\n$$sig(t) = \\frac{1}{1+e^{-t}}$$\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/5\/53\/Sigmoid-function-2.svg\" width=400 \/>\n\nThe S-shaped curve is called sigmoid because of its shape and it was widely used in population growth models in the previous century and hence, the [name logistic](https:\/\/en.wikipedia.org\/wiki\/Logistic_function).\n\nOur main objective for a classification task is to find the optimal decision boundary to separate the classes. For the logistic regression, the boundary is linear. For the case of two features, this linear boundary is simply a line in 2-dimensional plane, whereas for three features, the linear boundary would be a linear plane separating the two classes in 3-dimensional plane and in general, a $n-1$ dimensional linear hyperplane in a $n$-dimensional space.\n\n<img src=\"https:\/\/camo.githubusercontent.com\/f663cd4f29335972950dded4d422c07aeee8af55\/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a34473067737539327250684e2d636f397076315035414032782e706e67\" width=\"300\" height=\"250\" \/>\n<p style=\"text-align: center;\"> Logistic Regression classifier <\/p>\n\nTo be able to visualize and understand intuitively, we will first crack the formulation of logistic classifier, also known by its misnomer logistic regression, in the case of two features, say $x_1$ and $x_2$, as seen in the figure above.\n\nMath question: We represent the following line using the equation $x_1-x_2-1=0$. How do we mathematically represent the two regions that are separated by this line?\n\n![](https:\/\/github.com\/AashitaK\/ML-Workshops\/blob\/master\/Session%204\/figures\/fig1.png?raw=true)\n\nThe region containing the origin is given by $x_1-x_2-1<0$ whereas the other one by $x_1-x_2-1>0$.\n\n![](https:\/\/github.com\/AashitaK\/ML-Workshops\/blob\/master\/Session%204\/figures\/fig2.png?raw=true)\n\nAs seen above, the points in one region is characterized by $w_1*x_1 + w_2*x_2+b<0$ and the other region by $w_1*x_1 + w_2*x_2+b>0$. We combine this with the logistic (sigmoid) function above to get the equation for logistic regression:\n\n$$Prob(y=1) = sig(w_1*x_1 + w_2*x_2 + \\cdots + w_n*x_n + b) $$ \n\nwhere $sig$ is the sigmoid logistic function defined above. \n\nObservations:\n* The output of the sigmoid lies between 0 and 1, which corresponds to the probability in our case. \n* The logistic function (and hence the probability) approximates to 1 for the large positive values, whereas it converges to 0 for large negative values. \n* The value for $w_1*x_1 + w_2*x_2 + \\cdots + w_n*x_n + b$ is positive for points in the region on one side of the line and negative for the other. The magnitude of the values (positive or negative) is higher for points far away from the line.\n* In view of the above equation for logistic regression and the properties of sigmoid logistic function, the points farther away from the line will be classified with a high probability to one class or the other, whereas the probability will be closer to 0 for points close to the line.\n \nIn general, we set the threshold for probability to be 0.5. This means that whenever $w_1*x_1 + w_2*x_2 + \\cdots + w_n*x_n + b \\geq 0$, it is classified to the positive class, whereas whenever $w_1*x_1 + \\cdots + w_n*x_n + b < 0$, it is classified to the negative class. The points for which the value for $w_1*x_1 + \\cdots + w_n*x_n + b$ is not large in magnitude have probabilities that are closer to 0.5. Such points needs to be classified with extra care, as we will see later on in evaluation metrics. \n\nThe weights $w_1, w_2, \\dots, w_n$ and $b$ are learned by minimizing the cost function, as seen in the previous session. The cost function used is called the cross-entropy log loss and is defined below.\n\nFor points with label $y=1$, the cost is\n\n$$ c(y, p) = - \\log(p) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{ if }\\ \\  y = 1$$\n\nwhereas for points with label $y=0$, the cost is\n\n$$ c(y, p) = - \\log(1-p) \\ \\  \\text{ if }\\ \\  y = 0$$\n\nThe cost function takes the average over the costs for all points. The costs for the two classes $y=0$ and $y=1$ can be summed up in the following formula.\n\n$$ J = \\frac{1}{N} \\sum_{i=1}^N c(y, p) = - \\frac{1}{N} \\sum_{i=1}^N y \\log(p) + (1-y) \\log(1-p)$$\n\nwhere $p=Prob(y=1)$.\n\nThe updates to the weights are made in a similar fashion as seen earlier for linear regression by minimizing the cost function using gradient descent algorithm.","3280925a":"[Random Forest ensemble](https:\/\/machinelearningmastery.com\/bagging-and-random-forest-ensemble-algorithms-for-machine-learning\/): \n- Ensemble of decision tree classifiers which means it combines predictions from multiple decision tree classifiers to give a more accurate prediction than any individual tree.\n- As noted earlier, decision trees are unstable, that is they are sensitive to the changes in training data. For the ensemble, we take bootstrap samples from the training data and built trees on it.\n- Prone to over-fitting, though less prone than individual decision trees.","f5444a49":"Next, we try the option of 'soft' voting that sums the probabilities for all classifiers in the ensemble and then picks the class with the highest average probability.","1205e76b":"The above binary classification dataset was clearly separable by linear boundary, but that is not the case often. Next, we generate another dataset that is not linearly separable.","c83e3574":"We can also add weights for the option of 'soft' voting to weigh the probabilities of the classifiers","769ca7cb":"We will first generate a dataset suitable for demonstration and applying classification algorithms using built-in function [`make_classification`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.make_classification.html) in scikit-learn.","b8bb07b6":"Smoother boundary usually suggests that it is less likely to be subjected to the noise and fluctations in training set and hence, less likely to over-fit. \n\nFor this dataset:\n* The support vector classifier with minimal difference between training and testing set accuracy and smoother decision boundary seems to have performed the best\n* The decision tree algorithm seems to have suffered the most from over-fitting. \n* The logistic regression classifier seems to be under-fitting on account of being a poor fit for the problem.","51b35335":"We build a logistic regression model on this dataset.","7280a98e":"k-Nearest Neighbors classifier:\n- Uses k-nearest neighbors from the training data to predict the label \n\n![](https:\/\/www.mathworks.com\/matlabcentral\/mlc-downloads\/downloads\/03faee64-e85e-4ea0-a2b4-e5964949e2d1\/d99b9a4d-618c-45f0-86d1-388bdf852c1d\/images\/screenshot.gif)\n\nThe default value for k is 5 in scikit-learn implementation.","5a4d6ee7":"## Voting classifiers:\nIn machine learning, we often achieve better performance by ensembling various classifiers predict a final output. There are different rules to combine the result from the classifiers such as hard voting (or simple majority rule), soft voting (taking average of predicted probabilities) and soft voting with weights (taking weighted average of predicted probabilities). Please refer [here](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/EnsembleVoteClassifier\/) for a more detailed illustration and explanation.\n\n\nWe implement the ensemble for the voting classifiers using [`sklearn.ensemble.VotingClassifier`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingClassifier.html). First, we use the option of 'hard' voting that uses majority rule voting."}}