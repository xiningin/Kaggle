{"cell_type":{"f3c28a75":"code","a4e497c5":"code","a79fdd2e":"code","a4252e37":"code","8eabe559":"code","ef2458ad":"code","2f96b3fc":"code","2ba63799":"code","835b4235":"code","1b740727":"code","c9a9e543":"code","b1f652ab":"code","076ac260":"code","f8e870e6":"markdown","b2ed65b2":"markdown","65095cd9":"markdown","a975be7c":"markdown","a6ce981d":"markdown","39677de5":"markdown","555faac6":"markdown","1d4c92c3":"markdown"},"source":{"f3c28a75":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a4e497c5":"# Author - Gowtham Ch\n# https:\/\/www.linkedin.com\/in\/gauthamchowta\/","a79fdd2e":"import numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import LabelBinarizer\nimport pandas as pd","a4252e37":"columns = ['sent', 'class']\nrows = []\n\nrows = [['This is my book', 'stmt'], \n        ['They are novels', 'stmt'],\n        ['have you read this book', 'question'],\n        ['who is the author', 'question'],\n        ['what are the characters', 'question'],\n        ['This is how I bought the book', 'stmt'],\n        ['I like fictions', 'stmt'],\n        ['what is your favorite book', 'question']]\n\ndf = pd.DataFrame(rows, columns=columns)\ndf","8eabe559":"def convert_to_BOW(corpus):\n    \n    # Given text return the BOW representation of the words\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(corpus)\n    name_index = dict([(name,index) for index,name in enumerate(vectorizer.get_feature_names())])\n    return X.toarray(),name_index","ef2458ad":"def count_based_on_class(X,y):\n    \n    y  = np.array(y)\n    lb = LabelBinarizer()\n    y = lb.fit_transform(y)\n    if y.shape[1] == 1:\n        y = np.concatenate((1 - y, y), axis=1)\n    \n    #Counts\n    count_matrix = np.matmul(y.T,X)\n    class_count = y.sum(axis=0)\n    return count_matrix,y,lb.classes_,class_count\n    ","2f96b3fc":"def feature_log_probabilities(count_matrix,alpha=1):\n    # Adding alpha to the count\n    print('Count Matrix')\n    print(count_matrix)\n    smoothed_version = count_matrix+alpha\n    # Calculating the number of words in a given class\n    print('Smoothed version')\n    print(smoothed_version)\n    den = smoothed_version.sum(axis = 1)\n    # Reshaping it to 2D column\n    den = den.reshape(-1,1)\n    print('Denominator - total words present in a given class')\n    print(den)\n    # probability is num\/den -- log probability is log(num)- log(den)\n    log_probabilities = np.log(smoothed_version)-np.log(den)\n    \n    return log_probabilities","2ba63799":"def calculate_prior_probs(class_count):\n    \n    num = class_count\n    den = class_count.sum()\n    \n    return np.log(num)-np.log(den)\n    ","835b4235":"def predict(query_point,log_probabilities,prior_probabilities,classes):\n    output = np.matmul(log_probabilities,query_point.T) + prior_probabilities\n    index = np.argmax(output)\n    return classes[index]","1b740727":"X,name_index = convert_to_BOW(df.sent)\ncount_matrix,y,classes,class_count = count_based_on_class(X,df['class'])\n\nlog_probabilities = feature_log_probabilities(count_matrix,alpha = 1)\nprior_probabilities = calculate_prior_probs(class_count)\n\noutput = predict(X[2],log_probabilities,prior_probabilities,classes)\n\nprint('Predicted class - ',output)\nprint('Actual class -',df['class'][2])","c9a9e543":"from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\nclf.fit(X, df['class'])\nprint('Sklearn feature log-probabilities\\n',clf.feature_log_prob_)\nprint('Manually implemented probabilities\\n',log_probabilities)\nprint('Difference between actual and expected implementation\\n'\n      ,log_probabilities-clf.feature_log_prob_)\nprint()\nprint('Sklearn predict',clf.predict(X[4:5]))\nprint('Manual predict',predict(X[4:5],log_probabilities,prior_probabilities,classes))","b1f652ab":"clf.class_log_prior_","076ac260":"prior_probabilities","f8e870e6":"3. Calculating probabilities:\nFor example to calculate P(\u2018are\u2019\/y=\u2019question\u2019) we use the below formula. Laplace smoothing is added to avoid a zero probability case.\n![](https:\/\/miro.medium.com\/max\/1500\/1*w2tbf6u7C68IQXBER-cKBQ.jpeg)","b2ed65b2":"Step - 1 Convert the text into BOW using CountVectorizer:  \nThis is straightforward and self-explanatory.","65095cd9":"Steps:\n1. Convert the data into a vector using BOW.\n2. Calculate the counts based on the class.\n3. Calculate all the likelihood probabilities.\n4. Calculate the prior probability.\n5. Calculate the posterior probability.","a975be7c":"![](https:\/\/miro.medium.com\/max\/1050\/1*8VUupjgX8_5zKzAPzsksIw.jpeg)> ","a6ce981d":"Confused about how the count_matrix is working, let us take an example and make it clear:\n","39677de5":"Now, we have the probabilities and a query point. We can simply do a weighted sum of probabilities and query point using matrix multiplications.\n","555faac6":"Step - 2 Calculate the counts based on classes  \n* First, do a one-hot encoding of the target values. I am using LabelBinarizer here. Check the sample output in the below code.\n*  The shape of y now will be (n_classes*n_datapoints) and the shape of X is (n_datapoints*n_features).\n* To get the count based on the class, it is simple enough to multiply the transpose of y with X.","1d4c92c3":"Steps in calculating feature_log_probabilities:\n1. We already have the counts \u2192 Just add alpha to them for doing laplace smoothing.\n2. Now, to this calculate the sum row-wise to get the number of words in a particular class.\n![](https:\/\/miro.medium.com\/max\/1050\/1*6JwAraX7J9amJP7m7uT5eQ.png)\n\nWhen alpha = 1:  \nAdding one to all the values in count_matrix.  \nCalculating row-wise sum. As per the formula to the denominator, we have to add 21*alpha (21 unique words), are we doing this?\n\n> Yes, here in the numerator to each and every value we have added alpha so when we do just sum we are basically doing row sum (count_matrix)+ 21*alpha."}}