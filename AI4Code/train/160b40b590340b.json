{"cell_type":{"2c05b8e1":"code","8bcc73eb":"code","d407fcbe":"code","22bb5690":"code","580977f2":"code","77d5e5ef":"code","e6643b27":"code","42546237":"code","e542a518":"code","a3ba5076":"code","4be1d558":"code","d1eb3c66":"code","08cc3648":"code","3fb57bcd":"code","d2c347b4":"code","cd9832d2":"code","bb605b1e":"code","2bed22be":"code","1ae986e4":"code","de9ecf73":"code","57565b9a":"code","605ecc9c":"code","5d988f59":"code","564a5ed5":"code","829269e2":"code","522eed3e":"code","3bf5b0e1":"code","ccc012f2":"code","c6b6f2fc":"code","3ffb7970":"code","5f599c6c":"code","8d13352a":"code","95c44d03":"code","d41e9851":"code","b7876dcc":"code","b4979f94":"code","d6ed31d2":"code","d425d9a4":"code","c1c18524":"code","a7ae8e7c":"code","17672d81":"code","8bc01795":"code","d92340d4":"code","5e6285b9":"code","bb8e91dc":"code","5470238d":"code","1978ee42":"code","6285b961":"code","aaf90e28":"code","50415289":"code","2f5c9b10":"code","78e54652":"code","18b3e2a6":"code","35d25d2b":"code","4fbb6ebe":"code","b200d900":"code","ea205f3c":"markdown","7edf0521":"markdown","ae6935d9":"markdown","98c0f546":"markdown","64e0234a":"markdown","62a14346":"markdown","962a7b6b":"markdown","c110891a":"markdown","0edef168":"markdown","09b9e19c":"markdown","3a485de9":"markdown","898ca962":"markdown","b27017a2":"markdown","6a25e797":"markdown","7afee47b":"markdown","7049a915":"markdown","ed721fcc":"markdown","3da1240c":"markdown","8c9be58b":"markdown","19e3e0b4":"markdown","c8ff8cd8":"markdown"},"source":{"2c05b8e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8bcc73eb":"# Scikit-Learn libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \nwarnings.filterwarnings('ignore')","d407fcbe":"# Keras libraries\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import BatchNormalization\nfrom keras.optimizers import SGD\nfrom keras.layers import Dropout","22bb5690":"# Initialising the CNN\nclassifier = Sequential()","580977f2":"# No. of filters is 32 with size of 3*3\n# Input image size is 32*32 and 3 is for color image\n# relu is the activation function in the convolution layer\nclassifier.add(Convolution2D(32,3,3,input_shape = (32,32,3),activation='relu'))","77d5e5ef":"# Maxpooling layers is use to extract the most important feature from the feature Map(collection of filter detector)\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\n","e6643b27":"\nclassifier.add(Convolution2D(32,3,3,activation='relu'))\n\n\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\n","42546237":"# Converting the matrix of size 2*2 into 1D Array\n# This Flatten 1D array will be  input layer to the ANN model\nclassifier.add(Flatten())","e542a518":"import tensorflow as tf","a3ba5076":"classifier.add(Dense(output_dim = 100 , activation ='relu'))","4be1d558":"classifier.add(BatchNormalization())","d1eb3c66":"classifier.add(Dense(output_dim = 1 , activation ='softmax'))","08cc3648":"optimizer = SGD(lr=0.0001, momentum=0.9)","3fb57bcd":"classifier.compile(optimizer =optimizer,\n                   loss='binary_crossentropy',metrics=['accuracy'])","d2c347b4":"pwd","cd9832d2":"# Set the default directory\n# import os \n# os.chdir(\"\/Users\/ds\/Desktop\/LykiQLabs\/train_data\")","bb605b1e":"classifier.summary()","2bed22be":"from keras.preprocessing.image import ImageDataGenerator\ntrain_datagen = ImageDataGenerator(\n        rescale=1.\/255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_data = train_datagen.flow_from_directory(\n        '..\/input\/parking-lots-dataset\/train_data\/train',\n        target_size=(32, 32),\n        batch_size=32,\n        class_mode='binary')\n\ntest_data = test_datagen.flow_from_directory(\n        '..\/input\/parking-lots-dataset\/train_data\/test',\n        target_size=(32, 32),\n        batch_size=32,\n        class_mode='binary')","1ae986e4":"import pandas as pd\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)","de9ecf73":"def create_stack_bar_data(col, df):\n    aggregated = df[col].value_counts().sort_index()\n    x_values = aggregated.index.tolist()\n    y_values = aggregated.values.tolist()\n    return x_values, y_values","57565b9a":"train = pd.DataFrame(train_data.classes, columns=['classes'])\ntest = pd.DataFrame(test_data.classes, columns=['classes'])","605ecc9c":"x1, y1 = create_stack_bar_data('classes', train)\nx1 = list(train_data.class_indices.keys())\n\ntrace1 = go.Bar(x=x1, y=y1, opacity=0.75, name=\"Class Count\")\nlayout = dict(height=600, width=600, title='Class Distribution in Training Data', legend=dict(orientation=\"h\"), \n                yaxis = dict(title = 'Class Count'))\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);","5d988f59":"x2, y2 = create_stack_bar_data('classes', test)\nx2 = list(test_data.class_indices.keys())\n\nGraph = go.Bar(x=x2, y=y2, opacity=0.75, name=\"Class Count\")\nlayout = dict(height=600, width=600, title='Class Distribution in Validation Data', legend=dict(orientation=\"v\"), \n                yaxis = dict(title = 'Class Count'))\nfig = go.Figure(data=[Graph], layout=layout);\niplot(fig);","564a5ed5":"history_object = classifier.fit_generator(\n                                        train_data,\n                                        steps_per_epoch=381,\n                                        epochs=10,\n                                        validation_data=test_data,\n                                        validation_steps=164)","829269e2":"import matplotlib.pyplot as plt\n# summarize history for accuracy\nplt.plot(history_object.history['accuracy'])\nplt.plot(history_object.history['val_accuracy'])\n\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history_object.history['loss'])\nplt.plot(history_object.history['val_loss'])\n\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","522eed3e":"import os\nfrom os import listdir, makedirs\nfrom os.path import join, exists, expanduser\n\nfrom keras import applications\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import optimizers\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras import backend as K\nimport tensorflow as tf","3bf5b0e1":"## dimensions of our images.\n# Size of the image is 224*224*3 \nimg_width, img_height = 224, 224\n\ntrain_data_dir = '..\/input\/parking-lots-dataset\/train_data\/train'\nvalidation_data_dir = '..\/input\/parking-lots-dataset\/train_data\/test'\nnb_train_samples = 381 \nnb_validation_samples = 164\nbatch_size = 16","ccc012f2":"train_datagen = ImageDataGenerator(\n    rescale=1. \/ 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')","c6b6f2fc":"#import inception with pre-trained weights. do not include fully #connected layers\ninception_base = applications.ResNet50(weights='imagenet', include_top=False)\n\n# add a global spatial average pooling layer\nx = inception_base.output\nx = GlobalAveragePooling2D()(x)\n# add a fully-connected layer\nx = Dense(100, activation='relu')(x)\n# and a fully connected output\/classification layer\npredictions = Dense(2, activation='softmax')(x)\n# create the full network so we can train on it\ninception_transfer = Model(inputs=inception_base.input, outputs=predictions)","3ffb7970":"\ninception_base_vanilla = applications.ResNet50(weights=None, include_top=False)\n\n# add a global spatial average pooling layer\nx = inception_base_vanilla.output\nx = GlobalAveragePooling2D()(x)\n# add a fully-connected layer\nx = Dense(200, activation='relu')(x)\n# and a fully connected output\/classification layer\npredictions = Dense(2, activation='softmax')(x)\n# create the full network so we can train on it\ninception_transfer_vanilla = Model(inputs=inception_base_vanilla.input, outputs=predictions)","5f599c6c":"# Compiling for ResNet50 pre-trained model\ninception_transfer.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n              metrics=['accuracy'])\n# Compiling for ResNet50 vanilla pre-trained model\ninception_transfer_vanilla.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","8d13352a":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","95c44d03":"import tensorflow as tf\nwith tf.device(\"\/device:GPU:0\"):\n    history_pretrained = inception_transfer.fit_generator(\n    train_generator,\n    epochs=15, shuffle = True, verbose = 1, validation_data = validation_generator)","d41e9851":"import matplotlib.pyplot as plt\n# summarize history for accuracy\nplt.plot(history_pretrained.history['accuracy'])\nplt.plot(history_pretrained.history['val_accuracy'])\n\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_pretrained.history['loss'])\nplt.plot(history_pretrained.history['val_loss'])\n\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","b7876dcc":"pwd","b4979f94":"# Save the model according to the conditions\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n# checkpoint\nfilepath=\"\/kaggle\/working\/weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n","d6ed31d2":"with tf.device(\"\/device:GPU:0\"):\n    history_vanilla = inception_transfer_vanilla.fit_generator(\n    train_generator,\n    epochs=15, shuffle = True, verbose = 1, validation_data = validation_generator, callbacks=callbacks_list)","d425d9a4":"import matplotlib.pyplot as plt\n# summarize history for accuracy\nplt.plot(history_vanilla.history['accuracy'])\nplt.plot(history_vanilla.history['val_accuracy'])\n\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_vanilla.history['loss'])\nplt.plot(history_vanilla.history['val_loss'])\n\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","c1c18524":"import matplotlib.pyplot as plt\n# summarize history for accuracy\nplt.plot(history_pretrained.history['val_accuracy'])\nplt.plot(history_vanilla.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Pretrained', 'Vanilla'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_pretrained.history['val_loss'])\nplt.plot(history_vanilla.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Pretrained', 'Vanilla'], loc='upper left')\nplt.show()","a7ae8e7c":"import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2","17672d81":"# Reading the image\nParking_space = cv2.imread('..\/input\/images\/parking_lot1.jpg')\n","8bc01795":"Parking_space = cv2.imread('..\/input\/newline\/scene1380.jpg')","d92340d4":"Parking_space","5e6285b9":"def get_image():\n    return np.copy(Parking_space)\n\ndef show_image(image):\n    plt.figure(figsize=(13,12))\n    #Before showing image, bgr color order transformed to rgb order\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    plt.xticks([])\n    plt.yticks([])\n    plt.show()","bb8e91dc":"show_image(get_image())","5470238d":"#To transfrom the colorspace from BGR to grayscale so as to make things simpler\ngrayimg = cv2.cvtColor(Parking_space,cv2.COLOR_BGR2GRAY)\n","1978ee42":"#To plot the image\nplt.figure(figsize=(13,12))\nplt.imshow(grayimg,cmap='gray') #cmap has been used as matplotlib uses some default colormap to plot grayscale images\n\nplt.xticks([]) #To get rid of the x-ticks and y-ticks on the image axis\nplt.yticks([])\nprint('New Image Shape',grayimg.shape)","6285b961":"#To understand this further, let's display one entire row of the image matrix\nprint('The first row of the image matrix contains',len(grayimg[1]),'pixels')\nprint(grayimg[1])","aaf90e28":"#Okay let's look at the distribution of the intensity values of all the pixels\nplt.figure(figsize=(10,5))\n\nplt.subplot(1,2,1)\nsns.distplot(grayimg.flatten(),kde=False)#This is to flatten the matrix and put the intensity values of all the pixels in one single row vector\nplt.title('Distribution of intensity values')\n\n#To zoom in on the distribution and see if there is more than one prominent peak \nplt.subplot(1,2,2)\nsns.distplot(grayimg.flatten(),kde=False) \nplt.ylim(0,30000) \nplt.title('Distribution of intensity values (Zoomed In)')","50415289":"from skimage.filters import threshold_otsu\nthresh_val = threshold_otsu(grayimg)\nprint('The optimal seperation value is',thresh_val)","2f5c9b10":"mask=np.where(grayimg>thresh_val,1,0)","78e54652":"#To plot the original image and mask side by side\nplt.figure(figsize=(13,12))\nplt.subplot(1,2,1)\nplt.imshow(grayimg,cmap='gray')\nplt.title('Original Image')\n\nplt.subplot(1,2,2)\nmaskimg = mask.copy()\nplt.imshow(maskimg, cmap='rainbow')\nplt.title('Mask')","18b3e2a6":"#cv2.Sobel arguments - the image, output depth, order of derivative of x, order of derivative of y, kernel\/filter matrix size\nsobelx = cv2.Sobel(grayimg,int(cv2.CV_64F),1,0,ksize=3) #ksize=3 means we'll be using the 3x3 Sobel filter\nsobely = cv2.Sobel(grayimg,int(cv2.CV_64F),0,1,ksize=3)\n\n#To plot the vertical and horizontal edge detectors side by side\nplt.figure(figsize=(13,12))\nplt.subplot(1,2,1)\nplt.imshow(sobelx,cmap='gray')\nplt.title('Sobel X (vertical edges)')\nplt.xticks([])\nplt.yticks([])\n\nplt.subplot(1,2,2)\nplt.imshow(sobely,cmap='gray')\nplt.xticks([])\nplt.yticks([])\nplt.title('Sobel Y (horizontal edges)')","35d25d2b":"'''\n#Plotting the original image\nplt.figure(figsize=(12,8))\nplt.subplot(1,2,1)\nplt.imshow(grayimg,cmap='gray')\nplt.title('Original image')\n'''\n#Now to combine the 2 sobel filters\n\nsobel = np.sqrt(np.square(sobelx) + np.square(sobely))\nplt.figure(figsize=(13,12))\nplt.subplot(1,1,1)\nplt.imshow(sobel,cmap='gray')\nplt.title('Sobel Filter')","4fbb6ebe":"#To highlight the problem areas\nplt.figure(figsize=(12,6))\nplt.subplot(1,3,1)\nplt.imshow(grayimg[348:360,485:521],cmap='gray')\nplt.title('Original image (zoomed in)')\nplt.xticks([])\nplt.yticks([])\n\nplt.subplot(1,3,2)\nplt.imshow(sobel[348:360,485:521],cmap='gray')\nplt.title('Sobel Filter (zoomed in)')\nplt.xticks([])\nplt.yticks([])\n\nplt.subplot(1,3,3)\nplt.imshow(maskimg[348:360,485:521], cmap='gray')\nplt.title('Otsu\/K-Means (zoomed in)')\nplt.xticks([])\nplt.yticks([])","b200d900":"#To highlight the problem areas\nplt.figure(figsize=(12,6))\nplt.subplot(1,3,1)\nplt.imshow(grayimg[345:445,488:537],cmap='gray')\nplt.title('Original image (zoomed in)')\nplt.xticks([])\nplt.yticks([])\n\nplt.subplot(1,3,2)\nplt.imshow(sobel[345:445,488:537],cmap='gray')\nplt.title('Sobel Filter (zoomed in)')\nplt.xticks([])\nplt.yticks([])\n\nplt.subplot(1,3,3)\nplt.imshow(maskimg[345:445,488:537], cmap='gray')\nplt.title('Otsu\/K-Means (zoomed in)')\nplt.xticks([])\nplt.yticks([])","ea205f3c":"# Parking Spot Detector","7edf0521":", callbacks=callbacks_list","ae6935d9":"<h2> Compile the CNN <\/h2>","98c0f546":"##### Flatten ","64e0234a":"##### Output Layer","62a14346":"# ResNet50 Pre-trained Model\nResNet-50 with the ImageNet weights. We remove the top so that we can add our own layer according to the number of our classes. We then add our own layers to complete the model.","962a7b6b":"Intuition of the convolution layers:\nFirst take the image as input in input layers.\nThen apply the no. of  feature detector on the image to extract the features from the image","c110891a":"# Fitting ResNet50 vanilla pre-trained model on train and validation data","0edef168":"<h2>Fully Connected <\/h2>\n","09b9e19c":"##### Convolution CNN Layer","3a485de9":"### Building the CNN model ","898ca962":"#####  Problem Statement :- To detect the avalability of  parking space  for cars","b27017a2":"<h2> Image Preprocessing and Augumentation <\/h2>","6a25e797":"# Comparison plot between ResNet50 and ResNet50 vanilla model","7afee47b":"# Fitting ResNet50 pre-trained model on train and validation data","7049a915":"##### Hidden Layer ","ed721fcc":"# Parking Spots Detector","3da1240c":"# Data Visualization","8c9be58b":"# ResNet50 vanilla  pre-trained model \nResNet-50 vanilla pre-trained model with the no weights.we do not include fully connected layers in this model.","19e3e0b4":"# Compiling the Models\n\nWe set the loss function, the optimization algorithm to be used and metrics to be calculated at the end of each epoch.","c8ff8cd8":"##### Maxpooling Layer"}}