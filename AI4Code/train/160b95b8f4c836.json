{"cell_type":{"80bfdb0d":"code","8a9d960a":"code","6c2860c0":"code","6277e98f":"code","c741f647":"code","07d5fb4e":"code","beedcd84":"code","daa78f9f":"code","ab4bd5c3":"code","7444a23e":"code","0aadf003":"code","92312870":"code","e8be0578":"code","198dd55b":"code","4d0d409c":"markdown","44e4e5c9":"markdown"},"source":{"80bfdb0d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8a9d960a":"import json\n\ndef parse_data(file):\n    for l in open(file,'r'):\n        yield json.loads(l)\n\ndata = list(parse_data('\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json'))\n\nsentences = []\nlabels = []\nurls = []\n\nfor item in data:\n    sentences.append(item['headline'])\n    labels.append(item['is_sarcastic'])\n    urls.append(item['article_link'])\n    \n# sentences","6c2860c0":"import tensorflow as tf\nfrom tensorflow import keras\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","6277e98f":"vocab_size = 10000\nembedding_dim = 16\nmax_length = 100\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\ntraining_size = 20000","c741f647":"training_sentences = sentences[0:training_size]\ntesting_sentences = sentences[training_size:]\ntraining_labels = labels[0:training_size]\ntesting_labels = labels[training_size:]","07d5fb4e":"tokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\n\ntraining_sequences = tokenizer.texts_to_sequences(training_sentences)\n\ntraining_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating = trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","beedcd84":"## classifying on Sentiment\n\nmodel = keras.Sequential([\n    # embedding = direction of each word is learned epoch by epoch\n    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    # adding up the vectors\n    # keras.layers.GlobalAveragePooling1D(),\n    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n#     keras.layers.Bidirectional(keras.layers.LSTM(64), return_sequences=True),\n    # keras.layers.Dense(24, activation='relu'),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","daa78f9f":"model.summary()","ab4bd5c3":"import numpy as np\ntraining_padded = np.array(training_padded)\ntraining_labels = np.array(training_labels)\ntesting_padded = np.array(testing_padded)\ntesting_labels = np.array(testing_labels)\n\n\nnum_epochs = 30\n\nhistory = model.fit(\n    training_padded, \n    training_labels,\n    epochs=num_epochs, \n    validation_data=(\n        testing_padded, testing_labels\n    ), \n    verbose=1\n)\n","7444a23e":"sentence = [\n    \"granny starting to fear spiders in the garden might be real\",\n    \"the weather today is bright and sunny\",\n    \"How I accidently grew a melon the size of a buffalo\",\n    \"Biden Campaign Whittles VP Shortlist Down To Either Woman Or Man With Long Hair\",\n    \"Theoretical Astro-Fetishists Posit Black Holes Could Be Used For Anonymous Sex Across Parallel Universes\",\n#     \"Facebook Announces Plan To Break Up U.S. Government Before It Becomes Too Powerful\",\n    \"Facebook announces new application\",\n    \"Facebook\",\n    \"A melon grew the size of a buffalo\",\n    \"One day I will crush you\",\n    \"My Advice To Anyone Starting A Business Is To Remember That Someday I Will Crush You\"\n]\n\n\n## Predictions on RNN\n\nsequences = tokenizer.texts_to_sequences(sentence)\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\npredictions = model.predict(padded)\n\n# print(predictions * 10)\nresult = []\nfor i in range (len(sentence)):\n    result.append((sentence[i], predictions[i]))\n    \nresult","0aadf003":"## classifying on Sentiment\n\nmodel2 = keras.Sequential([\n    # embedding = direction of each word is learned epoch by epoch\n    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    # adding up the vectors\n    keras.layers.GlobalAveragePooling1D(),\n    keras.layers.Dense(24, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","92312870":"model2.summary()","e8be0578":"num_epochs = 30\n\nhistory2 = model2.fit(\n    training_padded, \n    training_labels,\n    epochs=num_epochs, \n    validation_data=(\n        testing_padded, testing_labels\n    ), \n    verbose=1\n)\n","198dd55b":"sentence2 = [\n    \"granny starting to fear spiders in the garden might be real\",\n    \"the weather today is bright and sunny\",\n    \"How I accidently grew a melon the size of a buffalo\",\n    \"Biden Campaign Whittles VP Shortlist Down To Either Woman Or Man With Long Hair\",\n    \"Theoretical Astro-Fetishists Posit Black Holes Could Be Used For Anonymous Sex Across Parallel Universes\",\n#     \"Facebook Announces Plan To Break Up U.S. Government Before It Becomes Too Powerful\",\n    \"Facebook announces new application\",\n    \"Facebook\",\n    \"A melon grew the size of a buffalo\",\n    \"One day I will crush you\",\n    \"My Advice To Anyone Starting A Business Is To Remember That Someday I Will Crush You\"\n]\n\n\n\nsequences2 = tokenizer.texts_to_sequences(sentence2)\npadded2 = pad_sequences(sequences2, maxlen=max_length, padding=padding_type, truncating=trunc_type)\npredictions2 = model2.predict(padded2)\n\n# print(predictions * 10)\nresult2 = []\nfor i in range (len(sentence2)):\n    result2.append((sentence2[i], predictions2[i]))\n    \nresult2","4d0d409c":"## Predictions on LSTM (RNN)","44e4e5c9":"## Predictions on Simple Model"}}