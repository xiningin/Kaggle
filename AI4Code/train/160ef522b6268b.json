{"cell_type":{"c2047953":"code","e7701050":"code","3aa1c00a":"code","e9d0673e":"code","3e7b6d5a":"code","9e693d1b":"code","6edaed7a":"code","969c5f0b":"code","1887b3d6":"code","e3136414":"code","828b324d":"code","27b1e117":"code","8d2b9037":"code","a4eed447":"code","e7c46bad":"code","f4c25f4f":"code","836271af":"code","c1dc4e90":"code","39e1fec2":"code","276e4026":"code","79e6f072":"code","3ac06db7":"code","9f074f8e":"code","57405b64":"code","411ca20b":"code","d9a4d5f1":"code","7ec4808f":"code","af4439bc":"code","76f7e4d7":"code","489e2303":"code","35f924c9":"code","dd0020e9":"code","dc552525":"markdown","85def1a4":"markdown","3e85a981":"markdown","33ceff97":"markdown"},"source":{"c2047953":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support as error_metric\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.feature_selection import VarianceThreshold","e7701050":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","3aa1c00a":"train.head()","e9d0673e":"test.head()","3e7b6d5a":"#Check for null values\ntrain.isnull().values.any()","9e693d1b":"test.isnull().values.any()","6edaed7a":"train.drop('subject', axis =1, inplace=True)\ntest.drop('subject', axis =1, inplace=True)","969c5f0b":"train.head()","1887b3d6":"rem_cols2 = test.columns.tolist()","e3136414":"# We check the datatypes \ntrain.dtypes.value_counts()","828b324d":"test.dtypes.value_counts()","27b1e117":"train.describe()  #we see that the min = -1 and the max = +1. so no need for scaling","8d2b9037":"train.dtypes.tail()","a4eed447":"object_feature = train.dtypes == np.object\nobject_feature = train.columns[object_feature]\nobject_feature","e7c46bad":"train.Activity.value_counts()","f4c25f4f":"le = LabelEncoder()\nfor x in [train, test]:\n    x['Activity'] = le.fit_transform(x.Activity)\n    ","836271af":"train.Activity.sample(5)","c1dc4e90":"test.Activity.sample(5)","39e1fec2":"feature_cols = train.columns[: -1]   #exclude the Activity column\n#Calculate the correlation values\ncorrelated_values = train[feature_cols].corr()\n#stack the data and convert to a dataframe\n\ncorrelated_values = (correlated_values.stack().to_frame().reset_index()\n                    .rename(columns={'level_0': 'Feature_1', 'level_1': 'Feature_2', 0:'Correlations'}))\ncorrelated_values.head()","276e4026":"#create an abs_correlation column\ncorrelated_values['abs_correlation'] = correlated_values.Correlations.abs()\ncorrelated_values.head()","79e6f072":"#Picking most correlated features\ntrain_fields = correlated_values.sort_values('Correlations', ascending = False).query('abs_correlation>0.8')\ntrain_fields.sample(5)","3ac06db7":"#Getting the split indexes\n\nsplit_data = StratifiedShuffleSplit(n_splits = 1, test_size = 0.3, random_state = 42)\ntrain_idx, val_idx = next(split_data.split(train[feature_cols], train.Activity))\n\n#creating the dataframes\n\nx_train = train.loc[train_idx, feature_cols]\ny_train = train.loc[train_idx, 'Activity']\n\nx_val = train.loc[val_idx, feature_cols]\ny_val = train.loc[val_idx, 'Activity']","9f074f8e":"y_train.value_counts(normalize = True)","57405b64":"y_val.value_counts(normalize = True)","411ca20b":"#Same ratio of classes in both the train and validation data thanks to StratifiedShuffleSPlit","d9a4d5f1":"lr = LogisticRegression()\nlr_l2 = LogisticRegressionCV(Cs=10, cv=4, penalty='l2')\nrf = RandomForestClassifier(n_estimators = 10)\n\nsvm = lr.fit(x_train, y_train)\n\nknn = rf.fit(x_train, y_train)\n\nhybrid = lr_l2.fit(x_train, y_train)\n","7ec4808f":"#predict the classes and probability  for each\n\ny_predict = list()\ny_proba = list()\n\nlabels = ['svm', 'knn', 'hybrid']\nmodels = [lr, lr_l2, rf]\n\nfor lab, mod in zip(labels, models):\n    y_predict.append(pd.Series(mod.predict(x_val), name = lab))\n    y_proba.append(pd.Series(mod.predict_proba(x_val).max(axis=1), name = lab))\n    #.max(axis = 1) for a 1 dimensional dataframe\n\ny_predict = pd.concat(y_predict, axis = 1)\ny_proba = pd.concat(y_proba, axis = 1)\n\ny_predict.head()","af4439bc":"y_proba.head(10)","76f7e4d7":"metrics = list()\nconfusion_m = dict()\n\nfor lab in labels:\n    precision, recall, f_score, _ = error_metric(y_val, y_predict[lab], average = 'weighted')\n    \n    accuracy = accuracy_score(y_val, y_predict[lab])\n    \n    confusion_m[lab] = confusion_matrix(y_val, y_predict[lab])\n    \n    metrics.append(pd.Series({'Precision': precision, 'Recall': recall,\n                            'F_score': f_score, 'Accuracy': accuracy}, name = lab))\n    \nmetrics= pd.concat(metrics, axis =1) ","489e2303":"metrics","35f924c9":"fig, axList = plt.subplots(nrows=2, ncols=2)\naxList = axList.flatten()\nfig.set_size_inches(12, 10)\n\naxList[-1].axis('off')\n\nfor ax,lab in zip(axList[:-1], labels):\n    sns.heatmap(confusion_m[lab], ax=ax, annot=True, fmt='d');\n    ax.set(title=lab);\n    \nplt.tight_layout()","dd0020e9":"#Remeber>..\ntrain_fields.sample(5)","dc552525":"## 6 - Calculating the Error Metrics","85def1a4":"They have the same data types. That is, mostly floats and one object feature. Lets see what the object feature is abd extract it from the rest","3e85a981":"## 5 - Predictive Modelling","33ceff97":"## 4 - Splitting the data into train and validation "}}