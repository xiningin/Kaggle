{"cell_type":{"5d3957a3":"code","318818a2":"code","6e222396":"code","ce8757b8":"code","6431001d":"code","ffd321e5":"code","ee0a66b3":"code","38d5b51e":"code","a2500542":"code","437acd97":"code","9603e453":"code","9d3e4731":"code","c046a062":"code","a7d50bd2":"code","d9f6f28a":"markdown","6ccada55":"markdown","28ba59b5":"markdown","25331815":"markdown","6f2f16dc":"markdown","8a595538":"markdown","be661cee":"markdown","b654ef47":"markdown","51c8474c":"markdown","b1147215":"markdown"},"source":{"5d3957a3":"#IMPORTING\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm\nimport cv2","318818a2":"#LOADING THE DATA \n\nface_data = []\nfor img in tqdm(os.listdir('..\/input\/data\/data')):\n    path = '..\/input\/data\/data\/{}'.format(img)\n    \n    image = plt.imread(path)\n    image = image.astype('float32')\n    face_data.append(image)\n    \nface_data = np.array(face_data)    ","6e222396":"import keras\n\nfrom keras.models import Sequential, Model, Input\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers import Dense\nfrom keras.layers import Conv2D\nfrom keras.layers import Conv2DTranspose\nfrom keras.layers import MaxPool2D, AvgPool2D\nfrom keras.layers import UpSampling2D\nfrom keras.layers import Lambda\nfrom keras.layers import Activation\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Flatten\nfrom keras.layers import Reshape\nfrom keras.layers import Add, Multiply\nfrom keras.losses import mse, binary_crossentropy\nimport keras.backend as K","ce8757b8":"np.random.seed(20)\n\n#NUMBER OF DIMENSIONS IN THE LATENT SPACE\nlatent_dims = 512","6431001d":"encoder_in = Input(shape=(64,64,3))  \n\nencoder_l1 = Conv2D(filters=32, kernel_size=5, strides=1, padding='same', input_shape=(64,64,3))(encoder_in)\nencoder_l1 = BatchNormalization()(encoder_l1)\nencoder_l1 = Activation(LeakyReLU(0.2))(encoder_l1)\n\nencoder_l1 = Conv2D(filters=64, kernel_size=5, strides=2, padding='same')(encoder_l1)\nencoder_l1 = BatchNormalization()(encoder_l1)\nencoder_l1 = Activation(LeakyReLU(0.2))(encoder_l1)\n\n\nencoder_l2 = Conv2D(filters=128, kernel_size=5, strides=2, padding='same')(encoder_l1)\nencoder_l2 = BatchNormalization()(encoder_l2)\nencoder_l2 = Activation(LeakyReLU(0.2))(encoder_l2)\n\nencoder_l3 = Conv2D(filters=256, kernel_size=5, strides=2, padding='same')(encoder_l2)\nencoder_l3 = BatchNormalization()(encoder_l3)\nencoder_l3 = Activation(LeakyReLU(0.2))(encoder_l3)\n\n\nencoder_l4 = Conv2D(filters=512, kernel_size=5, strides=2, padding='same')(encoder_l3)\nencoder_l4 = BatchNormalization()(encoder_l4)\nencoder_l4 = Activation(LeakyReLU(0.2))(encoder_l4)\n\nflatten = Flatten()(encoder_l4)\n\nencoder_dense = Dense(1024)(flatten)\nencoder_dense = BatchNormalization()(encoder_dense)\nencoder_out = Activation(LeakyReLU(0.2))(encoder_dense)\n\n\nmu = Dense(latent_dims)(encoder_out)\nlog_var = Dense(latent_dims)(encoder_out)\n\n\nepsilon = Input(tensor=K.random_normal(shape=(K.shape(mu)[0], latent_dims)))  ##INPUT EPSILON FOR RANDOM SAMPLING\n\nsigma = Lambda(lambda x: K.exp(0.5 * x))(log_var) # CHANGE log_var INTO STANDARD DEVIATION(sigma)\nz_eps = Multiply()([sigma, epsilon])\n\nz = Add()([mu, z_eps])\n\nencoder=Model([encoder_in,epsilon], z)\nencoder.summary()\n","ffd321e5":"decoder = Sequential()\ndecoder.add(Dense(1024, input_shape=(latent_dims,)))\ndecoder.add(BatchNormalization())\ndecoder.add(Activation(LeakyReLU(0.2)))\n\ndecoder.add(Dense(8192))\ndecoder.add(BatchNormalization())\ndecoder.add(Activation(LeakyReLU(0.2)))\n\ndecoder.add(Reshape(target_shape=(4,4,512)))\n\ndecoder.add(Conv2DTranspose(filters=256, kernel_size=5, strides=2, padding='same'))\ndecoder.add(BatchNormalization())\ndecoder.add(Activation(LeakyReLU(0.2)))\n\ndecoder.add(Conv2DTranspose(filters=128, kernel_size=5, strides=2, padding='same'))\ndecoder.add(BatchNormalization())\ndecoder.add(Activation(LeakyReLU(0.2)))\n\ndecoder.add(Conv2DTranspose(filters=64, kernel_size=5, strides=2, padding='same'))\ndecoder.add(BatchNormalization())\ndecoder.add(Activation(LeakyReLU(0.2)))\n\n\ndecoder.add(Conv2DTranspose(filters=32, kernel_size=5, strides=2, padding='same'))\ndecoder.add(BatchNormalization())\ndecoder.add(Activation(LeakyReLU(0.2)))\n\ndecoder.add(Conv2DTranspose(filters=3, kernel_size=5, strides=1, padding='same'))\ndecoder.add(BatchNormalization())\ndecoder.add(Activation('sigmoid'))\n\ndecoder.summary()\n","ee0a66b3":"# COMBINING ENCODER AND DECODER TO COMPLETE THE VARIATIONAL AUTO ENCODER\n\nvae_preds = decoder(z)\nvae = Model([encoder_in, epsilon], vae_preds)\nvae.summary()","38d5b51e":"def reconstruction_loss(y_true, y_pred):\n    return K.mean(K.square(y_true - y_pred))\n\ndef kl_loss(y_true, y_pred):\n    kl_loss = - 0.5 * K.mean(1 + log_var - K.square(mu) - K.exp(log_var), axis=-1)\n    return kl_loss\n\ndef vae_loss(y_true, y_pred):\n    return reconstruction_loss(y_true, y_pred) + 0.03 * kl_loss(y_true, y_pred)   #scaling kl_loss by 0.03 seem to help\n","a2500542":"vae.compile(optimizer='adam', loss=vae_loss , metrics=[reconstruction_loss, kl_loss])","437acd97":"vae.fit(face_data,face_data, epochs=54, batch_size=64)","9603e453":"def plot_images(rows, cols, images, title):\n    grid = np.zeros(shape=(rows*64, cols*64, 3))\n    for row in range(rows):\n        for col in range(cols):\n            grid[row*64:(row+1)*64, col*64:(col+1)*64, :] = images[row*cols + col]\n\n    plt.figure(figsize=(40,40))       \n    plt.imshow(grid)\n    plt.title(title)\n    plt.show()","9d3e4731":"predictions = face_data[:200]\nplot_images(20,8,predictions,\"ORIGINAL FACES\")","c046a062":"predictions  = vae.predict(face_data[:200])\nplot_images(20,8,predictions, \"RECONSTRUCTED FACES\")","a7d50bd2":"predictions= decoder.predict(np.random.randn(200, latent_dims))\nplot_images(20,8,predictions, \"GENERATED FACES (NEW FACES)\")","d9f6f28a":"## Loss Functions\n\nReconstruction loss \n* Mean Squared Loss\n\nkl_loss\n* kullback leibler loss\n","6ccada55":"## Conclusion\n\nOur model has generated the images. Perhaps, I can improve the quality of predictions by adding a lot more encoding and decoding layers. VARIATIONAL AUTOENCODERS ARE GENERATIVE MODELS. THIS SHOWS US HOW CREATIVE A DEEP LEARNING MODEL CAN BE!\n","28ba59b5":"## Building the encoder\n\nThe encoder compresses the image via a series of convolutions into a smaller dimensional space (z).\n\nThe output is split into 2 blocks, the mean (mu) and log_variance (log_var), which are then recombined to predict a distribution rather than a value.","25331815":"## PREDICTIONS","6f2f16dc":"## GENERATIVE MODEL (VARIATIONAL AUTOENCODER)\nAn autoencoder is made up of two parts,encoder and decoder. ","8a595538":"## RECONSTRUCTED FACES or COMPRESSED (*which tries to replicate the original images*)","be661cee":"## NEWLY GENERATED FACES","b654ef47":"## Building a Decoder","51c8474c":"## Full Variational Autoencoder","b1147215":"## ORIGINAL FACES"}}