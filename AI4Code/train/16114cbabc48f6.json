{"cell_type":{"34391ac0":"code","5f300c3a":"code","d3968822":"code","dcd741c9":"code","460c24ac":"code","481c847d":"code","914948b2":"code","fb85be21":"code","fba6fd73":"code","df16c9ed":"code","3bfd0a96":"markdown","95ebc861":"markdown","69b61b95":"markdown","e74367fc":"markdown","1698604f":"markdown","9ac31844":"markdown","a6f63a19":"markdown","80d1c80e":"markdown","c4a1dbce":"markdown"},"source":{"34391ac0":"import string\nimport torch\nimport torch.nn as nn\n\n# define a character vocabulary depending on the use case, I ll use all the possible printable characters here.\ncharacter_vocabulary = [char for char in string.printable]\nprint(f\"Character Vocabulary:\\n{character_vocabulary}\\n\")","5f300c3a":"# making a character to index mapping which will be used to access the one hot encoded representation of characters\nchar_to_idx_map = {char: idx for idx, char in enumerate(character_vocabulary)}\nprint(f\"Character to Index Mapping:\\n{char_to_idx_map}\\n\")\n\n# convert each char in the vocabulary to one hot encoded vector\n# this is actually equivalent to having an identity matrix.\n\nohe_characters = torch.eye(n=len(character_vocabulary))  # using the eye method for identity matrix\nprint(f\"One hot encoded characters:\\n{ohe_characters}\\n\")\n\n# Printing the one hot encoded representations of the digit '1'\nohe_repr_a = ohe_characters[char_to_idx_map['1']]\nprint(f\"One hot encoded representation of 1:\\n{ohe_repr_a}\\n\")\n# note that the 1th index is set to 1 and rest all are zeros -> 10 is actual index of a in the char_to_idx_map","d3968822":"# Since we have the individual one hot encoded representations of all the characters,\n# we can now form the character level representation of the word\nword = \"google\"\n\n# the character level ohe word representation of the word can be formed by \n# horizontally stacking the separate representations of the individual characters\n\n# 1. Convert the word into character indices\nword_char_idx_tensor = torch.tensor([char_to_idx_map[char] for char in word])\nprint(f\"Indexed Representation of the word '{word}': {word_char_idx_tensor}\")\n\n# converting the indexed word to one hot representation\nword_ohe_repr = ohe_characters[word_char_idx_tensor].T\nprint(f\"One Hot Encoded Representation: \\n{word_ohe_repr}\")","dcd741c9":"# pass the input word representation through the convolution layer:\nconvolution_layer = nn.Conv1d(in_channels=100, out_channels=256, kernel_size=3, bias=False)\nconv_out = convolution_layer(word_ohe_repr.unsqueeze(0))\n\nprint(f\"Shape of convolution output: {conv_out.shape}\")\nprint(f\"Convolution Output:\\n{conv_out}\\n\")\n\n# adding an activation layer so as to add non linearity to the output\nactivation_layer = nn.Tanh()\nactivation_out = activation_layer(conv_out)\nactivation_out\n\nprint(f\"Shape of activation output: {activation_out.shape}\")\nprint(f\"Activation Output:\\n{activation_out}\")\n","460c24ac":"# Max Pooling Layer\nmax_pooling_layer = nn.MaxPool1d(kernel_size=conv_out.shape[2])\n\n# finally here is our word embedding of size -> 256\nword_embedding = max_pooling_layer(activation_out).squeeze()\n\nprint(f\"Final word embedding shape: {word_embedding.shape}\")\nprint(f\"Final word embedding:\\n{word_embedding}\")","481c847d":"from torch.nn.functional import pad","914948b2":"# The same process that we applied above can be employed for a sentence and even for an entire document.\n# In this next part, I will be doing exactly same process for a sentence.\n# This part has a small trick\/catch which is padding of all the words to the same length, so that we can pass the\n# input together at once and can get the convoluted output.\n\nsentence = \"I wanna work for google someday\"\nwords = sentence.split()  # split the sentence into words\n\nmax_length = max([len(word) for word in words])\nprint(f\"Max length is: {max_length}, padding every word to length {max_length}\")\n\n# this tensor below can just be considered as an empty list \n# data is appended to this empty tensor as we loop through it\nohe_words = torch.empty(size=(0, len(character_vocabulary), max_length))\n\nfor word in words:\n    idx_representation = [char_to_idx_map[char] for char in word]\n    print(f\"Idx representation for word '{word}': {idx_representation}\")\n    ohe_representation = ohe_characters[idx_representation].T\n    print(f\"One hot encoded representation shape:\\n{ohe_representation.shape}\")\n    padded_ohe_representation = pad(ohe_representation, (0, max_length-len(word)))\n    print(f\"Padded one hot encoded representation shape:\\n{padded_ohe_representation.shape}\\n\")\n    ohe_words = torch.cat((ohe_words, padded_ohe_representation.unsqueeze(dim=0)))\n\n\nprint(\"-\"*70)\nprint(f\"Shape of matrix containing all the word representations: {ohe_words.shape}\")\nprint(f\"{ohe_words.shape[0]} words, {ohe_words.shape[1]} vocab size, {ohe_words.shape[2]} max_length\")\n","fb85be21":"# pass the input through convolution, activation and finally a max_pooling layer\nconv_out = convolution_layer(ohe_words)\nprint(f\"Output shapeafter convolution: {conv_out.shape}\")\n\nactivation_out = activation_layer(conv_out)\nmax_pool_out = max_pooling_layer(activation_out)\n\nprint(f\"Output shape after maxpooling: {max_pool_out.shape}\")\n\n# Finally we can squeeze the tensor to get the embeddings\nword_embeddings = max_pool_out.squeeze()\n\nprint(f\"Final word_embeddings: \\n {word_embeddings}\")","fba6fd73":"# Summing it all up in a function\n\ndef get_char_wise_word_embeddings(sentence: str, vocab: str = None, embed_size: int = 256, kernel_size: int = 3):\n    \n    # setting up the vocab\n    if not vocab:\n        vocab = [char for char in string.printable]\n        \n    char_to_idx_map = {char: idx for idx, char in enumerate(vocab)}\n    ohe_characters = torch.eye(n=len(vocab))\n\n    words = sentence.split()  # split the sentence into words\n\n    max_length = max([len(word) for word in words])\n    \n    ohe_words = torch.empty(size=(0, len(vocab), max_length))\n\n    for word in words:\n        idx_representation = [char_to_idx_map[char] for char in word]\n        ohe_representation = ohe_characters[idx_representation].T\n        padded_ohe_representation = pad(ohe_representation, (0, max_length-len(word)))\n        ohe_words = torch.cat((ohe_words, padded_ohe_representation.unsqueeze(dim=0)))\n        \n    # Initialising the layers\n    convolution_layer = nn.Conv1d(in_channels=len(vocab), out_channels=embed_size, kernel_size=kernel_size, bias=False)\n    activation_layer = nn.Tanh()\n    max_pooling_layer = nn.MaxPool1d(kernel_size=max_length-kernel_size+1)\n        \n    conv_out = convolution_layer(ohe_words)\n    activation_out = activation_layer(conv_out)\n    max_pool_out = max_pooling_layer(activation_out)\n        \n    return max_pool_out.squeeze()","df16c9ed":"get_char_wise_word_embeddings(\"Thanks for viewing my notebook, please vote for it\", embed_size=256, kernel_size=5)","3bfd0a96":"### Step 1. Define a Character Vocabulary.\n\nChoose a set of characters which would best suit your usecase. Look at your dataset and the problem that you are solving and try to observe what all characters are being used in the dataset. \n\nFor example:\n- It could be a log dataset which could be have all the possible characters in the english language.\n- It could be a dataset which only has lowercase alphabets and digits in it.\n\nLet's dive into code and see how do we do this.","95ebc861":"## Part II. Word Embeddings for a whole sentence\/document.\n\nIn the above cells we saw how to work with just a single word and not with a set of words (forming a sentence or a document). This part includes a short trick and rest of the part remains exactly the same.\n\nThe trick here is that all of the words present in the sentence are padded to a max_length and then the convolution and max_pooling takes place. Finally, we have all the word embeddings in our hand.\n\nLet's take a quick look of how to achieve this using pytorch.","69b61b95":"### Step 4: Build a Convolution Layer and pass the word representation through it.\n\nWe can have the one-hot encoded representation of any word possible, Now the thing left for us is to pass this representation through a convolution layer which will apply a number of filters\/kernels to it.\n\nI won't be diving deep into what convolution is and how it is implemented internally. If you wanna look at how the convolution process takes place, you can refer to this Convolution Visualizer by Edward Z. Yang.\n\nConvolution Visualizer: https:\/\/ezyang.github.io\/convolution-visualizer\/index.html\n\nRead more about convolution here: https:\/\/machinelearningmastery.com\/convolutional-layers-for-deep-learning-neural-networks\/\n\nThe Convolution layer requires us to pass essential parameters, these are:\n- in_channels: this should be equal to the length of the voabulary.\n- out_channels: this should be equal to the embedding size that you want to have. These are actually the number of filters\/kernels that will be used in the convolution process\n- kernel_size: this is the size of the filter and you may want to experiment with it by chaning its values.\n\nShape of input and output:\n\nInput: len(vocab) X len(word)\n\nOutput: embed_size X (len(word) - size_of_filter + 1)","e74367fc":"### Step 5. Pass the above output from a MaxPooling Layer to get the final embedding\n\nHere, I am assuming that you already know what MaxPooling means. If you are unaware of it, you can think of it as selection of a max value from a given set of values.","1698604f":"## Summing it up!\n\nGiven below is a function which sums all of the process that we talked about. The function takes the following inputs:\n- sentence\n- vocab\n- embed_size\n- kernel_size\n\nand gives output the character level word embeddings for all of the words present in the sentence.","9ac31844":"# Character Level Word Embeddings using 1D CNN\n\nIn this notebook, I will be walking you through the steps required to develop a Character Level Word Emebedding. Please make sure that you read all the markdown cells as well as the comments mentioned within the code-cells.\n\nThe entire notebook is divided into multiple parts which will help in better understanding. So let's start!","a6f63a19":"# Conclusion and Next steps\n\nThe output from the above output can be fed to a network of dense layers and can be used for a variety of tasks (as the pre-trained word embeddings are used).\n\n**While the forward propagation and back propagation will take place, the kernels\/filters would be updated and would be learnt. So, in short, the kernels would be learnt throughout the learning process.**\n\n**At the time of inference, we would have the fixed kernel weights with us. We simply have to scan the kernels through the one hot encoded representation of the word and tadaaaaa, we have our character-level word embeddings with us.**\n\nHope that this tutorial helped you unfold the mystery behind character embeddings. Upvote pleassse ;)","80d1c80e":"### Step 3. Decompose a word into its corresponding characters and form a One Hot Encoded representation\n\nAt this step, we have the one hot encoded representations of all the possible characters in our vocabulary. Now, we will decompose a word into its character and convert the word into its one hot encoded representation.\n\nLet's take an example of the word: **'bee'**\n\nDecomposition Step - bee -> {b, e, e}\n\nOne Hot Encoded Representation (considering that vocab just has a,b,c,d,e): pick the one hot encoded characters and just place them together to form a matrix.\n\nb   e   e\n\n0   0   0\n\n1   0   0\n\n0   0   0\n\n0   0   0\n\n0   1   1\n\n\n*Perfect! Now let's quickly turn this into code.*","c4a1dbce":"### Step 2. Form a Character-to-Index mapping and One Hot Encoded Representation for each character.\n\nIn this step we will simply assign every character a unique index and convert every character to its one hot encoded representation.\n\n**Let's try to understand it a bit better using the example below:**\n\nsuppose we have a vocabulary of just 5 characters: **(a, b, c, d, e)**\n\nWe convert this to a char_to_index mapping as follows: **{a: 0, b: 1, c: 2, d: 3, e: 4}**\n\nNow we finally convert each of the character in our vocabulary to it's one hot encoded representation:\n\na   b   c   d   e\n\n1   0   0   0   0\n\n0   1   0   0   0\n\n0   0   1   0   0\n\n0   0   0   1   0\n\n0   0   0   0   1\n\n**(Note - If you closely look at the above one hot encoded representation of all the characters together, you will notice that it is actually an Identity Matrix of shape 'n X n' where n = no. of characters present in our vocabulary)**\n\n*Yaaay! Now we have the one hot encoded representation of every character in our vocabulary.*\n\nLet's see how do we achieve this in code:"}}