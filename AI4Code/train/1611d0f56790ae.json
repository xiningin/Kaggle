{"cell_type":{"7b24e49b":"code","d65e5407":"code","218b1095":"code","e702bba8":"code","b75dc742":"code","1483bba8":"code","1ac0f5d5":"code","71344c07":"code","cbf24edf":"code","f2341c9b":"code","f1d63dd0":"code","026512bf":"code","eaece09c":"code","01159bbd":"code","304d832f":"code","29ad5215":"code","1948a076":"code","f4652d4b":"code","b9cc6290":"code","c904891a":"code","eee267fa":"code","e2f21b95":"code","37b6e6f9":"code","ae1d5eb0":"code","ef6b3d12":"code","42ad8367":"code","289bbadb":"code","b3072c9d":"code","20698014":"code","12eb8013":"code","9dae6c77":"code","61d6c2db":"code","23ad7483":"code","ea861285":"code","123f4fbb":"code","ddf87ffa":"code","f8c2b8e7":"code","994e6fbb":"code","a54e2bc6":"code","bd1832b9":"code","aaef6a4b":"code","c0cdfd8a":"code","24431902":"code","3d23619f":"code","7b4e61b6":"code","0ed4e6af":"code","4e1c6073":"code","62ece0b6":"code","798c93eb":"code","34e2f2a8":"code","188bc968":"code","79628e12":"code","3ef07496":"code","28478246":"code","0a66eaf4":"code","b3bb7548":"code","7395bf4c":"code","f6673a02":"code","88d87ecc":"code","f2ad9b72":"code","5ca8d424":"code","04637855":"code","3e80539e":"code","86c85c13":"code","53300843":"code","26362c3a":"code","5e2cd7c9":"code","fadbc485":"code","3481e376":"code","df433822":"code","27c56b95":"code","bcf825ec":"code","09075cb2":"code","ced7af21":"code","0c99a20a":"code","eda054b4":"code","a616e366":"code","1160c105":"code","2fd750e5":"code","67fe7e59":"markdown","20e753d6":"markdown","b4045b28":"markdown","90ce6f07":"markdown","3e7ade90":"markdown","6ee988cc":"markdown","34f84ae4":"markdown","4c032caa":"markdown","22c4dc18":"markdown","1e3b2a4c":"markdown","3973953e":"markdown","ab34b92a":"markdown","2410eb81":"markdown","966c7958":"markdown","ea6a4115":"markdown","c5224ba0":"markdown","a77ab5e9":"markdown","86afac09":"markdown","0bd93163":"markdown","0dcfa3a0":"markdown","e76dbf9a":"markdown","8679a90c":"markdown","8d52d904":"markdown","4fb86aa8":"markdown","38269471":"markdown","ea819cbd":"markdown","0ad96443":"markdown","ebfae534":"markdown","e00bba70":"markdown","04103383":"markdown","d25de74e":"markdown","9af73685":"markdown","ead5638c":"markdown","e547ee8b":"markdown","903fc028":"markdown","c26104d4":"markdown","a01858dc":"markdown","e18dcedf":"markdown","e8805957":"markdown","e94c197a":"markdown","df1b852f":"markdown","a2b3b771":"markdown","6b3bf9ec":"markdown","c688fb43":"markdown","7794b205":"markdown","b0da4034":"markdown","09b280b3":"markdown","202753b5":"markdown","a36660e4":"markdown","1526eb98":"markdown","92c2bbcc":"markdown"},"source":{"7b24e49b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\nkoi_cumm_path = os.path.join('..\/input', 'koi-cummulative\/koi_cummulative.csv')","d65e5407":"dfc = pd.read_csv(koi_cumm_path)\ndfc.shape","218b1095":"dfc = pd.read_csv(koi_cumm_path)\ndfc['koi_disposition'].unique()","e702bba8":"dfc['koi_pdisposition'].unique()","b75dc742":"# 2418 candidates\n\n(dfc['koi_disposition'] == \"CANDIDATE\").value_counts()","1483bba8":"dfc.head(10)    # first 20 samples","1ac0f5d5":"# the columns\n\ndfc.columns","71344c07":"dfc.info()","cbf24edf":"# all the non-numeric columns\n\ndf_numeric = dfc.copy()\n\nkoi_disposition_labels = {\n    \"koi_disposition\": {\n        \"CONFIRMED\": 1,\n        \"FALSE POSITIVE\": 0,\n        \"CANDIDATE\": 2,\n        \"NOT DISPOSITIONED\": 3\n    },\n    \"koi_pdisposition\": {\n        \"CONFIRMED\": 1,\n        \"FALSE POSITIVE\": 0,\n        \"CANDIDATE\": 2,\n        \"NOT DISPOSITIONED\": 3\n    }\n}\n\ndf_numeric.replace(koi_disposition_labels, inplace=True)\ndf_numeric","f2341c9b":"# this is train data\n\n# first we remove all string type columns from the dataframe\n\ndf_numeric = df_numeric.select_dtypes(exclude=['object']).copy()\ndf_test = df_numeric.copy()    # test data\n\n# second, we manually remove some columns which are not needed as mentioned above. \n# additionally, 'koi_teq_err1' and 'koi_teq_err2' have all null values so they too need to be removed\n\nrem_cols = ['kepid', 'koi_pdisposition', 'koi_score', 'koi_time0bk', 'koi_time0bk_err1', 'koi_time0bk_err2', 'koi_teq_err1', 'koi_teq_err2']\ndf_numeric.drop(rem_cols, axis=1, inplace=True)\n\n# this is test data\nrem_cols_test = [col for col in rem_cols if col not in ['koi_pdisposition', 'koi_score']]\ndf_test.drop(rem_cols_test, axis=1, inplace=True)\n\n\n\ndf_numeric.head()","f1d63dd0":"df_test.head()","026512bf":"df_numeric = df_numeric[df_numeric.isnull().sum(axis=1) == 0]\ndf_numeric.describe()","eaece09c":"index = df_numeric[df_numeric.koi_fpflag_nt == df_numeric.koi_fpflag_nt.max()].index\ndf_numeric.drop(index, inplace=True)","01159bbd":"df_numeric.info()","304d832f":"df_test = df_test[df_test.isnull().sum(axis=1) == 0]\ndf_test.info()","29ad5215":"df_test = df_test[df_test.koi_disposition == 2]\ndf_test","1948a076":"df_test.to_csv('koi_test.csv')","f4652d4b":"df_numeric.to_csv('koi_numeric.csv')","b9cc6290":"df_numeric1 = df_numeric.copy()","c904891a":"df_numeric1.info()","eee267fa":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(30, 30))\nsns.heatmap(df_numeric1.corr(), annot=True, cmap=\"RdYlGn\", ax=ax)","e2f21b95":"from sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler()\n\n# need to exclude the `koi_disposition` column from being standardized\n\n\ndf_numeric1.iloc[:, 5:] = std_scaler.fit_transform(df_numeric1.iloc[:, 5:])\n\n\n# df_numeric.iloc[:, 0].to_numpy().reshape(-1, 1).shape\n# df_standardized_w_labels = np.c_[df_standardized, df_numeric.iloc[:, 0].to_numpy().reshape(-1, 1)]\n# df_standardized_w_labels[:3]\n\ndf_numeric1.values","37b6e6f9":"import torch\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import random_split","ae1d5eb0":"class KeplerDataset(Dataset):\n    def __init__(self, test=False):\n        self.dataframe_orig = pd.read_csv(koi_cumm_path)\n\n        if (test == False):\n            self.data = df_numeric1[( df_numeric1.koi_disposition == 1 ) | ( df_numeric1.koi_disposition == 0 )].values\n        else:\n            self.data = df_numeric1[~(( df_numeric1.koi_disposition == 1 ) | ( df_numeric1.koi_disposition == 0 ))].values\n            \n        self.X_data = torch.FloatTensor(self.data[:, 1:])\n        self.y_data = torch.FloatTensor(self.data[:, 0])\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        return self.X_data[index], self.y_data[index]\n    \n    def get_col_len(self):\n        return self.X_data.shape[1]\n    \nkepler_df = KeplerDataset()","ef6b3d12":"feature, target = kepler_df[1]\ntarget, feature","42ad8367":"kepler_df.get_col_len()","289bbadb":"# splitting into training and validation set\n\ntorch.manual_seed(42)\n\nsplit_ratio = .7 # 70 \/ 30 split\n\ntrain_size = int(len(kepler_df) * split_ratio)\nval_size = len(kepler_df) - train_size\ntrain_ds, val_ds = random_split(kepler_df, [train_size, val_size])\n\nlen(train_ds), len(val_ds)","b3072c9d":"batch_size = 32\n\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size, num_workers=4, pin_memory=True)","20698014":"for features, target in train_loader:\n    print(features.size(), target.size())\n    break","12eb8013":"class KOIClassifier(nn.Module):\n    def __init__(self, input_dim, out_dim):\n        super(KOIClassifier, self).__init__()\n        self.linear1 = nn.Linear(input_dim, 32)    \n        self.linear2 = nn.Linear(32, 32)\n        self.linear3 = nn.Linear(32, 16)\n        self.linear4 = nn.Linear(16, 8)\n        self.linear5 = nn.Linear(8, out_dim)\n        \n        \n        \n    def forward(self, xb):\n        out = self.linear1(xb)\n        out = torch.sigmoid(out)\n        out = self.linear2(out)\n        out = torch.sigmoid(out)\n        out = self.linear3(out)\n        out = torch.sigmoid(out)\n        out = self.linear4(out)\n        out = torch.sigmoid(out)\n        out = self.linear5(out)\n        out = torch.sigmoid(out)\n\n    \n        return out\n    \n    \n    def predict(self, x):\n        pred = self.forward(x)\n        return pred\n    \n        \n    def print_params(self):\n        for params in self.parameters():\n            print(params)\n","9dae6c77":"input_dim = kepler_df.get_col_len()\nout_dim = 1\nmodel = KOIClassifier(input_dim, out_dim)\n\n\n","61d6c2db":"\"\"\"\n\nmodel_prev = KOIClassifier(input_dim, out_dim)\nconstruct = torch.load('..\/input\/first-nn-stats\/checkpoint.pth')\nmodel_prev.load_state_dict(construct['state_dict'])\n\nimport seaborn as sns\n%matplotlib inline\n\ncf_mat_train = pred_confusion_matrix(model_prev, train_loader)\ncf_mat_val = pred_confusion_matrix(model_prev, val_loader)\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 3))\n\nax1, ax2 = axes\nsns.heatmap(cf_mat_train, fmt='g', annot=True, ax=ax1)\nax1.set_title('Training Data')\n\nsns.heatmap(cf_mat_val, fmt='g', annot=True, ax=ax2)\nax2.set_title('Validation Data')\n\n\"\"\"","23ad7483":"# training phase\ncriterion = nn.BCELoss()\noptim = torch.optim.SGD(model.parameters(), lr=0.01)\nn_epochs = 1000\n\ndef train_model():\n    for X, y in train_loader:\n        for epoch in range(n_epochs):\n            optim.zero_grad()\n            y_pred = model.forward(X).flatten()\n            loss = criterion(y_pred, y)\n            loss.backward()\n            optim.step()\n\ntrain_model()","ea861285":"# testing the predictions\nfor X, y in train_loader:\n    y_pred = model.forward(X)\n    y_pred = y_pred > 0.5\n    y_pred = torch.tensor(y_pred, dtype=torch.int32)\n    print(y_pred)\n    break","123f4fbb":"from sklearn.metrics import confusion_matrix\ndef pred_confusion_matrix(model, loader):\n    with torch.no_grad():\n        all_preds = torch.tensor([])\n        all_true = torch.tensor([])\n        for X, y in loader:\n            y_pred = model(X)\n            y_pred = torch.tensor(y_pred > 0.5, dtype=torch.float32).flatten()\n            all_preds = torch.cat([all_preds, y_pred])\n\n            all_true = torch.cat([all_true, y])\n            \n    \n    return confusion_matrix(all_true.numpy(), all_preds.numpy())","ddf87ffa":"import seaborn as sns\n%matplotlib inline\n\ncf_mat_train = pred_confusion_matrix(model, train_loader)\ncf_mat_val = pred_confusion_matrix(model, val_loader)\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 3))\n\nax1, ax2 = axes\nsns.heatmap(cf_mat_train, fmt='g', annot=True, ax=ax1)\nax1.set_title('Training Data')\n\nsns.heatmap(cf_mat_val, fmt='g', annot=True, ax=ax2)\nax2.set_title('Validation Data')","f8c2b8e7":"checkpoint = {\n    'state_dict': model.state_dict(),\n    'optimizer': optim.state_dict()\n}\n\ntorch.save(checkpoint, 'checkpoint.pth')","994e6fbb":"# this is where we return back to the point from where we branched, we take the numeric dataframe again and apply some feature selection\ndf_new = pd.read_csv('koi_numeric.csv', index_col=0)\ndf_new.head()","a54e2bc6":"# a function to remove high correlation columns by selecting the upper triangle of the correlation matrix\n# and dropping all columns which have corr value > threshold at any row\n\ndef remove_high_corr(df, threshold):\n    corr_mat = df.corr()\n    trimask = corr_mat.abs().mask(~np.triu(np.ones(corr_mat.shape, dtype=bool), k=1))\n    blocklist = [col for col in trimask.columns if (trimask[col] > threshold).any()]\n    df.drop(columns=blocklist, axis=1,inplace=True)\n    return blocklist","bd1832b9":"remove_high_corr(df_new, 0.80)","aaef6a4b":"fig, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(df_new.corr(), cmap=\"Blues\", ax=ax)","c0cdfd8a":"df_new.head()","24431902":"df_new.to_csv('koi_numeric_reduced.csv')","3d23619f":"def get_default_device():\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n\ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)\n    \n    \ndevice = get_default_device()\ndevice","7b4e61b6":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nstd_scaler = StandardScaler()\n\ndataframe = pd.read_csv('koi_numeric_reduced.csv', index_col=0)\n\ntrain_data = dataframe.query('not koi_disposition == 2').values\n\nX = train_data[:, 1:]\ny = train_data[:, 0]\n\nval_size = .3\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, test_size=val_size, shuffle=True)\n\ntrain_X[:, 4:] = std_scaler.fit_transform(train_X[:, 4:])\nval_X[:, 4:] = std_scaler.fit_transform(val_X[:, 4:])\n\n\n# print(f'train_X = {train_X.shape}\\n\\nval_X = {val_X.shape}\\n')\n\nclass KOIDataset(Dataset):\n    def __init__(self, X_data, y_data):\n        self.X_data = torch.FloatTensor(X_data)\n        self.y_data = torch.FloatTensor(y_data)\n        \n    \n    def __len__(self):\n        return len(self.X_data)\n    \n    def __getitem__(self, index):\n        return self.X_data[index], self.y_data[index]\n    \n\n\n    \n\ntrain_ds = KOIDataset(train_X, train_y)\nval_ds = KOIDataset(val_X, val_y)\n\nfor feature, target in train_ds:\n    print(feature, target)\n    break\n    \n","0ed4e6af":"batch_size = 64\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size, num_workers=4, pin_memory=True)\n","4e1c6073":"train_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)\n","62ece0b6":"for features, target in train_loader:\n    print(target, features)\n    break","798c93eb":"# a function to measure prediction accuracy \n\ndef accuracy(outputs, labels):\n    output_labels = torch.round(torch.sigmoid(outputs))    # manually have to activate sigmoid since the nn does not incorporate sigmoid at final layer\n    \n    return torch.tensor(torch.sum(output_labels == labels.unsqueeze(1)).item() \/ len(output_labels))\n    ","34e2f2a8":"from collections import OrderedDict\n\ninput_dim = train_X.shape[1]\n\nclass KOIClassifierSeq(nn.Module):\n    def __init__(self):\n        super(KOIClassifierSeq, self).__init__()\n        self.model = nn.Sequential(OrderedDict([\n              ('fc1', nn.Linear(input_dim, 24)),\n              ('sigmoid1', nn.Sigmoid()),\n              ('batchnorm1', nn.BatchNorm1d(24)),\n              ('fc2', nn.Linear(24, 16)),\n              ('sigmoid2', nn.Sigmoid()),\n              ('batchnorm2', nn.BatchNorm1d(16)),\n              ('dropout', nn.Dropout(p=0.1)),\n              ('fc3', nn.Linear(16, 1))\n            ]))\n    \n    def forward(self, xb):\n        return self.model(xb)\n    \n    def training_step(self, batch):\n        features, label = batch \n        out = self(features)\n        loss = F.binary_cross_entropy_with_logits(out, label.unsqueeze(1)) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        features, label = batch \n        out = self(features)                    \n        loss = F.binary_cross_entropy_with_logits(out, label.unsqueeze(1))   # Calculate loss\n        acc = accuracy(out, label)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n    \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))","188bc968":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","79628e12":"model1 = to_device(KOIClassifierSeq(), device)\nmodel1","3ef07496":"num_epochs = 10\nlr = 1e-4\nhistory = fit(num_epochs, lr, model1, train_loader, val_loader, opt_func=torch.optim.Adam)","28478246":"num_epochs = 5\nlr = 1e-4\nhistory = fit(num_epochs, lr, model1, train_loader, val_loader, opt_func=torch.optim.Adam)","0a66eaf4":"# a function to calculate training accuracy\n\ndef train_accuracy(model):\n    train_acc = []\n    for X, y in train_loader:\n        out = model(X)\n        train_acc.append(accuracy(out, y))\n\n    return torch.stack(train_acc).mean().item()","b3bb7548":"train_accuracy(model1)","7395bf4c":"from sklearn.metrics import confusion_matrix\ndef pred_confusion_matrix(model, loader):\n    with torch.no_grad():\n        all_preds = to_device(torch.tensor([]), device)\n        all_true = to_device(torch.tensor([]), device)\n        for X, y in loader:\n            y_pred = model(X)\n            y_pred = torch.round(torch.sigmoid(y_pred))\n            all_preds = torch.cat([all_preds, y_pred])\n\n            all_true = torch.cat([all_true, y.unsqueeze(1)])\n            \n    \n    return confusion_matrix(all_true.cpu().numpy(), all_preds.cpu().numpy())","f6673a02":"cf_mat_train = pred_confusion_matrix(model1, train_loader)\ncf_mat_val = pred_confusion_matrix(model1, val_loader)\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 3))\n\nax1, ax2 = axes\nsns.heatmap(cf_mat_train, fmt='g', annot=True, ax=ax1)\nax1.set_title('Training Data')\n\nsns.heatmap(cf_mat_val, fmt='g', annot=True, ax=ax2)\nax2.set_title('Validation Data')","88d87ecc":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs')\n    \ndef plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs')","f2ad9b72":"plot_accuracies(history)","5ca8d424":"plot_losses(history)","04637855":"second_model = {\n    'state_dict': model1.state_dict()\n}\n\ntorch.save(second_model, 'second_model.pth')\n\n# I have uploaded the pth file to the \/input directory. If needed, you can load it from there and load it into a model instance of KOIClassifierSeq","3e80539e":"test_df = pd.read_csv('koi_test.csv', index_col=0)\ntest_df","86c85c13":"cols = [\n 'koi_disposition',\n 'koi_pdisposition',\n 'koi_period_err2',\n 'koi_impact_err2',\n 'koi_duration_err2',\n 'koi_depth_err2',\n 'koi_prad_err2',\n 'koi_insol_err1',\n 'koi_insol_err2',\n 'koi_steff_err2',\n 'koi_srad_err2']\n\ntest_df.drop(cols, axis=1, inplace=True)","53300843":"test_df.head()","26362c3a":"test_X = test_df.iloc[:, 1:].values\ntest_probs = test_df.iloc[:, 0].values\n\ntest_X[:, 4:] = std_scaler.fit_transform(test_X[:, 4:])\n\n  \n\nKOI_test = KOIDataset(test_X, test_probs)","5e2cd7c9":"batch_size = 64\ntest_loader = DataLoader(KOI_test, batch_size, num_workers=4, pin_memory=True)\ntest_loader = DeviceDataLoader(test_loader, device)\n\nfor X, y in test_loader:\n    print(X.size(), y.size())\n    break","fadbc485":"def predict_probs(model, X):\n    probs = torch.sigmoid(model(X))\n    return probs","3481e376":"torch.set_printoptions(precision=5, threshold=5000)\nwith torch.no_grad():\n    for X, y in test_loader:\n        #print(X, y)\n        preds = torch.sigmoid(model1(X))\n        for pred, true in zip(preds, y.unsqueeze(1)):\n            print(f'model prediction: {pred.item()}\\tKOI prediction: {true.item()}')\n        break","df433822":"def accuracy_test(outputs, label_prob):\n    output_labels = torch.round(torch.sigmoid(outputs))    \n    labels = torch.round(label_prob)\n    return torch.tensor(torch.sum(output_labels == labels.unsqueeze(1)).item() \/ len(output_labels))\n    \n    \ndef test_accuracy(model):\n    test_acc = []\n    with torch.no_grad():\n        for X, y in test_loader:\n            out = model(X)\n            test_acc.append(accuracy_test(out, y))\n\n    return torch.stack(test_acc).mean().item()","27c56b95":"test_accuracy(model1)","bcf825ec":"torch.save(model1.state_dict(), 'final_model_53_percent.pth')","09075cb2":"class KOIClassifierSimple(nn.Module):\n    def __init__(self):\n        super(KOIClassifierSimple, self).__init__()\n        self.model = nn.Sequential(OrderedDict([\n              ('fc1', nn.Linear(input_dim, 24)),\n              ('sigmoid1', nn.Sigmoid()),\n              ('fc2', nn.Linear(24, 16)),\n              ('sigmoid2', nn.Sigmoid()),\n              ('fc3', nn.Linear(16, 1))\n            ]))\n    \n    def forward(self, xb):\n        return self.model(xb)\n    \n    def training_step(self, batch):\n        features, label = batch \n        out = self(features)\n        loss = F.binary_cross_entropy_with_logits(out, label.unsqueeze(1)) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        features, label = batch \n        out = self(features)                    \n        loss = F.binary_cross_entropy_with_logits(out, label.unsqueeze(1))   # Calculate loss\n        acc = accuracy(out, label)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n    \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))","ced7af21":"model2 = to_device(KOIClassifierSimple(), device)\nmodel2","0c99a20a":"num_epochs = 10\nlr = 1e-3\nhistory2 = fit(num_epochs, lr, model2, train_loader, val_loader, opt_func=torch.optim.Adam)","eda054b4":"train_accuracy(model2)","a616e366":"plot_accuracies(history2)","1160c105":"plot_losses(history2)","2fd750e5":"test_accuracy(model2)","67fe7e59":"As we see, `koi_fpflag_nt` has an outlier max value of `465.0` which is improbable since it is a flag and all the other flags are 0 or 1 valued.","20e753d6":"As we see, a simpler model was able to give us a test accuracy of 67.9% which is a lot better than our previous model. This goes to show that a more complex model might not always be the go-to solution for every task. We could even use other machine learning algorithms like SVM or Decision Trees to come into agreeable accuracy. ","b4045b28":"Lets take a look at our dataset before deconstructing it.","90ce6f07":"As we can see, the predictions are not as accurate. ","3e7ade90":"Saving our model state similar as before. ","6ee988cc":"# Using a simpler model","34f84ae4":"Now we have a somewhat decent dataset, however, this dataset still has a lot of missing values. \n\nWe will simply discard the rows that have atleast one null entry and only consider the non-null dataset for our training.","4c032caa":"## Basic preprocessing","22c4dc18":"As we can see, both training and validation data are predicted super accurately. \nWe are not going to train any further. This is more than enough stats for a feedforward neural network classification.\n\nLet us plot the accuracies and predictions.","1e3b2a4c":"Test dataset should only contain `koi_disposition` value 2, since these are all candidate data. We will come back to this dataset later to predict","3973953e":"We create a copy of this dataframe now to use it for our first neural network training, but we will come back to this dataframe again later.\nFor now we are saving the `df_numeric` into a csv which can be later found in `input\/koinumeric\/koi_numeric.csv` file.","ab34b92a":"Finally, I have my model ready now. I ported the model to GPU again. The layers can be seen from the following output.","2410eb81":"The Cumulative data has 9564 data points, however, we will soon see that we wont be able to use all of them\n\nFor now, lets see what our Categorizations might be.\n\nEach observation in the KOI dataset has a disposition value which tells us what that Object is confirmed to be.\n\n> 1. CONFIRMED - confirmed planets, these are confirmed to be exoplanets. These are our positive examples.\n> 2. FALSE POSITIVE - as the name suggests, these were thought to be exoplanets but turned out to be false. These will serve us as negative examples.\n> 3. CANDIDATE - these are potential candidates for exoplanets. These will be used for prediction\n\nThe KOI dataset also has a `koi-pdisposition` value which tells us the most probable explanation. `koi-disposition` values are finalized after further observations and analyses. ","966c7958":"We got a test accuracy which is, unfortunately, not as good. But its a start. We will save the model at its current state.\n\n","ea6a4115":"This is where the training happens. \n* optimiser = SGD\n* number of epochs = 1000\n* learning-rate = 0.01\n* device of computation = CPU","c5224ba0":"We now want to filter out some more columns.\n\nkoi_score is not needed. It is the probability values for the categorization.\nHowever. these probability values will help us in the test phase on hunting new exoplanets among the candidates. So we will need it later.\n\nSO, it is ideal to make a copy of the dataframe at its current state because we will need to come back to some columns again.\n\n\nFinally, koi_time0bk and koi_time0bk_err1 and 2 are removed because they are the time of the first detected transit minus some offset which is not a useful feature.\n\n### for more info on columns check out https:\/\/exoplanetarchive.ipac.caltech.edu\/docs\/API_kepcandidate_columns.html","a77ab5e9":"We want to change the way our dataset class performs. Earlier we stiched together a bunch of modification but this time we want to maintain consistency.\nWe previously standardized the entire dataset, including the test set (which included `koi_disposition` value 2) which was a bad practice.\nWe will now only do standardization on the training data and validation data. \nWhen using test data we will do the standardization separately.","86afac09":"# Conclusion","0bd93163":"This time, I used a batch size of 64","0dcfa3a0":"## New feedforward network\n\n> The architecture is as followed.\n1. Input Layer (fully connected) (28 x 24)\n2. Sigmoid (Activation) (24)\n3. Batch Normalization Layer (1D) (24)\n4. Hidden Layer (1st) (24 x 16)\n5. Sigmoid (Activation) (16)\n6. Batch Normalization Layer (1D) (16)\n7. Dropout Layer with probability 0.1 (16)\n1. 8. Output Layer (fully connected) (16 x 1)\n\n","e76dbf9a":"So, at the end of training which was relatively fast, We have 97.7% training accuracy and 97.2% validation accuracy.\nLet us calculate confusion matrix and visualize our predictions.","8679a90c":"# Trying GPU accelaration, new Dataset and model architecture","8d52d904":"# Exoplanet prediction using feedforward net\n\nWe will use NASA's Kepler open exoplanet archive dataset (cumulative)\n\n[Source](https:\/\/exoplanetarchive.ipac.caltech.edu\/cgi-bin\/TblView\/nph-tblView?app=ExoTbls&config=cumulative)\n\nThis will be a binary classification task (i.e, planet or false positive aka not planet). The table contains useful properties of the planetary transit and other features like mass, orbital period etc. More details below.\n","4fb86aa8":"This is a rather simple feedforward architecture with just linear combinations and sigmoid activation.\nThe model architecture is as followed:\n\n1. Input-layer (fully connected) (37 x 32)\n2. Sigmoid activation (32)\n3. 1st Hidden-layer (fully connected) (32 x 16)\n4. Sigmoid activation (16)\n5. 2nd Hidden-layer (fully connected) (16 x 8)\n6. Sigmoid activation (8)\n7. Output-layer (fully connected) (8 x 1)\n8. Sigmoid activation (output) (1)\n\n\n**Note that this model incorporates sigmoid at the output layer, so BCELoss() is used.**","38269471":"# More preprocessing and feature selection","ea819cbd":"We perform standardization same as before.","0ad96443":"# First feedforward network model","ebfae534":"We use pandas.DataFrame.info() method to get more info on the dataset. As we see, so many columns have null values in them. Also so many columns which we do not need, like the kepler_name would not help in our classification at all.","e00bba70":"We filter out the non numeric columns as they would serve us no purpose. Except `koi_disposition` since that is for labelling purpose.\n\nNow, we need to encode the labels, ie, the koi_disposition values. We replace 'CONFIRMED' with 1 and 'FALSE POSITIVE' with 0 because we will use these label values for categorization. The other two values are arbitrary and serve only to filter out CANDIDATES and NOT DISPOSITIONED samples.\n\nAlso, we do the same with `koi-pdisposition` column as well.","04103383":"### We create a custom PyTorch dataset called `KeplerDataset` class by inheriting the `torch.utils.data.Dataset` class and overriding the `init` , `len` and `getitem` methods.\n### We have included a flag called `test` which, if set to True, will generate the dataset with `koi_disposition` value `2` or `CANDIDATE`, which we will use for testing","d25de74e":"Let us create two copies of the dataframe now, we will use one containing `koi-pdisposition` and `koi-score` for test phase, and the dataframe containing none of these in train phase.","9af73685":"Even though the model seems to perform exceptionally well, we have made some fatal mistakes. \nFirstly, we standardized the whole dataset, as a result, the informations about the test data got mixed up with the train data. The test data and train data should be separate.\nSecondly, we added so many columns which are not much needed, for example the columns which have very high correlation coefficient with some others.\n\nFinally, we need a more organized model with fewer parameters, otherwise we will risk overfitting.\n\n\nLet us try to reduce dimensions first by removing columns which have correlation coeffs higher than 0.80","ead5638c":"### I have already trained this model using the same hyperparameters, the stats are located in `\\input\\first-nn-stats`\n\n### If you want to use the previous stats then uncomment the following cell and run","e547ee8b":"There might be a number of reasons why our model failed to perform accurately in the test set. The test set is predominated by positive probabilities, with an uneven distribution of positive and negative candidates. For this reason our model might underperform. Another reason might be the case of overfitting. \nSimpler model is always better. Maybe by changing and tinkering with the network architecture a bit, we can come up to a decent enough prediction accuracy. \n\nAnd as evidently shown, a simpler model might be able to generalize better and give better estimations.\n\nAlthough, this is nowhere close to being an actual prediction modelling for exoplanet search. A much better analysis would be on time-series data or transit curve images using CNN architectures. \n\n\nHaving basically no idea about the deeper intricacies of Astronomy, and only relying on the column descriptions from the official website, it was pretty much a wild guess but the fact that a seemingly random prediction model was able to perform with 53% accuracy and then being able to get a 68% accuracy on the test data with an even simpler model was pretty nice! \n\nIf anyone is interested in tinkering with this notebook even more and has domain-specific knowledge as to which columns are more imoprtant in planetary predictions, feel free to fork this and modify it. Thanks!","903fc028":"# Importing the Dataset","c26104d4":"Now is a good time to save the `df_test` dataframe to csv for future use.","a01858dc":"So now we have a reduced dataset with 29 columns.","e18dcedf":"We plot a heatmap of the correlation matrix for our dataframe and we see that overall the data has a lot of uncertainties and very few columns are sufficiently correlated to the target `koi_disposition`","e8805957":"### Now that the preprocessing part is over, we want to create a PyTorch dataset to handle this data and create DataLoader batches\n","e94c197a":"Great! Now we can save this csv again and move on the the next parts","df1b852f":"Now, we want to split our data into training and validation set and also transfer the data to a cuda-enabled device before performing computations","a2b3b771":"Ported all the dataloaders to GPU for faster processing.","6b3bf9ec":"Let us save this reduced dataset also, so that we can use it in our pytorch dataset","c688fb43":"We try to standardize our dataframe because a lot of columns have huge values while others have very small values.","7794b205":"Congratulations. Now we have a complete dataframe with no null values, and also standardized for easier processing.","b0da4034":"We will now try a simpler model with only one hidden layer with no batchnorm or dropout layer and only sigmoid as activation. ","09b280b3":"The standardization process in the previous model was flawed because it standardized the entire dataset, introducing test data statistics into training and validation data.\nThis is bad, because then our model will be influenced by test data and will never truly learn anything. \n\nI therefore, used `sklearn.model_selection.train_test_split` to split the training data into training and validation data, and created a separate test data by filtering out based on `koi-disposition` values.\n\nI then used `StandardScaler` separately on each dataset to produce independently standardized samples.\n\nThe rest of it are similar as before.","202753b5":"this seems to perform really well. It got a steep jump in terms of accuracy. Let us keep training.","a36660e4":"We need to apply the same preprocessing steps on the test dataset as well.\n\nWe can remove `koi_disposition` column as only has value 2 for `CANDIDATE`,\nWe can remove `koi_pdisposition` aswell since it contains same data as `koi_disposition`.\nWe will use the `koi_score` to see our prediction accuracy.\nWe also have to remove the columns we had removed previously from the train and validation data, otherwise there will be a dimensionality mismatch.","1526eb98":"### Let us fit our model using Adam optimiser and a small learning rate `1e-5`","92c2bbcc":"# Testing on the Test data"}}