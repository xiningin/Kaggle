{"cell_type":{"cf8da1cf":"code","5a3b8732":"code","18fc921d":"code","a7cb28b6":"code","57642ce7":"code","f3516ae9":"code","5fe7307f":"code","4b09f1c8":"code","14e8a75a":"code","84b5de7a":"code","6025d754":"code","aa252834":"code","10e0a6c0":"code","8c7631af":"code","2f54e521":"code","1a6d52d8":"code","e94cda65":"code","06069834":"code","f6ee70cf":"code","aab9f891":"code","6bdec0ac":"code","53061d65":"code","ffcbc61d":"code","0ec8b049":"markdown","a68530ef":"markdown","9ef4c3b2":"markdown","efe1dfcb":"markdown","3f336060":"markdown","330e8a79":"markdown","01a5d900":"markdown","834b56cd":"markdown","6a78bd69":"markdown","04d12307":"markdown","0e65a41e":"markdown","be0bf1f0":"markdown","a2522c1d":"markdown","72952e6f":"markdown","f29b1a65":"markdown","af8aae01":"markdown","1ad5087e":"markdown","8ee13fdf":"markdown","50bf0a4b":"markdown","10bebf8e":"markdown","d5e95aaa":"markdown","07d2dc63":"markdown","035544fd":"markdown","9bd59c1a":"markdown","6c7a3ced":"markdown","dc4780af":"markdown"},"source":{"cf8da1cf":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.models import Sequential\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import Adam\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5a3b8732":"# Load the data\ntrain = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\nprint(f'train shape = {train.shape}\\ntest shape = {test.shape}')","18fc921d":"# Create the X and y variables and count the number of outputs\nX_train = train.drop('label', axis=1)\ny_train = train.label\nprint(f'X_train = {X_train.shape}')\nprint(f'test = {test.shape}')\n\n# Set the plots size\nsns.set(rc={'figure.figsize':(10,7)})\nsns.countplot(y_train)","a7cb28b6":"# Normalize the data \nX_train = X_train \/ 255.0\ntest = test \/ 255.0\n\n# Reshape the data\nX_train = X_train.values.reshape(-1, 28, 28, 1)\ntest = test.values.reshape(-1, 28, 28, 1)\n\n# label encoding \ny_train = to_categorical(y_train, num_classes = 10)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=5)\n\nprint(f'X_train = {X_train.shape}')\nprint(f'test = {test.shape}')","57642ce7":"# Take a look at the data\nrows = 5 # defining no. of rows in figure\ncols = 6 # defining no. of colums in figure\n\nf = plt.figure(figsize=(2*cols,2*rows)) # defining a figure \n\nfor i in range(rows*cols): \n    f.add_subplot(rows,cols,i+1) # adding sub plot to figure on each iteration\n    plt.imshow(X_train[i].reshape([28,28]),cmap=\"Blues\") \n    plt.axis(\"off\")","f3516ae9":"# Set a learning rate annealer\nfrom tensorflow.keras import callbacks\nfrom keras.callbacks import ReduceLROnPlateau\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3,  \n                                            factor=0.5, \n                                            min_lr=0.00001)","5fe7307f":"# optimizer \noptimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)","4b09f1c8":"# Data Augmentation\nfrom keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # dimesion reduction\n        rotation_range=5,  # randomly rotate images in the range 5 degrees\n        zoom_range = 0.1, # Randomly zoom image 10%\n        width_shift_range=0.1,  # randomly shift images horizontally 10%\n        height_shift_range=0.1,  # randomly shift images vertically 10%\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(X_train)","14e8a75a":"# CNN for discovering the best number of feature maps for our case.\n\nnets = 6\nmodel = [0] * nets\n\nfor j in range(6):\n    model[j] = Sequential([\n        layers.Conv2D(j*8+8,kernel_size=5,activation='relu',input_shape=(28,28,1)),\n        layers.MaxPool2D(pool_size=(2,2)),\n        \n        layers.Conv2D(j*16+16,kernel_size=5,activation='relu'),\n        layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n\n        layers.Flatten(),\n        layers.Dense(256, activation='relu'),\n        layers.Dense(10, activation='softmax'),\n    ])\n    \n    model[j].compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","84b5de7a":"# Fit the model\n\nepochs = 20\nbatch_size = 90\n\nhistory = [0] * nets\nnames = [\"8 maps\",\"16 maps\",\"24 maps\",\"32 maps\",\"48 maps\",\"64 maps\"]\nepochs = 20\nfor j in range(nets):\n    history[j] = model[j].fit(datagen.flow(X_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_test,y_test),\n                              verbose = 0, steps_per_epoch=X_train.shape[0] \/\/ batch_size,\n                              callbacks=[learning_rate_reduction])\n    print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        names[j],epochs,max(history[j].history['accuracy']),max(history[j].history['val_accuracy']) ))","6025d754":"nets = 8\nmodel = [0] *nets\n\nfor j in range(8):\n    model[j] = Sequential([\n        layers.Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)),\n        layers.MaxPool2D(pool_size=(2,2)),\n\n        layers.Conv2D(64,kernel_size=5,activation='relu'),\n        layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n        \n        layers.Flatten(),\n        layers.Dense(2**(j+4), activation='relu'),\n        layers.Dense(10, activation='softmax'),\n        \n    ])\n    \n    model[j].compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","aa252834":"history = [0] * nets\nnames = [\"16N\",\"32N\",\"64N\",\"128N\",\"256N\",\"512N\",\"1024N\",\"2048N\"]\nepochs = 20\nbatch_size = 90\n\nfor j in range(nets):\n    history[j] = model[j].fit(datagen.flow(X_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_test,y_test),\n                              verbose = 0, steps_per_epoch=X_train.shape[0] \/\/ batch_size,\n                              callbacks=[learning_rate_reduction])\n    print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        names[j],epochs,max(history[j].history['accuracy']),max(history[j].history['val_accuracy']) ))","10e0a6c0":"nets = 8\nmodel = [0] *nets\n\nfor j in range(8):\n    model[j] = Sequential([ \n        layers.Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)),\n        layers.MaxPool2D(pool_size=(2,2)),\n        layers.Dropout(j*0.1),\n\n        layers.Conv2D(64,kernel_size=5,activation='relu'),\n        layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n        layers.Dropout(j*0.1),\n        \n        layers.Flatten(),\n        layers.Dense(256, activation='relu'),\n        layers.Dropout(j*0.1),\n        layers.Dense(10, activation='softmax'),\n        \n    ])\n    \n    model[j].compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","8c7631af":"history = [0] * nets\nnames = [\"D=0\",\"D=0.1\",\"D=0.2\",\"D=0.3\",\"D=0.4\",\"D=0.5\",\"D=0.6\",\"D=0.7\"]\nepochs = 20\nbatch_size = 90\n\n\nfor j in range(nets):\n    history[j] = model[j].fit(datagen.flow(X_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_test,y_test),\n                              verbose = 0, steps_per_epoch=X_train.shape[0] \/\/ batch_size,\n                              callbacks=[learning_rate_reduction])\n    print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        names[j],epochs,max(history[j].history['accuracy']),max(history[j].history['val_accuracy']) ))","2f54e521":"nets = 6\nmodel = [0] * nets\n\nj = 0\nmodel[j] = Sequential([\n    layers.Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation ='relu', input_shape = (28,28,1)),\n    layers.Conv2D(filters = 32, kernel_size = (3,3), activation ='relu'),\n    layers.MaxPool2D(pool_size=(2,2)),\n    layers.Dropout(0.3),\n    \n    layers.Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu'),\n    layers.Conv2D(filters = 64, kernel_size = (3,3), activation ='relu'),\n    layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n    layers.Dropout(0.3),\n    \n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(10, activation='softmax'),\n])\nmodel[j].compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n\nj = 1\nmodel[j] = Sequential([\n    layers.Conv2D(filters = 32, kernel_size = (5), padding = 'Same', activation ='relu', input_shape = (28,28,1)),\n    layers.Conv2D(filters = 32, kernel_size = (5), activation ='relu'),\n    layers.MaxPool2D(pool_size=(2,2)),\n    layers.Dropout(0.3),\n\n    layers.Conv2D(filters = 64, kernel_size = (5), padding = 'Same', activation ='relu'),\n    layers.Conv2D(filters = 64, kernel_size = (5), activation ='relu'),\n    layers.MaxPool2D(pool_size=(2,2)),\n    layers.Dropout(0.3),\n\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(10, activation='softmax'),\n    \n])\nmodel[j].compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n\n\nj = 2\nmodel[j] = Sequential([\n    layers.Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation ='relu', input_shape = (28,28,1)),\n    layers.BatchNormalization(),\n    layers.Conv2D(filters = 32, kernel_size = (3,3), strides = 2, padding = 'Same', activation ='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPool2D(pool_size=(2,2)),\n    layers.Dropout(0.3),\n    \n    layers.Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu'),\n    layers.BatchNormalization(),\n    layers.Conv2D(filters = 64, kernel_size = (3,3), strides = 2, padding = 'Same', activation ='relu'),\n    layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n    layers.Dropout(0.3),\n    \n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(10, activation='softmax'),\n])\nmodel[j].compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n\nj = 3\nmodel[j] = Sequential([\n    layers.Conv2D(filters = 32, kernel_size = (5), padding = 'Same', activation ='relu', input_shape = (28,28,1)),\n    layers.BatchNormalization(),\n    layers.Conv2D(filters = 32, kernel_size = (5), padding = 'Same', activation ='relu'),\n    layers.BatchNormalization(),\n    layers.Conv2D(filters = 32, kernel_size = (5), padding = 'Same', activation ='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPool2D(pool_size=(2,2)),\n    layers.Dropout(0.3),\n    \n    layers.Conv2D(filters = 64, kernel_size = (3), padding = 'Same', activation ='relu'),\n    layers.BatchNormalization(),\n    layers.Conv2D(filters = 64, kernel_size = (3), padding = 'Same', activation ='relu'),\n    layers.BatchNormalization(),\n    layers.Conv2D(filters = 64, kernel_size = (3), padding = 'Same', activation ='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPool2D(pool_size=(2,2)),\n    layers.Dropout(0.3),\n    \n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dense(10, activation='softmax'),\n])\nmodel[j].compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n\nj = 4\nmodel[j] = Sequential([\n    layers.Conv2D(filters = 32, kernel_size = (5), padding = 'Same', activation ='relu', input_shape = (28,28,1)),\n    layers.BatchNormalization(),\n    layers.Conv2D(filters = 32, kernel_size = (5), padding = 'Same', activation ='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPool2D(pool_size=(2,2)),\n    layers.Dropout(0.3),\n    \n    layers.Conv2D(filters = 64, kernel_size = (3), padding = 'Same', activation ='relu'),\n    layers.BatchNormalization(),\n    layers.Conv2D(filters = 64, kernel_size = (3), padding = 'Same', activation ='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n    layers.Dropout(0.3),\n    \n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.4),\n    layers.Dense(10, activation='softmax'),\n])\nmodel[j].compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","1a6d52d8":"history = [0] * 5\nnames = [\"32C3\",\"32C5\",\"32C3S2\",\"3*32C5-3*64C3\",\"32C5-64C3\"]\nepochs = 35\nbatch_size = 90\n\nfor j in range(nets-1):\n    history[j] = model[j].fit(datagen.flow(X_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_test,y_test),\n                              verbose = 0, steps_per_epoch=X_train.shape[0] \/\/ batch_size,\n                              callbacks=[learning_rate_reduction])\n    print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        names[j],epochs,max(history[j].history['accuracy']),max(history[j].history['val_accuracy']) ))","e94cda65":"# Construct and compile the model\n\nmodel = Sequential([\n    layers.Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation ='relu', input_shape = (28,28,1)),\n    layers.Conv2D(filters = 32, kernel_size = (3,3), activation ='relu'),\n    layers.MaxPool2D(pool_size=(2,2)),\n    layers.Dropout(0.3),\n    \n    layers.Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu'),\n    layers.Conv2D(filters = 64, kernel_size = (3,3), activation ='relu'),\n    layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n    layers.Dropout(0.3),\n    \n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(10, activation='softmax'),\n])\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","06069834":"epochs = 35\nbatch_size = 90\n\nhistory = model.fit(datagen.flow(X_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_test,y_test),\n                              verbose = 1, steps_per_epoch=X_train.shape[0] \/\/ batch_size,\n                              callbacks=[learning_rate_reduction])","f6ee70cf":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot()","aab9f891":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\n# Predict the values from the validation dataset\nY_pred = model.predict(X_test)\n\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n\n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_test,axis = 1) \n\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\n\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","6bdec0ac":"# Display some error results \n\n# Errors are difference between predicted labels and true labels\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = X_test[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","53061d65":"results = model.predict(X_test)\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"mnist_pred.csv\",index=False)","ffcbc61d":"plt.figure(figsize=(15,6))\nfor i in range(40):  \n    plt.subplot(4, 10, i+1)\n    plt.imshow(X_test[i].reshape((28,28)),cmap=plt.cm.binary)\n    plt.title(\"predict=%d\" % results[i],y=0.9)\n    plt.axis('off')\nplt.subplots_adjust(wspace=0.3, hspace=-0.1)\nplt.show()","0ec8b049":"## Aknowledgements\n\nThese three notebooks below taught me a lot about CNNs, they were, in my opinion, the most complete kernels about the topic in this competition.\n\n* https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6\n* https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist\n* https:\/\/www.kaggle.com\/cdeotte\/how-to-choose-cnn-architecture-mnist\n* https:\/\/www.kaggle.com\/kanncaa1\/convolutional-neural-network-cnn-tutorial","a68530ef":"## 5.4 - Best CNN architecture\n\nWhich CNN architecture is best for our model? A 32C3-32C3, 32C3S2 architecture? Let's run some tests to find out.\n","9ef4c3b2":"## Digit Recognition unsing MNIST dataset\n\nThe MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\nIt is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. \nWe will first perform a few tests to see which CNN architecture is suitable for our dataset.\n\nThroughout this kernel, we'll use the following notation:\n\n* 24C5 means a convolution layer with 24 feature maps using a 5x5 filter and stride 1\n* 24C5S2 means a convolution layer with 24 feature maps using a 5x5 filter and stride 2\n* P2 means max pooling using 2x2 filter and stride 2\n* 256 means fully connected dense layer with 256 units","efe1dfcb":"Before we create our CNN, we are going to perform a few tests in order to discover which CNN achitecture to use. \n\nThroughout this kernel, the following notation will be used:\n\n* 32C3 means a convolution layer with 32 feature maps using a 3x3 filter and stride 1\n* 32C5S2 means a convolution layer with 32 feature maps using a 5x5 filter and stride 2\n* P2 means max pooling using 2x2 filter and stride 2\n* 256 means fully connected dense layer with 256 units","3f336060":"### Summary \n\nEach time you train a CNN you get a random result, therefore you must run the experiment a few times and take an average of the results. ","330e8a79":"The first step that we will take is to load the data into the variables \"train\" and \"test\".","01a5d900":"## Submit predictions","834b56cd":"## 5.1 - Number of feature maps \n\nThe feature map is the output of one filter applied to the previous layer. Each filter transforms a part of the image (defined by the kernel size) using the kernel filter. The kernel filter matrix is applied on the whole image. Filters can be seen as a transformation of the image.\n\n* input, kernel and output\n* Feature detector does not need to be 3 by 3 matrix. It can be 5 by 5 or 7 by 7.\n* Feature detector = kernel = filter\n\n<a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/image.ibb.co\/m4FQC9\/gec.jpg\" alt=\"gec\" border=\"0\"><\/a>\n\n* The pooling (MaxPool2D) layer acts as a downsampling filter. It looks at the 2 neighboring pixels and picks the maximal value\n\n#### How many feature maps should we have in our NN?","6a78bd69":"## 5.2 - Dense Layer\n\n* Neurons in a fully connected layer have connections to all activations in the previous layer\n* Artificial Neural Network\n\n<a href=\"https:\/\/ibb.co\/hsS14p\"><img src=\"https:\/\/preview.ibb.co\/evzsAU\/fullyc.jpg\" alt=\"fullyc\" border=\"0\"><\/a>\n\nHow many dense units should be used? Our options are:\n\n* 16 units\n* 32 units\n* 64 units\n* 128 units\n* 256 units\n* 512 units\n* 1024 units\n* 2048 units","04d12307":"## 2 - Define an Annealer\n\nIn order to make the olgorithm converges to the global minimum of the loss fuction, we will define an annealing method to the learning rate, meaning that the LR will decrease during trainning reaching the global minimum instead of falling into the local minima.\n\nWith the ReduceLROnPlateau function from Keras.callbacks, the LR will be reduced by half if the accuracy is not improved after 3 epochs.","0e65a41e":"# Steps to create the CNN","be0bf1f0":"## 3 - Optimizer \n\nThe ADAM optimizer will be used, we are gonna change the learning rate","a2522c1d":"it seems that our CNN has some troubles with the 4 digits, they are misclassified as 9. Sometimes it is very difficult to catch the difference between 4 and 9 when the person has a terrible handwriting. Our algorithm goes through the same problems we do.\n\nLet's take a look at some errors we got","72952e6f":"## 5.3 - Dropout\n\nDropout will prevent our network from overfitting. But how much dropout should we add to get the best results?\n\n* 0%, 10%, 20%, 30%, 40%, 50%, 60%, or 70%","f29b1a65":"## 6 - Modeling the CNN according the results os the tests","af8aae01":"### 6.1 - Epochs and Batch Size\n\n* Say you have a dataset of 10 examples (or samples). You have a batch size of 2, and you've specified you want the algorithm to run for 3 epochs. Therefore, in each epoch, you have 5 batches (10\/2 = 5). Each batch gets passed through the algorithm, therefore you have 5 iterations per epoch.\n\n* reference: https:\/\/stackoverflow.com\/questions\/4752626\/epoch-vs-iteration-when-training-neural-networks","1ad5087e":"Now, we are going to construct our CNN with the parameters we got from the tests. The chosen CNN was the 32C3 - 64C3 model, this achitecture performs well on all the simulations that were perform before. \n\nThe data has already been divided into train and test set. Our only task now will be to train the model.\n","8ee13fdf":"### Summary\nAs we can see, a dropout of 0.4 is leads to better results, considering the computational cost.","50bf0a4b":"### 6.2 - Plot the loss and val_loss along with accuracy ans val_accuracy","10bebf8e":"### 6.3 - Confusion matrix\n\nConfusion matrix can be very helpfull to see your model drawbacks.","d5e95aaa":"#### Summary\nit appears that 32 maps in the first convolutional layer and 64 maps in the second convolutional layer is the best choice for our CNN.","07d2dc63":"## 4 - Data augmentation \n\nTo avoid the overfitting problem, it's necessary to expand artificially our handwritten digit dataset. By altering the training data with small transformations we are able to reproduce the variations occuring when someone is writing a digit.\n\nMethods like the one described above that alter the training data in ways that change the array representation while keeping the label the same are known as \"data augmentation\" techniques. Some popular augmentations used are: grayscales, horizontal flips, vertical flips, random crops, color jitters, translations, rotations, and more.","035544fd":"#### Summary\n\nFrom this experiment, we can see that 128 units is the best choice. ","9bd59c1a":"## 1 - Normalization, Reshaping, and Encoding of the data\n\n* We normalize the data in order to reduce the effect of illumination's differences. By doing so, the algorithm works faster.\n* Reshape the data to 28x28x1, Keras needs an extra dimension in the end which corresponds to channels. If we had RGB images, the would be 28x28x3.\n* Transform the outputs to one hot vectors:  \n3 => [0,0,0,1,0,0,0,0,0,0]   \n5 => [0,0,0,0,0,1,0,0,0,0]","6c7a3ced":"### Conclusion\n\nFor those six cases, the model is not ridiculous. Some of these errors can be made by humans, especially for one the 9 that is very close to a 4. The last 8 is also very misleading, it seems for me that is a 0.","dc4780af":"# 5 - Which CNN achitecture to use?"}}