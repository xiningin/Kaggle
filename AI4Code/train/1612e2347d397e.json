{"cell_type":{"fc508235":"code","3d9064c7":"code","a00bb163":"code","508d018c":"code","ad3180d7":"code","0d6568ac":"code","12708ca3":"code","4411c5b2":"code","cd3eb49b":"code","daf18f16":"code","878200bd":"code","55109601":"code","56c41f1f":"code","7297b62a":"code","96d52712":"code","ab6ac373":"code","ca414ee7":"code","850f3577":"code","a47cf2a9":"markdown"},"source":{"fc508235":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"..\/input\/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3d9064c7":"import pandas as pd\nimport numpy as np\nimport glob\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, KFold\nfrom tqdm import tqdm\n\nimport gc","a00bb163":"config = {'input_trade_path': \"..\/input\/optiver-realized-volatility-prediction\/trade_\",\n          'input_book_path': \"..\/input\/optiver-realized-volatility-prediction\/book_\",\n          'train_path': '..\/input\/optiver-realized-volatility-prediction\/train.csv',\n          'test_path' : '..\/input\/optiver-realized-volatility-prediction\/test.csv'}","508d018c":"temp = pd.read_parquet(\"..\/input\/optiver-realized-volatility-prediction\/book_test.parquet\/stock_id=0\/7832c05caae3489cbcbbb9b02cf61711.parquet\")\ntemp","ad3180d7":"train_df = pd.read_csv(config['train_path'])\ntest_df = pd.read_csv(config['test_path'])\ntrain_df","0d6568ac":"test_df","12708ca3":"def rmspe(y_true, y_pred):\n    '''\n    Compute Root Mean Square Percentage Error between two arrays.\n    '''\n    loss = np.sqrt(np.mean(np.square(((y_true - y_pred) \/ y_true)), axis=0))\n\n    return loss","4411c5b2":"def read_trade_book_data(stock_id, inp_type, data_type):\n    trade_file = glob.glob(config[inp_type]+f'{data_type}.parquet\/stock_id={stock_id}\/*')[0]\n    trade = pd.read_parquet(trade_file)\n    return trade","cd3eb49b":"def get_final_df(df, data_type):\n    unique_id = df['stock_id'].unique().tolist()\n    \n    trade_final_df = pd.DataFrame()\n    book_final_df = pd.DataFrame()\n    for stock_id in tqdm(unique_id):\n        # Get book data\n        temp_book_stock_df = read_trade_book_data(stock_id=stock_id, \n                                                  inp_type='input_book_path', \n                                                  data_type=data_type)\n        temp_book_stock_df['stock_id'] = stock_id\n        book_final_df = pd.concat([book_final_df, temp_book_stock_df])\n        \n        # Get trade data\n        temp_trade_stock_df = read_trade_book_data(stock_id=stock_id, \n                                                   inp_type='input_trade_path', \n                                                   data_type=data_type)\n        temp_trade_stock_df['stock_id'] = stock_id\n        trade_final_df = pd.concat([trade_final_df, temp_trade_stock_df])\n        \n        gc.collect()\n        \n    book_final_df = book_final_df.reset_index(drop=True)\n    trade_final_df = trade_final_df.reset_index(drop=True)\n\n    return book_final_df, trade_final_df","daf18f16":"gc.collect()\ntrain_book_final_df, train_trade_final_df = get_final_df(df=train_df, data_type='train')\ntest_book_final_df, test_trade_final_df = get_final_df(df=train_df, data_type='test')\n\ntrain_book_final_df.shape, train_trade_final_df.shape, test_book_final_df.shape, test_trade_final_df.shape","878200bd":"def get_trade_agg_info(df):\n    agg_df = df.groupby(['stock_id', 'time_id']).agg(mean_sec_in_bucket = ('seconds_in_bucket', 'mean'), \n                                                     mean_price = ('price', 'mean'),\n                                                     mean_size = ('size', 'mean'),\n                                                     mean_order = ('order_count', 'mean'),\n                                                     max_sec_in_bucket = ('seconds_in_bucket', 'max'), \n                                                     max_price = ('price', 'max'),\n                                                     max_size = ('size', 'max'),\n                                                     max_order = ('order_count', 'max'),\n                                                     min_sec_in_bucket = ('seconds_in_bucket', 'min'), \n                                                     min_price = ('price', 'min'),\n                                                     min_size = ('size', 'min'),\n                                                     min_order = ('order_count', 'min'),\n                                                     median_sec_in_bucket = ('seconds_in_bucket', 'median'), \n                                                     median_price = ('price', 'median'),\n                                                     median_size = ('size', 'median'),\n                                                     median_order = ('order_count', 'median')\n                                                    ).reset_index()\n    \n    return agg_df\n\ndef get_book_agg_info(df):\n    agg_df = df.groupby(['stock_id', 'time_id']).agg(mean_sec_in_bucket = ('seconds_in_bucket', 'mean'), \n                                                     mean_price = ('price', 'mean'),\n                                                     mean_size = ('size', 'mean'),\n                                                     mean_order = ('order_count', 'mean'),\n                                                     max_sec_in_bucket = ('seconds_in_bucket', 'max'), \n                                                     max_price = ('price', 'max'),\n                                                     max_size = ('size', 'max'),\n                                                     max_order = ('order_count', 'max'),\n                                                     min_sec_in_bucket = ('seconds_in_bucket', 'min'), \n                                                     min_price = ('price', 'min'),\n                                                     min_size = ('size', 'min'),\n                                                     min_order = ('order_count', 'min'),\n                                                     median_sec_in_bucket = ('seconds_in_bucket', 'median'), \n                                                     median_price = ('price', 'median'),\n                                                     median_size = ('size', 'median'),\n                                                     median_order = ('order_count', 'median')\n                                                    ).reset_index()\n    \n    return agg_df","55109601":"train_final_df = get_final_df(df=train_df, data_type='train')\ntest_final_df = get_final_df(df=test_df, data_type='test')\ntrain_final_df.shape, test_final_df.shape","56c41f1f":"train_agg = get_agg_info(df=train_final_df)\ntest_agg = get_agg_info(df=test_final_df)\n\ntrain_agg.shape, test_agg.shape","7297b62a":"train_final_df = pd.merge(train_agg, train_df, on=['stock_id', 'time_id'], how='left')\ntest_final_df = pd.merge(test_df, test_agg, on=['stock_id', 'time_id'], how='left')\ntrain_final_df.fillna(-999, inplace=True)\ntest_final_df.fillna(-999, inplace=True)\n\nprint(train_final_df.shape, test_final_df.shape)","96d52712":"dep = 'target'\ndrop = ['stock_id', 'time_id']\nindep = train_final_df.columns.difference([dep]+drop)\nindep","ab6ac373":"RF = RandomForestRegressor(n_jobs=-1, n_estimators=15)\nRF.fit(train_final_df[indep], train_final_df[dep])\nRF_pred = RF.predict(test_final_df[indep])","ca414ee7":"sub_id = test_final_df.stock_id.astype(str) + '-' + test_final_df.time_id.astype(str)\nsubmission_df = pd.DataFrame({'row_id':sub_id, 'target':RF_pred})\nsubmission_df","850f3577":"submission_df.to_csv(\"submission.csv\", index=False)","a47cf2a9":"#### This is a starter notebook on how you can create stats features and build models. \n\nNote: i have used only the <B>trade<\/B> features and not used the <B>book<\/B> features yet. You can include them and improve your score. Take a look my notebook on [data aggregated](https:\/\/www.kaggle.com\/thanish\/data-aggregated) to consolidate the book and trader features together. You can use them to build models on top of it."}}