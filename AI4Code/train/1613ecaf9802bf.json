{"cell_type":{"fee9ff17":"code","e5971f51":"code","fb478f03":"code","8084f312":"code","bdd32c31":"code","a1dfcc73":"code","7e41fd10":"code","4d34a6ae":"code","df009da0":"code","70059a16":"code","562e13fa":"code","0bf4c307":"code","98a85ee8":"code","c47e64e5":"code","c758e68b":"code","b0216682":"code","4322da18":"code","78fa99e1":"code","5cb03498":"code","c14e37fd":"code","331346da":"code","4aee8700":"code","267cd0b7":"code","00ce7fe7":"code","827040fd":"code","8c8c84a2":"code","2f5eb1e1":"code","fdbf0636":"code","af83e21e":"code","c9c1898c":"code","1e53ba87":"code","a08a073f":"code","3f20a4b8":"code","6410d6f9":"code","e3a6d6f3":"code","02c99fa4":"code","91bc068a":"code","91f04d5e":"code","7c463b8f":"code","4fa67bf0":"code","2995b0cd":"code","124e719f":"code","ac867aa3":"code","caabe042":"code","3b2b80a9":"code","07067622":"code","ee66d66a":"code","e492716b":"code","9956695a":"code","5705d34e":"code","e7da87e2":"code","2af92c19":"code","581d528c":"code","f83acdde":"code","83020bbe":"code","06baba11":"markdown","19dd08f3":"markdown","d5d17ce4":"markdown","78da776f":"markdown"},"source":{"fee9ff17":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e5971f51":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","fb478f03":"df= pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv' ,usecols=[\"SalePrice\", \"MSSubClass\", \"MSZoning\", \"LotFrontage\", \"LotArea\",\n                                         \"Street\", \"YearBuilt\", \"LotShape\", \"1stFlrSF\", \"2ndFlrSF\"]).dropna()\ndf.head()","8084f312":"df.shape","bdd32c31":"df.head()","a1dfcc73":"df.info()","7e41fd10":"for i in df.columns:\n    print(\"Column names {} and Unique Values are {}\".format(i,len(df[i].unique())))","4d34a6ae":"import datetime\ndatetime.datetime.now().year","df009da0":"# We will create Derived Feature i.e Total Years ,we dont want YearBuilt \ndf['Total Years']=datetime.datetime.now().year-df['YearBuilt']","70059a16":"df.drop('YearBuilt',axis=1,inplace=True)","562e13fa":"df.columns","0bf4c307":"### Creating Categorical Variables\ncat_feat=[\"MSSubClass\",\"MSZoning\",\"Street\",\"LotShape\"]\nout_feat=\"SalePrice\"","98a85ee8":"## Unique Values of MSSubClass now we will conert in categorical variable and label encoding\n\nfrom sklearn.preprocessing import LabelEncoder\nlbl_encoders={}\nlbl_encoders[\"MSSubClass\"]=LabelEncoder()\nlbl_encoders[\"MSSubClass\"].fit_transform(df[\"MSSubClass\"])","c47e64e5":"lbl_encoders","c758e68b":"from sklearn.preprocessing import LabelEncoder\nlbl_encoders={}\nfor feature in cat_feat:\n    lbl_encoders[feature]=LabelEncoder()\n    df[feature]=lbl_encoders[feature].fit_transform(df[feature])","b0216682":"df","4322da18":"#Stacking and converting into Tensors\ncat_feat=np.stack([df[\"MSSubClass\"],df[\"MSZoning\"],df[\"Street\"],df[\"LotShape\"]],1)\ncat_feat","78fa99e1":"#Convert numpy to Tensors\n# Categorical Features cannot be converted to Float\nimport torch\ncat_feat= torch.tensor(cat_feat, dtype=torch.int64)\ncat_feat","5cb03498":"#### create continuous Variable\ncont_feat=[]\nfor i in df.columns:\n    if i in [\"MSSubClass\",\"MSZoning\",\"Street\",\"LotShape\",\"SalePrice\"]:\n        pass\n    else:\n        cont_feat.append(i)\n        ","c14e37fd":"cont_feat","331346da":"### Stacking continuous variable to a tensor\ncont_values=np.stack([df[i].values for i in cont_feat],axis=1)\ncont_values=torch.tensor(cont_values,dtype=torch.float)\ncont_values","4aee8700":"cont_values.dtype","267cd0b7":"### dependent Feature\ny=torch.tensor(df['SalePrice'].values,dtype=torch.float).reshape(-1,1)   ##converting to 2D feature\ny","00ce7fe7":"df.info()","827040fd":"cat_feat.shape,cont_values.shape,y.shape","8c8c84a2":"len(df['MSSubClass'].unique())","2f5eb1e1":"cat_dims=[len(df[col].unique()) for col in [\"MSSubClass\", \"MSZoning\", \"Street\", \"LotShape\"]]\ncat_dims","fdbf0636":"#Thumb Rule says--Output dimension ahould be set based on the input variable \n#The formula is (min(50,featur_dimension\/2))\nembedding_dims=[(x,min(50,(x+1)\/\/2)) for x in cat_dims]\nembedding_dims","af83e21e":"import torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nembed_representation=nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dims])\nembed_representation","c9c1898c":"cat_feat","1e53ba87":"cat_featz=cat_feat[:4]\ncat_featz","a08a073f":"pd.set_option('display.max_rows',500)\nembedding_val=[]\nfor i,e in enumerate(embed_representation):               ## e is responsible for converting value to Vector\n    \n    embedding_val.append(e(cat_feat[:,i]))","3f20a4b8":"embedding_val","6410d6f9":"# Stacking should be Column Wise So we will be using Concatination Operation using Embedding Value\n\nz=torch.cat(embedding_val,1)            # So now all are stacked in one row\nz","e3a6d6f3":"#We will apply Dropout layer which will help in avoiding Overfitting \n#After executing Some of the values become 0.So I am dropping 40% values\ndropout=nn.Dropout(.4)","02c99fa4":"final_embed=dropout(z)\nfinal_embed","91bc068a":"### Create a Feed Forward Neural network\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nclass FeedForwardNN(nn.Module):\n    \n    def __init__(self,embedding_dims,n_cont,out_sz,layers,p=0.5):\n        super().__init__()\n        self.embeds = nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dims])\n        self.emb_drop = nn.Dropout(p)\n        self.bn_cont = nn.BatchNorm1d(n_cont)\n        \n        layerlist=[]\n        n_emb= sum(out for inp,out in embedding_dims)                    ### calculate the total dimension of embedding layer\n        n_in= n_emb + n_cont\n        \n        for i in layers:\n            layerlist.append(nn.Linear(n_in,i))\n            layerlist.append(nn.ReLU(inplace=True))\n            layerlist.append(nn.BatchNorm1d(i))\n            layerlist.append(nn.Dropout(p))\n            n_in=i\n       \n        layerlist.append(nn.Linear(layers[-1],out_sz))\n        \n        self.layers=nn.Sequential(*layerlist)\n     \n    def forward(self,x_cat,x_cont):\n        embeddings=[]\n        for i,e in enumerate(self.embeds):\n            embeddings.append(e(x_cat[:,i]))\n        x= torch.cat(embeddings,1)                      ## concatinating the embeddings and applying Dropout\n        x= self.emb_drop(x)\n    \n        x_cont= self.bn_cont(x_cont)\n        x= torch.cat([x,x_cont], 1)\n        x= self.layers(x)\n        return x","91f04d5e":"torch.manual_seed(100)\nmodel=FeedForwardNN(embedding_dims, len(cont_feat),1,[100,50],p=0.1)","7c463b8f":"model","4fa67bf0":"loss_func= nn.MSELoss()       ## Convert into RMSE later\noptimizer= torch.optim.Adam(model.parameters(),lr=0.1)","2995b0cd":"df.shape","124e719f":"cont_values","ac867aa3":"cont_values.shape","caabe042":"# Train test split\n\nbatch_size=1200\ntest_size= int(batch_size*0.15)\ntrain_categorical=  cat_feat[:batch_size-test_size]\ntest_categorical= cat_feat[batch_size-test_size:batch_size]\ntrain_cont= cont_values[:batch_size-test_size]\ntest_cont= cont_values[batch_size-test_size:batch_size]\ny_train= y[:batch_size-test_size]\ny_test= y[batch_size-test_size:batch_size]","3b2b80a9":"len(train_categorical),len(test_categorical),len(train_cont),len(test_cont),len(y_train),len(y_test)","07067622":"\nepochs=5000\nfinal_losses=[]\nfor i in range(epochs):\n    i=i+1\n    y_pred= model(train_categorical,train_cont)\n    loss= torch.sqrt(loss_func(y_pred,y_train))     ## RMSE\n    final_losses.append(loss)\n    if i%10==1:\n        print(\"Epoch number: {} and the Loss: {}\".format(i,loss.item()))\n    optimizer.zero_grad()\n    loss.backward()         ##back propogation\n    optimizer.step()\n    ","ee66d66a":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot(range(epochs),final_losses)\nplt.ylabel('RMSE loss')\nplt.xlabel('Epochs');","e492716b":"#### Validate the test data\ny_pred=\"\"\nwith torch.no_grad():\n    y_pred= model(test_categorical,test_cont)\n    loss=torch.sqrt(loss_func(y_pred,y_test))\n    \nprint(\"RMSE: {}\" .format(loss))","9956695a":"data_verify= pd.DataFrame(y_test.tolist(),columns=[\"test\"])\ndata_predicted=pd.DataFrame(y_pred.tolist(),columns=[\"Prediction\"])","5705d34e":"data_predicted","e7da87e2":"final_output=pd.concat([data_verify,data_predicted],axis=1)\nfinal_output[\"Difference\"]= final_output['test']-final_output['Prediction']\nfinal_output.head()","2af92c19":"## Svaing the model\n## Save the model\ntorch.save(model,'HousePrice.pt')","581d528c":"torch.save(model.state_dict,'HouseWeights.pt')           ## state_dict helps in saving Weights","f83acdde":"## Loading the saved Model\nemb_size=[(15,8),(5,3),(2,1),(4,2)]\nmodel1= FeedForwardNN(emb_size,5,1,[100,50],p=0.4)\n","83020bbe":"model1.eval","06baba11":"### Define Loss and Optimizer","19dd08f3":"## Before Proceeding into the depth of notebook ,have a look at the given fastAI useful Links.These are useful because in this house price data we are dealing with Categorical Dataset and categorical embeddings\n### http:\/\/docs.fast.ai\/tabular.html\n### https:\/\/www.fast.ai\/2018\/04\/29\/categorical-embeddings\/\n","d5d17ce4":"# **Thankyou for visiting the kernel. I will be happy if you find this useful .Please leave an upvote which is a kind of motivation.**\n\n### I have tried to make the useful codes understandable by marking the comments still anything to learn or add .please feel free to do so in comments.","78da776f":"### Embedding Size For Categorical columns"}}