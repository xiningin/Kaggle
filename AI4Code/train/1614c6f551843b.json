{"cell_type":{"14a49f7d":"code","052fa1ae":"code","424d274c":"code","144a0392":"code","7fba44d7":"code","3b681860":"code","46e0c43d":"code","aaf4f756":"code","5da17afc":"code","010668af":"code","1d0024af":"code","4093622a":"code","fd01932e":"code","c5c31578":"code","fd687c5e":"code","379dff1e":"code","b5306824":"code","52bf1157":"code","6fca4db7":"code","677a3294":"code","96331c24":"code","d5fd7c89":"code","c8c9bf31":"code","07ad0606":"code","d739695d":"code","4537337b":"code","ea1d2fb8":"code","e3fbe980":"code","bf7c24bc":"code","eb22eae8":"code","0f54ba2b":"code","115cd71a":"code","b74ee7dc":"code","b74a3ed6":"code","03eb151c":"code","5d7af019":"code","4f6bdfdc":"code","b7e8c3b9":"code","532051a2":"code","945a0f4b":"code","7135a5be":"code","f347e737":"code","c046c8f2":"code","c0ae0b2b":"code","448c1c18":"code","f23ec7e6":"code","844f8dad":"code","5be76c72":"code","3a1a6494":"code","cabd0cc2":"code","fd086f9c":"code","cef51ddd":"code","e31386e8":"code","a2f88cad":"code","0d3798ba":"code","3517d2f8":"code","ef91ba45":"code","940db8bd":"code","ef418d8e":"code","aa3c545e":"code","dd295fa6":"code","491daa42":"code","e73690ff":"code","b7d2aeb5":"code","a31a4f70":"code","95cbda47":"code","d94093eb":"code","3a1c4a5e":"code","1affb6bf":"markdown","1b062fac":"markdown","ce6f44e9":"markdown","7465eebe":"markdown","1bff5720":"markdown","ffb9549d":"markdown","a2337b68":"markdown","357f97ef":"markdown","bea71328":"markdown","a84a8ef3":"markdown","188b64d2":"markdown","2cb54876":"markdown","8a720436":"markdown","45f9904c":"markdown","e2bf6f3d":"markdown","a1ed89d4":"markdown","682a1c86":"markdown","62511b12":"markdown","7a178e49":"markdown","8c0a04ca":"markdown","7b130e01":"markdown","977d2c36":"markdown","b67bce8d":"markdown","da9425d9":"markdown","ff8b9be6":"markdown","d006065f":"markdown","781beaae":"markdown","44cacbb0":"markdown","d36b0e95":"markdown","10fa1e6b":"markdown","a127cb90":"markdown","f2018cef":"markdown","06c370fc":"markdown","9bc47dd3":"markdown","01257f91":"markdown","17bbdc34":"markdown","56b21e9d":"markdown","364b6b4c":"markdown","a7ce91e6":"markdown","f42c9544":"markdown","c54a5d17":"markdown","46b9b074":"markdown","12a92aad":"markdown","76423527":"markdown","78f90b80":"markdown","4907bb08":"markdown","acba5a0a":"markdown","1a9f6178":"markdown","8554ebc5":"markdown","f36c6283":"markdown"},"source":{"14a49f7d":"import numpy as np\nimport pandas as pd\nfrom pandas import Series\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn')\nsns.set(font_scale=2.5)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"..\/input\"))","052fa1ae":"data = pd.read_csv('..\/input\/train.csv')","424d274c":"data.head()","144a0392":"data.isnull().sum()","7fba44d7":"f, ax = plt.subplots(1,2,figsize=(18,8))\ndata['Survived'].value_counts().plot.pie(explode=[0, 0.1], autopct='%1.1f%%', ax=ax[0], shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived', data=data, ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","3b681860":"data.groupby(['Sex', 'Survived'])['Survived'].count()","46e0c43d":"f, ax = plt.subplots(1, 2, figsize=(18,8))\ndata[['Sex', 'Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex')\nsns.countplot('Sex', hue='Survived', data=data, ax=ax[1])\nax[1].set_title('Sex: Survived vs Dead')\nplt.show()","aaf4f756":"pd.crosstab(data['Pclass'], data['Survived'], margins=True).style.background_gradient(cmap='summer_r')","5da17afc":"f, ax = plt.subplots(1,2,figsize=(18,8))\ndata['Pclass'].value_counts().plot.bar(color=['#CD7F32', '#FFDF00', '#D3D3D3'], ax=ax[0])\nax[0].set_title('Number of Passengers by Pclass')\nax[0].set_ylabel('Count')\nsns.countplot('Pclass', hue='Survived', data=data, ax=ax[1])\nax[1].set_title('Pclass: Survived vs Dead')","010668af":"pd.crosstab([data['Sex'], data['Survived']], data['Pclass'], margins=True).style.background_gradient(cmap='summer_r')","1d0024af":"sns.factorplot('Pclass', 'Survived', hue='Sex', data=data)\nplt.show()","4093622a":"print('Oldest Passenger was: {:.2f} Years'.format(data['Age'].max()))\nprint('Youngest Passenger was: {:.2f} Years'.format(data['Age'].min()))\nprint('Average Age on the ship was: {:.2f} Years'.format(data['Age'].mean()))","fd01932e":"f, ax = plt.subplots(1,2,figsize=(18,8))\nsns.violinplot('Pclass', 'Age', hue='Survived', data=data, split=True, scale='count', ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0,110,10))\nsns.violinplot('Sex', 'Age', hue='Survived', data=data, split=True, scale='count', ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0,110,10))","c5c31578":"data['Initial'] = 0\ndata['Initial'] = data['Name'].str.extract('([A-Za-z]+)\\.')","fd687c5e":"data.head()","379dff1e":"pd.crosstab(data['Initial'], data['Sex']).T.style.background_gradient(cmap='summer_r')","b5306824":"data['Initial'].replace(['Mlle', 'Mme', 'Ms', 'Dr', 'Major', 'Lady', 'Countess',\\\n                         'Jonkheer', 'Col', 'Rev', 'Capt', 'Sir','Don'], \\\n                       ['Miss', 'Miss', 'Miss', 'Mr', 'Mr', 'Mrs', 'Mrs', 'Other', 'Other',\\\n                       'Other', 'Mr', 'Mr', 'Mr'], inplace=True)","52bf1157":"data.groupby('Initial')['Age'].mean()","6fca4db7":"data.loc[(data['Age'].isnull()) & (data['Initial']=='Mr'), 'Age'] = 33\ndata.loc[(data['Age'].isnull()) & (data['Initial']=='Mrs'), 'Age'] = 36\ndata.loc[(data['Age'].isnull()) & (data['Initial']=='Master'), 'Age'] = 5\ndata.loc[(data['Age'].isnull()) & (data['Initial']=='Miss'), 'Age'] = 22\ndata.loc[(data['Age'].isnull()) & (data['Initial']=='Other'), 'Age'] = 46","677a3294":"data['Age'].isnull().any()","96331c24":"f, ax = plt.subplots(1,2,figsize=(18,8))\ndata[data['Survived']==0]['Age'].plot.hist(ax=ax[0], bins=20, edgecolor='black', color='red')\nax[0].set_title('Survived=0')\nx1=list(range(0,85,10))\nax[0].set_xticks(x1)\ndata[data['Survived']==1]['Age'].plot.hist(ax=ax[1], bins=20, edgecolor='black', color='green')\nax[1].set_title('Survived=1')\nax[1].set_xticks(x1)\nplt.show()","d5fd7c89":"sns.factorplot('Pclass', 'Survived', col='Initial', data=data)\nplt.show()","c8c9bf31":"pd.crosstab([data['Embarked'], data['Pclass']],[data['Sex'], data['Survived']], margins=True).style.background_gradient(cmap='summer_r')","07ad0606":"sns.factorplot('Embarked', 'Survived', data=data)\nfig=plt.gcf()\nfig.set_size_inches(5,3)\nplt.show()","d739695d":"f, ax = plt.subplots(2,2,figsize=(20, 15))\nsns.countplot('Embarked', data=data, ax=ax[0,0])\nax[0,0].set_title('No. of Passengers Boarded')\nsns.countplot('Embarked', hue='Sex', data=data, ax=ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked', hue='Survived', data=data, ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked', hue='Pclass', data=data, ax=ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()","4537337b":"sns.factorplot('Pclass', 'Survived', hue='Sex', col='Embarked', data=data)\nplt.show()","ea1d2fb8":"data['Embarked'].fillna('S', inplace=True)","e3fbe980":"data['Embarked'].isnull().any()","bf7c24bc":"pd.crosstab(data['SibSp'], data['Survived'], margins=True).style.background_gradient(cmap='summer_r')","eb22eae8":"f, ax = plt.subplots(1,2,figsize=(18,8))\nsns.barplot('SibSp', 'Survived', data=data, ax=ax[0])\nax[0].set_title('SibSp vs Survived')\nsns.factorplot('SibSp', 'Survived', data=data, ax=ax[1])\nax[1].set_title('SibSp vs Survived')\nplt.close(2)\nplt.show()","0f54ba2b":"pd.crosstab(data['SibSp'], data['Pclass']).style.background_gradient(cmap='summer_r')","115cd71a":"pd.crosstab(data['Parch'], data['Pclass']).style.background_gradient(cmap='summer_r')","b74ee7dc":"f, ax = plt.subplots(1,2,figsize=(18,8))\nsns.barplot('Parch', 'Survived', data=data, ax=ax[0])\nax[0].set_title('Parch vs Survived')\nsns.factorplot('Parch', 'Survived', data=data, ax=ax[1])\nax[1].set_title('Parch vs Survived')\nplt.close(2)\nplt.show()","b74a3ed6":"print('Highest Fare was {:.2f}'.format(data['Fare'].max()))\nprint('Lowest Fare was {:.2f}'.format(data['Fare'].min()))\nprint('Aberage Fare was {:.2f}'.format(data['Fare'].mean()))","03eb151c":"f, ax = plt.subplots(1,3,figsize=(20,8))\nsns.distplot(data[data['Pclass']==1]['Fare'], ax=ax[0])\nax[0].set_title('Fares Pclass 1')\nsns.distplot(data[data['Pclass']==2]['Fare'], ax=ax[1])\nax[1].set_title('Fares Pclass 2')\nsns.distplot(data[data['Pclass']==3]['Fare'], ax=ax[2])\nax[2].set_title('Fares Pclass 3')\nplt.show()","5d7af019":"sns.heatmap(data.corr(), annot=True, cmap='RdYlGn', linewidth=0.2)\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","4f6bdfdc":"data['Age_band']=0\ndata.loc[data['Age']<=16, 'Age_band']=0\ndata.loc[(16<data['Age']) & (data['Age']<=32), 'Age_band']=1\ndata.loc[(32<data['Age']) & (data['Age']<=48), 'Age_band']=2\ndata.loc[(48<data['Age']) & (data['Age']<=64), 'Age_band']=3\ndata.loc[64<data['Age'], 'Age_band']=4\ndata.head()","b7e8c3b9":"data['Age_band'].value_counts()","532051a2":"sns.factorplot('Age_band', 'Survived', col='Pclass', data=data)\nplt.show()","945a0f4b":"data['Family_size']=0\ndata['Family_size']=data['SibSp']+data['Parch']\ndata['Alone']=0\ndata.loc[data['Family_size']==0, 'Alone']=1","7135a5be":"f, ax = plt.subplots(1,2,figsize=(18,6))\nsns.factorplot('Family_size', 'Survived', data=data, ax=ax[0])\nax[0].set_title('Family_size vs Survived')\nsns.factorplot('Alone', 'Survived', data=data, ax=ax[1])\nax[1].set_title('Alone vs Survived')\nplt.close(2)\nplt.close(3)\nplt.show()","f347e737":"sns.factorplot('Alone', 'Survived', hue='Sex', col='Pclass', data=data)\nplt.show()","c046c8f2":"data['Fare_range']=pd.qcut(data['Fare'], 4)\ndata.groupby(['Fare_range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","c0ae0b2b":"data['Fare_cat']=0\ndata.loc[data['Fare']<=7.91, 'Fare_cat']=0\ndata.loc[(7.91<data['Fare']) & (data['Fare']<=14.454), 'Fare_cat']=1\ndata.loc[(14.454<data['Fare']) & (data['Fare']<=31.0), 'Fare_cat']=2\ndata.loc[(31.0<data['Fare']) & (data['Fare']<=513), 'Fare_cat']=3","448c1c18":"type(data)","f23ec7e6":"sns.factorplot('Fare_cat', 'Survived', hue='Sex', data=data)\nplt.show()","844f8dad":"data['Sex'].replace(['female', 'male'], [0,1], inplace=True)\ndata['Embarked'].replace(['S', 'C', 'Q'], [0,1,2], inplace=True)\ndata['Initial'].replace(['Mr', 'Mrs', 'Miss', 'Master', 'Other'], [0,1,2,3,4], inplace=True)","5be76c72":"data.drop(['Name', 'Age', 'Ticket', 'Fare', 'Cabin', 'Fare_range','PassengerId'], axis=1, inplace=True)","3a1a6494":"data.head()","cabd0cc2":"sns.heatmap(data.corr(), annot=True, cmap='RdYlGn', linewidth=0.2, annot_kws={'size':20})\nfig=plt.gcf()\nfig.set_size_inches(18,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","fd086f9c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix","cef51ddd":"data.head()","e31386e8":"train, test = train_test_split(data, test_size=0.3, random_state=2019, stratify=data['Survived'])","a2f88cad":"train_X = train[train.columns[1:]]\ntrain_Y = train[train.columns[:1]]\ntest_X = test[test.columns[1:]]\ntest_Y = test[test.columns[:1]]\nX = data[data.columns[1:]]\nY = data['Survived']","0d3798ba":"model = svm.SVC(kernel='rbf', C=1, gamma=0.1)\nmodel.fit(train_X, train_Y)\nprediction=model.predict(test_X)\nprint('Accuracy for rbf SVM is {:.6f}'.format(metrics.accuracy_score(prediction, test_Y)))","3517d2f8":"model=svm.SVC(kernel='linear', C=0.1, gamma=0.1)\nmodel.fit(train_X, train_Y)\nprediction2=model.predict(test_X)\nprint('Accuracy for linear SVM is {:.6f}'.format(metrics.accuracy_score(prediction2, test_Y)))","ef91ba45":"model=LogisticRegression()\nmodel.fit(train_X, train_Y)\nprediction3=model.predict(test_X)\nprint('Accuracy for Logistic Regression is {:.6f}'.format(metrics.accuracy_score(prediction3, test_Y)))","940db8bd":"model=DecisionTreeClassifier()\nmodel.fit(train_X, train_Y)\nprediction4=model.predict(test_X)\nprint('Accuracy for Decision Tree is {:.6f}'.format(metrics.accuracy_score(prediction4, test_Y)))","ef418d8e":"model=KNeighborsClassifier()\nmodel.fit(train_X, train_Y)\nprediction5=model.predict(test_X)\nprint('Accuracy for KNN is {:.6f}'.format(metrics.accuracy_score(prediction5, test_Y)))","aa3c545e":"a_index = list(range(1,11))\na=pd.Series()\nx=[0,1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    model=KNeighborsClassifier(n_neighbors=i)\n    model.fit(train_X, train_Y)\n    prediction=model.predict(test_X)\n    a=a.append(pd.Series(metrics.accuracy_score(prediction, test_Y)))\nplt.plot(a_index, a)\nplt.xticks(x)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\nprint('Accuracies for different values of n are: ', a.values, 'with the max value as ', a.values.max())","dd295fa6":"model=KNeighborsClassifier(n_neighbors=8)\nmodel.fit(train_X, train_Y)\nprediction5=model.predict(test_X)\nprint('Accuracy for KNN is {:.6f}'.format(metrics.accuracy_score(prediction5, test_Y)))","491daa42":"model=GaussianNB()\nmodel.fit(train_X, train_Y)\nprediction6=model.predict(test_X)\nprint('Accuracy for NB is {:.6f}'.format(metrics.accuracy_score(prediction6, test_Y)))","e73690ff":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X, train_Y)\nprediction7=model.predict(test_X)\nprint('Accuracy for RF is {:.6f}'.format(metrics.accuracy_score(prediction7, test_Y)))","b7d2aeb5":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict","a31a4f70":"kfold=KFold(n_splits=10, random_state=2019)\nxyz=[]\naccuracy=[]\nstd=[]\nclassifiers = ['Linear Svm', 'Radial Svm', 'Logistic Regression', 'KNN',\\\n               'Decision Tree', 'Naive Bayes', 'Random Forest']\nmodels=[svm.SVC(kernel='linear'), svm.SVC(kernel='rbf'), LogisticRegression(), \\\n       KNeighborsClassifier(n_neighbors=9), DecisionTreeClassifier(), \\\n       GaussianNB(), RandomForestClassifier(n_estimators=100)]\nfor i in models:\n    model=i\n    cv_result=cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')\n    cf_result=cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2 = pd.DataFrame({'CV Mean': xyz, 'Std':std}, index=classifiers)\nnew_models_dataframe2","95cbda47":"plt.subplots(figsize=(12,6))\nbox=pd.DataFrame(accuracy, index=[classifiers])\nbox.T.boxplot()","d94093eb":"new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\nplt.title('Average CV Mean Accuracy')\nfig=plt.gcf()\nfig.set_size_inches(8,5)\nplt.show()","3a1c4a5e":"f, ax=plt.subplots(3,3,figsize=(12,10))\n\ny_pred=cross_val_predict(svm.SVC(kernel='rbf'), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax=ax[0,0], annot=True, fmt='2.0f')\nax[0,0].set_title('Matrix for rbf-SVM')\n\ny_pred=cross_val_predict(svm.SVC(kernel='linear'), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax=ax[0,1], annot=True, fmt='2.0f')\nax[0,1].set_title('Matrix for Linear-SVM')\n\ny_pred=cross_val_predict(KNeighborsClassifier(n_neighbors=9), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax=ax[0,2], annot=True, fmt='2.0f')\nax[0,1].set_title('Matrix for KNN')\n\ny_pred=cross_val_predict(RandomForestClassifier(n_estimators=100), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax=ax[1,0], annot=True, fmt='2.0f')\nax[0,1].set_title('Matrix for RF')\n\ny_pred=cross_val_predict(LogisticRegression(), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax=ax[1,1], annot=True, fmt='2.0f')\nax[0,1].set_title('Matrix for Logistic Regression')\n\ny_pred=cross_val_predict(DecisionTreeClassifier(), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax=ax[1,2], annot=True, fmt='2.0f')\nax[0,1].set_title('Matrix for Decision Tree')\n\ny_pred=cross_val_predict(GaussianNB(), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax=ax[2,0], annot=True, fmt='2.0f')\nax[0,1].set_title('Matrix for Naive Bayes')\n\nplt.subplots_adjust(hspace=0.2, wspace=0.2)\nplt.show()","1affb6bf":"**Random Forests**","1b062fac":"## Part 1: Exploratory Data Analysis (EDA)","ce6f44e9":"Family_Size=0 means that the passenger is alone. Cleary, if you are alone or family_size=0, then chances for survival is very low. For family_size>4, the changes decreases, too. This also looks to be an important feature for the model. Let's examine this further. ","7465eebe":"**Interpreting the Heatmap**\n\nFrom the above heatmap, we can see that the features are not much correlated. The highest correlation is between SibSp and Parch i.e 0.41. So we can carry on with all features. ","1bff5720":"**Interpreting Confusion Matrix**\n\nThe left diagonal shows the number of correct predictions made for each class while the right diagonal shows the number of wrong predictions made. Let's consider the first plot for rbf-SVM:\n\n1) The no. of correct predictions are 491 (for dead) + 247 (for survived) with the mean CV accuracy being (491+247)\/891=82.8% which we did get ealier.\n\n2) Errors -> Wrongly Classified 58 dead people as survived and 95 survived as dead. Thus it has made more mistakes by predicting dead as survived.\n\nBy looking at all the matrices, we can say that rbf-SVM has a higher chance in correctly predicting dead passengers but NaiveBayes has a higher chance in correctly predicting passengers who survived.","ffb9549d":"**K-Nearest Neighbors (KNN)**","a2337b68":"The crosstab again shows that larger families were in Pclass 3.","357f97ef":"Observations:\n\n1) The number of children increases with Pclass and the survival rate for passengers below Age 10(i.e children) looks to be good irrespective of the Pclass.\n\n2) Survival chances for Passengers aged 20-50 from Pclass 1 is high and is even better for Women. \n\n3) For males, the survival chances decreases with an increas in age. \n\nAs we had seen earlier, the Age feature has 177 null values. To replace these NaN values, we can assign them the mean age of the dataset. \n\nBut the problem is, there were many people with many different ages. We just can't assign a 4 year kid with the mean age that is 29 years. Is there any way to find out what age-band does the passenger lie?\n\nBingo! We can check the Name feature. Looking upon the feature, we can see that the names have a salutation like Mr or Mrs. Thus we can assign the man values of Mr and Mrs to the respective groups. \n\n\"What is in a Name?\"","bea71328":"Observations:\n\nThe barplot and factorplot shows that if a passenger is alone onboard with no siblings, he have 34.5% survival rate. The graph roughly decreases if the number of siblings increase. This makes sense. That is, if I have a family on board, I will try to save them instead of saving myself first. Surprisingly the survival for families with 5-8 members is 0%. The reason may be Pclass??\n\nThe reason is Pclass. The crosstab shows that Person with SibSp > 3 were all in Pclass 3. It is imminent that all the large families in Pclass(>) died. \n\n**Parch**","a84a8ef3":"## Part2: Feature Engineering and Data Clearing\n\n**Age_band**\n\nAs I have mentioned earlier that Age is a continous feature, there is a problem with Continous Variables in Machine Learning Models. \n\nWe need to convert these continous values into categorical values by either binning or normalization. I will using binning i.e group a range of ages into a single bin or assign them a single value. \n\nThe maximum age of a passenger was 80. So let's divide the range from 0-80 into 5 bins. So 80\/5-16. So bins of size 16. ","188b64d2":"The chances for survival for Port C is highest around 0.55 while it is lowest for S.","2cb54876":"Now the above correlation plot, we can see some positively related features. Some of them being SibSp and Family_size and Parch and Family_size and some negative ones like Alone and Family_size. ","8a720436":"The Age, Cabin and Embarked have null values. I will try to fix them.","45f9904c":"**Contents of the Notebooks:**\n\n**Part1: Exploratory Data Analysis (EDA)**\n\n1) Analysis of the features\n\n2) Finding any relations or trends considering multiple features \n\n**Part3: Feature Engineering and Data Cleaning**\n\n1) Adding any few features\n\n2) Removing redundatnt features\n\n3) Converting features into suitable form for modeling\n\n**Part3: Predictive Modeling**\n\n1) Running Basick Algorithm\n\n2) Cross Validation\n\n3) Ensembling\n\n4) Important Features Extraction","e2bf6f3d":"True that... the survival rate decreases as the age increases irrespective of the Pclass. ","a1ed89d4":"**Observations in a Nutshell for all features:**\n\nSex: The chance of survival for women is high as compared to men. \n\nPclass: There is visible trend that being a 1st class passenger gives you better chances of survival. The survival rate for Pclass 3 is very low. For women, the chance of survival from Pclass 1 is almost 1 and is high too for those from Pclass 2. Money Wins!!\n\nAge: Children less than 5-10 years do have a high chance of survival. Passengers between age group 15 to 35 died a lot. \n\nEmbarked: This a very interesting feature. The chances of survival at C looks to be better than even though the majority of Pclass 1 passengers got up at S. Passengers at Q were all from Pclass 3. \n\nParch+SibSp: Hainv 1-2 siblings, spouse on board or 1-3 parents shows a greater chance of probabilitiy rather than beigna lone or having a large family travelling with you. \n\n**Correlation Between the Features**","682a1c86":"**Logistic Regression**","62511b12":"The Women and Child First Policy thus hold true irrespective of class. \n\n**Embarked --> Categorical Value**","7a178e49":"**Cross Validation**\n\nMany a times, the data is imbalanced, i.e there may be a high number of class 1 instances but less number of other class instances. Thus we should train and test our algorithm on each and every instance of the dataset. Then we can take an average of all the noted accuracies over the dataset. \n\n1) The K-Fold Cross Validation works by first dividing the dataset into k-subsets. \n\n2) Let's say we divide the dataset into (k=5) parts. We reserve 1 part for testing and train the algorithm over the 4 parts. \n\n3) We continue the process by changing the testing part in each iteration and training the algorithm over the other parts. The accuracies and errors are then averaged to get an average accuracy of the algorithm. \n\nThis is called K-Fold Cross Validation. \n\n4) An algorithm may underfit over a dataset for some data and sometimes also overfit the data for other training set. Thus with cross-validation, we can achieve a generalized model. ","8c0a04ca":"Observations:\n\nHere two the results are quite similar. Passengers with their parents onboard have greater chance of survival. It however reduces as the number goes up. \n\nThe chances of survival is good for somebody who has 1-3 parents on the ship. Being alone also proves to be fatal and the chances for survival decreases when somebody has >4 parents on the ship. \n\n**Fare --> Continuous Feature**","7b130e01":"It is visible that being alone is harmful irrespective of Sex of Pclas except for Pclass 3 where the chances of females who are alone is high than those with family. \n\n**Fare_Range**\n\nSince fare is also a continuous feature, we need to convert it into ordinal value. For this, we will use panda.qcut. \n\nSo what qcut does is it splits or arranges the values according the number of bins we have passed. So, if we pass for 5 bins, it will arrange the values equally spaced into 5 seperate bins or value ranges. ","977d2c36":"Clearly, as the Fare_cat increases, the survival chances increses. This feature may become an important feature during modeling along with the Sex. \n\n**Converting String Values into Numeric**\n\nSince we cannot pass strings to a machine learning model, we need to convert features of Sex, Embarked, etc. into numeric values. ","b67bce8d":"The lowest fare is 0.0. Wow! a free luxorious ride!","da9425d9":"There looks to be a large distribution in the fares of Passengers in Pclass 1 and this distribution goes on decreasing as the standars reduces. As this is also continous, we can convert into discrete values by using binning. ","ff8b9be6":"There are some misspelled initials like Mile or Mme that stand for Miss, I will replace them with Miss and same thing for other values. ","d006065f":"Observations:\n\n1) The survival chances are alomost 1 for women for Pclass 1 and Pclass 2 irrespective of the Pclass. \n\n2) Port S looks to be very unlucky for Pclass 3 Passengers as the survival rate for both men and women is very low. (Money matters)\n\n3) Port Q looks to unluckes for Men, as almost all were from Pclass 3. \n\n**Filling Embarked NaN**\n\nAs we saw that maximum passengers boarded from Port S, we replace NaN with S. ","781beaae":"## Part3: Predictive Modeling\n\nWe have gained some insights from the EDA part. But with that, we cannot accurately predict or tell whether a passenger will survive or die. So, we will predict the whether the Passenger will survive or not using some great Classification Algorithms. Following are the algorithms I will use to make the model:\n\n1) Logistic Regression\n\n2) Support Vector Machines\n\n3) Random Forest\n\n4) K-Nearest Neighbors\n\n5) Naive Bayes\n\n6) Deicison Tree","44cacbb0":"**Linear Support Vector Machine (linear-SVM)**","d36b0e95":"**Radial Support Vector Machines (rbf-SVM)**","10fa1e6b":"Observations:\n\n1) Maximum passengers boarded from S. Majority of them being from Pclass 3. \n\n2) The Passengers from C look to be lucky as a good proportion of them survived. The reason for this maybe the rescue of all the Pclass 1 and Pclass 2 Passengers. \n\n3) The Embark S looks to the port from where majority of the rich people boarded. Still the chances for survival is low here, that is because many passengers from Pclass 3 around 81% didn't survive. \n\n4) Port Q had almost 95% of the passengers were from Pclass 3. ","a127cb90":"**Check for total null values**","f2018cef":"As discussed above, we can clearly see that as the fare_range increases, the chances of survival increases. \n\nNow, we cannot pass the Fare_range values as it is. We should convert it into singleton values smae as we did in Age_band.","06c370fc":"Now the accuracy for the KNN model changes as we change the values for n_neighbors attribute. The default value is 5. Let's check the accuracies over various values of n_neighbors. ","9bc47dd3":"People say Money Can't Buy Everything. But we can clearly see that Passengers of Pclass 1 were given a very high priority while rescue. Even though the number of Passengers in Pclass 3 were a lot higher, still the number of survival from them is very low, somewhere around 25%. \n\nThe survival rate of Pclass 1 is around 63% while that of Pclass 2 is around 48%. So money and status matters. Such a materialistic world. \n\nLet's dive in a littel bit more and check for other interesting observations. Let's check survival rate with Sex and Pclass Together. ","01257f91":"Chances for Survival by Embarked","17bbdc34":"**How many Survived?**","56b21e9d":"**Family_size and Alone**\n\nAt this point, we can create a new feature called 'Family_size' and 'Alone' and analyze it. This feature is the summation of Parch and SibSp. It gives us a combined data so that we can check if survival rate have anything to do with family size of the passengers. Alone will denote whether a passenger is alone or not. ","364b6b4c":"**SibSp --> Discrete Feature**","a7ce91e6":"This looks interesting. The number of men on the ship is lot more than the number of women. Still the number of women saved is almost twice the number of males saved. The survival rates for women on the ship is around 75% while that for men is around 18-19%.\n\nThis looks to be a very important feature for modeling. But is it the best? Let's check other features. \n\n**Pclass --> Ordinal Feature**","f42c9544":"**Dropping UnNeeded Features**\n\nName: We don't need name feature as it cannot be converted into any categorical value. \n\nAge: We have the Age_band feature, so no need to this. \n\nTicket: It is any random string that cannot be categorised. \n\nFare: We have the Fare_cat feature, so unneeded. \n\nCabin: A lot of NaN values and also many passengers have multiple cabins. So, this is a useless feature. \n\nFare_range: We have the Fare_cat feature. \n\nPassengerId: Cannot be categorized. ","c54a5d17":"Observations:\n\n1) The toddlers(age<5) were saved in large numbers(the Women and Child First Policy)\n\n2) The oldest Passenger was saved (80years).\n\n3) Maximum number of deaths were in the age group of 30-40.","46b9b074":"We use FactorPlot in this case, because they make the seperation of categorical values easy. \n\nLooking at the CrossTab and the FactorPlot, we can easily infer that survival for Women from Pclass 1 is about 95-96%, as only 3 out of 94 Women from Pclass 1 died. \n\nIt is evident that regardless of Pclass, Women were given first priority while rescue. Let's analyze other features. \n\n**Age --> Continuous Feature**","12a92aad":"The classification accuracy can be sometimes misleading due to imbalance. We can get a summarized result with the help of confusion matrix, which shows where did the model go wrong, or which class did the model predict wrong. \n\n**Confusion Matrix**\n\nIt gives the number of correct and incorrect classification made by the classifier.","76423527":"It is evident that not many passengers survived the accident. \n\nOut of 891 passengers in training set, only around 350 survived, i.e only 38.4% of the total training set survived the crash. We need to dig down more to get better insights from the data and see which categories of the passengers did survived and who didn't. \n\nWe will try to check the survival rate by using the different features of the dataset. Some of the features being Sex, Port of Embarcation, Age, etc.\n\nFirst, let us understand the different types of features. \n\n**Types of Features**\n\n**Categorical Features:**\n\nA categorical variable is one that has two or more categories and each value in that feature can be categorized by them. For example, gender is a categorical variable having two categories (male and female). Now we cannot sort of give any ordering to such variables. They are also known as Nominal Variables. \n\nCategorical Features in the dataset: Sex, Embarked. \n\n**Ordinal Features:**\nAn ordinal variabe is similar to categorical values, but the difference between them is that we can have relative ordering or sorting between the values. For ex: If we have a feature like Height with values Tall, Medium, Short, then Height is an ordinal variable. Here we can have a relative sort in the variable. \n\nOrdinal Features in the dataset: Pclass\n\n**Continuouse Features:**\n\nA feature is said to be continuous if it can take values between any two points or between the minimum or maximum values in the feature colums. \n\nContinuous Featrues in the dataset: Age\n\n**Analysing the Features**\n\n**Sex--> Categorical Feature**","78f90b80":"**Gaussian Naive Bayes**","4907bb08":"# EDA To Prediction (DieTanic)\n\n*Sometimes life has a cruel sense of humor, giving you the thing you always wanted at the worst time possible.*\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colling with an iceberg, killing 1502 out of 2224 passengers and crew. That's why the name DieTanic. This is a very unforgetable disaster that no one in the world can forget. \n\nIt took about $7.5 million to bulid the Titanic and it sunk under the ocean due to collision. The Titanic Dataset is a very good dataset for beginners to start a journey in data science and participate in competition in Kagge. \n\nThe objective of this notebook is to give an idea how is the workflow in any predictive modeling problem. How do we check features, how do we add new features and some Machine Learning Concepts. I have tried to keep the notebook as basic as possible so that even newbies can understand every phase of it. \n\nIf you like the notebook and think that it helped you, Please UPVOTE. It will keep me motivated. ","acba5a0a":"Filling NaN Ages","1a9f6178":"This is the third time I have rewritten the kernel of [EDA To Prediction (DieTanic)](https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic) made by [Ashwini Swain](https:\/\/www.kaggle.com\/ash316), which is awesome as belows:","8554ebc5":"The accuracy of a model is not the only factor that determines the robustness of the classifier. Let's say that a classifier is trained over a training data and tested over the test data and it scores an accuracy of 90%. \n\nNow this seems to be very good accuracy for a classifier, but can we confirm that it will be 90% for all the new sets that come over? The answer is No, because we can't determine which all instances will the classifier will use to train itself. As the training and testing data changes, the accuracy will also change. It may increase or decrease. This is known as model variance. \n\nTo overcome this and get a generalized model, we use Cross Validation. ","f36c6283":"**Decision Tree**"}}