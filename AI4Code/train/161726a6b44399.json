{"cell_type":{"4364cf5e":"code","fa463282":"code","1bbc5ff6":"code","c0484053":"code","9f947584":"code","928d4a80":"code","0bf3ab9a":"code","87b1c65d":"code","aed6e410":"code","20b37541":"code","b70be741":"code","65d4e7ae":"code","e2d240e6":"code","69e11832":"code","48396937":"code","26770aa9":"code","9b95931f":"code","abbb8dab":"code","4afdf2fc":"code","95a08166":"code","4ab99f97":"code","30e7796d":"code","10222ccd":"code","30c755a1":"code","7721372d":"code","8c48288c":"code","a527f9a0":"code","49243e12":"code","aef14b96":"code","639f577d":"code","37e57585":"code","2b062956":"code","a6f0acf7":"code","2c54cdb6":"code","21f79a88":"code","148cc4fd":"code","d9e2e49c":"code","4f3b20be":"code","7a8994dd":"code","5d3c5d24":"code","01e37008":"code","f4a45cba":"code","02401f24":"code","ecd75602":"code","3de8999a":"code","035e8332":"code","4d248a60":"code","fad957bf":"code","dee28c9e":"code","f4bef100":"code","6cedc28c":"code","f090c7b4":"code","2e67eaf3":"code","0b01d600":"code","b330895b":"markdown"},"source":{"4364cf5e":"import pandas as pd\nimport numpy as np","fa463282":"from sklearn.ensemble import VotingRegressor","1bbc5ff6":"from sklearn.metrics import mean_squared_error","c0484053":"from pathlib import Path","9f947584":"from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold","928d4a80":"from sklearn.preprocessing import StandardScaler","0bf3ab9a":"import optuna","87b1c65d":"from xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor","aed6e410":"INPUT = Path('..\/input\/tabular-playground-series-aug-2021')\ntrain = pd.read_csv(INPUT\/'train.csv')\ntest = pd.read_csv(INPUT\/'test.csv')","20b37541":"train.shape","b70be741":"train.head()","65d4e7ae":"test.head()","e2d240e6":"train.info()","69e11832":"train.isnull().any().sum()","48396937":"test.isnull().any().sum()","26770aa9":"ss_features = [col for col in test.columns if 'f' in col]\nss = StandardScaler()\ntrain[ss_features] = ss.fit_transform(train[ss_features])\ntest[ss_features] = ss.transform(test[ss_features])","9b95931f":"# X = train.drop('loss', axis=1)\n# X.head()","abbb8dab":"X = train","4afdf2fc":"y = train['loss']\ny.head()","95a08166":"# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=42)","4ab99f97":"# my_model = XGBRegressor(n_estimators=500)\n# my_model.fit(X_train, y_train, \n#              early_stopping_rounds=10, \n#              eval_set=[(X_valid, y_valid)])","30e7796d":"# my_model2 = XGBRegressor(n_estimators=500, learning_rate=0.05)\n# my_model2.fit(X_train, y_train, \n#              early_stopping_rounds=10, \n#              eval_set=[(X_valid, y_valid)])","10222ccd":"# param_grid = {\n#     \"n_estimators\" : [250, 300, 350, 400, 450, 500],\n#     \"max_depth\": [6, 8, 10, 12],\n#     \"learning_rate\": [0.05, 0.03, 0.01]\n# }","30c755a1":"# xgb_model = XGBRegressor(tree_method = 'gpu_hist')","7721372d":"# #gareebi\n# model = GridSearchCV(\n#     estimator=xgb_model,\n#     param_grid=param_grid,\n#     scoring='neg_mean_squared_error',\n#     verbose=10,\n#     cv=5\n# )","8c48288c":"# param_grid = {\n#     \"n_estimators\" : np.arange(250, 500, 50),\n#     \"max_depth\": np.arange(6,15),\n#     \"learning_rate\": np.arange(0.01, 0.1, 0.02)\n# }","a527f9a0":"# model = RandomizedSearchCV(\n#     estimator=XGBRegressor(),\n#     param_distributions=param_grid,\n#     n_iter=10,\n#     scoring='neg_mean_squared_error',\n#     verbose=10,\n#     cv=2\n# )","49243e12":"# model.fit(X, y)","aef14b96":"# print(model.best_score_)\n# print(model.best_estimator_.get_params())","639f577d":"kf = KFold(n_splits=100, shuffle=True, random_state=42)\nfor fold, (train_indices, valid_indices) in enumerate(kf.split(X=X)):\n    X.loc[valid_indices, 'kfold'] = fold","37e57585":"X.head()","2b062956":"# def objective(trial, X=X, y=y):\n#     train_x, val_x, train_y, val_y = train_test_split(X, y, test_size=0.2,random_state=42)\n#     param = {\n#         'tree_method':'gpu_hist',  \n# #         'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n# #         'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n# #         'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n# #         'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n#         'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.02),\n#         'n_estimators': trial.suggest_int('n_estimators', 500, 20000),\n#         'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n# #         'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n#         'predictor': 'gpu_predictor'\n#     }\n#     model = XGBRegressor(**param)  \n    \n#     model.fit(train_x,train_y,eval_set=[(val_x,val_y)],early_stopping_rounds=100,verbose=False)\n    \n#     preds = model.predict(val_x)\n    \n#     rmse = mean_squared_error(val_y, preds,squared=False)\n    \n#     return rmse","a6f0acf7":"# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=50)\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', study.best_trial.params)","2c54cdb6":"# def objective(trial, X=X, y=y):\n#     train_x, val_x, train_y, val_y = train_test_split(X, y, test_size=0.2,random_state=42)\n#     param = {\n#         'loss_function': 'RMSE',\n#         'task_type': 'GPU',\n#         'max_bin': trial.suggest_int('max_bin', 200, 400),\n# #         'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n#         'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n#         'n_estimators':  trial.suggest_int('n_estimators', 1000, 10000),\n#         'max_depth': trial.suggest_categorical('max_depth', [7,10,14,16]),\n# #         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 300),\n#     }\n#     model = CatBoostRegressor(**param)   \n    \n#     model.fit(train_x,train_y,eval_set=[(val_x,val_y)],early_stopping_rounds=100,verbose=False)\n    \n#     preds = model.predict(val_x)\n    \n#     rmse = mean_squared_error(val_y, preds,squared=False)\n    \n#     return rmse","21f79a88":"# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=50)\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', study.best_trial.params)","148cc4fd":"# def objective(trial, X=X, y=y):\n#     train_x, val_x, train_y, val_y = train_test_split(X, y, test_size=0.2,random_state=42)\n#     params = {\n# #         'reg_alpha' : trial.suggest_loguniform('reg_alpha' , 0.47 , 0.5),\n# #         'reg_lambda' : trial.suggest_loguniform('reg_lambda' , 0.32 , 0.33),\n# #         'num_leaves' : trial.suggest_int('num_leaves' , 50 , 70),\n#         'learning_rate' : trial.suggest_uniform('learning_rate' , 0.005 , 0.01),\n#         'max_depth' : trial.suggest_int('max_depth', 30 , 40),\n#         'n_estimators' : trial.suggest_int('n_estimators', 500 , 1000),\n# #         'min_child_weight' : trial.suggest_loguniform('min_child_weight', 0.015 , 0.02),\n# #         'subsample' : trial.suggest_uniform('subsample' , 0.9 , 1.0), \n# #         'colsample_bytree' : trial.suggest_loguniform('colsample_bytree', 0.52 , 1),\n# #         'min_child_samples' : trial.suggest_int('min_child_samples', 76, 80),\n#         'metric' : 'rmse',\n#         'device_type' : 'gpu',\n#     }\n#     model = LGBMRegressor(**params)   \n    \n#     model.fit(train_x,train_y,eval_set=[(val_x,val_y)],early_stopping_rounds=100,verbose=False)\n    \n#     preds = model.predict(val_x)\n    \n#     rmse = mean_squared_error(val_y, preds,squared=False)\n    \n#     return rmse","d9e2e49c":"# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=50)\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', study.best_trial.params)","4f3b20be":"useful_features = [col for col in X.columns if col not in ('loss', 'kfold')]","7a8994dd":"# cat_params = {\n#     'max_bin': 371,\n#     'bagging_fraction': 0.8738144899637044,\n#     'learning_rate': 0.011193707939301828,\n#     'n_estimators': 8365, \n#     'max_depth': 7,\n#     'min_data_in_leaf': 131\n# }","5d3c5d24":"# lgb_params = {\n#     'reg_alpha': 0.48840802189984306,\n#     'reg_lambda': 0.32228168870395957,\n#     'num_leaves': 63,\n#     'learning_rate': 0.008666396072105235,\n#     'max_depth': 33,\n#     'n_estimators': 998,\n#     'min_child_weight': 0.016668851601889464,\n#     'subsample': 0.9578851892053223,\n#     'colsample_bytree': 0.9998763428647909,\n#     'min_child_samples': 77,\n#     'device_type' : 'gpu'\n# }","01e37008":"# xgb_params = {\n#     'lambda': 3.9781336661402147,\n#     'alpha': 0.239107928050113,\n#     'colsample_bytree': 1.0,\n#     'subsample': 0.7,\n#     'learning_rate': 0.009474189754361973,\n#     'n_estimators': 1623,\n#     'max_depth': 17,\n#     'min_child_weight': 1,\n#     'tree_method':'gpu_hist',  \n# }","f4a45cba":"# lgb_params = {\n#     'device_type' : 'gpu',\n#     'learning_rate': 0.009975639212494584, 'max_depth': 32, 'n_estimators': 1000\n    \n# }","02401f24":"xgb_params = {\n    'learning_rate': 0.013222817649672616,\n    'n_estimators': 12462,\n    'max_depth': 5,\n    'tree_method':'gpu_hist',   \n    'predictor': 'gpu_predictor'\n}","ecd75602":"# cat = CatBoostRegressor(**cat_params)\n# lgb = LGBMRegressor(**lgb_params)\nxgb = XGBRegressor(**xgb_params)","3de8999a":"xgb_predictions = []\nfor fold in range(100):\n    xtrain =  X[X.kfold != fold].reset_index(drop=True)\n    xvalid =  X[X.kfold == fold].reset_index(drop=True)\n\n    ytrain = xtrain.loss\n    yvalid = xvalid.loss\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n#     model = VotingRegressor(\n#             estimators = [\n#                 ('lgbm', lgb),\n# #                 ('cat', cat),\n#                 ('xgb', xgb)\n#             ],\n#             weights = [0.4, 0.6],\n#         verbose: True\n#         )\n    model = xgb\n    model.fit(xtrain, ytrain, early_stopping_rounds=10, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(test)\n    xgb_predictions.append(test_preds)\n    print(f\"fold: {fold}, rmse: {mean_squared_error(yvalid, preds_valid, squared=False)}\")","035e8332":"# lgb_predictions = []\n# for fold in range(10):\n#     xtrain =  X[X.kfold != fold].reset_index(drop=True)\n#     xvalid =  X[X.kfold == fold].reset_index(drop=True)\n\n#     ytrain = xtrain.loss\n#     yvalid = xvalid.loss\n    \n#     xtrain = xtrain[useful_features]\n#     xvalid = xvalid[useful_features]\n\n#     model = lgb\n#     model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n#     preds_valid = model.predict(xvalid)\n#     test_preds = model.predict(test)\n#     lgb_predictions.append(test_preds)\n#     print(f\"fold: {fold}, rmse: {mean_squared_error(yvalid, preds_valid, squared=False)}\")","4d248a60":"# xgb_predictions = np.array(xgb_predictions)\n# lgb_predictions = np.array(lgb_predictions)\n# final_predictions = 0.6*xgb_predictions + 0.4*lgb_predictions","fad957bf":"final_predictions = xgb_predictions","dee28c9e":"# final_model = XGBRegressor(**xgb_params)\n# final_model.fit(X_train, y_train, early_stopping_rounds=20, \n#              eval_set=[(X_valid, y_valid)])","f4bef100":"# final_predictions = final_model.predict(test)","6cedc28c":"# final_model.saves(\"final_model.h5\")","f090c7b4":"predictions = np.mean(np.column_stack(final_predictions), axis=1)","2e67eaf3":"submission = pd.read_csv(INPUT \/ 'sample_submission.csv')\nsubmission.head()","0b01d600":"submission['loss'] = predictions\n# submission['loss'] = final_predictions\nsubmission.to_csv(\"submission.csv\", index=False)\n\nsubmission","b330895b":"## As there are no missing values, we dont need to use a imputer"}}