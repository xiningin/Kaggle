{"cell_type":{"35a46224":"code","f087c6e2":"code","24c45199":"code","6d533864":"code","9444365b":"code","6c394c59":"code","28e75a68":"code","8c972e37":"code","4bb6b15b":"code","543b949c":"code","e072d679":"code","cb3df7bc":"code","6fdba56d":"code","ff0c6f66":"code","dba26845":"code","2eb6e754":"code","73789d44":"code","dd3d74d0":"code","bd8f2a4a":"code","6b44f0ed":"code","d3f743a9":"code","2d8a0313":"code","65762762":"code","697d7249":"code","e86cc06c":"code","ab8d8810":"code","6b7413c3":"code","1251063f":"code","a3f5b590":"code","632da2ee":"code","332a4b1e":"code","689cc90d":"code","f57a02c2":"code","557a000b":"code","04de749b":"code","e6417815":"code","89ca5486":"code","c6542d3b":"code","84f4f54a":"code","eff696bb":"code","2edf0daa":"code","84ea074b":"code","08176b90":"code","68fa087b":"markdown","a2389be1":"markdown","4034c017":"markdown","139f1e53":"markdown","d0ac6a9f":"markdown","0965e3a0":"markdown","5817e5aa":"markdown","6aedba0a":"markdown","56be6af5":"markdown","b40909d7":"markdown","f6557848":"markdown","aa358a74":"markdown","d0ff5f3a":"markdown","36c95c4c":"markdown","5d73cacc":"markdown","9b8320f7":"markdown","5135bb99":"markdown","4288cc28":"markdown","04e4ffbe":"markdown","4c8a4d2c":"markdown","6fdd4fe3":"markdown","c066aea0":"markdown","442e9fdf":"markdown","c3a8d0e8":"markdown","2afd246f":"markdown","30e04c4a":"markdown","07142e73":"markdown","04c614cc":"markdown","ef2cede8":"markdown","1c86cd14":"markdown","dcb0f42a":"markdown","3965d04e":"markdown","db59ed4e":"markdown","3d053bbd":"markdown","66249f47":"markdown","21522ad4":"markdown","b43b7f74":"markdown","ea518fe1":"markdown","333e4af6":"markdown","5ef4dd35":"markdown","1e948c18":"markdown","c9785a9b":"markdown","b502c4f1":"markdown","73b8afaa":"markdown","81265a20":"markdown","22cc7d3f":"markdown","a54b88e3":"markdown","80caaa35":"markdown","8ae28654":"markdown","cedc021d":"markdown","c69b3ae1":"markdown","ac91dd90":"markdown","d5ffc3fe":"markdown","28bc86dd":"markdown","5ced55d6":"markdown","80c6d564":"markdown","c1ff8c4b":"markdown","6fa8b582":"markdown","6c632cb8":"markdown","11d53966":"markdown","7865db26":"markdown","058abe23":"markdown","33905894":"markdown","1f1e7469":"markdown","4025ceff":"markdown","624f299c":"markdown","f2eeb0d8":"markdown"},"source":{"35a46224":"#importing the libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n#Blocking some nasty warnings\nimport warnings\nwarnings.filterwarnings('ignore', category=Warning)\nprint(os.listdir(\"..\/input\"))","f087c6e2":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\n\n# A brief look into our training data\n\ndf_train.head()","24c45199":"df_train.set_index('Id', inplace=True)\ndf_test.set_index('Id', inplace=True)","6d533864":"plt.figure(figsize=[40,30])\n_ = sns.heatmap(df_train.corr(),annot=True)","9444365b":"df_train.drop(['GarageArea','1stFlrSF','TotRmsAbvGrd','2ndFlrSF'], axis=1, inplace=True)\ndf_test.drop(['GarageArea','1stFlrSF','TotRmsAbvGrd','2ndFlrSF'], axis=1, inplace=True)","6c394c59":"plt.scatter(df_train[['GrLivArea']],df_train[['SalePrice']])\nplt.xlabel('Total Living Area Excluding Basement(square foot)')\nplt.ylabel('Sale Price')","28e75a68":"df_train = df_train[df_train['GrLivArea']<4500]","8c972e37":"plt.scatter(df_train[['GrLivArea']],df_train[['SalePrice']])\nplt.xlabel('Total Living Area Excluding Basement(square foot)')\nplt.ylabel('Sale Price')","4bb6b15b":"# A function to percentage missing of overall\ndef check_nulls(df):\n    percent_missing = (df.isnull().sum() * 100 \/ len(df)).sort_values()\n    return round(percent_missing,2)","543b949c":"check_nulls(df_train)","e072d679":"check_nulls(df_test)","cb3df7bc":"categorical_list = [col for col in df_train.columns if df_train[col].dtypes == object]\nnumerical_list = [col for col in df_train.columns if df_train[col].dtypes != object]\n\nprint('Categories:', categorical_list)\nprint('Numbers:', numerical_list)","6fdba56d":"def fill_missing_values(df):\n    lst = [\"Alley\",\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\n             \"BsmtFinType2\",\"Fence\",\"FireplaceQu\",\"GarageType\",\"GarageFinish\",\n             \"GarageQual\",\"GarageCond\",\"Electrical\",\"GarageFinish\",\"MiscFeature\",\"MasVnrType\",\"PoolQC\"]\n    for col in lst:\n        df[col] = df[col].fillna(\"Not present\")\n        \n    lst = ['GarageYrBlt','MasVnrArea','BsmtFinSF1','BsmtFinSF2','TotalBsmtSF',\n           'BsmtUnfSF','BsmtFullBath','BsmtHalfBath','MasVnrArea','GarageCars']\n    for col in lst:\n        df[col] = df[col].fillna(0)\n        \n    lst = ['Utilities','MSZoning','Exterior1st','Exterior2nd','Electrical','KitchenQual']\n    for col in lst:\n        df[col] = df[col].fillna(df[col].mode()[0])\n    \n    df['Functional'] = df['Functional'].fillna('Typ')\n    df['SaleType'] = df['SaleType'].fillna('Normal')\n    df['LotFrontage'] = df['LotFrontage'].fillna(df.LotFrontage.mean())\n    #removing 'PoolQC' as discussed\n    df.drop('PoolQC', axis=1, inplace=True)\n   \n","ff0c6f66":"fill_missing_values(df_train)\nfill_missing_values(df_test)","dba26845":"# Checking null values\nprint(df_train.isnull().sum().sum())\nprint(df_test.isnull().sum().sum())","2eb6e754":"# A function to label encode our ordinal data\ndef label_encode(df):\n    df['MSSubClass'] = df['MSSubClass'].astype(object)\n    df = df.replace({\"Alley\" : {\"Not present\" : 0, \"Grvl\" : 1, \"Pave\" : 2},\n    \"BsmtCond\" : {\"Not present\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"BsmtExposure\" : {\"Not present\" : 0, \"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n    \"BsmtFinType1\" : {\"Not present\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n    \"ALQ\" : 5, \"GLQ\" : 6},\n    \"BsmtFinType2\" : {\"Not present\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n    \"ALQ\" : 5, \"GLQ\" : 6},\n    \"BsmtQual\" : {\"Not present\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"CentralAir\" : {\"N\" : 0, \"Y\" : 1},\n    \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n    \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n    \"FireplaceQu\" : {\"Not present\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n    \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n    \"GarageCond\" : {\"Not present\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"GarageQual\" : {\"Not present\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"GarageFinish\" :{\"Not present\" : 0, \"Unf\" : 1, \"RFn\" : 2, \"Fin\" : 3},\n    \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n    \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n    \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n    \"PoolQC\" : {\"Not present\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n    \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n    \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4},\n    \"Fence\": {\"Not present\" : 0, \"MnWw\" : 1, \"GdWo\" : 2, \"MnPrv\" : 3, \"GdPrv\" : 4 }},\n                       \n                     )\n    return df\n\n","73789d44":"df_train = label_encode(df_train)\ndf_test = label_encode(df_test)","dd3d74d0":"_ = sns.distplot(df_train[\"SalePrice\"])","bd8f2a4a":"df_train['SalePrice']=np.log(df_train['SalePrice'])\n_ = sns.distplot(df_train[\"SalePrice\"])","6b44f0ed":"cat_list = [col for col in df_train.columns if df_train[col].dtypes == object]\nnum_list = [col for col in df_train.columns if df_train[col].dtypes != object]","d3f743a9":"categorical_data = df_train[cat_list]\nnumerical_data = df_train[num_list]\ndf_train = categorical_data.join(numerical_data)","2d8a0313":"num_list.remove('SalePrice')\ncat_test = df_test[cat_list]\nnum_test = df_test[num_list]\ndf_test = cat_test.join(num_test)","65762762":"X = df_train.drop('SalePrice', axis=1).values\ny = df_train['SalePrice'].values\ndf_test_values = df_test.values","697d7249":"#We import the LabelEncoder and OneHotEncoder classes\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n# Let us now define a function that will use the classes we imported to encode our data\ndef encode(X):\n    # We create an object of LabelEncoder\n    labelencoder = LabelEncoder()\n    for i in range(len(cat_list)):\n        \n        #Using the fit_transform method we will convert each column of the categorical data into\n        #numerical values\n        \n        X[:,i] = labelencoder.fit_transform(X[:,i])\n    for i in range(len(cat_list)):\n        # Now we will convert the values into dummy variables\n        onehotencoder = OneHotEncoder(categorical_features=[i])\n        X = onehotencoder.fit_transform(X).toarray()\n        #For each column, we will remove the first dummy variable. This is done to avoid dummy variable trap\n        X = X[:,i:]\n    ","e86cc06c":"encode(X)\nencode(df_test_values)","ab8d8810":"#We need to import the StandardScaler class\nfrom sklearn.preprocessing import StandardScaler\n\n#We create an object of the class\nsc = StandardScaler()\n\n#We scale both X (training data) and the testing data\nX = sc.fit_transform(X)\ndf_test_values = sc.transform(df_test_values)","6b7413c3":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)","1251063f":"import math\ndef rmsle(y, y_pred):\n    assert len(y) == len(y_pred)\n    terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n    return (sum(terms_to_sum) * (1.0\/len(y))) ** 0.5","a3f5b590":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\n\n#We train our model using 80% of the training data and predict\nlm.fit(X_train,y_train)\ny_pred_reg = lm.predict(X_test)","632da2ee":"lm.intercept_","332a4b1e":"rmsle(np.exp(y_test), np.exp(y_pred_reg))","689cc90d":"from sklearn.linear_model import Lasso\nlasso_model = Lasso()\nlasso_model.fit(X_train,y_train)","f57a02c2":"y_pred_lasso = lasso_model.predict(X_test)","557a000b":"rmsle(np.exp(y_test), np.exp(y_pred_lasso))","04de749b":"from sklearn.linear_model import LassoCV\nlcv = LassoCV()\nlcv.fit(X_train,y_train)","e6417815":"lcv.alpha_","89ca5486":"lcv.intercept_","c6542d3b":"y_pred_lassocv = lcv.predict(X_test)","84f4f54a":"rmsle(np.exp(y_test),np.exp(y_pred_lassocv))","eff696bb":"model = Lasso(lcv.alpha_)\nmodel.fit(X,y)\ny_pred = model.predict(df_test_values)\npredictions = np.exp(y_pred)","2edf0daa":"result=pd.DataFrame({'Id':df_test.index, 'SalePrice':predictions})\nresult.to_csv('submission.csv', index=False)","84ea074b":"fig = plt.figure(figsize=(20,10))\nplt.subplot(1, 3, 1)\nplt.plot(np.arange(len(y_test)), y_test, label='Testing')\nplt.plot(np.arange(len(y_pred_reg)), y_pred_reg, label='Regression')\nplt.legend()\nplt.subplot(1,3,2)\nplt.plot(np.arange(len(y_test)), y_test, label='Testing')\nplt.plot(np.arange(len(y_pred_lasso)), y_pred_lasso, label=\"Lasso\")\nplt.legend()\nplt.subplot(1,3,3)\nplt.plot(np.arange(len(y_test)), y_test, label='Testing')\nplt.plot(np.arange(len(y_pred_lassocv)), y_pred_lassocv, label=\"Lasso CV\")\nplt.legend()\nplt.show()","08176b90":"result.head()","68fa087b":"We believe this step is necessary as testing our model will be handy in assessing it's performance before we submit to Kaggle. We therefore split our test data into 80% train and 20% test. The code below does exactly that. Now we are ready to build our model.","a2389be1":"Lets create a function that fills in the null values","4034c017":"Now we can use the lists we generated above to filter our dataframes into categorical data and numerical data. We will then concatinate the two mini-dataframes. This has now allowed us to have categorical data first and then numerical data. This step is just for convenience.","139f1e53":"Our heatmap tells us that we have multicollinearity in our data. Let us deal with it as best as we can. Between two highly correlated predictors, we will keep the one that is more correlated to the 'SalePrice' and dump the other.","d0ac6a9f":"Performing the function to both df_train and df_test","0965e3a0":"Now that we can proceed to data preprocessing.","5817e5aa":"Importing the dataset","6aedba0a":"Let us now predict using our model","56be6af5":"### Correlation","b40909d7":"Our data contains values that are on a different scale. We have little numbers such as our ordinal data that we have looked at as well as large numbers which are in tens of thousands and hundreds of thousands. We should, therefore, perform feature scaling. This will allow our data to be on the same scale and therefore predictions will be more accurate. We will do this for all 'X' values for both the train and test data. Our 'y' variable is log transformed and therefore we choose not to scale it. For this step we will use the StandardScaler class. Such scaling is also known as standardization. It is prefered when the data possibly contains outliers. ","f6557848":"Now we can calculate the RMSLE","aa358a74":"Let us see the results.","d0ff5f3a":"Let us see our model's y-intercept","36c95c4c":"Now we can happily apply our function to our df_train","5d73cacc":"To do so we create two lists:\n1. categorical_list containing all column names of columns that contain strings.\n2. numerical_list containing all column names of columns that do not contain strings.","9b8320f7":"Much better isn't it? Now we can combine both the entire training data to train our model for the Kaggle submission. We will now use a Lasso with our calculated alpha.","5135bb99":"Now let us check the RMSLE","4288cc28":"Let us do the same to df_test. Since df_test has no 'SalePrice' we remove it from our list","04e4ffbe":"Our score looks good on the training set. Our target was to get a score less than or equal to 0.15 and with this model on Kaggle we achieve a score of 0.13","4c8a4d2c":"Before showing the model we submitted to Kaggle, we will briefly show our other attempts.","6fdd4fe3":"# Feature scaling","c066aea0":"Our first attempt at using lasso gave us a Kaggle score greater than 0.41. However we kept on playing with \"alpha\", giving it different values and our model started reaching our target as we went lower. We then tried to find the most optimal alpha using the LassoCV class. [Here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LassoCV.html) is the sklearn documentation about LassoCV","442e9fdf":"This project aims to solve a Kaggle problem. We are given two datasets that contain house data from Ames, Iowa that can be used to predict a house price. With 79 house features, the Ultimate goal of this project is to train a model based on the given data to predict the house price as accurately as possible evaluated by the Kaggle score. Our aim is to get a score of 0.15 or less; in this case less is better.","c3a8d0e8":"Let us consider plotting the scatterplots of the dependant variable and all the features that correlate with it the most. This will help us detect outliers. On that note. It seems rather pointless to plot 'OverallQual' as it contains ordinal data. 'GrLivArea' makes the most sense and the other two highly correlated values ('GarageCars' and 'GarageArea') are about garages and not all houses have garages. Therefore we will plot 'SalePrice' vs 'GrLivArea'","2afd246f":"We will now define a function to calculate RMSLE (Root Mean Square Logarithmic Error). This is the metric used by Kaggle. Your final score is calculated using this metric. We therefore, copied this function from Kaggle in order to assess our model's performance. By using this function on our train data, we will get an estimate of our final score.","30e04c4a":"We will now apply the label_encode function to our dataframes","07142e73":"## Splitting our test data into train and test","04c614cc":"We then apply a log transformation to our 'y' variable and run another distribution plot.","ef2cede8":"Let us see our plot again without the outliers","1c86cd14":"Due to the presence of multicollinearity in our data. The Lasso regression might be a more favourable choice than multiple linear regression. Below we will implement the Lasso regression.","dcb0f42a":"Save the predictions into a csv with columns: 'Id' and 'SalePrice'. This shows the predicted SalePrice for each house (identified by Id).","3965d04e":"Now we can happily apply our function to our df_test","db59ed4e":"![](https:\/\/github.com\/smtolo\/EDSA\/blob\/master\/comparison_table.PNG)","3d053bbd":"First we import all the libraries we wish to use for our EDA","66249f47":"# Model Comparison","21522ad4":"Now let us encode our categorical data.","b43b7f74":"### LassoCV","ea518fe1":"We can see that both df_train and df_test have missing values. Therefore they should be dealt with.To do that we will create a function to help us fill the missing data.\n\nOur friend \"data_description.txt\" has something to tell us about the missing data. The \"missing data\" is not actually missing, at least for the most part. The feature they are supposed to describe simply does not exist for that particular house. Therefore we will replace the \"NaN\" values with \"Not present\" in categorical data columns where \"NaN\" is said to mean the feature which does not exist and 0 for numerical ones. However, our Kaggle score gives us a pat in the back when we remove 'PoolQC'. It has more than 99% of missing values in both datasets. That means that less than 1% of the houses have pools. It is therefore not so relevant to include pool quality as a predictor. The rest we will replace with the most occuring value. In fact it would be good to see which of our data is numerical and which is categorical.","333e4af6":"We can clearly see that the houses above 4500 square feet are outliers. They have a very high area and a very little value. Let us remove them.","5ef4dd35":"# The Model","1e948c18":"This is the step where we  prepare our data for modelling. We will start by seperating our data into numerical and categorical just like we did earlier. Please note that now our numerical data also contains ordinal data. Remember the encoding we did?","c9785a9b":"Now we need to see correlation within our features. We will do that by plotting a heatmap","b502c4f1":"### Multiple Linear Regression","73b8afaa":"Using our function, we encode the X values of the training and testing datasets","81265a20":"### The Lasso","22cc7d3f":"# Exploratory Data Analysis","a54b88e3":"![Picture](https:\/\/user-images.githubusercontent.com\/31653400\/58634113-c1639d80-82ea-11e9-80b4-30ee39d9667a.PNG)","80caaa35":"Let us see the intercept our LassoCV has calculated below.","8ae28654":"Now we plot the predictions of each model against the 20% we reserved for testing. This will show us our model's performance.","cedc021d":"The table below shows performances for the different models we implemented. \"The Lasso\" is our implementation of Lasso with the default alpha and \"LassoCV\" is the implementation of LassoCV.","c69b3ae1":"Now we may want to look at the distribution of our independent variable.  This has been a useful step in reducing our RMSLE. This is the current distribution of our independent variable:","ac91dd90":"Do we still have these nasty \"NaN's\" ?","d5ffc3fe":"LassoCV is similar to Lasso. However LassoCV performs some cross validation to try and find the alpha that produces the optimal result. If you do not specify the type of cross validation to use, it will use the default 3-fold cross validation.","28bc86dd":"Now, let us set 'Id' as an index so that it is not used by our model but kept for later use.","5ced55d6":"Our models show some accuracy. Well that is except for the Lasso.","80c6d564":"1. A detailed explaination of EDA: https:\/\/www.youtube.com\/watch?v=zHcQPKP6NpM&t=2s\n2. A typical solution using Multiple Regression: https:\/\/www.youtube.com\/watch?v=eZgeYzf2QI4\n3. Comprehensive data exploration with Python: https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n4. Islr textbook on introduction to statistical learning: http:\/\/www-bcf.usc.edu\/~gareth\/ISL\/\n5. LassoCV: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LassoCV.html","c1ff8c4b":"Now we can seperate our training dataset to 'X' and 'y' variables. We will also create an array of X test values from df_test","6fa8b582":"Now that we have no more missing data we can carry on with the next step of our EDA.\n\nAgain after a careful look at data_description.txt we notice that some of our categorical data is infact ordinal. So let us encode them in order of importance. We can use the LabelEncoder class but it might not encode them in the order we want. Therefore we will use our own custom encoder function to do the job for us. Functions are awesome, aren't they?","6c632cb8":"Now let us check the RMSLE","11d53966":"#                       House Prices: Advanced regression techniques","7865db26":"# Data Preprocessing","058abe23":"*********************************************************************************************\nI see some \"NaN\" values. We cannot predict much with \"\"NaN's\", can we?\nWe need to see how much of our data is missing. To do that we create a function to help us.","33905894":"Now we can predict the y-values by using our model","1f1e7469":" We import the LinearRegression class","4025ceff":"# References","624f299c":"Let us see the alpha our LassoCV has calculated below.","f2eeb0d8":"Using all the data we have prepared we build a multiple linear regression model"}}