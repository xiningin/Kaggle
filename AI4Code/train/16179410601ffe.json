{"cell_type":{"58fda359":"code","f0ecbdeb":"code","ba4433ff":"code","ebcc4211":"code","9b3a14bc":"code","57f5c184":"code","f911fc1e":"code","9f1a84e2":"code","fdc8b327":"code","0c83c506":"code","95d61f28":"code","30169a91":"code","c680f03b":"code","d4fa9daa":"code","c3a383f6":"code","77b1344c":"markdown","b3b7b1a8":"markdown","e3d4d79e":"markdown","ce146c8b":"markdown","13fee075":"markdown","72f43e03":"markdown","09e11cd4":"markdown","656cb3f6":"markdown","f29ca024":"markdown","64a1a9d2":"markdown","503e6162":"markdown","bb4e4957":"markdown","d2d46329":"markdown","d5b9064e":"markdown","5ed55b60":"markdown"},"source":{"58fda359":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import optimizers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom keras import regularizers","f0ecbdeb":"df = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsample = pd.read_csv('..\/input\/sample_submission.csv')\ndf.head()","ba4433ff":"df.glucose_concentration.hist()\nplt.show()","ebcc4211":"df.serum_insulin.hist()\nplt.show()","9b3a14bc":"min_max_scaler = preprocessing.MinMaxScaler()\n#Scaling The Training Data\nx = df.values\nx_scaled = min_max_scaler.fit_transform(x)\ndf = pd.DataFrame(x_scaled)\n#Scaling The Testing Data\ny = test.values\ny_scaled = min_max_scaler.fit_transform(y)\ntest = pd.DataFrame(y_scaled)\ndf.head()","57f5c184":"df[2].hist() #Glucose Concentration\nplt.show()","f911fc1e":"df[5].hist() #Serum Insulin\nplt.show()","9f1a84e2":"train, dev = train_test_split(df, test_size=0.2)","fdc8b327":"hidden_units=300\nlearning_rate=0.005 #Learning rate was quite optimal\nhidden_layer_act='tanh'\noutput_layer_act='sigmoid'\nno_epochs=100 #Increasing The epochs would overfit\nbsize = 128 #Batch Size Of 128 ","0c83c506":"model = Sequential()\n\nmodel.add(Dense(hidden_units, input_dim=8, activation=hidden_layer_act))\nmodel.add(Dense(hidden_units, activation=hidden_layer_act))\nmodel.add(Dense(1, activation=output_layer_act))","95d61f28":"adam = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\nmodel.compile(loss='binary_crossentropy',optimizer=adam, metrics=['acc'])","30169a91":"train_x=train.iloc[:,1:9]\ntrain_y=train.iloc[:,9]\n\nmodel.fit(train_x, train_y, epochs=no_epochs, batch_size= bsize,  verbose=2)","c680f03b":"val_loss, val_acc = model.evaluate(dev.iloc[:,1:9], dev.iloc[:,9])\nprint(\"Validation Loss : \", val_loss)\nprint(\"Validation Acc : \",val_acc)","d4fa9daa":"test_x=test.iloc[:,1:9]\npredictions = model.predict(test_x)\nprint(predictions)","c3a383f6":"rounded = [int(round(x[0])) for x in predictions]\nprint(rounded)\nsample.diabetes = rounded\nsample.to_csv('submission.csv',index = False)","77b1344c":"**Submission File**","b3b7b1a8":"**Since there were a lot features with high magnitude and it was causing high variance\/Overfitting on the training data, I decided to Scale the features in the range of 0 to 1 using Min-Max_scaler.**","e3d4d79e":"**Visualizing The Important Features**","ce146c8b":"**Setting Up the Hyperparameters**","13fee075":"**Validation Loss and Accuracy**","72f43e03":"**Visualizing The Important Features After Scaling**","09e11cd4":"**Low Bias and Low Variance ** : \nCompared to the highest accuracy this model performed well on training data(Low Bias) and gave a equivalent accuracy on the validation set (Low Variance)","656cb3f6":"**Setting Up loss function, Optimizer, Metrics**","f29ca024":"**Spliting the training data into Traning and Developement Data**","64a1a9d2":"**Loading The Data**","503e6162":"# My First Kaggle InClass Competition","bb4e4957":"**Training the Model**","d2d46329":"**Model Architechture**","d5b9064e":"**Importing The Libraries**","5ed55b60":"**Predicting The outputs for the Training Data**"}}