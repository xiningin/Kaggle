{"cell_type":{"5a34c651":"code","b0eb3a7d":"code","87daa6f6":"code","57201ba7":"code","430b51d4":"code","49771b25":"code","aaf14ac9":"code","9893b3d9":"code","af2ec8e8":"code","e75cea3c":"code","5e073736":"code","2f6b2961":"code","5d2e9860":"code","58217d51":"code","f303d123":"code","c46e0a4c":"code","c4eca3b9":"code","fe15efee":"code","4aae830e":"code","034569c4":"code","59527af7":"code","916fced2":"code","948bda5e":"code","6b10dcdf":"code","4f56f749":"code","4a233b5f":"code","91e86dfc":"code","cf4a2825":"code","5e318c01":"code","99e8e971":"code","a3c43ea4":"code","676366cf":"code","2b7e1f8f":"code","e4f96bf2":"code","c5031d28":"code","533b9a21":"code","841d7f93":"code","c0d342d2":"markdown","e555df31":"markdown","71fe3c66":"markdown","b02f7388":"markdown","b9938b10":"markdown","14baf546":"markdown","bd8042c0":"markdown","da59501b":"markdown","ac52aa31":"markdown","0f11d547":"markdown","4d2b946f":"markdown","2efeebe3":"markdown","7a28bbc1":"markdown","8afb0f85":"markdown","225d3270":"markdown","b098e17c":"markdown","6b143c05":"markdown"},"source":{"5a34c651":"!pip install ftfy","b0eb3a7d":"import random\nimport numpy as np\nimport pandas as pd\nimport copy \nimport time\nfrom scipy.stats import uniform\nimport matplotlib.pyplot as plt\nimport requests\nimport io\n\nfrom ftfy import fix_text\nimport string\nimport re\nfrom gensim.test.utils import common_texts\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nprint(\"Loaded libs\")","87daa6f6":"data=pd.read_csv('..\/input\/sentiment-analysis-pmr3508\/data_train.csv', sep=',')","57201ba7":"data.head()","430b51d4":"data=data.drop_duplicates(subset='review', keep='first')","49771b25":"X_train = data.loc[:,'review'].tolist()\ny_train = np.array(data.loc[:,'positive'].tolist())","aaf14ac9":"def clean(text):\n    txt=text.replace(\"<br \/>\",\" \") \n    txt=fix_text(txt)\n    txt=txt.lower()\n    txt=txt.translate(str.maketrans('', '', string.punctuation)) \n    txt=txt.replace(\" \u2014 \", \" \") \n    txt=re.sub(\"\\d+\", ' <number> ', txt)\n    txt=re.sub(' +', ' ', txt)\n    return txt","9893b3d9":"X_train = [clean(x) for x in X_train]\nX_train = [x.split() for x in X_train]","af2ec8e8":"d2v = Doc2Vec.load('..\/input\/sentiment-analysis-pmr3508\/doc2vec')  ","e75cea3c":"def emb(txt, model, normalize=False): \n    model.random.seed(42)\n    x=model.infer_vector(txt, steps=20)\n    if normalize: return(x\/np.sqrt(x@x))\n    else: return(x)","5e073736":"X_train = [emb(x, d2v) for x in X_train] \nX_train = np.array(X_train)","2f6b2961":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score","5d2e9860":"X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2, random_state=42)","58217d51":"logreg = LogisticRegression(solver='liblinear',random_state=42)\nhyperparams = dict(C=np.linspace(0,10,100), \n                     penalty=['l2', 'l1'])\nclf = RandomizedSearchCV(logreg, hyperparams, scoring='roc_auc', n_iter=50, cv=2, n_jobs=-1, random_state=0, verbose=2)\nsearch_logreg = clf.fit(X_train_split, y_train_split)\n\nsearch_logreg.best_params_, search_logreg.best_score_","f303d123":"logreg = LogisticRegression(C=search_logreg.best_params_['C'], \n                            penalty=search_logreg.best_params_['penalty'],\n                            solver='liblinear', random_state=42)\nlogreg.fit(X_train_split, y_train_split)\nprint(\"Finished logreg\")","c46e0a4c":"print('AUC --- Log. Reg.: {:.4f}'.format(roc_auc_score(y_test_split, logreg.predict_proba(X_test_split)[:,1])))","c4eca3b9":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV","fe15efee":"mlp = MLPClassifier(early_stopping=True)\n\nsize= [x for x in range(10, 101, 10)]\nalpha = [0.001,0.01,0.1,1]\nparams= {'hidden_layer_sizes': size, 'alpha':alpha}\nneural1 = GridSearchCV(mlp, params, n_jobs=-1, verbose=1, scoring=\"roc_auc\")\nneural1.fit(X_train_split, y_train_split)","4aae830e":"neural1.best_params_, neural1.best_score_","034569c4":"alpha = neural1.best_params_[\"alpha\"]\nsize = neural1.best_params_[\"hidden_layer_sizes\"]\n\nskl1 = MLPClassifier(early_stopping=True, alpha=alpha, hidden_layer_sizes=size)\nskl1.fit(X_train_split, y_train_split)\nprint('AUC --- SciKit NN 1 Layer: {:.4f}'.format(roc_auc_score(y_test_split, skl1.predict_proba(X_test_split)[:,1])))\nprint(\"Finished skl1\")","59527af7":"size = [(x,y) for x in range(10, 101, 10) for y in range(10, x, 10)]\nalpha=[0.001,0.01,0.1]\nmlp = MLPClassifier(early_stopping=True)\nparameters= {'hidden_layer_sizes': size, 'alpha':alpha}\n\nneural2 = GridSearchCV(mlp, parameters, n_jobs=-1, verbose=1, scoring=\"roc_auc\")\nneural2.fit(X_train_split, y_train_split)","916fced2":"neural2.best_params_, neural2.best_score_","948bda5e":"alpha = neural2.best_params_[\"alpha\"]\nsize = neural2.best_params_[\"hidden_layer_sizes\"]\n\nskl2 = MLPClassifier(early_stopping=True, alpha=alpha, hidden_layer_sizes=size)\nskl2.fit(X_train_split, y_train_split)\nprint('AUC --- SciKit NN 2 Layers: {:.4f}'.format(roc_auc_score(y_test_split, skl2.predict_proba(X_test_split)[:,1])))\nprint(\"Finished skl2\")","6b10dcdf":"import tensorflow as tf\nfrom tensorflow.keras import Sequential, regularizers\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.python.framework import ops\nfrom tensorflow.keras.callbacks import EarlyStopping","4f56f749":"n_features = X_train_split.shape[1]\nn_iter=200\n\nneurons=[]\nreg=[]\n\n#Sorteando valores\nfor i in range(n_iter):\n    h1=random.randrange(25, 100, 5)\n    h2=random.randrange(20, h1, 5)\n    neurons.append((h1, h2))\n    l1=random.choice([0, 1e-15, 1e-10, 1e-5, 1e-3, 1e-2, 1e-1])\n    l2=random.choice([0, 1e-15, 1e-10, 1e-5, 1e-3, 1e-2, 1e-1])\n    reg.append((l1, l2))\n    \n#DataFrame\nhyper = {'neurons': neurons, 'reg': reg, 'epochs': n_iter*[None], 'auc': n_iter*[None]}\nhyper = pd.DataFrame(hyper)\nhyper = hyper[['neurons', 'reg', 'epochs', 'auc']]","4a233b5f":"def create_model(neurons=(10,10), reg=(.001, .001)):\n    ops.reset_default_graph() \n    model = Sequential()\n    model.add(Dense(neurons[0], input_shape=(n_features,), activation='relu', \n                    kernel_regularizer=regularizers.l1_l2(l1=reg[0], l2=reg[1]), \n                    bias_regularizer=regularizers.l1_l2(l1=reg[0], l2=reg[1])))\n    model.add(Dense(neurons[1], activation='relu', \n                    kernel_regularizer=regularizers.l1_l2(l1=reg[0], l2=reg[1]), \n                    bias_regularizer=regularizers.l1_l2(l1=reg[0], l2=reg[1])))\n    model.add(Dense(1, activation='sigmoid', \n                    kernel_regularizer=regularizers.l1_l2(l1=reg[0], l2=reg[1]), \n                    bias_regularizer=regularizers.l1_l2(l1=reg[0], l2=reg[1])))\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n    return model","91e86dfc":"for iter in range(n_iter):\n    if iter%10==0:\n        print(iter\/10\/20*100,\"%\")\n    model = create_model(hyper.loc[iter,'neurons'], hyper.loc[iter,'reg'])\n    \n    es = EarlyStopping(monitor='val_auc', patience=10)\n    \n    history = model.fit(X_train_split, y_train_split,\n                        epochs=50,\n                        validation_data=(X_test_split,y_test_split), \n                        batch_size=122, \n                        shuffle=True, \n                        verbose=False,\n                        callbacks=[es]) \n    \n    hyper.loc[iter,'epochs'] = len(history.history['val_auc'])\n    hyper.loc[iter,'auc'] = history.history['val_auc'][-1]","cf4a2825":"hyper = hyper.iloc[np.argsort(hyper.loc[:,'auc']),:]","5e318c01":"neurons = hyper.iloc[-1,0]\nreg = hyper.iloc[-1,1]\nepochs = hyper.iloc[-1,2]\ntf1 = create_model(neurons, reg)\ntf1.fit(X_train_split, y_train_split, \n         validation_split=0,\n         epochs=epochs, \n         batch_size=122, \n         shuffle=True, \n         verbose=False) \n","99e8e971":"print('AUC --- TensorFlow NN: {:.4f}'.format(roc_auc_score(y_test_split, tf1.predict(X_test_split))))\nprint(\"Finished tf1\")","a3c43ea4":"data_test=pd.read_csv('..\/input\/sentiment-analysis-pmr3508\/data_test1.csv', sep=',')\nX_test2 = data_test.loc[:,'review'].tolist()\ny_test2 = np.array(data_test.loc[:,'positive'].tolist())","676366cf":"X_test2 = [clean(x) for x in X_test2]\nX_test2 = [x.split() for x in X_test2]","2b7e1f8f":"X_test2 = [emb(x, d2v) for x in X_test2] \nX_test2 = np.array(X_test2)","e4f96bf2":"score1 = roc_auc_score(y_test2, logreg.predict_proba(X_test2)[:,1])\nscore2 = roc_auc_score(y_test2, skl1.predict_proba(X_test2)[:,1])\nscore3 = roc_auc_score(y_test2, skl2.predict_proba(X_test2)[:,1])\nscore4 = roc_auc_score(y_test2, tf1.predict(X_test2))\nbest = max([score1,score2,score3,score4])\nbest","c5031d28":"data_predict=pd.read_csv('..\/input\/sentiment-analysis-pmr3508\/data_test2_X.csv', sep=',')\nX_data_predict = data_predict.loc[:,'review'].tolist()\nX_data_predict = [clean(x) for x in X_data_predict]\nX_data_predict = [x.split() for x in X_data_predict]\nX_data_predict = [emb(x, d2v) for x in X_data_predict] \nX_data_predict = np.array(X_data_predict)","533b9a21":"y_pred = tf1.predict(X_data_predict).squeeze()\noutput = pd.DataFrame({'positive': y_pred})\noutput.to_csv(\"submission.csv\", index = True, index_label = 'Id')","841d7f93":"if best == score1:\n    print(\"Best was logreg\")\n    y_pred = logreg.predict_proba(X_data_predict)[:,1]\nelif best == score2:\n    print(\"Best was skl1\")\n    y_pred = skl1.predict_proba(X_data_predict)[:,1]\nelif best == score3:\n    print(\"Best was skl2\")\n    y_pred = skl2.predict_proba(X_data_predict)[:,1]\nelse:\n    print(\"Best was tf1\")\n    y_pred = tf1.predict(X_data_predict).squeeze()\n    \noutput = pd.DataFrame({'positive': y_pred})\noutput.to_csv(\"submission.csv\", index = True, index_label = 'Id')\noutput","c0d342d2":"Converting splitted words into values","e555df31":"Let's measure the area under the receiver operating characteristic curve (ROC AUC) from prediction scores.","71fe3c66":"Finally, let's use tensorflow to create a third model to predict our data. In that case, we will use the tuned model. We will tune two hyperparameters, L1 and L2, chosen between random numbers, and test with different combinations. We will also use early stopping, so if our model does not get better after ten iterations, we will stop. Finally, we will also try with different numbers of neurons. So, we have four hyper parameters. Lets see our results.","b02f7388":"Reading data from training file","b9938b10":"Now we will make another neural network, but this time with two hidden layers. The second layer will always be a little smaller than the first, with values varying from 10 to 100","14baf546":"Splitting our data with test and train. 80% will be training data, the rest is for testing","bd8042c0":"Finally, let's use the best model to generate our final submission","da59501b":"Now let's use sklearn to make a neural network with one hidden layer. We will try different combinations of layer sizes(or number of neuron) increasing by 10 from 10 to 100, and different regularizations, by testing with different alpha values. After that, we will make a random test to check which one performs the best ","ac52aa31":"Importing libraries to handle numbers, text, sklearn and tensorflow for ML ","0f11d547":"In the next blocks we will sanitize our data, so we don't have  uppercases, pontuation, html tags, et cetera. Alse, we will 'tokenize' the data, splitting each phase into a array of words.","4d2b946f":"Let's select the best model and train our neural network","2efeebe3":"Splitting out data in two lists, one for the comments and one for the label ","7a28bbc1":"Let's remove duplicated values, and keep the first one","8afb0f85":"Let's see our AUC score with the best parameters","225d3270":"Let's compare different scores to get which one performs the best","b098e17c":"Lets load doc2vec. It will help us to convert our words into values. Vectors, in that case.","6b143c05":"Now let's make a first guess using logistic regression, using randomized search to define hyperparameters in a random way. It will test which one has the best results"}}