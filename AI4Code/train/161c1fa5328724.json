{"cell_type":{"9db9e801":"code","e4e073d6":"code","5c0c0837":"code","eb4267a4":"code","da6736ff":"code","8a9ef279":"code","3549cb56":"code","7128acd0":"code","904432eb":"code","df3dae5d":"code","a3945344":"code","28bc281a":"code","6914d1c9":"code","a736c081":"code","4d92488f":"code","c545cf15":"code","0135bef7":"code","cd1ffe1b":"code","39922ac2":"code","1217a1a1":"code","42d15cda":"code","e95ea0d4":"code","f7bd0b8a":"code","7f0cfbd7":"code","097932d1":"code","89f74561":"code","866b5273":"code","7fdd9d17":"code","2c5ef7ce":"code","a5fc139c":"code","cb1f20ac":"code","20d07789":"code","ef63ed8f":"code","401fa4ab":"code","fc0d5d5e":"code","b876a544":"code","ee7f2511":"code","2614c8c9":"code","bf7ae118":"code","ca87adfb":"code","b99afe11":"code","f8c70ed8":"code","2effe08b":"code","c0abd4e3":"code","4ed09d8a":"code","c60cc9ed":"code","b26585de":"code","ceec210f":"code","0383c769":"code","cf3541cb":"code","48eef6c6":"code","560afb31":"code","4987a4f9":"code","1fa83226":"code","18d6d9a0":"code","10b19c74":"code","abfd295f":"code","9cad2ee1":"code","5a65a200":"code","12f9f6c7":"code","c150b4d2":"code","2c4edb10":"code","3e8350ef":"code","760c9d40":"code","42f35742":"code","0819a6da":"code","134b2e52":"code","64dc01f2":"code","acaad77c":"code","7bc6904a":"code","ea0334c3":"code","3cf20eb2":"code","2d377af3":"code","3981f672":"code","dcf24e77":"code","7d3b784c":"code","04961583":"code","210459a7":"code","54c1937f":"code","17be80fa":"code","aca83193":"code","44d29e83":"code","7fd97519":"code","336b492d":"code","46bb0cf1":"code","8407c7b4":"code","3345be71":"code","526af977":"code","58d68dac":"code","304f0dcb":"code","f4588845":"code","05f60d84":"code","4b4f21ae":"code","a0320e80":"code","246a1c91":"code","93501ccc":"code","24dd7fb8":"code","5982011e":"code","2ce6aeb0":"code","90d5639d":"code","0ff09453":"code","6d75fcea":"code","ff80715d":"code","0328ca01":"code","128abbe4":"code","ef062de1":"code","528e84c6":"code","f3dbbb88":"markdown","e9f4f82f":"markdown","603f5b16":"markdown","fdb73669":"markdown","a66fbf68":"markdown","092fedeb":"markdown","349e97e0":"markdown","e1cfc52b":"markdown","5cf87613":"markdown","58216e79":"markdown","d3407cc7":"markdown","b4d94b9f":"markdown","a0814c85":"markdown","0e18f6a9":"markdown","4e790738":"markdown","4242ff0e":"markdown","914e36f8":"markdown","a9845ff8":"markdown","06dc0613":"markdown","fea79951":"markdown","9b82da8c":"markdown","ee081448":"markdown","a8791f96":"markdown","c66d0caf":"markdown","f7152cd3":"markdown","02b83d52":"markdown","135eec24":"markdown","e2886cb9":"markdown","8f3ee3f2":"markdown","1eb9ba69":"markdown","ec400c5b":"markdown","8b09b94b":"markdown","535f041c":"markdown","616adcb5":"markdown","86aceebf":"markdown","15f4490d":"markdown","06b3f960":"markdown","b93c981c":"markdown","708a11ae":"markdown","ae2e77ed":"markdown","e63838b5":"markdown","4a3d7b8b":"markdown","5227874e":"markdown","c5004f24":"markdown","d35c0be3":"markdown","d34cb98f":"markdown","012d7506":"markdown","5876a737":"markdown","4bcdf49d":"markdown","f78f848b":"markdown","1335403c":"markdown","480a8d34":"markdown","ca358054":"markdown","16fb4a16":"markdown","fb1ef714":"markdown","7489a7aa":"markdown","7e5640be":"markdown","060543f0":"markdown","573b251a":"markdown","5d123546":"markdown"},"source":{"9db9e801":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nplt.rc('font',size=14)\nsns.set(style='white')\nsns.set(style='darkgrid',color_codes=True)\nfrom scipy.stats import zscore\nfrom sklearn.model_selection import train_test_split, cross_val_score\n%matplotlib inline","e4e073d6":"data = pd.read_csv(\"..\/input\/concrete.csv\")","5c0c0837":"data.head()","eb4267a4":"data.head()","da6736ff":"data.info()","8a9ef279":"data.dtypes","3549cb56":"data.shape","7128acd0":"# column names\ndata.columns.tolist()","904432eb":"# dataset distribution\ndata.describe().T","df3dae5d":"data.isna().sum()","a3945344":"features = [col for col in data.columns.tolist() if col not in ['strength']]","28bc281a":"def univariate_analysis(data):\n    for col in features:\n        print(\"*\"*50)\n        print(\"Column name: \", col)\n        print('Range of values: ', data[col].max() - data[col].min())\n        print(\"<- Pivote values -> \")\n        print('Minimum value: ', data[col].min())\n        print('Maximum value: ',data[col].max())\n        print('Mean value: ', data[col].mean())\n        print('Median value: ',data[col].median())\n        print('Standard deviation: ', data[col].std())\n        print('Null values: ',data[col].isnull().any())\n        print(\"<- Outlier Detection -> \")\n        Q1=data[col].quantile(q=0.25)\n        Q3=data[col].quantile(q=0.75)\n        print('1st Quartile (Q1) is: ', Q1)\n        print('3st Quartile (Q3) is: ', Q3)\n        print('Interquartile range (IQR) is ', stats.iqr(data[col]))\n        L_outliers=Q1-1.5*(Q3-Q1)\n        U_outliers=Q3+1.5*(Q3-Q1)\n        print(f'Lower outliers in {col}: {L_outliers}')\n        print(f'Upper outliers in {col}: {U_outliers}' )\n        print(f'Number of outliers in {col} upper : ', data[data[col] > U_outliers][col].count())\n        print(f'Number of outliers in {col} lower : ', data[data[col]<L_outliers][col].count())\n        print(f'% of Outlier in {col} upper: {round(data[data[col] > U_outliers][col].count()*100\/len(data))} %')\n        print(f'% of Outlier in {col} lower: {round(data[data[col]<L_outliers][col].count()*100\/len(data))} %')\n        fig, (ax1,ax2,ax3)=plt.subplots(1,3,figsize=(13,5))\n\n        #boxplot\n        sns.boxplot(x=col,data=data,orient='v',ax=ax1)\n        ax1.set_ylabel(col, fontsize=15)\n        ax1.set_title(f'Distribution of {col}', fontsize=15)\n        ax1.tick_params(labelsize=15)\n\n        #distplot\n        sns.distplot(data[col],ax=ax2)\n        ax2.set_xlabel(col, fontsize=15)\n        ax2.set_ylabel(col, fontsize=15)\n        ax2.set_title(f'{col} vs Strength', fontsize=15)\n        ax2.tick_params(labelsize=15)\n\n        #histogram\n        ax3.hist(data[col])\n        ax3.set_xlabel(col, fontsize=15)\n        ax3.set_ylabel(col, fontsize=15)\n        ax3.set_title(f'{col} vs Strength', fontsize=15)\n        ax3.tick_params(labelsize=15)\n\n        plt.subplots_adjust(wspace=0.5)\n        plt.tight_layout() \n        plt.show()\n        print(\"#\"*50)\nunivariate_analysis(data)","6914d1c9":"sns.pairplot(data)\nplt.show()","a736c081":"fig, ax2 = plt.subplots(3, 3, figsize=(16, 16))\nsns.distplot(data['cement'],ax=ax2[0][0])\nsns.distplot(data['slag'],ax=ax2[0][1])\nsns.distplot(data['ash'],ax=ax2[0][2])\nsns.distplot(data['water'],ax=ax2[1][0])\nsns.distplot(data['superplastic'],ax=ax2[1][1])\nsns.distplot(data['coarseagg'],ax=ax2[1][2])\nsns.distplot(data['fineagg'],ax=ax2[2][0])\nsns.distplot(data['age'],ax=ax2[2][1])\nsns.distplot(data['strength'],ax=ax2[2][2])","4d92488f":"corr = data.corr()\n\nplt.figure(figsize=(14,10))\nsns.heatmap(corr, annot=True, cmap='Blues')\nb, t = plt.ylim()\nplt.ylim(b+0.5, t-0.5)\nplt.title(\"Feature Correlation Heatmap\")\nplt.show()","c545cf15":"plt.figure(figsize=(14, 10))\nax = sns.distplot(data.strength)\nax.set_title(\"Compressive Strength Distribution\")","0135bef7":"fig, ax = plt.subplots(figsize=(14,10))\nsns.scatterplot(y=\"strength\", x=\"cement\", hue=\"water\", size=\"age\", data=data, ax=ax, sizes=(50, 300))\nax.set_title(\"Strength vs (Cement, Age, Water)\")\nax.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\nplt.show()","cd1ffe1b":"data.columns","39922ac2":"fig, ax = plt.subplots(figsize=(14,10))\nsns.scatterplot(y=\"strength\", x=\"fineagg\", hue=\"ash\", size=\"superplastic\", \n                data=data, ax=ax, sizes=(50, 300))\nax.set_title(\"Strength vs (Fine aggregate, Super Plasticizer, FlyAsh)\")\nax.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\nplt.show()","1217a1a1":"fig, ax = plt.subplots(figsize=(14,10))\nsns.scatterplot(y=\"strength\", x=\"fineagg\", hue=\"water\", size=\"superplastic\", \n                data=data, ax=ax, sizes=(50, 300))\nax.set_title(\"Strength vs (Fine aggregate, Super Plasticizer, Water)\")\nax.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\nplt.show()","42d15cda":"concrete_df1 = data.copy()\nconcrete_df1.boxplot(figsize=(35,15))","e95ea0d4":"print('Number of outliers in cement: ',concrete_df1[((concrete_df1.cement - concrete_df1.cement.mean()) \/ concrete_df1.cement.std()).abs() >3]['cement'].count())\nprint('Number of outliers in slag: ',concrete_df1[((concrete_df1.slag - concrete_df1.slag.mean()) \/ concrete_df1.slag.std()).abs() >3]['slag'].count())\nprint('Number of outliers in ash: ',concrete_df1[((concrete_df1.ash - concrete_df1.ash.mean()) \/ concrete_df1.ash.std()).abs() >3]['ash'].count())\nprint('Number of outliers in water: ',concrete_df1[((concrete_df1.water - concrete_df1.water.mean()) \/ concrete_df1.water.std()).abs() >3]['water'].count())\nprint('Number of outliers in superplastic: ',concrete_df1[((concrete_df1.superplastic - concrete_df1.superplastic.mean()) \/ concrete_df1.superplastic.std()).abs() >3]['superplastic'].count())\nprint('Number of outliers in coarseagg: ',concrete_df1[((concrete_df1.coarseagg - concrete_df1.coarseagg.mean()) \/ concrete_df1.coarseagg.std()).abs() >3]['coarseagg'].count())\nprint('Number of outliers in fineagg: ',concrete_df1[((concrete_df1.fineagg - concrete_df1.fineagg.mean()) \/ concrete_df1.fineagg.std()).abs() >3]['fineagg'].count())\nprint('Number of outliers in age: ',concrete_df1[((concrete_df1.age - concrete_df1.age.mean()) \/ concrete_df1.age.std()).abs() >3]['age'].count())","f7bd0b8a":"print('Records containing outliers in slag: \\n',concrete_df1[((concrete_df1.slag - concrete_df1.slag.mean()) \/ concrete_df1.slag.std()).abs() >3]['slag'])","7f0cfbd7":"print('Records containing outliers in water: \\n',concrete_df1[((concrete_df1.water - concrete_df1.water.mean()) \/ concrete_df1.water.std()).abs() >3]['water'])","097932d1":"print('Records containing outliers in superplastic: \\n',concrete_df1[((concrete_df1.superplastic - concrete_df1.superplastic.mean()) \/ concrete_df1.superplastic.std()).abs() >3]['superplastic'])","89f74561":"print('Records containing outliers in age: \\n',concrete_df1[((concrete_df1.age - concrete_df1.age.mean()) \/ concrete_df1.age.std()).abs() >3]['age'])","866b5273":"for col_name in concrete_df1.columns[:-1]:\n    q1 = concrete_df1[col_name].quantile(0.25)\n    q3 = concrete_df1[col_name].quantile(0.75)\n    iqr = q3 - q1\n    \n    low = q1-1.5*iqr\n    high = q3+1.5*iqr\n    concrete_df1.loc[(concrete_df1[col_name] < low) | (concrete_df1[col_name] > high), col_name] = concrete_df1[col_name].median()","7fdd9d17":"concrete_df1.boxplot(figsize=(35,15))","2c5ef7ce":"concrete_df_z = concrete_df1.apply(zscore)\nconcrete_df_z = pd.DataFrame(concrete_df_z,columns=data.columns) ","a5fc139c":"X = concrete_df_z.iloc[:,:-1]         # Features - All columns but last\ny = concrete_df_z.iloc[:,-1]          # Target - Last Column","cb1f20ac":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 123)","20d07789":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\n\ndt_model = DecisionTreeRegressor()\ndt_model.fit(X_train , y_train)\n\n#printing the feature importance\nprint('Feature importances: \\n',pd.DataFrame(dt_model.feature_importances_,columns=['Imp'],index=X_train.columns).sort_values('Imp', ascending=False))","ef63ed8f":"y_pred = dt_model.predict(X_test)\n# performance on train data\nprint('Performance on training data using Decision Tree:',dt_model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using Decision Tree:',dt_model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_DT=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_DT)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","401fa4ab":"results = pd.DataFrame({'Method':['Decision Tree'], 'accuracy': acc_DT},index={'1'})\nresults = results[['Method', 'accuracy']]\nresults","fc0d5d5e":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(dt_model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","b876a544":"tempResultsDf = pd.DataFrame({'Method':['Decision Tree k fold'], 'accuracy': [accuracy]},index={'2'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","ee7f2511":"concrete_df2=concrete_df_z.copy()","2614c8c9":"#independent and dependent variable\nX = concrete_df2.drop( ['strength','ash','coarseagg','fineagg'] , axis=1)\ny = concrete_df2['strength']\n# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 123)","bf7ae118":"dt_model = DecisionTreeRegressor()\ndt_model.fit(X_train , y_train)","ca87adfb":"print('Feature importances: \\n',pd.DataFrame(dt_model.feature_importances_,columns=['Imp'], index=X_train.columns).sort_values('Imp', ascending=False))","b99afe11":"y_pred = dt_model.predict(X_test)\n# performance on train data\nprint('Performance on training data using DT:',dt_model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using DT:',dt_model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_DT=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_DT)","f8c70ed8":"tempResultsDf = pd.DataFrame({'Method':['Decision Tree2'], 'accuracy': [acc_DT]},index={'3'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults\n","2effe08b":"X=concrete_df_z.iloc[:,0:8]\ny = concrete_df_z.iloc[:,8]\n# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 123)","c0abd4e3":"reg_dt_model = DecisionTreeRegressor( max_depth = 4,random_state=1,min_samples_leaf=5)\nreg_dt_model.fit(X_train, y_train)","4ed09d8a":"print('Feature importances: \\n',pd.DataFrame(reg_dt_model.feature_importances_,columns=['Imp'], index=X_train.columns).sort_values('Imp', ascending=False))","c60cc9ed":"!pip install pydotplus --quiet","b26585de":"from sklearn.tree import export_graphviz\nfrom io import StringIO  \nfrom IPython.display import Image  \nimport pydotplus\nimport graphviz\nbank_df=concrete_df_z\nxvar = bank_df.drop('strength', axis=1)\nfeature_cols = xvar.columns","ceec210f":"dot_data = StringIO()\nexport_graphviz(reg_dt_model, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('concrete_pruned.png')\nImage(graph.create_png())","0383c769":"y_pred = reg_dt_model.predict(X_test)\n# performance on train data\nprint('Performance on training data using DT:',reg_dt_model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using DT:',reg_dt_model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_RDT=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_RDT)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","cf3541cb":"tempResultsDf = pd.DataFrame({'Method':['Pruned Decision Tree'], 'accuracy': [acc_RDT]},index={'4'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","48eef6c6":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(reg_dt_model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","560afb31":"tempResultsDf = pd.DataFrame({'Method':['Pruned Decision Tree k fold'], 'accuracy': [accuracy]},index={'5'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","4987a4f9":"concrete_df3=concrete_df_z.copy()","1fa83226":"X = concrete_df3.drop( ['strength','ash','coarseagg','fineagg'] , axis=1)\ny = concrete_df3['strength']\n# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)","18d6d9a0":"reg_dt_model = DecisionTreeRegressor( max_depth = 4,random_state=1,min_samples_leaf=5)\nreg_dt_model.fit(X_train, y_train)","10b19c74":"y_pred = reg_dt_model.predict(X_test)\n# performance on train data\nprint('Performance on training data using DT:',reg_dt_model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using DT:',reg_dt_model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_RDT=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_RDT)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","abfd295f":"tempResultsDf = pd.DataFrame({'Method':['Pruned Decision Tree2'], 'accuracy': [acc_RDT]},index={'6'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","9cad2ee1":"from sklearn.cluster import KMeans","5a65a200":"cluster_range = range( 1, 15 )  \ncluster_errors = []\nfor num_clusters in cluster_range:\n  clusters = KMeans( num_clusters, n_init = 5)\n  clusters.fit(concrete_df1)\n  labels = clusters.labels_\n  centroids = clusters.cluster_centers_\n  cluster_errors.append( clusters.inertia_ )\nclusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\nclusters_df[0:15]","12f9f6c7":"plt.figure(figsize=(12,6))\nplt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )","c150b4d2":"cluster = KMeans( n_clusters = 6, random_state = 2354 )\ncluster.fit(concrete_df_z)","2c4edb10":"prediction=cluster.predict(concrete_df_z)\nconcrete_df_z[\"GROUP\"] = prediction     \n# Creating a mirror copy for later re-use instead of building repeatedly\nconcrete_df_z_copy = concrete_df_z.copy(deep = True)  ","3e8350ef":"centroids = cluster.cluster_centers_\ncentroids","760c9d40":"centroid_df = pd.DataFrame(centroids, columns = list(concrete_df1) )\ncentroid_df","42f35742":"# plot centroids and the data in the cluster into box plots\nconcrete_df_z.boxplot(by = 'GROUP',  layout=(3,3), figsize=(15, 10))","0819a6da":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor","134b2e52":"X=concrete_df_z.iloc[:,0:8]\ny = concrete_df_z.iloc[:,8]\n# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)","64dc01f2":"model=RandomForestRegressor()\nmodel.fit(X_train, y_train)","acaad77c":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using RFR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using RFR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_RFR=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_RFR)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","7bc6904a":"tempResultsDf = pd.DataFrame({'Method':['Random Forest Regressor'], 'accuracy': [acc_RFR]},index={'7'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","ea0334c3":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","3cf20eb2":"tempResultsDf = pd.DataFrame({'Method':['Random Forest Regressor k fold'], 'accuracy': [accuracy]},index={'8'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","2d377af3":"model=GradientBoostingRegressor()\nmodel.fit(X_train, y_train)","3981f672":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using GBR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using GBR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_GBR=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_GBR)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","dcf24e77":"tempResultsDf = pd.DataFrame({'Method':['Gradient Boost Regressor'], 'accuracy': [acc_GBR]},index={'9'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","7d3b784c":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","04961583":"tempResultsDf = pd.DataFrame({'Method':['Gradient Boost Regressor k fold'], 'accuracy': [accuracy]},index={'10'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","210459a7":"model=BaggingRegressor()\nmodel.fit(X_train, y_train)","54c1937f":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using GBR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using GBR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_BR=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_BR)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","17be80fa":"tempResultsDf = pd.DataFrame({'Method':['Bagging Regressor'], 'accuracy': [acc_BR]},index={'13'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","aca83193":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","44d29e83":"tempResultsDf = pd.DataFrame({'Method':['Bagging Regressor k fold'], 'accuracy': [accuracy]},index={'14'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","7fd97519":"from sklearn.neighbors import KNeighborsRegressor","336b492d":"error=[]\nfor i in range(1,30):\n    knn = KNeighborsRegressor(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error.append(np.mean(pred_i!=y_test))","46bb0cf1":"plt.figure(figsize=(12,6))\nplt.plot(range(1,30),error,color='red', linestyle='dashed',marker='o',markerfacecolor='blue',markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean error')","8407c7b4":"model = KNeighborsRegressor(n_neighbors=3)\nmodel.fit(X_train, y_train)","3345be71":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using KNNR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using KNNR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_K=metrics.r2_score(y_test, y_pred)\nprint('Accuracy KNNR: ',acc_K)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","526af977":"tempResultsDf = pd.DataFrame({'Method':['KNN Regressor'], 'accuracy': [acc_K]},index={'15'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","58d68dac":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","304f0dcb":"tempResultsDf = pd.DataFrame({'Method':['KNN Regressor k fold'], 'accuracy': [accuracy]},index={'16'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","f4588845":"from sklearn.svm import SVR","05f60d84":"model = SVR(kernel='linear')\nmodel.fit(X_train, y_train)","4b4f21ae":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using SVR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using SVR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_S=metrics.r2_score(y_test, y_pred)\nprint('Accuracy SVR: ',acc_S)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","a0320e80":"tempResultsDf = pd.DataFrame({'Method':['Support Vector Regressor'], 'accuracy': [acc_S]},index={'17'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","246a1c91":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","93501ccc":"tempResultsDf = pd.DataFrame({'Method':['SVR k fold'], 'accuracy': [accuracy]},index={'18'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","24dd7fb8":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import VotingRegressor\n\nLR=LinearRegression()\nKN=KNeighborsRegressor(n_neighbors=3)\nSVM=SVR(kernel='linear') ","5982011e":"evc=VotingRegressor(estimators=[('LR',LR),('KN',KN),('SVM',SVM)])\nevc.fit(X_train, y_train)","2ce6aeb0":"y_pred = evc.predict(X_test)\n# performance on train data\nprint('Performance on training data using ensemble:',evc.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using ensemble:',evc.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_E=metrics.r2_score(y_test, y_pred)\nprint('Accuracy ensemble: ',acc_E)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","90d5639d":"tempResultsDf = pd.DataFrame({'Method':['Ensemble'], 'accuracy': [acc_E]},index={'19'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","0ff09453":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(evc,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","6d75fcea":"tempResultsDf = pd.DataFrame({'Method':['Ensemble k fold'], 'accuracy': [accuracy]},index={'20'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","ff80715d":"concrete_XY = X.join(y)","0328ca01":"from sklearn.utils import resample\nvalues = concrete_XY.values\n# Number of bootstrap samples to create\nn_iterations = 1000        \n# size of a bootstrap sample\nn_size = int(len(concrete_df_z) * 1)    \n\n# run bootstrap\n# empty list that will hold the scores for each bootstrap iteration\nstats = list()   \nfor i in range(n_iterations):\n    # prepare train and test sets\n    train = resample(values, n_samples=n_size)  # Sampling with replacement \n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n    \n    \n     # fit model\n    gbmTree = GradientBoostingRegressor(n_estimators=50)\n    # fit against independent variables and corresponding target values\n    gbmTree.fit(train[:,:-1], train[:,-1]) \n    # Take the target column for all rows in test set\n\n    y_test = test[:,-1]    \n    # evaluate model\n    # predict based on independent variables in the test data\n    score = gbmTree.score(test[:, :-1] , y_test)\n    predictions = gbmTree.predict(test[:, :-1])  \n\n    stats.append(score)","128abbe4":"from matplotlib import pyplot\npyplot.hist(stats)\npyplot.show()\n# confidence intervals\nalpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)\/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(stats, p))  \np = (alpha+((1.0-alpha)\/2.0)) * 100\nupper = min(1.0, np.percentile(stats, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","ef062de1":"values = concrete_XY.values\n# Number of bootstrap samples to create\nn_iterations = 1000        \n# size of a bootstrap sample\nn_size = int(len(concrete_df_z) * 1)    \n\n# run bootstrap\n# empty list that will hold the scores for each bootstrap iteration\nstats = list()   \nfor i in range(n_iterations):\n    # prepare train and test sets\n    train = resample(values, n_samples=n_size)  # Sampling with replacement \n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n    \n    \n     # fit model\n    rfTree = RandomForestRegressor(n_estimators=100)\n    # fit against independent variables and corresponding target values\n    rfTree.fit(train[:,:-1], train[:,-1]) \n    # Take the target column for all rows in test set\n\n    y_test = test[:,-1]    \n    # evaluate model\n    # predict based on independent variables in the test data\n    score = rfTree.score(test[:, :-1] , y_test)\n    predictions = rfTree.predict(test[:, :-1])  \n\n    stats.append(score)","528e84c6":"from matplotlib import pyplot\npyplot.hist(stats)\npyplot.show()\n# confidence intervals\nalpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)\/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(stats, p))  \np = (alpha+((1.0-alpha)\/2.0)) * 100\nupper = min(1.0, np.percentile(stats, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","f3dbbb88":"### K fold cross validation","e9f4f82f":"* The acuracy on testing dataset is not improved, still it is an overfit model.","603f5b16":"##### Observations from CC Strength vs (Fine aggregate, Super Plasticizer, Water)\n* **Strength decreases** with **increase in water**, **strength increases** with **increase in Super plasticizer** (already from above plots)\n* **More Fine aggregate** is used when **less water**, **more Super plasticizer** is used.\n","fdb73669":"### Feature Engineering","a66fbf68":"### 1.a Univariate analysis","092fedeb":"## Bagging regressor  ","349e97e0":"### K-Mean Clustering","e1cfc52b":"### K Fold cross validation","5cf87613":"## Concrete Compressive Strength Prediction\n\n\nConcrete is one of the most important materials in Civil Engineering. Knowing the compressive strength of concrete is very important when constructing a building or a bridge. The Compressive Strength of Concrete is a highly nonlinear function of ingredients used in making it and their characteristics. Thus, using Machine Learning to predict the Strength could be useful in generating a combination of ingredients which result in high Strength.\n\n\n### Problem Statement\nPredicting Compressive Strength of Concrete given its age and quantitative measurements of ingredients.\n\n### Data Description\n\n* Number of instances - 1030\n* Number of Attributes - 9\n  * Attribute breakdown - 8 quantitative inputs, 1 quantitative output\n\n#### Attribute information\n##### Inputs\n* Cement\n* Blast Furnace Slag\n* Fly Ash\n* Water\n* Superplasticizer\n* Coarse Aggregate\n* Fine Aggregate\n\nAll above features measured in kg\/$m^3$\n\n* Age (in days)\n\n##### Output\n* Concrete Compressive Strength (Mpa)\n\n\n","58216e79":"#### Observations from Strength vs (Cement, Age, Water)\n* Compressive **strength increases with amount of cement**\n* Compressive **strength increases with age**\n* Cement with **low age** requires **more cement** for **higher strength**\n* The **older the cement** is the **more water** it requires\n* Concrete **strength increases** when **less water** is used in preparing it  ","d3407cc7":"##### Observations from CC Strength vs (Fine aggregate, Super Plasticizer, FlyAsh)\n* As **Flyash increases** the **strength decreases**\n* **Strength increases** with **Super plasticizer**","b4d94b9f":"* Here, we have used Standard deviation method to detect the outliers.If we have any data point that is more than 3 times the standard deviation, then those points are very likely to be outliers.\n* We can see that slag, water, superplastic and age contain outliers.","a0814c85":"### K fold cross validation","0e18f6a9":"### please **UPVOTE** if you find this notebook useful...!","4e790738":"##### checking datatypes","4242ff0e":"## Thanks for visiting this notebook!!!\n","914e36f8":"### Visualizing the Regularized Tree","a9845ff8":"### Regularising\/Pruning of Decision Tree\n","06dc0613":"### K fold cross validation","fea79951":"## Bootstrap Sampling","9b82da8c":"* After applying all the models we can see that Random Forest Regressor, Random Forest Regressor k fold, Gradient Boost Regressor, Gradient Boost Regressor k fold, Bagging Regressor are giving better results as compared to other models.\n* Now as the dataset have different gaussians, we can apply k means clustering and then we can apply the models and compare the accuracy.","ee081448":"- cement is the most important feature\n- Here, ash, coarseagg, fineagg, superplastic and slag are the less significant variable.These will impact less to the strength column. This we have seen in pairplot also.","a8791f96":"### K Fold Cross validation","c66d0caf":"## Support Vector Regressor","f7152cd3":"## Gradient Boosting Regressor","02b83d52":"### Using Random Forest Regressor","135eec24":" ### Observation :\n- cement is almost normal. \n- slag has  three gausssians and rightly skewed.\n- ash has two gaussians and rightly skewed.\n- water has three guassians and slighly left skewed.\n- superplastic has two gaussians and rightly skewed.\n- coarseagg has three guassians and almost normal.\n- fineagg has almost two guassians and looks like normal.\n- age has multiple guassians and rightly skewed.","e2886cb9":"##### Loading the Data ","8f3ee3f2":"##### There are 1030 rows and 9 columns","1eb9ba69":"# Exploratory data quality report","ec400c5b":"### Observations\n* There are'nt any **high** correlations between **Compressive strength** and other features except for **Cement**, which should be the case for more strength.\n* **Age** and **Super plasticizer** are the other two features which are strongly correlated with **Compressive Strength**.\n* **Super Plasticizer** seems to have a negative high correlation with **Water**, positive correlations with **Fly ash** and **Fine aggregate**.\n\nWe can further analyze these correlations visually by plotting these relations.","8b09b94b":"##### Drop least significant features","535f041c":"## Random Forest Regressor","616adcb5":"### 4.c Using Gradient Boosting Regressor","86aceebf":"* This model is also overfit.","15f4490d":"There seems to be no high correlation between independant variables (features). This can be further confirmed by plotting the **Pearson Correlation coefficients** between the features.","06b3f960":"* Here, None of the dimensions are good predictor of target variable.\n* For all the dimensions (variables) every cluster have a similar range of values except in one case.\n* We can see that the body of the cluster are overlapping.\n* So in k means, though, there are clusters in datasets on different dimensions. But we can not see any distinct characteristics of these clusters which tell us to break data into different clusters and build separate models for them.","b93c981c":"## Ensemeble KNN Regressor, SVR, LR","708a11ae":"###### Data types information","ae2e77ed":"### KNN Regressor","e63838b5":"- cement,slag,ash are left skewed.","4a3d7b8b":"### 3.a Model Building\n","5227874e":"###  2.c","c5004f24":"* There is a overfitting in the model as the dataset is performing 99% accurately in trainnig data. However, the accuracy on test data drops.","d35c0be3":"### Dealing wit outliers","d34cb98f":"### K fold cross validation","012d7506":"##### 1.b Checking the pairwise relations of Features.","5876a737":"we will replace the outliers with the median","4bcdf49d":"### K Fold Cross Validation","f78f848b":"###### 1.a -> Checking for 'null' values","1335403c":"##### Scaling \nStandardizing the data i.e. to rescale the features to have a mean of zero and standard deviation of 1.","480a8d34":"### Multivariate Analysis\n","ca358054":"# Please **upvote** if you liked this notebook!!!","16fb4a16":"##### seperate feature and targets","fb1ef714":"The bootstrap random forest  classification model performance is between 84%-90.8% which is better than other classification algorithms.","7489a7aa":"### Data Preprocessing","7e5640be":"### What you will learn!!!\n- Regression\n- Exploratory Data Analysis\n- Modeling\n- Hyper parameter tuning","060543f0":"Simplifying Column names, since they appear to be too lengthy.","573b251a":"#### Decision Trees\n\nWe can use Decision Trees, since we have a lot of zeros in some of the input features as seen from their distributions in the pair plot above. This would help the decision trees build trees based on some conditions on features which can further improve performance.","5d123546":"##### Splitting data into Training and Test. "}}