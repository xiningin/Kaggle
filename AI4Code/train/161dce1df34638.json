{"cell_type":{"8ca058a2":"code","663fdffa":"code","532e1b82":"code","33e7ab47":"code","c3f959e7":"code","8dcb4b1c":"code","7b64afc7":"code","a1af0997":"code","93c264c7":"code","adaa1984":"code","ddc7e59c":"code","abb78d1a":"code","b7869471":"code","6cc06b3e":"code","0b484ad4":"code","0ff49b2d":"code","3a375008":"code","4667f7d3":"code","18591187":"code","f50445c9":"code","7cf28c3d":"code","2f667786":"code","289e135c":"markdown"},"source":{"8ca058a2":"!unzip \/kaggle\/input\/aerial-cactus-identification\/train.zip\n!unzip \/kaggle\/input\/aerial-cactus-identification\/test.zip","663fdffa":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.image as pimg\nimport seaborn as sns\nimport math\nfrom tqdm import tqdm\nfrom PIL import Image\n\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, GlobalMaxPooling2D\nfrom tensorflow.keras import optimizers, regularizers\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, LearningRateScheduler","532e1b82":"print(sys.version)\nprint('tensorflow -> ', tf.__version__)","33e7ab47":"# When implementing machine learning code in TensorFlow, \n# TensorFlow often uses pseudo-random seed for things like weight initialization.\n# As a result, the results change each time the code is re-executed, \n# and it's impossible to tell whether the result is due to a change in data or parameters or a random seed.\n# Therefore, The random seed needs to be fixed.\n\nnp.random.seed(12)\ntf.random.set_seed(12)","c3f959e7":"main_df = pd.read_csv('\/kaggle\/input\/aerial-cactus-identification\/train.csv')\nsub_df = pd.read_csv('\/kaggle\/input\/aerial-cactus-identification\/sample_submission.csv')\n\ntrain_dir = '\/kaggle\/working\/train\/'\ntest_dir = '\/kaggle\/working\/test\/'","8dcb4b1c":"main_df.head()","7b64afc7":"print('shape: ', main_df.shape)\nprint('===================================')\nprint(main_df['has_cactus'].value_counts())","a1af0997":"plt.style.use('default')\nsns.set()\nsns.set_style('whitegrid')\nsns.set_palette('Pastel2')\n\nx = ['has cactus', 'hasn\\'t cactus']\ny = main_df.groupby('has_cactus').size()\n\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\nax.pie(y, labels=x, autopct=\"%1.1f%%\")\n\nplt.show()","93c264c7":"fig, ax = plt.subplots(2, 5, figsize = (12,6))\n\nfor i, idx in enumerate(main_df[main_df['has_cactus'] == 1]['id'][-5:]):\n    path = os.path.join(train_dir, idx)\n    img = load_img(path)\n    ax[0, i].axis('off')\n    ax[0, i].set_title('has cactus')\n    ax[0, i].imshow(img)\n    \nfor i, idx in enumerate(main_df[main_df['has_cactus'] == 0]['id'][-5:]):\n    path = os.path.join(train_dir, idx)\n    img = load_img(path)\n    ax[1, i].axis('off')\n    ax[1, i].set_title('hasn\\'t cactus')\n    ax[1, i].imshow(img)","adaa1984":"train_df, val_df = train_test_split(main_df, test_size=0.25, stratify=main_df['has_cactus'], shuffle=True, random_state=12)\n\ntrain_df = train_df.reset_index()\nval_df = val_df.reset_index()\n\ntotal_train = train_df.shape[0]\ntotal_val = val_df.shape[0]\n\nprint('total_train: {}, total_val: {}'.format(total_train, total_val))","ddc7e59c":"img_width, img_height = 32, 32\ntarget_size = (img_width, img_height)\n\n# Define Data Augmentation\ntrain_datagen = ImageDataGenerator(rescale=1.\/255)\nval_datagen = ImageDataGenerator(rescale=1.\/255)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\n\n# Convert the data type of 'has_cactus' to str to allow the model to be trained.\ntrain_df['has_cactus'] = train_df['has_cactus'].astype(str)\nval_df['has_cactus'] = val_df['has_cactus'].astype(str)","abb78d1a":"batch_size = 32\nx_col, y_col = 'id', 'has_cactus'\nclass_mode = 'binary'\n\n\ntrain_gen = train_datagen.flow_from_dataframe(train_df,\n                                            train_dir,\n                                            x_col=x_col,\n                                            y_col=y_col,\n                                            class_mode=class_mode,\n                                            target_size=target_size,\n                                            batch_size=batch_size,\n                                            )\n\nval_gen = val_datagen.flow_from_dataframe(val_df,\n                                        train_dir,\n                                        x_col=x_col,\n                                        y_col=y_col,\n                                        class_mode=class_mode,\n                                        target_size=target_size,\n                                        batch_size=batch_size,\n                                        )","b7869471":"input_shape = (img_width, img_height, 3)\noptimizer = optimizers.Adam(lr=1e-3)","6cc06b3e":"model = Sequential()\n\nmodel.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu', input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(128, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(GlobalMaxPooling2D())\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(1, activation='sigmoid'))\n\n\nmodel.compile(loss='binary_crossentropy', metrics=['acc'], optimizer=optimizer)\nmodel.summary()","0b484ad4":"# Vary the learning rate according to the number of epochs.\ndef step_decay(epoch):\n    initial_rate = 0.001\n    drop = 0.5\n    epochs_drop = 10.0\n    lrate = initial_rate * math.pow(drop, math.floor((epoch) \/ epochs_drop))\n    \n    return lrate","0ff49b2d":"lrate = LearningRateScheduler(step_decay)\nes = EarlyStopping(monitor='val_loss', min_delta=0, patience=5)\n\ncallbacks = [lrate, es]","3a375008":"epochs = 30\n\nhistory = model.fit(\n    train_gen,\n    epochs=epochs,\n    steps_per_epoch=total_train\/\/batch_size,\n    validation_data=val_gen,\n    validation_steps=total_val\/\/batch_size,\n    callbacks=callbacks,\n    )","4667f7d3":"sns.set_palette('Dark2')\nfig,ax = plt.subplots(2, 1)\n\nplot_acc = pd.DataFrame({'acc': history.history['acc'],\n                         'val_acc': history.history['val_acc']})\n\nplot_loss = pd.DataFrame({'loss': history.history['loss'],\n                          'val_loss': history.history['val_loss']})\n\nplot_acc.plot(ax=ax[0])\nplot_loss.plot(ax=ax[1])","18591187":"def predict(model, sub_df):\n    pred = np.empty((sub_df.shape[0],))\n    \n    for n in tqdm(range(sub_df.shape[0])):\n        image = np.array(Image.open(test_dir + sub_df.id[n]))\n        pred[n] = model.predict(image.reshape((1, 32, 32, 3))\/255.0)[0]\n    \n    sub_df['has_cactus'] = pred\n    return sub_df","f50445c9":"predictions = predict(model, sub_df)","7cf28c3d":"# If you ignore this process, you can't submit file.\n# Maybe it's because there's more than just a file to submit in the working directory.\n# Please let me know if you have any other solution to this.\n\n!rm -r *","2f667786":"predictions.to_csv('submission.csv', header=True, index=False)","289e135c":"# **This was created by a beginner for a beginner.**\n\nIt's common to want to learn about Transfer Learning and Fine-Tuning, but stumble over the process of getting the image files into a form that can be trained on a model. Maybe it's just me...\n\nYou can create a simple model and submission file in this notebook.\u3000So you can focus on studying data augmentation and Transfer Learning.\n\nIt's a newbie's notebook, so I'm sorry if the code is messy and the processing isn't as smart as it should be."}}