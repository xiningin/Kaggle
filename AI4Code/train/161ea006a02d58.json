{"cell_type":{"4e8b02a1":"code","f2d07ddd":"code","5f50b3d0":"code","272dbcbc":"code","cbab27ab":"code","30673380":"code","3b09f059":"code","d3666cad":"code","5c39b8c2":"code","dc27fde4":"code","9712ee63":"code","e88b17d3":"code","6d03960e":"code","d2e31a01":"code","8e41665f":"code","19f44189":"code","8f78bff9":"code","79440a62":"code","e61099a3":"code","4ddbfe19":"code","5592b6c2":"code","7bab7ce9":"code","33553006":"code","8cacb732":"code","adbc1106":"code","6cf96c52":"code","323551f6":"markdown","92429ad9":"markdown","f4ea9a6a":"markdown","cd855b59":"markdown","7e9d8451":"markdown","9d00c524":"markdown","3a60d1e6":"markdown","1c8792a9":"markdown","fbfbcf9d":"markdown","83bb4957":"markdown","533b7ea2":"markdown","f5fc59b7":"markdown","30674590":"markdown"},"source":{"4e8b02a1":"# Importing the necessary libraries \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","f2d07ddd":"from sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.datasets import load_boston","5f50b3d0":"# Loading the Boston dataset\n# Independent features\nboston=load_boston()\n\ndf=pd.DataFrame(boston.data,columns=boston.feature_names)","272dbcbc":"# Dependent feature\ndf['Price']=boston.target","cbab27ab":"# Checking for null values\ndf.info()","30673380":"# Splitting X and y\nX=df.drop('Price',1)\ny=df['Price']","3b09f059":"plt.subplots(figsize=(20,7))\nsns.heatmap(df.corr(),annot=True,cmap='coolwarm')","d3666cad":"corr=df.corr()\ncor_target=abs(corr['Price'])\nimp_features=cor_target[cor_target>=0.4]\nimp_features","5c39b8c2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","dc27fde4":"model=LinearRegression()\n\nrfe=RFE(model,10)\nrfe.fit(X_train,y_train)","9712ee63":"print(rfe.support_)\nprint(rfe.ranking_)","e88b17d3":"# Creating a dataframe to see which of the features are included and which ones are not\npd.DataFrame(list(zip(X.columns,rfe.support_,rfe.ranking_)),columns=['Features','Support','Rank']).T","6d03960e":"# Builing a RFE model using the features included. This model is not the actual number of features to include but is a\n# rough number lesser than the number of features\ny_pred=rfe.predict(X)\nprint(r2_score(y,y_pred))\nprint(np.sqrt(mean_squared_error(y,y_pred)))","d2e31a01":"nof_cols=np.arange(1,14)\n\nfrom sklearn.model_selection import train_test_split","8e41665f":"# Getting the optimal number of features by getting the r-squared value for different number of features.\nscore_list=[]\n\nmodel=LinearRegression()\n\nfor i in range(13):\n    rfe=RFE(model,i+1)\n    rfe.fit(X_train,y_train)\n    y_pred=rfe.predict(X_test)\n    score=r2_score(y_test,y_pred)\n    score_list.append(score)","19f44189":"plt.plot(nof_cols,score_list)\nplt.xticks(np.arange(0,14))\nplt.grid()","8f78bff9":"# We again build a RFE model, but this time with 9 as the number of features.\nrfe=RFE(model,9)\nrfe.fit(X_train,y_train)\npd.DataFrame(list(zip(X.columns,rfe.support_,rfe.ranking_)),columns=['Features','Support','Rank']).T","79440a62":"y_pred_rfe=rfe.predict(X_test)\nprint(r2_score(y_test,y_pred_rfe))","e61099a3":"from mlxtend.feature_selection import SequentialFeatureSelector as sfs\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs","4ddbfe19":"model_1 = LinearRegression()","5592b6c2":"# We first opt for all the features to find out the optimal number of features\nsfs1 = sfs(model_1 , k_features=13 , forward=True , scoring='r2')\nsfs1 = sfs1.fit(X_train,y_train)\nfig = plot_sfs(sfs1.get_metric_dict())\nplt.grid(True)\nplt.show()\n","7bab7ce9":"sfs1 = sfs(model_1 , k_features=9 , forward=True , scoring='r2')\nsfs1 = sfs1.fit(X_train,y_train)\nsfs1.k_feature_names_","33553006":"X_train1=X_train[['CRIM', 'CHAS', 'NOX', 'RM', 'DIS', 'RAD', 'PTRATIO', 'B', 'LSTAT']]\nX_test1=X_test[['CRIM', 'CHAS', 'NOX', 'RM', 'DIS', 'RAD', 'PTRATIO', 'B', 'LSTAT']]","8cacb732":"lr=LinearRegression().fit(X_train,y_train)\ny_pred_fs=lr.predict(X_test)\nr2_score(y_test,y_pred_fs)","adbc1106":"sfs2=sfs(model_1,k_features=1,forward=False,scoring='r2')\nsfs2 = sfs2.fit(X_train,y_train)\nfig = plot_sfs(sfs2.get_metric_dict())\nplt.grid(True)\nplt.show()\n","6cf96c52":"lr=LinearRegression().fit(X_train,y_train)\ny_pred_be=lr.predict(X_test)\nr2_score(y_test,y_pred_be)","323551f6":"### FORWARD SELECTION AND BACKWARD ELIMINATION","92429ad9":"#### FORWARD SELECTION","f4ea9a6a":"### Using Heat map to check for co-relation","cd855b59":"### FEATURE SELECTION METHODS\n\nFeature Selection is the method of selecting the features from the dataset that contribute significantly to the variance in the target feature.\n\n##### Reasons to use Feature Selection\n- It enables the machine learning algorithm to train faster.\n- It reduces the complexity of a model and makes it easier to interpret.\n- It improves the accuracy of a model if the right subset is chosen.\n- It reduces overfitting.\n\n#### WRAPPER METHODS\nIn wrapper methods, we try to use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset. The problem is essentially reduced to a search problem. These methods are usually computationally very expensive.\n\nSome common examples of wrapper methods are forward feature selection, backward feature elimination, recursive feature elimination, etc.\n\n##### i) Forward Selection:  \n- Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n\n##### ii) Backward Elimination:  \n- In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n\n##### iii) Recursive Feature elimination: \n- It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination.\n\n#### CO-RELATION\n- By looking at the co-relation matrix or at the heatmap, the magnitude of co-relation values will give the co-relation of the independent features with the target feature.\n- Higher the value, higher is the co-relation. Features with very less co-relation do not have any significance and hence can be dropped.","7e9d8451":"- The dataset has no null values","9d00c524":"<h4> After using the above techniques and extracting the optimal number of features for model building, we opt for the required Machine Learning model and use the accquired number of features.","3a60d1e6":"- From the score graph, we see that after 9 features, the score is almost constant and hence we choose 9 as our number of features to include ","1c8792a9":"<h4> BACKWARD ELIMINATION","fbfbcf9d":"<br>\n<h4> The co-relation matrix for the dependent variable is created and the magnitude of co-relation greater than 0.4 is considered to be significant and is taken for model building.","83bb4957":"### RECURSIVE FEATURE ELIMIATION- RFE","533b7ea2":"-  The RFE returns the support and ranking where in the support gives whether the featureis in the threshold of number of features.\n- Ranking returns the rank of the features based on its significance with the dependent feature.","f5fc59b7":"- From the score graph, we see that after 9 features, the score is almost constant and hence we choose 9 as our number of features to include ","30674590":"- From the score graph, we see that after 9 features, the score is almost constant and hence we drop 4 features and  include 9 features."}}