{"cell_type":{"b3949188":"code","5194ab67":"code","bd0a292f":"code","0e4cb89f":"code","deddca82":"code","c3e810a2":"code","f7e0cc6b":"code","42928124":"code","b19b660b":"code","fe1ce063":"code","baeaae4b":"code","78cf71a8":"code","25e08fbf":"code","9248bb83":"code","e0a58638":"code","e7171686":"code","ec26c2e5":"code","7d4a325d":"code","db28ee4d":"code","a1736a9f":"code","5f9e0ea4":"code","81c488fd":"code","dd327c95":"code","55d0791f":"code","bfd88c1d":"code","06ff2bf0":"code","b7f5df06":"code","6d0ba221":"code","9cde58f1":"code","6cc860ef":"code","0cc4b0aa":"code","4dd5abd5":"code","3067ebea":"code","03306a85":"code","406c563c":"code","4b859cf4":"markdown","e427356f":"markdown","1d4fbf8f":"markdown","f93a93b3":"markdown","652dff4c":"markdown","9f712268":"markdown","022c39c2":"markdown","574511fe":"markdown","c516d94a":"markdown","02fb024c":"markdown","216d6381":"markdown","b4576c94":"markdown","a523d9af":"markdown","c8b01e4a":"markdown","841ee457":"markdown","b87178a0":"markdown","cc138f86":"markdown","1f9ddfe4":"markdown","5f6bf4ce":"markdown","29185571":"markdown","cd246489":"markdown","32a47562":"markdown","ca83bee5":"markdown","e7556d9e":"markdown","3127f6b2":"markdown","7cb0e512":"markdown","6d935a24":"markdown","614a7f7c":"markdown","ffb7a3d4":"markdown","b10c44d3":"markdown","479f3662":"markdown"},"source":{"b3949188":"import matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nplt.style.use('dark_background')\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib.colors import ListedColormap\nfrom scipy.stats import norm, boxcox\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom collections import Counter\nfrom scipy import stats\nfrom tqdm import tqdm_notebook\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)","5194ab67":"dataset = pd.read_csv('..\/input\/defaulter\/credit_card_defaulter.csv')","bd0a292f":"dataset.head()","0e4cb89f":"dataset = dataset.drop('Unnamed: 0', axis = 1)","deddca82":"dataset.shape","c3e810a2":"dataset.describe().T.style.bar(\n    subset=['mean'],\n    color='#606ff2').background_gradient(\n    subset=['std'], cmap='PuBu').background_gradient(subset=['50%'], cmap='PuBu')","f7e0cc6b":"dataset.isnull().values.any()","42928124":"plt.figure(figsize=(12, 6))\nsns.countplot(x=\"default\", data=dataset, palette='husl');","b19b660b":"plt.figure(figsize=(12, 6))\nsns.countplot(x=\"student\", data=dataset, palette='husl');","fe1ce063":"cols = ['balance', 'income']","baeaae4b":"def boxPlotter(dataset, columnName):\n    \"\"\"\n    Plots boxplots for column given as parameter.\n    \"\"\"\n    sns.catplot(x=\"default\", y=columnName, data=dataset, kind=\"box\");\nfor column in tqdm_notebook(cols, desc = \"Your Charts are being ready\"):\n    boxPlotter(dataset, column)","78cf71a8":"def pieChartPlotter(dataset, columnName):\n    \"\"\"\n    Creates pie chart of the column given as parameter in the dataset\n    \"\"\"\n    values = dataset[columnName].value_counts()\n    labels = dataset[columnName].unique()\n    pie, ax = plt.subplots(figsize=[10, 6])\n\n    patches, texts, autotexts = ax.pie(values, labels=labels, autopct='%1.2f%%', shadow=True, pctdistance=.5,\n                                       )\n\n    plt.legend(patches, labels, loc=\"best\")\n    plt.title(columnName, color='white', fontsize=14)\n    plt.setp(texts, color='white', fontsize=20)\n    plt.setp(autotexts, size=10, color='black')\n    autotexts[1].set_color('black')\n    plt.axis('equal')\n    plt.tight_layout()\n    plt.show()","25e08fbf":"pieChartPlotter(dataset, 'default') \npieChartPlotter(dataset, 'student') ","9248bb83":"plt.figure(figsize=(20, 17))\nmatrix = np.triu(dataset.corr())\nsns.heatmap(dataset.corr(), annot=True,linewidth=.8, mask=matrix, cmap=\"rocket\");","e0a58638":"def distributionPlot(dataset):\n    \"\"\" \n    Creates distribution plot.\n    \"\"\"\n    fig = plt.figure(figsize=(20, 20))\n    for i in tqdm_notebook(range(0, len(dataset.columns)), desc = 'Your plots are being ready'):\n        fig.add_subplot(np.ceil(len(dataset.columns)\/3), 3, i+1)\n        sns.distplot(\n            dataset.iloc[:, i], color=\"lightcoral\", rug=True)\n        fig.tight_layout(pad=3.0)","e7171686":"plot_data = dataset.drop(columns=['default', 'student'], axis =1)","ec26c2e5":"distributionPlot(plot_data)","7d4a325d":"sns.pairplot(dataset, hue=\"default\", palette=\"husl\");","db28ee4d":"def skewnessCorrector(dataset,columnName):\n    import seaborn as sns\n    from scipy import stats\n    from scipy.stats import norm, boxcox\n    \"\"\"\n    This function returns two plots distplot and probability plot for non-normalized data and after normalizing the provided data. \n    Just provide it with two parameters dataset and the name of column.\n    It corrects the skewness of data applying Boxcox transformation on the provided data\n    \"\"\"\n    print('''Before Correcting''')\n    (mu, sigma) = norm.fit(dataset[columnName])\n    print(\"Mu before correcting {} : {}, Sigma before correcting {} : {}\".format(\n        columnName.capitalize(), mu, columnName.capitalize(), sigma))\n    plt.figure(figsize=(20, 10))\n    plt.subplot(1, 2, 1)\n    sns.distplot(dataset[columnName], fit=norm, color=\"lightcoral\");\n    plt.title(columnName.capitalize() +\n              \" Distplot before Skewness Correction\", color=\"black\")\n    plt.subplot(1, 2, 2)\n    stats.probplot(dataset[columnName], plot=plt)\n    plt.show()\n    # Applying BoxCox Transformation\n    dataset[columnName], lam_fixed_acidity = boxcox(\n        dataset[columnName])\n    \n    print('''After Correcting''')\n    (mu, sigma) = norm.fit(dataset[columnName])\n    print(\"Mu after correcting {} : {}, Sigma after correcting {} : {}\".format(\n        columnName.capitalize(), mu, columnName.capitalize(), sigma))\n    plt.figure(figsize=(20, 10))\n    plt.subplot(1, 2, 1)\n    sns.distplot(dataset[columnName], fit=norm, color=\"orange\");\n    plt.title(columnName.capitalize() +\n              \" Distplot After Skewness Correction\", color=\"black\")\n    plt.subplot(1, 2, 2)\n    stats.probplot(dataset[columnName], plot=plt)\n    plt.show()\n    return dataset","a1736a9f":"skewedColumns = ['income']\nfor column in skewedColumns:\n    skewnessCorrector(dataset,column)","5f9e0ea4":"features = dataset.iloc[:, 1:]\nlabels = dataset.iloc[:, 1]","81c488fd":"features , labels","dd327c95":"features.shape, labels.shape","55d0791f":"accuracy_scores ={}\ndef predictor(\n    features, labels, predictor ='lr', params={}, tune = False, test_size = .2, cv_folds =10,\n    random_state =42, hidden_layers = 4, output_units = 1, input_units = 6,\n    input_activation = 'relu', output_activation = 'sigmoid',optimizer = 'adam',\n    metrics= ['accuracy'], loss = 'binary_crossentropy',validation_split = .20, epochs = 100\n             ):\n    global accuracy_scores\n    \"\"\"\n    Encode Categorical Data then Applies SMOTE , Splits the features and labels in training and validation sets with test_size = .2 , scales X_train, X_val using StandardScaler.\n    Fits every model on training set and predicts results find and plots Confusion Matrix, \n    finds accuracy of model applies K-Fold Cross Validation \n    and stores its accuracies in a dictionary containing Model name as Key and accuracies as values and returns it\n    Applies GridSearch Cross Validation and gives best params out from param list.\n    \n    Parameters:\n        features : array \n                    features array\n                    \n        lables : array\n                    labels array\n                    \n        predictor : str\n                    Predicting model to be used\n                    Default 'lr'\n                         Predictor Strings:\n                            lr - Logisitic Regression\n                            svm -SupportVector Machine\n                            knn - K-Nearest Neighbours\n                            dt - Decision Trees\n                            nb - GaussianNaive bayes\n                            rfc- Random Forest Classifier\n                            xgb- XGBoost Classifier\n                            ann - Artificial Neural Network\n        params : dict\n                    contains parameters for model\n        tune : boolean  \n                when True Applies GridSearch CrossValidation   \n                Default is False\n            \n        test_size: float or int, default=.2\n                    If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to \n                    include in the test split. If int, represents the absolute number of test samples. \n        cv_folds : int\n                No. of cross validation folds. Default = 10\n        hidden_layers : int\n                No. of default layers of ann. Default = 4\n        inputs_units : int\n                No. of units in input layer. Default = 6\n        output_units : int\n                No. of units in output layer. Default = 6\n        input_activation : str \n                Activation function for Hidden layers. Default = 'relu'\n        output_activation : str \n                Activation function for Output layers. Default = 'sigmoid'\n        optimizer: str\n                Optimizer for ann. Default = 'adam'\n        loss : str\n                loss method for ann. Default = 'binary_crossentropy'\n        validation_split : float or int\n                Percentage of validation set splitting in ann. Default = .20\n        epochs : int\n                No. of epochs for ann. Default = 100\n                \n  \n    \n    EX: \n      For Logistic Regression\n            predictor\n                (\n                    features = features, \n                    labels = labels, \n                    predictor = 'lr', \n                    {'penalty': 'l1', 'solver': 'liblinear'}, \n                    tune = True, \n                    test_size = .25\n                )\n    \n    \"\"\"\n    print('Checking if labels or features are categorical! [*]\\n')\n    cat_features=[i for i in features.columns if features.dtypes[i]=='object']\n    if len(cat_features) >= 1 :\n        index = []\n        for i in range(0,len(cat_features)):\n            index.append(features.columns.get_loc(cat_features[i]))\n        print('Features are Categorical\\n')\n        from sklearn.compose import ColumnTransformer\n        from sklearn.preprocessing import OneHotEncoder\n        ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), index)], remainder='passthrough')\n        print('Encoding Features [*]\\n')\n        features = np.array(ct.fit_transform(features))\n        print('Encoding Features Done [',u'\\u2713',']\\n')\n    if labels.dtype == 'O':\n        from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n        le = LabelEncoder()\n        print('Labels are Categorical [*] \\n')\n        print('Encoding Labels \\n')\n        labels = le.fit_transform(labels)\n        print('Encoding Labels Done [',u'\\u2713',']\\n')\n    else:\n        print('Features and labels are not categorical [',u'\\u2713',']\\n')\n\n    print('Applying SMOTE [*]\\n')\n    from imblearn.over_sampling import SMOTE\n    sm=SMOTE(k_neighbors=4)\n    features,labels=sm.fit_resample(features,labels)\n    print('SMOTE Done [',u'\\u2713',']\\n')\n    \n    print('Splitting Data into Train and Validation Sets [*]\\n')\n    from sklearn.model_selection import train_test_split\n    X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size= test_size, random_state= random_state)\n    print('Splitting Done [',u'\\u2713',']\\n')\n    \n    print('Scaling Training and Test Sets [*]\\n')\n    from sklearn.preprocessing import StandardScaler\n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_val = sc.transform(X_val)\n    print('Scaling Done [',u'\\u2713',']\\n')\n    \n     \n    if predictor == 'lr':\n        print('Training Logistic Regression on Training Set [*]\\n')\n        from sklearn.linear_model import LogisticRegression\n        classifier = LogisticRegression(**params)\n        parameters= [{\n            'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n            'C' : np.logspace(-4, 4, 20),\n        }]\n\n    elif predictor == 'svm':\n        print('Training Support Vector Machine on Training Set [*]\\n')\n        from sklearn.svm import SVC\n        classifier = SVC(**params)\n        parameters = [\n            {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], 'C' : np.logspace(-4, 4, 20)},\n            {'kernel': ['linear'],'gamma': [1e-3, 1e-4,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], 'C' : np.logspace(-4, 4, 20)},\n            {'kernel': ['poly'], 'gamma': [1e-3, 1e-4,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], 'C' : np.logspace(-4, 4, 20)},\n            {'kernel': ['sigmoid'], 'gamma': [1e-3, 1e-4,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], 'C' : np.logspace(-4, 4, 20)},\n                     ]\n    elif predictor == 'knn':\n        print('Training K-Nearest Neighbours on Training Set [*]\\n')\n        from sklearn.neighbors import KNeighborsClassifier\n        classifier = KNeighborsClassifier(**params)\n        parameters = [{\n            'n_neighbors': list(range(0,31)),\n            'weights': ['uniform', 'distance'],\n            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n            'n_jobs': [1, 0, None]\n        }]\n\n    elif predictor == 'dt':\n        print('Training Decision Tree Classifier on Training Set [*]\\n')\n        from sklearn.tree import DecisionTreeClassifier\n        classifier = DecisionTreeClassifier(**params)\n        parameters= [{\n            'criterion': ['gini', 'entropy'],\n            'splitter': ['best', 'random'],\n            'max_features': [2, 3],\n            'max_depth': [4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150],\n\n        }]\n    elif predictor == 'nb':\n        print('Training Naive Bayes Classifier on Training Set [*]\\n')\n        from sklearn.naive_bayes import GaussianNB\n        classifier = GaussianNB(**params)\n        \n    elif predictor == 'rfc':\n        print('Training Random Forest Classifier on Training Set [*]\\n')\n        from sklearn.ensemble import RandomForestClassifier\n        classifier = RandomForestClassifier(**params)\n        parameters = [{\n            'criterion': ['gini', 'entropy'],\n            'n_estimators': [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 800, 900, 1000],\n            'bootstrap': [True,False],\n            'max_depth': [4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150],\n            'max_features': [2, 3],\n            'min_samples_leaf': [3, 4, 5],\n            'min_samples_split': [8, 10, 12],\n        }]\n    elif predictor == 'xgb':\n        print('Training XGBClassifier on Training Set [*]\\n')\n        from xgboost import XGBClassifier\n        classifier = XGBClassifier(**params)\n        parameters = {\n            'min_child_weight': [1, 5, 10],\n            'gamma': [1e-3, 1e-4,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n            'subsample': [0.6, 0.8, 1.0],\n            'colsample_bytree': [0.6, 0.8, 1.0],\n            'max_depth': [4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150],\n            'learning_rate': [0.3, 0.1, 0.03],\n        }\n    elif predictor =='ann':\n        print('Training ANN on Training Set [*]\\n')\n        def build_ann_model():\n            import tensorflow as tf\n            classifier = tf.keras.models.Sequential()\n            for i in range(0,hidden_layers):\n                classifier.add(tf.keras.layers.Dense(units = input_units, activation = input_activation))\n            classifier.add(tf.keras.layers.Dense(units = output_units, activation = output_activation))\n            classifier.compile(optimizer = optimizer, loss = loss, metrics = metrics,)\n            return classifier\n        classifier = build_ann_model()\n        ann_history = classifier.fit(X_train, y_train,validation_split = validation_split , validation_data = (X_val, y_val), epochs = epochs)\n\n    if not predictor == 'ann': \n        classifier.fit(X_train, y_train)\n    print('Model Training Done [',u'\\u2713',']\\n')\n                              \n    print('''Making Confusion Matrix [*]''')\n    from sklearn.metrics import confusion_matrix, accuracy_score\n    y_pred = classifier.predict(X_val)\n    if predictor == 'ann':\n        y_pred = (y_pred > 0.5)\n    cm = confusion_matrix(y_val, y_pred)\n    print(cm)\n    print('Confusion Matrix Done [',u'\\u2713',']\\n')\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, fmt='g', ax=ax);  \n    ax.set_xlabel('Predicted labels');\n    ax.set_ylabel('True labels'); \n    ax.set_title('Confusion Matrix'); \n    ax.xaxis.set_ticklabels(['0', '1']); \n    ax.yaxis.set_ticklabels(['0', '1']);\n    \n    print('''Evaluating Model Performance [*]''')\n    accuracy = accuracy_score(y_val, y_pred)\n    print('Validation Accuracy is :',accuracy)\n    print('Evaluating Model Performance [',u'\\u2713',']\\n')\n    \n    print('Applying K-Fold Cross validation [*]')\n    from sklearn.model_selection import cross_val_score\n    import tensorflow as tf\n    if predictor == 'ann':\n        classifier = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_ann_model, verbose=1)\n    accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=cv_folds,scoring='accuracy')\n    print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n    if not predictor == 'ann':\n        accuracy_scores[classifier] = accuracies.mean()*100\n    if predictor == 'ann':\n        accuracy_scores['ANN'] = accuracies.mean()*100\n    print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))   \n    print('K-Fold Cross validation [',u'\\u2713',']\\n')\n    if not predictor == 'nb' and tune and predictor =='ann':\n        print('Applying Grid Search Cross validation [*]')\n        from sklearn.model_selection import GridSearchCV,StratifiedKFold\n        \n        grid_search = GridSearchCV(\n            estimator=classifier,\n            param_grid=parameters,\n            scoring='accuracy',\n            cv=cv_folds,\n            n_jobs=-1,\n            verbose=4,\n        )\n        grid_search.fit(X_train, y_train)\n        best_accuracy = grid_search.best_score_\n        best_parameters = grid_search.best_params_\n        print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n        print(\"Best Parameters:\", best_parameters)\n        print('Applying Grid Search Cross validation [',u'\\u2713',']\\n')\n        \n    print('Complete [',u'\\u2713',']\\n')","bfd88c1d":"%%time\npredictor(features, labels, 'lr', {'C': 0.00026366508987303583, 'penalty': 'l2', 'solver': 'newton-cg'})","06ff2bf0":"%%time\npredictor(features, labels,'svm', {'C': .75, 'gamma': 0.2,\n          'kernel': 'linear', 'random_state': 42},tune = False)","b7f5df06":"%%time\npredictor(features, labels, 'knn', {'algorithm': 'kd_tree', 'n_jobs': 1, 'n_neighbors': 1, 'weights': 'uniform'})","6d0ba221":"%%time\npredictor(features, labels, 'dt', {'criterion': 'entropy', 'max_depth': 30, 'max_features': 'sqrt', 'splitter': 'best', 'random_state': 42})","9cde58f1":"%%time\npredictor(features, labels, 'nb', {})","6cc860ef":"%%time\npredictor(features, labels, 'rfc', {'criterion': 'gini', 'max_features': 'auto', 'n_estimators': 150,'random_state':0})","0cc4b0aa":"%%time\npredictor(features, labels, 'xgb', {'learning_rate':0.02, 'n_estimators':600, 'objective':'binary:logistic', 'nthread':1,'eval_metric':'logloss'})","4dd5abd5":"%%time\npredictor(features, labels, 'ann',epochs =5)","3067ebea":"accuracy_scores","03306a85":"maxKey = max(accuracy_scores, key=lambda x: accuracy_scores[x])\nprint('The model with highest K-Fold Validation Accuracy score is  {0} with an accuracy of  {1:.2f}'.format(\n    maxKey, accuracy_scores[maxKey]))","406c563c":"plt.figure(figsize=(12, 6))\nmodel_accuracies = list(accuracy_scores.values())\nmodel_names = ['LogisticRegression', 'SVM','KNN', 'Decisiontree', 'GaussianNB', 'RandomForest','XGBClassifier','ANN']\nsns.barplot(x=model_accuracies, y=model_names, palette='mako');","4b859cf4":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\">Note: These functions applied above are created by me. If you you use them in any case don't forget to give credit to me. I have spent alot of time building them.<\/h1>\n<\/div>\n","e427356f":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h2 style=\"text-align:center;\">Training XGBoost Classifier<\/h2>\n<\/div>\n\n","1d4fbf8f":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h2 style=\"text-align:center;\">Countplot for Default<\/h2>\n<\/div>\n","f93a93b3":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight:bold\">Data Preprocessing<\/h1>\n<\/div>\n\n","652dff4c":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\">Model training<\/h1>\n<\/div>\n\n\n","9f712268":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h2 style=\"text-align:center;\">Pie Chart<\/h2>\n<\/div>\n\n","022c39c2":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\">Summary<\/h1>\n    <h2 style=\"text-align:center;\"> All Models Performed almost close to 100 percent with validation data<\/h2>\n<\/div>\n","574511fe":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h2 style=\"text-align:center;\">Training Logistic Regression<\/h2>\n<\/div>\n\n\n\n","c516d94a":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\">Please give feedback by commenting below and If you liked my work please consider upvoting<\/h1>\n<\/div>\n","02fb024c":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h2 style=\"text-align:center;\">Correlation Plot<\/h2>\n<\/div>","216d6381":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\">Finding which model performed better<\/h1>\n<\/div>\n","b4576c94":"![](https:\/\/www.labotec.co.za\/wp-content\/uploads\/2016\/07\/finance-banner.png)\n#### [Image Source](https:\/\/www.labotec.co.za\/wp-content\/uploads\/2016\/07\/finance-banner.png)","a523d9af":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\"> Introduction <\/h1><\/div>\n\n","c8b01e4a":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h2 style=\"text-align:center;\">Training Random Forest<\/h2>\n<\/div>\n\n\n","841ee457":"\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\"> Exploratory Data Analysis <\/h1>\n<\/div>\n","b87178a0":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h2 style=\"text-align:center;\">Training ANN<\/h2>\n<\/div>\n\n","cc138f86":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h2 style=\"text-align:center;\">Training K-Nearest Neighnours<\/h2>\n<\/div>\n\n\n\n\n","1f9ddfe4":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h2 style=\"text-align:center;\">Training Decision Trees<\/h2>\n<\/div>\n\n","5f6bf4ce":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h2 style=\"text-align:center;\">Skewness Correction<\/h2>\n<\/div>\n\n","29185571":"\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\"> Importing Dataset <\/h1>\n<\/div>\n","cd246489":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h2 style=\"text-align:center;\">Training Support Vector Machine<\/h2>\n<\/div>\n\n\n\n","32a47562":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h2 style=\"text-align:center;\">Countplot for Student<\/h2>\n<\/div>\n","ca83bee5":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h2 style=\"text-align:center;\">Some Box Plots<\/h2>\n<\/div>\n","e7556d9e":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h2 style=\"text-align:center;\">Pairplots<\/h2>\n<\/div>\n\n","3127f6b2":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h2 style=\"text-align:center;\">Preparing Features and labels <\/h2>\n<\/div>\n\n","7cb0e512":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h3 style=\"text-align:center;\" >Above function's \n<a href=\"https:\/\/github.com\/d4rk-lucif3r\/EasifyML\">Github Repo<\/a><\/h3>\n\n<\/div>\n","6d935a24":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h2 style=\"text-align:center;\">Training Gausian naive Bayes<\/h2>\n<\/div>\n\n","614a7f7c":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\"> Importing Libraries <\/h1>\n<\/div>\n","ffb7a3d4":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h3 style=\"text-align:center;\" >Above function's \n<a href=\"https:\/\/github.com\/d4rk-lucif3r\/EasifyML\">Github Repo<\/a><\/h3>\n\n<\/div>\n","b10c44d3":"\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h2 style=\"text-align:center;\">Some Distribution Plots<\/h2>\n<\/div>\n","479f3662":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:yellow;\n           color:Black;\n           font-family:cursive\n           \">\n<h4 style=\"text-align:left;\">From above charts we can confirm<\/h4>\n<p style=\"text-align:left;\">1) There seems to be very less correlation between all the features.\n    <br>\n2) 'income' column is skewed. So, lets correct it<\/p>\n<\/div>\n"}}