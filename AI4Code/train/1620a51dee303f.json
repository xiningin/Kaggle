{"cell_type":{"4fe8818c":"code","f4f78da5":"code","6845fdb2":"code","5a1b4255":"code","642b1032":"code","0c0c6ab5":"code","ccabaea9":"code","8a53a9e0":"code","44ab4974":"code","a4db472b":"code","e5eb5ecb":"code","1fa5e33f":"code","4a99383e":"code","bb8a4512":"code","58182b75":"code","7003c773":"code","0c1ae2f2":"code","0da890ba":"code","da7ad847":"code","0c646a43":"code","0f89d6ea":"code","5d7f869e":"code","ce154ade":"code","076e3b2a":"code","c1b58cca":"code","0aa0a143":"code","cb1f474b":"code","dc22fb08":"code","f9c97409":"code","0690a927":"code","4fb69399":"code","fa9e116b":"code","233c3162":"code","e973ea01":"code","0cf21baa":"markdown","96c8df34":"markdown","9a9574a6":"markdown","a899fa39":"markdown","003348bd":"markdown","e14df0ae":"markdown","b8378bf9":"markdown","d57a7f49":"markdown","ea683b64":"markdown","6e448208":"markdown","88de8f5e":"markdown","401fe216":"markdown","ceae2512":"markdown","ec927763":"markdown","9c6b7ee2":"markdown","ca5d63a1":"markdown"},"source":{"4fe8818c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor\n%matplotlib inline\nimport seaborn as sns","f4f78da5":"train=pd.read_csv('..\/input\/train.csv')\ntest=pd.read_csv('..\/input\/test.csv')","6845fdb2":"print(\"shape of Training  dataset\", train.shape)\nprint(\"shape of test dataset\", test.shape)","5a1b4255":"train.head()","642b1032":"test.head()","0c0c6ab5":"train.info()","ccabaea9":"test.info()","8a53a9e0":"train.isnull().sum().sort_values(ascending=False)","44ab4974":"dtype_df = train.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","a4db472b":"unique_df = train.nunique().reset_index()\nunique_df.columns = [\"col_name\", \"unique_count\"]\n","e5eb5ecb":"unique_df.head(10)","1fa5e33f":"constant_df = unique_df[unique_df[\"unique_count\"]==1]\nconstant_df.shape","4a99383e":"train.drop(columns=constant_df['col_name'],inplace=True)\ntest.drop(columns=constant_df['col_name'],inplace=True)","bb8a4512":"print(\"updated train dataset shape\",train.shape)\nprint(\"update test dataset shape\", test.shape)","58182b75":"train.drop(\"ID\", axis = 1, inplace = True)\ny_train=train['target']\ntrain.drop(\"target\", axis = 1, inplace = True)\ntest.drop(\"ID\", axis = 1, inplace = True)","7003c773":"plt.figure(figsize=(10,6))\nsns.distplot(y_train,kde=False, bins=20).set_title('Histogram of target');\nplt.xlabel('Target')\nplt.ylabel('count');\n\n","0c1ae2f2":"plt.figure(figsize=(10,6))\nsns.distplot(np.log1p(y_train), bins=20,kde=False).set_title('Log histogram of Target');","0da890ba":"y_train=np.log1p(y_train)","da7ad847":"from sklearn.model_selection import train_test_split\nX_train, X_valid, Y_train, Y_valid = train_test_split(train, y_train, test_size=0.05, random_state=0)","0c646a43":"X_train.shape","0f89d6ea":"RF_clf=RandomForestRegressor(random_state=42,n_jobs=-1)\n","5d7f869e":"RF_clf.fit(X_train,Y_train)","ce154ade":"def evaluate(model, features, labels):\n    predictions = model.predict(features)\n    errors = abs(predictions - labels)\n    m = 100 * np.mean(errors \/ labels)\n    accuracy = 100 - m\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy","076e3b2a":"print('valid Accuracy')\nvalid_accuracy = evaluate(RF_clf, X_valid, Y_valid)","c1b58cca":"RF_target=np.expm1(RF_clf.predict(test))\n","0aa0a143":"NO_OF_FEATURES=1000\ncol = pd.DataFrame({'importance': RF_clf.feature_importances_, 'feature': train.columns}).sort_values(\n    by=['importance'], ascending=[False])[:NO_OF_FEATURES]['feature'].values","cb1f474b":"train=train[col]\ntest=test[col]\n","dc22fb08":"train.shape","f9c97409":"train[\"sum\"] = train.sum(axis=1)\ntest[\"sum\"] = test.sum(axis=1)\ntrain[\"var\"] = train.var(axis=1)\ntest[\"var\"] = test.var(axis=1)\ntrain[\"median\"] = train.median(axis=1)\ntest[\"median\"] = test.median(axis=1)\ntrain[\"mean\"] = train.mean(axis=1)\ntest[\"mean\"] = test.mean(axis=1)\ntrain[\"std\"] = train.std(axis=1)\ntest[\"std\"] = test.std(axis=1)\ntrain[\"max\"] = train.max(axis=1)\ntest[\"max\"] = test.max(axis=1)\ntrain[\"min\"] =train.min(axis=1)\ntest[\"min\"] = test.min(axis=1)\ntrain[\"skew\"] = train.skew(axis=1)\ntest[\"skew\"] = test.skew(axis=1)","0690a927":"from sklearn.model_selection import train_test_split\nX_train, X_valid, Y_train, Y_valid = train_test_split(train, y_train, test_size=0.1, random_state=0)","4fb69399":"X_train.shape","fa9e116b":"import lightgbm as lgb\n\n\ndef run_lgb(X_train, Y_train, X_valid, Y_valid, test):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"task\": \"train\",\n        \"boosting type\":'dart',\n        \"num_leaves\" :100,\n        \"learning_rate\" : 0.01,\n        \"bagging_fraction\" : 0.8,\n        \"feature_fraction\" : 0.8,\n        \"bagging_frequency\" : 6,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1,\n        \"seed\": 42\n    }\n    \n    lgtrain = lgb.Dataset(X_train, label=Y_train)\n    lgval = lgb.Dataset(X_valid, label=Y_valid)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 5000, \n                      valid_sets=[lgtrain, lgval], \n                      early_stopping_rounds=300, \n                      verbose_eval=100, \n                      evals_result=evals_result)\n    \n    lgb_prediction = np.expm1(model.predict(test, num_iteration=model.best_iteration))\n    return lgb_prediction, model, evals_result","233c3162":"lgb_pred, model, evals_result = run_lgb(X_train, Y_train, X_valid, Y_valid, test)\nprint(\"LightGBM Training Completed...\")","e973ea01":"sub=pd.read_csv('..\/input\/sample_submission.csv')\n\nsub_rf = pd.DataFrame()\nsub_rf[\"target\"] = RF_target\nsub_rf[\"ID\"] = sub[\"ID\"]\nsub_rf.to_csv(\"sub_rf.csv\", index=False)\n\nsub_lgb = pd.DataFrame()\nsub_lgb[\"target\"] = lgb_pred\nsub_lgb[\"ID\"] = sub[\"ID\"]\nsub_lgb.to_csv(\"sub_lgb.csv\", index=False)\n\n\nsub[\"target\"] = (sub_lgb[\"target\"] + sub_rf['target'] )\/2\n\nprint(sub.head())\nsub.to_csv('sub_lgb_xgb.csv', index=False)\nsub.head()","0cf21baa":"### columns with constant value","96c8df34":"## Random Forest","9a9574a6":"## checking missing values in train dataset","a899fa39":"## Selecting features by importance\nFilter out top 1000 features ","003348bd":"## checking unique values\n","e14df0ae":"## Evaluate  Random Forest Model","b8378bf9":"## Loading data","d57a7f49":"### Datatypes  of columns","ea683b64":"## prediction on Random Forest","6e448208":"## visualization of Target values\n","88de8f5e":"## Machine Learning Model","401fe216":"## dropping columns with constant value","ceae2512":"# LightGBM","ec927763":"## shape of the dataset","9c6b7ee2":"## Feature Engineering\n### adding some extra column features","ca5d63a1":"### splitting data into training and validation set"}}