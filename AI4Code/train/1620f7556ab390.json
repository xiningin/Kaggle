{"cell_type":{"bbc435f3":"code","b7ffa194":"code","1da48df3":"code","c9015deb":"code","5de6eca3":"code","9aceb940":"code","4e66e442":"code","1a7e55cc":"code","d7082b16":"code","5fd2a15d":"code","9699afce":"code","cc8f0015":"code","7df75904":"code","325ae72d":"code","f1357a11":"code","56ca61ec":"code","4f64f872":"code","aadadc10":"code","7b440b06":"code","80047341":"code","62327f81":"code","3726d871":"code","8e0d761c":"code","7c208998":"code","a6712648":"code","2dd66f03":"code","a48238bb":"code","e065077e":"code","8c28c106":"code","0d6c1e40":"code","d1084b3a":"code","93b29b09":"code","f795189b":"code","4c6ff9d8":"code","d1a6377f":"code","f39fc1c3":"code","1a2b1467":"code","e2626c26":"code","ef27a778":"code","cad37abb":"code","9f976638":"code","9953f927":"code","f39d8b81":"code","ee3de52a":"code","6acd0f3b":"code","061ce105":"code","2555f4c1":"code","a71940a5":"code","341af1ce":"code","745fe409":"code","65f0f7bc":"code","0764ff0c":"markdown","0ccc83e2":"markdown"},"source":{"bbc435f3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b7ffa194":"!git clone https:\/\/github.com\/ultralytics\/yolov5  # clone repo\n%cd yolov5\n\n# Install dependencies\n%pip install -qr requirements.txt  \n\n# change directory\n%cd ..\/\nimport torch\nprint(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")","1da48df3":"# # Install W&B \n# !pip install -q --upgrade wandb\n\n# # Login \n# import wandb\n\n# from kaggle_secrets import UserSecretsClient\n# user_secrets = UserSecretsClient() \n\n# personal_key_for_api = user_secrets.get_secret(\"ke\")\n# ! wandb login $personal_key_for_api","c9015deb":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom itertools import groupby\nimport numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport pickle\nimport cv2\nfrom multiprocessing import Pool\nimport matplotlib.pyplot as plt\n# import cupy as cp\nimport ast\nimport glob\nfrom os import listdir\nfrom os.path import isfile, join\nfrom glob import glob\nimport yaml\n\nimport shutil\nfrom shutil import copyfile\nimport sys\n\nfrom joblib import Parallel, delayed\n\n# --- Read data ---\nTRAIN_PATH = '\/kaggle\/input\/tensorflow-great-barrier-reef'","5de6eca3":"def get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\ndef get_path(row):\n    row['image_path'] = f'{TRAIN_PATH}\/train_images\/video_{row.video_id}\/{row.video_frame}.jpg'\n    return row\ndef load_image(image_path):\n    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n\ndef coco2yolo(image_height, image_width, bboxes):\n    \"\"\"\n    coco => [xmin, ymin, w, h]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # normolizinig\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]\/ image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]\/ image_height\n    \n    # converstion (xmin, ymin) => (xmid, ymid)\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\/2\n    \n    return bboxes\n\ndef yolo2coco(image_height, image_width, bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    coco => [xmin, ymin, w, h]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # denormalizing\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n    \n    # converstion (xmid, ymid) => (xmin, ymin) \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]\/2\n    \n    return bboxes\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None):\n    # Plots one bounding box on image img\n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) \/ 2) + 1  # line\/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label:\n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl \/ 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl \/ 3, [0, 0, 255], thickness=tf, lineType=cv2.LINE_AA)\n\n\n\ndef draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n    image = img.copy()\n    show_classes = classes if show_classes is None else show_classes\n    colors = (0, 255 ,0) if colors is None else colors\n    \n    if bbox_format == 'yolo':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:\n            \n                x1 = round(float(bbox[0])*image.shape[1])\n                y1 = round(float(bbox[1])*image.shape[0])\n                w  = round(float(bbox[2])*image.shape[1]\/2) #w\/2 \n                h  = round(float(bbox[3])*image.shape[0]\/2)\n\n                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(get_label(cls)),\n                             line_thickness = line_thickness)\n            \n    elif bbox_format == 'coco':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:            \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                w  = int(round(bbox[2]))\n                h  = int(round(bbox[3]))\n\n                voc_bbox = (x1, y1, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n\n    elif bbox_format == 'voc_pascal':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes: \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                x2 = int(round(bbox[2]))\n                y2 = int(round(bbox[3]))\n                voc_bbox = (x1, y1, x2, y2)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n    else:\n        raise ValueError('wrong bbox format')\n\n    return image\n\nnp.random.seed(8)\ncolors = (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\ncolors=(255,0,0)","9aceb940":"import pandas as pd\n# Read in the data CSV files\ndf = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv\")\ndf.head(5)","4e66e442":"df[\"NumBBox\"]=df['annotations'].apply(lambda x: str.count(x, 'x'))\ndf.head(5)","1a7e55cc":"df_train=df[df[\"NumBBox\"]>0]\ndf_train.sample(2)","d7082b16":"df_train['annotations'] = df_train['annotations'].progress_apply(lambda x: ast.literal_eval(x))\ndf_train['bboxes'] = df_train.annotations.progress_apply(get_bbox)\ndf_train.sample(2)","5fd2a15d":"df_train[\"Width\"]=1280\ndf_train[\"Height\"]=720\ndf_train.sample(2)\n\ndf_train = df_train.progress_apply(get_path, axis=1)\ndf_train.sample(2)","9699afce":"df_v = df_train[(df_train.NumBBox==13)].sample(2) \nfig,ax = plt.subplots(1,2,figsize=(30,20))\ni=0;\nfor index, row in df_v.iterrows():\n    img           = load_image(row.image_path)\n    image_height  = row.Height\n    image_width   = row.Width\n    bboxes_coco   = np.array(row.bboxes)\n    bboxes_yolo   = coco2yolo(image_height, image_width, bboxes_coco)\n    names         = ['COTS']*len(bboxes_coco)\n    labels        = [0]*len(bboxes_coco)\n    im=draw_bboxes(img = img,\n                           bboxes = bboxes_yolo, \n                           classes = names,\n                           class_ids = labels,\n                           class_name = True, \n                           colors = colors, \n                           bbox_format = 'yolo',\n                           line_thickness = 2)\n    ax[i].imshow(im)\n    ax[i].axis('OFF')\n    i=i+1","cc8f0015":"BATCH_SIZE = 16\nEPOCHS = 30\nIMG_SIZE=1280\nSelected_Fold=4  #0..4","7df75904":"from sklearn.model_selection import GroupKFold\nkf = GroupKFold(n_splits = 5) \ndf_train = df_train.reset_index(drop=True)\ndf_train['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df_train, y = df_train.video_id.tolist(), groups=df_train.sequence)):\n    df_train.loc[val_idx, 'fold'] = fold\ndisplay(df_train.fold.value_counts())","325ae72d":"\/Kaggle\/working\n    \/COTS\n         \/images\n             \/train\/img0.jpg\n             \/val\n         \/labels\n             \/train\/img0.txt\n             \/val\n    \/yolov5","f1357a11":"os.makedirs('COTS\/images\/train', exist_ok=True)\nos.makedirs('COTS\/images\/valid', exist_ok=True)\nos.makedirs('COTS\/labels\/train', exist_ok=True)\nos.makedirs('COTS\/labels\/valid', exist_ok=True)","56ca61ec":"for i in tqdm(range(len(df_train))):\n    row = df_train.loc[i]\n    if row.fold != Selected_Fold:\n        copyfile(f'{row.image_path}', f'COTS\/images\/train\/{row.image_id}.jpg')\n    else:\n        copyfile(f'{row.image_path}', f'COTS\/images\/valid\/{row.image_id}.jpg') ","4f64f872":"list1 = os.listdir('\/kaggle\/working\/COTS\/images\/train') # dir is your directory path\nnumber_files1 = len(list1)\nprint(\"Number of images in .\/COTS\/images\/train folder\",number_files1)\nlist2 = os.listdir('\/kaggle\/working\/COTS\/images\/valid') # dir is your directory path\nnumber_files2 = len(list2)\nprint(\"Number of images in .\/COTS\/images\/valid folder\",number_files2)","aadadc10":"import yaml\nwith open('\/kaggle\/working\/train.txt', 'w') as f:\n    for path in glob('\/kaggle\/working\/train\/*'):\n        f.write(path+'\\n')\n            \nwith open('\/kaggle\/working\/val.txt', 'w') as f:\n    for path in glob('\/kaggle\/working\/val\/*'):\n        f.write(path+'\\n')\n\ndata = dict(\n    train = '\/kaggle\/working\/COTS\/images\/train',\n    val = '\/kaggle\/working\/COTS\/images\/valid',\n    \n    nc    = 1, # number of classes\n    names =  ['cots'] # classes\n    )\n\nwith open('\/kaggle\/working\/yolov5\/data\/data.yaml', 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\n%cat \/kaggle\/working\/yolov5\/data\/data.yaml","7b440b06":"!ls '\/kaggle\/working\/yolov5\/data'","80047341":"all_bboxes = []\nfor row_idx in tqdm(range(df_train.shape[0])):\n    row = df_train.iloc[row_idx]\n    # Get image\n    image_name = row.image_id\n    image_height = row.Height\n    image_width  = row.Width\n    bboxes_coco  = np.array(row.bboxes).astype(np.float32).copy()\n    num_bbox     = len(bboxes_coco)\n    names        = ['cots']*num_bbox\n    labels       = [0]*num_bbox\n    if row.fold != Selected_Fold:\n        file_name = f'\/kaggle\/working\/COTS\/labels\/train\/{image_name}.txt'\n    else:\n        file_name = f'\/kaggle\/working\/COTS\/labels\/valid\/{image_name}.txt'\n\n    with open(file_name, 'w') as f:\n        bboxes_yolo  = coco2yolo(image_height, image_width, bboxes_coco)\n        bboxes_yolo  = np.clip(bboxes_yolo, 0, 1)\n        all_bboxes.extend(bboxes_yolo)\n        for bbox_idx in range(len(bboxes_yolo)):\n            bb=str(bboxes_yolo[bbox_idx])\n            bb=bb[1:-1]\n            #annot = [str(labels[bbox_idx])]+ list(bboxes_yolo[bbox_idx].astype(str))+(['\\n'] if num_bbox!=(bbox_idx+1) else [''])\n            annot = str(str(labels[bbox_idx])) + ' ' + bb + '\\n'\n            annot = ''.join(annot)\n            annot = annot.strip('')\n            f.write(annot)","62327f81":"list1 = os.listdir('\/kaggle\/working\/COTS\/labels\/train') # dir is your directory path\nnumber_files1 = len(list1)\nprint(\"Number of txt file in .\/COTS\/labels\/train folder\",number_files1)\nlist2 = os.listdir('\/kaggle\/working\/COTS\/labels\/valid') # dir is your directory path\nnumber_files2 = len(list2)\nprint(\"Number of txt file in .\/COTS\/labels\/valid folder\",number_files2)","3726d871":"%cat '\/kaggle\/working\/COTS\/labels\/train\/{list1[10]}'","8e0d761c":"%cd yolov5\/","7c208998":"#best_weights = '\/kaggle\/input\/nfl-weights\/yolov5\/kaggle-reef\/exp\/weights\/best.pt' --weights {best_weights} \\\n!python train.py --img {IMG_SIZE} \\\n                 --batch {BATCH_SIZE} \\\n                 --epochs {EPOCHS} \\\n                 --data data.yaml \\\n                 --weights yolov5s.pt \\\n                 --project kaggle-Reef ","a6712648":"#!ls \/kaggle\/working\/yolov5\/kaggle-NFL\/exp\nplt.figure(figsize=(10,10))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/P_curve.png'));","2dd66f03":"plt.figure(figsize=(10,10))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/PR_curve.png'));","a48238bb":"plt.figure(figsize=(10,10))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/F1_curve.png'));","e065077e":"plt.figure(figsize=(10,10))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/R_curve.png'));","8c28c106":"ig, ax = plt.subplots(3, 2, figsize = (2*5,3*5), constrained_layout = True)\nfor row in range(3):\n    ax[row][0].imshow(plt.imread(f'\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/val_batch{row}_labels.jpg'))\n    ax[row][0].set_xticks([])\n    ax[row][0].set_yticks([])\n    ax[row][0].set_title(f'\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/val_batch{row}_labels.jpg', fontsize = 12)\n    \n    ax[row][1].imshow(plt.imread(f'\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/val_batch{row}_pred.jpg'))\n    ax[row][1].set_xticks([])\n    ax[row][1].set_yticks([])\n    ax[row][1].set_title(f'\/kaggle\/working\/yolov5\/runs\/kaggle-Reef\/val_batch{row}_pred.jpg', fontsize = 12)","0d6c1e40":"plt.figure(figsize=(10,10))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/results.png'));","d1084b3a":"ls","93b29b09":"%%!\nzip -r \/kaggle\/working\/FinalTrainedYOLO5s.zip .\/working","f795189b":"ls","4c6ff9d8":"cd working","d1a6377f":"while(True):\n    pass","f39fc1c3":"cd yolov5","1a2b1467":"!python detect.py --source \/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\/1000.jpg","e2626c26":"import torch\n\n# Model\n# model = torch.hub.load('\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/weights\/best', 'yolov5s')\nmodel = torch.hub.load('ultralytics\/yolov5', 'custom', path='\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/weights\/best.pt')  # local model\n\n# Image\nimg = 'https:\/\/ultralytics.com\/images\/zidane.jpg'\n\n# Inference\nresults = model(img)\n\nresults.pandas().xyxy[0]","ef27a778":"import cv2\nimport torch\nfrom PIL import Image\n\n# Model\nmodel = torch.hub.load('ultralytics\/yolov5', 'custom', path='\/kaggle\/input\/trainedyolo5-on-cor\/yolov5\/kaggle-Reef\/exp\/weights\/best.pt')  # local model\n\n# Images\n# for f in ['0.jpg', '10.jpg']:\n#     torch.hub.download_url_to_file('..\/input\/tensorflow-great-barrier-reef\/train_images\/video_0' + f, f)  # download 2 images\nimg1 = cv2.imread('\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\/1002.jpg')[..., ::-1]  # PIL image\nimg2 = cv2.imread('\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\/100.jpg')[..., ::-1]  # OpenCV image (BGR to RGB)\nimgs = [img1, img2]  # batch of images\n\n# Inference\nresults = model(imgs)  # includes NMS\n\n# Results\nresults.print()  \nresults.save(\"\/kaggle\/working\/\")  # or .show()\n\nresults.xyxy[0]  # img1 predictions (tensor)\nresults.pandas().xyxy[0]  # img1 predictions (pandas)\n#      xmin    ymin    xmax   ymax  confidence  class    name\n# 0  749.50   43.50  1148.0  704.5    0.874023      0  person\n# 1  433.50  433.50   517.5  714.5    0.687988     27     tie\n# 2  114.75  195.75  1095.0  708.0    0.624512      0  person\n# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie","cad37abb":"import pandas as pd\n# Read in the data CSV files\ndf = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/test.csv\")\ndf.head(5)","9f976638":"def load_model(Best_Model, conf=0.25, iou=0.50):\n    model = torch.hub.load('ultralytics\/yolov5',\n                           'custom',\n                           path='\/kaggle\/input\/trainedyolo5-on-cor\/yolov5\/kaggle-Reef\/exp\/weights\/best.pt',\n                           force_reload=True)  # local repo\n    model.conf = conf  # NMS confidence threshold\n    model.iou  = iou  # NMS IoU threshold\n    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n    model.multi_label = False  # NMS multiple labels per box\n    model.max_det = 1000  # maximum number of detections per image\n    return model\n\n\ndef predict(model, img, size=768, augment=False):\n    height, width = img.shape[:2]\n    results = model(img, size=size, augment=augment)  # custom inference size\n    preds   = results.pandas().xyxy[0]\n    bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n    if len(bboxes):\n        bboxes  = voc2coco(bboxes,height,width).astype(int)\n        confs   = preds.confidence.values\n        return bboxes, confs\n    else:\n        return [],[]\n    \ndef format_prediction(bboxes, confs):\n    annot = ''\n    if len(bboxes)>0:\n        for idx in range(len(bboxes)):\n            xmin, ymin, w, h = bboxes[idx]\n            conf             = confs[idx]\n            limited_float = \"{:.2f}\".format(conf)\n            annot += f'{limited_float} {xmin} {ymin} {w} {h}'\n            annot +=' '\n        annot = annot.strip(' ')\n    return annot\n\ndef show_img(img, bboxes, bbox_format='yolo'):\n    names  = ['starfish']*len(bboxes)\n    labels = [0]*len(bboxes)\n    img    = draw_bboxes(img = img,\n                           bboxes = bboxes, \n                           classes = names,\n                           class_ids = labels,\n                           class_name = True, \n                           colors = colors, \n                           bbox_format = bbox_format,\n                           line_thickness = 2)\n    return Image.fromarray(img).resize((800, 400))","9953f927":"def voc2yolo(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    voc  => [x1, y1, x2, y1]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]\/ image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]\/ image_height\n    \n    w = bboxes[..., 2] - bboxes[..., 0]\n    h = bboxes[..., 3] - bboxes[..., 1]\n    \n    bboxes[..., 0] = bboxes[..., 0] + w\/2\n    bboxes[..., 1] = bboxes[..., 1] + h\/2\n    bboxes[..., 2] = w\n    bboxes[..., 3] = h\n    \n    return bboxes\n\ndef yolo2voc(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n    \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]\/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n    \n    return bboxes\n\ndef coco2yolo(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    coco => [xmin, ymin, w, h]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # normolizinig\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]\/ image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]\/ image_height\n    \n    # converstion (xmin, ymin) => (xmid, ymid)\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\/2\n    \n    return bboxes\n\ndef yolo2coco(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    coco => [xmin, ymin, w, h]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # denormalizing\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n    \n    # converstion (xmid, ymid) => (xmin, ymin) \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]\/2\n    \n    return bboxes\n\ndef voc2coco(bboxes, image_height=720, image_width=1280):\n    bboxes  = voc2yolo(bboxes, image_height, image_width)\n    bboxes  = yolo2coco(bboxes, image_height, image_width)\n    return bboxes\n\n\ndef load_image(image_path):\n    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n\n\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None):\n    # Plots one bounding box on image img\n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) \/ 2) + 1  # line\/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label:\n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl \/ 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl \/ 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n\ndef draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n     \n    image = img.copy()\n    show_classes = classes if show_classes is None else show_classes\n    colors = (0, 255 ,0) if colors is None else colors\n    \n    if bbox_format == 'yolo':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:\n            \n                x1 = round(float(bbox[0])*image.shape[1])\n                y1 = round(float(bbox[1])*image.shape[0])\n                w  = round(float(bbox[2])*image.shape[1]\/2) #w\/2 \n                h  = round(float(bbox[3])*image.shape[0]\/2)\n\n                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(get_label(cls)),\n                             line_thickness = line_thickness)\n            \n    elif bbox_format == 'coco':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:            \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                w  = int(round(bbox[2]))\n                h  = int(round(bbox[3]))\n\n                voc_bbox = (x1, y1, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n\n    elif bbox_format == 'voc_pascal':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes: \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                x2 = int(round(bbox[2]))\n                y2 = int(round(bbox[3]))\n                voc_bbox = (x1, y1, x2, y2)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n    else:\n        raise ValueError('wrong bbox format')\n\n    return image\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path'])\n    return row\n\n\nnp.random.seed(8)\ncolors = (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\ncolors=(255,0,0)","f39d8b81":"import greatbarrierreef\nenv = greatbarrierreef.make_env()   # initialize the environment\n   # an iterator which loops over the test set and sample submission","ee3de52a":"iter_test = env.iter_test() ","6acd0f3b":"DETECTION_THRESHOLD = 0.19\n\nsubmission_dict = {\n    'id': [],\n    'prediction_string': [],\n}","061ce105":"CONF= 0.15\nIOU= 0.50\n#model = load_model(Best_Model, conf=CONF, iou=IOU)\nfor (img, pred_df) in enumerate(tqdm(iter_test)):\n    bboxes, confs  = predict(model, img, size=IMG_SIZE, augment=True)\n    annot          = format_prediction(bboxes, confs)\n    #print(annot)\n    pred_df['annotations'] = annot\n    env.predict(pred_df)\n    if idx<3:\n        display(show_img(img, bboxes, bbox_format='coco'))","2555f4c1":"env.predict()","a71940a5":"bboxes, confs  = predict(model,img1,IMG_SIZE)\nannot          = format_prediction(bboxes, confs)","341af1ce":"annot","745fe409":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","65f0f7bc":"model('https:\/\/ultralytics.com\/images\/zidane.jpg').pandas().xyxy","0764ff0c":"<a href=\".\/FinalTrainedYOLO5s.zip\"> Download File <\/a>","0ccc83e2":"# Testing"}}