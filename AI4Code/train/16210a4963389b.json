{"cell_type":{"11db509d":"code","f13e6a6a":"code","4869d9ea":"code","8b547086":"code","9abbdc05":"code","19b3114c":"code","b8334d95":"code","40d9c06d":"code","d1ac7f18":"code","f8959ae6":"code","ccd13c28":"markdown","25edcdad":"markdown","de387266":"markdown","9ab37bf8":"markdown","ef0f33ac":"markdown","2ba5882f":"markdown","76fa6658":"markdown","82b7b2b1":"markdown","c58759c6":"markdown","bed5f7a5":"markdown","90d60971":"markdown","ae11e628":"markdown","4b9a03eb":"markdown"},"source":{"11db509d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport time\n\nfrom math import sqrt\nfrom numpy import loadtxt\nfrom itertools import product\nfrom tqdm import tqdm\nfrom sklearn import preprocessing\nfrom xgboost import plot_tree\nfrom matplotlib import pyplot\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nkernel_with_output = True","f13e6a6a":"sales_train = pd.read_csv('..\/input\/sales_train.csv.gz', compression='gzip', header=0, sep=',', quotechar='\"')\nitems = pd.read_csv('..\/input\/items.csv')\nshops = pd.read_csv('..\/input\/shops.csv')\nitem_categories = pd.read_csv('..\/input\/item_categories.csv')\ntest = pd.read_csv('..\/input\/test.csv.gz', compression='gzip', header=0, sep=',', quotechar='\"')\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv.gz', compression='gzip', header=0, sep=',', quotechar='\"')\n\n# Cleaned up a little of sales data after some basic EDA:\n#sales_train = sales_train[sales_train.item_price<110000]\n#sales_train = sales_train[sales_train.item_cnt_day<=1100]","4869d9ea":"# For every month we create a grid from all shops\/items combinations from that month\ngrid = []\nfor block_num in sales_train['date_block_num'].unique():\n    cur_shops = sales_train[sales_train['date_block_num']==block_num]['shop_id'].unique()\n    cur_items = sales_train[sales_train['date_block_num']==block_num]['item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n# Aggregations\nsales_train['item_cnt_day'] = sales_train['item_cnt_day'].clip(0,20)\ngroups = sales_train.groupby(['shop_id', 'item_id', 'date_block_num'])\ntrainset = groups.agg({'item_cnt_day':'sum', 'item_price':'mean'}).reset_index()\ntrainset = trainset.rename(columns = {'item_cnt_day' : 'item_cnt_month'})\ntrainset['item_cnt_month'] = trainset['item_cnt_month'].clip(0,20)\n\ntrainset = pd.merge(grid,trainset,how='left',on=index_cols)\ntrainset.item_cnt_month = trainset.item_cnt_month.fillna(0)\n\n# Get category id\ntrainset = pd.merge(trainset, items[['item_id', 'item_category_id']], on = 'item_id')\ntrainset.to_csv('trainset_with_grid.csv')\n\ntrainset.head()","8b547086":"# Set seeds and options\nnp.random.seed(10)\npd.set_option('display.max_rows', 231)\npd.set_option('display.max_columns', 100)\n\n# Feature engineering list\nnew_features = []\n\n# Periods range \nlookback_range = [1,2,11,12] #[1,2,3,4,5,6,7,8,9,10,11,12] #[1 ,2 ,3 ,4, 5, 12]\n\ntqdm.pandas()\n","9abbdc05":"current = time.time()\n\ntrainset = pd.read_csv('trainset_with_grid.csv')\nitems = pd.read_csv('..\/input\/items.csv')\nshops = pd.read_csv('..\/input\/shops.csv')\n\n\n# Only use more recent data\nstart_month = 0\nend_month = 33\ntrainset = trainset[['shop_id', 'item_id', 'item_category_id', 'date_block_num', 'item_price', 'item_cnt_month']]\ntrainset = trainset[(trainset.date_block_num >= start_month) & (trainset.date_block_num <= end_month)]\n\nprint('Loading test set...')\ntest_dataset = loadtxt('..\/input\/test.csv', delimiter=\",\" ,skiprows=1, usecols = (1,2), dtype=int)\ntestset = pd.DataFrame(test_dataset, columns = ['shop_id', 'item_id'])\n\nprint('Merging with other datasets...')\n# Get item category id into test_df\ntestset = testset.merge(items[['item_id', 'item_category_id']], on = 'item_id', how = 'left')\ntestset['date_block_num'] = 34\n# Make testset contains same column as trainset so we can concatenate them row-wise\ntestset['item_cnt_month'] = -1\n\ntrain_test_set = pd.concat([trainset, testset], axis = 0) \n\nend = time.time()\ndiff = end - current\nprint('Took ' + str(int(diff)) + ' seconds to train and predict val set')","19b3114c":"item_cat = pd.read_csv('..\/input\/item_categories.csv')\n\n# Fix category\nl_cat = list(item_cat.item_category_name)\nfor ind in range(0,1):\n    l_cat[ind] = 'PC Headsets \/ Headphones'\nfor ind in range(1,8):\n    l_cat[ind] = 'Access'\nl_cat[8] = 'Tickets (figure)'\nl_cat[9] = 'Delivery of goods'\nfor ind in range(10,18):\n    l_cat[ind] = 'Consoles'\nfor ind in range(18,25):\n    l_cat[ind] = 'Consoles Games'\nl_cat[25] = 'Accessories for games'\nfor ind in range(26,28):\n    l_cat[ind] = 'phone games'\nfor ind in range(28,32):\n    l_cat[ind] = 'CD games'\nfor ind in range(32,37):\n    l_cat[ind] = 'Card'\nfor ind in range(37,43):\n    l_cat[ind] = 'Movie'\nfor ind in range(43,55):\n    l_cat[ind] = 'Books'\nfor ind in range(55,61):\n    l_cat[ind] = 'Music'\nfor ind in range(61,73):\n    l_cat[ind] = 'Gifts'\nfor ind in range(73,79):\n    l_cat[ind] = 'Soft'\nfor ind in range(79,81):\n    l_cat[ind] = 'Office'\nfor ind in range(81,83):\n    l_cat[ind] = 'Clean'\nl_cat[83] = 'Elements of a food'\n\nlb = preprocessing.LabelEncoder()\nitem_cat['item_category_id_fix'] = lb.fit_transform(l_cat)\nitem_cat['item_category_name_fix'] = l_cat\ntrain_test_set = train_test_set.merge(item_cat[['item_category_id', 'item_category_id_fix']], on = 'item_category_id', how = 'left')\n_ = train_test_set.drop(['item_category_id'],axis=1, inplace=True)\ntrain_test_set.rename(columns = {'item_category_id_fix':'item_category_id'}, inplace = True)\n\n_ = item_cat.drop(['item_category_id'],axis=1, inplace=True)\n_ = item_cat.drop(['item_category_name'],axis=1, inplace=True)\n\nitem_cat.rename(columns = {'item_category_id_fix':'item_category_id'}, inplace = True)\nitem_cat.rename(columns = {'item_category_name_fix':'item_category_name'}, inplace = True)\nitem_cat = item_cat.drop_duplicates()\nitem_cat.index = np.arange(0, len(item_cat))\nitem_cat.head()","b8334d95":"for diff in tqdm(lookback_range):\n    feature_name = 'prev_shopitem_sales_' + str(diff)\n    trainset2 = train_test_set.copy()\n    trainset2.loc[:, 'date_block_num'] += diff\n    trainset2.rename(columns={'item_cnt_month': feature_name}, inplace=True)\n    train_test_set = train_test_set.merge(trainset2[['shop_id', 'item_id', 'date_block_num', feature_name]], on = ['shop_id', 'item_id', 'date_block_num'], how = 'left')\n    train_test_set[feature_name] = train_test_set[feature_name].fillna(0)\n    new_features.append(feature_name)\ntrain_test_set.head(3)","40d9c06d":"groups = train_test_set.groupby(by = ['shop_id', 'item_id', 'date_block_num'])\nfor diff in tqdm(lookback_range):\n    feature_name = 'prev_shopitem_price_' + str(diff)\n    result = groups.agg({'item_price':'mean'})\n    result = result.reset_index()\n    result.loc[:, 'date_block_num'] += diff\n    result.rename(columns={'item_price': feature_name}, inplace=True)\n    train_test_set = train_test_set.merge(result, on = ['shop_id', 'item_id', 'date_block_num'], how = 'left')\n    train_test_set[feature_name] = train_test_set[feature_name]\n    new_features.append(feature_name)        \ntrain_test_set.head(3)\n","d1ac7f18":"groups = train_test_set.groupby(by = ['item_id', 'date_block_num'])\nfor diff in tqdm(lookback_range):\n    feature_name = 'prev_item_price_' + str(diff)\n    result = groups.agg({'item_price':'mean'})\n    result = result.reset_index()\n    result.loc[:, 'date_block_num'] += diff\n    result.rename(columns={'item_price': feature_name}, inplace=True)\n    train_test_set = train_test_set.merge(result, on = ['item_id', 'date_block_num'], how = 'left')\n    train_test_set[feature_name] = train_test_set[feature_name]\n    new_features.append(feature_name)        \ntrain_test_set.head(3)","f8959ae6":"current = time.time()\n\nbaseline_features = ['shop_id', 'item_id', 'item_category_id', 'date_block_num'] +  new_features + ['item_cnt_month']\n#train_test_set.fillna(0)\n# Clipping to range 0-20\ntrain_test_set['item_cnt_month'] = train_test_set.item_cnt_month.fillna(0).clip(0,20)\n\n# train: want rows with date_block_num from 0 to 31\ntrain_time_range_lo = (train_test_set['date_block_num'] >= 0)\ntrain_time_range_hi =  (train_test_set['date_block_num'] <= 32)\n\n# val: want rows with date_block_num from 22\nvalidation_time =  (train_test_set['date_block_num'] == 33)\n\n# test: want rows with date_block_num from 34\ntest_time =  (train_test_set['date_block_num'] == 34)\n\n\n# Retrieve rows for train set, val set, test set\ncv_trainset = train_test_set[train_time_range_lo & train_time_range_hi]\ncv_valset = train_test_set[validation_time]\ncv_trainset = cv_trainset[baseline_features]\ncv_valset = cv_valset[baseline_features]\ntestset = train_test_set[test_time]\ntestset = testset[baseline_features]\n\n# Prepare numpy arrays for training\/val\/test\ncv_trainset_vals = cv_trainset.values.astype(int)\ntrainx = cv_trainset_vals[:, 0:len(baseline_features) - 1]\ntrainy = cv_trainset_vals[:, len(baseline_features) - 1]\n\ncv_valset_vals = cv_valset.values.astype(int)\nvalx = cv_valset_vals[:, 0:len(baseline_features) - 1]\nvaly = cv_valset_vals[:, len(baseline_features) - 1]\n\ntestset_vals = testset.values.astype(int)\ntestx = testset_vals[:, 0:len(baseline_features) - 1]\n\nprint('Fitting...')\nmodel = xgb.XGBRegressor(max_depth = 8, min_child_weight = 0.5, subsample = 1, eta = 0.2, num_round = 1000, seed = 1, nthread = 16)\nmodel.fit(trainx, trainy, eval_metric='rmse')\npreds = model.predict(valx)\n\nnpreds = valy-preds #Distance between predictions and targets\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(valx, npreds)\nsecondpreds = regressor.predict(valx)\n\nnewy = secondpreds+preds\n#set now max_depth param from 8 to 10 for improvement\nmetamodel = xgb.XGBRegressor(max_depth = 10, min_child_weight = 0.5, subsample = 1, eta = 0.2, num_round = 1000, seed = 1, nthread = 16)\nft = metamodel.fit(valx, newy, eval_metric='rmse')\nmetapredicts = metamodel.predict(testx)\n\n\n# Clipping to range 0-20\nmetapredicts = np.clip(metapredicts, 0,20)\n#print('val set rmse: ', sqrt(mean_squared_error(valy, p)))\n\ndf = pd.DataFrame(metapredicts, columns = ['item_cnt_month'])\ndf['ID'] = df.index\ndf = df.set_index('ID')\ndf.to_csv('test_preds.csv')\nprint('test predictions written to file')\n\nend = time.time()\ndiff = end - current\nprint('Took ' + str(int(diff)) + ' seconds to train and predict val, test set')\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(metamodel, max_num_features=50, height=0.8, ax=ax)\nplt.show()","ccd13c28":"# Add previous shop\/item price as feature (Lag feature)","25edcdad":"## Map Items Categorries\nMap Categories to more narrow onesory","de387266":"## Import necessary libraries","9ab37bf8":"# Part2\n## Set up global vars ","ef0f33ac":"# Add previous item price as feature (Lag feature)","2ba5882f":"# Conclusion\nI got a rmse score of 0.992784 on public LB and 0.996122 on private.\n\n\n","76fa6658":"## Insert missing rows and aggregations","82b7b2b1":"# Add previous shop\/item sales as feature (Lag feature)","c58759c6":"# Modelling & Cross Validation (XGBoost using residual based boosting technique to minimize the MSE)\nIt was used residual based boosting technique to minimize the root square error. By using XGBoost, it was found  the distance between the predictions and the target values and they used as new target (Y) to a new Linear Regression model. Then the predictions of this model are summed up with the first predictions to create a new target value. This new target was used again to XGBoost model to find the final (improvement) predictions. The max_depth and learning rate (eta) were also optimized in order the model to be sufficient. This technique helps to minimize the LB score.","bed5f7a5":"# Exploratory Data Analysis","90d60971":"## Data loading\nLoad all provided datasets provided","ae11e628":"This notebook takes inspiration from:\n\nhttps:\/\/www.kaggle.com\/alexeyb\/coursera-winning-kaggle-competitions, https:\/\/www.kaggle.com\/anqitu\/feature-engineer-and-model-ensemble-top-10\n\nBy adding some further techniques and improvements the LB score was improvement.\n\n# Part 1\n\nUnderstand our data better in \"Exploratory Data Analysis\" section, do necessary data wrangling\n","4b9a03eb":"## Load data"}}