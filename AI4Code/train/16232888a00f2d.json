{"cell_type":{"0d74d665":"code","d5ae3bd8":"code","9119cdf8":"code","b5a40189":"code","f6a6032e":"code","c443e9f2":"code","57f7fc75":"code","7ad523d7":"code","c0a72b32":"code","3db849b6":"code","2023c920":"code","fb3f0dea":"code","b04cab84":"code","93d19da1":"code","ae8ce9d7":"code","7a9d8137":"code","945f931d":"code","9a627319":"code","0ac3b448":"code","153fd380":"code","40aff2db":"code","eb4e9b6d":"code","ef8064f7":"code","beeff314":"code","0e3f5e12":"code","0ec0faf5":"code","9146841d":"code","1e4ddab7":"code","717e3eeb":"markdown","08925f3e":"markdown","100b83d5":"markdown","12f2ee8c":"markdown","3584f1a0":"markdown","5a0bc795":"markdown","64dd4285":"markdown","979a4867":"markdown","d70f4108":"markdown","271cdbd3":"markdown","ca5471ce":"markdown","84253c66":"markdown"},"source":{"0d74d665":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d5ae3bd8":"train_data = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntrain_data.head()","9119cdf8":"print(f\"dataset has {train_data.shape[0]} rows and {train_data.shape[1]} columns\")","b5a40189":"train_data.isnull().sum()","f6a6032e":"X = pd.DataFrame({\"tweet\": train_data[\"text\"]})\ny = train_data[\"target\"]","c443e9f2":"# Example of a tweet\nX.loc[112, \"tweet\"], y[112]","57f7fc75":"# Remove all special characters and lower words, Lemmatization along the way is made.\n\nimport re\nfrom nltk import download\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\n\ndownload(\"stopwords\")\n\n\ndef remove_special_chars(tweet):\n    \"\"\" Function to remove special characters from tweets \"\"\"\n    tweet = re.sub(\"[0-9@,'\\.#\\)\\(\\*\\?!\\$\\^\\-\\_\\+\\=~]+\", \"\", tweet)\n    return tweet\n\n\ndef lemmatize_tweets(tweet):\n    \"\"\" Lemmatize words \"\"\"\n    lemmatizer = WordNetLemmatizer()\n    return lemmatizer.lemmatize(tweet)\n\n\ndef word_tokenizer(tweet):\n    \"\"\" NLTK word Tokenizer \"\"\"\n    regex_tokenizer = RegexpTokenizer(r\"\\w{3,}\")\n    return regex_tokenizer.tokenize(tweet)\n    \n    \ndef remove_stopwords(tweet):\n    \"\"\" Stop words removal function \"\"\"\n    stopwords_list = stopwords.words(\"english\")\n    for word in tweet:\n        if word in stopwords_list:\n            tweet.remove(word)\n    \n    return tweet\n    \n\nX[\"tweet\"] = X[\"tweet\"].apply(lambda tweet: word_tokenizer(lemmatize_tweets(remove_special_chars(tweet.lower()))))\nX.head()","7ad523d7":"X[\"tweet\"] = X[\"tweet\"].apply(lambda tweet: \" \".join(remove_stopwords(tweet)))\nX[\"tweet\"].head()","c0a72b32":"# Vectorize and keep the 400 most frequent words\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\ncv = CountVectorizer(encoding=\"utf-8\", max_features=400)\nmatrix = cv.fit_transform(X[\"tweet\"])\ncounts = pd.DataFrame(matrix.toarray(),\n                  columns=cv.get_feature_names())\ncounts","3db849b6":"matrix.shape","2023c920":"y.shape","fb3f0dea":"cv.get_feature_names()[:10]","b04cab84":"tmp_cv = pd.DataFrame(matrix[0].T.todense(), index=cv.get_feature_names(), columns=[\"CounverVectorizer\"])\ntmp_cv.sort_values([\"CounverVectorizer\"], ascending=False)","93d19da1":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\n\n\nlr = LogisticRegression()\nlr.fit(matrix, y)\nlr_pred = lr.predict(matrix)\nprint(f\"Training score for Logistic Regression with CountVectorizer is {f1_score(y, lr_pred)}\")","ae8ce9d7":"from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n\n\ntf_idf = TfidfVectorizer(max_features=400)\ntfidf_matrix = tf_idf.fit_transform(X[\"tweet\"])\n\nX_matrix = pd.DataFrame(tfidf_matrix.toarray(), columns=tf_idf.get_feature_names())\nX_matrix.head()","7a9d8137":"tmp = pd.DataFrame(tfidf_matrix[0].T.todense(), index=tf_idf.get_feature_names(), columns=[\"tfidf\"])\ntmp.sort_values([\"tfidf\"], ascending=False)","945f931d":"lr_tfidf = LogisticRegression()\nlr_tfidf.fit(X_matrix, y)\n\ntfidf_pred = lr_tfidf.predict(X_matrix)\nprint(f\"Training score for Logistic Regression with TFidfVectorizer {f1_score(tfidf_pred, y)}\")","9a627319":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom transformers import TFDistilBertModel, DistilBertTokenizer\n\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\ntransformer = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")","0ac3b448":"# We will go with a fixed max length to pad sequences, but you can use the function bellow\nmax_len = 160\n\ndef bert_encode(tokenizer, data):\n    # Tokenize texts and convert it to sequences.\n    return data.apply(lambda tweet: tokenizer.encode(tweet, add_special_tokens=True))\n\n\ndef max_len_sequences(sequences):\n    \"\"\" Compute max len to pad sequences \"\"\"\n    max_len = 0\n    \n    for sequence in sequences:\n        if len(sequence) > max_len:\n            max_len = len(sequence)\n\n    return max_len\n\n\ndef pad_sequences(sequences, max_len):\n    return np.array([sequence + [0] * (max_len - len(sequence))\n                    for sequence in sequences])","153fd380":"sequences = bert_encode(tokenizer, X[\"tweet\"])\nsequences","40aff2db":"sequences = pad_sequences(sequences, max_len)\nsequences","eb4e9b6d":"# Inspired from https:\/\/www.kaggle.com\/xhlulu\n\ndef build_model(transformer, max_len=160):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model\n    \n\nmodel = build_model(transformer, max_len=max_len)\nmodel.summary()","ef8064f7":"train_history = model.fit(sequences, y, validation_split=.2, epochs=4, batch_size=32)","beeff314":"test_data = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ntest_data.head()","0e3f5e12":"X_test = pd.DataFrame({\"tweet\": test_data[\"text\"]})\nX_test.head()","0ec0faf5":"test_sequences = bert_encode(tokenizer, X_test[\"tweet\"])\npadded_test_sequences = pad_sequences(test_sequences, max_len)\npadded_test_sequences","9146841d":"y_pred_bert = model.predict(padded_test_sequences, verbose=1)","1e4ddab7":"submission = pd.concat([pd.Series(test_data[\"id\"]), pd.Series(y_pred_bert.round().astype(int).flatten())], axis=1)\nsubmission.columns = [\"id\", \"target\"]\nsubmission.head()\n\nsubmission.to_csv('submit_result.csv', index=False)","717e3eeb":"### Now let's move on to something powerful...transformers\n\n![](https:\/\/images.bfmtv.com\/-EYzXu1HDf_Hkx73tTM-Wnnd_2M=\/0x0:1280x720\/640x0\/images\/Transformers-5-1053646.jpg)","08925f3e":"## EDA","100b83d5":"### Testing part","12f2ee8c":"#### We will train simple Linear Regression model with features from the CountVectorizer","3584f1a0":"#### Clearly transformers outperform other traditional methods which by the way tend to work well, when submitted I ended up with around a 82% accuracy","5a0bc795":"Training evaluation of Logistic Regression with Tf-idf is slightly lower than CountVectorizer but the evaluation on the test set is better. \nWhen submitted I score 74% with CountVectorizer against 76% with Tf-idf","64dd4285":"This notebook will contains different methods to predict whether a tweet is a disaster or not. We will start with the simplest method to advanced ones.\n\n1. CountVectorizer\n2. Tf-idf\n3. Transformers (Bert)","979a4867":"Tf-idf values can be viewed as follows","d70f4108":"## 1. CountVectorizer","271cdbd3":"Let's checkout for missing values and look at what a tweet looks like, if any cleaning need to be done","ca5471ce":"#### Let's move on now to tf-idf...as you know main difference between CountVectorizer and Tf-idf is that tf-idf words will have float values rather than integer ones from CountVectorizer","84253c66":"Only missing value within columns we don't care about."}}