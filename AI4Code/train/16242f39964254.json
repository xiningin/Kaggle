{"cell_type":{"66296715":"code","d0e4100a":"code","562973fc":"code","41cd606f":"code","1541c931":"code","6315113e":"code","712bd656":"code","5197f96d":"code","39c3adf2":"code","202c50e7":"code","53bae299":"code","656c176e":"code","570122b7":"code","80d60e34":"code","4cfde3dd":"code","aaa28446":"code","61261a1d":"code","d1980bc2":"code","de8a3e41":"code","e3dd1a4b":"code","2b9cb9c3":"code","ea4a65c3":"code","91d5bc1d":"code","64ba86ba":"code","d180c125":"code","d870db16":"code","175a92d4":"markdown","3f6e134f":"markdown","443c39f1":"markdown","a0f4ec15":"markdown","e8a551a0":"markdown","c19668e2":"markdown","b13889fc":"markdown","fd613745":"markdown","0fa87465":"markdown","e3bdbdc6":"markdown","73bbd081":"markdown","1e641e57":"markdown","b3bec0e5":"markdown","13441145":"markdown","15f6955a":"markdown","a4148445":"markdown","c73f5d5c":"markdown","a8b20486":"markdown","556618e4":"markdown","1785e319":"markdown","aec0e4ac":"markdown","f0429552":"markdown","a000acd3":"markdown","e722d5cb":"markdown","cdedccd3":"markdown","8d288152":"markdown"},"source":{"66296715":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d0e4100a":"train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ngender_submission=pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\ngender_submission.head(3)","562973fc":"submission_x='PassengerId' # an integer starting from 892 to 1309\nsubmission_y='Survived' # 1 or 0","41cd606f":"all_features=list(train.head(0))\nprint(\"before removing the first two columns\")\nprint(all_features)\n\nprint(\"\\n\")\n\nprint(\"after removing the first two columns\")\nall_features=all_features[2:] #remove first two columns \nprint(all_features)\n\ntrain.head(10)","1541c931":"cat=['Pclass','Sex','Embarked','Cabin']\n#cat=['Pclass','Sex','Embarked']\nnum=['Age','SibSp','Parch','Fare']","6315113e":"import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 2)\nprint('axes are '+str(axes))\n\npl1=train.loc[train['Survived']==1][cat[0]].hist(ax=axes[0])\npl1.title.set_text('survivors')\n\npl2=train.loc[train['Survived']==0][cat[0]].hist(ax=axes[1])\npl2.title.set_text('victims')\n\nplt.show()","712bd656":"fig, axes = plt.subplots(1, 2)\nprint('axes are '+str(axes))\n\npl1=train.loc[train['Survived']==1][cat[1]].hist(ax=axes[0])\npl1.title.set_text('survivors')\n\npl2=train.loc[train['Survived']==0][cat[1]].hist(ax=axes[1])\npl2.title.set_text('victims')\n\nplt.show()","5197f96d":"fig, axes = plt.subplots(1, 2)\nprint('axes are '+str(axes))\n\npl1=train.loc[train['Survived']==1][cat[2]].hist(ax=axes[0])\npl1.title.set_text('survivors')\n\npl2=train.loc[train['Survived']==0][cat[2]].hist(ax=axes[1])\npl2.title.set_text('victims')\n\nplt.show()","39c3adf2":"fig, axes = plt.subplots(1, 2)\nprint('axes are '+str(axes))\n\npl1=train.loc[train['Survived']==1][cat[3]].hist(ax=axes[0])\npl1.title.set_text('survivors')\n\npl2=train.loc[train['Survived']==0][cat[3]].hist(ax=axes[1])\npl2.title.set_text('victims')\n\nplt.show()","202c50e7":"fig, axes = plt.subplots(1, 2)\nprint('axes are '+str(axes))\n\npl1=train.loc[train['Survived']==1][num[0]].hist(ax=axes[0])\npl1.title.set_text('survivors')\n\npl2=train.loc[train['Survived']==0][num[0]].hist(ax=axes[1])\npl2.title.set_text('victims')\n\nplt.show()","53bae299":"fig, axes = plt.subplots(1, 2)\nprint('axes are '+str(axes))\n\npl1=train.loc[train['Survived']==1][num[1]].hist(ax=axes[0])\npl1.title.set_text('survivors')\n\npl2=train.loc[train['Survived']==0][num[1]].hist(ax=axes[1])\npl2.title.set_text('victims')\n\nplt.show()","656c176e":"fig, axes = plt.subplots(1, 2)\nprint('axes are '+str(axes))\n\npl1=train.loc[train['Survived']==1][num[2]].hist(ax=axes[0])\npl1.title.set_text('survivors')\n\npl2=train.loc[train['Survived']==0][num[2]].hist(ax=axes[1])\npl2.title.set_text('victims')\n\nplt.show()","570122b7":"fig, axes = plt.subplots(1, 2)\nprint('axes are '+str(axes))\n\npl1=train.loc[train['Survived']==1][num[3]].hist(ax=axes[0])\npl1.title.set_text('survivors')\n\npl2=train.loc[train['Survived']==0][num[3]].hist(ax=axes[1])\npl2.title.set_text('victims')\n\nplt.show()","80d60e34":"frames=[train[all_features],test[all_features]]\ntrain_and_test=pd.concat(frames)","4cfde3dd":"dics={}\n\nfor feature in cat:\n    \n    # create an empty dictionary for each feature\n    dics[feature]={}\n    \n    n=0 # n is the corresponding integer value of a feature\n    \n    for i in train_and_test[feature]:\n        if not i in dics[feature]:\n            dics[feature][i]=n\n            n+=1\n            \n# add Survived\ndics['Survived']={0:0,1:1}\n\n# sanity check           \nfor sub_dic in dics:\n    print(sub_dic)\n    print(dics[sub_dic])\n    print('length is: '+str(len(dics[sub_dic])))\n    print('\\n')","aaa28446":"def cat_to_vec(train,feature):\n    \n    #input:  train is a pandas dataframe\n    #        feature is a string e.g. 'Pclass'\n    #ouput: 'result' is a python list of data, each data is a python list of one-hot vector\n    result=[]\n    for i in train[feature]:\n        # initialize one-hot vector\n        temp=[0 for _ in range(len(dics[feature]))]\n        #print(dics[feature][i])\n        #if isinstance(dics[feature][i],int)==False:\n            #print(dics[feature][i])\n            #assert isinstance(dics[feature][i],int)\n        temp[dics[feature][i]]=1\n        result.append(temp)\n        \n    return result","61261a1d":"def rescale(l):\n    \n    #input: a column of numeric data (e.g. train['...'])\n    #output: a list of floating data ranging from 0 to 1\n    \n    a=min(l)\n    b=max(l)\n    return [(i-a)\/(b-a) for i in l]","d1980bc2":"def pre_process(pandas_data):\n    \n    # input: train.iloc[:tot*split] or train.iloc[tot*splic:]\n    # output: train_data is an 'm x shape' numpy array\n    #         shape is the number of columns, meaning the dimension of the hugh feature vector\n\n    #initialize train array, it has 891 empty arrays\n    array_data=[[] for _ in range(len(pandas_data))]\n\n    # for each training training example, first get a huge vector that combines all the features in 'cat'\n    for i in cat:\n        temp=cat_to_vec(pandas_data, i)\n        for j in range(len(array_data)):\n            array_data[j]=array_data[j]+temp[j]\n\n    # then combines all the features in 'num'\n    for i in num:\n        temp=rescale(pandas_data[i].fillna((pandas_data[i].median()))) # fill with median\n        for j in range(len(array_data)):\n            array_data[j].append(temp[j])\n            \n    try:\n    \n        # train_target is an m x 2 matrix\n        array_target=cat_to_vec(pandas_data,'Survived')\n\n        #convert the train_data and train_target to a numpy array\n        array_data=np.array(array_data)\n        array_target=np.array(array_target)\n\n        # num of components in the huge vector representing each training example\n        shape=len(array_data[0])\n\n        return array_data, array_target, shape\n    \n    # test set has not 'survived' column, so it will not have array_target output\n    except:\n        \n        array_data=np.array(array_data)\n        \n        shape=len(array_data[0])\n        \n        return array_data, None, shape","de8a3e41":"from sklearn.utils import shuffle\ntrain = shuffle(train)\ntrain.head(5)","e3dd1a4b":"tot_num=len(train.index)\nsplit1=int(tot_num*0.8) # want 80% examples to be training data\nsplit2=int(tot_num*0.9) # want 10% examples to be cross validation\n\n# preprocess training set\ntrain_data, train_target, shape = pre_process(train.iloc[:split1])\nprint(\"\\n\")\n\n# preprocess cross validation set\ncross_data, cross_target, _ = pre_process(train.iloc[split1:split2])\nprint(\"\\n\")\n\n# preprocess cross validation set\ntest_data, test_target, _ = pre_process(train.iloc[split2:])\nprint(\"\\n\")\n\n# preprocess test set\nfinal_data, _, _ = pre_process(test)\n\nprint(\"train_data shape and train_target shape\")\nprint(train_data.shape, train_target.shape)\n\nprint(\"\\n\"+\"cross_data shape and cross_target shape\")\nprint(cross_data.shape, cross_target.shape)\n\nprint(\"\\n\"+\"test_data shape and test_target shape\")\nprint(test_data.shape, test_target.shape)\n\nprint(\"\\n\"+\"final_data shape\")\nprint(final_data.shape)","2b9cb9c3":"from keras.layers import Dense, Dropout, Activation\nfrom keras.models import Sequential\nfrom keras.losses import BinaryCrossentropy\n\nmodel = Sequential()\n\n#model.add(Input(input_shape=(shape,)))\n\nmodel.add(Dropout(0.2)) # Dropout(0.2, input_shape=...) means drop 20% of the neurons in the layer after the dropout layer\nmodel.add(Dense(128, activation='relu')) # kernel_regularizer='L2' \n\nmodel.add(Dropout(0.2)) \nmodel.add(Dense(16, activation='relu')) \n\nmodel.add(Dense(2))\nmodel.add(Activation(activation='softmax'))\n\nmodel.compile(optimizer='adam', loss= BinaryCrossentropy(),metrics=['binary_accuracy'])\nmodel.fit(train_data, train_target, batch_size=64, epochs=500, verbose=0)","ea4a65c3":"print(\"\\n\"+\"Evaluation\")\n#train = shuffle(train)\n#t_data, t_target, _ = pre_process(train)\nprint(\"\\n\")\n#model.evaluate(t_data, t_target)\nmodel.evaluate(train_data, train_target)\nmodel.evaluate(cross_data, cross_target)\nmodel.evaluate(test_data, test_target)","91d5bc1d":"final_predictions=model.predict(final_data)\nsubmission=pd.DataFrame({submission_x:test[submission_x],submission_y:np.argmax(final_predictions,axis=-1)})\nsubmission.head(10)\n#final_predictions","64ba86ba":"filename = 'NN_submission.csv'\n\nsubmission.to_csv(filename,index=False)","d180c125":"from sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(n_estimators=100, max_depth=7)\nrandom_forest.fit(train_data, train_target)\ny_pred = random_forest.predict(final_data)\nprint(random_forest.score(train_data, train_target))\nprint(random_forest.score(test_data, test_target))\nprint(random_forest.score(cross_data, cross_target))\n#print(random_forest.score(test_data, test_target))","d870db16":"submission=pd.DataFrame({submission_x:test[submission_x],submission_y:np.argmax(y_pred,axis=-1)})\nsubmission.head(10)\n\nfilename = 'submission_random_forest.csv'\n\nsubmission.to_csv(filename,index=False)","175a92d4":"**Shuffle training examples, <span style='color:red'> this is extremely important because otherwise the training and cross validation sets will not come from the same distribution. <\/span>**","3f6e134f":"# 4. RANDOM FOREST MODEL. THIS OUT-PERFORMS OUR DEEP NN MODEL.","443c39f1":"**Read in data.**","a0f4ec15":"# 2. FEATURE ENGINEERING.","e8a551a0":"# 3. TRAIN THE DEEP NN NETWORK.","c19668e2":"**Below is the histogram for Embarked.**","b13889fc":"**Below is the histogram for age.**","fd613745":"# 1. UNDERSTAND ALL FEATURES.","0fa87465":"**Create an array of all useful features.**","e3bdbdc6":"**Seperate categorical as 'cat' and numerical data as 'num'.**","73bbd081":"**Below is the histogram for sex.**","1e641e57":"**Now create custom one_hot funciton.**","b3bec0e5":"**Find the ultimate dictionary, <font color='red'> we haven't dealt with missing values yet so this dictionary maps all missing values to a integer.**","13441145":"**Submission format.**","15f6955a":"**Below is the histogram for sibsp.**","a4148445":"**The above 'Cabin' dictionary maps missing values (nan) to 0. Also 3 cabins <span style='color:Blue'> 'B51 B53 B55' is mapped to 115 <\/span>, which is not ideal but might be fine for the preliminary work.**","c73f5d5c":"**Create training and cross validation sets.**","a8b20486":"**Below is the histogram for parch.**","556618e4":"<font size=\"5\"> \n1.Pcalss: 1 is 1st class. 2 is 2nd class. 3 is 3rd class. <br>\n<span style='color:Red'> 2.Name: useless <\/span> <br>\n3.Sex: 'male' and 'female' <br>\n4.Age: float <br>\n5.SibSp: # of siblings \/ spouses aboard the Titanic <br>\n6.Parch: # of parents \/ children aboard the Titanic <br>\n<span style='color:Red'>7.Ticket: Ticket number (this feature might be useless)<\/span> <br>\n<span style='color:Blue'>8.Fare: Passenger fare (this might be correlated with pclass and embarked) <\/span> <br>\n9.Cabin: cabin number  <span style='color:Blue'> (the cabin near impact point might have a low survival rate) <\/span> <br> \n10.Embarked: Port of Embarkation. C = Cherbourg, Q = Queenstown, S = Southampton\n<\/font>","1785e319":"**Below is the histogram for Pclass.**","aec0e4ac":"**Below is the histogram for fare.**","f0429552":"**As can be seen from above, the huge feature vector has 200 components (mainly from 'Cabin') and the target has 2 components (0 or 1).**","a000acd3":"**Combine train and test set so that we can figure out e.g. how many different values for 'Cabin' are there, this will later help us to convert the 'Cabin' feature to a one-hot vector.**","e722d5cb":"**Create custom function that processes the raw data and create features matrix and target matrix.**","cdedccd3":"**Below is the histogram for Cabin. Notice how scattered the distribution is.**","8d288152":"**Define feature renormalization function.**"}}