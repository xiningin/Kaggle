{"cell_type":{"a201c753":"code","9422d0b2":"code","c24ceda7":"code","a5c440e5":"code","4412a174":"code","829d322c":"code","b8d6c365":"code","3b038900":"code","bb3b86e5":"code","8c258e49":"code","5d556c59":"code","69a461ba":"code","3af3b58c":"code","de6a3584":"code","53205e22":"code","25942936":"code","d0891370":"code","55d1c222":"code","2beee4dd":"code","537e1e36":"code","d4d5b5b2":"code","c8f57701":"code","0efcf0f4":"code","1f9db4ea":"code","27b0830b":"code","0e113554":"code","752bac43":"code","0de984f1":"code","9566b4cc":"code","543c9891":"code","62609d3b":"code","3c3e6f05":"code","eb84f8ad":"code","1253240e":"code","75dfe0c7":"code","c65885e9":"code","8d8c8429":"code","e2f256be":"code","dd5dc089":"code","a84623a5":"code","f3d5ccdb":"code","d7ce3462":"code","276f0bae":"code","81358d60":"code","a95afcf5":"code","2e964e00":"code","47e72e75":"code","f08b04fa":"code","cbb6436d":"code","b487fb7b":"code","dff4ff29":"code","fd9e8fc9":"code","6bbfeb2b":"code","61450f4d":"markdown","d386bae9":"markdown","13db36b0":"markdown","26df6c85":"markdown","a4164c43":"markdown","9383408d":"markdown","8d369295":"markdown","da569b55":"markdown","6861f689":"markdown","ac502a39":"markdown","4ba25781":"markdown","e8d101f1":"markdown","e59e0928":"markdown","cb1e7e2f":"markdown","5f18a8af":"markdown","4686ea13":"markdown","cf34576b":"markdown","86122582":"markdown","1af72531":"markdown","fc21a687":"markdown","3898567e":"markdown","941cff64":"markdown","05004667":"markdown","62a622bb":"markdown"},"source":{"a201c753":"import os \nimport sys \nimport re\n\nimport scipy\nimport numpy as np\nimport pandas as pd\nimport jieba.analyse\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\n\n# import sklearn modules \nimport sklearn.metrics as skm\nimport sklearn.model_selection\nimport sklearn.preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix as skm_conf_mat\nfrom collections import Counter\nfrom collections import defaultdict","9422d0b2":"datas = pd.read_csv(\"..\/input\/bilibilib_gongzuoxibao.csv\", sep = \",\")","c24ceda7":"colnames = datas.columns\nprint(colnames) # author, score, disliked, likes, liked, ctime, score.1, content, last_ex_index, cursor, date","a5c440e5":"datas.shape","4412a174":"datas.head()","829d322c":"datas['score'].value_counts()","b8d6c365":"x = list(sorted(datas['score'].unique()))\ny = list(datas['score'].value_counts())[::-1]\nplt.bar(x,y, color='orange')\nplt.xlabel('Score')\nplt.ylabel('')\nplt.title('Rating Frequencies')\nplt.show()","3b038900":"#%% Content Analysis \ntexts = ';'.join(datas['content'].tolist())\ncut_text = \" \".join(jieba.cut(texts))\n# TF_IDF\nkeywords = jieba.analyse.extract_tags(cut_text, topK=100, withWeight=True, allowPOS=('a','e','n','nr','ns'))\ntext_cloud = dict(keywords)\n###pd.DataFrame(keywords).to_excel('TF_IDF\u5173\u952e\u8bcd\u524d100.xlsx')","bb3b86e5":"# Remove all punctuation and expression marks \ntemp =  \"\\\\\u3010.*?\u3011+|\\\\\u300a.*?\u300b+|\\\\#.*?#+|[.!\/_,$&%^*()<>+\"\"'?@|:~{}#]+|[\u2014\u2014\uff01\\\\\\\uff0c\u3002=\uff1f\u3001\uff1a\u201c\u201d\u2018\u2019\uffe5\u2026\u2026\uff08\uff09\u300a\u300b\u3010\u3011]\"\ncut_text = re.sub(pattern = temp, repl = \"\", string = cut_text)","8c258e49":"del datas['ctime']\ndel datas['cursor']\ndel datas['liked']\ndel datas['disliked']\ndel datas['likes']\ndel datas['last_ep_index']\npd.isnull(datas).astype(int).aggregate(sum, axis = 0)","5d556c59":"perfect = datas[datas.score == 10]\nimperfect = datas[datas.score != 10]\nperfect_sample = perfect.sample(n = 1583, random_state = 1 )\nnew_data = pd.concat([perfect_sample, imperfect], axis = 0)\n\nfeatures = new_data['content']\nlabels = new_data['score']","69a461ba":"rTrain, rTest, y_train, y_test = train_test_split(features, labels, test_size = 0.3, random_state=42)\n# let's understand up a bit the data\n## print out the shapes of  resultant feature data\nprint(\"\\t\\t\\tFeature Shapes:\")\nprint(\"Train set: \\t\\t{}\".format(rTrain.shape), \n      #\"\\nValidation set: \\t{}\".format(rValidation.shape),\n      \"\\nTest set: \\t\\t{}\".format(rTest.shape))","3af3b58c":"texts = '\\n'.join(rTrain.tolist())\n#cut_text = jieba.lcut(texts)\ncut_text = \"\".join(jieba.cut(texts))\ncut_text = re.sub(pattern = temp, repl = \"\", string = cut_text)\n\nkeyword = jieba.analyse.extract_tags(cut_text, topK=100, allowPOS=('a','e','n','nr','ns'))  # list\ncut_text = cut_text.split('\\n')\nkeyword","de6a3584":"cutlist = []\n\nfor i in range(0, len(cut_text)):\n    cut_dic = defaultdict(int) \n    comment = cut_text[i]\n    comment_cut = jieba.lcut(comment)\n    for word in comment_cut: # word freq for every comment \n        if word in keyword:\n            cut_dic[word] += 1  \n    order = sorted(cut_dic.items(),key = lambda x:x[1],reverse = True) # word freq in descending order\n    #print(order)\n \n    myresult = \"\" \n    for j in range(0,len(order)): \n        result = order[j][0]+ \"-\" + str(order[j][1])\n        myresult = myresult + \" \" + result  \n    cutlist.append(myresult)\n#print(cutlist)","53205e22":"word_freqs = []\nfor raw in cutlist:\n    word_freq = {}\n    for word_freq_raw in raw.split():\n        index = word_freq_raw.find('-')\n        word = word_freq_raw[:index]\n        freq = int(word_freq_raw[index + 1])\n        word_freq[word] = freq\n    word_freqs.append(word_freq)\n    \nmatrix = []\nfor word_freq in word_freqs:\n    row = []\n    for word in keyword:\n        if word in word_freq:\n            row.append(word_freq[word])\n        else:\n            row.append(0)\n    matrix.append(row)\n#print(matrix)\nmatrix = np.array(matrix)","25942936":"grade1 = np.array([0.1\n,0\n,0\n,0.7\n,0.8\n,0.1\n,0\n,0.3\n,0\n,0\n,0\n,0\n,0.6\n,0.1\n,-1\n,0\n,0\n,1\n,0\n,0\n,0\n,0.5\n,-0.3\n,-0.1\n,0.8\n,0\n,0.4\n,0\n,0\n,0\n,0.6\n,0.6\n,0.8\n,0\n,0.6\n,0.4\n,0.6\n,1\n,0\n,-0.7\n,0\n,0.9\n,0\n,-0.2\n,0\n,0\n,0\n,0\n,0\n,0.7\n,0\n,1\n,0\n,0\n,0\n,0\n,-0.2\n,0\n,0\n,0.6\n,0.1\n,0\n,0.6\n,0.3\n,0\n,0.7\n,0.7\n,0\n,0\n,0\n,0\n,0\n,0\n,0\n,0\n,0.4\n,0\n,0.6\n,0\n,1\n,0.6\n,0\n,0\n,1\n,0.4\n,0.2\n,-1\n,0.8\n,-1\n,0\n,1\n,0\n,0.9\n,0.7\n,-0.3\n,0\n,0.2\n,0\n,0\n,0])","d0891370":"X = np.array(matrix) * grade1","55d1c222":"# import Logistic model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\n\nclf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X, y_train)\nclf.score(X, y_train)","2beee4dd":"np.unique(clf.predict(X))","537e1e36":"#Import Library of Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\n\ngnb.fit(X, y_train)\ngnb.score(X,y_train)","d4d5b5b2":"np.unique(gnb.predict(X))","c8f57701":"from sklearn.ensemble import RandomForestClassifier as RFClass\nmodel_rf = RFClass(n_estimators = 100, max_depth=5, random_state=2019)\nmodel_rf.fit(X, y_train)\nmodel_rf.score(X, y_train)","0efcf0f4":"np.unique(gnb.predict(X))","1f9db4ea":"texts = '\\n'.join(rTest.tolist())\n#cut_text = jieba.lcut(texts)\ncut_text = \"\".join(jieba.cut(texts))\ncut_text = re.sub(pattern = temp, repl = \"\", string = cut_text)\n\nkeyword = jieba.analyse.extract_tags(cut_text, topK=100, allowPOS=('a','e','n','nr','ns'))  # list\ncut_text = cut_text.split('\\n')\nkeyword","27b0830b":"cutlist = []\n\nfor i in range(0, len(cut_text)):\n    cut_dic = defaultdict(int) \n    comment = cut_text[i]\n    comment_cut = jieba.lcut(comment)\n    for word in comment_cut: # word freq for every comment \n        if word in keyword:\n            cut_dic[word] += 1  \n    order = sorted(cut_dic.items(),key = lambda x:x[1],reverse = True) # word freq in descending order\n    #print(order)\n \n    myresult = \"\" \n    for j in range(0,len(order)): \n        result = order[j][0]+ \"-\" + str(order[j][1])\n        myresult = myresult + \" \" + result  \n    cutlist.append(myresult)\n#print(cutlist)","0e113554":"word_freqs = []\nfor raw in cutlist:\n    word_freq = {}\n    for word_freq_raw in raw.split():\n        index = word_freq_raw.find('-')\n        word = word_freq_raw[:index]\n        freq = int(word_freq_raw[index + 1])\n        word_freq[word] = freq\n    word_freqs.append(word_freq)\n    \nmatrix = []\nfor word_freq in word_freqs:\n    row = []\n    for word in keyword:\n        if word in word_freq:\n            row.append(word_freq[word])\n        else:\n            row.append(0)\n    matrix.append(row)\n#print(matrix)\nmatrix = np.array(matrix)","752bac43":"grade2 = np.array([0.1\n,0\n,0\n,0.7\n,0.3\n,0\n,0\n,0.8\n,0.5\n,0\n,0.1\n,0.1\n,0\n,0\n,1\n,-1\n,0\n,0\n,0\n,0.4\n,0\n,0.6\n,0\n,0.6\n,0\n,0\n,1\n,0\n,0.8\n,-0.1\n,0\n,0\n,0.4\n,0\n,0\n,0\n,0.6\n,0.6\n,-0.4\n,0\n,0\n,0\n,0\n,0\n,0.4\n,1\n,-0.6\n,0\n,-0.7\n,0.9\n,-1\n,0.4\n,0.1\n,-0.2\n,-0.3\n,0.6\n,0\n,0.2\n,0\n,0\n,0\n,0\n,0.2\n,0\n,0.6\n,0\n,0.5\n,-1\n,0\n,0\n,0.9\n,0\n,0\n,-0.6\n,0.1\n,0\n,0.4\n,-0.8\n,0\n,0\n,-0.3\n,0\n,0.7\n,0.5\n,0\n,0.8\n,0\n,0\n,0\n,0\n,-0.2\n,0.6\n,0.5\n,0.7\n,0\n,0\n,0.8\n,0.5\n,0.7\n,-0.4])","0de984f1":"xTest = np.array(matrix) * grade2\nxTest.shape","9566b4cc":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.show()\n\nnp.set_printoptions(precision = 2)","543c9891":"clf_proba = clf.predict_proba(xTest)   # predict probability \nclf_pred = clf.predict(xTest)   # prediction result\nclf.score(xTest, y_test)","62609d3b":"clf_cm = skm_conf_mat(y_test, clf_pred)\nplot_confusion_matrix(clf_cm, classes = list(sorted(y_train.unique())), title = 'Confusion Matrix')","3c3e6f05":"clfcv = LogisticRegressionCV(cv=5, random_state=0, multi_class='multinomial').fit(X, y_train)\nclfcv.score(X, y_train)","eb84f8ad":"clfcv_proba = clfcv.predict_proba(xTest)\nclfcv_pred = clfcv.predict(xTest)\nclfcv.score(xTest, y_test)","1253240e":"clfcv_cm = skm_conf_mat(y_test, clf_pred)\nplot_confusion_matrix(clfcv_cm, classes = list(sorted(datas['score'].unique())), title = 'Confusion Matrix')","75dfe0c7":"rf_proba = model_rf.predict_proba(xTest)\nrf_pred = model_rf.predict(xTest)\nmodel_rf.score(xTest, y_test)","c65885e9":"# Tree Plot\nfrom graphviz import Source\nfrom sklearn import tree as treemodule\nSource(treemodule.export_graphviz(\n        model_rf.estimators_[1]\n        , out_file=None\n        , filled = True\n        , proportion = True #@@ try False and understand the differences\n        )\n)","8d8c8429":"rf_cm = skm_conf_mat(y_test, rf_pred)\nplot_confusion_matrix(rf_cm, classes = list(sorted(datas['score'].unique())), title = 'Confusion Matrix')","e2f256be":"rf_pred = pd.DataFrame(rf_pred)\nrf_pred.to_csv(\"Predictions on Ratings.csv\")","dd5dc089":"#score = (new_data.score == 2)|(new_data.score == 6)\nnew_data.loc[new_data.score == 6, 'score'] = 4\nnew_data.loc[new_data.score == 2, 'score'] = 4","a84623a5":"features = new_data['content']\nlabels = new_data['score']\n\nnew_data['score'].value_counts()","f3d5ccdb":"rTrain, rTest, y_train, y_test = train_test_split(features, labels, test_size = 0.3, random_state=42)\n# let's understand up a bit the data\n## print out the shapes of  resultant feature data\nprint(\"\\t\\t\\tFeature Shapes:\")\nprint(\"Train set: \\t\\t{}\".format(rTrain.shape), \n      #\"\\nValidation set: \\t{}\".format(rValidation.shape),\n      \"\\nTest set: \\t\\t{}\".format(rTest.shape))","d7ce3462":"texts = '\\n'.join(rTrain.tolist())\n#cut_text = jieba.lcut(texts)\ncut_text = \"\".join(jieba.cut(texts))\ncut_text = re.sub(pattern = temp, repl = \"\", string = cut_text)\n\nkeyword = jieba.analyse.extract_tags(cut_text, topK=100, allowPOS=('a','e','n','nr','ns'))  # list\ncut_text = cut_text.split('\\n')\nkeyword","276f0bae":"cutlist = []\n\nfor i in range(0, len(cut_text)):\n    cut_dic = defaultdict(int) \n    comment = cut_text[i]\n    comment_cut = jieba.lcut(comment)\n    for word in comment_cut: # word freq for every comment \n        if word in keyword:\n            cut_dic[word] += 1  \n    order = sorted(cut_dic.items(),key = lambda x:x[1],reverse = True) # word freq in descending order\n    #print(order)\n \n    myresult = \"\" \n    for j in range(0,len(order)): \n        result = order[j][0]+ \"-\" + str(order[j][1])\n        myresult = myresult + \" \" + result  \n    cutlist.append(myresult)\n#print(cutlist)","81358d60":"word_freqs = []\nfor raw in cutlist:\n    word_freq = {}\n    for word_freq_raw in raw.split():\n        index = word_freq_raw.find('-')\n        word = word_freq_raw[:index]\n        freq = int(word_freq_raw[index + 1])\n        word_freq[word] = freq\n    word_freqs.append(word_freq)\n    \nmatrix = []\nfor word_freq in word_freqs:\n    row = []\n    for word in keyword:\n        if word in word_freq:\n            row.append(word_freq[word])\n        else:\n            row.append(0)\n    matrix.append(row)\n#print(matrix)\nmatrix = np.array(matrix)","a95afcf5":"X = np.array(matrix) * grade1","2e964e00":"texts = '\\n'.join(rTest.tolist())\n#cut_text = jieba.lcut(texts)\ncut_text = \"\".join(jieba.cut(texts))\ncut_text = re.sub(pattern = temp, repl = \"\", string = cut_text)\n\nkeyword = jieba.analyse.extract_tags(cut_text, topK=100, allowPOS=('a','e','n','nr','ns'))  # list\ncut_text = cut_text.split('\\n')\nkeyword","47e72e75":"cutlist = []\n\nfor i in range(0, len(cut_text)):\n    cut_dic = defaultdict(int) \n    comment = cut_text[i]\n    comment_cut = jieba.lcut(comment)\n    for word in comment_cut: # word freq for every comment \n        if word in keyword:\n            cut_dic[word] += 1  \n    order = sorted(cut_dic.items(),key = lambda x:x[1],reverse = True) # word freq in descending order\n    #print(order)\n \n    myresult = \"\" \n    for j in range(0,len(order)): \n        result = order[j][0]+ \"-\" + str(order[j][1])\n        myresult = myresult + \" \" + result  \n    cutlist.append(myresult)\n#print(cutlist)","f08b04fa":"word_freqs = []\nfor raw in cutlist:\n    word_freq = {}\n    for word_freq_raw in raw.split():\n        index = word_freq_raw.find('-')\n        word = word_freq_raw[:index]\n        freq = int(word_freq_raw[index + 1])\n        word_freq[word] = freq\n    word_freqs.append(word_freq)\n    \nmatrix = []\nfor word_freq in word_freqs:\n    row = []\n    for word in keyword:\n        if word in word_freq:\n            row.append(word_freq[word])\n        else:\n            row.append(0)\n    matrix.append(row)\n#print(matrix)\nmatrix = np.array(matrix)","cbb6436d":"xTest = np.array(matrix) * grade2\nxTest.shape","b487fb7b":"clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X, y_train)\nclf.score(xTest, y_test)","dff4ff29":"clfcv = LogisticRegressionCV(cv=5, random_state=0, multi_class='multinomial').fit(X, y_train)\nclfcv.score(xTest, y_test)","fd9e8fc9":"gnb.fit(X, y_train)\ngnb.score(xTest, y_test)","6bbfeb2b":"model_rf.fit(X, y_train)\nprint(model_rf.score(X, y_train))\nprint(model_rf.score(xTest, y_test))","61450f4d":"#### cannot open Word Cloud picture ","d386bae9":"## 3. Model Selection\n> Import rTest","13db36b0":"### 0.2 Loading the Dataset","26df6c85":"### 2.3 Random Forests ","a4164c43":"### Cross Validation","9383408d":"### 0.1 Import Libraries ","8d369295":"### 1.2. Data Cleaning ","da569b55":"## 1. Exploratory Data Analysis \n### 1.1. Data Exploration ","6861f689":"* ## 3 Model Selection REVISED\n    * ### 3.1 Group Ratings by very high(10), high(8), and others(2-6) TO (4) ","ac502a39":"### RF","4ba25781":"* rTrain","e8d101f1":"> Until now, we attempted multiple approaches to improve the accuracy rate of predicting corresponding scores on sentiment analysis. \n\n> 1. Importing different sets of sentiment weights\n> 2. Lowering score dimensions to [4,8,10] rather [2,4,6,8,10] ","e59e0928":"* rTest ","cb1e7e2f":"## 4. Prediction Print on RF","5f18a8af":"* Name: SP Tian \n* Date: May 5, 2019 \n\n#            Sentiment Analysis on Anime Reviews \n\n* ## 0 Introduction \n    * ### 0.1 Import Libraries\n    * ### 0.2 Loading the Database \n\n* ## 1 Exploratory Data Analysis \n    * ### 1.1 Data Exploration \n        * #### 1.1.1 Rating Frequency Table\n        * #### 1.1.2 Word Cloud \n    * ### 1.2 Data Cleaning \n    * ### 1.3 Data Split - only get rTrain\n    * ### 1.4 Word Frequency Table \n    * ### 1.5  Import Sentiment Weights \n\n* ## 2 Train Models \n    * ### 2.1 Logistic Regression \n    * ### 2.2 Gaussian Naive Bayes \n    * ### 2.3 Random Forests \n\n* ## 3 Model Selection \n        * Cross Validatioin \n        * print skm_conf_mat\n\n* ## 4 Prediction on Random Forests ","4686ea13":"## 0. Introduction \n\n* Question: \nHow we can use Sentiment Analysis on comments to further predict viewers' ratings? \n\n* Source: \nA Japanese anime, from Chinese viewing website called bilibili.com, which went IPO in NY Exchange as ticker (BILI). The reviews are scrapped from the website using JSON and till the end of the date of May 6, 2019. \n\n* Deliverables: cvs.file on predicting Ratings \n\n## Note: \nTest data is split half (train\/test) and then 70-30, containing 2258 comments. ","cf34576b":"## 2. Train Model \n### 2.1. Logistic Regression","86122582":"### 1.3. Data Split","1af72531":"### Log prediction","fc21a687":"> function confusion matrix","3898567e":"### 1.4. Frequency Table for Top 100 ","941cff64":"### 2.2 Gaussian Naive Bayes","05004667":"### 1.5. Import Sentiment Weights \n> X_rTrain","62a622bb":"#### 1.1.1 Rating Frequency Table"}}