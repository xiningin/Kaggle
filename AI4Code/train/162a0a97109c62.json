{"cell_type":{"3ce7c138":"code","ecb858b1":"code","4dcbf265":"code","486084d5":"code","d8437a7a":"code","62e44779":"code","6ec2715f":"code","61c06435":"code","0c17e9a9":"code","63c3ee26":"code","83213828":"code","8caf0ccf":"code","34880448":"code","8f9d7018":"markdown","e863037a":"markdown","2118621b":"markdown","5479ef13":"markdown","922578aa":"markdown","9b5a653c":"markdown","80e0bc4c":"markdown","64a5316d":"markdown","a12d1761":"markdown","21900a3f":"markdown","6084cf13":"markdown","1f00ecb2":"markdown","43096f87":"markdown","c04246a7":"markdown","fc5311d6":"markdown","d57f8a82":"markdown"},"source":{"3ce7c138":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ecb858b1":"import nltk\nimport re\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer","4dcbf265":"data=pd.read_csv('\/kaggle\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv')\ndata.head()","486084d5":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ndata['Category']=le.fit_transform(data['Category'])\ndata.head()","d8437a7a":"wordnet=WordNetLemmatizer()\ncorpus=[]\nfor i in range(0,len(data)):\n    review = re.sub(r'https?:\/\/\\S+|www.\\S+', '', data[\"Message\"][i])\n    review = re.sub(r'<.*?>', '', review)\n    review = re.sub(r'[^a-zA-Z]+', ' ', review)\n    review = re.sub(r'[0-9]', '', review)\n    review=review.lower()\n    review=review.split()\n    review=[str(wordnet.lemmatize(word)) for word in review if not word in stopwords.words('english')]\n    review=' '.join(review)\n    corpus.append(review)","62e44779":"y=data[\"Category\"]\nX=pd.DataFrame(corpus,columns=['text'])","6ec2715f":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)","61c06435":"from sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer(max_features=3000)\ntrain_X=cv.fit_transform(X_train['text']).toarray()\ntest_X=cv.transform(X_test['text']).toarray()\ntrain_X","0c17e9a9":"print(train_X.shape)\nprint(test_X.shape)\nprint(y_train.shape)\nprint(y_test.shape)","63c3ee26":"from sklearn.naive_bayes import MultinomialNB\nnb=MultinomialNB().fit(train_X,y_train)\ny_pred_nb=nb.predict(test_X)","83213828":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred_nb)\nsns.heatmap(cm,cmap='BuPu',annot=True,fmt='d')","8caf0ccf":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier(random_state=0).fit(train_X,y_train)\ny_pred_rfc=rfc.predict(test_X)\ncm=confusion_matrix(y_test,y_pred_rfc)\nsns.heatmap(cm,cmap='BuPu',annot=True,fmt='d')","34880448":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nprint('******** Naive Bayes Classifier **********')\nprint('Precision Score: ',precision_score(y_test,y_pred_nb))\nprint('Recall Score: ',recall_score(y_test,y_pred_nb))\nprint('****************************************')\nprint('******** Random Forest Classifier **********')\nprint('Precision Score: ',precision_score(y_test,y_pred_rfc))\nprint('Recall Score: ',recall_score(y_test,y_pred_rfc))","8f9d7018":"Message- The text message which is to be categorized\n\nCategory- The classification whether the text message is a spam or not spam (Lol, Ham!)\n\nBut for a Machine Learning algorithm, it will not be easy to demarcate the statements as Spam or Ham- instead- our target would be to assign if the message is a spam or not. So let us apply a preprocessing- via which the two categories are marked by 0 and 1- which will in turn help the algorithm to best identify the inputs","e863037a":"**NLTK** is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.","2118621b":"**Importing the libraries**\n\nNumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\n\nPandas is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. It is free software released under the three-clause BSD license","5479ef13":"**Importing the Random Forest Classifier**","922578aa":"Use the Count Vectorizer module to pre-processing text values.","9b5a653c":"Now we shall be distributing our data into two parts:\n\n* Train Set- The Train Set is the data that the model is trained with (or in other words, the data from which the model learns)\n*  Test Set- The Test Set is the data which the model has to see, and predict the output.\n\nThe sklearn library of python provides an amazing option- Train_test_split","80e0bc4c":"Naive Bayes Algorithm\n\n\nIn the next step, we shall be incorporating the Naive Bayes Classifier and training it on the Train Data. Once trained, it will be used to predict the outputs from the test data set","64a5316d":"**Observations**:\n\nUsing the Random Forest Classifier:\n\n* 955 Non-Spam mails have been correctly classified\n* 141 Spam mails have been correctly classified\n* 0 Non-Spam mails have been classified as Spam mails (False Positives or Type I Error)\n* 19 Spam mails have been classified as Non-Spam (False Negatives or Type II Error)","a12d1761":"* Precision of Random Forest Classifier > Precision of Naive Bayes Classifier\n* Recall of Naive Bayes Classifier > Recall of Random Forest Classifier","21900a3f":"**Observations**:\n\nIts now time that we analyze the results:-\n\n* 948 Non-Spam mails have been correctly classified\n* 149 Spam mails have been correctly classified\n* 7 Non-Spam mails have been classified as Spam mails (False Positives or Type I Error)\n* 11 Spam mails have been classified as Non-Spam (False Negatives or Type II Error)","6084cf13":"The best approach in order to undertsand and estimate this is by evaluating the metrics. Usually the metrics to evaluate the performance of a Classification Problem are- Precision and Recall","1f00ecb2":"Separating the dependent and independent variables:\n\nX-> Independent Variables- The Basis in which the outputs have to be calculated\nX->Dependent Variable-The output being calculated from the inputs\nGenerally, X and y are the standard norms that we use in order to depict the Independent and Dependent variables repectively. However, it is completely the coder choice as to which will be the option you wish to choose.","43096f87":"**Cleaning the message**\nThe texts we recieve have multiple factors that are unwanted and unhelpful. providing that data to the machine learning algorthim will make it just suffer. Instead lets take on some approach to clean up the text as much as possible.\n\n**Lemmatization**\n\nLemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.","c04246a7":"Reading the dataset and display first 5 values:","fc5311d6":"A **confusion matrix** is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. ","d57f8a82":"**SMS Spam Classification** using:\n* Naive Bayes\n* Random Forest"}}