{"cell_type":{"d3edc953":"code","a4f02af5":"code","be0013b4":"code","31b5beb9":"code","78f732b7":"code","f55cd830":"code","05b0faa0":"code","b5f0bdab":"code","d3c132a8":"code","a7006723":"code","66b5f258":"code","3511b2fb":"code","f7eadf88":"code","4e192635":"code","253d2cbb":"code","f531a8ba":"code","718e372e":"code","a3f660b9":"markdown"},"source":{"d3edc953":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a4f02af5":"import time\nimport copy\nimport glob\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport matplotlib.pyplot as plt","be0013b4":"data_path = '\/kaggle\/input\/waste-classification-data\/DATASET\/TRAIN\/'\ntransform = transforms.Compose(\n                [\n#                     transforms.Resize([256, 256]),\n                    transforms.RandomResizedCrop(224),\n                    transforms.RandomHorizontalFlip(),\n                    transforms.ToTensor(),\n                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n                ])\ntrain_dataset = torchvision.datasets.ImageFolder(\n    root=data_path,\n    transform=transform\n)\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=32,\n    num_workers=8,\n    shuffle=True\n)\n\nprint(len(train_loader))","31b5beb9":"batch = next(iter(train_loader))\nprint(batch[0].shape)\nplt.imshow(batch[0][0].permute(1, 2, 0))\nprint(batch[1][0])\n","78f732b7":"resnet18 = models.resnet18(pretrained=True)","f55cd830":"print(resnet18)","05b0faa0":"def set_parameter_requires_grad(model, feature_extracting=True):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False\n            \nset_parameter_requires_grad(resnet18)","b5f0bdab":"# Initialize new output layer\nresnet18.fc = nn.Linear(512, 2)","d3c132a8":"# Check which layer in the model that will compute the gradient\nfor name, param in resnet18.named_parameters():\n    if param.requires_grad:\n        print(name, param.data)","a7006723":"def train_model(model, dataloaders, criterion, optimizer, device, num_epochs=25, is_train=True):\n    since = time.time()\n    \n    acc_history = []\n    loss_history = []\n\n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        running_loss = 0.0\n        running_corrects = 0\n\n        # Iterate over data.\n        for inputs, labels in dataloaders:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            model.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            _, preds = torch.max(outputs, 1)\n\n            # backward\n            loss.backward()\n            optimizer.step()\n\n            # statistics\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n\n        epoch_loss = running_loss \/ len(dataloaders.dataset)\n        epoch_acc = running_corrects.double() \/ len(dataloaders.dataset)\n\n        print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n\n        if epoch_acc > best_acc:\n            best_acc = epoch_acc\n\n        acc_history.append(epoch_acc.item())\n        loss_history.append(epoch_loss)\n        \n        torch.save(model.state_dict(), os.path.join('\/kaggle\/working\/', '{0:0=2d}.pth'.format(epoch)))\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best Acc: {:4f}'.format(best_acc))\n    \n    return acc_history, loss_history","66b5f258":"# Here we only want to update the gradient for the classifier layer that we initialized.\nparams_to_update = []\nfor name,param in resnet18.named_parameters():\n    if param.requires_grad == True:\n        params_to_update.append(param)\n        print(\"\\t\",name)\n            \noptimizer = optim.Adam(params_to_update)","3511b2fb":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Setup the loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Train model\ntrain_acc_hist, train_loss_hist = train_model(resnet18, train_loader, criterion, optimizer, device)","f7eadf88":"test_path = '\/kaggle\/input\/waste-classification-data\/DATASET\/TEST\/'\ntransform = transforms.Compose(\n                [\n                    transforms.Resize(224),\n                    transforms.CenterCrop(224),\n                    transforms.ToTensor(),\n                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n                ])\ntest_dataset = torchvision.datasets.ImageFolder(\n    root=test_path,\n    transform=transform\n)\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=32,\n    num_workers=1,\n    shuffle=False\n)\n\nprint(len(test_loader))","4e192635":"def eval_model(model, dataloaders, device):\n    since = time.time()\n    \n    acc_history = []\n    best_acc = 0.0\n\n    saved_models = glob.glob('\/kaggle\/working\/' + '*.pth')\n    saved_models.sort()\n    print('saved_model', saved_models)\n\n    for model_path in saved_models:\n        print('Loading model', model_path)\n\n        model.load_state_dict(torch.load(model_path))\n        model.eval()\n        model.to(device)\n\n        running_corrects = 0\n\n        # Iterate over data.\n        for inputs, labels in dataloaders:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            with torch.no_grad():\n                outputs = model(inputs)\n\n            _, preds = torch.max(outputs, 1)\n            running_corrects += torch.sum(preds == labels.data)\n\n        epoch_acc = running_corrects.double() \/ len(dataloaders.dataset)\n\n        print('Acc: {:.4f}'.format(epoch_acc))\n        \n        if epoch_acc > best_acc:\n            best_acc = epoch_acc\n\n        acc_history.append(epoch_acc.item())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Validation complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best Acc: {:4f}'.format(best_acc))\n    \n    return acc_history","253d2cbb":"val_acc_hist = eval_model(resnet18, test_loader, device)","f531a8ba":"plt.plot(train_acc_hist)\nplt.plot(val_acc_hist)\nplt.show()","718e372e":"plt.plot(train_loss_hist)\nplt.show()","a3f660b9":"# # Feature Extracting a Pretrained Model\n\nSince this pretrained model is trained on ImageNet dataset, the output layers has 1000 nodes. We want to reshape this last classifier layer to fit this dataset which has 2 classes. Furthermore, in feature extracting, we don't need to calculate gradient for any layers except the last layer that we initialize. For this we need to set `.requires_grad` to `False`"}}