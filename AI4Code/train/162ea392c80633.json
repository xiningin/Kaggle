{"cell_type":{"95019083":"code","7a2d3bae":"code","a0c94b2f":"code","7564ff60":"code","834e4ff1":"code","0e2b1c0c":"code","3e8bb683":"code","6a342d8c":"code","a0c39003":"code","a4a11633":"code","91662f61":"code","1d6ff0b1":"code","8b21c27a":"code","1160fb73":"code","76a41f66":"code","08864217":"code","8cc60554":"code","bd5526ed":"code","b1b0ba47":"code","845650e5":"code","8e96fb92":"markdown","7d8e5047":"markdown","227539b4":"markdown","1b2b803a":"markdown","6dbed1f6":"markdown","9ed33517":"markdown","7ce8c62f":"markdown","be5f8b8a":"markdown","d4e55fa4":"markdown","323b96c7":"markdown","83f3bbd1":"markdown","bb63e858":"markdown","8f158e2b":"markdown","0078d5ea":"markdown","0a27fcfa":"markdown","0f2a2ada":"markdown","2830a331":"markdown","428a8372":"markdown"},"source":{"95019083":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n!pip install gensim # Gensim is an open-source library for unsupervised topic modeling and natural language processing\nimport nltk\nnltk.download('punkt')\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nimport seaborn as sns \nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n","7a2d3bae":"df=pd.read_csv(\"..\/input\/source-based-news-classification\/news_articles.csv\")\nprint(df.shape)\ndf.head()","a0c94b2f":"#Checking for null values\ndf.isnull().sum()","7564ff60":"#dropping null values\ndf.dropna(inplace=True)","834e4ff1":"#real vs fake\nfig = px.pie(df,names='label',title='Proportion of Real vs. Fake News')\nfig.show()","0e2b1c0c":"sub_tf_df=df.groupby('language').apply(lambda x:x['language'].count()).reset_index(name='Counts')\nfig = px.bar(sub_tf_df, x=\"language\", y=\"Counts\",\n             color='Counts', barmode='group',\n             height=400)\nfig.show()","3e8bb683":"sub_check=df.groupby('type').apply(lambda x:x['type'].count()).reset_index(name='Counts')\nfig=px.bar(sub_check,x='type',y='Counts',color='Counts',title='Count of News Articles by type')\nfig.show()","6a342d8c":"from wordcloud import WordCloud \nwc = WordCloud(background_color=\"black\", max_words=100,\n               max_font_size=256,\n               random_state=42, width=1000, height=1000)\nwc.generate(' '.join(df['text_without_stopwords']))\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","a0c39003":"def get_top_n_words(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n","a4a11633":"common_words = get_top_n_words(df['text_without_stopwords'], 10)\ndf2 = pd.DataFrame(common_words,columns=['word','count'])\ndf2.groupby('word').sum()['count'].sort_values(ascending=False)\nfig=px.bar(df2,x='word',y='count',color='count',title='Top 10 unigrams')\nfig.show()","91662f61":"common_words = get_top_n_bigram(df['text_without_stopwords'], 10)\ndf2 = pd.DataFrame(common_words,columns=['word','count'])\ndf2.groupby('word').sum()['count'].sort_values(ascending=False)\nfig=px.bar(df2,x='word',y='count',color='count',title='Top 10 bigrams')\nfig.show()","1d6ff0b1":"d = df['author'].value_counts().sort_values(ascending=False).head(5)\nd = pd.DataFrame(d)\nd = d.reset_index() # dataframe with top 5 authors\n\n# Plotting\nsns.set()\nplt.figure(figsize=(15,4))\nsns.barplot(x='index', y='author', data=d)\nplt.xlabel(\"\\n Authors\")\nplt.ylabel(\"Number of Articles written\")\nplt.title(\"Top 5 authors\\n\")\nplt.show()","8b21c27a":"d = df[df['label'] == 'Fake']['site_url'].value_counts().sort_values(ascending=False).head(10)\nd = pd.DataFrame(d)\nd = d.reset_index() # dataframe with top 10 fake news site\n\n# Plotting\nsns.set()\nplt.figure(figsize=(25,7))\nsns.barplot(x='index', y='site_url', data=d)\nplt.xlabel(\"\\n site_url\")\nplt.ylabel(\"Number of Articles written\")\nplt.title(\"Top 10 Fake news sites\\n\")\nplt.show()\n","1160fb73":"d = df[df['label'] == 'Real']['site_url'].value_counts().sort_values(ascending=False).head(10)\nd = pd.DataFrame(d)\nd = d.reset_index() # dataframe with top 10 Trustworthy news site\n\n# Plotting\nsns.set()\nplt.figure(figsize=(25,7))\nsns.barplot(x='index', y='site_url', data=d)\nplt.xlabel(\"\\n site_url\")\nplt.ylabel(\"Number of Articles written\")\nplt.title(\"Top 10 Trustworthy news sites\\n\")\nplt.show()","76a41f66":"#Let's reshuffle the dataset\ndf = df.sample(frac = 1)\n\n#taking the features\nfeatures = df[['site_url', 'text_without_stopwords']]\nfeatures.head(5)\n\nfeatures['url_text'] = features[\"site_url\"].astype(str) + \" \" + features[\"text_without_stopwords\"]\nfeatures.drop(['site_url', 'text_without_stopwords'], axis = 1, inplace = True)\n\nfeatures.head()","08864217":"X = features\ny = df['type']\ny = y.tolist()","8cc60554":"#Splitting the dataset and using TF-IDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n\ntfidf_vectorizer = TfidfVectorizer(use_idf = True, stop_words = 'english')\n\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train['url_text'])\nX_test_tfidf = tfidf_vectorizer.transform(X_test['url_text'])","bd5526ed":"tfidf_train = pd.DataFrame(X_train_tfidf.A, columns = tfidf_vectorizer.get_feature_names())","b1b0ba47":"tfidf_train.head()","845650e5":"Adab = AdaBoostClassifier(DecisionTreeClassifier(max_depth=10),n_estimators=5,random_state=1)\nAdab.fit(tfidf_train, y_train)\ny_pred3 = Adab.predict(X_test_tfidf)\nABscore = metrics.accuracy_score(y_test,y_pred3)\nprint(\"accuracy: %0.3f\" %ABscore)","8e96fb92":"here we can se that most news are of type bs (i.e. bullshit)","7d8e5047":"### Visualizing top 5 authors ","227539b4":"Above is the representation of tf-idf matrix","1b2b803a":"### Let's have a look at the top 10 trustworthy news sites","6dbed1f6":"---\n\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 27px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #23527c\">Exploratory data analysis<\/p>","9ed33517":"### Languages of News Articles","7ce8c62f":"As we can see, English is the most common language in which most news articles are written","be5f8b8a":"## **We got a testing accuracy of 0.959! Yay!**","d4e55fa4":"### Let's visualize the proportion of real and fake news!","323b96c7":"### Visualizing count of news articles by type","83f3bbd1":"\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 20px; font-weight: bold; letter-spacing: 2px; text-align: left; color: #23527c\">please upvote if you liked my notebook! :D<\/p>","bb63e858":"<span style=\"font-family: Arials; font-size: 20px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #23527c\">1. INTRODUCTION<\/span>\n<hr style=\"height: 0.5px; border: 0; background-color: 'Black'\">\n\nSocial media is a vast pool of content, and among all the content available for users to access, news is an element that is accessed most frequently. These news can be posted by politicians, news channels, newspaper websites, or even common civilians. These posts have to be checked for their authenticity, since spreading misinformation has been a real concern in today\u2019s times, and many firms are taking steps to make the common people aware of the consequences of spread misinformation. The measure of authenticity of the news posted online cannot be definitively measured, since the manual classification of news is tedious and time-consuming, and is also subject to bias.","8f158e2b":"### Visualizing top 10 unigrams and bigrams","0078d5ea":"### Visualizing top 10 fake news site","0a27fcfa":"<p style=\"font-family: Arials; line-height: 1.3; font-size: 27px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #23527c\">Reading the dataset<\/p>","0f2a2ada":"![](http:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1508152385\/Fake_real-1_gebpwg.png)","2830a331":"<p style=\"font-family: Arials; line-height: 1.3; font-size: 30px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #23527c\">Exploring the news dataset<\/p>\n","428a8372":"---\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 27px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #23527c\">Modelling<\/p>"}}