{"cell_type":{"1d3a09a6":"code","9a693b60":"code","d9f50624":"code","0b76cac2":"code","58adf88a":"code","45c36893":"code","496e2fc9":"code","3c6c2611":"code","1c46ce57":"code","011b2831":"code","79ba1ece":"code","f682ff21":"code","0bc9c308":"code","f8022fef":"code","589a7559":"code","2ef93953":"code","85aea436":"code","eb934b84":"code","33bebd93":"code","dd96030c":"code","1e39dd4a":"code","4a78d5ea":"code","d3fdf205":"code","a1692621":"code","674f6f47":"code","d782d845":"code","b03116c8":"code","240af1b0":"code","02c9088c":"markdown","a87ecd7c":"markdown","7a111718":"markdown","dd27b13a":"markdown","5819c44f":"markdown"},"source":{"1d3a09a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.impute import SimpleImputer, KNNImputer\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom imblearn.under_sampling import RandomUnderSampler\nimport xgboost as xgb\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMModel,LGBMClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV,train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectKBest, f_regression,f_classif,chi2\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score ,mean_squared_error, mean_absolute_error \\\n, f1_score ,precision_score, recall_score ,precision_recall_curve,plot_confusion_matrix \\\n,confusion_matrix,make_scorer,roc_auc_score\nfrom sklearn.preprocessing import MinMaxScaler,PowerTransformer,QuantileTransformer,Normalizer,RobustScaler,StandardScaler,MaxAbsScaler\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a693b60":"data=pd.read_csv(\"..\/input\/iba-ml1-mid-project\/train.csv\")\ndata=data.drop(['Id'], axis=1)\ndata2=pd.read_csv(\"..\/input\/iba-ml1-mid-project\/test.csv\")\ndata2=data2.drop(['Id'], axis=1)\nprint(data.shape)\nprint(data.columns.values)\nprint(data2.shape)\nprint(data2.columns.values)","d9f50624":"# data[\"credit_line_utilization\"] = data[\"credit_line_utilization\"][:10]\n# data[\"credit_line_utilization\"] = pd.to_numeric(data[\"credit_line_utilization\"], downcast=\"float\")\n# data2[\"credit_line_utilization\"] = data2[\"credit_line_utilization\"][:10]\n# data2[\"credit_line_utilization\"] = pd.to_numeric(data2[\"credit_line_utilization\"], downcast=\"float\")","0b76cac2":"\ndata2[\"credit_line_utilization\"] = data2[\"credit_line_utilization\"].str.replace(',', '.')\ndata2[\"credit_line_utilization\"] = data2[\"credit_line_utilization\"].astype(float)\ndata[\"credit_line_utilization\"] = data[\"credit_line_utilization\"].str.replace(',', '.')\ndata[\"credit_line_utilization\"] = data[\"credit_line_utilization\"].astype(float)\n","58adf88a":"# for i in data.columns:\n   \n#     x = data[i].value_counts()\n#     print(\"Column name is:\\n\",i,\"\\nand it value is:\\n\",x)\n#     print() ","45c36893":"data.head(15)","496e2fc9":"data.tail(15)","3c6c2611":"data.describe()","1c46ce57":"X=data.drop(['defaulted_on_loan'], axis=1)\ny=data['defaulted_on_loan']\nrus = RandomUnderSampler(sampling_strategy=1)\nX_rus, y_rus = rus.fit_resample(X, y)\n\nprint(X_rus.shape) \nprint(y_rus.shape) \ndf_concat = pd.concat([X_rus, y_rus], axis=1)\n","011b2831":"# for i in df_concat.columns:\n   \n#     x = df_concat[i].value_counts()\n#     print(\"Column name is:\\n\",i,\"\\nand it value is:\\n\",x)\n#     print() \n","79ba1ece":"X_train, X_test, y_train, y_test = train_test_split(X_rus, y_rus ,shuffle=True,stratify=y_rus)","f682ff21":"def detect_outliers(var):\n    q_1, q_3 = np.percentile(var, [25, 75])\n    IQR = q_3 - q_1\n    lower_bound = q_1 - (IQR * 1.5)\n    upper_bound = q_3 + (IQR * 1.5)\n    return np.asarray((var > upper_bound) | (var < lower_bound)).nonzero()","0bc9c308":"from mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.linear_model import LinearRegression\n# Sequential Forward Selection(sfs)\nsfs = SFS(CatBoostClassifier(iterations=88,learning_rate=0.2,depth=6,logging_level='Silent'),\n          k_features=6,\n          forward=True,\n          floating=False,\n          scoring = 'roc_auc',\n          cv = 5)\nsfs.fit(X_train, y_train)\n","f8022fef":"sfs.k_feature_names_","589a7559":"# ('age',\n#  'number_of_credit_lines',\n# 'monthly_income'\n#  'number_of_credit_lines',\n# 'real_estate_loans',\n# 'ratio_debt_payment_to_income')\n","2ef93953":"imputer=KNNImputer(missing_values=np.nan, n_neighbors=5)\nX_train_i = imputer.fit_transform(X_train)\nX_train = pd.DataFrame(X_train_i, columns = X_train.columns)\n\nX_train = X_train.reset_index(drop=True)\ny_train = y_train.reset_index(drop=True)\n\n\nage_outliers = detect_outliers(X_train['age'])[0]\nndfm_outliers = detect_outliers(X_train['number_dependent_family_members'])[0]\nmonthly_income_outliers = detect_outliers(X_train['monthly_income'])[0]\nnocl_outliers = detect_outliers(X_train['number_of_credit_lines'])[0]\nrel_outliers = detect_outliers(X_train['real_estate_loans'])[0]\nrdpti_outliers = detect_outliers(X_train['ratio_debt_payment_to_income'])[0]\n\noverall_outliers = np.unique(np.concatenate((age_outliers, ndfm_outliers, monthly_income_outliers, nocl_outliers, rel_outliers, rdpti_outliers),axis=None))\noverall_outliers\nX_train = X_train.drop(overall_outliers)\ny_train = y_train.drop(overall_outliers)\n\n\n# X_train.head()\n\n\nprint(X_train.shape) \nprint(X_test.shape) \nprint(y_train.shape)\nprint(y_test.shape)\n","85aea436":"import optuna\n","eb934b84":"    def make_clf2(itere ,lr, dep,k):\n        numeric_transformer = Pipeline(steps=[\n                ('impute', KNNImputer(n_neighbors=6,weights='uniform')),\n                ('scaler', RobustScaler())\n            ])\n\n\n\n\n\n        column_transformer = ColumnTransformer(transformers=[\n                ('numeric', numeric_transformer, ['age',\n     'number_dependent_family_members',\n     'monthly_income',\n     'number_of_credit_lines',\n     'real_estate_loans',\n     'ratio_debt_payment_to_income'])\n         ])\n\n\n        model_pipeline =  Pipeline(steps=[\n                ('preprocessing', column_transformer),\n              #('poly',PolynomialFeatures(2)), \n            ('sl',SelectKBest(score_func=f_classif, k=k)),\n            ('classifier', CatBoostClassifier(iterations=itere,learning_rate=lr,depth=dep,logging_level='Silent'))\n            ])\n        return model_pipeline\n    #model_pipeline.fit(X_train,y_train)\n    #print(f1_score(y_train, model_pipeline.predict(X_train)))\n    #model_pipeline.predict_proba(data2)\n    #print(f1_score(y_test, model_pipeline.predict(X_test)))","33bebd93":"def objective(trial):\n    itere = trial.suggest_int('itere', 72, 99, step=1)\n    lr = trial.suggest_float('lr', 0.1, 0.397, step=0.033)\n    dep = trial.suggest_int('dep', 5, 10, step=1)\n    k =trial.suggest_int('k', 9, 15)\n    \n    \n    clf = make_clf2(itere, lr, dep,k)\n    \n    return cross_val_score(clf,X_rus, y_rus, cv=10, scoring=make_scorer(roc_auc_score)).mean()","dd96030c":"study = optuna.create_study(direction='maximize')","1e39dd4a":"#study.optimize(objective, n_trials=400)","4a78d5ea":"def make_clf(n_nestim,n_nestim2,n_nestim3,a,b,c,d,e,k):\n\n    \n    numeric_preprocessing1 = Pipeline(steps=[\n        ('imputation', KNNImputer(n_neighbors=6)),\n        ('scaling', QuantileTransformer())\n    ])\n    numeric_preprocessing2 = Pipeline(steps=[\n        ('imputation2', KNNImputer(n_neighbors=6)),\n        ('scaling2', QuantileTransformer())\n    ])\n    preprocessing = ColumnTransformer(transformers=[\n     \n        ('numeric', numeric_preprocessing1, [\n 'number_of_credit_lines',\n 'real_estate_loans',\n 'ratio_debt_payment_to_income',\n 'number_of_previous_late_payments_up_to_59_days',\n 'number_of_previous_late_payments_up_to_89_days',\n 'number_of_previous_late_payments_90_days_or_more','credit_line_utilization']),\n    \n            ('numeric2', numeric_preprocessing2, ['age'])\n    ])\n\n    clf = Pipeline(steps=[\n        ('preprocessing', preprocessing),\n        ('sl',SelectKBest(score_func=f_classif, k=k)),\n        ('classifier', VotingClassifier(estimators=[\n        ('knn15', RandomForestClassifier(n_estimators= n_nestim, random_state=100,max_depth=6)), \n        ('knn16', RandomForestClassifier(n_estimators= n_nestim2, random_state=100,max_depth=7)),\n        ('knn17', RandomForestClassifier(n_estimators= n_nestim3, random_state=100,max_depth=8)),  \n        ('classifier1', CatBoostClassifier(iterations=88,learning_rate=0.2,depth=4,logging_level='Silent')),\n\n       ('classifier3', xgb.XGBClassifier(eta=0.1, gamma=0.0, max_depth=4,use_label_encoder=False,subsample=0.2, min_child_weight=2\n                                            ,  alpha=1,objective = \"binary:logistic\",\n               eval_metric = \"logloss\"))   \n     \n    ], voting='soft', weights=[a,b,c,d,e]))\n    ])\n    return clf","d3fdf205":"def objective(trial):\n    n_nestim = trial.suggest_int('n_estimators', 100, 200, step=10)\n    n_nestim2 = trial.suggest_int('n_estimators2', 40, 100, step=5)\n    n_nestim3 = trial.suggest_int('n_estimators3', 5, 40, step=5)\n #   scaling_strategy = trial.suggest_categorical('scaling_strategy', [\n  #      'robust',  'quantile', 'minmax','power','quantile','norm','maxabs','standard'\n   # ])\n   \n    a =trial.suggest_int('a', 0, 6)\n    b =trial.suggest_int('b', 0, 6)\n   \n    c =trial.suggest_int('c', 0, 6)\n    d =trial.suggest_int('d', 0, 6)\n    e =trial.suggest_int('e', 0, 6)\n    k =trial.suggest_int('k', 6, 8)\n\n    clf = make_clf(n_nestim,n_nestim2,n_nestim3,a,b,c,d,e,k)\n    \n    \n    return cross_val_score(clf,X_train, y_train, cv=10, scoring=make_scorer(roc_auc_score)).mean()\n        ","a1692621":"study = optuna.create_study(direction='maximize')","674f6f47":"#study.optimize(objective, n_trials=500)","d782d845":"\nnumeric_preprocessing1 = Pipeline(steps=[\n        ('imputation', KNNImputer(n_neighbors=5)),\n        ('scaling',  QuantileTransformer())\n    ])\nnumeric_preprocessing2 = Pipeline(steps=[\n        ('imputation2', KNNImputer(n_neighbors=6)),\n        ('scaling2',  QuantileTransformer())\n    ])\npreprocessing = ColumnTransformer(transformers=[\n     \n        ('numeric', numeric_preprocessing1, [\n 'number_of_credit_lines',\n 'real_estate_loans',\n 'ratio_debt_payment_to_income',\n 'number_of_previous_late_payments_up_to_59_days',\n 'number_of_previous_late_payments_up_to_89_days',\n 'number_of_previous_late_payments_90_days_or_more','credit_line_utilization']),\n    \n            ('numeric2', numeric_preprocessing2, ['age'])\n    ])\n\nmodel_pipeline4 = Pipeline(steps=[\n        ('preprocessing', preprocessing),\n  \n         \n        ('classifier', VotingClassifier(estimators=[\n \n        ('knn16', RandomForestClassifier(n_estimators= 70, random_state=100,max_depth=7)),\n        ('knn17', RandomForestClassifier(n_estimators= 35, random_state=100,max_depth=8)),  \n        ('classifier1', CatBoostClassifier(iterations=88,learning_rate=0.2,depth=4,logging_level='Silent')),\n       \n       ('classifier3', xgb.XGBClassifier(eta=0.1, gamma=0.0, max_depth=3,use_label_encoder=False,subsample=0.5, min_child_weight=2\n                                            ,  alpha=1,objective = \"binary:logistic\",\n               eval_metric = \"logloss\"))   \n     \n    ], voting='soft', weights=[5,9,7,9]))\n ])\n\nrandom=5\nfor x in range(10):\n    f1_score_y_train=[]\n    f1_score_y_test=[]\n    roc_auc_score_y_train=[]\n    roc_auc_score_y_test=[] \n    \n    X_train, X_test, y_train, y_test = train_test_split(X_rus, y_rus ,shuffle=True,stratify=y_rus,random_state=random)\n    imputer=KNNImputer(missing_values=np.nan, n_neighbors=5)\n    X_train_i = imputer.fit_transform(X_train)\n    X_train = pd.DataFrame(X_train_i, columns = X_train.columns)\n\n    X_train = X_train.reset_index(drop=True)\n    y_train = y_train.reset_index(drop=True)\n\n    random+=5\n    age_outliers = detect_outliers(X_train['age'])[0]\n    ndfm_outliers = detect_outliers(X_train['number_dependent_family_members'])[0]\n    monthly_income_outliers = detect_outliers(X_train['monthly_income'])[0]\n    nocl_outliers = detect_outliers(X_train['number_of_credit_lines'])[0]\n    rel_outliers = detect_outliers(X_train['real_estate_loans'])[0]\n    rdpti_outliers = detect_outliers(X_train['ratio_debt_payment_to_income'])[0]\n   \n    overall_outliers = np.unique(np.concatenate((age_outliers, ndfm_outliers, monthly_income_outliers, nocl_outliers, rel_outliers, rdpti_outliers),axis=None))\n    overall_outliers\n    X_train = X_train.drop(overall_outliers)\n    y_train = y_train.drop(overall_outliers)\n    model_pipeline4.fit(X_train,y_train)\n    f1_score_y_train.append(f1_score(y_train, model_pipeline4.predict(X_train)))\n    f1_score_y_test.append(f1_score(y_test, model_pipeline4.predict(X_test)))\n    roc_auc_score_y_train.append(roc_auc_score(y_train, model_pipeline4.predict(X_train)))\n    roc_auc_score_y_test.append(roc_auc_score(y_test, model_pipeline4.predict(X_test)))\nprint(np.mean(f1_score_y_train))\nprint(np.mean(f1_score_y_test))\nprint(np.mean(roc_auc_score_y_train))\nprint(np.mean(roc_auc_score_y_test))\n","b03116c8":"model_pipeline4.fit(X_train, y_train)\na= model_pipeline4.predict_proba(data2)[:, 1]","240af1b0":"df = pd.DataFrame(a,columns=['Predicted'])\ndf.index += 1\ndf.to_csv('submission888.csv')","02c9088c":"# I used optuna to determine best hyperparameters for model while using it separately, then I used it to combine different models. Note: I tried different imputing techniques on age becouse it was most valueble feature for prediction,so it is separeted from other features.","a87ecd7c":"# Univariate outlier detection for most valueble features that are selected by Sequential Forward Selection.(Feature selection was made before fixing issue with [\"credit_line_utilization\"] so the results are completely different.)","7a111718":"# This commented code above is most essential part to take into account in this notebook.It was written to change [\"credit_line_utilization\"] from String to Float but sadly it made a huge loss of data such that all column [\"credit_line_utilization\"] became NAN. Unfortunately I didnt know about this mistake till the last few hours before deadline, so when I it fixed it and wrote as in code block bellow I was already very tired and the time was running out so model itself and first of all hyperparameters are perfectly suited for data without [\"credit_line_utilization\"], so take it into consideration when some parts of this model seems like useless or more complex than it could be.","dd27b13a":"# Final version of a model ","5819c44f":"0.7833040140732448\n0.77831715210356\n0.7893828778099706\n0.7814066018270664"}}