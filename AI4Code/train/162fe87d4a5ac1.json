{"cell_type":{"3429a4b1":"code","71635445":"code","33d73620":"code","ba972976":"code","5f50db3a":"code","686a4e4c":"code","47108ae7":"code","5bc720e5":"code","dd16fe7b":"code","d0ec3052":"code","be05b04d":"code","357d508c":"code","78d76b0e":"code","7fa7dd17":"code","952d13e9":"code","2030e6f2":"code","2b056258":"code","7b93e718":"code","d4bd3608":"code","ddd626e5":"code","6d97f762":"code","97492a5b":"code","8000775d":"code","a3fc6f36":"code","a4c83aa3":"code","60fd9669":"code","96e46f8a":"code","52fbc3a9":"code","5217025c":"code","9a5ed87c":"code","31fbce17":"code","bbeba915":"code","5c3e0562":"code","2a3ee286":"code","a5c32397":"code","3bb9975a":"code","132ef6c9":"code","507b1ac9":"code","df9b7294":"code","b4614def":"code","df618fd3":"code","465fedc3":"code","2475d7d9":"code","935fa710":"code","809ddfab":"code","d0976a1b":"code","b2ecd7e3":"code","ded7d1de":"code","cdb1eaae":"code","dc0ffc8b":"code","d67736c5":"markdown","83177350":"markdown","e96b52dd":"markdown","d70e4620":"markdown","e90d6718":"markdown","ab998cfb":"markdown","8378f77b":"markdown","123f9950":"markdown","38344844":"markdown","e63001fb":"markdown","71da39cb":"markdown","2eff416e":"markdown","f64ff230":"markdown","e8cc9111":"markdown","b0e7ce8e":"markdown","b129b75e":"markdown","160a93e7":"markdown","06a9d563":"markdown","0c283bcc":"markdown","76592e7a":"markdown"},"source":{"3429a4b1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# It's been created a new folder (train-n-test-data) with duplicates of train and test data because some minor issues with the competition folder\nimport sklearn \nimport os\nprint(os.listdir(\"..\/input\/train-n-test-data\"))\n#The train data\ntrain_data = pd.read_csv(\"..\/input\/train-n-test-data\/train_data.csv\")\ntrain_data.drop('Id',axis = 'columns')\n#As the data Id collumn just refers to a specific email, we need this information since the email will be classified in terms of the features and the target(ham) columns\ntrain_data.ham","71635445":"# The features to be classified \ntest_features = pd.read_csv(\"..\/input\/train-n-test-data\/test_features.csv\")\ntest_features_with_id = test_features\ntest_features = test_features.drop('Id',axis = 'columns')\ntest_features\n","33d73620":"train_data.shape\n\n","ba972976":"train_data['ham'].value_counts()","5f50db3a":"train_data","686a4e4c":"#for special characters let's create the excerpt from the dataset containing the Id, ham and char_freq\nspecial_char = train_data[['char_freq_;','char_freq_(', 'char_freq_[','char_freq_!','char_freq_$','char_freq_#','ham','Id']]\nspecial_char\ntest_special_char = test_features_with_id[['char_freq_;','char_freq_(', 'char_freq_[','char_freq_!','char_freq_$','char_freq_#','Id']]\n\n","47108ae7":"word_freq = train_data.loc[:,\"word_freq_make\":\"word_freq_conference\"]\ntest_word_freq = test_features_with_id.loc[:,\"word_freq_make\":\"word_freq_conference\"]\n#Adding the ham and the id collumn\nword_freq.loc[:,'ham'] = train_data[['ham']]\nword_freq.loc[:,'Id'] = train_data[['Id']]\ntest_word_freq.loc[:,'Id'] = test_features_with_id[['Id']]\nword_freq.shape\ntest_word_freq\n\n","5bc720e5":"word_freq","dd16fe7b":"capital_run = train_data[['capital_run_length_average','capital_run_length_longest','capital_run_length_total','ham','Id']]\ntest_capital_run_with_id = test_features_with_id[['capital_run_length_average','capital_run_length_longest','capital_run_length_total', 'Id']]\ntest_capital_run = test_capital_run_with_id.drop('Id', axis = 'columns')","d0ec3052":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nX_train_data= train_data\nX_train_data = X_train_data.drop('ham',axis = 'columns')\nX_train_data= X_train_data.drop('Id',axis = 'columns')\nY_train_data= train_data.loc[:,'ham':'ham']\n#setting a binary value for ham\nY_train_data = Y_train_data.astype(int)\n# to reset for boolean, just try ~ Y_train_data = Y_train_data.astype(bool)\nY_train_data.ham\nX_train_data","be05b04d":"Scores =[]\nfor i in range (1,40):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    scores = cross_val_score(knn, X_train_data, Y_train_data.ham, cv=10)\n    Scores.append(scores.mean())\nScores\n\n","357d508c":"#But scores is not a pd.DataFrame, so:\nScores_df = pd.DataFrame()\nScores_df['Knn Scores']= Scores\nScores_df\n\n\n\n","78d76b0e":"Scores_df.plot()\n","7fa7dd17":"knn = KNeighborsClassifier(n_neighbors = 1)\nknn.fit(X_train_data, Y_train_data.ham)\nY_test_Pred = knn.predict(test_features)\n","952d13e9":"Y_test_Pred = Y_test_Pred.astype(bool)","2030e6f2":"Predict = pd.DataFrame()\nPredict['Id'] = test_features_with_id['Id']\nPredict['ham'] = Y_test_Pred\nPredict.set_index('Id', inplace=True)\nPredict\n#Predict.drop('', axis = 'columns')","2b056258":"Predict.to_csv(\"Prediction_Knn.csv\")","7b93e718":"from sklearn.naive_bayes import GaussianNB,MultinomialNB,ComplementNB,BernoulliNB\n","d4bd3608":"#For Gaussian Naive Bayes, the classification(clf1) is \n\nclf1 = GaussianNB()\nclf1.fit(X_train_data, Y_train_data.ham)\nGaussianNB(priors=None, var_smoothing=1e-09)\nY_Pred_G_NB = clf1.predict(test_features)\nY_Pred_G_NB = Y_Pred_G_NB.astype(bool)\nY_Pred_G_NB\n\n","ddd626e5":"Pred_G_NB = pd.DataFrame()\nPred_G_NB['Id'] = test_features_with_id['Id']\nPred_G_NB['ham'] = Y_Pred_G_NB\nPred_G_NB.set_index('Id', inplace=True)\nPred_G_NB","6d97f762":"Pred_G_NB.to_csv(\"Prediction_GaussianNB.csv\")","97492a5b":"#For the Multinomial Naive Bayes Classifier (clf2)\nclf2 = MultinomialNB()\nclf2.fit(X_train_data, Y_train_data.ham)\nMultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\nY_Pred_M_NB = clf2.predict(test_features)\nY_Pred_M_NB = Y_Pred_M_NB.astype(bool)\nY_Pred_M_NB\n","8000775d":"Pred_M_NB = pd.DataFrame()\nPred_M_NB['Id'] = test_features_with_id['Id']\nPred_M_NB['ham'] = Y_Pred_M_NB\nPred_M_NB.set_index('Id', inplace=True)\nPred_M_NB","a3fc6f36":"Pred_M_NB.to_csv(\"Prediction_MultinomialNB.csv\")","a4c83aa3":"#For the ComplementNB, the clf3 (now when can make a little more compact)\n  \nclf3 = ComplementNB()\nclf3.fit(X_train_data, Y_train_data.ham)\nComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\nY_Pred_C_NB = clf3.predict(test_features)\nY_Pred_C_NB = Y_Pred_C_NB.astype(bool)\nPred_C_NB = pd.DataFrame()\nPred_C_NB['Id'] = test_features_with_id['Id']\nPred_C_NB['ham'] = Y_Pred_C_NB\nPred_C_NB.set_index('Id', inplace=True)\nPred_C_NB.to_csv(\"Prediction_ComplementNB.csv\")\nPred_C_NB","60fd9669":"#And finally, the bernoulli NB classifier :\n\nclf4 = BernoulliNB()\nclf4.fit(X_train_data, Y_train_data.ham)\nBernoulliNB(alpha=0.5, binarize=0.0, fit_prior=True)\nY_Pred_B_NB = clf4.predict(test_features)\nY_Pred_B_NB = Y_Pred_B_NB.astype(bool)\nPred_B_NB = pd.DataFrame()\nPred_B_NB['Id'] = test_features_with_id['Id']\nPred_B_NB['ham'] = Y_Pred_B_NB\nPred_B_NB.set_index('Id', inplace=True)\nPred_B_NB.to_csv(\"Prediction_BernoulliNB.csv\")\nPred_B_NB","96e46f8a":"def KNN(n,train_data,x_test,doc_name):\n    x_train_data = train_data.drop('ham',axis = 'columns')\n    x_train_data= x_train_data.drop('Id',axis = 'columns')\n    y_train_data= train_data.loc[:,'ham':'ham']\n    x_test_with_id = x_test\n    x_test = x_test.drop('Id',axis = 'columns')\n    #setting a binary value for ham\n    y_train_data = y_train_data.astype(int)\n    # to reset for boolean, just try ~ Y_train_data = Y_train_data.astype(bool)\n    knn = KNeighborsClassifier(n_neighbors = 1)\n    knn.fit(x_train_data, y_train_data.ham)\n    y_test_Pred = knn.predict(x_test)\n    y_test_Pred = y_test_Pred.astype(bool)\n    Predict = pd.DataFrame()\n    Predict['Id'] = x_test_with_id['Id']\n    Predict['ham'] = y_test_Pred\n    Predict.set_index('Id', inplace=True)\n    Predict.to_csv(doc_name)\n\n    \n\n","52fbc3a9":"#now using the function ...\n\nKNN(1,word_freq,test_word_freq,\"KNN_word_freq.csv\")","5217025c":"#for special characters...\nKNN(1,special_char,test_special_char,\"KNN_special_char.csv\")\n","9a5ed87c":"#Now, summarizing the four types of naive bayes used above\ndef Gaussian_NB(train_dt,test_feat,name,priors,smooth):\n    train_dt = train_dt.drop('Id', axis = 'columns')\n    test_feat = test_feat.drop('Id', axis = 'columns')\n    y_train_dt = train_dt.ham\n    x_train_dt = train_dt.drop('ham', axis ='columns')\n    clf = GaussianNB()\n    clf.fit(x_train_dt,y_train_dt)\n    GaussianNB(priors=priors, var_smoothing=smooth) #None, 1e-09\n    Y_Pred_GNB = clf.predict(test_feat)\n    Y_Pred_GNB = Y_Pred_GNB.astype(bool)\n    Y_Pred_GNB\n    Pred_GNB = pd.DataFrame()\n    Pred_GNB['Id'] = test_features_with_id['Id']\n    Pred_GNB['ham'] = Y_Pred_GNB\n    Pred_GNB.set_index('Id', inplace=True)\n    print(Pred_GNB)\n    Pred_GNB.to_csv(name)\ndef  Multinomial_NB(train_dt,test_feat,name,smooth,class_prior, fit_prior):\n    train_dt = train_dt.drop('Id', axis = 'columns')\n    test_feat = test_feat.drop('Id', axis = 'columns')\n    y_train_dt = train_dt.ham\n    x_train_dt = train_dt.drop('ham', axis ='columns')\n    clf = MultinomialNB()\n    clf.fit(x_train_dt,y_train_dt)\n    MultinomialNB(alpha=smooth, class_prior=class_prior, fit_prior=fit_prior) # Default = (1.0,None, True)\n    Y_Pred_MNB = clf.predict(test_feat)\n    Y_Pred_MNB = Y_Pred_MNB.astype(bool)\n    Y_Pred_MNB\n    Pred_MNB = pd.DataFrame()\n    Pred_MNB['Id'] = test_features_with_id['Id']\n    Pred_MNB['ham'] = Y_Pred_MNB\n    Pred_MNB.set_index('Id', inplace=True)\n    print(Pred_MNB)\n    Pred_MNB.to_csv(name)\ndef  Complement_NB(train_dt,test_feat,name,smooth,class_prior,fit_prior,norm):\n    train_dt = train_dt.drop('Id', axis = 'columns')\n    test_feat = test_feat.drop('Id', axis = 'columns')\n    y_train_dt = train_dt.ham\n    x_train_dt = train_dt.drop('ham', axis ='columns')\n    clf = ComplementNB()\n    clf.fit(x_train_dt,y_train_dt)\n    ComplementNB(alpha=smooth, class_prior=class_prior, fit_prior=fit_prior, norm=norm) #Default = (1.0,None, True,False)\n    Y_Pred_CNB = clf.predict(test_feat)\n    Y_Pred_CNB = Y_Pred_CNB.astype(bool)\n    Y_Pred_CNB\n    Pred_CNB = pd.DataFrame()\n    Pred_CNB['Id'] = test_features_with_id['Id']\n    Pred_CNB['ham'] = Y_Pred_CNB\n    Pred_CNB.set_index('Id', inplace=True)\n    print(Pred_CNB)\n    Pred_CNB.to_csv(name)\ndef  Bernoulli_NB(train_dt,test_feat,name,smooth,b,fit_prior):\n    train_dt = train_dt.drop('Id', axis = 'columns')\n    test_feat = test_feat.drop('Id', axis = 'columns')\n    y_train_dt = train_dt.ham\n    x_train_dt = train_dt.drop('ham', axis ='columns')\n    clf = BernoulliNB()\n    clf.fit(x_train_dt,y_train_dt)\n    BernoulliNB(alpha=smooth, binarize=b, fit_prior=fit_prior) # Default (1.0,0.0,True)\n    Y_Pred_BNB = clf.predict(test_feat)\n    Y_Pred_BNB = Y_Pred_BNB.astype(bool)\n    Y_Pred_BNB\n    Pred_BNB = pd.DataFrame()\n    Pred_BNB['Id'] = test_features_with_id['Id']\n    Pred_BNB['ham'] = Y_Pred_BNB\n    Pred_BNB.set_index('Id', inplace=True)\n    print(Pred_BNB)\n    Pred_BNB.to_csv(name)\n    \n\n","31fbce17":"word_freq\n","bbeba915":"Gaussian_NB(word_freq,test_word_freq,\"GaussianNB_word_freq.csv\",None, 1e-09)\n#0.74871","5c3e0562":"Multinomial_NB(word_freq,test_word_freq,\"MultinomialNB_word_freq.csv\",1.0,None, True)\n#0.83815\n\n","2a3ee286":"Complement_NB(word_freq,test_word_freq,\"ComplementNB_word_freq.csv\",1.0,None, True,False)\n#0.82773","a5c32397":"Bernoulli_NB(word_freq,test_word_freq,\"BernoulliNB_word_freq.csv\",1.0,0.0,True)\n#0.92374\n","3bb9975a":"Gaussian_NB(special_char, test_special_char, \"GaussianNB_special_char.csv\",None, 1e-09)\n#0.91857","132ef6c9":"Multinomial_NB(special_char, test_special_char, \"MultinomialNB_special_char.csv\",1.0,None, True)\n#0.93577","507b1ac9":"Complement_NB(special_char, test_special_char, \"ComplementNB_special_char.csv\",1.0,None, True,False)\n#0.89619","df9b7294":"Bernoulli_NB(special_char, test_special_char, \"BernoulliNB_special_char.csv\",1.0,0.0,True)\n#0.92638","b4614def":"Bernoulli_NB(capital_run, test_capital_run_with_id, \"BernoulliNB_capital_run.csv\",1.0,0.0,True)","df618fd3":"#I thought that changes in the smoothing parameter and the fit_prior would afect the classification but result remains the same\n#Still, the code used to test is presented below \nx=0.0\ny=str(0)+str(int(x*10))+ \"BNB\"\nname = y + (\".csv\")\nprint(name)","465fedc3":"for i in range(0,5):\n    x=i\/5\n    y=str(0)+str(int(x*10))+ \"BNB\"\n    name = y +(\".csv\")\n    Bernoulli_NB(train_data, test_features_with_id, name,x,0.0,False)","2475d7d9":"from matplotlib import pyplot as plt\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.metrics import fbeta_score, make_scorer\nfrom sklearn.metrics import roc_curve, auc\ntrain_w = train_data.drop('ham', axis = 'columns')\ntrain_w = train_data.drop('Id', axis = 'columns')\nf3 = make_scorer(fbeta_score, beta=3)\ntrain_f3 = cross_val_score(BernoulliNB(), train_w, train_data.ham, cv=10, scoring = f3 )\ntrain_f3.mean()","935fa710":"capital_run_w = capital_run.drop('ham', axis = 'columns')\ncapital_run_w  = capital_run.drop('Id', axis = 'columns')\nf3 = make_scorer(fbeta_score, beta=3)\ntrain_f3 = cross_val_score(BernoulliNB(), capital_run_w, capital_run.ham, cv=10, scoring = f3 )\ntrain_f3.mean()","809ddfab":"prob = cross_val_predict(BernoulliNB(), train_w, train_data.ham, cv=10, method = 'predict_proba')\nfpr, tpr ,thresholds =roc_curve(train_data.ham,prob[:,1]);\nlw=2\nplt.plot(fpr,tpr, color='red')\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='-.')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC curve')\nplt.show()","d0976a1b":"word_freq","b2ecd7e3":"x_train_data = train_data.drop('Id', axis = 'columns')\nx_train_data = x_train_data.drop('ham', axis = 'columns')\nx_train_data.loc[:,'char_freq_!2'] = train_data['char_freq_!']\ntest_features.loc[:,'char_freq_!2'] = train_data['char_freq_!']\nx_train_data.loc[:,'char_freq_!3'] = train_data['char_freq_!']\ntest_features.loc[:,'char_freq_!3'] = train_data['char_freq_!']\nx_train_data.shape\ntest_features.shape","ded7d1de":"x_train_data\n","cdb1eaae":"train_data.ham.shape","dc0ffc8b":"clf5 = BernoulliNB()\nclf5.fit(x_train_data, train_data.ham)\nBernoulliNB(alpha=0.1, binarize=0.0, fit_prior=True)\nY_Pred_B_NB = clf5.predict(test_features)\nY_Pred_B_NB = Y_Pred_B_NB.astype(bool)\nPred_B_NB = pd.DataFrame()\nPred_B_NB['Id'] = test_features_with_id['Id']\nPred_B_NB['ham'] = Y_Pred_B_NB\nPred_B_NB.set_index('Id', inplace=True)\nPred_B_NB.to_csv(\"Weight_BernoulliNB.csv\")\nPred_B_NB \n","d67736c5":"For the remaining the columns ...","83177350":"So, for the **word frequency base**:","e96b52dd":"Before the ROC curve, I would like to test the capital run potential","d70e4620":"So, after that, the most efficient (using the test database) classifier is the Bernoulli Naives Bayes that uses the complete train database. Now, playing with the variables:","e90d6718":"So we will predict the train data with knn = 1","ab998cfb":"(1,1) is point for very small alfa and (0,0) is a point for very large alfa and for a very large alfa we get a fewer false positive rate (and larger false negative rates). The ideal ROC curve would be one that disposes of two segmented lines from (0,0) to (0,1) and then from (0,1) to (1,1). But, as we couldn't get one of these, the best possible point would be the one closest to (0,1) where the line parallel to the no discrimination line (x=y) crosses the ROC curve. It will be the point with less chance of false positive and false negative errors (it is also called the  Youden's J statistic).","8378f77b":"First for the entire data frame, then for the selected parts\n","123f9950":"The Receiver Characteristic Curve is the ration between True Positives and False Positive Rates, where alfa(or treshold) denotes the ratio of false positives and false negatives. It means that, for alfa greater than 1, we let the misclassification error of giving one label positive when it should be negative less importancy (and thus, we calibrate the classifier to make this alfa result) than the error of giving one label negative when it should be positive","38344844":"**NAIVE BAYES**","e63001fb":"**Now, let's summarize the codes used above and apply them to select features (char_freq and word_freq)**","71da39cb":"Now analyzing a little bit the features from the train data ...\n\n\n","2eff416e":"-------------------------------------------------------------------------------------------------------------------------------------------------------","f64ff230":"In the end, the best classifier found to the spam\/ham problem was the one learned from a bernoulli distribution naive bayes (that is, multiple binomial and independent variables). There was an attempt to clear the features to better results, but until now, no evidence showed effiency in this case.","e8cc9111":"**KNN**","b0e7ce8e":"f3 scores weights the importance of **recall** (the ratio of true positives divided by all the positives within the actual label - TP and FN), so beta = 3 gives the recall 3 times more importancy than **precision** (the ratio of true positives divided by all the labelled positives - TP and FP)","b129b75e":"Now, Let's separate the excerpt with the 48 word frequency collumns \n","160a93e7":"So, graphically the efficiency of knn considering one i distance value is:","06a9d563":"Now, for the **special characters**:","0c283bcc":"Lastly, for the capital run portion of the databases:","76592e7a":"***Below is an attempt to weight some of the columns as word_freq_free, char_freq_!, but apparently this attempt backfired in the sense that the accuracy of the model dropped (probably by overfitting)***"}}