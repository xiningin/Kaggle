{"cell_type":{"d2e3be24":"code","666fa5ad":"code","cfbd14c2":"code","6523ddfd":"code","9fccd106":"code","bfbb5137":"code","a222636b":"code","1aea25d5":"code","d57142ab":"code","f5167476":"code","072728d3":"code","596cf924":"code","db8f242f":"code","1ab8aaa6":"code","82e788c8":"code","92fb55c2":"code","0f99d371":"code","c6238418":"code","2e7df372":"markdown","dceb4a37":"markdown","c65c27c4":"markdown","9722232d":"markdown","95f06e50":"markdown","39360215":"markdown","137b14f4":"markdown","82d89af9":"markdown","d73fbb20":"markdown","71308511":"markdown","60143763":"markdown","c7c7a6c6":"markdown","14a93614":"markdown"},"source":{"d2e3be24":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","666fa5ad":"data=pd.read_csv(os.path.join(dirname, filename),delim_whitespace=True, header=None)\ndata.columns = ['lever_position', 'ship_speed', 'gt_shaft', 'gt_rate', 'gg_rate', 'sp_torque', 'pp_torque',\n                     'hpt_temp', 'gt_c_i_temp', 'gt_c_o_temp', 'hpt_pressure', 'gt_c_i_pressure', 'gt_c_o_pressure',\n                     'gt_exhaust_pressure', 'turbine_inj_control', 'fuel_flow', 'gt_c_decay',  'gt_t_decay']\ndata = data.dropna()\ndata.describe()","cfbd14c2":"data.describe()","6523ddfd":"np.round(data.corr(),4)","9fccd106":"data=data.drop('gt_c_i_temp', axis=1)\ndata=data.drop('gt_c_i_pressure',axis=1)","bfbb5137":"data.head()","a222636b":"X=data[['lever_position', 'ship_speed', 'gt_shaft', 'gt_rate', 'gg_rate', 'sp_torque',\n        'pp_torque', 'hpt_temp', 'gt_c_o_temp', 'hpt_pressure', 'gt_c_o_pressure','gt_exhaust_pressure',\n        'turbine_inj_control', 'fuel_flow']]\nY1=data['gt_c_decay']\nY2=data['gt_t_decay']\nY=pd.DataFrame([Y1,Y2]).transpose()\nY","1aea25d5":"corr_mat= np.round(data.corr(),4)\nplt.figure(figsize = (18,9))\nsns.heatmap(corr_mat, annot = True)\nplt.show()","d57142ab":"corr_mat= Y.corr()\nplt.figure(figsize = (18,9))\nsns.heatmap(corr_mat, annot = True)\nplt.show()","f5167476":"for col in data.columns:\n    stat,p= stats.kstest(data[col],'norm')\n    if p<=0.05:\n        print('Feature: %s is not normal'%col)\n    else:\n        print('Feature: %s is normal'%col)","072728d3":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,Y)\nX_train1,X_test1,y_train1,y_test1=train_test_split(X,Y1)\nX_train2,X_test2,y_train2,y_test2=train_test_split(X,Y2)","596cf924":"y_train","db8f242f":"from sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import BaggingRegressor #Ensemble using averaging method\nfrom xgboost import XGBRegressor #Ensemble using boosting method\nfrom sklearn.metrics import mean_squared_error","1ab8aaa6":"svr=SVR()\nknn= KNeighborsRegressor()\ntree=DecisionTreeRegressor()\nbagg=BaggingRegressor()\nxgb=XGBRegressor()","82e788c8":"y1_train=[]\ny1_test=[]\nsvr.fit(X_train1, y_train1)\nknn.fit(X_train1, y_train1)\ntree.fit(X_train1, y_train1)\nbagg.fit(X_train1, y_train1)\nxgb.fit(X_train1, y_train1)\nprint('Accuracy of SVRegression on training set: {:.4f}'\n     .format(svr.score(X_train1, y_train1)))\ny1_train.append(svr.score(X_train1, y_train1))\nprint('Accuracy of SVRegression on test set: {:.4f}'\n     .format(svr.score(X_test1, y_test1)))\ny1_test.append(svr.score(X_test1, y_test1))\n\nprint('Accuracy of KNN Regressor on training set: {:.4f}'\n     .format(knn.score(X_train1, y_train1)))\ny1_train.append(knn.score(X_train1, y_train1))\nprint('Accuracy of KNN Regressor on test set: {:.4f}'\n     .format(knn.score(X_test1, y_test1)))\ny1_test.append(knn.score(X_test1, y_test1))\n\nprint('Accuracy of Decision Tree on training set: {:.4f}'\n     .format(tree.score(X_train1, y_train1)))\ny1_train.append(tree.score(X_train1, y_train1))\nprint('Accuracy of Decision Tree on test set: {:.4f}'\n     .format(tree.score(X_test1, y_test1)))\ny1_test.append(tree.score(X_test1, y_test1))\n\nprint('Accuracy of Bagging Regressor on training set: {:.4f}'\n     .format(bagg.score(X_train1, y_train1)))\ny1_train.append(bagg.score(X_train1, y_train1))\nprint('Accuracy of Bagging Regressor on test set: {:.4f}'\n     .format(bagg.score(X_test1, y_test1)))\ny1_test.append(bagg.score(X_test1, y_test1))\n\nprint('Accuracy of XG Boost Regressor on training set: {:.4f}'\n     .format(xgb.score(X_train1, y_train1)))\ny1_train.append(xgb.score(X_train1, y_train1))\nprint('Accuracy of XG Boost Regressor on test set: {:.4f}'\n     .format(xgb.score(X_test1, y_test1)))\ny1_test.append(xgb.score(X_test1, y_test1))","92fb55c2":"y2_train=[]\ny2_test=[]\nsvr.fit(X_train2, y_train2)\nknn.fit(X_train2, y_train2)\ntree.fit(X_train2, y_train2)\nbagg.fit(X_train2, y_train2)\nxgb.fit(X_train2, y_train2)\nprint('Accuracy of SVRegression on training set: {:.4f}'\n     .format(svr.score(X_train2, y_train2)))\ny2_train.append(svr.score(X_train2, y_train2))\nprint('Accuracy of SVRegression on test set: {:.4f}'\n     .format(svr.score(X_test2, y_test2)))\ny2_test.append(svr.score(X_test2, y_test2))\n\nprint('Accuracy of KNN Regressor on training set: {:.4f}'\n     .format(knn.score(X_train2, y_train2)))\ny2_train.append(knn.score(X_train2, y_train2))\nprint('Accuracy of KNN Regressor on test set: {:.4f}'\n     .format(knn.score(X_test2, y_test2)))\ny2_test.append(knn.score(X_test2, y_test2))\n\nprint('Accuracy of Decision Tree on training set: {:.4f}'\n     .format(tree.score(X_train2, y_train2)))\ny2_train.append(tree.score(X_train2, y_train2))\nprint('Accuracy of Decision Tree on test set: {:.4f}'\n     .format(tree.score(X_test2, y_test2)))\ny2_test.append(tree.score(X_test2, y_test2))\n\nprint('Accuracy of Bagging Regressor on training set: {:.4f}'\n     .format(bagg.score(X_train2, y_train2)))\ny2_train.append(bagg.score(X_train2, y_train2))\nprint('Accuracy of Bagging Regressor on test set: {:.4f}'\n     .format(bagg.score(X_test2, y_test2)))\ny2_test.append(bagg.score(X_test2, y_test2))\n\nprint('Accuracy of XG Boost Regressor on training set: {:.4f}'\n     .format(xgb.score(X_train2, y_train2)))\ny2_train.append(xgb.score(X_train2, y_train2))\nprint('Accuracy of XG Boost Regressor on test set: {:.4f}'\n     .format(xgb.score(X_test2, y_test2)))\ny2_test.append(xgb.score(X_test2, y_test2))","0f99d371":"model=['SVRegression','KNN Regressor','Decision Tree','Bagging Regressor','XG Boost Regressor']\nmod1=pd.DataFrame([model,y1_train,y1_test]).transpose()\nmod1.columns=['model','Train Accuracy','Test Accuracy']\nmod1.set_index('model')\nmod1.sort_values('Test Accuracy')","c6238418":"model=['SVRegression','KNN Regressor','Decision Tree','Bagging Regressor','XG Boost Regressor']\nmod2=pd.DataFrame([model,y2_train,y2_test]).transpose()\nmod2.columns=['model','Train Accuracy','Test Accuracy']\nmod2.set_index('model')\nmod2.sort_values('Test Accuracy')","2e7df372":"# Eliminate unneccessary features\n* We can easily see that gt_c_i_temp has std=0 that means it is a constant variable not an important variable that effect on our final result => drop  'gt_c_i_temp' column\n* Correlation of \"gt_c_i_pressure\" is 0 to all others => we can drop gt_c_i_pressure","dceb4a37":"# Machine Learning library Import","c65c27c4":"# Split data to X(Features) and Y(Responses)","9722232d":"# Split data for training model","95f06e50":"# Model selection\nBy training everything by it default setting, we will find out which model perform best in default setting\n","39360215":"# Y1 model selection\nBagging Regressor is the best model for Y1","137b14f4":"# Y2 model selection\nBagging Regressor is the best model for Y2","82d89af9":"# Further Correlation Analyze\n* Since all the features despite having high correlation between each other but very low correlation for the response, we should assume that the features and responses may have a non-linear relationship.","d73fbb20":"# Normality Test\nFrom normality test we are sure that our data do not follow normal distribution => Linear regression is not viable","71308511":"# Further Correlation Analyze\n* Our response Y1 and Y2 have extremely small correlation => We can consider them independent and treat them as two difference response and affect by different features.","60143763":"# Import Data","c7c7a6c6":"# Model Conclusion\n* Bagging Regressor gives both model for Y1 and Y2 quite good accuracy with 99.4% and 98.3% respectively\n* Here I only consider some algorithm that represent a kind of method. There might be an algorithm with higher accuracy that I haven't discover.","14a93614":"# Analyze data"}}