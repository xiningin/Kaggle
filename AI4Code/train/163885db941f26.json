{"cell_type":{"111244b0":"code","64ce8275":"code","d5704156":"code","9bdb1235":"code","d37e9144":"code","bcb70f2d":"code","5b7a85f4":"code","4b15489e":"code","b8f6b853":"code","1557f2ff":"code","8b998f65":"code","52194069":"code","3d15d55b":"code","931a619b":"code","0393ea27":"code","079cc52a":"code","717a0a10":"code","c4903135":"code","ab779323":"code","9b4bd757":"code","689eb96c":"code","ea14cce0":"code","536c0c05":"code","59bcc71b":"code","935605eb":"code","e02edbdf":"code","62e91b2e":"code","49469a96":"code","2b1a6d6e":"code","9a54d880":"code","596e260c":"code","317c4fc0":"code","f479ab09":"code","423ef6d4":"code","ce9854ee":"code","edac4534":"code","45b6f504":"code","33a6e50d":"markdown","8bfe0ec8":"markdown","d6b935ee":"markdown","5fd5f783":"markdown","9d63d28d":"markdown","b94d2ed1":"markdown","dd8be0c3":"markdown","76fc3895":"markdown","026cd155":"markdown","3ceeb8a0":"markdown","427ae404":"markdown","dab58a67":"markdown","43f4cbc6":"markdown","88462d40":"markdown","164085e4":"markdown","5185a067":"markdown","24f1c676":"markdown","bf586742":"markdown","d71053cc":"markdown","856a2404":"markdown","d72db645":"markdown"},"source":{"111244b0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","64ce8275":"####################### Import Modules ############\n# General tools\nimport os\nimport datetime\n\n#For data Handling\nimport pandas as pd\nimport numpy as np\n\n#For Visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px \n%matplotlib inline\n\n#Ignore Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Progreebar\nfrom tqdm import tqdm\n\n# For transformations and predictions\nfrom scipy.optimize import curve_fit\nfrom yellowbrick.target import FeatureCorrelation\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cluster import KMeans \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeRegressor\n\n# For scoring\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score,mean_absolute_error\n\n# For validation\nfrom sklearn.model_selection import train_test_split","d5704156":"#Load all 5 Datasets\n\ndf = pd.read_csv('\/kaggle\/input\/spotify-dataset-19212020-160k-tracks\/data.csv')\n\ndf_artist = pd.read_csv('\/kaggle\/input\/spotify-dataset-19212020-160k-tracks\/data_by_artist.csv')\n\ndf_by_genres = pd.read_csv('\/kaggle\/input\/spotify-dataset-19212020-160k-tracks\/data_by_genres.csv')\n\ndf_year = pd.read_csv('\/kaggle\/input\/spotify-dataset-19212020-160k-tracks\/data_by_year.csv')\n\ndf_w_genres = pd.read_csv('\/kaggle\/input\/spotify-dataset-19212020-160k-tracks\/data_w_genres.csv')","9bdb1235":"df.head()","d37e9144":"df.info()","bcb70f2d":"#Convert Milli secs duration into minutes\ndf['duration_min'] = df['duration_ms']\/60000\ndf['duration_min'] = df['duration_min'].round(2)\ndf_by_genres['duration_min'] = df_by_genres['duration_ms']\/60000\ndf_by_genres['duration_min'] = df_by_genres['duration_min'].round(2)\n\ndf['duration_min'].head()","5b7a85f4":"#Remove the Square Brackets from the artists\n\ndf[\"artists\"]=df[\"artists\"].str.replace(\"[\", \"\")\ndf[\"artists\"]=df[\"artists\"].str.replace(\"]\", \"\")\ndf[\"artists\"]=df[\"artists\"].str.replace(\"'\", \"\")\n\ndf.head()","4b15489e":"#Remove unnecessary Braces from the genres column\n\ndf_w_genres['genres'] = df_w_genres['genres'].str.replace(\"'\",\"\")\ndf_w_genres['genres'] = df_w_genres['genres'].str.replace(\"[\",\"\")\ndf_w_genres['genres'] = df_w_genres['genres'].str.replace(\"]\",\"\")\n\ndf_w_genres.head()","b8f6b853":"df.duplicated().any().sum()","1557f2ff":"df = df[~df.duplicated()==1]\ndf.shape","8b998f65":"df.duration_min.sum()\/(24*365*60)","52194069":"#Drop the columns\ndf.drop(['duration_ms'],inplace=True,axis=1)","3d15d55b":"#Visualisation of Variability for each column in the dataframe\nfig,ax = plt.subplots(3,4,figsize=(20,15))\n\nsns.distplot(df['valence'],ax=ax[0,0])\nsns.distplot(df['year'],ax=ax[0,1])\nsns.distplot(df['acousticness'],ax=ax[0,2])\nsns.distplot(df['danceability'],ax=ax[0,3])\n#sns.distplot(df['duration_min'],ax=ax[1,1])\nsns.distplot(df['energy'],ax=ax[1,0])\nsns.distplot(df['key'],ax=ax[1,1])\nsns.distplot(df['liveness'],ax=ax[1,2])\nsns.distplot(df['loudness'],ax=ax[1,3])\nsns.distplot(df['popularity'],ax=ax[2,0])\nsns.distplot(df['speechiness'],ax=ax[2,1])\nsns.distplot(df['tempo'],ax=ax[2,2])\nsns.distplot(df['mode'],ax=ax[2,3])","931a619b":"plt.figure(figsize=(16, 8))\nsns.set(style=\"whitegrid\")\ncorr = df.corr()\nsns.heatmap(corr,annot=True,cmap='BrBG_r')","0393ea27":"#Most Popular Tracks\nfig, axis = plt.subplots(figsize = (16,7))\npopular = df.groupby(\"name\")['popularity'].mean().sort_values(ascending=False).head(15)\naxis = sns.barplot(popular.index,popular)\naxis.set_title('Top 15 Popular Tracks')\naxis.set_ylabel('Popularity')\naxis.set_xlabel('Tracks')\nplt.xticks(rotation = 90)","079cc52a":"#Most Popular Artists\nfig, axis = plt.subplots(figsize = (16,7))\npopular = df.groupby(\"artists\")['popularity'].sum().sort_values(ascending=False)[:20]\naxis = sns.barplot(popular.index,popular)\naxis.set_title('Top 20 Artists with Popularity')\naxis.set_ylabel('Popularity')\naxis.set_xlabel('Tracks')\nplt.xticks(rotation = 90)","717a0a10":"# visualize the popularity of The Beatles songs over the year\nBeatles = df[df['artists'] == 'The Beatles']\nplt.rcParams['figure.figsize'] = (11,7)\n# line plot passing x,y\nsns.lineplot(x='year', y='popularity', data=Beatles, color='green')\n# Labels\nplt.title(\"The Beatles Popularity\")\nplt.xlabel('Year')\nplt.ylabel('Popularity')\nplt.show()","c4903135":"columns = [\"acousticness\",\"danceability\",\"energy\",\"speechiness\",\"liveness\",\"valence\"]\nplt.figure(figsize=(30,30))\nfor c in columns:\n    x = df.groupby('year')[c].mean()\n    sns.lineplot(x.index,x,label=c)\nplt.title('Audio characteristics over the years')\nplt.xlabel('Year',fontsize=30)\nplt.ylabel('Characteristics',fontsize=30)\nplt.legend(fancybox=True, framealpha=1, shadow=True, borderpad=1, prop={'size': 30}, loc = 'upper right')\nplt.show()","ab779323":"y=pd.DataFrame(df['artists'].value_counts().head()).reset_index()\ny.columns=['Artists','Songs_Count']\ny","9b4bd757":"year = pd.DataFrame(df['year'].value_counts())\nyear = year.sort_index()\nax=year.plot(kind='line',figsize=(15,8) ,color='#6f4a8e', linewidth=2)\nplt.title(\"Number of songs released Yearwise\",y=1.05,fontsize=20)\nplt.xlabel('Years')\nplt.ylabel('Count')\nax.axes.get_xaxis().set_visible(True)","689eb96c":"feature_names = ['acousticness', 'danceability', 'energy', 'instrumentalness',\n       'liveness', 'loudness', 'speechiness', 'tempo', 'valence','duration_min','explicit','key','mode','year']\n\nX, y = df[feature_names], df['popularity']\n\n# Create a list of the feature names\n\nfeatures = np.array(feature_names)\n\n# Instantiate the visualizer\nvisualizer = FeatureCorrelation(labels=features)\n\nplt.rcParams['figure.figsize']=(20,20)\nvisualizer.fit(X, y)        # Fit the data to the visualizer\nvisualizer.show()  ","ea14cce0":"cols = list(pd.read_csv('\/kaggle\/input\/spotify-dataset-19212020-160k-tracks\/data.csv'))\ndf1 = pd.read_csv('\/kaggle\/input\/spotify-dataset-19212020-160k-tracks\/data.csv', usecols=[i for i in cols if i not in ['id','name','release_date']])\n\n# Remove duplicated\ndf1 = df1[~df1.duplicated()==1]\n\n#Split the data to train and test\nX_train, X_test, y_train, y_test = train_test_split(df1.drop('popularity', axis=1), df1['popularity'], test_size = 0.2, random_state = 42)","536c0c05":"X_train.head()","59bcc71b":"n_features = df.shape[1]\nn_samples = df.shape[0]\n \ngrid = GridSearchCV(DecisionTreeRegressor(random_state=0), cv=3, n_jobs=-1, verbose=5,\n                    param_grid ={\n                    'max_depth': [None,5,6,7,8,9,10,11],\n                    'max_features': [None, 'sqrt', 'auto', 'log2', 0.3,0.5,0.7, n_features\/\/2, n_features\/\/3, ],\n                    'min_samples_split': [2,0.3,0.5, n_samples\/\/2, n_samples\/\/3, n_samples\/\/5],\n                    'min_samples_leaf':[1, 0.3,0.5, n_samples\/\/2, n_samples\/\/3, n_samples\/\/5]},\n                    )\n \ngrid.fit(X_train, y_train)\nprint('Train R^2 Score : %.3f'%grid.best_estimator_.score(X_train, y_train))\nprint('Test R^2 Score : %.3f'%grid.best_estimator_.score(X_test,y_test))\nprint('Best R^2 Score Through Grid Search : %.3f'%grid.best_score_)\nprint('Best Parameters : ',grid.best_params_)","935605eb":"class Artists:\n    \"\"\"\n     This transformer recives a DF with a feature 'artists' of dtype object\n      and convert the feature to a float value as follows:\n      1. Replace the data with the artists mean popularity\n      2. Replace values where artists appear less than MinCnt with y.mean()\n      3. Replace values where artists appear more than MaxCnt with 0\n      \n      PARAMETERS:\n      ----------\n      MinCnt (int): Minimal treshold of artisits apear in dataset, default = 3\n      MaxCnt (int): Maximal treshold of artisits apear in dataset, default = 600\n\n      RERTURN:\n      ----------\n      A DataFrame with converted artists str feature to ordinal floats\n    \n    \"\"\"\n    def __init__(self, MinCnt = 3.0, MaxCnt = 600.0):\n        self.MinCnt = MinCnt\n        self.MaxCnt = MaxCnt\n        self.artists_df = None\n\n    def fit (self, X, y):\n        self.artists_df =  y.groupby(X.artists).agg(['mean', 'count'])\n        self.artists_df.loc['unknown'] = [y.mean(), 1]\n        self.artists_df.loc[self.artists_df['count'] <= self.MinCnt, 'mean'] = y.mean()\n        self.artists_df.loc[self.artists_df['count'] >= self.MaxCnt, 'mean'] = 0\n        return self\n\n    def transform(self, X, y=None):\n        X['artists'] = np.where(X['artists'].isin(self.artists_df.index), X['artists'], 'unknown')\n        X['artists'] = X['artists'].map(self.artists_df['mean'])\n        return X\n  \n    \n#Instrumental Transformer Criteria\n\ndef instrumental(X):\n    X['instrumentalness'] = list(map((lambda x: 1 if x < 0.1 else (3 if x > 0.95 else 2)), X.instrumentalness))\n    \n    \nclass Tempo():\n    \"\"\"Eliminates Zero values from tempo columns and replace it \n       with the median or mean of non-zero values as specified.\n       defaut is set to 'median'.\n    \"\"\"\n\n    def __init__(self, method='median'):\n        self.method = method\n\n    def transform(self, X):\n        if self.method == 'median':\n            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].median()\n        elif self.method == 'mean':\n            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].mean()\n        else:\n            raise Exception(\"Method can be 'median' or 'mean' only!\")\n        return X\n    ","e02edbdf":"# Apply Aritists class on train and test seperatly\nartists_transformer = Artists(MinCnt=2)\nX_train = artists_transformer.fit(X_train, y_train).transform(X_train, y_train)\nX_test = artists_transformer.transform(X_test, y_test)\n\n#Apply Instrumental Criteria on train & test seperately\ninstrumentalness_tranformer = FunctionTransformer(instrumental)\ninstrumentalness_tranformer.transform(X_train)\ninstrumentalness_tranformer.transform(X_test)\n\n#Apply Tempo Transformer class on Train & Test seperately\ntempo_transformer = Tempo()\nX_train = tempo_transformer.transform(X_train)\nX_test = tempo_transformer.transform(X_test)","62e91b2e":"ohe = OneHotEncoder(categories='auto', drop='first')\n\n# Train\nfeature_arr = ohe.fit_transform(X_train[['instrumentalness','key']]).toarray()\ncolumns_key = ['key_'+str(i) for i in list(set(X_train['key'].values))[1:]]\ninstrumentalness_key = ['ins_'+str(i) for i in list(set(X_train['instrumentalness'].values))[1:]]\nfeature_labels = columns_key + instrumentalness_key\nfeature_labels = np.concatenate((feature_labels), axis=None)\nfeatures = pd.DataFrame(feature_arr, columns = feature_labels, index = X_train.index)\nX_train = pd.concat([X_train, features], axis=1).drop(['key','instrumentalness'], axis=1)\n\n# Test\nfeature_arr = ohe.fit_transform(X_test[['instrumentalness','key']]).toarray()\ncolumns_key = ['key_'+str(i) for i in list(set(X_test['key'].values))[1:]]\ninstrumentalness_key = ['ins_'+str(i) for i in list(set(X_test['instrumentalness'].values))[1:]]\nfeature_labels = columns_key + instrumentalness_key\nfeature_labels = np.concatenate((feature_labels), axis=None)\nfeatures = pd.DataFrame(feature_arr, columns = feature_labels, index = X_test.index)\nX_test = pd.concat([X_test, features], axis=1).drop(['key','instrumentalness'], axis=1)","49469a96":"scaler = MinMaxScaler()\ncols = ['artists','duration_ms','loudness','tempo']\nX_train[cols] = scaler.fit_transform(X_train[cols])\nX_test[cols] = scaler.fit_transform(X_test[cols])","2b1a6d6e":"# Divide the popularity by 100\ny_train = y_train \/ 100\ny_test = y_test \/ 100","9a54d880":"# Decision tree with tuned hyperparameters using GridSearchCV\ndec_tree2 = DecisionTreeRegressor(max_depth=9, max_features=None, \n                                  min_samples_leaf=1, min_samples_split=2, random_state=0)\ndec_tree2.fit(X_train,y_train)\ny_test_pred2 = dec_tree2.predict(X_test)\nmae2 = mean_absolute_error(y_test, y_test_pred2)\nprint(f'Mean absolute error of this model: {mae2:.3f}')","596e260c":"def Randomforest(X_train,y_train,X_test,y_test):\n    \"\"\"\n    Design Random Forest Regressor model and returns r2-score and Mean squared Error for Train and Test Datasets\n    INPUT:\n    :param X_train: \n    :param y_train: \n    :param X_test: \n    :param y_test: \n    OUTPUT:\n    Test Accuracy\n    Mean Absolute Error\n    r2-score for Train and Test datasets\n    \"\"\"\n    clf = RandomForestRegressor()\n    clf.fit(X_train, y_train)\n    \n    preds = clf.predict(X_test)\n\n    accuracy = clf.score(X_test, y_test)\n\n    mae = (abs(y_test - preds)).mean() \n    y_train_pred = clf.predict(X_train)\n    \n    y_test_pred = clf.predict(X_test)\n    \n    r2_train = r2_score(y_train, y_train_pred)\n    \n    r2_test= r2_score(y_test, y_test_pred)\n    \n    return accuracy,mae,r2_train,r2_test\n\naccuracy,mae,r2_train,r2_test =  Randomforest(X_train,y_train,X_test,y_test)\nprint(\"Test Accuracy: {:.4f}\".format(accuracy*100))\nprint(\"Mean Absolute Error: {:.4f} \".format(mae))\nprint(\"r2-score of Train Dataset is {} and Test Dataset is {}\".format(r2_train*100, r2_test*100))","317c4fc0":"def normalize_column(col):\n    \"\"\"\n    col - column in the dataframe which needs to be normalized\n    \"\"\"\n    max_d = df[col].max()\n    min_d = df[col].min()\n    df[col] = (df[col] - min_d)\/(max_d - min_d)","f479ab09":"#Normalize allnumerical columns so that min value is 0 and max value is 1\nnum_types = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnum = df.select_dtypes(include=num_types)\n        \nfor col in num.columns:\n    normalize_column(col)","423ef6d4":"km = KMeans(n_clusters=25)\npred = km.fit_predict(num)\ndf['pred'] = pred\nnormalize_column('pred')","ce9854ee":"class Song_Recommender():\n    \"\"\"\n    Neighbourhood Based Collborative Filterng REcoomendation System using similarity Metrics\n    Manhattan Distance is calculated for all songs and Recommend Songs that are similar to it based on any given song\n    \"\"\"\n    def __init__(self, data):\n        self.data_ = data\n    \n    #function which returns recommendations, we can also choose the amount of songs to be recommended\n    def get_recommendations(self, song_name, n_top):\n        distances = []\n        #choosing the given song_name and dropping it from the data\n        song = self.data_[(self.data_.name.str.lower() == song_name.lower())].head(1).values[0]\n        rem_data = self.data_[self.data_.name.str.lower() != song_name.lower()]\n        for r_song in tqdm(rem_data.values):\n            dist = 0\n            for col in np.arange(len(rem_data.columns)):\n                #indeces of non-numerical columns(id,Release date,name,artists)\n                if not col in [1,10]:\n                    #calculating the manhettan distances for each numerical feature\n                    dist = dist + np.absolute(float(song[col]) - float(r_song[col]))\n            distances.append(dist)\n        rem_data['distance'] = distances\n        #sorting our data to be ascending by 'distance' feature\n        rem_data = rem_data.sort_values('distance')\n        columns = ['artists', 'name']\n        return rem_data[columns][:n_top]","edac4534":"recommender = Song_Recommender(df)\nrecommender.get_recommendations('canon in d', 10)","45b6f504":"recommender.get_recommendations('dynamite', 10)","33a6e50d":"**Number of songs released Year Wise**","8bfe0ec8":"Random Forest Regressor ML Model","d6b935ee":"Conclusion\nLet\u2019s take a step back and look at the whole journey.\n1. The objective was to predict the popularity of any song and build a recommendation system to recommend songs\n2. Performed Exploratory data analysis to derive insights from the data\n3. Mostly 2000 popular songs are identified and added every year in Spotify\n4. The most popular Artist from 1921 to 2020 is the Beatles\n5. The most popular song is Dakiti by Bad Bunny & Jhay Cortez\u2019s released on 30th October 2020\n6. The data was split into \u2014 train (80%) and test datasets (20%) for model building and evaluation respectively\n7. Using GridSearchCV we could find the optimal hyperparameters for the decision tree Regressor model and achieved an accuracy of 76.6%","5fd5f783":"**Observation** - Most Popular artist from 1921-2020 - Beatles","9d63d28d":"Introduction - Spotify Datset contains information regarding Audio Features of 160000+ songs released in between 1921 and 2020\n\nDataset is taken from Kaggle Website\n\nIn this notebook i am interestested in Analysing Spotify Dataset and understand few findings from the Dataset:\n\nThere's a few questions we want to answer with our data here:\n\n1) Which Attributes\/features will likely lead a song to be more popular?\n\n2) What are the most Popular Tracks?\n\n3) Build a Song Recommendation System","b94d2ed1":"**Observation** - 'Tadeusz Dolega Mostowicz\t' is the artist with 1281 songs ","dd8be0c3":"**Observation** - Duplicates are now dropped from the dataset","76fc3895":"**Observation** - As we have noted throughout this project is that popularity is heavily dependent on the timeframe. As we see, drivers License has the highest popularity rating by this graph, but was released on January 8, 2021. Using this data in our regression will give us a snapshot as to the attributes popular songs have for recent releases, but may not work or become less relevant once we get further from this date.","026cd155":"**Most Popular Tracks**","3ceeb8a0":"From the above correlation table we can derive some basic insights as to what attributes make a song more popular.\n\n1) As expected popularity is highly correlated with the year released. This makes sense as the Spotify algorithm which makes this decision generates it's \"popularity\" metric by not just how many streams a song receives, but also how recent those streams are.\n\n2) Energy also seems to influence a song's popularity. Many popular songs are energetic, though not necessarily dance songs. Because the correlation here is not too high, low energy songs do have some potential to be more popular.\n\n3) Acousticness seems to be uncorrelated with popularity. Most popular songs today have either electronic or electric instruments in them. It is very rare that a piece of music played by a chamber orchestra or purely acoustic band becomes immesely popular (though, again, not impossible).\n\nOther things worth noting:\n\n1) Loudness and energy are highly correlated. This makes some sense as energy is definately influence by the volume the music is being played at.\n\n2) Acousticness is highly negatively correlated with energy, loudness, and year.\n\n3) Valence and dancability are highly coorelated. Dance songs are usually happier and in a major key\n\nThus, from this data, it would be better for an artist to create a high energy song with either electric instruments or electronic songs to have the best chance at generating the most popularity.","427ae404":"**Observation**\n\n1) Acousticness has decreased significantly. Most tracks past 1960 used electric instruments and, especially past the 1980s, electronic sounds. Most recorded music today includes both electric and electronic elements.\n\n2) Danceability has varied significantly, but has stayed mostly at the same level since 1980.\n\n3) Energy seems to be inversely related to acousticness: Was very low in the first part of the century, but then rose signficantly after 1960. It looks like it increased even more after 2000 as well.\n\n4) Speechiness looks like it varied a lot in the first part of the 20th century, but then settled low around 1960. Note we do see a slight increase after 1980. This is likely due to the growth of rap music. Mostly music, however, is still mostly sung.\n\n5) Liveness looks like it has always stayed relatively low. Most recorded music on Spotify was made with not audience present.\n\n6) Valence seems to have risen until 2000 with energy and danceability, but has fallen since.","dab58a67":"****Artist Recommendation System****\n****\nBuild a content-based recommendation engine which suggests similar songs based on any given song","43f4cbc6":"My Blog is published on Medium Platform\n\nhttps:\/\/medium.com\/swlh\/spotify-song-prediction-and-recommendation-system-b3bbc71398ad\n    ","88462d40":"**Observation** - There are no nulls in the dataset. Perfect!","164085e4":"****Data Cleaning****","5185a067":"**Observation** - Spotify has 454.06 days of music or 1.26 year of continous listening. In other words it will take 1.24 years for you to listen to all the songs","24f1c676":"**Observation** - \n- 2103 songs are released in year 2018\n\n- From the dataset creator's own comments, it's likely these are the 2000 most popular songs from each selected year.","bf586742":"**Artist with Maximum Number of Songs**","d71053cc":"Decision Tree Regressor Model with Grid Serach CV","856a2404":"**Observation** From the above graph it is clear that *5* features with negative correlation and *9* features with positive correlation\n\n**Feature Selection** - Choosing which Features can be used in model\n\n1) id: id is unique for each track, therfore cannot assist a model and will be dropped.\n\n2) name: There are 132,940 unique values. This is bit problematic categorical feature to insert in a model, and will be dropped.\n\n3) release_date\/year: Release date contains full date along with year. So instead of keeping both the columns Release_date can be dropped and year can be inserted into model","d72db645":"**Feature Selection**\n\nWe need to design a ML Model that can predicts the Popularity based on the features available. In order to select the features for the model I am using YellowbrickFeature Correlation Visualizer.\n\nThis visualizer calculates Pearson correlation coefficients and mutual information between features and the dependent variable. This visualization can be used in feature selection to identify features with high correlation or large mutual information with the dependent variable."}}