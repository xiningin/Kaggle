{"cell_type":{"2c4ee559":"code","58299bcd":"code","4b45b5c1":"code","3b951e7e":"code","fab6d33d":"markdown","8ae405e0":"markdown","4c124526":"markdown","7fedd685":"markdown","07d7129f":"markdown","731e3b96":"markdown","06582f9a":"markdown","edc52356":"markdown","a854b57d":"markdown"},"source":{"2c4ee559":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","58299bcd":"n = np.arange(1, 100001)\nprobs = 1 - (1 - (1\/n))**n\nfig, ax = plt.subplots(figsize = (10, 8))\nax.plot(n, probs)\nax.axhline(1 - 1\/np.e, linestyle = \":\", linewidth = 3, color = \"orange\", label = \"1 - 1\/e\")\nax.legend(loc = \"best\")\nax.set(xscale = \"log\", xlabel = \"Number of observations, n\", \n       ylabel = \"Probability jth observation is in the bootstrap sample of size n\");","4b45b5c1":"# Performing 10,000 simulations with n = 100 observations\n\n# Set seed for reproducible results\nnp.random.seed(312)\n# Create matrix where the row m, column n entry is the mth observation from the nth bootstrap sample\nbootstraps = np.trunc(np.random.random_sample((100, 10000))*100 + 1)\n# Count the number of the jth observation (e.g. j = 4) is in each bootstrap sample\nnum_j = (bootstraps == 4).sum(axis = 0)\n# Compute proportion of bootstrap samples which contain the jth observation at least once\n(num_j > 0).mean()","3b951e7e":"# Performing 1,000,000 simulations with n = 100 observations\n\n# Set seed for reproducible results\nnp.random.seed(312)\n# Create matrix where the row m, column n entry is the mth observation from the nth bootstrap sample\nbootstraps = np.trunc(np.random.random_sample((100, 1000000))*100 + 1)\n# Count the number of the jth observation (e.g. j = 4) is in each bootstrap sample\nnum_j = (bootstraps == 4).sum(axis = 0)\n# Compute proportion of bootstrap samples which contain the jth observation at least once\n(num_j > 0).mean()","fab6d33d":"## Answer\n\n1. To perform $k$-fold cross-validation, one randomly divides the set into $k$ equally-sized groups, or *folds*. Then, $k$ models are fit, $\\mathcal{M}_1, \\dots, \\mathcal{M}_k$, where model $\\mathcal{M}_j$ is fit using all of the observations except for fold $j$ as the set of training observations. After each model is fit, the desired accuracy score is computed for each model $\\mathcal{M}_j$ using the held-out fold $j$ as the validation set. For regression this is often the mean squared error, while for classification this is often the proportion of correctly classified observations. Once the accuracy score is computed for each model, they are averaged to obtain the cross-validated accuracy score. In practical terms, the `cv.glm()` function from the `boot` library can be used in R to perform $k$-fold cross-validation for any generalized linear model (e.g. least squares regression, logistic regression). With Python, $k$-fold cross-validation can be performed for any scikit-learn estimator [using the `cross_val_score()` function](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) from the `sklearn.model_selection` package.\n\n2. \n    1. Compared to the validation set approach, $k$-fold cross-validation is a little more computationally intensive, since it will require computing more than one model. However, since $k$-fold cross-validation involves averaging the accuracy scores of $k$ different models, there is usually much less variability in the accuracy score estimates compared to the validation set approach, which depends much more on which observations are included in the training set and which observations are included in the validation set.\n    2. Compared to leave-one-out cross-validation (LOOCV), $k$-fold cross-validation is much less computationally intensive, since in practice it usually involves fitting either $k = 5$ or $k = 10$ models, compared to the $k = n$ models required for LOOCV. Since each training set contains $n - 1$ observations in LOOCV, it will give approximately unbiased estimates of the test accuracy score, while $k$-fold cross validation will introduce a bit more bias due to the smaller training sets involved. However, since LOOCV is essentially averaging the outputs of $n$ fitted models which are each trained on nearly identical sets of observations,  those outputs are highly (positively) correlelated with each other due to the large overlaps between training sets for each model. On the other hand, $k$-fold cross-validation with $k < n$ involves averaging the outputs of $k$ fitted models that are somewhat less correlated with each other, since the overlap between the training sets of each model is smaller. Since the mean of many highly correlated quantities has a higher variance than does the mean of many quantities that are not as highly correlated, the accuracy score estimate resulting from LOOCV tends to have higher variance than does the accuracy score estimate resulting from $k$-fold cross-validation. Therefore, while the accuracy score estimates from $k$-fold cross-validation are more a bit biased than those from LOOCV, the reduction in variance often results in $k$-fold cross-validation giving more accurate estimates of the test accuracy score.","8ae405e0":"8. Instead of R code, we use Python to perform this simulation. The results obtained align with the computation done in Part 5 of this exercise above. We can also increase number of simulations we perform to provide further numerical evidence for the expected proportion of bootstrap samples which contain the $j$th observation for any $n$ of our choosing.","4c124526":"# Conceptual Exercise 2\n\n## Question\n\nWe will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of $n$ observations.\n\n1. What is the probability that the first bootstrap observation is *not* the $j$th observation from the original sample? Justify your answer.\n2. What is the probability that the second bootstrap observation is *not* the the $j$th observation from the original sample.\n3. Argue that the probability that the $j$th observation is *not* in the bootstrap sample is $(1 - 1\/n)^n$.\n4. When $n = 5$, what is the probability that the $j$th observation is in the bootstrap sample.\n5. When $n = 100$, what is the probability that the $j$th observation is in the bootstrap sample.\n6. When $n = 10000$, what is the probability that the $j$th observation is in the bootstrap sample.\n7. Create a plot that displays, for each integer value of $n$ from 1 to 100,000, the probability that the $j$th observation is in the bootstrap sample. Comment on what you observe.\n8. We will now investigate numerically the probability that a bootstrap observation of size $n = 100$ contains the $j$th observation. For the case $j = 4$ (though the result will be the same for any value of $j$) we can use the R code below to repeatedly create bootstrap samples, and each time record whether or not the $j$th observation is contained in the bootstrap sample. Write a snippet of code to do this and then comment on the results obtained.\n```\nThis is R code.\n> store = rep(NA, 10000)\n> for (i in 1:10000) {store[i] = sum(sample(1:100, rep = TRUE) == 4) > 0}\n> mean(store)\n```","7fedd685":"# Conceptual Exercise 1\n\n## Question\n\nUsing basic statistical properties of the variance, as well as single-variable calculus, derive\n\n\\begin{equation}\n    \\alpha = \\frac{\\sigma_Y^2 - \\sigma_{XY}}{\\sigma_X^2 + \\sigma_Y^2 - 2\\sigma_{XY}}.\n\\end{equation}\n\nIn other words, prove that $\\alpha$ given by the above formula does indeed minimize $\\text{Var}(\\alpha X + (1 - \\alpha)Y)$.","07d7129f":"## Answer\n\nFirst, we recall that for any random variables $X, Y$ and any real numbers $a, b$, that \n\n\\begin{align}\n    \\text{Var}(aX + bY) &= a^2\\text{Var}(X) + b^2\\text{Var}(Y) + 2ab\\text{Cov}(X, Y)\\\\\n    &= a^2\\sigma_X^2 + b^2\\sigma_Y^2 + 2ab\\sigma_{XY}.\n\\end{align}\n\nThus, in order to find the extrema for $\\alpha \\in [0, 1]$ we take the derivative of $f(\\alpha) = \\text{Var}(\\alpha X + (1 - \\alpha)Y)$ with respect to $\\alpha$, set it equal to zero, and solve for $\\alpha$. Using the formula for $\\text{Var}(aX + bY)$ and taking the derivative of $f(\\alpha)$ with respect to $\\alpha$, we have\n\n\\begin{align}\n    0 &= \\frac{df}{d\\alpha} = 2\\sigma_X^2\\alpha - 2(1 - \\alpha)\\sigma_Y^2 + 2(1 - 2\\alpha)\\sigma_{XY}\\\\\n    &= 2\\sigma_{XY} - 2\\sigma_Y^2 + 2\\alpha(\\sigma_X^2 + \\sigma_Y^2 - 2\\sigma_{XY}).\n\\end{align}\n\nSolving this equation for $\\alpha$, we get \n\n\\begin{equation}\n    \\alpha = \\frac{\\sigma_Y^2 - \\sigma_{XY}}{\\sigma_X^2 + \\sigma_Y^2 - 2\\sigma_{XY}},\n\\end{equation}\n\nthe value of $\\alpha$ which minimizes $\\text{Var}(\\alpha X + (1 - \\alpha)Y)$. Note that we know this is an absolute minimum since it is the $\\alpha$ value of the vertex of the parabola $f(\\alpha) = (\\sigma_X^2 + \\sigma_Y^2 - 2\\sigma_{XY})\\alpha^2 + 2(\\sigma_{XY} - \\sigma_Y^2)\\alpha + \\sigma_Y^2$ which we get when we rearrange the terms of $f(\\alpha) = \\text{Var}(\\alpha X + (1 - \\alpha)Y)$.","731e3b96":"# Conceptual Exercise 4\n\n## Question\n\nSuppose that we use some statistical learning method to make a prediction for the response $Y$ for a particular value of the predictor $X$. Carefully describe how we might estimate the standard deviation of our prediction.","06582f9a":"## Answer\n\n1. The probability that the first bootstrap observation is not the $j$th observation from the original sample is $1 - 1\/n$. This is because there are $n$ samples, and we are equally likely to pick each observation when taking our first bootstrap observation.\n\n2. Since bootstrap observations are sampled from the original observations *with replacement*, the probability that the second bootstrap observation is not the $j$th observation from the original sample is again $1 - 1\/n$.\n\n3. As the probability that any particular bootstrap observation is not the $j$th observation from the original sample is $1 - 1\/n$, and we are taking a total of $n$ bootstrap observations from the original set of $n$ observations, the probability that the $j$th observation is not in the bootstrap stample is $(1 - 1\/n)^n$. Note that we are multiplying probabilities because sampling with replacement means that the individual bootstrap observations in the bootstrap sample are independent.\n\n4. First note that the probability that the $j$th observation is in the bootstrap sample is $1 - (1 - 1\/n)^n$. This is because the $j$th observation being in the bootstrap sample is the event that is complementary to the $j$th observation not being in the bootstrap sample, which we saw had a probability of $(1 - 1\/n)^n$. Thus, when $n = 5$ the probability that the $j$th observation is in the bootstrap sample is $1 - (1 - 1\/5)^5 = \\frac{2101}{3125} \\approx 0.672$.\n\n5. When $n = 100$, the probability that the $j$th observation is in the bootstrap sample is $1 - (1 - 1\/100)^{100} \\approx 0.634$.\n\n6. When $n = 1000$, the probability that the $j$th observation is in the bootstrap sample is $1 - (1 - 1\/1000)^{1000} \\approx 0.632$.\n\n7. For this part, we use Python to create the plot. We observe that there is a horizontal asymptote at $1 - 1\/e \\approx 0.63212$. We can use [L'H\u00f4pital's rule](https:\/\/en.wikipedia.org\/wiki\/L%27H%C3%B4pital%27s_rule) on the limit $\\ln(L) = \\lim_{n \\to \\infty} n\\ln(1 - 1\/n)$ to confirm that $L = \\lim_{n \\to \\infty} (1 - 1\/n)^n = 1\/e$. Thus, a $n$ tends to infinity, the probability that the $j$th observation will be in the bootstrap sample approaches $1 - 1\/e$.","edc52356":"# Conceptual Exercise 3\n\n## Question\n\nWe now review $k$-fold cross-validation.\n\n1. Explain how $k$-fold cross-validation is implemented.\n2. What are the advantages and disadvantages of $k$-fold cross-validation relative to:\n    1. The validation set approach?\n    2. Leave-one-out cross-validation (LOOCV)?","a854b57d":"## Answer\n\nAssuming our dataset consists of independent observations (as opposed to something like time series data, in which observations that are close together in time are generally not independent and using a strategy such as a block bootstrap would be preferable), we can use the bootstrap to estimate the standard deviation of our prediction. Suppose our original data set had $n$ observations. To perform the bootstrap for this situation, we randomly sample, with replacement, $n$ observations from our original set of observations to use as a training set for our statistical learning method and obtain a bootstrap model $\\mathcal{M}^{*i}$. We then use the model $\\mathcal{M}^{*i}$ to predict the response $\\hat{Y}^{*i}$ for the value of the predictor $X$. We repeat this procedure $B$ times for some large value of $B$ in order to produce $B$ different bootstrap models $\\mathcal{M}^{*1}, \\dots, \\mathcal{M}^{*B}$ and $B$ corresponding $Y$ estimates, $\\hat{Y}^{*1}, \\dots, \\hat{Y}^{*B}$. Once we have done this, we can use the formula for the [sample standard deviation](https:\/\/en.wikipedia.org\/wiki\/Standard_deviation#Corrected_sample_standard_deviation) to compute an estimate of the standard deviation of our prediction.\n\n\\begin{equation}\n    \\text{SE}_{B} \\left( \\hat{Y} \\right) = \n    \\sqrt{ \\frac{1}{B - 1} \\sum_{j = 1}^B \\left( \\hat{Y}^{*j} - \\frac{1}{B} \\sum_{k = 1}^B \\hat{Y}^{*k} \\right)^2 }\n\\end{equation}"}}