{"cell_type":{"e5fc5c06":"code","9df72e21":"code","5a44b045":"code","2db29d3f":"code","2a79d2e7":"code","824e51ca":"code","70d3f2b5":"code","ebfc36a9":"code","7cfce127":"code","c607ab55":"code","607e5ebe":"code","373fd33c":"code","e73b6ce6":"code","27604597":"code","4f9f92cd":"code","8d7f8fdf":"code","cb77671f":"code","f9a1e36e":"code","bd914cfc":"code","caf318e9":"code","56726381":"markdown","9b67da8b":"markdown","42823ead":"markdown","5c91dc9d":"markdown","03c923c8":"markdown","b256c0e0":"markdown","5a51b90b":"markdown","7ae4c2a1":"markdown","b0b05a25":"markdown","a149b54d":"markdown"},"source":{"e5fc5c06":"# Data preprocessing\nimport numpy as np \nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\n\n\n# Plotting\nimport matplotlib.pyplot as plt\n\n# ANN + ML\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import CosineAnnealingLR,CosineAnnealingWarmRestarts, ReduceLROnPlateau\nimport lightgbm as lgb\n\n\n\n# Image preprocessing\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n\n# Miscellanous\nimport os\nimport sys\nimport math\nimport time\nimport pickle\nimport random\nsys.path.append('..\/input\/pytorch-image-models\/pytorch-image-models-master')\nimport timm\nimport warnings\n\n\nwarnings.filterwarnings('ignore')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","9df72e21":"train_df = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ntest_df = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')","5a44b045":"def get_train_file_path(image_id):\n    return \"..\/input\/petfinder-pawpularity-score\/train\/{}.jpg\".format(image_id)\n\ndef get_test_file_path(image_id):\n    return \"..\/input\/petfinder-pawpularity-score\/test\/{}.jpg\".format(image_id)\n\ntrain_df['file_path'] = train_df['Id'].apply(get_train_file_path)\ntest_df['file_path'] = test_df['Id'].apply(get_test_file_path)","2db29d3f":"train_df.head()","2a79d2e7":"test_df.head()","824e51ca":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\n\nOUTPUT_DIR = '.\/'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","70d3f2b5":"CFG = {'num_workers':4, \n       'size': 512, \n       'batch_size':32,\n       'model_name':'tf_efficientnet_b0_ns',\n       'seed':42,\n       'target_size':1,\n       'target_col':'Pawpularity',\n       'n_fold':5,\n       'apex': False,\n       'gradient_accumulation_steps':1,\n       'print_freq':10,\n       'max_grad_norm':1000,\n       'train':True,\n       'grad_cam':False,\n       'trn_fold':[0, 1, 2, 3, 4],\n       'lr':1e-4,\n       'weight_decay':1e-6,\n       'scheduler':'CosineAnnealingLR',\n       'target_col':'Pawpularity',\n       'epochs':3,\n       #'factor':0.2,# ReduceLROnPlateau\n       #'patience':4, # ReduceLROnPlateau\n       #'eps':1e-6, # ReduceLROnPlateau\n       'T_max':3, # CosineAnnealingLR\n       #T_0:3, # CosineAnnealingWarmRestarts\n       'min_lr':1e-6\n      }","ebfc36a9":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_true, y_pred):\n    score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n    return score\n\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=CFG['seed'])","7cfce127":"num_bins = int(np.floor(1 + np.log2(len(train_df))))\ntrain_df[\"bins\"] = pd.cut(train_df[CFG['target_col']], bins=num_bins, labels=False)\n\nFold = KFold(n_splits=CFG['n_fold'], shuffle=True, random_state=CFG['seed'])\nfor n, (train_index, val_index) in enumerate(Fold.split(train_df, train_df[\"Pawpularity\"])):\n    train_df.loc[val_index, 'fold'] = int(n)\n\ntrain_df['fold'] = train_df['fold'].astype(int)\n# train_df.groupby(['fold', \"bins\"]).size()","c607ab55":"train_df.head()","607e5ebe":"train_df.to_pickle(OUTPUT_DIR+'train_fold.pkl')","373fd33c":"class PawpularityDataset(Dataset):\n    def __init__(self, df, transform = None):\n        self.df = df\n        self.file_names = df['file_path'].values\n        self.labels = df[CFG['target_col']].values\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        filename = self.file_names[idx]\n        image = cv2.imread(filename)\n        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            image = self.transform(image = image)['image']\n        label = torch.tensor(self.labels[idx]).float()\n        return image, label\n\n    ","e73b6ce6":"def image_transform(data):\n    if data== 'train':\n        return A.Compose([A.RandomResizedCrop(CFG['size'],CFG['size'],scale = (0.85, 1.0)),\n                         A.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]),\n                          ToTensorV2()])\n    \n    elif data == 'valid':\n        return A.Compose([A.Resize(CFG['size'],CFG['size']),\n                         A.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]),\n                          ToTensorV2()])","27604597":"train_dataset = PawpularityDataset(train_df, transform=image_transform(data='train'))\n\nfor i in range(5):\n    plt.figure(figsize=(4, 4))\n    image, label = train_dataset[i]\n    plt.imshow(image[0],cmap = 'gray')\n    plt.title(f'label: {label}')\n    plt.show() ","4f9f92cd":"class PawpularityModel(nn.Module):\n    def __init__(self, cfg, pretrained = False):\n        super().__init__()\n        self.cfg = cfg\n        self.model = timm.create_model(self.cfg['model_name'], pretrained=pretrained)\n        self.features = self.model.classifier.in_features\n        self.model.classifier = nn.Identity()\n        self.fc = nn.Linear(self.features, self.cfg['target_size'])\n        \n    def feature(self, image):\n        feature = self.model(image)\n        return feature\n    \n    def forward(self, image):\n        feature = self.feature(image)\n        output = self.fc(feature)\n        return output","8d7f8fdf":"class RMSELoss(nn.Module):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        self.eps = eps\n\n    def forward(self, yhat, y):\n        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n        return loss","cb77671f":"# ====================================================\n# Helper functions\n# ====================================================\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s \/ (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\n\ndef train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n    model.train()\n    if CFG['apex']:\n        scaler = GradScaler()\n    losses = AverageMeter()\n    start = end = time.time()\n    global_step = 0\n    for step, (images, labels) in enumerate(train_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        if CFG['apex']:\n            with autocast():\n                y_preds = model(images)\n                loss = criterion(y_preds.view(-1), labels)\n        else:\n            y_preds = model(images)\n            loss = criterion(y_preds.view(-1), labels)\n        # record loss\n        losses.update(loss.item(), batch_size)\n        if CFG['gradient_accumulation_steps'] > 1:\n            loss = loss \/ CFG['gradient_accumulation_steps']\n        if CFG['apex']:\n            scaler.scale(loss).backward()\n        else:\n            loss.backward()\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG['max_grad_norm'])\n        if (step + 1) % CFG['gradient_accumulation_steps'] == 0:\n            if CFG['apex']:\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                optimizer.step()\n            optimizer.zero_grad()\n            global_step += 1\n        end = time.time()\n        if step % CFG['print_freq'] == 0 or step == (len(train_loader)-1):\n            print('Epoch: [{0}][{1}\/{2}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  'Grad: {grad_norm:.4f}  '\n                  'LR: {lr:.6f}  '\n                  .format(epoch+1, step, len(train_loader), \n                          remain=timeSince(start, float(step+1)\/len(train_loader)),\n                          loss=losses,\n                          grad_norm=grad_norm,\n                          lr=scheduler.get_lr()[0]))\n        print({f\"[fold{fold}] loss\": losses.val,\n                   f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n    return losses.avg\n\n\ndef valid_fn(valid_loader, model, criterion, device):\n    model.eval()\n    losses = AverageMeter()\n    preds = []\n    start = end = time.time()\n    for step, (images, labels) in enumerate(valid_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        # compute loss\n        with torch.no_grad():\n            y_preds = model(images)\n        loss = criterion(y_preds.view(-1), labels)\n        losses.update(loss.item(), batch_size)\n        # record accuracy\n        preds.append(y_preds.to('cpu').numpy())\n        if CFG['gradient_accumulation_steps'] > 1:\n            loss = loss \/ CFG['gradient_accumulation_steps']\n        end = time.time()\n        if step % CFG['print_freq'] == 0 or step == (len(valid_loader)-1):\n            print('EVAL: [{0}\/{1}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(step, len(valid_loader),\n                          loss=losses,\n                          remain=timeSince(start, float(step+1)\/len(valid_loader))))\n    predictions = np.concatenate(preds)\n    return losses.avg, predictions","f9a1e36e":"# ====================================================\n# Train loop\n# ====================================================\ndef train_loop(folds, fold):\n    \n    print(f\"========== fold: {fold} training ==========\")\n\n    # ====================================================\n    # loader\n    # ====================================================\n    trn_idx = folds[folds['fold'] != fold].index\n    val_idx = folds[folds['fold'] == fold].index\n\n    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n    valid_labels = valid_folds[CFG['target_col']].values\n\n    train_dataset = PawpularityDataset(train_folds, transform=image_transform(data='train'))\n    valid_dataset = PawpularityDataset(valid_folds, transform=image_transform(data='train'))\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=CFG['batch_size'], \n                              shuffle=True, \n                              num_workers=CFG['num_workers'], pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset, \n                              batch_size=CFG['batch_size'] * 2, \n                              shuffle=False, \n                              num_workers=CFG['num_workers'], pin_memory=True, drop_last=False)\n    \n    # ====================================================\n    # scheduler \n    # ====================================================\n    def get_scheduler(optimizer):\n        if CFG['scheduler']=='ReduceLROnPlateau':\n            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG['factor'], patience=CFG['patience'], verbose=True, eps=CFG['eps'])\n        elif CFG['scheduler']=='CosineAnnealingLR':\n            scheduler = CosineAnnealingLR(optimizer, T_max=CFG['T_max'], eta_min=CFG['min_lr'], last_epoch=-1)\n        elif CFG['scheduler']=='CosineAnnealingWarmRestarts':\n            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG['T_0'], T_mult=1, eta_min=CFG['min_lr'], last_epoch=-1)\n        return scheduler\n\n    # ====================================================\n    # model & optimizer\n    # ====================================================\n    model = PawpularityModel(CFG, pretrained=True)\n    model.to(device)\n\n    optimizer = Adam(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'], amsgrad=False)\n    scheduler = get_scheduler(optimizer)\n\n    # ====================================================\n    # loop\n    # ====================================================\n    criterion = RMSELoss()\n\n    best_score = np.inf\n    best_loss = np.inf\n    \n    for epoch in range(CFG['epochs']):\n        \n        start_time = time.time()\n        \n        # train\n        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n\n        # eval\n        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n        \n        if isinstance(scheduler, ReduceLROnPlateau):\n            scheduler.step(avg_val_loss)\n        elif isinstance(scheduler, CosineAnnealingLR):\n            scheduler.step()\n        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n            scheduler.step()\n\n        # scoring\n        score = get_score(valid_labels, preds)\n\n        elapsed = time.time() - start_time\n\n        print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n        print(f'Epoch {epoch+1} - Score: {score:.4f}')\n\n        if score < best_score:\n            best_score = score\n            print(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save({'model': model.state_dict(), \n                        'preds': preds},\n                        OUTPUT_DIR+'{}_fold{}_best.pth'.format(CFG['model_name'],fold))\n    \n    valid_folds['preds'] = torch.load(OUTPUT_DIR+'{}_fold{}_best.pth'.format(CFG['model_name'],fold), \n                                      map_location=torch.device('cpu'))['preds']\n\n    return valid_folds","bd914cfc":"# ====================================================\n# main\n# ====================================================\ndef main():\n\n    \"\"\"\n    Prepare: 1.train \n    \"\"\"\n\n    def get_result(result_df):\n        preds = result_df['preds'].values\n        labels = result_df[CFG['target_col']].values\n        score = get_score(labels, preds)\n        print(f'Score: {score:<.4f}')\n    \n    if CFG['train']:\n        # train \n        oof_df = pd.DataFrame()\n        for fold in range(CFG['n_fold']):\n            if fold in CFG['trn_fold']:\n                _oof_df = train_loop(train_df, fold)\n                oof_df = pd.concat([oof_df, _oof_df])\n                print(f\"========== fold: {fold} result ==========\")\n                get_result(_oof_df)\n        # CV result\n        print(f\"========== CV ==========\")\n        get_result(oof_df)\n        # save result\n        oof_df.to_csv(OUTPUT_DIR+'oof_df.csv', index=False)","caf318e9":"if __name__ == '__main__':\n    main()","56726381":"# CFG","9b67da8b":"# Utils","42823ead":"# MODEL","5c91dc9d":"# Helper functions","03c923c8":"# Transforms","b256c0e0":"# Train loop","5a51b90b":"# Data Loading","7ae4c2a1":"# Dataset","b0b05a25":"# Loss","a149b54d":"# CV split"}}