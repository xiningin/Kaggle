{"cell_type":{"6da4c97e":"code","c79700e5":"code","7b8446a9":"code","c49a2db3":"code","99b8fac4":"code","f9d30637":"code","b1c180cf":"code","424f5f16":"code","ceecff0b":"code","afedf573":"code","1ba2e99d":"code","ed724490":"code","98018d30":"code","82691ca6":"code","df454c26":"markdown","d72eab03":"markdown","e6b08bbb":"markdown","9dd4e45e":"markdown","819f11f0":"markdown","7bfa1627":"markdown","c33dfc14":"markdown","fbe9b113":"markdown"},"source":{"6da4c97e":"# Importing all the required libraries\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nimport optuna\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c79700e5":"df = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ndf['folds'] = -1\nkf = KFold(n_splits = 10, shuffle=True, random_state = 42)\nfor fold, (train_index,valid_index) in enumerate(kf.split(X=df)):\n    df.loc[valid_index,'folds'] = fold\ndf.to_csv('train10folds.csv',index=False)","7b8446a9":"df = pd.read_csv('\/kaggle\/input\/housing-10folds\/housing_train10folds.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/sample_submission.csv')","c49a2db3":"df.info()","99b8fac4":"num_cols = [col for col in df.columns if df[col].dtype in ['int64','float64'] and col not in ['Id','SalePrice','folds'] ]\ncat_cols = [col for col in df.columns if df[col].nunique() < 10 and df[col].dtype == \"object\"]","f9d30637":"temp = []\nfor col in list(df.columns):\n    temp.append([col, df[col].nunique(), df[col].isnull().sum()])\ncol_df = pd.DataFrame(temp,columns = ['Name','Unique','Null'])\ncol_df.set_index('Name',inplace=True)","b1c180cf":"col_df.loc[num_cols]","424f5f16":"col_df.loc[cat_cols]","ceecff0b":"cat_cols = list(col_df.loc[cat_cols][(col_df.loc[cat_cols]).Null < 100].index)","afedf573":"cat_cols","1ba2e99d":"def prep(xtrain, xvalid, xtest, cat_cols, num_cols):\n    \n    # Categorical Columns\n    imp1 = SimpleImputer(strategy = 'most_frequent')\n    imp_xtrain = pd.DataFrame(imp1.fit_transform(xtrain[cat_cols]))\n    imp_xvalid = pd.DataFrame(imp1.transform(xvalid[cat_cols]))\n    imp_xtest = pd.DataFrame(imp1.transform(xtest[cat_cols]))\n    \n    # SimpleImputer removes the column names, let's put it back.\n    imp_xtrain.columns = xtrain[cat_cols].columns\n    imp_xvalid.columns = xvalid[cat_cols].columns\n    imp_xtest.columns = xtest[cat_cols].columns\n    \n    # OneHotEncoding of all the categorical features.\n    OHE = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    OH_train = pd.DataFrame(OHE.fit_transform(imp_xtrain))\n    OH_valid = pd.DataFrame(OHE.transform(imp_xvalid))\n    OH_test = pd.DataFrame(OHE.transform(imp_xtest))\n\n    # One-hot encoding removed index let's put it back\n    OH_train.index = xtrain.index\n    OH_valid.index = xvalid.index\n    OH_test.index = xtest.index\n\n\n    ## Numerical Columns\n    imp2 = SimpleImputer(strategy = 'median')\n    imp2_xtrain = pd.DataFrame(imp2.fit_transform(xtrain[num_cols]))\n    imp2_xvalid = pd.DataFrame(imp2.transform(xvalid[num_cols]))\n    imp2_xtest = pd.DataFrame(imp2.transform(xtest[num_cols]))\n\n    imp2_xtrain.columns = xtrain[num_cols].columns\n    imp2_xvalid.columns = xvalid[num_cols].columns\n    imp2_xtest.columns = xtest[num_cols].columns\n    \n    # Finally concatenating both categorical and numerical features\n    xtrain = pd.concat([OH_train, imp2_xtrain], axis=1)\n    xvalid = pd.concat([OH_valid, imp2_xvalid], axis=1)\n    xtest = pd.concat([OH_test, imp2_xtest], axis=1)\n    \n    return xtrain, xvalid, xtest","ed724490":"def test(trial):\n    scores = []\n    for fold in range(10):\n        learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n        reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n        reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n        subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n        max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n        \n        xtrain = df[df['folds']!=fold].reset_index(drop=True)\n        xvalid = df[df['folds']==fold].reset_index(drop=True)\n        xtest = df_test.copy()\n        \n        ytrain = xtrain['SalePrice']\n        yvalid = xvalid['SalePrice']\n\n        x_train, x_valid, x_test = prep(xtrain, xvalid, xtest, cat_cols, num_cols)\n\n        model = XGBRegressor(random_state=42, n_jobs=4,\n                             tree_method='gpu_hist', gpu_id=0, predictor=\"gpu_predictor\",\n                             n_estimators=10000,\n                             learning_rate=learning_rate,\n                             reg_lambda=reg_lambda,\n                             reg_alpha=reg_alpha,\n                             subsample=subsample,\n                             colsample_bytree=colsample_bytree,\n                             max_depth=max_depth)\n        \n        model.fit(x_train, ytrain, early_stopping_rounds=500, \n                  eval_set=[(x_valid, yvalid)], verbose=1000)\n        \n        preds_valid = model.predict(x_valid)\n        rmse = mean_squared_error(yvalid,preds_valid, squared=False)\n        scores.append(rmse)\n        return np.mean(scores)\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(test, n_trials=5)","98018d30":"params = study.best_params\nfinal_preds = []\nfor fold in range(10):\n    xtrain = df[df['folds']!=fold].reset_index(drop=True)\n    xvalid = df[df['folds']==fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    ytrain = xtrain['SalePrice']\n    yvalid = xvalid['SalePrice']\n\n    x_train, x_valid, x_test = prep(xtrain, xvalid, xtest, cat_cols, num_cols)\n\n    model = XGBRegressor(random_state=42, n_jobs=4, tree_method='gpu_hist', \n                         gpu_id=0, predictor=\"gpu_predictor\",\n                         n_estimators = 10000, **params)\n    model.fit(x_train, ytrain, early_stopping_rounds=500, eval_set=[(x_valid, yvalid)], verbose=1000)\n    preds_valid = model.predict(x_valid)\n    preds_test = model.predict(x_test)\n    final_preds.append(preds_test)\n    rmse = mean_squared_error(yvalid,preds_valid, squared=False)\n    print('RMSE for',fold,'is',rmse)","82691ca6":"predictions = np.mean(np.column_stack(final_preds),axis=1)\nsample_submission.SalePrice = predictions\nsubmission = sample_submission.copy()\nsubmission.to_csv('submission2.csv',index=False)","df454c26":"### Categorical and Numerical Columns","d72eab03":"### Hyperparameter Turning\nWe will use Optuna to tune the hperparameters of our XGBRegressor Model.","e6b08bbb":"Let's use the optimized parameters to train our first model.","9dd4e45e":"### Spliting dataset for cross-validation\nWe will use K-Folds cross-validator from sklearn to split the training dataset into 10 random folds. You can decide any number of folds to split the data.","819f11f0":"Since the number of missing values in columns (Alley, FireplaceQu, PoolQC, Fence, MiscFeature) are huge, we will simply drop these columns.","7bfa1627":"### Now lets read our newly created train data with 10 fold\nUpload the created dataset and then read it.","c33dfc14":"### Preprossessing the Data\nLets create a function where we will impute the missing values and encode the categorical features in training, validation and test data.\nFor categorical features, we will replace the missing values with most frequent value.\nAnd for numerical features, we will replace the missing values with the median.\nWe will use SimpleImputer from sklearn for this task.","fbe9b113":"### Missing Values\nLet's see the number of unique and missing values in each column"}}