{"cell_type":{"849593ad":"code","2e90caa4":"code","870c89b3":"code","e0b3753f":"code","483f8d9b":"code","ed30c82b":"code","080ef2fe":"code","a491ba74":"code","4a4f3a3e":"code","4094af99":"code","4dd82d59":"code","3683b3b5":"code","909203bb":"code","659ad677":"code","72060a2b":"code","3f23acf7":"code","9c3404e5":"code","5b747fce":"code","5bb9f050":"code","e0ac20e4":"code","0172f42c":"code","0363f259":"markdown","9f813018":"markdown","041fcbab":"markdown"},"source":{"849593ad":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2e90caa4":"#IMPORT REQUIRED LIBRARIES \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport keras\nimport warnings \n\nimport tensorflow as tf\nfrom tensorflow.keras import layers","870c89b3":"#LOAD DATASET \ndf = pd.read_csv('\/kaggle\/input\/loan-data-csv\/loan_data.csv')","e0b3753f":"df.info()","483f8d9b":"df.head()","ed30c82b":"#EXPLORE VALUES FOR PURPOSE COLUMNS\ndf['purpose'].unique()","080ef2fe":"#ONE HOT ENCODING\npurpose = pd.get_dummies(df.purpose, drop_first=True)","a491ba74":"purpose","4a4f3a3e":"# CONCATINATE DATA TO THE MAIN DATAFRAME\ndf = pd.concat([df,purpose],axis=1)","4094af99":"df.shape\ndata = df","4dd82d59":"#DROP THE COLUMN PURPOSE AS WE HAVE GOT THE VALUES THROUGH OHC\ndata.drop('purpose',axis=1,inplace=True)","3683b3b5":"data.describe().T","909203bb":"# SPLIT THE DATA TO FEATURES AND TARGET \nX = data.drop('credit.policy',axis=1)\ny = data['credit.policy']","659ad677":"X.info()","72060a2b":"from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\ngb = GradientBoostingClassifier()\nrf = RandomForestClassifier()\ngb.fit(X,y)\nrf.fit(X,y)\nprint(gb.feature_importances_)\nprint(rf.feature_importances_)","3f23acf7":"# CORRELATION WITH THE DATA AVAILBLE \nplt.figure(figsize=(12,9))\nsns.heatmap(data.corr(),annot=True)\n","9c3404e5":"#PAIRPLOT\n#sns.pairplot(data,diag_kind='kde',hue='not.fully.paid')","5b747fce":"# SPLIT THE DATA TO TRAIN AND TEST\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)","5bb9f050":"# DEFINE MODEL\nmodel = tf.keras.models.Sequential()\n\n# MODEL INPUT \nmodel.add(tf.keras.layers.Reshape((18,),input_shape=(18,)))\nmodel.add(tf.keras.layers.BatchNormalization())\n\n#FIRST LAYER\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\n\n#SECOND LAYER\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\n\n#THIRD LAYER\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\n\n#FOURTH LAYER\n#model.add(tf.keras.layers.Dense(16, activation='relu'))\n#model.add(tf.keras.layers.BatchNormalization())\n\n# OUTPUT LAYER\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))","e0ac20e4":"sgd_optimizer = tf.keras.optimizers.Adam(lr=0.0005)\n\nmodel.compile(optimizer=sgd_optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n#model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])","0172f42c":"model.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=100, batch_size=32)","0363f259":"## Lending Club Loan Data Analysis\n\nDESCRIPTION\n\nCreate a model that predicts whether or not a loan will be default using the historical data.\n\n \n\nProblem Statement:  \n\nFor companies like Lending Club correctly predicting whether or not a loan will be a default is very important. In this project, using the historical data from 2007 to 2015, you have to build a deep learning model to predict the chance of default for future loans. As you will see later this dataset is highly imbalanced and includes a lot of features that makes this problem more challenging.\n\nDomain: Finance\n\nAnalysis to be done: Perform data preprocessing and build a deep learning prediction model. \n\nContent: \n\nDataset columns and definition:\n\n \n\ncredit.policy: 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise.\n\npurpose: The purpose of the loan (takes values \"credit_card\", \"debt_consolidation\", \"educational\", \"major_purchase\", \"small_business\", and \"all_other\").\n\nint.rate: The interest rate of the loan, as a proportion (a rate of 11% would be stored as 0.11). Borrowers judged by LendingClub.com to be more risky are assigned higher interest rates.\n\ninstallment: The monthly installments owed by the borrower if the loan is funded.\n\nlog.annual.inc: The natural log of the self-reported annual income of the borrower.\n\ndti: The debt-to-income ratio of the borrower (amount of debt divided by annual income).\n\nfico: The FICO credit score of the borrower.\n\ndays.with.cr.line: The number of days the borrower has had a credit line.\n\nrevol.bal: The borrower's revolving balance (amount unpaid at the end of the credit card billing cycle).\n\nrevol.util: The borrower's revolving line utilization rate (the amount of the credit line used relative to total credit available).\n\ninq.last.6mths: The borrower's number of inquiries by creditors in the last 6 months.\n\ndelinq.2yrs: The number of times the borrower had been 30+ days past due on a payment in the past 2 years.\n\npub.rec: The borrower's number of derogatory public records (bankruptcy filings, tax liens, or judgments).\n\n \n\nSteps to perform:\n\nPerform exploratory data analysis and feature engineering and then apply feature engineering. Follow up with a deep learning model to predict whether or not the loan will be default using the historical data.\n\n \n\nTasks:\n\n1.     Feature Transformation\n\nTransform categorical values into numerical values (discrete)\n\n2.     Exploratory data analysis of different factors of the dataset.\n\n3.     Additional Feature Engineering\n\nYou will check the correlation between features and will drop those features which have a strong correlation\n\nThis will help reduce the number of features and will leave you with the most relevant features\n\n4.     Modeling\n\nAfter applying EDA and feature engineering, you are now ready to build the predictive models\n\nIn this part, you will create a deep learning model using Keras with Tensorflow backend","9f813018":"WIth the above we could reach a accuracy of 96%","041fcbab":"## FIRST METHOD"}}