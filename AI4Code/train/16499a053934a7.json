{"cell_type":{"6e4b099f":"code","252bf574":"code","ca602192":"code","daf6be9d":"code","4f325dc0":"code","bcf97c58":"code","4c935b21":"code","c7f8541b":"code","dd953152":"code","72248a17":"code","952b1c1a":"code","6731b1ec":"code","bcbaab6d":"code","e3f75e92":"code","896860c8":"code","954dc5ac":"code","16192c4a":"code","809c4543":"code","27da369c":"code","bfe1f69e":"code","e404a333":"code","fba520ef":"code","099d2df9":"code","05ba5a70":"code","85705125":"code","ee3cd278":"code","e3fea7ff":"code","68632054":"code","97bcdc4a":"code","2643ad22":"code","b907c008":"code","7258aba4":"code","efbf8e1a":"code","8505aca5":"code","e3fe25ab":"code","e07b2132":"code","3285c385":"code","28ffb33c":"code","4f1968b0":"markdown","0e0efa3d":"markdown","26db99e4":"markdown","5694c898":"markdown","8166a70b":"markdown","6835e1dc":"markdown","985541ee":"markdown","3bc2c1f0":"markdown","d51d15d4":"markdown","4610b761":"markdown","ae1d6e69":"markdown","8b7b678a":"markdown","20fba395":"markdown","f1153cbc":"markdown","e2bb8416":"markdown","1e1d7ce6":"markdown","85bbac9f":"markdown","bdcda754":"markdown","88ac0d3f":"markdown","0221a2ab":"markdown"},"source":{"6e4b099f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, classification_report,roc_curve,auc,confusion_matrix\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.impute import KNNImputer","252bf574":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ca602192":"df=pd.read_csv('\/kaggle\/input\/hacked\/train.csv',parse_dates=['DATE'])\ndf=df.drop('INCIDENT_ID',axis=1)\ndf.head()","daf6be9d":"sns.countplot(df['MULTIPLE_OFFENSE'])","4f325dc0":"df['Day']=df['DATE'].dt.day_name()\ndf['Year']=df['DATE'].dt.year\ndf['Week']=df['DATE'].dt.week\n\nle=LabelEncoder()\ndf['Day']=le.fit_transform(df['Day'])\ndf=df.drop(['DATE'],axis=1)","bcf97c58":"from scipy.stats import chi2_contingency\nl=[]\ncategorical=['Day','Week','Year','X_1','X_4','X_5','X_9']\nfor i in categorical:\n    pvalue  = chi2_contingency(pd.crosstab(df['MULTIPLE_OFFENSE'],df[i]))[1]\n    l.append(1-pvalue)\nplt.figure(figsize=(7,5))\nsns.barplot(x=l, y=categorical)\nplt.title('Best Categorical Features')\nplt.axvline(x=(1-0.05),color='r')\nplt.show()","4c935b21":"print('Nan values in the columns')\ndf.isna().sum()","c7f8541b":"sns.boxplot(df['X_12'], orient='v')","dd953152":"cols=df.columns\nimp=KNNImputer(n_neighbors=5, missing_values=np.nan)\ndf=pd.DataFrame(imp.fit_transform(df.values),columns=cols)","72248a17":"from scipy.stats import ttest_ind\nnum=['X_3', 'X_7', 'X_2', 'X_6', 'X_8', 'X_10', 'X_11', 'X_12', 'X_13', 'X_14']\np=[]\n\nfor i in num:\n    df1=df.groupby('MULTIPLE_OFFENSE').get_group(0)\n    df2=df.groupby('MULTIPLE_OFFENSE').get_group(1)\n    t,pvalue=ttest_ind(df1[i],df2[i])\n    p.append(1-pvalue)\nplt.figure(figsize=(7,7))\nsns.barplot(x=p, y=num)\nplt.title('Best Numerical Features')\nplt.axvline(x=(1-0.05),color='r')\nplt.xlabel('1-p value')\nplt.show()","952b1c1a":"df=df.drop(['X_4','X_5','Week','X_13','X_7','X_6'],axis=1)","6731b1ec":"X=df.drop('MULTIPLE_OFFENSE',axis=1)\ny=df['MULTIPLE_OFFENSE']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","bcbaab6d":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","e3f75e92":"smt=SMOTE(random_state=10) #FOR CLASS IMBALANCE\nX_train, y_train = smt.fit_sample(X_train, y_train)","896860c8":"param_test1 = {\n 'max_depth':range(7,17,2),\n 'min_child_weight':range(1,6,2)\n}\ngsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n param_grid = param_test1, scoring='f1',n_jobs=4,iid=False, cv=3)\ngsearch1.fit(X_train,y_train)\ngsearch1.best_params_, gsearch1.best_score_","954dc5ac":"param_test3 = {\n 'gamma':[i\/10.0 for i in range(0,5)]\n}\ngsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=9,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test3, scoring='f1',n_jobs=4,iid=False, cv=5)\ngsearch3.fit(X_train,y_train)\ngsearch3.best_params_, gsearch3.best_score_","16192c4a":"param_test5 = {\n 'subsample':[i\/100.0 for i in range(75,90,5)],\n 'colsample_bytree':[i\/100.0 for i in range(75,90,5)]\n}\ngsearch5 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=9,\n min_child_weight=1, gamma=0.4, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test5, scoring='f1',n_jobs=4,iid=False, cv=5)\ngsearch5.fit(X_train,y_train)\ngsearch5.best_params_, gsearch5.best_score_","809c4543":"param_test7 = {\n 'reg_lambda':[0, 0.001, 0.005, 0.01, 0.05]\n}\ngsearch7 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=9,\n min_child_weight=1, gamma=0.4, subsample=0.8, colsample_bytree=0.75,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test7, scoring='f1',n_jobs=4,iid=False, cv=5)\ngsearch7.fit(X_train,y_train)\ngsearch7.best_params_, gsearch7.best_score_","27da369c":"param_test8 = {\n 'reg_lambda':[i\/1000.0 for i in range(9,20,1)]\n}\ngsearch8 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=9,\n min_child_weight=1, gamma=0.4, subsample=0.8, colsample_bytree=0.75,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test7, scoring='f1',n_jobs=4,iid=False, cv=5)\ngsearch8.fit(X_train,y_train)\ngsearch8.best_params_, gsearch8.best_score_","bfe1f69e":"xgb = XGBClassifier(\n learning_rate =0.01,\n n_estimators=800,\n max_depth=9,\n min_child_weight=1,\n gamma=0.4,\n subsample=0.8,\n colsample_bytree=0.75,\n reg_lambda=0.01,\n objective= 'binary:logistic',\n nthread=4)\nxgb.fit(X_train,y_train)\nypred=xgb.predict(X_test)\nrecall_score(ypred,y_test)","e404a333":"plt.figure(figsize=(7,10))\nsns.barplot(x=xgb.feature_importances_,y=X.columns)\nplt.title('Significant Features of the Final Model')\nplt.show()","fba520ef":"df=df.drop(['X_1','X_9','X_14'],axis=1)","099d2df9":"X=df.drop('MULTIPLE_OFFENSE',axis=1)\ny=df['MULTIPLE_OFFENSE']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","05ba5a70":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","85705125":"smt=SMOTE(random_state=10) #FOR CLASS IMBALANCE\nX_train, y_train = smt.fit_sample(X_train, y_train)","ee3cd278":"xgb = XGBClassifier(\n learning_rate =0.01,\n n_estimators=800,\n max_depth=9,\n min_child_weight=1,\n gamma=0.4,\n subsample=0.8,\n colsample_bytree=0.75,\n reg_lambda=0.01,\n objective= 'binary:logistic',\n nthread=4)\nxgb.fit(X_train,y_train)\nypred=xgb.predict(X_test)\nrecall_score(ypred,y_test)","e3fea7ff":"plt.figure(figsize=(7,10))\nsns.barplot(x=xgb.feature_importances_,y=X.columns)\nplt.title('Significant Features of the Final Model, after all the feature Engineering')\nplt.show()","68632054":"probs = xgb.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = roc_curve(y_test, preds)\nroc_auc = auc(fpr, tpr)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC-AUC Curve')\nplt.show()","97bcdc4a":"confusion_matrix(y_test,ypred)","2643ad22":"test=pd.read_csv('\/kaggle\/input\/hacked\/test.csv', parse_dates=['DATE'])\ntest.head()","b907c008":"test['Day']=test['DATE'].dt.day_name()\ntest['Year']=test['DATE'].dt.year\ntest['Week']=test['DATE'].dt.week","7258aba4":"le=LabelEncoder()\ntest['Day']=le.fit_transform(test['Day'])\nids=test['INCIDENT_ID']\ntest=test.drop(['INCIDENT_ID','DATE'],axis=1)","efbf8e1a":"cols=test.columns\nimp=KNNImputer(n_neighbors=5, missing_values=np.nan)\ntest=imp.fit_transform(test)\ntest=pd.DataFrame(test,columns=cols)","8505aca5":"test=test.drop(['X_1','X_4','X_5','Week','X_13','X_7','X_6','X_9','X_14'],axis=1)","e3fe25ab":"x=test.values","e07b2132":"x = scaler.transform(x)","3285c385":"pred=xgb.predict(x)","28ffb33c":"submission = pd.DataFrame({'INCIDENT_ID': ids,\n                           'MULTIPLE_OFFENSE':pred\n                           })\n\nsubmission.to_csv(\"final_sub.csv\",index=False)","4f1968b0":"## Test File","0e0efa3d":"We observe nan values in the 'X_12' column. Let's observe the distribution of the column.","26db99e4":"Its a highly skewed distribution, so we'll need a complex imputation strategy for the nan values.\n\nOut of the options of Simple Imputer, KNN Imputer, MICE or Iterative Imputer, we'll implement KNN Imputation for this purpose.","5694c898":"### Statistic tests for categorical variables","8166a70b":"From above feature importance graph, the features X_1, X_9 and X_14 were removed due to lesser contribution to prediction, except Day which reduced the F1 score of the final model.","6835e1dc":"Note: The above graph shows probability for accepting Alternative Hypothesis for visualisation representation purpose.","985541ee":"Lets search around 0.01 Lambda value","3bc2c1f0":"There's a huge class imbalance observed here. We'll try upsampling or Synthetic sampling techniques(SMOTE).","d51d15d4":"The algorithm has great a AUC Score, and has an impressive ROC-AUC Curve.","4610b761":"#### Putting all the best parameters from above exercise in our final classifier","ae1d6e69":"For dates, we'll extract day, year and week to see if there's any effect of either of them on the target variable.","8b7b678a":"Note: The above graph shows probability for accepting Alternative Hypothesis for visualisation representation purpose.","20fba395":"1. From statistical tests, the Day of the week has a significant impact in determing the target variable. The countplot doesn't show any trend (especially which on which day most offences occur), but certainly this variable is helpful, as also seen from the dramatic drop in performance upon its removal.\n2. Among the Logging parameters, X_10 variable has significant importance in predicting the target variable, followed by X_15 and X_12.\n3. X_1, X_9, X_14 didn't have significant contribution in the predictive power of the classifier.","f1153cbc":"## Hyperparameter Tuning of XGBoost","e2bb8416":"### Statistical test for Numerical columns","1e1d7ce6":"## Conclusion","85bbac9f":"We'll drop the below columns.","bdcda754":"We can see downward trend in number of cases observed, the maximum cases occuring in 2000. The trend is largely on a downturn, but is picking up in 2013. The cases where hack didn;t happen also shows upturn largely due to the sheer volume of cases.","88ac0d3f":"Small levels of overfit was solved by tuning the Lambda, which is the L2 regularisation parameter.","0221a2ab":"Since there are two target samples, we'll use T Test."}}