{"cell_type":{"089a63b1":"code","c7d4538d":"code","7f5b013f":"code","a18fcd6d":"code","8a3edc2f":"code","997c0de3":"code","cdabef7b":"code","98b9589c":"code","908c035c":"code","0a877d0c":"code","aa519d97":"code","b5f403b5":"code","d3c5673e":"code","a7c94e96":"code","14c25441":"code","817135a1":"code","776710b8":"code","a1f8cb5f":"code","9caf2d2e":"code","076e1518":"code","4def00b0":"code","56a25308":"code","3063e785":"code","b07d723a":"code","4ec35958":"code","6caff37f":"code","726b187a":"code","adef2893":"code","f92ba840":"code","e731e92b":"code","8453a558":"code","4bdcbd61":"code","cbe898c0":"code","9e04e7ab":"code","8e0a00a3":"markdown","505b5bfe":"markdown","ed898c92":"markdown","1b7d2993":"markdown","16d96e63":"markdown","8ec98006":"markdown","28b7b7ea":"markdown","c55ab566":"markdown","f16d5b46":"markdown","73897ea1":"markdown","45ac233e":"markdown","026e1b00":"markdown","5a6b27f4":"markdown","966abe96":"markdown","a62f33b4":"markdown","04b59dca":"markdown","68bac413":"markdown","5e13d8bc":"markdown","02599330":"markdown","620960a6":"markdown","34cf417b":"markdown","20258bec":"markdown","e20ac8e8":"markdown","c5398226":"markdown","7183a10c":"markdown","7b861389":"markdown","ddd21b61":"markdown","bae57cdd":"markdown"},"source":{"089a63b1":"import numpy as np\nimport pandas as pd","c7d4538d":"data=pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/train.csv\")","7f5b013f":"data.head()","a18fcd6d":"data.info()","8a3edc2f":"data.describe()","997c0de3":"for col in data.columns[1:20]:\n    print(\"unique values in {}:\\n\".format(col),data[col].unique())","cdabef7b":"ohd=pd.get_dummies(data, drop_first=True)\nohd.shape","98b9589c":"ohd.drop(\"id\",axis=1,inplace=True)\ny=ohd[\"target\"]\nx=ohd.drop(\"target\", axis=1, inplace=False)","908c035c":"from sklearn.model_selection import train_test_split","0a877d0c":"x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.15)","aa519d97":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()","b5f403b5":"x_tr = ss.fit_transform(x_train)","d3c5673e":"from lightgbm import LGBMClassifier\nlgbm=LGBMClassifier()","a7c94e96":"lgbm.fit(x_tr,y_train)","14c25441":"from sklearn.metrics import accuracy_score","817135a1":"x_ts = ss.transform(x_test)\ny_pred_tr=lgbm.predict(x_tr)\ny_pred_ts=lgbm.predict(x_ts)\ntrain_acc=accuracy_score(y_train,y_pred_tr)\ntest_acc=accuracy_score(y_test,y_pred_ts)\nprint(\"LGBM Results with OHE:\")\nprint(\"training accuracy = {}\".format(train_acc))\nprint(\"testing accuracy = {}\".format(test_acc))","776710b8":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.style as style\nstyle.use('seaborn-darkgrid')","a1f8cb5f":"feat_imp = lgbm.feature_importances_","9caf2d2e":"plt.figure(figsize=(25,7.5))\nsns.barplot(x.columns, feat_imp ,palette=\"cool_r\")\nplt.title(\"Feature Impotances\",fontsize=40)\nplt.xlabel(\"Features\",fontsize=30)\nplt.ylabel(\"Importance\",fontsize=30)","076e1518":"for t in [100,40,20,10,5,1]:\n    droplist=[]\n    for j in range(x.shape[1]):\n        if feat_imp[j]<t:\n            droplist.append(x.columns[j])\n    len(droplist)\n    x_sel=x.drop(droplist, axis=1, inplace=False)\n    print(\"Results for threshhold = {}\".format(t))\n    print(\"Shape of Dataframe: {}\".format(x_sel.shape))\n    x_train_sel, x_test_sel, y_train_sel, y_test_sel = train_test_split(x_sel,y,test_size=0.15)\n    x_tr_sel = ss.fit_transform(x_train_sel)\n    lgbm.fit(x_tr_sel,y_train_sel)\n    x_ts_sel = ss.transform(x_test_sel)\n    y_pred_tr=lgbm.predict(x_tr_sel)\n    y_pred_ts=lgbm.predict(x_ts_sel)\n    train_acc=accuracy_score(y_train,y_pred_tr)\n    test_acc=accuracy_score(y_test,y_pred_ts)\n    print(\"LGBM training accuracy = {}\".format(train_acc))\n    print(\"LGBM testing accuracy = {}\\n\".format(test_acc))","4def00b0":"from sklearn.decomposition import PCA\npca=PCA()","56a25308":"x_train_pca=pca.fit_transform(x_train)\nax=plt.figure(figsize=(25,20))\npca_features= list(range(0,pca.n_components_))\nsns.barplot(pca_features, pca.explained_variance_,palette=\"winter\")\nplt.title(\"Variation along PCA Components\", fontsize=40)\nplt.xlabel(\"Components\", fontsize=30)\nplt.ylabel(\"Variation\", fontsize=30)","3063e785":"for i in [15,35,100,200]:\n    pca=PCA(n_components=i)\n    x_tr = ss.fit_transform(x_train)\n    x_tr_pca=pca.fit_transform(x_tr)\n    lgbm.fit(x_tr_pca,y_train)\n    x_ts = ss.transform(x_test)\n    x_ts_pca=pca.fit_transform(x_ts)\n    y_pred_tr=lgbm.predict(x_tr_pca)\n    y_pred_ts=lgbm.predict(x_ts_pca)\n    train_acc=accuracy_score(y_train,y_pred_tr)\n    test_acc=accuracy_score(y_test,y_pred_ts)\n    print(\"PCA Components: {}\".format(i))\n    print(\"LGBM training accuracy = {}\".format(train_acc))\n    print(\"LGBM testing accuracy = {}\".format(test_acc))","b07d723a":"frqdata=pd.DataFrame()\nfor col in data.columns[20:]:\n    frqdata[col]=data[col]\nfor col in data.columns[1:20]:\n    d=data[col].value_counts().to_dict()\n    frqdata[col]=data[col].map(d)\/300000\nfrqdata.head()","4ec35958":"from sklearn.linear_model import SGDClassifier\nsgdc=SGDClassifier(max_iter=1000, tol=1e-3)","6caff37f":"x_frq = frqdata.drop(\"target\",axis=1,inplace=False)","726b187a":"x_train_frq, x_test_frq, y_train_frq, y_test_frq = train_test_split(x_frq,y,test_size=0.15)","adef2893":"sgdc.fit(x_train_frq,y_train_frq)\ny_pred_tr=sgdc.predict(x_train_frq)\ny_pred_ts=sgdc.predict(x_test_frq)\ntrain_acc=accuracy_score(y_train_frq,y_pred_tr)\ntest_acc=accuracy_score(y_test_frq,y_pred_ts)\nprint(\"SGDClassifier results with Frequency\/Count Encoding:\")\nprint(\"Without Feature Scaling:\")\nprint(\"training accuracy = {}\".format(train_acc))\nprint(\"testing accuracy = {}\".format(test_acc))\nx_train_frqs = ss.fit_transform(x_train_frq)\nsgdc.fit(x_train_frqs,y_train_frq)\ny_pred_tr=sgdc.predict(x_train_frqs)\nx_test_frqs = ss.fit_transform(x_test_frq)\ny_pred_ts=sgdc.predict(x_test_frqs)\ntrain_acc=accuracy_score(y_train_frq,y_pred_tr)\ntest_acc=accuracy_score(y_test_frq,y_pred_ts)\nprint(\"With Feature Scaling:\")\nprint(\"training accuracy = {}\".format(train_acc))\nprint(\"testing accuracy = {}\".format(test_acc))","f92ba840":"lgbm.fit(x_train_frq,y_train_frq)\ny_pred_tr=lgbm.predict(x_train_frq)\ny_pred_ts=lgbm.predict(x_test_frq)\ntrain_acc=accuracy_score(y_train_frq,y_pred_tr)\ntest_acc=accuracy_score(y_test_frq,y_pred_ts)\nprint(\"LGBM results with Frequency\/Count Encoding:\")\nprint(\"training accuracy = {}\".format(train_acc))\nprint(\"testing accuracy = {}\".format(test_acc))","e731e92b":"def gini_encoder(column,target,weighted=False,standardize='None'):\n    unique=column.unique()\n    total=column.shape[0]\n    gini_ind=dict()\n    \n    for i in unique:\n        x_i = column[column==i]\n        i_total = x_i.shape[0]\n        x_i_yes = x_i[target==1]\n        yes_count = x_i_yes.shape[0]\n        x_i_no = x_i[target==0]\n        no_count = x_i_no.shape[0]\n        gini = 1 - (yes_count\/i_total)**2 - (no_count\/i_total)**2\n        if weighted==True:\n            gini *= i_total\/total\n        gini_ind[i] = gini\n    \n    encoded = column.copy()\n    encoded = encoded.map(gini_ind)\n    \n    if standardize==\"mean\":\n        encoded = (encoded - encoded.mean())\/encoded.std()\n    elif standardize==\"median\":\n        encoded = (encoded - encoded.median())\/encoded.std()\n        \n    return encoded","8453a558":"gini_results_lgbm=pd.DataFrame(columns=[\"No Standardization\",\"Median Standardization\",\"Mean Standardization\"],\n                          index=[\"Training (Weighted)\",\"Testing (Weighted)\",\"Training (Unweighted)\",\"Testing (Unweighted)\"])\n\nweighted=[True,False]\nstandardize=[\"None\",\"median\",\"mean\"]\n\nfor w in weighted:\n    for s in standardize:\n        gini_data=pd.DataFrame()\n        for col in data.columns[20:]:\n            gini_data[col]=data[col]\n        for col in data.columns[1:20]:\n            gini_data[col]=gini_encoder(column=data[col],target=data[\"target\"], weighted=w, standardize=s)\n        x_gini=gini_data.drop(\"target\",axis=1)\n        x_train_gini, x_test_gini, y_train_gini, y_test_gini = train_test_split(x_gini,y,test_size=0.15)\n        lgbm.fit(x_train_gini,y_train_gini)\n        y_pred_tr=lgbm.predict(x_train_gini)\n        y_pred_ts=lgbm.predict(x_test_gini)\n        train_acc=accuracy_score(y_train_gini,y_pred_tr)\n        test_acc=accuracy_score(y_test_gini,y_pred_ts)\n        wi=weighted.index(w)\n        si=standardize.index(s)\n        \n        gini_results_lgbm.iloc[wi*2,si] = train_acc\n        gini_results_lgbm.iloc[1+wi*2,si] = test_acc","4bdcbd61":"print(\"LGBM Results with Gini Target Encoding:\")\ngini_results_lgbm","cbe898c0":"gini_results_sgdc=pd.DataFrame(columns=[\"No Standardization\",\"Median Standardization\",\"Mean Standardization\"],\n                          index=[\"Training (Weighted)\",\"Testing (Weighted)\",\"Training (Unweighted)\",\"Testing (Unweighted)\"])\n\nweighted=[True,False]\nstandardize=[\"None\",\"median\",\"mean\"]\n\nfor w in weighted:\n    for s in standardize:\n        gini_data=pd.DataFrame()\n        for col in data.columns[20:]:\n            gini_data[col]=data[col]\n        for col in data.columns[1:20]:\n            gini_data[col]=gini_encoder(column=data[col],target=data[\"target\"], weighted=w, standardize=s)\n        x_gini=gini_data.drop(\"target\",axis=1)\n        x_train_gini, x_test_gini, y_train_gini, y_test_gini = train_test_split(x_gini,data[\"target\"],test_size=0.15)\n        sgdc.fit(x_train_gini,y_train_gini)\n        y_pred_tr=sgdc.predict(x_train_gini)\n        y_pred_ts=sgdc.predict(x_test_gini)\n        train_acc=accuracy_score(y_train_gini,y_pred_tr)\n        test_acc=accuracy_score(y_test_gini,y_pred_ts)\n        wi=weighted.index(w)\n        si=standardize.index(s)\n        \n        gini_results_sgdc.iloc[wi*2,si] = train_acc\n        gini_results_sgdc.iloc[1+wi*2,si] = test_acc","9e04e7ab":"print(\"SGDClassifier Results with Gini Target Encoding:\")\ngini_results_sgdc","8e0a00a3":"### \u0633\u0628\u062d\u0627\u0646\u0643 \u0627\u0644\u0644\u0647\u0645 \u0648\u0628\u062d\u0645\u062f\u0643\u060c \u0623\u0634\u0647\u062f \u0623\u0646 \u0644\u0627 \u0625\u0644\u0647 \u0625\u0644\u0627 \u0623\u0646\u062a\u060c \u0623\u0633\u062a\u063a\u0641\u0631\u0643 \u0648\u0623\u0646\u0648\u0628 \u0625\u0644\u064a\u0643","505b5bfe":"So I just wrote a function (next cell) that implements this, with two options:  \n\n**Weighted**: If true, then each category will further be multiplied by its frequency.  \nThis could perhaps give better results as rare categories don't hold much information anyways.  \n\n**Standardize**: Name says it all.  \n\n### Feel free to copy and paste the following encoder in your work if you wish to use it, but please cite this notebook if you do.","ed898c92":"### Bottom line:\nTo be honest, I expected PCA and feature importance to improve results.  \nOHE certainly did increase dimensionality, but LGBM still did achieve reatively low bias and almost 0 variance.  \nLGBM also didn't take too long to train.  \nBut still, people often look for alternatives to OHE, and a smaller dataset takes less training time.  \nSo brace yourself, dear viewer, for frequency encoding!","1b7d2993":"We'll start with LGBM:","16d96e63":"Pretty much the same results as with OHE, but less training time.","8ec98006":"### That didn't work. How about PCA?","28b7b7ea":"##                                                                        \u0628\u0633\u0645 \u0627\u0644\u0644\u0647","c55ab566":"So I'm going to set a threshhold here, and everything below it will be thrown away.  \nI tried with ","f16d5b46":"## Target Encoding with Gini Index!\nYou just replace each category with its Gini index. The gini index basically tells you how much information that category tells you about the target variable.  \nOne could use entropy instead of gini, but gini -from what I've read- is faster to compute.","73897ea1":"## What I did:\n### First, I tried OHE.\nI ended up with more than 600 features.\nThe accuracy wasn't bad at all with LGBM, and it didn't overfit.\nI tried feature selection, but that considerably increased bias.\nPCA didn't do any good either, it only increased variance.\n### Then, I implemented frequency encoding.\nIt's when you replace each category in each categorical feature with its frequency.\nCategory Encoders (link at the end) is a library that implements count encoding, which is almost the same.\nBut I just hard coded it.\n### Finally, Target Encoding with Gini Index.\nTarget encoding is basically the replacement of each category with some kind of information that it tells about the target variable.\nFor continuous targets, it's usually the target mean for that category.\nFor categorical targets, the Category Encoders library uses, if I'm not mistaken, the posterior probability: P(target\/category).\nI've got a categorical (binary) target here, but instead of the posterior probability, I used Gini index.\nI just felt like trying it out, and it did well.","45ac233e":"### Would feature importance improve anything?","026e1b00":"High variance here.","5a6b27f4":"I'll be using LGBM for this one as it's fast.","966abe96":"That's all there is to it.  \nNow we train a model.  \nI'll be using the SGD Classifier from sklearn.  \nIt's a fast linear model that uses stochastic gradient descent.  \nIt seemed interesting when I read about it so I felt like trying it out.  \nI also expect standardization to make a difference, and LGBM isn't affected by it whereas SGDC is.  \nPS: As it wouldn't make sense to compare two encoding techniques with different models, I'll also try LGBM.","a62f33b4":"### Frequency Encoding!\nYou just replace categories with their frequencies. Simple as that.  \nIt's the same as count encoding except that you need to divide counts by the total number of samples to get frequencies.  \nIf you plan to standardize your data then counts and frequencies would give the same results.  \nI didn't use the categorical encoders library, I felt like doing it myself.","04b59dca":"BIAS.","68bac413":"Weighting only increased bias.  \nMedian Standardization slightly decreased variance.  ","5e13d8bc":"OHE will blow this up to 600+ dimensions.","02599330":"# **Hey there.**\n### In this notebook I experimented with a couple of encoding techniques that one can use when OHE increases dimensionality too much.","620960a6":"### Well, there you go, 2 alternatives to OHE.  \n### Use them wisely.\n### There are several other techniques that you can implement with the following library: https:\/\/contrib.scikit-learn.org\/category_encoders\/","34cf417b":"Surprisingly, weighting made no considerable difference.  \nSame with Standardization, but LGBM is already insensitive to scale.","20258bec":"## One-Hot Encoding","e20ac8e8":"## Data:\n### The usual tabular playground series, March 2021. These TPS datasets are really good for experimentation.\n### 300000 samples, 31 predictor features, and a binary target.","c5398226":"## Importing and Exploring the Data. ","7183a10c":"Beyond about 200 components, pretty much all is noise.","7b861389":"Surprisingly good results.  \nIt also didn't take long to train, but LGBM is fast.  \nIt would probably take much longer with other models.  ","ddd21b61":"Results are good, but I certainly didn't expect scaling to increase bias.  \nAnyways, let's try LGBM.","bae57cdd":"##### PS: It only works for a binary target.  \n##### You would need to make some adjustments for multi-class targets."}}