{"cell_type":{"8d5ece4e":"code","03aea244":"code","21775a00":"code","afdf39ad":"code","37d31862":"code","4941f9e1":"code","fb9e1d08":"code","0a51614d":"code","84c8ce43":"code","5b382cca":"code","e85ddd52":"code","0357fe3e":"markdown","2261cb39":"markdown","2436947d":"markdown","bd7811ef":"markdown","8997220c":"markdown","0a1d2b2c":"markdown","9b103044":"markdown","c841ad13":"markdown","a37fcdb7":"markdown"},"source":{"8d5ece4e":"print('Importing synthetic image generation script, which also installs RDKIt and a few other utilities.')\nprint('Please wait as this may take a while\u2026')\nfrom bms_synth import random_molecule_image\nprint('Done.')","03aea244":"import copy\nfrom datetime import datetime\nfrom io import BytesIO\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom skimage.io import imread\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torchvision.transforms import Pad, RandomCrop\nimport torch.nn.functional as F\n\nfrom PIL import Image\nfrom rdkit import Chem\n\nfrom matplotlib import pyplot as plt\nimport ipywidgets as widgets\n\nfrom bms_synth import stretch_image","21775a00":"DEVICE = 'cuda'\n\nTRAINING_OPTIONS = {\n    'num_iterations': 8000,\n    'margin_fraction': 0.025,\n    'patch_size': 96,\n    'patches_per_image': 10,\n    'batch_size': 15,\n    'edge_loss_weight': 0.8,\n    'white_pixel_weight': None,  # don't use - gives poor results\n    'white_pixel_threshold': 1e-3,\n    'device': DEVICE,\n    'model_name': 'cnsnet3',\n    'run_name': 'notebook001'\n}\n\nMODEL_OPTIONS = {\n    'training_options': copy.deepcopy(TRAINING_OPTIONS),\n    'use_batch_norm': False,  # False is best\n    'use_separable_convolutions': False,  # False is best\n    'use_global_pooling': True  # Global Average Pooling - True is best\n}","afdf39ad":"PROJECT_DIR = Path('..')\nINPUT_DIR = PROJECT_DIR \/ 'input'\nOUTPUT_DIR = Path('.')\nBMS_INPUT_DIR = INPUT_DIR \/ 'bms-molecular-translation'\nTRAIN_DATA_PATH = BMS_INPUT_DIR \/ 'train'\nTRAIN_LABELS_PATH = BMS_INPUT_DIR \/ 'train_labels.csv'","37d31862":"print('Reading training labels\u2026')\nTRAIN_LABELS = pd.read_csv(TRAIN_LABELS_PATH)\nprint(f'Read {len(TRAIN_LABELS)} training labels.')","4941f9e1":"def image_generator(num=None):\n    mol_index = np.random.randint(len(TRAIN_LABELS))\n    mol_id, inchi = TRAIN_LABELS['image_id'][mol_index], TRAIN_LABELS['InChI'][mol_index]\n    mol = Chem.inchi.MolFromInchi(inchi)\n    count = 0\n    while True:\n        img, orig_bond_img, orig_atom_img = random_molecule_image(mol, margin_fraction=TRAINING_OPTIONS['margin_fraction'])\n        yield (img, orig_bond_img, orig_atom_img)\n        count +=1\n        if num is not None and count == num:\n            return\n\n\ndef patch_generator(num=None, dtype=torch.float32):\n    patch_size = TRAINING_OPTIONS['patch_size']\n    patches_per_image = TRAINING_OPTIONS['patches_per_image']\n    cropper = RandomCrop(patch_size, pad_if_needed=True)\n    if num is None:\n        num_images = None\n    else:\n        num_images = num \/\/ patches_per_image\n        if num % patches_per_image != 0:\n            num_images += 1\n    patch_count = 0\n    for img, orig_bond_img, orig_atom_img in image_generator(num_images):\n        img = torch.tensor(img, dtype=dtype)\n        orig_bond_img = torch.tensor(orig_bond_img, dtype=dtype)\n        orig_atom_img = torch.tensor(orig_atom_img, dtype=dtype)\n        h, w = img.shape\n        # We may need to pad images that cannot fit a patch\n        if h < patch_size or w < patch_size:\n            dh, dw = max(0, patch_size - h), max(0, patch_size - w)\n            top_pad, left_pad = dh \/\/ 2, dw \/\/ 2\n            bottom_pad, right_pad = dh - top_pad, dw - left_pad\n            pad = Pad([left_pad, top_pad, right_pad, bottom_pad])\n            img = pad(img)\n            orig_bond_img = pad(orig_bond_img)\n            orig_atom_img = pad(orig_atom_img)\n        for _ in range(patches_per_image):\n            i, j, h, w = RandomCrop.get_params(img, (patch_size, patch_size))\n            img_patch, bond_patch, atom_patch = img[i:i+h, j:j+w], orig_bond_img[i:i+h, j:j+w], orig_atom_img[i:i+h, j:j+w]\n            yield img_patch, bond_patch, atom_patch\n            patch_count += 1\n            if num is not None and patch_count == num:\n                return\n\ndef image_widget(a, greyscale=True):\n    img_bytes = BytesIO()\n    img_pil = Image.fromarray(a)\n    if greyscale:\n        img_pil = img_pil.convert(\"L\")\n    else:\n        img_pil = img_pil.convert(\"RGB\")\n    img_pil.save(img_bytes, format='PNG')\n    return widgets.Image(value=img_bytes.getvalue(), margin='1em;')\n\ndef test_patch_generator(num):\n    inchi = 'InChI=1S\/C15H15F2NS\/c1-10(18-2)11-6-7-15(14(17)8-11)19-13-5-3-4-12(16)9-13\/h3-10,18H,1-2H3'\n    for im, bond_im, atom_im  in patch_generator(num):\n        #plt.figure(figsize=(4, 4\/3)); plt.imshow(torch.cat([bond_im, atom_im, im], axis=-1), cmap='gray_r', interpolation='none'); plt.show()\n        border = '1px solid pink'\n        widget1 = image_widget((1 - im).numpy()*255)\n        widget1.layout.border = border\n        widget2 = image_widget((1 - bond_im).numpy()*255)\n        widget2.layout.border = border\n        widget3 = image_widget((1 - atom_im).numpy()*255)\n        widget3.layout.border = border\n        image_row = widgets.HBox([widget1, widget2, widget3])\n        display(widgets.VBox([widgets.Label('training input + label (bonds + atoms)'), image_row]))\n    return 'Done.'\n\n\ntest_patch_generator(num=10)","fb9e1d08":"class BaseRes(nn.Module):\n    \n    # resize_power - must be 0, -1 (downscale \u00d72) or 1 (upscale \u00d72).\n    def __init__(\n        self,\n        in_channels, resize_power, out_channels, kernel_size,\n        padding, groups, bias,\n        separable, use_batch_norm, use_global_pooling\n    ):\n        super().__init__()\n        if groups is None:\n            if separable:\n                groups = in_channels\n            else:\n                groups = 1\n        if separable:\n            bias = None\n        if resize_power == 0:\n            self.pooling = 1  # Effectively, that is no pooling.\n            self.stride = 1\n            if padding is None:\n                padding = 1\n            if out_channels is None:\n                out_channels = in_channels\n        elif resize_power == -1:\n            self.pooling = 2\n            self.stride = 2\n            if padding is None:\n                padding = 1\n            if out_channels is None:\n                out_channels = 2*in_channels\n        elif resize_power == 1:\n            self.pooling = 1  # Effectively, that is no pooling.\n            self.stride = 2\n            if kernel_size is None:\n                kernel_size = 2\n            if padding is None:\n                padding = 0\n            if out_channels is None:\n                out_channels = in_channels \/\/ 2\n        else:\n            raise Exception('resize_power must be -1, 0 or 1')\n        if kernel_size is None:\n            kernel_size = 3\n        if self.pooling > 1:\n            self.pool = nn.MaxPool2d(self.pooling)\n        else:\n            self.pool = None\n        if resize_power < 1:\n            conv_class = nn.Conv2d\n        else:\n            conv_class = nn.ConvTranspose2d\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        conv = conv_class(in_channels, out_channels, stride=self.stride, kernel_size=kernel_size, padding=padding, groups=groups, bias=bias)\n        if use_batch_norm:\n            self.conv = nn.Sequential(conv, nn.BatchNorm2d(out_channels))\n        else:\n            self.conv = conv\n        # pointwise conv\n        self.use_global_pooling = use_global_pooling\n        if use_global_pooling:\n            global_avg_pooling_channels = out_channels\n            self.global_pooling = nn.AdaptiveAvgPool2d(1)\n        else:\n            global_avg_pooling_channels = 0\n            self.global_pooling = None\n        skip_channels = in_channels  # the skip connection shortcuts the input channels\n        # We mix down the extra channels with a pointwise convolution so we get out_channels of output.\n        mix_in_channels = out_channels + skip_channels + global_avg_pooling_channels\n        mix_conv = nn.Conv2d(mix_in_channels, out_channels, kernel_size=1)\n        if use_batch_norm:\n            self.mix = nn.Sequential(mix_conv, nn.BatchNorm2d(out_channels))\n        else:\n            self.mix = mix_conv\n    \n    def forward(self, x):\n        if self.pool is None:\n            skip = x\n        else:\n            skip = self.pool(x)\n        x = F.relu(self.conv(x))\n        if self.global_pooling is not None:\n            # throw some mean channels into the final mix\n            means_per_channel = self.global_pooling(x)  # shape: N x C x 1 x 1\n            global_avg_planes = means_per_channel*torch.ones_like(x)  # restored to: N x C x H x W\n            x = torch.cat([x, global_avg_planes], axis=1)  # cat'd to: N x 2*C x H x W\n        x = torch.cat((x, skip), axis=1)\n        x = self.mix(x)\n        return x\n\n\nclass ThruRes(BaseRes):\n    \n    def __init__(\n        self, in_channels,\n        out_channels=None,\n        kernel_size=None,\n        padding=None,\n        groups=None,\n        bias=True,\n        separable=False,\n        use_batch_norm=True,\n        use_global_pooling=False\n    ):\n        super().__init__(\n            in_channels,\n            resize_power=0,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            padding=padding,\n            groups=groups,\n            bias=bias,\n            separable=separable,\n            use_batch_norm=use_batch_norm,\n            use_global_pooling=use_global_pooling\n        )\n\n    \nclass ResPair(nn.Module):\n    \n    def __init__(self, first, second, use_batch_norm=True):\n        super(ResPair, self).__init__()\n        self.first = first\n        self.second = second\n        self.in_channels = first.in_channels\n        self.out_channels = second.out_channels\n        assert self.out_channels % self.in_channels == 0\n        self.pooling = first.pooling * second.pooling\n        if self.pooling > 1:\n            self.pool = nn.MaxPool2d(self.pooling)\n        # pointwise conv\n        mix_conv = nn.Conv2d(self.out_channels + self.in_channels, self.out_channels, kernel_size=1)\n        if use_batch_norm:\n            self.mix = nn.Sequential(mix_conv, nn.BatchNorm2d(self.out_channels))\n        else:\n            self.mix = mix_conv\n            \n    \n    def forward(self, x):\n        skip = x\n        if self.pooling > 1:\n            skip = self.pool(x)\n        x = self.second(self.first(x))\n        x = torch.cat((x, skip), axis=1)\n        x = self.mix(x)\n        return x","0a51614d":"class CleanAndSepNet3(nn.Module):\n    \n    def __init__(self, options):\n        super().__init__()\n        use_batch_norm = options['use_batch_norm']\n        separable = options['use_separable_convolutions']\n        use_global_pooling = options['use_global_pooling']\n        def thru(out_channels):\n            return ThruRes(out_channels, use_batch_norm=use_batch_norm, separable=separable, use_global_pooling=use_global_pooling)\n        def respair(module1, module2):\n            return ResPair(module1, module2, use_batch_norm=use_batch_norm)\n        def convpair(out_channels):\n            return respair(thru(out_channels), thru(out_channels))\n        intro_conv = ThruRes(1, out_channels=32, kernel_size=7, padding=3, use_batch_norm=use_batch_norm, separable=separable, use_global_pooling=use_global_pooling)\n        self.encoder = respair(\n            respair(\n                respair(intro_conv, thru(32)),\n                convpair(32)\n            ),\n            respair(\n                convpair(32),\n                convpair(32)\n            )\n        )\n        # pointwise convolution to extract the 2 output planes\n        self.decoder = nn.Conv2d(32, 2, kernel_size=1, padding=0)\n   \n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x","84c8ce43":"def batch_generator(batch_size):\n    patches_per_image = TRAINING_OPTIONS['patches_per_image']\n    patcher = patch_generator()\n    is_first = True\n    for im, bond_im, atom_im in patcher:\n        if is_first or batch_item_count == batch_size:\n            if is_first:\n                is_first = False\n            else:\n                yield torch.stack(input_batch, axis=0), torch.stack(label_batch, axis=0)\n            input_batch = []\n            label_batch = []\n            batch_item_count = 0\n        h, w = im.shape\n        # Augment with rotations\n        # We prefer no relfections for now because the original dataset does not seem to any characters mirrored.\n        krot = np.random.randint(4)\n        if krot > 0:\n            im = torch.rot90(im, krot, (0, 1))\n            bond_im = torch.rot90(bond_im, krot, (0, 1))\n            atom_im = torch.rot90(atom_im, krot, (0, 1))\n        im = im.reshape(1, h, w)\n        bond_im = bond_im.reshape(1, h, w)\n        atom_im = atom_im.reshape(1, h, w)\n        label_im = torch.cat([bond_im, atom_im], axis=0)\n        input_batch.append(im)\n        label_batch.append(label_im)\n        batch_item_count += 1\n\n\ndef mse_loss(label, prediction):\n    diff2 = (label - prediction)**2\n    return diff2.mean()\n\n\ndef weighted_loss(error_values, white_pixels, white_pixel_weight=0.5):\n    black_pixel_weight = 1 - white_pixel_weight\n    black_pixels = 1 - white_pixels\n    white_pixels_norm = white_pixels.sum()\n    black_pixels_norm = black_pixels.sum()\n    err_white = (error_values*white_pixels).sum()*white_pixel_weight\n    err_black = (error_values*black_pixels).sum()*black_pixel_weight\n    return err_white\/white_pixels_norm + err_black\/black_pixels_norm\n\n\ndef mse_loss_weighted(label, prediction, white_threshold=1e-3, white_pixel_weight=0.5):\n    white_pixels = (label > white_threshold)*1\n    diff2 = (label - prediction)**2\n    return weighted_loss(diff2, white_pixels, white_pixel_weight=0.5)\n\n\ndef train(model_factory, options, num_iters=None, report_every=100):\n    options = copy.deepcopy(options)\n    training_options = options['training_options']\n    model_name = training_options['model_name']\n    run_name = training_options['run_name']\n    device = training_options['device']\n    batch_size = training_options['batch_size']\n    t0 = datetime.now()\n    model = model_factory(options).to(device)\n    training_options['model'] = str(model)\n    net_type = model.__class__.__name__\n    options['net_type'] = net_type\n    optimiser = optim.Adam(model.parameters())\n    optimiser_type = optimiser.__class__.__name__\n    options['optimiser_type'] = optimiser_type\n    iter_count = 0\n    loss_buffer = []\n    print(f'Training \"{model_name}\" ({net_type}) \"{run_name}\" on {device} with minibatch size {batch_size}, optimised with {optimiser_type}\u2026\\n')\n    for inputs, labels in batch_generator(batch_size):\n        labels = labels\n        iter_count += 1\n        optimiser.zero_grad()\n        outputs = model(inputs.to(device)).cpu()\n        diff = outputs - labels\n        loss_edge_weight = training_options['edge_loss_weight']\n        white_pixel_weight = training_options['white_pixel_weight']\n        if white_pixel_weight is None:\n            mse_loss_pixels = mse_loss(labels, outputs)  #(diff**2).mean()\n        else:\n            white_threshold = training_options['white_pixel_threshold']\n            white_pixels = (labels > white_threshold)*1\n            mse_loss_pixels = mse_loss(labels, outputs)  #(diff**2).mean()\n        if loss_edge_weight is None:\n            loss = mse_loss_pixels\n        else:\n            dlabels_y = labels[:, :, 1:] - labels[:, :, :-1]\n            doutputs_y = outputs[:, :, 1:] - outputs[:, :, :-1]\n            dlabels_x = labels[:, :, :, 1:] - labels[:, :, :, :-1]\n            doutputs_x = outputs[:, :, :, 1:] - outputs[:, :, :, :-1]\n            if white_pixel_weight is None:\n                loss_edge_y = mse_loss(dlabels_y, doutputs_y)\n                loss_edge_x = mse_loss(dlabels_x , doutputs_x)\n            else:\n                loss_edge_y = weighted_loss((dlabels_y - doutputs_y)**2, white_pixels=1*(dlabels_y > white_threshold), white_pixel_weight=white_pixel_weight)\n                loss_edge_x = weighted_loss((dlabels_x - doutputs_x)**2, white_pixels=1*(dlabels_x > white_threshold), white_pixel_weight=white_pixel_weight)\n            loss_edge = (loss_edge_y + loss_edge_x)\/2\n            loss = (1 - loss_edge_weight)*mse_loss_pixels + loss_edge_weight*loss_edge\n        loss.backward()\n        optimiser.step()\n        loss_buffer.append(loss.detach().cpu().item())\n        if iter_count % (report_every\/\/10) == 0:\n            print('.', end='')\n        if iter_count == 1 or iter_count % report_every == 0:\n            print(f'\\n######################## ITERATION #{iter_count}')\n            print(f'Mean training loss (n={len(loss_buffer)}): {np.mean(loss_buffer)}')\n            print(f'Elapsed time: {(datetime.now() - t0).total_seconds()\/60:0.2f} minutes.')\n            outputs_np = outputs.detach().cpu().numpy()\n            outputs_bonds_np = outputs_np[0, 0]\n            outputs_atoms_np = outputs_np[0, 1]\n            outputs_combined_np = outputs_bonds_np + outputs_atoms_np\n            _, axs = plt.subplots(1, 6, figsize=(6*2.5, 2.5), facecolor='#eee');\n            axs[0].imshow(inputs.detach().cpu().numpy()[0, 0], vmin=0, vmax=1, cmap='gray_r')\n            axs[0].set_title('input')\n            axs[1].imshow(labels.detach().cpu().numpy()[0, 0], vmin=0, vmax=1, cmap='magma')\n            axs[1].set_title('label (bonds)')\n            axs[2].imshow(labels.detach().cpu().numpy()[0, 1], vmin=0, vmax=1, cmap='magma')\n            axs[2].set_title('label (atoms)')\n            axs[3].imshow(outputs_bonds_np, vmin=0, vmax=1, cmap='magma')\n            axs[3].set_title('output (bonds)')\n            axs[4].imshow(outputs_atoms_np, vmin=0, vmax=1, cmap='magma')\n            axs[4].set_title('output (atoms)')\n            axs[5].imshow(outputs_combined_np, vmin=0, vmax=1, cmap='gray_r')\n            axs[5].set_title('output')\n            for ax in axs:\n                ax.axis('off')\n            plt.show()\n            loss_buffer = []\n        if num_iters is not None and iter_count == num_iters:\n            break\n    print(f'Time taken: {(datetime.now() - t0).total_seconds()\/60:0.2f} minutes.')\n    print('\\nSaving model\u2026')\n    model = model.cpu()\n    model_file_name = f'{model_name}_{run_name}.pt'\n    model_path = OUTPUT_DIR \/ model_file_name\n    if model_path.exists():\n        print(f'WARNING: model file {model_path} already exists. Adding a timestamp to the file name and continuing\u2026')\n        model_file_name = f'{model_name}_{run_name}_{timestamp()}.pt'\n        model_path = OUTPUT_DIR \/ model_file_name\n    torch.save({'model': model.state_dict(), 'options': options}, model_path)\n    print(f'Saved to {model_path}.')\n    print('Done.')\n    return model_path\n\n\nCNS_NET_PATH = train(lambda options: CleanAndSepNet3(options), MODEL_OPTIONS, TRAINING_OPTIONS['num_iterations'], report_every=500)","5b382cca":"def load_model(filename):\n    path = OUTPUT_DIR \/ filename\n    loaded = torch.load(path)\n    print(f'Loaded model from {path}.')\n    options = loaded['options']\n    model = CleanAndSepNet3(options)\n    model.load_state_dict(loaded['model'])\n    return model\n\n\ndef image_widget(a, greyscale=True):\n    img_bytes = BytesIO()\n    img_pil = Image.fromarray(a)\n    if greyscale:\n        img_pil = img_pil.convert(\"L\")\n    else:\n        img_pil = img_pil.convert(\"RGB\")\n    img_pil.save(img_bytes, format='PNG')\n    return widgets.Image(value=img_bytes.getvalue(), margin='1em;')\n\n\ndef prepare_image(img, size=None):\n    h, w = img.shape\n    # we need to pad to dimensions that are multiples of 16\n    needs_pad = False\n    if size is None:\n        if h % 16 == 0:\n            new_h = h\n        else:\n            new_h = ((h \/\/ 16) + 1)*16\n            needs_pad = True\n        if w % 16 == 0:\n            new_w = w\n        else:\n            new_w = ((w \/\/ 16) + 1)*16\n            needs_pad = True\n    else:\n        if isinstance(size, int):\n            size = (size, size)\n        new_h, new_w = size\n        if h != size:\n            needs_pad = True\n        if w != size:\n            needs_pad = True\n    if needs_pad:\n        margin_h = new_h - h\n        margin_top = margin_h\/\/2\n        margin_bottom = margin_h - margin_top\n        margin_w = new_w - w\n        margin_left = margin_w\/\/2\n        margin_right = margin_w - margin_left\n        padding = ((margin_top, margin_bottom), (margin_left, margin_right))\n        img = np.pad(img, padding)\n    else:\n        padding = ((0, 0), (0, 0))\n    return img, padding\n\n\ndef evaluate_model(model, img):\n    img, _ = prepare_image(img)\n    h, w = img.shape\n    inputs = torch.tensor(img.reshape(1, 1, h, w), dtype=torch.float32)\n    outputs = model(inputs).numpy()\n    bond_img, atom_img = outputs[0, :2]\n    return img, bond_img, atom_img\n\n\ndef sanity_check_model(model):\n    mol_index = np.random.randint(len(TRAIN_LABELS))\n    mol_id = TRAIN_LABELS['image_id'][mol_index]\n    mol_train_img_path = TRAIN_DATA_PATH \/ mol_id[0] \/mol_id[1] \/ mol_id[2] \/ (mol_id + '.png')\n    img = imread(mol_train_img_path)\n    img = 1 - img\/255.0\n    with torch.no_grad():\n        img, bond_img, atom_img = evaluate_model(model, img)\n    output_img = bond_img + atom_img\n    #output_img = np.clip(output_img, 0, 1)\n    #output_img = stretch_image(output_img)\n    #output_img = output_img*(output_img > 0.1)\n    border = '1px solid pink'\n    widget1 = image_widget(255*(1 - img))\n    widget1.layout.border = border\n    widget2 = image_widget(255*(1 - output_img))\n    widget2.layout.border = border\n    img_row = widgets.HBox([\n        widgets.VBox([widgets.Label('Input'), widget1]),\n        widgets.VBox([widgets.Label('Output'), widget2])\n    ])\n    display(img_row)\n    widget1 = image_widget(255*(1 - bond_img))\n    widget1.layout.border = border\n    widget2 = image_widget(255*(1 - atom_img))\n    widget2.layout.border = border\n    img_row = widgets.HBox([\n        widgets.VBox([widgets.Label('Bond Output Layer'), widget1]),\n        widgets.VBox([widgets.Label('Atom Output Layer'), widget2])\n    ])\n    display(img_row)\n    print(f'No. of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n\n\nwith torch.no_grad():\n    sanity_check_model(load_model(CNS_NET_PATH))","e85ddd52":"# TODO: run batches?\ndef validate_model(model, num=100):\n    print(f'Validating {num} images\u2026')\n    t0 = datetime.now()\n    measures = {\n        'bond_mse': [],\n        'atom_mse': [],\n        'mse': [],\n        'mse_weighted': []\n    }\n    for mol_index in range(num):\n        mol_id, inchi = TRAIN_LABELS['image_id'][mol_index], TRAIN_LABELS['InChI'][mol_index]\n        mol = Chem.inchi.MolFromInchi(inchi)\n        img, bond_img, atom_img = random_molecule_image(mol, margin_fraction=TRAINING_OPTIONS['margin_fraction'])\n        h, w = img.shape\n        bond_img = bond_img.reshape(1, h, w)\n        atom_img = atom_img.reshape(1, h, w)\n        label = np.concatenate((bond_img, atom_img), axis=0)\n        orig_img = img\n        img, padding = prepare_image(img)\n        h_padded, w_padded = img.shape\n        inputs = torch.tensor(img.reshape(1, 1, h_padded, w_padded), dtype=torch.float32)\n        outputs = model(inputs).numpy()\n        # unpad\n        (margin_top, margin_bottom), (margin_left, margin_right) = padding\n        if h_padded != h:\n            outputs = outputs[:, :, margin_top:-margin_bottom, :]\n        if w_padded != w:\n            outputs = outputs[:, :, :, margin_left:-margin_right]\n        prediction = outputs[0, :]\n        pred_bond_img = outputs[0, 0]\n        pred_atom_img = outputs[0, 1]\n        pred_img = np.clip(pred_bond_img + pred_atom_img, 0, 1)\n        # find losses\n        measures['bond_mse'].append(mse_loss(pred_bond_img, bond_img))\n        measures['atom_mse'].append(mse_loss(pred_atom_img, atom_img))\n        assert prediction.shape == label.shape, f'{prediction.shape} v. {label.shape}'\n        mse = mse_loss(label, prediction)\n        mse_weighted = mse_loss_weighted(label, prediction, white_pixel_weight=0.95)\n        measures['mse'].append(mse)\n        measures['mse_weighted'].append(mse_weighted)\n        print('.', end='')\n    measures['ave_bond_mse'] = np.mean(measures['bond_mse'])\n    measures['ave_atom_mse'] = np.mean(measures['atom_mse'])\n    measures['ave_mse'] = np.mean(measures['mse'])\n    measures['ave_mse_weighted'] = np.mean(measures['mse_weighted'])\n    print()\n    print()\n    print(f'Average MSE: {measures[\"ave_mse\"]:0.8f}')\n    print(f'Average weighted MSE: {measures[\"ave_mse_weighted\"]:0.8f}')\n    print()\n    print(f'Finished. Time taken: {(datetime.now() - t0).total_seconds()\/60:0.2f} minutes.')\n\n\nwith torch.no_grad():\n    validate_model(load_model(CNS_NET_PATH), 500)","0357fe3e":"## Data generation\nDuring training, I grab random InChI IDs from the training set, using each of these to generate a synthetic 'degraded' image. I take a number of random patches from each image and supply these for training. Each patch has three layers: one for the degraded image - use for training inputs and a pair the bonds and the atom labels, which are used as labels.","2261cb39":"## Validation","2436947d":" Below I install the same synthetic image generator that's used in my notebook https:\/\/www.kaggle.com\/stainsby\/improved-synthetic-data-for-bms-competition-v3.","bd7811ef":"## Training\n\nAfter trying simple mean squared error loss between output and label pixels, I found that training was greatly improved by adding an MSE loss for the *differences* between neighbouring pixels (essentialy the first derivatives along each spacial dimension). This appears to give the resulting outputs cleaner edges. The 'edge loss' is so useful that it is weighted by 80% compared to the pixel MSE. You experiment with this yourself by changing the `edge_loss_weight` value in `TRAINING_OPTIONS`.\n\nThere is also a `white_pixel_weight` option to add weight to the pixels corresponding to non-background regions in the labels when calculating the MSEs, but I didn't find that this improved the results, so it is turned off.","8997220c":"## Purpose\n\nI present a small, custom CNN to remove noise from the images in the BMS dataset. The model has about 100K trainable parameters. It's not powerful enough to make missing bonds reappear, or to fix badly degraded atom labels.\n\nI hope you find this model useful for:\n* preprocessing the BMS datase\n* feature extraction\n* the beginning of a more complex model\n* cleaning other molecule images?\n* anything else (let me know!)\n\nThe model is trained using new synthetic data that's generated during training. Importantly, this model also perfoms well on the original BMS dataset images, meaninsg that we can use it to preprocess both original and newly generated images and be confident of consistent outptus to feed into later networks.\n\nAs an additional task, the CNN also separates the atom label text from the bond lines. This demonstrates that the network understands enough to be useful not only for cleaning data, but also for localised feature extraction.\n\n## Example result\n\nFrom this image in the original BMS dataset:\n\n![bms_dataset_image.png](attachment:bms_dataset_image.png)\n\nwe get:\n\n![bms_repaired_dataset_image.png](attachment:bms_repaired_dataset_image.png)\n\nAs a bonus, the atoms and bonds are outputted (partly successfully) as separate layers:\n\n![bms_atom_dataset_image.png](attachment:bms_atom_dataset_image.png)\n\n![bms_bond_dataset_image.png](attachment:bms_bond_dataset_image.png)\n\nInterestingly, it seems to want to make the 'O' labels into rectangles!","0a1d2b2c":"## Sanity Check\n\nBelow I check the model can handle an image from the _original_ BMS dataset. The results should be consistent when the model is applied to either the original synthetic images or the ones I generate during training. This appears to be have been successful.","9b103044":"## Training and model options\n\nThis model performs significantly better *without* batch niormalisation, I guess this is because of the regularity of the data and possibly the fact the the network is fairly shallow. The model can be made even smaller by switching on separable convolutions, although the accuracy will decrease a bit.\n\nThe device used by Pytorch is also configured below, so change this as needed.","c841ad13":"The NN modules below below are part of a larger collection. I haven't included some unused modules because they haven't been tested yet. `ResPair` allows you to recursivly build a network with skip connections at each power of two layers.","a37fcdb7":"## The model\n\nBelow is version 3 of my 'CNSNet'. I experimented with decreasing the spacial resolution at lower layers, and regenrating the resolution with transpose convolutions (like a Unet), and other mechanisms, but for this simple task, straight-through convolutions have worked the best so far - by a large margin. "}}