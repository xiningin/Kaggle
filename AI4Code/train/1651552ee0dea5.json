{"cell_type":{"c5d5612d":"code","c0c2e05c":"code","751c99c2":"code","75bfda06":"code","d58ac96b":"code","8d50f939":"code","b33b883f":"code","2dc9b017":"code","a187bd9f":"code","4ea67a43":"code","929ae5f5":"code","e5d22952":"code","09b19f6d":"code","9e7302c7":"code","bc20b5db":"code","5dad626a":"code","2f5b43f6":"code","8d0eb836":"code","d3bc3cde":"code","72aff5a3":"code","a51bf751":"code","74a17227":"code","928b6b7a":"code","ff5604ee":"code","50147b50":"code","69f9b57b":"code","a9204ddf":"code","868b2c32":"code","ef640d0c":"code","65f6e12b":"code","fd8c496c":"code","c991b544":"code","4a0ff67b":"code","848efa7d":"code","47219d5e":"code","f88f68b3":"code","19204497":"code","7f90dfc0":"code","743148fc":"code","6373ab15":"code","eca6eea4":"code","3209e0f6":"code","595f95ac":"code","08dd1a33":"code","14c0bb57":"code","c155b69c":"code","58382224":"code","2c784956":"code","8b0a7156":"code","a83bf53d":"code","abde4ba1":"code","4518b158":"code","6ba44257":"code","48467003":"code","1cc1fd7c":"code","8051d9be":"markdown","524d3763":"markdown","fa5171b5":"markdown","09fd0697":"markdown","6e47bceb":"markdown","9c23e2db":"markdown","286b4a1a":"markdown","5fc9412c":"markdown","5038ffec":"markdown","5fe1ad29":"markdown","7fa1ce5a":"markdown","190af77f":"markdown","db4ecc94":"markdown","02e261d2":"markdown","9af4c5f4":"markdown","1350f084":"markdown","116439c6":"markdown","3d5fb163":"markdown","eca4c0ad":"markdown","764d1580":"markdown","75d6a66c":"markdown","d520fc03":"markdown","dd4367fe":"markdown","cf7f1b87":"markdown","ece52e50":"markdown","4b4a977a":"markdown","06f54ee8":"markdown","f17e320d":"markdown","33c11e4b":"markdown","508ef03c":"markdown","fa652621":"markdown","487b8a1f":"markdown","fa44dddd":"markdown","6c5fb44f":"markdown","98f57d29":"markdown","f712f624":"markdown","342cef81":"markdown","655ebd7d":"markdown","18f87c6c":"markdown"},"source":{"c5d5612d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c0c2e05c":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","751c99c2":"df=pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\ndf.head()","75bfda06":"df.describe()","d58ac96b":"df.columns","8d50f939":"df.info()","b33b883f":"df.isnull().values.any()","2dc9b017":"plt.figure(figsize=(10,10))\nsns.heatmap(df.corr(),annot=True,fmt='.1f')\nplt.show()","a187bd9f":"df[\"target\"].value_counts()","4ea67a43":"sns.countplot(x='target', data=df)\nplt.xlabel(\"0 = with heart disease , 1= without heart disease\")\nplt.show()","929ae5f5":"NumNoDisease= len(df[df.target == 0])\nNumHaveDisease=len(df[df.target == 1])\nprint(\"Percentage of Patients without Heart Disease: {:.2f}%\".format((NumNoDisease \/ (len(df.target))*100)))\nprint(\"Percentage of Patients Have Heart Disease: {:.2f}%\".format((NumHaveDisease \/ (len(df.target))*100)))","e5d22952":"df.groupby('sex')['target'].value_counts()","09b19f6d":"pd.crosstab(df.sex,df.target).plot(kind=\"bar\",figsize=(5,5),color=['#11A5AA','#AA1190' ])\nplt.title('Heart Disease frequency by Sex')\nplt.xlabel('Sex')\nplt.ylabel('Frequency')\nplt.show()","9e7302c7":"df.groupby('fbs')['target'].value_counts()","bc20b5db":"pd.crosstab(df.fbs,df.target).plot(kind=\"bar\",figsize=(5,5),color=['#11A5AA','#AA1190' ])\nplt.title('Heart Disease frequency by Fasting blood Sugar')\nplt.xlabel('0=False,   1=True')\nplt.ylabel('Frequency')\nplt.show()","5dad626a":"df.groupby('cp')['target'].value_counts()","2f5b43f6":"pd.crosstab(df.cp,df.target).plot(kind=\"bar\",figsize=(10,6), color=['#11A5AA','#AA1190' ])\nplt.title('Heart Disease frequency by Chest Pain Type')\nplt.xlabel('Chest pain type (0 : Typical,  1:Atypical,  2: Non-typical, 3: Asymptomatic engina ) ')\nplt.ylabel('Frequency')\nplt.show()","8d0eb836":"df.groupby('slope')['target'].value_counts()","d3bc3cde":"pd.crosstab(df.slope,df.target).plot(kind=\"bar\",figsize=(8,6), color=['#11A5AA','#AA1190' ])\nplt.title('Heart Disease frequency by slope')\nplt.xlabel('slope type')\nplt.ylabel('Frequency')\nplt.show()","72aff5a3":"df.groupby('thal')['target'].value_counts()","a51bf751":"pd.crosstab(df.thal ,df.target).plot(kind=\"bar\",figsize= (8 ,6), color=['#11A5AA','#AA1190' ])\nplt.title('Heart Disease frequency by thalessimia ')\nplt.xlabel('thalessimia ')\nplt.ylabel('Frequency')\nplt.show()","74a17227":"df.groupby('ca')['target'].value_counts()","928b6b7a":"df.groupby('ca')['target'].value_counts()\npd.crosstab(df.ca,df.target).plot(kind=\"bar\",figsize=(8,6), color=['#11A5AA','#AA1190' ])\nplt.title('Heart Disease frequency by ca')\nplt.xlabel('blood vessels')\nplt.ylabel('Frequency')\nplt.show()","ff5604ee":"df.groupby('exang')['target'].value_counts()","50147b50":"pd.crosstab(df.exang,df.target).plot(kind=\"bar\",figsize=(8,6), color=['#11A5AA','#AA1190' ])\nplt.title('Heart Disease frequency by exang')\nplt.xlabel('exang')\nplt.ylabel('Frequency')\nplt.show()","69f9b57b":"plt.rcParams['figure.figsize'] = (10,10 )\ndf_new=df[['age', 'trestbps', 'chol', 'thalach', 'oldpeak']]\nbp = sns.boxplot(data=df_new, \n                 palette=\"colorblind\")","a9204ddf":"plt.figure(figsize=(15,6))\nsns.countplot(x='age',data = df, hue = 'target')\nplt.show()","868b2c32":"sns.pairplot(df, hue='target', vars = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak'])","ef640d0c":"dummy_variable_1 = pd.get_dummies(df[\"sex\"])\ndummy_variable_1.rename(columns={0:'female', 1:'male'}, inplace=True)\ndf = pd.concat([df, dummy_variable_1], axis=1)\ndf.drop('sex', axis = 1, inplace=True)\ndummy_variable_1.head()","65f6e12b":"dummy_variable_2 = pd.get_dummies(df[\"cp\"])\ndummy_variable_2.rename(columns= {0:'tyical_angina',1:'atypical_angina', 2: \"non_angina\", 3: \"asymptomatic\"}, inplace=True)\ndf = pd.concat([df, dummy_variable_2], axis=1)\ndf.drop('cp', axis = 1, inplace=True)\ndummy_variable_3 = pd.get_dummies(df[\"fbs\"])\ndummy_variable_3.rename(columns={0:'lower_than 120 mg\/ml', 1:'more_than_120mg\/ml'}, inplace=True)\ndf = pd.concat([df, dummy_variable_3], axis=1)\ndf.drop('fbs', axis = 1, inplace=True)\ndummy_variable_4= pd.get_dummies(df[\"restecg\"])\ndummy_variable_4.rename(columns={0:'ecg_normal', 1:'ecg_ST-T wave abnormality', 2: 'ecg_left ventricular_hypertrophy'}, inplace=True)\ndf = pd.concat([df, dummy_variable_4], axis=1)\ndf.drop('restecg', axis = 1, inplace=True)\ndummy_variable_5= pd.get_dummies(df[\"exang\"])\ndummy_variable_5.rename(columns={0:'exang_no', 1:'exang_yes'}, inplace=True)\ndf = pd.concat([df, dummy_variable_5], axis=1)\ndf.drop('exang', axis = 1, inplace=True)\ndummy_variable_6= pd.get_dummies(df[\"slope\"])\ndummy_variable_6.rename(columns={0:'unslope', 1:'flat_slope', 2: 'down_slope'}, inplace=True)\ndf = pd.concat([df, dummy_variable_6], axis=1)\ndf.drop('slope', axis = 1, inplace=True)\ndummy_variable_7= pd.get_dummies(df[\"thal\"])\ndummy_variable_7.rename(columns={0:'normal_thal', 1:'fixed_defect_thal', 2: 'rever_def_thal', 3: \"thal_4\"}, inplace=True)\ndf = pd.concat([df, dummy_variable_7], axis=1)\ndf.drop('thal', axis = 1, inplace=True)\ndummy_variable_8= pd.get_dummies(df[\"ca\"])\ndummy_variable_8.rename(columns={0:'ca_0', 1:'ca_1', 2: 'ca_2', 3: 'ca_3', 4:'ca_4'}, inplace=True)\ndf = pd.concat([df, dummy_variable_8], axis=1)\ndf.drop('ca', axis = 1, inplace=True)\ndf.head()","fd8c496c":"import scipy.optimize as opt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline \nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import StandardScaler","c991b544":"sc = StandardScaler()\nfeature_scaling = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ndf[feature_scaling] = sc.fit_transform(df[feature_scaling])","4a0ff67b":"from sklearn.model_selection import train_test_split\nX = df.drop('target', axis=1)\ny = df.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","848efa7d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlog_reg = LogisticRegression(solver='liblinear')\nlog_reg.fit(X_train, y_train)\nyhat = log_reg.predict(X_test)\nprint(\"Train Accuracy Logistic Regression : \", metrics.accuracy_score(y_train, log_reg.predict(X_train)))\nprint(\"Test Accuracy Logistic Regression: \", metrics.accuracy_score(y_test, yhat))\nresults_model = pd.DataFrame(data=[[\"Logistic Regression\",  metrics.accuracy_score(y_train, log_reg.predict(X_train)), metrics.accuracy_score(y_test, yhat)]],\n                        columns=['Model', 'Training Accuracy ', 'Testing Accuracy '])\nresults_model","47219d5e":"from sklearn.model_selection import GridSearchCV\nparameter_grid = {'C': np.logspace(-4, 4, 20), 'penalty': ['l1', 'l2'], \"solver\": [\"liblinear\"]}\nlog_reg1 = LogisticRegression()\nestimator = GridSearchCV(estimator=log_reg, param_grid=parameter_grid, scoring='accuracy', cv=5, iid=True)\nestimator.fit(X_train, y_train)","f88f68b3":"estimator.best_estimator_","19204497":"log_reg1 = LogisticRegression(C=0.23357214690901212,solver='liblinear')\nlog_reg1.fit(X_train, y_train)\nprint(\"Train Accuracy Logistic Regression Tuned : \", metrics.accuracy_score(y_train, log_reg1.predict(X_train)))\nprint(\"Test Accuracy Logistic Regression Tuned: \", metrics.accuracy_score(y_test, log_reg1.predict(X_test)))\ntuning_log = pd.DataFrame(data=[[\" Logistic Regression Tuned \",  metrics.accuracy_score(y_train, log_reg1.predict(X_train)) , metrics.accuracy_score(y_test, log_reg1.predict(X_test))]], \n                          columns=['Model', 'Training Accuracy ', 'Testing Accuracy '])\nlog_tun_df = results_model.append(tuning_log, ignore_index=True)\nlog_tun_df","7f90dfc0":"from sklearn import svm\nclf = svm.SVC(kernel='linear')\nclf.fit(X_train, y_train) \nyhat_svm_ln = clf.predict(X_test)\nprint(\"Train Accuracy SVM & Linear Function: \", metrics.accuracy_score(y_train, clf.predict(X_train)))\nprint(\"Test Accuracy SVM & Linear Function: \", metrics.accuracy_score(y_test, yhat_svm_ln))\nsvm = pd.DataFrame(data=[[\" SVM Linear Function\" , metrics.accuracy_score(y_train, clf.predict(X_train)), metrics.accuracy_score(y_test, yhat_svm_ln)]],\n                   columns=['Model', 'Training Accuracy ', 'Testing Accuracy '])\ndf1 =log_tun_df.append(svm, ignore_index=True)\ndf1","743148fc":"from sklearn import svm\nclf1 = svm.SVC(kernel='rbf')\nclf1.fit(X_train, y_train) \nyhat_svm_rbf = clf1.predict(X_test)\nprint(\"Train Accuracy SVM & RBF : \", metrics.accuracy_score(y_train, clf1.predict(X_train)))\nprint(\"Test Accuracy SVM & RBF: \", metrics.accuracy_score(y_test, yhat_svm_rbf))\nsvm_rbf = pd.DataFrame(data=[[\"SVM RBF \", metrics.accuracy_score(y_train, clf1.predict(X_train)), metrics.accuracy_score(y_test, yhat_svm_rbf)]], \n                          columns=['Model', 'Training Accuracy ', 'Testing Accuracy '])\ndf2 = df1.append(svm_rbf, ignore_index=True)\ndf2","6373ab15":"from sklearn.model_selection import GridSearchCV \nparam_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf', 'poly', \"linear\"]}  \ngrid = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3)  \ngrid.fit(X_train, y_train)","eca6eea4":"print(grid.best_params_) \nprint(grid.best_estimator_)","3209e0f6":"svm_model = svm.SVC(C=1000, gamma=1, kernel='linear')\nsvm_model.fit(X_train, y_train)\nnew_pred =svm_model.predict(X_test)\nprint(\"Train Accuracy SVM  grid search  : \", metrics.accuracy_score(y_train, svm_model.predict(X_train)))\nprint(\"Test Accuracy SVM  grid search: \", metrics.accuracy_score(y_test, new_pred))\ntuning_svm = pd.DataFrame(data=[[\"SVM Tuning \", metrics.accuracy_score(y_train, svm_model.predict(X_train)), metrics.accuracy_score(y_test, new_pred)]], \n                          columns=['Model', 'Training Accuracy ', 'Testing Accuracy '])\ndf3 = df2.append(tuning_svm, ignore_index=True)\ndf3","595f95ac":"from sklearn.neighbors import KNeighborsClassifier\nk = 6 \nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nyhat_knn = neigh.predict(X_test)\nprint(\"Train  Accuracy KNN: \", metrics.accuracy_score(y_train, neigh.predict(X_train)))\nprint(\"Test  Accuracy KNN: \", metrics.accuracy_score(y_test, yhat_knn))\nknn = pd.DataFrame(data=[[\"KNN Classifier \", metrics.accuracy_score(y_train, neigh.predict(X_train)), metrics.accuracy_score(y_test, yhat_knn)]], \n                          columns=['Model', 'Training Accuracy ', 'Testing Accuracy '])\ndf4 = df3.append(knn, ignore_index=True)\ndf4","08dd1a33":"Ks = 10\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\n\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat_knn=neigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat_knn)\n\n    \n    std_acc[n-1]=np.std(yhat_knn==y_test)\/np.sqrt(yhat_knn.shape[0])\n\nmean_acc","14c0bb57":"plt.plot(range(1,Ks),mean_acc,'r')\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.legend(('Accuracy ', '+\/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Nabors (K)')\nplt.tight_layout()\nplt.show()","c155b69c":"print( \"The best accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1)","58382224":"k1 = 8\nneigh = KNeighborsClassifier(n_neighbors = k1).fit(X_train,y_train)\nyhat_knn_tun = neigh.predict(X_test)\nprint(\"Train  Accuracy KNN: \", metrics.accuracy_score(y_train, neigh.predict(X_train)))\nprint(\"Test  Accuracy KNN: \", metrics.accuracy_score(y_test, yhat_knn))\nknn_tun = pd.DataFrame(data=[[\"KNN Classifier Tuning \", metrics.accuracy_score(y_train, neigh.predict(X_train)), metrics.accuracy_score(y_test, yhat_knn_tun)]], \n                          columns=['Model', 'Training Accuracy ', 'Testing Accuracy '])\ndf5= df4.append(knn_tun, ignore_index=True)\ndf5","2c784956":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(random_state=42)\ntree.fit(X_train, y_train)\nyhat_tree = tree.predict(X_test)\nprint(\"Train Accuracy Decision Tree  : \", metrics.accuracy_score(y_train, tree.predict(X_train)))\nprint(\"Test Accuracy Decidion Tree : \", metrics.accuracy_score(y_test, yhat_tree))\ndec_tree = pd.DataFrame(data=[[\"Decision Tree \", metrics.accuracy_score(y_train, tree.predict(X_train)),  metrics.accuracy_score(y_test, yhat_tree)]], \n                          columns=['Model', 'Training Accuracy ', 'Testing Accuracy '])\ndf6= df5.append(dec_tree, ignore_index=True)\ndf6","8b0a7156":"params = {\"criterion\":(\"gini\", \"entropy\"), \n          \"splitter\":(\"best\", \"random\"), \n          \"max_depth\":(list(range(1, 20))), \n          \"min_samples_split\":(list(range (1, 10))), \n          \"min_samples_leaf\":(list(range(1, 20)))\n          }\ntree = DecisionTreeClassifier(random_state=40)\ngrid_search= GridSearchCV(tree, params, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=5)\ngrid_search.fit(X_train, y_train)\n# grid_search.best_estimator_","a83bf53d":"tree = DecisionTreeClassifier(criterion='gini', \n                              max_depth=4,\n                              min_samples_leaf=11, \n                              min_samples_split=2, \n                              splitter='random')\ntree.fit(X_train, y_train)\nprint(\"Train Accuracy Decision Tree  : \", metrics.accuracy_score(y_train, tree.predict(X_train)))\nprint(\"Test Accuracy Decidion Tree : \", metrics.accuracy_score(y_test,  tree.predict(X_test)))\ndec_tree_tun = pd.DataFrame(data=[[\"Decision Tree Tuned \", metrics.accuracy_score(y_train, tree.predict(X_train)),  metrics.accuracy_score(y_test, tree.predict(X_test))]], \n                          columns=['Model', 'Training Accuracy ', 'Testing Accuracy '])\ndf7= df6.append(dec_tree_tun, ignore_index=True)\ndf7","abde4ba1":"from sklearn.ensemble import RandomForestClassifier\nrand_forest = RandomForestClassifier(n_estimators=1000, random_state=40)\ny_ran_for= rand_forest.fit(X_train, y_train)\nprint(\"Train Accuracy Random Forest  : \", metrics.accuracy_score(y_train, rand_forest.predict(X_train)))\nprint(\"Test Accuracy Randon Forest : \", metrics.accuracy_score(y_test, rand_forest.predict(X_test)))\nran_for = pd.DataFrame(data=[[\"Random Forest \", metrics.accuracy_score(y_train, rand_forest.predict(X_train)),  metrics.accuracy_score(y_test, rand_forest.predict(X_test))]], \n                          columns=['Model', 'Training Accuracy ', 'Testing Accuracy '])\ndf8= df7.append(ran_for, ignore_index=True)\ndf8","4518b158":"from sklearn.model_selection import RandomizedSearchCV\nn_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num=11)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\nrandom_grid = {'n_estimators': n_estimators, 'max_features': max_features,\n               'max_depth': max_depth, 'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}\nrand_forest = RandomForestClassifier(random_state=42)\nrf_random = RandomizedSearchCV(estimator=rand_forest, param_distributions=random_grid, n_iter=100, cv=3, \n                               verbose=2, random_state=42, n_jobs=-1)\nrf_random.fit(X_train, y_train)","6ba44257":" rf_random.best_estimator_","48467003":"rand_forest = RandomForestClassifier(bootstrap=True,\n                                     max_depth=10, \n                                     max_features='auto', \n                                     min_samples_leaf=4, \n                                     min_samples_split=5,\n                                     n_estimators=200)\nrand_forest.fit(X_train, y_train)","1cc1fd7c":"print(\"Train Accuracy Random forest after hyperparameters tuning : \", metrics.accuracy_score(y_train, rand_forest.predict(X_train)))\nprint(\"Test Accuracy Random forest after hyperparameters training  : \", metrics.accuracy_score(y_test, rand_forest.predict(X_test)))\nran_for_tun = pd.DataFrame(data=[[\"Random Forest Tuning \", metrics.accuracy_score(y_train, rand_forest.predict(X_train)),  metrics.accuracy_score(y_test, rand_forest.predict(X_test))]], \n                          columns=['Model', 'Training Accuracy ', 'Testing Accuracy '])\ndf9= df8.append(ran_for_tun, ignore_index=True)\ndf9","8051d9be":"## 6C. K-nearest neighbors","524d3763":" ### Linear Function","fa5171b5":"### Interpretation:\n#### The graph shows how different type chest pain  is related  to heart disease:\n* Typical Engina: 104 without and 39 with heart disease\n* Atypical engina :41 without and 9 with heart disease\n* Non-typical Engina :69 without and 18 with heart disease\n* Asymptomatic Engina :16 without and 7 with heart disease","09fd0697":" ## 6B . SVM (linear and RBF )","6e47bceb":"#### Interpretation   :\n\nthalessimia: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\nThe fixed defect is major cause of heart disease about 130 patients.","9c23e2db":"### Interpretation :\nWe can see that the values of target variable are plotted wrt sex : (0 = male; 1 = false ). Out of 96 females 72 have heart disease and 24 do not have heart disease. Out of 207 males 93 have heart disease and 114 do not have heart disease.","286b4a1a":"#### Interpretation\n142 patients have heart diseses which have exercise induced angina","5fc9412c":"## 5. Data Analysis Continous Variables","5038ffec":"### 1. Effect of sex on heart disease :","5fe1ad29":" # 2. Exploratory Data Analysis","7fa1ce5a":"# 4. Different Categorical variables and how they are related to heart disease:","190af77f":"After exploring the dataset, I observed that I need to convert some categorical variables into dummy variables and scale all the values before training the Machine Learning models. First, I'll use the get_dummies method to create dummy columns for categorical variables.\nI want to include specific names in the columns ","db4ecc94":" ## 6A. Logistic Regression","02e261d2":"###  5 .  Effect of thalessemian   on heart disease :","9af4c5f4":"## SVM Using Hyperparameter Tuning","1350f084":"### 4 .  Effect of Slope  on heart disease :","116439c6":"### The Below plot explain how age is related to heart disease","3d5fb163":"1. There are165 patients suffering from heart disease.\n1. There are 138 patients who do not have any heart disease.","eca4c0ad":"#### Interpretation : \nThe slope of the peak exercise ST segment show when the slope is downsloping  the heart disease is more as compared to upslope and flat values .","764d1580":"## 6D. Decision Tree Classifier","75d6a66c":"### 2. Effect of fasting blood sugar on heart disease :","d520fc03":"age: The person's age in years\n\nsex: The person's sex (1 = male, 0 = female)\n\ncp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n\ntrestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n\nchol: The person's cholesterol measurement in mg\/dl\n\nfbs: The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)\n\nrestecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n\nthalach: The person's maximum heart rate achieved\n\nexang: Exercise induced angina (1 = yes; 0 = no)\n\noldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n\nslope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n\nca: The number of major vessels (0-3)\n\nthal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n\ntarget: Heart disease (0 = no, 1 = yes)","dd4367fe":"# 3. Correlation Matrix","cf7f1b87":" ## Radial basis function","ece52e50":"###  7 .  Effect of exercise induced angina on heart disease :","4b4a977a":"## 6e. Random Forest  Classifier","06f54ee8":"### 6 .  Effect of blood vessels    on heart disease :","f17e320d":"### Interpretation: \nWe can see that the values of target variable are plotted wrt fbs  : (1 = true ; 0 = false).\nOut of 45  people with fbs 23 have heart disease while 22 don't  \nThere are 258 people without fbs  of which 142 are heart patients while 116 dont have heart disease","33c11e4b":"### Feature Scaling on continous variables","508ef03c":" ### Split data into training and testing set","fa652621":" # Conclusion: \n After comparing differnt models and finding the accuracy of the models with and without hyperparameters tuning it was fouund that  Support Vector Machine  with Radial Bias Function performs well in this classification problem. \n I think there is still room for improvemnt ","487b8a1f":"# 1.  Data Description ","fa44dddd":"### K-nearest neighbors Hyperparameter Tuning","6c5fb44f":"#### Interpretation  \nca - number of major vessels (0-3) colored by flourosopy\ncolored vessel means the doctor can see the blood passing through\nthe more blood movement the better (no clots)","98f57d29":"### 3.  Effect of Chest Pain  on heart disease :","f712f624":"### Decision Tree Classifier with Hyperparameter Tuning","342cef81":"## 6. Data Processing","655ebd7d":"## 6. Applying machine learning algorithms","18f87c6c":"## Logistic Regression Using Hyperparameter Tuning"}}