{"cell_type":{"4e85b497":"code","89ff9fdd":"code","7d98e2f7":"code","927156dd":"code","a21527bc":"code","9db988af":"code","fdbff2ef":"code","ae781218":"code","8a8ef80a":"code","b97ad538":"code","1aaa87f4":"code","00180009":"code","6325dabe":"code","3ca0d822":"code","0f846858":"code","07717495":"code","12db3daa":"code","3edf31ad":"code","afdc200f":"code","d28bbac4":"code","f949d5b0":"code","05428068":"code","bab76df5":"code","b3d3caf0":"code","6f9b5c63":"code","ef8d1cdc":"code","954c9e8c":"code","db797839":"code","5b1da651":"code","43e74747":"code","c7e97ba1":"code","8466e15c":"code","fc796529":"code","e1dfb7bf":"code","7bc9d8bd":"code","a0e95c84":"code","caad375f":"code","48e89466":"code","82f3bbd4":"code","c20f0c7c":"code","00cbbef9":"code","8ffafe8a":"code","b5d3ba8e":"code","31744361":"code","f0c37dbe":"code","83a8ab32":"code","d6e424a0":"code","2cb0849d":"code","34816b4f":"code","a6efedd8":"code","76c53c94":"code","9691b827":"code","3848fb85":"code","8825204a":"code","67ce2e2b":"code","d44e19a3":"code","dae24ac6":"code","b042938f":"code","66ad70e7":"code","e3b92312":"code","16714087":"code","ccfaa692":"code","fb3ae087":"code","2b09b6e3":"code","2d32bfc6":"code","e3bed11c":"code","a8e083e9":"code","677381df":"code","6aaec58e":"code","269bc9e2":"code","9f1de9d1":"code","8fc53bdd":"code","f21a60a1":"code","fae6ec3e":"code","08b5ac4f":"code","56ac64e5":"code","e4671b42":"code","b159bd7a":"code","002b2185":"code","edcfd0fb":"code","bf272299":"code","fd99dbf5":"code","9f075523":"code","b59c7da2":"code","0c6260f4":"code","3eb90d01":"code","bf6f94ce":"code","7df24641":"code","16e392c8":"code","77a8e848":"code","7311d710":"code","bd7e269f":"code","e3f9ddb6":"code","b02cbaab":"code","02fa9785":"code","5b77b69a":"code","af2d07bc":"code","b041f5f5":"code","c0fcc50c":"code","b3ea298c":"code","55117bbd":"code","22098428":"code","0087a26d":"code","e0e1e33d":"code","dc1259ef":"code","e9e934ba":"code","247a135f":"code","57328328":"markdown","f4224ef4":"markdown","910399c4":"markdown","0935237c":"markdown","18a14f32":"markdown","feb5179f":"markdown","eaf50477":"markdown","bf0c6cb2":"markdown","c15fde0c":"markdown","aee64d3d":"markdown","bb021044":"markdown","dd777da0":"markdown","a2617699":"markdown","885b3fe8":"markdown","9b0ed8b6":"markdown","7277f24b":"markdown","e799a0cf":"markdown","9dd0f5bd":"markdown","5a1b1b39":"markdown","1c8cedc8":"markdown","e8ba0f8a":"markdown","317fd0d5":"markdown","160f9be2":"markdown","fcaff527":"markdown","313db310":"markdown","7df1506a":"markdown","0f964466":"markdown","026b91d5":"markdown","e92e9ca4":"markdown","78eb1ab7":"markdown","4e5b9663":"markdown","08098eb0":"markdown","51477f37":"markdown","44110dfd":"markdown","3b074c2f":"markdown","9c28c6a5":"markdown","f568e6f6":"markdown","4969da52":"markdown","86362b44":"markdown","2a281ef7":"markdown","23e2a50b":"markdown","7548a45c":"markdown","20675962":"markdown","cfcf98ef":"markdown","c413e8d5":"markdown","5fbbfeb5":"markdown","604c0912":"markdown","bd3fc8e2":"markdown","0faf851a":"markdown","77ee1966":"markdown","803d2a50":"markdown","168979e9":"markdown","e1da6f01":"markdown","2fbd1621":"markdown","7e2c6dd5":"markdown","7789f21c":"markdown","fa153da7":"markdown","f7d30ed7":"markdown","82a00966":"markdown","e642047a":"markdown","26d6dfd9":"markdown","f15b90c6":"markdown","14e23570":"markdown","02cd5769":"markdown","41684b0b":"markdown","8b8be912":"markdown","72e588e2":"markdown","120fd3f2":"markdown","433f6fe1":"markdown","a512f5a2":"markdown","1e76ce04":"markdown","942216e8":"markdown","6305491e":"markdown","f1bc262a":"markdown","667205a4":"markdown","c88f398b":"markdown","911099dc":"markdown","c7016cd9":"markdown","31a8deb0":"markdown","7a66eb0a":"markdown","61924c5a":"markdown","83f62811":"markdown","14d6ef50":"markdown","f1c32f3a":"markdown","4861273a":"markdown","636237be":"markdown","580ba55c":"markdown","4b628cfd":"markdown","e7c60c1d":"markdown","b289f191":"markdown","60e7fb5e":"markdown","a19f2121":"markdown","bbb3d0de":"markdown","5458c0c5":"markdown","36d63259":"markdown","bf45b27e":"markdown","4c5dbc89":"markdown","1ac64ae5":"markdown","c0417cc4":"markdown","bc532ada":"markdown","c6e2e1c8":"markdown","8e4120ca":"markdown","ddc910c9":"markdown","981fd55f":"markdown","3bf11712":"markdown","d372e113":"markdown","740e15ed":"markdown","a8daad84":"markdown","17606263":"markdown","b69e14a8":"markdown","08143659":"markdown","aa41e5f5":"markdown","c51bb88c":"markdown","82f49b52":"markdown","ff503c9b":"markdown","a9eeba76":"markdown","6c050512":"markdown","4136feb0":"markdown","80446eb9":"markdown","de8535a6":"markdown","c426cc67":"markdown","3d4b9c68":"markdown","a7c0444c":"markdown","17bdf027":"markdown","b787a035":"markdown","73086712":"markdown","566adc7c":"markdown","3dc50a78":"markdown","fa059a3a":"markdown","1f5f9512":"markdown","acf5f8b5":"markdown","8541e1e1":"markdown","fe4156f2":"markdown","5a6f2355":"markdown","003f4ecb":"markdown","e861f087":"markdown","6e5b7c6e":"markdown","5a9fe992":"markdown"},"source":{"4e85b497":"from plotly.offline import init_notebook_mode, iplot_mpl, download_plotlyjs, plot, iplot\nimport plotly_express as px\nimport plotly.figure_factory as ff\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport warnings\nwarnings.filterwarnings('ignore')\ninit_notebook_mode(connected=True)\nimport pandas_profiling\nimport statsmodels.formula.api as sm\nfrom statsmodels.compat import lzip\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nfrom sklearn.preprocessing import RobustScaler\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix,f1_score,accuracy_score\nfrom sklearn.metrics import precision_score, roc_auc_score, recall_score, roc_curve, precision_recall_curve\nfrom pandas import DataFrame\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nimport graphviz\nfrom sklearn import tree\nfrom sklearn.tree.export import export_text\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom eli5 import show_prediction\nimport shap\nfrom sklearn.feature_extraction import DictVectorizer\nfrom yellowbrick.classifier import confusion_matrix, classification_report, ROCAUC, DiscriminationThreshold\nfrom yellowbrick.target import ClassBalance, FeatureCorrelation\nfrom yellowbrick.classifier import DiscriminationThreshold\nfrom yellowbrick.model_selection import CVScores, RFECV, LearningCurve\nfrom sklearn import preprocessing\n\nfrom pdpbox import pdp, info_plots\nfrom pdpbox.pdp import pdp_interact, pdp_interact_plot\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer, InterclusterDistance\nfrom yellowbrick.features import RadViz, PCA, pca_decomposition\nfrom sklearn.mixture import GaussianMixture\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import silhouette_score, silhouette_samples\nimport catboost\nfrom catboost import CatBoostClassifier, Pool, cv\nfrom time import time\n\nfrom catboost.utils import get_confusion_matrix\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.metrics import confusion_matrix\n\nfrom pdpbox import pdp, get_dataset, info_plots\n\nimport lime\nimport lime.lime_tabular\nplt.rcParams['figure.dpi'] = 300\n%config InlineBackend.figure_format = 'svg'\n# Need to load JS vis in the notebook\nshap.initjs()\n\n!pip install ppscore \n\nimport ppscore as pps","89ff9fdd":"data=pd.read_csv(\"..\/input\/sloan-digital-sky-survey-dr16\/Skyserver_12_30_2019 4_49_58 PM.csv\",nrows=8000)","7d98e2f7":"df=data.copy()","927156dd":"df.info()","a21527bc":"df.head()","9db988af":"#We delete the variables which we will not use\ndf.drop(['objid','run', 'rerun', 'camcol',\n       'field', 'specobjid','plate', 'mjd', 'fiberid'],axis=1,inplace=True)","fdbff2ef":"#We obtain the Dataset with the variables of interest\ndf.head()","ae781218":"### 2.1 Separaci\u00f3n DataSet entre atributos y target\nps=df[['u','g','r','i','z']]\n\n#Matriz de atributos\nX=df[['u','g','r','i','z','redshift']]\n\n#Target \nY=df['class']","8a8ef80a":"fig = px.scatter(df, x='ra',y='dec',color='class',\n        title=\"Equatorial coordinates of the picture samples\",\n                 color_discrete_sequence=px.colors.qualitative.Safe).update_traces(dict(marker_line_width=0,\n        marker_size=8,marker_line_color=\"black\",mode='markers')).update_layout( \n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\n\nfig.show()","b97ad538":"#Eliminamos las variables de las coordenadas equatoriales.\ndf.drop(['ra','dec'],inplace=True, axis=1)","1aaa87f4":"pandas_profiling.ProfileReport(df)","00180009":"fig = px.pie(Y, 'class', title='Astronomical class - pie chart distribution', \n             color_discrete_sequence=px.colors.qualitative.Safe).update_traces(hoverinfo='label+value', \n  textinfo='percent', textfont_size=16,\n).update_layout( \n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\nfig.show()","6325dabe":"df.redshift.describe()","3ca0d822":"fig = px.histogram(df, marginal='box', x=\"redshift\",log_y=True,nbins=30,\ntitle='Histogram - Redshift log count',\n             template='ggplot2',color_discrete_sequence=px.colors.qualitative.Safe).update_traces(dict(marker_line_width=1, marker_line_color=\"black\")).update_layout( \n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\n\nfig.show()","0f846858":"px.histogram(df, marginal='box',x=\"redshift\",nbins=30,log_y=True,color='class',\n             title=\"Histogram & Box Plots - Log escale count redshift per class\",template='ggplot2',facet_col='class',\n             color_discrete_sequence=px.colors.qualitative.Safe).update_traces(dict(marker_line_width=1,\n   marker_line_color=\"black\")).update_traces(dict(marker_line_width=1, \n             marker_line_color=\"black\")).update_layout( \n    paper_bgcolor='rgb(243, 243, 243)',plot_bgcolor='rgb(243, 243, 243)')","07717495":"melt = pd.concat([Y,ps],axis=1)\n\nmelt = pd.concat([Y,ps],axis=1)\nmelt = pd.melt(melt,id_vars=\"class\",\n                    var_name=\"Photometric System\",\n                    value_name='value')\n\nfig = px.box(melt, x=\"Photometric System\",y='value',title='Box plots - Photometric System (u, g, r, i, z)',\n             color_discrete_sequence=px.colors.qualitative.Safe).update_traces(dict(marker_line_width=1, \nmarker_line_color=\"black\")).update_layout( paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\nfig.show()","12db3daa":"fig = px.box(melt, x=\"Photometric System\",y='value',color='class',title='Box plots - Photometric System (u, g, r, i, z) by class',\n             color_discrete_sequence=px.colors.qualitative.Safe).update_traces(dict(marker_line_width=1, marker_line_color=\"black\")).update_layout( paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\nfig.show()","3edf31ad":"fig = px.imshow(X.corr(),x=list(X.corr().columns),y=list(X.corr().columns),\n                width=900, height=500,title='Correlation Matrix Photometric system (u,g,r,i,z) & red shift',\n                color_continuous_scale=px.colors.sequential.PuBuGn).update_layout( \n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\nfig.show()","afdc200f":"#Target label encoder\nle = preprocessing.LabelEncoder()\nle.fit(Y)\nY=le.transform(Y)\ndf['target']=Y","d28bbac4":"model = KMeans(random_state=42)\nvisualizer = KElbowVisualizer(model, k=(1,12), size=(3400, 1500))\n\nvisualizer.fit(X)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure","f949d5b0":"# Instantiate the clustering model and visualizer\nmodel = KMeans(3, random_state=42)\nvisualizer = SilhouetteVisualizer(model, colors='yellowbrick',size=(3700, 1500))\n\nvisualizer.fit(X)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure","05428068":"# Instantiate the clustering model and visualizer\nvisualizer = InterclusterDistance(model,size=(3700, 1500))\n\nvisualizer.fit(X)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure","bab76df5":"clus_pred = model.fit_predict(X)\ndf['KMcluster']=clus_pred\n\nvisualizer = PCA(scale=True, proj_features=True,size=(3000, 2000),alpha=0.50,random_state=1)\nvisualizer.fit_transform(X, clus_pred)\nvisualizer.show()","b3d3caf0":"visualizer = PCA(scale=True, classes=le.classes_, proj_features=True,size=(3000, 2000),alpha=0.50)\nvisualizer.fit_transform(X, Y)\nvisualizer.show()","6f9b5c63":"# we will perform PCA with the 5 photometric variables\nps.head()","ef8d1cdc":"visualizer = PCA(scale=True, classes=le.classes_, proj_features=True,size=(3000, 2000),alpha=0.5)\nvisualizer.fit_transform(ps, Y)\nvisualizer.show()","954c9e8c":"# We scale our Matrix (DataSet) First we transform our DF in a scaled Matrix (scaled by the \u03bc and \u03c3 of each columm) \ndf_=scale(ps)","db797839":"# from sklearn we import PCA module and we fit our DataSet, we specify the number of PC and call the fit() method \n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(df_)\n\n\n# Now we can transform our DataSet\ndf_2d = pca.transform(df_)\n\n# We generate a new Dataframe with our two components as variables\ndf_2d = pd.DataFrame(df_2d)\ndf_2d.columns = ['PC1',\"PC2\"]","5b1da651":"df_2d.head()","43e74747":"# We compute the explained variance ratio\nDataFrame(pca.explained_variance_ratio_.round(2), index=['PC'+str(i) for i in range(1,3)], \n          columns=['Explained Variance Ratio']).T","c7e97ba1":"df['PC1']=df_2d.PC1\ndf['PC2']=df_2d.PC2","8466e15c":"fig = px.scatter_matrix(df,\n    dimensions=[\"PC1\", \"PC2\", \"redshift\"],\n    color=\"class\",                                    \n    title=\"Scatter matrix of photometric PC1 & PC2 and RedShift by class\",\n    labels={col:col.replace('_', ' ') for col in df.columns},color_discrete_sequence=px.colors.qualitative.Safe).update_traces(dict(marker_line_width=1, marker_line_color=\"black\")).update_layout( paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)').update_traces(diagonal_visible=False)\n\nfig.show()","fc796529":"fig=px.scatter_3d(df, x='PC2',y='PC1',z='redshift',title=\"Photometric scatter plot PC1 & PC2 per class\",\n        color='class',color_discrete_sequence=px.colors.qualitative.Safe).update_traces(marker_size=5).update_layout( paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\n\nfig.show()","e1dfb7bf":"fig = px.imshow(df[['u','g','r','i','z','PC1','PC2']].corr(),x=list(df[['u','g','r','i','z','PC1','PC2']].corr().columns),y=list(df[['u','g','r','i','z','PC1','PC2']].corr().columns),\n                width=900, height=500,title='Correlation Matrix Photometric system (u,g,r,i,z) & PC1 PC2',\n                color_continuous_scale=px.colors.diverging.BrBG).update_layout( \n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\nfig.show()","7bc9d8bd":"# Instantiate the visualizer\nvisualizer = RadViz(classes=le.classes_,size=(3800, 2000),alpha=0.05)\n\nvisualizer.fit(X, Y)           # Fit the data to the visualizer\nvisualizer.transform(X)        # Transform the data\nvisualizer.show()","a0e95c84":"# Instaniate the visualizer\nvisualizer = FeatureCorrelation(\n    method='mutual_info-classification', feature_names=X.columns.tolist(), sort=True,\nsize=(3600, 2000))\n\nvisualizer.fit(X, Y)        # Fit the data to the visualizer\nvisualizer.show()    ","caad375f":"p=X.copy()\np['y']=df['class']\n\npredictors=pps.predictors(p, \"y\")\n\nfig = px.bar(predictors, x=\"x\", y='ppscore',\n              title=\"Bar Plot - Predictive power score atributes with target\",\n              color_discrete_sequence=px.colors.qualitative.Pastel2).update_layout( \n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\n\nfig.show()","48e89466":"pps.matrix(p)","82f3bbd4":"df1=df.groupby('class').aggregate(['mean','median','max','min','std'])\ndf1.iloc[:,:25].T","c20f0c7c":"aux=df.groupby('class').aggregate('mean')\ngz=aux[['u','g','r','i','z']].T\ngz['photometric variable']=gz.index\n\nfig = px.scatter(gz, x='photometric variable',y=['GALAXY','QSO','STAR'],\n        title=\"Mean values for each photometric variable (u,g,r,iz) per class \",\n                 color_discrete_sequence=px.colors.qualitative.Safe).update_traces(dict(marker_line_width=1,\n        marker_size=16,marker_line_color=\"black\",mode='markers+lines')).update_layout( \n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\n\nfig.show()","00cbbef9":"df1.iloc[:,25:30].T","8ffafe8a":"# We transform discrete variables into dummy variables\ndf ['GALAXY'] = pd.get_dummies (df ['target']). iloc [:, 0: 1]\n# We transform discrete variables into dummy variables\ndf ['QSO'] = pd.get_dummies (df ['target']). iloc [:, 1: 2]\n# We transform discrete variables into dummy variables\ndf ['STAR'] = pd.get_dummies (df ['target']). iloc [:, 2: 3]\n\n\nfig, axes, summary_df = info_plots.target_plot (\n     df = df, feature = 'redshift', feature_name = 'redshift', target = ['GALAXY'], num_grid_points = 10, ncols = 1,\nfigsize = (11, 7))","b5d3ba8e":"\nfig, axes, summary_df = info_plots.target_plot(\n    df=df, feature='redshift', feature_name='redshift', target=['QSO'],num_grid_points=10,ncols=1,\nfigsize=(11, 7))","31744361":"fig, axes, summary_df = info_plots.target_plot(\n    df=df, feature='redshift', feature_name='redshift', target=['STAR'],num_grid_points=10,ncols=1,\nfigsize=(11, 7))","f0c37dbe":"pd.cut(df['redshift'], bins=df['redshift'].quantile([.0,.25, .5, .75, 1.])).value_counts()","83a8ab32":"df['reds_dis']=pd.cut(df['redshift'], bins=df['redshift'].quantile([.0,.25, .5, .75, 1.]),labels=['q1',\"q2\",\"q3\",\"q4\"])","d6e424a0":"df['m_pho']=(df.u+df.g+df.r+df.i+df.z)\/5","2cb0849d":"pd.cut(df['m_pho'], bins=df['m_pho'].quantile([.0,.25, .5, .75, 1.])).value_counts()","34816b4f":"df['m_pho']=pd.cut(df['m_pho'], bins=df['m_pho'].quantile([.0,.25, .5, .75, 1.]),labels=[\"q1\",\"q2\",\"q3\",\"q4\"])","a6efedd8":"p['reds_dis']=df['reds_dis']\np['m_pho']=df['m_pho']\npredictors=pps.predictors(p, \"y\")","76c53c94":"predictors.iloc[1:2,:]","9691b827":"predictors.iloc[6:7,:]","3848fb85":"X_1=df[['u', 'g', 'r', 'i', 'z','redshift']]\n\nX_2=X_1.copy()\nX_2['reds_dis']=df.reds_dis\nX_2['m_pho']=df.m_pho\n\nX_3=X_1.copy()\nX_3['redshift*g']=X['redshift']*X['g']\nX_3['redshift*i']=X['redshift']*X['i']\n\nX_pca=df[['redshift']]\nX_pca['PC1']=df_2d['PC1']\nX_pca['PC2']=df_2d['PC2']\n\nY=df['class']","8825204a":"X_1.head(2)","67ce2e2b":"X_2.head(2)","d44e19a3":"X_3.head(2)","dae24ac6":"X_pca.head(2)","b042938f":"#We prepare the data so that the algorithm can process categorical attributes\ncategorical_features_indices = np.where(X.dtypes != np.float)[0]","66ad70e7":"#predictor matrix\nX_1.head()","e3b92312":"X_train, X_val, y_train, y_val = train_test_split(X_1, Y, test_size=0.2, random_state=0, stratify=Y)","16714087":"train_pool = Pool(X_train, y_train, cat_features=categorical_features_indices)\nvalidation_pool = Pool(X_val, y_val, cat_features=categorical_features_indices)","ccfaa692":"model = CatBoostClassifier(iterations=500,  random_seed=42, custom_loss=['F1','Accuracy','AUC'], early_stopping_rounds=50)\n\nmodel.fit(train_pool, eval_set=validation_pool, verbose=25, plot=True)","fb3ae087":"print(model.get_best_iteration())","2b09b6e3":"#The model has obtained the least errors with selected 420 trees\nmodel.tree_count_","2d32bfc6":"cv_params = model.get_params()\n\ncv_params.update({\n    'loss_function': 'MultiClass',\n    'iterations': 1000,\n    'custom_loss':['F1','Accuracy','AUC']})\n\ncv_data = cv(train_pool, cv_params, shuffle=True,fold_count=3, plot=True, verbose=50, stratified=True)","e3bed11c":"cv_data.tail(5)","a8e083e9":"y_pred=model.predict(X_val)\ncm=confusion_matrix(y_val, y_pred)\ncm_df = pd.DataFrame(cm,\n                     index = [\"GALAXY\", \"QSO\", \"STAR\"], \n                     columns = [\"GALAXY\", \"QSO\", \"STAR\"])\n\nplt.figure(figsize=(13,6))\nsns.heatmap(cm_df, annot=True,fmt='.4g',cmap=\"YlGnBu\")\nplt.title('CatBoost classifier\\nAccuracy:{0:.3f}'.format(accuracy_score(y_val, y_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","677381df":"#Create classification report\nprint(classification_report(y_pred,y_val))","6aaec58e":"model.predict(X_val)","269bc9e2":"model.predict_proba(X_val).round(3)","9f1de9d1":"X_pca.head()","8fc53bdd":"#Split data into train and validation\nX_pca_train, X_pca_val, y_pca_train, y_pca_val = train_test_split(X_pca, Y, test_size=0.2, random_state=0, stratify=Y)\n\ntrain_pool_pca = Pool(X_pca_train, y_pca_train)\nvalidation_pool_pca = Pool(X_pca_val, y_pca_val)","f21a60a1":"model_pca = CatBoostClassifier(iterations=500,  random_seed=42, custom_loss=['F1','Accuracy','AUC'], early_stopping_rounds=50)\n\nmodel_pca.fit(train_pool_pca, eval_set=validation_pool_pca, verbose=25, plot=True)","fae6ec3e":"#The model has selected 150 trees\nmodel_pca.tree_count_","08b5ac4f":"y_pca_pred=model_pca.predict(X_pca_val)","56ac64e5":"cm=confusion_matrix(y_val, y_pca_pred)\ncm_df = pd.DataFrame(cm,\n                     index = [\"GALAXY\", \"QSO\", \"STAR\"], \n                     columns = [\"GALAXY\", \"QSO\", \"STAR\"])\n\nplt.figure(figsize=(13,6))\nsns.heatmap(cm_df, annot=True,fmt='.4g',cmap=\"YlGnBu\")\nplt.title('PCA Catboost classifier \\nAccuracy:{0:.3f}'.format(accuracy_score(y_val, y_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","e4671b42":"#Create classification report\nclass_report=classification_report(y_pca_pred, y_pca_val)\nprint(class_report)","b159bd7a":"model.classes_","002b2185":"# Create the data that we will plot\npdp_redshift= pdp.pdp_isolate(model=model, dataset=X_train, model_features=X_train.columns,\n                                    feature='redshift',num_grid_points=1000)\n\nfig, axes = pdp.pdp_plot(pdp_redshift,'redshift',figsize=(12, 7), center=True,ncols=3,plot_params = {\n            # pdp line color, highlight color and line width\n            'pdp_color': 'coral',\n            'pdp_hl_color': 'coral',\n            'pdp_linewidth': 2,\n            # horizon zero line color and with\n            'zero_color': '#E75438',\n            'zero_linewidth': 1,\n            # pdp std fill color and alpha\n            'fill_color': 'coral',\n            'fill_alpha': 0.2,\n            # marker size for pdp line\n            'markersize': 3,\n        })\nplt.show() ","edcfd0fb":"# Create the data that we will plot\npdp_redshift= pdp.pdp_isolate(model=model, dataset=X_train, model_features=X_train.columns,\n                                    feature='u',num_grid_points=100)\n\nfig, axes = pdp.pdp_plot(pdp_redshift,'u',figsize=(12, 7),\n                         ncols=3, \n)\nplt.show() ","bf272299":"# Create the data that we will plot\npdp_redshift= pdp.pdp_isolate(model=model, dataset=X_train, model_features=X_train.columns,\n                                    feature='g',num_grid_points=1000)\n\nfig, axes = pdp.pdp_plot(pdp_redshift,'g',figsize=(12, 7), \n                         ncols=3, frac_to_plot=1, \n)\nplt.show() ","fd99dbf5":"# Create the data that we will plot\npdp_redshift= pdp.pdp_isolate(model=model, dataset=X_train, model_features=X_train.columns,\n                                    feature='z',num_grid_points=1000)\n\nfig, axes = pdp.pdp_plot(pdp_redshift,'z',figsize=(12, 7), \n                         ncols=3, frac_to_plot=1, \n)\nplt.show() ","9f075523":"# Create the data that we will plot\npdp_redshift= pdp.pdp_isolate(model=model, dataset=X_train, model_features=X_train.columns,\n                                    feature='r',num_grid_points=1000)\n\nfig, axes = pdp.pdp_plot(pdp_redshift,'r',figsize=(12, 7), \n                         ncols=3, frac_to_plot=1, \n)\nplt.show() ","b59c7da2":"# Create the data that we will plot\npdp_redshift= pdp.pdp_isolate(model=model, dataset=X_train, model_features=X_train.columns,\n                                    feature='g',num_grid_points=1000)\n\nfig, axes = pdp.pdp_plot(pdp_redshift,'g',figsize=(12, 7), \n                         ncols=3, frac_to_plot=1, \n)\nplt.show() ","0c6260f4":"ax=model.get_feature_importance(type='PredictionValuesChange',prettified=True)\n\nfig = px.bar(ax, x='Feature Id',y=['Importances'],\n              title=\"Cat Boost predictors importance - Prediction value change\",\n              color_discrete_sequence=px.colors.qualitative.Prism).update_layout( \n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\n\nfig.show()","3eb90d01":"ax=model.get_feature_importance(train_pool, type='LossFunctionChange',prettified=True)\n\nfig = px.bar(ax, x='Feature Id',y=['Importances'],\n              title=\"Cat Boost predictors importance - Loss Function Change\",\n              color_discrete_sequence=px.colors.qualitative.Vivid).update_layout( \n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\n\nfig.show()","bf6f94ce":"fi = model.get_feature_importance(train_pool,type=\"Interaction\", prettified=True)\nfi['First Feature Index'].replace({0:'u',1:'g',2:\"r\",3:\"i\",4:\"z\",5:\"redshift\",6:\"reds\",7:\"m_pho\"},inplace=True)\nfi['Second Feature Index'].replace({0:'u',1:'g',2:\"r\",3:\"i\",4:\"z\",5:\"redshift\",6:\"reds\",7:\"m_pho\"},inplace=True)\n\nfi['feature-interaction']=fi['First Feature Index']+\"-\"+fi['Second Feature Index']\n\nfig = px.bar(fi, x='feature-interaction',y=['Interaction'],\n              title=\"Cat Boost predictors importance- Feature interaction\",\n   color_discrete_sequence=px.colors.qualitative.Dark24).update_layout( \n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\n\nfig.show()","7df24641":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_val)\nshap.summary_plot(shap_values[0],X_val,alpha=0.5)","16e392c8":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_val)\nshap.summary_plot(shap_values[1], X_val,alpha=0.5)","77a8e848":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_val)\nshap.summary_plot(shap_values[2], X_val,alpha=0.5)","7311d710":"# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(model)\n\n# calculate shap values. This is what we will plot.\nshap_values = explainer.shap_values(X_val)\n\n# make plot.\nshap.dependence_plot(\"redshift\", shap_values=shap_values[0],interaction_index=\"g\",\n                     features=X_val,title='GALAXY Dependence Contribution Plot redshift-g')","bd7e269f":"shap.dependence_plot(\"redshift\", shap_values=shap_values[1],interaction_index=\"g\",\n                     features=X_val,title='QSO Dependence Contribution Plot redshift-g')","e3f9ddb6":"# make plot.\nshap.dependence_plot(\"redshift\", shap_values=shap_values[2],interaction_index=\"g\",\n                     features=X_val,title='STAR Dependence Contribution Plot redshift-g')","b02cbaab":"# make plot.\nshap.dependence_plot(\"g\", shap_values=shap_values[0],interaction_index=\"r\",\n                     features=X_val,title='GALAXY Dependence Contribution Plot g-r')","02fa9785":"# make plot.\nshap.dependence_plot(\"g\", shap_values=shap_values[1],interaction_index=\"r\",\n                     features=X_val,title='QSO Dependence Contribution Plot g-r')","5b77b69a":"# make plot.\nshap.dependence_plot(\"g\", shap_values=shap_values[2],interaction_index=\"r\",\n                     features=X_val,title='STAR Dependence Contribution Plot g-r')","af2d07bc":"# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(model)\n\n# calculate shap values. This is what we will plot.\nshap_values = explainer.shap_values(X_val)\n\n# visualize the validation set predictions\n# load JS visualization code to notebook\nshap.force_plot(explainer.expected_value[0], shap_values[0], X_val)","b041f5f5":"shap.force_plot(explainer.expected_value[1], shap_values[1], X_val)","c0fcc50c":"shap.force_plot(explainer.expected_value[2], shap_values[2], X_val)","b3ea298c":"explainer = shap.KernelExplainer(model.predict_proba, X_train)\nshap_values = explainer.shap_values(X_val.iloc[0,:])","55117bbd":"j=522\nprint(\"Object is a: \",y_val.iloc[j])\nshap.decision_plot(explainer.expected_value[1], shap_values[1], X_val.iloc[j,:])","22098428":"shap.force_plot(explainer.expected_value[1], shap_values[1], X_val.iloc[j,:])","0087a26d":"j=522\nprint(\"Object is a: \",y_val.iloc[j])\nshap.decision_plot(explainer.expected_value[0], shap_values[0], X_val.iloc[j,:])","e0e1e33d":"# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(model)\n\n# calculate shap values. This is what we will plot.\nshap_values = explainer.shap_values(X_val)\n\nj=522\nshap.multioutput_decision_plot(explainer.expected_value, shap_values,  feature_names=X_val.columns.tolist(),\n                               row_index=j,legend_labels=list(model.classes_),\n                               legend_location='lower right')   ","dc1259ef":"predict_fn = lambda x: model.predict_proba(x).astype(float)\nX_ = X_train.values\n\nexplainer = lime.lime_tabular.LimeTabularExplainer(X_, feature_names = X_train.columns.tolist(),\n                                                  class_names=['GALAXY',\"QSO\",'STAR'])","e9e934ba":"j=531\nprint(\"Real class:\",y_val.iloc[j])\nchoosen_instance = X_val.iloc[[j]].values[0]\n\nexp = explainer.explain_instance(choosen_instance, predict_fn,top_labels=1)\n\nexp.show_in_notebook(show_table=True, show_all=False)","247a135f":"j=855\nprint(\"Real class:\",y_val.iloc[j])\nchoosen_instance = X_val.iloc[[j]].values[0]\n\nexp = explainer.explain_instance(choosen_instance, predict_fn,top_labels=1)\n\nexp.show_in_notebook(show_table=True, show_all=True)","57328328":"### <a id='#1'> 5.3 Predictive Power Score PPS <\/a>\n\nPredictive Power Score or PPS is a data type agnostic and asymmetric metric that helps identify linear or nonlinear relationships between two variables in a data set. The spectrum of PPS values \u200b\u200bis between 0 (no predictive power) and 1 (highest predictive power). Through PPS we can find out how useful a variable will be to predict the values \u200b\u200bof another variable. Unlike the study of correlations, using decision trees the PPS can discover non-linear relationships between different columns that cannot be obtained from the correlation and can also handle categorical data. Also, unlike the correlation matrix, the PPS is asymmetric. This means that if variable A can predict the values \u200b\u200bof variable B, it does not mean that column B can also predict the values \u200b\u200bof column A, and PPS will show this interpretation.\n\nIf the task is ranking, we calculate the weighted F1 score (wF1) as the underlying evaluation metric (F1_model). As the F-1 score achieved is relative, a baseline score is computed for each prediction from a model that always predicts the most common class in the case of classification and the median prediction value in the case of regression.\n\nThe PPS is the result of the following normalization (always> = 0):\n\nPPS = (F1_model - F1_naive) \/ (1 - F1_naive)\n\nIf the method used is regression, the mean absolute error (MAE) evaluation metric is used. Again, in this case as well, we first calculate \"MAE for naive model\" and then, using this score, we generate the desired MAE for the predictive power score.\n\nHere the score is between 0 and 1, but since this score tells us about the error component, the lower it is, the better it is. The mathematical formula used to calculate the MAE is mentioned below.\nPPS = 1 - (MAE_model \/ MAE_naive)\n\nSource: https:\/\/github.com\/8080labs\/ppscore","f4224ef4":"Source text\n116 \/ 5000\nTranslation results\n### <a id='#1'> 5.4 PPS Matrix <\/a>\n\nThis table gives us the PPS between all the variables in the data set.","910399c4":"We can observe many relevant relationships. For example, while the redshift has a PPS of 0.96 with the target, the target has a PPS of 0.50 with the redshift. The photometric variables have a predictive power of 0 on the redshift.","0935237c":"### <a id='#1'> 3.1.3 KMeans Intercluster Distance Map <\/a>\n\nThis figure shows us in a reduced dimensional space the distance between the centroids of the 3 clusters using a multidimensional scale. How close or far the centers are in the visualization is derived from how close \/ far they are in the original data space. The size of the clusters is scaled by the number of instances that belong to each center, a fact that gives us an idea of the relative importance of each cluster.","18a14f32":"- Many low redshift values have a positive impact on the prediction of the stars, but there are also many stars with low redshift values whose impact on the model prediction was negative.\n\n- High values of g tend to have a negative impact on the prediction of stars and high values of z and i have a positive impact.","feb5179f":"We observe that cluster 0 (the one with the highest mean of the Silhouette score) is the one that is represented with more instances near its center.","eaf50477":"- The redshift is the variable that is most important according to the value of its SHAP values.\n\n- We observe how there are many observations with low redshift values that have a strong impact on the prediction of Galaxies and values with high redshift almost all have a negative impact on the prediction of Galaxies. There are many observations with low low redshift that also had a negative impact on the prediction of galaxies, so having low redshift does not seem like a very suitable predictor when identifying galaxies.\n\n- Having high ultraviolet values seems a fairly reliable characteristic when classifying galaxies.","bf0c6cb2":"We observe the same logic as in predictor i. Although the effects are very small, it seems to have somewhat positive effect for galaxies and stars and somewhat negative for quasars.","c15fde0c":"### <a id='#1'> 6.4 Creation of predictor matrices <\/a>\n\nWe create 4 matrices that we will use iteratively to train our best predictive model. We will use the matrix X of predictors that offers us better predictive power.\n\n**Final feature engineering assessment after training the model with cross-validation:**\n\n- Thanks to the insights obtained in our exploratory data analysis, we have obtained a comprehensive understanding of our variables and their relationship with the target. CatBoost models have been generated with 3 different predictor matrices. (X_1, X_2 and X_3).\n\n\n- The discretization of the redshift variable and the construction of the m_pho variable do not increase the predictive power of the model in the validation set. (model with matrix of predictors X_2).\n\n\n- Thanks to the insights obtained in the structural analysis of the model, it has been inferred that the interactions of \"redshift\" with \"g\" and with \"i\". (model with predictors X_3) had explanatory power over the target. The implementation of these interactions, however, has not improved the predictive power of the model. (loss of score F1 quasars).\n\n\n- The matrix X_1 of original predictors constructs the model with the best predictive capacity and also allows us a more direct interpretation of the behavior of the model.\n","aee64d3d":"These are the quarties intervals for the mean of the photometric variables.","bb021044":"<a id='#1'> Summary plot para clase QSO <\/a>","dd777da0":"### <a id='#1'> 7.1.5 Model evaluation: Confusion matrix and Classification Report <\/a>\n\nWe analyze the classificatory performance of our model by examining the Matrix confusion and the recall and precision of the model. The confusion matrix gives us the false negatives (lower left value) and false positives (upper right value). Correctly classified values \u200b\u200bare on the main diagonal. (class 0 as 0 and class 1 as 1)\n\n<a id='#1'> PRECISION <\/a>\n\nPrecision is defined as the number of true positives divided by the number of true positives plus the number of false positives. False positives are instances that the model incorrectly labels \"churn\" when they are actually negative. It expresses the proportion of the data points that our model says are relevant when in fact they were relevant.\n\n<a id='#1'> RECALL <\/a>\n\nThe recall is the number of true positives divided by the number of true positives plus the number of false negatives. True positives are data points classified as positive by the model that are actually positive (correct), and false negatives are data points that the model identifies as negative that are actually positive (incorrect). In the case of churn, the true positives are customers who left credit card services and are correctly identified, and the false negatives would be individuals labeled \"no churn\" by the model, but who actually dropped out. It expresses the ability to find all relevant instances in a data set.\n\n<a id='#1'> F1-SCORE <\/a>\n\nIn some situations, we may want to maximize recall or precision at the expense of the other metric. In the case that we are trying to \"churn\" customers, we can accept a lower precision if we can obtain a higher recall (we want to find many customers who intend to abandon credit card services)\n\nHowever, in cases where we want to find an optimal combination of precision and recall, we can combine the two metrics using what is called an F1 score.\n\nThe F1 score is the harmonic mean of precision and recall taking into account both metrics. The harmonic mean is used instead of a simple average because it punishes extreme values.","a2617699":"- We observe that the Box-Plot values of the redshift of the stars are so low compared to the quasars that they are not even visualized in the figure. All stars are in the first bin of the histogram [-0.1 0.1]\n\n\n- Galaxies have a slightly higher redshift than stars, but much lower than quasars. There is a galaxy with a redshift = 1.08, but more than 99% of the galaxies have a redshift <0.3\n\n\n- On a logarithmic scale, we observe how the vast majority of the high values of the redshift correspond to quasars. In the first bin [-0.1 0.1] there are only 17 quasars.","885b3fe8":"- In the partial dependence plot of the Galaxy redshift we observe that when the redshift is between 0-0.15 the probability that the object is a galaxy is pushed up to 1. From here the effect begins to decrease to practically 0 from a redshift of 1.5.\n\n\n- In the case of quasars, after a redshift> 0.1 approx, the probability that the object is a quasar increases progressively, being more than 90% from 1.5 to the end.\n\n\n- In the case of stars, the redshift causes a huge drop in class probability.","9b0ed8b6":"### <a id='#1'>3.1.1 Elbow method <\/a>","7277f24b":"92.7% of the quasars have a redshift between [0.18-5.46]","e799a0cf":"We observe that the redshift discrete variable has a moderately high PPS, but much lower than the original variable.","9dd0f5bd":"# <a id='#1'> 10. Model interpretability with Local Interpretable Model-agnostic Explanations (LIME) <\/a>\n\nLIME is a novel algorithm designed by Riberio Marco, Singh Sameer, Guestrin Carlos to access the behavior of the estimator of any base (model) using local interpretable surrogate models (for example, linear classifier \/ regressor). This form of comprehensive assessment helps to generate explanations that are locally accurate but may not align with global behavior. Basically, the LIME explanations are based on local surrogate models. These surrogate models are interpretable models (such as a linear model or a decision tree) that are learned about the predictions of the original model. But instead of trying to fit a global surrogate model, LIME focuses on fitting local surrogate models to explain why unique predictions have been made.\n\n LIME typically generates a new data set that consists of disturbed samples and the predictions of the associated black box model. On this data set, LIME then trains an interpretable model weighted by the proximity of the sampled instances to the instance of interest. Below is a standard high-level workflow for this.\n\n- Choose your instance of interest for which you want to have an explanation of the predictions of your black box model.\n- Disturb your data set and get the original model predictions for these new points.\n- Weight the new samples by their proximity to the instance of interest.\n- Fit a weighted and interpretable (surrogate) model in the data set with the variations.\n- Explain the prediction by interpreting the local model.\n\nBy default, Lime uses Ridge Regression as its interpretable linear model.","5a1b1b39":"El m\u00e9todo predict_proba de scikit learn nos permite obtener las probabilidades de que la observaci\u00f3n sea de una clase determinada. ","1c8cedc8":"We observe that modifying the redshift value, the model predictions vary by 77%. We observed that the ultraviolet, which obtained the score in the PPS and Mutual information has the second highest score.","e8ba0f8a":"# <a id='#1'> 3. Clustering <\/a>\n\nAlthough we have a data set already classified (each observation has one of the 3 astronomical classes assigned), we can explore how the data set responds with unsupervised clustering algorithms and if it is possible to find different groupings to the three astronomical objects of the DataSet.\n\nThe objective is to be able to generate cohesive groups and, at the same time, very different from each other in order to obtain groups with high intragroup similarity and low intergroup similarity.","317fd0d5":"##  <a id='#1'> 7.2 CatBoost with PCA <\/a>","160f9be2":"![sdss_asahi-2dnzj57.gif](attachment:sdss_asahi-2dnzj57.gif)","fcaff527":"From this point of view of the cost function of the model, the redshift also represents the most important variable.","313db310":"### <a id='#1'> 6.3 Examining PPS variables <\/a>\n\nUsing a PPS we can examine the predictive power attributed to the new variables.","7df1506a":"# <a id='#1'> 1. Selection and data preparation <\/a>\n\n### <a id='#1'> 1.1 DataSet Loading <\/a>\n\nWe only load the first 8,000 observations from the telescope so that we don't have to work with too heavy a data set.","0f964466":"With the quasar class the redshift relation is increasing, but from 0.5, the momentum of probability that the observation is a quasar has little marginal increase. We observe that observations with low values of green tend to drive the possibility that an object is a stone.","026b91d5":"This table gives us all the statistical data of each photometric variable for each of the astronomical objects.","e92e9ca4":"To better visualize the distribution we use a histogram with the count in logarithmic scale. We find a very asymmetric distribution to the left in which the value of the third quartile corresponds to only 0.093 redshift.","78eb1ab7":"<a id='#1'> Summary plot para clase STAR <\/a>","4e5b9663":"Regarding the distribution of the photometric variables by target, we observe how the ccasars tend to have higher values in all the predictors.","08098eb0":"- We observe the projections of maximum variability of the data for each of the photometric variables. The quasars are in the projection area of the variables z, i, r.\n\n- We also observe that there is a group of stars separated from the main group with high values of the variables z, i r.","51477f37":"This table has the statistical data of the redshift for each of the astronomical objects. We observe that the mean value of the stars is negative. In the case of the redshift we see that the caseres have an average redshift of 1.26 and the galaxies 0.077. In the case of stars, the maximum value of the redshift is 0.004153.","44110dfd":"We observe, for example, how the model predicts observation 522 of the validation set that corresponds to a Galaxy.","3b074c2f":"We observe that the model attributes the most important interaction to g-redshift.","9c28c6a5":"### <a id='#1'> 3.1.4 Principal Component Plot: Kmeans = 3 <\/a>\n\nWith PCA we can visually examine the distribution of the 3 clusters found. We can also compare the distribution of clusters with astronomical objects.","f568e6f6":"The effect of r is also quite low for all classes; but the probability that the object is a galaxy decreases slightly and the probability that it is a cuasar increases. It has practically no effect on the stars.","4969da52":"We can also visualize and explain decisions about multiple predictions and the direction of their impact. In the following figures we have all the predictions of the validation set for each of the classes. The default visualization shows some interesting decisions about model prediction patterns.","86362b44":"### <a id='#1'> 2.1 Pandas Profile Report <\/a>\n\nWith the Pandas Profile Report method we get a complete statistical exploration of our DataFrame.","2a281ef7":"# <a id='#1'> 11. Conclusions <\/a>\n\nIn this project, a data set (8,000 observations) has been selected, corresponding to spectral and photometric information on the capture of three types of astronomical objects: stars, galaxies and quasars. The data set collects photometric information from five optical bands (u, g, r, i z) and a spectroscopic study (red shift) of the astronomical objects produced by the study of images obtained by the telescope.\n\nThe statistical properties and the relationships, interactions and dependencies between the predictors and the target variable have been examined from multiple perspectives. A clustering of the data set has been carried out using K-means. The dimensionality of photometric attributes with PCA has been reduced. \n\nBy doing a split of 80%-20% of the data set (training-validation), a predictive model has been generated with the Gradient Boosting Catboost algorithm; achieving a high predictive capacity on the validation data. After analyzing the predictive capacity of the model with different combinations of predictor attributes, it was determined that the matrix of predictors without linear transformations (without reduction in PCA dimensionality) and without discretization offers the highest predictive power over the validation data (fewer errors). We have obtained a model with general accuracy of 99% and F1 scores of 0.99 for the GALAXY class, 0.98 for the QSO class and 0.99 for STAR. The model allows us to make predictions about new observations captured by the telescope with high reliability.\n\nFinally, the structural behavior of the model has been interpreted from different points of view, both from the global interpretative point of view, and in relation to the predictions of specific instances.","23e2a50b":"<a id='#1'> Force plot X_val for STAR <\/a>","7548a45c":"We can examine the 2-D distribution of astronomical objects with PCA to compare the distribution of objects in 2-d space with the three clusters of the k-means algorithm.","20675962":"We observe that after 400 iterations, the marginal improvement of the model errors begins to decline.","cfcf98ef":"We observe a totally decreasing trend from redshift> 0.1; when the redshift is close to zero. We see some blue points in the lower band of the shap value that also represent observations that have a low value of g. These observations drive the model to lower the probability of predicting that the object is a galazia.","c413e8d5":"The attribute that has the most mutual information with the target is the redshift by far from the others. The ultraviolet is the one that has less mutual information with the target. The other 4 variables have a similar mutual information with the target of over 20%.","5fbbfeb5":"### <a id='#1'> 3.1.2 Silhouette Plot <\/a>\n\nIn the following figure we have the Silhouette Plot of the 3 clusters that we have selected:","604c0912":"### <a id='#1'> 5.2 Mutual Info-classification <\/a>\n\nThis figure details the mutual information of each attribute with the target. Mutual information measures the reduction of uncertainty for one variable given a known value of another. The mutual information between two random variables X and Y can be formally established as follows:\n\n     I (X; Y) = H (X) - H (X | Y)\n\nWhere I (X; Y) is the mutual information for X and Y, H (X) is the entropy for X and H (X | Y) is the conditional entropy for X given Y.\n\nMutual information is a measure of dependency or \"mutual dependency\" between two random variables. As such, the measure is symmetric, which means that I (X, Y) = I (Y, X). Measures the average reduction in uncertainty about x that results from learning the value of y; or vice versa, the average amount of information that x conveys about y.","bd3fc8e2":"With the explained_variance_ratio method we will calculate the variance ratio of our PCs to know the total variance that they collect. As we know with PCA we find, in the data space, the dimensions with the greatest variation of the general variation of the data. In short, PCA replaces the original variables with new variables, called principal components, that are orthogonal (have zero covariations) and have variances (called eigenvalues) in decreasing order.","0faf851a":"The silhouette coefficient is a metric of the cohesion and separation of the groups. It measures how well an observation is doing in your group based on two factors:\n\n How close it is to the other observations in the group.\n How far it is from the rest of the observations of the other groups.\n\nThe silhouette coefficient values \u200b\u200bvary between -1 and 1.\n\n- A coefficient close to +1 means that the instance is within its own cluster and away from other groups.\n- A coefficient close to 0 means that it is close to another group.\n- A coefficient close to -1 means that the instance may have been assigned to the wrong cluster.\n\nThe silhouette coefficient of each cluster is the mean of all the observations in a score.\n\nThe silhouette coefficient of each observation =\na (b - a) \/ max (a, b)\n\nwhere a is the mean distance to the other observations in the same group (that is, the intragroup mean distribution) and b is the mean distance to the closest group.\n\nWe observe that cluster 0 is the one with the observations with the highest coefficient, which means that it has the highest cohesion as a cluster and is relatively relatively far from other groups.","77ee1966":"## <a id='#1'> 8.2 Feature importances <\/a>\n\nIn this section we will analyze the importance that the model attributes to predictors from different perspectives.\n\n### <a id='#1'> 8.2.1 Prediction Values Change <\/a>\n\nFor each predictor, the PredictionValuesChange shows how much the prediction changes on average if the value of the attribute changes. The greater the importance value, the greater the change in the prediction value on average, if this predictor is changed. The importance values of the characteristics are normalized so that the sum of the importances of all the characteristics equals 100.","803d2a50":"We train the model with 500 iterations validating the data with the validation set. From the properties of the DataSet and our predictors, the model establishes a learning rate of 0.146031. Using the validation set with the training of the data, the algorithm selects the number of trees that minimize the cost function.\n\nIn the log we can see how the learning error and the test or validation error decrease with each iteration (generation of new trees), as the model learns with the data. After 100 iterations, the model progressively has fewer errors in the validaton set. The model begins to overfit when the number of iterations increases but the number of validation errors does not decrease or begins to increase. As we have selected early_stopping_rounds = 50, the model stops when 50 iterations pass without reducing the errors of the validaton set.\n\n The model obtains the lowest errors with 420 trees. From here the model begins to overfit the training data.","168979e9":"The effect of g is similar to that of r, in relation to galaxies and quasars, but slightly negative for stars.","e1da6f01":"![SDSS.jpg](attachment:SDSS.jpg)","2fbd1621":"### <a id='#1'>2.2 Astrononomical class <\/a>","7e2c6dd5":"### <a id='#1'> 1.2 DataSet preparation <\/a>\n\nWe have observed that the DataSet does not contain any null values. Regarding the variables provided, we will only use those that contain some type of intrinsically relevant information about the astronomical objects of class. Therefore, we will only use the photometric ** (u, g, r, i, z,) ** and spectral ** (redshift) ** fields of the objects.","7789f21c":"<a id='#1'> Force plot X_val for QSO <\/a>","fa153da7":"### <a id='#1'> 9.2 Local interpretability <\/a>\n\nWe can break down individual predictions to show the impact of each attribute of the model on it. This casuistry is tremendously useful to make our predictions more interpretable. SHAP values do this in a way that guarantees a very important property. Specifically, we decompose a prediction with the following equation:\n\nsum (SHAP values for all attributes) = pred_for_instance - pred_for_baseline_values","f7d30ed7":"### <a id='#1'> Astronomical Class <\/a>\n\n- **Star:** A star is a luminous plasma spheroid that maintains its shape thanks to its own gravity.\n\n- **Galaxy:** A galaxy is a set of stars, gas clouds, planets, cosmic dust, dark matter and energy gravitationally bound together in a more or less defined structure.\n\n- **Quasar** (acronym for \"quasi-stellar radio source\", translation from English quasi-stellar radio source) is an astronomical source of electromagnetic energy, which includes radio frequencies and visible light.","82a00966":"In the case of quasars, we can observe a very definite positive relationship (in PDPs we have detected that these predictors had a similar behavior with quasars. As both values rise, the greater the positive effect on the quasars.","e642047a":"## <a id='#1'> 7.1 CatBoost with feature matrix X <\/a>","26d6dfd9":"We observe that the three clusters found have a clear separability with PCA. With PCA we project the directions of maximum variability of the original attributes, which help us to interpret the clusters obtained.\n\nClusters interpretation:\n    \n- Cluster 0: photometric variable mean values.\n    \n- Cluster 1: Low values of redshift and photometric variables.\n\n- Cluster 2: Higher redshift values and photometric variables except ultraviolet.","f15b90c6":"The report warns us that there are several variables with high levels of correlation. We also observe that the redshift distribution is very asymmetric. After this first examination of the data, we proceed to analyze the variables in detail.","14e23570":"- The ultraviolet has a positive effect on the probability that the object is a galaxy from u> 18.\n\n- The positive effect detected on the galacians turns into a negative effect on the quasars (to a similar magnitude).\n\n- The effect detected on the stars is almost null. It seems that at certain values between 18 and 20 the impulse is positively negative and others close to 0 or positive.","02cd5769":"### <a id='#1'>2.3 Redshift distribution  <\/a>","41684b0b":"### <a id='#1'> 2.4 Distributions Photometric System (u, g, r, i, z) <\/a>\n\nWe analyze the distributions of the photometric variables.","8b8be912":"*Interpretation instance 855 of the Validation set.*\n\nThis object was detected with negative redshift values, so it seems logical that the model almost certainly opted for it to be a star. The value of g g> 18.07 is the only predictor that makes the model fall for the probability that it is not a star.","72e588e2":"# <a id='#1'> 8. Model analysis <\/a>\n\nIn this section we will analyze the structural behavior of our model with the best score. We are going to examine its structural interpretability, both from the point of view of the importance \/ contribution of its predictors and the interpretative casuistry of its predictions.","120fd3f2":"###  <a id='#1'> 7.2.1 Train the model with PCA <\/a>","433f6fe1":"The distortion falls off rapidly when Kmeans reaches k = 3, but from there, it begins to decrease more slowly and its decrease becomes less significant.","a512f5a2":"After 150 trees, the ensemble has started to overfit and the number of errors with the validation set does not decrease; that is, the algorithm stops when adding 50 additional trees does not reduce the number of errors with the validation set.","1e76ce04":"This set of data presents a clear imbalance in the number of observations that we have for each class. Only 10.9% of the observations are quasars. Galaxies and stars are more balanced. When proposing our predictive algorithm we must take this casuistry into account in order not to generate a biased model.","942216e8":"In this figure we can observe the different mean values of each photometric variable for each astronomical object.\n\n- We observe that in all the photometric bands, the quasars have higher mean values, although the difference in ultraviolet is not very high.\n\n- Galaxies and stars have very similar mean values in all variables, but in u i and z there is a divergence that can be key to predicting classes.","6305491e":"### <a id='#1'> 4.1 PCA interpretation <\/a>","f1bc262a":"We observe the equatorial coordinates of the capture of the 8,000 images of the DataSet. We observed some blue spots, which mark regions where only stars were captured.","667205a4":"Although it appears that cluster 1 of k means roughly matches the cuasar class, since both have high redshift values and photometric attributes, the relationship of the clusters to stars and galaxies is much more diffuse.","c88f398b":"The error that we observe at each iteration is the mean of the errors of each fold (of the 3 total) of each iteration of the cross-validation.","911099dc":"###  <a id='#1'> 7.1.3 Train the model <\/a>","c7016cd9":"**Grouping por class: Variables fotom\u00e9tricas**","31a8deb0":"### <a id='#1'> 8.2.2 Loss Function Change <\/a>\n\nFor each predictor, the value represents the difference between the cost function of the model with and without this attribute. The model without this attribute is equivalent to what would have been trained if this attribute were excluded from the dataset. Since it is computationally expensive to retrain the model without one of the predictors, this model is roughly constructed using the original model with this predictor removed from all trees in the set. Calculating this importance requires a data set, and therefore the calculated value depends on the data set.","7a66eb0a":"### <a id='#1'> 2.4 Correlation matrix of photometric attributes and Red shift <\/a>","61924c5a":"# <a id='#1'> 4. PCA dimensionality reduction of photometric predictors <\/a>\n\nWe have detected very high levels of correlation between the photometric variables, so these variables are an ideal target to perform dimensionality reduction with PCA. The dimensionality reduction will help us to better visualize the relationships between the photometric data and the redshift for each type of astronomical object and to have fewer uncorrelated variables for predictive modeling.","83f62811":"### <a id='#1'> 9.2.1 Decision \/ Force plots <\/a>","14d6ef50":"In the case of stars, we observe that practically all values> 0 tend to lower the probability that the object is a star. In the case of the green, it seems that high green values somewhat boost the probability that an object is a star given the same redshift.","f1c32f3a":"We observe that only r, i and z have outlier values beyond the third quartile of their distribution. In general, we find quite asymmetric distributions with many outlier values. If necessary, we could carry out transformations with logarithms to have more normalized distributions.","4861273a":"- Examining the scatter plot Matrix of the PCs and the redshift we deduce that having a high redshift seems a very important indicator when it comes to identifying quasars.\n\n\n- The relationship between PC1 and PC2 is more difficult to visualize with the naked eye, but it seems that stars tend to have a high PC2, and galaxies a high PCA1.","636237be":"We observe that high values of g tend to be associated with high values of r. While observations with low values of g and r have little or no contribution to galaxies, most observations with both high values have a negative shap contribution.","580ba55c":"We also discretize the variable with its quartiles.","4b628cfd":"### <a id='#1'> 9.1.2 SHAP Dependence Contribution Plots <\/a>\n\nPreviously, we used Partial Dependency Plots to show how a single characteristic affects predictions. These are revealing and relevant to many real world use cases, but there is a lot of information that you don't see, for example what is the distribution of effects? Is the effect of having a certain value fairly constant or does it vary greatly depending on the values of other characteristics? Shap dependency contribution plots give us a similar view to PDPs, but add much more detail.\n\n\nEach point represents a row of data. The horizontal location is the actual value of the data set, and the vertical location shows the impact that value had on the prediction. We observe a spread between many observations (vertical separation between observations that had the same redshift). This indicates that there are other predictors that interact with the redshift and that influence the impact of the prediction. Color coding helps us interpret the impact of these interactions\n\n### <a id='#1'> SHAP Dependence Contribution Plots Redshift-G <\/a>\n\nWe will examine the dependence of the redshift contribution on the variable g (the variable that has obtained the highest score in the ranking of interactions) for each astronomical object.","e7c60c1d":"The discrete variable of the mean of the photometric values has a very low value, so it is highly probable that it is a variable of little use to build a robust predictive model.","b289f191":"These are the quartile intervals for the redshift.","60e7fb5e":"- High values of redshift is an essential characteristic when predicting quasars.\n\n- Having high ultraviolet values lowers the probability that an object is a stone\n\n- High values of g increase the probability that the object is a cuasar","a19f2121":"### <a id='#1'> Red Shift <\/a>\n\nIn astronomy, the  \"Red Shift\" study is a study of a section of the sky to measure the redshift of astronomical objects such as galaxies, galaxy clusters, stars, or quasars. Thus, using Hubble's law, the \"Red Shift\" can be used to estimate the distance of an object from Earth. By combining redshift with angular position data, a redshift study maps the 3D distribution of matter within a field of the sky. These observations are used to measure detailed statistical properties of the large-scale structure of the universe and can also be used to aid discrimination between astronomical objects such as stars, galaxies, and quasars.\n\nGenerally, the construction of a redshift study involves two phases: first images of the selected area of \u200b\u200bthe sky are taken with a wide-field telescope, then galaxies brighter than a defined boundary are selected from the resulting images as non-point objects.\n\nSecond, the selected galaxies are observed using spectroscopy, most commonly at visible wavelengths, to measure the wavelengths of prominent spectral lines; By comparing the observed and laboratory wavelengths, the redshift is obtained for each object.","bbb3d0de":"### <a id='#1'> SHAP Dependence Contribution Plots g-r <\/a>\nWe now examine the interaction of g and r, the interaction of photometric variables that Catboost has detected with greater importance","5458c0c5":"# <a id='#1'> 2. Variable exploratory analysis <\/a>\n\n### <a id='#1'> SDSS Photometric System (u, g, r, i, z) <\/a>\n\nIn astronomy, a photometric system is a set of optical filters with a known sensitivity to incident radiation. Sensitivity generally depends on the optical system, detectors and filters used.\n\nEach photometric letter designates a particular section of the electromagnetic spectrum.\n\n     \"U\" stands for ultraviolet.\n     \"G\" stands for green.\n     \"B\" stands for blue.\n     \"I\" stands for infrared.\n     \"Z\" stands for near-infrared.","36d63259":"### <a id='#1'> 5.5 Grouping by target class <\/a>\n\nUsing the Groupby method we can perform additional groupings such as sum (), mean (), median (), min () and max () according to various criteria. With this methodology we can obtain very complex insights about our data from different casuistry.","bf45b27e":"The first PC explains 88% of the overall variance, while the second represents 11%. The very high value of the first component is an indication of the high correlation between our variables. Thus, using both main components (and reducing it from 5 variables to 2) let us absorb 99% of the explanatory power of the data.","4c5dbc89":"We observe how all stars have a redshift <0.03.","1ac64ae5":"### <a id='#1'> 6.2 Discretization of the mean of the photometric variables (u, g, r, i, z) <\/a>\nWe can generate a new variable that collects the average of the five photometric variables.","c0417cc4":"- The dataset contains 8,000 labeled samples (stars, galaxies, and quasars) and 17 attributes.\n\n- The data set comes from an SQL query that joins two tables:\n\n     - \"PhotoObj\" which contains photometric data.\n     - \"SpecObj\" which contains spectral data.\n\n- 16 variables (double) and 1 additional variable (char) 'class'.\n\n- An astronomical object can be predicted from the other 16 variable\n\n###  <a id='#1'>Features<\/a>\n\n- objid = Object Identifier\n- ra = J2000 Right Ascension (r-band)\n- dec = J2000 Declination (r-band)\n- u = better of deV\/Exp magnitude fit (u-band)\n- g = better of deV\/Exp magnitude fit (g-band)\n- r = better of deV\/Exp magnitude fit (r-band)\n- i = better of deV\/Exp magnitude fit (i-band)\n- z = better of deV\/Exp magnitude fit (z-band)\n- run = Run Number\n- rerun = Rerun Number\n- camcol = Camera column\n- field = Field number\n- specobjid = Object Identifier\n- class = object class (galaxy, star or quasar object)\n- redshift = Final Redshift\n- plate = plate number\n- mjd = MJD of observation\n- fiberid = fiberID\n\n###  <a id='#1'>Target <\/a>\n\n- The variable 'class' identifies an object as a galaxy (GALAXY), a star (STAR) or a quasar (QSO).","bc532ada":"###  <a id='#1'> 7.1.4 Cross-validation evaluation <\/a>\n\nThe Cross-Validation function randomly divides the training set into K distinct subsets and\ntrain and evaluate the decision tree model k times, choosing a different subset for\nevaluation each time and training in the other k fold subsets. We will divide the data set into 10 subsets (k = 3).","c6e2e1c8":"We observe that 25% of the observations have a value less than -0.000033.","8e4120ca":"### <a id='#1'> 7.1.2 Creating the train and validation Pool <\/a>","ddc910c9":"### <a id='#1'> 9.2.2 Visualize multioutput predictions <\/a>\n\nDecision graphs can show how classificatory models arrive at predictions by showing the shap values obtained by all classes. For instance j = 522, we can see the model traversal for each attribute and class.","981fd55f":"We observe the same interpretive logic as in the other figures.\n\n- Redshift values around 0 drive the probability that the object is a star.\n- Values between 0.046-0.20 of approx redshift drive the probability that the object is a galaxy.\n- Redshift values> 0.077 impose the probability that the object is a cuasar.","3bf11712":"We observe how the quasars are projected onto the redshift. While the stars have a greater projection towards i and u. We also see a blue fringe of the galaxies projecting a bit onto the g.","d372e113":"# <a id='#1'> 9. Model interpretability with Shapley Additive explanations (SHAP) <\/a>\n\nThe Sahpley Additive explanations are game theory approach to explain the result of any machine learning model. Tree SHAP allows us to give an explanation of the behavior of the model, in particular of how each attribute impacts the predictions of the model. Each outcome \/ prediction is viewed as a sum of the contribution of each individual attribute.\n\nThe idea behind SHAP is to provide interpretability to machine learning models.\n\n- Global interpretability seeks to understand the general structure of the model. This involves doing a study on how the model works in general, not just on a specific prediction.\n\n- The local interpretability of the models consists in providing detailed explanations of why an individual prediction was made. This helps stakeholders to trust the model and know how to integrate its recommendations with other decision factors.\n\n### <a id='#1'> 9.1.1 Global interpretability with Summary plot <\/a>\n\nThis figure gives us a huge amount of information about the structure of the model. In the figure we can find:\n\n- Most important attribute of the model on the y-axis in descending order (at the top, the most important).\n\n- The SHAP value of the observations on the x-axis and shows whether the effect of that value caused a higher or lower prediction.\n\n- The value of each attribute with colors. A high value is represented by red, while a low value is represented by blue.\n\n- Each point represents a result of a prediction.","740e15ed":"The effect of i is very low for all classes; but it slightly boosts the probability that the object is a galaxy or a star and lowers the probability that it is a cuasar (but in very small magnitudes)","a8daad84":"### <a id='#1'> 9.1.3 Validation Set Force Plots <\/a>\n\nIn the following figures we have all the observations of the validaton set for each of the astronomical objects. For all the observations, we can examine the effect of each of the predictors on the impact that the object is of the mentioned class.","17606263":"<a id='#1'> Summary plot para clase GALAXY <\/a>","b69e14a8":"** PC1, PC2 interpretation: **\n\n- PC1: it is negatively correlated with all photometric elements, especially (g, r, i, z).\n\n\n- PC2: this component is more difficult to interpret. It is positively correlated with z, i and r and negatively with g. It is strongly negatively correlated with u.","08143659":"### <a id='#1'> 8.2.3 Feature Interaction <\/a>\n\nWe proceed to analyze the strength of the interactions between predictors for each pair. All partitions of the predictor pair of the model ensemble are examined to calculate the interaction between attributes. If there are divisions of both characteristics in the tree, then we are seeing how much the value of the leaf changes when the divisions have the same value and when they have opposite values.","aa41e5f5":"<a id='#1'> Force plot X_val for GALAXY <\/a>","c51bb88c":"## <a id='#1'> 8.1 Partial dependency plots\n\nPartial dependency graphs show the dependency between the target class and an attribute, removing the effects of all other attributes. These graphs can show if the relationship between the target class is linear, monotonous, ... In short, they help us better understand the dependencies between the attributes of our model and the target class. To compute these graphs, we predict the probability of the target class with new data, but changing the values of the variable before making a prediction. Thus, we first predict the class probability for the different values that the attribute takes and then we plot how the probability of the target class changes for the different values that the attribute takes.\n\nWe analyze the partial dependency plot of the redshift and the photometric metrics with the three astronomical objects. The area on the curve shows us the confidence interval with the standard deviation.\n    \nThe y value of the figures shows us the relative contribution of the predictor on each class.\n\n- Positive values mean a positive contribution by the mentioned class, by the values of the predictor.\n- Values 0 mean an absence of effect on the mentioned class by the predictor values\n- Negative values mean a negative contribution of the predictor on the mentioned class, due to the predictor values.","82f49b52":"### <a id='#1'> 7.1.1 Split your data into train and validation <\/a>\n\nWe keep 20% of the observations as validation set (out of sample data) to be able to predict with samples not used in training and to have an estimate of the errors. With the stratify = y method, we make sure that the two data sets have the same pore per class as the original data set.","ff503c9b":"The model obtains very high scores in all classes with the validation set. The lowest score is the precision of the quasars.","a9eeba76":"- With PCA we obtain results that are quite similar to the original data set, but with less precision when detecting quasars.\n\n\n- By having some main components that collect 99% of the explanatory power of the original data of the photometric variables, the predictive power of the PCA model remains robust.\n\n\n- If we are interested in having to work with fewer variables to have greater computational flexibility in training the model, we can have a model with good predictive capacity.\n\n\n- The disadvantage of working with PCA is that the interpretability of the model is much more complex, since it is difficult to correctly interpret what the PCA variables mean.","6c050512":"**Grouping por class: Redshift**","4136feb0":"The value of the redshift has been the one that has prompted the model to predict that it is a galaxy. If we interpret the same observation for the star class or when we obtain the opposite result.","80446eb9":"We build the CatBoost model using the matrix with PCA components.","de8535a6":"This figure shows us the predictive power score of all the attributes. In the line of the mutual information figure, we observe that the redshift is the attribute that has a very high score, close to 1. This metric attributes greater predictive power to variable z than to i. The PPS coincides with the MI that u is the variable with the lowest predictive power.","c426cc67":"### <a id='#1'> 7.2.2 Confusion matrix and Classification Report <\/a>","3d4b9c68":"<a id='#1'> CONFUSION MATRIX <\/a>\n\nThe confusion matrix gives us the false negatives and false positives of each class. Correctly classified values are on the main diagonal.","a7c0444c":"# <a id='#1'> 7. CatBoost Classifier <\/a>\n\nWe will create a predictor model with CatBoost, a decision tree Gradient boosting algorithm. This algorithm generates a very robust predictive model in the form of a set of weak decision trees. It builds the model in a stepwise fashion like other boosting methods do, and generalizes them allowing arbitrary optimization of a differentiable loss function. For more information see:\n\nhttps:\/\/catboost.ai\/news\/catboost-enables-fast-gradient-boosting-on-decision-trees-using-gpus\n\nGradient boosting with decision trees is a form of machine learning that works by progressively training more complex models to maximize the accuracy of predictions. The algorithm minimizes a cost function by iteratively choosing towards the negative gradient.","17bdf027":"We note that there are two stars that have been incorrectly classified as Galaxies and 6 galaxies that have been classified as stars. In relation to the quasars, the model erroneously predicts 7 quasars as Galaxies and one galaxy incorrectly classified as Cuasar.","b787a035":"# <a id='#1'> 6. Feature Engineering <\/a>\n\nIn data exploration we have examined the relationships and dependencies of the photometric and redshift variables with the different astronomical objects. Using the insights obtained we will create new variables that can help us to have more predictors and build more powerful predictive models.\n\n### <a id='#1'> 6.1 Redshift discretization <\/a>\n\n\nWe have observed that the Redshift has a very asymmetric right distribution with many small values and a few high values. The logarithmic transformation of the variable would give us a more normal and less asymmetric distribution, but the Redshift finds some negative values, which does not allow us to carry out the transformation.\n\nYes, we can discretize the variable according to the value intervals that cut each quartile. We have observed that the reshdift is a very important variable when it comes to differentiating astronomical objects.","73086712":"In the visualization PC1, PC2 with the redshift, we observe that the values with high redshift almost all caseres. Stars tend to have high PC2 values","566adc7c":"We observe that within the interval [0.03-0.18) almost all objects are galaxies. While only 0.073% of galaxies fall into the last bin of [0.18-5.46]","3dc50a78":"### <a id='#1'> 1.3 Equatorial coordinates of the picture samples <\/a>\n\nThe equatorial coordinate system is the preferred coordinate system for locating objects on the celestial sphere. Unlike the horizontal coordinate system, the equatorial coordinates are independent of the location of the observer and the time of the observation. This means that only one set of coordinates is required for each object, and that these same coordinates can be used by observers at different locations and at different times.\n\nThe equatorial coordinate system is basically the projection of the latitude and longitude coordinate system that we use here on Earth, onto the celestial sphere. By direct analogy, latitude lines are converted to declination lines (Dec; measured in degrees, arc minutes, and arc seconds) and indicate how far north or south of the celestial equator (defined by projecting Earth's equator above the celestial sphere) is the object. Lines of longitude have their equivalent in lines of right ascension (RA), but while longitude is measured in degrees, minutes and seconds east of the Greenwich meridian, RA is measured in hours, minutes and seconds east from where the The celestial equator intersects the ecliptic (the vernal equinox).","fa059a3a":"### <a id='#1'> 3.1 Clustering with K-means <\/a>\n\nTo carry out the Elbow method (we execute n = k iterations, increasing the value of K in each iteration and record the inertia (or sum of squared errors or distance of each point from its centroid). In the figure we observe that as k increases, the distortion decreases. This is because as more centroids are included, the distance from each point to its closest centroid decreases. The algorithm detects the optimal point (where the inertia curve begins to bend.) ak = 3, so having 3 groups matches the three types of astronomical objects.\n\nThe value k is chosen by striking a reasonable balance between inertia and a coherent grouping.","1f5f9512":"******\n- author: Xavier Martinez Bartra\n- date: December 2020\n******","acf5f8b5":"- Clase 0: GALAXY\n- Clase 1: QSO\n- Clase 2: STAR","8541e1e1":"- 0 We observe that the photometric variables are highly positively correlated with each other, especially (g, r, i, z), with correlations higher or close to 0.9. The u variable for ultraviolet light seems to have a more moderate correlation with the rest of the photometric variables.\n\n\n- The redshift is also moderately positively correlated with the photometric variables (values of 0.4 approx.) And very weakly with u.","fe4156f2":"In relation to stars, the dynamics are similar to those of galaxies, but even more defined.","5a6f2355":"*Interpretation instance 531 of the Validation set.*\n\nThe prediction seems to be logical considering the high value of the redshift. (1.98) (In the PDP we have seen that when the redshift increased by 1.5, the probability that the object is a cuasar was more than 90%. Although the ultraviolet value of this object is greater than 19.28, the probability that it is a cuasar has decreased somewhat, the high value of the redshift makes the model decide that the probability that it is a cuasar is 100%.","003f4ecb":"# <a id='#1'>Sloan Digital Sky Survey current DR16 Server: Data release with Galaxies, Stars and Quasars.<\/a>\n\n## <a id='#1'> Introduction <\/a>\n\nThe Sloan Digital Sky Survey or SDSS is an imagery space research project conducted on a specific 2.5-meter wide-angle telescope located at the Apache Point Observatory in New Mexico (United States). The project aims to map a quarter of the visible sky, obtain observations about 100 million objects and the spectrum of one million objects. The survey includes spectral and photometric information on all detected astronomical objects, including ** stars, galaxies and quasars. **\n\nThe Sloan Digital Sky Survey has created one of the most detailed three-dimensional maps of the Universe ever made, with deep, multi-color images of one-third of the sky and spectra for more than three million astronomical objects.\n\nThe data is published periodically and publicly. This dataset covers Data Release 16 (DR16) data.\n\nUsing the photometric characteristics and the spectroscopic analysis of the redshift we can predict which of the 3 types of celestial objects has been captured by the telescope. An important point to note from the data set is that the classes are not balanced, that is, there are many more stars and galaxies than quasars.","e861f087":"# <a id='#1'> 5. Predictor dependencies by target class <\/a>\n\nIn this section we will examine all the dependencies and relationships of the predictors by target class.\n\n### <a id='#1'> 5.1 Predictor dependencies by target class <\/a>\n\nRadViz is a multivariate data visualization algorithm that plots each attribute uniformly around the circumference of a circle and then plots points inside the circle such that the point normalizes its values on the axes from the center to each arc. This mechanism allows as many dimensions as can easily fit in a circle, expanding the dimensionality of the display. It gives us an idea of the separability between classes.","6e5b7c6e":"![orangepie.jpg](attachment:orangepie.jpg)","5a9fe992":"### <a id='#1'> 7.1.6 Predictions <\/a>\n\nThe model allows us to make predictions about new instances."}}