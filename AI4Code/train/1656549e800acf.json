{"cell_type":{"61cddf41":"code","64e210ef":"code","d382aeb0":"code","05d3c7e3":"code","2020eb1d":"code","09ce1722":"code","36eaa3dc":"code","250d8802":"code","e44370d9":"code","0bf4ace0":"code","062108a9":"code","aaad0f85":"code","3cd20092":"code","89e5aa4b":"code","9028ba8f":"code","40a8d287":"markdown","c1815415":"markdown","904802f5":"markdown","d5d805df":"markdown","e46c5cfd":"markdown","4f80cf10":"markdown","ca005d9e":"markdown"},"source":{"61cddf41":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport matplotlib as mpl\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.svm import SVC\n\nfrom xgboost import XGBClassifier\n","64e210ef":"plt.style.use(\"seaborn-bright\")\nmpl.rcParams[\"figure.figsize\"] = (10, 10)\nsns.set_theme(style=\"whitegrid\")","d382aeb0":"data = pd.read_csv('\/kaggle\/input\/diabetes-data-set\/diabetes.csv')\nprint(data.columns)\ndata.describe()","05d3c7e3":"#Tesing is we have missing values\nfor col in data.columns:\n    if data[col].isna().values.sum() > 0:\n        print(f\"Missing values in col: {col}\")\n#No outliers in this dataset\nsplitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\ntrain_data = None\ntest_data  = None\n\nfor train_ind, test_ind in splitter.split(data, data[\"Outcome\"]):\n    train_data = data.iloc[train_ind]\n    test_data = data.iloc[test_ind]\n\nprint(train_data.shape)\nprint(test_data.shape)\nprint(f\"Precentage of class 1 in train: {train_data['Outcome'].sum(axis=0)\/train_data.shape[0]}\")\nprint(f\"Precentage of class 1 in test: {test_data['Outcome'].sum(axis=0)\/test_data.shape[0]}\")\n\n","2020eb1d":"def visualizeDistribution(data, columnNames):\n    gs = GridSpec(3, 3)\n    axes = []\n    for i in range(0, 3):\n        for j in range(0, 3):\n            axes.append(plt.subplot(gs[i, j]))\n\n    for i in range(0, 3):\n        for j in range(0, 3):\n            data[columnNames[i*3 + j]].hist(ax=axes[i*3 +j], density=True);\n            data[columnNames[i*3 + j]].plot.kde(ax=axes[i*3 +j]);\n            axes[i*3 + j].set_yticks([])\n            axes[i*3 + j].set_xlabel(columnNames[i*3 + j])\n    #plt.title(\"Data Distribution using a Gaussian KDE\")\n    plt.show()\n    \ncolumnNames = list(data.columns)\nvisualizeDistribution(train_data, columnNames)","09ce1722":"fig, ax = plt.subplots(figsize=(15, 15))\nsns.boxplot(data = train_data, ax = ax)\nplt.show()","36eaa3dc":"#replacing outliers in the above mentioned columns by their means value\nmeans = {}\noutliers = {}\ncolumnsOutliers =  ['Pregnancies','Insulin','DiabetesPedigreeFunction', 'Age']\nfor col in columnsOutliers:\n    means[col] = train_data[col].mean(axis=0)\n\nprint(means)\n\nfor col in columnsOutliers:\n    outlier = train_data[col].quantile(0.98)\n    outliers[col] = outlier\n\n    \ndef replacingOutliers(data, means, outliers, columnsOutliers, dataType):\n    print(\"Outliers tranformation for \" + dataType)\n    for col in columnsOutliers:\n        print(f\"Replacing in {col}: {np.sum(data[col] > outliers[col])}\")\n        data.loc[data.index[data[col] > outliers[col]]][col] = means[col]\n\nreplacingOutliers(train_data, means, outliers, columnsOutliers, \"train\")\nreplacingOutliers(test_data, means, outliers, columnsOutliers, \"test\")\n\nvisualizeDistribution(train_data, columnNames)\nfig, ax = plt.subplots(figsize=(15, 15))\nsns.boxplot(data = train_data, ax = ax)\nplt.show()","250d8802":"# colors_diabetes = [\"red\", \"blue\"]\n#1 -> \"Diabetic\"\n#0 -> \"Not-Diabetic\"\ntrain_data.loc[train_data.index[train_data[\"Outcome\"] == 0]][\"Outcome\"] = \"Non-Diabetic\"\ncolumnNamesNoOutput = { key:value  for (key, value) in  enumerate(columnNames[0:len(columnNames)-1]) }\ncolumnNamesNoOutput\nprint(columnNamesNoOutput)\n\n\nwhile(True):\n    continueOrNot = int(input(\"Do you want to continue(1:cont, 0:break)\"))\n    if not continueOrNot:\n        break\n    col1 = int(input(\"Please input first column you want to see pair plot with(0 - 7):\"))\n    col2 = int(input(\"Please input second column you want to see pair plot with(0 - 7):\"))\n    if col1 in columnNamesNoOutput and col2 in columnNamesNoOutput:\n        sns.pairplot(train_data[[columnNamesNoOutput[col1], columnNamesNoOutput[col2], \"Outcome\"]], hue=\"Outcome\")\n        plt.show()","e44370d9":"corr = train_data[columnNamesNoOutput.values()].corr()# there is linear dependencies between-> glucode:Insulin, age:pregnancies and BMI: Insulin\ncorr","0bf4ace0":"X_train, y_train = np.array(train_data.drop(columns=[\"Outcome\"])), np.array(train_data[\"Outcome\"])\nX_test, y_test = np.array(test_data.drop(columns=[\"Outcome\"])), np.array(test_data[\"Outcome\"])\n\nstandardScalar = StandardScaler()\nstandardScalar.fit(X_train)\nstandardScalar.transform(X_train)\nstandardScalar.transform(X_test)\n","062108a9":"param_grid = [\n    {\"C\": [v for v in np.linspace(0.0001, 2, 10)]}\n]\n\nlogReg = LogisticRegression(random_state=42, penalty=\"l2\", max_iter=1000)\ngrid_search = GridSearchCV(logReg, param_grid, cv=5, scoring=\"neg_log_loss\", return_train_score=True)\ngrid_search.fit(X_train, y_train)\nprint(grid_search.best_params_)\nmodel = grid_search.best_estimator_","aaad0f85":"def results(model):\n    prediction = model.predict(X_train)\n    print(confusion_matrix(y_train, prediction))\n    print(classification_report(y_train, prediction))\n    prediction = model.predict(X_test)\n    print(confusion_matrix(y_test, prediction))\n    print(classification_report(y_test, prediction))\n\nresults(model)#Accuracy: 0.79 but bad recall score for diabetic class.","3cd20092":"param_grid = [\n    {\"kernel\": [\"rbf\"], \"C\": [v for v in np.logspace(-3, 2, 10)],\n    \"gamma\": [v for v in np.logspace(-3, 2, 10)]}\n]\n\nsvc = SVC(kernel=\"rbf\", random_state=42)\ngrid_search = GridSearchCV(svc, param_grid, cv=5)\n\ngrid_search.fit(X_train, y_train)\nprint(grid_search.best_params_)\n","89e5aa4b":"model = grid_search.best_estimator_\nresults(model)","9028ba8f":"model = XGBClassifier(booster=\"gbtree\",n_estimator=[10], max_depth=3, objective='binary:logistic', use_label_encoder=False )\nmodel.fit(X_train, y_train)\nresults(model)","40a8d287":"# Splitting the data into training and test set, in order not to do double dibbing","c1815415":"# Data transformation","904802f5":"It is evident from the above two figures that: Insulin, DiabetesPedigreeFunction, Age and Pregnancies.","d5d805df":"# Logistic Regression","e46c5cfd":"# Data visualization","4f80cf10":"# SVM for Classification","ca005d9e":"# Using XGBoost"}}