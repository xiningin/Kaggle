{"cell_type":{"a28d89ad":"code","1323f786":"code","e7df9c85":"code","64ac1a6d":"code","0916aaba":"code","cf2e5eea":"code","33dbbda0":"code","ea0d02cf":"code","f235aeca":"code","34cf6497":"code","8770045e":"code","a17b2edb":"code","1b807b1c":"code","e68070a2":"code","72a357f7":"code","df6ca722":"code","885b0f17":"code","2468d957":"code","4624f2df":"code","07d5a3d9":"code","c9ce4a84":"code","40c9dd1e":"code","401d70fa":"code","af041c3e":"code","c8fcbf9e":"code","e59a1a17":"code","ee070752":"markdown","ff01dc37":"markdown","e5f76d0d":"markdown","880bc584":"markdown","3fa71182":"markdown","c8805441":"markdown","70adaa38":"markdown","5802efd2":"markdown","1a7abf33":"markdown","15107318":"markdown","f17845ce":"markdown","ae5ac162":"markdown","60ec61d1":"markdown","7ddf02c3":"markdown","394a3222":"markdown","ec190ae1":"markdown","7c36fdb6":"markdown","2eb6a842":"markdown","e438fbfc":"markdown","6292beb6":"markdown","a5982bcd":"markdown","ce2af128":"markdown","afdb8bbe":"markdown","657abd9e":"markdown","ce19bf59":"markdown","9c3e7da9":"markdown","eedddc7d":"markdown","52b37101":"markdown","e24ce0a7":"markdown","68eb35f9":"markdown","6ba8281f":"markdown","0916069e":"markdown","4ef2a5db":"markdown","b60c6244":"markdown","ad67d87d":"markdown","61567cf6":"markdown","332ff871":"markdown","3424d937":"markdown","79c9e9c4":"markdown","443a8327":"markdown","40bb148a":"markdown","56cb12bb":"markdown","4eb4f9be":"markdown","49871d1e":"markdown","ab7ec044":"markdown","cb266733":"markdown","78d9a50b":"markdown","7d203c36":"markdown","0550e8b6":"markdown","521ae036":"markdown","648f67c0":"markdown","527f113d":"markdown","c512ba2d":"markdown","0da6c1a1":"markdown","1d2054f1":"markdown","d5a70545":"markdown","fe299c4f":"markdown","ddfa96a8":"markdown","a944e8a1":"markdown","ec564cdc":"markdown","4a5c5ba6":"markdown","bcd70ae9":"markdown","d4d0be6b":"markdown","0b454aeb":"markdown","e99f91c7":"markdown","408101cf":"markdown","c1ce920e":"markdown","9b31069f":"markdown","79e07a92":"markdown","2c56ec12":"markdown","f6fbad20":"markdown","c5b6de9c":"markdown","187844ed":"markdown","4765d5e0":"markdown","de00062e":"markdown","10b2ac52":"markdown","284af6b2":"markdown","f58bab38":"markdown","45a63267":"markdown","9b69b4a0":"markdown"},"source":{"a28d89ad":"!git clone -b CaseStudy_backend https:\/\/github.com\/WolfDev8675\/RepoSJX7.git\n!cp \/kaggle\/working\/RepoSJX7\/* \/kaggle\/working\/\n!pip install -r requirements_conda.txt","1323f786":"#imports\nfrom StatAnalysis import *\nfrom DataAccess import *\nfrom Intelligence_conda import * \nfrom Visuals import *\nfrom copy import deepcopy","e7df9c85":"#Data Access or Acquisition\ndHs=data_FixedTimeLine(\"BZ=F\",start=\"2015-10-01\",end=\"2019-12-30\")  #for model creation\ndhR=data_Live(period='5mo')                                        #for result comparison    ","64ac1a6d":"#Stock data show\nstockCandle(dHs,title=\" Brent Oil Price Fluctuations from October,2015 to December,2019 (BZ=F) \")","0916aaba":"dHs.head(10)","cf2e5eea":"dHs.info()","33dbbda0":"dHs.describe()","ea0d02cf":"dhR.head(10)","f235aeca":"dhR.info()","34cf6497":"dhR.describe()","8770045e":"# Trends and Seasonality(Exploration plot)\ntrendNseasonality(dHs[['Close']],model='additive',period=30)","a17b2edb":"# Interfield relation plots\nfullDataPlots(dHs[['Open','High','Low','Close']],title=\" Pairplot of %s\"%str(['Open','High','Low','Close'])[1:-1]) # pairplot \nfullDataPlots(dHs[['Open','High','Low','Close']],method=sns.boxplot,title='Box and Whiskers') # boxplot \nfullDataPlots(dHs[['Open','High','Low','Close']].corr(),method=sns.heatmap,title='Correlation HeatMap') # boxplot \nmultiPlots(dHs[['Open','High','Low','Close']])  # individual plots paired to each other (scatter pairs)","1b807b1c":"## per year open and close line curves \nfor year in sorted(set(dHs.index.year)):\n    yearVariation(dHs[['Close']],year)\n    yearVariation(dHs[['Open']],year)","e68070a2":"#Standardization\n#dHs=standardize(dHs[['Open','High','Low']])\n#***********\n#Normalization\n#dHs=normalization(dHs[['Open','High','Low']])","72a357f7":"# model class imports \nfrom sklearn.linear_model import LinearRegression as LinReg  #Linear Regression\nfrom sklearn.ensemble import RandomForestRegressor as RnForReg  #Random Forest Regression\nfrom sklearn.svm import SVR #Support Vector Machine Regression\nfrom sklearn.linear_model import BayesianRidge as BayesR   #Bayesian Ridge Regression ","df6ca722":"#Model Creation and study\nmodels_name={'LR':\"Linear Regression\",\n             'RF':\"Random Forest\",\n             'SV':\"Support Vector\",\n             'BR':\"Bayesian Ridge\"};\nmodels_mods_norm={'LR':Forecaster(model=LinReg()),\n                  'RF':Forecaster(model=RnForReg()),\n                  'SV':Forecaster(model=SVR()),\n                  'BR':Forecaster(model=BayesR())};\nmodels_mods_CVKF=deepcopy(models_mods_norm);","885b0f17":"# Train and Test (Stock process)\nfor mod_key in models_mods_norm:\n    models_mods_norm[mod_key].pushData(data=dHs,predicts=['Open','High','Low'],infers=\"Close\")\n    models_mods_norm[mod_key].normal_split()\n    models_mods_norm[mod_key].train()\n    models_mods_norm[mod_key].plotMetrics(data=dHs,title=\"Model: \"+models_name[mod_key])\n    models_mods_norm[mod_key].regression_report()","2468d957":"# Model training with K-Fold Cross Validation Splits\nfor mod_key in models_mods_CVKF:\n    models_mods_CVKF[mod_key].pushData(data=dHs,predicts=['Open','High','Low'],infers=\"Close\")\n    models_mods_CVKF[mod_key].crossval_KF_split(n_splits=5,random_state=None,shuffle=False)\n    models_mods_CVKF[mod_key].train()\n    models_mods_CVKF[mod_key].plotMetrics(data=dHs,title=\"Model: \"+models_name[mod_key]+\" Cross Validated \")\n    models_mods_CVKF[mod_key].regression_report()","4624f2df":"for mod_key in models_name:\n    print('\\n__________________________________________________________________\\n')\n    improvement(models_mods_norm[mod_key].accuracy(),models_mods_CVKF[mod_key].accuracy(),models_name[mod_key])\n    print(\"           Base Accuracy  : \",models_mods_norm[mod_key].accuracy())\n    print(\" Cross Validated Accuracy : \",models_mods_CVKF[mod_key].accuracy())","07d5a3d9":"for mod_K in models_name:\n    print('\\n__________________________________________________________________\\n')\n    print(models_name[mod_K])\n    models_mods_norm[mod_K].residualTest(title=\"General\",lineformat='C0:')\n    models_mods_CVKF[mod_K].residualTest(title=\"Cross Validated\",lineformat='C1-.');\n    print('\\n__________________________________________________________________\\n')","c9ce4a84":"#Generating Copies \nmodels_imprv_GEN=deepcopy(models_mods_norm);models_imprv_CVKF=deepcopy(models_mods_CVKF);","40c9dd1e":"# Linear Regression \nmodels_imprv_GEN['LR'].gradientEnhanced(alpha=0.01,iters=150);\nmodels_imprv_CVKF['LR'].gradientEnhanced(alpha=0.01,iters=150);\n\n# Bayesian Ridge Regression\nmodels_imprv_GEN['BR'].gradientEnhanced(alpha=0.01,iters=150);\nmodels_imprv_CVKF['BR'].gradientEnhanced(alpha=0.01,iters=150);\n","401d70fa":"parameters = {'kernel': ('linear', 'rbf','poly'), 'C':[1.5, 10],'gamma': [1e-7, 1e-4],'epsilon':[0.1,0.2,0.5,0.3]}\nmodels_imprv_GEN['SV'].model=models_imprv_GEN['SV'].hyperboost(param_grid=parameters);\nmodels_imprv_CVKF['SV'].model=models_imprv_CVKF['SV'].hyperboost(evaluator=GridSearchCV,param_grid=parameters);","af041c3e":"param_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\nmodels_imprv_GEN['RF'].model=models_imprv_GEN['RF'].hyperboost(param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 0);\nmodels_imprv_CVKF['RF'].model=models_imprv_CVKF['RF'].hyperboost(param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 0);","c8fcbf9e":"for keys in models_name:\n    print('\\n__________________________________________________________________\\n')\n    improvement(models_mods_norm[keys].accuracy(),models_mods_CVKF[keys].accuracy(),models_name[keys])\n    print(\"           Base Accuracy  : \",models_mods_norm[keys].accuracy())\n    print(\" Cross Validated Accuracy : \",models_mods_CVKF[keys].accuracy())\n    print('\\n__________________________________________________________________\\n')\n    improvement(models_mods_norm[keys].accuracy(),models_imprv_GEN[keys].accuracy(),models_name[keys])\n    print(\"           Base Accuracy  : \",models_mods_norm[keys].accuracy())\n    print(\"   Test Improved Accuracy : \",models_imprv_GEN[keys].accuracy())\n    print('\\n__________________________________________________________________\\n')\n    improvement(models_mods_CVKF[keys].accuracy(),models_imprv_CVKF[keys].accuracy(),models_name[keys])\n    print(\"                         Base Accuracy  : \",models_mods_CVKF[keys].accuracy())\n    print(\" Cross Validated Test Improved Accuracy : \",models_imprv_CVKF[keys].accuracy())\n    print('\\n__________________________________________________________________\\n')\n    print('\\n__________________________________________________________________\\n')","e59a1a17":"stockCandle(dhR,'Most Current Brent Oil Prices');\ntitle_string=models_name['SV']+'Hyperparameter boosted Cross Validated model'\nbest_mod_params=models_imprv_CVKF['SV'].model.get_params()\nprint(\"\\n__________________________________________________________________\\n\");print(title_string,'governing parameters');\nfor k in best_mod_params:\n    print(\" \",k,\":\",best_mod_params[k],sep='\\t')\nprint(\"\\n__________________________________________________________________\\n\");\nmodels_imprv_CVKF['SV'].plotMetrics(dhR,title=title_string)","ee070752":"<a id='@24'><\/a>\n### 24. **Why Regression? How the Regression Works ?**\n---\nA keen observer might have already noticed that we have brought only regression models into our field. \nRegression analysis is an incredibly powerful machine learning tool used for analyzing data. \n#### What Is Regression in Machine Learning?\nRegression analysis is a way of predicting future happenings between a dependent (target) and one or more independent variables (also known as a predictor). For example, it can be used to predict the relationship between reckless driving and the total number of road accidents caused by a driver, or, to use a business example, the effect on sales and spending a certain amount of money on advertising.\n\nRegression is one of the most common models of machine learning. It differs from classification models because it estimates a numerical value, whereas classification models identify which category an observation belongs to. _The main uses of regression analysis are forecasting, time series modeling and finding the cause and effect relationship between variables_. This also proves our point in usage of these algorithms.\n#### Why Is It Important?\nRegression has a wide range of real-life applications. It is essential for any machine learning problem that involves continuous numbers \u2013 this includes, but is not limited to, a host of examples, including:\n\n*  Financial forecasting (like house price estimates, or stock prices)\n*  Sales and promotions forecasting\n*  Testing automobiles\n*  Weather analysis and prediction\n*  Time series forecasting\n\nAs well as telling us whether a significant relationship exists between two or more variables, regression analysis can give specific details about that relationship. Specifically, it can estimate the strength of impact that multiple variables will have on a dependent variable. If we change the value of one variable (price, say), regression analysis should tell us what effect that will have on the dependent variable (sales).\n\nBusinesses can use regression analysis to test the effects of variables as measured on different scales. With it in your toolbox, you can assess the best set of variables to use when building predictive models, greatly increasing the accuracy of your forecasting.\n\nFinally, regression analysis is the best way of solving regression problems in machine learning using data modeling. By plotting data points on a chart and running the best fit line through them, we can predict each data point\u2019s likelihood of error: the further away from the line they lie, the higher their error of prediction (this best fit line is also known as a regression line).\n#### Terminologies used in Regression Analysis\n* **Outliers**\n    Suppose there is an observation in the dataset that has a very high or very low value as compared to the other observations in the data, i.e. it does not belong to the population, such an observation is called an outlier. In simple words, it is an extreme value. An outlier is a problem because many times it hampers the results we get.\n* **Multicollinearity**\n    When the independent variables are highly correlated to each other, then the variables are said to be multicollinear. Many types of regression techniques assume multicollinearity should not be present in the dataset. It is because it causes problems in ranking variables based on its importance, or it makes the job difficult in selecting the most important independent variable.\n* **Heteroscedasticity**\n    When the variation between the target variable and the independent variable is not constant, it is called heteroscedasticity. Example-As one\u2019s income increases, the variability of food consumption will increase. A poorer person will spend a rather constant amount by always eating inexpensive food; a wealthier person may occasionally buy inexpensive food and at other times, eat expensive meals. Those with higher incomes display a greater variability of food consumption.\n* **Underfit and Overfit**\n    When we use unnecessary explanatory variables, it might lead to overfitting. Overfitting means that our algorithm works well on the training set but is unable to perform better on the test sets. It is also known as a problem of high variance.\n    When our algorithm works so poorly that it is unable to fit even a training set well, then it is said to underfit the data. It is also known as a problem of high bias.\n    \n![Overfit&Underfit](https:\/\/lh6.googleusercontent.com\/1bN7Tlyb-tHmnIe2DWbEo4I3wcjkGkbkNevWo9thHcSQYE7LtZXj0sC8ordaUdneWn5r55sJu6YDaOFFlRGv0piQa4MldbeWnLj25QQPMv3sJgMjv0e1PEHWFjbsk-OpSHdvXUFkMeoGllNxww=s0)\n\n[\ud83e\udc09 Back to Contents](#@Contents)","ff01dc37":"<a id='@39'><\/a>\n### 39. **Closing Notes**\n---\nWe close our work with the content that our models have in fact quite more efficient. Exploration is misnormer on its limits, what feels a simple rabit's hole might take us deeper than expected. Although in future we hold wishes to keep the repository upgraded according to requirement and with versions of various libraries and python itself. We also want to include Neural Network algorithms, ARIMA, ARMA models in further iterations of the repository and is in works to produce a web interface for this prediction system, so that it is more suitable for future users and information seekers may find ease in using this system we tried to produce in the case study. \n\nFinally with these parting notes we close our work, <br> *__Thank you__ for your time and interest.*\n\n[\ud83e\udc09 Back to Contents](#@Contents)","e5f76d0d":"<a id='@36'><\/a>\n### 36. **Further Developments and Usage**\n---\nAs of now this case study just stands as a study analysis of machine learning methods as candidates in predicting the cost of a real life comodity of international importance. This work simply stands as a comparative research study of methods. Since this study is based on a time variant data or a time-series data option lies for us to explore possibilities like ARIMA, and ARMA like modelling for prediction analysis. Also a more potent candidate for these kinds of data are the Neural Networked Algorithms and venturing into the DNN(Deep Neural Networks) with RNN(Recurrent Neural Networks) involving a method of LSTM(Long Short Time Memory). Current research on these Networked models provide insight that these are more than enough potent for working with stock market data with special long time benefits. Although computing power required for these models is much more than the ones we explored here.\n\nUsage of this library and the current file is kept as a testimony of work done under the umbrella of BSE Institute&reg; and MAKAUT&reg; and gives them full control of the libraries with simple restrictions as is required by the licence set for protection of the work put in by the creator of these libraries in github repository control.\n\n[\ud83e\udc09 Back to Contents](#@Contents)","880bc584":"<a id='@14'><\/a>\n### 14. **Essential Library Imports and their Handling**\n---\nWith the information of the Library files shared in [Section 12](#@12) and with the imports and installation settled in [Section 13](#@13) we can import the required library files as follows. ","3fa71182":"---\n[\ud83e\udc09 Back to Contents](#@Contents)","c8805441":"<a id='@32'><\/a>\n### 32. **Improving the Models (Gradient Descent & Hyperparameter Boosting Methods)**\n---\nHaving the knowledge of working techniques of [Gradient Descent](#@30) and [Hyperparameter boosting](#@31) methods we can implement them as below.  ","70adaa38":"#### Random Forest Regressions","5802efd2":"---\n\n[\ud83e\udc09 Back to Contents](#@Contents)","1a7abf33":"<a id='@3'><\/a>\n### 3. **Why is Crude Oil Important**\n---\nGlobally, crude oil is one of the most important fuel sources and, historically, has contributed to over a third of the world\u2019s energy consumption. Discovering, extracting, shipping, and refining crude is a long process, and the infrastructure needed to support the process must be in place. This involves thousands of miles of oil pipelines across countries, storage facilities in major oil trading hubs, and multiple refineries. In aggregate, the global oil industry is a multi-trillion dollar industry.\n\nOil is especially important to businesses that heavily rely on fuel, such as airlines, plastic producers, and agricultural businesses. Being such an important source of energy, crude is a major import and export of numerous countries. The importance of this commodity creates a vast financial trading market for oil and oil derivatives such as futures, forwards, and options.\n\n[\ud83e\udc09 Back to Contents](#@Contents)","15107318":"<a id='@12'><\/a>\n### 12. **Library Structure (Files and their Importance)**\n---\nA detailed structure of the branch [CaseStudy_backend](https:\/\/github.com\/WolfDev8675\/RepoSJX7\/tree\/CaseStudy_backend) of the repository [RepoSJX7](https:\/\/github.com\/WolfDev8675\/RepoSJX7) is given. This is to also to be noted that the structure obtained is a response to a powershell tree command from local system configuration synced to Github and hence is in a coded format. \n```\nFolder PATH listing\nVolume serial number is 000000A7 1005:32D6\nE:\\SOURCE\\REPOS\\WOLFDEV8675\\REPOSJX7\n\u2502   Abstract.docx\n\u2502   DataAccess.py\n\u2502   ErrorTable.md\n\u2502   Intelligence.py\n\u2502   Intelligence_conda.py\n\u2502   LICENSE\n\u2502   README.md\n\u2502   requirements.txt\n\u2502   requirements_conda.txt\n\u2502   runtime.py\n\u2502   StatAnalysis.py\n\u2502   Visuals.py\n\u2502\n\u251c\u2500\u2500\u2500.vs\n\u2502   \u2502   ProjectSettings.json\n\u2502   \u2502   PythonSettings.json\n\u2502   \u2502   slnx.sqlite\n\u2502   \u2502   VSWorkspaceState.json\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500RepoSJX7\n\u2502       \u251c\u2500\u2500\u2500config\n\u2502       \u2502       applicationhost.config\n\u2502       \u2502\n\u2502       \u2514\u2500\u2500\u2500v16\n\u251c\u2500\u2500\u2500Data_static\n\u2502       BrentOilPrices.csv\n\u2502       HistoricalData_1628948226690.csv\n\u2502       HistoricalData_1628951636399.csv\n\u2502\n\u251c\u2500\u2500\u2500EarlyLife_module_tests\n\u2502       API_read_yfinance.py\n\u2502       API_yfinance.ipynb\n\u2502       api_yfinance.py\n\u2502       CSB_DataExplore.ipynb\n\u2502       csb_dataexplore.py\n\u2502\n\u2514\u2500\u2500\u2500__pycache__\n        Analysis.cpython-37.pyc\n        DataAccess.cpython-37.pyc\n        Intelligence.cpython-37.pyc\n        StatAnalysis.cpython-37.pyc\n        StatAnsys.cpython-37.pyc\n        Visuals.cpython-37.pyc\n```\nFolder Structure \n* *__ pycache__* : is cache folder created automatically for all Library files to be easily accessible by the Python compiler. All files are auto generated by the compiler and may not create any problem with their presence or absence.\n* *EarlyLife_module_tests*: folder contains files that were used to study modules at the infancy of this case study and is mostly irrelevant to the current standings of the study, but, undoubtedly help in paving the stepping stones.\n* *.vs* : is a Visual Studio 2019 system and git control presence or absence does not disturb operation. These kind of folder setup is created depending on the platform or IDE in which the codes are run. For example if we use IntelliJ Idea IDE then a simliar folder structure of the name *.idea* is created. \n* *Data_static*: is a set of preloaded datasets stored in CSV(comma separated variables).\n 1. BrentOilPrices.csv : Presupplied data only containing the date and close prices of the oil\n 2. HistoricalData_1628948226690.csv : Brent Oil Future shares data(BZ:NMX) from NASDAQ with a setting of 5 years data\n 3. HistoricalData_1628951636399.csv : Brent Oil Future shares data(BZ:NMX) from NASDAQ with a setting of MAX years data (10 years data)\nThe other files\n*   Abstract.docx : A task map of the functions we need to do \n*   DataAccess.py : Library file with functions specifically targetted to obtain data in a DataFrame from YahooFinance\n*   ErrorTable.md : A showcase tabulation in Markdown script implemented in the begining parts of the project to preview errors incurred in Models\n*   Intelligence.py : Library file with functions and classes to handle machine learning mechanisms from preparation till model training and assessment suitable for working in IDE environment.\n*   Intelligence_conda.py : Library file with the similar contents as the Intelligence.py file but modified a little bit to accomodate operation in Anaconda environment \n*   LICENSE : An MIT Opensource Licence linked in connection to the project \n*   README.md : Markdown script readme file to give explorers of the repository a brief of what is being done.\n*   requirements.txt : requirements file for the libraries required for proper operation of the project files in specified for IDE environments.\n*   requirements_conda.txt : requirements file with similar contents as the other requirements.txt file but modified to accomodate operation in Anaconda Environment\n*   runtime.py : IDE test file to test local operation of the modules and their tasks\n*   StatAnalysis.py : Library file with functions to perform statistical operations on the data and give insights of the nature of the data.\n*   Visuals.py : Library file with functions to handle all kinds of visualization operations, viz., graphs and plots and make them presentable \n\n#### Why we created the Library files?\nAt first glance the Library files might feel a bit over-engineered for the task but in reality they are just made to ease operation on the final end of the work, where instead of rewriting lengths of code over-and-over again for specific task but with different data or slightly different operation. \n#### Essential file set(Must have while coding)\n*   DataAccess.py\n*   Intelligence.py\n*   Intelligence_conda.py\n*   requirements.txt\n*   requirements_conda.txt\n*   StatAnalysis.py\n*   Visuals.py\n\n##### Exploration of the Essential Library files\n 1. DataAccess.py <br>\n     Code:\n     \n```\n    #!usr\/bin\/python\n\n    \"\"\" Module for accessing various amounts of data \"\"\"\n\n    # Imports \n\n    import pandas as pd\n    import yfinance as yf\n    import numpy as np\n    import warnings as warns\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize \n    from datetime import datetime as dt\n\n    def data_Prestored(location):\n        \"\"\"  Get access to prestored data \"\"\"\n        dataHist=pd.read_csv(location,index_col='Date',)\n        if 'Price' in dataHist.columns :\n            dataHist.rename(columns={'Price':'Close'},inplace=True)\n        return dataHist\n\n    def data_Live(ticker=\"BZ=F\",period=\"2M\"):\n        \"\"\" Access or import live\/current of a given stock \"\"\"\n        dataRaw=yf.Ticker(ticker)\n        DFrame=dataRaw.history(period=period)\n        return DFrame\n\n    def data_FixedTimeLine(ticker=\"BZ=F\",start=None,end=None):\n        \"\"\" Access or import stock data of a fixed timeline \"\"\"\n        if start is None or end is None:\n            warns.warn(\" Start or End limit must have some values, it shouldn't be (None) \",category=SyntaxWarning)\n            if start is None and end is not None:\n                dataRaw=yf.download(ticker,start=dt.now().date(),end=end)\n            if start is not None and end is None:\n                dataRaw=yf.download(ticker,start=start,end=dt.now().date())\n            if start is None and end is None:    \n                dataRaw=yf.Ticker(ticker).history()\n        else:\n            dataRaw=yf.download(ticker,start=start,end=end)\n        return dataRaw\n\n    def standardize(data,class_=StandardScaler):\n        \"\"\" Standardize the Data given \"\"\"\n        cols=data.columns\n        indexes=data.index\n        scaler=class_()\n        scaled=scaler.fit_transform(data)\n        return pd.DataFrame(data=scaled,columns=cols,index=indexes)\n\n    def normalization(data,func_=normalize):\n        \"\"\" Normalize the given data \"\"\"\n        cols=data.columns\n        indexes=data.index\n        normed=func_(data)\n        return pd.DataFrame(data=normed,columns=cols,index=indexes)\n ``` \n \n 2. Intelligence.py <br>\n     Code:\n     \n ```\n     #!usr\/bin\/python\n\n    \"\"\" Module for handling all Machine Learning models used in this Project \"\"\"\n\n    # imports\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split, KFold, GridSearchCV, RandomizedSearchCV \n    from sklearn import metrics \n    import matplotlib.pyplot as plots \n    \n    # Class definition for handling \n    class Forecaster():\n        \"\"\" Class Forecaster \n        target operation: Forcast with machine learning models,\n            the nature of data fluctuations in a stock price dataset. \"\"\"\n\n        # variables \n        model=None\n        predicts=None\n        infers=None\n        data=None\n        XData=None\n        YData=None\n        split_data=None\n        metric=None  #miscellaneous purpose metrics \n\n        def __init__(self, model, **kwargs):\n            \"\"\" Class Forcaster \n             creates a forcasting model \n\n             prameters: \n            model : the class of forcasting model \n            other parameters are with regards to specific variables in those classes \"\"\"\n\n            self.model=model\n            for var in kwargs:\n                if hasattr(self.model,var):\n                    setattr(self.model,var,kwargs[var])\n\n                # end of init\n\n        def pushData(self,data,predicts,infers):\n            \"\"\" Input the data that the model will use to train \n                parameters---\n                data: pandas.DataFrame object \n                predicts: list of field names used as predictors \n                infers: list of field name \"\"\"\n\n            if type(data) is not pd.DataFrame:\n                raise TypeError(\" data must be a pandas.DataFrame type object \")\n            if type(predicts) is not list:\n                raise TypeError(\" predicts must be a list datatype \")\n            self.data=data\n            self.predicts=predicts\n            if type(infers) is list:\n                self.infers=infers[0]\n            elif type(infers) is str:\n                self.infers=infers\n            else:\n                raise TypeError(\" infers must be a list or string datatype \")\n            self.XData=self.data[self.predicts]\n            self.YData=self.data[self.infers]\n\n            #end of pushData\n\n        def normal_split(self):\n            \"\"\" Trains the forecasting model \"\"\" \n            X_train,X_test,y_train,y_test=train_test_split(self.XData,self.YData,test_size=0.3,random_state=0)\n            self.split_data={'Train':{'x':X_train,'y':y_train},'Test':{'x':X_test,'y':y_test}}\n\n        def crossval_KF_split(self, **kwargs):\n            \"\"\" Creates the training and testing sets on K fold cross validation \"\"\"\n            CrsV=KFold();\n            for var in kwargs:\n                if hasattr(CrsV,var):\n                    setattr(CrsV,var,kwargs[var])\n            if [self.XData,self.YData] is not [None,None]:\n                for train_index, test_index in CrsV.split(self.XData):\n                    X_train,X_test=self.XData.iloc[train_index],self.XData.iloc[test_index]\n                    y_train,y_test=self.YData.iloc[train_index],self.YData.iloc[test_index]\n                self.split_data={'Train':{'x':X_train,'y':y_train},'Test':{'x':X_test,'y':y_test}}\n            else:\n                raise ValueError(\" No X,Y definition found \")\n\n        def train(self):\n            \"\"\" Trains the model on given data \"\"\"\n            self.model.fit(self.split_data['Train']['x'],self.split_data['Train']['y'])\n\n        def train_full(self):\n            \"\"\" Trains on the whole data \"\"\"\n            self.model.fit(self.XData,self.YData)\n\n        def plotMetrics(self,data,title=\"Model: unspecified\"):\n            \"\"\" Plots variation of the trained data \"\"\"\n            XData=data[self.predicts]\n            YData=data[self.infers]\n            prediction=self.model.predict(XData)\n            anotStr='\\n'.join((r\" Mean Absolute Error : %f\".ljust(45,' ')%metrics.mean_absolute_error(YData,prediction),\n            r\" Mean Squared Error : %f\".ljust(45,' ')%metrics.mean_squared_error(YData,prediction),\n            r\" Root Mean Squared Error : %f\".ljust(45,' ')%np.sqrt(metrics.mean_squared_error(YData,prediction)),\n            r\" R{:} : %f\".format('\\xb2').ljust(45,' ')%metrics.r2_score(YData,prediction),\n            r\" Adjusted R{:} : %f\".format('\\xb2').ljust(45,' ')%(1-(1-metrics.r2_score(YData,prediction))*(len(YData)-1)\/(len(YData)-XData.shape[1]-1)),\n            r\" Mean Absolute Percentage Error : %f\".ljust(45,' ')%metrics.mean_absolute_percentage_error(YData,prediction)))\n            Fig=plots.figure(figsize=(15,7.5));ax=Fig.add_subplot(111);\n            plots.plot(data.index,data[[self.infers]])\n            plots.plot_date(data.index,prediction,'g.-')\n            plots.text(0.147,0.915,anotStr,horizontalalignment='center',\n                verticalalignment='center', transform=ax.transAxes)\n            plots.title(title)\n            plots.show()\n\n        def getThetas(self):\n            \"\"\" Return the set of coefficients or thetas governing the model \"\"\"\n            C0=self.model.intercept_\n            CN=self.model.coef_\n            params=np.concatenate((np.array([C0]),CN))\n            return params\n\n        def cost_function(self,Xjs=None,Yjs=None,thetas=None):\n            \"\"\" Calculate the Cost incurred of Error residues \"\"\"\n            if Xjs==None and Yjs==None and thetas==None:\n                hypo=self.model.predict(self.XData)\n                delta=hypo-self.YData\n            else:\n                phi_thetas=Yjs\n                psi_theta=np.dot(Xjs,thetas)\n                deltas=psi_theta-phi_thetas\n            m=len(self.YData)\n            cost=np.sum(np.square(deltas))*0.5\/m\n            return cost\n\n        def grad_decent(self,alpha=0.01,iters=100):\n            \"\"\" Use Gradient Decent method to purify model \"\"\"\n            m=len(self.YData)\n            Xjs=np.c_[np.ones((len(self.XData),1)),self.XData]\n            phi_thetas=self.YData\n            thetas=self.model.getThetas()\n            History={}\n            for steps in range(iters):\n                psi_theta=np.dot(Xjs,thetas)\n                deltas=psi_theta-phi_thetas\n                del_J_del_theta=Xjs.T.dot(deltas)\/m\n                thetas=thetas-alpha*del_J_del_theta\n                History['Costs'][steps]=self.cost_function(Xjs=Xjs,Yjs=phi_thetas,thetas=thetas)\n                History['Thetas'][steps]=thetas\n            return History\n\n        def gradientEnhanced(self,alpha=0.01,iters=100):\n            \"\"\" Use Gradient descent functionality to provide an improved model\"\"\"\n            History=self.grad_decent(alpha=alpha,iters=iters)\n            minDeltaCost=min(History['Costs'].values)\n            best_cost=0\n            best_theta=0\n            for i in iters:\n                if History['Costs'][i]==minDeltaCost: \n                    best_cost=History['Costs'][i]\n                    best_theta=History['Thetas'][i]\n            self.model.intercept_=best_theta[0]\n            self.model.coef_=best_theta[1:]\n            self.metric={'ModelClass':type(self.model),'Best cost found':best_cost}\n            return self.model\n\n        def hyperboost(self,evaluator=GridSearchCV,**kwargs):\n            \"\"\" Variable boosting for model improvement using \n            process of Hyperparameter Boost \n            this is applicable for only the Vector Machines and Ensemble Forest methods \n            *********************\n            Variables \n            ---------------------\n            evaluator : GridSearchCV (default) also usable is RandomizedSearchCV \n            **kwargs : all settings that are applicable to be used in the evaluator algorithm \n            -----------------------\n            returns estimator model \n            (Created specifically with RandomForestRegressor and SupportVectorRegressor \n             other algorithms may be used if behaviour matches similar patterns)\n            _______________________\n            \"\"\"\n            worker=evaluator(estimator=self.model);\n            for var in kwargs:\n                if hasattr(worker,var):\n                    setattr(worker,var,kwargs[var])\n            worker.fit(self.split_data['Train']['x'],self.split_data['Train']['y'])\n            return worker.best_estimator_\n\n        def accuracy(self):\n            \"\"\" Calculate the Accuracy of the Regression Model \"\"\"\n            predict=self.model.predict(self.split_data['Test']['x'])\n            errs=abs(predict-self.split_data['Test']['y'])\n            MAE_P=100*np.mean(errs\/self.split_data['Test']['y'])\n            return 100- MAE_P\n\n        def regression_report(self):\n            \"\"\" Regression report collection \"\"\"\n            y_true,y_predic=self.split_data['Test']['y'],self.model.predict(self.split_data['Test']['x'])\n            errors=y_true-y_predic\n            percentiles=[5,25,50,75,95]\n            perc_vals=np.percentile(errors,percentiles)\n\n            reports=[\n                ('Mean Absolute Error',metrics.mean_absolute_error(y_true,y_predic)),\n                ('Mean Squared Error',metrics.mean_squared_error(y_true,y_predic)),\n                ('Root Mean Squared Error',metrics.mean_squared_error(y_true,y_predic)),\n                ('Mean Squared Log Error',metrics.mean_squared_log_error(y_true,y_predic)),\n                ('Median Absolute Error',metrics.median_absolute_error(y_true,y_predic)),\n                (' R{:} '.format('\\xb2'),metrics.r2_score(y_true,y_predic)),\n                (' Adjusted R{:} '.format('\\xb2'),(1-(1-metrics.r2_score(y_true,y_predic))*(len(y_true)-1)\/(len(y_true)-self.split_data['Test']['x'].shape[1]-1))),\n                ('Mean Poisson Deviance',metrics.mean_poisson_deviance(y_true,y_predic)),\n                ('Mean Gamma Deviance',metrics.mean_gamma_deviance(y_true,y_predic)),\n                ('Max Error',metrics.max_error(y_true,y_predic)),\n                ('Explained Variance',metrics.explained_variance_score(y_true,y_predic)),\n                ('Mean Absolute Percentage Error',metrics.mean_absolute_percentage_error(y_true,y_predic))]\n\n            print(\"\".center(70,'_'))\n            print(' Regression Metrics Report ')\n            for metric,value in reports:\n                print(f'{metric:>35s}: {value: >15.6f}')\n            print(\"\".center(70,'_'))\n            print(' Percentiles ')\n            for per,vals in zip(percentiles,perc_vals):\n                print(f'{per:>15d}: {vals: >15.6f}')\n\n    def improvement(oldVAL,newVAL,title_text=''):\n        \"\"\" Calculate improvement of a value over its old self \n         ******************\n         Variables \n        -------------------\n        oldVAL : old value of the variable \n        newVAL : new value of the variable \n        ____________________\n        prints improvement percentage \n        returns improvement in percentage (float) \"\"\"\n        imprv=100*(newVAL-oldVAL)\/oldVAL\n        print(title_text)\n        print(' Improved by {:0.3f}%'.format(imprv))\n        return imprv\n ```\n \n \n 3. Intelligence_conda.py <br>\n     Code:\n \n  ```\n    #!usr\/bin\/python\n\n    \"\"\" Module for handling all Machine Learning models used in this Project \"\"\"\n\n    # imports\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split, KFold, GridSearchCV, RandomizedSearchCV\n    from sklearn import metrics #@post-analysis\n    import matplotlib.pyplot as plots  #@visuals\n\n    class Forecaster():\n        \"\"\" Class Forecaster \n        target operation: Forcast with machine learning models,\n            the nature of data fluctuations in a stock price dataset. \"\"\"\n\n        # variables \n        model=None\n        predicts=None\n        infers=None\n        data=None\n        XData=None\n        YData=None\n        split_data=None\n        metric=None\n\n        def __init__(self, model, **kwargs):\n            \"\"\" Class Forcaster \n             creates a forcasting model \n\n             prameters: \n            model : the class of forcasting model \n            other parameters are with regards to specific variables in those classes \"\"\"\n\n            self.model=model\n            for var in kwargs:\n                if hasattr(self.model,var):\n                    setattr(self.model,var,kwargs[var])\n\n                # end of init\n\n        def pushData(self,data,predicts,infers):\n            \"\"\" Input the data that the model will use to train \n                parameters---\n                data: pandas.DataFrame object \n                predicts: list of field names used as predictors \n                infers: list of field name \"\"\"\n\n            if type(data) is not pd.DataFrame:\n                raise TypeError(\" data must be a pandas.DataFrame type object \")\n            if type(predicts) is not list:\n                raise TypeError(\" predicts must be a list datatype \")\n            self.data=data\n            self.predicts=predicts\n            if type(infers) is list:\n                self.infers=infers[0]\n            elif type(infers) is str:\n                self.infers=infers\n            else:\n                raise TypeError(\" infers must be a list or string datatype \")\n            self.XData=self.data[self.predicts]\n            self.YData=self.data[self.infers]\n\n            #end of pushData\n\n        def normal_split(self):\n            \"\"\" Trains the forecasting model \"\"\" \n            X_train,X_test,y_train,y_test=train_test_split(self.XData,self.YData,test_size=0.3,random_state=0)\n            self.split_data={'Train':{'x':X_train,'y':y_train},'Test':{'x':X_test,'y':y_test}}\n\n        def crossval_KF_split(self, **kwargs):\n            \"\"\" Creates the training and testing sets on K fold cross validation \"\"\"\n            CrsV=KFold();\n            for var in kwargs:\n                if hasattr(CrsV,var):\n                    setattr(CrsV,var,kwargs[var])\n            if [self.XData,self.YData] is not [None,None]:\n                for train_index, test_index in CrsV.split(self.XData):\n                    X_train,X_test=self.XData.iloc[train_index],self.XData.iloc[test_index]\n                    y_train,y_test=self.YData.iloc[train_index],self.YData.iloc[test_index]\n                self.split_data={'Train':{'x':X_train,'y':y_train},'Test':{'x':X_test,'y':y_test}}\n            else:\n                raise ValueError(\" No X,Y definition found \")\n\n        def train(self):\n            \"\"\" Trains the model on given data \"\"\"\n            self.model.fit(self.split_data['Train']['x'],self.split_data['Train']['y'])\n\n        def train_full(self):\n            \"\"\" Trains on the whole data \"\"\"\n            self.model.fit(self.XData,self.YData)\n\n        def plotMetrics(self,data,title=\"Model: unspecified\"):\n            \"\"\" Plots variation of the trained data \"\"\"\n            XData=data[self.predicts]\n            YData=data[self.infers]\n            prediction=self.model.predict(XData)\n            anotStr='\\n'.join((r\" Mean Absolute Error : %f\".ljust(45,' ')%metrics.mean_absolute_error(YData,prediction),\n            r\" Mean Squared Error : %f\".ljust(45,' ')%metrics.mean_squared_error(YData,prediction),\n            r\" Root Mean Squared Error : %f\".ljust(45,' ')%np.sqrt(metrics.mean_squared_error(YData,prediction)),\n            r\" R{:} : %f\".format('\\xb2').ljust(45,' ')%metrics.r2_score(YData,prediction),\n            r\" Adjusted R{:} : %f\".format('\\xb2').ljust(45,' ')%(1-(1-metrics.r2_score(YData,prediction))*(len(YData)-1)\/(len(YData)-XData.shape[1]-1)),\n            r\" Mean Absolute Percentage Error : %f\".ljust(45,' ')%self.mean_absolute_percentage_error(YData,prediction)))\n            Fig=plots.figure(figsize=(15,7.5));ax=Fig.add_subplot(111);\n            plots.plot(data.index,data[[self.infers]])\n            plots.plot_date(data.index,prediction,'g.-')\n            plots.text(0.147,0.915,anotStr,horizontalalignment='center',\n                verticalalignment='center', transform=ax.transAxes)\n            plots.title(title)\n            plots.show()\n\n        def getThetas(self):\n            \"\"\" Return the set of coefficients or thetas governing the model \"\"\"\n            C0=self.model.intercept_\n            CN=self.model.coef_\n            params=np.concatenate((np.array([C0]),CN))\n            return params\n\n        def cost_function(self,Xjs=None,Yjs=None,thetas=None):\n            \"\"\" Calculate the Cost incurred of Error residues \"\"\"\n            if Xjs==None and Yjs==None and thetas==None:\n                hypo=self.model.predict(self.XData)\n                delta=hypo-self.YData\n            else:\n                phi_thetas=Yjs\n                psi_theta=np.dot(Xjs,thetas)\n                deltas=psi_theta-phi_thetas\n            m=len(self.YData)\n            cost=np.sum(np.square(deltas))*0.5\/m\n            return cost\n\n        def grad_decent(self,alpha=0.01,iters=100):\n            \"\"\" Use Gradient Decent method to purify model \"\"\"\n            m=len(self.YData)\n            Xjs=np.c_[np.ones((len(self.XData),1)),self.XData]\n            phi_thetas=self.YData\n            thetas=self.model.getThetas()\n            History={}\n            for steps in range(iters):\n                psi_theta=np.dot(Xjs,thetas)\n                deltas=psi_theta-phi_thetas\n                del_J_del_theta=Xjs.T.dot(deltas)\/m\n                thetas=thetas-alpha*del_J_del_theta\n                History['Costs'][steps]=self.cost_function(Xjs=Xjs,Yjs=phi_thetas,thetas=thetas)\n                History['Thetas'][steps]=thetas\n            return History\n\n        def gradientEnhanced(self,alpha=0.01,iters=100):\n            \"\"\" Use Gradient descent functionality to provide an improved model\"\"\"\n            History=self.grad_decent(alpha=alpha,iters=iters)\n            minDeltaCost=min(History['Costs'].values)\n            best_cost=0\n            best_theta=0\n            for i in iters:\n                if History['Costs'][i]==minDeltaCost: \n                    best_cost=History['Costs'][i]\n                    best_theta=History['Thetas'][i]\n            self.model.intercept_=best_theta[0]\n            self.model.coef_=best_theta[1:]\n            self.metric={'ModelClass':type(self.model),'Best cost found':best_cost}\n            return self.model\n\n        def hyperboost(self,evaluator=GridSearchCV,**kwargs):\n            \"\"\" Variable boosting for model improvement using \n            process of Hyperparameter Boost \n            this is applicable for only the Vector Machines and Ensemble Forest methods \n            *********************\n            Variables \n            ---------------------\n            evaluator : GridSearchCV (default) also usable is RandomizedSearchCV \n            **kwargs : all settings that are applicable to be used in the evaluator algorithm \n            -----------------------\n            returns estimator model \n            (Created specifically with RandomForestRegressor and SupportVectorRegressor \n             other algorithms may be used if behaviour matches similar patterns)\n            _______________________\n            \"\"\"\n            worker=evaluator(estimator=self.model);\n            for var in kwargs:\n                if hasattr(worker,var):\n                    setattr(worker,var,kwargs[var])\n            worker.fit(self.split_data['Train']['x'],self.split_data['Train']['y'])\n            return worker.best_estimator_\n\n        def accuracy(self):\n            \"\"\" Calculate the Accuracy of the Regression Model \"\"\"\n            predict=self.model.predict(self.split_data['Test']['x'])\n            errs=abs(predict-self.split_data['Test']['y'])\n            MAE_P=100*np.mean(errs\/self.split_data['Test']['y'])\n            return 100- MAE_P\n\n        def regression_report(self):\n            \"\"\" Regression report collection \"\"\"\n            y_true,y_predic=self.split_data['Test']['y'],self.model.predict(self.split_data['Test']['x'])\n            errors=y_true-y_predic\n            percentiles=[5,25,50,75,95]\n            perc_vals=np.percentile(errors,percentiles)\n\n            reports=[\n                ('Mean Absolute Error',metrics.mean_absolute_error(y_true,y_predic)),\n                ('Mean Squared Error',metrics.mean_squared_error(y_true,y_predic)),\n                ('Root Mean Squared Error',metrics.mean_squared_error(y_true,y_predic)),\n                ('Mean Squared Log Error',metrics.mean_squared_log_error(y_true,y_predic)),\n                ('Median Absolute Error',metrics.median_absolute_error(y_true,y_predic)),\n                (' R{:} '.format('\\xb2'),metrics.r2_score(y_true,y_predic)),\n                (' Adjusted R{:} '.format('\\xb2'),(1-(1-metrics.r2_score(y_true,y_predic))*(len(y_true)-1)\/(len(y_true)-self.split_data['Test']['x'].shape[1]-1))),\n                ('Mean Poisson Deviance',metrics.mean_poisson_deviance(y_true,y_predic)),\n                ('Mean Gamma Deviance',metrics.mean_gamma_deviance(y_true,y_predic)),\n                ('Max Error',metrics.max_error(y_true,y_predic)),\n                ('Explained Variance',metrics.explained_variance_score(y_true,y_predic)),\n                ('Mean Absolute Percentage Error',self.mean_absolute_percentage_error(y_true,y_predic))]\n\n            print(\"\".center(70,'_'))\n            print(' Regression Metrics Report ')\n            for metric,value in reports:\n                print(f'{metric:>35s}: {value: >15.6f}')\n            print(\"\".center(70,'_'))\n            print(' Percentiles ')\n            for per,vals in zip(percentiles,perc_vals):\n                print(f'{per:>15d}: {vals: >15.6f}')\n\n        def mean_absolute_percentage_error(self,y_true,y_predic):\n            \"\"\" Function emulates the function of the same name as is\n            in sklearn.metrics of sklearn.__version__==0.24\n            this is done for conda\/anaconda related environment where sklearn.__version__==0.23.x \n            \"\"\"\n            errs=abs(y_predic-y_true)\n            MAE_P=100*np.mean(errs\/y_true)\n            return MAE_P\n\n    def improvement(oldVAL,newVAL,title_text=''):\n        \"\"\" Calculate improvement of a value over its old self \n         ******************\n         Variables \n        -------------------\n        oldVAL : old value of the variable \n        newVAL : new value of the variable \n        ____________________\n        prints improvement percentage \n        returns improvement in percentage (float) \"\"\"\n        imprv=100*(newVAL-oldVAL)\/oldVAL\n        print(title_text)\n        print(' Improved by {:0.3f}%'.format(imprv))\n        return imprv\n ```\n \n 4. requirements.txt <br>\n     Code:\n ```\n        matplotlib==3.4.3\n        matplotlib-inline==0.1.3\n        numpy==1.19.5\n        pandas==1.3.3\n        plotly==5.3.1\n        scikit-learn==1.0\n        scipy==1.7.1\n        seaborn==0.11.2\n        statsmodels==0.13.0\n        yfinance==0.1.63\n ```\n \n 5. requirements_conda.txt <br>\n     Code:\n ```\n        matplotlib\n        matplotlib-inline\n        numpy\n        pandas\n        plotly\n        scikit-learn\n        scipy\n        seaborn\n        statsmodels\n        yfinance\n ```\n \n 6. StatAnalysis.py <br>\n     Code:\n ```\n    #!usr\/bin\/python\n\n    \"\"\" Module for handling Statistical analysis of data \"\"\"\n\n    #imports\n    from statsmodels.tsa.seasonal import seasonal_decompose\n    from Visuals import singlePlot,sns,warns,plts\n    from DataAccess import pd,dt\n\n    def trendNseasonality(data,model='additive',period=30):\n        \"\"\" Visually discuss trend and seasonality of data \"\"\"\n\n        model_TS=seasonal_decompose(data,model=model,period=period)\n        #Fig=plts.figure(figsize=(15,7.5))\n        Fig=model_TS.plot()\n        Fig.suptitle(\" Trend and Seasonality in the Data \")\n        Fig.set_figwidth(15)\n        Fig.set_figheight(7.5)\n        plts.subplots_adjust(0.07,0.08,0.977,0.94)\n        return plts.show()\n\n    def yearVariation(data,year):\n        \"\"\" Generate visual statistics of variation of a time-series data with year \"\"\"\n        data_local=data[data.index.year==year]\n        cols=data.columns\n        Fig=plts.figure(figsize=(15,7.5));ax=Fig.add_subplot(111);\n        sns.lineplot(data=data_local)\n        plts.xlabel('Year : %d'%year)\n        plts.title(str(list(cols))[2:-2]+' for '+str(year))\n        return plts.show()\n\n ```\n \n 7. Visuals.py <br>\n     Code:\n ```\n    #!usr\/bin\/python\n\n    \"\"\" Module to help in plotting solutions \"\"\"\n\n    #imports\n    import pandas as pd\n    import numpy as np\n    import seaborn as sns\n    import matplotlib.pyplot as plts\n    import warnings as warns\n    import plotly.graph_objects as gr_objs\n\n    def manipulators(count,duplicated=True,repeated=True):\n        \"\"\" Sets up permutation pairs \"\"\"\n        mnv=0\n        mxv=count\n        variations=[(i,j) for i in range(mnv,mxv) for j in range(mnv,mxv)]\n        if not duplicated: \n            for k in range(mnv,mxv): variations.remove((k,k))\n        if not repeated:\n            for vars in variations: \n                if (vars[1],vars[0]) in variations: variations.remove(vars) \n        return variations\n\n    def singlePlot(plotter,X,Y,**kwargs):\n        \"\"\" Definition and plotting with window \n        arrangements for a single plot \n\n        NB: this function is compatible with line, scatter, bar charts only\n\n        *******************\n        Variable list \n        ------------------\n        plotter: function name definition to plot with \n        X : x variable ( iterable like* - multi dimensional )\n        Y : y variable ( iterable like* - one dimensional   ) \n\n        *iterable like - must be a list\/tuple\/set, etc., likewise python primitives\n        or numpy iterable objects or pandas iterable objects \n        ------------\n        returns plot figure matplotlib.pyplot figure object \n        ____________________________\n        Additional settings\n           x_label: Abscissa axis label of the plot \n           y_label: Ordinate axis label of the plot\n           title : Plot title or label heading of plot \n           window_dim : dimensions of the viewable window \n\n           ___________________*******___________________\n\n        \"\"\"\n        warns.filterwarnings('ignore')\n        xlabel,ylabel,title,window_dim=None,None,None,None;\n        xlabel= kwargs['xlabel'] if 'xlabel' in kwargs else ' - X - '\n        ylabel= kwargs['ylabel'] if 'ylabel' in kwargs else ' - Y - '\n        title= kwargs['title'] if 'title' in kwargs else \" Plot \"\n        window_dim= kwargs['window_dim'] if 'window_dim' in kwargs else (15,7.5)\n        Fig=plts.figure(figsize=window_dim);ax=Fig.add_subplot(111);\n        plotter(X,Y)\n        plts.xlabel(xlabel);plts.ylabel(ylabel);plts.title(title);\n        return plts.show()\n\n    def multiPlots(data,method=sns.scatterplot,unique_pairs=True):\n        \"\"\" Plot multiple plots dpending on the requirements set\n        ********************\n        Variables \n        -------------------\n        data :  pandas dataframe object \n        method : (deafult) sns.scatterplot -> plotting function  \n        unique_pairs : (default) True -> plots of pairs, only those of which are unique\n\n        ********************\n\n        #need later revision in parameter usage and detailing for 'method' and 'grid' variable \"\"\"\n\n        # type error handling\n        if type(data) is not pd.DataFrame: raise ValueError(\"Data type must be a pandas.Dataframe object\")\n\n        #Body\n        cols=data.columns\n        permutes=manipulators(len(cols),duplicated=False,repeated=False) if unique_pairs else manipulators(len(cols))\n        plotFunc= method\n        for a_pair in permutes:\n            x_index= cols[a_pair[0]]\n            y_index= cols[a_pair[1]]\n            titled= x_index + ' vs ' + y_index\n            singlePlot(plotFunc,data[x_index],data[y_index],xlabel=x_index,ylabel=y_index,title=titled,window_dim=(12,10))        \n        print(' Plot Success ***** ')    \n\n    def fullDataPlots(data,method=sns.pairplot,title=None,window_dim=(15,10)):\n        \"\"\" Plot functionalities for full dataset plots\n         like pairplots and boxplots \n         ********************\n        Variables \n        -----------------------\n        data : pandas.DataFrame object\n        method : sns plot function object  \n        title : plot title\n        ------------------------\n        returns figure object  \n        \"\"\"\n        #Fig=plts.figure(figsize=window_dim);#ax=Fig.add_subplot(111);\n        figHandler=method(data=data);Fig=plts.gcf()\n        Fig.set_figwidth(15)\n        Fig.set_figheight(7.5)\n        #Fig.resize(window_dim[0],window_dim[1])\n        Fig.suptitle(title)\n        plts.subplots_adjust(0.064,0.06,0.983,0.938)\n        return plts.show()\n\n    def stockCandle(data,title=''):\n        \"\"\" Create a Candlestick chart for stock market like data \n        *********************\n        Variables\n        ---------------------\n        data : pandas.DataFrame object with specified definition type \n        title: title to the chart \n\n        Dataframe definition type must follow\n        1. Date must be indexed \n        2. Date index must be a datetime object\n        3. Must have 'Open','High','Low','Close' columns of data \n        _________________________\n        returns a plotted candlestick chart with sliders \n        \"\"\"\n        Fig = gr_objs.Figure(data=[gr_objs.Candlestick(x=data.index,\n                    open=data.Open, high=data.High,\n                    low=data.Low, close=data.Close)],\n                    layout=gr_objs.Layout(title=gr_objs.layout.Title(text=title)))\n        return Fig.show()\n\n    def residualsPlot(X,Y_t,Y_p,title,linefmt):\n        \"\"\" Function plots for generating StemPlot natured residual plot\n        *******************\n        Variables \n        ----------------------\n        X  : x axis values (array like)\n        Y_t : y variable considered true (array like)\n        y_p : y variable considered as predicted (array like)\n            residuals are calculated as for each x the residue is calculated as (Y_t-Y_p) \n        title : the title to the corresponding plot \n        linefmt : the line format respective to the chart plotted \n        -------------------------\n        returns matplotlib.container.StemContainer object \n        _____________________________\n        \"\"\"\n        residuals=Y_t-Y_p;\n        stems=plts.stem(X,residuals,linefmt=linefmt);plts.title(title);\n        return stems \n ```\nBesides these important library files we have the runtime.py for local checking the code operation in a sequential manner for IDE systems. But, it is to be noted that running the runtime.py file is depreciated since it has non updated dependencies which might hinder its proper operation or create errors during execution. <br>\n Code:(*runtime.py*)\n ```\n    #!usr\/bin\/python\n\n    \"\"\" RUNTIME Backend Executable for the operation \"\"\"\n    # Target job: predict Brent blend Crude oil prices \n\n    #imports \n    from DataAccess import *\n    from Intelligence import * \n    from Visuals import *\n    from StatAnalysis import *\n    import matplotlib.pyplot as plots  #@visuals\n    from statsmodels.tsa.seasonal import seasonal_decompose  #@pre-analysis\n    from sklearn.model_selection import train_test_split  #@intelligence\n    from sklearn import metrics #@post-analysis\n    from sklearn.linear_model import LinearRegression as LinReg  #@intellligence\n    from sklearn.ensemble import RandomForestRegressor as RnForReg  #@intelligence\n    from sklearn.svm import SVR #@intelligence\n    from sklearn.linear_model import BayesianRidge as BayesR   #@intelligence\n    from copy import deepcopy\n\n    # historic data access\n    # dHs=data_Prestored(\"https:\/\/raw.githubusercontent.com\/WolfDev8675\/RepoSJX7\/CaseStudy_backend\/Data_static\/BrentOilPrices.csv\")\n\n    dHs=data_FixedTimeLine(\"BZ=F\",start=\"2017-10-01\",end=\"2019-10-01\")  #for prediction\n    dhs2=data_Live(period='5mo')                                        #for result comparison   \n\n    #Stock data show\n    stockCandle(dHs,title=\" Brent Oil Price Fluctuations (BZ=F) \")\n\n    # Season @analysis\n    trendNseasonality(dHs[['Close']],model='additive',period=30)\n\n    # EDA \n    fullDataPlots(dHs[['Open','High','Low','Close']],title=\" Pairplot of %s\"%str(['Open','High','Low','Close'])[1:-1]) # pairplot \n    fullDataPlots(dHs[['Open','High','Low','Close']],method=sns.boxplot,title='Box and Whiskers') # boxplot \n    multiPlots(dHs[['Open','High','Low','Close']])  # individual plots paired to each other \n    ## per year open and close line curves \n    for year in set(dHs.index.year):\n        yearVariation(dHs[['Close']],year)\n        yearVariation(dHs[['Open']],year)\n\n    #Model Creation and study\n    models_name={'LR':\"Linear Regression\",\n                 'RF':\"Random Forest\",\n                 'SV':\"Support Vector\",\n                 'BR':\"Bayesian Ridge\"};\n    models_mods_norm={'LR':Forecaster(model=LinReg()),\n                      'RF':Forecaster(model=RnForReg()),\n                      'SV':Forecaster(model=SVR()),\n                      'BR':Forecaster(model=BayesR())};\n    models_mods_CVKF=deepcopy(models_mods_norm);\n\n    # Train and Test \n    for mod_key in models_mods_norm:\n        models_mods_norm[mod_key].pushData(data=dHs,predicts=['Open','High','Low'],infers=\"Close\")\n        models_mods_norm[mod_key].normal_split()\n        models_mods_norm[mod_key].train()\n        models_mods_norm[mod_key].plotMetrics(data=dhs2,title=\"Model: \"+models_name[mod_key])\n\n    for mod_key in models_mods_CVKF:\n        models_mods_CVKF[mod_key].pushData(data=dHs,predicts=['Open','High','Low'],infers=\"Close\")\n        models_mods_CVKF[mod_key].crossval_KF_split(n_splits=5,random_state=None,shuffle=False)\n        models_mods_CVKF[mod_key].train()\n        models_mods_CVKF[mod_key].plotMetrics(data=dhs2,title=\"Model: \"+models_name[mod_key]+\" Cross Validated \")\n\n    #@check\n    for mod_key in models_mods_CVKF:\n        print(models_name[mod_key])\n        if mod_key is not 'RF' and  mod_key is not 'SV': print(models_mods_CVKF[mod_key].model.coef_)\n        else: print(models_mods_CVKF[mod_key].model.get_params()) \n\n    #Report on models \n    for mod_key in models_mods_CVKF:\n        if mod_key is not 'LR':\n            print(models_name[mod_key])\n            models_mods_CVKF[mod_key].regression_report()\n        else:\n            print(models_name[mod_key])\n            models_mods_CVKF[mod_key].grad_decent()\n            models_mods_CVKF[mod_key].ensemble_grading()\n            models_mods_CVKF[mod_key].boost()\n            models_mods_CVKF[mod_key].regression_report()\n\n    # Report on train vs test vs predicted\n    #  predicted vs test plot report  @visuals \n    #  kind of residuals plots  \n    # Reasons and conclusions + post decisions and future advs \n ```\n \nThus is concluded the Library file code set for the project\n\n[\ud83e\udc09 Back to Contents](#@Contents)","f17845ce":"<a id='@26'><\/a>\n### 26. **Training models(Stock Method - as is, fresh out of the box)**\n---\nLet us train the models as they are without any preset defined by us. Any preset values if any applied is solely as is shipped by sklearn library.","ae5ac162":"---\n\n[\ud83e\udc09 Back to Contents](#@Contents)","60ec61d1":"<a id='@21'><\/a>\n### 21. **Variation of Data with time limit set**\n---\nLet us now check the yearly variation of the data we have among the market opening and market closing prices of the Crude Oil","7ddf02c3":"<a id='@30'><\/a>\n### 30. **Gradient Descent (What Why How)**\n---\nGradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost).\nGradient descent is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm\n\nLet's say that our hypothesis is for $n+1$ independent $x$ variables from $0 \\to n$\n\n<center>$\\psi(\\theta) = \\theta_{0}x_{0} + \\theta_{1}x_{1} + . . . + \\theta_{n}x_{n}$<\/center>\n\nand our true values be \n<center>$\\phi(\\varepsilon)= \\varepsilon_{0}x_{0} + \\varepsilon_{1}x_{1} + . . . + \\varepsilon_{n}x_{n}$<\/center>\n\nLet us establish that for \n<center>$ \\lim\\limits_{\\theta \\to \\varepsilon} \\psi(\\theta)=\\phi(\\varepsilon)$<\/center>\nsuch that $|\\theta - \\varepsilon|$ is a very small quantity say $\\delta\\theta$ that need to be minimized. \nOr, \n<center>$\\theta \\pm \\delta\\theta = \\varepsilon$<\/center>\nFor errors incurred differentiating the hypothesis from true value. Lets say \n<center>$\\begin{array}\\\\ \\xi(\\theta)=\\psi(\\theta) - \\phi(\\varepsilon) \\\\ \n        = \\theta_{0}x_{0} + \\theta_{1}x_{1} + . . . + \\theta_{n}x_{n} \\\\ \n         -(\\varepsilon_{0}x_{0} + \\varepsilon_{1}x_{1} + . . . + \\varepsilon_{n}x_{n}) \\\\\n        = (\\theta_{0} - \\varepsilon_{0})x_{0} +(\\theta_{1} - \\varepsilon_{1})x_{1} + . . . + (\\theta_{n} - \\varepsilon_{n})x_{n} \\\\\n        = \\delta\\theta_{0}x_{0} + \\delta\\theta_{1}x_{1} + . . . + \\delta\\theta_{n}x_{n} = \\delta\\psi(\\theta)\n         \\end{array}$<\/center>\n\nNow, for $m$ observations we will have the Error Equation as  \n<center>$\\xi(\\theta) =  \\delta\\theta_{0i}x_{0i} + \\delta\\theta_{1i}x_{1i} + . . . + \\delta\\theta_{ni}x_{ni}= \\delta\\psi_{i}(\\theta)$ &nbsp; &nbsp; &nbsp; &nbsp; $\\forall$ &nbsp; &nbsp;$ i \\in [1,2,. . ., m]$<\/center>\n\nHence, the Cost of Errors or the Cost Function is given as \n<center>$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} {\\{\\xi_{i}(\\theta)\\}}^2$<\/center>\nSo, for $ \\lim\\limits_{\\theta \\to \\varepsilon} \\psi(\\theta)=\\phi(\\varepsilon)$  pertaining to this situatuion we need to minimize $J(\\theta)$. \n\nTo Minimize $J(\\theta)$ we need \n<center>$ \\frac{\\delta J(\\theta)}{\\delta\\theta} = 0$  &nbsp; &nbsp;for $\\delta\\theta = c $<\/center>\n<center> and $ \\frac{\\delta^2 J(\\theta)}{\\delta\\theta^2} > 0$  &nbsp; &nbsp;for $\\delta\\theta = c $<\/center>\n\nWe, write this since $J(\\theta)$ is a function $\\xi(\\theta)=\\delta\\psi(\\theta)$ or a function of $\\delta\\theta$ holding $x$ values constant where $c$ is the value of $\\delta\\theta$ when $J(\\theta)$ reaches a minima or is perfectly minimized.\n\nNow, generalizing the equation more let's say for each $\\theta_j$ where $j \\in [0,1, . . .,n]$ we have $m$ observations. Then our equation comes to a structure as \n<center>$ \\theta_j=\\theta_j - \\alpha \\frac{\\delta J(\\theta)}{\\delta\\theta_j}$ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; $ \\forall $ &nbsp; &nbsp;$j = 0, 1, . . . , n $ coefficients <\/center> \n<center> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; and &nbsp; &nbsp; $ \\forall$ &nbsp; &nbsp;$ i =1, 2, . . . , m $ observations <\/center>\nprovided at a certain value of $\\delta\\theta_j=c_j$ we have \n<center> $ \\frac{\\delta J(\\theta)}{\\delta\\theta_j}=0 $<\/center>\nand we will have $\\theta_j \\approx \\varepsilon_j$ &nbsp; or $\\theta \\approx \\varepsilon$  or we are reducing the gradient $ \\frac{\\delta J(\\theta_j)}{\\delta\\theta_j}$\nTherefore, \n<center> $\\begin{array}\\\\  \\theta_j = \\theta_j - \\alpha \\frac{\\delta}{\\delta\\theta_j} \\frac{1}{2m} \\sum_{i=1}^{m} {\\{\\xi_{i}(\\theta)\\}}^2 \\\\\n        = \\theta_j - \\alpha \\frac{1}{2m}. 2 . \\sum_{i=1}^{m} {\\xi_{i}(\\theta)  \\frac{\\delta}{\\delta\\theta_j} \\xi_{i}(\\theta)} \\\\\n        \\end{array}$<\/center>\nNow, since \n <center>$ \\frac{\\delta\\xi_i(\\theta)}{\\delta\\theta_j} = \\sum_{i=1}^{m} {\\frac{\\delta\\theta_{ji}x_{ji}}{\\delta\\theta_{j}}}=x_j$<\/center> \nas for each \n <center>$ \\frac{\\delta}{\\delta\\theta_j} {\\theta_{0}x_{0} + \\theta_{1}x_{1} + . . . + \\theta_{n}x_{n}} $<\/center>\n <center>equals  $x_0$ for $j=0$ ; $x_1$ for $j=1$ and likewise for others since this is a Partial Differential hence the other components are constants <\/center>\n \nthen, our solution is at \n<center>$ \\theta_j = \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^{m} {\\xi_i(\\theta) . x_j}$ &nbsp; &nbsp;$ \\forall $ &nbsp; $j$ from $1,2, . . . m$ <\/center>\n\n[\ud83e\udc09 Back to Contents](#@Contents)","394a3222":"## Certification of Approval\nThis document is hereby approved as credible study of the science subject carried out and represented in a manner to satisfy to the warrants of its acceptance as a prerequisite to the degree for which it has been submitted. \n\nMoreover, it is understood that by this approval the undersigned does not necessarily endorse or approve any statements made, the opinion expressed or conclusion drawn therein but approved only for the sole purpose for which it has been indeed submitted. \n\nAll credits of guidence goes to Prof Ashok Gupta who has helped us with every steps of the Case Study. ","ec190ae1":"<a id='@31'><\/a>\n### 31. **Hyperparameters Boosting (What Why How)**\n---\n#### What is a Model Parameter?\nA model parameter is a configuration variable that is internal to the model and whose value can be estimated from data.\n\n* They are required by the model when making predictions.\n* They values define the skill of the model on your problem.\n* They are estimated or learned from data.\n* They are often not set manually by the practitioner.\n* They are often saved as part of the learned model.\n\nParameters are key to machine learning algorithms. They are the part of the model that is learned from historical training data.\nIn classical machine learning literature, we may think of the model as the hypothesis and the parameters as the tailoring of the hypothesis to a specific set of data.\nOften model parameters are estimated using an optimization algorithm, which is a type of efficient search through possible parameter values.\n\n* Statistics: In statistics, you may assume a distribution for a variable, such as a Gaussian distribution. Two parameters of the Gaussian distribution are the mean ($\\mu$) and the standard deviation ($\\sigma$). This holds in machine learning, where these parameters may be estimated from data and used as part of a predictive model.\n* Programming: In programming, you may pass a parameter to a function. In this case, a parameter is a function argument that could have one of a range of values. In machine learning, the specific model you are using is the function and requires parameters in order to make a prediction on new data.\nWhether a model has a fixed or variable number of parameters determines whether it may be referred to as \u201cparametric\u201d or \u201cnonparametric\u201c.\n\nSome examples of model parameters include:\n\n* The weights in an artificial neural network.\n* The support vectors in a support vector machine.\n* The coefficients in a linear regression or logistic regression.\n\n#### What is a Model Hyperparameter?\nA model hyperparameter is a configuration that is external to the model and whose value cannot be estimated from data.\n\n* They are often used in processes to help estimate model parameters.\n* They are often specified by the practitioner.\n* They can often be set using heuristics.\n* They are often tuned for a given predictive modeling problem.\n\nWe cannot know the best value for a model hyperparameter on a given problem. We may use rules of thumb, copy values used on other problems, or search for the best value by trial and error.\n\n**In ML\/DL, a model is defined or represented by the model parameters. However, the process of training a model involves choosing the optimal hyperparameters that the learning algorithm will use to learn the optimal parameters that correctly map the input features (independent variables) to the labels or targets (dependent variable) such that you achieve some form of intelligence.** \nHyperparameters are parameters whose values control the learning process and determine the values of model parameters that a learning algorithm ends up learning. The prefix \u2018hyper_\u2019 suggests that they are \u2018top-level\u2019 parameters that control the learning process and the model parameters that result from it.\n\nDuring machine learning, for designing a model, we choose and set hyperparameter values that our learning algorithm will use before the training of the model even begins. In this light, hyperparameters are said to be external to the model because the model cannot change its values during learning\/training.\n\nHyperparameters are used by the learning algorithm when it is learning but they are not part of the resulting model. At the end of the learning process, we have the trained model parameters which effectively is what we refer to as the model. The hyperparameters that were used during training are not part of this model. We cannot for instance know what hyperparameter values were used to train a model from the model itself, we only know the model parameters that were learned.\n\nBasically, anything in machine learning and deep learning that you decide their values or choose their configuration before training begins and whose values or configuration will remain the same when training ends is a hyperparameter.\n\n[\ud83e\udc09 Back to Contents](#@Contents)","7c36fdb6":"<a id='@20'><\/a>\n### 20. **Relationship between fields**\n---\nNow, we have to a limit have understood how our data is behaving. Let us explore more into the relation between the fields among themselves. \nIn this connection we will explore \n1. [Pairplot](https:\/\/seaborn.pydata.org\/generated\/seaborn.pairplot.html#seaborn.pairplot): Plot pairwise relationships in a dataset.\n2. [Box and Whiskers plot](https:\/\/seaborn.pydata.org\/generated\/seaborn.boxplot.html#seaborn.boxplot): A box plot (or box-and-whisker plot) shows the distribution of quantitative data in a way that facilitates comparisons between variables or across levels of a categorical variable. The box shows the quartiles of the dataset while the whiskers extend to show the rest of the distribution, except for points that are determined to be \u201coutliers\u201d using a method that is a function of the inter-quartile range.\n3. [Correlation Heatmap](https:\/\/seaborn.pydata.org\/generated\/seaborn.heatmap.html#seaborn.heatmap): Plot rectangular data as a color-encoded matrix.\n\nand various other paired field scatterplots among the datafields we are studying.","2eb6a842":"---\n\n[\ud83e\udc09 Back to Contents](#@Contents)","e438fbfc":"Observation:  similar structure regarding date. Besides we are missing a field from the previous dataset and having two separate additional fields","6292beb6":"<a id='@33'><\/a>\n### 33. **How much improvement we achieved ?**\n---\nSince we feel like our models are trained much let us explore their accurary limits","a5982bcd":"<a id='@11'><\/a>\n### 11. **Scope of the Case Study**\n---\nThis Case study is made thanks to the efforts and collaboration of BSE(Bombay Stock Exchange) and MAKAUT(Maulana Abul Kalam University of Technology) for a specialized course programme of Post Graduate Diploma in Data Science as a completion project covering the whole syllabi and also as a showcase of the advancement in technology along with the obvious growth and distribution of knowledge for common benefits. \n\nThis Case Study is specific to determination of Brent Crude Oil Prices as we have already understood by now. This work is also a proof of completion of all the five modules the one year programme went through inspite of being hit heavily by the SARS-CoVID19 pandemic and its eventual lockdown. The modules include\n1. Big Data Essentials (MapReduce, PIG, HIVE, SPARK)\n2. Data Analytics.\n3. Data Preparation and Cleaning \n4. Data Visualization\n5. Machine Learning \nEven if not explicitly covered\/included, this work vouches to prove itself being direcly influenced and shaped by each of these modules.\n\n#### Targets for the Case Study\nThe targets we are setting out with is \n1. Understanding the data in connection with stocks and the prices, their rise and fall.\n2. Factors influencing the price change\n3. Understanding Trend and Seasonality factors in the Data. How their presence impacts the price.\n4. Learning the impact a Machine Learning Algorithm. How each algorithms approach works out.\n5. Visualization of the required solutions in a way to make it presentable.\n6. Explore Deficits of Conventional methods over Deep Algorithms\n\n#### Roadblocks \nThe Preset Roadblocks we have is \n1. Getting the Data, Cleaning to our needs if any required\n2. Understanding and Exploring the nook and crannies of the data\n3. Training a model \n4. Error analysis of predictions made by the model, hence improving the model\n5. Producing the final results\n6. Properly explaining how or why we got a particular result \n7. Reach the reader or the reviewer at a personal levels viz., satisfy the curiosity.\n\n_From here onwards we wish the reviewer enjoy every step forward as we enjoyed creating it._\n\n[\ud83e\udc09 Back to Contents](#@Contents)","ce2af128":"<a id='@Contents'><\/a>\n## Table of Contents \n---\n1. [Introduction](#@1)\n2. [What is Crude Oil](#@2)\n3. [Why is Crude Oil Important](#@3)\n4. [How is Crude Oil Classified](#@4)\n5. [Types of Crude Oil](#@5)\n6. [Blends of Crude Oil](#@6)\n7. [Brent Crude Oil](#@7)\n8. [What Determines Crude Oil Prices](#@8)\n9. [Effect of Crude Oil on Economy](#@9)\n10. [Why is Crude Oil Stocks Important ?](#@10)\n11. [Scope of the Case Study](#@11)\n12. [Library Structure (Files and their Importance)](#@12)\n13. [Repository Cloning and Python Distributions](#@13)\n14. [Essential Library Imports and their Handling](#@14)\n15. [Data Access](#@15)\n16. [Assessment of the Data](#@16)\n17. [Variable set definition (Why we are defining as we are defining)](#@17)\n18. [What is a Time Series](#@18)\n19. [Seasonality and Trends](#@19)\n20. [Relationship between fields](#@20)\n21. [Variation of Data with time limit set](#@21)\n22. [Standardization & Normalization](#@22)\n23. [Machine Learning and How it helps here (types and uses)](#@23)\n24. [Why Regression? How the Regression Works ?](#@24)\n25. [Definition of Models (A brief idea of their _modus operandi_)](#@25)\n26. [Training models(Stock Method - as is, fresh out of the box)](#@26)\n27. [What is Cross-Validation ? Why we use it ?](#@27)\n28. [How Cross-Validation improved our models ?](#@28)\n29. [Specialized improvements for the models](#@29)\n30. [Gradient Descent (What Why How)](#@30)\n31. [Hyperparameters Boosting (What Why How)](#@31)\n32. [Improving the Models (Gradient Descent & Hyperparameter Boosting Methods)](#@32)\n33. [How much improvement we achieved ?](#@33)\n34. [How much is too much ? (Accuracy and Metrics Benchmarking with Residuals)](#@34)\n35. [Best of Models and Implementation](#@35)\n36. [Further Developments and Usage](#@36)\n37. [Distribution and Licence](#@37)\n38. [What we achieved ? (from the **Case Study**)](#@38)\n39. [Closing Notes ](#@39)\n40. [Bibliography](#@Bibliography)","afdb8bbe":"<a id='@34'><\/a>\n### 34. **How much is too much ? (Accuracy and Metrics Benchmarking with Residuals)**\n---\n#### Model Skill Is Relative\nEvery predictive modeling problem is unique.\nThis includes the specific data we have, the tools we\u2019re using, and the skill we will achieve.\nOur predictive modeling problem has not been solved before. Therefore, we cannot know what a good model looks like or what skill it might have.\nWe might have ideas of what a skillful model looks like based on knowledge of the domain, but we don\u2019t know whether those skill scores are achievable.\nThe best that we can do is to compare the performance of machine learning models on our specific data to other models also trained on the same data.\nMachine learning model performance is relative and ideas of what score a good model can achieve only make sense and can only be interpreted in the context of the skill scores of other models also trained on the same data.\n\n#### Baseline Model Skill\nBecause machine learning model performance is relative, it is critical to develop a robust baseline.\nA baseline is a simple and well understood procedure for making predictions on your predictive modeling problem. The skill of this model provides the bedrock for the lowest acceptable performance of a machine learning model on your specific dataset.\nThe results for the baseline model provide the point from which the skill of all other models trained on your data can be evaluated.\nThree examples of baseline models include:\n* Predict the mean outcome value for a regression problem.\n* Predict the mode outcome value for a classification problem.\n* Predict the input as the output (called persistence) for a univariate time series forecasting problem.\n\nThe baseline performance on our problem can then be used as the yardstick by which all other models can be compared and evaluated.\nIf a model achieves a performance below the baseline, something is wrong (e.g. there\u2019s a bug) or the model is not appropriate for your problem.\n#### What Is the Best Score?\nIf we are working on a classification problem, the best score is 100% accuracy.\nIf we are working on a regression problem, the best score is 0.0 error.\nThese scores are an impossible to achieve upper\/lower bound. All predictive modeling problems have prediction error. Expecting it is cleverer. The error comes from a range of sources such as:\n* Incompleteness of data sample.\n* Noise in the data.\n* Stochastic nature of the modeling algorithm.\n\nWe cannot achieve the best score, but it is good to know what the best possible performance is for our chosen measure. We know that true model performance will fall within a range between the baseline and the best possible score.\nInstead, we must search the space of possible models on your dataset and discover what good and bad scores look like.\n\n#### Discover Limits of Model Skill\nOnce we have the baseline, we can explore the extent of model performance on our predictive modeling problem.\nIn fact, this is the hard work and the objective of the project: to find a model that we can demonstrate, works reliably well in making predictions on our specific dataset.\nThere are many strategies to this problem; two that we might wish to consider are:\n* Start High. Select a machine learning method that is sophisticated and known to perform well on a range of predictive model problems, such as random forest or gradient boosting. We need to evaluate the model on our problem and use the result as an approximate top-end benchmark, then find the simplest model that achieves similar performance.\n* Exhaustive Search. Evaluate all of the machine learning methods that one can think of on the problem and select the method that achieves the best performance relative to the baseline.\n\nThe \u201cStart High\u201d approach is fast and can help us define the bounds of model skill to expect on the problem and find a simple (e.g. Occam\u2019s Razor) model that can achieve similar results. It can also help you find out whether the problem is solvable\/predictable fast, which is important because not all problems are predictable.\n\nThe \u201cExhaustive Search\u201d is slow and is really intended for long-running projects where model skill is more important than almost any other concern. I often perform variations of this approach testing suites of similar methods in batches and call it the spot-checking approach.\n\nBoth methods will give us a population of model performance scores that you can compare to the baseline.\n\n[\ud83e\udc09 Back to Contents](#@Contents)","657abd9e":"Types of Data in the fields","ce19bf59":"<a id='@19'><\/a>\n### 19. **Seasonality and Trends**\n---\n####  _What Is Seasonality?_\nSeasonality is a characteristic of a time series in which the data experiences regular and predictable changes that recur every calendar year. Any predictable fluctuation or pattern that recurs or repeats over a one-year period is said to be seasonal. Seasonal effects are different from cyclical effects, as seasonal cycles are observed within one calendar year, while cyclical effects, such as boosted sales due to low unemployment rates, can span time periods shorter or longer than one calendar year. \n\nSeasonality refers to periodic fluctuations in certain business areas and cycles that occur regularly based on a particular season. A season may refer to a calendar season such as summer or winter, or it may refer to a commercial season such as the holiday season. Companies that understand the seasonality of their businesses can predict and time inventories, staffing, and other decisions to coincide with the expected seasonality of the associated activities, thereby reducing costs and increasing revenue.\n\nIt is important to consider the effects of seasonality when analyzing stocks from a fundamental point of view because it can have a big impact on an investor's profits and portfolio. A business that experiences higher sales during certain seasons may appear to make significant gains during peak seasons and significant losses during off-peak seasons. If this is not taken into consideration, an investor may choose to buy or sell securities based on the activity at hand without accounting for the seasonal change that subsequently occurs as part of the company\u2019s seasonal business cycle.\n\nSeasonality is also important to consider when tracking certain economic data. Economic growth can be affected by different seasonal factors including the weather and the holidays. Economists can get a better picture of how an economy is moving when they adjust their analyses based on these factors. For example, roughly two-thirds of U.S. gross domestic product (GDP) is made up of consumer spending\u2014which is a seasonal measure. The more consumers spend, the more the economy grows. Conversely, when they cut back on their purse strings, the economy will shrink. If this seasonality was not taken into account, economists would not have a clear picture of how the economy is truly moving. \n\n#### _What is Trend?_\nTrend is a pattern in data that shows the movement of a series to relatively higher or lower values over a long period of time. In other words, a trend is observed when there is an increasing or decreasing slope in the time series. Trend usually happens for some time and then disappears, it does not repeat. For example, some new song comes, it goes trending for a while, and then disappears. There is fairly any chance that it would be trending again.\n\nNow, since we have a time series data in our hand. Lets explore these possibilities of ***Seasons and Trends***.","9c3e7da9":"---\n\n[\ud83e\udc09 Back to Contents](#@Contents)","eedddc7d":"<a id='@18'><\/a>\n### 18. **What is a Time Series**\n---\nA time series is a sequence of data points that occur in successive order over some period of time.  A time series can be taken on any variable that changes over time. In investing, it is common to use a time series to track the price of a security over time. This can be tracked over the short term, such as the price of a security on the hour over the course of a business day, or the long term, such as the price of a security at close on the last day of every month over the course of five years.\n\nTime series analysis can be useful to see how a given asset, security, or economic variable changes over time. It can also be used to examine how the changes associated with the chosen data point compare to shifts in other variables over the same time period. Time series is also used in several non-financial contexts, such as measuring the change in population over time. \n\n#### Time Series Analysis\nSuppose we want to analyze a time series of daily closing stock prices for a given stock over a period of one year. We would obtain a list of all the closing prices for the stock from each day for the past year and list them in chronological order. This would be a one-year daily closing price time series for the stock.\n\nDelving a bit deeper, we might analyze time series data with technical analysis tools to know whether the stock's time series shows any seasonality. This will help to determine if the stock goes through peaks and troughs at regular times each year. Analysis in this area would require taking the observed prices and correlating them to a chosen season. This can include traditional calendar seasons, such as summer and winter, or retail seasons, such as holiday seasons.\n\nAlternatively, we can record a stock's share price changes as it relates to an economic variable, such as the unemployment rate. By correlating the data points with information relating to the selected economic variable, we can observe patterns in situations exhibiting dependency between the data points and the chosen variable. \n#### Time Series Forecasting\nTime series forecasting uses information regarding historical values and associated patterns to predict future activity. Most often, this relates to trend analysis, cyclical fluctuation analysis, and issues of seasonality. As with all forecasting methods, success is not guaranteed.\n\nThe Box-Jenkins Model, for instance, is a technique designed to forecast data ranges based on inputs from a specified time series. It forecasts data using three principles, autoregression, differencing, and moving averages. These three principles are known as p, d, and q respectively. Each principle is used in the Box-Jenkins analysis and together they are collectively shown as an autoregressive integrated moving average, or ARIMA (p, d, q). ARIMA can be used, for instance, to forecast stock prices or earnings growth.\n\nAnother method, known as rescaled range analysis, can be used to detect and evaluate the amount of persistence, randomness, or mean reversion in time series data. The rescaled range can be used to extrapolate a future value or average for the data to see if a trend is stable or likely to reverse. \n#### Box Jenkins Model\nThe Box-Jenkins Model is a mathematical model designed to forecast data ranges based on inputs from a specified time series. The Box-Jenkins Model can analyze several different types of time series data for forecasting purposes. Its methodology uses differences between data points to determine outcomes. The methodology allows the model to identify trends using autoregresssion, moving averages, and seasonal differencing to generate forecasts.Box-Jenkins Models are used for forecasting a variety of anticipated data points or data ranges, including business data and future security prices. \n\nThe Box-Jenkins Model was created by two mathematicians: George Box and Gwilym Jenkins. The two mathematicians discussed the concepts that comprise this model in a 1970 publication called \"Time Series Analysis: Forecasting and Control.\" Estimations of the parameters of the Box-Jenkins Model can be very complicated. Therefore, similar to other time-series regression models, the best results will typically be achieved through the use of programmable software. The Box-Jenkins Model is also generally best suited for short-term forecasting of 18 months or less.\n##### Box-Jenkins Methodology\nThe Box-Jenkins Model may be one of several, time series analysis models a forecaster will encounter when using programmed forecasting software. In many cases, the software will be programmed to automatically use the best fitting forecasting methodology based on the time series data to be forecasted. Box-Jenkins is reported to be a top choice for data sets that are mostly stable and have low volatility.\n\nThe Box-Jenkins Model forecasts data using three principles: autoregression, differencing, and moving average. These three principles are known as p, d, and q, respectively. Each principle is used in the Box-Jenkins analysis; together, they are collectively shown as ARIMA (p, d, q).\n\nThe autoregression (p) process tests the data for its level of stationarity. If the data being used is stationary, it can simplify the forecasting process. If the data being used is non-stationary it will need to be differenced (d). The data is also tested for its moving average fit (which is done in part q of the analysis process). Overall, initial analysis of the data prepares it for forecasting by determining the parameters (p, d, and q), which are then applied to develop a forecast. \n#### Autoregressive Integrated Moving Average (ARIMA)\nAn autoregressive integrated moving average, or ARIMA, is a statistical analysis model that uses time series data to either better understand the data set or to predict future trends. A statistical model is autoregressive if it predicts future values based on past values. For example, an ARIMA model might seek to predict a stock's future prices based on its past performance or forecast a company's earnings based on past periods. \n\n An autoregressive integrated moving average model is a form of regression analysis that gauges the strength of one dependent variable relative to other changing variables. The model's goal is to predict future securities or financial market moves by examining the differences between values in the series instead of through actual values.\n\nAn ARIMA model can be understood by outlining each of its components as follows:\n\n* Autoregression (AR): refers to a model that shows a changing variable that regresses on its own lagged, or prior, values.\n* Integrated (I): represents the differencing of raw observations to allow for the time series to become stationary (i.e., data values are replaced by the difference between the data values and the previous values).\n* Moving average (MA):  incorporates the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n\n##### ARIMA Parameters\nEach component in ARIMA functions as a parameter with a standard notation. For ARIMA models, a standard notation would be ARIMA with p, d, and q, where integer values substitute for the parameters to indicate the type of ARIMA model used. The parameters can be defined as:\n\n* p: the number of lag observations in the model; also known as the lag order.\n* d: the number of times that the raw observations are differenced; also known as the degree of differencing.\n* q: the size of the moving average window; also known as the order of the moving average.\n\nIn a linear regression model, for example, the number and type of terms are included. A 0 value, which can be used as a parameter, would mean that particular component should not be used in the model. This way, the ARIMA model can be constructed to perform the function of an ARMA model, or even simple AR, I, or MA models.\n\n##### Autoregressive Integrated Moving Average (ARIMA) and Stationarity\nIn an autoregressive integrated moving average model, the data are differenced in order to make it stationary. A model that shows stationarity is one that shows there is constancy to the data over time. Most economic and market data show trends, so the purpose of differencing is to remove any trends or seasonal structures. \n\n[Seasonality](#@19), or when data show regular and predictable patterns that repeat over a calendar year, could negatively affect the regression model. If a trend appears and stationarity is not evident, many of the computations throughout the process cannot be made with great efficacy. \n\n[\ud83e\udc09 Back to Contents](#@Contents)","52b37101":"and the brief description ","e24ce0a7":"Observation: the volume field is noticed again to have a large scale deviation compared to others","68eb35f9":"   Head of the Dataset \n\n      Count set for first 10 records","6ba8281f":"Here we have two sets of data \n1. dataset for model creation \n2. dataset for checking final prediction \nfrom the first dataset we will be creating the models hence we took a larger dataset of around two years from October,2017 to end of September,2019. \nIn the second dataset, we just need to see how the models fared, hence, we took a 5 months worth of data from the current timeline(_since this project is done in year 2021_) calculated from the current date.\n\n[\ud83e\udc09 Back to Contents](#@Contents)","0916069e":"<a id='@29'><\/a>\n### 29. **Specialized improvements for the models**\n---\nExtensive hours of studies on each algorithm's dynamics reveal that although the Cross Validation improved our models by some quantity and quality, further improvement of each model's predictive qualities could be made using specialized methods as \n1. Linear Regression and Bayesian Ridge Regression could be improved using Gradient Desent method since they are more oriented to their equation of state\n2. Support Vector Machines are as their name suggests are Vector Machines and are hyperparameter oriented hence need to be boosted by their Hyperparameters\n3. Random Forests are Ensemble models and are dependent on their hyperparameters thus need to be boosted by their hyperparameters.\n\n_Hence, proceeding into making those improvements_\n\n[\ud83e\udc09 Back to Contents](#@Contents)","4ef2a5db":"All required imports are already called from inside the various defined libraries we imported. So calling them explicitely is not required. <br>\n**To be Noted**: <br>\nWe also initiated the function ***deepcopy*** from the python native ***copy*** library. This is done to override a specific trait of memory management in Native Python language. In Python when a variable is created with a value the variable name stores the address location of the value instead of the value itself. \nLet's say we have a variable $a$ with value $= 5$ <br>\n$a=5$ <br>\nWe also have another valiable say $b$, where, $b=a$ <br>\nNow, if we say suppose change the value of $a$ to $9$ as <br>\n$a=9$ <br>\nthen $b$ will also store $9$ inspite of $b$ being untouched <br>\n**deepcopy** function just copies without the linkage by address references and both data variables inspite of having same data will be treated as individual variables \n\n[\ud83e\udc09 Back to Contents](#@Contents)","b60c6244":"<a id='@23'><\/a>\n### 23. **Machine Learning and How it helps here (types and uses)**\n---\nFrom the begining we have been telling about models and machine learning algorithms.\nThe use and development of computer systems that are able to learn and adapt without following explicit instructions, by using algorithms and statistical models to analyse and draw inferences from patterns in data.\n\nMachine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it to learn for themselves.\n\nThe process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide. The primary aim is to allow the computers learn automatically without human intervention or assistance and adjust actions accordingly.\n\nBut, using the classic algorithms of machine learning, text is considered as a sequence of keywords; instead, an approach based on semantic analysis mimics the human ability to understand the meaning of a text.\n\n\n#### Some Machine Learning Methods\nMachine learning algorithms are often categorized as supervised or unsupervised.\n\n* **Supervised machine learning algorithms** can apply what has been learned in the past to new data using labeled examples to predict future events. Starting from the analysis of a known training dataset, the learning algorithm produces an inferred function to make predictions about the output values. The system is able to provide targets for any new input after sufficient training. The learning algorithm can also compare its output with the correct, intended output and find errors in order to modify the model accordingly.\n* In contrast, **unsupervised machine learning algorithms** are used when the information used to train is neither classified nor labeled. Unsupervised learning studies how systems can infer a function to describe a hidden structure from unlabeled data. The system doesn\u2019t figure out the right output, but it explores the data and can draw inferences from datasets to describe hidden structures from unlabeled data.\n* **Semi-supervised machine learning algorithms** fall somewhere in between supervised and unsupervised learning, since they use both labeled and unlabeled data for training \u2013 typically a small amount of labeled data and a large amount of unlabeled data. The systems that use this method are able to considerably improve learning accuracy. Usually, semi-supervised learning is chosen when the acquired labeled data requires skilled and relevant resources in order to train it \/ learn from it. Otherwise, acquiring unlabeled data generally doesn\u2019t require additional resources.\n* **Reinforcement machine learning algorithms** is a learning method that interacts with its environment by producing actions and discovers errors or rewards. Trial and error search and delayed reward are the most relevant characteristics of reinforcement learning. This method allows machines and software agents to automatically determine the ideal behavior within a specific context in order to maximize its performance. Simple reward feedback is required for the agent to learn which action is best; this is known as the reinforcement signal.\nMachine learning enables analysis of massive quantities of data. While it generally delivers faster, more accurate results in order to identify profitable opportunities or dangerous risks, it may also require additional time and resources to train it properly. Combining machine learning with AI and cognitive technologies can make it even more effective in processing large volumes of information.\n\n<br>\n#### Why we are using Machine Learning here\nIt is a well-known fact that Machine Learning is a powerful technique in imagining, speech and natural language processing for a huge explicated dataset available. On the other hand,\n* Problems based on time series do not have usually interpreted datasets, even as data is collected from various sources so exhibit substantial variations in terms of features, properties, attributes, temporal scales, and dimensionality.\n* Time series analysis requires such sorting algorithms that can allow it to learn time-dependent patterns across multiples models different from images and speech.\n* Various machine learning tools such as classification, clustering, forecasting, and anomaly detection depend upon real-world business applications. \nAmong various defined applications, discussing here Time series forecasting, it is an important area of machine learning because there are multiple problems involving time components for making predictions. There are multiple models and methods used as approaches for time series forecasting, let\u2019s understand them more clearly;\n\nHere in connection to all the work mention must be made of Sklearn library which is being exclusively used for this case study.\n\nScikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\n\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by David Cournapeau. Its name stems from the notion that it is a \"SciKit\" (SciPy Toolkit), a separately-developed and distributed third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010 Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort and Vincent Michel, all from the French Institute for Research in Computer Science and Automation in Rocquencourt, France, took leadership of the project and made the first public release on February the 1st 2010. Of the various scikits, scikit-learn as well as scikit-image were described as \"well-maintained and popular\" in November 2012. Scikit-learn is one of the most popular machine learning libraries on GitHub.\n\n**Now lets import the machine learning models from sklearn for our work**","ad67d87d":"---\n\n[\ud83e\udc09 Back to Contents](#@Contents)","61567cf6":"   Head of the Dataset \n\n      Count set for first 10 records","332ff871":"With this visualization in hand let us check the parameters of the datasets we obtained. <br>\nIt is to be noted that for each of the datasets we have at our disposal we will be checking for \n1. Head of the dataset - to know the names of the columns or fields\n2. The types of data viz., the datatypes of the various columns or fields - which will help us in understanding how to handle one column from the other\n3. A brief description - statistical evaluation between fields of the dataset","3424d937":"---\n\n[\ud83e\udc09 Back to Contents](#@Contents)","79c9e9c4":"<a id='@22'><\/a>\n### 22. **Standardization & Normalization**\n---\n#### _Standardization_\nStandardization is the process of creating protocols to guide the creation of a good or service based on the consensus of all the relevant parties in the industry. The standards ensure that goods or services produced in a specific industry come with consistent quality and are equivalent to other comparable products or services in the same industry. Standardization also helps in ensuring the safety, interoperability, and compatibility of goods produced. Some of the parties involved in the standardization processes include users, interest groups, governments, corporations, and standards organizations.\n\nData standardization is the process of converting data to a common format to enable users to process and analyze it. Most organizations utilize data from a number of sources; this can include data warehouses, lakes, cloud storage, and databases. However, data from disparate sources can be problematic if it isn\u2019t uniform, leading to difficulties down the line (e.g., when we use that data to produce dashboards and visualizations, etc.).\n\nData standardization is crucial for many reasons. First of all, it helps us establish clear, consistently defined elements and attributes, providing a comprehensive catalog of our data. Whatever insights you\u2019re trying to get or problems we\u2019re attempting to solve, properly understanding your data is a crucial starting point. Getting there involves converting that data into a uniform format, with logical and consistent definitions. These definitions will form your metadata \u2014 the labels that identify the what, how, why, who, when, and where of our data. That\u2019s the basis of our data standardization process.\n\nFrom an accuracy perspective, standardizing the way we label data will improve access to the most relevant and current information. This will help make your analytics and reporting easier. Security-wise, mindful cataloging forms the basis of a powerful authentication and authorization approach, which will apply security restrictions to data items and data users as appropriate.\n#### _Normalization_\nNormalization is the process of reorganizing data in a database so that it meets two basic requirements:\n\n1. There is no redundancy of data, all data is stored in only one place.\n2. Data dependencies are logical,all related data items are stored together. \n\nNormalization is important for many reasons, but chiefly because it allows databases to take up as little disk space as possible, resulting in increased performance.\n\nNormalization is also known as data normalization.\n#### _Data standardization vs. data normalization_\nNow that we\u2019re familiar with the basics of data standardization, let\u2019s look at it in the context of feature scaling, commonly used in machine learning (ML) algorithms. For this purpose, data is generally processed in one of two ways: data standardization or data normalization, sometimes referred to as min-max scaling.\n\nData normalization refers to shifting the values of your data so they fall between 0 and 1. Data standardization, in this context, is used as a scaling technique to establish the mean and the standard deviation at 0 and 1, respectively.","443a8327":"<a id='@9'><\/a>\n### 9. **Effect of Crude Oil on Economy**\n---\n#### Microeconomic Levels\nAs a consumer, we already understand the microeconomic implications of higher oil prices. When observing higher oil prices, most of us are likely to think about the price of gasoline as well, since gasoline purchases are necessary for most households. When gasoline prices increase, a larger share of households\u2019 budgets is likely to be spent on it, which leaves less to spend on other goods and services. The same goes for businesses whose goods must be shipped from place to place or that use fuel as a major input (such as the airline industry). Higher oil prices tend to make production more expensive for businesses, just as they make it more expensive for households to do the things they normally do.\nIt turns out that oil and gasoline prices are indeed very closely related. Increases in oil prices are accompanied by increases in gasoline prices. Moreover, the monthly changes in oil prices and gasoline prices also are very highly and positively correlated.\nSo, when oil prices spike, we can always expect gasoline prices to spike as well, and that affects the costs faced by the vast majority of households and businesses.\n#### Macroeconomic Levels\nOil price increases are generally thought to increase inflation and reduce economic growth. In terms of inflation, oil prices directly affect the prices of goods made with petroleum products.  As mentioned above, oil prices indirectly affect costs such as transportation, manufacturing, and heating.  The increase in these costs can in turn affect the prices of a variety of goods and services, as producers may pass production costs on to consumers. The extent to which oil price increases lead to consumption price increases depends on how important oil is for the production of a given type of good or service.      \nOil price increases can also stifle the growth of the economy through their effect on the supply and demand for goods other than oil.  Increases in oil prices can depress the supply of other goods because they increase the costs of producing them. In economics terminology, high oil prices can shift up the supply curve for the goods and services for which oil is an input. \n\nHigh oil prices also can reduce demand for other goods because they reduce wealth, as well as induce uncertainty about the future ([Sill 2007](http:\/\/www.philadelphiafed.org\/files\/br\/2007\/br_q1-2007-3_oil-shocks.pdf)). One way to analyze the effects of higher oil prices is to think about the higher prices as a tax on consumers ([Fernald and Trehan 2005](https:\/\/www.frbsf.org\/economic-research\/publications\/economic-letter\/2005\/november\/why-the-jump-in-oil-prices-has-not-led-to-a-recession\/)). The simplest example occurs in the case of imported oil. The extra payment that U.S. consumers make to foreign oil producers can now no longer be spent on other kinds of consumption goods.   \nDespite these effects on supply and demand, the correlation between oil price increases and economic downturns in the U.S. is not perfect.  Not every sizeable oil price increase has been followed by a recession. However, five of the last seven U.S. recessions were preceded by considerable increases in oil prices ([Sill 2007](http:\/\/www.philadelphiafed.org\/files\/br\/2007\/br_q1-2007-3_oil-shocks.pdf)).\n#### Is the relationship between oil prices and the economy always the same?\nThe two aforementioned large oil shocks of the 1970s were characterized by low growth, high unemployment, and high inflation (also often referred to as periods of stagflation).  It is no wonder that changes in oil prices have been viewed as an important source of economic fluctuations. \nHowever, in the past decade research has challenged this conventional wisdom about the relationship between oil prices and the economy. As [Blanchard and Gali (2007)](http:\/\/www.nber.org\/papers\/w13368) note, the late 1990s and early 2000s were periods of large oil price fluctuations, which were comparable in magnitude to the oil shocks of the 1970s. However, these later oil shocks did not cause considerable fluctuations in inflation, real GDP growth, or the unemployment rate.\n\nA caveat is in order, however, because simply observing the movements of inflation and growth around oil shocks may be misleading. Keep in mind that oil shocks have often coincided with other economic shocks.  In the 1970s, there were large increases in commodity prices, which intensified the effects on inflation and growth. On the other hand, the early 2000s were a period of high productivity growth, which offset the effect of oil prices on inflation and growth. Therefore, to determine whether the relationship between oil prices and other variables has truly changed over time, one must go beyond casual observations and appeal to econometric analysis (which allows researchers to control for other developments in the economy when studying the link between oil prices and key macroeconomic variables).\n\nFormal studies find evidence that the link between oil prices and the macroeconomy has indeed deteriorated over time. For example, Hooker (2002) suggests that the structural break in the relationship between inflation and oil prices occurred at the end of 1980s.  Blanchard and Gali (2007) look at the responses of prices, wage inflation, output, and employment to oil shocks. They too find that the responses of all these variables to oil shocks have become muted since the mid-1980s.\n\n[\ud83e\udc09 Back to Contents](#@Contents)","40bb148a":"<a id='@7'><\/a>\n### 7. **Brent Crude Oil**\n---\n__Brent Crude Oil__ or sometimes __Brent Crude__ may refer to any or all of the components of the Brent Complex, a physically and financially traded oil market based around the North Sea of Northwest Europe. Colloquially, Brent Crude usually refers to the price of the ICE Brent Crude Oil futures contract or the contract itself. \n\n***BRENT CRUDE*** is the benchmark against which the majority of the 100m barrels of crude oil traded every day are priced. At the start of October the price of Brent crude rose above ***&dollar; 85*** a barrel, its highest level in four years. But the black stuff that makes up the Brent benchmark comprises a tiny fraction of the world\u2019s extracted oil.\n#### Why is it used to determine the value of 60% of oil on international markets?\nThe international oil trade is relatively new. It was not until 1861 that the first deal between two countries was recorded, when the Elizabeth Watts, a cargo ship, took a consignment of oil from Pennsylvania to London. Before the second world war, oil was mainly traded regionally. After the war the market became increasingly global as the number of producers expanded. Its price was largely set by oil companies, and later by the 15 countries that make up OPEC, the oil-producers\u2019 cartel. It is only since the late 1980s that prices have been determined by international markets. As crude oil differs in quality and availability depending on where it comes from, producers and traders need a reliable benchmark against which to judge the correct price. \n#### Importance of BRENT CRUDE\nThe most widely used benchmark is Brent crude, oil extracted in the North Sea. Brent makes such a good benchmark because it is easy to refine into products such as petrol, so demand is consistent. Because it comes from the sea, output can be raised as required and extra oil tankers can be chartered to ship the stuff. Supply of the more easily refined but landlocked West Texas Intermediate (WTI) crude, a benchmark commonly used in America, is constrained by pipeline capacity. When the Brent benchmark was first adopted for widespread use in 1985, the oil came from Shell\u2019s Brent oil field. But as production diminished, crude oil from other fields in the North Sea was added into the blend that makes up the benchmark. Today, the price of a barrel of Brent crude is taken from the most competitive of five different types of crude, only one of which comes from the Brent field. This has maintained the security and volume of supply essential for a reliable benchmark.\n\nAlthough Brent crude is well-established in the world\u2019s markets, its continued importance is not assured. North Sea oil reserves are depleting. New sources of crude from outside the North Sea could help guard against supply-side price swings, but also affect Brent\u2019s consistent quality. WTI, the second-most-common benchmark, has been gaining influence after the American government lifted a ban on oil exports three years ago. Earlier this year, China launched Shanghai crude futures, an attempt to create an Asian benchmark to rival the two Western incumbents. Many of the trades pegged to this benchmark use the yuan, which could make China\u2019s currency more important in the global economy. Shanghai crude has had some early success, but it needs to attract more foreign interest to become a global alternative. Brent crude is still the most prevalent gauge of the price of oil, though competition could one day have it over a barrel. \n\n[\ud83e\udc09 Back to Contents](#@Contents)","56cb12bb":"<a id='@2'><\/a>\n### 2. **What is Crude Oil**\n---\nCrude oil is a naturally occurring mixture of hydrocarbons found underground. It can appear in the form of a highly viscous liquid to a thick tar-like substance. The color of crude oil can also range from light yellow to dark brown or black. It is one of the most widely used fuel sources around the world, and oil, as well as oil derivatives, are globally traded in oil markets. Crude oil may also be referred to as just crude or oil. This fuel source must be refined before it can be used and, once refined, it falls under the category of petroleum products.\n\n[\ud83e\udc09 Back to Contents](#@Contents)","4eb4f9be":"Observation: our dataset has the date field set in the index, and informations in the volume field far surpasses the data in the other fields in their values.","49871d1e":"<a id='@10'><\/a>\n### 10. **Why is Crude Oil Stocks Important**\n---\nOil: lifeblood of the industrialised nations Oil has become the world's most important source of energy since the mid-1950s. Its products underpin modern society, mainly supplying energy to power industry, heat homes and provide fuel for vehicles and aeroplanes to carry goods and people all over the world.\n\nIn fact, oil meets 97 per cent of a nation's transport sector demand. In addition, it also benefits our lives in being vital to the production of many everyday essentials. Oil\u2019s refined products are used to manufacture almost all chemical products, such as plastics, fertilisers, detergents, paints and even medicines, plus a whole host of other products that you might not expect.\n\nFinal energy consumption by fuel, UK (1970 to 2014) <br>\n![Energy Consumption UK](https:\/\/www.ukogplc.com\/images\/assets\/whyoilisimportant%20(1).jpg) \n\n<br>\nDistribution by usage UK <br>\n![Distribution by usage](https:\/\/www.ukogplc.com\/images\/assets\/whyoilisimportant%20(2).jpg) \n\n<br>\n\n#### Current Account Deficit (CAD) and Currency depreciation:\nEvery USD10\/bbl increase in oil price leads to a 0.55% or 55 bps increase in the current account deficit. Crude oil is one of the most important commodities in recent time. For example, India is one of the largest importers of oil in the world. It imports more than three-fourths of its oil needs. Therefore, a fall in the price of crude oil will have a positive impact on India\u2019s current account deficit situation. Lower CAD will mean reduced stress on foreign currency outflows. This, in turn, may lead to rupee appreciation. If the value of rupee appreciates, the imports become cheaper. This will affect the companies who depend on import crude oil and other raw materials, for their business. The price of stocks of these companies will thus experience a rise.\n#### A rise in the cost of production:\nCompanies like tyre, lubricants, logistics, footwear, refinery, and airlines hugely depend on crude oil prices. Further, products like paints too will benefit from reduced crude oil prices. This is because; most paints used today are oil-based. A fall in crude-oil prices affects the input cost of producing these goods. Thus, a fall crude oil prices have a positive impact on the stocks of these companies.\n#### A rise in the transportation cost:\nA rise or fall in crude oil prices affects the transportation cost of goods. Crude oil prices have a considerable impact on the prices of consumer durables. These products are manufactured in industrial units and then sold in various cities across nationwide. A fall in the logistics cost of these goods will bring down their final price. A fall in prices of consumer goods raises its demand and thus its stock price.\n#### Inflation:\nEvery US &dollar;10\/bbl increase in oil price will result in a 0.3% or 30 bps increase in CPI. Crude oil has an impact on the prices of all goods and services. Agricultural commodities or manufactured goods, oil prices affect their MRP. A considerable fall in prices of goods and services will ease inflation. Inflation is often perceived negatively by an investor. Thus, a comparatively lower inflation level will be beneficial for the stock market.\n\nBrent crude crossed the *&dollar;86* per barrel mark, in October 2018. This was for the first time in nearly four years. However, it is trading at *&dollar;71* per barrel, a nearly three-month low. Therefore, the international benchmark has fallen nearly 20% in the past one month.\n\n[\ud83e\udc09 Back to Contents](#@Contents)","ab7ec044":"---\n\n[\ud83e\udc09 Back to Contents](#@Contents)","cb266733":"**For the model creation dataset**","78d9a50b":"<a id='@35'><\/a>\n### 35. **Best of Models and Implementation**\n---\nUnderstanding that there may be a better model among the ones we studied our choice of model goes to the Hyperparameter boosted Cross Validated model of the Support Vector Regressor Machine ","7d203c36":"<a id='@4'><\/a>\n### 4. **How is Crude Oil Classified**\n---\nWhen crude is discovered, there is no one single variety found. It exists in a multitude of forms, and its composition will determine how it is transported and refined. Crude is classified by both physical and chemical characteristics.\n\nCrude oil is referred to as either light, medium, or heavy, based on its density. The American Petroleum Institute gravity, commonly shortened to API gravity, compares the density of crude to water. An API gravity higher than 10 means the oil is less dense than water and will float on it. An API gravity lower than 10 means the oil is denser than water and will sink in it.\n\nWhen referring to oil, an API gravity greater than 31.1 degrees is considered light. An API gravity between 22.3 degrees and 31.1 degrees is considered medium. An API gravity between 10.0 degrees and 22.3 degrees is considered heavy. Finally, an API gravity of less than 10.0 degrees would be considered extra heavy.\n\nThe following shows the classifications for crude oil density: \n\n![Crude Oil Classification](https:\/\/cdn.corporatefinanceinstitute.com\/assets\/CrudeOil2-1200x389.jpg)\n\nCrude oil can also be referred to as sour or sweet, based on the sulfur content of the unrefined oil. Determining the sulfur content in crude oil is an important assessment of quality. Sulfur must be removed when refining crude. If it is not, when released into the atmosphere, it can cause pollution and acid rain.\n\nFurthermore, high sulfur content can lead to the degradation of metals used in the refining process. When working with crude that contains hydrogen sulfide, it can also be dangerous because it poses a breathing hazard. Crude oil with a sulfur content greater than 0.5% is considered sour; less than 0.5% is sweet.\n\nThe following shows the classifications for crude oil sulfur content: \n\n![Crude Oil Sulphur Content](https:\/\/cdn.corporatefinanceinstitute.com\/assets\/CrudeOil3-768x319.jpg)\n \n\n[\ud83e\udc09 Back to Contents](#@Contents)","0550e8b6":"<a id='@8'><\/a>\n### 8. **What Determines Crude Oil Prices**\n---\nCrude oil prices depend heavily on the two aforementioned classifications. Light crude is easier to refine and produces higher quantities of high-quality gasoline and diesel fuel. It also flows freely at room temperature. The heavier and denser the oil is, the harder it is to transport. Crude classified as extra heavy can also be referred to as bitumen. It is so thick that it must be diluted to transport.\n\nSulfur content is also very important in determining the quality, and thus the price, of crude. As noted, sulfur must be removed during the refining process. High quantities of sulfur also create problems related to transporting and working with the crude.  For these reasons, sweet crude is generally priced higher relative to sour oil.\n\nIn general, light, sweet crude oil is the most desirable. However, there is one other very important factor that affects the price of crude \u2013 the location of extraction. If crude is extracted near the coast, it is much easier to transport globally. When it is extracted further inland, it must be transported via pipeline systems to refineries and, eventually, to the coast if it is to be transported globally.\n\nWhen determining the price of crude oil, oil benchmarks are used as a pricing tool. There are various benchmark prices that correspond to specific oils, each with a distinct density and API gravity. The most commonly used benchmarks are West Texas Intermediate oil and Brent. Having an accessible price that corresponds to a specific geographical location, density, and gravity allows analysts to compare and determine the prices of different crude oils.\n#### Benchmarks and Trading Markets\nWTI is the benchmark crude for North America. \n\nThe NYMEX (New York Mercantile Exchange) division of the CME (Chicago Mercantile Exchange) lists futures contracts of WTI crude oil. Delivery for WTI crude futures occurs in Cushing, Oklahoma.3\n\nBrent crude oil futures trade on the Intercontinental Exchange (ICE). Brent crude is traded internationally, so the delivery locations will vary by country.\n\nSince both types of oil are used as benchmarks, different countries will use them in different manners. Asian countries tend to use a mixture of Brent and WTI benchmark prices to value their crude oil. \n####  Factors That Affect Benchmark Pricing\nBrent and WTI crude have different properties, which result in a price differential called a \"quality spread.\" They are also located in different parts of the world (Brent in Europe, and WTI in North America). This is referred to as a \"location spread.\"\n\nThe nominal price of crude oil is just one factor involved in understanding the crude oil market. \n\nAccording to CME Group, which runs the NYMEX commodities market, the WTI\/Brent Spread is influenced by four key factors:\n* U.S. crude oil production levels\n* Crude oil supply-and-demand balance in the U.S.\n* North Sea crude oil operations\n* Geopolitical issues in the international crude oil market\n\n####  How World Events Can Affect Crude Oil Prices\nPolitical shifts, weather events, and global health crises have been some of the biggest shock factors in the oil market.\n\nBecause of the coronavirus outbreak, the International Energy Agency cut its forecast for global oil demand in March 2020, predicting the first year-over-year decline in demand since 2009. The IEA predicted in its February 2021 report that demand would recover 60% of its 2020 losses over the course of the year.\n\nTo understand how world events can cause the spread between Brent and WTI to move dramatically for long periods, look back a few years. At the start of 2011, the Brent-WTI spread was close to flat. <br>\n![2000-2011 WTI vs Brent](https:\/\/www.eia.gov\/todayinenergy\/images\/2012.01.12\/2011BriefCrudeAnnual.png)\n\nFor the year 2011 <br>\n![2011 year variation](https:\/\/www.eia.gov\/todayinenergy\/images\/2012.01.12\/2011BriefCrudeDaily.png) \n\n<br><\/br>\nThe spread widened during 2011, with Brent trading at a premium compared to WTI. Around the time that the Arab Spring (an uprising across much of the Arabic region) began in Egypt in February of 2011, the spread widened.\nFears concerning the closure of the Suez Canal and a lack of available supply caused Brent crude oil to become more expensive than WTI. As tensions eased over the canal's operation, the spread reduced.\nThen, in late 2011, the Iranian government threatened to close the Straits of Hormuz, through which approximately 20% of the world's oil flows. Once again, the spread widened, as Brent soared to a ***&dollar; 25*** premium per barrel higher than WTI.\n\nIn 2015, a premium drop for Brent occurred for two reasons. First, an agreement with Iran was struck, allowing the country to export more oil, which should have increased the amount of Iranian crude flowing into the market on a daily basis. Since Brent is the pricing benchmark for Iranian crude, this development depressed the price of Brent at the time.\nSecond, U.S. rig counts dropped at about the same time. With expanding support for exporting U.S. crude abroad, that meant less drilling in the future and less U.S. production on a daily basis. Therefore, Brent prices moved lower by virtue of hints of more Iranian crude, and WTI strengthened because of less U.S. production and increasing exports. It is important to notice that mere anticipation of an influx of oil into the market was enough to cause price fluctuations.\nWeather, too, can have drastic effects on prices. In 2005, hurricanes led to sharp rises in oil prices, as refineries and production facilities shut down for the duration of the weather events.\n\n[\ud83e\udc09 Back to Contents](#@Contents)","521ae036":"---\n\n[\ud83e\udc09 Back to Contents](#@Contents)","648f67c0":"<a id='@15'><\/a>\n### 15. **Data Access**\n---\nGetting data for the project work. All data is accessed from Yahoo Finance for [Brent Oil](https:\/\/finance.yahoo.com\/quote\/BZ%3DF?p=BZ%3DF) with the ticker BZ=F\n\nLinks for Yahooo Finance Live data website\n* [Brent Oil Summary information](https:\/\/finance.yahoo.com\/quote\/BZ%3DF?p=BZ%3DF)\n* [Brent Oil Auto Generated chart](https:\/\/finance.yahoo.com\/quote\/BZ%3DF\/chart?p=BZ%3DF#eyJpbnRlcnZhbCI6ImRheSIsInBlcmlvZGljaXR5IjoxLCJ0aW1lVW5pdCI6bnVsbCwiY2FuZGxlV2lkdGgiOjEsImZsaXBwZWQiOmZhbHNlLCJ2b2x1bWVVbmRlcmxheSI6dHJ1ZSwiYWRqIjp0cnVlLCJjcm9zc2hhaXIiOnRydWUsImNoYXJ0VHlwZSI6ImxpbmUiLCJleHRlbmRlZCI6ZmFsc2UsIm1hcmtldFNlc3Npb25zIjp7fSwiYWdncmVnYXRpb25UeXBlIjoib2hsYyIsImNoYXJ0U2NhbGUiOiJsaW5lYXIiLCJzdHVkaWVzIjp7IuKAjHZvbCB1bmRy4oCMIjp7InR5cGUiOiJ2b2wgdW5kciIsImlucHV0cyI6eyJpZCI6IuKAjHZvbCB1bmRy4oCMIiwiZGlzcGxheSI6IuKAjHZvbCB1bmRy4oCMIn0sIm91dHB1dHMiOnsiVXAgVm9sdW1lIjoiIzAwYjA2MSIsIkRvd24gVm9sdW1lIjoiI2ZmMzMzYSJ9LCJwYW5lbCI6ImNoYXJ0IiwicGFyYW1ldGVycyI6eyJ3aWR0aEZhY3RvciI6MC40NSwiY2hhcnROYW1lIjoiY2hhcnQifX19LCJwYW5lbHMiOnsiY2hhcnQiOnsicGVyY2VudCI6MSwiZGlzcGxheSI6IkJaPUYiLCJjaGFydE5hbWUiOiJjaGFydCIsImluZGV4IjowLCJ5QXhpcyI6eyJuYW1lIjoiY2hhcnQiLCJwb3NpdGlvbiI6bnVsbH0sInlheGlzTEhTIjpbXSwieWF4aXNSSFMiOlsiY2hhcnQiLCLigIx2b2wgdW5kcuKAjCJdfX0sInNldFNwYW4iOm51bGwsImxpbmVXaWR0aCI6Miwic3RyaXBlZEJhY2tncm91bmQiOnRydWUsImV2ZW50cyI6dHJ1ZSwiY29sb3IiOiIjMDA4MWYyIiwic3RyaXBlZEJhY2tncm91ZCI6dHJ1ZSwiZXZlbnRNYXAiOnsiY29ycG9yYXRlIjp7ImRpdnMiOnRydWUsInNwbGl0cyI6dHJ1ZX0sInNpZ0RldiI6e319LCJyYW5nZSI6bnVsbCwic3ltYm9scyI6W3sic3ltYm9sIjoiQlo9RiIsInN5bWJvbE9iamVjdCI6eyJzeW1ib2wiOiJCWj1GIiwicXVvdGVUeXBlIjoiRlVUVVJFIiwiZXhjaGFuZ2VUaW1lWm9uZSI6IkFtZXJpY2EvTmV3X1lvcmsifSwicGVyaW9kaWNpdHkiOjEsImludGVydmFsIjoiZGF5IiwidGltZVVuaXQiOm51bGwsInNldFNwYW4iOm51bGx9XX0-)","527f113d":"Types of Data in the fields","c512ba2d":"---\n\n[\ud83e\udc09 Back to Contents](#@Contents)","0da6c1a1":"<a id='@13'><\/a>\n### 13. **Repository Cloning and Python Distributions**\n---\n NOTICE: (Github Repository) <br>\n  The library and all its contents are available in a separate branch _CaseStudy_backend_ created under the **RepoSJX7** repository in [Github.com](https:\/\/github.com). With a deep understanding of the future aspects of this Case Study project and the impact this project can make in the future, the repository is openly made available links of which are given below. Also since this project work is quite of a small scale contents of this work in future may be shifted to a new repository informations of which will be made available on the same branch\/repository itself and trials to make this a full python package may be initiated provided this branch gains much importance which requires its availability. Till then this is what we have at hand. \n * Repository [RepoSJX7](https:\/\/github.com\/WolfDev8675\/RepoSJX7)\n * Branch [CaseStudy_backend](https:\/\/github.com\/WolfDev8675\/RepoSJX7\/tree\/CaseStudy_backend)\n--- \n NOTICE: (Python Distributions) <br>\n  This library exclusively uses [Python 3.7.8 64bit](https:\/\/www.python.org\/downloads\/release\/python-378\/) distribution in local [Visual Studio 2019](https:\/\/visualstudio.microsoft.com\/vs\/) hence the library files are tested completely in the IDE environment but this notebook is created and tested in [Kaggle](https:\/\/www.kaggle.com\/) which uses the [Anaconda](https:\/\/anaconda.org\/anaconda\/conda\/files?version=4.10.3) which has its own dependencies and restrictions, pros and cons. To meet up with all sides some files are recreated to handle a specific environment better than the other. If recreation of this work is done it is to be made in a way keeping this problems in mind.\n  ([Example of an error we faced due to this dependency restrictions is this](https:\/\/stackoverflow.com\/questions\/66203398\/not-able-to-import-mean-absolute-percentage-error-from-sklearn-metrics#comment117103148_66203889))\n  To combat this problem two files of our original repository have been modified accomodating these issues and renamed exclusively to prevent misunderstanding\n  * [Intelligence.py](https:\/\/github.com\/WolfDev8675\/RepoSJX7\/tree\/CaseStudy_backend\/Intelligence.py) for algorithm handler IDE environment\n  * [Intelligence_conda.py](https:\/\/github.com\/WolfDev8675\/RepoSJX7\/tree\/CaseStudy_backend\/Intelligence_conda.py) for algorithm handler Anaconda environment\n  * [requirements.txt](https:\/\/github.com\/WolfDev8675\/RepoSJX7\/tree\/CaseStudy_backend\/requirements.txt) requirements file for IDE environment\n  * [requirements_conda.txt](https:\/\/github.com\/WolfDev8675\/RepoSJX7\/tree\/CaseStudy_backend\/requirements_conda.txt) requirements file for Anaconda environment\n---\nThis project uses git and python hence a little of terminologies \n* **Git** \n: [Git](https:\/\/git-scm.com\/) is a free and opensource distributed version control system designed to handle everything from small to very large projects with speed and efficiency \n* **Git Cloning** \n: [Git Cloning or Git Clone](https:\/\/git-scm.com\/book\/en\/v2\/Git-Basics-Getting-a-Git-Repository) is a Git command line utility which is used to target an existing repository and create a clone or copy of the target repository.\n* **pip** \n: [PIP](https:\/\/pip.pypa.io) is the package installer for Python. It is used to install packages from the [Python Package Index](https:\/\/pypi.org) and other indexes. \n* **requirements.txt** \n: A file specifying all the package required by the user to download and or install for proper operating of the package \n","1d2054f1":"---\n\n[\ud83e\udc09 Back to Contents](#@Contents)","d5a70545":"**For the final prediction dataset**","fe299c4f":"<a id='@27'><\/a>\n### 27. **What is Cross-Validation ? Why we use it ?**\n---\nCross-validation is a statistical method used to estimate the skill of machine learning models.\nIt is commonly used in applied machine learning to compare and select a model for a given predictive modeling problem because it is easy to understand, easy to implement, and results in skill estimates that generally have a lower bias than other methods.\n Cross validation is a model evaluation method that is better than residuals. The problem with residual evaluations is that they do not give an indication of how well the learner will do when it is asked to make new predictions for data it has not already seen. One way to overcome this problem is to not use the entire data set when training a learner. Some of the data is removed before training begins. Then when training is done, the data that was removed can be used to test the performance of the learned model on _\"new\"_ data. This is the basic idea for a whole class of model evaluation methods called cross validation.\n\nThe holdout method is the simplest kind of cross validation. The data set is separated into two sets, called the training set and the testing set. The function approximator fits a function using the training set only. Then the function approximator is asked to predict the output values for the data in the testing set (it has never seen these output values before). The errors it makes are accumulated as before to give the mean absolute test set error, which is used to evaluate the model. The advantage of this method is that it is usually preferable to the residual method and takes no longer to compute. However, its evaluation can have a high variance. The evaluation may depend heavily on which data points end up in the training set and which end up in the test set, and thus the evaluation may be significantly different depending on how the division is made.\n\nK-fold cross validation is one way to improve over the holdout method. The data set is divided into k subsets, and the holdout method is repeated k times. Each time, one of the k subsets is used as the test set and the other k-1 subsets are put together to form a training set. Then the average error across all k trials is computed. The advantage of this method is that it matters less how the data gets divided. Every data point gets to be in a test set exactly once, and gets to be in a training set k-1 times. The variance of the resulting estimate is reduced as k is increased. The disadvantage of this method is that the training algorithm has to be rerun from scratch k times, which means it takes k times as much computation to make an evaluation. A variant of this method is to randomly divide the data into a test and training set k different times. The advantage of doing this is that you can independently choose how large each test set is and how many trials you average over.\n\nLeave-one-out cross validation is K-fold cross validation taken to its logical extreme, with K equal to N, the number of data points in the set. That means that N separate times, the function approximator is trained on all the data except for one point and a prediction is made for that point. As before the average error is computed and used to evaluate the model. The evaluation given by leave-one-out cross validation error (LOO-XVE) is good, but at first pass it seems very expensive to compute. Fortunately, locally weighted learners can make LOO predictions just as easily as they make regular predictions. That means computing the LOO-XVE takes no more time than computing the residual error and it is a much better way to evaluate models. VizieR relies heavily on LOO-XVE to choose its metacodes. \n_The VizieR Catalogue Service is an astronomical catalog service provided by Centre de donn\u00e9es astronomiques de Strasbourg (CDS). The origin of the service dates back to 1993, when it was founded by the European Space Agency as the European Space Information System (ESIS) Catalogue Browser. Initially intended to serve the space science community, the ESIS project pre-dates the World Wide Web as a network database allowing uniform access to a heterogeneous set of catalogues and data._ \n#### Variations on Data splits and Cross-Validation\nThere are a number of variations on the k-fold cross validation procedure.\n* Train\/Test Split: Taken to one extreme, k may be set to 2 (not 1) such that a single train\/test split is created to evaluate the model. The one we accomplished in the [previous section](#@26)\n* LOOCV: Taken to another extreme, k may be set to the total number of observations in the dataset such that each observation is given a chance to be the held out of the dataset. This is called leave-one-out cross-validation, or LOOCV for short.\n* Stratified: The splitting of data into folds may be governed by criteria such as ensuring that each fold has the same proportion of observations with a given categorical value, such as the class outcome value. This is called stratified cross-validation.\n* Repeated: This is where the k-fold cross-validation procedure is repeated n times, where importantly, the data sample is shuffled prior to each repetition, which results in a different split of the sample.\n* Nested: This is where k-fold cross-validation is performed within each fold of cross-validation, often to perform hyperparameter tuning during model evaluation. This is called nested cross-validation or double cross-validation\n#### k-Fold Cross-Validation (detailed)\nCross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\nThe procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation.\n\nCross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\nIt is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train\/test split.\n\nThe general procedure is as follows:\n\n1. Shuffle the dataset randomly.\n2. Split the dataset into k groups\n3. For each unique group:\n  1. Take the group as a hold out or test data set\n  2. Take the remaining groups as a training data set\n  3. Fit a model on the training set and evaluate it on the test set\n  4. Retain the evaluation score and discard the model\n4. Summarize the skill of the model using the sample of model evaluation scores\n![Cross_val_IMG](https:\/\/miro.medium.com\/max\/2000\/1*fW_qNKmvmg8duWoQnp_PoQ.png)\n\n**_Let us evaluate the set of models with data splits made using K-Fold Cross Validation_**","ddfa96a8":"<a id='@1'><\/a>\n### 1. **Introduction**\n---\nThe majority of people turn to the performance of a country\u2019s stock market as the best indicator of how well that economy is doing. Stock markets cover all industries across all sectors of the economy. This means they serve as a barometer of what cycle the economy is in and the hopes and fears of the population who generate growth and wealth.\n\nOf course, today\u2019s markets are very different from share trading in the Dutch East India Company back in 1602, but stocks still remain the most popular investment choice thanks to their potential for returns and their opportunity to invest directly in individual companies.\n\nBack in the day, Businesses and other Institutions were able to store most of their data in Microsoft Excel Sheets. Even the modest Business Intelligence tools were capable of analyzing and processing this data. The presence of a lesser amount of data made the handling and managing of data easier. But with the passage of time, the amount of data produced every day kept increasing. \n\nA study by Forbes which states that nearly 2.5 Quintillion Bytes of data is generated every day. According to Raconteur, by 2025, 463 Exabytes of data is expected to be generated every day globally.\n\n![visual_intro](https:\/\/res.cloudinary.com\/hevo\/image\/upload\/f_auto,q_auto\/f_auto,q_auto\/$wpsize_!_cld_full!,w_1280,h_787,c_scale\/v1627460405\/hevo-learn\/Machine-Lerning-in-Data-Science.png) \n \nOil from the Outer Continental Shelf (OCS) helps meet the U.S. National Energy Needs. Crude oil is \nproduced from the OCS and other domestic locations and then is converted by refineries into several types \nof petroleum products which consumers use as both fuel and as a component of non-fuel products. \n\nIndia\u2019s crude oil import bill soared nearly threefold in the first quarter of the fiscal (Q1FY22) fuelled by a sharp rise in global oil prices, further raising concerns for policymakers.\n\nThe fuel import bill totalled about USD24.7 billion in the three months ended 30 June, compared with USD8.5 billion a year earlier, according to official data. Volume growth was, however, modest at 14.7% in the June quarter at 51.4 million tonnes.\n\n\n[\ud83e\udc09 Back to Contents](#@Contents)","a944e8a1":"<a id='@Begin'><\/a>\n# **Brent Crude Oil Price Prediction**\n![Crude oil-rig](https:\/\/economictimes.indiatimes.com\/thumb\/msid-81163820,width-2600,height-1500,resizemode-4,imgsize-58280\/crude-oil.jpg?from=mdr)","ec564cdc":"<a id='@37'><\/a>\n### 37. **Distribution and Licence**\n---\nExploration of the file systems are Open source and freely distributable provided the licence terms are maintained and the work is not used for any illegal activities. \n\nLICENCE (MIT)\nMIT License\n\nCopyright (c) 2021 Bishal Biswas\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n[Repository RepoSJX7-CaseStudy_backend licence file](https:\/\/github.com\/WolfDev8675\/RepoSJX7\/blob\/CaseStudy_backend\/LICENSE) <br> <\/br>\n[More Information on MIT Licence](https:\/\/tlo.mit.edu\/learn-about-intellectual-property\/software-and-open-source-licensing)\n\n[\ud83e\udc09 Back to Contents](#@Contents)","4a5c5ba6":"<a id='@17'><\/a>\n### 17. **Variable set definition (Why we are defining as we are defining)**\n---\n Noticing the observations in the [last section](#@16) we can clearly see that \n 1. The Model Creating data has fields: Open, High, Low, Close, Adj Close, Volume\n 2. The Final Prediction data has fields: Open, High, Low, Close, Volume, Dividends,StockSplits\n 3. The Volume field is awkwardly large values far surpassing the limit ranges of the others.\n thus we have a common set of fields: *Open, High, Low, Close, Volume*. \n \n  Now, our objective was to determine the price of oil stocks hence our target is to match closely the paces of the Close field and use the other fields to draw a relationship between them.\n  This is also observed the nature of Volume field so for determining the Close target value we need to drop Volume field from the race.\n  \n Thus we stand at a conclusion that our dependent and independent fields are \n * __Dependent__ or Response variable is $Response=Close$\n * __Independent__ or Predictor variables are $Predictors=[Open,High,Low]$\n \n Also notable is our data is a time-variant data. &nbsp;A dataset which is a map of changes of its value over time hence a time-series data.\n \n[\ud83e\udc09 Back to Contents](#@Contents)","bcd70ae9":"<a id='@16'><\/a>\n### 16. **Assessment of the Data**\n---\n\nThe datasets we obtained need to be checked for its contents and nature of the values to assess whether or not or specifically which data do we use to teach our machine learning algorithms. But, before that, let us first check the nature of the stock data we obtained with a interactive visual with the model creation dataset.","d4d0be6b":"---\n\n[\ud83e\udc09 Back to Contents](#@Contents)","0b454aeb":"<a id='@Bibliography'><\/a>\n### **Bibliography**\n---\n* https:\/\/corporatefinanceinstitute.com\/resources\/knowledge\/economics\/crude-oil-overview\/\n* https:\/\/www.epa.gov\/emergency-response\/types-crude-oil\n* https:\/\/corporate.exxonmobil.com\/Crude-oils\/Crude-trading\/Crude-oil-blends-by-API-gravity-and-by-sulfur-content#APIgravity\n* https:\/\/www.mckinseyenergyinsights.com\/resources\/refinery-reference-desk\/crude-grades\/\n* https:\/\/www.thebalance.com\/crude-oil-brent-versus-wti-808872\n* https:\/\/www.economist.com\/the-economist-explains\/2018\/10\/29\/what-is-brent-crude\n* https:\/\/www.imf.org\/external\/pubs\/ft\/oil\/2000\/\n* https:\/\/www.eia.gov\/todayinenergy\/detail.php?id=4550\n* https:\/\/www.frbsf.org\/education\/publications\/doctor-econ\/2007\/november\/oil-prices-impact-economy\/\n* http:\/\/www.philadelphiafed.org\/files\/br\/2007\/br_q1-2007-3_oil-shocks.pdf\n* https:\/\/www.frbsf.org\/economic-research\/publications\/economic-letter\/2005\/november\/why-the-jump-in-oil-prices-has-not-led-to-a-recession\/\n* http:\/\/www.nber.org\/papers\/w13368\n* http:\/\/www.nytimes.com\/2008\/06\/27\/opinion\/27krugman.html\n* https:\/\/www.frbsf.org\/economic-research\/publications\/economic-letter\/2005\/november\/why-the-jump-in-oil-prices-has-not-led-to-a-recession\/\n* http:\/\/www.dallasfed.org\/research\/swe\/2006\/swe0604e.pdf\n* https:\/\/www.kotaksecurities.com\/ksweb\/Meaningful-Minutes\/6-effects-of-rising-crude-oil-prices-on-the-Indian-economy\n* https:\/\/www.kotaksecurities.com\/ksweb\/meaningful-minutes\/the-impact-of-crude-oil-prices-on-indian-stock-markets\n* [Sharma, et al.: Relationship between Crude Oil Prices and Stock Market: Evidence from India](https:\/\/www.econjournals.com\/index.php\/ijeep\/article\/download\/4439\/3869)\n* https:\/\/www.nasdaq.com\/market-activity\/commodities\/bz%3Anmx\/historical\n* https:\/\/finance.yahoo.com\/quote\/BZ%3DF?p=BZ%3DF\n* https:\/\/www.investopedia.com\/terms\/t\/timeseries.asp\n* https:\/\/www.investopedia.com\/terms\/b\/box-jenkins-model.asp\n* https:\/\/www.investopedia.com\/terms\/a\/autoregressive-integrated-moving-average-arima.asp\n* https:\/\/www.investopedia.com\/terms\/s\/seasonality.asp\n* https:\/\/www.geeksforgeeks.org\/what-is-a-trend-in-time-series\/\n* https:\/\/towardsdatascience.com\/trend-seasonality-moving-average-auto-regressive-model-my-journey-to-time-series-data-with-edc4c0c8284b\n* https:\/\/www.oraylis.de\/blog\/2015\/trend-in-times-series-analysis\n* https:\/\/seaborn.pydata.org\/api.html\n* https:\/\/seaborn.pydata.org\/generated\/seaborn.heatmap.html#seaborn.heatmap\n* https:\/\/seaborn.pydata.org\/generated\/seaborn.pairplot.html#seaborn.pairplot\n* https:\/\/seaborn.pydata.org\/generated\/seaborn.boxplot.html#seaborn.boxplot\n* https:\/\/seaborn.pydata.org\/generated\/seaborn.scatterplot.html#seaborn.scatterplot\n* https:\/\/www.sisense.com\/glossary\/data-standardization\/\n* https:\/\/corporatefinanceinstitute.com\/resources\/knowledge\/economics\/standardization\/\n* https:\/\/www.techopedia.com\/definition\/1221\/normalization \n* https:\/\/www.ibm.com\/in-en\/cloud\/learn\/machine-learning\n* https:\/\/www.expert.ai\/blog\/machine-learning-definition\/\n* https:\/\/www.analyticssteps.com\/blogs\/introduction-time-series-analysis-time-series-forecasting-machine-learning-methods-models\n* https:\/\/en.wikipedia.org\/wiki\/Scikit-learn\n* https:\/\/scikit-learn.org\/stable\/\n* https:\/\/scikit-learn.org\/\n* https:\/\/www.appier.com\/blog\/5-types-of-regression-analysis-and-when-to-use-them\/\n* https:\/\/www.mygreatlearning.com\/blog\/what-is-regression\/\n* https:\/\/towardsdatascience.com\/introduction-to-bayesian-linear-regression-e66e60791ea7\n* https:\/\/towardsdatascience.com\/understanding-random-forest-58381e0602d2\n* https:\/\/www.section.io\/engineering-education\/introduction-to-random-forest-in-machine-learning\/\n* https:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/understaing-support-vector-machine-example-code\/\n* https:\/\/monkeylearn.com\/blog\/introduction-to-support-vector-machines-svm\/\n* https:\/\/machinelearningmastery.com\/k-fold-cross-validation\/\n* https:\/\/vizier.u-strasbg.fr\/viz-bin\/VizieR\n* https:\/\/www.cs.cmu.edu\/~schneide\/tut5\/node42.html \n* https:\/\/machinelearningmastery.com\/gradient-descent-for-machine-learning\/\n* https:\/\/machinelearningmastery.com\/difference-between-a-parameter-and-a-hyperparameter\/\n* https:\/\/towardsdatascience.com\/parameters-and-hyperparameters-aa609601a9ac\n* https:\/\/stats.stackexchange.com\/questions\/269053\/how-to-select-hyperparameters-for-svm-regression-after-grid-search\n* https:\/\/machinelearningmastery.com\/how-to-know-if-your-machine-learning-model-has-good-performance\/\n* https:\/\/tlo.mit.edu\/learn-about-intellectual-property\/software-and-open-source-licensing\n* http:\/\/www.opensource.org\/licenses\n* https:\/\/github.com\/WolfDev8675\/RepoSJX7\/tree\/CaseStudy_backend\n\n---\n[\ud83e\udc09 Back to Contents](#@Contents)\n\n\n[Back to Begining](#@Begin)","e99f91c7":"and the brief description ","408101cf":"Observation: similar to the previous observations volume field is disproportionate to others. Additionally Dividends and StockSplits are zero fields (filled with zero values)","c1ce920e":"#### Support Vector Machines","9b31069f":"#### Linear Regression and Bayesian Ridge Regression","79e07a92":"<a id='@6'><\/a>\n### 6. **Blends of Crude Oil**\n---\nCrude oil comes in hundreds of different varieties of blends or \"grades\". These grades are valued differently by refiners based on their crude qualities.\n\nSome of the more common crude oil grades that have regular price assessments done are:\n\n| Crude | API | Sulfur | Source |\n|:--------------:|:--------------:|:--------------:|:--------------:|\n|<div style=\"width:290px\"><\/div>|<div style=\"width:290px\"><\/div>|<div style=\"width:290px\"><\/div>|<div style=\"width:290px\"><\/div>|\n| Agbami |\t47.2| \t0.05| \tNigeria|\n|Akpo |\t45.8 |\t0.07 |\tNigeria |\n|Al Shaeen |\t30.3 |\t1.90 |\tQatar |\n|Amenam |\t37.0 |\t0.17 |\tNigeria |\n|Amna |\t37.0 |\t0.17 |\tLibya |\n|ANS |\t31.4 |\t0.96 |\tUS |\n|Arab Extra Light |\t40.0 |\t1.09 |\tSaudi Arabia |\n|Arab Heavy |\t28.0 |\t2.80 |\tSaudi Arabia |\n|Arab Light |\t33.0 |\t1.77 |\tSaudi Arabia |\n|Arab Medium |\t31.0 |\t2.55 |\tSaudi Arabia |\n|Arab Super Light |\t51.0 |\t0.09 |\tSaudi Arabia |\n|Ardjuna |\t37.0 |\t0.09 |\tIndonesia |\n|Attaka  |\t43.0 |\t0.09 |\tIndonesia |\n|Azeri Light |\t34.9 |\t0.55 |\tAzerbaijan |\n|Bach Ho |\t39.0 |\t0.04 |\tVietnam |\n|Bakken |\t42.1 |\t0.18 |\tUS |\n|Banoco |\t31.8 |\t2.45 |\tBahrain |\n|Basrah Heavy |\t23.6 |\t4.20 |\tIraq |\n|Basrah Light |\t30.0 |\t2.92 |\tIraq |\n|Belida |\t45.0 |\t0.02 |\tIndonesia |\n|Bonga |\t29.1 |\t0.29 |\tNigeria |\n|Bonito | \t33.5 |\t1.32 |\tUS |\n|Bonny Light |\t34.5 |\t0.14 |\tNigeria |\n|Bow River |\t24.7 |\t2.10 |\tCanada |\n|Brass River |\t36.5 |\t0.13 |\tNigeria |\n|Brega |\t42.0 |\t0.22 |\tLibya |\n|<span style=\"background-color:teal; width:290px\"> &nbsp; &nbsp; &nbsp;$Brent$ &nbsp; &nbsp; &nbsp;<\/span> |<span style=\"background-color:teal; width:290px\"> &nbsp; &nbsp; &nbsp;$37.9$ &nbsp; &nbsp; &nbsp;<\/span> |<span style=\"background-color:teal; width:290px\"> &nbsp; &nbsp; &nbsp;$0.45$ &nbsp; &nbsp; &nbsp;<\/span>|<span style=\"background-color:teal; width:290px\"> &nbsp; &nbsp; &nbsp;$UK$ &nbsp; &nbsp; &nbsp;<\/span>|\n|BTC |\t36.6 | \t0.16 |\tAzerbaijan |\n|Cabinda |\t32.5 |\t0.13 |\tAngola |\n|Castilla |\t18.8 |\t1.97 |\tColombia |\n|Champion |\t28.0 |\t0.12 |\tBrunei |\n|Cinta |\t32.7 |\t0.12 |\tIndonesia |\n|Cold Lake |\t19.9 |\t3.25 |\tCanada |\n|Cossack |\t48.0 |\t0.04 |\tAustralia |\n|CPC |\t46.6 |\t0.55 |\tKazakhstan |\n|Dalia |\t23.7 |\t0.49 |\tAngola |\n|Daqing |\t32.7 |\t0.10 |\tChina |\n|Dar |\t26.4 |\t0.12 |\tSudan |\n|Das |\t39.2 |\t1.30 |\tUAE |\n|Djeno |\t27.6 |\t0.34 |\tCongo |\n|Doba |\t25.1 |\t0.08 |\tChad |\n|Dubai |\t31.0 |\t2.04 |\tUAE |\n|DUC |\t33.5 |\t0.25 |\tDenmark |\n|Duri |\t21.5 |\t0.20 |\tIndonesia |\n|Eagle Ford |\t45.0 |\t0.20 |\tUS |\n|Ekofisk |\t37.5 |\t0.23 |\tNorway |\n|Enfield |\t22.0 |\t0.12 |\tAustralia |\n|Erha |\t33.7 |\t0.18 |\tNigeria |\n|Es Sider |\t36.2 |\t0.49 |\tLibya |\n|Escalante |\t24.1 |\t0.19 |\tArgentina |\n|Escravos |\t34.0 |\t0.15 |\tNigeria |\n|ESPO |\t36.0 |\t0.50 |\tRussia |\n|Eugene Island |\t36.0 |\t1.20 |\tUS |\n|Fateh |\t30.0 |\t3.00 |\tUAE |\n|Flotta |\t36.9 |\t0.82 |\tUK |\n|Forcados | \t30.0 |\t0.15 |\tNigeria |\n|Forties |\t40.3 |\t0.56 |\tUK |\n|Gippsland |\t48.0 |\t0.10 |\tAustralia |\n|Girassol |\t31.0 |\t0.33 |\tAngola |\n|Griffin |\t55.0 |\t0.03 |\tAustralia |\n|Gullfaks |\t36.2 |\t0.26 |\tNorway |\n|Handil |\t33.8 |\t0.07 |\tIndonesia |\n|Hibernia |\t34.6 |\t0.47 |\tCanada |\n|HLS |\t33.5 |\t0.42 |\tUS |\n|Hungo |\t27.4 |\t0.65 |\tAngola |\n|Iran Heavy |\t30.7 |\t1.80 |\tIran |\n|Iran Light |\t33.7 |\t1.50 |\tIran |\n|Isthmus |\t33.6 |\t1.30 |\tMexico |\n|Jubilee |\t36.4 |\t0.26 |\tGhana |\n|Kern River |\t13.0 |\t1.10 |\tCanada |\n|Khafji |\t29.0 |\t2.85 |\tNeutral Zone |\n|Kikeh |\t36.7 |\t0.06 |\tMalaysia |\n|Kimanis |\t38.6 |\t0.06 |\tMalaysia |\n|Kirkuk |\t36.0 |\t2.00 |\tIraq |\n|Kissanje |\t30.7 |\t0.36 |\tAngola |\n|Kole |\t30.2 |\t0.34 |\tCameroon |\n|Kumkol |\t41.2 |\t0.11 |\tKazakhstan |\n|Kutubu |\t45.0 |\t0.04 |\tPapua New Guinea |\n|Kuwait |\t31.0 |\t2.52 |\tKuwait |\n|Labuan |\t29.9 |\t0.03 |\tMalaysia |\n|LHS |\t42.0 |\t0.45 |\tUS |\n|Line 63 |\t28.0 |\t1.02 |\tUS |\n|LLB |\t20.9 |\t3.46 |\tCanada |\n|LLS |\t38.5 |\t0.39 |\tUS |\n|Loreto |\t18.1 |\t1.30 |\tPeru |\n|LSB |\t36.1 |\t1.05 |\tCanada |\n|Magdalena |\t20.0 |\t1.60 |\tColombia |\n|Marlim |\t19.2 |\t0.78 |\tBrazil |\n|Mars |\t28.0 |\t1.93 |\tUS |\n|Masila |\t31.0 |\t0.54 |\tYemen |\n|Maya |\t22.0 |\t3.30 |\tMexico |\n|MEH |\t42.0 |\t0.45 |\tUS |\n|Merey |\t16.0 |\t2.45 |\tVenezuela |\n|Mesa |\t30.0 |\t0.90 |\tVenezuela |\n|Midale |\t30.0 |\t2.35 |\tCanada |\n|Midway Sunset |\t13.0 |\t1.20 |\tUS |\n|Minas |\t35.0 |\t0.08 |\tIndonesia |\n|Miri |\t29.8 |\t0.08 |\tMalaysia |\n|MSW |\t40.5 |\t0.40 |\tCanada |\n|Murban |\t40.4 |\t0.79 |\tUAE |\n|Nanhai |\t39.5 |\t0.05 |\tChina |\n|Napo |\t19.0 |\t2.01 |\tEcuador |\n|Nemba |\t38.7 |\t0.19 |\tAngola |\n|Nile Blend |\t32.8 |\t0.05 |\tSudan |\n|Niobrara |\t39.0 |\t0.39 |\tUS |\n|Olmeca |\t39.3 |\t0.80 |\tMexico |\n|Oman |\t33.3 |\t1.06 |\tOman |\n|Oriente |\t24.0 |\t1.40 |\tEcuador |\n|Oseberg |\t39.6 |\t0.20 |\tNorway |\n|Palanca |\t37.2 |\t0.18 |\tAngola |\n|Pazflor |\t25.6 |\t0.43 |\tAngola |\n|Plutonio |\t33.2 |\t0.37 |\tAngola |\n|Poseidon |\t30.5 |\t1.70 |\tUS |\n|Pyrenees |\t19.0 |\t0.21 |\tAustralia |\n|Qatar Land |\t41.1 |\t1.22 |\tQatar |\n|Qatar Marine |\t36.2 |\t1.60 |\tQatar |\n|Qua Iboe |\t36.6 |\t1.60 |\tNigeria |\n|Rabi |\t33.2 |\t0.14 |\tGabon |\n|Roncador |\t33.2 |\t0.14 |\tGabon |\n|Saharan Blend |\t46.0 |\t0.10 |\tAlgeria |\n|Sakhalin Blend |\t44.7 |\t0.16 |\tRussia |\n|San Joaquin Light |\t26.0 |\t1.10 |\tUS |\n|Santa Barbara |\t36.0 |\t0.95 |\tVenezuela |\n|Sarir |\t38.0 |\t0.83 |\tLibya |\n|Senipah |\t51.0 |\t0.03 |\tIndonesia |\n|Seria Blend |\t38.0 |\t0.08 |\tBrunei |\n|Shengli |\t24.0 |\t0.90 |\tChina |\n|Siberian Light |\t34.8 |\t0.57 |\tRussia |\n|Sirtica |\t41.0 |\t0.40 |\tLibya |\n|Sokol |\t35.6 |\t0.27 |\tRussia |\n|Southern Green Canyon |\t28.5 |\t2.14 |\tUS |\n|Statfjord |\t39.1 |\t0.22 |\tNorway |\n|Su Tu Den |\t35.8 |\t0.05 |\tVietnam |\n|Suez Blend |\t30.4 |\t1.65 |\tEgypt |\n|Syncrude  |\t33.0 |\t0.16 |\tCanada |\n|Syrian Heavy |\t23.1 |\t4.19 |\tSyria |\n|Syrian Light |\t38.0 |\t0.68 |\tSyria |\n|Tapis |\t46.0 |\t0.02 |\tMalaysia |\n|Tengiz |\t47.5 |\t0.47 |\tKazakhstan |\n|Terra Nova |\t33.2 |\t0.48 |\tCanada |\n|THUMS |\t17.0 |\t1.50 |\tUS |\n|Thunder Horse |\t31.5 |\t0.96 |\tUS |\n|Troll |\t35.9 |\t0.14 |\tNorway |\n|Upper Zakum |\t34.0 |\t1.95 |\tUAE |\n|Urals |\t30.6 |\t1.48 |\tRussia |\n|Usan |\t30.6 |\t0.23 |\tNigeria |\n|Van Gogh |\t17.0 |\t0.37 |\tAustralia |\n|Vasconia |\t24.3 |\t0.83 |\tColombia |\n|Vincent |\t18.5 |\t0.55 |\tAustralia |\n|WCS |\t20.1 |\t3.64 |\tCanada |\n|White Cliffs |\t46.0 |\t0.39 |\tUS |\n|White Rose |\t46.0 |\t0.39 |\tCanada |\n|Widuri |\t33.3 |\t0.07 |\tIndonesia |\n|Wilmington |\t17.0 |\t1.20 |\tUS |\n|WTI |\t42.0 |\t0.45 |\tUS |\n|WTS |\t38.0 |\t0.50 |\tUS |\n|Wyoming Sweet |\t32.0 |\t0.90 |\tUS |\n|Zafiro |\t29.5 |\t0.26 |\tEquatorial Guinea |\n|Zarzaitine |\t42.8 |\t0.06 |\tAlgeria |\n|Zuetina |\t41.5 |\t0.31 |\tLibya | \n#### The Two Most Prevalent Grades of Crude Oil\nWhen it comes to physical oil, there are different grades. The most heavily traded grades are Brent North Sea crude (commonly known as \"Brent crude\") and West Texas Intermediate (commonly known as \"WTI\"). Brent is oil that is produced in the Brent oil fields and other sites in the North Sea.\nBrent crude's price is the benchmark for African, European, and Middle Eastern crude oil. The pricing mechanism for Brent dictates the value of roughly two-thirds of the world's crude oil production.\n\nThe percentage of sulfur in crude oil determines the amount of processing needed to refine the oil into energy products. \"Sweet crude\" is a term that refers to crude oil that has less than 1% sulfur.\nThe sulfur content of both Brent and WTI is well under 1%, making them both \u201csweet.\u201d These types are also less dense (\u201clighter\u201d) than many of the crude oils extracted elsewhere. Both of these characteristics make them easier to refine and more attractive to petroleum product producers\n\n\n[\ud83e\udc09 Back to Contents](#@Contents)","2c56ec12":"## Acknowledgement\nOur project and everything started during the ongoing reign of SARS \u2013 CoVID19, virtually crippling the society and world as a whole sending everything into a lockdown. Although at better stage but with a second wave looming on the pre-existing distress in-spite of new vaccines combating the situation. Overcoming all this chaos the course of Post Graduate Diploma in Data Science by BSE in collaboration with MAKAUT was made possible thanks to the diplomacy and steps taken by both institutes to combat the situation and make this course and project a possibility.\n\nI want to take this opportunity of the project to thank the people at BSE and MAKAUT who provided us this opportunity to have an exposure to real life scenarios and the status of the present market. I also want to thank Prof. Ashok Gupta for guiding with every step from imparting knowledge about the subject to the intricacies of the Data Preparation to Analysis and Model Creation including Cleaning, Visualization, clearing doubts and issues faced in addition to solving problems encountered, all the way through.\n\nI am also grateful to my batchmates and peers where our collective knowledgebase and doubt clearing helped a lot in completing this project. Lastly, I want to thank my family for the mental support they provided me that played a big part in completing this project.\n\n***Bishal Biswas.*** <br>\n***BSE GENERATED ID: PGDDSPJULY2020\/1*** <br>\n***MAKAUT ENROLMENT:20BIL001P12029005*** <br>\n***MAKAUT APPLICATION ID: 91268*** <br>\n***b.biswas_94587@ieee.org*** <br>\n\n&copy; [Bishal Biswas](https:\/\/github.com\/WolfDev8675)","f6fbad20":"<a id='@28'><\/a>\n### 28. **How Cross-Validation improved our models ?**\n---\nLet's evaluate the improvemnts we made from training with data splits of two methods \n1. [General Split (70% train - 30% test)](#@26)\n2. [Cross Validation Split(K - Fold at $K=5$)](#@27)","c5b6de9c":"while we are at this let us also check the residuals ","187844ed":"&copy;[Bishal Biswas(@WolfDev8675)](https:\/\/github.com\/WolfDev8675) <br>\n*(b.biswas_94587@ieee.org)*","4765d5e0":"Observation: here we see that we have 6 fields all with 64bit floating point data except for the volume field that has a 64bit integer data. Also to be noted that all have the same number of non-null counts (1056) and the dataset has 1056 entries hence dataset has no null values that requires filling or removal.","de00062e":"---\n\n[\ud83e\udc09 Back to Contents](#@Contents)","10b2ac52":"Observation: no null entries. Similar to the previous dataset the pricing fields are 64bit floating point numbers the others are 64bit integers","284af6b2":"<a id='@38'><\/a>\n### 38. **What we achieved ? (from the *Case Study*)**\n---\nA lot is achieved from the above work. It was in fact a deep experience of learning from the work. In short,\n* This work helped us to understand the mechanics of scikit-learn\n* We learnt a lot about Linear Regression and how a simple method can be yet effective over more complex functionalities\n* Sometimes too much correctness report of a system is rather a bluff than reality. The concept of Overfitting.\n* Vector Machines reported well with the hyperparameter boosting than Random Forests. Hence, we mustn't be quick to jump the wheels about a model's efficacy, there might always be underdogs in competition.\n* Grid-search and Randomized search methods could be used on all models although not used to avoid misinterpretation of conventionalities.\n* LaTeX is nice presentable method especially for equations but handling them for an absolute starter may feel disastrous in Jupyter Notebook environment.\n* IDE Python and Anaconda configured python is quite different in their handling capacities. Caution must be maintained while migrating from one environment to other.\n* Raw configured Python is more upgradable and quick to response than Conda, Anaconda distributions since their libraries are fixed set. On a sidenote, with Python 3.10 rolling in, its time the Anaconda webmasters and repository controllers might as well look to upgrade their library dependencies.\n\n\n[\ud83e\udc09 Back to Contents](#@Contents)","f58bab38":"<a id='@5'><\/a>\n### 5. **Types of Crude Oil**\n---\n\nThe petroleum industry often characterizes crude oils according to their geographical source, e.g., Alaska North Slope Crude. However, classification of crude oil types by geographical source is not a useful classification scheme for response personnel. This classification offers little information about general toxicity, physical state, and changes that occur with time and weathering. These characteristics are primary considerations in oil spill response. The classification scheme provided below is more useful in a response scenario.\n\n_Class A_: Light, Volatile Oils. These oils are:\n\n* highly fluid,\n* often clear,\n* spread rapidly on solid or water surfaces,\n* have a strong odor, \n* high evaporation rate, and\n* are usually flammable.\n\nThey penetrate porous surfaces such as dirt and sand, and may be persistent in such a matrix. They do not tend to adhere to surfaces. Flushing with water generally removes them. Class A oils may be highly toxic to humans, fish, and other organisms. Most refined products and many of the highest quality light crudes can be included in this class.\n\n_Class B_: Non-Sticky Oils. These oils have a waxy or oily feel. Class B oils are less toxic and adhere more firmly to surfaces than Class A oils, although they can be removed from surfaces by vigorous flushing. As temperatures rise, their tendency to penetrate porous substrates increases and they can be persistent. Evaporation of volatiles may lead to a Class C or D residue. Medium to heavy paraffin-based oils fall into this class.\n\n_Class C_: Heavy, Sticky Oils. Class C oils are characteristically:\n\n* viscous,\n* sticky or tarry, and\n* brown or black.\n\nFlushing with water will not readily remove this material from surfaces, but the oil does not readily penetrate porous surfaces. The density of Class C oils may be near that of water and they often sink. Weathering or evaporation of volatiles may produce solid or tarry Class D oil. Toxicity is low, but wildlife can be smothered or drowned when contaminated. This class includes residual fuel oils and medium to heavy crudes.\n\n_Class D:_ Nonfluid Oils. Class D oils are:\n\n* relatively non-toxic,\n* do not penetrate porous substrates, and\n* are usually black or dark brown in color.\n\nWhen heated, Class D oils may melt and coat surfaces making cleanup very difficult. Residual oils, heavy crude oils, some high paraffin oils, and some weathered oils fall into this class.\n\nThese classifications are dynamic for spilled oils. Weather conditions and water temperature greatly influence the behavior of oil and refined petroleum products in the environment. For example, as volatiles evaporate from a Class B oil, it may become a Class C oil. If a significant temperature drop occurs (e.g., at night), a Class C oil may solidify and resemble a Class D oil. Upon warming, the Class D oil may revert back to a Class C oil.\n\n[\ud83e\udc09 Back to Contents](#@Contents)","45a63267":"---\n\n[\ud83e\udc09 Back to Contents](#@Contents)","9b69b4a0":"<a id='@25'><\/a>\n### 25. **Definition of Models (A brief idea of their _modus operandi_)**\n---\n#### Linear regression\n &nbsp; &nbsp; &nbsp; &nbsp;One of the most basic types of regression in machine learning, linear regression comprises a predictor variable and a dependent variable related to each other in a linear fashion. Linear regression involves the use of a best fit line. We should use linear regression when our variables are related linearly. For example, if we are forecasting the effect of increased advertising spend on sales. However, this analysis is susceptible to outliers, so it should not be used to analyze big data sets.\n#### Bayesian Ridge\n &nbsp; &nbsp; &nbsp; &nbsp;In the Bayesian viewpoint, we formulate linear regression using probability distributions rather than point estimates. The response, y, is not estimated as a single value, but is assumed to be drawn from a probability distribution. The model for Bayesian Linear Regression with the response sampled from a normal distribution is:\n \n $y $~$ N(\\beta^T X, \\sigma^2 I)$\n\nThe output, y is generated from a normal (Gaussian) Distribution characterized by a mean and variance. The mean for linear regression is the transpose of the weight matrix multiplied by the predictor matrix. The variance is the square of the standard deviation \u03c3 (multiplied by the Identity matrix because this is a multi-dimensional formulation of the model).\n\nThe aim of Bayesian Linear Regression is not to find the single \u201cbest\u201d value of the model parameters, but rather to determine the posterior distribution for the model parameters. Not only is the response generated from a probability distribution, but the model parameters are assumed to come from a distribution as well. \n#### Random Forest\nA random forest is a machine learning technique that\u2019s used to solve regression and classification problems. It utilizes ensemble learning, which is a technique that combines many classifiers to provide solutions to complex problems. A random forest algorithm consists of many decision trees. The \u2018forest\u2019 generated by the random forest algorithm is trained through bagging or bootstrap aggregating. Bagging is an ensemble meta-algorithm that improves the accuracy of machine learning algorithms.\n\nThe (random forest) algorithm establishes the outcome based on the predictions of the decision trees. It predicts by taking the average or mean of the output from various trees. Increasing the number of trees increases the precision of the outcome. A random forest eradicates the limitations of a decision tree algorithm. It reduces the overfitting of datasets and increases precision. It generates predictions without requiring many configurations in packages (like scikit-learn).\n##### Features of a Random Forest Algorithm\n* It\u2019s more accurate than the decision tree algorithm.\n* It provides an effective way of handling missing data.\n* It can produce a reasonable prediction without hyper-parameter tuning.\n* It solves the issue of overfitting in decision trees.\n* In every random forest tree, a subset of features is selected randomly at the node\u2019s splitting point.\n\n##### How random forest algorithm works\n###### Understanding decision trees\nDecision trees are the building blocks of a random forest algorithm. A decision tree is a decision support technique that forms a tree-like structure. An overview of decision trees will help us understand how random forest algorithms work.\nA decision tree consists of three components: decision nodes, leaf nodes, and a root node. A decision tree algorithm divides a training dataset into branches, which further segregate into other branches. This sequence continues until a leaf node is attained. The leaf node cannot be segregated further.\n###### Applying decision trees in random forest\nThe main difference between the decision tree algorithm and the random forest algorithm is that establishing root nodes and segregating nodes is done randomly in the latter. The random forest employs the bagging method to generate the required prediction.\nBagging involves using different samples of data (training data) rather than just one sample. A training dataset comprises observations and features that are used for making predictions. The decision trees produce different outputs, depending on the training data fed to the random forest algorithm. These outputs will be ranked, and the highest will be selected as the final output.\n#### Support Vector Machines\n\"Support Vector Machine\" (SVM) is a supervised machine learning algorithm that can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is a number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well.\n##### How Does SVM Work?\nLet\u2019s imagine we have two tags: red and blue, and our data has two features: x and y. We want a classifier that, given a pair of (x,y) coordinates, outputs if it\u2019s either red or blue. We plot our already labeled training data on a plane:\n\n![genPlotSVM_HSW](https:\/\/monkeylearn.com\/static\/52081a1b625e8ba22c00210d547b4f1a\/d8712\/plot_original.webp) \n\nA support vector machine takes these data points and outputs the hyperplane (which in two dimensions it\u2019s simply a line) that best separates the tags. This line is the decision boundary: anything that falls to one side of it we will classify as blue, and anything that falls to the other as red\n\n![bestHplaneSVM_HSW](https:\/\/monkeylearn.com\/static\/57fd2448dfb67cfff990f32191463e80\/d8712\/plot_hyperplanes_2.webp)\n\nBut, what exactly is the best hyperplane? For SVM, it\u2019s the one that maximizes the margins from both tags. In other words: the hyperplane (remember it's a line in this case) whose distance to the nearest element of each tag is the largest. \n\n![cmpHplaneSVM_HSW](https:\/\/monkeylearn.com\/static\/7002b9ebbacb0e878edbf30e8ff5b01c\/d8712\/plot_hyperplanes_annotated.webp)\n\n**_Let us now generate the model dictionaries_**"}}