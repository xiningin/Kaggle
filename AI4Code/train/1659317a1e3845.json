{"cell_type":{"6b3de137":"code","879d2e42":"code","7cd7c58a":"code","55b10dad":"code","16f6304e":"code","114b5469":"code","0b2ecd36":"code","2d5f329b":"code","5a86cf76":"code","c393572c":"code","661f2b5d":"code","372b7c67":"code","5cc99cfb":"code","1d771ddc":"code","f749af6d":"code","7767504f":"code","72f952a5":"code","32514c15":"code","1278bfe6":"code","fefb8529":"code","30d6e65e":"code","6dfff050":"code","3e02c4ec":"code","b1551347":"code","8071efbc":"code","e3f75362":"code","c613c340":"code","4a221edd":"code","ef8febfa":"code","e2c18d76":"code","0b3aadea":"code","f9154af5":"code","e321ea9f":"code","3974a8d6":"code","85d8368a":"code","8119bf0b":"code","07c15945":"code","c7e17cd8":"code","aee68318":"code","0068ec45":"markdown","767a9885":"markdown","7ef9090e":"markdown","048cb94c":"markdown","d02fab1d":"markdown","01b4e79c":"markdown","a2b68aca":"markdown","5bfd4a4b":"markdown","58f90ee5":"markdown","bf09beb6":"markdown","087e2ffd":"markdown","5e07b909":"markdown","a87bbc16":"markdown","faf76cae":"markdown","f380961b":"markdown"},"source":{"6b3de137":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","879d2e42":"import pandas as pd\nimport numpy as np\n\ndef load_data():\n    #load and shuffle training set \n    train_df = pd.read_csv(\"..\/input\/train.csv\")\n    test_df = pd.read_csv(\"..\/input\/test.csv\")\n    \n    print(\"Train shape : \",train_df.shape)\n    print(\"Test shape : \",test_df.shape)\n    list_sentences_train = list(train_df[\"question_text\"].fillna(\"NAN_WORD\").values)\n    list_sentences_test = list(test_df[\"question_text\"].fillna(\"NAN_WORD\").values)\n    \n    return list_sentences_train, list_sentences_test","7cd7c58a":"list_sentences_train, list_sentences_test = load_data()","55b10dad":"from collections import Counter\ntmp = Counter([i for i in pd.read_csv(\"..\/input\/train.csv\")['target'].tolist()])\nprint(\"The percentage of insincere questions in train set is {:.2%} .\".format(tmp[1]\/(tmp[1]+tmp[0])))\n\ntrain_1_prc = tmp[1]\/(tmp[1]+tmp[0])\ndel(tmp)","16f6304e":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline  \n\nsns.set(rc={'figure.figsize':(12,6)})\nsns.distplot([len(i.split()) for i in list_sentences_train+list_sentences_test])\nplt.title('Distribution of the length of the question_text - all examples')","114b5469":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n%matplotlib inline  \n\nsns.set(rc={'figure.figsize':(12,6)})\nsns.distplot(np.array([len(i.split()) for i in list_sentences_train])[np.array([i==1 for i in pd.read_csv(\"..\/input\/train.csv\")['target'].tolist()])])\nplt.title('Distribution of the length of the question_text - only insincere questions')","0b2ecd36":"MAX_SEQUENCE_LENGTH = 70","2d5f329b":"def change_word(text: str,dict_:dict) -> str:\n    \"\"\"\n    Function that replace words base on dictionary.\n    \n    Parameters\n    ----------\n    text : str\n        Input text\n    dict_: dict\n        Dictionary with pairs {pattern: repl,..} ex. {\"'\":\"'\", \"\u2018\":\"'}\n        \n    Returns\n    -------\n    text : processed text \n    \n    Examples\n    --------\n    >>> change_word(text=\"sample ? text\", dict_={\"?\":\"question\", \"\u2018\":\"'\"})\n    'sample question text'\n    \"\"\"\n    for s in dict_.items():\n        text = text.replace(s[0],s[1])\n    return text","5a86cf76":"import re\ndef lower_first_in_sentence(text: str) -> str:\n    \"\"\"\n    Function that change first letter in sentence to lower. \n    \n    Parameters\n    ----------\n    text : str\n        Input text\n        \n    Returns\n    -------\n    text : processed text \n    \n    Examples\n    --------\n    >>> lower_first_in_sentence(\"Matt is smart - claims John. Yes, I think so. Wait.. he is not.\")\n    'matt is smart - claims John. yes, I think so. wait.. he is not.'\n    \"\"\"\n    spl = re.compile('(\\?+ *|\\!+ *|\\.+ *)')\n    def lower_firts(txt): return txt if (len(txt)<2 or txt[0] ==\"I\") else (txt[0].lower() + txt[1:])\n    return ''.join([lower_firts(i) for i in re.split(spl, text)])","c393572c":"import re\ndef strip(word:str ) -> str: \n    \"\"\"\n    Function that removes 's and solo apostrophe ' from end of the word. ex. AI's -> AI \n    \n    Parameters\n    ----------\n    text : str\n        Input text\n        \n    Returns\n    -------\n    text : processed text \n    \n    Examples\n    --------\n    >>> strip(\"exactly AI's VC's Donald'\")\n    'exactly AI VC Donald '\n    \"\"\"\n\n    return re.sub(\"('$ |'$|'s |'s)\",' ',word) if len(word)>2 else word","661f2b5d":"def clean_numbers(x):\n    \"\"\"\n    Function that replaces digits\n    \"\"\"\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x","372b7c67":"def load_embed(file):\n    def get_coefs(word,*arr): \n        return word, np.asarray(arr, dtype='float32')\n    \n    if file == '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec':\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n    else:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n        \n    return embeddings_index","5cc99cfb":"def check_coverage(vocab, embeddings_index):\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words \/ (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n\n    return unknown_words","1d771ddc":"import numpy as np\nfrom tqdm import tqdm\n\ndef concat_embed(first_embed, second_embed, word_index):\n    \"\"\"\n    Function that concat two embeddings and apply rules described here: \n      * concatenate first embedding and second embedding,\n      * first vector is used by itself if there is no second vector but not the other way around. \n      * If there is no first vector for word you looking for, transform word to lowercase and look again, \n      * Words without word vectors are replaced with a word vector for a word \"something\".\n      * Add additional value that was set to 1 if a word was written in all capital letters and 0 otherwise,      \n    \n    Parameters\n    ----------\n    first_embed : dict\n        First embedding\n\n    second_embed : dict\n        Second embedding\n        \n    word_index : dict\n        Dictionary that contains words you are looking for in a form {'ID':word} ex. {1:'the',2:'ok'...} \n        word_index = {t: i+1 for i,t in enumerate(vocab)}\n        \n    Returns\n    -------\n    wv_matrix : array of vectors (shape: nb_words x WV_DIM)\n    WV_DIM : embedding size\n    nb_words : number of words in embedding\n    \n    Examples:\n    tbc\n    --------\n\n    \"\"\"\n\n    WV_DIM=first_embed['I'].shape[0]+second_embed['I'].shape[0]+1\n    nb_words = len(word_index)+1\n\n    wv_matrix = np.zeros(shape=(nb_words, WV_DIM))\n    for word, i in tqdm(word_index.items()):\n        cap_flag = word.isupper() and word!='I'\n        cap = np.where(cap_flag,np.array([1]),np.array([0]))\n        if word in first_embed:\n            if word in second_embed:\n                #print('1')\n                wv_matrix[i] = np.hstack((second_embed[word],first_embed[word],cap))\n            else:\n                wv_matrix[i] = np.hstack((second_embed['something'],first_embed[word],cap))\n        else:\n            if word.lower() in first_embed:\n                if word.lower() in second_embed:\n                    wv_matrix[i] = np.hstack((second_embed[word.lower()],first_embed[word.lower()],cap))\n                else:\n                    wv_matrix[i] = np.hstack((second_embed['something'],first_embed[word.lower()],cap))\n            else:\n                wv_matrix[i] = np.hstack((second_embed['something'],first_embed['something'],cap))\n            \n    return wv_matrix, WV_DIM, nb_words","f749af6d":"import numpy as np\nfrom tqdm import tqdm\n\ndef avg_embed(first_embed, second_embed, word_index):\n    \"\"\"\n    Function that average two embeddings and apply rules described here: \n      * concatenate first embedding and second embedding,\n      * first vector is used by itself if there is no second vector but not the other way around. \n      * If there is no first vector for word you looking for, transform word to lowercase and look again, \n      * Words without word vectors are replaced with a word vector for a word \"something\".\n      * Add additional value that was set to 1 if a word was written in all capital letters and 0 otherwise,      \n    \n    Parameters\n    ----------\n    first_embed : dict\n        First embedding\n\n    second_embed : dict\n        Second embedding\n        \n    word_index : dict\n        Dictionary that contains words you are looking for in a form {'ID':word} ex. {1:'the',2:'ok'...} \n        word_index = {t: i+1 for i,t in enumerate(vocab)}\n        \n    Returns\n    -------\n    wv_matrix : array of vectors (shape: nb_words x WV_DIM)\n    WV_DIM : embedding size\n    nb_words : number of words in embedding\n    \n    Examples:\n    tbc\n    --------\n\n\n    \"\"\"\n    WV_DIM=np.mean([second_embed['I'],first_embed['I']], axis = 0).shape[0]+1\n    nb_words = len(word_index)+1\n\n    wv_matrix = np.zeros(shape=(nb_words, WV_DIM))\n    for word, i in tqdm(word_index.items()):\n        cap_flag = word.isupper() and word!='I'\n        cap = np.where(cap_flag,np.array([1]),np.array([0]))\n        if word in first_embed:\n            if word in second_embed:\n                #print('1')\n                wv_matrix[i] = np.hstack((np.mean([second_embed[word],first_embed[word]], axis = 0),cap))\n            else:\n                wv_matrix[i] = np.hstack((np.mean([second_embed['something'],first_embed[word]], axis = 0),cap))\n        else:\n            if word.lower() in first_embed:\n                if word.lower() in second_embed:\n                    wv_matrix[i] = np.hstack((np.mean([second_embed[word.lower()],first_embed[word.lower()]], axis = 0),cap))\n                else:\n                    wv_matrix[i] = np.hstack((np.mean([second_embed['something'],first_embed[word.lower()]], axis = 0),cap))\n            else:\n                wv_matrix[i] = np.hstack((np.mean([second_embed['something'],first_embed['something']], axis = 0),cap))        \n    return wv_matrix, WV_DIM, nb_words","7767504f":"from multiprocessing import Pool, cpu_count\nprint(\"Number of available cpu cores: {}\".format(cpu_count()))\n\ndef process_in_parallel(function, list_):\n    with Pool(cpu_count()) as p:\n        tmp = p.map(function, list_)\n    return tmp","72f952a5":"import re\nfrom tqdm import tqdm\nfrom collections import Counter, OrderedDict\nimport operator\nimport unicodedata as ud\n\nfrom nltk.tokenize import TweetTokenizer\n\ndef process_questions(list_sentences, dict_, exclude):\n    \"\"\"\n    Function that applies text preprocessing\n    \"\"\"\n    sleep(0.5)\n    print(\"Lower first in sentence\")\n    list_sentences = process_in_parallel(lower_first_in_sentence, list_sentences)\n    #list_sentences = [lower_first_in_sentence(s) for s in tqdm(list_sentences)]\n    \n    print(\"Change words - using dictionary\")\n    list_sentences = [change_word(s,dict_) for s in list_sentences]\n    \n    print(\"Remove 's and solo apostrophe ' from end of the word. ex. AI's -> AI \")\n    list_sentences = process_in_parallel(strip, list_sentences)\n    #list_sentences = [strip(s) for s in tqdm(list_sentences)]\n    \n    print(\"Replace digits with mask ex. 'Marek gets 3333' -> 'Marek gets ####'\")\n    list_sentences = process_in_parallel(clean_numbers,list_sentences)\n    #list_sentences = [clean_numbers(s) for s in tqdm(list_sentences)]\n    \n    print(\"Normalise unicode data to remove umlauts, accents etc.\")\n    #https:\/\/gist.github.com\/j4mie\/557354\n    list_sentences = [ud.normalize('NFKD', i).encode('ASCII', 'ignore') for i in list_sentences]\n    \n    print(\"Use TweetTokenizer() from NLTK for splitting words.\")\n    tokenizer = TweetTokenizer()\n    list_sentences = process_in_parallel(tokenizer.tokenize,list_sentences)\n    #list_sentences = [tokenizer.tokenize(i) for i in tqdm(list_sentences)]\n    \n    print(\"Create vocab\")\n    dictionary = Counter([item for sublist in list_sentences for item in sublist])\n    dictionary = OrderedDict(sorted(dictionary.items(), key=operator.itemgetter(1), reverse = True))\n\n    #exclude = [',','.','\"','(',')','[',']',\"'\",'\u2019',\"\\\\\",'{','}','\u2026','..']                      \n    print(\"Exclude some punctuations: {}\".format(\" \".join(exclude)))\n    list_sentences = [list(filter(lambda x: x not in exclude,i)) for i in list_sentences]\n\n    return list_sentences, dictionary","32514c15":"from time import sleep\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef text_features(list_sentences, MAX_SEQUENCE_LENGTH):\n    \"\"\"\n    Function creates matrix with additional text features.\n    Column 1: Unique words rate\n    Column 2: Rate of all-caps words\n    Column 3: Sentence length rate (number of word \/ max sequence length parameter)\n    \"\"\"\n    print(\"Column 1: Unique words rate\\nColumn 2: Rate of all-caps words\\nColumn 3: Sentence length rate (number of word \/ max sequence length parameter)\")\n    sleep(0.2)\n    #\"Unique words rate\" \n    def uwr(seq): return len(set(seq))\/len(seq) if len(seq)>0 else 0\n    #\"Rate of all-caps words\"\n    def acw(seq): return len(list(filter(lambda x: x.isupper() and x!='I', seq)))\/len(seq) if len(seq)>0 else 0\n    #\"sentence length rate\" \n    def slr(seq): return len(seq)\/MAX_SEQUENCE_LENGTH if len(seq)>0 else 0\n    \n    uwr_f = [uwr(i) for i in tqdm(list_sentences)]\n    acw_f = [acw(i) for i in tqdm(list_sentences)]\n    slr_f = [slr(i) for i in tqdm(list_sentences)]\n    scaler = MinMaxScaler()\n    return scaler.fit_transform(np.array([uwr_f,acw_f,slr_f]).transpose())","1278bfe6":"signs = {\"'\":\"'\", \"\u2018\":\"'\",\"\u00b4\": \"'\", \"\u00b0\": \"\",\"`\": \"'\", '\u201c': '\"', '\u201d': '\"', '\u201c': '\"',\n         \"\u20b9\": \"e\",\"\u20ac\": \"e\", \"\u2122\": \"tm\", \"\u221a\": \" sqrt \", \"\u00d7\": \"x\", \"\u00b2\": \"2\",\"\u2014\": \"-\", \n         \"\u2013\": \"-\", \"\u2019\": \"'\", \"_\": \"-\", \"\u00a3\": \"e\",'\u221e': 'infinity', '\u03b8': 'theta', '\u00f7': '\/', \n         '\u03b1': 'alpha', '\u2022': '.', '\u00e0': 'a', '\u2212': '-','\u03b2': 'beta', '\u2205': '', '\u00b3': '3', '\u03c0': 'pi'}\n\n\nmisspelled = { 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', \n                 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n                 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', \n                 'youtu ': 'youtube ', 'Quorans':'Quora','Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', \n                 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', \n                 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', \n                 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', \n                 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', \n                 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', \n                 '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex-boyfriend', 'airhostess': 'air hostess', \n                 \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', \n                 'demonitization': 'demonetization', 'demonetisation': 'demonetization',\n                  \"\u2019\":\"'\",\"\u2018\":\"'\",\"\u00b4\":\"'\",\"`\":\"'\",'9\/11':'terrorism',\"Quoran\":\"Koran\",'1\/2':'half',\n                  'cryptocurrencies':'cryptocurrency',\"Brexit\":'leave EU', \"Blockchain\":\"blockchain\",'..':''}\n\ncontractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \n                       \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \n                       \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \n                       \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n                       \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n                       \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n                       \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n                       \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n                       \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \n                       \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \n                       \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n                       \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n                       \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n                       \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n                       \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n                       \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n                       \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \n                       \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \n                       \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n                       \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n                       \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n                       \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n                       \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" ,\n                       \"Isn't\":\"is not\", \"\\u200b\":\"\", \"It's\": \"it is\",\"I'm\": \"I am\",\"don't\":\"do not\"}\n\ndict_= {}\ndict_.update(signs)\ndict_.update(misspelled)\ndict_.update(contractions)","fefb8529":"exclude_list = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]","30d6e65e":"#Apply text preprocessing\nsentences, vocab = process_questions(list_sentences_train+list_sentences_test, dict_, exclude_list)","6dfff050":"#Prepare matrix with additional features\nadditional_features = text_features(sentences, MAX_SEQUENCE_LENGTH)","3e02c4ec":"#Prepare embedding\n\nimport gc\nfrom time import sleep\nimport pandas as pd\nfrom keras.preprocessing.sequence import pad_sequences\n\ndef prepare_sequences():\n\n    word_index = {t: i+1 for i,t in enumerate(vocab)}\n    \n    glove = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\n    wiki_news = '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'\n    \n    print(\"Extracting GloVe embedding\")\n    embed_glove = load_embed(glove)\n\n    print(\"Extracting FastText embedding\")\n    embed_fasttext = load_embed(wiki_news)\n\n    print(\"Glove coverage: \")\n    oov_glove = check_coverage(vocab, embed_glove)\n\n    print(\"FastText coverage: \")\n    oov_fasttext = check_coverage(vocab, embed_fasttext)\n\n    print(\"Create embedding for word vocabulary\")\n    sleep(0.2)\n    wv_matrix, WV_DIM, nb_words = concat_embed(embed_glove, embed_fasttext, word_index)\n    \n    #del(vocab)\n    del(globals()['vocab'])\n    gc.collect()\n    del(embed_glove,embed_fasttext)\n    gc.collect()\n    sleep(5)\n\n    print(\"Create sequences\")\n    sleep(0.2)\n    sequences = [[word_index.get(t, 0) for t in sentence]\n                 for sentence in tqdm(sentences[:len(list_sentences_train)])]\n    test_sequences = [[word_index.get(t, 0)  for t in sentence] \n                      for sentence in tqdm(sentences[len(list_sentences_train):])]\n\n    print(\"Assign additional features to train \/ test list\")\n    sleep(0.2)    \n    additional_features_train = additional_features[:len(list_sentences_train),:]\n    additional_features_test = additional_features[len(list_sentences_train):,:]\n\n    del(globals()['list_sentences_train'])\n    del(globals()['list_sentences_test'])\n    \n    print(\"Pad sequences\")\n    sleep(0.2)\n    train_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\", truncating=\"post\")\n    #list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n    train_df = pd.read_csv(\"..\/input\/train.csv\")\n    train_y = train_df['target'].values\n    print('Shape of data tensor:', train_data.shape)\n    print('Shape of label tensor:', train_y.shape)\n\n    test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\",truncating=\"post\")\n    print('Shape of test_data tensor:', test_data.shape)\n\n    gc.collect()\n    sleep(5)\n    return word_index, wv_matrix, WV_DIM, nb_words, train_data, train_y, test_data, additional_features_train, additional_features_test","b1551347":"word_index, wv_matrix, WV_DIM, nb_words, train_data, train_y, test_data, train_data_a, test_data_a = prepare_sequences()","8071efbc":"import gc\nfrom time import sleep\ngc.collect()\nsleep(10)","e3f75362":"embed_size = WV_DIM # how big is each word vector\n#max_features = 95000 # how many unique words to use (i.e num rows in embedding vector)\nnb_words = nb_words #number of unique words\nmaxlen = MAX_SEQUENCE_LENGTH # max number of words in a question to use\n\nbatch_size = 2048\ntrain_epochs = 6\n\nSEED = 666\n\nprint(\"embed_size : {embed_size},\\nnb_words : {nb_words},\\nmaxlen : {maxlen},\\nbatch_size : {batch_size}, \\\n      \\ntrain_epochs : {train_epochs},\\nSEED : {SEED}\".format(\n    **{'embed_size':embed_size, 'nb_words':nb_words,'maxlen':maxlen,'batch_size':batch_size,'train_epochs':train_epochs,'SEED':SEED}))","c613c340":"import torch\nimport torch.nn as nn\nimport torch.utils.data","4a221edd":"import random\nimport os\nimport torch\n\ndef seed_torch(seed=666):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","ef8febfa":"class Attention(nn.Module):\n    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n        \n        self.supports_masking = True\n\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.step_dim = step_dim\n        self.features_dim = 0\n        \n        weight = torch.zeros(feature_dim, 1)\n        nn.init.xavier_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n        \n        if bias:\n            self.b = nn.Parameter(torch.zeros(step_dim))\n        \n    def forward(self, x, mask=None):\n        feature_dim = self.feature_dim\n        step_dim = self.step_dim\n\n        eij = torch.mm(\n            x.contiguous().view(-1, feature_dim), \n            self.weight\n        ).view(-1, step_dim)\n        \n        if self.bias:\n            eij = eij + self.b\n            \n        eij = torch.tanh(eij)\n        a = torch.exp(eij)\n        \n        if mask is not None:\n            a = a * mask\n\n        a = a \/ torch.sum(a, 1, keepdim=True) + 1e-10\n\n        weighted_input = x * torch.unsqueeze(a, -1)\n        return torch.sum(weighted_input, 1)","e2c18d76":"from torch.nn import * \n\nclass NeuralNet(nn.Module):\n    def __init__(self):\n        super(NeuralNet, self).__init__()\n        \n        hidden_size = 64\n        \n        self.embedding = nn.Embedding(nb_words, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(wv_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        \n        self.embedding_dropout = nn.Dropout2d(0.1)\n        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n        self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n        \n        self.lstm_attention = Attention(hidden_size*2, maxlen)\n        self.gru_attention = Attention(hidden_size*2, maxlen)\n    \n        self.AvgPool1d = nn.AdaptiveAvgPool1d(1)\n        self.MaxPool1d = nn.AdaptiveMaxPool1d(1)\n\n        self.linear = nn.Linear(399, 192)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.1)\n        self.linear2 = nn.Linear(192, 64)\n        self.out = nn.Linear(64, 1)\n        self.out_act = nn.Sigmoid()\n    \n    def forward(self, x, x_a):\n        h_embedding = self.embedding(x)\n        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n        \n        h_lstm, _ = self.lstm(h_embedding)\n        h_gru, _ = self.gru(h_lstm)\n        \n        h_lstm_atten = self.lstm_attention(h_lstm)\n        h_gru_atten = self.gru_attention(h_gru)\n        \n        avg_pool = torch.squeeze(self.AvgPool1d(h_gru))\n        max_pool = torch.squeeze(self.MaxPool1d(h_gru))\n        #avg_pool = torch.mean(h_gru, 1)\n        #max_pool, _ = torch.max(h_gru, 1)\n        #print(h_gru_atten.shape)\n        #print(avg_pool.shape)\n        #print(max_pool.shape)\n        #print(x_a.shape)\n\n        conc = torch.cat((h_gru_atten,h_lstm_atten, avg_pool, max_pool, x_a), 1)\n        #print(conc.shape)\n        conc = self.relu(self.linear(conc))\n        conc = self.dropout(conc)\n        conc = self.relu(self.linear2(conc))\n        conc = self.dropout(conc)       \n        out = self.out(conc)\n        y = self.out_act(out)\n        return y","0b3aadea":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))","f9154af5":"from sklearn.metrics import f1_score, roc_auc_score\n\ndef threshold_search(y_true, y_proba):\n    best_threshold = 0\n    best_score = 0\n    for threshold in tqdm([i\/100 for i in range(10,90)]):\n        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n    search_result = {'threshold': best_threshold, 'f1': best_score}\n    return search_result","e321ea9f":"from sklearn.model_selection import GridSearchCV, StratifiedKFold\nsplits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED).split(train_data, train_y))","3974a8d6":"import warnings\nwarnings.filterwarnings('always')\n\nimport time\n\ntrain_preds = np.zeros((len(train_data)))\ntest_preds = np.zeros((len(test_data)))\n\nseed_torch(SEED)\n\nx_test_cuda = torch.tensor(test_data, dtype=torch.long).cuda()\nx_test_a_cuda = torch.tensor(test_data_a, dtype=torch.float32).cuda()\n\ntest_dataset = torch.utils.data.TensorDataset(x_test_cuda,x_test_a_cuda)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nloss_kFold = []\n\nfor i, (train_idx, valid_idx) in enumerate(splits):\n    x_train_fold = torch.tensor(train_data[train_idx], dtype=torch.long).cuda()\n    x_train_a_fold = torch.tensor(train_data_a[train_idx], dtype=torch.float32).cuda()\n    y_train_fold = torch.tensor(train_y[train_idx, np.newaxis], dtype=torch.float32).cuda()\n    x_val_fold = torch.tensor(train_data[valid_idx], dtype=torch.long).cuda()\n    x_val_a_fold = torch.tensor(train_data_a[valid_idx], dtype=torch.float32).cuda()\n    y_val_fold = torch.tensor(train_y[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n    \n    model = NeuralNet()\n    model.cuda()\n    \n    loss_fn = torch.nn.BCELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n    \n    train = torch.utils.data.TensorDataset(x_train_fold,x_train_a_fold, y_train_fold)\n    valid = torch.utils.data.TensorDataset(x_val_fold, x_val_a_fold,y_val_fold)\n    \n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n    \n    print(f'Fold {i + 1}')\n    loss_epoch = []\n    for epoch in range(train_epochs):\n        start_time = time.time()\n        \n        model.train()\n        avg_loss = 0.\n        losses = []\n        for x_batch,x_a_batch, y_batch in tqdm(train_loader, disable=True):\n            \n            optimizer.zero_grad()\n            # (1) Forward\n            y_pred = model(x_batch,x_a_batch)\n            # (2) Compute diff\n            loss = loss_fn(y_pred, y_batch)\n            # (3) Compute gradients\n            loss.backward()\n            # (4) update weights\n            optimizer.step()\n            avg_loss += loss.item() \/ len(train_loader)\n            losses.append(loss.data.cpu().numpy())\n        \n        loss_epoch.append(losses)\n        \n        model.eval()\n        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n        test_preds_fold = np.zeros(len(test_data))\n        avg_val_loss = 0.\n        for i, (x_batch, x_2_batch, y_batch) in enumerate(valid_loader):\n            y_pred = model(x_batch,x_2_batch).detach()\n            avg_val_loss += loss_fn(y_pred, y_batch).item() \/ len(valid_loader)\n            valid_preds_fold[i * batch_size:(i+1) * batch_size] = y_pred.cpu().numpy()[:, 0]\n        \n        elapsed_time = time.time() - start_time \n        print('Epoch {}\/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n            epoch + 1, train_epochs, avg_loss, avg_val_loss, elapsed_time))\n        \n    for i, (x_batch,x_a_batch) in enumerate(test_loader):\n        y_pred = model(x_batch,x_a_batch).detach()\n        test_preds_fold[i * batch_size:(i+1) * batch_size] = y_pred.cpu().numpy()[:, 0]\n\n    train_preds[valid_idx] = valid_preds_fold\n    print(threshold_search(train_y[valid_idx], train_preds[valid_idx]))\n    \n    test_preds += test_preds_fold \/ len(splits)  \n    \n    loss_kFold.append(loss_epoch)","85d8368a":"search_result = threshold_search(train_y, train_preds)\nsearch_result","8119bf0b":"test_df = pd.read_csv(\"..\/input\/test.csv\")\nsub = pd.DataFrame({\"qid\": test_df[\"qid\"].values})\nsub['prediction'] = (test_preds > search_result['threshold']).astype(int)\nsub.to_csv(\"submission.csv\", index=False)","07c15945":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline  \n\nsns.set(rc={'figure.figsize':(12,6)})\nsns.distplot(test_preds)\nplt.title('Distribution of the test predictions probability')","c7e17cd8":"from collections import Counter\nprint(\"The percentage of insincere questions in train set is {:.2%} .\".format(train_1_prc))","aee68318":"test_pred_1_prc = Counter(sub[\"prediction\"])[1]\/len(sub[\"prediction\"])\nprint(\"The percentage of predicted insincere questions in test set is {:.2%} .\".format(test_pred_1_prc))","0068ec45":"### Choose MAX_SEQUENCE_LENGTH","767a9885":"### Define pseudo functions","7ef9090e":"### References:    \n* [@Dieter](http:\/\/https:\/\/www.kaggle.com\/christofhenkel) -  [How to: Preprocessing when using embeddings](https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-when-using-embeddings)\n* [@Theo Viel](https:\/\/www.kaggle.com\/theoviel) -  [Improve your score with some text preprocessing](https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-some-text-preprocessing)\n* [@Alexander Burmistrov](https:\/\/www.kaggle.com\/mrboor) -  [Toxic Comment Classification Challenge 3rd place](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/discussion\/52644)    \n* [@Hung The Nguyen](https:\/\/www.kaggle.com\/hung96ad) - [Pytorch starter](https:\/\/www.kaggle.com\/hung96ad\/pytorch-starter)","048cb94c":"### Quick summary:\n**1. Text preprocessing**:\n  \n  * replace words\/characters based on dictionary ex. don't -> do not, Brexit -> leave EU (*@Dieter, @Theo Viel*)\n  * change first letter in sentence to lower (size of first letter do matter in pretrained embedding, it's better to lower only the first letter in sentence instead of all of the words in text),   \n  * remove apostrophe and 's ending form word. ex. AI's -> AI (there isn't \"AI's\" vector in pretrained glove but there is \"AI\" vector),\n  * replace digits with mask, ex. 'Marek gets 3333' -> 'Marek gets ####'\"\n  * use TweetTokenizer() from NLTK for splitting words - I think it's the best tokenizer available for unformal text,   \n  * normalise unicode data to remove umlauts, accents etc.\n  * padding=\"pre\", truncating=\"post\"\n    \n**2. Text additional features (apply MinMaxScaler)**:\n  * unique words rate,\n  * rate of all-caps words,\n  * sentence length rate (number of word \/ max sequence length parameter),     \n  \n** 3. Embeddings:**      \n*ref Alexander Burmistrov*\n  * concatenated fasttext and glove twitter embeddings (I've prepare similar function for average embegings too),\n  * Glove vector is used by itself if there is no Fasttext vector but not the other way around. \n  * If there is no vector for word I'm looking for, transform word to lowercase and look again, \n  * Words without word vectors are replaced with a word vector for a word \"something\".\n  * Added additional value that was set to 1 if a word was written in all capital letters and 0 otherwise,      \n\n** 4. Architecture  -> PyTorch**    \n*ref Alexander Burmistrov, Hung The Nguyen :*\n\n1) Concatenated fasttext and glove twitter embeddings.    \n2) SpatialDropout1D(0.1)   \n3) First Layer of RNN: Bidirectional LSTM with a kernel size 64 -> LSTM    \n4) Attention of LSTM layer -> LSTM_Atten    \n5) Second layer of RNN: Bidirectional GRU with a kernel size 128 -> GRU    \n6) Attention of GRU layer -> GRU_Atten    \n7) A concatenation of the: [LSTM, LSTM_Atten, GRU, GRU_Atten, Text additional features]    \n8)  Dense layers:    Dense(192, relu, Dropout 0,1) -> Dense(64, relu, Dropout 0,1) -> Dense(1, sigmoid).      \n\n**Loss:** Binary Cross Entropy    \n**Optimizer:** Adam,","d02fab1d":"Define dictionaries that maps words and characters to change","01b4e79c":"### Load data","a2b68aca":"### Build model","5bfd4a4b":"### Train model and predict","58f90ee5":"https:\/\/www.kaggle.com\/hung96ad\/pytorch-starter","bf09beb6":"### Define functions ","087e2ffd":"# Preface    \n\n**Text preprocessing and embeddings** - I'm using some of the methods that are not presented in different notebooks at the point of time.   \nFor example: lower only first letter in sentence, TweetTokenizer, normalise unicode data, few-steps embedding concatenation.\n\n**Model** - as the base I use the Hung The Nguyen PyTorch model - [Pytorch starter](https:\/\/www.kaggle.com\/hung96ad\/pytorch-starter). I adapt it to the concept described by Alexander Burmistrov  [Toxic Comment Classification Challenge 3rd place](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/discussion\/52644) .\n","5e07b909":"# Notebook","a87bbc16":"### What is the percentage of insincere questions in training set?","faf76cae":"I think that max length equal 70 words will be good enough.","f380961b":"## Let's get work started    \n    \n### Prepare sequences and additional fetures\n"}}