{"cell_type":{"b62c5443":"code","5fb998bb":"code","6465e5e9":"code","fe7e7b2f":"code","914b295b":"code","5b908345":"code","8025c0b9":"code","e6809750":"code","73457fff":"code","b6265c4e":"code","2e082008":"code","8a08c175":"code","56a2a0f2":"code","a797b97e":"code","b7cc0e50":"code","eac921c3":"code","99d2a379":"code","1a6424d7":"code","9aecc28d":"code","7774b145":"markdown","d9bf82cf":"markdown","763a7e92":"markdown","0587c7fc":"markdown","1b378b8c":"markdown","a7f41968":"markdown","da64f387":"markdown"},"source":{"b62c5443":"import pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf.head()","5fb998bb":"df.info()","6465e5e9":"#Plot count of outcome variable\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nsns.countplot(x = \"Outcome\", data =df)","fe7e7b2f":"#plot pregnancies\n\nsns.countplot(x = \"Pregnancies\", data =df)","914b295b":"#plotting correlation\nplt.figure(figsize = (12,6))  #figsize is made (12,6) so that there is no congestion\/overlap of numbers\nsns.heatmap(df.corr(), annot = True) #annot true because we want the numbers on plots","5b908345":"df.Outcome.value_counts()","8025c0b9":"zero  = df[df['Outcome']==0]   #zero values in outcome column\none = df[df['Outcome']==1]  # one values in outcome column\nfrom sklearn.utils import resample\n#minority class that 1, we need to upsample\/increase that class so that there is no bias\n#n_samples = 500 means we want 500 sample of class 1, since there are 500 samples of class 0\ndf_minority_upsampled = resample(one, replace = True, n_samples = 500) \n#concatenate\ndf = pd.concat([zero, df_minority_upsampled])\n\nfrom sklearn.utils import shuffle\ndf = shuffle(df) # shuffling so that there is particular sequence","e6809750":"df.corr().abs()['Outcome'].sort_values(ascending = False)","73457fff":"X = df.drop(['Outcome'], axis = 1)\ny = df['Outcome']","b6265c4e":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nfeatures= X.columns\nX[features] = sc.fit_transform(X[features])","2e082008":"#all imports\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n","8a08c175":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1)","56a2a0f2":"lr = LogisticRegression(random_state=42)\n\nknn = KNeighborsClassifier()\n\ndt = DecisionTreeClassifier()\n\nrf = RandomForestClassifier()\n\ncb = CatBoostClassifier(loss_function='Logloss', verbose = 0)\n\npara_knn = {'n_neighbors':np.arange(2, 50)}  #parameters of knn\ngrid_knn = GridSearchCV(knn, param_grid=para_knn, cv=5) #grid search knn for 5 fold cross validation\n\n\n#parameters for decision tree\npara_dt = {'criterion':['gini','entropy'],'max_depth':np.arange(1, 50), 'min_samples_leaf':[1,2,4,5,10,20,30,40,80,100]}\ngrid_dt = GridSearchCV(dt, param_grid=para_dt, cv=5) #grid search decision tree for 5 fold cv\n#\"gini\" for the Gini impurity and \u201centropy\u201d for the information gain.\n#min_samples_leaf: The minimum number of samples required to be at a leaf node, have the effect of smoothing the model\n\n#parameters for random forest\n#n_estimators: The number of trees in the forest.\nparams_rf = {'n_estimators':[100, 350, 500], 'min_samples_leaf':[2, 10, 30]}\ngrid_rf = GridSearchCV(rf, param_grid=params_rf, cv=5)\n\n\nparams_cb = {'learning_rate': [0.03, 0.1], 'depth': [4, 6, 10], 'l2_leaf_reg': [1, 3, 5, 7, 9]}\nrs_cb = RandomizedSearchCV(cb, param_distributions=params_cb, n_iter=5, scoring='roc_auc', n_jobs=4, cv=3)","a797b97e":"#getting the best parametrs\ngrid_knn.fit(X_train, y_train)\ngrid_dt.fit(X_train, y_train)\ngrid_rf.fit(X_train, y_train)\nrs_cb.fit(X_train, y_train) \n\n\nprint(\"Best parameters for KNN:\", grid_knn.best_params_)\nprint(\"Best parameters for Decision Tree:\", grid_dt.best_params_)\nprint(\"Best parameters for Random Forest:\", grid_rf.best_params_)\nprint(\"Best parameters for CatBoost:\", rs_cb.best_params_)","b7cc0e50":"dt = DecisionTreeClassifier(criterion='entropy', max_depth=28, min_samples_leaf=1, random_state=42)\nknn = KNeighborsClassifier(n_neighbors=3)\nrf = RandomForestClassifier(n_estimators=350, min_samples_leaf=2, random_state=42)\ncb = CatBoostClassifier(learning_rate = 0.03, l2_leaf_reg = 7, depth = 10, loss_function = 'Logloss', verbose = 0)","eac921c3":"classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn),\n               ('Decision Tree', dt), ('Random Forest', rf), ('CatBoost', cb)]\n\nfor classifier_name, classifier in classifiers:\n \n    # Fit clf to the training set\n    classifier.fit(X_train, y_train)    \n   \n    # Predict y_pred\n    y_pred = classifier.predict(X_test)\n    accuracy = accuracy_score(y_test,y_pred)\n    \n\n   \n    # Evaluate clf's accuracy on the test set\n    print('{:s} : {:.1f}'.format(classifier_name, accuracy))","99d2a379":"from sklearn.metrics import classification_report\n\ny_pred_cb= cb.predict(X_test)\nprint(classification_report(y_test, y_pred_cb))","1a6424d7":"#XGBoost\nfrom xgboost import XGBClassifier\nmodel= XGBClassifier(n_estimators = 1000,learning_rate = 0.06,max_depth=29,\n                     max_leaves = 31,eval_metric = 'logloss', use_label_encoder = False,\n                     verbosity = 0)\nmodel.fit(X_train,y_train)","9aecc28d":"pred_xgb = model.predict(X_test)\naccuracy_score(y_test, pred_xgb)","7774b145":"## Upvote if you like it or fork it :)","d9bf82cf":"This is an imbalanced class, i.e for 1 (diabetes positive) there are only 268 values, whereas for 0 there are 500 values. So a balance is required or else the prediction will be biased towards 0 value.","763a7e92":"![](http:\/\/res.cloudinary.com\/grohealth\/image\/upload\/v1581692228\/DCUK\/Content\/iStock-9217203841.jpg)","0587c7fc":"No Null values!\nShape is (768,9)","1b378b8c":"## Catboost is the best performing model","a7f41968":"#### Age and pregnancies have strong correlation of about 0.54\n#### Outcome and glucose have a pretty good correlation, 0.47\n#### Insulin and skin thickness have a strong correlation of 0.44","da64f387":"Catboost performed better than XGBoost"}}