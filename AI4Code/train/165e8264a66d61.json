{"cell_type":{"8ebaddef":"code","86b3af0c":"code","edf6ea96":"code","4dd499ca":"code","56a4a253":"code","447cd718":"code","83333b37":"code","93409656":"code","a736af73":"code","bdf59704":"code","27b64a4b":"code","49f3d5ff":"code","562485ac":"code","22dfadbc":"code","29b71d4e":"code","7eb886b0":"code","3e4a4cdd":"code","c8c388ad":"code","00b75994":"code","2fa6fff2":"code","b792f0a2":"code","3aeb7960":"code","7a97a5a1":"code","e6d04ebe":"code","d7675686":"code","c7d0fb26":"markdown","9d95c53b":"markdown","04699de0":"markdown","ea4dd8b7":"markdown","38c31c7b":"markdown","d6198f80":"markdown","b511403d":"markdown","882f9378":"markdown","3c09767f":"markdown","5b950427":"markdown","c47d6a40":"markdown","29be8096":"markdown","add091f5":"markdown","47575fb8":"markdown","fa320600":"markdown","50d3b892":"markdown","4527e825":"markdown","ce316d67":"markdown","180192b2":"markdown","9a826489":"markdown","37cc5efc":"markdown","4d09d9bf":"markdown","6547f311":"markdown","7d99f57f":"markdown","3bae530d":"markdown","99be3d86":"markdown","6ea9e535":"markdown","e80825bd":"markdown"},"source":{"8ebaddef":"import numpy as np\nimport pandas as pd\nimport cv2\nimport json\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt","86b3af0c":"Input = '..\/input\/pku-autonomous-driving\/'\nfile_list = os.listdir(Input)\nprint(file_list)","edf6ea96":"train = pd.read_csv(Input+'train.csv')\ntrain.head()","4dd499ca":"print('Annotations for Image ',train['ImageId'][0],' are ',train['PredictionString'][0])","56a4a253":"annotations = train['PredictionString'][0].split(' ')\nprint(len(annotations) \/ 7)","447cd718":"\nposes = []\n\nfor index in range(0,int(len(annotations)\/7)):\n    i = index*7\n    poses.append(annotations[i:i+7])","83333b37":"print(poses)","93409656":"#Read camera parameters \nfx = 2304.5479\nfy = 2305.8757\ncx = 1686.2379\ncy = 1354.9849\ncamera_matrix = np.array([[fx, 0,  cx],\n           [0, fy, cy],\n           [0, 0, 1]], dtype=np.float32)","a736af73":"def convert2camera(worldCoord):\n    x,y,z = worldCoord[0],worldCoord[1],worldCoord[2]\n    return x * fx \/ z + cx, y * fy \/ z + cy","bdf59704":"# Plot inline\n%matplotlib inline\n\nx = []\ny = []\nimg = cv2.imread(Input+'train_images\/ID_8a6e65317.jpg')\n\nfor item in poses:\n    coord = (float(item[4]),float(item[5]),float(item[6]))\n    x.append(convert2camera(coord)[0])\n    y.append(convert2camera(coord)[1])\n   \nplt.scatter(x,y, color='red', s=100);\n\n\nplt.imshow(img)\nplt.title('2D visualisation')\nplt.show()\n   ","27b64a4b":"from math import sin, cos\n\n# convert euler angle to rotation matrix\ndef euler_to_Rot(yaw, pitch, roll):\n    Y = np.array([[cos(yaw), 0, sin(yaw)],\n                  [0, 1, 0],\n                  [-sin(yaw), 0, cos(yaw)]])\n    P = np.array([[1, 0, 0],\n                  [0, cos(pitch), -sin(pitch)],\n                  [0, sin(pitch), cos(pitch)]])\n    R = np.array([[cos(roll), -sin(roll), 0],\n                  [sin(roll), cos(roll), 0],\n                  [0, 0, 1]])\n    return np.dot(Y, np.dot(P, R))","49f3d5ff":"def draw_line(image, points):\n    color = (255, 0, 0)\n    cv2.line(image, tuple(points[0][:2]), tuple(points[3][:2]), color, 16)\n    cv2.line(image, tuple(points[0][:2]), tuple(points[1][:2]), color, 16)\n    cv2.line(image, tuple(points[1][:2]), tuple(points[2][:2]), color, 16)\n    cv2.line(image, tuple(points[2][:2]), tuple(points[3][:2]), color, 16)\n    return image\n\n\ndef draw_points(image, points):\n    for (p_x, p_y, p_z) in points:\n        cv2.circle(image, (p_x, p_y), int(1000 \/ p_z), (0, 255, 0), -1)\n    return image","562485ac":"def visualize(img, coords):\n    #Just the mean value of all type of car's dimension \n    x_l = 1.02\n    y_l = 0.80\n    z_l = 2.31\n    \n    img = img.copy()\n    for point in coords:\n        # Get values\n        x, y, z = float(point[4]), float(point[5]), float(point[6])\n        yaw, pitch, roll = -float(point[1]), -float(point[2]), -float(point[3])\n        # Math\n        Rt = np.eye(4)\n        t = np.array([x, y, z])\n        Rt[:3, 3] = t\n        Rt[:3, :3] = euler_to_Rot(yaw, pitch, roll).T\n        Rt = Rt[:3, :]\n        P = np.array([[x_l, -y_l, -z_l, 1],\n                      [x_l, -y_l, z_l, 1],\n                      [-x_l, -y_l, z_l, 1],\n                      [-x_l, -y_l, -z_l, 1],\n                      [0, 0, 0, 1]]).T\n        img_cor_points = np.dot(camera_matrix, np.dot(Rt, P))\n        img_cor_points = img_cor_points.T\n        img_cor_points[:, 0] \/= img_cor_points[:, 2]\n        img_cor_points[:, 1] \/= img_cor_points[:, 2]\n        img_cor_points = img_cor_points.astype(int)\n        # Drawing\n        img = draw_line(img, img_cor_points)\n        img = draw_points(img, img_cor_points[-1:])\n    \n    return img","22dfadbc":"img = cv2.imread(Input+'train_images\/ID_8a6e65317.jpg')\n\nbbox = visualize(img,poses)\n\nplt.imshow(bbox)\nplt.title('3D visualisation')\nplt.show()","29b71d4e":"#As we can see there're some inaccuracies in boxes","7eb886b0":"json_files = os.listdir(Input+'car_models_json')\nprint(len(json_files))","3e4a4cdd":"#Load the model\nwith open(Input+'car_models_json\/fengtian-SUV-gai.json') as json_file:\n    car_model_data = json.load(json_file)\n","c8c388ad":"for keys in enumerate(car_model_data):\n    print(keys)","00b75994":"from mpl_toolkits.mplot3d import Axes3D\n","2fa6fff2":"def plot_3d_car(model_json_file):\n    with open(Input+f'car_models_json\/{model_json_file}') as json_file:\n        car_model_data = json.load(json_file)\n\n    vertices = np.array(car_model_data['vertices'])\n    faces = np.array(car_model_data['faces']) - 1\n    car_type = car_model_data['car_type']\n    x, y, z = vertices[:,0], vertices[:,2], -vertices[:,1]\n    fig = plt.figure(figsize=(30, 10))\n    ax = plt.axes(projection='3d')\n    ax.plot_trisurf(x, y, faces, z,\n                    cmap='viridis', edgecolor='none')\n    ax.set_title(car_type)\n    ax.view_init(30, 0)\n    plt.show()\n    fig = plt.figure(figsize=(30, 10))\n    ax = plt.axes(projection='3d')\n    ax.plot_trisurf(x, y, faces, z,\n                    cmap='viridis', edgecolor='none')\n    ax.set_title(car_type)\n    ax.view_init(60, 0)\n    plt.show()\n    fig = plt.figure(figsize=(30, 10))\n    ax = plt.axes(projection='3d')\n    ax.plot_trisurf(x, y, faces, z,\n                    cmap='viridis', edgecolor='none')\n    ax.set_title(car_type)\n    ax.view_init(-20, 180)\n    plt.show()\n    return","b792f0a2":"plot_3d_car('fengtian-SUV-gai.json')","3aeb7960":"mask_files = os.listdir(Input+'train_masks')\nprint('Number of mask images : ',len(mask_files))\ntrain_images = os.listdir(Input+'train_images')\nprint('Number of train images : ',len(train_images))","7a97a5a1":"mask = cv2.imread(Input+'train_masks\/ID_8a6e65317.jpg')\nimage = cv2.imread(Input+'train_images\/ID_8a6e65317.jpg')","e6d04ebe":"plt.imshow(mask)\nplt.title('Mask')\nplt.show()\n\nplt.imshow(image)\nplt.title('Image')\nplt.show()","d7675686":"fig, ax = plt.subplots(figsize=(20, 25))\nplt.imshow(image)\nplt.imshow(mask, cmap=plt.cm.viridis, interpolation='none', alpha=0.5)\nplt.show()","c7d0fb26":"Import Modules","9d95c53b":"As per the data: \n    The primary data is images of cars and related pose information. The pose information is formatted as strings, as follows: **model type, yaw, pitch, roll, x, y, z**\n\n","04699de0":"####  Annotations for cars","ea4dd8b7":"# Masks","38c31c7b":"From Documentation :                                                                                                        \n**Some cars in the images are not of interest (too far away, etc.). Binary masks are provided to allow competitors to remove them from consideration.**","d6198f80":"## Let us load and analyse the data","b511403d":"### Plot the 3D model \nFrom https:\/\/www.kaggle.com\/robikscube\/autonomous-driving-introduction-data-review#Training-Set,-First-Car-Stats","882f9378":"# 3D car model json files","3c09767f":"### Visualise the x,y annotations on Image ID_8a6e65317  ","5b950427":"#### Conversion to camera coordinate","c47d6a40":"### How many models  do we have?","29be8096":"### Load the masks","add091f5":"   #### 2D Visualisation","47575fb8":"Adapted version of code from https:\/\/www.kaggle.com\/zstusnoopy\/visualize-the-location-and-3d-bounding-box-of-car","fa320600":"The File contains the car type, vertices and faces.","50d3b892":"#### Seperate the list of annotations to each car's pose","4527e825":"### What is Prediction String? ","ce316d67":"#### How many cars are there in Image ID_8a6e65317?","180192b2":"**There are different number of masks and images**","9a826489":"#### 3D Visualisation","37cc5efc":"# PKU Autonomous Driving Data Understanding and EDA","4d09d9bf":"List File folders","6547f311":"#### Read mask image","7d99f57f":"Load Train Dataset","3bae530d":"### Which cars are out of scope?","99be3d86":"### Lets examine one model","6ea9e535":"The points are in World Coordinates. They have to be converted to camera coordinate for visualisation","e80825bd":"The cars shown inside white boxes are out of scope"}}