{"cell_type":{"bc310a5b":"code","718edb37":"code","bede3f4f":"code","139a41fc":"code","ab1a5dbe":"code","a20bb480":"code","612c1c85":"code","690eabc5":"code","7e93c5d9":"code","5019a95f":"code","c49e1001":"code","70e5967f":"code","f8a87667":"code","5f6917a4":"code","ef6a9221":"code","82651465":"code","91f095f0":"code","7c61f574":"code","d28fd0ee":"code","49d10789":"code","0252269d":"code","cbefb4f7":"markdown","259530b9":"markdown","316936fe":"markdown","bceac1f7":"markdown","08efb346":"markdown","472d10e3":"markdown","b25d290a":"markdown","81254ef3":"markdown","4396308d":"markdown","57d7ebae":"markdown","0149a275":"markdown","0b31e0c5":"markdown","ce9d6af6":"markdown","2be0e0b3":"markdown","aec01bfe":"markdown","b80de82b":"markdown","c59dab32":"markdown","0fb1f3b7":"markdown"},"source":{"bc310a5b":"#Import the necessary libraries\n\nimport os\nimport gc\nimport numpy as np\nimport pandas as pd\n#tensorflow imports\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tokenizers\n#Get the BERT text tokenizer and associated model for tensorflow\nfrom transformers import BertTokenizer, BertConfig, TFBertModel\n#tqdm to show progress throughout iterations\nfrom tqdm import tqdm\n#regex library\nimport re\n\n#Allow support for loading bars in Pandas - this is just helpful\ntqdm.pandas()","718edb37":"#Competition data inside the Kaggle kernel is located inside tweet-sentiment-extraction\n#Global variables in python are capitalized\nDATA = \"\/kaggle\/input\/tweet-sentiment-extraction\/\"\n#load training set\ntrain = pd.read_csv(DATA+'train.csv')\n#load testing set\ntest = pd.read_csv(DATA+'test.csv')\n#load sample submission to get the format for the final data submission\nsubmission = pd.read_csv(DATA+'sample_submission.csv')","bede3f4f":"class config:\n    #Max length of a tweet is 128\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 64\n    VALID_BATCH_SIZE = 32\n    TEST_BATCH_SIZE = 32\n    EPOCHS = 10\n    #Add the location of the model's UNCAPTIALIZED configuration\n    BERT_CONFIG = '\/kaggle\/input\/bertconfig\/bert-base-uncased-config.json'\n    BERT_PATH = \"\/kaggle\/input\/bert-base-uncased-huggingface-transformer\/\"\n    TOKENIZER = tokenizers.BertWordPieceTokenizer(\"\/kaggle\/input\/bert-base-uncased-huggingface-transformer\/\/bert-base-uncased-vocab.txt\", \n        lowercase=True)\n    SAVEMODEL_PATH = '\/kaggle\/input\/tftweetfinetuned\/finetuned_bert.h5'\n    THRESHOLD = 0.4","139a41fc":"def process_data(tweet, selected_text, tokenizer):\n    len_st = len(selected_text)\n    idx0 = None\n    idx1 = None\n    \n    #Go through the tweet and its selected text and see where the common words exist\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n        if tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st\n            break\n    \n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1):\n            char_targets[ct] = 1\n    #Tokenize the string\n    tok_tweet = tokenizer.encode(tweet)\n    input_ids_orig = tok_tweet.ids\n    tweet_offsets = tok_tweet.offsets\n\n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n    #Find target words and return them\n    targets = [0] * len(input_ids_orig)\n    for idx in target_idx:\n        targets[idx] = 1\n    return targets","ab1a5dbe":"def cleanText(tweet):\n    #list of emoji patterns appearing in the tweets to be removed\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = str(tweet)\n    # Remove emojis\n    text = emoji_pattern.sub(r'', text)\n    # Remove twitter handles (@___)\n    text = re.sub(r'@\\w+', '', text)\n    # Remove links after research that t.co uses http still\n    text = re.sub(r'http.?:\/\/[^\/s]+[\/s]?', '', text)\n    return text.strip().lower()","a20bb480":"train['text'] = train['text'].apply(lambda x: cleanText(x))","612c1c85":"train['targets'] = train.progress_apply(lambda row: process_data(   str(row['text']), \n                                                                    str(row['selected_text']),\n                                                                    config.TOKENIZER),\n                                                                    axis=1)","690eabc5":"train['targets'] = train['targets'].apply(lambda x :x + [0] * (config.MAX_LEN-len(x)))","7e93c5d9":"def convert_to_transformer_inputs(text, tokenizer, max_sequence_length):\n    inputs = tokenizer.encode(text)\n    input_ids =  inputs.ids\n    input_masks = inputs.attention_mask\n    input_segments = inputs.type_ids\n    padding_length = max_sequence_length - len(input_ids)\n    padding_id = 0\n    input_ids = input_ids + ([padding_id] * padding_length)\n    input_masks = input_masks + ([0] * padding_length)\n    input_segments = input_segments + ([0] * padding_length)\n    return [input_ids, input_masks, input_segments]","5019a95f":"def compute_input_arrays(df, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df.iterrows()):\n        ids, masks, segments= convert_to_transformer_inputs(str(instance.text),tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]","c49e1001":"def compute_output_arrays(df, columns):\n    return np.asarray(df[columns].values.tolist())","70e5967f":"outputs = compute_output_arrays(train,'targets')\ninputs = compute_input_arrays(train, config.TOKENIZER, config.MAX_LEN)\ntest_inputs = compute_input_arrays(test, config.TOKENIZER, config.MAX_LEN)","f8a87667":"def create_model():\n    ids = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n    mask = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n    attn = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n    bert_conf = BertConfig() \n    bert_model = TFBertModel.from_pretrained(config.BERT_PATH+'\/bert-base-uncased-tf_model.h5', config=bert_conf)\n    \n    output = bert_model(ids, attention_mask=mask, token_type_ids=attn)\n    \n    out = tf.keras.layers.Dropout(0.1)(output[0]) \n    out = tf.keras.layers.Conv1D(1,1)(out)\n    out = tf.keras.layers.Flatten()(out)\n    out = tf.keras.layers.Activation('sigmoid')(out)\n    model = tf.keras.models.Model(inputs=[ids, mask, attn], outputs=out)\n    return model","5f6917a4":"K.clear_session()\nmodel = create_model()\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer)","ef6a9221":"if not os.path.exists(config.SAVEMODEL_PATH):\n    model.fit(inputs,outputs, epochs=config.EPOCHS, batch_size=config.TRAIN_BATCH_SIZE)\n    model.save_weights(f'finetuned_bert.h5')\nelse:\n    model.load_weights(config.SAVEMODEL_PATH)","82651465":"predictions = model.predict(test_inputs, batch_size=32, verbose=1)\nthreshold = config.THRESHOLD\npred = np.where(predictions>threshold, 1,0)","91f095f0":"def decode_tweet(original_tweet,idx_start,idx_end,offsets):\n    filtered_output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n    return filtered_output","7c61f574":"outputs = []\nfor test_idx in range(test.shape[0]):\n    indexes = list(np.where(pred[test_idx]==1)[0])\n    text = str(test.loc[test_idx,'text'])\n    encoded_text = config.TOKENIZER.encode(text)\n    if len(indexes)>0:\n        start = indexes[0]\n        end =  indexes[-1]\n    else:  #if nothing was found above threshold value\n        start = 0\n        end = len(encoded_text.ids) - 1\n    if end >= len(encoded_text.ids):\n        end = len(encoded_text.ids) - 1\n    if start>end: \n        selected_text = test.loc[test_idx,'text']\n    else:\n        selected_text = decode_tweet(text,start,end,encoded_text.offsets)\n    outputs.append(selected_text)\n    \ntest['selected_text'] = outputs","d28fd0ee":"def replacer(row):\n    if row['sentiment'] == 'neutral' or len(row['text'].split())<2:\n        return row['text']\n    else:\n        return row['selected_text']\ntest['selected_text'] = test.apply(replacer,axis=1)","49d10789":"submission['selected_text'] = test['selected_text']\nsubmission.to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 80)","0252269d":"submission.head()","cbefb4f7":"Train the model with keras. We are using the Binary cross entropy as the loss function as our classes (positive\/negative) are such polar opposites that it would be a good fit. Neutral class in general gets the whole message sent back so it's not an issue for us. Furthermore, other loss functions like Categorical Cross entropy tend to have extremely high loss function in testing, so we went with this.\nCalculating the learning rate was challenging for this task but we went with 0.00005 as we can't have too many epochs with this training (it just takes too much time).","259530b9":"Helps handle the case of the neutral tweets where the selected text is most often the actual tweet itself. ","316936fe":"Fit the model, and save the model that was just created out to a file.","bceac1f7":"Write locations of where each piece of data is.","08efb346":"In preparation of generating the submission csv, take the dataset and decode each tweet.","472d10e3":"Use the previous methods to perform the operations on the training and testing.","b25d290a":"Create the submission csv used for turning into Kaggle.","81254ef3":"Using the previous method,calculate the inner arrays of the training set.","4396308d":"Model creation using the Configuration class defined above. We use the sigmoid activation function","57d7ebae":"Another step in our preprocessing pipeline - try to remove extraneous data that is completely irrelevant to our model's classifiyng work.\nThis includes any emojis as well as hyperlinks.","0149a275":"Run through the testing set and decode the tweets","0b31e0c5":"Now apply this method to the training dataset - create a new column called targets\n","ce9d6af6":"### Prediction time. Now we input the testing dataset and work with that.","2be0e0b3":"Pad the targets in the event of variant length tweets. Padding is a technique in NLP that ensures that the length of the string doesn't make an impact on its classification. We referenced [this website](https:\/\/machinelearningmastery.com\/data-preparation-variable-length-input-sequences-sequence-prediction\/) to learn more about this topic. Essentially all tweets in the dataset have their lengths padded with \"dummy variables\" to ensure they look the same. ","aec01bfe":"## Group 14 - Machine Learning Spring'2020 Final Project\n### TensorFlow BERT Approach to Tweet Sentiment Analysis","b80de82b":"Create a class for the model's configuration, that way it can be passed easily.","c59dab32":"Given the data, convert it to the form that the BERT Transformer expects from the targets column.","0fb1f3b7":"Given a tweet and the training selected text, create a method to process that tweet and tokenize it for analysis. Tokenizers help train models on new vocabulary. We referenced [this Tokenizers library](https:\/\/github.com\/huggingface\/tokenizers\/tree\/master\/bindings\/python) to learn more."}}