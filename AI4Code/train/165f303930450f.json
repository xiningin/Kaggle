{"cell_type":{"be6267c5":"code","e76db7d9":"code","0af9290e":"code","80fa7010":"code","d5b381d0":"code","ae69d9f9":"code","8c20930c":"code","47c7fc9b":"markdown","81960460":"markdown","1001c823":"markdown","bc3b32d9":"markdown","3a8efb36":"markdown","987c3fd9":"markdown","ddf4c98a":"markdown"},"source":{"be6267c5":"import numpy as np","e76db7d9":"def sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Arguments:\n    z -- A scalar or numpy array of any size.\n\n    Return:\n    s -- sigmoid(z)\n    \"\"\"\n    s = 1 \/ (1 + np.exp(-z))\n\n    return s","0af9290e":"def initialize_with_zeros(dim):\n    \"\"\"\n    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias) of type float\n    \"\"\"\n    w = np.zeros(shape = (dim, 1))\n    b = 0\n    \n    return w, b","80fa7010":"def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (a * b * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (a * b * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if false, 1 if true) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n    \"\"\"\n    \n    m = X.shape[1]\n    A = sigmoid(np.dot(w.T, X) + b)\n    \n    cost = (- 1 \/ m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))\n    \n    dw = (1 \/ m) * np.dot(X, (A - Y).T)\n    db = (1 \/ m) * np.sum(A - Y)\n    \n    cost = np.squeeze(np.array(cost))\n\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost","d5b381d0":"def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:1\n    w -- weights, a numpy array of size (a * b * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (a * b * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if false, 1 if true), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \"\"\"\n    \n    w = copy.deepcopy(w)\n    b = copy.deepcopy(b)\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        grads, cost = propagate(w, b, X, Y)\n\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        \n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        \n        if i % 100 == 0:\n            costs.append(cost)\n        \n            if print_cost:\n                print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs","ae69d9f9":"def predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights, a numpy array of size (a * b * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (a * b * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0\/1) for the examples in X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1, m))\n    w = w.reshape(X.shape[0], 1)\n    \n    A = sigmoid(np.dot(w.T, X) + b)\n    \n    for i in range(A.shape[1]):\n        if A[0, i] > 0.5:\n            Y_prediction[0, i] = 1\n        else:\n            Y_prediction[0, i] = 0\n    \n    return Y_prediction","8c20930c":"def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (a * b * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (a * b * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to True to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    w, b = initialize_with_zeros(X_train.shape[0])\n\n    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n\n    w = params['w']\n    b = params['b']\n    \n\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n\n    if print_cost:\n        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d","47c7fc9b":"**Mathematical expression of the Logistic Regression algorithm**:\n\nFor one example $x^{(i)}$:\n$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n\nThe cost is then computed by summing over all training examples:\n$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$","81960460":"# Forward and Backward propagation\n\na function `propagate()` that computes the cost function and its gradient.\n\nForward Propagation:\n- You get X\n- You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))$\n\nHere are the two formulas we will be using: \n\n$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$","1001c823":"# Predict\n\n1. Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$","bc3b32d9":"Reference: Coursera","3a8efb36":"# Sigmoid\n\ncompute $sigmoid(z) = \\frac{1}{1 + e^{-z}}$ for $z = w^T x + b$ to make predictions","987c3fd9":"# Model","ddf4c98a":"# Optimization\n\nThe goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."}}