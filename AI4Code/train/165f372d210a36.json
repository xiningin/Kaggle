{"cell_type":{"85e894a1":"code","f60bf6f7":"code","cd433d3a":"code","5890487f":"code","276e6f5b":"code","9306f235":"code","a01a03fb":"code","024894b2":"code","a42accbe":"code","be8d634c":"code","4cafa027":"code","22a378f2":"code","64dabf61":"code","b1b2107f":"code","cb6c1ec7":"code","4d6072a3":"code","002e1e23":"code","752ed4ba":"code","0e683283":"code","3244757c":"code","7101a2bf":"code","7ed41704":"code","099dfe96":"code","c13545b4":"code","6ba807c0":"code","fd379c43":"code","67d09272":"code","6f8f7894":"code","2b165cc2":"code","e8e6a827":"code","ec9c6aec":"code","7f447e47":"code","f808315e":"code","7cdcc796":"code","05730748":"code","3b74c518":"code","1d4a9d48":"code","81b65f11":"code","6d2037e2":"code","56ae5c58":"code","03f0c019":"code","a2e26a93":"code","a4482cf7":"code","95aa3a77":"code","67666a48":"code","8c4b4e97":"code","2752cb3c":"code","a54c6990":"code","1db13512":"code","a1c41ffb":"code","39ed1763":"code","d1425a09":"code","c4293235":"code","3eef2e3c":"code","81a9b5c0":"code","a11eb1cc":"code","96744a0a":"code","20119666":"code","cf8f1b9a":"code","53f27b2f":"code","a0f5d78c":"code","720d1637":"code","f3b4ccbf":"code","e0f092e2":"code","73b90cd2":"code","b9fabb24":"code","d344cd98":"markdown","4b007500":"markdown","b4d75d57":"markdown","1269cacf":"markdown","869c4f77":"markdown","8e6ab88f":"markdown","856a1c55":"markdown","59258e45":"markdown","34483144":"markdown"},"source":{"85e894a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n        \nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f60bf6f7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint (os.listdir(\"..\/input\"))\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\noasisdf = pd.read_csv(\"..\/input\/oasis_longitudinal.csv\")\noasisdf.tail(10)","cd433d3a":"oasisdf.isnull().sum()","5890487f":"oasisdf.dropna().describe()","276e6f5b":"oasisdf.dropna(axis=1).describe()","9306f235":"oasisdf['SES'].describe()","a01a03fb":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\noasisdf['SES'] = imp.fit_transform(oasisdf[['SES']])\n\noasisdf['SES'].describe()","024894b2":"oasisdf['MMSE'].describe()","a42accbe":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\noasisdf['MMSE'] = imp.fit_transform(oasisdf[['MMSE']])\n\noasisdf['MMSE'].describe()","be8d634c":"from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\noasisdfScaled = mms.fit_transform(oasisdf[['SES', 'EDUC']])\noasisdf[['SES', 'EDUC']] = oasisdfScaled\noasisdf[['SES', 'EDUC']].describe()","4cafa027":"oasisdf.head()","22a378f2":"oasisdf = pd.get_dummies(oasisdf, columns = ['M\/F', 'Group']) \noasisdf.head()","64dabf61":"list(set(oasisdf.dtypes.tolist()))","b1b2107f":"oasisdf['Group_Demented'] = oasisdf['Group_Demented'].astype(np.float64)\noasisdf['Group_Demented'].dtypes","cb6c1ec7":"df_num = oasisdf.select_dtypes(include = ['float64', 'int64'])\ndf_num.head()","4d6072a3":"cols = ['Age', 'EDUC', 'SES', 'M\/F_F', 'Group_Demented', 'Hand']\nnr_rows = 3\nnr_cols = 2\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*8,nr_rows*4))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        \n        i = r*nr_cols+c       \n        ax = axs[r][c]\n        sns.countplot(oasisdf[cols[i]], hue=oasisdf[\"Group_Demented\"], ax=ax)\n        ax.set_title(cols[i])\n        ax.legend() \n        \nplt.tight_layout() ","002e1e23":"sns.barplot(x='SES', y='Group_Demented', data=oasisdf)\nplt.ylabel(\"Dementia Probability\")\nplt.title(\"Dementia as function of Social Class\")\nplt.show()","752ed4ba":"sns.barplot(x='EDUC', y='Group_Demented', data=oasisdf)\nplt.ylabel(\"Dementia Probability\")\nplt.title(\"Dementia as function of Education\")\nplt.show()","0e683283":"df_num.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8); ","3244757c":"oasisdf_num_corr = oasisdf.corr()['Group_Demented'][:-1]\ngolden_features_list = oasisdf_num_corr[abs(oasisdf_num_corr) > 0.2].sort_values(ascending=False)\nprint(\"There are {} strongly correlated values with Group_Demented:\\n{}\".format(len(golden_features_list), golden_features_list))","7101a2bf":"for i in range(0, len(df_num.columns), 5):\n    sns.pairplot(data=df_num,\n                x_vars=df_num.columns[i:i+5],\n                y_vars=['Group_Demented'])","7ed41704":"corrmat = df_num.corr()\nfig,ax = plt.subplots(figsize = (12,9))\nsns.heatmap(corrmat, vmax=.8, square=True, annot=True)","099dfe96":"sns.boxplot('Group_Demented','EDUC', data = df_num)\nplt.show()","c13545b4":"sns.boxplot('Group_Demented','SES', data = df_num)\nplt.show()","6ba807c0":"from pandas.tools.plotting import scatter_matrix\nsm = scatter_matrix(df_num, alpha=0.2, figsize=(14,14), diagonal='kde')","fd379c43":"import pandas as pd\nfrom pandas.tools.plotting import scatter_matrix\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom scipy.cluster.hierarchy import dendrogram, linkage # hacer enclaces\nfrom scipy.spatial import distance_matrix\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport sklearn as sk #algoritmos de machine learning\nimport seaborn as sns\nimport statsmodels.formula.api as smf\nimport statsmodels.formula.api as smf #regresi\u00f3n lineal","67d09272":"df_num.head()","6f8f7894":"features = ['Visit', 'MR Delay', 'Age', 'EDUC', 'SES', 'CDR', 'eTIV', 'nWBV', 'ASF','MMSE']\ndf_num['Group_Demented'].value_counts()","2b165cc2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_num[features],\n                                                    df_num[\"Group_Demented\"],\n                                                    test_size=0.3,\n                                                    stratify=df_num['Group_Demented'])\n","e8e6a827":"from sklearn import linear_model","ec9c6aec":"#http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state=0, solver='lbfgs',\n                         multi_class='multinomial').fit(X_train, y_train)\n\n\nprint(\"Logistic Regression score (Train): {0:.2}\".format(lr.score(X_train, y_train)))\nprint(\"Logistic Regression score (Test): {0:.2}\".format(lr.score(X_test, y_test)))","7f447e47":"#http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html\nfrom sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X_train, y_train)\nprint(\"KNN score (Train): {0:.2}\".format(neigh.score(X_train, y_train)))\nprint(\"KNN score (Test): {0:.2}\".format(neigh.score(X_test, y_test)))","f808315e":"#http:\/\/scikit-learn.org\/stable\/modules\/svm.html\nfrom sklearn import svm\nsvclass = svm.SVC(gamma='scale')\nsvclass.fit(X_train, y_train) \nprint(\"SVM score (Train): {0:.2}\".format(svclass.score(X_train, y_train)))\nprint(\"SVM score (Test): {0:.2}\".format(svclass.score(X_test, y_test)))","7cdcc796":"#http:\/\/scikit-learn.org\/stable\/modules\/tree.html\nfrom sklearn import tree\ndt = tree.DecisionTreeClassifier()\ndt = dt.fit(X_train, y_train)\nprint(\"Decision Tree score (Train): {0:.2}\".format(dt.score(X_train, y_train)))\nprint(\"Decision Tree score (Test): {0:.2}\".format(dt.score(X_test, y_test)))","05730748":"# http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(n_estimators=100,\n                                criterion='gini',\n                                max_depth=5,\n                                min_samples_split=10,\n                                min_samples_leaf=5,\n                                random_state=0)\nX_train.head()\nforest.fit(X_train, y_train)\nprint(\"Random Forest score (Train): {0:.2}\".format(forest.score(X_train, y_train)))\nprint(\"Random Forest score (Test): {0:.2}\".format(forest.score(X_test, y_test)))","3b74c518":"model=lr","1d4a9d48":"#WAAAAARNING: not all models have the \"feature_importances_\" functions\nplt.bar(np.arange(len(features)), model.feature_importances_)\nplt.xticks(np.arange(len(features)), features, rotation='vertical', ha='left')\nplt.tight_layout()","81b65f11":"from sklearn.model_selection import cross_val_score\ndef validate(model, X_train, y_train, k=10):\n    result = 'K-fold cross validation:\\n'\n    scores = cross_val_score(estimator=model,\n                             X=X_train,\n                             y=y_train,\n                             cv=k,\n                             n_jobs=1)\n    for i, score in enumerate(scores):\n        result += \"Iteration %d:\\t%.3f\\n\" % (i, score)\n    result += 'CV accuracy:\\t%.3f +\/- %.3f' % (np.mean(scores), np.std(scores))\n    return result","6d2037e2":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\n\ndef learningCurve(model, X_train, y_train, k=10):\n    train_sizes, train_scores, test_scores =\\\n                    learning_curve(estimator=model,\n                                   X=X_train,\n                                   y=y_train,\n                                   train_sizes=np.linspace(0.1, 1.0, 10),\n                                   cv=k,\n                                   n_jobs=1)\n\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    \n    plt.rcParams[\"figure.figsize\"] = [6,6]\n    fsize=14\n    plt.xticks(fontsize=fsize)\n    plt.yticks(fontsize=fsize)\n    plt.plot(train_sizes, train_mean,\n             color='blue', marker='o',\n             markersize=5, label='training accuracy')\n    plt.fill_between(train_sizes,\n                     train_mean + train_std,\n                     train_mean - train_std,\n                     alpha=0.15, color='blue')\n\n    plt.plot(train_sizes, test_mean,\n             color='green', linestyle='--',\n             marker='s', markersize=5,\n             label='validation accuracy')\n\n    plt.fill_between(train_sizes,\n                     test_mean + test_std,\n                     test_mean - test_std,\n                     alpha=0.15, color='green')\n\n    plt.grid()\n    plt.xlabel('Number of training samples', fontsize=fsize)\n    plt.ylabel('Accuracy', fontsize=fsize)\n    plt.legend(loc='lower right')\n    plt.ylim([0.4, 1.03])\n    plt.tight_layout()\n    plt.show()","56ae5c58":"from sklearn.model_selection import validation_curve\n\ndef validationCurve(model, X_train, y_train,p_name, p_range, k=10, scale=False):\n    train_scores, test_scores = validation_curve(\n                    estimator=model, \n                    X=X_train, \n                    y=y_train, \n                    param_name=p_name,\n                    param_range=p_range,\n                    cv=k)\n\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.rcParams[\"figure.figsize\"] = [6,6]\n    fsize=14\n    plt.xticks(fontsize=fsize)\n    plt.yticks(fontsize=fsize)\n    plt.plot(p_range, train_mean, \n             color='blue', marker='o', \n             markersize=5, label='training accuracy')\n\n    plt.fill_between(p_range, train_mean + train_std,\n                     train_mean - train_std, alpha=0.15,\n                     color='blue')\n\n    plt.plot(p_range, test_mean, \n             color='green', linestyle='--', \n             marker='s', markersize=5, \n             label='validation accuracy')\n\n    plt.fill_between(p_range, \n                     test_mean + test_std,\n                     test_mean - test_std, \n                     alpha=0.15, color='green')\n\n    plt.grid()\n    if scale:\n        plt.xscale('log')\n    plt.legend(loc='lower right')\n    plt.xlabel('Parameter %s' % p_name, fontsize=fsize)\n    plt.ylabel('Accuracy', fontsize=fsize)\n    plt.ylim([0.7, 1.0])\n    plt.tight_layout()\n    plt.show()","03f0c019":"from sklearn.metrics import roc_curve, roc_auc_score\n\ndef rocCurve(model, X_test, y_test):\n    y_scores = model.predict_proba(X_test)[:,1]\n    fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n    roc_auc = roc_auc_score(y_test, y_scores)\n    plt.figure()\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.rcParams[\"figure.figsize\"] = [8,8]\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    ","a2e26a93":"from sklearn.metrics import confusion_matrix\n\ndef confusionMatrix(model, X_train, y_train, X_test, y_test): \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n    print(confmat)\n    fig, ax = plt.subplots(figsize=(6, 6))\n    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.8)\n    for i in range(confmat.shape[0]):\n        for j in range(confmat.shape[1]):\n            ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n\n    plt.xlabel('Predicted label')\n    plt.ylabel('True label')\n    plt.tight_layout()\n    plt.show()","a4482cf7":"#http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state=0, solver='lbfgs',\n                         multi_class='multinomial').fit(X_train, y_train)\n\n\nprint(\"Logistic Regression score (Train): {0:.2}\".format(lr.score(X_train, y_train)))\nprint(\"Logistic Regression score (Test): {0:.2}\".format(lr.score(X_test, y_test)))\nprint(validate(lr, X_train, y_train))\n","95aa3a77":"learningCurve(lr, X_train, y_train)","67666a48":"rocCurve(lr, X_test, y_test)\n","8c4b4e97":"confusionMatrix(lr, X_train, y_train, X_test, y_test)","2752cb3c":"#http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html\nfrom sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X_train, y_train)\nprint(\"KNN score (Train): {0:.2}\".format(neigh.score(X_train, y_train)))\nprint(\"KNN score (Test): {0:.2}\".format(neigh.score(X_test, y_test)))\nprint(validate(neigh, X_train, y_train))","a54c6990":"learningCurve(neigh, X_train, y_train)","1db13512":"rocCurve(neigh, X_train, y_train)","a1c41ffb":"confusionMatrix(neigh, X_train, y_train, X_test, y_test)\n","39ed1763":"#http:\/\/scikit-learn.org\/stable\/modules\/svm.html\nfrom sklearn.svm import SVC\nsvclass = SVC(probability=True)\nsvclass.fit(X_train, y_train) \nprint(\"SVM score (Train): {0:.2}\".format(svclass.score(X_train, y_train)))\nprint(\"SVM score (Test): {0:.2}\".format(svclass.score(X_test, y_test)))\nprint(validate(svclass, X_train, y_train))","d1425a09":"learningCurve(svclass, X_train, y_train)","c4293235":"rocCurve(svclass, X_test, y_test)","3eef2e3c":"confusionMatrix(svclass, X_train, y_train, X_test, y_test)","81a9b5c0":"#http:\/\/scikit-learn.org\/stable\/modules\/tree.html\nfrom sklearn import tree\ndt = tree.DecisionTreeClassifier()\ndt = dt.fit(X_train, y_train)\nprint(\"Decision Tree score (Train): {0:.2}\".format(dt.score(X_train, y_train)))\nprint(\"Decision Tree score (Test): {0:.2}\".format(dt.score(X_test, y_test)))\nprint(validate(dt, X_train, y_train))","a11eb1cc":"learningCurve(dt, X_train, y_train)\n","96744a0a":"rocCurve(dt, X_train, y_train)","20119666":"confusionMatrix(dt, X_train, y_train, X_test, y_test)","cf8f1b9a":"# http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(n_estimators=3,\n                                criterion='gini',\n                                max_depth=3,\n                                min_samples_split=10,\n                                min_samples_leaf=5,\n                                random_state=0)\nX_train.head()\nforest.fit(X_train, y_train)\nprint(\"Random Forest score (Train): {0:.2}\".format(forest.score(X_train, y_train)))\nprint(\"Random Forest score (Test): {0:.2}\".format(forest.score(X_test, y_test)))\nprint(validate(forest, X_train, y_train))","53f27b2f":"learningCurve(forest, X_train, y_train)","a0f5d78c":"rocCurve(forest, X_test, y_test)","720d1637":"confusionMatrix(forest, X_train, y_train, X_test, y_test)","f3b4ccbf":"validationCurve(lr, X_train, y_train, p_name='C', p_range=[0.001, 0.01, 0.1, 1.0, 10.0, 100.0], scale=True)","e0f092e2":"validationCurve(neigh, X_train, y_train, p_name='n_neighbors', p_range=[1,2,3,4,5,10,15,20])","73b90cd2":"validationCurve(dt, X_train, y_train, p_name='max_depth', p_range=[1,2,3,4,5,10,20])","b9fabb24":"validationCurve(forest, X_train, y_train, p_name='n_estimators', p_range=[1,2,3,4,5,10,20])","d344cd98":"**AJUSTE DE HIPERPARAMETROS**","4b007500":"**CLASSIFICATION PERFORMANCE**","b4d75d57":"**NORMALIZACION DE VARIABLE**","1269cacf":"**TRATAMIENTO DE NULOS**","869c4f77":"**ENTRENAMIENTO**","8e6ab88f":"**ANALISIS EXPLORATORIO**","856a1c55":"**ONE-HOT ENCODING**","59258e45":"**CARGA DEL DATASET**","34483144":"**CAMBIO DE VARIABLES TIPO OBJETO A INTEGER**"}}