{"cell_type":{"4b55e130":"code","d0c3bfbe":"code","cfff98b9":"code","efa9a267":"code","67d68878":"code","a13bbb3d":"code","620e6fd8":"code","cf299473":"code","f55dc0fe":"code","3a2dac2b":"code","b55e3052":"code","6dac89cc":"code","398e6f73":"code","b34d4719":"code","66c9f021":"code","f5ecd79f":"code","fe970b1d":"code","19608ace":"code","41c417e7":"code","0d730198":"code","18f03fd2":"code","4743607e":"code","e95dc8d8":"code","0ab98c6b":"code","eb5dd31a":"code","c90e84b9":"code","754d828f":"code","f57cd187":"code","e0156f9f":"code","ab6e3538":"code","4a2a7388":"code","252813ca":"code","bee17ed3":"code","c605c0d4":"code","1c66fa8d":"code","876151e6":"code","5965d3ae":"code","b8bab24f":"code","4c0ac48a":"code","1543aa56":"markdown","186f24a1":"markdown","744bede8":"markdown","f81e7164":"markdown","8e7ea670":"markdown","a09b9902":"markdown","87ce6452":"markdown","d5a61e4f":"markdown","61ee5367":"markdown","52f9efe8":"markdown","3dff3e87":"markdown","aa5ee654":"markdown","f321ba6e":"markdown","ba5bcd0b":"markdown","071076b2":"markdown","aa446edd":"markdown","9129c211":"markdown","8ff0cdef":"markdown","2b6f22d2":"markdown","b09603b0":"markdown","8d8aa5af":"markdown","2f548eb9":"markdown","388b61a8":"markdown","32ad4d67":"markdown","2b3e8527":"markdown","fc1a0e55":"markdown","1c43f233":"markdown","390c7add":"markdown","108818da":"markdown","17c874e9":"markdown","c8a5308e":"markdown","1fdf2fcf":"markdown","c6107669":"markdown","4090b2f9":"markdown","ed766e29":"markdown"},"source":{"4b55e130":"# Standard data science libraries\nimport psutil\nimport humanize\nimport os\nfrom IPython.display import display_html\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\ndataDirectory= \"..\/input\/logos-bk-kfc-mcdonald-starbucks-subway-none\/logos_v3_mini\/logos3\" \nprint(os.listdir(dataDirectory))\n\n# Any results you write to the current directory are saved as output.        ","d0c3bfbe":"print(os.listdir(\"..\/input\"))","cfff98b9":"!rm -r ~\/.keras\n!mkdir ~\/.keras\n!mkdir ~\/.keras\/models\n# not enough space for both\n#!cp ..\/input\/keras-pretrained-models\/* ~\/.keras\/models\/ \n#!cp ..\/input\/vgg19\/* ~\/.keras\/models\n!cp ..\/input\/keras-pretrained-models\/*notop* ~\/.keras\/models\/\n!cp ..\/input\/keras-pretrained-models\/imagenet_class_index.json ~\/.keras\/models\/\n#!cp ..\/input\/keras-pretrained-models\/resnet50* ~\/.keras\/models\/\n","efa9a267":"import numpy as np\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.models import Model\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense, Flatten\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.core import Dropout\nfrom keras.layers.convolutional import *\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.inception_v3 import preprocess_input\nfrom keras.applications.inception_v3 import decode_predictions\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom keras.models import model_from_json\nimport itertools\nimport matplotlib.pyplot as plt\nimport time\nimport pandas as pd\n%matplotlib inline","67d68878":"train_path = dataDirectory+'\/train'\ntest_path  = dataDirectory+'\/test'\nprint(os.listdir(train_path))\nprint(os.listdir(test_path))","a13bbb3d":"train_datagen = ImageDataGenerator(\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        fill_mode='nearest',\n    validation_split=0.2) # set validation split\n\n\n","620e6fd8":"#['Burger King','HD Iskender','Kahve Dunyasi', 'KFC','McDonalds','Other', 'Ozsut','Popeyes',  'Starbucks', 'Subway', 'Tavuk Dunyasi'] \nselectedClasses = ['Burger King', 'KFC','McDonalds','Other', 'Starbucks', 'Subway'] ","cf299473":"batchSize=32\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_path,\n    target_size=(224, 224),\n    batch_size=batchSize,\n    classes=selectedClasses,\n    subset='training') # set as training data\n\nvalidation_generator = train_datagen.flow_from_directory(\n    train_path, # same directory as training data\n    target_size=(224, 224),\n    batch_size=batchSize,\n    classes=selectedClasses,\n    subset='validation') # set as validation data\n\ntest_generator = ImageDataGenerator().flow_from_directory(\n    test_path, \n    target_size=(224,224), \n    classes=selectedClasses,\n    shuffle= False,\n    batch_size = batchSize)# set as test data","f55dc0fe":"print (\"In train_generator \")\nfor cls in range(len (train_generator.class_indices)):\n    print(selectedClasses[cls],\":\\t\",list(train_generator.classes).count(cls))\nprint (\"\") \n\nprint (\"In validation_generator \")\nfor cls in range(len (validation_generator.class_indices)):\n    print(selectedClasses[cls],\":\\t\",list(validation_generator.classes).count(cls))\nprint (\"\") \n\nprint (\"In test_generator \")\nfor cls in range(len (test_generator.class_indices)):\n    print(selectedClasses[cls],\":\\t\",list(test_generator.classes).count(cls))\n\n","3a2dac2b":"#plots images with labels within jupyter notebook\ndef plots(ims, figsize = (22,22), rows=4, interp=False, titles=None, maxNum = 9):\n    if type(ims[0] is np.ndarray):\n        ims = np.array(ims).astype(np.uint8)\n        if(ims.shape[-1] != 3):\n            ims = ims.transpose((0,2,3,1))\n           \n    f = plt.figure(figsize=figsize)\n    #cols = len(ims) \/\/rows if len(ims) % 2 == 0 else len(ims)\/\/rows + 1\n    cols = maxNum \/\/ rows if maxNum % 2 == 0 else maxNum\/\/rows + 1\n    #for i in range(len(ims)):\n    for i in range(maxNum):\n        sp = f.add_subplot(rows, cols, i+1)\n        sp.axis('Off')\n        if titles is not None:\n            sp.set_title(titles[i], fontsize=20)\n        plt.imshow(ims[i], interpolation = None if interp else 'none')   ","b55e3052":"train_generator.reset()\nimgs, labels = train_generator.next()\n\n#print(labels)\n\nlabelNames=[]\nlabelIndices=[np.where(r==1)[0][0] for r in labels]\n#print(labelIndices)\n\nfor ind in labelIndices:\n    for labelName,labelIndex in train_generator.class_indices.items():\n        if labelIndex == ind:\n            #print (labelName)\n            labelNames.append(labelName)\n\n#labels","6dac89cc":"plots(imgs, rows=4, titles = labelNames, maxNum=8)","398e6f73":"#InceptionV3\n\nbase_model = InceptionV3(weights='imagenet', \n                                include_top=False, \n                                input_shape=(224, 224,3))\nbase_model.trainable = False\n\nx = base_model.output\nx = keras.layers.GlobalAveragePooling2D()(x)\n# let's add a fully-connected layer\nx = Dropout(0.5)(x)\n# and a sofymax\/logistic layer -- we have 6 classes\npredictions = Dense(len(selectedClasses), activation='softmax')(x)\n\n# this is the model we will train\nmodel = Model(input=base_model.input, output=predictions)\n\n\nmodel.summary()\n\n","b34d4719":"#Atutomatic rename with epoch number and val accuracy:\n#filepath=\"checkpoints\/weights-improvement-epeoch-{epoch:02d}-val_acc-{val_acc:.2f}.hdf5\"\n\n\n \nmodelName= \"InceptionTutorial\"\n#save the best weights over the same file with the model name\n\n#filepath=\"checkpoints\/\"+modelName+\"_bestweights.hdf5\"\nfilepath=modelName+\"_bestweights.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n","66c9f021":"model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])","f5ecd79f":"stepsPerEpoch= (train_generator.samples+ (batchSize-1)) \/\/ batchSize\nprint(\"stepsPerEpoch: \", stepsPerEpoch)\n\nvalidationSteps=(validation_generator.samples+ (batchSize-1)) \/\/ batchSize\nprint(\"validationSteps: \", validationSteps)\n\n\n#validationSteps=(test_generator.samples+ (batchSize-1)) \/\/ batchSize\n#print(\"validationSteps: \", validationSteps)\n\n\n","fe970b1d":"train_generator.reset()\nvalidation_generator.reset()\n\n# Fit the model\nhistory = model.fit_generator(\n    train_generator, \n    validation_data = validation_generator,\n    epochs = 3,\n    steps_per_epoch = stepsPerEpoch,\n    validation_steps= validationSteps,\n    callbacks=callbacks_list,\n    verbose=1)\n\n\n","19608ace":"# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'Validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","41c417e7":"timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n\n# serialize model to JSON\nmodel_json = model.to_json()\nwith open(timestr+\"_\"+modelName+\"_MODEL_3\"+\".json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(timestr+\"_\"+modelName+\"_3_LAST_WEIGHTS_\"+\".h5\")\n","0d730198":"\n# load json and create model\njson_file = open('20190107_220958_InceptionTutorial_MODEL_3.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nmodel = model_from_json(loaded_model_json)\n\n","18f03fd2":"# load weights into new model\nmodel.load_weights(\"InceptionTutorial_bestweights.hdf5\")","4743607e":"validation_generator.reset()\nscore = model.evaluate_generator(validation_generator, (validation_generator.samples + (batchSize-1)) \/\/batchSize)\nprint(\"For validation data set; Loss: \",score[0],\" Accuracy: \", score[1])","e95dc8d8":"test_generator.reset()\nscore = model.evaluate_generator(test_generator, (test_generator.samples + (batchSize-1)) \/\/ batchSize)\nprint(\"For test data set; Loss: \",score[0],\" Accuracy: \", score[1])","0ab98c6b":"test_generator.reset()\ntestStep = (test_generator.samples + (batchSize-1)) \/\/ batchSize\nprint(\"testStep: \", testStep)\npredictions = model.predict_generator(test_generator, steps = testStep ,  verbose = 1)\nlen(predictions)","eb5dd31a":"len(predictions)","c90e84b9":"predicted_class_indices=np.argmax(predictions,axis=1)\nprint(predicted_class_indices)\nlen(predicted_class_indices)","754d828f":"labels = (test_generator.class_indices)\nprint(labels)","f57cd187":"labels = dict((v,k) for k,v in labels.items())\nprint(labels)","e0156f9f":"predictedLables= [labels[k] for k in predicted_class_indices]\nprint(predictedLables)\nlen(predictedLables)","ab6e3538":"actualLables= [labels[k] for k in test_generator.classes]\nprint(actualLables)\nlen(actualLables)","4a2a7388":"accuracy_score(actualLables, predictedLables)","252813ca":"matrix = confusion_matrix(actualLables, predictedLables)\nprint(labels)\nmatrix","bee17ed3":"print(classification_report(actualLables, predictedLables))","c605c0d4":"recall_score( actualLables, predictedLables,average='weighted') ","1c66fa8d":"precision_score( actualLables, predictedLables,average='weighted') ","876151e6":"#Prepared code that is taken from SKLearn Website, Creates Confusion Matrix\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","5965d3ae":"cm_plot_labels = selectedClasses\nplot_confusion_matrix(matrix,cm_plot_labels, normalize=False\n                      , title = 'Confusion Matrix')","b8bab24f":"filenames=test_generator.filenames\ndirectory= test_generator.directory\nresults=pd.DataFrame({\"Directory\":directory,\n                      \"Filename\":filenames,\n                      \"Predictions\":predictedLables,\n                     \"Actuals\": actualLables })\nresults.to_csv(\"results.csv\",index=False)","4c0ac48a":"#import glob\n#import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\n\nres = results[260:280]\n\nimages = []\n#for img_path in glob.glob('images\/*.jpg'):\nfor img_path in \".\/\"+res['Directory']+\"\/\"+res['Filename']:\n    images.append(mpimg.imread(img_path))\n\nplt.figure(figsize=(80,80))\ncolumns = 4\nfor i, image in enumerate(images):\n    ax= plt.subplot(len(images) \/ columns + 1, columns, i + 1)\n    ax.set_title(res['Actuals'].iloc[i]+\" \"+res['Predictions'].iloc[i], fontsize=40)\n    plt.imshow(image)\n    \n","1543aa56":"## Load data from directory","186f24a1":"# Paths to data","744bede8":"### Select classes by name","f81e7164":"## Evaluation metrics based on a confusion matrix\n\nA confusion matrix is such that the cell at row  i  and column  j  is equal to the number of observations known to be in group  i  but predicted to be in group  j .\n\n","8e7ea670":"## Accuracy\nThe most classical evaluation metric for classifiers is the accuracy, which corresponds to the proportion of correctly classified instances. ","a09b9902":"# Evaulate the results\n\nBelow, we will see several methods for evaluating a classifier.\n\nMore on http:\/\/www.cse.chalmers.se\/~richajo\/dit865\/files\/Classification%20evaluation%20examples.html","87ce6452":"***predicted_class_indices** has the predicted labels, but you can\u2019t simply tell what the predictions are, because all you can see is numbers like 0,1,4,1,0,6\u2026\nand most importantly you need to map the predicted labels with their unique ids such as filenames to find out what you predicted for which image.","d5a61e4f":"# Data generators with flow from directory\n\nflow_from_directory\n\nhttps:\/\/keras.io\/preprocessing\/image\/\n\nTakes the path to a directory & generates batches of augmented data.\n\n## Arguments\n\n### directory: \nPath to the target directory. It should contain *** one subdirectory per class ***. Any PNG, JPG, BMP, PPM or TIF images inside each of the subdirectories directory tree will be included in the generator. See this script for more details.\n\n### target_size: \nTuple of integers (height, width), default: (256, 256). The dimensions to which all images found will be resized.\n\n### color_mode: \nOne of \"grayscale\", \"rbg\", \"rgba\". Default: \"rgb\". Whether the images will be converted to have 1, 3, or 4 channels.\n\n### classes: \nOptional list of class subdirectories (e.g. ['dogs', 'cats']). Default: None. ***If not provided, the list of classes will be automatically inferred from the subdirectory names\/structure under directory, where each subdirectory will be treated as a different class*** (and the order of the classes, which will map to the label indices, will be alphanumeric). The dictionary containing the mapping from class names to class indices can be obtained via the attribute class_indices.\n\n### class_mode: \nOne of \"categorical\", \"binary\", \"sparse\", \"input\", or None. Default: \"categorical\". Determines the type of label arrays that are returned:\n\"categorical\" will be 2D one-hot encoded labels,\n\"binary\" will be 1D binary labels, \"sparse\" will be 1D integer labels,\n\"input\" will be images identical to input images (mainly used to work with autoencoders).\nIf None, no labels are returned (the generator will only yield batches of image data, which is useful to use with model.predict_generator(),  model.evaluate_generator(), etc.). Please note that in case of class_mode None, the data still needs to reside in a subdirectory of directory for it to work correctly.\n\n### batch_size: \nSize of the batches of data (default: 32).\n\n### shuffle: \nWhether to shuffle the data (default: True)\n\n### seed: \nOptional random seed for shuffling and transformations.\n\n### save_to_dir: \nNone or str (default: None). This allows you to optionally specify a directory to which to save the augmented pictures being generated (useful for visualizing what you are doing).\n\n### save_prefix: \nStr. Prefix to use for filenames of saved pictures (only relevant if save_to_dir is set).\n\n### save_format: \nOne of \"png\", \"jpeg\" (only relevant if save_to_dir is set). Default: \"png\".\n\n### follow_links: \nWhether to follow symlinks inside class subdirectories (default: False).\n\n### subset: \nSubset of data (\"training\" or \"validation\") if *** validation_split *** is set in ImageDataGenerator.\n\n### interpolation: \nInterpolation method used to resample the image if the target size is different from that of the loaded image. Supported methods are \"nearest\", \"bilinear\", and \"bicubic\". If PIL version 1.1.3 or newer is installed, \"lanczos\" is also supported. If PIL version 3.4.0 or newer is installed,  \"box\" and \"hamming\" are also supported. By default, \"nearest\" is used.\n\n### Returns\n\nA DirectoryIterator yielding tuples of (x, y) where x is a numpy array containing a batch of images with shape (batch_size, *target_size, channels) and y is a numpy array of corresponding labels.\n","61ee5367":"\n## Decode Labels\n\nNow ***predictions*** has the probabilities for 6 classes for each test case!\n\nWe can find the class with the highest probability as the prediction label as follows:\n","52f9efe8":"***predictedLabels*** have the labels predicted by the model. We need to locate ***the actual labels*** for the same test data as follows:","3dff3e87":"To  transfer learning, we first need to download pre-trained model and its weights. To do so:\n* First from the right tool bar click \"**+Add Data**\" button.\n* Then search for \"**Keras Pretrained Models**\" and and add it to your notebook\n* Last run the below code to copy the necessary files into \"**Keras**\" directory where the notebook will use.","aa5ee654":"# Save Predictions\nFinally, save the results to a CSV file.","f321ba6e":"## The precision and recall metrics\nSeveral metrics can be derived from a confusion matrix. (See the Wikipedia article.) In particular, they tend to be based on the special case of a confusion matrix, where we assign one class to be the \"positive\" class that is important to us. This is sometimes called a table of confusion. In such a table, we speak of true positives, false positives, false negatives, and true negatives.\n\nThe precision and recall metrics are probably the most common metrics derived from such a table.\n\nP  = TP \/ (TP + FP)\n\nR  = TP \/ (TP + FN)\n\nFor example, What's the precision and recall of 'Burger King' in this case?","ba5bcd0b":"# Compile the model ","071076b2":"# Save the model and last weights","aa446edd":"### Load images from the selected directories","9129c211":"# Define Image Data Generators for train, validation & test data\ntf.keras.preprocessing.image.ImageDataGenerator:\n\nhttps:\/\/www.tensorflow.org\/versions\/r1.6\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator\n\nhttps:\/\/keras.io\/preprocessing\/image\/\n\nGenerate minibatches of image data with real-time data augmentation.\nThe data will be looped over (in batches).\n\n## Arguments:\n\n### validation_split: \nFloat. Fraction of images reserved for validation (strictly between 0 and 1).\n\n### featurewise_center: \nset input mean to 0 over the dataset.\n\n### samplewise_center: \nset each sample mean to 0.\n\n### featurewise_std_normalization: \ndivide inputs by std of the dataset.\n\n### samplewise_std_normalization: \ndivide each input by its std.\n\n### zca_whitening: \napply ZCA whitening.\n\n### zca_epsilon: \nepsilon for ZCA whitening. Default is 1e-6.\n\n### rotation_range: \ndegrees (0 to 180).\n\n### width_shift_range: \nfraction of total width, if < 1, or pixels if >= 1.\n\n### height_shift_range: \nfraction of total height, if < 1, or pixels if >= 1.\n\n### shear_range: \nshear intensity (shear angle in degrees).\n\n### zoom_range: \namount of zoom. if scalar z, zoom will be randomly picked in the range [1-z, 1+z]. A sequence of two can be passed instead to select this range.\n\netc...\n\n","8ff0cdef":"# Auxilary Functions for ploting images","2b6f22d2":"This notebook is a complete solution for classifiying images (Logos in this case) by using Transfer Learn from a Keras pre-trained network ( currently InceptionV3). \n\n**About Data:** Data was collected by a group of students at **At\u0131l\u0131m University** from food courts of several shooping mall at **Ankara, Turkey.** **5 Brand logos** were recorded and then extracted still images from these videos. Furthermore, there is a class  called \"**None**\" for images without any logos. Data is already splitted into two main directories as **Train and Test **for the user convenience. \n\n\n**About Notebook**: We need to develope a image classification model for helping blind people to be aware of the logos around themselves. Thus, we implement the solution by creating this notebook. During development, we noticed that there are many resources for Transfer Learning and Image Classification however, unfortunately, most of them are not either complete or lack of explanation. Thus, we decided to share this notebbok for anyone who would need similar requirements as we do.\n\n**Why study this notebook**: By studying this notebook you would be practicing on:\n* How to import **pre-trained** network model and its weights\n* How to define and use **Image Data Generators** for train, validation & test data\n* How to **agument** training data\n*  How to **plot images** in Image Data Generators\n*  How to **create your mode**l by **Transfer Learning** from InceptionV3 (or any Keras pre-trained model)\n*  How to set-up and use **Callbacks** such as **Checkpoints** and **EarlyStopping** \n* How to **compile** and **train** the model  by setting up **fit_generator**\n* How to calculate **steps_per_epoch** and **validation_steps** for fit_generator \n* How to monitor **Training History** using history object\n* How to **save** your model, best and last weights\n* How to **upload** your model, best and last weights from saved files\n* How to **evaulate** the model by using evaluate_generator\n* How to use your trained model to **predict** the classes of the test images by using **predict_generator**\n* How to **decode** the labels of the predicted classes\n* How to **save** the **predictions and actual labels** into a CSV file\n* How to **measeure the success** of your model by using **Accuracy**, **Precision**, and **Recall**\n* How to generate **classification_report**\n* How to prepare the **confusion matrix** to monitor the classification success\n* How to **plot** some sample predictions along with the corresponding true labels\n\n**In the end of this notebook**: We hope that you will have necessary skills to\n* **Understand** the Transfer Learning\n* **Apply** Image Detection\n* **Evaulate** the success of the classification\n* **Report** the results\n\nYou can leave comments or suggection to improve this tutorial.\nThanks in advanced!\nKMK\n","b09603b0":"# Upload the model and best weights","8d8aa5af":"# Train the model\n\n## Set up fit_generator\nhttps:\/\/keras.io\/models\/model\/\n\nfit_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n\nTrains the model on data generated batch-by-batch by a Python generator (or an instance of Sequence).\n\nThe generator is run in parallel to the model, for efficiency. For instance, this allows you to do real-time data augmentation on images on CPU in parallel to training your model on GPU.\n\nThe use of keras.utils.Sequence guarantees the ordering and guarantees the single use of every input per epoch when using use_multiprocessing=True.\n\n### Arguments\n\n#### generator: \nA generator or an instance of Sequence (keras.utils.Sequence) object in order to avoid duplicate data when using multiprocessing. The output of the generator must be either\n* a tuple (inputs, targets)\n* a tuple (inputs, targets, sample_weights).\nThis tuple (a single output of the generator) makes a single batch. Therefore, all arrays in this tuple must have the same length (equal to the size of this batch). Different batches may have different sizes. For example, the last batch of the epoch is commonly smaller than the others, if the size of the dataset is not divisible by the batch size. The generator is expected to loop over its data indefinitely. An epoch finishes when steps_per_epoch batches have been seen by the model.\n\n#### steps_per_epoch: \nInteger. Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch. ***It should typically be equal to the number of samples of your dataset divided by the batch size***. Optional for Sequence: if unspecified, will use the len(generator) as a number of steps.\n\n#### epochs: \nInteger. Number of epochs to train the model. An epoch is an iteration over the entire data provided, as defined by steps_per_epoch. Note that in conjunction with initial_epoch, epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs, but merely until the epoch of index epochs is reached.\n\n#### verbose: \nInteger. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n\n#### callbacks: \nList of keras.callbacks.Callback instances. List of callbacks to apply during training. See callbacks.\nvalidation_data: This can be either\n\n* a generator or a Sequence object for the validation data\n* tuple (x_val, y_val)\n* tuple (x_val, y_val, val_sample_weights)\non which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data.\n\n#### validation_steps: \nOnly relevant if validation_data is a generator. Total number of steps (batches of samples) to yield from validation_data generator before stopping at the end of every epoch. ***It should typically be equal to the number of samples of your validation dataset divided by the batch size.*** Optional for Sequence: if unspecified, will use the len(validation_data) as a number of steps.\n\n#### class_weight: \nOptional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class.\n\n#### max_queue_size: \nInteger. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10.\n\n#### workers: \nInteger. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread.\n\n#### use_multiprocessing: \nBoolean. If True, use process-based threading. If unspecified, use_multiprocessing will default to False. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes.\n\n#### shuffle: \nBoolean. Whether to shuffle the order of the batches at the beginning of each epoch. Only used with instances of Sequence (keras.utils.Sequence). Has no effect when steps_per_epoch is not None.\ninitial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run).\n\n### Returns\n\n#### A History object. \nIts History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n","2f548eb9":"# Make Predictions\n\nYou need to **reset** the test_generator before whenever you call the predict_generator. This is important, if you forget to reset the test_generator you will get outputs in a weird order.","388b61a8":"## Plot the confusion matrix","32ad4d67":"## Show Training History\nWe can plot the accuracy and loss values for each epoch using the history object as follows.","2b3e8527":"# Show some sample predictions with corresponding true labels\n","fc1a0e55":"### Number of samples of each class in all data generators","1c43f233":"The utility function ***classification_report*** prints the precision and recall values for all the categories. (The  F1  score combines the precision and recall values into a single value.)","390c7add":"# Create model by Transfer Learning from InceptionV3","108818da":"# Plot some train data","17c874e9":"## Train\nRun more epochs for increasing the accuracy. For example:\n\n**epochs = 30**","c8a5308e":"# Predict Generator\nhttps:\/\/keras.io\/models\/sequential\/\n\npredict_generator(generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n\nGenerates predictions for the input samples from a data generator.\n\nThe generator should return the same kind of data as accepted by predict_on_batch.\n\n## Arguments\n\n### generator: \nGenerator yielding batches of input samples or an instance of Sequence (keras.utils.Sequence) object in order to avoid duplicate data when using multiprocessing.\n\n### steps: \nTotal number of steps (batches of samples) to yield from generator before stopping. Optional for Sequence: if unspecified, will use the len(generator) as a number of steps.\n\n### max_queue_size: \nMaximum size for the generator queue.\n\n### workers: \nInteger. Maximum number of processes to spin up when using process based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread.\n\n### use_multiprocessing: \nIf True, use process based threading. Note that because this implementation relies on multiprocessing, you should not pass non picklable arguments to the generator as they can't be passed easily to children processes.\n\n### verbose: \nverbosity mode, 0 or 1.\n\n## Returns\n\nNumpy array(s) of predictions.","1fdf2fcf":"# Dependicies","c6107669":"# Evaulate the model\n\n***evaluate_generator(generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)***\n\nEvaluates the model on a data generator.\n\nThe generator should return the same kind of data as accepted by test_on_batch.\n\n## Arguments\n\n***generator:*** Generator yielding tuples (inputs, targets) or (inputs, targets, sample_weights) or an instance of Sequence (keras.utils.Sequence) object in order to avoid duplicate data when using multiprocessing.\n\n***steps:*** Total number of steps (batches of samples) to yield from generator before stopping. Optional for Sequence: if unspecified, will use the len(generator) as a number of steps.\n\n***max_queue_size:*** maximum size for the generator queue\n\n***workers:*** Integer. Maximum number of processes to spin up when using process based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread.\n\n***use_multiprocessing:*** if True, use process based threading. Note that because this implementation relies on multiprocessing, you should not pass non picklable arguments to the generator as they can't be passed easily to children processes.\n\n***verbose:*** verbosity mode, 0 or 1.\n\n***Returns***\nScalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and\/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.","4090b2f9":"### Calculate steps_per_epoch and validation_steps For fit_generator \nInteger. Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch. ***It should typically be equal to the number of samples of your dataset divided by the batch size.*** Optional for Sequence: if unspecified, will use the len(generator) as a number of steps.\n","ed766e29":"# Usage of callbacks\n\nhttps:\/\/keras.io\/callbacks\/\n\n\nA callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training. You can pass a list of callbacks (as the keyword argument callbacks) to the .fit() method of the Sequential or Model classes. The relevant methods of the callbacks will then be called at each stage of the training.\n\n\n## History\n\nkeras.callbacks.History()\n\nCallback that records events into a History object.\n\nThis callback is automatically applied to every Keras model. \n***The History object gets returned by the fit method of models.***\n\n\n## ModelCheckpoint\n\n***keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)***\n\n\nSave the model after every epoch.\n\n\nfilepath can contain named formatting options, which will be filled the value of epoch and keys in logs (passed in on_epoch_end).\n\n\nFor example: if filepath is weights.{epoch:02d}-{val_loss:.2f}.hdf5, then the model checkpoints will be saved with the epoch number and the validation loss in the filename.\n\n### Arguments\n\n***filepath:*** string, path to save the model file. \n\n*** monitor:*** quantity to monitor. \n\n***verbose:**** verbosity mode, 0 or 1. \n\n***save_best_only:*** if save_best_only=True, the latest best model according to the quantity monitored will not be overwritten. \n\n\n***mode:**** one of {auto, min, max}. \n\n***If save_best_only=True,**** the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity. For val_acc, this should be max, for val_loss this should be min, etc. \n\n***In auto mode,*** the direction is automatically inferred from the name of the monitored quantity. \n\n***save_weights_only:*** if True, then only the model's weights will be saved (model.save_weights(filepath)), else the full model is saved (model.save(filepath)). \n\n***period:*** Interval (number of epochs) between checkpoints.\n\n### Example:\n\n***Atutomatic rename with epoch number and val accuracy:***\n\nfilepath=\"checkpoints\/weights-improvement-epeoch-{epoch:02d}-val_acc-{val_acc:.2f}.hdf5\"\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n\ncallbacks_list = [checkpoint]\n\n\n## EarlyStopping\n\n***keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)***\n\nStop training when a monitored quantity has stopped improving.\n\n### Arguments\n\n***monitor:*** quantity to be monitored. \n\n***min_delta:*** minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement. \n\n***patience:*** number of epochs with no improvement after which training will be stopped. \n\n***verbose:*** verbosity mode. \n\n***mode:*** one of {auto, min, max}. **In min mode,** training will stop when the quantity monitored has stopped decreasing; **in max mode** it will stop when the quantity monitored has stopped increasing; **in auto mode,** the direction is automatically inferred from the name of the monitored quantity. \n\n***baseline:*** Baseline value for the monitored quantity to reach. Training will stop if the model doesn't show improvement over the baseline. \n\n***restore_best_weights:*** whether to restore model weights from the epoch with the best value of the monitored quantity. **If False,** the model weights obtained at the last step of training are used.\n\n\n\n\n\n\n"}}