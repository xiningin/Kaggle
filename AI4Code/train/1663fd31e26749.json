{"cell_type":{"51a17f80":"code","02a2b9a1":"code","d91db73a":"code","601126bd":"code","1882a0ec":"code","0ac68337":"code","91d08625":"code","8997b1f2":"code","ef44c37a":"code","85b089be":"code","01a8990e":"code","f55dae87":"code","0ac5e35d":"code","d032a7a4":"code","ddb341c2":"code","692cc68f":"code","df5869b9":"code","b8256d62":"code","eedba491":"code","0de42d40":"code","052d86b5":"code","c0c22e37":"code","f313a5d2":"code","bb72be5f":"code","d7ecd789":"code","1f5aa7e3":"code","45fab632":"code","23c6fb8e":"code","2ac842c6":"code","eee14a25":"code","e9fbf594":"code","71492cc0":"code","647bd1f2":"code","a28dda19":"code","75ccf6de":"markdown","ef0c2af5":"markdown","c04ac33e":"markdown","766222e6":"markdown","64b1f826":"markdown","d018cf31":"markdown","d9c99e53":"markdown","1a033f98":"markdown","a8b66f63":"markdown","104d11cc":"markdown","91125dad":"markdown","aee32fe2":"markdown"},"source":{"51a17f80":"#imports\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgbm\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier","02a2b9a1":"# options\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500) \nnp.set_printoptions(threshold=100)","d91db73a":"train_data = pd.read_csv('..\/input\/nba-enhanced-stats\/2016-17_teamBoxScore.csv')\ntest_data = pd.read_csv('..\/input\/nba-enhanced-stats\/2017-18_teamBoxScore.csv')\n\nbase_train_data = train_data.copy()\n\ntrain_data.head()","601126bd":"train_data.describe()","1882a0ec":"# Date processing \ndate_value = pd.to_datetime(train_data['gmDate'], errors='coerce')\ntime_value = pd.to_datetime(train_data['gmTime'], errors='coerce')\n\n\ntrain_data['year'] = date_value.dt.year \ntrain_data['month'] = date_value.dt.month \ntrain_data['day'] = date_value.dt.day \ntrain_data['hour'] = time_value.dt.hour \ntrain_data['minute'] = time_value.dt.minute\n\ndel train_data['gmDate']\ndel train_data['gmTime']","0ac68337":"train_data.head()","91d08625":"# Mapping of teamRslt column\nmapping = {'Loss': 2, 'Win': 1}\n\ntrain_data = train_data.replace({'teamRslt': mapping})\ntest_data = test_data.replace({'teamRslt': mapping})","8997b1f2":"# Drop columns with missing values\ncols_with_missing = [col for col in train_data.columns if train_data[col].isnull().any()] \ntrain_data.drop(cols_with_missing, axis=1, inplace=True)\ntest_data.drop(cols_with_missing, axis=1, inplace=True)","ef44c37a":"# Categorical data processing\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = train_data.columns.values.tolist()\nfor col in features:\n    if train_data[col].dtype in numerics: continue\n    categorical_columns.append(col)\nindexer = {}\nfor col in categorical_columns:\n    if train_data[col].dtype in numerics: continue\n    _, indexer[col] = pd.factorize(train_data[col])\n    \nfor col in categorical_columns:\n    if train_data[col].dtype in numerics: continue\n    train_data[col] = indexer[col].get_indexer(train_data[col])","85b089be":"train_data.head()","01a8990e":"corrmat = train_data.corr()\nf, ax = plt.subplots(figsize=(20,18))\nsns.heatmap(corrmat, vmax=.8, square=True)","f55dae87":"k = 12\ncols = corrmat.nlargest(k, 'teamRslt')['teamRslt'].index\nf, ax = plt.subplots(figsize=(10,6))\ncm = np.corrcoef(train_data[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, \n                 yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","0ac5e35d":"k = 12\ncols = corrmat.nlargest(k, 'teamPTS')['teamPTS'].index\nf, ax = plt.subplots(figsize=(10,6))\ncm = np.corrcoef(train_data[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, \n                 yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","d032a7a4":"k = 12\ncols = corrmat.nlargest(k, 'opptPTS')['opptPTS'].index\nf, ax = plt.subplots(figsize=(10,6))\ncm = np.corrcoef(train_data[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, \n                 yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","ddb341c2":"k = 12\ncols = corrmat.nlargest(k, 'teamDayOff')['teamDayOff'].index\nf, ax = plt.subplots(figsize=(10,6))\ncm = np.corrcoef(train_data[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, \n                 yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","692cc68f":"k = 12\ncols = corrmat.nlargest(k, 'teamLoc')['teamLoc'].index\nf, ax = plt.subplots(figsize=(10,6))\ncm = np.corrcoef(train_data[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, \n                 yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","df5869b9":"y = train_data['teamRslt']\n\ncolumns_to_delete = ['teamRslt', 'opptRslt', 'teamEDiff', 'teamFIC', 'opptEDiff', 'opptFIC']\n\ntrain_data.drop(columns_to_delete, axis=1, inplace=True)\n\nX = train_data;","b8256d62":"# data split for train\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","eedba491":"train_set = lgbm.Dataset(X_train, y_train, silent=False)\nvalid_set = lgbm.Dataset(X_valid, y_valid, silent=False)\n\nparams = {\n        'boosting_type':'gbdt', 'objective': 'regression', 'num_leaves': 31,\n        'learning_rate': 0.05, 'max_depth': -1, 'subsample': 0.8,\n        'bagging_fraction' : 1, 'max_bin' : 5000 , 'bagging_freq': 20,\n        'colsample_bytree': 0.6, 'metric': 'rmse', 'min_split_gain': 0.5,\n        'min_child_weight': 1, 'min_child_samples': 10, 'scale_pos_weight':1,\n        'zero_as_missing': True, 'seed':0,        \n    }\n\nmodelL = lgbm.train(params, train_set = train_set, num_boost_round=1000,\n                   early_stopping_rounds=50,verbose_eval=10, valid_sets=valid_set)\n\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgbm.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();","0de42d40":"data_tr  = xgb.DMatrix(X_train, label=y_train)\ndata_cv  = xgb.DMatrix(X_valid   , label=y_valid)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]\n\nparms = {'max_depth':8, #maximum depth of a tree\n         'objective':'reg:squarederror',\n         'eta'      :0.3,\n         'subsample':0.8,#SGD will use this percentage of data\n         'lambda '  :4, #L2 regularization term,>1 more conservative \n         'colsample_bytree ':0.9,\n         'colsample_bylevel':1,\n         'min_child_weight': 10}\nmodelx = xgb.train(parms, data_tr, num_boost_round=200, evals = evallist, early_stopping_rounds=30, maximize=False, verbose_eval=10)\n\nfig =  plt.figure(figsize = (15,25))\naxes = fig.add_subplot(111)\nxgb.plot_importance(modelx,ax = axes,height = 1)\nplt.show();plt.close()","052d86b5":"feature_columns = ['opptPTS', 'teamDrtg', 'teamTO', 'teamORB', 'teamFGM']\nX = X[feature_columns];\n\nX.head()","c0c22e37":"feature_columns = ['opptPTS', 'teamDrtg', 'teamTO', 'teamORB', 'teamFGM']\nsns.pairplot(train_data[feature_columns], height=2.5)\nplt.show()","f313a5d2":"def parseResult(data):\n    def parse(n): \n        left = n[0] \n        rigth = n[1]\n        return 1 if left > rigth else 2\n    \n    return list(map(parse, data))","bb72be5f":"# data split for model tuning\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","d7ecd789":"knn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\npred = knn.predict(X_valid)\nresults = []\nresult = accuracy_score(y_valid, pred) * 100\nresults.append(result)\nprint(result)","1f5aa7e3":"pred","45fab632":"y_valid","23c6fb8e":"clf = RandomForestClassifier(n_estimators=10)\nclf.fit(X_train, y_train)\npred = clf.predict(X_valid)\nresult = accuracy_score(y_valid, pred) * 100\nresults.append(result)\nprint(result)","2ac842c6":"clfgtb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, \n                                    max_depth=1, random_state=0).fit(X_train, y_train)\n\nresult = clfgtb.score(X_valid, y_valid) * 100\nresults.append(result)\nprint(result)","eee14a25":"x = np.arange(3)\n\nfig, ax = plt.subplots()\nplt.bar(x, results)\nax.set_ylim(bottom=75)\nplt.xticks(x, ('KNeighbors', 'RandomForest', 'GradientBoosting'))\nplt.show()","e9fbf594":"test_data.describe()","71492cc0":"x_new = test_data[feature_columns]\ny_new = test_data['teamRslt']\nx_new.head()","647bd1f2":"clss = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(X, y)\nclss.score(x_new, y_new)","a28dda19":"matches = [\n    {'home_team': 'MIN', 'away_team': 'BOS'},\n    {'home_team': 'CLE', 'away_team': 'LAL'},\n    {'home_team': 'CHA', 'away_team': 'MIN'},\n    {'home_team': 'ORL', 'away_team': 'HOU'},\n    {'home_team': 'DET', 'away_team': 'UTA'},\n]\n\nfor match in matches:\n    home_team = match['home_team']    \n    away_team = match['away_team']\n\n    prev_matches = base_train_data.loc[(base_train_data['teamAbbr'] == home_team) & (base_train_data['opptAbbr'] == away_team)][feature_columns]\n    avg = prev_matches.mean()\n\n    avg_prev = [prev_matches.mean().values.tolist()]\n\n    pred = clss.predict(avg_prev)\n    prob = clss.predict_proba(avg_prev)\n\n    print(home_team + ' vs ' + away_team)\n    print(pred)\n    print(prob)\n    print('-------------------------------\\n')","75ccf6de":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n \n 1. [Import libraries](#1)\n 1. [Download dataset](#2)\n 1. [Preparing to analysis](#3)\n 1. [FE](#4)\n 1. [Model tuning](#5)\n     -  [KNeighborsClassifier](#5.1)\n     -  [RandomForestClassifier](#5.2)\n     -  [GradientBoostingClassifier](#5.3)\n 1. [Prediction](#6)","ef0c2af5":"## 4. FE<a class=\"anchor\" id=\"4\"><\/a>\n \n### [Back to Table of Contents](#0.1)","c04ac33e":"## 2. Download dataset <a class=\"anchor\" id=\"2\"><\/a>\n \n### [Back to Table of Contents](#0.1)","766222e6":"### 5.1 KNeighborsClassifier <a class=\"anchor\" id=\"5.1\"><\/a>\n \n### [Back to Table of Contents](#0.1)","64b1f826":"### 5.3 GradientBoostingClassifier <a class=\"anchor\" id=\"5.3\"><\/a>\n \n### [Back to Table of Contents](#0.1)","d018cf31":"## 1. Import libraries <a class=\"anchor\" id=\"1\"><\/a>\n \n### [Back to Table of Contents](#0.1)","d9c99e53":"## 3. Preparing to analysis <a class=\"anchor\" id=\"3\"><\/a>\n \n### [Back to Table of Contents](#0.1)","1a033f98":"## Basketball Prediciton","a8b66f63":"## 6. Prediction<a class=\"anchor\" id=\"6\"><\/a>\n\n### [Back to Table of Contents](#0.1)","104d11cc":"### 5.2 RandomForestClassifier <a class=\"anchor\" id=\"5.2\"><\/a>\n\n### [Back to Table of Contents](#0.1)","91125dad":"## 5. Model tuning<a class=\"anchor\" id=\"5\"><\/a>\n \n### [Back to Table of Contents](#0.1)","aee32fe2":"**Thanks to:\nFE - https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg   \nModel tunung - https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models#FE,-tuning-and-comparison-of-the-20-popular-models"}}