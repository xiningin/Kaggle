{"cell_type":{"c2123cc9":"code","33487dcc":"code","e9497a5d":"code","bb994075":"code","c02eca84":"code","e1f646e1":"code","baa46985":"code","d2afc499":"code","7b6bb8c1":"code","9018c23e":"code","302adb1c":"code","33c7a104":"code","c1058b28":"code","59378ff7":"code","f07d7198":"code","bed6bc9a":"code","c230786e":"code","e8cbc6cb":"code","c5443be6":"code","36de2e36":"code","8e07619d":"markdown","cd69b453":"markdown","ca8f5e43":"markdown","07dfe8e7":"markdown","af847d5f":"markdown","5439044e":"markdown","5c100365":"markdown","af4eb93b":"markdown","ca9127e4":"markdown","0ed5bc14":"markdown","483267c9":"markdown","934001ba":"markdown","83f32e29":"markdown","0a51f9f7":"markdown","f6e5be8c":"markdown","c695526d":"markdown","13e827a7":"markdown","118cd47b":"markdown","7409bfdc":"markdown","b7f7ee59":"markdown","aa9d9f9f":"markdown"},"source":{"c2123cc9":"import pandas as pd \nimport numpy as np\n\nratings = pd.read_csv('..\/input\/reviews\/olist_order_reviews_dataset.csv', index_col=0)\nitems = pd.read_csv('..\/input\/orderitems\/olist_order_items_dataset.csv', index_col=0)\nproducts = pd.read_csv('..\/input\/product\/olist_products_dataset.csv', index_col=0)\norders = pd.read_csv('..\/input\/orders\/olist_orders_dataset.csv', index_col=0)","33487dcc":"# Dentro do dataset de pedidos, utilizaremos apenas duas colunas: ID do pedido e ID do cliente\norders.drop(orders.columns[[1,2,3,4,5,6]], axis=1, inplace=True)\norders.head()","e9497a5d":"# Exibe o dataset dos produtos\nproducts.drop(products.columns[[1,2,3,4,5,6,7]], axis=1, inplace=True)\nproducts.head()","bb994075":"# Dataset que cont\u00e9m as avalia\u00e7\u00f5es feitas pelos clientes.\n\nratings.drop(ratings.columns[[2,3,4]], axis=1, inplace=True)\nratings.head()","c02eca84":"# Dataset de pedido onde temos o ID do pedido relacionado com o ID do produto\n\nitems.drop(items.columns[[0,2,3,4,5]], axis=1, inplace=True)\nitems.head()","e1f646e1":"# Merge com o dataset de Avalia\u00e7\u00f5es, fazendo o relacionamento mostrando quais foram os pedidos relacionados com os pedidos, exibindo o pre\u00e7o e score\n\nitems=items.merge(ratings, on='order_id')\nitems.head(5)","baa46985":"# Merge com o dataset de pedido (orders), mostrando o ID do cliente que fez determinado pedido.\n# Merge com o dataset do produto(products), mostrando os ID's do produtos e suas caracteristicas.\n\n# O objetivo \u00e9 mostrar qual cliente fez determinado pedido, mostrando o ID do produto.\n\nitems=items.merge(orders, on='order_id')\nitems=items.merge(products, on='product_id')\nitems.head(5)","d2afc499":"# Reordena as colunas para aplicar o 'surprise'\ncolumn_names = [\"customer_id\", \"product_id\", \"review_score\"]\nitems = items.reindex(columns=column_names)\nitems.head()","7b6bb8c1":"# Importando pacotes\nfrom surprise import KNNWithMeans\nfrom surprise import Dataset\nfrom surprise import accuracy\nfrom surprise import Reader, SVD\nimport os\nfrom surprise.model_selection import train_test_split\nfrom sklearn.decomposition import TruncatedSVD\nfrom surprise.model_selection import cross_validate","9018c23e":"# Lendo o dataset\nreader = Reader(rating_scale=(1, 5))\ndata = Dataset.load_from_df(items, reader=reader)\n\n# Fazendo split dos dados \/ Test size de 30%\ntrainset, testset = train_test_split(data, test_size=0.3,random_state=10)\n\n# Cria\u00e7\u00e3o de um modelo baseado em item \nalgo = KNNWithMeans(k=5, sim_options={'user_based': False})\nalgo.fit(trainset)\n\n# Teste do modelo\ntest_pred = algo.test(testset)","302adb1c":"#Obtem o MAE\nprint(\"Item-based Model : Test Set\")\naccuracy.rmse(test_pred, verbose=True)","33c7a104":"svd = SVD()\ncross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)","c1058b28":"# Recebe o index do produto e determina 10 recomenda\u00e7\u00f5es de produto baseado no index 5\nalgo.get_neighbors(5, 10)","59378ff7":"# classifica\u00e7\u00e3o m\u00e9dia e n\u00famero de classifica\u00e7\u00f5es por produto\nratings_df = pd.DataFrame(items.groupby('product_id').review_score.mean())","f07d7198":"# Os 10 resultados obtidos acima ser\u00e3o colados dentro dos colchetes para obter o resultado final\nratings_df.iloc[[436, 1482, 2282, 15558, 0, 1, 2, 3, 4, 6]].index","bed6bc9a":"# Limitamos a 50k de linhas\nnew_df1=items.head(50000)\nratings_matrix = new_df1.pivot_table(values='review_score', index='customer_id', columns='product_id', fill_value=0)\nratings_matrix.head()\n\n# Exibe a matriz\nX = ratings_matrix.T\nX.head()","c230786e":"#Decomposi\u00e7\u00e3o da Matriz.\n\n#O fun\u00e7\u00e3o Truncated SVD vai reduzir a dimens\u00e3o da matriz esparsa no n\u00famero de componentes requisitados \nSVD_model = TruncatedSVD(n_components=10)\ndecomposed_matrix = SVD_model.fit_transform(X)\ndecomposed_matrix.shape\n\n# Matriz de Correla\u00e7\u00e3o\ncorrelation_matrix = np.corrcoef(decomposed_matrix)\ncorrelation_matrix.shape\n\n# Correla\u00e7\u00e3o para todos os itens com o item comprado por este cliente, com base em itens avaliados por outras pessoas de clientes que compraram o mesmo produto\ncorrelation_product_ID = correlation_matrix[1]\ncorrelation_product_ID.shape","e8cbc6cb":"# Digamos que um usu\u00e1rio compre um produto do \u00edndice 15\nX.index[15]","c5443be6":"i = '00ffe57f0110d73fd84d162252b2c784'\nproduct_names = list(X.index)\nproduct_ID = product_names.index(i)\nproduct_ID","36de2e36":"# Indica produtos que tem mais de 90% de correla\u00e7\u00e3o com outro produto\nRecommend = list(X.index[correlation_product_ID > 0.90])\n\n# Remove o produto j\u00e1 comprado pelo usu\u00e1rio (o primeiro produto exibido na lista)\n#Recommend.remove(i)\n\n# Top 10 produtos similares\nRecommend[0:10]","8e07619d":"movie_indices = [i[0] for i in sim_scores]\nmovie_indices","cd69b453":"items[items['customer_id'] == '58dbd0b2d70206bf40e62cd34e84d795']","ca8f5e43":"# Nova tentativa de aplicar a filtragem colaborativa","07dfe8e7":"trainset = data.build_full_trainset() # Do not split the dataset into folds and just return a trainset as is, built from the whole dataset.\nsvd.fit(trainset)","af847d5f":"# This function calculates the weighted average of similar users\n# to determine a potential rating for an input user and show\n\ndef predicted_rating(product_name, user):\n    sim_users = user_sim_df.sort_values(by=user, ascending=False).index[1:1000]\n    user_values = user_sim_df.sort_values(by=user, ascending=False).loc[:,user].tolist()[1:1000]\n    rating_list = []\n    weight_list = []\n    for j, i in enumerate(sim_users):\n        rating = piv.loc[i, product_name]\n        similarity = user_values[j]\n        if np.isnan(rating):\n            continue\n        elif not np.isnan(rating):\n            rating_list.append(rating*similarity)\n            weight_list.append(similarity)\n    return sum(rating_list)\/sum(weight_list)  ","5439044e":"svd = SVD()\ncross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)","5c100365":"# Primeira tentativa de aplicar FILTRAGEM COLABORATIVA","af4eb93b":"from surprise import Reader, Dataset, SVD\nfrom surprise.model_selection import cross_validate\nreader = Reader()","ca9127e4":"top_products('perfumaria')","0ed5bc14":"data = Dataset.load_from_df(items[['customer_id', 'product_category_name', 'review_score']], reader)","483267c9":"print(piv.shape)\npiv.fillna('', inplace=True)\npiv.head()","934001ba":"svd.predict('customer_id', 3671, verbose=True).est","83f32e29":"piv = items.pivot_table(index=['customer_id'], columns=['product_category_name'], values='review_score')","0a51f9f7":"# Utilizando SVD","f6e5be8c":"# Normalize the values\npiv_norm = piv.apply(lambda x: (x-np.mean(x))\/(np.max(x)-np.min(x)), axis=1)\n\n\n# Drop all columns containing only zeros representing users who did not rate\npiv_norm = piv_norm.T\npiv_norm = piv_norm.loc[:, (piv_norm != 0).any(axis=0)]","c695526d":"# This function will return the top 10 shows with the highest cosine similarity value\n\ndef top_products(product_name):\n    count = 1\n    print('Similar products to {} include:\\n'.format(product_name))\n    for item in item_sim_df.sort_values(by = product_name, ascending = False).index[1:11]:\n        print('No. {}: {}'.format(count, item))\n        count +=1  ","13e827a7":"item_similarity = cosine_similarity(piv_sparse)\nuser_similarity = cosine_similarity(piv_sparse.T)","118cd47b":"# This function constructs a list of lists containing the highest rated shows per similar user\n# and returns the name of the show along with the frequency it appears in the list\n\ndef similar_user_recs(user):\n    \n    if user not in piv_norm.columns:\n        return('No data available on user {}'.format(user))\n    \n    sim_users = user_sim_df.sort_values(by=user, ascending=False).index[1:11]\n    best = []\n    most_common = {}\n    \n    for i in sim_users:\n        max_score = piv_norm.loc[:, i].max()\n        best.append(piv_norm[piv_norm.loc[:, i]==max_score].index.tolist())\n    for i in range(len(best)):\n        for j in best[i]:\n            if j in most_common:\n                most_common[j] += 1\n            else:\n                most_common[j] = 1\n    sorted_list = sorted(most_common.items(), key=operator.itemgetter(1), reverse=True)\n    return sorted_list[:5]    ","7409bfdc":"# This function will return the top 5 users with the highest similarity value \n\ndef top_users(user):\n    \n    if user not in piv_norm.columns:\n        return('No data available on user {}'.format(user))\n    \n    print('Most Similar Users:\\n')\n    sim_values = user_sim_df.sort_values(by=user, ascending=False).loc[:,user].tolist()[1:11]\n    sim_users = user_sim_df.sort_values(by=user, ascending=False).index[1:11]\n    zipped = zip(sim_users, sim_values,)\n    for user, sim in zipped:\n        print('User #{0}, Similarity value: {1:.2f}'.format(user, sim)) ","b7f7ee59":"# Inserting the similarity matricies into dataframe objects\n\nitem_sim_df = pd.DataFrame(item_similarity, index = piv_norm.index, columns = piv_norm.index)\nuser_sim_df = pd.DataFrame(user_similarity, index = piv_norm.columns, columns = piv_norm.columns)","aa9d9f9f":"data"}}