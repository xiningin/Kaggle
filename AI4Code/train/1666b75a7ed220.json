{"cell_type":{"5f32d37f":"code","0b56ef04":"code","2dde0eba":"code","696b024f":"code","0a233471":"code","416765d6":"code","36a928da":"code","e8e5243a":"code","2dbd6fc2":"code","9b3d4d0e":"code","7d060f78":"code","649bc3f2":"markdown","f910421f":"markdown","4ce5ce14":"markdown","bc21c17c":"markdown","e97f587f":"markdown","187566a2":"markdown","ec865a91":"markdown","236a35ff":"markdown","badb4074":"markdown","ec6cb5e6":"markdown"},"source":{"5f32d37f":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as tt\nfrom torchvision.models import densenet161\nfrom torch.utils.data import DataLoader,random_split\nimport torchvision.transforms as tt\nfrom torchvision.datasets import ImageFolder\nimport torch.optim as optim\nfrom tqdm import tqdm\n\nepochs = 5\nval_size = 4000\nvalid_loss_min = np.Inf\npath = '..\/input\/upside-down-image-dataset\/data'\nbatch_size = 64\nlr = 1e-3\nepoch = 5\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntrans = tt.Compose(\n    [\n        tt.Resize(224),\n        tt.ToTensor(),\n        \n    ]\n)\ndataset = ImageFolder(root=path,transform=trans)\n\ntrain_size = len(dataset) - val_size\ntrain_set, test_set = random_split(dataset,[train_size,val_size])\n\ntrain_dataloader = DataLoader(train_set,batch_size=batch_size,shuffle=True)\ntest_dataloader = DataLoader(test_set,batch_size,shuffle=True)\n\n# def accuracy(outputs,labels):\n#     _,preds = torch.max(outputs,dim=1)\n#     return torch.tensor(torch.sum(preds==labels).item()\/len(preds))\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel  = densenet161(pretrained=True)\n# print(model.parameters)\nfor param in model.parameters():\n    param.requires_grad = False\n\nmodel.classifier = nn.Linear(2208,2)\nmodel =model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(),lr=1e-3)\n\nval_loss = []\nval_acc = []\ntrain_loss = []\ntrain_acc = []\ntotal_step = len(train_dataloader)\n\nfor epoch in range(1,epochs+1):\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    print(f'Epoch {epoch}\\n')\n    for batch_idx,(data_,target_) in enumerate(train_dataloader):\n        data_,target_ = data_.to(device),target_.to(device)\n        optimizer.zero_grad()\n        outputs = model(data_)\n        loss = criterion(outputs,target_)\n        loss.backward()\n        optimizer.step()\n        running_loss +=loss.item()\n        _,pred = torch.max(outputs,dim=1)\n        correct += torch.sum(pred==target_).item()\n        total += target_.size(0)\n        if (batch_idx)%50==0:\n            print('Epoch [{}\/{}], Step [{}\/{}], Loss: {:.4f}' \n                   .format(epoch, epochs, batch_idx, total_step, loss.item()))\n    train_acc.append(100 * correct\/total)\n    train_loss.append(running_loss\/total_step)\n    print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct\/total):.4f}')\n    batch_loss = 0\n    total_t = 0\n    correct_t = 0\n    with torch.no_grad():\n        model.eval()\n        for data_t,target_t in test_dataloader:\n            data_t,target_t = data_t.to(device),target_t.to(device)\n            outputs_t = model(data_t)\n            loss_t = criterion(outputs_t,target_t)\n            batch_loss+=loss.item()\n            _,pred_t = torch.max(outputs_t,dim=1)\n            correct_t += torch.sum(pred_t==target_t).item()\n            total_t += target_t.size(0)\n        val_acc.append(100 * correct_t\/total_t)\n        val_loss.append(batch_loss\/len(test_dataloader))\n        network_learned = batch_loss < valid_loss_min\n        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t\/total_t):.4f}\\n')\n\n        \n        if network_learned:\n            valid_loss_min = batch_loss\n            torch.save(model.state_dict(), 'densenet.pt')\n            print('Improvement-Detected, save-net')\n    model.train()\n","0b56ef04":"import matplotlib.pyplot as plt\n%matplotlib inline","2dde0eba":"\ndef predict_img(img,model):\n    xb = img.unsqueeze(0)\n    xb = xb.to(device)\n    yb = model(xb)\n    _,preds = torch.max(yb,dim=1)\n    return dataset.classes[preds[0].item()]","696b024f":"img, label = test_set[600]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_img(img, model))","0a233471":"img, label = test_set[700]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_img(img, model))","416765d6":"img, label = test_set[2]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_img(img, model))","36a928da":"img, label = test_set[60]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_img(img, model))","e8e5243a":"img, label = test_set[20]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_img(img, model))","2dbd6fc2":"img, label = test_set[3]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_img(img, model))","9b3d4d0e":"plt.plot(val_loss,label='val_loss')\nplt.plot(train_loss, label=\"train_loss\")\nplt.legend()","7d060f78":"plt.plot(val_acc,label='val_acc')\nplt.plot(train_acc, label=\"train_acc\")\nplt.legend()","649bc3f2":"# Look at some of the images that were classified incorrectly","f910421f":"## Some Correct Predictions","4ce5ce14":"# Fatima Fellowship Quick Coding Challenge","bc21c17c":"# Train it to classify image orientation until a reasonable accuracy is reached\n\nThe Model is trained with a training accuracy of 99.7562% and validation accuracy of 99.9750% in 5 epochs.","e97f587f":"**Challenge**\n\n**Deep Learning Vision**\n**Upside down detector: Train a model to detect if images are upside down**\n\n1. Pick an image dataset (you can pick any dataset that you're comfortable with)\n2. Synthetically turn some of images upside down\n3. Build a neural network (using Tensorflow, PyTorch, or any framework you like)\n4. Train it to classify image orientation until a reasonable accuracy is reached\n5. Look at some of the images that were classified incorrectly\n6. Answer the following question: what is 1 idea that you have to improve your model's performance on this dataset (you don't have to implement it)","187566a2":"## Some Incorrect Predictions","ec865a91":"# Synthetically turn some of images upside down\n\nThe Faces images were made upside down i.e 180degree rotate using this script which  i have written while managing the dataset.\n```\nimport cv2\nimport os\nfrom tqdm import tqdm\nsource_dir = Your source path\ndestination_dir = Your Destination Path\n\nfile_name = os.listdir(source_dir)\n\nfor i in tqdm(file_name):\n    image_path = os.path.join(source_dir,i)\n    img = cv2.imread(image_path)\n    img = cv2.rotate(img,cv2.ROTATE_180)\n    os.chdir(destination_dir)\n    img_save = cv2.imwrite(i,img)\nprint(\"Rotate Successfully\")\n\n```","236a35ff":"# Dataset\n\nYou can Downlaod the Dataset from [here](https:\/\/www.kaggle.com\/aquib5559\/upside-down-image-dataset).\n\nThis Dataset is Derived from [**Flickers Faces HQ Dataset**](https:\/\/github.com\/NVlabs\/ffhq-dataset#:~:text=The%20dataset%20consists%20of%2070%2C000,age%2C%20ethnicity%20and%20image%20background.&text=The%20images%20were%20crawled%20from,aligned%20and%20cropped%20using%20dlib.)\n\nThis Dataset contains 20k images with 10k original and 10k Upside_Down images.Each Image has size of 256 * 256.\n","badb4074":"# Build a neural network (using Tensorflow, PyTorch, or any framework you like)\n\n\nI choose PyTorch to train my model.The reason i choose PyTorch as in PyTorch things are way more imperative and dynamic: you can define, change and execute nodes as you go, no special session interfaces or placeholders.\n\n**My Approach to solve the problem**\nI use transfer learning techniques which provide SOTA(state-of-the-art).There are many pretrained model like Resnets,VGG,Densenets etc.\nThe reason why i choose **Densenet** over **Resnet** is Dense Networks are a relatively recent implementation of Convolutional Neural Networks, that expand the idea proposed for Residual Networks, which have become a standard implementation for feature extraction on image data. Similar to Residual Networks that add a connection from the preceding layer, Dense Nets add connections to all the preceding layers, to produce a Dense Block. The problem that Residual Nets addressed was the vanishing gradient problem that was due to many layers of the network. This helped build bigger and more efficient networks and reduce the error rate on classification tasks on ImageNet. So the idea of Densely Connected Networks, is that every layer is connected to all its previous layers and its succeeding ones, thus forming a Dense Block.\n\n![](https:\/\/miro.medium.com\/max\/3000\/1*04TJTANujOsauo3foe0zbw.jpeg)\n\n**Problems that Densenet solves**\n\nCounter-intuitively, by connecting this way DenseNets require fewer parameters than an equivalent traditional CNN, as there is no need to learn redundant feature maps. Furthermore, some variations of ResNets have proven that many layers are barely contributing and can be dropped. In fact, the number of parameters of ResNets are big because every layer has its weights to learn. Instead, DenseNets layers are very narrow (e.g. 12 filters), and they just add a small set of new feature-maps. Another problem with very deep networks was the problems to train, because of the mentioned flow of information and gradients. DenseNets solve this issue since each layer has direct access to the gradients from the loss function and the original input image.","ec6cb5e6":"# Answer the following question: what is 1 idea that you have to improve your model's performance on this dataset (you don't have to implement it)\n\n\nAs the model already achived a good accuracy.But we can do some steps to make sure that it doesn't overfit.\n1. Do **Cross Validation**\n2. Add Some more Linear Layers with dropout and batch normalisation at last layer.\n3. HyperParameter Tuning"}}