{"cell_type":{"74a607dd":"code","4cabf9c0":"code","baea2f7d":"code","a89cd879":"code","91b75f80":"code","061e6fa5":"code","beeee1dc":"code","9861f6d1":"code","2aae238d":"code","f1af1358":"code","c2346701":"code","ee4a415f":"code","fb0da831":"code","67b3dc1b":"code","e001034e":"code","48f5c879":"code","c6f79ab5":"code","5d2e93fe":"markdown","d3b93f66":"markdown","8fdd4282":"markdown","1b0edb92":"markdown","446e644d":"markdown","cab57885":"markdown","0aef2e94":"markdown","7afeb4ac":"markdown","babcfcf0":"markdown","68740ccc":"markdown"},"source":{"74a607dd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.decomposition import PCA # PCA\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.model_selection import KFold\nfrom sklearn import linear_model\nfrom sklearn.metrics import make_scorer\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import svm\nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\nimport seaborn\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","4cabf9c0":"train = pd.read_csv('..\/input\/train.csv')\nlabels=train[\"SalePrice\"]\ntest = pd.read_csv('..\/input\/test.csv')\ndata = pd.concat([train,test],ignore_index=True)\ndata = data.drop(\"SalePrice\", 1)\nids = test[\"Id\"]","baea2f7d":"train.head()","a89cd879":"# Count the number of rows in train\ntrain.shape[0]","91b75f80":"# Count the number of rows in total\ndata.shape[0]","061e6fa5":"# Count the number of NaNs each column has.\nnans=pd.isnull(data).sum()\nnans[nans>0]","beeee1dc":"# Remove id and columns with more than a thousand missing values\ndata=data.drop(\"Id\", 1)\ndata=data.drop(\"Alley\", 1)\ndata=data.drop(\"Fence\", 1)\ndata=data.drop(\"MiscFeature\", 1)\ndata=data.drop(\"PoolQC\", 1)\ndata=data.drop(\"FireplaceQu\", 1)","9861f6d1":"# Count the column types\ndata.dtypes.value_counts()","2aae238d":"all_columns = data.columns.values\nnon_categorical = [\"LotFrontage\", \"LotArea\", \"MasVnrArea\", \"BsmtFinSF1\", \n                   \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\", \n                   \"2ndFlrSF\", \"LowQualFinSF\", \"GrLivArea\", \"GarageArea\", \n                   \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \n                   \"ScreenPorch\",\"PoolArea\", \"MiscVal\"]\n\ncategorical = [value for value in all_columns if value not in non_categorical]","f1af1358":"#\u00a0One Hot Encoding and nan transformation\ndata = pd.get_dummies(data)\n\nimp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\ndata = imp.fit_transform(data)\n\n# Log transformation\ndata = np.log(data)\nlabels = np.log(labels)\n\n# Change -inf to 0 again\ndata[data==-np.inf]=0","c2346701":"pca = PCA(whiten=True)\npca.fit(data)\nvariance = pd.DataFrame(pca.explained_variance_ratio_)\nnp.cumsum(pca.explained_variance_ratio_)","ee4a415f":"pca = PCA(n_components=36,whiten=True)\npca = pca.fit(data)\ndataPCA = pca.transform(data)","fb0da831":"# Split traing and test\ntrain = data[:1460]\ntest = data[1460:]","67b3dc1b":"# R2 Score\n\ndef lets_try(train,labels):\n    results={}\n    def test_model(clf):\n        \n        cv = KFold(n_splits=5,shuffle=True,random_state=45)\n        r2 = make_scorer(r2_score)\n        r2_val_score = cross_val_score(clf, train, labels, cv=cv,scoring=r2)\n        scores=[r2_val_score.mean()]\n        return scores\n\n    clf = linear_model.LinearRegression()\n    results[\"Linear\"]=test_model(clf)\n    \n    clf = linear_model.Ridge()\n    results[\"Ridge\"]=test_model(clf)\n    \n    clf = linear_model.BayesianRidge()\n    results[\"Bayesian Ridge\"]=test_model(clf)\n    \n    clf = linear_model.HuberRegressor()\n    results[\"Hubber\"]=test_model(clf)\n    \n    clf = linear_model.Lasso(alpha=1e-4)\n    results[\"Lasso\"]=test_model(clf)\n    \n    clf = BaggingRegressor()\n    results[\"Bagging\"]=test_model(clf)\n    \n    clf = RandomForestRegressor()\n    results[\"RandomForest\"]=test_model(clf)\n    \n    clf = AdaBoostRegressor()\n    results[\"AdaBoost\"]=test_model(clf)\n    \n    clf = svm.SVR()\n    results[\"SVM RBF\"]=test_model(clf)\n    \n    clf = svm.SVR(kernel=\"linear\")\n    results[\"SVM Linear\"]=test_model(clf)\n    \n    results = pd.DataFrame.from_dict(results,orient='index')\n    results.columns=[\"R Square Score\"] \n    results.plot(kind=\"bar\",title=\"Model Scores\")\n    axes = plt.gca()\n    axes.set_ylim([0.5,1])\n    return results\n\nlets_try(train,labels)","e001034e":"# Split traing and test\ntrain = dataPCA[:1460]\ntest = dataPCA[1460:]\n\nlets_try(train,labels)","48f5c879":"cv = KFold(n_splits=5,shuffle=True,random_state=45)\n\nparameters = {'alpha': [1000,100,10],\n              'epsilon' : [1.2,1.25,1.50],\n              'tol' : [1e-10]}\n\nclf = linear_model.HuberRegressor()\nr2 = make_scorer(r2_score)\ngrid_obj = GridSearchCV(clf, parameters, cv=cv,scoring=r2)\ngrid_fit = grid_obj.fit(train, labels)\nbest_clf = grid_fit.best_estimator_ \n\nbest_clf.fit(train,labels)","c6f79ab5":"# Make predictions\n\npredictions_huber = best_clf.predict(test)\npredictions_huber = np.exp(predictions_huber)\n\nsub = pd.DataFrame({\n        \"Id\": ids,\n        \"SalePrice\": predictions_huber\n    })\n\nsub.to_csv(\"Huber_submission.csv\", index=False)\n#print(sub)","5d2e93fe":"**The idea is:**\n\n - Feature reduction with PCA\n - Data transformation (log, hot encoding, nan)\n - Test different regression models\n\n**Things found:**\n\n- Applying log transformation increases the accuracy.\n- Using PCA with 36 components makes the learning and testing much (much much) faster.\n- Removing columns with more than 1000 NaNs gives better result than applying \"mean\" to them.\n- There are outliers. Instead of removing them, using HuberRegressor seems to provide a good result. HuberRegressor is a model robust to outliers.","d3b93f66":"## Data Model Selection ##\n\nSimple test to run multiple models against our data. First, with raw features. No PCA.","8fdd4282":"# Introduction\n- The aim of this notebook is not to get a good position on the leaderboard,instead it is to learn about PCA and how to apply it.","1b0edb92":"## Importing Libraries","446e644d":"That's all about PCA.\nTHANK YOU","cab57885":"## Data Manipulation ##\n\n- Apply hot encoding, convert categorical variable into dummy\/indicator variables.\n- Fill NaN with median for that column.\n- Log transformation.\n- Change -inf to 0.","0aef2e94":"## Data Load ##\n\nI mix data and test to manipulate all the data just once. SalePrice is extracted to its own variable \"labels\". Finally, SalesPrice is remove from data.","7afeb4ac":"## Feature reduction ##\n\nThere are many features, so we are going to use PCA to reduce them. The idea is to start with n_components = number of columns. Then select the number of components that add up to 1 variance_ratio.","babcfcf0":"Now, let's try the same but using data with PCA applied.","68740ccc":"## What is PCA?\nThe main idea of principal component analysis (PCA) is to reduce the dimensionality of a data set consisting of many variables correlated with each other, either heavily or lightly, while retaining the variation present in the dataset, up to the maximum extent. The same is done by transforming the variables to a new set of variables, which are known as the principal components (or simply, the PCs) and are orthogonal, ordered such that the retention of variation present in the original variables decreases as we move down in the order.\n<img src='https:\/\/s3.amazonaws.com\/files.dezyre.com\/images\/Tutorials\/Principal+Component+Analysis.jpg'>\n<b>Dimensionality<\/b>: It is the number of random variables in a dataset or simply the number of features, or rather more simply, the number of columns present in your dataset.<br>\n<b>Correlation<\/b>: It shows how strongly two variable are related to each other. The value ranges from -1 to +1. Positive indicates that when one variable increases, the other increases as well, while negative indicates the other decreases on increasing the former. And the modulus value of indicates the strength of relation.<br>\n<b>Orthogonal<\/b>: Uncorrelated to each other, i.e., correlation between any pair of variables is 0.<br>"}}