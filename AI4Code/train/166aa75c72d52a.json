{"cell_type":{"b38b56b2":"code","85b6f888":"code","3f1e710a":"code","1de4ffe6":"code","379799f0":"code","6eefc662":"code","46366f9f":"code","1e78c06d":"code","911fc8b8":"code","7349cc49":"code","39ac6580":"code","f809826f":"code","93c5776d":"code","e39d8394":"code","a08fd877":"code","bb307b72":"code","f8387cb9":"code","7c1a52e5":"code","6ac9848c":"code","dde82116":"code","aa5f3fdc":"code","705158fa":"code","9429db1c":"code","58bdc417":"code","f6a09e16":"code","edf8a007":"markdown","d929f1b9":"markdown","93ed690a":"markdown","35a40aaa":"markdown","4a11c53d":"markdown","687e663d":"markdown","0a5ed43a":"markdown","490dcae6":"markdown","8dfcb426":"markdown","008d1b96":"markdown","9f95b59e":"markdown","f0ea7736":"markdown","40176646":"markdown","1a4d7f07":"markdown","91d9736f":"markdown","be682d33":"markdown","dd85a6fb":"markdown","e1308dd5":"markdown"},"source":{"b38b56b2":"#Import utils and libraries\nimport pandas as pd\nimport numpy as np\nimport os\nimport pickle\nfrom collections import Counter\n\nfrom typing import Callable, Optional\nfrom copy import deepcopy\n\n# Import pytoch libraries and modules \nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch import optim\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\n#Import libraries for text procesing\nimport nltk\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\n","85b6f888":"!pip install pkbar\nimport pkbar","3f1e710a":"!pip install rouge\nfrom rouge import Rouge","1de4ffe6":"class Parameters:\n  # Model Parameters\n  hidden_size: int = 150  # of the encoder; default decoder size is doubled if encoder is bidi\n  dec_hidden_size: Optional[int] = 200  # if set, a matrix will transform enc state into dec state\n  embed_size: int = 100 # Size of the embedding vectors\n  eps=1e-31\n  batch_size=16 \n  enc_bidi = True #Set the encoder as bidirectional\n  enc_rnn_dropout = 0.1 # Set the dropout parameter in the encoder\n  enc_attn = True #Activate the encoder attention\n  dec_attn = True #Activate the decoder attention\n  pointer = True #Activate the pointer generator mechanism\n  # Set different dropout probabilities in the decoder\n  dec_in_dropout=0.1\n  dec_rnn_dropout=0.1\n  dec_out_dropout=0.1\n  # Vocabulary and data parameters\n  max_src_len: int = 65  # exclusive of special tokens such as EOS\n  max_tgt_len: int = 15  # exclusive of special tokens such as EOS\n  vocab_min_frequency: int = 3\n  # Data paths\n  embed_file: Optional[str] = '\/kaggle\/input\/glove6b100dtxt\/glove.6B.100d.txt'  # use pre-trained embeddings\n  data_path: str = '\/kaggle\/input\/cleaned-news-summary\/cl_train_news_summary_more.csv'\n  val_data_path: Optional[str] = '\/kaggle\/input\/cleaned-news-summary\/cl_train_news_summary_more.csv'\n  test_data_path: str = '\/kaggle\/input\/cleaned-news-summary\/cl_valid_news_summary_more.csv'\n  # Parameters to save the model\n  resume_train = False\n  encoder_weights_path='encoder_sum.pt'\n  decoder_weights_path='decoder_sum.pt'\n  encoder_decoder_adapter_weights_path='adapter_sum.pt'\n  losses_path='val_losses.pkl'\n  print_every = 100","379799f0":"def simple_tokenizer(text, lower=False, newline=None):\n  if lower:\n    text = text.lower()\n  if newline is not None:  # replace newline by a token\n    text = text.replace('\\n', ' ' + newline + ' ')\n  return text.split()\n","6eefc662":"class Vocab(object):\n  PAD = 0\n  SOS = 1\n  EOS = 2\n  UNK = 3\n\n  def __init__(self):\n    ''' Initialize the structuresto store the information about the vocabulary'''\n    self.word2index = {}\n    self.word2count = Counter()\n    self.reserved = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n    self.index2word = self.reserved[:]\n    self.embeddings = None\n\n  def add_words(self, words):\n    ''' Add words to the vocabulary'''\n    for word in words:\n      #if it is an unseen word  \n      if word not in self.word2index:\n        #Include the word in the mapping from word to index\n        self.word2index[word] = len(self.index2word)\n        # Include the word in the indexes\n        self.index2word.append(word)\n    # Increment the count of ocurrencies of the word to 1\n    self.word2count.update(words)\n  \n  def load_embeddings(self, file_path: str, dtype=np.float32) -> int:\n    ''' Load the embedding vectors from a file into the vocabulary'''\n    num_embeddings = 0\n    vocab_size = len(self)\n    with open(file_path, 'rb') as f:\n      # For every word in the embedding vectors\n      for line in f:\n        line = line.split()\n        word = line[0].decode('utf-8')\n        # self.add_words([word])\n        # Get the index of the embedded word\n        idx = self.word2index.get(word)\n        if idx is not None:\n          # Extract the embedding vector of the word\n          vec = np.array(line[1:], dtype=dtype)\n          #If the embedding vector is not initialized\n          if self.embeddings is None:\n            # Set the embeddings dimension, initialize the embedding vector to zeros\n            n_dims = len(vec)\n            self.embeddings = np.random.normal(np.zeros((vocab_size, n_dims))).astype(dtype)\n            self.embeddings[self.PAD] = np.zeros(n_dims)\n          # Store the embedding in the array of embeddings \n          self.embeddings[idx] = vec          \n          num_embeddings += 1\n    return num_embeddings\n\n  def save_to_file(self, filename):\n    ''' Save the Vocab object to a file'''\n    with open(filename,'wb') as f:\n        pickle.dump(self,f) \n\n  def __getitem__(self, item):\n    ''' Get the next item when iterating over the instance'''\n    if type(item) is int:\n      return self.index2word[item]\n    return self.word2index.get(item, self.UNK)\n\n  def __len__(self):\n    ''' Return the length of the instance or vocabulary'''\n    return len(self.index2word)\n\n\ndef load_vocab(filename):\n    ''' Load a Vocab instance from a file'''\n    with open(filename,'rb') as f:\n        v = pickle.load(f)\n    return v","46366f9f":"class Dataset(object):\n  ''' Create a Class to store the data input and its features'''\n  def __init__(self, filename: str, tokenize: Callable=simple_tokenizer, max_src_len: int=None,\n               max_tgt_len: int=None, max_rows: int=None, truncate_src: bool=False, truncate_tgt: bool=False):\n    print(\"Reading dataset %s...\" % filename, end=' ', flush=True)\n    # Save the filename and initialize the variables\n    self.filename = filename\n    self.pairs = []\n    self.src_len = 0\n    self.tgt_len = 0\n    self.max_rows = max_rows\n\n    #Read the csv file, using max rows if it is defined\n    if max_rows is None:\n        df = pd.read_csv(filename, encoding='utf-8')\n    else:\n        df = pd.read_csv(filename, encoding='utf-8', nrows=max_rows\n                        )\n    # Tokenize the source texts\n    sources = df['text'].apply(lambda x : tokenize(x))\n    # Truncate the sources texts\n    if truncate_src:\n        sources = [src[:max_src_len] if len(src)>max_src_len else src for src in sources]\n    # Tokenize the targets\n    targets = df['summary'].apply(lambda x : tokenize(x))\n    # Trucate the targets\n    if truncate_tgt:\n        targets = [tgt[:max_tgt_len] if len(tgt)>max_tgt_len else tgt for tgt in targets]\n        \n    # Calculate the length of every source and targets        \n    src_length = [len(src)+1 for src in sources]\n    tgt_length = [len(tgt)+1 for tgt in targets]\n    #Calculate the max length of the sources and the targets\n    max_src = max(src_length)\n    max_tgt = max(tgt_length)\n    #Create a tuple contaiing source,target,source length, target length\n    self.src_len = max_src\n    self.tgt_len = max_tgt\n    # Insert the source text and target in the pairs class atribute\n    self.pairs.append([(src, tgt, src_len, tgt_len) for src,tgt,src_len,tgt_len in zip(sources,targets,src_length,tgt_length)])\n    self.pairs = self.pairs[0]\n    print(\"%d pairs.\" % len(self.pairs))\n\n  def build_vocab(self, min_freq, embed_file: str=None) -> Vocab:\n    ''' Build the vocabulary extracted from the texts in the object class\n        Input:\n        - min_freq: integer, minimum ocurrencies needed to include the word in the vocab\n        - embed_file: string, path + filename of the embeddings file\n    '''\n    # Extract the words in the whole corpus\n    total_words=[src+tgr for src,tgr,len_src,len_tgr in self.pairs]\n    total_words = [item for sublist in total_words for item in sublist]\n    # Create a counter to count the ocurrencies of every word in the corpus\n    word_counts = Counter(total_words)\n    # Create a vocabulary object\n    vocab=Vocab()\n    for word,count in word_counts.items():\n        # If occurencies of the word are bigger then min_freq\n        if(count>min_freq):\n            # Include the cord in the vocabulary\n            vocab.add_words([word])  \n    # Load the embeddings in the vocab object\n    count = vocab.load_embeddings(embed_file)\n    print(\"%d pre-trained embeddings loaded.\" % count)\n\n    return vocab  \n","1e78c06d":"class MyDataset(nn.Module):\n    ''' A Dataset Class where we store all the data needed during the training phase'''\n    \n    def __init__(self, src_sents, trg_sents, vocab):\n      '''Initialize the instance and store the source texts, targets or summaries '''\n      self.src_sents = src_sents\n      self.trg_sents = trg_sents\n      self.vocab=vocab\n      # Keep track of how many data points.\n      self._len = len(src_sents)\n\n    def __getitem__(self, index):\n        ''' Return the ith items from the object\n            Input:\n            - Index: integer, index of the items to return\n            Output:\n            - a dictionary with keys x the source texts, y the targets, \n              x_len length of source texts, y_len the length of targets\n        '''\n        return {'x':self.src_sents[index], \n                'y':self.trg_sents[index], \n                'x_len':len(self.src_sents[index]), \n                'y_len':len(self.trg_sents[index])}\n    \n    def __len__(self):\n        ''' Return the length of the object'''\n        return self._len\n","911fc8b8":"def tensorize(vocab, tokens):\n    ''' Convert the tokens received to a tensor '''\n    return torch.tensor([vocab[token] for token in tokens])\n\ndef pad_sequence(vectorized_sent, max_len):\n    ''' Padding the sentence (tensor) to max_len '''\n    pad_dim = (0, max_len - len(vectorized_sent))\n    return F.pad(vectorized_sent, pad_dim, 'constant').tolist()\n\ndef preprocess(x,y,p,vocab):\n    ''' Prepare a source text x and a target summary y: convert them to tensors,\n        pads the sentences to its max length.\n    '''\n    # Convert x and y to tensors using the vocabulary\n    tensors_src = tensorize(vocab, x)\n    tensors_trg = tensorize(vocab, y) \n    # Return the padded sequence of x and y and its length\n    return {'x':pad_sequence(tensors_src, p.max_src_len), #\u00bfmax_source_len?\n          'y':pad_sequence(tensors_trg, p.max_tgt_len), #\u00bf,ax_target_len?\n          'x_len':len(tensors_src), \n          'y_len':len(tensors_trg)}\n\ndef sort_batch_by_len(data_dict,p,vocab):\n    ''' Return a batch of sentences processed and ordered by its length\n    '''\n    data=[]\n    res={'x':[],'y':[],'x_len':[],'y_len':[]}\n    # For every x and y in the data input\n    for i in range(data_dict['x_len']):\n        # Preprocess and tokenize the x and y\n        data.append(preprocess(data_dict['x'][i],data_dict['y'][i],p,vocab))\n    # For every preprocessed text, recreate the x and y lists\n    for i in range(len(data)):\n        res['x'].append(data[i]['x'])\n        res['y'].append(data[i]['y'])\n        res['x_len'].append(len(data[i]['x']))\n        res['y_len'].append(len(data[i]['y']))  \n    \n    # Sort indices of data in batch by lengths.\n    sorted_indices = np.array(res['x_len']).argsort()[::-1].tolist()\n    # Create a batch of data ordered by its length\n    data_batch = {name:[_tensor[i] for i in sorted_indices]\n                  for name, _tensor in res.items()}\n    return data_batch\n","7349cc49":"class EncoderRNN(nn.Module):\n  ''' Define an encoder in a seq2seq architecture'''\n  def __init__(self, embed_size, hidden_size, bidi=True, rnn_drop: float=0):\n    super(EncoderRNN, self).__init__()\n    # Set the hidden size\n    self.hidden_size = hidden_size\n    # Activate bidirectional mode\n    self.num_directions = 2 if bidi else 1\n    # Define the GRU layer of the encoder\n    self.gru = nn.GRU(embed_size, hidden_size, bidirectional=bidi, dropout=rnn_drop)\n\n  def forward(self, embedded,hidden,input_lengths=None):\n    ''' Run a Forward pass of the encoder to return outputs\n        Input:\n        - embedded: tensor, the embedding of the input data (word of the soure text)\n        - hidden: a tensor, the previous hidden state of the encoder\n        - input:lengths: a list of integers, length of the inputs \n    '''\n    # Pack the padded sequence of the embedded input\n    if input_lengths is not None:\n      embedded = pack_padded_sequence(embedded, input_lengths,batch_first=True)\n    \n    # Apply the GRU layer of the encoder\n    output, hidden = self.gru(embedded,hidden)\n    \n    # Pad the sequence output\n    if input_lengths is not None:\n      output, _ = pad_packed_sequence(output)\n    # If bidirectional\n    if self.num_directions > 1:\n      # Transform the hidden state tensor  \n      # hidden: (num directions, batch, hidden) => (1, batch, hidden * 2)\n      batch_size = hidden.size(1)\n      hidden = hidden.transpose(0, 1).contiguous().view(1, batch_size,\n                                                        self.hidden_size * self.num_directions)\n    return output, hidden\n\n  def init_hidden(self, batch_size, device):\n    ''' Initialize the hidden state of the encoder to zeros: num_directions, batch size, hidden size '''\n    return torch.zeros(self.num_directions, batch_size, self.hidden_size, device=device) \n","39ac6580":"class DecoderRNN(nn.Module):\n  ''' Define a decoder with atention in a seq2seq architecture'''\n  def __init__(self, vocab_size, embed_size, hidden_size, enc_attn=True, dec_attn=True,\n               enc_attn_cover=True, pointer=True,\n               in_drop: float=0, rnn_drop: float=0, out_drop: float=0, enc_hidden_size=None,\n               epsilon: float=0.0, device: str=\"cpu\"):\n    ''' Initialize the decoder instance defining its parameters:\n            Input:\n                - vocab_size: integer, number of words in the vocabulary \n                - embed_size: integer, size of the embedding layer\n                - hidden_size: integer, size of the hidden layer (Hyperparameter)\n                - enc_attn: activate the attention in the encoder\n                - dec_attn: activate the attention in the decoder\n                - enc_attn_cover: activate the coverage mechanism in the attention\n                - pointer: activate the pointer generation\n                - in_drop: dropout probability to apply to the input of the decoder\n                - rnn_drop: dropout probability to apply to the GRU layer of the decoder\n                - out_drop: dropout probability to apply to the output of the decoder\n                - enc_hidden_size: dimension if the hidden state of the encoder\n                - epsilon: float\n                - device: cpu or gpu, device to store the tensors\n    '''\n\n    super(DecoderRNN, self).__init__()\n    # Set the attibutes\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size\n    self.combined_size = hidden_size\n    self.device = device\n    self.eps = epsilon\n    # Define the input dropout layer \n    self.in_drop = nn.Dropout(in_drop) if in_drop > 0 else None\n    # Define the GRU layer\n    self.gru = nn.GRU(embed_size, hidden_size, dropout=rnn_drop)\n    \n    # Set the hidden size of the encoder to the hidden size of the decoder if it is not defined\n    if not enc_hidden_size: enc_hidden_size = self.hidden_size\n    # Bilinear layer of the encoder\n    self.enc_bilinear = nn.Bilinear(hidden_size, enc_hidden_size, 1)\n    \n    self.combined_size += enc_hidden_size\n    if enc_attn_cover:\n      # Initialize the weights of the coverage mechanism\n      self.cover_weight = nn.Parameter(torch.rand(1))\n\n    # Bilinear layer of the encoder\n    self.dec_bilinear = nn.Bilinear(self.hidden_size, self.hidden_size, 1)\n    self.combined_size += self.hidden_size\n    \n    # Define the output dropout layer\n    self.out_drop = nn.Dropout(out_drop) if out_drop > 0 else None\n    # Define the pointer generator layer\n    self.ptr = nn.Linear(self.combined_size, 1)\n\n    # Define the linear layer at the output\n    self.out = nn.Linear(self.combined_size, vocab_size)\n\n  def forward(self, embedded, hidden, encoder_hidden=None, decoder_states=None, coverage_vector=None, *,\n              encoder_word_idx=None, ext_vocab_size: int=None, log_prob: bool=True):\n    ''' Run a Forward pass of the decoder to return outputs\n        Input:\n        - embedded: tensor, the embedding of the input data (decoder output in the last step\n        - hidden: a tensor, the previous hidden state of the decoder\n        - decoder_states: tensor, hidden state of the decoder in the last step\n        - coverage_vector: tensor, coverage vector at this step\n        - encoder_word_idx: tensor, indexes of the words in the source text\n        - ext_vocab_size: integer, vocabulary size of the extended vocabulary\n        - log_prob: bool, use of Log Softmax or Softmax in the output\n    '''\n    #print('Self Dev:',self.device)\n    # Set the batch sze and initialize the combined context vectors\n    batch_size = embedded.size(0)\n    combined = torch.zeros(batch_size, self.combined_size, device=self.device)\n    # Apply the dropout layer to the input data\n    if self.in_drop: embedded = self.in_drop(embedded)\n    # Apply the GRU layer\n    output, hidden = self.gru(embedded.unsqueeze(0), hidden)  # unsqueeze and squeeze are necessary\n    combined[:, :self.hidden_size] = output.squeeze(0)        # as RNN expects a 3D tensor (step=1)\n    offset = self.hidden_size\n    enc_attn, prob_ptr = None, None  # for visualization\n\n    # Set the encoder steps and total size\n    num_enc_steps = encoder_hidden.size(0)\n    enc_total_size = encoder_hidden.size(2)\n    #print('Encoder hidden: ', encoder_hidden.shape,' Hidden: ', hidden.shape)\n    # Apply the Bilinear layer\n    enc_attn = self.enc_bilinear(hidden.expand(num_enc_steps, batch_size, -1).contiguous(),encoder_hidden)\n    # print(hidden.shape,hidden.expand(num_enc_steps, batch_size, -1).shape,encoder_hidden.shape,enc_attn.shape)\n    \n    # Update the attention weigths\n    if coverage_vector is not None:\n        enc_attn += self.cover_weight * torch.log(coverage_vector.transpose(0, 1).unsqueeze(2) + self.eps)\n    # Transpose the attention vectors\n    # transpose => (batch size, num encoder states, 1)\n    enc_attn = F.softmax(enc_attn, dim=0).transpose(0, 1)\n    \n    # Calculate the context vectors \n    enc_context = torch.bmm(encoder_hidden.permute(1, 2, 0), enc_attn)\n    # print(enc_context.shape,enc_context.squeeze(2).shape)\n    \n    # Update the combined vector with the context vectors \n    combined[:, offset:offset+enc_total_size] = enc_context.squeeze(2)\n    offset += enc_total_size\n    enc_attn = enc_attn.squeeze(2)\n    \n    # Set the decoder attention vectors\n    if decoder_states is not None and len(decoder_states) > 0:\n      dec_attn = self.dec_bilinear(hidden.expand_as(decoder_states).contiguous(),\n                                      decoder_states)\n      dec_attn = F.softmax(dec_attn, dim=0).transpose(0, 1)\n      dec_context = torch.bmm(decoder_states.permute(1, 2, 0), dec_attn)\n      combined[:, offset:offset + self.hidden_size] = dec_context.squeeze(2)\n      offset += self.hidden_size\n    \n    # Prepare the data to apply the pointer generator\n    out_embed = combined\n    logits = self.out(out_embed)  # (batch size, vocab size)\n\n    # Distribute probabilities between generator and pointer\n    prob_ptr = torch.sigmoid(self.ptr(combined))  # (batch size, 1)\n    prob_gen = 1 - prob_ptr\n    # add generator probabilities to output\n    gen_output = F.softmax(logits, dim=1)  # can't use log_softmax due to adding probabilities\n    output = prob_gen * gen_output\n    # Dfine the padding dimension\n    pad_dim = (0, ext_vocab_size - output.size(1))\n    # Pad the output tensir\n    output=F.pad(output, pad_dim, 'constant')\n\n    # add pointer probabilities to output\n    ptr_output = enc_attn\n    encoder_word_idx_l = encoder_word_idx.long()\n    # Calculate the output tensor    \n    try:\n        output.scatter_add_(1, encoder_word_idx_l, prob_ptr * ptr_output)\n    except:\n        prob_po = prob_ptr * ptr_output \n        print(output.shape,encoder_word_idx_l.shape,prob_ptr.shape, ptr_output.shape, prob_po.shape)\n        print(output)\n        print(encoder_word_idx_l)\n        print(prob_po)\n        output.scatter_add_(1, encoder_word_idx_l, prob_po)\n        \n    # Apply the log in the output\n    output = torch.log(output + self.eps)\n\n    return output, hidden, enc_attn, prob_ptr \n","f809826f":"def get_coverage_vector(enc_attn_weights):\n    \"\"\"Combine the past attention weights into one vector\"\"\"\n    coverage_vector = torch.sum(torch.cat(enc_attn_weights), dim=0)\n    \n    return coverage_vector  \n","93c5776d":"def get_next_batch(data, p, vocab, i, batch_size, device):\n    ''' Generate and return the next batch of the data during training\n        Input:\n        - data: list, input data to the model\n        - p: a class Parameters object, model and training parameters\n        - vocab: a class Vocab object, vocabulary of the data\n        - i: integer, index or iterator\n        - batch_size: integer, batch size\n        - device: string, where to train the model, cpu or gpu \n    '''\n    #Create a copy of the vocabulary\n    vocab_ext=deepcopy(vocab)\n\n    #Get the next batch\n    try:\n        data_dict=data[i:i+batch_size]\n    except:\n        data_dict=data[i:len(data)]\n    # Create a batch from an extended cocabulary\n    data_batch = sort_batch_by_len(data_dict,p,vocab_ext)\n    # Create an extended cocabulary\n    for word in data_dict['x']:\n        vocab_ext.add_words(word)\n            \n    # Create a batch from an extended cocabulary\n    data_batch_extra=sort_batch_by_len(data_dict,p,vocab_ext)\n    #Create tha inputs in the extended version        \n    x_extra=torch.tensor(data_batch_extra['x']).to(device)\n    \n    # Transform the batch to tensors\n    x, x_len = torch.tensor(data_batch['x']).to(device), torch.tensor(data_batch['x_len']).to(device)\n    y, y_len = torch.tensor(data_batch['y']).to(device), torch.tensor(data_batch['y_len']).to(device)\n\n    return x, x_len, y, y_len, x_extra, vocab_ext\n","e39d8394":"def train(dataset,val_dataset,vocab,p,embedding_weights, learning_rate, num_epochs):\n    ''' Run all the steps in the training phase\n        Input:\n        - dataset: Dataset object, training data\n        - val_dataset: Dataset object, validation data\n        - vocab: a class Vocab object, the vocabulary of the datasets\n        - p: a class Parameters object, model and training parameters\n        - embedding_weigths: tensor, the embedding vectors\n        - learning_rate: float, learning rate parameter\n        - num_epochs: integer, number of epochs of the training\n    '''\n    # Set some variables like eps, batch size and device\n    eps = p.eps\n    batch_size =p.batch_size\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    #Create an adapter between encoder hidden state to decoder hidden size \n    enc_dec_adapter = nn.Linear(p.hidden_size * 2, p.dec_hidden_size).to(DEVICE)\n    #Create an embedding layer with pretrained weigths\n    embedding = nn.Embedding(len(vocab), p.embed_size, padding_idx=vocab.PAD,\n                             _weight=embedding_weights).to(DEVICE)\n    \n    # Do not train the embeddings\n    embedding.weight.requires_grad=False\n    #Create the encoder\n    encoder = EncoderRNN(p.embed_size, p.hidden_size, p.enc_bidi,rnn_drop=p.enc_rnn_dropout).to(DEVICE)\n    #Create the decoder\n    decoder = DecoderRNN(len(vocab), p.embed_size, p.dec_hidden_size,\n                                  enc_attn=p.enc_attn, dec_attn=p.dec_attn,\n                                  pointer=p.pointer,\n                                  in_drop=p.dec_in_dropout, rnn_drop=p.dec_rnn_dropout,\n                                  out_drop=p.dec_out_dropout, enc_hidden_size=p.hidden_size * 2,\n                                  device=DEVICE, epsilon=p.eps).to(DEVICE)\n    \n    # If the model components have been training, we restore them from a previous save\n    if(os.path.exists(p.encoder_weights_path) and p.resume_train):\n        encoder.load_state_dict(torch.load(p.encoder_weights_path,map_location=torch.device(DEVICE)))\n    if(os.path.exists(p.decoder_weights_path) and p.resume_train):\n        decoder.load_state_dict(torch.load(p.decoder_weights_path,map_location=torch.device(DEVICE)))\n    if(os.path.exists(p.encoder_decoder_adapter_weights_path) and p.resume_train):   \n        enc_dec_adapter.load_state_dict(torch.load(p.encoder_decoder_adapter_weights_path,map_location=torch.device(DEVICE)))\n    \n    # Create a Dataset class containing the training data\n    cnn_data=MyDataset([pair[0] for pair in dataset.pairs],[pair[1] for pair in dataset.pairs],vocab)\n    \n    # Create a Dataset class containing the validation data\n    #CHECK IF CREATING A VOCAB FOR VALIDATION IS RIGHT\n    val_data=MyDataset([pair[0] for pair in val_dataset.pairs],[pair[1] for pair in val_dataset.pairs],vocab)\n    # print(cnn_data[:3]['x_len'])\n    \n    # DEfine the loss function\n    criterion = nn.NLLLoss(ignore_index=vocab.PAD)\n    # Define the optimizers for the encoder, decoder and the adapter\n    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n    adapter_optimizer=optim.Adam([{'params':enc_dec_adapter.parameters()}], lr=learning_rate)\n    # Record the losses\n    losses=[]\n    val_losses=[]\n    #Load the losses from previous trainings\n    if(os.path.exists(p.losses_path) and p.resume_train):\n      with open(p.losses_path,'rb') as f:\n        val_losses=pickle.load(f)\n        \n    #Run training for num_epochs\n    for _e in range(num_epochs):\n        i=0\n        #Create a progress bar \n        print('\\nEpoch: %d\/%d' % (_e + 1, num_epochs))\n        kbar = pkbar.Kbar(target=len(cnn_data), width=8)\n        #for every batch in the training data \n        while i<len(cnn_data):\n            \n            # Reset the gradients for the forward phase\n            encoder_optimizer.zero_grad()\n            decoder_optimizer.zero_grad()\n            adapter_optimizer.zero_grad()\n\n            # Extract the data for the next batch\n            x, x_len, y, y_len, x_extra, vocab_ext = get_next_batch(cnn_data, p, vocab, i, batch_size, device=DEVICE)\n    \n            # Apply the embedding layer in the encoder\n            encoder_embedded = embedding(x)\n            # Create the init hidden state of the encoder\n            encoder_hidden=encoder.init_hidden(x.size(0), DEVICE)\n            # Forward pass in the encoder\n            encoder_outputs, encoder_hidden =encoder(encoder_embedded,encoder_hidden,x_len)\n            #Create the init input to the encoder\n            decoder_input = torch.tensor([vocab.SOS] * x.size(0), device=DEVICE)\n            # Adapt the encoder hidden to the encoder hidden size\n            decoder_hidden = enc_dec_adapter(encoder_hidden)\n            \n            decoder_states = []\n            enc_attn_weights = []\n            loss=0\n            # For every token in the target\n            for di in range(y.size(1)):\n                #Apply the embedding layer to the decoder input\n                decoder_embedded = embedding(decoder_input)\n                # If activation of encoder attention is on \n                if enc_attn_weights:\n                    coverage_vector = get_coverage_vector(enc_attn_weights)\n                else:\n                    coverage_vector = None\n                    \n                #Forward pass to the decoder\n                decoder_output, decoder_hidden, dec_enc_attn, dec_prob_ptr = decoder(decoder_embedded, decoder_hidden, encoder_outputs,\n                            torch.cat(decoder_states) if decoder_states else None, coverage_vector,\n                            encoder_word_idx=x_extra,log_prob=True,ext_vocab_size=len(vocab_ext))  \n                #Move the tensors to the device\n                decoder_output.to(DEVICE)\n                decoder_hidden.to(DEVICE)\n                dec_enc_attn.to(DEVICE)\n                dec_prob_ptr.to(DEVICE)\n                \n                #Save the decoder hidden state\n                decoder_states.append(decoder_hidden)\n                #Calculate the probability distribution of the decoder outputs\n                prob_distribution = torch.exp(decoder_output)# if log_prob else decoder_output\n                #Get the largest element \n                _, top_idx = decoder_output.data.topk(1)\n                # Set the current target word to our goal\n                gold_standard = y[:,di]\n                # Apply the loss function\n                nll_loss= criterion(decoder_output, gold_standard)    \n                loss+=nll_loss\n                \n                #Set the decoder input to the target word or token \n                decoder_input = y[:,di]\n                #Calculate the coverage loss\n                if (coverage_vector is not None and criterion): \n                    coverage_loss = torch.sum(torch.min(coverage_vector, dec_enc_attn)) \/ batch_size #* cover_loss            \n                    loss+=coverage_loss\n                    \n                #Store the attention weights\n                enc_attn_weights.append(dec_enc_attn.unsqueeze(0)) \n                \n            #Apply the backward to get the loss\n            loss.backward()\n            # Clipping the weights in the encoder, decoder and the adapter\n            clip_grad_norm_(encoder.parameters(), 1)\n            clip_grad_norm_(decoder.parameters(), 1)\n            clip_grad_norm_(enc_dec_adapter.parameters(), 1)\n            # Update the parameters\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n            adapter_optimizer.step() \n            #Print the progress bar\n            if i%(p.print_every*batch_size)==0:\n                kbar.update(i, values=[(\"loss\", loss.data.item())])\n            # Get the next batch\n            i+=batch_size\n            \n        # Calculate the final loss on the training    \n        loss=loss.data.item()\/x.size(0)\n        kbar.add(1, values=[(\"loss\", loss)])\n        \n        #Repeat the process on the validation dataset\n        kbar2 = pkbar.Kbar(target=len(val_data), width=8)\n        \n        # calculating validation loss\n        val_loss=0\n        i=0\n        while(i<len(val_data)):\n            # Reset the gradients for the forward phase\n            encoder_optimizer.zero_grad()\n            decoder_optimizer.zero_grad()\n            adapter_optimizer.zero_grad()\n            # Get the next batch of the validation data\n            x, x_len, y, y_len, x_extra, vocab_ext = get_next_batch(val_data, p, vocab, i, batch_size, device=DEVICE)\n            # Forward pass of the encoder\n            encoder_embedded = embedding(x)\n            encoder_hidden=encoder.init_hidden(x.size(0), device=DEVICE)\n            encoder_outputs, encoder_hidden =encoder(encoder_embedded,encoder_hidden,x_len)\n            decoder_input = torch.tensor([vocab.SOS] * x.size(0), device=DEVICE)\n            decoder_hidden = enc_dec_adapter(encoder_hidden)\n            \n            decoder_states = []\n            enc_attn_weights = []\n            # For every word in the output\n            for di in range(y.size(1)):\n                try:\n                    #Get the embedding vector of the input to the decoder\n                    decoder_embedded = embedding(decoder_input)\n                except:\n                    print('Dec input: ',decoder_input.shape,' x:', x.shape,' x_len:',x_len.shape, ' Vocab:', \n                          vocab.embeddings.shape,' Vocab Ext:', vocab_ext.embeddings.shape)\n                    decoder_embedded = embedding(decoder_input)\n                # Generate the coverage vectors if neccessary\n                if enc_attn_weights:\n                    coverage_vector = get_coverage_vector(enc_attn_weights)\n                else:\n                    coverage_vector = None\n                # Fordward pass ti the decoder\n                decoder_output, decoder_hidden, dec_enc_attn, dec_prob_ptr = decoder(decoder_embedded, decoder_hidden, encoder_outputs,\n                            torch.cat(decoder_states) if decoder_states else None, coverage_vector,\n                            encoder_word_idx=x_extra,log_prob=True,ext_vocab_size=len(vocab_ext))  \n                # Move the tensors to the device, gou or cpu\n                decoder_output.to(DEVICE)\n                decoder_hidden.to(DEVICE)\n                dec_enc_attn.to(DEVICE)\n                dec_prob_ptr.to(DEVICE)\n                # Stores the hidden states of the decoder\n                decoder_states.append(decoder_hidden)      \n                prob_distribution = torch.exp(decoder_output)# if log_prob else decoder_output\n                # Get the output token with the highest probability\n                _, top_idx = decoder_output.data.topk(1)\n                gold_standard = y[:,di]\n                # Apply the loss function\n                nll_loss= criterion(decoder_output, gold_standard)    \n                val_loss+=nll_loss.data.item()\n                \n                # Set the decoder input to the last output from the decoder\n                decoder_input = top_idx.view(-1) #y[:,di]\n                # update the coverage vector\n                if (coverage_vector is not None and criterion): #and cover_loss > 0:\n                    coverage_loss = torch.sum(torch.min(coverage_vector, dec_enc_attn)) \/ batch_size #* cover_loss            \n                    val_loss+=coverage_loss.data.item()\n                # Collect the attention weights in the step    \n                enc_attn_weights.append(dec_enc_attn.unsqueeze(0))  \n            # Print the progress\n            if i%(p.print_every*batch_size)==0:\n                kbar2.update(i, values=[(\"Val loss\", val_loss)])\n\n            i+=batch_size\n            \n        #Calculate the validation loss\n        avg_val_loss=val_loss\/len(val_data)        \n        #print('training loss:{}'.format(loss),'validation loss:{}'.format(avg_val_loss))\n        kbar2.add(1, values=[(\"Train loss\", loss), (\"Val loss\", val_loss), (\"Avg Val loss\", avg_val_loss)])\n        \n        # Save the mnodel and results to disk\n        if(len(val_losses)>0 and avg_val_loss<min(val_losses)):\n            torch.save(encoder.state_dict(), p.encoder_weights_path)\n            torch.save(decoder.state_dict(), p.decoder_weights_path)\n            torch.save(enc_dec_adapter.state_dict(), p.encoder_decoder_adapter_weights_path)\n            # torch.save(embedding.state_dict(), '\/home\/svu\/e0401988\/NLP\/summarization\/embedding_sum.pt')\n        val_losses.append(avg_val_loss) \n    \n    with open(p.losses_path,'wb') as f:\n        pickle.dump(val_losses,f) \n","a08fd877":"def predict(sent,vocab,p,batch_size=1):\n    ''' Function to predict the summary of the source text sentence\n        Input:\n        - sent: string, text to summarize\n        - vocab: a class Vocab object, vocabulary of the texts\n        - p: a class Parameters object, model parameters\n        - batch_size: integer, batch size of the data to predict\n    '''\n    eps=p.eps\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    # Create a tensor with the embedding vectors\n    embedding_weights = torch.from_numpy(vocab.embeddings).to(DEVICE)\n    # Create the layer to transform inputs between encoder and decoder\n    enc_dec_adapter = nn.Linear(p.hidden_size * 2, p.dec_hidden_size).to(DEVICE)\n    # Create the embedding layer using the embedding vectors\n    embedding = nn.Embedding(len(vocab), p.embed_size, padding_idx=vocab.PAD,_weight=embedding_weights).to(DEVICE)\n    # Create the encoder and the decoder\n    encoder = EncoderRNN(p.embed_size, p.hidden_size, p.enc_bidi,rnn_drop=p.enc_rnn_dropout).to(DEVICE)\n    decoder = DecoderRNN(len(vocab), p.embed_size, p.dec_hidden_size,\n                                  enc_attn=p.enc_attn, dec_attn=p.dec_attn,\n                                  pointer=p.pointer,\n                                  in_drop=p.dec_in_dropout, rnn_drop=p.dec_rnn_dropout,\n                                  out_drop=p.dec_out_dropout, enc_hidden_size=p.hidden_size * 2,\n                                  device=DEVICE).to(DEVICE) \n    # Tokenize the input text \n    sent_vec=[vocab[word] for word in sent.split()]\n    vocab_ext=deepcopy(vocab)\n    # Extend the vocabulary\n    for word in sent.split():\n      vocab_ext.add_words(word)\n    # Tokenize the text with the extended  vocabulary\n    sent_vec_extra=[vocab_ext[word] for word in sent.split()] \n    # Pad the text if neccessary\n    if(len(sent_vec_extra)<p.max_src_len):\n        pad_dim = (0, p.max_src_len-len(sent_vec_extra))\n        #sent_vec_extra=F.pad(sent_vec_extra, pad_dim , 'constant')\n        sent_vec_extra_tensor=F.pad(torch.tensor(sent_vec_extra), pad_dim , 'constant')\n    else:\n        sent_vec_extra_tensor=torch.tensor(sent_vec_extra)\n        \n    # Pad the text if neccessary\n    if(len(sent_vec)<p.max_src_len):\n        pad_dim = (0, p.max_src_len-len(sent_vec))\n        #sent_vec=F.pad(sent_vec, pad_dim, 'constant')\n        sent_vec_tensor=F.pad(torch.tensor(sent_vec), pad_dim, 'constant')\n    else:\n        sent_vec_tensor=torch.tensor(sent_vec)\n        \n    # Load the encoder model from file\n    if(os.path.exists(p.encoder_weights_path)):\n        encoder.load_state_dict(torch.load(p.encoder_weights_path,map_location=torch.device(DEVICE)))\n    # Load the decoder model from file\n    if(os.path.exists(p.decoder_weights_path)):\n        decoder.load_state_dict(torch.load(p.decoder_weights_path,map_location=torch.device(DEVICE)))\n    # Load the encoder -decoder adapter component from file\n    if(os.path.exists(p.encoder_decoder_adapter_weights_path)):    \n        enc_dec_adapter.load_state_dict(torch.load(p.encoder_decoder_adapter_weights_path,map_location=torch.device(DEVICE)))\n\n    x=sent_vec_tensor.view(1,-1).to(DEVICE)\n    x_extra=sent_vec_extra_tensor.view(1,-1).to(DEVICE)\n    # Apply the embedding layer\n    encoder_embedded = embedding(x)\n    # Initiaize the hidden state of the encoder\n    encoder_hidden=encoder.init_hidden(x.size(0), DEVICE)\n    # Fordward pass to the encoder\n    encoder_outputs, encoder_hidden =encoder(encoder_embedded,encoder_hidden,\n                                             torch.tensor(p.max_src_len).view(1).to(DEVICE))\n    # Initialize the decoder input to SOS tokens\n    decoder_input = torch.tensor([vocab.SOS] * batch_size, device=DEVICE)\n    decoder_hidden = enc_dec_adapter(encoder_hidden)\n    \n    #Initialize the hidden states and weights of some variables\n    decoder_states = []\n    enc_attn_weights = []\n    output=[]\n    # For every word in the output sequence \n    for di in range(p.max_tgt_len):\n        # Apply the embedding layer of the decoder\n        decoder_embedded = embedding(decoder_input)\n        # Get the coverage vector\n        if enc_attn_weights:\n            coverage_vector = get_coverage_vector(enc_attn_weights)\n        else:\n            coverage_vector = None\n        # fordward pass to the decoder\n        decoder_output, decoder_hidden, dec_enc_attn, dec_prob_ptr = decoder(decoder_embedded, decoder_hidden, encoder_outputs,\n                    torch.cat(decoder_states).to(DEVICE) if decoder_states else None, coverage_vector,\n                    encoder_word_idx=x_extra,log_prob=True,ext_vocab_size=len(vocab_ext))  \n        # Move the tensor to the device\n        decoder_output.to(DEVICE)\n        decoder_hidden.to(DEVICE)\n        dec_enc_attn.to(DEVICE)\n        dec_prob_ptr.to(DEVICE)\n        # Store the hidden state of the decoder\n        decoder_states.append(decoder_hidden)\n        # Generate the probability distribution\n        prob_distribution = torch.exp(decoder_output)# if log_prob else decoder_output\n        # Get the element in the decoder output with the highest probability (the best output)\n        _, top_idx = decoder_output.data.topk(1)\n        # Store the output (word) to the output text\n        output.append(top_idx.squeeze().data.item())\n        # Store the encoder attention weights\n        enc_attn_weights.append(dec_enc_attn.unsqueeze(0))\n        # Set the decoder input in the next iter\n        decoder_input = top_idx.view(-1)\n    # Transform the outputs (words) to a list of text or words\n    output=[vocab_ext[idx] for idx in output]    \n    return output ","bb307b72":"def prediction(sent,vocab,embedding, encoder, enc_dec_adapter, decoder, device, p,batch_size=1):\n    ''' Function to predict the summary of the source text sentence\n        Input:\n        - sent: string, text to summarize\n        - vocab: a class Vocab object, vocabulary of the texts\n        - p: a class Parameters object, model parameters\n        - batch_size: integer, batch size of the data to predict\n    '''\n    eps=p.eps\n    # Tokenize the input text \n    sent_vec=[vocab[word] for word in sent.split()]\n    vocab_ext=deepcopy(vocab)\n    # Extend the vocabulary\n    for word in sent.split():\n      vocab_ext.add_words(word)\n    # Tokenize the text with the extended  vocabulary\n    sent_vec_extra=[vocab_ext[word] for word in sent.split()] \n    # Pad the text if neccessary\n    if(len(sent_vec_extra)<p.max_src_len):\n        pad_dim = (0, p.max_src_len-len(sent_vec_extra))\n        #sent_vec_extra=F.pad(sent_vec_extra, pad_dim , 'constant')\n        sent_vec_extra_tensor=F.pad(torch.tensor(sent_vec_extra), pad_dim , 'constant')\n    else:\n        sent_vec_extra_tensor=torch.tensor(sent_vec_extra)\n        \n    # Pad the text if neccessary\n    if(len(sent_vec)<p.max_src_len):\n        pad_dim = (0, p.max_src_len-len(sent_vec))\n        #sent_vec=F.pad(sent_vec, pad_dim, 'constant')\n        sent_vec_tensor=F.pad(torch.tensor(sent_vec), pad_dim, 'constant')\n    else:\n        sent_vec_tensor=torch.tensor(sent_vec)\n        \n    x=sent_vec_tensor.view(1,-1).to(device)\n    x_extra=sent_vec_extra_tensor.view(1,-1).to(device)\n    # Apply the embedding layer\n    encoder_embedded = embedding(x)\n    # Initiaize the hidden state of the encoder\n    encoder_hidden=encoder.init_hidden(x.size(0), device)\n    # Fordward pass to the encoder\n    encoder_outputs, encoder_hidden =encoder(encoder_embedded,encoder_hidden,\n                                             torch.tensor(p.max_src_len).view(1).to(device))\n    # Initialize the decoder input to SOS tokens\n    decoder_input = torch.tensor([vocab.SOS] * batch_size, device=device)\n    decoder_hidden = enc_dec_adapter(encoder_hidden)\n    \n    #Initialize the hidden states and weights of some variables\n    decoder_states = []\n    enc_attn_weights = []\n    output=[]\n    # For every word in the output sequence \n    for di in range(p.max_tgt_len):\n        # Apply the embedding layer of the decoder\n        decoder_embedded = embedding(decoder_input)\n        # Get the coverage vector\n        if enc_attn_weights:\n            coverage_vector = get_coverage_vector(enc_attn_weights)\n        else:\n            coverage_vector = None\n            \n        # fordward pass to the decoder\n        decoder_output, decoder_hidden, dec_enc_attn, dec_prob_ptr = decoder(decoder_embedded, decoder_hidden, encoder_outputs,\n                    torch.cat(decoder_states).to(device) if decoder_states else None, coverage_vector,\n                    encoder_word_idx=x_extra,log_prob=True,ext_vocab_size=len(vocab_ext))  \n        # Move the tensor to the device\n        decoder_output.to(device)\n        decoder_hidden.to(device)\n        dec_enc_attn.to(device)\n        dec_prob_ptr.to(device)\n        # Store the hidden state of the decoder\n        decoder_states.append(decoder_hidden)\n        # Generate the probability distribution\n        prob_distribution = torch.exp(decoder_output)# if log_prob else decoder_output\n        # Get the element in the decoder output with the highest probability (the best output)\n        _, top_idx = decoder_output.data.topk(1)\n        # Store the output (word) to the output text\n        output.append(top_idx.squeeze().data.item())\n        # Store the encoder attention weights\n        enc_attn_weights.append(dec_enc_attn.unsqueeze(0))\n        # Set the decoder input in the next iter\n        decoder_input = top_idx.view(-1)\n        \n    # Transform the outputs (words) to a list of text or words\n    output=[vocab_ext[idx] for idx in output]    \n    return output ","f8387cb9":"def eval_metrics(preds, targets, avg=True):\n    ''' Evaluate the ROUGE metrics ROUGE-2 and ROUGE-L for every pair predicted summary - target summary\n    \n        Input:\n           - preds: list of strings, predicted summaries\n           - targets: list of string, target summaries\n        Output:\n            - rouge2_f_metric: list of float, the Rouge-2 fscore for every predicted summary\n            - rougel_f_metric: list of float, the Rouge-L fscore for every predicted summary\n    '''\n    #Lets calculate the rouge metrics for every document\n    rouge = Rouge()\n    scores = rouge.get_scores(preds, targets, avg)\n    # Create the output variables\n    if avg:\n        rouge2_f_metric = scores['rouge-2']['f']\n        rouge2_p_metric = scores['rouge-2']['p']\n        rouge2_r_metric = scores['rouge-2']['r']\n        rougel_f_metric = scores['rouge-l']['f']\n        rougel_p_metric = scores['rouge-l']['p']\n        rougel_r_metric = scores['rouge-l']['r']\n    else:\n        rouge2_f_metric = [score['rouge-2']['f'] for score in scores]\n        rouge2_p_metric = [score['rouge-2']['p'] for score in scores]\n        rouge2_r_metric = [score['rouge-2']['r'] for score in scores]\n        rougel_f_metric = [score['rouge-l']['f'] for score in scores]\n        rougel_p_metric = [score['rouge-l']['p'] for score in scores]\n        rougel_r_metric = [score['rouge-l']['r'] for score in scores]\n    \n    return rouge2_f_metric, rouge2_p_metric, rouge2_r_metric, rougel_f_metric, rougel_p_metric, rougel_r_metric\n\ndef save_to_df(text, labeled_summaries, predicted_summaries, r2_f, r2_p, r2_r, rl_f, rl_p, rl_r):\n    ''' Stores the metric results into a pandas dataframe'''\n    results = pd.DataFrame(columns=['text', 'summary','pred_summary','rouge2-f','rouge2-p','rouge2-r','rougel-f', 'rougel-p', 'rougel-r'])\n    results['text'] = text\n    results['summary'] = labeled_summaries\n    results['pred_summary'] = predicted_summaries\n    results['rouge2-f'] = r2_f\n    results['rouge2-p'] = r2_p\n    results['rouge2-r'] = r2_r\n    results['rougel-f'] = rl_f\n    results['rougel-p'] = rl_p\n    results['rougel-r'] = rl_r\n\n    return results","7c1a52e5":"def get_predictions(x_test, vocab, params, print_every=20):\n    ''' Generate the predicted summaries of the source texts on x_test\n        Input:\n        - x_test: list of strings, the source texts\n        - vocab: a Vocab Class object, vocabulary of the texts\n        - params: a Parameters object, parameter of the model\n        - print_every: integer, print progress every print_every iterations\n    '''\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    # Create a tensor with the embedding vectors\n    embedding_weights = torch.from_numpy(vocab.embeddings).to(DEVICE)\n    # Create the layer to transform inputs between encoder and decoder\n    enc_dec_adapter = nn.Linear(params.hidden_size * 2, params.dec_hidden_size).to(DEVICE)\n    # Create the embedding layer using the embedding vectors\n    embedding = nn.Embedding(len(vocab), params.embed_size, padding_idx=vocab.PAD,_weight=embedding_weights).to(DEVICE)\n    # Create the encoder and the decoder\n    encoder = EncoderRNN(params.embed_size,params.hidden_size, params.enc_bidi,rnn_drop=params.enc_rnn_dropout).to(DEVICE)\n    decoder = DecoderRNN(len(vocab), params.embed_size, params.dec_hidden_size,\n                                  enc_attn=params.enc_attn, dec_attn=params.dec_attn,\n                                  pointer=params.pointer,\n                                  in_drop=params.dec_in_dropout, rnn_drop=params.dec_rnn_dropout,\n                                  out_drop=params.dec_out_dropout, enc_hidden_size=params.hidden_size * 2,\n                                  device=DEVICE).to(DEVICE) \n\n    # Load the encoder model from file\n    if(os.path.exists(params.encoder_weights_path)):\n        encoder.load_state_dict(torch.load(params.encoder_weights_path,map_location=torch.device(DEVICE)))\n    # Load the decoder model from file\n    if(os.path.exists(params.decoder_weights_path)):\n        decoder.load_state_dict(torch.load(params.decoder_weights_path,map_location=torch.device(DEVICE)))\n    # Load the encoder -decoder adapter component from file\n    if(os.path.exists(params.encoder_decoder_adapter_weights_path)):    \n        enc_dec_adapter.load_state_dict(torch.load(params.encoder_decoder_adapter_weights_path,map_location=torch.device(DEVICE)))\n\n        \n    predicted_summaries = []\n    # Set a progress bar\n    kbar = pkbar.Kbar(target=len(x_test), width=8)\n    # For every text in the validation dataset\n    for i,doc in enumerate(x_test):\n        # Predict the summary for the document\n        pred_summ = prediction(doc,vocab,embedding,encoder,enc_dec_adapter,decoder,DEVICE,params,batch_size=1)\n        #pred_summ = predict(doc,vocab,params,batch_size=1)\n        predicted_summaries.append(' '.join(pred_summ))\n        #Show the progress\n        if i%print_every==0:\n            kbar.update(i)\n            \n    # Set the labeled summaries as the y_test variable, column summary of our dataset\n    return predicted_summaries\n","6ac9848c":"def generate_predictions(x_test, vocab, params, print_every=20):\n    ''' Generate the predicted summaries of the source texts on x_test\n        Input:\n        - x_test: list of strings, the source texts\n        - vocab: a Vocab Class object, vocabulary of the texts\n        - params: a Parameters object, parameter of the model\n        - print_every: integer, print progress every print_every iterations\n    '''\n\n    predicted_summaries = []\n    # Set a progress bar\n    kbar = pkbar.Kbar(target=len(x_test), width=8)\n    # For every text in the validation dataset\n    for i,doc in enumerate(x_test):\n        # Predict the summary for the document\n        pred_summ = predict(doc,vocab,params,batch_size=1)\n        predicted_summaries.append(' '.join(pred_summ))\n        #Show the progress\n        if i%print_every==0:\n            kbar.update(i)\n            \n    # Set the labeled summaries as the y_test variable, column summary of our dataset\n    return predicted_summaries\n","dde82116":"# Create an object with the model and training parameters\nparams = Parameters()","aa5f3fdc":"# Load the training dataset using the simple tokenizer\ndataset = Dataset(params.data_path, simple_tokenizer, params.max_src_len, params.max_tgt_len, max_rows=64000,\n                        truncate_src=True, truncate_tgt=True)\n# Load the validation dataset using the simple tokenizer\nvalid_dataset = Dataset(params.val_data_path, simple_tokenizer, params.max_src_len, params.max_tgt_len, max_rows= 3200,\n                        truncate_src=True, truncate_tgt=True)\n#Show the length to check the loadings\nprint(dataset.src_len, valid_dataset.src_len,dataset.tgt_len, valid_dataset.tgt_len)","705158fa":"# convert the embeddings to a tensor\nvocab = dataset.build_vocab(params.vocab_min_frequency, embed_file=params.embed_file)\nvocab.save_to_file('vocab_train.pkl')\n# convert the embeddings to a tensor\nembedding_weights = torch.from_numpy(vocab.embeddings)","9429db1c":"# Load the vocab from file\n#vocab= load_vocab('\/kaggle\/input\/pointer-trained-model\/vocab_train.pkl')\n#print(vocab.embeddings.shape, len(vocab.index2word))\n#embedding_weights = torch.from_numpy(vocab.embeddings)","58bdc417":"train(dataset,valid_dataset,vocab, params, embedding_weights,learning_rate=0.001,num_epochs = 10)","f6a09e16":"test_dataset = Dataset(params.test_data_path, simple_tokenizer, params.max_src_len, params.max_tgt_len, max_rows= 3200,\n                        truncate_src=True, truncate_tgt=True)\n# Prepare the validation dataset to be used in the evaluation\nprint('Length Test Dataset:', len(test_dataset.pairs))\nx_test = [' '.join(pair[0]) for pair in test_dataset.pairs]\ny_test = [' '.join(pair[1]) for pair in test_dataset.pairs]\n# Predict the summaries\n#preds = generate_predictions(x_test, vocab, params, print_every=10)\npreds = get_predictions(x_test, vocab, params, print_every=100)\n# Calculate the Rouge-2 and Rouge-L metrics for the validation dataset\nr2_f, r2_p, r2_r, rl_f, rl_p, rl_r = eval_metrics(preds, y_test, False)\nprint('\\nMean Rouge-2 FScore: ',np.mean(r2_f), 'Mean Rouge-L FScore: ',np.mean(rl_f))\n# Store the evaluation results to a CSV file\ntest_results = save_to_df(x_test, y_test, preds, r2_f, r2_p, r2_r, rl_f, rl_p, rl_r)\ntest_results.to_csv('test_results_pointer_gen.csv', index=False)\ntest_results.head(5)","edf8a007":"### Define the encoder","d929f1b9":"# Text summarization using machine learning techniques\n\n### A Pointer Generator network for document summarization\n\nExtracted from: https:\/\/github.com\/NirmalenduPrakash\/Document-Summarizer\/blob\/master\/summarizer.py","93ed690a":"Now, it is time to train the model using the parameters in the params variable","35a40aaa":"Lets define some helper functions to create the tensors","4a11c53d":"### Create a Class to store all the parameters in the code\n\nThis is a optimal way to store and access the parameters ","687e663d":"The next function will be used to tokenize the texts. If we want to apply another moethod we only need to change this function","0a5ed43a":"Lets create a Class to contain our Dataset, it will simplified how the training process work with the data","490dcae6":"Next, we build our vocabulary, load the embedding vectors and transform them to a tensor","8dfcb426":"## Main code\n\nNow that we have defined the group of functions we need to work with our datasets, we can invoke them an train and evaluate out model","008d1b96":"## Define the steps of the training process\n\nFirst we define a function to get the next batch from the data and the vocabulary","9f95b59e":"### Prediction","f0ea7736":"Load the test dataset to evaluate the model","40176646":"Import specific libraries for metrics evaluation and plotting progress","1a4d7f07":"Create a function to get the coverage vector from the attention weights of the encoder","91d9736f":"## Create the model components","be682d33":"### Functions to evaluate the model and calculate the metrics","dd85a6fb":"### Define the Decoder","e1308dd5":"### Define and create the vocabularies\n\nNow we create a Vocab (vocabulary) Class to store the vocabulary, the mapping between words and its numeric representation and functions to add words and sentences to the vocabulary. There is also some function to transform a word to its vector representation and to transform the representation to a Torch tensor. "}}