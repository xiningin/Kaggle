{"cell_type":{"8a3706aa":"code","d12fcc34":"code","c09acb4e":"code","0c89f465":"code","49a60cb4":"code","0b1921ae":"code","ea8bc191":"code","b5586efb":"code","ba1308c2":"code","f3192beb":"code","375b35cd":"code","c2a31130":"code","200f1158":"code","1fbce863":"code","32b425a8":"code","7064c4a3":"code","b39771ce":"markdown","fc7e0f0f":"markdown","7d646889":"markdown","0a617651":"markdown","5fa5e245":"markdown","1dae37fe":"markdown","8dd33972":"markdown","1e071039":"markdown","ad51832a":"markdown"},"source":{"8a3706aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n\n# Any results you write to the current directory are saved as output.\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, TensorDataset,random_split\n# from torch.\n\nfrom pandas.api.types import is_numeric_dtype\nfrom matplotlib import pyplot as plt\nfrom IPython.display import clear_output","d12fcc34":"def read_data():\n    train = pd.read_csv('..\/input\/train.csv')\n    test = pd.read_csv('..\/input\/test.csv')\n    return train, test\n\ndef get_cat_cols(data_df):\n    return list(data_df.select_dtypes(include=['category', 'object']))\n\n\ndef get_num_cols(data_df):\n    return list(data_df.select_dtypes(exclude=['category', 'object']))","c09acb4e":"train, test = read_data()\nprint('train.shape: ', train.shape)\nprint('test.shape : ', test.shape)\n\ntarget = 'SalePrice'\nignore_cols = 'Id'\nprint('target: ', target)","0c89f465":"cat_cols, cont_cols = [],[]\nfor col in train:\n    if col == target or col in ignore_cols:\n        continue\n    if (train[col].dtype == 'int' and train[col].unique().shape[0] > 20) or train[col].dtype == 'float':\n        cont_cols.append(col)\n    else:\n        cat_cols.append(col)\n\nprint('cat_cols: {0}, cont_cols: {1}'.format(len(cat_cols), len(cont_cols)))","49a60cb4":"\"\"\"\nFill missing values for categorical and continuous features\n\"\"\"\nclass FillMissing():\n    cont_cols = []\n    col_filler = {}\n    \n    def __init__(self,cont_cols,cat_cols):\n        self.cont_cols = cont_cols\n        self.cat_cols = cat_cols\n        \n    def _fill_cat_numeric(self, df, col):\n        if is_numeric_dtype(df[col]):\n            df[col] = df[col].fillna(-999)\n        else:\n            df[col] = df[col].fillna(\"#na#\")\n    \n    def apply_train(self, df):\n        for col in self.cont_cols:\n            filler = df[col].median()\n            self.col_filler[col] = filler\n            df[col] = df[col].fillna(filler)\n        for col in self.cat_cols:\n            self._fill_cat_numeric(df, col)\n        \n    def apply_test(self, df):\n        for col in self.cont_cols:\n            df[col] = df[col].fillna(self.col_filler.get(col))\n        for col in self.cat_cols:\n            self._fill_cat_numeric(df, col)\n \n\"\"\"\nNormalize continuous featurs and also the target (for regression)\n\"\"\"\nclass Normalize():\n    cont_cols = []\n    means = {}\n    stds = {}\n    \n    def __init__(self, cont_cols):\n        self.cont_cols = cont_cols\n        \n    def apply_train(self, df):\n        for col in self.cont_cols:\n            self.means[col] = df[col].mean()\n            self.stds[col] = df[col].std()\n        self.apply_test(df)\n        \n    def apply_test(self, df):\n        for col in self.cont_cols:\n            df[col] = (df[col] - self.means[col])\/(1e-7 + self.stds[col])\n            \n    def denorm(self, series):\n        denormed = series * (1e-7 + self.stds[series.name]) + self.means[series.name]\n        return denormed\n\n\n\"\"\"\nConvert categorical values into code, so that they can be used in Embeddings. \n\"\"\"\nclass Categorify():\n    cat_cols = []\n    categories = {}\n    \n    def __init__(self, cat_cols):\n        self.cat_cols = cat_cols\n        \n    def apply_train(self, df):\n        for col in self.cat_cols:\n            df[col] = df[col].astype('category')\n            self.categories[col] = df[col].cat.categories\n            df[col] = df[col].cat.codes\n    \n    def apply_test(self, df):\n        for col in self.cat_cols:\n            df[col] = pd.Categorical(df[col],categories=self.categories[col])\n            df[col] = df[col].cat.codes\n            #if code is -1 (data is not present in training set, the set it  different category.Embedding size is nunique + 1)\n            df[col].replace(-1,len(self.categories[col]),inplace=True)","0b1921ae":"fillMissing = FillMissing(cont_cols,cat_cols)\nnormalize_x = Normalize(cont_cols)\nnormalize_y = Normalize([target])\ncategorify = Categorify(cat_cols)\n\n#preprocess training\nfillMissing.apply_train(train)\nnormalize_x.apply_train(train)\ncategorify.apply_train(train)\nnormalize_y.apply_train(train)\n\n#preprocess test\nfillMissing.apply_test(test)\nnormalize_x.apply_test(test)\ncategorify.apply_test(test)","ea8bc191":"from collections import OrderedDict\nemb_sizes = []\nfor col in cat_cols:\n    ni = train[col].nunique() + 1\n    nd = int(min(np.ceil(ni\/2),50))\n    emb_sizes.append((ni,nd))","b5586efb":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\ndef get_ds(pdf, cat_cols, cont_cols, target,device,bs=1000):\n    cat_tensor = torch.from_numpy(pdf[cat_cols].astype(np.int64).values).to(torch.long).to(device)\n    cont_tensor = torch.from_numpy(pdf[cont_cols].values).to(torch.float32).to(device)\n    if target:\n        target_tensor = torch.from_numpy(pdf[target].values.astype(np.float32)).to(torch.float32).to(device)\n    else:\n        target_tensor = torch.zeros(pdf.shape[0]).to(device)  \n        \n    ds = TensorDataset(cat_tensor, cont_tensor, target_tensor)\n    return ds\n\ntrain_ds = get_ds(train,cat_cols,cont_cols, target,device=device)\ntest_ds = get_ds(test, cat_cols, cont_cols, target=None,device=device) \n\n#create val set\nval_split = 0.15\nval_size = int(train.shape[0] * val_split)\ntrain_size = train.shape[0] - val_size\ntrain_ds, val_ds = random_split(train_ds,(train_size,val_size))\ntrain_dl = DataLoader(train_ds, 2000,shuffle=True)\nval_dl = DataLoader(val_ds, 5000,shuffle=False)\nprint('train_size:val_size - {0}:{1}'.format(train_size,val_size))\n","ba1308c2":"from operator import add\n\nclass HousingModel(nn.Module):\n    def __init__(self,cont_cols, emb_sizes,fc_sizes):\n        super(HousingModel, self).__init__()\n        self.embeds = nn.ModuleList([nn.Embedding(num_embeddings=emb_size[0],embedding_dim=emb_size[1]) for emb_size in emb_sizes])\n        hidden_1 = len(cont_cols) + sum([e[1] for e in emb_sizes])\n        hidden_sizes = [hidden_1] + fc_sizes\n        self.fc_layers = nn.ModuleList([nn.Linear(hidden_sizes[o],hidden_sizes[o+1]) for o in range(len(fc_sizes))])\n        self.bn_layers = nn.ModuleList([nn.BatchNorm1d(hidden_sizes[o]).to(device) for o in range(len(fc_sizes))])\n        self.relu = nn.ReLU()\n    \n    def forward(self, xb_cat, xb_cont):\n        if len(self.embeds) > 0:\n            x_embs = [e(xb_cat[:,i]) for i, e in enumerate(self.embeds)]\n            x_embs = torch.cat(x_embs, 1)\n        x = torch.cat([x_embs, xb_cont],1)\n        for idx, fc_layer in enumerate(self.fc_layers):\n            x = self.fc_layers[idx](self.relu(self.bn_layers[idx](x)))\n        return x.squeeze()\n\nmodel = HousingModel(cont_cols,emb_sizes,fc_sizes=[64,1])\nxb_cat, xb_cont, y = next(iter(train_dl))\nprint('xb_cat.shape: ', xb_cat.shape)\nprint('xb_cont.shape: ', xb_cont.shape)\nyb_pred = model(xb_cat, xb_cont)\nyb_pred.shape\nxb_cont","f3192beb":"from ignite.engine import Engine, Events\nfrom ignite.metrics import Loss\nfrom ignite.handlers import ModelCheckpoint, EarlyStopping\nfrom ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\nfrom ignite._utils import convert_tensor","375b35cd":"def prep_batch(batch, device=None,non_blocking=False):\n    x_cat, x_cont, y = batch\n    return (convert_tensor(x_cat, device=device, non_blocking=non_blocking),\n        convert_tensor(x_cont, device=device, non_blocking=non_blocking),\n        convert_tensor(y, device=device, non_blocking=non_blocking))\n\ndef create_trainer(model, optimizer, loss_func,device,prepare_batch=prep_batch):\n    if device:\n        model.to(device)\n    \n    def _update_model(engine, batch):\n        model.train()\n        optimizer.zero_grad()\n        x_cat, x_cont, y = prepare_batch(batch, device=device)\n        y_pred = model(x_cat, x_cont)\n        loss = loss_func(y_pred, y)\n        loss.backward()\n        optimizer.step()\n        return loss.item()\n    return Engine(_update_model)\n\ndef create_evaluator(model,metrics={},device=None, prepare_batch=prep_batch):\n    if device:\n        model.to(device)\n        \n    def _evaluate(engine, batch):\n        model.eval()\n        \n        with torch.no_grad():\n            x_cat, x_cont, y = prepare_batch(batch, device=device)\n            y_pred = model(x_cat, x_cont)\n            return y_pred, y\n        \n    engine = Engine(_evaluate)\n    for name, metric in metrics.items():\n        metric.attach(engine, name)\n    return engine\n\ndef eval_submit(model, test_ds,test_df):\n    #evaluate\n    model.eval()\n    y_preds = model(test_ds.tensors[0],test_ds.tensors[1])\n    preds = normalize_y.denorm(pd.Series(y_preds.detach().numpy(),name=target))\n\n    #create submission\n    my_submission = pd.DataFrame({'Id': test_df.Id, target: preds})\n    my_submission['Id'] = my_submission['Id'].astype(np.int32)\n    my_submission.to_csv('submission.csv', index=False)\n\n\nclass PlotLosses(object):\n    def __init__(self,evaluator,train_dl,val_dl):\n        self.i = 0\n        self.x = []\n        self.losses = []\n        self.val_losses = []\n        self.fig = plt.figure()\n        self.logs = []\n        self.evaluator = evaluator\n        \n    def __call__(self, engine):\n        #get training loss\n        evaluator.run(train_dl)\n        train_metrics = evaluator.state.metrics\n        self.losses.append(train_metrics['loss'])\n        \n        #get validation loss\n        evaluator.run(val_dl)\n        val_metrics = evaluator.state.metrics\n        self.val_losses.append(val_metrics['loss'])\n        \n        self.x.append(self.i)\n        self.i += 1\n        #if (self.i)%10 == 0:\n        #    print(engine.state.epoch, ' training loss: ',train_metrics['loss'],'\\t val loss: ',val_metrics['loss'] )\n            \n        #plot\n        clear_output(wait=True)\n        plt.plot(self.x,self.losses,label='train_loss')\n        plt.plot(self.x,self.val_losses,label='val_loss')\n        plt.legend()\n        plt.show()","c2a31130":"model = HousingModel(cont_cols,emb_sizes,fc_sizes=[64,8,1])\noptimizer = optim.Adam(model.parameters())\nloss_func = nn.MSELoss(size_average = False) \n\ntrainer = create_trainer(model, optimizer,loss_func,device,prepare_batch=prep_batch)\nevaluator = create_evaluator(model,metrics={'loss':Loss(loss_func)})\n\n#event handler to plot losses\nplot_losses = PlotLosses(evaluator,train_dl,val_dl)\ntrainer.add_event_handler(Events.EPOCH_COMPLETED,plot_losses)\n\n#start training\ntrainer.run(train_dl, max_epochs=100)","200f1158":"eval_submit(model, test_ds,test)\n#this gave a score of 27117.28 and rank 5679","1fbce863":"model = HousingModel(cont_cols,emb_sizes,fc_sizes=[64,8,1])\noptimizer = optim.Adam(model.parameters())\nloss_func = nn.MSELoss(size_average = False) \n\ntrainer = create_trainer(model, optimizer,loss_func,device,prepare_batch=prep_batch)\nevaluator = create_evaluator(model,metrics={'loss':Loss(loss_func)})\n\n#event handler to plot losses\nplot_losses = PlotLosses(evaluator,train_dl,val_dl)\ntrainer.add_event_handler(Events.EPOCH_COMPLETED,plot_losses)\n\n\n#--> cosine annealing for lr\nlr_scheduler = CosineAnnealingScheduler(optimizer,'lr',0.01,0.001,len(train_dl))\ntrainer.add_event_handler(Events.ITERATION_COMPLETED, lr_scheduler)\ntrainer.run(train_dl, max_epochs=70)","32b425a8":"eval_submit(model, test_ds,test)","7064c4a3":"\nmodel = HousingModel(cont_cols,emb_sizes,fc_sizes=[64,8,1])\noptimizer = optim.Adam(model.parameters())\nloss_func = nn.MSELoss(size_average = False) \n\ntrainer = create_trainer(model, optimizer,loss_func,device,prepare_batch=prep_batch)\nevaluator = create_evaluator(model,metrics={'loss':Loss(loss_func)})\n\n#event handler to plot losses\n# plot_losses = PlotLosses(evaluator,train_dl,val_dl)\n# trainer.add_event_handler(Events.EPOCH_COMPLETED,plot_losses)\n\n#--> cosine annealing for lr\nlr_scheduler = CosineAnnealingScheduler(optimizer,'lr',0.01,0.001,len(train_dl))\ntrainer.add_event_handler(Events.ITERATION_COMPLETED, lr_scheduler)\n\ntrain_ds = get_ds(train,cat_cols,cont_cols, target,device=device)\ntrain_dl = DataLoader(train_ds,2000,shuffle=True)\n\ntrainer.run(train_dl, max_epochs=70)","b39771ce":"### Preprocessing classes","fc7e0f0f":"> ### Reading the data","7d646889":"#### Identify categorical and continuous variables\nA numeric column can either be categorical or continuous. If the datatype is int and there are more than 20 unique values, we will consider it as continuous.[](http:\/\/)","0a617651":"### Purpose\nAttempting the housing problem using deep learning (pytorch). We try not to use any preprocessing libraries so that we can understand the basics. Using only pytorch and pytorch-ignite are used for visualizing training loss. \n\nWe are also not doing EDA in this notebook. Trying for a general approach. ","5fa5e245":"That gave a score of 18667.51 and rank 1703 (a jump of 3,977 places). \n\nWith same hyperparams, let us just train with all the data (no validation set)","1dae37fe":"### Embeddings for categorical features","8dd33972":"Let us add Learning Rate annealing","1e071039":"This gives a score of 16826.59 and rank 1370.","ad51832a":"### Train using apache Ignite"}}