{"cell_type":{"a2758f78":"code","145c54d4":"code","b152dc86":"code","23ccdd7b":"code","3615918e":"code","a8d6f9f9":"code","90cdbc1d":"code","f8e0ccba":"code","6aa98938":"code","e327e1dd":"code","eaeaae68":"code","9f32d1b4":"code","924debdc":"code","7b42279f":"code","b071b3ee":"code","21fc46d5":"code","d40afc1f":"code","163674c0":"code","54640042":"code","a9fed9d9":"code","d19113e4":"code","9ad03765":"code","e82c4c25":"code","643bb762":"code","7743f7ea":"code","d7c37f0c":"code","762a4ec9":"code","71235dc8":"code","e1fd48d7":"code","fa4a4ef9":"code","0e6c00a7":"code","547c88b0":"code","b4104423":"code","efd5b2a9":"code","737d3a65":"code","35244f53":"code","115de93d":"code","54412772":"code","4d9eabfe":"code","a305e985":"code","3c207ce9":"code","368e500f":"markdown","f8188929":"markdown","75c4a14c":"markdown","9c9d391d":"markdown","ad12fff3":"markdown","f19ba223":"markdown","6b7004ce":"markdown","224edc7d":"markdown","66b3fa70":"markdown","d5a66d63":"markdown","386f5e5e":"markdown","c43793eb":"markdown","f7a93256":"markdown","dddd95e7":"markdown","61e2de8f":"markdown","7f4245c6":"markdown"},"source":{"a2758f78":"import pandas as pd","145c54d4":"train = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')\ntrain.head()","b152dc86":"cat_cols = [x for x in train.columns if x.startswith('cat')]\ncat_cols","23ccdd7b":"test_new_cats = pd.DataFrame(columns=test.columns, data = [[1]+['Z']*len(cat_cols)+[0]*14])\ntest_new_cats","3615918e":"from sklearn.preprocessing import LabelEncoder","a8d6f9f9":"le = LabelEncoder()\nle.fit(train['cat8'])","90cdbc1d":"le.transform(['A'])","f8e0ccba":"le.transform(['Z'])","6aa98938":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom pandas.api.types import CategoricalDtype\n\nclass CategoricalTransform(BaseEstimator, TransformerMixin):\n    def __init__(self, cat_cols):\n        self.cat_cols = cat_cols\n        \n    def _transform_column(self, col, col_name):\n        return col.astype(self.cat_type[col_name]) \n        \n    def transform(self, df, **transform_params):\n        df_cat = df.copy()\n        for col in self.cat_cols:\n            df_cat[col] = self._transform_column(df_cat[col], col)\n        return df_cat\n        \n    def fit(self, X, y=None, **fit_params):\n        self.cat_type = dict()\n        for col in self.cat_cols:\n            self.cat_type[col] = CategoricalDtype(X[col].unique())\n        return self","e327e1dd":"ct = CategoricalTransform(cat_cols)","eaeaae68":"t = ct.fit_transform(train)\nt.info()","9f32d1b4":"test_new_cats","924debdc":"ct.transform(test_new_cats)","7b42279f":"from sklearn.pipeline import Pipeline\nfrom lightgbm.sklearn import LGBMRegressor\np = Pipeline([('cat_trans', CategoricalTransform(cat_cols)), \n              ('lgbm', LGBMRegressor(n_jobs=-2))])","b071b3ee":"x_train = train.drop(columns=['target','id'])\ny_train = train['target']\np.fit(x_train, y_train)","21fc46d5":"p.predict(test.drop(columns=['id']))","d40afc1f":"from sklearn.model_selection import cross_validate\nscores = cross_validate(p, X=x_train, y=y_train, cv=5, return_train_score = True,\n                         scoring='neg_root_mean_squared_error')\nscores","163674c0":"scores['test_score'].mean()","54640042":"p.predict(test_new_cats.drop(columns=['id']))","a9fed9d9":"class IntegerCategoricalTransform(CategoricalTransform):\n    def _transform_column(self, col, col_name):\n        return super()._transform_column(col, col_name).values.codes","d19113e4":"ct = IntegerCategoricalTransform(cat_cols)","9ad03765":"t = ct.fit_transform(train)\nt.info()","e82c4c25":"ct.transform(test_new_cats)","643bb762":"from sklearn.pipeline import Pipeline\nfrom catboost import CatBoostRegressor\np = Pipeline([('cat_trans', IntegerCategoricalTransform(cat_cols)), \n              ('cb', CatBoostRegressor(iterations=50, thread_count=3, cat_features=cat_cols))])","7743f7ea":"x_train = train.drop(columns=['target','id'])\ny_train = train['target']\np.fit(x_train, y_train)","d7c37f0c":"p.predict(test.drop(columns=['id']))","762a4ec9":"from sklearn.model_selection import cross_validate\nscores = cross_validate(p, X=x_train, y=y_train, cv=5, return_train_score = True,\n                         scoring='neg_root_mean_squared_error')\nscores","71235dc8":"scores['test_score'].mean()","e1fd48d7":"p.predict(test_new_cats.drop(columns=['id']))","fa4a4ef9":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass OneHotTransform(BaseEstimator, TransformerMixin):\n    def transform(self, df, **transform_params):\n        return pd.get_dummies(df)\n    \n    def fit(self, X, y=None, **fit_params):\n        return self","0e6c00a7":"oh_pipe = Pipeline([('cat_trans', CategoricalTransform(cat_cols)),\n                    ('oh_trans', OneHotTransform())])","547c88b0":"train_oh = oh_pipe.fit_transform(train)\ntrain_oh.head()","b4104423":"cat5_cols = [col for col in train_oh.columns if col.startswith('cat5')]\ncat5_cols","efd5b2a9":"train_oh[cat5_cols]","737d3a65":"oh_pipe.transform(test_new_cats)[cat5_cols]","35244f53":"from sklearn.ensemble import RandomForestRegressor\np = Pipeline([('oh_trans', oh_pipe), \n              ('rf', RandomForestRegressor(n_jobs=-2))])","115de93d":"x_train = train.drop(columns=['target','id'])\ny_train = train['target']\np.fit(x_train.head(10000), y_train.head(10000))","54412772":"p.predict(test.drop(columns=['id']))","4d9eabfe":"from sklearn.model_selection import cross_validate\nscores = cross_validate(p, X=x_train.head(10000), y=y_train.head(10000), cv=5, return_train_score = True,\n                         scoring='neg_root_mean_squared_error')\nscores","a305e985":"scores['test_score'].mean()","3c207ce9":"p.predict(test_new_cats.drop(columns=['id']))","368e500f":"# Robustly Encoding categorical features\n\nThe categorical features in the tabular playground series are stored as strings. They need to be encoded to something else to be used in machine learning models. I would like to build some encoding that is robust to having new categories that have not yet been seen in the training data.","f8188929":"## Missing values are encoded as -1","75c4a14c":"# Using pandas CategoricalDtype","9c9d391d":"# One hot encoding\nRobust One hot encoding can be achieved by chaining the categorical transformer with the OneHotTransform below.","ad12fff3":"## The transformer can be embedded in a sklearn Pipeline","f19ba223":"# Test sample with new categories to validate encodings","6b7004ce":"## Non-existing categories are encoded as NaN:","224edc7d":"## New categories are encoded as zeros for each category column.","66b3fa70":"# Transform to integer value\nIf you need to transform to an integer value, for example to train an embedding in tensorflow, you can use the codes from the categorical feature instead. Below is a small transformer to do so.","d5a66d63":"prediction with new categories","386f5e5e":"# Using the LabelEncoder\nThe sklearn LabelEncoder can encode strings to some integer label.","c43793eb":"The CategoricalDtype from pandas can be used in many machine learning models, including LightGBM and CatBoost. I created a simple transformer that 'learns' the available categories from the training data and encodes strings to categories.","f7a93256":"# Load data","dddd95e7":"# Using this in a sklearn Pipeline\nThe sklearn Randomforestregressor does not support categorical variables, so in this example I use one hot encoding for the categorical features. Note that the one hot pipeline defined before can be used as an element in the new pipeline.","61e2de8f":"The LabelEncoder is meant to encode the target variable, not the features. It cannot deal with not previously seen categories.","7f4245c6":"# Using this in a sklearn Pipeline\nHere I'm using CatBoost, as it is not trivial to use integer encoded features in the sklearn API of LightGBM."}}