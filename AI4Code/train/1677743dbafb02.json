{"cell_type":{"191801e6":"code","3a489e52":"code","146d1ab6":"code","1a32438c":"code","589021ab":"code","9e4fd2b4":"code","e2ebc80b":"code","10382262":"code","d1cb6a61":"code","e2e0e8b2":"code","c4dee9af":"code","20f63d88":"code","5d0ef624":"code","8e49fdd8":"code","8d839b80":"code","a5d90a3f":"code","7a5dfb74":"markdown","129daca8":"markdown"},"source":{"191801e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3a489e52":"import numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom numpy import array","146d1ab6":"#df1 = pd.read_csv('\/kaggle\/input\/suicide-rates-overview-1985-to-2016\/master.csv', delimiter=',', nrows = nRowsRead)\ndf1 = pd.read_csv('\/kaggle\/input\/suicide-rates-overview-1985-to-2016\/master.csv', delimiter=',')\ndf1.dataframeName = 'master.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf1.head(5)","1a32438c":"print(df1.describe())\nprint(\"----------------------------\")\nprint(df1.dtypes)\nprint(df1.groupby('HDI for year').size())\nprint(\"----------------------------\")\n","589021ab":"#Deletion of unnecessary columns\nprint(df1.columns.values)\ndf1 = df1.drop(\"suicides_no\", axis=1)\ndf1 = df1.drop(\"country-year\", axis=1)\ndf1 = df1.drop(\" gdp_for_year ($) \", axis=1)\nprint(\"df1.shape after column deletion= \",df1.shape)\nprint(df1.dtypes)\n\n#Check missing data\nprint(\"Missing data----------------------\")\nmissing_val_count_by_column = (df1.isnull().sum())\nprint(missing_val_count_by_column)\nprint(\"HDI for year= \")\nprint(df1['HDI for year'].head(4))\nprint(\"HDI for year mean= \",df1['HDI for year'].mean())\nprint(\"HDI for year min= \",df1['HDI for year'].min())\nprint(\"HDI for year max= \",df1['HDI for year'].max())\n\nprint(\"Data after filling NaN----------------------\") #filling NaN with mean value\n\ndf1['HDI for year']=df1['HDI for year'].fillna(df1['HDI for year'].mean())\nprint(\"HDI for year= \")\nprint(df1['HDI for year'].head(4))\nmissing_val_count_by_column2 = (df1.isnull().sum())\nprint(missing_val_count_by_column2)","9e4fd2b4":"# Categorical encoding \ndf1[\"country\"] = df1[\"country\"].astype('category')  #Change type from object to category\ndf1[\"sex\"] = df1[\"sex\"].astype('category')\ndf1[\"age\"] = df1[\"age\"].astype('category')\ndf1[\"generation\"] = df1[\"generation\"].astype('category')\nprint(df1.dtypes)\n\ndf1[\"country_cat\"] = df1[\"country\"].cat.codes #Append new column to \ndf1[\"sex_cat\"] = df1[\"sex\"].cat.codes\ndf1[\"age_cat\"] = df1[\"age\"].cat.codes\ndf1[\"generation_cat\"] = df1[\"generation\"].cat.codes\n\nprint(\"df1.shape after adding categorical column= \",df1.shape)\nprint(df1.head(5))\nprint(\"unique values country_cat = \",df1[\"country_cat\"].nunique())\nprint(\"unique values sex_cat = \",df1[\"sex_cat\"].nunique())\nprint(\"unique values age_cat = \",df1[\"age_cat\"].nunique())\nprint(\"unique values generation_cat = \",df1[\"generation_cat\"].nunique())\n\n\n","e2ebc80b":"#Check relationship of each features\n\ndf1.plot(kind=\"scatter\", x=\"population\", y=\"suicides\/100k pop\")\ndf1.plot(kind=\"scatter\", x=\"country_cat\", y=\"suicides\/100k pop\")\ndf1.plot(kind=\"scatter\", x=\"age_cat\", y=\"suicides\/100k pop\")\ndf1.plot(kind=\"scatter\", x=\"generation_cat\", y=\"suicides\/100k pop\")\ndf1.plot(kind=\"scatter\", x=\"year\", y=\"suicides\/100k pop\")\ndf1.plot(kind=\"scatter\", x=\"HDI for year\", y=\"suicides\/100k pop\")\ndf1.plot(kind=\"scatter\", x=\"gdp_per_capita ($)\", y=\"suicides\/100k pop\")\ndf1.plot(kind=\"scatter\", x=\"sex_cat\", y=\"suicides\/100k pop\")\n\n'''\n#fails to create automatic loops\nprint(df1.columns[1])\nprint(df1.columns.dtype)\n\na=df1.columns.to_numpy()\nprint(\"a[0:3]====================\")\nprint(a[0:3])\n\n#str_obj=repr(a[0])\n#str_obj=str(a[0])\n#astr=\"country\"\nb = df1.columns.map(str)\n#b = df1.columns.astype(str)\nprint(b)\nprint(b[0]==\"country\")\n\n\nfor i in range (df1.shape[1]):\n    #print(df1.columns[i])\n    #str=df1.columns[i]\n    str=\"population\"\n    #str=str(a[i])\n    df1.plot(kind=\"scatter\", x=str, y=\"suicides\/100k pop\")\n\n\n    \n'''\n","10382262":"import seaborn as sns\n\ncorrelation_matrix = df1.corr()\nprint(correlation_matrix)\n\nsns.heatmap(correlation_matrix) #most features are unrelated to suicides rate except sex_cat","d1cb6a61":"#X & y separation\nX=df1.drop(\"suicides\/100k pop\", axis=1)\nX=X.drop(\"country\", axis=1)\nX=X.drop(\"sex\", axis=1)\nX=X.drop(\"age\", axis=1)\nX=X.drop(\"generation\", axis=1)\n\ny=df1[\"suicides\/100k pop\"]\n\nprint(\"X.shape= \",X.shape)\nprint(X.head(3))\nprint(X.dtypes)\nprint(\"------------------------------------------\")\nprint(\"y.shape= \",y.shape)\nprint(y.head(3))\nprint(y.dtypes)","e2e0e8b2":"# Split training & test\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\nprint(\"X_train.shape= \",X_train.shape)\nprint(\"X_test.shape= \",X_test.shape)\nprint(\"y_train.shape= \",y_train.shape)\nprint(\"y_test.shape= \",y_test.shape)","c4dee9af":"from sklearn.preprocessing import StandardScaler\n\nprint(X_train.head(3))\n\nscaler = StandardScaler()\nX_train=scaler.fit_transform(X_train)   # train only on X_train not the whole set\nprint(\"X after StandardScaler-------------------------\")\nprint(X_train[:3,:])\n\nX_test = scaler.transform (X_test)  # only transform","20f63d88":"# Training & Evaluation\n\n# LinearRegression (vanilla)\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\nclassifier_linreg =  LinearRegression()\nclassifier_linreg.fit(X_train, y_train)\n\ny_pred = classifier_linreg.predict(X_test)\n\nprint (\"Linreg (vanilla)-----------------------------\")\nprint(\"Training error: \" + str(mean_squared_error(y_train, classifier_linreg.predict(X_train))))\nprint(\"Test error: \" + str(mean_squared_error(y_test, classifier_linreg.predict(X_test))))\nprint (\"Use CV, score= -----------------------------\")\nCV_score=cross_val_score(classifier_linreg, X_train, y_train,scoring=\"neg_mean_squared_error\", cv=4)\nprint(CV_score)\n\n\n# LinearRegression (poly)\ntrafo = PolynomialFeatures(4)      #set degree of polynomials = 4\nX_train_poly = trafo.fit_transform(X_train)\nX_test_poly = trafo.fit_transform(X_test)\nprint (\"\")\nclassifier_linreg_poly = LinearRegression()\nclassifier_linreg_poly.fit(X_train_poly, y_train)\nprint (\"Linreg (polynomials)-----------------------------\")\nprint(\"Training error Poly: \" + str(mean_squared_error(y_train, classifier_linreg_poly.predict(X_train_poly))))\nprint(\"Test error Poly: \" + str(mean_squared_error(y_test, classifier_linreg_poly.predict(X_test_poly))))\nprint (\"Use CV, score= -----------------------------\")\nCV_score=cross_val_score(classifier_linreg_poly, X_train_poly, y_train,scoring=\"neg_mean_squared_error\", cv=4)\nprint(CV_score)\n#Test Error is getting worse if we increase the degree of polynomials","5d0ef624":"# LinearRegression (vanilla) for only 1 feature\n\nprint (\"Linreg (vanilla) for only 1 feature----------------------------\")\n\nfeature_idx=[]\n\nfor i in range(X_train.shape[1]):    \n    feature_idx.append(i)\n\nfor i in feature_idx:\n    X_train_01 =X_train[:,i]\n    X_test_01 =X_test[:,i]\n\n    classifier_linreg_01 =  LinearRegression()\n    classifier_linreg_01.fit(X_train_01.reshape(-1, 1), y_train)\n\n    y_pred = classifier_linreg_01.predict(X_test.reshape(-1, 1))\n\n    print(\"Use feature no\",i)\n    print(\"   Training error: \" + str(mean_squared_error(y_train, classifier_linreg_01.predict(X_train_01.reshape(-1, 1)))))\n    print(\"   Test error: \" + str(mean_squared_error(y_test, classifier_linreg_01.predict(X_test_01.reshape(-1, 1)))))\n","8e49fdd8":"# DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\n\ntree_reg = DecisionTreeRegressor()\ntree_reg. fit(X_train, y_train)\n\ny_pred = tree_reg. predict(X_test)\ntree_mse = mean_squared_error(y_test, y_pred)\ntree_rmse = np.sqrt(tree_mse)\nprint(\"tree_rmse= \",tree_rmse)\nprint(\"y_train.mean= \",y_train.mean())\n\nprint(\"----------Use CV-------------------\")\nscores = cross_val_score(tree_reg, X_train, y_train,scoring=\"neg_mean_squared_error\", cv=4)\nrmse_scores = np.sqrt(-scores)\nprint(\"Scores: \", scores)\nprint(\"Mean: \", scores.mean())\nprint(\"Standard deviation: \", scores.std())\n","8d839b80":"# RandomForestRegressor\nfrom sklearn. ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor()\nforest_reg.fit(X_train, y_train)\n\ny_pred = forest_reg. predict(X_test)\ntree_mse = mean_squared_error(y_test, y_pred)\ntree_rmse = np.sqrt(tree_mse)\nprint(\"tree_rmse= \",tree_rmse)\nprint(\"y_train.mean= \",y_train.mean())\n\nprint(\"----------Use CV-------------------\")\nscores = cross_val_score(tree_reg, X_train, y_train,scoring=\"neg_mean_squared_error\", cv=4)\nrmse_scores = np.sqrt(-scores)\nprint(\"Scores: \", scores)\nprint(\"Mean: \", scores.mean())\nprint(\"Standard deviation: \", scores.std())\n","a5d90a3f":"'''\nmodel.fit(X_train, y_train)\nprint(\"training error: \" + str(model.evaluate(X_train, y_train, verbose=0)))\nprint(\"test error: \" + str(model.evaluate(X_test, y_test, verbose=0)))\n\n#accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, valid_y, is_neural_net=True)\nprint (\"CNN accuracy\",  accuracy)\n'''\n","7a5dfb74":"# Deep NN\n\nfrom keras.layers import Conv1D, MaxPooling1D\n\ninput_size=X_train.shape[1]\n'''\ninput_layer = layers.Input(shape=(input_size, ))\n\n# Add the convolutional Layer\n#conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(input_layer)\nconv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(input_layer)\n\nmodel.add(Convolution1D(64, 3, border_mode='same', input_shape=(10, 32)))\n\n# Add the pooling Layer\npooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n\n# Add the output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n# Compile the model\nmodel = models.Model(inputs=input_layer, outputs=output_layer2)\nmodel.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\nplot_model(model, show_shapes=True, show_layer_names=True)\n'''\n#-------------------------------------------------------------------------\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n#-------------------------------------------------------------------------\n\n","129daca8":"#Shallow NN\nfrom keras import layers, models, optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.metrics import categorical_accuracy\nfrom keras.utils.vis_utils import plot_model\n\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\ninput_size=X_train.shape[1]\n'''\nnet = Sequential()\nnet.add(Dense(100, activation='relu' , input_shape=(input_size,)))\nnet.add(Dense(1, activation='sigmoid' ))\nnet.compile(loss='binary_crossentropy' , optimizer=optimizers.Adam(), metrics=['accuracy'] )\nnet.summary()\n\nplot_model(net, show_shapes=True, show_layer_names=True)\n\nnet.fit(X_train, y_train, epochs=10, verbose=0)\nprint(\"training error: \" + str(net.evaluate(X_train, y_train, verbose=0)))\nprint(\"test error: \" + str(net.evaluate(X_test, y_test, verbose=0)))\n'''\n\ndef baseline_model():\n    net = Sequential()\n    net.add(Dense(100, activation='relu' , input_shape=(input_size,)))\n    net.add(Dense(1, activation='sigmoid' ))\n    net.compile(loss='binary_crossentropy' , optimizer=optimizers.Adam(), metrics=['accuracy'] )\n    return net\n\nestimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\nkfold = KFold(n_splits=2)\nresults = cross_val_score(estimator, X_train, y_train, cv=kfold)\nprint(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n\n# NN performs the worse"}}