{"cell_type":{"84879c35":"code","81b746d5":"code","fc09209f":"code","f2c0cda6":"code","2126623a":"code","6c774c4a":"code","724859a3":"code","09f2aed4":"code","3fb76654":"code","2b48da65":"code","e429d7f2":"code","1066c47b":"code","3ecf7b17":"code","718812b5":"code","516a0c34":"code","476c9042":"code","031c3ab9":"code","0f888639":"markdown","5173b574":"markdown","0e191a18":"markdown","831cd878":"markdown","63e84a43":"markdown"},"source":{"84879c35":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm\nimport os\nimport gc\nimport scipy.stats as stats\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom boruta import BorutaPy\nfrom joblib import Parallel, delayed\n!ls ..\/input\/dsa-junho","81b746d5":"N_SPLITS = 5\nRANDOM_SEED = 1234\n\nFEATS_EXCLUDED = ['first_active_month', 'target', 'card_id',\n                  'hist_purchase_date_max', 'hist_purchase_date_min', 'hist_card_id_size',\n                 'new_purchase_date_max', 'new_purchase_date_min', 'new_card_id_size']\n\nnp.random.seed(RANDOM_SEED)","fc09209f":"%%time\ntrain_data = pd.read_csv('..\/input\/competicao-dsa-machine-learning-jun-2019\/dataset_treino.csv')\ntest_data = pd.read_csv('..\/input\/competicao-dsa-machine-learning-jun-2019\/dataset_teste.csv')\nhistory = pd.read_csv('..\/input\/dsa-junho\/transacoes_historicas.csv\/transacoes_historicas.csv', \n                    dtype={'city_id': np.int16, 'installments': np.int16, 'merchant_category_id': np.int16, \n                   'month_lag': np.int8, 'purchace_amount': np.float32, 'category_2': np.float32,\n                   'state_id': np.int8, 'subsector_id': np.int8})\nnew_transact = pd.read_csv('..\/input\/dsa-junho\/novas_transacoes_comerciantes.csv\/novas_transacoes_comerciantes.csv',\n                   dtype={'city_id': np.int16, 'installments': np.int16, 'merchant_category_id': np.int16, \n                   'month_lag': np.int8, 'purchace_amount': np.float32, 'category_2': np.float32,\n                   'state_id': np.int8, 'subsector_id': np.int8})","f2c0cda6":"# Missing values\ndef fill_na(data):\n    data['category_2'].fillna(1.0,inplace=True) #Moda\n    data['category_3'].fillna('A',inplace=True) #Moda\n    data['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True) #Moda\n    data['installments'].replace(-1, np.nan,inplace=True)\n    data['installments'].replace(999, np.nan,inplace=True)\n\n    data['authorized_flag'] = data['authorized_flag'].map({'Y': 1, 'N': 0}).astype(np.uint8)\n    data['category_1'] = data['category_1'].map({'Y': 1, 'N': 0}).astype(np.uint8)\n    data['category_3'] = data['category_3'].map({'A':0, 'B':1, 'C':2}).astype(np.uint8)\n    \n    data['merchant_id'] = data['merchant_id'].astype('category').cat.codes\n    data['merchant_category_id'] = data['merchant_category_id'].astype('category').cat.codes","2126623a":"fill_na(history)\nfill_na(new_transact)\ngc.collect()","6c774c4a":"def get_features(data, prefix, num_rows=None):\n    # Colunas com tipo Datetime\n    print('Feature datetime')\n    data['purchase_date'] = pd.to_datetime(data['purchase_date'])\n    data['month'] = data['purchase_date'].dt.month.astype(np.uint8)\n    data['day'] = data['purchase_date'].dt.day.astype(np.uint8)\n    data['hour'] = data['purchase_date'].dt.hour.astype(np.uint8)\n    data['weekofyear'] = data['purchase_date'].dt.weekofyear.astype(np.uint8)\n    data['weekday'] = data['purchase_date'].dt.weekday.astype(np.uint8)\n    data['weekend'] = (data['purchase_date'].dt.weekday >=5).astype(np.uint8)\n    \n    \n\n    data['parcela'] = (data['purchase_amount'] \/ data['installments']).astype(np.float16)\n\n    print('Features holidays')\n    data['natal_2017']=(pd.to_datetime('2017-12-25')-data['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0).astype(np.uint8)\n    data['dia_maes_2017']=(pd.to_datetime('2017-05-13')-data['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0).astype(np.uint8)\n    data['dia_pais_2017']=(pd.to_datetime('2017-08-13')-data['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0).astype(np.uint8)\n    data['dia_criancas_2017']=(pd.to_datetime('2017-10-12')-data['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0).astype(np.uint8)\n    data['dia_namorados_2017']=(pd.to_datetime('2017-06-12')-data['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0).astype(np.uint8)\n    data['black_friday_2017']=(pd.to_datetime('2017-11-24') - data['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0).astype(np.uint8)\n    data['dia_maes_2018']=(pd.to_datetime('2018-05-13')-data['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0).astype(np.uint8)\n\n    data['month_diff'] = (((dt.datetime.today() - data['purchase_date']).dt.days)\/\/30).astype(np.uint8)\n    data['month_diff'] += data['month_lag']\n\n    data['prazo'] = (data['purchase_amount']*data['month_diff']).astype(np.float16)\n    data['amount_month_ratio'] = (data['purchase_amount']\/data['month_diff']).astype(np.float16)\n\n    col_unique =['subsector_id', 'merchant_id', 'merchant_category_id']\n    col_seas = ['month', 'hour', 'weekofyear', 'weekday', 'day']\n    aggs = {}\n    for col in col_unique:\n        aggs[col] = ['nunique']\n\n    for col in col_seas:\n        aggs[col] = ['nunique', 'mean', 'min', 'max'] \n\n    def interval(fun):\n        def diff_interval_(x):\n            if len(x)>2:\n                diff = np.diff(x.sort_values()).astype(np.int64)*1e-9                \n                return fun(diff)\n            else:\n                return np.nan\n        diff_interval_.__name__ = 'interval_{}'.format(fun.__name__)\n        return diff_interval_\n\n    aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n    aggs['installments'] = ['sum','max','mean','var','skew']\n    aggs['purchase_date'] = ['max','min', interval(np.mean), interval(np.min), interval(np.max), interval(np.std)]\n    aggs['month_lag'] = ['max','min','mean','var','skew']\n    aggs['month_diff'] = ['max','min','mean','var','skew']\n    aggs['authorized_flag'] = ['mean']\n    aggs['weekend'] = ['mean'] \n    aggs['weekday'] = ['mean'] \n    aggs['day'] = ['nunique', 'mean', 'min'] \n    aggs['category_1'] = ['mean', lambda x: stats.mode(x)[0][0]]\n    aggs['category_2'] = ['mean']\n    aggs['category_3'] = ['mean']\n    aggs['card_id'] = ['size','count']\n    aggs['parcela'] = ['sum','mean','max','min','var']\n    aggs['natal_2017'] = ['mean']\n    aggs['dia_maes_2017'] = ['mean']\n    aggs['dia_pais_2017'] = ['mean']\n    aggs['dia_criancas_2017'] = ['mean']\n    aggs['dia_namorados_2017'] = ['mean']\n    aggs['black_friday_2017'] = ['mean']\n    aggs['dia_maes_2018'] = ['mean']\n    aggs['prazo']=['mean','min','max','var','skew']\n    aggs['amount_month_ratio']=['mean','min','max','var','skew']\n\n    print('Agregacoes iniciais')\n    for col in ['category_2','category_3','merchant_id']:\n        data[col+'_mean'] = data.groupby([col])['purchase_amount'].transform('mean').astype(np.float16)\n        data[col+'_std'] = data.groupby([col])['purchase_amount'].transform('std').astype(np.float16)\n        data[col+'_min'] = data.groupby([col])['purchase_amount'].transform('min').astype(np.float16)\n        data[col+'_max'] = data.groupby([col])['purchase_amount'].transform('max').astype(np.float16)\n        data[col+'_sum'] = data.groupby([col])['purchase_amount'].transform('sum').astype(np.float16)\n        aggs[col+'_mean'] = ['mean']\n        aggs[col+'_std'] = ['mean']\n\n    print('Iniciando Agregacao...')\n    # S2 paralelismo ;)\n    def aggregate(df):\n        return df.groupby('card_id').agg(aggs)\n    data['reduced_card_id'] = data.reset_index()['card_id'].str[:8]\n    data_parts = Parallel(n_jobs=-1, verbose=1)(delayed(aggregate)(group) for _,group in data.groupby('reduced_card_id'))\n    data = pd.concat(data_parts, axis=0)\n    print('Agregado.')\n\n    # Padroniza nomes\n    data.columns = pd.Index([e[0] + \"_\" + e[1] for e in data.columns.tolist()])\n    data.columns = [prefix + c for c in data.columns]\n\n    data[prefix + 'purchase_date_diff'] = (data[prefix + 'purchase_date_max']-data[prefix + 'purchase_date_min']).dt.days\n    data[prefix + 'purchase_date_average'] = data[prefix + 'purchase_date_diff']\/data[prefix + 'card_id_size']\n    data[prefix + 'purchase_date_uptonow'] = (dt.datetime.today()-data[prefix + 'purchase_date_max']).dt.days\n    data[prefix + 'purchase_date_uptomin'] = (dt.datetime.today()-data[prefix + 'purchase_date_min']).dt.days\n\n    #Colunas float16 geram problemas no modelo\n    cols = [c for c,d in zip(data.columns, data.dtypes) if d == np.dtype('float16')]\n    data[cols] = data[cols].astype(np.float32)\n\n    return data","724859a3":"history = get_features(history, 'hist_')\ngc.collect()","09f2aed4":"new_transact = get_features(new_transact, 'new_')\ngc.collect()","3fb76654":"features = history.merge(new_transact, how='left', left_index=True, right_index=True)\ndel new_transact\ndel history\ngc.collect()\n\ntrain_features = train_data.merge(features.reset_index(), how='left', on='card_id')\ntest_features = test_data.merge(features.reset_index(), how='left', on='card_id')","2b48da65":"reg_idx = train_features['target']>-50\nreg_X = train_features.loc[reg_idx].drop(FEATS_EXCLUDED, axis=1)\nreg_y = train_features.loc[reg_idx, 'target']\ntest_reg_X = test_features.drop([x for x in FEATS_EXCLUDED if x!='target'], axis=1)\n\nclf_X = train_features.drop(FEATS_EXCLUDED, axis=1)\nclf_y = (train_features['target']<-33).astype(np.uint8)\ntest_clf_X = test_reg_X","e429d7f2":"stratifications = pd.cut(reg_y, 5, labels=False)\nsplits = list(StratifiedKFold(n_splits=N_SPLITS, random_state=RANDOM_SEED, shuffle=True).split(reg_X, stratifications))\n\nreg_test_pred = []\nreg_metrics = []\nreg_feature_importances = []\n\nfor fold_id, (train_idx, val_idx) in enumerate(splits):\n    print('Training Fold {}'.format(fold_id))\n    \n    train_X, val_X = reg_X.iloc[train_idx], reg_X.iloc[val_idx]\n    train_y, val_y = reg_y.iloc[train_idx], reg_y.iloc[val_idx]\n    \n    model = lightgbm.LGBMRegressor(**{\n                'task': 'train',\n                'objective': 'regression',\n                'metric': 'rmse',\n                'learning_rate': 0.01,\n                'subsample': 0.9,\n                'max_depth': -1,\n                'num_leaves': 64,\n                'reg_alpha': 5.0,\n                'colsample_bytree': 0.7,\n                'min_split_gain': 1.0,\n                'reg_lambda': 5.0,\n                'min_data_in_leaf': 50,\n                'early_stopping_rounds': 100,\n                'num_iterations': 300,\n                'nthreads': -1,\n                })\n    model.fit(train_X, train_y, eval_set=(val_X, val_y), verbose=100)\n    \n    reg_test_pred.append(model.predict(test_reg_X))\n    reg_feature_importances.append(model.feature_importances_)\n    reg_metrics.append(model.best_score_['valid_0']['rmse'])\n    print('Fold RMSE: {}'.format(reg_metrics[-1]))\nprint('Validation RMSE: {} +- {}'.format(np.mean(reg_metrics), np.std(reg_metrics)))\nreg_feature_importances = pd.DataFrame(np.stack(reg_feature_importances, axis=1), columns=[i for i in range(N_SPLITS)], index=reg_X.columns)\nreg_feature_importances = reg_feature_importances.loc[reg_feature_importances.mean(axis=1).sort_values(ascending=False).index]","1066c47b":"_, ax = plt.subplots(1, figsize=(12,8))\nlightgbm.plot_importance(model, max_num_features=25, height=0.5, ax=ax)\nplt.tight_layout()","3ecf7b17":"splits = list(StratifiedKFold(n_splits=N_SPLITS, random_state=RANDOM_SEED, shuffle=True).split(clf_X, clf_y))\n\nclf_test_pred = []\nclf_metrics = []\nclf_feature_importances = []\n\nfor fold_id, (train_idx, val_idx) in enumerate(splits):\n    print('Training Fold {}'.format(fold_id))\n    \n    train_X, val_X = clf_X.iloc[train_idx], clf_X.iloc[val_idx]\n    train_y, val_y = clf_y.iloc[train_idx], clf_y.iloc[val_idx]\n    \n    model = lightgbm.LGBMClassifier(**{\n                'objective': 'binary',\n                'boosting': 'gbdt',\n                'metric': 'auc',\n                'learning_rate': 0.01,\n                'subsample': 0.99,\n                'max_depth': 7,\n                'num_leaves': 128,\n                'reg_alpha': 1.0,\n                'colsample_bytree': 0.6,\n                'reg_lambda': 1.0,\n                'min_data_in_leaf': 10,\n                'pos_scale_weight': 1.0,\n                'early_stopping_rounds': 200,\n                'num_iterations': 10000,\n                'nthreads': -1,\n                })\n    model.fit(train_X, train_y, eval_set=(val_X, val_y), verbose=200)\n    \n    clf_test_pred.append(model.predict_proba(test_clf_X)[:, 1])\n    clf_feature_importances.append(model.feature_importances_)\n    clf_metrics.append(roc_auc_score(val_y, model.predict_proba(val_X)[:, 1]))\n    print('Fold AUC: {}'.format(clf_metrics[-1]))\nprint('Validation AUC: {} +- {}'.format(np.mean(clf_metrics), np.std(clf_metrics)))\nclf_feature_importances = pd.DataFrame(np.stack(clf_feature_importances, axis=1), columns=[i for i in range(N_SPLITS)], index=clf_X.columns)\nclf_feature_importances = clf_feature_importances.loc[clf_feature_importances.mean(axis=1).sort_values(ascending=False).index]","718812b5":"_, ax = plt.subplots(1, figsize=(12,8))\nlightgbm.plot_importance(model, max_num_features=25, height=0.5, ax=ax)\nplt.tight_layout()","516a0c34":"sub = pd.read_csv('..\/input\/competicao-dsa-machine-learning-jun-2019\/sample_submission.csv')\ntest_predictions = np.mean(reg_test_pred, axis=0)\ntest_outliers = np.mean(clf_test_pred, axis=0)\nsub['target'] = (1-test_outliers)*test_predictions + test_outliers*-9\nsub['target'] *= 0.9","476c9042":"sub['target'].describe()","031c3ab9":"plt.figure(figsize=(9,7))\nplt.hist(sub['target'], bins=30)\n_ = plt.title('Test Set: Target Distribution', fontsize=15)","0f888639":"# Feature Engineering","5173b574":"# Competi\u00e7\u00e3o DSA - Junho\/2019\n\nNeste kernel, desenvolveremos o c\u00f3digo para solu\u00e7\u00e3o do Desafio DSA - Junho 2019.  \nA abordagem aqui consiste em utilizar features agregadas por `card_id`, dos conjuntos `novas_transacoes_comerciantes` e `transacoes_historicas`, e treinar dois modelos:\n1. Modelo de regress\u00e3o para previs\u00e3o dos valores do \u00edndice de lealdade\n2. Modelo de classifica\u00e7\u00e3o para previs\u00e3o de outliers (valores -33.21928095)\n\nEsses modelos ser\u00e3o combinados da seguinte forma:  \n`indice_lealdade = prob_outlier * outlier_val + (1 - prob_outlier) * regression_prediction`  \n\nOnde:  \n`indice_lealdade`: vari\u00e1vel que queremos prever  \n`prob_outlier`: output do modelo de classifica\u00e7\u00e3o  \n`outlier_val`: valor do outlier. Inicialmente usar\u00edamos -33.21928095 mas, verificando a diferen\u00e7a de distribui\u00e7\u00e3o dos conjuntos, selecionamos um valor de forma a otimizar o score do leaderboard  \n`regression_redictions`: output do modelo de regress\u00e3o  \n\n### Algumas observa\u00e7\u00f5es\n\nA principal observa\u00e7\u00e3o, diz respeito a enorme diferen\u00e7a da m\u00e9trica na valida\u00e7\u00e3o local (RMSE = 3.66) e no leaderboard (RMSE = 0.54). Com a publica\u00e7\u00e3o do resultado parcial no conjunto de testes privado (RMSE = 0.5x), fica claro que existe um s\u00e9rio problema nos dados. A distribui\u00e7\u00e3o da vari\u00e1vel `target` no conjunto de treino \u00e9 completamente diferente da distribui\u00e7\u00e3o no conjunto de testes.  \nNesse caso, \u00e9 f\u00e1cil notar (com alguns submits) que melhorias na modelagem de treino n\u00e3o afetam positivamente o resultado do leaderboard. Algumas ideias que foram eliminadas nessa fase:\n\n1. **Uso de modelos alternativos**: RNN, MLP, ...\n2. **Features mais trabalhadas**: agrega\u00e7\u00f5es de previs\u00f5es do \u00edndice de lealdade baseadas em transa\u00e7\u00f5es individuais\n\nAlgumas ideias que n\u00e3o apresentaram ganho no conjunto de testes:\n\n1. **Feature Selection**: Boruta e Permuation Importance\n2. **Otimiza\u00e7\u00e3o de hiperpar\u00e2metros**: Optuna**\n\nPara efeito de resultado final, foi mais efetivo a utiliza\u00e7\u00e3o de features simples e ajustes no p\u00f3s-processamento dos resultados, do que uma modelagem mais profunda e focada nos dados de treino.","0e191a18":"# Models","831cd878":"O resultado pode melhorar se combinarmos diversos modelos utilizando subconjuntos das features utilizadas. Essa estrat\u00e9gia se mostrou particularmente boa quando consideramos diferentes vers\u00f5es das bases de dados:\n1. Treino\/Teste + Hist\u00f3rico\n2. Treino\/Teste + Novas_Transacoes\n3. Treino\/Teste + Hist\u00f3rico + Novas_Transacoes\n4. Treino\/Teste + Hist\u00f3rico[authorized_flag=='Y'] + Novas_Transacoes[authorized_flag=='Y']","63e84a43":"## Final merge\n\n** Com a diferen\u00e7a de distribui\u00e7\u00e3o dos conjuntos de treino e teste, foi necess\u00e1rio ajustar os valores (baseado nos submits) para um melhor LB score**"}}