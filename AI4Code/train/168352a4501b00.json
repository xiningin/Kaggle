{"cell_type":{"6f29d5c1":"code","6f8f19ec":"code","5a0f532f":"code","7b627e30":"code","78677748":"code","ea6ba6c0":"code","2c47853f":"code","69f9e2a3":"code","23729ba7":"code","3eafa377":"code","24603a04":"code","8158d75b":"code","643cfef6":"code","e070c5d5":"code","74ed9499":"code","254536a9":"code","c99bbac0":"code","1fab7dc1":"code","7520304e":"code","979dfbea":"code","e3cc95b4":"code","d2d8d7cf":"code","0a277305":"code","b62ebe9b":"code","fc318798":"markdown","5cf88be6":"markdown","c485a36f":"markdown","2f66ac8a":"markdown","9fd02a3e":"markdown","fb41d49e":"markdown"},"source":{"6f29d5c1":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport gensim ","6f8f19ec":"import warnings\nwarnings.filterwarnings(action='ignore')","5a0f532f":"# \uacbd\ub85c\uc758 \uacbd\uc6b0 \uac01\uc790\uc758 \ud658\uacbd\uc5d0 \ub9de\uac8c \uc124\uc815\ud574\uc8fc\uba74 \ub429\ub2c8\ub2e4. \npath = '..\/input\/t-academy-recommendation2\/movies\/'","7b627e30":"movie = pd.read_csv(path + 'ratings.csv', low_memory=False)\nmovie.head(2)","78677748":"movie = movie.sort_values(by='timestamp', ascending=True).reset_index(drop=True)\nmovie.head()","ea6ba6c0":"# \uc601\ud654\uc758 Metadata\ub97c \ubd88\ub7ec\uc640\uc11c movieID\uc5d0 \ub9de\ub294 TITLE\uc744 \uad6c\ud574\uc90d\ub2c8\ub2e4. \nmeta = pd.read_csv(path + 'movies_metadata.csv', low_memory=False)\nmeta.head(2)","2c47853f":"meta.columns","69f9e2a3":"meta = meta.rename(columns={'id':'movieId'})\nmovie['movieId'] = movie['movieId'].astype(str)\nmeta['movieId'] = meta['movieId'].astype(str)\n\nmovie = pd.merge(movie, meta[['movieId', 'original_title']], how='left', on='movieId')","23729ba7":"movie = movie[movie['original_title'].notnull()].reset_index(drop=True)","3eafa377":"agg = movie.groupby(['userId'])['original_title'].agg({'unique'})\nagg.head()","24603a04":"movie['original_title'].unique()","8158d75b":"# int\ud615\uc2dd\uc740 Word2vec\uc5d0\uc11c \ud559\uc2b5\uc774 \uc548\ub418\uc5b4\uc11c String\uc73c\ub85c \ubcc0\uacbd\ud574\uc90d\ub2c8\ub2e4. \nsentence = []\nfor user_sentence in agg['unique'].values:\n    sentence.append(list(map(str, user_sentence)))","643cfef6":"# Word2vec\uc758 \ud559\uc2b5\uc744 \uc9c4\ud589\ud574\uc90d\ub2c8\ub2e4. \nfrom gensim.models import Word2Vec\nembedding_model = Word2Vec(sentence, size=20, window = 5, \n                           min_count=1, workers=4, iter=200, sg=1)","e070c5d5":"embedding_model.wv.most_similar(positive=['Spider-Man 2'], topn=10)","74ed9499":"from gensim.models import doc2vec","254536a9":"meta = pd.read_csv(path + 'movies_metadata.csv', low_memory=False)\nmeta = meta[meta['original_title'].notnull()].reset_index(drop=True)\nmeta = meta[meta['overview'].notnull()].reset_index(drop=True)","c99bbac0":"from nltk.corpus import stopwords \nfrom tqdm.notebook import tqdm\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport re \nstop_words = set(stopwords.words('english')) \n\noverview = []\nfor words in tqdm(meta['overview']):\n    word_tokens = word_tokenize(words)\n    sentence = re.sub('[^A-Za-z0-9]+', ' ', str(word_tokens))\n    sentence = sentence.strip()\n    \n    sentence_tokens = word_tokenize(sentence)\n    result = ''\n    for token in sentence_tokens: \n        if token not in stop_words:\n            result += ' ' + token \n    result = result.strip().lower()\n    overview.append(result)","1fab7dc1":"meta['pre_overview'] = overview","7520304e":"doc_vectorizer = doc2vec.Doc2Vec(\n    dm=0,            # PV-DBOW \/ default 1\n    dbow_words=1,    # w2v simultaneous with DBOW d2v \/ default 0\n    window=10,        # distance between the predicted word and context words\n    size=100,        # vector size\n    alpha=0.025,     # learning-rate\n    seed=1234,\n    min_count=5,    # ignore with freq lower\n    min_alpha=0.025, # min learning-rate\n    workers=4,   # multi cpu\n    hs = 1,          # hierar chical softmax \/ default 0\n    negative = 10   # negative sampling \/ default 5\n)","979dfbea":"from collections import namedtuple\n\nagg = meta[['id', 'original_title', 'pre_overview']]\nTaggedDocument = namedtuple('TaggedDocument', 'words tags')\ntagged_train_docs = [TaggedDocument((c), [d]) for d, c in agg[['original_title', 'pre_overview']].values]","e3cc95b4":"doc_vectorizer.build_vocab(tagged_train_docs)\nprint(str(doc_vectorizer))","d2d8d7cf":"# \ubca1\ud130 \ubb38\uc11c \ud559\uc2b5\nfrom time import time\n\nstart = time()\n\nfor epoch in tqdm(range(5)):\n    doc_vectorizer.train(tagged_train_docs, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\n    doc_vectorizer.alpha -= 0.002 # decrease the learning rate\n    doc_vectorizer.min_alpha = doc_vectorizer.alpha # fix the learning rate, no decay\n\n#doc_vectorizer.train(tagged_train_docs, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\nend = time()\nprint(\"During Time: {}\".format(end-start))","0a277305":"doc_vectorizer.docvecs.most_similar('Toy Story', topn=20)","b62ebe9b":"doc_vectorizer.docvecs.most_similar('Harry Potter and the Deathly Hallows: Part 1', topn=20)","fc318798":"![](https:\/\/drive.google.com\/uc?export=view&id=1g2ausKfoaAT0jMwSatRUG3fiGWfDuysV\n)","5cf88be6":"# \uc804\ucc98\ub9ac \uc218\ud589 \n# \uc131\ub2a5\ud5a5\uc0c1\uc744 \uc704\ud574 5\ud68c \ubbf8\ub9cc \uad6c\ub9e4\ub41c \uc0c1\ud488, 5\ud68c \ubbf8\ub9cc \uad6c\ub9e4\ud55c \uace0\uac1d\uc740 \uc81c\uc678\ud558\uace0 \ubd84\uc11d\uc9c4\ud589\nmovie['check1'] = 0\nmovie.loc[movie['title'].isin(item_agg), 'check1'] = 1\n\nmovie['check2'] = 0\nmovie.loc[movie['userId'].isin(user_agg), 'check2'] = 1\n\nmovie = movie.loc[(movie['check1'] == 1) & (movie['check2'] == 1)].reset_index(drop=True)","c485a36f":"## Word2Vec \uc54c\uace0\ub9ac\uc998\n","2f66ac8a":"## Doc2Vec \uc801\uc6a9","9fd02a3e":"Word2vec \uc801\uc6a9","fb41d49e":"# UserId \ubcc4 MovidID \uad6c\ub9e4 \ubaa9\ub85d\uc744 \uc0dd\uc131 \nitem_agg = movie.groupby(['title'])['userId'].agg({'nunique'}).reset_index()\nitem_agg = item_agg[item_agg['nunique'] >= 5]['title'].values\n\nuser_agg = movie.groupby(['userId'])['title'].agg({'nunique'}).reset_index()\nuser_agg = user_agg[user_agg['nunique'] >= 5]['userId'].values"}}