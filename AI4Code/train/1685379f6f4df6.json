{"cell_type":{"a9dfbc5b":"code","2ad3e317":"code","ff6306d0":"code","d5de6747":"code","710e4179":"code","2e8af9e3":"code","4fa9957a":"code","41cb8e5f":"code","cbb93799":"code","5abd5d37":"code","9dbac95a":"code","93566768":"code","150d58ea":"code","3eb72e25":"code","3824774e":"code","828c3323":"code","7639a6d7":"code","c16a2c4a":"code","a70efb99":"code","bf17aef9":"code","41d3bae4":"code","6d2006bb":"code","e49393fd":"code","b79c34fa":"code","d89a2746":"code","000e64e1":"code","109e1121":"code","de253fb8":"code","e14576d0":"code","d34495b6":"code","e571c6bd":"code","dd705570":"code","a49b20ae":"code","c1641b75":"code","f02d0d30":"code","5bb8fcc9":"code","957f2f9a":"code","04c237c4":"code","cccbf75d":"code","7ac4f503":"code","191ba420":"code","85114e0a":"code","1d06efe0":"code","1248931d":"code","4e676d2b":"code","ad93cfd4":"code","f8c472aa":"code","df306d9f":"code","c0d6fac3":"code","582bf129":"code","6232cbe5":"code","dad592af":"code","9de996d1":"code","965ba510":"code","bed0c7b7":"code","d54f8dbb":"code","dde63d2a":"code","e4e9b625":"code","6ce35424":"code","e547acb1":"code","3a6329cf":"code","07d4e2da":"code","7852a335":"code","7ce5538d":"code","39627b97":"code","6049eec6":"code","e01af4bd":"code","d1e693e9":"code","131e5240":"code","a43913fc":"code","1ff1dc98":"code","4acbd45a":"code","0b2cc91f":"markdown","19027b01":"markdown","4b2bf16f":"markdown","51154a6e":"markdown","0105f4ae":"markdown","4c145c58":"markdown","5cfddcbb":"markdown","8c488fc2":"markdown","67b2c41f":"markdown","775e4033":"markdown","c0166e8c":"markdown","65683a9a":"markdown","9f7bea5e":"markdown","1e79e3b7":"markdown","c827ec15":"markdown","4ffc18b9":"markdown","581f6beb":"markdown","7fbc5b11":"markdown","d21678cd":"markdown"},"source":{"a9dfbc5b":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling \nimport math\nimport numpy as np\nimport gc\nfrom pandas.api.types import CategoricalDtype\nfrom scipy.special import boxcox1p\n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nsns.set(rc={'figure.figsize':(11.7,8.27)})\nimport warnings\nwarnings.filterwarnings(action='once')","2ad3e317":"import zipfile\nzip_paths = ['\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip','\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip','\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv.zip','\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip']\ndirectory_to_extract_to = '\/kaggle\/working'\nfor path_to_zip_file in zip_paths:\n    with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n        zip_ref.extractall(directory_to_extract_to)","ff6306d0":"for dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d5de6747":"rtrain = pd.DataFrame(pd.read_csv('\/kaggle\/working\/train.csv',parse_dates=[2],index_col=2,squeeze=True))\nrfeatures = pd.DataFrame(pd.read_csv('\/kaggle\/working\/features.csv',parse_dates=[1],index_col=1,squeeze=True))\nrtest = pd.DataFrame(pd.read_csv('\/kaggle\/working\/test.csv',parse_dates=[2],index_col=2,squeeze=True))\nrstore = pd.DataFrame(pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv'))","710e4179":"rtrain","2e8af9e3":"# plot all the sales at once\nsns.lineplot(data=rtrain,x=rtrain.index, y=\"Weekly_Sales\")","4fa9957a":"# following takes awefull amount of time and very unclear to see proper patterns. so we plot just a slice of it in below cell.\nsns.lineplot(data=rtrain,x=rtrain.index, y=\"Weekly_Sales\",hue=\"Store\")","41cb8e5f":"storeid = [val*5 for val in range(1,10)]\ntmp_rtrain = pd.DataFrame(rtrain.loc[rtrain['Store'].isin(storeid)])\ntmp_rtrain['Store'].unique()","cbb93799":"# we plot weekly sales of above stores to look in depth\nsns.lineplot(data=tmp_rtrain,x=tmp_rtrain.index, y=\"Weekly_Sales\",hue=\"Store\",ci=None)","5abd5d37":"tmp_rtrain = pd.DataFrame(rtrain.loc[rtrain['Store']==1])\ntmp_rtrain['Store'].unique()                          ","9dbac95a":"sns.lineplot(data=tmp_rtrain,x=tmp_rtrain.index, y=\"Weekly_Sales\",hue=\"Dept\",ci=None)","93566768":"tmp_rtrain = pd.DataFrame(rtrain.loc[rtrain['Store']==2])\nprint (tmp_rtrain['Store'].unique())\nsns.lineplot(data=tmp_rtrain,x=tmp_rtrain.index, y=\"Weekly_Sales\",hue=\"Dept\",ci=None)","150d58ea":"storeid = rtrain['Store'].unique()\nprint (len(storeid))\nprint (len(rtrain['Dept'].unique()))\nprint (45*81)","3eb72e25":"# utilities class \nclass Utils:\n    @classmethod\n    def check_holiday_same(cls, rtrain_orig, rfeatures_orig, storeid):\n        '''\n        This checks the IsHoliday feature of training data is same as IsHoliday feature of features data\n        '''\n        rtrain = rtrain_orig.copy()\n        rfeatures = rfeatures_orig.copy()\n        tmp_rf = rfeatures.loc[rfeatures['Store']==storeid]\n        tmp_df = rtrain.loc[(rtrain['Store']==storeid) & (rtrain['Dept']==1)]\n        tmp_df = tmp_df.rename(columns={'IsHoliday':'storeHoliday'})\n        tmp_df2 = pd.merge(tmp_df,tmp_rf, left_index=True,right_index=True)\n        val = tmp_df2['storeHoliday'].equals(tmp_df2['IsHoliday'])\n        return val\n    \n    @classmethod\n    def missing_vals(cls, indf, id_str=None):\n        if (id_str is None):\n            id_str = 'Id'\n        countdf = indf.count()\n        missdict = {}\n        for key,val in countdf.items():\n            missdict[key] = countdf[id_str] - val\n        missdf = pd.DataFrame(missdict.items(),columns=['name','miss_val'])\n        miss_pct = pd.DataFrame((missdf['miss_val']\/countdf[id_str])*100)\n        miss_pct = miss_pct.rename(columns={'miss_val':'miss_pct'})\n        missdf = pd.concat([missdf,miss_pct],axis=1,join='inner')\n        missdf = missdf.sort_values(by='miss_pct',ascending=False)\n        return missdf\n    \n    @classmethod\n    def apply_boxcox1p(cls, indf, collist,lmd):\n        temp = indf.copy()\n        df = pd.DataFrame(boxcox1p(temp[collist],lmd))\n        temp = temp.drop(columns=collist,axis=1)\n        outdf = pd.concat([temp,df],join=\"inner\",axis=1)\n        return outdf\n    \n    @classmethod\n    def get_dummies(cls, indf, collist):\n        tmp = indf.copy()\n        for col in collist:\n            dummy = pd.get_dummies(indf[col],prefix=col)\n            tmp = pd.concat([tmp, dummy],axis=1,join='inner')\n            tmp.drop(columns=[col],axis=1,inplace=True)\n        return tmp","3824774e":"# check whether IsHoliday in features.csv is duplicate of IsHoliday in train data.\ntmp_list = [Utils.check_holiday_same(rtrain, rfeatures, sid) for sid in range(1,46)]\n'False' in tmp_list","828c3323":"rfeatures.drop(columns='IsHoliday', inplace=True, axis=1)","7639a6d7":"# we should combine features data, store data, into train and test data.\n# merge with index and column is not working. so we are pulling index and making a column and \n# then merging with two columns (data, store)\n\ntemp_train = rtrain.copy()\ntemp_test = rtest.copy()\ntemp_feat = rfeatures.copy()\ntemp_train['Date_col'] = temp_train.index\ntemp_feat['Date_col'] = temp_feat.index\ntemp_test['Date_col'] = temp_test.index\n\ntemp_train = temp_train.merge(temp_feat,how='left')\ntemp_test = temp_test.merge(temp_feat,how='left')\n\ntemp_train = pd.merge(temp_train,rstore, on=['Store'])\ntemp_test = pd.merge(temp_test,rstore, on=['Store'])\n\ntemp_train.rename(columns={'Date_col':'Date'},inplace=True)\ntemp_train.index = temp_train['Date']\ntemp_train.drop('Date',axis=1,inplace=True)\n\ntemp_test.rename(columns={'Date_col':'Date'},inplace=True)\ntemp_test.index = temp_test['Date']\ntemp_test.drop('Date',axis=1,inplace=True)\n\nrtrain = temp_train\nrtest = temp_test\n","c16a2c4a":"del temp_train\ndel temp_test\n# del rfeatures\n# del rstore\n\ngc.collect()","a70efb99":"rtrain.loc[rtrain.IsHoliday == True,'IsHoliday'] =1 \nrtrain.loc[rtrain.IsHoliday == False,'IsHoliday'] =0\n\nrtest.loc[rtest.IsHoliday == True,'IsHoliday'] =1 \nrtest.loc[rtest.IsHoliday == False,'IsHoliday'] =0","bf17aef9":"# lets check missing values of train, test\n\nmiss_train = Utils.missing_vals(rtrain,\"Store\")\nmiss_test = Utils.missing_vals(rtest,\"Store\")","41d3bae4":"miss_train","6d2006bb":"miss_test","e49393fd":"dp_list = ['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']\nrtrain.drop(columns=dp_list, axis=1,inplace=True)\nrtest.drop(columns=dp_list, axis=1,inplace=True)","b79c34fa":"rtest","d89a2746":"# rtest[rtest['CPI'].isnull()].head(40) tells us that in every (store, dept) some equal percent of \n# values are missing in 'CPI' and in 'Unemployment'.\n# so we are filling those missing values as avg of rest of the rows in each of (store, dept)\n\nitr_gp = pd.DataFrame(rtest.groupby(['Store','Dept']))\ntmp_gp = rtest.groupby(['Store','Dept'])\n# ttmp = tmp_gp.get_group((1,1)).mean()\n\npd.set_option('mode.chained_assignment', None)\n\nfor row in itr_gp[0]:\n    storeid = row[0]\n    deptid = row[1]\n    ttmp = tmp_gp.get_group((storeid,deptid)).mean()\n    cpival = ttmp['CPI']\n    unemp = ttmp['Unemployment']\n    rtest.loc[(rtest.Store == storeid) & (rtest.Dept==deptid) & (rtest.CPI.isnull()), 'CPI'] = cpival\n    rtest.loc[(rtest.Store == storeid) & (rtest.Dept==deptid) & (rtest.Unemployment.isnull()), 'Unemployment'] = unemp\n\npd.set_option('mode.chained_assignment', 'raise')","000e64e1":"miss_test = Utils.missing_vals(rtest,\"Store\")\nmiss_test","109e1121":"rtest[(rtest['Store']==4) & (rtest['Dept']==39)]","de253fb8":"tmp_gp2 = rtest.groupby(['Store'])\nfor row in itr_gp[0]:\n    storeid = row[0]\n    deptid = row[1]\n    ttmp = tmp_gp2.get_group(storeid).mean()\n    cpival = ttmp['CPI']\n    unemp = ttmp['Unemployment']\n    rtest.loc[(rtest.Store == storeid)  & (rtest.CPI.isnull()), 'CPI'] = cpival\n    rtest.loc[(rtest.Store == storeid) & (rtest.Unemployment.isnull()), 'Unemployment'] = unemp\n","e14576d0":"# to remove heteroscedasticity, we should apply log transformation. \ncollist = ['Weekly_Sales','Temperature','CPI','Unemployment','Size']\nrtrain = Utils.apply_boxcox1p(rtrain,collist,0)\ncollist = ['Temperature','CPI','Unemployment','Size']\nrtest = Utils.apply_boxcox1p(rtest,collist,0)","d34495b6":"rtrain = Utils.get_dummies(rtrain, ['Type'])\nrtest = Utils.get_dummies(rtest, ['Type'])","e571c6bd":"rtrain.replace([np.inf,-np.inf],np.nan,inplace=True)\nrtest.replace([np.inf,-np.inf],np.nan,inplace=True)","dd705570":"# checked where, when temp is null then changing accordingly.\n# droping all the NaNs for weekly_sales\n# tmp = rtrain.loc[rtrain['Temperature'].isnull()]\n# tmp\ntmp2 = rtrain.loc['2011-02-11',['Store','Dept','Temperature']]\nval = tmp2.loc[tmp2['Store']==7].Temperature.unique()\nval = val[0]\nrtrain.loc[rtrain['Temperature'].isnull(),'Temperature'] = val\nrtrain.dropna(axis=0,inplace=True)","a49b20ae":"miss_train = Utils.missing_vals(rtrain,\"Store\")\nmiss_train","c1641b75":"# we got null temp on 13-01-04 (d1), 13-01-11(d2),13-01-18 (d3) at store 7. \n# so we are replacing them with nearby temp of store 7.\n\nbefore_temp = rtest.loc[(rtest.index=='2012-12-28') & (rtest.Store == 7),'Temperature'][0]\nafter_temp = rtest.loc[(rtest.index=='2013-01-25') & (rtest.Store == 7),'Temperature'][0]\n\nrtest.loc[(rtest.index=='2013-01-04') & (rtest.Store == 7),'Temperature'] = before_temp\nrtest.loc[(rtest.index=='2013-01-11') & (rtest.Store == 7),'Temperature'] = (before_temp+after_temp)\/2\nrtest.loc[(rtest.index=='2013-01-18') & (rtest.Store == 7),'Temperature'] = after_temp","f02d0d30":"miss_test = Utils.missing_vals(rtest,\"Store\")\nmiss_test","5bb8fcc9":"# IsHoliday is of type 'object' lets convert that into int\nrtrain['IsHoliday'] = rtrain.IsHoliday.astype('int')\nrtest['IsHoliday'] = rtest.IsHoliday.astype('int')","957f2f9a":"# some Weekly_sales are -ve. so lets make them min i.e. 0\n# sales can be -ve and it can affect future or past sales. So keeping it.\n# rtrain.loc[rtrain['Weekly_Sales']<0,'Weekly_Sales'] =0","04c237c4":"rtrain.describe()","cccbf75d":"rtest.describe()","7ac4f503":"sample_sid_did = [[1,1],[2,2],[3,3],[4,4],[5,5]]\nsample_sid = [row[0] for row in sample_sid_did]\nsample_did = [row[1] for row in sample_sid_did]\n\nsample_train = rtrain.loc[(rtrain.Store == 1) & (rtrain.Dept ==1)]\nfor row in sample_sid_did:\n    if ((row[0]==1) & (row[1]==1)):\n        continue\n    tmp = rtrain.loc[(rtrain.Store == row[0]) & (rtrain.Dept ==row[1])]\n    sample_train = pd.concat([sample_train,tmp])","191ba420":"# Weekly_sales vs predictors \n\nsns.lineplot(data=sample_train, x=sample_train.index, y='Temperature',hue='Store',palette=\"Set1\")","85114e0a":"fig, axes = plt.subplots(2,2,figsize=(15,8))\nax1, ax2, ax3, ax4 = axes.flatten()\n\nsns.lineplot(data=sample_train, x=sample_train.index, y='Fuel_Price',hue='Store',palette=\"Set1\",ax=ax1)\nsns.lineplot(data=sample_train, x=sample_train.index, y='CPI',hue='Store',palette=\"Set1\",ax=ax2)\nsns.lineplot(data=sample_train, x=sample_train.index, y='Unemployment',hue='Store',palette=\"Set1\",ax=ax3)\nsns.lineplot(data=sample_train, x=sample_train.index, y='Size',hue='Store',palette=\"Set1\",ax=ax4)\n\n# ax2.set_ylabel(\"CPI\") \nplt.show()","1d06efe0":"# correlation between predictors\ntemp_df = sample_train.loc[(sample_train.Store==1)& (sample_train.Dept==1)]\ndcorr = temp_df.corr()\nsns.heatmap(dcorr,cmap=\"coolwarm\",annot=True)","1248931d":"# autocorrelation plot of 'Weekly_sales'\nfrom pandas.plotting import autocorrelation_plot\n\nfor row in sample_sid_did:\n    weekly_sales = rtrain.loc[(rtrain.Store==row[0]) & (rtrain.Dept==row[1]),'Weekly_Sales']\n#     weekly_sales = weekly_sales.diff()\n#     weekly_sales.dropna(inplace=True)\n    ax = autocorrelation_plot(weekly_sales)\n    ax.set_xticks(np.arange(0, 140, 4))\nplt.grid()\nplt.show()","4e676d2b":"# lets check the trend and seasonality of the sample data using seasonal_decompose\n\n","ad93cfd4":"# ADF test to check the stationarity of sample data\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\n\nfor row in sample_sid_did:\n    sid=row[0]\n    did=row[1]\n    weekly_sales = sample_train.loc[(sample_train.Store==sid)& (sample_train.Dept==did)].Weekly_Sales\n    result = adfuller(weekly_sales,autolag='AIC')\n    print (\"store=\"+str(sid)+\", deptid=\"+str(did))\n    print ('ADF statistic: %f' % result[0])\n    print ('p-value: %f'%result[1])\n    print ('critical values')\n    for key, val in result[4].items():\n        print ('\\t%s: %.3f'%(key,val))\n\n    if (result[0]<result[4]['5%']):\n        print (\"----> TS is stationary\")\n    else:\n        print (\"----> TS is not stationary\")","f8c472aa":"# acf and pcaf on original data.\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nwarnings.filterwarnings(action='ignore')\nnumlags=130\nfor row in sample_sid_did:\n    sid=row[0]\n    did=row[1]\n    weekly_sales = sample_train.loc[(sample_train.Store==sid)& (sample_train.Dept==did)].Weekly_Sales\n    fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(20,4))\n#     ax1.plot(weekly_sales);\n    plot_acf(weekly_sales, ax=ax1,lags=numlags)\n    ax1.set_xticks(np.arange(0, 140, 4))\n    ax2.set_xticks(np.arange(0, 140, 4))\n    plot_pacf(weekly_sales, ax=ax2,lags=numlags)\n    plt.show()\n\nwarnings.filterwarnings(action='once')","df306d9f":"# lag1 acf, pacf plots of timeseries.\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nnumlags=70\nfor row in sample_sid_did:\n    sid=row[0]\n    did=row[1]\n    weekly_sales = sample_train.loc[(sample_train.Store==sid)& (sample_train.Dept==did)].Weekly_Sales\n    weekly_sales = weekly_sales.diff()\n    fig, (ax1, ax2,ax3) = plt.subplots(1, 3,figsize=(20,4))\n    ax3.plot(weekly_sales);\n    plot_acf(weekly_sales, ax=ax1,lags=numlags)\n    ax1.set_xticks(np.arange(0, 140, 4))\n    ax2.set_xticks(np.arange(0, 140, 4))\n    plot_pacf(weekly_sales, ax=ax2,lags=numlags)\n    plt.show()","c0d6fac3":"\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\ni=0\nitr_gp = pd.DataFrame(rtest.groupby(['Store','Dept']))\nsample_stores = itr_gp[0]\ntmp_list = []\nerror_cnt=0\nsample_count=200\nfor row in sample_stores:\n    if (i==sample_count):\n        break\n    i=i+1\n    sid=row[0]\n    did=row[1]\n    weekly_sales = rtrain.loc[(rtrain.Store==sid)& (rtrain.Dept==did)].Weekly_Sales\n    weekly_sales = weekly_sales.diff()\n    weekly_sales = weekly_sales.dropna()\n    try:\n        result = adfuller(weekly_sales,autolag='AIC')\n    except:\n        error_cnt=error_cnt+1\n        continue\n#     print (\"store=\"+str(sid)+\", deptid=\"+str(did))\n#     print ('ADF statistic: %f' % result[0])\n#     print ('p-value: %f'%result[1])\n#     print ('critical values')\n#     for key, val in result[4].items():\n#         print ('\\t%s: %.3f'%(key,val))\n#     if (result[0]<result[4]['5%']):\n#         print (\"----> TS is stationary\")\n#     else:\n#         print (\"----> TS is not stationary\")\n    tmp_list.append(result[0]<result[4]['5%'])\n    \nval = (sum(tmp_list)\/(sample_count-error_cnt))\nprint (\"%f percent of samples in %d are stationary\"%(val*100,sample_count))","582bf129":"# lets see the graphs of each timeseires and their lag1.\n\nfig, axes = plt.subplots(5,2,figsize=(20,18))\n\ni=0\nfor row in sample_sid_did:\n    sid=row[0]\n    did=row[1]\n    weekly_sales = rtrain.loc[(rtrain.Store==sid)& (rtrain.Dept==did)].Weekly_Sales\n    sns.lineplot(data=weekly_sales, ax=axes[i,0])\n    weekly_sales_lag1 = weekly_sales.diff()\n    sns.lineplot(data=weekly_sales_lag1, ax=axes[i,1])\n    i=i+1\n# ax2.set_ylabel(\"CPI\") \nplt.show()","6232cbe5":"itr_gp = pd.DataFrame(rtest.groupby(['Store','Dept']))\nsample_stores = itr_gp[0]\ni=0\nwarnings.filterwarnings(action='ignore')\nfor row in sample_stores:\n    weekly_sales = rtrain.loc[(rtrain.Store==row[0]) & (rtrain.Dept==row[1]),'Weekly_Sales']\n    weekly_sales = weekly_sales.diff()\n    weekly_sales.dropna(inplace=True)\n    ax = autocorrelation_plot(weekly_sales)\n    ax.set_xticks(np.arange(0, 140, 4))\n    if (i==10):\n        break\n    i=i+1\n    plt.grid()\n    plt.show()\n\nwarnings.filterwarnings(action='once')","dad592af":"#lets see the data points for depts of each store.\nitr_gp = pd.DataFrame(rtest.groupby(['Store','Dept']))\nsample_stores = itr_gp[0]\ndata_points = []\nfor row in sample_stores:\n    cnt = rtrain.loc[(rtrain.Store==row[0]) & (rtrain.Dept==row[1])].Store.count()\n    data_points.append(cnt)\ndata_points = pd.DataFrame(data_points,columns=['points'])","9de996d1":"data_points.loc[data_points.points<52]","965ba510":"# we are using auto_arima to get best possible values and we cross check them with ours.\n# \"pip install pmdarima\" to install pmdarima using console in kaggle notebook\n# import pmdarima as pm\n# dont know why it says no module named 'pmdarima' even after installing by pip\n\n# so we use manual sarima model.","bed0c7b7":"# following is the official error prediction formula.\n# weighted mean absolute error (weight =5 on holidays)\n\ndef WMAE(dataset, real, predicted):\n    weights = dataset.IsHoliday.apply(lambda x: 5 if x else 1)\n    return np.round(np.sum(weights*abs(real-predicted))\/(np.sum(weights)), 2)","d54f8dbb":"# mtrain is 70% data and mvalid is 30%.\nmtrain = rtrain['2010-02-05':'2012-02-10']\nmvalid = rtrain['2012-02-17':]","dde63d2a":"from statsmodels.tsa.statespace.sarimax import SARIMAX\nimport time\n\nnon_seasonal_order=(0,1,1)\nmyseasonal_order=(1,0,0,52)\nsid=1\ndid=1\nexo_features = ['Store', 'Dept', 'IsHoliday', 'Fuel_Price',\n       'Temperature', 'CPI', 'Unemployment', 'Size', 'Type_A', 'Type_B',\n       'Type_C']\nmtrain_sales = mtrain.loc[(mtrain.Store==sid) & (mtrain.Dept==did),'Weekly_Sales']\nmtrain_exo = mtrain.loc[(mtrain.Store==sid) & (mtrain.Dept==did),exo_features]\n\nstart_time = time.time()\nmodel = SARIMAX(mtrain_sales,order=non_seasonal_order, seasonal_order=myseasonal_order,\n                enforce_stationarity=False, enforce_invertibility=False,exogenous=mtrain_exo)\nmodel_fit = model.fit()\nend_time = time.time()\nprint ('Model fit done in: '+str(end_time-start_time)+\" sec\")","e4e9b625":"print (model_fit.summary())","6ce35424":"import statsmodels.api as sm\ntrain_resid = model_fit.resid\nfig,ax = plt.subplots(2,1,figsize=(15,8))\nfig = sm.graphics.tsa.plot_acf(train_resid, lags=50, ax=ax[0])\nfig = sm.graphics.tsa.plot_pacf(train_resid, lags=50, ax=ax[1])\nplt.show()","e547acb1":"mvalid_exo = mvalid.loc[(mvalid.Store==sid) & (mvalid.Dept==did),exo_features]\nmvalid_actual_result = mvalid.loc[(mvalid.Store==sid) & (mvalid.Dept==did),'Weekly_Sales']\nforecast_vals = model_fit.predict(start='2012-02-17',end='2012-10-26',exog=mvalid_exo)\npredictions = pd.DataFrame(forecast_vals, index=mvalid_exo.index,columns=['pred'])\nvalid_residuals = mvalid_actual_result - predictions.pred","3a6329cf":"sns.lineplot(y=predictions.pred, x=predictions.index,legend='brief',label='pred')\nsns.lineplot(y=mvalid_actual_result,x=predictions.index, legend='brief',label='actual')","07d4e2da":"sns.lineplot(y=valid_residuals,x=predictions.index)","7852a335":"val = WMAE(mvalid_exo, real=mvalid_actual_result,predicted=predictions.pred)\nval","7ce5538d":"# lets create a method to perform modelling for each store, dept combination.\nfrom scipy.special import inv_boxcox\n\ndef sarimax_driver(tot_trdata, train_data_pct,ftest=None):\n    itr_gp = pd.DataFrame(tot_trdata.groupby(['Store','Dept']))\n    all_stores = itr_gp[0]\n    exo_features = ['Store', 'Dept', 'IsHoliday', 'Fuel_Price',\n           'Temperature', 'CPI', 'Unemployment', 'Size', 'Type_A', 'Type_B',\n           'Type_C']\n    fresult = pd.DataFrame(columns=['Id','Weekly_Sales'])\n    val_result = pd.DataFrame(columns=['sales','Store','Dept'])\n    val_result.index.names = ['Date']\n    i=0\n    for row in all_stores:\n        sid = row[0]\n        did = row[1]\n#         if (i==10):\n#             break\n#         i+=1\n#         if (not ((sid==1) & (did==51))):\n#             continue\n        cnt = tot_trdata.loc[(tot_trdata.Store==sid) & (tot_trdata.Dept==did)].Store.count()\n        # if there are very less observations(10 which is less than 10% of most of (sid,did)pairs (143)), it is not a good idea to pred\n        if (cnt<=10):\n            continue\n        trcnt = int(cnt*train_data_pct)\n        valcnt = cnt - trcnt\n        store_data = pd.DataFrame(tot_trdata.loc[(tot_trdata.Store==sid) & (tot_trdata.Dept==did)])\n        trdata = store_data[0:trcnt]\n        valdata = store_data[trcnt:].copy()\n        trsales = trdata.Weekly_Sales\n        trexo = trdata[exo_features]\n        if (ftest is None):\n#             print (str(sid)+\" \"+str(did)+\"\\t\",end=\"\")\n            valexo = valdata[exo_features]\n            # since some of observations are missing in the validation data. We can not use datetime in start, end. we have to use start, end row number.\n#             start_row= valdata.index[0].strftime('%Y-%m-%d')\n#             end_row = valdata.index[len(valdata)-1].strftime('%Y-%m-%d')\n            start_row = trcnt\n            end_row = cnt-1\n            predictions = sarimax_specific(trsales, trexo, valexo, start_row, end_row)\n            if (predictions is None):\n                print (\"Exception in train\/val (sid, did)\"+str(sid)+\" \"+str(did))\n                continue\n            preds = pd.DataFrame(data=predictions, columns=['sales'])\n            preds.index.names = ['Date']\n            valindex = valdata.index\n            predictions = preds.copy()\n            predictions.loc[:,'sales'] = inv_boxcox(predictions['sales'],0)\n            valdata.loc[:,'Weekly_Sales'] = inv_boxcox(valdata['Weekly_Sales'],0)\n            # following is done to modify number index (for non-periodic validation timseries) to datetime index.\n            dtstr = str(predictions.index[0])\n            words = dtstr.split(\"-\")\n            if (len(words)==1):\n                predictions['Date'] = valdata.index\n                predictions.set_index('Date',inplace=True)\n#             print (\"final pred\\n\",predictions)\n            sales_join = pd.DataFrame(pd.merge(valdata['Weekly_Sales'],predictions['sales'],left_index=True,right_index=True))\n            store_result = pd.DataFrame(predictions)\n            store_result['Store'] = sid\n            store_result['Dept'] = did\n            store_result['Weekly_Sales'] = sales_join['Weekly_Sales']\n            store_result['IsHoliday'] = valdata['IsHoliday']\n            val_result = val_result.append(store_result,ignore_index=True)\n            \n        else:\n            test_exo = ftest.loc[(ftest.Store==sid) & (ftest.Dept==did), exo_features]\n            test_cnt = len(test_exo)\n            # some of the store, deptid are present in train data but not in test data.\n            if (test_cnt==0):\n                continue\n            start_row = trcnt\n            end_row = trcnt+test_cnt-1\n            predictions = sarimax_specific(trsales, trexo, test_exo, start_row, end_row)\n            if (predictions is None):\n                print (\"Exception in train\/test (sid, did)\"+str(sid)+\" \"+str(did))\n                continue\n            predictions = inv_boxcox(predictions,0)\n            preds = pd.DataFrame(predictions, columns=['Weekly_Sales'])\n            preds = preds.reset_index()\n            test_date = pd.DataFrame(test_exo.index.strftime('%Y-%m-%d'))\n            test_date['Date'] = str(sid)+\"_\"+str(did)+\"_\"+test_date['Date']\n            test_date.rename(columns={'Date':'Id'},inplace=True)\n            test_date['Weekly_Sales'] = preds['Weekly_Sales']\n#             print (\"final result\\n\",test_date.head())\n            fresult = fresult.append(test_date,ignore_index=True)     \n            \n    if (ftest is None):\n        return pd.DataFrame(val_result)\n    else:\n        return fresult\n            \n\ndef sarimax_specific(trsales, trexo, test_exo, start_row, end_row):\n    non_seasonal_order=(0,1,1)\n    myseasonal_order=(1,0,0,52)\n    try:\n        model = SARIMAX(trsales,order=non_seasonal_order, seasonal_order=myseasonal_order,\n                        enforce_stationarity=False, enforce_invertibility=False,exogenous=trexo)\n        model_fit = model.fit()\n        predictions = model_fit.predict(start=start_row,end=end_row,exog=test_exo)\n    except Exception as e:\n        print (e)\n        predictions = None\n    return predictions\n","39627b97":"# training and validation of all the timeseries models\nimport time\n\nwarnings.filterwarnings(action='ignore')\nt_start = time.time()\nfresult = sarimax_driver(rtrain,0.7)\nt_end = time.time()\nprint (\"training and validation completed in \"+str((t_end-t_start)\/60)+\" mins\")\nwarnings.filterwarnings(action='once')","6049eec6":"# there are still some null Weekly_sales rows(108 out of 0.4 million rows). so we are going to drop them.\n\nval = len (fresult.loc[fresult.Weekly_Sales.isnull()])\nprint (\"Null value rows count: \"+str(val))\nfresult.dropna(inplace=True)\nval = WMAE(fresult,real=fresult['Weekly_Sales'],predicted=fresult['sales'])\nprint (\"Validation score on all the timeseries models: \"+ str(val))","e01af4bd":"# forecasting final test data\nimport time\n\nwarnings.filterwarnings(action='ignore')\nt_start = time.time()\nfresult = sarimax_driver(rtrain,1,rtest)\nt_end = time.time()\nprint (\"training and validation completed in \"+str((t_end-t_start)\/60)+\" mins\")\nwarnings.filterwarnings(action='once')","d1e693e9":"print (fresult.loc[fresult.Weekly_Sales.isnull()])\nprint (len(fresult))","131e5240":"# Evaluation Exception: Submission must have 115064 rows\n# we have 114706 rows. We are not forecasting some of the sid_did pairs which has too less data (<10 rows, sometimes 0 rows of training data(10,99);)\n# so lets add them with 0 value.\n\nsample_sub = pd.DataFrame(pd.read_csv('\/kaggle\/working\/sampleSubmission.csv')) #,parse_dates=[2],index_col=2,squeeze=True))\nsample_sub_set = set(sample_sub['Id'])\nsub_set = set(fresult[\"Id\"])\ndiff_set = sample_sub_set.difference(sub_set)\ndiff_list = list (diff_set)\ndiff_df = pd.DataFrame()\ndiff_df['Id'] = diff_list\ndiff_df['Weekly_Sales'] = 0\ndiff_df","a43913fc":"fresult = fresult.append(diff_df, ignore_index=True)","1ff1dc98":"len(fresult)","4acbd45a":"fresult.to_csv(\"\/kaggle\/working\/sarimax_basic.csv\",index=False)","0b2cc91f":"# Time-series Analysis and Modelling\n\n## Time-series Analysis\n\nWe will apply following tests to check the stationarity and apply some transformations to make it stationary.\n\n* Stationarity Test\n    * ADF\n    \n\n## Modelling\n\nWe will try different models to train\/test the data and compare the results. <br>\nSo lets start with basic to complex. <br>\nOnce done with all the models, lets use mix of models (weighted forecast of models).\n\nFollowing models are used in the order: <br>\n\n1. SARIMA\n    * This algo needs 'Weekly_sales' and it doesn't use other predictors to forecast. So we should put this model accuracy as min-threshold.\n    * There is strong seasonality in the 'Weekly_sales' data. So we used SARIMA (Not ARIMA)\n    * \n2. SARIMAX\n    * This algo considers exogeneous (external) variables to forecast.\n    * \n3. Random Forest and Gradient boosting\n    * These are more general models (unlike ARIMA models). These models use predictors to forecast 'Weekly_sales'. So we make this model accuracy as min-threshold for more advanced models such as 'VAR' and 'Multi-linear regression model with ARIMA'\n    * \n4. VAR (vector auto regression)\n    * This model uses all the predictors and Target.\n    * This model's assumption is \"every predictor is dependent on every other predictor\"\n    * \n5. Multi-linear regression model with ARIMA\n\n6. Mix of multiple models\n    * select best 2,3 models and avg their result to forecast.\n    * This is basically done to reduce over-fitting. ","19027b01":"# Prelim analysis \n\n* Given 2 years of train data and we need to forecast for 1 year of test data.\n* data is weekly\n* Each store, dept pair has 124 weeks of data. \n\nmain points\n1. \"Weekly_sales\" has No trend\n2. \"Weekly_sales\" has strong Seasonality at year end\n3. Each store has different timeseries (evident from plot001)\n    * so we timeseries model for each store (Total 45 store)\n4. Each dept in a perticular store has different timeseries (see plot002)\n    * so we should have timeseries model for each dept (Total 3645 depts = 45 store * 81 dept per store)\n5. ","4b2bf16f":"# Intro\n\nThis is a very interesting problem in which we have not 1 but over 3600 time-series models to forecast. Each model is different in its own way but similar in overall bahaviour of seasonality and trend. <br>\n\nFollowing activities are done on the data (each activity is described detail in the following sections): \n* Data cleaning, transformation, engineering, filling the missing values, etc\n* Exploratory data analysis (EDA) is done to visualize trend and seasonality of sample time-series models and also combination of all the time-series models. \n* Mutliple exciting models are planned to forecast such as \n    * SARIMAX\n    * Vector Auto regression\n    * Multi linear regression with ARIMA\n    * Ensemble of above models\n\nBut in v1, I've used only SARIMAX which performed good-enough for a starter.<br>\nOfficial error evaluation metric is WMAE (weighted mean absolute error) <br>\n\nFollowing are the results:\n\n* Validation error (train-test-split=0.7:0.3) is 2683 \n* Submission error is 3207 (Top 35%)(Total 690 submissions)\n* Note: For comarision 1st rank submission error is 2300.\n\n<br>\n\nFuture activities to be done on this problem:\n* Try above mentioned intended models.\n* Use grid search to find better ARIMA parameters\n* \n","51154a6e":"# EDA conclusion\n\n* 'Temperature' column has strong seasonality\n* 'Fuel_Price' column has both trend and seasonality\n* rest of the columns seems stationary \n* 'Weekly_sales' has strong seasonality at lag52. \n* ","0105f4ae":"As we can see at lag-52 we have lot of correlation for majority of timeseries. So we use it to remove the seasonality. (plot mark: plot_02)","4c145c58":"### SARIMAX model\n\nPoints considered to apply sarima model: \n* As we already analysed trend, seasonal part of timeseries. we will give that info to sarima model and the original training data(not the differenced one.)\n* When we did first diff, trend is gone so (p,d,q)=(0,1,1).\n    * This is non-seasonal part of model\n* As for seasonal part which is still in train.diff data, \n    * from plot_03 we can clearly see there is 52 week seasonality in most of the time-series. So we use that information.\n    * (P,D,Q)m = (1,0,0)52 which means\n    * Auto regress to 52 weeks in seasonal model and use 52 weeks data of MA.\n    * m=52 means, these many data points per year\n    \n    \nConclusion:\n \n* residuals doesn't show any patterns. so, we can consider the model has learned good amount of information and left little for the residuals. \n* model can be improved very much, so lets try other models.\n\nResult:\n* validation score(wmae) = 2683.36 using (p,d,q)=(0,1,1); (P,D,Q)m = (1,0,0)52; train-test-split=0.7\n    * submission score with above specs= ","5cfddcbb":"some time series are stationary and some are not. So, lets make them all stationary by removing the seasonality that we discovered in plot_02 (autocorrelation plot)","8c488fc2":"below is plot002","67b2c41f":"Following is a manual implementation of one timeseries model with SARIMAX algo where automating it to over 3600 timeseries models is done afterwards. ","775e4033":"# EDA (exploratory data analysis)\n\nThis includes \n* Weekly_sales to predictors relationships\n* correlation between predictors to predictors\n* [TODO] seasonal plot (of window 52 weeks). As we can clearly see seasonality, we dont need this now.\n* ","c0166e8c":"All those records whose CPI is still NaN, are those which didn't have values other than NaN. <br>\nso lets fill all those NaN's with their store avg values.","65683a9a":"# Data preprocessing conclusion \n\n* Drop IsHoliday column from features df, As that column is same as IsHoliday of train data.\n* Merged features data with train data (merge on Date, storeid)\n* Merged store data with train data (merge on storeid)\n* Transformed IsHoliday of train\/test data into 1\/0 and nothing else to transform in test data.\n* As Markdown1-5 missing rate is more than 65% and we have lot to do. Lets drop them and continue modelling. Once we complete without Markdown, we add if it requires.\n* rtest[rtest['CPI'].isnull()].head(40) tells us that in every (store, dept) some equal percent of values are missing in 'CPI', 'Unemployment'. so we are filling those missing values as avg of rest of the rows in each of (store, dept)\n* apply boxcox1p(0) transformation on ['Weekly_Sales','Temperature','CPI','Unemployment','Size'] of train\/test data to remove heteroscedasticity.\n* change Type (A\/B) to Type (1\/0) in both train\/test data.\n* check np.inf -np.inf in data and replace it with NaN\n    * In train data, some weekly_sales records became NaN. So i'm just dropping them because i can't do anything about the result. can i?\n    * In train\/test data, we got some Temp as NaN.\n        * In train data, We got NaN for (store 7) on \"2011-02-04\" only. So we are replacing the temp on \"2011-02-11\" (next week) of the store 7.\n        * In test data, we got null temp on 13-01-04 (d1), 13-01-11(d2),13-01-18 (d3) at store 7. so we are replacing them with nearby temp of store 7.\n* check value consistency across all the features\n    * [Not done as sales can be -ve] In train data, some Weekly_Sales are -ve. so make them 0\n        * [improvement_1] all these (Weekly_Sales=0) is skewed. so if we can remove them. it would be better than using them.\n    * All test data is good.\n\n\n**Now the data is in correct form to model.**\n","9f7bea5e":"So we can drop IsHoliday from features data","1e79e3b7":"below is plot001","c827ec15":"As lag1 gives almost all the time series models (for each (storeid, deptid)combination). So we will use lag1 in SARIMA model.","4ffc18b9":"below is plot_03","581f6beb":"[TODO]I think we can drop those (store, dept) which doesn't have 1 year worth of data observations. <br>\ni dont know ","7fbc5b11":"[TODO] Dont know why they are blank. Similarly, their pcf, acf values are also 0. ","d21678cd":"* acf, pacf isn't providing much information as time series is not stationary.\n* lets take a lag1 and check the stationarity of the time series."}}