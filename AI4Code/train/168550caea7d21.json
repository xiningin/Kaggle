{"cell_type":{"683add9c":"code","938c87d8":"code","62b88b37":"code","aa019216":"code","ea6ee35f":"code","a2c2470f":"code","74536730":"code","47c4ee7b":"code","5aa06189":"code","ff8f4183":"code","7c45e6af":"code","77f7bf9c":"code","198c3388":"code","2463a078":"code","448b74c3":"code","0d371a1f":"code","ee08f2c4":"code","79e9f632":"code","bfd0ac4e":"code","c729ad26":"code","cd9c9392":"code","a528b23a":"code","69cf0514":"code","fc4364f9":"code","a64688d6":"code","639f3e21":"code","cea2b289":"code","b35fa9bf":"code","1536497f":"code","f21bf759":"code","0ff14832":"code","f8fa5903":"code","7925e484":"code","aa2529e8":"code","d9c77f9e":"code","caa1e926":"code","f14aedc5":"code","b02af2a2":"code","55ebc1f5":"code","c4d7cc79":"code","b9a0b186":"code","885d44fe":"code","c1463944":"code","3eea5a22":"code","4129cd7a":"code","29e5bf6f":"code","27f7787a":"code","a716b809":"code","4b0c1e5a":"code","7e809d21":"code","c26859d8":"code","58d9c611":"code","905c6ca2":"code","68b9eaea":"code","aeaec540":"code","d2bb0f95":"code","036b4207":"code","ea82e297":"code","d2181adf":"code","352da3f0":"code","772ddeac":"code","c6b97533":"code","e8c273a5":"code","8f6225ca":"code","70cc5759":"code","ae7d5cc7":"code","819fc2ed":"code","ac45e807":"code","090c1d18":"code","d5e08328":"code","66a7aa73":"code","a0bc3c3a":"code","9788ceae":"code","cffa7591":"markdown","a7c32ff7":"markdown","1b556f21":"markdown","9e17cb48":"markdown","6abab04c":"markdown","86c501f2":"markdown","5ecf9fab":"markdown","a39dda40":"markdown","f341e543":"markdown","6b768dd5":"markdown","c600a9d3":"markdown","81de572f":"markdown","e40a95ba":"markdown","dc5ec440":"markdown","3e4eac65":"markdown","749d5f5d":"markdown","542bf96b":"markdown","0622c0e1":"markdown","fff04d05":"markdown","eafaaf30":"markdown","155fa624":"markdown","6dc2a590":"markdown","d612aeed":"markdown"},"source":{"683add9c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","938c87d8":"##firstly need to take data and parting it to know how much \ntrain_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\nprint(train_data.shape,test_data.shape)\n","62b88b37":"## now need to classify data columns according to null values\ntotal_rows = train_data.shape[0]\nremain = pd.DataFrame()\nremove = pd.DataFrame()\nfeatures = train_data.columns.values   ## columns.values gives array otherwise got index\nfeatures.shape","aa019216":"## need to assign respective column names\n## Need to remove features with more than 50% Nulls\nfor feature in features:\n    if train_data[feature].isnull().sum()\/ total_rows > 0.5:\n        remove[feature] = train_data[feature]\n    else:\n        remain[feature] = train_data[feature]\nprint('shape of remove ',remove.columns.values.shape)\nprint('shape of remove ',train_data.shape)\n","ea6ee35f":"### Now need to to EDA(Exploratory Data Analysis) i.e need to remove features not of use i.e summarizing characteristic as very large number of columns \n## Firstly need to analyse the histogram then taking decision accordingly\nimport matplotlib.pyplot as plt\n\n## plotting sale price which is our target\nplt.hist(train_data['SalePrice'])\nplt.title('SalePrice')\nplt.show()\n","a2c2470f":"## Removing the columns having more than 50% null(remove_me contains features names having more than 50% Nulls)\nremove_me = remove.columns.values\ntrain_data = train_data.drop(remove_me,axis = 1)\n\n\n\n## applying on each row\nremain_1 = train_data.columns.values  ## getting remain columns\n\n## Also need to remove columns(features) with more that 30 % zeros\nfor i in remain_1:\n  \n    ## now counting all zeros\n    zero_count = (train_data[i] == 0).sum()\n    ratio = zero_count\/total_rows\n    ## checking conditions\n    if ratio > 0.3: ## drop the column with more than 30% zeros\n        train_data = train_data.drop([i],axis = 1)\n\n## Checking the shape of train data\nprint(train_data.shape)\n        \n    \n    \n","74536730":"## Analysing coorelation using heatmap\n## firstly need to classify which is numerical data and which is categorical data\nnum_data = train_data.select_dtypes(include = 'number')\ncat_data = train_data.select_dtypes(include = 'object')","47c4ee7b":"num_data","5aa06189":"cat_data","ff8f4183":"## Now we have to see coorelation using seaborn(which is an visualization library)\nimport seaborn as sns\n## need to get correlation matrix\ncorrmat = num_data.corr()\n## taken help from other notebook\nfig, ax = plt.subplots(figsize=(12, 12))\nsns.set(font_scale=1.25)\nsns.heatmap(corrmat, vmax=.8, annot=True, square=True, annot_kws={'size': 8}, fmt='.2f')\nplt.show()","7c45e6af":"corrmat ## an table contains coorelation between variables(features)","77f7bf9c":"## selecting only those correlation which has larger effect on sale price(means only considering highly coorelated features)\ncorrmat_sorted = corrmat.sort_values('SalePrice',ascending = False)\ncorrmat_sorted","198c3388":"## selecting Top 10 features\ncorrmat_required = corrmat_sorted[:10]\ncorrmat_required","2463a078":"## getting name of top features(through sorted sales price)\ntop_features = corrmat_required.index\n## taking only that features in column also\ncorrmat_top_10 = corrmat_required[top_features]\n## also plotting their heatmap\n## Now we have to see coorelation using seaborn(which is an visualization library)\n\n## taken help from other notebook\nfig, ax = plt.subplots(figsize=(12, 12))\nsns.set(font_scale=1.25)\nsns.heatmap(corrmat_top_10, vmax=.8, annot=True, square=True, annot_kws={'size': 8}, fmt='.2f')\nplt.show()","448b74c3":"## plotting scatter plot\nsns.set()  ## to set theme for plot(can be for matplot and seaborn both)\nmost_largest_features = corrmat.nlargest(10, 'SalePrice')['SalePrice'].index  ## seelcting top 7 features\nsns.pairplot(num_data[most_largest_features.values], size=1.5)   ## plotting scatter plot\nplt.show()\nprint(most_largest_features)","0d371a1f":"## from the upper scatter plots, we can see the linear ones: \n##(need to consider single from them the one who is highly coorelated with target)\ncorrmat[['TotalBsmtSF','BsmtUnfSF','YearBuilt','YearRemodAdd','GrLivArea','1stFlrSF']]\n","ee08f2c4":"## 3:  Now removing these features ,\nremove_after_corr = ['BsmtUnfSF','YearRemodAdd','1stFlrSF']\n\nnum_data = num_data.drop(remove_after_corr,axis = 1)\ntrain_data = train_data.drop(remove_after_corr,axis = 1)\n","79e9f632":"## 4: Now also need to remove neutral features\nneutral_features = num_data[(num_data['SalePrice'] >=-0.1) & (num_data['SalePrice']<=0.2)].index","bfd0ac4e":"num_data = num_data.drop(neutral_features,axis = 1)\ntrain_data = train_data.drop(neutral_features,axis = 1)","c729ad26":"## seeing shapes of datas after features removal\nprint('shape of training data',train_data.shape,'shape of numerical features table',num_data.shape)","cd9c9392":"## for numerical data\nnum_features = num_data.columns.values\nfor feature in num_features:\n    num_data[feature] = num_data[feature].fillna(num_data[feature].mode()[0])\n    train_data[feature] = train_data[feature].fillna(train_data[feature].mode()[0])","a528b23a":"num_data.isnull().sum().sum()","69cf0514":"### Handling missing data in categorical data and in numerical data also\n## we can replace it with mode value\ncat_features = cat_data.columns.values\nfor feature in num_features:\n    num_data[feature] = num_data[feature].fillna(num_data[feature].mean())\n    train_data[feature] = train_data[feature].fillna(train_data[feature].mean())\nfor feature in cat_features:\n    cat_data[feature] = cat_data[feature].fillna(cat_data[feature].mode()[0])","fc4364f9":"## seeing presence of null values\nprint(num_data.isnull().sum().sum(),cat_data.isnull().sum().sum())","a64688d6":"## for this need to plot scatter plot for num data\n## need to plot each feature with respect to \nfor feature in num_data.columns.values:\n    plt.scatter(train_data[feature], train_data['SalePrice'])\n    print('given below is plot for ',feature)\n    plt.show()","639f3e21":"num_data","cea2b289":"## we got outliers in \n#LotFrontage(limit 200)\n#LotArea(limit 100000)\n#TotalBstmSF(limit 4000)\n# GrLivArea(limit 4000)\n#SalePrice(Limit 650000)\nlimits = [['LotFrontage',200],['LotArea',100000],['TotalBsmtSF',4000],['GrLivArea',4000],['SalePrice',650000]]","b35fa9bf":"## Now to make them excluding,from our normal data(NEED TO DELETE THOSE ROWS)\nremove_index = []\nfor limit in limits:\n    index = list(num_data[num_data[limit[0]] > limit[1]].index)\n    print(index)\n    remove_index += index\nremove_index = (list(set(remove_index)))\nremove_index\n","1536497f":"## Now need to remove these outliers,i.e need to remove these rows\n## need id first and then remove them\nremove_ids = []\nfor index in remove_index:\n    ids = (num_data['Id'])[index]\n    remove_ids.append(ids)\nremove_ids","f21bf759":"num_data","0ff14832":"## some debugging for how to get index using ids\nlist(num_data[(num_data['Id'] == 504)].index)","f8fa5903":"## need to remove these ids , because if we remove index one by one, whole data index changes every time and we are not able to remove proper row\n## dropping for axis = 0 ,means from row\nfor i in remove_ids:\n    required = list(num_data[(num_data['Id'] == i)].index)\n    num_data = num_data.drop(required,axis = 0)  \n    cat_data = cat_data.drop(required,axis = 0)  ## need to drop from categorical data also otherwise after concatenating we get nulls\n    \n    ## searching everytime ith id and giving index to num_data so need to drop it","7925e484":"num_data[(num_data['Id'] == 707)] ## We are checking that ids removed or not","aa2529e8":"## making our new dataset(CONCATENATING NUMERICAL AND CATEGORICAL DATA)\nmodified_trained_data_old = pd.concat([num_data,cat_data],axis = 1) ## axis = 1 means concat on columns","d9c77f9e":"modified_trained_data_old[modified_trained_data_old['Id'] == 707]","caa1e926":"## checking nulls here\nmodified_trained_data_old.isnull().sum()","f14aedc5":"cat_data.isnull().sum().sum()","b02af2a2":"## Now need to convert categorical to dummies( i.e equivalent vectors)\nmodified_trained_data = pd.get_dummies(modified_trained_data_old)\nmodified_trained_data","55ebc1f5":"## Id is of no use for price prediciton, so  dropping it\nmodified_train_data = modified_trained_data.drop(['Id'],axis = 1)","c4d7cc79":"## seeing distribution of num_data features to see need of normalisation or not\n\n## need to import statistics tools first\n# Statistics\nfrom scipy.stats import norm\nfrom scipy import stats\n\n## first for sale price\nsns.distplot(modified_train_data['SalePrice'], fit=norm)  ## To get Histogram to see distribution\nfig = plt.figure()\n\n","b9a0b186":"## now seeing for all numerical features:\nfor feature in num_data.columns.values:\n    if feature != 'Id':\n        sns.distplot(modified_train_data[feature], fit=norm)  ## To get Histogram to see distribution\n        fig = plt.figure()\n","885d44fe":"## Need to normalise(can infer from the plots below given features need normalisation to follow normal curve)\nnor_feature = ['LotFrontage',\n'LotArea',\n'TotalBsmtSF',\n'GrLivArea',\n'GarageArea',\n'SalePrice']","c1463944":"## As Target is positively skewed so taking log of it can solve our problem\n## to normalise other features we can divide them by (square root of sum of squares for respective column)\nfor feature in nor_feature:\n    normalising_constant = np.sqrt(np.sum((np.square(modified_train_data[feature]))))\n    modified_train_data[feature] = modified_train_data[feature].apply(lambda x: np.log(x) if feature == 'SalePrice' else x\/normalising_constant)\n    ## plotting the histogram to see result after normalisation\n    sns.distplot(modified_train_data[feature], fit=norm)  ## To get Histogram to see distribution\n    fig = plt.figure()\n    ","3eea5a22":"modified_train_data.shape  ### we got 257 as columns because we created vector using dummy for categorical","4129cd7a":"## Now need to use algorithms, and also need to import metrics for evaluation, and also removing error that comes\n# ML\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error  ## (Evaluation metrics)\nfrom sklearn.linear_model import LinearRegression                       ## Basic linear regressor\nfrom sklearn.model_selection import train_test_split                    \nfrom sklearn.ensemble import RandomForestRegressor                      ## random forest regressor (bagging technique)\nimport xgboost as xg                                                    ## Boosting technique\n\n# Another(SO NOT GET WARNING EVERY TIME)\nimport warnings\nwarnings.filterwarnings('ignore')","29e5bf6f":"## need to make train data\ntarget = modified_train_data['SalePrice']\ntrain_data_to_model = modified_train_data.drop(['SalePrice'],axis = 1)","27f7787a":"modified_train_data['SalePrice'].mode()[0] ### need to select 0th index to replace","a716b809":"train_data_to_model.shape","4b0c1e5a":"from sklearn.model_selection import train_test_split\ntrain_x,val_x,train_y,val_y = train_test_split(train_data_to_model,target,test_size = 0.2)\n\n## Note: Splitting into validation to test accuracy score of algorithms before submitting","7e809d21":"## simplifying Notation\nX,y = train_x, train_y\n","c26859d8":"## reverfying that data not contain null\nfor feature in X.columns.values:\n    logic = X[feature].isnull().values.any()\n    total_nan = X[feature].isnull().sum()\n    if logic == True:\n        print('This is bad column %s and the number of nulls %d'%(feature,total_nan))\n        ## replacing all of them with mode, \n        X[feature].fillna(X[feature].mode()[0],inplace = True) #### taking  first element \n        \n","58d9c611":"lg = LinearRegression() ## calling algo\n## need to fit(train)\nlg.fit(X,y)\n## need to do prediciton(using training data to see training accuracy)\npred = lg.predict(X)\n## checking score\nprint('train accurfacy %.2f and validation accuracy %.2f'%(lg.score(X,y),lg.score(val_x,val_y)))\n\n## checking MSLE(Mean squared log error)\nprint(mean_squared_log_error(pred,y))\n","905c6ca2":"## now using random forest\nrf = RandomForestRegressor()\nrf.fit(X,y)\n## predicting\npred = rf.predict(X) ## cheecking on test data\n## getting score\nprint('train accurfacy %.2f and validation accuracy %.2f'%(rf.score(X,y),rf.score(val_x,val_y)))\n\n## getting MSLE\nprint(mean_squared_log_error(pred,y))","68b9eaea":"## Using ensemble method\nfrom sklearn import metrics\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n## training\ng = GradientBoostingRegressor(n_estimators = 200, learning_rate = 1.5, max_depth = 3)\ntrain = g.fit(X,y)\nscore = g.score(X,y)\npercentage = \"{:.0%}\".format(score)\ny_pred = g.predict(val_x)\nprint('MAE:', metrics.mean_absolute_error(val_y, y_pred))  \nprint('MSE:', metrics.mean_squared_error(val_y, y_pred))  \nprint('RMSE:', np.sqrt(metrics.mean_squared_error(val_y, y_pred)))\nprint('VarScore:',metrics.explained_variance_score(val_y,y_pred))\nprint('Acc_Score:',percentage)","aeaec540":"## checking null values in test data\ntest_data.isnull().sum().sum()","d2bb0f95":"test_data.shape","036b4207":"test_data.shape","ea82e297":"## handling missing in test data \nimport random\n## dividing in numerical and categorical need column names\nnum_data_test = (test_data.select_dtypes(include = 'number')).columns.values\ncat_data_test = (test_data.select_dtypes(include = 'object')).columns.values\n## handling null separately\nfor m in num_data_test:\n    test_data[m] = test_data[m].fillna(test_data[m].mean())  ## mean in numerical values\nfor n in cat_data_test:\n    test_data[n] = test_data[n].fillna(test_data[n].mode()[0]) \n\n    \nmodified_test_data = test_data.copy()","d2181adf":"##first checking which feature has maximum number of nulls so operating it\ntest_data.isnull().sum().sum()","352da3f0":"modified_test_data","772ddeac":"## as we normalised these features in our test data so normalising them here also\nnor_feature = ['LotFrontage',\n'LotArea',\n'TotalBsmtSF',\n'GrLivArea',\n'GarageArea']\n\nfor feature in nor_feature:\n    normalising_constant = np.sqrt(np.sum((np.square(modified_test_data[feature]))))\n    modified_test_data[feature] = modified_test_data[feature].apply(lambda x: x\/normalising_constant)\n    ## plotting the histogram to see result after normalisation\n    sns.distplot(modified_test_data[feature], fit=norm)  ## To get Histogram to see distribution\n    fig = plt.figure()","c6b97533":"modified_test_data.shape","e8c273a5":"## checking total nulls present\nmodified_test_data.isnull().sum().sum()","8f6225ca":"## now producing dummies for categorical values(i.e one hot encoded vector)\nmodified_test_data = pd.get_dummies(modified_test_data)","70cc5759":"print(modified_test_data.shape,modified_train_data.shape)  ","ae7d5cc7":"## need to store ids first\nids_test = test_data['Id']\nmodified_test_data = modified_test_data.drop(['Id'],axis = 1)  \n","819fc2ed":"## dropping columns which are not in train data\ntrained_features = X.columns.values\nfor i in modified_test_data.columns.values:\n    if i not in trained_features:  ## means if feature not present so not true so go into loop\n        modified_test_data = modified_test_data.drop([i],axis = 1)  \n        ","ac45e807":"## to get equal columns as training making those zero who are not present in testing\n## (so to have same number of features in training also)\nrest = set(X.columns.values) - set(list(modified_test_data.columns.values))\nfor feature in list(rest):   ## rest contains features which are in training not in testing, so making them zero\n    modified_test_data[feature] = 0","090c1d18":" modified_test_data.shape","d5e08328":"## using lg regressor as it has maximum accuracy\n#predict_lg = lg.predict(modified_test_data)\n## need to denormalise it\n#prediction_denorm_lg = np.exp(predict_lg)  ## we have taken exponential to neutral the effect of normalisation done during training","66a7aa73":"## using GB regressor\npredict_gbr = g.predict(modified_test_data)\n## denormalise it\npredicted_denorm_gbr = np.exp(predict_gbr)\n","a0bc3c3a":"## now making our prediction to act as dataframe\nsubmission = pd.DataFrame({\n        \"Id\": ids_test,\n        \"SalePrice\": predicted_denorm_gbr})\nsubmission.to_csv('submission.csv', index=False)\n","9788ceae":"submission","cffa7591":"## Now need to analyse using scatter plot to see who is having linear relation, removing those features\n","a7c32ff7":"# Select the feature having more correlation with target\n## 1: TotalBsmtSF and BsmtUnfSF --> TotalBsmtSF\n## 2: YearBuilt and YearRemodAdd --> YearBuilt\n## 3: GrLivArea and lstFlrSF --> GrLivArea\n## 4: GrLivArea and TotalBsmtSF --> GrLivArea\n","1b556f21":"### Need to handle outliers which are disturbing our model","9e17cb48":"###### BELOW CODE IS ONLY FOR UPLOADING PURPOSE KINDLY IGNORE","6abab04c":"### Using Algorithms which are used for regression tasks","86c501f2":"## Now our test data ready need to predict.....","5ecf9fab":"# HANDLING OUTLIERS","a39dda40":"### At last before feeding data to algorithm, need to normalise them.","f341e543":"## Firstly need to load Data","6b768dd5":"### Now we remained with  fetaures need to filter out some more features\n## 1: we removed features with high nulls(>50%)\n## 2: removed features with high zeros(>30%)\n## 3: remove coorelated features(using heat map and scatter plot)\n## 4: remove neutral features i.e whose corr is between [-0.1,0.2]","c600a9d3":"Applying statistics and visualization principles ","81de572f":"## Step-1 NEED TO DO DATA ENGINEERING","e40a95ba":"## Yes, Successfully removed outliers","dc5ec440":"#### Now need to train our model with this featured data.","3e4eac65":"## NORMALISATION","749d5f5d":"## Modifications For Uploading","542bf96b":"THANK YOU VERY MUCH TO READ MY BAD PROGRAMMING SKILLS, CURRENTLY I AM LEARNING AND TRY TO KEEP CODE AS SIMPLE AS POSSIBLE AND USING LESS INBUILT FUNCTIONS BECAUSE I THINK LOGIC MATTERS THE MOST THAT HOW ONE CAN TACKLE AN PROBLEM","0622c0e1":"## Handling Missing data","fff04d05":"# **EDA(Exploratory Data Analysis)**","eafaaf30":"### PLEASE LEAVE YOUR VALUABLE COMMENTS SO I CAN LEARN MORE","155fa624":"#### We successfully reduced features in numerical data,and also in train data which can make our model more accurate","6dc2a590":"Filling of missing data can be in several ways:\n\n**For numerical Data:**\n1. Fill with mean\n2. Fill with median\n3. Fill with (mean - std) to (mean + std) as according to normal distribution curve within one standard deviation 67 % of data was there\n\n**For Categorical Data:**\n1. Fill With Mode\n2. Fill with step 1 but also analysing error at the same time to make it more accurate","d612aeed":"## TRAINING"}}