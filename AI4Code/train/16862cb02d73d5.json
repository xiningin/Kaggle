{"cell_type":{"1945867f":"code","6771e9e0":"code","cc6eb949":"code","905e62a1":"code","b3de0d72":"code","610f061f":"code","215a78c7":"code","22f3b6e4":"code","5de5e70b":"code","12c51187":"code","07ae2321":"code","3e7430da":"code","8226a87a":"code","3eeaa4d2":"code","76e6b071":"markdown","e11aa381":"markdown","21d4ccbc":"markdown","a9349d6e":"markdown","3ce41ae4":"markdown","10fda675":"markdown","c6be01e5":"markdown","d6648ddc":"markdown","d2a66de1":"markdown","c4392978":"markdown","2047ba17":"markdown","3b614a08":"markdown","e1ed3ae8":"markdown","28393083":"markdown","2fc17883":"markdown","4f7edc2b":"markdown","e0fcf0db":"markdown"},"source":{"1945867f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport warnings  \nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6771e9e0":"\ndf=pd.read_csv(\"..\/input\/metric_data.csv\")\ndf.head()","cc6eb949":"metrics_df=pd.pivot_table(df,values='actuals',index='load_date',columns='metric_name')\nmetrics_df.head()","905e62a1":"metrics_df.reset_index(inplace=True)\nmetrics_df.fillna(0,inplace=True)\nmetrics_df.head()","b3de0d72":"metrics_df.columns\n","610f061f":"#specify the 12 metrics column names to be modelled\nto_model_columns=metrics_df.columns[1:13]","215a78c7":"from sklearn.ensemble import IsolationForest\nclf=IsolationForest(n_estimators=100, max_samples='auto', \\\n                        max_features=1.0, bootstrap=False, n_jobs=-1, random_state=42, verbose=0)\nclf.fit(metrics_df[to_model_columns])","22f3b6e4":"pred = clf.predict(metrics_df[to_model_columns])\nmetrics_df['anomaly']=pred\noutliers=metrics_df.loc[metrics_df['anomaly']==-1]\noutlier_index=list(outliers.index)\n#print(outlier_index)\n#Find the number of anomalies and normal points here points classified -1 are anomalous\nprint(metrics_df['anomaly'].value_counts())\n","5de5e70b":"import matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom mpl_toolkits.mplot3d import Axes3D\npca = PCA(n_components=3)  # Reduce to k=3 dimensions\nscaler = StandardScaler()\n#normalize the metrics\nX = scaler.fit_transform(metrics_df[to_model_columns])\nX_reduce = pca.fit_transform(X)\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.set_zlabel(\"x_composite_3\")\n\n# Plot the compressed data points\nax.scatter(X_reduce[:, 0], X_reduce[:, 1], zs=X_reduce[:, 2], s=4, lw=1, label=\"inliers\",c=\"green\")\n\n# Plot x's for the ground truth outliers\nax.scatter(X_reduce[outlier_index,0],X_reduce[outlier_index,1], X_reduce[outlier_index,2],\n           lw=2, s=60, marker=\"x\", c=\"red\", label=\"outliers\")\nax.legend()\nplt.show()","12c51187":"from sklearn.decomposition import PCA\npca = PCA(2)\npca.fit(metrics_df[to_model_columns])\n\n\nres=pd.DataFrame(pca.transform(metrics_df[to_model_columns]))\n\nZ = np.array(res)\nfigsize=(12, 7)\nplt.figure(figsize=figsize)\nplt.title(\"IsolationForest\")\nplt.contourf( Z, cmap=plt.cm.Blues_r)\n\nb1 = plt.scatter(res[0], res[1], c='blue',\n                 s=40,label=\"normal points\")\n\nb1 = plt.scatter(res.iloc[outlier_index,0],res.iloc[outlier_index,1], c='red',\n                 s=40,  edgecolor=\"red\",label=\"predicted outliers\")\nplt.legend(loc=\"upper right\")\nplt.show()\n","07ae2321":"#Installing specific version of plotly to avoid Invalid property for color error in recent version which needs change in layout\n!pip install plotly==2.7.0","3e7430da":"from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.plotly as py\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\ndef plot_anomaly(df,metric_name):\n    df.load_date = pd.to_datetime(df['load_date'].astype(str), format=\"%Y%m%d\")\n    dates = df.load_date\n    #identify the anomaly points and create a array of its values for plot\n    bool_array = (abs(df['anomaly']) > 0)\n    actuals = df[\"actuals\"][-len(bool_array):]\n    anomaly_points = bool_array * actuals\n    anomaly_points[anomaly_points == 0] = np.nan\n    #A dictionary for conditional format table based on anomaly\n    color_map = {0: \"'rgba(228, 222, 249, 0.65)'\", 1: \"yellow\", 2: \"red\"}\n    \n    #Table which includes Date,Actuals,Change occured from previous point\n    table = go.Table(\n        domain=dict(x=[0, 1],\n                    y=[0, 0.3]),\n        columnwidth=[1, 2],\n        # columnorder=[0, 1, 2,],\n        header=dict(height=20,\n                    values=[['<b>Date<\/b>'], ['<b>Actual Values <\/b>'], ['<b>% Change <\/b>'],\n                            ],\n                    font=dict(color=['rgb(45, 45, 45)'] * 5, size=14),\n                    fill=dict(color='#d562be')),\n        cells=dict(values=[df.round(3)[k].tolist() for k in ['load_date', 'actuals', 'percentage_change']],\n                   line=dict(color='#506784'),\n                   align=['center'] * 5,\n                   font=dict(color=['rgb(40, 40, 40)'] * 5, size=12),\n                   # format = [None] + [\",.4f\"] + [',.4f'],\n                   # suffix=[None] * 4,\n                   suffix=[None] + [''] + [''] + ['%'] + [''],\n                   height=27,\n                   fill=dict(color=[test_df['anomaly_class'].map(color_map)],#map based on anomaly level from dictionary\n                   )\n                   ))\n    #Plot the actuals points\n    Actuals = go.Scatter(name='Actuals',\n                         x=dates,\n                         y=df['actuals'],\n                         xaxis='x1', yaxis='y1',\n                         mode='line',\n                         marker=dict(size=12,\n                                     line=dict(width=1),\n                                     color=\"blue\"))\n\n    #Highlight the anomaly points\n    anomalies_map = go.Scatter(name=\"Anomaly\",\n                               showlegend=True,\n                               x=dates,\n                               y=anomaly_points,\n                               mode='markers',\n                               xaxis='x1',\n                               yaxis='y1',\n                               marker=dict(color=\"red\",\n                                           size=11,\n                                           line=dict(\n                                               color=\"red\",\n                                               width=2)))\n\n\n    axis = dict(\n        showline=True,\n        zeroline=False,\n        showgrid=True,\n        mirror=True,\n        ticklen=4,\n        gridcolor='#ffffff',\n        tickfont=dict(size=10))\n\n    layout = dict(\n        width=1000,\n        height=865,\n        autosize=False,\n        title=metric_name,\n        margin=dict(t=75),\n        showlegend=True,\n        xaxis1=dict(axis, **dict(domain=[0, 1], anchor='y1', showticklabels=True)),\n        yaxis1=dict(axis, **dict(domain=[2 * 0.21 + 0.20, 1], anchor='x1', hoverformat='.2f')))\n\n    fig = go.Figure(data=[table, anomalies_map, Actuals], layout=layout)\n\n    iplot(fig)\n    pyplot.show()\n    #return res","8226a87a":"def classify_anomalies(df,metric_name):\n    df['metric_name']=metric_name\n    df = df.sort_values(by='load_date', ascending=False)\n    #Shift actuals by one timestamp to find the percentage chage between current and previous data point\n    df['shift'] = df['actuals'].shift(-1)\n    df['percentage_change'] = ((df['actuals'] - df['shift']) \/ df['actuals']) * 100\n    #Categorise anomalies as 0-no anomaly, 1- low anomaly , 2 - high anomaly\n    df['anomaly'].loc[df['anomaly'] == 1] = 0\n    df['anomaly'].loc[df['anomaly'] == -1] = 2\n    df['anomaly_class'] = df['anomaly']\n    max_anomaly_score = df['score'].loc[df['anomaly_class'] == 2].max()\n    medium_percentile = df['score'].quantile(0.24)\n    df['anomaly_class'].loc[(df['score'] > max_anomaly_score) & (df['score'] <= medium_percentile)] = 1\n    return df\n","3eeaa4d2":"import warnings  \nwarnings.filterwarnings('ignore')\nfor i in range(1,len(metrics_df.columns)-1):\n    clf.fit(metrics_df.iloc[:,i:i+1])\n    pred = clf.predict(metrics_df.iloc[:,i:i+1])\n    test_df=pd.DataFrame()\n    test_df['load_date']=metrics_df['load_date']\n    #Find decision function to find the score and classify anomalies\n    test_df['score']=clf.decision_function(metrics_df.iloc[:,i:i+1])\n    test_df['actuals']=metrics_df.iloc[:,i:i+1]\n    test_df['anomaly']=pred\n    #Get the indexes of outliers in order to compare the metrics with use case anomalies if required\n    outliers=test_df.loc[test_df['anomaly']==-1]\n    outlier_index=list(outliers.index)\n    test_df=classify_anomalies(test_df,metrics_df.columns[i])\n    plot_anomaly(test_df,metrics_df.columns[i])","76e6b071":"Next I will write more on **forecasting time series and identifying anomalies**,methods of doing it along with their pros and cons. \n\n**Thank you.Your suggestions and comments are welcome.**\n\n**Please do share the kernal if you find it useful and interesting.**","e11aa381":"Yes, from the plots we are able to capture the **sudden spikes, dips in the metrics and projects them**. \n\nAlso the **conditional formatted table** gives us insights on cases like data not present(value is zero) captured as high anomaly which could be a **potential result of broken pipeline in data processing** which needs fixing along with highlighting high and low level anomalies.","21d4ccbc":"The anomalies identified by the algorithm should make sense when viewed **visually**(sudden dip\/peaks) by the business user to action upon it. So creating a** good visualization is equally important in this process**.\n\nThis function creates **actuals plot on a time series with anomaly points highlighted on it**. Also a table which provides actual data, the change and conditional formatting based on anomalies.","a9349d6e":"A sudden spike or dip in a metric is an **anomalous** behavior and both the cases needs attention. Detection of anomaly can be solved by supervised learning algorithms if we have information on anomalous behavior before modeling, but initially without feedback its difficult to identify that points. So we model this as an **unsupervised** problem using algorithms like **Isolation Forest**,One class SVM and LSTM. Here we are identifying anomalies using isolation forest.","3ce41ae4":"How to use this?\n\nIf the **current timestamp is anomalous** for a use case **drill down** to metrics figure out the set of metrics which have high anomalies in the timestamp to perform RCA on it.\n\nAlso a **feedback** from the business user can be updated back in the data which would help in turning this to a supervised\/semi supervised learning problem and compare their results.\n\nA enhancement here would be to **combine anomalous behaviour which occur continously** . For eg. big sale days which would result in spike in metrics for few days could be shown as a single behaviour.\n\n\n","10fda675":"So a 2D plot gives us a clear picture that the algorithm classifies anomalies points in the use case rightly. \n\nAnomalies are highlighted as **red edges** and normal points are indicated with **green points** in the plot. \n\nHere the **contamination** parameter plays a great factor.\nOur idea here is to capture all the anomalous point in the system. \nSo its **better to identify few points which might be normal as anomalous(false positives) ,but not to miss out catching an anomaly(true negative)**.(So i have specified 12% as contamintion which varies based on use case)\n","c6be01e5":"Now as we see the 3D point the anomaly points are mostly wide from the cluster of normal points,but a 2D point will help us to even judge better.\nLets try plotting the same fed to a PCA reduced to 2 dimensions.","d6648ddc":"Hi everyone.\nThis is my first kernel in kaggle and thought of writing about **anomaly detection** and its **visualization**. First I would like to thank **Sudalai Rajkumar** for his kernels which are great inspiration to me.","d2a66de1":"Identify anomalies for individual metrics and plot the results. \n\n**X axis -  date \nY axis - Actual values and anomaly points.**\n\nActual values of metrics are indicated in the **blue line** and anomaly points are highlighted as **red points**.\n\nIn the table, background **red** indicates high anomalies and **yellow** indicates low anomalies.","c4392978":"Special thanks to my manager **Shrinivas Ron**, **Goldee** and my teammates **Sudarson, Adam**    in Myntra for their help in anomaly detection.","2047ba17":"Define **isolation forest** and specify parameters.\u00a0\n\nIsolation forest tries to **separate each point** in the data.In case of 2D it randomly creates a line and tries to single out a point. Here an **anomalous point could be separated in few steps while normal points which are closer could take significantly more steps to be segregated.**\n\nI am not going deep into each parameter.**Contamination** is an important parameter here and I have not specified the any value for it as its unsupervised and we dont have information on percentage of outliers.You can also specify it by trial and error on validating its results with outliers in 2D plot or if your data is supervised use that information to specify it. It stands for **percentage of outlier points** in the data.\n\n\nI am using **sklearn's Isolation Forest** here as it is a small dataset with few months of data, while recently **h2o's **isolation forest is also available which is more scalable on high volume datasets would be worth exploring.\n\n*More details of the algorithm can be found here\u00a0: https:\/\/cs.nju.edu.cn\/zhouzh\/zhouzh.files\/publication\/icdm08b.pdf\n\nMore details on H2O Isolation forest\u00a0: https:\/\/github.com\/h2oai\/h2o-tutorials\/tree\/master\/tutorials\/isolation-forest\n\u00a0       *","3b614a08":"The data here is for a **use case**(eg revenue,traffic etc ) is at a day level with 12 metrics.We have to identify first if there is an anomaly at an use case level.Then for better actionalbility we **drill down** to individual metrics and identify anomalies in them. \n","e1ed3ae8":"Now I do a **pivot** on the dataframe to create a dataframe with all metrics at a **date level.**","28393083":"A helper function to find percentage change,classify anomaly based on **severity**.\n\nThe **predict function** classifies the data as anomalies based on the results from **decision function** on crossing a threshold.\nSay if the business needs to find the next level of anomalies which might have an impact,this could be used to identify those points.\n\nThe top 12 quantile are identified anomalies(high severity),based on decision function here we identify the 12-24 quantile points and classify them as low severity anomalies.\n","2fc17883":"Level the multi-index pivot dataframe and treat na with 0","4f7edc2b":"Now we have figured the **anomalous behaviour** at a use case level.But to be actionable on the **anomaly** its important to identify and provide information on **which metrics are anomalous in it individually.**","e0fcf0db":"Now here we have 12 metrics on which we have classified anomalies based on isolation forest.We will try to **visualize** the results and check if the classification makes sense.\n\nNormalize and fit the metrics to a **PCA** to reduce the number of dimensions and then plot them in 3D highlighting the anomalies."}}