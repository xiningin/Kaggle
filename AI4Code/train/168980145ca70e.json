{"cell_type":{"f2b36122":"code","3cce54e2":"code","e26c2b96":"code","26a3873a":"code","c80a6158":"code","75ced69a":"code","26556d18":"code","0b178c06":"code","45997a9f":"code","cf00fce3":"code","64804ef4":"code","c54274be":"code","c5ae1b58":"code","6daf69e9":"code","c36ceb3c":"code","874e9991":"code","8ab49a42":"code","42ce87e2":"code","a32fd244":"code","5de72548":"code","efaffe14":"code","0e8d1b06":"code","0bf04a3f":"code","f113051a":"code","6dfeaf24":"code","71b9c31f":"code","dee1ed69":"markdown","13cfa65b":"markdown","d024ff6a":"markdown","1617a3bd":"markdown","9720ced4":"markdown","ef4eae18":"markdown","0b5cded5":"markdown","77f3d0ec":"markdown","ba86bc5c":"markdown","c2fcfae3":"markdown","29b1c497":"markdown","d1a3d2d7":"markdown","70d095f7":"markdown","3a00edb9":"markdown","d95b33f5":"markdown","8ef6f41b":"markdown","8f176ccf":"markdown","6c2edbe1":"markdown","9dbeee29":"markdown","8e8e2edf":"markdown","d0b8aec5":"markdown","1a077aff":"markdown","d610562a":"markdown","a7056373":"markdown","fb46943e":"markdown","40dd14bf":"markdown","15b0465c":"markdown","6b0f987f":"markdown","3479759c":"markdown","918dba11":"markdown","bfda5c4d":"markdown","599e1d91":"markdown","6388e902":"markdown"},"source":{"f2b36122":"import os\nimport shutil\nimport glob\nimport urllib.request\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf ","3cce54e2":" glob.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/*\/*')","e26c2b96":"img_normal = plt.imread('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train\/NORMAL\/IM-0131-0001.jpeg')\nimg_penumonia_bacteria = plt.imread('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train\/PNEUMONIA\/person1017_bacteria_2948.jpeg')\nimg_penumonia_virus = plt.imread('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train\/PNEUMONIA\/person1021_virus_1711.jpeg')\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1,3,1).set_title('NORMAL')\nplt.imshow(img_normal, cmap='gray')\n\nplt.subplot(1,3,2).set_title('PNEUMONIA')\nplt.imshow(img_penumonia_bacteria, cmap='gray')\n\nplt.subplot(1,3,3).set_title('PNEUMONIA')\nplt.imshow(img_penumonia_virus, cmap='gray')\n\nplt.tight_layout()","26a3873a":"def get_labeled_files(folder):\n    x = []\n    y = []\n    \n    for folderName in os.listdir(folder):\n        if not folderName.startswith('.'):\n            if folderName in ['NORMAL']:\n                label = 0\n            elif folderName in ['PNEUMONIA']:\n                label = 1\n            else:\n                label = 2\n                continue # we do not investigate other dirs\n            for image_filename in os.listdir(folder + folderName):\n                img_path = folder + folderName + '\/' + image_filename\n                if img_path is not None and str.endswith(img_path, 'jpeg'):\n                    x.append(img_path)\n                    y.append(label)\n    \n    x = np.asarray(x)\n    y = np.asarray(y)\n    return x, y","c80a6158":"x, y = get_labeled_files('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train\/')\n\nlist(zip(x, y))[:10]","75ced69a":"NUM_CLASSES = 2\n\n# This function takes image paths as arguments and reads corresponding images\ndef input_parser(img_path, label):\n    # convert the label to one-hot encoding\n    one_hot = tf.one_hot(label, NUM_CLASSES)\n    # read the img from file and decode it using tf\n    img_file = tf.read_file(img_path)\n    img_decoded = tf.image.decode_jpeg(img_file, channels=3, name=\"decoded_images\")\n    return img_decoded, one_hot\n\n# This function takes image and resizes it to smaller format (150x150)\ndef image_resize(images, labels):\n    # Be very careful with resizing images like this and make sure to read the doc!\n    # Otherwise, bad things can happen - https:\/\/hackernoon.com\/how-tensorflows-tf-image-resize-stole-60-days-of-my-life-aba5eb093f35\n    resized_image = tf.image.resize_images(images, (150, 150), align_corners=True)\n    resized_image_asint = tf.cast(resized_image, tf.int32)\n    return resized_image_asint, labels    ","26556d18":"# Execution plan is defined here.\n# Since it uses lazy evaluation, the images will not be read after calling build_pipeline_plan()\n# We need to use iterator defined here in tf context\ndef build_pipeline_plan(img_paths, labels, batch_size):\n\n    # We build a tensor of image paths and labels\n    tr_data = tf.data.Dataset.from_tensor_slices((img_paths, labels))\n    # First step of input pipeline - read images in paths as jpegs\n    tr_data_imgs = tr_data.map(input_parser)\n    # Apply resize to each image in the pipeline\n    tr_data_imgs = tr_data_imgs.map(image_resize)\n    # Gives us opportuinty to batch images into small groups\n    tr_dataset = tr_data_imgs.batch(batch_size)\n    # create TensorFlow Iterator object directly from input pipeline\n    iterator = tr_dataset.make_one_shot_iterator()\n    next_element = iterator.get_next()\n    return next_element\n\n# Function to execute defined pipeline in Tensorflow session\ndef process_pipeline(next_element):\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n        # get each element of the training dataset until the end is reached\n        # in our case only one iteration since we read everything as 1 batch\n        # can be multiple iterations if we decrease BATCH_SIZE to eg. 10\n        images = []\n        labels_hot = []\n        while True:\n            try:\n                elem = sess.run(next_element)\n                images = elem[0]\n                labels_hot = elem[1]\n            except tf.errors.OutOfRangeError:\n                print(\"Finished reading the dataset\")\n                return images, labels_hot","0b178c06":"def load_dataset(path, batch_size):\n    tf.reset_default_graph()\n    files, labels = get_labeled_files(path)\n    p = tf.constant(files, name=\"train_imgs\")\n    l = tf.constant(labels, name=\"train_labels\")\n    \n    next_element = build_pipeline_plan(p, l, batch_size=batch_size)\n    imgs, labels = process_pipeline(next_element)\n    return imgs, labels","45997a9f":"x_train, y_train = load_dataset(\"..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train\/\", 6000)\nx_test, y_test = load_dataset(\"..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/test\/\", 6000)\nx_val, y_val = load_dataset(\"..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/val\/\", 6000)","cf00fce3":"print(\"Training Dataset\")\nprint(x_train.shape)\nprint(y_train.shape)\nprint(\"\\nTesting Dataset\")\nprint(x_test.shape)\nprint(y_test.shape)\nprint(\"\\n Validation  Dataset\")\nprint(x_val.shape)\nprint(y_val.shape)","64804ef4":"import matplotlib.pyplot as plt\nimport seaborn as sns \n\nplt.subplot(1,3,1)\nsns.countplot(np.argmax(y_train, axis=1)).set_title('TRAIN')\n\nplt.subplot(1,3,2)\nsns.countplot(np.argmax(y_test, axis=1)).set_title('TEST')\n\nplt.subplot(1,3,3)\nsns.countplot(np.argmax(y_val, axis=1)).set_title('VALIDATION')\n\nplt.tight_layout()","c54274be":"print(x_train.shape)\n\nplt.figure(figsize=(5, 3))\n\ny_train_classes = np.argmax(y_train, axis = 1)\n\nplt.subplot(1,2,1).set_title('NORMAL')\nplt.imshow(x_train[np.argmax(y_train_classes == 0)])\n\nplt.subplot(1,2,2).set_title('PNEUMONIA')\nplt.imshow(x_train[np.argmax(y_train_classes == 1)])\n\nplt.tight_layout()","c5ae1b58":"import keras\nfrom keras import backend as K\nfrom keras.models import Model\nfrom keras.layers import Flatten, Dense, BatchNormalization, Dropout\nfrom keras.applications.vgg16 import VGG16\n#from keras.applications.vgg19 import VGG19\n#from keras.applications.densenet import DenseNet\nfrom keras.applications.inception_v3 import InceptionV3\n\nK.clear_session()\n\nNUM_CLASSES = 2\n\nbase_model = VGG16(weights='..\/input\/vgg16\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False, input_shape=(150, 150, 3))\n#base_model = VGG19(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n#base_model = DenseNet(blocks=10, weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n#base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n\nx = base_model.output\nx = Flatten()(x)\nx = Dense(NUM_CLASSES, activation='softmax')(x)\n\nmodel = Model(inputs=base_model.input, outputs=x)\n\nmodel.summary()","6daf69e9":"def print_layers(model):\n    for idx, layer in enumerate(model.layers):\n        print(\"layer {}: {}, trainable: {}\".format(idx, layer.name, layer.trainable))","c36ceb3c":"for layer in model.layers[0:18]:\n    layer.trainable = False\n    \nprint_layers(model)","874e9991":"model.trainable_weights","8ab49a42":"import numpy as np\nimport keras.backend as K\nfrom itertools import product\nfrom functools import partial","42ce87e2":"def w_categorical_crossentropy(y_true, y_pred, weights):\n    nb_cl = len(weights)\n    final_mask = K.zeros_like(y_pred[:, 0])\n    y_pred_max = K.max(y_pred, axis=1)\n    y_pred_max = K.reshape(y_pred_max, (K.shape(y_pred)[0], 1))\n    y_pred_max_mat = K.cast(K.equal(y_pred, y_pred_max), K.floatx())\n    for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n        final_mask += (weights[c_t, c_p] * y_pred_max_mat[:, c_p] * y_true[:, c_t])\n    return K.categorical_crossentropy(y_pred, y_true) * final_mask\n\n\nw_array = np.ones((2,2))\nw_array[1,0] = 30 # penalizing FN\nw_array[0,1] = 1 # penalizing FP\n\nspec_loss = lambda y_true, y_pred: w_categorical_crossentropy(y_true, y_pred, weights=w_array)","a32fd244":"# Create the loss function object using the wrapper function abov\noptimizer = keras.optimizers.Adam(lr=0.0001)\n\nmodel.compile(loss='categorical_crossentropy',     #  loss=spec_loss,\n              optimizer=optimizer, \n              metrics=['accuracy'])","5de72548":"from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau, EarlyStopping\n\n# This callback saves the wieights of the model after each epoch\ncheckpoint = ModelCheckpoint(\n    'model\/weights.epoch_{epoch:02d}.hdf5',\n    monitor='val_loss', \n    save_best_only=True, \n    save_weights_only=False,\n    mode='auto',\n    verbose=1\n)\n\n# This callback writes logs for TensorBoard\ntensorboard = TensorBoard(\n    log_dir='.\/Graph', \n    histogram_freq=0,  \n    write_graph=True\n)","efaffe14":"from sklearn.utils import class_weight\ny_labels = np.argmax(y_train, axis=1)\nclassweight = class_weight.compute_class_weight('balanced', np.unique(y_labels), y_labels)\nprint(classweight)","0e8d1b06":"# prepare a directory to store the model weights\nos.makedirs('.\/model', exist_ok=True)\n\nhistory = model.fit(\n    x=x_train, y=y_train,\n    class_weight=classweight,\n    validation_split=0.3,\n    callbacks=[tensorboard],\n    shuffle=True,\n    batch_size=256,\n    epochs=60,\n    verbose=1\n)","0bf04a3f":"def plot_learning_curves(history):\n    plt.figure(figsize=(12,4))\n    \n    plt.subplot(1,2,1)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    \n    plt.subplot(1,2,2)\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    \n    plt.tight_layout()\n    \nplot_learning_curves(history)","f113051a":"score = model.evaluate(x_test, y_test, verbose=0)\nprint('Model Loss: {}, Accuracy: {}'.format(score[0], score[1]))","6dfeaf24":"from sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\ny_pred = model.predict(x_test)\n# to get the prediction, we pick the class with with the highest probability\ny_pred_classes = np.argmax(y_pred, axis = 1) \ny_true = np.argmax(y_test, axis = 1) \n\n\nconf_mtx = confusion_matrix(y_true, y_pred_classes) \nplot_confusion_matrix(conf_mtx, figsize=(12,8), hide_ticks=True, cmap=plt.cm.Blues)\nplt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.show()","71b9c31f":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n\nfor i in range(NUM_CLASSES):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n                                 \nplt.figure(figsize=(7, 5))\n\nfor i in range(NUM_CLASSES):\n    plt.plot(fpr[i], tpr[i], lw=2,\n             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n    \nplt.plot(fpr[0], fpr[0], 'k-', label = 'random guessing')\n\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc=\"lower right\")\n\nplt.tight_layout()","dee1ed69":"Print the different sizes of the datasets. How images we have in each. ","13cfa65b":"**Excercise 3.1.** We would like to achive a better performance on the test dataset. Try tuning hyperparameters i.e. learning rate. Can you reach better accuracy?","d024ff6a":"## Looking at performance \n\nAS you have seen in the previous lab regarding Malaria there are a lot of alterntative of plotting and calculating the performance. In this case lets us ethe Confusion Matrix. ","1617a3bd":"\nFinal investigation. Lets see what we have before we start training.","9720ced4":"   So we have all the different models floating around the web. WE have all heard about the Alexnet, VGG, Resnet, Inception and so on. they were all designed to find cats and dogs and toothbrushes, but how many times do we look for that in real life ? \n\nIn real life one needs to download a known neural network and use transfer learning to apply it to his\/her problem. This generally means to cut of the last fully connected layers at the end and add others that are fit for the problem. \n\nBelow please how this transfer learning would like: \n\n<img src=\"images\/transfer.png\">\n\n","ef4eae18":"By locking the trainable layers we decrease ammount of trainable parameters to 16'384.","0b5cded5":"We would like to train only the most bottom classifier layers.","77f3d0ec":"Let's see what the data actually holds. We see three directories: the test, train and the validaton sets","ba86bc5c":"## Pneumonia\n\nSource : Wikipedia\n\nPneumonia is an inflammatory condition of the lung affecting primarily the small air sacs known as alveoli. Typically symptoms include some combination of productive or dry cough, chest pain, fever, and trouble breathing. Severity is variable.\n\nVaccines to prevent certain types of pneumonia are available.Other methods of prevention include handwashing and not smoking.Treatment depends on the underlying cause. Pneumonia believed to be due to bacteria is treated with antibiotics. If the pneumonia is severe, the affected person is generally hospitalized. Oxygen therapy may be used if oxygen levels are low.\n\nPneumonia affects approximately 450 million people globally (7% of the population) and results in about 4 million deaths per year.\n\nIn 2008, pneumonia occurred in approximately 156 million children (151 million in the developing world and 5 million in the developed world).In 2010, it resulted in 1.3 million deaths, or 18% of all deaths in those under five years, of which 95% occurred in the developing world.\n\n<img src=\"images\/deaths.png\">\n","c2fcfae3":"This network has over 14M trainable wegihts.","29b1c497":"Lets calculate what we have in each Dataset. ","d1a3d2d7":"Now lets see some examples of what we have just done. ","70d095f7":"The initial dataset was actually published here:  [Mendeley Data - Chest X-Ray Images](https:\/\/data.mendeley.com\/datasets\/rscbjbr9sj\/2) ","3a00edb9":"As the Dataset has been matched we need to read and decode the jpeg files. we are doing that with the function below. Once that is done we need to also resize the images to something that our VGG16 can actually understand. ","d95b33f5":"Load the dataset and return it into images and labels. ","8ef6f41b":"<img src=\"images\/vgg16.png\">","8f176ccf":"... and ROC curve","6c2edbe1":"We have to  build the dataset to ingest into our neural network. We need to sets. X and Y: \n* X should be the list of all images, each entry being a path to the a file\n* Y should be either 0 or 1. 0 if the image contains a Normal patient and 1 if the image contains a PNEUMONIA infested patient. ","9dbeee29":"Create the pipeline in TF, moving from images to tensors. ","8e8e2edf":"Lets check what we have done.","d0b8aec5":"Measure Loss and Accuracy on the test dataset","1a077aff":"The training set is un-balanaced. Let's calculate class weights","d610562a":"# Transfer Learning, from dogs and cats  to Pneumonia ","a7056373":"**Excercise 3.3.** Try training some of the conv layers. Does this help?","fb46943e":"**Excercise 3.2.** Try to use another base network i.e. InceptionV3, ResNet50, DenseNet.\n\nWhat do you observe? Is training time different? How many parameters does the function have?","40dd14bf":"Below let's look at the data and vizualise some images: either normal or pneumonia ","15b0465c":"## the Neural Network \n\nAs we have shown that we are working with the original data and we have worked it into something usable in Tensorflow now it is time to take it from the very beginning and start with downloading the VGG16 network. \n\nthe VGG16 network contains the weights trained for solving the ImageNet challange. \n\n","6b0f987f":"Next, we'll replace the orignal classification layer and build a new one:)\n\nIt will be composed of:\n\n- Flatten transformation that reshapes the MaxPool output (4 x 4 x 512) into (1 x 1 x 8192)\n- Fully Connected Dense layer with Softmax activation function with 2 outputs (1 x 1 x 2)\n\nSoftmax function normalizes input vector into a probability distribution that sums to 1.0","3479759c":"Here we use the \"cross-entropy\" loss function, which works well for learning probability distributions for classification. \n\nSee e.g.: https:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/loss_functions.html#cross-entropy","918dba11":"The data for the lab is downloaded from  Kaggle (https:\/\/www.kaggle.com\/paultimothymooney\/chest-xray-pneumonia) and it contains 5863 images taken from pediatric patients from 1 to 5 years old.  the images are labelled by specialists in two classes: Normal and Pneumonia. \n\n<img src=\"images\/dataset.png\">\n","bfda5c4d":"## The Lab\n\nLets train a neural network to recognize the Pneumonia from the normal cases. \n\nWe will start by loading the necessary libraries. ","599e1d91":"## The Data ","6388e902":"**Excercise 3.4.** can you think about a better metric than accuracy, which captures the fact that false negatives are much (much) worse than false positives?"}}