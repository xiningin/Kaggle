{"cell_type":{"ea1b0711":"code","4c9698c9":"code","815b2b61":"code","14960c69":"code","7e67645a":"code","652f1dc6":"code","26f4bb63":"code","17cae894":"code","7b7a0abe":"code","67766cd0":"code","bbf3baeb":"code","867b582f":"code","eb7876be":"code","8775e09c":"markdown","241efa27":"markdown","2b0a4334":"markdown","38a38f2a":"markdown","bb0c936a":"markdown","81a41e0f":"markdown","e3815a8c":"markdown"},"source":{"ea1b0711":"!pip install ..\/input\/datatable0110\/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl","4c9698c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","815b2b61":"import os\nimport datatable as dt\nimport janestreet\nimport xgboost as xgb\nimport lightgbm as lgbm\nfrom sklearn.metrics import make_scorer\n\nimport seaborn as sns\nfrom sklearn import ensemble \nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score,StratifiedKFold\nfrom sklearn import decomposition\nfrom sklearn import preprocessing \nfrom sklearn import pipeline\nfrom skopt import gp_minimize\nfrom functools import partial\nfrom skopt import space \nfrom hyperopt import hp,fmin,tpe,Trials\nfrom hyperopt.pyll.base import scope \nimport lightgbm as lgbm\n\nfrom catboost import CatBoostClassifier, Pool\nimport torch\n\n\n\nimport plotly.express as px\nimport plotly.graph_objects as go \nfrom sklearn.model_selection import StratifiedKFold , KFold, RepeatedKFold,GroupKFold , GridSearchCV , train_test_split ,TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\ncolor = sns.color_palette()\n\n\nimport multiprocessing as mp \n\n\nimport warnings \nwarnings.filterwarnings('ignore')","14960c69":"%%time\ntrain = dt.fread('\/kaggle\/input\/jane-street-market-prediction\/train.csv').to_pandas()","7e67645a":"train = train[1590491+400000:]","652f1dc6":"train['action'] = (train['resp'] > 0).astype('int')\nfeatures = [c for c in train.columns if 'feature' in c] + ['weight']\nX_Train = train.loc[:, features]\ny_train = train.loc[:, 'action']","26f4bb63":"params = dict(\n    objective='binary:logistic',\n    max_depth=8,\n    learning_rate=0.01,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    tree_method='gpu_hist')","17cae894":"dtrain = xgb.DMatrix(X_Train, y_train)\nxg_boost_clf = xgb.train(params, dtrain, num_boost_round=500)","7b7a0abe":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_Train, y_train, random_state=42, test_size=0.2)","67766cd0":"import shap \n\n# load JS visualization code to notebook\nshap.initjs()\n\nexplainer = shap.TreeExplainer(xg_boost_clf)\nshap_values = explainer.shap_values(X_test)\n\n#use matplotlib=True","bbf3baeb":"# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\nshap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:])","867b582f":"shap_values = explainer.shap_values(X_test[2:3])\nshap.force_plot(explainer.expected_value, shap_values, X_test[2:3])","eb7876be":"# sort the features indexes by their importance in the model\n# (sum of SHAP value magnitudes over the validation dataset)\n\n\nexplainer = shap.TreeExplainer(xg_boost_clf)\nshap_values = explainer.shap_values(X_test)\n\n\ntop_inds = np.argsort(-np.sum(np.abs(shap_values), 0))\n\n# make SHAP plots of the three most important features\nfor i in range(len(top_inds)):\n    shap.dependence_plot(top_inds[i], shap_values, X_test)","8775e09c":"### Consider last 800,000 records for ease of model building, While submitting prediction consider full dataset","241efa27":"<font color=\"red\" size=3>Please upvote this kernel if you like it. It motivates me to create kernal with great content  :) <\/font>","2b0a4334":"\n## <a name=\"Jane Street Market Prediction\">About this Competition<\/a>\n\nIn this competition, if one is able to generate a highly predictive model which selects the right trades to execute, they would also be playing an important role in sending the market signals that push prices closer to \u201cfair\u201d values. That is, a better model will mean the market will be more efficient going forward. However, developing good models will be challenging for many reasons, including a very low signal-to-noise ratio, potential redundancy, strong feature correlation, and difficulty of coming up with a proper mathematical formulation. This is a Code Competition and you need to submit notebooks for evaluation.\n","38a38f2a":"## <a name=\"dataset_description\"> Mission of this Kernal <\/a>: \n\nThis kernal dive into more details on model interpretability and help to understand the featues interaction with target variables , It greatly helps in feature engineering ","bb0c936a":"## <a name=\"dataset_description\">Dataset Description<\/a>: \n\n\nThis dataset contains an anonymized set of features, feature_{0...129}, representing real stock market data. Each row in the dataset represents a trading opportunity, for which you will be predicting an action value: 1 to make the trade and 0 to pass on it. Each trade has an associated weight and resp, which together represents a return on the trade. The date column is an integer which represents the day of the trade, while ts_id represents a time ordering. In addition to anonymized feature values, you are provided with metadata about the features in features.csv.\n\nIn the training set, train.csv, you are provided a resp value, as well as several other resp_{1,2,3,4} values that represent returns over different time horizons. These variables are not included in the test set. Trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation.\n\nThis is a code competition that relies on a time-series API to ensure models do not peek forward in time. To use the API, follow the instructions on the Evaluation page. When you submit your notebook, it will be rerun on an unseen test:\n\nDuring the model training phase of the competition, this unseen test set is comprised of approximately 1 million rows of historical data.\nDuring the live forecasting phase, the test set will use periodically updated live market data.\nNote that during the second (forecasting) phase of the competition, the notebook time limits will scale with the number of trades presented in the test set. Refer to the Code Requirements for details.\n\nFiles\n1. train.csv - the training set, contains historical data and returns\n2. example_test.csv - a mock test set which represents the structure of the unseen test set. You will not be directly using the test set or sample submission in this competition, as the time-series API will get\/set the test set and predictions.\n3. example_sample_submission.csv - a mock sample submission file in the correct format\n4. features.csv - metadata pertaining to the anonymized features\n\n   ","81a41e0f":"# Evaluation metrics \n\n\nThis competition is evaluated on a utility score. Each row in the test set represents a trading opportunity for which you will be predicting an action value, 1 to make the trade and 0 to pass on it. Each trade j has an associated weight and resp, which represents a return.\n\nFor each date i, we define: p_i = \\sumj(weight{ij} resp_{ij} action_{ij})\n\nt = \\frac{\\sum p_i }{\\sqrt{\\sum p_i^2}} * \\sqrt{\\frac{250}{|i|}}\n\nwhere |i| is the number of unique dates in the test set. The utility is then defined as: u = min(max(t,0), 6) \\sum p_i.\n","e3815a8c":"<font color=\"red\" size=3>Please upvote this kernel if you like it. It motivates me to create kernal with great content  :) <\/font>"}}