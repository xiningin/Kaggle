{"cell_type":{"8bd3b0e6":"code","44ea40ff":"code","ce78ebc1":"code","ed15f5a6":"code","cdf514be":"code","7d7b8953":"code","9b5caa0a":"code","1bc75eb0":"code","d947c94d":"code","d3b8d3ad":"code","78979f3d":"code","4f84f4cc":"code","82302e62":"code","ad55a03f":"markdown","dcbfb02d":"markdown","ed5b4278":"markdown","6b140d8c":"markdown","2cd482cd":"markdown","2ff3cddb":"markdown","48699362":"markdown","a6952f27":"markdown"},"source":{"8bd3b0e6":"#Let's load all the required libraries\n\n#General\nimport numpy as np\nimport pandas as pd\n\n\n#Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#Building the models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_score, recall_score, precision_recall_curve, f1_score, confusion_matrix, classification_report, roc_curve, auc\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.feature_selection import RFECV, f_classif, VarianceThreshold, SelectKBest, f_regression\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n#deep learning keras\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Dropout","44ea40ff":"df = pd.read_csv(\"\/kaggle\/input\/company-bankruptcy-prediction\/data.csv\")\ncolumns = df.columns\nprint(df.shape)\nprint(\"total null values\", df.isnull().sum().sum())\nprint(\"total potential duplicated rows\", df.duplicated().sum())\nprint(\"----------------------------------\")\nBankrupt, Bankrupt_perc = (df[\"Bankrupt?\"].value_counts(), round(df[\"Bankrupt?\"].value_counts(normalize=True),2))\ndisplay(Bankrupt, Bankrupt_perc)\nprint(\"----------------------------------\")\ndf.info()","ce78ebc1":"#Drop constant columns (if any)\nvar_thres = VarianceThreshold(threshold=0).fit(df)\nconstant_columns = [column for column in df.columns\n                    if column not in df.columns[var_thres.get_support()]]\nfor feature in constant_columns:\n     print(feature)\ndf.drop(constant_columns,axis=1, inplace=True)","ed15f5a6":"#Normalize data for faster processing\ndef data_scaling(DataFrame):\n    scaler = StandardScaler()\n    DataFrame.iloc[:,1:] = scaler.fit_transform(DataFrame.iloc[:,1:])\n    return(DataFrame)\ndf_ml = data_scaling(df)\n\n#Split dataframe for feature selection\nX = df_ml.drop(columns=[\"Bankrupt?\"])\ny = df_ml[\"Bankrupt?\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\ndf_train =pd.concat([y_train, X_train], axis=1)\n\n\n#Sort columns from the less correlated to the most correlated\ndf_train_corr = df_train.corr()\ndf_train_corr = df_train_corr.reindex(df_train_corr[\"Bankrupt?\"].abs().sort_values(ascending=True).index).T\ncolumn_names = np.array(df_train_corr.columns)\ndf_train= df_train.reindex(columns=column_names)\n\n#Isolate the input features which have a high correlation between themselves\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr\ncorr_features = correlation(X_train, 0.65)\ndisplay(len(corr_features))\n\n#Now let's go ahead and drop them:\nX_train.drop(corr_features, axis=1, inplace=True)\nX_test.drop(corr_features, axis=1, inplace=True)\ndf_train.drop(corr_features,axis=1, inplace=True)\ndf_train.head()","cdf514be":"#Feature selection for linear regression\n#Use rfecv to determine which features should remain in our model\nlgr = LogisticRegression(max_iter=1000)\nrfecv = RFECV(estimator=lgr, step=5, cv=5, scoring='f1')\nrfecv = rfecv.fit(X_train, y_train)\n\nselect_features_rfecv = pd.DataFrame({'Features': list(X_train.columns),\n                                  'Ranking_cv': rfecv.ranking_})\nselect_features_rfecv = select_features_rfecv.sort_values(by='Ranking_cv')\n\n#Let's also get their score on correlation\nselect_features = SelectKBest(score_func=f_classif, k=10).fit(X_train, y_train)\nselect_features_kbest = pd.DataFrame({'Features': list(X_train.columns),\n                                  'Scores': select_features.scores_})\nselect_features_kbest.sort_values(by='Scores', ascending = False)\n\n#Let's display the result\nselect_features_rfecv.merge(select_features_kbest, how='left', on='Features').head(20)","7d7b8953":"# Select the 9 most important features. We are very likely to remove the 3 features with scores lower than 1, but I am interested\n# in peeking at their basic information.\nfeatures = [\"Bankrupt?\"]\nfor i in range (9):\n    features.append(select_features_rfecv.iloc[i,0])\nfinal_df = df.loc[:, df.columns.isin(features)]\nround(final_df.describe(),2)","9b5caa0a":"#change name for clarity purpose:\nfinal_df = final_df.rename(columns={' ROA(C) before interest and depreciation before interest': ' ROA(C)'})\n\n#plotting boxplot to look for outliers:\nfig, saxis = plt.subplots(3, 3,figsize=(15,15))\nfor i in range(3):\n    sns.boxplot(x = \"Bankrupt?\", y=final_df.columns[i+1], data=final_df, ax=saxis[0,i])\n    saxis[0,i].set_title(f\"Bankrupt vs {final_df.columns[i+1]}\")\n\nfor i in range(3):\n    sns.boxplot(x = \"Bankrupt?\", y=final_df.columns[i+4], data=final_df, ax=saxis[1,i])\n    saxis[1,i].set_title(f\"Bankrupt vs {final_df.columns[i+4]}\")\n    \nfor i in range(3):\n    sns.boxplot(x = \"Bankrupt?\", y=final_df.columns[i+7], data=final_df, ax=saxis[2,i])\n    saxis[2,i].set_title(f\"Bankrupt vs {final_df.columns[i+7]}\")","1bc75eb0":"#In the end, the 3 features with low k scores only seem to have impact mostly for outliers.\nfinal_df.drop(' Accounts Receivable Turnover', axis=1, inplace=True)\nfinal_df.drop(' Total income\/Total expense', axis=1, inplace=True)\nfinal_df.drop(' Operating Profit Rate', axis=1, inplace=True)\nfinal_df.shape","d947c94d":"#Split the df:\nX = final_df.drop(columns=[\"Bankrupt?\"])\ny = final_df[\"Bankrupt?\"]\n\n#metrics\naccuracy_score = []\nf1_score = []\n\n#kfold\nkfold = StratifiedKFold(n_splits=5,shuffle=True)\n\ny_real = []\ny_proba = []\nplt.figure(figsize=(12,12))\n#Let's use a Logistic function to check the results:\nfor i, (train_fold_index, test_fold_index) in enumerate(kfold.split(X, y)):\n    #split the data\n    X_train_fold, X_test_fold = X.iloc[train_fold_index], X.iloc[test_fold_index]\n    y_train_fold, y_test_fold = y.iloc[train_fold_index], y.iloc[test_fold_index]\n    X_train_fold = StandardScaler().fit_transform(X_train_fold)\n    #upsample the data - deleted in favor of class weights:\n    #X_train_fold_upsample, y_train_fold_upsample = ros.fit_resample(X_train_fold, y_train_fold)\n    \n    #fit the model\n    model2 = LogisticRegression(solver='lbfgs',max_iter=1000, class_weight = {0:1 , 1:6}).fit(X_train_fold, y_train_fold)\n    y_pred_fold = model2.predict(X_test_fold)\n    decision = model2.decision_function(X_test_fold)\n    \n    #Score the model:\n    score1 = round(np.mean(cross_val_score(model2, X_test_fold, y_test_fold, scoring='accuracy', cv=kfold, n_jobs=1)),2)\n    score2 = round(np.mean(cross_val_score(model2, X_test_fold, y_test_fold, scoring='f1', cv=kfold, n_jobs=1)),2)\n    accuracy_score.append(score1)\n    f1_score.append(score2)\n    \n    #plot PR-curve\n    precision, recall, thresholds = precision_recall_curve(y_test_fold, decision)\n    area = auc(recall, precision)\n    lab = 'Fold %d AUC=%.4f' % (i+1, auc(recall, precision))\n    plt.step(recall, precision, label=lab)\n    y_real.append(y_test_fold)\n    y_proba.append(decision)\n\n#plotting the average PR curve\ny_real = np.concatenate(y_real)\ny_proba = np.concatenate(y_proba)\nprecision, recall, _ = precision_recall_curve(y_real, y_proba)\nlab = 'Overall AUC=%.4f' % (auc(recall, precision))\nplt.step(recall, precision, label=lab, lw=2, color='black')\n\nplt.title('Precision Recall Curve')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend(loc='upper right', fontsize='small')\nprint('f1_score:', np.mean(f1_score))","d3b8d3ad":"#Updated this part with new feature selection based on tree models (I was curious to see if it would make a difference, considering tree models did not achieve a high f1 previously)\n#https:\/\/bmcgenomdata.biomedcentral.com\/articles\/10.1186\/s12863-018-0633-8\n#Researches suggest that RF do not scale well for high dimension data which are highly correlated, so we kept the first step.\n\n#Let's now do the same with trees:\n#Use rfecv to determine which features should remain in our model\nrf = RandomForestClassifier(max_depth=10, random_state=0, n_estimators = 100).fit(X_train, y_train)\nrfecv = RFECV(estimator=rf, step=5, cv=5, scoring='f1')\nrfecv = rfecv.fit(X_train, y_train)\n\nselect_features_rfecv = pd.DataFrame({'Features': list(X_train.columns),\n                                  'Ranking_cv': rfecv.ranking_})\nselect_features_rfecv = select_features_rfecv.sort_values(by='Ranking_cv')\n\n#Let's also get their score on correlation\nselect_features = SelectKBest(score_func=f_classif, k=10).fit(X_train, y_train)\nselect_features_kbest = pd.DataFrame({'Features': list(X_train.columns),\n                                  'Scores': select_features.scores_})\nselect_features_kbest.sort_values(by='Scores', ascending = False)\n\n#Let's display the result\nselect_features_rfecv.merge(select_features_kbest, how='left', on='Features').head(40)\n","78979f3d":"features = [\"Bankrupt?\"]\nfor i in range (19):\n    features.append(select_features_rfecv.iloc[i,0])\nfinal_df = df.loc[:, df.columns.isin(features)]\n#Split the df:\nX = final_df.drop(columns=[\"Bankrupt?\"])\ny = final_df[\"Bankrupt?\"]\n\n#gridsearchcv\nparam_dict = {'max_leaf_nodes': [3,10,20,30,40], 'min_samples_split': [2, 3, 4,8], 'max_depth': [5,8,11,14,17,20]}\ngrid = GridSearchCV(DecisionTreeClassifier(class_weight = {0:1 , 1:6}), param_grid=param_dict, cv=5,verbose=1,n_jobs=-1, scoring='f1')\ngrid.fit(X, y)\nmodel_grid = grid.best_estimator_\n\n\nkfold = StratifiedKFold(n_splits=5,shuffle=True)\nlist_tree_score ={\"accuracy\":[],\n                 \"f1_score\": []}\n\n\nfor train_fold_index, test_fold_index in kfold.split(X, y):\n    #split the data\n    X_train_fold, X_test_fold = X.iloc[train_fold_index], X.iloc[test_fold_index]\n    y_train_fold, y_test_fold = y.iloc[train_fold_index], y.iloc[test_fold_index]\n    X_train_fold = StandardScaler().fit_transform(X_train_fold)\n    \n    #upsample the data:\n    #X_train_fold_upsample, y_train_fold_upsample = ros.fit_resample(X_train_fold, y_train_fold)\n    \n    #fit the model\n    model = model_grid.fit(X_train_fold, y_train_fold)\n    \n    #Score the model:\n    score1 = round(np.mean(cross_val_score(model, X_test_fold, y_test_fold, scoring='accuracy', cv=kfold, n_jobs=1)),2)\n    score2 = round(np.mean(cross_val_score(model, X_test_fold, y_test_fold, scoring='f1', cv=kfold, n_jobs=1)),2)\n    list_tree_score[\"accuracy\"].append(score1)\n    list_tree_score[\"f1_score\"].append(score2)\n \naccuracy = np.mean(list_tree_score[\"accuracy\"])\nf1_score = np.mean(list_tree_score[\"f1_score\"])\n\nprint(f\"the accuracy of the model is {accuracy}\")\nprint(f\"the f1_score of the model is {f1_score}\")\n","4f84f4cc":"#We will now build a simple Neural Network using Keras\ndef classification_model():\n    #initiating the model\n    model = Sequential()\n    model.add(Dense(input_dim=X_train.shape[1], units=6, activation=\"relu\"))\n    \n    #for loop to add layers (can be fine tuned): \n    for i in range(2):\n        model.add(Dense(units=6, activation=\"relu\"))\n        #If we add more layers, we would want to avoid overfitting with a Dropout layer model.add(Dropout(.1))\n    \n    #last layer (2 categories)\n    model.add(Dense(units=2, activation='sigmoid'))\n    \n    #compile the model\n    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    return model","82302e62":"#set class weights to offset the class imbalance:\nweights = {0:1 , 1:4}\nlist_scores_keras =[]\nconfusion_matrix_average1 = np.zeros((2, 2))\n\n#Let's create a loop to check our average accuracy:\n\nfor i in range(10):\n    \n    #Let's split the data into training and testing data\n    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,train_size=0.75, random_state=0+i)\n    \n    #Now let's one hot encode outputs\n    y_train = to_categorical(y_train)\n    y_test = to_categorical(y_test)\n\n    #We will convert all the DataFrame to arrays:\n    X_train = StandardScaler().fit_transform(X_train)\n    X_test = X_test.values\n\n    \n    #Initiate the model:\n    model = classification_model()\n    #fitting the model:\n    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=15, verbose=0,class_weight=weights)\n\n    #confusion matrix:\n    model_pred = model.predict(X_test)\n    matrix = confusion_matrix(y_test.argmax(axis=1), model_pred.argmax(axis=1))\n    confusion_matrix_average1 = confusion_matrix_average1 + matrix\n    print(f\"loop {i} classification report:\", classification_report(y_test.argmax(axis=1), model_pred.argmax(axis=1)))\n    \n    #evaluate the model:\n    tn, fp, fn, tp = matrix.ravel()\n    f1_score = tp\/ (tp+0.5*(fp+fn))\n    list_scores_keras.append(f1_score)\n    \n#Let's plot the result to visualize how robust our model is:\nfig = plt.figure()\nax = plt.axes()\nax.plot(list_scores_keras, scaley=False)\nax.set_title(f\"The average f1_score of the model is {round(np.mean(list_scores_keras),3)}\")\nplt.xlabel('loop number')\nplt.ylabel(\"weighted f1_score\")\n\nprint('--------------------------------------------------------')\ndisplay(confusion_matrix_average1)\nprint('--------------------------------------------------------')\n#print(\"last epoch classification report:\", classification_report(y_test.argmax(axis=1), model_pred.argmax(axis=1)))","ad55a03f":"# Predicting company bankruptcy \n-- Creating as much value as possible within a time limit of 1 day.\n\n## Context\nThe data was collected from the Taiwan Economic Journal from 1999 to 2009. Company bankruptcy was defined based on the business regulations of the Taiwan Stock Exchange.\n\n**In the article published by Deron Lianga, Chia-Chi Lu, Chih-Fong Tsaic and Guan-An Shiha (Financial ratios and corporate governance indicators in bankruptcy prediction: A comprehensive study, 2016), I managed to find some additional information regarding how this dataset was collected.**\n\n1. \"The sample companies had to have at least three years of complete public information before the occurrence of the financial crisis.\" However, there was no information on the year the data were taken from (year of the bankruptcy (Y), Y-1, Y-2, or Y-3).\n\n2. The resultant sample includes: \n    * companies from the manufacturing industry composed of industrial and electronics companies (346 companies), \\n- the service industry composed of shipping, tourism, and retail companies (39 companies)\n    * and others (93 companies), but no financial companies.\n\n\n\n## Project Overview\n\n**Goals:** As a former Restructuring consultant, I thought it would be interesting:\n* to see whether we can accurately predict which company will face bankruptcy in the future with Machine Learning. \n* to leverage on this model to try to create some additional value such as developping a credit scoring tool (not performed in this notebook)\n\n**However, I will perform those tasks in depth through a future dataset that I will scrape myself on European or North American companies. Important information are missing in this dataset (such as the company's industry). Furthermore, I would like to scrape data from up to 1 year prior to the actual bankruptcy.**\n\n**Constraint:** Due to the dataset limitations, I decided to spend only 1 day working on this project.\nMy goals are:\n* To get used to working and publishing on Kaggle (as it is the first project that I will share on it),\n* To see how much value I could produce in a limited timeframe,\n* To increase the number of dataset and challenges I will be able to work with.\n\n\n**edit 16\/06:**\n\nI actually created a repository to deploy this model on github thanks to flask\/heroku: https:\/\/github.com\/AymericPeltier\/Bankruptcy_Prediction\n* Currently waiting from support on how to fix a dependancy issue with numpy (mlk not supported yet)\n* Furthermore, I updated 2\/3 part of the codes to improve the f1_score, especially on the Linear_Regression (which is the model I deployed)\n=> Please refer to version 7 for what I achieved only in one day.\n\n**edit 19\/06:**\n** Added classification report \/ confusion matrix for the NN\n\n# Index:\n\n- [Describe the data](#p2)\n- [Data pre-processing](#p3)\n- [Weak learners: Decision Tree and Logistic Regression](#p4)\n- [Shallow Neural Network (SNN) with Keras](#p5)\n- [Conclusion](#p6)","dcbfb02d":" [Back to top](#Index:) \n<a id='p3'><\/a>\n### Data Pre-processing & Features Selection\n\nIn this section, we will prepare our data for consumption.\n* We will normalize our data\n\n* We will select a reduced amount of features (to avoid overfitting and make the models simple to implement if needed)\n\n**Selection process:**\n* First, we want to drop the features which have the lowest correlation to bankruptcy and with a correlation >0.7 to other features *(i.e: ROA(B) and ROA(A) have 0.99 correlation to ROA(C), and ROA(C) has the highest correlation to \"Bankrupt?\" => drop ROA(A) and ROA(B))*. -- 34 features were dropped doing so.\n\n* Then, we use feature ranking with recursive feature elimination and cross-validated selection of the best number of features. As my goal is to make something easy to understand and simple to use, I'll restrain the features to those with the highest importance. -- at this stage, 9 features were selected.\n\n* Finally, after looking at their k score (SelectKbest) and their boxplot (data distribution), we will decide whether to use any more parameters. -- 6 features were kept.\n \n\n**At the end of our steps, we have 6 features remaining:**\n* ROA(C), \n* Net Value Per Share (B), \n* Debt ratio %, \n* Total Asset Turnover, \n* Cash\/Total Assets, \n* Equity to Liability\n\nI decided to drop 'Accounts Receivable Turnover', 'Total income\/Total expense', and 'Operating Profit Rate' as those parameters might lead to overfitting. If we obtained more datas, it would be interesting to check whether those parameters are actually useful or not. \n\n**Without going too much in depth due to the time constraint, they seem to efficiently capture the main aspects that lead a company to bankrupty:**\n* **ROA & Total Asset Turnover** provide information regarding how effective the investing strategy (into asset) is. It also gives an indice on profitability (as one is calculated based on net sales, and the other on net income)\n* **Net value and debt ratio** provide interesting relations between total assets and total debt (= financial risk)\n* **Cash\/Total Assets** may indicates a degree of surety if not low (and too high)\n* **Equity to Liability** gives a picture of the leverage being used\n\nOverall, I think that we are missing very important information in this dataset:\n* the company industry: Financial ratios can strongly vary from an industry to another.\n* time series ratio. Most of this exercise value would be to predict risk of going bankrupt as soon as possible.\n\n**While I think we could actually achieve an even better selection taking more time and using domain knowledge, this set of parameter seems quite decent**\n\n**I won't be eliminating any outlier values for multiple reasons:**\n* the minority class sample is already quite small, I am unwilling to reduce it even further\n\n* We have no information regarding the companie's industry, financial ratio can vary strongly from one industry to another. Those outliers may actually be normal.\n\n* Bankrupt companies could present some highly skewed financial ratios.\n\nHowever, with more time for considerations and data exploration, I might do a different choice.\n\n**I did not perform any feature engineering as all the financial ratios are already quite precise and encompass many if not all the main financial aspect of a company.**","ed5b4278":" [Back to top](#Index:) \n<a id='p6'><\/a>\n### Conclusion\n\nBased on what we have done previously, we can conclude that the 6 features selected are quite representative of the risk of going bankrupt.\n* ROA(C), \n* Net Value Per Share (B), \n* Debt ratio %, \n* Total Asset Turnover, \n* Cash\/Total Assets, \n* Equity to Liability\n\n**Overall, the Logistic regression and SNN performed decently. Considering that some of the features distribution are not linear (they have optimum for some and have bad impact on both tails), it would be interesting to check how well some more advanced tree models perform. \n\n","6b140d8c":" [Back to top](#Index:) \n<a id='p2'><\/a>\n### Describe the Data\n\n**Data type and quality:**\n- There are 96 columns (95 input features + 1 output feature) in the dataset, and 6819 rows (=companies)\n- 93 output features are numerical & 2 are categorical (one of which has only one value)\n- There is no missing data, no null values, no duplicated rows \n\n**Out of the 6819 companies in the dataset:**\n- 6599 (97%) did not go bankrupt\n- 220 (3%) went bankrupt\n\n\n**First impressions:**\n* The dataset is really clean, there is no need for extensive data cleaning.\n\n* Looking at the column names and taking in account that those are financial ratios, I can already guess that many features are highly correlated.\n\n* We can see a strong class imbalance. Using accuracy as our evaluation metric (total accurate predictions) is not relevant as any model could achieve 97% accuracy by just predicting that every companies will not go bankrupt. I would also argue that predicting that a company will go bankrupt has more importance than correctly predicting than a company will not, because actions must be taken in the former case. (F1 score seems more relevant)\n","2cd482cd":" [Back to top](#Index:) \n<a id='p4'><\/a>\n\n### Weak learners: Decision Tree and Logistic Regression\n\n**Disclaimer: this section is more of a training ground as I did not implement the most efficient models (considering what we know so far), I am convinced that XGBoost or a Random Forest would achieve more satisfactory results. Please skip to the Shallow Neural Network if you are more interested in performance ! **\n\nFirst, let's see if what we can achieve using some simple models.\n\n#### Logistic Regression:\n\n**Steps taken:**\n* We will once again use a stratified approach to limit the variance of our results\n* We will plot the Precision-Recall curve to have a better evaluation of how our model perform in regards to predicting whether a company will go bankrupt or not.\n\n**Results:**\n* The f1 score is around 0.4 with class weights, which is 0.2 higher than when using a RandomOverSampler (previous version)\n","2ff3cddb":"**Looking at the boxplots, it appears that the 3 previous features with the lowest k score are mainly impactful due to outliers and do not provide a decisive information. Thus, I will drop them.**","48699362":"#### Decistion Tree classifiers:\n\n**Steps taken:**\n* I corrected previous version by addind a quick feature selection step using a random forest (instead of Logistic regression, which was a mistake),\n* Added a gridcv to see if we could improve the f1 score\n* Also removed the class imbalance used from previous version (as explained before, it was merely to practice).\n\n**Results:**\n* The f1 score is stable when the depth is over 5,\n* The average f1 score is around 0.3.\n","a6952f27":" [Back to top](#Index:) \n<a id='p5'><\/a>\n### Shallow Neural Network (SNN) with Keras\n\nNow that we have noticed that simple models do not perform well in predicting the positive class (=companies going bankrupt), let's use Shallow Neural Networks.\n\n**Model considerations:**\n* A Shallow Neural Network will be sufficient to deal with this problem\n* We will train the model 10 times with different split (as the data is small) to reduce the variance of the result\n* **We assign the following weight to the accuracy evaluation metric: 1 for correctly predicting non-bankruptcy and 4 for accurately predicting bankruptcy.** The reason behind is that False negatives are less problematic than False positives\n\n**Results:**\n* The f1 score of the SNN using our 6 selected features varies between 0.35 and 0.45 depending on the loops due to the small numbers of positive classes. \n"}}