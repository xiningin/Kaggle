{"cell_type":{"97993b36":"code","b8533465":"code","b2ae89fe":"code","f6f420a3":"code","7e28613e":"code","b9b97926":"code","c9555fb1":"code","520192f2":"code","8ebd182f":"code","a4defbd4":"code","8e366405":"code","760956b0":"code","512ec6d4":"code","e8e37168":"code","65f8e2ff":"code","f1b13651":"code","7a8630cd":"code","07e0711a":"code","6b20b03b":"code","b9a1a821":"code","8ad9d557":"code","97ce39fb":"code","8907c49a":"code","4aa355d1":"code","a0810fba":"code","317f1ffe":"code","a5d980ca":"code","1bc40901":"code","29ef18c0":"code","2925f13a":"code","31e46a0d":"code","540f1464":"code","8a22d89e":"code","1dc6f6b0":"code","07d724f9":"code","e37bb24a":"code","cf387be3":"code","82b4f595":"code","2b5bcda2":"code","c83b7ed7":"code","2c7c3a0d":"code","ddd903b6":"code","6c37f258":"code","760ecd63":"code","e6d9db47":"markdown","030eec41":"markdown","7d958081":"markdown","2cb88364":"markdown","a16610f0":"markdown","943f732e":"markdown","0d066b33":"markdown","236c2ef1":"markdown","9ecfb2dd":"markdown","10165879":"markdown","2eb6be32":"markdown","f5883522":"markdown","8e90ee0b":"markdown","720822c2":"markdown","c4562331":"markdown","da3a8194":"markdown","2e5c7a4a":"markdown","744d8bac":"markdown","a462b880":"markdown","333d2966":"markdown","74ffb9e1":"markdown","ee959058":"markdown"},"source":{"97993b36":"%cd \/kaggle\/\n%ls","b8533465":"!mkdir training\n%cd training","b2ae89fe":"# Download YOLOv5\n!git clone https:\/\/github.com\/ultralytics\/yolov5  # clone repo\n%cd yolov5\n# Install dependencies\n%pip install -qr requirements.txt  # install dependencies\n\n%cd ..\/\n\n%pip install --upgrade torch \n%pip install --upgrade torchvision\n\nimport torch\nimport torchvision\n\nprint(f\"Setup complete. Using torch {torch.__version__}, torchvision {torchvision.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")","f6f420a3":"# # Start tensorboard\n# # Launch after you have started training to all the graphs needed for inspection\n\n# since there are some probelem when using tensorbaord in kaggle, you can have a try with yourself with local machine.\n# There is another ways to lunach the tensorboard feature, \n# just uncomment the tensorboard code after line 289 in the file `\\yolov5\\models\\yolo.py`, just check the code for detail","7e28613e":"# # logs save in the folder \"yolov5\/runs\"  (option)\n%load_ext tensorboard\n%tensorboard --logdir \/kaggle\/training\/yolov5\/runs","b9b97926":"# Install W&B \n%pip install -q --upgrade wandb\n# Login with token, follow with the guide\nimport wandb\nwandb.login()","c9555fb1":"# Import lib\nimport os\nimport gc\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport json\nimport yaml\nfrom shutil import copyfile\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","520192f2":"%cd \/kaggle\/training\/\n%ls ","8ebd182f":"# Firstly, We will transfer the coco format to yolo format\n\n#load json file\njson_file_path = '\/kaggle\/input\/cowboyoutfits\/train.json'\n\ndata = json.load(open(json_file_path, 'r'))\nyolo_anno_path = '\/kaggle\/training\/yolo_anno\/'\n\nif not os.path.exists(yolo_anno_path):\n    os.makedirs(yolo_anno_path)","a4defbd4":"# \u9700\u8981\u6ce8\u610f\u4e0b\u56e0\u4e3a\u6211\u4eec\u7684annotation lable\u662f\u4e0d\u8fde\u7eed\u7684,\u4f1a\u5bfc\u81f4\u540e\u9762\u62a5\u9519,\u6240\u4ee5\u8fd9\u91cc\u751f\u6210\u4e00\u4e2amap\u6620\u5c04\ncate_id_map = {}\nnum = 0\nfor cate in data['categories']:\n    cate_id_map[cate['id']] = num\n    num+=1","8e366405":"cate_id_map","760956b0":"#\u5bf9\u6bd4\u4e0b\ndata['categories']","512ec6d4":"# convert the bounding box from COCO to YOLO format.\n\ndef cc2yolo_bbox(img_width, img_height, bbox):\n    dw = 1. \/ img_width\n    dh = 1. \/ img_height\n    x = bbox[0] + bbox[2] \/ 2.0\n    y = bbox[1] + bbox[3] \/ 2.0\n    w = bbox[2]\n    h = bbox[3]\n \n    x = x * dw\n    w = w * dw\n    y = y * dh\n    h = h * dh\n    return (x, y, w, h)","e8e37168":"# transfer the annotation, and generated a train dataframe file\nf = open('train.csv','w')\nf.write('id,file_name\\n')\nfor i in tqdm(range(len(data['images']))):\n    filename = data['images'][i]['file_name']\n    img_width = data['images'][i]['width']\n    img_height = data['images'][i]['height']\n    img_id = data['images'][i]['id']\n    yolo_txt_name = filename.split('.')[0] + '.txt' #remove .jpg\n    \n    f.write('{},{}\\n'.format(img_id, filename))\n    yolo_txt_file = open(os.path.join(yolo_anno_path, yolo_txt_name), 'w')\n    \n    for anno in data['annotations']:\n        if anno['image_id'] == img_id:\n            yolo_bbox = cc2yolo_bbox(img_width, img_height, anno['bbox']) # \"bbox\": [x,y,width,height]        \n            yolo_txt_file.write('{} {} {} {} {}\\n'.format(cate_id_map[anno['category_id']], yolo_bbox[0], yolo_bbox[1], yolo_bbox[2], yolo_bbox[3]))\n    yolo_txt_file.close()\nf.close()","65f8e2ff":"# generate training dataframe\ntrain = pd.read_csv('\/kaggle\/training\/train.csv')\ntrain.head()","f1b13651":"train_df, valid_df = train_test_split(train, test_size=0.10, random_state=233)\n\nprint(f'Size of total training images: {len(train)}, training images: {len(train_df)}. validation images: {len(valid_df)}')\n\n# \u8bf4\u660e\u4e0b\uff0c\u8fd9\u91cc\u7ed9\u7684validation set \u5c310.01\uff0c\u662f\u56e0\u4e3a\u8fd9\u6b21\u7684\u6bd4\u8d5b\u6311\u6218\u4e4b\u4e00\u5c31\u662f\u6570\u636e\u96c6\u5f88\u5c0f\uff0c\u8fd8\u662f\u5e0c\u671b\u80fd\u591f\u7ed9\u66f4\u591a\u7684\u6570\u636e\u6765\u8bad\u7ec3\u3002\n# \u5176\u6b21\uff0c\u8001\u5e08\u5df2\u7ecf\u5212\u5206\u597d\u4e86validation\u96c6\u4e86\uff0c\u8fd9\u91cc\u76840.01\u7684\u9a8c\u8bc1\u66f4\u591a\u662f\u6d4b\u8bd5\u81ea\u5df1\u7b97\u6cd5\u662f\u5426\u6709\u9519\u8bef\uff0c\u771f\u6b63\u770bperformance\u8fd8\u662f\u63d0\u4ea4\u4e0a\u53bb\u770b\u597d\u3002\n# 100\u6b21\u7684\u63d0\u4ea4\u4e5f\u8db3\u591f\u9a8c\u8bc1performance\u4e86.","7a8630cd":"# generate new train data frame with spliter mark\n\ntrain_df.loc[:, 'split'] = 'train'\nvalid_df.loc[:, 'split'] = 'valid'\ndf = pd.concat([train_df, valid_df]).reset_index(drop=True)\ndf.sample(10)","07e0711a":"%cd \/kaggle\/training\/\n%ls","6b20b03b":"# mdke directory for traning section\nos.makedirs('\/kaggle\/training\/cowboy\/images\/train', exist_ok=True)\nos.makedirs('\/kaggle\/training\/cowboy\/images\/valid', exist_ok=True)\n\nos.makedirs('\/kaggle\/training\/cowboy\/labels\/train', exist_ok=True)\nos.makedirs('\/kaggle\/training\/cowboy\/labels\/valid', exist_ok=True)\n\n%ls","b9a1a821":"# move the images and annotations to relevant splited folders\n\nfor i in tqdm(range(len(df))):\n    row = df.loc[i]\n    name = row.file_name.split('.')[0]\n    if row.split == 'train':\n        copyfile(f'\/kaggle\/input\/cowboyoutfits\/images\/{name}.jpg', f'\/kaggle\/training\/cowboy\/images\/train\/{name}.jpg')\n        copyfile(f'\/kaggle\/training\/yolo_anno\/{name}.txt', f'\/kaggle\/training\/cowboy\/labels\/train\/{name}.txt')\n    else:\n        copyfile(f'\/kaggle\/input\/cowboyoutfits\/images\/{name}.jpg', f'\/kaggle\/training\/cowboy\/images\/valid\/{name}.jpg')\n        copyfile(f'\/kaggle\/training\/yolo_anno\/{name}.txt', f'\/kaggle\/training\/cowboy\/labels\/valid\/{name}.txt')","8ad9d557":"# Create  yaml file\n\ndata_yaml = dict(\n    train = '..\/cowboy\/images\/train\/',\n    val = '..\/cowboy\/images\/valid',\n    nc = 5,\n    names = ['belt', 'sunglasses', 'boot', 'cowboy_hat', 'jacket']\n)\n\n# we will make the file under the yolov5\/data\/ directory.\nwith open('\/kaggle\/training\/yolov5\/data\/data.yaml', 'w') as outfile:\n    yaml.dump(data_yaml, outfile, default_flow_style=True)\n    \n%cat \/kaggle\/training\/yolov5\/data\/data.yaml # show your YAML file","97ce39fb":"# Hyperparameters for COCO training from scratch\n# python train.py --batch 40 --cfg yolov5m.yaml --weights '' --data coco.yaml --img 640 --epochs 300\n# See tutorials for hyperparameter evolution https:\/\/github.com\/ultralytics\/yolov5#tutorials\n\n\nlr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\nlrf: 0.2  # final OneCycleLR learning rate (lr0 * lrf)\nmomentum: 0.937  # SGD momentum\/Adam beta1\nweight_decay: 0.0005  # optimizer weight decay 5e-4\nwarmup_epochs: 3.0  # warmup epochs (fractions ok)\nwarmup_momentum: 0.8  # warmup initial momentum\nwarmup_bias_lr: 0.1  # warmup initial bias lr\nbox: 0.05  # box loss gain\ncls: 0.5  # cls loss gain\ncls_pw: 1.0  # cls BCELoss positive_weight\nobj: 1.0  # obj loss gain (scale with pixels)\nobj_pw: 1.0  # obj BCELoss positive_weight\niou_t: 0.20  # IoU training threshold\nanchor_t: 4.0  # anchor-multiple threshold\n# anchors: 3  # anchors per output layer (0 to ignore)\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\nhsv_h: 0.015  # image HSV-Hue augmentation (fraction)\nhsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\nhsv_v: 0.4  # image HSV-Value augmentation (fraction)\ndegrees: 0.0  # image rotation (+\/- deg)\ntranslate: 0.1  # image translation (+\/- fraction)\nscale: 0.5  # image scale (+\/- gain)\nshear: 0.0  # image shear (+\/- deg)\nperspective: 0.0  # image perspective (+\/- fraction), range 0-0.001\nflipud: 0.0  # image flip up-down (probability)\nfliplr: 0.5  # image flip left-right (probability)\nmosaic: 1.0  # image mosaic (probability)\nmixup: 0.0  # image mixup (probability)\ncopy_paste: 0.0  # segment copy-paste (probability)\n","8907c49a":"#IMG_SIZE = 640  # the default image size in yolo is 640, it will automated resize our image during training and valudation.\nBATCH_SIZE = 32 # wisely choose, use the largest size that can feed up all your gpu ram\nEPOCHS = 5\nMODEL = 'yolov5m.pt'  # 5s, 5m 5l\nname = f'{MODEL}_BS_{BATCH_SIZE}_EP_{EPOCHS}'","4aa355d1":"# we are ready to training our model with w&b\n%cd \/kaggle\/training\/yolov5\/","a0810fba":"!python train.py --batch {BATCH_SIZE} \\\n                 --epochs {EPOCHS} \\\n                 --data data.yaml \\\n                 --weights {MODEL} \\\n                 --save_period 1 \\\n                 --project \/kaggle\/working\/kaggle-cwoboy \\\n                 --name {name} \\\n                 --cache-images","317f1ffe":"# Training result will be showed online with W&B\n\n# Here we can also zip our training result for local visualization\n\n!zip -r \/kaggle\/working\/output.zip \/kaggle\/working\/kaggle-cwoboy\n","a5d980ca":"# In the Dev phase, we will only use the valid data for predicition. \n# Don't forget change it to test data in the Final pahse.\n\nvalid_df = pd.read_csv('\/kaggle\/input\/cowboyoutfits\/valid.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/cowboyoutfits\/test.csv')\nvalid_df.head()","1bc40901":"valid_df.shape","29ef18c0":"%cd \/kaggle\/training\/\n%ls","2925f13a":"# make directory to store the validation data.\nos.makedirs('\/kaggle\/inference\/valid', exist_ok=True)\nos.makedirs('\/kaggle\/inference\/test', exist_ok=True)","31e46a0d":"# copy the validation image to inference folder for detection process\nfor i in tqdm(range(len(valid_df))):\n    row = valid_df.loc[i]\n    name = row.file_name.split('.')[0]\n    copyfile(f'\/kaggle\/input\/cowboyoutfits\/images\/{name}.jpg', f'\/kaggle\/inference\/valid\/{name}.jpg')","540f1464":"VALID_PATH = '\/kaggle\/inference\/valid\/'\nMODEL_PATH = '\/kaggle\/input\/cowboy-object-detection-models\/v0_ep20_best.pt'\nIMAGE_PATH = '\/kaggle\/input\/cowboyoutfits\/images\/'","8a22d89e":"# go to yolov5 main folder for detection\n%cd \/kaggle\/training\/yolov5\/","1dc6f6b0":"!python detect.py --weights {MODEL_PATH} \\\n                  --source {VALID_PATH} \\\n                  --conf 0.546 \\\n                  --iou-thres 0.5 \\\n                  --save-txt \\\n                  --save-conf \\\n                  --augment","07d724f9":"# read the output log , indicated our prediction result was saved under `runs\/detect\/exp\/`\n\nPRED_PATH = '\/kaggle\/training\/yolov5\/runs\/detect\/exp\/labels\/'","e37bb24a":"with open('\/kaggle\/training\/yolov5\/runs\/detect\/exp\/labels\/010fb53ff39a0ea1.txt', 'r') as file:\n    for line in file:\n        print(line)","cf387be3":"from PIL import Image\nImage.open('\/kaggle\/training\/yolov5\/runs\/detect\/exp\/010fb53ff39a0ea1.jpg')","82b4f595":"# list our prediction files path\nprediction_files = os.listdir(PRED_PATH)\nprint('Number of test images with detections: ', len(prediction_files))","2b5bcda2":"# convert yolo to coco annotation format\ndef yolo2cc_bbox(img_width, img_height, bbox):\n    x = (bbox[0] - bbox[2] * 0.5) * img_width\n    y = (bbox[1] - bbox[3] * 0.5) * img_height\n    w = bbox[2] * img_width\n    h = bbox[3] * img_height\n    \n    return (x, y, w, h)","c83b7ed7":"# reverse the categories numer to the origin id\nre_cate_id_map = dict(zip(cate_id_map.values(), cate_id_map.keys()))\n\nprint(re_cate_id_map)","2c7c3a0d":"def make_submission(df, PRED_PATH, IMAGE_PATH):\n    output = []\n    for i in tqdm(range(len(df))):\n        row = df.loc[i]\n        image_id = row['id']\n        file_name = row['file_name'].split('.')[0]\n        if f'{file_name}.txt' in prediction_files:\n            img = Image.open(f'{IMAGE_PATH}\/{file_name}.jpg')\n            width, height = img.size\n            with open(f'{PRED_PATH}\/{file_name}.txt', 'r') as file:\n                for line in file:\n                    preds = line.strip('\\n').split(' ')\n                    preds = list(map(float, preds)) #conver string to float\n                    cc_bbox = yolo2cc_bbox(width, height, preds[1:-1])\n                    result = {\n                        'image_id': image_id,\n                        'category_id': re_cate_id_map[preds[0]],\n                        'bbox': cc_bbox,\n                        'score': preds[-1]\n                    }\n\n                    output.append(result)\n    return output","ddd903b6":"sub_data = make_submission(valid_df, PRED_PATH, IMAGE_PATH)","6c37f258":"op_pd = pd.DataFrame(sub_data)\n\nop_pd.sample(10)","760ecd63":"import zipfile \n\nop_pd.to_json('\/kaggle\/working\/answer.json',orient='records')\nzf = zipfile.ZipFile('\/kaggle\/working\/sample_answer.zip', 'w')\nzf.write('\/kaggle\/working\/answer.json', 'answer.json')\nzf.close()","e6d9db47":"If you don't have a wb account, you should create a new account","030eec41":"# Inference Hyperparameter\nYOLOv5 also provided a lot of hyperparameters for inference process, we can check the detect.py file for detail\n\nHere I list some parameters:\n* --weights {MODEL_PATH} \\ # path to the best model.\n* --source {TEST_PATH} \\ # absolute path to the test images.\n* --img {IMG_SIZE} \\ # Size of image\n* --conf 0.25 \\ # Confidence threshold (default is 0.25)\n* --iou-thres 0.45 \\ # IOU threshold (default is 0.45)\n* --max-det 100 \\ # Number of detections per image (default is 1000) \n* --save-txt \\ # Save predicted bounding box coordinates as txt files\n* --save-conf # Save the confidence of prediction for each bounding box\n* --augment # augmented inference, TTA\n* --project 'runs\/detect'  # save results to project\/name\n* --name 'exp'  # save results to project\/name\n* --half False  # use FP16 half-precision inference","7d958081":"If you are new to yolo like me, we can use the default values, and choose some higher level parameters for our traning.\n\n* --img {IMG_SIZE} \\ # Input image size.\n* --batch {BATCH_SIZE} \\ # Batch size\n* --epochs {EPOCHS} \\ # Number of epochs\n* --data data.yaml \\ # Configuration file\n* --weights yolov5s.pt \\ # Model name\n* --save_period 1\\ # Save model after interval\n* --project kaggle-cow-boy # W&B project name\n* --name exp # experiment name\n\nYolo provided a lot of differen [pretrained models](https:\/\/github.com\/ultralytics\/yolov5#pretrained-checkpoints). Here we will use  the smallest one for study purpose.\n\n* YOLOv5s\n* YOLOv5m\n* YOLOv5l\n* YOLOv5x\netc\u2026","2cb88364":"# Splitting data into training and validation","a16610f0":"# Cowboy Outfits Detection: \ud83e\udd20 \ud83d\udc62 \ud83d\udd76\ufe0f \ud83e\udde5 \ud83e\udd4b\n   \n   In this kernel, we will introduce: \n\n* how to generate the specific required folder structure for yolov5\n* Transfer COCO to YOLOv5 annotations and reverse it\n* Do your project with Weight and Biases\n* Inference section\n* tuning hyperparameter (TBD)\n* model selection (TBD)\n* bounding box augmentation (TBD)\n* cross-validation (TBD)","943f732e":"# Create .yaml file\n\nThe `data.yaml` file is the dataset configuration file that contains information about the datast like path of images, annotaions. \nWe should specific the following items:\n*     the path of our training and validation data\n*     the number of classes to be detected\n*     the names corresponding to those classes\n\n> \u203c\ufe0f in this competition, the categories_id provided in the train.json is not a sequential number, we need re_map it.or we will get an error here.\n\n> !! we can put our `YAML` file anywhere, since we can reference the path later, but can also simply put it in the `data` folder under `yolov5`","0d066b33":"# Prepare required data folder structure \n\nThe Yolov5 requires a specific directory structure for custom training, you can get more information from the [official example](https:\/\/github.com\/ultralytics\/yolov5\/wiki\/Train-Custom-Data)\n\n```\n\/training    --temp_traning space\n    \/dataset --dataset fold, split into training and validation\n         \/images\n         \/labels\n    \/yolov5  --yolov5 main\n```","236c2ef1":"there are tons of information in the json file,  you should check it by your self. \n\n>Example code like: \n`data['info']`\n`data['image']`\n`data['annotations']`\n`data['categories']`\n\nIt can help us get better understand about the json structure. Some features might help like `data['annotations'][i][iscrowd]`. \nIf possible we can do another EDA kernel for this match :D","9ecfb2dd":"Result are all in the [project page in W&B here](https:\/\/wandb.ai\/momo233\/kaggle-cwoboy). Here is an eaxmple screenshot from my result, there are a lot of usefull information in W&B\n\n<img src=\"https:\/\/api.wandb.ai\/files\/momo233\/kaggle-cwoboy\/2rgyap5g\/media\/images\/Results_20_0.png\" alt=\"drawing\" width=\"800\"\/>\n","10165879":"\u5177\u4f53\u8fd0\u884c\u7684\u7ed3\u679c\u53ef\u4ee5\u53bb[W&B\u7684\u9879\u76ee\u9875\u9762](https:\/\/wandb.ai\/momo233\/kaggle-cwoboy)\u67e5\u770b\u3002\u56e0\u4e3akaggle save\u8001\u662f\u51fa\u9519\uff0c\u6240\u4ee5\u8fd9\u6b21\u5c31\u6ca1\u6709train, \u53ea\u505a\u4e86inference. \u53ef\u4ee5\u770bversion 3, \u6709\u7b2c\u4e00\u6b21\u7684train \u7684log.\n\u867d\u7136\u540e\u9762\u6d4b\u8bd5\u4e86\u4e0d\u540c\u6a21\u578b\uff0c\u4f46\u662f\u8fd9\u91cc\u6211\u90fd\u662f\u4f7f\u7528\u4e86\u7b2c\u4e00\u6b21train\u7684model\uff0c\u7136\u540e\u66f4\u6539\u4e86\u4e0d\u540c\u7684conf \u548c iou threat \u53c2\u6570\uff0c\u6765\u67e5\u770b\u76f8\u540c\u6a21\u578b\u7684performace\u3002 \u4ee5\u540e\u518d\u66f4\u65b0\u4e0d\u540c\u6a21\u578b\u597d\u4e86\u3002\n\n| Model | train_setting | inference_setting| result|\n| :-----| :---- | :---- | :---- |\n| Yolov5s | epoch[20] bs[16] | conf[0.546]   iou[0.5] TTA[false]| [50.1540154015] |\n| Yolov5s | epoch[20] bs[16] | conf[0.546]   iou[0.5] TTA[true]| [57.9936350778] |\n| Yolov5s | epoch[20] bs[16] | conf[0.5]     iou[0.5] TTA[true]| [58.7389096052] |\n\n\n\u56e0\u4e3a\u4e5f\u662f\u7b2c\u4e00\u6b21\u7528yolo\uff0c\u8fd8\u6709\u5f88\u591a\u5730\u65b9\u4e0d\u61c2\u3002\u76ee\u524d\u51e0\u4e4e\u90fd\u662f\u521d\u59cb\u8bbe\u7f6e\u8dd1\u901a\u4e00\u4e0b\u9879\u76ee\u800c\u5df2\u3002 \n\n\u6b22\u8fce\u8bc4\u8bba\u7559\u8a00\uff0c\u548c\u63d0\u51fa\u5efa\u8bae\ud83d\ude03","2eb6be32":"# Training Hyperparameters","f5883522":"## visualize our prediction","8e90ee0b":"We will use the default settings for inference. Here, we will simply introduced one example to pick the right confidence score which is based on our F1 score which is showed below. It can be access from our W&B result page. You can get access it from [here](https:\/\/wandb.ai\/momo233\/kaggle-cwoboy\/runs\/2rgyap5g?workspace=user-momo233). From the F1 score, we just pick the confidence with 0.546. it seems a little bit high, but can cover all the classes.\n\n<img src=\"https:\/\/api.wandb.ai\/files\/momo233\/kaggle-cwoboy\/2rgyap5g\/media\/images\/Results_20_2.png\" alt=\"drawing\" width=\"600\"\/>","720822c2":"# GO TRAIN","c4562331":"# GO Detection","da3a8194":"# Manage your experiment with W&B\n\nWeights and Biases (W&B) is a power tool which can help us do the machine learning experiment tracking, dataset versioning, and model evaluation. If you had already worked with tensorboard before, then the W&B should be easy for you.\n[check the official documentation for more information](https:\/\/docs.wandb.ai\/). [\u4e2d\u6587\u6587\u6863](https:\/\/docs.wandb.ai\/v\/zh-hans\/)\n\nThe W&B had arealdy integrated into the latest `yolov5` We can easily train our model with it.\n\nSince it is an online tool, if you can not get access to interent or you have some privacy concerns, yolov5 can also works with tensorboard. ","2e5c7a4a":"# Transfer annotations bbox from COCO to YOLO format","744d8bac":"#H Hyperparameter evolution\n","a462b880":"# Make Submission","333d2966":"# Inference Section\n\nOnce we finished the training section, we should refer to [W&B Artifacts tab](https:\/\/wandb.ai\/momo233\/kaggle-cwoboy\/artifacts\/model\/run_2rgyap5g_model\/4141ab8a754564a37bb1) to choose our target model for inference.\n\n>Download the `best` performance model and upload to kaggle.","74ffb9e1":"During training you can open your [project webpage](https:\/\/wandb.ai\/momo233\/kaggle-cwoboy) to visualize your training process.","ee959058":"There are about 25 hyperparameters in tranining setting.You can get access it from `yolov5\/data\/hyps\/hyp.scratch.yaml`. Here is useful [discussion](https:\/\/github.com\/ultralytics\/yolov5\/issues\/607) about the hyperparameters in yolov5. \nHere is the example content:"}}