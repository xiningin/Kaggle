{"cell_type":{"c1432021":"code","e4627c4a":"code","b5af7575":"code","ce83dfd2":"code","4a6ea4bb":"code","012ed27c":"code","cb9624aa":"code","429ec709":"code","4a8de82d":"code","df415325":"code","395846f2":"code","ffc3d448":"code","d8702f55":"code","569ec279":"code","6b3458ad":"code","dd182114":"code","79880e3f":"code","2b543ee1":"markdown","581b87e4":"markdown","5815da56":"markdown","7a405975":"markdown","8ddfb672":"markdown","a6364f9f":"markdown","3e34698d":"markdown","547d3d2a":"markdown"},"source":{"c1432021":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\n\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers.experimental import preprocessing\n","e4627c4a":"flowers_path = \"..\/input\/flowers-recognition\/flowers\"\n\nclasses = os.listdir(flowers_path)\nclasses","b5af7575":"image_size = (128, 128)\nbatch_size = 32\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    flowers_path,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=1337,\n    image_size=image_size,\n    batch_size=batch_size,\n    label_mode=\"categorical\",\n    class_names=classes\n)\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    flowers_path,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=1337,\n    image_size=image_size,\n    batch_size=batch_size,\n    label_mode=\"categorical\",\n    class_names=classes\n)","ce83dfd2":"#list(train_ds.as_numpy_iterator())[0][0][0][0]","4a6ea4bb":"plt.figure(figsize=(20, 10))\nfor images, labels in train_ds.take(1):\n    for i in range(15):\n        ax = plt.subplot(3, 5, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(classes[np.argmax(labels[i])])\n        plt.axis(\"off\")\nplt.show()","012ed27c":"#Original model\n# tamano de la imagen de entrada\ninput_shape = (128, 128, 3)\n\n# definicion del modelo Perceptron\nmodel = keras.Sequential(\n    [\n        keras.Input(shape=input_shape),\n        layers.Flatten(),\n        layers.Dense(200, activation=\"relu\"),\n        \n        # Cree mas capas Dense aqui: \n        # layers.Dense(XX, activation=\"AA\"),\n        \n        layers.Dense(len(classes), activation=\"softmax\"),\n    ]\n)\n\n# Construir el modelo y ver la arquitectura\nmodel.build(input_shape)\nmodel.summary()","cb9624aa":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","429ec709":"history = model.fit(train_ds, epochs=20, verbose=1, validation_data=val_ds)","4a8de82d":"initializer = tf.keras.initializers.truncated_normal(mean=0, stddev=1)\n\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=0.1,\n    decay_steps=1000,\n    decay_rate=0.01)\n\ntf.keras.optimizers.Adamax(learning_rate=lr_schedule)\n\n# Model definition\nmodel2 = keras.Sequential(\n    [\n        \n        keras.Input(shape=input_shape),\n        preprocessing.RandomFlip(\"horizontal\"),\n        #preprocessing.RandomRotation(0.25),\n        #preprocessing.RandomZoom(-0.1),\n        layers.Flatten(),\n        tf.keras.layers.BatchNormalization(),\n        layers.Dense(2000, kernel_initializer=initializer, activation=\"relu\"),\n        tf.keras.layers.BatchNormalization(),\n        layers.Dense(1000, kernel_initializer=initializer, activation=\"relu\"),\n        tf.keras.layers.BatchNormalization(),\n        #tf.keras.layers.GaussianNoise(1),\n        tf.keras.layers.Dropout(0.1),\n        layers.Dense(250, kernel_initializer=initializer, activation=\"relu\"),\n        \n        layers.Dense(len(classes), activation=\"softmax\"),\n    ]\n)\n\n# Construir el modelo y ver la arquitectura\nmodel2.build(input_shape)\nmodel2.summary()","df415325":"model2.compile(loss='categorical_crossentropy', optimizer='Adamax', metrics=['accuracy'])","395846f2":"history2 = model2.fit(train_ds, epochs=20, verbose=1, validation_data=val_ds)","ffc3d448":"xepochs = [n for n in range(len(history.history['accuracy']))]\n\nfig = make_subplots(rows=1, cols=2, subplot_titles=(\"Accuracy over time\", \"Loss over time\"))\n\nfor metric in ['accuracy', 'val_accuracy']:\n    fig.add_trace(go.Scatter(x=xepochs, y=history.history[metric], mode='lines+markers', name=metric), row=1, col=1)\n\nfor metric in ['loss', 'val_loss']:\n    fig.add_trace(go.Scatter(x=xepochs, y=history.history[metric], mode='lines+markers', name=metric), row=1, col=2)\n\n\nfig.update_layout(title_text=\"Original Model\")\n\nfig.update_xaxes(title_text=\"Epoch\", row=1, col=1)\nfig.update_xaxes(title_text=\"Epoch\", row=1, col=2)\n\nfig.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\nfig.update_yaxes(title_text=\"Loss\", row=1, col=2)\n\nfig.show()","d8702f55":"xepochs = [n for n in range(len(history2.history['accuracy']))]\n\nfig = make_subplots(rows=1, cols=2, subplot_titles=(\"Accuracy over time\", \"Loss over time\", \"Accuracy original vs Tuned Model - Train data\", \"Accuracy original vs Tuned Model - Test data\"))\n\nfor metric in ['accuracy', 'val_accuracy']:\n    fig.add_trace(go.Scatter(x=xepochs, y=history2.history[metric], mode='lines+markers', name=metric), row=1, col=1)\n\nfor metric in ['loss', 'val_loss']:\n    fig.add_trace(go.Scatter(x=xepochs, y=history2.history[metric], mode='lines+markers', name=metric), row=1, col=2)\n\nfig.update_layout(title_text=\"Tuned Model\")\n    \nfig.update_xaxes(title_text=\"Epoch\", row=1, col=1)\nfig.update_xaxes(title_text=\"Epoch\", row=1, col=2)\n\nfig.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\nfig.update_yaxes(title_text=\"Loss\", row=1, col=2)\n\nfig.show()","569ec279":"predictions = model2.predict(val_ds)\nlabels = tf.concat([y for [_, y] in val_ds], axis=0).numpy()","6b3458ad":"cm = confusion_matrix(np.argmax(labels, axis=1), np.argmax(predictions, axis=1))\n\nfig = px.imshow(\n    cm,\n    x=classes,\n    y=classes\n)\n\nfig.show()","dd182114":"#Classification Report\nprint(classification_report(\n    np.argmax(labels, axis=1), \n    np.argmax(predictions, axis=1),\n    target_names=classes\n))","79880e3f":"plt.figure(figsize=(20, 10))\nfor images, labels in val_ds.take(1):\n    for i in range(15):\n        ax = plt.subplot(3, 5, i + 1)\n        \n        img_array = images[i].numpy().astype(\"uint8\")\n        prediction = model2.predict(np.array([img_array]))\n        prediction_name = classes[np.argmax(prediction)]\n        real_name = classes[np.argmax(labels[i])]\n        \n        plt.imshow(img_array)\n        if prediction_name == real_name:\n            plt.title(f'real: {real_name}\\npred:{prediction_name}', fontdict={'color': 'g'})\n        else:\n            plt.title(f'real: {real_name}\\npred:{prediction_name}', fontdict={'color': 'r'})\n        \n        plt.axis(\"off\")","2b543ee1":"# Imports","581b87e4":"# Predictions","5815da56":"# Data Import","7a405975":"# History Plots","8ddfb672":"# Neural Network","a6364f9f":"# Conclusions\n\nThe accuracy is better than the original model, however the new model has overfitting. The following techniques were used in order to reduce it, but it was not enough to get accuracy of test data close to training data.\n\n1. [Modify learning rate](https:\/\/keras.io\/api\/callbacks\/learning_rate_scheduler\/)\n2. Add preprocessing layers, such as: [random rotation](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/RandomRotation), [random zoom](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/RandomZoom), [random flip](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/RandomFlip), [batch normalization](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/BatchNormalization), [Gaussian noise](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/GaussianNoise) and [dropout](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Dropout).\n3. Changed [initializer](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/initializers\/TruncatedNormal)\n4. Reduce complexity of the model (reduce nodes and layers).\n\nWithout handling the overfitting it was possible to get accuracy greather than 0.9 for training data, but results were even worse for test data (<0.4).\n\nAdding too many layers to reduce overfitting, drasticaly reduces the accuracy for training data ( didn't get values over 0.5).","3e34698d":"# Data Analysis","547d3d2a":"# Flower Classification with Neural Network\n\nC\u00e9sar Jim\u00e9nez"}}