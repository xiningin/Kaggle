{"cell_type":{"5176bd66":"code","cb5b803f":"code","39e0082b":"code","7f1aa16f":"code","7f72ffa0":"code","47d19c54":"code","b3de9901":"code","f0868650":"code","e6281be7":"code","99f58b04":"code","f37afe89":"code","2fe684a4":"code","84fcc1b2":"code","99861a1b":"code","6054d726":"code","c66b1898":"code","09559fb2":"code","cac971c8":"code","35a6d87b":"code","8e07bc04":"code","16d38f43":"code","f9fb4bb8":"code","9eff3628":"code","1b224689":"code","8ac15cc9":"code","4c932490":"code","7d45e976":"code","c23021c5":"code","ad5eab09":"code","021a508a":"code","19eebd51":"markdown","392613e4":"markdown","5cafaaaa":"markdown","3bf2de90":"markdown","4786f1e8":"markdown","ef87b4a5":"markdown","0ec15216":"markdown","727cd9a2":"markdown","166aba0b":"markdown"},"source":{"5176bd66":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","cb5b803f":"import seaborn as sns               \nimport matplotlib.pyplot as plt     \nfrom scipy.stats import *                \nfrom sklearn.decomposition import PCA   \nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.metrics import classification_report\n","39e0082b":"#reading the dataset\ndata = pd.read_csv(\"..\/input\/bda-2019-ml-test\/Train_Mask.csv\")   ","7f1aa16f":"#previewing the first 5 rows of the dataframe\ndata.head()","7f72ffa0":"#obtaining the dimensions of the dataframe\ndata.shape","47d19c54":"#column names of the dataframe\ndata.columns","b3de9901":"#data type of each column\ndata.dtypes","f0868650":"#checking for missing values \ndata.isna().sum()","e6281be7":"#checking for class imbalance\nnew_data['flag'].value_counts()\/new_data.shape[0]","99f58b04":"#descriptive statistics for each column\ndata.describe()","f37afe89":"#segregating the data into independent and dependent variables\nX = data.drop(['flag','timeindex'],1)\ny = data['flag']","2fe684a4":"#plotting KDE plots to observe the distribution of each column for each category of 'flag'\nfor i, col in enumerate(X.columns):\n    plt.figure(i)\n    dat1=data[col][data['flag']==0]\n    dat2 = data.currentBack[data['flag']==1]\n    plt.ylabel('Probability')\n    plt.xlabel(col)\n    plt.title('KDE Plot for column '+ col)\n    sns.kdeplot(dat1, label='Anomaly')\n    sns.kdeplot(dat2, label='Normal')\n    plt.show()","84fcc1b2":"#to compute the Kruskal-Wallis test statistic to detect the relationship between y and each continuous variable\nfor i, col in enumerate(X.columns):\n    dat1=data[col][data['flag']==0]\n    dat2 = data.currentBack[data['flag']==1]\n    print(kruskal(dat1,dat2))","99861a1b":"#correlation matrix\ncor = round(X.corr(),2)\ncor","6054d726":"#heatmap depicting the correlation between independent variables\nsns.heatmap(cor, cmap=\"Blues\", linewidths=0.3)","c66b1898":"#computing principal components to reduce multicollinearity\npca = PCA(n_components=5)     #5 principal components used\nX=pca.fit_transform(X)","09559fb2":"#calculating the variance explained by each principal component\nvar = pca.explained_variance_ratio_   \nvar","cac971c8":"#calculating the cumulative variance explained by the principal components\ncum_var = np.cumsum(pca.explained_variance_ratio_)*100\ncum_var","35a6d87b":"#dataframe with the principal components\npca_X=pd.DataFrame(X)","8e07bc04":"#correlation matrix for the principal components, to confirm the removal of multicollinearity\ncor_pca = round(pca_X.corr(),2)\ncor_pca","16d38f43":"#splitting the dataset into training and test data; test size = 30%\nX_train, X_test, y_train, y_test = train_test_split(pca_X,y, test_size=0.3)","f9fb4bb8":"#building a SVM model on the training data\nsvc_model = svm.SVC()\nsvc_model.fit(X_train,y_train)","9eff3628":"#predicting the class for the test data \ny_pred = svc_model.predict(X_test)","1b224689":"#to get the F1 score for each class\nprint(classification_report(y_test, y_pred))","8ac15cc9":"#reading the sample data\nsample = pd.read_csv('..\/input\/bda-2019-ml-test\/Test_Mask_Dataset.csv')\nsample = sample.drop('timeindex',1)\nsample.shape","4c932490":"#computing the principal components for the sample data\nsample=pca.transform(sample)","7d45e976":"#predicting the class for the records in the sample data\ny = svc_model.predict(sample)\ny.shape","c23021c5":"#reading the Sample Submission csv file\noutput = pd.read_csv(\"..\/input\/bda-2019-ml-test\/Sample Submission.csv\")","ad5eab09":"#updating the values in the 'flag column'\noutput['flag']=y","021a508a":"#writing the new csv file\noutput.to_csv('Sample Submission.csv', index=False)","19eebd51":"From the Kruskal Walllis test conducted and KDE plots plotted, it can be seen that all the features are significant in determining the class.","392613e4":"Approach taken:\n* After the data was read, it was checked for missing values and class imbalance. \n* KDE plots were drawn for each column and Kruskal Wallis Test was conducted to determine the relevant features in determining the 'flag' class.\n* On detecting high multicollinearity between the features, PCA was used. 5 Principal COmponents were used. \n* A SVM model, was built and the F1 score was calculated for said model","5cafaaaa":"# Model Building","3bf2de90":"# Data","4786f1e8":"# Libraries ","ef87b4a5":"Principal Components","0ec15216":"# Sample predictions","727cd9a2":"Feature selection","166aba0b":"Multicollinearity check"}}