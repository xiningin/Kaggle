{"cell_type":{"c88566c8":"code","87f188e4":"code","f0ee3f28":"code","ddec456a":"code","afef5a6b":"code","63789164":"code","76b3960d":"code","466ac20d":"code","909eecfc":"code","cf374d54":"code","d1eeeed3":"code","4ccc0483":"code","ad43d791":"code","7dfdb693":"code","d37d6154":"code","56fb82dd":"markdown","fbb79936":"markdown","2c2b8bdb":"markdown","67ea4484":"markdown","d500a4b8":"markdown","8642a111":"markdown","eddceb80":"markdown","2548209a":"markdown","eccc87cc":"markdown"},"source":{"c88566c8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport seaborn as sns\nimport sklearn\nimport imblearn\nimport matplotlib.pyplot as plt\nimport time\nimport sklearn.metrics as m\nimport xgboost as xgb\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Settings\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n#Probably can`t be finished because of huge amount of data with kaggle hardware, add nrows parameter to run here\n#Load Data\n\ncols = [' Bwd Packet Length Std',' PSH Flag Count',' min_seg_size_forward',' Min Packet Length',' ACK Flag Count',' Bwd Packet Length Min',' Fwd IAT Std','Init_Win_bytes_forward',' Flow IAT Max',' Bwd Packets\/s',' URG Flag Count','Bwd IAT Total',' Label']\ndf1=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\", usecols = cols)#,nrows = 50000\ndf2=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\", usecols = cols)\ndf3=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Friday-WorkingHours-Morning.pcap_ISCX.csv\", usecols = cols)\ndf5=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\", usecols = cols)\ndf6=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\", usecols = cols)\n\n# df4, df7 and df8 are being left out as they only have the benign samples","87f188e4":"df = pd.concat([df1,df2])\ndel df1,df2\ndf = pd.concat([df,df3])\ndel df3\ndf = pd.concat([df,df5])\ndel df5\ndf = pd.concat([df,df6])\ndel df6\n\ndata = df.copy()\n\nfor column in data.columns:\n    if data[column].dtype == np.int64:\n        maxVal = data[column].max()\n        if maxVal < 120:\n            data[column] = data[column].astype(np.int8)\n        elif maxVal < 32767:\n            data[column] = data[column].astype(np.int16)\n        else:\n            data[column] = data[column].astype(np.int32)\n            \n    if data[column].dtype == np.float64:\n        maxVal = data[column].max()\n        minVal = data[data[column]>0][column]\n        if maxVal < 120 and minVal>0.01 :\n            data[column] = data[column].astype(np.float16)\n        else:\n            data[column] = data[column].astype(np.float32)\n            \n            \n\nattackType = data[' Label'].unique()\ndata[' Label'] = data[' Label'].astype('category')\ndata[' Label'] = data[' Label'].astype(\"category\").cat.codes","f0ee3f28":"y = data[' Label'].copy()\nX = data.drop([' Label'],axis=1)","ddec456a":"from imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler('majority')\nX_rus, y_rus = rus.fit_sample(X, y)","afef5a6b":"y_rus.value_counts()","63789164":"df = X_rus\ndf[' Label'] = y_rus\nminor = pd.DataFrame(df[(df[' Label']!=4) & (df[' Label']!=2)])\nmajor = pd.DataFrame(df[(df[' Label']==4) | (df[' Label']==2)])\nminor[' Label'].value_counts()","76b3960d":"from imblearn.over_sampling import SMOTE\ny_rus_ =  minor[' Label']\nX_rus_ =  minor.drop([' Label'],axis=1)\nstrategy = {1:2000, 5:1600, 7:800, 3:300, 6:200, 0:200}\nsm = SMOTE(sampling_strategy=strategy)\nX_sm, y_sm = sm.fit_sample(X_rus_, y_rus_)\nX_min,y_min = X_sm, y_sm ","466ac20d":"major[' Label'].value_counts()","909eecfc":"from imblearn.under_sampling import RandomUnderSampler\ny_rus_ =  major[' Label']\nX_rus_ =  major.drop([' Label'],axis=1)\nstrategy = {4:10000, 2:6000}\ntom = RandomUnderSampler(sampling_strategy=strategy)\nX_tom, y_tom = tom.fit_sample(X_rus_, y_rus_)\ny_tom.value_counts()","cf374d54":"X_maj,y_maj = X_tom, y_tom\nX,y = pd.concat([X_maj,X_min]), pd.concat([y_maj,y_min])\nX.info()","d1eeeed3":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# extract numerical attributes and scale it to have zero mean and unit variance  \ncols = X.select_dtypes(include=['float32','float16','int32','int16','int8']).columns\ntrain_X = scaler.fit_transform(X.select_dtypes(include=['float32','float16','int32','int16','int8']))\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(train_X,y,train_size=0.70, random_state=2)\n\n\nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n# Train Random Forest\nRFC_Classifier = RandomForestClassifier(max_depth=40)\nRFC_Classifier.fit(X_train, Y_train)\nprint ('RF Classifier run')\n\n# Train SVC\nSVM_Classifier = SVC()\nSVM_Classifier.fit(X_train, Y_train)\nprint ('SV Classifier run')\n# Train Decision Tree Model\nDTC_Classifier = tree.DecisionTreeClassifier(criterion='gini', max_depth=33, random_state=20, max_features=12, splitter='random')\nDTC_Classifier.fit(X_train, Y_train)\nprint ('DTC Classifier run')","4ccc0483":"from sklearn import metrics\n\nmodels = []\nmodels.append(('Random Forest Classifier', RFC_Classifier))\nmodels.append(('Decision Tree Classifier', DTC_Classifier))\nmodels.append(('Support Vector Classifier',SVM_Classifier))\n\n\nfor i, v in models:\n    Xpred =  v.predict(X_train)\n    scores = cross_val_score(v, X_train, Y_train, cv=10)\n    accuracy = metrics.accuracy_score(Y_train, Xpred)\n    confusion_matrix = metrics.confusion_matrix(Y_train, Xpred)\n    classification = metrics.classification_report(Y_train, Xpred)\n    print()\n    print('============================== {} Model Evaluation =============================='.format(i))\n    print()\n    print (\"Cross Validation Mean Score:\" \"\\n\", scores.mean())\n    print()\n    print (\"Model Accuracy:\" \"\\n\", accuracy)\n    print()\n    print(\"Confusion matrix:\" \"\\n\", confusion_matrix)\n    print()\n    print(\"Classification report:\" \"\\n\", classification) \n    print()","ad43d791":"for i, v in models:\n    pred = v.predict(X_test)\n    accuracy = metrics.accuracy_score(Y_test,pred)\n    confusion_matrix = metrics.confusion_matrix(Y_test, pred)\n    classification = metrics.classification_report(Y_test, pred)\n    print()\n    print('============================== {} Model Test Results =============================='.format(i))\n    print()\n    print (\"Model Accuracy:\" \"\\n\", accuracy)\n    print()\n    print(\"Confusion matrix:\" \"\\n\", confusion_matrix)\n    print()\n    print(\"Classification report:\" \"\\n\", classification) \n    print()    ","7dfdb693":"from sklearn.ensemble import VotingClassifier\n\nclf1 = tree.DecisionTreeClassifier(criterion='gini', max_depth=33, random_state=20, max_features=12, splitter='random')\nclf2 = RandomForestClassifier(criterion='gini', max_depth=40, random_state=20)\nclf3 = SVC()\n\nvotingC = VotingClassifier(estimators=[('dc',clf1), ('rf', clf2),('svc',clf3)],voting='hard', weights=[2,2,1],flatten_transform=True)\nvotingC.fit(X_train,Y_train)","d37d6154":"pred = votingC.predict(X_test)\naccuracy = metrics.accuracy_score(Y_test,pred)\nconfusion_matrix = metrics.confusion_matrix(Y_test, pred)\nclassification = metrics.classification_report(Y_test, pred)\nprint()\nprint('============================== {} Model Test Results =============================='.format('Voting Classifier'))\nprint()\nprint (\"Model Accuracy:\" \"\\n\", accuracy)\nprint()\nprint(\"Confusion matrix:\" \"\\n\", confusion_matrix)\nprint()\nprint(\"Classification report:\" \"\\n\", classification) \nprint()    ","56fb82dd":"## Data Preprocessing","fbb79936":"## Machine Learning Models","2c2b8bdb":"# Hence we have been able to achive an accuracy of 96% and F1-score of 89%. ","67ea4484":"Since this data is very imbalanced, F1-score is given more priority over accuracy of the model. This model has been made to optimize the f1-macro score of the model. Steps are as following - \n1. Data Loading\n2. Data Preprocessing\n3. Balancing Imbalanced Dataset\n4. Machine Learning Models\n5. Ensemble Model ","d500a4b8":"## Data Loading","8642a111":"## Ensemble Model","eddceb80":"DecisionTreeClassifier(max_depth=35, random_state=10, splitter='random') - 0.85\n\nDecisionTreeClassifier(max_depth=33, random_state=20, splitter='random') - 0.88","2548209a":"# CICIDS ML PipeLine - 90% F1-score","eccc87cc":"## Balancing The Imbalanced Data"}}