{"cell_type":{"e432d2cd":"code","bee5f07b":"code","c7ce5cc2":"code","013901ab":"code","44819773":"code","988f6816":"code","aa0def0e":"code","89c172cc":"code","0fda9e44":"code","3569f0b6":"code","a0a520fe":"code","29649f0d":"code","e5ddceca":"code","92b7d1a3":"code","2afa3cc6":"code","b0dacd9a":"code","3e2edd10":"code","649646c1":"code","0e4eec75":"code","e0f3827a":"code","815132f7":"code","3c03a1f3":"code","66bc007d":"code","d88fd20a":"code","f7dabc43":"code","f2b8ad46":"code","1365cebf":"code","f0c96bdd":"code","ab788940":"code","d8cadd2c":"code","94c5ba4d":"code","d6021ff7":"code","606f939b":"code","96e3bcc1":"code","a42385bb":"code","d6b798ef":"code","4ee35489":"code","9931b45d":"code","9088dca5":"code","3f90d253":"code","7cad9cd3":"code","be0ef12c":"code","092ab770":"code","13cea2b1":"code","8f476e4e":"code","84c02a60":"code","a815a701":"code","5c5fd370":"code","a63d1b32":"code","3ed22ae7":"code","551a2345":"code","5ab26ccd":"code","5f86158f":"code","7d0895e8":"code","59b24156":"code","c5438ce6":"code","4736927f":"code","8e931cdd":"code","0c5cb5bf":"code","b1ab12d3":"code","796e1bd8":"code","f8274ddc":"code","0394615c":"code","00fd93ff":"code","1d4765a6":"code","2e2fb36d":"code","1d82b1f8":"code","f4a18e9c":"code","2c9749a0":"code","e1949502":"markdown","b91f60cb":"markdown","455fe823":"markdown","7c1ef174":"markdown","57ee65c8":"markdown","849e7fe7":"markdown","62f6d3c2":"markdown","b9eb3465":"markdown","f7a9cdc6":"markdown","cd7db722":"markdown","8286b4c6":"markdown","5aa603b1":"markdown","cc2235af":"markdown","160c5eb1":"markdown","dabe6fdb":"markdown","55551afc":"markdown","1e5c884d":"markdown","dad05e42":"markdown","e6dab0b0":"markdown","d645b7de":"markdown","f91cd27a":"markdown","1dae6681":"markdown","35d8cdaa":"markdown","638b90ee":"markdown","ea59c5b7":"markdown","1bb03f5c":"markdown","f606cec9":"markdown","aebc7e19":"markdown","cf54d859":"markdown","afea0e95":"markdown","a11b5635":"markdown","7d57979f":"markdown","299538ed":"markdown","945afd49":"markdown","29620d35":"markdown","ad5e0adb":"markdown","37d0842a":"markdown","cbf99e81":"markdown","37c24799":"markdown","2306f031":"markdown","547d5317":"markdown","aaf260ff":"markdown","fe2afe12":"markdown"},"source":{"e432d2cd":"import numpy as np \nimport pandas as pd \nimport json\nimport matplotlib.pyplot as plt\nfrom datetime import datetime,timedelta\nimport re as re\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bee5f07b":"sub_path_to_data = '\/kaggle\/input\/competitive-data-science-predict-future-sales\/'\ncategories = pd.read_csv(sub_path_to_data + 'item_categories.csv')\nitems = pd.read_csv(sub_path_to_data + 'items.csv')\nsales_train = pd.read_csv(sub_path_to_data + 'sales_train.csv')\nshops = pd.read_csv(sub_path_to_data + 'shops.csv')\ntest = pd.read_csv(sub_path_to_data + 'test.csv')","c7ce5cc2":"sales_train.head(1)","013901ab":"sales_train.shape","44819773":"test.head(1)","988f6816":"test.shape","aa0def0e":"items.head(1)","89c172cc":"items.shape","0fda9e44":"categories.head(1)","3569f0b6":"categories.shape","a0a520fe":"shops.head(1)","29649f0d":"shops.shape","e5ddceca":"sales_train.info()","92b7d1a3":"sales_train.describe(include=[np.number]).T","2afa3cc6":"sales_train.isnull().sum()","b0dacd9a":"duplicate_sales = sales_train.loc[sales_train.duplicated(keep=False)]","3e2edd10":"duplicate_sales","649646c1":"sales_train.loc[sales_train.duplicated(subset= ['date', 'shop_id', 'item_id'], keep=False) &\\\n                ~sales_train.duplicated(keep=False)]","0e4eec75":"sales_train.loc[sales_train.duplicated(subset= ['date', 'shop_id', 'item_id', 'item_price'], keep=False) & ~sales_train.duplicated(keep=False)]","e0f3827a":"sales_train.drop_duplicates(keep=\"first\", inplace=True)","815132f7":"sales_train.loc[sales_train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n\nsales_train.loc[sales_train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n\nsales_train.loc[sales_train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","3c03a1f3":"sales_train.loc[~sales_train['date'].str.match('^[0-3]\\d\\.[0-1]\\d\\.20\\d\\d$')]","66bc007d":"sales_train['date'] =  pd.to_datetime(sales_train['date'], format='%d.%m.%Y')","d88fd20a":"sales_sum = sales_train.groupby(['date'])['item_cnt_day'].sum().reset_index().set_index(\"date\")","f7dabc43":"sales_sum.loc[sales_sum.item_cnt_day > 7000]","f2b8ad46":"sales_train.loc[sales_train.item_price > 50000]","1365cebf":"sales_train = sales_train.loc[sales_train.item_price < 50000]","f0c96bdd":"sales_sum.plot(kind='bar', color='black', figsize=(24,6))","ab788940":"train = sales_train.groupby(['date_block_num', 'shop_id', 'item_id'])['item_cnt_day'].sum().reset_index()","d8cadd2c":"train.rename(columns={\"item_cnt_day\": \"item_cnt_month\"}, inplace=True)","94c5ba4d":"train['item_cnt_month'] = train['item_cnt_month'].fillna(0).clip(0,20)","d6021ff7":"shops_list = train['shop_id'].unique()\n\nfig, axs = plt.subplots(30,2, figsize=(20, 200), facecolor='w', edgecolor='k')\nfig.subplots_adjust(hspace = .5, wspace=.1)\naxs = axs.ravel()\n\nfor index, shop_id in enumerate(shops_list):\n    shop_sales = train.loc[train['shop_id'] == shop_id]\n    sales_per_month = shop_sales.groupby('date_block_num')['item_cnt_month'].sum().to_frame()\n    \n    axs[index].plot(sales_per_month.index, sales_per_month['item_cnt_month'], 'o-')\n    axs[index].set_xticks(sales_per_month.index)\n    axs[index].grid()\n    \n    axs[index].title.set_text(\"sales for shop {0}\".format(shop_id))\n    axs[index].set_xlabel('month')\n    axs[index].set_ylabel('number of sales')\n    \nplt.show()","606f939b":"shop_ids_test = test.shop_id.unique()\nfor shop_id in [0,1,11,20,8]:\n    print (shop_id in shop_ids_test)","96e3bcc1":"#train = train.loc[~(train.shop_id.isin([0,1,11,20,8]))]\n","a42385bb":"test['date_block_num'] = 34\ntrain = pd.concat([train, test.drop('ID', axis=1)], ignore_index=True, sort=False, keys=['date_block_num', 'shop_id', 'item_id'])\ntrain.fillna(0, inplace=True)","d6b798ef":"index_cols = ['shop_id', 'item_id', 'date_block_num']\n\ngrid = [] \nfor block_num in train.date_block_num.unique():\n    cur_shops = train[train['date_block_num']==block_num]['shop_id'].unique()\n    cur_items = train[train['date_block_num']==block_num]['item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\ntrain = pd.merge(grid,train,how='left',on=index_cols).fillna(0)\n\ntrain.sort_values(['date_block_num','shop_id','item_id'],inplace=True)","4ee35489":"cumsum = train.groupby('item_id')['item_cnt_month'].cumsum() - train['item_cnt_month']\ncumc = train.groupby('item_id').cumcount() + 1\ntrain['item_target_enc'] = cumsum\/cumc\ntrain['item_target_enc'].fillna(0.3343, inplace=True) \n\nencoded_feature = train['item_target_enc'].values","9931b45d":"train['month'] = train['date_block_num'].map(lambda month: month-12*(month\/\/12))","9088dca5":"def number_of_weekens_in_month(first_day):\n    ndays = first_day.daysinmonth\n    weekends = 0\n    for i in range(ndays):\n        if (pd.to_datetime(first_day + timedelta(days=(np.long(i)))).dayofweek in [5, 6]): \n            weekends = weekends + 1\n    return weekends","3f90d253":"first_days_map = sales_train.groupby('date_block_num')['date'].min().map(lambda date: date.replace(day=1))","7cad9cd3":"train['number_of_weekends'] = train['date_block_num'].map(first_days_map.map(number_of_weekens_in_month))","be0ef12c":"sales_train['month'] = sales_train['date_block_num'].map(lambda month: month-12*(month\/\/12))","092ab770":"train['days_in_month'] = train['date_block_num'].map(first_days_map.map(lambda day: day.daysinmonth))","13cea2b1":"def specify_the_accuracy_group(row):\n    num_of_block = row.date_block_num - 1\n    if (num_of_block != -1):\n        values = train.loc[(train['shop_id']==row.shop_id) & (train['item_id']==row.item_id)&\\\n                         (train['date_block_num']==num_of_block),['item_cnt_month']].values\n        return 0 if (values.size==0) else values[0][0]\n    else:\n        return 0","8f476e4e":"def previous_months_value(df, collumn, offset):\n    previous_month_values = df.copy()\n    previous_month_values['date_block_num'] = previous_month_values['date_block_num'] + offset\n    previous_month_values.rename(columns={collumn: 'prev_month_' + str(offset) + '_' + collumn}, inplace=True)\n    return pd.merge(df, previous_month_values[['date_block_num','shop_id','item_id','prev_month_' + str(offset) + '_' + collumn]], on=['date_block_num','shop_id','item_id'], how='left')","84c02a60":"offsets = [1,2,3,9]","a815a701":"column_to_offset = 'item_cnt_month'","5c5fd370":"for offset in offsets:\n    train = previous_months_value(train, column_to_offset, offset)\n    values = {'prev_month_' + str(offset) + '_' + column_to_offset: 0}\n    train = train.fillna(value=values)","a63d1b32":"item_price_map = sales_train.loc[sales_train.item_price > 0].groupby(['item_id'])['item_price'].mean()","3ed22ae7":"train['price'] = train['item_id'].map(item_price_map)","551a2345":"first_month_product_appeared = sales_train.groupby(['shop_id', 'item_id'])['date_block_num'].min().reset_index()\nfirst_month_product_appeared.rename(columns={'date_block_num': 'first_appeared'}, inplace=True)\ntrain = pd.merge(train,first_month_product_appeared, on=['shop_id','item_id'], how='left')\ntrain['first_appeared'] = train['date_block_num'] - train['first_appeared'] + 1\ntrain['first_appeared'] = train['first_appeared'].map(lambda n: 0 if (n < 0) else n)\nvalues = {'first_appeared': 0}\ntrain = train.fillna(value=values)","5ab26ccd":"new_shop = train.groupby('shop_id')['date_block_num'].min().map(lambda month: 1 if (month > 10) else 0)","5f86158f":"train['new_shop'] = train['shop_id'].map(new_shop)","7d0895e8":"sales_incompleate_data = train.groupby('shop_id')['date_block_num'].max().map(lambda month: 1 if (month < 33) else 0)","59b24156":"train['incompleate_data'] = train['shop_id'].map(sales_incompleate_data)","c5438ce6":"shops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])","4736927f":"shops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'","8e931cdd":"shops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]","0c5cb5bf":"categories['split'] = categories['item_category_name'].str.split('-')\ncategories['type'] = categories['split'].map(lambda x: x[0].strip())\ncategories['type_code'] = LabelEncoder().fit_transform(categories['type'])\n\ncategories['subtype'] = categories['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncategories['subtype_code'] = LabelEncoder().fit_transform(categories['subtype'])\ncategories = categories[['item_category_id','type_code', 'subtype_code']]\n","b1ab12d3":"train = pd.merge(train, shops, on=['shop_id'], how='left')","796e1bd8":"train.drop('month', axis=1, inplace=True)","f8274ddc":"train = pd.merge(train, items, on=['item_id'], how='left')\ntrain = pd.merge(train, categories, on=['item_category_id'], how='left')\ntrain['city_code'] = train['city_code'].astype(np.int8)\ntrain['item_category_id'] = train['item_category_id'].astype(np.int8)\ntrain['type_code'] = train['type_code'].astype(np.int8)\ntrain['subtype_code'] = train['subtype_code'].astype(np.int8)","0394615c":"train.drop(['item_name'], axis=1, inplace=True)","00fd93ff":"X_train = train[train.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = train[train.date_block_num < 33]['item_cnt_month']\nX_valid = train[train.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = train[train.date_block_num == 33]['item_cnt_month']\nX_test = train[train.date_block_num == 34].drop(['item_cnt_month'], axis=1)","1d4765a6":"baseline_preds = np.full((len(X_valid)), train.loc[train.date_block_num < 33, 'item_cnt_month'].mean())\nrmse_b = np.sqrt(mean_squared_error(Y_valid.values, baseline_preds))\nprint(\"RMSE: %f\" % (rmse_b))","2e2fb36d":"\nparameters = {'tree_method': 'exact'}\nmodel = XGBRegressor(\n    max_depth=8,\n    tree_method='exact',\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)","1d82b1f8":"X_test.days_in_month = 30\nX_test.number_of_weekends = 9","f4a18e9c":"X_test = pd.merge(test.drop('ID', axis=1), X_test, on=['shop_id', 'item_id', 'date_block_num'], how='left')","2c9749a0":"Y_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)","e1949502":"Drop them","b91f60cb":"### Add prices data","455fe823":"getting numerical columns info","7c1ef174":"### Add date related features","57ee65c8":"### Add a featur 'new shop' to compare shops age","849e7fe7":"looks good","62f6d3c2":"### Mean encode item_id","b9eb3465":"Looking at dates with number of sales more than 7000","f7a9cdc6":"### Add a feature 'incompleate data'","cd7db722":"We saw previously, that data for some shops is incompleat. Use simple criteria, ofcause it's possible to find a better criteria, or select shops with incompleate data manually based on grafs above","8286b4c6":"# Take an initial look onto the data","5aa603b1":"- we see some peaks\n- May be there is a global trend of sales\n\nBoth things could be useful","cc2235af":"Check for outliers first","160c5eb1":"checking that all dates are in the correct format","dabe6fdb":"I think it makes sense to remove shops 0,1,11 if they are not in the test set","55551afc":"### Predate data for training","1e5c884d":"We are interested in sales sums for each product separately per month, but it could be useful to take a look at the sum sales for all products and all shops","dad05e42":"No other duplicates","e6dab0b0":"### Add feature 'new product'","d645b7de":"I see that sometimes the difference is in price, which looks reasonable. Maybe it's because of selling used\/damaged\/something else items. Need keep in mind it for later. Now adding 'same price' criteria to the duplicate search ","f91cd27a":"### Converting type of the date to work with it later","1dae6681":"claculating monthly values and clipping to remove outliers","35d8cdaa":"### Taking initial look on the sales dynamics","638b90ee":"## Explore sales data","ea59c5b7":"### Calculate target value for the previous months","1bb03f5c":"checking for missing data","f606cec9":"Check for outliers","aebc7e19":"Some data cleaning","cf54d859":"Explore total number of sales for each shop","afea0e95":"So, we have nothing to drop ","a11b5635":"### Predict test values","7d57979f":"### Train","299538ed":"Checking for super high prices","945afd49":"### Prepare simple baseline","29620d35":"All dates are good. Converting:","ad5e0adb":"checking for duplicates","37d0842a":"### We can already add test data","cbf99e81":"### Encode categorical features","37c24799":"Another assumption is that we shouldhave only one row for each combination of\u00a0'date', 'shop_id', 'item_id'. Let's do this check with excluding already found duplicates:","2306f031":"Lookin at plots above we can say that:\n1. For some shops data for some month is missing \n2. For some shops we have data only for the first two months. Need to check that those shops are in the test set\n3. The age of the shops is very different\n4. Total dynamic of sales is defferent from shop to shop\n5. For some shops only one month provided\n6. For some shopes last months not provided","547d5317":"#### Calculating values of sales per month","aaf260ff":"Now preparing the diagram","fe2afe12":"Those values could be real duplicates, or we just need to sum them to get the correct value of item_cnt_day.\nAfter comparing score with dropping and summing duplicates, I see that score is better when keep duplicates and sume tham later"}}