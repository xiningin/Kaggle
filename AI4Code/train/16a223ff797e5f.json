{"cell_type":{"dcad42b7":"code","2d399438":"code","7bf37d72":"code","c63bea44":"code","e0f4d194":"code","4ca20ab7":"code","e1b77352":"code","edb8a281":"code","53ae1ff5":"code","426f6f20":"code","1a06257d":"code","1d649c7c":"code","2574a4bc":"code","d7ceed14":"code","6a50ef79":"code","2edfc6d1":"code","c61fcc39":"code","3484c236":"code","ad96702d":"code","0906e856":"code","9b8a9a3d":"code","0038824f":"code","c86748bf":"code","9dde8964":"markdown","a78ccdb0":"markdown","143fa451":"markdown","da78a75f":"markdown","2c3ce4a1":"markdown","3292eb65":"markdown","0d0f2a14":"markdown","665e6459":"markdown","1c74b451":"markdown","e62dc22f":"markdown","35ee6975":"markdown","19e70984":"markdown","2b1b39cb":"markdown","56f78e1d":"markdown","5a2c4b2d":"markdown","af398ef3":"markdown","7a4b3b71":"markdown","f1fa20dd":"markdown","d439ca72":"markdown"},"source":{"dcad42b7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","2d399438":"training_set = pd.read_csv('\/kaggle\/input\/unsw-nb15\/UNSW_NB15_training-set.csv')\ntraining_set.head()","7bf37d72":"training_set.info()","c63bea44":"training_set.head()","e0f4d194":"mask = (training_set.dtypes == np.object)\nprint(training_set.loc[:,mask].head())\nlist_cat = training_set.loc[:,mask].columns.tolist()\nprint(list_cat)\nprint(training_set.loc[:,mask].values)","4ca20ab7":"mask = (training_set.dtypes != np.object)\nprint(training_set.loc[:,mask].head())\nlist_cat = training_set.loc[:,mask].columns.tolist()\nprint(list_cat)\ntraining_set.loc[:,mask].describe()","e1b77352":"#  Check whether the positive label (1) match attack categories, and whether attack categories match labelled data.\n\n# all(iterable) returns True if all elements of the iterable are considered as true values\nprint(all(((training_set.label == 1) & (training_set.attack_cat != 'Normal')) == (training_set.attack_cat != 'Normal')))\nprint(all(((training_set.attack_cat != 'Normal') & (training_set.label == 1)) == (training_set.label == 1)))","edb8a281":"# number of occurrences for each attack category\ntraining_set.attack_cat.value_counts()","53ae1ff5":"mask = (training_set.label == 1)\nprint(training_set.loc[mask,:].service.value_counts())\nprint(training_set.loc[mask,:].proto.value_counts())","426f6f20":"mask = (training_set.label == 0)\nprint(training_set.loc[mask,:].service.value_counts())\nprint(training_set.loc[mask,:].proto.value_counts())","1a06257d":"Y = training_set.label\nX = training_set.drop(columns=['id','attack_cat','label'])\nmask = (X.dtypes == np.object)\nlist_cat = X.loc[:,mask].columns.tolist()\nlist_cat","1d649c7c":"X = pd.get_dummies(X, columns=list_cat)\nX.head()","2574a4bc":"Y.head()","d7ceed14":"import sklearn\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)","6a50ef79":"import xgboost as xgb\nfrom sklearn.metrics import classification_report,roc_auc_score,average_precision_score","2edfc6d1":"params = {\n    'max_depth': 10,\n    'objective': 'multi:softmax',  # error evaluation for multiclass training\n    'num_class': 2,                # Number of classes \n    'n_gpus': 4\n}\n\nxg_clf = xgb.XGBClassifier(**params)\nxg_clf.fit(X_train, y_train)","c61fcc39":"pred = xg_clf.fit(X_train, y_train).predict(X_test)\nprint(classification_report(y_test, pred))","3484c236":"roc_auc_score(y_test, pred)","ad96702d":"print('AUPRC = {}'.format(average_precision_score(y_test,pred)))","0906e856":"## PLOT IMPORTANCE OF FEATURES with type cover\n# \u201dcover\u201d is the average coverage of splits which use the feature where coverage is defined as the number of samples affected by the split\nxgb.plot_importance(xg_clf, importance_type='cover')\nplt.rcParams['figure.figsize'] = [10, 20]\nplt.show()","9b8a9a3d":"## PLOT IMPORTANCE OF FEATURES with type weight\n# \u201dweight\u201d is the number of times a feature appears in a tree\nxgb.plot_importance(xg_clf, importance_type='weight')\nplt.show()","0038824f":"## PLOT IMPORTANCE OF FEATURES with type gain\n# \u201dgain\u201d is the average gain of splits which use the feature\nxgb.plot_importance(xg_clf, importance_type='gain')\nplt.show()","c86748bf":"# plot single tree\nfrom xgboost import plot_tree\nfrom matplotlib.pylab import rcParams\n##set up the parameters\nrcParams['figure.figsize'] = 30,50\nprint('This is a plot of the first decision tree in the model (index 0), showing the features and feature values for each split as well as the output leaf nodes.!')\nplot_tree(xg_clf, num_trees=0, rankdir='LR')\nplt.show()","9dde8964":"## Objective of this lab session","a78ccdb0":"## 1. Loading Data","143fa451":"### AUROC Score","da78a75f":"#### Which protocols and services appear in the positively labelled entries? ","2c3ce4a1":"## Machine Learning Analysis","3292eb65":"## Statistics Analysis","0d0f2a14":"# Predicting attacks : UNSW-NB 15 dataset","665e6459":"### Numeric variables","1c74b451":"### Categorical variables","e62dc22f":"#### In the negatively labelled ones?","35ee6975":"The dataset used for this analysis is the UNSW-NB 15 dataset of Australian Centre for Cyber\nSecurity (ACCS). It was first published in 2015.","19e70984":"The objective of this lab session is twofold:\n* Deploy data analysis for the analysis of cybersecurity data\n* Perform anomaly detection using XGBoost and Linear Regression algorithms and evaluate its learning profile.","2b1b39cb":"### AUPRC Score","56f78e1d":"Let's use Linear Regression to evaluate the contribution of these independent parameters to the\ncharacterization as attack","5a2c4b2d":"### Evaluation of the training dataset","af398ef3":"Let's apply a Principal Component Analysis (PCA) to the dataset X, to extract independent parameters\nof the model","7a4b3b71":"## 2. Dataset Observation","f1fa20dd":"## Dataset","d439ca72":"## Data cleaning"}}