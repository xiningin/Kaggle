{"cell_type":{"e457c454":"code","6d78e493":"code","7b81a20a":"code","5032022d":"code","1f138f6e":"code","f3f8bedb":"code","9993f49f":"code","b90f424a":"code","e200a002":"code","e572895d":"code","f3b1d0fd":"code","488e8a01":"code","20f17de1":"code","e549bebb":"code","6f7eb4a0":"code","5afab899":"code","e22803b9":"code","461c62c0":"code","cd02dd90":"code","c0e7407e":"code","30ae6c81":"code","d0214ca6":"code","b82f14f3":"code","23b76c61":"code","9aba0d20":"code","cb9dc540":"code","48e4ec65":"code","184a436a":"code","51cd39ca":"code","22c0c729":"code","ce8c5d64":"code","236fe6ce":"code","8f97f0ad":"code","12830c6c":"code","ba14fd9a":"code","4c1ffe4a":"code","040553d0":"code","06c37bcf":"code","d72a722a":"code","b2fe6bda":"code","63b2a7b2":"code","0bc78402":"code","8f837ba7":"code","3d5dbbb3":"code","ac2a5eb8":"code","b76e70b4":"code","16c52cd8":"code","b7d59ef7":"code","44c83a14":"code","be38c5ed":"code","0f8bc136":"code","36a67f67":"code","760a9640":"code","49c8690c":"code","cf070039":"code","181ba577":"code","561660f4":"code","9a86be0e":"code","a3c62b8a":"markdown","2a7c6a1b":"markdown","5d340a37":"markdown","af147db5":"markdown","ac7ea252":"markdown","8f3bd091":"markdown","c8f18305":"markdown","0eb2024d":"markdown","3086c4cd":"markdown","a51a364b":"markdown","2aa0eac6":"markdown","fd5b9808":"markdown","0f36d7a4":"markdown","b0f01c03":"markdown","4fdd3da5":"markdown","75f8e0a6":"markdown","8f3911ad":"markdown","dfd6bbc0":"markdown","e22aaa6e":"markdown","03d09c7d":"markdown","e47e81e3":"markdown","f9766077":"markdown","b7f260c9":"markdown","f429532e":"markdown","315e0c9e":"markdown","456743f3":"markdown","227b4fe1":"markdown","2274c23d":"markdown","1697e988":"markdown","b8491ed0":"markdown"},"source":{"e457c454":"# Check the tensorflow version\n\nimport tensorflow as tf\ntf.__version__","6d78e493":"import pandas as pd\nimport numpy as np\nimport os\n\nimport cv2\n\nimport albumentations as albu\nfrom albumentations import Compose, ShiftScaleRotate, Resize\nfrom albumentations.pytorch import ToTensor\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nfrom sklearn.metrics import classification_report\n\nimport shutil\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","7b81a20a":"IMAGE_HEIGHT = 224\nIMAGE_WIDTH = 224\nIMAGE_CHANNELS = 3\n","5032022d":"os.listdir('..\/input\/rice-leaf-diseases\/rice_leaf_diseases')","1f138f6e":"# Source: Scikit Learn website\n# http:\/\/scikit-learn.org\/stable\/auto_examples\/\n# model_selection\/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n# selection-plot-confusion-matrix-py\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n\n\n","f3f8bedb":"leaf_smut_list = \\\nos.listdir('..\/input\/rice-leaf-diseases\/rice_leaf_diseases\/Leaf smut')\nbrown_spot_list = \\\nos.listdir('..\/input\/rice-leaf-diseases\/rice_leaf_diseases\/Brown spot')\nbacterial_leaf_blight_list = \\\nos.listdir('..\/input\/rice-leaf-diseases\/rice_leaf_diseases\/Bacterial leaf blight')\n\nprint(len(leaf_smut_list))\nprint(len(brown_spot_list))\nprint(len(bacterial_leaf_blight_list))","9993f49f":"# Create the train and val sets\n\ndf_leaf_smut = pd.DataFrame(leaf_smut_list, columns=['image'])\ndf_leaf_smut['target'] = 'leaf_smut'\n\ndf_brown_spot = pd.DataFrame(brown_spot_list, columns=['image'])\ndf_brown_spot['target'] = 'brown_spot'\n\ndf_bacterial_leaf_blight = pd.DataFrame(bacterial_leaf_blight_list, columns=['image'])\ndf_bacterial_leaf_blight['target'] = 'bacterial_leaf_blight'\n\n\n# Create a val set for each class\n\n# Sample 5 validation images from each class\ndf_leaf_smut_val = df_leaf_smut.sample(n=5, random_state=101)\ndf_brown_spot_val = df_brown_spot.sample(n=5, random_state=101)\ndf_bacterial_leaf_blight_val = df_bacterial_leaf_blight.sample(n=5, random_state=101)\n\n\nprint(len(df_leaf_smut_val))\nprint(len(df_brown_spot_val))\nprint(len(df_bacterial_leaf_blight_val))","b90f424a":"# Create the train set for each class\n\n# leaf_smut\n# get a list of val images\nval_list = list(df_leaf_smut_val['image'])\n# filter out the val images\ndf_leaf_smut_train = df_leaf_smut[~df_leaf_smut['image'].isin(val_list)] # ~ means notin\n\n# brown_spot\n# get a list of val images\nval_list = list(df_brown_spot_val['image'])\n# filter out the val images\ndf_brown_spot_train = df_brown_spot[~df_brown_spot['image'].isin(val_list)] # ~ means notin\n\n# bacterial_leaf_blight\n# get a list of val images\nval_list = list(df_bacterial_leaf_blight_val['image'])\n# filter out the val images\ndf_bacterial_leaf_blight_train = \\\ndf_bacterial_leaf_blight[~df_bacterial_leaf_blight['image'].isin(val_list)] # ~ means notin\n\n\n\nprint(len(df_leaf_smut_train))\nprint(len(df_brown_spot_train))\nprint(len(df_bacterial_leaf_blight_train))","e200a002":"# Create df_data, df_train and df_val\n\ndf_data = pd.concat([df_leaf_smut, df_brown_spot, df_bacterial_leaf_blight], axis=0).reset_index(drop=True)\n\ndf_train = \\\npd.concat([df_leaf_smut_train, df_brown_spot_train, df_bacterial_leaf_blight_train], axis=0).reset_index(drop=True)\n\ndf_val = \\\npd.concat([df_leaf_smut_val, df_brown_spot_val, df_bacterial_leaf_blight_val], axis=0).reset_index(drop=True)\n\ndf_data = shuffle(df_data)\ndf_train = shuffle(df_train)\ndf_val = shuffle(df_val)\n\nprint(df_data.shape)\nprint(df_train.shape)\nprint(df_val.shape)","e572895d":"df_data['target'].value_counts()","f3b1d0fd":"df_train['target'].value_counts()","488e8a01":"df_val['target'].value_counts()","20f17de1":"# Create the target as index values\n\n# combine val, train and test\nval_len = len(df_val)\ntrain_len = len(df_train)\ndf_combined =  pd.concat(objs=[df_val, df_train], axis=0).reset_index(drop=True)\n\n# create the dummy variables\ndf_combined = pd.get_dummies(df_combined, columns=['target'])\n\n# separate the train and val sets\ndf_val = df_combined[:val_len]\ndf_train = df_combined[val_len:]\n\n\nprint(df_train.shape)\nprint(df_val.shape)","e549bebb":"df_combined.head()","6f7eb4a0":"df_train.head()","5afab899":"df_val.head()","e22803b9":"df_combined.to_csv('df_combined.csv.gz', compression='gzip', index=False)\n\ndf_train.to_csv('df_train.csv.gz', compression='gzip', index=False)\ndf_val.to_csv('df_val.csv.gz', compression='gzip', index=False)\n","461c62c0":"!ls","cd02dd90":"# Create a new directory\nimage_dir = 'image_dir'\nos.mkdir(image_dir)\n\n!ls","c0e7407e":"leaf_smut_list = \\\nos.listdir('..\/input\/rice-leaf-diseases\/rice_leaf_diseases\/Leaf smut')\nbrown_spot_list = \\\nos.listdir('..\/input\/rice-leaf-diseases\/rice_leaf_diseases\/Brown spot')\nbacterial_leaf_blight_list = \\\nos.listdir('..\/input\/rice-leaf-diseases\/rice_leaf_diseases\/Bacterial leaf blight')\n\n\n# Transfer the leaf_smut images\nfor fname in leaf_smut_list:\n    \n    path = '..\/input\/rice-leaf-diseases\/rice_leaf_diseases\/Leaf smut\/'\n    \n    # source path to image\n    src = os.path.join(path, fname)\n    # destination path to image\n    dst = os.path.join(image_dir, fname)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dst)\n    \n    \n    \n    \n# Transfer the brown_spot images\nfor fname in brown_spot_list:\n    \n    path = '..\/input\/rice-leaf-diseases\/rice_leaf_diseases\/Brown spot\/'\n    \n    # source path to image\n    src = os.path.join(path, fname)\n    # destination path to image\n    dst = os.path.join(image_dir, fname)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dst)\n    \n    \n    \n    \n# Transfer the bacterial_leaf_blight images\nfor fname in bacterial_leaf_blight_list:\n    \n    path = '..\/input\/rice-leaf-diseases\/rice_leaf_diseases\/Bacterial leaf blight\/'\n    \n    # source path to image\n    src = os.path.join(path, fname)\n    # destination path to image\n    dst = os.path.join(image_dir, fname)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dst)\n       ","30ae6c81":"# Check how many images are now in image_dir\n\nlen(os.listdir('image_dir'))","d0214ca6":"# set up the canvas for the subplots\nplt.figure(figsize=(15,15))\n\n# Image 1\nplt.subplot(1,3,1)   # 1 row and 3 columns\nitem = 'DSC_0512.jpg'\npath = '..\/input\/rice-leaf-diseases\/rice_leaf_diseases\/Leaf smut\/' + item\nimage = plt.imread(path)\nplt.imshow(image)\nplt.xlabel('Leaf smut', fontsize=20)\n\n# Image 2\nplt.subplot(1,3,2)   # 1 row and 3 columns\nitem = 'DSC_0108.jpg'\npath = '..\/input\/rice-leaf-diseases\/rice_leaf_diseases\/Brown spot\/' + item\nimage = plt.imread(path)\nplt.imshow(image)\nplt.xlabel('Brown spot', fontsize=20)\n\n# Image 2\nplt.subplot(1,3,3)   # 1 row and 3 columns\nitem = 'DSC_0402.JPG'\npath = '..\/input\/rice-leaf-diseases\/rice_leaf_diseases\/Bacterial leaf blight\/' + item\nimage = plt.imread(path)\nplt.imshow(image)\nplt.xlabel('Bacterial leaf blight', fontsize=20)\n\n\n\n\n\n\n# set up the canvas for the subplots\nplt.figure(figsize=(15,15))\n\n# Image 1\nplt.subplot(1,3,1)   # 1 row and 3 columns\nitem = 'DSC_0316.JPG'\npath = '..\/input\/rice-leaf-diseases\/rice_leaf_diseases\/Leaf smut\/' + item\nimage = plt.imread(path)\nplt.imshow(image)\nplt.xlabel('Leaf smut', fontsize=20)\n\n# Image 2\nplt.subplot(1,3,2)   # 1 row and 3 columns\nitem = 'DSC_0303.JPG'\npath = '..\/input\/rice-leaf-diseases\/rice_leaf_diseases\/Brown spot\/' + item\nimage = plt.imread(path)\nplt.imshow(image)\nplt.xlabel('Brown spot', fontsize=20)\n\n# Image 2\nplt.subplot(1,3,3)   # 1 row and 3 columns\nitem = 'DSC_0702.jpg'\npath = '..\/input\/rice-leaf-diseases\/rice_leaf_diseases\/Bacterial leaf blight\/' + item\nimage = plt.imread(path)\nplt.imshow(image)\nplt.xlabel('Bacterial leaf blight', fontsize=20)\n\nplt.show()","b82f14f3":"# Pneumothorax - 1st place solution\n# Source: https:\/\/www.kaggle.com\/c\/siim-acr-pneumothorax-segmentation\/discussion\/107824#latest-620521\n\n# Albumentations\n\nimport albumentations as albu\n\n\ndef augment_image(augmentation, image):\n    \n    \"\"\"\n    Uses the Albumentations library.\n    \n    Inputs: \n    1. augmentation - this is the instance of type of augmentation to do \n    e.g. aug_type = HorizontalFlip(p=1) \n    # p=1 is the probability of the transform being executed.\n    \n    2. image - image with shape (h,w)\n    \n    Output:\n    Augmented image as a numpy array.\n    \n    \"\"\"\n    # get the transform as a dict\n    aug_image_dict =  augmentation(image=image)\n    # retrieve the augmented matrix of the image\n    image_matrix = aug_image_dict['image']\n    \n    \n    return image_matrix\n\n","23b76c61":"# Define the transforms\n\n\naug_types = albu.Compose([\n            albu.HorizontalFlip(),\n             albu.OneOf([\n                albu.HorizontalFlip(),\n                albu.VerticalFlip(),\n                ], p=0.8),\n            albu.OneOf([\n                albu.RandomContrast(),\n                albu.RandomGamma(),\n                albu.RandomBrightness(),\n                ], p=0.3),\n            albu.OneOf([\n                albu.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n                albu.GridDistortion(),\n                albu.OpticalDistortion(distort_limit=2, shift_limit=0.5),\n                ], p=0.3),\n            albu.ShiftScaleRotate()\n            ])\n\n","9aba0d20":"# Get an image to test transformations\n\n# get a list of train png images\npath = 'image_dir\/'\nimage_list = os.listdir('image_dir')\n\nfname = image_list[1]\nimage_path = path + fname\n\nimage = plt.imread(image_path)\nplt.imshow(image)","cb9dc540":"# Test the transformation setup.\n# The image will be different each time this cell is run.\n\naug_image = augment_image(aug_types, image)\n\nplt.imshow(aug_image)","48e4ec65":"df_train.head()","184a436a":"def train_generator(batch_size=8):\n    \n    while True:\n        \n        # load the data in chunks (batches)\n        for df in pd.read_csv('df_train.csv.gz', chunksize=batch_size):\n            \n            # get the list of images\n            image_id_list = list(df['image'])\n            \n            # Create empty X matrix - 3 channels\n            X_train = np.zeros((len(df), IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), dtype=np.uint8)\n            \n\n        \n            \n            # Create X_train\n            #================\n            \n            for i in range(0, len(image_id_list)):\n              \n              \n                # get the image and mask\n                image_id = image_id_list[i]\n\n\n                # set the path to the image\n                path = 'image_dir\/' + image_id\n\n                # read the image\n                image = cv2.imread(path)\n                \n                # convert to from BGR to RGB\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                \n                # resize the image\n                image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n                \n                \n            \n            \n            # Create y_train\n            # ===============\n                cols = ['target_bacterial_leaf_blight', 'target_brown_spot', 'target_leaf_smut']\n                y_train = df[cols]\n                y_train = np.asarray(y_train) \n\n                # change the shape to (batch_size, 1)\n                #y_train = y_train.reshape((-1, 1)) # -1 tells numpy to automatically detect the batch size\n       \n              \n            # Augment the image and mask\n            # ===========================\n\n                aug_image = augment_image(aug_types, image)\n              \n                # insert the image into X_train\n                X_train[i] = aug_image\n                \n                          \n                \n            # Normalize the images\n            X_train = X_train\/255\n\n            yield X_train, y_train","51cd39ca":"# Test the generator\n\n# initialize\ntrain_gen = train_generator(batch_size=8)\n\n# run the generator\nX_train, y_train = next(train_gen)\n\nprint(X_train.shape)\nprint(y_train.shape)","22c0c729":"y_train","ce8c5d64":"# Print the first image in X_train\n# Remember that train images have been augmented.\n\nimage = X_train[0,:,:,:]\nplt.imshow(image)","236fe6ce":"def val_generator(batch_size=5):\n    \n    while True:\n        \n        # load the data in chunks (batches)\n        for df in pd.read_csv('df_val.csv.gz', chunksize=batch_size):\n            \n            # get the list of images\n            image_id_list = list(df['image'])\n            \n            # Create empty X matrix - 3 channels\n            X_val = np.zeros((len(df), IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), dtype=np.uint8)\n            \n\n        \n            \n            # Create X_val\n            #================\n            \n            for i in range(0, len(image_id_list)):\n              \n              \n                # get the image and mask\n                image_id = image_id_list[i]\n\n\n                # set the path to the image\n                path = 'image_dir\/' + image_id\n\n                # read the image\n                image = cv2.imread(path)\n                \n                # convert to from BGR to RGB\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                \n                # resize the image\n                image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n\n                # insert the image into X_train\n                X_val[i] = image\n                \n                \n            \n            \n            # Create y_val\n            # ===============\n\n                cols = ['target_bacterial_leaf_blight', 'target_brown_spot', 'target_leaf_smut']\n                y_val = df[cols]\n                y_val = np.asarray(y_val) \n\n                # change the shape to (batch_size, 1)\n                #y_val = y_val.reshape((-1, 1)) # -1 tells numpy to automatically detect the batch size\n       \n            \n                \n                          \n                \n            # Normalize the images\n            X_val = X_val\/255\n\n            yield X_val, y_val","8f97f0ad":"# Test the generator\n\n# initialize\nval_gen = val_generator(batch_size=5)\n\n# run the generator\nX_val, y_val = next(val_gen)\n\nprint(X_val.shape)\nprint(y_val.shape)","12830c6c":"y_val","ba14fd9a":"# print the image from X_val\nimage = X_val[0,:,:,:]\nplt.imshow(image)","4c1ffe4a":"def test_generator(batch_size=1):\n    \n    while True:\n        \n        # load the data in chunks (batches)\n        for df in pd.read_csv('df_val.csv.gz', chunksize=batch_size):\n            \n            # get the list of images\n            image_id_list = list(df['image'])\n            \n            # Create empty X matrix - 3 channels\n            X_test = np.zeros((len(df), IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), dtype=np.uint8)\n            \n\n        \n            \n            # Create X_test\n            #================\n            \n            for i in range(0, len(image_id_list)):\n              \n              \n                # get the image and mask\n                image_id = image_id_list[i]\n\n\n                # set the path to the image\n                path = 'image_dir\/' + image_id\n\n                # read the image\n                image = cv2.imread(path)\n                \n                # convert to from BGR to RGB\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                \n                # resize the image\n                image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n\n                # insert the image into X_train\n                X_test[i] = image\n                \n                 \n                \n            # Normalize the images\n            X_test = X_test\/255\n\n            yield X_test","040553d0":"# Test the generator\n\n# initialize\ntest_gen = test_generator(batch_size=1)\n\n# run the generator\nX_test = next(test_gen)\n\nprint(X_test.shape)","06c37bcf":"# print the image from X_test\n\nimage = X_test[0,:,:,:]\nplt.imshow(image)","d72a722a":"from tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nfrom tensorflow.keras.metrics import categorical_accuracy\n\nfrom tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau, \n                                        ModelCheckpoint, CSVLogger, LearningRateScheduler)\n","b2fe6bda":"from tensorflow.keras.applications.mobilenet import MobileNet\n\nmodel = MobileNet(weights='imagenet')\n\n# Exclude the last 2 layers of the above model.\nx = model.layers[-2].output\n\n# Create a new dense layer for predictions\n# 3 corresponds to the number of classes\npredictions = Dense(3, activation='softmax')(x)\n\n# inputs=model.input selects the input layer, outputs=predictions refers to the\n# dense layer we created above.\n\nmodel = Model(inputs=model.input, outputs=predictions)\n\nmodel.summary()","63b2a7b2":"TRAIN_BATCH_SIZE = 8\nVAL_BATCH_SIZE = 5\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = TRAIN_BATCH_SIZE\nval_batch_size = VAL_BATCH_SIZE\n\n# determine numtrain steps\ntrain_steps = np.ceil(num_train_samples \/ train_batch_size)\n# determine num val steps\nval_steps = np.ceil(num_val_samples \/ val_batch_size)","0bc78402":"# Initialize the generators\ntrain_gen = train_generator(batch_size=TRAIN_BATCH_SIZE)\nval_gen = val_generator(batch_size=VAL_BATCH_SIZE)\n\nmodel.compile(\n    Adam(lr=0.0001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy'],\n)\n\n\n\nfilepath = \"model.h5\"\n\n#earlystopper = EarlyStopping(patience=10, verbose=1)\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, \n                             save_best_only=True, mode='max')\n\n#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, \n                                   #verbose=1, mode='min')\n\n\n\nlog_fname = 'training_log.csv'\ncsv_logger = CSVLogger(filename=log_fname,\n                       separator=',',\n                       append=False)\n\ncallbacks_list = [checkpoint, csv_logger]\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, epochs=100, \n                              validation_data=val_gen, validation_steps=val_steps,\n                             verbose=1,\n                             callbacks=callbacks_list)","8f837ba7":"# Display the training log\n\ntrain_log = pd.read_csv('training_log.csv')\n\ntrain_log.head()","3d5dbbb3":"# get the metric names so we can use evaulate_generator\nmodel.metrics_names","ac2a5eb8":"model.load_weights('model.h5')\n\nval_gen = val_generator(batch_size=1)\n\nval_loss, val_acc = \\\nmodel.evaluate_generator(val_gen, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)","b76e70b4":"# display the loss and accuracy curves\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\n\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.show()","16c52cd8":"test_gen = test_generator(batch_size=1)\n\npreds = model.predict_generator(test_gen, steps=len(df_val), verbose=1)\n","b7d59ef7":"# get y_pred as index values\n\ny_pred = np.argmax(preds, axis=1)\n\ny_pred","44c83a14":"# get y_true as index values\n\ncols = ['target_bacterial_leaf_blight', 'target_brown_spot', 'target_leaf_smut']\ny_true = df_val[cols]\ny_true = np.asarray(y_true) \n\ny_true = np.argmax(y_true, axis=1)\n\ny_true","be38c5ed":"# Compare y_true and y_pred\n\nprint(y_pred)\nprint(y_true)","0f8bc136":"from sklearn.metrics import confusion_matrix\nimport itertools\n\ncm = confusion_matrix(y_true, y_pred)","36a67f67":"# bacterial_leaf_blight = 0\n# brown_spot = 1\n# leaf_smut = 2\n\ncm_plot_labels = ['leaf_blight', 'brown_spot', 'leaf_smut']\n\nplot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')","760a9640":"from sklearn.metrics import classification_report\n\n# Generate a classification report\nreport = classification_report(y_true, y_pred, target_names=['bacterial_leaf_blight', 'brown_spot', 'leaf_smut'])\n\nprint(report)","49c8690c":"# --ignore-installed is added to fix an error.\n\n# https:\/\/stackoverflow.com\/questions\/49932759\/pip-10-and-apt-how-to-avoid-cannot-uninstall\n# -x-errors-for-distutils-packages\n\n!pip install tensorflowjs --ignore-installed","cf070039":"# Use the command line conversion tool to convert the model\n\n!tensorflowjs_converter --input_format keras model.h5 tfjs\/model","181ba577":"# check that the folder containing the tfjs model files has been created\n!ls","561660f4":"# Delete the test_dir directory we created to prevent a Kaggle error.\n# Kaggle allows a max of 500 files to be saved.\n\nshutil.rmtree('image_dir')","9a86be0e":"!ls","a3c62b8a":"## Train the Model","2a7c6a1b":"## Set up and test the Augmentations","5d340a37":"## Helper Functions","af147db5":"### Rice Leaf Disease Analyzer\nby Marsh [ @vbookshelf ]<br>\n4 March 2020","ac7ea252":"## Citations\n\n- Prajapati HB, Shah JP, Dabhi VK. Detection and classification of rice plant diseases. Intelligent Decision Technologies. 2017 Jan 1;11(3):357-73, doi: 10.3233\/IDT-170301.\n\n- Ref Paper:<br>\nDetection and classification of rice plant diseases\nhttps:\/\/www.researchgate.net\/publication\/318437440_Detection_and_classification_of_rice_plant_diseases\n\n- Header image by HoangTuan_photography on Pixabay\n","8f3bd091":"### [ 3 ] Test Generator","c8f18305":"## Prepare the Data","0eb2024d":"**Recall** = Given a class, will the classifier be able to detect it?<br>\n**Precision** = Given a class prediction from a classifier, how likely is it to be correct?<br>\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.","3086c4cd":"## Build the Data Generators","a51a364b":"## Conclusion\n\nHere are a few things you could try:\n- Implement 10 fold cross validation.\n- Use a different pre-trained model e.g. Densenet169 or Resnet18.\n- Use different image pre-processing methods.\n- Use a different augmentation setup or none at all.\n\nThank you for reading.","2aa0eac6":"## Display some images by class","fd5b9808":"## Approach\n\n- Resize all images to 224x224x3\n- Use 25 images from each class for training (105 training images)\n- Use 5 images from each class for validation (15 validation images)\n- Fine tune a Mobilenet model that was pre-trained on imagenet.\n- Use Adam optimizer, categorical crossentropy loss and a constant learning rate of 0.0001\n- We won't use the pre-processing method that was applied to the imagenet images that were used to pre-train Mobilenet. Instead we will simply normalize all images by dividing by 255.\n- Perform image augmentation using the Albumentations library. Image augmentation will help to reduce overfitting, improve model performance and help the model generalize better.","0f36d7a4":"## Convert the Model to Tensorflow.js","b0f01c03":"<img src=\"http:\/\/rice.test.woza.work\/assets\/rice_kaggle.jpg\" width=\"500\"><\/img>\n","4fdd3da5":"### [ 2 ] Val Generator","75f8e0a6":"## Model Architecture","8f3911ad":"### [ 1 ] Train Generator","dfd6bbc0":"## Helpful Resources\n\n- Albumentations paper:<br>\nAlbumentations: fast and flexible image augmentations<br>\nhttps:\/\/arxiv.org\/abs\/1809.06839\n\n- If you would like to learn how to build Tensorflow.js web apps I recommend this video tutorial:<br>\nhttps:\/\/www.youtube.com\/watch?v=HEQDRWMK6yY\n\n- I've also included a few practical app implementation tips on the readme page in this repo:<br>\nhttps:\/\/github.com\/vbookshelf\/Skin-Lesion-Analyzer","e22aaa6e":"## Make a prediction on the val set","03d09c7d":"### Move all images to the same folder","e47e81e3":"## Introduction\n\nIn this kernel we will build a Tensorflow_2.0 Keras model to classify three types of rice leaf diseases. These diseases include leaf smut, brown spot and bacterial leaf blight. The dataset consists of only 120 images. Normally, when using deep learning, one would not expect to get very good results with so few training images. However, by using image augmentation combined with a pre-trained Mobilenet model our validation accuracy score will be greater than 90%. In fact, while doing experiments I found that it was possible to get a 100% accuracy score by fine tuning either Keras Densenet169 or Pytorch Resnet18. \n\nIn this kernel I've used Mobilenet because my aim was to deploy the trained model online in a Tensorflow.js web app. Mobilenet was designed for use on the web. It's small which means that it downoads quickly and it runs fast.\n\nThis is the link to the live app. The html, css, and javascript code is available on Github. I recommend using the latest version of the Chrome browser. \n> Web App<br>\n> http:\/\/rice.test.woza.work\/\n>\n> Github<br>\n> https:\/\/github.com\/vbookshelf\/Rice-Leaf-Disease-Analyzer\n\n\n","f9766077":"## Plot the training curves","b7f260c9":"## Classification Report","f429532e":"### Transform the target \n\nHere we will one hot encode the target classes.","315e0c9e":"## Confusion Matrix","456743f3":"## Notes\n\n1. For validation I've used a simple train\/test split. However, because the dataset is so small using 5 or 10 fold cross validation is a wiser validation strategy. It will provide a better estimate of the model's quality. This is just a starter kernel so I will leave this for you to experiment with.\n\n2. I've defined an image augmentation setup below. If you display some of the augmented images they may look strange. It's easy to think that these images will confuse the model during training - this is what I thought at first. However, even though the augmented images may look a bit crazy, this approach actually works.","227b4fe1":"## Evaluate the model on the val set","2274c23d":"## Save the dataframes as compressed csv files\nThese csv files will allow us to use Pandas chunking to feed images into the generators.","1697e988":"### Create a dataframe containing all the images","b8491ed0":"## Data Summary\n\n- 120 jpg images of various sizes\n- 3 classes --> leaf smut, brown spot and bacterial leaf blight\n- 40 images per class\n- There are no images of normal rice leaves i.e. not disease infected.\n- According to the [paper](https:\/\/www.researchgate.net\/publication\/318437440_Detection_and_classification_of_rice_plant_diseases) the leaves were placed against a white background before being photographed. \n- There are images in the dataset that appear to have been processed e.g. the background has been removed. Therefore, we have a mixture of raw and processed images.\n\n"}}