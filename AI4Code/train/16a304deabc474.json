{"cell_type":{"fd1a37fa":"code","64a791cf":"code","ea0b9e8b":"code","746e3d1f":"code","7a1bdac7":"code","74ca161d":"code","d8eff582":"code","004d240f":"code","15b1330e":"code","bb4453df":"code","25886d9c":"code","c5721cf5":"code","35c59ec3":"code","d65828ce":"code","3ad5b6ae":"code","fb464ea9":"markdown","da2bf3a2":"markdown","a56b82be":"markdown","5c9ea25e":"markdown","2c9d2cbc":"markdown","685b6702":"markdown","9417280d":"markdown","0fea34ee":"markdown","6809a614":"markdown","88d8497d":"markdown","5f302353":"markdown","88405374":"markdown"},"source":{"fd1a37fa":"!pip install -U lightautoml","64a791cf":"# Standard python libraries\nimport logging\nimport os\nimport time\nimport requests\nlogging.basicConfig(format='[%(asctime)s] (%(levelname)s): %(message)s', level=logging.INFO)\n\n# Installed libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nimport torch\n\n# Imports from our package\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task","ea0b9e8b":"N_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 3600 # Time in seconds for automl run - UPDATED VALUE FOR UTILIZATION\nTARGET_NAME = 'final_price' # Target column name","746e3d1f":"%%time\n\ntrain_data = pd.read_csv('..\/input\/lightautomlcourse-hw1\/train_data.csv')\ntrain_data.head()","7a1bdac7":"test_data = pd.read_csv('..\/input\/lightautomlcourse-hw1\/test_data.csv')\ntest_data.head()","74ca161d":"submission = pd.read_csv('..\/input\/lightautomlcourse-hw1\/sample_submission.csv')\nsubmission.head()","d8eff582":"%%time\n\ndef create_extra_features(data):\n    data['NANs_cnt'] = data.isnull().sum(axis = 1) \n    \ndef create_col_with_min_freq(data, col, min_freq = 10):\n    # replace rare values (less than min_freq rows) in feature by RARE_VALUE\n    data[col + '_fixed'] = data[col].astype(str)\n    data.loc[data[col + '_fixed'].value_counts()[data[col + '_fixed']].values < min_freq, col + '_fixed'] = \"RARE_VALUE\"\n    data.replace({'nan': np.nan}, inplace = True)\n\ndef create_gr_feats(data):\n    # create aggregation feats for numeric features based on categorical ones\n    for cat_col in ['vehicle_manufacturer', 'vehicle_model', 'vehicle_category',\n                   'vehicle_gearbox_type', 'doors_cnt', 'wheels', 'vehicle_color', \n                   'vehicle_interior_color', 'deal_type']:\n        create_col_with_min_freq(data, cat_col, 15)\n        for num_col in ['current_mileage', 'vehicle_year', 'car_leather_interior']:\n            for n, f in [('mean', np.mean), ('min', np.nanmin), ('max', np.nanmax)]:\n                data['FIXED_' + n + '_' + num_col + '_by_' + cat_col] = data.groupby(cat_col + '_fixed')[num_col].transform(f)\n                \n    # create features with counts\n    for col in ['vehicle_manufacturer', 'vehicle_model', 'vehicle_category',\n               'current_mileage', 'vehicle_year', 'vehicle_gearbox_type', 'doors_cnt',\n               'wheels', 'vehicle_color', 'vehicle_interior_color', 'car_vin', 'deal_type']:\n        data[col + '_cnt'] = data[col].map(data[col].value_counts(dropna = False))\n    \n        \n\ncreate_extra_features(train_data)\ncreate_extra_features(test_data)\n\nall_df = pd.concat([train_data, test_data]).reset_index(drop = True)\ncreate_gr_feats(all_df)\ntrain_data, test_data = all_df[:len(train_data)], all_df[len(train_data):]\nprint(train_data.shape, test_data.shape)","004d240f":"train_data.head()","15b1330e":"%%time\n# COMPETITION METRIC IS MAE SO WE SET IT FOR OUR TASK\ntask = Task('reg', loss='mae', metric='mae')","bb4453df":"%%time\n\nroles = {'target': TARGET_NAME,\n         'drop': ['row_ID']\n         }","25886d9c":"%%time \n# CHANGED TabularAutoML to TabularUtilizedAutoML for timeout utilization\nautoml = TabularUtilizedAutoML(task = task, \n                       timeout = TIMEOUT,\n                       cpu_limit = N_THREADS,\n                       general_params = {'use_algos': [['linear_l2', 'lgb', 'lgb_tuned']]},\n                       reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE},\n                      )\noof_pred = automl.fit_predict(train_data, roles = roles)\nlogging.info('oof_pred:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))","c5721cf5":"%%time\n\n# Fast feature importances calculation\nfast_fi = automl.get_feature_scores('fast')\nfast_fi.set_index('Feature')['Importance'].plot.bar(figsize = (20, 10), grid = True)","35c59ec3":"%%time\n\ntest_pred = automl.predict(test_data)\nlogging.info('Prediction for test data:\\n{}\\nShape = {}'\n              .format(test_pred, test_pred.shape))\n\nlogging.info('Check scores...')\nlogging.info('OOF score: {}'.format(mean_absolute_error(train_data[TARGET_NAME].values, oof_pred.data[:, 0])))","d65828ce":"submission[TARGET_NAME] = test_pred.data[:, 0]\nsubmission.head()","3ad5b6ae":"submission.to_csv('lightautoml_baseline_custom_fe.csv', index = False)","fb464ea9":"Roles setup here set target column and base date, which is used to calculate date differences:","da2bf3a2":"## Step 4. Predict to test data and check scores","a56b82be":"## Step 5. Generate submission","5c9ea25e":"# Step 0.4. Some user feature preparation ","2c9d2cbc":"# Step 0.1. Import necessary libraries ","685b6702":"## Step 2. Setup columns roles","9417280d":"# Step 0.0. Install LightAutoML","0fea34ee":"# ========= AutoML preset usage =========\n\n\n## Step 1. Create Task","6809a614":"## Step 3. Create AutoML from preset","88d8497d":"# Step 0.3. Data load ","5f302353":"To create AutoML model here we use `TabularAutoML` preset, which looks like:\n\n![TabularAutoML preset pipeline](https:\/\/github.com\/sberbank-ai-lab\/LightAutoML\/raw\/master\/imgs\/tutorial_2_pipeline.png)\n\nAll params we set above can be send inside preset to change its configuration:","88405374":"# Step 0.2. Parameters "}}