{"cell_type":{"4d1818d1":"code","a565ebe6":"code","3db13216":"code","1da19f8a":"code","570daab0":"code","d0c2a864":"code","32a45d00":"code","8aa02721":"code","a11ecd08":"code","a80abe9f":"code","b5d35dc8":"code","1979b821":"code","c8437cfb":"code","9a3b3a38":"code","5ad47f3c":"code","e5a36d11":"code","5589c4ea":"code","5f855afb":"code","5c3a8d60":"code","eee60bf1":"code","4587c6b1":"code","ec02eaa0":"code","0a7484ec":"markdown","aa6bcff7":"markdown","6a5fff72":"markdown","65cede96":"markdown","948148e2":"markdown","06fe2512":"markdown","9b81c3af":"markdown","b043a541":"markdown","ba1f91ae":"markdown","b5477c3a":"markdown","b4284b1c":"markdown","d1eb76cd":"markdown","62197923":"markdown"},"source":{"4d1818d1":"import sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')","a565ebe6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nimport os\nimport math","3db13216":"from sklearn.metrics import accuracy_score\nfrom sklearn.utils import class_weight\nfrom PIL import Image as pil_image\nfrom tqdm import tqdm\nimport scipy\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go","1da19f8a":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nimport timm\nfrom timm.optim import Lookahead, RAdam","570daab0":"print(os.listdir(\"\/kaggle\/input\/\"))\nprint(os.listdir(\"\/kaggle\/input\/hotelid-trained-models\"))","d0c2a864":"SEED = 42\nPROJECT_FOLDER = \"\/kaggle\/input\/hotel-id-2021-fgvc8\/\"\nTRAIN_DATA_FOLDER = \"\/kaggle\/input\/hotelid-images-512x512-padded\/\"\nTEST_DATA_FOLDER = PROJECT_FOLDER + \"test_images\/\"","32a45d00":"print(os.listdir(PROJECT_FOLDER))","8aa02721":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","a11ecd08":"import albumentations as A\nimport albumentations.pytorch as APT\nimport cv2 \n\nIMG_SIZE = 512\n\nbase_transform = A.Compose([\n    A.ToFloat(),\n    APT.transforms.ToTensor(),\n])\n\n\ntest_tta_transforms = {\n    \"base\": A.Compose([A.ToFloat(), APT.transforms.ToTensor(),]),\n    \"h_flip\": A.Compose([A.ToFloat(), A.HorizontalFlip(p=1), APT.transforms.ToTensor(),]),\n    \"v_flip\": A.Compose([A.ToFloat(), A.VerticalFlip(p=1), APT.transforms.ToTensor(),]),\n    \"rotate+90\": A.Compose([A.ToFloat(), A.Rotate(limit=90, p=1), APT.transforms.ToTensor(),]),\n    \"rotate-90\": A.Compose([A.ToFloat(), A.Rotate(limit=-90, p=1), APT.transforms.ToTensor(),]),\n#     \"rand_bright\": A.Compose([A.ToFloat(), A.RandomBrightness(p=1), APT.transforms.ToTensor(),]),\n}","a80abe9f":"def pad_image(img):\n    w, h, c = np.shape(img)\n    if w > h:\n        pad = int((w - h) \/ 2)\n        img = cv2.copyMakeBorder(img, 0, 0, pad, pad, cv2.BORDER_CONSTANT, value=0)\n    else:\n        pad = int((h - w) \/ 2)\n        img = cv2.copyMakeBorder(img, pad, pad, 0, 0, cv2.BORDER_CONSTANT, value=0)\n        \n    return img\n\n\ndef open_and_preprocess_image(image_path):\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = pad_image(img)\n    return cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n\n\nclass HotelImageDataset:\n    def __init__(self, data, transform=None, data_folder=\"train_images\/\"):\n        self.data = data\n        self.data_folder = data_folder\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        record = self.data.iloc[idx]\n        image_path = self.data_folder + record[\"image\"]\n        \n        if \"test\" in self.data_folder:\n            image = np.array(open_and_preprocess_image(image_path)).astype(np.uint8)\n        else:\n            image = np.array(pil_image.open(image_path)).astype(np.uint8)\n\n        if self.transform:\n            transformed = self.transform(image=image)\n        \n        return {\n            \"image\" : transformed[\"image\"],\n        }","b5d35dc8":"# source: https:\/\/github.com\/ronghuaiyang\/arcface-pytorch\/blob\/master\/models\/metrics.py\nclass ArcMarginProduct(nn.Module):\n    r\"\"\"Implement of large margin arc distance: :\n        Args:\n            in_features: size of each input sample\n            out_features: size of each output sample\n            s: norm of input feature\n            m: margin\n            cos(theta + m)\n        \"\"\"\n    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n        output *= self.s\n\n        return output\n\nclass HotelIdModel(nn.Module):\n    def __init__(self, out_features, embed_size=256, backbone_name=\"efficientnet_b3\"):\n        super(HotelIdModel, self).__init__()\n\n        self.embed_size = embed_size\n        self.backbone = timm.create_model(backbone_name, pretrained=False)\n        in_features = self.backbone.get_classifier().in_features\n\n        fc_name, _ = list(self.backbone.named_modules())[-1]\n        if fc_name == 'classifier':\n            self.backbone.classifier = nn.Identity()\n        elif fc_name == 'head.fc':\n            self.backbone.head.fc = nn.Identity()\n        elif fc_name == 'fc':\n            self.backbone.fc = nn.Identity()\n        else:\n            raise Exception(\"unknown classifier layer: \" + fc_name)\n\n        self.arc_face = ArcMarginProduct(self.embed_size, out_features, s=30.0, m=0.50, easy_margin=False)\n\n        self.post = nn.Sequential(\n            nn.utils.weight_norm(nn.Linear(in_features, self.embed_size*2), dim=None),\n            nn.BatchNorm1d(self.embed_size*2),\n            nn.Dropout(0.2),\n            nn.utils.weight_norm(nn.Linear(self.embed_size*2, self.embed_size)),\n            nn.BatchNorm1d(self.embed_size),\n        )\n\n        print(f\"Model {backbone_name} ArcMarginProduct - Features: {in_features}, Embeds: {self.embed_size}\")\n        \n    def forward(self, input, targets = None):\n        x = self.backbone(input)\n        x = x.view(x.size(0), -1)\n        x = self.post(x)\n        \n        if targets is not None:\n            logits = self.arc_face(x, targets)\n            return logits\n        \n        return x","1979b821":"class EmbeddingNet(nn.Module):\n    def __init__(self, n_classes=100, embed_size=64, backbone_name=\"efficientnet_b0\"):\n        super(EmbeddingNet, self).__init__()\n        \n        self.embed_size = embed_size\n        self.backbone = timm.create_model(backbone_name, pretrained=False)\n        in_features = self.backbone.get_classifier().in_features\n\n        fc_name, _ = list(self.backbone.named_modules())[-1]\n        if fc_name == 'classifier':\n            self.backbone.classifier = nn.Identity()\n        elif fc_name == 'head.fc':\n            self.backbone.head.fc = nn.Identity()\n        elif fc_name == 'fc':\n            self.backbone.fc = nn.Identity()\n        else:\n            raise Exception(\"unknown classifier layer: \" + fc_name)\n        \n        self.post = nn.Sequential(\n            nn.utils.weight_norm(nn.Linear(in_features, self.embed_size*2), dim=None),\n            nn.BatchNorm1d(self.embed_size*2),\n            nn.Dropout(0.2),\n            nn.utils.weight_norm(nn.Linear(self.embed_size*2, self.embed_size)),\n        )\n\n        self.classifier = nn.Sequential(\n            nn.BatchNorm1d(self.embed_size),\n            nn.Dropout(0.2),\n            nn.Linear(self.embed_size, n_classes),\n        )\n        \n        print(f\"Model {backbone_name} EmbeddingNet - Features: {in_features}, Embeds: {self.embed_size}\")\n        \n    def embed_and_classify(self, x):\n        x = self.forward(x)\n        return x, self.classifier(x)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        x = x.view(x.size(0), -1)\n        x = self.post(x)\n        return x","c8437cfb":"from sklearn.metrics.pairwise import cosine_similarity\n\ndef get_embeds(loader, model, bar_desc=\"Generating embeds\"):\n    outputs_all = []\n    \n    model.eval()\n    with torch.no_grad():\n        t = tqdm(loader, desc=bar_desc)\n        for i, sample in enumerate(t):\n            input = sample['image'].to(args.device)\n            output = model(input)\n            outputs_all.extend(output.detach().cpu().numpy())\n#             outputs_all.extend(output.detach().cpu().numpy().astype(np.float16))\n            \n            \n    return outputs_all","9a3b3a38":"# def predict(loader, base_df, base_embeds, model, n_matches=5, bar_desc=\"Generating embeds\"):\n#     preds = []\n#     model.eval()\n#     with torch.no_grad():\n#         t = tqdm(loader, desc=bar_desc)\n#         for i, sample in enumerate(t):\n#             input = sample['image'].to(args.device)\n#             output = model(input)\n#             distances = cosine_similarity(output.detach().cpu().numpy(), base_embeds)\n            \n#             for j in range(0, output.shape[0]):\n#                 tmp_df = base_df.copy()\n#                 tmp_df[\"distance\"] = distances[j]\n#                 tmp_df = tmp_df.sort_values(by=[\"distance\", \"hotel_id\"], ascending=False).reset_index(drop=True)\n#                 preds.extend([tmp_df[\"hotel_id\"].unique()[:n_matches]])\n\n#     return preds\n\n# def find_closest_match_tta(args, test_df, tta_transforms, base_loader, model, n_matches=5):\n#     base_embeds = get_embeds(base_loader, model, \"Generating embeds for train\")\n\n#     test_dataset = HotelImageDataset(test_df, tta_transforms[\"base\"], data_folder=TEST_DATA_FOLDER)\n#     test_loader = DataLoader(test_dataset, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n#     preds = predict(test_loader, base_loader.dataset.data, base_embeds, model, n_matches, f\"Generating predictions\")\n        \n#     return preds","5ad47f3c":"def get_distances(input, base_embeds, model_array):\n    distances = None\n    for i, model in enumerate(model_array):\n        output = model(input)\n        output = output.detach().cpu().numpy()\n#         output = output.detach().cpu().numpy().astype(np.float16)\n        model_base_embeds = base_embeds[i]\n        output_distances = cosine_similarity(output, model_base_embeds)\n        \n        if distances is None:\n            distances = output_distances\n        else:\n            distances = distances * output_distances\n            \n    return distances\n    \n\ndef predict(loader, base_df, base_embeds, model_array, n_matches=5, bar_desc=\"Generating embeds\"):\n    preds = []\n    with torch.no_grad():\n        t = tqdm(loader, desc=bar_desc)\n        for i, sample in enumerate(t):\n            input = sample['image'].to(args.device)\n            distances = get_distances(input, base_embeds, model_array)\n            \n            for j in range(len(distances)):\n                tmp_df = base_df.copy()\n                tmp_df[\"distance\"] = distances[j]\n                tmp_df = tmp_df.sort_values(by=[\"distance\", \"hotel_id\"], ascending=False).reset_index(drop=True)\n                preds.extend([tmp_df[\"hotel_id\"].unique()[:n_matches]])\n\n    return preds\n\ndef find_closest_match(args, test_loader, base_loader, model_array, n_matches=5):\n    base_embeds = {}\n    for i, model in enumerate(model_array):\n        base_embeds[i] = get_embeds(base_loader, model, \"Generating embeds for train\")\n    \n    preds = predict(test_loader, base_loader.dataset.data, base_embeds, model_array, n_matches, f\"Generating predictions\")\n        \n    return preds","e5a36d11":"# data_df = pd.read_csv(PROJECT_FOLDER + \"train.csv\", parse_dates=[\"timestamp\"])\n# data_df = data_df[data_df[\"hotel_id\"].isin(data_df[\"hotel_id\"].unique()[-500:])]\n# test_df = data_df.groupby(\"hotel_id\").sample(1, random_state=SEED)\n# data_df = data_df[~data_df[\"image\"].isin(test_df[\"image\"])]\n\n# TEST_DATA_FOLDER = TRAIN_DATA_FOLDER\n\n# print(f\"Base: {len(data_df)}, test: {len(test_df)}\")","5589c4ea":"data_df = pd.read_csv(PROJECT_FOLDER + \"train.csv\", parse_dates=[\"timestamp\"])\nsample_submission_df = pd.read_csv(PROJECT_FOLDER + \"sample_submission.csv\")\ntest_df = pd.DataFrame(data={\"image\": os.listdir(TEST_DATA_FOLDER), \"hotel_id\": \"\"}).sort_values(by=\"image\")","5f855afb":"def get_model(model_type, backbone_name, embed_size, checkpoint_path, args):\n    if model_type == 'arcmargin':\n        model = HotelIdModel(7770, embed_size, backbone_name)\n    else:\n        model = EmbeddingNet(7770, embed_size, backbone_name)\n        \n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint[\"model\"])\n    model = model.to(args.device)\n    \n    return model","5c3a8d60":"class args:\n    batch_size = 32\n    num_workers = 4\n    n_classes = data_df[\"hotel_id\"].nunique()\n    device = ('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    \nseed_everything(seed=SEED)\n\nbase_dataset = HotelImageDataset(data_df, base_transform, data_folder=TRAIN_DATA_FOLDER)\nbase_loader = DataLoader(base_dataset, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n\ntest_dataset = HotelImageDataset(test_df, test_tta_transforms[\"base\"], data_folder=TEST_DATA_FOLDER)\ntest_loader = DataLoader(test_dataset, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)","eee60bf1":"model_array = [get_model(\"arcmargin\", \n                         \"efficientnet_b1\", 4096,\n                         \"..\/input\/hotelid-trained-models\/checkpoint-arcmargin-model-efficientnet_b1-512x512-4096embeds-7770hotels.pt\", \n                         args),\n               \n               get_model(\"cosface\", \n                         \"ecaresnet50d_pruned\", 4096,\n                         \"..\/input\/hotelid-trained-models\/checkpoint-cosface-model-ecaresnet50d_pruned-512x512-4096embeds-7770hotels.pt\", \n                         args),\n               \n               get_model(\"classification\", \n                         \"eca_nfnet_l0\", 4096,\n                         \"..\/input\/hotelid-trained-models\/checkpoint-classification-model-eca_nfnet_l0-512x512-4096embeds-7770hotels.pt\", \n                         args),\n               \n               get_model(\"arcmargin\", \n                         \"eca_nfnet_l0\", 1024,\n                         \"..\/input\/hotelid-trained-models\/checkpoint-arcmargin-model-eca_nfnet_l0-512x512-1024embeds-7770hotels.pt\", \n                         args),\n              ]","4587c6b1":"%%time\n\nif len(test_df) > 3:\n    preds = find_closest_match(args, test_loader, base_loader, model_array, n_matches=5)\n    test_df[\"hotel_id\"] = [str(list(l)).strip(\"[]\").replace(\",\", \"\") for l in preds]\n\ntest_df.to_csv(\"submission.csv\", index=False)\ntest_df.head()","ec02eaa0":"# preds = find_closest_match(args, test_loader, base_loader, model_array, n_matches=5)\n\n# test_df[\"hotel_id_pred\"] = [str(list(l)).strip(\"[]\").replace(\",\", \"\") for l in preds]\n\n# y = np.repeat([test_df[\"hotel_id\"]], repeats=5, axis=0).T\n# preds = np.array(preds)\n\n# acc_top_1 = (preds[:, 0] == test_df[\"hotel_id\"]).mean()\n# acc_top_5 = (preds == y).any(axis=1).mean()\n\n# print(f\"Accuracy: {acc_top_1:0.4f}, top 5 accuracy: {acc_top_5:0.4f}\")","0a7484ec":"# Model","aa6bcff7":"### for submission","6a5fff72":"# Setup","65cede96":"# Dataset and transformations","948148e2":"# Prepare data","06fe2512":"# Model helper functions","9b81c3af":"### submission","b043a541":"### for validation","ba1f91ae":"# Train and evaluate","b5477c3a":"# Global","b4284b1c":"### validation","d1eb76cd":"# Imports","62197923":"# Helper functions - seed and metric calculator"}}