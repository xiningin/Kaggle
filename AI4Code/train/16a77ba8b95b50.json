{"cell_type":{"be272f33":"code","2faf4b57":"code","bcac081e":"code","cd8955cc":"code","a62defb8":"code","3460fb2e":"code","2b4adde3":"code","dad519e3":"code","86dfaec7":"code","39c9e566":"code","5616cbc8":"code","ee1823b2":"code","409e0c18":"code","6bfbc99c":"code","4144e1e9":"code","5c1c6320":"code","ffc9168f":"code","51888097":"code","628a4d18":"code","18c8aa82":"code","79038adb":"code","774a9866":"code","2d1e4e67":"code","1dc34dea":"code","03b7cb4f":"code","421fecd6":"markdown"},"source":{"be272f33":"import numpy as np\nimport pandas as pd\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        \nfrom keras import backend as K\nfrom keras.preprocessing.image import load_img, save_img, img_to_array\nfrom keras.applications import vgg19\nfrom keras.models import Model\nimport keras\nfrom tensorflow.keras import models\nimport tensorflow.compat.v1 as tf\n\nfrom tqdm import tqdm\n\nimport time\n\nfrom scipy.optimize import fmin_l_bfgs_b\n\nimport matplotlib.pyplot as plt\n\nimport random","2faf4b57":"# tf.compat.v1.enable_eager_execution()\n# K.clear_session()\n# tf.compat.v1.reset_default_graph()\ntf.disable_v2_behavior()","bcac081e":"dirr_style = '..\/input\/best-artworks-of-all-time\/images\/images\/'\ndirr_photo = '..\/input\/image-classification\/validation\/validation\/travel and adventure\/'\n# dirr_photo = '..\/input\/image-classification\/images\/images\/travel and  adventure\/'\n\ndirr_folder = sorted(os.listdir('..\/input\/image-classification\/validation\/validation\/travel and adventure\/'))\n# dirr_folder = sorted(os.listdir('..\/input\/image-classification\/images\/images\/travel and  adventure\/'))\n\nnum = 4\nimage_path = os.path.join(dirr_photo, dirr_folder[num])\nstyle_path = dirr_style + 'Vincent_van_Gogh\/Vincent_van_Gogh_368.jpg'","cd8955cc":"image_path","a62defb8":"fig = plt.figure(figsize=(10, 8))\n\nfig.add_subplot(1, 2, 1)\nplt.title('\u041f\u0440\u0438\u043c\u0435\u0440 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f')\nim = load_img(image_path)\nplt.imshow(im)\n\nfig.add_subplot(1, 2, 2)\nplt.title('\u041f\u0440\u0438\u043c\u0435\u0440 \u0441\u0442\u0438\u043b\u044f')\nim = load_img(style_path)\nplt.imshow(im)\n\nplt.show()","3460fb2e":"width, height = load_img(image_path).size\n\nimg_height = 400\nimg_width = 500\n# img_width = int(width * img_height \/ height)\n(width, height), (img_width, img_height)","2b4adde3":"width \/ img_width, height \/ img_height","dad519e3":"def preprocess_image(model, image_path, rows, cols):\n    img = load_img(image_path, target_size=(rows, cols))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = model.preprocess_input(img)\n    return img\n\ndef deprocess_image(x):\n    # \u0446\u0435\u043d\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    # \u0432 RGB \u0438\u0437 BGR\n    x = x[:, :, ::-1]\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x","86dfaec7":"image = K.variable(preprocess_image(vgg19, image_path, img_height, img_width))\nstyle = K.variable(preprocess_image(vgg19, style_path, img_height, img_width))\ncombination_image = K.placeholder((1, img_height, img_width, 3))","39c9e566":"type(image), type(combination_image)","5616cbc8":"# \u043a\u043e\u043d\u043a\u0430\u0442\u0435\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u0442\u0435\u043d\u0437\u043e\u0440 \u0438\u0437 \u0442\u0440\u0451\u0445 \u0444\u043e\u0442\u043e\ninput_tensor = K.concatenate([image, style, combination_image], axis=0)\n\n# \u043c\u043e\u0434\u0435\u043b\u044c\nmodel = vgg19.VGG19(input_tensor=input_tensor, weights='imagenet', include_top=False)","ee1823b2":"combination_image","409e0c18":"def content_loss(base, combination):\n    return K.sum(K.square(combination - base))","6bfbc99c":"def gram_matrix(x):\n    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n    gram = K.dot(features, K.transpose(features))\n    return gram\n\n\ndef style_loss(style, combination):\n    S = gram_matrix(style)\n    C = gram_matrix(combination)\n    channels = 3\n    size = img_height * img_width\n    return K.sum(K.square(S - C)) \/ (4. * (channels ** 2) * (size ** 2))","4144e1e9":"def total_variation_loss(x):\n    a = K.square(\n        x[:, :img_height - 1, :img_width - 1, :] - x[:, 1:, :img_width - 1, :])\n    b = K.square(\n        x[:, :img_height - 1, :img_width - 1, :] - x[:, :img_height - 1, 1:, :])\n    return K.sum(K.pow(a + b, 1.25))","5c1c6320":"# \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0439 \u0441\u043b\u043e\u0451\u0432 \u0432 \u0442\u0435\u043d\u0437\u043e\u0440\u044b \noutputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n\n# \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0441\u043b\u043e\u044f \u0434\u043b\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u043c\u043e\u0433\u043e\ncontent_layer = 'block5_conv2'\n\n# \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u0441\u043b\u043e\u0451\u0432 \u0434\u043b\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c \u0441\u0442\u0438\u043b\u044f\nstyle_layers = [\n    'block1_conv1',\n    'block2_conv1',\n    'block3_conv1',\n    'block4_conv1',\n    'block5_conv1'\n]","ffc9168f":"outputs_dict","51888097":"lay = outputs_dict[content_layer]\nlay","628a4d18":"# \u0432\u0435\u0441\u0430 \u0434\u043b\u044f \u0432\u0437\u0432\u0435\u0448\u0435\u043d\u043d\u043e\u0439 \u0441\u0443\u043c\u043c\u044b \u0444\u0443\u043d\u043a\u0446\u0438\u0439 \u043f\u043e\u0442\u0435\u0440\u044c\ntotal_variation_weight = 1e-4\nstyle_weight = 1.\ncontent_weight = 0.025","18c8aa82":"# \u043f\u043e\u0442\u0435\u0440\u044f\nloss = K.variable(0.)\n\n# shape \u0441\u043b\u043e\u044f \u043f\u043e\u0442\u0435\u0440\u044c \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u043c\u043e\u0433\u043e\nlayer_features = outputs_dict[content_layer]\n\n# \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 - \u043f\u0435\u0440\u0432\u043e\u0435\ntarget_image_features = layer_features[0, :, :, :]\n\n# \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 (\u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u0445\u043e\u0442\u0438\u043c \u043f\u0440\u0438\u0431\u043b\u0438\u0437\u0438\u0442\u044c) - \u0442\u0440\u0435\u0442\u044c\u0435\ncombination_features = layer_features[2, :, :, :]\n\n# \u043f\u043e\u0442\u0435\u0440\u044f += \u043f\u043e\u0442\u0435\u0440\u044f \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u043c\u043e\u0433\u043e\nloss = loss + content_weight * content_loss(target_image_features, combination_features)\n# loss += content_weight * content_loss(target_image_features, combination_features)\n\n# \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u043b\u043e\u044f, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u043e\u0433\u043e \u0434\u043b\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c \u0441\u0442\u0438\u043b\u044f\nfor layer_name in style_layers:\n    # shape \u0441\u043b\u043e\u044f\n    layer_features = outputs_dict[layer_name]\n    # \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0441\u0442\u0438\u043b\u044f - \u0432\u0442\u043e\u0440\u043e\u0435\n    style_reference_features = layer_features[1, :, :, :]\n    # \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435\n    combination_features = layer_features[2, :, :, :]\n    # \u043f\u043e\u0442\u0435\u0440\u044f += \u043f\u043e\u0442\u0435\u0440\u044f \u0441\u0442\u0438\u043b\u044f \u0434\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0441\u043b\u043e\u044f\n    loss = loss + (style_weight \/ len(style_layers)) * style_loss(style_reference_features, combination_features)\n    \n# \u043f\u043e\u0442\u0435\u0440\u044f += \u043f\u043e\u0442\u0435\u0440\u044f \u0432\u0430\u0440\u0438\u0430\u0446\u0438\u0438 (\u0430\u043d\u0430\u043b\u043e\u0433 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c)\nloss = loss + total_variation_weight * total_variation_loss(combination_image)","79038adb":"combination_features","774a9866":"# \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u044b \u0441\u0433\u0435\u043d\u0435\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c\ngrads = K.gradients(loss, combination_image)[0]\n\n# \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c \u0438 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043e\u0432\nfetch_loss_and_grads = K.function([combination_image], [loss, grads])\n\n# \u043a\u043b\u0430\u0441\u0441, \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u044e\u0449\u0438\u0439 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c \u0438 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043e\u0432 \u043e\u0434\u043d\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e \u0434\u043b\u044f BFGS\nclass Evaluator(object):\n\n    def __init__(self):\n        self.loss_value = None\n        self.grads_values = None\n\n    def loss(self, x):\n        assert self.loss_value is None\n        x = x.reshape((1, img_height, img_width, 3))\n        outs = fetch_loss_and_grads([x])\n        loss_value = outs[0]\n        grad_values = outs[1].flatten().astype('float64')\n        self.loss_value = loss_value\n        self.grad_values = grad_values\n        return self.loss_value\n\n    def grads(self, x):\n        assert self.loss_value is not None\n        grad_values = np.copy(self.grad_values)\n        self.loss_value = None\n        self.grad_values = None\n        return grad_values\n\nevaluator = Evaluator()","2d1e4e67":"result_prefix = 'style'\nresult_prefix2 = 'origin'\niterations = 5\n\nx = preprocess_image(vgg19, image_path, img_height, img_width)\n# \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 (img_height, img_width)\nimg = deprocess_image(x[0].copy())\nfname = result_prefix2 + f'_it_{num}.png'\nsave_img(path=fname, x=img, data_format='channels_last')\n\nx = x.flatten()\n\nfor i in tqdm(range(iterations)):\n    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x, fprime=evaluator.grads, maxfun=20)\n\n# \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435\nimg = x.copy().reshape((img_height, img_width, 3))\nimg = deprocess_image(img)\nfname = result_prefix + f'_it_{num}.png'\nsave_img(path=fname, x=img, data_format='channels_last')","1dc34dea":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 8))\nlist_images = [\n    load_img(image_path, target_size=(img_height, img_width)),\n    load_img(style_path, target_size=(img_height, img_width)),\n    img\n]\n\nfor ax, im in zip(axes.flatten(), list_images):\n    ax.imshow(im)","03b7cb4f":"!zip -r files.zip .\/","421fecd6":"\u041f\u0440\u043e loss = loss + ...\n\nhttps:\/\/stackoverflow.com\/questions\/65705507\/variable-value-not-supported-problem-python"}}