{"cell_type":{"98859dab":"code","6fd4831b":"code","bc6b9c6b":"code","a19b8713":"code","3bc3c70a":"code","b8f56fea":"code","e70a1ac1":"code","542e62ad":"code","bd5342f4":"code","ba9d2dc5":"code","72cd50e1":"code","0d8d012d":"code","8994ddd9":"code","1b7ed16b":"code","778d569a":"code","5ced76c8":"code","ca9995ab":"code","49784617":"code","adb49639":"code","c46b7444":"code","17600d85":"code","13ce6544":"code","78706606":"code","01c9a4fe":"code","6a73a665":"code","53550525":"code","331d050c":"code","a327c006":"code","8a092a37":"code","e817fa1a":"code","13c9c0ab":"code","5f44161d":"code","d11b9e33":"code","2842beb3":"code","4e6dd627":"code","cd0d53bd":"code","56f8bbef":"code","37f3a6f3":"code","ddcc8f41":"code","7ff83b81":"code","c445b268":"code","791f6b0b":"code","31ee2a2e":"code","15370675":"code","a5fd9527":"code","b394b4c5":"code","94a07423":"code","98f4d95a":"code","24aa3462":"code","869574ff":"code","2ea51089":"code","3118c7bf":"code","38eb8ba4":"code","e983bbcf":"code","1abcdf63":"code","8e6e1c37":"code","b7792176":"code","a49a80bc":"code","61c49a61":"code","c77e72ff":"code","319c9c73":"code","b6df8a54":"code","5bfae6b2":"code","a9e5b6ad":"code","b9684352":"code","73afdcff":"code","d19ba230":"code","1ca2fca1":"code","7c3f6062":"code","79b6d62f":"code","1aad3c87":"code","848ac9ad":"code","dbfd20e7":"code","245fd24e":"code","eed80a5c":"code","a5783193":"code","0a1643cb":"code","6ba9a177":"code","e09858ea":"code","7adba441":"code","d17b18f0":"code","0935c1c8":"code","a8420c97":"code","39e983b2":"code","2a807edd":"code","859df5be":"code","7d38620c":"code","2b085fc0":"code","1c9f6126":"code","33978979":"code","328aa57f":"code","30cb7d5a":"code","94f0a893":"code","701de3c9":"code","e1cb237c":"markdown","a03e1519":"markdown","a676c878":"markdown","923a102d":"markdown","a6822661":"markdown","cda4c63e":"markdown","7734a4fe":"markdown","8c545298":"markdown","d10bb20e":"markdown","1012340f":"markdown","b100631e":"markdown","b791ce7b":"markdown","d35047c9":"markdown","1a155753":"markdown","19032cb0":"markdown","3fbfb6f6":"markdown","2a2ade45":"markdown","bb2b031b":"markdown","9c0626d9":"markdown"},"source":{"98859dab":"import numpy as np \nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(font_scale=1)","6fd4831b":"df_train=pd.read_csv('..\/input\/club-data-set\/club_churn_train.csv',encoding = \"ISO-8859-1\")","bc6b9c6b":"df_train.head()","a19b8713":"df_test=pd.read_csv('..\/input\/club-data-set\/club_churn_test.csv',encoding = \"ISO-8859-1\")","3bc3c70a":"def des_stat_analyze(df_input):\n    # check number of rows, cols\n    no_rows = df_input.shape[0]\n    no_cols = df_input.shape[1]\n    print(\"No. observations:\", no_rows )\n    print(\"No. features:\", no_cols )\n  \n    # checking type of features\n    name = []\n    cols_type = []\n    for n,t in df_input.dtypes.iteritems():\n        name.append(n)\n        cols_type.append(t)\n\n    # checking distinction (unique values) of features\n    ls_unique = []\n    for cname in df_input.columns:\n        try:\n            nunique = df_input[cname].nunique()\n            pct_unique = nunique*100.0\/ no_rows\n            ls_unique.append(\"{} ({:0.2f}%)\".format(nunique, pct_unique))\n        except:\n            ls_unique.append(\"{} ({:0.2f}%)\".format(0,0))\n            continue\n\n    # checking missing values of features\n    ls_miss = []\n    for cname in df_input.columns:\n        try:\n            nmiss = df_input[cname].isnull().sum()\n            pct_miss = nmiss*100.0\/ no_rows\n            ls_miss.append(\"{} ({:0.2f}%)\".format(nmiss, pct_miss))\n        except:\n            ls_miss.append(\"{} ({:0.2f}%)\".format(0,0))\n            continue \n      \n    # checking zeros\n    ls_zeros = []\n    for cname in df_input.columns:\n        try:\n            nzeros = (df_input[cname] == 0).sum()\n            pct_zeros = nzeros * 100.0\/ no_rows\n            ls_zeros.append(\"{} ({:0.2f}%)\".fornat(nzeros, pct_zeros))\n        except:\n            ls_zeros.append(\"{} ({:0.2f}%)\".format(0,0))\n            continue\n      \n    # checking negative values\n    ls_neg = []\n    for cname in df_input.columns:\n        try:\n            nneg = (df_input[cname].astype(\"float\")<0).sum()\n            pct_neg =nneg * 100.0 \/ no_rows\n            ls_neg.append(\"{} ({:0.2f}%)\".format(nneg, pct_neg))\n        except:\n            ls_neg.append(\"{} ({:0.2f}%)\".format(0,0))\n            continue\n      \n    # extracting the output\n    data = {\n      \"name\": name,\n      \"col_type\": cols_type,\n      \"n_unique\": ls_unique,\n      \"n_miss\": ls_miss,\n      \"n_zeros\":ls_zeros,\n      \"n_neg\":ls_neg      \n    }\n  \n    # statistical info\n    df_stats = df_input.describe().transpose()\n    ex_stats = pd.concat([df_input.median(),df_input.kurtosis(),df_input.skew()], axis = 1)\n    ex_stats = ex_stats.rename(columns={0:\"median\",1:\"kurtosis\",2:\"skew\"})\n    df_stats = df_stats.merge(ex_stats, left_index=True, right_index=True)\n    #ls_stats = []\n    for stat in df_stats.columns:\n        data[stat] = []\n        for cname in df_input.columns:\n            try:\n                data[stat].append(df_stats.loc[cname, stat])\n            except:\n                data[stat].append(\"NaN\")\n        \n    # take samples\n    df_sample = df_input.sample(frac = .5).head().transpose()\n    df_sample.columns = [\"sample_{}\".format(i) for i in range(5)]\n  \n    # repair the output\n    col_ordered = [\"name\",\"col_type\",\"count\",\"n_unique\",\"n_miss\",\"n_zeros\",\"n_neg\",\n                \"25%\",\"50%\",\"75%\",\"max\",\"min\",\"mean\",\"median\",\"std\",\"kurtosis\",\"skew\"]\n    df_data = pd.DataFrame(data, columns = col_ordered).set_index(\"name\")\n    df_data = pd.concat([df_data, df_sample], axis = 1)\n\n    return df_data.reset_index().sort_values(by=['col_type'])  ","b8f56fea":"des_stat_analyze(df_train)\n","e70a1ac1":"des_stat_analyze(df_test)","542e62ad":"df_train['MEMBERSHIP_STATUS_new'] = df_train.MEMBERSHIP_STATUS.astype('category').cat.codes","bd5342f4":"df_train['MEMBER_AGE_AT_ISSUE_bin']=pd.cut(x=df_train['MEMBER_AGE_AT_ISSUE'],bins=[0,28,39,50,54,65,92])\ndf_test['MEMBER_AGE_AT_ISSUE_bin']=pd.cut(x=df_test['MEMBER_AGE_AT_ISSUE'],bins=[0,28,39,50,54,65,92])","ba9d2dc5":"df_train['MEMBERSHIP_TERM_YEARS_bin']=pd.cut(x=df_train['MEMBERSHIP_TERM_YEARS'],bins=[0,12,20,50,100])\ndf_test['MEMBERSHIP_TERM_YEARS_bin']=pd.cut(x=df_test['MEMBERSHIP_TERM_YEARS'],bins=[0,12,20,50,100])","72cd50e1":"labels = ['Poor','Medium','Rich','VeryRich']\ndf_train['MEMBER_ANNUAL_INCOME_bin'] = pd.cut(df_train['MEMBER_ANNUAL_INCOME'], bins=[9995.999,290000,1000000,18000000,999999996], labels=labels, right=False)\nlabels = ['Poor','Medium','Rich','VeryRich']\ndf_test['MEMBER_ANNUAL_INCOME_bin'] = pd.cut(df_test['MEMBER_ANNUAL_INCOME'], bins=[9995.999,290000,1000000,18000000,999999996], labels=labels, right=False)","0d8d012d":"labels = ['VeryLow','Medium','High','VeryHigh']\ndf_train['ANNUAL_FEES_bin'] = pd.cut(df_train['ANNUAL_FEES'], bins=[99999.989,125000,200000,500000,10100000], labels=labels, right=False)\ndf_test['ANNUAL_FEES_bin'] = pd.cut(df_test['ANNUAL_FEES'], bins=[99999.989,125000,200000,500000,10100000], labels=labels, right=False)","8994ddd9":"print(\"Training set - MEMBERSHIP_TERM_YEARS\")\ntrain_MEMBERSHIP_TERM_YEARS = df_train[['MEMBERSHIP_TERM_YEARS_bin','MEMBERSHIP_NUMBER']].groupby(['MEMBERSHIP_TERM_YEARS_bin']).count()\ntrain_MEMBERSHIP_TERM_YEARS = train_MEMBERSHIP_TERM_YEARS \/ train_MEMBERSHIP_TERM_YEARS.sum()\ntrain_MEMBERSHIP_TERM_YEARS.columns = ['Proportions']\nprint(train_MEMBERSHIP_TERM_YEARS.round(2))\n\nprint(\"Testing set - MEMBERSHIP_TERM_YEARS\")\ntest_MEMBERSHIP_TERM_YEARS = df_test[['MEMBERSHIP_TERM_YEARS_bin','MEMBERSHIP_NUMBER']].groupby(['MEMBERSHIP_TERM_YEARS_bin']).count()\ntest_MEMBERSHIP_TERM_YEARS = test_MEMBERSHIP_TERM_YEARS \/ test_MEMBERSHIP_TERM_YEARS.sum()\ntest_MEMBERSHIP_TERM_YEARS.columns = ['Proportions']\nprint(test_MEMBERSHIP_TERM_YEARS.round(2))","1b7ed16b":"print(\"Training set - MEMBER_AGE_AT_ISSUE\")\ntrain_MEMBER_AGE_AT_ISSUE = df_train[['MEMBER_AGE_AT_ISSUE_bin','MEMBERSHIP_NUMBER']].groupby(['MEMBER_AGE_AT_ISSUE_bin']).count()\ntrain_MEMBER_AGE_AT_ISSUE = train_MEMBER_AGE_AT_ISSUE \/ train_MEMBER_AGE_AT_ISSUE.sum()\ntrain_MEMBER_AGE_AT_ISSUE.columns = ['Proportions']\nprint(train_MEMBER_AGE_AT_ISSUE.round(2))\n\nprint(\"Testing set - MEMBER_AGE_AT_ISSUE\")\ntest_MEMBER_AGE_AT_ISSUE = df_test[['MEMBER_AGE_AT_ISSUE_bin','MEMBERSHIP_NUMBER']].groupby(['MEMBER_AGE_AT_ISSUE_bin']).count()\ntest_MEMBER_AGE_AT_ISSUE = test_MEMBER_AGE_AT_ISSUE \/ test_MEMBER_AGE_AT_ISSUE.sum()\ntest_MEMBER_AGE_AT_ISSUE.columns = ['Proportions']\nprint(test_MEMBER_AGE_AT_ISSUE.round(2))","778d569a":"print(\"Training set - ADDITIONAL_MEMBERS\")\ntrain_ADDITIONAL_MEMBERS = df_train[['ADDITIONAL_MEMBERS','MEMBERSHIP_NUMBER']].groupby(['ADDITIONAL_MEMBERS']).count()\ntrain_ADDITIONAL_MEMBERS = train_ADDITIONAL_MEMBERS \/ train_ADDITIONAL_MEMBERS.sum()\ntrain_ADDITIONAL_MEMBERS.columns = ['Proportions']\nprint(train_ADDITIONAL_MEMBERS.round(2))\n\nprint(\"Testing set - ADDITIONAL_MEMBERS\")\ntest_ADDITIONAL_MEMBERS = df_test[['ADDITIONAL_MEMBERS','MEMBERSHIP_NUMBER']].groupby(['ADDITIONAL_MEMBERS']).count()\ntest_ADDITIONAL_MEMBERS = test_ADDITIONAL_MEMBERS \/ test_ADDITIONAL_MEMBERS.sum()\ntest_ADDITIONAL_MEMBERS.columns = ['Proportions']\nprint(test_ADDITIONAL_MEMBERS.round(2))","5ced76c8":"print(\"Training set - ANNUAL_FEES\")\ntrain_ANNUAL_FEES_bin = df_train[['ANNUAL_FEES_bin','MEMBERSHIP_NUMBER']].groupby(['ANNUAL_FEES_bin']).count()\ntrain_ANNUAL_FEES_bin = train_ANNUAL_FEES_bin \/ train_ANNUAL_FEES_bin.sum()\ntrain_ANNUAL_FEES_bin.columns = ['Proportions']\nprint(train_ANNUAL_FEES_bin.round(2))\n\nprint(\"Testing set - ANNUAL_FEES\")\ntest_ANNUAL_FEES_bin = df_test[['ANNUAL_FEES_bin','MEMBERSHIP_NUMBER']].groupby(['ANNUAL_FEES_bin']).count()\ntest_ANNUAL_FEES_bin = test_ANNUAL_FEES_bin \/ test_ANNUAL_FEES_bin.sum()\ntest_ANNUAL_FEES_bin.columns = ['Proportions']\nprint(test_ANNUAL_FEES_bin.round(2))","ca9995ab":"print(\"Training set - MEMBER_ANNUAL_INCOME\")\ntrain_MEMBER_ANNUAL_INCOME_bin = df_train[['MEMBER_ANNUAL_INCOME_bin','MEMBERSHIP_NUMBER']].groupby(['MEMBER_ANNUAL_INCOME_bin']).count()\ntrain_MEMBER_ANNUAL_INCOME_bin = train_MEMBER_ANNUAL_INCOME_bin \/ train_MEMBER_ANNUAL_INCOME_bin.sum()\ntrain_MEMBER_ANNUAL_INCOME_bin.columns = ['Proportions']\nprint(train_MEMBER_ANNUAL_INCOME_bin.round(4))\n\nprint(\"Testing set - MEMBER_ANNUAL_INCOME\")\ntest_MEMBER_ANNUAL_INCOME_bin = df_test[['MEMBER_ANNUAL_INCOME_bin','MEMBERSHIP_NUMBER']].groupby(['MEMBER_ANNUAL_INCOME_bin']).count()\ntest_MEMBER_ANNUAL_INCOME_bin = test_MEMBER_ANNUAL_INCOME_bin \/ test_MEMBER_ANNUAL_INCOME_bin.sum()\ntest_MEMBER_ANNUAL_INCOME_bin.columns = ['Proportions']\nprint(test_MEMBER_ANNUAL_INCOME_bin.round(4))","49784617":"print(\"Training set - MEMBER_OCCUPATION_CD\")\ntrain_MEMBER_OCCUPATION_CD = df_train[['MEMBER_OCCUPATION_CD','MEMBERSHIP_NUMBER']].groupby(['MEMBER_OCCUPATION_CD']).count()\ntrain_MEMBER_OCCUPATION_CD = train_MEMBER_OCCUPATION_CD \/ train_MEMBER_OCCUPATION_CD.sum()\ntrain_MEMBER_OCCUPATION_CD.columns = ['Proportions']\nprint(train_MEMBER_OCCUPATION_CD.round(4))\n\nprint(\"Testing set - MEMBER_OCCUPATION_CD\")\ntest_MEMBER_OCCUPATION_CD = df_test[['MEMBER_OCCUPATION_CD','MEMBERSHIP_NUMBER']].groupby(['MEMBER_OCCUPATION_CD']).count()\ntest_MEMBER_OCCUPATION_CD = test_MEMBER_OCCUPATION_CD \/ test_MEMBER_OCCUPATION_CD.sum()\ntest_MEMBER_OCCUPATION_CD.columns = ['Proportions']\nprint(test_MEMBER_OCCUPATION_CD.round(4))","adb49639":"print(\"Training set - MEMBER_MARITAL_STATUS\")\ntrain_MEMBER_MARITAL_STATUS = df_train[['MEMBER_MARITAL_STATUS','MEMBERSHIP_NUMBER']].groupby(['MEMBER_MARITAL_STATUS']).count()\ntrain_MEMBER_MARITAL_STATUS = train_MEMBER_MARITAL_STATUS \/ train_MEMBER_MARITAL_STATUS.sum()\ntrain_MEMBER_MARITAL_STATUS.columns = ['Proportions']\nprint(train_MEMBER_MARITAL_STATUS.round(4))\n\nprint(\"Testing set - MEMBER_MARITAL_STATUS\")\ntest_MEMBER_MARITAL_STATUS = df_test[['MEMBER_MARITAL_STATUS','MEMBERSHIP_NUMBER']].groupby(['MEMBER_MARITAL_STATUS']).count()\ntest_MEMBER_MARITAL_STATUS = test_MEMBER_MARITAL_STATUS \/ test_MEMBER_MARITAL_STATUS.sum()\ntest_MEMBER_MARITAL_STATUS.columns = ['Proportions']\nprint(test_MEMBER_MARITAL_STATUS.round(4))","c46b7444":"print(\"Training set - MEMBER_GENDER\")\ntrain_MEMBER_GENDER = df_train[['MEMBER_GENDER','MEMBERSHIP_NUMBER']].groupby(['MEMBER_GENDER']).count()\ntrain_MEMBER_GENDER = train_MEMBER_GENDER \/ train_MEMBER_GENDER.sum()\ntrain_MEMBER_GENDER.columns = ['Proportions']\nprint(train_MEMBER_GENDER.round(4))\n\nprint(\"Testing set - MEMBER_GENDER\")\ntest_MEMBER_GENDER = df_test[['MEMBER_GENDER','MEMBERSHIP_NUMBER']].groupby(['MEMBER_GENDER']).count()\ntest_MEMBER_GENDER = test_MEMBER_GENDER \/ test_MEMBER_GENDER.sum()\ntest_MEMBER_GENDER.columns = ['Proportions']\nprint(test_MEMBER_GENDER.round(4))","17600d85":"print(\"Training set - MEMBERSHIP_PACKAGE\")\ntrain_MEMBERSHIP_PACKAGE = df_train[['MEMBERSHIP_PACKAGE','MEMBERSHIP_NUMBER']].groupby(['MEMBERSHIP_PACKAGE']).count()\ntrain_MEMBERSHIP_PACKAGE = train_MEMBERSHIP_PACKAGE \/ train_MEMBERSHIP_PACKAGE.sum()\ntrain_MEMBERSHIP_PACKAGE.columns = ['Proportions']\nprint(train_MEMBERSHIP_PACKAGE.round(4))\n\nprint(\"Testing set - MEMBERSHIP_PACKAGE\")\ntest_MEMBERSHIP_PACKAGE = df_test[['MEMBERSHIP_PACKAGE','MEMBERSHIP_NUMBER']].groupby(['MEMBERSHIP_PACKAGE']).count()\ntest_MEMBERSHIP_PACKAGE = test_MEMBERSHIP_PACKAGE \/ test_MEMBERSHIP_PACKAGE.sum()\ntest_MEMBERSHIP_PACKAGE.columns = ['Proportions']\nprint(test_MEMBERSHIP_PACKAGE.round(4))","13ce6544":"print(\"Training set - PAYMENT_MODE\")\ntrain_PAYMENT_MODE = df_train[['PAYMENT_MODE','MEMBERSHIP_NUMBER']].groupby(['PAYMENT_MODE']).count()\ntrain_PAYMENT_MODE = train_PAYMENT_MODE \/ train_PAYMENT_MODE.sum()\ntrain_PAYMENT_MODE.columns = ['Proportions']\nprint(train_PAYMENT_MODE.round(4))\n\nprint(\"Testing set - PAYMENT_MODE\")\ntest_PAYMENT_MODE = df_test[['PAYMENT_MODE','MEMBERSHIP_NUMBER']].groupby(['PAYMENT_MODE']).count()\ntest_PAYMENT_MODE = test_PAYMENT_MODE \/ test_PAYMENT_MODE.sum()\ntest_PAYMENT_MODE.columns = ['Proportions']\nprint(test_PAYMENT_MODE.round(4))","78706606":"print(\"Training set - MEMBERSHIP_STATUS\")\ntrain_MEMBERSHIP_STATUS = df_train[['MEMBERSHIP_STATUS','MEMBERSHIP_NUMBER']].groupby(['MEMBERSHIP_STATUS']).count()\ntrain_MEMBERSHIP_STATUS = train_MEMBERSHIP_STATUS \/ train_MEMBERSHIP_STATUS.sum()\ntrain_MEMBERSHIP_STATUS.columns = ['Proportions']\nprint(train_MEMBERSHIP_STATUS.round(4))\n","01c9a4fe":"df_train.corr()","6a73a665":"df_train = df_train.rename({'MEMBER_ANNUAL_INCOME_bin': 'Income', 'ANNUAL_FEES_bin': 'Fee', 'MEMBERSHIP_STATUS_new':'Status','MEMBER_OCCUPATION_CD':'Occupation','MEMBER_GENDER':'Gender','ADDITIONAL_MEMBERS':'Additional','PAYMENT_MODE':'Mode'}, axis=1)","53550525":"from matplotlib.pyplot import show\nax = sns.countplot(x=\"Status\", hue=\"Additional\", data=df_train)\ntotal = float(len(df_train)) # one person per row \nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/total),\n            ha=\"center\") \nshow()\n\"\"\"\nLess additional less cancelled\n\"\"\"","331d050c":"from matplotlib.pyplot import show\nax = sns.countplot(x=\"Status\", hue=\"MEMBER_MARITAL_STATUS\", data=df_train)\ntotal = float(len(df_train)) # one person per row \nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/total),\n            ha=\"center\") \nshow()\n\"\"\"\nMarried people much more 'cancelled'\n\"\"\"","a327c006":"from matplotlib.pyplot import show\nax = sns.countplot(x=\"Status\", hue=\"Gender\", data=df_train)\ntotal = float(len(df_train)) # one person per row \nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/total),\n            ha=\"center\") \nshow()\n\"\"\"\nMale much more 'cancelled'\n\"\"\"\n","8a092a37":"\nfrom matplotlib.pyplot import show\nax = sns.countplot(x=\"Status\", hue=\"MEMBERSHIP_PACKAGE\", data=df_train)\ntotal = float(len(df_train)) # one person per row \nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/total),\n            ha=\"center\") \nshow()\n\n\"\"\"\nType A less 'Cancelled' in percentage compared to TypeB\n\"\"\"","e817fa1a":"df_train.columns","13c9c0ab":"sourceCount = df_train.groupby(['MEMBERSHIP_PACKAGE','Income','Fee','Status'])['MEMBERSHIP_NUMBER'].count()\nprint(sourceCount)\n\"\"\"\nThe fees are customized for every member's personal package not logically\n\"\"\"","5f44161d":"\"\"\"\nAnnual payment mode is more 'cancelled, think about the promotion for this mode'\n\"\"\"\nfrom matplotlib.pyplot import show\nax = sns.countplot(x=\"Status\", hue=\"Mode\", data=df_train)\ntotal = float(len(df_train)) # one person per row \nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/total),\n            ha=\"center\") \nshow()","d11b9e33":"\"\"\"\nCustomers mostly from 30 above\n\"\"\"\nfrom matplotlib.pyplot import show\nax = sns.countplot(x=\"Status\", hue=\"MEMBER_AGE_AT_ISSUE_bin\", data=df_train)\ntotal = float(len(df_train)) # one person per row \nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/total),\n            ha=\"center\") \nshow()","2842beb3":"\"\"\"\nMedium income dominated the customers of the club- the highest rate of cancelled\n\"\"\"\nfrom matplotlib.pyplot import show\nax = sns.countplot(x=\"Status\", hue=\"Income\", data=df_train)\ntotal = float(len(df_train)) # one person per row \nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/total),\n            ha=\"center\") \nshow()","4e6dd627":"# g = sns.FacetGrid(df_train, row=\"Income\", col=\"Fee\")\n# g=(g.map(sns.distplot, \"Status\",color=\"r\")\n#       .set(xlim=(0, 40), ylim=(0, 6),\n#            xticks=[10, 30, 50], yticks=[2, 6, 10]))","cd0d53bd":"# g = sns.FacetGrid(df_train, row=\"Gender\", col=\"Income\")\n# g=(g.map(sns.distplot, \"Status\",color=\"r\")\n#       .set(xlim=(0, 40), ylim=(0, 6),\n#            xticks=[10, 30, 50], yticks=[2, 6, 10]))","56f8bbef":"# g = sns.FacetGrid(df_train, row=\"Additional\", col=\"Mode\")\n# g=(g.map(sns.distplot, \"Status\",color=\"r\")\n#       .set(xlim=(0, 40), ylim=(0, 6),\n#            xticks=[10, 30, 50], yticks=[2, 6, 10]))","37f3a6f3":"df_train1=pd.read_csv('..\/input\/club-data-set\/club_churn_train.csv',encoding = \"ISO-8859-1\")","ddcc8f41":"df_test1=pd.read_csv('..\/input\/club-data-set\/club_churn_test.csv',encoding = \"ISO-8859-1\")","7ff83b81":"df_train1.info()","c445b268":"agent_cancelled = df_train1.loc[df_train1.MEMBERSHIP_STATUS=='CANCELLED','AGENT_CODE']","791f6b0b":"df_train1['agent_cancelled'] = agent_cancelled","31ee2a2e":"count_member_by_agent=df_train1.groupby(['agent_cancelled'])['AGENT_CODE'].count()","15370675":"count_member_by_agent = pd.DataFrame(count_member_by_agent)","a5fd9527":"count_member_by_agent=count_member_by_agent.reset_index()","b394b4c5":"count_member_by_agent=count_member_by_agent.rename(columns={\"AGENT_CODE\": \"total_cancelled_by_agents\", \"agent_cancelled\": \"AGENT_CODE\"})","94a07423":"count_member_by_agent = pd.DataFrame(count_member_by_agent)\ndf_train_2 = pd.merge(df_train1, count_member_by_agent, how='left', on='AGENT_CODE')\ndf_train_2.head()","98f4d95a":"df_test1['AGENT_CODE']=df_test1['AGENT_CODE'].apply(str)","24aa3462":"df_test_2 = pd.merge(df_test1, count_member_by_agent, how='left', on='AGENT_CODE')\ndf_test_2.head()","869574ff":"df_train_2.info()","2ea51089":"df_test_2.info()","3118c7bf":"df_train_2['Type_data'] = 'Train'\ndf_test_2['Type_data'] = 'Test'","38eb8ba4":"def concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set on axis 0\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)","e983bbcf":"df_all = concat_df(df_train_2, df_test_2)","1abcdf63":"print(df_all.isnull().sum())","8e6e1c37":"df_all['total_cancelled_by_agents'] = df_all['total_cancelled_by_agents'].fillna(0)","b7792176":"df_all['MEMBER_ANNUAL_INCOME'] = df_all['MEMBER_ANNUAL_INCOME'].fillna(np.nanmedian(df_all['MEMBER_ANNUAL_INCOME']))\ndf_all['MEMBER_GENDER'] = df_all['MEMBER_GENDER'].fillna(df_all['MEMBER_GENDER'].mode()[0])\ndf_all['MEMBER_MARITAL_STATUS'] = df_all['MEMBER_MARITAL_STATUS'].fillna(df_all['MEMBER_MARITAL_STATUS'].mode()[0])\ndf_all['MEMBER_OCCUPATION_CD'] = df_all['MEMBER_OCCUPATION_CD'].fillna(df_all['MEMBER_OCCUPATION_CD'].mode()[0])","a49a80bc":"df_all.info()","61c49a61":"df_train3 = df_all.loc[df_all.Type_data=='Train']\ndf_test3 = df_all.loc[df_all.Type_data=='Test']","c77e72ff":"df_train3=df_train3.reset_index()\ndf_test3=df_test3.reset_index()","319c9c73":"df_train_final = df_train3.drop(['index','AGENT_CODE','END_DATE','MEMBERSHIP_NUMBER','START_DATE','Type_data','agent_cancelled'],axis=1)","b6df8a54":"df_train_final.info()","5bfae6b2":"df_train_final['MEMBERSHIP_PACKAGE'] = df_train_final.MEMBERSHIP_PACKAGE.astype('category').cat.codes\ndf_train_final['MEMBERSHIP_STATUS'] = df_train_final.MEMBERSHIP_STATUS.astype('category').cat.codes\ndf_train_final['MEMBER_GENDER'] = df_train_final.MEMBER_GENDER.astype('category').cat.codes\ndf_train_final['MEMBER_MARITAL_STATUS'] = df_train_final.MEMBER_MARITAL_STATUS.astype('category').cat.codes\ndf_train_final['MEMBER_OCCUPATION_CD'] = df_train_final.MEMBER_OCCUPATION_CD.astype('category').cat.codes\ndf_train_final['PAYMENT_MODE'] = df_train_final.PAYMENT_MODE.astype('category').cat.codes","a9e5b6ad":"df_train_final['MEMBERSHIP_STATUS'].value_counts()","b9684352":"df_train_final['ANNUAL_FEES']=df_train_final['ANNUAL_FEES'].astype(int)\ndf_train_final['MEMBER_ANNUAL_INCOME']=df_train_final['MEMBER_ANNUAL_INCOME'].astype(int)\ndf_train_final['total_cancelled_by_agents']=df_train_final['total_cancelled_by_agents'].astype(int)","73afdcff":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt","d19ba230":"df_train_final.columns","1ca2fca1":"from sklearn.model_selection import train_test_split\ny=df_train_final['MEMBERSHIP_STATUS']\nX= df_train_final[['ADDITIONAL_MEMBERS', 'ANNUAL_FEES', 'MEMBERSHIP_PACKAGE',\n       'MEMBERSHIP_TERM_YEARS', 'MEMBER_AGE_AT_ISSUE',\n       'MEMBER_ANNUAL_INCOME', 'MEMBER_GENDER']]\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25 , random_state=0)","7c3f6062":"# X_test\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'min_samples_leaf': [1,3],\n    'min_samples_split': [3,5,7],\n    'n_estimators': [100, 150, 200, 250]\n}\nrf = GridSearchCV(estimator = RandomForestClassifier(n_estimators=150, min_samples_split= 2,n_jobs=-1,verbose=1),\n                  param_grid = param_grid, \n                  cv = 3, n_jobs = -1, verbose = 2)","79b6d62f":"%%time\nrf.fit(x_train,y_train)","1aad3c87":"from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,roc_auc_score,roc_curve,auc,recall_score\npred_prob=rf.predict_proba(x_test)\npred_var=rf.predict(x_test)\n\nprecision_score(pred_var,y_test),recall_score(pred_var,y_test),accuracy_score(pred_var,y_test)","848ac9ad":"from sklearn.model_selection import train_test_split\ny=df_train_final['MEMBERSHIP_STATUS']\nX= df_train_final.drop(['MEMBERSHIP_STATUS'],axis=1)\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25 , random_state=0)","dbfd20e7":"X.info()","245fd24e":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt","eed80a5c":"# X_test\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'min_samples_leaf': [1,3],\n    'min_samples_split': [3,5,7],\n    'n_estimators': [100, 150, 200, 250]\n}\nrf = GridSearchCV(estimator = RandomForestClassifier(n_estimators=150, min_samples_split= 2,n_jobs=-1,verbose=1),\n                  param_grid = param_grid, \n                  cv = 3, n_jobs = -1, verbose = 2)","a5783193":"%%time\nrf.fit(x_train,y_train)","0a1643cb":"from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,roc_auc_score,roc_curve,auc,recall_score\n","6ba9a177":"pred_prob=rf.predict_proba(x_test)\npred_var=rf.predict(x_test)","e09858ea":"print(pd.DataFrame(confusion_matrix(y_test, pred_var, labels=[1,0]), index=['true:1', 'true:0'], columns=['pred:1', 'pred:0']))","7adba441":"precision_score(pred_var,y_test),recall_score(pred_var,y_test),accuracy_score(pred_var,y_test)","d17b18f0":"labels = df_train_final['MEMBERSHIP_STATUS']\nfeatures = df_train_final.drop('MEMBERSHIP_STATUS', axis=1)\n# List of features for later use\nfeature_list = list(features.columns)\n\n# Get numerical feature importances\nimportances = list(rf.best_estimator_.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n\n\nimport matplotlib.pyplot as plt\n\nfeat_importances = pd.Series(rf.best_estimator_.feature_importances_, index=feature_list)\nplt.ylabel('Features')\nplt.xlabel('Importance---->')\nplt.title('Feature Importance plot')\nfeat_importances.nlargest(9).plot(kind='barh', color=['orange', 'red', 'green', 'blue', 'cyan','yellow'])","0935c1c8":"from sklearn.externals import joblib \n  \n# Save the model as a pickle in a file \njoblib.dump(rf, 'rf_model_01Mar_test.pkl') \n  \n# Load the model from the file \nrf_from_joblib = joblib.load('rf_model_01Mar_test.pkl')  ","a8420c97":"rf","39e983b2":"from sklearn.model_selection import KFold\n\nkf = KFold(n_splits=10)\noutcomes = []\n    \nfold = 0\nfor train_index, test_index in kf.split(X):\n    fold += 1\n    X_train, X_test = X.values[train_index], X.values[test_index]\n    y_train, y_test = y.values[train_index], y.values[test_index]\n    rf.fit(X_train, y_train)\n    predictions = rf.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    outcomes.append(accuracy)\n    print(\"Fold {0} accuracy: {1}\".format(fold, accuracy))     \nmean_outcome = np.mean(outcomes)\nprint(\"\\n\\nMean Accuracy: {0}\".format(mean_outcome)) ","2a807edd":"df_test_final = df_test3.drop(['MEMBERSHIP_STATUS','index','AGENT_CODE','END_DATE','MEMBERSHIP_NUMBER','START_DATE','Type_data','agent_cancelled'],axis=1)","859df5be":"df_test_final.info()","7d38620c":"df_test_final['MEMBERSHIP_PACKAGE'] = df_test_final.MEMBERSHIP_PACKAGE.astype('category').cat.codes\ndf_test_final['MEMBER_GENDER'] = df_test_final.MEMBER_GENDER.astype('category').cat.codes\ndf_test_final['MEMBER_MARITAL_STATUS'] = df_test_final.MEMBER_MARITAL_STATUS.astype('category').cat.codes\ndf_test_final['MEMBER_OCCUPATION_CD'] = df_test_final.MEMBER_OCCUPATION_CD.astype('category').cat.codes\ndf_test_final['PAYMENT_MODE'] = df_test_final.PAYMENT_MODE.astype('category').cat.codes","2b085fc0":"pred2=rf_from_joblib.predict(df_test_final)    \nprobs_=rf_from_joblib.predict_proba(df_test_final)","1c9f6126":"df=pd.DataFrame()\ndf['p']=probs_[:,1]\ndf['pred2'] =df['p'].map(lambda x: 'CANCELLED' if x > 0.9 else 'INFORCE')","33978979":"df['MEMBERSHIP_NUMBER']=df_test3['MEMBERSHIP_NUMBER']","328aa57f":"df.info()","30cb7d5a":"df.head()","94f0a893":"df_sub = pd.merge(df_test3, df, how='inner', on='MEMBERSHIP_NUMBER')\ndf_sub.head()","701de3c9":"df.to_csv(r'C:\\Users\\347997\\Desktop\\sub.csv')","e1cb237c":"* **MEMBER_MARITAL_STATUS, MEMBER_GENDER, MEMBER_ANNUAL_INCOME, MEMBER_OCCUPATION_CD has missing values.**\nwe will see how to fill missing values next.","a03e1519":"# 6. LAST MODEL","a676c878":"Create 2 more variables:\n\n*Create diff features by using Enddate-Startdate\n\n*Calculate total cancelled by agents\n","923a102d":"# 5. BASE_LINE MODEL","a6822661":"# MODEL","cda4c63e":"*Timing varibales correlation with label\n","7734a4fe":"* **MEMBER_MARITAL_STATUS, MEMBER_GENDER, MEMBER_ANNUAL_INCOME, MEMBER_OCCUPATION_CD has missing values.**\nwe will see how to fill missing values next.","8c545298":"# SOLUTION:\n* Check again the package price offer suitable for customer with income poor, medium, rich, very rich\n* Offer promotion for annual package to retain customers\n* Implment survey for group of customer with 30 years-50 years old for checking their statisfaction\n* Checking agent servicing for group of agents with the most canceled members.","d10bb20e":"# ENCODING","1012340f":"*1: Enforeced\n 0: Cancelled","b100631e":"# 4. Feature_Engineering","b791ce7b":"## ** Loading the Dataset**\n* Training set has **9324** rows and test set has **1036** rows.\n* Training set have **15** features and test set have **13** features.\n* One extra feature in the training set is `MEMBERSHIP_STATUS` & `END_DATE`  feature","d35047c9":"# 2. Data Quality Assessment\n>> ### **0.3. CHECKING DISTRIBUTION BETWEEN TRAIN & TEST**\n>*The distribution between train and test set is the same","1a155753":" # **0. Introduction** - Use Case\n\n>A certain premium club boasts a large customer membership. The members pay an annual membership fee in return for using the exclusive facilities offered by this club. The fees are customized for every member's personal package. \n>\n>In the last few years, however, the club has been facing an issue with a lot of members cancelling their memberships.\n>\n>The club management plans to address this issue by proactively addressing customer grievances. They, however, do not have enough bandwidth to reach out to the entire customer base individually and are looking to see whether a statistical approach can help them identify customers at risk.\n>\n>Can you help them ? Relevant historical data is provided in the \u201cclub_churn_train.csv\u201d\n\n","19032cb0":"# PREDICTION","3fbfb6f6":"# **1. Data Dictionary** - Data Set\n\n*MEMBERSHIP_TERM_YEARS: duration of member services\n\n*MEMBER_AGE_AT_ISSUE: age of members at the time enforced or cancelled\n\n*ADDITIONAL_MEMBERS: add-in members \n\n*START_DATE: the beginning date of members using club services\n\n*ANNUAL_FEES: club services annual fee\n\n*MEMBER_ANNUAL_INCOME: annual income of members\n\n*MEMBER_OCCUPATION_CD: Occupation of members\n\n*END_DATE: the end date of members using club services\n\n*MEMBERSHIP_NUMBER: Member ID (unique)\n\n*MEMBER_MARITAL_STATUS: Member marriage status including: Divorced, Married, Single, Widowed\n\n*MEMBER_GENDER: Member gender\n\n*MEMBERSHIP_PACKAGE: Member package club\n\n*PAYMENT_MODE: Annual, semi-annual, monthly, quarterly, single-premium\n\n*AGENT_CODE: Agent ID service the member\n\n*MEMBERSHIP_STATUS: Cancelled, Enforced (Label)","2a2ade45":"*The distribution between train and test set for each features is the same","bb2b031b":">> ### **0.1. Libraries**","9c0626d9":"# **3. Data Exploration - Visualization**"}}