{"cell_type":{"c3df44d5":"code","01484729":"code","51f6bef2":"code","fd3ca69c":"code","3eca230f":"code","df9a5453":"code","d43bb9b9":"code","778e9e64":"code","b6942f07":"code","34e78770":"code","77769800":"code","7d1e62de":"code","6f8094b0":"code","41142416":"code","ef4ae02c":"code","1d87a07e":"code","288d71b9":"code","1fd331f8":"code","55ed72ec":"code","60855127":"code","3b0ba187":"markdown","b2437e20":"markdown","547f49bf":"markdown","ab722fc8":"markdown","583f320f":"markdown","12bb8235":"markdown","a084e810":"markdown","456bb09c":"markdown","d5905ef1":"markdown","133c323d":"markdown","c09faf2b":"markdown","2c014d26":"markdown","82701d0c":"markdown","0d11cea8":"markdown"},"source":{"c3df44d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","01484729":"# we need feature_engine library for categorical encoding\n!pip install feature_engine","51f6bef2":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom collections import Counter\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\n# for the model \nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import (\n    GradientBoostingClassifier,\n    RandomForestClassifier)\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.pipeline import Pipeline\n\n# for feature engineering\nfrom feature_engine import encoding as ce\n\n# for ensemble sampling\nfrom imblearn.ensemble import EasyEnsembleClassifier","fd3ca69c":"data = pd.read_csv('\/kaggle\/input\/bank-marketing-data-set\/bank-additional-full.csv',\n                  delimiter=';')\nprint(data.shape)\ndata.head()","3eca230f":"# we have imbalanced data\ndata['y'].value_counts() \/ len(data['y'])","df9a5453":"data.drop('duration', axis=1, inplace=True)\ndata.shape","d43bb9b9":"## create age_group\n# let's divide age into the buckets \nbuckets = [16, 30, 40, 50, 60, 100]\n\n# bucket labels\nlabels = ['17-30', '31-40', '41-50', '51-60', '>60']\n\n# discretization\ndata['age_group'] = pd.cut(data['age'], bins=buckets, labels=labels, include_lowest=True)\ndata['age_group'] = data['age_group'].astype('O') # change dtype\n\n# drop the original age column\ndata.drop('age', axis=1, inplace=True)","778e9e64":"## create pdays_group\n# pdays has 999 values which mean that \n# client was not previous contacted. \n# I will change 999 to -1 \ndata['pdays'] = data['pdays'].replace(999, -1)\n\n# let's divide pdays into 3 groups\nbins = [0, 7, 14, 30]\n\n# labels\nlabels = ['1w', '2w', '>2w']\n\ndata['pdays_group'] = pd.cut(data['pdays'], bins=bins, labels=labels, \n                             include_lowest=False) # False makes -1 value to missing values\n\n# fill missing values to Not contacted\ndata['pdays_group'] = data['pdays_group'].astype('O')\ndata['pdays_group'].fillna('Not contacted', inplace=True)\n\n# drop the original column\ndata.drop('pdays', axis=1, inplace=True)","b6942f07":"# change binary value of y to numerical values\ndata['y'] = data['y'].map({'no':0, 'yes': 1})","34e78770":"# Create lists for categorical and numerical variables\n\ncat_vars = [var for var in data.columns if var != 'y' and data[var].dtype=='O']\nnum_vars = [var for var in data.columns if var != 'y' and data[var].dtype!='O']\n\nprint('Number of Categorical variables: {}'.format(len(cat_vars)))\nprint('Number of Numerical variables: {}'.format(len(num_vars)))","77769800":"# separate train and test set\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop('y', axis=1),\n    data['y'],\n    test_size=0.3,\n    random_state=0\n)\n\nX_train.shape, X_test.shape","7d1e62de":"# using OrdinalEncoder from feature-engine\n\nordinal_enc = ce.OrdinalEncoder(\n    encoding_method='arbitrary',\n    variables=cat_vars)\n\nordinal_enc.fit(X_train) ","6f8094b0":"X_train = ordinal_enc.transform(X_train)\nX_test = ordinal_enc.transform(X_test)","41142416":"# set up scaler\nscaler = StandardScaler()\n\n# fit the scaler to the train set, it will learn the parameters\nscaler.fit(X_train)\n\n# transform train and test set\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","ef4ae02c":"def run_RFs(X_train, X_test, y_train, y_test):\n    \"\"\" Function to evaluate random forests performance\"\"\"\n    rf = RandomForestClassifier(n_estimators=20)\n    rf.fit(X_train, y_train)\n    \n    print('Train set')\n    pred = rf.predict_proba(X_train)\n    print('RF roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n    \n    print('Test set')\n    pred = rf.predict_proba(X_test)\n    print('RF roc-auc: {}'.format(roc_auc_score(y_test, pred[:, 1])))","1d87a07e":"def run_easy(X_train, X_test, y_train, y_test):\n    \"\"\"Function to evaluate EasyEnsemble performance\"\"\"\n    easy = EasyEnsembleClassifier(\n        n_estimators=20,\n        sampling_strategy='auto', \n        n_jobs=1,\n        random_state=42)\n    \n    easy.fit(X_train, y_train)\n    \n    print('Train set')\n    pred = easy.predict_proba(X_train)\n    print('EasyEnsemble roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n    \n    print('Test set')\n    pred = easy.predict_proba(X_test)\n    print('EasyEnsemble roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","288d71b9":"# base model for RF\nrun_RFs(X_train, X_test, y_train, y_test)","1fd331f8":"# base model\nrun_easy(X_train, X_test, y_train, y_test)","55ed72ec":"# Default estimator for easyensemble is AdaBoost\n# however, I found gbm shows better performance for this dataset\n# so, I will use gbm as base_estimator \n\n# set up gbm\ngbm = GradientBoostingClassifier()\n\n# set up classifier\neasy = EasyEnsembleClassifier(n_estimators=98,\n                             sampling_strategy='auto',\n                             base_estimator=gbm,\n                             n_jobs=1,\n                             random_state=42)\neasy.fit(X_train, y_train)","60855127":"X_train_preds = easy.predict_proba(X_train)[:, 1]\nX_test_preds = easy.predict_proba(X_test)[:, 1]\n\nprint('Train roc-auc: ', roc_auc_score(y_train, X_train_preds))\nprint('Test roc-auc: ', roc_auc_score(y_test, X_test_preds))","3b0ba187":"## Building a model\n\nEasyEnsemble involves creating balanced samples of the training datset by selecting all examples from the minority class and a subset from the majority class. Rather than using pruned decision trees, boosted decision trees are used on each subset, specifically the AdaBoost algorithm.\n\nThe EasyEnsembleClassifier class from the imbalanced-learn library provides an implmentation of the easy ensemble technique. I will use EasyEnsembleClassifier in this notebook, because it shows the best performance. I compared other balancing sampling techniques such over- and under-sampling techniques, so if you want to see more detail, please see [Bank_Marketing_BalancingSampling.ipynb](https:\/\/github.com\/yejiseoung\/BankMarketing\/blob\/main\/Bank_Marketing_BalancingSampling.ipynb)\n\nWe can choose the classifiers to build the best model, so I found the best hyperparameters and estimators for this problem. I will skip this process because it takes a long time to find best hyperparatmers. You can see the entire process [Bank_Marketing_Model.ipynb](https:\/\/github.com\/yejiseoung\/BankMarketing\/blob\/main\/Bank_Marketing_Model.ipynb)","b2437e20":"### Separate train and test data","547f49bf":"Here, I will drop `duration` and create `age_group` and `pdays_group` by using `age` and `pdays` variables, and drop the original variables. ","ab722fc8":"### Import","583f320f":"### Load data","12bb8235":"Random forest shows over-fitting, and we can solve this problem by tuning the hyperparamters, but I won't do it here because I wanted to create base model. ","a084e810":"## Data Clean","456bb09c":"### Feature Engineering","d5905ef1":"### Introduction\n\n__The main goal of this project is to predict if the client will subscribe a term deposit (variable y).__ I will show how to approach developing a model to predict client's subscriptions using an imbalanced dataset. \n\nThe dataset is provided by [UCI-Machine Learning Repository](https:\/\/archive.ics.uci.edu\/ml\/datasets\/bank+marketing). The data contains direct marketing campaigns (phone calls) of a Portuguese banking institution. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be 'yes' or 'no' subscribed. \n\n\n### Feature Engineering\nThe original dataset has 10 numerical variables (9 continuous and 1 discrete), 10 categorical variables. I created 2 categorical variables `age_group` and `pdays_group` by using `age` and `pdays`, and removed the original variables. \nAlso, I removed `duration` variable due to the given information: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\nSo, the final dataset has 11 numerical variables and 8 categorical variables. \n\nBefore building a model, we need to encode categorical variables as numerical values. So, I will use integer encoding by using feature engine package. Also, I will use StandardScaler from sklearn to scale all numerical variables. \n\nI'll show the feature engineering process in this notebook with less detail, so if you want to see more, please take a look at [Bank_Marketing_EDA.ipynb](https:\/\/github.com\/yejiseoung\/BankMarketing\/blob\/develop\/Bank_Marketing_EDA.ipynb) and [Bank_Marketing_FeatureEngineering.ipynb](https:\/\/github.com\/yejiseoung\/BankMarketing\/blob\/develop\/Bank_Marketing_FeatureEngineering.ipynb) in my Github. Or, you can find [Simple EDA, Feature Engineering, and Visualization](https:\/\/www.kaggle.com\/yejiseoung\/simple-eda-feature-engineering-and-visualization) in Kaggle. You can find the visualizations and more detail in those notebooks. \n\n\n\n\n### Imbalanced data\nWe have a classification problem. The target is a binary variable (no-yes), and it shows 89% of no and 11% of yes responses. This dataset is a typical imbalanced data which means that datasets have many more instances of certain classes (no - 89%) than of others (yes - 11%). Since most machine learning algorithms assume balanced distributions, samples from the minority class (in this case, yes label) are likely misclassified. So, we need to deal with this very carefully. Through the notebook, I will implement EasyEnsemble technique which is an ensemble algorithms that were designed to work with imbalanced datasets. \n\nYou might wonder why I won't use under- or over-sampling techniques. I tried all possible techinques to balance imbalanced dataset and none of under- or over-sampling techinques did not improve the model performance. You can find the whole process in [Bank_Marketing_BalancingSampling.ipynb](https:\/\/github.com\/yejiseoung\/BankMarketing\/blob\/develop\/Bank_Marketing_BalancingSampling.ipynb)\n\n\n- __Reference__: [Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014\n\n","133c323d":"By tuning hyperparameters of EasyEnsembleClassifier, we got slightly improved performance of the model.","c09faf2b":"Before building a model, we need to engineer categorical variables and scale numerical variables. \n\n\nYou can see more detail in [Bank_Marketing_EDA.ipynb](https:\/\/github.com\/yejiseoung\/BankMarketing\/blob\/develop\/Bank_Marketing_EDA.ipynb) and [Bank_Marketing_FeatureEngineering.ipynb](https:\/\/github.com\/yejiseoung\/BankMarketing\/blob\/develop\/Bank_Marketing_FeatureEngineering.ipynb) in my Github. Or, you can find [Simple EDA, Feature Engineering, and Visualization](https:\/\/www.kaggle.com\/yejiseoung\/simple-eda-feature-engineering-and-visualization) in Kaggle. ","2c014d26":"## Feature Scaling - StandardScaler","82701d0c":"# Bank Marketing Data - Building EasyEnsemble Model","0d11cea8":"### Encoding Categorical variables"}}