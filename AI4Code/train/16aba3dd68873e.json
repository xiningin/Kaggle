{"cell_type":{"2152f06d":"code","1d897eba":"code","a0063bb0":"code","ec2c9e3e":"code","10da9180":"code","5ee13fc4":"code","fe5450c6":"code","5fe63d4c":"code","f8d0842c":"code","4a129779":"code","9dbc7c9c":"code","29ad6eab":"code","53b8501c":"code","16cb473c":"code","63465dbc":"code","2cb1ff1f":"code","caa0f8a3":"code","79afe167":"code","e44732ed":"code","3f10f1f5":"code","07920247":"code","d69bf094":"code","f2a563d1":"code","0d0220cf":"code","b5b5a53f":"code","d8479f62":"code","db89356f":"code","c307c4fb":"code","312e9098":"code","2592e991":"code","ba48a314":"code","0aadba60":"code","2dc8108d":"code","b79216c4":"code","5fad8db8":"code","9a8a5b9a":"code","077bb9be":"code","c763fe6c":"code","58ea05c0":"code","58111a89":"code","9f91fc57":"code","0f9ebf81":"code","2f3d3954":"code","349046a5":"code","bf7b9292":"code","b16e037e":"code","072693fd":"code","b1be605a":"code","f577239d":"code","f0a7a364":"code","d613b2c6":"code","6816679b":"code","2cf942d8":"code","3934006d":"markdown","856207f5":"markdown"},"source":{"2152f06d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1d897eba":"#Loading libraries \nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 8.0)\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm","a0063bb0":"#loading data\ntrain = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","ec2c9e3e":"train.head()","10da9180":"train.shape","5ee13fc4":"print ('The train data has {0} rows and {1} columns'.format(train.shape[0],train.shape[1]))\nprint ('----------------------------')\nprint ('The test data has {0} rows and {1} columns'.format(test.shape[0],test.shape[1]))","fe5450c6":"train.info()","5fe63d4c":"#missing value counts in each of these columns \nmiss = train.isnull().sum()\/len(train)\nmiss = miss[miss > 0]\nmiss.sort_values(inplace=True)\nmiss","f8d0842c":"#visualising missing values\nmiss = miss.to_frame()\nmiss.columns = ['count']\nmiss.index.names = ['Name']\nmiss['Name'] = miss.index","4a129779":"#plot the missing value count\nsns.set(style=\"whitegrid\", color_codes=True)\nsns.barplot(x = 'Name', y = 'count', data=miss)\nplt.xticks(rotation = 90)\nplt.show()","9dbc7c9c":"#SalePrice\nsns.distplot(train['SalePrice'])\n","29ad6eab":"#now transforming the target variable\ntarget = np.log(train['SalePrice'])\nprint ('Skewness is', target.skew())\nsns.distplot(target)","53b8501c":"#separate variables into new data frames\nnumeric_data = train.select_dtypes(include=[np.number])\ncat_data = train.select_dtypes(exclude=[np.number])\nprint (\"There are {} numeric and {} categorical columns in train data\".format(numeric_data.shape[1],cat_data.shape[1]))","16cb473c":"del numeric_data['Id']","63465dbc":"#correlation plot\ncorr = numeric_data.corr()\nsns.heatmap(corr)","2cb1ff1f":"print (corr['SalePrice'].sort_values(ascending=False)[:15], '\\n') #top 15 values\nprint ('----------------------')\nprint (corr['SalePrice'].sort_values(ascending=False)[-5:]) #last 5 values`\n","caa0f8a3":"train['OverallQual'].unique()","79afe167":"#let's check the mean price per quality and plot it.\npivot = train.pivot_table(index='OverallQual', values='SalePrice', aggfunc=np.median)\npivot.sort_values","e44732ed":"pivot.plot(kind='bar', color='green')\n","3f10f1f5":"#GrLivArea variable\nsns.jointplot(x=train['GrLivArea'], y=train['SalePrice'])","07920247":"cat_data.describe()","d69bf094":"sp_pivot = train.pivot_table(index='SaleCondition', values='SalePrice', aggfunc=np.median)\nsp_pivot","f2a563d1":"sp_pivot.plot(kind='bar',color='green')","0d0220cf":"import warnings\nwarnings.filterwarnings('ignore')\n\ncat = [f for f in train.columns if train.dtypes[f] == 'object']\ndef anova(frame):\n    anv = pd.DataFrame()\n    anv['features'] = cat\n    pvals = []\n    for c in cat:\n           samples = []\n           for cls in frame[c].unique():\n                  s = frame[frame[c] == cls]['SalePrice'].values\n                  samples.append(s)\n           pval = stats.f_oneway(*samples)[1]\n           pvals.append(pval)\n    anv['pval'] = pvals\n    return anv.sort_values('pval')\n\ncat_data['SalePrice'] = train.SalePrice.values\nk = anova(cat_data) \nk['disparity'] = np.log(1.\/k['pval'].values) \nsns.barplot(data=k, x = 'features', y='disparity') \nplt.xticks(rotation=90) \nplt ","b5b5a53f":"#create numeric plots\nnum = [f for f in train.columns if train.dtypes[f] != 'object']\nnum.remove('Id')\nnd = pd.melt(train, value_vars = num)\nn1 = sns.FacetGrid (nd, col='variable', col_wrap=4, sharex=False, sharey = False)\nn1 = n1.map(sns.distplot, 'value')\nn1\n","d8479f62":"def boxplot(x,y,**kwargs):\n            sns.boxplot(x=x,y=y)\n            x = plt.xticks(rotation=90)\n\ncat = [f for f in train.columns if train.dtypes[f] == 'object']\n\np = pd.melt(train, id_vars='SalePrice', value_vars=cat)\nM = sns.FacetGrid (p, col='variable', col_wrap=2, sharex=False, sharey=False, size=5)\nM = M.map(boxplot, 'value','SalePrice')\nM","db89356f":"#removing outliers\ntrain.drop(train[train['GrLivArea'] > 4000].index, inplace=True)\ntrain.shape ","c307c4fb":"#imputing using mode\ntest.loc[666, 'GarageQual'] = \"TA\" #stats.mode(test['GarageQual']).mode\ntest.loc[666, 'GarageCond'] = \"TA\" #stats.mode(test['GarageCond']).mode\ntest.loc[666, 'GarageFinish'] = \"Unf\" #stats.mode(test['GarageFinish']).mode\ntest.loc[666, 'GarageYrBlt'] = \"1980\" #np.nanmedian(test['GarageYrBlt'])` ","312e9098":"#mark as missing\ntest.loc[1116, 'GarageType'] = np.nan","2592e991":"#importing function\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndef factorize(data, var, fill_na = None):\n      if fill_na is not None:\n            data[var].fillna(fill_na, inplace=True)\n      le.fit(data[var])\n      data[var] = le.transform(data[var])\n      return data","ba48a314":"#combine the data set\nalldata = train.append(test)\nalldata.shape","0aadba60":"#impute lotfrontage by median of neighborhood\nlot_frontage_by_neighborhood = train['LotFrontage'].groupby(train['Neighborhood'])\n\nfor key, group in lot_frontage_by_neighborhood:\n                idx = (alldata['Neighborhood'] == key) & (alldata['LotFrontage'].isnull())\n                alldata.loc[idx, 'LotFrontage'] = group.median()","2dc8108d":"#imputing missing values\nalldata[\"MasVnrArea\"].fillna(0, inplace=True)\nalldata[\"BsmtFinSF1\"].fillna(0, inplace=True)\nalldata[\"BsmtFinSF2\"].fillna(0, inplace=True)\nalldata[\"BsmtUnfSF\"].fillna(0, inplace=True)\nalldata[\"TotalBsmtSF\"].fillna(0, inplace=True)\nalldata[\"GarageArea\"].fillna(0, inplace=True)\nalldata[\"BsmtFullBath\"].fillna(0, inplace=True)\nalldata[\"BsmtHalfBath\"].fillna(0, inplace=True)\nalldata[\"GarageCars\"].fillna(0, inplace=True)\nalldata[\"GarageYrBlt\"].fillna(0.0, inplace=True)\nalldata[\"PoolArea\"].fillna(0, inplace=True)","b79216c4":"qual_dict = {np.nan: 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\nname = np.array(['ExterQual','PoolQC' ,'ExterCond','BsmtQual','BsmtCond','HeatingQC','KitchenQual','FireplaceQu', 'GarageQual','GarageCond'])\n\nfor i in name:\n     alldata[i] = alldata[i].map(qual_dict).astype(int)\n\nalldata[\"BsmtExposure\"] = alldata[\"BsmtExposure\"].map({np.nan: 0, \"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4}).astype(int)\n\nbsmt_fin_dict = {np.nan: 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\nalldata[\"BsmtFinType1\"] = alldata[\"BsmtFinType1\"].map(bsmt_fin_dict).astype(int)\nalldata[\"BsmtFinType2\"] = alldata[\"BsmtFinType2\"].map(bsmt_fin_dict).astype(int)\nalldata[\"Functional\"] = alldata[\"Functional\"].map({np.nan: 0, \"Sal\": 1, \"Sev\": 2, \"Maj2\": 3, \"Maj1\": 4, \"Mod\": 5, \"Min2\": 6, \"Min1\": 7, \"Typ\": 8}).astype(int)\n\nalldata[\"GarageFinish\"] = alldata[\"GarageFinish\"].map({np.nan: 0, \"Unf\": 1, \"RFn\": 2, \"Fin\": 3}).astype(int)\nalldata[\"Fence\"] = alldata[\"Fence\"].map({np.nan: 0, \"MnWw\": 1, \"GdWo\": 2, \"MnPrv\": 3, \"GdPrv\": 4}).astype(int)\n\n#encoding data\nalldata[\"CentralAir\"] = (alldata[\"CentralAir\"] == \"Y\") * 1.0\nvarst = np.array(['MSSubClass','LotConfig','Neighborhood','Condition1','BldgType','HouseStyle','RoofStyle','Foundation','SaleCondition'])\n\nfor x in varst:\n         factorize(alldata, x)\n\n#encode variables and impute missing values\nalldata = factorize(alldata, \"MSZoning\", \"RL\")\nalldata = factorize(alldata, \"Exterior1st\", \"Other\")\nalldata = factorize(alldata, \"Exterior2nd\", \"Other\")\nalldata = factorize(alldata, \"MasVnrType\", \"None\")\nalldata = factorize(alldata, \"SaleType\", \"Oth\")  ","5fad8db8":"#creating new variable (1 or 0) based on irregular count levels\n#The level with highest count is kept as 1 and rest as 0\nalldata[\"IsRegularLotShape\"] = (alldata[\"LotShape\"] == \"Reg\") * 1\nalldata[\"IsLandLevel\"] = (alldata[\"LandContour\"] == \"Lvl\") * 1\nalldata[\"IsLandSlopeGentle\"] = (alldata[\"LandSlope\"] == \"Gtl\") * 1\nalldata[\"IsElectricalSBrkr\"] = (alldata[\"Electrical\"] == \"SBrkr\") * 1\nalldata[\"IsGarageDetached\"] = (alldata[\"GarageType\"] == \"Detchd\") * 1\nalldata[\"IsPavedDrive\"] = (alldata[\"PavedDrive\"] == \"Y\") * 1\nalldata[\"HasShed\"] = (alldata[\"MiscFeature\"] == \"Shed\") * 1\nalldata[\"Remodeled\"] = (alldata[\"YearRemodAdd\"] != alldata[\"YearBuilt\"]) * 1\n\n#Did the modeling happen during the sale year?\nalldata[\"RecentRemodel\"] = (alldata[\"YearRemodAdd\"] == alldata[\"YrSold\"]) * 1\n\n# Was this house sold in the year it was built?\nalldata[\"VeryNewHouse\"] = (alldata[\"YearBuilt\"] == alldata[\"YrSold\"]) * 1\nalldata[\"Has2ndFloor\"] = (alldata[\"2ndFlrSF\"] == 0) * 1\nalldata[\"HasMasVnr\"] = (alldata[\"MasVnrArea\"] == 0) * 1\nalldata[\"HasWoodDeck\"] = (alldata[\"WoodDeckSF\"] == 0) * 1\nalldata[\"HasOpenPorch\"] = (alldata[\"OpenPorchSF\"] == 0) * 1\nalldata[\"HasEnclosedPorch\"] = (alldata[\"EnclosedPorch\"] == 0) * 1\nalldata[\"Has3SsnPorch\"] = (alldata[\"3SsnPorch\"] == 0) * 1\nalldata[\"HasScreenPorch\"] = (alldata[\"ScreenPorch\"] == 0) * 1\n\n#setting levels with high count as 1 and the rest as 0\n#you can check for them using the value_counts function\nalldata[\"HighSeason\"] = alldata[\"MoSold\"].replace({1: 0, 2: 0, 3: 0, 4: 1, 5: 1, 6: 1, 7: 1, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0})\nalldata[\"NewerDwelling\"] = alldata[\"MSSubClass\"].replace({20: 1, 30: 0, 40: 0, 45: 0,50: 0, 60: 1, 70: 0, 75: 0, 80: 0, 85: 0,90: 0, 120: 1, 150: 0, 160: 0, 180: 0, 190: 0})","9a8a5b9a":"alldata.shape","077bb9be":"#create alldata2\nalldata2 = train.append(test)\n\nalldata[\"SaleCondition_PriceDown\"] = alldata2.SaleCondition.replace({'Abnorml': 1, 'Alloca': 1, 'AdjLand': 1, 'Family': 1, 'Normal': 0, 'Partial': 0})\n\n# house completed before sale or not\nalldata[\"BoughtOffPlan\"] = alldata2.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})\nalldata[\"BadHeating\"] = alldata2.HeatingQC.replace({'Ex': 0, 'Gd': 0, 'TA': 0, 'Fa': 1, 'Po': 1})","c763fe6c":"#calculating total area using all area columns\narea_cols = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF','OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'LowQualFinSF', 'PoolArea' ]\n\nalldata[\"TotalArea\"] = alldata[area_cols].sum(axis=1)\nalldata[\"TotalArea1st2nd\"] = alldata[\"1stFlrSF\"] + alldata[\"2ndFlrSF\"]\nalldata[\"Age\"] = 2010 - alldata[\"YearBuilt\"]\nalldata[\"TimeSinceSold\"] = 2010 - alldata[\"YrSold\"]\nalldata[\"SeasonSold\"] = alldata[\"MoSold\"].map({12:0, 1:0, 2:0, 3:1, 4:1, 5:1, 6:2, 7:2, 8:2, 9:3, 10:3, 11:3}).astype(int)\nalldata[\"YearsSinceRemodel\"] = alldata[\"YrSold\"] - alldata[\"YearRemodAdd\"]\n\n# Simplifications of existing features into bad\/average\/good based on counts\nalldata[\"SimplOverallQual\"] = alldata.OverallQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2, 7 : 3, 8 : 3, 9 : 3, 10 : 3})\nalldata[\"SimplOverallCond\"] = alldata.OverallCond.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2, 7 : 3, 8 : 3, 9 : 3, 10 : 3})\nalldata[\"SimplPoolQC\"] = alldata.PoolQC.replace({1 : 1, 2 : 1, 3 : 2, 4 : 2})\nalldata[\"SimplGarageCond\"] = alldata.GarageCond.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplGarageQual\"] = alldata.GarageQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplFireplaceQu\"] = alldata.FireplaceQu.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplFireplaceQu\"] = alldata.FireplaceQu.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplFunctional\"] = alldata.Functional.replace({1 : 1, 2 : 1, 3 : 2, 4 : 2, 5 : 3, 6 : 3, 7 : 3, 8 : 4})\nalldata[\"SimplKitchenQual\"] = alldata.KitchenQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplHeatingQC\"] = alldata.HeatingQC.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplBsmtFinType1\"] = alldata.BsmtFinType1.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2})\nalldata[\"SimplBsmtFinType2\"] = alldata.BsmtFinType2.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2})\nalldata[\"SimplBsmtCond\"] = alldata.BsmtCond.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplBsmtQual\"] = alldata.BsmtQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplExterCond\"] = alldata.ExterCond.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplExterQual\"] = alldata.ExterQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n\n#grouping neighborhood variable based on this plot\ntrain['SalePrice'].groupby(train['Neighborhood']).median().sort_values().plot(kind='bar')","58ea05c0":"neighborhood_map = {\"MeadowV\" : 0, \"IDOTRR\" : 1, \"BrDale\" : 1, \"OldTown\" : 1, \"Edwards\" : 1, \"BrkSide\" : 1,` ` \"Sawyer\" : 1, \"Blueste\" : 1, \"SWISU\" : 2, \"NAmes\" : 2, \"NPkVill\" : 2, \"Mitchel\" : 2, \"SawyerW\" : 2, \"Gilbert\" : 2, \"NWAmes\" : 2, \"Blmngtn\" : 2, \"CollgCr\" : 2, \"ClearCr\" : 3,  \"Veenker\" : 3, \"Somerst\" : 3, \"Timber\" : 3, \"StoneBr\" : 4, \"NoRidge\" : 4, \"NridgHt\" : 4}\n\nalldata['NeighborhoodBin'] = alldata2['Neighborhood'].map(neighborhood_map)\nalldata.loc[alldata2.Neighborhood == 'NridgHt', \"Neighborhood_Good\"] = 1\nalldata.loc[alldata2.Neighborhood == 'Crawfor', \"Neighborhood_Good\"] = 1\nalldata.loc[alldata2.Neighborhood == 'StoneBr', \"Neighborhood_Good\"] = 1\nalldata.loc[alldata2.Neighborhood == 'Somerst', \"Neighborhood_Good\"] = 1\nalldata.loc[alldata2.Neighborhood == 'NoRidge', \"Neighborhood_Good\"] = 1\nalldata[\"Neighborhood_Good\"].fillna(0, inplace=True)\nalldata[\"SaleCondition_PriceDown\"] = alldata2.SaleCondition.replace({'Abnorml': 1, 'Alloca': 1, 'AdjLand': 1, 'Family': 1, 'Normal': 0, 'Partial': 0})\n\n# House completed before sale or not\nalldata[\"BoughtOffPlan\"] = alldata2.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})\nalldata[\"BadHeating\"] = alldata2.HeatingQC.replace({'Ex': 0, 'Gd': 0, 'TA': 0, 'Fa': 1, 'Po': 1})\nalldata.shape","58111a89":"#create new data\ntrain_new = alldata[alldata['SalePrice'].notnull()]\ntest_new = alldata[alldata['SalePrice'].isnull()]\n","9f91fc57":"#get numeric features\nnumeric_features = [f for f in train_new.columns if train_new[f].dtype != object]\n\n#transform the numeric features using log(x + 1)\nfrom scipy.stats import skew\nskewed = train_new[numeric_features].apply(lambda x: skew(x.dropna().astype(float)))\nskewed = skewed[skewed > 0.75]\nskewed = skewed.index\ntrain_new[skewed] = np.log1p(train_new[skewed])\ntest_new[skewed] = np.log1p(test_new[skewed])\ndel test_new['SalePrice']","0f9ebf81":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(train_new[numeric_features])\nscaled = scaler.transform(train_new[numeric_features])\n\nfor i, col in enumerate(numeric_features):\n       train_new[col] = scaled[:,i]\n\nnumeric_features.remove('SalePrice')\nscaled = scaler.fit_transform(test_new[numeric_features])\n\nfor i, col in enumerate(numeric_features):\n      test_new[col] = scaled[:,i]","2f3d3954":"def onehot(onehot_df, df, column_name, fill_na):\n       onehot_df[column_name] = df[column_name]\n       if fill_na is not None:\n            onehot_df[column_name].fillna(fill_na, inplace=True)\n\n       dummies = pd.get_dummies(onehot_df[column_name], prefix=\"_\"+column_name)\n       onehot_df = onehot_df.join(dummies)\n       onehot_df = onehot_df.drop([column_name], axis=1)\n       return onehot_df\n\ndef munge_onehot(df):\n       onehot_df = pd.DataFrame(index = df.index)\n\n       onehot_df = onehot(onehot_df, df, \"MSSubClass\", None)\n       onehot_df = onehot(onehot_df, df, \"MSZoning\", \"RL\")\n       onehot_df = onehot(onehot_df, df, \"LotConfig\", None)\n       onehot_df = onehot(onehot_df, df, \"Neighborhood\", None)\n       onehot_df = onehot(onehot_df, df, \"Condition1\", None)\n       onehot_df = onehot(onehot_df, df, \"BldgType\", None)\n       onehot_df = onehot(onehot_df, df, \"HouseStyle\", None)\n       onehot_df = onehot(onehot_df, df, \"RoofStyle\", None)\n       onehot_df = onehot(onehot_df, df, \"Exterior1st\", \"VinylSd\")\n       onehot_df = onehot(onehot_df, df, \"Exterior2nd\", \"VinylSd\")\n       onehot_df = onehot(onehot_df, df, \"Foundation\", None)\n       onehot_df = onehot(onehot_df, df, \"SaleType\", \"WD\")\n       onehot_df = onehot(onehot_df, df, \"SaleCondition\", \"Normal\")\n\n       #Fill in missing MasVnrType for rows that do have a MasVnrArea.\n       temp_df = df[[\"MasVnrType\", \"MasVnrArea\"]].copy()\n       idx = (df[\"MasVnrArea\"] != 0) & ((df[\"MasVnrType\"] == \"None\") | (df[\"MasVnrType\"].isnull()))\n       temp_df.loc[idx, \"MasVnrType\"] = \"BrkFace\"\n       onehot_df = onehot(onehot_df, temp_df, \"MasVnrType\", \"None\")\n\n       onehot_df = onehot(onehot_df, df, \"LotShape\", None)\n       onehot_df = onehot(onehot_df, df, \"LandContour\", None)\n       onehot_df = onehot(onehot_df, df, \"LandSlope\", None)\n       onehot_df = onehot(onehot_df, df, \"Electrical\", \"SBrkr\")\n       onehot_df = onehot(onehot_df, df, \"GarageType\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"PavedDrive\", None)\n       onehot_df = onehot(onehot_df, df, \"MiscFeature\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"Street\", None)\n       onehot_df = onehot(onehot_df, df, \"Alley\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"Condition2\", None)\n       onehot_df = onehot(onehot_df, df, \"RoofMatl\", None)\n       onehot_df = onehot(onehot_df, df, \"Heating\", None)\n\n       # we'll have these as numerical variables too\n       onehot_df = onehot(onehot_df, df, \"ExterQual\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"ExterCond\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"BsmtQual\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"BsmtCond\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"HeatingQC\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"KitchenQual\", \"TA\")\n       onehot_df = onehot(onehot_df, df, \"FireplaceQu\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"GarageQual\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"GarageCond\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"PoolQC\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"BsmtExposure\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"BsmtFinType1\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"BsmtFinType2\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"Functional\", \"Typ\")\n       onehot_df = onehot(onehot_df, df, \"GarageFinish\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"Fence\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"MoSold\", None)\n\n       # Divide  the years between 1871 and 2010 into slices of 20 years\n       year_map = pd.concat(pd.Series(\"YearBin\" + str(i+1), index=range(1871+i*20,1891+i*20))  for i in range(0, 7))\n       yearbin_df = pd.DataFrame(index = df.index)\n       yearbin_df[\"GarageYrBltBin\"] = df.GarageYrBlt.map(year_map)\n       yearbin_df[\"GarageYrBltBin\"].fillna(\"NoGarage\", inplace=True)\n       yearbin_df[\"YearBuiltBin\"] = df.YearBuilt.map(year_map)\n       yearbin_df[\"YearRemodAddBin\"] = df.YearRemodAdd.map(year_map)\n\n       onehot_df = onehot(onehot_df, yearbin_df, \"GarageYrBltBin\", None)\n       onehot_df = onehot(onehot_df, yearbin_df, \"YearBuiltBin\", None)\n       onehot_df = onehot(onehot_df, yearbin_df, \"YearRemodAddBin\", None)\n       return onehot_df\n\n","349046a5":"#create one-hot features\nonehot_df = munge_onehot(train)\n\nneighborhood_train = pd.DataFrame(index=train_new.shape)\nneighborhood_train['NeighborhoodBin'] = train_new['NeighborhoodBin']\nneighborhood_test = pd.DataFrame(index=test_new.shape)\nneighborhood_test['NeighborhoodBin'] = test_new['NeighborhoodBin']\n\nonehot_df = onehot(onehot_df, neighborhood_train, 'NeighborhoodBin', None)","bf7b9292":"train_new = train_new.join(onehot_df) \ntrain_new.shape","b16e037e":"#adding one hot features to test\nonehot_df_te = munge_onehot(test)\nonehot_df_te = onehot(onehot_df_te, neighborhood_test, \"NeighborhoodBin\", None)\ntest_new = test_new.join(onehot_df_te)\ntest_new.shape","072693fd":"#dropping some columns from the train data as they are not found in test\ndrop_cols = [\"_Exterior1st_ImStucc\", \"_Exterior1st_Stone\",\"_Exterior2nd_Other\",\"_HouseStyle_2.5Fin\",\"_RoofMatl_Membran\", \"_RoofMatl_Metal\", \"_RoofMatl_Roll\", \"_Condition2_RRAe\", \"_Condition2_RRAn\", \"_Condition2_RRNn\", \"_Heating_Floor\", \"_Heating_OthW\", \"_Electrical_Mix\", \"_MiscFeature_TenC\", \"_GarageQual_Ex\",  \"_PoolQC_Fa\"]\ntrain_new.drop(drop_cols, axis=1, inplace=True)\ntrain_new.shape","b1be605a":"#removing one column missing from train data\ntest_new.drop([\"_MSSubClass_150\"], axis=1, inplace=True)\n\n# Drop these columns\ndrop_cols = [\"_Condition2_PosN\", # only two are not zero\n         \"_MSZoning_C (all)\",\n         \"_MSSubClass_160\"]\n\ntrain_new.drop(drop_cols, axis=1, inplace=True)\ntest_new.drop(drop_cols, axis=1, inplace=True)","f577239d":"#adding one hot features to test\n#onehot_df_te = munge_onehot(test)\n#onehot_df_te = onehot(onehot_df_te, neighborhood_test, \"NeighborhoodBin\", None)\ntest_new = test_new.join(onehot_df_te)\ntest_new.shape","f0a7a364":"#create a label set\nlabel_df = pd.DataFrame(index = train_new.index, columns = ['SalePrice'])\nlabel_df['SalePrice'] = np.log(train['SalePrice'])\nprint(\"Training set size:\", train_new.shape)\nprint(\"Test set size:\", test_new.shape)\n\n('Training set size:', (1456, 414))\n('Test set size:', (1459, 413))","d613b2c6":"import xgboost as xgb\nregr = xgb.XGBRegressor(colsample_bytree=0.2,\n                       gamma=0.0,\n                       learning_rate=0.05,\n                       max_depth=6,\n                       min_child_weight=1.5,\n                       n_estimators=7200,\n                       reg_alpha=0.9,\n                       reg_lambda=0.6,\n                       subsample=0.2,\n                       seed=42,\n                       silent=1)\n\nregr.fit(train_new, label_df)","6816679b":"from sklearn.metrics import mean_squared_error\ndef rmse(y_test,y_pred):\n      return np.sqrt(mean_squared_error(y_test,y_pred))\n\n# run prediction on training set to get an idea of how well it does\ny_pred = regr.predict(train_new)\ny_test = label_df\nprint(\"XGBoost score on training set: \", rmse(y_test, y_pred))\n\n# make prediction on test set\ny_pred_xgb = regr.predict(test_new_one)\n\n#submit this prediction and get the score\npred1 = pd.DataFrame({'Id': test['Id'], 'SalePrice': np.exp(y_pred_xgb)})\npred1.to_csv('xgbnono.csv', header=True, index=False)","2cf942d8":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.preprocessing import StandardScaler\n\nnp.random.seed(10)\n\n#create Model\n#define base model\ndef base_model():\n     model = Sequential()\n     model.add(Dense(20, input_dim=398, init='normal', activation='relu'))\n     model.add(Dense(10, init='normal', activation='relu'))\n     model.add(Dense(1, init='normal'))\n     model.compile(loss='mean_squared_error', optimizer = 'adam')\n     return model\n\nseed = 7\nnp.random.seed(seed)\n\nscale = StandardScaler()\nX_train = scale.fit_transform(train_new)\nX_test = scale.fit_transform(test_new)\n\nkeras_label = label_df.as_matrix()\nclf = KerasRegressor(build_fn=base_model, nb_epoch=1000, batch_size=5,verbose=0)\nclf.fit(X_train,keras_label)\n\n#make predictions and create the submission file \nkpred = clf.predict(X_test) \nkpred = np.exp(kpred)\npred_df = pd.DataFrame(kpred, index=test[\"Id\"], columns=[\"SalePrice\"]) \npred_df.to_csv('keras1.csv', header=True, index_label='Id') ","3934006d":"##Table of Contents\n\n1.Process of Machine Learning Predictions\n2.Housing Data Set,,,\n      Understand the problem\n      Hypothesis Generation\n      Get Data\n      Data Exploration\n      Data Pre-Processing\n      Feature Engineering - Create 331 new features\n      Model Training - XGBoost, Neural Network, Lasso\n      Model Evaluation","856207f5":"###Introduction\n\n\nFor freshers, projects are the best way to highlight their data science knowledge. In fact, not just freshers, up to mid-level experienced professionals can keep their resumes updated with new, interesting projects. After all, they don't come easy. It takes a lot of time to create a project which can truly showcase the depth and breadth of your knowledge.\n\nI hope this project will help you gain much needed knowledge and help your resume get shortlisted faster. This project shows all the steps (from scratch) taken to solve a Machine Learning problem. For your understanding, I've taken a simple yet challenging data set where you can engineer features at your discretion as well.\n\nWe'll participate in a Kaggle competition and make our way up the leaderboard among ~ top 14% participants.\n\nThis project is most suitable for people who have a basic understanding of python and Machine Learning. Even if you are absolutely new to it, give it a try. And ask questions in Comments below. R users can refer to this equivalent R script and follow the explanation given below."}}