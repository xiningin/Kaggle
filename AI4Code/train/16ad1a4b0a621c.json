{"cell_type":{"aa9f5b69":"code","50e81da0":"code","4e026790":"code","2fcd5ee5":"code","e5a3f93d":"code","2fbeab31":"code","ee965c03":"code","1e7ebdee":"code","82425f4f":"code","4a711531":"code","5cfceb90":"code","4a5cd706":"code","79122118":"code","95b5d857":"code","237653b6":"code","41593c1b":"code","38eb64a3":"code","109f1afe":"code","371b6e17":"code","818ebe29":"code","94f335af":"markdown","3b74f35b":"markdown","7e182135":"markdown","3ef9c54a":"markdown","cb81fc26":"markdown"},"source":{"aa9f5b69":"#https:\/\/www.kaggle.com\/boojum\/connecting-voxel-spaces\/","50e81da0":"!pip install pyradiomics","4e026790":"import os\nimport sys \nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport pydicom\nimport torch\nimport nibabel as nib\nimport matplotlib.pyplot as plt\nimport SimpleITK as sitk\nimport radiomics\n\ntrain_path = '..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/'","2fcd5ee5":"train_dirs = sorted(os.listdir(train_path))","e5a3f93d":"reader = sitk.ImageSeriesReader()\nreader.LoadPrivateTagsOn()","2fbeab31":"def resample(image, ref_image):\n\n    resampler = sitk.ResampleImageFilter()\n    resampler.SetReferenceImage(ref_image)\n    resampler.SetInterpolator(sitk.sitkLinear)\n    \n    resampler.SetTransform(sitk.AffineTransform(image.GetDimension()))\n\n    resampler.SetOutputSpacing(ref_image.GetSpacing())\n\n    resampler.SetSize(ref_image.GetSize())\n\n    resampler.SetOutputDirection(ref_image.GetDirection())\n\n    resampler.SetOutputOrigin(ref_image.GetOrigin())\n\n    resampler.SetDefaultPixelValue(image.GetPixelIDValue())\n\n    resamped_image = resampler.Execute(image)\n    \n    return resamped_image","ee965c03":"def normalize(data):\n    return (data - np.min(data)) \/ (np.max(data) - np.min(data))","1e7ebdee":"%%time\ndef get_img(index):\n    filenamesDICOM = reader.GetGDCMSeriesFileNames(f'{train_path}\/{train_dirs[index]}\/T1w')\n    reader.SetFileNames(filenamesDICOM)\n    t1_sitk = reader.Execute()\n\n    filenamesDICOM = reader.GetGDCMSeriesFileNames(f'{train_path}\/{train_dirs[index]}\/FLAIR')\n    reader.SetFileNames(filenamesDICOM)\n    flair_sitk = reader.Execute()\n\n    filenamesDICOM = reader.GetGDCMSeriesFileNames(f'{train_path}\/{train_dirs[index]}\/T1wCE')\n    reader.SetFileNames(filenamesDICOM)\n    t1wce_sitk = reader.Execute()\n\n    flair_resampled = resample(flair_sitk, t1_sitk)\n    t1wce_resampled = resample(t1wce_sitk, t1_sitk)\n\n    t1_sitk_array = normalize(sitk.GetArrayFromImage(t1_sitk))\n    flair_resampled_array = normalize(sitk.GetArrayFromImage(flair_resampled))\n    t1wce_resampled_array = normalize(sitk.GetArrayFromImage(t1wce_resampled))\n\n    stacked = np.stack([t1_sitk_array, flair_resampled_array, t1wce_resampled_array])\n\n    to_rgb = stacked[:,t1_sitk_array.shape[0]\/\/2,:,:].transpose(1,2,0)\n    im = Image.fromarray((to_rgb * 255).astype(np.uint8))\n    return im","82425f4f":"model = torch.hub.load('mateuszbuda\/brain-segmentation-pytorch', 'unet',\n    in_channels=3, out_channels=1, init_features=32, pretrained=True)","4a711531":"im = get_img(3)","5cfceb90":"test_img = np.array([np.moveaxis(np.array(im.resize((256, 256))), -1, 0)])\ntest_res = model(torch.Tensor(test_img))","4a5cd706":"plt.imshow(test_img[0, 1, :, :])","79122118":"plt.imshow(test_res[0][0].detach().cpu().numpy() > 0.5)","95b5d857":"shape = radiomics.shape2D.RadiomicsShape2D(\n    sitk.GetImageFromArray(test_img), \n    sitk.GetImageFromArray(np.array([\n        test_res[0][0].detach().cpu().numpy() > 0.5\n    ]).astype(np.uint8)),\n    force2D=True\n)","237653b6":"shape.getMeshSurfaceFeatureValue(), \\\nshape.getPixelSurfaceFeatureValue(),\\\nshape.getPerimeterFeatureValue(), \\\nshape.getPerimeterSurfaceRatioFeatureValue(), \\\nshape.getSphericityFeatureValue(), \\\nshape.getSphericalDisproportionFeatureValue(), \\\nshape.getMaximumDiameterFeatureValue(), \\\nshape.getMajorAxisLengthFeatureValue(), \\\nshape.getMinorAxisLengthFeatureValue(), \\\nshape.getElongationFeatureValue(), \\","41593c1b":"im = get_img(6)\ntest_img = np.array([np.moveaxis(np.array(im.resize((256, 256))), -1, 0)])\ntest_res = model(torch.Tensor(test_img))\n\nf, axarr = plt.subplots(1,5, figsize=(20, 20))\naxarr[0].imshow(test_img[0, 0])\naxarr[1].imshow(test_img[0, 1])\naxarr[2].imshow(test_img[0, 2])\naxarr[3].imshow(test_res[0][0].detach().cpu().numpy() > 0.5)\naxarr[4].imshow((test_res[0][0].detach().cpu().numpy() > 0.5) * (test_img[0, 1] != 0))","38eb64a3":"im = get_img(2)\ntest_img = np.array([np.moveaxis(np.array(im.resize((256, 256))), -1, 0)])\ntest_res = model(torch.Tensor(test_img))\n\nf, axarr = plt.subplots(1,5, figsize=(20, 20))\naxarr[0].imshow(test_img[0, 0])\naxarr[1].imshow(test_img[0, 1])\naxarr[2].imshow(test_img[0, 2])\naxarr[3].imshow(test_res[0][0].detach().cpu().numpy() > 0.5)\naxarr[4].imshow((test_res[0][0].detach().cpu().numpy() > 0.5) * (test_img[0, 1] != 0))","109f1afe":"im = get_img(9)\ntest_img = np.array([np.moveaxis(np.array(im.resize((256, 256))), -1, 0)])\ntest_res = model(torch.Tensor(test_img))\n\nf, axarr = plt.subplots(1,5, figsize=(20, 20))\naxarr[0].imshow(test_img[0, 0])\naxarr[1].imshow(test_img[0, 1])\naxarr[2].imshow(test_img[0, 2])\naxarr[3].imshow(test_res[0][0].detach().cpu().numpy() > 0.5)\naxarr[4].imshow((test_res[0][0].detach().cpu().numpy() > 0.5) * (test_img[0, 1] != 0))","371b6e17":"im = get_img(20)\ntest_img = np.array([np.moveaxis(np.array(im.resize((256, 256))), -1, 0)])\ntest_res = model(torch.Tensor(test_img))\n\nf, axarr = plt.subplots(1,5, figsize=(20, 20))\naxarr[0].imshow(test_img[0, 0])\naxarr[1].imshow(test_img[0, 1])\naxarr[2].imshow(test_img[0, 2])\naxarr[3].imshow(test_res[0][0].detach().cpu().numpy() > 0.5)\naxarr[4].imshow((test_res[0][0].detach().cpu().numpy() > 0.5) * (test_img[0, 1] != 0))","818ebe29":"im = get_img(24)\ntest_img = np.array([np.moveaxis(np.array(im.resize((256, 256))), -1, 0)])\ntest_res = model(torch.Tensor(test_img))\n\nf, axarr = plt.subplots(1,5, figsize=(20, 20))\naxarr[0].imshow(test_img[0, 0])\naxarr[1].imshow(test_img[0, 1])\naxarr[2].imshow(test_img[0, 2])\naxarr[3].imshow(test_res[0][0].detach().cpu().numpy() > 0.5)\naxarr[4].imshow((test_res[0][0].detach().cpu().numpy() > 0.5) * (test_img[0, 1] != 0))","94f335af":"#### For this type of broken segmentation like in '00009' we could perform a little trick to fix. \nWe need to multiply mask to the empty space of the original image","3b74f35b":"## P.S. Warning\nThe segmentation doesn't not working properly on all images due to the different tumor modalities. The model was trained on low-grade tumors, so be aware. Let's see the examples","7e182135":"#### For the \"00003\" image the broken segmentation could be due to the bad registration and this trick won't work","3ef9c54a":"I thought that all pipelines presented on public notebooks are giving random output, so I decided to think a little bit differently. The idea is to extract features using MRI images and tumor mask. Here I'm giving an example for one image, maybe I'm doing something wrong, but I'll be glad to get feedback from the community. Also I'm worried that it will be out of time on the whole test set\n\n### Registration \nhttps:\/\/www.kaggle.com\/boojum\/connecting-voxel-spaces\/\n![](https:\/\/sun9-45.userapi.com\/impg\/Flbnug2OUli1ecXsoIKeUasIGXGj_5hqjX4cRg\/z2nfz8-b3a0.jpg?size=2560x1153&quality=96&sign=0536543610f1655d967af88dbc775e98&type=album)\n\n### Segmentation \nUnet\nhttps:\/\/pytorch.org\/hub\/mateuszbuda_brain-segmentation-pytorch_unet\/\n\n### Features\n\nhttps:\/\/pyradiomics.readthedocs.io\/en\/latest\/features.html#module-radiomics.shape2D","cb81fc26":"#### Let's see more examples of broken segmentation"}}