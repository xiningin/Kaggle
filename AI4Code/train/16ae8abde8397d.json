{"cell_type":{"6200d608":"code","2019db97":"code","2be611e3":"code","92ed30f4":"code","906e96c2":"code","6ce697f1":"code","0195468d":"code","fc7d8d09":"code","398e3e6d":"code","7046d2f3":"code","810bbf81":"code","03295179":"code","d5d4c0d9":"code","f2e932de":"code","ca814fc8":"code","8fa5e8e6":"code","6591d6d5":"code","cc24b815":"code","4d1330a7":"code","b75065ac":"code","e544d9ea":"code","6296a3f2":"code","d31a7edc":"code","464dd08f":"code","132b63af":"code","7f9995d2":"code","d11cd54e":"code","a9227067":"code","e95c9f25":"code","026d0ebf":"code","2172382f":"code","c4078d69":"code","9f34a6b8":"code","362d4f83":"code","d70b3ef4":"code","ff2902cb":"code","87364014":"code","e5f174ea":"code","2ba89635":"code","4d28ac64":"code","e429404b":"code","acc0d11b":"code","bb098cf4":"code","1b7c2b7e":"code","476968b5":"code","e17ab2eb":"code","10b19f7e":"code","7987a792":"code","8145b123":"code","7cb31127":"code","37d3f73d":"code","71f07bb6":"code","8b3f6350":"code","d17cfcc7":"code","847cda50":"code","9e795690":"code","9a140224":"code","d08d96ef":"code","9b1ff9b2":"code","f653b0d9":"code","341d9902":"code","b252cea6":"code","5be88f5c":"code","6f03e0f0":"code","bd949db1":"code","dc51f1a9":"code","5c29defa":"code","0bef461d":"code","f0841a17":"code","51e24ea9":"code","b7fdb53c":"code","9165d0e8":"code","cee78d9a":"code","f338680c":"code","34b6c2aa":"code","2cbe21e0":"code","8ff3c370":"code","b2e3a663":"code","413e2b99":"code","e3f5a3fd":"code","514c5ec0":"code","60db6fc5":"code","0e658cab":"code","e0187c31":"code","c8104995":"code","d85e5b89":"code","04a27453":"code","4c5c1e13":"code","45c1ea0e":"code","fe0654d4":"code","4373df3c":"code","86123ab8":"code","803717e5":"code","606d5982":"code","a97d61e3":"code","80099f74":"code","f5800164":"code","6eabdbba":"code","9b69c4dc":"code","a86e718d":"code","b16c5e31":"code","aa43d5cc":"code","f0a1e531":"code","7806da19":"code","a823edb2":"code","09005b5d":"code","be8a4dba":"code","5669b313":"code","19104919":"code","3b0be162":"code","951634fc":"code","b7efe171":"code","0622a6bd":"code","9d09a9ee":"code","a3fa1c9e":"code","be443179":"code","b6437a5e":"code","de31647f":"code","df7fc230":"code","de8c8ec8":"code","3d18eb43":"markdown","ff775b60":"markdown","7d19671c":"markdown","915a2a79":"markdown","60436d9e":"markdown","21a9b49e":"markdown","81370df7":"markdown","c2df224f":"markdown","59176bb1":"markdown","dc7e71f6":"markdown","6ac787d0":"markdown","d66bd71e":"markdown","50882f7d":"markdown","db440d8d":"markdown","10340185":"markdown","089118fc":"markdown","1cc13da6":"markdown","7ff4d0d5":"markdown","71c2da48":"markdown","059891a0":"markdown","35f94a5d":"markdown","74aa0fc3":"markdown"},"source":{"6200d608":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom shapely.geometry import LineString\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2019db97":"bank_data = pd.read_csv(\"\/kaggle\/input\/bank-marketing-dataset\/bank.csv\")\nbank_data.head()","2be611e3":"#rename the column 'y' to 'target'\nbank_data.rename(columns={\"deposit\": \"target\"},inplace = True)","92ed30f4":"bank_data.shape","906e96c2":"bank_data.info()","6ce697f1":"#Check if any entry is null\nbank_data.isnull().sum()","0195468d":"bank_data.describe()","fc7d8d09":"bank_data.columns","398e3e6d":"sns.set_style(\"whitegrid\")\nfig = plt.figure(figsize = [15,20])\ncols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\ncnt = 1\nfor col in cols :\n    ax = plt.subplot(4,2,cnt)\n    sns.distplot(bank_data[col], hist_kws=dict(edgecolor=\"k\", linewidth=1,color='grey'), color='red')\n    cnt+=1\n    plot_name = \"Data distribution of column : \"+col\n    ax.set_title(plot_name,fontsize = 15)\nplt.tight_layout()\nplt.show() ","7046d2f3":"sns.set_style(\"whitegrid\")\nfig = plt.figure(figsize = [15,15])\ncols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\ncnt = 1\nfor col in cols :\n    ax = plt.subplot(4,2,cnt)\n    sns.boxplot(bank_data[col])\n    cnt+=1\n    plot_name = \"IQR for column : \"+col\n    ax.set_title(plot_name,fontsize = 15)\nplt.tight_layout()\nplt.show() ","810bbf81":"sns.set_style(\"whitegrid\")\nfig = plt.figure(figsize = [15,15])\ncols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\ncnt = 1\nfor col in cols :\n    ax = plt.subplot(3,2,cnt)\n    sns.violinplot(data = bank_data,x = col, y='target')\n    cnt+=1\n    plot_title = \"Data distribution of \"+col+\" for output labels\"\n    ax.set_title(plot_title,fontsize = 15)\nplt.tight_layout()\nplt.show() ","03295179":"g = sns.pairplot(data = bank_data,hue = \"target\",diag_kws={'bw': 0.2})\ng.fig.suptitle(\"Pairplot of numerical columns in the dataset\",y = 1)\nplt.show()","d5d4c0d9":"sns.set_style(\"whitegrid\")\nfig = plt.figure(figsize = [15,20])\ncols = ['marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'job', 'target']\ncnt = 1\nfor col in cols :\n    ax = plt.subplot(5,2,cnt)\n    sns.countplot(data = bank_data, x = col, order = bank_data[col].value_counts().index)\n    if col == 'job' :\n        plt.xticks(rotation = 90)\n    cnt+=1\n    plot_name = \"Countplot for column : \"+col\n    ax.set_title(plot_name,fontsize = 15)\nplt.tight_layout()\nplt.show()  ","f2e932de":"bank_data.target.value_counts()","ca814fc8":"bank_data.target.value_counts().plot(kind = 'pie', autopct='%.2f')\nplt.title(\"Target variable percentage\",fontsize = 15)\nplt.show()","8fa5e8e6":"bank_data_yes = bank_data[bank_data.target == 'yes']\nbank_data_yes.head()","6591d6d5":"sns.set_style(\"whitegrid\")\nfig = plt.figure(figsize = [15,20])\ncols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\ncnt = 1\nfor col in cols :\n    ax = plt.subplot(4,2,cnt)\n    sns.distplot(bank_data_yes[col], hist_kws=dict(edgecolor=\"k\", linewidth=1,color='grey'), color='red')\n    cnt+=1\n    plot_name = \"Data distribution of column : '\"+col+\"' for target label 1\"\n    ax.set_title(plot_name,fontsize = 15)\nplt.tight_layout()\nplt.show() ","cc24b815":"sns.set_style(\"whitegrid\")\nfig = plt.figure(figsize = [15,15])\ncols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\ncnt = 1\nfor col in cols :\n    ax = plt.subplot(4,2,cnt)\n    sns.boxplot(bank_data_yes[col])\n    cnt+=1\n    plot_name = \"IQR for column : \"+col\n    ax.set_title(plot_name,fontsize = 15)\nplt.tight_layout()\nplt.show() ","4d1330a7":"sns.set_style(\"whitegrid\")\nfig = plt.figure(figsize = [15,20])\ncols = ['marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'job']\ncnt = 1\nfor col in cols :\n    ax = plt.subplot(5,2,cnt)\n    sns.countplot(data = bank_data_yes, x = col, order = bank_data_yes[col].value_counts().index)\n    if col == 'job' :\n        plt.xticks(rotation = 90)\n    plot_name = \"Countplot for column : '\"+col+\"' for target label 1\"\n    ax.set_title(plot_name,fontsize = 15)\n    cnt+=1\nplt.tight_layout()\nplt.show()  ","b75065ac":"# List of variables to map\n\nvarlist =  ['housing', 'loan', 'default', 'target']\n\n# Defining the map function\ndef binary_map(x):\n    return x.map({'yes': 1, \"no\": 0})\n\n# Applying the function to the housing list\nbank_data[varlist] = bank_data[varlist].apply(binary_map)","e544d9ea":"bank_data.head()","6296a3f2":"def month_converter(month):\n    months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n    return months.index(month) + 1","d31a7edc":"bank_data.month = bank_data.month.apply(month_converter)\nbank_data.head()","464dd08f":"# correlation matrix \nplt.figure(figsize = (20,10))        \nsns.heatmap(bank_data.corr(),annot = True, fmt='.2f')\nplt.yticks(rotation=0) \nplt.show()","132b63af":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy = pd.get_dummies(bank_data[['marital', 'education', 'contact', 'poutcome', 'job']], drop_first=True)\n\n# Adding the results to the master dataframe\nbank_data = pd.concat([bank_data, dummy], axis=1)","7f9995d2":"bank_data.shape","d11cd54e":"# We have created dummies for the below variables, so we can drop them\nbank_data = bank_data.drop(['marital', 'education', 'contact', 'poutcome', 'job'], 1)\nbank_data.shape","a9227067":"scaler = StandardScaler()\ncols = ['age','balance','day','month','duration','campaign', 'pdays', 'previous']\nbank_data[cols] = scaler.fit_transform(bank_data[cols])\nbank_data.head()","e95c9f25":"bank_data.info()","026d0ebf":"def logReg(x,y) :\n    X_train_sm = sm.add_constant(x)\n    logm1 = sm.GLM(y,X_train_sm, family = sm.families.Binomial())\n    res = logm1.fit()\n    return res","2172382f":"def get_classification_report(res, y_df, X_df, prob) :\n    y_df_pred = res.predict(X_df)#.values.reshape(-1)\n    y_df_pred_final = pd.DataFrame({'Target':y_df.values, 'Target_Prob':y_df_pred})\n    y_df_pred_final['predicted'] = y_df_pred_final.Target_Prob.map(lambda x: 1 if x > prob else 0)\n    #print(classification_report(y_df_pred_final.Target, y_df_pred_final.predicted))\n    return y_df_pred_final","c4078d69":"def precision_recall(actual,predicted) :\n    recall = round(recall_score(actual,predicted),2)\n    print(\"precision : \",round(precision_score(actual,predicted),2))\n    print(\"recall : \",recall)\n    return recall","9f34a6b8":"def get_confusion_matrix(actual,predicted) :\n    cm = confusion_matrix(actual,predicted)\n    df = pd.DataFrame(cm)\n    return df","362d4f83":"df = bank_data.copy()\nX = df.drop('target',axis = 1)\ny = df['target']\nX_aux, X_test, y_aux, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\nX_train, X_val, y_train, y_val = train_test_split(X_aux, y_aux, train_size=0.75, test_size=0.25, random_state=0)","d70b3ef4":"res = logReg(X_train,y_train)\nres.summary()","ff2902cb":"def get_vif(X_train) :\n    vif = pd.DataFrame()\n    vif['Features'] = X_train.columns\n    vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    return vif","87364014":"get_vif(X_train)","e5f174ea":"X_train_new = X_train.drop(['poutcome_unknown'],axis = 1)\nX_val_new = X_val.drop(['poutcome_unknown'],axis=1)\nX_test_new = X_test.drop(['poutcome_unknown'],axis = 1)\nres = logReg(X_train_new,y_train)","2ba89635":"get_vif(X_train_new)","4d28ac64":"y_df_pred_final = get_classification_report(res,y_train,sm.add_constant(X_train_new),0.5)\nprecision_recall(y_df_pred_final.Target,y_df_pred_final.predicted)","e429404b":"y_df_pred_final","acc0d11b":"def output_for_probability_range(target, target_prob) :\n    y_df_pred_final = pd.DataFrame({'Target':target, 'Target_Prob':target_prob})\n    numbers = [float(x)\/10 for x in range(10)]\n    for i in numbers:\n        y_df_pred_final[i]= y_df_pred_final.Target_Prob.map(lambda x: 1 if x > i else 0)\n        \n    cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n    num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n    for i in num:\n        cm1 = metrics.confusion_matrix(y_df_pred_final.Target, y_df_pred_final[i] )\n        total1=sum(sum(cm1))\n        accuracy = (cm1[0,0]+cm1[1,1])\/total1\n        speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n        sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n        cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n    return cutoff_df","bb098cf4":"cutoff_df = output_for_probability_range(y_df_pred_final.Target,y_df_pred_final.Target_Prob)\ncutoff_df","1b7c2b7e":"def get_optimal_threshold_curve(cutoff_df) :\n    x = cutoff_df.prob\n    f = cutoff_df.accuracy\n    g = cutoff_df.sensi\n    h = cutoff_df.speci\n\n    plt.plot(x, f, '-',label = 'accuracy')\n    plt.plot(x, g, '-',label = 'sensitivity')\n    plt.plot(x, h, '-',label = 'specificity')\n\n    first_line = LineString(np.column_stack((x, f)))\n    second_line = LineString(np.column_stack((x, g)))\n    intersection = first_line.intersection(second_line)\n    plt.xticks(np.arange(0, 1, step=0.1))\n    plt.plot(*intersection.xy, 'o')\n    plt.legend()\n    plt.grid()\n    plt.savefig('optimal_probability_graph.png', bbox_inches='tight')\n    plt.show()\n    x,y = intersection.xy\n    prob_threshold = list(x)[0]\n    return prob_threshold","476968b5":"prob_threshold = get_optimal_threshold_curve(cutoff_df)\nprob_threshold","e17ab2eb":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.grid()\n    plt.legend(loc=\"lower right\")\n    plt.savefig('Receiver_Operating_Characteristic.png', bbox_inches='tight')\n    plt.show()\n\n    return None","10b19f7e":"y_train_pred_final = get_classification_report(res, y_train, sm.add_constant(X_train_new), prob_threshold)\ndraw_roc(y_train_pred_final.Target, y_train_pred_final.Target_Prob)","7987a792":"precision_recall(y_train_pred_final.Target, y_train_pred_final.predicted)","8145b123":"X_val_sm = sm.add_constant(X_val_new)\ny_pred_lr = res.predict(X_val_sm)\ny_df_pred_final = get_classification_report(res, y_val, X_val_sm, prob_threshold)","7cb31127":"recall_lr = precision_recall(y_df_pred_final.Target, y_df_pred_final.predicted)","37d3f73d":"X_test_sm = sm.add_constant(X_test_new)\ny_pred_lr = res.predict(X_test_sm)\ny_df_pred_final = get_classification_report(res, y_test, X_test_sm, prob_threshold)","71f07bb6":"precision_recall(y_df_pred_final.Target, y_df_pred_final.predicted)","8b3f6350":"get_confusion_matrix(y_df_pred_final.Target, y_df_pred_final.predicted)","d17cfcc7":"params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}","847cda50":"elasticnet = ElasticNet()\nfolds = 5\n# cross validation\nmodel_cv = GridSearchCV(estimator = elasticnet, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train, y_train) ","9e795690":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","9a140224":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.plot(np.log10(cv_results['param_alpha']), cv_results['mean_train_score'])\nplt.plot(np.log10(cv_results['param_alpha']), cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.grid()\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","d08d96ef":"model_cv.best_estimator_","9b1ff9b2":"alpha = model_cv.best_estimator_.alpha\nelasticnet = ElasticNet(alpha=alpha)       \nelasticnet.fit(X_train, y_train) \nelasticnet.coef_","f653b0d9":"y_pred_reg = elasticnet.predict(X_test)\nprint(\"Mean squared error with LogReg : \",np.round(metrics.mean_squared_error(y_test,y_pred_lr),2))\nprint(\"Mean squared error with Regularization : \",np.round(metrics.mean_squared_error(y_test,y_pred_reg),2))","341d9902":"print(\"R2 score with LogReg : \",np.round(metrics.r2_score(y_test,y_pred_lr),2))\nprint(\"R2 score with Regularization : \",np.round(metrics.r2_score(y_test,y_pred_reg),2))","b252cea6":"y_df_pred_final_reg = get_classification_report(elasticnet, y_test, X_test, prob_threshold)\nprecision_recall(y_df_pred_final_reg.Target, y_df_pred_final_reg.predicted)","5be88f5c":"get_confusion_matrix(y_df_pred_final_reg.Target, y_df_pred_final_reg.predicted)","6f03e0f0":"# Create a Decision Tree\ndt_basic = DecisionTreeClassifier(max_depth=10)\ndt_basic","bd949db1":"# Fit the training data\ndt_basic.fit(X_train,y_train)","dc51f1a9":"# Predict based on test data\ny_preds = dt_basic.predict(X_val)","5c29defa":"precision_recall(y_val,y_preds)","0bef461d":"# Calculate the number of nodes in the tree\ndt_basic.tree_.node_count","f0841a17":"dt_basic.tree_.max_depth","51e24ea9":"# Create a Parameter grid\nparam_grid = {\n    'classification__max_depth' : range(10,16),\n    'classification__min_samples_leaf' : range(5,60,5),\n    'classification__min_samples_split' : range(5,60,5),\n    'classification__criterion' : ['gini','entropy'],\n    'classification__max_features' : ['auto','sqrt','log2']\n}","b7fdb53c":"n_folds = 4","9165d0e8":"# Create a Decision Tree\ndtree = DecisionTreeClassifier()","cee78d9a":"model = Pipeline([\n        ('sampling', SMOTE()),\n        ('classification', dtree)\n    ])","f338680c":"# Create a Grid with parameters\ngrid = GridSearchCV(model, param_grid, cv = n_folds, n_jobs = -1,return_train_score=True,scoring = 'recall')","34b6c2aa":"%%time\ngrid.fit(X_train,y_train)","2cbe21e0":"scores = grid.cv_results_","8ff3c370":"cv_result = pd.DataFrame(scores)\ncv_result.head()","b2e3a663":"# Plot accuracy vs param_max_depth\nplt.figure\nplt.plot(scores['param_classification__max_depth'].data,scores['mean_train_score'], label = \"training_accuracy\")\nplt.plot(scores['param_classification__max_depth'].data,scores['mean_test_score'], label = \"test_accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"accuracy\")\nplt.legend()\nplt.show()","413e2b99":"grid.best_params_","e3f5a3fd":"best_grid = grid.best_estimator_\nbest_grid","514c5ec0":"best_grid.fit(X_train,y_train)","60db6fc5":"dtree_prob = best_grid.predict_proba(X_train)","0e658cab":"cutoff_df = output_for_probability_range(y_train,dtree_prob[:,1])\ncutoff_df","e0187c31":"prob_threshold_dt = get_optimal_threshold_curve(cutoff_df)\nprob_threshold_dt","c8104995":"def get_classification_report_tree(res, y_df, X_df, prob) :\n    y_df_pred = res.predict_proba(X_df)[:,1]\n    y_df_pred_final = pd.DataFrame({'Target':y_df.values, 'Target_Prob':y_df_pred})\n    y_df_pred_final['predicted'] = y_df_pred_final.Target_Prob.map(lambda x: 1 if x > prob else 0)\n    return y_df_pred_final","d85e5b89":"dt_y_train_pred_final = get_classification_report_tree(best_grid, y_train, X_train, prob_threshold_dt)\ndraw_roc(dt_y_train_pred_final.Target, dt_y_train_pred_final.Target_Prob)","04a27453":"y_val_pred = best_grid.predict(X_val)\nrecall_dt = precision_recall(y_val,y_val_pred)","4c5c1e13":"y_preds = best_grid.predict(X_test)\nprecision_recall(y_test,y_preds)","45c1ea0e":"get_confusion_matrix(y_test,y_preds)","fe0654d4":"rf = RandomForestClassifier(random_state=0, n_estimators=10, max_depth=4)","4373df3c":"rf.fit(X_train, y_train)","86123ab8":"y_preds = rf.predict(X_val)\nprecision_recall(y_val,y_preds)","803717e5":"X_train.shape","606d5982":"cols = X_train.columns\nlen(cols)","a97d61e3":"# Create the parameter grid based on the results of random search \nparams = {\n    'classification__max_depth': [7,8,10],\n    'classification__min_samples_leaf': range(50,100,10),\n    'classification__max_features': range(4,12,2),\n    'classification__n_estimators': range(20,100,20),\n    'classification__criterion' : ['entropy','gini']\n}","80099f74":"classifier_rf = RandomForestClassifier(random_state=0, n_jobs=-1)","f5800164":"model = Pipeline([\n        ('sampling', SMOTE()),\n        ('classification', classifier_rf)\n    ])","6eabdbba":"# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator=model, param_grid=params, \n                          cv=4, n_jobs=-1, verbose=1)","9b69c4dc":"%%time\ngrid_search.fit(X_train,y_train)","a86e718d":"rf_best = grid_search.best_estimator_\nrf_best","b16c5e31":"rf_best.fit(X_train, y_train)","aa43d5cc":"rf_prob = rf_best.predict_proba(X_train)","f0a1e531":"cutoff_df = output_for_probability_range(y_train,rf_prob[:,1])\ncutoff_df","7806da19":"prob_threshold_rf = get_optimal_threshold_curve(cutoff_df)\nprob_threshold_rf","a823edb2":"rf_y_train_pred_final = get_classification_report_tree(rf_best, y_train, X_train, prob_threshold_rf)\ndraw_roc(rf_y_train_pred_final.Target, rf_y_train_pred_final.Target_Prob)","09005b5d":"y_val_pred = rf_best.predict(X_val)\nrecall_rf = precision_recall(y_val,y_val_pred)","be8a4dba":"y_preds = rf_best.predict(X_test)\nprecision_recall(y_test,y_preds)","5669b313":"get_confusion_matrix(y_test,y_preds)","19104919":"weight_lr = 1\/(100 - recall_lr*100)\nweight_dt = 1\/(100 - recall_dt*100)\nweight_rf = 1\/(100 - recall_rf*100)","3b0be162":"def predict_probability(df) :\n    df_lr = sm.add_constant(df.drop(['poutcome_unknown'],axis = 1))\n    predict_prob_lr = np.array(res.predict(df_lr))\n    predict_prob_dt = best_grid.predict_proba(df)[:,1]\n    predict_prob_rf = rf_best.predict_proba(df)[:,1]\n    return predict_prob_lr,predict_prob_dt,predict_prob_rf","951634fc":"def weighted_probability(lr_df,dt_df,rf_df) :\n    size = len(lr_df)\n    weighted_prob = []\n    for i in range(0,size) :\n        prob = weight_lr * lr_df[i] + weight_dt * dt_df[i] + weight_rf * rf_df[i]\n        weighted_prob.append(prob)\n    return weighted_prob","b7efe171":"predict_prob_lr,predict_prob_dt,predict_prob_rf = predict_probability(X_train)","0622a6bd":"weighted_prob = weighted_probability(predict_prob_lr,predict_prob_dt,predict_prob_rf)","9d09a9ee":"cutoff_df = output_for_probability_range(y_train,weighted_prob)\ncutoff_df","a3fa1c9e":"prob_threshold_ens = get_optimal_threshold_curve(cutoff_df)\nprob_threshold_ens","be443179":"draw_roc(y_train, weighted_prob)","b6437a5e":"predict_prob_lr,predict_prob_dt,predict_prob_rf = predict_probability(X_test)\nweighted_prob = weighted_probability(predict_prob_lr,predict_prob_dt,predict_prob_rf)","de31647f":"y_pred_final = pd.DataFrame({'Target' : y_test, 'Predicted_prob' : weighted_prob})\ny_pred_final['Predicted'] = y_pred_final.Predicted_prob.apply(lambda x : 1 if x >= prob_threshold_ens else 0)\ny_pred_final.head()","df7fc230":"precision_recall(y_pred_final.Target,y_pred_final.Predicted)","de8c8ec8":"get_confusion_matrix(y_pred_final.Target,y_pred_final.Predicted)","3d18eb43":"# EDA","ff775b60":"### Creating dummy variables for categorical data [One-hot encoding] ","7d19671c":"## Ensemble","915a2a79":"# Model Training","60436d9e":"### Feature Scaling","21a9b49e":"### Hyper-parameter tuning [RF]","81370df7":"The aim of the analysis is to come up with a model which can be used by the bank in identifying the potential customers who would be willingly to opt for the term deposit offered by the bank. In order to build such a model, the follwoing steps have been followed :\n1. Exploratory Data Analysis\n2. Data Cleaning\/Imputation [Since the data in this dataset does not have any null\/junk values, this step was not needed.]\n3. Data Preparation\/Feature Engineering\n4. Model Training","c2df224f":"### Finding Optimal Probability threshold","59176bb1":"### Hyperparameter tuning Optimization [DT]","dc7e71f6":"Since the aim is to identify maximum number of users who will be willing to opt for 'term deposit' plan, we need to reduce the number of False Negatives predicted by a model. Thus the model used should have high 'recall'.","6ac787d0":"### Converting month to numerical column","d66bd71e":"# Data Preparation","50882f7d":"## Regularization","db440d8d":"Logistic Regression, Decision Tree & Random Forest have been combined to form an ensemble to get the predictions.\nThe weight for each model has been calculated as :\n> weight = 1\/(100 - recall*100)\n- This weight is multiplied with the probability predicted by the model and the sum of these weighted probabilities is taken as the output probability given by the Ensemble.","10340185":"## Logistic Regression","089118fc":"### ROC ","1cc13da6":"### VIF","7ff4d0d5":"## Random Forest","71c2da48":"### Analysis of features for target == yes","059891a0":"The columns don't show much correlation with each other. A maximum correlation of 0.51 is shown between 'pdays' & 'previous'.","35f94a5d":"## Decision Trees","74aa0fc3":"### Converting binary variables (Yes\/No) to 0\/1"}}