{"cell_type":{"fc7f3d6f":"code","46182167":"code","217037e3":"code","68a5a788":"code","abb9027c":"code","3e1b56c4":"code","ad583828":"code","b53a2aad":"code","c8576e09":"code","2222570c":"code","60fb1591":"code","c7b1a76e":"code","1ed83b87":"code","b71fd47d":"code","e4368f23":"code","1e1a6416":"code","8b87e6a0":"code","cbbed2b4":"code","b411b48e":"code","ed7127c2":"code","f12c02bd":"code","f1dcccd1":"code","4f530775":"code","66156683":"code","91cc2871":"code","19cd78bb":"code","0dc55db2":"code","1d6c96a5":"code","52acbcdb":"code","9b253f45":"code","6ce2ed35":"code","f2f6e373":"code","07478cec":"code","23cdfa65":"code","a9f98340":"code","69c24e2a":"code","3e34fe1c":"code","5ecb5418":"code","b276373f":"code","1c0b56e6":"code","e46434e7":"code","3f4f67aa":"code","4c27069b":"code","54e0e217":"code","de995d19":"code","94d009ca":"code","64e9e68c":"code","43aacf07":"markdown","eb4abac3":"markdown","4dc542f6":"markdown","ed942af2":"markdown","adbfa23e":"markdown","ac5e9ba7":"markdown","df752f20":"markdown","5d6f8e0b":"markdown","554506ac":"markdown","5f651b0e":"markdown","752e025f":"markdown","2315ad86":"markdown","5a511d99":"markdown","622d8e51":"markdown","b88cae87":"markdown","d5357a33":"markdown","2ac9fefb":"markdown","4cf275aa":"markdown","48514cf6":"markdown","39706209":"markdown","6c52ba11":"markdown","171b31e7":"markdown","d960a78b":"markdown","2a8fd03b":"markdown","d91be0d4":"markdown","9eafab92":"markdown","10e92a7d":"markdown","579a9bc3":"markdown","a88bb486":"markdown","f292734b":"markdown","8dcf551a":"markdown","6ca045ea":"markdown","dc9ac6a4":"markdown","f7521e41":"markdown","85e5eca1":"markdown","c529d336":"markdown","973252a4":"markdown","0e4f2e08":"markdown","a588703d":"markdown","9956047b":"markdown","275c7fd1":"markdown","5ae8c020":"markdown","124615af":"markdown","0d0121b4":"markdown","73f5d967":"markdown","8bad8874":"markdown"},"source":{"fc7f3d6f":"!pip uninstall --y typing","46182167":"!pip install nvtx dask_cuda","217037e3":"!pip install git+https:\/\/github.com\/NVIDIA\/NVTabular.git@4c92dffac4354d816178264bcfcdec722db2ec1c","68a5a788":"%time\n!mkdir \/raid\n!cp -r \/kaggle\/input\/criteo-dataset-parquet\/criteo-parquet \/raid\/","abb9027c":"import os\nfrom time import time\nimport re\nimport glob\nimport warnings\nimport cudf\nimport gc\n\n# tools for data preproc\/loading\nimport torch\nimport rmm\nimport nvtabular as nvt\nfrom nvtabular.ops import Normalize,  Categorify,  LogOp, FillMissing, Clip, get_embedding_sizes\nfrom nvtabular.loader.torch import TorchAsyncItr, DLDataLoader\nfrom nvtabular.utils import device_mem_size, get_rmm_size\n\n# tools for training\nfrom fastai.basics import Learner\nfrom fastai.tabular.model import TabularModel\nfrom fastai.tabular.data import TabularDataLoaders\nfrom fastai.metrics import accuracy\nfrom fastai.callback.progress import ProgressCallback\n\nimport dask\nimport dask.dataframe as dd\n\nimport numpy as np","3e1b56c4":"# define some information about where to get our data\nINPUT_DATA_DIR = os.environ.get('INPUT_DATA_DIR', '\/raid\/criteo-parquet\/')\n# where we'll save our processed data to.\n# Although the environment has no RAID, we call the directory raid as many of our examples \n# are developed on machines with RAID.\nOUTPUT_DATA_DIR = os.environ.get('OUTPUT_DATA_DIR', '\/raid\/test_dask') \ntrain_paths = [INPUT_DATA_DIR + '\/train_train.parquet']\nvalid_paths = [INPUT_DATA_DIR + '\/train_valid.parquet']\ncpu_time = 0","ad583828":"start = time()\ndd_train = dd.read_parquet(train_paths[0])\ndd_valid = dd.read_parquet(valid_paths[0])\ncpu_time = time() - start","b53a2aad":"CONTINUOUS_COLUMNS = ['I' + str(x) for x in range(1,14)]\nCATEGORICAL_COLUMNS =  ['C' + str(x) for x in range(1,27)]\ncat_dict = {}","c8576e09":"start = time()\n\ndd_train[CONTINUOUS_COLUMNS] = np.log1p(dd_train[CONTINUOUS_COLUMNS].fillna(0).clip(lower=0))\ndd_valid[CONTINUOUS_COLUMNS] = np.log1p(dd_valid[CONTINUOUS_COLUMNS].fillna(0).clip(lower=0))\ndd_cont_mean = dd_train[CONTINUOUS_COLUMNS].mean().persist()\ndd_cont_std = dd_train[CONTINUOUS_COLUMNS].std().persist()\n    \ncpu_time += time() - start","2222570c":"start = time()\n\nfor col in CATEGORICAL_COLUMNS:\n    dd_tmp = dd_train[col].value_counts()\n    df_tmp = dd_tmp[dd_tmp>=30].compute()\n    df_tmp = df_tmp.reset_index().reset_index()\n    df_tmp = df_tmp.drop(col, axis=1)\n    df_tmp.columns = [col, col + '_old']\n    df_tmp[col] = df_tmp[col]+1\n    cat_dict[col] = df_tmp\n    \ncpu_time += time() - start","60fb1591":"start = time()\n\ndd_train[CONTINUOUS_COLUMNS] = (dd_train[CONTINUOUS_COLUMNS]-dd_cont_mean)\/dd_cont_std\ndd_valid[CONTINUOUS_COLUMNS] = (dd_valid[CONTINUOUS_COLUMNS]-dd_cont_mean)\/dd_cont_std\n\nfor col in CATEGORICAL_COLUMNS:\n    dd_train[col] = dd_train[[col]].merge(cat_dict[col], how='left', left_on=col, right_on=col + '_old')[col + '_y'].fillna(0)\n    dd_valid[col] = dd_valid[[col]].merge(cat_dict[col], how='left', left_on=col, right_on=col + '_old')[col + '_y'].fillna(0)\n\ncpu_time += time() - start","c7b1a76e":"start = time()\n\ndd_train.to_parquet('\/raid\/cpu_train_test.parquet')\ndd_valid.to_parquet('\/raid\/cpu_valid_test.parquet')\n\ncpu_time += time() - start","1ed83b87":"print('Dask\/CPU time: ' + str(cpu_time) + ' in seconds')","b71fd47d":"del dd_train, dd_valid, dd_cont_mean, dd_cont_std, cat_dict\ngc.collect()","e4368f23":"CATEGORICAL_COLUMNS =  ['C' + str(x) for x in range(1,27)]\n# Let's print only the first 10 column names\nCATEGORICAL_COLUMNS[0:10]","1e1a6416":"%%time\n\ncat_features = CATEGORICAL_COLUMNS >> Categorify(freq_threshold=30)","8b87e6a0":"%%time\n\ncat_features.graph","cbbed2b4":"CONTINUOUS_COLUMNS = ['I' + str(x) for x in range(1,14)]\n# Let's print only the first 10 column names\nprint(CONTINUOUS_COLUMNS[0:10])","b411b48e":"%%time\n\ncont_features = CONTINUOUS_COLUMNS >> FillMissing() >> Clip(min_value=0) >> LogOp() >> Normalize()\ncont_features.graph","ed7127c2":"features = cat_features+cont_features+['label']","f12c02bd":"features.graph","f1dcccd1":"%%time\n\nworkflow = nvt.Workflow(features)","4f530775":"%%time\n\ntrain_dataset = nvt.Dataset(train_paths, engine='parquet', part_mem_fraction=0.15)\nvalid_dataset = nvt.Dataset(valid_paths, engine='parquet', part_mem_fraction=0.15)","66156683":"%%time\n\nstart = time()\n\nworkflow.fit(train_dataset)\n\ngpu_time = time() - start","91cc2871":"output_train_dir = os.path.join(OUTPUT_DATA_DIR, 'train\/')\noutput_valid_dir = os.path.join(OUTPUT_DATA_DIR, 'valid\/')\n! mkdir -p $output_train_dir\n! mkdir -p $output_valid_dir","19cd78bb":"%%time\n\nstart = time()\n\nworkflow.transform(train_dataset).to_parquet(output_path=output_train_dir,\n                                             shuffle=nvt.io.Shuffle.PER_PARTITION, \n                                             out_files_per_proc=5)\n\ngpu_time += time() - start","0dc55db2":"%%time\n\nstart = time()\n\nworkflow.transform(valid_dataset).to_parquet(output_path=output_valid_dir, out_files_per_proc=5)\n\ngpu_time += time() - start","1d6c96a5":"print('Dask\/CPU time: ' + str(cpu_time) + ' in seconds')\nprint('NVTabular\/GPU time: ' + str(gpu_time) + ' in seconds')\nprint('Speed-up: ' + str(cpu_time\/gpu_time))","52acbcdb":"# We use the NVTabular dataset object and convert it to a dask_cudf dataframe\n\ndf = train_dataset.to_ddf()\ndf.head()","9b253f45":"df['I1'].mean().compute(), df['I1'].std().compute()","6ce2ed35":"del df","f2f6e373":"df = cudf.read_parquet(glob.glob(output_train_dir + '\/*.parquet')[0], num_rows=10000)\ndf.head()","07478cec":"df['I1'].mean(), df['I1'].std()","23cdfa65":"df['C17'].min(), df['C17'].max(), df['C17'].drop_duplicates().shape[0]","a9f98340":"del df","69c24e2a":"import gc\ngc.collect()","3e34fe1c":"BATCH_SIZE = int(os.environ.get('BATCH_SIZE', 1024*32))           # Batch-size for training neural networks\nPARTS_PER_CHUNK = int(os.environ.get('PARTS_PER_CHUNK', 1))       # How many chunks the data loader uses for shuffling","5ecb5418":"!nvidia-smi","b276373f":"train_paths = glob.glob(os.path.join(output_train_dir, \"*.parquet\"))\nvalid_paths = glob.glob(os.path.join(output_valid_dir, \"*.parquet\"))\ntrain_paths, valid_paths","1c0b56e6":"train_data = nvt.Dataset(train_paths, engine=\"parquet\")\nvalid_data = nvt.Dataset(valid_paths, engine=\"parquet\")","e46434e7":"train_data_itrs = TorchAsyncItr(\n    train_data,\n    batch_size=BATCH_SIZE,\n    cats=CATEGORICAL_COLUMNS,\n    conts=CONTINUOUS_COLUMNS,\n    labels=['label'],\n    parts_per_chunk=PARTS_PER_CHUNK\n)\nvalid_data_itrs = TorchAsyncItr(\n    valid_data,\n    batch_size=BATCH_SIZE,\n    cats=CATEGORICAL_COLUMNS,\n    conts=CONTINUOUS_COLUMNS,\n    labels=['label'],\n    parts_per_chunk=PARTS_PER_CHUNK\n)","3f4f67aa":"def gen_col(batch):\n    return (batch[0], batch[1], batch[2].long())","4c27069b":"train_dataloader = DLDataLoader(train_data_itrs, collate_fn=gen_col, batch_size=None, pin_memory=False, num_workers=0)\nvalid_dataloader = DLDataLoader(valid_data_itrs, collate_fn=gen_col, batch_size=None, pin_memory=False, num_workers=0)\ndatabunch = TabularDataLoaders(train_dataloader, valid_dataloader)","54e0e217":"embeddings = list(get_embedding_sizes(workflow).values())\nembeddings","de995d19":"model = TabularModel(emb_szs=embeddings, n_cont=len(CONTINUOUS_COLUMNS), out_sz=2, layers=[512, 256]).cuda()\nlearn =  Learner(databunch, model, loss_func = torch.nn.CrossEntropyLoss(), metrics=[accuracy], cbs=ProgressCallback())","94d009ca":"from fastai.callback.schedule import fit_one_cycle","64e9e68c":"learning_rate = 1.32e-2\nepochs = 1\nstart = time()\n#learn.fit(epochs, learning_rate)\nfit_one_cycle(learn, n_epoch=epochs, lr_max=learning_rate)\nt_final = time() - start\nprint(t_final)","43aacf07":"Now, we collect statistics on the train set by calling the `fit` function on the `train_dataset`. For example, the mean and std for normalizing the continuous features OR the frequency count of unique categorical values from `Categorify`.","eb4abac3":"We initialize the FastAI `TabularModel`, as usual. It requires no code changes.","4dc542f6":"We can visualize our calculation pipeline with `graphviz` by calling `.graph`. Note, that our calculation pipeline is a DAG (directed acyclic graph).<br><br>We only applied a single operator, therefore, the resulting DAG is pretty simple.","ed942af2":"Let's delete all unused objects.","adbfa23e":"First, we need to categorify the categorical input features with a frequency treshold of 30. This converts the categorical values of a feature into continuous integers (0, ..., C), which is required by an embedding layer of a neural network.<br><br>\nWe define the column names for categorical features.","ac5e9ba7":"Let\u2019s first check our GPU memory","df752f20":"We persist the data to disk.","5d6f8e0b":"We initialize NVTabular datasets.","554506ac":"Install NVTabular. As mentioned, we will use the new NVTabular API. We will install NVTabular from GitHub, using a specific commit. Later, we will change this to the v0.4 release.","5f651b0e":"<a name=\"nvtabular\"><\/a>\n## Feature Engineering and Preprocessing with NVTabular\n\nNow, we are ready to define our feature engineering and preprocessing pipeline using NVTabular.<br><br>\nWe use NVTabular to define the data pipeline:\n1. Categorical features: Use `Categorify` to convert categorical values from String to continuous integer 0, ..., C with C the cardinality of the features. We apply a frequency treshhold of 30 to group low frequent categories together.\n2. Continuous features: `NaN` values are filled with 0, clipped to a minimum of 0, applied with a logarithm function and normalized to mean=0 and std=1.\n\nIn NVTabular most commands do not trigger an calculation. They register and define the pipeline, but do not require significant time. We will measure only the time for commands, which trigger an execution (`fit` and `transform`). ","752e025f":"We initialize the FastAI `TabularDataLoaders` based on the NVTabular data loader. ","2315ad86":"Let's build our next pipeline. We need to process the continuous input features by filling in missing values, clipping, applying logarithm function and normalizing them.\n\nEach `op` returns a new `ColumnGroup`. We can chain these operations by using `>>` multiple times.","5a511d99":"We train our model for 1 epoch.","622d8e51":"# Faster ETL for Tabular Data\n\n## Overview\n\nMachine learning problems for tabular data often require to manual create feature and preprocess the dataset before training a model. A common strategy is to load the full dataset into host (or GPU) memory and develop the data pipeline. Kaggle.com provides NVIDIA T4 GPU with 16GB GPU memory and integrated [RAPIDs cuDF](https:\/\/github.com\/rapidsai\/cudf), a GPU-accelerated dataframe library, into their containers to enable GPU-accelerated dataframe manipulations. However, these dataframe manipulations are bounded by host memory (13GB) or GPU memory (16GB). Tabular data science competitions or real-world industry problems can easily scale to 100GB-10TB datasets.<br><br>\n**How to process tabular datasets which are bigger than host \/ GPU memory?**<br>\nIn the end, we need to design a data pipeline, which iterates over subsets (chunks) of the datasets and apply the transformations. However, some transformations require to calculate statistics over the full dataframe. For example, normalization requires to collect the mean and std over the full dataset. In this notebook, we will show how to use **NVTabular**, an easy-to-use and open source ETL library, to execute feature engineering and preprocessing pipelines on GPU without being bounded on the GPU memory.<br><br>\n**What will we learn?**<br>\nDeep Learning models often require to normalize numerical input features to mean=0 and std=1. Categorical features should be transformed to continuous integers from 0, ..., C with C is the cardinality (unique values per categorical features). Both transformation require to collect statistics over the full dataset. We will use the high-level API of NVTabular to process the Criteo Data Set, that it is ready to use for deep learning.<br><br>\n**Dataset:**<br>\nWe use the [preprocessed Criteo dataset](https:\/\/www.kaggle.com\/benediktschifferer\/criteo-dataset-parquet) in parquet format from the [Criteo Display Advertising Challenge](https:\/\/www.kaggle.com\/c\/criteo-display-ad-challenge). This [notebook](https:\/\/www.kaggle.com\/benediktschifferer\/preprocess-criteo-to-parquet\/) converted a [subset of the dataset](https:\/\/www.kaggle.com\/mrkmakr\/criteo-dataset) to parquet.<br><br>\n\n### NVTabular:\n\nNVTabular is a feature engineering and preprocessing library for tabular data designed to quickly and easily manipulate terabyte scale datasets used to train deep learning based recommender systems. It provides a high level abstraction to simplify code and accelerates computation on the GPU using the RAPIDS cuDF library.<br><br>\nGitHub: https:\/\/github.com\/NVIDIA\/NVTabular\/tree\/main\/nvtabular<br><br>\nWe observed a speed-up of **3800x**, pushing the feature engineering and preprocssing for Criteo Ads Click Prediction from [5 days to 1.9 minutes](https:\/\/developer.nvidia.com\/blog\/announcing-the-nvtabular-open-beta-with-multi-gpu-support-and-new-data-loaders\/). This notebooks is an example for a subset of the kaggle dataset. We adapted it for the Kaggle platform based on the official tutorials: [optimize-criteo.ipynb](https:\/\/github.com\/NVIDIA\/NVTabular\/blob\/main\/examples\/optimize_criteo.ipynb) and [criteo-example.ipynb](https:\/\/github.com\/NVIDIA\/NVTabular\/blob\/main\/examples\/criteo-example.ipynb).","b88cae87":"We initialize a NVTabular Dataset, which iterates over subsets\/chunks of dataset to avoid `OutOfMemory` errors. `train_paths` and `valid_paths` are lists of filenames.","d5357a33":"We can take a look at a categorical feature. `C17` has 9 unique values and the minimum is 1 and the maximum is 10. `C17`'s values are continuous incremental integers. The value `0` is reserved for categories in the validation set, which are not observable in the training dataset.","2ac9fefb":"## Feature Engineering and Preprocessing with Dask on CPU","4cf275aa":"We apply the workflow to the validation dataset. Since we split the validation set from the training set, it is reasonable to assume the validation set follows the same data distribution as the train set. Here, we use the collected statistics from the training dataset to transform the validation dataset.","48514cf6":"## Reviewing the data\n\nBefore moving on to training, let's take a brief look at the original dataset and the preprocessed dataset to verify our preprocess operations.\n\nLet's start with the original dataset. We can see that the continuous values have missing values and are not normalized. The categorical values have a huge range as Integers.","39706209":"First, we import the required libraries.","6c52ba11":"### Note: The Dask CPU section will take 30min to run. If you are interested in NVTabular, you can continue with the [next section.](#nvtabular)","171b31e7":"We visualize our pipeline for continuous features.","d960a78b":"Finally, we can combine both pipelines. We should not forget the label column. As the label column is in neither pipelines, it would not be part of the output. We need to add it to the workflow.","2a8fd03b":"First, we will execute the feature engineering and preprocessing with dask on CPU to measure the calculation time.<br><br>\nWe use the following pipeline:\n1. Categorical features: Use `Categorify` to convert categorical values from String to continuous integer 0, ..., C with C the cardinality of the features. We apply a frequency treshhold of 30 to group low frequent categories together.\n2. Continuous features: `NaN` values are filled with 0, clipped to a minimum of 0, applied with a logarithm function and normalized to mean=0 and std=1.\n\nAs we want to only measure the calculation time, we explain less the steps in the dask workflow.","d91be0d4":"We define some hyperparameters.","9eafab92":"## Training a Neural Network with FastAI\n\nOur dataset is ready to train a deep learning model. In this example, we use the [FastAI](https:\/\/github.com\/fastai\/fastai) library. As a bonus, we will use the NVTabular data loader for PyTorch to feed our dataset to the deep learning model. Data loading is often a bottleneck for Tabular Deep Learning models. NVTabular additionally provides an easy-to-use data loader, which can stream the data from disk without requiring to load the full dataset into host memory.<br><br>\nLets define some hyperparameters.","10e92a7d":"We define the column names.","579a9bc3":"We initialize an NVTabular data loader for the training and one for the validation dataset. Note that it requires knowing the data schema. Otherwise, it is similar to the PyTorch data loader API.","a88bb486":"First, we copy the data from the `Kaggle dataset` to the local machine to avoid network traffic during execution time.","f292734b":"### Execute our Data Pipeline\n\nWe initialize an NVTabular `Workflow` with our pipeline.","8dcf551a":"Next, we want to apply our transformation to our `train_dataset` and `valid_dataset`. Therefore, we need to define the output paths for each dataset,","6ca045ea":"## Try it out yourself\n\nYou can easily clone this notebook and try out NVTabular yourself. We provide more [operators](https:\/\/nvidia.github.io\/NVTabular\/main\/HowItWorks.html#operations), including Joins, TargetEncoding, CountEncoding, etc. and more complicated examples for [Rossmann Sales Prediction](https:\/\/github.com\/NVIDIA\/NVTabular\/tree\/main\/examples\/rossmann), [RecSys2020](https:\/\/github.com\/NVIDIA\/NVTabular\/blob\/main\/examples\/recsys2020.ipynb) and [Outbrain](https:\/\/github.com\/NVIDIA\/NVTabular\/tree\/main\/examples\/wnd_outbrain).<br><br>\nIn this Kaggle.com notebook, we focused on the GPU-accelerated feature engineering and preprocessing. In our next Kaggle.com notebook, we will benchmark the speed of our data loader (stay tuned).\n\n","dc9ac6a4":"### Note: We will use the new and more flexible NVTabular API, which will be released with v0.4 in February 2021.","f7521e41":"We define `dask` dataframes.","85e5eca1":"## Installing NVTabular on Kaggle\n\nKaggle added cuDF=0.16 support, which enables to use NVTabular in Kaggle kernels. First, be sure that GPU acceleration and Internet are activated.\n\n<img src=\"https:\/\/bsopenbucket.s3-eu-west-1.amazonaws.com\/kaggle\/GPUInternet.png\" width=\"200px\">\n\nNext uninstall typing, as typing creates a conflict to install NVTabular.","c529d336":"Let's compare the calculation time.","973252a4":"Install the library nvtx and dask_cudf.","0e4f2e08":"We define to collect statistics and the pipeline.","a588703d":"We apply the transformation to the train and valid dataset and persist it to disk.","9956047b":"We can visualize the full pipeline.","275c7fd1":"We define our calculation pipeline with the `Categorify` operator with `freq_threshold=30`.","5ae8c020":"### Defining our Data Pipeline\nThe first step is to define the feature engineering and preprocessing pipeline.\n\nNVTabular has already implemented multiple calculations, called ops. An op can be applied to a `ColumnGroup` from an overloaded `>>` operator, which in turn returns a new `ColumnGroup`. A `ColumnGroup` is a list of column names as `String`.\n\n**Example:**<br>\n```python\nfeatures = [ column_name, ...] >> op1 >> op2 >> ...\n```\n\nThis may sounds more complicated as it is. Let's define our first pipeline for the criteo dataset.","124615af":"Now, we can take a look at the preprocessed data. We can see that the continuous features are close to mean=0 and std=1 (we use only a sample of 10,000 rows). We can see, that the categorical values are continuous integers.","0d0121b4":"We define the training and validation input dataset files.","73f5d967":"We collect the statistics to define the embedding tables sizes (input cardinality and embedding dimension). These statistics are collected from the NVTabular workflow.","8bad8874":"To use the NVTabular with the FastAI Tabular Data Learner, we need to rearrange the batches with a custom function."}}