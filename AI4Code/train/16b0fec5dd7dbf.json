{"cell_type":{"7be71abe":"code","e6cc28b4":"code","e7a91fd0":"code","87d847c3":"code","4477e59f":"code","08974118":"code","361d1536":"code","c1117d2f":"code","85d9a119":"code","15532acc":"code","f33374e7":"code","51d43b9b":"code","6ea92c51":"code","5d5100ac":"code","54ad5896":"code","2ed8be6c":"code","87ea16ff":"code","7371f058":"code","74c7737c":"code","1368ddb7":"code","1fc9e69c":"code","be7722a1":"code","ce093166":"code","5bfe27c9":"code","99e4b01d":"code","448bc44c":"code","dcb5ea09":"code","357cebf3":"code","ed6d5370":"code","bab55d53":"markdown","99a36667":"markdown","64ee3cb6":"markdown","7f1387d4":"markdown","0df94369":"markdown","36008b36":"markdown","d395fdb6":"markdown","17c32511":"markdown","ca17c325":"markdown","59f1d167":"markdown","7139dc02":"markdown","aadef718":"markdown","08f97c79":"markdown","6fd29599":"markdown","0aa6e2ce":"markdown","142e0eff":"markdown","230b222d":"markdown","011399a4":"markdown","c1398ebf":"markdown","f4e43134":"markdown","27582271":"markdown","e0faac77":"markdown","a534124d":"markdown","cfbae11f":"markdown","8fd66824":"markdown"},"source":{"7be71abe":"import pandas as pd\nimport numpy as np\nimport os\n!pip install sacremoses\nimport sacremoses\nimport tqdm\nimport re\nimport string\n!pip install sklearn\nfrom sklearn.model_selection import train_test_split","e6cc28b4":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","e7a91fd0":"train.head()","87d847c3":"train.describe()","4477e59f":"# https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub('', text)\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub('', text)\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\ndef text_cleaning(x):\n    return remove_punct(remove_html(remove_URL(remove_emoji(x))))\ntrain['text'] = train['text'].apply(lambda x: text_cleaning(x))\ntest['text'] = test['text'].apply(lambda x: text_cleaning(x))","08974118":"x_test = test.text.values\nx_train, x_val, y_train, y_val = train_test_split(\n    train.text.values, train.target.values, test_size=0.3, random_state=101\n)","361d1536":"print(\"shape of test set:\", x_test.shape)\nprint(\"shape of train set:\", x_train.shape)\nprint(\"shape of val set:\", x_val.shape)\nprint(\"true disaster rate in the train set:\", round(sum(y_train)\/len(y_train), 2))\nprint(\"true disaster rate in the val set:\", round(sum(y_val)\/len(y_val), 2))","c1117d2f":"number = 101\nprint(\"sample text:\", x_train[number])\nprint(\"sample ans:\", \"true disaster\" if y_train[number] else \"not a disaster\")","85d9a119":"def load_glove(path, embedding_dim):\n    with open(path) as f:\n        pad_token, unk_token = \"<PAD>\", \"<UNK>\"\n        token_li = [pad_token, unk_token] # note that index 1 is <UNK>, which means unknown word\n        embedding_ls = [np.zeros(embedding_dim), np.random.rand(embedding_dim)]\n        for line in f:\n            token, raw_embedding = line.split(maxsplit=1)\n            token_li.append(token)\n            embedding_ls.append(np.array([float(i) for i in raw_embedding.split()]))\n    return token_li, np.array(embedding_ls)\npath, embedding_dim = \"..\/input\/glove6b300d-50k\/glove.6B.300d__50k.txt\", 300\nvocab, embeddings = load_glove(path, embedding_dim)","15532acc":"def featurize(data, labels, tokenizer, vocal, max_seq_len = 128):\n    voc_to_idx = {word:idx for idx, word in enumerate(vocab)}\n    text_data = [[voc_to_idx.get(token, 1) for token in tokenizer.tokenize(text.lower())] for text in tqdm.tqdm_notebook(data)]\n    label_data = labels\n    return text_data, label_data\ntokenizer = sacremoses.MosesTokenizer()\ntrain_idc, train_lab = featurize(x_train, y_train, tokenizer, vocab)\nval_idc, val_lab = featurize(x_val, y_val, tokenizer, vocab)","f33374e7":"print(\"\\nTrain text first 5 examples:\\n\", train_idc[:5])\nprint(\"\\nTrain label first 5 examples:\\n\", train_lab[:5])","51d43b9b":"import torch\nfrom torch.utils.data import dataloader, Dataset\n\nclass data_loader(Dataset):\n    def __init__(self, data_li, target_li, max_sen_len=128):\n        self.data_li = data_li\n        self.target_li = target_li\n        self.max_sen_len = max_sen_len\n        assert (len(self.data_li) == len(self.target_li))\n    def __len__(self):\n        return len(self.data_li)\n    def __getitem__(self, key, max_sen_len=None):\n        if not max_sen_len: max_sen_len = self.max_sen_len\n        token_idx = self.data_li[key][:max_sen_len]\n        label = self.target_li[key]\n        return [token_idx, label]\n    def collate_func(self, batch):\n        data_list = [] # store padded sequences\n        label_list = []\n        max_batch_seq_len = min(len(max(batch, key=lambda x: len(x[0]))[0]), 128)\n        for row in batch:\n            sen_len = len(row[0])\n            data_list.append(row[0]+[0]*(max_batch_seq_len-sen_len) if sen_len < max_batch_seq_len else row[0][:max_batch_seq_len])\n            label_list.append(row[1])\n        return [torch.tensor(np.array(data_list)), torch.tensor(np.array(label_list))]\nbatch_size, max_sen_len = 64, 60\ntrain_dataset = data_loader(train_idc, train_lab, max_sen_len)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, collate_fn=train_dataset.collate_func, shuffle=True)\nval_dataset = data_loader(val_idc, val_lab, train_dataset.max_sen_len)\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=train_dataset.collate_func, shuffle=False)","6ea92c51":"data_batch, labels = next(iter(train_loader))\nprint(\"data_batch\\n\", \"shape: \", data_batch.shape, \"\\n\", \"sample:\", data_batch)\nprint(\"labels\\n\", \"shape: \", labels.shape, \"\\n\", \"sample:\", labels)","5d5100ac":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nclass LSTMClassifier(nn.Module):\n    def __init__(self, embeddings, hidden_size, num_layers, num_class, bidirectional, dropout_prob=0.3):\n        super().__init__()\n        self.embedding_layer = self.load_pretrain_embeddings(embeddings)\n        self.embeddings_dim = embeddings.shape[1]\n        self.bilstm = nn.LSTM(input_size=embeddings.shape[1], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout_prob, bidirectional=bidirectional)\n        self.dropout = nn.Dropout(p=dropout_prob)\n        self.non_linear = nn.ReLU()\n        self.clf = nn.Linear(2*hidden_size, num_classes) # classifier layer: 2 is due to bidirectional\n        self.maxpool2 = nn.MaxPool2d(kernel_size=1)\n    def load_pretrain_embeddings(self, embeddings):\n        embedding_layer = nn.Embedding(embeddings.shape[0], embeddings.shape[1], padding_idx=0)\n        embedding_layer.weight.data = torch.Tensor(embeddings).float()\n        return embedding_layer\n    def forward(self, inputs):\n        X = self.embedding_layer(inputs)\n        bilstm_out, (h_n, c_n) = self.bilstm(X)\n        out = torch.max(input=bilstm_out, dim=1)\n        out = self.non_linear(out.values)\n        logits = self.clf(out)\n        return logits","54ad5896":"def evaluate(model, dataloader, device):\n    model.eval()\n    with torch.no_grad():\n        all_preds, all_labels = [], []\n        for batch_text, batch_label in dataloader:\n            y_preds = model(batch_text.to(device))\n            all_preds.append(y_preds.detach().cpu().numpy())\n            all_labels.append(batch_label.numpy())\n    preds, labels = np.concatenate(np.array(all_preds), axis = 0), np.concatenate(np.array(all_labels), axis = 0)\n    return (preds.argmax(-1)==labels).mean()","2ed8be6c":"hidden_size = 32\nnum_layers = 1\nnum_classes = 2\nbidirectional = True\ntorch.manual_seed(1234)\ndevice = torch.device('cpu')\n# device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device('cpu')\n# we will use cpu and leave the GPU resource for method 2\nprint(\"device: \", device)\nmodel1 = LSTMClassifier(embeddings, hidden_size, num_layers, num_classes, bidirectional)\nmodel1.to(device)\ncriterion = nn.CrossEntropyLoss()\nlearning_rate = 0.005\noptimizer = optim.Adam(model1.parameters(), lr=learning_rate)","87ea16ff":"print(\"model structure:\\n\", model1)","7371f058":"train_loss_history = []\nval_acc_history = []\nbest_val_accuracy = 0\ntolerance = 0\nearly_stop_patience = 2\nnum_epochs = 10\n  \nfor epoch in tqdm.tqdm_notebook(range(num_epochs)):\n    model1.train()\n    for i, (batch_data, batch_label) in enumerate(train_loader):\n        y_preds = model1(batch_data.to(device))\n        loss = criterion(y_preds, batch_label.to(device)) # note that the prediction value need to be infront of the true value\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        train_loss_history.append(loss.item())\n    val_acc = evaluate(model=model1, dataloader=val_loader, device=device)\n    val_acc_history.append(val_acc)\n    torch.save(model1, \"pytorch_bilstm_best.pt\")    \n    print(\"epoch: {}; val_accuracy: {}\".format(epoch, val_acc))\n    if val_acc > best_val_accuracy: best_val_accuracy = val_acc\n    else: tolerance += 1\n    if tolerance > early_stop_patience: break   \nprint(\"Best validation accuracy is: \", best_val_accuracy)","74c7737c":"test_idc, test_lab = featurize(x_test, [0]*len(x_test), tokenizer, vocab) # test label is fake data\ntest_dataset = data_loader(test_idc, test_lab, max_sen_len)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, collate_fn=test_dataset.collate_func, shuffle=True)","1368ddb7":"model1.eval()\nwith torch.no_grad():\n    all_preds = []\n    for batch_text, _ in test_loader:\n        preds = model1(batch_text.to(device))\n        all_preds.append(preds.detach().cpu().numpy())\n    all_preds = np.concatenate(np.array(all_preds), axis = 0)\n\npred_res1 = all_preds.argmax(-1)","1fc9e69c":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n# gpus = tf.config.experimental.list_physical_devices('GPU')\n# tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\nimport tokenization","be7722a1":"bert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\", trainable=True)\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","ce093166":"text = \"Coronavirus: WHO advises to wear masks in public areas\"\ntokenize_data = tokenizer.tokenize(text)\nprint(\"Text after tokenization:\\n\", tokenize_data, \"\\n\")\n\nmax_len = 20\ntext = tokenize_data[:max_len-2]\ninput_seq = [\"[CLS]\"] + text + [\"[SEP]\"]\nprint(\"After adding [CLS] and [SEP]:\\n\", input_seq, \"\\n\")\n\npad_len = max_len-len(input_seq)\ntokens = tokenizer.convert_tokens_to_ids(input_seq)+[0]*pad_len\nprint(\"After converting Tokens to Id and adding the pad:\\n\", tokens, \"\\n\")\n\npad_masks = [1]*len(input_seq) + [0]*pad_len\nprint(\"Pad Masking:\\n\", pad_masks, \"\\n\")","5bfe27c9":"def pre_process(context_data, tokenizer, max_len=128):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    for text in context_data:\n        input_seq = [\"[CLS]\"]+ tokenizer.tokenize(text)[:max_len-2] + [\"[SEP]\"] # 2 for [CLS] and [SEP]\n        pad_len = max_len-len(input_seq)\n        tokens = tokenizer.convert_tokens_to_ids(input_seq) + [0]*pad_len\n        pad_masks = [1]*len(input_seq) + [0]*pad_len\n        segment_ids = [0]*max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","99e4b01d":"max_len = 128\ntrain_input = pre_process(train.text.values, tokenizer, max_len)\ntrain_label = train.target.values","448bc44c":"input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\ninput_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\nsegment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\npooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n# Fine Tuning\nclf_output = sequence_output[:, 0, :]\nout = Dense(1, activation='sigmoid')(clf_output)\n\nmodel2 = Model(\n    inputs=[input_word_ids, input_mask, segment_ids],\n    outputs=out\n)\nmodel2.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\nmodel2.summary()","dcb5ea09":"checkpoint = ModelCheckpoint('keras_bert_model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_label,\n    validation_split=0.3,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)","357cebf3":"model2.load_weights('keras_bert_model.h5')\ntest_input = pre_process(test.text.values, tokenizer, max_len)\npred_res2 = model2.predict(test_input).round().astype(int)","ed6d5370":"pred_res = pred_res2 # pred_res1 or pred_res2\n\npred_res = np.array(sorted(np.array([list(test[\"id\"])]+[list(pred_res)]).T, key=lambda x: x[0]))\n\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nassert(list(submission.id)==list(pred_res[:,0])) # check the number of test data\nsubmission[\"target\"] = pred_res[:,1]\nsubmission.to_csv('submission.csv', index=False)","bab55d53":"Start training now and validate at the same time with tolerances","99a36667":"## Package Import","64ee3cb6":"If this is your first time to use keras, highly recommend [TensorFlow in Practice Specialization](https:\/\/www.coursera.org\/specializations\/tensorflow-in-practice) in the Coursera. ","7f1387d4":"## Method 1: GloVe + BiLSTM with Pytorch\nThe code below was referred from the NYU [Nature Language Understanding](https:\/\/cims.nyu.edu\/~sbowman\/teaching.shtml) Course taught by [Sam Bowman](https:\/\/cims.nyu.edu\/~sbowman\/index.shtml)","0df94369":"[Tokenize](https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/tokenization-1.html):\n\nsentence -> tokenize\n\n\"Coronavirus: WHO advises to wear masks in public areas\" -> ['Coronavirus', ':', 'WHO', 'advises', 'to', 'wear', 'masks', 'in', 'public', 'areas']","36008b36":"## Data Preprocessing","d395fdb6":"If you want to know more about the transformer and the bert, I had listed two videos below. These are the best tutorial I know so far.\n* [Transformer Neural Networks - EXPLAINED! (Attention is all you need)](https:\/\/www.youtube.com\/watch?v=TQQlZhbC5ps&t=2s)\n* [BERT Neural Network - EXPLAINED!](https:\/\/www.youtube.com\/watch?v=xI0HHN5XKDo)","17c32511":"Check data loader:","ca17c325":"Define the evaluation function","59f1d167":"This is the first competition that I participated in. I hope the kernel will provide some intuitive to novice for building their deep learning model.\n\n**Please note you have to run the method 2 first, otherwise there will be a source allocation issue on GPU**","7139dc02":"## Sumbit the result","aadef718":"### BiLSTM model\n\nBiLSTM is the bidirectional LSTM model. It can capture the both front and back information per vocabulary for a sentence\n\nMore for BiLSTM:\n* [Bi-LSTM](https:\/\/medium.com\/@raghavaggarwal0089\/bi-lstm-bc3d68da8bd0)\n* [The Bidirectional Language Model](https:\/\/medium.com\/@plusepsilon\/the-bidirectional-language-model-1f3961d1fb27)","08f97c79":"Test evaluation","6fd29599":"### Text Cleaning","0aa6e2ce":"Tuning hyperparameters","142e0eff":"Forming training dataset, validation dataset, testing dataset","230b222d":"Check the tokenize result below:","011399a4":"Model construction","c1398ebf":"### Evaluate the test set\n\nConstruct test data loader","f4e43134":"## Method 2: Bert sample on TfHub with Keras\n\nThe following code has referred the web page below:\n* [bert_en_uncased_L-24_H-1024_A-16](https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/2)\n* [Disaster NLP: Keras BERT using TFHub](https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub)","27582271":"Load model with test evaluation","e0faac77":"### Pytorch: \n\nIf this is the first time you use the pytorch, highly recommend that go and check out the [DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ](https:\/\/pytorch.org\/tutorials\/beginner\/deep_learning_60min_blitz.html) first before scroll down below","a534124d":"Preprocess with adding [CLS], [SEP], padding, and tokenize","cfbae11f":"## Kernel Explanation\nThis Kernel constructed the model with two different kinds of methods\n\n1. BiLSTM (pytorch + GloVe)\n2. Bert (keras + TfHub)","8fd66824":"### GloVe:\n\nGloVe is an unsupervised learning algorithm for obtaining vector representations for words. More explanations have been attached below:\n* [GloVe: Global Vectors for Word Representation](https:\/\/nlp.stanford.edu\/projects\/glove\/)\n* [NLP \u2014 Word Embedding & GloVe](https:\/\/medium.com\/@jonathan_hui\/nlp-word-embedding-glove-5e7f523999f6)\n* [GloVe\u8be6\u89e3 - \u8303\u6c38\u52c7](www.fanyeong.com\/2018\/02\/19\/glove-in-detail\/) (If you can understand Mandarin, this is a good source to learn from)"}}