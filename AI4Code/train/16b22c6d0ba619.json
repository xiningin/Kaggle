{"cell_type":{"c76838e2":"code","e001f631":"code","38046f4d":"code","578b170c":"code","a16dc9f6":"code","1bb4f09e":"code","95bfc7e8":"code","754ca675":"code","1f689b7f":"code","3f144c48":"code","419dfb1a":"markdown","e9153e4b":"markdown","a406a3c1":"markdown","57a95ece":"markdown","a006d703":"markdown","f1e8f84d":"markdown","10514fb9":"markdown","b57a244b":"markdown","0e1d0dc1":"markdown","811285e2":"markdown","e2dc39a6":"markdown","d3729c7b":"markdown","2b2b2e3c":"markdown","e598a070":"markdown"},"source":{"c76838e2":"import pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom lwoku import RANDOM_STATE, N_JOBS, VERBOSE, get_prediction\nfrom grid_search_utils import plot_grid_search, table_grid_search\n\nimport pickle","e001f631":"# Read training and test files\nX_train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id', engine='python')\nX_test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id', engine='python')\n\n# Define the dependent variable\ny_train = X_train['Cover_Type'].copy()\n\n# Define a training set\nX_train = X_train.drop(['Cover_Type'], axis='columns')","38046f4d":"VERBOSE=1","578b170c":"ab_clf = AdaBoostClassifier(random_state=RANDOM_STATE)","a16dc9f6":"parameters = {\n    'n_estimators': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 20, 30]\n}\nclf = GridSearchCV(ab_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","1bb4f09e":"parameters = {\n    'learning_rate': [(0.97 + x \/ 100) for x in range(0, 8)]\n}\nclf = GridSearchCV(ab_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","95bfc7e8":"parameters = {\n    'algorithm': ['SAMME', 'SAMME.R']\n}\nclf = GridSearchCV(ab_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","754ca675":"parameters = {\n    'n_estimators': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 20],\n    'learning_rate': [(0.97 + x \/ 100) for x in range(0, 8)],\n    'algorithm': ['SAMME', 'SAMME.R']\n}\nclf = GridSearchCV(ab_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","1f689b7f":"with open('clf.pickle', 'wb') as fp:\n    pickle.dump(clf, fp)","3f144c48":"clf.best_estimator_","419dfb1a":"# learning_rate\n##### : float, optional (default=1.)\n\nLearning rate shrinks the contribution of each classifier by\n``learning_rate``. There is a trade-off between ``learning_rate`` and\n``n_estimators``.\n","e9153e4b":"# n_estimators\n##### : integer, optional (default=50)\n\nThe maximum number of estimators at which boosting is terminated.\nIn case of perfect fit, the learning procedure is stopped early.\n","a406a3c1":"# Exhaustive search","57a95ece":"# Prepare data","a006d703":"## Export grid search results","f1e8f84d":"# algorithm\n##### : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n\nIf 'SAMME.R' then use the SAMME.R real boosting algorithm.\n``base_estimator`` must support calculation of class probabilities.\nIf 'SAMME' then use the SAMME discrete boosting algorithm.\nThe SAMME.R algorithm typically converges faster than SAMME,\nachieving a lower test error with fewer boosting iterations.","10514fb9":"The previous assumptions have not been very successful, since the parameters depend on each other.","b57a244b":"SAMME algorithm seems to have better score.","0e1d0dc1":"# Search over parameters","811285e2":"# base_estimator\n##### : object, optional (default=None)\n\nThe base estimator from which the boosted ensemble is built.\nSupport for sample weighting is required, as well as proper\n``classes_`` and ``n_classes_`` attributes. If ``None``, then\nthe base estimator is ``DecisionTreeClassifier(max_depth=1)``","e2dc39a6":"The relation between the estimators and the score is pretty chaotic.\nThe best score is with 6 estimators.\nAn even number scores more than an odd number.","d3729c7b":"There's no visible relation between `learning_rate` and score.","2b2b2e3c":"# Introduction\n\nThe aim of this notebook is to optimize the Extra-trees model.\n\nFirst, all [AdaBoost classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) parameters are analysed separately.\n\nThen, a grid search is carried out.\nThis is a search through all the combinations of parameters,\nwhich optimize the internal score in the train set.\n\nThe results are collected at [Tactic 03. Hyperparameter optimization](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization).","e598a070":"**Note**: Not evaluated"}}