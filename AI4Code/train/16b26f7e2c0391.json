{"cell_type":{"80777caf":"code","4acee3dd":"code","114fc525":"code","2a73eaa4":"code","b2fbf3a1":"code","3faaa9e5":"code","c2d2366c":"code","8046b92c":"code","c68537f4":"code","ead0a4bc":"code","678fa19a":"code","f177b747":"code","0eb9dd7d":"code","3fd845d5":"code","8ab9e72c":"code","468ca8b9":"code","26359252":"code","a888b849":"code","0ab9e760":"code","1dee6f79":"code","3d8f6739":"code","271c46ce":"code","b5f14a9c":"code","a047cb00":"code","fc6a46f6":"code","b45db08f":"code","4678621f":"code","c4c9e6d1":"markdown","dbd9d11a":"markdown","e22d8fdb":"markdown","5fb5a0ec":"markdown"},"source":{"80777caf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4acee3dd":"df=pd.read_csv('\/kaggle\/input\/california-house-prices\/train.csv')\ndf.info()\ndf.sample(3)","114fc525":"df_train=df.copy()\ndf_test=pd.read_csv('\/kaggle\/input\/california-house-prices\/test.csv')\nfor field in ['Listed On', 'Last Sold On']:\n    df_train[field]=pd.to_datetime(df[field])\n    df_test[field]=pd.to_datetime(df_test[field])","2a73eaa4":"cate_cols = []\nnum_cols = []\ndate_cols = []\ndtypes = df_train.dtypes\nfor col, dtype in dtypes.items():\n    if dtype=='object':\n        cate_cols.append(col)\n    elif dtype.name.startswith('datetime'):\n        date_cols.append(col)\n    else:\n        num_cols.append(col)","b2fbf3a1":"id_col = 'Id'\ntarget_col = 'Sold Price'\n\nfor col in [id_col, target_col]:\n    num_cols.remove(col)\n\nprint(cate_cols, num_cols, date_cols)","3faaa9e5":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype\n\nclass Num_Features(BaseEstimator, TransformerMixin):\n    def __init__(self, cols = [], fillna = False, addna = False):\n        self.fillna = fillna\n        self.cols = cols\n        self.addna = addna\n        self.na_cols = []\n        self.imputers = {}\n    def fit(self, X, y=None):\n        for col in self.cols:\n            if self.fillna:\n                self.imputers[col] = X[col].median()\n            if self.addna and X[col].isnull().sum():\n                self.na_cols.append(col)\n        print(self.na_cols, self.imputers)\n        return self\n    def transform(self, X, y=None):\n        df = X.loc[:, self.cols]\n        for col in self.imputers:\n            df[col].fillna(self.imputers[col], inplace=True)\n        for col in self.na_cols:\n            df[col+'_na'] = pd.isnull(df[col])\n        return df","c2d2366c":"class Imputer(BaseEstimator, TransformerMixin):\n    def __init__(self, strategy, fill_value):\n        self.strategy = strategy\n        self.fill_value = fill_value\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        for col, content in X.items():\n            X[col].fillna(self.fill_value, inplace=True)\n        return X","8046b92c":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer, StandardScaler\nfrom sklearn.impute import SimpleImputer\nnum_pipeline = Pipeline([\n    ('select_num', Num_Features(cols=num_cols, fillna='median', addna=True)),\n])\nX_num = num_pipeline.fit_transform(df_train)","c68537f4":"class CatEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self,cols, max_n_cat=7, onehot_cols=[], orders={}):\n        self.cols = cols\n        self.onehot_cols=onehot_cols\n        self.cats = {}\n        self.max_n_cat = max_n_cat\n        self.orders = orders\n    def fit(self, X, y=None):\n        df_cat =  X.loc[:, self.cols]\n        for n,c in df_cat.items():\n            df_cat[n].fillna('NAN', inplace=True)\n            df_cat[n] = c.astype('category').cat.as_ordered()\n            if n in self.orders:\n                df_cat[n].cat.set_categories(self.orders[n], ordered=True, inplace=True)\n            cats_count = len(df_cat[n].cat.categories)\n            if cats_count<=2 or cats_count>self.max_n_cat:\n                self.cats[n] = df_cat[n].cat.categories\n                if n in self.onehot_cols:\n                    self.onehot_cols.remove(n)\n            elif n not in self.onehot_cols:\n                self.onehot_cols.append(n)\n\n        print(self.onehot_cols)\n        return self\n    def transform(self, df, y=None):\n        X = df.loc[:, self.cols]\n        for col in self.cats:\n            X[col].fillna('NAN', inplace=True)\n            X.loc[:,col] = pd.Categorical(X[col], categories=self.cats[col], ordered=True)\n            X.loc[:,col] = X[col].cat.codes\n\n#         for n,c in X.items():\n#             if n in self.cats:\n#                 X[n] = pd.Categorical(c, categories=self.cats[n], ordered=True)\n#                 X[n] = X[n].cat.codes + 1\n#             else:\n#                 X[n] = c.astype('category').cat.as_ordered()\n        if len(self.onehot_cols):\n            df_1h = pd.get_dummies(X[self.onehot_cols], dummy_na=True)\n            df_drop=X.drop(self.onehot_cols,axis=1)\n            return pd.concat([df_drop, df_1h], axis=1)\n\n        return X","ead0a4bc":"cat_pipeline = Pipeline([\n    ('cat_encoder', CatEncoder(cols=cate_cols))\n])\nX_cate = cat_pipeline.fit_transform(df_train)","678fa19a":"def add_datepart(df, field_name, prefix=None, drop=True, time=False):\n    field = df[field_name]\n    if prefix is None:\n        prefix = re.sub('[Dd]ate$', '', field_name)\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    if time: attr = attr + ['Hour', 'Minute', 'Second']\n    # Pandas removed `dt.week` in v1.1.10\n    week = field.dt.isocalendar().week.astype(field.dt.day.dtype) if hasattr(field.dt, 'isocalendar') else field.dt.week\n    for n in attr: df[prefix + n] = getattr(field.dt, n.lower()) if n != 'Week' else week\n    mask = ~field.isna()\n    df[prefix + 'Elapsed'] = np.where(mask,field.values.astype(np.int64) \/\/ 10 ** 9,np.nan)\n    if drop: df.drop(field_name, axis=1, inplace=True)\n    return df","f177b747":"import re\nclass Datepart(BaseEstimator, TransformerMixin):\n    def __init__(self, cols, time=False):\n        self.cols = cols\n        self.time = time\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        df_dates = X.loc[:, self.cols]\n        for col in self.cols:\n            add_datepart(df_dates, col, time=False)\n        return df_dates\n    \ndate_pipeline = Pipeline([\n    ('datepart', Datepart(cols=date_cols)),\n    ('imputer', Imputer(strategy=\"constant\", fill_value=-1)),\n])","0eb9dd7d":"X_date = date_pipeline.fit_transform(df_train)","3fd845d5":"y_train = np.log(df_train[target_col])\nX_train = pd.concat([X_num, X_cate,X_date], axis=1)\nX_train.shape, y_train.shape","8ab9e72c":"from sklearn.model_selection import ParameterGrid\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(oob_score=True, random_state=3, n_jobs=-1)\nparams ={\n    'n_estimators': [200],# [300, 400, 500, 600, 700],\n    'min_samples_leaf': [2],# [1, 2, 3, 5, 10, 25],\n    'max_features': [0.5],# [None, 0.5, 'sqrt', 'log2'],\n    'max_depth': [13],# [5, 6, 7, 8, 10, 15, 20],\n    'min_samples_split': [2]# [2, 3, 4]\n}\n\nbest_score = 0\nfor g in ParameterGrid(params):\n    model.set_params(**g)\n    model.fit(X_train, y_train)\n    if model.oob_score_ > best_score:\n        best_score = model.oob_score_\n        best_grid = g\n        print('oob:', best_score, best_grid)","468ca8b9":"from sklearn.ensemble import RandomForestRegressor\nm = RandomForestRegressor(n_jobs=-1, n_estimators=200, oob_score=True, max_depth=17, min_samples_leaf=4, min_samples_split=2, max_features=0.5)\nm.fit(X_train, y_train)\nm.oob_score_","26359252":"def rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}).sort_values('imp', ascending=False)\n\nfi = rf_feat_importance(m, X_train)\nfi[:50]","a888b849":"del_cols = []#'Annual tax amount', 'Bathrooms', 'Last Sold OnYear', 'Summary', 'Address', 'Last Sold OnDayofyear','City','Heating features',\n            #'Last Sold OnDay', 'Listed OnDayofyear', 'Elementary School', 'Listed OnDay', 'Type', 'Full bathrooms',\n           #'High School', 'Middle School', 'Parking']#, 'Listed OnYear' # 'Last Sold OnYear', 'Region', # [ ]\nkeep_cols = ['Listed Price', 'Tax assessed value', 'Annual tax amount', 'Last Sold Price', 'Total interior livable area', 'Zip']\n\nThreshold = 0.0009\nto_keep = fi[fi.imp>Threshold].cols\nto_keep = [col for _, col in to_keep.items()]","0ab9e760":"for col in del_cols:\n    if col in to_keep:\n        to_keep.remove(col)\nfor col in keep_cols:\n    if col not in to_keep:\n        to_keep.append(col)\nprint(to_keep)\ndf_keep = X_train[to_keep].copy()","1dee6f79":"m1 = RandomForestRegressor(n_jobs=-1, random_state=3, n_estimators=300, oob_score=True, max_depth=13, min_samples_leaf=2, min_samples_split=2, max_features=0.5)\nm1.fit(df_keep, y_train)\nprint(m1.oob_score_)","3d8f6739":"m1.oob_score_","271c46ce":"cols = to_keep\nscores = []\nfeats = []\nfor col in cols:\n    tmp = to_keep.copy()\n    if col in keep_cols:\n        continue\n    tmp.remove(col)\n    df_tmp = X_train[tmp].copy()\n    m1 = RandomForestRegressor(n_jobs=-1, random_state=3, n_estimators=30, oob_score=True, max_depth=13, min_samples_leaf=2, min_samples_split=2, max_features=0.5)\n    m1.fit(df_tmp, y_train)\n    scores.append(m1.oob_score_)\n    feats.append(col)\n#     print(col, m1.oob_score_)\n\nto_del = sorted(zip(scores, feats), reverse=True)\nto_del","b5f14a9c":"# \u6700\u597d\u63d0\u4ea4\u7684\u7279\u5f81\uff0c18\u4e2a\nto_keep_final=['Listed Price', 'Tax assessed value', 'Last Sold Price', 'Zip', 'Total interior livable area', 'Elementary School Score', \n'Listed OnElapsed', 'Last Sold OnElapsed',\n'Full bathrooms', 'Year built', \n'Listed OnYear', \n'Lot', \n'Parking','Type', 'Middle School Score', 'High School Distance', 'Elementary School Distance', 'Bedrooms']\n# to_keep_final=['Listed Price', 'Tax assessed value', 'Last Sold Price', 'Zip', 'Total interior livable area', 'Listed OnElapsed', 'Elementary School Score', 'Last Sold OnElapsed', 'Year built', 'Listed OnYear', 'High School Distance', 'Lot', 'Parking', 'Middle School Score', 'Elementary School Distance', 'Region', 'Bedrooms', 'High School Score', 'Heating', 'Appliances included', 'Flooring', 'Middle School Distance']\nX_train_final = X_train[to_keep_final].copy()","a047cb00":"# 2nd pass grid search to determine the final parameters\nfrom sklearn.model_selection import ParameterGrid\nmodel = RandomForestRegressor(oob_score=True, random_state=3, n_jobs=-1, max_features=0.5)\nparams ={\n    'n_estimators': [500],# [400, 500, 600, 700, 900, 1000, 1100],\n    'min_samples_leaf': [2],# [1, 2, 3, 5, 10, 25],\n    'max_features': [0.5],# [0.5, 'sqrt', 'log2'],\n    'max_depth': [10],# [5, 6, 7, 8],\n    'min_samples_split': [2]# [2, 3, 4]\n}\n\nbest_score = 0\nfor g in ParameterGrid(params):\n    model.set_params(**g)\n    model.fit(X_train_final, y_train)\n    if model.oob_score_ > best_score:\n        best_score = model.oob_score_\n        best_grid = g\n        print('best oob:', best_score, best_grid)","fc6a46f6":"# \u6700\u597d\u6210\u7ee9\u7684\u8d85\u53c2\u6570\nmodel_final = RandomForestRegressor(n_jobs=-1, n_estimators=550, max_depth=17, min_samples_leaf=4, min_samples_split=2, max_features=0.45)\nmodel_final.fit(X_train_final, y_train)","b45db08f":"X_test_num = num_pipeline.transform(df_test)\nX_test_cate = cat_pipeline.transform(df_test)\nX_test_date = date_pipeline.transform(df_test)\ndf_t = pd.concat([X_test_num, X_test_cate, X_test_date], axis=1)\ndf_t = df_t[to_keep_final]","4678621f":"pred=model_final.predict(df_t)\ndf_pred=pd.DataFrame({'Id':df_test['Id'],'Sold Price': np.exp(pred)})\nprint(df_pred.head())\ndf_pred.to_csv('submission.csv', index=False)","c4c9e6d1":"# Hyper Parameter Tuning\nuse grid search to produce a baseline model","dbd9d11a":"\u6700\u6734\u7d20\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u627e\u51fa\u53ef\u4ee5\u53bb\u9664\u7684\u7279\u5f81\uff0c\u53cd\u590d\u505a\u4e0d\u65ad\u63d0\u9ad8oob_score","e22d8fdb":"## Feature engineering\n\u4ee5\u4e0b\u662f\u4e00\u4e9b\u5e73\u51d1\u7684\u4ee3\u7801\u3002\u76ee\u6807\u662f\u505a\u534a\u81ea\u52a8\u7279\u5f81\u5de5\u7a0b\uff0c\u6709\u4e9b\u529f\u80fd\u5e76\u672a\u5b9e\u73b0\uff0c\u53ef\u80fd\u6709bug\u3002\u5c31\u672c\u9879\u76ee\u6765\u8bf4\u8fd8\u80fd\u7528","5fb5a0ec":"# Feature Selection\nfeature importance"}}