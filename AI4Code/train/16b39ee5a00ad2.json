{"cell_type":{"2b813bf9":"code","f10b9f25":"code","897cf81a":"code","b9e7a2b0":"code","d7389d16":"code","f912ce59":"code","cada0836":"code","b8f7d904":"code","07db68a2":"code","d08c4bd0":"code","28145a03":"code","1008b7fc":"code","350bd887":"code","82f2ef03":"code","9d3d381b":"code","85404adc":"code","6502c46d":"code","09e319f2":"code","98bdea26":"code","aa11078f":"code","a552ad32":"code","2e27a735":"code","aa82afa9":"code","26e1f196":"code","a17301c7":"code","dedf1007":"code","dceab34f":"code","a18b8e8f":"code","59278288":"code","9b9c2dd4":"code","7ecfba94":"code","f12e8305":"code","9e16d090":"code","5c247c31":"code","85caba6f":"code","8c63161f":"code","5a74277a":"code","64d97984":"code","ad4b1d9e":"code","2b861e9c":"code","d6293af3":"code","a3c9d652":"code","e526cdeb":"code","f21cb593":"markdown","36f08a04":"markdown","f075a1c8":"markdown","a4b44245":"markdown","a82367f6":"markdown","d3cef0ef":"markdown","13601d1b":"markdown","9aba0db8":"markdown","ee6173cd":"markdown","f381ad49":"markdown","0929cae2":"markdown","c5d41cb9":"markdown","6af8fedd":"markdown","dbc8effd":"markdown","bef5c6e7":"markdown","8278fb96":"markdown","c81aaa26":"markdown","99823ac1":"markdown"},"source":{"2b813bf9":"# The installation of pycaret 2.3.2, the latest version came up with an error.\n# -------\n# ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n# pdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.4.1 which is incompatible.\n# -------\n# Therefore, I decided to install version 2.2.1 which happened to work.\n\n#!python -m pip download pycaret==2.2.1 -d .\/pycaret-2.2.1\n!python -m pip install --find-links=..\/input\/mylibraries pycaret==2.2.1\n!python -m pip install --find-links=..\/input\/mylibraries\/textstat-0.7.1 textstat==0.7.1\n\n# In case internet access is allowed, pip install is the easiest way.\n#!pip install pycaret\n#!pip install textstat","f10b9f25":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport time\n\nimport pandas_profiling as pdp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nfrom nltk import pos_tag\nfrom wordcloud import WordCloud,STOPWORDS\n\nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport textstat\n\nfrom pycaret.regression import *","897cf81a":"train_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")","b9e7a2b0":"train_df.head()","d7389d16":"train_df.excerpt[0]","f912ce59":"fig, ax = plt.subplots(1,3,figsize=(18,6))\n\nsns.histplot(train_df['target'], ax=ax[0], kde=True, alpha=0.2)\nsns.histplot(train_df['standard_error'], ax=ax[1], kde=True, alpha=0.2)\n\nsns.histplot(\n    train_df, x=\"target\", y=\"standard_error\",\n    bins=30, discrete=(False, False), log_scale=(False, False),\n    cbar=True, cbar_kws=dict(shrink=.75), \n    ax=ax[2]\n)\nplt.show()","cada0836":"test_df.head()","b8f7d904":"profile = pdp.ProfileReport(train_df)\nprofile","07db68a2":"pdp.ProfileReport(test_df)","d08c4bd0":"for idx,row in train_df.sort_values(by=['target'], ascending=False).head(3).iterrows():\n    print('index:' + str(idx) + ', target ' + str(row.target))\n    print(row.excerpt + '\\n')","28145a03":"for idx,row in train_df.sort_values(by=['target'], ascending=False).tail(3).iterrows():\n    print('index:' + str(idx) + ', target ' + str(row.target))\n    print(row.excerpt + '\\n')","1008b7fc":"excerpt1 = train_df['excerpt'].min()\nprint(\"Before preprocessing: \\n\")\nprint(excerpt1)","350bd887":"e = re.sub(\"[^a-zA-Z]\", \" \", excerpt1) # \u30a2\u30eb\u30d5\u30a1\u30d9\u30c3\u30c8\u4ee5\u5916\u306f\u7a7a\u767d\u306b\u5909\u63db\ne = e.lower() # \u5c0f\u6587\u5b57\u306b\u5909\u63db\ne = nltk.word_tokenize(e) # tokenizer \u3092\u4f7f\u3063\u3066\u5358\u8a9e\u306b\u5206\u5272\ne[:8]","82f2ef03":"e = [word for word in e if not word in set(stopwords.words(\"english\"))] # stopwords \u306b\u767b\u9332\u3055\u308c\u3066\u3044\u308b\u5358\u8a9e\u306f\u9664\u5916\u3059\u308b\nstopwords.words(\"english\")[:10] # \u3061\u306a\u307f\u306b stopwords \u306b\u767b\u9332\u3055\u308c\u3066\u3044\u308b\u5358\u8a9e\u306f\u3053\u3093\u306a\u306e","9d3d381b":"lemma = nltk.WordNetLemmatizer()\ne = [lemma.lemmatize(word) for word in e] # lemmatizer \u3092\u4f7f\u3063\u3066lemmatize\u3059\u308b\nnltk.WordNetLemmatizer().lemmatize(\"dogs\") # lemmatize \u306e\u4f8b dogs -> dog","85404adc":"e=\" \".join(e)\nprint(\"After preprocessing: \\n\")\nprint(e)","6502c46d":"def preprocess(data):\n    excerpt_processed=[]\n    lemma = nltk.WordNetLemmatizer()\n    for e in data['excerpt']:\n        e = re.sub(\"[^a-zA-Z]\", \" \", e) # \u30a2\u30eb\u30d5\u30a1\u30d9\u30c3\u30c8\u4ee5\u5916\u306f\u7a7a\u767d\u306b\u5909\u63db\n        e = e.lower() # \u5c0f\u6587\u5b57\u306b\u5909\u63db\n        e = nltk.word_tokenize(e) # tokenizer \u3092\u4f7f\u3063\u3066\u5358\u8a9e\u306b\u5206\u5272\n        e = [word for word in e if not word in set(stopwords.words(\"english\"))] # stopwords \u306b\u767b\u9332\u3055\u308c\u3066\u3044\u308b\u5358\u8a9e\u306f\u9664\u5916\u3059\u308b\n        e = [lemma.lemmatize(word) for word in e] # lemmatizer \u3092\u4f7f\u3063\u3066 lemmatize \u3059\u308b\n        e=\" \".join(e)\n        excerpt_processed.append(e)\n    return excerpt_processed ","09e319f2":"train_df['preprocessed_excerpt'] = preprocess(train_df)\ntest_df['preprocessed_excerpt'] = preprocess(test_df)\n\n# \u6642\u9593\u304c\u304b\u304b\u308b\u306e\u3067\u3001\u4fdd\u5b58\u3057\u3066\u304a\u304f\n#train_df.to_csv(\"train_excerpt_preprocessed.csv\")\n#test_df.to_csv(\"test_excerpt_preprocessed.csv\")","98bdea26":"plt.figure(figsize=(16, 8))\nsns.countplot(y=\"license\",data=train_df,linewidth=3)\nplt.title(\"License Distribution\")\nplt.show()","aa11078f":"print('train data \u306e\u6570:', len(train_df['preprocessed_excerpt']))\nvec = CV(ngram_range=(1, 1)).fit(train_df['preprocessed_excerpt'])\nprint('vocabulary \u306e\u7a2e\u985e:', len(vec.vocabulary_))\nprint('vocabulary \u306e\u4f8b:', list(vec.vocabulary_.keys())[:3])\n\nbow = vec.transform(train_df['preprocessed_excerpt'])\nprint('bag of words \u306e shape', bow.shape)\n\nsum_words = bow.sum(axis=0)\nprint('sum of words \u306e shape', sum_words.shape)\nprint('sum of words', sum_words)\n\nwords_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\nprint('\u983b\u51fa\u5358\u8a9e\u306e\u4f8b', words_freq[:5])\n\n# \u767b\u5834\u56de\u6570\u3067\u4e26\u3079\u66ff\u3048\nwords_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\nprint('\u983b\u51fa\u5358\u8a9e\u4e0a\u4f4d', words_freq[:5])","a552ad32":"def get_top_n_words(corpus, n=None):\n    vec = CV().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_n_bigram(corpus, n=None):\n    vec = CV(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_n_trigram(corpus, n=None):\n    vec = CV(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","2e27a735":"common_words = get_top_n_words(train_df['preprocessed_excerpt'], 20)\ncommon_words_df1 = pd.DataFrame(common_words,columns=['word','freq'])\nplt.figure(figsize=(16, 6))\nax = sns.barplot(x='freq', y='word', data=common_words_df1,\n                 facecolor=(0, 0, 0, 0),linewidth=3,\n                 edgecolor=sns.color_palette(\"ch:start=3, rot=.1\",20))\n\nplt.title(\"Top 20 unigrams\",font='Serif')\nplt.xlabel(\"Frequency\", fontsize=14)\nplt.yticks(fontsize=13)\nplt.xticks(rotation=45, fontsize=13)\nplt.ylabel(\"\");\n\ncommon_words = get_top_n_bigram(train_df['preprocessed_excerpt'], 20)\ncommon_words_df2 = pd.DataFrame(common_words,columns=['word','freq'])\nplt.figure(figsize=(16, 6))\nax = sns.barplot(x='freq', y='word', data=common_words_df2,\n                 facecolor=(0, 0, 0, 0),linewidth=3,\n                 edgecolor=sns.color_palette(\"ch:start=3, rot=.1\",20))\n\nplt.title(\"Top 20 bigrams\",font='Serif')\nplt.xlabel(\"Frequency\", fontsize=14)\nplt.yticks(fontsize=13)\nplt.xticks(rotation=45, fontsize=13)\nplt.ylabel(\"\");\n\ncommon_words = get_top_n_trigram(train_df['preprocessed_excerpt'], 20)\ncommon_words_df2 = pd.DataFrame(common_words,columns=['word','freq'])\nplt.figure(figsize=(16, 6))\nax = sns.barplot(x='freq', y='word', data=common_words_df2,\n                 facecolor=(0, 0, 0, 0),linewidth=3,\n                 edgecolor=sns.color_palette(\"ch:start=3, rot=.1\",20))\n\nplt.title(\"Top 20 trigrams\",font='Serif')\nplt.xlabel(\"Frequency\", fontsize=14)\nplt.yticks(fontsize=13)\nplt.xticks(rotation=45, fontsize=13)\nplt.ylabel(\"\");","aa82afa9":"plt.subplots(figsize=(16,16))\nwc = WordCloud(stopwords=STOPWORDS,background_color=\"white\", \n               contour_width=2, contour_color='blue', width=1600, height=800,\n               max_words=150, max_font_size=256,random_state=42)\nwc.generate(' '.join(train_df['preprocessed_excerpt']))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","26e1f196":"def avg_word_len(df):\n    \"\"\" \u767b\u5834\u3059\u308b\u5358\u8a9e\u306e\u6587\u5b57\u6570\u306e\u5e73\u5747\u3092\u7b97\u51fa\u3059\u308b \"\"\"\n    df = df.str.split().apply(\n        lambda x : [len(i) for i in x] # \u5404\u5358\u8a9e\u306e\u6587\u5b57\u6570\n    ).map(lambda x: np.mean(x)) # \u6587\u5b57\u6570\u306e\u5e73\u5747\n    return df","a17301c7":"train_df['excerpt'][0]","dedf1007":"# \u6587\u5b57\u6570 \/\/ number of characters in the text\ntext_len = train_df['excerpt'].str.len()\ntext_len_pre = train_df['preprocessed_excerpt'].str.len()\ntext_len_dif = train_df['excerpt'].str.len() - train_df['preprocessed_excerpt'].str.len()\n\n# \u5358\u8a9e\u306e\u6587\u5b57\u6570\u306e\u5e73\u5747 \/\/ average of number of characters in words present in the text\navg_text = avg_word_len(train_df['excerpt'])\navg_text_pre = avg_word_len(train_df['preprocessed_excerpt'])\n\n# \u5358\u8a9e\u6570 \/\/ number of words present in the text\nlexicon_count = [] \nlexicon_count_pre = []\nlexicon_count_dif = []\n# \u6587\u7ae0\u6570 \/\/ number of sentences present in the text\nsentence_count = []\n# \u6587\u7ae0\u3042\u305f\u308a\u306e\u5358\u8a9e\u6570 \/\/ mean\/max\/min number of words in each sentence\nmean_lexicon_count_per_st = []\nmax_lexicon_count_per_st = []\nmin_lexicon_count_per_st = []\n\nfor i in range(len(train_df)):\n    lc = textstat.lexicon_count(train_df['excerpt'][i])\n    lcp = textstat.lexicon_count(train_df['preprocessed_excerpt'][i])\n    lcd = lc - lcp\n    sc = textstat.sentence_count(train_df['excerpt'][i])\n    lexicon_count.append(lc)\n    lexicon_count_pre.append(lcp)\n    lexicon_count_dif.append(lcd)\n    sentence_count.append(sc)\n    mean_lexicon_count_per_st.append(np.mean([textstat.lexicon_count(x) for x in train_df['excerpt'][i].split(\".\")]))\n    max_lexicon_count_per_st.append(np.max([textstat.lexicon_count(x) for x in train_df['excerpt'][i].split(\".\")]))\n    min_lexicon_count_per_st.append(np.min([textstat.lexicon_count(x) for x in train_df['excerpt'][i].split(\".\") if textstat.lexicon_count(x) > 1]))","dceab34f":"text_props = train_df.copy()\ntext_props['text_len'] = text_len\ntext_props['text_len_pre'] = text_len_pre\ntext_props['text_len_dif'] = text_len_dif\ntext_props['lexicon_count'] = lexicon_count\ntext_props['lexicon_count_pre'] = lexicon_count_pre\ntext_props['lexicon_count_dif'] = lexicon_count_dif # preprocess \u3068 original \u306e\u5dee\ntext_props['avg_text'] = avg_text\ntext_props['avg_text_pre'] = avg_text_pre\ntext_props['sentence_count'] = sentence_count\ntext_props['mean_lexicon_count_per_st'] = mean_lexicon_count_per_st\ntext_props['max_lexicon_count_per_st'] = max_lexicon_count_per_st\ntext_props['min_lexicon_count_per_st'] = min_lexicon_count_per_st\ntext_props.head(3)","a18b8e8f":"def plot_distribution(col1,col2,title1,title2):\n    fig, ax = plt.subplots(1,2,figsize=(12,6))\n    sns.kdeplot(data=text_props, x=col1,label=\"Excerpt\",ax=ax[0])\n    sns.kdeplot(data=text_props, x=col2,label=\"Excerpt preprocessed\",ax=ax[0])\n    ax[0].set_title(title1,font=\"Serif\")\n    ax[0].legend()\n\n    sns.scatterplot(data=text_props,x=col1,y='target',label=\"Excerpt\",ax=ax[1],markers='.')\n    sns.scatterplot(data=text_props,x=col2,y='target',label=\"Excerpt preprocessed\", ax=ax[1],markers='.', alpha=0.3)\n    ax[1].set_title(title2,font=\"Serif\")\n    ax[1].legend()\n\n    plt.show()\n\nplot_distribution(\"text_len\",\"text_len_pre\",\"Character count distribution\",\"Character count vs Target\")\nplot_distribution(\"lexicon_count\",\"lexicon_count_pre\",\"Word count distribution\",\"Word count vs Target\")\nplot_distribution(\"avg_text\",\"avg_text_pre\", \"Average word length distribution\",\"Average word length vs Target\")\n\nfig, ax = plt.subplots(1,2,figsize=(12,6))\nsns.kdeplot(data=text_props, x=sentence_count,label=\"Excerpt\",ax=ax[0])\nax[0].set_title(\"Sentence count distribution\",font=\"Serif\")\nax[0].set_xlabel(\"sentence_count\")\nsns.scatterplot(data=text_props,x='sentence_count',y='target',ax=ax[1],markers='.')\nax[1].set_title(\"Sentence count vs Target\",font=\"Serif\")\nplt.show()\n\nfig, ax = plt.subplots(1,2,figsize=(12,6))\nsns.kdeplot(data=text_props, x=text_len_dif,label=\"Excerpt\",ax=ax[0])\nax[0].set_title(\"Character count reduction distribution\",font=\"Serif\")\nax[0].set_xlabel(\"Character count reduction by preprocess\")\nsns.scatterplot(data=text_props,x='text_len_dif',y='target',ax=ax[1],markers='.')\nax[1].set_title(\"Character count reduction by preprocess vs Target\",font=\"Serif\")\nplt.show()\n\nfig, ax = plt.subplots(1,2,figsize=(12,6))\nsns.kdeplot(data=text_props, x=lexicon_count_dif,label=\"Excerpt\",ax=ax[0])\nax[0].set_title(\"Word count reduction distribution\",font=\"Serif\")\nax[0].set_xlabel(\"Word count reduction by preprocess\")\nsns.scatterplot(data=text_props,x='lexicon_count_dif',y='target',ax=ax[1],markers='.')\nax[1].set_title(\"Word count reduction by preprocess vs Target\",font=\"Serif\")\nplt.show()\n\nfig, ax = plt.subplots(1,2,figsize=(12,6))\nsns.kdeplot(data=text_props, x=mean_lexicon_count_per_st,label=\"Excerpt\",ax=ax[0])\nax[0].set_title(\"Mean of word count per centence\",font=\"Serif\")\nax[0].set_xlabel(\"Word count\")\nsns.scatterplot(data=text_props,x='mean_lexicon_count_per_st',y='target',ax=ax[1],markers='.')\nax[1].set_title(\"Mean of word count per centence vs Target\",font=\"Serif\")\nplt.show()\n\nfig, ax = plt.subplots(1,2,figsize=(12,6))\nsns.kdeplot(data=text_props, x=max_lexicon_count_per_st,label=\"Excerpt\",ax=ax[0])\nax[0].set_title(\"Max of word count per centence\",font=\"Serif\")\nax[0].set_xlabel(\"Word count\")\nsns.scatterplot(data=text_props,x='max_lexicon_count_per_st',y='target',ax=ax[1],markers='.')\nax[1].set_title(\"Max of word count per centence vs Target\",font=\"Serif\")\nplt.show()\n\nfig, ax = plt.subplots(1,2,figsize=(12,6))\nsns.kdeplot(data=text_props, x=min_lexicon_count_per_st,label=\"Excerpt\",ax=ax[0])\nax[0].set_title(\"Min of word count per centence\",font=\"Serif\")\nax[0].set_xlabel(\"Word count\")\nsns.scatterplot(data=text_props,x='min_lexicon_count_per_st',y='target',ax=ax[1],markers='.')\nax[1].set_title(\"Min of word count per centence vs Target\",font=\"Serif\")\nplt.show()\n\nnum_cols = ['text_len','text_len_pre','text_len_dif', \n            'lexicon_count','lexicon_count_pre','lexicon_count_dif', \n            'avg_text','avg_text_pre','sentence_count',\n            'mean_lexicon_count_per_st', 'max_lexicon_count_per_st', 'min_lexicon_count_per_st',\n            'target']\ncorr = text_props[num_cols].corr()\n\nfig = plt.figure(figsize=(8,8),dpi=80)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap='BuPu', robust=True, center=0, annot=True,\n            square=True, linewidths=.5)\nplt.title('Correlation of text properties', fontsize=15,font=\"Serif\")\nplt.show()","59278288":"text_props['pos_tags'] = text_props['preprocessed_excerpt'].str.split().map(pos_tag)\n\nprint(text_props['preprocessed_excerpt'][0][:50])\nprint(text_props['pos_tags'][0][:5])","9b9c2dd4":"def count_tags(pos_tags):\n    tag_count = {}\n    for word,tag in pos_tags:\n        if tag in tag_count:\n            tag_count[tag] += 1\n        else:\n            tag_count[tag] = 1\n    return tag_count\n\ntext_props['tag_counts'] = text_props['pos_tags'].map(count_tags)\nprint(text_props['tag_counts'].head())","7ecfba94":"set_pos = set([tag for tags in text_props['tag_counts'] for tag in tags])\ntag_cols = list(set_pos)\n\nfor tag in tag_cols:\n    text_props[tag] = text_props['tag_counts'].map(lambda x: x.get(tag, 0))\ntext_props[tag_cols].head()","f12e8305":"pos = text_props[tag_cols].sum().sort_values(ascending = False)\nplt.figure(figsize=(16,8))\nax = sns.barplot(x=pos.index, y=pos.values)\nplt.xticks(rotation = 50)\nax.set_yscale('log')\nplt.title('POS tags frequency',fontsize=15,font=\"Serif\")\nplt.show()","9e16d090":"flesch_re, flesch_kg, fog_scale, automated_r,coleman, linsear, text_standard  = ([] for i in range(7))\nfor i in range(len(text_props)):\n    flr = textstat.flesch_reading_ease(train_df['excerpt'][i])\n    flkg = textstat.flesch_kincaid_grade(train_df['excerpt'][i])\n    fs = textstat.gunning_fog(train_df['excerpt'][i])\n    ar = textstat.automated_readability_index(train_df['excerpt'][i])\n    cole = textstat.coleman_liau_index(train_df['excerpt'][i])\n    lins = textstat.linsear_write_formula(train_df['excerpt'][i])\n    ts = textstat.text_standard(train_df['excerpt'][i])\n    \n    flesch_re.append(flr)\n    flesch_kg.append(flkg)\n    fog_scale.append(fs)\n    automated_r.append(ar)\n    coleman.append(cole)\n    linsear.append(lins)\n    text_standard.append(ts)\n    \ntext_props['flesch_re'] = flesch_re\ntext_props['flesch_kg'] = flesch_kg\ntext_props['fog_scale'] = fog_scale\ntext_props['automated_r'] = automated_r\ntext_props['coleman'] = coleman\ntext_props['linsear'] = linsear\ntext_props['text_standard'] = text_standard","5c247c31":"readability_cols = ['flesch_re','flesch_kg','fog_scale','automated_r','coleman','linsear','text_standard','target']\n\ncorr = text_props[readability_cols].corr()\nfig = plt.figure(figsize=(8,8),dpi=80)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap='PuBuGn', robust=True, center=0,\n            square=True, linewidths=.5,annot=True)\nplt.title('Correlation of readability tests', fontsize=15,font=\"Serif\")\nplt.show()","85caba6f":"fig, ax = plt.subplots(1,2,figsize=(12,6))\nsns.kdeplot(data=text_props, x=flesch_re,ax=ax[0])\nax[0].set_title(\"Flesch Reading Ease Distribution\",font=\"Serif\")\nax[0].set_xlabel(\"Flesch Reading Ease Test Score\")\nsns.scatterplot(data=text_props,x='flesch_re',y='target',ax=ax[1],markers='.')\nax[1].set_title(\"Flesch Reading Ease Test Score vs Target\",font=\"Serif\")\nplt.show()","8c63161f":"def count_character_lexicon_sentence(df):\n\n    # \u6587\u7ae0\u4e2d\u306b\u767b\u5834\u3059\u308b\u6587\u5b57\u306e\u6570 \/\/ number of characters in the text\n    text_len = df['excerpt'].str.len()\n    text_len_pre = df['preprocessed_excerpt'].str.len()\n    text_len_dif = df['excerpt'].str.len() - df['preprocessed_excerpt'].str.len()\n\n    # \u6587\u7ae0\u4e2d\u306b\u767b\u5834\u3059\u308b\u5358\u8a9e\u306e\u6587\u5b57\u6570\u306e\u5e73\u5747 \/\/ average of number of characters in words present in the text\n    avg_text = avg_word_len(df['excerpt'])\n    avg_text_pre = avg_word_len(df['preprocessed_excerpt'])\n\n    # \u5358\u8a9e\u6570 \/\/ number of words present in the text\n    lexicon_count = [] \n    lexicon_count_pre = []\n    lexicon_count_dif = []\n    # \u6587\u7ae0\u6570 \/\/ number of sentences present in the text\n    sentence_count = []\n    # \u6587\u7ae0\u3042\u305f\u308a\u306e\u5358\u8a9e\u6570 \/\/ mean\/max\/min number of words in each sentence\n    mean_lexicon_count_per_st = []\n    max_lexicon_count_per_st = []\n    min_lexicon_count_per_st = []\n    \n    for i in range(len(df)):\n        lc = textstat.lexicon_count(df['excerpt'][i])\n        lcp = textstat.lexicon_count(df['preprocessed_excerpt'][i])\n        lcd = lc - lcp\n        sc = textstat.sentence_count(df['excerpt'][i])\n        lexicon_count.append(lc)\n        lexicon_count_pre.append(lcp)\n        lexicon_count_dif.append(lcd)\n        sentence_count.append(sc)\n        mean_lexicon_count_per_st.append(np.mean([textstat.lexicon_count(x) for x in df['excerpt'][i].split(\".\")]))\n        max_lexicon_count_per_st.append(np.max([textstat.lexicon_count(x) for x in df['excerpt'][i].split(\".\")]))\n        min_lexicon_count_per_st.append(np.min([textstat.lexicon_count(x) for x in df['excerpt'][i].split(\".\") if textstat.lexicon_count(x) > 1]))\n\n    df['text_len'] = text_len\n    df['text_len_pre'] = text_len_pre\n    df['text_len_dif'] = text_len_dif\n    df['lexicon_count'] = lexicon_count\n    df['lexicon_count_pre'] = lexicon_count_pre\n    df['lexicon_count_dif'] = lexicon_count_dif # preprocess \u3068 original \u306e\u5dee\n    df['avg_text'] = avg_text\n    df['avg_text_pre'] = avg_text_pre\n    df['sentence_count'] = sentence_count\n    df['mean_lexicon_count_per_st'] = mean_lexicon_count_per_st\n    df['max_lexicon_count_per_st'] = max_lexicon_count_per_st\n    df['min_lexicon_count_per_st'] = min_lexicon_count_per_st\n    return df\n\n\ndef count_pos(df1, df2):\n    df1['pos_tags'] = df1['preprocessed_excerpt'].str.split().map(pos_tag)\n    df1['tag_counts'] = df1['pos_tags'].map(count_tags)\n\n    df2['pos_tags'] = df2['preprocessed_excerpt'].str.split().map(pos_tag)\n    df2['tag_counts'] = df2['pos_tags'].map(count_tags)\n    \n    # train, test \u4e21\u65b9\u306b\u767b\u5834\u3059\u308bpos\u306e\u307f\u3092\u6271\u3046\n    set_pos1 = set([tag for tags in df1['tag_counts'] for tag in tags])\n    set_pos2 = set([tag for tags in df2['tag_counts'] for tag in tags])\n    tag_cols = list(set_pos1.intersection(set_pos2))\n\n    for tag in tag_cols:\n        df1[tag] = df1['tag_counts'].map(lambda x: x.get(tag, 0))\n        df2[tag] = df2['tag_counts'].map(lambda x: x.get(tag, 0))\n    return df1,df2\n\ndef extract_readability(df):\n    flesch_re, flesch_kg, fog_scale, automated_r,coleman, linsear, text_standard  = ([] for i in range(7))\n    for i in range(len(df)):\n        flr = textstat.flesch_reading_ease(df['excerpt'][i])\n        flkg = textstat.flesch_kincaid_grade(df['excerpt'][i])\n        fs = textstat.gunning_fog(df['excerpt'][i])\n        ar = textstat.automated_readability_index(df['excerpt'][i])\n        cole = textstat.coleman_liau_index(df['excerpt'][i])\n        lins = textstat.linsear_write_formula(df['excerpt'][i])\n        ts = textstat.text_standard(df['excerpt'][i])\n    \n        flesch_re.append(flr)\n        flesch_kg.append(flkg)\n        fog_scale.append(fs)\n        automated_r.append(ar)\n        coleman.append(cole)\n        linsear.append(lins)\n        text_standard.append(ts)\n    \n    df['flesch_re'] = flesch_re\n    df['flesch_kg'] = flesch_kg\n    df['fog_scale'] = fog_scale\n    df['automated_r'] = automated_r\n    df['coleman'] = coleman\n    df['linsear'] = linsear\n    df['text_standard'] = text_standard\n\n    return df\n\ntrain_df = count_character_lexicon_sentence(train_df)\ntest_df = count_character_lexicon_sentence(test_df)\n\ntrain_df, test_df = count_pos(train_df, test_df)\n    \ntrain_df = extract_readability(train_df)\ntest_df = extract_readability(test_df)\n\ntrain_df.to_csv(\"train_excerpt_preprocessed.csv\")\ntest_df.to_csv(\"test_excerpt_preprocessed.csv\")\n\n#train_df = pd.read_csv(\"train_excerpt_preprocessed.csv\", index_col=0)\n#test_df = pd.read_csv(\"test_excerpt_preprocessed.csv\", index_col=0)","5a74277a":"train_df.info()","64d97984":"test_df.info()","ad4b1d9e":"reg = setup(data = train_df, \n             target = 'target',\n             #numeric_imputation = 'mean',\n             categorical_features = ['text_standard'], \n             ignore_features = ['id', 'url_legal', 'license', 'standard_error', 'excerpt', 'preprocessed_excerpt',\n                               'pos_tags', 'tag_counts'],\n             normalize = True,\n             silent = True,\n             session_id = 123)","2b861e9c":"start = time.time()\ncompare_models()\nprint('elapsed time : ', time.time() - start)","d6293af3":"import time\nstart = time.time()\nridge = create_model('ridge')\nprint('elapsed time : ', time.time() - start)","a3c9d652":"start = time.time()\ntuned = tune_model(ridge, optimize='RMSE')\nprint('elapsed time : ', time.time() - start)","e526cdeb":"# prediction\npredictions = predict_model(tuned, data = test_df)\n\n# submission file\nsubmission_df = pd.DataFrame({'id': test_df.id, 'target': 0})\nsubmission_df.target = predictions['Label']\n\nsubmission_df.to_csv('.\/submission.csv', index=False)\n\nsubmission_df","f21cb593":"- avg_text (\u5358\u8a9e\u306e\u6587\u5b57\u6570\u306e\u5e73\u5747)\u3084text_len\uff08\u6587\u5b57\u6570\uff09\u306f target \u3068\u8ca0\u306e\u76f8\u95a2\u2192\u6587\u5b57\u6570\u304c\u5c11\u306a\u3044\u307b\u3069\u8aad\u307f\u3084\u3059\u3044\n- sentence_count\uff08\u6587\u7ae0\u306e\u6570\uff09\u306f\u3001target \u3068\u5f31\u3044\u6b63\u306e\u76f8\u95a2\u2192\u6587\u7ae0\u6570\u304c\u5c11\u306a\u3044\u307b\u3069\u8aad\u307f\u3084\u3059\u3044\n- \u3044\u305a\u308c\u3082\u76f4\u89b3\u7684\u306a\u611f\u899a\u3068\u4e00\u81f4\u3059\u308b","36f08a04":"\u983b\u51fa\u5358\u8a9e(unigram), \u983b\u51fabigram, trigram \u3092\u62bd\u51fa\u3059\u308b","f075a1c8":"stop words \u306b \u5165\u308c\u305f\u307b\u3046\u304c\u3088\u3044word:u \n\u52d5\u8a5e\u306f\u904e\u53bb\u5f62\u306e\u3082\u306e\u3082\u6b8b\u3063\u3066\u3044\u308b","a4b44245":"# Set up","a82367f6":"### \u6587\u7ae0\u4e2d\u306b\u767b\u5834\u3059\u308b\u5358\u8a9e\u306e\u54c1\u8a5e\u3092\u30ab\u30a6\u30f3\u30c8\u3059\u308b","d3cef0ef":"KeyError: '[\\'WP\\', \\'TO\\', \"\\'\\'\", \\'WP$\\', \\'NNPS\\', \\'POS\\', \\'WDT\\', \\'EX\\', \\'$\\', \\'PRP$\\', \\'SYM\\', \\'PRP\\', \\'PDT\\', \\'NNP\\', \\'RBS\\'] not in index'set","13601d1b":"\u3053\u3061\u3089\u306e\u65b9\u306e\u307b\u307c\u5199\u7d4c \/\/ this notebook is copied and modified from the link below.\n\nhttps:\/\/www.kaggle.com\/ruchi798\/commonlit-readability-prize-eda-baseline\/comments#Pre-processing-excerpt-%E2%9C%82%EF%B8%8F","9aba0db8":"# Profile data","ee6173cd":"\u3053\u3053\u307e\u3067\u306e\u51e6\u7406\u3092\u95a2\u6570\u5316\u3057\u3066\u3001\u5b66\u7fd2\u30c7\u30fc\u30bf\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u51e6\u7406\u3057\u3066\u304a\u304f","f381ad49":"Flesch Readability Ease \u306e\u5024\u3068 target \u306b\u306f\u6b63\u306e\u76f8\u95a2","0929cae2":"trigram \u306e\uff12\u4f4d\u306b\u767b\u5834\u3059\u308b \"th\" \u306f stop words \u306b\u52a0\u3048\u305f\u307b\u3046\u304c\u3088\u3044\uff1f","c5d41cb9":"url, license \u306f\u3001\u8a13\u7df4\u30c7\u30fc\u30bf\u306e 70% \u304c\u6b20\u640d\u3057\u3066\u3044\u308b","6af8fedd":"\u6700\u5f8c\u307e\u3067\u8aad\u3093\u3067\u3044\u305f\u3060\u304d\u3069\u3046\u3082\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\u5c11\u3057\u3067\u3082\u304a\u5f79\u306b\u7acb\u3066\u305d\u3046\u3068\u611f\u3058\u3066\u3044\u305f\u3060\u3051\u305f\u3089\u3001Upvote\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002\u3068\u3066\u3082\u52b1\u307f\u306b\u306a\u308a\u307e\u3059!!\nThank for your iterest. Please upvote if you think this notebook would be helpful for you. It is really encouraging me.","dbc8effd":"'JJ' \u3068\u304b 'NNS' \u3068\u304b\u304c\u54c1\u8a5e\u306e\u7a2e\u985e\u3001\u5f8c\u306b\u7d9a\u304f value \u304c\u767b\u5834\u56de\u6570\u3092\u8868\u3059\n\n\u5b9a\u7fa9\u306foriginal kernel \u53c2\u7167\n\nhttps:\/\/www.kaggle.com\/ruchi798\/commonlit-readability-prize-eda-baseline\/comments?scriptVersionId=62607090&cellId=47","bef5c6e7":"# Load Data","8278fb96":"excerpt (\u629c\u7c8b) \u304b\u3089\u3001\u4eba\u306b\u3088\u3063\u3066\u3064\u3051\u3089\u308c\u305f\u6587\u7ae0\u306e\u8aad\u307f\u3084\u3059\u3055\u30b9\u30b3\u30a2\u307f\u305f\u3044\u306a\u306e\uff08target\uff09\u3092\u3092\u5f53\u3066\u308b\n\n\u30b9\u30b3\u30a2\u306f\u8907\u6570\u4eba\u3067\u3064\u3051\u308b\u305f\u3081\u3001\u4eba\u306b\u3088\u308b\u63a1\u70b9\u3070\u3089\u3064\u304d\uff08standard error\uff09\u304c\u3042\u308b","c81aaa26":"- [original \u306e kernel](https:\/\/www.kaggle.com\/ruchi798\/commonlit-readability-prize-eda-baseline\/comments?scriptVersionId=62607090&cellId=54)\u306b\u3088\u308b\u3068\u3001textstat \u306b\u306f\u8aad\u307f\u3084\u3059\u3055\u306b\u95a2\u3059\u308b\u30b9\u30b3\u30a2\u306e\u7b97\u51fa\u65b9\u6cd5\u304c\u5b9f\u88c5\u3055\u308c\u3066\u3044\u308b\u305d\u3046\u306a\n","99823ac1":"# Preprocess Data\n\u4ee5\u4e0b\u306e\u6d41\u308c\u3067\u51e6\u7406\n1. \u6587\u7ae0\u5185\u306e\u30a2\u30eb\u30d5\u30a1\u30d9\u30c3\u30c8\u4ee5\u5916\u3092\u7a7a\u767d\u306b\u5909\u63db\n2. stopwords \u306e\u524a\u9664\n3. \u5358\u8a9e\u306e\u30ec\u30f3\u30de\u5316"}}