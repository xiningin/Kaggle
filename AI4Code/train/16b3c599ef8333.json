{"cell_type":{"48ceaeb0":"code","ebebc774":"code","1c1abb4a":"code","61bc2166":"code","e58a116b":"code","cbfeded6":"code","10a2f460":"code","3863b14f":"code","9e7f56fe":"code","aee0340e":"code","b7c723e2":"code","53d751d8":"code","0e769cd6":"code","2443cf01":"code","5e173856":"code","edee520b":"code","20ecfdea":"code","8992b677":"code","83fb0346":"code","02c84636":"code","9a39ea06":"code","eefe06d0":"markdown","778a76c0":"markdown","6bfc1e9a":"markdown","fd30a7b7":"markdown","dbec50fb":"markdown","a15a5024":"markdown","24834d30":"markdown","a9e0ea99":"markdown","fad3752e":"markdown","e68d5ba0":"markdown","641a37fc":"markdown","e1e27503":"markdown","6d98da1b":"markdown","7063bac6":"markdown","a9c4363f":"markdown","229f7d13":"markdown","b62eaede":"markdown","c41af564":"markdown","03b427bb":"markdown","9856663d":"markdown","91a1b03f":"markdown","37a9db3a":"markdown","4b41535e":"markdown","75206f55":"markdown","15bdc2d8":"markdown","3fe3fa93":"markdown"},"source":{"48ceaeb0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ebebc774":"df= pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndf","1c1abb4a":"df.describe()","61bc2166":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('darkgrid')\n\nplt.figure(figsize=(10,10))\nsns.heatmap(df.corr(), annot=True, square=True)","e58a116b":"df.corr()['DEATH_EVENT'].apply(np.abs).sort_values(ascending=False)","cbfeded6":"sns.pairplot(df, vars=['serum_creatinine', 'ejection_fraction', 'age', 'serum_sodium', 'platelets'], hue = 'DEATH_EVENT')","10a2f460":"df['DEATH_EVENT'].mean()","3863b14f":"#smoke_sex = df.groupby(['smoking','sex', 'DEATH_EVENT'])['DEATH_EVENT'].count().unstack()\npd.crosstab([df.smoking, df.sex ], df['DEATH_EVENT']) ","9e7f56fe":"# normalise index because we want to see the proportion of each group\npd.crosstab([df.smoking, df.sex], df['DEATH_EVENT'], normalize='index') ","aee0340e":"from sklearn.model_selection import train_test_split\n\n# Select subset of predictors\ncols_to_use = df.columns[: - 2]\nX = df[cols_to_use]\n\n# Select target\ny = df.iloc[:,-1]\n\n# Separate data into training and validation sets\n# as splits have randomness, we apply a random_state seed for reproducibility\n# stratified as imbalanced classes\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=20, stratify = y)\n    ","b7c723e2":"# define useful helper function for displaying cv scores.\n\ndef display_scores(scores, return_scores = False):\n    if return_scores: \n        print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())","53d751d8":"# import models\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC, NuSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# import method for selection criteria\nfrom sklearn.model_selection import cross_val_score\n\n\n# create list of Classification models to choose from\nmodels = [XGBClassifier(),\n          SVC(C= 0.1),\n          NuSVC(),\n          LogisticRegression(),\n          RandomForestClassifier(),\n          DecisionTreeClassifier()]\n\n# loop through models and evaluate their performance based on 10-fold cross validation \ndef loopthruModels(models, X, y, cv=10): \n    for model in models: \n        print(str(model))\n        scores = cross_val_score(model, X,y, scoring=\"f1_macro\", cv= cv)\n        display_scores(scores)\n        print('---')\n        \nloopthruModels(models, X_train, y_train)","0e769cd6":"from sklearn.model_selection import GridSearchCV\n\nxg_param_grid = [\n    {'n_estimators': [50, 70, 100, 500, 1000], \n     'learning_rate':[0.001, 0.01, 0.05, 0.1,  0.5],\n     'n_jobs': [4]}\n]\n\nxgmodel = XGBClassifier()\n\ngrid_search = GridSearchCV(xgmodel, xg_param_grid, cv=10, scoring=\"f1_macro\", return_train_score= True)\ngrid_search.fit(X_train,y_train)\nprint(grid_search.best_params_)\n\ncvres = grid_search.cv_results_\n\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(mean_score, params)","2443cf01":"forest_param_grid = [\n    {'n_estimators': [200, 700, 1000], \n     'max_features':[0.2, 0.5, None],\n    'n_jobs': [4]}\n]\n\nforest = RandomForestClassifier()\n\ngrid_search = GridSearchCV(forest, forest_param_grid, cv=10, scoring=\"f1_macro\", return_train_score=True)\ngrid_search.fit(X_train,y_train)\nprint(grid_search.best_params_)\n\ncvres = grid_search.cv_results_\n\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(mean_score, params)","5e173856":"forest_model = RandomForestClassifier()\n\nforest_model.fit(X_train, y_train)\n\n# evaluate each feature's importance\nimportances = forest_model.feature_importances_\n\n# sort by descending order of importances\nindices = np.argsort(importances)[::-1]\n\n#create sorted dictionary\nforest_importances = {}\n\nprint(\"Feature ranking:\")\nfor f in range(X.shape[1]):\n    forest_importances[X.columns[indices[f]]] = importances[indices[f]]\n    print(\"%d. %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))","edee520b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\n\n# Set the regularization parameter C=1\nlogistic = LogisticRegression(C = .1, penalty=\"l1\", solver='liblinear', random_state=7).fit(X_train, y_train)\nmodel = SelectFromModel(logistic, prefit=True)\n\nX_new = model.transform(X)\nX_new\n\n# Get back the kept features as a DataFrame with dropped columns as all 0s\nselected_features = pd.DataFrame(model.inverse_transform(X_new), \n                                 index=X.index,\n                                 columns=X.columns)\n\n# Dropped columns have values of all 0s, keep other columns \nreg_selected_columns = selected_features.columns[selected_features.var() != 0]\nreg_selected_columns","20ecfdea":"# define function for evaluating different feature selections\n\ndef eval_features(model, param_grid, feature_list): \n    for feature in feature_list: \n        grid_search = GridSearchCV(model, param_grid, cv=10, scoring=\"f1_macro\", return_train_score= True)\n        grid_search.fit(X_train[feature],y_train)\n        print(grid_search.best_params_)\n\n        cvres = grid_search.cv_results_\n        max_score = 0 \n\n        for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n            if mean_score > max_score:\n                max_score = mean_score\n                best_params = params    \n        print(max_score, best_params)","8992b677":"xgmodel = XGBClassifier()\n\n# parameter grid for XGBoost\nxg_param_grid = [\n    {'n_estimators': [25, 50,  500, 1000], \n     'learning_rate':[0.001, 0.01, 0.05, 0.1,  0.5],\n     'n_jobs': [4]}\n]\n\n# we want to test this series of features\nfeature_list = [['serum_creatinine', 'ejection_fraction'],\n                ['serum_creatinine', 'ejection_fraction', 'age'], \n                ['serum_creatinine', 'ejection_fraction', 'age', 'creatinine_phosphokinase'],\n                ['serum_creatinine', 'ejection_fraction', 'age', 'creatinine_phosphokinase', 'platelets'],\n                reg_selected_columns]\n\neval_features(xgmodel, xg_param_grid, feature_list)    ","83fb0346":"forest = RandomForestClassifier()\n\nforest_param_grid = [\n    {'n_estimators': [200, 700, 1000], \n     'max_features':[0.2, 0.5, None],\n    'n_jobs': [4]}\n]\n\n# use same feature list\neval_features(forest, forest_param_grid, feature_list)   ","02c84636":"# defining useful functions\nfrom sklearn.metrics import confusion_matrix\ndef getScore(model, X_train= X_train, y_train= y_train): \n    my_pipeline = Pipeline(steps=[\n                                  ('model', model)\n                                 ])\n    # Preprocessing of training data, fit model \n    my_pipeline.fit(X_train, y_train)\n\n    # Preprocessing of validation data, get predictions\n    preds = my_pipeline.predict(X_valid[sel_cols])\n    score = my_pipeline.score(X_valid[sel_cols], y_valid)\n    return score\n\ndef getconfusion(model, y_valid, y_pred, f1= True): \n    cf_matrix = confusion_matrix(y_valid, y_pred)\n\n    f1 = model.score(X_valid[sel_cols], y_valid)\n    print('The F1 score is:', f1)\n    sns.heatmap(cf_matrix, annot=True)\n    plt.xlabel('Predicted values')\n    plt.ylabel('True values')\n","9a39ea06":"from sklearn.pipeline import Pipeline\n\n# relevant columns for a good prediction\nsel_cols = ['serum_creatinine', 'ejection_fraction', 'age']\n\n# model with optimised hyperparameters\nmodel = XGBClassifier(n_estimators=1000, learning_rate= 0.5)\n\nsteps=[\n       ('model', model)\n      ]\n\n# Bundle preprocessing and modeling code in a pipeline\npipe = Pipeline(steps)\n\n# Preprocessing of training data, fit model \npipe.fit(X_train[sel_cols], y_train)\n\n# Preprocessing of validation data, get predictions\npreds = pipe.predict(X_valid[sel_cols])\n\n# Evaluate the model using the confusion matrix\ngetconfusion(pipe, y_valid, preds)\n","eefe06d0":"---\n# 2. Modelling\n\nNow as we have rougly understood the data, we can move onto the next step. \n\nWe would like to apply machine learning models to this context with the aim of trying to predict whether a person is likely to have heart disease based on some knowledge of predetermining factors.","778a76c0":"As we can see, our model is not perfect but its value is undeniable! Notice that for this particular split, the model gives more false negatives than false positives.","6bfc1e9a":"Apparently the models with the Random Forest evaluated feature importances perform the best-- even better than the model using all the features for both the XGBoost and Random Forest model. ","fd30a7b7":"It is particularly interesting to see that the use of two features already gives a pretty decent F1 score (about 0.70). The more features we use, the higher the change that overfitting is going to impact our model.","dbec50fb":"## 1.2 Correlation heatmap\n\nThe correlation heatmap shows the strength of linear correlation between each variable, from a matrix of univariate regressions. It gives us a simple yet powerful overview of the underlying relationships between the variables. ","a15a5024":"The series below is a sorted vesion of the relevant part of the correlation matrix above, indicating each factor's salience in successfully predicting the death event.","24834d30":"As our dataset is a bit on the smaller side, we would like to explore whether all the features are explicitly needed in determining whether a person is predisposed to the fatal heart failure. The next section concerns such exploration.","a9e0ea99":"## 5. Final Model\n\nFrom section 4, it was observed that reducing the number of features indeed gave way to a higher F1-score. I will select the set with three features `['serum_creatinine', 'ejection_fraction', 'age']` for the construction of our final model, as the low number of features will make our model quite resistant to overfitting but it still offers a slightly better score than simply using two features, as the original study suggested.\n\nI am more keen to use the XGBoost Classifier for the final model as it is much quicker but offers basically the same level of accuracy. ","fad3752e":"## 1.4 Variable frequencies\n\nWe note that the target class distribution is skewed, indicating that we will have to perform stratified sampling of our data.","e68d5ba0":"## 4. Evaluating feature importances\n\nWe used L1 regularisation and a random forest to find the metrics which have highest feature importance and interestingly they gave slightly different results. ","641a37fc":"The F1-score was used instead of accuracy to evaluate the effectiveness of our classification models, because our dataset is skewed towards negative classes. It is given by the formula\n$$F1 = {{2 (prec \\times rec)} \\over {prec + rec} }$$\n\nwhere $prec$ is the precision (how many selected items are relevant) and $rec$ is the recall (how many relevant items are selected). \n\nThe three best models with highest performance, based on their mean F1-score is the XGBoost Classifier, the Random Forest Classifier and the Logistic Regression model.\n\nHowever, do keep in mind that there is a certain degree of randomness to such computations, mainly based on what the outcome of the `train_test_split` function is, because our dataset is relatively small. It was noted that the Logistic Regression model sometimes failed to converge in some particular test splits.\n\n","e1e27503":"# Heart Failure Predictions\nCardiovascular diseases are gradually becoming one of the most prominent causes of death in the modern world. Common knowledge dictates that this disease is highly linked with habits such as smoking or sedentary behaviour; other diseases such as obesity and diabetes and also genetics. Predicting the likelihood of a person having heart failure would be extremely valuable to the medical field as early diagnosis would enhance the possibility of sound recovery. Machine learning would be a tool to forecast whether a particular patient is likely to suffer from heart failure and hence obtain more resources for treatment.\n\nThis dataset that concerns this notebook was provided in the journal article [Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone](https:\/\/bmcmedinformdecismak.biomedcentral.com\/articles\/10.1186\/s12911-020-1023-5) by Davide Chicco & Giuseppe Jurman. It contains a set of features that can be used to predict whether a fatal heart failure will occur in that person. ","6d98da1b":"## Final Pipeline","7063bac6":"## 2.1 Model Selection\n\nWe would like to try a few classification models on our data and evaluate their success in predicting our target variable.","a9c4363f":"### 3.2 Random Forest\n\nA Random Forest is another powerful ensemble method that bunches together Decision Trees.","229f7d13":"# 6. Conclusion\n\nAn XGBoost model was used to model whether a person would suffer from fatal heart failure and an F1-score of ~73% was consistently achieved.\n\nThree predictors were more than sufficient in making this model function with a high score, namely the `ejection_fraction`-- the percentage of blood leaving the heart at each contraction, `serum_creatinine`-- the level of creatinine in blood and the `age`. \n\nThe main source of improvement for this project would be to obtain more data. Perhaps, we could also try obtaining a probabilistic value for the likelihood of someone developing a fatal heart failure, which might be more useful for medical professionals to identify marginal cases!","b62eaede":"## 1.3 Pairplot\n\nThe useful aspects of this plot is the kernel density estimation as it separates the distribution of features into the two target classes. This shows that those who die from heart disease tend to have\n* higher age\n* higher creatinine values\n* but lower ejection fractions\n\nThis correlation is also captured by the heat map above. ","c41af564":"# Destined for (Heart <3) Failure? ","03b427bb":"---\n# 3. Hyperparameter Optimisation\n\nWe now focus on the XGBoost and Random Forest models because they gave the best F1-scores in the previous section.\n\nUsing `GridSearchCV`, we can inspect the parameter space and do a grid search for the best hyperparameters, which are specifications of the model that are provided at the start and do not change as the model is trained. \n\n\n### 3.1 XGBoost\nXGBoost is an ensemble method that bunches together several weak learning models to create a stronger one, iteratively correcting the previous one. XGBoost refers to Extreme Gradient Boost, which fits successive predictors to the *residual errors* made, and improves each round because This library is aimed to be fast and scalable. \n\nA good mathematical description of the principles of XGBoost can be found [here](https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/model.html) in XGBoost's documentation.\n\nA good balance between the `learning_rate` and `n_estimators` was key to balance between the overfitting and underfitting of our data. ","9856663d":"## 1.5 Sidetrack: Smoking, sex and heart disease\n\nThe following tables gives insight that, in our sample\n* a lot more males smoke compared to females\n* there is a similar number of non-smoking males and females who die due to heart disease\n* there is no obvious difference in fractions and hence correlation between smoking\/sex and presence of heart disease","91a1b03f":"#### Data Leakage\nNote that the 'time' feature has an unusually large correlation with the DEATH_EVENT target and constitutes as a form of data leakage. This signifies that those who have longer (perhaps earlier) treatment are more likely to find a suitable cure for heart disease. We won't be able to use this attribute for prediction as it is only available when we know the outcome of DEATH_EVENT.\n\n#### Promising Features\nIt seems that serum_creatinine, ejection_fraction, age and serum_sodium are promising features to use in this task. We will confirm this with the pairplot.","37a9db3a":"### 4.2 L1 feature importance evaluation","4b41535e":"# 1. Exploratory Data Analysis\n\nWe find that the data is pre-cleaned, with categorial columns such as sex, presence of anaemia, diabetes and high blood pressure pre-labelled, which leads itself to the high usability of data. The one concern is that there are only 299 instances in this dataset, which might lead to some overfitting of data. ","75206f55":"## 4.3 Reevaluating models having selected features","15bdc2d8":"### 4.1 Forest Feature evaluation","3fe3fa93":"## 1.1 Features and Target\n### Features and their description\nWe notice a set of inherent, habitual and biologically measurable factors available at our disposal to use. \n* age: The person's age (numeric) \n* anaemia: Decrease of red blood cells or hemoglobin (boolean)\n* creatine_phosphokinase: Level of the CPK enzyme in the blood (mcg\/L)\n* diabetes: If the patient has diabetes (boolean)\n* ejection_fraction: Percentage of blood leaving the heart at each contraction (percentage)\n* high_blood_pressure: If the patient has hypertension (boolean)\n* platelets: Platelets in the blood (kiloplatelets\/mL)\n* serum_creatinine: Level of serum creatinine in the blood (mg\/dL)\n* serum_sodium: Level of serum sodium in the blood (mEq\/L)\n* gender: Woman or man (binary)\n* smoking: If the patient smokes or not (boolean)\n* time: Follow-up period (days)\n\n### Target\n* DEATH_EVENT: If the patient deceased during the follow-up period (boolean)"}}