{"cell_type":{"84c7d212":"code","972b447b":"code","947f3f38":"code","444374c9":"code","f3836e17":"code","c9f188dc":"code","b7f399c7":"code","df308682":"code","c40850ce":"code","20fb34f0":"code","d642976a":"code","d557354e":"code","bc0679d7":"code","8f6585ef":"code","748b1067":"code","4bcada6a":"code","fd0a277a":"code","dfe9551e":"code","a9690033":"code","9c295ff2":"code","2b697f1f":"code","d249ec71":"code","9ba46762":"code","675e251a":"code","acfd14fa":"code","79196882":"code","9a8abe1d":"code","4920b6ab":"code","35b2de7e":"code","a24d68a4":"code","f69c7f33":"code","678d5827":"code","97e1368c":"code","0bfb534e":"code","fd0c3ccb":"code","4b33624c":"code","d023550c":"markdown","9e77736f":"markdown","a75eed21":"markdown","77342b5c":"markdown","e4769b3d":"markdown","b56133bc":"markdown","0d73c942":"markdown","986c87ca":"markdown","8970e886":"markdown","0a3defb0":"markdown","082ef601":"markdown","5a42a30b":"markdown","45daa8f5":"markdown","10d10887":"markdown","5ad62b7c":"markdown","5a10d470":"markdown","515d08be":"markdown","11939180":"markdown","3c71181c":"markdown","afa88bed":"markdown","c18c4d13":"markdown","0805fd69":"markdown","d86ffb9b":"markdown","ad853568":"markdown"},"source":{"84c7d212":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom collections import Counter\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,roc_auc_score\nfrom sklearn.feature_selection import RFECV","972b447b":"\n\ntrain=pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ntest=pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')\nsample_submission=pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/sample_submission.csv')","947f3f38":"train.shape,test.shape","444374c9":"train.head()","f3836e17":"test.head()","c9f188dc":"train['target'].value_counts(normalize=True)","b7f399c7":"train.dtypes","df308682":"obj_cols=train.select_dtypes('object').columns\nnumeric_cols=[c for c in train.columns if c not in obj_cols if c not in ('target')]","c40850ce":"for c in obj_cols:\n    if list(set(test[c])-set(train[c])):\n        print(f\"For column {c} Available only in test are {list(set(test['city'])-set(train['city']))}\")\n    print(\"No instances found\")","20fb34f0":"test.loc[:,'target']=-1\ndata=pd.concat([train,test],ignore_index=True)","d642976a":"#https:\/\/www.kaggle.com\/artgor\/is-this-malware-eda-fe-and-lgb-updated\nstat_cols=[]\nfor c in obj_cols:\n    stat_cols.append((c,data[c].nunique(),data[c].isnull().sum()*100\/data[c].shape[0],data[c].value_counts(normalize=True,dropna=False).values[0]*100))\n    stat_df=pd.DataFrame(stat_cols,columns=['column_name','unique_values','null_value_perc','perc_of_max_value'])\n    stat_df.sort_values('unique_values',ascending=False,inplace=True)\nstat_df","d557354e":"data['company_type'].value_counts(normalize=True,dropna=False)","bc0679d7":"null_cols=[c for c in obj_cols if c not in ['city','relevent_experience']]\nfor n in null_cols:\n    print(f'Imputing null values in column {n}')\n    data.loc[:,n]=data[n].fillna(f'NONE_{n}').astype('str')","8f6585ef":"data['company_type'].value_counts(normalize=True,dropna=False)","748b1067":"stat_cols=[]\nfor c in obj_cols:\n    stat_cols.append((c,data[c].nunique(),data[c].isnull().sum()*100\/data[c].shape[0],data[c].value_counts(normalize=True,dropna=False).values[0]*100))\n    stat_df=pd.DataFrame(stat_cols,columns=['column_name','unique_values','null_value_perc','perc_of_max_value'])\n    stat_df.sort_values('unique_values',ascending=False,inplace=True)\nstat_df","4bcada6a":"for c in ['relevent_experience','enrolled_university','gender']:\n    temp=pd.get_dummies(data[c],prefix='OHE')\n    data=pd.concat([data,temp],axis=1)\n    print(f'OHE {c}.Now removing original column {c} from df')\n    data.drop(c,axis=1,inplace=True)","fd0a277a":"data[numeric_cols].isnull().sum()","dfe9551e":"data.loc[:,'enrollee_id']=data.loc[:,'enrollee_id'].astype('object')","a9690033":"train=data.loc[data['target']!=-1,:].reset_index(drop=True)\ntest=data.loc[data['target']==-1,:].reset_index(drop=True)","9c295ff2":"X=train.drop('target',axis=1)\ny=train.target.values","2b697f1f":"#Distribution of numeric columns:\nfig,ax=plt.subplots(figsize=(12,10))\nplt.subplot(2,2,1)\nsns.distplot(train['city_development_index'],color='darkblue')\nplt.title(\"Distribution of city development index-Train\",fontsize=15)\nplt.xlabel('City development index',fontsize=10)\nplt.ylabel('frequency')\nplt.subplot(2,2,2)\nsns.distplot(test['city_development_index'],color='violet')\nplt.title(\"Distribution of city development index-Test\",fontsize=15)\nplt.xlabel('City development index',fontsize=10)\nplt.ylabel('frequency')\nplt.subplot(2,2,3)\nsns.distplot(train['training_hours'],color='darkblue')\nplt.title(\"Distribution of training hours-Train\",fontsize=15)\nplt.xlabel('Training Hours',fontsize=10)\nplt.ylabel('frequency')\nplt.subplot(2,2,4)\nsns.distplot(test['training_hours'],color='violet')\nplt.title(\"Distribution of training hours-Test\",fontsize=15)\nplt.xlabel('Training Hours',fontsize=10)\nplt.ylabel('frequency')","d249ec71":"train['training_hours'].describe(),test['training_hours'].describe()","9ba46762":"freq_cols=['city','experience','company_size','major_discipline','company_type','last_new_job','education_level']\nnum_cols=['city_development_index','training_hours']","675e251a":"required_cols=[c for c in X.columns if c not in ('enrollee_id')]","acfd14fa":"folds=StratifiedKFold(n_splits=5,shuffle=True,random_state=42)","79196882":"def freq_encode(trn_df,val_df,cols):\n    for c in cols:\n        df=pd.concat([trn_df[[c]],val_df[[c]]])\n        foo=df[c].value_counts().to_dict()\n        trn_df[c]=trn_df[c].map(foo)\n        val_df[c]=val_df[c].map(foo)\n    return trn_df[cols],val_df[cols]","9a8abe1d":"pred_df=np.zeros(len(test))\nscores=[]\nroc=[]\nfor i,(trn_idx,val_idx) in enumerate(folds.split(X,y)):\n    print(f'***Starting fold {i+1}***')\n    trn_x,trn_y=X[required_cols].iloc[trn_idx],y[trn_idx]\n    val_x,val_y=X[required_cols].iloc[val_idx],y[val_idx]\n    trn_x[freq_cols],val_x[freq_cols]=freq_encode(trn_x,val_x,freq_cols)\n    #val_x[freq_cols]=freq_encode(val_x,freq_cols)\n    clf=RandomForestClassifier(n_estimators=1000,oob_score=True,n_jobs=-1,random_state=40,max_features='sqrt')\n    clf.fit(trn_x,trn_y)\n    preds=clf.predict(val_x)\n    score=f1_score(val_y,preds)\n    roc_score=roc_auc_score(val_y,preds)\n    scores.append(score)\n    roc.append(roc_score)\n    print(f'F1 score for fold {i+1} is {score} ROC score {roc_score}')\n    \n    test[freq_cols],_=freq_encode(train[required_cols],test[required_cols],freq_cols)\n    test_preds=clf.predict(test[required_cols])\n    pred_df+=test_preds\nprint(f'Average f1 score for 5 folds {np.mean(scores)} .Avg roc score for 5 folds {np.mean(roc)}')\npred_df\/=5\n    ","4920b6ab":"train_df=X[required_cols].copy()\ntrain_df[freq_cols],_=freq_encode(train,test,freq_cols)","35b2de7e":"##recursive feature elimination with cross validation:\nmodel=RandomForestClassifier(n_estimators=1000,oob_score=True,n_jobs=-1,random_state=40,max_features='sqrt')\nrfecv=RFECV(estimator=model,\n           cv=StratifiedKFold(n_splits=5,shuffle=True,random_state=42).split(train_df[required_cols],y),\n           step=5,\n           scoring='roc_auc',\n           verbose=2)\nrfecv.fit(train_df[required_cols],y)\n","a24d68a4":"print(f\"Optimal number of features {rfecv.n_features_}\")\n","f69c7f33":"plt.figure(figsize=(8,8))\nplt.plot(range(1,len(rfecv.grid_scores_)+1),rfecv.grid_scores_)\nplt.xlabel('Number of features selected')\nplt.ylabel('Cross validation score')\nplt.show()","678d5827":"ranking=pd.DataFrame({'features':required_cols})\nranking['Rank']=np.asarray(rfecv.ranking_)\nranking.sort_values('Rank',ascending=False)","97e1368c":"score=np.max(rfecv.grid_scores_)\nscore","0bfb534e":"sample_submission['target']=rfecv.predict(test[required_cols])","fd0c3ccb":"sample_submission.head()","4b33624c":"sample_submission['target'].value_counts()","d023550c":"## Building a baseline model","9e77736f":"## Import library and dataset","a75eed21":"Lets consider both training and test sets for our analysis.","77342b5c":"Thus all null values in the column is taken care of.Lets now check the numerical columns.","e4769b3d":"14 columns with target is our y variable.","b56133bc":"1.Artgor's [Malware prediction kernel](https:\/\/www.kaggle.com\/artgor\/is-this-malware-eda-fe-and-lgb-updated)\n\n2.Abhishek Thakur's [Approaching Almost any Machine Learning Problem](https:\/\/www.amazon.in\/Approaching-Almost-Machine-Learning-Problem-ebook\/dp\/B089P13QHT\/ref=sr_1_1?crid=3VJFCEROX0U8&dchild=1&keywords=approaching+almost+any+machine+learning+problem&qid=1609395728&sprefix=approachin%2Caps%2C317&sr=8-1)\n\n3.[Recursive feature elimination with CV](https:\/\/scikit-learn.org\/stable\/auto_examples\/feature_selection\/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py)\n\n4.[eliminate features recursively](https:\/\/www.kaggle.com\/tilii7\/eliminate-features-recursively-cv)","0d73c942":"Since there is an imbalance in target,we use stratifiedKfold for cross validation.","986c87ca":"Lets check company type column again,","8970e886":"No Null values in numeric columns.From the column name it is seen that enrolee_id is more nominal rather than a continuous column.Lets change the dtype.","0a3defb0":"* The distribution of city development index looks similar in both training and test set.The development index is having a peak at values 0.9 and 0.6.The range is also between 0.4 to 1.0\n* Training hours is right skewed with peak between 0-50.The range is also similar.","082ef601":"## Handling null values","5a42a30b":"Now,we split the data again into train and test.","45daa8f5":"For categorical data with cardinality less than 5 , we use one hot encoding while for cardinality greater than 5 we use frequency encoding.Inorder to avoid data leak,we do the frequency encoding in our cross validation setup.","10d10887":"## Preparing categorical data for Model Input","5ad62b7c":"We use a technique described in Abhishek Thakur's book for imputing missing values.NaN's will be considered a separate category and imputed.","5a10d470":"## Reference","515d08be":"## Category columns","11939180":"The model with Random forest classifier is not the best.Lets try to improve this baseline score.","3c71181c":"# HR Analytics","afa88bed":"There are no new categories available in testset.Lets combine both train and test for the next section of our analysis.","c18c4d13":"The target column which is the predictor variable seems to be imbalanced .We have 75% of rows as 0 whereas 25% is 1.","0805fd69":"The above table provides a summary of the categorical columns.City and relevent experience have no null values whereas company_type has 32 % null values.The cardinality of city and experience is higher.75 % of the columns in major discipline are of the same category  followed similarly in enrolled_university,relevent_experience columns.Lets first handle null columns.","d86ffb9b":"Lets check if there are any new categories available in test which are not present in train,","ad853568":"## Numeric columns"}}