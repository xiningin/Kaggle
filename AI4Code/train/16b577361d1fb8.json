{"cell_type":{"e734cb0d":"code","a2bb1c59":"code","97bcb4c8":"code","98a64422":"code","9736e889":"code","dcadccf5":"code","34098de3":"code","d2e6c6b3":"code","fe444c61":"code","c758f7fc":"code","dfdb59b3":"code","7da578fb":"code","7f7d8983":"code","b7080fc9":"code","45289fb0":"code","a27bb47b":"code","8b42f5e0":"code","f5f3a570":"code","638db930":"code","6fdaef52":"code","5799d87a":"markdown","9b579871":"markdown","b29e220d":"markdown","492545ba":"markdown","ff6dd2ec":"markdown","9db2704a":"markdown","c93e4625":"markdown","b7495590":"markdown","5df52a2f":"markdown","cc55f3d1":"markdown","0fbd0b75":"markdown","8f7e1e68":"markdown"},"source":{"e734cb0d":"import os\nimport sys\nimport random\nimport warnings\nfrom zipfile import ZipFile\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport cv2\n\nfrom tqdm import notebook, tnrange\nfrom itertools import chain\nfrom skimage.io import imread, imshow, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers.core import Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\n\nimport tensorflow as tf\n\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img","a2bb1c59":"for dirname, _, filenames in os.walk('..\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","97bcb4c8":"ZipFile('..\/input\/tgs-salt-identification-challenge\/train.zip').extractall()\nZipFile('..\/input\/tgs-salt-identification-challenge\/test.zip').extractall('test')","98a64422":"# Set some parameters\nim_width = 128\nim_height = 128\nim_chan = 1\npath_train = '..\/working\/'\npath_test = '..\/working\/test\/'","9736e889":"ids= ['1f1cc6b3a4','5b7c160d0d','6c40978ddf','7dfdf6eeb8','7e5a6e5013']\n\nrow = 2\ncolumn = 5\nplt.figure(figsize=(20,10))\nfor j, img_name in enumerate(ids):\n    q = column+j+1\n    img = load_img('..\/working\/images\/' + img_name + '.png')\n    img_mask = load_img('..\/working\/\/masks\/' + img_name + '.png')\n    #print(\"Image of j : {} j : {} q : {}\\n\\n\".format(j,j, q))\n    plt.subplot(row, column,j+1)\n    plt.imshow(img)\n    plt.subplot(row, column,q)\n    plt.imshow(img_mask)\nplt.show()","dcadccf5":"train_ids = next(os.walk(path_train+\"images\"))[2]\ntest_ids = next(os.walk(path_test+\"images\"))[2]","34098de3":"# Get and resize train images and masks\nX_train = np.zeros((len(train_ids), im_height, im_width, im_chan), dtype=np.uint8)\n\n# Since our masked image contains two colors, we'll use bool in this scenario\n# and that's why we've used sigmoid later as an activation function\nY_train = np.zeros((len(train_ids), im_height, im_width, im_chan), dtype=np.bool) \nprint('Getting and resizing train images and masks ... ')\n\n# here total parameter in tqdm is arbitrary\nfor n, id_ in notebook.tqdm(enumerate(train_ids), total=len(train_ids)):\n    \n    # images\n    img = img_to_array(load_img(path_train + '\/images\/' + id_))[:,:,1]\n    X_train[n] = resize(img, (128, 128, 1), mode='constant', preserve_range=True)\n    \n    # masks\n    mask = img_to_array(load_img(path_train + '\/masks\/' + id_))[:,:,1]\n    Y_train[n] = resize(mask, (128, 128, 1), mode='constant', preserve_range=True)\n    \nprint('Finished \ud83d\ude03')","d2e6c6b3":"# Check if training data looks all right\nix = random.randint(0, len(train_ids))\nplt.imshow(np.dstack((X_train[ix],X_train[ix],X_train[ix])))\nplt.show()\n\n# As I've already stored the value of Y_train as boolean, to plot that for now,\n# I've to squeeze it to float or uint8 value\ntmp = np.squeeze(Y_train[ix]).astype(np.uint8)\nplt.imshow(np.dstack((tmp,tmp,tmp)))\nplt.show()","fe444c61":"# Build U-Net model\ninputs = Input((im_height, im_width, im_chan))\ns = Lambda(lambda x: x \/ 255) (inputs)\n\nc1 = Conv2D(8, (3, 3), activation='relu', padding='same') (s)\nc1 = Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\np1 = MaxPooling2D((2, 2)) (c1)\n\nc2 = Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\nc2 = Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\np2 = MaxPooling2D((2, 2)) (c2)\n\nc3 = Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\nc3 = Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\np3 = MaxPooling2D((2, 2)) (c3)\n\nc4 = Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\nc4 = Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\np4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n\nc5 = Conv2D(128, (3, 3), activation='relu', padding='same') (p4)\nc5 = Conv2D(128, (3, 3), activation='relu', padding='same') (c5)\n\nu6 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c5)\nu6 = concatenate([u6, c4])\nc6 = Conv2D(64, (3, 3), activation='relu', padding='same') (u6)\nc6 = Conv2D(64, (3, 3), activation='relu', padding='same') (c6)\n\nu7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c6)\nu7 = concatenate([u7, c3])\nc7 = Conv2D(32, (3, 3), activation='relu', padding='same') (u7)\nc7 = Conv2D(32, (3, 3), activation='relu', padding='same') (c7)\n\nu8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c7)\nu8 = concatenate([u8, c2])\nc8 = Conv2D(16, (3, 3), activation='relu', padding='same') (u8)\nc8 = Conv2D(16, (3, 3), activation='relu', padding='same') (c8)\n\nu9 = Conv2DTranspose(8, (2, 2), strides=(2, 2), padding='same') (c8)\nu9 = concatenate([u9, c1], axis=3)\nc9 = Conv2D(8, (3, 3), activation='relu', padding='same') (u9)\nc9 = Conv2D(8, (3, 3), activation='relu', padding='same') (c9)\n\noutputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n\nmodel = Model(inputs=[inputs], outputs=[outputs])\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.MeanIoU(num_classes=2)])\nmodel.summary()","c758f7fc":"reduce = ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1)\nearlystopper = EarlyStopping(patience=5, verbose=1)\ncheckpointer = ModelCheckpoint('model-tgs-salt-1.h5', verbose=1, save_best_only=True)\nresults = model.fit(X_train, Y_train, validation_split=0.1, batch_size=8, epochs=50, \n                    callbacks=[earlystopper, checkpointer, reduce])","dfdb59b3":"# Get and resize test images\nX_test = np.zeros((len(test_ids), im_height, im_width, im_chan), dtype=np.uint8)\nsizes_test = []\nprint('Getting and resizing test images ... ')\n\nfor n, id_ in notebook.tqdm(enumerate(test_ids), total=len(test_ids)):\n    img = img_to_array(load_img(path_test + '\/images\/' + id_))[:,:,1]\n    X_test[n] = resize(img, (128, 128, 1), mode='constant', preserve_range=True)\n    \n    sizes_test.append([img.shape[0], img.shape[1]]) \n    \n\nprint('Done!')","7da578fb":"# Predict on train, val and test\nmodel.load_weights('model-tgs-salt-1.h5')\npreds_train = model.predict(X_train[:int(X_train.shape[0]*0.9)], verbose=1)\npreds_val = model.predict(X_train[int(X_train.shape[0]*0.9):], verbose=1)\npreds_test = model.predict(X_test, verbose=1)\n\n# Threshold predictions\npreds_train_t = (preds_train > 0.5).astype(np.uint8)\npreds_val_t = (preds_val > 0.5).astype(np.uint8)\npreds_test_t = (preds_test > 0.5).astype(np.uint8)","7f7d8983":"# Create list of upsampled test masks\npreds_test_upsampled = []\nfor i in tnrange(len(preds_test)):\n    preds_test_upsampled.append(resize(np.squeeze(preds_test[i]),#image \n                                       (sizes_test[i][0], sizes_test[i][1]), #output(row, column)\n                                       mode='constant', preserve_range=True))","b7080fc9":"preds_test_upsampled[0].shape","45289fb0":"# Perform a sanity check on some random training samples\nix = random.randint(0, len(preds_train_t))\n#ix = 50\nplt.imshow(np.dstack((X_train[ix],X_train[ix],X_train[ix])))\nplt.show()\ntmp = np.squeeze(Y_train[ix]).astype(np.float32)\nplt.imshow(np.dstack((tmp,tmp,tmp)))\nplt.show()\ntmp = np.squeeze(preds_train_t[ix]).astype(np.float32)\nplt.imshow(np.dstack((tmp,tmp,tmp)))\nplt.show()","a27bb47b":"def RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs","8b42f5e0":"%%time\n# here -4 is used to avoid the last four words \".png\"\npred_dict = { \n\n    fn[:-4] : # All but the last four characters\n                 RLenc(np.round(preds_test_upsampled[i]))\n    \n    for i,fn in notebook.tqdm(enumerate(test_ids), \n                                           total = len(test_ids))\n}","f5f3a570":"print(type(pred_dict))","638db930":"sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('seismicSalatDetectionSubmission.csv')","6fdaef52":"sub.head()","5799d87a":"**0.9** ==> that's a manual way to create both the training and validation sets. It should be select 90% example for training, leave the others for validation","9b579871":"np.zeros returns a new array of given shape and type, filled with zeros.","b29e220d":"We have many examples without salt, as you can see by the masks that are entirely dark. That's great, an algorithm we build will then know that patches exist entirely without salt. Talk about biasing your data.\n\nWe can draw heavily on other work, instead of regurgitating the geophysics work that has been done before. I mentioned that seismic is kind of like ultrasound. So I had a look at https:\/\/www.kaggle.com\/keegil\/keras-u-net-starter-lb-0-277\n\nLet's throw a Unet at our data. I am blatanly stealing from Ketil at this point. All credit goes to him and his nice code.\nFirst we'll need to get our data into a shape that works for U-Nets. That means, it should be a power of 2. Let's do it quick and dirty for now, but eventually, consider aliasing and all that fun.","492545ba":"# Train Model","ff6dd2ec":"# Test Data","9db2704a":"# Importing Packages","c93e4625":"for each pixel we get a value between 0 to 1.<br>\n0 represents no salt and 1 represents salt.<br>\nWe take 0.5 as the threshold to decide whether to classify a pixel as 0 or 1.","b7495590":"# Prepare Submission","5df52a2f":"shape[0] means row and shape[1] means column","cc55f3d1":"# Data Exploration","0fbd0b75":"    os.walk('C:\\dir1\\dir2\\startdir').next()[0] # returns 'C:\\dir1\\dir2\\startdir'\n    os.walk('C:\\dir1\\dir2\\startdir').next()[1] # returns all the dirs in 'C:\\dir1\\dir2\\startdir'\n    os.walk('C:\\dir1\\dir2\\startdir').next()[2] # returns all the files in 'C:\\dir1\\dir2\\startdir'\n    \n    Here, 0,1,2 implies successively current_path, directories in current_path, files in current_path","8f7e1e68":"it's (101,101), cz we didn't change the dimension on test set images any further."}}