{"cell_type":{"1275b47c":"code","235308df":"code","a456bb7a":"code","e2558bfb":"code","063a8243":"code","4e689df4":"code","f8eb5dd3":"code","1ef66854":"code","dfdadfaf":"code","6970e478":"code","896e37b7":"code","dcc29cbb":"code","c1dd18ac":"code","fdae5318":"code","22d19c72":"code","7b6b745e":"code","3e49f4d0":"code","8eb7e0c5":"code","1642917f":"markdown","763db141":"markdown","7964dfb3":"markdown","d6ff316c":"markdown","dd8f2218":"markdown","a4e356e7":"markdown","e7ad3249":"markdown","4a45516c":"markdown","13e0331a":"markdown","83c27c96":"markdown","3e376839":"markdown","c117050a":"markdown","c497fd75":"markdown","3e15ece6":"markdown","f4de736e":"markdown"},"source":{"1275b47c":"# Imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport numpy as np\nimport os\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import make_pipeline\n!pip install category_encoders ### Uncomment this went running notebook for the first time ###\nimport category_encoders as ce","235308df":"# Create the dataframes from the csv's\ntrain = pd.read_csv('..\/input\/train_features.csv')\ntest = pd.read_csv('..\/input\/test_features.csv')\ntrain_labels = pd.read_csv('..\/input\/train_labels.csv')\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')","a456bb7a":"test_ids = test['id']","e2558bfb":"def wrangle(X):\n    \"\"\"Wrangles train, validate, and test sets in the same way\"\"\"\n    X = X.copy()\n\n    # Convert date_recorded to datetime\n    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n    \n    # Extract components from date_recorded, then drop the original column\n    X['year_recorded'] = X['date_recorded'].dt.year\n    X['month_recorded'] = X['date_recorded'].dt.month\n    X['day_recorded'] = X['date_recorded'].dt.day\n    X = X.drop(columns='date_recorded')\n    \n    # Engineer feature: how many years from construction_year to date_recorded\n    X['years'] = X['year_recorded'] - X['construction_year']    \n    \n    # Drop recorded_by (never varies) and id (always varies, random)\n    unusable_variance = ['recorded_by', 'id', 'num_private', \n                         'wpt_name', 'permit', 'management_group',\n                         'water_quality', 'year_recorded',\n                         'extraction_type_group']\n    X = X.drop(columns=unusable_variance)\n    \n    # Drop duplicate columns\n    duplicate_columns = ['quantity_group']\n    X = X.drop(columns=duplicate_columns)\n    \n    # About 3% of the time, latitude has small values near zero,\n    # outside Tanzania, so we'll treat these like null values\n    X['latitude'] = X['latitude'].replace(-2e-08, np.nan)\n    \n    # When columns have zeros and shouldn't, they are like null values\n    cols_with_zeros = ['construction_year', 'longitude', 'latitude',\n                       'gps_height', 'population']\n    for col in cols_with_zeros:\n        X[col] = X[col].replace(0, np.nan)\n        \n    return X","063a8243":"train['status_group'] = train_labels['status_group']","4e689df4":"from sklearn.cluster import KMeans\ntrain['longitude'].values\npoints = train[['longitude', 'latitude']].fillna(0).values\nkmeans = KMeans(n_clusters=14, random_state=42)\nkmeans.fit(points)\ny_km = kmeans.fit_predict(points)\n\ndata = pd.DataFrame({'lon': points[:,0], 'lat': points[:,1], 'cluster':y_km})\n\ntrain['cluster'] = data['cluster']\n\n# color = data['cluster'].replace({0:'r', 1:'orange', 2:'y', 3:'g',\n#                                  4:'b', 5:'indigo', 6:'violet', 7:'black',\n#                                  8:'lightblue', 9:'lightgreen', 10:'grey', \n#                                  11:'r', 12:'brown', 13:'brown', 14:'purple',\n#                                  15:'black'})\n\n\n# plt.figure(figsize=(10, 10))\n# plt.style.use('ggplot')\n# plt.scatter(train['longitude'], train['latitude'], s=2, c = color)\n# plt.xlabel('Longitude', fontsize=20)\n# plt.ylabel('Latitude', fontsize=20)","f8eb5dd3":"test['longitude'].values\npoints = test[['longitude', 'latitude']].fillna(0).values\nkmeans = KMeans(n_clusters=14)\nkmeans.fit(points)\ny_km = kmeans.fit_predict(points)\n\ndata = pd.DataFrame({'lon': points[:,0], 'lat': points[:,1], 'cluster':y_km})\n\ntest['cluster'] = data['cluster']","1ef66854":"train, val = train_test_split(train, test_size= test.shape[0], stratify=train['status_group'])","dfdadfaf":"train = wrangle(train)\nval = wrangle(val)\ntest = wrangle(test)","6970e478":"X_train = train.drop('status_group', axis=1)\ny_train = train['status_group']\nX_val = val.drop('status_group', axis=1)\ny_val = val['status_group']\nX_test = test","896e37b7":"rfc = RandomForestClassifier(n_estimators=1000,\n                               min_samples_split=6,\n                               max_depth = 23,\n                               criterion='gini',\n                               max_features='auto',\n                               random_state=42,\n                               n_jobs=-1)\n\npipe = make_pipeline(\n    ce.OrdinalEncoder(),\n    SimpleImputer(strategy='median'),\n    rfc)\n\npipe.fit(X_train, y_train)\npipe.score(X_train, y_train)","dcc29cbb":"y_pred = pipe.predict(X_val)\naccuracy_score(y_pred, y_val)","c1dd18ac":"importances = rfc.feature_importances_\nfeatures = X_train.columns\nplt.style.use('ggplot')\nplt.figure(figsize=(10, 10))\nplt.barh(features, importances)\nplt.axvline(.008, c='b')","fdae5318":"y_pred = pipe.predict(X_test)","22d19c72":"y_pred = pipe.predict(X_test)\nsub = pd.DataFrame(data = {\n    'id': test_ids,\n    'status_group': y_pred\n})\nsub.to_csv('submission.csv', index=False)","7b6b745e":"color = train['status_group'].replace({'functional': 'g', 'functional needs repair': 'y', 'non functional': 'r'})\nplt.figure(figsize=(10, 10))\nplt.style.use('ggplot')\nplt.scatter(train['longitude'], train['latitude'], s=2, color = color)\nplt.xlabel('Longitude', fontsize=20)\nplt.ylabel('Latitude', fontsize=20)\nred_patch = mpatches.Patch(color='red', label='Non-Functional')\ngreen_patch = mpatches.Patch(color='green', label='Functional')\nyellow_patch = mpatches.Patch(color='yellow', label='Functional, Needs Work')\nplt.legend(handles=[green_patch, yellow_patch, red_patch])\n","3e49f4d0":"train.head()","8eb7e0c5":"plt.scatter(train['population'], train['quantity'], alpha=.1)\n","1642917f":"# Might as well graph these importances and add an arbitrary vline","763db141":"# Create pipeline and fit with Random Forest Classifier, can't forget the SimpleImputer and OrdinalEncoder!","7964dfb3":"# Let's go ahead and cluster those testy bois too","d6ff316c":"# Grab the test ids for later","dd8f2218":"# Here's another visualization for the fun of it.","a4e356e7":"# Gonna slap the labels onto my training df","e7ad3249":"# Use that wrangle function from earlier on the train, validation, and test dataframes! (Thanks again Ryan)","4a45516c":"# Imports","13e0331a":"# Create an X_train, X_val, y_train, y_val, and X_test","83c27c96":"# Train Validation split!","3e376839":"# Bring in the CSV's!","c117050a":"# Wrangle code with this function(Thanks Ryan Herr)","c497fd75":"# Clustering with KMeans! Pretty fancy graphs if I do say so myself!","3e15ece6":"# Some fancy predictions","f4de736e":"# Turns out I like the results, let's predict the test features now!"}}