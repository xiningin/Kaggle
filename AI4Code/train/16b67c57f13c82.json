{"cell_type":{"d05fe942":"code","58fdfdd7":"code","284c8685":"code","b9eeb64e":"code","354805f2":"code","138052f5":"code","d4687b9e":"code","81deb623":"code","78a9af25":"code","68f8cc13":"code","c2d50f04":"code","c7e33cf4":"code","844d6e33":"code","de32c0c9":"code","71af7f70":"code","b7f0ebad":"code","33a5b386":"code","1bf48316":"code","011108fb":"code","aeb45f43":"code","b6ab721e":"code","8caaae90":"code","a985ced1":"code","6c553204":"markdown","1d33ad03":"markdown","7e5eaed1":"markdown","89ce0381":"markdown","a13d1961":"markdown","d3b0ed24":"markdown","52923349":"markdown"},"source":{"d05fe942":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","58fdfdd7":"df_train = pd.read_csv(\"\/kaggle\/input\/toxicvaleur\/comment_train.csv\",encoding=\"UTF8\",sep=\",\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/toxicvaleur\/comment_test.csv\",encoding=\"UTF8\",sep=\",\")","284c8685":"import pickle","b9eeb64e":"def get_embedding(embedding_path):\n    embedding_path = embedding_path\n    # Load embedding indexes\n    with open(embedding_path, 'rb') as f:\n        embedding_indexes = pickle.load(f)\n    # Return\n    return embedding_indexes","354805f2":"embedding_indexes = get_embedding(\"\/kaggle\/input\/toxicvaleur\/cc.en.300.pkl\")","138052f5":"from keras.preprocessing.text import Tokenizer","d4687b9e":"x_train = df_train.comment_text.values\nx_test = df_test.comment_text.values\ny_train = df_train.toxic.values\n","81deb623":"tokenizer = Tokenizer(num_words=100000)\ntokenizer.fit_on_texts(x_train)","78a9af25":"word_index = {e: i for e, i in tokenizer.word_index.items() if i <= tokenizer.num_words}\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\ni = 0\nfor word in list(word_index.keys()):\n    embedding_vector = embedding_indexes.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n        i = i + 1\nprint(f\"Taille de la matrice d'embedding (i.e. nombre de match sur input) : {len(embedding_matrix)}\")","68f8cc13":"from keras.preprocessing.sequence import pad_sequences","c2d50f04":"max_seq_len = 120","c7e33cf4":"sequences = tokenizer.texts_to_sequences(x_train)\nx_train_keras = pad_sequences(sequences, maxlen=max_seq_len, padding='post', truncating='pre')","844d6e33":"sequences = tokenizer.texts_to_sequences(x_test)\nx_test_keras = pad_sequences(sequences, maxlen=max_seq_len, padding='post', truncating='pre')","de32c0c9":"from keras.layers import (ELU, LSTM, AveragePooling1D,\n                                            BatchNormalization, Bidirectional,\n                                            Conv1D, Dense, Dropout, Embedding,\n                                            Flatten, GlobalAveragePooling1D,\n                                            GlobalMaxPooling1D, Input,\n                                            LeakyReLU, MaxPooling1D, ReLU,\n                                            SpatialDropout1D, add, concatenate)\nfrom keras.models import Model, Sequential\nfrom keras.models import load_model as load_model_keras\nfrom keras.optimizers import Adam\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM\nfrom tensorflow.compat.v1.keras.layers import CuDNNGRU","71af7f70":"def get_model():\n\n    # Get input dim\n    input_dim = embedding_matrix.shape[0]\n\n    # Get model\n    num_classes = 1\n    # Process\n    LSTM_UNITS = 50\n    DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n    words = Input(shape=(max_seq_len,))\n    x = Embedding(input_dim, 300, weights=[embedding_matrix], trainable=False)(words)\n    x = BatchNormalization(momentum=0.9)(x)\n    x = SpatialDropout1D(0.5)(x)    \n\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x) \n    x = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x) \n    x = BatchNormalization(momentum=0.9)(x)\n    \n\n    avg_pool1 = GlobalAveragePooling1D()(x)\n    max_pool1 = GlobalMaxPooling1D()(x)\n\n    x = concatenate([avg_pool1, max_pool1])\n    # Last layer\n    activation = 'sigmoid' \n    x = BatchNormalization(momentum=0.9)(x)\n    out = Dense(num_classes, activation=activation, kernel_initializer='glorot_uniform')(x)\n\n    # Compile model\n    model = Model(inputs=words, outputs=[out])\n    metrics = ['acc']\n    model.compile(optimizer=Adam(lr=0.01,decay=0.001), loss='binary_crossentropy', metrics=metrics)\n    model.summary()\n\n\n    # Return\n    return model\n","b7f0ebad":"from keras.callbacks import (CSVLogger, EarlyStopping,TerminateOnNaN)\n\ncallbacks = [EarlyStopping(monitor='val_loss', patience=12, mode='auto', restore_best_weights=True)]\ncallbacks.append(CSVLogger(filename='logger.csv', separator=';', append=False))\ncallbacks.append(TerminateOnNaN())\n","33a5b386":"NUM_MODELS=4","1bf48316":"predictions_model2 = np.zeros((x_test_keras.shape[0],1))\nfor model_idx in range(NUM_MODELS):\n    print(\"MODEL n\u00b0\"+ str(model_idx))\n    model = get_model()\n\n    history = model.fit(x_train_keras, y_train,\n                        epochs=999,\n                        batch_size=128,\n                        verbose=1,\n                        validation_split=0.2,\n                        callbacks = callbacks)\n    predictions_model2=predictions_model2+(model.predict(x_test_keras, batch_size=64))\n    \npredictions_2=predictions_model2\/(NUM_MODELS)","011108fb":"res_test = predictions_2","aeb45f43":"res_test","b6ab721e":"submission = df_test\nsubmission[\"toxic\"] = np.round(res_test).astype(int)\nsubmission = submission[[\"id\",\"toxic\"]]","8caaae90":"print(submission)","a985ced1":"submission.to_csv(\"submission.csv\",encoding=\"utf8\",sep=',',index=False)","6c553204":"### obtention des pr\u00e9dictions sur le test","1d33ad03":"### build model","7e5eaed1":"### embedding","89ce0381":"### Sauvegarde du fichier au format demand\u00e9","a13d1961":"### build sequences","d3b0ed24":"### Create embedding matrix","52923349":"### Load data"}}