{"cell_type":{"3dd691ec":"code","b6b140c6":"code","8346d3ac":"code","d26bd0cb":"code","f79f57e5":"code","fe6cb6a0":"code","a48cd060":"code","499dbf27":"code","3a151124":"code","b64a8b7f":"code","4bfe4879":"code","c4c26cc7":"code","66a4fc40":"code","baf060b9":"code","0f309fe9":"code","049eb81d":"code","0343e99e":"code","2cdff9f5":"code","e2cc3242":"code","c4b5186a":"code","48c08827":"code","b535247b":"code","8131723e":"code","535a84b8":"code","1990b979":"code","d2b0162b":"code","9d88e658":"markdown","e8adf3d4":"markdown","136272a6":"markdown","0171a52e":"markdown","ff1bfee6":"markdown"},"source":{"3dd691ec":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn import decomposition\nfrom sklearn.model_selection import train_test_split\nimport time\nimport os\n# import warnings\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')","b6b140c6":"X=np.load('..\/input\/Sign-language-digits-dataset\/X.npy')\nY=np.load('..\/input\/Sign-language-digits-dataset\/Y.npy')","8346d3ac":"plt.figure(figsize=(10,10))\nfor i in range(0, 8):\n    plt.subplot(440 + 1 + i)\n    plt.tight_layout()\n    plt.imshow(X[i*100], cmap=plt.get_cmap('gray'))\n    plt.axis('off')\n    plt.title(Y[i*100].argmax())    \nplt.show()","d26bd0cb":"X.shape","f79f57e5":"Y.shape","fe6cb6a0":"num_class = np.unique(Y.argmax(axis=1), return_counts=True) \nplt.title(\"Number of class\")\nplt.xticks(num_class[0])\nplt.bar(num_class[0], num_class[1],color = (0.2, 0.4, 0.6, 0.6) )\nplt.show()","a48cd060":"X_flat = np.array(X).reshape((-1, 64*64))\nX_train, X_test, y_train, y_test = train_test_split(X_flat, Y, test_size=0.3, random_state=42)","499dbf27":"X_flat.shape","3a151124":"from sklearn.decomposition import PCA\npca_dims = PCA()\npca_dims.fit(X_flat)\ncumsum = np.cumsum(pca_dims.explained_variance_ratio_)\nd = np.argmax(cumsum >= 0.95) + 1\nd","b64a8b7f":"pca = PCA(n_components=d)\nX_reduced = pca.fit_transform(X_flat)\nX_recovered = pca.inverse_transform(X_reduced)","4bfe4879":"print(\"reduced shape: \" + str(X_reduced.shape))\nprint(\"recovered shape: \" + str(X_recovered.shape))","c4c26cc7":"f = plt.figure()\nf.add_subplot(1,2, 1)\nplt.title(\"original\")\nplt.imshow(X_train[0].reshape((64,64)))\nf.add_subplot(1,2, 2)\n\nplt.title(\"PCA compressed\")\nplt.imshow(X_recovered[0].reshape((64,64)))\nplt.show(block=True)\n","66a4fc40":"X_recovered","baf060b9":"X = X.reshape(-1, 64, 64, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=20)","0f309fe9":"X_train.shape","049eb81d":"y_train.shape","0343e99e":"X_train = X_train \/255\nX_test = X_test\/255","2cdff9f5":"#X_test","e2cc3242":"from sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten,Convolution2D, MaxPool2D\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n","c4b5186a":"import keras\nfrom keras import layers,models\nfrom keras.layers import BatchNormalization\n\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.core import Activation\nfrom keras.layers.core import Flatten\nfrom keras.layers.core import Dense,Dropout\nfrom keras.utils import np_utils\nfrom keras.layers.convolutional import Conv2D\nfrom keras.optimizers import SGD, RMSprop, Adam","48c08827":"classifier = Sequential()\n\n# Step 1 - Convolution\nclassifier.add(Conv2D(68, (5, 5), input_shape = (64, 64, 1), activation = 'relu'))\n\n# Step 2 - Pooling\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\n\n# Adding a second convolutional layer\nclassifier.add(Conv2D(68, (5, 5), activation = 'relu'))\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\n\nclassifier.add(Conv2D(68, (5, 5), activation = 'relu'))\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\n\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\n# Step 3 - Flattening\nclassifier.add(Flatten())\n\n# Step 4 - Full connection\nclassifier.add(Dense(units = 256, activation = 'relu'))\nclassifier.add(BatchNormalization())\nclassifier.add(Dense(units = 10, activation = 'softmax'))\n\n# Compiling the CNN\nclassifier.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])","b535247b":"from keras.preprocessing.image import ImageDataGenerator\n\n# Generate Images\ntrain_datagen = ImageDataGenerator(\n                                   shear_range = 0.08,\n                                   zoom_range = 0.08,\n                                   horizontal_flip = False,\n                                   width_shift_range= 0.02,\n                                   height_shift_range= 0.02)\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\n\n# fit parameters from data\ntraining_set = train_datagen.flow(X_train, y_train, batch_size=64)\ntest_set = test_datagen.flow(X_test, y_test, batch_size=64)\n\nhistory = classifier.fit_generator(training_set,\n                         steps_per_epoch = 60,\n                         epochs =20,\n                         validation_data = test_set,\n                         validation_steps = 500)","8131723e":"scores = classifier.evaluate(X_test, y_test, verbose=0)\nprint(\"{}: {:.2f}%\".format(classifier.metrics_names[1], scores[1]*100))","535a84b8":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# \n# Predict the values from the validation dataset\nY_pred = classifier.predict(X_test)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_test,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"BuPu\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","1990b979":"plt.plot(history.history['val_loss'], color='b', label=\"validation loss\")\nplt.plot(history.history['loss'], color='r', label=\"training loss\")\nplt.title(\"Test Loss\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","d2b0162b":"# Plot the accuracy curve for validation \nplt.plot(history.history['val_acc'], color='g', label=\"validation accuracy\")\nplt.title(\"Validation Accuracy\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","9d88e658":"### Upload Dataset\n\n##### Dataset GitHub Page: github.com\/ardamavi\/Sign-Language-Digits-Dataset\n\n#### Details of datasets:\n* Image size: 64x64\n* Color space: Grayscale\n* File format: npy\n* Number of classes: 10 (Digits: 0-9)\n* Number of participant students: 218\n* Number of samples per student: 10","e8adf3d4":"### Train-Test Split\nLets split X and Y into train and test sets.","136272a6":"Gray-scale","0171a52e":"### Create Model","ff1bfee6":"#### Principle Componenet Analysis (PCA)\n* Pca is a dimension reduction method to better analyze data"}}