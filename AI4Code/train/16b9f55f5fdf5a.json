{"cell_type":{"6e41b3a9":"code","7704d115":"code","e30c0d16":"code","1138c344":"code","8cbb5bf5":"code","c49f2bcb":"code","bceb7449":"code","dd59649e":"code","b7c1e5ff":"code","4cb7eff6":"code","412dcdb4":"code","56df75eb":"code","3f7c1a55":"code","34638ec6":"markdown","9a93f04f":"markdown","02731f03":"markdown","fffd8613":"markdown"},"source":{"6e41b3a9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nDATA_PATH = \"..\/input\/s1-pytorch-speaker-verification-data-prep\"\ntest_data = []\ntrain_data = []\nfor dirname, _, filenames in os.walk(DATA_PATH):\n    for filename in filenames:\n        str = os.path.join(dirname, filename)\n        if \"test\" in str:\n            test_data.append(str)\n        elif \"train\" in str:\n            train_data.append(str)\n            \n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7704d115":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport time\n\nimport matplotlib.pyplot as plt\nimport random\n","e30c0d16":"def get_hyper_parameters():\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    print(f\"Device type available {device}\")\n    hparam_dict = {\n\n        # genereal parameters\n        \"is_training_mode\": True,\n        \"device\": device,\n\n        \"model_path\": \"pre_trained_models\",\n        \"log_file\": \"logs\/log.txt\",\n        \"checkpoint_dir\": \"chk_pts\",\n\n        \"is_data_preprocessed\": True,\n        \"sr\": 16000,\n\n        # For mel spectrogram preprocess\n        \"nfft\": 512,\n        \"window\": 0.025,  # (s)\n        \"hop\": 0.01,  # (s)\n        \"n_mels\": 40,  # Number of mel energies\n        \"tisv_frame\": 180,  # Max number of time steps in input after preprocess\n\n        # model hyper parameters\n        \"hidden\": 768,  # Number of LSTM hidden layer units\n        \"num_layer\": 3,  # Number of LSTM layers\n        \"proj\": 256,  # Embedding size\n\n        # train:\n        \"training_N\": 4,  # Number of speakers in batch\n        \"training_M\": 5,  # Number of utterances per speaker\n        \"training_num_workers\": 0,  # number of workers for dataloader\n        \"lr\": 0.01,\n        \n        \"training_epochs\": 10000,  # Max training speaker epoch\n        \"log_interval\": 30,  # Epochs before printing progress\n        \"checkpoint_interval\": 2000,  # Save model after x speaker epochs\n        \"restore_existing_model\": False,  # Resume training from previous model path\n        \"verbose\": True,\n\n        \n        # small error\n        \"small_err\": 1e-6,\n\n\n    }\n    return hparam_dict\n\n\nclass DictWithDotNotation(dict):\n    \"\"\"\n    a dictionary that supports dot notation \n    as well as dictionary access notation \n    usage: d = DotDict() or d = DotDict({'val1':'first'})\n    set attributes: d.val2 = 'second' or d['val2'] = 'second'\n    get attributes: d.val2 or d['val2']\n    \"\"\"\n    __getattr__ = dict.__getitem__\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n\n    def __init__(self, dct=None):\n        dct = dict() if not dct else dct\n        for key, value in dct.items():\n            if hasattr(value, 'keys'):\n                value = Dict_with_dot_notation(value)\n            self[key] = value\n\n\nclass HyperParameters(DictWithDotNotation):\n\n    def __init__(self, hp_dict=None):\n        super(DictWithDotNotation, self).__init__()\n\n        if hp_dict is None:\n            hp_dict = get_hyper_parameters()\n\n        hp_dotdict = DictWithDotNotation(hp_dict)\n        for k, v in hp_dotdict.items():\n            setattr(self, k, v)\n\n    __getattr__ = DictWithDotNotation.__getitem__\n    __setattr__ = DictWithDotNotation.__setitem__\n    __delattr__ = DictWithDotNotation.__delitem__\n\n\nhp = HyperParameters()\nhp","1138c344":"\nclass SpeakerDatasetPreprocessed(Dataset):\n\n    def __init__(self, shuffle=True, utter_start=0):\n\n        self.lst_train_data = train_data\n        self.utter_num = hp.training_M\n\n        self.file_list = train_data #os.listdir(self.path)\n        self.shuffle = shuffle\n        self.utter_start = utter_start\n\n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx):\n\n        np_file_list = self.file_list\n\n        if self.shuffle:\n            # select random speaker\n            selected_file = random.sample(np_file_list, 1)[0]\n        else:\n            selected_file = np_file_list[idx]\n\n#         print(selected_file)\n        # load utterance spectrogram of selected speaker\n        utters = np.load(selected_file)\n        if self.shuffle:\n            # select M utterances per speaker\n            utter_index = np.random.randint(0, utters.shape[0], self.utter_num)\n            utterance = utters[utter_index]\n        else:\n            # utterances of a speaker [batch(M), n_mels, frames]\n            utterance = utters[self.utter_start: self.utter_start + self.utter_num]\n\n        # TODO implement variable length batch size\n        utterance = utterance[:, :, :160]\n\n        utterance = torch.tensor(np.transpose(utterance, axes=(0, 2, 1)))  # transpose [batch, frames, n_mels]\n        return utterance","8cbb5bf5":"train_dataset = SpeakerDatasetPreprocessed()\ntrain_loader = DataLoader(train_dataset,\n                          batch_size=hp.training_N,  # number of speakers\n                          shuffle=True,\n                          num_workers=hp.training_num_workers,\n                          drop_last=True)","c49f2bcb":"for mel_db_batch in train_loader:\n    print(mel_db_batch.shape)","bceb7449":"class SpeechEmbedModel(nn.Module):\n\n    def __init__(self):\n        super(SpeechEmbedModel, self).__init__()\n\n        # this creates a three stacks (hp.num_layer) of LSTM\n        self.LSTM_stack = nn.LSTM(hp.n_mels, hp.hidden, num_layers=hp.num_layer, batch_first=True)\n\n        for name, param in self.LSTM_stack.named_parameters():\n            if 'bias' in name:\n                nn.init.constant_(param, 0.0)\n            elif 'weight' in name:\n                nn.init.xavier_normal_(param)\n\n        # feed forward layer\n        self.projection = nn.Linear(hp.hidden, hp.proj)\n\n    def forward(self, x):\n        x, _ = self.LSTM_stack(x.float())  # (batch, frames, n_mels)\n        # only use last frame\n        x = x[:, x.size(1) - 1]\n        x = self.projection(x.float())\n\n        # The embedding vector (d-vector) is defined as the L2 normalization of the network output\n        x = x \/ torch.norm(x, dim=1).unsqueeze(1)\n        return x\n","dd59649e":"# testing the untrained model\ntmp_model = SpeechEmbedModel()\ntotal_utterances = hp.training_N * hp.training_M\n\nfor mel_db_batch in train_loader:\n    # mel is returned as 4x5x160x40 (batchxnum_speakerxutterlenxn_mel)and we will reshape it to 20x160x40\n    new_shape = (total_utterances, mel_db_batch.size(2), mel_db_batch.size(3))\n    mel_db_batch = torch.reshape(mel_db_batch, new_shape)\n    res = tmp_model(mel_db_batch)\n    break\n\nres.shape, res","b7c1e5ff":"class GE2ELoss(nn.Module):\n\n    def __init__(self, device):\n        super(GE2ELoss, self).__init__()\n        self.w = nn.Parameter(torch.tensor(10.0).to(device), requires_grad=True)\n        self.b = nn.Parameter(torch.tensor(-5.0).to(device), requires_grad=True)\n        self.device = device\n\n    def forward(self, embeddings):\n        # Clamps all elements in input into the range [ min, max ]\n        # https:\/\/pytorch.org\/docs\/stable\/generated\/torch.clamp.html\n        torch.clamp(self.w, hp.small_err)\n        centroids = GE2ELoss.get_centroids(embeddings)\n        cos_similarity = GE2ELoss.get_cos_sim(embeddings, centroids)\n\n        # this is eq (5) from the paper https:\/\/arxiv.org\/abs\/1710.10467\n        sim_matrix = self.w * cos_similarity.to(self.device) + self.b\n\n        loss, _ = GE2ELoss.calc_loss(sim_matrix)\n        return loss\n\n    # eq. (1)\n    @staticmethod\n    def get_centroids(embeddings):\n        # calculating centroid of all the utterances  as per eq. (1)\n        # centroid for neg similarity\n        centroids = embeddings.mean(dim=1)\n        return centroids\n\n    # calculates cos similarities of eq. (5)\n    @staticmethod\n    def get_cos_sim(embeddings, centroids):\n        # number of utterances per speaker\n        num_utterances = embeddings.shape[1]\n        utterance_centroids = GE2ELoss.get_utterance_centroids(embeddings)\n\n        # flatten the embeddings and utterance centroids to just utterance,\n        # so we can do cosine similarity\n        utterance_centroids_flat = utterance_centroids.view(\n            utterance_centroids.shape[0] * utterance_centroids.shape[1],\n            -1)\n        embeddings_flat = embeddings.view(embeddings.shape[0] * num_utterances, -1)\n        # the cosine distance between utterance and the associated centroids\n        # for that utterance\n        # this is each speaker's utterances against his own centroid, but each\n        # comparison centroid has the current utterance removed\n        cos_same = F.cosine_similarity(embeddings_flat, utterance_centroids_flat)\n\n        # now we get the cosine distance between each utterance and the other speakers' centroids\n        # to do so requires comparing each utterance to each centroid. To keep the\n        # operation fast, we vectorize by using matrices L (embeddings) and\n        # R (centroids) where L has each utterance repeated sequentially for all\n        # comparisons and R has the entire centroids frame repeated for each utterance\n        centroids_expand = centroids.repeat((num_utterances * embeddings.shape[0], 1))\n        embeddings_expand = embeddings_flat.unsqueeze(1).repeat(1, embeddings.shape[0], 1)\n        embeddings_expand = embeddings_expand.view(\n            embeddings_expand.shape[0] * embeddings_expand.shape[1],\n            embeddings_expand.shape[-1]\n        )\n        cos_diff = F.cosine_similarity(embeddings_expand, centroids_expand)\n        cos_diff = cos_diff.view(\n            embeddings.size(0),\n            num_utterances,\n            centroids.size(0)\n        )\n        # assign the cosine distance for same speakers to the proper idx\n        same_idx = list(range(embeddings.size(0)))\n        cos_diff[same_idx, :, same_idx] = cos_same.view(embeddings.shape[0], num_utterances)\n        cos_diff = cos_diff + hp.small_err\n        return cos_diff\n\n    # eq (8)\n    @staticmethod\n    def get_centroid(embeddings, speaker_num, utterance_num):\n        # c(j\u2212i) = 1  # mmX=1  # ejm, (8)\n        # eq 8\n        centroid = 0\n        for utterance_id, utterance in enumerate(embeddings[speaker_num]):\n            if utterance_id == utterance_num:\n                continue\n            centroid = centroid + utterance\n        centroid = centroid \/ (len(embeddings[speaker_num]) - 1)\n        return centroid\n\n    @staticmethod\n    def get_utterance_centroids(embeddings):\n        \"\"\"\n        Returns the centroids for each utterance of a speaker, where\n        the utterance centroid is the speaker centroid without considering\n        this utterance\n\n        Shape of embeddings should be:\n            (speaker_ct, utterance_per_speaker_ct, embedding_size)\n        \"\"\"\n        sum_centroids = embeddings.sum(dim=1)\n        # we want to subtract out each utterance, prior to calculating the\n        # the utterance centroid\n        sum_centroids = sum_centroids.reshape(sum_centroids.shape[0], 1, sum_centroids.shape[-1])\n        # we want the mean but not including the utterance itself, so -1\n        num_utterances = embeddings.shape[1] - 1\n        centroids = (sum_centroids - embeddings) \/ num_utterances\n        return centroids\n\n    @staticmethod\n    def calc_loss(sim_matrix):\n        same_idx = list(range(sim_matrix.size(0)))\n\n        # eq. (6)\n        pos = sim_matrix[same_idx, :, same_idx]\n        neg = (torch.exp(sim_matrix).sum(dim=2) + hp.small_err).log_()\n        per_embedding_loss = -1 * (pos - neg)  # this is the loss as per eq. (6)\n\n        # eq. (10) final loss\n        # the final GE2E loss LG is\n        # the sum of all losses over the similarity matrix (1 \u2264 j \u2264 N, and 1 \u2264 i \u2264 M)\n        loss = per_embedding_loss.sum()\n        return loss, per_embedding_loss\n","4cb7eff6":"# testing the loss function\nimport torch.autograd as grad\n\nw = grad.Variable(torch.tensor(1.0))\nb = grad.Variable(torch.tensor(0.0))\n\nlst = [[0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\nembeddings = torch.tensor(lst).to(torch.float).reshape(3, 2, 3)\ncentroids = GE2ELoss.get_centroids(embeddings)\ncos_sim = GE2ELoss.get_cos_sim(embeddings, centroids)\nsim_matrix = w * cos_sim + b\nloss, per_embedding_loss = GE2ELoss.calc_loss(sim_matrix)\n\nprint(loss)\nprint(per_embedding_loss)","412dcdb4":"def train_model(model_path, train_dataset, lr_reduce=2000, epoch_print=100, dot_print=10):\n    \"\"\"\n        ARGS:\n        model_path - path where the model will be stored after the training\n        train_dataset - object of dataset to create dataloader\n        lr_reduce - num of epochs after which the learning rate will be reduced to half\n        epoch print - how many epochs to club together to print e.g. Epoch 100\/4000 \n        dot_print - how many epochs to print \".\" between each epoch print line\n    \"\"\"\n    device = torch.device(hp.device)\n    \n    train_loader = DataLoader(train_dataset,\n                              batch_size=hp.training_N,  # number of speakers\n                              shuffle=True,\n                              num_workers=hp.training_num_workers,\n                              drop_last=True)\n\n    model = SpeechEmbedModel().to(device)\n    if hp.restore_existing_model:\n        model.load_state_dict(torch.load(model_path))\n\n    ge2e_loss = GE2ELoss(device)\n    # Both net and loss have trainable parameters\n    lst_train_params_for = [\n        {'params': model.parameters()},\n        {'params': ge2e_loss.parameters()}\n    ]\n    lr = hp.lr\n    optimizer = torch.optim.SGD(lst_train_params_for, lr=lr)\n\n    # creating the folder to save checkpoints\n    os.makedirs(hp.checkpoint_dir, exist_ok=True)\n\n    # setting model to train mode\n    model.train()\n    losses = []\n    total_utterances = hp.training_N * hp.training_M\n    \n    training_st = time.time()\n    batch_loss = []\n    flushed = True\n    for e in range(hp.training_epochs):\n        \n        if flushed:\n            print(f\"Epoch:[{e+epoch_print}\/{hp.training_epochs}] \", end=\"\")\n            flushed = False\n        \n        for mel_db_batch in train_loader:\n\n            # sending data to GPU\/TPU for calculation\n            mel_db_batch = mel_db_batch.to(device)\n\n            # mel is returned as 4x5x160x40 (batchxnum_speakerxutterlenxn_mel)and we will reshape it to 20x160x40\n            new_shape = (total_utterances, mel_db_batch.size(2), mel_db_batch.size(3))\n            mel_db_batch = torch.reshape(mel_db_batch, new_shape)\n\n            perm = random.sample(range(0, total_utterances), total_utterances)\n            unperm = list(perm)\n\n            # saving the unpermutated status of the utterances, this will be used to fetch correct utterance per person\n            for i, j in enumerate(perm):\n                unperm[j] = i\n\n            mel_db_batch = mel_db_batch[perm]\n\n            # gradient accumulates\n            optimizer.zero_grad()\n\n            embeddings = model(mel_db_batch)\n            embeddings = embeddings[unperm]\n\n            # changing the shape back num_speakers x utter_per_speaker x embedding_vector\n            embeddings = torch.reshape(embeddings, (hp.training_N, hp.training_M, embeddings.size(1)))\n\n            # get loss, call backward, step optimizer\n            loss = ge2e_loss(embeddings)  # wants (Speaker, Utterances, embedding)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n            torch.nn.utils.clip_grad_norm_(ge2e_loss.parameters(), 1.0)\n            optimizer.step()\n\n            batch_loss.append(loss.to(\"cpu\").detach().numpy())\n            \n\n        if (e + 1) % dot_print == 0:\n            print(\".\", end=\"\")\n        \n        if (e + 1) % epoch_print == 0:\n            # batch_loss containts 4xepoch_print number of losses\n            # then we take average and print it\n            curr_loss = np.mean(batch_loss)\n            # list of all the losses for each epoch\n            losses.append(curr_loss)\n            \n            # reset the batch loss\n            batch_loss = []\n\n            if e == epoch_print - 1:\n                epoch_st = training_st\n            \n            epoch_et = time.time()\n            hours, rem = divmod(epoch_et - epoch_st, 3600)\n            minutes, seconds = divmod(rem, 60)\n            time_msg = \"{:0>2}:{:0>2}:{:0.0f}\".format(int(hours), int(minutes), seconds)\n            msg = \" Loss:{0:.4f}\\t\".format(curr_loss)\n            msg = msg + time_msg\n            print(msg, end=\"\\n\")\n            \n            flushed = True\n            epoch_st = epoch_et\n\n        \n        # reducing the learning rate by half\n        if (e+1) % lr_reduce == 0:\n            print(f\"Reducing learning rate from {lr} to {lr\/2}\")\n            lr = lr \/ 2\n            optimizer.param_groups[0]['lr'] = lr\n        \n        if hp.checkpoint_dir is not None and (e + 1) % hp.checkpoint_interval == 0:\n            model.eval().cpu()\n            ckpt_model_filename = f\"ckpt_epoch_{e + 1}.pth\"\n            ckpt_model_path = os.path.join(hp.checkpoint_dir, ckpt_model_filename)\n            torch.save(model.state_dict(), ckpt_model_path)\n            model.to(device).train()\n\n    # save model\n    model.eval().cpu()\n    save_model_filename = f\"final_epoch_{e + 1}.model\"\n    save_model_path = os.path.join(hp.checkpoint_dir, save_model_filename)\n    torch.save(model.state_dict(), save_model_path)\n\n    print(\"Completed!! Trained model is saved at: \\n\", save_model_path)\n\n    return model, losses\n\n","56df75eb":"hp.lr = 0.05\nhp.training_epochs = 10000\nmodel, losses = train_model(hp.model_path, train_dataset, lr_reduce=2000, epoch_print=100, dot_print=10)","3f7c1a55":"plt.plot(losses)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")","34638ec6":"# Defining the MODEL","9a93f04f":"# Training the model","02731f03":"# Part2 of 3\n# In this notebook, we will train the model","fffd8613":"# GE2E loss definition"}}