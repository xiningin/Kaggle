{"cell_type":{"620e6f76":"code","5030f049":"code","c4de1707":"code","0b19fd36":"code","64bd6279":"code","054b2a5b":"code","75c17f82":"code","06d5a877":"code","331b8b89":"code","3c50a1aa":"code","32c99b5c":"code","09cb733c":"code","6e9c14ac":"code","01a2e256":"code","56c462eb":"code","b15acdc7":"code","cece0f8c":"code","253c9686":"code","01d5d3c0":"code","5ddd6350":"code","802d1b5f":"code","0e8add5c":"code","bf049d73":"code","78331258":"code","2adf55d5":"code","3c02a336":"code","89f6dad1":"code","03de2fb0":"code","66dbf7f7":"code","63a1da21":"code","ae125241":"code","fe3e21d7":"code","b05f4fa9":"code","8fa7e527":"code","5e9a1871":"code","ab638caa":"code","7afe8f85":"code","9c13de3f":"code","f130b98d":"code","790e26a1":"code","9adf7669":"code","4801ca52":"code","d917cfb0":"code","3bacf1a3":"code","d783d573":"code","fa0d0630":"code","343339d9":"code","e4b4a917":"code","db49f50a":"code","ced718ae":"markdown","6fd86d8f":"markdown","84707f02":"markdown","1b2fd0c4":"markdown","60d8103e":"markdown","92a47d75":"markdown","2a0c3046":"markdown","0964db59":"markdown","1c48e02b":"markdown","44b237c9":"markdown","f0471541":"markdown","7b6d10a4":"markdown","e097a4f3":"markdown","f9997e22":"markdown","e90c8de7":"markdown","b8b79f93":"markdown","32693d39":"markdown"},"source":{"620e6f76":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport seaborn as sns\nimport math\nimport matplotlib as p\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport scipy.stats as sps\nimport re","5030f049":"train = pd.read_csv('..\/input\/widsdatathon2020\/training_v2.csv')\ntest = pd.read_csv('..\/input\/widsdatathon2020\/unlabeled.csv')\nst = pd.read_csv('..\/input\/widsdatathon2020\/solution_template.csv')\nss = pd.read_csv('..\/input\/widsdatathon2020\/samplesubmission.csv')\ndictionary = pd.read_csv('..\/input\/widsdatathon2020\/WiDS Datathon 2020 Dictionary.csv')\n\npd.set_option('display.max_columns', 500)\nprint('solution template shape', st.shape)\ndisplay(st.head())\nprint('dictionary shape', dictionary.shape)\ndisplay(dictionary.T.head())\nprint('train shape', train.shape)\ndisplay(train.head())\nprint('test shape', test.shape)\ndisplay(test.head())","c4de1707":"# Dropping patient_id for now\ntrain = train.copy().drop('patient_id', axis = 1)\ntest = test.copy().drop('patient_id', axis = 1)","0b19fd36":"from sklearn.model_selection import train_test_split\n\nTrain, Validation = train_test_split(train, test_size = 0.3)","64bd6279":"X_train = Train.copy().drop('hospital_death', axis = 1)\ny_train = Train[['encounter_id','hospital_death']]\nX_val = Validation.copy().drop('hospital_death', axis = 1)\ny_val = Validation[['encounter_id','hospital_death']]","054b2a5b":"X_test = test.copy().drop('hospital_death', axis = 1)\ny_test = test[['encounter_id','hospital_death']]","75c17f82":"sns.catplot('hospital_death', data= train, kind='count', alpha=0.7, height=6, aspect=1)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = train['hospital_death'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of Hospital Deaths', fontsize = 20, color = 'black')\nplt.show()","06d5a877":"plt.figure(figsize=(30,15))\nethnicity_vs_death = sns.catplot(x='ethnicity', col='hospital_death', kind='count', data=train, \n                                 order = train['ethnicity'].value_counts().index, height = 7, aspect = 1);\nethnicity_vs_death.set_xticklabels(rotation=90);","331b8b89":"plt.figure(figsize=(30,15))\nhas_vs_death = sns.catplot(x='hospital_admit_source', col='hospital_death', kind='count', data=train, \n                           order = train['hospital_admit_source'].value_counts().index, height = 7, aspect = 1.5);\nhas_vs_death.set_xticklabels(rotation=90);","3c50a1aa":"plt.figure(figsize=(30,15))\nias_vs_death = sns.catplot(x='icu_admit_source', col='hospital_death', kind='count', data=train, \n                           order = train['icu_admit_source'].value_counts().index, height = 7, aspect = 1.5);\nias_vs_death.set_xticklabels(rotation=90);","32c99b5c":"# Freq plot of Hospital ID for hospital_death = 0\ntrain_hID_0 = train[train['hospital_death'] == 0]\nplt.figure(figsize=(30,15))\nhID_vs_death = sns.catplot(y='hospital_id',  orient = \"v\", kind='count', data=train_hID_0, order = train_hID_0['hospital_id'].value_counts().index, \n                           height = 30, aspect = 1)","09cb733c":"# Freq plot of Hospital ID for hospital_death = 1\ntrain_hID_1 = train[train['hospital_death'] != 0]\nplt.figure(figsize=(30,20))\nhID_vs_death = sns.catplot(y='hospital_id',  orient = \"v\", kind='count', data=train_hID_1, order = train_hID_1['hospital_id'].value_counts().index, \n                           height = 30, aspect = 1);","6e9c14ac":"# Freq plot of Hospital ID for hospital_death = 0 & 1\nplt.figure(figsize=(30,40))\nhID_vs_death = sns.catplot(x = 'hospital_id', col='hospital_death', kind='count', data=train, order = train['hospital_id'].value_counts().index, \n                           height = 5, aspect = 2.8);\nhID_vs_death.set_xticklabels(rotation=90);","01a2e256":"plt.figure(figsize = (15,5))\nsns.kdeplot(train_hID_1['age'], shade=True, color=\"r\")\nsns.kdeplot(train_hID_0['age'], shade=True, color=\"b\")","56c462eb":"sns.jointplot(x=\"age\", y=\"bmi\", data=train, kind = \"kde\")\nsns.jointplot(x=\"age\", y=\"height\", data=train, kind = \"kde\")\nsns.jointplot(x=\"age\", y=\"weight\", data=train, kind = \"kde\")","b15acdc7":"dataset = pd.concat(objs=[X_train, X_val], axis=0)","cece0f8c":"col_1 = dataset.columns","253c9686":"for i in col_1:\n    if X_train[i].nunique() == 1:\n        print('in Train', i)\n    if X_val[i].nunique() == 1:\n        print('in Val', i)\n    if X_test[i].nunique() == 1:\n        print('in Test', i)\n    ","01d5d3c0":"# Dropping 'readmission_status'\nX_train = X_train.drop(['readmission_status'], axis=1)\nX_val = X_val.drop(['readmission_status'], axis=1)\nX_test = X_test.drop(['readmission_status'], axis=1)","5ddd6350":"print('For Train')\nd1 = X_train.nunique()\nprint(sorted(d1))\nprint(\"==============================\")\nprint('For Validation')\nd2 = X_val.nunique()\nprint(sorted(d2))\n\n# Considering columns with <= 15 unique values for conversion","802d1b5f":"d = pd.concat(objs=[X_train, X_val], axis=0)","0e8add5c":"col = d.columns ","bf049d73":"# For Train data\nl1 = []\nfor i in col:\n    if X_train[i].nunique() <= 15:\n        l1.append(i)\n        \nl1","78331258":"# For Val data\nl2 = []\nfor i in col:\n    if X_val[i].nunique() <= 15:\n        l2.append(i)\n        \nl2","2adf55d5":"# For Test data\nl3 = []\nfor i in col:\n    if X_test[i].nunique() <= 15:\n        l3.append(i)\n        \nl3","3c02a336":"# Checking for columns in X_train and X_validation\nset(l1) & set(l2)","89f6dad1":"# Checking for columns in X_train and X_test\nset(l1) & set(l3)","03de2fb0":"print('Train', len(l1))\nprint('Validation', len(l2))\nprint('Common', len(set(l1) & set(l2)))","66dbf7f7":"print('Train', len(l1))\nprint('Test', len(l3))\nprint('Common', len(set(l1) & set(l3)))","63a1da21":"X_train[l1].dtypes","ae125241":"X_val[l2].dtypes\n# Not a necessary step since we already confirmed the common columns. Included just for reference. ","fe3e21d7":"X_train[l1] = pd.Categorical(X_train[l1])\nX_val[l2] = pd.Categorical(X_val[l2])\nX_test[l3] = pd.Categorical(X_test[l3])\nprint('Train dtypes:')\nprint(X_train[l1].dtypes)\nprint('======================================')\nprint('Validation dtypes:')\nprint(X_val[l2].dtypes)\nprint('======================================')\nprint('Test dtypes:')\nprint(X_test[l3].dtypes)","b05f4fa9":"# On train data\npd.set_option('display.max_rows', 500)\nNA_col = pd.DataFrame(X_train.isna().sum(), columns = ['NA_Count'])\nNA_col['% of NA'] = (NA_col.NA_Count\/len(X_train))*100\nNA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')","8fa7e527":"# On val data\npd.set_option('display.max_rows', 500)\nNA_col = pd.DataFrame(X_val.isna().sum(), columns = ['NA_Count'])\nNA_col['% of NA'] = (NA_col.NA_Count\/len(X_val))*100\nNA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')","5e9a1871":"# On test data\npd.set_option('display.max_rows', 500)\nNA_col = pd.DataFrame(X_test.isna().sum(), columns = ['NA_Count'])\nNA_col['% of NA'] = (NA_col.NA_Count\/len(X_test))*100\nNA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')","ab638caa":"cols = X_train.columns\nnum_cols = X_train._get_numeric_data().columns\ncat_cols = list(set(cols) - set(num_cols))\ncat_cols","7afe8f85":"# Courtesy: https:\/\/www.kaggle.com\/jayjay75\/wids2020-lgb-starter-script\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfor usecol in cat_cols:\n    X_train[usecol] = X_train[usecol].astype('str')\n    X_val[usecol] = X_val[usecol].astype('str')\n    X_test[usecol] = X_test[usecol].astype('str')\n    \n    #Fit LabelEncoder\n    le = LabelEncoder().fit(\n            np.unique(X_train[usecol].unique().tolist()+\n                      X_val[usecol].unique().tolist()+\n                     X_test[usecol].unique().tolist()))\n\n    #At the end 0 will be used for dropped values\n    X_train[usecol] = le.transform(X_train[usecol])+1\n    X_val[usecol]  = le.transform(X_val[usecol])+1\n    X_test[usecol]  = le.transform(X_test[usecol])+1\n    \n    X_train[usecol] = X_train[usecol].replace(np.nan, 0).astype('int').astype('category')\n    X_val[usecol]  = X_val[usecol].replace(np.nan, 0).astype('int').astype('category')\n    X_test[usecol]  = X_test[usecol].replace(np.nan, 0).astype('int').astype('category')","9c13de3f":"X_train.set_index('encounter_id', inplace = True)\ny_train.set_index('encounter_id', inplace = True)\nX_val.set_index('encounter_id', inplace = True)\ny_val.set_index('encounter_id', inplace = True)\nX_test.set_index('encounter_id', inplace = True)\ny_test.set_index('encounter_id', inplace = True)","f130b98d":"# y_test.hospital_death = y_test.hospital_death.fillna(0)","790e26a1":"# y_train['hospital_death'] = pd.Categorical(y_train['hospital_death'])\n# y_train.dtypes","9adf7669":"# y_test['hospital_death'] = pd.Categorical(y_test['hospital_death'])\n# y_test.dtypes","4801ca52":"# from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n# from pandas import Series\n# l=LabelEncoder() \n# l.fit(y_train['hospital_death']) \n# l.classes_ \n# y_train['hospital_death']=Series(l.transform(y_train['hospital_death']))  #label encoding our target variable \n# y_train['hospital_death'].value_counts() ","d917cfb0":"# l.fit(y_test['hospital_death']) \n# l.classes_ \n#y_test['hospital_death'].fillna(0.0, inplace = True)\n# y_test['hospital_death']=Series(l.transform(y_test['hospital_death']))  #label encoding our target variable \n# y_test['hospital_death'].value_counts() ","3bacf1a3":"import lightgbm as lgbm\n\nlgbm_train = lgbm.Dataset(X_train, y_train, categorical_feature=cat_cols)\n# lgbm_test = lgbm.Dataset(X_test, y_test, categorical_feature=cat_cols)\nlgbm_val = lgbm.Dataset(X_val, y_val, reference = lgbm_train)","d783d573":"params = {'feature_fraction': 0.9,\n          'lambda_l1': 1,\n          'lambda_l2': 1,\n          'learning_rate': 0.01,\n          'max_depth': 10,\n          'metric': 'auc',\n          'num_leaves': 500,\n          'min_data_in_leaf': 100,\n          'subsample_freq': 1,\n          'scale_pos_weight':1,\n          'metric': 'auc',\n          'is_unbalance': 'true',\n          'boosting': 'gbdt',\n          'bagging_fraction': 0.5,\n          'bagging_freq': 10,}","fa0d0630":"evals_result = {}  # to record eval results for plotting\nmodel_lgbm = lgbm.train(params,\n                lgbm_train,\n                num_boost_round=100,\n                valid_sets=[lgbm_train, lgbm_val],\n                feature_name=['f' + str(i + 1) for i in range(X_train.shape[-1])],\n                categorical_feature= [182],\n                evals_result=evals_result,\n                verbose_eval=10)","343339d9":"ax = lgbm.plot_metric(evals_result, metric='auc', figsize=(15, 8))\nplt.show()","e4b4a917":"test[\"hospital_death\"] = model_lgbm.predict(X_test, predition_type = 'Probability')\ntest[[\"encounter_id\",\"hospital_death\"]].to_csv(\"submission_lgbm1.csv\",index=False)","db49f50a":"test[[\"encounter_id\",\"hospital_death\"]].head()","ced718ae":"## Reading and Preview","6fd86d8f":"## Extracting column with unique values <= 15","84707f02":"# Model Building","1b2fd0c4":"**1. Frequency plot of Hospital Deaths**","60d8103e":"## Converting to Categorical","92a47d75":"**5. Hospital ID vs Hospital Deaths**","2a0c3046":"# Libraries and Reading Data","0964db59":"## Checking the number of unique values for remaining columns","1c48e02b":"**2. Ethnicity vs Hospital Deaths**","44b237c9":"## Checking the data types of these columns for conversion to Categorical","f0471541":"**3. Hospital Admit Source vs Hospital Deaths**","7b6d10a4":"**6. Freq plot of Age for hospital_death 0 and 1**","e097a4f3":"## Checking for columns with only one value","f9997e22":"# Visualization","e90c8de7":"## Checking NAs","b8b79f93":"**7. Age vs Bmi, Height, Weight**","32693d39":"**4. ICU Admit Source vs Hospital Deaths**"}}