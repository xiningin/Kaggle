{"cell_type":{"70519703":"code","cd0b8f04":"code","93e51fe3":"code","9ae555e4":"code","034f6aea":"code","9c4f30d9":"code","91463105":"code","d97d6871":"code","a72c9595":"code","55af4a18":"code","7224a8bf":"code","d1df8300":"code","e44625c5":"code","750ade97":"code","2d450838":"code","c781b8aa":"code","41752898":"code","c6c8062a":"code","4516f037":"code","77958b42":"code","cb413a34":"code","00189a6c":"code","57c6c910":"code","9fb31627":"code","0e18778b":"code","9bf35df7":"code","6c9d6907":"code","f714bf8c":"code","fccf4065":"code","3b0c664f":"code","575b73a3":"code","8676610c":"code","9ee1c655":"code","44149ba6":"code","f048a8ae":"code","05c86a31":"code","a4fe5dbe":"code","1277ed4e":"code","26095cfd":"code","e19049b0":"code","18a45077":"code","61a47180":"markdown","ac237c46":"markdown","5d9cc746":"markdown","2ec6cc93":"markdown","a8fbbe5e":"markdown","f79febaf":"markdown","e6d60052":"markdown","92d97ae7":"markdown","7e213189":"markdown","99d7e969":"markdown","257e1ed0":"markdown","dd9f7281":"markdown","5d8704b2":"markdown","bd4dd7c2":"markdown","b183b1b9":"markdown","4c8f2ecc":"markdown","59666815":"markdown","9ec37a3e":"markdown","b0bc74ec":"markdown","f673a737":"markdown","da5df0c1":"markdown"},"source":{"70519703":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cd0b8f04":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno as msno\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import AdaBoostRegressor\n#from xgboost import XGBRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport lightgbm as lgb\n\nimport tensorflow as tf \n","93e51fe3":"train_df=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')","9ae555e4":"train_df.head(3)","034f6aea":"train_df.shape","9c4f30d9":"train_df.describe().transpose()","91463105":"test_df=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nId_test=test_df['Id']\ntest_df.shape","d97d6871":"test_df","a72c9595":"# Defining our evaluation metric\ndef rmse_log(y_test,preds):\n    return  (np.sqrt(np.mean(np.square(np.log(y_test) - np.log(preds)))))","55af4a18":"#The function is used to determine the type of the column:Categorical or Numerical  \ndef utils_recognize_type(dtf, col, max_cat):\n    if (dtf[col].dtype == \"O\") | (dtf[col].nunique() < max_cat):\n        return \"cat\"\n    else:\n        return \"num\"","7224a8bf":"dic_cols = {col:utils_recognize_type(train_df, col, max_cat=10) for col in train_df.columns}\nheatmap = train_df.isnull()\nfor k,v in dic_cols.items():\n    if v == \"num\":\n        heatmap[k] = heatmap[k].apply(lambda x: 0.5 if x is False else 1)\n    else:\n        heatmap[k] = heatmap[k].apply(lambda x: 0 if x is False else 1)\n        \nplt.figure(figsize=(15,5))\nsns.heatmap(heatmap, cbar=False).set_title('Train Dataset Overview')\nplt.show()\nprint(\"\\033[1;37;40m Categerocial \", \"\\033[1;30;41m Numeric \", \"\\033[1;30;47m NaN \")","d1df8300":"msno.matrix(train_df)","e44625c5":"train_df['SalePrice'].describe().transpose()","750ade97":"plt.title('Sales price')\nsns.boxplot(x=train_df['SalePrice'])","2d450838":"sns.distplot(train_df['SalePrice'])","c781b8aa":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train_df['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_df['SalePrice'].kurt())","41752898":"Num_df=train_df.select_dtypes(include=np.number)\ncorr = Num_df.corr()\n# plot the heatmap\nfig, ax = plt.subplots(figsize=(15,15)) \nax.set_title('dataset totale')\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns, annot=False, cmap= 'Blues',ax=ax)","c6c8062a":"Columns=['LotFrontage','LotArea','OverallQual','YearBuilt','YearRemodAdd','MasVnrArea','BsmtFinSF1',\n         'BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','GrLivArea','BsmtFullBath','FullBath','HalfBath'\n         ,'BedroomAbvGr','TotRmsAbvGrd','Fireplaces', 'GarageYrBlt',\n         'GarageCars','GarageArea','WoodDeckSF','OpenPorchSF','SalePrice']\nNew_df=Num_df[Columns]\ncorr1 = New_df.corr()\n# plot the heatmap\nfig, ax = plt.subplots(figsize=(15,15)) \nax.set_title('Important features')\nsns.heatmap(corr1,xticklabels=corr1.columns,yticklabels=corr1.columns, annot=False, cmap= 'Blues',ax=ax)","4516f037":"top10=corr1.SalePrice.sort_values(ascending=False).head(9).index\nscatter_df=Num_df[top10]","77958b42":"scatter_df.columns","cb413a34":"sns.set()\nsns.pairplot(scatter_df, size = 2.5)\nplt.show();","00189a6c":"#Splitting dataset into trainin and testing\ntarget=scatter_df['SalePrice']\ndel scatter_df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(scatter_df, target, test_size=0.2,random_state=42)","57c6c910":"param_grid1={'max_depth': [5, 6, 7],'n_estimators': [10, 50, 100, 400]}\ngsc = GridSearchCV(estimator=RandomForestRegressor(),\n        param_grid=param_grid1,cv=5, scoring='neg_mean_squared_log_error', verbose=0,n_jobs=-1)\n\n##########################\ngrid_result = gsc.fit(X_train, y_train)","9fb31627":"best_params = grid_result.best_params_\nrfr_best = RandomForestRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"],\n                            random_state=42, verbose=False)\n\nrfr_best.fit(X_train, y_train)\npreds = rfr_best.predict(X_test)\nprint('rmse_log:', rmse_log(y_test, preds))","0e18778b":"param_grid2= {'n_estimators': [10, 50, 100, 400],'learning_rate' : [0.01, 0.05, 0.1, 0.5],\n 'loss' : ['linear', 'square', 'exponential'] }\ngsc2 = GridSearchCV(estimator=AdaBoostRegressor(),param_grid=param_grid2,cv=5, scoring='neg_mean_squared_log_error', verbose=0,n_jobs=-1)","9bf35df7":"##########################\ngrid_result2 = gsc2.fit(X_train, y_train)","6c9d6907":"best_params2 = grid_result2.best_params_\nAda_best = AdaBoostRegressor(loss=best_params2[\"loss\"], n_estimators=best_params2[\"n_estimators\"],learning_rate =best_params2['learning_rate'],\n                            random_state=False)\n\nAda_best.fit(X_train, y_train)\npreds2 = Ada_best.predict(X_test)\nprint('rmse_log:', rmse_log(y_test, preds2))","f714bf8c":"param_grid3= {'learning_rate': [0.01, 0.05, 0.1, 0.5],'max_depth': [5, 6, 7],'subsample': [0.7],'colsample_bytree': [0.7],'n_estimators': [10, 50, 100, 400]}","fccf4065":"gsc3 = GridSearchCV(estimator=XGBRegressor(),param_grid=param_grid3,cv=5, scoring='neg_mean_squared_log_error', verbose=0,n_jobs=-1)","3b0c664f":"##########################\ngrid_result3 = gsc3.fit(X_train, y_train)","575b73a3":"best_params3 = grid_result3.best_params_\nXGreg_best = XGBRegressor(learning_rate=best_params3['learning_rate'], subsample=best_params3['subsample'], colsample_bytree=best_params3['colsample_bytree'],\n                          n_estimators=best_params3['n_estimators'],max_depth=best_params3['max_depth'])\n\nXGreg_best.fit(X_train, y_train)\npreds3 = XGreg_best.predict(X_test)\nprint('rmse_log:', rmse_log(y_test, preds3))","8676610c":"param = {'objective': 'regression'}\nregr = lgb.LGBMRegressor(**param)","9ee1c655":"param_grid4 = {'reg_alpha': [0.1, 0.5],'min_data_in_leaf': [30, 50, 100, 300],\n    'lambda_l1': [0, 1, 1.5],'lambda_l2': [0, 1],'num_leaves': [31, 127], \n          'max_depth':[5, 6, 7],'n_estimators': [10, 50, 100, 400], 'colsample_bytree' :[0.7], \n              'reg_lambda':np.arange(0.1, 1.0, 0.2), 'min_child_samples':[10,20,30],'subsample': [0.7] }","44149ba6":"gsc4 = GridSearchCV(estimator=regr,param_grid=param_grid4,cv=5, scoring='neg_mean_squared_log_error', verbose=0,n_jobs=-1)","f048a8ae":"grid_result4 = gsc4.fit(X_train, y_train)","05c86a31":"best_params4 = grid_result4.best_params_\nLgb_best = lgb.LGBMRegressor(reg_alpha=best_params4['reg_alpha'], min_data_in_leaf=best_params4['min_data_in_leaf'], \n                         lambda_l1=best_params4['lambda_l1'], lambda_l2=best_params4['lambda_l2'],\n                         num_leaves=best_params4['num_leaves'],max_depth=best_params4['max_depth'], n_estimators=best_params4['n_estimators'],\n                         colsample_bytree=best_params4['colsample_bytree'],reg_lambda=best_params4['reg_lambda'],\n                         min_child_samples=best_params4['min_child_samples'], subsample=best_params4['subsample'])\n\nLgb_best.fit(X_train, y_train)\npreds4 = Lgb_best.predict(X_test)\nprint('rmse_log:', rmse_log(y_test, preds4))","a4fe5dbe":"test_df=test_df[['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF','1stFlrSF', 'FullBath', 'TotRmsAbvGrd']] # Not to consider SalePrice : it does not exists \ndic_cols = {col:utils_recognize_type(test_df, col, max_cat=10) for col in test_df.columns}\nheatmap = test_df.isnull()\nfor k,v in dic_cols.items():\n    if v == \"num\":\n        heatmap[k] = heatmap[k].apply(lambda x: 0.5 if x is False else 1)\n    else:\n        heatmap[k] = heatmap[k].apply(lambda x: 0 if x is False else 1)\n        \nplt.figure(figsize=(15,5))\nsns.heatmap(heatmap, cbar=False).set_title('Test Dataset Overview')\nplt.show()\nprint(\"\\033[1;37;40m Categerocial \", \"\\033[1;30;41m Numeric \", \"\\033[1;30;47m NaN \")","1277ed4e":"test_df[\"GarageCars\"]=test_df[\"GarageCars\"].fillna(test_df[\"GarageCars\"].mean())\ntest_df[\"GarageArea\"]=test_df[\"GarageArea\"].fillna(test_df['GarageArea'].mean())\ntest_df[\"TotalBsmtSF\"]=test_df[\"TotalBsmtSF\"].fillna(test_df[\"TotalBsmtSF\"].mean())","26095cfd":"y_pred_sub=rfr_best.predict(test_df)","e19049b0":"colum=['Id','SalePrice']\ndat={'Id':Id_test, 'SalePrice':y_pred_sub}\nsub_df=pd.DataFrame(data=dat, columns=colum)","18a45077":"sub_df.to_csv('Submission.csv',index=False)","61a47180":"#### Scatter plots between 'SalePrice' and msot correlated variables","ac237c46":"Subbmission must be like the next format : \nFor each Id in the test set, you must predict the value of the SalePrice variable. ","5d9cc746":"### 3.1- Random Forest ","2ec6cc93":"Here is a second option to verify if any column of the provided dataframe has some missing values. From the previous and the next graphs, it is clear that 'Alley' and 'PoolQC' are almost empty (contain many nan values) so we can exclude them.","a8fbbe5e":"In the first part of this notebook, I decided to study the correlation between all numerical columns in our dataset with the target feature the 'SalePrice'","f79febaf":"### 2.1.1- Analysing our target 'SalePrice'","e6d60052":"### 3.2- AdaBoost","92d97ae7":"### 2.1.2- Relationship with numerical variables: ","7e213189":"# 1- Data Reading \n## 1.1- Training data","99d7e969":"## Defining some useful functions ","257e1ed0":"# Submission","dd9f7281":"- Has a peak \n- Some deviation with respect to normal distribution\n- Positive skewness exists","5d8704b2":"## 3- Application of ensemble learning algorithm","bd4dd7c2":"# Importing required libraries ","b183b1b9":"# 2- EDA: Exploratory Data Analysis\n## 2.1- Categorical features Vs numerical features","4c8f2ecc":"##### The most correlated features with SalePrice are the next ","59666815":"In this notebook we are goint to : \n   - Predict the sales price for each house.\n   - Evaluate our models using a customized metric: RMSE=np.sqrt(((log(ypred) - log(y)) ** 2).mean())","9ec37a3e":"### 3.3- XGBoost","b0bc74ec":"### 3.4- LightGBM","f673a737":"## 1.2- Testing ","da5df0c1":"This machine learning approach is called ensemble learning because it aims to improve the predictive performance via combining the predictions from multiple models.\n\nThe three main classes of ensemble learning methods are: \n- Bagging: which involves fitting many decision trees on different samples of the same dataset and averaging the predictions.\n- Stacking : which involves fitting many different models types on the same data and using another model to learn how to best combine the predictions.\n- Boosting : which involves adding ensemble members sequentially that correct the predictions made by prior models and outputs a weighted average of the predictions.\n\nIn this notebook I am going to adopt the next four algorithms: \n- Random Forest\n- AdaBoost\n- XGBoost\n- LightGBM"}}