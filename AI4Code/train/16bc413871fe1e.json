{"cell_type":{"d71a5e47":"code","535b7032":"code","183ebba1":"code","f2fb615e":"code","b8bde268":"code","cf7eda41":"code","00401a31":"code","780f13af":"code","6317fa5a":"code","fe2134d7":"code","566270ec":"code","666c9d76":"code","25795d4e":"code","f1763df5":"code","5b22bbb5":"code","f47692b5":"code","d554e9ee":"code","3e1be741":"code","5d82a713":"code","6d308d74":"code","a7038b32":"code","79931853":"code","3e6d1e4d":"code","96d43eb6":"code","415c1ece":"code","2e1b0a34":"code","eb4e13d3":"code","6c2386a9":"markdown","3f220218":"markdown","45300a16":"markdown","c72a23eb":"markdown","5f67c3d9":"markdown","4aa6f64a":"markdown","d5bc1d00":"markdown","0c80d53a":"markdown","bda5e391":"markdown","a3124ab3":"markdown","87455d6a":"markdown","26e2c1ac":"markdown","1f70f6db":"markdown","6be99505":"markdown","b914bbaf":"markdown","64009087":"markdown","dba8894a":"markdown","3fe7164b":"markdown","6841427f":"markdown","4d08c1c5":"markdown","2e2ccb44":"markdown","c3c3ee0a":"markdown","00778ff1":"markdown","6b3ede44":"markdown","eebc9304":"markdown","d8578da0":"markdown","fa94b918":"markdown","403b8795":"markdown","5586338b":"markdown","2bb04f7d":"markdown","8a24f4a9":"markdown","76148df4":"markdown","d548b1bb":"markdown","2b6d927c":"markdown","aebfab65":"markdown","b1749643":"markdown","8d5dcf9e":"markdown","98a00976":"markdown","d9657236":"markdown","4d50a5d5":"markdown","76a431ad":"markdown","536b7396":"markdown"},"source":{"d71a5e47":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","535b7032":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nfrom tensorflow.keras.layers import Dense, Conv2D, Input, Dropout, BatchNormalization, MaxPooling2D, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time","183ebba1":"train_df = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntrain_df.head()","f2fb615e":"X = train_df.drop(columns=[\"label\"]).values\nY = train_df[\"label\"].values\n\nprint(\"Shape of features:\", X.shape)\nprint(\"Shape of features:\", Y.shape)\n\nprint(\"Type of X:\", type(X))\nprint(\"Type of Y:\", type(Y))","b8bde268":"X_square = X.reshape(-1, 28, 28)\n\n# Just a value to index both the features and the labels\n# Feel free to change this number to visualize the samples\nindex = 100\n\nplt.imshow(X_square[100])\nprint(f\"Label is {Y[100]}\")","cf7eda41":"uniques, count = np.unique(Y, return_counts=True)\nplt.bar(uniques, count)\nplt.xlabel(\"Digit Category\")\nplt.ylabel(\"Number of samples\")\nplt.show()","00401a31":"X_square = X_square \/ 255.\nX = X \/ 255.","780f13af":"test_ratio = 0.1  # 10%\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_ratio, random_state=42)\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n\nuniques, count = np.unique(y_train, return_counts=True)\nplt.bar(uniques, count, label=\"train\")\nuniques, count = np.unique(y_test, return_counts=True)\nplt.bar(uniques, count, label=\"test\")\nplt.xlabel(\"Digit Category\")\nplt.ylabel(\"Number of samples\")\nplt.legend()\nplt.show()\n","6317fa5a":"from sklearn.tree import DecisionTreeClassifier\n\ndecision_tree_classifier = DecisionTreeClassifier(random_state=0)\ndecision_tree_classifier.fit(x_train, y_train)\nstart = time.time()\npredictions = decision_tree_classifier.predict(x_test)\nend = time.time()\n\naccuracy = accuracy_score(y_test, predictions)\nconf_matrix = confusion_matrix(y_test, predictions)\n\nprint(\"Decision Tree\")\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix\\n\", conf_matrix)\nprint(f\"Elapsed Time: {end - start: .4f} second\")","fe2134d7":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest_classifier = RandomForestClassifier()\nrandom_forest_classifier.fit(x_train, y_train)\nstart = time.time()\npredictions = random_forest_classifier.predict(x_test)\nend = time.time()\n\naccuracy = accuracy_score(y_test, predictions)\nconf_matrix = confusion_matrix(y_test, predictions)\n\nprint(\"Random Forest\")\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix\\n\", conf_matrix)\nprint(f\"Elapsed Time: {end - start: .4f} second\")","566270ec":"from sklearn.svm import SVC\n\nsvm_classifier = SVC()\nsvm_classifier.fit(x_train, y_train)\nstart = time.time()\npredictions = svm_classifier.predict(x_test)\nend = time.time()\n\naccuracy = accuracy_score(y_test, predictions)\nconf_matrix = confusion_matrix(y_test, predictions)\n\nprint(\"Support Vector Machine\")\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix\\n\", conf_matrix)\nprint(f\"Elapsed Time: {end - start: .4f} second\")","666c9d76":"input_layer = Input(shape=(X.shape[-1]))\n\nx = Dense(700, activation=\"relu\")(input_layer)\nx = Dense(350, activation=\"relu\")(x)\nx = Dense(175, activation=\"relu\")(x)\nx = Dense(100, activation=\"relu\")(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dense(25, activation=\"relu\")(x)\nx = Dense(15, activation=\"relu\")(x)\noutput_layer = Dense(10, activation=\"softmax\")(x)\n\nmodel1 = Model(inputs=input_layer, outputs=output_layer)\n\nmodel1.summary()\n\nloss = SparseCategoricalCrossentropy()\noptimizer = Adam(learning_rate=0.01)\n\nmodel1.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\nmodel1.fit(\n    x=x_train,\n    y=y_train,\n    epochs=10,\n    batch_size=64,\n    validation_split=0.1\n)","25795d4e":"pd.DataFrame(model1.history.history).plot()","f1763df5":"input_layer = Input(shape=(X.shape[-1]))\n\nx = Dense(700, activation=\"relu\")(input_layer)\nx = Dense(350, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(175, activation=\"relu\")(x)\nx = Dense(100, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dense(25, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(15, activation=\"relu\")(x)\noutput_layer = Dense(10, activation=\"softmax\")(x)\n\nmodel2 = Model(inputs=input_layer, outputs=output_layer)\nmodel2.summary()\n\nloss = SparseCategoricalCrossentropy()\noptimizer = Adam(learning_rate=0.01)\n\nmodel2.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\nmodel2.fit(\n    x=x_train,\n    y=y_train,\n    epochs=10,\n    batch_size=64,\n    validation_split=0.1\n)","5b22bbb5":"pd.DataFrame(model2.history.history).plot()","f47692b5":"input_layer = Input(shape=(X.shape[-1]))\n\nx = Dense(700, activation=\"relu\")(input_layer)\nx = Dense(350, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(175, activation=\"relu\")(x)\nx = Dense(100, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dense(25, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(15, activation=\"relu\")(x)\noutput_layer = Dense(10, activation=\"softmax\")(x)\n\nmodel3 = Model(inputs=input_layer, outputs=output_layer)\nmodel3.summary()\n\nloss = SparseCategoricalCrossentropy()\noptimizer = Adam(learning_rate=0.01)\nearly_stopping = EarlyStopping(monitor=\"val_loss\", patience=2)\n\nmodel3.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\nmodel3.fit(\n    x=x_train,\n    y=y_train,\n    epochs=10,\n    batch_size=64,\n    validation_split=0.1,\n    callbacks=[early_stopping]\n)","d554e9ee":"pd.DataFrame(model3.history.history).plot()","3e1be741":"# This function only to print out the learning rate\ndef get_lr_metric(optimizer):\n    def lr(y_true, y_pred):\n        return optimizer.lr\n    return lr\n\n\ninput_layer = Input(shape=(X.shape[-1]))\n\nx = Dense(700, activation=\"relu\")(input_layer)\nx = Dense(350, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(175, activation=\"relu\")(x)\nx = Dense(100, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dense(25, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(15, activation=\"relu\")(x)\noutput_layer = Dense(10, activation=\"softmax\")(x)\n\nmodel4 = Model(inputs=input_layer, outputs=output_layer)\nmodel4.summary()\n\nloss = SparseCategoricalCrossentropy()\noptimizer = Adam(learning_rate=0.01)\nearly_stopping = EarlyStopping(monitor=\"val_loss\", patience=3)\ndecaying_learning_rate = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_delta=0.1, min_lr=0.0001)\nlr_metric = get_lr_metric(optimizer)\n\nmodel4.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\", lr_metric])\nmodel4.fit(\n    x=x_train,\n    y=y_train,\n    epochs=20,\n    batch_size=64,\n    validation_split=0.1,\n    callbacks=[early_stopping, decaying_learning_rate]\n)","5d82a713":"pd.DataFrame(model4.history.history).plot()","6d308d74":"# This function only to print out the learning rate\ndef get_lr_metric(optimizer):\n    def lr(y_true, y_pred):\n        return optimizer.lr\n    return lr\n\n\ninput_layer = Input(shape=(X.shape[-1]))\n\nx = Dense(700, activation=\"relu\")(input_layer)\nx = BatchNormalization()(x)\nx = Dense(350, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\nx = Dense(175, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dense(100, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dense(25, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\nx = Dense(15, activation=\"relu\")(x)\nx = BatchNormalization()(x)\noutput_layer = Dense(10, activation=\"softmax\")(x)\n\nmodel5 = Model(inputs=input_layer, outputs=output_layer)\nmodel5.summary()\n\nloss = SparseCategoricalCrossentropy()\noptimizer = Adam(learning_rate=0.01)\nearly_stopping = EarlyStopping(monitor=\"val_loss\", patience=3)\ndecaying_learning_rate = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_delta=0.1, min_lr=0.0001)\nlr_metric = get_lr_metric(optimizer)\n\nmodel5.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\", lr_metric])\nmodel5.fit(\n    x=x_train,\n    y=y_train,\n    epochs=20,\n    batch_size=64,\n    validation_split=0.1,\n    callbacks=[early_stopping, decaying_learning_rate]\n)","a7038b32":"pd.DataFrame(model5.history.history).plot()","79931853":"start = time.time()\npredictions = model5.predict(x_test)\nend = time.time()\n\n# Decode the one-hotted output\npredictions = [np.argmax(p) for p in predictions]\n\naccuracy = accuracy_score(y_test, predictions)\nconf_matrix = confusion_matrix(y_test, predictions)\n\nprint(\"Support Vector Machine\")\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix\\n\", conf_matrix)\nprint(f\"Elapsed Time: {end - start: .4f} second\")","3e6d1e4d":"x_train_square = x_train.reshape(-1, 28, 28)\nx_test_square = x_test.reshape(-1, 28, 28)","96d43eb6":"# This function only to print out the learning rate\ndef get_lr_metric(optimizer):\n    def lr(y_true, y_pred):\n        return optimizer.lr\n    return lr\n\n\ninput_layer = Input(shape=(x_train_square.shape[1], x_train_square.shape[2], 1))\n\nx = Conv2D(16, (2, 2), padding=\"same\", activation=\"relu\")(input_layer)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Conv2D(32, (2, 2), padding=\"same\", activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Conv2D(128, (2, 2), padding=\"same\", activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Flatten()(x)\n\nx = Dense(700, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dense(350, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\nx = Dense(175, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dense(100, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dense(25, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\nx = Dense(15, activation=\"relu\")(x)\nx = BatchNormalization()(x)\n\noutput_layer = Dense(10, activation=\"softmax\")(x)\n\nmodel6 = Model(inputs=input_layer, outputs=output_layer)\nmodel6.summary()\n\nloss = SparseCategoricalCrossentropy()\noptimizer = Adam(learning_rate=0.01)\nearly_stopping = EarlyStopping(monitor=\"val_loss\", patience=3)\ndecaying_learning_rate = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_delta=0.1, min_lr=0.0001)\nlr_metric = get_lr_metric(optimizer)\n\nmodel6.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\", lr_metric])\nmodel6.fit(\n    x=x_train_square,\n    y=y_train,\n    epochs=20,\n    batch_size=64,\n    validation_split=0.1,\n    callbacks=[early_stopping, decaying_learning_rate]\n)","415c1ece":"pd.DataFrame(model6.history.history).plot()","2e1b0a34":"start = time.time()\npredictions = model6.predict(x_test_square)\nend = time.time()\n\n# Decode the one-hotted output\npredictions = [np.argmax(p) for p in predictions]\n\naccuracy = accuracy_score(y_test, predictions)\nconf_matrix = confusion_matrix(y_test, predictions)\n\nprint(\"Support Vector Machine\")\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix\\n\", conf_matrix)\nprint(f\"Elapsed Time: {end - start: .4f} second\")","eb4e13d3":"df_test = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\n\ntest_data = df_test.values\ntest_data = test_data \/ 255.\n\ntest_data_square = test_data.reshape(-1, 28, 28)\npredictions = model6.predict(test_data_square)\n\n# Decode the one-hotted output\npredictions = [np.argmax(p) for p in predictions]\n\nwith open(\"\/kaggle\/working\/submission.csv\", \"w\") as out:\n    out.write(\"ImageId,Label\\n\")\n    for i, p in enumerate(predictions):\n        out.write(f\"{i+1},{p}\\n\")","6c2386a9":"## Model 5 \"Add Batch Normalization\"","3f220218":"## Model 2 \"Add Dropouts\"","45300a16":"Hope that was helpful and useful, if you enjoyed it please **Upvote** :D","c72a23eb":"Firstly let's get the MNIST training data which is represented in csv format where a row represents one image sample. and each row contains **785** column.\n\nThe first column is the **Label** column which contains the ground truth of this image sample, and the rest of the **784 columns** are the values of pixels of this image sample. \n\nLet's load it","5f67c3d9":"# Conclusion","4aa6f64a":"So according to this bar chart, we have a good distribution of the samples for each category. Now we can safely state that this dataset is **Balanced**","d5bc1d00":"# Neural Network","0c80d53a":"# Experiments","bda5e391":"# Data Anlysis","a3124ab3":"Now if we gave a look to the features we will find that a single features may have an integer number in the range of (0:255), and that's a large range for some models that prefers to work with data of features varies between (0 to 1). Therefore, we need to normalize the features","87455d6a":"In order to have a good model, we need to prepare a test set that covers most of the cases in our dataset to have a proper evaluation for our models to compare between them.\n\nOther thing we also need to **unify** the test set for all of the following experiments","26e2c1ac":"In this model we're going to build a simple fully connected neural network with input shape = (784,) and output shape = (10,) activated with softmax so that we can have 10 probabilities for the input image.\n\nThe reason why we've used \"Sparse Categorical Cross Entropy\" is that this is a categorical classification problem, where the labels are integers ranged from 0 to 9. If we decided to change the labels to be one-hot encoded then we will need to change that loss function to be the normal \"Categorical Cross Entropy\"","1f70f6db":"# Random Forest Classifier","6be99505":"As we can see the model overfitted because we over trained it on the data. So let's try to solve this problem by adding some Dropout layers and observe the difference.","b914bbaf":"**Okay, That's a good gain to be honest :D**\n\nLet's try this model over the test set","64009087":"One good fact, is that there's nothing called \"Best Classifier Algorithm Ever\". A classifier algorithm can fit well over a dataset which is linearlly separable - for example the SVM -, can't really fit very well over other types of data. \n\nTherefore it's always a good practice to test multiple algorithms on your dataset before choosing one, then you can fine tune that model till you get yourself the **best model** for this specific **dataset**","dba8894a":"Well now we can say that we have a **balanced** and **well-distributed** test set","3fe7164b":"# Prepare Train and Test sets","6841427f":"# Imports","4d08c1c5":"**We are now ready to do some experiments !!**","2e2ccb44":"In this notebook we will go through different classyfication algorithms that you may think of when you want to implement a model that can distinguish between different hand written and how you can enhance your model and the right way to perform different sequence of experiments on your dataset till you find the optimum point.","c3c3ee0a":"# Support Vector Machine","00778ff1":"Let's try to visualize the data to make sure that everything is good till now.<br>\nIn order to do this we need to reshape each sample from being one row of shape (784,1) to be a square matrix of shape (28,28)","6b3ede44":"There are a lot of other experiments that you can do over this data set, try to increase the size of the network, reduce it, try different activations and different combinations of trainig techniques.","eebc9304":"One good practice in trainig a neural network activated with relu, is to always add batch normalization after the activation to avoid any overshooting in the weights.","d8578da0":"Well the Neural network did very well actually, let's try to improve it a little bit by chnging the input itself. Let's try to extract some features first before we feeding it to the network, and in order to do that, let's add some base line of convolutional and pooling layers before the network to get a set of features and feed it to our network ","fa94b918":"The second step you need always to do whenever you want to analyse a dataset is to check the distribustion of your samples throughout the space of the targets. In simplified words, you need to see how many samples does this dataset has in each category. so basically we have 10 categories (the digits from 0 to 10) and we want to check how many samples we have for each digit.","403b8795":"Let's now divide the dataset into **Features (X)** and their corespoinding **Labels (Y)**","5586338b":"but first we need to square our **x_train** and **x_test**","2bb04f7d":"It looks like that the model was doing good till the third epoch then it overfitted. let's then try to add an early stopping call back that will stop the training and save the best weights whenever the model starts to overfit. and in order to decide that the model overfitted we will monitor the validation loss, if it started to increase for 2 epochs then it will stop","8a24f4a9":"**It looks like we're going somewhere now :D**","76148df4":"**ALRIGHT that's a very optimistic result**, let's try it on the test set","d548b1bb":"In this section we will dive into different types of experiments to try to find the best classifier that can fit over this dataset. \n\nLet's start !!","2b6d927c":"## Model 4 \"Add Decaying Learning Rate\"","aebfab65":"# Decision Tree Classifier","b1749643":"## Model 1 \"simple Fully connected Neural Network\"","8d5dcf9e":"## 1. Decision Tree","98a00976":"Well then, at least now we have a better model. but that's not enough. Why don't we try to reduce the learning rate to avoid falling in some local minimum. The idea is to star with some \"relatively\" big learning rate to move quickly from the initial point then whenever an overfitting is triggering, the learning rate will be reduced with some factor to avoid it. once we reach the minimum learning rate, and some overfitting started to happen then the early stopping will prevent it. now we can safely increase the number of epochs","d9657236":"## Model 3 \"Add Early Stopping\"","4d50a5d5":"So basically now we have **42000 training samples** each one has **784 features** and **1 label** and they are represented in numpy arrays","76a431ad":"# Convolutional Neural Network","536b7396":"# Submit"}}