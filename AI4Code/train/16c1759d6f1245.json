{"cell_type":{"f70f0593":"code","b37dec58":"code","5caab677":"code","9ec35afc":"code","2d5e22ab":"code","71a46ba0":"code","7f3a82da":"code","c5385c86":"code","d9865e71":"code","e43f56be":"code","d05c507d":"code","3498fd91":"code","4f6981d3":"code","eb2add35":"code","f8008968":"code","7b56d524":"code","c1be9afa":"code","08d13911":"code","c6a87c30":"code","99ab40cf":"code","a77380a6":"code","de5aabb2":"code","9ba27870":"code","d0bee557":"markdown","b4001b2d":"markdown","c51ea0ab":"markdown","d19d7865":"markdown","b62587de":"markdown","e559e15d":"markdown","ce2c5a9f":"markdown","e2dc09a4":"markdown","b94c0010":"markdown","8407b552":"markdown","ce2444f2":"markdown","933aa907":"markdown","4ee6b06d":"markdown","ae7df5c8":"markdown","af0fac2f":"markdown","c060da25":"markdown","8fc87eea":"markdown"},"source":{"f70f0593":"from learntools.core import binder\nbinder.bind(globals())\nfrom learntools.game_ai.ex4 import *","b37dec58":"# Fill in the blank\nbest_option = 'C'\n\n# Check your answer\nq_1.check()","5caab677":"# Lines below will give you solution code\n#q_1.solution()","9ec35afc":"# Check your answer (Run this code cell to receive credit!)\nq_2.solution()","2d5e22ab":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","71a46ba0":"!pip install 'tensorflow==1.15.0'","7f3a82da":"import tensorflow as tf\nfrom kaggle_environments import make, evaluate\nfrom gym import spaces","c5385c86":"!apt-get update\n!apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev\n!pip install \"stable-baselines[mpi]==2.9.0\"","d9865e71":"from stable_baselines.bench import Monitor \nfrom stable_baselines.common.vec_env import DummyVecEnv\nfrom stable_baselines import PPO1, A2C, ACER, ACKTR, TRPO\nfrom stable_baselines.a2c.utils import conv, linear, conv_to_fc\nfrom stable_baselines.common.policies import CnnPolicy","e43f56be":"class ConnectFourGym:\n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n        # Learn about spaces here: http:\/\/gym.openai.com\/docs\/#spaces\n        self.action_space = spaces.Discrete(self.columns)\n        self.observation_space = spaces.Box(low=0, high=2, \n                                            shape=(self.rows,self.columns,1), dtype=np.int)\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 1)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n    def reset(self):\n        self.obs = self.env.reset()\n        return np.array(self.obs['board']).reshape(self.rows,self.columns,1)\n    def change_reward(self, old_reward, done):\n        if old_reward == 1: # The agent won the game\n            return 1\n        elif done: # The opponent won the game\n            return -1\n        else: # Reward 1\/42\n            return 1\/(self.rows*self.columns)\n    def step(self, action):\n        # Check if agent's move is valid\n        is_valid = (self.obs['board'][int(action)] == 0)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n        else: # End the game and penalize agent\n            reward, done, _ = -10, True, {}\n        return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _","d05c507d":"# Create ConnectFour environment\nenv = ConnectFourGym(agent2=\"random\")\n\n# Create directory for logging training information\nlog_dir = \"log\/\"\nos.makedirs(log_dir, exist_ok=True)\n\n# Logging progress\nmonitor_env = Monitor(env, log_dir, allow_early_resets=True)\n\n# Create a vectorized environment\nvec_env = DummyVecEnv([lambda: monitor_env])","3498fd91":"# Neural network for predicting action values\ndef modified_cnn(scaled_images, **kwargs):\n    activ = tf.nn.relu\n    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, \n                         init_scale=np.sqrt(2), **kwargs))\n    layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, \n                         init_scale=np.sqrt(2), **kwargs))\n    layer_2 = conv_to_fc(layer_2)\n    return activ(linear(layer_2, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  ","4f6981d3":"class CustomCnnPolicy(CnnPolicy):\n    def __init__(self, *args, **kwargs):\n        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)","eb2add35":"# Initialize agent\nmodel = PPO1(CustomCnnPolicy, vec_env, verbose=0)\n\n# Train agent\nmodel.learn(total_timesteps=1_000) #100_000\n\n# Plot cumulative reward\nwith open(os.path.join(log_dir, \"monitor.csv\"), 'rt') as fh:    \n    firstline = fh.readline()\n    assert firstline[0] == '#'\n    df = pd.read_csv(fh, index_col=None)['r']\ndf.rolling(window=1000).mean().plot()\nplt.show()","f8008968":"def agent_deep_rl(obs, config):\n    \n    # Use the best model to select a column\n    col, _ = model.predict(np.array(obs['board']).reshape(6,7,1))\n    \n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    \n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])","7b56d524":"# Create the game environment\nenv = make(\"connectx\")\n\n# Two random agents play one game round\nenv.run([agent_deep_rl, \"random\"])\n\n# Show the game\nenv.render(mode=\"ipython\")","c1be9afa":"def get_win_percentages(agent1, agent2, n_rounds=100):\n    # Use default Connect Four setup\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds\/\/2)\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds\/\/2)]\n    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])\/len(outcomes), 2))\n    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])\/len(outcomes), 2))\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))","08d13911":"get_win_percentages(agent1=agent_deep_rl, agent2=\"random\")","c6a87c30":"# Create the game environment\nenv = make(\"connectx\")\n\n# Two random agents play one game round\nenv.run([agent_deep_rl, agent_deep_rl])\n\n# Show the game\nenv.render(mode=\"ipython\")","99ab40cf":"get_win_percentages(agent1=agent_deep_rl, agent2=agent_deep_rl)","a77380a6":"def agent_deep_rl(obs, config):\n    import os\n    import random\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    print('Line 1')\n\n    !pip install 'tensorflow==1.15.0'\n    print('Line 2')\n    \n    import tensorflow as tf\n    from kaggle_environments import make, evaluate\n    from gym import spaces\n    print('Line 3')\n    \n    !apt-get update\n    !apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev\n    !pip install \"stable-baselines[mpi]==2.9.0\"\n    print('Line 4')\n\n    from stable_baselines.bench import Monitor \n    from stable_baselines.common.vec_env import DummyVecEnv\n    from stable_baselines import PPO1, A2C, ACER, ACKTR, TRPO\n    from stable_baselines.a2c.utils import conv, linear, conv_to_fc\n    from stable_baselines.common.policies import CnnPolicy\n    print('Line 5')\n    \n    class ConnectFourGym:\n        def __init__(self, agent2=\"random\"):\n            ks_env = make(\"connectx\", debug=True)\n            self.env = ks_env.train([None, agent2])\n            self.rows = ks_env.configuration.rows\n            self.columns = ks_env.configuration.columns\n            # Learn about spaces here: http:\/\/gym.openai.com\/docs\/#spaces\n            self.action_space = spaces.Discrete(self.columns)\n            self.observation_space = spaces.Box(low=0, high=2, \n                                                shape=(self.rows,self.columns,1), dtype=np.int)\n            # Tuple corresponding to the min and max possible rewards\n            self.reward_range = (-10, 1)\n            # StableBaselines throws error if these are not defined\n            self.spec = None\n            self.metadata = None\n        def reset(self):\n            self.obs = self.env.reset()\n            return np.array(self.obs['board']).reshape(self.rows,self.columns,1)\n        def change_reward(self, old_reward, done):\n            if old_reward == 1: # The agent won the game\n                return 1\n            elif done: # The opponent won the game\n                return -1\n            else: # Reward 1\/42\n                return 1\/(self.rows*self.columns)\n        def step(self, action):\n            # Check if agent's move is valid\n            is_valid = (self.obs['board'][int(action)] == 0)\n            if is_valid: # Play the move\n                self.obs, old_reward, done, _ = self.env.step(int(action))\n                reward = self.change_reward(old_reward, done)\n            else: # End the game and penalize agent\n                reward, done, _ = -10, True, {}\n            return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _\n    \n    print('Line 6')\n    # Create ConnectFour environment\n    env = ConnectFourGym(agent2=\"random\")\n\n    # Create directory for logging training information\n    log_dir = \"log\/\"\n    os.makedirs(log_dir, exist_ok=True)\n\n    # Logging progress\n    monitor_env = Monitor(env, log_dir, allow_early_resets=True)\n\n    # Create a vectorized environment\n    vec_env = DummyVecEnv([lambda: monitor_env])\n\n    print('Line 7')    \n    # Neural network for predicting action values\n    def modified_cnn(scaled_images, **kwargs):\n        activ = tf.nn.relu\n        layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, \n                             init_scale=np.sqrt(2), **kwargs))\n        layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, \n                             init_scale=np.sqrt(2), **kwargs))\n        layer_2 = conv_to_fc(layer_2)\n        return activ(linear(layer_2, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  \n\n    class CustomCnnPolicy(CnnPolicy):\n        def __init__(self, *args, **kwargs):\n            super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)\n\n    print('Line 8')\n    # Initialize agent\n    model = PPO1(CustomCnnPolicy, vec_env, verbose=0)\n\n    print('Line 9')\n    # Train agent\n    model.learn(total_timesteps=1_000)\n    \n    print('Line 10')\n    # Use the best model to select a column\n    col, _ = model.predict(np.array(obs['board']).reshape(6,7,1))\n    \n    print('Line 11')\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    \n    print('Line 12')\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])","de5aabb2":"import inspect\nimport os\n\ndef write_agent_to_file(function, file):\n    with open(file, \"w\" if os.path.exists(file) else \"w\") as f:\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n\nwrite_agent_to_file(agent_deep_rl, \"submission.py\")","9ba27870":"validate_submission_file = True\nif validate_submission_file:\n\n    import sys\n    from kaggle_environments import utils, make\n\n    out = sys.stdout\n    #submission = utils.read_file(\"\/kaggle\/working\/submission.py\")\n    #agent = utils.get_last_callable(submission)\n    agent = utils.read_file(\"\/kaggle\/working\/submission.py\")\n    sys.stdout = out\n\n    env = make(\"connectx\", debug=True)\n    env.run([agent, agent])\n    print(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")","d0bee557":"### Lets calculate how it performs on average, against the other agent","b4001b2d":"### Submit to the competition","c51ea0ab":"# Introduction\n\nIn the tutorial, you learned a bit about reinforcement learning and used the `stable-baselines` package to train an agent to beat a random opponent.  In this exercise, you will check your understanding and tinker with the code to deepen your intuition.","d19d7865":"### 1) Set the architecture\n\nIn the tutorial, you learned one way to design a neural network that can select moves in Connect Four.  The neural network had an output layer with seven nodes (???  self.action_space = spaces.Discrete(self.columns)): one for each column in the game board.\n\nSay now you wanted to create a neural network that can play chess.  How many nodes should you put in the output layer?\n\n- Option A: 2 nodes (number of game players)\n- Option B: 16 nodes (number of game pieces that each player starts with)\n- Option C: 4672 nodes (number of possible moves)\n- Option D: 64 nodes (number of squares on the game board)\n\nUse your answer to set the value of the `best_option` variable below.  Your answer should be one of `'A'`, `'B'`, `'C'`, or `'D'`.","b62587de":"Next, run the code cell below to train an agent with PPO and view how the rewards evolved during training.  This code is identical to the code from the tutorial.","e559e15d":"### Lets see the outcome of one game round against ourselves","ce2c5a9f":"**This notebook is an exercise in the [Intro to Game AI and Reinforcement Learning](https:\/\/www.kaggle.com\/learn\/intro-to-game-ai-and-reinforcement-learning) course.  You can reference the tutorial at [this link](https:\/\/www.kaggle.com\/alexisbcook\/deep-reinforcement-learning).**\n\n---\n","e2dc09a4":"### 2) Decide reward\n\nIn the tutorial, you learned how to give your agent a reward that encourages it to win games of Connect Four.  Consider now training an agent to win at the game [Minesweeper](https:\/\/bit.ly\/2T5xEY8).  The goal of the game is to clear the board without detonating any bombs.\n\nTo play this game in Google Search, click on the **[Play]** button at [this link](https:\/\/www.google.com\/search?q=minesweeper).  \n\n<center>\n<img src=\"https:\/\/i.imgur.com\/WzoEfKY.png\" width=50%><br\/>\n<\/center>\n\nWith each move, one of the following is true:\n- The agent selected an invalid move (in other words, it tried to uncover a square that was uncovered as part of a previous move).  Let's assume this ends the game, and the agent loses.\n- The agent clears a square that did not contain a hidden mine.  The agent wins the game, because all squares without mines are revealed.\n- The agent clears a square that did not contain a hidden mine, but has not yet won or lost the game.\n- The agent detonates a mine and loses the game.\n\nHow might you specify the reward for each of these four cases, so that by maximizing the cumulative reward, the agent will try to win the game?\n\nAfter you have decided on your answer, run the code cell below to get credit for completing this question.","b94c0010":"### Lets evaluate our agent","8407b552":"### 3) (Optional) Amend the code\n\nIn this next part of the exercise, you will amend the code from the tutorial to experiment with creating your own agents!  There are a lot of hyperparameters involved with specifying a reinforcement learning agent, and you'll have a chance to amend them, to see how performance is affected.\n\nFirst, we'll need to make sure that your Kaggle Notebook is set up to run the code.  Begin by looking at the \"Settings\" menu to the right of your notebook.  Your menu will look like one of the following:\n\n<center>\n<img src=\"https:\/\/i.imgur.com\/kR1az0y.png\" width=100%><br\/>\n<\/center>\n\nIf your \"Internet\" setting appears as a \"Requires phone verification\" link, click on this link.  This will bring you to a new window; then, follow the instructions to verify your account.  After following this step, your \"Internet\" setting will appear \"Off\", as in the example to the right.\n\nOnce your \"Internet\" setting appears as \"Off\", click to turn it on.  You'll see a pop-up window that you'll need to \"Accept\" in order to complete the process and have the setting switched to \"On\".  Once the Internet is turned \"On\", you're ready to proceed!\n\n<center>\n<img src=\"https:\/\/i.imgur.com\/gOVh6Aa.png\" width=100%><br\/>\n<\/center>\n\nBegin by running the code cell below. ","ce2444f2":"Then, follow these steps to submit your agent to the competition:\n\n1. Begin by clicking on the blue Save Version button in the top right corner of the window. This will generate a pop-up window.\n2. Ensure that the Save and Run All option is selected, and then click on the blue Save button.\n3. This generates a window in the bottom left corner of the notebook. After it has finished running, click on the number to the right of the Save Version button. This pulls up a list of versions on the right of the screen. Click on the ellipsis (...) to the right of the most recent version, and select Open in Viewer. This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the Output tab on the right of the screen. Then, click on the blue Submit button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\nIf you want to keep working to improve your performance, select the blue Edit button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.\n\nGo to \"My Submissions\" to view your score and episodes being played.","933aa907":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https:\/\/www.kaggle.com\/learn-forum\/161477) to chat with other Learners.*","4ee6b06d":"### Validate your submission file\nThe code cell below has the agent in your submission file play one game round against itself.\n\nIf it returns \"Success!\", then you have correctly defined your agent.","ae7df5c8":"# Congratulations!\n\nYou have completed the course, and it's time to put your new skills to work!  \n\nThe next step is to apply what you've learned to a **[more complex game: Halite](https:\/\/www.kaggle.com\/c\/halite)**.  For a step-by-step tutorial in how to make your first submission to this competition, **[check out the bonus lesson](https:\/\/www.kaggle.com\/alexisbcook\/getting-started-with-halite)**!\n\nYou can find more games as they're released on the **[Kaggle Simulations page](https:\/\/www.kaggle.com\/simulations)**.\n\nAs we did in the course, we recommend that you start simple, with an agent that follows your precise instructions.  This will allow you to learn more about the mechanics of the game and to build intuition for what makes a good agent.  Then, gradually increase the complexity of your agents to climb the leaderboard!","af0fac2f":"If your agent trained well, the plot (which shows average cumulative rewards) should increase over time.\n\nOnce you have verified that the code runs, try making amendments to see if you can get increased performance.  You might like to:\n- change `PPO1` to `A2C` (or `ACER` or `ACKTR` or `TRPO`) when defining the model in this line of code: `model = PPO1(CustomCnnPolicy, vec_env, verbose=0)`.  This will let you see how performance can be affected by changing the algorithm from Proximal Policy Optimization [PPO] to one of:\n  - Advantage Actor-Critic (A2C),\n  - or Actor-Critic with Experience Replay (ACER),\n  - Actor Critic using Kronecker-factored Trust Region (ACKTR), or \n  - Trust Region Policy Optimization (TRPO).\n- modify the `change_reward()` method in the `ConnectFourGym` class to change the rewards that the agent receives in different conditions.  You may also need to modify `self.reward_range` in the `__init__` method (this tuple should always correspond to the minimum and maximum reward that the agent can receive).\n- change `agent2` to a different agent when creating the ConnectFour environment with `env = ConnectFourGym(agent2=\"random\")`.  For instance, you might like to use the `\"negamax\"` agent, or a different, custom agent.  Note that the smarter you make the opponent, the harder it will be for your agent to train!","c060da25":"### Lets calculate how we perform on average, against ourselves","8fc87eea":"### Lets see the outcome of one game round against another agent"}}