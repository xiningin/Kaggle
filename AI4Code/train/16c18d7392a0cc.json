{"cell_type":{"799c6163":"code","69692439":"code","8fe78485":"code","7d84bc11":"code","f9248ab5":"code","9f94c3a9":"code","cd812479":"code","bb9f6adb":"code","8d8e47c3":"code","5384591c":"code","a87cd69c":"code","f10f73cd":"code","9e044236":"code","e253decc":"code","a516dd81":"code","2cc55636":"code","1131c2db":"code","dad4ff4a":"code","5b3d58da":"code","627039cd":"code","08c52182":"code","8a10b307":"code","9e328cf4":"code","956c96fd":"code","9cf50850":"code","6c5194be":"code","e07159fb":"code","83cd22b8":"code","2414d602":"code","c82bf071":"code","49f80154":"code","12ab943b":"code","44ae0085":"code","79e90c8f":"code","a1941cf6":"code","2395fdf3":"code","f832507b":"code","df31137b":"code","0b036f39":"code","72542191":"code","7f733ef5":"code","c1a34fd7":"code","22329558":"code","01f393a9":"code","8c9cb24c":"code","585e8ce0":"code","3b52f8c5":"code","1c045694":"code","c38a7aab":"code","094ca5de":"code","b32e7c91":"code","0640195a":"code","0fae4e67":"code","e3cec30a":"code","232a3e24":"code","f2d3c121":"code","485b7dd4":"code","62b73543":"code","0daf9e9b":"code","86d3977b":"code","4d05d6f7":"code","906ed2f1":"code","0d5f7d37":"code","4f8b740d":"code","4702e2ad":"code","779f3ec3":"code","c25a5f16":"code","0933fe4c":"code","477ccda9":"code","75dbd497":"code","8d4aca72":"code","d0ad970e":"code","c9dbe190":"code","ced7e3ff":"code","4eda4d3d":"code","61cd4fc4":"code","bce6f134":"code","003370e5":"code","0835f5c1":"code","4a0170be":"code","0671091c":"code","fc5d3866":"code","4c8b8efe":"code","36fbc6be":"code","f595dfc0":"code","37351c0c":"code","10528313":"code","a65e8599":"code","e69b8ef6":"code","980b33e7":"code","2619301a":"code","6adc5d3f":"code","8dd9934c":"markdown","b164e98a":"markdown","486cd342":"markdown","9ff987fc":"markdown","0f83dccb":"markdown","e79462da":"markdown","b2b65ab8":"markdown","d3886e5d":"markdown","2e901a32":"markdown","73b31405":"markdown","1d6ec092":"markdown","9392bf56":"markdown","68c9f4c1":"markdown","d5f724c0":"markdown","df4692eb":"markdown","ca33c9f1":"markdown","f7427117":"markdown","bf1f07bc":"markdown","6ebe0f50":"markdown","a584cb73":"markdown","a29486c2":"markdown","aeddf596":"markdown","e6fb9fb3":"markdown","670762a3":"markdown","dca8e28d":"markdown"},"source":{"799c6163":"import torch\nimport torchvision\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nfrom torch.utils.data import random_split\nimport torch.nn.functional as F\nfrom torchvision.utils import make_grid\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ntorch.manual_seed(42)","69692439":"project_name='wildlife'","8fe78485":"dataset = ImageFolder(root='\/kaggle\/input\/african-wildlife\/', transform=transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()]))","7d84bc11":"dataset_size = len(dataset)\ndataset_size","f9248ab5":"classes = dataset.classes\nclasses","9f94c3a9":"num_classes = len(dataset.classes)\nnum_classes","cd812479":"test_size = 100\nnontest_size = len(dataset) - test_size\n\nnontest_ds, test_ds = random_split(dataset, [nontest_size, test_size])\nlen(nontest_ds), len(test_ds)","bb9f6adb":"val_size = 100\ntrain_size = len(nontest_ds) - val_size\n\ntrain_ds, val_ds = random_split(nontest_ds, [train_size, val_size])\nlen(train_ds), len(val_ds)","8d8e47c3":"batch_size = 128\n\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_ds, batch_size*2, num_workers=4, pin_memory=True)","5384591c":"for images, _ in train_loader:\n    print('images.shape:', images.shape)\n    plt.figure(figsize=(16,8))\n    plt.axis('off')\n    plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))\n    break","a87cd69c":"input_size = 3 * 256*256\nnum_classes = 4","f10f73cd":"class WildlifeModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(input_size, num_classes)\n        \n    def forward(self, xb):\n        xb = xb.reshape(-1 , 3 * 256 * 256)\n        out = self.linear(xb)\n        return out\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss, 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n    \nmodel = WildlifeModel()","9e044236":"def evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","e253decc":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","a516dd81":"def plot_losses(history):\n    losses = [x['val_loss'] for x in history]\n    plt.plot(losses, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.title('Loss vs. No. of epochs');","2cc55636":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');","1131c2db":"result0 = evaluate(model, val_loader)\nresult0","dad4ff4a":"history1 = fit(50, 0.000005, model, train_loader, val_loader)","5b3d58da":"history2 = fit(10, 0.000001, model, train_loader, val_loader)","627039cd":"history3 = fit(25, 0.000001, model, train_loader, val_loader)","08c52182":"history4 = fit(10, 0.0000005, model, train_loader, val_loader)","8a10b307":"history5 = fit(10, 0.0000005, model, train_loader, val_loader)","9e328cf4":"history6 = fit(10, 0.0000005, model, train_loader, val_loader)","956c96fd":"# Replace these values with your results\nhistory = [result0] + history1 + history2 + history3 + history4 + history5 + history6\naccuracies = [result['val_acc'] for result in history]\nplt.plot(accuracies, '-x')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.title('Accuracy vs. No. of epochs');","9cf50850":"input_size = 3 * 256 * 256\nhidden_size1 = 128 # you can change this\nhidden_size2 = 32\nhidden_size3 = 64\nhidden_size4 = 32\noutput_size = 4","6c5194be":"class FeedforwardModel(nn.Module):\n    \"\"\"Feedfoward neural network with 1 hidden layer\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size1)\n        self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n        self.linear3 = nn.Linear(hidden_size2, hidden_size3)\n        self.linear4 = nn.Linear(hidden_size3, hidden_size4)\n        self.linear5 = nn.Linear(hidden_size4, output_size)\n        \n    def forward(self, xb):\n        # Flatten images into vectors\n        out = xb.view(xb.size(0), -1)\n        # Apply layers & activation functions\n        out = self.linear1(out)\n        out = F.relu(out)\n        out = self.linear2(out)\n        out = F.relu(out)\n        out = self.linear3(out)\n        out = F.relu(out)\n        out = self.linear4(out)\n        out = F.relu(out)\n        out = self.linear5(out)\n        return out\n    \n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss, 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))","e07159fb":"model = FeedforwardModel()","83cd22b8":"for t in model.parameters():\n    print(t.shape)","2414d602":"torch.cuda.is_available()","c82bf071":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')","49f80154":"device = get_default_device()\ndevice","12ab943b":"def to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)","44ae0085":"for images, labels in train_loader:\n    print(images.shape)\n    images = to_device(images, device)\n    print(images.device)\n    break","79e90c8f":"class DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","a1941cf6":"train_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)","2395fdf3":"for xb, yb in val_loader:\n    print('xb.device:', xb.device)\n    print('yb:', yb)\n    break","f832507b":"# Model (on GPU)\nmodel = FeedforwardModel()\nto_device(model, device)","df31137b":"history = [evaluate(model, val_loader)]\nhistory","0b036f39":"history += fit(30, 0.01000, model, train_loader, val_loader)","72542191":"history += fit(30, 0.00500, model, train_loader, val_loader)","7f733ef5":"history += fit(30, 0.00010, model, train_loader, val_loader)","c1a34fd7":"history += fit(30, 0.00005, model, train_loader, val_loader)","22329558":"plot_losses(history)","01f393a9":"plot_accuracies(history)","8c9cb24c":"import torch\nimport torchvision\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nfrom torch.utils.data import random_split\nimport torch.nn.functional as F\nfrom torchvision.utils import make_grid\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ntorch.manual_seed(42)","585e8ce0":"dataset = ImageFolder(root='\/kaggle\/input\/african-wildlife', transform=transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()]))","3b52f8c5":"dataset_size = len(dataset)\ndataset_size","1c045694":"classes = dataset.classes\nclasses","c38a7aab":"num_classes = len(dataset.classes)\nnum_classes","094ca5de":"test_size = 100\nnontest_size = len(dataset) - test_size\n\nnontest_ds, test_ds = random_split(dataset, [nontest_size, test_size])\nlen(nontest_ds), len(test_ds)","b32e7c91":"val_size = 100\ntrain_size = len(nontest_ds) - val_size\n\ntrain_ds, val_ds = random_split(nontest_ds, [train_size, val_size])\nlen(train_ds), len(val_ds)","0640195a":"batch_size = 16\n\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_ds, batch_size*2, num_workers=4, pin_memory=True)","0fae4e67":"train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)","e3cec30a":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))","232a3e24":"class CnnModel(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n\n            nn.Flatten(), \n            nn.Linear(256*16*16 * 4, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10))\n        \n    def forward(self, xb):\n        return self.network(xb)","f2d3c121":"model = CnnModel()\nmodel","485b7dd4":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","62b73543":"device = get_default_device()\ndevice","0daf9e9b":"train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)\nto_device(model, device);","86d3977b":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","4d05d6f7":"model = to_device(CnnModel(), device)\n\nfor images, labels in train_dl:\n    print('images.shape:', images.shape)\n    out = model(images)\n    print('out.shape:', out.shape)\n    print('out[0]:', out[0])\n    break","906ed2f1":"num_epochs1 = 10\nnum_epochs2 = 10\nnum_epochs3 = 10\nopt_func = torch.optim.Adam\nlr1 = 0.000010\nlr2 = 0.0000005\nlr3 = 0.0000001\n\n\nevaluate(model, val_dl)","0d5f7d37":"history = fit(num_epochs1, lr1, model, train_dl, val_dl, opt_func)","4f8b740d":"history = fit(num_epochs2, lr2, model, train_dl, val_dl, opt_func)","4702e2ad":"history = fit(num_epochs3, lr3, model, train_dl, val_dl, opt_func)","779f3ec3":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');","c25a5f16":"plot_accuracies(history)","0933fe4c":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","477ccda9":"plot_losses(history)","75dbd497":"import os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, random_split, DataLoader\nfrom PIL import Image\nimport torchvision.models as models\nfrom tqdm.notebook import tqdm\nimport torchvision.transforms as T\nfrom sklearn.metrics import f1_score\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torchvision.utils import make_grid\nfrom torchvision.datasets import ImageFolder\nimport PIL\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(42)","8d4aca72":"dataset = ImageFolder(root='\/kaggle\/input\/african-wildlife\/')\n\ndataset_size = len(dataset)\ndataset_size","d0ad970e":"classes = dataset.classes\nclasses","c9dbe190":"num_classes = len(dataset.classes)\nnum_classes","ced7e3ff":"test_size = 100\nnontest_size = len(dataset) - test_size\n\nnontest_df, test_df = random_split(dataset, [nontest_size, test_size])\nlen(nontest_df), len(test_df)","4eda4d3d":"val_size = 100\ntrain_size = len(nontest_df) - val_size\n\ntrain_df, val_df = random_split(nontest_df, [train_size, val_size])\nlen(train_df), len(val_df)","61cd4fc4":"imagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\ntrain_tfms = T.Compose([\n    #T.RandomCrop(256, padding=8, padding_mode='reflect'),\n     #T.RandomResizedCrop(256, scale=(0.5,0.9), ratio=(1, 1)), \n    #T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n    T.Resize((256, 256)),\n    T.RandomHorizontalFlip(), \n    T.RandomRotation(10),\n    T.ToTensor(), \n     T.Normalize(*imagenet_stats,inplace=True), \n    #T.RandomErasing(inplace=True)\n])\n\nvalid_tfms = T.Compose([\n     T.Resize((256, 256)), \n    T.ToTensor(), \n     T.Normalize(*imagenet_stats)\n])","bce6f134":"test_df.dataset.transform = valid_tfms\nval_df.dataset.transform = valid_tfms\n\ntrain_df.dataset.transform = train_tfms","003370e5":"batch_size = 16\n\ntrain_dl = DataLoader(train_df, batch_size, shuffle=True, \n                      num_workers=3, pin_memory=True)\nval_dl = DataLoader(val_df, batch_size*2, \n                    num_workers=2, pin_memory=True)\ntest_dl = DataLoader(test_df, batch_size*2, \n                    num_workers=2, pin_memory=True)","0835f5c1":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))","4a0170be":"class CnnModel2(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        # Use a pretrained model\n        self.network = models.wide_resnet101_2(pretrained=True)\n        # Replace last layer\n        num_ftrs = self.network.fc.in_features\n        self.network.fc = nn.Linear(num_ftrs, 4)\n    \n    def forward(self, xb):\n        return torch.sigmoid(self.network(xb))\n\n\n# In[40]:\n\n\nmodel = CnnModel2()\nmodel\n","0671091c":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)\n\n\n# In[42]:\n\n\ndevice = get_default_device()\ndevice","fc5d3866":"train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)\ntest_dl = DeviceDataLoader(test_dl, device)\nto_device(model, device);","4c8b8efe":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","36fbc6be":"model = to_device(CnnModel2(), device)\n\nfor images, labels in train_dl:\n    print('images.shape:', images.shape)\n    out = model(images)\n    print('out.shape:', out.shape)\n    print('out[0]:', out[0])\n    break","f595dfc0":"num_epochs1 = 10\nopt_func = torch.optim.Adam\nlr1 = 0.000010\n\nevaluate(model, val_dl)","37351c0c":"history = fit(num_epochs1, lr1, model, train_dl, val_dl, opt_func)","10528313":"plot_accuracies(history)","a65e8599":"plot_losses(history)","e69b8ef6":"model2 = CnnModel2()\nmodel2.load_state_dict(torch.load('\/kaggle\/input\/weights100\/weights1.pth'))","980b33e7":"model2 = to_device(model2, device)","2619301a":"evaluate(model2, val_dl)","6adc5d3f":"evaluate(model2, test_dl)","8dd9934c":"# Feedforward Neural Network","b164e98a":"The validation accuracy for this CNN was 53.12%. Not a great improvement. Perhaps using a larger network, such as ResNet, can help us achieve our goal. Note that we will be using data augmentation now as well.","486cd342":"EDIT: I reran the code on my other server, and was able to save the weights for the 100% accuracy model. I couldn't get 100% on the kaggle server (guess it doesn't have the magic touch that Compute Canada's Helios server does), and since I can already see the sun rising loading in the weights will have to do.","9ff987fc":"### Load in the Data","0f83dccb":"# Logistic Regression","e79462da":"### Train the model","b2b65ab8":"These old school methods did not perform very well for image classification. Let's try using some CNNs!","d3886e5d":"Mapping of animal classes to integers is as follows:\n\n0 -------> buffalo\n\n1 -------> elephant\n\n2 -------> rhino\n\n3 -------> zebra","2e901a32":"The validation accuracy from the tuned Logistic Regression model was 44.0%. Considering there are only 4 classes, and guessing randomly would get about 25% accuracy, this isn't great. Let's try a feedforward neural network.","73b31405":"Interesting... the model no longer fits at 100% accuracy, even though it did when I ran the code on another server:\n![](http:\/\/i.ibb.co\/TWLfJKy\/lavie.png)\nThis was most likely caused by a lucky start to the gradient descent, that let it find a nice minimum. Restarting the training several times will likely find it again. Let this be a lesson to save your model weights when you get a record breaking score! Live and learn, c'est la vie :'(","1d6ec092":"# Final Remarks","9392bf56":"We have reached 100% accuracy in not only the validation set, but in the test set as well! Therefore, I have surpassed the previous state of the art (98%), and have built a classifier that can be relied upon for classifying the four African animals. This can have further applications in preventing the poaching of endangered species, and can be of interest to conservation groups and other wildlife researchers.\n\nOf course, this notebook is considerably longer than the previous state-of-the-art, which was created using Fast.AI, a framework built on PyTorch that can create models in a fraction of the number of lines of code used for this notebook. Is the extra time and energy worth a 2% increase in model accuracy? That's up to you to decide ;) \n\nThank you for reading until the end! If you have any questions or comments either leave them below the kernel or reach out to me at sergei740@gmail.com. \n\n![](http:\/\/memegenerator.net\/img\/instances\/66006142\/thank-you-for-reading-this-article.jpg)","68c9f4c1":"### Perform Train-Validation-Test Split","d5f724c0":"# African Wildlife Classification\n## By Sergei Issaev","df4692eb":"### Perform Train-Validation-Test Split","ca33c9f1":"\n### Resnet","f7427117":"### Perform Train-Validation-Test Split","bf1f07bc":"### Import Libraries","6ebe0f50":"### Train the Logistic Regression Model","a584cb73":"Since we have already done the imports and dataloaders, we won't repeat them here, and will go straight to defining and training the feedforward neural network.","a29486c2":"# CNN","aeddf596":"![](http:\/\/www.memekingz.net\/img\/memes\/201805\/aceb324e10bb797ec061881347548226.jpg)","e6fb9fb3":"Hello and welcome to my final course project for the 2020 course offering of PyTorch: Zero to GANS, presented by Jovian.ml. I will be attempting to build a classifier capable of taking an input image containing either a buffalo, and elephant, a rhino or a zebra, then outputting the correct class label for the input image. The data was kindly provided by Bianca Ferreira, and can be found here: https:\/\/www.kaggle.com\/biancaferreira\/african-wildlife. \n\nI will be using the current state-of-the-art performance as a benchmark for my own work. From the five notebooks published for this dataset at the time of this writing, the best accuracy was obtained by Leogalbu, who attained an accuracy of 98.0%. His notebook can be found here:\nhttps:\/\/www.kaggle.com\/leogalbu\/african-wildlife-fastai-progressive-resize.\n\nAs per the course instructions, several different methods will be applied to the dataset. They are as follows:\n* 1) Logistic Regression\n* 2) Feedforward Neural Network\n* 3) CNN\n* 4) Transfer learning with wide ResNet and data augmentation\n\nThe links to the github code, Kaggle notebook, and my social media links are included at the end of the article. Please don't forget to upvote if you liked my work !\n","670762a3":"Let's try transfer learning from wide_resnet.","dca8e28d":"The validation accuracy from the tuned feedforward neural network model was 52.0%. This is an improvement, however it is still far below the baseline. Let's try using a convolutional neural network, which is known to do well with image data."}}