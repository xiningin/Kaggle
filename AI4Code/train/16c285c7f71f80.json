{"cell_type":{"e4f0a2b9":"code","825de336":"code","8ceffcf3":"code","7633b907":"code","3a3fa882":"code","6d125fd8":"code","7d6754e1":"code","c49cc969":"code","e581b042":"code","b83c9f0d":"code","d42c02cd":"code","908801bc":"code","c5ce5796":"code","d26e3d44":"code","efb54046":"code","be6396ab":"code","f89c18c5":"code","2229e68a":"code","22088579":"code","b071d99f":"code","3c917ae5":"code","c4255030":"code","6336f14d":"code","b72da5e2":"code","ed7e5e58":"code","1f54a7dc":"code","6a60482c":"code","3f5a396a":"code","d3a980b4":"code","a60b96d7":"code","041155b9":"code","ed406b10":"code","f3132369":"code","9d0d1497":"code","d1769cc1":"code","d7cc78d7":"markdown","d2d34f0e":"markdown","6535ecdd":"markdown","01e69bae":"markdown","0b90a1a8":"markdown","962a6b9d":"markdown","720ad382":"markdown","bb4e7bc6":"markdown","db3b8794":"markdown","d78a23b2":"markdown","f92c791c":"markdown","5f761e86":"markdown","40f97f15":"markdown"},"source":{"e4f0a2b9":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport string\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","825de336":"#load the dataset\nfake_news = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')\ntrue_news = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')","8ceffcf3":"fake_news.head()","7633b907":"true_news.head()","3a3fa882":"fake_news['label'] = 1\ntrue_news['label'] = 0\nnews_data = pd.concat([fake_news,true_news])\nprint('No. of examples:',len(news_data))\nnews_data.head()","6d125fd8":"print(news_data['label'].value_counts())\nsns.countplot(news_data['label'])","7d6754e1":"#drop the subjects and date column\nnews_data.drop('subject',axis=1,inplace=True)\nnews_data.drop('date',axis=1,inplace=True)\nnews_data.head()","c49cc969":"#merge the title and text columns into one\nnews_data['news'] = news_data['title']+\" \"+news_data['text']\nnews_data.head()","e581b042":"#title and text columns no longer needed\nnews_data.drop('title',axis=1,inplace=True)\nnews_data.drop('text',axis=1,inplace=True)\nnews_data.head()","b83c9f0d":"def remove_urls(text):\n  return re.sub('https?:\\S+','',text)","d42c02cd":"def remove_punctuation(text):\n  return text.translate(str.maketrans('','',string.punctuation))","908801bc":"def remove_tags(text):\n  return re.sub('<.*?>',\" \",text)","c5ce5796":"def remove_numbers(text):\n  return re.sub('[0-9]+','',text)","d26e3d44":"#remove urls from text\nnews_data['news'] = news_data['news'].apply(remove_urls)","efb54046":"#remove any tags present in the text\nnews_data['news'] = news_data['news'].apply(remove_tags)","be6396ab":"#remove punctuation from text\nnews_data['news'] = news_data['news'].apply(remove_punctuation)","f89c18c5":"#remove numbers from the text\nnews_data['news'] = news_data['news'].apply(remove_numbers)","2229e68a":"import nltk\nfrom nltk.corpus import stopwords\nstops = stopwords.words('english')","22088579":"def remove_stopwords(text):\n  cleaned = []\n  for word in text.split():\n    if word not in stops:\n      cleaned.append(word)\n  return \" \".join(cleaned)","b071d99f":"news_data['news'] = news_data['news'].apply(remove_stopwords)\nnews_data.head(10)","3c917ae5":"#convert words to lower case\nnews_data['news'] = news_data['news'].apply(lambda word : word.lower())\nnews_data.head()","c4255030":"#stemming\/lemmatization\nfrom nltk.stem import WordNetLemmatizer\n#nltk.download('wordnet')\nlemmatizer = WordNetLemmatizer()\n\ndef lemmatize_words(text):\n  lemmas = []\n  for word in text.split():\n    lemmas.append(lemmatizer.lemmatize(word))\n  return \" \".join(lemmas)","6336f14d":"news_data['lemmatized_news'] = news_data['news'].apply(lemmatize_words)","b72da5e2":"#split into X and y sets\n#shuffle the dataset\nnews_data = news_data.sample(frac=1).reset_index(drop=True)\nnews_x = news_data['lemmatized_news'].values\nnews_y = news_data['label'].values","ed7e5e58":"#tokenization,padding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","1f54a7dc":"tokenizer = Tokenizer() \ntokenizer.fit_on_texts(news_x)\nword_to_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(news_x)","6a60482c":"vocab_size = len(word_to_index)\nmax_length = 200\nembedding_dim = 100\npadded_sequences = pad_sequences(sequences,maxlen=max_length,padding='post',truncating='post')","3f5a396a":"embeddings_index = {};\nwith open('\/kaggle\/working\/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embeddings_index[word] = coefs;\n\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\nfor word, i in word_to_index.items():\n    embedding_vector = embeddings_index.get(word);\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector;","d3a980b4":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Embedding,SpatialDropout1D,LSTM,Bidirectional,Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam","a60b96d7":"model = Sequential([\n    Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    SpatialDropout1D(0.2),\n    Bidirectional(LSTM(128,return_sequences=True)),\n    Dropout(0.2),\n    LSTM(64),\n    Dense(32,activation='relu'),\n    Dense(1,activation='sigmoid')\n])\noptimizer = Adam(learning_rate=0.01)\ncallbacks = ReduceLROnPlateau(monitor='val_accuracy',patience=2,factor=0.5,min_lr=0.00001)\nmodel.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\nmodel.summary()","041155b9":"#split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(padded_sequences,news_y,test_size=0.15,random_state=1)\nprint('No. of training samples:',len(X_train))\nprint('No. of testing samples:',len(X_test))","ed406b10":"epochs = 10\nhistory = model.fit(X_train,y_train,epochs=epochs,validation_data=(X_test,y_test),batch_size=64,callbacks=[callbacks])","f3132369":"#plot the losses and accuracy\nfig,axes = plt.subplots(1,2)\nfig.set_size_inches(30,10)\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = list(range(10))\naxes[0].plot(epochs,acc,label='training accuracy')\naxes[0].plot(epochs,val_acc,label='validation accuracy')\naxes[0].set_xlabel('epoch no.')\naxes[0].set_ylabel('accuracy')\naxes[0].legend()\naxes[1].plot(epochs,loss,label='training loss')\naxes[1].plot(epochs,val_loss,label='validation loss')\naxes[1].set_xlabel('epoch no.')\naxes[1].set_ylabel('loss')\naxes[1].legend()","9d0d1497":"#model evaluation\ntrain_stats = model.evaluate(X_train,y_train)\ntest_stats = model.evaluate(X_test,y_test)\nprint('training accuracy:',train_stats[1]*100)\nprint('testing accuracy:',test_stats[1]*100)","d1769cc1":"#classification report\nfrom sklearn.metrics import classification_report,confusion_matrix\ny_pred = model.predict_classes(X_test)\nprint(classification_report(y_test,y_pred))\nprint('Confusion matix:\\n',confusion_matrix(y_test,y_pred))","d7cc78d7":"### The token sequences will be of different lengths. We set a maximum length for the sequences and pad(or truncate) the sequences accordingly so that each sequence will have the same length","d2d34f0e":"### next we clean our news dataset. We shall do the following:\n#### 1. Remove any links from the text.\n#### 2. Remove any html tags from the text.\n#### 3. Remove punctuation from the text.\n#### 4. Remove numbers from the text.\n#### 5. Remove stopwords.\n#### 6. Convert everything to lower case.\n#### 7. Lemmatize the words.","6535ecdd":"## Stopwords: these are words that do not contribute much to the semantic meaning of the sentence. eg. 'i','am','are','we','your' etc.","01e69bae":"### Next, we merge the title and text columns into one. Removing the title column might lead to loss of some valuable information. So we use both the title as well as the text","0b90a1a8":"### We need to merge the two datasets into one. For that we add an addtional columns in both datasets for the class labels","962a6b9d":"### We create the model. We shall use a multilayer LSTM network. We start with an intial learning rate of 0.01 which is then reduced by a factor of 0.5 if there is no improvement in the validation accuracy for 2 epochs.","720ad382":"### We can see that the classes are balanced. No need to overform undersampling or oversampling. The subject column contains different values for the fake and true news sets. We remove it. We also remove the date column","bb4e7bc6":"### Let us check the distribution of classes","db3b8794":"### Now we construct the embeddings matrix. It is a (nxd) matrix where n is the no. of words in the vocabulary and d is the dimension of the glove vectors","d78a23b2":"## We convert the words to their base or root form. For example: play,playing,plays,played will be converted to play. We can do this in two ways- stemming and lemmatization. Stemming might result in words which are not in the language, for eg: trouble is stemmed to troubl. So we perform lemmatization.","f92c791c":"## Vectorization:\n### now, we vectorize the tokens. Vectorize can be done on several ways. One way is to one-hot encode each vector. But it doesn't reflect the realtionship between the words. For example words 'love' and 'like' have similar meaning but this is not reflected by the one hot vectors. To preserve such relationships we make use of word embeddings. Word embeddings are high dimensional vectors which portray the semantic relationship between words. For example, words 'love' and 'like' will have similar embeddings representation implying that these two words are similar.","5f761e86":"### Our news data is now ready. Next we split our dataset into X and y sets. We then tokenize the sentences. Tokenization is the process of converting a sentence into a sequence of tokens where each token is the index of the corresponding word in the vocabulary. These tokenized words can then be vectorized","40f97f15":"### We use glove word embeddings for the same. The embeddings can be dowloaded from the official glove website:\nhttps:\/\/nlp.stanford.edu\/projects\/glove\/"}}