{"cell_type":{"c2033c69":"code","d050ea5d":"code","27da1434":"code","7901c9a9":"code","281baadb":"code","e23cd2b1":"code","0cbd4c6b":"code","47cae950":"code","c2b6ef0a":"code","daae4515":"code","a232b268":"code","0fb2b428":"code","4351c37d":"code","8ccf04fe":"code","d2273151":"code","386fefec":"code","f86aaa5d":"code","74433dc7":"code","64364d38":"code","c44947c2":"code","2f324367":"code","036a9fe6":"code","8c42eba8":"code","5c32bb74":"code","572cb8ca":"code","78a3ac88":"code","16ac7bdd":"code","709fd8dc":"code","c742bb88":"code","c71db274":"code","71fc6365":"code","4598b1db":"code","da12fe70":"code","4d2cb436":"code","46bbd1ad":"code","ae4de4c8":"markdown","7bd73653":"markdown","2ecf7089":"markdown","b93fb575":"markdown","d458ee2a":"markdown"},"source":{"c2033c69":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d050ea5d":"#importing requiste libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.python.keras import Sequential\nfrom tensorflow.python.keras.layers import LSTM\nfrom sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor,AdaBoostRegressor\nimport matplotlib.pyplot as plt","27da1434":"class dataImport():\n    \n    def __init__(self,filename,type, df=None):\n        self.df = df\n    \n    def import_and_describe_data(self,filename,type):\n      if type=='csv':\n        self.df=pd.read_csv(filename)\n        print('------------------------------------DF Head-----------------------------------------')\n        print(self.df.head())\n        print('-------------------------------------Shape------------------------------------------')\n        print(self.df.shape)\n        print('--------------------------------------Info------------------------------------------')\n        print(self.df.info())\n        print('------------------------------DataSet Description-----------------------------------')\n        print(self.df.describe())\n        return self\n    \n    #Drop columns with single value.\n    def drop_single_value_columns(self):\n        drop_cols = list(filter(lambda x : len(self.df[x].unique()) < 2, self.df.columns))        \n        print('------------------------------Single Value coulmns----------------------------------')\n        print('Columns dropped: ',drop_cols)\n        self.df.drop(drop_cols,axis=1, inplace=True)   \n        return self\n","7901c9a9":"\ntrain=(dataImport(f'\/kaggle\/input\/covid19-global-forecasting-week-2\/train.csv','csv')\n.import_and_describe_data(f'\/kaggle\/input\/covid19-global-forecasting-week-2\/train.csv','csv')\n.drop_single_value_columns()).df","281baadb":"test=(dataImport(f'\/kaggle\/input\/covid19-global-forecasting-week-2\/test.csv','csv')\n.import_and_describe_data(f'\/kaggle\/input\/covid19-global-forecasting-week-2\/test.csv','csv')\n.drop_single_value_columns()).df","e23cd2b1":"temperature_date=(dataImport(f'\/kaggle\/input\/covid19-global-weather-data\/temperature_dataframe.csv','csv')\n.import_and_describe_data(f'\/kaggle\/input\/covid19-global-weather-data\/temperature_dataframe.csv','csv')\n.drop_single_value_columns()).df","0cbd4c6b":"GDP_Density_data=(dataImport(f'\/kaggle\/input\/countries-of-the-world\/countries of the world.csv','csv')\n.import_and_describe_data(f'\/kaggle\/input\/countries-of-the-world\/countries of the world.csv','csv')\n.drop_single_value_columns()).df","47cae950":"#removing spaces in string columns and coverting all textual data to lower case\ntrain=train.apply(lambda x: x.astype(str).str.lower().replace(' ','', regex=True))\ntest=test.apply(lambda x: x.astype(str).str.lower().replace(' ','', regex=True))\ntemperature_date=temperature_date.apply(lambda x: x.astype(str).str.lower().replace(' ','', regex=True))\nGDP_Density_data=GDP_Density_data.apply(lambda x: x.astype(str).str.lower().replace(' ','', regex=True))\n#Province is more or less have Nan values replacing with country's value\ntrain['Province_State']=np.where(train.Province_State=='nan', train.Country_Region, train.Province_State)\ntest['Province_State']=np.where(test.Province_State=='nan', test.Country_Region, test.Province_State)\ntemperature_date['province']=np.where(temperature_date.province=='nan', temperature_date.country, temperature_date.province)","c2b6ef0a":"#for 177 rows temparature is null replacing with 0\ntemperature_date=temperature_date.fillna(0)\ntemperature_date.info()","daae4515":"#3-4 countries\/rows have none values will be replaced with 0's.\n# Will go for machine learning driven imputation for now will proceed with replacing 0's\nGDP_Density_data=GDP_Density_data.fillna(0)\nGDP_Density_data.info()","a232b268":"# there are few countries which are represented differently in temparature and covid data set changing them in temparature data set\ntemperature_date['country']=temperature_date.country.replace(\"usa\",\"us\")\ntemperature_date['country']=temperature_date.country.replace(\"uk\",\"unitedkingdom\")\ntemperature_date['country']=temperature_date.country.replace(\"taiwan\",\"taiwan*\")\ntemperature_date['country']=temperature_date.country.replace(\"korea\",\"korea,south\")\ntemperature_date['country']=temperature_date.country.replace(\"uae\",\"unitedarabemirates\")","0fb2b428":"#changing temparature field to float \ntemperature_date.tempC=temperature_date.tempC.astype(float)\n#taking mean of temparature for every country\ntemperature_date=temperature_date.groupby(temperature_date['country'])['tempC'].mean().reset_index()","4351c37d":"#mereging temperature data set with train\/test\ntrain = pd.merge(train,temperature_date[['tempC','country']], how='left',  left_on=['Country_Region'], right_on=['country'])\ntest = pd.merge(test,temperature_date[['tempC','country']], how='left',  left_on=['Country_Region'], right_on=['country'])\ntrain.head()","8ccf04fe":"#few african countries don't have tempartures and simple google search inidcates temp around 20\n# as said earlier will replace with ML imputation technique after looking how this metric influences the model\ntrain.tempC.fillna(20,inplace=True)\ntest.tempC.fillna(20,inplace=True)\ntrain.info()","d2273151":"#correcting country names in Density data set\nGDP_Density_data['Country']=GDP_Density_data.Country.replace(\"unitedstates\",\"us\")\nGDP_Density_data['Country']=GDP_Density_data.Country.replace(\"taiwan\",\"taiwan*\")\nGDP_Density_data['Country']=GDP_Density_data.Country.replace(\"korea\",\"korea,south\")\nGDP_Density_data['Country']=GDP_Density_data.Country.replace(\"uae\",\"unitedarabemirates\")","386fefec":"train = pd.merge(train,GDP_Density_data, how='left',  left_on=['Country_Region'], right_on=['Country'])\ntest = pd.merge(test,GDP_Density_data, how='left',  left_on=['Country_Region'], right_on=['Country'])\ntrain.info()","f86aaa5d":"#imputing 0's where data is not there\ntrain.fillna(0,inplace=True)\ntest.fillna(0,inplace=True)\ntrain.head()","74433dc7":"#dropping unwanted columns\ntrain=train.drop(['country','Country','Region'], axis = 1)\ntest=test.drop(['country','Country','Region'], axis = 1)\ntest.head()","64364d38":"#identifying numerical data and converting to float values\ncols=['ConfirmedCases','Fatalities','Population','Area (sq. mi.)','Pop. Density (per sq. mi.)'\n      ,'Coastline (coast\/area ratio)','Net migration','Infant mortality (per 1000 births)',\n     'GDP ($ per capita)','Literacy (%)','Phones (per 1000)','Climate','Birthrate','Deathrate']\ncols_test=['Population','Area (sq. mi.)','Pop. Density (per sq. mi.)'\n      ,'Coastline (coast\/area ratio)','Net migration','Infant mortality (per 1000 births)',\n     'GDP ($ per capita)','Literacy (%)','Phones (per 1000)','Climate','Birthrate','Deathrate']\n#from locale import atof\ntrain[cols]=train[cols].astype(str).apply(lambda x: x.str.replace(',', '').astype(float), axis=1)\ntest[cols_test]=test[cols_test].astype(str).apply(lambda x: x.str.replace(',', '').astype(float), axis=1)","c44947c2":"#dropping columns which not be required\ntrain=train.drop(['Area (sq. mi.)','Coastline (coast\/area ratio)','Net migration','Industry','Phones (per 1000)','Arable (%)','Crops (%)','Other (%)','Climate','Agriculture','Service'], axis = 1)\ntest=test.drop(['Area (sq. mi.)','Coastline (coast\/area ratio)','Net migration','Industry','Phones (per 1000)','Arable (%)','Crops (%)','Other (%)','Climate','Agriculture','Service'], axis = 1)\n\ntrain.info()","2f324367":"#imputing most commonly applied values\ndef fillna(col):\n    col.fillna(col.value_counts().index[0], inplace=True)\n    return col\ntrain=train.apply(lambda col:fillna(col))\ntest=test.apply(lambda col:fillna(col))\ntrain.info()","036a9fe6":"#will bin temparature into 9 bins to see if any impact on confirmed cases\/fatalities\n\ntrain['quan']=pd.qcut(train['tempC'], q=[0,.1, .25, .5, .75,.80,.85,.90,.95,.99], labels=[1,2,3,4,5,6,7,8,9])\np=train[['tempC','quan','ConfirmedCases','Fatalities','Pop. Density (per sq. mi.)','Population']][(train.ConfirmedCases>0)]\nlists=['Fatalities','Pop. Density (per sq. mi.)']\n\np.groupby(p['quan'])['tempC'].mean()#.reset_index()\n\np=p[['quan','ConfirmedCases','tempC','Fatalities','Pop. Density (per sq. mi.)','Population']].groupby(p['quan'])['ConfirmedCases','tempC','Fatalities','Pop. Density (per sq. mi.)','Population'].max().reset_index()\np=pd.DataFrame(p)\n#sns.lineplot(x='tempC',y=lists, markers=True,data=p)\nax = p.plot(x=\"tempC\", y=\"ConfirmedCases\", legend=False,color='b')\nax2 = ax.twinx()\np.plot(x=\"tempC\", y=[\"Pop. Density (per sq. mi.)\",\"Fatalities\"],ax=ax2, legend=False,color=['r','y'])\nax.figure.legend()\nplt.show()","8c42eba8":"#dropping quan\ntrain=train.drop(['quan'], axis = 1)\ntrain.head()","5c32bb74":"#checing for correlation between different I\/P parameters\ntrain.corr()","572cb8ca":"#'tempC' removing for now once ml imputer is implemented will bring this back\ntrain=train.drop(['tempC'], axis = 1)\ntest=test.drop(['tempC'], axis = 1)\n'''\nX=train[['Province_State', 'Country_Region', 'Date',  'Population', 'Pop. Density (per sq. mi.)',\n       'Infant mortality (per 1000 births)', 'GDP ($ per capita)',\n       'Literacy (%)', 'Birthrate', 'Deathrate']]\nX1=test[['Province_State', 'Country_Region', 'Date',  'Population', 'Pop. Density (per sq. mi.)',\n       'Infant mortality (per 1000 births)', 'GDP ($ per capita)',\n       'Literacy (%)', 'Birthrate', 'Deathrate']]   \n'''\nX=train[['Province_State', 'Country_Region', 'Date']]\nX1=test[['Province_State', 'Country_Region', 'Date']]   \n\nY_ConfirmedCases=train['ConfirmedCases']\nY_Fatalities=train['Fatalities']","78a3ac88":"from sklearn.preprocessing import MinMaxScaler,LabelEncoder,StandardScaler\nautoscaler = LabelEncoder()\nX['Province_State']=autoscaler.fit_transform(X['Province_State'])\nX['Country_Region']=autoscaler.fit_transform(X['Country_Region'])\nX['Date']=autoscaler.fit_transform(X['Date'])\nX1['Province_State']=autoscaler.fit_transform(X1['Province_State'])\nX1['Country_Region']=autoscaler.fit_transform(X1['Country_Region'])\nX1['Date']=autoscaler.fit_transform(X1['Date'])\n","16ac7bdd":"\nminmaxscale=StandardScaler()\n\nminmaxscale.fit(pd.concat([X,X1]))\n                              \ntrain_X=minmaxscale.fit_transform(X)\ntest_X=minmaxscale.fit_transform(X1)\n","709fd8dc":"\ntuned_models = [\n               RandomForestRegressor(n_estimators= 8,\n                               criterion= 'mse',\n                               max_features = 'log2',#log2\n                               min_samples_split = 60,\n                               random_state = 40)]  \ntuned_parameters = {    'base_estimator':tuned_models,\n                        'loss' : ['exponential']#exponential\n                        ,'random_state' : [43]\n                        ,'learning_rate' : [0.1]\n                         }\n#exponential\nclf_ConfirmedCases = GridSearchCV(AdaBoostRegressor(), tuned_parameters, cv=4)\nclf_ConfirmedCases.fit(train_X,Y_ConfirmedCases)\n\nclf_Fatalities = GridSearchCV(AdaBoostRegressor(), tuned_parameters, cv=4)\nclf_Fatalities.fit(train_X,Y_Fatalities)","c742bb88":"pred_xgbrf_ConfirmedCases = clf_ConfirmedCases.predict(train_X)\nmetrics.r2_score(Y_ConfirmedCases,pred_xgbrf_ConfirmedCases)","c71db274":"pred_xgbrf_Fatalities = clf_Fatalities.predict(train_X)\nmetrics.r2_score(Y_Fatalities,pred_xgbrf_Fatalities)","71fc6365":"train_X1=train\ntrain_X1=pd.concat([pd.DataFrame(pred_xgbrf_ConfirmedCases,columns=['ConfirmedCases_Predicted']),train_X1],axis=1)\ntrain_X1=pd.concat([pd.DataFrame(pred_xgbrf_Fatalities,columns=['Fatalities_Predicted']),train_X1],axis=1)\ntrain_X1[train_X1.Country_Region=='india']","4598b1db":"\nclf_ConfirmedCases_pred_test=clf_ConfirmedCases.predict(test_X)\n\nclf_Fatalities_pred_test=clf_Fatalities.predict(test_X)","da12fe70":"\ntest_X1=test\ntest_X1=pd.concat([pd.DataFrame(clf_ConfirmedCases_pred_test,columns=['ConfirmedCases_Predicted']),test_X1],axis=1)\ntest_X1=pd.concat([pd.DataFrame(clf_Fatalities_pred_test,columns=['Fatalities_Predicted']),test_X1],axis=1)\ntest_X1\n","4d2cb436":"test_X1[test_X1.Country_Region=='spain']","46bbd1ad":"output=test_X1[['ForecastId','ConfirmedCases_Predicted','Fatalities_Predicted']].astype('int')\noutput.columns=['ForecastId','ConfirmedCases_Predicted','Fatalities_Predicted']\noutput.to_csv('submission.csv', index=False)","ae4de4c8":"As we see above there seems to more reported cases where temparature is less than 25 degress<br>\nALso we see density of population one more factor with tempearature cause for confirmed cases<br>\nWill have GDP and other social indicators also considered as in near future that may determine how things moves in another 2-3 months.<br>\nThere are 2 reasons to capture social\/economic indicators like GDP and birth\/mortality rate:<br>\na. Countries with better socio economic indicators may have more people travelling which may be the reson for spike in covid cases.<br>\nb. Poorer countries may have less travllers, but lacking proper testing and lack of health care may be a deterent, time will tell how this will progress in next 2-3 months.\n","7bd73653":"Working on few time series forecasting method like var, vecm and lag model will update the note book once done.<br>\nAlso will include few more analysis on univariate, bi-variate and multi variate by considering other related data sets like climate and socio- economic data.<br>\n","2ecf7089":"Weather data source: https:\/\/www.kaggle.com\/winterpierre91\/covid19-global-weather-data<br>\nPopulation density and socio-economic data source: https:\/\/www.kaggle.com\/fernandol\/countries-of-the-world","b93fb575":"1. Data Cleansing","d458ee2a":"1. Importing Data"}}