{"cell_type":{"f3de9c2c":"code","c6389b60":"code","b54b4007":"code","0a0c5fde":"code","67a27154":"code","1870c994":"code","9536a4a5":"code","b9a3206a":"code","dd23cc52":"code","f9c260a2":"markdown","59d266b7":"markdown","3c14aeb4":"markdown","0fed79b9":"markdown"},"source":{"f3de9c2c":"import pandas as pd\nimport lightgbm\nimport numpy as np\nimport random\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nrandom.seed(64)\nnp.random.seed(64)","c6389b60":"%time df_test = pd.read_parquet('..\/input\/tpsdec2021parquet\/test.pq')\n%time df = pd.read_parquet('..\/input\/tpsdec2021parquet\/train.pq')","b54b4007":"%time pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')\n%time csv_train = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')","0a0c5fde":"df.info()","67a27154":"label_encoder = LabelEncoder()\n\nX_train, y_train = df.drop(columns=['Id', 'Cover_Type']), label_encoder.fit_transform(df.Cover_Type)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, shuffle=True, test_size=.2)\nX_test = df_test.drop(columns=['Id'])","1870c994":"%%time\n\nsane_defaults = {\n    'objective': 'multiclass',\n    'num_class': len(label_encoder.classes_),\n    'learning_rate': .025,\n    'seed': 64,\n    'boosting': 'goss',\n    'feature_fraction': .5,\n    'force_row_wise': True,\n    'metric': ['multi_logloss', 'multi_error'],\n    'verbosity': -1,\n    'first_metric_only': True,\n}\n\nbooster = lightgbm.train(\n    params=sane_defaults,\n    train_set=lightgbm.Dataset(X_train, label=y_train),\n    num_boost_round=3000,\n    valid_sets=[lightgbm.Dataset(X_val, label=y_val)],\n    early_stopping_rounds=50,\n    verbose_eval=100,\n)","9536a4a5":"px.bar(\n    x=booster.feature_name(),\n    y=booster.feature_importance(),\n    title='importance_type = \"split\"'\n)","b9a3206a":"px.bar(\n    x=booster.feature_name(),\n    y=booster.feature_importance('gain'),\n    title='importance_type = \"gain\"'\n)","dd23cc52":"sub = df_test[['Id']].assign(\n    Cover_Type=label_encoder.inverse_transform(booster.predict(X_test).argmax(axis=1))\n)\n\nsub.to_csv('submission.csv', index=False)","f9c260a2":"That takes about 1 second on kaggle. Let's compare that with the CSV files:","59d266b7":"At more than 20 seconds, there's really no contest -- especially, because the csv train file has all the wrong datatypes, whereas the parquet file remembers our selection from [the last notebook](https:\/\/www.kaggle.com\/kaaveland\/tps202112-parquet?scriptVersionId=81264309):","3c14aeb4":"Reading parquet data\n==\n\nIn [this notebook](https:\/\/www.kaggle.com\/kaaveland\/tps202112-parquet) I converted the competition data to parquet format, so that I don't need to read the data from csv in future notebooks for this competition.\n\nCSV files have some pretty annoying disadvantages:\n\n- It is slow to read data from CSV -- this is particularly egregious if your data has timestamp columns in it, or if you have strings.\n- They are big. This is part of the reason why they're slow, it simply takes a while to move so much text data from disk into memory.\n- They are untyped. In CSV, everything is a string -- it's up to the reading program to decide how to interpret the strings.\n\nParquet files do much better in all of these aspects, at the cost of not being human-readable text files.\n\nIn this case, our `train.pq` is 77MB, vs 548MB for `train.csv` -- even though it contains the same data!\n\nThis is a pretty normal compression ratio, in my experience -- when there are low cardinality columns, or repeated values, parquet files use tricks like run-length encoding to achieve compression of ratios between 2-10 compared to CSV.\n\nLet's measure how long it takes to read the parquet files:","0fed79b9":"Use `lightgbm` to estimate feature importances\n==\n\nHere's a baseline model I typed up to get some feature importance plots:"}}