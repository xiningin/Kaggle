{"cell_type":{"d708c1aa":"code","01f57f3d":"code","6892733a":"code","36e1aeb7":"code","c8412e35":"code","9ba54040":"code","b2098171":"code","26fa35b8":"code","2bfdcc8a":"code","fc072111":"code","3449d449":"code","62f06b76":"code","0c5ccea6":"code","b70ae0c8":"code","4738468e":"code","7b7e1de5":"code","a8bcd954":"code","9ce827e7":"markdown","903e4995":"markdown","5da238a6":"markdown","81e97530":"markdown","7233aaec":"markdown","89abeb41":"markdown","84f65b3f":"markdown","37f5da55":"markdown","f1e5dc01":"markdown","da485f9c":"markdown","5217f1a4":"markdown","9a07137a":"markdown"},"source":{"d708c1aa":"# Libraries\nimport numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn')\n%matplotlib inline\nimport copy\nimport datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import LabelEncoder\n\nimport os\nimport re\nimport gc\nimport pickle  \nimport random\nimport keras\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n#import tensorflow_hub as hub\nimport keras.backend as K\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, Lambda\nfrom keras.optimizers import Adam\nfrom keras.callbacks import Callback\nfrom scipy.stats import spearmanr, rankdata\nfrom os.path import join as path_join\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import MultiTaskElasticNet\nfrom sklearn.linear_model import Ridge\nimport glob\nfrom sklearn.model_selection import train_test_split\n\nseed(42)\nrandom.seed(42)","01f57f3d":"data_dict = {}\nfor i in glob.glob('..\/input\/m5-forecasting-accuracy\/*'):\n    name = i.split('\/')[-1].split('.')[0]\n    if name != 'MTeamSpellings':\n        data_dict[name] = pd.read_csv(i)\n    else:\n        data_dict[name] = pd.read_csv(i, encoding='cp1252')","6892733a":"data_dict.keys()","36e1aeb7":"# check the subimission fotmat\ndata_dict['sample_submission']","c8412e35":"# name the data respectively \n\ncal = data_dict['calendar']\nstv = data_dict['sales_train_validation']\nss = data_dict['sample_submission']\nsellp = data_dict['sell_prices']","9ba54040":"stv_df = stv.drop(['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], axis=1).set_index('id').T\nstv_df['d'] = stv_df.index","b2098171":"df = pd.merge(cal, stv_df, left_on='d', right_on='d', how='left')","26fa35b8":"def event_detector(x):\n    if x == None:\n        return 0\n    else:\n        return 1","2bfdcc8a":"#drop the date we won't use\ndrp = ['wm_yr_wk', 'weekday', 'year', 'd', 'event_type_1', 'event_type_2']\n\n#we will use these columns\ncols_x = ['wday', 'month', 'event_name_1', 'event_name_2','snap_CA', 'snap_TX', 'snap_WI']\n\n\n# process events to binary\ndf = df.drop(drp, axis=1)\ndf['event_name_1'] = df['event_name_1'].apply(lambda x: event_detector(x))\ndf['event_name_2'] = df['event_name_2'].apply(lambda x: event_detector(x))","fc072111":"# separate validation data and evaluation data\nddf = df[(pd.to_datetime(df['date']) < '2016-04-25')&(pd.to_datetime(df['date']) >= '2015-06-19')].drop('date', axis=1)\nvalid_df = df[(pd.to_datetime(df['date']) >= '2016-04-25')&(pd.to_datetime(df['date']) < '2016-05-23')].drop('date', axis=1)\neval_df = df[pd.to_datetime(df['date']) >= '2016-05-23'].drop('date', axis=1)","3449d449":"X_ddf = ddf[cols_x]\ny_ddf = ddf.drop(cols_x, axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(\n         X_ddf, y_ddf, test_size=0.33, random_state=42)","62f06b76":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.callbacks import TensorBoard\nimport keras.backend as K\nEarlyStopping = tf.keras.callbacks.EarlyStopping()\n\n\ndef rmse(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred -y_true)))\n\nepochs=250\nbatch_size = 96\nverbose = 1\nvalidation_split = 0.2\ninput_dim = X_train.shape[1]\nn_out = y_train.shape[1]\n\nmodel = Sequential([\n                Dense(512, input_shape=(input_dim,)),\n                Activation('relu'),\n                Dropout(0.2),\n                Dense(512),\n                Activation('relu'),\n                Dropout(0.2),\n                Dense(n_out),\n                Activation('relu'),\n                    ])\n\nmodel.compile(loss='mse',\n                 optimizer='adam',\n                 metrics=['mse', rmse])\nhist = model.fit(X_train, y_train,\n                         batch_size = batch_size, epochs = epochs,\n                         callbacks = [EarlyStopping],\n                         verbose=verbose, validation_split=validation_split)\n\nscore = model.evaluate(X_test, y_test, verbose=verbose)\nprint(\"\\nTest score:\", score[0])","0c5ccea6":"plt.clf()\nplt.figsize=(15, 10)\nloss = hist.history['loss']\nval_loss = hist.history['val_loss']\nepochs = range(1, len(loss) +1)\n\nplt.plot(epochs, loss, 'bo', label = 'Training loss')\nplt.plot(epochs, val_loss, 'b', label = 'Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","b70ae0c8":"import shap\ndf_train_normed = X_train\ndf_train_normed_summary = shap.kmeans(df_train_normed.values, 25)\n# Instantiate an explainer with the model predictions and training data summary\nexplainer = shap.KernelExplainer(model.predict, df_train_normed_summary)\n# Extract Shapley values from the explainer\nshap_values = explainer.shap_values(df_train_normed.values)","4738468e":"X_valid = valid_df[cols_x]\nX_eval = eval_df[cols_x]\n\n# predict validation and evaluation respectively\npred_valid = pd.DataFrame(model.predict(X_valid), columns = ss[0:int(len(ss)\/2)].set_index('id').T.columns)\npred_eval = pd.DataFrame(model.predict(X_eval), columns = ss[int(len(ss)\/2):].set_index('id').T.columns)\nss_valid =  pred_valid.T\nss_eval = pred_eval.T\n\n# concatenate val and eval\nsubmission_df = pd.concat([ss_valid, ss_eval]).reset_index()\nsubmission_df.columns = ss.columns\n\nsubmission_df.head()","7b7e1de5":"#check some of prediction\nd_cols = [c for c in submission_df.columns if 'F' in c]\npred_example = submission_df.sample(10, random_state=2020).set_index('id')[d_cols].T\n\nfig, axs = plt.subplots(5, 2, figsize=(15, 10))\naxs = axs.flatten()\nax_idx = 0\nfor item in pred_example.columns:\n    pred_example[item].plot(title=item,\n                              ax=axs[ax_idx])\n    ax_idx += 1\nplt.tight_layout()\nplt.show()","a8bcd954":"submission_df.to_csv('submission.csv', index=False)","9ce827e7":"# modeling","903e4995":"More:\n* visualize the feature importances of NN model: https:\/\/www.kaggle.com\/kmatsuyama\/shap-feature-importances-plot-of-nn-models","5da238a6":"# Overview\n* This kernel will produce:\n    * a starter model to predict the future sales using keras\n* This kernel will not produce:\n    * EDA\n    * particular feature engineering\n    ","81e97530":"# make prediction","7233aaec":"# processing","89abeb41":"In this part, we will:\n* make predictions of validation and evaluation periods\n    * validation period: 2016-04-25 to 2016-05-22 \uff08d_1914 - d_1941) \n    * evaluation period: 2016-05-23 to 2016-06-19 \uff08d_1942 - d_1969\uff09\n* visualize the predicted sales of some products","84f65b3f":"In this part, we will:\n* load the data\n* name the date respectively\n\n\nabout this part, thanks to great kernel!  \nhttps:\/\/www.kaggle.com\/robikscube\/m5-forecasting-starter-data-exploration","37f5da55":"In this part, we will:\n* extract wday, month, events and SNAP availability of stores as features\n* process the events columns to binary (any event:1, no event:0)\n\n\nSNAP is a program of US that support food-purchase for low (or no) income person.  \nmore at: https:\/\/en.wikipedia.org\/wiki\/Supplemental_Nutrition_Assistance_Program","f1e5dc01":"In this part, we will:\n* prepare the data for training; we will use 2015-06-19 ~ 2016-04-25 as the training period\n* make a very simple NN model using Keras\n* visualize the process of training ","da485f9c":"Further:\n* more understanding the data\n* feature engineerings based on EDA\n* model tuning","5217f1a4":"# Loading the data","9a07137a":"# Next\nVisualize the featuer importances of NN model: https:\/\/www.kaggle.com\/kmatsuyama\/shap-feature-importances-plot-of-nn-models"}}