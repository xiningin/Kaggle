{"cell_type":{"77881eae":"code","7bd7d84c":"code","a9d0c7a0":"code","1f1f6202":"code","268e0228":"code","10909774":"code","aed6ae34":"code","dc98df70":"code","11253130":"code","36120ba3":"code","f8a942eb":"markdown","bc7ff4c4":"markdown","ff5c634a":"markdown","0dbd1920":"markdown"},"source":{"77881eae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport time\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7bd7d84c":"from sklearn.model_selection import StratifiedKFold\n\nimport multiprocessing, time, itertools\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score, precision_score, recall_score, roc_curve, auc, roc_auc_score\n\nfrom collections import namedtuple, Counter\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\n\ndef get_model_ml_(params):\n\n  if params.split()[0] == 'svc':\n    if params.split()[1] == 'none':\n      C_parameter=1.0\n    else:\n      C_parameter=float(params.split()[1])\n\n    if params.split()[2] == 'none':\n      gamma_parameter='scale'\n    else:\n      gamma_parameter=float(params.split()[2])\n      \n    clf = svm.SVC(C=C_parameter, gamma=gamma_parameter, kernel='rbf', probability=True)\n\n  elif params.split()[0] == 'rfc':\n    if len(params.split()) != 4:\n      if params.split()[1].lower()  == 'none':\n        clf = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=int(params.split()[2]))\n      else:        \n        clf = RandomForestClassifier(n_estimators=100, max_depth=int(params.split()[1]), min_samples_split=int(params.split()[2]))\n    else: # aqui eh pra entrar o n_est\n      try:\n        max_depth = int(params.split()[1])\n      except:\n        max_depth = None\n      try:\n        min_samples_split = int(params.split()[2])\n      except:\n        print ('Erro '+params)\n        sys.exit(0)\n      try:\n        n_estimators = int(params.split()[3])\n      except:\n        print ('Erro '+params)\n        sys.exit(0)\n      clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split)\n\n  elif params.split()[0] == 'logit':\n    if params.split()[1].lower()  == 'none':\n      clf = LogisticRegression(penalty='l2', solver='lbfgs', multi_class='multinomial', C=1.0)\n    else:\n      clf = LogisticRegression(penalty='l2', solver='lbfgs', multi_class='multinomial', C=float(params.split()[1]))\n\n  elif params.split()[0] == 'ada':\n      clf = AdaBoostClassifier(n_estimators=int(params.split()[1]), learning_rate=float(params.split()[2]))\n\n  else:\n    print ('Nao foi identificado o classificador. {}'.format(params))\n    \n  return clf\n\ndef cros_val(clf, X, Y, metrics=['accuracy', 'recall'], resampling='undersampling', cv=3, multiclass=False):\n\n  n_classes = len(set(Y))\n\n  # falta instanciar\n  return_named_tuple = namedtuple('return_named_tuple', ('clf', 'smote', 'cv', 'accuracy', 'recall', 'auc', 'f1_score'))\n\n  # laco dos folds\n  cv_folds = StratifiedKFold(n_splits=cv, shuffle=True, random_state=int(time.time()))\n\n  scores_f1 = list()\n  scores_precision = list()\n  scores_recall = list()\n  scores_auc = list()\n\n  Y_pred_proba_geral = np.zeros(shape=Y.shape)\n  Y_pred_geral = np.zeros(shape=Y.shape)\n\n  for train, test in cv_folds.split(X, Y):\n\n    # essa linha h soh pra setar caso o augmented seja None\n    if resampling == None:\n      X_train_aug, Y_train_aug = X[train], Y[train]\n\n    # agora eh necessario checar o aumento de dados\n    if resampling == 'smote':\n      X_train_aug, Y_train_aug = SMOTE().fit_resample(X[train], Y[train])\n\n    if resampling == 'undersampling':\n      s = s + 'aug check: undersampling\\n'\n      rus = RandomUnderSampler()\n      X_train_aug, Y_train_aug = rus.fit_resample(X[train], Y[train])\n\n    if resampling == 'oversampling':\n      s = s + 'aug check: oversampling\\n'\n      ros = RandomOverSampler()\n      X_train_aug, Y_train_aug = ros.fit_resample(X[train], Y[train])\n    \n    # treino e predicoes\n    clf.fit(X_train_aug, Y_train_aug)\n    Y_pred = clf.predict(X[test])\n    Y_true = Y[test]\n\n    # guarda na matrizona geral\n    if multiclass:\n      roc_temporario_ = 0\n      for i in range(1, n_classes+1):\n        Y_pred_proba_geral[test] = clf.predict_proba(X[test])[:, i-1] # pegar o proba 1 aqui \n        roc_temporario_ = roc_temporario_ + roc_auc_score((Y_true==i).astype('int'), Y_pred_proba_geral[test])\n      roc_temporario_ = roc_temporario_ \/ n_classes\n      scores_auc.append(roc_temporario_)\n    else:\n      Y_pred_proba_geral[test] = clf.predict_proba(X[test])[:, 1] # pegar o proba 1 aqui \n      scores_auc.append(roc_auc_score(Y_true, Y_pred_proba_geral[test]))\n\n\n    #Y_pred_proba_geral[test] = clf.predict_proba(X[test])[:, 1].copy() # pegar o proba 1 aqui \n    Y_pred_geral[test] = clf.predict(X[test]).copy()\n\n\n    # guardando os scores\n    #scores_f1.append(f1_score(Y_true, Y_pred, average=average))\n    #scores_precision.append(precision_score(Y_true, Y_pred, average=average))\n    #scores_recall.append(recall_score(Y_true, Y_pred, average=average))\n    #scores_auc.append(roc_auc_score(Y_true, Y_pred_proba_geral[test]))\n\n    # guardando as confmatrix de cada fold\n    confm = confusion_matrix(Y_true, Y_pred) \n    #s = s + str(confm) + '\\n'\n\n  scores_f1 = np.array(scores_f1)\n  scores_precision = np.array(scores_precision)\n  scores_recall = np.array(scores_recall)\n  scores_auc = np.array(scores_auc)\n\n  # conf matrix\n  #Y_pred = cross_val_predict(clf, X, Y, cv=cv)\n  #Y_true = Y.copy()\n  #confm = confusion_matrix(Y_true, Y_pred_geral)\n\n  r = return_named_tuple (clf, resampling, cv, scores_precision, scores_recall, scores_auc, scores_f1)\n  \n  return r\n\n\n### function to assist parallel nested cross-validation implementation\ndef f(params, X, Y, cv, resampling):\n\n  print ('rodando params: {}'.format(params))\n\n  clf = get_model_ml_(params)\n  \n  if len(set(Y)) > 2:\n    multiclass = True\n  elif len(set(Y)) == 2:\n    multiclass = False\n  else:\n    print ('Erro! flag multiclass.')\n\n  return_ = cros_val(clf, X, Y, metrics=['accuracy', 'recall'], resampling=resampling, cv=cv, multiclass=multiclass)\n\n  return return_.auc.mean()\n\n  print ('params {} finalizado.'.format(params))\n\n\ndef grid_search_nested_parallel(X, Y, cv=3, writefolder=None, n_jobs=30, resampling='undersample'):\n\n  n_classes = len(set(Y))\n\n  if len(set(Y)) > 2:\n    multiclass = True\n  elif len(set(Y)) == 2:\n    multiclass = False\n  else:\n    print ('Erro! flag multiclass.')\n\n  # laco dos folds\n  cv_folds = StratifiedKFold(n_splits=cv, shuffle=True, random_state=int(time.time()))\n\n  Y_pred_proba_geral = np.zeros(shape=Y.shape)\n  Y_pred_geral = np.zeros(shape=Y.shape)\n\n  \n  # SVC\n  C_list = np.logspace(np.log10(0.1), np.log10(1000), num=3)\n  C_list = [str(x) for x in C_list]\n  gamma_list = np.logspace(np.log10(0.0001), np.log10(1), num=3)\n  gamma_list = [str(x) for x in gamma_list]\n\n  svc_kernel = 'rbf'\n  svc_params_list = list(itertools.product(C_list, gamma_list))\n  svc_params_list = ['svc '+' '.join(x) for x in svc_params_list]\n  \n  # RF\n  max_depth_list = ['1', '2', 'None']\n  min_samples_split_list = ['2', '4']\n\n  rfc_params_list = list(itertools.product(max_depth_list, min_samples_split_list))\n  rfc_params_list = ['rfc '+' '.join(x) for x in rfc_params_list]\n\n  # Logit\n  C_list = np.logspace(np.log10(0.0001), np.log10(10), num=10)\n  C_list = [str(x) for x in C_list]\n  logit_params_list = ['logit '+' '+x for x in C_list]\n\n  params_list = svc_params_list + rfc_params_list + logit_params_list\n   \n  params_scores = np.zeros((len(params_list),))\n  params_std_scores = np.zeros((len(params_list),))\n\n  s = ''\n\n  if (writefolder != None):\n    plt.figure(figsize=(18,10))\n    plt.ylabel('AUC score')\n    plt.xlabel('Parameter set number')\n    plt.title('')\n\n\n  best_params_all = list()\n  best_auc_scores_holdout = list()\n  \n  #plt.figure(figsize=(12,8))\n\n  for i, (train, test) in enumerate(cv_folds.split(X, Y)):\n\n\n    parameters_vector_total = [(x, X[train], Y[train], cv, resampling) for x in params_list]\n\n    params_scores_partial = list()\n    for parameters_vector in [parameters_vector_total[j:j+n_jobs] for j in range(0, len(parameters_vector_total), n_jobs)]:\n      with multiprocessing.Pool(processes=n_jobs) as pool:\n        params_scores_partial = params_scores_partial + pool.starmap(f, parameters_vector)\n\n    params_scores = np.array(params_scores_partial)\n  \n    best_params = params_list[params_scores.argmax()]\n    best_params_all.append(best_params)\n    best_params_idx = params_scores.argmax()\n\n    clf = get_model_ml_(best_params)\n    \n    ##\n    if resampling == None or resampling == 'None':\n      X_train_aug, Y_train_aug = X[train], Y[train]\n\n    # agora eh necessario checar o aumento de dados\n    if resampling == 'smote':\n      X_train_aug, Y_train_aug = SMOTE().fit_resample(X[train], Y[train])\n\n    if resampling == 'undersampling':\n      rus = RandomUnderSampler()\n      X_train_aug, Y_train_aug = rus.fit_resample(X[train], Y[train])\n\n    if resampling == 'oversampling':\n      ros = RandomOverSampler()\n      X_train_aug, Y_train_aug = ros.fit_resample(X[train], Y[train])\n    \n    # treino e predicoes\n    clf.fit(X_train_aug, Y_train_aug)\n    Y_pred = clf.predict(X[test])\n    #Y_true = Y[test]\n    ##\n    \n    \n    #clf.fit(X[train], Y[train])\n    Y_true = Y[test]\n    \n\n    if writefolder:\n      s = s + '####### FOLD {} of {} #####\\n'.format(i+1, cv)\n      for param, score, std in zip(params_list, params_scores, params_std_scores):\n        s = s + 'param: {}, score: {:.3} ({:.4})\\n'.format(param, score, std)\n      s = s + '* Best params: {}, idx: {} - score: {:.3}\\n'.format(best_params, best_params_idx, params_scores[best_params_idx])\n      \n      s = s + '*** Evaluation phase ***\\n'\n      \n      if multiclass:\n        roc_temporario_ = 0\n        for j in range(1, n_classes+1):\n          Y_pred_proba_geral[test] = clf.predict_proba(X[test])[:, j-1] # pegar o proba 1 aqui \n          roc_temporario_ = roc_temporario_ + roc_auc_score((Y_true==j).astype('int'), Y_pred_proba_geral[test])\n        roc_temporario_ = roc_temporario_ \/ n_classes\n        auc_ = roc_temporario_\n      else:\n        auc_ = roc_auc_score(Y[test], clf.predict_proba(X[test])[:, 1])\n\n\n      s = s + 'AUC Ev. score: {:.3}\\n'.format(auc_)\n      s = s + '###########################\\n'\n\n    best_auc_scores_holdout.append(auc_)\n\n\n    if (writefolder != None):\n      plt.plot(params_scores, 's-', label='fold {}'.format(i+1))\n      plt.plot([0,len(params_scores)], [auc_, auc_], label='auc fold {}: {:.3}'.format(i+1, auc_))\n\n  \n  file_ = open(writefolder+'\/'+'report_nested_cross_validation_hyperparameter_tuning.txt', 'w')\n  file_.write(s)\n  file_.close()\n\n  if (writefolder != None):\n    plt.legend(loc=\"lower right\")\n    plt.savefig(writefolder+'\/'+'nested_cross_validation_scores.png', dpi=100)\n  \n\n  return best_params_all, best_auc_scores_holdout\n","a9d0c7a0":"\n# you can test combinations here\nfeatures = [\n  #'Patient age quantile',\n  'SARS-Cov-2 exam result',\n  #'Hematocrit', #correlacao 1 com hemoglob\n  #'Platelets', # good\n  #'Mean platelet volume ', # good\n  'Leukocytes', # amazing!\n  'Basophils',\n  'Eosinophils', #good\n  'Monocytes', #good\n  #'Rhinovirus_Enterovirus',\n  #'Proteina C reativa mg_dL', #good\n  #'Creatinine', # good\n]\n\nyname = 'SARS-Cov-2 exam result'","1f1f6202":"# Choose one\n#resampling = 'smote'\nresampling = 'undersampling'\n#resampling = None\n#\n\nfile_ = '\/kaggle\/input\/covid19\/dataset.xlsx'\nwork_folder = '\/kaggle\/working'\n\ndf = pd.read_excel(file_)[features]","268e0228":"# took this code block https:\/\/www.kaggle.com\/ossamum\/exploratory-data-analysis-and-feature-importance\n# changed a little bit\nfull_null_series = (df.isnull().sum() == df.shape[0])\nfull_null_columns = full_null_series[full_null_series == True].index\nprint(full_null_columns.tolist())\ndf.drop(full_null_columns, axis=1, inplace=True)\n\ncontain_null_series = (df.isnull().sum() != 0).index\ntarget = yname\njust_one_target = []\nfor col in contain_null_series:\n    i = df[df[col].notnull()][target].nunique()\n    if i == 1:\n        just_one_target.append(col)    \n# columns that only are present when covid is negative        \nprint(just_one_target)\nfor col in just_one_target:\n    print(df[df[col].notnull()][target].unique())\n\ndf.drop(just_one_target, axis=1, inplace=True)\n\n# dataprep categorical\nmask_pos_neg = {'positive': 1, 'negative': 0}\nmask_detected = {'detected': 1, 'not_detected': 0}\nmask_notdone_absent_present = {'not_done': np.nan, 'absent': 0, 'present': 1}\nmask_normal = {'normal': 1}\nmask_urine_color = {'light_yellow': 1, 'yellow': 2, 'citrus_yellow': 3, 'orange': 4}\nmask_urine_aspect = {'clear': 1, 'lightly_cloudy': 2, 'cloudy': 3, 'altered_coloring': 4}\nmask_realizado = {'N\u00e3o Realizado': 0}\nmask_urine_leuk = {'<1000': 0}\nmask_urine_crys = {'Ausentes': 1, 'Urato Amorfo --+': 0, 'Oxalato de C\u00e1lcio +++': 0,\n                   'Oxalato de C\u00e1lcio -++': 0, 'Urato Amorfo +++': 0}\n#df = df.replace(mask_detected)\ndf = df.replace(mask_pos_neg)\n#df = df.replace(mask_notdone_absent_present)\n#df = df.replace(mask_normal)\n#df = df.replace(mask_realizado)\n#df = df.replace(mask_urine_leuk)\n#df = df.replace(mask_urine_color)\n#df = df.replace(mask_urine_aspect)\n#df = df.replace(mask_urine_crys)\n#df['Urine - pH'] = df['Urine - pH'].astype('float')\n#df['Urine - Leukocytes'] = df['Urine - Leukocytes'].astype('float')\n\n#x = df.drop(['Patient ID', 'SARS-Cov-2 exam result'], axis=1)\n#x.fillna(999999, inplace=True)\n#y = df['SARS-Cov-2 exam result']\n###","10909774":"# I'll skip the EDA stage. There is a lot of good notebooks about this out there.\ndf","aed6ae34":"# randomize here\ndf = df.sample(frac=1, random_state=int(time.time()))\ndf = df.reset_index(drop=True)\n\n# I'd like to use data imputation here but in this dataset it is very dangerous. Let's stick to the dropna().\ndf = df.dropna()\n\nreg_y0 = df[df[yname]==0]\nreg_y1 = df[df[yname]==1]\n\nprint ('Records with y==1:{}'.format(len(reg_y1)))\nprint ('Records with y==0:{}'.format(len(reg_y0)))\n\nif resampling == 'undersampling':\n  rus = RandomUnderSampler()\n  X_resampled, y_resampled = rus.fit_resample(df, df[yname].values)\n  df = pd.DataFrame(X_resampled, columns=df.columns)\n\n  resampling_par = None # ugly thing but I do not want to another routine to resample the data\n    \n  print ('After undersampling:')\n  reg_y0 = df[df[yname]==0]\n  reg_y1 = df[df[yname]==1]\n  print ('Records with y==1:{}'.format(len(reg_y1)))\n  print ('Records with y==0:{}'.format(len(reg_y0)))\nelif resampling == 'smote':\n  print ('The smote procedure will be take place inside cros_val function.')","dc98df70":"# standardize \ndf2 = (df-df.mean())\/df.std()\n\n# data standard\ndf_X = df2.drop([yname], axis=1)\ndf_Y = np.rint(df.copy()[yname])\n\n","11253130":"# Here I'll explore a few different hyperparameters due to the cpu processing limits, in my computer I run usually thousands.\n# Look at output folder for the complete report file\nlist_best_models, auc_scores = grid_search_nested_parallel(df_X.values, df_Y.values, cv=3, writefolder=work_folder, n_jobs=1, resampling=resampling_par)","36120ba3":"lst_features = list(df.columns)\nlst_features.remove(yname)\nreg_y0 = df[df[yname]==0]\nreg_y1 = df[df[yname]==1]\n\nprint ('Features: ' + ', '.join(lst_features))\nprint ('registros y==1:{}, y==0:{}'.format(len(reg_y0), len(reg_y1)))\nprint ('resampling: {}'.format(str(resampling)))\nfor model, auc in zip(list_best_models, auc_scores):\n  print ('model:{}, AUC score holdout:{}'.format(model, auc))\n","f8a942eb":"# And now?\n\nNow you can test whatever feature combination and see the nested cross-validation results.\nCheck a bunch of results I compiled in input folder, file nested_results.txt. There is some very interesting results, for example using just the 'Leukocytes' feature the results are around 0.75 AUC, amazing! We must to talk to a specialized professional about this because this feature can predict well this specific population (during the pandemic), if the idea is to generalize even further it is good to get tips for other features.\n\nThis routine may encourage a bunch of tests\/combinations like in nested_results.txt. But you cannot just pick the best. Well, you can pick the best (although now its 3 numbers scores) but you do not know anymore the real performance, you will need another hold-out data and you dont have anymore.\n\nYou can chose some combinations working with TOGETHER WITH THE SPECIALIZED PROFFESIONAL, using theoretical hypothesis and other justifications.\n\n# And if I want to greedy explore all the feature combinations?\n\nYou can do this! But you have to put these combinations inside the nested cross-validation. Then for each feature combination you will have to run all the parameters combination. \n\nThere is another problem here too, if different features make the dataset size to vary, the comparison is complicated.","bc7ff4c4":"I'm going to use my last notebook about prior selection of features:\n\nhttps:\/\/www.kaggle.com\/attilalr\/covid19-selecting-features-using-univariate-graphs\n\nThis procedure checks if a feature is importante (is correlated) to the output variable. A feature can be uncorrelated, noisy and can be discarded due to lack of data. Then, it is usual to discard a good potential feature due to the lack of data.\n","ff5c634a":"This routine is for task 1 analysis. For task 2 a similar one can be made.","0dbd1920":"# Let's use nested cross-validation\n\nWe will have to work with a small due to the amount of missing data.\nMost of the work is to decide which of the features are good enough AND have less possible missing data.\nWith a few records it is very important to perform a Nested cross-validation if you are going to select a model, hyperparameter tuning... etc..\n\nProblems with the nested cross-validation: \n* You wil perform much more training, like in cross validation but its worse.\n* To my knowledge there is no implementation of nested cross validation with data augmentation in the mainstream libraries.\n* This is huge, when doing data augmentation like smote you can only generate sintetic data when training, never in testing.\n\nThen I'm gonna use my implementation, but, BEWARE of ugly, hard-coded and potential WRONG code. You'll not unsee anymore.\n\nThe nested cross-validation.\nWhat the nested cross-validation do?\nYou know when you want to select a model, do a hyperparameter tuning? You usually do a cross-validation (with k2 folds, I'm gonna call k2 here) and select the model with best metric. Well, you can do this but to check the real performance of your model you must test the winner model against an unseen holdout data.\nYou can then use 20% of your data to use as holdout data and redo the same procedure with the rest 80%. Now you have a winner and you can test with the holdout to check the final performance of your winner.\n\n# BUT!\n\nYou chose some 20% of you data as holdout. But if you chose another 20% part of your data, the winner model will be the same?\nThe answer is usually no. In the 20% case, the number of possible folds to repeat the procedure is k1=5. Then you can have k1=5 different best models for each fold.\n\n# What bugs people about nested cross-validation?\n\n* People sometimes do not like nested cross-validation due to the fact that it will output k1 best models. People usually want 1 best model. Sometimes they want to take the best of the best of the k1 models. But it is WRONG. If the best of the best of the k1 models are really the best why it was not the best on all k1 folds?\n\n# What you are gonna do with k1 models then? \nI don't know, combine\/ensemble then. Or you can use other types of justification using theoretical hypothesis or metric characteristics of one model in all folds.\n\nPeople sometimes asks 'why then there is all those papers in machine learning, deep learning and etc.. which do not use nested cross-validation'? Well it because the necessity for the nested cross-validation raises as the amount of data reduces. The necessity is related to the question if your test\/holdout data is a good representative of your population. This is why people on deep learning which have bazillions of records sometimes just do the training\/test procedure (and the training takes long time). "}}