{"cell_type":{"2442613e":"code","001598b8":"code","2dd1e325":"code","e92b9985":"code","54babced":"code","65497daf":"code","0954d0ed":"code","248ad85e":"code","ceb05e1a":"code","7f70c266":"code","08411cae":"code","5a8332f6":"code","7837bf71":"code","87186c77":"code","570e2a91":"code","0ed9276f":"code","5cb8344a":"code","b8830a40":"code","b3c8f82e":"code","e3165e51":"code","fad861e6":"code","6f4bb2fc":"code","aad7dca7":"code","6f292059":"code","371ea4c0":"code","af196036":"code","559341df":"code","16635ac8":"code","0d963a13":"code","89803009":"code","4af0bc74":"code","ef7c8e78":"code","18be1dab":"code","6d766e9a":"code","66da6888":"code","e1f5fa5e":"code","8177dae2":"code","144abdde":"code","3044591d":"code","819f6d4f":"code","ad5df356":"code","4b8dec1f":"code","b3766c8b":"code","2bb68730":"code","eaee6e58":"code","10901fbe":"code","97174545":"code","447cf7c8":"code","d0343c94":"code","a11bf467":"code","abb35547":"code","b1f6f548":"code","d077ddba":"code","3265ec97":"code","37df2e2a":"code","ba62b89e":"code","560c0c86":"code","696b87cd":"code","935080d2":"code","509fd02c":"code","512813a0":"code","d6fa0f5e":"code","7c1421d6":"code","44e6a138":"code","e9fcc289":"code","00b8090c":"code","9875fae4":"code","d17fda8a":"code","e00390c2":"code","006c56c1":"code","0441c3c4":"code","7af2fe31":"code","196beffb":"code","25c03c3b":"code","51453ca3":"code","c2daf7c2":"code","1ef02e4b":"code","ba057925":"code","87e33e00":"code","88ef6941":"code","d237bb2e":"code","9d1035a4":"code","6647f244":"code","bad32df0":"code","e473e40f":"code","1377c5d2":"code","c0fa58e7":"code","4eeac6b2":"code","76b159bc":"code","3e96d2cf":"markdown","824815b8":"markdown","76df2c50":"markdown","1af09ea5":"markdown","25b6f219":"markdown","fd9e3639":"markdown","c245a967":"markdown","89cb0e69":"markdown","22a52d01":"markdown","827008bc":"markdown","d8a102f5":"markdown","08c6605e":"markdown","c03e713c":"markdown","e693f22a":"markdown"},"source":{"2442613e":"#importing all the modules that might be used later on\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.snowball import SnowballStemmer\n\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os","001598b8":"df = pd.read_csv('..\/input\/amazon-fine-food-reviews\/Reviews.csv')","2dd1e325":"print(df.shape)\ndf.head()","e92b9985":"df = df.drop_duplicates(subset = {'UserId', 'Time', 'Text'})\ndf.shape","54babced":"df = df.loc[lambda df: df['Score'] != 3]\nprint(df.shape)\ndf['Score'].unique()","65497daf":"def scorer(x):\n    if x > 3:\n        return 1\n    return 0\n\nscores = df['Score']\nscores_binary = scores.map(scorer)\ndf['Score'] = scores_binary\ndf['Score'].unique()","0954d0ed":"df = df[df.HelpfulnessDenominator >= df.HelpfulnessNumerator]\ndf.shape","248ad85e":"df = df.sort_values('Time', axis = 0, inplace = False, ascending = True)\ntexts = df['Text']\ntexts.head()","ceb05e1a":"# removing all the url from the text\ndef remove_url(s):\n  return re.sub(r'http\\S+', '', s)\ntest = \"hello https:\/\/www.google.com\/ world\"\nprint(remove_url(test))\ntexts = texts.map(remove_url)","7f70c266":"# removing all the tags from the text\ndef remove_tag(s):\n  return re.sub(r'<.*?>', ' ', s)\ntest = \"<p> hello world <\/p>\"\nprint(remove_tag(test))\ntexts = texts.map(remove_tag)","08411cae":"#converting strings into only lowercase.\ndef lower_words(s):\n   return s.lower()\ntest = \"HELLO world\"\nprint(lower_words(test))\ntexts = texts.map(lower_words)","5a8332f6":"# decontracting all contracted words\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\ntest = \"it'll take time to complete this notebook\"\nprint(decontracted(test))\ntexts = texts.map(decontracted)","7837bf71":"# deleting all the words with numbers in them\n\ndef remove_words_with_nums(s):\n  return re.sub(r\"\\S*\\d\\S*\", \"\", s)\ntest = \"hello0 world\"\nprint(remove_words_with_nums(test))\ntexts = texts.map(remove_words_with_nums)","87186c77":"# deleting words with special character in them\n\ndef remove_special_character(s):\n  return re.sub('[^A-Za-z0-9]+', ' ', s)\ntest = \"hello-world\"\nprint(remove_special_character(test))\ntexts = texts.map(remove_special_character)","570e2a91":"# defining the set of stop words according to our problem basically we'll remove all the negations from the pre-defined set of stopwords\n# i removed some stopwords from basic english language stopwords set, the removed elements are related to negations that generally express a negative emotion\n\nstopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren'])","0ed9276f":"def remove_stopword(s):\n    res = ' '.join([word for word in s.split(' ') if word not in stopwords])\n    return res\n\ntest = \"hello my world\"\nprint(remove_stopword(test))\ntexts = texts.map(remove_stopword)","5cb8344a":"test_texts = texts[:10000:].copy()\ntest_texts.shape","b8830a40":"#Stemming (Snowball Stemmer)\nstemmer = SnowballStemmer('english')\ndef stemming(s):\n    res = ' '.join([stemmer.stem(word) for word in s.split(' ')])\n    return res\ntest = \"running and walking\"\nprint(stemming(test))\nstemmed_texts = test_texts.map(stemming)","b3c8f82e":"lemmatizer = WordNetLemmatizer()\ndef lemmatization(s):\n    res = ' '.join([lemmatizer.lemmatize(word) for word in s.split(' ')])\n    return res\ntest = \"was running\"\nlemmatized_texts = test_texts.map(lemmatization)\nprint(lemmatization(test))","e3165e51":"#texts\nX = test_texts\ny = df['Score'][:10000:].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\ncount_vect = CountVectorizer()\nX_train = count_vect.fit_transform(X_train)\nX_test = count_vect.transform(X_test)\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import roc_auc_score\npredictions = model.predict(X_test)\nprint('AUC: ', roc_auc_score(y_test, predictions))","fad861e6":"#texts\nX = stemmed_texts\ny = df['Score'][:10000:].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\ncount_vect = CountVectorizer()\nX_train = count_vect.fit_transform(X_train)\nX_test = count_vect.transform(X_test)\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import roc_auc_score\npredictions = model.predict(X_test)\nprint('AUC: ', roc_auc_score(y_test, predictions))","6f4bb2fc":"#texts\nX = lemmatized_texts\ny = df['Score'][:10000:].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\ncount_vect = CountVectorizer()\nX_train = count_vect.fit_transform(X_train)\nX_test = count_vect.transform(X_test)\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import roc_auc_score\npredictions = model.predict(X_test)\nprint('AUC: ', roc_auc_score(y_test, predictions))","aad7dca7":"text = texts[:100000:]\nfrom nltk.stem import PorterStemmer\nst = PorterStemmer()\nstemmed_data = []\nfor review in text:\n    stemmed_data.append(st.stem(review))\nprint('Done')","6f292059":"X = stemmed_data\ny = df['Score'][:100000:].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\ncount_vect = CountVectorizer()\ntrain_bow = count_vect.fit_transform(X_train)\ntest_bow = count_vect.transform(X_test)\nprint(train_bow.shape)","371ea4c0":"c_dist = []\nfor x in range(-2, 3):\n    mul = 10 ** (-x + 1)\n    center = 10 ** x\n    for y in range(-5,6):\n        c_dist.append(y\/mul + center)\nprint(c_dist)\nmax_iter = []\nfor x in range (75, 130, 5):\n    max_iter.append(x)\nprint(max_iter)\nparam_dist = {'C' : c_dist, 'max_iter' : max_iter}\nprint(param_dist)","af196036":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nrandom_model = RandomizedSearchCV(LogisticRegression(class_weight='balanced', penalty='l1'), param_dist, cv = 10, scoring = 'accuracy', n_jobs=-1)\nrandom_model.fit(train_bow, y_train)","559341df":"print(random_model.best_estimator_)\npred = random_model.predict(test_bow)\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy :', accuracy_score(y_test, pred)*100)","16635ac8":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,pred))","0d963a13":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nconfusion = confusion_matrix(y_test , pred)\nprint(confusion)\ndf_cm = pd.DataFrame(confusion , index = ['Negative','Positive'])\nsns.heatmap(df_cm ,annot = True)\nplt.xticks([0.5,1.5],['Negative','Positive'],rotation = 45)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n","89803009":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nrandom_model = RandomizedSearchCV(LogisticRegression(class_weight='balanced', penalty='l2'), param_dist, cv = 10, scoring = 'accuracy', n_jobs=-1)\nrandom_model.fit(train_bow, y_train)","4af0bc74":"print(random_model.best_estimator_)\npred = random_model.predict(test_bow)\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy :', accuracy_score(y_test, pred)*100)","ef7c8e78":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,pred))","18be1dab":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nconfusion = confusion_matrix(y_test , pred)\nprint(confusion)\ndf_cm = pd.DataFrame(confusion , index = ['Negative','Positive'])\nsns.heatmap(df_cm ,annot = True)\nplt.xticks([0.5,1.5],['Negative','Positive'],rotation = 45)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","6d766e9a":"count_vect = CountVectorizer(ngram_range = (1, 2))\ntrain_bow = count_vect.fit_transform(X_train)\ntest_bow = count_vect.transform(X_test)\nprint(train_bow.shape)","66da6888":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nrandom_model = RandomizedSearchCV(LogisticRegression(class_weight='balanced', penalty='l1'), param_dist, cv = 10, scoring = 'accuracy', n_jobs=-1)\nrandom_model.fit(train_bow, y_train)","e1f5fa5e":"print(random_model.best_estimator_)\npred = random_model.predict(test_bow)\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy :', accuracy_score(y_test, pred)*100)","8177dae2":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,pred))","144abdde":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nconfusion = confusion_matrix(y_test , pred)\nprint(confusion)\ndf_cm = pd.DataFrame(confusion , index = ['Negative','Positive'])\nsns.heatmap(df_cm ,annot = True)\nplt.xticks([0.5,1.5],['Negative','Positive'],rotation = 45)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","3044591d":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nrandom_model = RandomizedSearchCV(LogisticRegression(class_weight='balanced', penalty='l2'), param_dist, cv = 10, scoring = 'accuracy', n_jobs=-1)\nrandom_model.fit(train_bow, y_train)","819f6d4f":"print(random_model.best_estimator_)\npred = random_model.predict(test_bow)\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy :', accuracy_score(y_test, pred)*100)","ad5df356":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,pred))","4b8dec1f":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nconfusion = confusion_matrix(y_test , pred)\nprint(confusion)\ndf_cm = pd.DataFrame(confusion , index = ['Negative','Positive'])\nsns.heatmap(df_cm ,annot = True)\nplt.xticks([0.5,1.5],['Negative','Positive'],rotation = 45)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","b3766c8b":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","2bb68730":"tfidf_vect = TfidfVectorizer()\ntrain_tfidf = tfidf_vect.fit_transform(X_train)\ntest_tfidf = tfidf_vect.transform(X_test)\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler(with_mean = False)\nx_train = sc.fit_transform(train_tfidf)\nx_test = sc.transform(test_tfidf)","eaee6e58":"c_dist = []\nfor x in range(-2, 3):\n    mul = 10 ** (-x + 1)\n    center = 10 ** x\n    for y in range(-5,6):\n        c_dist.append(y\/mul + center)\nprint(c_dist)\nmax_iter = []\nfor x in range (75, 130, 5):\n    max_iter.append(x)\nprint(max_iter)\nparam_dist = {'C' : c_dist, 'max_iter' : max_iter}\nprint(param_dist)","10901fbe":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nrandom_model = RandomizedSearchCV(LogisticRegression(class_weight='balanced', penalty='l1'), param_dist, cv = 10, scoring = 'accuracy', n_jobs=-1)\nrandom_model.fit(x_train, y_train)","97174545":"print(random_model.best_estimator_)\npred = random_model.predict(x_test)\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy :', accuracy_score(y_test, pred)*100)","447cf7c8":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,pred))","d0343c94":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nconfusion = confusion_matrix(y_test , pred)\nprint(confusion)\ndf_cm = pd.DataFrame(confusion , index = ['Negative','Positive'])\nsns.heatmap(df_cm ,annot = True)\nplt.xticks([0.5,1.5],['Negative','Positive'],rotation = 45)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","a11bf467":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nrandom_model = RandomizedSearchCV(LogisticRegression(class_weight='balanced', penalty='l2'), param_dist, cv = 10, scoring = 'accuracy', n_jobs=-1)\nrandom_model.fit(train_tfidf, y_train)","abb35547":"print(random_model.best_estimator_)\npred = random_model.predict(test_tfidf)\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy :', accuracy_score(y_test, pred)*100)","b1f6f548":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,pred))","d077ddba":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nconfusion = confusion_matrix(y_test , pred)\nprint(confusion)\ndf_cm = pd.DataFrame(confusion , index = ['Negative','Positive'])\nsns.heatmap(df_cm ,annot = True)\nplt.xticks([0.5,1.5],['Negative','Positive'],rotation = 45)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","3265ec97":"tfidf_vect = TfidfVectorizer(ngram_range=(1,2))\ntrain_tfidf = tfidf_vect.fit_transform(X_train)\ntest_tfidf = tfidf_vect.transform(X_test)\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler(with_mean = False)\nx_train = sc.fit_transform(train_tfidf)\nx_test = sc.transform(test_tfidf)","37df2e2a":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nrandom_model = RandomizedSearchCV(LogisticRegression(class_weight='balanced', penalty='l1'), param_dist, cv = 10, scoring = 'accuracy', n_jobs=-1)\nrandom_model.fit(x_train, y_train)","ba62b89e":"print(random_model.best_estimator_)\npred = random_model.predict(x_test)\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy :', accuracy_score(y_test, pred)*100)","560c0c86":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,pred))","696b87cd":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nconfusion = confusion_matrix(y_test , pred)\nprint(confusion)\ndf_cm = pd.DataFrame(confusion , index = ['Negative','Positive'])\nsns.heatmap(df_cm ,annot = True)\nplt.xticks([0.5,1.5],['Negative','Positive'],rotation = 45)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","935080d2":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nrandom_model = RandomizedSearchCV(LogisticRegression(class_weight='balanced', penalty='l2'), param_dist, cv = 10, scoring = 'accuracy', n_jobs=-1)\nrandom_model.fit(x_train, y_train)","509fd02c":"print(random_model.best_estimator_)\npred = random_model.predict(x_test)\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy :', accuracy_score(y_test, pred)*100)","512813a0":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,pred))","d6fa0f5e":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nconfusion = confusion_matrix(y_test , pred)\nprint(confusion)\ndf_cm = pd.DataFrame(confusion , index = ['Negative','Positive'])\nsns.heatmap(df_cm ,annot = True)\nplt.xticks([0.5,1.5],['Negative','Positive'],rotation = 45)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","7c1421d6":"list_of_sent_train = []\nfor i in X_train:\n    sent = []\n    for word in i.split():\n        sent.append(word)\n    list_of_sent_train.append(sent)","44e6a138":"from gensim.models import Word2Vec\nw2v_model = Word2Vec(list_of_sent_train,min_count = 5,size = 50,workers = 4)\nsent_vectors_train = []\nfor sent in list_of_sent_train:\n    sent_vec = np.zeros(50)\n    cnt_word = 0\n    for word in sent:\n        try:\n            vec = w2v_model.wv[word]\n            sent_vec += vec\n            cnt_word += 1\n        except:\n            pass\n    sent_vec \/= cnt_word\n    sent_vectors_train.append(sent_vec)\nprint(len(sent_vectors_train))","e9fcc289":"list_of_sent_test = []\nfor i in X_test:\n    sent = []\n    for word in i.split():\n        sent.append(word)\n    list_of_sent_test.append(sent)","00b8090c":"import warnings\nwarnings.filterwarnings(\"ignore\")\nfrom gensim.models import Word2Vec\nw2v_model = Word2Vec(list_of_sent_test,min_count = 5,size = 50,workers = 4)\nsent_vectors_test = []\nfor sent in list_of_sent_test:\n    sent_vec = np.zeros(50)\n    cnt_word = 0\n    for word in sent:\n        try:\n            vec = w2v_model.wv[word]\n            sent_vec += vec\n            cnt_word += 1\n        except:\n            pass\n    sent_vec \/= cnt_word\n    sent_vectors_test.append(sent_vec)\nprint(len(sent_vectors_test))","9875fae4":"np.where(np.isnan(sent_vectors_test))","d17fda8a":"sent_vectors_train = pd.DataFrame(sent_vectors_train)\nsent_vectors_test = pd.DataFrame(sent_vectors_test)\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\nimputer = imputer.fit(sent_vectors_train)\nsent_vectors_train = imputer.transform(sent_vectors_train)\nsent_vectors_test = imputer.transform(sent_vectors_test)","e00390c2":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nw2v_train = sc.fit_transform(sent_vectors_train)\nw2v_test = sc.transform(sent_vectors_test)\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nrandom_model = RandomizedSearchCV(LogisticRegression(class_weight='balanced', penalty='l1'), param_dist, cv = 10, scoring = 'accuracy', n_jobs=-1)\nrandom_model.fit(w2v_train, y_train)","006c56c1":"print(random_model.best_estimator_)\npred = random_model.predict(w2v_test)\nfrom sklearn.metrics import accuracy_score\nacc = accuracy_score(y_test,pred)\nprint('accuracy is',acc*100)","0441c3c4":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,pred))","7af2fe31":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nconfusion = confusion_matrix(y_test , pred)\nprint(confusion)\ndf_cm = pd.DataFrame(confusion , index = ['Negative','Positive'])\nsns.heatmap(df_cm ,annot = True)\nplt.xticks([0.5,1.5],['Negative','Positive'],rotation = 45)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","196beffb":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nw2v_train = sc.fit_transform(sent_vectors_train)\nw2v_test = sc.transform(sent_vectors_test)\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nrandom_model = RandomizedSearchCV(LogisticRegression(class_weight='balanced', penalty='l2'), param_dist, cv = 10, scoring = 'accuracy', n_jobs=-1)\nrandom_model.fit(w2v_train, y_train)","25c03c3b":"print(random_model.best_estimator_)\npred = random_model.predict(w2v_test)\nfrom sklearn.metrics import accuracy_score\nacc = accuracy_score(y_test,pred)\nprint('accuracy is',acc*100)","51453ca3":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,pred))","c2daf7c2":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nconfusion = confusion_matrix(y_test , pred)\nprint(confusion)\ndf_cm = pd.DataFrame(confusion , index = ['Negative','Positive'])\nsns.heatmap(df_cm ,annot = True)\nplt.xticks([0.5,1.5],['Negative','Positive'],rotation = 45)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","1ef02e4b":"tfidf_vect = TfidfVectorizer()\ntrain_tfidf = tfidf_vect.fit_transform(X_train)\ntest_tfidf = tfidf_vect.transform(X_test)","ba057925":"tf_idf_feat = tfidf_vect.get_feature_names()\ntfidf_sent_vec_train = []\nrow = 0\nw2v_model = Word2Vec(list_of_sent_train,min_count = 5,size = 50,workers = 4)\nfor sent in list_of_sent_train:\n    sent_vec = np.zeros(50)\n    weight_sum = 0\n    for word in sent:\n        try:\n            vec = w2v_model.wv[word]\n            tfidf = train_tfidf[row,tf_idf_feat.index(word)]\n            sent_vec += (vec*tfidf)\n            weight_sum += tfidf\n        except:\n            pass\n    sent_vec\/= weight_sum\n    tfidf_sent_vec_train.append(sent_vec)\n    row += 1","87e33e00":"tf_idf_feat = tfidf_vect.get_feature_names()\ntfidf_sent_vec_test = []\nrow = 0\nw2v_model = Word2Vec(list_of_sent_test,min_count = 5,size = 50,workers = 4)\nfor sent in list_of_sent_test:\n    sent_vec = np.zeros(50)\n    weight_sum = 0\n    for word in sent:\n        try:\n            vec = w2v_model.wv[word]\n            tfidf = test_tfidf[row,tf_idf_feat.index(word)]\n            sent_vec += (vec*tfidf)\n            weight_sum += tfidf\n        except:\n            pass\n    sent_vec\/= weight_sum\n    tfidf_sent_vec_test.append(sent_vec)\n    row += 1","88ef6941":"np.where(np.isnan(tfidf_sent_vec_train))","d237bb2e":"tfidf_sent_vec_train = pd.DataFrame(tfidf_sent_vec_train)\ntfidf_sent_vec_test = pd.DataFrame(tfidf_sent_vec_test)\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\nimputer = imputer.fit(tfidf_sent_vec_train)\ntfidf_sent_vec_train = imputer.transform(tfidf_sent_vec_train)\ntfidf_sent_vec_test = imputer.transform(tfidf_sent_vec_test)","9d1035a4":"sc =  StandardScaler()\ntfidf_w2v_train = sc.fit_transform(tfidf_sent_vec_train)\ntfidf_w2v_test = sc.transform(tfidf_sent_vec_test)\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nrandom_model = RandomizedSearchCV(LogisticRegression(class_weight='balanced', penalty='l1'), param_dist, cv = 10, scoring = 'accuracy', n_jobs=-1)\nrandom_model.fit(tfidf_w2v_train, y_train)","6647f244":"from sklearn.metrics import accuracy_score\nprint(random_model.best_estimator_)\npred = random_model.predict(tfidf_w2v_test)\nacc = accuracy_score(y_test,pred)\nprint('accuracy is',acc*100)","bad32df0":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,pred))","e473e40f":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nconfusion = confusion_matrix(y_test , pred)\nprint(confusion)\ndf_cm = pd.DataFrame(confusion , index = ['Negative','Positive'])\nsns.heatmap(df_cm ,annot = True)\nplt.xticks([0.5,1.5],['Negative','Positive'],rotation = 45)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","1377c5d2":"sc =  StandardScaler()\ntfidf_w2v_train = sc.fit_transform(tfidf_sent_vec_train)\ntfidf_w2v_test = sc.transform(tfidf_sent_vec_test)\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nrandom_model = RandomizedSearchCV(LogisticRegression(class_weight='balanced', penalty='l2'), param_dist, cv = 10, scoring = 'accuracy', n_jobs=-1)\nrandom_model.fit(tfidf_w2v_train, y_train)","c0fa58e7":"from sklearn.metrics import accuracy_score\nprint(random_model.best_estimator_)\npred = random_model.predict(tfidf_w2v_test)\nacc = accuracy_score(y_test,pred)\nprint('accuracy is',acc*100)","4eeac6b2":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,pred))","76b159bc":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nconfusion = confusion_matrix(y_test , pred)\nprint(confusion)\ndf_cm = pd.DataFrame(confusion , index = ['Negative','Positive'])\nsns.heatmap(df_cm ,annot = True)\nplt.xticks([0.5,1.5],['Negative','Positive'],rotation = 45)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","3e96d2cf":"Now I'll replace score greater than 3 with 1 to signify a good review and score less than 3 with 0 to signify a bad review","824815b8":"Since we want to simply classify a review as either good or bad, we will remove the rows with score 3 because it signifies the review is neither good nor bad","76df2c50":"tfidf w2v","1af09ea5":"We have two options when it comes to reducing the words to their root form, stemming and lemmatization, none of the two techinque is better than other so I would apply both of them and then see which one of them provides better result and will use that one for future reference.","25b6f219":"For the basic comparison i'll apply bag of words model on texts, stemmed_texts, and lemmatized_texts and i'll continue with the one that provides best accuracy","fd9e3639":"AVG W2VEC","c245a967":"Text is the major factor while deciding score, so I'll seprate out texts from the dataframe","89cb0e69":"**Things to do in future**\n* Multicollinearity Check    ","22a52d01":"Since the extracted text would have things like numbers, url etc the things that arent really important for the decision making so i'll remove them.","827008bc":"Lets Begin by deleting the reviews that are same across various products of same type i.e reviews that are shared amongst different products","d8a102f5":"Now I'll remove rows in which helpfulness numerator is greater than helpfulness denominator because thats not possible and if that is happening it means that it is wrong observation","08c6605e":"**Amazon Fine Food: Preprocessing and classification using logistic Regression**\n\nAccuracy: 94%\n\nConfusion Matrix: [[ 2704, 980]\n                   [812, 25504]]","c03e713c":"Tfidf","e693f22a":"We'll continue with the stemmed version as it performs similar to othe ones but will provide us with smaller vectors"}}