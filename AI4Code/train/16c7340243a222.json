{"cell_type":{"83036e64":"code","e723ab12":"code","53130781":"code","6e5dce86":"code","e3d8d338":"code","a923da1c":"code","c0436699":"code","273e4f12":"code","ef1dc101":"code","e45d3fda":"code","156ff0b4":"code","612f9f0a":"code","170bd67f":"code","60b1901a":"code","3c64043f":"code","784c769b":"code","325309b5":"code","d47bc774":"code","b5d48b25":"code","7c018562":"code","1f6cc844":"code","c24f048b":"code","5b1b94c9":"code","bbf50b78":"code","f190c7c8":"markdown","0074f310":"markdown","1f4b3598":"markdown","a8fa1b88":"markdown","7a9f2190":"markdown","4551f3a1":"markdown","97115081":"markdown","edd2c6f2":"markdown","7b518090":"markdown","fb5c806d":"markdown","f6f04126":"markdown","00075d6d":"markdown","2a203950":"markdown","fe39fc6a":"markdown","310c9df1":"markdown","88ca8d1d":"markdown","078cd973":"markdown","8c933b50":"markdown","e5d35ddc":"markdown","79ba8c1b":"markdown","5dc14370":"markdown","5023f57c":"markdown","8227d11f":"markdown","1ddf7411":"markdown","b760ab76":"markdown"},"source":{"83036e64":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","e723ab12":"full_data = pd.read_csv('\/kaggle\/input\/faults.csv')","53130781":"print(full_data.shape)\nprint(\"Number of rows: \"+str(full_data.shape[0]))\nprint(\"Number of columns: \"+str(full_data.shape[1]))","6e5dce86":"full_data.head()","e3d8d338":"full_data.columns","a923da1c":"full_data.describe().T","c0436699":"fig, ax=plt.subplots(1,2,figsize=(15,6))\n_ = sns.countplot(x='target', data=full_data, ax=ax[0])\n_ = full_data['target'].value_counts().plot.pie(autopct=\"%1.1f%%\", ax=ax[1])","273e4f12":"full_data.hist(figsize=(15,15))\nplt.show()","ef1dc101":"full_data.plot(kind=\"density\", layout=(6,5), \n             subplots=True,sharex=False, sharey=False, figsize=(15,15))\nplt.show()","e45d3fda":"full_data.isnull().sum()","156ff0b4":"full_data.X_Maximum.fillna(full_data.X_Maximum.median(),inplace=True)\nfull_data.Steel_Plate_Thickness.fillna(full_data.Steel_Plate_Thickness.median(),inplace=True)\nfull_data.Empty_Index.fillna(np.mean(full_data.Empty_Index),inplace=True)","612f9f0a":"full_data.isnull().sum()","170bd67f":"def draw_univariate_plot(dataset, rows, cols, plot_type):\n    column_names=dataset.columns.values\n    number_of_column=len(column_names)\n    fig, axarr=plt.subplots(rows,cols, figsize=(30,35))\n\n    counter=0\n    \n    for i in range(rows):\n        for j in range(cols):\n\n            if column_names[counter]=='target':\n                break\n            if 'violin' in plot_type:\n                sns.violinplot(x='target', y=column_names[counter],data=dataset, ax=axarr[i][j])\n            elif 'box'in plot_type :\n                #sns.boxplot(x='target', y=column_names[counter],data=dataset, ax=axarr[i][j])\n                sns.boxplot(x=None, y=column_names[counter],data=dataset, ax=axarr[i][j])\n\n            counter += 1\n            if counter==(number_of_column-1,):\n                break","60b1901a":"draw_univariate_plot(dataset=full_data, rows=7, cols=4,plot_type=\"box\")","3c64043f":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nX=full_data.drop('target',axis=1)\nY=le.fit_transform(full_data['target'])","784c769b":"le.classes_","325309b5":"le.inverse_transform([0,1,2,3,4,5,6])","d47bc774":"dict(zip(le.inverse_transform([0,1,2,3,4,5,6]),[0,1,2,3,4,5,6]))","b5d48b25":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X, Y, stratify=Y, test_size = 0.3,random_state = 42)","7c018562":"def draw_confusion_matrix(cm):\n    plt.figure(figsize=(12,8))\n    sns.heatmap(cm,annot=True,fmt=\"d\", center=0, cmap='autumn') \n    plt.title(\"Confusion Matrix\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","1f6cc844":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nlogreg = LogisticRegression(random_state=42)\nlogreg.fit(X_train, y_train)\n\ny_predict_train_logreg = logreg.predict(X_train)\ny_predict_test_logreg = logreg.predict(X_test)\n\ntrain_accuracy_score_logreg = accuracy_score(y_train, y_predict_train_logreg)\ntest_accuracy_score_logreg = accuracy_score(y_test, y_predict_test_logreg)\n\nprint(train_accuracy_score_logreg)\nprint(test_accuracy_score_logreg)","c24f048b":"cm_logreg = confusion_matrix(y_test,y_predict_test_logreg)\ndraw_confusion_matrix(cm_logreg)","5b1b94c9":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nrf = RandomForestClassifier(random_state=42, n_estimators=50, max_depth=6, criterion = 'entropy', \n                            min_samples_leaf= 1,min_samples_split= 2)\nrf.fit(X_train, y_train)\n\ny_predict_train_rf = rf.predict(X_train)\ny_predict_test_rf = rf.predict(X_test)\n\ntrain_accuracy_score_rf = accuracy_score(y_train, y_predict_train_rf)\ntest_accuracy_score_rf = accuracy_score(y_test, y_predict_test_rf)\n\nprint(train_accuracy_score_rf)\nprint(test_accuracy_score_rf)","bbf50b78":"cm_rf = confusion_matrix(y_test,y_predict_test_rf)\ndraw_confusion_matrix(cm_rf)","f190c7c8":"## Data Preprocessing","0074f310":"## Data Visualisation","1f4b3598":"### Lets try to understand what this data is all about.  ","a8fa1b88":"### Since this is a classification problem it would be important and interesting to the distribution of target variables for the data.","7a9f2190":"### Introduction to Confusion Matrix\n\nThe confusion matrix is a technique used for summarizing the performance of a classification algorithm i.e. it has binary outputs.\n![](https:\/\/cdn-images-1.medium.com\/max\/1600\/0*-GAP6jhtJvt7Bqiv.png)\n\n\n\n### ***In the famous cancer example***:\n\n\n###### Cases in which the doctor predicted YES (they have the disease), and they do have the disease will be termed as TRUE POSITIVES (TP). The doctor has correctly predicted that the patient has the disease.\n\n###### Cases in which the doctor predicted NO (they do not have the disease), and they don\u2019t have the disease will be termed as TRUE NEGATIVES (TN). The doctor has correctly predicted that the patient does not have the disease.\n\n###### Cases in which the doctor predicted YES, and they do not have the disease will be termed as FALSE POSITIVES (FP). Also known as \u201cType I error\u201d.\n\n###### Cases in which the doctor predicted NO, and they have the disease will be termed as FALSE NEGATIVES (FN). Also known as \u201cType II error\u201d.\n\n![](https:\/\/cdn-images-1.medium.com\/max\/1600\/0*9r99oJ2PTRi4gYF_.jpg)","4551f3a1":"### To understand the data better that we are going to deal with we would like to have a look at the basic numerical stats of the data like the mean, maximum etc. columnwise.\n### The .describe() function helps us.","97115081":"### Among the various questions that arise, one crucial question is to ask whether the data contains any missing values? Lets see how we can find the answer to that.\n","edd2c6f2":"#### There are two main ways to fill any missing values:\n\n1. By Mean\n2. By Median\n\nAnd the decision of filling by mean or median is based on the distribution of the data columns individually. Here's when the histogram and density plots come into play. If the distribution of the column is skewed, we choose median to fill the missing values and if the distribution is normal we go for the mean.\n\nLet's look at the term skew in some more detail.","7b518090":"### The first step to any kind of exploration and modelling requires us to load the data file into the environment.\n### The .read_csv() function helps us.","fb5c806d":"### One of the first steps is to gather all possible information about the data and understand the problem statement that we are going to target with the data.","f6f04126":"### By looking at the numbers we can understand that the result is pointing to the number of missing values in each column of the data.","00075d6d":"### Let's check the distribution of data using Histogram and Density visualisation method.","2a203950":"### Let's take a look at the top 5 rows of the data to understand what this file contains.\n### The .head() function helps us.","fe39fc6a":"#### So there are 7 classes of faults in the steel. We can see that the distribution of the classes is greatly disbalanced. 'Other_Faults' class is in majority while 'Dirtiness' class is the minority here.","310c9df1":"## Test Train Split and Cross Validation methods\n\n\n\n***Train Test Split*** : To have unknown datapoints to test the data rather than testing with the same points with which the model was trained. This helps capture the model performance much better.\n\n![](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*-8_kogvwmL1H6ooN1A1tsQ.png)\n\n\n***About Stratify*** : Stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify.\n\nFor example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's.\n\nFor Reference : https:\/\/towardsdatascience.com\/train-test-split-and-cross-validation-in-python-80b61beca4b6","88ca8d1d":"* X_Maximum   - skew\n* Steel_Plate_Thickness - skew\n* Empty_Index - No Skew","078cd973":"#### Since the score of accuracy is so low for training and testing it means that the data is not following any linear trend. We should try non linear machine learning algorithms.","8c933b50":"## Exploratory Data Analysis - EDA","e5d35ddc":"## Skewness\n\nA ***left-skewed distribution*** has a long left tail. Left-skewed distributions are also called negatively-skewed distributions. That\u2019s because there is a long tail in the negative direction on the number line. The mean is also to the left of the peak.\n\nA ***right-skewed distribution*** has a long right tail. Right-skewed distributions are also called positive-skew distributions. That\u2019s because there is a long tail in the positive direction on the number line. The mean is also to the right of the peak.\n\n\n![](https:\/\/www.statisticshowto.datasciencecentral.com\/wp-content\/uploads\/2014\/02\/pearson-mode-skewness.jpg)\n\n\n#### to learn more about skewness\nhttps:\/\/www.statisticshowto.datasciencecentral.com\/probability-and-statistics\/skewed-distribution\/","79ba8c1b":"### As we can see the target variable has text values as the name of classes. When a machine learning algorithm takes input it expects all values to be numerical and can not handle text values directly. It will simply throw an error if text value is fed to the model. Hence we need to replace text value with a number that can represent the class. Label Encoder tool helps us perform the same.","5dc14370":"### We can check the name of all columns in the dataset using the .columns property","5023f57c":"DataFrame.describe() method generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution, excluding NaN values. This method tells us a lot of things about a dataset. One important thing is that the describe() method deals only with numeric values. It doesn't work with any categorical values. So if there are any categorical values in a column the describe() method will ignore it and display summary for the other columns unless parameter include=\"all\" is passed.\n\nNow, let's understand the statistics that are generated by the describe() method:\n\n* count tells us the number of NoN-empty rows in a feature.\n* mean tells us the mean value of that feature.\n* std tells us the Standard Deviation Value of that feature.\n* min tells us the minimum value of that feature.\n* 25%, 50%, and 75% are the percentile\/quartile of each features. This quartile information helps us to detect Outliers.\n* max tells us the maximum value of that feature.","8227d11f":"### Its important to check for missing values and get rid of them because most of the machine learning algorithms have not been designed to handle missing values and whenever row having missing values would go for training or prediction in the algorithm it would raise an error.","1ddf7411":"### Once we know that the data contains missing values, it is important that we fill the missing places. Now we will be looking at some of the majorly used techniques for missing value filling, usually referred to as missing value 'imputation'.","b760ab76":"### Let's Check the dimensions of this data"}}