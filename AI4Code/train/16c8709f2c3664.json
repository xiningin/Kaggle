{"cell_type":{"f0ef1af1":"code","c75ebac6":"code","a5417394":"code","79594c60":"code","ba83e902":"code","9be5a82f":"code","411a8748":"code","84a710c5":"code","22daa945":"code","02d62dae":"code","e6931bc2":"code","bc7fc08f":"code","a8bfc214":"code","aabe9e1d":"code","6b487a44":"code","dd3c077c":"code","643169fd":"code","f224e813":"markdown","e83da227":"markdown","23888cc5":"markdown","61ce4e11":"markdown","b1940140":"markdown","55b3fd58":"markdown","982fe1b2":"markdown","77487fff":"markdown","dcd00cc9":"markdown","513eda93":"markdown","52a35024":"markdown","5d313e8d":"markdown","5eb2ddf9":"markdown"},"source":{"f0ef1af1":"import os\n!cp -r '\/kaggle\/input\/cocoapi' '\/kaggle\/'\nos.chdir('\/kaggle\/cocoapi\/cocoapi\/PythonAPI')\n!make\n!make install\n!python setup.py install","c75ebac6":"!conda install '\/kaggle\/input\/pydicom-conda-helper\/libjpeg-turbo-2.1.0-h7f98852_0.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/libgcc-ng-9.3.0-h2828fa1_19.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/gdcm-2.8.9-py37h500ead1_1.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/conda-4.10.1-py37h89c1867_0.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/certifi-2020.12.5-py37h89c1867_1.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/openssl-1.1.1k-h7f98852_0.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/notebook6765ef7aa0\/webcolors-1.11.1-pyhd8ed1ab_0.tar.bz2' -c conda-forge -y","a5417394":"!cp -r \/kaggle\/input\/efficientdet-training-better-than-yolov5\/Yet-Another-EfficientDet-Pytorch \/kaggle\/","79594c60":"os.chdir('\/kaggle\/Yet-Another-EfficientDet-Pytorch\/')","ba83e902":"import pandas as pd\nfrom glob import glob\n\nif pd.read_csv('\/kaggle\/input\/siim-covid19-detection\/sample_submission.csv').shape[0] == 2477:\n    fast_sub = True\n    \n    INPUT_PATH = 'datasets\/siim\/tmp\/'\n    \n    # move just 5 folders\n    sample = os.listdir('\/kaggle\/input\/siim-covid19-detection\/test\/')[:5]\n    os.makedirs(INPUT_PATH, exist_ok=True)\n    for i in sample:\n        !cp -r '\/kaggle\/input\/siim-covid19-detection\/test\/'{i} {INPUT_PATH}\n    \n    tmp = []\n    for path in glob(INPUT_PATH + '*'):\n        study_name = path.split('\/')[-1][:-4]\n        tmp.append([f'{study_name}_study', 'negative 1 0 0 1 1'])\n\n    for path in glob(INPUT_PATH + '*\/*\/*'):\n        image_name = path.split('\/')[-1][:-4]\n        tmp.append([f'{image_name}_image', 'none 1 0 0 1 1'])\n\n    submission = pd.DataFrame(tmp, columns=['id', 'PredictionString'])\n\nelse:\n    fast_sub = False\n    INPUT_PATH = '\/kaggle\/input\/siim-covid19-detection\/test\/'\n    submission = pd.read_csv('\/kaggle\/input\/siim-covid19-detection\/sample_submission.csv')","9be5a82f":"import os\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\nimport torch\n\ndef read_xray(path, voi_lut=False, fix_monochrome=True):\n    # Original from: https:\/\/www.kaggle.com\/raddar\/convert-dicom-to-np-array-the-correct-way\n    dicom = pydicom.read_file(path)\n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to\n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n\n    return data\n\n    \ndef resize(array, size, keep_ratio=False, resample=Image.LANCZOS):\n    # Original from: https:\/\/www.kaggle.com\/xhlulu\/vinbigdata-process-and-resize-to-image\n    im = Image.fromarray(array)\n\n    if keep_ratio:\n        im.thumbnail((size, size), resample)\n    else:\n        im = im.resize((size, size), resample)\n\n    return im","411a8748":"from glob import glob\n\nsave_dir = \"datasets\/siim\/test\"\n!rm -rf {save_dir}\nos.makedirs(save_dir, exist_ok=True)\n\nimage_size_dict = {}\nfor path in tqdm(glob(INPUT_PATH + '*\/*\/*')):\n    # set keep_ratio=True to have original aspect ratio\n    xray = read_xray(path)\n    image_size_dict[path.split('\/')[-1][:-4]] = xray.shape\n    im = resize(xray, size=256)\n    im.save(os.path.join(save_dir, path.split('\/')[-1][:-3]+'jpg'))","84a710c5":"submission[submission.id.str.contains('image')]","22daa945":"weight_file = !ls logs\/siim\/\n! python coco_eval.py -c 0 -p siim -w \"logs\/siim\/{weight_file[-2]}\"","02d62dae":"name2color = {\n    'Negative' : [0, 0, 0], # Typical Appearance\n    'Typical': [66,9,255], # Typical Appearance\n    'Indeterminate': [255,186,8], # Indeterminate Appearance\n    'Atypical': [247,37,69], # Atypical Appearance\n}\n\n\"\"\"\nSimple Inference Script of EfficientDet-Pytorch\n\"\"\"\nimport time\nimport torch\nfrom torch.backends import cudnn\nfrom matplotlib import colors\n\nfrom backbone import EfficientDetBackbone\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom efficientdet.utils import BBoxTransform, ClipBoxes\nfrom utils.utils import preprocess, invert_affine, postprocess, STANDARD_COLORS, standard_to_bgr, get_index_label, plot_one_box\n","e6931bc2":"compound_coef = 0\nforce_input_size = None  # set None to use default size\nimg_path = 'datasets\/siim\/test\/'\nimg_list = glob(img_path + '*')\n\n# replace this part with your project's anchor config\nanchor_ratios = [(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)]\nanchor_scales = [2 ** 0, 2 ** (1.0 \/ 3.0), 2 ** (2.0 \/ 3.0)]\n\nthreshold = 0.2\niou_threshold = 0.2\n\nuse_cuda = True\nuse_float16 = False\ncudnn.fastest = True\ncudnn.benchmark = True\n\nobj_list = ['Typical', 'Indeterminate', 'Atypical']\n\n\ncolor_list = standard_to_bgr(STANDARD_COLORS)\n# tf bilinear interpolation is different from any other's, just make do\ninput_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]\ninput_size = input_sizes[compound_coef] if force_input_size is None else force_input_size\n\nmodel = EfficientDetBackbone(compound_coef=compound_coef, num_classes=len(obj_list),\n                             ratios=anchor_ratios, scales=anchor_scales)\nmodel.load_state_dict(torch.load('logs\/siim\/efficientdet-d0_39_12600.pth', map_location='cpu'))\nmodel.requires_grad_(False)\nmodel.eval()\n\nif use_cuda:\n    model = model.cuda()\nif use_float16:\n    model = model.half()","bc7fc08f":"def display(preds, imgs):\n    for i in range(len(imgs)):\n        if len(preds[i]['rois']) == 0:\n            continue\n\n        imgs[i] = imgs[i].copy()\n\n        for j in range(len(preds[i]['rois'])):\n            x1, y1, x2, y2 = preds[i]['rois'][j].astype(np.int)\n            obj = obj_list[preds[i]['class_ids'][j]]\n            cv2.rectangle(ori_imgs[i], (x1, y1), (x2, y2), name2color[obj], 2)\n            score = float(out[i]['scores'][j])\n\n            cv2.putText(ori_imgs[i], '{:.3f}'.format(score),\n                        (x1, y1 + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n                        (255,255,255), 1)\n        return imgs[i]\n    \nimgs = []\nfor k, img_path in enumerate(img_list[:5]):\n    ori_imgs, framed_imgs, framed_metas = preprocess(img_path, max_size=input_size)\n\n    if use_cuda:\n        x = torch.stack([torch.from_numpy(fi).cuda() for fi in framed_imgs], 0)\n    else:\n        x = torch.stack([torch.from_numpy(fi) for fi in framed_imgs], 0)\n\n    x = x.to(torch.float32 if not use_float16 else torch.float16).permute(0, 3, 1, 2)\n\n    with torch.no_grad():\n        features, regression, classification, anchors = model(x)\n\n        regressBoxes = BBoxTransform()\n        clipBoxes = ClipBoxes()\n\n        out = postprocess(x,\n                          anchors, regression, classification,\n                          regressBoxes, clipBoxes,\n                          threshold, iou_threshold)\n\n    out = invert_affine(framed_metas, out)\n    imgs.append(display(out, ori_imgs))\n    \nfig = plt.figure(figsize=(5, len(imgs)*5))\nfor i, img in enumerate(imgs[:30]):\n    img = cv2.resize(img, (300,300))\n    ax = fig.add_subplot(len(imgs), 1, i+1)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    plt.imshow(img)","a8bfc214":"pcs_imgs = []\nfor k, img_path in enumerate(img_list):\n    ori_imgs, framed_imgs, framed_metas = preprocess(img_path, max_size=input_size)\n\n    if use_cuda:\n        x = torch.stack([torch.from_numpy(fi).cuda() for fi in framed_imgs], 0)\n    else:\n        x = torch.stack([torch.from_numpy(fi) for fi in framed_imgs], 0)\n\n    x = x.to(torch.float32 if not use_float16 else torch.float16).permute(0, 3, 1, 2)\n    \n    pcs_imgs.append(x)\n    with torch.no_grad():\n        features, regression, classification, anchors = model(x)\n\n        regressBoxes = BBoxTransform()\n        clipBoxes = ClipBoxes()\n\n        out = postprocess(x,\n                          anchors, regression, classification,\n                          regressBoxes, clipBoxes,\n                          threshold, iou_threshold)\n\n    out = invert_affine(framed_metas, out)\n    imgs.append(display(out, ori_imgs))","aabe9e1d":"outs = []\nbatch = 32\nwith torch.no_grad():\n    for i in range(len(pcs_imgs)\/\/batch + 1):\n        features, regression, classification, anchors = model(torch.cat(pcs_imgs[batch*i:batch*(i+1)]))\n\n        regressBoxes = BBoxTransform()\n        clipBoxes = ClipBoxes()\n\n        out = postprocess(torch.cat(pcs_imgs[batch*i:batch*(i+1)]),\n                          anchors, regression, classification,\n                          regressBoxes, clipBoxes,\n                          threshold, iou_threshold)\n        outs += out","6b487a44":"outs[0]","dd3c077c":"# for idx, name in enumerate(tqdm(img_list)):\n#     name = name.split('\/')[-1][:-4]\n#     img_size = image_size_dict[name]\n#     data_id = submission[submission.id==name+'_image'].index\n#     ans = \"\"\n#     for label in range(len(outs[idx]['rois'])):\n#         xmin, ymin, xmax, ymax = outs[idx]['rois'][label]\n#         confidence = outs[idx]['scores'][label]\n#         xmin = int(xmin * img_size[1]\/512)\n#         xmax = int(xmax * img_size[1]\/512)\n#         ymin = int(ymin * img_size[0]\/512)\n#         ymax = int(ymax * img_size[0]\/512)\n#         ans += f\"opacity {confidence} {xmin} {ymin} {xmax} {ymax} \"\n#     submission.loc[data_id, 'PredictionString'] = ans","643169fd":"submission.to_csv('\/kaggle\/working\/submission.csv', index=False)  ","f224e813":"batch size is 32.","e83da227":"# Preprocessing Test Image","23888cc5":"# EfficientDet\n\nARXIV [https:\/\/arxiv.org\/pdf\/1911.09070.pdf](https:\/\/arxiv.org\/pdf\/1911.09070.pdf)  \nGithub [https:\/\/github.com\/zylo117\/Yet-Another-EfficientDet-Pytorch](https:\/\/github.com\/zylo117\/Yet-Another-EfficientDet-Pytorch)  \n\nEfficientDet employs EfficientNet as the backbone network, BiFPN as the feature network, and shared class\/box prediction network. Both BiFPN layers and class\/box net layers are repeated multiple times based on different resource constraints.\n\n[Object Detection SOTA model](https:\/\/paperswithcode.com\/sota\/object-detection-on-coco)  \nThis page shows object detection models' score on COCO test-dev. I focus AP50 score, because this competitions' metric is AP50.    \n1. DyHead (Based Swin-L) : 78.5\n2. DetectoRS (Based ResNeXt) : 74.2\n3. YOLOv4-P7 (Based Scaled-YOLO) : 73.3\n4. EfficientDet-D7 (Based EfficientNet) : 72.4\n5. YOLOv4-608 (Based YOLO) : 65.7\n\n<figure>\n<img src=\"https:\/\/blog.roboflow.com\/content\/images\/2020\/06\/yolov5-performance.png\" style=\"width:700px\">\n    <figcaption>EfficientDet is better model than YOLOv5 on AP.<\/figcaption>\n<\/figure>\n\n\nEveryone used model based YOLOv4 or YOLOv5, but this model isn't SOTA model. I'll try EfficientDet first and then Scaled-YOLOv4, DyHead.  \n  \n### Three notebooks summarize how to use this model.\n#### [Preprocessing](https:\/\/www.kaggle.com\/adldotori\/efficientdet-preprocessing-better-than-yolov5\/)\n#### [Training](https:\/\/www.kaggle.com\/adldotori\/efficientdet-training-better-than-yolov5\/)\n#### [Inference](https:\/\/www.kaggle.com\/adldotori\/efficientdet-inference-better-than-yolov5\/)\n\nThis notebook is third notebook which includes how to inference EfficientDet.  \nLet's start!  \n\nThis picture shows the rough structure of efficientdet.\n![image](https:\/\/aihub-storage.s3.ap-northeast-2.amazonaws.com\/file\/efficientdet.png)","61ce4e11":"![image](https:\/\/aihub-storage.s3.ap-northeast-2.amazonaws.com\/file\/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-07-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_10.53.32.png)","b1940140":"Result looks like this. \n  \n**rois** : box position  \n**class_ids** : 0 => typical, 1 => indeterminate, 2=> atypical  \n**scores** : confidence  ","55b3fd58":"## Inference","982fe1b2":"# Environment","77487fff":"blue box : typical  \nyellow box : indeterminate  \nred box : atypical  ","dcd00cc9":"## Display","513eda93":"## Evaluation","52a35024":"## Model Upload","5d313e8d":"# TEST\n## Setting","5eb2ddf9":"# Move Directory"}}