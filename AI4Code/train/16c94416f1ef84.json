{"cell_type":{"199b1920":"code","8d447c4d":"code","574b77de":"code","808b1bf6":"code","3f40838d":"code","097920bc":"code","f0aa6e53":"code","dc53726c":"code","267245ea":"code","9c61f569":"code","c4e7908c":"code","29a029f9":"code","92a8bac6":"code","2f6a441f":"code","201e5513":"code","ba1453eb":"code","be84a7e6":"code","56755156":"code","28cd14f5":"code","31b346b0":"code","507ae6a2":"code","7f5ad79a":"code","3e04eb28":"code","91c6ca1c":"code","9253ba5b":"code","3b9674e7":"code","8928f542":"code","e5ef3627":"code","bddb5f2e":"code","2167bf9b":"code","7880b8a6":"code","c6f5a5c8":"code","3e4fe13e":"code","1b20cc94":"code","22082e72":"code","bba2e320":"markdown","36778466":"markdown","ec29b56e":"markdown","dceee300":"markdown","08f45daf":"markdown","d1b6ec63":"markdown","63a4a23e":"markdown","03f8f1dc":"markdown","7098baf2":"markdown"},"source":{"199b1920":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#pd.set_option(\"display.max_rows\", 999)\n#pd.set_option(\"display.max_columns\", 999)\n#pd.reset_option(\"display.max_rows\")\n#pd.reset_option(\"display.max_columns\")\n\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8d447c4d":"# Data loading\ntrain = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")","574b77de":"# Lets look at 5 rows of train set\ntrain.head()","808b1bf6":"# Lets look at 5 rows of test set\ntest.head()","3f40838d":"# Target variable\ny = train.pop(\"label\")\n","097920bc":"# Lets look at 5 rows of the target variable\ny.head()","f0aa6e53":"# Unique values and their frequiencies in the target variable\ny.value_counts()","dc53726c":"# train set has 784 feature(pixels) and 42000 photos, test set has 784 feature and 28000 photos.  \ntrain.shape,y.shape,test.shape","267245ea":"# Gets info quickly about data set\ntest.info()","9c61f569":"# Gets info quickly about data set\ntrain.info()","c4e7908c":"# Type of the values in y variable\ny.dtype","29a029f9":"# scale the input values to type float32 and (normalize) the input values within the interval [0, 1]\n\ntrain = train.astype('float32')\/255\ntest = test.astype('float32')\/255\ny = y.astype('float32')\n\n","92a8bac6":"# Converting pandas Dataframe to numpy array\n\"\"\"\nKeras models accept three types of inputs:\n\nNumPy arrays, just like Scikit-Learn and many other Python-based libraries. This is a good option if your data fits in memory.\n\nTensorFlow Dataset objects. This is a high-performance option that is more suitable for datasets that do not fit in memory and that are streamed from disk or from a distributed filesystem.\n\nPython generators that yield batches of data (such as custom subclasses of the keras.utils.Sequence class).\n\"\"\"\ntrain = pd.DataFrame.to_numpy(train)\ntest = pd.DataFrame.to_numpy(test)","2f6a441f":"# Splitting training set into train and dev set \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.33, random_state=42)","201e5513":"# Y variable has 10 different classes. Therefore we need to represent each values in y as vector. \n# This converst for example  1 to [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.] vector. \n\"\"\"\n# label encoding to y variable\nfrom keras.utils import to_categorical\ny = to_categorical(y, num_classes=10)\n\"\"\"\ny_onehot = tf.one_hot(y, depth=10)\ny_onehot_train = tf.one_hot(y_train, depth=10)\ny_onehot_test = tf.one_hot(y_test, depth=10)","ba1453eb":"from tensorflow import keras\n# Importing libraries\nfrom keras.models import Sequential              # creates sequential model\nfrom keras.layers.core import Dense, Activation # creates layers and calls activation functions\nfrom tensorflow.keras.layers import (\n    Dense,\n    Dropout,\n    Flatten\n)\n\nfrom kerastuner import HyperModel  # It helps to tune hyperparameters.","be84a7e6":"# Hyperparameter Tuning\n# https:\/\/keras-team.github.io\/keras-tuner\/documentation\/hyperparameters\/\n# For Beta and Epsilon:  https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/optimizers\/Adam\n\n\"\"\"\nHyperparameter tuning is heart of  ANN model and it directly affects the performance of the model. We can tune hyperparameters such as:\n\nLearning Rate : It determines how quick the model will learn. It should be selected carefully. If it is small, the model speed will be very slow which means the derivative \nof loss function goes to its minimum point very slowly. If it is very high, the derivative of the loss fucntion cannot reach to its global minimum point. \nTherefore I preffered to choose its values as 1e-2, 1e-3, 1e-4.\n\nThe number of nodes: Nodes are points of the Layers on ANN. We need to optimize them and it is very general that they can be choosen as 32,64,128,256 and 512. \n\nThe number of layer: It determines the complexity of the ANN model like the nodes. If you choose very high number, it can result in \"B\u0130AS\". If you choose very small like 2,\nit may not be good to solve complex and non-linear problems. We use dense function to create layers.\n\nActivation function: Normally ANN is the linear method (Z=W*X+b), but we use activation function to make ANN non-linear. The most famous activation functions are relu and \ntanh for layers. If you use binary classification, you need to use \"sigmoid\" funcion. If you classify more than 2 classes, you need to use \"softmax\" function.\n\nL2 Regularization: Regularization is used to reduce \"VAR\u0130ANCE\" problem. One of the regularization techniques is L2 that is added to loss function to punish the weights.\nBy doing this weights getting closer to zero which reduces the model complexity.\n\nDropout: It is another regularization techniues. It is based on to close some of nodes randomly in determined layers. It uses \"BERNOULL\u0130 PROBABL\u0130TY\" to determine which nodes \nis getting closed. It is very effective like L2 regularization. It is very commen to use both L2 and Dropout regularization.\n\nAdam optimazation: There are different optimazation methods like \"momentum\" and \"RMSProp\" to speed the model and increase the model performance. \"ADAM\" optimization technique\nuses noth momentum and RMSProp(Root Mean Square Prop)\n\nBatch-size : It is based on to divide data into small datasets and train them. It increase the performance beside speeding model training time. Exponentially weighted avarages \nstatistical technique is used to calculate avarage loss on this technique. \n\n\n\n\n\"\"\"\nclass AnnHyperModel(HyperModel): \n    def __init__(self, input_shape):\n        self.input_shape = input_shape\n        \n        \n    def build(self, hp):\n        model = Sequential()\n        model.add(\n            layers.Dense(\n                units=hp.Int('units', 8, 64, 4, default=8), \n                activation=hp.Choice(\n                    'dense_activation', \n                    values=['relu', 'tanh', 'elu'],\n                    default='relu'), \n                activity_regularizer=tf.keras.regularizers.l2(0.001),\n                input_shape=input_shape) )\n        \n        model.add( \n            layers.Dense(\n            units=hp.Int('units', 8, 64, 4, default=16), \n            activation=hp.Choice(\n                'dense_activation', \n                values=['relu', 'tanh', 'elu'], \n                default='relu')))\n\n        model.add( \n            layers.Dropout(\n                hp.Float( \n                    'dropout',\n                    min_value=0.0, \n                    max_value=0.1, \n                    default=0.005, \n                    step=0.01)))\n        \n    \n        model.add(layers.Dense(10,activation = \"softmax\"))\n        \n        model.compile(\n            optimizer=keras.optimizers.Adam(\n            hp.Choice('learning_rate',\n                      values=[1e-2, 1e-3, 1e-4]),\n                      beta_1=0.9,\n                      beta_2=0.999,\n                      epsilon=1e-07)\n            ,loss='mse',\n            metrics=['accuracy'] )\n\n        return model","56755156":"# Ceate the object from the class\ninput_shape = (X_train.shape[1],)\nhypermodel = AnnHyperModel(input_shape)","28cd14f5":"from kerastuner.tuners import RandomSearch","31b346b0":"# RandomSearch is here to do the hyperparameter search. \n# For more tuners: https:\/\/keras-team.github.io\/keras-tuner\/documentation\/tuners\/\ntuner_rs = RandomSearch( hypermodel,\n                        objective='val_accuracy', \n                        seed=42, \n                        max_trials=12)","507ae6a2":"# You can print a summary of the search space\ntuner_rs.search_space_summary()","7f5ad79a":"# fit the model to find best model\ntuner_rs.search(X_train, y_onehot_train, epochs=10, validation_data=(X_test, y_onehot_test), verbose=2)","3e04eb28":"# choosing best model among the models\nbest_model = tuner_rs.get_best_models(num_models=1)[0] \nloss, mse = best_model.evaluate(X_test,y_onehot_test)","91c6ca1c":"# Shows layers of the model\nbest_model.layers","9253ba5b":"# Shows weights of the model (w,b)\nbest_model.weights","3b9674e7":"# used to see the content of the model. It gives a summary of the model.\n# here is the total number of parameters entering the nodes in each layer, which is called params. \n# There is 784 inputs in the first layer,that is, 784 w and 2 b and since there are 2 nodes, the total parameter entered into the nodes = 2 * 784 +2 = 1570\nbest_model.summary()\n","8928f542":"# model fitting\n# batch_size_step = X_train\/batch_size \nbest_model.fit(X_train, y_onehot_train,\n          batch_size=100, epochs=10)   # epochs = number of iterations","e5ef3627":"# Model evaluation\ntest_loss, test_acc = best_model.evaluate(X_test, y_onehot_test)\nprint(\"Dev set accuracy: \", test_acc)\nprint(\"Dev set loss: \", test_loss)","bddb5f2e":"# Plot confusion matrix \n# Note: This code snippet for confusion-matrix is taken directly from the SKLEARN website.\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=30)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('Actual class')\n    plt.xlabel('Predicted class')","2167bf9b":"from collections import Counter\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\n# Predict the values from the validation dataset\nY_pred = best_model.predict(X_test)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred, axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_onehot_test, axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10))","7880b8a6":"# The Predict () method returns a vector containing the predictions of all dataset items.\npredictions = best_model.predict(X_test)","c6f5a5c8":"# Returning the index of the position containing the highest value of the vector, we know which class gives the highest probability of belonging with the argmax function of Numpy.\nnp.argmax(predictions[9])","3e4fe13e":"# We can use sum to see that all values in a vector are zero. Because these are probability values.\nnp.sum(predictions[11])","1b20cc94":"# The Predict () method returns a vector containing the predictions of all dataset items.\ntest_result = best_model.predict(test)","22082e72":"# Saving the results to a csv file\n\n# Convert one-hot vector to number\nresults = np.argmax(test_result,axis = 1) # this gives us the corresponding y value relative to the highest probability in the prediction vector, such as 2 or 3\n\nresults = pd.Series(results,name=\"Label\")\n\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"test_submission.csv\",index=False)","bba2e320":"## I AM LOOKING FORWARD TO YOUR COMMENTS AND UPVOTES \n\n## If you have any questi\u0307on, please donot hesi\u0307tate to ask.\n\n","36778466":"# Introduction\n\nIn this workshop, I will show you how to build Artificial Neural Network(ANN) and how to tune its hyperparameters. I will use Digit Recognizer data set that is very famous among \"Kagglers\" who is specially interested in Neural Network. The data sets consist of the number of hand written digits that range from 0 to 9. The size og digits' photos er 28x28 which means each photo has 784 features(pixels). Also the data set consist of tran  and test sets. Train sets dimension is (42000x784) that means there are 42000 different photos in the train set while the test set dimension is (28000x784).\n\n**Metric:** I used \"accuracy metric\" as a metric to evaluate model performance.\n\n**Train set splitting:** I splitted the train set as %66 of train and 33% of dev set.\n\n<font color = 'blue'>\n Content:\n   \n   1. [Data Loading and Pre-processing](#1)\n   \n   2. [Building Model and Optimize Hyperparameters](#2)","ec29b56e":"<a id = '1'><\/a><br>\n## 1. Data Loading and Pre-processing","dceee300":"# Prediction test-set","08f45daf":"## Confusion Matrix\n\n\"\" In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one. \"\"","d1b6ec63":"<a id = '2'><\/a><br>\n##  2. Building Model and Optimize Hyperparameters","63a4a23e":"**Note: Keras accepts data with type of float32, and normilizing pixels by dividing 255 will sharply increase the speed of the ANN.**","03f8f1dc":"## Write results to csv ","7098baf2":"# Prediction dev-set"}}