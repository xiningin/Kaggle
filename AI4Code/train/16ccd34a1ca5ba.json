{"cell_type":{"08e2c92a":"code","bf7edd98":"code","89c862e2":"code","b365c23e":"code","261c8e00":"markdown","e055a870":"markdown"},"source":{"08e2c92a":"import numpy as np \nimport pandas as pd \n\nraw_train =  pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\nraw_test = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\n\n# Making a copy of raw data\ntrain = raw_train.copy()\ntest = raw_test.copy()\n\n# Shape\nprint(\"train shape :\" , train.shape)\nprint(\"test shape :\" , test.shape)\n\n# Split target and remove de id column  in both dataset\ntarget = raw_train.claim\ntrain.drop(['id','claim'], axis = 1, inplace = True)\ntest.drop('id', axis = 1, inplace = True)","bf7edd98":"from sklearn.impute import SimpleImputer\n\nfeatures = train.columns\n\ntrain['n_missing'] = train[features].isna().sum(axis=1)\ntest['n_missing'] = test[features].isna().sum(axis=1)\n\ntrain['std'] = train[features].std(axis=1)\ntest['std'] = test[features].std(axis=1)\n\nsi = SimpleImputer(strategy ='mean')\ntrain[features] = si.fit_transform(train[features])\ntest[features] = si.transform(test[features])","89c862e2":"import lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\n\nparams = {   \n               'objective': 'binary',\n               'boosting_type': 'gbdt',\n               'num_leaves': 6,\n               'max_depth': 2,\n               'learning_rate': 0.1,\n               'n_estimators': 40000,\n               'reg_alpha': 25.0,\n               'reg_lambda': 76.7,\n               'bagging_seed': 42, \n               'feature_fraction_seed': 42,\n               'n_jobs': 4,\n               'subsample': 0.98,\n               'subsample_freq': 1,\n               'colsample_bytree': 0.69,\n               'min_child_samples': 54,\n               'min_child_weight': 256,\n}\n\npreds_valid_f = {}\npreds_test = []\ntotal_auc = []\n\nkf = KFold(n_splits=5,random_state=0,shuffle=True)\n\nfor fold,(train_index, valid_index) in enumerate(kf.split(train,target)):\n\n    X_train,X_valid = train.loc[train_index], train.loc[valid_index]\n    y_train,y_valid = target.loc[train_index], target.loc[valid_index]\n    \n    # Preprocessing\n    index_valid  = X_valid.index.tolist()\n    #--------------------------------------------------------\n    # model\n    model = lgb.LGBMClassifier(**params,random_state = 0)\n    model.fit(X_train, y_train,\n              verbose=False,\n              # These three parameters will stop training before a model starts overfitting \n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"auc\",\n              early_stopping_rounds=300,\n              )\n    # oof\n    preds_valid = model.predict_proba(X_valid)[:,1]\n    #--------------------------------------------------------\n    preds_test.append(model.predict_proba(test)[:,1])\n    #--------------------------------------------------------\n    preds_valid_f.update(dict(zip(index_valid, preds_valid)))\n\n    # Getting score for a fold model\n    fold_auc = roc_auc_score(y_valid, preds_valid)\n    print(f\"Fold {fold} roc_auc_score: {fold_auc}\")\n    # Total rmse\n    total_auc.append(fold_auc)\n    \nprint(f\"mean roc_auc_score: {np.mean(total_auc)}, std: {np.std(total_auc)}\")","b365c23e":"output = pd.DataFrame({'id': raw_test.id,\n                       'claim': np.mean(preds_test, axis = 0)})\noutput.to_csv('preds.csv', index=False)\noutput.head()","261c8e00":"# How to get a good score?  TPS September 2021 (competition)\n### Following the next steps ","e055a870":"### Features Engineering and Cleaning"}}