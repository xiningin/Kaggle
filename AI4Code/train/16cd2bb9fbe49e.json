{"cell_type":{"771c7d06":"code","ff80e347":"code","e990ccce":"code","7797e3aa":"code","e0a9c3ac":"code","4e8063df":"code","43e765cb":"code","62214432":"code","44e7d2f7":"code","9ad125cc":"code","227b2d01":"code","1e54dbfb":"code","a5461ec9":"code","de92368b":"code","72696c56":"code","60c5ca4b":"code","3c4783d1":"code","fef79e49":"code","dd2d5338":"code","7bf431e8":"code","da34621a":"code","8bf5d602":"code","39da298b":"code","50db26fc":"code","a8533136":"code","4af0cfa3":"code","050bf883":"code","68df4f0c":"code","fd20ed51":"code","c28f2ad1":"code","a830c6e2":"code","69d26b28":"code","7bf9b97a":"code","13cb4ad2":"code","e76bf223":"code","da7d79c4":"code","b495555a":"code","1bfddad7":"code","482aab64":"code","79169b9a":"code","244ae361":"code","dbd01a8b":"code","c1b83df7":"code","2a8b620e":"code","cbbfb373":"code","282f5556":"code","5aa6a84f":"code","b8bd97dc":"code","0f8ac40d":"code","1176e002":"code","7555f94a":"code","f38ed36e":"code","dd76888d":"code","9a4f9162":"code","7c4de291":"code","892bd91f":"code","fcd29770":"code","1f70c29a":"code","a3d93bcf":"code","15fe0358":"code","b93205c6":"code","8d8fa8b3":"code","209accc2":"code","352d71fd":"code","52dbc8c8":"code","42e78df1":"code","a19eb925":"code","32bbd5da":"code","b50dead6":"code","b01e6ed9":"code","0d4dce45":"code","1cc9bd2e":"code","ee247737":"code","bc4e3e58":"code","e0da2fd9":"code","a59d4bb2":"code","f590f239":"code","ec9ebebe":"code","92cf6285":"markdown","328d8828":"markdown","d6af6e78":"markdown","1ad38438":"markdown","36260620":"markdown","b8f781f7":"markdown","8851aabf":"markdown","a5963f61":"markdown","fb24e61f":"markdown","30e07596":"markdown","652f8098":"markdown","fc1c2076":"markdown","0ea1fc8a":"markdown","40004ca3":"markdown","57031d7f":"markdown","e773dce6":"markdown"},"source":{"771c7d06":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ff80e347":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n\nsns.set(style='white', context='notebook', palette='deep')","e990ccce":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","7797e3aa":"train.head()","e0a9c3ac":"test.head()","4e8063df":"train.shape","43e765cb":"test.shape","62214432":"train.info()","44e7d2f7":"test.info()","9ad125cc":"train.isnull().sum()","227b2d01":"test.isnull().sum()","1e54dbfb":"def bar_chart(features):\n    survived = train[train['Survived']==1][features].value_counts()\n    dead = train[train['Survived']==0][features].value_counts()\n    df = pd.DataFrame([survived, dead])\n    df.index = ['Survived', 'Dead']\n    df.plot(kind='bar', stacked=True, figsize=(10,5))","a5461ec9":"bar_chart('Sex')","de92368b":"bar_chart('Pclass')","72696c56":"bar_chart('SibSp')","60c5ca4b":"bar_chart('Parch')","3c4783d1":"bar_chart('Embarked')","fef79e49":"train_test_data = [train,test]\n\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Name'].str.extract('([A-za-a]+)\\.',expand=False)","dd2d5338":"train['Title'].value_counts()","7bf431e8":"test['Title'].value_counts()","da34621a":"title_mapping = {\"Mr\":0, \"Miss\":1, \"Mrs\":2, \"Master\":3, \"Dr\":3, \"Rev\":3, \"Col\":3, \"Major\":3, \"Mile\":3, \"Jonkheer\":3, \"Countess\":3, \"Sir\":3, \"Ms\":3, \"Lady\":3, \"Mme\":3, \"Don\":3, \"Capt\":3, \"Dona\":3}","8bf5d602":"for dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)","39da298b":"train.head()","50db26fc":"train.drop('Name', axis=1, inplace=True)\ntest.drop('Name', axis=1, inplace=True)","a8533136":"sex_mapping = {'male':0, 'female':1}\n\nfor dataset in train_test_data:\n    dataset['Sex'] = dataset['Sex'].map(sex_mapping)","4af0cfa3":"train.head()","050bf883":"train[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\ntest[\"Age\"].fillna(test.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)","68df4f0c":"train.head(20)","fd20ed51":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()\n \nplt.show()","c28f2ad1":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()\nplt.xlim(0,20)","a830c6e2":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()\nplt.xlim(20,40)","69d26b28":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()\nplt.xlim(40,60)","7bf9b97a":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()\nplt.xlim(60,100)","13cb4ad2":"for dataset in train_test_data:\n    dataset.loc[dataset['Age']<=16, 'Age'] = 0\n    dataset.loc[(dataset['Age']>16)& (dataset['Age']<=23), 'Age'] =1\n    dataset.loc[(dataset['Age']>23)& (dataset['Age']<=34), 'Age'] =2\n    dataset.loc[(dataset['Age']>34)& (dataset['Age']<=42), 'Age'] =3\n    dataset.loc[(dataset['Age']>42)& (dataset['Age']<=58), 'Age'] =4\n    dataset.loc[dataset['Age']>58, 'Age'] = 5","e76bf223":"train.head(10)","da7d79c4":"Pclass1 = train[train['Pclass']==1]['Embarked'].value_counts()\nPclass2 = train[train['Pclass']==2]['Embarked'].value_counts()\nPclass3 = train[train['Pclass']==3]['Embarked'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class', '2nd class', '3rd class']\ndf.plot(kind='bar', stacked=True, figsize=(10,5))","b495555a":"for dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n    dataset['Title'] = dataset['Title'].fillna(0)","1bfddad7":"train.isnull().sum()","482aab64":"test.isnull().sum()","79169b9a":"embarked_mapping = {'S': 0, \"C\": 1, \"Q\": 2}\n\nfor dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)","244ae361":"train[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntest[\"Fare\"].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)","dbd01a8b":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Fare',shade= True)\nfacet.set(xlim=(0, train['Fare'].max()))\nfacet.add_legend()\n \nplt.show()","c1b83df7":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Fare',shade= True)\nfacet.set(xlim=(0, train['Fare'].max()))\nfacet.add_legend()\nplt.xlim(0,20)","2a8b620e":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Fare',shade= True)\nfacet.set(xlim=(0, train['Fare'].max()))\nfacet.add_legend()\nplt.xlim(20,40)","cbbfb373":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Fare',shade= True)\nfacet.set(xlim=(0, train['Fare'].max()))\nfacet.add_legend()\nplt.xlim(40,60)","282f5556":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Fare',shade= True)\nfacet.set(xlim=(0, train['Fare'].max()))\nfacet.add_legend()\nplt.xlim(60,100)","5aa6a84f":"for dataset in train_test_data:\n    dataset.loc[dataset['Fare']<=17, 'Fare'] = 0\n    dataset.loc[(dataset['Fare']>17) & (dataset['Fare']<=29), 'Fare']= 1\n    dataset.loc[(dataset['Fare']>29) & (dataset['Fare']<=100), 'Fare']= 2\n    dataset.loc[dataset['Fare']>100, 'Fare'] = 3","b8bd97dc":"train.head(10)","0f8ac40d":"train.Cabin.value_counts()","1176e002":"for dataset in train_test_data:\n    dataset['Cabin'] = dataset['Cabin'].str[:1]","7555f94a":"Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts()\nPclass2 = train[train['Pclass']==2]['Cabin'].value_counts()\nPclass3 = train[train['Pclass']==3]['Cabin'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class', '2nd class', '3rd class']\ndf.plot(kind ='bar', stacked=True, figsize=(10,5))","f38ed36e":"cabin_mapping = {\"A\":0, \"B\":0.4, \"C\":0.8, \"D\":1.2, \"E\":1.6, \"F\":2.0, \"G\":2.4, \"T\":2.8}\nfor dataset in train_test_data:\n    dataset['Cabin']=dataset['Cabin'].map(cabin_mapping)","dd76888d":"train['Cabin'].fillna(train.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)\ntest['Cabin'].fillna(test.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)","9a4f9162":"train.head(20)","7c4de291":"train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1","892bd91f":"train['FamilySize'].max()","fcd29770":"family_mapping = {1:0, 2:0.4, 3:0.8, 4:1.2, 5:1.6, 6:2.0, 7:2.4, 8:2.8, 9:3.2, 10:3.6, 11:4.0}\nfor dataset in train_test_data:\n    dataset['FamilySize']=dataset['FamilySize'].map(family_mapping)","1f70c29a":"train.head(10)","a3d93bcf":"features_drop = ['SibSp', 'Ticket', 'Parch']\ntrain = train.drop(features_drop, axis=1)\ntest = test.drop(features_drop, axis=1)\ntrain = train.drop(['PassengerId'], axis=1)","15fe0358":"train_data = train.drop(['Survived'], axis=1)\ntarget = train['Survived']","b93205c6":"train_data.head(10)","8d8fa8b3":"train.info()","209accc2":"test.info()","352d71fd":"k_fold = StratifiedKFold(n_splits=10)","52dbc8c8":"random_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, train_data, y = target, scoring = \"accuracy\", cv = k_fold, n_jobs=-1))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n    \ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","42e78df1":"DTC = DecisionTreeClassifier(max_depth=5)\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,10,20,30,40,50],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=k_fold, scoring=\"accuracy\", n_jobs= -1, verbose = 1,return_train_score = True)\n\ngsadaDTC.fit(train_data, target)\n\nada_best = gsadaDTC.best_estimator_","a19eb925":"gsadaDTC.best_score_","32bbd5da":"ExtC = ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [5],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[500,1000],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=k_fold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsExtC.fit(train_data,target)\n\nExtC_best = gsExtC.best_estimator_","b50dead6":"gsExtC.best_score_","b01e6ed9":"RFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [2],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,500],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=k_fold, scoring=\"accuracy\", n_jobs=-1, verbose = 1)\n\ngsRFC.fit(train_data,target)\n\nRFC_best = gsRFC.best_estimator_\n","0d4dce45":"gsRFC.best_score_","1cc9bd2e":"GBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [1000,2000,3000],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [19],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=k_fold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsGBC.fit(train_data,target)\n\nGBC_best = gsGBC.best_estimator_","ee247737":"gsGBC.best_score_","bc4e3e58":"SVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [ 0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=k_fold, scoring=\"accuracy\", n_jobs=-1, verbose = 1)\n\ngsSVMC.fit(train_data,target)\n\nSVMC_best = gsSVMC.best_estimator_","e0da2fd9":"gsSVMC.best_score_","a59d4bb2":"votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=-1)\n\nvotingC = votingC.fit(train_data, target)","f590f239":"test_data = test.drop(\"PassengerId\", axis=1).copy()\nprediction = votingC.predict(test_data)","ec9ebebe":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": prediction\n    })\nsubmission.to_csv('submission.csv', index=False)","92cf6285":"* To distinguish the 'Cabin', we extract only the first letter. because 'Cabin' is related to class, we\n divide 'Cabin' by class. \n* As a result, we can confirm that cabin belongs to 1st class only.\n* The way to fill in the lost data of the 'Cabin' was the basis for class and filled it with a median value according to 'Pclass'.","328d8828":"* The way to fill in the lost data of the 'Fare' feature could be the average or median of all passengers fare, but i thought that 'Fare' was the basis for class and filled it with a median value according to 'Pclass'.  ","d6af6e78":"* Passengers embarked at the S dock occupy more than 70 percent of all class.\n* so the two lost data are filled with S dock.","1ad38438":"* Prepare fuctions to easily determine the ratio of survival and death according to features.","36260620":"I studied 6 popular classifiers and evaluate the mean accuracy of each of them by a stratified kfold cross validation procedure.\n\n1. SVC\n2. DecisionTree\n3. AdaBoost\n4. RandomForest\n5. ExtraTree\n6. GradientBoosting\n","b8f781f7":"# My First Submission(beginner, unfamiliar English)\n\nHello, This is my first kaggle challenge and the first entry into data science. i will focus on feature engineering , ensemble model and feature analysis.","8851aabf":"* Predict and submission","a5963f61":"Results\n1. Train data has 891 data and 12 features. you have to fill in lost data(Age, Cabin, Embarked).\n2. Test data has 418 data and 11 features. you have to fill in lost data(Age, Cabin, Fare).\n\nLost data should be replaced with meaningful data through feature engineeing. just applying the model can cause problems. \n \n Of course, the test data has 11 features because there is no survival feature.","fb24e61f":"* The way to fill in the lost data of the 'Age' feature could be the average or median of all passengers ages, but i thought that 'Title' was the basis for gender and filled it with a median value according to 'Title'. ","30e07596":"These are the information i can get from the charts.\n\n1. 'Sex' chart shows that the survival rate of women is relatively higher than men.\n2. 'Pclass' chart shows that 1st class passengers have a higher survival rate than in other spaces.\n3. 'SibSp' chart shows that passengers with realtive or spouse have a higher survival rate than alone. too many 'SibSp' are not.\n4. 'Parch' chart shows that passengers with family or child have a higher survival rate than alone. too many 'Parch' are not.\n5. 'Embarked' chart shows that passengers in the C dock have a higher survival rate than other dock.\n","652f8098":"I decided to choose the SVC, AdaBoost, RandomForest , ExtraTrees and the GradientBoosting classifiers for the ensemble modeling.","fc1c2076":"test.isnull().sum()","0ea1fc8a":"**Let's take a quick look at train data and test data.**","40004ca3":"* The important information you can get from the 'Name' feature is the title.\n* extract the title from the name and add a new feature('Title').\n* Except for the three titles with many numbers, the remains are mapping '3'.","57031d7f":"* Ensemble medeling \n\nWe will use the voting classifier to combine the predictions from the 5 classifiers for ensemble modeling.\n","e773dce6":"If you have seen this notebook helpful, please upvote it. And many more advice and questions. They will be a great help to me.::))"}}