{"cell_type":{"e990fd8f":"code","e971f197":"code","03882e75":"code","9eb37345":"code","bf14c446":"code","17e22508":"code","dd5b1799":"code","0fbc0dd0":"code","8079342c":"code","71db8f05":"code","a430e58e":"code","2bbe3f66":"code","20b36557":"code","886f4efe":"code","9e70855e":"code","8d82c425":"code","f01d3eac":"code","69d1dbf7":"code","7fd181f4":"code","2da310bb":"code","f9c77fb4":"code","62bfea52":"code","98a18f05":"code","e07a0fd8":"code","40153963":"code","3e799c0a":"code","1c7de2bc":"code","2dbb8827":"code","6145fa0a":"code","41e5abd5":"code","85aef84d":"code","e4066e89":"code","26a14488":"code","83606d47":"code","0f8c0a87":"code","e769f32d":"code","84549568":"code","6c8ae3d5":"code","0880249d":"code","b6155a20":"code","721844f6":"code","bad0691b":"code","9b80493b":"code","4b36c89c":"code","e2624a9b":"code","64dde77e":"code","d77735f6":"code","563c3ab3":"markdown","7c99853a":"markdown","e5e9a98a":"markdown","8d078846":"markdown","1ad082a6":"markdown","83f657d5":"markdown","f501633d":"markdown","61e45ce4":"markdown","9545a85a":"markdown","70cd83a4":"markdown","0d0e1141":"markdown","f956f7ed":"markdown","eba4ab75":"markdown","3dbe65eb":"markdown","8685834c":"markdown","e7bf0f04":"markdown"},"source":{"e990fd8f":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score","e971f197":"df = pd.read_csv('..\/input\/ldss-v1\/Merge2.csv')\ndf.set_index('PropertyId', inplace=True)\ndf.drop('Built in year', axis='columns', inplace=True)\n#df.drop('Location', axis='columns', inplace=True)\n#df = df.filter(['Location','Price_Category'])\nspecificity = 0","03882e75":"df_dummy = pd.get_dummies(df['Location'], drop_first=True)\ndf_dummy['Athal'] = 0\ndf_dummy['AWT Army Welfare Trust'] = 0\ndf_dummy['Burma Town'] = 0\ndf_dummy['Canal View'] = 0\ndf_dummy['D-18'] = 0\ndf_dummy['Elite Town'] = 0\ndf_dummy['F-15'] = 0\ndf_dummy['G-8'] = 0\ndf_dummy['Jinnah Avenue'] = 0\ndf_dummy['Lahore - Islamabad Motorway'] = 0\ndf_dummy['Mughalpura'] = 0\ndf_dummy['OPF Housing Scheme'] = 0\ndf_dummy['PCSIR Housing Scheme'] = 0\ndf_dummy['Super Highway'] = 0\ndf_dummy","9eb37345":"df.drop('Location', axis='columns', inplace=True)\ndf = pd.merge(df_dummy, df, on='PropertyId')","bf14c446":"df.head()","17e22508":"X = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values","dd5b1799":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","0fbc0dd0":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","8079342c":"type(X_test)","71db8f05":"result_comparison = pd.DataFrame(columns=['Model', 'Cross Validation Mean Accuracy', 'Cross Validation Standard Deviation', 'Test Data Accuracy', 'Test Data Precision', 'Test Data Recall', 'Test Data Specificity' ])","a430e58e":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)","2bbe3f66":"accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","20b36557":"y_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","886f4efe":"if (cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0]) == 0:\n    precision_a = 0\nelse:\n    precision_a = cm[0,0]\/(cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0])\n    \nif (cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1]) == 0:\n    precision_b = 0\nelse:\n    precision_b = cm[1,1]\/(cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1])\n    \nif (cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2]) == 0:\n    precision_c = 0\nelse:\n    precision_c = cm[2,2]\/(cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2])\n    \nif (cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3]) == 0:\n    precision_d = 0\nelse:\n    precision_d = cm[3,3]\/(cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3])\nmicro_precision = (precision_a+precision_b+precision_c+precision_d)\/4\nprint(micro_precision)\n\nif (cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3]) == 0:\n    recall_a = 0\nelse:\n    recall_a = cm[0,0]\/(cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3])\n    \nif (cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3]) == 0:\n    recall_b = 0\nelse:\n    recall_b = cm[1,1]\/(cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3])\n    \nif (cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3]) == 0:\n    recall_c = 0\nelse:\n    recall_c = cm[2,2]\/(cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3])\n    \nif (cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3]) == 0:\n    recall_d = 0\nelse:\n    recall_d = cm[3,3]\/(cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3])\n\t\nmicro_recall = (recall_a+recall_b+recall_c+recall_d)\/4\nprint(micro_recall)","9e70855e":"result_comparison = result_comparison.append({'Model':'Logistic Regression', 'Cross Validation Mean Accuracy': accuracies.mean()*100, 'Cross Validation Standard Deviation': accuracies.std()*100, 'Test Data Accuracy': accuracy_score(y_test, y_pred)*100, 'Test Data Precision':micro_precision, 'Test Data Recall':micro_recall, 'Test Data Specificity':0}, ignore_index=True)","8d82c425":"#import statsmodels.api as sm\n#\n#X2 = sm.add_constant(X)\n#est = sm.OLS(y, X2)\n#est2 = est.fit()\n#print(est2.summary())","f01d3eac":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","69d1dbf7":"accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","7fd181f4":"y_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)*100","2da310bb":"if (cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0]) == 0:\n    precision_a = 0\nelse:\n    precision_a = cm[0,0]\/(cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0])\n    \nif (cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1]) == 0:\n    precision_b = 0\nelse:\n    precision_b = cm[1,1]\/(cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1])\n    \nif (cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2]) == 0:\n    precision_c = 0\nelse:\n    precision_c = cm[2,2]\/(cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2])\n    \nif (cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3]) == 0:\n    precision_d = 0\nelse:\n    precision_d = cm[3,3]\/(cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3])\nmicro_precision = (precision_a+precision_b+precision_c+precision_d)\/4\nprint(micro_precision)\n\nif (cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3]) == 0:\n    recall_a = 0\nelse:\n    recall_a = cm[0,0]\/(cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3])\n    \nif (cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3]) == 0:\n    recall_b = 0\nelse:\n    recall_b = cm[1,1]\/(cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3])\n    \nif (cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3]) == 0:\n    recall_c = 0\nelse:\n    recall_c = cm[2,2]\/(cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3])\n    \nif (cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3]) == 0:\n    recall_d = 0\nelse:\n    recall_d = cm[3,3]\/(cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3])\n\t\nmicro_recall = (recall_a+recall_b+recall_c+recall_d)\/4\nprint(micro_recall)","f9c77fb4":"result_comparison = result_comparison.append({'Model':'Decision Tree', 'Cross Validation Mean Accuracy': accuracies.mean()*100, 'Cross Validation Standard Deviation': accuracies.std()*100, 'Test Data Accuracy': accuracy_score(y_test, y_pred)*100, 'Test Data Precision':micro_precision, 'Test Data Recall':micro_recall, 'Test Data Specificity':specificity}, ignore_index=True)","62bfea52":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\n\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix\")\nprint(cm)\nprint (\"Accuracy on Test Set: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\n\nif (cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0]) == 0:\n    precision_a = 0\nelse:\n    precision_a = cm[0,0]\/(cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0])\n    \nif (cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1]) == 0:\n    precision_b = 0\nelse:\n    precision_b = cm[1,1]\/(cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1])\n    \nif (cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2]) == 0:\n    precision_c = 0\nelse:\n    precision_c = cm[2,2]\/(cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2])\n    \nif (cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3]) == 0:\n    precision_d = 0\nelse:\n    precision_d = cm[3,3]\/(cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3])\nmicro_precision = (precision_a+precision_b+precision_c+precision_d)\/4\nprint(micro_precision)\n\nif (cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3]) == 0:\n    recall_a = 0\nelse:\n    recall_a = cm[0,0]\/(cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3])\n    \nif (cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3]) == 0:\n    recall_b = 0\nelse:\n    recall_b = cm[1,1]\/(cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3])\n    \nif (cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3]) == 0:\n    recall_c = 0\nelse:\n    recall_c = cm[2,2]\/(cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3])\n    \nif (cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3]) == 0:\n    recall_d = 0\nelse:\n    recall_d = cm[3,3]\/(cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3])\n\t\nmicro_recall = (recall_a+recall_b+recall_c+recall_d)\/4\nprint(micro_recall)\n","98a18f05":"result_comparison = result_comparison.append({'Model':'Random Forest', 'Cross Validation Mean Accuracy': accuracies.mean()*100, 'Cross Validation Standard Deviation': accuracies.std()*100, 'Test Data Accuracy': accuracy_score(y_test, y_pred)*100, 'Test Data Precision':micro_precision, 'Test Data Recall':micro_recall, 'Test Data Specificity':specificity}, ignore_index=True)","e07a0fd8":"from sklearn.svm import SVC\nclassifier = SVC(kernel = 'linear')\nclassifier.fit(X_train, y_train)\n\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix\")\nprint(cm)\nprint (\"Accuracy on Test Set: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\n\nif (cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0]) == 0:\n    precision_a = 0\nelse:\n    precision_a = cm[0,0]\/(cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0])\n    \nif (cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1]) == 0:\n    precision_b = 0\nelse:\n    precision_b = cm[1,1]\/(cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1])\n    \nif (cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2]) == 0:\n    precision_c = 0\nelse:\n    precision_c = cm[2,2]\/(cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2])\n    \nif (cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3]) == 0:\n    precision_d = 0\nelse:\n    precision_d = cm[3,3]\/(cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3])\nmicro_precision = (precision_a+precision_b+precision_c+precision_d)\/4\nprint(micro_precision)\n\nif (cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3]) == 0:\n    recall_a = 0\nelse:\n    recall_a = cm[0,0]\/(cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3])\n    \nif (cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3]) == 0:\n    recall_b = 0\nelse:\n    recall_b = cm[1,1]\/(cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3])\n    \nif (cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3]) == 0:\n    recall_c = 0\nelse:\n    recall_c = cm[2,2]\/(cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3])\n    \nif (cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3]) == 0:\n    recall_d = 0\nelse:\n    recall_d = cm[3,3]\/(cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3])\n\t\nmicro_recall = (recall_a+recall_b+recall_c+recall_d)\/4\nprint(micro_recall)","40153963":"result_comparison = result_comparison.append({'Model':'SVM', 'Cross Validation Mean Accuracy': accuracies.mean()*100, 'Cross Validation Standard Deviation': accuracies.std()*100, 'Test Data Accuracy': accuracy_score(y_test, y_pred)*100, 'Test Data Precision':micro_precision, 'Test Data Recall':micro_recall, 'Test Data Specificity':specificity}, ignore_index=True)","3e799c0a":"classifier = SVC(kernel = 'rbf')\nclassifier.fit(X_train, y_train)\n\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix\")\nprint(cm)\nprint (\"Accuracy on Test Set: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\n\nif (cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0]) == 0:\n    precision_a = 0\nelse:\n    precision_a = cm[0,0]\/(cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0])\n    \nif (cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1]) == 0:\n    precision_b = 0\nelse:\n    precision_b = cm[1,1]\/(cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1])\n    \nif (cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2]) == 0:\n    precision_c = 0\nelse:\n    precision_c = cm[2,2]\/(cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2])\n    \nif (cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3]) == 0:\n    precision_d = 0\nelse:\n    precision_d = cm[3,3]\/(cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3])\nmicro_precision = (precision_a+precision_b+precision_c+precision_d)\/4\nprint(micro_precision)\n\nif (cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3]) == 0:\n    recall_a = 0\nelse:\n    recall_a = cm[0,0]\/(cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3])\n    \nif (cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3]) == 0:\n    recall_b = 0\nelse:\n    recall_b = cm[1,1]\/(cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3])\n    \nif (cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3]) == 0:\n    recall_c = 0\nelse:\n    recall_c = cm[2,2]\/(cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3])\n    \nif (cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3]) == 0:\n    recall_d = 0\nelse:\n    recall_d = cm[3,3]\/(cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3])\n\t\nmicro_recall = (recall_a+recall_b+recall_c+recall_d)\/4\nprint(micro_recall)","1c7de2bc":"result_comparison = result_comparison.append({'Model':'Kernel SVM', 'Cross Validation Mean Accuracy': accuracies.mean()*100, 'Cross Validation Standard Deviation': accuracies.std()*100, 'Test Data Accuracy': accuracy_score(y_test, y_pred)*100, 'Test Data Precision':micro_precision, 'Test Data Recall':micro_recall, 'Test Data Specificity':specificity}, ignore_index=True)","2dbb8827":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix\")\nprint(cm)\nprint (\"Accuracy on Test Set: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\n\nif (cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0]) == 0:\n    precision_a = 0\nelse:\n    precision_a = cm[0,0]\/(cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0])\n    \nif (cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1]) == 0:\n    precision_b = 0\nelse:\n    precision_b = cm[1,1]\/(cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1])\n    \nif (cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2]) == 0:\n    precision_c = 0\nelse:\n    precision_c = cm[2,2]\/(cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2])\n    \nif (cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3]) == 0:\n    precision_d = 0\nelse:\n    precision_d = cm[3,3]\/(cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3])\nmicro_precision = (precision_a+precision_b+precision_c+precision_d)\/4\nprint(micro_precision)\n\nif (cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3]) == 0:\n    recall_a = 0\nelse:\n    recall_a = cm[0,0]\/(cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3])\n    \nif (cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3]) == 0:\n    recall_b = 0\nelse:\n    recall_b = cm[1,1]\/(cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3])\n    \nif (cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3]) == 0:\n    recall_c = 0\nelse:\n    recall_c = cm[2,2]\/(cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3])\n    \nif (cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3]) == 0:\n    recall_d = 0\nelse:\n    recall_d = cm[3,3]\/(cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3])\n\t\nmicro_recall = (recall_a+recall_b+recall_c+recall_d)\/4\nprint(micro_recall)","6145fa0a":"result_comparison = result_comparison.append({'Model':'Naive Bayes', 'Cross Validation Mean Accuracy': accuracies.mean()*100, 'Cross Validation Standard Deviation': accuracies.std()*100, 'Test Data Accuracy': accuracy_score(y_test, y_pred)*100, 'Test Data Precision':micro_precision, 'Test Data Recall':micro_recall, 'Test Data Specificity':specificity}, ignore_index=True)","41e5abd5":"from xgboost import XGBClassifier\nclassifier = XGBClassifier()\nclassifier.fit(X_train, y_train)\n\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix\")\nprint(cm)\nprint (\"Accuracy on Test Set: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\n\nif (cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0]) == 0:\n    precision_a = 0\nelse:\n    precision_a = cm[0,0]\/(cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0])\n    \nif (cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1]) == 0:\n    precision_b = 0\nelse:\n    precision_b = cm[1,1]\/(cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1])\n    \nif (cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2]) == 0:\n    precision_c = 0\nelse:\n    precision_c = cm[2,2]\/(cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2])\n    \nif (cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3]) == 0:\n    precision_d = 0\nelse:\n    precision_d = cm[3,3]\/(cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3])\nmicro_precision = (precision_a+precision_b+precision_c+precision_d)\/4\nprint(micro_precision)\n\nif (cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3]) == 0:\n    recall_a = 0\nelse:\n    recall_a = cm[0,0]\/(cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3])\n    \nif (cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3]) == 0:\n    recall_b = 0\nelse:\n    recall_b = cm[1,1]\/(cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3])\n    \nif (cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3]) == 0:\n    recall_c = 0\nelse:\n    recall_c = cm[2,2]\/(cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3])\n    \nif (cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3]) == 0:\n    recall_d = 0\nelse:\n    recall_d = cm[3,3]\/(cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3])\n\t\nmicro_recall = (recall_a+recall_b+recall_c+recall_d)\/4\nprint(micro_recall)","85aef84d":"result_comparison = result_comparison.append({'Model':'XG Boost', 'Cross Validation Mean Accuracy': accuracies.mean()*100, 'Cross Validation Standard Deviation': accuracies.std()*100, 'Test Data Accuracy': accuracy_score(y_test, y_pred)*100, 'Test Data Precision':micro_precision, 'Test Data Recall':micro_recall, 'Test Data Specificity':specificity}, ignore_index=True)","e4066e89":"# Params Copied from https:\/\/www.kaggle.com\/para24\/comparing-the-performance-of-12-classifiers\n\nparams = {'learning_rate': 0.014724527414939945,\n          'num_boost_round': 3451,\n          'gamma': 0.4074467665676125,\n          'reg_lambda': 31.082862686792716,\n          'reg_alpha': 0.008543705214252668,\n          'max_depth': 7,\n          'min_child_weight': 3.2435633342899867e-06,\n          'subsample': 0.15432895096353877,\n          'colsample_bytree': 0.7665394913603492}\nclassifier = XGBClassifier(**params,\n                        random_state=0, n_jobs=-1)\nclassifier.fit(X_train, y_train)\n\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix\")\nprint(cm)\nprint (\"Accuracy on Test Set: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\n\nif (cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0]) == 0:\n    precision_a = 0\nelse:\n    precision_a = cm[0,0]\/(cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0])\n    \nif (cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1]) == 0:\n    precision_b = 0\nelse:\n    precision_b = cm[1,1]\/(cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1])\n    \nif (cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2]) == 0:\n    precision_c = 0\nelse:\n    precision_c = cm[2,2]\/(cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2])\n    \nif (cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3]) == 0:\n    precision_d = 0\nelse:\n    precision_d = cm[3,3]\/(cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3])\nmicro_precision = (precision_a+precision_b+precision_c+precision_d)\/4\nprint(micro_precision)\n\nif (cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3]) == 0:\n    recall_a = 0\nelse:\n    recall_a = cm[0,0]\/(cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3])\n    \nif (cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3]) == 0:\n    recall_b = 0\nelse:\n    recall_b = cm[1,1]\/(cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3])\n    \nif (cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3]) == 0:\n    recall_c = 0\nelse:\n    recall_c = cm[2,2]\/(cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3])\n    \nif (cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3]) == 0:\n    recall_d = 0\nelse:\n    recall_d = cm[3,3]\/(cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3])\n\t\nmicro_recall = (recall_a+recall_b+recall_c+recall_d)\/4\nprint(micro_recall)","26a14488":"result_comparison = result_comparison.append({'Model':'XG Boost with Params', 'Cross Validation Mean Accuracy': accuracies.mean()*100, 'Cross Validation Standard Deviation': accuracies.std()*100, 'Test Data Accuracy': accuracy_score(y_test, y_pred)*100, 'Test Data Precision':micro_precision, 'Test Data Recall':micro_recall, 'Test Data Specificity':specificity}, ignore_index=True)","83606d47":"from catboost import CatBoostClassifier\nclassifier = CatBoostClassifier()\n\nclassifier.fit(X_train, y_train, \n                 eval_set=(X_train, y_train),\n                 verbose=False)\n\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix\")\nprint(cm)\nprint (\"Accuracy on Test Set: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\n\nif (cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0]) == 0:\n    precision_a = 0\nelse:\n    precision_a = cm[0,0]\/(cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0])\n    \nif (cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1]) == 0:\n    precision_b = 0\nelse:\n    precision_b = cm[1,1]\/(cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1])\n    \nif (cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2]) == 0:\n    precision_c = 0\nelse:\n    precision_c = cm[2,2]\/(cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2])\n    \nif (cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3]) == 0:\n    precision_d = 0\nelse:\n    precision_d = cm[3,3]\/(cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3])\nmicro_precision = (precision_a+precision_b+precision_c+precision_d)\/4\nprint(micro_precision)\n\nif (cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3]) == 0:\n    recall_a = 0\nelse:\n    recall_a = cm[0,0]\/(cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3])\n    \nif (cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3]) == 0:\n    recall_b = 0\nelse:\n    recall_b = cm[1,1]\/(cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3])\n    \nif (cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3]) == 0:\n    recall_c = 0\nelse:\n    recall_c = cm[2,2]\/(cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3])\n    \nif (cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3]) == 0:\n    recall_d = 0\nelse:\n    recall_d = cm[3,3]\/(cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3])\n\t\nmicro_recall = (recall_a+recall_b+recall_c+recall_d)\/4\nprint(micro_recall)","0f8c0a87":"result_comparison = result_comparison.append({'Model':'CatBoost', 'Cross Validation Mean Accuracy': accuracies.mean()*100, 'Cross Validation Standard Deviation': accuracies.std()*100, 'Test Data Accuracy': accuracy_score(y_test, y_pred)*100, 'Test Data Precision':micro_precision, 'Test Data Recall':micro_recall, 'Test Data Specificity':specificity}, ignore_index=True)","e769f32d":"from lightgbm import LGBMClassifier\nclassifier = LGBMClassifier(n_jobs=-1)\nclassifier.fit(X_train, y_train);\n\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix\")\nprint(cm)\nprint (\"Accuracy on Test Set: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\n\nif (cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0]) == 0:\n    precision_a = 0\nelse:\n    precision_a = cm[0,0]\/(cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0])\n    \nif (cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1]) == 0:\n    precision_b = 0\nelse:\n    precision_b = cm[1,1]\/(cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1])\n    \nif (cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2]) == 0:\n    precision_c = 0\nelse:\n    precision_c = cm[2,2]\/(cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2])\n    \nif (cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3]) == 0:\n    precision_d = 0\nelse:\n    precision_d = cm[3,3]\/(cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3])\nmicro_precision = (precision_a+precision_b+precision_c+precision_d)\/4\nprint(micro_precision)\n\nif (cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3]) == 0:\n    recall_a = 0\nelse:\n    recall_a = cm[0,0]\/(cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3])\n    \nif (cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3]) == 0:\n    recall_b = 0\nelse:\n    recall_b = cm[1,1]\/(cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3])\n    \nif (cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3]) == 0:\n    recall_c = 0\nelse:\n    recall_c = cm[2,2]\/(cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3])\n    \nif (cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3]) == 0:\n    recall_d = 0\nelse:\n    recall_d = cm[3,3]\/(cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3])\n\t\nmicro_recall = (recall_a+recall_b+recall_c+recall_d)\/4\nprint(micro_recall)","84549568":"result_comparison = result_comparison.append({'Model':'Light GBM', 'Cross Validation Mean Accuracy': accuracies.mean()*100, 'Cross Validation Standard Deviation': accuracies.std()*100, 'Test Data Accuracy': accuracy_score(y_test, y_pred)*100, 'Test Data Precision':micro_precision, 'Test Data Recall':micro_recall, 'Test Data Specificity':specificity}, ignore_index=True)","6c8ae3d5":"df_test = pd.read_csv('..\/input\/ldss-v1\/Merge2_Test.csv')\ndf_test.set_index('PropertyId', inplace=True)\ndf_test.drop('Built in year', axis='columns', inplace=True)\nspecificity = 0","0880249d":"df_test","b6155a20":"df_dummy_test = pd.get_dummies(df_test['Location'], drop_first=True)\ndf_dummy_test['Airport Enclave'] = 0\ndf_dummy_test['Al Jalil Garden'] = 0\ndf_dummy_test['Al Rehman Garden'] = 0\ndf_dummy_test['Alfalah Town'] = 0\ndf_dummy_test['Atari Saroba'] = 0\ndf_dummy_test['Bahria Nasheman'] = 0\ndf_dummy_test['Bahria Orchard'] = 0\ndf_dummy_test['Bankers Avenue'] = 0\ndf_dummy_test['Bankers Co-operative Housing Society'] = 0\ndf_dummy_test['Bedian Road'] = 0\ndf_dummy_test['Bin Qasim Town'] = 0\ndf_dummy_test['BOR Board of Revenue Housing Society'] = 0\ndf_dummy_test['C-18'] = 0\ndf_dummy_test['Canal Garden'] = 0\ndf_dummy_test['Cavalry Ground'] = 0\ndf_dummy_test['Chaman Park'] = 0\ndf_dummy_test['Chatha Bakhtawar'] = 0\ndf_dummy_test['Chinar Bagh'] = 0\ndf_dummy_test['Clifton'] = 0\ndf_dummy_test['D-12'] = 0\ndf_dummy_test['D-17'] = 0\ndf_dummy_test['Defence Road'] = 0\ndf_dummy_test['Defence View Society'] = 0\ndf_dummy_test['DHA 11 Rahbar'] = 0\ndf_dummy_test['E-14'] = 0\ndf_dummy_test['E-7'] = 0\ndf_dummy_test['Eden'] = 0\ndf_dummy_test['Emaar Canyon Views'] = 0\ndf_dummy_test['EME Society'] = 0\ndf_dummy_test['F-17'] = 0\ndf_dummy_test['Faisal Town'] = 0\ndf_dummy_test['Faisal Town - F-18'] = 0\ndf_dummy_test['Falcon Complex Faisal'] = 0\ndf_dummy_test['Fateh Garh'] = 0\ndf_dummy_test['Fazaia Housing Scheme'] = 0\ndf_dummy_test['Fort Villas'] = 0\ndf_dummy_test['G-12'] = 0\ndf_dummy_test['G-14'] = 0\ndf_dummy_test['G-6'] = 0\ndf_dummy_test['G-7'] = 0\ndf_dummy_test['Gulfishan Colony'] = 0\ndf_dummy_test['Gulistan-e-Malir'] = 0\ndf_dummy_test['Gulshan-e-Lahore'] = 0\ndf_dummy_test['Hamza Town'] = 0\ndf_dummy_test['I-14'] = 0\ndf_dummy_test['Iqbal Avenue'] = 0\ndf_dummy_test['Islamabad - Murree Expressway'] = 0\ndf_dummy_test['Judicial Town'] = 0\ndf_dummy_test['Kashmir Road'] = 0\ndf_dummy_test['KDA Scheme 1'] = 0\ndf_dummy_test['Lahore Medical Housing Society'] = 0\ndf_dummy_test['Lake City'] = 0\ndf_dummy_test['Lawrence Road'] = 0\ndf_dummy_test['Lawyers Society'] = 0\ndf_dummy_test['Liaquatabad'] = 0\ndf_dummy_test['Main Canal Bank Road'] = 0\ndf_dummy_test['Manghopir Road'] = 0\ndf_dummy_test['Manzoor Colony'] = 0\ndf_dummy_test['Margalla Town'] = 0\ndf_dummy_test['Military Accounts Housing Society'] = 0\ndf_dummy_test['Model Town'] = 0\ndf_dummy_test['Mumtaz City'] = 0\ndf_dummy_test['Muslimabad Society'] = 0\ndf_dummy_test['Mustafa Town'] = 0\ndf_dummy_test['Nasheman-e-Iqbal'] = 0\ndf_dummy_test['National Police Foundation'] = 0\ndf_dummy_test['Naval Housing Scheme'] = 0\ndf_dummy_test['New Karachi'] = 0\ndf_dummy_test['NFC 1'] = 0\ndf_dummy_test['Omega Homes'] = 0\ndf_dummy_test['Paragon City'] = 0\ndf_dummy_test['Park Enclave'] = 0\ndf_dummy_test['Park Road'] = 0\ndf_dummy_test['Park View City'] = 0\ndf_dummy_test['Park View Villas'] = 0\ndf_dummy_test['PECHS'] = 0\ndf_dummy_test['PIA Housing Scheme'] = 0\ndf_dummy_test['Punjab Coop Housing Society'] = 0\ndf_dummy_test['Punjab Govt Employees Society'] = 0\ndf_dummy_test['Punjab Govt Servant Society'] = 0\ndf_dummy_test['Punjab Small Industries Colony'] = 0\ndf_dummy_test['Punjab University Employees Society'] = 0\ndf_dummy_test['Rail Town (Canal City)'] = 0\ndf_dummy_test['Raja Akhtar Road'] = 0\ndf_dummy_test['Revenue Society'] = 0\ndf_dummy_test['River Garden'] = 0\ndf_dummy_test['Samanabad'] = 0\ndf_dummy_test['Sangjani'] = 0\ndf_dummy_test['Shadab Garden'] = 0\ndf_dummy_test['Shah Allah Ditta'] = 0\ndf_dummy_test['Shah Faisal Town'] = 0\ndf_dummy_test['Shah Jamal'] = 0\ndf_dummy_test['Shaheed Millat Road'] = 0\ndf_dummy_test['Shershah Colony - Raiwind Road'] = 0\ndf_dummy_test['Shoukat Town'] = 0\ndf_dummy_test['Simly Dam Road'] = 0\ndf_dummy_test['Sui Gas Housing Society'] = 0\ndf_dummy_test['Tajpura'] = 0\ndf_dummy_test['Tariq Gardens'] = 0\ndf_dummy_test['Tariq Road'] = 0\ndf_dummy_test['UET Housing Society'] = 0\ndf_dummy_test['Wafaqi Colony'] = 0\ndf_dummy_test","721844f6":"df_test.drop('Location', axis='columns', inplace=True)\ndf_test = pd.merge(df_dummy_test, df_test, on='PropertyId')","bad0691b":"X_test_2 = df_test.to_numpy()\nX_test_2 = sc.transform(X_test_2)\nX_test_2","9b80493b":"type(classifier)","4b36c89c":"y_pred_test = classifier.predict(X_test_2)\ny_pred_test","e2624a9b":"df_test_submit = df_test.filter(['PropertyId'])\ndf_test_submit['Price_Category'] = y_pred_test\ndf_test_submit.head()","64dde77e":"df_test_submit.to_csv(\"Light_Dummy.csv\")","d77735f6":"result_comparison","563c3ab3":"# Imports and Data Preperations","7c99853a":"# Logistic Regression","e5e9a98a":"# CatBoost","8d078846":"# Kernel SVM","1ad082a6":"### K-Fold Cross Validation","83f657d5":"# Random Frost","f501633d":"## Test","61e45ce4":"### Micro Precision and Recall","9545a85a":"# Result and Comparison","70cd83a4":"## SVM","0d0e1141":"# XGBoost","f956f7ed":"# Decision Tree","eba4ab75":"### Confusion Matrix","3dbe65eb":"# Naive Bayes","8685834c":" # Light GBM","e7bf0f04":"### XG Boost with Params \n\nCopied from https:\/\/www.kaggle.com\/para24\/comparing-the-performance-of-12-classifiers"}}