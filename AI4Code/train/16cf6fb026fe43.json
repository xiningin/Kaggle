{"cell_type":{"7e228956":"code","77099b92":"code","c14741bb":"code","7de20314":"code","4275ec1d":"code","a029c8f2":"code","1fd968b8":"code","5f3fd08a":"code","8cc8a67b":"code","7e26e111":"markdown","08ca0d6c":"markdown","01b6e5c2":"markdown","2c99cdf5":"markdown","1fb49ee1":"markdown","29d698eb":"markdown","e9f8ccb7":"markdown"},"source":{"7e228956":"X = [[2,6],\n     [1,3],\n     [3,9]]\n\nY = [0,1,1]","77099b92":"# Utilizamos numpy y pandas para el manejo de los datos\nimport numpy as np \nimport pandas as pd\nimport random\nimport math\n\n# Se cargaron los archivos suministrados por la profesora a Kaggle\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","c14741bb":"# Declaramos la clase Perceptron para modelar su comportamiento\nclass Perceptron():\n    def __init__(self, dimension=1, interval=(0,1), learning_rate=1):\n        self.weights = []\n        for i in range(dimension):\n            random.seed()\n            self.weights.append(random.uniform(interval[0],interval[1]))\n        self.input = [0]*dimension\n        self.learning_rate = learning_rate\n        self.output = 0\n\n    def activate(self, weight_vector, x_vector, bias=0):\n        weighted_sum = 0\n        for i in range(len(weight_vector)-1):\n            weighted_sum = weighted_sum + weight_vector[i] * x_vector[i]\n\n#         Se solicita implementar la siguiente funcion de activacion,\n#         pero obtenemos mejores resultados con la funcion lineal\n#         print('Prev Output',self.output)\n#         if weighted_sum > 0:\n#             self.output = 1\n#         else:\n#             self.output = 0\n\n        self.output = weighted_sum\n        return self.output\n\n    def update_weights(self,difference,learning_rate,x_vector):\n        i = 0\n        for i in range(len(self.weights)-1):\n            self.weights[i] = self.weights[i] + learning_rate*difference*x_vector[i]\n            \n\n# Clase Layer o Capa, que agrupa distintos perceptrones.\nclass Layer():\n    def __init__(self,num_neurons=1, input_length=1, interval=(0,1), learning_rate=0.1):\n        self.neurons = [\n            Perceptron(\n                dimension=input_length,\n                interval=interval,\n                learning_rate=learning_rate\n            ) for i in range(num_neurons)\n        ]\n\n    def __repr__(self):\n        return(str(\"Layer of \"+str(len(self.neurons))+\" neurons\"))\n    \n    print(\"Ready\")","7de20314":"# Se obtienen los datos suministrados a Kaggle\n\nprint('Loading training data...')\ntrain_df = pd.read_csv('\/kaggle\/input\/mnist_train.csv',header=None)\n\nprint('OK')","4275ec1d":"# Ejecuci\u00f3n del algoritmo de perceptr\u00f3n para\n# distintas tasas de entrenamiento\n\nfrom sklearn.utils import shuffle\n\ndef train_network(learning_rate, num_samples, epochs):\n\n    nn_layer = Layer(num_neurons=10, input_length=785, interval=(-0.05,0.05), learning_rate=0.01)\n\n    num_attributes = train_df.shape[1]\n\n    X = train_df.iloc[:num_samples,1:num_attributes+1]\/255\n    X = X.values\n    Y = [y[0] for y in train_df.iloc[:num_samples,[0,]].values]\n\n    X = np.array(X)\n    Y = np.array(Y)\n\n    print('Preparing algorithm')\n    print('Using Learning Rate = ',learning_rate)\n    for epoch in range(0,epochs):\n        print('\\tStarting Epoch',epoch)\n        score = 0\n\n        shuffler = np.random.permutation(len(X))\n        X_shuffle = X[shuffler]\n        Y_shuffle = Y[shuffler]\n\n        for i in range(num_samples):\n            if i%1000 == 0:\n                print('\\t\\tTraining sample #',i,'...')\n            y_train = Y_shuffle[i]\n            target_vector = [0]*10\n            target_vector[y_train] = 1\n            x_train = X_shuffle[i]\n\n            neuron_identifier = 0\n            for neuron in nn_layer.neurons:\n                output = neuron.activate(neuron.weights, list(x_train))\n                difference = target_vector[neuron_identifier]-output\n                neuron.update_weights(difference, learning_rate, list(x_train))\n                neuron_identifier+=1\n\n            layer_output = [neuron.output for neuron in nn_layer.neurons]\n            prediction = layer_output.index(max(layer_output))\n\n            validation = target_vector.index(max(target_vector))\n#             print('Expected',target_vector.index(max(target_vector)),'Got',layer_output.index(max(layer_output)))\n            if target_vector.index(max(target_vector)) == layer_output.index(max(layer_output)):\n                score+=1\n\n        print('\\tScore:',str(score\/num_samples*100),'%')\n        \n    return nn_layer","a029c8f2":"# Entrenamos los modelos con distintas tasas de aprendizaje\n# No usaremos la totalidad de los datos para poder\n# suministrar un resultado de forma breve.\n\nlearning_rates = [0.001, 0.01, 0.1]\n\nmodel_dict = {}\n\nfor rate in learning_rates:\n    model_dict[rate] = train_network(learning_rate=rate, num_samples=25000, epochs= 1)\n\n# train_network(0.1,5000,3)","1fd968b8":"# Uso del conjunto de prueba\n\nprint('Loading testing data...')\ntest_df = pd.read_csv('\/kaggle\/input\/mnist_test.csv',header=None)\n\nprint('OK')","5f3fd08a":"def test_model(nn_layer,learning_rate):\n    num_attributes = test_df.shape[1]\n    num_samples = test_df.shape[0]\n\n    X = test_df.iloc[:num_samples,1:num_attributes+1]\/255\n    X = X.values\n    \n    Y = [y[0] for y in test_df.iloc[:num_samples,[0,]].values]\n\n    X = np.array(X)\n    Y = np.array(Y)\n\n    score = 0\n\n    shuffler = np.random.permutation(len(X))\n    X_shuffle = X[shuffler]\n    Y_shuffle = Y[shuffler]\n\n    for i in range(num_samples):\n        if i%1000 == 0:\n            print('\\t\\tTesting sample #',i,'...')\n        y_train = Y_shuffle[i]\n        target_vector = [0]*10\n        target_vector[y_train] = 1\n        x_train = X_shuffle[i]\n\n        neuron_identifier = 0\n        for neuron in nn_layer.neurons:\n            output = neuron.activate(neuron.weights, list(x_train))\n            difference = target_vector[neuron_identifier]-output\n            neuron.update_weights(difference, learning_rate, list(x_train))\n            neuron_identifier+=1\n\n        layer_output = [neuron.output for neuron in nn_layer.neurons]\n        prediction = layer_output.index(max(layer_output))\n\n        validation = target_vector.index(max(target_vector))\n#             print('Expected',target_vector.index(max(target_vector)),'Got',layer_output.index(max(layer_output)))\n        if target_vector.index(max(target_vector)) == layer_output.index(max(layer_output)):\n            score+=1\n\n    print('\\t Test score:',str(score\/num_samples*100),'%')","8cc8a67b":"test_model(model_dict[0.001],0.001)","7e26e111":"## 2. Implemente su propio perceptr\u00f3n para multiples clases en el lenguaje de su preferencia.\n\n## 3. Entrene 10 perceptrones que en conjunto puedan clasificar los d\u00edgitos en la base de datos MNIST. \n\n\nA continuaci\u00f3n se presenta el c\u00f3digo en distintos m\u00f3dulos para implementar la red neuronal artificial de una sola capa. Cada segmento puede ejecutarse de forma individual, lo que permite un mejor manejo del tiempo de ejecuci\u00f3n.","08ca0d6c":"## Demostraci\u00f3n:\n\nEl conjunto X es linealmente separable si existen vectores Wk, con k = 1,...,K tal que si una entrada X que pertenece a la clase Ci (i-\u00e9sima clase): \n\n### Wi \\* X > Wj \\* X para todo j diferente de i\n\n\nEntendamos '\\*' tambi\u00e9n como multiplicador de vectores\n\nSupongamos que existen vectores W1,W2 (ya que son dos clases C0=0 y C1=1)\n\nSe supone que si X1 pertenece a la clase C0\n\n### W1 \\* X1 > W2 \\* X1\n\nLuego, vemos que si X2 pertenece a la clase C1\n\n### W2 \\* X2 > W1 \\* X2\n\ny X3 pertenece a la clase C0\n\n### W1 \\* X3 > W2 \\* X3\n\nTenemos las siguientes relaciones:\n\n### W1 \\* [2,6] > W2 \\* [2,6] (I)\n### W2 \\* [1,3] > W1 \\* [1,3] (II)\n### W1 \\* [3,9] > W2 \\* [3,9] (III)\n\nFactorizando (I)\n### W1 \\* 2\\*[1,3] > W2 \\* 2\\*[1,3]\n### W1 \\* [1,3] > W2 \\* [1,3]\n\n\nTenemos que (I) ^ (II) implica que W1 > W2 (IV)\n### W1 \\* [1,3] > W2 \\* [1,3] (I)\n### W2 \\* [1,3] > W1 \\* [1,3] (II)\n\nPero \n### W1 \\* X3 > W2 \\* X3 (III)\n### W1 = W2 (IV)\n\nEntonces\n### W1 \\* X3 > W1 \\* X3 (III)\n\nLo cual es una contradiccion. Entonces el conjunto de observaciones X y clases Y no es linealmente separable.","01b6e5c2":"Se solicita para la asignaci\u00f3n lo siguiente:\n## 1. Suponga que se deasea entrenar un perceptr\u00f3n de dos entradas al siguiente problema de clasificaci\u00f3n\n","2c99cdf5":"### Precision en prueba de 84.15%","1fb49ee1":"## Conclusi\u00f3n sobre las tasas de aprendizaje:\n\n\n### Parece que las tasas de aprendizaje m\u00e1s bajas son m\u00e1s efectivas para este problema particular, ya que el ajuste de los pesos sin\u00e1pticos es m\u00e1s minucioso\n\n\nCon una tasa de 0.001, tuvimos una precisi\u00f3n de 83.7%.\nCon 0.01, 69.2%.\nFinalmente, con 0.001, alrededor de 10%\n\n## Resultados: \n\nMejor ejecuci\u00f3n:\n- 83.70% de precisi\u00f3n\n- Uso de funci\u00f3n de activaci\u00f3n lineal\n- Tasa de aprendizaje de 0.001","29d698eb":"## Demuestre que el perceptr\u00f3n no puede aprender esta tarea.","e9f8ccb7":"UNIVERSIDAD SIMON BOLIVAR\n\nCO-6612, Introducci\u00f3n a las redes neuronales\n\nTarea 2: Perceptr\u00f3n\n\n\nDaniel Francis. 12-10863"}}