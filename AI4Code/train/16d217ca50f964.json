{"cell_type":{"3aa962f6":"code","f988953b":"code","cb7ef3a4":"code","dba4f506":"code","32816063":"code","b58901e0":"code","42121342":"code","e49c1f9c":"code","f8ae24db":"code","4f4acf7e":"code","20f638c6":"code","14791d60":"code","ae9d7de8":"code","a5d06fe0":"code","19194b92":"code","e08c296f":"code","f0ab390b":"code","6ccf8ab8":"code","7ba38d1e":"code","bdad7e75":"code","7aa8ff74":"code","ab420ad0":"code","e6359e74":"code","ac0f4a96":"code","8d49db07":"code","be86fb2d":"code","f5315cd7":"code","46382277":"code","92bb9d56":"code","6ec63df2":"code","9b3c47a3":"code","667412b0":"code","44783dd3":"code","d0210230":"code","69adb2e3":"code","83e01cd5":"code","b8a9210e":"code","2426f94e":"code","f52ecffb":"code","555cd446":"code","495917a7":"code","aa93cc3d":"code","d6df51c0":"code","74eb1b0e":"code","2b60c339":"code","d8ce1947":"code","715b59b5":"code","eb356d1a":"code","e50c9738":"code","56038b80":"code","f10ded3c":"code","b764891e":"code","d94c92c7":"code","013a1db0":"code","74a4607d":"code","b1385a9d":"markdown","b0470116":"markdown","2e7428fc":"markdown","25b92c41":"markdown","e5756b3b":"markdown","fd257533":"markdown","79b638d6":"markdown","2fd8bf0c":"markdown","8f206dcd":"markdown","6a83c14b":"markdown","96928d18":"markdown","11513ed9":"markdown","d55b2986":"markdown","9f19140a":"markdown","87f7c1e8":"markdown","94e187a2":"markdown","53f37657":"markdown","5b602749":"markdown","cc7dc51f":"markdown","8f6cfba8":"markdown","b4d40ebd":"markdown","2c09bfcc":"markdown","5103fef7":"markdown","332c2f4d":"markdown","6dc03d80":"markdown","264673bc":"markdown","89426734":"markdown","50395037":"markdown","0ed3af65":"markdown","51e948c9":"markdown","9854a795":"markdown","ad8c7bd8":"markdown","975617e6":"markdown","cbca651b":"markdown","270a5b98":"markdown","b4b5c792":"markdown","99a43a10":"markdown","d91c60ba":"markdown","5c5350c3":"markdown","6749f489":"markdown","2bda0261":"markdown","99406e49":"markdown","213f247f":"markdown","8f8c5bdc":"markdown","18f08b65":"markdown","b362a673":"markdown","3427c44b":"markdown","183de2b5":"markdown"},"source":{"3aa962f6":"import warnings # to ignore warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd # data processing,\ndf = pd.read_csv(\"..\/input\/parkinson-disease-detection\/Parkinsson disease.csv\")","f988953b":"# Displaying the head of the dataset\ndf.head(10)","cb7ef3a4":"# Displaying the shape and datatype for each attribute\n\nprint('Shape of the dataset: ',df.shape,'\\n\\n')\n\ndf.info()","dba4f506":"# Dispalying the descriptive statistics describe each attribute\n\ndf.describe().T","32816063":"# Checking Null or Empty Values\n\ndf.isna().sum()","b58901e0":"df = df.drop('name',1)  # as we said earlier dropping the 'name' column as it is not significant for model building","42121342":"# Plotting histogram of the columns to study the data distribution\n\nimport seaborn as sns  #importing seaborn for plotting\nimport matplotlib.pyplot as plt   #importing matplotlib\n\n\nk=1\nplt.figure(figsize=(20,30))\n\n# using for loop to iterate over all the columns in the dataframe and plot the histogram of those\n\nfor col in df.columns[0:]:\n    plt.subplot(6,4,k)\n    plt.hist(df[col],color='red', edgecolor = 'black', alpha = 0.5)\n#     sns.distplot(df[col],kde=False)\n    plt.title(col)\n    k=k+1","e49c1f9c":"# Using histogrm from seaborn plotting of spread1 for status column\n\nsns.distplot( df[df.status == 0]['spread1'],color='red'); # spread1 for who are normal\nsns.distplot( df[df.status == 1]['spread1'],color='blue'); # spread1 for who have PD","f8ae24db":"fig, ax = plt.subplots(1,2,figsize=(15,6))\n\n# Bivariate Boxplot to see the difference between NHR and HNR\nsns.boxplot(x=df['status'],y=df['NHR'],ax=ax[0]);   # boxplot of status Vs NHR\nsns.boxplot(x=df['status'],y=df['HNR'],ax=ax[1]);   # boxplot of status Vs NHR","4f4acf7e":"plt.figure(figsize=(8,8))\nplt.pie(df.status.value_counts(),colors=['lightblue','yellow'],explode=[0,0.02],autopct='%1.0f%%',labels=['0(healthy)',\"1(parkinson's)\"]);","20f638c6":"# checking the correlation of dataset \nfig, ax = plt.subplots(figsize=(20, 20))\nax = sns.heatmap(df.corr(),cmap=\"YlGnBu\",square=True,annot = True,linewidth=0.2)","14791d60":"# correlation coefficient values in each attributes.\n\ncorrelation_values=df.corr()['status']\npd.DataFrame(correlation_values.sort_values(ascending=False))","ae9d7de8":"from sklearn.model_selection import train_test_split\n\nx = df.drop('status',1)  # predictors\ny = df.status            # target attributez\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state = 42)  # making 70:30 split\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","a5d06fe0":"## as we checked above there are no null values in the dataset","19194b92":"### As the almost columns in the dataset are skewed so we are going to use MinMax scaler","e08c296f":"from sklearn.preprocessing import MinMaxScaler\n\nrc = MinMaxScaler() # instantiating the object for minmaxscaler\n\ncolumns = list(x_train.columns)  # storing the columns\n\nx_train_scaled = pd.DataFrame(rc.fit_transform(x_train))\nx_train_scaled.columns = columns  # assigning the columns after scaling the values\n\nx_test_scaled = pd.DataFrame(rc.fit_transform(x_test))\nx_test_scaled.columns = columns  # assigning the columns after scaling the values","f0ab390b":"# **Logistic Regression is a classification algorithm. \n# **It is used to predict a binary outcome (1 \/ 0, Yes \/ No, True \/ False) given a set of independent variables,\n\nfrom sklearn.linear_model import LogisticRegression\n\n# create an instance for LogisticRegression\nLogistic = LogisticRegression(solver=\"liblinear\")\n\n# fit the model\nLogistic.fit(x_train_scaled, y_train)\n\n# predict on created model\nlogistic_predict = Logistic.predict(x_test_scaled)","6ccf8ab8":"# checking the score of the testset\nacc_logistic_test = Logistic.score(x_test_scaled, y_test)*100","7ba38d1e":"# storing accuracy results of each model in the dataframe for final comparision \nresult_df = pd.DataFrame({'Model': ['Logistic Regression'], 'Accuracy' : [acc_logistic_test]}).drop_duplicates()\nresult_df","bdad7e75":"# Bayes Theorem assumes predictors or input features are independent of each other,\n\nfrom sklearn.naive_bayes import GaussianNB # using Gaussian algorithm from Naive Bayes as all the columns are numerical\n\n# create an instance for GaussianNB\nnaive_model = GaussianNB()\n\n# fit the model\nnaive_model.fit(x_train_scaled, y_train)\n\n# prediction using created model\nnaive_predict = naive_model.predict(x_test_scaled)","7aa8ff74":"# checking the score of the test set\nacc_naive_test = naive_model.score(x_test_scaled, y_test)*100","ab420ad0":"# storing accuracy results of each model in the dataframe for final comparision\ntempResult_df = pd.DataFrame({'Model': ['Naive Bayes'], 'Accuracy' : [acc_naive_test]})\nresult_df = pd.concat([result_df,tempResult_df]).drop_duplicates()\nresult_df","e6359e74":"from sklearn.neighbors import KNeighborsClassifier\n\n# create instance for KNeighborsClassifier and using k value = 5\nknn_model = KNeighborsClassifier(n_neighbors=5)\n\n# fit the model\nknn_model.fit(x_train_scaled, y_train)\n\n# prediction using created model\nknn_predict = knn_model.predict(x_test_scaled)","ac0f4a96":"# checking the score of the test set\nacc_knn_test = knn_model.score(x_test_scaled, y_test)*100 ","8d49db07":"# storing accuracy results of each model in the dataframe for final comparision \ntempResult_df = pd.DataFrame({'Model': ['KNN Scaled'], 'Accuracy' : [acc_knn_test]})\nresult_df = pd.concat([result_df,tempResult_df]).drop_duplicates()\nresult_df\nresult_df","be86fb2d":"from sklearn.neighbors import KNeighborsClassifier\n\n# create instance for KNeighborsClassifier and using k value = 5\nknn_model2 = KNeighborsClassifier(n_neighbors=5)\n\n# fit the model\nknn_model2.fit(x_train, y_train)  # fiiting the model on data for which the scaling operation is not made\n\n# prediction using created model\nknn_predict2 = knn_model2.predict(x_test)\nacc_knn_test2 = knn_model2.score(x_test, y_test)*100 ","f5315cd7":"# storing accuracy results of each model in the dataframe for final comparision \ntempResult_df = pd.DataFrame({'Model': ['KNN Not Scaled'], 'Accuracy' : [acc_knn_test2]})\nresult_df = pd.concat([result_df,tempResult_df]).drop_duplicates()\nresult_df\nresult_df","46382277":"# Decision tree algorithm falls under the category of supervised learning. \n# Decision tree uses the tree representation to solve the problem in which each leaf node corresponds to a class label and attributes are represented on the internal node of the tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n# using entropy technique we are making splits\ndecision_tree = DecisionTreeClassifier(criterion = 'gini', max_depth = 6, random_state = 100) \n\n# fitting the model\ndecision_tree.fit(x_train_scaled, y_train) \n\n# predicting the model on test set\ndescion_pred = decision_tree.predict(x_test_scaled)","92bb9d56":"# checking the score of the testset\nacc_DT_test = decision_tree.score(x_test_scaled, y_test)*100","6ec63df2":"# storing accuracy results of each model in the dataframe for final comparision \ntempResult_df = pd.DataFrame({'Model': ['Decision Tree'], 'Accuracy' : acc_DT_test})\nresult_df = pd.concat([result_df,tempResult_df])\nresult_df","9b3c47a3":"from mlxtend.classifier import StackingClassifier  # importing stacking classifier package","667412b0":"from sklearn.svm import SVC  # importing SVM classifier\n\n# creating four individual classification models\nmodel1 = DecisionTreeClassifier(criterion = 'entropy',max_depth = 6)\nmodel2 = KNeighborsClassifier(n_neighbors=5)\nmodel3 = GaussianNB()\nmodel4 = SVC(C = 10,gamma=0.01)\n\n# giving logistic regression as meta classifier\/model\nmeta_model = LogisticRegression()","44783dd3":"# calling stacking classifier with all the base models and meta model\nstcl = StackingClassifier(classifiers = [model1,model2,model3,model4], meta_classifier = meta_model)","d0210230":"from sklearn.model_selection import cross_val_score\n\n# loop through all the models created with meta model\nfor models, label in zip ([model1,model2,model3,model4, stcl], ['DecisionTreeClassifier','KNN','NaiveBayes','SVM','StackingClassifier']):\n    \n    scores = cross_val_score (models, x, y, cv=10, scoring='accuracy')\n    print(scores,label)\n#     print(\"Accuracy:\",scores.mean(),label)","69adb2e3":"# storing accuracy results of each model in the dataframe for final comparision \ntempResult_df = pd.DataFrame({'Model': ['Stacking Classifier'], 'Accuracy' : scores.mean()*100})\nresult_df = pd.concat([result_df,tempResult_df])\nresult_df","83e01cd5":"from sklearn.ensemble import RandomForestClassifier  # importing random forest classifier\n\nrfcl = RandomForestClassifier() # calling the randomforest with 20 decision trees\nrfcl = rfcl.fit(x_train_scaled, y_train)  # fitting the model","b8a9210e":"rfcl.score(x_test_scaled, y_test)  # score of train and test set","2426f94e":"# Importing classification report and confusion matrix from sklearn metrics\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score\nrf_pred = rfcl.predict(x_test_scaled)","f52ecffb":"# Let's check the report of our default model\nprint(classification_report(y_test,rf_pred))","555cd446":"# Printing the accuracy score of actual values and predictions\nacc_rf = accuracy_score(y_test,rf_pred)*100\nprint('Accuracy score of Random Forest Classifier: ',acc_rf,'%','\\n')\n\n# Printing confusion matrix\ncm = confusion_matrix(y_test,rf_pred)\n\ndf1 = pd.DataFrame(cm,columns=['No','Yes'], index = ['No','Yes'])\nprint('\\t\\tConfusion matrix')\nsns.heatmap(df1,annot=True,cbar=False);","495917a7":"df1","aa93cc3d":"# storing accuracy results of each model in the dataframe for final comparision\ntempResult_df = pd.DataFrame({'Model': ['Random Forest'], 'Accuracy' : acc_rf})\nresult_df = pd.concat([result_df,tempResult_df]).drop_duplicates()\nresult_df","d6df51c0":"# Creating the parameter grid based on the results of random search \nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'max_depth': [2,4,8,10],\n    'n_estimators': [50,100,200, 300], \n    'max_features': [5, 10, 15]\n    }\n\n# Create a base model\nrf = RandomForestClassifier(random_state=100)\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1,verbose = 1,scoring='accuracy')","74eb1b0e":"grid_search.fit(x_train_scaled, y_train);","2b60c339":"# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","d8ce1947":"rf_tuned = RandomForestClassifier(max_depth= 8, max_features= 5, n_estimators= 50)","715b59b5":"rf_tuned.fit(x_train_scaled,y_train)","eb356d1a":"rf_tuned.score(x_test_scaled,y_test)  ","e50c9738":"result_df","56038b80":"### Random forest performing better","f10ded3c":"result_df","b764891e":"print('RandomForest train accuracy',rf_tuned.score(x_train_scaled,y_train)*100)\nprint('KNN train accuracy',knn_model.score(x_train_scaled,y_train)*100)","d94c92c7":"from sklearn import metrics\nprint('KNN')\npd.DataFrame(metrics.confusion_matrix(y_test,knn_predict))","013a1db0":"print('Random Forest')\npd.DataFrame(metrics.confusion_matrix(y_test,rf_pred))","74a4607d":"### Bar plot to show the models accuracyfig=plt.figure(figsize=(12,5))\nfig.suptitle('All the models comparision')\nsns.barplot(result_df['Model'],result_df['Accuracy']);","b1385a9d":"#### 6.2 Naive Bayes classifier","b0470116":"## 5. Prepare the data for training - Scale the data if necessary, get rid of missing values (if any) etc","2e7428fc":"#### 6.1 Logistic Regression","25b92c41":"### We can observe confusion matrix of both the models, there are very less number of mis-classifications\n\n\n## Ultimately we can conclude that based on scaling operation, model performance differs but looking at the confusion matrix there is one more misclassification in the random forest than KNN but we can improve the score of RF by tuning the parameters again and we can get more accuracy than KNN.\n\n### As we are dealing with the medical domain In the real-world, predicting the person as not having the disease but when he\/she actually has disease is more dangerous than predicting the person has a disease when he\/she actually don't have it. Hence it is more important for us to identify True Positive.\n\n### Since there are zero mis-classifications(True Positive Rate is 100%) on predicting the 1's for both the models and it is more accurate,we can say both of them performs well in the production.\n\n### As per my observation i can say that 'Random Forest' is the best model and performs well in this dataset.","e5756b3b":"#","fd257533":"### We can see that as the recall score for predicting the 1's is 100%, so zero misclassifications on predicting 1's","79b638d6":"#### -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","2fd8bf0c":"#### 8.1 Random Forest","8f206dcd":"### We can see there are no null values in the dataset so now we can safely go ahead","6a83c14b":"### We are going to build the classification and ensemble of models using the above dataset classify the patients into the respective labels using the attributes from their voice recordings","96928d18":"#### This is a clasification problem,that is why classifier works better like K-NN as you can see 91% in the test set","11513ed9":"## Accuracies of all the Models implemented so far","d55b2986":"#### So here we are doing kfold cross validation by making 10 splits and taking mean accuracy of all the individual model using we are using cross validation score.\n\n#### Creation of individual models we got good accuracy as you can see above but using stacking technique we are combining individual weak learners and we are slightly getting better accuracy but not more.","9f19140a":"#### 6.3 K-Nearest Neighberhood Classifier","87f7c1e8":"### Spread1 and Spread2 columns looks normally distributed and we are also going to see how its impacting on target attribute","94e187a2":"## 8. Train at least one standard Ensemble model - Random forest, Bagging, Boosting etc, and note the accuracy","53f37657":"## 2. Eye-ball raw data to get a feel of the data in terms of number of records, structure of the file, number of attributes, types of attributes and a general idea of likely challenges in the dataset.","5b602749":"#### 6.4 Decision Tree classifier","cc7dc51f":"## * The measure of tonal component of frequency is shown above i.e (NHR, HNR)\n\n### The value NHR is right skewed for there are so many observations in the area, but they seem to be with very minimal values. The maximum number of observations is between 0 and 0.04. \n\n### The value HNR looks like slightly normally distributed, but it look there seems to be a slight negative skewness in the data.\n\n","8f6cfba8":"### Univariate analysis","b4d40ebd":"### There are 196 records and 24 columns","2c09bfcc":"## * The measures of vocal fundamental frequency are shown in the first 3 histograms\n\n### There is a positive skewness for minimum vocal fundemental frequency(MDVP:Flo(Hz)) with more high values between 75Hz and 125Hhz. \n\n### The average vocal frequency is almost normally distributed(MDVP:Fo(Hz)) with more values ranging 115Hz and 125Hz. We can see that big bar is there.\n\n### The high vocal frequency(MDVP:Fhi(Hz)) does not have any skewness, but some range of values are at the right most tail and more values are at left.","5103fef7":"### From the above distribution we can observe the followings\n\n### * Spread1 is normally distributed betweeen the person who is normal and who have PD\n\n### * more person who have spread1 between -8.5 and -7.5 has PD\n\n### * more person who have spread1 between -6.5 and -5 are normal","332c2f4d":"### We can see that there are more number of healthy patients in the dataset than who's having parkinson's disease i.e.(75:25)","6dc03d80":"### We can clearly see that there are number of columns which are highly positively correlated to each other and almost all the columns are highly negatively correlated to HNR column\n\n### MDVP:Jitter(%) has a very high correlation with MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP\n\n### MDVP:Shimmer has a very high correlation with MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA this may be because they are related to each other. \n\n### The target variable status has a weak positive corelation with all the variables in the dataset","264673bc":"## Grid Search to Find Optimal Hyperparameters","89426734":"## 9. Comparing all the models and pick the best one among them","50395037":"#### K-NN is a supervised algorithm, it is non-parametric and lazy (instance-based) it does not care about dependency of the variables.For KNN the input consists of the k closest training examples in the feature space.If k = 1, then the object is simply assigned to the class of that single nearest neighbor.","0ed3af65":"## 6. Train at least 3 standard classification algorithms - Logistic Regression, Naive Bayes\u2019, SVM, K-NN etc.","51e948c9":"## 1. Load the dataset","9854a795":"### Almost all the columns' mean is greater than the median(50%)\n\n### The mean is greater we can say that there are more number of columns are highly skewed to the right.\n\n\n","ad8c7bd8":"### From the above Data Frame we can observe that 'Random Forest' and 'KNN Scaled' models are having highest accuracy i.e. 91.52%(KNN) and 89.83%(RF) compared to all other models.\n\n### All other models have 80+ accuracy\n\n### If we perform the scaling operation KNN tends to increase the accuracy and if we dont scale, decreases the accuracy.\n\n### As per my observation here KNN with scaled data performs better than any other model as it does not care about dependency of the variables.For KNN the input consists of the k closest training examples in the feature space. ### If k = 1, then the object is simply assigned to the class of that single nearest neighbor, but we can implement many other models also like in the boosting so that we can get the better accuracy.\n\n### If we don't perform scaling we can say 'Random Forest' is the best model as we can observe it gives higher accuracy after tuning the parameters as it consists a large number of decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model\u2019s prediction.\n\n### I can say both the models i.e. KNN and Random Forest generalizes well in the production neither overfitting nor underfitting.\n","975617e6":"### There are 24 attributes with one dependent attribute i.e. 'status', except that all are 'float' datatype and there are no null values in the dataset\n\n### Encoding the Categorical values into numerical values is not required in this dataset. Because all values we have floating and integer type only. we have name column as a categorical values but we are not going to use that column in model prediction as it doesn't hold any value.","cbca651b":"#### Stacking is ensemble learning technique where the predictions of multiple classifiers are used as new features to train a meta-classifier. The meta-classifier can be any classifier of choice.\n\n#### The predictions of individual weak learners get stacked to the meta classifier and are used as features to train the meta-classifier which makes the final prediction","270a5b98":"### * NHR,HNR - Two measures of ratio of noise to tonal components in the voice\n\n### * As i studied lower NHR and Higher HNR indicate superior voice quality.\n\n### * People who have PD(status=1) has higher NHR and opposite for normal people. And we can also observe the outliers that there are many people who has higher level of NHR. \n\n### * Also loking at the HNR ratio people who have PD(status=1) has lower levels\n","b4b5c792":"### Above is the correlation values in descending order, we have correaltion values in each attribute\n### we can see that the below columns in the dataframe have lower corelation to the target attribute","99a43a10":"## 7. Train a meta-classifier and note the accuracy on test data","d91c60ba":"## * MDVP:Shimmer, MDVP:Shimmer(dB), Shimmer:APQ3, Shimmer:APQ5, MDVP:APQ,           Shimmer:DDA \n\n### For all of the above columns ditribution, we can observe that the measure of variation in amplitude is positively skewed.\n","5c5350c3":"#### -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n### Observations\n","6749f489":"### Building KNN model without scaling the data","2bda0261":"#### -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","99406e49":"### Bivariate analysis","213f247f":"## 4. Split the dataset into training and test set in the ratio of 70:30 (Training:Test)","8f8c5bdc":"#### -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","18f08b65":"## The target column distribution.","b362a673":"## 3. Using univariate & bivariate analysis to check the individual attributes for their basic statistics such as central values, spread, tails, relationships between variables etc.","3427c44b":"## Dataset : Parkinsons Disease Data Set\n## Domain : Medical","183de2b5":"### 2.1 Check shape of the data"}}