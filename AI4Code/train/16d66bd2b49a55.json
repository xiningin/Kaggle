{"cell_type":{"904b4b06":"code","4061f4c3":"code","4393caee":"code","dfa56dd7":"code","e814109e":"code","5cee817d":"code","5e26d1cb":"code","5ad06baf":"code","8b125f6a":"code","1c62fe93":"code","7106dfc1":"code","ffb1897e":"code","521d079c":"code","34badcc2":"code","1286a123":"code","fd75a780":"code","909e773e":"code","949b8bab":"code","85cf3fed":"code","6e142843":"code","7a9b9fb4":"code","f942d9df":"code","aceda252":"code","3723a7c3":"code","ebaf0201":"code","2fb14e56":"code","2c3f60a4":"code","b4448b14":"code","2c883225":"code","037d3c49":"code","23366dd5":"code","da95399e":"code","da6de727":"code","5218f91b":"code","88fd2b32":"code","a7dbcf31":"code","7b0d0f75":"code","f36525a9":"code","d1a67218":"code","e1977cb8":"code","6c5a628b":"code","38321646":"code","3e35edee":"markdown","0cef1547":"markdown","c0690d2b":"markdown","7f8409b6":"markdown","b16e4be7":"markdown","9286157f":"markdown","cbd810fd":"markdown","3e4ab307":"markdown","e8f9c140":"markdown","69913a39":"markdown","da511e3b":"markdown","62458c56":"markdown","388380d9":"markdown","11cba179":"markdown","a25a007b":"markdown","f3a5ba36":"markdown","02fddae1":"markdown","70a262ad":"markdown","e06ee0ea":"markdown","c95874d8":"markdown","294bfb1b":"markdown","938609f5":"markdown","db272f36":"markdown","67a64f8e":"markdown","990b26c3":"markdown","39a481e6":"markdown","8df29552":"markdown","6d8701e8":"markdown"},"source":{"904b4b06":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4061f4c3":"\n\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt # for plotting graphs\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split # for splitting the dataset\nfrom sklearn.metrics import mean_absolute_error      # for finding mean squared error\n\n# ml algorithms\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor    # for stacking models\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\n\n# preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# hyperparameter tuning\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n# feature selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.ensemble import ExtraTreesClassifier\n","4393caee":"import os\nprint((os.listdir('..\/input\/')))","dfa56dd7":"df_train = pd.read_csv('..\/input\/wecrec2020\/Train_data.csv')\ndf_test = pd.read_csv('..\/input\/wecrec2020\/Test_data.csv')","e814109e":"df_test.head()","5cee817d":"df_train.head()","5e26d1cb":"df_train.describe()","5ad06baf":"df_test.describe()","8b125f6a":"test_index=df_test['Unnamed: 0']","1c62fe93":"def checking_null(X):\n    return X.isnull().values.any()\n    \nprint(checking_null(df_train))\nprint(checking_null(df_test))","7106dfc1":"df_train.drop(['F1', 'F2'], axis = 1, inplace = True)\n","ffb1897e":"continuous = ['F6','F10','F13','F14','F15','F16','F17']      \ncategorical = ['F3','F4','F5','F7','F8','F9','F11','F12']","521d079c":" \ndef correlation(X,cols):\n    corr = X[cols].corr()\n    return corr.style.background_gradient(cmap='coolwarm')\n","34badcc2":"# finding the correlation matrix of the dataset\ncol = df_train.columns\ncorrelation(df_train,col)","1286a123":"def feature_engineering_train(X):\n    X['F18'] = (X['F15'] + X['F13'])\/2\n    \n    # rearranging the columns to the normal order\n    cols = X.columns.tolist()\n    cols = ['F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F15','F16','F17','F18','O\/P']\n    X = X[cols]\n    return X\ndef feature_engineering_test(X):\n    X['F18'] = (X['F15'] + X['F13'])\/2\n   \n    \n    return X\n    \ndf_train = feature_engineering_train(df_train)\ndf_test = feature_engineering_test(df_test)","fd75a780":"X = df_train.loc[:, 'F3':'F18']\ny = df_train.loc[:, 'O\/P']\ntest_X = df_test.loc[:,'F3':'F18']","909e773e":"\n#applying SelectKBest class to extract best features\nbestfeatures = SelectKBest(score_func=f_classif, k=15)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Feature_Importance']  #naming the dataframe columns\nprint(featureScores.nlargest(15,'Feature_Importance'))  #printing the features in the order of their score\n\n","949b8bab":"featureScores.plot(kind = 'bar',x = 'Specs',y = 'Feature_Importance')\nplt.show()","85cf3fed":"# finding the distribution of continuous variables \nfor col in continuous:\n    sns.distplot(X[col], hist_kws=dict(color='plum',    edgecolor=\"k\", linewidth=1))\n    plt.show()","6e142843":"# visualizing the categroical variables v\/s the target variable\nfor feature in categorical:\n    data=df_train.copy()\n    data.groupby(feature)['O\/P'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('O\/P')\n    plt.title(feature)\n    plt.show()","7a9b9fb4":"def drop_columns(X):\n    \n    \n    X.drop(['F6','F10','F14','F15','F13'], axis = 1, inplace = True)\n    return X\ndrop_columns(X)\ndrop_columns(test_X)\n","f942d9df":"def standardising(X,column):\n    \n    scaler = StandardScaler()\n   \n    X_continuous_std = pd.DataFrame(data=scaler.fit_transform(X[column]), columns=column)   # standardizing the \n                                                                                                    # continuous variables\n\n    X = pd.merge(X_continuous_std, X[categorical+continuous - column], left_index=True, right_index=True)   # replacing numeric columns with\n                                                                                        # standardized entries\n    cols  = X.columns.tolist()\n\n    cols = ['F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F15','F16','F17']  # rearranging the columns \n    X = X[cols]                                                                                  # back to normal order\n    \n    return X","aceda252":"# since standardizing reduced the performance of the model, it is not done\n\"\"\"\nX = standardising(X,continuous)\ntest_X = standardising(test_X,continuous)\"\"\"","3723a7c3":"print(X)","ebaf0201":"def one_hot_encode(X_train,categorical,numerical):\n    low_cardinality_cols = [col for col in categorical if X_train[col].nunique() < 10] # removing categorical variables\n                                                                                       # with many unique values\n    X_train = X_train[categorical + numerical ]\n    OHE=OneHotEncoder(handle_unknown='ignore',sparse=False)\n\n    cat_features_train=pd.DataFrame(OHE.fit_transform(X_train[low_cardinality_cols]))  # encoding the categorical variables\n    \n    cat_features_train.index=X_train.index\n    \n    print(cat_features_train.shape)\n\n    num_train=X_train[numerical]                  \n \n\n    X_train=pd.concat([num_train,cat_features_train],axis=1)                        \n    \n    return X_train\n   \n        ","2fb14e56":"# since one-hot encode reduced the performance of the model, it was not used.\n\"\"\"\nX = one_hot_encode(X,categorical,continuous)\ntest_X = one_hot_encode(test_X,categorical,continuous)\nX, test_X = X.align(test_X,join='inner',axis=1)\n\"\"\"","2c3f60a4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25,shuffle=True,random_state = 42)","b4448b14":"print(test_X)\nprint(X_train)","2c883225":"def hyperparameter_tuning_random_forest(X_train,y_train):\n    \n    # checking with different possible values for each hyperparameter \n    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n   \n    max_features = ['auto', 'sqrt']\n   \n    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n    max_depth.append(None)\n   \n    min_samples_split = [2, 5, 10]\n   \n    min_samples_leaf = [1, 2, 4]\n   \n    bootstrap = [True, False]\n    # Creating the random grid\n    random_grid = {'n_estimators': n_estimators,\n                   'max_features': max_features,\n                   'max_depth': max_depth,\n                   'min_samples_split': min_samples_split,\n                   'min_samples_leaf': min_samples_leaf,\n                   'bootstrap': bootstrap}\n    # Use the random grid to search for best hyperparameter\n    # First create the base model to tune\n    rf = RandomForestRegressor()\n    # Random search of parameters, using 3 fold cross validation, \n    # search across 25 different combinations, and use all available cores\n    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 25, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n    # Fit the random search model\n    rf_random.fit(X_train, y_train)\n    return rf_random","037d3c49":"# the best values for hyperparameters were found by running this and those values have been used in the model. hence currently commented out\n\n#best_parameters = hyperparameter_tuning_random_forest(X_train,y_train).best_params_\n","23366dd5":"def hyperparameter_tuning_lgbm(X_train,y_train):\n   # checking with different possible values for each hyperparameter\n    n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 50)]\n   \n    num_leaves = [int(x) for x in np.linspace(start = 20, stop = 200, num = 10)]\n   \n    max_depth = [int(x) for x in np.linspace(1, 50, num = 11)]\n    max_depth.append(None)\n\n    \n    learning_rate = [0.01,0.05,0.25]\n    # Create the random grid\n    random_grid = {'n_estimators': n_estimators,\n                   'num_leaves': num_leaves,\n                   'max_depth': max_depth,\n                   'learning_rate': learning_rate}\n\n    # Use the random grid to search for best hyperparameters\n    # First create the base model to tune\n    lgbm = LGBMRegressor()\n    # Random search of parameters, using 3 fold cross validation, \n    # search across 25 different combinations, and use all available cores\n    lgbm_random = RandomizedSearchCV(estimator = lgbm, param_distributions = random_grid, n_iter = 25, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n    # Fit the random search model\n    lgbm_random.fit(X_train, y_train)\n    return lgbm_random\n","da95399e":"# the best values for hyperparameters were found by running this and those values have been used in the model. hence currently commented out\n\n#best_parameters = hyperparameter_tuning_lgbm(X_train,y_train).best_params_\n","da6de727":"def hyperparamter_tuning_xgb(X_train,y_train):\n    # checking with different possible values for each hyperparameter\n    n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 50)]\n   \n    min_child_weight = [int(x) for x in np.linspace(start = 0, stop = 10, num = 1)]\n  \n    max_depth = [int(x) for x in np.linspace(1, 50, num = 11)]\n    max_depth.append(None)\n\n   \n    learning_rate = [0.01,0.05,0.25]\n    # Create the random grid\n    random_grid = {'n_estimators': n_estimators,\n                   'min_child_weight': min_child_weight,\n                   'max_depth': max_depth,\n                   'learning_rate': learning_rate}\n\n    # Use the random grid to search for best hyperparameters\n    # First create the base model to tune\n    xgb = XGBRegressor()\n    # Random search of parameters, using 3 fold cross validation, \n    # search across 50 different combinations, and use all available cores\n    xgb_random = RandomizedSearchCV(estimator = xgb, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n    # Fit the random search model\n    xgb_random.fit(X_train, y_train)\n    return xgb_random","5218f91b":"#best_parameters = hyperparameter_tuning_xgb(X_train,y_train).best_params_\n","88fd2b32":"lgbm = LGBMRegressor(learning_rate=0.05,n_estimators= 1000,max_depth = 6)\nrf = RandomForestRegressor(n_estimators=50,max_depth=17, random_state=42)\nxgb = XGBRegressor(n_estimators = 1000, learning_rate = 0.05,random_state = 42)","a7dbcf31":"stack = StackingCVRegressor(regressors=( rf, lgbm, xgb),\n                            meta_regressor=xgb, cv=12,\n                            use_features_in_secondary=True,\n                            store_train_meta_features=True,\n                            shuffle=False,\n                            random_state=42)","7b0d0f75":"stack.fit(X_train.values, y_train.values)","f36525a9":"\npred = stack.predict(X_test.values)\nmae = mean_absolute_error(pred,y_test)\nprint(mae)","d1a67218":"pred = stack.predict(X_train.values)\nmae = mean_absolute_error(pred,y_train)\nprint(mae)","e1977cb8":"pred = stack.predict(test_X.values)\nprint(pred)","6c5a628b":"result=pd.DataFrame()\nresult['Id'] = test_index\nresult['PredictedValue'] = pd.DataFrame(pred)\nresult.head()","38321646":"result.to_csv('output.csv', index=False)\n","3e35edee":"tuning hyperparameters for LGBMRegressor","0cef1547":"## Packing it into output file","c0690d2b":"## Checking accuracy obtained for the validation set","7f8409b6":"## One-hot encoding of the categorical variables","b16e4be7":"## Using a stacked model\nA stacked model is used as the final model. In the base level, random forest, LGBMRegressor and XGBRegressor are used. \nIn the meta model, XGBRegressor is used.\nThese models were selected as they had high performance individually and are also diverse range of models that make different assumption","9286157f":"## Reading input data","cbd810fd":"## Feature Engineering\nSince F13 and F15 were similar, we will make a new column F18 with the average of F13 and F15 and later drop both F15 and F13","3e4ab307":"##### We can see that  there are no null values and hence filling with median,mode etc need not have to be done.","e8f9c140":"## Checking for null values ","69913a39":"## Standardising  the continuous variables\nMaking the mean of the column zero and bringing the values between -1 to 1","da511e3b":"## Data visualization through graphs\nHelps in checking for outliers, data variation, checking for linearity etc.","62458c56":"## Selecting the best features","388380d9":"## Separating the categorical and continuous coloumns","11cba179":"## Feature selection and feature importance\nWe will check which are the features that are more important in determining the output and discard the features that don't affect the output much","a25a007b":"## Predicting output for the test set","f3a5ba36":"## Dropping columns\nWe are dropping column 'F6,'F10'and 'F14' as it has very less importance and we also drop F13 and F15 as they were replaced by a new column\n  ","02fddae1":"#### We can see that the columns F6,F10 and F14 have very less correlation to the target variable indicating those columns can be removed. We can also see that F13 and F15 have high correlation.","70a262ad":"### Dropping F1 as it is just index and F2 is datestamp\n\n","e06ee0ea":"## Splitting the training data","c95874d8":"## Visualizing input data","294bfb1b":"## Checking accuracy obtained on the training set \nThis helps us get an idea about the bias and variance of the model","938609f5":"## Initializing all models with the best hyperparameters ","db272f36":"tuning the hyperparameters for XGBRegressor","67a64f8e":"#### We can see that F7 is the most important feature and F8,F6 and F14 are the least important ","990b26c3":"## Separating Input and Output Fields","39a481e6":"## Hyperparameter tuning using RandomizedSearchCV\ntuning the hyperparameters of random forest model","8df29552":"## Correlation Matrix\nCorrelation matrix shows the relation between two columns i.e a value close to zero shows that those two columns are not inter-related and a value close to 1 or -1 shows that the columns are highly related.","6d8701e8":"## Fitting the model"}}