{"cell_type":{"756884e8":"code","b1172311":"code","a3e00e27":"code","6085a35a":"code","dd3a8fb6":"code","53f4eeca":"code","c8067f16":"code","a1f9c0ee":"code","3dd223df":"code","0e8249b7":"code","b94a732e":"code","ba68cbbb":"code","dd2c0def":"code","f564e333":"code","71fc35f1":"code","0a234567":"code","49dfaffd":"code","9929b7a8":"code","4b61d0cd":"code","98e9ab63":"code","2475a6aa":"code","2780703d":"code","c6df7cdb":"code","6028b816":"code","3e09c61b":"code","5efc2dc2":"code","8abdc354":"code","4a41081e":"code","231d0d3b":"code","64e3d7c5":"code","97424ea7":"code","eb87f41b":"code","2a62b022":"code","f611cc77":"code","17a4b8e6":"code","17480d35":"code","dd106a09":"code","51eb4cb4":"code","84cd1622":"code","9329017e":"code","e36b8428":"code","318431e7":"code","6957862a":"code","81cdc557":"code","517848c1":"code","5f60220e":"code","c2f9d3b1":"code","d9bbe157":"code","3fabe767":"code","a4c5bc7f":"markdown","628f8caa":"markdown","fb97c30e":"markdown","c88eabfc":"markdown","d1287954":"markdown","95ac5b71":"markdown","48e435e9":"markdown","d19627cd":"markdown","3e921352":"markdown","5b00a09b":"markdown","7b16a566":"markdown","d81cfde8":"markdown","803ee243":"markdown","57d567cb":"markdown","1490b4cb":"markdown","dc4d2f77":"markdown","a75130ed":"markdown","33608682":"markdown","74995d4e":"markdown","de1cb22d":"markdown","aa59e69c":"markdown","ce4da9d1":"markdown","44bfb5b4":"markdown","50052664":"markdown","bb80dd57":"markdown","2f3d6169":"markdown","bf242717":"markdown","88467b0e":"markdown","ab4ad447":"markdown","9916e36e":"markdown","6aebb7db":"markdown","2e62df2c":"markdown","4cce235d":"markdown","bea30e02":"markdown","86d19fec":"markdown","b9b929ee":"markdown","5309eeaf":"markdown","b77b803a":"markdown","8b292524":"markdown","63b253e2":"markdown","99fc407e":"markdown","f1983955":"markdown","62073ec7":"markdown","77fe2a17":"markdown","d248a8e5":"markdown","b3cb2d59":"markdown","06c1b5f0":"markdown"},"source":{"756884e8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport collections\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectPercentile , f_classif \n","b1172311":"data = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv' )  \ntest = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv' )  ","a3e00e27":"data = data[:10000]","6085a35a":"def unique(feature) : \n    global data\n    print(f'Number of unique vaure are {len(list(data[feature].unique()))} which are : \\n {list(data[feature].unique())}')\n\ndef count_nulls() : \n    global data\n    for col in data.columns : \n        if not data[col].isna().sum() == 0 : \n            print(f'Column   {col}    got   {data[col].isna().sum()} nulls  ,  Percentage : {round(100*data[col].isna().sum()\/data.shape[0])} %')\n\ndef cplot(feature) : \n    global data\n    sns.countplot(x=feature, data=data,facecolor=(0, 0, 0, 0),linewidth=5,edgecolor=sns.color_palette(\"dark\", 3))\n\ndef encoder(feature , new_feature, drop = True) : \n    global data\n    enc  = LabelEncoder()\n    enc.fit(data[feature])\n    data[new_feature] = enc.transform(data[feature])\n    if drop == True : \n        data.drop([feature],axis=1, inplace=True)\n    \ndef MakeCloud(text , title = 'Word Clouds' , w = 15 , h = 15):\n    plt.figure(figsize=(w,h))\n    plt.imshow(WordCloud(background_color=\"white\",stopwords=set(stopwords.words('english')))\n               .generate(\" \".join([i for i in text.str.lower()])))\n    plt.axis(\"off\")\n    plt.title(title)\n    plt.show()\n\n\ndef SelectedData(data , feature , value , operation, selected_feature ):\n    if operation==0 : \n        result = data[data[feature]==value][selected_feature]\n    elif operation==1 : \n        result = data[data[feature] > value][selected_feature]\n    elif operation==2 : \n        result = data[data[feature]< value][selected_feature]\n    \n    return result \n\n\n\ndef CommonWords(text ,show = True , kk=10) : \n    all_words = []\n\n    for i in range(text.shape[0]) : \n        this_phrase = list(text)[i]\n        for word in this_phrase.split() : \n            all_words.append(word)\n    common_words = collections.Counter(all_words).most_common()\n    k=0\n    word_list =[]\n    for word, i in common_words : \n        if not word.lower() in  nlp.Defaults.stop_words :\n            if show : \n                print(f'The word is   {word}   repeated   {i}  times')\n            word_list.append(word)\n            k+=1\n        if k==kk : \n            break\n            \n    return word_list\n\ndef RemoveWords(data , feature , new_feature, words_list ) : \n    new_column = []\n    for i in range(data.shape[0]) : \n        this_phrase = data[feature][i]\n        new_phrase = []\n        for word in this_phrase.split() : \n            if not word.lower() in words_list : \n                new_phrase.append(word)\n        new_column.append(' '.join(new_phrase))\n    \n    data.insert(data.shape[1],new_feature,new_column)\n    \n\n    \ndef CountWords(text) :  \n    \n    all_words = []\n\n    for i in range(text.shape[0]) : \n        this_phrase = list(text)[i]\n        for word in this_phrase.split() : \n            all_words.append(word)\n\n    print(f'Total words are {len(all_words)} words')   \n    print('')\n    print(f'Total unique words are {len(set(all_words))} words')   \n\ndef SlicedData(feature_list, dropna = False) : \n    global data\n    if dropna :\n        return data.loc[:, feature_list ].dropna()\n    else : \n        return data.loc[:, feature_list ]","dd3a8fb6":"data.shape","53f4eeca":"data.head()","c8067f16":"data.describe()","a1f9c0ee":"SlicedData(['obscene','identity_attack', 'insult' , 'thread']).describe()","3dd223df":"SlicedData(['asian','atheist', 'bisexual' , 'black']).describe()","0e8249b7":"data['target'].min() , data['target'].max()","b94a732e":"data['target sector'] = round(data['target']*3)","ba68cbbb":"unique('target sector')","dd2c0def":"cplot('target sector')","f564e333":"temp_data = data[data['target sector'] > 0]['target sector']","71fc35f1":"plt.figure(figsize=(10,10))\nplt.pie(temp_data.value_counts(),labels=list(temp_data.value_counts().index),autopct ='%1.2f%%',labeldistance = 1.1)\nplt.show()","0a234567":"count_nulls()","49dfaffd":"data['comments']  =  data['comment_text'].str.lower()","9929b7a8":"SlicedData(['comment_text' , 'comments']).head(20)","4b61d0cd":"CountWords(data['comments'])","98e9ab63":"common = CommonWords(data['comments'])","2475a6aa":"RemoveWords(data , 'comments' , 'filtered comments', common)\nSlicedData(['comments' , 'filtered comments']).head(20)","2780703d":"MakeCloud(data['filtered comments'])","c6df7cdb":"def showclouds(n) : \n    this_list = ['asian', 'atheist', 'bisexual','black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n                 'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability','jewish', 'latino', 'male', 'muslim',\n                 'other_disability','other_gender', 'other_race_or_ethnicity', 'other_religion','other_sexual_orientation', \n                 'physical_disability','psychiatric_or_mental_illness', 'transgender', 'white' ]\n\n    for item in this_list[n*3:(n*3)+3] : \n        this_data =  SelectedData(data ,item , 0.1 , 1 , 'filtered comments')\n        print(f'for item    {item}')\n        print(f'Number of selected rows {this_data.shape[0]}')\n        print('common words : ')\n        _ = CommonWords(this_data)\n        if this_data.shape[0] >0 : \n            MakeCloud(this_data , str(f'Word Cloud for {item}'), 8 ,8)\n        print('--------------------------')","6028b816":"showclouds(0)","3e09c61b":"showclouds(1)","5efc2dc2":"showclouds(2)","8abdc354":"showclouds(3)","4a41081e":"showclouds(4)","231d0d3b":"showclouds(5)","64e3d7c5":"showclouds(6)","97424ea7":"showclouds(7)","eb87f41b":"X = data['filtered comments']\ny = data['target sector']","2a62b022":"X.head(10)","f611cc77":"y.head(10)","17a4b8e6":"X.isnull().sum() , y.isnull().sum()","17480d35":"VecModel = TfidfVectorizer()\nX = VecModel.fit_transform(X)\nprint(f'The new shape for X is {X.shape}')","dd106a09":"FeatureSelection = SelectPercentile(score_func = f_classif, percentile=1)\nX = FeatureSelection.fit_transform(X, y)","51eb4cb4":"print('X Shape is ' , X.shape)","84cd1622":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=44, shuffle =True)\n\nprint('X_train shape is ' , X_train.shape)\nprint('X_test shape is ' , X_test.shape)\nprint('y_train shape is ' , y_train.shape)\nprint('y_test shape is ' , y_test.shape)","9329017e":"GBCModel = GradientBoostingClassifier(n_estimators=500,max_depth=5,random_state=33) \nGBCModel.fit(X_train, y_train)\n\n\nprint('GBCModel Train Score is : ' , GBCModel.score(X_train, y_train))\nprint('GBCModel Test Score is : ' , GBCModel.score(X_test, y_test))\n","e36b8428":"test.head()","318431e7":"test['comments']  =  test['comment_text'].str.lower()\ntest.head()","6957862a":"X_test = test['comments']","81cdc557":"X_test.shape","517848c1":"X_test = VecModel.transform(X_test)","5f60220e":"X_test.shape","c2f9d3b1":"X_test = FeatureSelection.transform(X_test)","d9bbe157":"X_test.shape","3fabe767":"y_pred = GBCModel.predict(X_test)\ny_pred_prob = GBCModel.predict_proba(X_test)\nprint('Predicted Value for GBCModel is : ' , y_pred[:10])\nprint('Prediction Probabilities Value for GBCModel is : ' , y_pred_prob[:10])","a4c5bc7f":"then we have to transform it with the Vectorize Model , which fitted in the training data","628f8caa":"then we need to know the number of total words & unique words in the whole text ","fb97c30e":"great , then split them ","c88eabfc":"how about to see the could words for each category . . \n\nwe'll make a function now , which will show cloud words for only rows which got offensive value more than 0.1 in each category , of the 24 category we have here \n\n& to make it easier for us to read it , we'll show it 3 by 3 \n","d1287954":"______\n\nthen : 'other_religion','other_sexual_orientation'  ,'physical_disability'","95ac5b71":"how X looks like ? ","48e435e9":"# Treating Output\n\nsince output 'target' is a numerical value & the model shuld be regression , we'll concert the output into 4 classes , depend on how toxic is the text \n\nso instead have a huge numbers in the output , it will be either 0 , 1, 2 or 3 , & this to make the model easier , specially that we have a huge sample size & will use a big number of features (due to TF Vectorizer)\n\nso let's see max & min value for the output","d19627cd":"now it should have only 257 feature","3e921352":"almost two-third of text are kinda toxic , then moderate & little bit which is very toxic\n\nnow how about nulls in the database ","5b00a09b":"now how it looks like","7b16a566":"______\n\nnow : 'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability','jewish'","d81cfde8":"then we'll use TF Vectorizer tool , to create sparse matrix for all words","803ee243":"_____\n\n# Treating Text\n\nnow it's time to focus in text its self before we build the model \n\nfirst we need to lowercase all words , to avoid any misleading in training","57d567cb":"_____\n\nalmost a 600K word in only 10K row , which based on 54K unqiue words\n\nnow we need to know most common words in the whole text , & we'll use the feature 'comments' , which is lowered case , not the original feature 'comment_text'","1490b4cb":"almost 90% of text is non-toxic , now how about the toxic texts , lets temporarly drop all non-toxic text , to see distribution of ther categories","dc4d2f77":"# Toxic Text Recognition \nby : Hesham Asem\n\n________________\n\nhere we have a huge database with about 1.8 million sample size , which got a variety comments & texts , some of them are classified as offensive & toxic \n\nwe need to build a model able to train from this database , so he can classify the test data\n\ndatabase  : https:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/data\n\n\nso let's import needed libraries","a75130ed":"_______\n\nthen :'black', 'buddhist', 'christian'","33608682":"the data now should have 25K feature\n","74995d4e":"\n____\n\n\n# Data Processing\n\nnow how data looks like","de1cb22d":"great , now if we multiply 3 time all numbers then round it into integers , we'll have all outputs either 0 , 1 , 2 or 3 , depend on how toxic is the text","aa59e69c":"great , well done !\n\nhope you like it & found it useful . . ","ce4da9d1":"erfect , now we have to apply again the feature selection model , to only pick 1% of same features ","44bfb5b4":"then we'll read the data","50052664":"ok , almost 25K feature , which is so much & will consume a huge amount of time , specially that we have 10K sample size , so we'll use sklearn to only select 1% of those features","bb80dd57":"now we are ready for traning\n\n_____\n\n# Build the Model\n\nlet's use GBC since it's more suitable for big amout of data . \n\nwe'll use 500 estimators with max_depth =5\n\nlet's start the train & check accuracy for train & test data","2f3d6169":"now how X looks like","bf242717":"and since the database is huge , we'll just got 10K rows from it to save time in training , but you can deactivate this line if you need to take the whole data","88467b0e":"& y ? ","ab4ad447":"great , now phrases become more extinctive \n\n_____\n\n# Cloud Words\n\nit's useful to use wordcloud tool , to know most repeated words \n\nlet's see most repeated words now in all filtered comments , after we eliminate those 10 common words","9916e36e":"we need to be sure there is no nulls in both of them ","6aebb7db":"_____\n\nthen :  'female', 'heterosexual', 'hindu'","2e62df2c":"____\n\n\nnow start with :   'asian', 'atheist', 'bisexual'","4cce235d":"what is the shape ","bea30e02":"it looks like these 10 words are non-leading words , which might appear in toxic or non-toxic words , so it'll be a good idea to remove it from all phrases , to ust leave the most important words\n\nlet's create a new feature called 'filtered comments' , which contain all phrases exclude these common words","86d19fec":"it was so obvious how distinctive words are appear in each category , & that will help us in the classification\n\n_____\n\n# Data Preparing\n\nnow we are ready to define X & y ","b9b929ee":"____\n\nand last : 'psychiatric_or_mental_illness', 'transgender', 'white'","5309eeaf":"______\n\nnow  :  'latino', 'male', 'muslim'","b77b803a":"now to pie chart it","8b292524":"____\n\nnow it's time to define needed functions\n","63b253e2":"let's check it","99fc407e":"_____\n\nthen : 'other_disability','other_gender', 'other_race_or_ethnicity'","f1983955":"now how is the distribution of output","62073ec7":"ok 84% is not a bad accuracy , might be better if we increase the data more than 100K but it will need more time\n\nnow let's predict the Test Data. . ","77fe2a17":"now it's ready for predicting using the same GBC Model","d248a8e5":"and put it in the variable X_test","b3cb2d59":"let's lower case it as we did in trainging data","06c1b5f0":"______\n\nso we need to have a close look to some features"}}