{"cell_type":{"9a3861c9":"code","b22d1ba0":"code","23ea41d8":"code","b92dbb78":"code","7fb8bd3c":"code","a1f8bddd":"code","6b60761b":"code","00efd7cd":"code","725e0258":"code","a3299fee":"code","4586c066":"code","ad6cac6e":"code","06a7270e":"code","ac3956f5":"code","0ce0057f":"code","497275e8":"code","59b3f99d":"code","4d62d234":"code","5f3e1c8a":"code","9bf82c83":"code","ba617de6":"code","c2274908":"code","df6be3e4":"code","3814740e":"code","fcf6b8b1":"code","2cd0a098":"code","495e4b59":"code","47c54cad":"code","c4263e6a":"code","4446915a":"code","47d9e4ca":"code","e03cbc7c":"code","9558b530":"code","95e15622":"markdown","71109047":"markdown","ad65f001":"markdown","fb449ce2":"markdown","3e97fbe1":"markdown","b85fa4cd":"markdown","dc029d34":"markdown","de15c315":"markdown","f691671f":"markdown","3236c563":"markdown","47f6cf3b":"markdown","5a56e638":"markdown","78612a0d":"markdown","e8d5f76c":"markdown","cd7dd97a":"markdown"},"source":{"9a3861c9":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\nfrom matplotlib import pyplot\nfrom numpy import concatenate\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom tensorflow.python.keras.layers import Dense, LSTM , Dropout, CuDNNLSTM\nfrom tensorflow.python.keras import Sequential\nfrom math import sqrt","b22d1ba0":"TIME_STEPS = 4\nNUM_FEATURES = 4\nTOTAL_FEATURES = TIME_STEPS * NUM_FEATURES","23ea41d8":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b92dbb78":"#dataset preprocessing\n#datset loading dropping na values\n\ndataset= pd.read_csv('..\/input\/beijing-pm25-data-data-set\/PRSA_data_2010.1.1-2014.12.31.csv');\ndataset=dataset.dropna();","7fb8bd3c":"dataset=dataset.drop('No',axis=1);\ndataset=dataset.drop('year',axis=1);\ndataset=dataset.drop('month',axis=1);\ndataset=dataset.drop('day',axis=1);\ndataset=dataset.drop('hour',axis=1);\ndataset.head();\n\nvalues=dataset.values;\nfoo = values.copy()","a1f8bddd":"dataset.head()","6b60761b":"# specify columns to plot\ngroups = [0, 1, 2, 3, 5, 6, 7]\ni = 1\n# plotting each column\npyplot.figure()\nfor group in groups:\n\tpyplot.subplot(len(groups), 1, i)\n\tpyplot.plot(values[:, group])\n\tpyplot.title(dataset.columns[group], y=0.5, loc='right')\n\ti += 1\npyplot.show()","00efd7cd":"# integer encode direction\nencoder = LabelEncoder()\nvalues[:,4] = encoder.fit_transform(values[:,4])\n# ensure all data is float\nvalues = values.astype('float32')","725e0258":"values[:, 4]","a3299fee":"values","4586c066":"foo = values.copy()","ad6cac6e":"values.T","06a7270e":"corr = np.corrcoef(values.T)\nprint(corr)","ac3956f5":"corr = np.corrcoef(values.T) # ti\u0300m \u0111\u00f4\u0323 t\u01b0\u01a1ng quan cu\u0309a ma tran chuyen vi\n_mask = np.zeros_like(corr) # tao mang 0 co cung shape voi mang ban dau\n_mask[np.triu_indices_from(_mask)] = 1\n\nnp.triu_indices_from(_mask) # tra ve index cua tam giac tren\nnp.tril_indices_from(_mask) # tra ve index cua tam giac duoi\n\n# _mask[np.tril_indices_from(_mask)] = True # danh dau\n# d = np.tril(_mask) # tra\u0309 v\u00ea\u0300 tuple, 2 ma\u0309ng array\n# print(d[0])\n\nwith sns.axes_style(\"white\"):\n    f, ax = pyplot.subplots(figsize=(7, 5))\n    ax = sns.heatmap(corr, mask=_mask,annot=True, vmax=.2, square=False)","0ce0057f":"print(foo[:, 0])","497275e8":"values = np.delete(values, [1,3,6,7], 1) # tra ve mang moi, \n    # xoa index duoc chon [1, 3, 6, 7], chua lai nhung cot con lai\nprint(values)","59b3f99d":"# convert series to supervised learning\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1] \n    # neu kieu du lieu la list -> 1 cot else thi la shape[1] cua data -> so cot\n#     print(data.shape)\n    df = DataFrame(data)\n    cols, names = list(), list()\n    # input sequence (t-n, ... t-1)\n#     print(df.shift(3))\n#     print(df.shift(2))\n#     print(df.shift(1))\n\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    # put it all together\n    agg = concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","4d62d234":"# frame as supervised learning\nreframed = series_to_supervised(values, TIME_STEPS, 1)\n#drop columns we don't want to predict\nrem_cols = list(range(reframed.shape[1]-NUM_FEATURES+1,reframed.shape[1]))\nreframed.drop(reframed.columns[rem_cols], axis=1, inplace=True)\nprint(reframed.head())","5f3e1c8a":"assert reframed.shape[1] == NUM_FEATURES * TIME_STEPS + 1","9bf82c83":"# normalize features\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(reframed)","ba617de6":"# split into train and test sets\nvalues = scaled\nn_train_hours = 365 * 24*4\ntrain = values[:n_train_hours, :]\ntest = values[n_train_hours:, :]\n# split into input and outputs\ntrain_X, train_y = train[:, :TOTAL_FEATURES], train[:, TOTAL_FEATURES]\ntest_X, test_y = test[:, :TOTAL_FEATURES], test[:, TOTAL_FEATURES]","c2274908":"train_X = train_X.reshape((train_X.shape[0], TIME_STEPS, NUM_FEATURES))\ntest_X = test_X.reshape((test_X.shape[0], TIME_STEPS, NUM_FEATURES))\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape)","df6be3e4":"train_X[0]","3814740e":"if tf.test.is_gpu_available():\n    lstm_layer = CuDNNLSTM\nelse:\n    import functools\n    lstm_layer = functools.partial(\n            LSTM, recurrent_activation='sigmoid')","fcf6b8b1":"# design network\nmodel = Sequential()\nmodel.add(LSTM(100, return_sequences = True, input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel.add(Dropout(0.3))\n\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(1,activation='linear'))\n\nmodel.compile(loss='mse', optimizer='adam')","2cd0a098":"model.summary()","495e4b59":"# fit network\nearlystop_callback = tf.keras.callbacks.EarlyStopping(\n  monitor='val_loss',\n  patience=10)\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint('air_poll_lstm_3_ts.h5', monitor='val_loss', verbose=1, save_best_only=True)\nhistory = model.fit(train_X, train_y, epochs=50, callbacks=[model_checkpoint, earlystop_callback],\n                    batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)","47c54cad":"# plot history\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","c4263e6a":"# make a prediction\nyhat = model.predict(test_X)\nm_test_X = test_X.reshape((test_X.shape[0], TOTAL_FEATURES))\nprint(yhat.shape, m_test_X.shape)","4446915a":"# invert scaling for forecast\ninv_yhat = concatenate((yhat, m_test_X[:, :]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]","47d9e4ca":"# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = concatenate((test_y, m_test_X[:, :]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]","e03cbc7c":"# calculate RMSE and MAE\nrmse = sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)\nmae = (mean_absolute_error(inv_y, inv_yhat))\nprint('Test MAE: %.3f' % mae)","9558b530":"print('Actual :', inv_y)\nprint('Predicted:', inv_yhat)\n# plot history\npyplot.plot(inv_y, label='Actual')\npyplot.plot(inv_yhat, label='Predicted')\npyplot.legend()\npyplot.show()","95e15622":"# Splitting into train and test sets\nWe will only fit the model on the first 2 years(365* 24 * 2 hours) of data, then evaluate it on the remaining 3 years of data. ","71109047":"# **1. Feature Selection**\nThere are 8 features important for the forecast: PM2.5, dew point, temperature, pressure, wind direction, wind speed and the cumulative number of hours of snow and rain. Hence, dropping other features.","ad65f001":"# **4. Converting to Time Series Data**\nSince we use the LSTM neural network, we must sort the data according to the time. The dataset is transformed into a supervised learning problem. The weather variables for the hour to be predicted (t) are then removed. So, we have features for previous timestep (t-1) and for prediction of pollution PM2.5 taking its current timestep (t) data.","fb449ce2":"# **3. Check Correlation**","3e97fbe1":"# **Plotting the graph of Train Loss and Test Loss**","b85fa4cd":"# **Making Prediction\/Forecasting**\nForecasting the results and invert the scaling of the prediction and test data to check.","dc029d34":"# **Designing Network**\nWe will define the LSTM with 100 neurons in the first hidden layer and a Dropout Layer of 0.3,Next there will be another hidden layer of 50 neurons and a Dropout of 0.2. Similarly, there will two more hidden layers with respective 0.2 Dropouts and 50 neurons. In Final Layer, 1 neuron in the output layer for predicting pollution. In the activation Function we used linear function, because of sequential dataset. In the batch size we used three days (24*3 Hours) data. Optimizer function we used Adam method. Loss function we used mean squared error. By monitoring the value of test data loss function, stop the training model when it is not decreasing, and save the current best model.","de15c315":"# reshaping input to be 3D [samples, timesteps, features]\n\n(17520, 1, 8) (17520,) (24236, 1, 8) (24236,)\n","f691671f":"# **Calculating the RMSE and MAE values**\n\nTest RMSE: 27.188\nTest MAE: 17.932","3236c563":"# Data Normalization\nData Normalization is done using MinMaxScaler function of sklearn.","47f6cf3b":"# **Plotting the Graph of Actual vs Predicted**","5a56e638":"**DATASET PREPROCESSING**\n\n\n\n\nDataset Loading and dropping the rows with NA values.Plotting the graph for each columns. Further preprocessing includes:\n\n\n\n\n1. Feature Selection\n2. Label Encoding\n3. Check correlation\n4. Converting to Time Series data","78612a0d":"# **Fitting the Network**\nNetwork is fit with epochs size of 50 , batch size of 72.","e8d5f76c":"## Note:\n- Neural network\n- ","cd7dd97a":"# **2.** **Label** **Encoding**\nThe Wind direction doesn\u2019t contain numerical values so label encoding is done.\n"}}