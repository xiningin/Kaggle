{"cell_type":{"69a935e1":"code","d67a7cc3":"code","6b84fabf":"code","9f10bfe6":"code","17b57ce7":"code","15351d52":"code","08d26b3e":"code","c74eab1c":"code","c9d1d463":"code","b8ee4515":"code","8c0c914e":"code","90fc2be6":"code","95dfff4d":"code","3d273bb5":"code","4d7cc59b":"code","1e671672":"code","f163e8b3":"code","45c0a179":"code","e49edece":"code","9cc9bff1":"code","0128d2ec":"code","d3d5eeff":"code","ef7790c5":"code","148730a0":"code","be73fb08":"code","3c055bf0":"code","e6e4b491":"code","8f2428ef":"code","e2bf59b9":"code","44cddc08":"code","dda5f66f":"code","ab62ac9f":"code","5dd66ffe":"markdown","b3ac5cfe":"markdown","9ddca22f":"markdown","edb30c87":"markdown","255fbbb4":"markdown","ff07f20a":"markdown","7191d18c":"markdown","cdf727ee":"markdown","068044f4":"markdown","14520855":"markdown","86648928":"markdown","10e449e6":"markdown","b1d6056f":"markdown","e7a3e558":"markdown","19e55083":"markdown","0cba123a":"markdown","9632761b":"markdown","1adb5013":"markdown","2927dcbf":"markdown","9caf3d06":"markdown","82b56f95":"markdown","d8fc17e4":"markdown","861f9a7c":"markdown","025f1554":"markdown","d887198a":"markdown","918b87a2":"markdown","e4af297f":"markdown","85e6f680":"markdown"},"source":{"69a935e1":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d67a7cc3":"df = pd.read_csv('..\/input\/glass\/glass.csv')","6b84fabf":"df","9f10bfe6":"df.info()","17b57ce7":"df.corr()['Type'].sort_values()","15351d52":"plt.figure(figsize= (10,7))\nsns.heatmap(df.corr(), annot = True, fmt= ' .1g')","08d26b3e":"df.notnull().all()","c74eab1c":"X = df.drop(columns= ['Type', 'Ca']).values\ny = df.iloc[:, -1].values","c9d1d463":"for column in df.columns[:-1]:\n  sns.distplot(df[column], color= 'y')\n  plt.grid(True)\n  plt.show()","b8ee4515":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","8c0c914e":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","90fc2be6":"accuracies = {}","95dfff4d":"from sklearn.tree import DecisionTreeClassifier\n\nclassifier = DecisionTreeClassifier(criterion= 'entropy', random_state= 0)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)","3d273bb5":"plot_confusion_matrix(classifier, X_test, y_test, cmap = plt.cm.BuGn, normalize = 'true')\nplt.show()","4d7cc59b":"accuracy = accuracy_score(y_test, y_pred)\naccuracies['Decision Tree Classification'] = accuracy\n\nprint(accuracy)","1e671672":"from sklearn.ensemble import RandomForestClassifier\n\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n","f163e8b3":"plot_confusion_matrix(classifier, X_test, y_test, cmap = plt.cm.BuGn, normalize = 'true')\nplt.show()","45c0a179":"accuracy = accuracy_score(y_test, y_pred)\naccuracies['Random Forest Classification'] = accuracy\n\nprint(accuracy)","e49edece":"from sklearn.neighbors import KNeighborsClassifier\n\n# metric = 'minkowski', p = 2 means Euclidean distance.\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)","9cc9bff1":"plot_confusion_matrix(classifier, X_test, y_test, cmap = plt.cm.BuGn, normalize = 'true')\nplt.show()","0128d2ec":"accuracy = accuracy_score(y_test, y_pred)\naccuracies['K-NN'] = accuracy\n\nprint(accuracy)","d3d5eeff":"from sklearn.svm import SVC\n\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)","ef7790c5":"plot_confusion_matrix(classifier, X_test, y_test, cmap = plt.cm.BuGn, normalize = 'true')\nplt.show()","148730a0":"accuracy = accuracy_score(y_test, y_pred)\naccuracies['Support Vector Machine'] = accuracy\n\nprint(accuracy)","be73fb08":"from sklearn.naive_bayes import GaussianNB\n\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)","3c055bf0":"plot_confusion_matrix(classifier, X_test, y_test, cmap = plt.cm.BuGn, normalize = 'true')\nplt.show()","e6e4b491":"accuracy = accuracy_score(y_test, y_pred)\naccuracies['Naive Bayes'] = accuracy\n\nprint(accuracy)","8f2428ef":"from sklearn.linear_model import LogisticRegression\n\nclassifier = LogisticRegression()\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)","e2bf59b9":"plot_confusion_matrix(classifier, X_test, y_test, cmap = plt.cm.BuGn, normalize= 'true')\nplt.show()","44cddc08":"accuracy = accuracy_score(y_test, y_pred)\naccuracies['Logistic Regression'] = accuracy\n\nprint(accuracy)","dda5f66f":"accuracy_df  = pd.DataFrame(list(accuracies.items()),columns = ['Model Name', 'Accuracy Score']) \naccuracy_df","ab62ac9f":"f, ax = plt.subplots(figsize = (8,6))\nsns.set_color_codes('pastel')\nsns.barplot(y = 'Model Name', x = 'Accuracy Score', data = accuracy_df, color = 'pink')\nplt.show()","5dd66ffe":"## Check if there is null value or not","b3ac5cfe":"### Accuracy","9ddca22f":"### Confusion Matrix","edb30c87":"### Confusion Matrix","255fbbb4":"### Accuracy","ff07f20a":"<a id= 8> <\/a>\n## Random Forest Classification","7191d18c":"### Accuracy","cdf727ee":"### Confusion Matrix","068044f4":"<a id= 5> <\/a>\n# 5. Feature Scaling","14520855":"<a id= 6> <\/a>\n# 6. Classification Models","86648928":"### Confusion Matrix","10e449e6":"<a id= 13> <\/a>\n# 7. Results\n","b1d6056f":"<a id= 7> <\/a>\n## Decision Tree Classification","e7a3e558":"<a id= 10> <\/a>\n## Support Vector Machine","19e55083":"### Confusion Matrix","0cba123a":"<a id= 2> <\/a>\n# 2. Import Libraries","9632761b":"<a id= 11> <\/a>\n## Naive Bayes","1adb5013":"### Confusion Matrix","2927dcbf":"## Distributions","9caf3d06":"<a id= 1> <\/a>\n# 1. Variable Description\n\n### Independent Variables\n  1. Id number: 1 to 214 (removed from CSV file)\n  2. RI: refractive index\n  3. Na: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)\n  4. Mg: Magnesium\n  5. Al: Aluminum\n  6. Si: Silicon\n  7. K: Potassium\n  8. Ca: Calcium\n  9. Ba: Barium\n  10. Fe: Iron\n\n### Dependent Variable\n  Type of glass: (class attribute)\n  * buildingwindowsfloatprocessed: 1  \n  * buildingwindowsnonfloatprocessed: 2  \n  * vehiclewindowsfloatprocessed: 3\n  * vehiclewindowsnonfloatprocessed (none in this database): 4\n  * containers: 5\n  * tableware: 6\n  * headlamps: 7\n","82b56f95":"### Accuracy","d8fc17e4":"### Accuracy","861f9a7c":"<a id= 9> <\/a>\n## K-NN","025f1554":"### Accuracy","d887198a":"<a id= 3> <\/a>\n# 3. Load the Dataset","918b87a2":"<a id= 4> <\/a>\n# 4. Train Test Split","e4af297f":"<a id= 12> <\/a>\n## Logistic Regression","85e6f680":"# Introduction\nThis is a Glass Identification Data Set from UCI.\n\n## Content:\n  1. [Variable Description](#1)\n  2. [Import Libraries](#2)\n  3. [Load the Dataset](#3)\n  4. [Train Test Split](#4)\n  5. [Feature Scaling](#5)\n  6. [Classification Models](#6)\n    * [Decision Tree Classification](#7)\n    * [Random Tree Classification](#8)\n    * [K-NN](#9)\n    * [Support Vector Machine](#10)\n    * [Naive Bayes](#11)\n    * [Logistic Regression](#12)\n  7. [Results](#13)"}}