{"cell_type":{"2ffd2db1":"code","b41bc2c6":"code","f21cf778":"code","42109aaf":"code","09f0157b":"code","09d2a5fd":"code","e4442e0f":"code","48d84f14":"code","eb8dd1b9":"code","c636dde2":"code","f1b5179d":"code","7f141938":"code","fba7486b":"code","c61a68e5":"code","79da7e4f":"code","7118dcf3":"code","a8585f88":"markdown","654ffa29":"markdown","384f717c":"markdown","93bb44be":"markdown","db294b61":"markdown","bd38716a":"markdown","afd9914c":"markdown","beaa2b62":"markdown","03b031c9":"markdown","414cceb7":"markdown","96d8d310":"markdown","b7edd974":"markdown","757ca234":"markdown","9722aa47":"markdown","fed8338c":"markdown","627413e6":"markdown"},"source":{"2ffd2db1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b41bc2c6":"from requests import get\nurl = 'https:\/\/www.imdb.com\/search\/name\/?birth_place=India&adult=include&count=100&start=1&ref_=rlm'\nresponse = get(url)\nprint(response.text[:500])\n","f21cf778":"from bs4 import BeautifulSoup\nhtml_soup = BeautifulSoup(response.text, 'html.parser')\ntype(html_soup)","42109aaf":"stars_containers = html_soup.find_all('div', class_ = 'lister-item mode-detail')\nprint(type(stars_containers))\nprint(len(stars_containers))","09f0157b":"# Picking up the first division container.\nfirst_star=stars_containers[0]","09d2a5fd":"names=[]\ncategory=[]\nimages=[]\nwell_known_movie=[]\nbio=[]\n\nstars_containers = html_soup.find_all('div', class_ = 'lister-item mode-detail')\n\nfor container in stars_containers:\n    \n    #Name of celebrity\n    name = container.h3.a.text.strip()\n    names.append(name)\n    \n    #Image of celebrity\n    image=container.find('img')['src']\n    images.append(image)\n    \n    #Category of celebrirty\n    categ=container.p.text.split('|')[0].strip()\n    category.append(categ)\n    \n    #Well known movie of celebrity\n    well=container.p.a.text.strip()\n    well_known_movie.append(well)\n    \n    #bio of celebrity\n    bioa=container.text.split(well)[1].strip()\n    biot=list(bioa.split(\".\"))[0]\n    bio.append(biot)\n    ","e4442e0f":"#Now we will be scrapping multiple pages","48d84f14":"# Create a list called pages, and populate it with the strings \npages = [str(i) for i in range(1,4766,100)]","eb8dd1b9":"from time import sleep\nfrom random import randint\n","c636dde2":"from time import time\nstart_time = time()\nrequests = 0\nfor _ in range(5):\n# A request would go here\n    requests += 1\n    sleep(randint(1,3))\n    elapsed_time = time() - start_time\n    print('Request: {}; Frequency: {} requests\/s'.format(requests, requests\/elapsed_time))\n","f1b5179d":"from IPython.core.display import clear_output\nstart_time = time()\nrequests = 0\nfor _ in range(5):\n# A request would go here\n    requests += 1\n    sleep(randint(1,3))\n    current_time = time()\n    elapsed_time = current_time - start_time\n    print('Request: {}; Frequency: {} requests\/s'.format(requests, requests\/elapsed_time))\nclear_output(wait = True)","7f141938":"from warnings import warn\nwarn(\"Warning Simulation\")","fba7486b":"names=[]\ncategory=[]\nimages=[]\nwell_known_movie=[]\nbio=[]\n\nstart_time = time()\nrequests = 0\n\nfor page in pages:\n    \n    response = get('https:\/\/www.imdb.com\/search\/name\/?birth_place=India&adult=include&count=100&start=' + page + \n    '&ref_=rlm')\n    \n    # Pause the loop\n    sleep(randint(8,15))\n\n    # Monitor the requests\n    requests += 1\n    elapsed_time = time() - start_time\n    print('Request:{}; Frequency: {} requests\/s'.format(requests, requests\/elapsed_time))\n    clear_output(wait = True)\n\n    # Throw a warning for non-200 status codes\n    if response.status_code != 200:\n        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n\n    # Break the loop if the number of requests is greater than expected\n    if requests > 72:\n        warn('Number of requests was greater than expected.')\n        break\n        \n    # Parse the content of the request with BeautifulSoup\n    page_html = BeautifulSoup(response.text, 'html.parser') \n    \n    stars_containers = page_html.find_all('div', class_ = 'lister-item mode-detail')\n    \n    for container in stars_containers:\n        \n        if container.find('span', class_='ghost') is not None:\n        \n            #Name of celebrity\n            name = container.h3.a.text.strip()\n            names.append(name)\n\n            #Image of celebrity\n            image=container.find('img')['src']\n            images.append(image)\n\n            #Category of celebrirty\n            categ=container.p.text.split('|')[0].strip()\n            category.append(categ)\n\n            #Well known movie of celebrity\n            well=container.p.a.text.strip()\n            well_known_movie.append(well)\n\n            #bio of celebrity\n            bioa=container.text.split(well)[1].strip()\n            biot=list(bioa.split(\".\"))[0]\n            bio.append(biot)\n    ","c61a68e5":"import pandas as pd\nCelebrity_info = pd.DataFrame({'Name': names,\n'Category': category,\n'Image': images,\n'Well_Known_Movie': well_known_movie,\n'Bio': bio\n})\nprint(Celebrity_info.info())\nCelebrity_info.head(10)","79da7e4f":"# To print in HTML format.\nfrom IPython.core.display import HTML\n\n# convert your links to html tags \ndef path_to_image_html(path):\n    return '<img src=\"'+ path + '\" width=\"60\" >'\n\npd.set_option('display.max_colwidth', -1)\n\nHTML(Celebrity_info.to_html(escape=False ,formatters=dict(Image=path_to_image_html)))","7118dcf3":"# Let's save it.\nCelebrity_info.to_csv('celeb_info_csv.csv')","a8585f88":"Scraping multiple pages is a bit more challenging. We\u2019ll build upon our one-page script by doing three more things:\n\n* Making all the requests we want from within the loop.\n* Controlling the loop\u2019s rate to avoid bombarding the server with requests.\n* Monitoring the loop while it runs.","654ffa29":"Now let\u2019s use the find_all() method to extract all the div containers that have a class attribute of lister-item mode-detail:","384f717c":"## Introduction\n\nTo source data for data science projects, you\u2019ll often rely on SQL and NoSQL databases, APIs, or ready-made CSV data sets.\n\nThe problem is that you can\u2019t always find a data set on your topic, databases are not kept current and APIs are either expensive or have usage limits.\n\nIf the data you\u2019re looking for is on an web page, however, then the solution to all these problems is web scraping.\n\nIn this tutorial we\u2019ll learn to scrape multiple web pages with Python using BeautifulSoup and requests. We\u2019ll then perform some simple analysis using pandas, and matplotlib.","93bb44be":"## Piecing everything together","db294b61":"Let\u2019s experiment with this monitoring technique at a small scale first. In the following code cell we will:\n\n* Set a starting time using the time() function from the time module, and assign the value to start_time.\n* Assign 0 to the variable requests which we\u2019ll use to count the number of requests.\n* Start a loop, and then with each iteration:\n\n    * Simulate a request.\n    * Increment the number of requests by 1.\n    * Pause the loop for a time interval between 8 and 15 seconds.\n    * Calculate the elapsed time since the first request, and assign the value to elapsed_time.\n    * Print the number of requests and the frequency.","bd38716a":"To parse our HTML document and extract the 100 div containers, we\u2019ll use a Python module called BeautifulSoup, the most common web scraping module for Python.\n\nIn the following code cell we will:\n\nImport the BeautifulSoup class creator from the package bs4.\nParse response.text by creating a BeautifulSoup object, and assign this object to html_soup. The 'html.parser' argument indicates that we want to do the parsing using Python\u2019s built-in HTML parser.","afd9914c":"**Let\u2019s start writing the script by requesting the content of this single web page: (https:\/\/www.imdb.com\/search\/name\/?birth_place=India&adult=include&count=100&start=1&ref_=rlm) In the following code cell we will:\n\n* Import the get() function from the requests module.\n* Assign the address of the web page to a variable named url.\n* Request the server the content of the web page by using get(), and store the server\u2019s response in the variable response.\n* Print a small part of response\u2018s content by accessing its .text attribute (response is now a Response object).**","beaa2b62":"**This is default first cell in any kaggle kernel. They import NumPy and Pandas libraries and it also lists the available Kernel files. NumPy is the fundamental package for scientific computing with Python. Pandas is the most popular python library that is used for data analysis**","03b031c9":"For now, let\u2019s just import these two functions to prevent overcrowding in the code cell containing our main sleep from loop","414cceb7":"**I decided to create five columns.**\n\n* Name\n* Category\n* Images\n* Well_Known_Movie\n* Bio","96d8d310":"# Web Scrapping Tutorial for Beginners\n\n\n**In this tutorial, I will be teaching you web scrapping and based on that we will create a data set which contain information about movie celebrities who are born in India. **\n\n* This is my third tutorial. Do point out my mistakes in comment section.\n* Do upvote if you find this notebook interesting.\n* Website Source(https:\/\/www.imdb.com\/search\/name\/?birth_place=India&adult=include&count=100&start=1&ref_=rlm) ","b7edd974":"![![image.png](attachment:image.png)](http:\/\/)","757ca234":"**Now let\u2019s merge the data into a pandas DataFrame to examine what we\u2019ve managed to scrape. If everything is as expected, we can move on with cleaning the data to get it ready for analysis.**","9722aa47":"# Thank you\n\nGuys,do put your query in comment section and if you like the implementation method, do upvote it. You can download the data set from https:\/\/www.kaggle.com\/rishabhdhyani4\/indian-born-celebrity-info","fed8338c":"## Controlling the crawl-rate ","627413e6":"**To monitor the status code we\u2019ll set the program to warn us if there\u2019s something off. A successful request is indicated by a status code of 200. We\u2019ll use the warn() function from the warnings module to throw a warning if the status code is not 200.**"}}