{"cell_type":{"b3ca9f2a":"code","2c6df9a8":"code","43e62dc2":"code","0254523d":"code","68a84df5":"code","106ddee2":"code","0dffaf93":"code","f5322a63":"code","96d733ac":"code","48cd8432":"markdown","4ad3bed9":"markdown","a3a6bdf2":"markdown","79796b24":"markdown","65517039":"markdown","96039735":"markdown","2ddcca5d":"markdown","b86d5f1d":"markdown"},"source":{"b3ca9f2a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport warnings\nwarnings.filterwarnings('ignore')","2c6df9a8":"df=pd.read_csv('\/kaggle\/input\/titanic\/titanic_cleaned.csv')\ndf.head()","43e62dc2":"df.shape","0254523d":"df.info()","68a84df5":"df.isnull().sum()","106ddee2":"X=df.drop(columns=['Survived'])\ny=df[['Survived']]\nprint(X.shape)\nprint(y.shape)","0dffaf93":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=42)\nprint(X_train.shape)\nprint(y_test.shape)","f5322a63":"from sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nlr=LogisticRegression().fit(X_train, y_train)\nsgd=SGDClassifier().fit(X_train, y_train)\nrf=RandomForestClassifier().fit(X_train, y_train)\ndtc=DecisionTreeClassifier().fit(X_train, y_train)\nsvc=SVC().fit(X_train, y_train)\nnb=GaussianNB().fit(X_train, y_train)\n\ny_lr=lr.predict(X_test)\ny_sgd=sgd.predict(X_test)\ny_rf=rf.predict(X_test)\ny_dtc=dtc.predict(X_test)\ny_svc=svc.predict(X_test)\ny_nb=nb.predict(X_test)","96d733ac":"from sklearn.metrics import classification_report\nprint('Logistic Regression: ',classification_report(y_lr, y_test) )\nprint('Stochastic Gradient Descent: ',classification_report(y_sgd, y_test) )\nprint('Random Forest: ',classification_report(y_rf, y_test) )\nprint('Decision Tree: ',classification_report(y_dtc, y_test) )\nprint('Support Vector: ',classification_report(y_svc, y_test) )\nprint('Naive Bayes: ',classification_report(y_nb, y_test) )","48cd8432":"Now Let's load the dataset","4ad3bed9":"Let's explore the dataset a bit.","a3a6bdf2":"1. Now Let's train the models-in fact 6 models","79796b24":"Observation: Logistic Regression is the best classifier with accuracy rate 82%","65517039":"Great job! well done.","96039735":"Now let's separate the target column from feature column","2ddcca5d":"Now split the dataset into train and test by 80:20","b86d5f1d":"Now evaluate the model using classification report from scikit-learn"}}