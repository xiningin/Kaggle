{"cell_type":{"f98d4270":"code","4265a60a":"code","7dce5239":"code","91f44b51":"code","a91304f3":"code","bef48f8c":"code","50cb900b":"code","22bd5c5a":"code","a49e5791":"code","6c042cf5":"code","93ff9f29":"code","eb28bc29":"code","2c6ddd13":"code","6f5dff0b":"code","37b8ab61":"code","3c8a67ad":"code","167de47c":"code","44017159":"code","bc6b4c0b":"code","5aa0567c":"code","9cc9bbf4":"code","1e878ab9":"code","d381cc83":"markdown","2b9fc744":"markdown","c8844e66":"markdown","25c90122":"markdown","f74ad433":"markdown","4ac414fc":"markdown","156838db":"markdown","4f5efda3":"markdown","06d1e40c":"markdown","bb558245":"markdown","ebe436d9":"markdown","4663f89e":"markdown","d1f24f86":"markdown","c3e3752c":"markdown","c34ecb21":"markdown","98cbc6c7":"markdown","08cba24f":"markdown","8bfe3870":"markdown","b13498a4":"markdown","e7c92543":"markdown","819f5854":"markdown","87cead46":"markdown","2b4e9a59":"markdown","c2569ae7":"markdown","28b38440":"markdown","e2a4b552":"markdown","a8fca522":"markdown","c7d088d3":"markdown","8fb1ff4e":"markdown","e4cf4847":"markdown","3f1b420e":"markdown","7f9c537a":"markdown"},"source":{"f98d4270":"from IPython.display import Image\nImage(filename= \"..\/input\/feature-decp-img\/Feature_Description.png\", height=700, width=700)","4265a60a":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport matplotlib as mpl\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import StratifiedKFold, learning_curve, ShuffleSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import VotingClassifier, IsolationForest\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import confusion_matrix\n\nplt.style.use('seaborn-whitegrid')\nplt.rcParams['lines.linewidth'] = 2\nplt.rcParams['font.sans-serif'] = 'Arial'\nplt.rcParams['text.color'] = 'black'\nplt.rcParams['axes.labelcolor']= 'black'\nplt.rcParams['xtick.color'] = 'black'\nplt.rcParams['ytick.color'] = 'black'\nplt.rcParams['font.size']=12","7dce5239":"train = pd.read_csv('..\/input\/data-for-flight-crash-severity-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/data-for-flight-crash-severity-prediction\/test.csv')\n\ntrain.head(10)","91f44b51":"train.info()","a91304f3":"train.describe(include='all')","bef48f8c":"X = train.drop(columns=['Accident_ID', 'Severity'])\ny = train.Severity\ny.map({'Minor_Damage_And_Injuries' : 0, 'Significant_Damage_And_Serious_Injuries' : 1, \n      'Significant_Damage_And_Fatalities' : 2,  'Highly_Fatal_And_Damaging' : 3})","50cb900b":"print('Percentage of each class in Target Variable \\n')\nprint((train.Severity.value_counts()\/len(train))*100)","22bd5c5a":"rfc = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=10, max_features=0.5 , bootstrap=False)\ncross_val_score(rfc, X, y, cv=4, n_jobs=-1, verbose=1, scoring='f1_weighted').mean()","a49e5791":"train_sizes_abs, train_scores, val_scores = learning_curve(rfc, X, y, train_sizes=np.arange(0.3, 1, 0.05), n_jobs=-1, cv=4, \n                                                           scoring='f1_weighted', random_state=2, verbose=1)\nfig, axes = plt.subplots(2, 2, figsize=(15,15))\n\naxes[0, 0].plot(train_sizes_abs, train_scores[:, 0], label='Train Score')\naxes[0, 0].plot(train_sizes_abs, val_scores[:, 0], label='Val Score')\naxes[0, 1].plot(train_sizes_abs, train_scores[:, 1], label='Train Score')\naxes[0, 1].plot(train_sizes_abs, val_scores[:, 1], label='Val Score')\naxes[1, 0].plot(train_sizes_abs, train_scores[:, 2], label='Train Score')\naxes[1, 0].plot(train_sizes_abs, val_scores[:, 2], label='Val Score')\naxes[1, 1].plot(train_sizes_abs, train_scores[:, 3], label='Train Score')\naxes[1, 1].plot(train_sizes_abs, val_scores[:, 3], label='Val Score')\n\naxes[0, 0].legend(), axes[1, 0].legend(), axes[0, 1].legend(), axes[1, 1].legend()\naxes[0, 0].set_xlabel(\"No.of Examples Used\"), axes[0, 0].set_ylabel(\"Score\")\naxes[0, 1].set_xlabel(\"No.of Examples Used\"), axes[0, 1].set_ylabel(\"Score\")\naxes[1, 0].set_xlabel(\"No.of Examples Used\"), axes[1, 0].set_ylabel(\"Score\")\naxes[1, 1].set_xlabel(\"No.of Examples Used\"), axes[1, 1].set_ylabel(\"Score\")","6c042cf5":"f, ax = plt.subplots(5, 2, figsize=(15,15))\n\nfor i in range(5):\n    sns.distplot(X.iloc[:, i], color=\"b\", bins=20, hist_kws=dict(alpha=1), hist=False, label='Train', ax=ax[i, 0] )\n    sns.distplot(test.iloc[:, i], color=\"r\", bins=20, hist_kws=dict(alpha=1), hist=False, label='Test', ax=ax[i, 0])\n    plt.legend()\nfor i in range(5):\n    sns.distplot(X.iloc[:, i+5], color=\"b\", bins=20, hist_kws=dict(alpha=1), hist=False, label='Train', ax=ax[i, 1] )\n    sns.distplot(test.iloc[:, i+5], color=\"r\", bins=20, hist_kws=dict(alpha=1), hist=False, label='Test', ax=ax[i, 1])\n    plt.legend()","93ff9f29":"f, ax = plt.subplots(1, 1, figsize=(11,11))\nsns.barplot(X.Violations, y, ax=ax)\nplt.legend()","eb28bc29":"sns.pairplot(train, hue=\"Severity\")","2c6ddd13":"y = train.Severity\ny = y.str.replace('Minor_Damage_And_Injuries', '0').str.replace('Significant_Damage_And_Serious_Injuries', '1').str.replace('Significant_Damage_And_Fatalities', '2').str.replace('Highly_Fatal_And_Damaging', '3').astype(int)\ncorr = pd.concat(objs=[X, y], axis=1).corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","6f5dff0b":"f,ax=plt.subplots(1,2,figsize=(18,8))\ntrain['Accident_Type_Code'].value_counts().plot.bar(color=['grey','orange','blue'],ax=ax[0])\nax[0].set_title('Accident_Type_Code')\nax[0].set_ylabel('Count')\nsns.countplot('Accident_Type_Code',hue='Severity',data=train,ax=ax[1])\nax[1].set_title('Pclass:Survived vs Dead')\nplt.show()","37b8ab61":"X_test = test.drop(columns=['Accident_ID'])\nconcat = pd.concat(objs=[X, X_test], axis=0)\nconcat['Type_7_acc'] = (concat.Accident_Type_Code == 7)\nconcat['More_than_2_violations'] = (concat.Violations>2)\nconcat['Safety_score\/Days_since_inspection'] = concat.Safety_Score\/(concat.Days_Since_Inspection)\nconcat.drop(columns=['Violations', 'Max_Elevation', 'Adverse_Weather_Metric', \n                     'Turbulence_In_gforces', 'Cabin_Temperature', 'Accident_Type_Code'], inplace=True)\n\nss = StandardScaler()\nconcat = ss.fit_transform(concat)\n\nX, X_test = concat[:len(train), :], concat[len(train):, :]","3c8a67ad":"rfc = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=10, max_features=0.5 , bootstrap=False)\ncross_val_score(rfc, X, y, cv=4, n_jobs=-1, verbose=1, scoring='f1_weighted').mean()","167de47c":"nnc = MLPClassifier(hidden_layer_sizes=(10,5), max_iter=300,activation = 'relu',solver='adam', \n                           random_state=1, batch_size=50)\ncross_val_score(nnc, X, y, cv=5, n_jobs=-1, verbose=1, scoring='f1_weighted').mean()","44017159":"knc = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\ncross_val_score(knc, X, y, cv=5, n_jobs=-1, verbose=1, scoring='f1_weighted').mean()","bc6b4c0b":"lgbmc = LGBMClassifier(random_state=2, n_estimators=100, colsample_bytree=0.6, \n                       max_depth=10, learning_rate=0.5, boosting_type='gbdt')\ncross_val_score(lgbmc, X, y, cv=5, n_jobs=-1, verbose=1, scoring='f1_weighted').mean()","5aa0567c":"xgbc = XGBClassifier(seed=7, n_jobs=-1, n_estimators=900, random_state=0, max_depth=7, learning_rate=0.7)\ncross_val_score(xgbc, X, y, cv=5, n_jobs=-1, verbose=1, scoring='f1_weighted').mean()","9cc9bbf4":"from sklearn.ensemble import VotingClassifier\nensemble = VotingClassifier(estimators=[('rfc', rfc), ('xgbc', xgbc), ('knc', knc),\n                                        ('nnc', nnc), ('lgbmc', lgbmc)],\n                                         voting='soft', n_jobs=-1)\ncross_val_score(ensemble, X, y, cv=5, n_jobs=-1, verbose=1, scoring='f1_weighted').mean()","1e878ab9":"ensemble.fit(X, y)\ny_pred = ensemble.predict(X_test)\ny_pred = pd.Series(y_pred).astype(str).str.replace('0', 'Minor_Damage_And_Injuries').str.replace('1', 'Significant_Damage_And_Serious_Injuries').str.replace('2', 'Significant_Damage_And_Fatalities').str.replace('3', 'Highly_Fatal_And_Damaging')\nsub = pd.DataFrame(data={'Accident_ID' : test.Accident_ID, 'Severity' : y_pred})\nsub.to_csv('final_pred.csv', index=False)","d381cc83":"# Plane Crash Severity Prediction","2b9fc744":"$$score = {100 * (f1\\_score(actual\\_values, predicted\\_values, average='weighted'))}$$","c8844e66":"##### The Ensemble is used to decrease overfit and to make sure that none of the scores above were a lucky guess and won't work on the test.","25c90122":"## EDA Time","f74ad433":"##### Relations Worth Exploring \n##### 1) Safety_score and Days_since_inspection\n##### 2) Explore adverse_weather_metric and Accident type_code\n##### 3) adverse weather metric and Max_elevation\n##### 4) Turbulence_In_gforces and Control_Metric","4ac414fc":"##### Inference: feature-> more than 2 violations","156838db":"## Evaluation Criteria","4f5efda3":"### 4) LightGBM","06d1e40c":"## The Dataset","bb558245":"### 1) Random Forest","ebe436d9":"##### Infernece: Train and Test have the same distribution so random spliting of data is the right strategy for cross-validation.","4663f89e":"## Predicting Time","d1f24f86":"## The Random Forest Baseline and Learning Curves","c3e3752c":"## The Leaderboard top 1% Approach","c34ecb21":"**Here we infer that train and test have similar distribution but the dataset is imbalanced and Accident_type_code is categorical. Ordinal Classification is definitely worth trying for this.**","98cbc6c7":"##### Move to the top and see the difference in scores with respect to the baseline.","08cba24f":"**This kernel is related to a problem statement on Hackerearth on Plane crash Severity Production.**","8bfe3870":"## The Ensemble","b13498a4":"##### All the hyperparameters are tuned manually, basically the approach I follow is learn about the model and hyperparameters and then tune them with trial and error. It is much faster than grid search and much reliable than randomized search","e7c92543":"##### Inference: The second plot can be used to help generate new features like (Accident_Type_Code == 7)","819f5854":"## The Problem Statement","87cead46":"## Experimenting with Models","2b4e9a59":"## Feature Engineering","c2569ae7":"**Here Severity is our Target Variable**","28b38440":"The dataset comprises 3 files: \n\nTrain.csv: [10000 x 12 excluding the headers] contains Training data\n\nTest.csv: [2500 x 11 excluding the headers] contains Test data\n\nsample_submission.csv: contains a sample of the format in which the Results.csv needs to be","e2a4b552":"### 2) Multi Layer Perceptron","a8fca522":"*Loading all the important libraries*","c7d088d3":"### 3) K-Neighbours Classifier","8fb1ff4e":" ##### *Inference: This is clearly a fully tuned random forest overfitting the data*","e4cf4847":"### 5) XGBoost","3f1b420e":"**Imagine you have been hired by a leading airline. You are required to build Machine Learning models to anticipate and classify the severity of any airplane accident based on past incidents. With this, all airlines, even the entire aviation industry, can predict the severity of airplane accidents caused due to various factors and, correspondingly, have a plan of action to minimize the risk associated with them.**","7f9c537a":"##### Inference: You can Analyse Features worth dropping by checking their correlation with target."}}