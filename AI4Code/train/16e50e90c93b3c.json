{"cell_type":{"5906ef1d":"code","7061a73f":"code","dd96ed04":"code","7cbb8c43":"code","d0f9e721":"code","f066d75b":"code","73d7b236":"code","ede82cdd":"code","6acf7f03":"code","6a35e1eb":"code","3403c604":"code","e9dc5ef7":"code","53b9ecad":"code","afd8af43":"code","44d7d5c5":"code","9ff35e0b":"code","fcbdec0e":"code","bd13c0e2":"code","740b1fa1":"code","557be978":"code","3a7a368b":"code","2a1cb0e5":"code","f2cc67c4":"code","675707d4":"code","18a403ef":"code","5f9629c0":"code","ca59b9ac":"code","f17ea82d":"code","081d6b08":"code","46905be9":"code","6d203c0a":"code","9bdb0fa8":"code","051f797c":"code","88fc8256":"code","8d8970c3":"code","ba02f26e":"code","06ae6c38":"code","4968d44e":"code","74027589":"code","bd4ef259":"code","cabc2aa0":"code","17c76c91":"code","1183391c":"markdown","c08eda88":"markdown","f9d9aac8":"markdown","53e15111":"markdown","88cfbff5":"markdown","d3561896":"markdown","a0c329f1":"markdown","7d08dc61":"markdown","88a172d8":"markdown","706f41d2":"markdown","aa91fd20":"markdown","d7128c7e":"markdown","c0ef063a":"markdown","c14e8a8f":"markdown","050107f5":"markdown","f0791ed5":"markdown","813148e2":"markdown","b985bb45":"markdown","8c1a24f2":"markdown","b84fe337":"markdown","0e441e8a":"markdown","39aa716c":"markdown","9af1ac59":"markdown"},"source":{"5906ef1d":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport gc\nimport glob\nimport io\nimport IPython\nimport json\nimport numpy as np\nimport pathlib\nimport pandas as pd\nimport sys\nimport cv2\nimport math\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nimport tensorflow as tf\n\nINPUT_DIR = '\/kaggle\/input\/tensorflow-great-barrier-reef\/'\nsys.path.insert(0, INPUT_DIR)\nimport greatbarrierreef","7061a73f":"print(tf.__version__)\nprint(tf.test.is_gpu_available())\nprint(tf.config.list_physical_devices('GPU'))","dd96ed04":"CUSTOM_MODEL_NAME = 'my_efficientdet_d2' \nPRETRAINED_MODEL_NAME = 'efficientdet_d2_coco17_tpu-32'\nPRETRAINED_MODEL_URL = 'http:\/\/download.tensorflow.org\/models\/object_detection\/tf2\/20200711\/efficientdet_d2_coco17_tpu-32.tar.gz'\nLABEL_MAP_NAME = 'label_map.pbtxt'","7cbb8c43":"folders = {\n    'APIMODEL_PATH': 'api',\n    'DATA_PATH': 'data',\n    'MODEL_PATH': 'my_models',\n    'PRETRAINED_MODEL_PATH': 'pre-trained-models',\n    'CHECKPOINT_PATH': os.path.join('my_models',CUSTOM_MODEL_NAME), \n    'OUTPUT_PATH': os.path.join('my_models',CUSTOM_MODEL_NAME, 'export')\n}\n\nfiles = {\n    'PIPELINE_CONFIG':os.path.join(folders['MODEL_PATH'], CUSTOM_MODEL_NAME, 'pipeline.config'),\n    'LABELMAP': os.path.join(folders['DATA_PATH'], LABEL_MAP_NAME),\n    'VERIFICATION_SCRIPT': os.path.join(folders['APIMODEL_PATH'], 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py'),\n    'TRAINING_SCRIPT': os.path.join(folders['APIMODEL_PATH'], 'research', 'object_detection', 'model_main_tf2.py'),\n    'EXPORTER_SCRIPT': os.path.join(folders['APIMODEL_PATH'], 'research', 'object_detection', 'exporter_main_v2.py')\n}","d0f9e721":"for path in folders.values():\n    if not os.path.exists(path):\n        !mkdir -p {path}","f066d75b":"!wget {PRETRAINED_MODEL_URL} -P {folders['PRETRAINED_MODEL_PATH']}\n!cd {folders['PRETRAINED_MODEL_PATH']} && tar -zxvf {PRETRAINED_MODEL_NAME+'.tar.gz'}","73d7b236":"if not os.path.exists(os.path.join(folders['APIMODEL_PATH'], 'research', 'object_detection')):\n    !git clone https:\/\/github.com\/tensorflow\/models {folders['APIMODEL_PATH']}","ede82cdd":"%%bash\ncd api\/research\n\nprotoc object_detection\/protos\/*.proto --python_out=.\n\n# cp object_detection\/packages\/tf2\/setup.py .\nwget https:\/\/storage.googleapis.com\/odml-dataset\/others\/setup.py\npip install -q --user .\n\npip install -q imagesize","6acf7f03":"! python {files['VERIFICATION_SCRIPT']}","6a35e1eb":"from object_detection.utils import dataset_util, label_map_util, config_util\n\nfrom object_detection.utils import visualization_utils as viz_utils\nfrom object_detection.builders import model_builder\nfrom object_detection.protos import pipeline_pb2\nfrom google.protobuf import text_format\nimport imagesize","3403c604":"TRAINING_RATIO = 0.8\ndata_df = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))","e9dc5ef7":"split_index = int(TRAINING_RATIO * len(data_df))\n\nwhile data_df.iloc[split_index - 1].sequence == data_df.iloc[split_index].sequence:\n    split_index += 1\n\ntrain_data_df = data_df.iloc[:split_index].sample(frac=1).reset_index(drop=True)\nval_data_df = data_df.iloc[split_index:].sample(frac=1).reset_index(drop=True)\n\ntrain_positive_count = len(train_data_df[train_data_df.annotations != '[]'])\nval_positive_count = len(val_data_df[val_data_df.annotations != '[]'])\n\nprint('Training ratio (all samples):', f\"{(float(len(train_data_df)) \/ (len(train_data_df) + len(val_data_df))):.2f}\")\nprint('Training ratio (positive samples):', f\"{(float(train_positive_count) \/ (train_positive_count + val_positive_count)):.2f}\")","53b9ecad":"train_data_df = train_data_df[train_data_df.annotations != '[]'].reset_index()\nprint('Number of positive images used for training:', len(train_data_df))\nval_data_df = val_data_df[val_data_df.annotations != '[]'].reset_index()\nprint('Number of positive images used for validation:', len(val_data_df))","afd8af43":"train_data_df.head()","44d7d5c5":"def load_image_into_np(path):\n    img_data = tf.io.gfile.GFile(path, 'rb').read()\n    image = Image.open(io.BytesIO(img_data))\n    (im_width, im_height) = image.size\n    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n\ndef plot_detections(image_np, boxes, classes, scores, category_index, unc=True):\n    image_np_with_annotations = image_np.copy()\n    viz_utils.visualize_boxes_and_labels_on_image_array(image_np_with_annotations,\n                                                       boxes,\n                                                       classes,\n                                                       scores,\n                                                       category_index,\n                                                       use_normalized_coordinates=unc,\n                                                       min_score_thresh=0.05)\n    return image_np_with_annotations\n\ndef get_image_with_annotation(df, idx):\n    row = df.iloc[idx]\n    img = load_image_into_np(row.image_path)\n    boxes = np.asarray(row.bboxes)\n    num_boxes = len(boxes)\n    classes = np.ones(num_boxes, dtype='int32')\n    scores = np.ones(num_boxes)\n    category_index = {1: {'id': 1, 'name': 'COTS'}}\n    unc = True\n    \n    img = plot_detections(img, boxes, classes, scores, category_index, unc)\n    return img\n\n\ndef get_bbox(row):\n    bboxes = []\n    annotations = json.loads(row.annotations.replace(\"'\", '\"'))\n    for annotation in annotations:\n            bboxes.append([annotation['y'] \/ row.height, \n                           annotation['x'] \/ row.width,\n                           (annotation['y'] + annotation['height']) \/ row.height, \n                           (annotation['x'] + annotation['width']) \/ row.width])\n    row[\"bboxes\"] = bboxes\n    return row\n\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row[\"image_path\"])\n    return row\n\ndef get_path(row):\n    row[\"image_path\"] = os.path.join(INPUT_DIR, \"train_images\", f'video_{row.video_id}', f'{row.video_frame}.jpg')\n    return row","9ff35e0b":"train_data_df = train_data_df.progress_apply(get_path,axis=1)\nval_data_df = val_data_df.progress_apply(get_path,axis=1)\n\ntrain_data_df = train_data_df.progress_apply(get_imgsize,axis=1)\nval_data_df = val_data_df.progress_apply(get_imgsize,axis=1)\n\ntrain_data_df = train_data_df.progress_apply(get_bbox,axis=1)\nval_data_df = val_data_df.progress_apply(get_bbox,axis=1)","fcbdec0e":"train_data_df.head()","bd13c0e2":"%matplotlib inline\nidx = np.random.randint(0,train_data_df.shape[0]) \nimg = get_image_with_annotation(train_data_df, idx)\nplt.figure(figsize=(20,10))\nplt.imshow(img)\nplt.title(f\"Image Index {idx}\")\nplt.show()","740b1fa1":"def create_tf_example(row):\n    with tf.io.gfile.GFile(row[\"image_path\"], 'rb') as fid:\n        encoded_jpg = fid.read()\n\n    height = row[\"height\"]\n    width = row[\"width\"]\n    filename = f'{row[\"video_id\"]}:{row[\"video_frame\"]}'.encode('utf8') \n    image_format = 'jpeg'.encode() \n\n    bb = row[\"bboxes\"]\n    \n    xmins = [i[1] for i in bb]\n    xmaxs = [i[3] for i in bb]     \n    ymins = [i[0] for i in bb] \n    ymaxs = [i[2] for i in bb] \n            \n    classes_text = ['COTS'.encode() for i in bb]\n    classes_id = [1 for i in bb] \n\n    tf_example = tf.train.Example(features=tf.train.Features(feature={\n      'image\/height': dataset_util.int64_feature(height),\n      'image\/width': dataset_util.int64_feature(width),\n      'image\/filename': dataset_util.bytes_feature(filename),\n      'image\/source_id': dataset_util.bytes_feature(filename),\n      'image\/encoded': dataset_util.bytes_feature(encoded_jpg),\n      'image\/format': dataset_util.bytes_feature(image_format),\n      'image\/object\/bbox\/xmin': dataset_util.float_list_feature(xmins),\n      'image\/object\/bbox\/xmax': dataset_util.float_list_feature(xmaxs),\n      'image\/object\/bbox\/ymin': dataset_util.float_list_feature(ymins),\n      'image\/object\/bbox\/ymax': dataset_util.float_list_feature(ymaxs),\n      'image\/object\/class\/text': dataset_util.bytes_list_feature(classes_text),\n      'image\/object\/class\/label': dataset_util.int64_list_feature(classes_id),\n    }))\n    \n    return tf_example.SerializeToString()\n\n\ndef convert_to_tfrecord(data_df, filename):\n    with tf.io.TFRecordWriter(os.path.join(folders[\"DATA_PATH\"],filename)) as writer:\n        for _, row in tqdm(data_df.iterrows()):\n            tf_example = create_tf_example(row)\n            writer.write(tf_example)","557be978":"convert_to_tfrecord(train_data_df, 'train.tfrec')\nconvert_to_tfrecord(val_data_df, 'valid.tfrec')","3a7a368b":"labels = [{'name':'COTS', 'id':1}]\n\nwith open(files['LABELMAP'], 'w') as f:\n    for label in labels:\n        f.write('item { \\n')\n        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n        f.write('\\tid:{}\\n'.format(label['id']))\n        f.write('}\\n')\n        \ncategory_index = label_map_util.create_category_index_from_labelmap(files['LABELMAP'])","2a1cb0e5":"# def deserialize_example(serialized_string):\n#     feature={\n#       'image\/height': tf.io.FixedLenFeature([], tf.int64),\n#       'image\/width': tf.io.FixedLenFeature([], tf.int64),\n#       'image\/filename': tf.io.FixedLenFeature([], tf.string),\n#       'image\/source_id': tf.io.FixedLenFeature([], tf.string),\n#       'image\/encoded': tf.io.FixedLenFeature([], tf.string),\n#       'image\/format': tf.io.FixedLenFeature([], tf.string),\n#       'image\/object\/bbox\/xmin': tf.io.VarLenFeature(tf.float32),\n#       'image\/object\/bbox\/xmax': tf.io.VarLenFeature(tf.float32),\n#       'image\/object\/bbox\/ymin': tf.io.VarLenFeature(tf.float32),\n#       'image\/object\/bbox\/ymax': tf.io.VarLenFeature(tf.float32),\n#       'image\/object\/class\/text': tf.io.VarLenFeature(tf.string),\n#       'image\/object\/class\/label': tf.io.VarLenFeature(tf.int64),\n#     }\n\n    \n#     parsed_record = tf.io.parse_single_example(serialized_string, feature)\n#     image = tf.io.decode_jpeg(parsed_record['image\/encoded'])\n#     xmins = tf.sparse.to_dense(parsed_record['image\/object\/bbox\/xmin']).numpy()\n#     xmaxs = tf.sparse.to_dense(parsed_record['image\/object\/bbox\/xmax']).numpy()\n#     ymins = tf.sparse.to_dense(parsed_record['image\/object\/bbox\/ymin']).numpy()\n#     ymaxs = tf.sparse.to_dense(parsed_record['image\/object\/bbox\/ymax']).numpy()\n    \n#     bb = [[ymins[i], xmins[i], ymaxs[i], xmaxs[i]] for i in range(len(xmins))]\n\n#     return image, bb","f2cc67c4":"# train_set = tf.data.TFRecordDataset(os.path.join(folders[\"DATA_PATH\"],\"train.tfrec\"))","675707d4":"# ds = train_set.take(1)\n# for sample in ds:\n#     image, bb = deserialize_example(sample)\n    \n# boxes = np.asarray(bb)\n# num_boxes = len(boxes)\n\n# img = plot_detections(np.array(image), boxes, np.ones(num_boxes, dtype='int32'), np.ones(num_boxes), category_index)\n# plt.figure(figsize=(20,10))\n# plt.imshow(img)\n# plt.show()","18a403ef":"# del train_set","5f9629c0":"!cp {os.path.join(folders['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'pipeline.config')} {os.path.join(folders['CHECKPOINT_PATH'])}","ca59b9ac":"config = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])\nconfig","f17ea82d":"pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\nwith tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"r\") as f:                                                                                                                                                                                                                     \n    proto_str = f.read()                                                                                                                                                                                                                                          \n    text_format.Merge(proto_str, pipeline_config)  ","081d6b08":"pipeline_config.model.ssd.num_classes = len(labels)\npipeline_config.model.ssd.image_resizer.keep_aspect_ratio_resizer.min_dimension = 1280\npipeline_config.model.ssd.image_resizer.keep_aspect_ratio_resizer.max_dimension = 1280\npipeline_config.train_config.data_augmentation_options[1].random_scale_crop_and_pad_to_square.output_size = 1280\npipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.y_scale = 10.0\npipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.x_scale = 10.0\npipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.height_scale = 5.0\npipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.width_scale = 5.0\npipeline_config.train_config.batch_size = 2\n# pipeline_config.train_config.fine_tune_checkpoint = os.path.join(folders['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'checkpoint', 'ckpt-0')\npipeline_config.train_config.fine_tune_checkpoint = \"..\/input\/tfodtoutput\/export7-D2-Plus10_000\/export\/checkpoint\/ckpt-0\"\npipeline_config.train_config.use_bfloat16 = False\npipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\npipeline_config.train_config.sync_replicas = False\npipeline_config.train_config.replicas_to_aggregate = 1\npipeline_config.train_config.optimizer.momentum_optimizer.learning_rate.cosine_decay_learning_rate.learning_rate_base = 1e-3\npipeline_config.train_config.optimizer.momentum_optimizer.learning_rate.cosine_decay_learning_rate.warmup_learning_rate = 1e-4\npipeline_config.train_config.optimizer.momentum_optimizer.learning_rate.cosine_decay_learning_rate.total_steps = 10000\npipeline_config.train_config.optimizer.momentum_optimizer.learning_rate.cosine_decay_learning_rate.warmup_steps = 2000\npipeline_config.model.ssd.post_processing.batch_non_max_suppression.score_threshold = 1e-8\npipeline_config.model.ssd.post_processing.batch_non_max_suppression.iou_threshold = 0.5\npipeline_config.train_config.num_steps = 10000\npipeline_config.train_input_reader.label_map_path= files['LABELMAP']\npipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(folders['DATA_PATH'], 'train.tfrec')]\npipeline_config.eval_input_reader[0].label_map_path = files['LABELMAP']\npipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(folders['DATA_PATH'], 'valid.tfrec')]","46905be9":"config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \nwith tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"wb\") as f:                                                                                                                                                                                                                     \n    f.write(config_text)   ","6d203c0a":"!cat {files[\"PIPELINE_CONFIG\"]}","9bdb0fa8":"gc.collect()","051f797c":"!python {files['TRAINING_SCRIPT']}\\\n    --model_dir={folders['CHECKPOINT_PATH']}\\\n    --pipeline_config_path={files['PIPELINE_CONFIG']}\\\n    --num_train_steps=10000","88fc8256":"!python {files['TRAINING_SCRIPT']}\\\n    --model_dir={folders['CHECKPOINT_PATH']}\\\n    --pipeline_config_path={files['PIPELINE_CONFIG']}\\\n    --checkpoint_dir={folders['CHECKPOINT_PATH']}\\\n    --eval_timeout=0 ","8d8970c3":"!python {files['EXPORTER_SCRIPT']}\\\n    --input_type image_tensor \\\n    --pipeline_config_path={files['PIPELINE_CONFIG']} \\\n    --trained_checkpoint_dir={folders['CHECKPOINT_PATH']} \\\n    --output_directory={folders['OUTPUT_PATH']}","ba02f26e":"!ls {folders[\"CHECKPOINT_PATH\"]}\/ckpt-*.index","06ae6c38":"configs = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])\ndetection_model = model_builder.build(model_config=configs['model'], is_training=False)\n\nckps = glob.glob(folders[\"CHECKPOINT_PATH\"]+\"\/ckpt-*.index\")\nckps.sort(key=os.path.getmtime)\n\nckp_file = ckps[-1][:-6]\nprint(ckp_file)\nckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\nckpt.restore(ckp_file).expect_partial()\n\ndef detect_fn(image):\n    image, shapes = detection_model.preprocess(image)\n    prediction_dict = detection_model.predict(image, shapes)\n    detections = detection_model.postprocess(prediction_dict, shapes)\n    return detections","4968d44e":"def get_image_pred(df, idx):\n\n    image_np = load_image_into_np(df.iloc[idx].image_path)\n    \n    height, width, _ = image_np.shape\n    input_tensor = tf.cast(np.expand_dims(image_np, 0), tf.float32)\n    detections = detect_fn(input_tensor)\n\n    num_detections = detections['num_detections'][0].numpy().astype(np.int32) \n    bboxes = []\n    scores = []\n    classes = []\n    DETECTION_THRESHOLD = 0.15\n    \n    for i in range(num_detections):\n        score = detections['detection_scores'][0][i].numpy()\n        \n        if score < DETECTION_THRESHOLD:\n            continue\n\n        bboxes.append(list(detections['detection_boxes'][0][i].numpy()))\n        scores.append(score)\n        classes.append(1)\n    img = plot_detections(image_np, np.array(bboxes), np.array(classes), np.array(scores), category_index, unc=True)\n    return img\n\ncnt = 10\nfig, ax = plt.subplots(cnt, 2, figsize = (20,40))\nfor row in range(cnt):\n    idx = np.random.randint(0,val_data_df.shape[0])\n    gt = get_image_with_annotation(val_data_df, idx)\n    pred = get_image_pred(val_data_df, idx)\n    \n    ax[row][0].imshow(gt)\n    ax[row][0].set_xticks([])\n    ax[row][0].set_yticks([])\n    ax[row][0].set_title(f\"GT_{idx}\")\n    \n    ax[row][1].imshow(pred)\n    ax[row][1].set_xticks([])\n    ax[row][1].set_yticks([])\n    ax[row][1].set_title(f\"PR_{idx}\")\nplt.tight_layout()\nplt.show()","74027589":"!zip .\/my_model.zip -r {folders[\"OUTPUT_PATH\"]}","bd4ef259":"env = greatbarrierreef.make_env()  \niter_test = env.iter_test() ","cabc2aa0":"DETECTION_THRESHOLD = 0.15\n\nfor (image_np, df) in iter_test:\n    height, width, _ = image_np.shape\n    \n    input_tensor = tf.cast(np.expand_dims(image_np, 0), tf.float32)\n    detections = detect_fn(input_tensor)\n    \n    num_detections = detections['num_detections'][0].numpy().astype(np.int32)\n    predictions = []\n    \n    for index in range(num_detections):\n        score = detections['detection_scores'][0][index].numpy()\n\n        if score < DETECTION_THRESHOLD:\n            continue\n\n        bbox = detections['detection_boxes'][0][index].numpy()\n        y_min = int(bbox[0] * height)\n        x_min = int(bbox[1] * width)\n        y_max = int(bbox[2] * height)\n        x_max = int(bbox[3] * width)\n\n        bbox_width = x_max - x_min\n        bbox_height = y_max - y_min\n\n        predictions.append(f'{score:.2f} {x_min} {y_min} {bbox_width} {bbox_height}')\n        \n        \n    prediction_str = ' '.join(predictions)\n    df['annotations'] = prediction_str\n    env.predict(df)\n\nsub_df = pd.read_csv('submission.csv')\nsub_df.head()","17c76c91":"!rm -rf .\/api\n!rm -rf .\/data\n!rm -rf .\/pre-trained-models","1183391c":"## Create Label Map","c08eda88":"# Evaluate Model","f9d9aac8":"# Load Model from Checkpoints","53e15111":"## Helper Functions","88cfbff5":"# Install TF Object Detection API & Download Pre-Trained Model","d3561896":"## Test TFRecords","a0c329f1":"## Visualize One Sample","7d08dc61":"# Submission","88a172d8":"<a href=\".\/my_models.zip\"> Download File my_model.zip <\/a>","706f41d2":"## Add Some Useful Columns","aa91fd20":"# Export Model","d7128c7e":"## Install TFODT API","c0ef063a":"# Config Model","c14e8a8f":"# Prepare Data","050107f5":"## Download Pre-Trained Model","f0791ed5":"## Create TFRecord Files","813148e2":"## Import TFODT API","b985bb45":"Created folders:\n1. **api**: for storing tensorflow object detection model api files\n2. **pre-trained-models**: for storing downloaded pretrained models including checkpoints and config\n3. **my_models**: for storing trained model and new config\n4. **data**: for storing tfrecords files and `label_map.pbtxt` file","8c1a24f2":"# Train Model","b84fe337":"## Create Configs and Folders ","0e441e8a":"# Prediction","39aa716c":"## Split Dataset","9af1ac59":"## Download TFODT API"}}