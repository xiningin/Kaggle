{"cell_type":{"490cd82c":"code","1aa325d0":"code","0ca054e2":"code","f824dad6":"code","0146ecf6":"code","f82b8ba5":"code","01d5e3a2":"code","4071a2c6":"code","55b59427":"markdown","29dfbaa4":"markdown","0e7cf18b":"markdown","81f5e497":"markdown","54277651":"markdown"},"source":{"490cd82c":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom torchvision.datasets import ImageFolder\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision.models import vgg19\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import save_image, make_grid\nfrom PIL import Image\nfrom tqdm import tqdm_notebook as tqdm\nrandom.seed(42)\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1aa325d0":"class ImageDl(Dataset):\n    def __init__(self,files):\n        self.files = files\n        self.lr_transform = transforms.Compose([\n            transforms.Resize((64,64),Image.BICUBIC),\n            transforms.CenterCrop(64),\n            transforms.ToTensor(),\n            transforms.Normalize(*stats)   \n        ])\n        self.hr_transform = transforms.Compose([\n            transforms.Resize((256,256),Image.BICUBIC),\n            transforms.CenterCrop(256),\n            transforms.ToTensor(),\n            transforms.Normalize(*stats)   \n        ])\n\n    def __getitem__(self, index):\n        img = self.files[index % len(self.files)][0]\n        img_lr = self.lr_transform(img)\n        img_hr = self.hr_transform(img)\n        return {\"lr\": img_lr, \"hr\": img_hr}\n    \n    def __len__(self):\n        return len(self.files)","0ca054e2":"os.makedirs(\"generated-images\", exist_ok=True)\ndataset = ImageFolder('..\/input\/animefacedataset\/')\nbatch_size = 20\nstats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\ntrain_dataloader = DataLoader(ImageDl(dataset), batch_size, shuffle=True, num_workers=3, pin_memory=True)\n# pin_memory: Host to GPU copies are much faster when they originate from pinned (page-locked) memory\nprint('Total batches:',len(train_dataloader))","f824dad6":"class ResidualBlock(nn.Module):\n    \n    def __init__(self,in_channels):\n        super(ResidualBlock,self).__init__()\n        self.res_block = nn.Sequential(\n            nn.Conv2d(in_channels,in_channels,kernel_size=3,stride=1,padding=1),\n            nn.BatchNorm2d(in_channels,0.8),\n            nn.PReLU(),\n            nn.Conv2d(in_channels,in_channels,kernel_size=3,stride=1,padding=1),\n            nn.BatchNorm2d(in_channels,0.8)\n        )\n        \n    def forward(self,x):\n        return x + self.res_block(x)","0146ecf6":"class Generator(nn.Module):\n    \n    def __init__(self,in_channels=3,out_channels=3,res_block_size=16):\n        super(Generator,self).__init__()\n        # first conv block\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels,64,kernel_size=9,stride=1,padding=4),      \n            nn.PReLU()\n        )\n        # residual blocks\n        res_blocks = []\n        for _ in range(res_block_size):\n            res_blocks.append(ResidualBlock(64))\n        \n        self.res_blocks = nn.Sequential(*res_blocks)\n        # second conv layer\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(64,64,kernel_size=3,stride=1,padding=1),\n            nn.BatchNorm2d(64,0.8),            \n        )\n        # upsampling layers\n        upsample_layers = []\n        for out_features in range(2):\n            upsample_layers += [\n                nn.Conv2d(64,256,kernel_size=3,stride=1,padding=1),       \n                nn.BatchNorm2d(256),\n                nn.PixelShuffle(upscale_factor=2),\n                nn.PReLU(),       \n            ]\n        self.upsample_layers = nn.Sequential(*upsample_layers)\n        # third conv layer\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(64,out_channels,kernel_size=9,stride=1,padding=4),\n            nn.Tanh()\n        )\n        \n    def forward(self,x):\n        out1 = self.conv1(x)\n        out = self.res_blocks(out1)\n        out2 = self.conv2(out)\n        out_final = torch.add(out1,out2)\n        out_final = self.upsample_layers(out_final)\n        out_final = self.conv3(out_final)\n        return out_final\n    ","f82b8ba5":"class Discriminator(nn.Module):\n    def __init__(self,input_shape):\n        super(Discriminator,self).__init__()\n        self.input_shape = input_shape\n        in_channels,in_height,in_width = self.input_shape\n        \n        def disc_block(in_filters, out_filters,first_block=False):\n            layers = []\n            layers.append(nn.Conv2d(in_filters,out_filters,kernel_size=3,stride=1,padding=1))\n            if not first_block :\n                layers.append(nn.BatchNorm2d(out_filters))\n            \n            layers.append(nn.LeakyReLU(0.2,inplace=True))\n            layers.append(nn.Conv2d(out_filters,out_filters,kernel_size=3,stride=2,padding=1))\n            layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2,inplace=True))\n            return layers\n        \n        layers = []\n        in_filters = in_channels\n        for i, out_filters in enumerate([64,128,256,512]):\n            layers.extend(disc_block(in_filters,out_filters,first_block=(i == 0)))\n            in_filters = out_filters\n            \n        layers.append(nn.Conv2d(out_filters,1,kernel_size=3,stride=1,padding=1))\n        self.disc = nn.Sequential(*layers)\n        \n    def forward(self,img):\n        return self.disc(img)","01d5e3a2":"generator = Generator()\ndiscriminator = Discriminator(input_shape=(3,256,256))\nvgg19_model = vgg19(pretrained=True)\n# Extract features from vgg pretrained vgg model\nfeature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])       \nfeature_extractor.eval()\nadversarial_crit = torch.nn.MSELoss()\ncontent_crit = torch.nn.L1Loss()\n\ncuda = torch.cuda.is_available()\nif cuda:\n    Tensor = torch.cuda.FloatTensor    \n    generator = generator.cuda()\n    discriminator = discriminator.cuda()\n    feature_extractor = feature_extractor.cuda()\n    adversarial_crit = adversarial_crit.cuda()\n    content_crit = content_crit.cuda()\n    \nelse :\n    Tensor = torch.Tensor    \n\nlr = 0.00008\nEPOCHS = 4\ngen_opt = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\ndisc_opt = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))","4071a2c6":"for epoch in range(EPOCHS):\n    tqdm_bar = tqdm(train_dataloader)\n    for idx, imgs in enumerate(tqdm_bar):\n        generator.train(); discriminator.train()\n        imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n        imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n        valid = Variable(Tensor(np.ones((imgs_lr.size(0),1,16,16))), requires_grad=False)\n        fake = Variable(Tensor(np.zeros((imgs_lr.size(0),1,16,16))), requires_grad=False)\n        gen_opt.zero_grad()\n        gen_hr = generator(imgs_lr)\n        loss_adv = adversarial_crit(discriminator(gen_hr), valid)\n        gen_features = feature_extractor(gen_hr)\n        real_features = feature_extractor(imgs_hr)\n        loss_content = content_crit(gen_features, real_features.detach())\n        gen_loss = loss_content + 1e-3 * loss_adv\n        gen_loss.backward()\n        gen_opt.step()\n        disc_opt.zero_grad()\n        real_loss = adversarial_crit(discriminator(imgs_hr), valid)\n        fake_loss = adversarial_crit(discriminator(gen_hr.detach()), fake)\n        disc_loss = (real_loss + fake_loss) \/ 2\n        disc_loss.backward()\n        disc_opt.step()\n\n        if idx > 3174:\n            imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n            gen_hr = make_grid(gen_hr, nrow=1, normalize=True)\n            imgs_lr = make_grid(imgs_lr, nrow=1, normalize=True)\n            img_grid = torch.cat((imgs_lr, gen_hr), -1)\n            save_image(img_grid, f\"generated-images\/{idx}.png\", normalize=False)\n\n","55b59427":"## Generator","29dfbaa4":"## Import Libraries","0e7cf18b":"## Training","81f5e497":"## Import Images","54277651":"## Discriminator"}}