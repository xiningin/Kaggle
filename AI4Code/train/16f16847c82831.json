{"cell_type":{"ec8c8b6c":"code","a16294fc":"code","189759ec":"code","43e739a1":"code","d3349462":"code","1dc254aa":"code","8875be78":"code","c5f50635":"code","366f76c7":"code","71fa4a34":"code","920334c9":"code","81adbcb6":"markdown","7edac384":"markdown","16dfb73c":"markdown","18dfbdb7":"markdown","abc4090b":"markdown","b9f4c3a9":"markdown"},"source":{"ec8c8b6c":"import numpy as np\n\n#l5kit imports  \nfrom l5kit.data import ChunkedDataset, LocalDataManager\nfrom l5kit.dataset import EgoDataset, AgentDataset\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.configs import load_config_data\nfrom l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\nfrom l5kit.geometry import transform_points\nfrom l5kit.data import PERCEPTION_LABELS\nimport os\n\nimport torch\nimport pandas as pd","a16294fc":"cfg = {\n    'format_version': 4,\n    'data_path': \"..\/input\/lyft-motion-prediction-autonomous-vehicles\/\",\n    'model_params': {\n        'model_architecture': 'resnet18',\n        'history_num_frames': 10,\n        'history_step_size': 1,\n        'history_delta_time': 0.1,\n        'future_num_frames': 50,\n        'future_step_size': 1,\n        'future_delta_time': 0.1,\n    },\n\n    'raster_params': {\n        'raster_size': [224, 224],\n        'pixel_size': [0.5, 0.5],\n        'ego_center': [0.25, 0.5],\n        'map_type': 'py_semantic',\n        'satellite_map_key': 'aerial_map\/aerial_map.png',\n        'semantic_map_key': 'semantic_map\/semantic_map.pb',\n        'dataset_meta_key': 'meta.json',\n        'filter_agents_threshold': 0.5\n    },\n\n    'sample_data_loader': {\n        'key': 'scenes\/sample.zarr',\n        'batch_size': 16,\n        'shuffle': False,\n        'num_workers': 4\n    },\n    'train_data_loader': {\n        'key': 'scenes\/train.zarr',\n        'batch_size': 16,\n        'shuffle': True,\n        'num_workers': 4\n    },\n    \n    'val_data_loader': {\n        'key': 'scenes\/validate.zarr',\n        'batch_size': 16,\n        'shuffle': False,\n        'num_workers': 4\n    },\n    \n    'test_data_loader': {\n        'key': 'scenes\/test.zarr',\n        'batch_size': 32,\n        'shuffle': False,\n        'num_workers': 4\n    },\n\n    'train_params': {\n        'max_num_steps': 12000,\n        'checkpoint_every_n_steps': 500,\n    }\n}","189759ec":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f'device {device}')","43e739a1":"DIR_INPUT = cfg[\"data_path\"]\nos.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\ndm = LocalDataManager(None)\nrasterizer = build_rasterizer(cfg, dm)","d3349462":"\"\"\"\nThe agents['label_probabilities'] cannot be loaded entirely into kaggle memory\nHence dividing the dataset into batches and summing up at the end\n\nUsing GPU to run argmax improves runtime massively\n\"\"\"\n\ndef count_Agentlabels_zarr(data_loader_key):\n    dataset_path = dm.require(cfg[data_loader_key][\"key\"])\n    zarr_dataset = ChunkedDataset(dataset_path).open()\n    \n    # get agents from dataset\n    agents = zarr_dataset.agents\n    no_batches = 8\n    batch_len = round(len(agents) \/ no_batches )\n    total_count = []\n\n    for i in range(no_batches):\n        probabilities = torch.from_numpy(agents[i * batch_len : (i+1) * batch_len][\"label_probabilities\"]).to(device)\n        labels_indexes = torch.argmax(probabilities, dim=1)\n\n        counts = []\n        for idx_label, label in enumerate(PERCEPTION_LABELS):\n            counts.append(torch.sum(labels_indexes == idx_label))\n        counts = [x.cpu().numpy().item() for x in counts]\n        print(f'{i} batch result = {counts}')\n        total_count.append(counts)\n        del(probabilities)\n\n    agent_count =  np.sum(total_count, axis = 0).tolist()\n    print(f'Agents distribution for {data_loader_key} is ', agent_count)","1dc254aa":"%%time\n\n\"\"\" Uncomment following lines to run the code \"\"\"\n#train_counts = count_Agentlabels_zarr(\"train_data_loader\")\n#val_counts = count_Agentlabels_zarr(\"val_data_loader\")\n#test_counts = count_Agentlabels_zarr(\"test_data_loader\")","8875be78":"train_counts = [0, 214185074, 0, 96737892, 0, 0, 0, 0, 0, 0, 0, 0, 1667331, 0, 7534327, 0, 0]\nval_counts   = [0, 209417891, 0, 94918785, 0, 0, 0, 0, 0, 0, 0, 0, 1574639, 0, 6706572, 0, 0]\ntest_counts  = [0, 58143776, 0, 27888998, 0, 0, 0, 0, 0, 0, 0, 0, 486857, 0, 2075290, 0, 0]","c5f50635":"df = pd.DataFrame({\"LABELS\" : PERCEPTION_LABELS, \n                   \"train_counts\" : train_counts, \n                   \"val_counts\" : val_counts,\n                   \"test_counts\" : test_counts\n                  })\n\ndf = df.set_index('LABELS')\n\n# selecting only necessary rows\ndf = df[(df.T != 0).any()]\ndf = df.drop('PERCEPTION_LABEL_UNKNOWN')","366f76c7":"df","71fa4a34":"# normalising to percentages\nfor dataset in ['train_counts', 'val_counts', 'test_counts']:\n    df[dataset] = df[dataset] \/ df[dataset].sum()","920334c9":"df","81adbcb6":"## Configuration","7edac384":"## Loading CUDA device","16dfb73c":"## Loading Dataset","18dfbdb7":"## Import libraries","abc4090b":"## Results for Agent distribution in differnt datasets","b9f4c3a9":"## Conclusion\n\n> ### 1. Distribution of different agents is equal for all the three datasets, which is very crucial for consistent results across training, validation and test sets. \n  \n> ### 2. Percentages of Pedestrian is around 7%, so increasing the pixel resolution might have will most likely have impact in max 7% increase (decisive for top kagglers, less for entry-level people). So, decision on pixel resolution can be decided based on tradeoff between resource and performance objective\n\n\n"}}