{"cell_type":{"4d5f963b":"code","195dc700":"code","dfe7ea08":"code","42989d70":"code","6437d3d0":"code","e4abf2d0":"code","a5f9da0d":"code","c9557958":"code","0eac8e6f":"code","ef7a112d":"code","2b6f7b19":"code","39de9af6":"code","78a5acf4":"code","6b4cb730":"markdown","ccc93d0e":"markdown","cac53913":"markdown","6c1bde01":"markdown","91c86ad2":"markdown","468f6639":"markdown"},"source":{"4d5f963b":"# Some error occured while training using TPU. Don't know why. Use GPU instead.\nimport tensorflow as tf\nAUTO = tf.data.experimental.AUTOTUNE\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","195dc700":"%matplotlib inline\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras import models, layers\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# show all output, not only the last one.\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n\ntrainExceptionList = {\n    \"buildings\": {\"2362.jpg\",\"2670.jpg\",\"3928.jpg\",\"5579.jpg\",\"5743.jpg\",\"7443.jpg\",\"7859.jpg\",\"14903.jpg\",\"15545.jpg\",\"15626.jpg\",\"17064.jpg\", \"20054.jpg\",\"19800.jpg\", \"17731.jpg\", \"17451.jpg\", \"16931.jpg\", \"16807.jpg\", \"16594.jpg\", \"16123.jpg\", \"15626.jpg\", \"15236.jpg\", \"15242.jpg\", \"13796.jpg\", \"12578.jpg\", \"0.jpg\", \"304.jpg\", \"1112.jpg\", \"1647.jpg\", \"1751.jpg\", \"2670.jpg\", \"2880.jpg\", \"6685.jpg\", \"7311.jpg\", \"8475.jpg\", \"10502.jpg\", \"10849.jpg\", \"11209.jpg\", \"13111.jpg\", \"\"},\n    \"forest\": {\"1080.jpg\",\"1331.jpg\",\"1705.jpg\",\"2222.jpg\",\"4900.jpg\",\"5610.jpg\",\"7874.jpg\",\"11978.jpg\", \"1705.jpg\", \"2013.jpg\", \"2145.jpg\", \"5610.jpg\", \"6662.jpg\", \"9381.jpg\", \"10155.jpg\", \"\"},\n    \"glacier\": {\"68.jpg\",\"496.jpg\",\"550.jpg\",\"697.jpg\",\"746.jpg\",\"910.jpg\",\"1232.jpg\",\"1512.jpg\",\"1669.jpg\",\"1864.jpg\",\"2000.jpg\",\"2239.jpg\",\"2249.jpg\",\"2254.jpg\",\"2559.jpg\",\"3510.jpg\",\"3659.jpg\",\"3808.jpg\",\"4084.jpg\",\"4385.jpg\",\"5078.jpg\",\"5217.jpg\",\"5683.jpg\",\"6076.jpg\",\"6283.jpg\",\"6418.jpg\",\"6734.jpg\",\"6967.jpg\",\"7127.jpg\",\"7831.jpg\",\"7840.jpg\",\"8137.jpg\",\"8419.jpg\",\"8987.jpg\",\"9091.jpg\",\"9201.jpg\",\"9406.jpg\",\"10505.jpg\",\"10704.jpg\",\"10848.jpg\",\"10935.jpg\",\"10970.jpg\",\"11070.jpg\",\"11821.jpg\", \"2837.jpg\", \"2917.jpg\", \"3747.jpg\", \"3808.jpg\", \"4385.jpg\", \"4476.jpg\", \"5078.jpg\", \"5204.jpg\", \"2659.jpg\", \"2372.jpg\", \"2429.jpg\", \"2479.jpg\", \"2510.jpg\", \"2511.jpg\", \"2559.jpg\", \"1864.jpg\", \"2000.jpg\", \"2125.jpg\", \"2113.jpg\", \"2205.jpg\", \"2239.jpg\", \"2249.jpg\", \"2254.jpg\", \"2280.jpg\", \"893.jpg\", \"910.jpg\", \"1512.jpg\", \"1527.jpg\", \"1626.jpg\", \"1669.jpg\", \"1676.jpg\", \"1734.jpg\", \"1778.jpg\", \"496.jpg\", \"16544.jpg\", \"16467.jpg\", \"16416.jpg\", \"16334.jpg\", \"15884.jpg\", \"20028.jpg\", \"19988.jpg\", \"19975.jpg\", \"19441.jpg\", \"17926.jpg\", \"17925.jpg\", \"17926.jpg\", \"17957.jpg\", \"17841.jpg\", \"17682.jpg\", \"17593.jpg\", \"17531.jpg\", \"17168.jpg\", \"17312.jpg\", \"15477.jpg\", \"15468.jpg\", \"15401.jpg\", \"15290.jpg\", \"15267.jpg\", \"15196.jpg\", \"15164.jpg\", \"15039.jpg\", \"15036.jpg\", \"14979.jpg\", \"15032.jpg\", \"14923.jpg\", \"14860.jpg\", \"14851.jpg\", \"14431.jpg\", \"13955.jpg\", \"13953.jpg\", \"13608.jpg\", \"13417.jpg\", \"13384.jpg\", \"12624.jpg\", \"12723.jpg\", },\n    \"mountain\": {\"369.jpg\",\"380.jpg\",\"518.jpg\",\"571.jpg\",\"659.jpg\",\"820.jpg\",\"822.jpg\",\"857.jpg\",\"959.jpg\",\"1069.jpg\",\"1087.jpg\", \"1155.jpg\",\"1183.jpg\",\"1237.jpg\",\"1287.jpg\",\"1454.jpg\",\"1612.jpg\",\"2110.jpg\",\"2196.jpg\",\"2618.jpg\",\"2730.jpg\",\"3234.jpg\",\"3994.jpg\",\"5263.jpg\",\"5514.jpg\",\"5720.jpg\",\"6022.jpg\",\"6157.jpg\",\"6222.jpg\",\"6267.jpg\",\"8377.jpg\",\"8549.jpg\",\"8960.jpg\",\"9120.jpg\",\"9799.jpg\",\"12014.jpg\",\"12747.jpg\",\"13245.jpg\",\"13934.jpg\",\"13885.jpg\",\"15704.jpg\",\"18656.jpg\",\"19051.jpg\", \"9120.jpg\", \"5610.jpg\", \"7874.jpg\", \"10155.jpg\"},\n    \"sea\": {\"1.jpg\",\"89.jpg\",\"97.jpg\",\"846.jpg\",\"1175.jpg\",\"1236.jpg\",\"1282.jpg\",\"1635.jpg\",\"1791.jpg\",\"2489.jpg\",\"2907.jpg\",\"4529.jpg\",\"4788.jpg\",\"5127.jpg\",\"5129.jpg\",\"5231.jpg\",\"5407.jpg\",\"7081.jpg\",\"8217.jpg\",\"9313.jpg\",\"10776.jpg\",\"11039.jpg\",\"11253.jpg\",\"11526.jpg\",\"12479.jpg\",\"12562.jpg\",\"13858.jpg\",\"16206.jpg\",\"18689.jpg\", \"406.jpg\", \"2154.jpg\", \"2460.jpg\", \"4762.jpg\", \"4788.jpg\", \"5129.jpg\", \"9313.jpg\", \"10776.jpg\"},\n    \"street\": {\"2.jpg\", \"9.jpg\", \"12.jpg\", \"19.jpg\", \"872.jpg\",\"1142.jpg\",\"1730.jpg\",\"8055.jpg\",\"10691.jpg\",\"11052.jpg\",\"15115.jpg\",\"15607.jpg\"}\n}\n\ntestExceptionList = {\n    \"buildings\":{\"20968.jpg\",\"21580.jpg\"},\n    \"forest\":{\"aaa\"},\n    \"glacier\":{\"20227.jpg\",\"20491.jpg\",\"20745.jpg\",\"21129.jpg\",\"21248.jpg\",\"21452.jpg\",\"21383.jpg\",\"21559.jpg\",\"21840.jpg\",\"21949.jpg\",\"22011.jpg\",\"22054.jpg\",\"22258.jpg\",\"22259.jpg\",\"22473.jpg\",\"22625.jpg\",\"22942.jpg\",\"22943.jpg\",\"23108.jpg\",\"23282.jpg\",\"23314.jpg\",\"23450.jpg\",\"23593.jpg\",\"23819.jpg\",\"24096.jpg\",\"24176.jpg\",\"24279.jpg\",\"24323.jpg\"},\n    \"mountain\":{\"20378.jpg\",\"20584.jpg\",\"21725.jpg\"},\n    \"sea\":{\"20138.jpg\",\"20256.jpg\",\"20257.jpg\",\"20300.jpg\",\"20494.jpg\",\"20519.jpg\",\"20622.jpg\",\"20679.jpg\",\"20687.jpg\",\"20722.jpg\",\"20942.jpg\",\"21837.jpg\",\"22839.jpg\",\"23324.jpg\"},\n    \"street\":{\"21479.jpg\",\"24315.jpg\"}\n}\n\n# get data\n# 'cat' is short for category\nrootPath = os.path.abspath(\"..\/input\/intel-image-classification\")\nsubDir = os.listdir(rootPath)\ndataset_raw = []\n\nfor file in subDir:\n    if file == \"seg_pred\":\n        continue\n    path = os.path.join(rootPath, file, file)\n    cats = os.listdir(path)\n    for cat in cats:\n        catPath = os.path.join(path, cat)\n        imgs = os.listdir(catPath)\n        expList1 = trainExceptionList[cat]\n        expList2 = testExceptionList[cat]\n        for img in imgs:\n            if img in expList1 or img in expList2:\n                continue\n            imgPath = os.path.join(catPath, img)\n            dataset_raw.append([imgPath, cat])\n        \n         \ndf = pd.DataFrame(dataset_raw, columns=['imgPath', 'label'])\n\n\npd.set_option('display.max_colwidth', None)\ndf.head(5)\ndf.describe()\ndf['label'].value_counts()","dfe7ea08":"trainData, testData = train_test_split(df, test_size = 0.2, random_state = 0, shuffle = True)\n\ntrain_generator = ImageDataGenerator(\n    validation_split = 0.2,\n    rescale = 1.0 \/ 255,\n#     rotation_range = 4,\n#     zoom_range = 0.1,\n#     horizontal_flip = True,\n#     brightness_range = (0.5, 1.5),\n#     width_shift_range=0.2, \n#     height_shift_range=0.2, \n)\n\ntest_generator = ImageDataGenerator(\n    rescale = 1.0 \/ 255\n)\n\nbatchSize = 96\nimgSize = (224, 224)\n\ntrainImgs = train_generator.flow_from_dataframe(\n    dataframe = trainData,\n    x_col = \"imgPath\",\n    y_col = \"label\",\n    target_size = imgSize,\n    color_mode = \"rgb\",\n    class_mode = \"sparse\",\n    batch_size = batchSize,\n    shuffle = True,\n    subset = \"training\"\n)\n\n\nvalImgs = train_generator.flow_from_dataframe(\n    dataframe = trainData,\n    x_col = \"imgPath\",\n    y_col = \"label\",\n    target_size = imgSize,\n    color_mode = \"rgb\",\n    class_mode = \"sparse\",\n    batch_size = batchSize,\n    shuffle = True,\n    subset = \"validation\"\n)\n\ntestImgs = test_generator.flow_from_dataframe(\n    dataframe = testData,\n    x_col = \"imgPath\",\n    y_col = \"label\",\n    target_size = imgSize,\n    color_mode = \"rgb\",\n    class_mode = \"sparse\",\n    batch_size = batchSize,\n    shuffle = False\n)\n\nstop_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\nreduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience = 4, verbose=1, min_lr = 1e-6)","42989d70":"import random as r\nf,ax = plt.subplots(3,3) \nf.subplots_adjust(0,0,2,3,wspace=0, hspace=0)\ncount = 0\nfor i in range(0,3,1):\n    for j in range(0,3,1):\n        \n        ax[i,j].imshow(trainImgs[0][0][count])\n        #ax[i,j].set_title(df[imgPath][rnd_number])\n        ax[i,j].axis('off')\n        count = count + 1","6437d3d0":"with strategy.scope():\n\n    model = models.Sequential()\n\n    model.add(layers.Conv2D(64, (2, 2), padding = \"Same\", activation = \"relu\", input_shape=(224, 224, 3)))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Dropout(0.4))\n    \n    model.add(layers.Conv2D(96, (2, 2), padding = \"Same\", activation = 'relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    \n    model.add(layers.Conv2D(128, (3, 3), padding = \"Same\", activation = 'relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Dropout(0.4))\n\n    model.add(layers.Conv2D(256, (3, 3), padding = \"Same\", activation = 'relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Dropout(0.2))\n    \n    model.add(layers.Flatten())\n\n    model.add(layers.Dense(128, activation = 'relu'))\n    model.add(layers.Dropout(0.2))\n    \n    model.add(layers.Dense(64, activation = 'relu'))\n    model.add(layers.Dropout(0.2))\n    \n    model.add(layers.Dense(6, activation = 'softmax'))\n\n    model.compile(\n        optimizer = 'Adam',\n        loss = 'SparseCategoricalCrossentropy',\n        metrics = ['accuracy']\n    )\n\n    model.summary()\n","e4abf2d0":"    shape=(224,224,3) \n    input_tensor=keras.Input(shape=shape)\n    base_model=keras.applications.DenseNet169(input_tensor=input_tensor,weights=None,include_top=False)\n    \n    for layer in base_model.layers:\n        layer.trainable = False\n        \n#     avg=keras.layers.GlobalAveragePooling2D()(base_model.output)\n#     x2 = layers.Dense(480 ,activation = 'relu')(avg)\n#     preds=keras.layers.Dense(6,activation='softmax',\n#                               kernel_initializer=keras.initializers.RandomNormal(mean=0.0,stddev=0.01),\n#                               bias_initializer=keras.initializers.Zeros(),)(x2)\n\n    avg=keras.layers.AveragePooling2D(2,padding='valid')(base_model.output)\n    depth=keras.layers.DepthwiseConv2D(3,\n                                          depthwise_initializer=keras.initializers.RandomNormal(mean=0.0,stddev=0.01),\n                                          bias_initializer=keras.initializers.Zeros(),depthwise_constraint=keras.constraints.NonNeg())(avg)\n    flat=keras.layers.Flatten()(depth)\n    preds=keras.layers.Dense(6,activation='softmax',\n                              kernel_initializer=keras.initializers.RandomNormal(mean=0.0,stddev=0.01),\n                              bias_initializer=keras.initializers.Zeros(),)(flat)\n    model=keras.Model(inputs=base_model.input, outputs=preds)  \n\n    ##################################\n    \n        \n    model.compile(optimizer=\"Adam\", loss='SparseCategoricalCrossentropy',metrics=['accuracy'])\n    model.summary()","a5f9da0d":"md = keras.applications.MobileNetV2(input_shape = (224, 224, 3), weights = \"imagenet\", include_top = False)\nfor layer in md.layers:\n      layer.trainable = False\nx1 = layers.GlobalAvgPool2D(name='global_avg')(md.output)\nx2 = tf.keras.layers.Flatten()(x1)\nx3 = layers.Dense(768 ,activation = 'relu')(x2)\ndrop1 = tf.keras.layers.Dropout(0.4)(x3)\nx4 = layers.Dense(256 ,activation = 'relu')(drop1)\ndrop2 = tf.keras.layers.Dropout(0.3)(x4)\nprediction = layers.Dense(6, activation='softmax')(drop2)\nmodel = keras.Model(inputs=md.input, outputs=prediction)\nmodel.summary()\noptimizer = keras.optimizers.Adam(learning_rate = 1e-3)\n#optimizer = keras.optimizers.Adamax()\n\nmodel.compile(\n    loss='SparseCategoricalCrossentropy',\n    optimizer= optimizer,\n    metrics=['accuracy'],\n)","c9557958":"with strategy.scope():\n    res = tf.keras.applications.resnet_v2.ResNet50V2(\n        input_shape=(224, 224, 3), \n        include_top=False , weights= 'imagenet', input_tensor=None, pooling=None,\n    )\n\n    for layer in res.layers:\n          layer.trainable = False\n    Global_pool = tf.keras.layers.GlobalAveragePooling2D()(res.output)\n    #Global_pool=keras.layers.AveragePooling2D(2,padding='valid')(res.output)\n\n    flat = tf.keras.layers.Flatten()(Global_pool)\n    drop = tf.keras.layers.Dropout(0.3)(flat)\n    dense1 = tf.keras.layers.Dense(960,activation = 'relu')(drop)\n    drop = tf.keras.layers.Dropout(0.3)(dense1)\n    dense2 = tf.keras.layers.Dense(540,activation = 'relu')(drop)\n    last_layer = tf.keras.layers.Dense(6 ,activation = 'softmax')(dense2)\n    resmodel = tf.keras.Model (inputs = res.input ,outputs = last_layer)\n\n    resmodel.compile(\n        optimizer = tf.keras.optimizers.RMSprop(),\n        loss = 'SparseCategoricalCrossentropy',\n        metrics = 'accuracy' \n    )\n\n    resmodel.summary()","0eac8e6f":"reshistory = resmodel.fit(trainImgs, epochs = 16, validation_data = valImgs, verbose = 1, callbacks = [stop_callback,reduce_lr])","ef7a112d":"history = model.fit(trainImgs, epochs = 30, validation_data = valImgs, verbose = 1, callbacks = [stop_callback,reduce_lr])","2b6f7b19":"#plot graphs\nhis = reshistory\nhistory_dic = his.history\ntrain_acc = history_dic['accuracy']\ntrain_loss = history_dic['loss']\nval_acc = history_dic['val_accuracy']\nval_loss = history_dic['val_loss']\nepochs = range(1, len(train_acc) + 1)\n\nfigure, axis = plt.subplots(1, 2, figsize=(20, 10))\naxis[0].plot(epochs, train_acc, label=\"train\")\naxis[0].plot(epochs, val_acc, label=\"val\")\naxis[0].set_title('Accuracy')\naxis[0].legend()\naxis[1].plot(epochs, train_loss, label=\"train\")\naxis[1].plot(epochs, val_loss, label=\"val\")\naxis[1].set_title('Loss')\naxis[1].legend()\n\nplt.show()","39de9af6":"mdl = resmodel\npred = mdl.predict(testImgs)\n\noutput = np.argmax(pred, axis=1)\nprint(output)\nlabels = testImgs.class_indices\nprint(labels)\npredLabel = []\n\nindexToLabel = dict((v, k) for k, v in labels.items())\n\nfor i in output:\n    predLabel.append(indexToLabel[i])\n\n\nprint(classification_report(testData.label, predLabel, digits = 4))\nprint(confusion_matrix(testData.label, predLabel))","78a5acf4":"!nvidia-smi ","6b4cb730":"### \u4f7f\u7528\u6734\u7d20CNN","ccc93d0e":"### \u4f7f\u7528DenseNet","cac53913":"### \u4f7f\u7528MobileNet","6c1bde01":"### \u8bad\u7ec3\u5176\u5b83\u6a21\u578b","91c86ad2":"### \u4f7f\u7528ResNet","468f6639":"### \u8bad\u7ec3\u57fa\u4e8eResNet\u7684\u6a21\u578b"}}