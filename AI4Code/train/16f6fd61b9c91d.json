{"cell_type":{"fd0c3c96":"code","4e36b761":"code","c84411a7":"code","b109e836":"code","fdd38640":"code","08946ed7":"code","b5880ddd":"code","ac44aa8d":"code","e401e1b3":"code","1dca149d":"code","e650b47b":"code","92d9713a":"code","02caf267":"code","52a06bdb":"code","e489bd2e":"code","c06a92ea":"code","095ddc73":"code","b038d085":"code","d8ffa908":"code","39b98f55":"code","eb226f47":"code","196c829a":"code","9ffd528c":"code","b35f50ed":"code","9228b00f":"code","8882c360":"code","ffc35ada":"code","465b0ce5":"code","b8c30db0":"code","e91d0979":"code","31357232":"code","eed13922":"code","b47f4eea":"code","28f242cc":"code","aec4e658":"code","6a5ea6a5":"code","55df28ba":"code","8e1e9e92":"code","a183774c":"code","b509f6d3":"code","464e90aa":"code","9887f442":"code","09546ff9":"code","9f21f3e5":"code","a40e9a3a":"code","7d6a0d8a":"code","543b9aa7":"code","0b12af88":"code","0ede5356":"code","5ecb81f1":"code","b94aecfe":"code","671bd450":"code","e23bad39":"code","c05b37b2":"code","8b47c097":"code","233b7760":"code","0f00859c":"code","04c6810f":"markdown","23be3cf8":"markdown","366807fa":"markdown","9f45f3ee":"markdown","4bc8412f":"markdown","2741e1fe":"markdown","0219791d":"markdown","81c8806d":"markdown","0a7e8658":"markdown","c65443af":"markdown","a74163e6":"markdown"},"source":{"fd0c3c96":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVC, SVC\nfrom sklearn.metrics import accuracy_score, classification_report,confusion_matrix\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\n\nimport json\nimport ast\nimport eli5\nimport shap\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, train_test_split\nfrom sklearn.linear_model import Ridge, RidgeCV\nimport gc\nfrom catboost import CatBoostClassifier\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport altair as alt\nfrom IPython.display import HTML\nfrom sklearn.linear_model import LinearRegression\n\nfrom scipy.signal import hilbert\nfrom scipy.signal import hann\nfrom scipy.signal import convolve\nfrom scipy import stats\nfrom sklearn.kernel_ridge import KernelRidge\n\nfrom bayes_opt import BayesianOptimization\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4e36b761":"data = pd.read_csv('..\/input\/weatherAUS.csv')","c84411a7":"data.shape","b109e836":"data.head()","fdd38640":"data.tail()","08946ed7":"data.sample(5)","b5880ddd":"data.columns","ac44aa8d":"data.dtypes","e401e1b3":"data.count().sort_values()","1dca149d":"data = data.drop(columns=['Date','Location','RISK_MM','Evaporation','Sunshine','Cloud9am','Cloud3pm'])","e650b47b":"data = data.dropna(how='any')","92d9713a":"data.isnull().sum()","02caf267":"data.shape","52a06bdb":"plt.figure(figsize=(8,8))\nsns.countplot(data=data,x='WindGustDir')","e489bd2e":"plt.figure(figsize=(8,8))\nsns.countplot(data=data,x='WindDir9am')","c06a92ea":"plt.figure(figsize=(8,8))\nsns.countplot(data=data,x='WindDir3pm')","095ddc73":"plt.figure(figsize=(8,8))\nsns.countplot(data=data,x='RainToday')","b038d085":"plt.figure(figsize=(8,8))\nsns.countplot(data=data,x='RainTomorrow')","d8ffa908":"plt.figure(figsize=(8,8))\nsns.FacetGrid(data, hue=\"RainTomorrow\", size=8).map(sns.kdeplot, \"MinTemp\").add_legend()\nplt.ioff() \nplt.show()","39b98f55":"plt.figure(figsize=(8,8))\nsns.boxplot(data=data,x=\"RainTomorrow\",y=\"MinTemp\")","eb226f47":"plt.figure(figsize=(8,8))\nsns.FacetGrid(data, hue=\"RainTomorrow\", size=8).map(sns.kdeplot, \"MaxTemp\").add_legend()\nplt.ioff() \nplt.show()","196c829a":"plt.figure(figsize=(8,8))\nsns.boxplot(data=data,x=\"RainTomorrow\",y=\"MaxTemp\")","9ffd528c":"plt.figure(figsize=(8,8))\nsns.FacetGrid(data, hue=\"RainTomorrow\", size=8).map(sns.kdeplot, \"Rainfall\").add_legend()\nplt.ioff() \nplt.show()","b35f50ed":"plt.figure(figsize=(8,8))\nsns.boxplot(data=data,x=\"RainTomorrow\",y=\"Rainfall\")","9228b00f":"plt.figure(figsize=(8,8))\nsns.FacetGrid(data, hue=\"RainTomorrow\", size=8).map(sns.kdeplot, \"WindGustSpeed\").add_legend()\nplt.ioff() \nplt.show()","8882c360":"plt.figure(figsize=(8,8))\nsns.boxplot(data=data,x=\"RainTomorrow\",y=\"WindGustSpeed\")","ffc35ada":"plt.figure(figsize=(8,8))\nsns.FacetGrid(data, hue=\"RainTomorrow\", size=8).map(sns.kdeplot, \"WindSpeed9am\").add_legend()\nplt.ioff() \nplt.show()","465b0ce5":"plt.figure(figsize=(8,8))\nsns.boxplot(data=data,x=\"RainTomorrow\",y=\"WindSpeed9am\")","b8c30db0":"plt.figure(figsize=(8,8))\nsns.FacetGrid(data, hue=\"RainTomorrow\", size=8).map(sns.kdeplot, \"WindSpeed3pm\").add_legend()\nplt.ioff() \nplt.show()","e91d0979":"plt.figure(figsize=(8,8))\nsns.boxplot(data=data,x=\"RainTomorrow\",y=\"WindSpeed3pm\")","31357232":"plt.figure(figsize=(8,8))\nsns.FacetGrid(data, hue=\"RainTomorrow\", size=8).map(sns.kdeplot, \"Humidity9am\").add_legend()\nplt.ioff() \nplt.show()","eed13922":"plt.figure(figsize=(8,8))\nsns.boxplot(data=data,x=\"RainTomorrow\",y=\"Humidity9am\")","b47f4eea":"plt.figure(figsize=(8,8))\nsns.FacetGrid(data, hue=\"RainTomorrow\", size=8).map(sns.kdeplot, \"Humidity3pm\").add_legend()\nplt.ioff() \nplt.show()","28f242cc":"plt.figure(figsize=(8,8))\nsns.boxplot(data=data,x=\"RainTomorrow\",y=\"Humidity3pm\")","aec4e658":"plt.figure(figsize=(8,8))\nsns.FacetGrid(data, hue=\"RainTomorrow\", size=8).map(sns.kdeplot, \"Pressure9am\").add_legend()\nplt.ioff() \nplt.show()","6a5ea6a5":"plt.figure(figsize=(8,8))\nsns.boxplot(data=data,x=\"RainTomorrow\",y=\"Pressure9am\")","55df28ba":"plt.figure(figsize=(8,8))\nsns.FacetGrid(data, hue=\"RainTomorrow\", size=8).map(sns.kdeplot, \"Pressure3pm\").add_legend()\nplt.ioff() \nplt.show()","8e1e9e92":"plt.figure(figsize=(8,8))\nsns.boxplot(data=data,x=\"RainTomorrow\",y=\"Pressure3pm\")","a183774c":"plt.figure(figsize=(8,8))\nsns.FacetGrid(data, hue=\"RainTomorrow\", size=8).map(sns.kdeplot, \"Temp9am\").add_legend()\nplt.ioff() \nplt.show()","b509f6d3":"plt.figure(figsize=(8,8))\nsns.boxplot(data=data,x=\"RainTomorrow\",y=\"Temp9am\")","464e90aa":"plt.figure(figsize=(8,8))\nsns.FacetGrid(data, hue=\"RainTomorrow\", size=8).map(sns.kdeplot, \"Temp3pm\").add_legend()\nplt.ioff() \nplt.show()","9887f442":"plt.figure(figsize=(8,8))\nsns.boxplot(data=data,x=\"RainTomorrow\",y=\"Temp3pm\")","09546ff9":"data['RainToday'].replace({'No': 0, 'Yes': 1},inplace = True)\ndata['RainTomorrow'].replace({'No': 0, 'Yes': 1},inplace = True)\ny = data['RainTomorrow']\ndata = data.drop(columns=['RainTomorrow'])\ntrain_df = pd.get_dummies(data,columns=['WindGustDir', 'WindDir3pm', 'WindDir9am'])","9f21f3e5":"train_df.head()","a40e9a3a":"n_fold = 20\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=11)","7d6a0d8a":"X_train,X_test,y_train,y_test = train_test_split(train_df,y,random_state=0)","543b9aa7":"X_train.shape","0b12af88":"X_test.shape","0ede5356":"def lgbm_evaluate(**params):\n    warnings.simplefilter('ignore')\n    \n    params['num_leaves'] = int(params['num_leaves'])\n    params['max_depth'] = int(params['max_depth'])\n        \n    clf = lgb.LGBMClassifier(**params, n_estimators=20000, nthread=-1)\n\n    test_pred_proba = np.zeros((X_train.shape[0], 2))\n    \n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_train, y_train)):\n        X_train_bo, X_valid = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n        y_train_bo, y_valid = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n        \n        model = lgb.LGBMClassifier(**params, n_estimators = 10000, n_jobs = -1)\n        model.fit(X_train_bo, y_train_bo, \n                eval_set=[(X_train_bo, y_train_bo), (X_valid, y_valid)], eval_metric='binary_logloss',\n                verbose=False, early_stopping_rounds=200)\n\n        y_pred_valid = model.predict_proba(X_valid)\n\n        test_pred_proba[valid_idx] = y_pred_valid\n\n    return accuracy_score(y_valid, y_pred_valid.argmax(1))","5ecb81f1":"# Bayesian optimization requires a very long time.\n# I only the results here.\n'''\nparams = {'colsample_bytree': (0.6, 1),\n     'learning_rate': (.001, .08), \n      'num_leaves': (8, 124), \n      'subsample': (0.6, 1), \n      'max_depth': (3, 25), \n      'reg_alpha': (.05, 15.0), \n      'reg_lambda': (.05, 15.0), \n      'min_split_gain': (.001, .03),\n      'min_child_weight': (12, 80)}\n\nbo = BayesianOptimization(lgbm_evaluate, params)\nbo.maximize(init_points=5, n_iter=20)\n'''","b94aecfe":"# bo.max['params']\n\n# Bayesian optimization results \n\n# {'colsample_bytree': 0.6041479784261461,\n# 'learning_rate': 0.01792647253091717,\n#  'max_depth': 22.893284639055306,\n#  'min_child_weight': 12.821009963761202,\n#  'min_split_gain': 0.004300308462511252,\n#  'num_leaves': 122.66462568820884,\n#  'reg_alpha': 0.364297696554819,\n#  'reg_lambda': 14.493771665517722,\n#  'subsample': 0.9037283661609925}","671bd450":"def eval_acc(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'acc', accuracy_score(labels, preds.argmax(1)), True\n\ndef train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n\n    oof = np.zeros((len(X), 2))\n    prediction = np.zeros((len(X_test), 2))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators = 10000, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='multi_logloss',\n                    verbose=5000, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict_proba(X_valid)\n            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict_proba(X_valid)\n            score = accuracy_score(y_valid, y_pred_valid.argmax(1))\n            print(f'Fold {fold_n}. Accuracy: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_test)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=20000,  eval_metric='MAE', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid\n        scores.append(accuracy_score(y_valid, y_pred_valid.argmax(1)))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction\n    \n    else:\n        return oof, prediction","e23bad39":"\"\"\"\nparams = {'num_leaves': int(bo.max['params']['num_leaves']),\n          'min_data_in_leaf': int(bo.max['params']['min_child_weight']),\n          'min_split_gain': bo.max['params']['min_split_gain'],\n          'objective': 'binary',\n          'max_depth': int(bo.max['params']['max_depth']),\n          'learning_rate': bo.max['params']['learning_rate'],\n          \"boosting\": \"gbdt\",\n          \"bagging_freq\": 5,\n          \"bagging_fraction\": bo.max['params']['subsample'],\n          \"bagging_seed\": 11,\n          \"verbosity\": -1,\n          'reg_alpha': bo.max['params']['reg_alpha'],\n          'reg_lambda': bo.max['params']['reg_lambda'],\n          \"num_class\": 1,\n          'nthread': -1\n         }\n\"\"\"\n# I use bayesian optimization results\nparams = {'num_leaves': int(122.66462568820884),\n          'min_data_in_leaf': int(12.821009963761202),\n          'min_split_gain': 0.004300308462511252,\n          'objective': 'binary',\n          'max_depth': int(22.893284639055306),\n          'learning_rate': 0.01792647253091717,\n          \"boosting\": \"gbdt\",\n          \"bagging_freq\": 5,\n          \"bagging_fraction\": 0.9037283661609925,\n          \"bagging_seed\": 11,\n          \"verbosity\": -1,\n          'reg_alpha': 0.364297696554819,\n          'reg_lambda': 14.493771665517722,\n          \"num_class\": 1,\n          'nthread': -1\n         }\noof_lgb, prediction_lgb, feature_importance = train_model(X=X_train, X_test=X_test, y=y_train, params=params, model_type='lgb', plot_feature_importance=True)","c05b37b2":"print(\"Test score: \",accuracy_score(y_test,prediction_lgb.argmax(1)))","8b47c097":"import itertools\n\ndef plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix', size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()","233b7760":"plot_confusion_matrix(y_test, prediction_lgb.argmax(1), ['Not Rainy','Rainy'])","0f00859c":"print(classification_report(y_test, prediction_lgb.argmax(1)))","04c6810f":"# Data Visualization","23be3cf8":"We can observe that some features show difference clearly.  (For example , Humidity9am and Humidity3pm)  \nLet's create predict model using LGBM!\n","366807fa":"## Train model","9f45f3ee":"RainTomorrow is imbalance.  ","4bc8412f":"# Data Load","2741e1fe":"## Bayesian Optimization","0219791d":"Evaporation, Sunshine,Cloud9am, and Cloud3pm is considerably missing value.  \nRisk-MM can not use classification since Risk-MM leak answers to our model.  \nDate and Location don't use classification.  \nSo I dropt those columns.  ","81c8806d":"# Introduction\nHello,kaggle.  \nThis kernel predict whether it will rain tomorrow.  \nIf you like it, please upvote ;)  ","0a7e8658":"OK ;)","c65443af":"# About this file  \nThis dataset contains daily weather observations from numerous Australian weather stations.  \n\nThe target RainTomorrow means: Did it rain the next day? Yes or No.  \n\nNote: **You should exclude the variable Risk-MM when training your binary classification model.** If you don't exclude it, you will leak the answers to your model and reduce its predictability. Read more about it here.  \n\n* Date\u2026The date of observation  \n* Location\u2026The common name of the location of the weather station\n* MinTemp\u2026The minimum temperature in degrees celsius\n* MaxTemp\u2026The maximum temperature in degrees celsius\n* Rainfall\u2026The amount of rainfall recorded for the day in mm\n* Evaporation\u2026The so-called Class A pan evaporation (mm) in the 24 hours to 9am\n* Sunshine\u2026The number of hours of bright sunshine in the day.\n* WindGustDir\u2026The direction of the strongest wind gust in the 24 hours to midnight\n* WindGustSpeed\u2026The speed (km\/h) of the strongest wind gust in the 24 hours to midnight\n* WindDir9am\u2026Direction of the wind at 9am\n* WindDir3pm\u2026Direction of the wind at 3pm\n* WindSpeed9am\u2026Wind speed (km\/hr) averaged over 10 minutes prior to 9am\n* WindSpeed3pm\u2026Wind speed (km\/hr) averaged over 10 minutes prior to 3pm\n* Humidity9am\u2026Humidity (percent) at 9am\n* Humidity3pm\u2026Humidity (percent) at 3pm\n* Pressure9am\u2026Atmospheric pressure (hpa) reduced to mean sea level at 9am\n* Pressure3pm\u2026Atmospheric pressure (hpa) reduced to mean sea level at 3pm\n* Cloud9am\u2026Fraction of sky obscured by cloud at 9am. This is measured in \"oktas\", which are a unit of eigths. It records how many eigths of the sky are obscured by cloud. A 0 measure indicates completely clear sky whilst an 8 indicates that it is completely overcast.\n* Cloud3pm\u2026Fraction of sky obscured by cloud (in \"oktas\": eighths) at 3pm. See Cload9am for a description of the values\n* Temp9am\u2026Temperature (degrees C) at 9am\n* Temp3pm\u2026Temperature (degrees C) at 3pm\n* RainToday\u2026Boolean: 1 if precipitation (mm) in the 24 hours to 9am exceeds 1mm, otherwise 0\n* RISK_MM\u2026The amount of rain. A kind of measure of the \"risk\".\n* RainTomorrow\u2026The target variable. Did it rain tomorrow?","a74163e6":"# Build model"}}