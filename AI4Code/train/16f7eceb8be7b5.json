{"cell_type":{"0ea2b2ee":"code","f0696e48":"code","a7085bf0":"code","9fb76f5a":"code","d1a8d337":"code","53759cd8":"code","2dbdb7e6":"code","652f5f64":"code","98e9bdf9":"code","09e9b664":"code","4584fb61":"code","225b05d0":"code","c382e0b1":"code","f39cb3ac":"code","35dd92cf":"code","9b1d7e9e":"code","cf237605":"code","840dec19":"code","ba43362b":"code","5c3faea1":"code","c5c186b8":"code","b6180abe":"code","dcb1d260":"markdown","ac5379d7":"markdown","fd9b3e82":"markdown","dff06828":"markdown","1b201eed":"markdown","fc07d639":"markdown","cd7322f0":"markdown","946ecac3":"markdown","6435c506":"markdown","d3b4b378":"markdown","e67a907e":"markdown"},"source":{"0ea2b2ee":"# libraries\nimport random\nimport os, math\nimport numpy as np \nimport pandas as pd \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly\nimport plotly.express as px\n\nfrom collections import Counter, defaultdict\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport optuna\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n# Finding the weighting coefficients for each of the fold-models\n\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, IterativeImputer\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder","f0696e48":"df_train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-feb-2022\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-feb-2022\/test.csv\")","a7085bf0":"print(\"train.csv shape: \", df_train.shape)\nprint(\"test.csv shape: \", df_test.shape)","9fb76f5a":"# Multi-class distribution in the train dataset\nsns.countplot(x=\"target\", data=df_train);\nplt.xticks(rotation=90);","d1a8d337":"for f in df_train.columns:\n    missing_cnt = df_train[f].isnull().sum()\n    if missing_cnt > 0:\n        print(f, missing_cnt\/df_train.shape[0])","53759cd8":"df_train.head()","2dbdb7e6":"# Any categorical features?\nfor col in df_train.columns:\n    if df_train[col].dtype == \"object\":\n        print(col)","652f5f64":"encoder = LabelEncoder()\ndf_train.target = encoder.fit_transform(df_train.target.values)","98e9bdf9":"# Encoded target label map\nprint(\"=\"*30)\nprint(\"TARGET LABEL ENCODING MAP\")\nprint(\"=\"*30)\nfor code in range(10):\n    bacteria_sp = encoder.inverse_transform([code])[0]\n    print(code, \"==>\",  bacteria_sp)","09e9b664":"from scipy.stats import pearsonr","4584fb61":"for f in tqdm(df_train.columns):\n    if f != \"row_id\" or f != \"target\":\n        corr, p = pearsonr(df_train.target.values, df_train[f].values)\n        if abs(corr) > 0.07:\n            print(f, \": \", round(corr, 3), \",  p=\", round(p, 3))","225b05d0":"# train data\ny = df_train.target\nX = df_train.drop(columns=[\"row_id\", \"target\"])\n\n# test data\nX_test = df_test.drop(columns=[\"row_id\"])\ntest_row_ids = df_test.row_id.values","c382e0b1":"from umap import UMAP\nimport umap.plot","f39cb3ac":"mapper = UMAP(n_neighbors=25, n_components=2, metric=\"minkowski\").fit(X[:10000])\numap.plot.points(mapper, labels=y[:10000], theme='fire');","35dd92cf":"# 3D plot\n_umap = UMAP(n_neighbors=25, n_components=3, metric=\"minkowski\", init='random', random_state=22)\nx_umap = _umap.fit_transform(X[:10000])\numap_df = pd.DataFrame(x_umap)\ny_sr = pd.Series(y[:10000], name='label')\ntmp = pd.concat([umap_df, y_sr], axis=1)\nfig = px.scatter_3d(tmp, x=0, y=1, z=2, color='label', labels={'color': 'number'})\nfig.update_traces(marker_size=1)\nfig.show();","9b1d7e9e":"# Data preparation for model training and prediction\n# We will reduce X to lower dimension (try: 25) and use the data for training\n\n# um = UMAP(n_neighbors=25, n_components=15, metric=\"minkowski\", random_state=22)\n# X_red = um.fit_transform(X)\n# X_test_red = um.transform(X_test)","cf237605":"# Evaluation Metric: Categorical accuracy\n# custom evaluation metric: for LGB model\nn_class = 10\ndef accuracy_lgb(y_true, y_hat):\n    y_hat = np.array(y_hat).reshape(len(y_true), n_class)\n    y_hat = np.argmax(y_hat, axis=1)\n    return 'ACCURACY', metrics.accuracy_score(y_true, y_hat), True","840dec19":"NFOLDS = 5\nn_class = 10\nst_kfolds = StratifiedKFold(n_splits=NFOLDS, shuffle=True)\n\nlgb_params = {\n                \"objective\": \"multiclass\",\n                \"boosting_type\": \"dart\",\n                \"n_estimators\": 2400,\n                \"is_unbalance\": \"false\",\n                \"max_depth\": 6,\n                \"num_leaves\": 55,\n                \"reg_alpha\": 5.0,\n                \"reg_lambda\": 10.0,\n                \"learning_rate\": 0.1, \n                \"metric\": [\"multi_logloss\"],\n                #\"device\": \"gpu\",\n                \"verbose\": -1\n            }\n\n\n# f_labels = [f\"feature_{i}\" for i in range(15)]\n# X_red = pd.DataFrame(data=X_red, columns=f_labels)\n\npredictions = np.zeros((X_test.shape[0], n_class))\nmean_acc_score = 0\nfor fold, (tr_idx, val_idx) in enumerate(st_kfolds.split(X, y)):\n    if fold > 0:\n        print(\"=\"*65)\n    print(\"FOLD: \", fold)\n    \n    X_train, y_train = X.iloc[tr_idx], y.iloc[tr_idx]\n    X_valid, y_valid = X.iloc[val_idx], y.iloc[val_idx]\n    \n    print(\"Train size: \", X_train.shape[0])\n    print(\"Validation size: \", X_valid.shape[0])\n    \n    # Model with parameters\n    model = lgb.LGBMClassifier(**lgb_params)\n    model.fit(\n        X_train, y_train, \n        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n        #eval_metric=accuracy_lgb,\n        early_stopping_rounds=100,\n        verbose=100\n    )\n\n    # validation data:\n    valid_proba = model.predict_proba(X_valid)\n    valid_pred = np.argmax(valid_proba, axis=1)\n    acc_score = metrics.accuracy_score(y_valid, valid_pred)\n    print(\"Validation ACC: \", acc_score)\n    mean_acc_score += acc_score\/NFOLDS\n    \n    predictions += model.predict_proba(X_test)\/NFOLDS","ba43362b":"print(\"Mean CV accuracy: \", mean_acc_score)","5c3faea1":"# encoding back to original target class label names\nedcoded_res = np.argmax(predictions, axis=1)\npred_results = encoder.inverse_transform(edcoded_res)","c5c186b8":"# test results submission\nans = pd.DataFrame(data={\"row_id\": test_row_ids, \"target\": pred_results})\nans.to_csv(\"submission.csv\", index=False)","b6180abe":"ans.head()","dcb1d260":"- ***Great!,*** no class imbalance in the train dataset.","ac5379d7":"# **<span style=\"color:#e76f51;\">TPS: Feb 2022: Bacterial Species Classification<\/span>**\n\n- Multi-class classification problem\n- Features: 10-mer snippets of DNA that are sampled and analyzed to give the histogram of base count.\n- What do all the numbers in the features represent? Well, plot below might help a little bit....\n\n![TPS-Feb22 - DNA Composition2.png](attachment:226efe80-0944-4c96-bc13-3166fd10dd49.png)\n\nImage source: https:\/\/www.frontiersin.org\/articles\/10.3389\/fmicb.2020.00257\/full","fd9b3e82":"- **Import required python packages**","dff06828":"## **<span style=\"color:#e76f51;\">Prediction & Submission <\/span>**","1b201eed":"## **<span style=\"color:#e76f51;\">Model & Training<\/span>**\n\n- 5-fold Stratified Cross-Validation training, and oof prediction on test data.","fc07d639":"### **<span style=\"color:#e76f51;\">LabelEncoder<\/span>**\n> Encoding target class (bacterial species name) into integers","cd7322f0":"### **<span style=\"color:#e76f51;\">Dimensionality Reduction<\/span>**\n\n- Reducing the dimension, using UMAP, and plotting on lower dimension.\n- 2D and 3D dimensionality reductions","946ecac3":"- ***Okay***,  no missing values either.","6435c506":"## **<span style=\"color:#e76f51;\">Data<\/span>**\n- > train.csv: row_id, target, and 286 features,\n- > test.csv: row_id, and 286 features,","d3b4b378":"**Thanks for reading!**","e67a907e":"### Training and Testing Data"}}