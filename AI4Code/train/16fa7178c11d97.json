{"cell_type":{"8a2a9d6f":"code","430fa1ac":"code","69a02c61":"code","d16a8496":"code","eb261e0e":"code","bcab0448":"code","659acc7f":"code","14b29846":"code","e58baf54":"code","20b06a23":"code","3396341e":"code","d5b03eff":"code","58377556":"code","2840bd80":"code","57fc9b78":"code","2eabe626":"code","ba05a507":"code","25b7b400":"code","c39f1110":"code","900409bc":"code","c47e6461":"code","f7363eae":"code","260863b1":"code","25cd8c5e":"code","1df1511d":"code","41d731a8":"code","8e84b73a":"code","7e0d88d8":"code","ce12678a":"code","e6cd93e4":"code","0513f970":"code","7347adb7":"code","42718717":"code","17480649":"code","072e3615":"code","585a4c41":"code","1bcc255e":"code","bcaaf0df":"code","f9457928":"code","d26dcc3d":"code","b33f191b":"code","bdf8e9a9":"code","fb187443":"code","b5161b35":"code","1b8f3155":"code","3c5de477":"code","deb207ea":"code","b9038bd1":"code","6232e7ef":"code","6e3930e8":"code","063b3ba4":"code","77ac1976":"code","f01b4aea":"code","e2befb57":"code","e42ba243":"code","6baa9fb0":"code","1f310c15":"code","ee839303":"code","4d08d4cf":"markdown","75f0f8a7":"markdown","5a1cda96":"markdown","fc99d290":"markdown","21c56f89":"markdown","20ef74e2":"markdown","fee9188b":"markdown","ead611eb":"markdown","cc7fbb85":"markdown","6887e5ba":"markdown","13344e3a":"markdown","077c2b6e":"markdown","0134f75f":"markdown","13a7b055":"markdown","d10d7234":"markdown","3afb1cd8":"markdown","5d90a1ed":"markdown","011a2f37":"markdown","f78ab068":"markdown","832942ff":"markdown","442cdf4b":"markdown","dac290bf":"markdown","95dcad26":"markdown","5e0149d6":"markdown"},"source":{"8a2a9d6f":"import pandas as pd\nimport numpy as np","430fa1ac":"train=pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","69a02c61":"train.head()","d16a8496":"train.info()","eb261e0e":"train['Class'].unique()","bcab0448":"train.describe()","659acc7f":"train.isnull().sum()","14b29846":"train.tail()","e58baf54":"train.shape","20b06a23":"train.Amount[train.Class == 1].describe()","3396341e":"train.Amount[train.Class == 0].describe()","d5b03eff":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression","58377556":"y=train['Class']\nX=train.drop(['Class'],axis=1)","2840bd80":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10)","57fc9b78":"model=LogisticRegression(solver='lbfgs',max_iter=1000)","2eabe626":"model.fit(X_train,y_train)","ba05a507":"model.predict(X_test)","25b7b400":"model.score(X_test,y_test)","c39f1110":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec","900409bc":"v_features = train.ix[:,1:29].columns","c47e6461":"plt.figure(figsize=(12,28*4))\ngs = gridspec.GridSpec(28, 1)\nfor i, cn in enumerate(train[v_features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(train[cn][train.Class == 1], bins=50)\n    sns.distplot(train[cn][train.Class == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('histogram of feature: ' + str(cn))\nplt.show();","f7363eae":"correlation_matrix = train.corr()\nfig = plt.figure(figsize=(12,9))\nsns.heatmap(correlation_matrix,vmax=0.8,square = True)\nplt.show()","260863b1":"df=train","25cd8c5e":"df.head()","1df1511d":"df = df.drop(['V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8'], axis =1)","41d731a8":"df.head()","8e84b73a":"y_1=df['Class']\nX_1=df.drop(['Class'],axis=1)","7e0d88d8":"X_train, X_test, y_train, y_test = train_test_split(X_1, y_1, test_size=0.20)","ce12678a":"model.fit(X_train,y_train)","e6cd93e4":"model.score(X_test,y_test)","0513f970":"sns.countplot(train['Class'])","7347adb7":"sns.boxplot(x=\"Class\", y=\"Amount\", hue=\"Class\",data=train, palette=\"PRGn\",showfliers=False)","42718717":"train.head()","17480649":"df_fraud=train[train.Class == 1]","072e3615":"df_fraud.head()","585a4c41":"df_fraud.info()","1bcc255e":"df_genuine=train[train.Class == 0]","bcaaf0df":"df_genuine.info()","f9457928":"# Randomly selecting 4000 rows from the genuine dataset\n\ndf_new_genuine=df_genuine.iloc[58457:60457]\n\n#df_new_gen=df_gen.sample(4000)","d26dcc3d":"# combining the both dataset the dataset with genuine transaction details and fraud transaction details\ntrain_new = pd.concat([df_new_genuine, df_fraud],ignore_index=True, sort =False)","b33f191b":"train_new.head()","bdf8e9a9":"train_new.info()","fb187443":"y_2=train_new['Class']\nX_2=train_new.drop(['Class'],axis=1)","b5161b35":"X_train, X_test, y_train, y_test = train_test_split(X_2, y_2, test_size=0.20)","1b8f3155":"model.fit(X_train,y_train)","3c5de477":"model.score(X_test,y_test)","deb207ea":"train.head()","b9038bd1":"for _ in range(5):\n    df_fraud = pd.concat([df_fraud, df_fraud],ignore_index=True, sort =False)","6232e7ef":"df_fraud_new=df_fraud","6e3930e8":"df_fraud_new.info()","063b3ba4":"df_fraud_new.head()","77ac1976":"train_new_1 = pd.concat([df_genuine, df_fraud_new],ignore_index=True, sort =False)","f01b4aea":"train_new_1 .iloc[np.random.permutation(len(train_new_1 ))]","e2befb57":"train_new_1.info()","e42ba243":"y_3=train_new_1['Class']\nX_3=train_new_1.drop(['Class'],axis=1)","6baa9fb0":"X_train, X_test, y_train, y_test = train_test_split(X_3, y_3, test_size=0.20)","1f310c15":"model.fit(X_train,y_train)","ee839303":"model.score(X_test,y_test)","4d08d4cf":"# OVER-SAMPLING","75f0f8a7":"###### Now the dataframe has 15744 rows for the fraud class.\n###### we can proceed with this over-sampled data for training and prediction.","5a1cda96":"###### Shuffling the dataset ","fc99d290":"###### running the above cell will replicate the df_fraud dataframe and thus it will create a dataframe with more rows with the same data.","21c56f89":"###### let's do some data visualisation to get some idea about the behaviour of the features","20ef74e2":"Dropping some of the columns from the dataset that were similar with the other columns","fee9188b":"###### it's clearly visible that the dataset is highly imbalanced\n###### the number of rows for both the classes in the target variable differs with a very large margin,which is not good\n###### so we don't have enough examples of one class(ie the fraud class in this case) to give it to our model for training.\n###### we will take some steps for this imbalanced dataset but let's try first with this dataset.","ead611eb":"first we used logistic regression on the imbalaced data then we did some sampling and again performed logistic regression on the data.\n\nwe did both under-sampling and over-sampling and checked the performance of our model.\n\nit's always good to do sampling on the dataset if it is imbalanced to give the model appropriate number of examples of both the target for better training.","cc7fbb85":"###### the accuracy with a logistic regression model is really good somewhere around 99.8%","6887e5ba":"#  UNDER-SAMPLING","13344e3a":"#### Time - Number of seconds elapsed between this transaction and the first transaction in the dataset\n\n##### Amount - Transaction amount\n\n##### Class 1 for fraudulent transactions , 0 otherwise","077c2b6e":"###### loading the data","0134f75f":"###### no null values in the dataset\n###### that's really good","13a7b055":"as we dont know exactly what are these features dropping columns is not a good idea but to check how the model performs in this data we are dropping these columns.","d10d7234":"###### Clearly the data is highly imbalaced","3afb1cd8":"# If you like my work give it a thumps UP.","5d90a1ed":"###### Collecting all the rows of fraud class.","011a2f37":"###### Concatenating the two dataframes df_genuine and df_fraud_new to create a training dataset for further use. ","f78ab068":"Data imbalance can be treated with resampling the data.\ndata resampling can be of two types\n\n1.under sampling\n\nunder sampling-Undersampling aims to balance class distribution by randomly eliminating majority class examples.  This is done until the majority and minority class instances are balanced out.\n\n2.over sampling\n\nOver-Sampling increases the number of instances in the minority class by randomly replicating them in order to present a higher representation of the minority class in the sample.\n\nfor more information about resampling\nrefer analytics vidhya's blog https:\/\/www.analyticsvidhya.com\/blog\/2017\/03\/imbalanced-classification-problem\/\n","832942ff":"##### cheching for null values in the dataset","442cdf4b":"###### The result is almost the same in both the cases \n###### Let's do something about the data imbalance","dac290bf":"###### collecting all the rows of genuine transaction.","95dcad26":"##### getting basic information about the dataset","5e0149d6":"###### So there are 284807 rows in the dataset and 31 columns .\n###### Let's check the count of different target variables\n"}}