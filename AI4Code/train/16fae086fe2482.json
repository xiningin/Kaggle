{"cell_type":{"e92bda99":"code","7b899c06":"code","c2a70f36":"code","f5b4fc30":"code","c50da2bd":"code","81cb05a4":"code","6f81835d":"code","9e32962c":"code","cb61e2dc":"code","7a779c38":"code","22d78b9c":"code","39298a34":"code","1b4fda1a":"code","bb11e839":"code","03e03e88":"code","2bba8f33":"code","f293aa57":"code","acecc52b":"code","caf33608":"code","6f4091a4":"code","2621a879":"code","1670deb3":"code","4941ec61":"code","c81971f3":"markdown","e9346f8f":"markdown","99936209":"markdown","2af1dff7":"markdown","fa4749c2":"markdown","652f3c58":"markdown","68225e41":"markdown","2df4ddb4":"markdown","44af3e58":"markdown","5af2204c":"markdown","e12d499e":"markdown","4606da11":"markdown","5728d946":"markdown","4526eedc":"markdown","7e56c608":"markdown","6ecdb75d":"markdown"},"source":{"e92bda99":"# Vamos usar os dados das not\u00edcias da Folha de S\u00e3o Paulo\nimport pandas as pd\nnoticias = pd.read_csv('..\/input\/news-of-the-site-folhauol\/articles.csv')\n\n# O dataset completo cont\u00e9m not\u00edcias de Jan\/15 a Set\/17. Vamos filtrar para usar apenas o m\u00eas de junho\/17\nnoticias = noticias[(noticias['date'] >= '2017-06-01') & (noticias['date'] <= '2017-06-30')]\nnoticias.shape","7b899c06":"# CONFIGURACOES DO PANDAS\n\n# Para mostrar todos os caracteres\npd.set_option('display.max_colwidth', -1)\nnoticias.head()\n\n# Para voltar a truncar os caracteres a 50 caracteres\npd.set_option('display.max_colwidth', 50)\nnoticias.head()","c2a70f36":"# Pr\u00e9-processamento. Vamos concatenar a coluna titulo e a texto\nimport re\nnotic = (noticias['title'].astype(str)) # 8 mil palavras + \" \" + noticias['text'].astype(str)) # 65 mil palavras\nnotic = [re.sub(\"(\\\\d|\\\\W)+\", \" \", e) for e in notic]\n\n# Define as stopwords\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstopwords = stopwords.words('portuguese')\n\n# Bag of Words\nfrom sklearn.feature_extraction.text import CountVectorizer\nvocab = CountVectorizer(stop_words=stopwords, lowercase=True)\nvetor=vocab.fit_transform(notic) \nprint(len(vocab.vocabulary_))\ntopicos = vocab.get_feature_names()\nbow = pd.DataFrame(vetor.toarray(),columns=topicos)\nbow.head()","f5b4fc30":"# Bag of Words - Com Alta Dimensionalidade\nbow.shape\n\n# Dataset de Noticias - N\u00e3o tem Alta Dimensionalidade\nnoticias.shape\n\n# Imprime o formato da matriz\nimport numpy as np\nnp.shape(vetor)","c50da2bd":"# Checa a esparsidade da matriz. Percentual de celulas que cont\u00e9m zeros\nprint(\"Esparsidade: \", ((vetor.todense() == 0).sum()\/vetor.todense().size)*100, \"%\")","81cb05a4":"# Importa as bibliotecas\nfrom numpy import array\nfrom scipy.sparse import csr_matrix\n\n# Cria uma matriz de exemplo\nA = array([[1, 0, 0, 1, 0, 0], [0, 0, 2, 0, 0, 1], [0, 0, 0, 2, 0, 0]])\nprint(A)\n\n# Calcula a esparsidade\nprint(\"Esparsidade: \", ((A == 0).sum()\/A.size)*100, \"%\")","6f81835d":"# convert to sparse matrix (CSR method)\n# formato: (linha,coluna) valor\nS = csr_matrix(A)\nprint(S)","9e32962c":"# Cria o LDA\nfrom sklearn.decomposition import LatentDirichletAllocation\nlda = LatentDirichletAllocation(n_components=3, max_iter=10).fit(vetor)","cb61e2dc":"# Avalia\u00e7\u00e3o do Score. Quanto maior, melhor\nprint(lda.score(vetor))\n\n# Avalia\u00e7\u00e3o da perplexidade. Quanto menor, melhor\nprint(lda.perplexity(vetor))","7a779c38":"# Mostra o Resultado\ndef resultado(modelo, feature_names, qtd_topicos):\n    for topic_idx, topic in enumerate(modelo.components_):\n        print(\"Topic %d:\" % (topic_idx))\n        print(\" \".join([feature_names[i]\n                        for i in topic.argsort()[:-qtd_topicos - 1:-1]]))\n\nresultado(lda, topicos, 10)","22d78b9c":"# Visualiza\u00e7\u00e3o Gr\u00e1fica\nfrom pyLDAvis import sklearn as sklearn_lda\nimport pickle \nimport pyLDAvis\npyLDAvis.enable_notebook()\nvis = sklearn_lda.prepare(lda, vetor, vocab)\nvis","39298a34":"! pip install pyicu \n! pip install pycld2\n! pip install morfessor\n! polyglot download embeddings2.en\n! polyglot download embeddings2.pt\n! polyglot download sentiment2.sk\n! polyglot download sentiment2.en # Faz o download do lexicon em English\n! polyglot download sentiment2.pt # Faz o download do lexicon em Portugues\nimport polyglot\nfrom polyglot.text import Text, Word","1b4fda1a":"# Polaridade das Palavras: +1 positivo, 0 neutro, -1 negativo\ntext = Text(\"O filme foi bom, mas a legenda estava ruim\")\n\nprint(\"{:<16}{}\".format(\"Word\", \"Polarity\")+\"\\n\"+\"-\"*30)\nfor w in text.words:\n    print(\"{:<16}{:>2}\".format(w, w.polarity))","bb11e839":"# Baixa pacotes.\n! polyglot download sentiment2.la\n! polyglot download sentiment2.gl\n! polyglot download sentiment2.lv\n! polyglot download sentiment2.it\n! polyglot download sentiment2.ca\n! polyglot download sentiment2.es\n! polyglot download sentiment2.war\n! polyglot download sentiment2.ht\n! polyglot download sentiment2.lt\n! polyglot download sentiment2.tk\n! polyglot download sentiment2.ga\n! polyglot download sentiment2.fi","03e03e88":"# Apenas para a primeira not\u00edcia da folha de sao paulo\nText(noticias.iloc[0,0]).polarity\n    \n# Cria um loop para calcular a polaridade das 100 primeiras noticias\nnoticias['polaridade'] = 0\n\n#for w in range(len(noticias)):    # Para todas as noticias        \nfor w in range(100):               # Apenas para as 100 primeiras noticias\n    try:\n        noticias.iloc[w,6] = Text(noticias.iloc[w,0]).polarity\n    except ZeroDivisionError:\n        noticias.iloc[w,6] = 0\n\n# Polaridade das Noticias em Geral\nnoticias.polaridade.describe()\n\n# Polaridade por Categoria\nnoticias.groupby('category')['polaridade'].describe()","2bba8f33":"# Importa os dados, mostra a propor\u00e7\u00e3o de sentimentos e imprime as primeiras linhas\nimport pandas as pd\ntrain=pd.read_csv('..\/input\/bag-of-words-meets-bags-of-popcorn-\/labeledTrainData.tsv',delimiter='\\t',quoting=3)\ntest=pd.read_csv('..\/input\/bag-of-words-meets-bags-of-popcorn-\/testData.tsv',delimiter='\\t',quoting=3)\n\nprint(train.sentiment.value_counts())\ntrain.head(4)","f293aa57":"# Analisa o primeiro coment\u00e1rio\ntrain[\"review\"][0]","acecc52b":"# Vamos diminuir os datasets para 1.000 linhas cada pra ser mais rapido\ntrain=train[0:1000]\ntest=test[0:1000]","caf33608":"# Pre-processamento\nimport re\nimport nltk\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nstopwords=set(stopwords.words('english')+list(punctuation))\n\ndef preprocessamento(review):\n    # Remove as tags HTML\n    review = BeautifulSoup(review,'html.parser').get_text()\n    # Remove o for diferente de letras\n    review = re.sub(\"[^a-zA-Z]\", \" \", review)\n    # Transforma para letra minuscula\n    review = review.lower()\n    # Faz a tokenizacao, separacao em palavras\n    token = nltk.word_tokenize(review)\n    # Remove Stopwords\n    token = [w for w in token if w not in stopwords]\n    # Stemming\n    review = [nltk.stem.SnowballStemmer('english').stem(w) for w in token]\n    # Junta as palavras de uma opini\u00e3o em uma string separadas por espa\u00e7oe\n    return \" \".join(review)\n\ntreino = train.review.apply(preprocessamento)\nteste = test.review.apply(preprocessamento)","6f4091a4":"# TF-IDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import words\nvetor = TfidfVectorizer(max_features=40000)\nvetortreino = vetor.fit_transform(treino)\nvetorteste = vetor.transform(teste) \nvocab = vetor.get_feature_names()\nprint(vocab[:50])","2621a879":"# Modelo SVM\nfrom sklearn import metrics\nfrom sklearn.svm import LinearSVC\nsvm = LinearSVC(random_state=1)\nsvm.fit(vetortreino, train.sentiment)\nprevisaosvm = svm.predict(vetorteste)\nmetrics.accuracy_score(train.sentiment, previsaosvm)","1670deb3":"# Modelo Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nmodelorf=RandomForestClassifier(random_state=1, n_estimators=200, max_depth=11)\nmodelorf.fit(vetortreino, train.sentiment)\nprevisaorf=modelorf.predict(vetorteste)\nmetrics.accuracy_score(train.sentiment, previsaorf)","4941ec61":"# Random Forest com GridSearch\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'n_estimators':[10,100,200],'criterion':['entropy','gini'],\n               'max_depth':[5,7,9,11]}\ngrid_search = GridSearchCV(estimator = modelorf,\n                            param_grid = parameters,\n                            scoring = 'accuracy',\n                            cv = 10,\n                            n_jobs = -1)\ngrid_search = grid_search.fit(vetortreino, train.sentiment)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint('Melhor Resultado :',best_accuracy)\nprint('Melhores Par\u00e2metros:\\n',best_parameters)","c81971f3":"## Matrizes Esparsas\nMatrizes que cont\u00e9m a maior parte de valores iguais a zero s\u00e3o chamadas de **matrizes esparsas**. Diferentes de **matrizes densas** em que a maioria dos valores s\u00e3o diferentes de zero. Matrizes esparsas s\u00e3o comuns na minera\u00e7\u00e3o de textos. Por\u00e9m, \u00e9 um problema porque demanda alto poder computacional ou alto custo de infraestrutura para processamento destas matrizes. Saber lidar com matrizes esparsas pode salvar muito tempo e recursos computacionais de infraestrutura. Quanto maior a matriz, maior a complexidade. Uma matriz \u00e9 considerada como esparsa ou densa atrav\u00e9s do c\u00e1lculo da propor\u00e7\u00e3o de elementos iguais a 0 divididos pela quantidade total de elementos. O ideal \u00e9 que se tenha a menor esparsidade poss\u00edvel para economizar recursos. Uma matriz \u00e9 esparsa, quando sua esparsidade \u00e9 maior que 50%.\n\n`esparsidade = qtd elementos iguais a 0 \/ qtd total de elementos`","e9346f8f":"## Alta Dimensionalidade\n**Dimensionalidade** se refere \u00e0 quantidade de vari\u00e1veis ou atributos (colunas) de um dataset. A alta dimensionalidade acontece quando a quantidade de dimens\u00f5es (d) \u00e9 maior do que a quantidade de observa\u00e7\u00f5es (n). Podemos chegar o tamanho com o `shape`. Em casos em que a quantidade de dimens\u00f5es \u00e9 expressivamente maior, o processamento destes dados se torna demorado e requer mais recursos de infraestrutura.","99936209":"# Usando Machine Learning para An\u00e1lise de Sentimentos\nUsamos classifica\u00e7\u00e3o para fazer a an\u00e1lise de sentimentos de uma forma autom\u00e1tica. \u00c9 mais f\u00e1cil de escalar, mais assertiva, por\u00e9m requer dados de treino.\n![](https:\/\/content-static.upwork.com\/blog\/uploads\/sites\/3\/2017\/06\/27091802\/image-8.png)\n\nTeste na Web: https:\/\/text-processing.com\/demo\/sentiment\/\n\n## Vamos fazer an\u00e1lise de sentimentos de opini\u00f5es sobre filmes","2af1dff7":"## Como resolver o problema da matriz esparsa\nPara lidar com matrizes esparsas, usamos uma estrutura de dados alternativa que usa t\u00e9cnicas de \u00e1lgebra linear. Nesta estrutura, os valores 0 s\u00e3o ignorados e apenas os valores diferentes de zero s\u00e3o armazenados. Assim, fazemos a **compress\u00e3o de linhas esparsas** ou Compressed Sparse Row - **CSR** atrav\u00e9s da fun\u00e7\u00e3o `csr_matriz()` do SciPy.","fa4749c2":"# 1. Problemas Comuns no Uso de Textos e Como Resolver\n* Alta dimensionaldade\n* Matrizes Esparsas","652f3c58":"# 2. Extra\u00e7\u00e3o de T\u00f3picos\nA extra\u00e7\u00e3o de t\u00f3picos \u00e9 \u00fatil para identificar qual o assunto do texto. O m\u00e9todo **LDA - Latent Dirichlet Allocation** \u00e9 um dos mais usados. O LDA \u00e9 um modelo de aprendizado **n\u00e3o-supervisionado** que recebe documentos como entrada e identifica os t\u00f3picos como sa\u00edda. O modelo tamb\u00e9m diz em que percentual o documento diz sobre cada t\u00f3pico. Utiliza a probabilidade de uma palavra aparecer no t\u00f3pico ou no documento.\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*RcyXi1lMESMq3X_RC7bL0Q.png)\n![](https:\/\/cdn-images-1.medium.com\/max\/1200\/1*GpPEknlKMQ09UkEtY6MZOw.png)\nComo par\u00e2metro podemos definir o n\u00famero de t\u00f3picos, a quantidade de palavras por t\u00f3pico e o n\u00famero de t\u00f3picos por documento. O LDA recebe como entrada a bag of words.","68225e41":"## Vantagens e Desvantagens do LDA\nO LDA \u00e9 geralmente r\u00e1pido, mas pode ser bem demorado se aplicado em grandes documentos, em muitos documentos ou em situa\u00e7\u00f5es com vastos vocabul\u00e1rios. \u00c9 f\u00e1cil interpretar a sa\u00edda porque possui uma abordagem bem intuitiva, por\u00e9m depende muito da interpreta\u00e7\u00e3o humana. Como os t\u00f3picos s\u00e3o identificados pelo computador numa abordagem n\u00e3o-supervisionada, n\u00e3o podemos influenciar na sa\u00edda de quais t\u00f3picos s\u00e3o gerados. Neste caso, a sa\u00edda para melhorar o resultado \u00e9 caprichar no pr\u00e9-processamento do texto antes do uso do LDA. O pr\u00e9-processamento pode incluir uma ou mais t\u00e9cnicas como: a remo\u00e7\u00e3o de caracteres especiais e n\u00fameros, remo\u00e7\u00e3o de stopwords, remo\u00e7\u00e3o de palavras raras que acontecem em poucos documentos e aplica\u00e7\u00e3o de stemming.","2df4ddb4":"## Como resolver o problema da alta dimensionalidade\nTer muitas dimens\u00f5es pode causar overfitting. Para mitigar o problema da alta dimensionalidade, devemos diminuir a quantidade de dimens\u00f5es, selecionar menos features, selecionar as features mais importantes, chamamos isso de **feature selection**. S\u00e3o exemplos de t\u00e9cnicas sele\u00e7\u00e3o de vari\u00e1veis: \n1. Descartar vari\u00e1veis que cont\u00e9m em sua **maioria valores ausentes** (NaN)\n2. Descartar vari\u00e1veis que tem **baixa vari\u00e2ncia**. Por exemplo, todos os dados de UF s\u00e3o iguais a SP. \n3. Descartar uma das vari\u00e1veis que s\u00e3o altamente correlacionadas com outras. Se usar uma vari\u00e1vel ou outra teria o resultado parecido. Ent\u00e3o descartamos uma delas.\n4. Visualizar as **vari\u00e1veis mais importantes** e descartar as de menor import\u00e2ncia. Ex: Random Forest.\n5. Eliminar vari\u00e1veis que n\u00e3o agregam na **performance do modelo**. Para descobrir quais s\u00e3o, podemos usar o **Backward Feature Elimination** ou **Foward Feature Elimination**. No Backward, treinamos o modelo v\u00e1rias vezes come\u00e7ando com todas as vari\u00e1veis e, em cada itera\u00e7\u00e3o, remover uma das vari\u00e1veis para descobrir qual das vari\u00e1veis tem menor impacto no resultado do modelo. J\u00e1 no Foward, fazemos o contr\u00e1rio, come\u00e7ando com apenas uma vari\u00e1vel e vamos aumentando a sele\u00e7\u00e3o de vari\u00e1veis em cada itera\u00e7\u00e3o e avaliamos quais os impactos de cada configura\u00e7ao. Por fim, remover as vari\u00e1veis que n\u00e3o agregam tanto no modelo.\n\nOutra alternativa \u00e9 utilizar t\u00e9cnicas avan\u00e7adas de extra\u00e7\u00e3o de vari\u00e1veis. A **Feature Extraction** busca descobrir o menor conjunto de vari\u00e1veis, que combinadas, cont\u00e9m as informa\u00e7\u00f5es mais relevantes que s\u00e3o aquelas que mais explicam a variancia dos dados. Como T\u00e9cnicas Avan\u00e7adas de feature extraction, temos, por exemplo: \n* PCA - Principal Component Analysis: \n* LDA - Linear Discriminant Analysis\n* t-SNE - t-Distributed Stochastic Neighbor Embedding\n* LDA - Linear Discriminant Analysis\n* SVD - Singular Value Decomposition","44af3e58":"## EXTRA!!! Grid Search + Cross Validation","5af2204c":"--- \n## Material Complementar\n1. [Livro online de NLTK em Python](http:\/\/www.nltk.org\/book\/)\n\n---\n### Pr\u00f3xima Aula\n[10. Raspagem de Sites](https:\/\/www.kaggle.com\/debkings\/10-raspagem-de-sites)","e12d499e":"## Tipos\n* Lexicon \n* Machine Learning","4606da11":"**[Voltar para a P\u00e1gina Inicial do Curso](https:\/\/www.kaggle.com\/c\/ml-em-python)**\n\n# **Minera\u00e7\u00e3o de Textos 2**\n1. Problemas comuns no uso de textos (alta dimensionalidade, matrizes esparsas, etc) e como resolv\u00ea-los.\n2. Extra\u00e7\u00e3o de T\u00f3picos \n3. An\u00e1lise de sentimentos\n\n![](https:\/\/www.kdnuggets.com\/images\/cartoon-machine-learning-class.jpg)\n## Caso de Uso: Not\u00edcias da Folha de S\u00e3o Paulo\n","5728d946":"Tamb\u00e9m existe o **VADER** do NLTK no Python. O Vader \u00e9 muito usado para pequenos textos, como tweets, avalia\u00e7\u00f5es e coment\u00e1rios em redes sociais. Mede a polaridade de palavras e tamb\u00e9m considera emoticons.","4526eedc":"### Agora se voc\u00ea quiser gastar um pouquinho ...\n* Ferramenta Google: https:\/\/cloud.google.com\/natural-language\/\n* Ferramenta Microsoft: https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/text-analytics\/text-analytics-user-scenarios\n* MeaningCloud: https:\/\/www.meaningcloud.com\/developer\/topics-extraction\n","7e56c608":"# 3. An\u00e1lise de Sentimentos\nComo as pessoas se sentem sobre determinado produto, not\u00edcia ou filme? Tamb\u00e9m chamado de minera\u00e7\u00e3o de opini\u00e3o, a **An\u00e1lise de Sentimentos** busca descobrir a opini\u00e3o das pessoas, utilizando o texto de suas avalia\u00e7\u00f5es, coment\u00e1rios ou postagens em redes sociais. \n\n![](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*FRd4BsrZ2VxKLbvVYJQC6w.png)\n\nPodemos tamb\u00e9m aprofundar o n\u00edvel da an\u00e1lise e ao inv\u00e9s de analisar a **polaridade** do sentimento (positivo, negativo), podemos analisar as **emo\u00e7\u00f5es**.\n![](https:\/\/img.elo7.com.br\/product\/main\/19C5C37\/cartoes-de-emocoes-decoracao-infantilx-decoracao-para-bebex-decoracao-para-quarto-de-crianx-decoraca.jpg)","6ecdb75d":"# Lexicon\nConsiste em um banco de dados de palavras j\u00e1 classificadas como positivas, neutras ou negativas. Conta a quantidade de vezes em que as palavras positivas e negativas aparecem. O problema \u00e9 que n\u00e3o tem capacidade de entender o contexto, sarcarsmo, varia\u00e7\u00f5es das palavras conhecidas. Ou seja, pode errar muito. Usaremos o Polyglot porque consegue entender nosso idioma **Portugu\u00eas**, dentre v\u00e1rios outros idiomas.\n\nPolyglot: https:\/\/polyglot.readthedocs.io\/en\/latest\/Sentiment.html"}}