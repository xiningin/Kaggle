{"cell_type":{"591d01fa":"code","f2a95841":"code","d65e2908":"code","e49221ba":"code","20307f03":"code","6f779efc":"code","cefad027":"code","713d22a5":"code","f63285ff":"code","03857681":"code","84c35ced":"code","4ec6a56a":"code","93fdb3c7":"code","01353fc8":"code","03719ddb":"code","517eeab1":"code","2451d15f":"code","8040e8ac":"code","60f6081b":"code","b5ff88b3":"code","b5055051":"code","5e159f49":"code","a8c56121":"markdown","a8c6b302":"markdown","24825c98":"markdown","9b015218":"markdown","cfa46c57":"markdown","c6a51a61":"markdown","7426cbfb":"markdown","ca01bdda":"markdown","e53b7b04":"markdown","b3f193e7":"markdown","af5058a7":"markdown","48b2bf4f":"markdown","c60ef3a9":"markdown","a05eee99":"markdown","94433b55":"markdown","e7267f00":"markdown"},"source":{"591d01fa":"import numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics # for score \/ accuracy","f2a95841":"#display input file path\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d65e2908":"#Load dataset\ndataset = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\", delimiter=\",\")\n\n# print dataset\ndataset[:]","e49221ba":"#print all columns (features and lable)\ndataset.columns\n# len(dataset.columns)","20307f03":"#Select Features \nX = dataset[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst','perimeter_worst', 'area_worst', 'smoothness_worst','compactness_worst','concavity_worst', 'concave points_worst','symmetry_worst', 'fractal_dimension_worst']].values\nX [0:2]","6f779efc":"#display Unique Lable along with its count\ndataset['diagnosis'].value_counts()","cefad027":"y = dataset['diagnosis'] \ny","713d22a5":"# Split dataset into two part train and test dataset 70% training and 30% testset (Random)\nfrom sklearn.model_selection import train_test_split\nX_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.3, random_state=3)","f63285ff":"def MyDecisionTreeClassifier(heuristic, tree_depth = None):\n    decision_tree_clfr = DecisionTreeClassifier(criterion = heuristic, max_depth = tree_depth)\n    \n    #Apply classifier on training dataset\n    decision_tree_clfr.fit(X_trainset, y_trainset)\n    \n    return decision_tree_clfr","03857681":"heuristic = \"entropy\"\ndecision_tree = MyDecisionTreeClassifier(heuristic)\n\n# predict lables using remaining testset\npredTree = decision_tree.predict(X_testset)","84c35ced":"print(\"Decision Trees's Accuracy using entropy: \", metrics.accuracy_score(y_testset, predTree))\nprint(\"Depth of Decision Tree: \", decision_tree.tree_.max_depth)","4ec6a56a":"!conda install -c conda-forge pydotplus -y\n!conda install -c conda-forge python-graphviz -y","93fdb3c7":"import matplotlib.pyplot as plt\nfrom sklearn.externals.six import StringIO\nimport pydotplus\nimport matplotlib.image as mpimg\nfrom sklearn import tree\nimport numpy as np","01353fc8":"fileName_entropy = \"decision-tree-entropy.png\"\n\ndot_data = StringIO()\nfeatureNames = dataset.columns[2:32]\nlabedNames = dataset[\"diagnosis\"].unique().tolist()\n    \n# export_graphviz will convert decision tree classifier into dot file\ntree.export_graphviz(decision_tree,feature_names = featureNames, out_file = dot_data,\n                         class_names = np.unique(y_trainset), filled = True,  special_characters = True,rotate = False) \n    \n# Convert dot file int pgn using pydotplus\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n    \n#write pgn into file\ngraph.write_png(fileName_entropy)\n\n#display tree image\nimg_entropy = mpimg.imread(fileName_entropy)\nplt.figure(figsize=(100, 200))\nplt.imshow(img_entropy, interpolation='nearest')","03719ddb":"heuristic_g = \"gini\"\ndecision_tree_g = DecisionTreeClassifier(criterion = heuristic_g)\n\n#Apply classifier on train dataset\ndecision_tree_g.fit(X_trainset,y_trainset)\n\n# predict lables using remaining testset\npredTree_g = decision_tree_g.predict(X_testset)","517eeab1":"print(\"Decision Trees's Accuracy using Gini: \", metrics.accuracy_score(y_testset, predTree_g))\nprint(\"Depth of Decision Tree: \", decision_tree_g.tree_.max_depth)","2451d15f":"fileName_g = \"decision-tree-gini.png\"\n\n\ndot_data = StringIO()\nfeatureNames = dataset.columns[2:32]\nlabedNames = dataset[\"diagnosis\"].unique().tolist()\n    \n# export_graphviz will convert decision tree classifier into dot file\ntree.export_graphviz(decision_tree_g,feature_names = featureNames, out_file = dot_data,\n                         class_names = np.unique(y_trainset), filled = True,  special_characters = True,rotate = False) \n    \n# Convert dot file int pgn using pydotplus\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n    \n#write pgn into file\ngraph.write_png(fileName_g)\n\n#display tree image\nimg_g = mpimg.imread(fileName_g)\nplt.figure(figsize=(100, 200))\nplt.imshow(img_g, interpolation='nearest')","8040e8ac":"startingPoint = 2\naccuracy_1 = np.zeros((decision_tree.tree_.max_depth - 1))\n\nfor x in range(startingPoint, decision_tree.tree_.max_depth + 1):\n    heuristic = \"entropy\"\n    decision_tree = MyDecisionTreeClassifier(heuristic)\n\n    # predict lables using remaining testset\n    predTree = decision_tree.predict(X_testset)\n    \n    accuracy_1 [x-startingPoint] = metrics.accuracy_score(y_testset, predTree)\n    \n    print(\"Decision Trees's Accuracy (entropy) with depth:\", x , \" is \", accuracy_1 [x-startingPoint],\"\")","60f6081b":"import matplotlib.pyplot as plt\ndef ShowAccuracy(_range, data):\n    plt.plot(range(2,_range+1),data,'g')\n    plt.legend(('Accuracy'))\n    plt.ylabel('Accuracy ')\n    plt.xlabel('Depth')\n    plt.tight_layout()\n    plt.show()","b5ff88b3":"ShowAccuracy(decision_tree.tree_.max_depth, accuracy_1)","b5055051":"startingPoint = 2\ndepth = np.zeros((decision_tree_g.tree_.max_depth - 1))\naccuracy_2 = np.zeros((decision_tree_g.tree_.max_depth - 1))\n\nfor x in range(startingPoint, decision_tree_g.tree_.max_depth + 1):\n    heuristic = \"gini\"\n    decision_tree = MyDecisionTreeClassifier(heuristic)\n\n    # predict lables using remaining testset\n    predTree = decision_tree.predict(X_testset)\n    \n    depth [x-startingPoint] = x;\n    accuracy_2 [x-startingPoint] = metrics.accuracy_score(y_testset, predTree)\n    \n    print(\"Decision Trees's Accuracy (Gini) with depth:\",x, \" is \", accuracy_2 [x-startingPoint],\"\")","5e159f49":"ShowAccuracy(decision_tree_g.tree_.max_depth, accuracy_2)","a8c56121":"## Visualization","a8c6b302":"<h1><center>Decision Trees for Binary Classification on Breast Cancer Wisconsin (Diagnostic) Data Set<\/center><\/h1>","24825c98":"## Conclusion","9b015218":"## Pruning to Avoid Overfitting\n\nHow we can avoid Overfitting ?\nOverfitting can be avoided by using these parameters \n* max_leaf_nodes\n* min_samples_leaf\n* max_depth\n\nDescription\n1. max_leaf_nodes: This parameter can be used to define the max number of leaf nodes\n1. min_samples_leaf: This parameter can be userd to restrict the size of sample leaf\n1. max_depth: It can be used to reduce the depth of the tree to build a generalized tree","cfa46c57":"## Load Dataset","c6a51a61":"## Import usefull libraries\n\n* numpy (as np)\n* pandas\n* DecisionTreeClassifier from sklearn.tree","7426cbfb":"## Evaluation","ca01bdda":"## Apply Second Heuristic (Gini)","e53b7b04":"## Visualization\nInstall below package for visualization\n1. Install pydotplus \n1. Install python-graphviz","b3f193e7":"## DataSet \nBreast Cancer Wisconsin (Diagnostic) Data Set (https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data)\n\n## Overview\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n\nAttribute Information:\n\n1) ID number 2) Diagnosis (M = malignant, B = benign) 3-32)\n\nTen real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter) b) texture (standard deviation of gray-scale values) c) perimeter d) area e) smoothness (local variation in radius lengths) f) compactness (perimeter^2 \/ area - 1.0) g) concavity (severity of concave portions of the contour) h) concave points (number of concave portions of the contour) i) symmetry j) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\n## Reference\n1. https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data\n1. https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29\n1. https:\/\/www.coursera.org\/learn\/machine-learning-with-python\/home\/welcome\n1. https:\/\/scikit-learn.org\/stable\/modules\/tree.html\n1. https:\/\/statinfer.com\/204-3-10-pruning-a-decision-tree-in-python\/\n1. https:\/\/www.datacamp.com\/community\/tutorials\/decision-tree-classification-python","af5058a7":"## Select Features","48b2bf4f":"## Task\n* Apply Decision Tree on the selected dataset\n* Apply two different heuristics for split (Entropy, Gini)\n* Apply pruning as well","c60ef3a9":"## Evaluation","a05eee99":"## Apply First Heuristic (Entropy)","94433b55":"### Pruning Results\n\n* in case of first heuristic the best accuracy was 0.96 when tree depth was 4\n* In case of second heuristic the best accuracy was 0.95 when tre depth was 7","e7267f00":"## Lables\n\n1. **M** = malignant\n1. **B** = benign"}}