{"cell_type":{"df0a3fd9":"code","1e89abcb":"code","d5a2d22b":"code","9e6608ff":"code","74e8a6c8":"code","f819de59":"code","5222b4d1":"code","c7c06172":"code","73c4f583":"code","b212a73d":"markdown","abfcf078":"markdown","5c7cd881":"markdown","475960c3":"markdown","b9325b57":"markdown","28386fcd":"markdown","13709503":"markdown","5409c543":"markdown","15d3b2b2":"markdown","569abd08":"markdown"},"source":{"df0a3fd9":"from sklearn.datasets import make_blobs\n\nX1, Y1 = make_blobs(n_features = 4,\n                    n_samples = 100,\n                    centers = 4,\n                    random_state = 4,\n                    cluster_std = 2)","1e89abcb":"print(X1[:5])\nprint(Y1[:5])","d5a2d22b":"from sklearn import decomposition\nimport pandas as pd\n\n# class declaration for PCA\npca = decomposition.PCA(n_components = 4, # number of PC\n                        random_state = 4) # SEED\n\n# transformation X1 data\n# fit_transform > data will be transformated based on X1\npc = pca.fit_transform(X1)\n\npc_df = pd.DataFrame(data = pc,\n                     columns = ['PC1', 'PC2', 'PC3', 'PC4'])\n\n# check result\npc_df['Cluster'] = Y1\npc_df.head()","9e6608ff":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ndef scree_plot(pca):\n    # explained_variance_ratio_: percentage of variance by PC factor\n    num_components = len(pca.explained_variance_ratio_)\n    ind = np.arange(num_components)\n    vals = pca.explained_variance_ratio_\n    \n    ax = plt.subplot()\n    # np.cumsum: calculate the cumulative sum of elements\n    cumvals = np.cumsum(vals)\n    # bar plot\n    ax.bar(ind, vals, color = ['#00da75', '#f1c40f',  '#ff6f15', '#3498db'])\n    # line plot\n    ax.plot(ind, cumvals, color = '#c0392b')\n    \n    for i in range(num_components):\n        # annotate at screeplot\n        ax.annotate(r\"%s\" % ((str(vals[i]*100)[:3])),\n                    (ind[i], vals[i]),\n                    va = \"bottom\",\n                    ha = \"center\",\n                    fontsize = 13)\n     \n    ax.set_xlabel(\"PC\")\n    ax.set_ylabel(\"Variance\")\n    plt.title('Scree plot')\n\nscree_plot(pca)","74e8a6c8":"x, y = make_blobs(n_samples = 100,\n                  centers = 3,\n                  n_features = 2,\n                  random_state = 42)\n\ndf = pd.DataFrame(dict(x = x[:, 0],\n                       y = x[:, 1],\n                       label = y))\n\npoints = df.drop('label',\n                 axis = 1)\npoints.head()","f819de59":"from sklearn.cluster import KMeans\n\n# class declaration for K-means\nkmeans = KMeans(n_clusters = 3, # number of centroid(= cluster)\n                random_state = 42) # SEED\n\n# assign data to each cluster\nkmeans.fit(x)\nlabels = kmeans.labels_\n\n# enter new cluster at dataframe\nnew_series = pd.Series(labels)\ndf['clusters'] = new_series.values\ndf.head(15)","5222b4d1":"def get_centroids(df, column_header):\n    new_centroids = df.groupby(column_header).mean()\n    return new_centroids\n\n# calculate centroid\ncentroids = get_centroids(df, 'clusters')\n\n# check result\ndf.groupby('clusters').mean()","c7c06172":"def plot_clusters(df, column_header, centroids):\n    colors = {0 : 'red', 1 : 'cyan', 2 : 'yellow'}\n    fig, ax = plt.subplots()\n\n    # centroid\n    ax.plot(centroids.iloc[0].x, centroids.iloc[0].y, \"ok\") \n    ax.plot(centroids.iloc[1].x, centroids.iloc[1].y, \"ok\")\n    ax.plot(centroids.iloc[2].x, centroids.iloc[2].y, \"ok\")\n\n    # all data\n    grouped = df.groupby(column_header)\n\n    for key, group in grouped:\n        group.plot(ax = ax,\n                 kind = 'scatter',\n                 x = 'x',\n                 y = 'y',\n                 label = key,\n                 color = colors[key])\n    plt.show()\n\nplot_clusters(df, 'clusters', centroids)","73c4f583":"# elbow methods\nsum_of_squared_distances = []\nK = range(1, 15)\n\nfor k in K:\n    km = KMeans(n_clusters = k)\n    km = km.fit(points)\n    # inertia_: the degree to which the data included in the group is spread\n    sum_of_squared_distances.append(km.inertia_)\n\nplt.plot(K, sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum of squared distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","b212a73d":"### **Check result**\n\n- X1: total sample data\n- Y1: allocated cluster per data","abfcf078":"## **1. Principal Component Analysis**","5c7cd881":"### **Declare & Apply PCA**","475960c3":"### **Assign data**","b9325b57":"### **Calculate centroid**","28386fcd":"### **Result visualization**","13709503":"### **Make sample data**\n<br>\n\n**make_blobs parameter(default)**\n- n_features(20): number of independent variables\n- n_samples(100): number of total sample data\n- centers(3): number of cluster\n- cluster_std(1.0): cluster standard deviation\n- center_box(-10.0, 10.0): bounding box of cluster\n- shuffle(True): whether to shuffle numbers","5409c543":"### **Make sample data**","15d3b2b2":"### **Scree plot(Visualization)**","569abd08":"## **2. K-Means Clustering**"}}