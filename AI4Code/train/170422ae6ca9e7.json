{"cell_type":{"b6ac3fe8":"code","6b2f3ceb":"code","411bf323":"code","09da7ff8":"code","30459842":"code","946a4500":"code","dc225ca7":"code","cda65061":"markdown","06c1c5fc":"markdown","5f3ab578":"markdown","f27a48fa":"markdown","7c44de1c":"markdown","66aa2e54":"markdown"},"source":{"b6ac3fe8":"import numpy as np\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Dense, LSTM, GRU, Dropout\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import metrics, losses, models\n\nfrom tensorflow import autograph\nautograph.set_verbosity(0)\nimport tensorflow_addons as tfa","6b2f3ceb":"X_train = pd.read_csv('..\/input\/lstm-gru-sentiment-analysis-data-prep\/X_train.csv').to_numpy()\ny_train = pd.read_csv('..\/input\/lstm-gru-sentiment-analysis-data-prep\/y_train.csv').to_numpy()\nX_test  = pd.read_csv('..\/input\/lstm-gru-sentiment-analysis-data-prep\/X_test.csv').to_numpy()\ny_test  = pd.read_csv('..\/input\/lstm-gru-sentiment-analysis-data-prep\/y_test.csv').to_numpy()\n\nprint(\"Shape of training examples : \"+str(X_train.shape)+\", and training labels : \"+str(y_train.shape))\n\nembedding_matrix  = pd.read_csv('..\/input\/lstm-gru-sentiment-analysis-data-prep\/embedding_matrix.csv').to_numpy()\nnum_words = np.shape(embedding_matrix)[0]\n\nprint(\"Number of words : \"+str(num_words))\nprint(\"Shape of the embedding matrix : \"+str(np.shape(embedding_matrix)))","411bf323":"INITIAL_DROPOUT = 0.1\nNUM_UNITS_LSTM  = 32\nLSTM_DROPOUT    = 0.2\nNUM_UNITS_GRU   = 32\nGRU_DROPOUT     = 0.2\n\nDENSE_ACT = 'sigmoid'\nLOSS_FCT  = 'mean_squared_error'\nOPTIMIZER = Adam(learning_rate=0.002)\nMETRICS   = [metrics.CategoricalAccuracy()]","09da7ff8":"print(\"Shape of the X_train matrix\" + str(X_train.shape) + \" -- (num_examples, length of examples)\")\nprint(\"Shape of the y_train vector\" + str(y_train.shape) + \" -- (num_examples,)\")\nprint(\"Number of words in the corpus\/embedding : \"+str(num_words))\n\nmodel = Sequential()\nmodel.add(Input(shape=(X_train.shape[1],)))\nmodel.add(Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),trainable=False))\n\n#model.add(Dropout(INITIAL_DROPOUT))\n\nmodel.add(LSTM(NUM_UNITS_LSTM, return_sequences=True, dropout=LSTM_DROPOUT))\n\nmodel.add(GRU(NUM_UNITS_GRU, dropout=GRU_DROPOUT))         \n    \nmodel.add(Dense(3, activation = DENSE_ACT))\n\nmodel.compile(loss=LOSS_FCT,optimizer=OPTIMIZER,metrics=METRICS)\n\nmodel.summary()","30459842":"BATCH_SIZE = 100\nNUM_EPOCHS = 10\n\nhistory = model.fit(X_train,y_train,batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,validation_data=(X_test,y_test))\n\nmodel.save(\"lstm-gru_1.h5\")","946a4500":"#from tensorflow.keras import models\n#NUM_EPOCHS_2 = 5\n#model1 = models.load_model('lstm-gru_1.h5')\n#history_2 = model1.fit(X_train,y_train,batch_size=BATCH_SIZE,epochs=NUM_EPOCHS_2,validation_data=(X_test,y_test))\n#model1.save(\"lstm-gru_1.h5\")","dc225ca7":"import matplotlib.pyplot as plt\n\nepochs = range(1, NUM_EPOCHS+1)\n\nplt.plot(epochs, history.history['categorical_accuracy'])\nplt.plot(epochs, history.history['val_categorical_accuracy'])\nplt.title('model categorical accuracy')\nplt.ylabel('categorical accuracy')\nplt.xlabel('epochs')\nplt.legend(['train', 'test'], loc='upper left')\nplt.xticks(epochs, epochs)\nplt.show()\n\nplt.plot(epochs, history.history['loss'])\nplt.plot(epochs, history.history['val_loss'])\nplt.title('model loss')\nplt.xlabel('epochs')\nplt.legend(['train', 'test'], loc='upper left')\nplt.xticks(epochs, epochs)\nplt.show()","cda65061":"# Training - Second pass","06c1c5fc":"# Datasets","5f3ab578":"# Metrics and analysis","f27a48fa":"# Training - First pass","7c44de1c":"# Libraries","66aa2e54":"# Model"}}