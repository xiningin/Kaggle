{"cell_type":{"4c2ab74d":"code","0b021497":"code","e91ddecf":"code","a157070a":"code","0663dfda":"code","45beba1c":"code","8ad7844f":"code","d7675b7b":"code","0660cd2b":"code","8257637e":"code","977f1060":"code","a996077a":"code","65c6e4e8":"code","41c7ee9b":"code","b289c89a":"code","fdaf7a78":"code","85d178a3":"code","84ab13ca":"code","e33208b3":"code","af88479c":"code","31337dd2":"code","a0ea9e6e":"code","422f46e0":"code","663bad86":"code","4f302d66":"code","a558eccf":"code","96b63968":"code","1fdd41e6":"markdown","d4e92f51":"markdown","4c567ce0":"markdown","7507952f":"markdown","a5cb5aed":"markdown","b966147b":"markdown","f9c4c3a3":"markdown","fff8125d":"markdown","5f4ac5ee":"markdown","f38d92bb":"markdown","35c44b62":"markdown","dfd6356c":"markdown","9a6589b5":"markdown","e2988b66":"markdown","ac86d68b":"markdown","a4bee355":"markdown","77cb5597":"markdown"},"source":{"4c2ab74d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.tools.plotting import parallel_coordinates\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom itertools import cycle, islice\n%matplotlib inline","0b021497":"## Reading Dataset","e91ddecf":"wine_df = pd.read_csv('..\/input\/Wine.csv', sep=',',names=[\"Cultivar\",\"Alcohol\",\"Malic\",\"Ash\",\"Alkalinity\",\"Magnesium\",\"Phenols\",\"Flavanoids\",\"Nonflav\",\"Proanthocyan\",\"Color\",\"Hue\",\"OD280\",\"Proline\"])\nprint(type(wine_df))","a157070a":"# Finds number of rows and columns in dataframe\nrows, cols = wine_df.shape\nprint(\"Dataframe has\", rows, \"records and\", cols, \"variables.\\n\")\nwine_df.head()","0663dfda":"attributes = wine_df.columns\nfeatures = attributes[1:]\nclassification = attributes[0]\nprint(\"The attributes in the dataframe are:\\n\", attributes)\n","45beba1c":"# Are there any records with NaN data?\nNaN_data_flag = wine_df.isnull().any()\nif NaN_data_flag.any():\n    print(\"Some records have NaN values. These will be removed...\\n\")\n    before_rows, before_cols = wine_df.shape\n    wine_df = wine_df.dropna()\n    after_rows, after_cols = wine_df.shape\n    print(\"Dropped\", after_rows - before_rows, \"records. Cleaned dataframe has\", after_rows, \"records.\\n\")\nelse:\n    print(\"There are no records with NaN values. Dataframe is already clean.\\n\")\n    ","8ad7844f":"wine_df.describe().round(3)","d7675b7b":"# Samples per Cultivar\nregion_counts = wine_df['Cultivar'].value_counts()\nexplode = (0, 0.1, 0)\nregion_counts.plot(kind='pie',autopct='%.0f%%', shadow=True, figsize=(4,4), radius=1.0)\n","0660cd2b":"hist_quality = wine_df['Alcohol']\nplt.hist(hist_quality, 10, normed=False, facecolor='teal')\nplt.xlabel('Alcohol')\nplt.ylabel('Count')\nplt.title('Alcohol Content Distribution')\nplt.grid(True)\nplt.show()","8257637e":"hist_fixed = wine_df['Phenols']\nplt.hist(hist_fixed, 10, normed=False, facecolor='blue')\nplt.xlabel('Phenols')\nplt.ylabel('Count')\nplt.title('Phenols Distribution')\nplt.grid(True)\nplt.show()\n","977f1060":"hist_volatile = wine_df['Color']\nplt.hist(hist_volatile, 10, normed=False, facecolor='red')\nplt.xlabel('Color')\nplt.ylabel('Count')\nplt.title('Color Distribution')\nplt.grid(True)\nplt.show()","a996077a":"hist_citric = wine_df['Nonflav']\nplt.hist(hist_citric, 10, normed=False, facecolor='lime')\nplt.xlabel('Nonflavanoids')\nplt.ylabel('Count')\nplt.title('Nonflavanoids Distribution')\nplt.grid(True)\nplt.show()","65c6e4e8":"classification_data = wine_df.copy()\nlabel_mapping = {1:'Cultivar1', 2:'Cultivar2', 3:'Cultivar3'}\nclassification_data['Cultivar_Label'] = classification_data['Cultivar'].map(label_mapping)\nclassification_data.head()","41c7ee9b":"# Target is stored in Y\nY = classification_data[['Cultivar_Label']].copy()\n\n# Training features are stored in X\nfeatures = ['Alcohol', 'Malic', 'Ash', 'Alkalinity', 'Magnesium', 'Phenols', \n            'Flavanoids', 'Nonflav', 'Proanthocyan', 'Color', 'Hue', 'OD280', 'Proline']\nX = classification_data[features].copy()\n","b289c89a":"# Split data into test and training\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.4, random_state = 42)\n","fdaf7a78":"# Train decision tree\nquality_classifier = DecisionTreeClassifier(max_leaf_nodes = 15, random_state = 42)\nquality_classifier.fit(X_train, Y_train)\n","85d178a3":"# Testing\npredictions = quality_classifier.predict(X_test)\npredictions[:10]","84ab13ca":"# Percentage of target data by label\npercentages = Y_test['Cultivar_Label'].value_counts(normalize=True)\npercentages","e33208b3":"# Measure accuracy\naccuracy_score(y_true = Y_test, y_pred = predictions)","af88479c":"features = ['Alcohol', 'Malic', 'Ash', 'Alkalinity', 'Magnesium', 'Phenols', \n            'Flavanoids', 'Nonflav', 'Proanthocyan', 'Color', 'Hue', 'OD280', 'Proline']\nX_cluster = StandardScaler().fit_transform(wine_df[features])","31337dd2":"# Finds best number of clusters using Inertia_ metric\nSSE_data =[]\nfor n in range(3, 20):\n    # Perform the clustering\n    kmeans = KMeans(n_clusters = n)\n    model = kmeans.fit(X_cluster)\n    SSE_data.append(model.inertia_)\n    \n# Plot the SSE values to find the elbow that gives the best number of clusters\nSSE_series = pd.Series(SSE_data)\nx = np.arange(3., 20., 1.0)\nplt.scatter(x, SSE_series, c=\"b\", marker='o', label=\"SSE vs. n_clusters\")\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"SSE\")\nplt.legend(loc=1)\nplt.ylim(ymin=1)\nplt.grid()\nplt.show()","a0ea9e6e":"# Clustering\nkmeans = KMeans(n_clusters = 10)\nmodel = kmeans.fit(X_cluster)\nprint(\"Model\\n\", model)\n\n# Dispay centroids\ncenters = model.cluster_centers_\ncenters.round(2)","422f46e0":"# Determines cluster for each sample\npredictedCluster = kmeans.predict(X_cluster)\npredictedCluster","663bad86":"#Plots\n# Function that creates a DataFrame with a column for Cluster Number\n\ndef pd_centers(featuresUsed, centers):\n\tcolNames = list(featuresUsed)\n\tcolNames.append('prediction')\n\n\t# Zip with a column called 'prediction' (index)\n\tZ = [np.append(A, index) for index, A in enumerate(centers)]\n\n\t# Convert to pandas data frame for plotting\n\tP = pd.DataFrame(Z, columns=colNames)\n\tP['prediction'] = P['prediction'].astype(int)\n\treturn P\n\n# Function that creates Parallel Plots\n\ndef parallel_plot(data):\n    my_colors = list(islice(cycle(['b', 'r', 'g', 'y', 'k', 'c', 'm', 'lime', 'salmon', 'grey']), None, len(data)))\n    fig = plt.figure(figsize=(15,8)).gca().axes.set_ylim([-2,+5])\n    parallel_coordinates(data, 'prediction', color = my_colors, marker='o')\n    plt.savefig('FeaturePlot.png')\n    \nP = pd_centers(features, centers)\nparallel_plot(P)\n","4f302d66":"# Cultivar Representation in Clusters\ncultivar1Clusters = predictedCluster[wine_df['Cultivar'] == 1]\ncultivar2Clusters = predictedCluster[wine_df['Cultivar'] == 2]\ncultivar3Clusters = predictedCluster[wine_df['Cultivar'] == 3]\n\n# Centroid counts in each cultivar\ntotClusters = 10\ncultivar1Counts = []\ncultivar2Counts = []\ncultivar3Counts = []\nfor i in range(totClusters):\n    cultivar1Counts.append(np.count_nonzero(cultivar1Clusters == i))\n    cultivar2Counts.append(np.count_nonzero(cultivar2Clusters == i))\n    cultivar3Counts.append(np.count_nonzero(cultivar3Clusters == i))\n    \n# Plot the distribution of cultivar samples per centroid\nind = np.arange(totClusters)    \nwidth = 0.45                    \ncult1Pluscult2 = [sum(x) for x in zip(cultivar1Counts, cultivar2Counts)]\n\np1 = plt.bar(ind, cultivar1Counts, width, color='b')\np2 = plt.bar(ind, cultivar2Counts, width, bottom = cultivar1Counts, color='r')\np3 = plt.bar(ind, cultivar3Counts, width, bottom = cult1Pluscult2, color='lime')\n\nplt.ylabel('Cultivar counts')\nplt.xlabel('Centroids')\nplt.title('Cultivar counts per centroid')\nplt.xticks(ind, ('C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10'))\nplt.yticks(np.arange(0, 51, 10))\nplt.legend((p1[0], p2[0], p3[0]), ('Cultivar1', 'Cultivar2', 'Cultivar3'))\nplt.show()\n","a558eccf":"wine_df['Cultivar1'] = wine_df['Cultivar'] == 1\nwine_df['Cultivar2'] = wine_df['Cultivar'] == 2\nwine_df['Cultivar3'] = wine_df['Cultivar'] == 3\ncorrVariables = ['Cultivar1', 'Cultivar2', 'Cultivar3', 'Alcohol', 'Magnesium',\n                'Phenols', 'Flavanoids', 'Nonflav', 'Ash', 'Alkalinity', 'Color', 'Hue', \n                 'Proanthocyan', 'OD280', 'Malic', 'Proline']\ncultivars_df = wine_df[corrVariables]\n\n\n# Correlation\ncorr = cultivars_df.corr().round(2)\ncorr = corr[['Cultivar1', 'Cultivar2', 'Cultivar3']]\ncultivarCorr = corr.drop(['Cultivar1', 'Cultivar2', 'Cultivar3'], axis = 0)\ncultivarCorr\n            ","96b63968":"# Heatmap of the correlation matrix above\nheatmapRows = ['Cultivar1', 'Cultivar2', 'Cultivar3']\nheatmapCols = ['Alcohol', 'Magnesium', 'Phenols', 'Flavanoids', 'Nonflav', 'Ash', \n               'Alkalinity', 'Color', 'Hue', 'Proanthocyan', 'OD280', 'Malic', 'Proline']\n\nfig, ax = plt.subplots(figsize=(10, 10))\nim = ax.imshow(cultivarCorr, interpolation='nearest')\n\n# We want to show all ticks...\nax.set_xticks(np.arange(len(heatmapRows)))\nax.set_yticks(np.arange(len(heatmapCols)))\n# ... and label them with the respective list entries\nax.set_xticklabels(heatmapRows)\nax.set_yticklabels(heatmapCols)\n\n# Rotate the tick labels and set their alignment.\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n         rotation_mode=\"anchor\")\n\n# Loop over data dimensions and create text annotations.\nfor i in range(len(heatmapCols)):\n    for j in range(len(heatmapRows)):\n        text = ax.text(j, i, cultivarCorr.iloc[i][j], ha=\"center\", va=\"center\", color=\"w\")\n\nax.set_title(\"Heatmap between Cultivars and Chemical Attributes\")\nfig.tight_layout()\nplt.show()\n\n","1fdd41e6":"## Research Questions\nBased on the dataset's attributes, the following two research questions will be investigated:\n\na) Is it possible to predict the *Cultivar* of a wine by analyzing its chemical features?  \nb) Which chemical features are more strongly related to a wine\u2019s *Cultivar*?\n","d4e92f51":"![](http:\/\/)# Italian Cultivars and their Wine Characteristics\n\n## Dataset\nThis project analyzes a wines dataset available to the public through the University of California Irvine Machine Learning repository. The dataset consists of 178 Italian wine samples, all from the same geographic region, but from 3 different cultivars. Each sample has 14 features: one is the cultivar's identifier, and the others are chemical attributes. The cultivar is a categorical variable that can take only 3 values: (Region) 1, (Region) 2, and (Region) 3. The chemical attributes are all numerical and continuous. This is a list of the 14 variables:\n\n* Cultivar\n* Alcohol\n* Malic acid\n* Ash\n* Alcalinity of ash  \n* Magnesium\n* Total phenols\n* Flavanoids\n* Nonflavanoid phenols\n* Proanthocyanins\n* Color intensity\n* Hue\n* OD280\/OD315 of diluted wines\n* Proline  \n","4c567ce0":"## Back to correlation\nNow that it's clear we have strong relationships between cultivars and chemical properties, we can run correlation identifying the cultivar.\n","7507952f":"## Classification\nClassification will be done with a decission tree algorithm. This supervised learning algorithm that will used to learn the *Cultivar* from where the wine comes from based on its chemical features. \nFor clarity, a copy of the original dataframe is made and a additional column is added: the *Cultivar_Label*. This is just a mapping of the *Cultivar* variable that takes values 1, 2, or 3 into a label that takes vales *Cultivar1*, *Cultivar2*, or *Cultivar3*. \nAll the chemical attributes will be used as features (X), while the *Cultivar_Label* will be used as the target (Y).","a5cb5aed":"## Clustering\nThe second algorithm is a kmeans method that groups the data into clusters based on sample similarity. Since we want to figure out whether samples from the same cultivar cluster together, no information on the cultivar is included in the training or testing data. Remember that clustering is an unsupervised learning task. The idea is to segment the samples based on similarity.\nPrior to clustering, the data is normalized. Several numbers of clusters are tried so as to identify a number that appears to be good enough. ","b966147b":"## Importing Libraries","f9c4c3a3":"## Selecting appropriate number of clusters\nFrom the plot above that shows SSE (Sum of Square Errors) as a function of total clusters, it seems it's a good idea to select between 10 and 12 clusters. The following analysis will use 10 clusters.\n","fff8125d":"## Plotting centroids with a parallel plot","5f4ac5ee":"## Variables\n","f38d92bb":"## Cluster interpretation\nThe above cluster composition indicate that some cultivars have specific chemical signatures. For instance, centroids C0 and C7 have signatures proper of *Cultivar1*. From the *feature vs centroid* parallel graph, those two centroids are characterized by high values in the alcohol, phenols, flavanoids and proline levels. Likewise, centroid C5 is mostly representative of *Cultivar1*, and C5 has high alcohol and propine levels. \n\n*Cultivar2* is heavily represented in clusters C1, C6, C8 and C9. Centroid C8 has very high Magnesium and Proanthocyanins levels, and is a very small cluster. Centroids C1, C6 and C9 are characterized by low alcohol, nonflavanoids, and color intensity levels. \n\nFinally, *Cultivar3* is heavily represented in clusters C3 and C4. These centroids have high levels of malic acid, and low levels of phenols, flavanoids, OD280, and hue.","35c44b62":"## Descriptive Statistics\nDescriptive statitics can give information not only on characteristics of the individual attributes but also shed light on how the data can be further cleaned. \n","dfd6356c":"## Variable Distributions\nIt's imporant to know if the dataset contains sufficient samples of each cultivar. If one cultivar has few samples, then it'd be difficult to use decision trees to do an effective classification.","9a6589b5":"## Data Cleaning\n","e2988b66":"## Exploring Dataset\n","ac86d68b":"## Classification Analysis\nThe test results of the decision tree classifier indicate an accuracy of approximately 94%. This is a very good result, particularly taking into account that no feature engineering was implemented.   ","a4bee355":"## Which chemical characteristics relate to specific cultivars?\nThis question can be answered by figuring out which centroids are more representative of each cultivar.","77cb5597":"## Research Methods\nTo answer the first question, we'll analyze the dataset's attributes and create a classifier that maps chemical features to cultivars. The classifier model will be trained with about 60% of the available data, and then tested for performance with the balance 40%. The classifier will be a decision tree. \n\nTo answer the second question, we'll cluster the data into a suitable number of clusters using the kmeans nearest neighbors algorithm. Then, we'll determine which clusters are more representative of specific cultivars. From here, we'll figure out which chemical combinations can be considered as signatures of a given cultivar."}}