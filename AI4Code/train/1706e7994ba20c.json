{"cell_type":{"5e44a9b9":"code","aadec855":"code","a79e26ed":"code","bb7ad31e":"code","da88877c":"code","1d95026c":"code","7157ae5e":"code","7a57b5a5":"code","7cc7d1ba":"code","128b740a":"code","7ef2cc72":"code","38ecd5ec":"code","590f0dd7":"code","a98092e2":"code","733990e6":"code","b297acfb":"code","6791750f":"code","6c0c2d45":"code","dfd24fa9":"code","a592a8f1":"code","849edc92":"code","36ff468d":"code","4df5f633":"code","b3c17775":"code","a794554a":"code","cd5abc24":"code","3107387d":"code","83c66ae7":"code","ed3f9d0c":"code","d39cb92d":"code","224c41fb":"code","a84b5b13":"code","08e8c544":"code","81e70ad8":"code","92744ffc":"code","904ecc0e":"code","d4b24df8":"code","199e601a":"code","cc50bb86":"code","7f774ca6":"code","84875379":"code","21602455":"code","614ffcf6":"code","a5986bbf":"code","428cfc57":"code","54d37ccd":"code","8dcfd903":"code","3329e0e0":"code","c176e16f":"code","72a081a8":"code","a13ea634":"code","d17a7778":"code","dbbfcfcb":"code","182f96d3":"code","75dba532":"code","079b9459":"code","1481af0e":"code","add27b59":"code","b22c1685":"code","3e1c3287":"code","937505e7":"code","5b5c8286":"code","dc724af9":"code","a15364e1":"code","91b41e70":"code","66bfd946":"code","25b38337":"code","42c7ab38":"code","9bfd27d9":"code","fc4854fa":"code","5a656eb7":"code","37acb979":"code","6fb68d0b":"code","486fcdbd":"code","e9ee4c54":"code","40c76d29":"code","4d949019":"code","89b7e5d0":"code","554ba841":"code","1ba1a7cf":"code","7abe5200":"code","f412830d":"code","a1c4f52e":"code","23599f6f":"code","17a74007":"code","64a87a48":"code","ae52a5f7":"code","c23f857b":"code","3846b5dc":"code","412e2b09":"code","5e3d99fb":"code","b46c0b43":"code","100e6910":"markdown","1b8ef662":"markdown","adfab1d3":"markdown","a21c90f9":"markdown","7b41cf9e":"markdown","aa2c757a":"markdown","5c5218f1":"markdown","ad7d0f0d":"markdown","757a7d46":"markdown","07a2d916":"markdown","03aa3d2b":"markdown","9e30fd4c":"markdown","0dcb188a":"markdown","80b21b6e":"markdown","646647d6":"markdown","d67acac4":"markdown","c1395e61":"markdown","dbfb80a1":"markdown","c48bed70":"markdown","5d8753ec":"markdown","bae67094":"markdown","a8b04a80":"markdown","4dd2c817":"markdown","9dbcbdb5":"markdown","d24d4fe9":"markdown","cffa5ae7":"markdown","7838ea38":"markdown","7c0362a3":"markdown","811c91f6":"markdown","e0a75708":"markdown","63dde722":"markdown","79b3b9e3":"markdown","dc2b09cb":"markdown","d852a3fb":"markdown","7226fb65":"markdown","1a6ffe1f":"markdown"},"source":{"5e44a9b9":"# Warning Libraries :\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Scientific and Data Manipulation Libraries :\nimport pandas as pd\nimport numpy as np\nimport math\nimport gc\nimport os\n\n# ML Libraries :\nfrom sklearn.preprocessing            import LabelEncoder, OneHotEncoder \nfrom sklearn.preprocessing            import StandardScaler, MinMaxScaler, Normalizer, RobustScaler, MaxAbsScaler\nfrom sklearn.model_selection          import KFold, StratifiedKFold, train_test_split, cross_val_score\nfrom sklearn.tree                     import DecisionTreeClassifier\nfrom sklearn.ensemble                 import VotingClassifier, RandomForestClassifier\nfrom sklearn.metrics                  import f1_score, confusion_matrix, classification_report\n\n                    \n# Data Visualization Libraries :\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px","aadec855":"rs=1331 ##random_state","a79e26ed":"train= pd.read_csv('..\/input\/hranalysis\/train.csv')\ntest= pd.read_csv('..\/input\/hranalysis\/test.csv')","bb7ad31e":"train.head()","da88877c":"test.head()","1d95026c":"print(\"Train data shape\",train.shape)\nprint(\"Test data shape\",test.shape)","7157ae5e":"train.info()","7a57b5a5":"train.describe(include='all')","7cc7d1ba":"test.info()","128b740a":"test.describe(include='all')","7ef2cc72":"train.isna().sum()","38ecd5ec":"#Using missingno to visualize null values in train data\nimport missingno as msno\nmsno.bar(train, color = '#6389df', figsize = (10,8))  \n","590f0dd7":"test.isna().sum()","a98092e2":"#Using missingno to visualize null values in test data\nmsno.bar(test, color = '#6389df', figsize = (10,8))  ","733990e6":"#unique value in education feature\ntrain.education.value_counts()","b297acfb":"#plotting a pie chart\nsize = [36669,14925,805]\nlabel=[\"Bachelor's\",\"Master's & above\",'Below Secondary']\ncolor=['#6389df','#1f2b6c','#a3ccf4']\nexplode = [0.1, 0.2 , 0.3]\nplt.figure(figsize=(8,8))\nplt.pie(size,labels=label,colors=color,explode=explode,shadow=True,autopct=\"%.1f%%\")\nplt.title(\"Pie Chart of the Employees Degrees\", fontsize = 20)\nplt.axis('off')\nplt.legend(title='Education Degrees')\nplt.show()","6791750f":"#unique value in gender feature\ntrain.gender.value_counts()","6c0c2d45":"#plotting a pie chart\nsize = [38496,16312]\nlabel=[\"Male\",\"Female\"]\ncolor=['#6389df','#1f2b6c']\nexplode = [0.1, 0.2 ]\nplt.figure(figsize=(8,8))\nplt.pie(size,labels=label,colors=color,explode=explode,shadow=True,autopct=\"%.1f%%\")\nplt.title(\"Pie Chart of the GenderGap\", fontsize = 20)\nplt.axis('off')\nplt.legend(title='Gender')\nplt.show()","dfd24fa9":"plt.subplots(figsize=(15,5))\nsns.countplot(x = 'education', data = train, hue = 'gender', palette = 'Paired')\nplt.title('Showing Degree & Gender ratio', fontsize = 20)\nplt.show()","a592a8f1":"train['recruitment_channel'].value_counts()","849edc92":"size=[30446,23220,1142]\nlabel=[\"Other\",\"Sourcing\",'Referred']\ncolor=['#6389df','#1f2b6c','#a3ccf4']\nexplode=[.05,.05,.05]\nplt.figure(figsize=(8,8))\nplt.pie(size,labels=label,colors=color, startangle=90,shadow=True,autopct=\"%.2f%%\",pctdistance=.85)\n\ncenter_circle=plt.Circle((0,0),.7,fc='white')\nfig=plt.gcf()\nfig.gca().add_artist(center_circle)\n\nplt.title('A Pie Chart Representing Recruitment_Channel', fontsize = 30)\nplt.axis('off')\nplt.legend()\nplt.show()\n","36ff468d":"plt.figure(figsize=(15,5))\nsns.distplot(train['age'],color='#6389df')\nplt.title('Distribution of Age of Employees', fontsize = 30)\nplt.grid(axis='both')\nplt.show()","4df5f633":"#pie chart for the KPIs_met\ntrain['KPIs_met >80%'].value_counts()","b3c17775":"train['awards_won?'].value_counts()","a794554a":"size = [50140, 4668]\nlabels = \"NOT Promoted \", \"Promoted \"\ncolor=['#a3ccf4','#6389df']\nexplode = [0, 0.1]\n\n#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.rcParams['figure.figsize'] = (8, 8)\nplt.pie(size, labels = labels, colors = color, explode = explode, shadow =False, autopct = \"%.2f%%\",startangle=180)\nplt.title('Showing a Percentage of employees who Promoted ' , fontsize = 20)\nplt.axis('off')\nplt.legend()\nplt.show()","cd5abc24":"import plotly.express as px\nfig = px.parallel_categories(train[['department','education','gender','previous_year_rating','KPIs_met >80%',\n                                    'recruitment_channel',\n                                   'is_promoted']], \n                             color=\"is_promoted\", \n                             color_continuous_scale=px.colors.sequential.Aggrnyl  )\nfig.show()","3107387d":"#  Removes Data Duplicates while Retaining the First one - Similar to SQL DISTINCT :\ndef remove_duplicate(data):\n    \n    print(\"BEFORE REMOVING DUPLICATES - No. of Rows = \",data.shape[0])\n    data.drop_duplicates(keep=\"first\", inplace=True) \n    print(\"AFTER REMOVING DUPLICATES  - No. of Rows = \",data.shape[0])\n    return \"Checked Duplicates\"\n# Remove Duplicates from \"train\" data :\nremove_duplicate(train)\n# No Duplicates at all !!!","83c66ae7":"##missing value function which return an dataframe with total null values and percentage\ndef missing_data(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data(train)","ed3f9d0c":"missing_data(test)","d39cb92d":"train.previous_year_rating=train.previous_year_rating.fillna(0)\ntest.previous_year_rating=test.previous_year_rating.fillna(0)","224c41fb":"train['Fresher']=train['previous_year_rating'].apply(lambda x: 'Fresher' if x==0 else 'Experienced')\ntest['Fresher']=test['previous_year_rating'].apply(lambda x: 'Fresher' if x==0 else 'Experienced')","a84b5b13":"train['education']=train['education'].ffill(axis=0)\ntrain['education']=train['education'].bfill(axis=0)\n\ntest['education']=test['education'].ffill(axis=0)\ntest['education']=test['education'].bfill(axis=0)","08e8c544":"display(missing_data(train))\ndisplay(missing_data(test))","81e70ad8":"#BINNING THE AGE FEATURE IN 20-29 , 29-39 , 39-49 \nsns.distplot(train['age'])\n\ntrain['age'] = pd.cut( x=train['age'], bins=[20, 29, 39, 49], labels=['20', '30', '40'] )\ntest['age']  = pd.cut( x=test['age'], bins=[20, 29, 39, 49],  labels=['20', '30', '40'] )","92744ffc":"train.age.value_counts(dropna=False)","904ecc0e":"train.drop(['employee_id'],axis=1,inplace=True)\ntest_d=test","d4b24df8":"test_d.drop(['employee_id'],axis=1,inplace=True)","199e601a":"def data_encoding( encoding_strategy , encoding_data , encoding_columns ):\n    \n    if encoding_strategy == \"LabelEncoding\":\n        print(\"IF LabelEncoding\")\n        Encoder = LabelEncoder()\n        for column in encoding_columns :\n            print(\"column\",column )\n            encoding_data[ column ] = Encoder.fit_transform(tuple(encoding_data[ column ]))\n        \n    elif encoding_strategy == \"OneHotEncoding\":\n        print(\"ELIF OneHotEncoding\")\n        encoding_data = pd.get_dummies(encoding_data)\n        \n    dtypes_list =['float64','float32','int64','int32']\n    encoding_data.astype( dtypes_list[0] ).dtypes\n    \n    return encoding_data","cc50bb86":"encoding_columns  = [ \"region\", \"age\",\"department\", \"education\", \"gender\", \"recruitment_channel\" ]\nencoding_strategy = [ \"LabelEncoding\", \"OneHotEncoding\"]\n\ntrain_encode = data_encoding( encoding_strategy[1] , train , encoding_columns )\ntest_encode =  data_encoding( encoding_strategy[1] , test  , encoding_columns )","7f774ca6":"test_encode.head()","84875379":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, RobustScaler, MaxAbsScaler\ndef data_scaling( scaling_strategy , scaling_data , scaling_columns ):\n    \n    if    scaling_strategy ==\"RobustScaler\" :\n        scaling_data[scaling_columns] = RobustScaler().fit_transform(scaling_data[scaling_columns])\n        \n    elif  scaling_strategy ==\"StandardScaler\" :\n        scaling_data[scaling_columns] = StandardScaler().fit_transform(scaling_data[scaling_columns])\n        \n    elif  scaling_strategy ==\"MinMaxScaler\" :\n        scaling_data[scaling_columns] = MinMaxScaler().fit_transform(scaling_data[scaling_columns])\n        \n    elif  scaling_strategy ==\"MaxAbsScaler\" :\n        scaling_data[scaling_columns] = MaxAbsScaler().fit_transform(scaling_data[scaling_columns])\n        \n    else :  # If any other scaling send by mistake still perform Robust Scalar\n        scaling_data[scaling_columns] = RobustScaler().fit_transform(scaling_data[scaling_columns])\n    \n    return scaling_data","21602455":"scaling_st = [\"RobustScaler\" ,\"StandardScaler\",\"MinMaxScaler\",\"MaxAbsScaler\"]\n\ntrain_scale=data_scaling(scaling_st[0],train_encode,train_encode.columns)\ntest_scale=data_scaling(scaling_st[0],test_encode,test_encode.columns)","614ffcf6":"X=train_scale.drop(['is_promoted'],axis=1)\nY=train.is_promoted","a5986bbf":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,stratify=True)","428cfc57":"X_train.head()","54d37ccd":"X_train_rf = X_train.drop(['previous_year_rating'], axis=1)\nX_test_rf = X_test.drop(['previous_year_rating'], axis=1)\nY_train_rf = Y_train\nY_test_rf = Y_test\n\nfrom sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor()\nrfr.fit(X_train_rf, Y_train_rf)\npred = rfr.predict(X_test_rf)","8dcfd903":"X_test_rf['is_promoted'] = pred\nX_test_rf.head()","3329e0e0":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","c176e16f":"model = Sequential()\nX_train_uo = X_train.drop(['previous_year_rating'], axis=1)\nX_test_uo = X_test.drop(['previous_year_rating'], axis=1)\nmodel.add(Dense(128, input_dim = X_train_uo.shape[1], activation = 'relu'))\nmodel.add(Dense(256, 'relu'))\nmodel.add(Dense(256, 'relu'))\nmodel.add(Dense(128, 'relu'))\nmodel.add(Dense(1, kernel_initializer='normal', activation = 'sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","72a081a8":"model.fit(X_train_uo, Y_train, epochs = 400, validation_data = (X_test_uo, Y_test))","a13ea634":"pred = model.predict(X_test_uo)\nprint('MAE : ', metrics.mean_absolute_error(Y_test, pred))\nprint('MSE : ', metrics.mean_squared_error(Y_test, pred))\nprint('RMSE : ', np.sqrt(metrics.mean_squared_error(Y_test, pred))","d17a7778":"from tensorflow.keras.callbacks import ModelCheckPoint\nfrom sklearn import metrics\nX_train_cp = X_train.drop(['previous_year_rating'], axis=1)\nX_test_cp = X_test.drop(['previous_year_rating'], axis=1)\ncheckpoint = ModelCheckPoint('best_model', monitor = 'val_loss', save_best_only = True, mode = 'auto')\ncallbacks = [checkpoint]\nmodel.fit(X_train_cp, Y_train, epochs=40, validation_data = (X_test_cp, Y_test), callbacks = callbacks)","dbbfcfcb":"pred = model.predict(X_test_cp)\nprint('MAE : ', metrics.mean_absolute_error(Y_test, pred))\nprint('MSE : ', metrics.mean_squared_error(Y_test, pred))\nprint('RMSE : ', np.sqrt(metrics.mean_squared_error(Y_test, pred))","182f96d3":"Y_train.value_counts(normalize=True)*100","75dba532":"from imblearn.combine import SMOTETomek\ndef oversample(X,Y):\n    over_sample = SMOTETomek(random_state=rs)\n    X_over,Y_over = over_sample.fit_resample(X,Y)\n    return X_over,Y_over","079b9459":"X_train_os,Y_train_os=oversample(X_train,Y_train)","1481af0e":"clf = RandomForestClassifier(n_estimators=100, random_state=0)\nclf.fit(X_train, Y_train)\n\n### FEATURE SCORES \nfeature_scores = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\nprint(feature_scores)\n\n### PLOT TO VISUALIZE\nsns.barplot(x=feature_scores, y=feature_scores.index)\n# Add labels to the graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\n# Add title to the graph\nplt.title(\"Visualizing Important Features\")\n# Visualize the graph\nplt.show()\n","add27b59":"from sklearn.ensemble          import RandomForestClassifier\nfrom sklearn.tree              import DecisionTreeClassifier\n\nfrom sklearn.metrics           import accuracy_score\nfrom sklearn.metrics           import classification_report\nfrom sklearn.metrics           import confusion_matrix\n\nfrom sklearn.model_selection   import RandomizedSearchCV\nfrom sklearn.model_selection   import KFold,cross_val_score\n","b22c1685":"## PASSING THE TRAIN DATA IN THE CROSS VALIDATION\nkf=KFold(n_splits=5,random_state=rs,shuffle=True)\ncnt = 1\n# split()  method generate indices to split data into training and test set.\nfor train_index, test_index in kf.split(X_train_os, Y_train_os):\n    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n    cnt+=1","3e1c3287":"score = cross_val_score(RandomForestClassifier(random_state= rs), X_train_os, Y_train_os, cv= kf, scoring=\"accuracy\")\nprint(f'Scores for each fold are: {score}')\nprint(f'Average score: {\"{:.2f}\".format(score.mean())}')","937505e7":"rfc=RandomForestClassifier(random_state=rs)\nrfc.fit(X_train_os,Y_train_os)\ny_pred_rf=rfc.predict(X_train_os)\nprint(accuracy_score(y_pred_rf,Y_train_os))","5b5c8286":"cm = confusion_matrix(Y_train_os, y_pred_rf)\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.heatmap(cm, annot = True, annot_kws = {'size':15}, cmap = 'PuBu',fmt=\".1f\")","dc724af9":"y_pred_test=rfc.predict(X_test)\nprint(accuracy_score(y_pred_test,Y_test))","a15364e1":"cm = confusion_matrix(Y_test, y_pred_test)\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.heatmap(cm, annot = True, annot_kws = {'size':15}, cmap = 'PuBu',fmt=\".1f\")","91b41e70":"print(classification_report(Y_test,y_pred_test))","66bfd946":"#Randomized Search CV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","25b38337":"random_grid={'n_estimators': n_estimators,\n            'max_features': max_features,\n            'max_depth': max_depth,\n            'min_samples_split': min_samples_split,\n            'min_samples_leaf': min_samples_leaf}","42c7ab38":"rf=RandomForestClassifier()","9bfd27d9":"rf_random = RandomizedSearchCV(estimator = rf,param_distributions = random_grid,\n                                scoring='f1',\n                                n_iter = 10, cv = 5,\n                                verbose=2, random_state=rs,\n                                n_jobs = 1)\nrf_random.fit(X_train_os,Y_train_os)","fc4854fa":"rf_random.best_params_","5a656eb7":"rfc= RandomForestClassifier(random_state=rs,\n                            n_estimators=1100,\n                            min_samples_split=5,\n                            max_features='auto',\n                            min_samples_leaf= 1,\n                            max_depth=20,oob_score=True)\nrfc.fit(X_train_os,Y_train_os)","37acb979":"y_pred_rf_ht=rfc.predict(X_train_os)\nprint(accuracy_score(y_pred_rf_ht,Y_train_os))\ncm = confusion_matrix(Y_train_os, y_pred_rf_ht)\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.heatmap(cm, annot = True, annot_kws = {'size':15}, cmap = 'PuBu',fmt=\".1f\")","6fb68d0b":"y_pred_test_ht=rfc.predict(X_test)\nprint(accuracy_score(y_pred_test_ht,Y_test))","486fcdbd":"cm = confusion_matrix(Y_test, y_pred_test_ht)\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.heatmap(cm, annot = True, annot_kws = {'size':15}, cmap = 'PuBu',fmt=\".1f\")","e9ee4c54":"dtc=DecisionTreeClassifier(random_state=rs)\nscore = cross_val_score(dtc, X_train_os, Y_train_os, cv= kf, scoring=\"accuracy\")\nprint(f'Scores for each fold are: {score}')\nprint(f'Average score: {\"{:.2f}\".format(score.mean())}')","40c76d29":"def hyperparameter_tuning(X,Y,rf):\n#Randomized Search CV\n# Number of features to consider at every split\n    max_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\n    max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\n    min_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\n    min_samples_leaf = [1, 2, 5, 10]\n\n    random_grid={\n            'max_features': max_features,\n            'max_depth': max_depth,\n            'min_samples_split': min_samples_split,\n            'min_samples_leaf': min_samples_leaf}\n\n    rf_random = RandomizedSearchCV(estimator = rf,param_distributions = random_grid,\n                                scoring='f1', \n                                n_iter = 10, cv = 5,\n                                verbose=0, random_state=rs,\n                                n_jobs = 1)\n    rf_random.fit(X,Y)\n    return rf_random.best_params_","4d949019":"param_dt=hyperparameter_tuning(X_train_os,Y_train_os,dtc)","89b7e5d0":"## Printing the best parameters obtained after randomizesearch CV or hyperparameter tuning\nprint(param_dt)","554ba841":"dtc=DecisionTreeClassifier(random_state=rs,\n                           min_samples_split=10,\n                           min_samples_leaf=2,\n                           max_features='auto',\n                           max_depth=25)\ndtc.fit(X_train_os,Y_train_os)\ny_pred_train_dc_ht=dtc.predict(X_train_os)\ny_pred_test_dt_ht=dtc.predict(X_test)\nprint('Test Accuracy',accuracy_score(y_pred_test_dt_ht,Y_test))\nprint('Train Accuracy',accuracy_score(y_pred_train_dc_ht,Y_train_os))","1ba1a7cf":"# Boosting Algorithms :\nfrom xgboost                          import XGBClassifier\nfrom catboost                         import CatBoostClassifier\nfrom lightgbm                         import LGBMClassifier\n\nfrom scipy.stats                      import randint","7abe5200":"mod= CatBoostClassifier(random_state=rs)\n\npar={'max_depth':[5,10,None],\n              'n_estimators':[200,300,400,500,600],'learning_rate':[0.1,0.01,0.001]}\ndef hyperparameter_tuning(mod,param_d,p,q):\n    rdmsearch=  RandomizedSearchCV(mod, param_distributions=param_d,n_jobs=-1,cv=9,scoring='roc_auc')\n    rdmsearch.fit(p,q)\n    ht_params = rdmsearch.best_params_\n    ht_score = rdmsearch.best_score_\n    return ht_params, ht_score\n\n\nrf_parameters, rf_ht_score = hyperparameter_tuning(mod, par,  X_train_os,Y_train_os)\n","f412830d":"print(rf_parameters, rf_ht_score)","a1c4f52e":"mod=XGBClassifier(random_state=rs)\nparam_tuning = {\n        'learning_rate': [0.01, 0.1],\n        'max_depth': [3, 5, 7, 10], \n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.5, 0.7],\n        'colsample_bytree': [0.5, 0.7],\n        'n_estimators' : [100, 200, 500]\n                }\nrf_parameters_xgb, rf_ht_score_xgb = hyperparameter_tuning(mod, param_tuning,  X_train_os,Y_train_os)","23599f6f":"print(rf_parameters_xgb, rf_ht_score_xgb)","17a74007":"lgb = LGBMClassifier()\nlgb.fit(X_train_os,Y_train_os)\n\nlgb_pred = lgb.predict(X_test)\n\nprint(\"Training Accuracy :\", lgb.score(X_train_os, Y_train_os))","64a87a48":"# Create a Dictionary (Key->Value Pairs) for \"ML Model Name\"-> \"ML Model Functions with Hyper-Parameters\" :\n\nClassifiers = {'0.XGBoost' : XGBClassifier(learning_rate =0.1, \n                                           n_estimators=394, \n                                           max_depth=10, \n                                           subsample = 0.50, \n                                           verbosity = 0,\n                                           scale_pos_weight = 2.5,\n                                           updater =\"grow_histmaker\",\n                                           base_score  = 0.2,\n                                          min_child_weight=1),\n                            \n               '1.CatBoost' : CatBoostClassifier(learning_rate=0.1, \n                                                 n_estimators=300, \n                                                 subsample=0.085, \n                                                 max_depth=10, \n                                                 scale_pos_weight=2.5),\n               \n               '2.LightGBM' : LGBMClassifier(subsample_freq = 2, \n                                             objective =\"binary\",\n                                             importance_type = \"gain\",\n                                             verbosity = -1, \n                                             max_bin = 60,\n                                             num_leaves = 300,\n                                             boosting_type = 'dart',\n                                             learning_rate=0.10, \n                                             n_estimators=494,\n                                             max_depth=10, \n                                             scale_pos_weight=2.5)\n }\n\nprint( list(Classifiers.keys()) )\nprint(\"--#--\"*25)\nprint( list(Classifiers.values()) )","ae52a5f7":"from sklearn.ensemble import VotingClassifier\nvoting_model = VotingClassifier(estimators=[\n                                              ('XGBoost_Best', list(Classifiers.values())[0]), \n                                              ('CatBoost_Best', list(Classifiers.values())[1]),\n                                              ('LightGBM_Best', list(Classifiers.values())[2]),\n                                             ], \n                                              voting='soft',weights=[5,5,5.2])\n\nvoting_model.fit(X_train_os,Y_train_os) \n\npredictions_of_voting = voting_model.predict_proba( test_encode )[::,1]","c23f857b":"predictions_of_voting\n","3846b5dc":"y_pred_class = [int(round(value)) for value in predictions_of_voting]","412e2b09":"### Final ensembel model after hyperparameter tuning ","5e3d99fb":"# Data Visualization Libraries :\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px","b46c0b43":"Classifiers = {'0.XGBoost' : XGBClassifier(learning_rate =0.1, \n                                           n_estimators=394, \n                                           max_depth=5,\n                                           subsample = 0.70, \n                                           verbosity = 0,\n                                           scale_pos_weight = 2.5,\n                                           updater =\"grow_histmaker\",\n                                           base_score  = 0.2),\n               \n            \n               '1.CatBoost' : CatBoostClassifier(learning_rate=0.15, \n                                                 n_estimators=300, \n                                                 max_depth=5, \n                                                 scale_pos_weight=2.5,\n                                                verbose=False),\n               \n               '2.LightGBM' : LGBMClassifier(learning_rate=0.15, \n                                             n_estimators=494,\n                                             subsample_freq = 2, \n                                             objective =\"binary\",\n                                             importance_type = \"gain\",\n                                             verbosity = -1, \n                                             max_bin = 60,\n                                             num_leaves = 300,\n                                             boosting_type = 'dart',                                            \n                                             max_depth=5, \n                                             scale_pos_weight=2.5)\n                }\n\nprint( list(Classifiers.keys()) )\n\nclf1 = list(Classifiers.values())[0]\nclf2 =list(Classifiers.values())[1]\nclf3 = list(Classifiers.values())[2]\nX = np.array([[-1.0, -1.0], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\ny = np.array([1, 1, 2, 2])\n\neclf = VotingClassifier(estimators=[('xgboost', clf1), \n                                    ('catboost', clf2), \n                                    ('lgbm', clf3)],\n                        voting='soft',\n                        weights=[5, 5, 5.2])\n\n# predict class probabilities for all classifiers\nprobas = [c.fit(X, y).predict_proba(X) for c in (clf1, clf2, clf3, eclf)]\n\n# get class probabilities for the first sample in the dataset\nclass1_1 = [pr[0, 0] for pr in probas]\nclass2_1 = [pr[0, 1] for pr in probas]\n\n# plotting\n\nN = 4  # number of groups\nind = np.arange(N)  # group positions\nwidth = 0.35  # bar width\n\nfig, ax = plt.subplots()\n\n# bars for classifier 1-3\np1 = ax.bar(ind, np.hstack(([class1_1[:-1], [0]])), width,\n            color='green', edgecolor='k')\np2 = ax.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), width,\n            color='lightgreen', edgecolor='k')\n\n# bars for VotingClassifier\np3 = ax.bar(ind, [0, 0, 0, class1_1[-1]], width,\n            color='blue', edgecolor='k')\np4 = ax.bar(ind + width, [0, 0, 0, class2_1[-1]], width,\n            color='steelblue', edgecolor='k')\n\n# plot annotations\nplt.axvline(2.8, color='k', linestyle='dashed')\nax.set_xticks(ind + width)\nax.set_xticklabels(['XGBoost\\nweight 5',\n                    'CatBoost\\nweight 5',\n                    'LightGBM\\nweight 5.2',\n                    'VotingClassifier\\n(average probabilities)'],\n                   rotation=40,\n                   ha='right')\nplt.ylim([0, 1])\nplt.title('Class probabilities for sample 1 by different classifiers')\nplt.legend([p1[0], p2[0]], ['is_promoted=No', 'is_promoted=Yes'], loc='upper right')\nplt.tight_layout()\nplt.show()","100e6910":"## DP : CHECKPOINTS","1b8ef662":"## DP :  NEUTRAL CLASS\n**CAN'T APPLY!**\n\nAs this is not a multi-class problem.","adfab1d3":"### Hyperparameter Tuning \n- CatBoostClassifier\n","a21c90f9":"## DP : DISTRIBUTION STRATEGY\n**CAN'T APPLY!**\n\nBecause of the scale of problem at hand is small, the machine doesn't have multiple GPU's  and distribution strategy is applied over large neural networks to have millions of\nparameters and be trained on massive amounts of data. It would be an overkill.","7b41cf9e":"**Used Voting classifier** -A Voting Classifier is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class as the output.\n\n **Soft Voting** -In soft voting, the output class is the prediction based on the average of probability given to that class. Suppose given some input to three models, the prediction probability for class A = (0.30, 0.47, 0.53) and B = (0.20, 0.32, 0.40). So the average for class A is 0.4333 and B is 0.3067, the winner is clearly class A because it had the highest probability averaged by each classifier.\n","aa2c757a":"## 1. Missing Value Analysis ","5c5218f1":"### 2. Cat Boost Classifier","ad7d0f0d":"## DP : FEATURE CROSS","757a7d46":"## DP : ENSEMBLE","07a2d916":"## DP : BRIDGED SCHEMA \n**CAN'T APPLY!**\n\nAs there is no new data that the model is going to work on.","03aa3d2b":"### Hyperparameter Tuning \n- Decision Tree CLASSIFIER\n","9e30fd4c":"## DP : REPEATABLE SPLITTING\n### Stratified Split\nWe need to ensure that there are examples of each king in both test and train datasets. Though this becomes less concerning as the dataset grows, still a good practice to make sure that stratify = True.","0dcb188a":"- {'n_estimators': 1100,\n- 'min_samples_split': 5,\n- 'min_samples_leaf': 1,\n- 'max_features': 'auto',\n- 'max_depth': 20}","80b21b6e":"## DP : EMBEDDING","646647d6":"Cleary we can see data is unbalanced with only 8% of 1s in it so we have to use over Sampling on the data so as to make it balanced using Smote Over Sampling Method ","d67acac4":"- {'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 25}","c1395e61":"## DP : REFRAMING","dbfb80a1":"### Split target variable and predictors","c48bed70":"## DP : POOLING \n**CAN'T APPLY!**\n\nCan't be applied as it is not an image dataset.","5d8753ec":"### a. BAGGING\n#### 1.Randomforest \n#### 2. Decision Tree\n\n### b. BOOSTING\n#### 1.CatBoost\n#### 2.XG Boost \n#### 3.LGBM","bae67094":"## 2. Data Cleaning \nDuplicate removal","a8b04a80":"### 1. Random Forest Classifier","4dd2c817":"## DP : HYPERPARAMETER TUNING \n- RANDOM FOREST CLASSIFIER\n","9dbcbdb5":"## DP : CONTINUED MODEL EVALUATION\n**CAN'T APPLY!**\n\nAs this model is not in production. ","d24d4fe9":"* `ffill is used to forward fill the missing values in the dataset - https:\/\/www.geeksforgeeks.org\/python-pandas-dataframe-ffill\/`\n\n* `bfill is used to backward fill the missing values in the dataset - https:\/\/www.geeksforgeeks.org\/python-pandas-dataframe-bfill\/`","cffa5ae7":"## DP : CASCADE\n\n**CAN'T APPLY!**\n\nSince the problem is relatively easy, there is no need to apply Cascade DP to it.","7838ea38":"### Loading the Data","7c0362a3":"## FEATURE SCALING","811c91f6":"## DP : TWO PHASE PREDICTION\n\n**CAN'T APPLY!**\n\nAs the model is not in production.","e0a75708":"## DP : MULTILABEL \n**CAN'T APPLY!**\n\nThe less-common scenario is when each training example can be assigned more than one label, which is what this pattern addresses. Our dataset doesn't have more than one label.","63dde722":"## Encoding \n`Converting the categorical features into binary or numerical counterparts`","79b3b9e3":"## DP : REBALANCING\n### Oversampling using SMOTE ","dc2b09cb":"## DP : TRANSFER LEARNING\n**CAN'T APPLY!**\n\nThe problem isn't that complex and the dataset is relatively small and specific, we can't apply transfer learning to find pre-trained weights and parameters.","d852a3fb":"## 2. Decision Tree\n","7226fb65":"### FEATURE SELECTION","1a6ffe1f":"## DP : USEFUL OVERFITTING"}}