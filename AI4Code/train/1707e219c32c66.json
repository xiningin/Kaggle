{"cell_type":{"19f01ff9":"code","154ba35a":"code","448b7e40":"code","719e42ea":"code","2643f195":"code","7d691076":"code","17d639c5":"code","761c7548":"code","e2b8074b":"code","acaa9266":"code","e5b63acc":"code","0f91c850":"code","86e3d4db":"code","2ebd790b":"code","a90813c6":"code","9a54978e":"code","ad96bb72":"code","3acdf1f7":"code","3575c9e3":"code","79dcd889":"code","65160c5f":"code","21a9f987":"code","7e6dcd18":"code","506fa4d4":"code","44fb7158":"markdown","37c2cb0e":"markdown","c050d04e":"markdown","fe1dd9e4":"markdown","241015ee":"markdown","746e2f45":"markdown","7d08088e":"markdown","485fbc6a":"markdown"},"source":{"19f01ff9":"%%capture\n\n# Dirty code to make it work\n\nimport sys\n!cp -r ..\/input\/openai-clip\/CLIP\/CLIP-main \/tmp\/\n\n# Kaggle likes to unpack .gz files in datasets... so we have to pack it back\n!gzip -c \/tmp\/CLIP-main\/clip\/bpe_simple_vocab_16e6.txt > \/tmp\/CLIP-main\/clip\/bpe_simple_vocab_16e6.txt.gz\nsys.path.append('\/tmp\/CLIP-main')\n\n!pip install ..\/input\/openai-clip\/ftfy-5.9\/ftfy-5.9 \\\n             ..\/input\/openai-clip\/torch-1.7.1+cu110-cp37-cp37m-linux_x86_64.whl \\\n             ..\/input\/openai-clip\/torchvision-0.8.2+cu110-cp37-cp37m-linux_x86_64.whl \\\n             ..\/input\/faiss-163\/faiss_gpu-1.6.3-cp37-cp37m-manylinux2010_x86_64.whl","154ba35a":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import Sampler\nimport clip\nfrom PIL import Image\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport re\nfrom clip.simple_tokenizer import SimpleTokenizer\nimport faiss\nimport matplotlib.pyplot as plt\nfrom triplet_loss import TripletLoss\n\n%matplotlib inline","448b7e40":"df_test = pd.read_csv('..\/input\/shopee-product-matching\/test.csv', index_col='posting_id')","719e42ea":"# Run train only for commit\nRUN_ON_TRAIN = len(df_test) == 3","2643f195":"_tokenizer = SimpleTokenizer()\n\n# Copied from https:\/\/github.com\/openai\/CLIP\/blob\/beba48f35392a73c6c47ae67ddffced81ad1916d\/clip\/clip.py#L164\n# but with relaxed exception\ndef tokenize(texts, context_length: int = 77) -> torch.LongTensor:\n    if isinstance(texts, str):\n        texts = [texts]\n\n    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n\n    for i, tokens in enumerate(all_tokens):\n        n = min(len(tokens), context_length)\n        result[i, :n] = torch.tensor(tokens)[:n]\n        if len(tokens) > context_length:\n            result[i, -1] = tokens[-1]\n\n    return result","7d691076":"# Remove EMOJI\nRE_EMOJI = re.compile(r\"\\\\x[A-Za-z0-9.\/]+\", flags=re.UNICODE)\n\ndef strip_emoji(text):\n    return RE_EMOJI.sub(r'', text)","17d639c5":"class RollingMean():\n    def __init__(self):\n        self.n = 0\n        self.mean = 0\n        \n    def update(self, value):\n        self.mean = (self.mean * self.n + value) \/ (self.n+1)\n        self.n += 1\n        \n    def result(self):\n        return self.mean","761c7548":"class SameGroupSampler(Sampler):\n    def __init__(self, df ,ds):\n        super().__init__(ds)\n        \n        # Create a dictionary of posting_id -> index in dataset\n        self.index_to_position = dict(zip(df.index, range(len(df))))\n        \n        # Create a Series of label_group -> set(posting_id)\n        self.label_group = df.reset_index().groupby('label_group')['posting_id'].apply(set).map(sorted).map(np.array)\n\n    def __len__(self):\n        return len(self.label_group)\n        \n    def __iter__(self):\n        for _ in range(len(self)):\n            # Sample one label_group\n            label_group_sample = self.label_group.sample(1).iloc[0]\n            \n            # Sample two posting_id's\n            sample1, sample2 = np.random.choice(label_group_sample, 2, replace=False)\n            \n            yield self.index_to_position[sample1]\n            yield self.index_to_position[sample2]            ","e2b8074b":"class MyDataset(Dataset):\n    def __init__(self, df, images_path):\n        super().__init__()\n        self.df = df\n        self.images_path = images_path\n        self.has_target = ('label_group' in df)\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        image = preprocess(Image.open(self.images_path \/ row['image']))\n        text = tokenize([strip_emoji(row['title'])])[0]\n        \n        if self.has_target:\n            return image, text, row['label_group']\n        else:\n            return image, text, 0","acaa9266":"# Load CLIP\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"..\/input\/openai-clip\/ViT-B-32.pt\", device=device, jit=False)\n\n# Get embedding size\nembed_dim = model.text_projection.shape[1]\nembed_dim","e5b63acc":"# Load train data\ntrain_images_path = Path('..\/input\/shopee-product-matching\/train_images')\n\ndf_train = pd.read_csv('..\/input\/shopee-product-matching\/train.csv', index_col='posting_id')\n\ndstrain = MyDataset(df_train, train_images_path)\ndltrain = DataLoader(dstrain, batch_size=128, num_workers=2, sampler=SameGroupSampler(df_train, dstrain))","0f91c850":"n_epochs = 1","86e3d4db":"# optim = torch.optim.AdamW(model.parameters(), lr=1e-4, eps=1e-8, weight_decay=1e-2)\noptim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.2)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optim, 1e-2, total_steps=n_epochs * (2*len(dltrain)-1),\n                                               base_momentum=0.0, max_momentum=0.5, pct_start=0.1, div_factor=1e2, final_div_factor=1e4)\ncriterion = TripletLoss(device)","2ebd790b":"for epoch in range(n_epochs):\n    with tqdm(total=2*len(dltrain)-1) as bar:\n        loss_mean = RollingMean()\n        for images, texts, targets in dltrain:\n            targets = targets.to(device)\n            \n            # Generate train and text features\n            images_features = model.encode_image(images.to(device))\n            texts_features = model.encode_text(texts.to(device))\n\n            optim.zero_grad()\n\n            # Join train and test features\n            features = torch.hstack([images_features, texts_features])\n            \n            # L2-normalize features\n            features = features \/ features.norm(2, dim=1, keepdim=True)\n\n            # Apply Triplet SemiHardLoss\n            loss = criterion(features, targets)\n\n            loss.backward()\n            optim.step()\n            scheduler.step()\n\n            # Update metric and progress bar\n            loss_mean.update(loss.item())\n            bar.update()\n            bar.set_description('{:.4f}'.format(loss_mean.result()))","a90813c6":"def find_similarities_and_indexes(df, images_path, top_n=100, features_file=None):\n    # Create pytorch Dataset\/DataLoader\n    ds = MyDataset(df, images_path)\n    dl = DataLoader(ds, batch_size=32, shuffle=False, num_workers=2)\n\n    # Allocate memory for features\n    features = np.empty((len(df), 2*embed_dim), dtype=np.float32)\n\n    # Begin predict\n    i = 0\n    for images, texts, _ in tqdm(dl):\n        n = len(images)\n        with torch.no_grad():\n            # Generate image and text features\n            images_features = model.encode_image(images.to(device))\n            texts_features = model.encode_text(texts.to(device))\n\n        # Concat features (first images then texts)\n        features[i:i+n, :embed_dim] = images_features.cpu()\n        features[i:i+n, embed_dim:] = texts_features.cpu()\n\n        i += n\n\n    # Option to save these features (may be usefull to tune cut value)\n    if features_file is not None:\n        np.save(features_file, features)\n\n    # l2-normalize\n    features \/= np.linalg.norm(features, 2, axis=1, keepdims=True)\n\n    # Create index\n    index = faiss.IndexFlatIP(2*embed_dim)\n    index.add(features)\n\n    # Search index\n    return index.search(features, top_n)\n\n    # TODO: try range_search\n    # lims, similarities, indexes = index_test.range_search(test_features, GROUP_CUT)","9a54978e":"if RUN_ON_TRAIN:\n    # Perform search of similiar items\n    similarities, indexes = find_similarities_and_indexes(df_train, train_images_path, features_file='features-no-norm.npy')\n    \n    # `similarities` will have shape (n, 100) and will have the similarites scores for closest matches\n    # `indexes` will have shape (n, 100) and have the index closest matches.\n    # Both arrays are aligned\n\n    # Convert index to groups, will have shape (n, 100)\n    found_groups = df_train['label_group'].values[indexes]\n\n    # Check if matches are from same group. Will create a boolean vector of (n, 100)\n    is_same_group = (found_groups == df_train['label_group'].values[:, np.newaxis])\n\n    # Plot similarities score from same group and different groups\n    plt.figure(figsize=(10, 5))\n    plt.hist([similarities[is_same_group], similarities[~is_same_group]], density=False, bins=51,\n         label=['Same group', 'Different group'], histtype='stepfilled', alpha=0.75)\n    plt.xlim(0, 1)\n    plt.xlabel('Similarity score')\n    plt.legend();","ad96bb72":"# SRC: https:\/\/www.kaggle.com\/c\/shopee-product-matching\/discussion\/224782#1233338\n# With some adaptation\ndef row_wise_f1_score(y_true, y_pred):\n    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    fp = y_pred.apply(lambda x: len(x)).values - tp\n    fn = y_true.apply(lambda x: len(x)).values - tp\n\n    precision = tp \/ (tp + fp)\n    recall = tp \/ (tp + fn)\n    f1 = 2 * ((precision * recall) \/ (precision + recall))\n    return f1\n\n\ndef calc_score(cut_value):\n    # Apply cutoff of similarities\n    groups_are_same = (similarities > cut_value)\n\n    # Build results\n    results = []\n    for i, (group_is_same, index_result) in enumerate(zip(groups_are_same, indexes)):\n        row_results = df_train.index[index_result[group_is_same]]\n\n        # Keep found matches as a `set`\n        results.append(set(row_results))\n\n    df_results = pd.Series(results, index=df_answer.index)\n    \n    # Evaluate results\n    return row_wise_f1_score(df_answer, df_results).mean()","3acdf1f7":"if RUN_ON_TRAIN:\n    # Create answer dataframe. This will have posting_id on index and a set of label_group as values \n    groups = df_train.reset_index().groupby('label_group')['posting_id'].apply(set)\n    df_answer = df_train['label_group'].map(groups)\n\n    # Cut values to evaluate\n    cuts = np.linspace(0.5, 0.95, 51)\n    scores = [calc_score(c) for c in tqdm(cuts)]\n\n    # Plot curve\n    plt.plot(cuts, scores)\n    plt.xlabel('Cutoff value')\n    plt.ylabel('F1 score')\n\n    print('Best cutoff is {:.2f} with expected F1 score of {:.4f}'.format(cuts[np.argmax(scores)], max(scores)))","3575c9e3":"GROUP_CUT = 0.71  # Use option `RUN_ON_TRAIN` to find this number","79dcd889":"test_images_path = Path('..\/input\/shopee-product-matching\/test_images')","65160c5f":"# Find similar matches\nsimilarities, indexes = find_similarities_and_indexes(df_test, test_images_path)","21a9f987":"# Apply cutoff of similiarites\ntest_are_same_groups = (similarities > GROUP_CUT)","7e6dcd18":"# Build submission\nresults = []\n\nfor i, (test_is_same_group, index_result) in enumerate(zip(test_are_same_groups, indexes)):\n    row_results = set(df_test.index[index_result[test_is_same_group]])\n    \n    results.append({\n        'posting_id': df_test.index[i],\n        'matches': ' '.join(row_results)\n    })\n    \ndf_sub = pd.DataFrame(results)","506fa4d4":"df_sub.to_csv('submission.csv', index=False)","44fb7158":"## Finetune CLIP on train data\n\nHere we use the triplet loss principe to ajust CLIP:\n\n![Triplet loss](https:\/\/user-images.githubusercontent.com\/18154355\/61485418-1cbb1f00-a96f-11e9-8de8-3c46eef5a7dc.png)\n\n\nWe didn't provide a validation set (yet!), so we are deliberating overfiting.","37c2cb0e":"## OpenAI CLIP with train\n\nThis notebook uses [OpenAI CLIP](https:\/\/github.com\/openai\/CLIP) to generate images and text features.","c050d04e":"\u2600\ufe0f\u2600\ufe0f\u2600\ufe0f Have a nice day! \u2600\ufe0f\u2600\ufe0f\u2600\ufe0f","fe1dd9e4":"### Tune CUT\n\nIn this last step we will move the `cut_value` to find optimal F1-score.","241015ee":"### Utility classes and functions","746e2f45":"## Run on train\n\nIn this section we will generate features using CLIP and perform a similiarity search to find the closest matches.\n\nWe create the final set by taking away those results bellow a threshold similiarity (less 0.7)","7d08088e":"### Sampler and dataset\n\nWe implement a sampler that ensures that in every batch, two samples of the same group are always present.\n\nThis is important in order to use Triplet SemiHardLoss (I'm using [this implementation](https:\/\/github.com\/alfonmedela\/triplet-loss-pytorch\/blob\/master\/loss_functions\/triplet_loss.py))","485fbc6a":"## Run on test"}}