{"cell_type":{"75fdbb92":"code","483d3346":"code","77257b22":"code","95c0a96e":"code","72b0a35d":"code","6843994e":"code","5b46b896":"code","be44203f":"code","c87c2cc4":"code","1ad93f1f":"code","84e81e5a":"code","f0387a9f":"code","74ccddbd":"code","9290de8c":"code","6fd832f6":"code","a5d9fc6f":"code","f8381dad":"code","4e8c9323":"code","7be4a1e6":"code","fb27711e":"code","3c100f31":"code","d0b0a11a":"code","60f74196":"code","4b816c23":"code","c7dc03e0":"code","77d46931":"code","40fb981e":"code","e572e9fd":"code","30350397":"markdown","f8f8486d":"markdown","87068d76":"markdown","829463d6":"markdown","e5ee3f45":"markdown","ca0738e1":"markdown","f3275af9":"markdown","bef360b5":"markdown","d6bec9b2":"markdown","238b4bf2":"markdown"},"source":{"75fdbb92":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport numpy as np\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk.tokenize import TweetTokenizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, accuracy_score, f1_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, recall_score, precision_score","483d3346":"train_data = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_data  =pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain_data.head(10)\ntrain_data.dtypes","77257b22":"import re\ndef  clean_text(df, text_field, new_text_field_name):\n    df[new_text_field_name] = df[text_field].str.lower()\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)|^rt|http.+?\", \"\", elem))  \n    # remove numbers\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n    \n    return df\ndata_clean_train = clean_text(train_data, 'text', 'text_clean')\ndata_clean_test = clean_text(test_data, 'text', 'text_clean')\n\n","95c0a96e":"import nltk.corpus\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\ndata_clean_train['text_clean'] = data_clean_train['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ndata_clean_test['text_clean'] = data_clean_test['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))","72b0a35d":"train, test = train_test_split(data_clean_train, test_size=0.2, random_state=1)\nX_train = train['text_clean'].values\nX_test = test['text_clean'].values\ny_train = train['target']\ny_test = test['target']","6843994e":"test = test['text_clean']\ndef tokenize(text): \n    tknzr = TweetTokenizer()\n    return tknzr.tokenize(text)\n\ndef stem(doc):\n    return (stemmer.stem(w) for w in analyzer(doc))\n\nen_stopwords = set(stopwords.words(\"english\")) \n\nvectorizer = CountVectorizer(\n    analyzer = 'word',\n    tokenizer = tokenize,\n    lowercase = True,\n    ngram_range=(1, 1),\n    stop_words = en_stopwords)","5b46b896":"kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)","be44203f":"np.random.seed(1)\n\npipeline_svm = make_pipeline(vectorizer, \n                            SVC(probability=True, kernel=\"linear\", class_weight=\"balanced\"))\n\ngrid_svm = GridSearchCV(pipeline_svm,\n                    param_grid = {'svc__C': [0.01, 0.1, 1]}, \n                    cv = kfolds,\n                    scoring=\"roc_auc\",\n                    verbose=1,   \n                    n_jobs=-1) \n\ngrid_svm.fit(X_train, y_train)\ngrid_svm.score(X_test, y_test)","c87c2cc4":"grid_svm.best_params_","1ad93f1f":"grid_svm.best_score_","84e81e5a":"def report_results(model, X, y):\n    pred_proba = model.predict_proba(X)[:, 1]\n    pred = model.predict(X)        \n\n    auc = roc_auc_score(y, pred_proba)\n    acc = accuracy_score(y, pred)\n    f1 = f1_score(y, pred)\n    prec = precision_score(y, pred)\n    rec = recall_score(y, pred)\n    result = {'auc': auc, 'f1': f1, 'acc': acc, 'precision': prec, 'recall': rec}\n    return result","f0387a9f":"report_results(grid_svm.best_estimator_, X_test, y_test)","74ccddbd":"#Another way to do it \nfrom sklearn.metrics import classification_report\npred = grid_svm.predict(X_test)\nprint(classification_report(y_test, pred))","9290de8c":"def get_roc_curve(model, X, y):\n    pred_proba = model.predict_proba(X)[:, 1]\n    fpr, tpr, _ = roc_curve(y, pred_proba)\n    return fpr, tpr","6fd832f6":"roc_svm = get_roc_curve(grid_svm.best_estimator_, X_test, y_test)","a5d9fc6f":"fpr, tpr = roc_svm\nplt.figure(figsize=(14,8))\nplt.plot(fpr, tpr, color=\"red\")\nplt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Roc curve')\nplt.show()","f8381dad":"from sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, test_scores = \\\n    learning_curve(grid_svm.best_estimator_, X_train, y_train, cv=5, n_jobs=-1, \n                   scoring=\"roc_auc\", train_sizes=np.linspace(.1, 1.0, 10), random_state=1)","4e8c9323":"def plot_learning_curve(X, y, train_sizes, train_scores, test_scores, title='', ylim=None, figsize=(14,8)):\n\n    plt.figure(figsize=figsize)\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"lower right\")\n    return plt","7be4a1e6":"plot_learning_curve(X_train, y_train, train_sizes, \n                    train_scores, test_scores, ylim=(0.7, 1.01), figsize=(14,6))\nplt.show()","fb27711e":"grid_svm.predict([\"flying with @united is always a great experience\"])","3c100f31":"grid_svm.predict([\"flying with @united is always a great experience. If you don't lose your luggage\"])","d0b0a11a":"grid_svm.predict([\"I love @united. Sorry, just kidding!\"])","60f74196":"grid_svm.predict([\"@united very bad experience!\"])","4b816c23":"grid_svm.predict([\"@united very bad experience!\"])","c7dc03e0":"submission_test_clean = test_data.copy()\nsubmission_test_clean = clean_text(submission_test_clean, \"text\",\"text_clean\")\nsubmission_test_clean['text_clean'] = submission_test_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\nsubmission_test_clean = submission_test_clean['text_clean']\nsubmission_test_clean.head()","77d46931":"submission_test_pred = grid_svm.predict(submission_test_clean)","40fb981e":"id_col = test_data['id']\nsubmission_df_1 = pd.DataFrame({\n                  \"id\": id_col, \n                  \"target\": submission_test_pred})\nsubmission_df_1.head()","e572e9fd":"submission_df_1.to_csv(\"submisssions.csv\", index=False)","30350397":"## Examples\n\nWe are going to apply the obtained machine learning model to some example text. If the output is **1** it means that the text has a negative sentiment associated:","f8f8486d":"We split the data into training and testing set:","87068d76":"Let's see if our model has some bias or variance problem ploting its learning curve:","829463d6":"Let's see how the model (with the best hyperparameters) works on the test data:","e5ee3f45":"It looks like there isn't a big bias or variance problem, but it is clear that our model would work better with more data:. if we can get more labeled data the model performance will increase.","ca0738e1":"We are going to use cross validation and grid search to find good hyperparameters for our SVM model. We need to build a pipeline to don't get features from the validation folds when building each training model.","f3275af9":"We are going to distinguish two cases: tweets with negative sentiment and tweets with non-negative sentiment","bef360b5":"## Machine Learning Model","d6bec9b2":"## Data loading and cleaning","238b4bf2":"This code gives us 0.8 accruacy without too much cleaning of the data "}}