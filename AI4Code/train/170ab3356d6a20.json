{"cell_type":{"8eb65167":"code","ecd890a9":"code","a9ba6a76":"code","22887fd1":"code","1aaaca8a":"code","71f196c4":"code","6e320fd5":"code","c4e382a9":"code","a8ebeb8d":"code","1fe01a96":"code","1ab56076":"code","8939c112":"code","ae6a76fa":"code","03ca9f8b":"code","0fb90c75":"code","897a6967":"code","dc21611d":"code","f075d10e":"code","d0387246":"code","c29be2a4":"code","425389fa":"code","8b5145dc":"code","574216c4":"code","38637e01":"code","3cfd2a61":"code","807cc74e":"code","c02be6a1":"code","b3bfc238":"code","0b937c0a":"code","d2d2b93c":"code","86e99b94":"code","a2eda091":"code","73da4806":"code","3916ee62":"code","20b8d38e":"markdown","ade3939d":"markdown","3b915d26":"markdown","10892e2e":"markdown","6fe758a2":"markdown","38f534f4":"markdown","382ac19d":"markdown","e41e7bee":"markdown","ac61d3cd":"markdown","f044ad64":"markdown","64b89603":"markdown","cce271df":"markdown","88bbe38c":"markdown","854a37d4":"markdown","37a1e13d":"markdown","2d3cf402":"markdown","7026894e":"markdown","4215acf7":"markdown","513fff46":"markdown","37339b3c":"markdown","9d26040c":"markdown"},"source":{"8eb65167":"import pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom tensorflow.python.client import device_lib\ntf.test.gpu_device_name()","ecd890a9":"# Reading dataframe\ndf = pd.read_csv('..\/input\/consume-complaints-dataset-fo-nlp\/complaints_processed.csv')\ndf.head()","a9ba6a76":"# Renaming columns \ndf = df.rename(columns={'narrative':'tweet' })\n\n# Removing SNo column\ndf.drop(['Unnamed: 0'], axis=1, inplace=True)\ndf.head()","22887fd1":"# Analysing dataframe attributes \nprint('* Size of dataframe: {}\\n'.format(df.shape))\nprint('* Datatype of columns are:\\n {}\\n'.format(df.dtypes))\nprint('* Count of different product categories:\\n {}\\n'.format(df['product'].value_counts()))\nprint('* Number of NaNs among tweets are: {}\\n'.format(df['tweet'].isnull().sum())) ","1aaaca8a":"# Removing NaNs\ndf = df.dropna(subset=['tweet'])\nprint('NaNs are removed from the dataframe. Number of NaNs can be confirmed to be {}. The size of dataframe has reduced to {}'.format(df['tweet'].isnull().sum(), df.shape))","71f196c4":"# Plotting word lenghts of tweets\nword_length = [len(x) for x in df['tweet']]\nplt.plot(word_length)","6e320fd5":"# Converting sentences to string\ndf['tweet'] = df['tweet'].astype(str)","c4e382a9":"# Types of products\ndf['product'].value_counts()","a8ebeb8d":"# Plotting product value counts\ndf.groupby('product').count().plot(kind='bar')\nplt.show()","1fe01a96":"# Importing train test splilt library \nfrom sklearn.model_selection import train_test_split\n\n# Train-Test Splitting\ntrain_data, test_data = train_test_split(df, test_size = 0.20)","1ab56076":"# Train and test data dimensions\ntrain_data.shape, test_data.shape","8939c112":"# Balance of train data\ntrain_data.groupby('product').count().plot(kind='bar')\nplt.show()","ae6a76fa":"# Train set value counts \ntrain_data.groupby('product').count()","03ca9f8b":"# Randomly selecting 7000 indices in classes with low value count\nimport numpy as np\nto_add_1 = np.random.choice(train_data[train_data['product']=='credit_card'].index,size = 7000,replace=False)   \nto_add_2 = np.random.choice(train_data[train_data['product']=='debt_collection'].index,size = 7000,replace=False) \nto_add_3 = np.random.choice(train_data[train_data['product']=='mortgages_and_loans'].index,size = 7000,replace=False)  \nto_add_4 = np.random.choice(train_data[train_data['product']=='retail_banking'].index,size=7000,replace=False)\n\n# Indices to be added\nto_add = np.concatenate((to_add_1, to_add_2, to_add_3, to_add_4 ))\nlen(to_add)","0fb90c75":"# Forming a dataframe for randomly selected indices\ndf_replicate = train_data[train_data.index.isin(to_add)]\ndf_replicate  ","897a6967":"# Concatenating replicated df to orinigal df\ntrain_data = pd.concat([train_data, df_replicate])\ntrain_data['product'].value_counts()","dc21611d":"# Importing NLTK Libraries\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import *","f075d10e":"# Declaring function for text preprocessing \n\ndef preprocess_text(main_df):\n  df_1 = main_df.copy()\n\n  # remove stopwords\n  nltk.download('stopwords')         # Downloading stopwords\n  stop = stopwords.words('english')  \n  df_1['tweet'] = df_1['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop)) \n  \n  # remove punctuations and convert to lower case\n  df_1['tweet'] = df_1['tweet'].apply(lambda x: re.sub('[!@#$:).;,?&]', '', x.lower()))\n  \n  # remove double spaces\n  df_1['tweet'] = df_1['tweet'].apply(lambda x: re.sub(' ', ' ', x))\n\n  return df_1  ","d0387246":"# Preprocessing training and test data \ntrain_data = preprocess_text(train_data)\ntest_data = preprocess_text(test_data)","c29be2a4":"# Verifying text preprocessing\ntrain_data['tweet'].head()","425389fa":"# Declaring train labels\ntrain_labels = train_data['product'] \ntest_labels = test_data['product']","8b5145dc":"# Converting labels to numerical features\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(train_labels)\ntrain_labels = le.transform(train_labels)\ntest_labels = le.transform(test_labels)\n\nprint(le.classes_)\nprint(np.unique(train_labels, return_counts=True))\nprint(np.unique(test_labels, return_counts=True))","574216c4":"# Changing labels to categorical features\nimport numpy as np\nfrom tensorflow.python.keras.utils import np_utils\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\n\ntrain_labels = to_categorical(np.asarray(train_labels))\ntest_labels = to_categorical(np.array(test_labels))","38637e01":"from tensorflow.keras.preprocessing.text import Tokenizer\n\n# Defining training parameters\nmax_sequence_length = 170   \nmax_words = 2500   \n\n# Tokenizing tweets\/sentences wrt num_words\ntokenizer = Tokenizer(num_words = max_words)  # Selects most frequent words \ntokenizer.fit_on_texts(train_data.tweet)      # Develops internal vocab based on training text\ntrain_sequences = tokenizer.texts_to_sequences(train_data.tweet)  # converts text to sequence\n\ntest_sequences = tokenizer.texts_to_sequences(test_data.tweet)","3cfd2a61":"# Fixing the sequence length \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ntrain_data = pad_sequences(train_sequences, maxlen = max_sequence_length)\ntest_data = pad_sequences(test_sequences, maxlen = max_sequence_length)\ntrain_data.shape, test_data.shape","807cc74e":"# Model Parameters\nembedding_dim = 32  ","c02be6a1":"# Importing Libraries\n\nimport tensorflow as tf\nimport sys, os, re, csv, codecs, numpy as np, pandas as pd\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\nfrom tensorflow.keras.layers import Dense, Input, Input, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding ","b3bfc238":"embedding_dim = 32  \n\n# Model Training\nmodel = Sequential()\nmodel.add(Embedding(max_words, \n                   embedding_dim,\n                   input_length=max_sequence_length))\n\n# Bidirectional LSTM \nmodel.add(Bidirectional(LSTM(16, return_sequences=True, dropout=0.4, recurrent_dropout=0)))   \n\nmodel.add(GlobalMaxPool1D())\n\nmodel.add(Dense(5,activation='softmax'))  \n\nmodel.summary()","0b937c0a":"model.compile(loss = 'binary_crossentropy', optimizer='RMSProp', metrics = ['accuracy'])   ","d2d2b93c":"# declaring weights of product categories\nclass_weight = {0: 4,          \n                1: 5,    \n                2: 3,      \n                3: 3,     \n                4: 4}      \n\n# training and validating model \nhistory = model.fit(train_data, train_labels, batch_size=48, epochs= 20, class_weight = class_weight, validation_data=(test_data, test_labels)) # best 89(now) or 48 or 60 epochs # default epochs = 23 # batch_size changed to 1 (takes 2.30hrs) from 16","86e99b94":"# Prediction on Test Data\npredicted_bi_lstm = model.predict(test_data)\npredicted_bi_lstm","a2eda091":"import sklearn\nfrom sklearn.metrics import precision_recall_fscore_support as score\nprecision, recall, fscore, support = score(test_labels, predicted_bi_lstm.round())\n\nprint('precision: {}'.format(precision))\nprint('recall: {}'.format(recall))\nprint('fscore: {}'.format(fscore))\nprint('support: {}'.format(support))\nprint('################################')\nprint(sklearn.metrics.classification_report(test_labels, predicted_bi_lstm.round()))","73da4806":"def accuracy_plot(history):\n    \n    fig, ax = plt.subplots(1, 2, figsize=(12,5))\n    \n    fig.suptitle('Model Performance with Epochs', fontsize = 16)\n    # Subplot 1 \n    ax[0].plot(history.history['accuracy'])\n    ax[0].plot(history.history['val_accuracy'])\n    ax[0].set_title('Model Accuracy', fontsize = 14)\n    ax[0].set_xlabel('Epochs', fontsize = 12)\n    ax[0].set_ylabel('Accuracy', fontsize = 12)\n    ax[0].legend(['train', 'validation'], loc='best')\n    \n    # Subplot 2\n    ax[1].plot(history.history['loss'])\n    ax[1].plot(history.history['val_loss'])\n    ax[1].set_title('Model Loss', fontsize = 14)\n    ax[1].set_xlabel('Epochs', fontsize = 12)\n    ax[1].set_ylabel('Loss', fontsize = 12)\n    ax[1].legend(['train', 'validation'], loc='best')\n    \n    \naccuracy_plot(history)","3916ee62":"# Declaring function for plotting confusion matrix\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_cm(model, test_data, test_labels):\n    \n    products = ['Credit Card', 'Credit Reporting', 'Debt Collection', 'Mortgages \\nand Loans', 'Retail Banking']\n        \n    # Calculate predictions\n    pred = model.predict(test_data)\n    \n    # Declaring confusion matrix\n    cm = confusion_matrix(np.argmax(np.array(test_labels),axis=1), np.argmax(pred, axis=1))\n    \n    # Heat map labels\n\n    group_counts = ['{0:0.0f}'.format(value) for value in cm.flatten()]\n    group_percentages = ['{0:.2%}'.format(value) for value in cm.flatten()\/np.sum(cm)]\n    \n    labels = [f\"{v2}\\n{v3}\" for v2, v3 in zip(group_counts, group_percentages)]\n    labels = np.asarray(labels).reshape(5,5)\n\n    # Plotting confusion matrix\n    plt.figure(figsize=(12,8))\n    \n    sns.heatmap(cm, cmap=plt.cm.Blues, annot=labels, annot_kws={\"size\": 15}, fmt = '',\n                xticklabels = products,\n                yticklabels = products)\n    \n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12, rotation = 'horizontal')\n    plt.title('Confusion Matrix\\n', fontsize=19)\n    plt.xlabel('Predicted Labels', fontsize=17)\n    plt.ylabel('Actual Labels', fontsize=17)\n    \nplot_cm(model, test_data, test_labels)","20b8d38e":"## # 7.2 Passing Data Through Network","ade3939d":"# 9. References\n\n1. NLP Implementation: https:\/\/www.kaggle.com\/the0electronic0guy\/nlp-with-disaster-tweets\n\n2. NLP Book: Kulkarni, Akshay, and Adarsha Shivananda. Natural language processing recipes. Apress, 2019.\n\n3. LSTM: https:\/\/www.kaggle.com\/kritanjalijain\/twitter-sentiment-analysis-lstm\n\n4. Bi-LSTM: https:\/\/www.kaggle.com\/kritanjalijain\/twitter-sentiment-analysis-lstm-2#Bidirectional-LSTM-Using-NN \n\n5. Bi-LSTM: https:\/\/www.kaggle.com\/eashish\/bidirectional-gru-with-convolution\n\n6. Bi-LSTM: https:\/\/www.kaggle.com\/victorbnnt\/classification-using-lstm-85-accuracy\n\n7. Imbalanced Datasets: https:\/\/towardsdatascience.com\/yet-another-twitter-sentiment-analysis-part-1-tackling-class-imbalance-4d7a7f717d44\n\n8. Multiclass Classification: https:\/\/towardsdatascience.com\/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a","3b915d26":"# 2. Train-Test Splitting","10892e2e":"Model <b>accuracy<\/b> is verified with confusion matrix. ","6fe758a2":"* credit_card, debt_collection, mortgages_and_loans, retail_banking columns consist of <b>very few values<\/b>. So, the values in these columns will be increased using <b>random oversampling<\/b>.\n* Oversampling is done in train set because this will prevent <b>data leakage<\/b> to test set.  ","38f534f4":"## #8.1Model Performance Attributes","382ac19d":"# 4. Text Preprocessing","e41e7bee":"# 5. Label Encoding","ac61d3cd":"# 8. Model Evaluation","f044ad64":"Value counts of minority classes have <b>increased<\/b>. ","64b89603":"# 6. Tokenizing Sentences and Fixing Sentence Length","cce271df":"Dataframe is <b>imbalanced<\/b>. Improving the balance of the dataframe can improve <b>accuracy<\/b>.","88bbe38c":"## #8.3 Confusion Matrix","854a37d4":"## <u>Introduction<\/u>\n\nThe dataframe consists of <b>162421<\/b> complaints for <b>5 products<\/b>. The dataframe is considerably <b>imbalanced<\/b>. \n\nThe notebook implements <b>Bi-LSTM<\/b> model for <b>multiclass<\/b> classification of product complaints. Based on the provided <b>complaints<\/b>, the goal of the notebook is to accurately <b>identify the product<\/b>.    ","37a1e13d":"# 3. Random Oversampling","2d3cf402":"# 7. Bi-LSTM Model","7026894e":"## <u>Table of Contents<\/u>\n*  [1. Reading and Analyzing Dataframe](#1)\n*  [2. Train-Test Splitting](#2)\n*  [3. Random Oversampling](#3)\n*  [4. Text Preprocessing](#4)\n*  [5. Label Encoding](#5)\n*  [6. Tokenizing Sentences and Fixing Sentence Length](#6)\n*  [7. Bi-LSTM Model](#7)\n*  [8. Model Evaluation](#8)\n*  [9. References](#9)","4215acf7":"## # 7.1 Declaring Model","513fff46":"## #8.2 Model Performance with Epochs\n","37339b3c":"# 1. Reading and Analysing DataFrame","9d26040c":"# Libraries"}}