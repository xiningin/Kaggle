{"cell_type":{"96709b12":"code","2e69a708":"code","944ca509":"code","b91f8378":"code","7778a8d3":"code","fad2a91c":"code","a6952a9c":"code","fad6a989":"code","7f1f720b":"code","8faa15cc":"code","66108b88":"code","2e28ae33":"code","238b858d":"code","33149c1c":"code","c96b7b5e":"code","7a1ef899":"code","89d1db21":"code","0583e3db":"code","199f1bda":"code","96edbdf1":"code","3a8faa8b":"code","2f8866fc":"code","579170e9":"code","b9d5bde0":"code","403ced6f":"code","dbf211f3":"code","79d6e749":"code","4df869b7":"code","bcf56a46":"code","4d902d38":"code","cbbd7b8f":"code","1f277d0c":"code","dd16ce87":"code","14edec90":"code","3b1cc860":"code","0edb89c5":"code","35d6a95b":"code","6f4b239f":"code","9c698522":"code","19cbc020":"code","610080af":"markdown","a865a4fa":"markdown","c552a506":"markdown","e748b4e2":"markdown","61be658f":"markdown","34f8df17":"markdown","0ee313e1":"markdown","cb54f443":"markdown","1808a7e6":"markdown","dfda4256":"markdown","ccc0c7fe":"markdown","e973eb0f":"markdown","0bc5e038":"markdown","80f12c75":"markdown","4736b15b":"markdown","ccbe6682":"markdown","6b7f0dd6":"markdown","9401023d":"markdown","cae45c62":"markdown","e29a38d7":"markdown","4140691b":"markdown","6547092d":"markdown","25d0b697":"markdown","3ffdbd31":"markdown","f7c4ba07":"markdown"},"source":{"96709b12":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction import FeatureHasher\nfrom category_encoders import LeaveOneOutEncoder\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler, LabelEncoder, OrdinalEncoder\nfrom sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold, cross_val_score, RandomizedSearchCV\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, LogisticRegressionCV\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom mlxtend.classifier import StackingCVClassifier\nimport copy","2e69a708":"train = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/train.csv\", index_col=['id'])\ntest = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/test.csv\", index_col=['id'])","944ca509":"train.dtypes","b91f8378":"display(train.head())","7778a8d3":"X = train.drop(\"target\", axis = 1)\ny = train.loc[:,\"target\"]","fad2a91c":"X.bin_3 = X.bin_3.apply(lambda x: 1 if x == \"T\" else 0)\nX.bin_4 = X.bin_4.apply(lambda x: 1 if x == \"Y\" else 0)\n\nprint(X.columns)","a6952a9c":"# h = FeatureHasher(input_type='string', n_features=1000)\n# X[['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']].values\n# hash_X = h.fit_transform(X[['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']].values)\n# hash_X = pd.DataFrame(hash_X.toarray())\n\n# hash_X.columns\n# X = X.drop([\"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\"], axis=1).join(hash_X)\n\nloo_encoder = LeaveOneOutEncoder(cols=[\"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\"])\nloo_X = loo_encoder.fit_transform(X[[\"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\"]], y)\nX = X.drop([\"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\"], axis=1).join(loo_X)\n\nX = X.drop([\"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\", \"nom_4\"], axis=1) \\\n        .join(pd.get_dummies(X[[\"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\", \"nom_4\"]]))\n\nprint(X.columns)","fad6a989":"X.ord_1.replace(to_replace = ['Novice', 'Contributor','Expert', 'Master', 'Grandmaster'],\n                         value = [0, 1, 2, 3, 4], inplace = True)\nX.ord_2.replace(to_replace = ['Freezing', 'Cold', 'Warm', 'Hot','Boiling Hot', 'Lava Hot'],\n                         value = [0, 1, 2, 3, 4, 5], inplace = True)\n\nfor i in [\"ord_3\", \"ord_4\"]:\n   le = LabelEncoder()\n   X[[i]] = le.fit_transform(X[[i]])\n\noe = OrdinalEncoder(categories='auto')\nX.ord_5 = oe.fit_transform(X.ord_5.values.reshape(-1,1))\n\nprint(X.columns)","7f1f720b":"def date_cyc_enc(df, col, max_vals):\n   df[col + '_sin'] = np.sin(2 * np.pi * df[col]\/max_vals)\n   df[col + '_cos'] = np.cos(2 * np.pi * df[col]\/max_vals)\n   return df\n\nX = date_cyc_enc(X, 'day', 7)\nX = date_cyc_enc(X, 'month', 12)\nX.drop(['day', 'month'], axis=1, inplace = True)\n\nprint(X.columns)","8faa15cc":"# lr = LogisticRegression()\n# scores_lr = cross_val_score(lr, X, y, cv=5, n_jobs=-1)\n# print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_lr.mean(), scores_lr.std() * 2))","66108b88":"# rc = RidgeClassifier()\n# scores_rc = cross_val_score(rc, X, y, cv=5, n_jobs=-1)\n# print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_rc.mean(), scores_rc.std() * 2))","2e28ae33":"# lda = LinearDiscriminantAnalysis()\n# scores_lda = cross_val_score(lda, X_new, y, cv=5, n_jobs=1)\n# print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_lda.mean(), scores_lda.std() * 2))","238b858d":"# linear_svm = LinearSVC(penalty=\"l2\")\n# scores_linear_svm = cross_val_score(linear_svm, X, y, cv=5, n_jobs=-1)\n# print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_linear_svm.mean(), scores_linear_svm.std() * 2))","33149c1c":"# fr = DecisionTreeClassifier(random_state=0)\n# scores_dt = cross_val_score(fr, X, y, cv=5, n_jobs=2)\n# print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_dt.mean(), scores_dt.std() * 2))","c96b7b5e":"# sgdc = SGDClassifier()\n# scores_sgdc = cross_val_score(sgdc, X, y, cv=5, n_jobs=4)\n# print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_sgdc.mean(), scores_sgdc.std() * 2))","7a1ef899":"# ab = AdaBoostClassifier()\n# scores_ab= cross_val_score(ab, X, y, cv=5, n_jobs=-1)\n# print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_ab.mean(), scores_ab.std() * 2))","89d1db21":"# gbm = GradientBoostingClassifier()\n# scores_gbm= cross_val_score(gbm, X, y, cv=5, n_jobs=-1)\n# print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_gbm.mean(), scores_gbm.std() * 2))","0583e3db":"# rf = RandomForestClassifier()\n# scores_rf= cross_val_score(rf, X, y, cv=5, n_jobs=-1)\n# print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_rf.mean(), scores_rf.std() * 2))","199f1bda":"# et = ExtraTreesClassifier()\n# scores_et= cross_val_score(et, X, y, cv=5, n_jobs=-1)\n# print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_et.mean(), scores_et.std() * 2))","96edbdf1":"# xgb = XGBClassifier()\n# scores_xgb= cross_val_score(xgb, X, y, cv=5, n_jobs=-1)\n# print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_xgb.mean(), scores_xgb.std() * 2))","3a8faa8b":"# params = {\n#         'min_child_weight': [1, 5, 10, 13, 15],\n#         'gamma': [0.5, 1, 1.5, 2, 5],\n#         'subsample': [0.2, 0.4, 0.6, 0.8, 1.0],\n#         'colsample_bytree': [0.6, 0.8, 1.0],\n#         'max_depth': [3, 4, 5, 10, 20]\n#         }\n# xgb = XGBClassifier(silent=True, nthread=1)\n# folds = 3\n# param_comb = 5\n\n# skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\n# random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, \n#                                    scoring='accuracy', n_jobs=-1, cv=skf.split(X, y), \n#                                    verbose=3, random_state=1001 )\n# random_search.fit(X, y)\n\n# print('\\n All results:')\n# print(random_search.cv_results_)\n# print('\\n Best estimator:')\n# print(random_search.best_estimator_)\n# print('\\n Best hyperparameters:')\n# print(random_search.best_params_)","2f8866fc":"# means = random_search.cv_results_['mean_test_score']\n# stds = random_search.cv_results_['std_test_score']\n# for mean, std, params in zip(means, stds, random_search.cv_results_['params']):\n#     print(\"%0.3f (+\/-%0.03f) for %r\"% (mean, std * 2, params))","579170e9":"# params2 = {\n#         'n_estimators': [50, 100, 300, 800],\n#         'learning_rate': [0.01, 0.1, 0.5, 1],\n#         'max_depth': [3, 10, 20, 50],\n#         'min_samples_split': [100, 200, 500, 800],\n#         'subsample': [0.2, 0.4, 0.6, 0.8, 1.0]\n#         }\n# gbm = GradientBoostingClassifier(random_state=1001)\n\n# random_search2 = RandomizedSearchCV(gbm, param_distributions=params2, n_iter=param_comb, \n#                                    scoring='accuracy', n_jobs=-1, cv=skf.split(X, y), \n#                                    verbose=3, random_state=1001 )\n# random_search2.fit(X, y)\n\n# print('\\n All results:')\n# print(random_search2.cv_results_)\n# print('\\n Best estimator:')\n# print(random_search2.best_estimator_)\n# print('\\n Best hyperparameters:')\n# print(random_search2.best_params_)","b9d5bde0":"# means = random_search2.cv_results_['mean_test_score']\n# stds = random_search2.cv_results_['std_test_score']\n# for mean, std, params in zip(means, stds, random_search2.cv_results_['params']):\n#     print(\"%0.3f (+\/-%0.03f) for %r\"% (mean, std * 2, params))","403ced6f":"# lr_params = {'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'], \n#              'C': [0.01, 0.1, 0.5, 1]\n#             }\n# lr = LogisticRegression(random_state=1001)\n\n# random_search3 = RandomizedSearchCV(lr, param_distributions=lr_params, n_iter=param_comb, \n#                                    scoring='accuracy', n_jobs=-1, cv=skf.split(X, y), \n#                                    verbose=3, random_state=1001 )\n# random_search3.fit(X, y)\n\n# print('\\n All results:')\n# print(random_search3.cv_results_)\n# print('\\n Best estimator:')\n# print(random_search3.best_estimator_)\n# print('\\n Best hyperparameters:')\n# print(random_search3.best_params_)","dbf211f3":"# means = random_search3.cv_results_['mean_test_score']\n# stds = random_search3.cv_results_['std_test_score']\n# for mean, std, params in zip(means, stds, random_search3.cv_results_['params']):\n#     print(\"%0.3f (+\/-%0.03f) for %r\"% (mean, std * 2, params))","79d6e749":"# xgb_clf = XGBClassifier(booster='gbtree', gamma=5, colsample_bytree=0.8,\n#                         learning_rate=0.1, max_depth=10, \n#                         min_child_weight=10, n_estimators=100, \n#                         silent=True, subsample=0.8)\n\n# ab_clf = AdaBoostClassifier(n_estimators=200,\n#                             base_estimator=DecisionTreeClassifier(\n#                                 min_samples_leaf=2,\n#                                 random_state=1001),\n#                             random_state=1001)\n\n# gbm_clf = GradientBoostingClassifier(n_estimators=300, min_samples_split=100,\n#                                  max_depth=50, learning_rate=1, subsample=0.8,\n#                                  random_state=1001)\n\n# lr = LogisticRegression()\n\n# stack = StackingCVClassifier(classifiers=[xgb_clf, gbm_clf, ab_clf], \n#                             meta_classifier=lr,\n#                             cv=5,\n#                             stratify=True,\n#                             shuffle=True,\n#                             use_probas=True,\n#                             use_features_in_secondary=True,\n#                             verbose=1,\n#                             random_state=1001,\n#                             n_jobs=-1)\n# stack = stack.fit(X, y)","4df869b7":"X_train = train.drop(\"target\", axis = 1)\ny_train = train.loc[:,\"target\"]","bcf56a46":"X_train.bin_3 = X_train.bin_3.apply(lambda x: 1 if x == \"T\" else 0)\nX_train.bin_4 = X_train.bin_4.apply(lambda x: 1 if x == \"Y\" else 0)\n\nX_train.ord_1.replace(to_replace = ['Novice', 'Contributor','Expert', 'Master', 'Grandmaster'],\n                         value = [0, 1, 2, 3, 4], inplace = True)\nX_train.ord_2.replace(to_replace = ['Freezing', 'Cold', 'Warm', 'Hot','Boiling Hot', 'Lava Hot'],\n                         value = [0, 1, 2, 3, 4, 5], inplace = True)\n\nfor i in [\"ord_3\", \"ord_4\"]:\n   le = LabelEncoder()\n   X_train[[i]] = le.fit_transform(X_train[[i]])\n\noe = OrdinalEncoder(categories='auto')\nX_train.ord_5 = oe.fit_transform(X_train.ord_5.values.reshape(-1,1))","4d902d38":"X_test = copy.deepcopy(test)","cbbd7b8f":"X_test.bin_3 = X_test.bin_3.apply(lambda x: 1 if x == \"T\" else 0)\nX_test.bin_4 = X_test.bin_4.apply(lambda x: 1 if x == \"Y\" else 0)\n\nX_test.ord_1.replace(to_replace = ['Novice', 'Contributor','Expert', 'Master', 'Grandmaster'],\n                         value = [0, 1, 2, 3, 4], inplace = True)\nX_test.ord_2.replace(to_replace = ['Freezing', 'Cold', 'Warm', 'Hot','Boiling Hot', 'Lava Hot'],\n                         value = [0, 1, 2, 3, 4, 5], inplace = True)\n\nfor i in [\"ord_3\", \"ord_4\"]:\n    le = LabelEncoder()\n    X_test[[i]] = le.fit_transform(X_test[[i]])\n\noe = OrdinalEncoder(categories='auto')\nX_test.ord_5 = oe.fit_transform(X_test.ord_5.values.reshape(-1,1))","1f277d0c":"data = pd.concat([X_train, X_test])\nprint(data.shape)","dd16ce87":"columns = data.columns\ndummies = pd.get_dummies(data,\n                         columns=columns,\n#                          drop_first=True,\n                         sparse=True)","14edec90":"print(dummies.shape)\nprint(X_train.shape[0])","3b1cc860":"X_train = dummies.iloc[:X_train.shape[0], :]\nX_test = dummies.iloc[X_train.shape[0]:, :]","0edb89c5":"del dummies\ndel data\nprint (X_train.shape)\nprint(X_test.shape)","35d6a95b":"X_train = X_train.sparse.to_coo().tocsr()\nX_test = X_test.sparse.to_coo().tocsr()\n\nX_train = X_train.astype(\"float32\")\nX_test = X_test.astype(\"float32\")","6f4b239f":"lr = LogisticRegression()\nlr.fit(X_train, y_train)\npred = lr.predict_proba(X_test)\npred[:10,1]","9c698522":"# lr = LogisticRegression(solver=\"lbfgs\", C=0.1, max_iter=10000)\n# lr.fit(X_train, y_train)\n# pred2 = lr.predict_proba(X_test)\n# pred2[:10,1]","19cbc020":"predictions = pd.Series(pred[:,1], index=test.index, dtype=y.dtype)\npredictions.to_csv(\".\/submission.csv\", header=['target'], index=True, index_label='id')","610080af":"### Predictions","a865a4fa":"### Reference\n\n1. [Handling Categorical Variables:Encoding & Modeling](https:\/\/www.kaggle.com\/vikassingh1996\/handling-categorical-variables-encoding-modeling)\n2. [An Overview of Encoding Techniques](https:\/\/www.kaggle.com\/shahules\/an-overview-of-encoding-techniques\/notebook)\n3. [Categorical Data encoding techniques](https:\/\/www.kaggle.com\/ruchibahl18\/categorical-data-encoding-techniques)\n4. [Category Encoders Examples](https:\/\/www.kaggle.com\/discdiver\/category-encoders-examples)\n5. [Entity embeddings to handle categories](https:\/\/www.kaggle.com\/abhishek\/entity-embeddings-to-handle-categories)\n6. [Why Not Logistic Regression?](https:\/\/www.kaggle.com\/peterhurford\/why-not-logistic-regression)","c552a506":"LogisticRegression, RidgeClassifier and LinearDiscriminantAnalysis revel better accuracy than LinearSVC.","e748b4e2":"### Backgroud","61be658f":"#### Try it on classification model","34f8df17":"#### Timeseries data","0ee313e1":"### Encoding for catagory ","cb54f443":"#### Grid Search for best params of XGBClassifier model","1808a7e6":"I pick up some methods from excellent notebooks that have published in this kaggle competition, and then try it as the following.\n\n* To binary data, I will do logical judgement.\n* To low ordial data, I will try LabelEncoder or simple replacement. But to high ordial data, I maybe use OrdinalEncoder.\n* To low nominal data, dummy variable is a common method. But to high nonmial data, I maybe use LeaveOneOutEncoder or FeatureHasher","dfda4256":"In fact, as above analysis, I found the stack model in test data is very bad !\n\nAt least, I found if I use LogisticRegression model only or other linear model, the result is better. **For example, if I preprocess all features as above steps and perform LR model with default parameters, the score of this competition is 0.80266**\n\nSo I decided to preprocess the train data again with one-hot-encoding, and build model of LR. The result is shown below.","ccc0c7fe":"* **Binary data**: A binary variable is a variable with only two values, like 1\/0, such as bin_0,bin_1,bin_2.\n* **Categorical data** \n    * **Ordinal data**: An ordinal variable is a categorical variable with a ordering. For low ordinal features, like ord_1, ord_2, ord_3, ord_4. For high ordinal data, like ord_5.\n    * **Nominal data**: Nominal variables contain two or more categories without a natural ordering. For low nominal features, like nom_0, nom_1, nom_2, nom_3, nom_4. For high-cardinality nominal features, like nom_5, nom_6, nom_7, nom_8, nom_9.\n* **Timeseries data**: Time series data, like day or moth, it seems to be a cyclical continuous features.","e973eb0f":"#### Ordinal data","0bc5e038":"#### Encoding for train\/test data","80f12c75":"### Stacked model","4736b15b":"#### Nominal data","ccbe6682":"We can see that GradientBoostingClassifier, XGBClassifier maybe have the best accuracy in above methods, which the accuracy of AdaBoostClassifier is also close to. Otherwise, others maybe performent not good.","6b7f0dd6":"#### Export predictions and submission","9401023d":"#### Binary data","cae45c62":">**In fact, as above preprocess, I first transform some features with some normal methods ,and then I preprocess all features with get_dummies function, the score of competition drops from 0.80266 to 0.80184.**\n**However, if I adjust LR model with tuning parameters, the score will be upgrade to 0.80801.**\n**But I think these three scores are keeping in same level, so those scores can not indicate that which preprocess or encoding way is better.**","e29a38d7":"### Which Encoding methods is suitable to deal with above categorical features?","4140691b":"### Model refinement\n\n#### Try it on linear model","6547092d":"#### Grid Search for best params of LogisticRegression model","25d0b697":"### Import packages and data","3ffdbd31":"#### Grid Search for best params of GradientBoostingClassifier model","f7c4ba07":"#### Making predictions"}}