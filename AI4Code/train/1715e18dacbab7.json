{"cell_type":{"03b76a8e":"code","e60fccf1":"code","6901ce77":"code","30c0b0e8":"code","a80ce059":"code","38297622":"code","81a9d884":"code","3349a51b":"code","211f71ba":"code","496e95c0":"code","6d891d63":"code","c63954a2":"code","425e6aeb":"code","5f60c762":"code","2a8ce3dc":"markdown","04d95315":"markdown","5507edfc":"markdown","316b749c":"markdown","f8130c3e":"markdown","079e325e":"markdown"},"source":{"03b76a8e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns #visualization\nimport matplotlib.pyplot as plt #visualization\n%matplotlib inline\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\nplt.style.use('fivethirtyeight')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e60fccf1":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf = pd.read_csv('..\/input\/cusersmarildownloadswinecsv\/wine.csv', delimiter=';', encoding = \"utf8\", nrows = nRowsRead)\ndf.dataframeName = 'wine.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf.head()","6901ce77":"# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)","30c0b0e8":"def apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings","a80ce059":"def plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs","38297622":"def make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef score_dataset(X, y, model=XGBRegressor()):\n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\", \"object\"]):\n        X[colname], _ = X[colname].factorize()\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    score = cross_val_score(\n        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score","81a9d884":"features = [\n    \"citric_acid\",\n    \"total_sulfur_dioxide\",\n    \"pH\",\n    \"sulphates\",\n]\n\nprint(\"Correlation with Quality:\\n\")\nprint(df[features].corrwith(df.quality))","3349a51b":"X = df.copy()\ny = X.pop(\"quality\")\nX = X.loc[:, features]\n\n\npca, X_pca, loadings = apply_pca(X)\nprint(loadings)","211f71ba":"X = df.copy()\ny = X.pop(\"quality\")\n\n\nX[\"Feature1\"] = X.citric_acid + X.total_sulfur_dioxide\nX[\"Feature2\"] = X.pH * X.sulphates\n\nscore = score_dataset(X, y)\nprint(f\"Your score: {score:.5f} RMSLE\")","496e95c0":"X = df.copy()\ny = X.pop(\"quality\")\n\n\nX = X.join(X_pca)\n\nscore = score_dataset(X, y)\nprint(f\"Your score: {score:.5f} RMSLE\")","6d891d63":"#Do Not change anything in this snippet.\n\nsns.catplot(\n    y=\"value\",\n    col=\"variable\",\n    data=X_pca.melt(),\n    kind='boxen',\n    sharey=False,\n    col_wrap=2,\n);","c63954a2":"# You can change PC1 to PC2, PC3, or PC4\ncomponent = \"PC1\"\n\nidx = X_pca[component].sort_values(ascending=False).index\ndf.loc[idx, [\"quality\", \"fixed_acidity\", \"alcohol\"] + features]","425e6aeb":"# You can change PC1 to PC2, PC3, or PC4\n#I changed to PCA4\n\ncomponent = \"PC4\"\n\nidx = X_pca[component].sort_values(ascending=False).index\ndf.loc[idx, [\"quality\", \"fixed_acidity\", \"alcohol\"] + features]","5f60c762":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Be patient. Mar\u00edlia Prata, @mpwolke Was here' )","2a8ce3dc":"#There are several points lying at the extreme ends of the distributions -- outliers, that is.","04d95315":"#Trust on PCA to untangle the correlational structure of these features and suggest relationships that might be usefully modeled with new features.","5507edfc":"#Solution 2: Use components","316b749c":"#This notebook is an exercise in the Feature Engineering course by Ryan Holbrook. ","f8130c3e":"![](https:\/\/images.slideplayer.com\/12\/3387910\/slides\/slide_36.jpg)slideplayer.com","079e325e":"#Create New Features\n\nAdd one or more new features to the dataset X. For a correct solution, get a validation score below 0.140 RMSLE.\n\n#Solution 1: Inspired by loadings (Add new features)"}}