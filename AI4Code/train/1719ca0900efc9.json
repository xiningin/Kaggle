{"cell_type":{"a4078027":"code","708c7864":"code","8ed47495":"code","cf0a6eaf":"code","f89defa5":"code","26e78ca7":"code","857dfa82":"code","3d567bb3":"code","202894b9":"code","244083f1":"code","0f890379":"code","75718462":"code","77b2339b":"code","cd0af980":"code","7271723c":"code","aef18417":"code","ea71f264":"code","f6a54cb9":"code","f114d1cf":"code","457ebc4f":"code","bda55c31":"code","330f4bbd":"code","38f06175":"code","1a31251d":"code","d7efde9c":"code","1a8aac16":"code","69019570":"code","6088d7a7":"code","b93684cd":"code","0c4c1675":"code","31ce73ef":"code","b613a503":"code","d3551fda":"code","e60db725":"code","90c93e99":"code","898493cc":"code","75b13ff8":"code","4a9e49ba":"code","603b7cde":"code","759a597d":"code","7435ac99":"code","3c2f2124":"code","bd453f8c":"markdown","e34a875a":"markdown","060411cc":"markdown","2248958b":"markdown","a0153d36":"markdown","31d83933":"markdown","b5d27fcf":"markdown","612c4d33":"markdown","b393af3f":"markdown","6f361fb4":"markdown","d865f913":"markdown","13808697":"markdown","dc3b62b2":"markdown","e683fb85":"markdown","26d9f6a6":"markdown","2fd801c2":"markdown","190378d3":"markdown","99ca9694":"markdown","8ddb7788":"markdown","cebccfe8":"markdown","bfc87c3d":"markdown","9f66596b":"markdown","b8daf55d":"markdown","1402e03c":"markdown","24fddee0":"markdown","57057fa6":"markdown","e7059b42":"markdown","7fa560c0":"markdown","6526af27":"markdown","3a8def5a":"markdown","3a1e5720":"markdown","5abc0c60":"markdown","bf5ddeb7":"markdown","12427c61":"markdown","16166235":"markdown","bdb4df43":"markdown","26961b3d":"markdown","00c52202":"markdown","df524a80":"markdown","10fe06c8":"markdown"},"source":{"a4078027":"'''\nnumpy library\npandas library\nmatplot library\nplotly library\nwordcloud library\nnltk library\nPIL library\nseaborn library\nre library\nscikit learn library\n'''","708c7864":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.colors import n_colors\nfrom plotly.subplots import make_subplots\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport nltk\nnltk.download('stopwords')\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n#stop=set(stopwords.words('english'))\n\nfrom PIL import Image\nimport seaborn as sns\nimport re\nfrom sklearn.datasets import make_blobs\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nimport chardet\n\n# tried building a nlp model for sentiments, but it was a time killer as i had prior exposure only on ANN and CNN \n#So made use of prebuild nlp model that predicts sentiments\n\n'''\n#from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom collections import Counter\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences'''","8ed47495":"filepath = r'..\/input\/covid19-tweets\/covid19_tweets.csv'\ninfo_path = r'..\/input\/covid-19-tweet-supporting-files\/columns.txt'\n#sentiment_filepath = r'C:\/Users\/riaz\/learning\/user_sentiment.csv'\ndf = pd.read_csv(filepath)\n#sentiment = pd.read_csv(sentiment_filepath,encoding = 'latin',header=None,names=['target','id','time','query','usr','text'])\ninfo = pd.read_csv(info_path,delimiter='->',names=['title','description'])\n","cf0a6eaf":"plt.figure(figsize=(5,3))\nsns.heatmap(df.corr(),annot=True,linecolor='white',linewidths=0)","f89defa5":"def unique_features(data_frame):\n    unique_dataframe = pd.DataFrame()\n    unique_dataframe['features'] = data_frame.columns\n    uniques = []\n    for col in data_frame.columns:\n        u = data_frame[col].nunique()\n        uniques.append(u)\n    unique_dataframe['uniques'] = uniques\n    return unique_dataframe","26e78ca7":"features = unique_features(df)\n#print(features)\n\nplt.figure(figsize=(10,7))\nfeatures = features.sort_values(by='uniques',ascending=False)\nsns.barplot(x='uniques',y='features',data=features,palette='Dark2')","857dfa82":"df['user_created'] = pd.to_datetime(df['user_created'])\ndf['only_date'] = pd.to_datetime(df['user_created']).dt.date\ndf['created_year'] = df['user_created'].apply(lambda date : date.year)\ndf['created_month'] = df['user_created'].apply(lambda date : date.month)\ndf['created_day'] = df['user_created'].apply(lambda date : date.day)","3d567bb3":"data = df.isnull().sum().sort_values(ascending=False)[:4]\nplt.figure(figsize=(3,3))\ndata.plot(kind='barh',grid=False,sort_columns=True,title='total_missing_values')","202894b9":"df['user_location'].fillna('Unknown', inplace=True)\ndf['user_description'].fillna('Unknown', inplace=True)\ndf['source'].fillna('Unknown', inplace=True)\ndf['hashtags'].fillna('None', inplace=True)","244083f1":"#splitting a string into 2 words, where the first word corresponds to city name and second word to the country name\n#this acts only if the given str has a ',' in it else it would return the same\ndf['country'] = df['user_location'].apply(lambda x: x.split(\",\")[-1].strip() if (\",\" in x) else x)\ndf['city'] = df['user_location'].apply(lambda x: x.split(\",\")[0].strip() if (\",\" in x) else x)\n\n#replacing the two digit US city names with USA except UK and EU\ndf['country'] = df['country'].apply(lambda x: 'USA' if (len(x.lower().strip())<3) and ((x!='uk')|(x!='eu')) else x)\n#replacing lower case country names with standard ones\ndf['country'] = df['country'].apply(lambda x: 'USA' if x.lower().strip() in (\"united states,Alabama,\u00a0Alaska,\u00a0American Samoa,\u00a0Arizona,\u00a0Arkansas,\u00a0California,\u00a0Colorado,\u00a0Connecticut,\u00a0Delaware,\u00a0District of Columbia,\u00a0Florida,\u00a0Georgia,\u00a0Guam,\u00a0Hawaii,\u00a0Idaho,\u00a0Illinois,\u00a0Indiana,\u00a0Iowa,\u00a0Kansas,\u00a0Kentucky,\u00a0Louisiana,\u00a0Maine,\u00a0Maryland,\u00a0Massachusetts,\u00a0Michigan,\u00a0Minnesota,\u00a0Mississippi,\u00a0Missouri,\u00a0Montana,\u00a0Nebraska,\u00a0Nevada,\u00a0New Hampshire,\u00a0New Jersey,\u00a0New Mexico,\u00a0New York,\u00a0North Carolina,\u00a0North Dakota,\u00a0Northern Mariana Islands,\u00a0Ohio,\u00a0Oklahoma,\u00a0Oregon,\u00a0Pennsylvania,\u00a0Puerto Rico,\u00a0Rhode Island,\u00a0South Carolina,\u00a0South Dakota,\u00a0Tennessee,\u00a0Texas,\u00a0U.S. Virgin Islands,\u00a0Utah,\u00a0Vermont,\u00a0Virginia,\u00a0Washington,\u00a0West Virginia,\u00a0Wisconsin,\u00a0Wyoming california, texas, usa,new york, us\") else x)\ndf['country'] = df['country'].apply(lambda x: 'Canada' if x.lower().strip() in (\"ontario,toronto,quebec,montreal,quebec city,vancouver\") else x)\ndf['country'] = df['country'].apply(lambda x: 'UK' if x.lower().strip() in (\"united kingdom,london, england,uk, britain,great britain\") else x)\ndf['country'] = df['country'].apply(lambda x: 'India' if x.lower().strip() in (\"india,mumbai,tamil nadu,chennai,karnataka,bengaluru,kerala,thiruvanandhipuram,kochi,patna,delhi,new delhi,uttar pradesh,andhra pradesh,telengana,vishakapatinam,hyderabad,himachal pradesh,goa,jammu,jammu and kashmir,ladhak\") else x)\ndf['country'] = df['country'].apply(lambda x: 'N\/A' if x.lower().strip() in (\"worldwide, earth, global\") else x)\n\n\n#if country name is found in city name,\ncountry_list = ['finland','netherlands','ireland','sweden','germany','denmark','switzerland','norway','france','spain','canada','bulgaria','belgium','estonia','uk','luxembourg','newzealand','austria','italy','australia','latvia','cyprus','singapore','japan','northmacedonia','southkorea','moldova','slovakia','romania','portugal','poland','czechrepublic','slovenia','costarica','chile','iceland','lithuania','georgia','hungary','usa','russia','greece','india','malaysia','armenia','southafrica']\n\ndef checker(x):\n    #checking for city names in country list\n    if x['city'].lower().strip() in country_list:\n        #if city name is not same as country name but is in country list\n        if x['city'].lower().strip() != x['country'].lower().strip():\n            return x['city']\n        else:\n            return x['country']\n    else:\n        return x['country']\n\ndf['country'] = df[['city','country']].apply(checker,axis=1)","0f890379":"corona_keys = ['covid19','covid_19','covid','pandemic','covid-19','corona','virus','coronavirus','coronavirusupdates']\n\ndef hashtag_grouper(a):\n    b = a.replace(\"\\'\",\"\").strip().strip(\"[\").strip(\"]\").lower()\n    c = b.split(',')\n    for item in c:\n        if item in corona_keys:\n            return 'Covid 19'\n        else:\n            return item\n\ndf['category'] = df['hashtags'].apply(hashtag_grouper)\ndf['covid'] = df['category'].apply(lambda x: True if x == 'Covid 19' else False)\ncoronadf = df[df['covid']>0]","75718462":"#dataFrame of countries that tweeted general content\nlocation_count = pd.DataFrame(df['country'].value_counts())\nlocation_count.reset_index(inplace=True)\nlocation_count = location_count.set_axis(['country', 'count'], axis=1)\nlocation_count = location_count.sort_values(by='count',ascending=False)\n\n\n#dataFrame of countries that tweeted about covid 19\nlocation_count2 = pd.DataFrame(coronadf['country'].value_counts())\nlocation_count2.reset_index(inplace=True)\nlocation_count2 = location_count2.set_axis(['country', 'cv_count'], axis=1)\nlocation_count2 = location_count2.sort_values(by='cv_count',ascending=False)","77b2339b":"count_graph = px.bar(x='country',y='count',data_frame=location_count[1:16],color='country',\n                    labels={'x':'Countries','y':'Counts'},title='Generalised Tweets')\ncount_graph.show()","cd0af980":"count_graph2 = px.bar(x=location_count2['country'][1:16],y=location_count2['cv_count'][1:16],\n                      color=location_count2['country'][1:16],labels={'x':'Countries','y':'Counts'},\n                      title='Tweets based on corona\/Covid 19')\ncount_graph2.show()","7271723c":"def graph(data, feature, title, pallete):\n    f, ax = plt.subplots(1,1, figsize=(18,6))\n    total = float(len(df))\n    if feature == 'user_location' or 'country' or 'city' or 'user_names':\n        g = sns.countplot(data[feature],hue= data['covid'],order = data[feature].value_counts().index[1:11], palette=pallete)  \n    elif feature == 'source':\n        g = sns.countplot(data[feature],hue= data['covid'],order = data[feature].value_counts().index[:16], palette=pallete)\n    else:\n        g = sns.countplot(data[feature],hue= data['covid'], order = data[feature].value_counts().index[0:11], palette=pallete)\n    g.set_title(\"Number and percentage of {}\".format(title))\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n\n    plt.title('Counts & Percentage representation of {} that were actually tweeting about Corona'.format(feature))\n    plt.ylabel('Frequency', fontsize=12)\n    plt.xlabel(title, fontsize=12)\n    plt.xticks(rotation=90)\n    plt.show()","aef18417":"graph(df[df.created_year > 2019 ], 'user_name', 'User Names','CMRmap')","ea71f264":"graph(df[df.created_year > 2019 ], 'country', 'User Locations', 'gist_rainbow')","f6a54cb9":"graph(df[df.created_year > 2019 ], 'source','Source', 'plasma_r')","f114d1cf":"verified_corona = df[ (df.user_verified == True) & (df.covid == True) & (df.country == ('USA')) | \n             (df.country == 'India') | (df.country == 'Australia') | (df.country =='UK') | (df.country =='Canada') ]  \n\nverified_corona_20 = verified_corona[verified_corona.created_year == 2020]\nverified_corona = verified_corona[verified_corona.created_year == 2019]","457ebc4f":"print('Number of tweets from top 5 countries')\nf, axes = plt.subplots(1, 2,figsize=(16,6))\naxes[0].set_title(\"Year 2019\")\naxes[1].set_title(\"Year 2020\")\norder_type = sorted(verified_corona['created_month'].unique())\nsns.countplot(x=verified_corona.created_month, hue=verified_corona.country,palette=\"Dark2\",order=order_type,ax=axes[0])\norder_type = sorted(verified_corona_20['created_month'].unique())\nsns.countplot(x=verified_corona_20.created_month, hue=verified_corona_20.country,palette=\"Dark2\",order=order_type,ax=axes[1])\n\nf.tight_layout()\nplt.show()\n#'Number of verified profile tweets in the year 2019 vs 2020'","bda55c31":"tweet_from_2019 = df[df['created_year'] == 2019]\ntweet_from_2020 = df[df['created_year'] == 2020]\ntweets_month_19 =  tweet_from_2019['created_month'].value_counts().to_frame().reset_index().rename(columns={'index':'month','created_month':'count'})   \ntweets_month_20 =  tweet_from_2020['created_month'].value_counts().to_frame().reset_index().rename(columns={'index':'month','created_month':'count'})   \ntweets_month_19 = tweets_month_19.sort_values('month',ascending=True)\ntweets_month_20 = tweets_month_20.sort_values('month',ascending=True)\n\nfig=go.Figure()\nfig.add_trace(go.Scatter(x=tweets_month_19['month'], y=tweets_month_19['count'],\n                         mode='markers+lines',marker_color='firebrick',\n                         name='Year 2019',line = dict(color='grey', width=4, dash='dot')))\n\n#fig.update_layout(title_text='Tweets per Month and Date in the year 2019 and 2020 ',template=\"plotly\", title_x=0.5)\nfig.add_trace(go.Scatter(x=tweets_month_20['month'], y=tweets_month_20['count'],\n                         mode='markers+lines',marker_color='darkred',name='Year 2020',\n                        line=dict(color='orange', width=4,dash='longdashdot')))\nfig.show()","330f4bbd":"ds = df['user_name'].value_counts().reset_index()\nds.columns = ['user_name', 'tweets_count']\nds = ds.sort_values(['tweets_count'])\ndf = pd.merge(df, ds, on='user_name')\n\ndata = df.sort_values('user_followers', ascending=False)\ndata = data.drop_duplicates(subset='user_name', keep=\"first\")\ndata = data[['user_name', 'user_followers', 'tweets_count']]\ndata = data.sort_values('user_followers')\nfig = px.bar(data.tail(40), x=\"user_followers\", y=\"user_name\",color='tweets_count',orientation='h', \n             title='Top 40 users by number of followers', width=1000, height=1000,template=\"plotly_dark\")\nfig.show()","38f06175":"def remove_tag(string):\n    text=re.sub('<.*?>','',string)\n    return text\ndef remove_mention(text):\n    line=re.sub(r'@\\w+','',text)\n    return line\ndef remove_hash(text):\n    line=re.sub(r'#\\w+','',text)\n    return line\n\ndef remove_newline(string):\n    text=re.sub('\\n','',string)\n    return text\ndef remove_url(string): \n    text = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',string)\n    return text\ndef remove_number(text):\n    line=re.sub(r'[0-9]+','',text)\n    return line\ndef remove_punct(text):\n    line = re.sub(r'[!\"\\$%&\\'()*+,\\-.\\\/:;=#@?\\[\\\\\\]^_`{|}~]*','',text)\n    #string=\"\".join(line)\n    return line\ndef text_strip(string):\n    line=re.sub('\\s{2,}', ' ', string.strip())\n    return line\ndef remove_thi_amp_ha_words(string):\n    line=re.sub(r'\\bamp\\b|\\bthi\\b|\\bha\\b',' ',string)\n    return line","1a31251d":"df['clean_tweet'] =  df['text'].str.lower()\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_tag(str(x)))\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_mention(str(x)))\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_hash(str(x)))\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_newline(x))\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_url(x))\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_number(x))\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_punct(x))\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_thi_amp_ha_words(x))\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x:text_strip(x))\n\ndf['text_length']=df['clean_tweet'].str.split().map(lambda x: len(x))","d7efde9c":"def build_wordcloud(var, title):\n    wordcloud = WordCloud(background_color='black',colormap=\"bwr\", \n                          stopwords=set(STOPWORDS),max_words=80, min_font_size= 8,\n                          max_font_size=40, random_state=666).generate(str(var))\n\n    fig = plt.figure(1, figsize=(15,15))\n    plt.axis('off')\n    fig.suptitle(title, fontsize=16)\n    fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud,interpolation='bilinear')\n    plt.show()","1a8aac16":"build_wordcloud(df['clean_tweet'], 'Popular keywords used in all tweets')","69019570":"vec = TfidfVectorizer(stop_words=\"english\")\nvec.fit(df['clean_tweet'].values)\nfeatures = vec.transform(df['clean_tweet'].values)\n\n#taking an arbitary value say 5, to get 5 cluster values\nkmeans = KMeans(n_clusters = 5,init ='k-means++', max_iter=300, random_state=0,verbose=1)\ny_kmeans =  kmeans.fit_predict(features)\ndf['Cluster']  = y_kmeans\n#df","6088d7a7":"#df[df['Cluster'] == 0].head(20)['text'].tolist()\n#df[df['Cluster'] == 1].head(20)['text'].tolist()","b93684cd":"build_wordcloud(df[df['Cluster'] == 0]['text'], '1st word cluster using KMeans')","0c4c1675":"build_wordcloud(df[df['Cluster'] == 1]['text'], '2nd Word Cluster using KMeans')","31ce73ef":"build_wordcloud(df[df['Cluster'] == 2]['text'], '3rd word cluster using KMeans')","b613a503":"build_wordcloud(df[df['Cluster'] == 3]['text'], '4th word cluster using KMeans')","d3551fda":"build_wordcloud(df[df['Cluster'] == 4]['text'], 'And finally the 5th word cluster using KMeans ')","e60db725":"nltk.download('vader_lexicon')\nsid = SentimentIntensityAnalyzer()\n\ndef get_score(text):\n    dict_res = sid.polarity_scores(text)\n    return dict_res[\"compound\"]\n\ndf[\"Score\"] = df[\"clean_tweet\"].apply(lambda x: get_score(x))","90c93e99":"df[\"Score\"].describe()","898493cc":"pred_df=pd.DataFrame({'text':df['clean_tweet'],'pred_sentiment':df['Score'],'country':df['country'],'text_length':df['text_length']})\npred_df['pred_sentiment']=np.where(pred_df['pred_sentiment']>0.5,1,0)\npred_df[['text','pred_sentiment']].head(4)","75b13ff8":"bird = np.array(Image.open('..\/input\/covid-19-tweet-supporting-files\/twitter_logo.jpg'))\nfig, (ax2, ax3) = plt.subplots(1, 2, figsize=[30, 15])\nwordcloud2 = WordCloud( background_color='white',mask= bird,colormap=\"Reds\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[pred_df['pred_sentiment']==0]['text']))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative Sentiment Bird',fontsize=35);\n\nwordcloud3 = WordCloud( background_color='white',mask= bird,colormap=\"Blues\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[pred_df['pred_sentiment']==1]['text']))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Positive Sentiment Bird',fontsize=35)","4a9e49ba":"sentiment_countries = pd.DataFrame()\nsentiment_countries[\"Score\"] = df[\"Score\"]\nsentiment_countries[\"country\"] = df[\"country\"]\n\nsentiment_countries = sentiment_countries.sort_values(by = \"Score\",ascending=False)\nsentiment_countries = sentiment_countries.groupby(\"country\").sum().sort_values(by = \"Score\",ascending=False)[:10]\n\nplt.figure(figsize=(9,10))\nsns.barplot(list(sentiment_countries.values.flatten()),sentiment_countries.index,)\nplt.title(\"Top 10 Location By Positive Score\")\nplt.xlabel(\"Sentiment Score\")\nplt.ylabel(\"Location\")\nplt.show()","603b7cde":"df_clean = df.drop(['user_description','user_location',\t'user_created','text','is_retweet','date', 'hashtags' ],axis=1)\ndf_clean_19 = df_clean[(df_clean.created_year == 2019)]\ntop_3 = df_clean_19[(df_clean_19.country == 'USA') | (df_clean_19.country == 'India') | (df_clean_19.country == 'UK')]","759a597d":"fig = px.parallel_categories(top_3, dimensions=['country', 'user_verified','covid', 'created_month'],\n                color=\"Score\", color_continuous_scale=px.colors.sequential.Inferno,\n                labels={'sex':'Payer sex', 'smoker':'Smokers at the table', 'day':'Day of week'})\nfig.show()","7435ac99":"#df_clean = df.drop(['user_description','user_location',\t'user_created','text','is_retweet','date', 'hashtags' ],axis=1)\ndf_clean_20 = df_clean[(df_clean.created_year == 2020)]\ntop_3_20 = df_clean_20[(df_clean_20.country == 'USA') | (df_clean_20.country == 'India') | (df_clean_20.country == 'UK')]","3c2f2124":"fig = px.parallel_categories(top_3_20, dimensions=['country', 'user_verified','covid', 'created_month'],\n                color=\"Score\", color_continuous_scale=px.colors.sequential.Inferno,\n                labels={'sex':'Payer sex', 'smoker':'Smokers at the table', 'day':'Day of week'})\nfig.show()","bd453f8c":"### Import Statements","e34a875a":"## 1. Countries that tweeted the most on generalised content","060411cc":"##### creating a dataFrame that has count of  total number of tweets per country","2248958b":"##### From this **Line Chart**  we could see quite a spike in number of tweets(February,2020 to March,2020), that is when the WHO declared Global Pandemic and many nations started security measurments such as Lockdowns","a0153d36":"##### 7.2 Word-cloud of Cluster 2","31d83933":"### 4. Finding a pattern\/Spike in tweets","b5d27fcf":"##### 7.1 Word-cloud of Cluster 1","612c4d33":"### 9. Top 10 countries that had positive sentiment of tweets","b393af3f":"### 2.2 This plot symbolizes Where did the most tweets come from and whether the content was related to Covid19","6f361fb4":"### 2.1 This plot symbolizes how frequent did the top user tweeted and whether the content was related to Covid19","d865f913":"# **Analysing COVID19 Tweets!**\n# Hello Data Scientist, Welcome to my notebook!\n### <div align=\"left\">  Today we shall perform EDA, Clustering using KMeans and Sentiment analysis on the Tweets (related to #Covid19) to answer the following questions:\nLet us begin : \n<\/div>\n\n\n1. Which countries tweeted the most? (based on Generalised Content)\n> 1.1 which countries tweeted the most based on #Covid19 or tweets that are related to Corona\/Covid19\n2. Was the tweet based on Corona if so how frequent was the tweet based on \n> 2.1 User Accounts \/ User Names\n>> 2.2 Country \/ Location \n>>> 2.3 Source \/ Device they tweeted from\n3. How much did the Verified profile people tweeted in the year 2019 and 2020 based out of the country?\n4. Monthly tweet trend of 2019 vs 2020\n5. Who are the top 40 people\/accounts that has most followers and their tweet count?\n6. What are the most common words in the tweets?\n7. Clustering the most commonly used tweet words into 5 groups\n> 7.1 Group 1\n>> 7.2 Group 2\n>>> 7.3 Group 3\n>>>> 7.4 Group 4\n>>>>> 7.5 Group 5\n8. What are the positive and negative sentiment words?\n> Word-cloud of Positive and Negative twitter bird\n9. What are the top 10 countries that tweeted with positive sentiments?\n10. How is the data (countries, people, tweet content, month) distributed with sentiments?\n> 10.1 In the Year 2019\n>> 10.2 In the Year 2020\n\n<div align=\"center\"> \n    If you loved my notebook, please upvote to encourage me! \n<\/div> ","13808697":"### 8. Sentiment Analysis","dc3b62b2":"### 3. This EDA show us how did the \"verified_Profile\" users reacted from the top 5 countries in the year 2019 and 2020","e683fb85":"### 7. Clustering the most commonly used word using KMeans\n Cheers! \ud83e\udd42 learnt from ( https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.KMeans.html )","26d9f6a6":"###### Checking for empty objects \/ nan ","2fd801c2":"### Filepath of the dataset","190378d3":"###### As predicted a  huge  propotion of words are about Covid!","99ca9694":"### Word clouds","8ddb7788":"###### Grouping \/ cleaning unknown dataset","cebccfe8":"### Data cleaning","bfc87c3d":"### Correlation of given datas with heatmap","9f66596b":"### 2.3 This plot symbolizes from which devices did the tweets originated and whether the content was related to Covid19","b8daf55d":"###### Pre-requirements","1402e03c":"#### 10.1 Overview of User Sentiments among the top three countries that tweeted during 2019","24fddee0":"### CONVERTING TIME STAMPS","57057fa6":"\n### <div align=\"center\">   With endless possibilites of representation and uncertainity, I hereby end my assessment with good hope!\n\n<div align=\"center\"> Thanks and Regards,\n<div align=\"center\"> Mohamed Riaz | mohamed.riaz1307@gmail.com \n","e7059b42":"###### Here we shall use sentiments of  two types i.e. Postivie Sentiments and  Negative Sentiments and extracting sentiments from a pretrained model","7fa560c0":"## 1.1 Now, we shall see countries that specifically tweeted the most about Covid 19 and their tweet counts respectively","6526af27":"### 10. Data distribution","3a8def5a":"###### Cleaning and Extracting  country name from \"user_ locations\"","3a1e5720":"##### 7.3 Word-cloud of Cluster 3","5abc0c60":"###### Hashtage cleaner fuction, which groups tweets that are specific to Covid 19 ","bf5ddeb7":"##### 7.4 Word-cloud of Cluster 4","12427c61":"### 2. EDA on tweets from the year 2019 (i.e. 2019 + 2020) based on","16166235":"###### Preprocession text from tweeted content","bdb4df43":"##### 7.5 Word-cloud of Cluster 5","26961b3d":"### 6. Creating a word cloud for the most popular words used","00c52202":"### 5. Most popular user with thier tweet count and followers","df524a80":"#### Plotting unique features in the dataset","10fe06c8":"##### 10.2 Overview of User Sentiments among the top three countries that tweeted during 2010"}}