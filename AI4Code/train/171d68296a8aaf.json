{"cell_type":{"5a062c7f":"code","f1af0745":"code","79a00268":"code","29c8e2f8":"code","89aabf56":"code","4bc138ad":"code","2b299b5e":"code","947f1003":"code","9e0fd9cd":"code","4e87b78e":"code","3e8a655f":"code","8c4d03a2":"code","9608aa56":"code","f11d3a00":"code","5bd33878":"code","ace49dea":"code","4ddc06bf":"code","c3855b55":"code","30eed4f1":"code","006ee0a1":"code","d19974db":"code","c10304d9":"code","a64eb0c8":"code","46405bcc":"code","a7ad1b38":"code","40a445f8":"code","62164b0a":"code","387e2932":"code","e12bc751":"code","82e24f02":"code","29c536c4":"code","598fa9dc":"code","b29e77fd":"code","f05e58cd":"code","c7553a46":"code","c8791a83":"code","be9261b4":"code","a0cf112f":"code","2dc9eb8c":"code","45850ce5":"code","9a58658e":"code","01dda9e1":"code","dd3151cc":"code","6d85a75b":"code","7a3f9365":"code","69748b70":"code","492cfad6":"code","665e9772":"code","07ce6708":"code","599bba02":"code","71cef9bd":"code","e44fcc7e":"code","abe860c2":"code","b0cf3b58":"code","a3a285fa":"code","e43f4a9e":"code","0d18f140":"code","0db54703":"code","375adb9f":"code","d63fb47c":"code","34c6e7c7":"code","ed2d2c78":"code","6fd24257":"code","3b155762":"markdown","9d4ec819":"markdown","3e6dc07b":"markdown","534fb694":"markdown","69159bc9":"markdown","e6d80d61":"markdown","ee7347c4":"markdown","50703175":"markdown","2dede770":"markdown","d6b1e33a":"markdown","9db12be7":"markdown","10500c7b":"markdown","9466d43f":"markdown","823bec49":"markdown","2e4e8538":"markdown","aa607159":"markdown","0577cd43":"markdown","7f38ccd0":"markdown","a50cef55":"markdown","b6b09ad4":"markdown","0e63397c":"markdown","ae6ca764":"markdown","cf95a913":"markdown","5a5e792e":"markdown","4d105c97":"markdown","77b50500":"markdown","f6a6d7a8":"markdown","0609b1a9":"markdown","15697499":"markdown","1c062dab":"markdown","729273c3":"markdown","eab71430":"markdown","9834de9d":"markdown","46e89ecd":"markdown","0041f7d1":"markdown","130c15d8":"markdown","c003080c":"markdown","ddae63d9":"markdown","5f3289ce":"markdown"},"source":{"5a062c7f":"# Importing Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","f1af0745":"features = pd.read_csv('..\/input\/walmart\/features.csv')\nfeatures.head()","79a00268":"stores = pd.read_csv('..\/input\/walmart\/stores.csv')\nstores.head()","29c8e2f8":"train = pd.read_csv('..\/input\/walmart\/train.csv')\ntrain.head()","89aabf56":"features.shape , stores.shape, train.shape","4bc138ad":"# We will merge our datasets\n\ntrain_df = train.merge(features, 'left').merge(stores, 'left')","2b299b5e":"train_df","947f1003":"# Train data are too large to process so we can take only sample of it.\ntrain_df = train_df[0:1000]","9e0fd9cd":"train_df.shape","4e87b78e":"# Creating a copy of this dataset to generate seasonality trends later\ntdf = train_df.copy()","3e8a655f":"train_df.info()","8c4d03a2":"# Set Date column as Index\ntrain_df.set_index('Date',inplace=True)\ntrain_df.head()","9608aa56":"train_df.info()","f11d3a00":"train_df.describe().transpose()","5bd33878":"# Check correlation between features of data\ntrain_df.corr()","ace49dea":"# Check correlation of other features with Weekly Sales.\ntrain_df.corr()['Weekly_Sales'].sort_values(ascending= False)","4ddc06bf":"# Check for missing values\ntrain_df.isna().sum()","c3855b55":"train_df = train_df.fillna(0)","30eed4f1":"train_df.isna().sum()","006ee0a1":"# Let's check distribution of dependent feature\nplt.figure(figsize=(13,5))\nsb.distplot(train_df['Weekly_Sales'],bins=50,color='purple')","d19974db":"# Lets separate numerical data aside\ndf_numeric = train_df.select_dtypes(exclude = [\"object\",\"bool\"])","c10304d9":"df_numeric.hist(figsize=(18,12))\nplt.show()","a64eb0c8":"# Plot heatmap of correlation between features\nplt.figure(figsize=(20, 10))\nsb.heatmap(train_df.corr(),yticklabels=True,annot=True,cbar=True,cmap='Spectral_r')","46405bcc":"# visualising Sales by various features\nfig, ax = plt.subplots(2, 2, figsize= (18,10))\nax[0,0].scatter(train_df['Temperature'], train_df['Weekly_Sales'])\nax[0,0].set_title('Weekly Sales by Temperature',fontsize=15)\n\nax[0,1].scatter(train_df['Fuel_Price'], train_df['Weekly_Sales'],color='violet')\nax[0,1].set_title('Weekly Sales by fuel price',fontsize=15)\n\nax[1,0].scatter(train_df['Dept'], train_df['Weekly_Sales'],color='violet')\nax[1,0].set_title('Weekly Sales by Departments',fontsize=15)\n\nax[1,1].scatter(train_df['CPI'], train_df['Weekly_Sales'])\nax[1,1].set_title('Weekly Sales by CPI',fontsize=15)\nplt.show()","a7ad1b38":"plt.figure(figsize=(13,5))\nsb.histplot(train_df, x=\"Weekly_Sales\", hue=\"IsHoliday\", element=\"poly\")","40a445f8":"sb.barplot(x=\"IsHoliday\", y=\"Weekly_Sales\", data=train_df)\nplt.title(\"Holidays vs Weekly Sales\",fontsize=15)\nplt.xlabel(\"Type\")\nplt.ylabel(\"Weekly Sales\")\nplt.show()\n","62164b0a":"def plot_df(train_df, x, y, title=\"\", xlabel='Date', ylabel='Value', dpi=100):\n    plt.figure(figsize=(12,8), dpi=dpi)\n    plt.scatter(x, y, color='tab:red')\n    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n    plt.show()\n\nplot_df(train_df, x=train_df.index, y=train_df['Weekly_Sales'], title='Daily Sales count')    ","387e2932":"# Aggregating the Time Series to a monthly scaled index\ny = tdf[['Date','Weekly_Sales']].copy()\ny.set_index('Date', inplace=True)\ny.index = pd.to_datetime(y.index)\ny = y.resample('1M').mean()\ny['Date'] = y.index\n\n# Plot the Monthly Volume Lineplot\nfig, ax = plt.subplots(figsize=(15, 6))\nsb.lineplot(y['Date'], y['Weekly_Sales'])\n\nax.set_title('Monthly Sales', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\nax.set_xlabel('Year', fontsize = 16, fontdict=dict(weight='bold'))\nplt.tick_params(axis='y', which='major', labelsize=16)\nplt.tick_params(axis='x', which='major', labelsize=16)","e12bc751":"from statsmodels.tsa.stattools import adfuller","82e24f02":"# check for stationarity\ndef adf_test(series, title=''):\n  \n    print('Augmented Dickey-Fuller Test: {}'.format(title))\n    # .dropna() handles differenced data\n    result = adfuller(series,autolag='AIC') \n    \n    labels = ['ADF test statistic','p-value','# lags used','# observations']\n    out = pd.Series(result[0:4],index=labels)\n\n    for key,val in result[4].items():\n        out['critical value ({})'.format(key)]=val\n        \n    # .to_string() removes the line \"dtype: float64\"\n    print(out.to_string())          \n    \n    if result[1] <= 0.05:\n    \n        print(\"Rejects the null hypothesis\")\n        print(\"Data has no unit root and is stationary\")\n    else:\n        print(\"Fail to reject the null hypothesis\")\n        print(\"Data has a unit root and is non-stationary.\")   ","29c536c4":"adf_test(train_df['Weekly_Sales'], title ='ADF TEST')","598fa9dc":"rolmean = tdf['Weekly_Sales'].rolling(12).mean()\nrolstd = tdf['Weekly_Sales'].rolling(12).std()\n\n#plot rolling statistics\nplt.figure(figsize=(20,8))\norig = plt.plot(tdf['Weekly_Sales'],color='Blue',label='Original')\nmean = plt.plot(rolmean,color='Red',label='Rolling Mean')\nstd = plt.plot(rolstd,color='Black',label='Rolling Standard Deviation')\n\nplt.legend()\nplt.title('Rolling Mean & Standard Deviation',fontsize=20)\nplt.show()","b29e77fd":"from statsmodels.tsa.stattools import acf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nfig, ax = plt.subplots(nrows=1, ncols=2,figsize=(15, 6))\nautocorr = acf(train_df['Weekly_Sales'], nlags=60, fft=False)\nprint(autocorr)\n\nplot_acf(train_df['Weekly_Sales'].tolist(), lags=60, ax=ax[0], fft=False, color='Red');\nplot_pacf(train_df['Weekly_Sales'].tolist(), lags=60, ax=ax[1], color='Green');","f05e58cd":"from pandas.plotting import lag_plot\nlag_plot(train_df['Weekly_Sales'])","c7553a46":"from statsmodels.tsa.statespace.tools import diff\n\nfig, ax = plt.subplots(nrows=2, ncols=2,figsize=(15, 11))\n# Set the differences from the initial data\ntdf['OpenDiff1'] = diff(tdf['Weekly_Sales'],k_diff=1)\ntdf['OpenDiff2'] = diff(tdf['Weekly_Sales'],k_diff=2)\ntdf['OpenDiff3'] = diff(tdf['Weekly_Sales'],k_diff=3)\n\n# Plot the initial data and as well as Differences in initial data\ntdf['Weekly_Sales'].plot(title=\"Initial Data\",ax=ax[0][0], color='Black').autoscale(axis='x',tight=True);\ntdf['OpenDiff1'].plot(title=\"First Difference Data\",ax=ax[0][1],color='Orange').autoscale(axis='x',tight=True);\ntdf['OpenDiff2'].plot(title=\"Second Difference Data\",ax=ax[1][0],color='Orange').autoscale(axis='x',tight=True);\ntdf['OpenDiff3'].plot(title=\"Third Difference Data\",ax=ax[1][1],color='Orange').autoscale(axis='x',tight=True);\n\nfig.autofmt_xdate()","c8791a83":"from pylab import rcParams\nimport statsmodels.api as sm\n\n# Aggregating the Time Series to a monthly scaled index\ny = tdf[['Date','Weekly_Sales']].copy()\ny.set_index('Date', inplace=True)\ny.index = pd.to_datetime(y.index)\ny = y.resample('1M').mean()\n\n# Setting rcparams\nrcParams['figure.figsize'] = 15, 12\nrcParams['axes.labelsize'] = 20\nrcParams['ytick.labelsize'] = 16\nrcParams['xtick.labelsize'] = 16\n\n# DECOMPOSING TIME SERIES\ndecomposition = sm.tsa.seasonal_decompose(y, model='multiplicative', freq = 12)\ndecomp = decomposition.plot()\ndecomp.suptitle('Open decomposition', fontsize=22)","be9261b4":"# Aggregating the Time Series to a monthly scaled index\ny = tdf[['Date','Weekly_Sales']].copy()\ny.set_index('Date', inplace=True)\ny.index = pd.to_datetime(y.index)\ny = y.resample('1M').mean()\n\ny['MA3'] = y.rolling(window=3).mean() \ny.plot(figsize=(15,6));","a0cf112f":"from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing\n# Setting the span value\nspan = 6\n# Weights of observations\nalpha = 2\/(span+1)\n\n# Plot Simple exponential smoothing\ntdf['ES3'] = SimpleExpSmoothing(tdf['Weekly_Sales']).fit(smoothing_level = alpha, optimized = False).fittedvalues.shift(-1)\ntdf[['Weekly_Sales','ES3']].plot(figsize=(15,6));","2dc9eb8c":"# Plot Double and Triple exponential smoothing\ntdf['DESmul3'] = ExponentialSmoothing(tdf['Weekly_Sales'], trend = 'add').fit().fittedvalues.shift(-1)\ntdf['TESmul3'] = ExponentialSmoothing(tdf['Weekly_Sales'], trend = 'add', seasonal = 'add', seasonal_periods = 24).fit().fittedvalues.shift(-1)\ntdf[['Weekly_Sales', 'TESmul3', 'DESmul3']].plot(figsize = (15,6));","45850ce5":"train_df = train_df.sort_values(by='Date')","9a58658e":"train_df.shape","01dda9e1":"# Reset index column to put Date column back into Dataframe\ntrain_df.reset_index(level=0, inplace=True)","dd3151cc":"#Creating train and test set \ntrain = train_df[0:700] \ntest= train_df[700:]","6d85a75b":"fig, ax = plt.subplots(figsize=(15, 6))\nsb.lineplot(train['Date'], train['Weekly_Sales'], color = 'black')\nsb.lineplot(test['Date'], test['Weekly_Sales'], color = 'blue')\n\n# Formatting\nax.set_title('Train & Test data', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\nax.set_xlabel('Date', fontsize = 16, fontdict=dict(weight='bold'))\nax.set_ylabel('Weekly Sales', fontsize = 16, fontdict=dict(weight='bold'))\nplt.tick_params(axis='y', which='major', labelsize=16)\nplt.tick_params(axis='x', which='major', labelsize=16)\nplt.legend(loc='upper right' ,labels = ('train', 'test'))","7a3f9365":"dd= np.asarray(train.Weekly_Sales)\ny_hat = test.copy()\ny_hat['naive'] = dd[len(dd)-1]\nplt.figure(figsize=(12,8))\nplt.plot(train.index, train['Weekly_Sales'], label='Train')\nplt.plot(test.index,test['Weekly_Sales'], label='Test')\nplt.plot(y_hat.index,y_hat['naive'], label='Naive Forecast')\nplt.legend(loc='best')\nplt.title(\"Naive Forecast\")\nplt.show()","69748b70":"from sklearn.metrics import mean_squared_error,mean_absolute_error\nfrom math import sqrt\nmae_i = sqrt(mean_absolute_error(test.Weekly_Sales, y_hat.naive))\nprint(\"MAE :\",mae_i)\nrmse_i = sqrt(mean_squared_error(test.Weekly_Sales, y_hat.naive))\nprint(\"RMSE :\",rmse_i)\n","492cfad6":"y_hat_avg = test.copy()\ny_hat_avg['moving_avg_forecast'] = train['Weekly_Sales'].rolling(60).mean().iloc[-1]\nplt.figure(figsize=(16,8))\nplt.plot(train['Weekly_Sales'], label='Train')\nplt.plot(test['Weekly_Sales'], label='Test')\nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast')\nplt.legend(loc='best')\nplt.show()","665e9772":"mae_ii = sqrt(mean_absolute_error(test.Weekly_Sales, y_hat_avg.moving_avg_forecast))\nprint(\"MAE :\",mae_ii)\nrmse_ii = sqrt(mean_squared_error(test.Weekly_Sales, y_hat_avg.moving_avg_forecast))\nprint(\"RMSE is:\",rmse_ii)","07ce6708":"import statsmodels.api as sm\nfrom   statsmodels.tsa.holtwinters import  Holt\ny_hat_avg = test.copy()\nmodel_fit = Holt(np.asarray(train['Weekly_Sales'])).fit()\ny_hat_avg['Holt_linear'] = model_fit.forecast(len(test))","599bba02":"print('')\nprint('==Holt model Exponential Smoothing Parameters ==')\nprint('')\n#alpha_value = np.round(model_fit.params['smoothing_level'], 4)\n#beta_value  = np.round(model_fit.params['smoothing_Slope'], 4)\n\nprint('Smoothing Level', alpha_value )\nprint('Smoothing Slope', beta_value)\nprint('Initial Level',   np.round(model_fit.params['initial_level'], 4))\nprint('')","71cef9bd":"Pred_Holt = test.copy()\n\nPred_Holt['Opt'] = model_fit.forecast(len(test['Weekly_Sales']))","e44fcc7e":"plt.figure(figsize=(16,8))\nplt.plot(tdf['Weekly_Sales'], label='Train')\nplt.plot(test['Weekly_Sales'], label='Test')\nplt.plot(Pred_Holt['Opt'], label='HoltOpt')\nplt.legend(loc='best')\nplt.show()","abe860c2":"df_pred_opt =  pd.DataFrame({'Y_hat':Pred_Holt['Opt'] ,'Y':test['Weekly_Sales'].values})\n\nmae_iii= mean_absolute_error(df_pred_opt.Y, df_pred_opt.Y_hat)\nprint(\"MAE is \",mae_iii)\nrmse_iii= sqrt(mean_squared_error(df_pred_opt.Y, df_pred_opt.Y_hat))\nprint(\"RMSE is \",rmse_iii)","b0cf3b58":"y_hat_avg = test.copy()\nfit_model = ExponentialSmoothing(np.asarray(train_df['Weekly_Sales']),seasonal_periods=20,trend='add',seasonal='add').fit()\ny_hat_avg['Holt winter']= fit_model.forecast(len(test))","a3a285fa":"plt.figure(figsize=(16,8))\nplt.plot(tdf['Weekly_Sales'],label='Train')\nplt.plot(test['Weekly_Sales'],label='Test')\nplt.plot(y_hat_avg['Holt winter'],label='Holt winter')\nplt.legend()\nplt.show()","e43f4a9e":"rmse= np.sqrt(mean_squared_error(test['Weekly_Sales'],y_hat_avg['Holt winter']))\nfrom sklearn.metrics import mean_absolute_error\nmae_iv= mean_absolute_error(test['Weekly_Sales'],y_hat_avg['Holt winter'])\nprint('MAE : ',mae_iv)\nrmse_iv= sqrt(mean_squared_error(test['Weekly_Sales'],y_hat_avg['Holt winter']))\nprint('RMSE : ',rmse_iv)","0d18f140":"train_log = np.log(train['Weekly_Sales'])\ntest_log = np.log(test['Weekly_Sales'])\ntrain_log_diff = train_log - train_log.shift(1)\ntrain_log = train_log.dropna()\ntest_log = test_log.dropna()","0db54703":"from statsmodels.tsa.stattools import acf,pacf\nlag_acf = acf(train_log_diff.dropna(), nlags=10)\nlag_pacf = pacf(train_log_diff.dropna(), nlags=10, method='ols')","375adb9f":"#Plot ACF & PACF : \nplt.figure(figsize=(15,6))\nplt.subplot(1,2,1)\nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-7.96\/np.sqrt(len(train_log_diff)),linestyle='--',color='gray')\nplt.axhline(y=7.96\/np.sqrt(len(train_log_diff)),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')\n\n#Plot PACF:\nplt.subplot(1,2,2)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-7.96\/np.sqrt(len(train_log_diff)),linestyle='--',color='gray')\nplt.axhline(y=7.96\/np.sqrt(len(train_log_diff)),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()","d63fb47c":"# Optimal values fot ARIMA(p,d,q) model are (1,1,1). Hence plot the ARIMA model using the value (1,1,1)\nfrom statsmodels.tsa.arima_model import ARIMA\nmodel = ARIMA(train_log, order=(2, 1, 1))  \nresults_ARIMA = model.fit(disp=-20,start_ar_lags= 40) \nplt.figure(figsize=(16,8))\nplt.plot(train_log_diff.dropna(),label='Actual')\nplt.plot(results_ARIMA.fittedvalues, color='red',label='Predictions')\nplt.legend(loc='upper right')\nsum = ((results_ARIMA.fittedvalues-train_log_diff)**2).sum()\nplt.title('RSS: %.2f'% sum)\nplt.show()\n","34c6e7c7":"fc, se, conf = results_ARIMA.forecast(300, alpha=0.05)\nfc.shape","ed2d2c78":"# Report performance\nmae_v = mean_absolute_error(test['Weekly_Sales'], fc)\nprint('MAE: ',mae_v)\nrmse_v = np.sqrt(mean_squared_error(test['Weekly_Sales'], fc))\nprint('RMSE: ',rmse_v)","6fd24257":"models = pd.DataFrame({\n    'Model': [ 'Naive Approch', 'Moving Average', \"Holt's Linear trend\",\n               \"Holt Winter's Model\",\"ARIMA\"],\n    'MAE': [\n        mae_i, \n        mae_ii,  \n        mae_iii, \n        mae_iv,\n        mae_v]\n\n\n       })\nprint(\"--------TIME FORECASTING MODELS---------\")\nmodels.sort_values(by='MAE', ascending=True)","3b155762":"### --> Lag Scatter plot","9d4ec819":"> ## iii) Holt's Linear Trend Model","3e6dc07b":"- We can see only MarkDown columns have missing values.\n- Missing values in Markdown columns probably means that there is no markdown, so we can fill those by 0.","534fb694":"- From the plot, We can see seasonal trend via Monthly Sales plotting.\n- There is high spike in Sales in January month and it goes downward in February.\n- Between months of May and September, Sales fulctuates on average rate.","69159bc9":"## 2. Data Exploration","e6d80d61":"> ## iv) Holt's Winter's Model","ee7347c4":"## 4. Data Visualization","50703175":"### --> Differences in Time Series","2dede770":"- Moving average is the estimation of the trend-cycle at time , and is obtained by averaging the values of the time series within number of periods of time. \n- The Observations that are nearby in time are also likely to be close in value.","d6b1e33a":"# Consider UPVOTING if you find it useful\/fork this notebook!!\ud83e\udd13","9db12be7":"### --> Exponential Smoothing","10500c7b":"- We can see the increasing trend over the time in data.\n- The seasonal component remains the same over time. But this doesn't mean years far apart won't have different seasonal patterns.\n- The residual component displays what is left over when the seasonal and trend-cycle components have been subtracted from the data.","9466d43f":"- Using the plot we can determine the values for p and q respectively :\n\n    - p: the lag value where the PACF cuts off (drop to 0) for the first time. So here p = 1.\n    - q: the lag value where the ACF chart crosses the upper confidence interval for the first time.so, q=1.","823bec49":"### --> Check stationarity of Time series ","2e4e8538":"- The graph is very noisy and unable to read when plotted on daily basis. So lets plot the graph on monthly basis to check trends and seasonality","aa607159":"### --> Time series Components","0577cd43":"- We have positive Correlation in dependent feature.","7f38ccd0":"- The dependent variable is positively skewed.","a50cef55":"> ## i) Naive's Approach","b6b09ad4":"- We need to plot AF & PCAF first.\n","0e63397c":"## Conclusion:\n# For this dataset, **Moving Average model** is best suited for Forecasting.","ae6ca764":"## 5. Time Series Analysis","cf95a913":"## Table of Contents :\n- #### 1.Data Collection\n- #### 2.Data Exploration\n- #### 3.Data Cleaning\n- #### 4.Data Visualization\n- #### 4.Time Series Analysis\n- #### 5.Time Series Forecasting\n - i) Niave Approach\n - ii) Moving Average Model\n - iii) Holt's linear trend model\n - iv) Holt winter's Model\n - v) ARIMA\n- #### 7.Model Evaluation\n- ##### Conclusion","5a5e792e":"### --> Visualize the stationary data with Rolling mean & standard deviation","4d105c97":"### --> Autocorrelation & Partial Autocorrelation","77b50500":"##  6. Time Series Forecasting","f6a6d7a8":"- Autocorrelation measures the linear relationship between lagged values of a time series.\n- Partial autocorrelations measure the linear dependence of one variable after removing the effect of other variable that affect both variables.","0609b1a9":"- After running ADF Test, we can come to conclusion that our **data is Stationary**, so there's no need of conversion.","15697499":"### Comparing all the models:","1c062dab":"![](https:\/\/www.geico.com\/living\/wp-content\/uploads\/writing-perfect-thank-you-post.jpg)","729273c3":"## 7. Model Evaluation","eab71430":"- Here, we are choosing **Mean Absolute Error** as main for evaluation.","9834de9d":"## 3. Data Cleaning","46e89ecd":"### --> Moving Average(MA)","0041f7d1":"# <center> WALMART SALES FORECASTING\ud83d\udcc8\n<h2><center>TIME SERIES ANALYSIS TUTORIAL\u2705<\/h2>\n    \n![hj](https:\/\/miro.medium.com\/max\/1400\/1*tgc5PuEaW36qa-60V7_KaA.jpeg)    \n#### <center>Aim: To predict future sales in Walmart.","130c15d8":"> ## v) ARIMA Model","c003080c":"> ## ii) Moving Average Model ","ddae63d9":"- Exponential smoothing assigns exponentially decreasing weights for newest to oldest observations.\n- The weights decrease exponentially as the observations come from the past.\n- The smallest weights are associated with oldest observations.","5f3289ce":"## 1. Data Collection"}}