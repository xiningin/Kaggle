{"cell_type":{"bbaae2a6":"code","4c9564a8":"code","4c560b39":"code","7ad1fc67":"code","94ca986f":"code","96d0741e":"code","b72aca03":"code","0d51aa8c":"code","4735f133":"code","72e5b1d9":"code","de08164b":"code","37a9e26f":"code","b5183356":"code","1f476101":"code","a6fce3ca":"code","9ffd6c76":"code","11024d0f":"code","23a28cb7":"code","1faa3da4":"code","f93b393e":"code","b578d555":"code","d29bbd1f":"code","0a4b226c":"code","075e1514":"code","7d2628b0":"code","5f36866a":"code","31ab9535":"code","2fb08277":"code","576cace5":"code","97b8aa2d":"code","7d7cc3c5":"code","57b4a362":"code","2e0d59ba":"code","4317b8cd":"code","1538300e":"code","6d4d982f":"code","dead5bc6":"code","86fffa80":"code","c96ee557":"code","b179939f":"code","89833e9a":"code","67be1b11":"code","2878b274":"code","a98693f6":"code","b913c7d3":"code","f4f5f781":"code","2faf6b47":"code","e509a3a1":"code","7642df28":"code","0d835988":"code","215c8bb3":"code","336e9641":"code","7bd84125":"code","e7aa72e3":"code","e8963c71":"code","ed5bde80":"code","dd34a5d8":"code","55407cac":"code","9a2b94e5":"code","d79918f9":"code","abcbe917":"code","7020b3d0":"code","b7b7987b":"code","53a1c953":"code","2913d3a8":"code","28d9274f":"code","057022e2":"code","c60d7306":"code","31196e63":"code","20157789":"code","b5284111":"code","5ba54cf1":"code","203c3dc5":"code","594868c8":"code","7fc2e18a":"code","d61ae8c6":"code","0ce2cc8d":"code","7c6b2565":"code","a1eb7840":"code","732307e4":"code","2bd0edda":"code","10294d1f":"code","14c58770":"code","95e66a98":"code","171e30d6":"code","4fbf0fd2":"code","558eb7d1":"code","bb66ca50":"code","d3bf3ce7":"code","22bb9efd":"code","54f82e64":"code","f7018979":"code","4950eed3":"code","3a47d660":"markdown","d655e890":"markdown","a8ec101e":"markdown","4a105854":"markdown","71942367":"markdown","7ae2fb56":"markdown","56a3b2ee":"markdown","6a97bc14":"markdown","4681226c":"markdown","7355ec3a":"markdown","6e037dc0":"markdown","eebbf30d":"markdown","7244955e":"markdown","1634cf15":"markdown","7ba51dac":"markdown","1d02d020":"markdown","d417d0e3":"markdown","9393d3c4":"markdown","7416b6f5":"markdown","a1e48b0c":"markdown","a9069064":"markdown","e8acdceb":"markdown"},"source":{"bbaae2a6":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nimport statsmodels.api as sm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.subplots as make_subplots","4c9564a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4c560b39":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","7ad1fc67":"train_df.head(2)","94ca986f":"train_df.isnull().sum()","96d0741e":"test_df.isnull().sum()","b72aca03":"df = pd.concat([train_df, test_df], axis = 0, sort = False)","0d51aa8c":"print(f'Combined shape = {df.shape}')\nprint(f'Test shape = {test_df.shape}')\nprint(f'Train shape = {train_df.shape}')","4735f133":"df = df.reset_index()","72e5b1d9":"df.drop(columns = ['index'], inplace = True)","de08164b":"df.info()","37a9e26f":"df.describe()","b5183356":"null_cols = df.isnull().sum()\nnull_cols = pd.DataFrame(null_cols)\nnull_cols = null_cols.reset_index()\nnull_cols = null_cols.rename(columns = {'index' : 'Features', 0 : '# of Missing Values'})\nnull_cols","1f476101":"fig = px.bar(null_cols, \n            x = 'Features',\n            y = '# of Missing Values',\n            hover_data = ['# of Missing Values'],\n            color = 'Features',\n            title = 'Missing values in columns')\nfig.show()","a6fce3ca":"Sex = pd.DataFrame(df['Sex'].value_counts())\nSex = Sex.reset_index().rename(columns = {'index' : 'Gender', 'Sex' : 'Count'})\nfig = px.pie(Sex,\n            values = 'Count',\n            names = 'Gender',\n            title = '# of Male and Female in dataset')\nfig.show()","9ffd6c76":"Embarked = pd.DataFrame(df['Embarked'].value_counts())\nEmbarked = Embarked.reset_index().rename(columns = {'index' : 'Embarked', 'Embarked' : 'Count'})\nfig = px.pie(Embarked,\n            values = 'Count',\n            names = 'Embarked',\n            title = 'Embarkment')\nfig.show()","11024d0f":"PassengerClass = pd.DataFrame(df['Pclass'].value_counts())\nPassengerClass = PassengerClass.reset_index().rename(columns = {'index' : 'Pclass', 'Pclass' : 'Count'})\nfig = px.pie(PassengerClass,\n            values = 'Count',\n            names = 'Pclass',\n            title = '# of passengers in different class')\nfig.show()","23a28cb7":"df['Age'].fillna(df.groupby(\"Pclass\")[\"Age\"].transform(lambda x: x.fillna(x.median())), inplace = True)\ndf.isnull().sum()","1faa3da4":"P = pd.DataFrame()\nP['age_range'] = pd.cut(df['Age'], [0,20,40,60,100], include_lowest=True)\nAgeRange = pd.DataFrame(P['age_range'].value_counts())\nAgeRange = AgeRange.reset_index().rename(columns = {'index' : 'Range', 'age_range' : 'Count'})\nplt.pie(AgeRange['Count'],\n       labels=AgeRange['Range'],\n       autopct='%.3f%%')\nplt.title('Age bracket Comparison')\nplt.show()","f93b393e":"df['Embarked'].fillna(df['Embarked'].mode()[0], inplace = True)\ndf.isna().sum()","b578d555":"df['Fare'].fillna(df.groupby(\"Pclass\")[\"Fare\"].transform(lambda x: x.fillna(x.median())), inplace = True)\ndf.isnull().sum()","d29bbd1f":"df.drop(columns = ['Cabin'], inplace = True)","0a4b226c":"df.isnull().sum()","075e1514":"Classwise_Embarkment = pd.DataFrame(df[['PassengerId','Pclass', 'Embarked']].groupby(['Pclass','Embarked']).count())\nClasswise_Embarkment","7d2628b0":"ClasswiseGender=pd.DataFrame(df[['Pclass', 'PassengerId', 'Sex']].groupby(['Pclass', 'Sex']).count()).rename(columns = {'PassengerId' : 'Count'})\nClasswiseGender","5f36866a":"df.drop(columns = ['Ticket'], inplace = True)\n","31ab9535":"df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand = False)\ndf['Title'].value_counts()","2fb08277":"title_list = list(df['Title'].unique())\nlist_rare = ['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']\nfor title in df['Title']:\n    df['Title'] = df['Title'].replace(list_rare, 'Rare')\n    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n    df['Title'] = df['Title'].replace('Ms', 'Miss')\n    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n    df['Title'] = df['Title'].replace('Miss', 'Miss')\n    df['Title'] = df['Title'].replace('Mrs', 'Mrs')\n    df['Title'] = df['Title'].replace('Mr', 'Mr')\n    df['Title'] = df['Title'].replace('Master', 'Master')\ndf['Title'].value_counts()","576cace5":"df['Accompany'] = df['SibSp']+df['Parch']","97b8aa2d":"df[['Pclass', 'Accompany', 'PassengerId']].groupby(['Pclass', 'Accompany']).count()","7d7cc3c5":"df.drop(columns = ['Name', 'SibSp', 'Parch'], inplace = True)\ndf.reset_index(inplace = True)","57b4a362":"gender_survival = df[['Sex', 'Survived']].groupby('Survived').count()\ngender_survival = gender_survival.reset_index().rename(columns = {'index' : 'Survived'})\ngender_survival","2e0d59ba":"labels = ['Female', 'Male']\nfig = px.pie(gender_survival,\n            values = 'Sex',\n            names = labels,\n            title = 'Genderwise chance of Survival')\nfig.show()","4317b8cd":"Classwise_survival = df[['Pclass', 'Survived', 'PassengerId']].groupby(['Pclass', 'Survived']).count()\nClasswise_survival = Classwise_survival.reset_index().rename(columns = {'index' : 'Pclass'})\nClasswise_survival","1538300e":"Class1 = Classwise_survival[Classwise_survival['Pclass'] == 1]\nClass2 = Classwise_survival[Classwise_survival['Pclass'] == 2]\nClass3 = Classwise_survival[Classwise_survival['Pclass'] == 3]\nfig = go.Figure(data=[\n    go.Bar(name='Pclass1', x=Class1['Survived'], y=Class1['PassengerId']),\n    go.Bar(name='Pclass2', x=Class2['Survived'], y=Class2['PassengerId']),\n    go.Bar(name='Pclass3', x=Class3['Survived'], y=Class3['PassengerId'])\n])\n# Change the bar mode\nfig.update_layout(barmode='group', title = 'Classwise Chance of survival')\nfig.show()","6d4d982f":"Embarked = pd.DataFrame(df['Embarked'].value_counts())\nEmbarked.reset_index().rename(columns = {'index' : 'E'})","dead5bc6":"# from sklearn.preprocessing import LabelEncoder\n# df['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n# df['Sex'].value_counts()\n# df.head()\nfrom sklearn.preprocessing import OneHotEncoder\nonehotencoder = OneHotEncoder() \ndata = pd.DataFrame(onehotencoder.fit_transform(df[['Sex']]).toarray())\ndata = data.reset_index()\ndf = pd.merge(left = df, right = data, on = 'index')\n#test_df.rename(columns = {0:'Master', 1:'Miss', 2:'Mr', 3:'Mrs', 4:'Rare'}, inplace = True)\ndf = df.drop(columns = ['Sex'])\ndf.sort_values(by = 'PassengerId', inplace = True)","86fffa80":"df.rename(columns = {0:'Female', 1:'Male'}, inplace= True)","c96ee557":"from sklearn.preprocessing import OneHotEncoder\nonehotencoder = OneHotEncoder() \ndata = pd.DataFrame(onehotencoder.fit_transform(df[['Embarked']]).toarray())\ndata = data.reset_index()\ndf = pd.merge(left = df, right = data, on = 'index')\ndf.rename(columns = {0 : 'C', 1 : 'Q', 2 : 'S'}, inplace = True)\ndf.drop(columns = 'Embarked', inplace = True)","b179939f":"onehotencoder = OneHotEncoder() \ndata = pd.DataFrame(onehotencoder.fit_transform(df[['Title']]).toarray())\ndata = data.reset_index()\ndf = pd.merge(left = df, right = data)\ndf.rename(columns = {0:'Master', 1:'Miss', 2:'Mr', 3:'Mrs', 4:'Rare'}, inplace = True)\ndf = df.sort_values(by = 'PassengerId', ascending = True)","89833e9a":"df.drop(columns = 'Title', inplace = True)","67be1b11":"df['Fare'] = np.log1p(df['Fare'])","2878b274":"# from sklearn.feature_selection import SelectKBest, f_classif\n\n# predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"Title\", \"Accompany\"]\n\n# # Perform feature selection\n# selector = SelectKBest(f_classif, k=5)\n# selector.fit(train_df[predictors], train_df[\"Survived\"])\n\n# # Get the raw p-values for each feature, and transform from p-values into scores\n# scores = -np.log10(selector.pvalues_)\n\n# # Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n# plt.bar(range(len(predictors)), scores)\n# plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n# plt.show()","a98693f6":"df.drop(columns = ['index'], inplace = True)","b913c7d3":"df['Age'] = pd.cut(df['Age'], 5)","f4f5f781":"from sklearn.preprocessing import LabelEncoder\ndf['Age'] = LabelEncoder().fit_transform(df['Age'])\ndf['Age'].value_counts()","2faf6b47":"test_df = df[df['PassengerId'] > 891]\ntest_df.shape","e509a3a1":"train_df = df[df['PassengerId'] <= 891]\ntrain_df.shape","7642df28":"test_df.isnull().sum()","0d835988":"test_df.drop(columns = ['Survived'], inplace = True)","215c8bb3":"passenger_id = test_df['PassengerId']\ntest_df.drop(columns = ['PassengerId'], inplace = True)","336e9641":"test_df.isnull().sum()","7bd84125":"x_train = train_df.drop(columns = 'Survived')\ny_train = train_df['Survived']\nx_train.drop(columns = ['PassengerId'], inplace = True)","e7aa72e3":"y_train = pd.DataFrame(y_train)","e8963c71":"y_train = y_train['Survived'].astype(int)","ed5bde80":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(test_df)\nacc_log = round(logreg.score(x_train, y_train) * 100, 2)\nacc_log","dd34a5d8":"from sklearn.model_selection import GridSearchCV\nclf = LogisticRegression()\ngrid_values = {'penalty': ['l1', 'l2'],\n               'C':[1,3,5,7,10,11,12]\n              }\ngrid = GridSearchCV(clf, param_grid = grid_values,scoring = 'recall', cv = 5)\ngrid.fit(x_train, y_train)","55407cac":"grid.best_params_","9a2b94e5":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C= 3, penalty= 'l2')\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(test_df)\nacc_log = round(logreg.score(x_train, y_train) * 100, 2)\nacc_log","d79918f9":"# from sklearn.model_selection import cross_val_score\n# clf = LogisticRegression(C= 10, penalty= 'l1', solver= 'liblinear')\n# scores = cross_val_score(clf, x_train, y_train, cv=5)\n# scores = scores.mean()\n# scores","abcbe917":"from sklearn.model_selection import cross_val_score\nclf = LogisticRegression(C= 3, penalty= 'l2')\nscores = cross_val_score(clf, x_train, y_train, cv=5)\nscores = scores.mean()\nscores","7020b3d0":"# Support Vector Machines\nsvc = SVC(C= 3, gamma= 0.06, kernel= 'linear')\nsvc.fit(x_train, y_train)\nY_pred = svc.predict(test_df)\nacc_svc = round(svc.score(x_train, y_train) * 100, 2)\nacc_svc","b7b7987b":"# from sklearn.model_selection import GridSearchCV \n# param_grid = {'C': [1, 3, 5, 7, 9], \n#               'gamma': [0.06, 0.08, 0.1, 0.12, 0.14], \n#               'kernel': ['rbf']}  \n  \n# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \n  \n# # fitting the model for grid search \n# grid.fit(x_train, y_train) ","53a1c953":"# grid.best_params_","2913d3a8":"# from sklearn.model_selection import cross_val_score\n# clf = SVC(C= 2, gamma = 1, kernel= 'linear')\n# scores = cross_val_score(clf, x_train, y_train, cv=5)\n# scores = scores.mean()\n# scores","28d9274f":"from sklearn.model_selection import cross_val_score\nclf = SVC(C= 3, gamma= 0.06, kernel= 'linear')\nscores = cross_val_score(clf, x_train, y_train, cv=5)\nscores = scores.mean()\nscores","057022e2":"# knn = KNeighborsClassifier(n_neighbors = 3)\n# knn.fit(x_train, y_train)\n# Y_pred = knn.predict(test_df)\n# acc_knn = round(knn.score(x_train, y_train) * 100, 2)\n# acc_knn","c60d7306":"# from sklearn.model_selection import GridSearchCV\n# from sklearn.neighbors import KNeighborsClassifier\n# #making the instance\n# model = KNeighborsClassifier(n_jobs=-1)\n# #Hyper Parameters Set\n# params = {'n_neighbors':[3,4,5,6,7,8,9,10],\n#           'leaf_size':[10,20,30,40,50,60],\n#           'weights':['uniform', 'distance'],\n#           'algorithm':['auto', 'ball_tree','kd_tree','brute'],\n#           'n_jobs':[-1]}\n# #Making models with hyper parameters sets\n# model1 = GridSearchCV(model, param_grid=params, n_jobs=1)\n# #Learning\n# model1.fit(x_train,y_train)","31196e63":"# model1.best_params_","20157789":"knn = KNeighborsClassifier(algorithm = 'auto', leaf_size = 40, n_neighbors = 7, weights = 'uniform')\nknn.fit(x_train, y_train)\nY_pred = knn.predict(test_df)\nacc_knn = round(knn.score(x_train, y_train) * 100, 2)\nacc_knn","b5284111":"# from sklearn.model_selection import cross_val_score\n# clf = KNeighborsClassifier()\n# scores = cross_val_score(clf, x_train, y_train, cv=5)\n# scores = scores.mean()\n# scores","5ba54cf1":"from sklearn.model_selection import cross_val_score\nclf = KNeighborsClassifier(algorithm = 'auto', leaf_size = 40, n_neighbors = 7, weights = 'uniform')\nscores = cross_val_score(clf, x_train, y_train, cv=5)\nscores = scores.mean()\nscores","203c3dc5":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\nY_pred = gaussian.predict(test_df)\nacc_gaussian = round(gaussian.score(x_train, y_train) * 100, 2)\nacc_gaussian","594868c8":"from sklearn.model_selection import cross_val_score\nclf = GaussianNB()\nscores = cross_val_score(clf, x_train, y_train, cv=5)\nscores = scores.mean()\nscores","7fc2e18a":"# decision_tree = DecisionTreeClassifier()\n# decision_tree.fit(x_train, y_train)\n# Y_pred = decision_tree.predict(test_df)\n# acc_decision_tree = round(decision_tree.score(x_train, y_train) * 100, 2)\n# acc_decision_tree","d61ae8c6":"# from sklearn.model_selection import GridSearchCV\n# from sklearn.tree import DecisionTreeClassifier\n# #making the instance\n# model= DecisionTreeClassifier(random_state=0)\n# #Hyper Parameters Set\n# params = {\n#           'max_features': ['auto', 'sqrt', 'log2'],\n#           'min_samples_split': [10,15,20,25,30,35,40,45,50,55,60,65,70], \n#           'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,11],\n#           }\n\n# model1 = GridSearchCV(model, param_grid=params, n_jobs=-1)\n# #Learning\n# model1.fit(x_train,y_train)","0ce2cc8d":"# model1.best_params_","7c6b2565":"decision_tree = DecisionTreeClassifier(max_features= 'auto', min_samples_leaf= 2, min_samples_split= 20)\ndecision_tree.fit(x_train, y_train)\nY_pred = decision_tree.predict(test_df)\nacc_decision_tree = round(decision_tree.score(x_train, y_train) * 100, 2)\nacc_decision_tree","a1eb7840":"from sklearn.model_selection import cross_val_score\nclf = DecisionTreeClassifier(max_features= 'auto', min_samples_leaf= 2, min_samples_split= 20)\nscores = cross_val_score(clf, x_train, y_train, cv=5)\nscores = scores.mean()\nscores","732307e4":"from sklearn.model_selection import cross_val_score\nclf = DecisionTreeClassifier(max_features= 'auto', min_samples_leaf= 2, min_samples_split= 20)\nscores = cross_val_score(clf, x_train, y_train, cv=5)\nscores = scores.mean()\nscores","2bd0edda":"# random_forest = RandomForestClassifier(n_estimators=100)\n# random_forest.fit(x_train, y_train)\n# Y_pred = random_forest.predict(test_df)\n# random_forest.score(x_train, y_train)\n# acc_random_forest = round(random_forest.score(x_train, y_train) * 100, 2)\n# acc_random_forest","10294d1f":"# from sklearn.model_selection import GridSearchCV\n# from sklearn.ensemble import RandomForestClassifier\n# #making the instance\n# model=RandomForestClassifier()\n# #hyper parameters set\n# params = {'criterion':['gini','entropy'],\n#           'n_estimators':[11,13,15,17,19,21],\n#           'min_samples_leaf':[1,2,3,4,5,6],\n#           'min_samples_split':[2,3,4,5,6],\n#           'n_jobs':[-1]}\n# #Making models with hyper parameters sets\n# model1 = GridSearchCV(model, param_grid=params, n_jobs=-1)\n# #learning\n# model1.fit(x_train,y_train)","14c58770":"# model1.best_params_","95e66a98":"random_forest = RandomForestClassifier(criterion= 'gini',\n                                       min_samples_leaf= 6,\n                                       min_samples_split= 6,\n                                       n_estimators= 21,\n                                       n_jobs= -1)\nrandom_forest.fit(x_train, y_train)\nY_pred = random_forest.predict(test_df)\nrandom_forest.score(x_train, y_train)\nacc_random_forest = round(random_forest.score(x_train, y_train) * 100, 2)\nacc_random_forest","171e30d6":"from sklearn.model_selection import cross_val_score\nclf = RandomForestClassifier(criterion= 'gini',\n                            min_samples_leaf= 6,\n                            min_samples_split= 6,\n                            n_estimators= 21,\n                            n_jobs= -1)\nscores = cross_val_score(clf, x_train, y_train, cv=5)\nscores = scores.mean()\nscores","4fbf0fd2":"# from sklearn.model_selection import cross_val_score\n# clf = RandomForestClassifier(n_estimators=100)\n# scores = cross_val_score(clf, x_train, y_train, cv=5)\n# scores = scores.mean()\n# scores","558eb7d1":"from sklearn.ensemble import VotingClassifier\nensemble=VotingClassifier(estimators=[('Decision Tree', decision_tree), \n                                      ('Random Forest', random_forest), \n                                      ('KNN', knn), \n                                      ('SVC', svc), \n                                      ('Logistic Regression', logreg)], \n                                      voting='soft', \n                                      weights=[1,1,2,1,1]).fit(x_train,y_train)\nscores = cross_val_score(clf, x_train, y_train, cv=10)\nacc_voting = scores.mean()*100\nacc_voting\n\n\n# from mlxtend.classifier import StackingClassifier\n# clf = StackingClassifier(classifiers=[random_forest, decision_tree, knn, svc, logreg], \n#                           meta_classifier=logreg)\n# scores = cross_val_score(clf, x_train, y_train, cv=10)\n# acc_stacking = scores.mean()*100\n# acc_stacking\n# #, gaussian\n\n\n\n# # for clf, label in zip([random_forest, decision_tree, gaussian, knn, svc, logreg], ['Random Forest', 'Decision Tree', 'GaussianNB', 'KNN', 'SVC', 'Logistic Regression']):\n# #     scores = cross_val_score(clf, x_train, y_train, cv=10, scoring='accuracy')\n# #     print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","bb66ca50":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes',\n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","d3bf3ce7":"passenger_id = pd.DataFrame(passenger_id)\npassenger_id.head(2)","22bb9efd":"test_df.reset_index(inplace = True)\ntest_df.head(2)","54f82e64":"test_df['PassengerId'] = test_df['index']+1\ntest_df.head(2)","f7018979":"test_df.tail(2)","4950eed3":"train_df.reset_index()\ntest_df.reset_index()\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)","3a47d660":"Out of all the passengers who boarded first class, 141 embarked from C, only 3 from Q while 179 embarked from S","d655e890":"# Prediction","a8ec101e":"### Gaussian Naive Bayes","4a105854":"# Model Evaluation","71942367":"Replacing the age value in the test and train dataset with the median value of the age column in their respective dataset","7ae2fb56":"In above plot, 1 on x-axis indicates survival whereas 0 indicates not survived. As it's visible from the plot above, Passengers of firstclass are more likely to survive than anyone else while passengers of class 3 are least likely to survive.","56a3b2ee":"101 passengers from third class embarked from C, 113 embarked from Q while 495 embarked from S","6a97bc14":"Replacing the embarked column with the most frequently occuring value in the Embarked column since there are a really few na present","4681226c":"### Logistic Regression","7355ec3a":"# Cleaning","6e037dc0":"Cabin column is dropped because there are a lot of na present in them","eebbf30d":"There are 86 missing rows in 'Age', 327 rows in 'Column' and 1 row in 'Fare' column of test dataset","7244955e":"# Reading the dataset","1634cf15":"### Decision Tree Classifier","7ba51dac":"## Voting","1d02d020":"### k-Nearest Neighbour","d417d0e3":"From the above description, its can be seen that the dataset consists of 11 features and 1 traget variable (i.e. Survived)\n\nPassengerId : Unique id of each passenger\nSurvived : Passenger survived or not\nPclass : Ticket class\nName : Name of the passenger\nSex : Gender of passenger\nAge : Age of the passenger\nSibSp : No. of sibbling\/spouse aboard the Titanic\nParch : No. of parents\/children aboard the Titanic\nTicket : Ticket number of the passenger\nFare : Fare of the ticket\nCabin : Cabin number of the passenger\nEmbarked : Port of Embarcation","9393d3c4":"### RandomForestClassifier","7416b6f5":"Replacing the fare column in the test dataset with the median value of the column","a1e48b0c":"### Support Vector Machine","a9069064":"There are 177 empty rows in 'Age' column, 687 empty rows in 'Cabin' column and 2 empty rows in Embarked column of training dataset","e8acdceb":"28 of the total passengers from second class embarked from C. 7 embarked from Q while 242 embarked by S"}}