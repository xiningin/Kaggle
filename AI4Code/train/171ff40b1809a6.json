{"cell_type":{"8198879c":"code","044315fa":"code","47045593":"code","188f4a6c":"code","a8dc2720":"code","af2e426a":"code","02da6a81":"code","6777fca9":"code","787588df":"code","648ef326":"code","7fe78ffd":"code","e9bc820c":"code","daf1e796":"code","70a14820":"code","ae4db1a4":"code","907433ea":"code","2495d4d6":"code","68a986fa":"code","b0d73d70":"code","389b3093":"code","41614b9a":"code","34b46f5f":"code","52d15fb3":"code","e843667a":"code","09f3fb64":"code","a0ff7896":"code","e882b2c0":"code","bed81d1b":"code","9ef8c9b5":"code","a5d7e63e":"code","adff85bd":"code","8a356e46":"code","bce77280":"code","32586de6":"code","0450ff89":"code","b944a2ec":"code","ccee6099":"code","50ad2c64":"code","41c212fe":"code","4ad0bf2a":"code","31c2cb99":"code","0a4a366c":"code","639a5d09":"code","aa87f4bf":"code","e786f95e":"code","040a5d9f":"code","ecd2bf09":"code","924033a4":"code","1f5cace0":"code","783759e3":"code","666ce107":"code","98e72ddb":"code","65fa118e":"code","ac699cc8":"code","f1887d7c":"code","72a2848b":"code","2f3d1c7f":"code","3b8ea733":"code","56d0f914":"code","2ddd4d06":"code","306045e8":"code","767e4836":"code","1ceebfbc":"code","6ea193d7":"code","b1c2821b":"code","64a95040":"code","7e4c613b":"code","840a36f7":"code","5abe052e":"code","e4e1b0e4":"code","8ff40549":"code","f0110ca2":"markdown","abc83d33":"markdown","49d245b9":"markdown","8f7c574f":"markdown","6d2a7923":"markdown","d061a58a":"markdown","11951411":"markdown","1b7c7004":"markdown","2c2b02e6":"markdown","de4e2b38":"markdown","001ce43a":"markdown","dc8f0ce0":"markdown","5fe54ac2":"markdown","7ae5af61":"markdown","b6770fe3":"markdown","7844436d":"markdown","2904e5b5":"markdown","49001a02":"markdown","2c4d329a":"markdown","e8b4ca2e":"markdown","e3275b39":"markdown","4f0024f5":"markdown","bd47f26c":"markdown"},"source":{"8198879c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","044315fa":"import nltk\nfrom textblob import TextBlob","47045593":"blob = TextBlob(\"Jacob is learning NLP\")\nfor noun in blob.noun_phrases:\n    print(noun)","188f4a6c":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity","a8dc2720":"documents = (\n\"I like NLP\",\n\"I am exploring NLP\",\n\"I am a beginner in NLP\",\n\"I want to learn NLP\",\n\"I like advanced NLP\"\n)","af2e426a":"# We will first need to create a vector\ntfidf = TfidfVectorizer()\ntfidf = tfidf.fit_transform(documents)","02da6a81":"tfidf.shape","6777fca9":"# Calculating similarity of first sentence with other sentences\ncosine_similarity(tfidf[0], tfidf)","787588df":"!pip install soundex","648ef326":"import soundex","7fe78ffd":"# Running the soundex function\nworker = soundex.Soundex()","e9bc820c":"worker.soundex(\"natural\")","daf1e796":"worker.soundex(\"nataral\")","70a14820":"worker.soundex(\"nutural\")","ae4db1a4":"worker.soundex(\"language\")","907433ea":"worker.soundex(\"processing\")","2495d4d6":"# Achieving this using NLTK\nText = \"I like to eat apples. They are healthy\"\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nstop_words = set(stopwords.words('english'))","68a986fa":"# First we do sentence tokenization\ntoks = sent_tokenize(Text)\nfor i in toks:\n    # Word tokenize and removing stop words\n    words = nltk.word_tokenize(i)\n    words = [w for w in words if not w in stop_words]\n    # POS\n    tags = nltk.pos_tag(words)\n    print(tags)","b0d73d70":"sent = \"John is studying at Stanford University in California\"","389b3093":"from nltk import ne_chunk\nfrom nltk import word_tokenize","41614b9a":"s = ne_chunk(nltk.pos_tag(word_tokenize(sent)), binary=False)\nprint(s)","34b46f5f":"!python -m spacy download en_core_web_md","52d15fb3":"!python -m spacy link en_core_web_md en","e843667a":"import spacy","09f3fb64":"nlp = spacy.load('en')","a0ff7896":"doc = nlp(u'Samsung is launching their new phone for 1000 dollars in Amsterdam')","e882b2c0":"for i in doc.ents:\n    print(i.text, i.start_char, i.end_char, i.label_)","bed81d1b":"# Let us work with an example\ns1 = \"I am learning NLP, it is very interesting and exciting. it includes machine learning and deep learning\"\ns2 = \"My father is a data scientist and he is nlp expert\"\ns3 = \"My sister has experience in android development\"","9ef8c9b5":"s_final = [s1, s2, s3]\ns_final","a5d7e63e":"# Cleaning the data\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport string\n\nstop = set(stopwords.words('english'))\nexclude = set(string.punctuation)\nlemma = WordNetLemmatizer()\ndef clean(doc):\n    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n    punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n    return normalized\ndoc_clean = [clean(doc).split() for doc in s_final]\ndoc_clean","adff85bd":"# We will prepare a document term matrix\nimport gensim\nfrom gensim import corpora\n\n# Create term dictionary of our corpus \n# Each unique term is assigned an index\n\ndictionary = corpora.Dictionary(doc_clean)\nprint(dictionary)","8a356e46":"dictionary[4]","bce77280":"# Converting list of docs into matrix using dictionary prepared above\ndoc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\ndoc_term_matrix","32586de6":"Lda = gensim.models.ldamodel.LdaModel\n# Running and Training LDA model on the document term matrix\n# for 3 topics.\nldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)","0450ff89":"print(ldamodel.print_topics())","b944a2ec":"!pip install -U wn==0.0.22","ccee6099":"!pip install pywsd","50ad2c64":"from nltk.stem import PorterStemmer\nfrom itertools import chain\nfrom pywsd.lesk import simple_lesk","41c212fe":"sentences = ['The river bank was full of fish today',\n            'Christopher works at the bank as an accountant']","4ad0bf2a":"print(\"Context-1: \", sentences[0])\npreds = simple_lesk(sentences[0], 'bank')\nprint(\"Sense: \", preds)\nprint(\"Definition: \", preds.definition())","31c2cb99":"print(\"Context-2: \", sentences[1])\npreds = simple_lesk(sentences[1], 'bank')\nprint(\"Sense: \", preds)\nprint(\"Definition: \", preds.definition())","0a4a366c":"# Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport string\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom textblob import TextBlob\nfrom nltk.stem import PorterStemmer\nfrom textblob import Word\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nimport sklearn.feature_extraction.text as text\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.model_selection import train_test_split","639a5d09":"df = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv')","aa87f4bf":"df.head()","e786f95e":"df.shape","040a5d9f":"df.isnull().sum()","ecd2bf09":"# We can extract the useful columns\ndf = df[['v1', 'v2']]\ndf.head()","924033a4":"df.dtypes","1f5cace0":"# Converting all data into lowercase\ndf.rename(columns={\"v1\":\"label\", \"v2\":\"message\"}, inplace=True)","783759e3":"df.head()","666ce107":"# Converting to lowercase\ndf['message'] = df['message'].apply(lambda x: \" \".join(x.lower() for x in x.split()))","98e72ddb":"# Removing stop-words\nwords = stopwords.words('english')\ndf['message'] = df['message'].apply(lambda x: \" \".join(x for x in x.split() if x not in words))","65fa118e":"# Stemming\nporter = PorterStemmer()\ndf['message'] = df['message'].apply(lambda x: \" \".join(porter.stem(word) for word in x.split()))","ac699cc8":"# Lemmatization\ndf['message'] = df['message'].apply(lambda x: \" \".join(Word(word).lemmatize() for word in x.split()))","f1887d7c":"df.head()","72a2848b":"# Proceeding to split the data\nx_train, x_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.2)","2f3d1c7f":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","3b8ea733":"# Converting our categorical variables to numerical\nle = preprocessing.LabelEncoder()\ny_train = le.fit_transform(y_train)\ny_test = le.fit_transform(y_test)","56d0f914":"y_train","2ddd4d06":"tfidf = TfidfVectorizer(analyzer='word')\ntfidf.fit(df['message'])","306045e8":"x_train_t = tfidf.transform(x_train)\nx_test_t = tfidf.transform(x_test)","767e4836":"x_train_t.shape","1ceebfbc":"# Creating a generalized model training function\ndef model_training(clf, x_train, y_train, x_test):\n    # fit the training dataset on the classifier\n    clf.fit(x_train, y_train)\n    preds = clf.predict(x_test)\n    return metrics.accuracy_score(preds, y_test)","6ea193d7":"accuracy = model_training(naive_bayes.MultinomialNB(), x_train_t, y_train, x_test_t)\nprint(\"Accuracy using Naive Bayes:\", accuracy)","b1c2821b":"accuracy = model_training(linear_model.LogisticRegression(), x_train_t, y_train, x_test_t)\nprint(\"Accuracy using logistic regression:\", accuracy)","64a95040":"# Our sample data\nreview1 = \"The phone is amazing to use. It is fast and has a strong build quality\"\nreview2 = \"Using this phone has been a bad experience. It is sloggy, bulky, clunky and annoying to use.\"","7e4c613b":"# Pre-processing function\ndef processRow(row):\n    import re\n    import nltk\n    from textblob import TextBlob\n    from nltk.corpus import stopwords\n    from nltk.stem import PorterStemmer\n    from textblob import Word\n    from nltk.util import ngrams\n    import re\n    from wordcloud import WordCloud, STOPWORDS\n    from nltk.tokenize import word_tokenize\n    tweet = row\n    #Lower case\n    tweet.lower()\n    #Removes unicode strings like \"\\u002c\" and \"x96\"\n    tweet = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r\"\", tweet)\n    tweet = re.sub(r'[^\\x00-\\x7f]',r\"\",tweet)\n    #convert any url to URL\n    tweet = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))','URL',tweet)\n    #Convert any @Username to \"AT_USER\"\n    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n    #Remove additional white spaces\n    tweet = re.sub('[\\s]+', ' ', tweet)\n    tweet = re.sub('[\\n]+', ' ', tweet)\n    #Remove not alphanumeric symbols white spaces\n    tweet = re.sub(r'[^\\w]', ' ', tweet)\n    #Removes hastag in front of a word \"\"\"\n    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n    #Replace #word with word\n    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n    #Remove :( or :)\n    tweet = tweet.replace(':)',\"\")\n    tweet = tweet.replace(':(',\"\")\n    #remove numbers\n    tweet = \"\".join([i for i in tweet if not i.isdigit()])\n    #remove multiple exclamation\n    tweet = re.sub(r\"(\\!)\\1+\", ' ', tweet)\n    #remove multiple question marks\n    tweet = re.sub(r\"(\\?)\\1+\", ' ', tweet)\n    #remove multistop\n    tweet = re.sub(r\"(\\.)\\1+\", ' ', tweet)\n    #lemma\n    from textblob import Word\n    tweet =\" \".join([Word(word).lemmatize() for word in tweet.split()])\n    #stemmer\n    #st = PorterStemmer()\n    #tweet=\" \".join([st.stem(word) for word in tweet.split()])\n    #Removes emoticons from text\n    tweet = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=\/|:\/|X\\-\\(|>\\.<|>=\\(|D:', \"\", tweet)\n    #trim\n    tweet = tweet.strip('\\'\"')\n    row = tweet\n    return row","840a36f7":"review1 = processRow(review1)\nreview1","5abe052e":"review2 = processRow(review2)\nreview2","e4e1b0e4":"# We get a positive score - Positive sentiment\nblob = TextBlob(review1)\nblob.sentiment","8ff40549":"# We get a negative score - negative sentiment\nblob = TextBlob(review2)\nblob.sentiment","f0110ca2":"We will now create a matrix that will consist of numerical values and their frequency. The index will be mapped from the dictionary","abc83d33":"We can see how the first and last sentence have high similarity scores.","49d245b9":"We can see the weights associated with the topics. They are currently very close to each other. We can perform this on huge data sets to extract significant topics with high weights. ","8f7c574f":"Here we can see \n* John is tagged as a person\n* Stanford is tagged as an organization \n* California is tagged as a Geopolitical entity (GPE)","6d2a7923":"Before getting into recipes, let\u2019s understand the NLP pipeline and life\ncycle first. There are so many concepts we are implementing in this book,\nand we might get overwhelmed by the content of it. To make it simpler\nand smoother, let\u2019s see what is the flow that we need to follow for an NLP\nsolution.\nFor example, let\u2019s consider customer sentiment analysis and\nprediction for a product or brand or service.\n\n* Define the Problem: Understand the customer sentiment across the products.\n* Understand the depth and breadth of the problem: Understand the customer\/user sentiments across the product; why we are doing this? What is the business impact? Etc.\n* Data requirement brainstorming: Have a brainstorming activity to list out all possible data points.\n* Data collection: Based on the data andthe problem, we might have to incorporate different data collection methods. In this case, we can use web scraping and Twitter APIs.\n* Text Preprocessing: We know that data won\u2019t always be clean. We need to spend a significant amount of time to process it and extract insight out of it using different methods \n* Text to feature: As we discussed, texts are characters and machines will have a tough time understanding them. We have to convert them to features that machines and algorithms can understand using encoding methodolgies (discussed in a separate notebook)\n* Machine learning\/Deep learning: Machine learning\/Deep learning is a part of an artificial intelligence umbrella that will make systems automatically learn patterns in the data without being programmed. Most of the NLP solutions are based on this, and since we converted text to features, we can leverage machine learning or deep learning algorithms to achieve the goals like text classification, natural language generation, etc.\n* Insights and deployment: There is absolutely no use for building NLP solutions without proper insights being communicated to the business. Always take time to connect the dots between model\/analysis output and the business, thereby creating the maximum impact.","d061a58a":"<h2>Phonetic matching<\/h2>\n\nRough matching of two words or sentences, also creates a string as an encoded version of the text or word. It is useful in searching corpus of texts, correcting spelling errors etc.","11951411":"<h2>Text Disambiguation<\/h2>\n\nSentences have ambiguity that arises due to different meanings of words for different contexts:\n\nFor eg: **Bank may have different meanings in:**\n\n* The bank gave me a loan\n* She was at the river bank\n\nWe need a method to understand and clear up this ambiguity. The Lesk algorithm can assist us:","1b7c7004":"<h2>Basics on Sentiment Analysis<\/h2>\nSentiment analysis is one of the widely\nused techniques across the industries to understand the sentiments of the\ncustomers\/users around the products\/services. Sentiment analysis gives\nthe sentiment score of a sentence\/statement tending toward positive or\nnegative.","2c2b02e6":"<h2>Noun Phrase extraction","de4e2b38":"We will be working with Cosine similarity as it is one of the most used, and simplest methods to achieve similarity scores:","001ce43a":"# Advanced Natural language processing and handling basic scenarios\n\nWe will cover various advanced NLP techniques in this notebook, while also leveraging some popular ML algorithms to extract data from next.\n\nSome of the parts covered include:\n* Noun phrase extraction\n* Pronounciation\/Sound classification\n* Speech tagging\n* Text similarity\n* Word sense disambiguation \n\nWe will also go through basics of different scenarios in NLP, including:\n* Sentiment analysis using Blobs\n* Classification\n* Text Disambiguation","dc8f0ce0":"<h2>Classifying data - working with spam<\/h2>\n\nWe will put all our currently gathered knowledge from basic NLP techniques to build a spam classifier using vanilla machine learning algorithms and vectors.","5fe54ac2":"The returned values of POS tagging are many, here are a few to get you started with:\n\n* CC coordinating conjunction\n* CD cardinal digit\n* DT determiner\n* EX existential there (like: \u201cthere is\u201d ... think of it like \u201cthere exists\u201d)\n* FW foreign word\n* IN preposition\/subordinating conjunction\n* JJ adjective \u2018big\u2019\n* JJR adjective, comparative \u2018bigger\u2019\n* JJS adjective, superlative \u2018biggest\u2019\n* LS list marker 1)\n* MD modal could, will\n* NN noun, singular \u2018desk\u2019\n* NNS noun plural \u2018desks\u2019","7ae5af61":"<h2>Tagging parts of speech<\/h2>","b6770fe3":"Cleaning up the data","7844436d":"<h2>Extracting topics from text<\/h2>\n\nIf we have an online library with multiple departments based on the genre, we should be able to identify which section a new book belongs to. This process can use topic modelling for NLP. This is the procedure of document tagging and clustering.","2904e5b5":"<h2>Entity extraction<\/h2>\n\nThis is also known as named entity recognition. We have multiple different libraries to help us in this task that include NLTK chunker, SpaCy, WatsonNLU, AlchemyAPI, Google Cloud etc.","49001a02":"Speech tagging enables labeling of words with parts of speech such as noun, adjective, verb etc. It is the foundation for named entity recognition, answering questions and word sense disambiguation. There are two main ways to build taggers\n\n* Rule based - Created manually - tag a word belonging to a particular POS\n\n* Stochastic based - Capture sequence of words and tag probability using hidden Markov models.","2c4d329a":"<h2>Text similarity<\/h2>\n\nThis can be achieved in multiple ways:\n\n* Cosine similarity: Calculates the cosine of the angle between the two vectors.\n* Jaccard similarity: The score is calculated using the intersection or union of words.\n\n    `Jaccard Index = (the number in both sets) \/ (the number in either set) * 100`.\n\n* Levenshtein distance: Minimal number of insertions, deletions, and replacements required for transforming string \u201ca\u201d into string \u201cb.\u201d\n* Hamming distance: Number of positions with the same symbol in both strings. But it can be defined only for strings with equal length","e8b4ca2e":"Here we see it has recognized:\n* Samsung as an organization (ORG)\n* 1000 Dollars as money (MONEY)\n* Amsterdam as a geopolitical entity (GPE)","e3275b39":"We can see in the first list element we have (4, 3). 4 refers to the index of 'learning' and 3 is the frequency of the word in the first sentence. We will now create an LDA model","4f0024f5":"We see that we get the same outputs for words that are the same but are only slightly different. This helps in identifying typos and also formatting:","bd47f26c":"We can do this through training our models on data like the spam classification tasks above, but with different sentiments. Here we will proceed with a library implementation. The other notebooks will cover deep-learning implementations using RNNs, LSTMs and transformers.\n\nThe TextBlob library will give us two metrics:\n* Polarity - Range of [-1, 1] wehere 1 means a positive statement and -1 means a negative statement\n\n* Subjectivity - Refers to it being a public opinion and not factual information [0, 1]"}}