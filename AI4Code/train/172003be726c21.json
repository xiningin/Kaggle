{"cell_type":{"70734dc7":"code","490ab79f":"code","1e745b62":"code","e0d637a5":"code","c5bcd6df":"code","809e0422":"code","54b0ea7b":"code","314d49cf":"code","bb0ffb87":"code","3f333892":"code","0013477f":"code","1d1a902b":"code","0d497265":"code","30231cc6":"code","f7ed28af":"code","f8848a56":"code","22185da4":"code","9dceb746":"code","f763aaaf":"code","f82c3e73":"code","1661f549":"code","cf8b15d5":"code","4ab2b915":"code","c8934622":"code","f2cab66a":"code","1d9da2d0":"code","ad44f7be":"code","0b10252c":"code","275e00c3":"markdown","97ef9873":"markdown","d4882f5c":"markdown","c76a1c5b":"markdown","c565b675":"markdown","023ddfc9":"markdown","5373b94c":"markdown","1f6c595b":"markdown","ed9fd890":"markdown","fdd1cc11":"markdown","8e6076a1":"markdown","7623339f":"markdown","60d791e1":"markdown","72220a64":"markdown","9815eb87":"markdown","0edffe75":"markdown","1d27c1d7":"markdown","a2e4164e":"markdown"},"source":{"70734dc7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom  sklearn.ensemble import RandomForestRegressor\nplt.style.use('fivethirtyeight')\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","490ab79f":"train = pd.read_csv('\/kaggle\/input\/mldub-comp1\/train_data.csv')\ntest = pd.read_csv('\/kaggle\/input\/mldub-comp1\/test_data.csv')\nsample_sub = pd.read_csv('\/kaggle\/input\/mldub-comp1\/sample_sub.csv')","1e745b62":"train.head()","e0d637a5":"test.head()","c5bcd6df":"train.info()","809e0422":"numerics = ['photos', 'videos', 'comments']\n\nfor feature in numerics:\n    plt.figure()\n    train[feature].hist()\n    plt.title(feature)\n    plt.show()","54b0ea7b":"# Checking minimum of each feature to safely use log transformation.\nfor feature in numerics:\n    print('{} min value: {}'.format(feature, train[feature].min()))","314d49cf":"# To use log the feature must be a positive number, so let's take abs and log then.\n# Do not forget to apply the same transformation to test data as well!\n# We are safe to use abs, because negative number of videos probably indicates parsing\n# error on data preparation stage.\n\nfor feature in numerics:\n    train[feature] = np.log1p(np.abs(train[feature]))\n    test[feature] = np.log1p(np.abs(test[feature]))","bb0ffb87":"for feature in numerics:\n    plt.figure()\n    train[feature].hist()\n    plt.title(feature)\n    plt.show()","3f333892":"plt.figure()\ntrain['target_variable'].hist()\nplt.title('Target distribution')\nplt.show()\n\nplt.figure()\ntrain['target_variable'].apply(lambda x: np.log1p(x-train['target_variable'].min())).hist()\nplt.title('Log-transformed target distribution')\nplt.show()","0013477f":"train['about'].head()","1d1a902b":"train['about_len'] = train['about'].fillna('').apply(len)\ntest['about_len'] = test['about'].fillna('').apply(len)","0d497265":"train['about_len'].hist(alpha=0.4)\ntest['about_len'].hist(alpha=0.4)","30231cc6":"plt.figure()\nplt.scatter(train['about_len'], train['target_variable'])\nplt.title('Target vs. about section length')\nplt.show()","f7ed28af":"train['status'].unique()","f8848a56":"test['status'].unique()","22185da4":"for df in [train, test]:\n    df.loc[~df['status'].isin(['Deadpool', 'Submission', 'Confirmed']), 'status'] = 'Unknown'","9dceb746":"# Check results\ntrain['status'].unique()","f763aaaf":"# Here we use manual mapping, you can use other tools instead.\nmapping = {'Deadpool': 0,\n           'Submission': 1,\n           'Confirmed': 2,\n           'Unknown': -1}\n\ntrain['status'] = train['status'].map(mapping)\ntest['status'] = test['status'].map(mapping)","f82c3e73":"selected_features = ['videos', 'comments', 'photos', 'about_len', 'status']\n\nX_train = train[selected_features]\ny_train = train['target_variable']\n\nX_test = test[selected_features]","1661f549":"# Creating Kfold splitter object.\ncv = KFold(3, shuffle=True, random_state=42)","cf8b15d5":"model = RandomForestRegressor(n_estimators=10,\n                              random_state=42,\n                              n_jobs=-1,\n                              verbose=2)","4ab2b915":"cv_results = cross_val_score(model,\n                             X_train,\n                             y_train,\n                             cv=cv,\n                             scoring='neg_mean_squared_error')","c8934622":"# There is no RMSE score function in sklearn, so we use the MSE option\n# and take the square root then. You can check that it is the same :)\nnp.sqrt([-x for x in cv_results])","f2cab66a":"model.fit(X_train, y_train)","1d9da2d0":"preds = model.predict(X_test)","ad44f7be":"submission = sample_sub\nsubmission['target_variable'] = preds","0b10252c":"submission.to_csv('submission.csv', index=False)","275e00c3":"Looks like there are 3 numeric features in the dataset (exept target and id): photos, videos and comments. Let's plot their destributions.","97ef9873":"Let's check how this feature is distributed in train and test sets.","d4882f5c":"Looks like memes with length of 'About' section > 1000 are not very popular...","c76a1c5b":"Let's check if the data were uploaded correctly.","c565b675":"## Categorical features\nLet's explore one of the categories from our data.","023ddfc9":"## Validation and model\nFirst, let's pick features we are going to use to build the model","5373b94c":"Now let's perform label encoding for the category.","1f6c595b":"Looks like we have kind of long-tailed distribution so let's take log and plot the distribution again.","ed9fd890":"For validation we will use KFold technique.","fdd1cc11":"Let's check also the target distribution.","8e6076a1":"## Numeric features","7623339f":"Let's try to create new numeric feature using simpliest option - just count the number of letters in the text feature.","60d791e1":"# Reading the data","72220a64":"Let's check the distributions after the transformation.","9815eb87":"## EDA & feature engineering\nLet's dig into data properties and try to generate some features!","0edffe75":"There are some weird data points in this category, let's just replace them with 'Unknown' for both train and test.","1d27c1d7":"Now let's see how this feature interacts with target.","a2e4164e":"## Text features\nWe have multiple text features in the dataset so let's take one of them and generate some new features for our model."}}