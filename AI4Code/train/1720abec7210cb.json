{"cell_type":{"4f0ac2ac":"code","db5145d7":"code","ab4d1bc2":"code","c199bbde":"code","0cc7f764":"code","dbef4e75":"code","1bdc53ff":"code","d7933f00":"code","75209844":"code","a3b6dced":"code","10171d0f":"markdown","efd96952":"markdown","2ead4622":"markdown","c0371243":"markdown","8d982307":"markdown","13df9bab":"markdown","4bf1b605":"markdown"},"source":{"4f0ac2ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","db5145d7":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\n\nimport gc\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import activations,callbacks\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import initializers\n\nfrom keras.models import Model","ab4d1bc2":"train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")\nsubmission = submission.set_index('id')","c199bbde":"targets = pd.get_dummies(train['target'])","0cc7f764":"def custom_metric(y_true, y_pred):\n    y_pred = K.clip(y_pred, 1e-15, 1-1e-15)\n    loss = K.mean(cce(y_true, y_pred))\n    return loss\n\ncce = tf.keras.losses.CategoricalCrossentropy()\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_custom_metric', min_delta=1e-05, patience=5, verbose=0,\n    mode='min', baseline=None, restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_custom_metric', factor=0.7, patience=2, verbose=0,\n    mode='min')","dbef4e75":"def conv_model():\n\n    conv_inputs = layers.Input(shape = (75))\n    #----------- Embedding layers ----------------------\n    embed = layers.Embedding (input_dim = 354, \n                              output_dim = 7,\n                              embeddings_regularizer='l2')(conv_inputs)\n    #----------- Convolution layers ----------------------\n    embed = layers.Conv1D(12,1,activation = 'relu')(embed)        \n    embed = layers.Flatten()(embed)\n    hidden = layers.Dropout(0.3)(embed)\n    \n    #----------- Residual blocks layers ----------------------\n    hidden = tfa.layers.WeightNormalization(\n                layers.Dense(\n                units=32,\n                activation ='selu',\n                kernel_initializer = \"lecun_normal\"))(hidden)\n    \n    output = layers.Dropout(0.3)(layers.Concatenate()([embed, hidden]))\n    output = tfa.layers.WeightNormalization(\n    layers.Dense(\n                units = 32,\n                activation='relu',\n                kernel_initializer = \"lecun_normal\"))(output) \n    output = layers.Dropout(0.4)(layers.Concatenate()([embed, hidden, output]))\n    output = tfa.layers.WeightNormalization(\n    layers.Dense(\n                units = 32, \n                activation = 'elu',\n                kernel_initializer = \"lecun_normal\"))(output)\n    \n    #----------- Final layer -----------------------\n    conv_outputs = layers.Dense(\n                units = 9, \n                activation ='softmax',\n                kernel_initializer =\"lecun_normal\")(output)\n    \n    #----------- Model instantiation  ---------------\n    model = Model(conv_inputs,conv_outputs)\n    \n    return model","1bdc53ff":"oof_NN_a = np.zeros((train.shape[0],9))\npred_NN_a = np.zeros((test.shape[0],9))\n\nN_FOLDS = 20\nSEED = 2021\nEPOCH = 50\n\n\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\nfor fold, (tr_idx, ts_idx) in enumerate(skf.split(train,train.iloc[:,-1])):\n    print(f\"\\n ====== TRAINING FOLD {fold} =======\\n\")\n\n    X_train = train.iloc[:,1:-1].iloc[tr_idx]\n    y_train = targets.iloc[tr_idx]\n    X_test = train.iloc[:,1:-1].iloc[ts_idx]\n    y_test = targets.iloc[ts_idx]\n\n    K.clear_session()\n\n    #================= NN CONV MODEL training =========\n    \n    print(\"\\n-----Convolution model Training----\\n\")\n\n    model_conv = conv_model()\n\n    model_conv.compile(loss='categorical_crossentropy', \n                            optimizer = keras.optimizers.Adam(learning_rate=2e-4), \n                            metrics=custom_metric)\n    model_conv.fit(X_train, y_train,\n              batch_size = 256, epochs = EPOCH,\n              validation_data=(X_test, y_test),\n              callbacks=[es, plateau],\n              verbose = 0)\n   \n    #============== Convolution Model prediction ==========\n\n    pred_a = model_conv.predict(X_test) \n    oof_NN_a[ts_idx] += pred_a \n    score_NN_a = log_loss(y_test, pred_a)\n    print(f\"\\nFOLD {fold} Score convolution model: {score_NN_a}\\n\")\n    pred_NN_a += model_conv.predict(test.iloc[:,1:]) \/ N_FOLDS \n \nscore_a = log_loss(targets, oof_NN_a)\nprint(f\"\\n=== FINAL SCORE CONVOLUTION MODEL : {score_a}===\\n\") ","d7933f00":"pred_embedding = pred_NN_a","75209844":"submission = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")\nsubmission['Class_1']=pred_embedding[:,0]\nsubmission['Class_2']=pred_embedding[:,1]\nsubmission['Class_3']=pred_embedding[:,2]\nsubmission['Class_4']=pred_embedding[:,3]\nsubmission['Class_5']=pred_embedding[:,4]\nsubmission['Class_6']=pred_embedding[:,5]\nsubmission['Class_7']=pred_embedding[:,6]\nsubmission['Class_8']=pred_embedding[:,7]\nsubmission['Class_9']=pred_embedding[:,8]","a3b6dced":"submission.to_csv(\"Keras_7.csv\", index=False)","10171d0f":"### Preparing for Submission","efd96952":"### Training the Model","2ead4622":"### Saving the predicted values","c0371243":"### Setting Up the metrics for Model","8d982307":"### Loading the Dataset","13df9bab":"### Importing Important LIbraries","4bf1b605":"### Model Creation"}}