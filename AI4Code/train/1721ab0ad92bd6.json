{"cell_type":{"2c3bd047":"code","e61ee363":"code","d412e426":"code","f03f810e":"code","7df89246":"code","b1f6fd4a":"code","2f53a790":"code","7e07e3ae":"code","a05c7442":"code","118ec410":"code","a9c285f1":"code","b4dd3ac8":"code","871c9c61":"code","1e37872a":"code","883d8997":"code","0af1c111":"code","e35ba302":"code","e9af1bb2":"code","138ff34e":"code","fa4fc00f":"code","07c6a5b1":"markdown","65cc1bcd":"markdown","d9e87f24":"markdown","c566f726":"markdown","29b806a0":"markdown","5fed3dc3":"markdown"},"source":{"2c3bd047":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFECV\ntrain = pd.read_csv(\"..\/input\/train.csv\")\nholdout = pd.read_csv(\"..\/input\/test.csv\")\ntrain.head()","e61ee363":"holdout.head()","d412e426":"train.info()","f03f810e":"train.isnull().sum()","7df89246":"holdout.isnull().sum()","b1f6fd4a":"def process_missing(df):\n    df[\"Fare\"] = df[\"Fare\"].fillna(train[\"Fare\"].mean())\n    df[\"Embarked\"] = df[\"Embarked\"].fillna(\"S\")\n    return df\n\ndef process_age(df):\n    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n    cut_points = [-1,0,5,12,18,35,60,100]\n    label_names = [\"Missing\",\"Infant\",\"Child\",\"Teenager\",\"Young Adult\",\"Adult\",\"Senior\"]\n    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n    return df\n\ndef process_fare(df):\n    cut_points = [-1,12,50,100,1000]\n    label_names = [\"0-12\",\"12-50\",\"50-100\",\"100+\"]\n    df[\"Fare_categories\"] = pd.cut(df[\"Fare\"],cut_points,labels=label_names)\n    return df\n\ndef process_cabin(df):\n    df[\"Cabin_type\"] = df[\"Cabin\"].str[0]\n    df[\"Cabin_type\"] = df[\"Cabin_type\"].fillna(\"Unknown\")\n    df = df.drop('Cabin',axis=1)\n    return df\n\ndef process_titles(df):\n    titles = {\n        \"Mr\" :         \"Mr\",\n        \"Mme\":         \"Mrs\",\n        \"Ms\":          \"Mrs\",\n        \"Mrs\" :        \"Mrs\",\n        \"Master\" :     \"Master\",\n        \"Mlle\":        \"Miss\",\n        \"Miss\" :       \"Miss\",\n        \"Capt\":        \"Officer\",\n        \"Col\":         \"Officer\",\n        \"Major\":       \"Officer\",\n        \"Dr\":          \"Officer\",\n        \"Rev\":         \"Officer\",\n        \"Jonkheer\":    \"Royalty\",\n        \"Don\":         \"Royalty\",\n        \"Sir\" :        \"Royalty\",\n        \"Countess\":    \"Royalty\",\n        \"Dona\":        \"Royalty\",\n        \"Lady\" :       \"Royalty\"\n    }\n    extracted_titles = df[\"Name\"].str.extract(' ([A-Za-z]+)\\.',expand=False)\n    df[\"Title\"] = extracted_titles.map(titles)\n    return df\n\ndef create_dummies(df,column_name):\n    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n    df = pd.concat([df,dummies],axis=1)\n    return df","2f53a790":"def pre_process(df):\n    df = process_missing(df)\n    df = process_age(df)\n    df = process_fare(df)\n    df = process_titles(df)\n    df = process_cabin(df)\n\n    for col in [\"Age_categories\",\"Fare_categories\",\n                \"Title\",\"Cabin_type\",\"Sex\"]:\n        df = create_dummies(df,col)\n    \n    return df\n\ntrain = pre_process(train)\nholdout = pre_process(holdout)\ntrain.head(2)","7e07e3ae":"dff=train.groupby('Pclass')\nx=train.pivot_table(index='Pclass',values=['Fare','Age','Survived'])\nx","a05c7442":"import matplotlib.pyplot as plt\n%matplotlib inline\nexplore_cols = [\"SibSp\",\"Parch\",\"Survived\"]\nexplore = train[explore_cols].copy()\nexplore[\"familysize\"] = explore[[\"SibSp\",\"Parch\"]].sum(axis=1)\nexplore.drop(\"Survived\",axis=1).plot.hist(alpha=0.5,bins=10)\nplt.xticks(range(11))\nplt.show()","118ec410":"for col in explore.columns.drop(\"Survived\"):\n    pivot = explore.pivot_table(index=col,values=\"Survived\")\n    pivot.plot.bar(ylim=(0,1),yticks=np.arange(0,1,.1))\n    plt.show()","a9c285f1":"plt.scatter(train['Fare'],train['Age'],c=train['Survived'])\nplt.legend()","b4dd3ac8":"pivot = train.pivot_table(index='Survived',values=\"Fare\")\npivot.plot.bar(color=\"lightgreen\")\nplt.show()","871c9c61":"pivot = train.pivot_table(index='Embarked',values=\"Survived\")\npivot.plot.bar()\nplt.show()","1e37872a":"x=x.drop(['Survived'],axis=1)\nx.plot(kind='bar')","883d8997":"def process_isalone(df):\n    df[\"familysize\"] = df[[\"SibSp\",\"Parch\"]].sum(axis=1)\n    df[\"isalone\"] = 0\n    df.loc[(df[\"familysize\"] == 0),\"isalone\"] = 1\n    df = df.drop(\"familysize\",axis=1)\n    return df\n\ntrain = process_isalone(train)\nholdout = process_isalone(holdout)\ncorr_matrix=train.corr()\ncorr_matrix.shape","0af1c111":"abs_matrix=corr_matrix['Survived'].abs()\nsorted_matrix=abs_matrix[abs_matrix>=0.2]\nindexed_matrix=sorted_matrix.index\nindexed_matrix","e35ba302":"temp_df=train[indexed_matrix]\ncorr_temp_df=temp_df.corr()\nimport seaborn as sns\nsns.heatmap(corr_temp_df,annot=True)","e9af1bb2":"def select_features(df):\n    df = df.select_dtypes([np.number]).dropna(axis=1)\n    all_X = df.drop([\"Survived\",\"PassengerId\"],axis=1)\n    all_y = df[\"Survived\"]\n    \n    clf = LogisticRegression(random_state=4)\n    selector = RFECV(clf,cv=10)\n    selector.fit(all_X,all_y)\n    \n    best_columns = list(all_X.columns[selector.support_])\n    print(\"Best Columns \\n\"+\"-\"*12+\"\\n{}\\n\".format(best_columns))\n    \n    return best_columns\n\ncols = select_features(train)","138ff34e":"sns.set(style='white')\nmask=np.zeros_like(train[cols].corr(),dtype=np.bool)\nmask[np.triu_indices_from(mask)]=True\nf,ax=plt.subplots(figsize=(11,9))\ncmap=sns.diverging_palette(220,10,as_cmap=True)\nsns.heatmap(train[cols].corr(),mask=mask,cmap=cmap,annot=True)","fa4fc00f":"from sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nimport warnings\n#warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\ndef select_model(df,features):\n    \n    all_X = df[features]\n    all_y = df[\"Survived\"]\n    models = [\n        {\n            \"name\": \"LogisticRegression\",\n            \"estimator\": LogisticRegression(),\n            \"hyperparameters\":\n                {\n                    \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"]\n                }\n        },\n        {\n            \"name\": \"SupportVectorMachine\",\n            \"estimator\": SVC(),\n            \"hyperparameters\":\n            {\n                \"kernel\":[\"linear\", \"rbf\",\"poly\",\"sigmoid\"]\n            }\n        },\n        {\n            \"name\": \"RandomForestClassifier\",\n            \"estimator\": RandomForestClassifier(random_state=1),\n            \"hyperparameters\":\n                {\n                    \"n_estimators\": [5, 10, 14,15,20],\n                    \"criterion\": [\"entropy\", \"gini\"],\n                    \"max_depth\": [10,15,20],\n                    \"max_features\": [\"log2\", \"sqrt\",\"auto\"],\n                    \"min_samples_leaf\": [1, 3,4,5]\n                }\n        }\n    ]\n\n    for model in models:\n        print(model['name'])\n        print('-'*len(model['name']))\n\n        grid = GridSearchCV(model[\"estimator\"],\n                            param_grid=model[\"hyperparameters\"],\n                            cv=10)\n        grid.fit(all_X,all_y)\n        model[\"best_params\"] = grid.best_params_\n        model[\"best_score\"] = grid.best_score_\n        model[\"best_model\"] = grid.best_estimator_\n\n        print(\"Best Score: {}\".format(model[\"best_score\"]))\n        print(\"Best Parameters: {}\\n\".format(model[\"best_params\"]))\n\n    return models\ncolumn_list=[\"Title_Mr\",\"Pclass\",\"Fare\",\"Age\",\"SibSp\",\"Sex_female\",\"Cabin_type_Unknown\"]\nresult = select_model(train,cols)","07c6a5b1":"# Feature Selection\nHere the sklearn library method RFECV is used - Feature ranking with recursive feature elimination and cross-validated selection of the best number of features.","65cc1bcd":"# Cleaning and standardizing the data\n\nGetting the data ready for analysis and extracting what informatin we can from the data visualization. Cleaning is a necessary step as empty values or NaN values in data set  can becme a outliner or result in errors respectively. Therefore, removal or adjustment of NaN or empty is important. This process can be done by the following ways:-\nBy removal of all or few of the Nan values from the data\nOr by imputing the data with mean, mode or median \nOr by random filling of the data.\nAfter taking care of the cleaning data, next step comes is the standardizing and aggregating the data to remove biasness towards a column in the data.\nHere we are filling the empty data with the mean and doing one hot encoding to all the columns to  convert them to binary data.","d9e87f24":"Visualizing the survival rate across a family by mapping Survival column of the data against number of siblings, number of parents and the total family size.","c566f726":"# Getting to know the data\n\nTitanic data exploration and survival prediction is one of he most important step in kaggle's world. Titanic survival prediction is one of the longest running competition on kaggle and its a good one to start and get to know the kaggle competition environment and how it works. \nThe Data involves 2 data files one is training data file and another testing file. The training data file contains 13 columns(the testing data cntains 12 columns as we need to predict survival rate):-\nPassengerID-The id of the passenger\nSurvived(target column)-column determining whether the person survived or not.(0 for not surviving and 1 for surviving)\nPclass -Ticket class\t(1 = 1st, 2 = 2nd, 3 = 3rd)\nSex\t-Sex of the passenegr\nAge\t-Age of the passenger in years\t\nsibsp\t-Number of siblings \/ spouses aboard the Titanic\t\nparch\t-Number of parents \/ children aboard the Titanic\t\nticket\t-Ticket number\t\nfare\t-Passenger fare\t\ncabin\t-Cabin number\t\nembarked\t-Port of Embarkation\t(C = Cherbourg, Q = Queenstown, S = Southampton)","29b806a0":"# Data Exploration and Analysis\nUsing the pivot_table method of the pandas library the reduce and condense the data for better data exploration and data insights.","5fed3dc3":"GridSearchCV is a very efficient method to use multiple machine learning algorithms against each other and check there accuracy for the best algo."}}