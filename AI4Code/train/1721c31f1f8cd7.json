{"cell_type":{"9f5e5498":"code","1aa080bf":"code","54b24706":"code","e013cb48":"code","b82ecf3b":"code","f715337c":"code","c090b667":"code","d9237977":"code","91b7deec":"code","4f0f767f":"code","330878e1":"code","d08c4fbf":"code","a6822b91":"code","f5b9fea6":"code","1a33447b":"markdown"},"source":{"9f5e5498":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1aa080bf":"df_train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv')\ndf_simple_sub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\n","54b24706":"df_train","e013cb48":"df_test.info()","b82ecf3b":"df_train.isnull().sum().sum()","f715337c":"df_test.isnull().sum().sum()","c090b667":"fea = df_test.columns.to_list()\nfea.remove('id')","d9237977":"from tensorflow.keras.layers.experimental import preprocessing\nnormalizer = preprocessing.Normalization(axis=-1)\nnormalizer.adapt(np.array(df_train[fea]))","91b7deec":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","4f0f767f":"def build_and_compile_model(norm):\n  model = keras.Sequential([\n      norm,\n\n      # layers.BatchNormalization(),\n      layers.Dense(128, activation='relu'),\n      # layers.Dropout(0.2),\n      \n      # layers.BatchNormalization(),\n      layers.Dense(64, activation='relu'),\n      # layers.Dropout(0.2),\n\n      layers.Dense(32, activation='relu'),\n      # layers.Dropout(0.2),\n\n      layers.Dense(1)\n  ])\n\n  model.compile(loss='mse',\n                optimizer=tf.keras.optimizers.Adam(0.001))\n  return model","330878e1":"dnn_model = build_and_compile_model(normalizer)\ndnn_model.summary()","d08c4fbf":"from tensorflow.keras.callbacks import EarlyStopping\nes = EarlyStopping(\n    min_delta=0.001,\n    patience=5,\n    restore_best_weights=True,\n)","a6822b91":"%%time\nhistory = dnn_model.fit(\n    x=df_train[fea], \n    y=df_train['loss'],\n    validation_split=0.2,\n    batch_size=20,\n    # verbose=0, \n    epochs=10,\n    callbacks=[es])","f5b9fea6":"# \u6ca1\u6709\u8003\u8651\u7cbe\u5ea6\u524d\u7684\u7ed3\u679c\ny_test_pred = dnn_model.predict(df_test[fea])\ndf_simple_sub['loss'] = y_test_pred\ndf_simple_sub.to_csv('simple_dnn_submission.csv', index=False)","1a33447b":"# load&overview Data"}}