{"cell_type":{"33e5c67c":"code","b3dfaf94":"code","37577c26":"code","0d7fbbb5":"code","465827da":"code","b0facb4a":"code","e1e07d0b":"code","63bb60a0":"code","2d0c2c2f":"code","deb6d6db":"code","9f3cd412":"code","38be1a96":"code","952c736f":"code","7c2a0414":"code","020b8277":"code","50663680":"code","f57c7bd4":"code","53dafc40":"code","301c2f15":"code","9f655df5":"code","d0a27f2b":"code","c3eb073f":"code","ac7608f2":"code","9868d845":"code","f6e6fcf3":"code","80cecf6b":"code","ed92c75e":"code","5b3fbfde":"code","ef44973c":"code","506e01f1":"code","56c405cf":"code","4917a8f1":"code","acbc99f4":"code","acd88633":"code","9effbecc":"code","9bf69dbe":"code","f6b04b07":"code","3ce552e1":"code","43741ddd":"code","5305c38f":"code","7e7e7fd1":"code","abf430f4":"code","648cba67":"code","a6667419":"code","197bd40f":"code","d541e727":"code","d814fa14":"code","e1bdf3a4":"code","ca620ea0":"code","18ae2a38":"code","188c8fca":"code","3fe15f4c":"code","759aea52":"markdown","12916545":"markdown","f10ff9ff":"markdown","6ace4aeb":"markdown","da0c2511":"markdown","32c8d485":"markdown","e474cec0":"markdown","baeb39d4":"markdown","e2434f8d":"markdown","a9a8b36b":"markdown","9eb738e5":"markdown","6f58117f":"markdown","35b6a488":"markdown","9cc90b34":"markdown","325f3d37":"markdown","3671b3ed":"markdown","951d5210":"markdown","9a275b5f":"markdown"},"source":{"33e5c67c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b3dfaf94":"# manipulation data\nimport pandas as pd\nimport numpy as np\n\n#visualiation data\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport matplotlib\nimport plotly.graph_objects as go\nimport plotly.express as px","37577c26":"train = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ntrain.head()","0d7fbbb5":"train.info()","465827da":"train.dtypes.value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True)\nplt.title('type of our data');","b0facb4a":"train.columns","e1e07d0b":"train.describe()","63bb60a0":"train.isnull().sum()","2d0c2c2f":"train.hist(figsize=(15,15),edgecolor='black');","deb6d6db":"train.DEATH_EVENT.value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True)\nplt.title('the % of deaths')","9f3cd412":"plt.figure(figsize=(20,6))\nsns.countplot(x='age',data=train)\nplt.xticks(rotation=90)\nplt.title('the ages of our persone')","38be1a96":"# Distribution of Age\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x = train['age'],\n    xbins=dict( # bins used for histogram\n        start=40,\n        end=95,\n        size=2\n    ),\n    marker_color='#e8ab60',\n    opacity=1\n))\n\nfig.update_layout(\n    title_text='Distribution of Age',\n    xaxis_title_text='AGE',\n    yaxis_title_text='COUNT', \n    bargap=0.05, # gap between bars of adjacent location coordinates\n    xaxis =  {'showgrid': False },\n    yaxis = {'showgrid': False },\n    template = 'presentation'\n)\n\nfig.show()","952c736f":"# Distribution of AGE Vs DEATH_EVENT\nfig = px.histogram(train, x=\"age\", color=\"DEATH_EVENT\", marginal=\"violin\", hover_data=train.columns, \n                   title =\"Distribution of AGE Vs DEATH_EVENT\", \n                   labels={\"age\": \"AGE\"},\n                   template=\"plotly\",                   \n                  )\nfig.show()","7c2a0414":"train.sex.value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True)","020b8277":"sns.countplot(x='sex',hue='DEATH_EVENT',data=train)\nplt.legend(['yes','no'])","50663680":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nd1 = train[(train[\"DEATH_EVENT\"]==0) & (train[\"sex\"]==1)]\nd2 = train[(train[\"DEATH_EVENT\"]==1) & (train[\"sex\"]==1)]\nd3 = train[(train[\"DEATH_EVENT\"]==0) & (train[\"sex\"]==0)]\nd4 = train[(train[\"DEATH_EVENT\"]==1) & (train[\"sex\"]==0)]\n\nlabel1 = [\"Male\",\"Female\"]\nlabel2 = ['Male - Survived','Male - Died', \"Female -  Survived\", \"Female - Died\"]\n\nvalues1 = [(len(d1)+len(d2)), (len(d3)+len(d4))]\nvalues2 = [len(d1),len(d2),len(d3),len(d4)]\n\n# Create subplots: use 'domain' type for Pie subplot\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=label1, values=values1, name=\"GENDER\"),\n              1, 1)\nfig.add_trace(go.Pie(labels=label2, values=values2, name=\"GENDER VS DEATH_EVENT\"),\n              1, 2)\n\n# Use `hole` to create a donut-like pie chart\nfig.update_traces(hole=.4, hoverinfo=\"label+percent\")\nfig.update_layout(\n    title_text=\"GENDER DISTRIBUTION IN THE DATASET  \\\n                   GENDER VS DEATH_EVENT\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='GENDER', x=0.19, y=0.5, font_size=10, showarrow=False),\n                 dict(text='GENDER VS DEATH_EVENT', x=0.84, y=0.5, font_size=9, showarrow=False)],\n    autosize=False,width=1200, height=500, paper_bgcolor=\"white\")\n\nfig.show()","f57c7bd4":"sns.barplot(x='sex',y='smoking',hue='DEATH_EVENT',data=train);","53dafc40":"sns.countplot(x='sex',hue='diabetes',data=train)\nplt.legend(['yes','no']);","301c2f15":"train.diabetes.value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True)","9f655df5":"sns.countplot(x='diabetes',hue='DEATH_EVENT',data=train)\nplt.legend(['yes','no']);","d0a27f2b":"sns.boxplot(x = train.ejection_fraction, color = 'green')\nplt.show()","c3eb073f":"train[train['ejection_fraction']>=70]","ac7608f2":"train = train[train['ejection_fraction']<70]","9868d845":"import plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x = train['ejection_fraction'],\n    xbins=dict( # bins used for histogram\n        start=14,\n        end=80,\n        size=2\n    ),\n    marker_color='#A7F432',\n    opacity=1\n))\n\nfig.update_layout(\n    title_text='EJECTION FRACTION DISTRIBUTION',\n    xaxis_title_text='EJECTION FRACTION',\n    yaxis_title_text='COUNT', \n    bargap=0.05, # gap between bars of adjacent location coordinates\n\n    template = 'plotly_dark'\n)\n\nfig.show()","f6e6fcf3":"sns.boxplot(x=train.time, color = 'yellow')\nplt.show()","80cecf6b":"sns.boxplot(x=train.serum_creatinine, color = 'red')\nplt.show()","ed92c75e":"train.corr().style.background_gradient(cmap='coolwarm').set_precision(1)","5b3fbfde":"# Feature Selection\n\nplt.rcParams['figure.figsize']=15,6 \nsns.set_style(\"darkgrid\")\n\nx = train.iloc[:, :-1]\ny = train.iloc[:,-1]\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier()\nmodel.fit(x,y)\nprint(model.feature_importances_) \nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(12).plot(kind='barh')\nplt.show()","ef44973c":"train=train.drop(['anaemia','creatinine_phosphokinase','diabetes','high_blood_pressure','platelets','sex','smoking','age'],axis=1)","506e01f1":"train","56c405cf":"train.corr().style.background_gradient(cmap='coolwarm').set_precision(3)","4917a8f1":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score","acbc99f4":"x=train.drop('DEATH_EVENT',axis=1)\ny=train.DEATH_EVENT","acd88633":"print(x.shape)\nprint(y.shape)","9effbecc":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)","9bf69dbe":"print(x_train)\nprint(y_test)","f6b04b07":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","3ce552e1":"# Making Confusion Matrix and calculating accuracy score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nmodel = LogisticRegression()\n\n#Fit the model\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\n\nmylist = []\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\n# accuracy score\nacc_logreg = accuracy_score(y_test, y_pred)\n\nmylist.append(acc_logreg)\nprint(cm)\nprint(acc_logreg)","43741ddd":"# Finding the optimum number of neighbors \n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nlist1 = []\nfor neighbors in range(3,10):\n    classifier = KNeighborsClassifier(n_neighbors=neighbors, metric='minkowski')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\nplt.plot(list(range(3,10)), list1)\nplt.show()","5305c38f":"# Training the K Nearest Neighbor Classifier on the Training set\n\nclassifier = KNeighborsClassifier(n_neighbors=5)\nclassifier.fit(x_train, y_train)","7e7e7fd1":"# Predicting the Test set results\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)","abf430f4":"# Making the confusion matrix and calculating accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nacc_knn = accuracy_score(y_test, y_pred)\nmylist.append(acc_knn)\nprint(cm)\nprint(acc_knn)","648cba67":"#Finding the optimum number of n_estimators\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor estimators in range(10,30):\n    classifier = RandomForestClassifier(n_estimators = estimators, random_state=0, criterion='entropy')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(10,30)), list1)\nplt.show()","a6667419":"# Training the RandomForest Classifier on the Training set\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 15, criterion='entropy', random_state=0)\nclassifier.fit(x_train,y_train)","197bd40f":"# Predicting the test set results\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)","d541e727":"# Making the confusion matrix and calculating the accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nacc_randomforest = accuracy_score(y_test, y_pred)\nmylist.append(acc_randomforest)\nprint(cm)\nprint(acc_randomforest)","d814fa14":"# Finding the optimum number of max_leaf_nodes\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor leaves in range(2,15):\n    classifier = DecisionTreeClassifier(max_leaf_nodes = leaves, random_state=0, criterion='entropy')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(2,15)), list1)\nplt.show()","e1bdf3a4":"# Training the Decision Tree Classifier on the Training set\n\nclassifier = DecisionTreeClassifier(max_leaf_nodes = 10, random_state=0, criterion='entropy')\nclassifier.fit(x_train, y_train)","ca620ea0":"# Predicting the test set results\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)","18ae2a38":"# Making the confusion matrix and calculating accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nacc_decisiontree = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(acc_decisiontree)\nmylist.append(acc_decisiontree)","188c8fca":"models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', \n              'Random Forest',   \n              'Decision Tree'],\n    'Score': [acc_knn, acc_logreg, \n              acc_randomforest,acc_decisiontree\n              ]})\nmodels.sort_values(by='Score', ascending=False)","3fe15f4c":"plt.rcParams['figure.figsize']=15,6 \nsns.set_style(\"darkgrid\")\nax = sns.barplot(x=models.Model, y=models.Score, palette = \"rocket\", saturation =1.5)\nplt.xlabel(\"Classifier Models\", fontsize = 20 )\nplt.ylabel(\"% of Accuracy\", fontsize = 20)\nplt.title(\"Accuracy of different Classifier Models\", fontsize = 20)\nplt.xticks(fontsize = 12, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width\/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","759aea52":"# Logistic Regression","12916545":"# sex","f10ff9ff":"# Feature Scaling","6ace4aeb":"# split data","da0c2511":"**Time**\n* features selection","32c8d485":"# RANDOM FOREST CLASSIFCATION","e474cec0":"# Finding missing values","baeb39d4":"**Columns description**\n* anaemia:Decrease of red blood cells or hemoglobin (boolean)\n* creatinine_phosphokinase:Level of the CPK enzyme in the blood (mcg\/L)\n* diabetes:If the patient has diabetes (boolean)\n* ejection_fraction:Ejection fraction (EF) is a measurement, expressed as a percentage, of how much blood the left ventricle pumps out with each contraction\n* high_blood_pressure:blood hypertension\n* platelets:are a component of blood whose function (along with the coagulation factors)\n* serum_creatinine:Serum creatinine is widely interpreted as a measure only of renal function\n* serum_sodium: to see how much sodium is in your blood it is particularly important for nerve and muscle function.","e2434f8d":"like we can c that some of our feature had a corrolation almost aqual to 0 so we gonna drop them like :\n* anaemia\n* creatinine_phosphokinase\n* diabetes\n* high_blood_pressure\n* platelets\n* sex\n* smoking\n\nWe will select only 3 features : time, ejection_fraction, serum_creatinine","a9a8b36b":"# Feature selection","9eb738e5":"# Ejection_fraction","6f58117f":"# DecisionTreeClassifier","35b6a488":"# KNN","9cc90b34":"# Diabet","325f3d37":"# Death Events","3671b3ed":"# visualization","951d5210":"# Age","9a275b5f":"We can see there are two outliers. Lets remove them (70 and 80)"}}