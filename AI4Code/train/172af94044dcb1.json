{"cell_type":{"60148a8a":"code","d281e85d":"code","7711b895":"code","813379a5":"code","ad44d5ba":"code","2365e6ed":"code","ed4fc0b9":"code","7819de48":"code","47bf7468":"code","444f7866":"code","ead83396":"code","f1531c5f":"code","700f370d":"code","0b8371a4":"code","92317c84":"code","2196ca33":"code","1feaa4b7":"code","46a74f75":"code","bc6fe72d":"code","afe49cdc":"code","86aa0c90":"code","b5bb411f":"code","55fe0b57":"code","c24df161":"code","f1a2e6bd":"code","3e5d7508":"code","8aca6446":"code","4dfd5c29":"markdown","38c2452e":"markdown","3b4adab0":"markdown","c805c979":"markdown","698dad98":"markdown","d54a4e56":"markdown","e41d5d35":"markdown","d24d4e18":"markdown","a9492347":"markdown","57afc295":"markdown","a364e5ee":"markdown","7d2e0368":"markdown","8247fd0b":"markdown","e7e84100":"markdown","6ebaec70":"markdown","f54b0e0a":"markdown","21e951ec":"markdown","b685552f":"markdown","dbd12c82":"markdown","37a8c849":"markdown","9464f5cd":"markdown","fb9db958":"markdown","76216117":"markdown","83b1bb66":"markdown","9f4d9102":"markdown","1640c473":"markdown"},"source":{"60148a8a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n#Set the `python` built-in pseudo-random generator at a fixed value for reproducibility\nimport random\nseed_value=0\nrandom.seed(seed_value)\nnp.random.seed(seed_value)","d281e85d":"titanic_data=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntitanic_data.head()","7711b895":"test_data=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","813379a5":"#Bar chart on Victims and those who survived\nimport matplotlib.pyplot as plt\n\nmyplt=titanic_data[\"Survived\"].value_counts().plot(kind='bar',color=['r','g'])\nmyplt.set(ylabel=\"# of passengers\")\nmyplt.set_xticklabels([\"Victims\",\"Survived\"])\n","ad44d5ba":"#seaborn is a good visualization library\nimport seaborn as sns\nfig, axs = plt.subplots(nrows=1, figsize=(10, 10))\n\n\nsns.heatmap(titanic_data.drop(['PassengerId'], axis=1).corr(), annot=True,square=True,cmap='vlag')","2365e6ed":"#Ensure that the internet option is turned on in the settings tab (right side) in Kaggle Notebook. \n#This allows us to install external libraries. We need nameparser for splitting Title from Name field.\n#First ensure that the pip version is the latest\n!pip install --upgrade pip\n!pip install nameparser\nfrom nameparser import HumanName\n\ntitanic_data['NameTitle']=titanic_data['Name'].apply(lambda x: HumanName(x).title)\ntest_data['NameTitle']=test_data['Name'].apply(lambda x: HumanName(x).title)\n","ed4fc0b9":"titanic_data.groupby(['NameTitle']).agg('count')\n#titanic_test.groupby(['NameTitle']).agg('count')","7819de48":"feature_list=[\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"NameTitle\", \"Survived\"] \nfeature_list_minus_y=[\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"NameTitle\"] \n\ntitanic_data=titanic_data[feature_list]\ntitanic_test=test_data #Keep a copy of the test data with all the fields\ntest_data=test_data[feature_list_minus_y]","47bf7468":"#let's check the training data for missing values\nimport missingno as msno\nmsno.bar(titanic_data)","444f7866":"msno.bar(test_data)","ead83396":"#Define a null replace fuction for Age. \ndef fillna_Values(df):\n    df['Age'] = df.groupby(['Sex', 'NameTitle'])['Age'].apply(lambda x: x.fillna(x.mean()))\n    df['Age'] = df.groupby(['Sex'])['Age'].apply(lambda x: x.fillna(x.mean()))   #we reapply for any nulls left behind\n    df['Fare'] = df.groupby(['Pclass'])['Fare'].apply(lambda x: x.fillna(x.mean()))\n    return df","f1531c5f":"#Call function to Replace nulls\npd.options.mode.chained_assignment = None  # default='warn'\ntitanic_data=fillna_Values(titanic_data)\ntest_data=fillna_Values(test_data)\n#Now le'ts check again the nulls\nmsno.bar(titanic_data)","700f370d":"#Define a function for standardization\ndef standardizeData(X):\n    from sklearn.preprocessing import StandardScaler\n    \n    #We want to redue the name titles\n    X=X.replace(to_replace =[\"Capt.\",\"Col.\",\"Sir.\"],  \n                            value =\"Mr.\") \n    X=X.replace(to_replace =[\"Lady.\",\"Mlle.\",\"Mme.\",\"Ms.\",\"the Countess. of\"],  \n                            value =\"Mrs.\") \n   \n    train_numerical_features = [\"Age\",\"SibSp\",\"Parch\",\"Fare\"]\n        \n    \n    from sklearn.preprocessing import MinMaxScaler\n    ss_scaler = MinMaxScaler(feature_range=(0,1))    \n    X_ss = pd.DataFrame(data = X)\n   \n    X_ss[train_numerical_features] = ss_scaler.fit_transform(X_ss[train_numerical_features])\n    \n    \n    X_ss=pd.get_dummies(X_ss, columns=[\"Pclass\",\"Sex\",\"NameTitle\"])\n    return X_ss","0b8371a4":"#Call the function to Standarize numerical values and handle categorical values\ntitanic_data=standardizeData(titanic_data)\ntest_data=standardizeData(test_data)","92317c84":"titanic_data.head()","2196ca33":"from sklearn.model_selection import train_test_split\n\ntrain_data, mytest_data = train_test_split(titanic_data, test_size=0.2)","1feaa4b7":"#Prepare X and y\ny_train=train_data[\"Survived\"]\ny_test=mytest_data[\"Survived\"]\n\n#drop Survived clolum in y_test data\nX_train=train_data.drop(\"Survived\",axis=1)\nX_test=mytest_data.drop(\"Survived\",axis=1)","46a74f75":"from sklearn.ensemble import GradientBoostingClassifier\n#Fit the model\nmodel=GradientBoostingClassifier(n_estimators=100,max_depth=5,random_state=1)\nmodel.fit(X_train,y_train)","bc6fe72d":"#Predict mytest and print the accuracy\nmypredictions=model.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test, mypredictions))\nfrom sklearn.metrics import f1_score\nprint('F1 score:',f1_score(y_test, mypredictions))","afe49cdc":"#Creating a deep neural network\n#Also a dropout layer to prevent overfitting.\nimport tensorflow as tf\n\n#Set the random generator at a fixed value for reproducibility\ntf.random.set_seed(seed_value)\n\n#By trial and error we fine tuned the architecture with  hidden layers and dropouts to regularize\n\nnmodel=tf.keras.models.Sequential([tf.keras.layers.InputLayer(input_shape=(16,)),    \n                                   tf.keras.layers.Dense(8,activation=tf.nn.relu),\n                                   #tf.keras.layers.BatchNormalization(),\n                                   tf.keras.layers.Dropout(0.4),  \n                                   tf.keras.layers.Dense(8,activation=tf.nn.relu),\n                                   tf.keras.layers.Dropout(0.4),  \n                                   tf.keras.layers.Dense(8,activation=tf.nn.relu),\n                                   tf.keras.layers.Dropout(0.4),  \n                                   tf.keras.layers.Dense(1,activation='sigmoid')])\n","86aa0c90":"print(nmodel.summary())","b5bb411f":"#Compile model - use optimizer as Adam & loss function as mean_squared_error \n\n\n#nmodel.compile(optimizer='Adam',     #use default learning_rate i.e.0.001      \n#             loss='mean_squared_error',metrics=['accuracy'])\n\nnmodel.compile(optimizer='Adam',     #use default learning_rate i.e.0.001      \n             loss='mean_squared_error',metrics=['accuracy'])\n\n\n#Train model for 50 epochs\nnmodel.fit(X_train,y_train,batch_size=32,epochs=50)","55fe0b57":"#Let's check and see how the model is working with test set\nfrom sklearn.metrics import accuracy_score\n\ny_pred=nmodel.predict(X_test)\ny_pred = [np.round(x) for x in y_pred]\nprint('Accuracy:',accuracy_score(y_test, y_pred))\n\nscore=nmodel.evaluate(X_test,y_test,verbose=1)\n","c24df161":"#Train for entire train_data\n#Prepare X and y\n#Drop Survived column.\ny_train=titanic_data[\"Survived\"]\n\n#Copy the full training data\nX_train=titanic_data\nX_train=X_train.drop(\"Survived\",axis=1)","f1a2e6bd":"import matplotlib.pyplot as plt\n#Train the model with the entire training data. Run for 250 epochs\ntraining = nmodel.fit(X_train, y_train, epochs=250, batch_size=32, validation_split=0.2, verbose=0)\n\n\n#Plot the accuracy of training & validation\nplt.plot(training.history['accuracy'])\nplt.plot(training.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","3e5d7508":"#Finally use the neural network model, predict \nfinal_df=titanic_test\nfinal_df[\"Survived\"]=nmodel.predict(test_data)\nfinal_df[\"Survived\"]=titanic_test[\"Survived\"].apply(lambda x: round(x,0)).astype('int')\n","8aca6446":"#Submit the result\noutput=pd.DataFrame({'PassengerId':final_df.PassengerId,'Survived':final_df.Survived})\noutput.to_csv('my_submission.csv',index=False)\nprint(\"Your submission was successfuly saved!\")","4dfd5c29":"Now train the model with training set","38c2452e":"**Let's use a heatmap to understand the correlation between various features**","3b4adab0":"In the training data, about 20% of Age data are missing. \nAge is an important feature. So we should be fixing that. Cabin - may be important since the cabin letter shows the class, and the class may have significance for getting help for survival. But too many missing values. Let's take a look at the test data to see how that looks.","c805c979":"# Fix Nulls","698dad98":"How is the distribution of data for different nametitle? Look at the data for Name Title \"Miss\" almost all have survived. So the nameTitle is a significant feature","d54a4e56":"**Feature engineering**\n\nLet's get to know the features more. All features are important, however in the Name, we are only interested in the Title, since Title may have some significance on whether they will get priority for the lifeboat. For example Titles for Ladies and Children may matter.","e41d5d35":"Take a peak at the data","d24d4e18":"Now let us train with the entire data and see the accuracy","a9492347":"# Preprocess data\n1. Identify features to work with\n2. Check for null and fix it\n3. Standardize numerical values and handle categorical values","57afc295":"For neural network architecture there are so many different architectural choices. \n* How many layers? - By trial and error we are finding that 3 hidden layer is giving a good accuracy for validation data.\n* How many neurons? - We have very less training data (for images it can be millions!!) so we choose only 8 neurons per layer\n* Regularization? - Neural network has the tendancy to overfit. Dropout is a good way to reduce overfitting or variance.\n* What optimizer to use? - We have the choice of differnt optimizers like SGD, Adam, Nadam,RMSProp etc. Adam gives a good result and hence we chose that\n* What learning rate to use? - We have taken default value (0.001). By trials we find that gives the best result","a364e5ee":"# Do the prediction for the competition's test data","7d2e0368":"# Split training data to train and test","8247fd0b":"# Finally submit the result","e7e84100":"# Let's learn a bit more about the data","6ebaec70":"# Import data files","f54b0e0a":"# Now let's try the neural network","21e951ec":"# Now let us train the model with the entire titanic data","b685552f":"# Illustrating the use of neural network for solving titanic problem\nIdeally neural network requires large data set since it creates derived features through nodes in the network. It has the tendency of overfitting to the training data and hence the need for a large data set to generalize. \nHowever let's  just experiment with neural network.\n\n**Note: If you are copying the notebook please ensure that the internet option is turned on in the settings tab (right side) in Kaggle Notebook. This allows us to install external libraries. We need nameparser for splitting Title from Name field.**\n\nThe steps here  are\n* First prepare the features\n* Identify null features and fix them\n* Transform features by standardising data etc.\n* Split to training set and test for dev testing\n* Try with gradient boosting classifier\n* Then try neural network - with training and validation set \n* Train the neural network with entire data\n* Predict the test set & submit","dbd12c82":"There is a function which will do the job for us","37a8c849":"# Use test data to check for accuracy","9464f5cd":"# Now check for Null and Fix it","fb9db958":"We are particularly interested in the correlation between Survived and other features. ","76216117":"# Standardize numerical values and handle categorical values\nML algorithm can understand only numbers. Also we don't want the algorithm to be influenced by large values. So normalize all of them to take a value between 0 and 1. \nAlso categorical values like NameTitle has to be converted to columns to make them numerical.","83b1bb66":"****Out of total of 783 in the training data, how many survived? How many perished?****","9f4d9102":"Similar pattern of 20% of missing info of Age in test data too.In addition there is one missing in 'Fare' column also. Let's create a function for fixing Age and Fare nulls.","1640c473":"Let's pick other features we want to work with. We are not interested in 'Name' - since we have picked up the useful Titles from it. Also  80% of Cabin data is missing and hence we are not picking that as well."}}