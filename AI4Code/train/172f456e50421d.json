{"cell_type":{"7ee286ea":"code","0ad7f06e":"code","7f35af62":"code","6fca2b58":"code","9ecdc808":"code","91dabcd9":"code","f8afc48f":"code","7a227366":"code","b7c01db8":"code","b9bfe67f":"code","4b6cb2ac":"code","1c79e8db":"code","71ac642e":"code","606941a0":"code","cb4f8be9":"code","1ab24366":"code","fb932a7e":"code","894a0076":"code","f274d0e7":"code","58515590":"code","8e53e83b":"code","91ce612b":"code","fe42505a":"code","8bce10bf":"markdown","0a9eaa42":"markdown","fcb640f3":"markdown","e3084255":"markdown","c11068ea":"markdown","7e68c66a":"markdown","d3c37aa2":"markdown","f876139c":"markdown","a0600970":"markdown","88a3d51d":"markdown","487f19b3":"markdown","d48ede44":"markdown","c2a5df33":"markdown","d92c4cf6":"markdown","5a8778aa":"markdown"},"source":{"7ee286ea":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout","0ad7f06e":"file = \"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\"\ndf = pd.read_csv(file)","7f35af62":"print(df.shape)\ndf.head()","6fca2b58":"df.info()","9ecdc808":"sns.pairplot(df, hue=\"quality\")","91dabcd9":"mask = np.zeros_like(df.corr())\nmask[np.triu_indices_from(mask)] = True\n\nplt.figure(figsize=(18,8))\nsns.heatmap(df.corr(), cmap='viridis', mask=mask, annot=False, square=True)","f8afc48f":"# check correlations above 0.6 for fixed acidity', 'volatile acidity' and 'total sulfur dioxide' features\n\n(abs(df.corr()[['fixed acidity', 'volatile acidity', 'total sulfur dioxide']])>0.6)*1","7a227366":"df.corr()['quality'].iloc[:-1].sort_values().plot(kind='bar')","b7c01db8":"sns.histplot(df.quality)","b9bfe67f":"df.quality.value_counts()","4b6cb2ac":"scaler = MinMaxScaler()\n\nX_train = pd.DataFrame(scaler.fit_transform(df))\nX_train.columns = df.columns","1c79e8db":"sns.set_theme(style=\"ticks\", palette=\"pastel\")\n\nplt.figure(figsize=(18,8))\nsns.boxplot(data=X_train)","71ac642e":"# dropping 'citric acid', 'density', 'pH', 'total sulfur dioxide':\n\ndf = df.drop(columns=['citric acid', 'density', 'pH', 'total sulfur dioxide'])","606941a0":"df.head()","cb4f8be9":"X = df.loc[:, df.columns != 'quality'].values\ny = df.quality.values\nle = LabelEncoder()\ny = le.fit_transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ny_train_cat = to_categorical(y_train, 6)\ny_test_cat = to_categorical(y_test, 6)","1ab24366":"scaler = RobustScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nprint(X_train.shape)\nprint(X_test.shape)","fb932a7e":"xavier_init = tf.keras.initializers.GlorotNormal()\n\nmodel = Sequential()\nmodel.add(Dense(64, kernel_initializer=xavier_init,  activation='relu'))\nmodel.add(Dense(32, kernel_initializer=xavier_init, activation='relu'))\nmodel.add(Dense(16, kernel_initializer=xavier_init, activation='relu'))\nmodel.add(Dense(6, kernel_initializer=xavier_init, activation='sigmoid'))\n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","894a0076":"model.fit(X_train,\n          y_train_cat,\n          epochs=30,\n          validation_data=(X_test,y_test_cat),\n          verbose=1)","f274d0e7":"losses = pd.DataFrame(model.history.history)\n\nlosses[['loss','val_loss']].plot()\nlosses[['accuracy','val_accuracy']].plot()","58515590":"print(model.metrics_names)\nprint(model.evaluate(X_test,y_test_cat,verbose=0))","8e53e83b":"predictions = le.inverse_transform(np.argmax(model.predict(X_test), axis=-1))","91ce612b":"print(classification_report(le.inverse_transform(y_test),\n                            predictions))","fe42505a":"CM = confusion_matrix(le.inverse_transform(y_test), predictions)\nCM","8bce10bf":"# <a id=\"eda\">Exploratory Data Analysis<\/a>","0a9eaa42":"1. <a href=\"#load\"> Loading data <\/a>\n2. <a href=\"#eda\">Exploratory data analysis<\/a>\n3. <a href=\"#proc\">Pre-processing<\/a>\n4. <a href=\"#prep\">Data preparation<\/a>\n    * <a href=\"#tts\">train-test split \/ LabelEncoder<\/a>\n    * <a href=\"#scal\">Scaler<\/a>\n5. <a href=\"#modl\">Build Model<\/a>\n6. <a href=\"#eval\">Evaluate Model<\/a>","fcb640f3":"# <a id=\"prep\">Data preparation<\/a>","e3084255":"### <a id=\"scal\">Scaler<\/a>","c11068ea":"Data preparation steps:\n* train-test split\n* MinMax scaling","7e68c66a":"now, let's scale all the data, aiming at checking a boxplot of all features, so we can easily see its variability.\n\nWARNING: note that this scaling will be used only for this observation purpose! do not use this specific scaler for posterior training purposes, as you may incur in data snooping.","d3c37aa2":"Checking linear correlation among features:","f876139c":"# <a id=\"load\"> Loading data <\/a>","a0600970":"clearly imbalanced dataset... Maybe we could think about applying SMOTE or some cost-sensitive learning technique. https:\/\/machinelearningmastery.com\/multi-class-imbalanced-classification\/","88a3d51d":"Based on our previous data exploration, let's drop some high correlated features:","487f19b3":"### <a id=\"tts\">train-test split \/ LabelEncoder<\/a>","d48ede44":"# <a id=\"modl\">Build Model<\/a>","c2a5df33":"# <a id=\"eval\">Evaluate Model<\/a>","d92c4cf6":"# <a id=\"proc\">Pre-processing<\/a>","5a8778aa":"It seems there are some high correlated features in dataset, mainly among the following features (their correlations are all above 0.6):\n\n* 'citric acid', 'density' and 'pH' to 'fixed acidity'\n* 'citric acid' to 'volatile acidity'\n* 'total sulfur dioxide' to 'free sulfur dioxide'.\n\nLet's now check correlations to the target (_quality_ feature) and its distribution:"}}