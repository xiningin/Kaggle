{"cell_type":{"bb7bbdb5":"code","57ff8177":"code","b135454f":"code","7a5a8c33":"code","eceb0b5f":"code","131b2477":"code","c694e13d":"code","ab697700":"code","589efe3d":"code","11964750":"code","f894054d":"code","07b5a133":"code","052bee13":"code","b494c1fa":"code","e3c97375":"code","1702b4d2":"code","eae1d166":"code","1cb1ab6f":"code","9f02cd76":"code","b6e5cd38":"code","88efaef6":"code","9c62bf9e":"code","3d730d0b":"code","04edcd58":"code","b094c6f4":"code","c1f76d6e":"code","477b9bdd":"code","2e741e5b":"code","92923e5e":"code","9215bb72":"code","0a5de02a":"code","b2fddc8c":"code","0b7d0c0d":"code","b2d78236":"code","bb2c4ccf":"code","6dca74fa":"code","bc890693":"code","7f88477e":"code","f8d3af66":"code","5557d005":"code","43a9ab04":"code","dfbdefc1":"code","1ec5859f":"code","77053d22":"code","4381c07b":"code","6b5204a4":"code","5bf7c95d":"code","78f9bcfc":"code","d9892dc6":"code","67203252":"code","6f3dd128":"code","e79b303a":"code","93f9a2fa":"code","e5bcfb41":"code","a5c0098b":"code","e14f74a8":"code","e05fdee3":"code","d2220590":"code","46f026a3":"code","28c93489":"code","44f7c13f":"code","d385b3bb":"code","556706bf":"code","9e784b37":"code","ebfba6ae":"code","0fa557d8":"code","891cef2d":"code","02966ab3":"code","b4e899b0":"code","a6120d67":"code","8de8878f":"code","cd9beb14":"code","f4e1fb20":"code","9a5a1eda":"code","9736350e":"code","6e924063":"code","d71730d2":"code","c5898e89":"code","366ecff1":"code","cc00350e":"code","4bdf21c5":"code","249a0425":"code","6d35f4c0":"code","d6f3387e":"code","8524921e":"code","3994d3ca":"code","397ec1e0":"code","795a8081":"code","11f5f93a":"code","02bc825b":"code","4da3148b":"code","59a4673b":"code","af656e73":"code","5677733f":"code","715d6097":"code","ca4e368e":"code","ad4a9f5d":"code","c869ff2d":"code","458e68a1":"code","5e363306":"code","6d724244":"markdown","51813593":"markdown","53e2b97a":"markdown","f1462662":"markdown","1cebc98d":"markdown","5edf99f2":"markdown","3786501f":"markdown","af3d6995":"markdown","f5945f1b":"markdown","57806986":"markdown","5752e716":"markdown","77e68687":"markdown","f8533a00":"markdown","c4e47f39":"markdown","8d0c9a6b":"markdown","814b80a4":"markdown","fbd6ac76":"markdown","7c21cbfd":"markdown","c2239b04":"markdown","5bc03063":"markdown","24d41148":"markdown","f077a3c6":"markdown","1454d024":"markdown","ce3e32fb":"markdown","a5692534":"markdown","d7dd4c05":"markdown","922eca18":"markdown","4139fd44":"markdown","633889c9":"markdown","ed511d1b":"markdown","48485134":"markdown","154e3100":"markdown","d1c48ce4":"markdown","6cfd0c04":"markdown","5a22a95e":"markdown","d18f84b3":"markdown","f3a1e6b6":"markdown","7edf1d51":"markdown","b1923b5e":"markdown","72de7148":"markdown","9abb17d5":"markdown","0ee96297":"markdown","e5b23002":"markdown","b3e8cb0e":"markdown","c6a6575f":"markdown","4d518341":"markdown","43c5ed5f":"markdown","78b0bd77":"markdown","56c29b4f":"markdown","aa7aa3fa":"markdown","8d92e25a":"markdown","959e0bcd":"markdown","4aaa6fa7":"markdown","9fd1a7e7":"markdown","744611d9":"markdown","cfa191a8":"markdown","a4e981a2":"markdown","96c2febc":"markdown","218ccec9":"markdown","5fc7c065":"markdown","66876029":"markdown","4d8b242f":"markdown","944e2da2":"markdown","1db15e39":"markdown","0d95a8ab":"markdown","ca229b9e":"markdown","6097c54f":"markdown","565663dd":"markdown","03db2b04":"markdown","c050af1e":"markdown","b945e43c":"markdown","78d0c24b":"markdown","6bb10329":"markdown","cc47223b":"markdown","614c962a":"markdown","c6bfe38d":"markdown","6b812df5":"markdown","2aa579fd":"markdown","a6506c8b":"markdown","aec5b98b":"markdown","2a995b5c":"markdown","8dccdebc":"markdown","0204ee2f":"markdown","9d7698fa":"markdown","dd8c30f0":"markdown","0e87e907":"markdown","1922c591":"markdown","84d53e5d":"markdown","725aa038":"markdown","2095eaa9":"markdown"},"source":{"bb7bbdb5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix\n\nfrom imblearn.over_sampling import SVMSMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n\nfrom yellowbrick.model_selection import FeatureImportances","57ff8177":"import warnings\nwarnings.filterwarnings('ignore')","b135454f":"df = pd.read_csv('\/kaggle\/input\/income-classification\/income_evaluation.csv')","7a5a8c33":"df.head()","eceb0b5f":"df.columns","131b2477":"df.columns = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain',\n              'capital-loss','hours-per-week','native-country','income']\n\ndf.columns = df.columns.str.replace('-','_')","c694e13d":"df.head()","ab697700":"df.shape","589efe3d":"df.info()","11964750":"df.describe().T","f894054d":"categorical = [var for var in df.columns if df[var].dtype=='O']\n\nprint(categorical)","07b5a133":"df[categorical].describe()","052bee13":"df.isnull().sum()","b494c1fa":"data = df.copy()","e3c97375":"sns.countplot(data['income'])","1702b4d2":"sns.distplot(data['age'])","eae1d166":"labels = ['10-20','20-30','30-40','40-50','50-60','60-70','70-80','80-90']\nbins = [10,20,30,40,50,60,70,80,90]\nfreq_df = data.groupby(pd.cut(data['age'],bins = bins,labels = labels)).size()\nfreq_df = freq_df.reset_index(name = 'count')","1cb1ab6f":"freq_df","9f02cd76":"plt.bar(freq_df['age'],freq_df['count'])","b6e5cd38":"data['workclass'].value_counts()","88efaef6":"data['workclass'] = data.workclass.str.replace('?','Unknown')","9c62bf9e":"sns.countplot(data['workclass'])\nplt.xticks(rotation = 90)","3d730d0b":"sns.distplot(data['fnlwgt'])","04edcd58":"data['education'].value_counts()","b094c6f4":"sns.countplot(data['education'])\nplt.xticks(rotation = 90)","c1f76d6e":"data['education_num'].value_counts()","477b9bdd":"sns.countplot(data['education_num'])","2e741e5b":"data['marital_status'].value_counts()","92923e5e":"sns.countplot(data['marital_status'])\nplt.xticks(rotation = 90)","9215bb72":"data['occupation'].value_counts()","0a5de02a":"data['occupation'] = data.occupation.str.replace('?','Unknown')","b2fddc8c":"sns.countplot(data['occupation'])\nplt.xticks(rotation = 90)","0b7d0c0d":"data['relationship'].value_counts()","b2d78236":"sns.countplot(data['relationship'])\nplt.xticks(rotation = 90)","bb2c4ccf":"data['race'].value_counts()","6dca74fa":"sns.countplot(data['race'])\nplt.xticks(rotation = 90)","bc890693":"data['sex'].value_counts()","7f88477e":"sns.countplot(data['sex'])","f8d3af66":"data['capital_loss'].value_counts().nlargest(15)","5557d005":"data['capital_gain'].value_counts().nlargest(15)","43a9ab04":"data['native_country'].value_counts()","dfbdefc1":"data['native_country'] = data.native_country.str.replace('?','Unknown')","1ec5859f":"sns.countplot(data['native_country'])\nplt.xticks(rotation = 90)","77053d22":"x = data.drop('income',axis = 1)\ny = data['income']","4381c07b":"x_dummy = pd.get_dummies(x)","6b5204a4":"x_train,x_test,y_train,y_test = train_test_split(x_dummy,y,test_size = 0.2,random_state = 0)","5bf7c95d":"def fit_model(model,x,y):\n    model.fit(x,y)\n    y_pred = model.predict(x_test)\n    print(\"Accuracy: \",model.score(x_test,y_test))\n    print(\"------------------------------\")\n    print(\"Classification Report\")\n    print(\"------------------------------\")\n    print(classification_report(y_test,y_pred))\n    print(\"------------------------------\")\n    print(\"Confusion Matrix\")\n    print(\"------------------------------\")\n    print(confusion_matrix(y_test,y_pred))\n    print(\"------------------------------\")","78f9bcfc":"lr = LogisticRegression(max_iter = 1000)\n\nfit_model(lr,x_train,y_train)","d9892dc6":"dtree = DecisionTreeClassifier()\n\nfit_model(dtree,x_train,y_train)","67203252":"rf = RandomForestClassifier(random_state = 0)\n\nfit_model(rf,x_train,y_train)","6f3dd128":"gbm = GradientBoostingClassifier(random_state = 0)\n\nfit_model(gbm,x_train,y_train)","e79b303a":"data1 = data.copy()","93f9a2fa":"data1['workclass'].value_counts() \/ len(data1)","e5bcfb41":"names = ['State-gov','Self-emp-inc','Federal-gov','Without-pay','Never-worked']\n\nfor i in names:\n    data1['workclass'] = data1.workclass.str.replace(i,'Other')","a5c0098b":"fig, ax =plt.subplots(1,2,figsize = (25,10))\nsns.countplot(data['workclass'],ax = ax[0])\nsns.countplot(data1['workclass'],ax = ax[1])","e14f74a8":"names1 = ['11th','9th','7th-8th','5th-6th','10th','1st-4th','Preschool','12th']\n\nfor i in names1:\n    data1['education'] = data1.education.str.replace(i,'Non Graduate')\n    \nnames2 = ['Assoc-acdm','Assoc-voc','Doctorate','Prof-school']\n\nfor i in names2:\n    data1['education'] = data1.education.str.replace(i,'Other')","e05fdee3":"fig, ax =plt.subplots(1,2,figsize = (25,10))\nsns.countplot(data['education'],ax = ax[0])\nsns.countplot(data1['education'],ax = ax[1])","d2220590":"names1 = [1,2,3,4]\n\nfor i in names1:\n    data1['education_num'] = data1.education_num.replace(i,'1-4')\n    \nnames2 = [5,6,7,8]\n\nfor i in names2:\n    data1['education_num'] = data1.education_num.replace(i,'5-8')\n    \nnames3 = [9,10,11,12]\n\nfor i in names3:\n    data1['education_num'] = data1.education_num.replace(i,'9-12')\n    \nnames4 = [13,14,15,16]\n\nfor i in names4:\n    data1['education_num'] = data1.education_num.replace(i,'13-16')","46f026a3":"fig, ax =plt.subplots(1,2,figsize = (25,10))\nsns.countplot(data['education_num'],ax = ax[0])\nsns.countplot(data1['education_num'],ax = ax[1])","28c93489":"data1['marital_status'].value_counts() \/ len(data1)","44f7c13f":"names = ['Married-spouse-absent','Separated','Married-AF-spouse','Widowed']\n\nfor i in names:\n    data1['marital_status'] = data1.marital_status.str.replace(i,'Other')","d385b3bb":"fig, ax =plt.subplots(1,2,figsize = (25,10))\nsns.countplot(data['marital_status'],ax = ax[0])\nsns.countplot(data1['marital_status'],ax = ax[1])","556706bf":"data1['occupation'].value_counts() \/ len(data1)","9e784b37":"names = ['Handlers-cleaners','Transport-moving','Farming-fishing','Tech-support','Protective-serv','Armed-Forces','Priv-house-serv']\n\nfor i in names:\n    data1['occupation'] = data1.occupation.str.replace(i,'Other')","ebfba6ae":"fig, ax =plt.subplots(1,2,figsize = (25,10))\nsns.countplot(data['occupation'],ax = ax[0])\nsns.countplot(data1['occupation'],ax = ax[1])","0fa557d8":"data1['race'].value_counts() \/ len(data1)","891cef2d":"names = ['Asian-Pac-Islander','Amer-Indian-Eskimo','Other']\n\nfor i in names:\n    data1['race'] = data1.race.str.replace(i,'Other')","02966ab3":"fig, ax =plt.subplots(1,2,figsize = (25,10))\nsns.countplot(data['race'],ax = ax[0])\nsns.countplot(data1['race'],ax = ax[1])","b4e899b0":"data1['native_country'].value_counts() \/ len(data1)","a6120d67":"na = ['Cuba','Jamaica','Puerto-Rico','Honduras','Haiti','Dominican-Republic','El-Salvador','Guatemala','Nicaragua','United-States',\n      'Mexico','Canada']\n\nfor i in na:\n    data1['native_country'] = data1.native_country.str.replace(i,'NAmerica')\n    \ndata1['native_country'] = data1.native_country.str.strip().replace('Outlying-US(Guam-USVI-etc)','Outlying-US')\ndata1['native_country'] = data1.native_country.str.replace('Outlying-US','NAmerica')\n\nsa = ['Trinadad&Tobago','Columbia','Ecuador','Peru']\n\nfor i in sa:\n    data1['native_country'] = data1.native_country.str.replace(i,'SAmerica')\n    \nai = ['India','South','Iran','Philippines','Cambodia','Thailand','Laos','Taiwan','China','Japan','Vietnam','Hong']\n\nfor i in ai:\n    data1['native_country'] = data1.native_country.str.replace(i,'Asia')\n    \neu = ['England','Germany','Italy','Poland','Portugal','France','Yugoslavia','Scotland','Greece','Ireland','Hungary','Holand-Netherlands']\n\nfor i in eu:\n    data1['native_country'] = data1.native_country.str.replace(i,'Europe')","8de8878f":"data1.rename(columns = {'native_country':'region'}, inplace = True) ","cd9beb14":"fig, ax =plt.subplots(1,2,figsize = (25,10))\nsns.countplot(data['native_country'],ax = ax[0])\nsns.countplot(data1['region'],ax = ax[1])","f4e1fb20":"x = data1.drop('income',axis = 1)\ny = data1['income']","9a5a1eda":"x_dummy = pd.get_dummies(x)","9736350e":"x_train,x_test,y_train,y_test = train_test_split(x_dummy,y,test_size = 0.2,random_state = 0)","6e924063":"lr = LogisticRegression(max_iter = 1000)\n\nfit_model(lr,x_train,y_train)","d71730d2":"dtree = DecisionTreeClassifier()\n\nfit_model(dtree,x_train,y_train)","c5898e89":"rf = RandomForestClassifier(random_state = 0)\n\nfit_model(rf,x_train,y_train)","366ecff1":"gbm = GradientBoostingClassifier(random_state = 0)\n\nfit_model(gbm,x_train,y_train)","cc00350e":"#param_grid = {'solver':['newton-cg','lblinear','lbfgs']}","4bdf21c5":"#lr = LogisticRegression(max_iter = 1000)\n\n#gs = GridSearchCV(lr,param_grid,cv = 5,scoring = 'accuracy',n_jobs = -1,verbose = True)\n\n#gs.fit(x_train,y_train)","249a0425":"#gs.best_params_","6d35f4c0":"#param_grid = {'penalty':['l1','l2'],\n              #'C':[100.0,10.0,1.0,0.1,0.01]\n    \n#}","d6f3387e":"#lr = LogisticRegression(solver = 'newton-cg',penalty = 'l2',max_iter = 1000)\n\n#gs = GridSearchCV(lr,param_grid,cv = 5,scoring = 'accuracy',n_jobs = -1,verbose = True)\n\n#gs.fit(x_train,y_train)","8524921e":"#gs.best_params_","3994d3ca":"lr = LogisticRegression(C = 0.1,solver = 'newton-cg',penalty = 'l2',max_iter = 1000)\n\nfit_model(lr,x_train,y_train)","397ec1e0":"#param_grid = {'criterion':['gini','entropy'],\n              #'splitter':['best','random'],\n              #'max_features':['auto','sqrt','log2'],\n              #'max_depth': np.arange(2,7,1),\n              #'min_samples_split': np.arange(2,10,1),\n              #'min_samples_leaf': np.arange(2,7,1)\n#}","795a8081":"#dtree = DecisionTreeClassifier()\n\n#gs = GridSearchCV(dtree,param_grid,cv = 5,scoring = 'accuracy',n_jobs = -1,verbose = True)\n\n#gs.fit(x_train,y_train)","11f5f93a":"#gs.best_params_","02bc825b":"dtree = DecisionTreeClassifier(criterion = 'gini',max_depth = 6,max_features = 'auto',min_samples_leaf = 4,min_samples_split = 5,\n                               splitter = 'best')\n\nfit_model(dtree,x_train,y_train)","4da3148b":"#param_grid = {'criterion':['gini','entropy'],\n              #'bootstrap': [True,False],\n              #'n_estimators':[10,100,200,500,1000],\n              #'max_features':['auto','sqrt','log2'],\n              #'max_depth': [2,3,4,5,6,7,None],\n              #'min_samples_split': np.arange(2,10,1),\n              #'min_samples_leaf': np.arange(2,7,1)\n#}","59a4673b":"#rf = RandomForestClassifier(random_state = 0)\n\n#gs = GridSearchCV(rf,param_grid,cv = 5,scoring = 'accuracy',n_jobs = -1,verbose = True)\n\n#gs.fit(x_train,y_train)","af656e73":"#gs.best_params_","5677733f":"rf = RandomForestClassifier(bootstrap =True,criterion = 'entropy',max_depth = None,min_samples_leaf = 2,min_samples_split = 100,\n                            max_features = 17,n_estimators = 10,random_state = 0)\n\nfit_model(rf,x_train,y_train)","715d6097":"#param_grid = {'n_estimators':range(20,81,10),\n              #'max_depth':range(5,16,2),\n              #'min_samples_split':range(1000,2100,200),\n              #'min_samples_leaf':range(30,71,10),\n              #'max_features':[range(7,20,2),None],\n              #'subsample':[0.6,0.7,0.75,0.8,0.85,0.9]\n             #}","ca4e368e":"#gbm = GradientBoostingClassifier(n_estimators = 80,max_depth = 13,min_samples_split = 1000,min_samples_leaf = 30,max_features = None,\n                                 #random_state = 0)\n\n#gs = GridSearchCV(gbm,param_grid,cv = 5,scoring = 'accuracy',n_jobs = -1,verbose = True)\n\n#gs.fit(x_train,y_train)","ad4a9f5d":"#gs.best_params_","c869ff2d":"gbm = GradientBoostingClassifier(n_estimators = 80,max_depth = 13,min_samples_split = 1000,min_samples_leaf = 30,max_features = None,\n                                 random_state = 0)\n\nfit_model(gbm,x_train,y_train)","458e68a1":"print(plot_confusion_matrix(gbm,x_test,y_test))\nprint(classification_report(y_test,gbm.predict(x_test)))","5e363306":"plt.rcParams['figure.figsize'] = (12,8)\nplt.style.use(\"ggplot\")\n\ngbm = GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n                           learning_rate=0.08, loss='deviance', max_depth=5,\n                           max_features=None, max_leaf_nodes=None,\n                           min_impurity_decrease=0.0, min_impurity_split=None,\n                           min_samples_leaf=30, min_samples_split=1000,\n                           min_weight_fraction_leaf=0.0, n_estimators=80,\n                           n_iter_no_change=None, presort='deprecated',\n                           random_state=0, subsample=1.0, tol=0.0001,\n                           validation_fraction=0.1, verbose=0,\n                           warm_start=False)\n\nviz = FeatureImportances(gbm)\nviz.fit(x_train, y_train)\nviz.show();","6d724244":"#### Ignore warnings","51813593":"#### Male subjects are more than female subjects.","53e2b97a":"### Out of the 32561 observations, 31042 are 0.","f1462662":"## General information about the features present in the dataset","1cebc98d":"#### I hope you found the kernel useful. Any suggestions or improvements are welcome.","5edf99f2":"### The variable 'occupation' also exhibits high cardinality.","3786501f":"# 6.Reducing the cardinality of variables","af3d6995":"## Column names","f5945f1b":"#### This variable also contains '?' as observations.","57806986":"Decision Tree gives an accuracy of 0.81.\n\nThe precision and recall for the majority class (<=50K) in the target varible is good and there is an improvment in the minority class (>50K) as compared to Logistic Regression.","5752e716":"### The 'native_country' variable has 42 categories and most of the data is concentrated in the 'United States' category. This is a very high number and will affect the accuracy of the model. ","77e68687":"# 8.Hyperparameter Tuning","f8533a00":"### After reducing the cardinality of the variables:\n1. Accuracy of Logistic Regression improved significantly\n2. Accuracy of Decision Tree Classifier reduced by a small margin\n3. Accuracy of Random Forest Classifier reduced by a small margin\n4. Accuracy of Gradient Boosting Classifier improved by a small margin","c4e47f39":"'native_country' reduced from 42 categories to 5 categories","8d0c9a6b":"## Shape of the dataset","814b80a4":"# 5.Baseline Models","fbd6ac76":"### The 'marital_status' variable also exhibits the same trend. ","7c21cbfd":"### Out of the 32561 observations, 29849 are 0.","c2239b04":"## Decision Tree Classifier","5bc03063":"## Checking the dataset for any missing values","24d41148":"## Gradient Boosting Classifier","f077a3c6":"#### The accuracy of random forest also decreased by a small margin but it can be increased.","1454d024":"#### This plot seems better the previous distplot. \n#### As seen from the plot, most of the subjects are concentrated between the ages 20 to 60.","ce3e32fb":"#### The accuracy of Gradient Boosting Classifier increased from 0.861 to 0.865 after tuning the hyperparameters. ","a5692534":"'workclass' reduced from 9 categories to 5 categories.","d7dd4c05":"'education_num' reduced from 16 categories to 4 categories.","922eca18":"# 1.Problem Statement\n\n### This is a binary classification problem where the target variable is whether a person's income is lower than or equal to 50K (<=50K) or higher than 50K (>50K). \n\n### The models that will be used include LogisticRegression, DecisionTreeClassifier, RandomForestClassifier and GradientBoostingClassifier. The accuracy for each of the models will be evaluated and the best performing model will be selected as the final model.","4139fd44":"#### The accuracy of the decision tree decreased by a small margin but it can be improved by tuning the hyperparameters.","633889c9":"## Overview of the categorical variables","ed511d1b":"# 4.Exploratory Data Analysis","48485134":"In the following steps, the cardinality of the variables will be reduced individually.","154e3100":"## Random Forest Classifier","d1c48ce4":"#### I could not find more information about this variable anywhere so I don't exactly know what it represents.","6cfd0c04":"#### Creating a list of the categorical variables for easy indexing","5a22a95e":"## Overview of the numerical columns","d18f84b3":"#### 'newton-cg' is the best performing solver","f3a1e6b6":"Logistic Regression gives an accuracy of 0.79.\n\nThe precision and recall for the majority class (<=50K) in the target varible is good but is not satisfactory for the minority class (>50K).","7edf1d51":"#### Replacing '?' with unknown","b1923b5e":"#### The accuracy of Random Forest Classifier improved from 0.847 to 0.858 after tuning the hyperparameters.","72de7148":"#### Replace '?' with 'Unknown'","9abb17d5":"#### If you can see in the output above, there are unnecessary whitespaces present in the column names which may lead to an error while indexing the data in the further steps. Below, the whitespaces have been removed manually.\n\n#### Then, the hyphens in the column names have been replaced with underscores.","0ee96297":"### The same trend is seen in the 'education_num' variable as most of the data is concentrated in specific categories.","e5b23002":"## Logistic Regression","b3e8cb0e":"### As some of the independent variables are categorical, they have to be converted into numerical data as the models require the data to be numeric for fitting. There are multiple ways to do this and here the 'pd.getdummies()' function is used. ","c6a6575f":" #### Checking for the best value of solver","4d518341":"#### Checking for the best value of penalty and C","43c5ed5f":"Gradient Boosting Classifier gives an accuracy of 0.86.\n\nThere is also an improvement in the precision and recall.","78b0bd77":"## Random Forest Classifier","56c29b4f":"## Logistic Regression","aa7aa3fa":"## Defining a function to evalute the models\n\n#### This is not a necessary step but makes process easier as it prevents from writing the same lines of code for every model.","8d92e25a":"#### As seen in the output above, the dataset does not seem to have any missing data.","959e0bcd":"'race' reduced from 5 categories to 3 categories.","4aaa6fa7":"'marital_status' reduced from 7 categories to 4 categories.","9fd1a7e7":"#### Seperating the data into dependent and independent variables","744611d9":"#### Best performing values of 'C' and 'penalty' are 0.1 and l2 respectively.","cfa191a8":"#### The best performing model among the 4 models was the Gradient Boosting Classifier with an accuracy of 0.865.","a4e981a2":"#### As seen in the plot above, there is a moderate imbalance in the two classes of the target variable.","96c2febc":"The variables with high cardinality were:\n1. workclass\n2. education\n3. education_num\n4. marital_status\n5. occupation\n6. race\n7. native_country","218ccec9":"#### Most of the subjects are white.","5fc7c065":"# 7.Evaluating models on updated dataset","66876029":"#### In the following steps, each feature will be studied individually to get a better understanding about the dataset.","4d8b242f":"### This was the performance of the models on the data without any transformations. After, performing transformations on the data it is expected that the accuracy of the models will increase.","944e2da2":"### The variable 'education' also has a lot of categories with most of the data being concentrated in specific categories. This variable also has high cardinality.","1db15e39":"#### The accuracy of Decision Tree Classifier improved from 0.807 to 0.823 after tuning the hyperparameters.","0d95a8ab":"## Gradient Boosting Classifier","ca229b9e":"#### The accuracy of Logistic Regression improved from 0.832 to 0.846 after tuning the hyperparameters.","6097c54f":"'occupation' reduced from 15 categories to 9 categories.","565663dd":"### Feature Importance","03db2b04":"'education_num' has categories from 1 to 16. The categories will be binned into 4 seperate categories each category containing 4 categories serially.","c050af1e":"#### The accuracy of the model improved from 0.79 to 0.83 with an increase in the precision and recall of the minority class (>50K) too.","b945e43c":"### During the initial analysis it was seen that some of the variables have a lot of categories while the data is concentrated in specific categories and some of the categories have very less data.The presence of a large number of categories affects the accuracy of the model.","78d0c24b":"Random Forest gives an accuracy of 0.84.\n\nThere is also an improvement in the precision of the minority class (>50K).","6bb10329":"# 9.Conclusion","cc47223b":"## Logistic Regression","614c962a":"'education' reduced from 16 categories to 6 categories.","c6bfe38d":"#### Creating a copy of the dataset for further processing","6b812df5":"#### The distplot does not give much information about the age variable. To gain some more insight, bins of age groups are created manually. ","2aa579fd":"### Models fill be fit on the raw data to get a baseline for each model. This will help in understanding if the models improved after performing some feature engineering.","a6506c8b":"### Train-Test Split","aec5b98b":"## Decision Tree Classifier","2a995b5c":"# 2.Import Libraries","8dccdebc":"#### When checked previously, there were no missing values but '?' is present as observations in the data.","0204ee2f":"## Dataset with the formatted column names","9d7698fa":"All the categories with less than 5% of the data will be clubbed together as one category called 'Other'.","dd8c30f0":"# 4.Overview of the dataset","0e87e907":"# 3.Import Dataset","1922c591":"#### Classification report and confusion matrix of the best performing model ","84d53e5d":"#### The general trend of the exploration will be:\n1. Distplot for numerical data\n2. Checking unique categories in categorical data followed by a countplot ","725aa038":"#### The accuracy of gradient boosting classifier improved with an improvement in the precision of the minority class (>50K).","2095eaa9":"### As seen in the data, majority of the 'workclass' is 'Private'. Some of the categories have very few observations and lead to an unnecessary increase in the cardinality of the variable. The problem of cardinality will be addressed later."}}