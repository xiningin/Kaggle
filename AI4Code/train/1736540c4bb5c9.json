{"cell_type":{"68dc74a1":"code","b01d8c40":"code","c0bb100e":"code","b0f7d07a":"code","ace7a312":"code","256a61e9":"code","20d7baa4":"code","1c846904":"code","1cf93f71":"code","f1224a85":"code","a6b18af8":"code","f77a8211":"code","e080575a":"code","1da4feb7":"code","4497fa6a":"code","aae73401":"code","f30acf3e":"code","26c18330":"code","586c3a81":"code","292141c2":"code","261ff6ce":"code","99c28f2a":"code","457e27f5":"code","c8dcd076":"code","d2b4e10e":"code","8ca630db":"markdown","5f4751ae":"markdown","f142d9ef":"markdown","00d30bd8":"markdown","196af602":"markdown","781ee0af":"markdown","a46244c5":"markdown","b08ec899":"markdown","93d077d2":"markdown","78d3b0a9":"markdown","39eab73a":"markdown","29b76723":"markdown","19670ef8":"markdown","d3f60ef3":"markdown","a0733d80":"markdown","d8c8b133":"markdown","7d07f14c":"markdown","ccabeb89":"markdown","b71a8a10":"markdown"},"source":{"68dc74a1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","b01d8c40":"# load library\nfrom sklearn.datasets import load_iris\n\n#import IRIS dataset\niris = load_iris()","c0bb100e":"# Describe the data\nprint(iris.DESCR)","b0f7d07a":"x = iris.data\ny = iris.target\nfeatures = iris.feature_names\ntarget = iris.target_names\n\nprint(\"Feature Names:\",features)\nprint(\"-\"*100)\nprint(\"Target Names:\", target)\nprint(\"-\"*100)\nprint(\"data:\", x[:10])\nprint(\"-\"*100)","ace7a312":"# checking shape of dataset before spliting\nprint(x.shape)\nprint(y.shape)\n\nfrom sklearn.model_selection import train_test_split\n\n# spliting data in test and train set keeping 70% data in train set and 30% data in test set. \nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 1)\n","256a61e9":"# checking shape of dataset \"after\" spliting\nprint(\"Train Data Details\")\nprint(X_train.shape)\nprint(X_test.shape)\n\nprint(\"-\"*50)\n\nprint(\"Test Data Details\")\nprint(y_train.shape)\nprint(y_test.shape)","20d7baa4":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics","1c846904":"\n# DT Classifier\ndt = DecisionTreeClassifier()\n\n# Lets fit the data into classifier \ndt.fit(X_train, y_train)\n\n# predict on test data\ny_pred = dt.predict(X_test)\n\n#confusion matrix\ncm = metrics.confusion_matrix(y_test, y_pred)\n\n\n# Confusion Matrix\nimport seaborn as sns\nsns.heatmap(cm, annot=True)","1cf93f71":"# let's check Classification accuracy\nprint( \"Classification Accuracy:  \", dt.score(X_test, y_test))\n\nprint(\"*\"*50)\n\n# Recall Score\nfrom sklearn.metrics import recall_score\nprint(\"Recall Score: \", recall_score(y_test, y_pred, average='macro'))\n\nprint(\"*\"*50)\n\n# Precision Score\nfrom sklearn.metrics import precision_score\nprint(\"Precision Score: \", precision_score(y_test, y_pred, average='macro'))\n\n\nprint(\"*\"*50)\n\n# F1 Score \n# f1 Score = 2 * (precision * recall) \/ (precision + recall)\n\nfrom sklearn.metrics import f1_score\nprint(\"F1 Score\", f1_score(y_test, y_pred, average='macro'))\n","f1224a85":"from sklearn.svm import LinearSVC\n\n#Linear SVC Classifier\nclf = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, C=100, multi_class='ovr')\n\n# Fitting training data into classifier\nclf.fit(X_train,y_train)\n\n# Accuracy\nprint('Accuracy of linear SVC on training set: {:.2f}'.format(clf.score(X_train, y_train) * 100))\nprint('Accuracy of linear SVC on test set: {:.2f}'.format(clf.score(X_test, y_test) *100))\n\ny_pred = clf.predict(X_test)","a6b18af8":"#confusion matrix\ncm = metrics.confusion_matrix(y_test, y_pred)\n\n# Confusion Matrix\nimport seaborn as sns\nsns.heatmap(cm, annot=True)","f77a8211":"# Recall Score\nfrom sklearn.metrics import recall_score\nprint(\"Recall Score: \", recall_score(y_test, y_pred, average='macro'))\n\nprint(\"*\"*50)\n\n# Precision Score\nfrom sklearn.metrics import precision_score\nprint(\"Precision Score: \", precision_score(y_test, y_pred, average='macro'))\n\n\nprint(\"*\"*50)\n\n# F1 Score \n# f1 Score = 2 * (precision * recall) \/ (precision + recall)\n\nfrom sklearn.metrics import f1_score\nprint(\"F1 Score\", f1_score(y_test, y_pred, average='macro'))\n","e080575a":"from sklearn.svm import SVC\n\n# SVC Classifier\nclf_SVC = SVC(C=100.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, \n          probability=False, tol=0.001, cache_size=200, class_weight=None, \n          verbose=0, max_iter=-1, decision_function_shape=\"ovr\", random_state = 0)\n\n# Fitting training data\nclf_SVC.fit(X_train,y_train)\n\n# predictions\ny_pred = clf_SVC.predict(X_test)\n\n# predicting accuracies\nprint('Accuracy of SVC on training set: {:.2f}'.format(clf_SVC.score(X_train, y_train) * 100))\n\nprint('Accuracy of SVC on test set: {:.2f}'.format(clf_SVC.score(X_test, y_test) * 100))\n","1da4feb7":"#confusion matrix\ncm = metrics.confusion_matrix(y_test, y_pred)\n\n# Confusion Matrix\nimport seaborn as sns\nsns.heatmap(cm, annot=True)","4497fa6a":"# Recall Score\nfrom sklearn.metrics import recall_score\nprint(\"Recall Score: \", recall_score(y_test, y_pred, average='macro'))\n\nprint(\"*\"*50)\n\n# Precision Score\nfrom sklearn.metrics import precision_score\nprint(\"Precision Score: \", precision_score(y_test, y_pred, average='macro'))\n\n\nprint(\"*\"*50)\n\n# F1 Score \n# f1 Score = 2 * (precision * recall) \/ (precision + recall)\n\nfrom sklearn.metrics import f1_score\nprint(\"F1 Score\", f1_score(y_test, y_pred, average='macro'))\n","aae73401":"from sklearn.neural_network import MLPClassifier # neural network\n\n# Classifier\nclf = MLPClassifier(alpha=1e-5, hidden_layer_sizes=(3, 3), random_state=1)  # try once with solver='lbfgs',\n\n#Fiting trainging data\nclf.fit(X_train, y_train)\n\n#predicting the data\ny_pred = clf.predict(X_test)\n\nprint('The accuracy of the Multi-layer Perceptron is:',metrics.accuracy_score(y_pred, y_test))","f30acf3e":"#confusion matrix\ncm = metrics.confusion_matrix(y_test, y_pred)\n\n# Confusion Matrix\nimport seaborn as sns\nsns.heatmap(cm, annot=True)","26c18330":"# Recall Score\nfrom sklearn.metrics import recall_score\nprint(\"Recall Score: \", recall_score(y_test, y_pred, average='macro'))\n\nprint(\"*\"*50)\n\n# Precision Score\nfrom sklearn.metrics import precision_score\nprint(\"Precision Score: \", precision_score(y_test, y_pred, average='macro'))\n\n\nprint(\"*\"*50)\n\n# F1 Score \n# f1 Score = 2 * (precision * recall) \/ (precision + recall)\n\nfrom sklearn.metrics import f1_score\nprint(\"F1 Score\", f1_score(y_test, y_pred, average='macro'))\n","586c3a81":"from sklearn.datasets import fetch_20newsgroups\ntwenty_train = fetch_20newsgroups()\n\nprint(twenty_train.DESCR)","292141c2":"\n# text data sample\ntrain_data = twenty_train.data\n\n# length of data\nprint(\"Length of complete Data: \",len(train_data))\nprint(\"*\"*50)\n\n#sample data point (news)\nprint(\"sample data point \\n\"+\"* *\"*20)\nprint(train_data[1])","261ff6ce":"import nltk\nfrom nltk.corpus import stopwords\nprint(\"STOPWORDS \\n\",stopwords.words('english'))","99c28f2a":"# Reference: https:\/\/gist.github.com\/sebleier\/554280\n\nimport re\nfrom tqdm import tqdm\nfrom bs4 import BeautifulSoup\n\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n\npreprocessed = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(train_data):\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = decontracted(sentance)\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords.words('english'))\n    preprocessed.append(sentance.strip())","457e27f5":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vect = CountVectorizer() #in scikit-learn\ncount_vect.fit(preprocessed)\nprint(\"some feature names \", count_vect.get_feature_names()[:10])\nprint('='*50)\n\nfinal_counts = count_vect.transform(preprocessed)\nprint(\"the type of count vectorizer \",type(final_counts))\nprint(\"the shape of out text BOW vectorizer \",final_counts.get_shape())\nprint(\"the number of unique words \", final_counts.get_shape()[1])","c8dcd076":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntf_idf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=10)\ntf_idf_vect.fit(preprocessed)\nprint(\"some sample features(unique words in the corpus)\",tf_idf_vect.get_feature_names()[0:10])\nprint('='*50)\n\nfinal_tf_idf = tf_idf_vect.transform(preprocessed)\nprint(\"the type of count vectorizer \",type(final_tf_idf))\nprint(\"the shape of out text TFIDF vectorizer \",final_tf_idf.get_shape())\nprint(\"the number of unique words including both unigrams and bigrams \", final_tf_idf.get_shape()[1])","d2b4e10e":"#import the necessary module\nfrom sklearn import preprocessing\n\n# create the Labelencoder object\nle = preprocessing.LabelEncoder()\n\n#convert the categorical columns into numeric\nencoded_value = le.fit_transform([\"NLP\", \"ML\", \"Artificial Intelligence\", \"Deep Learning\", \"ML\"])\nprint(\"\"\"Encoding of [\"NLP\", \"ML\", \"Artificial Intelligence\", \"Deep Learning\", \"ML\"] \"\"\")\nprint(encoded_value)","8ca630db":"### 2.2.1 Linear SVM Classifier","5f4751ae":"Separating ipnut variables and output variables in the data according to the data we receiv from scikit-learn (iris dataset).","f142d9ef":"### Sklearn Preprocessing (Label Encoder)\n\nIf we had a dataset with strings in it! We would have needed to convert the categorical values into the numeric values. \nSee below how each \"string\" value has been encoded below.","00d30bd8":"## 2.1 Decision Tree Classifier","196af602":"### 2.2.2 SVC Classifier","781ee0af":"# 2. MODELS","a46244c5":"## Basic Preprocessing","b08ec899":"## 2.3 Multi Layer Perceptron (MLP) Classifier","93d077d2":"## Bag of Words Vectoriser","78d3b0a9":"### Different Metrics on predictions from Decision tree classifier","39eab73a":"# Let's start with basics of scikit-learn","29b76723":"## 1.1 Describing the dataset","19670ef8":"## TFIDF","d3f60ef3":"## 2.2 SVM Classifier","a0733d80":"# What if we had TEXT data?\n","d8c8b133":"-------------------\n--------------\n--------------\n","7d07f14c":"## 1. Loading Dataset","ccabeb89":"## 1.2 Data Split","b71a8a10":"--------------\nLater will add modeling on text data too. "}}