{"cell_type":{"efa158bc":"code","a70fd43c":"code","2c20513b":"code","fd18b0f6":"code","b79cb99e":"code","1ce9edea":"code","2ecbc46c":"code","d497e3bb":"code","49494009":"code","5cba6f4c":"code","38891093":"code","3b21e069":"code","06463e82":"code","71c4d22f":"code","0fd09a74":"code","1f0cec16":"code","0e1e1cd6":"code","312bce82":"code","13010c29":"code","d7c9867f":"code","3f2185bf":"code","eb8712df":"code","3f47b5f1":"code","b4715981":"code","c9f44940":"code","2a416271":"code","9e0ed62b":"code","26b51008":"code","3c253917":"code","fe15b6df":"code","cadba4ad":"code","cc7c13ac":"markdown"},"source":{"efa158bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a70fd43c":"# importing libraries\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport tensorflow as tf\nimport numpy as np\n\n\nfrom tensorflow.keras import layers\nimport time\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","2c20513b":"dtypes = {\n\"duration\": np.int8,\n\"protocol_type\": np.object,\n\"service\": np.object,\n\"flag\": np.object,\n\"src_bytes\":  np.int8,\n\"dst_bytes\":  np.int8,\n\"land\": np.int8,\n\"wrong_fragment\":  np.int8,\n\"urgent\": np.int8,\n\"hot\": np.int8,\n\"m_failed_logins\":  np.int8,\n\"logged_in\":  np.int8,\n\"num_compromised\":  np.int8,\n\"root_shell\":  np.int8,\n\"su_attempted\":  np.int8,\n\"num_root\": np.int8,\n\"num_file_creations\":  np.int8,\n\"num_shells\":  np.int8,\n\"num_access_files\":  np.int8,\n\"num_outbound_cmds\":  np.int8,\n\"is_host_login\":  np.int8,\n\"is_guest_login\":  np.int8,\n\"count\": np.int8,\n\"srv_count\":  np.int8,\n\"serror_rate\": np.float16,\n\"srv_serror_rate\": np.float16,\n\"rerror_rate\": np.float16,\n\"srv_rerror_rate\": np.float16,\n\"same_srv_rate\": np.float16,\n\"diff_srv_rate\": np.float16,\n\"srv_diff_host_rate\": np.float16,\n\"dst_host_count\":  np.int8,\n\"dst_host_srv_count\":  np.int8,\n\"dst_host_same_srv_rate\": np.float16,\n\"dst_host_diff_srv_rate\": np.float16,\n\"dst_host_same_src_port_rate\": np.float16,\n\"dst_host_srv_diff_host_rate\": np.float16,\n\"dst_host_serror_rate\": np.float16,\n\"dst_host_srv_serror_rate\": np.float16,\n\"dst_host_rerror_rate\": np.float16,\n\"dst_host_srv_rerror_rate\": np.float16,\n\"label\": np.object\n}\n\ncolumns = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"m_failed_logins\",\n\"logged_in\", \"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\"num_shells\",\"num_access_files\",\n\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\n\"same_srv_rate\",\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\n\"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\n\"dst_host_srv_rerror_rate\",\"label\"]\n\ndf = pd.read_csv(\"\/kaggle\/input\/kdd-cup-1999-data\/kddcup.data_10_percent_corrected\", sep=\",\", names=columns, dtype=dtypes, index_col=None)","fd18b0f6":"df.label.value_counts()","b79cb99e":"#Feature Selection\nnew_features=['dst_bytes',\n 'logged_in',\n 'count',\n 'srv_count',\n 'dst_host_count',\n 'dst_host_srv_count',\n 'dst_host_same_srv_rate',\n 'dst_host_same_src_port_rate','label']","1ce9edea":"# 0 for normal data and 1 for abnormalities\ndf.label=df.label.apply(lambda x: 0 if x == 'normal.' else 1)","2ecbc46c":"df=df[new_features]","d497e3bb":"#label encoding \nfor column in df.columns:\n    if df[column].dtype == np.object:\n        encoded = LabelEncoder()\n        \n        encoded.fit(df[column])\n        df[column] = encoded.transform(df[column])","49494009":"#randomly sample 500 data point for training\ndf_train=df[df.label==0].sample(500)","5cba6f4c":"#remove the item sampled from our dataset\nindex_list=df_train.index\ndf=df.drop(index_list)","38891093":"#drop the label columns\ndf_train=df_train.drop('label',axis=1)","3b21e069":"df_train.shape","06463e82":"def make_generator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(16, use_bias=False, input_shape=(100,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Dense(16))\n    assert model.output_shape == (None,16 ) # Note: None is the batch size\n    \n    model.add(layers.Dense(32))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Dense(32))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Dense(8))\n    assert model.output_shape == (None,8 )\n   \n    \n    return model","71c4d22f":"def make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(16, use_bias=False,\n                                    input_shape=[1,8]))\n   \n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Dense(32, use_bias=True))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n   \n    model.add(layers.Dense(1))\n   # model.add(layers.Softmax())\n\n    return model","0fd09a74":"generator=make_generator_model()\ndiscriminator=make_discriminator_model()","1f0cec16":"cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)","0e1e1cd6":"def discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss","312bce82":"bce = tf.keras.losses.BinaryCrossentropy()\nloss = bce([1., 1., 1., 1.], [1., 1., 1., 1.])\nprint('Loss: ', loss.numpy())  # Loss: 11.522857","13010c29":"def generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","d7c9867f":"generator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)","3f2185bf":"EPOCHS = 1000\nnoise_dim = 100\n#num_examples_to_generate = 16\nBATCH_SIZE = 64\n\n","eb8712df":"@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n    images=tf.reshape(images,(1,8))\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n      generated_images = generator(noise, training=True)\n\n      real_output = discriminator(images, training=True)\n      fake_output = discriminator(generated_images, training=True)\n\n      gen_loss = generator_loss(fake_output)\n      disc_loss = discriminator_loss(real_output, fake_output)\n     \n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n    \n    return (gen_loss,disc_loss)","3f47b5f1":"history=dict()\nhistory['gen']=[]\nhistory['dis']=[]\ndef train(dataset, epochs):\n    for epoch in range(epochs):\n        start = time.time()\n\n        for batch in dataset:\n\n           gen_loss,dis_loss= train_step(batch)\n        history['gen'].append(gen_loss)\n        history['dis'].append(dis_loss)\n        print ('Time for epoch {} is {} sec '.format(epoch + 1, time.time()-start))","b4715981":"x_train=df_train.values\ntrain(x_train,EPOCHS) ","c9f44940":"plt.plot(history['gen'])","2a416271":"plt.plot(history['dis'])","9e0ed62b":"y_test=df.label\nx_test=df.drop('label',axis=1).values.reshape(-1,1,8)","26b51008":"y_pred=discriminator.predict(x_test)","3c253917":"from sklearn.metrics import average_precision_score, accuracy_score ,recall_score, f1_score\n\n","fe15b6df":"#range of output value\nplt.figure(figsize=(7,7))\nplt.plot(range(len(y_pred)),y_pred.reshape(-1,1))\nplt.ylabel('y_pred')","cadba4ad":"#looking for the optimal probability threshold\n#prob=[0.5,0.55,0.6,0.65,0.75,0.8,0.85,0.9,0.95,1,2,2.5,5]\nprob=[-0.2,-0.1,-0.05,0,0.005,0.1]\nfor p in prob:\n    pred_value =[1 if i<p   else 0 for i in y_pred]\n    f1=f1_score(y_test,pred_value)\n    acc=accuracy_score(y_test,pred_value)\n    precision=average_precision_score(y_test,pred_value)\n    recall= recall_score(y_test,pred_value)\n    print(f'prob ={p} and f1 score ={f1} : accuracy={acc} : precision={precision} : recall={recall}')\n    print(pred_value.count(0),pred_value.count(1))","cc7c13ac":"The best threshold value is prob = 0;\n\nWhile GAN is widely used in computer vision problem, it does perform very well in tabular data. One of the most common problem in anomaly detection is class imbalance. While using GAN, we indirectly overcome the problem as we only training our model with just one of the classes. Besides, we only training our model with just 500 data which is a very small amount when comparing to the data required by other ML\/DL models.\n\nIf you find out any mistakes in my notebook, pls let me know.\n\nReference : https:\/\/www.tensorflow.org\/tutorials\/generative\/dcgan"}}