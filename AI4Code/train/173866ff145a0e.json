{"cell_type":{"a579967c":"code","65580eac":"code","949fbbaf":"code","f79e6d01":"code","79df7388":"code","d764edbd":"code","5b3b93ab":"code","ea98458a":"code","a9ccb9bd":"code","6ce0ba02":"code","f11266ca":"code","25259736":"code","ae7573eb":"code","5f8741fd":"code","00713cb0":"code","357548f7":"markdown","11a8d524":"markdown","a36b39c0":"markdown","5475f2d4":"markdown","ca0f8221":"markdown","24dca94e":"markdown","14c72076":"markdown","a3427088":"markdown","be5474ec":"markdown","3a28d69d":"markdown","519fd189":"markdown","bbfb174a":"markdown","53339a6b":"markdown","2a6ff347":"markdown","82bdf4e8":"markdown","9c644d50":"markdown","3f88eabc":"markdown","b499ae58":"markdown","246a2718":"markdown","26352422":"markdown","bc68d605":"markdown","8ec5ce45":"markdown"},"source":{"a579967c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","65580eac":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\nimport matplotlib.pyplot as plt","949fbbaf":"data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col = 'Id')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col = 'Id')","f79e6d01":"y = data.SalePrice #target data\nX_full = data.copy()\nX = data.drop(['SalePrice'], axis = 1)\nX_test = test.copy()","79df7388":"X_full.head()","d764edbd":"X_full.describe()","5b3b93ab":"X_correlation = X_full.corr() # correlation function\nbest_correlation_variable = X_correlation.index[abs(X_correlation['SalePrice']) > 0.4]\n\nplt.figure(figsize = (10,10))\ngraph = sns.heatmap(X_full[best_correlation_variable].corr(), annot = True, cmap=\"Blues\")","ea98458a":"plt.figure(figsize = (12,10))\n\nplt.xlabel(\"OverallQual\",fontsize = 14)\nplt.ylabel(\"SalePrice\",fontsize = 14)\nsns.barplot(X_full.OverallQual,X_full.SalePrice)\nplt.show()","a9ccb9bd":"X_train_full, X_valid_full, y_train_full, y_valid_full = train_test_split(X, y, \n                                                        train_size=0.8, test_size=0.2,\n                                                      random_state=0)\n","6ce0ba02":"# All categorical columns\nobject_cols = [col for col in X_train_full.columns if X_train_full[col].dtype == \"object\"]\n\n# Columns that can be safely label encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_train_full[col]) == set(X_valid_full[col])]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n\nX_train_full.drop(bad_label_cols, axis = 1, inplace = True)\nX_valid_full.drop(bad_label_cols, axis = 1, inplace = True)\n\n# Remove missing columns\ncols_with_missing = [col for col in X_train_full.columns \n                     if X_train_full[col].isnull().sum() > 0.1*X_full.shape[0]]\n\nX_train_full.drop(cols_with_missing, axis = 1, inplace = True)\nX_valid_full.drop(cols_with_missing, axis = 1, inplace = True)\n\n# Columns that will be one-hot encoded\nlow_cardinality_cols = [cname for cname in X_train_full.columns \n                        if X_train_full[cname].nunique() < 10 \n                        and X_train_full[cname].dtype == 'object']\n\n# Columns of numerical data\nnume_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype\n                                                    in ['int64', 'float64']]\n\nmy_cols = low_cardinality_cols + nume_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = test[my_cols].copy()","f11266ca":"def model_definition(nume_cols, low_cardinality_cols, si_numerical_strategy, \n                     si_categorical_strategy, \n                     n_estimators, random_state):\n\n    numerical_transformer = SimpleImputer(strategy = si_numerical_strategy)\n\n    categorical_transformer = Pipeline(steps = [\n        ('imputer', SimpleImputer(strategy = si_categorical_strategy)),\n        ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n    ])\n\n    preprocessor = ColumnTransformer(\n        transformers = [\n            ('num', numerical_transformer, nume_cols),\n            ('cat', categorical_transformer, low_cardinality_cols)\n        ])\n    \n    model_RFR = RandomForestRegressor(n_estimators = n_estimators, \n                                      random_state = random_state)\n      \n    my_pipeline_RFR = Pipeline(steps = [('preprocessor', preprocessor), ('model', model_RFR)\n                                       ])\n\n    return my_pipeline_RFR\n\n\ndef cross_validation_scores(my_pipeline, X, y):\n    # Predicting the data\n    mae_score = -1 * cross_val_score(my_pipeline, X, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n\n    return mae_score.mean()\n\ndef get_XGB(X_train, X_valid, X_test, y_train, y_valid, learning_rate, \n            n_estimator, n_jobs):\n    \n    X_train = pd.get_dummies(X_train)\n    X_valid = pd.get_dummies(X_valid)\n    X_test = pd.get_dummies(X_test)\n    X_train, X_valid = X_train.align(X_valid, join='left', axis=1)\n    X_train, X_test = X_train.align(X_test, join='left', axis=1)\n    \n    model_xgb = XGBRegressor(n_estimators = n_estimator, learning_rate = learning_rate, \n                         n_jobs = n_jobs)\n    model_xgb.fit(X_train, y_train, \n             early_stopping_rounds = 5, \n             eval_set = [(X_valid, y_valid)], \n             verbose = False)\n    \n    prediction = model_xgb.predict(X_valid)\n    mae_xgb = mean_absolute_error(prediction, y_valid)\n    \n    return model_xgb, mae_xgb","25259736":"si_categorical_strategy = ['most_frequent', 'constant']\n\nn_estimators = []\nresults_mae = {} # dictionary for the results\nfor estimator in range(100,1000,100):\n    pipeline_RFR = model_definition(nume_cols, low_cardinality_cols, 'mean', \n                                        'most_frequent', estimator, 0)\n    results_mae[estimator] = cross_validation_scores(pipeline_RFR, X, y)\n    n_estimators.append(estimator)","ae7573eb":"plt.figure(figsize = (20,12))\nplt.xlabel(\"Estimators Number\",fontsize = 16)\nplt.ylabel(\"Mean Absolute Error (MAE)\",fontsize  =16)\nplt.title(\"Number of RandomForest Estimators vs Mean Absolute Error (MAE)\",fontsize  =16)\nplt.plot(list(results_mae.keys()), list(results_mae.values()), label = 'Mean absolute error')\nplt.show()","5f8741fd":"maes_XGB = []\nmodels_XGB = []\n\nfor learn_r in np.arange(0.01,0.11,0.01):\n    \n    model_xgb, mae_xgb = get_XGB(X_train, X_valid, X_test, y_train_full, y_valid_full, \n                                 learn_r, 900, 2)\n    \n    models_XGB.append(model_xgb)\n    maes_XGB.append(mae_xgb)","00713cb0":"plt.figure(figsize=(20,10))\nplt.xlabel(\"Learning rate value\",fontsize=16)\nplt.ylabel(\"Mean Absolute Error (MAE)\",fontsize=16)\nplt.title(\"Learning rate vs MAE (n_estimators = 900)\",fontsize=16)\nplt.plot(np.arange(0.01,0.11,0.01),maes_XGB)\nplt.legend()\nplt.show()","357548f7":"Using the n_estimator = 900, since only then its possible to get the lowest MAE.","11a8d524":"As can be seen from the graph, the value of the MAE falls as the *n_estimators* grows. For *n_estimators = 900* the curve has its minimum. The strategy adopted was to use the *most_frequent* method for pre-processing categorical data and the *mean* method for numerical data, since these had better performance during the theoretical study. Thus, the MAE tends to increase as the *n_estimator* value decreases. \n\n* The strategy adopted was remove the columns that are not in the train and validation dataset.","a36b39c0":"Weverton Domingos de Medeiros - weverton.medeiros@ee.ufcg.edu.br\n\n2nd project - **Basic and Intermediate Machine Learning.**\n\n![](https:\/\/thumbs.dreamstime.com\/z\/rising-house-prices-icon-stock-vector-154532259.jpg)","5475f2d4":"# Pipeline","ca0f8221":"# Preprocessing\n\nIn this section, the initial selection of the features to be considered will be made. Features that had at least 10% of missing data are automatically disregarded. Another exclusion factor was the cardinality of categorical variables. Variables with a cardinality greater than 10 were removed. \n\nIn addition, variables X_train_full, X_valid_full, y_train, y_valid were created from the method *train_test_split*.\n\n* X_train_full: contains all train variables from dataset\n* X_valid_full: contains all validation variables from dataset\n* y_train: contains the target variable from train dataset\n* y_valid: contains the target variable from  validation dataset\n\nSeparating 80% of the training dataset to train the model, and 20% for validation, has up:","24dca94e":"Libraries applied to the development of the EDA, preprocessing and model evaluation.","14c72076":"By the analysis of *OverallQual*, that is the variable with highest correlation with *SalePrice*, it is remarkable that, although the high standard deviation, the price is as high as the quality of the material and the finishing of the house.","a3427088":"# Conclusions\n\nFor the *Random Forest Regressor* model was possible to find the ideal number os estimators based on the strategies adopted during the pre-processing and creation of the pipeline. About XGBRegressor, it can be observed that it reduces the MAE, but changing learning_rate, but there are other parameters that can be studied and applied in a trail to achieve better performances. With this project was possible to increase the performance of the model. Then, the project objectives were achieved, since it was possible to apply all knowledge in the processing of variables and evaluation of results","be5474ec":"# Xtreme Gradient Boosting","3a28d69d":"Next, processing was performed with respect to variables with high quantity of missing values, columns with high cardinality. Further, columns that were not present in both training and validation data sets have been removed.","519fd189":"Data collect from dataset.","bbfb174a":"* X_full: variable which have all the dataset provided on the Kaggle, less the target.\n* X_test: variable that contains the testset.\n* X_full: variable that contains all the dataset.","53339a6b":"All the variables have a considerably good correlation with *Sale Price*, but the variables that stand out due to the high correlation are *OverallQual*, *GrLivArea*, *GarageCars*, *GarageArea*, as presented on the heatmap above.","2a6ff347":"In this project is proposed the use of some Machine Learning techniques, such as Random Forest Regressor, to calculate the value of the properties associated with key features of the Dataset, as well as to evaluate them too.","82bdf4e8":"## Model: *Random Forest Regressor*\n\nBelow, the coding techniques are applied and the training process begins through the previously declared model_definition() function. The n_estimators parameter of the model was varied from 100 to 1000, with a step of 50, and then was observed the mean absolute error (MAE). From then on, was possible to decide which number of estimators produces the best result in the validation.","9c644d50":"## Functions developed for the project\n\n* **model_definition(nume_cols, low_cardinality_cols, si_numerical_strategy, si_categorical_strategy, n_estimators, random_state):** a function that returns the model and pipelined defined for the post-processing.\n\n* **cross_validation_scores(my_pipeline, X, y):** a function thar return the cross validation scores for the pipeline defined to the model, aiming better model performance.\n\n* **def get_XGB(X_train, X_valid, X_test, y_train, y_valid, learning_rate,             n_estimator, n_jobs):** a function to do the gradient boosting.","3f88eabc":"# Dataset description\n## Name - *House Prices: Advanced Regression Techniques*\n\n### Main variables to this analysis\n\nData fields\nHere's a brief version of what you'll find in the data description file.\n\n* SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n* MSSubClass: The building class\n* MSZoning: The general zoning classification\n* LotFrontage: Linear feet of street connected to property\n* LotArea: Lot size in square feet\n* Street: Type of road access\n* Alley: Type of alley access\n* LotShape: General shape of property\n* LandContour: Flatness of the property\n* Utilities: Type of utilities available\n* LotConfig: Lot configuration\n* LandSlope: Slope of property\n* Neighborhood: Physical locations within Ames city limits\n* Condition1: Proximity to main road or railroad\n* Condition2: Proximity to main road or railroad (if a second is present)\n* BldgType: Type of dwelling\n* HouseStyle: Style of dwelling\n* OverallQual: Overall material and finish quality\n* OverallCond: Overall condition rating\n* YearBuilt: Original construction date\n* YearRemodAdd: Remodel date\n* RoofStyle: Type of roof\n* RoofMatl: Roof material\n* Exterior1st: Exterior covering on house\n* Exterior2nd: Exterior covering on house (if more than one material)\n* MasVnrType: Masonry veneer type\n* MasVnrArea: Masonry veneer area in square feet\n* ExterQual: Exterior material quality\n* ExterCond: Present condition of the material on the exterior\n* Foundation: Type of foundation\n* BsmtQual: Height of the basement\n* BsmtCond: General condition of the basement\n* BsmtExposure: Walkout or garden level basement walls\n* BsmtFinType1: Quality of basement finished area\n* BsmtFinSF1: Type 1 finished square feet\n* BsmtFinType2: Quality of second finished area (if present)\n* BsmtFinSF2: Type 2 finished square feet\n* BsmtUnfSF: Unfinished square feet of basement area\n* TotalBsmtSF: Total square feet of basement area\n* Heating: Type of heating\n* HeatingQC: Heating quality and condition\n* CentralAir: Central air conditioning\n* Electrical: Electrical system\n* 1stFlrSF: First Floor square feet\n* 2ndFlrSF: Second floor square feet\n* LowQualFinSF: Low quality finished square feet (all floors)\n* GrLivArea: Above grade (ground) living area square feet\n* BsmtFullBath: Basement full bathrooms\n* BsmtHalfBath: Basement half bathrooms\n* FullBath: Full bathrooms above grade\n* HalfBath: Half baths above grade\n* Bedroom: Number of bedrooms above basement level\n* Kitchen: Number of kitchens\n* KitchenQual: Kitchen quality\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n* Functional: Home functionality rating\n* Fireplaces: Number of fireplaces\n* FireplaceQu: Fireplace quality\n* GarageType: Garage location\n* GarageYrBlt: Year garage was built\n* GarageFinish: Interior finish of the garage\n* GarageCars: Size of garage in car capacity\n* GarageArea: Size of garage in square feet\n* GarageQual: Garage quality\n* GarageCond: Garage condition\n* PavedDrive: Paved driveway\n* WoodDeckSF: Wood deck area in square feet\n* OpenPorchSF: Open porch area in square feet\n* EnclosedPorch: Enclosed porch area in square feet\n* 3SsnPorch: Three season porch area in square feet\n* ScreenPorch: Screen porch area in square feet\n* PoolArea: Pool area in square feet\n* PoolQC: Pool quality\n* Fence: Fence quality\n* MiscFeature: Miscellaneous feature not covered in other categories\n* MiscVal: Value of miscellaneous feature\n* MoSold: Month Sold\n* YrSold: Year Sold\n* SaleType: Type of sale\n* SaleCondition: Condition of sale\n\n### File description\n\n* train.csv - the training set\n* test.csv - the test set\n* data_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here","b499ae58":"# Libraries import and functions developed","246a2718":"The sale price of the house varies widely, depending on their features. It can be seen that the standard deviation varies almost S$79,500.00, and the difference between the lowest and highest value is very high. For these reasons a correlation analysis among the *SalePrice* variable and the other variables is plausible ot understand which ones have the greatest influence on the price of the houses. ","26352422":"For the method **Gradient Boosting**, there is a parameter called *learning_rate*, which is multipled by the model predictions. The idea is that each new tree added to the model help less without overfitting. With a low learning rate and a high number of estimators is possible to achieve predictions more accurate, with lowest MAE. The trade-off here is between the gain and the time, because it takes longer to process.\n\n* For the XGBoost, was used n_estimator = 900 and the lowest was achieved with the learning_rate = 0.09.","bc68d605":"# Exploratory data analysis\n\nDataset overview about the characteristics of its variables.","8ec5ce45":"### Reading training, test and defining target data"}}