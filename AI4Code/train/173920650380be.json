{"cell_type":{"7432f9f7":"code","45a2680a":"code","99c5a7ea":"code","118a40ec":"code","7a016b1c":"code","489fa3e3":"code","43387329":"code","f0c1a426":"code","e8665a23":"code","6a853f73":"code","bf5dc66d":"code","caa3e0ef":"code","f29819e2":"code","5b733b97":"code","f3ebfba3":"code","7fd02321":"code","ec3831b2":"code","91b01f49":"code","14713950":"code","291bdc8f":"code","2529ad44":"code","0a96c41f":"code","ef7fd3b0":"code","e95155f2":"code","ca37b39c":"code","1d2ee61a":"code","0b0b89e7":"code","1a72a826":"code","ab29f088":"code","39e70bac":"code","73c2cf35":"code","2b4adb14":"markdown","f66262c6":"markdown","1f8ba046":"markdown","7594ca56":"markdown","3997c890":"markdown","211f74f3":"markdown","30085f41":"markdown"},"source":{"7432f9f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","45a2680a":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, BatchNormalization, AveragePooling2D, Flatten\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nrandom_seed = 1\nnp.random.seed(random_seed)","99c5a7ea":"# loading training data\ntrain = pd.read_csv('..\/input\/train.csv')\ntrain.shape","118a40ec":"# loading test data\ntest = pd.read_csv('..\/input\/test.csv')\ntest.shape","7a016b1c":"train.head()","489fa3e3":"test.head()","43387329":"# Dividing Independent and Dependent features in dataset\nY = train[\"label\"].values\nX = train.drop([\"label\"],axis=1).values","f0c1a426":"# Dimensions of each example (a gray image with 28 x 28 is flattern into 784)\nX[0].shape","e8665a23":"# Visualizing first example in dataset\nplt.imshow(np.reshape(X[0],(28,28)))\nplt.title(Y[0])\nplt.show()","6a853f73":"# Plotting count of each digit\nsns.countplot(Y)","bf5dc66d":"# Count of each digit\ntrain[\"label\"].value_counts()","caa3e0ef":"# Checking null values\ntrain.isnull().sum().sum()","f29819e2":"# Plotting grid of first 12 examples\nplt.figure(figsize=(10,6))\nfor i in range(12):\n    plt.subplot(3,4,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(np.reshape(X[i],(28,28)))\n    plt.title(Y[i])\n \nplt.show()","5b733b97":"# Normalizing pixel values from (0 to 255) to (0 to 1)\nX = X\/255\ntest \/= 255\n#Y is digit label, No need of normalization and Test data do not have labels.","f3ebfba3":"# Checking normalized values\nprint(X.max(), X.min())","7fd02321":"# Reshaping images\nX = X.reshape(-1,28,28,1)  # training examples\ntest = test.values.reshape(-1,28,28,1) #test examples","ec3831b2":"# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nY = to_categorical(Y, num_classes = 10)","91b01f49":"# Spliting into Train dataset and Validating(development dataset)\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.1, random_state=random_seed)","14713950":"# Checking shape of train data\nX_train.shape","291bdc8f":"# Defining LeNet model\ndef lenet5():\n    model = Sequential()\n    model.add(Conv2D(filters=6, kernel_size=(5,5), strides=(1,1),padding='same', activation='relu', name='Conv2D0', input_shape=(28,28,1)))\n    model.add(BatchNormalization(axis=-1, name='bn0'))\n    model.add(AveragePooling2D())\n    \n    model.add(Conv2D(filters=16, kernel_size=(5,5), strides=(1,1), activation='relu', name='Conv2D1'))\n    model.add(BatchNormalization(axis=-1, name='bn1'))\n    model.add(AveragePooling2D())\n    \n    model.add(Flatten())\n    model.add(Dense(units=120, activation='relu'))\n    model.add(Dense(units=84, activation='relu'))\n    model.add(Dense(units=10, activation='softmax'))\n    return model","2529ad44":"# Initializing lenet model and its data flow\nmodel = lenet5()\nmodel.summary()","0a96c41f":"# Compiling our model and training on train data\nmodel.compile(optimizer='Adam',loss='categorical_crossentropy', metrics=[\"accuracy\"])\nhistroy = model.fit(X_train, Y_train, epochs=5, batch_size=16)","ef7fd3b0":"# Plotting Training accuracy and Training Loss curves\ntrain_accuracy = histroy.history['acc']\ntrain_loss = histroy.history['loss']\n\niterations = range(len(train_accuracy))\nplt.plot(iterations, train_accuracy, label='Training accuracy')\nplt.title('epochs vs Training accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(iterations, train_loss, label='Training Loss')\nplt.title('epochs vs Training Loss')\nplt.legend()","e95155f2":"# Evaluating our model with dev dataset\npreds = model.evaluate(x=X_val, y=Y_val)\n\nprint (\"\\nLoss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))","ca37b39c":"# Look at confusion matrix \n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","1d2ee61a":"model.save(\"DigiModel.h5\")","0b0b89e7":"# Predicting on Test data\ny_pred = model.predict(test)\n","1a72a826":"# Checking sample submission format\nsam_sub = pd.read_csv('..\/input\/sample_submission.csv')\nsam_sub.head()","ab29f088":"# Compressing prediction to labels (ex :  [0,0,1,0,0,0,0,0,0,0] -> 2)\ny_pred =  np.argmax(y_pred,axis = 1)\n","39e70bac":"\n# Making our predictions as Pandas Series \ny_pred = pd.Series(y_pred,name=\"Label\")\ny_pred.shape","73c2cf35":"# Saving our model prediction of test data\n# step 1 -> Create a DataFrame with ImageId(given in test data) and Label (Model predicted value)\n# step 2 -> Save that DataFrame to make submission in kaggle competition for evaluation\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),y_pred],axis = 1)\n\nsubmission.to_csv(\"lenet5_mnist.csv\",index=False)","2b4adb14":"# Conclusion:\n1. Applied a Same convolution in First layer because given image size is 28 x 28 where Lenet designed for 32 x 32 input size.\n2. Trained 5 epochs with 16 batch size\n3. Achieved 98% accuracy","f66262c6":"# Data preprocessing","1f8ba046":"# Testing our model","7594ca56":"# Digit Recognizer\n<html>\n    <body>\n<p><t>There are 2 files. They are <li>train.csv<\/li> <li>test.csv<\/li> <p>which are used for training and testing respectively. <br>Each row in dataset contain <b>28 x 28 gray-scale image<\/b> of hand-drawn digit flattened as <b>784 pixels<\/b>.This pixel values from 0 to 255. These number are from <b>zero to nine<\/b> indicated by \"label\" column in train dataset.<\/p>\n<p> This kernel consists of 4 sections<p>\n    <ol>\n        <li> Data Analysis<\/li>\n        <li> Data preprocessing<\/li>\n        <li> Training<\/li>\n        <li> Testing<\/li>\n    <\/ol>\n<p>Dataset consists of <ul><li>Train data 42,000 rows<\/li> <li>Test data 28,000 rows<\/li><\/ul>\n\nThe train dataset is divided into training (90%) and development (10%) datasets.\n\nThis data is training on LeNet-5 model which have 2 Convolution layers and 3 fully connected layers published this <a href = \"http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/lecun-01a.pdf\"> paper<\/a> by Yann LeCun 1998 who received Turning award in 2019.<br><br>\n<b>Applied a trick to make Lenet architecture comfortable with these data dimensions. Let's see what is that!\n    <\/body>\n<\/html>","3997c890":"# Train our model","211f74f3":"# Data Analysis","30085f41":"### Now data is ready to train, Let's design our model"}}