{"cell_type":{"b2b1a01d":"code","c6907520":"code","4dbab36f":"code","9a47f3f0":"code","dbe1df27":"code","4e19411b":"code","c05bf98a":"code","1f962d90":"code","4aa1d955":"code","2ddbfcc5":"code","ba143f50":"code","9f40d986":"code","7546e992":"code","aefcecbd":"code","c0ef3b26":"code","ac107ddc":"code","3c4ddc3e":"code","b0c0d689":"code","b5ea23da":"code","77ce36e9":"code","dbed1a7a":"code","3bec5cc4":"code","1ee49c5b":"code","291d75a7":"code","8e473555":"code","f9b0d3b1":"code","39c89568":"code","2ba3bb8e":"code","e208307d":"markdown","de2123ed":"markdown","ec3b24f5":"markdown","acc8b049":"markdown","c6c76414":"markdown","3860d9cf":"markdown","085a1816":"markdown","e0b59b76":"markdown","5f963733":"markdown","76d1fe94":"markdown","99e562cf":"markdown","c16da9c4":"markdown"},"source":{"b2b1a01d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb \n\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import plot_importance\n\nimport joblib","c6907520":"train = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\ntrain.set_index(['SK_ID_CURR'], inplace=True)\ntrain.shape","4dbab36f":"train.head()","9a47f3f0":"train.info(max_cols = 200)","dbe1df27":"# Display a DataFrame with proportion of Survived\ns = train.TARGET\ncts = s.value_counts()\npct = s.value_counts(normalize = True).mul(100).round(1)\npd.DataFrame({'counts': cts, 'percent': pct})","4e19411b":"#convert catergorical festures to cat\ncat_cols = ['FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE', \n            'FLAG_PHONE', 'FLAG_EMAIL', 'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY',\n            'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION',\n            'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY']\n\ntrain[cat_cols] = train[cat_cols].astype('category')","c05bf98a":"#create credit annuity ratio feature\ntrain['CRED_ANNUITY'] = train['AMT_CREDIT'] \/ train['AMT_ANNUITY']\n\n#set max income to 2.5 million\ntrain = train[train['AMT_INCOME_TOTAL'] < 2500000]\n\n#replace 365243 in days employed with nan\ntrain['DAYS_EMPLOYED'].replace(365243, np.nan, inplace = True)\n\n#convert age to years\ntrain['AGE'] = train['DAYS_BIRTH'] \/ - 365\n\n\n#create avg of each row of EXIT_SOURCE values\ntrain['AVG_EXT'] = train.iloc[:, 40:43].sum(axis=1)\/(3- train.iloc[:,40:43].isnull().sum(axis=1))\ntrain.EXT_SOURCE_1.fillna(train.AVG_EXT, inplace=True)\ntrain.EXT_SOURCE_2.fillna(train.AVG_EXT, inplace=True)\ntrain.EXT_SOURCE_3.fillna(train.AVG_EXT, inplace=True)","1f962d90":"#remove columns with mode and median building information \ndels = ['DAYS_BIRTH', 'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG', 'CNT_FAM_MEMBERS',  'OBS_30_CNT_SOCIAL_CIRCLE', 'ELEVATORS_AVG', \n        'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE', 'COMMONAREA_MODE', \n        'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', \n        'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', \n        'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI', \n        'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', \n        'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'TOTALAREA_MODE', \n        'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5',\n        'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', \n        'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', \n        'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21']\n\ntrain1 = train.drop(train[dels], axis = 1)\ntrain1.head()","4aa1d955":"train1.info(max_cols = 100)","2ddbfcc5":"#create a list of numerical features\nnum_feat = ['CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE', \n            'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'OWN_CAR_AGE', 'HOUR_APPR_PROCESS_START', 'EXT_SOURCE_1', \n            'EXT_SOURCE_2', 'EXT_SOURCE_3', 'APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'CRED_ANNUITY', 'AGE']\n\n#create a list of categorical features \ncat_feat = ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE',\n            'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE', 'REGION_RATING_CLIENT', \n            'REGION_RATING_CLIENT_W_CITY', 'WEEKDAY_APPR_PROCESS_START', 'REG_REGION_NOT_LIVE_REGION',\n            'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY', \n            'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY', 'ORGANIZATION_TYPE']\n\n\nprint(num_feat)\nprint(cat_feat)","ba143f50":"#combine the numeric and categorical lists into a list called features\nfeatures = num_feat + cat_feat\nprint(features)","9f40d986":"#create a Pipeline for processing the num_feat\nnum_pipe = Pipeline(\n    steps = [('imputer', SimpleImputer(strategy = 'median')),\n           ('scaler', StandardScaler())\n    ])\n\n\n#create a Pipeline for processing the cat_feat\ncat_pipe = Pipeline(\n    steps = [('imputer', SimpleImputer(strategy = 'constant', fill_value = 'missing')),\n           ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n\n\n#create a ColumnTransformer that combines the two pipelines\npreprocessor = ColumnTransformer(\n    transformers = [('num', num_pipe, num_feat),\n                    ('cat', cat_pipe, cat_feat)\n    ])","7546e992":"# Fit the preprocessor to the training data, selecting only the columns in the 'features' list. \npreprocessor.fit(train1[features])","aefcecbd":"train30k = train.sample(frac=0.10, replace=False, random_state=1)\n","c0ef3b26":"#define y_train, apply the fitted preprocessor to the training data\ny_train = train30k['TARGET'].values\nX_train = preprocessor.transform(train30k[features])","ac107ddc":"#print the shapes of X_train and y_train\n\nprint('Shape of features: ', X_train.shape)\nprint('Shape of target: ', y_train.shape)","3c4ddc3e":"%%time\n\nXGB_clf = XGBClassifier(objective='binary:logistic', use_label_encoder=False)\n\nXGB_parameters = {\n    'max_depth': range (1, 2, 3),\n    'n_estimators': range(25, 100, 200),\n    'learning_rate': [0.05, 0.01, 1]\n}\n\nXGB_grid = GridSearchCV(XGB_clf, XGB_parameters, cv=10, n_jobs=10, verbose=True, scoring= 'roc_auc')\nXGB_grid.fit(X_train, y_train)\n\n\nXGB_model = XGB_grid.best_estimator_\n\nprint('Best Parameters:', XGB_grid.best_params_)\nprint('Best CV Score:  ', XGB_grid.best_score_)\nprint('Training Acc:   ', XGB_model.score(X_train, y_train))","b0c0d689":"%%time\n\nLGBM_clf = lgb.LGBMClassifier(boosting_type='gbdt',n_estimators= 5000, \n                              class_weight='balanced', subsample=0.8, colsample_bytree= 0.7, n_jobs=-1)\n\nLGBM_parameters = {\n    'max_depth': range (1, 2, 3),\n    'learning_rate': [0.05, 0.03, 0.01], \n    'metric' : ['auc', 'binary_logloss']\n}\n\n\nLGBM_grid = GridSearchCV(LGBM_clf, LGBM_parameters, cv=10, n_jobs=10, verbose=True, scoring= 'roc_auc')\nLGBM_grid.fit(X_train, y_train)\n\n\nLGBM_model = LGBM_grid.best_estimator_\n\nprint('Best Parameters:', LGBM_grid.best_params_)\nprint('Best CV Score:  ', LGBM_grid.best_score_)\nprint('Training Acc:   ', LGBM_model.score(X_train, y_train))","b5ea23da":"#save your pipeline to a file. \njoblib.dump(preprocessor, 'wk2default_preprocessor.joblib')","77ce36e9":"#determine the best model found above and save that to a file. \nXGB_model = XGBClassifier(objective='binary:logistic', use_label_encoder=False, max_depth = 1, n_estimators = 25, learning_rate = 1)\nXGB_model.fit(X_train, y_train)\njoblib.dump(XGB_model, 'XGB_default_model.joblib')","dbed1a7a":"LGBM_model = lgb.LGBMClassifier(boosting_type='gbdt',n_estimators= 5000, class_weight='balanced',\n                                subsample=0.8, colsample_bytree= 0.7, n_jobs=-1, \n                                learning_rate = 0.03, max_depth = 1)\nLGBM_model.fit(X_train, y_train)\njoblib.dump(LGBM_model, 'LGBM_default_model.joblib')","3bec5cc4":"%%time \n\nlr_clf = LogisticRegression(max_iter= 250, solver='saga', penalty='elasticnet')\n\nlr_parameters = {\n    'l1_ratio':[0, 0.5, 1],\n    'C': [0.01, 0.1, 1]\n}\n\nlr_grid = GridSearchCV(lr_clf, lr_parameters, cv=10, refit='True', n_jobs=-1, verbose=10, scoring= 'roc_auc')\nlr_grid.fit(X_train_10K, y_train_10K)\n\nlr_model = lr_grid.best_estimator_\n\nprint('Best Parameters:', lr_grid.best_params_)\nprint('Best CV Score:  ', lr_grid.best_score_)\nprint('Training Acc:   ', lr_model.score(X_train_10K, y_train_10K))","1ee49c5b":"# view the CV results\n\nlr_summary = pd.DataFrame(lr_grid.cv_results_['params'])\nlr_summary['cv_score'] = lr_grid.cv_results_['mean_test_score']\n\nfor r in lr_parameters['l1_ratio']:\n    temp = lr_summary.query(f'l1_ratio == {r}')\n    plt.plot(temp.C, temp.cv_score, label=r)\nplt.xscale('log')\nplt.xlabel('Regularization Parameter (C)')\nplt.ylabel('CV Score')\nplt.legend(title='L1 Ratio', loc='lower right')\nplt.grid()\nplt.show()\n\nprint(lr_summary.to_string(index=False))","291d75a7":"%%time \n\ndt_clf = DecisionTreeClassifier(random_state=1)\n\ndt_parameters = {\n    'max_depth': [1, 5, 7, 10],\n    'min_samples_leaf': [2, 3, 4, 5]\n}\n\ndt_grid = GridSearchCV(dt_clf, dt_parameters, cv=75, refit='True', n_jobs=-1, verbose=0, scoring='roc_auc')\ndt_grid.fit(X_train_10K, y_train_10K)\n\ndt_model = dt_grid.best_estimator_\n\nprint('Best Parameters:', dt_grid.best_params_)\nprint('Best CV Score:  ', dt_grid.best_score_)\nprint('Training Acc:   ', dt_model.score(X_train_10K, y_train_10K))","8e473555":"#view the CV results.\n\ndt_summary = pd.DataFrame(dt_grid.cv_results_['params'])\ndt_summary['cv_score'] = dt_grid.cv_results_['mean_test_score']\n\nfor ms in dt_parameters['min_samples_leaf']:\n    temp = dt_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nprint(dt_summary.to_string(index=False))","f9b0d3b1":"%%time \n\nrf_clf = RandomForestClassifier(random_state = 1, n_estimators = 25)\n\nrf_parameters = {\n    'max_depth': [1, 3, 5, 7, 11],\n    'min_samples_leaf': [4, 6, 10, 14]\n}\n\nrf_grid = GridSearchCV(rf_clf, rf_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='roc_auc')\nrf_grid.fit(X_train_10K, y_train_10K)\n\nrf_model = rf_grid.best_estimator_\n\nprint('Best Parameters:', rf_grid.best_params_)\nprint('Best CV Score:  ', rf_grid.best_score_)\nprint('Training Acc:   ', rf_model.score(X_train_10K, y_train_10K))","39c89568":"#view the CV results.\n\nrf_summary = pd.DataFrame(rf_grid.cv_results_['params'])\nrf_summary['cv_score'] = rf_grid.cv_results_['mean_test_score']\n\nfor ms in rf_parameters['min_samples_leaf']:\n    temp = rf_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nprint(rf_summary.to_string(index=False))","2ba3bb8e":"#determine the best model found above and save that to a file. \n#rf_model = RandomForestClassifier(random_state=1, n_estimators=25, max_depth = 3, min_samples_leaf = 14)\n#rf_model.fit(X_train_10K, y_train_10K)\n\n#joblib.dump(rf_model, 'rf_default_model.joblib')","e208307d":"## Project Week 1 Models","de2123ed":"**Light GBM**","ec3b24f5":"**XGBoost**","acc8b049":"## Project Week 2 Models","c6c76414":"#### Decision Tree","3860d9cf":"#### Save pipeline and model for submission","085a1816":"from sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\n\n\nfeat = train1.drop(['SK_ID_CURR', 'TARGET'], axis=1)\nlabel = train1['TARGET']\n\nX_train, X_valid, y_train, y_valid = train_test_split(feat, label, test_size=0.2, random_state=3)\nX_train.shape, X_valid.shape\n\n\n\n\nclf = LGBMClassifier(\n        n_jobs = -1,\n        n_estimators = 2000,\n        learning_rate = 0.1,\n        num_leaves = 16,\n        max_depth = 10,\n        silent = -1,\n        verbose = -1\n        )\n\nclf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'auc', verbose= 100, \n        early_stopping_rounds= 50)\n        \nfrom lightgbm import plot_importance\n\nplot_importance(train1, figsize=(18, 40))","e0b59b76":"### LGBM Classifier","5f963733":"### Check label distribution","76d1fe94":"### Random Forest","99e562cf":"### Data Preparation & Feature Engineering","c16da9c4":"### Build Pipelines"}}