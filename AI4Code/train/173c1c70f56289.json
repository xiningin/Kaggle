{"cell_type":{"e7136651":"code","b2df6ad7":"code","65e749db":"code","ad47b2a0":"code","3d624809":"code","8e774b58":"code","9365c2af":"code","bea9b586":"code","fe0144fd":"code","51a759a0":"code","525d0946":"code","f2f8ba5a":"code","16b46305":"code","caca7538":"code","31735cd1":"code","ac256924":"code","a770a395":"code","649c18d4":"code","6146043f":"code","b779e384":"code","588d7efa":"markdown","6f8c36ce":"markdown","a9ad5ff6":"markdown","e5a4ff92":"markdown","b4df0529":"markdown","da233fa8":"markdown","05e9a517":"markdown","eebec080":"markdown","0d93aa48":"markdown","732b40cf":"markdown","e52294ef":"markdown","75bfd24e":"markdown"},"source":{"e7136651":"import pandas as pd\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nimport re","b2df6ad7":"rcc_train = pd.read_csv(\"\/kaggle\/input\/interbank20\/rcc_train.csv\")\nse_train = pd.read_csv(\"\/kaggle\/input\/interbank20\/se_train.csv\", index_col=\"key_value\")\nsunat_train = pd.read_csv(\"\/kaggle\/input\/interbank20\/sunat_train.csv\")\ny_train = pd.read_csv(\"\/kaggle\/input\/interbank20\/y_train.csv\", index_col=\"key_value\").target\n\nrcc_test= pd.read_csv(\"\/kaggle\/input\/interbank20\/rcc_test.csv\")\nse_test= pd.read_csv(\"\/kaggle\/input\/interbank20\/se_test.csv\", index_col=\"key_value\")\nsunat_test= pd.read_csv(\"\/kaggle\/input\/interbank20\/sunat_test.csv\")","65e749db":"rcc_train[(rcc_train.key_value == 4) & (rcc_train.cod_instit_financiera == 33)].sort_values(\"codmes\")","ad47b2a0":"rcc_train[(rcc_train.key_value == 4) & (rcc_train.cod_instit_financiera == 61)].sort_values(\"codmes\")","3d624809":"bins = [-1, 0, 10, 20, 30, 60, 90, 180, 360, 720, float(\"inf\")]\nrcc_train[\"condicion\"] = pd.cut(rcc_train.condicion, bins)\nrcc_train[\"condicion\"] = rcc_train[\"condicion\"].cat.codes\nrcc_test[\"condicion\"] = pd.cut(rcc_test.condicion, bins)\nrcc_test[\"condicion\"] = rcc_test[\"condicion\"].cat.codes","8e774b58":"def makeCt(df, c, aggfunc=sum):\n    try:\n        ct = pd.crosstab(df.key_value, df[c].fillna(\"N\/A\"), values=df.saldo, aggfunc=aggfunc)\n    except:\n        ct = pd.crosstab(df.key_value, df[c], values=df.saldo, aggfunc=aggfunc)\n    ct.columns = [f\"{c}_{aggfunc.__name__}_{v}\" for v in ct.columns]\n    return ct","9365c2af":"train = []\ntest = []\naggfuncs = [len, sum, min, max]\nfor c in rcc_train.drop([\"codmes\", \"key_value\", \"saldo\"], axis=1):\n    print(\"haciendo\", c)\n    train.extend([makeCt(rcc_train, c, aggfunc) for aggfunc in aggfuncs])\n    test.extend([makeCt(rcc_test, c, aggfunc) for aggfunc in aggfuncs])","bea9b586":"pd.crosstab(rcc_train.key_value, rcc_train.condicion.fillna(\"N\/A\"), values=rcc_train.saldo, aggfunc=\"mean\")","fe0144fd":"pd.crosstab(rcc_train.key_value, rcc_train.condicion.fillna(\"N\/A\"), values=rcc_train.saldo, aggfunc=len)","51a759a0":"import gc\n\ndel rcc_train, rcc_test\ngc.collect()","525d0946":"train = pd.concat(train, axis=1)\ntest = pd.concat(test, axis=1)","f2f8ba5a":"sunat_train","16b46305":"pd.crosstab(sunat_train.key_value, sunat_train.ciiu)","caca7538":"train = train.join(pd.crosstab(sunat_train.key_value, sunat_train.ciiu)).join(se_train)\ntest = test.join(pd.crosstab(sunat_test.key_value, sunat_test.ciiu)).join(se_test)\n\ndel sunat_train, se_train, sunat_test, se_test\ngc.collect()","31735cd1":"keep_cols = list(set(train.columns).intersection(set(test.columns)))\ntrain = train[keep_cols]\ntest = test[keep_cols]\nlen(set(train.columns) - set(test.columns)) , len(set(test.columns) - set(train.columns))","ac256924":"train.columns = [str(c) for c in train.columns]\ntrain = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_-]+', '', x))\n\ntest.columns = [str(c) for c in test.columns]\ntest = test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_-]+', '', x))","a770a395":"folds = [train.index[t] for t, v in KFold(5).split(train)]","649c18d4":"from sklearn.model_selection import ParameterGrid\n\nparams = ParameterGrid({\"min_child_samples\": [150, 250, 500, 1000], \"boosting_type\": [\"gbdt\", \"goss\"]})","6146043f":"best_score = 0\nbest_probs = []\nfor param in params:\n    test_probs = []\n    train_probs = []\n    p  = \"\/\/\/\".join([f\"{k}={v}\" for k, v in param.items()])\n    print(\"*\"*10, p, \"*\"*10)\n    for i, idx in enumerate(folds):\n        Xt = train.loc[idx]\n        yt = y_train.loc[Xt.index]\n\n        Xv = train.drop(Xt.index)\n        yv = y_train.loc[Xv.index]\n\n        learner = LGBMClassifier(n_estimators=1000, **param)\n        learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n                    eval_set=[(Xt, yt), (Xv, yv)], verbose=False)\n        test_probs.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n        train_probs.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n\n    test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n    train_probs = pd.concat(train_probs)\n    score = roc_auc_score(y_train, train_probs.loc[y_train.index])\n    print(f\"roc auc estimado para {p}: {score}\")\n    if score > best_score:\n        print(\"*\"*10, f\"{p} es el nuevo mejor modelo\", \"*\"*10)\n        best_score = score\n        best_probs = test_probs\n    ","b779e384":"best_probs.name = \"target\"\nbest_probs.to_csv(\"benchmark3.csv\")","588d7efa":"### Sunat\n","6f8c36ce":"### Incorporamos la Informaci\u00f3n adicional existente en las tablas socio econ\u00f3micas y del censo. Es un simple join porque ambas tienen key_value \u00fanicos\n#### Por el momento no incorporamos la informaci\u00f3n tributaria porque requiere un tratamiento m\u00e1s complejo que queda para futuras revisiones","a9ad5ff6":"### Lectura de las Bases\n\nObservamos los datos que tenemos disponibles en https:\/\/www.kaggle.com\/c\/interbank20\/data\n\n\n#### En este caso, vamos a descartar la base de censo, dado que empeora lo resultados. Este tipo de comportamiento es inusual pero no inesperado. Merecer\u00eda un estudio detallado para saber con m\u00e1s precisi\u00f3n por qu\u00e9 esto ocurre, pero a priori, parecer\u00eda ser que las variables del censo son relevantes (ergo, el modelo las usa) pero, por su antig\u00fcedad, son menos relevantes para el per\u00edodo de test (probablemente por no estar ya vigentes). Por lo tanto, estar\u00edamos frente a una perturbaci\u00f3n de la distribuci\u00f3n del input, lo que perjudica al modelo que depende de las variables alteradas.","e5a4ff92":"### \u00bfC\u00f3mo podemos procesar rcc para extraer informaci\u00f3n \u00fatil?","b4df0529":"En este caso, no es una serie de tiempo pero tenemos multiples filas por cada persona, dadas por la multiplicidad de rubros anotados","da233fa8":"### Entrenamiento del Modelo\n\nPara entrenar nuestro modelo vamos a usar LightGBM. A diferencia del notebook anterior, esta vez vamos a agregar la optimizaci\u00f3n de hyper-par\u00e1metro. Se usan s\u00f3lo dos con algunos pocos posibles valores, a modo de ejemplo para que los participantes lo puedan ir mejorando. ","05e9a517":"### Por la naturaleza de las variables creadas, nos aseguramos que solo se utilicen variables existentes en ambos conjuntos de datos (train y test)","eebec080":"# Script para generar la soluci\u00f3n del Tercer Benchmark de la Competencia\n\n## Si no presentaste a\u00fan tu primera soluci\u00f3n, tenes la oportunidad de hacerlo en pocos Clicks!\n\n**Hola! **  \n  \nEste Script es un Ejemplo de Procesamiento de los Datos, Modelado y Generaci\u00f3n de una Soluci\u00f3n.\n\nAgregamos una peque\u00f1a explicaci\u00f3n de lo que se hace en cada paso para ayudar a los que est\u00e1n comenzando ahora\n","0d93aa48":"### Vamos a trabajar ahora con la base de **RCC**:\n\nEl principal problema que tiene esta base es su estructura temporal, que consiste de m\u00faltiples series de tiempo, una por cada producto en cada banco. \n","732b40cf":"\nPrimero discretizamos los d\u00edas de atraso para poder manipularla mejor.","e52294ef":"### Importamos las librer\u00edas que vamos a utilizar","75bfd24e":"### Guardado de las predicciones modelo para hacer la presentaci\u00f3n\n\nFinalmente creamos el archivo CSV que podemos subir como nuestra Soluci\u00f3n a la competencia"}}