{"cell_type":{"e764144e":"code","934163f9":"code","43d63276":"code","4e146c5d":"code","2e80786b":"code","92d931ee":"code","3ea2eabd":"code","a79d1fad":"code","60cda0c3":"code","25cb0329":"code","d693cfff":"code","bd11756f":"code","d4e3b76e":"code","df582587":"code","b5b44c47":"code","41e5d0a6":"code","fd757619":"code","bff2e39e":"code","95179f07":"code","f4e08254":"code","a107ae91":"code","b7ad22f2":"code","c28ea4ee":"code","51a740df":"code","ece077a5":"code","cf2fac5a":"code","fd6d62bd":"code","781237d8":"code","f6ad4530":"code","ace769c4":"code","b49d97be":"code","2889b8d7":"code","49488747":"code","68b70caf":"code","5ebba2ce":"code","212ac675":"code","9eaff471":"markdown","aa72d284":"markdown","2ea54d1e":"markdown","251a604d":"markdown","54060268":"markdown"},"source":{"e764144e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","934163f9":"# Please switch on the TPU before running these lines.\n\n!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","43d63276":"import torch_xla\nimport torch_xla.core.xla_model as xm","4e146c5d":"#huggingface libraries \nimport transformers\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\nfrom transformers import AdamW\n#torch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, Dataset, TensorDataset\n\n#random libraries\nfrom tqdm import tqdm \nimport pandas as pd\nimport numpy as np\nimport os\nimport gc\nimport random\nfrom datasets import load_dataset\n\n# set a seed value\ntorch.manual_seed(555)\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nprint(torch.__version__)","2e80786b":"train_data=pd.read_csv(\"\/kaggle\/input\/contradictory-my-dear-watson\/train.csv\")\ntest_data=pd.read_csv(\"\/kaggle\/input\/contradictory-my-dear-watson\/test.csv\")\ntrain_data","92d931ee":"!pip install datasets","3ea2eabd":"mnli = load_dataset('glue', 'mnli')\nprint(len(mnli))\nprint(mnli['train'])","a79d1fad":"mnli_df=pd.DataFrame()\nmnli_df['id']=mnli['train']['idx']\nmnli_df['premise']=mnli['train']['premise']\nmnli_df['hypothesis']=mnli['train']['hypothesis']\nmnli_df['lang_abv']=['en']*len(mnli['train']['idx'])\nmnli_df['language']=['English']*len(mnli['train']['idx'])\nmnli_df['label']=mnli['train']['label']\nmnli_df","60cda0c3":"# xnli = load_dataset ('xnli', 'all_languages')\n# # xnli   = load_dataset ('xnli')\n# print(xnli['train'])\n","25cb0329":"# xnli_df=pd.DataFrame()\n# xnli_df['premise']=mnli['train']['premise']\n# xnli_df['hypothesis']=mnli['train']['hypothesis']\n# xnli_df['label']=mnli['train']['label']","d693cfff":"train_data1=train_data[['premise','hypothesis','label']]\nval_data=train_data1[:len(train_data1)\/\/2]\ntrain_data1=train_data1[len(train_data1)\/\/2:]\nmnli_df=mnli_df[['premise','hypothesis','label']]\nframes=[train_data1,mnli_df]\ntrain_data1=pd.concat(frames)","bd11756f":"print(train_data1)\nprint(len(val_data))","d4e3b76e":"print(train_data['label'].shape)","df582587":"#model_name = 'bert-base-multilingual-uncased'\n#model_name = 'xlm-roberta-base'\nmodel_name = 'joeddav\/xlm-roberta-large-xnli' \nbatch_size = 32\nMAX_LENGTH = 256\nNUM_EPOCHS = 3\nL_RATE = 1e-5\nNUM_CORES = os.cpu_count()\n\nNUM_CORES","b5b44c47":"device = xm.xla_device()\n\nprint(device)","41e5d0a6":"#tokenizer=BertTokenizer.from_pretrained(model_name, do_lower_case=True)\ntokenizer=XLMRobertaTokenizer.from_pretrained(model_name, do_lower_case=True)","fd757619":"# sample_data = list(zip(train_data['premise'][:2], train_data['hypothesis'][:2]))\n# print(sample_data)\n# tokenized_sample = tokenizer.batch_encode_plus(sample_data, **kwargs)\n# print(tokenized_sample)\ndef preprocess(data1, tokenizer):\n    kwargs = { 'truncation': True,\n    'max_length': MAX_LENGTH,\n    'padding': 'max_length',\n     'return_attention_mask': True, \n    'return_token_type_ids': True     \n    }\n    data = list(zip(data1['premise'], data1['hypothesis']))\n    tokenized = tokenizer.batch_encode_plus(data,**kwargs)\n    input_ids = torch.LongTensor(tokenized.input_ids)\n    attention_masks = torch.LongTensor(tokenized.attention_mask)\n    token_type_ids = torch.LongTensor(tokenized.token_type_ids)\n    return input_ids, attention_masks, token_type_ids","bff2e39e":"print(train_data1['label'].shape)\n#labels=np. reshape(train_data1['label'], (791464,1))\nlabels = torch.Tensor(train_data1['label']).reshape(-1, 1)\nprint(labels)","95179f07":"input_ids, attention_masks, token_type_ids = preprocess(train_data1,tokenizer)\nlabels = torch.Tensor(train_data1['label']).reshape(-1, 1)\ntrain_dataset_final = TensorDataset(input_ids, attention_masks, token_type_ids,labels)\ntrain_dataloader = DataLoader(train_dataset_final, sampler=RandomSampler(train_dataset_final), batch_size=batch_size)\ntrain_dataset_final","f4e08254":"input_ids1, attention_masks1, token_type_ids1 = preprocess(val_data,tokenizer)\nlabels1 = torch.Tensor(val_data['label']).reshape(-1, 1)\nval_dataset_final = TensorDataset(input_ids1, attention_masks1, token_type_ids1,labels1)\nval_dataloader = DataLoader(val_dataset_final, sampler=RandomSampler(val_dataset_final), batch_size=batch_size)\nlen(val_dataloader)","a107ae91":"print(len(train_dataloader))","b7ad22f2":"input_ids_test, attention_masks_test, token_type_ids_test = preprocess(test_data,tokenizer)\ntest_dataset_final = TensorDataset(input_ids_test, attention_masks_test, token_type_ids_test)\ntest_dataloader = DataLoader(test_dataset_final, sampler=SequentialSampler(test_dataset_final), batch_size=batch_size)\nlen(test_dataloader)","c28ea4ee":"#model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3, output_hidden_states=False, output_attentions=False)\nmodel = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=3)\nmodel.to(device)","51a740df":"batch = next(iter(train_dataloader))\nb_input_ids = batch[0].to(device)\nb_input_mask = batch[1].to(device)\nb_token_type_ids = batch[2].to(device)\nb_labels = batch[3].to(device)","ece077a5":"outputs = model(b_input_ids, \n                token_type_ids=b_token_type_ids, \n                attention_mask=b_input_mask,\n                labels=b_labels)\n","cf2fac5a":"#print(outputs)","fd6d62bd":"print(outputs[0])\nprint(outputs[0].item())","781237d8":"optimizer = AdamW(model.parameters(),\n              lr = L_RATE, \n              eps = 1e-8\n            )","f6ad4530":"gc.collect()","ace769c4":"seed_val = 1024\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","b49d97be":"for epoch in range(NUM_EPOCHS):\n    model.train()\n    torch.set_grad_enabled(True)\n    total_train_loss=0\n    epoch_acc_scores_list = []\n    for i,batch in tqdm(enumerate(train_dataloader)):\n        model.zero_grad()\n        input_ids, attention_masks, token_type_ids, labels=batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device)\n        outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks, labels=labels)\n        loss=outputs[0]\n        if i%10==0:\n            print(f'loss of batch {i}: {loss}')\n        total_train_loss+=loss.item()\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n        xm.optimizer_step(optimizer, barrier=True) \n    print(f'total loss of epoch {epoch}: {total_train_loss}')\n    gc.collect()\n    \n    model.eval()\n    torch.set_grad_enabled(False)\n    total_val_loss = 0\n    targets_list=[]\n    for j, val_batch in enumerate(val_dataloader):\n        b_input_ids = val_batch[0].to(device)\n        b_input_mask = val_batch[1].to(device)\n        b_token_type_ids = val_batch[2].to(device)\n        b_labels = val_batch[3].to(device)      \n        outputs = model(b_input_ids, \n                token_type_ids=b_token_type_ids, \n                attention_mask=b_input_mask, \n                labels=b_labels)\n        loss = outputs[0]\n        total_val_loss = total_val_loss + loss.item()\n        preds = outputs[1]\n        val_preds = preds.detach().cpu().numpy()\n        targets_np = b_labels.to('cpu').numpy()\n        targets_list.extend(targets_np)\n        if j == 0:  # first batch\n            stacked_val_preds = val_preds\n\n        else:\n            stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n    y_true = targets_list\n    y_pred = np.argmax(stacked_val_preds, axis=1)\n    val_acc = accuracy_score(y_true, y_pred)\n    print('val accuracy: ',val_acc)\n","2889b8d7":"stacked_val_labels = []\nmodel.eval()\n\ntorch.set_grad_enabled(False)\ntotal_val_loss = 0\n\nfor j, h_batch in enumerate(test_dataloader):\n\n    b_input_ids = h_batch[0].to(device)\n    b_input_mask = h_batch[1].to(device)\n    b_token_type_ids = h_batch[2].to(device)     \n    outputs = model(b_input_ids, \n            token_type_ids=b_token_type_ids, \n            attention_mask=b_input_mask)\n    preds = outputs[0]\n    val_preds = preds.detach().cpu().numpy()\n    if j == 0:  # first batch\n        stacked_val_preds = val_preds\n\n    else:\n        stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n        #stacked_val_preds.extend(val_preds)\n    #print(len(stacked_val_preds))\n    \n            \nprint('\\nPrediction complete.')\n","49488747":"print(stacked_val_preds)","68b70caf":"test_preds = np.argmax(stacked_val_preds, axis=1)","5ebba2ce":"path = '..\/input\/contradictory-my-dear-watson\/sample_submission.csv'\n\ndf_sample = pd.read_csv(path)\n\nprint(df_sample.shape)\ndf_sample['prediction'] = test_preds\n\ndf_sample.head()\ndf_sample.to_csv('submission.csv', index=False)","212ac675":"# def dataset_lang(data,lang_abv,language):\n#     mnli_df=pd.DataFrame()\n#     mnli_df['id']=[i for i in range(len(data['train']['premise']))]\n#     mnli_df['premise']=mnli['train']['premise']\n#     mnli_df['hypothesis']=mnli['train']['hypothesis']\n#     mnli_df['lang_abv']=[lang_abv]*len(data['train']['premise'])\n#     mnli_df['language']=[language]*len(data['train']['premise'])\n#     mnli_df['label']=mnli['train']['label']\n#     return mnli_df\n# lang_dict={}\n# lang_abv=train_data['lang_abv'].unique()\n# language=train_data['language'].unique()\n# print(lang_abv)\n# for i in range(len(lang_abv)):\n#     lang_dict[lang_abv[i]]=language[i]\n# xnli_df=pd.DataFrame()  \n# ind=1\n# for i in tqdm(lang_dict):\n#     abv=i\n#     lang=lang_dict[i]\n#     data1=load_dataset ('xnli', abv)\n#     get_df=dataset_lang(data1,abv,lang)\n#     if ind==1: \n#         xnli_df=get_df\n#         ind+=1\n#     else:\n#         xnli_df=pd.concat([xnli_df,get_df])\n# print(xnli_df)        \n    ","9eaff471":"# Model ","aa72d284":"# Training model","2ea54d1e":"# extra code","251a604d":"# Preprocessing data","54060268":"# testing model "}}