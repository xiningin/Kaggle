{"cell_type":{"ff49f649":"code","4ae3f297":"code","decaec45":"code","36f13714":"code","213efe6b":"code","13e683f7":"code","a82279ed":"code","7d8e97e4":"code","1e19d504":"code","52cccb89":"code","46839ff5":"code","ecbe44bc":"code","554c56b0":"code","dc50a063":"code","6f3132cd":"code","d57b4712":"code","91498532":"code","891f454d":"code","72de679f":"code","26386ed3":"code","5e7ca7ab":"code","60568cd8":"code","b51a3ec1":"code","f7a5e941":"code","83f40f34":"code","06b7f057":"code","c2d41c5a":"code","1dcb1145":"code","99ee2da5":"code","72fed373":"code","24ee92b7":"code","44409c43":"markdown","3f12b810":"markdown","d9551a02":"markdown","7d4423a8":"markdown","e3764669":"markdown","4d4a86f6":"markdown","a7c07c67":"markdown","8bfb8bcb":"markdown","16a347cf":"markdown","e6727618":"markdown","f90b8d61":"markdown","c628fcb0":"markdown","9ecb442f":"markdown","f62e062d":"markdown","d95a57b0":"markdown"},"source":{"ff49f649":"import time\nstart_time = time.time()","4ae3f297":"from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\nfrom sklearn.utils import shuffle\nfrom sklearn.svm import SVC\nfrom sklearn import model_selection\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport re\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","decaec45":"!pip install tensorflow-text==2.0.0 --user","36f13714":"import tensorflow as tf\nimport tensorflow_hub as hub","213efe6b":"import tensorflow_text as text","13e683f7":"#it helps to print full tweet , not a part\npd.set_option('display.max_colwidth', -1)\n","a82279ed":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","7d8e97e4":"def clean(text):\n    text = re.sub(r\"http\\S+\", \" \", text) # remove urls\n    text = re.sub(r\"RT \", \" \", text) # remove RT\n    # remove all characters if not in the list [a-zA-Z#@\\d\\s]\n    text = re.sub(r\"[^a-zA-Z#@\\d\\s]\", \" \", text)\n    text = re.sub(r\"[0-9]\", \" \", text) # remove numbers\n    text = re.sub(r\"\\s+\", \" \", text) # remove extra spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n    return text","1e19d504":"train.text = train.text.apply(clean)\ntest.text = test.text.apply(clean)","52cccb89":"train['text'][50:70]","46839ff5":"test['text'][:5]","ecbe44bc":"use = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-multilingual-large\/3\")","554c56b0":"X_train = []\nfor r in tqdm(train.text.values):\n    emb = use(r)\n    review_emb = tf.reshape(emb, [-1]).numpy()\n    X_train.append(review_emb)\n\nX_train = np.array(X_train)\ny_train = train.target.values\n\nX_test = []\nfor r in tqdm(test.text.values):\n    emb = use(r)\n    review_emb = tf.reshape(emb, [-1]).numpy()\n    X_test.append(review_emb)\n\nX_test = np.array(X_test)","dc50a063":"train_arrays, test_arrays, train_labels, test_labels = train_test_split(X_train,\n                                                                        y_train,\n                                                                        random_state =42,\n                                                                        test_size=0.05)","6f3132cd":"import xgboost as xgb\nfrom xgboost import XGBClassifier","d57b4712":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\ndef xgboost_param_selection(X, y, nfolds):\n    depth_m=[2,3,5,7,9]\n    base_learners=[2,5,50,70,100]\n    parameters=dict(n_estimators=base_learners , max_depth=depth_m)\n    clf=RandomizedSearchCV(XGBClassifier(n_jobs=-1, class_weight='balanced') ,parameters, scoring='roc_auc', refit=True, cv=3)\n\n    clf.fit(X, y)\n#     cv_error=clf.cv_results_['mean_test_score']\n#     train_error=clf.cv_results_['mean_train_score']\n#     pred=clf.predict(X_train_bow)\n#     score=roc_auc_score(y_train, pred)\n#     estimator=clf.best_params_['n_estimators']\n    clf.best_params_\n#     depth=clf.best_params_['max_depth']\n    return clf\n\n# model = xgboost_param_selection(train_arrays,train_labels, 5)","91498532":"def svc_param_selection(X, y, nfolds):\n    Cs = [1.07]\n    gammas = [2.075]\n    param_grid = {'C': Cs, 'gamma' : gammas}\n    grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=nfolds, n_jobs=8)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    return grid_search\n\n# model = svc_param_selection(train_arrays,train_labels, 5)","891f454d":"from keras.utils import np_utils \nfrom keras.datasets import mnist \nimport seaborn as sns\nfrom keras.initializers import RandomNormal\nimport time\n# https:\/\/gist.github.com\/greydanus\/f6eee59eaf1d90fcb3b534a25362cea4\n# https:\/\/stackoverflow.com\/a\/14434334\n# this function is used to update the plots for each epoch and error\ndef plt_dynamic(x, vy, ty, ax, colors=['b']):\n    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n    ax.plot(x, ty, 'r', label=\"Train Loss\")\n    plt.legend()\n    plt.grid()\n    fig.canvas.draw()\n    \ntrain_labels = np_utils.to_categorical(train_labels, 2) \n# y_test = np_utils.to_categorical(y_test, 10)\n\nfrom keras.models import Sequential \nfrom keras.layers import Dense, Activation\noutput_dim = 2\ninput_dim = train_arrays.shape[1]\n\nbatch_size = 128 \n","72de679f":"model = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(input_dim,)))\nmodel.add(Dense(128, activation='relu'))\n# The model needs to know what input shape it should expect. \n# For this reason, the first layer in a Sequential model \n# (and only the first, because following layers can do automatic shape inference)\n# needs to receive information about its input shape. \n# you can use input_shape and input_dim to pass the shape of input\n\n# output_dim represent the number of nodes need in that layer\n# here we have 10 nodes\n\nmodel.add(Dense(output_dim, input_dim=input_dim, activation='softmax'))\nmodel.summary()","26386ed3":"train_labels.shape","5e7ca7ab":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nnb_epoch = 10\nhistory = model.fit(train_arrays,train_labels, batch_size=batch_size, epochs=nb_epoch, verbose=1)","60568cd8":"# model.best_params_","b51a3ec1":"pred = model.predict(test_arrays)","f7a5e941":"# cm = confusion_matrix(train_labels,pred.round())\n# cm","83f40f34":"# accuracy = accuracy_score(test_labels,pred)\n# accuracy","06b7f057":"test_pred = model.predict(X_test)\nsubmission['target'] = test_pred.round().astype(int)\n#submission.to_csv('submission.csv', index=False)","c2d41c5a":"train_df_copy = train\ntrain_df_copy = train_df_copy.fillna('None')\nag = train_df_copy.groupby('keyword').agg({'text':np.size, 'target':np.mean}).rename(columns={'text':'Count', 'target':'Disaster Probability'})\n\nag.sort_values('Disaster Probability', ascending=False).head(20)","1dcb1145":"count = 2\nprob_disaster = 0.9\nkeyword_list_disaster = list(ag[(ag['Count']>count) & (ag['Disaster Probability']>=prob_disaster)].index)\n#we print the list of keywords which will be used for prediction correction \nkeyword_list_disaster","99ee2da5":"ids_disaster = test['id'][test.keyword.isin(keyword_list_disaster)].values\nsubmission['target'][submission['id'].isin(ids_disaster)] = 1","72fed373":"submission.to_csv(\"submission.csv\", index=False)\nsubmission.head(10)","24ee92b7":"print(\"--- %s seconds ---\" % (time.time() - start_time))","44409c43":"### Some words about Universal Sentence Encoders and Transformer","3f12b810":"### Using keywords for better prediction.","d9551a02":"### Data preprosessing","7d4423a8":"Load the multilingual encoder module.","e3764669":"### Training and Evaluating","4d4a86f6":"A Universal Sentence Encoders encode sentencies to fixed length vectors (The size is 512 in the case of the Multilingual Encoder). The encoders are pre trained on several different tasks: (research article) https:\/\/arxiv.org\/pdf\/1803.11175.pdf. And a use case: https:\/\/towardsdatascience.com\/use-cases-of-googles-universal-sentence-encoder-in-production-dd5aaab4fc15 <br>\nTwo architectures are in use in the encoders: Transformer and Deep Averaging Networks.\nTransformer use \"self attention mechanism\" that learns contextual relations between words and (depending on model) even subwords in a sentence. Not only a word , but it position in a sentence is also taking into account (like positions of other words). There are different ways to implement the intuitive notion of \"contextual relation between words in a sentence\" ( so, different ways to construct \"representation space\" for the contextual words relation). If the several \"ways\" are implemented in a model in the same time: the term \"multi head attention mechanism\" is used.<br>\nTransformers have 2 steps. Encoding: read the text and transform it in vector of fixed length, and decoding: decode the vector (produce prediction for the task). For example: take sentence in English, encode, and translate (decode) in sentence in German.<br>\nFor our model we need only encoding mechanism: sentencies are encoded in vectors and supplied for classification to Support Vector Machine.<br>\nGood and intuitive explanation of the Transformer: http:\/\/jalammar.github.io\/illustrated-transformer\/ ; The original and quite famous now paper \"Attention is all you need\": (research article)\nhttps:\/\/arxiv.org\/pdf\/1706.03762.pdf. More about multi head attention: (research article)\nhttps:\/\/arxiv.org\/pdf\/1810.10183.pdf. How Transformer is used in BERT: https:\/\/towardsdatascience.com\/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270.<br>\n\nThe Multilingual Universal Sentence Encoder:(research articles) https:\/\/arxiv.org\/pdf\/1810.12836.pdf; https:\/\/arxiv.org\/pdf\/1810.12836.pdf;\nExample code: https:\/\/tfhub.dev\/google\/universal-sentence-encoder-multilingual-large\/3\nThe Multilingual Encoder uses very interesting Sentence Piece tokenization to make a pretrained vocabulary: (research articles) https:\/\/www.aclweb.org\/anthology\/D18-2012.pdf; https:\/\/www.aclweb.org\/anthology\/P18-1007.pdf.<br>\n\nAbout the text preprocessing and importance of its coherence with the text preprocessing that is conducted for pretraining + about the different models of text tokeniation:\n\nvery good article:\nhttps:\/\/mlexplained.com\/2019\/11\/06\/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp\/.<br>\n\nBelow the encoding is applied to every sentence in train.text and test.text columns and the resulting vectors are saved to lists.<br>","a7c07c67":"I did experiments with different parameters and text preprocessing within the model configuration. It gives many \"clever\" variants with a good score. But the very model is attractive by its simplicity.","8bfb8bcb":"Please, upvote, if you like","16a347cf":"The results from :\nhttps:\/\/www.kaggle.com\/ihelon\/starter-nlp-svm-tf-idf ;\nhttps:\/\/www.kaggle.com\/dmitri9149\/svm-expm-v0\/edit\/run\/27808847 ;\nhttps:\/\/www.kaggle.com\/rerere\/disaster-tweets-svm ;\nshow the Support Vector Machine works quite well for the Real or Not ? (disaster) Tweets classification with with TF-ID for tokenization.<br>\nIn https:\/\/www.kaggle.com\/gibrano\/disaster-universal-sentences-encoder-svm the Multilingual Universal Sentence Encoder is used for sentence encoding. Here I follow the work in using the Multilingual Universal Sentence Encoder (from tensorflow_hub).<br>\nThe approach from https:\/\/www.kaggle.com\/bandits\/using-keywords-for-prediction-improvement is applied for final filtering of the results basing on the 'keywords'.<br>\n\nThe resulting model is quite simple and relativelly fast (700....900 seconds execution time without GPU). This makes the model suitable for experiments with different parameters and text preprocessing.","e6727618":"### Data loading","f90b8d61":"The keywords are used for the corretion.","c628fcb0":"#### Accuracy and confusion matrix","9ecb442f":"Here I follow https:\/\/www.kaggle.com\/bandits\/using-keywords-for-prediction-improvement The idea is that some keywords with very high probability (sometimes = 1) signal about disaster (or usual) tweets. It is possible to add the extra 'keyword' feature to the model, but the simple approach also works. I make correction for the disaster tweets prediction to the model basing on the \"disaster\" keywords.","f62e062d":"How the text looks like after the cleaning.","d95a57b0":"### Make Support Vector Machine prediction."}}