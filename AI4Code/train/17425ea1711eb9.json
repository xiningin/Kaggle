{"cell_type":{"be6e7f9f":"code","7962c219":"code","744f3774":"code","c01462d1":"code","4df52e8c":"code","e1d0d590":"code","4d02b340":"code","28fdaec7":"code","4581e8dc":"code","a27317b7":"code","dcf7a27b":"code","7d6b5576":"code","6558d9f9":"code","86c02881":"code","d09c4878":"code","1dd96bc2":"code","d5951a21":"code","55050a81":"code","3b3a9a7c":"code","06b6ce5f":"code","461ccfd9":"code","4d1133dd":"code","14b2be9d":"code","2301dac0":"code","807929b7":"code","28bcbc3f":"code","82a32bef":"code","74d8fa9c":"code","e2450250":"code","352c0760":"code","866f72c4":"markdown","72f8a9d7":"markdown","ee55ce21":"markdown","f1bdbfa5":"markdown","340b6a9b":"markdown"},"source":{"be6e7f9f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n","7962c219":"data_yelp = pd.read_csv(\"..\/input\/imdb-yelp-and-amazon-reviews\/yelp_labelled.txt\", sep = '\\t', header= None)","744f3774":"data_yelp.head()\n#review and sentiment\n#0-Negative and 1-Positive review","c01462d1":"#assign column names\ncolumn_name= ['Review', 'Sentiment']\ndata_yelp.columns= column_name","4df52e8c":"data_yelp.head()","e1d0d590":"data_yelp.shape","4d02b340":"data_amazon = pd.read_csv('..\/input\/imdb-yelp-and-amazon-reviews\/amazon_cells_labelled.txt', sep = '\\t', header= None)","28fdaec7":"data_amazon.head()\n#review and sentiment\n#0-Negative and 1-Positive review","4581e8dc":"#assign column names\ndata_amazon.columns= column_name","a27317b7":"data_amazon.head()","dcf7a27b":"data_amazon.shape","7d6b5576":"data_imdb = pd.read_csv('..\/input\/imdb-yelp-and-amazon-reviews\/imdb_labelled.txt', sep = '\\t', header= None)","6558d9f9":"data_imdb.head()\n","86c02881":"data_imdb.columns = column_name","d09c4878":"data_imdb.head()","1dd96bc2":"data_imdb.shape","d5951a21":"#Append all the data in a single dataframe","55050a81":"data = data_yelp.append([data_amazon, data_imdb], ignore_index=True)","3b3a9a7c":"data.shape","06b6ce5f":"data.head()","461ccfd9":"# check distribution of sentiments\ndata['Sentiment'].value_counts()","4d1133dd":"#check for null values\ndata.isnull().sum()","14b2be9d":"# here we will remove stop words and punctuations\n# as well as we will apply lemmatization","2301dac0":"import string","807929b7":"punct= string.punctuation","28bcbc3f":"punct","82a32bef":"from spacy.lang.en.stop_words import STOP_WORDS","74d8fa9c":"stopwords= list(STOP_WORDS) #list of stopwords\nstopwords","e2450250":"#creating a function for data cleaning","352c0760":"def text_data_cleaning(sentence):\n  doc = nlp(sentence)\n\n  tokens = [] #list of tokens\n  for token in doc:\n    if token.lemma_!=\"PRON-\":\n      temp = token.lemma","866f72c4":"## Importing the datasets","72f8a9d7":"## Data Cleaning","ee55ce21":"## Predict the Test set Results","f1bdbfa5":"## Train the Model","340b6a9b":"## Create a function to clean the data "}}