{"cell_type":{"bf575759":"code","165c4831":"code","86083d73":"code","52285078":"code","d707864d":"code","462c6447":"code","3dfc55c4":"code","77500590":"code","13950225":"markdown","c644186d":"markdown","0ea4ac64":"markdown","bb638668":"markdown","e9cddcbb":"markdown","88708d39":"markdown","d023592e":"markdown","1131b4f8":"markdown","b5db5883":"markdown","0d7ee31b":"markdown","a0ea9d01":"markdown","93c92509":"markdown","5b78e07c":"markdown","11d7b456":"markdown","c23a83a9":"markdown","828c80b9":"markdown","788da3fb":"markdown","3f1eb614":"markdown","818d73d3":"markdown","d6261eed":"markdown","36cdde9b":"markdown","99af46cc":"markdown","84b4c3c9":"markdown","319c8cf8":"markdown","ab68f70e":"markdown","6baf9a24":"markdown","f7fe0527":"markdown","33be4743":"markdown","32f210f3":"markdown","0716817c":"markdown","2f17b5a9":"markdown","79437b18":"markdown","7baf9628":"markdown","fef06eb0":"markdown","a774f9fc":"markdown","2d9b41b4":"markdown","23ef87bb":"markdown","be9f6528":"markdown","307ff00a":"markdown","1a024279":"markdown","cfae354b":"markdown","42e73758":"markdown"},"source":{"bf575759":"# if x1 > x2:\n#     t=x1\n#     x1=x2\n#     x2=t\n    \n# if y1 > y2:\n#     t=y1\n#     y1=y2\n#     y2=t\n","165c4831":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport cv2\nimport json\nfrom PIL import Image\nimport shutil\nfrom tqdm import tqdm\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os","86083d73":"!mkdir -p \/root\/.config\/Ultralytics\n!cp \/kaggle\/input\/yolov5-font\/Arial.ttf \/root\/.config\/Ultralytics\/","52285078":"import torch\n\n# Model\nmodel = torch.hub.load('\/kaggle\/input\/yolov5-lib-ds', 'custom', path='\/kaggle\/input\/weights\/best.pt', source='local', force_reload=True)  # or yolov5m, yolov5l, yolov5x, custom\nmodel.conf = 0.1","d707864d":"import greatbarrierreef\n\nenv = greatbarrierreef.make_env()   # initialize the environment\niter_test = env.iter_test()","462c6447":"idc = 0\nsubmission_dict = {\n    'index': [],\n    'annotations': [],\n}\n\n# env.predict(sample_prediction_df)\nfor (image, sample_prediction_df) in iter_test:\n    results = model(image)\n    predictions = []\n#     print('\\n', results.xyxy[0])\n    submission_dict['index'].append(idc)\n    for i in results.xyxy[0].tolist():\n        x1, y1, x2, y2, c, clas = map(int, i)\n        print(x1, y1, x2, y2)\n        predictions.append('{:.2f} {} {} {} {}'.format(c, x1, y1, x2, y2))\n    \n    prediction_str = ' '.join(predictions)\n    if len(prediction_str)==0:\n        prediction_str = \"NaN\"\n    submission_dict['annotations'].append(prediction_str)\n    sample_prediction_df['annotations'] = prediction_str\n    env.predict(sample_prediction_df)\n    idc+=1\n    \n    \ndf= pd.DataFrame(submission_dict)\nprint(\"Done!!\")","3dfc55c4":"df","77500590":"df.to_csv(\"submission.csv\")","13950225":"import torch\nfrom fishyolov5 import utils\ndisplay = utils.notebook_init()","c644186d":"%cd ..","0ea4ac64":"img = plt.imread(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\/64.jpg\")\nfor i in results.xyxy[0].tolist():\n    print(map(int, i))\n    x1, y1, x2, y2, c, clas = map(int, i)   \n    print(x1)\n    \n    cv2.rectangle(img, (x1,y1), (x2, y2), (0,255,255), 4)\n\nplt.figure(figsize=(20,10))\nplt.imshow(img)\n    ","bb638668":"!mkdir train","e9cddcbb":"!python train.py --img 640 --batch 16 --epochs 30 --data data\/fish.yaml --weights yolov5s.pt","88708d39":"!cp \/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\/60.jpg \/kaggle\/working\/60.jpg","d023592e":"%cd \/kaggle\/working\/fishyolov5","1131b4f8":"data = pd.read_csv(\"..\/input\/tensorflow-great-barrier-reef\/train.csv\")\nprint(\"Total images = \",data.shape)\nimage_dir = \"..\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\"\nimage_names = []\nfor img in os.listdir(image_dir):\n    image_names.append(img)\n    \ndata = data[data['annotations'] != \"[]\"]\nprint(\"Total images with annotations = \",data.shape)","b5db5883":"def drawAnnotation(annotaions, img):\n    annotaions = json.loads(annotaions.replace(\"\\'\", \"\\\"\"))\n    for i in annotaions:\n        width, height = i['width'], i['height']\n        x, y = i['x'], i['y']\n        cv2.rectangle(img, (x,y), (x+width, y+height), (0,255,255), 4)\n    \n    return img","0d7ee31b":"show_annotations_and_images(data.loc[4438])","a0ea9d01":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport cv2\nimport json\nfrom PIL import Image\nimport shutil\nfrom tqdm import tqdm\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimage_paths = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if filenames == \"train_images\":\n            image_paths.append(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","93c92509":"from tqdm import tqdm\nimport os\nimport time\n\nstart = time.time() \nimage_files = []\nfor filename in tqdm(os.listdir(r\"\/kaggle\/working\/train\")):   # prive the appropriate path\n    if filename.endswith(\".jpg\") or filename.endswith(\".JPG\") or filename.endswith(\".png\") or filename.endswith(\".PNG\"):\n        image_files.append(\"\/kaggle\/working\/train\/\" + filename) # prive the appropriate path (Note: dont forget to put he backslash '\/' at the end)\nos.chdir(\".\/\")\nwith open(\"train.txt\", \"w\") as outfile:\n    for image in tqdm(image_files):\n        outfile.write(image)\n        outfile.write(\"\\n\")\n    outfile.close()\nos.chdir(\"..\")\n\nprint(f'\\nTime: {time.time() - start}')","5b78e07c":"!git clone https:\/\/github.com\/rukon-uddin\/fishyolov5.git","11d7b456":"%cd weights\n%ls","c23a83a9":"!cp best.pt \/kaggle\/working\/best.pt","828c80b9":"!cp best.pt \/kaggle\/working\/best.pt","788da3fb":"!cp best.pt \/kaggle\/working\/best.pt","3f1eb614":"data","818d73d3":"%cd fishyolov5\n%pip install -qr requirements.txt\nimport torch\nfrom fishyolov5 import utils\ndisplay = utils.notebook_init()","d6261eed":"img_path_list, class_name_list = ret_full_image_path(data)\ndata = data.assign(image_path = img_path_list)\ndata = data.assign(Class = class_name_list)","36cdde9b":"def ret_full_image_path(d):\n    image_path_list = []\n    class_name = []\n    for i, k in d.iterrows():\n        v_n_f = k['image_id']\n        video_number = v_n_f.split(\"-\")[0]\n        videoFrame = v_n_f.split(\"-\")[1]\n        ll = video_number \n        image_path = \"..\/input\/tensorflow-great-barrier-reef\/train_images\/video_{}\/{}.jpg\".format(video_number, videoFrame)\n        image_path_list.append(image_path)\n        class_name.append(0)\n    return image_path_list, class_name","99af46cc":"x1, y1, x2, y2, c, clas = map(int, results.xyxy[0].tolist()[0])","84b4c3c9":"for idx, r in train.iterrows():\n    annotaitons = r[\"annotations\"]\n    annotations = json.loads(annotaitons.replace(\"\\'\", \"\\\"\"))\n    print(r[\"image_path\"])\n    break","319c8cf8":"len(os.listdir(\".\/train\"))","ab68f70e":"%cd ..","6baf9a24":"train = data[[\"image_path\",\"image_id\", \"annotations\", \"Class\"]]","f7fe0527":"import torch\n\n# Model\nmodel = torch.hub.load('ultralytics\/yolov5', 'custom', path='\/kaggle\/working\/best.pt')  # or yolov5m, yolov5l, yolov5x, custom\n\n# Images\n\n# Results","33be4743":"!git clone https:\/\/github.com\/rukon-uddin\/fishyolov5.git\n%cd fishyolov5\n%pip install -qr requirements.txt\nimport torch\nfrom fishyolov5 import utils\ndisplay = utils.notebook_init()\n!cp best.pt \/kaggle\/working\/best.pt","32f210f3":"%cd fishyolov5","0716817c":"%cd ..","2f17b5a9":"p = \".\/\"\nfor i in os.listdir(p):\n    os.remove(p+i)","79437b18":"!tensorboard --logdir runs\/train","7baf9628":"tain_folder_list = []\nex = []\nfor idx, r in tqdm(train.iterrows()):\n    image_id = r[\"image_id\"]\n    annotaitons = r[\"annotations\"]\n    image_name = image_id.split(\"-\")[1]\n    vn = image_id.split(\"-\")[0]\n    annotations = json.loads(annotaitons.replace(\"\\'\", \"\\\"\"))\n    image_path = r[\"image_path\"]\n    destination = \".\/train\/\"\n    shutil.copyfile(image_path, destination+\"{}_{}.jpg\".format(vn, image_name))\n    \n    txt_name = \"{}_{}.txt\".format(vn, image_name)\n    if txt_name in tain_folder_list:\n        ex.append(image_path)\n    \n    with open(\".\/train\/{}_{}.txt\".format(vn, image_name), \"w\") as f:\n        image_path = r[\"image_path\"]\n        img = Image.open(image_path)\n        ww= int(img.size[0])\n        hh= int(img.size[1])\n        for i in annotations:\n            x_min = i[\"x\"]\n            y_min = i[\"y\"]\n            x_max = i[\"width\"] + x_min\n            y_max = i[\"height\"] + y_min\n            b = (x_min, x_max, y_min, y_max)\n            x, y, w, h = convert((ww,hh), b)\n            writee = \"0 {} {} {} {}\".format(x, y, w, h)\n            f.write(writee + \"\\n\")\n            \n        tain_folder_list.append(\"{}_{}.txt\".format(vn, image_name))\n        \n        ","fef06eb0":"img = '\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\/64.jpg'\nresults = model(img)\nprint('\\n', results.xyxy[0])","a774f9fc":"train.head(50)","2d9b41b4":"def show_annotations_and_images(k):\n    annotations = k[\"annotations\"]\n    print(annotations)\n    v_n_f = k['image_id']\n    video_number = v_n_f.split(\"-\")[0]\n    videoFrame = v_n_f.split(\"-\")[1]\n    image_path = \"..\/input\/tensorflow-great-barrier-reef\/train_images\/video_{}\/{}.jpg\".format(video_number, videoFrame)\n    img = cv2.imread(image_path)\n    img = drawAnnotation(annotations, img)\n    plt.figure(figsize=(20,10))\n#     img[:,:,0] = np.zeros([img.shape[0], img.shape[1]])+100\n#     plt.imshow(img[...,::-1])\n#     plt.imshow(img[:,:,::-1])\n    plt.imshow(img)","23ef87bb":"!tensorboard --logdir runs\/train --load_fast=false --bind_all","be9f6528":"!pwd","307ff00a":"shutil.rmtree(\".\/fishyolov5\")","1a024279":"%cd fishyolov5\n%pip install -qr requirements.txt","cfae354b":"def convert(size, box):\n    dw = 1.\/size[0]\n    dh = 1.\/size[1]\n    x = (box[0] + box[1])\/2.0\n    y = (box[2] + box[3])\/2.0\n    w = box[1] - box[0]\n    h = box[3] - box[2]\n    x = x*dw\n    w = w*dw\n    y = y*dh\n    h = h*dh\n    return (x,y,w,h)","42e73758":"!python detect.py --weights \/kaggle\/working\/best.pt --img 640 --conf 0.1 --source \/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\/64.jpg\ndisplay.Image(filename='\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\/64.jpg', width=600)"}}