{"cell_type":{"7a0b92d3":"code","f6cbdc77":"code","e0d68c91":"code","dd647287":"code","bc6afe50":"code","8313156c":"code","363afe9b":"code","cd0c56f2":"code","844df33e":"code","f73e4fc1":"code","226de804":"code","ab32015b":"code","41e250e9":"code","11140421":"code","6755e05e":"code","82f3883b":"code","0b224bf0":"code","7b4be1ca":"code","b28da3e2":"code","a43648f1":"code","9583ebb6":"code","24daa5e5":"code","92676949":"code","e6795709":"code","ee5840d8":"code","aeb6dea0":"code","33682c38":"code","bc719c1a":"code","f6753e31":"code","8dd4499b":"code","834762b1":"code","a5b0300f":"code","ce9e42e1":"code","5a19b1c9":"code","708c4b4f":"code","cda78f4e":"code","5ff62813":"code","41a58ba4":"markdown","41d817c0":"markdown","3e88b716":"markdown","96993ed7":"markdown","337143f7":"markdown","67e95aad":"markdown","1f7de1ef":"markdown","ee03e9d5":"markdown","f95f7f6a":"markdown","74f978b6":"markdown","dd926ef5":"markdown"},"source":{"7a0b92d3":"import numpy as np \nimport pandas as pd \nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout","f6cbdc77":"train = pd.read_csv(r'..\/input\/analytics-vidhya-identify-the-sentiments\/train.csv')\ntest = pd.read_csv(r'..\/input\/analytics-vidhya-identify-the-sentiments\/test.csv')\nsubmission = pd.read_csv(r'..\/input\/analytics-vidhya-identify-the-sentiments\/sample_submission.csv')","e0d68c91":"pd.set_option('display.max_colwidth',200)","dd647287":"train.shape, test.shape, submission.shape","bc6afe50":"train.head()","8313156c":"train.drop('id',axis=1,inplace=True)\ntest.drop('id',axis=1,inplace=True)","363afe9b":"train[train['label']==0].head()","cd0c56f2":"train[train['label']==1].head()","844df33e":"train['label'].value_counts()","f73e4fc1":"length_train = train['tweet'].str.len()\nlength_test = test['tweet'].str.len()\nplt.hist(length_train, bins=20,label='train_tweets')\nplt.hist(length_test,bins=20,label='test_tweets')\nplt.legend()\nplt.show()","226de804":"def clean_tweet(text):\n    \n    # lower-case all characters\n    text=text.lower()\n    \n    # remove twitter handles\n    text= re.sub(r'@\\S+', '',text) \n    \n    # remove urls\n    text= re.sub(r'http\\S+', '',text) \n    text= re.sub(r'pic.\\S+', '',text)\n      \n    \n      \n    # regex only keeps characters\n    text= re.sub(r\"[^a-zA-Z+']\", ' ',text)\n    \n \n    # regex removes repeated spaces, strip removes leading and trailing spaces\n    text= re.sub(\"\\s[\\s]+\", \" \",text).strip()  \n    \n    return text","ab32015b":"train['tweet'] =train['tweet'].apply(lambda x: clean_tweet(x)) \ntest['tweet'] =test['tweet'].apply(lambda x: clean_tweet(x)) \ntrain.head()","41e250e9":"X = train.drop('label',axis=1)\ny = train['label']","11140421":"### Vocabulary size\nvoc_size=10000","6755e05e":"messages=X.copy()\nmessages['tweet'][1]","82f3883b":"messages.reset_index(inplace=True)","0b224bf0":"import nltk\nfrom nltk.corpus import stopwords\n\n### Dataset Preprocessing\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncorpus = []\nfor i in range(0, len(messages)):\n    print(i)\n    review = re.sub('[^a-zA-Z]', ' ', messages['tweet'][i])\n    review = review.lower()\n    review = review.split()\n    \n    review = ' '.join(review)\n    corpus.append(review)","7b4be1ca":"from tensorflow.keras.preprocessing.text import one_hot\nonehot_repr=[one_hot(words,voc_size)for words in corpus]","b28da3e2":"sent_length=20\nembedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\nprint(embedded_docs)","a43648f1":"embedded_docs[0]","9583ebb6":"## Creating model\nembedding_vector_features=40\nmodel=Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())","24daa5e5":"len(embedded_docs),y.shape","92676949":"X_final=np.array(embedded_docs)\ny_final=np.array(y)","e6795709":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.33, random_state=42)","ee5840d8":"### Finally Training\nhistory = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=15,batch_size=128)","aeb6dea0":"y_pred=model.predict_classes(X_test)","33682c38":"from sklearn.metrics import confusion_matrix\nprint(f'confusion_matrix: {confusion_matrix(y_test,y_pred)}')","bc719c1a":"from sklearn.metrics import accuracy_score\nprint(f'accuracy_score: {accuracy_score(y_test,y_pred)}')","f6753e31":"# saving model\nfilename = 'nlp_model.h5'\nmodel.save(filename)","8dd4499b":"Z = test\n### Vocabulary size\nvoc_size=10000\n\nmessages=Z.copy()\nmessages['tweet'][1]","834762b1":"messages.reset_index(inplace=True)","a5b0300f":"import nltk\nfrom nltk.corpus import stopwords\n\n### Dataset Preprocessing\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncorpus = []\nfor i in range(0, len(messages)):\n    print(i)\n    review = re.sub('[^a-zA-Z]', ' ', messages['tweet'][i])\n    review = review.lower()\n    review = review.split()\n    \n    review = ' '.join(review)\n    corpus.append(review)","ce9e42e1":"from tensorflow.keras.preprocessing.text import one_hot\nonehot_repr=[one_hot(words,voc_size)for words in corpus]","5a19b1c9":"sent_length=20\nembedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\nprint(embedded_docs)","708c4b4f":"pred = model.predict_classes(embedded_docs)","cda78f4e":"submission['label'] = pred\nsubmission.to_csv(f'submission.csv',index=False)","5ff62813":"submission","41a58ba4":"Text is a highly unstructured form of data, various types of noise are present in it and the data is not readily analyzable without any pre-processing. The entire process of cleaning and standardization of text, making it noise-free and ready for analysis is known as text preprocessing. We will divide it into 2 parts:\n\n- Data Inspection\n- Data Cleaning\n\n**Data Inspection**\n\nLet\u2019s check out a few positive tweets.","41d817c0":"#### **if you like this notebook plz upvote it**\n#### **Thank you**","3e88b716":"Let\u2019s check out a few negative tweets.","96993ed7":"#### **Performance Metrics And Accuracy**","337143f7":"In the train dataset, we have 2,026 (26%) tweets labeled as negative, and 5,894 (74%) tweets labeled as positive. So, it is an imbalanced classification challenge.\n\nNow we will check the distribution of length of the tweets, in terms of words, in both train and test data.","67e95aad":"#### **Predict on test**","1f7de1ef":"### **Problem statement**\nSentiment analysis remains one of the key problems that has seen extensive application of natural language processing. This time around, given the tweets from customers about various tech firms who manufacture and sell mobiles, computers, laptops, etc, the task is to identify if the tweets have a negative sentiment towards such companies or products.","ee03e9d5":"#### **Model Training**","f95f7f6a":"**Data Cleaning**\n\nIn any natural language processing task, cleaning raw text data is an important step. It helps in getting rid of the unwanted words and characters which helps in obtaining better features. If we skip this step then there is a higher chance that you are working with noisy and inconsistent data. The objective of this step is to clean noise those are less relevant to find the sentiment of tweets such as punctuation, special characters, numbers, and terms which don\u2019t carry much weightage in context to the text.\nBefore we begin cleaning, let\u2019s first combine train and test datasets. Combining the datasets will make it convenient for us to preprocess the data. Later we will split it back into train and test data.","74f978b6":"![](https:\/\/datahack-prod.s3.ap-south-1.amazonaws.com\/__sized__\/contest_cover\/sentiments_1920x480-thumbnail-1200x1200-90.jpg)","dd926ef5":"There are quite a many words and characters which are not really required. So, we will try to keep only those words which are important and add value.\n\nLet\u2019s have a glimpse at label-distribution in the train dataset."}}