{"cell_type":{"fefe4db8":"code","798c188e":"code","2ee39ff3":"code","7cfa7a4d":"code","90d4fee0":"code","13e7efd4":"code","29d9628e":"code","a69daeb4":"code","0abcfb11":"code","3394540a":"code","2c887fbc":"code","db2d6fe8":"code","9d58dece":"code","972ab09e":"code","c8bd84b0":"markdown","3ebe0876":"markdown","d7ba4ef5":"markdown","d499dba3":"markdown","e61cc528":"markdown","67c7439d":"markdown","19a534af":"markdown","2cf915a2":"markdown"},"source":{"fefe4db8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport shutil\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","798c188e":"!pip install 'gym[box2d]'\n!apt-get install python-opengl -y\n!apt install xvfb -y\n!pip install pyvirtualdisplay\n!pip install https:\/\/github.com\/pyglet\/pyglet\/archive\/pyglet-1.5-maintenance.zip\n!apt-get install ffmpeg -y","2ee39ff3":"from pyvirtualdisplay import Display\nimport gym\nfrom gym import wrappers\nfrom gym import envs\nimport matplotlib.pyplot as plt","7cfa7a4d":"display = Display(visible=0,size=(600,600))\ndisplay.start()","90d4fee0":"env = gym.make('MountainCar-v0')\n# state = env.reset()","13e7efd4":"monitor_dir = os.getcwd()\nenv = wrappers.Monitor(env,monitor_dir,video_callable=lambda ep_id: ep_id%1000 == 0,force=True)","29d9628e":"print(env.action_space.n)","a69daeb4":"L_R = 0.1\nDisc = 0.95\nEpis = 4001\nepsilon = 1\nstart_decay = 1\nend_decay = Epis\/\/2\nepsilon_decay = epsilon\/(end_decay - start_decay)\nstats_for = 100\nepi_rewards = []\nreward_stats = {'epi':[],'avg_r':[],'max_r':[],'min_r':[]}","0abcfb11":"print(env.observation_space.high)\nprint(env.observation_space.low)\nbuckets_shape = [20,20]\nbucket_range = (env.observation_space.high - env.observation_space.low)\/buckets_shape\nprint(bucket_range)\nq_table = np.random.uniform(low=-2,high=0,size=(buckets_shape + [env.action_space.n]))\nprint(q_table.shape)","3394540a":"def dis_from_cont(cont_state):\n    state = (cont_state - env.observation_space.low)\/bucket_range\n    return tuple(state.astype(np.int))","2c887fbc":"# env.reset()\nfor ep in range(Epis):\n    epi_r = 0\n    dis_state = dis_from_cont(env.reset())\n    done = False\n    while not done:\n        if np.random.random() > epsilon:\n            action = np.argmax(q_table[dis_state])\n        else:\n            action = np.random.randint(0,env.action_space.n)\n    #     action = env.action_space.sample()\n        new_state,reward,done,_ = env.step(action)\n        epi_r += reward\n        new_dis_state = dis_from_cont(new_state)\n        if not done:\n            #update q\n            max_future_q = np.max(q_table[new_dis_state])\n            curr_q = q_table[dis_state + (action,)]\n            new_q = (1 - L_R)*curr_q + L_R*(reward + Disc*max_future_q)\n            q_table[dis_state + (action,)] = new_q\n        elif new_state[0] >= env.goal_position:\n            q_table[dis_state + (action,)] = reward\n    #     print(action,state,reward,done,_)\n        dis_state = new_dis_state\n    epi_rewards.append(epi_r)\n    #track metrics for rewards\n    if ep%stats_for == 0:\n        reward_stats['epi'].append(ep)\n        avg_r = sum(epi_rewards[-stats_for:])\/stats_for\n        reward_stats['avg_r'].append(avg_r)\n        reward_stats['max_r'].append(max(epi_rewards[-stats_for:]))\n        reward_stats['min_r'].append(min(epi_rewards[-stats_for:]))\n        print(f\"Episode {ep}, avg reward {avg_r:.1f}, epsilon {epsilon:.2f}\")\n    if start_decay <= ep <= end_decay:\n        epsilon -= epsilon_decay\n    \nenv.close()","db2d6fe8":"from IPython.display import HTML\nfrom base64 import b64encode\n\nvideo = [v for v in os.listdir('.\/') if 'mp4' in v]\nvideo.sort()\nprint(len(video))\n# print(video[:26])\nvid_1 = open(video[0],'rb').read()\ndata_url_1 = \"data:video\/mp4;base64,\" + b64encode(vid_1).decode()\nHTML(\"\"\"\n<video width=600 height=600 controls>\n      <source src=\"%s\" type=\"video\/mp4\">\n<\/video>\n\"\"\" % data_url_1)","9d58dece":"vid_2 = open(video[-1],'rb').read()\ndata_url_2 = \"data:video\/mp4;base64,\" + b64encode(vid_2).decode()\nHTML(\"\"\"\n<video width=600 height=600 controls>\n      <source src=\"%s\" type=\"video\/mp4\">\n<\/video>\n\"\"\" % data_url_2)","972ab09e":"plt.plot(reward_stats['epi'],reward_stats['avg_r'],label='average reward')\nplt.plot(reward_stats['epi'],reward_stats['max_r'],label='max reward')\nplt.plot(reward_stats['epi'],reward_stats['min_r'],label='min reward')\nplt.grid(True)\nplt.legend(loc=2)\nplt.show()","c8bd84b0":"**For playing video of the episodes and saving them**","3ebe0876":"# This notebook shows an example of Q-learning for Mountain car problem\n> **Main points covered here are:**\n1. Training an agent for Mountain-car v0 from OpenAI gym using Q-Learning\n2. Displaying the video of the episodes in kaggle and saving them\n3. Tracking metrics of rewards and plotting them","d7ba4ef5":"**Converting continous observation space into 20 discrete values**","d499dba3":"**Wrapping the env with monitor and staring the display (vidoes will be automatically stored in CWD)**","e61cc528":"**Installing required packages**","67c7439d":"**Training using Q-Learning**","19a534af":"**Plotting metrics**","2cf915a2":"**Code to display video in kaggle**"}}