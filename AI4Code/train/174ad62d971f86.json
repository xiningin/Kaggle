{"cell_type":{"d23dba78":"code","367eed40":"code","57dea4a1":"code","0d8d1506":"code","a664c69f":"code","05cdba8a":"code","cd08e4f5":"code","d86c77e0":"code","2c5fc55d":"code","1648f56f":"code","6e70487d":"code","35cd495e":"code","f4739396":"code","8e3c41d9":"code","f62457c2":"code","b9034060":"code","8ada348d":"code","d68914ba":"code","8aa21e5e":"code","618e29d5":"code","8f4a008d":"code","feeffacb":"code","75f04e54":"code","2ae656e5":"code","6cd340d3":"code","754dd56c":"code","44e8ffb6":"code","d6a598c6":"code","60f66800":"code","26c05343":"code","8e478ecb":"code","c3788e30":"code","37da5128":"code","69b071f0":"code","84f02fd0":"code","a64ccd01":"code","d25aa71b":"code","5fbe908c":"code","b781d00a":"code","3199a807":"code","0ba9314c":"code","d9485c05":"code","ee51bf2f":"code","1ae82ce1":"code","3e241c06":"markdown","baed18ca":"markdown","e2206d92":"markdown","6de9c4fb":"markdown","ddf91c36":"markdown","aad3418f":"markdown","99c191fb":"markdown","82a190a7":"markdown","6220692b":"markdown","5200ce28":"markdown"},"source":{"d23dba78":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nfrom scipy.stats import zscore\nimport math as math\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom vecstack import stacking\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.neural_network import MLPRegressor\n%matplotlib inline\n# sns.set(color_codes=True)\nsns.set(style=\"darkgrid\", color_codes=True)\n","367eed40":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","57dea4a1":"#step 2.1: Read the dataset\nrdata=pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv')","0d8d1506":"testdata=pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv')","a664c69f":"# step 2.1: browse through the first few columns\nrdata.head()","05cdba8a":"# Step 2.2: Understand the shape of the data\nshape_data=rdata.shape\nprint('The shape of the dataframe is',shape_data,'which means there are',shape_data[0],'rows of observations and',shape_data[1],'attributes ')","cd08e4f5":"# Step 2.3: Identify Duplicate records in the data \n# It is very important to check and remove data duplicates. \n# Else our model may break or report overly optimistic \/ pessimistic performance results\ndupes=rdata.duplicated()\nprint(' The number of duplicates in the dataset are:',sum(dupes), '\\n','This means there are no duplicates in the train dataset')","d86c77e0":"# Step 2.4: Lets analyze the data types\nrdata.info()","2c5fc55d":"#EDA 1: lets evaluate statistical details of the dataset. \ncname=rdata.columns\ndata_desc=rdata.describe().T\ndata_desc","1648f56f":"t1data=rdata.drop(['id'],axis=1)","6e70487d":"t1data_test=testdata.drop('id',axis=1)","35cd495e":"t1data.info()","f4739396":"# Step 1.8: lets evaluate correlation between different attributes.\n# The column ID has been ignored from the correlation heatmap. The reason for the same will be explained in the next section.\nusecols =[i for i in t1data.columns if i != 'ID']\ncorr=t1data[usecols].corr()\nfig, ax = plt.subplots(figsize=(20,15))\nsns.heatmap(corr,annot=True,linewidth=0.5,ax=ax, fmt= '.2f');","8e3c41d9":"# Attributes in the Group\nAtr1g1='cont1'\nAtr2g1='cont2'\nAtr3g1='cont3'\nAtr4g1='cont4'\nAtr5g1='cont5'\nAtr6g1='cont6'\nAtr7g1='cont7'\nAtr8g1='cont8'\nAtr9g1='cont9'\nAtr10g1='cont10'\nAtr11g1='cont11'\nAtr12g1='cont12'\nAtr13g1='cont13'\nAtr14g1='cont14'","f62457c2":"#EDA 1: Outliar Detection leveraging Box Plot\ndata=t1data\nfig, ax = plt.subplots(1,14,figsize=(38,16)) \nsns.boxplot(x=Atr1g1,data=data,ax=ax[0],orient='v') \nsns.boxplot(x=Atr2g1,data=data,ax=ax[1],orient='v')\nsns.boxplot(x=Atr3g1,data=data,ax=ax[2],orient='v')\nsns.boxplot(x=Atr4g1,data=data,ax=ax[3],orient='v')\nsns.boxplot(x=Atr5g1,data=data,ax=ax[4],orient='v')\nsns.boxplot(x=Atr6g1,data=data,ax=ax[5],orient='v')\nsns.boxplot(x=Atr7g1,data=data,ax=ax[6],orient='v')\nsns.boxplot(x=Atr8g1,data=data,ax=ax[7],orient='v')\nsns.boxplot(x=Atr9g1,data=data,ax=ax[8],orient='v')\nsns.boxplot(x=Atr10g1,data=data,ax=ax[9],orient='v')\nsns.boxplot(x=Atr11g1,data=data,ax=ax[10],orient='v')\nsns.boxplot(x=Atr12g1,data=data,ax=ax[11],orient='v')\nsns.boxplot(x=Atr13g1,data=data,ax=ax[12],orient='v')\nsns.boxplot(x=Atr14g1,data=data,ax=ax[13],orient='v')","b9034060":"data=t1data\n#EDA 2: Skewness check\nAtr1g1_skew=round(stats.skew(data[Atr1g1]),4)\nAtr2g1_skew=round(stats.skew(data[Atr2g1]),4)\nAtr3g1_skew=round(stats.skew(data[Atr3g1]),4)\nAtr4g1_skew=round(stats.skew(data[Atr4g1]),4)\nAtr5g1_skew=round(stats.skew(data[Atr5g1]),4)\nAtr6g1_skew=round(stats.skew(data[Atr6g1]),4)\nAtr7g1_skew=round(stats.skew(data[Atr7g1]),4)\nAtr8g1_skew=round(stats.skew(data[Atr8g1]),4)\nAtr9g1_skew=round(stats.skew(data[Atr9g1]),4)\nAtr10g1_skew=round(stats.skew(data[Atr10g1]),4)\nAtr11g1_skew=round(stats.skew(data[Atr11g1]),4)\nAtr12g1_skew=round(stats.skew(data[Atr12g1]),4)\nAtr13g1_skew=round(stats.skew(data[Atr13g1]),4)\nAtr14g1_skew=round(stats.skew(data[Atr14g1]),4)\n\nprint(' The skewness of',Atr1g1,'is', Atr1g1_skew)\nprint(' The skewness of',Atr2g1,'is', Atr2g1_skew)\nprint(' The skewness of',Atr3g1,'is', Atr3g1_skew)\nprint(' The skewness of',Atr4g1,'is', Atr4g1_skew)\nprint(' The skewness of',Atr5g1,'is', Atr5g1_skew)\nprint(' The skewness of',Atr6g1,'is', Atr6g1_skew)\nprint(' The skewness of',Atr7g1,'is', Atr7g1_skew)\nprint(' The skewness of',Atr8g1,'is', Atr8g1_skew)\nprint(' The skewness of',Atr9g1,'is', Atr9g1_skew)\nprint(' The skewness of',Atr10g1,'is', Atr9g1_skew)\nprint(' The skewness of',Atr11g1,'is', Atr9g1_skew)\nprint(' The skewness of',Atr12g1,'is', Atr9g1_skew)\nprint(' The skewness of',Atr13g1,'is', Atr9g1_skew)\nprint(' The skewness of',Atr14g1,'is', Atr9g1_skew)","8ada348d":"##EDA 3: Spread\ndata=t1data\nfig, ax = plt.subplots(1,14,figsize=(16,8)) \nsns.distplot(data[Atr1g1],ax=ax[0]) \nsns.distplot(data[Atr2g1],ax=ax[1]) \nsns.distplot(data[Atr3g1],ax=ax[2])\nsns.distplot(data[Atr4g1],ax=ax[3])\nsns.distplot(data[Atr5g1],ax=ax[4])\nsns.distplot(data[Atr6g1],ax=ax[5])\nsns.distplot(data[Atr7g1],ax=ax[6])\nsns.distplot(data[Atr8g1],ax=ax[7])\nsns.distplot(data[Atr9g1],ax=ax[8])\nsns.distplot(data[Atr10g1],ax=ax[9])\nsns.distplot(data[Atr11g1],ax=ax[10])\nsns.distplot(data[Atr12g1],ax=ax[11])\nsns.distplot(data[Atr13g1],ax=ax[12])\nsns.distplot(data[Atr14g1],ax=ax[13])\nsns.distplot(data[Atr15g1],ax=ax[14])","d68914ba":"# Lets visually understand if there is any correlation between the independent variables. \nusecols =[i for i in t1data.columns if i != 'target']\nsns.pairplot(t1data,diag_kind='kde');","8aa21e5e":"# before proceeding further, lets first create a copy of the data-set.\n# we will use simple imputer and strategy as mean for addressing outliars","618e29d5":"t2data = t1data.copy()\nt2data_test=t1data_test.copy()","8f4a008d":"def outliar_detection(col):\n    Q1=t2data[col].quantile(0.25)\n    Q3=t2data[col].quantile(0.75)\n    IQR=Q3-Q1\n    Lower_Whisker = Q1-1.5*IQR\n    Upper_Whisker = Q3+1.5*IQR\n    t2data[col][t2data[col]> Upper_Whisker] = np.nan\n    t2data[col][t2data[col]< Lower_Whisker] = np.nan\n    return t2data[col][t2data[col].isnull()]","feeffacb":"ol_columns=['cont1','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10','cont11','cont12','cont13','cont14']\nol_columns=np.array(ol_columns)","75f04e54":"for i in ol_columns:\n    outliar_detection(i)","2ae656e5":"t2data.info()","6cd340d3":"# Imputing the missing values with mean\ncolumns=t2data.columns\nimp_median = SimpleImputer(missing_values=np.nan, strategy='mean')\n\nimp_median.fit_transform(t2data)\n# imp_median.fit(t2data)\nt2data=pd.DataFrame(imp_median.transform(t2data))\nt2data.columns=columns","754dd56c":"def outliar_detection_test(col):\n    Q1=t2data_test[col].quantile(0.25)\n    Q3=t2data_test[col].quantile(0.75)\n    IQR=Q3-Q1\n    Lower_Whisker = Q1-1.5*IQR\n    Upper_Whisker = Q3+1.5*IQR\n    t2data_test[col][t2data_test[col]> Upper_Whisker] = np.nan\n    t2data_test[col][t2data_test[col]< Lower_Whisker] = np.nan\n    return t2data_test[col][t2data_test[col].isnull()]","44e8ffb6":"for i in ol_columns:\n    outliar_detection_test(i)","d6a598c6":"t2data_test.info()","60f66800":"# Imputing the missing values with mean\ncolumns=t2data_test.columns\nimp_median = SimpleImputer(missing_values=np.nan, strategy='mean')\n\nimp_median.fit_transform(t2data_test)\n# imp_median.fit(t2data)\nt2data_test=pd.DataFrame(imp_median.transform(t2data_test))\nt2data_test.columns=columns","26c05343":"data=t2data\nfig, ax = plt.subplots(1,14,figsize=(38,16)) \nsns.boxplot(x=Atr1g1,data=data,ax=ax[0],orient='v') \nsns.boxplot(x=Atr2g1,data=data,ax=ax[1],orient='v')\nsns.boxplot(x=Atr3g1,data=data,ax=ax[2],orient='v')\nsns.boxplot(x=Atr4g1,data=data,ax=ax[3],orient='v')\nsns.boxplot(x=Atr5g1,data=data,ax=ax[4],orient='v')\nsns.boxplot(x=Atr6g1,data=data,ax=ax[5],orient='v')\nsns.boxplot(x=Atr7g1,data=data,ax=ax[6],orient='v')\nsns.boxplot(x=Atr8g1,data=data,ax=ax[7],orient='v')\nsns.boxplot(x=Atr9g1,data=data,ax=ax[8],orient='v')\nsns.boxplot(x=Atr10g1,data=data,ax=ax[9],orient='v')\nsns.boxplot(x=Atr11g1,data=data,ax=ax[10],orient='v')\nsns.boxplot(x=Atr12g1,data=data,ax=ax[11],orient='v')\nsns.boxplot(x=Atr13g1,data=data,ax=ax[12],orient='v')\nsns.boxplot(x=Atr14g1,data=data,ax=ax[13],orient='v')","8e478ecb":"# lets build our regression model\n\nX=t1data.copy()\nX=X.drop('target',axis=1)\ny=t1data['target'].copy()","c3788e30":"X.head()","37da5128":"X.info()","69b071f0":"X.info()","84f02fd0":"# X_test=t3data_test.copy()\nX_test=t1data_test.copy()\n# X_test=pdata_test.copy()","a64ccd01":"X_test.head()","d25aa71b":"X_test.info()","5fbe908c":"estimators = [\n    GradientBoostingRegressor(learning_rate= 0.008,max_depth= 6,n_estimators= 2000,random_state= 1,subsample= .8\n                                  ,max_features='log2',min_samples_split=2000,min_samples_leaf = 50\n                                 ),\n    LGBMRegressor(n_estimators=300),\n    XGBRegressor(eta= 0.008,min_child_weight=4,eval_metric='logloss',gamma=0.0,reg_alpha=0.005,max_depth= 6,n_estimators= 2000,random_state= 1,subsample= 1),\n    CatBoostRegressor(n_estimators=300),\n    RandomForestRegressor(n_estimators=200)\n]","b781d00a":"S_train, S_test = stacking(estimators,                   \n                           X, y, X_test,   \n                           regression=True,\n                           metric=mean_squared_error, \n                           n_folds=5,\n                           shuffle=True,  \n                           random_state=0,\n                           verbose=2) ","3199a807":"model = GradientBoostingRegressor(learning_rate= 0.008,max_depth= 6,n_estimators= 2000,random_state= 1,subsample= .8\n                                  ,max_features='log2',min_samples_split=2000,min_samples_leaf = 50\n                                 )\nmodel=model.fit(S_train, y)","0ba9314c":"y_pred = model.predict(S_test)","d9485c05":"submission_stacking=pd.DataFrame({'id':testdata['id'],'target':y_pred})","ee51bf2f":"filename_stacking = '\/content\/drive\/MyDrive\/Competitions\/Jan-2021\/submissions\/submission_stacking1.csv'","1ae82ce1":"submission_stacking.to_csv(filename_stacking,index=False)","3e241c06":"#### Stacking","baed18ca":"#### 1.b Multivariate analysis \nBi-variate analysis between the predictor variables and between the predictor variables and target column.","e2206d92":"#### 1.c Outliar Addressal","6de9c4fb":"Observation:\n- Attributes cont7. cont9 and cont10 have outliars.\n- we will work on the outliars in the next section","ddf91c36":"### Deliverable -1 (Exploratory data quality report)","aad3418f":"#### Pre-Step2: Load the dataset","99c191fb":"### 2. Deliverable -2 (create the model )","82a190a7":"#### 1.a Univariate analysis","6220692b":"#### Pre-steps 1: Import the necessary libraries","5200ce28":"When we remove outliers and replace with mean, the distribution shape changes, the standard deviation becomes tighter creating new outliers. The new outliers would be much closer to the centre than original outliers so we accept them without modifying them"}}