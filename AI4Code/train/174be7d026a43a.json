{"cell_type":{"9c326b8b":"code","e1b12a8b":"code","3602f5c9":"code","13798209":"code","5c468dc7":"code","1571263d":"code","63f55990":"code","4ed7b9c3":"code","01cdc1a0":"code","e39c25f1":"code","e255e84f":"code","2db0f032":"code","ffad468c":"code","3f612d03":"code","cc80e264":"code","50a284be":"code","53d61c86":"code","7894f7ed":"code","7603f5ff":"code","0243edb9":"code","3b7b35f5":"code","7e106484":"code","fc89cce6":"code","7301d561":"code","08b77660":"code","ff3b6eef":"markdown","56b8dd32":"markdown","60dbdb82":"markdown","48e90cd2":"markdown","dbf9cca4":"markdown","dc60a399":"markdown","79d64430":"markdown","77456f4a":"markdown"},"source":{"9c326b8b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport unicodedata\nimport re\nimport time\n\nfrom transformers import TransfoXLConfig, TFTransfoXLModel\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e1b12a8b":"train_data = pd.read_csv('\/kaggle\/input\/chaii-hindi-and-tamil-question-answering\/train.csv')\nprint(len(train_data))\ntrain_data.head()","3602f5c9":"train_data.dropna(axis=0,how='any',inplace=True)\nprint(len(train_data))","13798209":"def _unicode_to_ascii(s):\n    \"\"\"\u5c06\u6587\u672c\u4eceunicode\u8f6cascii\n\n    Parameters\n    ----------\n    s : str\n        \u8f93\u5165\u6587\u672c\n    Returns\n    -------\n    s : str\n        \u5904\u7406\u540e\u7684\u6587\u672c\n    \"\"\"\n    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')","5c468dc7":"train_data['context'] = train_data.apply(lambda x: _unicode_to_ascii(x['context']),axis=1)\ntrain_data['question'] = train_data.apply(lambda x: _unicode_to_ascii(x['question']),axis=1)\ntrain_data['answer_text'] = train_data.apply(lambda x: _unicode_to_ascii(x['answer_text']),axis=1)","1571263d":"train_data['language'].value_counts()","63f55990":"def include(str1, str2):\n    a = set(str(str1).lower().split())\n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return round(float(len(c)) \/ len(b), 4)","4ed7b9c3":"context_question_similar = []\nfor index, row in train_data.iterrows():\n    sentence1 = row.context\n    sentence2 = row.question\n    score = include(sentence1, sentence2)\n    context_question_similar.append([sentence1, sentence2, score])\n    \ncontext_question_similar = pd.DataFrame(context_question_similar, columns=['context', 'question', 'score'])\ncontext_question_similar = context_question_similar.sort_values(by='score', ascending=False)\nf, ax = plt.subplots(figsize=(6, 15))\nsns.set_color_codes(\"pastel\")\nsns.countplot(y=\"score\",data=context_question_similar, color=\"b\")","01cdc1a0":"context_answer_text_similar = []\nfor index, row in train_data.iterrows():\n    sentence1 = row.context\n    sentence2 = row.answer_text\n    score = include(sentence1, sentence2)\n    context_answer_text_similar.append([sentence1, sentence2, score])\n    \ncontext_answer_text_similar = pd.DataFrame(context_answer_text_similar, columns=['context', 'answer_text', 'score'])\ncontext_answer_text_similar['score'].value_counts()","e39c25f1":"train_data['context_include_answertext'] = train_data.apply(lambda x: include(x['context'], x['answer_text'])==1,axis=1)\ntrain_data = train_data[train_data['context_include_answertext']==True]\n\nlen(train_data)","e255e84f":"index_len = []\ncount = 0\nfor index, row in train_data.iterrows():\n    count += 1\n    index_len.append([count, len(str(row['context']).split())])\n    \nindex_len = pd.DataFrame(index_len, columns=['index', 'len'])\nindex_len = index_len.sort_values(by='len', ascending=False)\n\nf, ax = plt.subplots(figsize=(15, 6))\nsns.lineplot(x=\"index\", y=\"len\", data=index_len)","2db0f032":"index_len = []\ncount = 0\nfor index, row in train_data.iterrows():\n    count +=1\n    index_len.append([count, len(str(row['answer_text']).split())])\n    \nindex_len = pd.DataFrame(index_len, columns=['index', 'len'])\nindex_len = index_len.sort_values(by='len', ascending=False)\n\nf, ax = plt.subplots(figsize=(15, 6))\nsns.lineplot(x=\"index\", y=\"len\", data=index_len)","ffad468c":"restructure_data = []\nfor index, row in train_data.iterrows():\n    contexts = re.split(r'(\\n|;|\\.)', row['context'])\n\n    true_flag = False\n    false_count = 0\n    for context in contexts:\n        if re.search(r'^\\s+$', context) is not None or len(context) < len(row['answer_text']):\n            continue\n        result = context.find(row['answer_text'])\n        \n        if result != -1 and true_flag == False:\n            score = 1\n            answer_start = result\n            answer_end = result + len(row['answer_text'])\n            restructure_data.append([context, row['question'], row['answer_text'], answer_start, answer_end, score,row['language']])\n            true_flag = True\n#         elif true_flag:\n#             break\n        elif false_count<3:\n            false_count += 1\n            score = 0\n            answer_start = 0\n            answer_end = 0\n            restructure_data.append([context, row['question'], row['answer_text'], answer_start, answer_end, score,row['language']])\n        elif false_count>=5 and true_flag:\n            break\n        \nrestructure_data = pd.DataFrame(restructure_data, columns=['context', 'question', 'answer_text', 'answer_start', 'answer_end', 'score','language'])\n","3f612d03":"len(restructure_data)","cc80e264":"restructure_data.head()","50a284be":"restructure_data['context_len'] = restructure_data.apply(lambda x: len(str(x['context']).split()) < 140,axis=1)\nrestructure_data = restructure_data[restructure_data['context_len']==True]\n\nlen(restructure_data)","53d61c86":"index_len = []\ncount = 0\nfor index, row in restructure_data.iterrows():\n    count += 1\n    index_len.append([count, len(str(row['context']).split())])\n    \nindex_len = pd.DataFrame(index_len, columns=['index', 'len'])\nindex_len = index_len.sort_values(by='len', ascending=False)\n\nf, ax = plt.subplots(figsize=(15, 6))\nsns.lineplot(x=\"index\", y=\"len\", data=index_len[:1000])","7894f7ed":"metadata = np.concatenate((train_data['context'],train_data['question'],train_data['answer_text']),axis=0)\nprint(np.shape(metadata))\n\ntokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(metadata, target_vocab_size=2**13)\n# sample_string = metadata[0]\n# print ('Tokenized sample_string is {}'.format(sample_string))\n# tokenized_string = tokenizer.encode(sample_string)\n# print ('Tokenized string is {}'.format(tokenized_string))","7603f5ff":"# bert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_multi_cased_L-12_H-768_A-12\/2\",trainable=True)\n# vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n# do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n# tokenizer = bert.tokenization.FullTokenizer(vocab_file, do_lower_case)","0243edb9":"def encode_sentence(s, tokenizer):\n    tokens = tokenizer.encode(s)\n    tokens.append(tokenizer.vocab_size+2)\n    return tokens\n#     tokens = list(tokenizer.tokenize(s))\n#     tokens.append('[SEP]')\n#     return tokenizer.convert_tokens_to_ids(tokens)\n\ndef bert_encode(glue_dict, tokenizer):\n    num_examples = len(glue_dict[\"context\"])\n\n    sentence1 = tf.ragged.constant([\n        encode_sentence(s, tokenizer) for s in np.array(glue_dict[\"context\"])])\n    sentence2 = tf.ragged.constant([\n        encode_sentence(s, tokenizer) for s in np.array(glue_dict[\"question\"])])\n\n    cls = [[tokenizer.vocab_size]]*sentence1.shape[0]\n#     cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n\n    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n\n    input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n    type_cls = tf.zeros_like(cls)\n    type_s1 = tf.zeros_like(sentence1)\n    type_s2 = tf.ones_like(sentence2)\n    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor()\n    \n    \n    inputs = {\n        'input_word_ids': tf.cast(input_word_ids.to_tensor(), dtype=tf.int64),\n        'input_mask': tf.cast(input_mask, dtype=tf.int64),\n        'input_type_ids': tf.cast(input_type_ids, dtype=tf.int64)}\n\n    return inputs\n\ndef bert_encode_label(glue_dict):\n    answer_start = np.expand_dims(glue_dict['answer_start'], axis=-1)\n    answer_end = np.expand_dims(glue_dict['answer_end'], axis=-1)\n    score = np.expand_dims(glue_dict['score'], axis=-1)\n    \n    labels = tf.concat([answer_start, answer_end, score], axis=-1)\n    labels = tf.cast( labels, dtype=tf.int64)\n    return labels\n        \ninput_shape = ()\nglue_train = bert_encode(restructure_data, tokenizer)\nfor key, value in glue_train.items():\n    input_shape = value.shape\n    print(f'{key:15s} shape: {value.shape}')\n    \nglue_train_labels = bert_encode_label(restructure_data)\nprint(f'glue_train_labels shape: {glue_train_labels.shape}')","3b7b35f5":"BUFFER_SIZE = 40000\nBATCH_SIZE = 2\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((glue_train, glue_train_labels))\n\ntrain_dataset = train_dataset.cache()\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n\ntrain_inp, train_tar = next(iter(train_dataset))\ntrain_inp, train_tar","7e106484":"class TransfoXLQA(tf.keras.Model):\n    def __init__(self, input_shape):\n        super(TransfoXLQA, self).__init__()\n        configuration = TransfoXLConfig()\n        self.TransfoXL = TFTransfoXLModel(configuration)\n        self.gapool1d = tf.keras.layers.GlobalAveragePooling1D()\n        \n        self.start = tf.keras.layers.Dense(input_shape[-1])\n        self.end = tf.keras.layers.Dense(input_shape[-1])\n        self.score = tf.keras.layers.Dense(input_shape[-1])\n        \n    def call(self, input_word_ids, input_mask, input_type_ids):\n        \n        outputs = self.TransfoXL({'input_ids': input_word_ids}, training=True)\n        last_hidden_states = outputs.last_hidden_state\n        pool_out = self.gapool1d(last_hidden_states)\n        start = self.start(pool_out)\n        end = self.end(pool_out)\n        score = self.score(pool_out)\n        \n        output = tf.concat([start[:,tf.newaxis,:], end[:,tf.newaxis,:], score[:,tf.newaxis,:]], axis=1)\n        return output","fc89cce6":"transfoXLQA = TransfoXLQA(input_shape)\n\n# Set up epochs and steps\nepochs = 3\n\n# creates an optimizer with learning rate schedule\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n  def __init__(self, d_model, warmup_steps=4000):\n    super(CustomSchedule, self).__init__()\n\n    self.d_model = d_model\n    self.d_model = tf.cast(self.d_model, tf.float32)\n\n    self.warmup_steps = warmup_steps\n\n  def __call__(self, step):\n    arg1 = tf.math.rsqrt(step)\n    arg2 = step * (self.warmup_steps ** -1.5)\n\n    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n\nlearning_rate = CustomSchedule(1024)\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,  epsilon=1e-9)\n\n\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')","7301d561":"train_step_signature = [\n    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n    tf.TensorSpec(shape=(None, None), dtype=tf.int64)\n]\n\n@tf.function(input_signature=train_step_signature)\ndef train_step(input_word_ids, input_mask, input_type_ids, tar):\n    with tf.GradientTape() as tape:\n        predictions = transfoXLQA(input_word_ids, input_mask, input_type_ids)\n        loss = loss_fn(tar[:,2], predictions[:,2,:])\n    gradients = tape.gradient(loss, transfoXLQA.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, transfoXLQA.trainable_variables))\n    \n    train_loss(loss)\n    train_accuracy(tar[:,2], predictions[:,2,:])","08b77660":"for epoch in range(epochs):\n    start = time.time()\n\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n    \n    for (batch, (inp, tar)) in enumerate(train_dataset):\n        input_word_ids = inp['input_word_ids']\n        input_mask = inp['input_mask']\n        input_type_ids = inp['input_type_ids']\n        \n        train_step(input_word_ids, input_mask, input_type_ids, tar)\n\n        if batch % 50 == 0:\n            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result()} Accuracy {train_accuracy.result()}')\n    \n    print(f'Epoch {epoch + 1} Loss {train_loss.result()} Accuracy {train_accuracy.result()}')\n    print(f'Time taken for 1 epoch: {time.time() - start} secs\\n')","ff3b6eef":"context\u53e5\u5b50\u957f\u5ea6\u5206\u5e03","56b8dd32":"### tokenize","60dbdb82":"### language\u7c7b\u522b\u60c5\u51b5","48e90cd2":"### \u53e5\u5b50\u9884\u5904\u7406","dbf9cca4":"answer_text\u53e5\u5b50\u957f\u5ea6\u5206\u5e03","dc60a399":"answer_text\u4e0econtext\u7684\u4ea4\u96c6\u9664\u4ee5answer_text","79d64430":"# \u6570\u636e\u91cd\u6784","77456f4a":"question\u4e0econtext\u7684\u4ea4\u96c6\u9664\u4ee5question"}}