{"cell_type":{"7a300d7d":"code","f6c1e121":"code","3046d02f":"code","fc36c467":"code","7d02a24e":"code","8be4e6cd":"code","3781a3eb":"code","20ea1a7b":"code","22cee12b":"code","0a6efc5f":"code","ba88b5fa":"code","e171d79a":"code","60dbe9a1":"code","2aaeb4c7":"code","f48437da":"code","852f1c3b":"code","5e516401":"code","e972a087":"code","b210d04d":"code","df703944":"code","fbc7c771":"code","5cdb78ef":"code","8cb81a26":"code","86eb76e4":"code","5294e72b":"code","10c6250b":"code","553c2aa5":"code","c365a5c4":"code","a36c12c1":"code","da770cf7":"code","146c584a":"code","44a340f8":"code","918a1c87":"code","f63031af":"code","0f3226c4":"markdown","0c86dbed":"markdown","cc96dce1":"markdown","b274b0f8":"markdown","bba38efc":"markdown","59895d8f":"markdown","f89befec":"markdown","166f9d8b":"markdown","10fe46fd":"markdown","44aa1f33":"markdown","14c4aa97":"markdown","a61b5b6e":"markdown","ff0c3e0d":"markdown","c92a2ce3":"markdown","991039ce":"markdown","686a1123":"markdown","963b9712":"markdown","fac55de0":"markdown","149dc9b0":"markdown","d301b752":"markdown","76470e41":"markdown","386e941c":"markdown","775afe05":"markdown","5bbb737c":"markdown"},"source":{"7a300d7d":"import numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\nfrom sklearn.utils import class_weight\nfrom sklearn.preprocessing import minmax_scale\nimport random\nimport cv2\nfrom imgaug import augmenters as iaa\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Input, BatchNormalization, GlobalAveragePooling2D\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.experimental import CosineDecay\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.applications import EfficientNetB3\nfrom tensorflow.keras.layers.experimental.preprocessing import RandomCrop,CenterCrop, RandomRotation","f6c1e121":"training_folder = '..\/input\/cassava-leaf-disease-classification\/train_images\/'","3046d02f":"samples_df = pd.read_csv(\"..\/input\/cassava-leaf-disease-classification\/train.csv\")\nsamples_df = shuffle(samples_df, random_state=42)\nsamples_df[\"filepath\"] = training_folder+samples_df[\"image_id\"]\nsamples_df.head()","fc36c467":"training_percentage = 0.8\ntraining_item_count = int(len(samples_df)*training_percentage)\nvalidation_item_count = len(samples_df)-int(len(samples_df)*training_percentage)\ntraining_df = samples_df[:training_item_count]\nvalidation_df = samples_df[training_item_count:]","7d02a24e":"batch_size = 8\nimage_size = 512\ninput_shape = (image_size, image_size, 3)\ndropout_rate = 0.4\nclasses_to_predict = sorted(training_df.label.unique())","8be4e6cd":"training_data = tf.data.Dataset.from_tensor_slices((training_df.filepath.values, training_df.label.values))\nvalidation_data = tf.data.Dataset.from_tensor_slices((validation_df.filepath.values, validation_df.label.values))","3781a3eb":"def load_image_and_label_from_path(image_path, label):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    return img, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntraining_data = training_data.map(load_image_and_label_from_path, num_parallel_calls=AUTOTUNE)\nvalidation_data = validation_data.map(load_image_and_label_from_path, num_parallel_calls=AUTOTUNE)","20ea1a7b":"training_data_batches = training_data.shuffle(buffer_size=1000).batch(batch_size).prefetch(buffer_size=AUTOTUNE)\nvalidation_data_batches = validation_data.shuffle(buffer_size=1000).batch(batch_size).prefetch(buffer_size=AUTOTUNE)","22cee12b":"adapt_data = tf.data.Dataset.from_tensor_slices(training_df.filepath.values)\ndef adapt_mode(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = layers.experimental.preprocessing.Rescaling(1.0 \/ 255)(img)\n    return img\n\nadapt_data = adapt_data.map(adapt_mode, num_parallel_calls=AUTOTUNE)\nadapt_data_batches = adapt_data.shuffle(buffer_size=1000).batch(batch_size).prefetch(buffer_size=AUTOTUNE)","0a6efc5f":"data_augmentation_layers = tf.keras.Sequential(\n    [\n        layers.experimental.preprocessing.RandomCrop(height=image_size, width=image_size),\n        layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n        layers.experimental.preprocessing.RandomRotation(0.25),\n        layers.experimental.preprocessing.RandomZoom((-0.2, 0)),\n        layers.experimental.preprocessing.RandomContrast((0.2,0.2))\n    ]\n)","ba88b5fa":"image = Image.open(\"..\/input\/cassava-leaf-disease-classification\/train_images\/3412658650.jpg\")\nplt.imshow(image)\nplt.show()","e171d79a":"image = tf.expand_dims(np.array(image), 0)","60dbe9a1":"plt.figure(figsize=(10, 10))\nfor i in range(9):\n  augmented_image = data_augmentation_layers(image)\n  ax = plt.subplot(3, 3, i + 1)\n  plt.imshow(augmented_image[0])\n  plt.axis(\"off\")","2aaeb4c7":"efficientnet = EfficientNetB3(weights=\"..\/input\/efficientnetb3-notop\/efficientnetb3_notop.h5\", \n                              include_top=False, \n                              input_shape=input_shape, \n                              drop_connect_rate=dropout_rate)\n\ninputs = Input(shape=input_shape)\naugmented = data_augmentation_layers(inputs)\nefficientnet = efficientnet(augmented)\npooling = layers.GlobalAveragePooling2D()(efficientnet)\ndropout = layers.Dropout(dropout_rate)(pooling)\noutputs = Dense(len(classes_to_predict), activation=\"softmax\")(dropout)\nmodel = Model(inputs=inputs, outputs=outputs)\n    \nmodel.summary()","f48437da":"%%time\nmodel.get_layer('efficientnetb3').get_layer('normalization').adapt(adapt_data_batches)","852f1c3b":"epochs = 8\ndecay_steps = int(round(len(training_df)\/batch_size))*epochs\ncosine_decay = CosineDecay(initial_learning_rate=1e-4, decay_steps=decay_steps, alpha=0.3)\n\ncallbacks = [ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(cosine_decay), metrics=[\"accuracy\"])","5e516401":"history = model.fit(training_data_batches,\n                  epochs = epochs, \n                  validation_data=validation_data_batches,\n                  callbacks=callbacks)","e972a087":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss over epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","b210d04d":"model.load_weights(\"best_model.h5\")","df703944":"def scan_over_image(img_path, crop_size=512):\n    '''\n    Will extract 512x512 images covering the whole original image\n    with some overlap between images\n    '''\n    \n    img = Image.open(img_path)\n    img_height, img_width = img.size\n    img = np.array(img)\n    \n    y = random.randint(0,img_height-crop_size)\n    x = random.randint(0,img_width-crop_size)\n\n    x_img_origins = [0,img_width-crop_size]\n    y_img_origins = [0,img_height-crop_size]\n    img_list = []\n    for x in x_img_origins:\n        for y in y_img_origins:\n            img_list.append(img[x:x+crop_size , y:y+crop_size,:])\n  \n    return np.array(img_list)","fbc7c771":"def display_samples(img_path):\n    '''\n    Display all 512x512 images extracted from original images\n    '''\n    \n    img_list = scan_over_image(img_path)\n    sample_number = len(img_list)\n    fig = plt.figure(figsize = (8,sample_number))\n    for i in range(0,sample_number):\n        ax = fig.add_subplot(2, 4, i+1)\n        ax.imshow(img_list[i])\n        ax.set_title(str(i))\n    plt.tight_layout()\n    plt.show()\n\ndisplay_samples(\"..\/input\/cassava-leaf-disease-classification\/train_images\/3412658650.jpg\")","5cdb78ef":"test_time_augmentation_layers = tf.keras.Sequential(\n    [\n        layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n        layers.experimental.preprocessing.RandomZoom((-0.2, 0)),\n        layers.experimental.preprocessing.RandomContrast((0.2,0.2))\n    ]\n)","8cb81a26":"def predict_and_vote(image_filename, folder, TTA_runs=4):\n    '''\n    Run the model over 4 local areas of the given image,\n    before making a decision depending on the most predicted\n    disease.\n    '''\n    \n    #apply TTA to each of the 4 images and sum all predictions for each local image\n    localised_predictions = []\n    local_image_list = scan_over_image(folder+image_filename)\n    for local_image in local_image_list:\n        duplicated_local_image = tf.convert_to_tensor(np.array([local_image for i in range(TTA_runs)]))\n        augmented_images = test_time_augmentation_layers(duplicated_local_image)\n        \n        predictions = model.predict(augmented_images)\n        localised_predictions.append(np.sum(predictions, axis=0))\n    \n    #sum all predictions from all 4 images and retrieve the index of the highest value\n    global_predictions = np.sum(np.array(localised_predictions),axis=0)\n    final_prediction = np.argmax(global_predictions)\n    \n    return final_prediction","86eb76e4":"def run_predictions_over_image_list(image_list, folder):\n    predictions = []\n    with tqdm(total=len(image_list)) as pbar:\n        for image_filename in image_list:\n            pbar.update(1)\n            predictions.append(predict_and_vote(image_filename, folder))\n    return predictions","5294e72b":"validation_df[\"results\"] = run_predictions_over_image_list(validation_df[\"image_id\"], training_folder)","10c6250b":"!cat ..\/input\/cassava-leaf-disease-classification\/label_num_to_disease_map.json","553c2aa5":"validation_df[:20]","c365a5c4":"true_positives = 0\nprediction_distribution_per_class = {\"0\":{\"0\": 0, \"1\": 0, \"2\":0, \"3\":0, \"4\":0},\n                                     \"1\":{\"0\": 0, \"1\": 0, \"2\":0, \"3\":0, \"4\":0},\n                                     \"2\":{\"0\": 0, \"1\": 0, \"2\":0, \"3\":0, \"4\":0},\n                                     \"3\":{\"0\": 0, \"1\": 0, \"2\":0, \"3\":0, \"4\":0},\n                                     \"4\":{\"0\": 0, \"1\": 0, \"2\":0, \"3\":0, \"4\":0}}\nnumber_of_images = len(validation_df)\nfor idx, pred in validation_df.iterrows():\n    if int(pred[\"label\"]) == pred.results:\n        true_positives+=1\n    prediction_distribution_per_class[str(pred[\"label\"])][str(pred.results)] += 1\nprint(\"accuracy: {}%\".format(true_positives\/number_of_images*100))","a36c12c1":"prediction_distribution_per_class","da770cf7":"heatmap_df = pd.DataFrame(columns={\"groundtruth\",\"prediction\",\"value\"})\nfor key in prediction_distribution_per_class.keys():\n    for pred_key in prediction_distribution_per_class[key].keys():\n        value = prediction_distribution_per_class[key][pred_key]\/validation_df.query(\"label==@key\").count()[0]\n        heatmap_df = heatmap_df.append({\"groundtruth\":key,\"prediction\":pred_key,\"value\":value}, ignore_index=True)   \n\nheatmap = heatmap_df.pivot(index='groundtruth', columns='prediction', values='value')\nsns.heatmap(heatmap,cmap=\"Blues\")","146c584a":"test_folder = '..\/input\/cassava-leaf-disease-classification\/test_images\/'\nsubmission_df = pd.DataFrame(columns={\"image_id\",\"label\"})\nsubmission_df[\"image_id\"] =  os.listdir(test_folder)\nsubmission_df[\"label\"] = 0","44a340f8":"submission_df[\"label\"] = run_predictions_over_image_list(submission_df[\"image_id\"], test_folder)","918a1c87":"submission_df","f63031af":"submission_df.to_csv(\"submission.csv\", index=False)","0f3226c4":"First, we will check that we perform on similar level on both the training and validation. The training curve will also tell us if we stopped training too early or may have overfitted in comparison to the validation data.","0c86dbed":"First, I test my entire prediction pipeline on the validation set as we have little visibility over the test set.","cc96dce1":"The code below allows to load the data from the dataframes using `tf.data`.","b274b0f8":"Initially, I had set the image size to 300x300 in order to fit the original input size to the EfficientnetB3 model. However, as we are randomly cropping these images from the original 600x800-pixel images, it appears that using a dimension of 512x512 pixels leads to better results.","bba38efc":"Simply reusing some of the code from [this tutorial](https:\/\/www.tensorflow.org\/tutorials\/images\/data_augmentation) to show what our augmentations look like. I add the image previously opened to a batch and pass it through the data augmentation layers.","59895d8f":"This notebook presents a full pipeline to load the data, apply advanced data augmentation, train an EfficientNet and use the model to predict over the test images. To make it possible to run within the allocated time for notebooks, this notebook will only present a single fold with a split of 80% for training and 20% for validation. Due to the original image size of 600x800 pixels, we will randomly crop 512x512 images from original images in order to keep the highest image resolution possible for our model training. Previous versions of this notebook used resized images and the results were extremely poor in comparison (~0.42 accuracy).","f89befec":"# Prepare the training and validation data generators","166f9d8b":"# Verification of the training process","10fe46fd":"At this point, you may have noticed that I have not used any kind of normalization or rescaling. I recently discovered that there is a Normalization layer included in Keras'pretrained EfficientNet, as mentioned [here](https:\/\/keras.io\/examples\/vision\/image_classification_efficientnet_fine_tuning\/#keras-implementation-of-efficientnet).","44aa1f33":"# EfficientNet+Augmentation for Cassava Disease Classification using TF.Keras","14c4aa97":"Version notes:\n* 27: Fix bug in the TTA process.\n* 26: Move from using `ImageDataGenerator` to `tf.data` to load data into the model. This allows to tune the Normalization layer in the pretrained EfficientNetB3 model provided by `tf.keras`. By default, images are normalized using the `imagenet` dataset's mean and standard deviation. *- score: 0.888*\n* 24: Test CosineDecay instead of ReduceLRonPlateau and increase the dropout rate within the EfficientNetB3 model. *- score: 0.885*\n* 22: Replace the custom generator and data augmentation using `imgaug` with the new Keras preprocessing layers. Also increase the image size from 300x300 to 512x512. *- score: 0.880*\n* 21: *- score: 0.883*\n* 20: Fix bug activating the \"training mode\" by default in the custom generator, even during validation. Removed data standardization and class weights, simplify the bottleneck layers of the model, and remove the dropout from the data augmentation techniques. *- score: 0.873*\n* 17: Additional data augmentation techniques. *- score: 0.821*\n* 16: Tighter scan of the images at test time. *- score: 0.857*\n* 15: Add data standardization. (Cannot remember if there was a bug but the score was abnormaly low) *- score: 0.692*\n* 11: Train the model by randomly cropping 300x300pixel tiles from the original images *- score: 0.847*\n* 9: Classification from resized images. *- score: 0.421* ","a61b5b6e":"The 3rd layer of the Efficientnet is the Normalization layer, which can be tuned to our new dataset instead of `imagenet`. Be patient on this one, it does take a bit of time as we're going through the entire training set.","ff0c3e0d":"### Thanks for reading this notebook! If you found this notebook helpful, please give it an upvote. It is always greatly appreciated!","c92a2ce3":"I wanted to try the new `CosineDecay` function implemented in `tf.keras` as it seemed promising and I struggled to find the right settings (if there were any) for the `ReduceLROnPlateau`.","991039ce":"We load the best weights that were kept from the training phase. Just to check how our model is performing, we will attempt predictions over the validation set. This can help to highlight any classes that will be consistently miscategorised.","686a1123":"Below are some version notes which were written at version 20 so I only included what I could remember.","963b9712":"I am using an EfficientNetB3 on top of which I add some outputs layers to predict our 5 disease classes. I decided to load the imagenet pretrained weights locally to keep the internet off (part of the requirements to submit a kernel to this competition).","fac55de0":"I keep 80% of the data provided for training and retain the other 20% for validation during my training process.","149dc9b0":"We can also have a better understanding of where this new model misclassifies diseases by plotting a heatmap from the results. Each row in this heatmap is normalised to highlight the classification distribution per disease without being bothered by the fact that the dataset is imbalanced.","d301b752":"The data augmentation preprocessing layers below will be used when training the model but disabled in inference mode.","76470e41":"I also prepare a special dataset that will be fed to the Normalization layer. The EfficientnetB3 provided by `tf.keras` includes an out-of-the-box Normalization layer fit onto the `imagenet` dataset. Therefore, we can pull that layer and use the `adapt` function to refit it to the Cassava Disease dataset.","386e941c":"# Build the model","775afe05":"# Prediction on test images","5bbb737c":"I apply some very basic test time augmentation to every local image extracted from the original 600x800 image. We know we can do some fancy augmentation with `imgaug` or `albumentations` but I wanted to do that exclusively with Keras' preprocessing layers to keep the \"cleanest\" pipeline possible."}}