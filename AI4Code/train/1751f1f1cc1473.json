{"cell_type":{"65230135":"code","00b05e73":"code","a5f6ff9b":"code","aa6fc1c6":"code","e7aa39f3":"code","20b4bc64":"code","6801d75c":"code","14059698":"code","4e64ac45":"code","4e8cc83c":"code","d463f30f":"code","5735508d":"code","8fd98fdb":"code","aa442a97":"code","83e03e15":"code","9ed83985":"markdown","5c5bc51b":"markdown","9a28d760":"markdown","e0ff0169":"markdown"},"source":{"65230135":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nfrom tensorflow import keras\nimport random\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import BatchNormalization\nfrom sklearn.model_selection import train_test_split, KFold","00b05e73":"train = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/train.csv\")\nto_test = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/test.csv\").drop(\"id\", axis = 1)","a5f6ff9b":"fake = pd.read_csv(\"..\/input\/tps08-nn-file\/7.87122.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/test.csv\")\ntest[\"loss\"] = fake.loss\ntrain = pd.concat([train, test]).reset_index(drop = True)","aa6fc1c6":"y = train.loss\nX = train.drop([\"id\", \"loss\"], axis = 1)","e7aa39f3":"sns.distplot(y, hist=True, kde=False, \n             bins=int(180\/5), color = 'blue',\n             hist_kws={'edgecolor':'black'})","20b4bc64":"def cmlp(dr, hu1,hu2, hu3, hu4, hu5, hu6, hu7, hu8, hu9, lr):\n    HIDDEN_LAYER_1 = [hu1, hu2]    #[256, 256]\n    HIDDEN_LAYER_2 = [hu3, hu4, hu5]    #[160, 160, 160]\n    HIDDEN_LAYER_3 = [hu6, hu7, hu8, hu9]    #[128, 128, 128, 128]\n    TARGET_NUM = 1 \n    input = tf.keras.layers.Input(shape=(X.shape[1], ))\n    #part_1\n    x1 = tf.keras.layers.BatchNormalization()(input)\n    x1 = tf.keras.layers.Dropout(dr)(x1)\n    for units in HIDDEN_LAYER_1:\n        x1 = tf.keras.layers.Dense(units)(x1)\n        x1 = tf.keras.layers.BatchNormalization()(x1)\n        x1 = tf.keras.layers.Activation(tf.keras.activations.swish)(x1)\n        x1 = tf.keras.layers.Dropout(dr)(x1)\n    # part_2\n    x2 = tf.keras.layers.BatchNormalization()(input)\n    x2 = tf.keras.layers.Dropout(dr)(x2)\n    for units in HIDDEN_LAYER_2:\n        x2 = tf.keras.layers.Dense(units)(x2)\n        x2 = tf.keras.layers.BatchNormalization()(x2)\n        x2 = tf.keras.layers.Activation(tf.keras.activations.swish)(x2)\n        x2 = tf.keras.layers.Dropout(dr)(x2)\n    # part_3\n    x3 = tf.keras.layers.BatchNormalization()(input)\n    x3 = tf.keras.layers.Dropout(dr)(x3)\n    for units in HIDDEN_LAYER_3:\n        x3 = tf.keras.layers.Dense(units)(x3)\n        x3 = tf.keras.layers.BatchNormalization()(x3)\n        x3 = tf.keras.layers.Activation(tf.keras.activations.swish)(x3)\n        x3 = tf.keras.layers.Dropout(dr)(x3)\n    x = tf.keras.layers.concatenate([x1, x2, x3])\n    output = tf.keras.layers.Dense(TARGET_NUM)(x)\n    model = tf.keras.models.Model(inputs=input, outputs=output)\n    model.compile(\n        optimizer = tf.optimizers.Adam(learning_rate = lr),\n        metrics   = [tf.keras.metrics.RootMeanSquaredError()],\n        loss      = \"MSE\"\n    )\n    return model\ndef lm(path):\n    model = tf.keras.models.load_model(path) \n    return model\nfrom sklearn.metrics import mean_squared_error\ndef my_metrics(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred, squared=False)\ndef set_seed(seed = 2021):\n    np.random.seed(seed)\n    random_state = np.random.RandomState(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n    return random_state","6801d75c":"folds = 7\nseed_list = [20]","14059698":"def objective(trial):\n    params = {\n        'dr' : trial.suggest_uniform('dr' ,0.01 , 0.3),\n        'hu1': trial.suggest_int('hu1' ,220 , 300),\n        'hu2': trial.suggest_int('hu2' ,230 , 300),\n        'hu3': trial.suggest_int('hu3' ,145 , 155),\n        'hu4': trial.suggest_int('hu4' ,135 , 150),\n        'hu5': trial.suggest_int('hu5' ,135 , 150),\n        'hu6': trial.suggest_int('hu6' ,110 , 120),\n        'hu7': trial.suggest_int('hu7' ,80 , 100),\n        'hu8': trial.suggest_int('hu8' ,80 , 100),\n        'hu9': trial.suggest_int('hu9' ,96 , 113),\n        'lr' : trial.suggest_uniform('lr' , 0.002 , 0.004)\n    }\n    score = 0\n    for seed in seed_list: \n        kf = KFold(n_splits = folds ,random_state= seed, shuffle = True)\n        fn = 1\n        for idx_train,idx_test in kf.split(X, y):\n            X_train,X_test=X.iloc[idx_train],X.iloc[idx_test]\n            y_train,y_test=y.iloc[idx_train],y.iloc[idx_test]\n            cp = tf.keras.callbacks.ModelCheckpoint(f'.\/model{fn}.h5', save_best_only = True, monitor = 'val_root_mean_squared_error', mode = 'min')\n            es = EarlyStopping(monitor = 'val_root_mean_squared_error', patience = 25, mode = 'min')\n            pc = optuna.integration.TFKerasPruningCallback(trial, 'val_root_mean_squared_error')\n            model = cmlp(**params) \n            set_seed(seed)\n            model.fit(X_train ,  y_train, validation_data = (X_test, y_test), epochs = 300, \n                       batch_size = 2000, callbacks = [cp, es\n                                                      , pc\n                                            ] , verbose = 0 )\n            model = lm(f'.\/model{fn}.h5')\n            y_pred = model.predict(X_test).ravel()\n            fn += 1\n            score += my_metrics(y_test.values.ravel(), y_pred)                \n        del model\n    return score \/ 7\nimport optuna\nstudy = optuna.create_study(direction = 'minimize' , study_name = 'nn'\n                         #   , pruner = optuna.pruners.HyperbandPruner()\n                           )\nstudy.optimize(objective , n_trials = 220)\nprint('numbers of the finished trials:' , len(study.trials))\nprint('the best params:' , study.best_trial.params)\nprint('the best value:' , study.best_value)\nprint(\"done\")\n#time.sleep(60)","4e64ac45":"pcmlp = {'dr': 0.17815556906426416, 'hu1': 278, 'hu2': 251, \n         'hu3': 155, 'hu4': 141, 'hu5': 141, 'hu6': 111, \n         'hu7': 88, 'hu8': 80, 'hu9': 104, \n         'lr': 0.0038937535106663996} # Best is trial 2 with value: 6.259858319677164.","4e8cc83c":"pcmlp = {'dr': 0.2714603811596955, 'hu1': 227, 'hu2': 233, \n         'hu3': 149, 'hu4': 139, 'hu5': 137, 'hu6': 113, 'hu7': 96, 'hu8': 96, \n         'hu9': 103, 'lr': 0.0029735538680568385} # the best value: 7.893019035495328","d463f30f":"score = 0\nfor seed in seed_list: \n    set_seed(seed)\n    print(\"seed\", seed)\n    kf = KFold(n_splits = folds, random_state= seed, shuffle = True)\n    fn = 1\n    for idx_train,idx_test in kf.split(X, y):\n        X_train,X_test=X.iloc[idx_train],X.iloc[idx_test]\n        y_train,y_test=y.iloc[idx_train],y.iloc[idx_test]\n        cp = tf.keras.callbacks.ModelCheckpoint(f'.\/3 layers regression_fold_{fn}_seed_{seed}.h5', \n                            save_best_only = True, monitor = 'val_root_mean_squared_error', mode = 'min')\n        es = EarlyStopping(monitor = 'val_root_mean_squared_error', patience = 50, mode = 'min')\n        model = cmlp(**pcmlp) \n        model.fit(X_train ,  y_train, validation_data = (X_test, y_test), epochs = 300, \n                   batch_size = 2000, callbacks = [cp, es], verbose = 1 )\n        model = lm(f'.\/3 layers regression_fold_{fn}_seed_{seed}.h5')\n        y_pred = model.predict(X_test).ravel()\n        score_temp = my_metrics(y_test.values.ravel(), y_pred)  \n        print(\"fold\", fn, \"model\", \"Validation score\", score_temp)\n        score += score_temp\n        fn += 1\nprint(\"Mean CV score\", (score \/ 7)\/ len(seed_list))","5735508d":"import glob","8fd98fdb":"output = []  \nfor filepath in glob.iglob('.\/*.h5'): # Mean CV score 7.892253827629668\n    model = lm(filepath)\n    pred = model.predict(to_test).ravel()\n    output.append(pred)\n    del model\n    del pred\ny_pred = sum(output) \/ len(output)","aa442a97":"final_pred = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")\nfinal_pred.loss = y_pred\nfinal_pred","83e03e15":"final_pred.to_csv('submission.csv',index=False)","9ed83985":"# Tuning","5c5bc51b":"# Check the distribution of the target value","9a28d760":"# pseudo labeling","e0ff0169":"# Config"}}