{"cell_type":{"e302a77c":"code","c356e0e2":"code","bbd07905":"code","e9cb38b3":"code","d14d39c6":"code","29cee0d3":"code","4005d1d4":"code","04cd4931":"code","d8dbd250":"code","5df30fa3":"code","d56e2e4b":"markdown","c5cd1f45":"markdown","07ff0dab":"markdown","177a1e15":"markdown","8add12db":"markdown"},"source":{"e302a77c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers as L\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nimport joblib\n","c356e0e2":"data = pd.read_csv('..\/input\/song-popularity-prediction\/train.csv')\nprint(data.shape)\ndata.head()","bbd07905":"test = pd.read_csv('..\/input\/song-popularity-prediction\/test.csv')\nX_test = test.drop(['id'], axis=1)","e9cb38b3":"X = data.drop(['id', 'song_popularity'], axis=1)\ny = data['song_popularity']","d14d39c6":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)","29cee0d3":"class NeuralDecisionTree(keras.Model):\n    def __init__(self, depth, num_features, used_features_rate, num_classes):\n        super(NeuralDecisionTree, self).__init__()\n        self.depth = depth\n        self.num_leaves = 2 ** depth\n        self.num_classes = num_classes\n        \n        num_used_features = int(num_features * used_features_rate)\n        one_hot = np.eye(num_features)\n        sampled_feature_indicies = np.random.choice(\n            np.arange(num_features), num_used_features, replace=False\n        )\n        self.used_features_mask = one_hot[sampled_feature_indicies]\n\n        self.pi = tf.Variable(\n            initial_value=tf.random_normal_initializer()(\n                shape=[self.num_leaves, self.num_classes]\n            ),\n            dtype=\"float32\",\n            trainable=True,\n        )\n        \n        self.decision_fn = L.Dense(\n            units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"\n        )\n\n    def call(self, features):\n        batch_size = tf.shape(features)[0]\n        \n        features = tf.matmul(\n            features, self.used_features_mask, transpose_b=True\n        )  \n        decisions = tf.expand_dims(\n            self.decision_fn(features), axis=2\n        )  \n        decisions = L.concatenate(\n            [decisions, 1 - decisions], axis=2\n        ) \n\n        mu = tf.ones([batch_size, 1, 1])\n\n        begin_idx = 1\n        end_idx = 2\n        \n        for level in range(self.depth):\n            mu = tf.reshape(mu, [batch_size, -1, 1]) \n            mu = tf.tile(mu, (1, 1, 2))\n            level_decisions = decisions[\n                :, begin_idx:end_idx, :\n            ]\n            mu = mu * level_decisions\n            begin_idx = end_idx\n            end_idx = begin_idx + 2 ** (level + 1)\n\n        mu = tf.reshape(mu, [batch_size, self.num_leaves])\n        probabilities = keras.activations.softmax(self.pi)\n        outputs = tf.matmul(mu, probabilities)\n        return outputs\n    \nclass NeuralDecisionForest(keras.Model):\n    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):\n        super(NeuralDecisionForest, self).__init__()\n        self.ensemble = []\n        self.num_classes = num_classes\n        for _ in range(num_trees):\n            self.ensemble.append(\n                NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n            )\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        outputs = tf.zeros([batch_size, self.num_classes])\n\n        for tree in self.ensemble:\n            outputs += tree(inputs)\n        outputs \/= len(self.ensemble)\n        return outputs","4005d1d4":"get_cat_pipeline = lambda: Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')), \n    ('encoder', OneHotEncoder(sparse=False))\n])\n\nget_num_pipeline = lambda: Pipeline([\n    ('imputer', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])","04cd4931":"class model_config:\n    NUMERIC_FEATURE_NAMES=[\n        'song_duration_ms', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness',\n        'speechiness', 'tempo', 'audio_valence'\n    ]\n    CATEGORICAL_FEATURE_NAMES=[\n        'key','audio_mode','time_signature'   \n    ]\n\nMAX_EPOCHS  = 250\n\nget_callbacks = lambda : [\n    keras.callbacks.EarlyStopping(min_delta=1e-4, patience=10, verbose=1, restore_best_weights=True),\n    keras.callbacks.ReduceLROnPlateau(patience=3, verbose=1)\n]","d8dbd250":"preds_tree = []\npreds_forest = []\n\nfor fold, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n\n    num_pipeline = get_num_pipeline().fit(X_train[model_config.NUMERIC_FEATURE_NAMES])\n    cat_pipeline = get_cat_pipeline().fit(X_train[model_config.CATEGORICAL_FEATURE_NAMES])\n    \n    X_train = np.hstack((\n        num_pipeline.transform(X_train[model_config.NUMERIC_FEATURE_NAMES]),\n        cat_pipeline.transform(X_train[model_config.CATEGORICAL_FEATURE_NAMES])\n    ))\n    X_valid = np.hstack((\n        num_pipeline.transform(X_valid[model_config.NUMERIC_FEATURE_NAMES]),\n        cat_pipeline.transform(X_valid[model_config.CATEGORICAL_FEATURE_NAMES])\n    ))\n    X_test_ = np.hstack((\n        num_pipeline.transform(X_test[model_config.NUMERIC_FEATURE_NAMES]),\n        cat_pipeline.transform(X_test[model_config.CATEGORICAL_FEATURE_NAMES])\n    ))\n    \n    neural_decsion_tree = NeuralDecisionTree(\n        depth=5, num_features=X_train.shape[1], used_features_rate=1.0, num_classes=2\n    )\n    neural_decsion_tree.compile(\n        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy']\n    )\n    neural_decsion_tree.fit(\n        X_train, y_train, validation_data=(X_valid, y_valid), callbacks=get_callbacks(), \n        epochs=MAX_EPOCHS\n    )  \n    preds_tree.append(neural_decsion_tree.predict(X_test_))\n    \n    neural_decsion_forest = NeuralDecisionForest(\n        num_trees=10, depth=5, num_features=X_train.shape[1], used_features_rate=0.8, num_classes=2\n    )\n    neural_decsion_forest.compile(\n        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy']\n    )\n    neural_decsion_forest.fit(\n        X_train, y_train, validation_data=(X_valid, y_valid), callbacks=get_callbacks(), \n        epochs=MAX_EPOCHS\n    )  \n    preds_forest.append(neural_decsion_forest.predict(X_test_))","5df30fa3":"submissions = pd.read_csv('..\/input\/song-popularity-prediction\/sample_submission.csv')\nsubmissions['song_popularity'] = np.array([arr[:, 1] for arr in preds_tree]).mean(axis=0)\nsubmissions.to_csv('preds_tree.csv', index=False)\n\nsubmissions['song_popularity'] = np.array([arr[:, 1] for arr in preds_forest]).mean(axis=0)\nsubmissions.to_csv('preds_forest.csv', index=False)","d56e2e4b":"# Model","c5cd1f45":"# Training","07ff0dab":"# Data","177a1e15":"# Submissions","8add12db":"# Deep Neural Decision Forests\n\nEven though deep learning has attained trendendous success on data domains such as images, audio and texts.\nGDBT still rule the domain of tabular data.\n\nIn this note we will discuss [Deep Neural Decision Forests](https:\/\/ieeexplore.ieee.org\/document\/7410529) for tabular deep learning"}}