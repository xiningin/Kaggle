{"cell_type":{"f79be819":"code","98ece65d":"code","adc7424c":"code","e1611560":"code","8aa0559f":"code","5241dba8":"code","28213937":"code","39ddc2c9":"code","e2de40b5":"code","46554d99":"code","f1373272":"code","6f6aad9c":"code","d046f472":"code","985f21b5":"code","2fb79bf9":"code","22a7a751":"code","b0a3c901":"code","96978cf5":"code","0de99496":"code","79a48137":"code","4c0bc6a0":"markdown","c7392be1":"markdown","f1096231":"markdown","faa9eeff":"markdown","c801fce1":"markdown","e0a76f1a":"markdown","87a6cbc5":"markdown","f2d586e0":"markdown","b55ea0b5":"markdown","2b78988f":"markdown","efe7df33":"markdown","60ccd639":"markdown","49e07eca":"markdown","3a9ba08c":"markdown"},"source":{"f79be819":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nimport tensorflow as tf\nfrom tensorflow.python import keras\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline ","98ece65d":"data_train = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\ndata_test = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')","adc7424c":"x_train = np.array(data_train.iloc[:, 1:])\ny_train = np.array(data_train.iloc[:, 0])\n\nx_test = np.array(data_test.iloc[:, 1:])\ny_test = np.array(data_test.iloc[:, 0])\n\nprint(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)","e1611560":"print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n\nx_train = x_train.reshape(x_train.shape[0], 28, 28)\nx_test = x_test.reshape(x_test.shape[0], 28, 28)\n\n# Print the number of training and test datasets\nprint(x_train.shape[0], 'train set')\nprint(x_test.shape[0], 'test set')\n\n# Define the text labels\nfashion_mnist_labels = [\"T-shirt\/top\",  # index 0\n                        \"Trouser\",      # index 1\n                        \"Pullover\",     # index 2 \n                        \"Dress\",        # index 3 \n                        \"Coat\",         # index 4\n                        \"Sandal\",       # index 5\n                        \"Shirt\",        # index 6 \n                        \"Sneaker\",      # index 7 \n                        \"Bag\",          # index 8 \n                        \"Ankle boot\"]   # index 9","8aa0559f":"# Create a dictionary for each type of label \nlabels = {0 : \"T-shirt\/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\",\n          5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle Boot\"}\n\ndef get_classes_distribution(data):\n    # Get the count for each label\n    label_counts = data[\"label\"].value_counts()\n\n    # Get total number of samples\n    total_samples = len(data)\n\n\n    # Count the number of items in each class\n    for i in range(len(label_counts)):\n        label = labels[label_counts.index[i]]\n        count = label_counts.values[i]\n        percent = (count \/ total_samples) * 100\n        print(\"{:<20s}:   {} or {}%\".format(label, count, percent))\n\nget_classes_distribution(data_train)","5241dba8":"def plot_label_per_class(data):\n    f, ax = plt.subplots(1,1, figsize=(12,4))\n    g = sns.countplot(data.label, order = data[\"label\"].value_counts().index)\n    g.set_title(\"Number of labels for each class\")\n\n    for p, label in zip(g.patches, data[\"label\"].value_counts().index):\n        g.annotate(labels[label], (p.get_x(), p.get_height()+0.1))\n    plt.show()  \n    \nplot_label_per_class(data_train)","28213937":"get_classes_distribution(data_test)","39ddc2c9":"plot_label_per_class(data_test)","e2de40b5":"x_train = x_train.astype('float32') \/ 255\nx_test = x_test.astype('float32') \/ 255","46554d99":"# Further break training data into train \/ validation sets\n(x_train, x_valid) = x_train[5000:], x_train[:5000] \n(y_train, y_valid) = y_train[5000:], y_train[:5000]\n\n# Reshape input data\nx_train = x_train.reshape(x_train.shape[0], 28,28, 1)\nx_valid = x_valid.reshape(x_valid.shape[0], 28,28, 1)\nx_test = x_test.reshape(x_test.shape[0], 28,28, 1)\n\n# One-hot encode the labels\ny_train = tf.keras.utils.to_categorical(y_train, 10)\ny_valid = tf.keras.utils.to_categorical(y_valid, 10)\ny_test = tf.keras.utils.to_categorical(y_test, 10)\n\n# Print training set shape\nprint(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n\n# Print the number of training, validation, and test datasets\nprint(x_train.shape[0], 'train set')\nprint(x_valid.shape[0], 'validation set')\nprint(x_test.shape[0], 'test set')","f1373272":"def plot_count_per_class(yd):\n    ydf = pd.DataFrame(yd)\n    f, ax = plt.subplots(1,1, figsize=(12,4))\n    g = sns.countplot(ydf[0], order = np.arange(0,10))\n    g.set_title(\"Number of items for each class\")\n    g.set_xlabel(\"Category\")\n    \n    for p, label in zip(g.patches, np.arange(0,10)):\n        g.annotate(labels[label], (p.get_x(), p.get_height()+0.1))\n        \n    plt.show()  \n\ndef get_count_per_class(yd):\n    ydf = pd.DataFrame(yd)\n    # Get the count for each label\n    label_counts = ydf[0].value_counts()\n\n    # Get total number of samples\n    total_samples = len(yd)\n\n\n    # Count the number of items in each class\n    for i in range(len(label_counts)):\n        label = labels[label_counts.index[i]]\n        count = label_counts.values[i]\n        percent = (count \/ total_samples) * 100\n        print(\"{:<20s}:   {} or {}%\".format(label, count, percent))\n    \nplot_count_per_class(np.argmax(y_train,axis=1))\nget_count_per_class(np.argmax(y_train,axis=1))","6f6aad9c":"plot_count_per_class(np.argmax(y_valid,axis=1))\nget_count_per_class(np.argmax(y_valid,axis=1))","d046f472":"model = tf.keras.Sequential()\n\n# Must define the input shape in the first layer of the neural network\nmodel.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=2))\nmodel.add(tf.keras.layers.Dropout(0.2))\n\nmodel.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=2))\nmodel.add(tf.keras.layers.Dropout(0.2))\n\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n\n# Take a look at the model summary\nmodel.summary()","985f21b5":"model.compile(loss='categorical_crossentropy',\n             optimizer='adam',\n             metrics=['accuracy'])","2fb79bf9":"history = model.fit(x_train,y_train, batch_size=100, epochs=30, validation_data=(x_valid, y_valid))","22a7a751":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([-1,1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([-1,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","b0a3c901":"# Evaluate the model on test set\nscore = model.evaluate(x_test, y_test, verbose=0)\n\n# Print test accuracy\nprint('\\n', 'Test accuracy:', score[1])","96978cf5":"y_hat = model.predict(x_test)\npredicted_classes = model.predict_classes(x_test)\ny_true = data_test.iloc[:, 0]","0de99496":"target_names = [\"Class {} ({}) :\".format(i,labels[i]) for i in range(10)]\nprint(classification_report(y_true, predicted_classes, target_names=target_names))","79a48137":"figure = plt.figure(figsize=(20, 8))\nfor i, index in enumerate(np.random.choice(x_test.shape[0], size=15, replace=False)):\n    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n    # Display each image\n    ax.imshow(np.squeeze(x_test[index]))\n    predict_index = np.argmax(y_hat[index])\n    true_index = np.argmax(y_test[index])\n    # Set the title for each image\n    ax.set_title(\"{} ({})\".format(fashion_mnist_labels[predict_index], \n                                  fashion_mnist_labels[true_index]),\n                                  color=(\"green\" if predict_index == true_index else \"red\"))","4c0bc6a0":"# *Please upvote the kernel if you find it insightful*","c7392be1":"# Predicted labels Visualization","f1096231":"# Class imbalance for Validationset","faa9eeff":"# Model definition and Training","c801fce1":"# Data Preprocessing","e0a76f1a":"# Import Libraries","87a6cbc5":"# Class Distribution for Testset","f2d586e0":"# Image Classification with CNN - Tutorial\n\n> Image classification is the primary domain, in which deep neural networks play the most important role of medical image analysis. The image classification accepts the given input images and produces output classification.In image classification, CNNs are the recent state-of-the-art methods. The CNNs learned about natural images, showing strong performance and encountering the accuracy of human expert systems.\n\n\n> The Convolution layer is always the first. \u0422he image (matrix with pixel values) is entered into it. Imagine that the reading of the input matrix begins at the top left of image. Next the software selects a smaller matrix there, which is called a filter. Then the filter produces convolution, i.e. moves along the input image. The filter\u2019s task is to multiply its values by the original pixel values. All these multiplications are summed up. One number is obtained in the end. \n\n> The nonlinear layer is added after each convolution operation. It has an activation function, which brings nonlinear property.\n\n> The pooling layer follows the nonlinear layer. It works with width and height of the image and performs a downsampling operation on them. As a result the image volume is reduced.\n\n> After completion of series of convolutional, nonlinear and pooling layers, it is necessary to attach a fully connected layer. This layer takes the output information from convolutional networks.","b55ea0b5":"# Class Distribution for Trainset","2b78988f":"# Class imbalance for Trainset","efe7df33":"# Evaluation","60ccd639":"# Prediction","49e07eca":"# Classification Report","3a9ba08c":"# Importing the dataset"}}