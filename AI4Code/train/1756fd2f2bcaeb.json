{"cell_type":{"bc7e413f":"code","2b9db7de":"code","77e8d257":"code","247edc59":"code","c6c4fbeb":"code","98f37dc4":"code","e5e85bfc":"code","9c94cb8c":"code","f2a282d4":"code","57a21127":"code","0c7ad60d":"code","15274c2a":"code","ca9fb027":"code","6ad14008":"code","0b51454e":"code","604c5c48":"code","99a7f033":"code","c74465f0":"code","fa2be9b7":"code","8afcd973":"code","623168d7":"code","53c0ede8":"markdown","835ddf80":"markdown","ce34135d":"markdown","05481df7":"markdown","002adaf2":"markdown","bbfdf4ca":"markdown"},"source":{"bc7e413f":"!pip install -q --no-deps ..\/input\/fasthugs","2b9db7de":"from fastai.text.all import *\nfrom fasthugs.data import TransformersTextBlock, TextGetter\nfrom fasthugs.learner import TransLearner\n\nfrom transformers import AutoModelForSequenceClassification\nfrom sklearn.model_selection import StratifiedKFold\nimport gc","77e8d257":"path = Path('..\/input\/commonlitreadabilityprize')\noutput_path = Path('.\/')\npath.ls()","247edc59":"train_df = pd.read_csv(path\/'train.csv')\ntrain_df.head()","c6c4fbeb":"train_df.describe()","98f37dc4":"cv_lbls = (train_df.target.to_numpy() > 0).astype(np.float)\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=8)\nvalid_idxs = []\nfor _, valid_idx in cv.split(np.arange(len(train_df)), cv_lbls):\n    valid_idxs += [valid_idx]","e5e85bfc":"model_name = '..\/input\/roberta-transformers-pytorch\/distilroberta-base'","9c94cb8c":"dblock = DataBlock(blocks = [TransformersTextBlock(pretrained_model_name=model_name), RegressionBlock()],\n                   get_x=TextGetter('excerpt'),\n                   get_y=ItemGetter('target'),\n                   splitter=IndexSplitter(valid_idxs[0]))","f2a282d4":"bs = 16\ndls = dblock.dataloaders(train_df, bs=bs, val_bs=bs*2, num_workers=2)","57a21127":"dls.show_batch(max_n=4)","0c7ad60d":"p_hdrop = 0.1\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1, hidden_dropout_prob=p_hdrop)\nmetrics = [rmse, R2Score(), PearsonCorrCoef(), SpearmanCorrCoef()]\nopt_func = Adam\nlearn = TransLearner(dls, model, metrics=metrics, path=output_path, opt_func=opt_func)","15274c2a":"lr = 2e-5\nwd = [0.05, 0.05, 0.05]","ca9fb027":"cbs=[SaveModelCallback(monitor='_rmse', fname='model_0', comp=np.less, reset_on_fit=False), GradientAccumulation(32)]\nlearn.fit_one_cycle(4, lr, wd=wd, cbs=cbs)","6ad14008":"(output_path\/'models').ls()","0b51454e":"learn.validate()","604c5c48":"all_preds = []","99a7f033":"test_df = pd.read_csv(path\/'test.csv')\ntest_dl = dls.test_dl(test_df)\ntest_dl.show_batch()","c74465f0":"preds, _ = learn.get_preds(dl=test_dl)\nall_preds += [preds]","fa2be9b7":"for i in range (1, len(valid_idxs)):\n    dblock = DataBlock(blocks = [TransformersTextBlock(pretrained_model_name=model_name), RegressionBlock()],\n                   get_x=TextGetter('excerpt'),\n                   get_y=ItemGetter('target'),\n                   splitter=IndexSplitter(valid_idxs[i]))\n    dls = dblock.dataloaders(train_df, bs=bs, val_bs=bs*2, num_workers=2)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1, hidden_dropout_prob=p_hdrop)\n    learn = TransLearner(dls, model, metrics=metrics, opt_func=opt_func)\n    cbs=[SaveModelCallback(monitor='_rmse', fname=f'model_{i}', comp=np.less)]\n    learn.fit_one_cycle(4, lr, wd=wd, cbs=cbs, div=10, div_final=1000)\n    preds, _ = learn.get_preds(dl=test_dl)\n    all_preds += [preds]\n    del learn; gc.collect()\n    torch.cuda.empty_cache()","8afcd973":"preds = torch.cat(all_preds, dim=1)\npreds","623168d7":"submission = pd.read_csv(path\/'sample_submission.csv', index_col='id')\nsubmission['target'] = preds.mean(dim=-1).numpy()\nsubmission.to_csv('submission.csv')","53c0ede8":"## Data preprocessing","835ddf80":"Let's fit models on remaining folds and save all the prediction.","ce34135d":"The best performing model is stored and loaded at the end of the training by `SaveModelCallback`:","05481df7":"## Cross validation","002adaf2":"## Submission\nFinally we can average the predictions from all models and submit:","bbfdf4ca":"## Training on first fold"}}