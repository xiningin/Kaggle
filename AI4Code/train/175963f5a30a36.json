{"cell_type":{"a2998c71":"code","1632e3ea":"code","76a5acd9":"code","e3841123":"code","e628cc2e":"code","2f64add9":"code","262c860a":"code","f85e73a7":"code","f96db151":"code","0524cd7d":"code","4026a7ba":"code","da145fa0":"code","a9648d07":"code","22631084":"code","fc00a9de":"code","e31ca2d1":"code","10006ded":"code","9a717aa1":"code","d8166414":"code","e5f99ff1":"code","c9fa78fd":"code","b48ef894":"code","4a6f57d6":"code","c07e0af4":"code","94deee7c":"code","2fc3119a":"code","1a2436b3":"code","35dfd562":"code","40531b33":"code","39c63e03":"code","886101c7":"code","8179591e":"code","b4ec19a6":"code","7fb038ee":"code","87caabf5":"code","c9fe2673":"code","48a1e057":"code","61ab4cf2":"code","1e2c4009":"code","693028fc":"code","f000e1a5":"code","d9bd766e":"markdown","17ace769":"markdown","9f3eae3b":"markdown","28166397":"markdown","37cd0441":"markdown","73cc656c":"markdown","f3851118":"markdown","73d0ab51":"markdown","d4b06efd":"markdown","4445a014":"markdown"},"source":{"a2998c71":"import numpy as np\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn import preprocessing\n\nfrom sklearn.linear_model import LinearRegression, LassoLarsCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n# from xgboost import XGBRegressor\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1632e3ea":"# Load Data\n\ntrain = pd.read_csv(\"..\/input\/bigquery-geotab-intersection-congestion\/train.csv\").sample(frac=0.15,random_state=42)#,nrows=123456)\ntest = pd.read_csv(\"..\/input\/bigquery-geotab-intersection-congestion\/test.csv\")\n","76a5acd9":"train.nunique()","e3841123":"print(train[\"City\"].unique())\nprint(test[\"City\"].unique())","e628cc2e":"# test.groupby([\"City\"]).apply(np.unique)\ntest.groupby([\"City\"]).nunique()","2f64add9":"train.isna().sum(axis=0)","262c860a":"test.isna().sum(axis=0)","f85e73a7":"directions = {\n    'N': 0,\n    'NE': 1\/4,\n    'E': 1\/2,\n    'SE': 3\/4,\n    'S': 1,\n    'SW': 5\/4,\n    'W': 3\/2,\n    'NW': 7\/4\n}","f96db151":"train['EntryHeading'] = train['EntryHeading'].map(directions)\ntrain['ExitHeading'] = train['ExitHeading'].map(directions)\n\ntest['EntryHeading'] = test['EntryHeading'].map(directions)\ntest['ExitHeading'] = test['ExitHeading'].map(directions)","0524cd7d":"train['diffHeading'] = train['EntryHeading']-train['ExitHeading']  # TODO - check if this is right. For now, it's a silly approximation without the angles being taken into consideration\n\ntest['diffHeading'] = test['EntryHeading']-test['ExitHeading']  # TODO - check if this is right. For now, it's a silly approximation without the angles being taken into consideration\n\ntrain[['ExitHeading','EntryHeading','diffHeading']].drop_duplicates().head(10)","4026a7ba":"### code if we wanted the diffs, without changing the raw variables:\n\n# train['diffHeading'] = train['ExitHeading'].map(directions) - train['EntryHeading'].map(directions)\n# test['diffHeading'] = test['ExitHeading'].map(directions) - test['EntryHeading'].map(directions)","da145fa0":"train.head()","a9648d07":"train[\"same_street_exact\"] = (train[\"EntryStreetName\"] ==  train[\"ExitStreetName\"]).astype(int)\ntest[\"same_street_exact\"] = (test[\"EntryStreetName\"] ==  test[\"ExitStreetName\"]).astype(int)","22631084":"le = preprocessing.LabelEncoder()\n# le = preprocessing.OneHotEncoder(handle_unknown=\"ignore\") # will have all zeros for novel categoricals, [can't do drop first due to nans issue , otherwise we'd  drop first value to avoid colinearity","fc00a9de":"train[\"Intersection\"] = train[\"IntersectionId\"].astype(str) + train[\"City\"]\ntest[\"Intersection\"] = test[\"IntersectionId\"].astype(str) + test[\"City\"]\n\nprint(train[\"Intersection\"].sample(6).values)","e31ca2d1":"train.head()","10006ded":"test.head()","9a717aa1":"# pd.concat([train,le.transform(train[\"Intersection\"].values.reshape(-1,1)).toarray()],axis=1).head()","d8166414":"pd.concat([train[\"Intersection\"],test[\"Intersection\"]],axis=0).drop_duplicates().values","e5f99ff1":"le.fit(pd.concat([train[\"Intersection\"],test[\"Intersection\"]]).drop_duplicates().values)\ntrain[\"Intersection\"] = le.transform(train[\"Intersection\"])\ntest[\"Intersection\"] = le.transform(test[\"Intersection\"])","c9fa78fd":"train.head()","b48ef894":"pd.get_dummies(train[\"City\"],dummy_na=False, drop_first=False).head()","4a6f57d6":"# pd.get_dummies(train[[\"EntryHeading\",\"ExitHeading\",\"City\"]].head(),prefix = {\"EntryHeading\":'en',\"ExitHeading\":\"ex\",\"City\":\"city\"})","c07e0af4":"train = pd.concat([train,pd.get_dummies(train[\"City\"],dummy_na=False, drop_first=False)],axis=1).drop([\"City\"],axis=1)\ntest = pd.concat([test,pd.get_dummies(test[\"City\"],dummy_na=False, drop_first=False)],axis=1).drop([\"City\"],axis=1)","94deee7c":"train.shape,test.shape","2fc3119a":"test.head()","1a2436b3":"train.columns","35dfd562":"FEAT_COLS = [\"IntersectionId\",\n             'Intersection',\n           'diffHeading',  'same_street_exact',\n           \"Hour\",\"Weekend\",\"Month\",\n          'Latitude', 'Longitude',\n          'EntryHeading', 'ExitHeading',\n            'Atlanta', 'Boston', 'Chicago',\n       'Philadelphia']","40531b33":"train.head()","39c63e03":"train.columns","886101c7":"X = train[FEAT_COLS]\ny1 = train[\"TotalTimeStopped_p20\"]\ny2 = train[\"TotalTimeStopped_p50\"]\ny3 = train[\"TotalTimeStopped_p80\"]\ny4 = train[\"DistanceToFirstStop_p20\"]\ny5 = train[\"DistanceToFirstStop_p50\"]\ny6 = train[\"DistanceToFirstStop_p80\"]","8179591e":"y = train[['TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80',\n        'DistanceToFirstStop_p20', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']]","b4ec19a6":"testX = test[FEAT_COLS]","7fb038ee":"## kaggle kernel performance can be very unstable when trying to use miltuiprocessing\n# lr = LinearRegression()\nlr = RandomForestRegressor(n_estimators=100,min_samples_split=3)#,n_jobs=3) #different default hyperparams, not necessarily any better","87caabf5":"## Original: model + prediction per target\n#############\n\nlr.fit(X,y1)\npred1 = lr.predict(testX)\nlr.fit(X,y2)\npred2 = lr.predict(testX)\nlr.fit(X,y3)\npred3 = lr.predict(testX)\nlr.fit(X,y4)\npred4 = lr.predict(testX)\nlr.fit(X,y5)\npred5 = lr.predict(testX)\nlr.fit(X,y6)\npred6 = lr.predict(testX)\n\n\n# Appending all predictions\nall_preds = []\nfor i in range(len(pred1)):\n    for j in [pred1,pred2,pred3,pred4,pred5,pred6]:\n        all_preds.append(j[i])   \n\nsub  = pd.read_csv(\"..\/input\/bigquery-geotab-intersection-congestion\/sample_submission.csv\")\nsub[\"Target\"] = all_preds\nsub.to_csv(\"benchmark_beat_rfr_multimodels.csv\",index = False)\n\nprint(len(all_preds))","c9fe2673":"## New\/Alt: multitask -  model for all targets\n\nlr.fit(X,y)\nprint(\"fitted\")\n\nall_preds = lr.predict(testX)","48a1e057":"## convert list of lists to format required for submissions\nprint(all_preds[0])\n\ns = pd.Series(list(all_preds) )\nall_preds = pd.Series.explode(s)\n\nprint(len(all_preds))\nprint(all_preds[0])","61ab4cf2":"sub  = pd.read_csv(\"..\/input\/bigquery-geotab-intersection-congestion\/sample_submission.csv\")\nprint(sub.shape)\nsub.head()","1e2c4009":"sub[\"Target\"] = all_preds.values\nsub.sample(5)","693028fc":"sub.to_csv(\"benchmark_beat_rfr_multitask.csv\",index = False)","f000e1a5":"train.drop(\"Path\",axis=1).to_csv(\"train_danFeatsV1.csv.gz\",index = False,compression=\"gzip\")\ntest.drop(\"Path\",axis=1).to_csv(\"test_danFeatsV1.csv.gz\",index = False,compression=\"gzip\")","d9bd766e":"* Forked from : https:\/\/www.kaggle.com\/pulkitmehtawork1985\/beating-benchmark\n* Copies feature code over from my other kernel; https:\/\/www.kaggle.com\/danofer\/basic-features-geotab-intersections\n\n* V6 - try  a multitask model in addition to a model per target. Likely to have worse performance, but will be faster","17ace769":"### Skip OHE intersections for now - memory issues\n* Intersection IDs aren't unique  etween cities - so we'll make new ones\n\n* Running fit on just train reveals that **the test data has a \"novel\" city + intersection!** ( '3Atlanta'!) (We will fix this)\n     * Means we need to be careful when OHEing the data\n     \n * There are 2,796 intersections, more if we count unique by city (~4K) = many, many columns. gave me memory issues when doing one hot encoding\n     * Could try count or target mean encoding. \n     \n* For now - ordinal encoding","9f3eae3b":"# Export featurized data\n\n* Uncomment this to get the features exported for further use. ","28166397":"## Data Cleaning\n","37cd0441":"#### with ordinal encoder - ideally we'd encode all the \"new\" cols with a single missing value, but it doesn't really matter given that they're Out of Distribution anyway (no such values in train). \n* So we'll fit on train+Test in order to avoid encoding errors - when using the ordinal encoder! (LEss of a n issue with OHE)","73cc656c":"* entering and exiting on same street\n* todo: clean text, check if on same boulevard, etc' ","f3851118":" #### Approach: We will make 6 predictions based on features we derived - IntersectionId , Hour , Weekend , Month , entry & exit directions .\n * Target variables will be TotalTimeStopped_p20 ,TotalTimeStopped_p50,TotalTimeStopped_p80,DistanceToFirstStop_p20,DistanceToFirstStop_p50,DistanceToFirstStop_p80 .\n \n * I leave in the original IntersectionId just in case there's meaning accidentally encoded in the numbers","73d0ab51":"* ALT : multitask model","d4b06efd":"### ORIG  OneHotEncode\n##### We could Create one hot encoding for entry , exit direction fields - but may make more sense to leave them as continous\n\n\n* Intersection ID is only unique within a city","4445a014":"## Add features\n\n##### turn direction: \nThe cardinal directions can be expressed using the equation: $$ \\frac{\\theta}{\\pi} $$\n\nWhere $\\theta$ is the angle between the direction we want to encode and the north compass direction, measured clockwise.\n\n* This is an **important** feature, as shown by janlauge here : https:\/\/www.kaggle.com\/janlauge\/intersection-congestion-eda\n\n* We can fill in this code in python (e.g. based on: https:\/\/www.analytics-link.com\/single-post\/2018\/08\/21\/Calculating-the-compass-direction-between-two-points-in-Python , https:\/\/rosettacode.org\/wiki\/Angle_difference_between_two_bearings#Python , https:\/\/gist.github.com\/RobertSudwarts\/acf8df23a16afdb5837f ) \n\n* TODO: circularize \/ use angles"}}