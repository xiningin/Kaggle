{"cell_type":{"60f377ba":"code","477c9446":"code","36d3884a":"code","180f536a":"code","0a28fc20":"code","990e25b0":"code","a65319f8":"code","77c96c20":"code","97d1f37e":"code","0275f602":"code","0829b4e7":"code","ceb5b516":"code","33088d38":"code","85fd450e":"code","eaf21fe9":"code","d3dbdac9":"code","77ce5893":"code","167236db":"code","6c1aae09":"code","b53dbf8e":"code","e70587be":"code","24fe2bb2":"code","50d0efa0":"code","f814116b":"code","e48f8cd4":"code","250d38aa":"code","5c5006a5":"code","564c7db4":"markdown","371cbdbf":"markdown","c4c595f4":"markdown","381b7294":"markdown","09943214":"markdown","a659474c":"markdown","efe65956":"markdown","69b15d76":"markdown","a360fb54":"markdown","241b9d84":"markdown","46a90789":"markdown","de7ef202":"markdown"},"source":{"60f377ba":"import cv2\nimport pydicom\nimport numpy as np\nimport pandas as pd\nimport pydicom\nimport os\nimport math\nimport random\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn import decomposition\nfrom typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, QuantileTransformer\nfrom sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\nimport itertools\nfrom sklearn.linear_model import BayesianRidge\n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nfrom matplotlib_venn import venn2\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\nstyle.use('fivethirtyeight')\npd.options.display.max_columns = None\n\nimport warnings\nwarnings.filterwarnings('ignore')","477c9446":"# CONFIG\nINPUT_DIR = \"..\/input\/osic-pulmonary-fibrosis-progression\"\nSEED = 42\nNFOLD = 10\nSCALER = 'MinMax'","36d3884a":"def read_tabular():\n    train = pd.read_csv(INPUT_DIR + '\/train.csv')\n    test = pd.read_csv(INPUT_DIR + '\/test.csv')\n    sub = pd.read_csv(INPUT_DIR + '\/sample_submission.csv')\n    return train, test, sub\ntrain, test, sub = read_tabular()","180f536a":"print(train.shape)\ntrain.head()","0a28fc20":"print(test.shape)\ntest.head()","990e25b0":"sub['Patient'] = sub['Patient_Week'].apply(lambda x:x.split('_')[0])\nsub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\nprint(sub.shape)\nsub.head()","a65319f8":"sub = pd.merge(sub[['Patient','Weeks','Confidence','Patient_Week']], test.drop(columns=['Weeks']), on='Patient')\ntrain['where'] = 'train'\ntest['where'] = 'test'\nsub['where'] = 'sub'\ndata = pd.concat([train, test, sub], ignore_index=True)\nprint(data.shape)","77c96c20":"# construct train input\ndef fe(data):\n    data['min_week'] = data['Weeks']\n    data.loc[data['where'] == 'test','min_week'] = np.nan\n    data['min_week'] = data.groupby('Patient')['min_week'].transform('min')\n    \n    base = data.loc[data.Weeks == data.min_week]\n    base = base[['Patient','FVC', 'Percent']].copy()\n    base.columns = ['Patient','base_FVC', 'base_Percent']\n    base['nb'] = 1\n    base['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\n    base = base[base.nb==1]\n    base.drop('nb', axis=1, inplace=True)\n\n    data = data.merge(base, on='Patient', how='left')\n    data['base_week'] = data['Weeks'] - data['min_week']\n    del base\n    \n    train = data.loc[data['where'] == 'train', :].reset_index(drop=True)\n    test = data.loc[data['where'] == 'test', :].reset_index(drop=True)\n    sub = data.loc[data['where'] == 'sub', :].reset_index(drop=True)\n\n    return train, test, sub\ntrain, test, sub = fe(data)","97d1f37e":"print(train.shape)\ntrain.head()","0275f602":"test = sub\nprint(test.shape)\ntest.head()","0829b4e7":"venn2([set(train['Patient'].values.tolist()), set(test['Patient'].values.tolist())])","ceb5b516":"# plot weeks vs Percent (and FVC)\ndef plot_weeks_vs(patient : str):\n    fig, ax = plt.subplots(1, 1, figsize=(7, 4))\n    colors = sns.color_palette('deep', 3)\n    weeks = train.loc[train['Patient'] == patient, 'Weeks'].values\n    \n    ax.plot(weeks, train.loc[train['Patient'] == patient, 'Percent'].values, '-o', color=colors[0], alpha=0.4)\n    ax.set_ylabel('Percent', color=colors[0])\n    ax.set_xlabel('Weeks')\n    ax.tick_params(axis='y', labelcolor=colors[0])\n    ax.set_title(patient)\n    \n    ax2 = ax.twinx()\n    ax2.plot(weeks+1, train.loc[train['Patient'] == patient, 'FVC'].values, '-^', color=colors[1], alpha=0.4)\n    ax2.plot(weeks[-3:]+1, train.loc[train['Patient'] == patient, 'FVC'].values[-3:], '-s', color=colors[2])\n    ax2.set_ylabel('FVC', color=colors[1])\n    ax2.tick_params(axis='y', labelcolor=colors[1])\n    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n    plt.show()\n    \nplot_weeks_vs(train['Patient'].unique()[0])","33088d38":"plot_weeks_vs(train['Patient'].unique()[1])","85fd450e":"plot_weeks_vs(test['Patient'].unique()[0])","eaf21fe9":"plot_weeks_vs(test['Patient'].unique()[2])","d3dbdac9":"import random\nfrom collections import Counter, defaultdict\nfrom sklearn import model_selection\n\n# ---- GroupKFold ----\nclass GroupKFold(object):\n    \"\"\"\n    GroupKFold with random shuffle with a sklearn-like structure\n    \"\"\"\n\n    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def get_n_splits(self, X=None, y=None, group=None):\n        return self.n_splits\n\n    def split(self, X, y, group):\n        kf = model_selection.KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n        unique_ids = X[group].unique()\n        for fold, (tr_group_idx, va_group_idx) in enumerate(kf.split(unique_ids)):\n            # split group\n            tr_group, va_group = unique_ids[tr_group_idx], unique_ids[va_group_idx]\n            train_idx = np.where(X[group].isin(tr_group))[0]\n            val_idx = np.where(X[group].isin(va_group))[0]\n            yield train_idx, val_idx","77ce5893":"# to normal\nID = 'Patient_Week'\ntarget = 'FVC'\ngroup = 'Patient'\nfeatures = ['Weeks', 'Percent', 'min_week', 'base_FVC', 'base_Percent', 'base_week']\n\n# starndarditest\nif SCALER == \"MinMax\":\n    scaler = MinMaxScaler()\nelif SCALER == \"Standard\":\n    scaler = StandardScaler()\ntrain[features] = scaler.fit_transform(train[features])\ntest[features] = scaler.transform(test[features])","167236db":"print(len(features))\nfeatures","6c1aae09":"def lb_metric(data, oof):\n    data['FVC_pred'] = oof[:, 0]\n    data['Confidence'] = oof[:, -1]\n    data['sigma_clipped'] = data['Confidence'].apply(lambda x: max(x, 70))\n    data['diff'] = abs(data['FVC'] - data['FVC_pred'])\n    data['delta'] = data['diff'].apply(lambda x: min(x, 1000))\n    data['score'] = -math.sqrt(2)*data['delta']\/data['sigma_clipped'] - np.log(math.sqrt(2)*data['sigma_clipped'])\n    score = data['score'].mean()\n    return score","b53dbf8e":"%%time\nypred = np.zeros((test.shape[0], 2))\noof = np.zeros((train.shape[0], 2))\nkf = GroupKFold(n_splits=NFOLD, shuffle=True, random_state=SEED)\nkf = kf.split(train, train[target], group)\n\nfor cnt, (tr_idx, val_idx) in enumerate(kf):\n    print(f\"FOLD {cnt}\")\n    \n    # fit\n    model = BayesianRidge()\n    model.fit(train[features].values[tr_idx, :], train[target].values[tr_idx])\n    \n    # evaluate\n    yme, ystd = model.predict(train[features].values[val_idx, :], return_std=True) # return_std=True!\n    oof[val_idx, 0] = yme \n    oof[val_idx, 1] = 2 * ystd\n    yme, ystd = model.predict(test[features].values, return_std=True)\n    ypred[:, 0] += yme \/ NFOLD \n    ypred[:, 1] += 2 * ystd \/ NFOLD\n    print(r'Fold {}: score = {}'.format(cnt, lb_metric(train.iloc[val_idx], oof[val_idx, :])))","e70587be":"score = lb_metric(train, oof)\nprint(f'Overall CV = {score}')","24fe2bb2":"plt.hist(oof[:, -1])\nplt.title(\"uncertainty in prediction\")\nplt.show()","50d0efa0":"submission = pd.read_csv(INPUT_DIR + '\/sample_submission.csv')\nsubmission.head()","f814116b":"test['FVC_pred'] = ypred[:, 0]\ntest['Confidence'] = ypred[:, 1]","e48f8cd4":"test[[ID, 'FVC_pred', 'Confidence']].head()","250d38aa":"test[['FVC_pred', 'Confidence']].describe().T","5c5006a5":"sub = submission.drop(columns=['FVC', 'Confidence']).merge(test[['Patient_Week', 'FVC_pred', 'Confidence']], \n                                                           on='Patient_Week')\nsub.columns = submission.columns\nsub.to_csv('submission.csv', index=False)\nsub.head()","564c7db4":"Here is the groupkfold with shuffle=True, which cannot be done in the native sklearn groupkfold method.","371cbdbf":"Did you know that sklearn's BayesianRidge predict method has ```return_std=True``` where the standard deviation of the prediction is returned?\n","c4c595f4":"# Libraries","381b7294":"# Fitting","09943214":"# Fitting","a659474c":"# Results","efe65956":"# EDA","69b15d76":"# Prediction","a360fb54":"Given there is a super-high positive correlation between FVC and Percent as a function of Weeks, I would simply use features related to weeks, FVC, and Percent.","241b9d84":"# Config","46a90789":"# Feature engineering","de7ef202":"# Load data"}}