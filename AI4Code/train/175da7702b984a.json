{"cell_type":{"3f4ee581":"code","63ec1ea7":"code","a440a301":"code","2f3876a7":"code","973fbddb":"code","d779dcb0":"code","44be49a6":"code","b51f094a":"code","7f3b6de8":"code","c06ef99d":"code","1deea16b":"code","fc524a80":"code","4ba8ebc3":"code","74c9760a":"code","837f72c6":"code","0dd23053":"code","556a95a4":"code","bd5f8d05":"code","fbacbde3":"code","ffbbc8e6":"code","a2609fd5":"code","4f8e896e":"code","ef2b05e3":"code","952c0ebe":"code","b1e4ce98":"code","9a4aa9ab":"code","2562da23":"code","adc18689":"code","5fb6dc71":"code","28f3b6b9":"code","761698d8":"code","7616594f":"code","8448d43f":"code","117e66cb":"code","6c366fa7":"code","9465d9eb":"markdown","47fea787":"markdown","668e964b":"markdown","1fede8f6":"markdown","5ef23df7":"markdown","037600d3":"markdown","d6d487eb":"markdown","4855c027":"markdown","8895703e":"markdown","e138ca45":"markdown","9ca7e923":"markdown","af18df47":"markdown","9e515644":"markdown","bdf42144":"markdown","a632308b":"markdown","a669ec6b":"markdown","7631d1d7":"markdown","490590f3":"markdown"},"source":{"3f4ee581":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd\nimport numpy as np\nimport missingno as mno\nimport matplotlib.pyplot as plt\nfrom scipy.cluster import hierarchy\nimport itertools\nfrom scipy.stats import spearmanr\nfrom collections import defaultdict\nimport seaborn as sns\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","63ec1ea7":"train=pd.read_csv('..\/input\/customer-segmentation\/Train.csv')\ntest=pd.read_csv('..\/input\/customer-segmentation\/Test.csv')\n\n#Joining both the testa and train data frames together \ndf=pd.concat([train,test],axis=0)","a440a301":"#Lets look at the data type of the features!\nprint(df.info())","2f3876a7":"#Visualize the missing values in the dataframe! \nmno.matrix(df)","973fbddb":"df.isnull().sum()","d779dcb0":"#obtainign the categroical columns alone\ncatcols = []\nfor i in df.columns:\n  if df[i].dtype == \"object\":\n      catcols.append(i)\ncatcols     ","44be49a6":"catcols[:-1]","b51f094a":"#Replacing the missing values in the categorical variables as \"not_available\"\ndf[catcols[:-1]] = df[catcols[:-1]].fillna(\"not_available\")\ndf.isnull().sum()","7f3b6de8":"gender_map = {'Female': 1, 'Male': 0}\nmarriage_map = {'not_available': 99, 'No': 0, 'Yes': 1}\ngraduate_map = {'not_available': 99, 'No': 0, 'Yes': 1}\nprofession_map = {'Artist': 0,'Doctor': 1,'Engineer': 2,'Entertainment': 3,'Executive': 4,'Healthcare': 5,\n                   'Homemaker': 6,'Lawyer': 7,'Marketing': 8,'not_available': 99}\nspending_map = {'Average': 1, 'High': 2, 'Low': 0}\nvar_map = {'Cat_1': 1,'Cat_2': 2,'Cat_3': 3,'Cat_4': 4,'Cat_5': 5, 'Cat_6': 6, 'Cat_7': 7,'not_available': 99}\ntarget_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3}","c06ef99d":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf[\"Prof+Grad\"] = df[\"Profession\"]+\"_\"+df[\"Graduated\"].astype(str)\ndf[\"Prof+Grad\"] = le.fit_transform(df['Prof+Grad'].astype(str))\n\ndf[\"Gender\"] =  df[\"Gender\"].map(gender_map)\ndf[\"Ever_Married\"] =  df[\"Ever_Married\"].map(marriage_map)\ndf[\"Graduated\"] =  df[\"Graduated\"].map(graduate_map)\ndf[\"Profession\"] =  df[\"Profession\"].map(profession_map)\ndf[\"Spending_Score\"] =  df[\"Spending_Score\"].map(spending_map)\ndf[\"Var_1\"] =  df[\"Var_1\"].map(var_map)\ndf[\"Segmentation\"] =  df[\"Segmentation\"].map(target_map)\n","1deea16b":"# Now lets move with replacing the numeric variables \ndf.isnull().sum()","fc524a80":"train['Work_Experience'].hist()\nprint(\"Median value of Family size feature is:\",train['Work_Experience'].median())","4ba8ebc3":"train['Family_Size'].hist()\nprint(\"Median value of Family size feature is:\",train['Family_Size'].median())","74c9760a":"df['Work_Experience']=df['Work_Experience'].fillna(train['Work_Experience'].median())\ndf['Family_Size']=df['Family_Size'].fillna(train['Family_Size'].median())","837f72c6":"age_bins=[0,20,40,60,80,100]\nage_labels=[ \"<=20\",\"21-40\",\"41-60\", \"61-80\",\">80\"]\n\ndf['Age']=pd.cut(df['Age'], bins=age_bins,labels=age_labels)\ndf['Age'].value_counts()","0dd23053":"df['Age']=le.fit_transform(df['Age'])\ndf","556a95a4":"temp = df.groupby(['Age']).agg({'Spending_Score':['count','mean','sum'],\n                                   'Work_Experience':['count','sum','min','max','mean'],\n                                   'Profession':['min','max'],\n                                       'Family_Size':['sum','min','max'],\n                                       'Age':['count'],\n                                    'Var_1':['count','max','min']})\ntemp.columns = ['_'.join(x) for x in temp.columns]\ndf = pd.merge(df,temp,on=['Age'],how='left')","bd5f8d05":"temp = df.groupby(['Profession']).agg({\n                                       'Age':['count','sum','min','max']})\ntemp.columns = ['_Prof_'.join(x) for x in temp.columns]\ndf = pd.merge(df,temp,on=['Profession'],how='left')","fbacbde3":"df=df.set_index('ID')\ndf_corr=df[df.columns[~df.columns.isin(['Segmentation','train_or_test'])]]","ffbbc8e6":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(60, 30))\ncorr = spearmanr(df_corr).correlation\ncorr_linkage = hierarchy.ward(np.nan_to_num(corr))\ndendro = hierarchy.dendrogram(corr_linkage, labels=df_corr.columns, ax=ax1,\n                              leaf_rotation=90)\ndendro_idx = np.arange(0, len(dendro['ivl']))\n\nax2.imshow(corr[dendro['leaves'], :][:, dendro['leaves']])\nax2.set_xticks(dendro_idx)\nax2.set_yticks(dendro_idx)\nax2.set_xticklabels(dendro['ivl'], rotation='vertical')\nax2.set_yticklabels(dendro['ivl'])\nfig.tight_layout()\nplt.show()\nfig.savefig('test2png.png', dpi=400)","a2609fd5":"sns.heatmap(corr)","4f8e896e":"numvar=[]\nfor i in np.arange(0.0, 2.1, 0.2):\n    cluster_ids = hierarchy.fcluster(corr_linkage, i, criterion='distance')\n    cluster_id_to_feature_ids = defaultdict(list)\n    for idx, cluster_id in enumerate(cluster_ids):\n        cluster_id_to_feature_ids[cluster_id].append(idx)\n    selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n    numvar.append([i,len(selected_features)])","ef2b05e3":"# selecting features based on chosen dendrogram y-axis value\ncluster_ids = hierarchy.fcluster(corr_linkage, 1.4, criterion='distance')\ncluster_id_to_feature_ids = defaultdict(list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\nselected_features = list(np.array(selected_features)+1)\nselected_features.append(-1)\nselected_features.insert(0,0)\ndf_corr = df_corr.iloc[:,selected_features[0:-2]]\n#df_corr.to_csv('modelreadydf.csv',index=0)\n#df_corr = pd.read_csv('modelreadydf.csv')","952c0ebe":"featured_df=df_corr\nfeatured_df.head(10)","b1e4ce98":"X_train = featured_df.iloc[0:len(train),:]\nX_test = featured_df.iloc[len(train):,:]\ny_train=train['Segmentation']\ny_test=test['Segmentation']","9a4aa9ab":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42)\nclassifier.fit(X_train, y_train)\npred_X=classifier.predict(X_train)\nprint('Training accuracy is {}'.format(accuracy_score(y_train, pred_X)))\n\npred_rf_baseline = classifier.predict(X_test)\nprint('Testing accuracy is {}'.format(accuracy_score(y_test, pred_rf_baseline)))","2562da23":"from sklearn.ensemble import RandomForestClassifier\n\nn_estimators = [100, 300, 500, 800, 1200]\nmax_depth = [5, 8, 15, 25, 30]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10] \nforest= RandomForestClassifier()\n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split, \n             min_samples_leaf = min_samples_leaf)\n\ngridF = GridSearchCV(forest, hyperF, cv = 3, verbose = 1, \n                      n_jobs = -1)\nbestF = gridF.fit(X_train, y_train)","adc18689":"print(\"Best depth:\",bestF.best_estimator_.get_params()['max_depth'])\nprint(\"Best n_estimators:\",bestF.best_estimator_.get_params()['n_estimators'])\nprint(\"Best min_samples_split:\",bestF.best_estimator_.get_params()['min_samples_split'])\nprint(\"Best min_samples_leaf:\",bestF.best_estimator_.get_params()['min_samples_leaf'])","5fb6dc71":"model_rf = RandomForestClassifier(random_state = 1,\n                                  n_estimators = 300,\n                                  max_depth = 8, \n                                  min_samples_split = 15,  min_samples_leaf = 1) \nmodel_rf = model_rf.fit(X_train, y_train)\npred_X = model_rf.predict(X_train)\nprint('Training accuracy is {}'.format(accuracy_score(y_train, pred_X)))\npred_rf= model_rf.predict(X_test)\nprint('Testing accuracy is {}'.format(accuracy_score(y_test, pred_rf)))","28f3b6b9":"pred_rf","761698d8":"import lightgbm as lgbm\n\nlgbm_clf = lgbm.LGBMClassifier(n_estimators=3000, cat_feature = [0,1,2,3,7,8,9,10,11,12,13,14,15,16,17,18], label_gain = [5], num_leaves=8, max_depth=20, \n                               learning_rate=0.01, random_state=42)\nlgbm_clf.fit(X_train, y_train)\npred_X = lgbm_clf.predict(X_train)\nprint('Training accuracy is {}'.format(accuracy_score(y_train, pred_X)))\npred_lgbm= lgbm_clf.predict(X_test)\nprint('Testing accuracy is {}'.format(accuracy_score(y_test, pred_lgbm)))\n","7616594f":"pred_lgbm","8448d43f":"### LGBM - RandomCV","117e66cb":"parameters = {'n_estimators':[1000,2000,3000,5000,10000], \n             'num_leaves':[15,25,30],\n             'learning_rate':[0.001,0.003,0.01,0.03],\n             'max_depth':[8,12,18,25],\n             'min_data_in_leaf':[40,50,60],\n             'reg_alpha':[i for i in np.arange(1,2,0.2)],\n             'reg_lambda':[i for i in np.arange(1,2,0.2)],\n             'subsample':[0.5,0.7,1]}\nlgbm_grid = RandomizedSearchCV(estimator=lgbm_clf1, param_grid=parameters, n_jobs=-1, cv = 5, scoring='accuracy', verbose=10)\n#Implement the lgbm_grid in the above lgbm baseline model to obtain better accuracy!","6c366fa7":"output=pd.DataFrame(columns=['ID','Segmentation'])\noutput['ID']=test['ID']\noutput['Segmentation']=pred_lgbm\noutput.to_csv('output.csv',index=False)","9465d9eb":"#### Since most of the classification algorithms accepts only numerical features, we make sure that the categorical features are converted into numerical features!","47fea787":"### Looks like we have missing values in both categorical and numerical features! \n\n#### Categorical features: \n1. 'Gender',\n2. 'Ever_Married',\n3. 'Graduated',\n4. 'Profession',\n5. 'Spending_Score',\n6. 'Var_1'\n\n#### Numerical features:\n1. 'Work_Experience'\n2. 'Family_Size'","668e964b":"#### using label encoder to convert the categorical features into numerical features!!!","1fede8f6":"### Selecting the best set of features ","5ef23df7":"#### Now since the age variable has a wide range of values it's better to bin the age values into different buckets and turn them into categorical variables!","037600d3":"### Splitting the train and test data","d6d487eb":"## Generating aggregated features with columns Age and profession","4855c027":"#### First let's fix the missing values in the categorical variables","8895703e":"### Before fixing the missign values in the numerical features always understand the distribution of that paticular variable!\n### For instance the Work_experience variable has a log-normal distribution! In such cases imputing the missing values with mean value wont be suitable! When the distribution is skewed, always use median values to replace the missing values!","e138ca45":"### Construction of model\n\n1. RF-baseline\n2. RF-gridsearchcv\n3. LGBM-gridsearchcv","9ca7e923":"### LGBM- Baseline","af18df47":"#### These are the following pr","9e515644":"### Random forest using gridsearchCV","bdf42144":"### Random forest baseline","a632308b":"#### Lets get the data","a669ec6b":"# No lets see if there is any multicollinearity present \n\n# Handling Multicollinearity using dendrogram\n* One way to handle multicollinear features is by performing hierarchical clustering on the Spearman rank-order correlations, picking a threshold, and keeping a single feature from each cluster.\n* We manually pick a threshold by visual inspection of the dendrogram to group our features into clusters and choose a feature from each cluster to keep. We only select those features from our dataset.\n","7631d1d7":"### As u can see the only two numeric features which possess missing values is work experience and family size! so lets fix it !","490590f3":"#### Looks like we have converted all the categorical features to numerical features and finally took care of the missing values as well!"}}