{"cell_type":{"9ab6c40d":"code","99973aaa":"code","2c57eeaa":"code","7c8ada65":"code","8d9e405c":"code","77babfe8":"code","9d5ddf21":"markdown","5a242d6e":"markdown","fdf30e11":"markdown","cc96c401":"markdown","22ca722b":"markdown","a31a6933":"markdown","91832bd8":"markdown"},"source":{"9ab6c40d":"import pandas\nimport numpy\n\ntrain = pandas.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/train.csv\")\ntest = pandas.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/test.csv\")\ntrain","99973aaa":"cont_features = [\n    \"cont0\", \"cont1\", \"cont2\", \"cont3\", \"cont4\", \"cont5\", \"cont6\", \"cont7\",\n    \"cont8\", \"cont9\", \"cont10\",\n]\ncat_features = [\n    \"cat0\", \"cat1\", \"cat2\", \"cat3\", \"cat4\", \"cat5\", \"cat6\", \"cat7\",\n    \"cat8\", \"cat9\", \"cat10\", \"cat11\", \"cat12\", \"cat13\", \"cat14\", \"cat15\",\n    \"cat16\", \"cat17\", \"cat18\"\n]\ntarget = train[\"target\"]","2c57eeaa":"from category_encoders import LeaveOneOutEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nxgb_cat_features = []\nlgb_cat_features = []\ncb_cat_features = []\nridge_cat_features = []\nsgd_cat_features = []\nhgbc_cat_features = []\n\nloo_features = []\nle_features = []\n\ndef label_encode(train_df, test_df, column):\n    le = LabelEncoder()\n    new_feature = \"{}_le\".format(column)\n    le.fit(train_df[column].unique().tolist() + test_df[column].unique().tolist())\n    train_df[new_feature] = le.transform(train_df[column])\n    test_df[new_feature] = le.transform(test_df[column])\n    return new_feature\n\ndef loo_encode(train_df, test_df, column):\n    loo = LeaveOneOutEncoder()\n    new_feature = \"{}_loo\".format(column)\n    loo.fit(train_df[column], train_df[\"target\"])\n    train_df[new_feature] = loo.transform(train_df[column])\n    test_df[new_feature] = loo.transform(test_df[column])\n    return new_feature\n\nfor feature in cat_features:\n    loo_features.append(loo_encode(train, test, feature))\n    le_features.append(label_encode(train, test, feature))\n    \nxgb_cat_features.extend(loo_features)\nlgb_cat_features.extend(le_features)\ncb_cat_features.extend(cat_features)\nridge_cat_features.extend(loo_features)\nsgd_cat_features.extend(loo_features)\nhgbc_cat_features.extend(loo_features)","7c8ada65":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nrandom_state = 2021\nn_folds = 10\nk_fold = StratifiedKFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n\nxgb_train_preds = numpy.zeros(len(train.index), )\nxgb_test_preds = numpy.zeros(len(test.index), )\nxgb_features = xgb_cat_features + cont_features\n\nlgb_train_preds = numpy.zeros(len(train.index), )\nlgb_test_preds = numpy.zeros(len(test.index), )\nlgb_features = lgb_cat_features + cont_features\n\ncb_train_preds = numpy.zeros(len(train.index), )\ncb_test_preds = numpy.zeros(len(test.index), )\ncb_features = cb_cat_features + cont_features\n\nridge_train_preds = numpy.zeros(len(train.index), )\nridge_test_preds = numpy.zeros(len(test.index), )\nridge_features = ridge_cat_features + cont_features\n\nsgd_train_preds = numpy.zeros(len(train.index), )\nsgd_test_preds = numpy.zeros(len(test.index), )\nsgd_features = sgd_cat_features + cont_features\n\nhgbc_train_preds = numpy.zeros(len(train.index), )\nhgbc_test_preds = numpy.zeros(len(test.index), )\nhgbc_features = hgbc_cat_features + cont_features\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train, target)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = target.iloc[train_index]\n    y_valid = target.iloc[test_index]\n\n    xgb_x_train = pandas.DataFrame(train[xgb_features].iloc[train_index])\n    xgb_x_valid = pandas.DataFrame(train[xgb_features].iloc[test_index])\n\n    lgb_x_train = pandas.DataFrame(train[lgb_features].iloc[train_index])\n    lgb_x_valid = pandas.DataFrame(train[lgb_features].iloc[test_index])\n\n    cb_x_train = pandas.DataFrame(train[cb_features].iloc[train_index])\n    cb_x_valid = pandas.DataFrame(train[cb_features].iloc[test_index])\n\n    ridge_x_train = pandas.DataFrame(train[ridge_features].iloc[train_index])\n    ridge_x_valid = pandas.DataFrame(train[ridge_features].iloc[test_index])\n\n    sgd_x_train = pandas.DataFrame(train[sgd_features].iloc[train_index])\n    sgd_x_valid = pandas.DataFrame(train[sgd_features].iloc[test_index])\n\n    hgbc_x_train = pandas.DataFrame(train[hgbc_features].iloc[train_index])\n    hgbc_x_valid = pandas.DataFrame(train[hgbc_features].iloc[test_index])\n\n    xgb_model = XGBClassifier(\n        seed=random_state,\n        n_estimators=10000,\n        verbosity=1,\n        eval_metric=\"auc\",\n        tree_method=\"gpu_hist\",\n        gpu_id=0,\n        alpha=7.105038963844129,\n        colsample_bytree=0.25505629740052566,\n        gamma=0.4999381950212869,\n        reg_lambda=1.7256912198205319,\n        learning_rate=0.011823142071967673,\n        max_bin=338,\n        max_depth=8,\n        min_child_weight=2.286836198630466,\n        subsample=0.618417952155855,\n    )\n    xgb_model.fit(\n        xgb_x_train,\n        y_train,\n        eval_set=[(xgb_x_valid, y_valid)], \n        verbose=0,\n        early_stopping_rounds=200\n    )\n\n    train_oof_preds = xgb_model.predict_proba(xgb_x_valid)[:,1]\n    test_oof_preds = xgb_model.predict_proba(test[xgb_features])[:,1]\n    xgb_train_preds[test_index] = train_oof_preds\n    xgb_test_preds += test_oof_preds \/ n_folds\n    print(\": XGB - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n\n    lgb_model = LGBMClassifier(\n        cat_feature=[x for x in range(len(lgb_cat_features))],\n        random_state=random_state,\n        cat_l2=25.999876242730252,\n        cat_smooth=89.2699690675538,\n        colsample_bytree=0.2557260109926193,\n        early_stopping_round=200,\n        learning_rate=0.00918685483594994,\n        max_bin=788,\n        max_depth=81,\n        metric=\"auc\",\n        min_child_samples=292,\n        min_data_per_group=177,\n        n_estimators=1600000,\n        n_jobs=-1,\n        num_leaves=171,\n        reg_alpha=0.7115353581785044,\n        reg_lambda=5.658115293998945,\n        subsample=0.9262904583735796,\n        subsample_freq=1,\n        verbose=-1,\n    )\n    lgb_model.fit(\n        lgb_x_train,\n        y_train,\n        eval_set=[(lgb_x_valid, y_valid)], \n        verbose=0,\n    )\n\n    train_oof_preds = lgb_model.predict_proba(lgb_x_valid)[:,1]\n    test_oof_preds = lgb_model.predict_proba(test[lgb_features])[:,1]\n    lgb_train_preds[test_index] = train_oof_preds\n    lgb_test_preds += test_oof_preds \/ n_folds\n    print(\": LGB - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n\n    cb_model = CatBoostClassifier(\n        verbose=0,\n        eval_metric=\"AUC\",\n        loss_function=\"Logloss\",\n        random_state=random_state,\n        num_boost_round=20000,\n        od_type=\"Iter\",\n        od_wait=200,\n        task_type=\"GPU\",\n        devices=\"0\",\n        cat_features=[x for x in range(len(cb_cat_features))],\n        bagging_temperature=1.288692494969795,\n        grow_policy=\"Depthwise\",\n        l2_leaf_reg=9.847870133539244,\n        learning_rate=0.01877982653902465,\n        max_depth=8,\n        min_data_in_leaf=1,\n        penalties_coefficient=2.1176668909602734,\n    )\n    cb_model.fit(\n        cb_x_train,\n        y_train,\n        eval_set=[(cb_x_valid, y_valid)], \n        verbose=0,\n    )\n\n    train_oof_preds = cb_model.predict_proba(cb_x_valid)[:,1]\n    test_oof_preds = cb_model.predict_proba(test[cb_features])[:,1]\n    cb_train_preds[test_index] = train_oof_preds\n    cb_test_preds += test_oof_preds \/ n_folds\n    print(\": CB - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    \n    ridge_model = CalibratedClassifierCV(\n        RidgeClassifier(random_state=random_state),\n        cv=3,\n    )\n    ridge_model.fit(\n        ridge_x_train,\n        y_train,\n    )\n\n    train_oof_preds = ridge_model.predict_proba(ridge_x_valid)[:,-1]\n    test_oof_preds = ridge_model.predict_proba(test[ridge_features])[:,-1]\n    ridge_train_preds[test_index] = train_oof_preds\n    ridge_test_preds += test_oof_preds \/ n_folds\n    print(\": Ridge - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    \n    sgd_model = CalibratedClassifierCV(\n        SGDClassifier(\n            random_state=random_state,\n            n_jobs=-1,\n            loss=\"squared_hinge\",\n        ),\n        cv=3,\n    )\n    sgd_model.fit(\n        sgd_x_train,\n        y_train,\n    )\n\n    train_oof_preds = sgd_model.predict_proba(sgd_x_valid)[:,-1]\n    test_oof_preds = sgd_model.predict_proba(test[sgd_features])[:,-1]\n    sgd_train_preds[test_index] = train_oof_preds\n    sgd_test_preds += test_oof_preds \/ n_folds\n    print(\": SGD - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    \n    hgbc_model = HistGradientBoostingClassifier(\n        l2_regularization=1.766059063693552,\n        learning_rate=0.10675193678150449,\n        max_bins=128,\n        max_depth=31,\n        max_leaf_nodes=185,\n        random_state=2021\n    )\n    hgbc_model.fit(\n        hgbc_x_train,\n        y_train,\n    )\n\n    train_oof_preds = hgbc_model.predict_proba(hgbc_x_valid)[:,-1]\n    test_oof_preds = hgbc_model.predict_proba(test[hgbc_features])[:,-1]\n    hgbc_train_preds[test_index] = train_oof_preds\n    hgbc_test_preds += test_oof_preds \/ n_folds\n    print(\": HGBC - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    print(\"\")\n    \nprint(\"--> Overall metrics\")\nprint(\": XGB - ROC AUC Score = {}\".format(roc_auc_score(target, xgb_train_preds, average=\"micro\")))\nprint(\": LGB - ROC AUC Score = {}\".format(roc_auc_score(target, lgb_train_preds, average=\"micro\")))\nprint(\": CB - ROC AUC Score = {}\".format(roc_auc_score(target, cb_train_preds, average=\"micro\")))\nprint(\": Ridge - ROC AUC Score = {}\".format(roc_auc_score(target, ridge_train_preds, average=\"micro\")))\nprint(\": SGD - ROC AUC Score = {}\".format(roc_auc_score(target, sgd_train_preds, average=\"micro\")))\nprint(\": HGBC - ROC AUC Score = {}\".format(roc_auc_score(target, hgbc_train_preds, average=\"micro\")))","8d9e405c":"from scipy.special import expit\nfrom sklearn.calibration import CalibratedClassifierCV\n\nrandom_state = 2021\nn_folds = 10\nk_fold = StratifiedKFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n\nl1_train = pandas.DataFrame(data={\n    \"xgb\": xgb_train_preds.tolist(),\n    \"lgb\": lgb_train_preds.tolist(),\n    \"cb\": cb_train_preds.tolist(),\n    \"ridge\": ridge_train_preds.tolist(),\n    \"sgd\": sgd_train_preds.tolist(),\n    \"hgbc\": hgbc_train_preds.tolist(),\n    \"target\": target.tolist()\n})\nl1_test = pandas.DataFrame(data={\n    \"xgb\": xgb_test_preds.tolist(),\n    \"lgb\": lgb_test_preds.tolist(),\n    \"cb\": cb_test_preds.tolist(),\n    \"sgd\": sgd_test_preds.tolist(),\n    \"ridge\": ridge_test_preds.tolist(),    \n    \"hgbc\": hgbc_test_preds.tolist(),\n})\n\ntrain_preds = numpy.zeros(len(l1_train.index), )\ntest_preds = numpy.zeros(len(l1_test.index), )\nfeatures = [\"xgb\", \"lgb\", \"cb\", \"ridge\", \"sgd\", \"hgbc\"]\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(l1_train, target)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = target.iloc[train_index]\n    y_valid = target.iloc[test_index]\n\n    x_train = pandas.DataFrame(l1_train[features].iloc[train_index])\n    x_valid = pandas.DataFrame(l1_train[features].iloc[test_index])\n    \n    model = CalibratedClassifierCV(\n        RidgeClassifier(random_state=random_state), \n        cv=3\n    )\n    model.fit(\n        x_train,\n        y_train,\n    )\n\n    train_oof_preds = model.predict_proba(x_valid)[:,-1]\n    test_oof_preds = model.predict_proba(l1_test[features])[:,-1]\n    train_preds[test_index] = train_oof_preds\n    test_preds += test_oof_preds \/ n_folds\n    print(\": ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    print(\"\")\n    \nprint(\"--> Overall metrics\")\nprint(\": ROC AUC Score = {}\".format(roc_auc_score(target, train_preds, average=\"micro\")))","77babfe8":"submission = pandas.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/sample_submission.csv\")\nsubmission[\"target\"] = test_preds.tolist()\nsubmission.to_csv(\"submission.csv\", index=False)","9d5ddf21":"# Define Features","5a242d6e":"# Introduction\n\nThis model demonstrates some of the concepts that I discussed in the [4th place entry](https:\/\/www.kaggle.com\/c\/tabular-playground-series-feb-2021\/discussion\/222791) in the February 2021 Tabular Playground series. In particular, this kernel stacks XGBoost, LightGBM, CatBoost, Ridge, SGD, and HistGradientBoosting classification models. It uses 10-fold cross validation to build each model, and makes both test and training predictions out-of-fold. Those results are then fed into the level 2 Ridge model, where 10-fold cross validation is used again to make out-of-fold predictions for the submission result. Note that no feature engineering has been performed at all outside of categorical encodings. Note as well that first glances at EDAs suggest a lot of possibilities for categorical data. If you like the model, please consider upvoting!","fdf30e11":"# Generate Level 1 Models","cc96c401":"# Load Data","22ca722b":"# Encode Features","a31a6933":"# Build Level 2 Model","91832bd8":"# Save Predictions"}}