{"cell_type":{"1fc8c520":"code","c9ae91f2":"code","262f482c":"code","5a1f393a":"code","99e14679":"code","fbbe2d13":"code","fe1fa837":"code","5c6aa331":"code","16d1c7ee":"code","b1dd56b3":"code","8a06db9a":"code","3accd3bc":"code","e496acf1":"code","528d1e2a":"code","cffe2020":"code","4421b804":"code","43411b0b":"code","d42d0795":"code","6dd57b74":"code","b293f809":"code","328d77b0":"code","dfaebe64":"code","1de14322":"code","3a428a5c":"code","2ef4e6eb":"code","65db2eab":"code","53cd0747":"code","6791d3b0":"code","e74239eb":"code","3b16729d":"code","dd7d380b":"code","36d52ba3":"code","3482975c":"code","358807b1":"code","5b9a6aab":"code","7c37f8c7":"code","eea20a84":"code","2a3fb92e":"code","a4c6f674":"code","d8eae925":"code","d9fbe74c":"code","e7ce8291":"code","db07f707":"code","54177068":"code","321b1d05":"code","4835bc14":"code","18961abf":"markdown","ebd81751":"markdown","36ef6549":"markdown"},"source":{"1fc8c520":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt###\n                               ### Visualisation tools\nimport seaborn as sns          ###\n\nfrom sklearn.linear_model import LinearRegression,LogisticRegression,SGDRegressor , Ridge,Lasso\nfrom sklearn.model_selection import train_test_split,GridSearchCV,KFold,cross_val_score\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nfrom sklearn.metrics import accuracy_score,precision_score,confusion_matrix\nfrom sklearn.metrics import classification_report,roc_curve,roc_auc_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c9ae91f2":"train = pd.read_csv('\/kaggle\/input\/fraud-detection\/fraudTrain.csv')\ntest = pd.read_csv('\/kaggle\/input\/fraud-detection\/fraudTest.csv')","262f482c":"#Concatenate the splitted data\ndf = pd.concat([train,test],ignore_index=True)\ndf.drop('Unnamed: 0',axis=1,inplace=True)","5a1f393a":"df.head()","99e14679":"#Shape of the data (rows,columns)\ndf.shape","fbbe2d13":"#Finding dtypes and other basic info about the features\ndf.info()","fe1fa837":"#dividing data into categorical and numerical\ndf_cat = df.select_dtypes(include = 'object')\ndf_num = df.select_dtypes(exclude = 'object')\nle = LabelEncoder()","5c6aa331":"nan_df = pd.DataFrame(data = (df.isnull().sum()\/len(df))*100,columns = ['% of missing values'])","16d1c7ee":"nan_df","b1dd56b3":"#there is no null values","8a06db9a":"df['gender_le']  = le.fit_transform(df['gender'])\n","3accd3bc":"# Seperating  numerical from nominal\n# cutting off some data to avoid heavy cpu usage\ndf2 = df.select_dtypes(exclude = 'object')\ndf2 = df2.loc[:149999]\n","e496acf1":"df2.head()","528d1e2a":"# dropping the feature which is not useful for data analysis\ndf2 = df2.drop(['cc_num'],axis=1)\n# seperating target and independent features\nx = df2.drop('is_fraud',axis=1)\ny = df2['is_fraud']","cffe2020":"xtrain,xtest,ytrain,ytest = train_test_split(x,y,train_size = 0.7 , random_state = 10)","4421b804":"df_num = df_num.drop(['cc_num'],axis=1)","43411b0b":"df_num = df_num.drop('is_fraud',axis=1)","d42d0795":"# Scaling the data\nss = StandardScaler()\nxtrain[df_num.columns] = ss.fit_transform(xtrain[df_num.columns])\nxtest[df_num.columns] = ss.fit_transform(xtest[df_num.columns])\n","6dd57b74":"#Building a Logistic Regression Model\n\nlr = LogisticRegression()\nmodel_v1 = lr.fit(xtrain,ytrain)","b293f809":"#  predict and confusion matrix\n\nypred = model_v1.predict(xtest)\ncm  = confusion_matrix(ytest,ypred)\nsns.heatmap(cm,annot=True)\nplt.show()","328d77b0":"#classification report\nprint(classification_report(ytest,ypred))","dfaebe64":"#KNN model\nknn = KNeighborsClassifier()\nmodel_v2 = knn.fit(xtrain,ytrain)\nypred1 = model_v2.predict(xtest)","1de14322":"cm_knn = confusion_matrix(ytest,ypred1)\nsns.heatmap(cm_knn,annot=True)","3a428a5c":"print(classification_report(ytest,ypred))","2ef4e6eb":"#Naive Bayes model\nnb =GaussianNB()\nmodel_v3 = nb.fit(xtrain,ytrain)\n","65db2eab":"ypred = model_v3.predict(xtest)\ncm = confusion_matrix(ytest,ypred)\nsns.heatmap(cm,annot=True)","53cd0747":"print(classification_report(ytest,ypred))","6791d3b0":"#### ROC curve\n## Logistic Regression Model\nprob = model_v1.predict_proba(xtest)[:,1]\nfpr,tpr,threshold = roc_curve(ytest,prob)\n#Plotting Roc Curve\nplt.plot(fpr,tpr)\nplt.plot([[0,0],[1,1]],color='red',linestyle = '-')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC Curve')\nplt.show()","e74239eb":"## KNN Classifier Model\nprob1 = model_v2.predict_proba(xtest)[:,1]\nfpr,tpr,threshold = roc_curve(ytest,prob1)\n#Plotting Roc Curve\nplt.plot(fpr,tpr)\nplt.plot([[0,0],[1,1]],color='red',linestyle = '-')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC Curve')\nplt.show()","3b16729d":"## Naive Bayes Model\nprob2 = model_v3.predict_proba(xtest)[:,1]\nfpr,tpr,threshold = roc_curve(ytest,prob2)\n#Plotting Roc Curve\nplt.plot(fpr,tpr)\nplt.plot([[0,0],[1,1]],color='red',linestyle = '-[]')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC Curve')\nplt.show()","dd7d380b":"# Grid search for KNN\nparams = ({'n_neighbors' : np.arange(1,30,2),\n          \"metric\" : ['minkowski','euclidean','chebyshev','manhattan']})\nknn = KNeighborsClassifier()\ngrid = GridSearchCV(estimator=knn , param_grid = params , scoring = 'f1_weighted' , cv = 5 )\ngrd = grid.fit(xtrain,ytrain)","36d52ba3":"grd_df = pd.DataFrame(grd.cv_results_)\n","3482975c":"grd_df[grd_df['rank_test_score']==1]","358807b1":"grd_df[grd_df['rank_test_score']==5]","5b9a6aab":"\nknn = KNeighborsClassifier(n_neighbors=5,metric='manhattan')\nmodel_v4 = knn.fit(xtrain,ytrain)","7c37f8c7":"ypred = model_v4.predict(xtest)\nprob = model_v4.predict_proba(xtest)[:,1]\n","eea20a84":"cm = confusion_matrix(ytest,ypred)\nsns.heatmap(cm,annot=True)","2a3fb92e":"print(classification_report(ytest,ypred))","a4c6f674":"k = KFold(n_splits=5,shuffle=True,random_state=10) \nscores = cross_val_score(knn,xtrain,ytrain,scoring='f1_weighted',cv=5) \n\nprint(scores)\nprint('Bias error:',(1- np.mean(scores))*100) \nprint(\"Variance error:\", (np.std(scores)\/np.mean(scores))*100) ","d8eae925":"fpr,tpr,threshold = roc_curve(ytest,prob)\n","d9fbe74c":"#Plotting Roc Curve\nplt.plot(fpr,tpr)\nplt.plot([[0,0],[1,1]],color='red',linestyle = '--')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC Curve')\nplt.show()","e7ce8291":"ypred = model_v4.predict(xtest)","db07f707":"prediction = pd.DataFrame()\nprediction['Serial No.'] = xtest.index\nprediction['is_fraud_pred'] = ypred","54177068":"prediction[prediction['is_fraud_pred']==1]","321b1d05":"real = xtest.join([ytest])","4835bc14":"real[real['is_fraud']==1]","18961abf":"**Manhattan distance with 1 neighbor is the best parameters to choose. But the number of neighbors is low for upto rank 4 , so we choose rank 5**","ebd81751":"# Prediction","36ef6549":"# <h>Conclusion<\/h>\n1. KNN Classifier gave the best ROC curve followed by Logistic Regression then Naive Bayes\n    1. Out of the grid search we done here , best parameter is manhattan with 1 neighbor\n    2. But 1 neighbor is too low to consider as it can lead to overfitting\n    3. So I choose the 5th ranked one as it has 5 neighbors\n2. But when it comes to scores , I consider f1_weighted average score as the best one ; in that regard all 3 models gave 0.99 in classification report\n3. Since KNN Classifier had edge in ROC curve , I am concluding it is as the most reliable model to give a prediction out of the three models"}}