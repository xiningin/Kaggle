{"cell_type":{"84b81ca0":"code","d26da0f5":"code","28339080":"code","aae65679":"code","58e9ac30":"code","bdd922d5":"code","9baba087":"code","82f16984":"code","f193fbf6":"code","7ed470d0":"code","616a872b":"code","48a7c092":"code","370c8ca4":"code","d777fac5":"code","8f345b38":"code","a662c3a5":"code","75f2787a":"code","c2f5a2ca":"code","e33834c4":"markdown","b6b25c29":"markdown"},"source":{"84b81ca0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nfrom keras.applications.vgg19 import VGG19\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense,Flatten\nfrom keras.datasets import cifar10\nimport cv2\nfrom keras import backend as K\nimport matplotlib.pyplot as plt\nfrom keras.optimizers import SGD\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d26da0f5":"from os import listdir, makedirs\nfrom os.path import join, exists, expanduser\n\ncache_dir = expanduser(join('~', '.keras'))\nif not exists(cache_dir):\n    makedirs(cache_dir)\ndatasets_dir = join(cache_dir, 'datasets') # \/cifar-10-batches-py\nif not exists(datasets_dir):\n    makedirs(datasets_dir)\n\n\n!cp ..\/input\/cifar-10-python.tar.gz ~\/.keras\/datasets\/\n!ln -s  ~\/.keras\/datasets\/cifar-10-python.tar.gz ~\/.keras\/datasets\/cifar-10-batches-py.tar.gz\n!tar xzvf ~\/.keras\/datasets\/cifar-10-python.tar.gz -C ~\/.keras\/datasets\/\n# this part means\n#Inclusion of data set in working environment","28339080":"(x_train,y_train),(x_test,y_test) = cifar10.load_data() # load the data ","aae65679":"numberOfClass = 2000\ny_train = to_categorical(y_train,numberOfClass)# y train and y test transform to categorical type\ny_test = to_categorical(y_test,numberOfClass)\ninput_shape = x_train.shape[1:]","58e9ac30":"#Visualize \nplt.imshow(x_train[3119].astype(np.uint8)) #3119 is random number \nplt.axis(\"off\") # close the axis\nplt.show()\n#this img shape is 32,32,3","bdd922d5":"#increase dimension\ndef resize_img(img):\n    numberOfImage = img.shape[0]\n    new_array = np.zeros((numberOfImage,48,48,3))# first img shape is 32,32,3 but we want shape 48,48,3 \n    for i in range(numberOfImage):\n        new_array[i] = cv2.resize(img[i,:,:,:],(48,48))\n    return new_array","9baba087":"x_train = resize_img(x_train)\nx_test = resize_img(x_test)","82f16984":"plt.figure()\nplt.imshow(x_train[3119].astype(np.uint8)) #3119 is random number \nplt.axis(\"off\") # close the axis\nplt.show()\n# And this img shape is 48,48,3","f193fbf6":"#VGG19\nvgg19 = VGG19(include_top = False, weights = \"imagenet\",input_shape = (48,48,3))","7ed470d0":"print(vgg19.summary())\n# Son sat\u0131r MaxPooling2D bundan sonra fuly con layers flatten dense olmal\u0131yd\u0131 .include_top = False bunlar\u0131 \u00e7\u0131kart\u0131r.\n#Transfer learningde genelde bunlar  \u00e7\u0131kart\u0131l\u0131r\n\n# Last line MaxPooling2D should now be fuly con layers flatten dense .include_top = False removes them.\n#Transfer learning is usually removed","616a872b":"vgg19_layer_list =vgg19.layers\nprint(vgg19_layer_list)","48a7c092":"model = Sequential()\nfor layer in vgg19_layer_list:\n    model.add(layer)\nprint(model.summary())","370c8ca4":"#transfer learning\nfor layer in model.layers:\n    layer.trainable = False","d777fac5":"#fuly con layers\nmodel.add(Flatten())\nmodel.add(Dense(1024))\nmodel.add(Dense(numberOfClass, activation=\"softmax\"))\nprint(model.summary())","8f345b38":"#compile part\nmodel.compile( loss = \"categorical_crossentropy\", optimizer =\"rmsprop\", metrics =[\"accuracy\"])","a662c3a5":"history = model.fit(x_train ,y_train, validation_split= 0.2 , epochs = 15, batch_size = 1000)","75f2787a":"# model save\nmodel.save_weights(\"example.h5\")","c2f5a2ca":"#visualize\nplt.plot(history.history[\"loss\"], label = \"train loss\")\nplt.plot(history.history[\"val_loss\"], label = \"val loss\")\nplt.legend()\nplt.show()\nplt.figure()\nplt.plot(history.history[\"acc\"], label = \"train acc\")\nplt.plot(history.history[\"val_acc\"], label = \"val acc\")\nplt.legend()\nplt.show()","e33834c4":"biz bunlar\u0131 kendimize transfer etmeliyiz. ve sonra  flatten dense fuly con layers ekleyece\u011fiz\n\nwe must transfer them to ourselves. and then we'll add full connected flatten dense fuly con layers","b6b25c29":"0.60 acc biliyorum d\u00fc\u015f\u00fck ama bunun bir \u00f6rnek oldu\u011funu ve transfer learning i\u00e7in elimden geldi\u011fince a\u00e7\u0131klamaya \u00e7al\u0131\u015ft\u0131\u011f\u0131m\u0131 bilmenizi isterim. Az\u0131c\u0131k bile yarar\u0131 olduysa uptove ederseniz \u00e7ok m\u00fcte\u015fekkir kalaca\u011f\u0131m .Zaman ay\u0131rd\u0131\u011f\u0131n\u0131z i\u00e7in te\u015fekk\u00fcrler.\nI know 0.60 acc low .But this kernel vgg19 and transfer learning example.I tried to explain as best as I could. \nIf it's even helped, you will make me happy.\nthanks for your time!"}}