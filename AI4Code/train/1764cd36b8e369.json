{"cell_type":{"92bb55e3":"code","ba3868e4":"code","836193dd":"code","f6162c10":"code","155f99df":"code","6a1a3740":"code","a353ed89":"code","0b3e0c6f":"code","beb76d7c":"code","c0bca293":"code","6341851b":"code","395beb8b":"code","cdb65b57":"code","b205fcad":"code","9457799a":"code","82d26484":"code","97f27d23":"code","a8bf5d84":"code","1f3b6441":"code","c2af3f3b":"code","8cde15ca":"code","5872c1f0":"code","944538de":"code","ab74f570":"code","b1b85937":"code","220135c7":"code","9d0c4053":"code","50d8a96e":"code","7d48c6ec":"code","e6d76812":"code","2a1b6bc5":"code","deda2918":"markdown","050a49c6":"markdown","3c1582ae":"markdown","60f4b253":"markdown","0bc7ac91":"markdown","e3bbdbe2":"markdown","493c3059":"markdown","1b7c19f0":"markdown","0e04235b":"markdown","943c8f3c":"markdown","14e8ca9a":"markdown","d42a4fc7":"markdown","434395eb":"markdown","ae1eb30d":"markdown","cf9a7238":"markdown","c5c24035":"markdown","dee891d3":"markdown","957f057f":"markdown","ecc2d6a2":"markdown","1da3ac2e":"markdown","92e484ac":"markdown","be5b4044":"markdown","0cf73b28":"markdown","72c50885":"markdown","db58b8ba":"markdown"},"source":{"92bb55e3":"#import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom fancyimpute import KNN\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDRegressor \nfrom sklearn.linear_model import Ridge \nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nprint(\"libraries loaded successfully\")","ba3868e4":"#load data\ndata_train  = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\nprint(\"Data loaded successfully\")","836193dd":"#exploar data\nprint(\"data shape : \",data_train.shape)\ndata_train.describe()","f6162c10":"#get total count of data including missing data\ntotal = data_train.isnull().sum().sort_values(ascending=False)\n\n#get percent of missing data relevant to all data\npercent = (data_train.isnull().sum()\/data_train.isnull().count()).sort_values(ascending=False)\n\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)","155f99df":"#drop PoolQC, MiscFeature, Alley, Fence columns\ndata_train = data_train.drop(['PoolQC','MiscFeature','Alley','Fence'], axis=1)\n\n#after drop thes columns data shape will be (1460, 77) insted of (1460, 81)\nprint(\"data shape : \",data_train.shape)","6a1a3740":"#get continuous features\ncolnames_numerics_only = data_train.iloc[:,1:-1].select_dtypes(include=np.number).columns.tolist()\nprint('numerical features')\nprint(colnames_numerics_only)\n\nprint(\"----------------------------------------\")\n\nprint(\"number of numerics features = \",len(colnames_numerics_only))","a353ed89":"#impute missing values of continuous features using KNN\ndata_train[colnames_numerics_only] = KNN(k=5).fit_transform(data_train[colnames_numerics_only])\nprint('missing values of continuous features imputed successfully')","0b3e0c6f":"#get categorical features\ncolnames_categorical_only = data_train.iloc[:,1:-1].select_dtypes(include='object').columns.tolist()\nprint('categorical features')\nprint(colnames_categorical_only)\n\nprint(\"----------------------------------------\")\n\nprint(\"number of categorical features = \",len(colnames_categorical_only))","beb76d7c":"for categorical_col in colnames_categorical_only:\n    most_frequent = data_train[categorical_col].value_counts().idxmax()\n    hasCol       = 'Has'+categorical_col\n    \n    #create new col \n    data_train[hasCol] = pd.Series(len(data_train[categorical_col]), index=data_train.index)\n    \n    #set new col = 1\n    data_train[hasCol] = 1\n    \n    #set new col = 0 if data_train[categorical_col] not empty\n    data_train.loc[data_train[categorical_col].isnull(),hasCol] = 0\n    \n    #set data_train[categorical_col] = most_frequent if new col = 0\n    #if location of new col = 0 this mean that data_train[categorical_col] in this location is empty\n    data_train.loc[data_train[hasCol] == 0,categorical_col] = most_frequent\n    \n    #drop new col\n    data_train = data_train.drop(hasCol, axis=1)\n    \nprint('missing values of categorical features imputed successfully')    ","c0bca293":"#print max count number of null values\nprint('Number of missing values = ',data_train.isnull().sum().max())","6341851b":"#box plot\ncols = ['MSSubClass','LotFrontage','LotArea','OverallQual']\nfor col in cols:\n    plt.figure()\n    ax = sns.boxplot(x=data_train[col])","395beb8b":"Q1 = data_train[colnames_numerics_only].quantile(0.25)\nQ3 = data_train[colnames_numerics_only].quantile(0.75)\nIQR = Q3 - Q1\n\nhasOutlier = (data_train[colnames_numerics_only] < (Q1 - 1.5 * IQR)) | (data_train[colnames_numerics_only] > (Q3 + 1.5 * IQR))\nhasOutlier","cdb65b57":"num_data = data_train[colnames_numerics_only]\n\nfor numeric_col in colnames_numerics_only: \n    data_train = data_train.drop(data_train.loc[hasOutlier[numeric_col]].index)","b205fcad":"#after drop thes raws which contain outliers data raws will be less than 1460 raw \nprint(\"data raws number : \",data_train.shape[0])","9457799a":"data_train = data_train.drop('Id', axis=1)\nprint('Id column deleted successfully')","82d26484":"#correlation matrix\ncorrmat = data_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True)","97f27d23":"#scatterplot\nsns.set()\ncols = ['LotFrontage','LotArea','OverallQual','YearBuilt','YearRemodAdd','MasVnrArea','TotalBsmtSF','1stFlrSF',\n        'GrLivArea','FullBath','TotRmsAbvGrd','GarageYrBlt','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF']\n\ngroup_1 = cols[0:8]\ngroup_1.insert(0, \"SalePrice\")\n\n#draw scatter plot of first group\nsns.pairplot(data_train[group_1], size = 2)\nplt.show();","a8bf5d84":"group_2 = cols[8:]\ngroup_2.insert(0, \"SalePrice\")\n\n#draw scatter plot of first group\nsns.pairplot(data_train[group_2], size = 2)\nplt.show();","1f3b6441":"# all numerical features in our data\nallNumericalFeatures = colnames_numerics_only\n\n# numerical features which we use it in our model\nselectedNumericalFeatures = cols\n\n# numerical features that we will drop it\ndeletedFeatures =  list(set(allNumericalFeatures) - set(selectedNumericalFeatures))\n\nprint(\"data shape before delete features = \",data_train.shape)\n\n# delete unwanted features\ndata_train = data_train.drop(deletedFeatures, axis=1)\n\nprint(\"data shape after delete features = \",data_train.shape)\n\nprint(\"unwanted features deleted successfully\")","c2af3f3b":"#convert categorical variable into lables\nlabelEncoder = LabelEncoder()\n\nfor categorical_col in colnames_categorical_only:\n    data_train[categorical_col] =  labelEncoder.fit_transform(data_train[categorical_col])\n    \nprint(\"categorical columns converted successfully\")","8cde15ca":"print(colnames_categorical_only)","5872c1f0":"#data scaling\nscaler = StandardScaler(copy=True, with_mean=True, with_std=True)\ndata_train[selectedNumericalFeatures] = scaler.fit_transform(data_train[selectedNumericalFeatures])\n\nprint(\"data scaling successfully\")\ndata_train.describe()","944538de":"X = data_train.drop('SalePrice', axis=1)\ny = data_train['SalePrice']\n\nX_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.40, random_state=55, shuffle =True)\nprint('data splitting successfully')","ab74f570":"#model bulding\nSGDRRegModel = SGDRegressor(random_state=55,loss = 'squared_loss')\nSelectedParameters = {\n                      'alpha':[0.1,0.5,0.01,0.05,0.001,0.005],\n                      'max_iter':[100,500,1000,5000,10000],\n                      'tol':[0.0001,0.00001,0.000001],\n                      'penalty':['l1','l2','none','elasticnet']\n                      }\n\nGridSearchModel = GridSearchCV(SGDRRegModel,SelectedParameters, cv = 5,return_train_score=True)\nGridSearchModel.fit(X_train,y_train)\n\nSGDRRegModel = GridSearchModel.best_estimator_\nSGDRRegModel.fit(X_train,y_train)\n\nprint(\"stochastic gradient model run successfully\")","b1b85937":"RidgeRegModel = Ridge(random_state= 55, copy_X=True)\nSelectedParameters = {\n                      'alpha':[0.1,0.5,0.01,0.05,0.001,0.005],\n                      'normalize':[True,False],\n                      'max_iter':[100,500,1000,5000,10000],\n                      'tol':[0.0001,0.00001,0.000001],\n                      'solver':['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg']\n                      }\n\nGridSearchModel = GridSearchCV(RidgeRegModel,SelectedParameters, cv = 5,return_train_score=True)\nGridSearchModel.fit(X_train,y_train)\n\nRidgeRegModel = GridSearchModel.best_estimator_\nRidgeRegModel.fit(X_train,y_train)\n\nprint(\"Ridge model run successfully\")","220135c7":"LassoRegModel = Lasso(random_state= 55 ,copy_X=True)\nSelectedParameters = {\n                      'alpha':[0.1,0.5,0.01,0.05,0.001,0.005],\n                      'normalize':[True,False],\n                      'tol':[0.0001,0.00001,0.000001],\n                      }\n\nGridSearchModel = GridSearchCV(LassoRegModel,SelectedParameters, cv = 5,return_train_score=True)\nGridSearchModel.fit(X_train,y_train)\n\nLassoRegModel = GridSearchModel.best_estimator_\nLassoRegModel.fit(X_train,y_train)\n\nprint(\"lasso model run successfully\")","9d0c4053":"linearRegModel = LinearRegression(copy_X=True)\nlinearRegModel.fit(X_train,y_train)\nprint(\"Linear regression model run successfully\")","50d8a96e":"decisionTreeModel = DecisionTreeRegressor(random_state=55)\n\nSelectedParameters = {\n                      'criterion': ['mse','friedman_mse','mae'] ,\n                      'max_depth': [None,2,3,4,5,6,7,8,9,10],\n                      'splitter' : ['best','random'],\n                      'min_samples_split':[2,3,4,5,6,7,8,9,10],\n                      }\n\nGridSearchModel = GridSearchCV(decisionTreeModel,SelectedParameters, cv = 5,return_train_score=True)\nGridSearchModel.fit(X_train,y_train)\n\ndecisionTreeModel = GridSearchModel.best_estimator_\ndecisionTreeModel.fit(X_train,y_train)\n\nprint(\"decision Tree Regressor model run successfully\")","7d48c6ec":"XGBRModel = XGBRegressor(n_jobs = 4)\n\nSelectedParameters = {\n                      'n_estimators': [100,1000,10000] ,\n                      'learning_rate': [0.1,0.5,0.01,0.05],\n                      }\n\nGridSearchModel = GridSearchCV(XGBRModel,SelectedParameters, cv = 5,return_train_score=True)\nGridSearchModel.fit(X_train,y_train)\n\nXGBRModel = GridSearchModel.best_estimator_\nXGBRModel.fit(X_train,y_train)\n\nprint(\"Xgboost Regressor model run successfully\")","e6d76812":"#evaluation Details\nmodels = [SGDRRegModel, RidgeRegModel, LassoRegModel, linearRegModel, decisionTreeModel,XGBRModel]\n\nfor model in models:\n    print(type(model).__name__,' Train Score is   : ' ,model.score(X_train, y_train))\n    print(type(model).__name__,' Test Score is    : ' ,model.score(X_test, y_test))\n    print('--------------------------------------------------------------------------')","2a1b6bc5":"#predict\nfor model in models:\n    print(type(model).__name__,\" error metrics\")\n    print('---------------------------------------------------------')\n    y_pred = model.predict(X_test)\n\n    MAE = mean_absolute_error(y_test,y_pred)\n    print(\"mean absolute error = \",MAE)\n\n    MSE = mean_squared_error(y_test,y_pred)\n    print(\"mean squared error = \",MSE) \n\n    RMSE = np.sqrt(mean_squared_error(y_test,y_pred))\n    print(\"root mean squared error = \",RMSE) \n    print()","deda2918":"Frome above table \u261d we noticed that some columns have missing values as **LotFrontage** because there count less than 1460 and **MasVnrArea**.<br>\nAnd columns have different range of value as **MSSubClass** and **LotFrontage** approximately have same range of values but other columns like **LotArea** has different range of values.<br>\n\nThis is only **Big picture** of our data. Now let's deal with **missing data**\n\n## 1.2 Missing data \ud83d\udc47","050a49c6":"At this point we dealing with **correlation matrix** and **Scatter plot** to choose best features for our model. But these methods don't include any feedback to know if our choices true or not all of them depend only on native statistical techniques.<br>\n\nSo we need to some method tell us **Were we successful in our selection of features ?**\n\nFor this we will use **exhaustive feature selection algorithm** or **brute force features selector algorithm**<br>It work as **Grid search** work to choose best model parameter. This exhaustive feature selection algorithm is a wrapper approach for brute-force evaluation of feature subsets; the best subset is selected by optimizing a specified performance metric given an arbitrary regressor or classifier.<br>\n\n**For Exampel,**\n\nIf we have the following features **0,1,2** <code>(if min_features=1 and max_features=3)<\/code> the compinations will be <br>\n\n1. {0}\n2. {1}\n3. {2}\n4. {0, 1} \n5. {0, 2} \n6. {1, 2}\n7. {0, 1, 2}\n\nThe main disadvantage of this algorithm is **Time consuming** because large number of combination. For this we will not use it in this notebook Because large number of features. Now we will drob numerical features which we don't use them in our model.","3c1582ae":"Now we will claen our data from outliers <span style='font-size:25px;font-weight:bold;'>\ud83d\uddd1<span>","60f4b253":"# 5. Model evaluation\nNow we will evaluate our model using following metrics \ud83d\udc47\n1. Model score\n2. Mean absolute error (MAE)\n3. Mean squared error (MSE)\n4. Root mean squared error (RMSE)\n\nlet's go \ud83d\ude0a","0bc7ac91":"## 4. Linear Regression","e3bbdbe2":"We can choose beast feature if we well know a correlation between each feature and target variable (SalePrice), To do this we can use \ud83d\udc47 <br>\n* Correlation matrix\n* Scatter Plot\n\n### Correlation matrix \ud83d\udc47 \n\nA correlation matrix is a table showing correlation coefficients between variables. Each cell has a value between <code>1 to -1<\/code>\n<br>\nif cell value = <code>1<\/code> this mean high positive correlation else if cell value = <code>-1<\/code> this mean high negative correlation","493c3059":"Now we will using **KNN (K Nearest Neighbors)** with number of neighbors = 5 to impute missing values.<br>\n\nThe distance metric varies according to the type of data:<br>\n1. **Continuous Data:** The commonly used distance metrics for continuous data are Euclidean, Manhattan and Cosine.\n2. **Categorical Data:** Hamming distance is generally used in this case.","1b7c19f0":"Frome above table \u261d we noticed that some columns have many missing values as **PoolQC** 0.99 of this column is missing<br> And **MiscFeature** 0.96 of this column is missing and other column have large count of missing values like **Alley** and **Fence**.<br>\n\n### How to Handle Missing Data ? \ud83d\ude44\nOne of the most common problems I have faced in Data Cleaning\/Exploratory Analysis is handling the missing values.<br>\nThis is a picture that give us a guide to deal with missingg data \ud83d\udc47 <br>\n<img src='https:\/\/miro.medium.com\/max\/1528\/1*_RA3mCS30Pr0vUxbp25Yxw.png' width=\"550px\" style='float:left;'>\n<div style='clear:both'><\/div>\n<br>\nAs we see in above picture there are many ways to deal with Missing Data. In this **Kernel** i will use two of them on at each branch.<br><br>\nIn **Deletion** I will use **Deleting Columns** technique.<br>\n\nSometimes we can drop variables if the data is missing for more than 60% observations because these variables are useless.\n\nIn **Imputation** because our problem is a general problem I will use predictive models that impute the missing data.\nAs we know our data contain both **Categorical** and **Continuous** features I will use a **KNN (K Nearest Neighbors)** to impute data \nbecuse it can work with both features types.<br>\n\nI will Delete the following Columns **PoolQC, MiscFeature, Alley, Fence** because missing data in these columns more than **60%** observations. ","0e04235b":"# 3. Data Scaling\nNow we will scale our data using standardization. \n\nLet's go \ud83d\ude0a","943c8f3c":"<br>\n**From above results we will use XGBRegressor for submission task**\n\n# 6. References \nThese some reference i used it in this kernel \ud83d\udc47\n\n1. [Ways to Detect and Remove the Outliers](https:\/\/towardsdatascience.com\/ways-to-detect-and-remove-the-outliers-404d16608dba)\n2. [Handling Missing Values in Machine Learning: Part 2](https:\/\/towardsdatascience.com\/handling-missing-values-in-machine-learning-part-2-222154b4b58e)\n3. [How to Handle Missing Data](https:\/\/towardsdatascience.com\/how-to-handle-missing-data-8646b18db0d4)\n\n<p style='font-size:25px;font-weight:bold'>Please If you find this kernel useful, upvote it to help others see it \ud83d\ude0a<\/p>","14e8ca9a":"## 3. Lasso","d42a4fc7":"**Congratulations \ud83d\udc4f now we don't have any missing values in our data**<br>\n\nNow let's talk about **anomaly detection** or in other word **outliers**\n\n## 1.3 Outliers \ud83d\udc47\nIn statistics, an outlier is an observation point that is distant from other observations.<br>\n<img src='https:\/\/miro.medium.com\/max\/869\/1*N_C1Mhiz8hzZkKrUfjez3A.jpeg' width='300px' style='float:left;'>\n<div style='clear:both'><\/div>\n<br>\nIn above \u261d image we noticed that all numbers in the 30\u2019s range except number 3.<br>\n\nBut **why we must discover outliers?** This below \ud83d\udc47 image show outliers effect on predictive line.<br>\n<img src='https:\/\/i.imgur.com\/1YBK3E1.png' width='300px' style='float:left;'>\n<div style='clear:both'><\/div>\n<br>\nAs we see <span style='color:blue'>blue<\/span> line represent predictive line in case of our data don't include outliers. But <span style='color:orange'>orange<\/span> line in case of existence of outliers. **All we see the difference** \ud83d\ude0e\n\n### How to discover outliers ? \ud83d\udc40\n\nWe can dicover outliers by **visualization tools** or **numirical methods**<br>\n\n**Discover outliers with visualization tools** \ud83d\udcc8<br>\n\nWe can using visualization tools to detect outliers as\n<ul>\n    <li>**Box plot**<\/li>\n    <li>**Scatter plot**<\/li>\n<\/ul>\n\n**Discover outliers with numerical methods**<br>\n\nWe can using numerical methods to detect outliers as\n<ul>\n    <li>**IQR score**<\/li>\n    <li>**Z-Score**<\/li>\n<\/ul>\n\nIn this kernel I will use **Box plot** and **IQR score** to detect outliers.\n#### Box plot \ud83d\udc47\n**Wikipedia Definition,**\n> **In descriptive statistics,** a box plot is a method for graphically depicting groups of numerical data through their quartiles.<br>**Outliers** may be plotted as individual points. \n\nNow we loop over some numeric features and draw **Box plot** for each one.","434395eb":"## 6. Xgboost Regressor","ae1eb30d":"As we see some of above \u261d boxplots include outliers like **MSSubClass, LotFrontage, and OverallQual** Now we will use **IQR score** to discover outliers.<br>\n\n#### IQR score\nBox plot use the IQR method to display data and outliers.<br>\n\n**Wikipedia Definition,**\n> The interquartile range (IQR) is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, <code>IQR = Q3 \u2212 Q1.<\/code>\n\nIf <code> value < Q1 - 1.5 x IQR <\/code>  or <code> value > Q3 + 1.5 x IQR <\/code> this mean that this value is **outlier**<br>\n    \nNow we loop over all numeric features and calculate **IQR** for each one.\n","cf9a7238":"As we see from abov \u261d correlation matrix that the following feature more related to target variable **(SalePrice)**\n<table style='float:left;'>\n    <tr>\n        <th rowspan='2'>Features Names<\/th>\n        <td style='text-align:center;'>LotFrontage<\/td>\n        <td style='text-align:center;'>LotArea<\/td>\n        <td style='text-align:center;'>OverallQual<\/td>\n        <td style='text-align:center;'>YearBuilt<\/td>\n        <td style='text-align:center;'>YearRemodAdd<\/td>\n        <td style='text-align:center;'>MasVnrArea<\/td>\n        <td style='text-align:center;'>TotalBsmtSF<\/td>\n        <td style='text-align:center;'>1stFlrSF<\/td>\n    <\/tr>\n    <tr>\n        <td style='text-align:center;'>GrLivArea<\/td>\n        <td style='text-align:center;'>FullBath<\/td>\n        <td style='text-align:center;'>TotRmsAbvGrd<\/td>\n        <td style='text-align:center;'>GarageYrBlt<\/td>\n        <td style='text-align:center;'>GarageCars<\/td>\n        <td style='text-align:center;'>GarageArea<\/td>\n        <td style='text-align:center;'>WoodDeckSF<\/td>\n        <td style='text-align:center;'>OpenPorchSF<\/td>\n    <\/tr>\n<table>\n<div style=\"clear:both\"><\/div>    \nLet's draw scatter plots of this features \ud83d\udcc8\n    \n### Scatter plot \ud83d\udc47\nA scatter plot is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. **For this** we can know a relationship between **feature** and **target variable**<br>\n\nNow we will draw scatter plots of this features. we will divied it into two groups for clearing plots.   ","c5c24035":"# Step By Step : From Data Exploration To Model Building\n\n<img src='https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/5407\/media\/housesbanner.png' alt='houses'>\n<p>\n    Welcome all \ud83d\udc4b<br><br>\n    In this **NoteBook** we will go with **House Prices data** step by step. This NoteBook will be devided into the following parts \ud83d\udc47 <br>\n    <ol>\n        <li><b>Data Preprocessing<\/b><\/li>\n        <li><b>Feature Selection<\/b><\/li>\n        <li><b>Data Scaling<\/b><\/li>\n        <li><b>Model Bulding<\/b><\/li>\n        <li><b>Model evaluation<\/b><\/li>\n        <li><b>References<\/b><\/li>\n    <\/ol>\n<\/p>\n\n**First We will import libraries and load data \ud83d\udc47**","dee891d3":"## 5. Decision Tree Regressor","957f057f":"**Big Congratulations \ud83d\udc4f\ud83d\udc4f now we don't have any missing values or outliers in our data**<br>\n\n# 2. Feature Selection\nBefore go with feature selection we will drop **Id** column first","ecc2d6a2":"## 2. Ridge Regression","1da3ac2e":"# 1. Data Preprocessing\n\nIn this section especially i want to thank [Pedro Marcelino](https:\/\/www.kaggle.com\/pmarcelino)<br>Because i learned awesome things from his Kernel [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python#5.-Getting-hard-core).<br><br>Now we will start **Data Preprocessing** by exploar our data.\n\n## 1.1 Data exploration \ud83d\udc47","92e484ac":"# 4. Model Bulding\nNow we will build our model. we wil use **Stochastic Gradient Descent Regressor** model used for **regression** problems. But before using it we will split our data to train and test set first.","be5b4044":"According to categorical features I don't find way to impute it using KNN so if any one know any way to do that i will thank him \ud83d\ude0a<br>\nNow I will use my own custom simpel imputer it will act as simpel **sklearn** imputer by set **strategy = most_frequent** but on categorical data. This may not best choice.","0cf73b28":"**Moment of truth** \ud83d\ude27<br>\nNow let's know if our data contain any missing value","72c50885":"Now we will use **GridSearchModel** for choose best parameters for our models we will start by using **stochastic gradient descent** model \ud83d\udcaa \n## 1. stochastic gradient descent","db58b8ba":"Now let's deal with **Categorical** columns \ud83d\uded2<br>\n\nWe have 3 approach to preprocess categorical columns like \ud83d\udc47\n* Drop Categorical Variables\n* Label Encoding\n* One-Hot Encoding\n\nwe will use **Label Encoding** to preprocess our categorical columns.<br>\n\nLet's start \ud83d\ude0a\n### LabelEncoder \ud83d\udc47"}}