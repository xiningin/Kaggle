{"cell_type":{"c020a56c":"code","5c4a9edf":"code","8da018ad":"code","784b87c1":"code","0208da24":"code","7bb24d12":"code","61be8eb2":"code","36f3a8e3":"code","7db9f996":"code","40fae50d":"code","850949fe":"code","6b2ef9c9":"code","aa9079d5":"code","4b171285":"code","abcb7715":"code","7fb7ca54":"code","6b85b8bc":"code","03116dc7":"code","d87ece5f":"code","827b28fc":"code","2a1569ce":"code","f091005c":"code","241578c6":"code","c73c32f9":"code","bab49277":"code","e001bc69":"code","bf667e31":"code","342d0d41":"code","f9724ca5":"code","db3833ba":"code","f7f3078f":"code","bb5b2798":"code","74e9a8b6":"code","afe76759":"code","079cf0fa":"code","93bc003f":"code","96aec14c":"code","e9df54b5":"code","c7a6fa7b":"code","b4340771":"markdown","ae7ebfed":"markdown","f550405d":"markdown","8a87b491":"markdown","379f556b":"markdown","e66d4e11":"markdown","56e01ac3":"markdown","2efddf82":"markdown","b14bbfd0":"markdown","873613de":"markdown","021299eb":"markdown","a667b9ca":"markdown","b59bc029":"markdown","a9f8d093":"markdown","cf413b6a":"markdown","93e161ae":"markdown","fe2d561e":"markdown","d5ac21ef":"markdown","43df3ddf":"markdown","15a2b1bb":"markdown","69b2be94":"markdown","02ea0140":"markdown","87e731aa":"markdown","8c055ede":"markdown"},"source":{"c020a56c":"DO_SUBMISSION = True\nDO_TRAIN_FOR_ENSEMBLE = False\nDO_VIRTUAL_SUBMISSION = False\nassert (sum([DO_SUBMISSION, DO_TRAIN_FOR_ENSEMBLE, DO_VIRTUAL_SUBMISSION]) == 1), \"select `ONE` mode\"","5c4a9edf":"%%time\n# for tabnet\n!pip install --no-index --find-links ..\/input\/pytorchtabnet\/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet","8da018ad":"%%time\n# for nn.py\n!pip install ..\/input\/iterative-stratification\/iterative-stratification-master\/","784b87c1":"import os\nimport gc\nimport sys\nimport random\nimport shutil\nimport warnings\nimport typing as tp\nfrom pathlib import Path\nfrom copy import deepcopy\n\nimport yaml\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import log_loss\n\nwarnings.resetwarnings()\nwarnings.simplefilter('ignore', FutureWarning)\nwarnings.simplefilter('ignore', DeprecationWarning)\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"","0208da24":"import torch\nfrom torch import nn\nfrom torch.utils import data\ntorch.backends.cudnn.benchmark = True\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nsys.path.append(\"..\/input\/pytorch-pfn-extras\/pytorch-pfn-extras-0.3.1\/\")\nimport pytorch_pfn_extras as ppe\nfrom pytorch_pfn_extras.training import extensions as ppe_extensions","7bb24d12":"def generate_virtual_private(test_feat, smpl_sub, rate_to_pub=4):\n    \n    assert (test_feat.sig_id == smpl_sub.sig_id).all()\n    \n    # public \u306f\u542b\u307e\u308c\u308b\u306e\u3067\u3001\u305d\u306e\u307e\u307e\u5165\u308c\u308b.\n    test_feat_list = [test_feat]\n    smpl_sub_list = [smpl_sub]\n    \n    for i in range(1, rate_to_pub):\n        tmp_test_feat = test_feat.copy()\n        tmp_smpl_sub = smpl_sub.copy()\n        \n        # #  id \u3092\u5909\u66f4\n        tmp_test_feat.sig_id = tmp_test_feat.sig_id + f\"_{i}\"\n        tmp_smpl_sub.sig_id = tmp_smpl_sub.sig_id + f\"_{i}\"\n        assert (tmp_test_feat.sig_id == tmp_smpl_sub.sig_id).all()\n        \n        # # `c-*` \u3068 `g-*` \u306b\u9069\u5f53\u306a\u5024\u3092\u52a0\u3048\u308b. \u8907\u88fd\u3057\u3066\u5897\u3084\u3059\u3060\u3051\u3060\u3068\u3059\u308a\u629c\u3051\u308b\u5834\u5408\u304c\u3042\u3063\u305f\u305f\u3081.\n        tmp_test_feat.iloc[:, 4:] += i * 10\n        assert (tmp_test_feat.iloc[:, 4:] != test_feat.iloc[:, 4:]).all().all()\n        \n        test_feat_list.append(tmp_test_feat)\n        smpl_sub_list.append(tmp_smpl_sub)\n        \n    # # \u7d50\u5408\n    test_feat_concat = pd.concat(test_feat_list, axis=0, ignore_index=True)\n    smpl_sub_concat = pd.concat(smpl_sub_list, axis=0, ignore_index=True)\n    \n    return test_feat_concat, smpl_sub_concat","61be8eb2":"TARGET_COL = ['5-alpha_reductase_inhibitor', '11-beta-hsd1_inhibitor', 'acat_inhibitor', 'acetylcholine_receptor_agonist', 'acetylcholine_receptor_antagonist', 'acetylcholinesterase_inhibitor', 'adenosine_receptor_agonist', 'adenosine_receptor_antagonist', 'adenylyl_cyclase_activator', 'adrenergic_receptor_agonist', 'adrenergic_receptor_antagonist', 'akt_inhibitor', 'aldehyde_dehydrogenase_inhibitor', 'alk_inhibitor', 'ampk_activator', 'analgesic', 'androgen_receptor_agonist', 'androgen_receptor_antagonist', 'anesthetic_-_local', 'angiogenesis_inhibitor', 'angiotensin_receptor_antagonist', 'anti-inflammatory', 'antiarrhythmic', 'antibiotic', 'anticonvulsant', 'antifungal', 'antihistamine', 'antimalarial', 'antioxidant', 'antiprotozoal', 'antiviral', 'apoptosis_stimulant', 'aromatase_inhibitor', 'atm_kinase_inhibitor', 'atp-sensitive_potassium_channel_antagonist', 'atp_synthase_inhibitor', 'atpase_inhibitor', 'atr_kinase_inhibitor', 'aurora_kinase_inhibitor', 'autotaxin_inhibitor', 'bacterial_30s_ribosomal_subunit_inhibitor', 'bacterial_50s_ribosomal_subunit_inhibitor', 'bacterial_antifolate', 'bacterial_cell_wall_synthesis_inhibitor', 'bacterial_dna_gyrase_inhibitor', 'bacterial_dna_inhibitor', 'bacterial_membrane_integrity_inhibitor', 'bcl_inhibitor', 'bcr-abl_inhibitor', 'benzodiazepine_receptor_agonist', 'beta_amyloid_inhibitor', 'bromodomain_inhibitor', 'btk_inhibitor', 'calcineurin_inhibitor', 'calcium_channel_blocker', 'cannabinoid_receptor_agonist', 'cannabinoid_receptor_antagonist', 'carbonic_anhydrase_inhibitor', 'casein_kinase_inhibitor', 'caspase_activator', 'catechol_o_methyltransferase_inhibitor', 'cc_chemokine_receptor_antagonist', 'cck_receptor_antagonist', 'cdk_inhibitor', 'chelating_agent', 'chk_inhibitor', 'chloride_channel_blocker', 'cholesterol_inhibitor', 'cholinergic_receptor_antagonist', 'coagulation_factor_inhibitor', 'corticosteroid_agonist', 'cyclooxygenase_inhibitor', 'cytochrome_p450_inhibitor', 'dihydrofolate_reductase_inhibitor', 'dipeptidyl_peptidase_inhibitor', 'diuretic', 'dna_alkylating_agent', 'dna_inhibitor', 'dopamine_receptor_agonist', 'dopamine_receptor_antagonist', 'egfr_inhibitor', 'elastase_inhibitor', 'erbb2_inhibitor', 'estrogen_receptor_agonist', 'estrogen_receptor_antagonist', 'faah_inhibitor', 'farnesyltransferase_inhibitor', 'fatty_acid_receptor_agonist', 'fgfr_inhibitor', 'flt3_inhibitor', 'focal_adhesion_kinase_inhibitor', 'free_radical_scavenger', 'fungal_squalene_epoxidase_inhibitor', 'gaba_receptor_agonist', 'gaba_receptor_antagonist', 'gamma_secretase_inhibitor', 'glucocorticoid_receptor_agonist', 'glutamate_inhibitor', 'glutamate_receptor_agonist', 'glutamate_receptor_antagonist', 'gonadotropin_receptor_agonist', 'gsk_inhibitor', 'hcv_inhibitor', 'hdac_inhibitor', 'histamine_receptor_agonist', 'histamine_receptor_antagonist', 'histone_lysine_demethylase_inhibitor', 'histone_lysine_methyltransferase_inhibitor', 'hiv_inhibitor', 'hmgcr_inhibitor', 'hsp_inhibitor', 'igf-1_inhibitor', 'ikk_inhibitor', 'imidazoline_receptor_agonist', 'immunosuppressant', 'insulin_secretagogue', 'insulin_sensitizer', 'integrin_inhibitor', 'jak_inhibitor', 'kit_inhibitor', 'laxative', 'leukotriene_inhibitor', 'leukotriene_receptor_antagonist', 'lipase_inhibitor', 'lipoxygenase_inhibitor', 'lxr_agonist', 'mdm_inhibitor', 'mek_inhibitor', 'membrane_integrity_inhibitor', 'mineralocorticoid_receptor_antagonist', 'monoacylglycerol_lipase_inhibitor', 'monoamine_oxidase_inhibitor', 'monopolar_spindle_1_kinase_inhibitor', 'mtor_inhibitor', 'mucolytic_agent', 'neuropeptide_receptor_antagonist', 'nfkb_inhibitor', 'nicotinic_receptor_agonist', 'nitric_oxide_donor', 'nitric_oxide_production_inhibitor', 'nitric_oxide_synthase_inhibitor', 'norepinephrine_reuptake_inhibitor', 'nrf2_activator', 'opioid_receptor_agonist', 'opioid_receptor_antagonist', 'orexin_receptor_antagonist', 'p38_mapk_inhibitor', 'p-glycoprotein_inhibitor', 'parp_inhibitor', 'pdgfr_inhibitor', 'pdk_inhibitor', 'phosphodiesterase_inhibitor', 'phospholipase_inhibitor', 'pi3k_inhibitor', 'pkc_inhibitor', 'potassium_channel_activator', 'potassium_channel_antagonist', 'ppar_receptor_agonist', 'ppar_receptor_antagonist', 'progesterone_receptor_agonist', 'progesterone_receptor_antagonist', 'prostaglandin_inhibitor', 'prostanoid_receptor_antagonist', 'proteasome_inhibitor', 'protein_kinase_inhibitor', 'protein_phosphatase_inhibitor', 'protein_synthesis_inhibitor', 'protein_tyrosine_kinase_inhibitor', 'radiopaque_medium', 'raf_inhibitor', 'ras_gtpase_inhibitor', 'retinoid_receptor_agonist', 'retinoid_receptor_antagonist', 'rho_associated_kinase_inhibitor', 'ribonucleoside_reductase_inhibitor', 'rna_polymerase_inhibitor', 'serotonin_receptor_agonist', 'serotonin_receptor_antagonist', 'serotonin_reuptake_inhibitor', 'sigma_receptor_agonist', 'sigma_receptor_antagonist', 'smoothened_receptor_antagonist', 'sodium_channel_inhibitor', 'sphingosine_receptor_agonist', 'src_inhibitor', 'steroid', 'syk_inhibitor', 'tachykinin_antagonist', 'tgf-beta_receptor_inhibitor', 'thrombin_inhibitor', 'thymidylate_synthase_inhibitor', 'tlr_agonist', 'tlr_antagonist', 'tnf_inhibitor', 'topoisomerase_inhibitor', 'transient_receptor_potential_channel_antagonist', 'tropomyosin_receptor_kinase_inhibitor', 'trpv_agonist', 'trpv_antagonist', 'tubulin_inhibitor', 'tyrosine_kinase_inhibitor', 'ubiquitin_specific_protease_inhibitor', 'vegfr_inhibitor', 'vitamin_b', 'vitamin_d_receptor_agonist', 'wnt_inhibitor']","36f3a8e3":"MODEL_NAMES = [\"NN(drugCV)\", \"TabNet\", \"ResNet\", \"ThrNN\", \"ThrNN(drugCV)\"]","7db9f996":"%%time\nif DO_SUBMISSION:\n    !python ..\/input\/moa-nn-tabnet-fix5\/nn-inference-0.01833.py\n\nelif DO_VIRTUAL_SUBMISSION:\n    !python ..\/input\/moa-nn-tabnet-fix5\/virtual-nn-use-train-public-inference.py\n\nelif DO_TRAIN_FOR_ENSEMBLE:\n    pass\nelse:\n    raise ValueError","40fae50d":"%%time\nif DO_SUBMISSION:\n    !python ..\/input\/moa-nn-tabnet-fix5\/tabnet-inference-0.01840.py\n\nelif DO_VIRTUAL_SUBMISSION:\n    !python ..\/input\/moa-nn-tabnet-fix5\/virtual-tabnet-inference-add-param-n-shared.py\n\nelif DO_TRAIN_FOR_ENSEMBLE:\n    pass\nelse:\n    raise ValueError","850949fe":"%%time\nif DO_SUBMISSION:\n    !python ..\/input\/moa-takapy-script\/tf-rn-transfer-1layerother-selcol100.py  # 0.01862\n\nelif DO_VIRTUAL_SUBMISSION:\n    !python ..\/input\/moa-takapy-script\/virtual-tf-rn-transfer-1layerother-selcol100.py\n\nelif DO_TRAIN_FOR_ENSEMBLE:\n    pass\nelse:\n    raise ValueError","6b2ef9c9":"%%time\nif DO_SUBMISSION:\n    !python ..\/input\/moa-tawara-scripts-for-final-submission\/moa-for-final-thrnn-seed-cv-0.01836.py\n\nelif DO_VIRTUAL_SUBMISSION:\n    !python ..\/input\/moa-tawara-scripts-for-final-submission\/virtual-moa-for-final-thrnn-seed-cv-0.01836.py\n\nelif DO_TRAIN_FOR_ENSEMBLE:\n    pass\nelse:\n    raise ValueError","aa9079d5":"%%time\nif DO_SUBMISSION:\n    !python ..\/input\/moa-tawara-scripts-for-final-submission\/moa-for-final-thrnn-drug-seed-cv-0.01841.py\n\nelif DO_VIRTUAL_SUBMISSION:\n    !python ..\/input\/moa-tawara-scripts-for-final-submission\/virtual-moa-for-final-thrnn-drug-seed-cv-0.01841.py\n\nelif DO_TRAIN_FOR_ENSEMBLE:\n    pass\nelse:\n    raise ValueError","4b171285":"def order_sub(sub) : \n    return sub.sort_values('sig_id').reset_index(drop=True)\n\nif DO_SUBMISSION or DO_VIRTUAL_SUBMISSION:\n    sub_list = [\n        pd.read_csv('.\/submission-sinchir0-nn.csv'),\n        pd.read_csv('.\/submission_sinchir0_tabnet.csv'),\n        pd.read_csv(\".\/submission_takapy_tf-resnet.csv\"),\n        pd.read_csv('.\/submission_tawara_thrnn_seed_cv.csv'),\n        pd.read_csv('.\/submission_tawara_thrnn_drug_seed_cv.csv'),\n    ]\nelse:\n    sub_list = [\n        pd.read_csv('..\/input\/nn-use-train-public\/submission.csv'),\n        pd.read_csv('..\/input\/tabnet-train-public-add-n-shared-1\/submission.csv'),\n        pd.read_csv(\"..\/input\/moa-takapy-tf-resnet-transfer\/submission.csv\"),\n        pd.read_csv('..\/input\/moa-weight-thrnn-seed-cv\/submission.csv'),\n        pd.read_csv('..\/input\/moa-weight-thrnn-drug-seed-cv\/submission.csv'),\n    ]\n    \nfor i, name in enumerate([\"NN(drugCV)\", \"TabNet\", \"ResNet\", \"ThrNN\", \"ThrNN(drugCV)\"]):\n    print(f\"[{name}]:\", sub_list[i].shape)\n\nsub_list = [order_sub(sub_df) for sub_df in sub_list]","abcb7715":"class MoAStackingDataset(data.Dataset):\n    \n    def __init__(self, feat: np.ndarray, label: np.ndarray = None):\n        \"\"\"\"\"\"\n        self.feat = feat\n        if label is None:\n            self.label = np.full((len(feat), 1), -1)\n        else:\n            self.label = label\n        self.model_order = None\n        \n    def __len__(self):\n        \"\"\"\"\"\"\n        return len(self.feat)\n    \n    def __getitem__(self, index: int):\n        \"\"\"\"\"\"\n        return [\n            torch.from_numpy(self.feat[index]).float(),\n            torch.from_numpy(self.label[index]).float()\n        ]\n    \n    def reset_model_order(self):\n        \"\"\"Dummy Method.\"\"\"\n        pass\n        \n    def shuffle_model_order(self, seed):\n        \"\"\"Dummy Method.\"\"\"\n        pass\n\n\nclass MoAStackingDatasetForCNN(data.Dataset):\n    \n    def __init__(self, feat: np.ndarray, label: np.ndarray = None):\n        \"\"\"\"\"\"\n        self.feat = feat\n        if label is None:\n            self.label = np.full((len(feat), 1), -1)\n        else:\n            self.label = label\n        self.reset_model_order()\n        \n    def reset_model_order(self):\n        self.model_order = np.arange(self.feat.shape[-1])\n        \n    def shuffle_model_order(self, seed):\n        np.random.seed(seed)\n        self.model_order = np.random.permutation(self.model_order)\n        \n    def __len__(self):\n        \"\"\"\"\"\"\n        return len(self.feat)\n    \n    def __getitem__(self, index: int):\n        \"\"\"\"\"\"\n        return [\n            torch.from_numpy(self.feat[index][..., self.model_order]).float(),\n            torch.from_numpy(self.label[index]).float()\n        ]\n    \n    \nclass MoAStackingDatasetForGCN(data.Dataset):\n    \n    def __init__(self, feat: np.ndarray, label: np.ndarray = None):\n        \"\"\"\"\"\"\n        self.feat = feat\n        if label is None:\n            self.label = np.full((len(feat), 1), -1)\n        else:\n            self.label = label\n        self.model_order = None\n        \n    def reset_model_order(self):\n        \"\"\"Dummy Method.\"\"\"\n        pass\n        \n    def shuffle_model_order(self, seed):\n        \"\"\"Dummy Method.\"\"\"\n        pass\n        \n    def __len__(self):\n        \"\"\"\"\"\"\n        return len(self.feat)\n    \n    def __getitem__(self, index: int):\n        \"\"\"\"\"\"\n        return [\n            torch.from_numpy(self.feat[index]).float(),\n            torch.from_numpy(self.label[index]).float()\n        ]","7fb7ca54":"def get_activation(activ_name: str=\"relu\"):\n    \"\"\"\"\"\"\n    act_dict = {\n        \"relu\": nn.ReLU(),\n        \"tanh\": nn.Tanh(),\n        \"sigmoid\": nn.Sigmoid(),\n        \"identity\": nn.Identity()}\n    if activ_name in act_dict:\n        return act_dict[activ_name]\n    elif re.match(r\"^htanh\\_\\d{4}$\", activ_name):\n        bound = int(activ_name[-4:]) \/ 1000\n        return nn.Hardtanh(-bound, bound)\n    else:\n        raise NotImplementedError\n\nclass LBAD(nn.Module):\n    \"\"\"Linear (-> BN) -> Activation (-> Dropout)\"\"\"\n    \n    def __init__(\n        self, in_features: int, out_features: int, drop_rate: float=0.0,\n        use_bn: bool=False, use_wn: bool=False, activ: str=\"relu\"\n    ):\n        \"\"\"\"\"\"\n        super(LBAD, self).__init__()\n        layers = [nn.Linear(in_features, out_features)]\n        if use_wn:\n            layers[0] = nn.utils.weight_norm(layers[0])\n        \n        if use_bn:\n            layers.append(nn.BatchNorm1d(out_features))\n        \n        layers.append(get_activation(activ))\n        \n        if drop_rate > 0:\n            layers.append(nn.Dropout(drop_rate))\n        \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        return self.layers(x)\n    \n    \nclass BDLA(nn.Module):\n    \"\"\"(BN -> Dropout ->) Linear -> Activation\"\"\"\n    \n    def __init__(\n        self, in_features: int, out_features: int, drop_rate: float=0.0,\n        use_bn: bool=False, use_wn: bool=False, activ: str=\"relu\"\n    ):\n        \"\"\"\"\"\"\n        super(BDLA, self).__init__()\n        layers = []\n        if use_bn:\n            layers.append(nn.BatchNorm1d(in_features))\n            \n        if drop_rate > 0:\n            layers.append(nn.Dropout(drop_rate))\n        \n        layers.append(nn.Linear(in_features, out_features))\n        if use_wn:\n            layers[-1] = nn.utils.weight_norm(layers[-1])\n            \n        layers.append(get_activation(activ))\n        \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        return self.layers(x)\n    \n\nclass LABD(nn.Module):\n    \"\"\"Linear -> Activation (-> BN -> Dropout) \"\"\"\n    \n    def __init__(\n        self, in_features: int, out_features: int, drop_rate: float=0.0,\n        use_bn: bool=False, use_wn: bool=False, activ: str=\"relu\"\n    ):\n        \"\"\"\"\"\"\n        super(LABD, self).__init__()\n        layers = [nn.Linear(in_features, out_features), get_activation(activ)]\n        \n        if use_wn:\n            layers[0] = nn.utils.weight_norm(layers[0])\n        \n        if use_bn:\n            layers.append(nn.BatchNorm1d(out_features))\n        \n        if drop_rate > 0:\n            layers.append(nn.Dropout(drop_rate))\n        \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        return self.layers(x)","6b85b8bc":"# # for GCNs\ndef vector_wise_matmul(X: torch.Tensor, W: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    See input matrixes X as bags of vectors, and multiply corresponding weight matrices by vector.\n    \n    Args:\n        X: Input Tensor, shape: (batch_size, **n_vectors**, in_features)\n        W: Weight Tensor, shape: (**n_vectors**, out_features, in_features)\n    \"\"\"\n    X = torch.transpose(X, 0, 1)  # shape: (n_vectors, batch_size, in_features)\n    W = torch.transpose(W, 1, 2)  # shape: (n_vectors, in_features, out_features)\n    H = torch.matmul(X, W)        # shape: (n_vectors, batch_size, out_features)\n    H = torch.transpose(H, 0, 1)  # shape: (batch_size, n_vectors, out_features)\n    \n    return H\n\n\ndef vector_wise_shared_matmul(X: torch.Tensor, W: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    See input matrixes X as bags of vectors, and multiply **shared** weight matrices.\n    \n    Args:\n        X: Input Tensor, shape: (batch_size, **n_vectors**, in_features)\n        W: Weight Tensor, shape: (out_features, in_features)\n    \"\"\"\n    # W = torch.transpose(W, 0, 1)  # shape: (in_features, out_features)\n    # H = torch.matmul(X, W)        # shape: (batch_size, n_vectors, out_features)\n    \n    H = nn.functional.linear(X, W)  # shape: (batch_size, n_vectors, out_features)\n    \n    return H","03116dc7":"def _calculate_fan_in_and_fan_out_for_vwl(tensor) -> tp.Tuple[int]:\n    \"\"\"\n    Input tensor: (n_vectors, out_features, in_features) or (out_features, in_features)\n    \"\"\"\n    dimensions = tensor.dim()\n    if dimensions < 2:\n        raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\n\n    fan_in = tensor.size(-1)\n    fan_out = tensor.size(-2)\n\n    return fan_in, fan_out\n    \n\ndef _calculate_correct_fan_for_vwl(tensor, mode) -> int:\n    \"\"\"\"\"\"\n    mode = mode.lower()\n    valid_modes = ['fan_in', 'fan_out']\n    if mode not in valid_modes:\n        raise ValueError(\"Mode {} not supported, please use one of {}\".format(mode, valid_modes))\n\n    fan_in, fan_out = _calculate_fan_in_and_fan_out_for_vwl(tensor)\n    return fan_in if mode == 'fan_in' else fan_out\n\n\ndef kaiming_uniform_for_vwl(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu'):\n    \"\"\"\"\"\"\n    fan = _calculate_correct_fan_for_vwl(tensor, mode)\n    gain = nn.init.calculate_gain(nonlinearity, a)\n    std = gain \/ np.sqrt(fan)\n    bound = np.sqrt(3.0) * std  # Calculate uniform bounds from standard deviation\n    with torch.no_grad():\n        return tensor.uniform_(-bound, bound)","d87ece5f":"class VectorWiseLinear(nn.Module):\n    \"\"\"\n    For mini batch which have several matrices,\n    see as these matrixes as bags of vectors, and multiply weight matrices by vector.\n    \n    input    X: (batch_size, **n_vectors**, in_features)\n    weight W: (**n_vector**, out_features, in_features)\n    output  Y: (batch_size, **n_vectors**, out_features)\n\n    **Note**: For simplicity, bias is not described.\n    \n    X and W are can be seen as below.\n    X: [\n            [vec_{ 1, 1}, vec_{ 1, 2}, ... vec_{ 1, n_vectors}],\n            [vec_{ 2, 1}, vec_{ 2, 2}, ... vec_{ 2, n_vectors}],\n                                            .\n                                            .\n            [vec_{bs, 1}, vec_{bs, 2}, ... vec_{bs, n_vectors}]\n        ]\n    W: [\n            Mat_{1}, Mat_{2}, ... , Mat_{n_vectors}\n        ]\n    Then Y is calclauted as:\n    Y: [\n        [ Mat_{1} vec_{ 1, 1}, Mat_{2} vec_{ 1, 2}, ... Mat_{n_vectors} vec_{ 1, n_vectors}],\n        [ Mat_{1} vec_{ 2, 1}, Mat_{2} vec_{ 2, 2}, ... Mat_{n_vectors} vec_{ 2, n_vectors}],\n        .\n        .\n        [ Mat_{1} vec_{bs, 1}, Mat_{2} vec_{bs, 2}, ... Mat_{n_vectors} vec_{bs, n_vectors}],\n    ]\n    \"\"\"\n    \n    def __init__(\n        self,\n        in_features: int, out_features: int, n_vectors: int,\n        bias: bool=True, weight_shared: bool=True\n    ) -> None:\n        \"\"\"Initialize.\"\"\"\n        super(VectorWiseLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.n_vectors = n_vectors\n        self.weight_shared = weight_shared\n        \n        if self.weight_shared:\n            self.weight = nn.Parameter(\n                torch.Tensor(self.out_features, self.in_features))\n            self.matmul_func = vector_wise_shared_matmul\n        else:\n            self.weight = nn.Parameter(\n                torch.Tensor(self.n_vectors, self.out_features, self.in_features))\n            self.matmul_func = vector_wise_matmul\n            \n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(out_features))\n        else:\n            self.register_parameter('bias', None)\n            \n        self.reset_parameters()\n        \n    def reset_parameters(self) -> None:\n        \"\"\"Initialize weight and bias.\"\"\"\n        kaiming_uniform_for_vwl(self.weight, a=np.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = _calculate_fan_in_and_fan_out_for_vwl(self.weight)\n            bound = 1 \/ np.sqrt(fan_in)\n            nn.init.uniform_(self.bias, -bound, bound)\n             \n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        \"\"\"Forward.\"\"\"\n        H = self.matmul_func(X, self.weight)\n        if self.bias is not None:\n            H = H + self.bias\n        \n        return H","827b28fc":"class GraphConv(nn.Module):\n    \"\"\"Basic Graph Convolution Layer.\"\"\"\n    \n    def __init__(\n        self, \n        in_channels: int, out_channels: int, n_nodes: int, shrare_msg: bool=True,\n        model_self: bool=True, share_model_self: bool=True,\n        bias: bool=True, share_bias: bool=True\n    ) -> None:\n        \"\"\"Intialize.\"\"\"\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.n_nodes = n_nodes\n        self.model_self = model_self\n        super(GraphConv, self).__init__()\n        \n        # # message\n        self.msg = VectorWiseLinear(\n            in_channels, out_channels, n_nodes, False, shrare_msg)\n\n        # # self-modeling\n        if model_self:\n            self.model_self = VectorWiseLinear(\n                in_channels, out_channels, n_nodes, False, share_model_self)\n        \n        # # bias\n        if bias:\n            if share_bias:\n                self.bias = nn.Parameter(torch.Tensor(out_channels))\n            else:\n                self.bias = nn.Parameter(torch.Tensor(n_nodes, out_channels))\n            bound = 1 \/ np.sqrt(out_channels)\n            nn.init.uniform_(self.bias, -bound, bound)\n        else:\n            self.register_parameter('bias', None)\n            \n    \n    def forward(self, X: torch.Tensor, A: torch.Tensor, W: torch.Tensor=None) -> torch.Tensor:\n        \"\"\"Forward.\n        \n        Args:\n            X: (batch_size, n_nodes, n_channels)\n                Array which represents bags of vectors.\n                X[:, i, :] are corresponded to feature vectors of node i.\n            A: (batch_size, n_nodes, n_nodes)\n                Array which represents adjacency matrices.\n                A[:, i, j] are corresponded to weights (scalar) of edges from node j to node i.\n            W: (batch_size, n_nodes, n_nodes)\n                Array which represents weight matrices between nodes.\n        \"\"\"\n        if W is not None:\n            A = A * W  # shape: (batch_size, n_nodes, n_nodes)\n        \n        # # update message\n        M = X  #  shape: (batch_size, n_nodes, in_channels)\n        # # # send message\n        M = self.msg(M)  # shape: (batch_size, n_nodes, out_channels)\n        # # # aggregate\n        M = torch.matmul(A, M)  # shape: (batch_size, n_nodes, out_channels)\n            \n        # # update node\n        # # # self-modeling\n        H = M\n        if self.model_self:\n            H = H + self.model_self(X)\n        if self.bias is not None:\n            H = H + self.bias\n        \n        return H","2a1569ce":"class MLP(nn.Module):\n    \"\"\"Stacked Dense layers\"\"\"\n    \n    def __init__(\n        self, n_features_list: tp.List[int], use_tail_as_out: bool=False,\n        drop_rate: float=0.0, use_bn: bool=False, use_wn: bool=False,\n        activ:str=\"relu\", block_name: str=\"LBAD\"\n    ):\n        \"\"\"\"\"\"\n        super(MLP, self).__init__()\n        n_layers = len(n_features_list) - 1\n        block_class = {\n            \"LBAD\": LBAD, \"BDLA\": BDLA, \"LABD\": LABD}[block_name]\n        layers = []\n        for i in range(n_layers):\n            in_feats, out_feats = n_features_list[i: i + 2]\n            if i == n_layers - 1 and use_tail_as_out:\n                if block_name in [\"BDLA\"]:\n                    layer = block_class(in_feats, out_feats, drop_rate, use_bn,  use_wn, \"identity\")\n                else:\n                    layer = nn.Linear(in_feats, out_feats)\n                    if use_wn:\n                        layer = nn.utils.weight_norm(layer)\n            else:\n                layer = block_class(in_feats, out_feats, drop_rate, use_bn,  use_wn, activ)\n            layers.append(layer)\n                \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        return self.layers(x)\n\n\nclass CNNStacking1d(nn.Module):\n    \"\"\"1D-CNN for Stacking.\"\"\"\n    \n    def __init__(\n        self, n_models: int,\n        n_channels_list: tp.List[int], use_bias: bool=False,\n        kwargs_head: tp.Dict={},\n    ):\n        \"\"\"\"\"\"\n        super(CNNStacking1d, self).__init__()\n        self.n_conv_layers = len(n_channels_list) - 1\n        for i in range(self.n_conv_layers):\n            in_ch = n_channels_list[i]\n            out_ch = n_channels_list[i + 1]\n            layer = nn.Sequential(\n                nn.Conv1d(\n                    in_ch, out_ch, kernel_size=3, stride=1, padding=0, bias=use_bias),\n                # nn.BatchNorm1d(out_ch),\n                nn.ReLU(inplace=True))\n            setattr(self, \"conv{}\".format(i + 1), layer)\n        \n        kwargs_head[\"n_features_list\"][0] = (n_models - 2 * self.n_conv_layers) * n_channels_list[-1]\n        self.head = MLP(**kwargs_head)\n    \n    def forward(self, x: torch.FloatTensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        bs = x.shape[0]\n        h = x  # shape: (bs, n_classes, n_models)\n        for i in range(self.n_conv_layers):\n            h = getattr(self, \"conv{}\".format(i + 1))(h)\n            \n        h = torch.reshape(h, (bs, -1))\n        h = self.head(h)\n        return h\n    \n    \n    \nclass CNNStacking2d(nn.Module):\n    \"\"\"2D-CNN for Stacking.\"\"\"\n    \n    def __init__(\n        self, n_models: int, n_classes: int,\n        n_channels_list: tp.List[int], use_bias: bool=False,\n        kwargs_head: tp.Dict={},\n    ):\n        \"\"\"\"\"\"\n        super(CNNStacking2d, self).__init__()\n        self.n_conv_layers = len(n_channels_list) - 1\n        for i in range(self.n_conv_layers):\n            in_ch = n_channels_list[i]\n            out_ch = n_channels_list[i + 1]\n            layer = nn.Sequential(\n                nn.Conv2d(\n                    in_ch, out_ch, kernel_size=(1, 3), stride=1, padding=0, bias=use_bias),\n                # nn.BatchNorm2d(out_ch),\n                nn.ReLU(inplace=True))\n            setattr(self, \"conv{}\".format(i + 1), layer)\n        \n        kwargs_head[\"n_features_list\"][0] = (n_models - 2 * self.n_conv_layers) * n_classes * n_channels_list[-1]\n        self.head = MLP(**kwargs_head)\n    \n    def forward(self, x: torch.FloatTensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        bs = x.shape[0]\n        h = x  # shape: (bs, 1, n_classes, n_models)\n        for i in range(self.n_conv_layers):\n            h = getattr(self, \"conv{}\".format(i + 1))(h)\n        \n        h = torch.reshape(h, (bs, -1))\n        h = self.head(h)\n        return h\n    \n    \nclass GCNStacking(nn.Module):\n    \"\"\"GCN for Stacking.\"\"\"\n    \n    def __init__(\n        self, n_classes: int,\n        n_channels_list: tp.List[int],\n        add_self_loop: bool=False,\n        kwargs_head: tp.Dict={},\n    ):\n        \"\"\"\"\"\"\n        super(GCNStacking, self).__init__()\n        self.n_conv_layers = len(n_channels_list) - 1\n        for i in range(self.n_conv_layers):\n            in_ch = n_channels_list[i]\n            out_ch = n_channels_list[i + 1]\n            # layer = CustomGraphConv(in_ch, out_ch, n_classes)\n            layer = GraphConv(\n                in_ch, out_ch, n_classes,\n                shrare_msg=False, share_model_self=False, share_bias=False)\n            setattr(self, \"conv{}\".format(i + 1), layer)\n        \n        self.relu = nn.ReLU(inplace=True)\n        if add_self_loop:\n            adj_mat = torch.ones(n_classes, n_classes) \/ n_classes\n        else:\n            adj_mat = (1 - torch.eye(n_classes, n_classes)) \/ (n_classes - 1) \n        self.register_buffer(\"A\", adj_mat.float())\n               \n        kwargs_head[\"n_features_list\"][0] = n_classes * n_channels_list[-1]\n        self.head = MLP(**kwargs_head)\n    \n    def forward(self, X: torch.FloatTensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        bs, n_classes = X.shape[:2]\n        H = X  # shape: (bs, n_classes, n_models)\n        for i in range(self.n_conv_layers):\n            H = getattr(self, \"conv{}\".format(i + 1))(H, self.A[None, ...])\n            H = self.relu(H)\n        \n        h = torch.reshape(H, (bs, -1))\n        h = self.head(h)\n        return h","f091005c":"def set_random_seed(seed: int = 42, deterministic: bool = False):\n    \"\"\"Set seeds\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    if deterministic:\n        torch.backends.cudnn.deterministic = True  # type: ignore","241578c6":"def inference_function(settings, model, loader, device):\n    model.to(device)\n    model.eval()\n    pred_list = []\n    with torch.no_grad():\n        for x, t in loader:\n            y = model(x.to(device))\n            pred_list.append(y.sigmoid().detach().cpu().numpy())\n        \n        pred_arr = np.concatenate(pred_list)\n        del pred_list\n    return pred_arr","c73c32f9":"def inference_test(stgs, model_dir, test_dataset, target_col_list):\n    \n    device = torch.device(stgs[\"globals\"][\"device\"])\n    stgs_list = []\n\n    # # for seed avg\n    for tmp_seed in stgs[\"globals\"][\"seeds_for_avg\"]:\n        stgs_list.append(deepcopy(stgs))\n        stgs_list[-1][\"globals\"][\"seed\"] = tmp_seed\n    \n    test_pred_arr_avg = np.zeros((len(test_dataset), len(target_col_list)))\n    for m_id, tmp_stgs in enumerate(stgs_list):\n        n_folds = tmp_stgs[\"globals\"][\"n_folds_split\"]\n        tmp_seed = tmp_stgs[\"globals\"][\"seed\"]\n        test_preds_arr = np.zeros((n_folds, len(test_dataset), len(target_col_list)))\n        \n        test_dataset.reset_model_order()\n        test_dataset.shuffle_model_order(tmp_seed)\n        print(\"[model {}'s order]: {}\".format(m_id, test_dataset.model_order))\n\n        for fold_id in range(n_folds):\n            print(\"[fold: {} - model: {}]\".format(fold_id, m_id))\n            # # load model\n            model_path = model_dir \/ \"best_model_fold{}_model{}.pth\".format(fold_id, m_id)\n            model = eval(tmp_stgs[\"model\"][\"name\"])(**tmp_stgs[\"model\"][\"params\"])\n            model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n\n            # # inference test\n            test_loader = data.DataLoader(test_dataset, **tmp_stgs[\"loader\"][\"val\"])\n            test_pred = inference_function(tmp_stgs, model, test_loader, device)\n            test_preds_arr[fold_id] = test_pred\n            del test_loader; del test_pred;\n            gc.collect()\n        \n        test_pred_arr_avg += test_preds_arr.mean(axis=0)\n\n    test_pred_arr_avg \/= len(stgs_list)\n    \n    return test_pred_arr_avg","bab49277":"X_test = np.concatenate([\n    sub_df.iloc[:, 1:].values for sub_df in sub_list], axis=1)\nprint(X_test.shape)\n\nX_test_2d = np.stack([\n    sub_df.iloc[:, 1:].values for sub_df in sub_list], axis=2)\nprint(X_test_2d.shape)\n\nX_test_3d = X_test_2d[:, None, ...]\nprint(X_test_3d.shape)\n\nX_test_node = np.stack([\n    sub_df.iloc[:, 1:].values for sub_df in sub_list], axis=2)\n\ntest_dataset_mlp = MoAStackingDataset(X_test, None)\ntest_dataset_1dcnn = MoAStackingDatasetForCNN(X_test_2d, None)\ntest_dataset_2dcnn = MoAStackingDatasetForCNN(X_test_3d, None)\ntest_dataset_gcn = MoAStackingDatasetForGCN(X_test_node, None)","e001bc69":"settings_str_mlp = \"\"\"\nglobals:\n  seed: 1990\n  seeds_for_avg: [1990, 42, 0, 1086, 39]\n  max_epoch: 20\n  n_folds_split: 7\n  patience: 10\n  cuda_visible_devices: 0\n  device: cuda\n  fast_commit: False\n\nloader:\n  train: {\n    batch_size: 128, shuffle: True, num_workers: 2,\n    pin_memory: True, drop_last: True}\n  val: {\n    batch_size: 256, shuffle: False, num_workers: 2,\n    pin_memory: True, drop_last: False}\n\nmodel:\n  name: MLP\n  params:\n    n_features_list: [1030, 1024, 1024, 206]\n    use_tail_as_out: True\n    drop_rate: 0.2\n    use_bn: False\n    use_wn: True\n    block_name: LABD\n        \nloss:\n  name: MyLSLogLoss\n  params: {k: 2, alpha: 1.0e-03}\n\noptimizer:\n  name: Adam\n  params: {lr: 1.0e-03}\n\nscheduler:\n  name: OneCycleLR\n  params: {pct_start: 0.1, div_factor: 1.0e+3, max_lr: 1.0e-02}\n\"\"\"\n\nsettings_mlp = yaml.safe_load(settings_str_mlp)\nmodel_dir_mlp = Path(\"..\/input\/moa-mlp-stacking-drug-cv\")","bf667e31":"settings_str_1dcnn = \"\"\"\nglobals:\n  seed: 1990\n  seeds_for_avg: [1990, 42, 0, 1086, 39]\n  max_epoch: 20\n  n_folds_split: 7\n  patience: 10\n  cuda_visible_devices: 0\n  device: cuda\n  fast_commit: False\n\nloader:\n  train: {\n    batch_size: 128, shuffle: True, num_workers: 2,\n    pin_memory: True, drop_last: True}\n  val: {\n    batch_size: 256, shuffle: False, num_workers: 2,\n    pin_memory: True, drop_last: False}\n\nmodel:\n  name: CNNStacking1d\n  params:\n    n_models: 5\n    n_channels_list: [206, 512, 1024]\n    use_bias: True\n    kwargs_head:\n        n_features_list: [1024, 2048, 206]\n        use_tail_as_out: True\n        drop_rate: 0.8\n        use_bn: False\n        use_wn: True\n        block_name: LABD\n        \nloss:\n  name: MyLSLogLoss\n  params: {k: 2, alpha: 1.0e-03}\n\noptimizer:\n  name: Adam\n  params: {lr: 1.0e-03}\n\nscheduler:\n  name: OneCycleLR\n  params: {pct_start: 0.1, div_factor: 1.0e+3, max_lr: 1.0e-02}\n\"\"\"\n\nsettings_1dcnn = yaml.safe_load(settings_str_1dcnn)\nmodel_dir_1dcnn = Path(\"..\/input\/moa-1dcnn-stacking-drug-cv\")","342d0d41":"settings_str_2dcnn = \"\"\"\nglobals:\n  seed: 1990\n  seeds_for_avg: [1990, 42, 0, 1086, 39]\n  max_epoch: 20\n  n_folds_split: 7\n  patience: 10\n  cuda_visible_devices: 0\n  device: cuda\n  fast_commit: False\n\nloader:\n  train: {\n    batch_size: 128, shuffle: True, num_workers: 2,\n    pin_memory: True, drop_last: True}\n  val: {\n    batch_size: 256, shuffle: False, num_workers: 2,\n    pin_memory: True, drop_last: False}\n\nmodel:\n  name: CNNStacking2d\n  params:\n    n_models: 5\n    n_classes: 206\n    n_channels_list: [1, 8, 16]\n    use_bias: True\n    kwargs_head:\n        n_features_list: [-1, 2048, 206]\n        use_tail_as_out: True\n        drop_rate: 0.8\n        use_bn: False\n        use_wn: True\n        block_name: LABD\n        \nloss:\n  name: MyLSLogLoss\n  params: {k: 2, alpha: 1.0e-03}\n\noptimizer:\n  name: Adam\n  params: {lr: 1.0e-03}\n\nscheduler:\n  name: OneCycleLR\n  params: {pct_start: 0.1, div_factor: 1.0e+3, max_lr: 1.0e-02}\n\"\"\"\n\nsettings_2dcnn = yaml.safe_load(settings_str_2dcnn)\nmodel_dir_2dcnn = Path(\"..\/input\/moa-2dcnn-stacking-drug-cv\")","f9724ca5":"settings_str_gcn = \"\"\"\nglobals:\n  seed: 1990\n  seeds_for_avg: [1990, 42, 0, 1086, 39]\n  max_epoch: 20\n  n_folds_split: 7\n  patience: 10\n  cuda_visible_devices: 0\n  device: cuda\n  fast_commit: False\n\nloader:\n  train: {\n    batch_size: 128, shuffle: True, num_workers: 2,\n    pin_memory: True, drop_last: True}\n  val: {\n    batch_size: 256, shuffle: False, num_workers: 2,\n    pin_memory: True, drop_last: False}\n\nmodel:\n  name: GCNStacking\n  params:\n    n_classes: 206\n    n_channels_list: [5, 16, 16, 16, 16]\n    add_self_loop: True\n    kwargs_head:\n        n_features_list: [-1, 2048, 206]\n        use_tail_as_out: True\n        drop_rate: 0.8\n        use_bn: False\n        use_wn: True\n        block_name: LABD\n        \nloss:\n  name: MyLSLogLoss\n  params: {k: 2, alpha: 1.0e-03}\n\noptimizer:\n  name: Adam\n  params: {lr: 1.0e-03}\n\nscheduler:\n  name: OneCycleLR\n  params: {pct_start: 0.1, div_factor: 1.0e+3, max_lr: 1.0e-02}\n\"\"\"\n\nsettings_gcn = yaml.safe_load(settings_str_gcn)\nmodel_dir_gcn = Path(\"..\/input\/moa-gcn-stacking-drug-cv\")","db3833ba":"if not torch.cuda.is_available():\n    settings_mlp[\"globals\"][\"device\"] = \"cpu\"\n\ntest_pred_mlp = inference_test(settings_mlp, model_dir_mlp, test_dataset_mlp, TARGET_COL)","f7f3078f":"if not torch.cuda.is_available():\n    settings_1dcnn[\"globals\"][\"device\"] = \"cpu\"\n\ntest_pred_1dcnn = inference_test(settings_1dcnn, model_dir_1dcnn,test_dataset_1dcnn, TARGET_COL)","bb5b2798":"if not torch.cuda.is_available():\n    settings_2dcnn[\"globals\"][\"device\"] = \"cpu\"\n\ntest_pred_2dcnn = inference_test(\n    settings_2dcnn, model_dir_2dcnn, test_dataset_2dcnn, TARGET_COL)","74e9a8b6":"if not torch.cuda.is_available():\n    settings_gcn[\"globals\"][\"device\"] = \"cpu\"\n\ntest_pred_gcn = inference_test(settings_gcn, model_dir_gcn, test_dataset_gcn, TARGET_COL)","afe76759":"opt_weights = [0.1709929 ,  0.22160569,  0.07207321,  0.95431566, -0.42108426]\n\ntest_pred_wo = np.zeros_like(sub_list[0].iloc[:, 1:].values)\nfor w, sub_df in zip(opt_weights, sub_list):\n    test_pred_wo += sub_df.iloc[:, 1:].values * w","079cf0fa":"test_pred_list = [\n    test_pred_mlp,\n    test_pred_1dcnn,\n    test_pred_2dcnn,\n    test_pred_gcn,\n    test_pred_wo\n]\n\ntest_pred_avg = np.zeros_like(test_pred_list[0])\nfor test_pred in test_pred_list:\n    test_pred_avg += test_pred\n    \ntest_pred_avg \/= len(test_pred_list)","93bc003f":"BLEND = sub_list[0].copy()\nBLEND.iloc[:, 1:] = test_pred_avg\n\nprint(\"shape:\", BLEND.shape)\ndisplay(BLEND.head())","96aec14c":"if DO_SUBMISSION or DO_TRAIN_FOR_ENSEMBLE:\n    df_test = pd.read_csv(\"..\/input\/lish-moa\/test_features.csv\")\n    submission = pd.read_csv(\"..\/input\/lish-moa\/sample_submission.csv\")\n\nelif DO_VIRTUAL_SUBMISSION:\n    df_test = pd.read_csv(\"..\/input\/lish-moa\/test_features.csv\")\n    submission = pd.read_csv(\"..\/input\/lish-moa\/sample_submission.csv\")\n    print(df_test.shape, submission.shape)\n    df_test, submission = generate_virtual_private(df_test, submission)\n    print(df_test.shape, submission.shape)\n\nelse:\n    raise ValueError","e9df54b5":"# submission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\n\n# df = pd.read_csv(\"\/kaggle\/input\/lish-moa\/sample_submission.csv\")\ndf = submission.copy()\n\npublic_id = list(df['sig_id'].values)\n\n# df_test = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntest_id = list(df_test['sig_id'].values)\n\nprivate_id = list(set(test_id)-set(public_id))\n\ndf_submit = pd.DataFrame(index = public_id+private_id, columns=TARGET_COL)\ndf_submit.index.name = 'sig_id'\ndf_submit[:] = 0\ndf_predict = BLEND.copy()\ndf_submit.loc[df_predict.sig_id,:] = df_predict[TARGET_COL].values\ndf_submit.loc[df_test[df_test.cp_type =='ctl_vehicle'].sig_id] = 0\ndf_submit.to_csv('submission.csv',index=True)","c7a6fa7b":"print(df_submit.shape)\ndf_submit.head()","b4340771":"## Inference Test by Stage1 Models","ae7ebfed":"### inference and output csv","f550405d":"### dataset","8a87b491":"### settings, model dir","379f556b":"### utils","e66d4e11":"## definition","56e01ac3":"# About","2efddf82":"### 2D-CNN","b14bbfd0":"#### 1D-CNN","873613de":"## prepare data","021299eb":"## import","a667b9ca":"#### MLP","b59bc029":"## pip install","a9f8d093":"# Inference Test by Stage2 Models","cf413b6a":"This notebook simply averages predictions of 4 stacking models and weight optimization.\n\nI used **the same stage1 models** as [our 34th solution](https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/200798) (Private Score: 0.01608), but achieved a Private Score of **0.01599**.\n\nFor more details, see this topic:  \nhttps:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/204685\n\n<br>\n\n## score of each model\n\n| name   | Local CV | Public  | Private | Notebook | Trained Weight | \n|:------:|:--------:|:-------:|:-------:|:--------:|:--------------:|\n| MLP    | 0.016232 | 0.01829 | 0.01617 | [Link](https:\/\/www.kaggle.com\/ttahara\/stacking-mlp-drugcv\/) | [Link](https:\/\/www.kaggle.com\/ttahara\/moa-mlp-stacking-drug-cv) |\n| 1D-CNN | 0.016123 | 0.01829 | 0.01610 | [Link](https:\/\/www.kaggle.com\/ttahara\/stacking-1d-cnn-drugcv\/) | [Link](https:\/\/www.kaggle.com\/ttahara\/moa-1dcnn-stacking-drug-cv) |\n| 2D-CNN | 0.015817 | 0.01827 | 0.01609 | [Link](https:\/\/www.kaggle.com\/ttahara\/stacking-2d-cnn-drugcv) | [Link](https:\/\/www.kaggle.com\/ttahara\/moa-2dcnn-stacking-drug-cv) |\n| GCN    | 0.016089 | 0.01829 | 0.01607 | [Link](https:\/\/www.kaggle.com\/ttahara\/stacking-gcn-drugcv\/) | [Link](https:\/\/www.kaggle.com\/ttahara\/moa-gcn-stacking-drug-cv) |\n| Weight Optimize | - | 0.01831\t| 0.01618 | - | - |\n\n<br>\n<br>\n\n## model overview\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-forum-message-attachments\/o\/inbox%2F473234%2Fdbcb5b751535155e65ca392191560283%2Fstacking_all_stars.png?generation=1608115573302050&alt=media\" width=75%>","93e161ae":"### models","fe2d561e":"# Preparation","d5ac21ef":"### test data","43df3ddf":"### load csv","15a2b1bb":"### averaging","69b2be94":"## Make Submission","02ea0140":"### GCN","87e731aa":"## inference","8c055ede":"## mode config"}}