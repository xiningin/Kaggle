{"cell_type":{"401b4c99":"code","3b5c1c6f":"code","6268b069":"code","e7561ff6":"code","de7bbb92":"code","8649d75c":"code","42d9380b":"code","f2a45962":"code","979f5801":"code","3be99fea":"code","97aa0ad6":"code","18d3faf9":"code","8bc14b26":"code","2b250605":"code","f989790c":"code","238d25d0":"code","04b9fbde":"code","b1445cdc":"code","37577cee":"code","4b9857c9":"code","a0aaee77":"code","ef0dc11c":"code","c524e39b":"code","25dba3d8":"code","ef0bacb7":"code","41efd2b7":"code","d601f2dc":"code","1782afcb":"code","6e2b98f8":"code","8316cd63":"code","a3e447e7":"code","9cde6413":"code","efa1b20f":"code","5137b94b":"code","cd205e46":"code","75af3e7b":"code","13a5ae76":"code","bab07c9e":"code","39d5cddd":"code","8d451156":"code","995d53ab":"code","10c739bc":"code","efa21f71":"code","7a0a031f":"code","8b0e5589":"code","640b5561":"code","d8d5e687":"code","3f1cde70":"code","d6a02909":"code","b05a7ae7":"code","0e1f243d":"code","3df28b14":"code","129a5af4":"code","7fde6a83":"code","3832d4d3":"code","d12a893c":"code","d64baf78":"code","0f57db6a":"code","422ccfc1":"code","82caca56":"code","b0b242d2":"code","d92bcec7":"code","c0a13cb9":"code","f421a30e":"code","713c4e74":"code","bf44afe2":"code","26769101":"code","7bca84b1":"code","60d5a319":"code","85f3f368":"code","f7ae6777":"code","d42e93c9":"code","00d8593b":"code","388ac493":"code","ad53ecff":"code","6f98223b":"code","849d55ab":"code","886af44b":"code","a8074e96":"code","cd01065c":"code","68798165":"code","d08560e1":"code","de9adb75":"code","1f6ff1f8":"code","e3f110a0":"code","bf8e7afd":"code","af8c490e":"code","b8d27e8c":"code","bd4f1be6":"code","296912a8":"code","0abe770c":"code","8970e8fc":"code","fe91ec02":"code","8e8740fb":"code","9a9d2725":"code","45194fc3":"markdown","b08e39bd":"markdown","92d2dcc9":"markdown","335bf19c":"markdown","8c5af50c":"markdown","427b7da4":"markdown","f0d28c13":"markdown","54072cdf":"markdown","13275606":"markdown","433e413f":"markdown","e1acc934":"markdown","789e4c64":"markdown","9c72fc22":"markdown","2eb1d994":"markdown","ece34116":"markdown","4c91a655":"markdown","18789496":"markdown","28a366bf":"markdown","679f171d":"markdown","3e4929fa":"markdown","3596ae46":"markdown","683e31bc":"markdown","8efc200e":"markdown","4fc7d67e":"markdown","2cce4c9a":"markdown","47d2182c":"markdown","13e1b7c3":"markdown","1d730c22":"markdown"},"source":{"401b4c99":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b5c1c6f":"# Import data analysis libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Importing sklearn libraries\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV, cross_val_score, cross_val_predict\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n# Importing model evaluation libraries\nfrom sklearn.metrics import precision_score, recall_score, precision_recall_curve, auc, roc_auc_score, roc_curve, confusion_matrix, fbeta_score\n%matplotlib inline","6268b069":"data = pd.read_csv(\"\/kaggle\/input\/water-potability\/water_potability.csv\")\ndata.head()","e7561ff6":"data.info()","de7bbb92":"# Identifying columns with missing values %s\nnp.round(data.isnull().sum()[data.isnull().sum()>0]\/len(data)*100,1)","8649d75c":"data.isnull().sum()","42d9380b":"np.round(data[\"Potability\"].value_counts(normalize= True)*100.0,1)","f2a45962":"sns.displot(x = data[\"Potability\"], kde = False)\nplt.title(\"Distribution of classes\", fontsize = 14)\nplt.show()\n","979f5801":"data.describe()","3be99fea":"# Features of each class\ngro_by_Portability = data.groupby(\"Potability\")\ngro_by_Portability.mean().T","97aa0ad6":"# # For Potable Class only\n# plt.figure(figsize = (20,20))\n# sns.pairplot(data.query(\"Potability ==1\" ), diag_kind = \"hist\")\n# plt.legend()\n# plt.title(\"Pair Plot\")\n# plt.show()","18d3faf9":"plt.figure(figsize = (20,20))\nsns.pairplot(data, diag_kind = \"hist\", hue = \"Potability\")\nplt.legend()\nplt.title(\"Pair Plot\")\nplt.show()","8bc14b26":"gro_by_Portability[\"Chloramines\"].mean()[0]","2b250605":"fig = plt.figure(figsize =(8,6))\nsns.displot(data = data, x = \"Chloramines\", hue = \"Potability\")\nplt.axvline(x = gro_by_Portability[\"Chloramines\"].mean()[0], c = 'red')\nplt.axvline(x = gro_by_Portability[\"Chloramines\"].mean()[1], c = 'blue')\nplt.title(\"Distribution of Class with Chloramines\", fontsize = 13)\nplt.show()\n","f989790c":"# Plotting the correlation heatmap to check for multicollinearity in the feature space\nplt.figure(figsize = (8,8))\nsns.heatmap(data.corr(), annot = True, cmap = 'YlGnBu')\nplt.show()","238d25d0":"data.dropna(axis =0).drop(\"Potability\", axis = 1).columns","04b9fbde":"# Checking the VIF of all the predictors\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\ndata_for_vif = data.dropna(axis =0).drop(\"Potability\", axis = 1).copy()\ndata_for_vif[\"Constant\"] = 1","b1445cdc":"vif_df= pd.DataFrame()\nvif_df[\"Feature\"] = data_for_vif.columns\nvif_df[\"vif\"] = [variance_inflation_factor(data_for_vif.values, i) for i in range(len(data_for_vif.columns))]\nvif_df","37577cee":"# CHecking the missing values per group\nattribs_with_nan = [\"ph\",\"Sulfate\",\"Trihalomethanes\"]\ndata.set_index(\"Potability\")[attribs_with_nan].isna().groupby(\"Potability\").sum()","4b9857c9":"from imblearn.under_sampling import RandomUnderSampler\nunder = RandomUnderSampler()\n","a0aaee77":"X = data.drop(\"Potability\", axis = 1).values\ny = data[\"Potability\"].values\ny_best = data[\"Potability\"].values\n\nfrom collections import Counter\nCounter(y)","ef0dc11c":"X, y = under.fit_resample(X, y)\nCounter(y)\n","c524e39b":"pipeline = Pipeline(\n[(\"imputer\", SimpleImputer(strategy = \"mean\")),\n (\"std_sclaer\", StandardScaler())\n ])","25dba3d8":"X_prepared = pipeline.fit_transform(X)\nX_prepared","ef0bacb7":"X_train, X_test, y_train, y_test = train_test_split(X_prepared, y, random_state = 1, test_size = 0.2)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n","41efd2b7":"def create_confusion_matrix_df(y_actual, y_pred):\n    return pd.DataFrame(confusion_matrix(y_actual, y_pred), index = [\"Actual Not Potable\", \"Actual Potable\"], columns = [\"Pred Not Potable\", \"Pred Potable\"])","d601f2dc":"# Using Logistic Regression - Since, the classes are not linearly separable and hence, logistic regression may not be the best model. Let's give it a try\nlog_reg_clf = LogisticRegression()\nlog_reg_clf.fit(X_train, y_train)\n# Let us see the training accuracy\ny_train_pred_log_reg = log_reg_clf.predict(X_train)\n","1782afcb":"cm_log_reg_train = create_confusion_matrix_df(y_train, y_train_pred_log_reg)\ncm_log_reg_train","6e2b98f8":"log_reg_train_score = pd.DataFrame(np.c_[log_reg_clf.predict_proba(X_train)[:,:-1], y_train_pred_log_reg.astype(int), log_reg_clf.predict_proba(X_train), y_train], columns = [\"Predict_Proba\", \"Y_predicted\", \"Class0_Prob\",\"Class1 Prob\", \"Y_Actual\"])\nlog_reg_train_score[\"Y_predicted\"] = log_reg_train_score[\"Y_predicted\"].astype(int)\nlog_reg_train_score[\"Y_Actual\"] = log_reg_train_score[\"Y_Actual\"].astype(int)\nlog_reg_train_score","8316cd63":"# Cross Validatiing the Logistic Reression Model\n\ny_scores = cross_val_predict(log_reg_clf, X_train, y_train, cv = 5, method = \"predict_proba\")\ny_scores","a3e447e7":"# Plotting the precision Recall Curve for Logistic Regression Model\n\nprec, rec, thrs = precision_recall_curve(y_train, y_scores[:,0])\n\nfig = plt.figure(figsize = (8,6))\nplt.plot(thrs, prec[:-1], 'b--', label = \"Precision\")\nplt.plot(thrs, rec[:-1], 'g-', label = \"Recall\")\nplt.xlabel(\"Threshold - Probability Score\")\nplt.ylabel(\"Precision \/  Recall\")\nplt.title(\"Precision\/Recall vs Threshold curve of Logistic Regression\")\nplt.legend()\nplt.show()\n","9cde6413":"plt.plot(rec, prec)\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision Vs Recall for Logistics Regression\")\nplt.show()","efa1b20f":"fpr, tpr, threholds = roc_curve(y_train, y_scores[:,1])\nplt.plot(fpr, tpr, 'b-')\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title(\"ROC Curve of Logistic Regression\")\nplt.text(0, 0.8, s = \"ROC AUC score : {}\".format(roc_auc_score(y_train, y_scores[:,1])))\nplt.show()","5137b94b":"sgd_clf = SGDClassifier(random_state = 1, loss = \"log\")\nsgd_clf.fit(X_train, y_train)\ny_train_pred_sgd =sgd_clf.predict(X_train) # In the training Set","cd205e46":"cm_sgd_train = create_confusion_matrix_df(y_train, y_train_pred_sgd)\ncm_sgd_train","75af3e7b":"from scipy.stats import hmean # harmonic mean\nprecisions_train_sgd = cm_sgd_train.iloc[1,1]\/(cm_sgd_train.iloc[1,1] + cm_sgd_train.iloc[0,1])\nrecalls_train_sgd = cm_sgd_train.iloc[1,1]\/(cm_sgd_train.iloc[1,1] + cm_sgd_train.iloc[1,0])\nf1_train_sgd = hmean([precisions_train_sgd, recalls_train_sgd])\nprecisions_train_sgd, recalls_train_sgd, f1_train_sgd","13a5ae76":"# Checking teh Precision and Recall Score for SGDCLassifier\n# precsion_SGD_train = precision_score(y_train, y_train_pred_sgd)\n# Recall_SGD_train = recall_score(y_train, y_train_pred_sgd)\n# f1_train_sgd = f1_score(y_train, y_train_pred_sgd)\n# precsion_SGD_train, Recall_SGD_train, f1_sgd_train","bab07c9e":"#Using Cross Validation to see the variation of Precision and Recall with the threshold\nfrom sklearn.model_selection import cross_val_predict\n\ny_train_scores_sgd = cross_val_predict(sgd_clf, X_train, y_train, cv = 10, method = 'predict_proba', verbose = 10)\ny_train_scores_sgd.shape","39d5cddd":"def display_scores(scores):\n    print(\"Precisions: \", scores)\n    print(\"Mean Precision: \", scores.mean())\n    print(\"Std. of precisions: \", scores.std())\n","8d451156":"y_cross_val_scores = cross_val_score(sgd_clf, X_train, y_train , cv = 10, scoring = 'f1')\ndisplay_scores(y_cross_val_scores)\n\n","995d53ab":"#Plotting Threshold vs True Positive Rate\nfrom sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_train_scores_sgd[:,1])\nthresholds","10c739bc":"from sklearn.metrics import plot_precision_recall_curve\nplot_precision_recall_curve(sgd_clf, X_train, y_train,response_method = \"predict_proba\" )\nplt.show()\n","efa21f71":"# Plotting Precision vs Threshold, Recall Vs Threshold\nfig = plt.figure(figsize = (8,6))\nax = plt.subplot(111)\nplt.plot(thresholds, precisions[:-1], 'b--', label = \"Precision\")\nplt.plot(thresholds, recalls[:-1], 'g-', label = \"Recalls\")\nplt.legend()\n#ax.set_xticks(np.linspace(-1,1,20))\nplt.ylabel(\"Precisions and Recalls\")\nplt.xlabel(\"Thresholds\")\nplt.title(\"Precisions\/Recalls vs Thresholds\", fontsize = 14)\nplt.show()","7a0a031f":"# Plotting the threshold with Precision only\nplt.plot(thresholds, precisions[:-1])\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precisions vs Threshold\", fontsize = 13)\nplt.show()","8b0e5589":"#Plotting Precsions Vs Recall for the scores\n\nfig = plt.figure(figsize = (8,6))\nax = plt.subplot(111)\nplt.plot(recalls, precisions) # excluding border values\n#ax.set_xticks(np.linspace(0,1,50))\nplt.xlabel(\"Recalls\")\nplt.ylabel(\"Precisions\")\nplt.title(\"Precisions vs Recalls\", fontsize = 14)\nplt.show()\n","640b5561":"# Calculating ROC AUC score for SGD and plottting\nroc_auc_score(y_train, y_train_scores_sgd[:,1])\nfpr, tpr, threholds = roc_curve(y_train, y_train_scores_sgd[:,1])\nplt.plot(fpr, tpr, 'b-')\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title(\"ROC Curve of SGD Classifier\")\nplt.text(0, 0.8, s = \"ROC AUC score : {}\".format(roc_auc_score(y_train, y_train_scores_sgd[:,1])))\nplt.show()","d8d5e687":"dtree_clf = DecisionTreeClassifier(random_state=1)\ndtree_clf.fit(X_train, y_train)","3f1cde70":"y_train_pred_dtree = dtree_clf.predict(X_train)\n\ncreate_confusion_matrix_df(y_train, y_train_pred_dtree)","d6a02909":"#Using Cross Validation to see the variation of Precision and Recall with the threshold\nfrom sklearn.model_selection import cross_val_predict\n\ny_train_scores_dtree = cross_val_predict(dtree_clf, X_train, y_train, cv = 10, method = 'predict_proba')\ny_train_scores_dtree","b05a7ae7":"y_cross_val_scores = cross_val_score(dtree_clf, X_train, y_train , cv = 10, scoring = 'f1')\ndisplay_scores(y_cross_val_scores)","0e1f243d":"from sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_train_scores_dtree[:,1])\n\n# Plotting Precision vs Threshold, Recall Vs Threshold\nfig = plt.figure(figsize = (8,6))\nax = plt.subplot(111)\nplt.plot(thresholds, precisions[:-1], 'b--', label = \"Precision\")\nplt.plot(thresholds, recalls[:-1], 'g-', label = \"Recalls\")\nplt.legend()\nplt.ylabel(\"Precisions and Recalls\")\nplt.xlabel(\"Thresholds\")\nplt.title(\"Precisions\/Recalls vs Thresholds\", fontsize = 14)\nplt.show()","3df28b14":"# Plotting the threshold with Precision only\nplt.plot(thresholds, precisions[:-1])\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precisions vs Threshold\", fontsize = 13)\nplt.show()","129a5af4":"fig = plt.figure(figsize = (8,6))\nax = plt.subplot(111)\nplt.plot(recalls, precisions)\nplt.xlabel(\"Recalls\")\nplt.ylabel(\"Precisions\")\nplt.title(\"Precisions vs Recalls\", fontsize = 14)\nplt.show()","7fde6a83":"# Calculating ROC AUC score for SGD and plottting\nroc_auc_score(y_train, y_train_scores_dtree[:,1])\nfpr, tpr, threholds = roc_curve(y_train, y_train_scores_dtree[:,1])\nplt.plot(fpr, tpr, 'b-')\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title(\"ROC Curve of Decision Tree Classifier\")\nplt.text(0, 0.8, s = \"ROC AUC score : {}\".format(roc_auc_score(y_train, y_train_scores_dtree[:,1])))\nplt.show()","3832d4d3":"rand_frst = RandomForestClassifier(random_state = 1)\nrand_frst.fit(X_train, y_train)\n\ny_train_pred_forest = rand_frst.predict(X_train)\n\ncreate_confusion_matrix_df(y_train, y_train_pred_forest)","d12a893c":"#Using Cross Validation to see the variation of Precision and Recall with the threshold\nfrom sklearn.model_selection import cross_val_predict\n\ny_train_scores_forest = cross_val_predict(rand_frst, X_train, y_train, cv = 10, method = 'predict_proba')\n#y_train_scores_forest","d64baf78":"y_cross_val_scores = cross_val_score(rand_frst, X_train, y_train , cv = 10, scoring = 'f1')\ndisplay_scores(y_cross_val_scores)","0f57db6a":"from sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_train_scores_forest[:,1])\n\n# Plotting Precision vs Threshold, Recall Vs Threshold\nfig = plt.figure(figsize = (8,6))\nax = plt.subplot(111)\nplt.plot(thresholds, precisions[:-1], 'b--', label = \"Precision\")\nplt.plot(thresholds, recalls[:-1], 'g-', label = \"Recalls\")\nplt.legend()\nplt.ylabel(\"Precisions and Recalls\")\nplt.xlabel(\"Thresholds\")\nplt.title(\"Precisions\/Recalls vs Thresholds\", fontsize = 14)\nplt.show()","422ccfc1":"# Plotting the threshold with Precision only\nplt.plot(thresholds, precisions[:-1])\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precisions vs Threshold\", fontsize = 13)\nplt.show()","82caca56":"fig = plt.figure(figsize = (8,6))\nax = plt.subplot(111)\nplt.plot(recalls, precisions)\nplt.xlabel(\"Recalls\")\nplt.ylabel(\"Precisions\")\nplt.title(\"Precisions vs Recalls\", fontsize = 14)\nplt.show()","b0b242d2":"# Calculating ROC AUC score for SGD and plottting\nroc_auc_score(y_train, y_train_scores_forest[:,1])\nfpr, tpr, threholds = roc_curve(y_train, y_train_scores_forest[:,1])\nplt.plot(fpr, tpr, 'b-')\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title(\"ROC Curve of Random Forest Classifier\")\nplt.text(0, 0.8, s = \"ROC AUC score : {}\".format(roc_auc_score(y_train, y_train_scores_forest[:,1])))\nplt.show()","d92bcec7":"from sklearn.svm import SVC\nsvc = SVC(random_state = 1, probability = True)\nsvc.fit(X_train, y_train)\ny_train_pred_svm = svc.predict(X_train)\ncreate_confusion_matrix_df(y_train, y_train_pred_svm)","c0a13cb9":"# Let us try to cross validate the SVM \ny_scores_svc = cross_val_predict(svc, X_train, y_train, cv = 10, method = \"predict_proba\")","f421a30e":"y_cross_val_scores = cross_val_score(svc, X_train, y_train , cv = 10, scoring = 'f1')\ndisplay_scores(y_cross_val_scores)","713c4e74":"## Plotting a precision recall curve of SVC\nprecision, recall, threshold = precision_recall_curve(y_train, y_scores_svc[:,1])\nplt.plot(threshold, precision[:-1], 'b--')\nplt.plot(threshold, recall[:-1], 'g-')\nplt.show()","bf44afe2":"fig = plt.figure(figsize = (8,6))\nax = plt.subplot(111)\nplt.plot(recalls, precisions)\nplt.xlabel(\"Recalls\")\nplt.ylabel(\"Precisions\")\nplt.title(\"Precisions vs Recalls\", fontsize = 14)\nplt.show() ","26769101":"# Calculating ROC AUC score for SGD and plottting\nroc_auc_score(y_train, y_scores_svc[:,1])\nfpr, tpr, threholds = roc_curve(y_train, y_scores_svc[:,1])\nplt.plot(fpr, tpr, 'b-')\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title(\"ROC Curve of SVC Classifier\")\nplt.text(0, 0.8, s = \"ROC AUC score : {}\".format(roc_auc_score(y_train, y_scores_svc[:,1])))\nplt.show()","7bca84b1":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 9)\nknn.fit(X_train, y_train)\ny_train_pred_knn = knn.predict(X_train)\ncreate_confusion_matrix_df(y_train, y_train_pred_knn)\n","60d5a319":"# Let us try to cross validate the SVM \ny_scores_knn = cross_val_predict(knn, X_train, y_train, cv = 10, method = \"predict_proba\")\ny_scores_knn","85f3f368":"y_cross_val_scores = cross_val_score(knn, X_train, y_train , cv = 10, scoring = 'precision')\ndisplay_scores(y_cross_val_scores)","f7ae6777":"## Plotting a precision recall curve of SVC\nprecision, recall, threshold = precision_recall_curve(y_train, y_scores_svc[:,1])\nplt.plot(threshold, precision[:-1], 'b--')\nplt.plot(threshold, recall[:-1], 'g-')\nplt.show()\n","d42e93c9":"fig = plt.figure(figsize = (8,6))\nax = plt.subplot(111)\nplt.plot(recalls, precisions)\nplt.xlabel(\"Recalls\")\nplt.ylabel(\"Precisions\")\nplt.title(\"Precisions vs Recalls\", fontsize = 14)\nplt.show()","00d8593b":"# Calculating ROC AUC score for SGD and plottting\nroc_auc_score(y_train, y_scores_knn[:,1])\nfpr, tpr, threholds = roc_curve(y_train, y_scores_knn[:,1])\nplt.plot(fpr, tpr, 'b-')\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title(\"ROC Curve of KNN Classifier\")\nplt.text(0, 0.8, s = \"ROC AUC score : {}\".format(roc_auc_score(y_train, y_scores_knn[:,1])))\nplt.show()","388ac493":"baseline_models = [\"Logistic Regression\", \"SGD Classifier\", \"Decisions Tree Classifer\", \"Random Forest\", \"SVC\", \"KNN\"]\nlog_reg_scores_prec = cross_val_score(log_reg_clf, X_train, y_train , cv = 10, scoring = 'precision')\nsgd_scores_prec = cross_val_score(sgd_clf, X_train, y_train , cv = 10, scoring = 'precision')\ndtree_scores_prec = cross_val_score(dtree_clf, X_train, y_train , cv = 10, scoring = 'precision')\nrand_frst_scores_prec = cross_val_score(rand_frst, X_train, y_train , cv = 10, scoring = 'precision')\nSVC_scores_prec = cross_val_score(svc, X_train, y_train , cv = 10, scoring = 'precision')\nknn_scores_prec = cross_val_score(knn, X_train, y_train , cv = 10, scoring = 'precision')\n\nlog_reg_scores_f1 = cross_val_score(log_reg_clf, X_train, y_train , cv = 10, scoring = 'f1')\nsgd_scores_f1 = cross_val_score(sgd_clf, X_train, y_train , cv = 10, scoring = 'f1')\ndtree_scores_f1 = cross_val_score(dtree_clf, X_train, y_train , cv = 10, scoring = 'f1')\nrand_frst_scores_f1 = cross_val_score(rand_frst, X_train, y_train , cv = 10, scoring = 'f1')\nSVC_scores_f1= cross_val_score(svc, X_train, y_train , cv = 10, scoring = 'f1')\nknn_scores_f1= cross_val_score(knn, X_train, y_train , cv = 10, scoring = 'f1')\n\n\nlog_reg_scores_acc = cross_val_score(log_reg_clf, X_train, y_train , cv = 10, scoring = 'accuracy')\nsgd_scores_acc = cross_val_score(sgd_clf, X_train, y_train , cv = 10, scoring = 'accuracy')\ndtree_scores_acc = cross_val_score(dtree_clf, X_train, y_train , cv = 10, scoring = 'accuracy')\nrand_frst_scores_acc = cross_val_score(rand_frst, X_train, y_train , cv = 10, scoring = 'accuracy')\nSVC_scores_acc= cross_val_score(svc, X_train, y_train , cv = 10, scoring = 'accuracy')\nknn_scores_acc= cross_val_score(knn, X_train, y_train , cv = 10, scoring = 'accuracy')\n\nlog_reg_scores_bal_acc = cross_val_score(log_reg_clf, X_train, y_train , cv = 10, scoring = 'balanced_accuracy')\nsgd_scores_bal_acc = cross_val_score(sgd_clf, X_train, y_train , cv = 10, scoring = 'balanced_accuracy')\ndtree_scores_bal_acc = cross_val_score(dtree_clf, X_train, y_train , cv = 10, scoring = 'balanced_accuracy')\nrand_frst_scores_bal_acc = cross_val_score(rand_frst, X_train, y_train , cv = 10, scoring = 'balanced_accuracy')\nSVC_scores_bal_acc= cross_val_score(svc, X_train, y_train , cv = 10, scoring = 'balanced_accuracy')\nknn_scores_bal_acc= cross_val_score(knn, X_train, y_train , cv = 10, scoring = 'balanced_accuracy')","ad53ecff":"scores_df_prec = pd.DataFrame(np.array([log_reg_scores_prec, sgd_scores_prec,dtree_scores_prec,rand_frst_scores_prec, SVC_scores_prec,knn_scores_prec]), columns = [\"Fold_\" + str(i) for i in range(1,11)], index = baseline_models)\nscores_df_prec\n\nscores_df_prec.T.plot(figsize = (18,6))\nplt.xlabel(\"Folds\")\nplt.ylabel(\"Precisions\")\nplt.title(\"Precisions score of Baseline models\")\nplt.show()\n","6f98223b":"scores_df_f1 = pd.DataFrame(np.array([log_reg_scores_f1, sgd_scores_f1,dtree_scores_f1,rand_frst_scores_f1, SVC_scores_f1, knn_scores_f1]), columns = [\"Fold_\" + str(i) for i in range(1,11)], index = baseline_models)\nscores_df_f1\n\nscores_df_f1.T.plot(figsize = (18,6))\nplt.xlabel(\"Folds\")\nplt.ylabel(\"F1 Scores\")\nplt.title(\"F1 score of Baseline models\")\nplt.show()","849d55ab":"scores_df_acc = pd.DataFrame(np.array([log_reg_scores_acc, sgd_scores_acc,dtree_scores_acc,rand_frst_scores_acc, SVC_scores_acc,knn_scores_acc]), columns = [\"Fold_\" + str(i) for i in range(1,11)], index = baseline_models)\nscores_df_acc\n\nscores_df_acc.T.plot(figsize = (18,6))\nplt.xlabel(\"Folds\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy score of Baseline models\")\nplt.show()","886af44b":"scores_df_bal_acc = pd.DataFrame(np.array([log_reg_scores_bal_acc, sgd_scores_bal_acc,dtree_scores_bal_acc,rand_frst_scores_bal_acc, SVC_scores_bal_acc,knn_scores_bal_acc]), columns = [\"Fold_\" + str(i) for i in range(1,11)], index = baseline_models)\nscores_df_acc\n\nscores_df_acc.T.plot(figsize = (18,6))\nplt.xlabel(\"Folds\")\nplt.ylabel(\"Balanced Accuracy\")\nplt.title(\"Balanced Accuracy score of Baseline models\")\nplt.show()","a8074e96":"mean_scores_cv_df = pd.DataFrame(index = baseline_models)\n#mean_scores_cv_df[\"model\"] = baseline_models\nmean_scores_cv_df[\"Precision\"] = [log_reg_scores_prec.mean(),sgd_scores_prec.mean(), dtree_scores_prec.mean(),rand_frst_scores_prec.mean(),SVC_scores_prec.mean(),knn_scores_prec.mean()]\nmean_scores_cv_df[\"F1\"] = [log_reg_scores_f1.mean(),sgd_scores_f1.mean(), dtree_scores_f1.mean(),rand_frst_scores_f1.mean(),SVC_scores_f1.mean(),knn_scores_f1.mean()]\nmean_scores_cv_df[\"Accuracy\"] = [log_reg_scores_acc.mean(),sgd_scores_acc.mean(), dtree_scores_acc.mean(),rand_frst_scores_acc.mean(),SVC_scores_acc.mean(),knn_scores_acc.mean()]\nmean_scores_cv_df[\"Balanced\"] = [log_reg_scores_bal_acc.mean(),sgd_scores_bal_acc.mean(), dtree_scores_bal_acc.mean(),rand_frst_scores_bal_acc.mean(),SVC_scores_bal_acc.mean(),knn_scores_bal_acc.mean()]\nmean_scores_cv_df","cd01065c":"plt.figure(figsize = (8,6))\nsns.heatmap(mean_scores_cv_df, annot = True)\nplt.title(\"Heat Map representation of Mean accuracy scores\", fontsize = 14)\nplt.show()","68798165":"# Let us separate a part of training set as a Hold Out set\nX_ho = X_train[-100:,:]\ny_ho = y_train[-100:]\nX_ho.shape, y_ho.shape","d08560e1":"param_grid = [{\"criterion\":[\"gini\", \"entropy\"],\n               \"max_depth\": [10,50,100],\n               \"min_samples_split\": [10,100,500],\n               \"n_estimators\": [10,100,1000],\n               \"bootstrap\" :[True, False]\n              }]\nrand_frst_grid = GridSearchCV(rand_frst,param_grid, cv = 5, scoring = \"f1\", verbose = 1, n_jobs = -1)\nrand_frst_grid.fit(X_train[:-100,:], y_train[:-100])","de9adb75":"rand_frst_grid.best_estimator_, rand_frst_grid.best_score_","1f6ff1f8":"rand_frst_grid_res = rand_frst_grid.cv_results_\n#rand_frst_grid_res","e3f110a0":"rand_forest_grid_res_df = pd.DataFrame(columns = [\"Params\", \"Scores\"])\nrand_frst_grid_res = rand_frst_grid.cv_results_\n#plt.plot(rand_frst_grid_res[\"params\"], rand_frst_grid_res[\"mean_test_score\"])\nrand_forest_grid_res_df[\"Params\"] = [str(i) for i in rand_frst_grid_res[\"params\"]]\nrand_forest_grid_res_df[\"Scores\"] = rand_frst_grid_res[\"mean_test_score\"].astype(\"float64\")\n\n\n\nfig = plt.figure(figsize = (15, 15))\nax = plt.subplot(111)\nsns.barplot(data = rand_forest_grid_res_df.sort_values(by=\"Scores\" , ascending = False).head(30),x = \"Scores\", y = \"Params\" )\nplt.xticks(rotation = 90)\nplt.show()","bf8e7afd":"rand_forest_grid_res_df.sort_values(by=\"Scores\" , ascending = False)","af8c490e":"from sklearn.metrics import classification_report","b8d27e8c":"estimator = rand_frst_grid.best_estimator_\nestimator.fit(X_train[:-100,:], y_train[:-100])\n\ny_ho_pred = estimator.predict(X_ho)\n\nprint(classification_report(y_ho, y_ho_pred))\ncreate_confusion_matrix_df(y_ho, y_ho_pred)","bd4f1be6":"estimator.fit(X_train, y_train)\ny_pred = estimator.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\ncreate_confusion_matrix_df(y_test, y_pred)","296912a8":"# param_grid = [{\"kernel\": [\"poly\"],\n#                \"C\": [0.001,0.01,0.1,10,100,1000],\n#                \"degree\": [2,3],\n#                \"coef0\": [1]\n#               }]\n# svc_grid = GridSearchCV(svc,param_grid, cv = 5, scoring = \"f1\", verbose = 1, n_jobs = -1)\n# svc_grid.fit(X_train[:-100,:], y_train[:-100])","0abe770c":"svc_grid.best_estimator_, svc_grid.best_score_","8970e8fc":"svc_grid_res_df = pd.DataFrame(columns = [\"Params\", \"Scores\"])\nsvc_grid_res = svc_grid.cv_results_\n#plt.plot(rand_frst_grid_res[\"params\"], rand_frst_grid_res[\"mean_test_score\"])\nsvc_grid_res_df[\"Params\"] = [str(i) for i in svc_grid_res[\"params\"]]\nsvc_grid_res_df[\"Scores\"] = svc_grid_res[\"mean_test_score\"].astype(\"float64\")\n\n\n\nfig = plt.figure(figsize = (15, 15))\nax = plt.subplot(111)\nsns.barplot(data = svc_grid_res_df.sort_values(by=\"Scores\" , ascending = False).head(30),x = \"Scores\", y = \"Params\" )\nplt.xticks(rotation = 90)\nplt.show()","fe91ec02":"svc_grid_res_df.sort_values(by=\"Scores\" , ascending = False)","8e8740fb":"estimator = svc_grid.best_estimator_\nestimator.fit(X_train[:-100,:], y_train[:-100])\n\ny_ho_pred = estimator.predict(X_ho)\n\nprint(classification_report(y_ho, y_ho_pred))\ncreate_confusion_matrix_df(y_ho, y_ho_pred)","9a9d2725":"estimator.fit(X_train, y_train)\ny_pred = estimator.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\ncreate_confusion_matrix_df(y_test, y_pred)","45194fc3":"## Testing the accuracy of the best estimator in Test Dataset","b08e39bd":"## Testing the accuracy of the best estimator in Test Dataset","92d2dcc9":"A much better baseline model. We can expect a very good precision after tuning.","335bf19c":"### Decision Tree Classifier","8c5af50c":"### Splitting the data into Train and Test","427b7da4":"## Importing important libraries","f0d28c13":"### Pair Plotting\nVisualize the separation of each class with the featues","54072cdf":"### KNN classifier","13275606":"### Analysis of Logistic Regression Training Results","433e413f":"Plotting ROC AUC curve for Logistic Regression","e1acc934":"### Observations from Pair Plot\n1. The classes are not linearly separable. So, Linear Classifiers may not fetch accurate results\n2. From the diagnoal hitsogram, it is evident that the mean values of the features are overlapping between the classes ","789e4c64":"### Before we continue with the Modeling, let us decide our selection criteria\n\nClass 0 -> Not Potable\nClass 1 -> Potable\n\n* True Positive -> Actual Potable, Predicted Potable\n* True Negative -> Actual Not Potable, Predicted Not Potable\n* False Positive - > Actual Not Potable, Predictec Potable\n* False Negative -> Actual Potable, Predicted Not Potable\n\nFor Model evaluation, we should choose a metric that -\n* reduces the risk of classifying a Not potable water as Potable(False Positives) \n* should correctly classify Potable water(True Positive).\n\n\nFor now, it matters less if potable water is missclassified as Non Potable(False Negatives)\n\nAn ideal model evaluation metric would be - Precision and specificity\n\nPrecision = True Positives \/ (True Positives + False Positives)\n\nSpecificity = True Negatives \/ (True Negatives + False Positives)\n\nF1-Score can also be used since it takes into account both the precision and recall - Both the False Positives and the False Negatives","9c72fc22":"### Random Forest Classifier","2eb1d994":"I have tuned both the SVC with polynomial kernel with degree 2 and 3 with C value .001 to 1000 and a Random Forest Classifier with different hyperparameters. The Random forest performed better than the SVC. However, SVC had a slightly better training accuracy. The model has a lot of scope for improvements with and ensemble models can also be tried to improve the accuracy scores","ece34116":"## Hyperparameter Tuning - Random Forest","4c91a655":"### Observations\nThere is an imbalance in the data. Non-Potable label has 15% more records than the Potable class.\nSo, we have two options:\n* To undersample the Class 0 - Non Potable class to take only 1250 records, instead of all the 2000 records\n* To user SMOTE to synthesize new records of Class 1 - Potable\n\nLet's use the 1st Option - To undersample the Non Impotable class\n","18789496":"## Hyperparameter Tuning - SVM","28a366bf":"### SGD Classifier","679f171d":"There is no multicollinearity in the feature space as the VIF for all the features are < 5. So, we can consider all the columns for further modeling","3e4929fa":"### Exploratory Data Analysis","3596ae46":"## Data loading","683e31bc":"The percentage of missing values of a columns will help to determine the importance. Drop, if there is a majority of columns have missing values.\nIn the dataset, the \"ph\", \"sulphate\" and \"Trihalomethanes\" contain a few missing values. Since, the % of missing values is very small, we may impute the values.","8efc200e":"### Logistic Regreesion for Classification","4fc7d67e":"### Data Preparation Pipelines","2cce4c9a":"### SVM Classifier","47d2182c":"### Checking for Class Distribution","13e1b7c3":"## Data Descriptive Analysis - Data as is\n\n### Checking for datatypes and missing values","1d730c22":"## Testing the tuned Random Forest Classifier in the hold out dataset"}}