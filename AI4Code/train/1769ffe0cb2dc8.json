{"cell_type":{"0e79a86b":"code","493b04e6":"code","5364ff6e":"code","3aa083e3":"code","a81b676b":"code","5fd0633a":"code","5699b869":"code","69171718":"code","fd29d9f0":"code","b4c85abb":"code","61a3a46e":"code","c4808dbd":"code","e96ddee5":"code","12624f7e":"code","0991de4c":"code","b701a6b5":"code","9a4edc3f":"code","4188e881":"code","eef3b141":"code","07508d80":"code","2213108c":"code","fae246fb":"code","14d4eb10":"markdown","aaf2399e":"markdown"},"source":{"0e79a86b":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization, Conv1D, MaxPooling1D, AveragePooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D, Lambda, Flatten\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.callbacks import Callback, EarlyStopping\nimport keras.backend as K\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom keras.regularizers import l1, l2, l1_l2\nimport warnings\nfrom sklearn import metrics\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom imblearn.keras import BalancedBatchGenerator\n\nimport time\nimport gc\n\n# Charts\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","493b04e6":"import tflearn\n\ndef roc_loss(y_true, y_pred):\n    return tflearn.objectives.roc_auc_score(y_pred, y_true)\n\nalpha=0.25\ngamma=2.0\n    \ndef focal_loss(y_true, y_pred):\n    bce = K.binary_crossentropy(y_true, y_pred)\n\n    y_pred = K.clip(y_pred, K.epsilon(), 1.- K.epsilon())\n    p_t = (y_true*y_pred) + ((1-y_true)*(1-y_pred))\n    alpha_t =  y_true*alpha + ((1-alpha)*(1-y_true))\n\n    return K.mean(alpha_t*K.pow((1-p_t), gamma)*bce, axis=-1)","5364ff6e":"epochs = 1000\n\nbatch_size = 1000\npatience = 10\nscaler = StandardScaler()\nexpand_data = True\nn_splits = 10\n\nmax_evals = 5\n\nmodel_type = \"conv1d\"\nconv_activation = \"relu\"\nbatch_normalization = True\ndropout = True\n\nimport featuretools as ft\ntrans_primitives = ['divide_by_feature']","3aa083e3":"from sklearn.preprocessing import power_transform\n\ntrain_data = pd.read_csv('..\/input\/train.csv')\n\ndef rank_gauss(data):\n    df = pd.DataFrame()\n    for i in data.columns:\n        df[i] = rank_gauss_element(data[i].values)\n    return df\n\ndef rank_gauss_element(x):\n    from scipy.special import erfinv\n    N = x.shape[0]\n    temp = x.argsort()\n    rank_x = temp.argsort() \/ N\n    rank_x -= rank_x.mean()\n    rank_x *= 2\n    efi_x = erfinv(rank_x)\n    efi_x -= efi_x.mean()\n    return efi_x\n\ndef features_preprocessing(data):\n    if expand_features:\n        features = expand_features(data)\n        #features = deep_feature_synthesis(data)\n    else:\n        features = data\n    return scale_features(features)\n\ndef deep_feature_synthesis(data):\n    # Deep Feature Synthesis\n    columns = [c for c in data.columns if c not in ['target']]\n    es = ft.EntitySet(id = 'transactions')\n    es.entity_from_dataframe(entity_id = 'data', \n                             dataframe = data[columns], \n                             index = 'ID_code')\n    \n    feature_matrix, feature_names = ft.dfs(entityset=es, \n                   target_entity = 'data', \n                   max_depth = 1,\n                   agg_primitives = [],\n                   trans_primitives = trans_primitives,\n                   verbose = 1,\n                   n_jobs = -1,\n                   chunk_size = 100)\n    \n    return feature_matrix\n    \ndef scale_features(data):\n    columns = [c for c in data.columns if c not in ['ID_code', 'target']]\n    if scaler is not None:\n        if scaler == \"rank_gauss\":\n            return rank_gauss(data[columns])\n        else:\n            return pd.DataFrame(scaler.fit_transform(data[columns]), columns=columns)\n    else:\n        return data[columns]\n\ndef expand_features(train_features):\n    features = [c for c in train_features.columns if c not in ['ID_code', 'target']]\n    for feature in features:\n        train_features['mean_'+feature] = (train_features[feature].mean()-train_features[feature])\n        train_features['z_'+feature] = (train_features[feature] - train_features[feature].mean())\/train_features[feature].std(ddof=0)\n        train_features['sq_'+feature] = (train_features[feature])**2\n        train_features['sqrt_'+feature] = (train_features['sq_'+feature])**(1\/4)\n        train_features['log_'+feature] = np.log(train_features['sq_'+feature]+10)\/2\n        \n    return train_features","a81b676b":"preprocess_train_data = features_preprocessing(train_data)\n\nnumber_features = len(preprocess_train_data.columns)\n\nX_train, X_test, y_train, y_test = train_test_split(preprocess_train_data, train_data['target'], stratify=train_data['target'], train_size = 0.9, test_size = 0.1, random_state = 1337)\n\npreprocess_train_data.describe()","5fd0633a":"def class_weights(y) :\n    return dict(enumerate(compute_class_weight(\"balanced\", np.unique(y), y)))","5699b869":"import sklearn\n\ndef roc_chart(fpr, tpr, auc_score, label):\n    plt.figure(1)\n    plt.plot(fpr, tpr, label='{0} (area = {1:.3f})'.format(label, auc_score))\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve')\n    plt.legend(loc='best')\n    plt.show()\n    \ndef roc_calculation_chart(x, y, label):\n    y_pred = model.predict(x)\n    roc_score = sklearn.metrics.roc_auc_score(y, y_pred)\n    fpr, tpr, thresholds = sklearn.metrics.roc_curve(y, y_pred)\n    roc_chart(fpr, tpr, roc_score, label)\n    \ndef roc_calculation(model, x_train, y_train, x_test, y_test):\n    roc_calculation_chart(x_train, y_train, \"roc-auc\")\n    roc_calculation_chart(x_test, y_test, \"val-roc-auc\")\n\nclass roc_callback(Callback):\n    def __init__(self,training_data,validation_data):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred = self.model.predict(self.x)\n        roc = sklearn.metrics.roc_auc_score(self.y, y_pred)\n        y_pred_val = self.model.predict(self.x_val)\n        roc_val = sklearn.metrics.roc_auc_score(self.y_val, y_pred_val)\n        print('\\rroc-auc: %s - roc-auc_val: %s' % (str(round(roc,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return","69171718":"def add_dense_layer(model, dense_size = 1024, dense_activation = 'relu', dropout_percentage = 0.5, kernel_regularizer = None, batch_normalization = False, dropout = False, hasInput = False):\n    layer_name = 'dense_' + str(dense_size)\n    if hasInput:\n        model.add(Dense(dense_size, activation=dense_activation, kernel_regularizer = kernel_regularizer, name=layer_name, input_shape=(number_features,)))\n    else:\n        model.add(Dense(dense_size, activation=dense_activation, kernel_regularizer = kernel_regularizer, name=layer_name))\n    model.add(BatchNormalization()) if (batch_normalization) else False \n    model.add(Dropout(dropout_percentage)) if (dropout) else False\n\ndef add_conv1d_layer(model, filter_size, batch_normalization, dropout, hasInput, number_per_layer_conv, dropout_percentage, kernel_size, pooling): \n    if hasInput:\n        layer_name = 'conv1d_' + str(filter_size) + '_input_layer'\n        model.add(Conv1D(filters = filter_size, kernel_size = (kernel_size), padding = 'Same', activation = conv_activation, name=layer_name, input_shape=(number_features,1))) \n        model.add(BatchNormalization()) if (batch_normalization) else False \n        model.add(Dropout(dropout_percentage)) if (dropout) else False\n        number_per_layer_conv -= 1\n    \n    for layer_number in range(number_per_layer_conv):\n        layer_name = 'conv1d_' + str(filter_size) + '_' + str(layer_number + 1)\n        model.add(Conv1D(filters = filter_size, kernel_size = (kernel_size), padding = 'Same', activation = conv_activation, name=layer_name))\n        model.add(BatchNormalization()) if (batch_normalization) else False   \n        model.add(Dropout(dropout_percentage)) if (dropout) else False\n    \n    model.add(pooling(pool_size=(2),strides=(2)))\n    model.add(BatchNormalization()) if (batch_normalization) else False   \n    model.add(Dropout(dropout_percentage)) if (dropout) else False\n    \ndef add_all_conv1d_layers(model, number_conv1d_layers, number_dense_layers, number_per_layer_conv, first_dense_layer, first_filter_layer, dense_activation, dropout_percentage, kernel_regularizer, batch_normalization, dropout, kernel_size, pooling, global_pooling):\n    first_layer = first_filter_layer\n    for layer_number in range(number_conv1d_layers):\n        add_conv1d_layer(model, int(first_layer), False, False, first_layer == first_filter_layer, number_per_layer_conv, dropout_percentage, kernel_size, pooling)\n        first_layer *= 2\n    \n    if (number_conv1d_layers > 0):\n        model.add(global_pooling())\n        #model.add(BatchNormalization()) if (batch_normalization) else False  \n        model.add(Dropout(dropout_percentage)) if (dropout) else False\n    \n    add_all_dense_layers(model, number_dense_layers, first_dense_layer, dense_activation, dropout_percentage, kernel_regularizer, batch_normalization, dropout, number_conv1d_layers <= 0)\n    \n    \ndef add_all_dense_layers(model, number_layers = 5, first_dense_layer = 512, dense_activation = 'relu', dropout_percentage = 0.5, kernel_regularizer = None, batch_normalization = False, dropout = False, addInput = True):\n    first_layer = first_dense_layer\n    for layer_number in range(number_layers):\n        add_dense_layer(model, int(first_layer), dense_activation, dropout_percentage, kernel_regularizer, batch_normalization, dropout, first_layer == first_dense_layer and addInput)\n        first_layer \/= 2\n\ndef create_dense_model(model):\n    add_all_dense_layers(model, number_of_dense_layers, kernel_regularizer, batch_normalization, dropout)\n\ndef create_conv1d_model(model, number_of_dense_layers, number_of_conv1d_layers, number_per_layer_conv, first_dense_layer, first_filter_layer, dense_activation, dropout_percentage, kernel_size, kernel_regularizer, pooling, global_pooling):\n    if (number_of_conv1d_layers > 0):\n        model.add(Lambda(lambda i: K.expand_dims(i, axis=2), input_shape=(number_features,)))\n    add_all_conv1d_layers(model, number_of_conv1d_layers, number_of_dense_layers, number_per_layer_conv, first_dense_layer, first_filter_layer, dense_activation, dropout_percentage, kernel_regularizer, batch_normalization, dropout, kernel_size, pooling, global_pooling)\n\ndef create_model(number_of_dense_layers, number_of_conv1d_layers, number_per_layer_conv, first_dense_layer, first_filter_layer, dense_activation, dropout_percentage, kernel_size, kernel_regularizer, pooling, global_pooling):\n    model = Sequential()\n    if model_type == 'dense':\n        create_dense_model(model)\n    else:\n        create_conv1d_model(model, number_of_dense_layers, number_of_conv1d_layers, number_per_layer_conv, first_dense_layer, first_filter_layer, dense_activation, dropout_percentage, kernel_size, kernel_regularizer, pooling, global_pooling)\n    \n    return model\n\ndef build_model(number_of_dense_layer, \n                number_of_conv1d_layers, \n                number_per_layer_conv,\n                first_dense_layer,\n                first_filter_layer,\n                dense_activation,\n                dropout_percentage,\n                kernel_size,\n                kernel_regularizer,\n                pooling,\n                global_pooling,\n                optimizer,\n                loss):\n    model = create_model(number_of_dense_layer, number_of_conv1d_layers, number_per_layer_conv, first_dense_layer, first_filter_layer, dense_activation, dropout_percentage, kernel_size, kernel_regularizer, pooling, global_pooling)\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid', name='binary_classification'))\n    \n    model.compile(loss=loss['function'], optimizer=optimizer, metrics=[roc_loss,'acc',focal_loss])\n    \n    return model\n\ndef train_model(model, X_train, X_test, y_train, y_test, batch_size, epochs, weights):\n    validation_steps = int(len(X_test) \/ batch_size) + 1\n    steps_per_epoch = int(len(X_train) \/ batch_size) + 1\n    training_generator = BalancedBatchGenerator(X_train, y_train,\n                                                batch_size=batch_size,\n                                                random_state=42)\n    earlystopper = EarlyStopping(monitor='val_focal_loss', patience=patience, verbose=1, restore_best_weights=True, mode='auto')\n    roc = roc_callback(training_data=(X_train, y_train),validation_data=(X_test, y_test))\n    return model.fit_generator(generator=training_generator, \n                              validation_data=(X_test,y_test),\n                              callbacks=[earlystopper],\n                              epochs=epochs, \n                              verbose=0,\n                              class_weight = weights,\n                              steps_per_epoch=steps_per_epoch,\n                              validation_steps=validation_steps)","fd29d9f0":"from sklearn.model_selection import StratifiedShuffleSplit\n\ndef train_k_folds(model, epochs, batch_size, weights, n_splits, train_features, train_targets):\n    sss = StratifiedShuffleSplit(n_splits=n_splits)\n    fold_number = 0\n    for train_index, test_index in sss.split(train_features, train_targets):\n        start_time = time.time()\n        X_train, X_test = train_features[train_index], train_features[test_index]\n        y_train, y_test = train_targets[train_index], train_targets[test_index]\n        \n        training = train_model(model, X_train, X_test, y_train, y_test, batch_size, epochs, weights)\n        \n        eval_model(model, training, X_test, y_test)\n        test_pred = model.predict_classes(X_test)\n        eval_accuracy(y_test, test_pred)\n        \n        del X_train, X_test, y_train, y_test, training, test_pred\n        gc.collect()\n        elapsed_time = time.time() - start_time\n        print(\"Fold {0} run in {1}\".format(fold_number + 1, elapsed_time))\n        fold_number += 1\n    \n    return","b4c85abb":"def eval_model(model, training, X_test, y_test):\n    \"\"\"\n    Model evaluation: plots, classification report\n    @param training: model training history\n    @param model: trained model\n    \"\"\"\n    if training is not None:\n        ## Trained model analysis and evaluation\n        f, ax = plt.subplots(3,1, figsize=(10,10))\n        ax[0].plot(training.history['loss'], label=\"Loss\")\n        ax[0].plot(training.history['val_loss'], label=\"Validation loss\")\n        ax[0].set_title('Loss')\n        ax[0].set_xlabel('Epoch')\n        ax[0].set_ylabel('Loss')\n        ax[0].legend()\n\n        # Accuracy\n        ax[1].plot(training.history['acc'], label=\"Accuracy\")\n        ax[1].plot(training.history['val_acc'], label=\"Validation accuracy\")\n        ax[1].set_title('Accuracy')\n        ax[1].set_xlabel('Epoch')\n        ax[1].set_ylabel('Accuracy')\n        ax[1].legend()\n\n        # Roc Loss\n        ax[2].plot(training.history['roc_loss'], label=\"Roc Loss\")\n        ax[2].plot(training.history['val_roc_loss'], label=\"Validation roc loss\")\n        ax[2].set_title('Roc Loss')\n        ax[2].set_xlabel('Epoch')\n        ax[2].set_ylabel('Roc Loss')\n        ax[2].legend()\n        plt.tight_layout()\n        plt.show()\n    \n    test_res = model.evaluate(X_test, y_test, verbose=0)\n    for i, model_name in enumerate(model.metrics_names):\n        print('{0}: {1}'.format(model_name, test_res[i]))\n\n    return test_res\n\ndef eval_accuracy(test_truth, test_pred):\n    # Print metrics\n    print(\"Classification report\")\n    \n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        print(metrics.classification_report(test_truth, test_pred))","61a3a46e":"from hyperopt import hp, fmin, tpe, hp, STATUS_OK, Trials, space_eval\n\nspace = { \n    'number_of_dense_layers': hp.choice('number_of_dense_layers',[3]),\n    'number_of_conv1d_layers': hp.choice('number_of_conv1d_layers',[2,3]),\n    'number_per_layer_conv': hp.choice('number_per_layer_conv',[2]),\n    'first_dense_layer': hp.choice('first_dense_layer', [1024, 512]),\n    'first_filter_layer': hp.choice('first_filter_layer', [32, 64]),\n    'dense_activation': hp.choice('dense_activation', ['tanh']),\n    'dropout_percentage': hp.choice('dropout_percentage',[0.75, 0.90]),\n    'kernel_size': hp.choice('kernel_size', [3,5]),\n    'regularizer': hp.choice('regularizer', [l1_l2(0.01)]),\n    'pooling': hp.choice('pooling', [AveragePooling1D]),\n    'global_pooling': hp.choice('global_pooling', [Flatten]),\n    'optimizer': hp.choice('optimizer', [RMSprop(lr=1e-4)]),\n    'loss': hp.choice('loss', [{ 'name': 'focal_loss', 'function': focal_loss }])\n}","c4808dbd":"def objective(params):\n    print(params)\n    model = build_model(\n        params['number_of_dense_layers'],\n        params['number_of_conv1d_layers'],\n        params['number_per_layer_conv'],\n        params['first_dense_layer'],\n        params['first_filter_layer'],\n        params['dense_activation'],\n        params['dropout_percentage'],\n        params['kernel_size'],\n        params['regularizer'],\n        params['pooling'],\n        params['global_pooling'],\n        params['optimizer'],\n        params['loss']\n    )\n    train_k_folds(model, epochs, batch_size, weights, n_splits, X_train.values, y_train.values)\n    \n    loss = eval_model(model, None, X_test.values, y_test.values)\n    \n    y_pred = model.predict(X_test)\n    roc_score = sklearn.metrics.roc_auc_score(y_test, y_pred)\n    \n    del y_pred\n    gc.collect()\n    \n    return {'loss': loss[model.metrics_names.index(params['loss']['name'])], 'status': STATUS_OK, 'roc_score': roc_score, 'best_model': model}","e96ddee5":"weights = class_weights(train_data['target'].values)\nprint(weights)","12624f7e":"trials = Trials()\n\nbest = fmin(objective, space, algo=tpe.suggest, trials=trials, max_evals=max_evals)","0991de4c":"print (trials.best_trial['result'])\nprint(best)\nprint(space_eval(space, best))\nmodel = trials.best_trial['result']['best_model']","b701a6b5":"params = dict()\nparams['number_of_dense_layers'] = 2\nparams['number_of_conv1d_layers'] = 2\nparams['number_per_layer_conv'] = 2\nparams['first_dense_layer'] = 512\nparams['first_filter_layer'] = 16\nparams['dense_activation'] = 'relu'\nparams['dropout_percentage'] = 0.50\nparams['kernel_size'] = 3\nparams['regularizer'] = l2(0.01)\nparams['pooling'] = MaxPooling1D\nparams['global_pooling'] = Flatten\nparams['optimizer'] = Adam(lr=1e-4)\nparams['loss'] = { 'name': 'focal_loss', 'function': focal_loss }\n\nmodel = build_model(\n        params['number_of_dense_layers'],\n        params['number_of_conv1d_layers'],\n        params['number_per_layer_conv'],\n        params['first_dense_layer'],\n        params['first_filter_layer'],\n        params['dense_activation'],\n        params['dropout_percentage'],\n        params['kernel_size'],\n        params['regularizer'],\n        params['pooling'],\n        params['global_pooling'],\n        params['optimizer'],\n        params['loss']\n    )\n#training = train_model(model, X_train.values, X_test.values, y_train.values, y_test.values, batch_size, epochs, None)","9a4edc3f":"model.summary()","4188e881":"eval_model(model, None, X_test.values, y_test.values)","eef3b141":"test_pred = model.predict_classes(X_test)\neval_accuracy(y_test, test_pred)","07508d80":"roc_calculation(model, X_train, y_train, X_test, y_test)","2213108c":"del X_train, X_test, y_train, y_test, train_data, preprocess_train_data\ngc.collect()","fae246fb":"test_data = pd.read_csv('..\/input\/test.csv')\nX_predictions = test_data.drop(columns=['ID_code'], axis=1)\npredictions = model.predict(features_preprocessing(X_predictions))\nfile_submission = pd.DataFrame(test_data['ID_code'])\nfile_submission['target'] = predictions\nfile_submission.to_csv('santander_predictions.csv', index=False)","14d4eb10":"# Links\n\n* https:\/\/machinelearningmastery.com\/evaluate-performance-deep-learning-models-keras\/\n* https:\/\/stackoverflow.com\/questions\/41032551\/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n* https:\/\/github.com\/keras-team\/keras\/issues\/1732\n* https:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets\n* https:\/\/imbalanced-learn.readthedocs.io\/en\/latest\/auto_examples\/applications\/porto_seguro_keras_under_sampling.html\n* https:\/\/www.kaggle.com\/karangautam\/keras-nn\n* https:\/\/www.kaggle.com\/mathormad\/knowledge-distillation-with-nn-rankgauss","aaf2399e":"train_k_folds(model, epochs, batch_size, weights, n_splits, X_train.values, y_train.values)"}}