{"cell_type":{"eee8da31":"code","30704db0":"code","71ee7825":"code","bda410c4":"code","26065327":"code","421d87e5":"code","7e37d6ab":"code","e4408b82":"code","4c57d073":"code","c05e15dc":"code","edc7c45b":"code","b6e36c2d":"code","bf99880e":"code","ce4e619e":"code","35a33002":"code","d5441991":"code","7d387107":"code","1e9b83d1":"code","1560d744":"code","e6f5da51":"code","e2820bb5":"markdown","06bbffe4":"markdown","65143432":"markdown","8006c055":"markdown","5f100cea":"markdown","9c89f806":"markdown","41410b36":"markdown","c92c2212":"markdown","1700fb06":"markdown","ba476c4d":"markdown","5b770a15":"markdown","d005cf59":"markdown","22b1d753":"markdown","9f6c1a16":"markdown","054ce307":"markdown","0fcd62ec":"markdown","69c5544e":"markdown","e2677f05":"markdown"},"source":{"eee8da31":"# Dependencies\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport inspect\n\nfrom transformers import (\n    AdamW,\n    RobertaModel,\n    RobertaForMaskedLM,\n    RobertaTokenizer,\n    modeling_roberta,\n)\n\nfrom collections import defaultdict, OrderedDict\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import Tensor\nfrom torch import nn\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\nimport argparse, collections, json, re, time, sys, pickle, os, random, math\nimport torch.utils.data as data\nfrom torch.autograd import Variable\nimport keyword\nfrom torch.utils.tensorboard import SummaryWriter\nfrom itertools import permutations \nimport itertools\nfrom IPython.display import Image\nimport spacy\n\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" # so the IDs match nvidia-smi\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # \"0, 1\" for multiple","30704db0":"Image(filename='\/kaggle\/input\/trialsdata\/50years_antiviral_history.png', width=1200, height=800)","71ee7825":"Image(filename='\/kaggle\/input\/trialsdata\/approveddrugs.png', width=1200, height=800)\n# In particular, get data on approved flu drugs","bda410c4":"def extract_candidates(df):\n    candidates  = [item.split('|') for item in df['Interventions'] if type(item) is str]\n    cdrugs = [item.split(':')[1].lstrip() for subl in candidates for item in subl if item.split(':')[0] in 'Drug']\n    # remove duplicates and nondrugs\n    cdrugs = [item for item in cdrugs if not 'placebo' in item.lower()]\n    cdrugs = [item.split()[np.argmax([len(iota) for iota in item.split()])] if 'mg' in item.lower() else item for item in cdrugs]\n    cdrugs = [item.split(',')[0] for item in cdrugs]\n    dupes = [('Symmetrel', 'Amantadine'), ('Copegus', 'Rebetol', 'Virazole', 'Ribavirin'), ('Flumadine', 'Rimantadine'), ('Relenza', 'Zanamivir'), ('(Tamiflu\u00ae)', '[Tamiflu]', 'Tamiflu', 'Oseltamivir'), ('Inavir', 'Laninamivir octanoate'), ('Rapivab', 'Peramivir'), ('Avigan', 'Favipiravir')]\n    cdrugs = [generic[-1] if generic[-1] in item.split() else item for item in cdrugs for generic in dupes]\n    cdrugs = ['Oseltamivir' if 'oseltamivir' in item.lower().split() else item for item in cdrugs ]    \n    cdrugs = [drug for drug in cdrugs if 'vaccine' not in drug.lower()]\n    cdrugs = list(set(cdrugs))\n    return cdrugs\n\n# grab candidates from clinicaltrials.gov read all csvs\nrootfid = '\/kaggle\/input\/trialsdata\/'\nyeardrug={}\nfor fid in os.listdir(rootfid):\n    if fid[:3] in 'flu' and fid[-4:] in '.csv':\n        yeardrug[fid.split('flu')[1][:4]] = pd.read_csv(rootfid+fid)\n\ncdrugyear={}\nfor k,v in yeardrug.items():\n    cdrugyear[k] = extract_candidates(v)\n\napproved09 = ['Amantadine', 'Ribavirin', 'Rimantadine', 'Zanamivir', 'Oseltamivir']\napproved09 = [drug.lower() for drug in approved09]\nundisdrug09 = ['Laninamivir', 'Peramivir', 'Favipiravir', 'Baloxavir'] # Baloxavir approved in 2018, doesn't appear in the dataset\nundisdrug09 = [drug.lower() for drug in undisdrug09]\nundisdrug13 = undisdrug09[1:]\napproved13 = approved09 + ['laninamivir']    \nnumcands = [len(cdrugyear[str(year)]) for year in range(2006,2015)]\napproveddrugs = [5, 5, 5, 5, 6, 6, 6, 6, 8]\n\nwidth = 1.0     # gives histogram aspect to the bar diagram \nplt.rcParams['figure.figsize'] = [10, 5]\n\nfig, ax = plt.subplots()\nax.bar(range(2006,2015), numcands, width, color='g')\nplt.ylabel('Number of Candidate Drugs in the USA')\nplt.title('Candidate Influenza Drugs Trialed Before Years End')\nax.plot(range(2006,2015), approveddrugs, 'k-', label ='Total Influenza Drugs granted approval')\nlegend = ax.legend(loc='upper left', shadow=True, fontsize='x-large')\nfig.autofmt_xdate()\nplt.show()","26065327":"## Let's introduce our method, \nImage(filename='\/kaggle\/input\/trialsdata\/roberta_arch.png', width=1200, height=800)","421d87e5":"# We work from the 3\/20 release of the full text dataset with 44220 articles.\n# Data prep is straightforward, change below path to \/kaggle\/input\/ if running on competition machines. Glob doesn't seem to work quite right on competition machines.\nfrom tqdm import tqdm\nfrom glob import glob\ndataroot = '\/kaggle\/input\/CORD-19-research-challenge\/'\n\ndef readjsonbody(fid):\n    jjson = json.load(open(fid, 'r'))\n    return [text['text'] for text in jjson['body_text']]\n\n# read in json texts\npassage = []\nfor fid in tqdm(glob(dataroot+'*\/*\/*.json')):\n    try:\n        passage += readjsonbody(fid)\n    except:\n        print('Whiffed on', fid)\n\n# now write to a text line and 80% for train 20% for test, for language modeling task\nrandom.seed(4)\nrandom.shuffle(passage)\n\nsavefid = '\/kaggle\/working\/covidpassages.txt'\nwith open(savefid, 'w') as f:\n    for idx in tqdm(range(0,int(len(passage)*.8))):\n        f.writelines(passage[idx]+'\\n')\n        \ntestfid = '\/kaggle\/working\/testcovidpassages.txt'        \nwith open(testfid, 'w') as f:\n    for idx in tqdm(range(int(len(passage)*.8), int(len(passage))) ):\n        f.writelines(passage[idx]+'\\n')\n\n# Run the below on a good machine with at least 32GB per GPU\n# !python \/home\/transformers\/examples\/run_language_modeling.py \\\n#     --output_dir=covidrobertamodel \\\n#     --model_type=roberta \\\n#     --model_name_or_path=roberta-large \\\n#     --do_train \\\n#     --train_data_file=\/kaggle\/working\/covidpassages.txt \\\n#     --do_eval \\\n#     --eval_data_file=\/kaggle\/working\/testcovidpassages.txt \\\n#     --mlm --save_steps 10000 --max_steps 100000 --per_gpu_train_batch_size 4\n!cat \/kaggle\/input\/covidrobertamodel\/eval_results.txt","7e37d6ab":"class roberta_score(nn.Module):\n    def __init__(self, roberta_model = '\/kaggle\/input\/covidrobertamodel\/', dataset=None):\n        super(roberta_score, self).__init__()        \n        self.outmodel = RobertaForMaskedLM.from_pretrained(roberta_model)\n        \n    def forward(self, word_id, word_mask, target_id, target_mask):\n        m = nn.Softmax(dim=2)\n        output_embeds = self.outmodel(word_id, token_type_ids=None, attention_mask = word_mask)\n        soft_embeds = m(output_embeds[0])        \n        seqscore = torch.sum(soft_embeds.detach()[:,:,target_id], dim=2)        \n        score = torch.sum(seqscore, dim=1)        \n        score = score \/ (np.count_nonzero(target_id)*np.count_nonzero(word_id))        \n        return score, seqscore","e4408b82":"modeldir = '\/kaggle\/input\/covidrobertamodel\/'\n\nconfig = modeling_roberta.RobertaConfig.from_json_file(modeldir+'config.json')\nmodel = RobertaForMaskedLM(config)\ntokenizer = RobertaTokenizer.from_pretrained(modeldir)\ndicts=model.load_state_dict(torch.load(modeldir+\"pytorch_model.bin\",map_location ='cpu'))\ndef transpose_for_scores(config, x):\n    new_x_shape = x.size()[:-1] + (config.num_attention_heads, int(config.hidden_size \/ config.num_attention_heads))\n    x = x.view(*new_x_shape)\n    return x.permute(0, 2, 1, 3)\n\ndef get_attention_scores(model,i,text):\n    tokenized=tokenizer.tokenize(text)\n\n    indexed_tokens=tokenizer.convert_tokens_to_ids(tokenized)\n\n    segment_ids=[0]*len(indexed_tokens)\n    t_tensor=torch.tensor([indexed_tokens])\n    s_ids=torch.tensor([segment_ids])\n\n    outputs_query= []\n    outputs_key= []\n\n    def hook_query(module, input, output):\n        #print ('in query')\n        outputs_query.append(output)\n\n    def hook_key(module, input, output):\n        #print ('in key')\n        outputs_key.append(output)\n\n    model.roberta.encoder.layer[i].attention.self.query.register_forward_hook(hook_query)\n    model.roberta.encoder.layer[i].attention.self.key.register_forward_hook(hook_key)\n    l=model(t_tensor,s_ids)\n    \n    query_layer = transpose_for_scores(config,outputs_query[0])\n    key_layer = transpose_for_scores(config,outputs_key[0])\n    \n    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n    attention_scores = attention_scores \/ math.sqrt(int(config.hidden_size \/ config.num_attention_heads))\n    attention_probs = nn.Softmax(dim=-1)(attention_scores)\n    \n    return attention_probs,tokenized\n\ntext = 'favipiravir triphosphate shows broad-spectrum inhibitory activities against the RNA polymerases of influenza'\nx,tokens=get_attention_scores(model.cpu(),-24,text) # second term is layer\nmap1=np.asarray(x[0][1].detach().numpy())\n# reduction across favipiravir tokens\nfavrow = np.sum(map1[:10, :], axis =0)\nfavrow = np.expand_dims(favrow, axis=0)\nmap2 = np.concatenate((favrow, map1[10:,:]), axis=0)\nfavcol = np.sum(map2[:,:10], axis=1)\nfavcol = np.expand_dims(favcol, axis=1)\nmap3 = np.concatenate((favcol, map2[:,10:]),axis=1)\ntok= ['favipiravir']+tokens[10:]\nnp.shape(map1[:10, :]), tokens[:10], np.shape(favrow), np.shape(map2), np.shape(map3), tok\n# normalize\nmap3[:,0] = map3[:,0]\/3.16\nmap3[0,:] = map3[0,:]\/3.16\n\nplt.clf()\nf=plt.figure(figsize=(10,10))\nax = f.add_subplot(1,1,1)\ni=ax.imshow(map3,interpolation='nearest',cmap='gray')\nax.set_yticks(range(len(tok)))\nax.set_yticklabels([token.replace('\u0120','') for token in tok])\nax.set_xticks(range(len(tok)))\nax.set_xticklabels([token.replace('\u0120','') for token in tok],rotation=60)\nax.set_xlabel('key')\nax.set_ylabel('query')\nax.grid(linewidth = 0.8)\n\n\ndef plot_attention(attmap, tokens):\n    plt.clf()\n    f=plt.figure(figsize=(10,10))\n    ax = f.add_subplot(1,1,1)\n    i=ax.imshow(attmap,interpolatlsion='nearest',cmap='gray')\n    ax.set_yticks(range(len(tokens)))\n    ax.set_yticklabels([token.replace('\u0120','') for token in tokens])\n    ax.set_xticks(range(len(targettokens)))\n    ax.set_xticklabels([token.replace('\u0120','') for token in targettokens], rotation=60)\n    ax.set_xlabel('key')\n    ax.set_ylabel('query')","4c57d073":"def get_query_target_scores(model,i, text, target):\n    tokenized=tokenizer.tokenize(text)\n    indexed_tokens=tokenizer.convert_tokens_to_ids(tokenized)    \n    segment_ids=[0]*len(indexed_tokens)\n    t_tensor=torch.tensor([indexed_tokens])\n    s_ids=torch.tensor([segment_ids])\n\n    targettokenized=tokenizer.tokenize(target)\n    targetindexed_tokens=tokenizer.convert_tokens_to_ids(targettokenized)    \n    targetsegment_ids=[0]*len(targetindexed_tokens)\n    targett_tensor=torch.tensor([targetindexed_tokens])\n    targets_ids=torch.tensor([targetsegment_ids])\n        \n    outputs_query= []\n    outputs_key= []\n\n    def hook_query(module, input, output):\n        #print ('in query')\n        outputs_query.append(output)\n\n    def hook_key(module, input, output):\n        #print ('in key')\n        outputs_key.append(output)\n\n    model.roberta.encoder.layer[i].attention.self.query.register_forward_hook(hook_query)\n    model.roberta.encoder.layer[i].attention.self.key.register_forward_hook(hook_key)\n    l=model(t_tensor,s_ids)\n    \n    query_layer = transpose_for_scores(config,outputs_query[0])\n    key_layer = transpose_for_scores(config,outputs_key[0])\n\n    t=model(targett_tensor, targets_ids)\n    targetquery_layer = transpose_for_scores(config,outputs_query[0])\n    targetkey_layer = transpose_for_scores(config,outputs_key[0])\n    attention_scores = torch.matmul(query_layer, targetkey_layer[:,:,:len(targetindexed_tokens),:].transpose(-1, -2))\n    attention_scores = attention_scores \/ math.sqrt(int(config.hidden_size \/ config.num_attention_heads))\n    attention_probs = nn.Softmax(dim=-1)(attention_scores)\n    \n    return attention_probs, tokenized, targettokenized\n\ntext = 'favipiravir'\ntarget = 'efficacy and side effects'\nx, tokens, targettokens = get_query_target_scores(model.cpu(), -2, text, target)\nmap1=np.asarray(x[0][1].detach().numpy())\nfavrow = np.sum(map1[:10, :], axis =0)\nfavrow = np.expand_dims(favrow, axis=0)\nmap2 = np.concatenate((favrow, map1[10:,:]), axis=0)\nfavcol = np.sum(map2[:,1:], axis=1)\nfavcol = np.expand_dims(favcol, axis=1)\nmap3 = np.concatenate((favcol, map2[:,2:]),axis=1)\ntok= ['favipiravir']+tokens[10:]\n\nplt.clf()\nf=plt.figure(figsize=(5,5))\nax = f.add_subplot(1,1,1)\ni=ax.imshow(map3,interpolation='nearest',cmap='gray')\nax.set_yticks(range(len(tok)))\nax.set_yticklabels([token.replace('\u0120','') for token in tok])\nax.set_xticks(range(len(targettokens)-1))\ntargetlabels = ['efficacy']\ntargetlabels += targettokens[2:]\ntargetlabels = [token.replace('\u0120','') for token in targetlabels]\nax.set_xticklabels(targetlabels,rotation=60)","c05e15dc":"class roberta_passage_score(nn.Module):\n    # Score a source phrase and target phrase for an analogy\/prediction score\n    def __init__(self, roberta_model = '\/kaggle\/input\/covidrobertamodel', dataset=None):\n        super(roberta_passage_score, self).__init__()        \n        self.outmodel = RobertaForMaskedLM.from_pretrained(roberta_model)\n        \n    def forward(self, word_id, word_mask, target_id, target_mask):\n        output_embeds = self.outmodel(word_id, token_type_ids=None, attention_mask = word_mask)\n        # index by target word ids, selecting those probabilities for the sequences\n        seqscore = torch.mean(output_embeds[0].detach()[:,:,target_id], dim=2)\n        # average all sequence scores\n        score = torch.mean(seqscore, dim=1)\/np.sqrt(np.sum(word_mask.detach().cpu().numpy()))\n        return score, seqscore\n    \nmax_seq_length = 128\n# testing analogy now\ndef convert_text_to_score_input(text):\n    tokenized_text = tokenizer.tokenize(text)\n    if len(tokenized_text) > max_seq_length:\n        tokenized_text = tokenized_text[:max_seq_length]\n    token_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n    padding = [0] * (max_seq_length - len(token_ids))\n    token_ids += padding\n    tokensid_tensor = torch.tensor(token_ids)\n\n    idxes = torch.arange(0,max_seq_length,out=torch.LongTensor(max_seq_length)).unsqueeze(0)\n    mask = Variable((idxes<len(tokenized_text)).float())\n    return tokensid_tensor.unsqueeze(0), mask\n\ndef convert_target_to_score_input(text):\n    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))    \n\nmodel = roberta_passage_score()    \ntext = 'Favipiravir (also known as T-705), 6-fluoro-3-hydroxy-2-pyrazine carboxamide, has been primarily pursued for the treatment of influenza infections (329\u2013331). Approved in Japan, favipiravir can be used in the treatment of influenza A, B, and C virus infections (Table 2). According to the mechanism of drug action postulated by Furuta et al. (332), favipiravir is converted intracellularly to its ribofuranosyl monophosphate form by the phosphoribosyl transferase; two phosphorylations subsequently convert the ribofuranosyl monophosphate form to the triphosphate form, the active metabolite of favipiravir. Importantly, favipiravir triphosphate shows broad-spectrum inhibitory activities against the RNA polymerases of influenza A viruses (including the highly pathogenic H5N1 viruses) (330, 333) and many other positive-sense RNA and negative-sense RNA viruses (331). Recently, favipiravir has been proposed to treat patients infected with Ebola virus (EBOV) (334). Preliminary results suggest that favipiravir efficiently inhibits Ebola virus infections in mouse models (335, 336), but further investigations are still needed (337). In addition, favipiravir can inhibit the replication of human norovirus (325, 326) and human arenaviruses (Junin, Machupo, and Pichinde viruses) (338, 339), but these new applications require further evidence from clinical trials.'\ntargettext = 'efficacy'\ntargets = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(targettext))\ntargetinput, targetmask = convert_text_to_score_input(targettext)\nscoreinput, mask = convert_text_to_score_input(text.split('.')[0])\nscore, seqscore = model(scoreinput, mask, targets, mask) # dont use targetinput, outputs an unreduced seq\n\ntextscores =[]\nfor sent in text.split('.'):\n    scoreinput, mask = convert_text_to_score_input(sent)\n    score, seqscore = model(scoreinput, mask, targets, mask)\n    textscores.append(score)\n\nprint('High scores for:', text.split('.')[3:4], textscores[3:5])\n\n# find start and end positions of entity to tag, slight clean the two highest phrases\ndetectionsb = ['favipiravir is converted intracellularly to its ribofuranosyl monophosphate form by the phosphoribosyl transferase; two phosphorylations subsequently convert the ribofuranosyl monophosphate form to the triphosphate form, the active metabolite of favipiravir', 'Importantly, favipiravir triphosphate shows broad-spectrum inhibitory activities against the RNA polymerases of influenza A viruses (including the highly pathogenic H5N1 viruses) (330, 333) and many other positive-sense RNA and negative-sense RNA viruses']\nstartends = []\nfor detection in detectionsb:\n    start, end = text.find(detection), text.find(detection)+len(detection)\n    startends.append((start,end))\nstartends    \n\n# Write the entity dictionary\nents=[]\nfor idx in range(len(startends)):\n    ent={}\n    ent['start']=startends[idx][0]\n    ent['end']=startends[idx][1]\n    ent['label']='efficacy'\n    ents.append(ent)\n\n# Create a renderable dictionary\nex = [{\"text\": text,\n       \"ents\": ents,\n       \"title\": 'Favipiravir efficacy passage highlighting'}]\nspacy.displacy.render(ex, style=\"ent\", manual=True)","edc7c45b":"target = 'clinical trials efficacy'\ntarget = convert_target_to_score_input(target)\n\nclass roberta_score(nn.Module):\n    def __init__(self, roberta_model = '\/kaggle\/input\/covidrobertamodel', dataset=None):\n        super(roberta_score, self).__init__()        \n        self.outmodel = RobertaForMaskedLM.from_pretrained(roberta_model)\n        \n    def forward(self, word_id, word_mask, target_id, target_mask):\n        m = nn.Softmax(dim=2)\n        output_embeds = self.outmodel(word_id, token_type_ids=None, attention_mask = word_mask)\n        soft_embeds = m(output_embeds[0])        \n        seqscore = torch.sum(soft_embeds.detach()[:,:,target_id], dim=2)        \n        score = torch.sum(seqscore, dim=1)        \n        score = score \/ (np.count_nonzero(target_id)*np.count_nonzero(word_id))        \n        return score, seqscore \n    \ndef return_undis_coverage_in_topN(cdrugs10, passdrugs10, N=5):\n    scores10=[]\n    dict10 = {}\n    for text in cdrugs10:\n        scoreinput, mask = convert_text_to_score_input(text)\n        score, seqscore = model(scoreinput, mask, target, mask)\n\n        scores10.append(score)\n        dict10[text]=score\n    sortscores10 = sorted(dict10.items(), key=lambda x: x[1])\n    grades10top10 = [True if druguple[0].split()[0].lower() in passdrugs10 else False for druguple in sortscores10[-N:]]\n    grades10 = [True if druguple[0].split()[0].lower() in passdrugs10 else False for druguple in sortscores10]\n    top10acc, allacc = np.sum(grades10top10)\/len(passdrugs10), np.sum(grades10)\/len(grades10)\n    return top10acc, allacc, sortscores10[-N:]   \n\n# model = roberta_score()\napproved09 = ['Amantadine', 'Ribavirin', 'Rimantadine', 'Zanamivir', 'Oseltamivir']\napproved09 = [drug.lower() for drug in approved09]\nundisdrug09 = ['Laninamivir', 'Peramivir', 'Favipiravir', 'Baloxavir']\nundisdrug09 = [drug.lower() for drug in undisdrug09]\nundisdrug13 = undisdrug09[1:]\napproved13 = approved09 + ['laninamivir']\nundisdrug16 = undisdrug09[-1] # balovir approved in 2018\napproved16 = approved09+undisdrug09[:-1]\napproved19 = approved09+undisdrug09\n\ncoverage={}\n# build histogram with top N coverage while also filtering by even feasible\nyear=2006\nN=30\nwhile year<2010:        \n    candidates = cdrugyear[str(year)]\n    candidates = [drug.lower() for drug in candidates]\n    # only assume coverage where even possible, ie. trials exist\n    validundis = [drug for drug in undisdrug09 if drug in candidates]\n    candidates = [drug for drug in candidates if drug not in approved09]\n    coverage[(year)] = return_undis_coverage_in_topN(candidates, validundis, N=N)\n    year+=1\nwhile year<2014 and year>=2010:\n    candidates = cdrugyear[str(year)]\n    candidates = [drug.lower() for drug in candidates]    \n    validundis = [drug for drug in undisdrug13 if drug in candidates]\n    candidates = [drug  for drug in candidates if drug not in approved13]\n    coverage[(year)] = return_undis_coverage_in_topN(candidates, validundis, N=N) \n    year+=1    \nfor year in range(2006,2014):\n    print(coverage[(year)][2])\ndruglift = [yeardat[0]\/yeardat[1] for yeardat in coverage.values()]","b6e36c2d":"# prediction rank of probenecid, peramivir, favipiravir\ncumulrank, fcrank, pcrank =[], [],[]\nfor year in range(2006,2014):\n    probrank = [idx for idx, drugpair in enumerate(coverage[(year)][2]) if drugpair[0] == 'probenecid']\n    if probrank != []:\n        truerank = len(coverage[(year)][2]) - probrank[0] if probrank[0] > len(coverage[(year)][2])\/\/2-1 else probrank[0]+len(coverage[(year)][2])-1\n        cumulrank.append(truerank)\n    else:\n        cumulrank.append(99)\n        \n    favirank = [idx for idx, drugpair in enumerate(coverage[(year)][2]) if drugpair[0] == 'favipiravir']\n    if favirank != []:\n        truerank = len(coverage[(year)][2])-favirank[0] if favirank[0]> len(coverage[(year)][2])\/\/2-1 else favirank[0]+len(coverage[(year)][2])-1\n        fcrank.append(truerank)\n    else:\n        fcrank.append(99)\n    \n    perarank = [idx for idx, drugpair in enumerate(coverage[(year)][2]) if drugpair[0] == 'peramivir']\n    if perarank != []:\n        truerank = len(coverage[(year)][2])-perarank[0] if perarank[0] > len(coverage[(year)][2])\/\/2-1 else perarank[0]+len(coverage[(year)][2])        \n        pcrank.append(truerank)\n    else:\n        pcrank.append(99)\n    \ncumulrank, fcrank, pcrank\n    \nwidth = 1.0     # gives histogram aspect to the bar diagram \nplt.rcParams['figure.figsize'] = [10, 5]\nfig, ax = plt.subplots()\nplt.ylabel('Prediction Rank')\nplt.title('Prediction Rank vs Year')\nax.plot(range(2006,2014), pcrank, 'k-', label ='Peramivir')\nax.plot(range(2012,2014), fcrank[-2:], 'b-', label ='Favipiravir')\nax.plot(range(2007,2014), cumulrank[1:], 'g-', label ='Probenecid')\nax.plot([2013], [11], marker='*', markersize=15, color=\"y\")\nax.plot([2013], [13], marker='*', markersize=15, color=\"y\", label='Approved')\n\nlegend = ax.legend(loc='best', shadow=True, fontsize='x-large')\nplt.gca().invert_yaxis()\n\nfig.autofmt_xdate()\nplt.show()\ncumulrank, pcrank, fcrank","bf99880e":"# Define drugs and task targets\ndrugs = ['Losartan', 'Hydroxychloroquine', 'Remdesivir', 'CD24Fc', 'Nitric Oxide Gas', 'Aviptadil', 'Sarilumab', 'PUL-042 Inhalation Solution', 'Favipiravir']\ntargets = ['COVID-19 efficacy']\ncombo2 = list(itertools.combinations(drugs, 2)) \ncombo3 = list(itertools.combinations(drugs, 3)) \n\ndef return_combocat(combo):\n    concats=[]\n    for grams in combo:\n           concats.append( (''.join([w+' ' for w in grams])).strip())\n    return concats   \n\ndrugcombo2 = return_combocat(combo2)\ndrugcombo3 = return_combocat(combo3)\nlen(drugs), len(targets), len(combo2), len(combo3), len(drugcombo2), len(drugcombo3)","ce4e619e":"# control drugs\nclass roberta_score(nn.Module):\n    def __init__(self, roberta_model = '\/kaggle\/input\/covidrobertamodel\/', dataset=None):\n        super(roberta_score, self).__init__()        \n        self.outmodel = RobertaForMaskedLM.from_pretrained(roberta_model)\n        \n    def forward(self, word_id, word_mask, target_id, target_mask):\n        m = nn.Softmax(dim=2)\n        output_embeds = self.outmodel(word_id, token_type_ids=None, attention_mask = word_mask)\n        soft_embeds = m(output_embeds[0])        \n        seqscore = torch.sum(soft_embeds.detach()[:,:,target_id], dim=2)        \n        score = torch.sum(seqscore, dim=1)        \n        score = score \/ (np.count_nonzero(target_id)*np.count_nonzero(word_id))        \n        return score, seqscore        \n    \nmodel = roberta_score()\ncontroldrugs = ['Zanamivir', 'Oseltamivir', 'Peramivir', 'Laninamivir octanoate']\ntargets = ['COVID-19 efficacy']\n\n# Simple prediction from drugs and targeted effect\ncontrolresults={}\nfor drug in controldrugs:\n    for targ in targets:\n        text, target = drug, targ\n        scoreinput, mask = convert_text_to_score_input(text)\n        target = convert_target_to_score_input(target)\n        score, seqscore = model(scoreinput, mask, target, mask)\n        controlresults[drug] = score\n        \nwidth = 1.0     # gives histogram aspect to the bar diagram \nplt.rcParams['figure.figsize'] = [15, 10]\nfig, ax = plt.subplots()\nscontrolresults = sorted((value, key) for (key,value) in controlresults.items())\nax.bar([val[1] for val in scontrolresults], [val[0] for val in scontrolresults], width, color='g')\n\nplt.title('Approved \"Controls\" ranking by confidence score COVID-19 efficacy')\nfig.autofmt_xdate()\nplt.show()","35a33002":"# Simple prediction from drugs and targeted effect\ndrugresults={}\neffectresults={}\ncomboresults={}\nfor drug in drugs:\n    for targ in targets:\n        text, target = drug, targ\n        scoreinput, mask = convert_text_to_score_input(text)\n        target = convert_target_to_score_input(target)\n        score, seqscore = model(scoreinput, mask, target, mask)\n        comboresults[drug] = score        \n\nwidth = 1.0     # gives histogram aspect to the bar diagram \nplt.rcParams['figure.figsize'] = [20, 10]\n\nfig, ax = plt.subplots()\nscomboresults = sorted((value, key) for (key,value) in comboresults.items())\nax.bar([val[1] for val in scomboresults], [val[0] for val in scomboresults], width, color='g')\n\nplt.title('Ranking by Confidence score COVID-19 efficacy, on-going clinical trials')\nfig.autofmt_xdate()\nplt.show()","d5441991":"# Simple prediction from drugs and targeted effect\ncomboresults={}\nfor drug in drugcombo2:\n    for targ in targets:\n        text, target = drug, targ\n        scoreinput, mask = convert_text_to_score_input(text)\n        target = convert_target_to_score_input(target)\n        score, seqscore = model(scoreinput, mask, target, mask)\n        comboresults[drug] = score\n        \nwidth = 1.0     # gives histogram aspect to the bar diagram \nplt.rcParams['figure.figsize'] = [25, 15]\nfig, ax = plt.subplots()\nscomboresults = sorted((value, key) for (key,value) in comboresults.items())\nax.bar([val[1] for val in scomboresults], [val[0] for val in scomboresults], width, color='g')\n\nplt.title('Ranking by Confidence score COVID-19 efficacy, drug combinations')\nfig.autofmt_xdate()\nplt.show()","7d387107":"targets = ['side effects']\ncontrolresults={}\nfor drug in controldrugs:\n    for targ in targets:\n        text, target = drug, targ\n        scoreinput, mask = convert_text_to_score_input(text)\n        target = convert_target_to_score_input(target)\n        score, seqscore = model(scoreinput, mask, target, mask)\n        controlresults[drug] = score\n        \nwidth = 1.0     # gives histogram aspect to the bar diagram \nplt.rcParams['figure.figsize'] = [15, 10]\nfig, ax = plt.subplots()\nscontrolresults = sorted((value, key) for (key,value) in controlresults.items())\nax.bar([val[1] for val in scontrolresults], [val[0] for val in scontrolresults], width, color='g')\n\nplt.title('Approved \"Controls\" ranking by side effect score')\nfig.autofmt_xdate()\nplt.show()","1e9b83d1":"targets = ['side effects']\ncomboresults={}\nfor drug in drugs:\n    for targ in targets:\n        text, target = drug, targ\n        scoreinput, mask = convert_text_to_score_input(text)\n        target = convert_target_to_score_input(target)\n        score, seqscore = model(scoreinput, mask, target, mask)\n        comboresults[drug] = score        \n\nwidth = 1.0     # gives histogram aspect to the bar diagram \nplt.rcParams['figure.figsize'] = [20, 10]\n\nfig, ax = plt.subplots()\nscomboresults = sorted((value, key) for (key,value) in comboresults.items())\nax.bar([val[1] for val in scomboresults], [val[0] for val in scomboresults], width, color='g')\n\nplt.title('Ranking by side effects, on-going clinical trials')\nfig.autofmt_xdate()\nplt.show()","1560d744":"targets = ['side effects']\ncomboresults={}\nfor drug in drugcombo2:\n    for targ in targets:\n        text, target = drug, targ\n        scoreinput, mask = convert_text_to_score_input(text)\n        target = convert_target_to_score_input(target)\n        score, seqscore = model(scoreinput, mask, target, mask)\n        comboresults[drug] = score\n        \nwidth = 1.0     # gives histogram aspect to the bar diagram \nplt.rcParams['figure.figsize'] = [25, 15]\nfig, ax = plt.subplots()\nscomboresults = sorted((value, key) for (key,value) in comboresults.items())\nax.bar([val[1] for val in scomboresults], [val[0] for val in scomboresults], width, color='g')\n\nplt.title('Ranking by side effect score, drug combinations')\nfig.autofmt_xdate()\nplt.show()                ","e6f5da51":"Image(filename='\/kaggle\/input\/trialsdata\/covid_embedding.png', width=800, height=800) \n","e2820bb5":"Now let's make our COVID-19 predictions now that we have some understanding of the Query-Target method.  We specifically test the target of \"COVID-19 efficacy\".","06bbffe4":"# A Transformer Query-Target Knowledge Discovery Method\n\nWe introduce a straight-forward extension of the \"Unsupervised word embeddings capture latent knowledge from materials science literature\" method using the Roberta transformer-based architecture, enabling many modern benefits including state of the art language modeling performance (full-text processing), negation handling, and flexible relationship analysis. \n\nThe insight of the \"latent knowledge\" paper is to realize the power of domain specific analogies and the ability of the word2vec algorithm to capture these relationships to some degree. Yet there are challenges to adapting this method to modern transformer architectures. First, what is the task that best maps to the skip-gram approach to mine analogies? How should the algorithm adapt from the straight-forward single hidden layer method in word2vec to the deep encoder decoder architectures used in modern language models? Namely, how can the scoring methods adapt to the new pretrain-finetune paradigm? \n\nWe make first strides in answering these questions in light of the practical application of approval of influenza drugs over the past 50 years, pulling data from clinicaltrials.gov and the excellent review article \"Approved Antiviral Drugs over the Past 50 Years\", which itself appears in the CORD-19 dataset. Significant effort is spent to build some intuition on the method and verify against the known dataset of clinical trials and influenza drugs, before adapting for prediction of COVID-19 drugs and current clinical trials.\n\nNote: We've now released the model in the attached public dataset \/kaggle\/input\/covidrobertmodel\/\n\n## Methods\nWe train a Roberta-large model to high accuracy on CORD-19 (pplx=2.4696) and use prediction scores on a query filtered by the target tokens to score relevance.  Using 'efficacy' as a target, prediction scores are demonstrated on a forward chaining analysis, analyzing historical clincial drug trials for influenza and when the drugs were approved. The ranking of drugs that were historically approved are in the top 3 by rank order prior to approval, though the approval dataset is limited (influenza drugs are well-covered by the CORD-19 dataset though only a few influenza drugs were approved in the past decade or so). Subsequently, we issue predictions on ongoing clinical trials for COVID-19, as well as drug discovery predictions on novel drug combinations.\n\n## Perspectives\nWe discuss several details on the experimental method and a few caveat emptor points, pending further experiments and peer review on this work in progress. \n\nSpecial thanks to Greg Peterfreund, MD, PhD, Robert Chang, MD, and Jason Su for helpful comments.","65143432":"## How to cite\nIf the above work influenced or informed your work or was in some way helpful, please consider citing using the following information.\n\n```\n@article{TEND,\n        author = {Tam, L.K., Wang, X., Xu, D.},    \n        title = {Transformer Query-Target Knowledge Discovery (TEND): Drug Discovery from CORD-19},\n        year = {2020},    \n        month = {November},    \n        location = {NVIDIA, Santa Clara, CA, USA},     \n        url = {https:\/\/arxiv.org\/abs\/2012.04682}\n        howpublished = {ArXiv},\n}\n```","8006c055":"On the horizon, COVID-19 embedding visualization","5f100cea":"# Global clinical trials data\nFDA and global clinical trials data is available at https:\/\/www.clinicaltrials.gov\/. The search terms are matching the data available from \"Approved Antiviral Drugs over the Past 50 Years\".\nNamely, we use influenza as the condition, check the search term to filter by drugs (though it appears sometimes vaccines will creep in as this filter isn't tight), and focus on years prior to 2016. Let's examine some of the influenza data from \"Approved Antiviral Drugs\".","9c89f806":"# Drug discovery\nThe power of knowledge discovery methods is considering drugs that have not even been surfaced.  We can make predictions on all combinations of clinical drugs in a fairly straightforward fashion by changing the input tokens.","41410b36":"The Query-Target map is concise and adaptable to scoring.  \n\n# Forward chaining analysis\nLet's use these prediction scores to estimate which drugs will pass clinical trials.  In particular we'll perform a forward chaining analysis, where we limit the candidate drugs to the data available at that year. We test the target of \"clinical trials efficacy\". \n\nFor the intellectual rigorous, we should train a new model for each year. Such experiments are underway \u2013 please search for the Arxiv submission soon!","c92c2212":"The above is showing that negative assocations can arise. Favipiravir is an approved antiviral drug which means it probably had to pass some initial assessments on side effects and efficacy.\n\n# Extractive summarization\n\nWe can use the Query-Target method to highlight sections of the text. Here's a passage from the \"50 Years\" article.\n\nFavipiravir (also known as T-705), 6-fluoro-3-hydroxy-2-pyrazine carboxamide, has been primarily pursued for the treatment of influenza infections (329\u2013331). Approved in Japan, favipiravir can be used in the treatment of influenza A, B, and C virus infections (Table 2). According to the mechanism of drug action postulated by Furuta et al. (332), favipiravir is converted intracellularly to its ribofuranosyl monophosphate form by the phosphoribosyl transferase; two phosphorylations subsequently convert the ribofuranosyl monophosphate form to the triphosphate form, the active metabolite of favipiravir. Importantly, favipiravir triphosphate shows broad-spectrum inhibitory activities against the RNA polymerases of influenza A viruses (including the highly pathogenic H5N1 viruses) (330, 333) and many other positive-sense RNA and negative-sense RNA viruses (331). Recently, favipiravir has been proposed to treat patients infected with Ebola virus (EBOV) (334). Preliminary results suggest that favipiravir efficiently inhibits Ebola virus infections in mouse models (335, 336), but further investigations are still needed (337). In addition, favipiravir can inhibit the replication of human norovirus (325, 326) and human arenaviruses (Junin, Machupo, and Pichinde viruses) (338, 339), but these new applications require further evidence from clinical trials.","1700fb06":"The prediction implementation is mostly on the forward pass.\n\n# Attention visualization\nLet's visualize some of the attentional weightings in our model and from our Query-Target method. We build on some visualization code from https:\/\/github.com\/kexinhuang12345\/clinicalBERT.","ba476c4d":"# Experimental architecture\nAs stated, a Roberta-large model for the masked language task on CORD-19 (pplx=2.4696) is used to calculate prediction scores on a query filtered by the target tokens to score relevance.  Using 'efficacy' as a target, prediction scores are calculated.","5b770a15":"We reach a pretty good perplexity of 2.4696, that's an effective branching factor of less than 3, the model really knows what's coming next.  In the original \"Latent Knowledge\" study, their dataset was approxmiately 600M words with heavy filtering and the full text CORD-19 corpus appears to fall in the same range and be of similar high quality. We earnestly thank the Allen Institute, CZR, MSR, Georgetown, White House, the NIH and all affilifates for their excellent dataset curation.\n\nNow let's used this masked LM for predictions, as per the architecture figure above.","d005cf59":"Above we show the self-attention for a layer close to a word embedding for the input of \"favipiravir triphosphate shows broad-spectrum inhibitory activities against the RNA polymerases of influenza\". The model is learning some context-dependent associations. How about the query-target scoring?\n\n# Query-target attention visualization","22b1d753":"It's at least understood that Oseltamivir (Tamiflu) is well tolerated.","9f6c1a16":"We spare readers the eye chart of the triplet combinations.\n\n# Concluding thoughts\n\nWe present a flexible transformer query-target for knowledge discovery that enables diverse applications such as extractive summarization given a specific target and prediction ranking for discovering drugs, in the vein of the \"Latent Knowledge\" approach.\n\nIn order to further boost confidence in the method, we need to verfiy against the original \"Latent Knowledge\" approach. Unfortunately, the data splits for the \"Latent Knowledge\" approach are not available (https:\/\/github.com\/materialsintelligence\/mat2vec\/issues\/22), but we have developed approximately 400 \"drug analogies\" similar to the \"Latent Knowledge\" materials analogies.  One of the weaknesses of the old (2013) skip-gram approach for domain specific applications is the massive vocabulary needed, 500k for the \"Latent Knowledge\" study and in our first training of word2vec on CORD-19, at least 400K vocabulary.  This appears to complicate the analogy task evaluation at the present moment, as simple lookup as per the original method can be expensive (essentially running inference for 500k examples for each test analogy if strictly apples to apples).  Another issue is overcoming the pretrain-finetune paradigm for analogies.  It really makes sense to finetune a model particularly for the analogy task as the vector space structure is not as straight-forward in deep transformer architectures as it is with word2vec, likely due to the pipeline across the entire layers (24 in the case of Roberta-large).  In any case, we have made in-roads on these issues and hope to release our results soon.","054ce307":"We can examine if we change the target to side effects.","0fcd62ec":"There are only eight influenza drugs approved for use! Let's clean the data a bit and disambguiate trade names.","69c5544e":"Oseltamivir (Tamiflu) is a common flu drug, but it's known to be not too specific a target for COVID-19 (https:\/\/www.uchicagomedicine.org\/forefront\/prevention-and-screening-articles\/wuhan-coronavirus).","e2677f05":"The 44000 articles are used to train and test our model in an 80\/20 split.\n\n# Training the language model\nJust use the run_language_modeling.py script in the pytorch transformers library: https:\/\/github.com\/huggingface\/transformers\/blob\/master\/examples\/run_language_modeling.py. Thanks for the hardwork from HuggingFace, AllenNLP, and Facebook!\n\nWe dust off our DGX-2 and run it at batch size 4 per GPU across 16 GPUs for 100k steps (~36 hours).\n\nWhy do we choose the masked language model task? The rationale is based off examining Fig. 2c from the \"Latent Knowledge\" paper, namely the MLM task is closest to the skip-gram training, which is predicting the target word given a context around the word. We take a leap of faith that training this on an appropriately large scale will give good results since the small models available publicly on generic data are hardly representive of our application."}}