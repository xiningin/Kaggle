{"cell_type":{"4857a484":"code","51f8e2b5":"code","e1c5fec0":"code","20cee18f":"code","9eedf23e":"code","3b98d9d1":"code","aae28d8e":"code","fcc5a92a":"code","7ababe13":"code","703d730a":"code","b796f4ac":"code","9e312cad":"code","e95a6e19":"code","5ec4204e":"code","4c9a9062":"code","7debc614":"code","199ca7ee":"code","6839bc86":"code","d54e7abf":"code","e3945b09":"code","30314bf1":"code","6d6a3222":"code","a22b2b34":"markdown","08b859c2":"markdown","67322437":"markdown","49c3028f":"markdown","2d2c6992":"markdown","9e278645":"markdown","1f2b4f94":"markdown","d69314f2":"markdown","2f268e5a":"markdown","68af6837":"markdown","c0a3dd97":"markdown"},"source":{"4857a484":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n","51f8e2b5":"dataset =pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","e1c5fec0":"print('Total number of entries in the train dataset are:', len(dataset))\ndataset .head()\n","20cee18f":"dataset.shape","9eedf23e":"dataset.info()","3b98d9d1":"dataset.dtypes","aae28d8e":"dataset.isnull().sum()","fcc5a92a":"dataset.isna()","7ababe13":"text = dataset['text']\ntext","703d730a":"!pip install autocorrect\n\nfrom autocorrect import Speller\nspell = Speller()\n","b796f4ac":"import nltk\nfrom nltk.corpus import stopwords\nimport re\nimport string \nimport nltk.data\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emojis(data):\n    emoj = re.compile(\"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002500-\\U00002BEF\"  # chinese char\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        u\"\\U0001f926-\\U0001f937\"\n        u\"\\U00010000-\\U0010ffff\"\n        u\"\\u2640-\\u2642\" \n        u\"\\u2600-\\u2B55\"\n        u\"\\u200d\"\n        u\"\\u23cf\"\n        u\"\\u23e9\"\n        u\"\\u231a\"\n        u\"\\ufe0f\"  # dingbats\n        u\"\\u3030\"\n                      \"]+\", re.UNICODE)\n    return re.sub(emoj, '', data)\n\ndef create_corpus(text):\n  text=re.sub('[^a-zA-Z]', ' ',str(text))\n  text=text.lower()\n  text=nltk.word_tokenize(text)\n  ps=PorterStemmer()\n  all_stopwords = stopwords.words('english')\n  #all_stopwords.remove('not')\n  text= [ps.stem(word) for word in text if not word in set(all_stopwords)]\n  text=' '.join(text)\n  return text\n","9e312cad":"dataset['text']=dataset['text'].apply(remove_emojis)\ndataset['text']=dataset['text'].apply(remove_punct)\ndataset['text']=dataset['text'].apply(remove_URL)\ndataset['text']=dataset['text'].apply(remove_html)\ndataset.fillna({'location': 'Missing', 'keyword': 'Missing'}, inplace=True)","e95a6e19":"corpus=[]\nfreqs = {}\nfor i in range(0, len(dataset)):\n  tweet=create_corpus(dataset['text'][i])\n  location=create_corpus(dataset['location'][i])\n  keyword=create_corpus(dataset['keyword'][i])\n  corpus.append(tweet+location+keyword)\n  for word in corpus[i].split():\n      pair=(word,dataset['target'][i])\n      if pair in freqs:\n           freqs[pair] += 1\n      else:\n           freqs[pair] = 1","5ec4204e":"X = np.zeros((len(corpus), 3))\nfor i in range(0,len(corpus)):\n     X[i,0]=1\n     for word in corpus[i].split():\n      X[i,1] += freqs.get((word,1.0), 0)   \n      X[i,2] += freqs.get((word,0.0), 0)\n        \n        ","4c9a9062":"print(X)","7debc614":"from sklearn import feature_extraction, linear_model, model_selection, preprocessing\ny=dataset.iloc[:, -1].values\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n","199ca7ee":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\n","6839bc86":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)\n","d54e7abf":"test=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nprint(len(test))\n","e3945b09":"#Create a  DataFrame with the passengers ids and our prediction regarding whether they survived or not\ntest['text']=test['text'].apply(remove_emojis)\ntest['text']=test['text'].apply(remove_punct)\ntest['text']=test['text'].apply(remove_URL)\ntest['text']=test['text'].apply(remove_html)\ntest.fillna({'location': 'Missing', 'keyword': 'Missing'}, inplace=True)\n\ncorpus_test=[]\nfor i in range(0, len(test)):\n  tweet=create_corpus(test['text'][i])\n  location=create_corpus(test['location'][i])\n  keyword=create_corpus(test['keyword'][i])\n  corpus_test.append(tweet+location+keyword)\n\n            \nX_test= np.zeros((len(corpus_test), 3))\nfor i in range(0,len(corpus_test)):\n     X_test[i,0]=1\n     for word in corpus_test[i].split():\n      X_test[i,1] += freqs.get((word,1.0), 0)   \n      X_test[i,2] += freqs.get((word,0.0), 0)\n        \n\nfinal_predictions = classifier.predict(X_test)\n\nprint(len(final_predictions))\n","30314bf1":"submission = pd.DataFrame({'id':test['id'],'target':final_predictions})\n\n#Visualize the first 5 rows\nsubmission.head()\n","6d6a3222":"#Convert DataFrame to a csv file that can be uploaded\n#This is saved in the same directory as your notebook\nfilename = 'Predictions 1.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)\n","a22b2b34":"## **Data Loading**","08b859c2":"## **Preliminary Analysis**","67322437":"# **Natural Language Processing with Classification and Vector Spaces Week 1 Method Coursera**\n","49c3028f":"## **Cleaning our dataset**","2d2c6992":"## **Word Correcting**","9e278645":"## **submissions**","1f2b4f94":"## **Cleaning The Data into a Corpus and Creating A Freqs array for feature Extraction**","d69314f2":"## **getting the accuracy of our model**","2f268e5a":"## **Reading our dataset**","68af6837":"## **Importing Libraries**","c0a3dd97":"## **Applying a classification model**\n"}}