{"cell_type":{"bde85d8f":"code","ffe02b9e":"code","7ff54987":"code","b8808b16":"code","222fdfc6":"code","d709e975":"code","613aa9a5":"code","ab5e78e3":"code","97e42f84":"code","8c772502":"code","9c68b5cc":"code","a6d68565":"code","163fbcf8":"markdown","f7e652f5":"markdown","c3eb8fa6":"markdown","02349e45":"markdown"},"source":{"bde85d8f":"!pip install -U nltk==3.4","ffe02b9e":"import pandas as pd\nimport numpy as np\nimport regex as re\nimport nltk\nimport random\nfrom nltk.lm.preprocessing import padded_everygram_pipeline\nfrom nltk.tokenize import TweetTokenizer\nimport emoji\nfrom nltk.lm import MLE\n","7ff54987":"data = pd.read_csv(\"..\/input\/large-random-tweets-from-pakistan\/Random \"\n                   \"Tweets from Pakistan- Cleaned- Anonymous.csv\",encoding_errors = 'ignore')\n\ndata = data['full_text']\ndata = data.dropna()","b8808b16":"print('Tweet before preprocessing and tokenization: \\n', data[10])\n\n# Removing Urdu language\nreg = re.compile(r'[\\u0600-\\u06ff]+', re.UNICODE)\ndata = data.apply(lambda x: re.sub(reg, \"\", x))\n\n# removing extra spaces\ndata = data.apply(lambda x: re.sub(r'[  ]+', \" \", x))\n\n# converting to lowercase letters\ndata = data.apply(lambda x: x.strip().lower())\n\n# remove hyperlinks\ndata = data.apply(lambda x: re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', x))\n\n# removing @Mentions\ndata = data.apply(lambda x:re.sub(r'@.+?\\s', '', x))\n\n# removing extra symbols\ndata = data.apply(lambda x: re.sub(r'#', '', x))\ndata = data.apply(lambda x: re.sub(r'rt : ', '', x))\ndata = data.apply(lambda x: re.sub(r'\\n', ' ', x))\n\n# Dropping duplicates\ndata = data.drop_duplicates()\n\n# Tokenizing text\ntokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                           reduce_len=True)\ndata = data.apply(tokenizer.tokenize)\n\n# removing emoji\ndef clean(x):\n    return [y for y in x if not emoji.is_emoji(y)]\n\n# removing tweets with less than 3 words\ndata = data.apply(clean)\ndata = data.apply(lambda x:np.nan if len(x)<3 else x)\ndata.dropna(inplace = True)\n\nprint('Tweet After preprocessing and tokenization: \\n', data[10])","222fdfc6":"# Preprocess the tokenized text for 3-grams language modelling\nn_gram_size = 3\ntrain_data, padded_sents = padded_everygram_pipeline(n_gram_size, data)\n","d709e975":"# tranining a probablistic model using maximum liklihood estimation\nmodel = MLE(n_gram_size)\nmodel.fit(train_data, padded_sents)\nprint(model.vocab)\nlen(model.vocab)","613aa9a5":"# Testing model on Out Of Vocabulary words, unknown tokens\nprint(model.vocab.lookup(['here', 'is', 'a', 'solution', 'for','it'])) \nprint(model.vocab.lookup(['cant','find','blabla','and', 'blablabla']))","ab5e78e3":"# Total number of 3-grams\nprint(\"\",model.counts)","97e42f84":"# frequencies of occurences\nprint(\"Count of unigram 'pakistan': \",model.counts['pakistan'])\nprint(\"Count of bigram 'is\/pakistan': \",model.counts[['pakistan']]['is'])\nprint(\"Count of trigram 'a\/pakistan is': \",model.counts[['pakistan', 'is']]['a'])","8c772502":"# probablities of occurences\nprint(\"probability of 'pakistan' given <s>: \",model.score('pakistan'))\nprint(\"probability of 'is' given 'pakistan': \",model.score('is',['pakistan']))\nprint(\"probability of 'a' given 'pakistan is': \",model.score('a',['pakistan','is']))","9c68b5cc":"# Using our 3-gram language to generate text\n# max number of tokens\nlength = 20\nrand_seed=random.randint(0,100)\n\ngen_tweet =' '.join(model.generate(length,random_seed=rand_seed ))\n\n# removing extra ending tokens generated by model\nre.sub('[<\/s>]+','',gen_tweet)\n","a6d68565":"# Perplexity of our model on sample text\nmodel.perplexity('pakistan')","163fbcf8":"#### <span style='color:#ff6622'>Perplexity is high, better preprocessing like removing roman urdu may help<\/span>","f7e652f5":"# Language model on tweets","c3eb8fa6":"### Preprocessing and tokenization","02349e45":"# Generating new tweets using trained language model"}}