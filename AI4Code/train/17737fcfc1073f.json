{"cell_type":{"851005a5":"code","42bdcbaa":"code","f66cb44a":"code","5b64e6ad":"code","2a5695ae":"code","11ed1e84":"code","b6ee5493":"code","65193cd4":"code","74eb08ef":"code","3a3eed68":"code","8756123b":"code","e2070a0d":"code","b6270de5":"code","848753dc":"code","7594be0b":"code","d5d8ebe4":"code","30995a62":"code","11790fc1":"code","f47995f2":"code","f9477455":"code","fcf98ddb":"code","1658648d":"code","9ced0608":"code","db358a39":"code","7bb1e446":"code","f8300e17":"code","b11059f4":"code","00644577":"code","076fb656":"code","45940878":"markdown","39a2220c":"markdown","26d51a4c":"markdown","39d5acbe":"markdown","67e526cc":"markdown","0fe7b2df":"markdown","3aa4d81f":"markdown"},"source":{"851005a5":"import pandas as pd\n\nFILE_PATH = '..\/input\/heart-disease-uci\/heart.csv'\n\ndf = pd.read_csv(FILE_PATH)\ndf.head()","42bdcbaa":"df.columns = [\n    'age',\n    'sex',\n    'chest_pain_type',\n    'resting_blood_pressure',\n    'cholesterol',\n    'fasting_blood_sugar',\n    'rest_ecg',\n    'max_heart_rate_achieved',\n    'exercise_induced_angina',\n    'st_depression',\n    'st_slope',\n    'num_major_vessels',\n    'thalassemia',\n    'target'\n]","f66cb44a":"df['sex'][df['sex'] == 0] = 'female'\ndf['sex'][df['sex'] == 1] = 'male'\n\ndf['chest_pain_type'][df['chest_pain_type'] == 0] = 'asymptomatic'\ndf['chest_pain_type'][df['chest_pain_type'] == 1] = 'typical angina'\ndf['chest_pain_type'][df['chest_pain_type'] == 2] = 'atypical angina'\ndf['chest_pain_type'][df['chest_pain_type'] == 3] = 'non-anginal pain'\ndf['chest_pain_type'][df['chest_pain_type'] == 4] = 'asymptomatic'\n\ndf['fasting_blood_sugar'][df['fasting_blood_sugar'] == 0] = 'lower than 120mg\/ml'\ndf['fasting_blood_sugar'][df['fasting_blood_sugar'] == 1] = 'greater than 120mg\/ml'\n\ndf['rest_ecg'][df['rest_ecg'] == 0] = 'normal'\ndf['rest_ecg'][df['rest_ecg'] == 1] = 'ST-T wave abnormality'\ndf['rest_ecg'][df['rest_ecg'] == 2] = 'left ventricular hypertrophy'\n\ndf['exercise_induced_angina'][df['exercise_induced_angina'] == 0] = 'no'\ndf['exercise_induced_angina'][df['exercise_induced_angina'] == 1] = 'yes'\n\ndf['st_slope'][df['st_slope'] == 1] = 'upsloping'\ndf['st_slope'][df['st_slope'] == 2] = 'flat'\ndf['st_slope'][df['st_slope'] == 3] = 'downsloping'\n\ndf['thalassemia'][df['thalassemia'] == 1] = 'normal'\ndf['thalassemia'][df['thalassemia'] == 2] = 'fixed defect'\ndf['thalassemia'][df['thalassemia'] == 3] = 'reversable defect'\n\ndf.head(100)","5b64e6ad":"df.drop(df.index[df['st_slope']==0], inplace=True)\ndf.drop(df.index[df['thalassemia']==0], inplace=True)","2a5695ae":"a = pd.get_dummies(df['sex'], prefix='sex')\nb = pd.get_dummies(df['chest_pain_type'], prefix='chest_pain_type')\nc = pd.get_dummies(df['fasting_blood_sugar'], prefix='fasting_blood_sugar')\nd = pd.get_dummies(df['rest_ecg'], prefix='rest_ecg')\ne = pd.get_dummies(df['exercise_induced_angina'], prefix='exercise_induced_angina')\nf = pd.get_dummies(df['st_slope'], prefix='st_slope')\ng = pd.get_dummies(df['thalassemia'], prefix='thalassemia')\n\nframes = [df, a, b, c, d, e, f, g]\ndf = pd.concat(frames, axis = 1)\ndf = df.drop(columns=[\n    'sex',\n    'chest_pain_type',\n    'fasting_blood_sugar',\n    'rest_ecg',\n    'exercise_induced_angina',\n    'st_slope',\n    'thalassemia'\n])","11ed1e84":"len(df.keys())","b6ee5493":"from sklearn.preprocessing import MinMaxScaler\n\nx = df.values\nmin_max_scaler = MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndf_scaled = pd.DataFrame(x_scaled, columns=df.columns)","65193cd4":"df.to_csv('.\/clean_heart.csv')\ndf_scaled.to_csv('.\/clean_scaled_heart.csv')","74eb08ef":"import tensorflow as tf\n\nx_0 = df_scaled[df_scaled['target']==0].drop(columns='target').values\nx_1 = df_scaled[df_scaled['target']==1].drop(columns='target').values\n\nx_0 = tf.cast(tf.convert_to_tensor(x_0), tf.float32)\nx_1 = tf.cast(tf.convert_to_tensor(x_1), tf.float32)\n\nbatch_size = 32\ndataset_0 = tf.data.Dataset.from_tensor_slices(x_0).shuffle(1000).batch(batch_size)\ndataset_1 = tf.data.Dataset.from_tensor_slices(x_1).shuffle(1000).batch(batch_size)","3a3eed68":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dense\n\ndef build_generator():\n    model = Sequential([\n        Input(shape=16),\n        Dense(16, activation='relu'),\n        Dense(16, activation='relu'),\n        Dense(24, activation='sigmoid'),\n    ])\n    return model\n\n\ndef build_discriminator():\n    model = Sequential([\n        Input(shape=24),\n        Dense(16, activation='relu'),\n        Dense(8, activation='relu'),\n        Dense(1, activation='softmax')\n    ])\n    return model","8756123b":"def train_step(real_data, generator, discriminator):\n    random_latent_vectors = tf.random.normal(shape=(batch_size, 16))\n    generated_data = generator(random_latent_vectors)\n    combined_data = tf.concat([generated_data, real_data], axis=0)\n\n    labels = tf.concat(\n        [tf.ones((batch_size, 1)), tf.zeros((real_data.shape[0], 1))], axis=0\n    )\n    labels += 0.05 * tf.random.uniform(labels.shape)\n\n    # Train the discriminator\n    with tf.GradientTape() as tape:\n        predictions = discriminator(combined_data)\n        d_loss = loss_fn(labels, predictions)\n    grads = tape.gradient(d_loss, discriminator.trainable_weights)\n    d_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n\n    random_latent_vectors = tf.random.normal(shape=(batch_size, 16))\n    misleading_labels = tf.zeros((batch_size, 1))\n\n    # Train the generator (note that we should *not* update the weights\n    # of the discriminator)!\n    with tf.GradientTape() as tape:\n        predictions = discriminator(generator(random_latent_vectors))\n        g_loss = loss_fn(misleading_labels, predictions)\n    grads = tape.gradient(g_loss, generator.trainable_weights)\n    g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n    return d_loss, g_loss, generated_data\n\ndef train(epochs, dataset, generator, discriminator):\n    for epoch in range(epochs):\n        print(epoch)\n        for step, real_data in enumerate(dataset):\n            d_loss, g_loss, generated_data = train_step(real_data, generator, discriminator)\n            if step % 200 == 0:\n                # Print metrics\n                print(\"discriminator loss at step %d: %.2f\" % (step, d_loss))\n                print(\"adversarial loss at step %d: %.2f\" % (step, g_loss))","e2070a0d":"generator_0 = build_generator()\ndiscriminator_0 = build_discriminator()\n\ngenerator_1 = build_generator()\ndiscriminator_1 = build_discriminator()\n\nd_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\ng_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n\nloss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)","b6270de5":"epochs = 20000\n# train(epochs, dataset_0, generator_0, discriminator_0)","848753dc":"epochs = 20000\n# train(epochs, dataset_1, generator_1, discriminator_1)","7594be0b":"synthetic_0 = pd.DataFrame(generator_0(tf.random.normal([20000,16])).numpy(), columns=df.drop(columns='target').columns)\nsynthetic_0.head()","d5d8ebe4":"import numpy as np\n\nlabel_0 = np.zeros((20000,))\nsynthetic_0['target'] = label_0\nsynthetic_0.head()","30995a62":"synthetic_1 = pd.DataFrame(generator_1(tf.random.normal([20000,16])).numpy(), columns=df.drop(columns='target').columns)\nsynthetic_1","11790fc1":"label_1 = np.ones((20000,))\nsynthetic_1['target'] = label_1\nsynthetic_1","f47995f2":"synthetic_merge = pd.concat([synthetic_0, synthetic_1])\nsynthetic_merge","f9477455":"# synthetic_merge.to_csv('.\/synthetic_merge.csv')","fcf98ddb":"import pandas as pd\n\nsyn_path = '..\/input\/syntheticheartdiseaseuci\/synthetic_merge.csv'\ndf_syn = pd.read_csv(syn_path)\ndf_syn.head()","1658648d":"import tensorflow as tf\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nx = df_syn.drop(columns='target').values\ny = df_syn.target.values","9ced0608":"from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\npca = PCA(n_components=5)\nx = pca.fit_transform(x)\nfor i in range(2):\n  d = x[np.where(y == i)]\n  plt.scatter(d[:,0],d[:,1])","db358a39":"from sklearn.preprocessing import MinMaxScaler\n\nmin_max_scaler = MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\nfor i in range(2):\n  d = x[np.where(y == i)]\n  plt.scatter(d[:,0],d[:,1])","7bb1e446":"x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.5, random_state=1)\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=1)\n\nx_train = tf.cast(tf.convert_to_tensor(x_train), tf.float32)\ny_train = tf.cast(tf.convert_to_tensor(y_train), tf.float32)\nx_val = tf.cast(tf.convert_to_tensor(x_val), tf.float32)\ny_val = tf.cast(tf.convert_to_tensor(y_val), tf.float32)\nx_test = tf.cast(tf.convert_to_tensor(x_test), tf.float32)\ny_test = tf.cast(tf.convert_to_tensor(y_test), tf.float32)","f8300e17":"train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(256)\nval_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(10000).batch(256)\ntest_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).shuffle(10000).batch(256)","b11059f4":"from tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras import Sequential\n\nmodel = Sequential([\n    Input(shape=5),\n    Dense(3, activation='relu'),\n    Dense(1, activation='sigmoid'),\n])\n\noptimizer = tf.keras.optimizers.Adam()\nloss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\ntrain_acc_metric = tf.keras.metrics.BinaryAccuracy()\nval_acc_metric = tf.keras.metrics.BinaryAccuracy()\n\ndef train_step(x, y):\n    with tf.GradientTape() as tape:\n        logits = model(x)\n        loss_val = loss_fn(y, logits)\n    grads = tape.gradient(loss_val, model.trainable_weights)\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n    train_acc_metric.update_state(y, logits)\n    return loss_val\n\ndef train(epochs):\n    train_loss_history = []\n    val_loss_history = []\n    train_acc_history = []\n    val_acc_history = []\n    for epoch in range(epochs):\n        for step, (x_train, y_train) in enumerate(train_dataset):\n            train_loss = train_step(x_train, y_train)\n        for step, (x_val, y_val) in enumerate(val_dataset):\n            logits = model(x_val)\n            val_loss = loss_fn(y_val, logits)\n            val_acc_metric.update_state(y_val, logits)\n        print('EPOCH: %d - TRAIN LOSS: %.3f - VAL LOSS: %.3f' % (epoch, train_loss, val_loss))\n        print('EPOCH: %d - TRAIN ACCU: %.3f - VAL ACCU: %.3f' % (epoch, train_acc_metric.result(), val_acc_metric.result()))\n        train_loss_history.append(train_loss)\n        val_loss_history.append(val_loss)\n        train_acc_history.append(train_acc_metric.result())\n        val_acc_history.append(val_acc_metric.result())\n        \n        train_acc_metric.reset_states()\n        val_acc_metric.reset_states()\n    return train_loss_history, val_loss_history, train_acc_history, val_acc_history\n        \ntrain_loss, val_loss, train_acc, val_acc = train(25)","00644577":"test_acc_metric = tf.keras.metrics.BinaryAccuracy()\n\ndef test():\n    for step, (x_test, y_test) in enumerate(test_dataset):\n        logits = model(x_test)\n        val_loss = loss_fn(y_test, logits)\n        test_acc_metric.update_state(y_test, logits)\n    print('TEST ACCU: %.3f' % (test_acc_metric.result()))\n    test_acc_metric.reset_states()\n\ntest()","076fb656":"plt.plot(train_loss)\nplt.plot(val_loss)\nplt.plot(train_acc)\nplt.plot(val_acc)\n\nplt.title('Metrics')\nplt.xlabel('epoch')\nplt.legend(['Train Loss', 'Val Loss', 'Train Accuracy', 'Val Accuracy'])\n\nplt.show()","45940878":"# Prediction Model","39a2220c":"## Dataset Creation","26d51a4c":"## Define Model","39d5acbe":"## Create Two Models","67e526cc":"## Prediction Model","0fe7b2df":"## Dataset Creation","3aa4d81f":"# GAN"}}