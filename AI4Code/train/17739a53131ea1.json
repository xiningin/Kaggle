{"cell_type":{"8032e611":"code","f6c17643":"code","8467ad86":"code","19a7c739":"code","b858b069":"code","988b29f4":"code","e3001dbe":"code","92c9ee65":"code","d5ac68c5":"code","82067de9":"markdown"},"source":{"8032e611":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n!pip install scikit-optimize==0.8.1\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport xgboost as xg\nfrom functools import partial\nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope","f6c17643":"# Reading train dataset in the environment.\ndataset_pd = pd.read_csv(\"\/kaggle\/input\/otto-group-product-classification-challenge\/train.csv\", index_col = 0)\nprint(dataset_pd.shape)\n# Reading test dataset in the environment.\ndataset_pd2 = pd.read_csv(\"\/kaggle\/input\/otto-group-product-classification-challenge\/test.csv\", index_col = 0)\nprint(dataset_pd2.shape)","8467ad86":"# Creating a predictor matrix (removing the response variable column)\ndataset_train = dataset_pd.values\nX = dataset_train[:,0:93] # Predictors\ny = dataset_train[:,93] # Response \n\n# XGBoost do not take a categorical variable as input. We can use LabelEncoder to assign labels to categorical variables.\nlabel_encoder = LabelEncoder()\nlabel_encoder = label_encoder.fit(y)\nlabel_encoder_y = label_encoder.transform(y)","19a7c739":"# optimize function \ndef optimize(params, x, y):\n\n    model = xg.XGBClassifier(**params)\n    kf = StratifiedKFold(n_splits = 5)\n    accuracies = []\n    for idx in kf.split(X = x, y = y):\n        train_idx , test_idx = idx[0], idx[1]\n        xtrain = x[train_idx]\n        ytrain = y[train_idx]\n        \n        xtest = x[test_idx]\n        ytest = y[test_idx]\n        \n        model.fit(xtrain, ytrain)\n        preds = model.predict(xtest)\n        fold_acc = accuracy_score(ytest, preds)\n        accuracies.append(fold_acc)\n    \n    return -1.0 * np.mean(accuracies)","b858b069":"# Parameter Space for XGBoost\nparam_space = {\n    'max_depth' : scope.int(hp.quniform('max_depth', 3,15, 1)),\n    'n_estimators' : scope.int(hp.quniform('n_estimators', 100, 600, 1)),\n    'criterion' : hp.choice('criterion', ['gini', 'entropy']),\n    'colsample_bytree' : hp.uniform('colsample_bytree', 0.01,1),\n    'learning_rate' : hp.uniform('learning_rate', 0.001,1) \n}","988b29f4":"# Optimization Function\noptimization_function = partial(\n    optimize,\n    x = X,\n    y = label_encoder_y\n)","e3001dbe":"trials = Trials()\nresult = fmin(fn = optimization_function,\n                    space = param_space,\n                    algo = tpe.suggest,\n                    max_evals = 15,\n                    trials = trials\n)\nprint(result)","92c9ee65":"# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X, label_encoder_y, test_size = 0.33, random_state = 7)\n\nclassifier = xg.XGBClassifier(n_thread = 6, \n                              n_estimators = 396, \n                              max_depth = 6, \n                              colsample_bytree = 0.9292372781188178,\n                              learning_rate = 0.28725052863307404,\n                              criterion = \"gini\")\nclassifier.fit(X_train, y_train)\n\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, classifier.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, classifier.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","d5ac68c5":"# code for submission file.\ndataset_test = dataset_pd2.values\n\nclassifier = xg.XGBClassifier(n_thread = 6, \n                              n_estimators = 396, \n                              max_depth = 6, \n                              colsample_bytree = 0.9292372781188178,\n                              learning_rate = 0.28725052863307404,\n                              criterion = \"gini\")\nclassifier.fit(X, label_encoder_y)\n\nprediction_sub = classifier.predict(dataset_test)\n\n#dataset_pd2[\"prediction\"] = prediction_sub\nX_sub = np.array(prediction_sub).reshape(-1,1)\nonehot_encoder = OneHotEncoder(sparse = False)\nsubmission_file = onehot_encoder.fit_transform(X_sub)\n\nsubmission_file_df = pd.DataFrame(submission_file, \n                                  columns = ['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6',\n                                            'Class_7','Class_8','Class_9'], index = dataset_pd2.index)\n\n\nsubmission_file_df.to_csv(\"submission_otto_ver2.csv\")","82067de9":"### Brief about the notebook.\n\nIn this notebook we will be using Hyperopt for hyperparamter optimization. We will be using [Otto Group production classification Problem](https:\/\/www.kaggle.com\/c\/otto-group-product-classification-challenge). \n\n1. This is multiclass classification problem (9 classes).\n2. Dataset have 93 numerical features and 61878 observations.\n\nWe will be using hyperopt for hyperparamter tuning for our XGBoost model."}}