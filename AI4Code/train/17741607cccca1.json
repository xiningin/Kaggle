{"cell_type":{"ecf34170":"code","1370bedf":"code","de7ca16e":"code","2a7a88b3":"code","4a58d480":"code","f2db7600":"code","202a80e2":"code","b3339d0b":"code","63c190cd":"code","5333cc98":"code","ecfc5034":"code","4e521a43":"code","675a7738":"code","cbbd0afa":"code","6fba3cbc":"code","cef69fb6":"code","c3bdb339":"code","a192f8a0":"code","6a834386":"code","ea457a4d":"code","96a9ce5c":"code","af2dfac5":"code","42980879":"code","acccab75":"code","9cec09a5":"code","89720f01":"code","8663e690":"markdown","cb305870":"markdown","ddc38867":"markdown","a6c8a40d":"markdown","3ae60d23":"markdown","09cd2eb8":"markdown","ce3741eb":"markdown","d5bb0f2d":"markdown","c012b83f":"markdown","1ba5d751":"markdown","3170682c":"markdown","d1f75ba5":"markdown","00141af5":"markdown","77ab500a":"markdown","f4ac2d02":"markdown","d8899e30":"markdown","0715eb50":"markdown","69ced6d2":"markdown","4bfa79f2":"markdown","6b7394af":"markdown","a0344f73":"markdown","7a73e271":"markdown","5f85fd8b":"markdown","e9a66bab":"markdown","de649857":"markdown","fa59af7b":"markdown"},"source":{"ecf34170":"raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]","1370bedf":"# univariate data preparation\nfrom numpy import array\n\n# Split univariate sequence into samples\ndef split_sequence(sequence,n_steps):\n    X,y = list(),list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_idx = i+n_steps\n        # Check if we are beyond the sequence \n        if end_idx  > len(sequence)-1:\n            break\n        # gather i\/o parts of the pattern\n        seq_x,seq_y = sequence[i:end_idx],sequence[end_idx]\n        X.append(seq_x)\n        y.append(seq_y)\n        \n    return array(X),array(y)\n\n# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps = 3\n# split into samples\nX, y = split_sequence(raw_seq, n_steps)\n# summarize the data\nfor i in range(len(X)):\n\tprint(X[i], y[i])","de7ca16e":"# define model\n# model = Sequential()\n# model.add(LSTM(50,activation='relu',input_shape=(n_steps,n_features)))\n# model.add(Dense(1))\n# model.compile(optimizer='adam',loss='mse')","2a7a88b3":"# univariate lstm example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\n\n# split a univariate sequence into samples\ndef split_sequence(sequence,n_steps):\n    X,y = list(),list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_idx = i+n_steps\n        # Check if we are beyond the sequence \n        if end_idx  > len(sequence)-1:\n            break\n        # gather i\/o parts of the pattern\n        seq_x,seq_y = sequence[i:end_idx],sequence[end_idx]\n        X.append(seq_x)\n        y.append(seq_y)\n        \n    return array(X),array(y)\n\n# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps = 3\n# split into samples\nX, y = split_sequence(raw_seq, n_steps)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\n# define model\nmodel = Sequential()\nmodel.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=200, verbose=0)\n# demonstrate prediction\nx_input = array([70, 80, 90])\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","4a58d480":"# univariate lstm example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\n\n# split a univariate sequence into samples\ndef split_sequence(sequence,n_steps):\n    X,y = list(),list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_idx = i+n_steps\n        # Check if we are beyond the sequence \n        if end_idx  > len(sequence)-1:\n            break\n        # gather i\/o parts of the pattern\n        seq_x,seq_y = sequence[i:end_idx],sequence[end_idx]\n        X.append(seq_x)\n        y.append(seq_y)\n        \n    return array(X),array(y)\n\n# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps = 3\n# split into samples\nX, y = split_sequence(raw_seq, n_steps)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\n# define model\nmodel = Sequential()\nmodel.add(LSTM(50, activation='relu',return_sequences=True, input_shape=(n_steps, n_features)))\nmodel.add(LSTM(50,activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=200, verbose=0)\n# demonstrate prediction\nx_input = array([70, 80, 90])\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","f2db7600":"# univariate lstm example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Bidirectional\n\n# split a univariate sequence into samples\ndef split_sequence(sequence,n_steps):\n    X,y = list(),list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_idx = i+n_steps\n        # Check if we are beyond the sequence \n        if end_idx  > len(sequence)-1:\n            break\n        # gather i\/o parts of the pattern\n        seq_x,seq_y = sequence[i:end_idx],sequence[end_idx]\n        X.append(seq_x)\n        y.append(seq_y)\n        \n    return array(X),array(y)\n\n# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps = 3\n# split into samples\nX, y = split_sequence(raw_seq, n_steps)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\n# define model\nmodel = Sequential()\nmodel.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=200, verbose=0)\n# demonstrate prediction\nx_input = array([70, 80, 90])\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","202a80e2":"# univariate cnn lstm example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import TimeDistributed\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\n\n# split a univariate sequence into samples\ndef split_sequence(sequence,n_steps):\n    X,y = list(),list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_idx = i+n_steps\n        # Check if we are beyond the sequence \n        if end_idx  > len(sequence)-1:\n            break\n        # gather i\/o parts of the pattern\n        seq_x,seq_y = sequence[i:end_idx],sequence[end_idx]\n        X.append(seq_x)\n        y.append(seq_y)\n        \n    return array(X),array(y)\n\n# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps = 4\n# split into samples\nX, y = split_sequence(raw_seq, n_steps)\n# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\nn_features = 1\nn_seq = 2\nn_steps = 2\nX = X.reshape((X.shape[0], n_seq, n_steps, n_features))\n# define model\nmodel = Sequential()\nmodel.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\nmodel.add(TimeDistributed(MaxPooling1D(pool_size=2)))\nmodel.add(TimeDistributed(Flatten()))\nmodel.add(LSTM(50, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=500, verbose=0)\n# demonstrate prediction\nx_input = array([60, 70, 80, 90])\nx_input = x_input.reshape((1, n_seq, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","b3339d0b":"# univariate cnn lstm example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import ConvLSTM2D\n\n# split a univariate sequence into samples\ndef split_sequence(sequence,n_steps):\n    X,y = list(),list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_idx = i+n_steps\n        # Check if we are beyond the sequence \n        if end_idx  > len(sequence)-1:\n            break\n        # gather i\/o parts of the pattern\n        seq_x,seq_y = sequence[i:end_idx],sequence[end_idx]\n        X.append(seq_x)\n        y.append(seq_y)\n        \n    return array(X),array(y)\n\n# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps = 4\n# split into samples\nX, y = split_sequence(raw_seq, n_steps)\n# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\nn_features = 1\nn_seq = 2\nn_steps = 2\nX = X.reshape((X.shape[0], n_seq, 1,n_steps, n_features))\n# define model\nmodel = Sequential()\nmodel.add(ConvLSTM2D(filters=64,kernel_size=(1,2),activation=\"relu\",input_shape=(n_seq, 1, n_steps, n_features)))\nmodel.add(Flatten())\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=500, verbose=0)\n# demonstrate prediction\nx_input = array([60, 70, 80, 90])\nx_input = x_input.reshape((1, n_seq, 1,n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","63c190cd":"# define input sequence\nin_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\nin_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])","5333cc98":"out_seq","ecfc5034":"from numpy import hstack\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2, out_seq))","4e521a43":"dataset","675a7738":"# multivariate data preparation\nfrom numpy import array\nfrom numpy import hstack\n# define input sequence\nin_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\nin_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2, out_seq))\nprint(dataset)","cbbd0afa":"# split a multivariate sequence into samples\ndef split_sequence(sequences,n_steps):\n    X,y = list(),list()\n    for i in range(len(sequences)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the dataset\n        if end_ix > len(sequences):\n            break\n        # gather input and output parts of the pattern\n        seq_x,seq_y = sequences[i:end_ix,:-1],sequences[end_ix-1,-1]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X),array(y)","6fba3cbc":"# multivariate data preparation\nfrom numpy import array\nfrom numpy import hstack\n\n# split a multivariate sequence into samples\ndef split_sequences(sequences,n_steps):\n    X,y = list(),list()\n    for i in range(len(sequences)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the dataset\n        if end_ix > len(sequences):\n            break\n        # gather input and output parts of the pattern\n        seq_x,seq_y = sequences[i:end_ix,:-1],sequences[end_ix-1,-1]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X),array(y)\n\n\n# define input sequence\nin_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\nin_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2, out_seq))\n# choose a number of time steps\nn_steps = 3\n# convert into input\/output\nX, y = split_sequences(dataset, n_steps)\nprint(X.shape, y.shape)\n# summarize the data\nfor i in range(len(X)):\n\tprint(X[i], y[i])","cef69fb6":"# multivariate lstm example\nfrom numpy import array\nfrom numpy import hstack\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\n\n# split a multivariate sequence into samples\ndef split_sequences(sequences,n_steps):\n    X,y = list(),list()\n    for i in range(len(sequences)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the dataset\n        if end_ix > len(sequences):\n            break\n        # gather input and output parts of the pattern\n        seq_x,seq_y = sequences[i:end_ix,:-1],sequences[end_ix-1,-1]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X),array(y)\n\n# define input sequence\nin_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\nin_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2, out_seq))\n# choose a number of time steps\nn_steps = 3\n# convert into input\/output\nX, y = split_sequences(dataset, n_steps)\n# the dataset knows the number of features, e.g. 2\nn_features = X.shape[2]\n# define model\nmodel = Sequential()\nmodel.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=200, verbose=0)\n# demonstrate prediction\nx_input = array([[80, 85], [90, 95], [100, 105]])\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","c3bdb339":"# multivariate output data prep\nfrom numpy import array\nfrom numpy import hstack\n\n# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequences)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the dataset\n        if end_ix > len(sequences)-1:\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)\n\n# define input sequence\nin_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\nin_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2, out_seq))\n# choose a number of time steps\nn_steps = 3\n# convert into input\/output\nX, y = split_sequences(dataset, n_steps)\nprint(X.shape, y.shape)\n# summarize the data\nfor i in range(len(X)):\n\tprint(X[i], y[i])","a192f8a0":"# multivariate output stacked lstm example\nfrom numpy import array\nfrom numpy import hstack\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\n\n# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequences)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the dataset\n        if end_ix > len(sequences)-1:\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)\n\n# define input sequence\nin_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\nin_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2, out_seq))\n# choose a number of time steps\nn_steps = 3\n# convert into input\/output\nX, y = split_sequences(dataset, n_steps)\n# the dataset knows the number of features, e.g. 2\nn_features = X.shape[2]\n# define model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\nmodel.add(LSTM(100, activation='relu'))\nmodel.add(Dense(n_features))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=400, verbose=0)\n# demonstrate prediction\nx_input = array([[70,75,145], [80,85,165], [90,95,185]])\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","6a834386":"# Split a univariate sequence into samples\ndef split_sequence(sequence,n_steps_in,n_steps_out):\n    X,y = list(),list()\n    for i in range(len(sequence)):\n        # find the end of this pattern:\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out\n        # check if we are beyond the sequence\n        if out_end_ix > len(sequence):\n            break\n        # gather input\/output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix],sequence[end_ix:out_end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X),array(y)","ea457a4d":"# multi-step data preparation\nfrom numpy import array\n# Split a univariate sequence into samples\ndef split_sequence(sequence,n_steps_in,n_steps_out):\n    X,y = list(),list()\n    for i in range(len(sequence)):\n        # find the end of this pattern:\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out\n        # check if we are beyond the sequence\n        if out_end_ix > len(sequence):\n            break\n        # gather input\/output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix],sequence[end_ix:out_end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X),array(y)\n\n# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps_in, n_steps_out = 3, 2\n# split into samples\nX, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n# summarize the data\nfor i in range(len(X)):\n    print(X[i], y[i])","96a9ce5c":"# Tying all of this together, the Stacked LSTM for multi-step \n# forecasting with a univariate time series is listed below\n# univariate multi-step vector-output stacked lstm example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\n\n# split a univariate sequence into samples\ndef split_sequence(sequence,n_steps_in,n_steps_out):\n    X,y = list(),list()\n    for i in range(len(sequence)):\n        # find the end of this pattern:\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out\n        # check if we are beyond the sequence\n        if out_end_ix > len(sequence):\n            break\n        # gather input\/output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix],sequence[end_ix:out_end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X),array(y)\n\n# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps_in, n_steps_out = 3, 2\n# split into samples\nX, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\n# define model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\nmodel.add(LSTM(100, activation='relu'))\nmodel.add(Dense(n_steps_out))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=50, verbose=0)\n# demonstrate prediction\nx_input = array([70, 80, 90])\nx_input = x_input.reshape((1, n_steps_in, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","af2dfac5":"# univariate multi-step encoder-decoder lstm example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\n\n# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps_in, n_steps_out):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out\n        # check if we are beyond the sequence\n        if out_end_ix > len(sequence):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)\n\n# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps_in, n_steps_out = 3, 2\n# split into samples\nX, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\ny = y.reshape((y.shape[0], y.shape[1], n_features))\n# define model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', input_shape=(n_steps_in, n_features)))\nmodel.add(RepeatVector(n_steps_out))\nmodel.add(LSTM(100, activation='relu', return_sequences=True))\nmodel.add(TimeDistributed(Dense(1)))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=100, verbose=0)\n# demonstrate prediction\nx_input = array([70, 80, 90])\nx_input = x_input.reshape((1, n_steps_in, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","42980879":"# multivariate multi-step data preparation\nfrom numpy import array\nfrom numpy import hstack\n \n# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps_in, n_steps_out):\n    X, y = list(), list()\n    for i in range(len(sequences)):\n        # find the end of this pattern\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out-1\n        # check if we are beyond the dataset\n        if out_end_ix > len(sequences):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)\n \n# define input sequence\nin_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\nin_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2, out_seq))\n# choose a number of time steps\nn_steps_in, n_steps_out = 3, 2\n# covert into input\/output\nX, y = split_sequences(dataset, n_steps_in, n_steps_out)\nprint(X.shape, y.shape)\n# summarize the data\nfor i in range(len(X)):\n\tprint(X[i], y[i])","acccab75":"# multivariate multi-step stacked lstm example\nfrom numpy import array\nfrom numpy import hstack\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\n\n# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps_in, n_steps_out):\n    X, y = list(), list()\n    for i in range(len(sequences)):\n        # find the end of this pattern\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out-1\n        # check if we are beyond the dataset\n        if out_end_ix > len(sequences):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)\n \n\n# define input sequence\nin_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\nin_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2, out_seq))\n# choose a number of time steps\nn_steps_in, n_steps_out = 3, 2\n# covert into input\/output\nX, y = split_sequences(dataset, n_steps_in, n_steps_out)\n# the dataset knows the number of features, e.g. 2\nn_features = X.shape[2]\n# define model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\nmodel.add(LSTM(100, activation='relu'))\nmodel.add(Dense(n_steps_out))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=200, verbose=0)\n# demonstrate prediction\nx_input = array([[70, 75], [80, 85], [90, 95]])\nx_input = x_input.reshape((1, n_steps_in, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","9cec09a5":"# multivariate multi-step data preparation\nfrom numpy import array\nfrom numpy import hstack\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\n\n# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps_in, n_steps_out):\n\tX, y = list(), list()\n\tfor i in range(len(sequences)):\n\t\t# find the end of this pattern\n\t\tend_ix = i + n_steps_in\n\t\tout_end_ix = end_ix + n_steps_out\n\t\t# check if we are beyond the dataset\n\t\tif out_end_ix > len(sequences):\n\t\t\tbreak\n\t\t# gather input and output parts of the pattern\n\t\tseq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n\t\tX.append(seq_x)\n\t\ty.append(seq_y)\n\treturn array(X), array(y)\n\n# define input sequence\nin_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\nin_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2, out_seq))\n# choose a number of time steps\nn_steps_in, n_steps_out = 3, 2\n# covert into input\/output\nX, y = split_sequences(dataset, n_steps_in, n_steps_out)\nprint(X.shape, y.shape)\n# summarize the data\nfor i in range(len(X)):\n\tprint(X[i], y[i])","89720f01":"# multivariate multi-step encoder-decoder lstm example\nfrom numpy import array\nfrom numpy import hstack\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\n\n# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps_in, n_steps_out):\n\tX, y = list(), list()\n\tfor i in range(len(sequences)):\n\t\t# find the end of this pattern\n\t\tend_ix = i + n_steps_in\n\t\tout_end_ix = end_ix + n_steps_out\n\t\t# check if we are beyond the dataset\n\t\tif out_end_ix > len(sequences):\n\t\t\tbreak\n\t\t# gather input and output parts of the pattern\n\t\tseq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n\t\tX.append(seq_x)\n\t\ty.append(seq_y)\n\treturn array(X), array(y)\n\n# define input sequence\nin_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\nin_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2, out_seq))\n# choose a number of time steps\nn_steps_in, n_steps_out = 3, 2\n# covert into input\/output\nX, y = split_sequences(dataset, n_steps_in, n_steps_out)\n# the dataset knows the number of features, e.g. 2\nn_features = X.shape[2]\n# define model\nmodel = Sequential()\nmodel.add(LSTM(200, activation='relu', input_shape=(n_steps_in, n_features)))\nmodel.add(RepeatVector(n_steps_out))\nmodel.add(LSTM(200, activation='relu', return_sequences=True))\nmodel.add(TimeDistributed(Dense(n_features)))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=300, verbose=0)\n# demonstrate prediction\nx_input = array([[60, 65, 125], [70, 75, 145], [80, 85, 165]])\nx_input = x_input.reshape((1, n_steps_in, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","8663e690":"We want to reuse the same CNN model when reading in each sub-sequence of data separately.\n\nThis can be achieved by wrapping the entire CNN model in a TimeDistributed wrapper that will apply the entire model once per input, in this case, once per input subsequence.\n\nThe CNN model first has a convolutional layer for reading across the subsequence that requires a number of filters and a kernel size to be specified. The number of filters is the number of reads or interpretations of the input sequence. The kernel size is the number of time steps included of each \u2018read\u2019 operation of the input sequence.\n\nThe convolution layer is followed by a max pooling layer that distills the filter maps down to 1\/2 of their size that includes the most salient features. These structures are then flattened down to a single one-dimensional vector to be used as a single input time step to the LSTM layer.","cb305870":"## Vanilla LSTM\n- A Vanilla LSTM is an LSTM model that has a single hidden layer of LSTM units, and an output layer used to make a prediction.\n\nWe can define a Vanilla LSTM for univariate time series forecasting as follows.","ddc38867":"# Multivariate Multi-Step LSTM Models\nIn the previous sections, we have looked at univariate, multivariate, and multi-step time series forecasting.\n\nIt is possible to mix and match the different types of LSTM models presented so far for the different problems. This too applies to time series forecasting problems that involve multivariate and multi-step forecasting, but it may be a little more challenging.\n\nIn this section, we will provide short examples of data preparation and modeling for multivariate multi-step time series forecasting as a template to ease this challenge, specifically:\n\n# Multiple Input Multi-Step Output.\nMultiple Parallel Input and Multi-Step Output.\nPerhaps the biggest stumbling block is in the preparation of data, so this is where we will focus our attention.\n\nMultiple Input Multi-Step Output\nThere are those multivariate time series forecasting problems where the output series is separate but dependent upon the input time series, and multiple time steps are required for the output series.\n\nFor example, consider our multivariate time series from a prior section:","a6c8a40d":"We can use either the Vector Output or Encoder-Decoder LSTM to model this problem. In this case, we will use the Encoder-Decoder model.\n\nThe complete example is listed below.\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/'","3ae60d23":"We are now ready to fit an LSTM model on this data.\n\nAny of the varieties of LSTMs in the previous section can be used, such as a Vanilla, Stacked, Bidirectional, CNN, or ConvLSTM model.\n\nWe will use a Vanilla LSTM where the number of time steps and parallel series (features) are specified for the input layer via the input_shape argument.","09cd2eb8":"# Multiple Parallel Input and Multi-Step Output","ce3741eb":"Running the example first prints the shape of the X and y components.\n\nWe can see that the X component has a three-dimensional structure.\n\nThe first dimension is the number of samples, in this case 7. The second dimension is the number of time steps per sample, in this case 3, the value specified to the function. Finally, the last dimension specifies the number of parallel time series or the number of variables, in this case 2 for the two parallel series.\n\nThis is the exact three-dimensional structure expected by an LSTM as input. The data is ready to use without further reshaping.\n\nWe can then see that the input and output for each sample is printed, showing the three time steps for each of the two input series and the associated output for each sample.","d5bb0f2d":"reference link: https:\/\/machinelearningmastery.com\/timedistributed-layer-for-long-short-term-memory-networks-in-python\/","c012b83f":"# CNN LSTM\nA convolutional neural network, or CNN for short, is a type of neural network developed for working with two-dimensional image data.\n\nThe CNN can be very effective at automatically extracting and learning features from one-dimensional sequence data such as univariate time series data.\n\nA CNN model can be used in a hybrid model with an LSTM backend where the CNN is used to interpret subsequences of input that together are provided as a sequence to an LSTM model to interpret. This hybrid model is called a CNN-LSTM.\n\nThe first step is to split the input sequences into subsequences that can be processed by the CNN model. For example, we can first split our univariate time series data into input\/output samples with four steps as input and one as output. Each sample can then be split into two sub-samples, each with two time steps. The CNN can interpret each subsequence of two time steps and provide a time series of interpretations of the subsequences to the LSTM model to process as input.\n\nWe can parameterize this and define the number of subsequences as n_seq and the number of time steps per subsequence as n_steps. The input data can then be reshaped to have the required structure","1ba5d751":"> We can now develop an LSTM model for multi-step predictions.\n\nA vector output or an encoder-decoder model could be used. In this case, we will demonstrate a vector output with a Stacked LSTM.\n\nThe complete example is listed below.","3170682c":"# Stacked LSTM \nMultiple hidden LSTM layers can be stacked one on top of another in what is referred to as a Stacked LSTM model.\n\nAn LSTM layer requires a three-dimensional input and LSTMs by default will produce a two-dimensional output as an interpretation from the end of the sequence.\n\nWe can address this by having the LSTM output a value for each time step in the input data by setting the return_sequences=True argument on the layer. This allows us to have 3D output from hidden LSTM layer as input to the next.\n\nWe can therefore define a Stacked LSTM as follows.","d1f75ba5":"As with the univariate time series, we must structure these data into samples with input and output elements.\n\nAn LSTM model needs sufficient context to learn a mapping from an input sequence to an output value. LSTMs can support parallel input time series as separate variables or features. Therefore, we need to split the data into samples maintaining the order of observations across the two input sequences.\n\nIf we chose three input time steps, then the first sample would look as follows:\n\n### Input:\n\n10, 15\n20, 25\n30, 35\n### Output:\n\n65\n","00141af5":"The **split_sequence()** function below implements this behavior and will split a given univariate sequence into multiple samples where each sample has a specified number of time steps and the output is a single time step","77ab500a":"# Bidirectional LSTM\nOn some sequence prediction problems, it can be beneficial to allow the LSTM model to learn the input sequence both forward and backwards and concatenate both interpretations.\n\nThis is called a Bidirectional LSTM.\n\nWe can implement a Bidirectional LSTM for univariate time series forecasting by wrapping the first hidden layer in a wrapper layer called Bidirectional.\n\nAn example of defining a Bidirectional LSTM to read input both forward and backward is as follows.","f4ac2d02":"# Vector Output Model\nLike other types of neural network models, the LSTM can output a vector directly that can be interpreted as a multi-step forecast.\n\nThis approach was seen in the previous section were one time step of each output time series was forecasted as a vector.\n\nAs with the LSTMs for univariate data in a prior section, the prepared samples must first be reshaped. The LSTM expects data to have a three-dimensional structure of [samples, timesteps, features], and in this case, we only have one feature so the reshape is straightforward.","d8899e30":"# Univariate LSTM Models\nLSTMs can be used to model univariate time series forecasting problems.\n\nThese are problems comprised of a single series of observations and a model is required to learn from the series of past observations to predict the next value in the sequence.\n\nWe will demonstrate a number of variations of the LSTM model for univariate time series forecasting.\n\nThis section is divided into six parts; they are:\n\nData Preparation\nVanilla LSTM\nStacked LSTM\nBidirectional LSTM\nCNN LSTM\nConvLSTM\nEach of these models are demonstrated for one-step univariate time series forecasting, but can easily be adapted and used as the input part of a model for other types of time series forecasting problems.\n\nData Preparation\nBefore a univariate series can be modeled, it must be prepared.\n\nThe LSTM model will learn a function that maps a sequence of past observations as input to an output observation. As such, the sequence of observations must be transformed into multiple examples from which the LSTM can learn","0715eb50":"\nThat is, the first three time steps of each parallel series are provided as input to the model and the model associates this with the value in the output series at the third time step, in this case, 65.\n\nWe can see that, in transforming the time series into input\/output samples to train the model, that we will have to discard some values from the output time series where we do not have values in the input time series at prior time steps. In turn, the choice of the size of the number of input time steps will have an important effect on how much of the training data is used.\n\nWe can define a function named split_sequences() that will take a dataset as we have defined it with rows for time steps and columns for parallel series and return input\/output samples.","69ced6d2":"Running the example first prints the shape of the prepared X and y components.\n\nThe shape of X is three-dimensional, including the number of samples (6), the number of time steps chosen per sample (3), and the number of parallel time series or features (3).\n\nThe shape of y is two-dimensional as we might expect for the number of samples (6) and the number of time variables per sample to be predicted (3).\n\nThe data is ready to use in an LSTM model that expects three-dimensional input and two-dimensional output shapes for the X and y components of each sample.\n\nThen, each of the samples is printed showing the input and output components of each sample.","4bfa79f2":"We can divide the sequence into multiple input\/output patterns called samples, where three time steps are used as input and one time step is used as output for the one-step prediction that is being learned.","6b7394af":"# Multiple Parallel Series","a0344f73":"# ConvLSTM\nA type of LSTM related to the CNN-LSTM is the ConvLSTM, where the convolutional reading of input is built directly into each LSTM unit.\n\nThe ConvLSTM was developed for reading two-dimensional spatial-temporal data, but can be adapted for use with univariate time series forecasting.\n\nThe layer expects input as a sequence of two-dimensional images, therefore the shape of input data must be:\n\n - [samples, timesteps, rows, columns, features]\n\n\nFor our purposes, we can split each sample into subsequences where timesteps will become the number of subsequences, or n_seq, and columns will be the number of time steps for each subsequence, or n_steps. The number of rows is fixed at 1 as we are working with one-dimensional data.\n\nWe can now reshape the prepared samples into the required structure","7a73e271":"We are now ready to fit an LSTM model on this data.\n\nAny of the varieties of LSTMs in the previous section can be used, such as a Vanilla, Stacked, Bidirectional, CNN, or ConvLSTM model.\n\nWe will use a Stacked LSTM where the number of time steps and parallel series (features) are specified for the input layer via the input_shape argument. The number of parallel series is also used in the specification of the number of values to predict by the model in the output layer; again, this is three.","5f85fd8b":"# Multi-Step LSTM Models\n\nA time series forecasting problem that requires a prediction of multiple time steps into the future can be referred to as multi-step time series forecasting.\n\nSpecifically, these are problems where the forecast horizon or interval is more than one time step.\n\nThere are two main types of LSTM models that can be used for multi-step forecasting; they are:\n\n- 1.Vector Output Model\n- 2.Encoder-Decoder Model\nBefore we look at these models, let\u2019s first look at the preparation of data for multi-step forecasting\n\n# Data Preparation\nAs with one-step forecasting, a time series used for multi-step time series forecasting must be split into samples with input and output components.\n\nBoth the input and output components will be comprised of multiple time steps and may or may not have the same number of steps.\n\nFor example, given the univariate time series:\n\n- [10, 20, 30, 40, 50, 60, 70, 80, 90]\n\nWe could use the last three time steps as input and forecast the next two time steps.\n\nThe first sample would look as follows:\n\nInput:\n\n- [10, 20, 30]\n\nOutput:\n\n- [40, 50]\n\nThe split_sequence() function below implements this behavior and will split a given univariate time series into samples with a specified number of input and output time steps.","e9a66bab":"# Encoder-Decoder Model\n\nA model specifically developed for forecasting variable length output sequences is called the Encoder-Decoder LSTM.\n\nThe model was designed for prediction problems where there are both input and output sequences, so-called sequence-to-sequence, or seq2seq problems, such as translating text from one language to another.\n\nThis model can be used for multi-step time series forecasting.\n\nAs its name suggests, the model is comprised of two sub-models: the encoder and the decoder.\n\nThe encoder is a model responsible for reading and interpreting the input sequence. The output of the encoder is a fixed length vector that represents the model\u2019s interpretation of the sequence. The encoder is traditionally a Vanilla LSTM model, although other encoder models can be used such as Stacked, Bidirectional, and CNN models.\n\n...\nmodel.add(LSTM(100, activation='relu', input_shape=(n_steps_in, n_features)))\n","de649857":"Key in the definition is the shape of the input; that is what the model expects as input for each sample in terms of the number of time steps and the number of features.\n\nWe are working with a univariate series, so the number of features is one, for one variable.\n\nThe number of time steps as input is the number we chose when preparing our dataset as an argument to the split_sequence() function.\n\nThe shape of the input for each sample is specified in the input_shape argument on the definition of first hidden layer.","fa59af7b":"# **Multivariate LSTM Models**\nMultivariate time series data means data where there is more than one observation for each time step.\n\nThere are two main models that we may require with multivariate time series data; they are:\n\nMultiple Input Series.\nMultiple Parallel Series.\nLet\u2019s take a look at each in turn.\n\n### **Multiple Input Series**\nA problem may have two or more parallel input time series and an output time series that is dependent on the input time series.\n\nThe input time series are parallel because each series has an observation at the same time steps.\n\nWe can demonstrate this with a simple example of two parallel input time series where the output series is the simple addition of the input series."}}