{"cell_type":{"ec648760":"code","b351f56d":"code","1b1471b0":"code","39405fe2":"code","8e0bbcd0":"code","8e117695":"code","e2cf9602":"code","06038a71":"code","239910f7":"code","2d88a8c4":"markdown","3d9baf44":"markdown","968e3af6":"markdown","2fba936c":"markdown"},"source":{"ec648760":"import numpy as np  # linear algebra\nimport pandas as pd  #\nfrom datetime import datetime\n\nfrom scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nfrom mlxtend.regressor import StackingCVRegressor\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nimport os","b351f56d":"#print(os.listdir(\".\/\"))\ntrain = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')\ncomp_ID = test['Id']\nprint(\"Train set size:\", train.shape)\nprint(\"Test set size:\", test.shape)\nprint('START data processing',   )\n\ntrain_ID = train['Id']\ntest_ID = test['Id']\n# Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)\n\n# Deleting outliers\ntrain = train[train.GrLivArea < 4500]\ntrain.reset_index(drop=True, inplace=True)\n\n# We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\ny = train.SalePrice.reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test\n\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)\nprint(features.shape)\n# Some of the non-numeric predictors are stored as numbers; we convert them into strings \nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)\n\nfeatures['Functional'] = features['Functional'].fillna('Typ')\nfeatures['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\nfeatures['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n\nfeatures[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    features[col] = features[col].fillna(0)\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    features[col] = features[col].fillna('None')\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    features[col] = features[col].fillna('None')\n\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\nobjects = []\nfor i in features.columns:\n    if features[i].dtype == object:\n        objects.append(i)\n\nfeatures.update(features[objects].fillna('None'))\n\nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n# Filling in the rest of the NA's\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics.append(i)\nfeatures.update(features[numerics].fillna(0))\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics2.append(i)\n\nskew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))\n\nfeatures = features.drop(['Utilities', 'Street', 'PoolQC', ], axis=1)\n\nfeatures['YrBltAndRemod'] = features['YearBuilt'] + features['YearRemodAdd']\nfeatures['TotalSF'] = features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n\nfeatures['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])\n\n# simplified features\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nprint(features.shape)\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\nprint(final_features.shape)\n\nX = final_features.iloc[:len(y), :]\nX_sub = final_features.iloc[len(X):, :]\n\nprint('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)\n\noutliers = [30, 88, 462, 631, 1322]\nX = X.drop(X.index[outliers])\ny = y.drop(y.index[outliers])\n\noverfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\noverfit.append('MSZoning_C (all)')\n\nX = X.drop(overfit, axis=1).copy()\nX_sub = X_sub.drop(overfit, axis=1).copy()\n\nprint('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)","1b1471b0":"# ################## ML ########################################\nprint('START ML',   )\n\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n# rmsle\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n# build our model scoring function\ndef cv_rmse(model):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return rmse\n\n# setup models    \nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\nridge = make_pipeline(RobustScaler(),\n                      RidgeCV(alphas=alphas_alt, cv=kfolds))\n\nlasso = make_pipeline(RobustScaler(),\n                      LassoCV(max_iter=1e7, alphas=alphas2,\n                              random_state=42, cv=kfolds))\n\nelasticnet = make_pipeline(RobustScaler(),\n                           ElasticNetCV(max_iter=1e7, alphas=e_alphas,\n                                        cv=kfolds, l1_ratio=e_l1ratio))\n\nsvr = make_pipeline(RobustScaler(),\n                    SVR(C=20, epsilon=0.008, gamma=0.0003, ))\n\ngbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                max_depth=4, max_features='sqrt',\n                                min_samples_leaf=15, min_samples_split=10,\n                                loss='huber', random_state=42)\n\nlightgbm = LGBMRegressor(objective='regression',\n                         num_leaves=4, learning_rate=0.01,\n                         n_estimators=5000, max_bin=200,\n                         bagging_fraction=0.75,\n                         bagging_freq=5,\n                         bagging_seed=7,\n                         feature_fraction=0.2,\n                         feature_fraction_seed=7,\n                         verbose=-1,\n                         # min_data_in_leaf=2,\n                         # min_sum_hessian_in_leaf=11\n                            )\n\nxgboost = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n                       max_depth=3, min_child_weight=0,\n                       gamma=0, subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:squarederror', nthread=-1,\n                       scale_pos_weight=1, seed=27,\n                       reg_alpha=0.00006)\n\n# stack\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost, use_features_in_secondary=True)\n\nXv = X.values\n\nclfs = []\nprint('START Fit')\n\nprint(  'elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\nclfs.append(elasticnet)\n\nprint(  'lasso')\nlasso_model_full_data = lasso.fit(X, y)\nclfs.append(lasso)\n\nprint(  'ridge')\nridge_model_full_data = ridge.fit(X, y)\nclfs.append(ridge)\n\nprint(  'svr')\nsvr_model_full_data = svr.fit(X, y)\nclfs.append(svr)\n\nprint(  'GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\nclfs.append(gbr)\n\nprint(  'xgboost')\nxgb_model_full_data = xgboost.fit(X, y)\nclfs.append(xgboost)\n\nprint(  'lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)\nclfs.append(lightgbm)\n\nprint(  'StackingCVRegressor')\nstack_gen_model = stack_gen.fit(X.values,y)\n#clfs.append(stack_gen)\n\ndef blend_models_predict(X=X):\n    return ((0.1* elastic_model_full_data.predict(X)) + \n            (0.1 * lasso_model_full_data.predict(X)) + \n            (0.05 * ridge_model_full_data.predict(X)) + \n            (0.1 * svr_model_full_data.predict(X)) + \n            (0.1 * gbr_model_full_data.predict(X)) + \n            (0.15 * xgb_model_full_data.predict(X)) + \n            (0.1 * lgb_model_full_data.predict(X)) + \n            (0.3 * stack_gen_model.predict(X.values)))\n\nprint('RMSLE score on train data:')\nprint(rmsle(y, blend_models_predict(X)))\nprint('MSE score on train data:')\nprint(mean_squared_error(y, blend_models_predict(X)))\nprint('MAE score on train data:')\nprint(mean_absolute_error(np.expm1(y), np.floor(np.expm1(blend_models_predict(X)))))","39405fe2":"len(clfs)","8e0bbcd0":"## Optimization\n\nfrom scipy.optimize import minimize\n\npredictions = []\nfor clf in clfs:\n    predictions.append(clf.predict(X))\n    \npredictions.append(stack_gen.predict(X.values))\n\ndef mse_func(weights):\n    #scipy minimize will pass the weights as a numpy array\n    final_prediction = 0\n    for weight, prediction in zip(weights, predictions):\n            final_prediction += weight*prediction\n    #return np.mean((y_test-final_prediction)**2)\n    return np.sqrt(mean_squared_error(y, final_prediction))\n    \nstarting_values = [0]*len(predictions)\n\ncons = ({'type':'ineq','fun':lambda w: 1-sum(w)})\n#our weights are bound between 0 and 1\nbounds = [(0,1)]*len(predictions)\n\nres = minimize(mse_func, starting_values, method='SLSQP', bounds=bounds, constraints=cons)\n\nprint('Ensamble Score: {best_score}'.format(best_score=res['fun']))\nprint('Best Weights: {weights}'.format(weights=res['x']))","8e117695":"print(res)","e2cf9602":"yy = 'abcdefgh'\ndd={yy[i]: k for i,k in enumerate(res['x'])}","06038a71":"def blend_models_predict(X1, **dd):\n    return (dd['a']*elasticnet.predict(X1) + dd['b']*lasso.predict(X1) + \n            dd['c']*ridge.predict(X1) + dd['d']*svr.predict(X1) + \n            dd['e']*gbr.predict(X1) + dd['f']*xgboost.predict(X1) +\n            dd['g']*lightgbm.predict(X1) + dd['h']*stack_gen.predict(X1.values))","239910f7":"subm = np.exp(blend_models_predict(X_sub, **dd))\nsubmission = pd.DataFrame({'Id': comp_ID, 'SalePrice': subm})\nsubmission.to_csv(\"submission28.csv\", index=False)","2d88a8c4":"Objective of this notebook is to improve the notebook published by Alex Lekov given below. In the notebook it is not clear how the author determined the weights. We tried to improve the solution by applying optimization to determine the best weights.\n\nhttps:\/\/www.kaggle.com\/itslek\/stack-blend-lrs-xgb-lgb-house-prices-k-v17\n\nhttps:\/\/www.kaggle.com\/hsperr\/finding-ensamble-weights","3d9baf44":"# Submit","968e3af6":"### Applying Optimization to determine best weights","2fba936c":"## Ensemble of 8 models - estimate optimal weights"}}