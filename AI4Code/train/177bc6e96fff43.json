{"cell_type":{"a3cd6c79":"code","49507d39":"code","5b5ea80d":"code","18a65f1b":"code","4baea2e3":"code","fb9274ee":"code","641892f7":"code","c20b9477":"code","6ac5b286":"code","e1455e6d":"code","7e1f3273":"code","86cb3737":"code","879a620a":"code","e6f81b3e":"code","9405b99e":"code","ad4c05fe":"code","948d320a":"code","201004c8":"code","df21b0c0":"code","a1f90198":"code","afbfe45f":"code","160605be":"code","060c9c83":"code","f67591a5":"code","52c39df9":"code","23f3a730":"code","2774f709":"code","5b4ec929":"code","e05f855d":"code","f588c543":"code","4b45c7fd":"code","d8172b46":"code","762c7b59":"code","d4a2d5ca":"code","60eb6d53":"code","e957bedd":"code","12a2ab7d":"code","7c31097a":"code","be221421":"code","2c6aba8d":"code","7b16ca16":"code","842c9cfb":"code","2462fe49":"code","8fbe4463":"code","b13875c2":"code","870ebe44":"markdown","85d90de9":"markdown","a572ecc6":"markdown","0584de43":"markdown","1b446965":"markdown","f11daba6":"markdown","90e9ba52":"markdown","17d6a693":"markdown","c7af54cb":"markdown","98e6dbce":"markdown","6a6a029d":"markdown","1b5fa0d6":"markdown","24e7a6ca":"markdown","1994f56f":"markdown"},"source":{"a3cd6c79":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","49507d39":"# Reading .csv file \nimport pandas as pd\ndata = pd.read_csv('..\/input\/large-random-tweets-from-pakistan\/Random Tweets from Pakistan- Cleaned- Anonymous.csv', encoding='ISO-8859-1')","5b5ea80d":"# To get the dimentions of the dataset.\ndata.shape","18a65f1b":"# To get the column names of the dataset\ndata.columns","4baea2e3":"# to print the consise summary of the data set\ndata.info()","fb9274ee":"# to compute a summary of statistics pertaining to the DataFrame columns\ndata.describe()","641892f7":"# to return the first five rows of the data frame\ndata.head()","c20b9477":"# To get the data types of the different features\/ columns\ndata.dtypes","6ac5b286":"# to check the missing or null values in the data set \ndata.isnull().sum()                                                     \nmiss_val = data.isnull().sum().sort_values(ascending=False)\nmiss_val = pd.DataFrame(data=data.isnull().sum().sort_values(ascending=False), columns=['MissvalCount'])\n\n# Add a new column to the dataframe and fill it with the percentage of missing values\nmiss_val['Percent'] = miss_val.MissvalCount.apply(lambda x : '{:.2f}'.format(float(x)\/data.shape[0] * 100)) \nmiss_val = miss_val[miss_val.MissvalCount > 0]\nmiss_val","e1455e6d":"# to check the the text of tweet at specified row\ndata['full_text'][10]","7e1f3273":"# to compute a summary of statistics pertaining to the DataFrame column full_text\ndata['full_text'].describe()","86cb3737":"# to compute a summary of statistics pertaining to the DataFrame location\ndata['location'].describe()","879a620a":"text = data['full_text']\nlocation = data['location']\n\nfor i in np.random.randint(1000, size=10):\n    print(f'Tweet # {i}: ', text[i], '=> Location: ', location[i], end='\\n' * 3)","e6f81b3e":"location = data.groupby('location')\nlocation.head()","9405b99e":"# drop rows with any missing values\ndata.dropna(inplace=True)","ad4c05fe":"# Removing duplicates\ndata.drop_duplicates()","948d320a":"data.head()","201004c8":"# drop the columns with highest missing values \ndata = data.drop(['Unnamed: 0', 'created_at_tweet', 'retweet_count', 'favorite_count','reply_count', 'location'], axis=1)\ndata.head()","df21b0c0":"from sklearn import decomposition\nfrom scipy import linalg\nimport re\n\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport string\nimport nltk\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n%matplotlib inline","a1f90198":"def remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n        \n    return input_txt ","afbfe45f":"# remove twitter handles (@user)\ndata['tidy_text'] = np.vectorize(remove_pattern)(data['full_text'], \"@[\\w]*\")","160605be":"# remove special characters, numbers, punctuations\ndata['tidy_text'] = data['tidy_text'].str.replace(\"[^a-zA-Z#]\", \" \")","060c9c83":"#Removing Short Words\ndata['tidy_text'] = data['tidy_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))","f67591a5":"def cleaning_URLs(data):\n    return re.sub('((www.[^s]+)|(https?:\/\/[^s]+))',' ',data)","52c39df9":"data['tidy_text'] = data['tidy_text'].apply(lambda x: cleaning_URLs(x))","23f3a730":"from textblob import TextBlob","2774f709":"def getSubjectivity(text):\n    return TextBlob(text).sentiment.subjectivity","5b4ec929":"def getpolarity(text):\n    return TextBlob(text).sentiment.polarity","e05f855d":"data['Subjectivity'] = data['tidy_text'].apply(getSubjectivity)","f588c543":"data['Polarity'] = data['tidy_text'].apply(getpolarity)","4b45c7fd":"data.head(10)","d8172b46":"def getPositiveNegativeWordCount(score):\n    if score < 0:\n        return 'Negative'\n    else:\n        return 'Positive'","762c7b59":"data['Positive Negative Word Count'] =  data['Polarity'].apply(getPositiveNegativeWordCount)","d4a2d5ca":"import matplotlib.pyplot as plt \nimport seaborn as sns\nimport string\nimport nltk\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n%matplotlib inline\nfrom wordcloud import WordCloud ","60eb6d53":"negative_words =' '.join([text for text in data['tidy_text'][data['Positive Negative Word Count'] == 'Negative']])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(negative_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()\n","e957bedd":"positive_words =' '.join([text for text in data['tidy_text'][data['Positive Negative Word Count'] == 'Positive']])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(positive_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","12a2ab7d":"def wordcount_extract(x):\n    wordCount = []\n    # Loop over the words in the tweet\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        wordCount.append(ht)\n\n    return wordCount","7c31097a":"negativeWordCount = wordcount_extract(data['tidy_text'][data['Positive Negative Word Count'] == 'Negative'])\n\n# extracting hashtags from racist\/sexist tweets\npositiveWordCount = wordcount_extract(data['tidy_text'][data['Positive Negative Word Count'] == 'Positive'])\n\n# unnesting list\nnegativeWordCount = sum(negativeWordCount,[])\npositiveWordCount = sum(positiveWordCount,[])","be221421":"neg = nltk.FreqDist(negativeWordCount)\nnegWordCount = sum(list(neg.values()))","2c6aba8d":"pos = nltk.FreqDist(positiveWordCount)\nposWordCount = sum(list(pos.values()))","7b16ca16":"print(\"Negative Word Count: \",negWordCount)","842c9cfb":"print(\"Positive Word Count: \",posWordCount)","2462fe49":"data['Positive Negative Word Count'].value_counts()\n\nplt.title('Positive Negative Word Count')\nplt.xlabel('')\nplt.ylabel('Counts')\ndata['Positive Negative Word Count'].value_counts().plot(kind='bar')\nplt.show()","8fbe4463":"all_words = ' '.join([text for text in data['tidy_text']])\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","b13875c2":"all_words = ' '.join([text for text in data['tidy_text']])\nfrom wordcloud import WordCloud\n\nnormal_word = r\"(?:\\w[\\w']+)\"\nemoji = r\"(?:[^\\s])(?<![\\w{ascii_printable}])\".format(ascii_printable=string.printable)\nregexp = r\"{normal_word}|{emoji}\".format(normal_word=normal_word,emoji=emoji)\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, regexp=regexp ).generate(all_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","870ebe44":"# Remove Short Words","85d90de9":"# EXPLORATORY DATA ANALYSIS","a572ecc6":"# Droping rows with missing Values","0584de43":"# Removing Missing Values","1b446965":"# Removing Duplicates","f11daba6":"# Create a plot between positive and negative word counts","90e9ba52":"# Remove special characters, numbers, punctuations","17d6a693":"# Emoji Cloud","c7af54cb":"**Positive Words**","98e6dbce":"**Negative Words**","6a6a029d":"# Word Cloud","1b5fa0d6":"# Remove twitter handles (@user)","24e7a6ca":"# Remove URLs","1994f56f":"#  Claening Tweets"}}