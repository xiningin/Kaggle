{"cell_type":{"ca80eba9":"code","99522d46":"code","f444b2c0":"code","8be60af9":"code","88aa33b1":"code","723bd377":"code","ca193422":"code","54fda15b":"code","d03b404a":"code","c279fd78":"code","74638a3d":"code","a543f690":"code","e0c7171c":"code","4130f606":"code","bf05d21b":"code","f3a0895d":"code","33be9e9e":"code","417b632a":"code","1b51d059":"code","756fbf7a":"code","ddc65c3b":"code","1f41df7b":"code","0f1415df":"code","fe38db57":"code","41d881a7":"code","70f2c6cd":"code","447668db":"code","b06058cc":"code","17582ba4":"code","7ba179f1":"code","97c6eab0":"code","ae4db625":"markdown","4d2546b4":"markdown","d8a51aef":"markdown","8daec169":"markdown","d66e301a":"markdown","7123a43e":"markdown","dafa658a":"markdown","d928ddf2":"markdown","cdd39671":"markdown","fc90e32b":"markdown","30916ce8":"markdown","0ec2d03a":"markdown","ac218920":"markdown","8455af92":"markdown","ae532b79":"markdown","90ae7351":"markdown","2f8487ca":"markdown","3f1a0363":"markdown"},"source":{"ca80eba9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","99522d46":"# imports\n\n# seaborn for visualization\nimport seaborn as sns\n\n# preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Pipeline\nfrom sklearn.pipeline import Pipeline\n\n# column transformer\nfrom sklearn.compose import ColumnTransformer\n\n# cross validation\nfrom sklearn.model_selection import cross_val_score\n\n# hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\n\n# model\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n","f444b2c0":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","8be60af9":"train_df.head(3)","88aa33b1":"train_df.describe().iloc[:3,:]","723bd377":"train_df.info()","ca193422":"# check for null values in training data\ntrain_df.isna().sum()","54fda15b":"# check for null values in testing data\ntest_df.isna().sum()","d03b404a":"sns.heatmap(train_df.corr(),annot=True)","c279fd78":"print('Length of training data', len(train_df))\nX = train_df.drop(columns=['Survived'])\ny = train_df['Survived']","74638a3d":"#  Features and transformers\n\n#  integer category\nint_cat_features = ['Pclass', 'SibSp', 'Parch']\nint_cat_transformers = Pipeline(steps=[('imputer', SimpleImputer()),\\\n                                      ('scale', StandardScaler())])\n\n# string category\nstr_cat_features = ['Sex', 'Cabin', 'Embarked']\nstr_cat_transformers = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\\\n                                       ('one-hot', OneHotEncoder(handle_unknown='ignore'))])\n\n# continues neumerical - floats\nnum_features = ['Age', 'Fare']\nnum_transformers = Pipeline(steps=[('imputer', SimpleImputer()),\\\n                                   ('scale', StandardScaler())])","a543f690":"# Model building\n\ndef model_building(model):\n    #applying transformations\n    preprocess = ColumnTransformer(transformers=[('int_cat', int_cat_transformers, int_cat_features),\\\n                                                 ('str_cat', str_cat_transformers, str_cat_features),\\\n                                                 ('numeric', num_transformers, num_features)])\\\n    # preprocessing and modeling pipeline\n    pipe = Pipeline(steps=[('preprocessing', preprocess),\\\n                           ('modeling', model)])\n    \n    return pipe\n    \n# cross validating\ndef cross_validate_pipeline(pipeline, X, y):\n    cv_scores = cross_val_score(pipeline, X, y)\n    return cv_scores","e0c7171c":"\nmodels = [('RandomForest',RandomForestClassifier()), \\\n          ('LogisticRegression',LogisticRegression()), \\\n          ('GradientBoosting',GradientBoostingClassifier()), \\\n          ('SVC',SVC()), \\\n          ('SGDClassifier',SGDClassifier()), \\\n          ('XGBClassifier',XGBClassifier(use_label_encoder=False, eval_metric='logloss')) \\\n         ]\n\nfor name,model in models:\n    model_pipeline = model_building(model)\n    cv_scores = cross_validate_pipeline(model_pipeline, X, y)\n    print(f'{name :20} {cv_scores.mean()} ')","4130f606":"models = [('RandomForest', \\\n           RandomForestClassifier(), \\\n           {'modeling__max_depth':[i for i in range(4,12)]}), \\\n          \n          ('LogisticRegression', \\\n           LogisticRegression(), \\\n           {'modeling__C':[i*0.1 for i in range(10,15)]}), \\\n          \n          ('GradientBoosting', \\\n           GradientBoostingClassifier(), \\\n           {'modeling__n_estimators':[i for i in range(100,300,50)]}), \\\n          \n          ('SVC', \\\n           SVC(), \\\n           {'modeling__C':[i for i in range(1,10)]}), \\\n          \n          ('SGDClassifier',SGDClassifier(), \\\n           {'modeling__warm_start':[True,False], \\\n            'modeling__early_stopping':[True,False], \\\n            'modeling__average':[True,False]}), \\\n          \n          ('XGBClassifier', \\\n           XGBClassifier(use_label_encoder=False, eval_metric='logloss'), \\\n           {'modeling__colsample_bytree':[0.7], \\\n            'modeling__colsample_bylevel':[0.5], \\\n            'modeling__colsample_bynode':[0.7], \\\n            'modeling__subsample':[0.6,0.7]}) \\\n         ]\n\nfor name, model, param_grid in models:\n    pipe = model_building(model)\n    gs = GridSearchCV(pipe, param_grid)\n    gs.fit(X,y)\n    print(f'{name :20} {gs.best_score_}')","bf05d21b":"# classifier for making predictions\nclf = SVC()\n# modeling\nmodel = model_building(clf)\n# training\nmodel.fit(X,y)\n# making predictions\npreds = model.predict(test_df)","f3a0895d":"output = pd.DataFrame(data={'PassengerId':test_df['PassengerId'],'Survived':preds})\noutput.to_csv('submission.csv', index=False)","33be9e9e":"print('Your submission was successfully saved!')","417b632a":"import tensorflow as tf","1b51d059":"titanic_types = {}\n# create a dict of column names and type\nfor k,v in train_df.items():\n    titanic_types[k]=str(v.dtype)\n    \ntitanic_types","756fbf7a":"# Feature selection\n\nint_cat_features = ['Pclass', 'SibSp', 'Parch']\nstr_cat_features = ['Sex', 'Cabin', 'Embarked']\nnum_features = ['Age', 'Fare']\n\nall_features=[*int_cat_features, *str_cat_features, *num_features]\n\ntitanic_types = {}\n# create a dict of column names and type\nfor k,v in train_df[all_features].items():\n    titanic_types[k]=v.dtype\n    \ntitanic_types","ddc65c3b":"# create Symbolic Tensors \ninputs={}\n\nfor k,v in titanic_types.items():        \n    inputs[k]=tf.keras.Input(shape=(1,),name=k, dtype=v)\n\n# input features \ninputs","1f41df7b":"# Data preprocessing util\n\ndef encode_numeric(numeric_features, feature_names, df):\n    df = df.copy()\n    df = df[feature_names].fillna(method='ffill')\n    normalizer = tf.keras.layers.Normalization()\n    normalizer.adapt(np.array(df))\n    return normalizer(numeric_features)\n\ndef integer_categorical_encoding(cat_feature, feature_name, df):\n    df = df.copy()\n    df = df[feature_name].fillna(-1)\n    lookup = tf.keras.layers.IntegerLookup()\n    lookup.adapt(df)\n    cat_encode = tf.keras.layers.CategoryEncoding(num_tokens=lookup.vocabulary_size()) \n    return cat_encode(lookup(cat_feature))\n\ndef string_categorical_encoding(cat_feature, feature_name, df):\n    df = df.copy()\n    df = df[feature_name].fillna('missing')\n    lookup = tf.keras.layers.StringLookup()\n    lookup.adapt(df)\n    cat_encode = tf.keras.layers.CategoryEncoding(num_tokens=lookup.vocabulary_size()) \n    return cat_encode(lookup(cat_feature))","0f1415df":"\nnumerical_encoding =[]\n\n# features that need identical preprocessing \nnumerical_inputs = [inputs[name] for name in num_features]\n\n# concatenate them together before applying the preprocessing.\nnumerical_features = tf.keras.layers.Concatenate()(numerical_inputs)\n\n# applying preprocessing\nencoded = encode_numeric(numerical_features, num_features, train_df)\n\nnumerical_encoding.append(encoded)\n","fe38db57":"# encoding categorical integers\nint_cat_encoding = []\nfor name in int_cat_features:\n    feature = inputs[name]\n    encoding = integer_categorical_encoding(feature, name, train_df)\n    int_cat_encoding.append(encoding)\n    \n# encoding categorical string\nstr_cat_encoding = []\nfor name in str_cat_features:\n    feature = inputs[name]\n    encoding = string_categorical_encoding(feature, name, train_df)\n    str_cat_encoding.append(encoding)","41d881a7":"# all encoded inputs \nencoded_inputs = [*numerical_encoding, *int_cat_encoding, *str_cat_encoding ]\nencoded_inputs","70f2c6cd":"\npreprocessed = tf.keras.layers.Concatenate()(encoded_inputs)\n\ntitanic_preprocessing = tf.keras.Model(inputs, preprocessed)\n\n# lets plot the preprocessing\n\ntf.keras.utils.plot_model(titanic_preprocessing,\n                          show_shapes=True,\n                          rankdir='LR')","447668db":"# BODY OF THE MODEL\nfully_connected = tf.keras.Sequential([ \\\n    tf.keras.layers.Dense(32, activation='relu'), \\\n    tf.keras.layers.Dropout(0.2), \\\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(1), \\\n]) ","b06058cc":"def build_tf_model(inputs, preprocessing, fullyconnected):\n    prep = preprocessing(inputs)\n    result = fullyconnected(prep)\n    model = tf.keras.Model(inputs, result)\n    \n    # compile the model\n    model.compile(optimizer='adam', \\\n                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \\\n                  metrics=['accuracy'] )\n    return model","17582ba4":"# Training data for out tensorflow model\n\ntraining_data = {}\nfor k,v in titanic_types.items():\n    na_impute = 'missing' if v=='object' else -1\n    vals = train_df[k].fillna(na_impute)\n    training_data[k]=tf.cast(vals, dtype=v)\n\ntraining_labels = train_df['Survived']","7ba179f1":"# early stopping callback\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=1)\n\n# building titanic model\ntf_model = build_tf_model(inputs,\\\n                          titanic_preprocessing, \\\n                          fully_connected)\n# training \ntf_history = tf_model.fit(x=training_data, \\\n                          y=training_labels, \\\n                          validation_split=0.2, \\\n                          callbacks=[early_stopping], \\\n                          epochs=10)","97c6eab0":"# ploting model performance\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplt.plot(tf_history.history['accuracy'],color='green')\nplt.title(\"MODEL ACCURACY\")\nplt.xlabel('Epochs')\nplt.subplot(1,2,2)\nplt.plot(tf_history.history['loss'],color='salmon')\nplt.title(\"MODEL LOSS\")\nplt.xlabel('Epochs')\nplt.show()","ae4db625":"## Modeling","4d2546b4":"## Kaggle submission","d8a51aef":"**Titanic model using Tensorflow**","8daec169":"**Now lets build the neural network on top this**","d66e301a":"Just for fun lets solve this problem using Tensorflow\n## Using Tensorflow","7123a43e":"Concatenate all the preprocessed features along the depth axis, so each dictionary-example is converted into a single vector. The vector contains categorical features, numeric features","dafa658a":"**Continuous numerical encoding**\n\nNote: If you have many features that need identical preprocessing it's more efficient to concatenate them together befofre applying the preprocessing.","d928ddf2":"## Load the Data","cdd39671":"## Explore the data","fc90e32b":"It you're passing a heterogenous DataFrame to Keras, each column may need unique preprocessing. You could do this preprocessing directly in the DataFrame, but for a model to work correctly, inputs always need to be preprocessed the same way. So, the best approach is to build the preprocessing into the model.\n\n\n\n**`\"Symbolic\" tensors`**. <br>\nNormal \"eager\" tensors have a value. In contrast these \"symbolic\" tensors do not. Instead they keep track of which operations are run on them, and build representation of the calculation, that you can run later.\n\nThe functional API operates on Symbolic\" tensors","30916ce8":"**Model Training**","0ec2d03a":"**Categorical encoding**","ac218920":"## Visualize the data","8455af92":"## Making Final predictions","ae532b79":"## Model Improvement ","90ae7351":"Keras models don't automatically convert Pandas DataFrames because it's not clear if it should be converted to one tensor or to a dictionary of tensors. So convert it to a dictionary of tensors:","2f8487ca":"## Model comparison","3f1a0363":"## 1. Modeling \n> `sklearn.pipeline.Pipeline` Sequentially apply a list of transforms and a final estimator.The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters.\n\n## 2. Data preprocessing\n* Check for null values\n> if there are null values then impute them\n\n* Check for Categorical feature columns and encode them \n>  feature-column `Sex` can be ecoded using a One-hot-encoder <br>\n>  feature-column `Pclass` can be ecoded using a One-hot-encoder <br>\n>  etc....\n\n* Check the mean and standard deviation of the features <br>\n> Its a good practice to normalize the features that have different scales and range.\nThis is important because the features are multiplied by model weights so the scale of the output and the scale of the gradient are affected by the scale of the inputs\n\n\n### Numerical categorical Features\nLet's encode them using OneHotEncoder\n> * 'Pclass'\n> * 'SibSp'\n> * 'Parch'\n\n### String Categorical Features\nLet's first impute the missing values using a SimpleImputer then\nLet's encode them using OneHotEncoder\n> * 'Sex'\n> * 'Cabin'\n> * 'Embarked'\n\n### Numerical features\nLet's first impute the missing values using a SimpleImputer then\nLet's Standardize features by removing the mean and scaling to unit variance\n> * 'Fare'\n> * 'Age'\n\n## 4. Cross-validate the model\n> Evaluate a score by cross-validation\n\n## 5. Compare different model scores\n> * RandomForestClassifier\n> * GradientBoostingClassifier\n> * LogisticRegression\n> * SGDClassifier\n> * SVC\n> * XGBClassifier\n\n## 6. Hyperparameter tuning \/ model improvement\n> we can use Grid-search-cv to find best model parameters that give us best score\n\n## 7. Kaggle submission\n> we can use the hypertuned \/ best model to make kaggle submission"}}