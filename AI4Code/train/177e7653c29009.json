{"cell_type":{"5302a0bc":"code","9b0ea19c":"code","e3f235cd":"code","f285907d":"code","d435dfdc":"code","3e89638e":"code","3fb006cd":"code","1b24a102":"code","779b2ac8":"code","6e640e06":"code","0a9543e6":"code","658b5dda":"code","54a0d6f3":"code","54446a9c":"code","fb55cc44":"code","08c4e41e":"code","532cec5c":"code","431e6028":"code","1a46e34b":"code","c7b80a59":"code","5f940327":"code","c36f1a8d":"code","a4e17439":"code","f47ffeee":"code","1a53005d":"code","705df72b":"code","26cea05f":"code","e59bf775":"code","9db3da0f":"code","dc34d616":"code","1c8050de":"code","bddc4a42":"code","de2423a3":"code","d69489a0":"code","cb414088":"code","45a80eed":"code","63ea5575":"code","1b672c3b":"code","98403c2d":"code","df04eec7":"code","e13dbc9a":"code","4e1adf32":"code","ff07c237":"code","a0ff5272":"code","4dd72afb":"markdown","257faf0d":"markdown","b85d5a6b":"markdown","a219b006":"markdown","4a90932b":"markdown","708f4eed":"markdown","c7304d4e":"markdown","126c71e0":"markdown","0c368562":"markdown","6f57d45e":"markdown","b69b3a76":"markdown","0738e293":"markdown","bc23e583":"markdown","ce0659dc":"markdown","132aefa9":"markdown","f006f997":"markdown","b5182bd0":"markdown","4af7e9c5":"markdown","19ab46b8":"markdown","85c37055":"markdown","601a6975":"markdown"},"source":{"5302a0bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9b0ea19c":"data=pd.read_csv(\"\/kaggle\/input\/graduate-admissions\/Admission_Predict.csv\")","e3f235cd":"data","f285907d":"data.info()","d435dfdc":"# Let's delete the parts we will not use.\ndata.drop([\"Serial No.\"],axis=1,inplace =True)","3e89638e":"data","3fb006cd":"y=data.Research.values\n\nx_data=data.drop([\"Research\"],axis=1)\n\nx=(x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\n","1b24a102":"# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size = 0.2,random_state=42)\n\nx_train=x_train.T\nx_test=x_test.T\ny_train =y_train.T\ny_test =y_test.T\n","779b2ac8":"def initialize_weights_and_bias(dimension):\n    \n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\n\n# w,b = initialize_weights_and_bias(30)\n\ndef sigmoid(z):\n    \n    y_head = 1\/(1+ np.exp(-z))\n    return y_head\n# print(sigmoid(0))\n\n# %%\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients\n\n#%% Updating(learning) parameters\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n\n#%%  # prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n\n# %% logistic_regression\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 300)    \n","6e640e06":"# %% logistic_regression\nfrom sklearn.linear_model import LogisticRegression\n\nlr=LogisticRegression()\n\nlr.fit(x_train.T,y_train.T)\n\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))","0a9543e6":"color_list = ['red' if i==1 else 'green' for i in data.loc[:,'Research']]","658b5dda":"pd.plotting.scatter_matrix(data.loc[:, data.columns != 'Research'],\n                                       c=color_list,\n                                       figsize= [20,20],\n                                       diagonal='hist',\n                                       alpha=0.3,\n                                       s = 200,\n                                       marker = '*',\n                                       edgecolor= \"black\")","54a0d6f3":"experienced=data[data.Research== 1]\n\ninexperienced=data[data.Research==0]\n","54446a9c":"#%% normalization\n\nplt.scatter(experienced[\"GRE Score\"],experienced[\"TOEFL Score\"],color=\"yellow\",alpha=0.8)\n\nplt.scatter(inexperienced[\"GRE Score\"],inexperienced[\"TOEFL Score\"],color=\"red\",alpha=0.8)\nplt.xlabel(\"GRE Score\")\nplt.ylabel(\"TOEFL Score\")\nplt.legend()\nplt.show()\n","fb55cc44":"y=data.Research.values\nx_data=data.drop([\"Research\"],axis=1)\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))\n","08c4e41e":"# %% train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=1)\n","532cec5c":"# knn model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_train,y_train)\nprediction=knn.predict(x_test)\n\nprint(\" {} nn score: {} \".format(3,knn.score(x_test,y_test)))","431e6028":"#%% normalization\n\nscore_list=[]\nfor each in range(1, 25):\n    knn2=KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,25),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","1a46e34b":"#%% normalization\n\nplt.scatter(experienced[\"GRE Score\"],experienced[\"TOEFL Score\"],color=\"yellow\",alpha=0.8)\nplt.scatter(inexperienced[\"GRE Score\"],inexperienced[\"TOEFL Score\"],color=\"red\",alpha=0.8)\nplt.xlabel(\"GRE Score\")\nplt.ylabel(\"TOEFL Score\")\nplt.legend()\nplt.show()","c7b80a59":"y=data.Research.values\n\nx_data=data.drop([\"Research\"],axis=1)\n\nx=(x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))","5f940327":"# %% train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size = 0.2,random_state=1)","c36f1a8d":"# %% SVM\nfrom sklearn.svm import SVC\n\nsvm=SVC(random_state=1)\nsvm.fit(x_train,y_train)\n\nprint(\"score :\",svm.score(x_test,y_test))","a4e17439":"#%% normalization\n\nplt.scatter(experienced[\"GRE Score\"],experienced[\"TOEFL Score\"],color=\"yellow\",alpha=0.8)\nplt.scatter(inexperienced[\"GRE Score\"],inexperienced[\"TOEFL Score\"],color=\"red\",alpha=0.8)\nplt.xlabel(\"GRE Score\")\nplt.ylabel(\"TOEFL Score\")\nplt.legend()\nplt.show()","f47ffeee":"y=data.Research.values\nx_data=data.drop([\"Research\"],axis=1)\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))\n","1a53005d":"# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state=1)\n","705df72b":"# %% Naive bayes \nfrom sklearn.naive_bayes import GaussianNB\nnb=GaussianNB()\nnb.fit(x_train,y_train)\n\nprint(\"score :\",nb.score(x_test,y_test))","26cea05f":"# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test,y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state = 42)\n","e59bf775":"# %% Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n\nprint(\"score :\",dt.score(x_test, y_test))","9db3da0f":"#%% normalization\n\nplt.scatter(experienced[\"GRE Score\"],experienced[\"TOEFL Score\"],color=\"yellow\",alpha=0.8)\nplt.scatter(inexperienced[\"GRE Score\"],inexperienced[\"TOEFL Score\"],color=\"red\",alpha=0.8)\nplt.xlabel(\"GRE Score\")\nplt.ylabel(\"TOEFL Score\")\nplt.legend()\nplt.show()","dc34d616":"y=data.Research.values\nx_data=data.drop([\"Research\"],axis=1)\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))\n","1c8050de":"dt_liistem_esemble=[]\n\nfor i in range(1,20):\n    from sklearn.model_selection import train_test_split\n    x_train, x_test,y_train, y_test = train_test_split(x,y,test_size = i\/100,random_state = 42)\n    from sklearn.tree import DecisionTreeClassifier\n    dt = DecisionTreeClassifier()\n    dt.fit(x_train,y_train)\n    #print(\"{} .decision tree score:{} \".format(i,dt.score(x_test,y_test)))\n    dt_liistem_esemble.append(dt.score(x_test,y_test))\n","bddc4a42":"plt.plot(range(1,20),dt_liistem)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","de2423a3":"# Let's find the best result\nprint(\"max score :\",dt_liistem_esemble.index(max(dt_liistem_esemble)))","d69489a0":"# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test,y_train, y_test = train_test_split(x,y,test_size = 0.14,random_state = 42)\n","cb414088":"#%% decision tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nprint(\"decision tree score: \", dt.score(x_test,y_test))","45a80eed":"#%%  random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 40,random_state = 1)\nrf.fit(x_train,y_train)\nprint(\"random forest algo result: \",rf.score(x_test,y_test))","63ea5575":"dt_liistem_eva=[]\n\nfor i in range(2,99): \n    from sklearn.model_selection import train_test_split\n    x_train, x_test,y_train, y_test = train_test_split(x,y,test_size = i\/100,random_state = 42)\n    from sklearn.ensemble import RandomForestClassifier\n    rf = RandomForestClassifier(n_estimators = 100,random_state = 1)\n    rf.fit(x_train,y_train)\n    dt_liistem_eva.append(dt.score(x_test,y_test))\n","1b672c3b":"print(\"max score :\",max(dt_liistem_eva))","98403c2d":"dt_liistem_eva.index( max(dt_liistem_eva) )","df04eec7":"# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test,y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state = 42)","e13dbc9a":"#%%  random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 100,random_state = 1)\nrf.fit(x_train,y_train)\nprint(\"random forest algo result: \",rf.score(x_test,y_test))\n\n\ny_pred = rf.predict(x_test)\ny_true = y_test","4e1adf32":"#%% confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n","ff07c237":"# %% cm visualization\n\nf, ax = plt.subplots(figsize =(8,8))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","a0ff5272":"print(\"Logistic Regression Classification {}\".format(0.7875 ))\n\nprint(\" {}K-Nearest Neighbors (KNN): {} \".format(3,knn.score(x_test,y_test)))\n\nprint(\"Support Vector Machine(SVM) Classification :\",svm.score(x_test,y_test))\n\nprint(\"Naive Bayes Classification) :\",nb.score(x_test,y_test))\n\nprint(\"Decision Tree Classification :\",dt.score(x_test, y_test))\n\nprint(\"decision tree score: \", dt.score(x_test,y_test))\n\nprint(\"Random Forest Classification: \",rf.score(x_test,y_test))\n","4dd72afb":"<a id=\"7\"><\/a> <br>\n## 5. Decision Tree Classification","257faf0d":"<a id=\"2\"><\/a> <br>\n## B. SUPERVISED LEARNING","b85d5a6b":"## sample model;\n    \n* A confusion matrix is not a metric to evaluate a model, but it provides insight into the predictions. It is important to learn confusion matrix in order to comprehend other classification metrics such as precision and recall.\n* Confusion matrix goes deeper than classification accuracy by showing the correct and incorrect (i.e. true or false) predictions on each class. In case of a binary classification task, a confusion matrix is a 2x2 matrix. If there are three different classes, it is a 3x3 matrix and so on.\n\n<a ><img src=\"https:\/\/miro.medium.com\/max\/399\/1*kwwUKYNpy9LUF--FHprWCw.png\" alt=\"1\" border=\"0\">    \n    \n* Let\u2019s assume class A is positive class and class B is negative class. The key terms of confusion matrix are as follows:\n* True positive (TP): Predicting positive class as positive (ok)\n* False positive (FP): Predicting negative class as positive (not ok)\n* False negative (FN): Predicting positive class as negative (not ok)\n* True negative (TN): Predicting negative class as negative (ok)","a219b006":"* In this section we will look at the next steps after linear regression.\n\n<a ><img src=\"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2018\/08\/8-Machine-Learning-Algorithms-to-Learn-01-1.jpg\" alt=\"1\" border=\"0\">\n* Previously we showed Regression, now we will show Classification.\n* Let's first show the difference between an image and them.\n<a ><img src=\"https:\/\/i0.wp.com\/vinodsblog.com\/wp-content\/uploads\/2018\/11\/Classification-vs-Regression.png?fit=2048%2C1158&ssl=1\" alt=\"1\" border=\"0\">","4a90932b":"<a id=\"9\"><\/a> <br>\n## 7. Evaluation Classification Models","708f4eed":"* The dataset contains several parameters which are considered important during the application for Masters Programs.\n* The parameters included are :\n* GRE Scores ( out of 340 )\n* TOEFL Scores ( out of 120 )\n* University Rating ( out of 5 )\n* Statement of Purpose and Letter of Recommendation Strength ( out of 5 )\n* Undergraduate GPA ( out of 10 )\n* Research Experience ( either 0 or 1 )\n* Chance of Admit ( ranging from 0 to 1 )","c7304d4e":"**Content:**\n1. [data](#1)    \n1. [Supervised Learning](#2)\n    1. [Logistic Regression Classification](#3)\n    1. [K-Nearest Neighbors (KNN)](#4)\n    1. [Support Vector Machine(SVM) Classification](#5)\n    1. [Naive Bayes Classification)](#6)\n    1. [Decision Tree Classification](#7)\n    1. [Random Forest Classification](#8)\n    1. [Evaluation Classification Models](#9)\n            ","126c71e0":"<a id=\"5\"><\/a> <br>\n## 3. Support Vector Machine(SVM) Classification","0c368562":"## sample model;\n\n<a ><img src=\"https:\/\/miro.medium.com\/max\/564\/0*ToYXqRes95eMvIKV.png\" alt=\"1\" border=\"0\">\n    \n* Decision tree builds classification or regression models in the form of a tree structure. \n* It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. \n* The final result is a tree with decision nodes and leaf nodes. A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy). \n* Leaf node (e.g., Play) represents a classification or decision. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data. ","6f57d45e":"## sample model;\n\n<a ><img src=\"https:\/\/www.globalsoftwaresupport.com\/wp-content\/uploads\/2018\/02\/naivebayes8-768x389.png\" alt=\"1\" border=\"0\">\n    \n# Principle of Naive Bayes Classifier:\n* A Naive Bayes classifier is a probabilistic machine learning model that\u2019s used for classification task. The crux of the classifier is based on the Bayes theorem.\n# Bayes Theorem:\n<a ><img src=\"https:\/\/miro.medium.com\/max\/459\/1*tjcmj9cDQ-rHXAtxCu5bRQ.png\" alt=\"1\" border=\"0\">\n\n* Using Bayes theorem, we can find the probability of A happening, given that B has occurred. Here, B is the evidence and A is the hypothesis. The assumption made here is that the predictors\/features are independent. That is presence of one particular feature does not affect the other. Hence it is called naive.","b69b3a76":"## sample model;\n\n<a ><img src=\"https:\/\/helloacm.com\/wp-content\/uploads\/2016\/03\/logistic-regression-example.jpg\" alt=\"1\" border=\"0\">\n    \n* Logistic Regression is a regression method that works in the classification process.\n* Used in categorical or numerical classification. It works if the dependent variable, that is the result, can only have 2 different values. (Yes \/ No, Male \/ Female, Fat \/ Thin etc.)\n* Commonly used in linear classification problems. Therefore, it is not very similar to Linear Regression.","0738e293":"## sample model;\n\nLet\u2019s take a simple case to understand this algorithm. Following is a spread of red circles (RC) and green squares (GS) :\n<a ><img src=\"https:\/\/www.analyticsvidhya.com\/wp-content\/uploads\/2014\/10\/scenario1.png\" alt=\"1\" border=\"0\">\n\nYou intend to find out the class of the blue star (BS). BS can either be RC or GS and nothing else. The \u201cK\u201d is KNN algorithm is the nearest neighbor we wish to take the vote from. Let\u2019s say K = 3. Hence, we will now make a circle with BS as the center just as big as to enclose only three datapoints on the plane. Refer to the following diagram for more details:\n\n<a ><img src=\"https:\/\/www.analyticsvidhya.com\/wp-content\/uploads\/2014\/10\/scenario2.png\" alt=\"1\" border=\"0\">\n    \nThe three closest points to BS is all RC. Hence, with a good confidence level, we can say that the BS should belong to the class RC. Here, the choice became very obvious as all three votes from the closest neighbor went to RC. The choice of the parameter K is very crucial in this algorithm. Next, we will understand what are the factors to be considered to conclude the best K.\n","bc23e583":"<a id=\"8\"><\/a> <br>\n## 6. Random Forest Classification","ce0659dc":"*  **Let's choose a chart now.**","132aefa9":"## sample model;\n\n<a ><img src=\"https:\/\/miro.medium.com\/max\/1170\/1*58f1CZ8M4il0OZYg2oRN4w.png\" alt=\"1\" border=\"0\">\n    \n# Ensemble Algorithm :\n* Ensemble algorithms are those which combines more than one algorithms of same or different kind for classifying objects. For example, running prediction over Naive Bayes, SVM and Decision Tree and then taking vote for final consideration of class for test object.","f006f997":"<a id=\"3\"><\/a> <br>\n## 1. Logistic Regression Classification\n","b5182bd0":"<a id=\"4\"><\/a> <br>\n## 2.K-Nearest Neighbors (KNN)\n\n","4af7e9c5":"<a id=\"6\"><\/a> <br>\n## 4. Naive Bayes Classification","19ab46b8":"<a id=\"1\"><\/a> <br>\n## A. DATA","85c37055":"## sample model;\n\n<a ><img src=\"https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/support-vector-machine-algorithm.png\" alt=\"1\" border=\"0\">\n\n* Support Vector Machine is a classification algorithm similar to Logistic Regression. Both try to find the best line that separates the two classes. \n* The algorithm allows the line to be drawn to be adjusted in two classes so that it passes the furthest place to its elements. \n* It is a classifier that takes no parameters (nonparametric). \n* SVM can also classify linear and nonlinear data, but generally tries to classify the data linearly.    ","601a6975":"# ** Classification **"}}