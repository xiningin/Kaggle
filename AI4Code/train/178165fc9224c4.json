{"cell_type":{"de7a000e":"code","764d9a55":"code","00340353":"code","e80709c4":"code","cd1eb2d4":"code","9542c6c7":"code","2d03be6a":"code","9a4e672a":"code","15b7d773":"code","73ece0b3":"markdown","ce907998":"markdown","5e0a3edc":"markdown","1d924d9e":"markdown","58381e5b":"markdown","0b21278b":"markdown","d4953222":"markdown","33631f7d":"markdown","be37e056":"markdown"},"source":{"de7a000e":"import os\nimport codecs\nimport PIL.Image as Image\nimport torchvision\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn, optim\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom tqdm.notebook import tqdm","764d9a55":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","00340353":"!tar -xf \/kaggle\/input\/qaida-dataset\/train_200.tar.xz\n!tar -xf \/kaggle\/input\/qaida-dataset\/test_200.tar.xz","e80709c4":"class QaidaDataset(Dataset):\n    def __init__(self, data_dir, transform=None, max_classes=0):\n\n        if transform is None:\n            self.transform = torchvision.transforms.ToTensor()\n        else:\n            self.transform = transform\n\n        self.image_paths, self.labels = self.init(data_dir, max_classes)\n        self.__len = len(self.labels)\n\n    def init(self, data_dir, max_classes):\n        \"\"\"\n        Prepare sorted hdf file readers\n        \"\"\"\n\n        list_cls_dirs = sorted(os.listdir(data_dir), key=int)\n\n        if max_classes == 0:\n            print(\"Selecting max classes to : {}\".format(len(list_cls_dirs)))\n\n        elif max_classes > 0 and max_classes < len(list_cls_dirs):\n            list_cls_dirs = list_cls_dirs[:max_classes]\n            print(\"Selecting max classes to : {}\".format(max_classes))\n\n        else:\n            print(\"Invalid arg max_classes: {}, Selecting max classes to : {}\".format(max_classes, len(list_cls_dirs)))\n\n        labels = []\n        image_paths = []\n        for cls_dir in list_cls_dirs:\n            imgs_in_cls = [os.path.join(data_dir, cls_dir, img) for img in\n                           os.listdir(os.path.join(data_dir, cls_dir))]\n\n            num_imgs_in_cls = len(imgs_in_cls)\n            cls_id = int(cls_dir)\n\n            labels.extend([cls_id] * num_imgs_in_cls)\n            image_paths.extend(imgs_in_cls)\n\n        assert len(image_paths) == len(labels), \"Size of images do not match size of labels\"\n\n        return image_paths, labels\n\n    def __len__(self):\n        return self.__len\n\n    def __getitem__(self, idx):\n\n        lbl = int(self.labels[idx])\n\n        img_path = self.image_paths[idx]\n        img = Image.open(img_path)\n        img = img.convert('RGB')\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img.double(), lbl","cd1eb2d4":"class QRN18(nn.Module):\n    def __init__(self, target_classes):\n        super(QRN18, self).__init__()\n        self._model = torchvision.models.resnet18(pretrained=True)\n\n        # Freeze all feature extraction layers\n        for param in self._model.parameters():\n            param.requires_grad = False\n\n        # Replace the prediction head\n        fc = nn.Linear(512, target_classes)\n        fc.requires_grad = True\n        \n        self._model.fc = fc\n        self._model.fc.requires_grad = True\n\n    def forward(self, images):\n        return self._model(images)","9542c6c7":"dataset = QaidaDataset(\".\/train_200\/\")\n\n# load ligatures\nwith codecs.open('\/kaggle\/input\/qaida-dataset\/ligatures_map_200', encoding='UTF-16LE') as ligature_file:\n    ligatures_map = ligature_file.readlines()\n\n# display an image grid with random samples\nfig, ax = plt.subplots(5,5, figsize = (10,10))\nfor row in range(5):\n    for col in range(5):\n        idx = np.random.randint(len(dataset))\n        img, lbl = dataset[idx]\n        ax[row][col].imshow(np.transpose(img, (1,2,0)))\n        ax[row][col].set_title(lbl)\n        ax[row][col].axis(\"off\")\n        print (\"Class: {} -> {}\".format(lbl, ligatures_map[lbl]))\n\nplt.show()","2d03be6a":"# create mini dataset instance \nsmall_dataset = QaidaDataset(\".\/train_200\/\", max_classes= 10)\n\n# create dataloader\nsmall_dataloader = DataLoader(small_dataset, batch_size = 256, shuffle=True, num_workers=4)\n\n# train on GPU if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# create model_instance\nQRN18_model = QRN18(target_classes=10)\nQRN18_model = QRN18_model.double()\nQRN18_model.to(device)\n\n# create optimizer\ntrainable_parameters = [param for param in QRN18_model.parameters() if param.requires_grad]\noptimizer = optim.Adam(QRN18_model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n\n# Training loop\nepochs = 20 # total epochs\n\n\ntrain_loss =[]\n\nfor e in range(epochs):\n    running_loss = 0\n    for imgs, lbls in tqdm(small_dataloader):\n        imgs, lbls = imgs.to(device), lbls.to(device)\n\n        optimizer.zero_grad()\n        img = QRN18_model(imgs)\n        \n        loss = criterion(img, lbls)\n        running_loss += loss\n        loss.backward()\n        optimizer.step()\n    running_loss \/= len(small_dataloader)\n    print(\"Epoch : {}\/{}..\".format(e+1, epochs),\n      \"Training Loss: {:.6f}\".format(running_loss)) \n    train_loss.append(running_loss)","9a4e672a":"plt.plot(train_loss)\nplt.title(\"Training loss curve\")\nplt.xlabel(\"Training epochs\")\nplt.ylabel(\"Cross entropy loss\")\nplt.show()","15b7d773":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nQRN18_model.to(device)\nQRN18_model.eval()\n\nrunning_acc =0\nfor imgs, lbls in tqdm(small_dataloader):\n    imgs = imgs.to(device)\n    res = QRN18_model(imgs)\n    pred = torch.argmax(res.to('cpu'), axis =1)\n    acc = float(sum(pred == lbls))\/lbls.shape[0]\n    \n    running_acc += acc\n\ntotal_acc = round(running_acc\/len(small_dataloader), 3)\n\nprint (\"Total Acc: {}\".format(total_acc))","73ece0b3":"### Create a with ResNet18 backbone","ce907998":"### Check the list of files available","5e0a3edc":"# Create a sample workflow in PyTroch\nThis notebooks contains the code for creating a sample worflow including:\n- Custom pytorch dataloader\n- Creating a dataloader\n- Creating a model with resnet18 backbone\n- Training (overfitting) on first 10 classes to make sure the worflow is working\n\n**NOTE:** *This notebook does not provide the code for training with validation. Although all the components are already there, feel free to use them for training in 200\/2000\/complete(18569) classes.*","1d924d9e":"### Create a Custom PyTorch dataset class","58381e5b":"### Plot the training curve","0b21278b":"### Test the accuracy","d4953222":"### Extract the data","33631f7d":"### Create a dataset instance and display some sample images","be37e056":"### Overfit on 10 classes to test the sanity of the workflow"}}