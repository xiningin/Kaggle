{"cell_type":{"6795bc03":"code","dc689ce5":"code","7f70330f":"code","b892dc14":"code","1343b7b4":"code","2227acdd":"code","b5b8b2f0":"code","437ea08c":"code","f5f57f78":"code","09c96b59":"code","6f6d791f":"code","6ea6eb96":"code","4c369a9f":"code","221a486d":"code","2c2e0967":"code","c4718ac7":"code","dd9ff036":"code","43c98b2e":"code","2724728d":"code","2e5642e0":"code","370a7039":"code","5900adfa":"code","2540a0b5":"code","00142834":"code","d7861217":"code","27a5ded0":"code","8e430c71":"code","84d930c3":"code","2e11f35e":"code","44fa765d":"code","79395fde":"code","fc2ddc7e":"code","24cb4f84":"code","88d8e137":"code","2e884049":"code","f159cb6d":"code","19f021af":"code","7303a0fc":"code","6b226fff":"code","229619f8":"code","f180f5e9":"code","91588eaa":"code","b061a6c2":"code","4c9d4a0f":"code","d1e7d850":"code","845ad287":"code","13c4e1b9":"code","2c5cfdf3":"code","550c6b5f":"code","315550b4":"code","13f8e026":"code","589a926d":"code","f3daf3e6":"code","c2c9361e":"code","99b74647":"code","a2f4cb9d":"code","b619d576":"code","cc9789ac":"code","a51ee03f":"code","a66f85fa":"code","e948e50a":"code","24fbbd37":"code","da0079d6":"code","c17cb293":"code","7b12414d":"code","bbf0603a":"code","2fc609d9":"code","07da25c0":"code","22237f85":"code","d26b98cc":"code","158ea2b4":"code","a062d5a7":"code","99ebc6eb":"code","be7408be":"code","dd152c3e":"code","6e913a65":"code","c76dd9f4":"code","f8a69a0b":"code","b8498a0f":"code","3d7ad03d":"markdown","afbe4816":"markdown","40dd314b":"markdown","545cc2b5":"markdown","6cf5f755":"markdown","28ac11cf":"markdown","c819ba6a":"markdown","e7f44bae":"markdown","2b1ed34f":"markdown","9ad646b3":"markdown","0e9188c8":"markdown","a1d66720":"markdown","9fd30434":"markdown","0eae1e43":"markdown","49e72c54":"markdown","9c2b341c":"markdown","c97de698":"markdown","dd0c00c1":"markdown","a998a4f1":"markdown","c2eab5d9":"markdown","63dab911":"markdown","571cd650":"markdown","04ee4799":"markdown","dfb77bb1":"markdown","c79d5e9f":"markdown","7a5ba0dd":"markdown","17a00628":"markdown","31c7f237":"markdown","c5694c95":"markdown","e74f0a25":"markdown","7118ee66":"markdown","50a3cdf9":"markdown"},"source":{"6795bc03":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n## Setting max displayed rows to 500, in order to display the full output of any command \npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","dc689ce5":"# read the data \ndf = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","7f70330f":"df.head()","b892dc14":"df.describe()","1343b7b4":"df.corr()[\"SalePrice\"].sort_values(ascending = False)","2227acdd":"plt.figure(figsize = (8, 4), dpi = 100)\nsns.scatterplot(data = df, x = \"OverallQual\", y = \"SalePrice\");","b5b8b2f0":"plt.figure(figsize = (8, 4), dpi = 100)\nsns.scatterplot(data = df, x = \"GrLivArea\", y = \"SalePrice\");","437ea08c":"plt.figure(figsize = (8, 4), dpi = 100)\nsns.scatterplot(data = df, x = \"TotalBsmtSF\", y = \"SalePrice\");","f5f57f78":"df[(df[\"SalePrice\"] < 200000) & (df[\"OverallQual\"] > 8)]","09c96b59":"df[(df[\"SalePrice\"] < 200000) & (df[\"OverallQual\"] > 8) & (df[\"GrLivArea\"] > 4000)]","6f6d791f":"drop_index = df[(df[\"SalePrice\"] < 200000) & (df[\"OverallQual\"] > 8) & (df[\"GrLivArea\"] > 4000)].index","6ea6eb96":"df = df.drop(drop_index, axis = 0)","4c369a9f":"plt.figure(figsize = (8, 4), dpi = 100)\nsns.scatterplot(data = df, x = \"GrLivArea\", y = \"SalePrice\");","221a486d":"df.head()","2c2e0967":"df = df.drop(\"Id\", axis = 1)","c4718ac7":"df.info()","dd9ff036":"## lets create a functions that can be used for any future data\ndef percent_missing_data(df):\n    missing_count = df.isna().sum().sort_values(ascending = False)\n    missing_percent = 100 * df.isna().sum().sort_values(ascending = False) \/ len(df)\n    missing_count = pd.DataFrame(missing_count[missing_count > 0])\n    missing_percent = pd.DataFrame(missing_percent[missing_percent > 0])\n    missing_table = pd.concat([missing_count,missing_percent], axis = 1)\n    missing_table.columns = [\"missing_count\", \"missing_percent\"]\n    \n    return missing_table","43c98b2e":"percent_nan = percent_missing_data(df)\npercent_nan","2724728d":"plt.figure(figsize = (8,4), dpi = 100)\nsns.barplot(x = percent_nan.index, y = percent_nan.values[:,1])\nplt.xticks(rotation = 90)\nplt.show()","2e5642e0":"## lets see the features that has less than on percent missing\nplt.figure(figsize = (8,4), dpi = 100)\nsns.barplot(x = percent_nan.index, y = percent_nan.values[:,1])\nplt.xticks(rotation = 90)\nplt.ylim(0,1)\nplt.show()","370a7039":"percent_nan[percent_nan[\"missing_percent\"] < 1]","5900adfa":"index = percent_nan[percent_nan[\"missing_percent\"] < 1].index\nfor name in index:\n    print(df[df[\"Electrical\"].isnull()][name])","2540a0b5":"df[df[\"GarageType\"].isnull()][\"GarageFinish\"]","00142834":"df = df.dropna(axis = 0, subset = [\"GarageType\"])","d7861217":"percent_nan = percent_missing_data(df)\npercent_nan","27a5ded0":"df[df[\"BsmtFinType1\"].isnull()]","8e430c71":"for col in df.columns:\n    if \"Bsmt\" in col:\n        print(col)","84d930c3":"## basement numeric features ==> fillna 0\nbsmt_num_cols = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']\ndf[bsmt_num_cols] = df[bsmt_num_cols].fillna(0)\n\n## basement string features ==> fillna none\nbsmt_str_cols =  ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\ndf[bsmt_str_cols] = df[bsmt_str_cols].fillna('None')","2e11f35e":"# now if you check again, you will find no nulls\ndf[df[\"BsmtFinSF1\"].isnull()]","44fa765d":"percent_nan = percent_missing_data(df)\npercent_nan","79395fde":"df[df[\"Electrical\"].isnull()]","fc2ddc7e":"# You have the choice of filling it with the mode or dropping it, I will drop it\ndf = df.dropna(axis = 0, subset = [\"Electrical\"])\n","24cb4f84":"percent_nan = percent_missing_data(df)\npercent_nan","88d8e137":"df[[\"MasVnrArea\"]] = df[[\"MasVnrArea\"]].fillna(0)\ndf[[\"MasVnrType\"]] = df[[\"MasVnrType\"]].fillna(\"None\")","2e884049":"percent_nan = percent_missing_data(df)\npercent_nan","f159cb6d":"plt.figure(figsize = (8,4), dpi = 100)\nsns.barplot(x = percent_nan.index, y = percent_nan.values[:,1])\nplt.xticks(rotation = 90)\nplt.show()","19f021af":"df = df.drop([\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\"], axis = 1)","7303a0fc":"percent_nan = percent_missing_data(df)\npercent_nan","6b226fff":"plt.figure(figsize = (8,4), dpi = 100)\nsns.barplot(x = percent_nan.index, y = percent_nan.values[:,1])\nplt.xticks(rotation = 90)\nplt.show()","229619f8":"df[\"FireplaceQu\"].value_counts()","f180f5e9":"df[\"FireplaceQu\"] = df[\"FireplaceQu\"].fillna(\"None\")","91588eaa":"percent_nan = percent_missing_data(df)\npercent_nan","b061a6c2":"df[\"LotFrontage\"].value_counts()","4c9d4a0f":"plt.figure(figsize = (8, 4), dpi = 100)\nsns.boxplot(x = \"Neighborhood\", y = \"LotFrontage\", data = df)\nplt.xticks(rotation = 90)\nplt.show()","d1e7d850":"df.groupby(\"Neighborhood\")[\"LotFrontage\"].mean()","845ad287":"df[\"LotFrontage\"] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda value: value.fillna(value.mean()))","13c4e1b9":"percent_nan = percent_missing_data(df)\npercent_nan","2c5cfdf3":"df[\"MSSubClass\"].dtypes","550c6b5f":"df[\"MSSubClass\"] = df[\"MSSubClass\"].apply(str)","315550b4":"df[\"MSSubClass\"].dtypes","13f8e026":"#Select all Object Features \ndf.select_dtypes(include = \"object\")","589a926d":"df_object = df.select_dtypes(include = \"object\")\ndf_numeric = df.select_dtypes(exclude = \"object\")","f3daf3e6":"df_object_dummies = pd.get_dummies(df_object, drop_first = True)\ndf_object_dummies","c2c9361e":"df_final = pd.concat([df_numeric, df_object_dummies], axis = 1)\ndf_final.head()","99b74647":"print(df_final.shape)","a2f4cb9d":"corr = abs(df_final.corr()[\"SalePrice\"]).sort_values(ascending = False)\nlarge_corr = corr[corr > 0.3]\n\nplt.figure(figsize = (10, 4), dpi = 100)\nsns.barplot(x = large_corr.index, y = large_corr.values)\nplt.xticks(rotation = 90)\nplt.show()","b619d576":"# Split the data for X and y\nX = df_final.drop(\"SalePrice\", axis = 1)\ny = df_final[\"SalePrice\"]","cc9789ac":"# Train test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)","a51ee03f":"# scaling the X data \nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train) # only fit to training data to aviod data leakage\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","a66f85fa":"# Create the Ridge model\nfrom sklearn.linear_model import Ridge\nridge1 = Ridge(alpha = 100)\nridge1.fit(X_train, y_train)\n\n# testing the model\nfrom sklearn.metrics import mean_absolute_error\ny_predict = ridge1.predict(X_test)\nmean_absolute_error(y_test, y_predict)","e948e50a":"# first split\nfrom sklearn.model_selection import train_test_split\nX_train, X_other, y_train, y_other = train_test_split(X, y, test_size=0.3, random_state=101)\n\n# second split: 50% of 30% = 15% of all data \nX_eval, X_test, y_eval, y_test = train_test_split(X_other, y_other, test_size=0.5, random_state=101)","24fbbd37":"# scaling the X data \nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train) # only fit to training data to aviod data leakage\n\nX_train = scaler.transform(X_train)\nX_eval = scaler.transform(X_eval)\nX_test = scaler.transform(X_test)","da0079d6":"# Create the Ridge model\nfrom sklearn.linear_model import Ridge\nridge1 = Ridge(alpha = 100)\nridge1.fit(X_train, y_train)\n\n# testing the model\nfrom sklearn.metrics import mean_absolute_error\ny_predict = ridge1.predict(X_eval)\nmean_absolute_error(y_eval, y_predict)","c17cb293":"# Create the Ridge model\nalpha_list = []\nmse_list = []\nfor alpha_val in np.arange(0.01, 200):\n    from sklearn.linear_model import Ridge\n    ridge1 = Ridge(alpha = alpha_val)\n    ridge1.fit(X_train, y_train)\n    alpha_list.append(alpha_val)\n    \n    # testing the model\n    from sklearn.metrics import mean_absolute_error\n    y_predict = ridge1.predict(X_eval)\n    mse = mean_absolute_error(y_eval, y_predict)\n    mse_list.append(mse)","7b12414d":"alpha_list = pd.DataFrame(alpha_list)\nmse_list = pd.DataFrame(mse_list)\nalpha_mse = pd.concat([alpha_list, mse_list], axis = 1)\nalpha_mse.columns = [\"alpha_list\", \"mse_list\"]","bbf0603a":"alpha_mse[alpha_mse[\"mse_list\"] == alpha_mse[\"mse_list\"].min()]","2fc609d9":"# Create the Ridge model\nfrom sklearn.linear_model import Ridge\nridge3 = Ridge(alpha = 81.01)\nridge3.fit(X_train, y_train)\n\n# testing the model\nfrom sklearn.metrics import mean_absolute_error\ny_predict = ridge3.predict(X_eval)\nprint(mean_absolute_error(y_eval, y_predict))\n\ny_final_test_predict = ridge3.predict(X_test)\nprint(mean_absolute_error(y_test, y_final_test_predict))","07da25c0":"# Create the Ridge model\nfrom sklearn.linear_model import Lasso\nls = Lasso(alpha = 100)\nls.fit(X_train, y_train)\n\n# testing the model\nfrom sklearn.metrics import mean_absolute_error\ny_predict = ls.predict(X_eval)\nprint(mean_absolute_error(y_eval, y_predict))\n\ny_final_test_predict = ls.predict(X_test)\nprint(mean_absolute_error(y_test, y_final_test_predict))","22237f85":"# Create the Ridge model\nalpha_list = []\nmse_list = []\nfor alpha_val in np.arange(0.01, 200):\n    from sklearn.linear_model import Lasso\n    ls = Lasso(alpha = alpha_val)\n    ls.fit(X_train, y_train)\n    alpha_list.append(alpha_val)\n    \n    # testing the model\n    from sklearn.metrics import mean_absolute_error\n    y_predict = ls.predict(X_eval)\n    mse = mean_absolute_error(y_eval, y_predict)\n    mse_list.append(mse)","d26b98cc":"alpha_list = pd.DataFrame(alpha_list)\nmse_list = pd.DataFrame(mse_list)\nalpha_mse = pd.concat([alpha_list, mse_list], axis = 1)\nalpha_mse.columns = [\"alpha_list\", \"mse_list\"]","158ea2b4":"alpha_mse[alpha_mse[\"mse_list\"] == alpha_mse[\"mse_list\"].min()]","a062d5a7":"# Create the optimal Ridge model\nfrom sklearn.linear_model import Lasso\nls = Lasso(alpha = 199.01)\nls.fit(X_train, y_train)\n\n# testing the model\nfrom sklearn.metrics import mean_absolute_error\ny_predict = ls.predict(X_eval)\nprint(mean_absolute_error(y_eval, y_predict))\n\ny_final_test_predict = ls.predict(X_test)\nprint(mean_absolute_error(y_test, y_final_test_predict))","99ebc6eb":"from sklearn.linear_model import ElasticNetCV\nelastic_model = ElasticNetCV(l1_ratio= np.linspace(0.01, 1, 100),tol=0.01)\nelastic_model.fit(X_train,y_train)","be7408be":"elastic_model.l1_ratio_","dd152c3e":"# testing the model\nfrom sklearn.metrics import mean_absolute_error\ny_predict = elastic_model.predict(X_eval)\nprint(mean_absolute_error(y_eval, y_predict))\n\ny_final_test_predict = elastic_model.predict(X_test)\nprint(mean_absolute_error(y_test, y_final_test_predict))","6e913a65":"#Import the poly conerter \nfrom sklearn.preprocessing import PolynomialFeatures\npolynomial_converter = PolynomialFeatures(degree=2,include_bias=False)\n\n#convert X data \npoly_features_train = polynomial_converter.fit_transform(X_train)\npoly_features_eval = polynomial_converter.fit_transform(X_eval)\npoly_features_test = polynomial_converter.fit_transform(X_test)","c76dd9f4":"poly_features_train.shape","f8a69a0b":"#import elastic net \nfrom sklearn.linear_model import ElasticNetCV\nelastic_model = ElasticNetCV(l1_ratio= 1,tol=0.01)\nelastic_model.fit(poly_features_train,y_train)","b8498a0f":"# testing the model\nfrom sklearn.metrics import mean_absolute_error\ny_predict = elastic_model.predict(poly_features_eval)\nprint(mean_absolute_error(y_eval, y_predict))\n\ny_final_test_predict = elastic_model.predict(poly_features_test)\nprint(mean_absolute_error(y_test, y_final_test_predict))","3d7ad03d":"Id is just an identifier, it has no numeric value for the model. Set it as index, or drop it. Dropping it will not make any problems, because we have the default identifier (0, 1, 2, 3, ... ) ","afbe4816":"Both \"Mas Vnr Area\" and \"Mas Vnr Type\" have less than 1 percent of null values. How to deal with them? \n\nGoing back to data description, we found that there is a category for none: It does not have \"Mas Vnr\". We can assume that those missing values are also none but they are mistakenly filled with Nan.","40dd314b":"### Polynomial Regression ","545cc2b5":"To achieve the intended result, we will use pandas transform method. I calls group by and fill in missing vsalues based on it. ","6cf5f755":"#### Train | Validation | Test Split Procedure \n\nThis is often also called a \"hold-out\" set, since you should not adjust parameters based on the final test set, but instead use it *only* for reporting final expected performance.\n\n0. Clean and adjust data as necessary for X and y\n1. Split Data in Train\/Validation\/Test for both X and y\n2. Fit\/Train Scaler on Training X Data\n3. Scale X Eval Data\n4. Create Model\n5. Fit\/Train Model on X Train Data\n6. Evaluate Model on X Evaluation Data (by creating predictions and comparing to Y_eval)\n7. Adjust Parameters as Necessary and repeat steps 5 and 6\n8. Get final metrics on Test set (not allowed to go back and adjust after this!)","28ac11cf":"As we can see there are some points with very high quality (10\/10) but very low price. Lets explore other highly correlated features with Sale Price","c819ba6a":"### 3. Model Building and evaluation ","e7f44bae":"Note that we will not calculate coeff_ for each single feature. Using regularization, we will be able to drop non-important features lets now cocatinate the two data frames","2b1ed34f":"lets now look at these rows, there might be houses with missing values across all features","9ad646b3":"|| | Data without outlier |  | Data with outlier | \n|--||--||--|\n|**Data**| |1,2,3,3,4,5,4 |  |1,2,3,3,4,5,**400** | \n|**Mean**| |3.142 | |**59.714** |  \n|**Median**| |3|  |3|\n|**Standard Deviation**| |1.345185| |**150.057**|","0e9188c8":"### 1. Checking for outliers\nThe following example shows why outliers are very dangerous. They significantly affect the mean and the standard deviation and thus affecting the estimators of the model.","a1d66720":"**Yeah! Congratulations! we did it. Nothing is missing any more!**\n \nLets now move to encoding options. Essentially we will use one hot encoding with variables of the type \"Object\". However, There is one varaible that seems Numeric where in fact it is categorical. It is \"MS SubClass\".\n\nIf we go back to data description we will find that:\n\nMSSubClass: Identifies the type of dwelling involved in the sale.\n\n        20\t1-STORY 1946 & NEWER ALL STYLES\n        30\t1-STORY 1945 & OLDER\n        40\t1-STORY W\/FINISHED ATTIC ALL AGES\n        45\t1-1\/2 STORY - UNFINISHED ALL AGES\n        50\t1-1\/2 STORY FINISHED ALL AGES\n        60\t2-STORY 1946 & NEWER\n        70\t2-STORY 1945 & OLDER\n        75\t2-1\/2 STORY ALL AGES\n        80\tSPLIT OR MULTI-LEVEL\n        85\tSPLIT FOYER\n        90\tDUPLEX - ALL STYLES AND AGES\n       120\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n       150\t1-1\/2 STORY PUD - ALL AGES\n       160\t2-STORY PUD - 1946 & NEWER\n       180\tPUD - MULTILEVEL - INCL SPLIT LEV\/FOYER\n       190\t2 FAMILY CONVERSION - ALL STYLES AND AGES\n\nThese numbers has no ordinal meaning. So we should covert the variable from integer to text.","9fd30434":"Since it is a categorical variable we can fill missing data with \"None\"","0eae1e43":"Now we are left with just two columns. You have to be carefull and do a lot of thinking because you can not just drop the rows nor the feature columns. Not enough to drop the feature but not too little to drop the rows.","49e72c54":"Overall Quality is the most important feature for our model. But there is a problem. It is most likely to be generated by human judgment, therefore model deployment will be dependent on the existence of that human who judge the quality of the house and feed it to the model. \n\nLets now proceed to model building and evaluation.","9c2b341c":"In order to visually see outliers, we need a box plot or a scatter plot. \nTherefore, lets see the most correlated features with sale price to plot them a gainst each others.","c97de698":"In principle we should go through each feature and decide whether we will keep it, fill it or drop it. When we speak about dropping we can drop columns or rows.\n\nFor example Pool QC values are missing for 99.6 percent of houses. This might be due to:\n1. These houses have no pools, and instead of nan it should have been zero.\n2. These houses have pools, but the data is actually missing.\n\nWe should go back to the description file and try to understand it better. But now, lets deal with columns with very few missing values.","dd0c00c1":"It seems that all features related Basement have very high number of missing values. If we go back to data description you will find that Nan actually means that the house do not has a basement. It is not missing, it just has one. Therefore, it does make sense to replace nan values with a string saying that the house has no Basement. This will work for Basement string columns, as for Basement numeric columns we will replace them with zero.","a998a4f1":"Some of the above features have more than 99 percent missing data, dropping these features can be the best strategy to opt for.","c2eab5d9":"As we can see each category is unique enough to make the assumption that we can impute the LotFrontage based on Neighborhood categories. ","63dab911":"The data set describes the sale of individual residential property in Ames, Iowa\nfrom 2006 to 2010. The data set contains 2930 observations and a large number of explanatory\nvariables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous) involved in assessing home\nvalues.\n\nIn this note book we will explore the Ames housing data set. We will focus on:\n1. Removing outliers \n2. Dealing with missing data\n3. Building and assessing the model","571cd650":"### Elastic Net Model","04ee4799":"#### Lets now repeat one of the scatter plots that we had before","dfb77bb1":"#### What to do with the rest?\nThe rest of the features have more than 1% missing data. We need to carefully look at each one and decide how to deal with them. For sure, dropping rows is not a possible strategy any more. so we need to figure out something else. We have two options:\n1. Fill in missing values\n2. Drop thr feature column","c79d5e9f":"The points that indicate very high price and also very high living area (at the top right corner) are not outliers. They make sense as they are follwing a trend, therefore they will not hurt our model.\n\nOn the other hand The 3 points at the right-lower corner indicate very high living area but very low price. They are very likely to be outliers because they are not following the general trend.\n\n\n\n#### Lets now check those points closely","7a5ba0dd":"It is tricky, it is numeric. I can not longer go back to the description and fill it with a convenient text. \nWe will use the Neighborhood feature calculate the missing feature.\n\nNeighborhood: Physical locations within Ames city limits\n\nLotFrontage: Linear feet of street connected to property\n\nWe will operate under the assumption that the Lot Frontage is related to what neighborhood a house is in.","17a00628":"The original elastic net model is the best one. ","31c7f237":"Electrical still has 1 missing value, lets look at it closely and decide","c5694c95":"### 2. Dealing with missing data","e74f0a25":"#### Train | Test Split Procedure \n\n1. Split Data in Train\/Test for both X and y\n2. Fit\/Train Scaler on Training X Data\n3. Scale X Test Data\n4. Create Model\n5. Fit\/Train Model on X Train Data\n6. Evaluate Model on X Test Data (by creating predictions and comparing to Y_test)\n7. Adjust Parameters as Necessary and repeat steps 5 and 6","7118ee66":"### Lasso Regression ","50a3cdf9":"Disadvantages of classic train test split:\n1. Getting the right parameter is quite tedious\n2. It is not the most fair evaluation, because we adjusted the parameters to have better performance on that specific test data.\n\nTherfore its useful to hold some data aside. The model has never been adjusted to this data before, therfore it reflects the true evaluation matrix."}}