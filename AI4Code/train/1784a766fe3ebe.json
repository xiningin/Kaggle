{"cell_type":{"64a533df":"code","3eca8930":"code","d0e35c08":"code","e2de5b7c":"code","33c98c31":"code","1acd3d90":"code","4c2b4970":"code","7f3e9c0a":"code","16d2a54e":"code","caf07847":"code","1e8dcb64":"code","bc19cbcc":"code","edd200a6":"code","55fb961d":"code","c9b14bab":"markdown","3c13599f":"markdown","868d2d1d":"markdown","ddc53e5d":"markdown","a169e5e9":"markdown","f5e4ce6a":"markdown","098db5a8":"markdown","eaf263a0":"markdown","c2d32419":"markdown","79a7bb86":"markdown"},"source":{"64a533df":"import matplotlib.pyplot as plt\nfrom pandas import read_csv\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n\nvdf = read_csv('\/kaggle\/input\/voicegender\/voice.csv')\nprint(vdf.shape)\nvdf.info()\n#NO null data, all numeric except label","3eca8930":"print(vdf['label'].unique())\nvdf[\"label\"] = vdf[\"label\"].astype('category')\ny = vdf[\"label\"].cat.codes #save label code as y variabl\n\n#drop label from dataframe\nx = vdf.drop(['label'],axis=1)\nfeatures = x.columns.tolist() #save all the features","d0e35c08":"print(x[features].round(2).describe().transpose())","e2de5b7c":"feature_mean = x.mean()\nfeature_std = x.std()\n#center and scale the data\nx = (x - feature_mean)\/feature_std\nprint(x[features].round(2).describe().transpose())","33c98c31":"from numpy.linalg import matrix_rank\nprint(matrix_rank(x))","1acd3d90":"max_corr = 0.9 #largest acceptable correlation value\ncorr_matrix = x.corr().abs() #get absolute values for correlation\n#work with upper triangular matrix, corr_matrix is symmetric\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nsns.heatmap(upper>max_corr); #check for high collinearity","4c2b4970":"#drop columns\/features\nto_drop = [column for column in upper.columns if any(upper[column] > max_corr)]\nx.drop(to_drop, axis=1, inplace=True)\nprint('Drop features: ', to_drop)\nprint('Rank: ', matrix_rank(x), '\\nShape: ', x.shape)","7f3e9c0a":"#check the new correlation matrix\ncorr_matrix = x.corr().abs();\nsns.heatmap(corr_matrix);","16d2a54e":"sns.countplot(x=y); #equal counts of male and female data\nplt.xticks(np.arange(2), ('Male','Female'));","caf07847":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\n#split into training and testing data\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,shuffle=True)","1e8dcb64":"from sklearn.svm import SVC\n#create classifier objects.\nsvm = SVC(kernel='linear')\n#fit the model\nsvm.fit(x_train,y_train)\n#perform cross validation\nscores = cross_val_score(svm,x,y)#get cross validation score\n#do prediction\ny_pred = svm.predict(x_test)\nprint(\"SVM training accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), 100*scores.std()))\nprint(\"SVM prediction accuracy: %0.2f\" % accuracy_score(y_test, y_pred))\n#check confusion matrix\nconfusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True, cmap=\"Greens\");","bc19cbcc":"from sklearn.linear_model import LogisticRegression\nLR = LogisticRegression(max_iter=200)\nLR.fit(x_train, y_train)\nscores = cross_val_score(LR,x,y)\ny_pred = svm.predict(x_test)\nprint(\"LR training accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), 100*scores.std()))\nprint(\"LR prediction accuracy: %0.2f\" % accuracy_score(y_test, y_pred))\nconfusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True, cmap=\"Greens\");","edd200a6":"from sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier(max_depth=10)\nRF.fit(x_train, y_train)\ny_pred = RF.predict(x_test)\nscores = cross_val_score(RF,x,y)\nprint(\"RF training accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), 100*scores.std()))\nprint(\"RF prediction accuracy: %0.2f\" % accuracy_score(y_test, y_pred))\nconfusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True, cmap=\"Greens\");","55fb961d":"from sklearn.neural_network import MLPClassifier\n\nNN = MLPClassifier(random_state = 100,max_iter=500)\nNN.fit(x_train, y_train);\nscores = cross_val_score(NN,x,y)\ny_pred = NN.predict(x_test)\nprint(\"NN training accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))\nprint(\"NN prediction accuracy: %0.2f\" % accuracy_score(y_test, y_pred))\nconfusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True, cmap=\"Greens\");","c9b14bab":"# **Logistic Regression**","3c13599f":"# Cross-validation and Model training","868d2d1d":"The label containes male and female entries, which we categorize and attach label as follows:\n\n$$ y = \\begin{cases} 0 &\\mbox{if } \\text{label = 'male'}\\\\ \n                     1 & \\mbox{if } \\text{label = 'female'}\n        \\end{cases}.$$\n\nOnce, converted to a numeric label ($y$), we drop them from the dataframe ($x$).","ddc53e5d":"# Is data balanced?\n\nYes, observations have 50\/50 split across male and female labels.","a169e5e9":"# **Random Forest**","f5e4ce6a":"# **Neural Network**","098db5a8":"# **SVM**\n","eaf263a0":"# Is there any linear dependency between columns?\n\n\nNext, we check for linearly dependent columns. We can check the rank of a matrix, if rank < num_features, then we have colinearity. Here, the rank is 17 < 20, therefore we can remove highly correlated columns and hopefully, we can get a full column rank matrix. We check for correlation heat map, where correlation is more than 0.9. The heatmap below shows that features dfrange, meanfun, kurt, Q25, and median have high correlation values and should be dropped from the dataframe. \n\nAs a result, we have 15 features and a full column rank matrix. I found the code to drop linearly dependent columns [here](https:\/\/chrisalbon.com\/machine_learning\/feature_selection\/drop_highly_correlated_features\/).","c2d32419":"# Introduction\n\nCouple of days ago, I worked on my first data set: Titani survivability prediction. As a result, I officially dipped my toes in data science and python. I will continue to dip my toes, and perhaps, my feet with this project. The data in this project is all numeric with one column being binary information. The columns define various voice chararcteristics and the label column indicates whether it is male or female. A quick peak tells me that the data is clean and usable with no null values - perfect for a newbie. \n\nThere are 3168 observations.","79a7bb86":"# Is data in standard format?\n\nNo, data is not centered and scaled - identified by non-zero means. We can do that as follows:"}}