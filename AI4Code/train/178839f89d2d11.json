{"cell_type":{"6a6ec614":"code","a8af4e72":"code","4a2200e4":"code","68dc4a3e":"code","0a097259":"code","15458ac9":"code","43ed1e30":"code","ac2691af":"code","88bf5c15":"code","de80f400":"code","3cc1419d":"code","f3f03f8a":"code","ba1b6e98":"code","862dfcd1":"code","ded62076":"code","5922a94d":"code","f5710d37":"code","933aab67":"code","06e0c71a":"code","7ff9a9d7":"code","3f80e893":"code","b85f2c35":"code","34295739":"code","17b590df":"code","13806dde":"code","60413317":"code","ae19eee4":"code","70dc1f1a":"code","b9979072":"code","9bb4f516":"code","8fef8571":"code","db9da17c":"code","86f87975":"code","8e42b469":"code","926fad97":"code","5b48d8c8":"code","ed4f6791":"code","c7c7a8b9":"code","e159e4da":"code","7086b113":"code","c6e7345a":"code","e8c37e53":"markdown","878d6a69":"markdown","877f0b41":"markdown","2f25e9f7":"markdown","569c6f29":"markdown","c0dd8d8f":"markdown","cd0edff3":"markdown","bc75e8e6":"markdown","96d9e607":"markdown","05ca68c3":"markdown","32ef37cd":"markdown","285a9199":"markdown","dd6a298a":"markdown","e21df98e":"markdown","14d4622f":"markdown","53353ad0":"markdown","8e24b7bb":"markdown","5d7fbccc":"markdown","7674323d":"markdown","c07b0364":"markdown","9919f7da":"markdown","bbc96a8c":"markdown","551fb083":"markdown"},"source":{"6a6ec614":"# Import modules just as you would in a regular Python script\nimport numpy as np\nimport matplotlib.pyplot as plt\n# This will allow you to display plots inside the notebook.\n#%matplotlib inline","a8af4e72":"# assign values \na = 3","4a2200e4":"# The value of the last object in the cell will be displayed\nb = 2\na","68dc4a3e":"a + b","0a097259":"plt.hist(np.random.normal(size=1000))\nplt.show()","15458ac9":"# get documentation of Python object with ?\n# get source code with ?? (if implementation is in C (or other) then equal to ?)\nplt.hist?\nplt.hist??","43ed1e30":"import pandas as pd\n# Don't worry about this statement. This just suppresses warnings later on.\npd.options.mode.chained_assignment = None","ac2691af":"# Use ! to execute a shell command from the notebook\n!ls \/kaggle\/input\/kit-mathsee-ml-2021","88bf5c15":"# We can just peek at the data from the command line\n!head \/kaggle\/input\/kit-mathsee-ml-2021\/pp_train.csv","de80f400":"df_train = pd.read_csv('\/kaggle\/input\/kit-mathsee-ml-2021\/pp_train.csv', index_col=0)","3cc1419d":"# Before we play around with the data, let's save a copy\ndf_train_raw = df_train.copy()\n# But watch out for RAM usage for larger datasets!","f3f03f8a":"# Show the first five rows. .tail() for the last five\ndf_train.head()","ba1b6e98":"# How many rows\nlen(df_train)","862dfcd1":"# What are the columns?\ndf_train.columns","ded62076":"df_test = pd.read_csv('\/kaggle\/input\/kit-mathsee-ml-2021\/pp_test.csv', index_col=0)","5922a94d":"df_train.describe()","f5710d37":"# Display the data type of each column\ndf_train.dtypes","933aab67":"df_train.station.nunique()","06e0c71a":"df_train['t2m_obs'].head()","7ff9a9d7":"df_train.t2m_obs.hist(range=(-30, 40), bins=100, label='obs')\ndf_train.t2m_fc_mean.hist(range=(-30, 40), bins=100, alpha=0.5, label='fc')\nplt.legend();","3f80e893":"import seaborn as sns\n# Only take every 1000th value\nsns.regplot(data=df_train.iloc[::1000], x='t2m_obs', y='t2m_fc_mean');","b85f2c35":"df_train.isna().sum()","34295739":"df_test.isna().sum()","17b590df":"df_train = df_train.dropna(subset=['t2m_obs'])","13806dde":"# Replace missing soil moisture values with mean value\ndf_train.loc[:, 'sm_fc_mean'].replace(np.nan, df_train['sm_fc_mean'].mean(), inplace=True)\n# Same for test dataset, using the training values\ndf_test.loc[:, 'sm_fc_mean'].replace(np.nan, df_train['sm_fc_mean'].mean(), inplace=True)","60413317":"# Split features and targets\n# At the moment we only want continuous variables in the data set\n# so we will throw away the station ID, along with the time\nX_train = df_train.drop(['t2m_obs', 'time', 'station'], axis=1)\ny_train = df_train['t2m_obs']\n\nstations = df_train['station']","ae19eee4":"X_train = (X_train - X_train.mean()) \/ X_train.std()","70dc1f1a":"X_train.head()","b9979072":"def preproc(df_in, means=None, stds=None, drop_vars=['time', 'station']):\n    df = df_in.copy()\n    if 't2m_obs' in df.columns: df.dropna(subset=['t2m_obs'], inplace=True)\n    df['sm_fc_mean'].replace(np.nan, df['sm_fc_mean'].mean(), inplace=True)\n    \n    y = df.pop('t2m_obs') if 't2m_obs' in df.columns else None\n    X = df.drop(drop_vars, 1)\n    \n    if means is None: means = X.mean()\n    if stds  is None: stds  = X.std()\n    \n    X = (X - means) \/ stds\n    return X, y, means, stds","9bb4f516":"X_train, y_train, means, stds = preproc(df_train)","8fef8571":"split_date = '2015-01-01'","db9da17c":"X_train, y_train, means, stds = preproc(df_train[df_train.time < split_date])","86f87975":"X_valid, y_valid, _, _ = preproc(df_train[df_train.time >= split_date], means, stds)","8e42b469":"X_test, _, _, _ = preproc(df_test, means, stds)","926fad97":"X_train.shape, X_valid.shape, X_test.shape","5b48d8c8":"# For a simpler dataset for linear regression with just 1 predictor, we could alternatively use:\n\n# X_train = df_train[df_train.time < split_date][['t2m_fc_mean']]\n# y_train = df_train[df_train.time < split_date]['t2m_obs']\n# X_valid = df_train[df_train.time >= split_date][['t2m_fc_mean']]\n# y_valid = df_train[df_train.time >= split_date]['t2m_obs']","ed4f6791":"def create_sub(preds, fn=None):\n    df =  pd.DataFrame({'Id': range(len(preds)), 'Prediction': preds})\n    return df","c7c7a8b9":"df_sub = create_sub([df_train.t2m_fc_mean.mean()]*len(df_test))","e159e4da":"df_sub.head()","7086b113":"# create a csv file for the submission\ndf_sub.to_csv('submission.csv', index=False)","c6e7345a":"X_train.to_csv('X_train.csv')\ny_train.to_csv('y_train.csv')\nX_valid.to_csv('X_valid.csv')\ny_valid.to_csv('y_valid.csv')\nX_test.to_csv('X_test.csv')","e8c37e53":"### Clean the dataset\nNext we need to check whether there are missing values and what to do with them.","878d6a69":"Pandas Series objects have some build in plotting functionality with is based on Matplotlib.","877f0b41":"Here we can see that all columns are real numbers except `time` which is a time string and `station` which is an integer and represents the station ID. We can seehow many unique stations we have.\n\nEach column is a Pandas Series","2f25e9f7":"### Split data into features (inputs) and targets (outputs)\n\nTo train our algorithm we need to split the variables we want to give the algorithm to make a prediction and the variable it should predict, in our case `t2m_obs`. Furthermore we will throw away the variables which we are not using at this time: `time` and `station`. These variables might be useful, but are not simple continuous variables and require some more thought.","569c6f29":"What are all these variables? --> https:\/\/doi.org\/10.1175\/MWR-D-18-0187.1 --> Table 1\n\nfc indicates that these are NWP forecasts (specifically 2 day forecasts from the ECMWF ensemble). mean indicates that these values are the ensemble mean.\n","c0dd8d8f":"You do not have to work from top to bottom. You can always edit cells above.\n\nBut pay attention: This means that the notebook might throw an error when executed after restarting the kernel.","cd0edff3":"## Introduction to Jupyter notebooks\n### Basic usage\nTo start Jupyter notebook, simply type 'jupyter notebook' in the command line. If you do this locally (e. g. on your laptop) this should automatically open up a browser window with the dashboard.\n\nOn the dashboard you can see files in your currrent directory. (Note that you can click on any file and edit it.) Jupyter notebook files end in '.ipynb'. To create a new notebook, click on 'New' and select a Python version. Now a new tab will apear with your empty notebook. \n\n### Cells\n\nJupyter notebooks have cells such as this one. There are different types of cells. The two you will use most frequently are code cells and Markdown cells. This here is a Markdown cell, which allows you to write formatted text using Markdown. For a quick Markdown introduction, check out: https:\/\/github.com\/adam-p\/markdown-here\/wiki\/Markdown-Cheatsheet\n\nNote that in markdown you can also type Latex:\n\n$x=\\frac{a}{b}$\n\nTo switch the cell type, either use the toolbar above or press `y` for code and `m` for markdown in command mode, which we will get to now.\n\n### Modes\n\nTo navigate Jupyter notebooks there are two modes: Edit mode which you can enter by pressing `Enter` and command mode, which you can enter by pressing `esc`. In edit mode you can type text into the cell. This mode is indicated by a green cell bordering. In command mode you can edit the notebook as a whole but not type into individual cells. It is indicated by a grey cell border with a blue left margin. Moreover, in command mode, the keyboard is mapped to a set of shortcuts. If you get confused about all the keys just press `h` in command mode, which will bring up a list of all keyboard shortcuts. Of course, you can also use the mouse and the toolbar.   \n\n### Executing a cell\n\nTo execute a cell, press `Shift + Enter`. This will run the code or render the text. If you would like to insert a cell below, type `Alt + Enter`.\n\nNow let's start by actually writing some code.","bc75e8e6":"### Explore data","96d9e607":"### Preprocessing in one function","05ca68c3":"The training and testing datasets have the same variables except for t2m_obs, which is only available for the training dataset.\n\nWe also want to check what datatype each column has.","32ef37cd":"Finally, we want to save the pre-processed data so that we can reuse it in later notebooks.","285a9199":"The first dataset we will use in this workshop is tabular. Tabular data are what you would use in Excel. To work with tabular data in Python, we will use Pandas. Pandas is very widely used which means that you will find an answer to any question you can think of online. Here is a quick overview of basic functionalities: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/basics.html\n\nTabular data files are often saved as `.csv` files which we can easily read into a Pandas `DataFrame`.\n\nThere are two datasets: a training and a test datset. First, let's ignore the test set and open the training set.","dd6a298a":"To submit this file, click \"Save Version\" and chose \"Save and Run All\". This will rerun the entire notebook and save the output.\n\nOnce done, click the back arrow on the top left (this will close the notebook though) or navigate to the notebook in another tab. After clicking on the notebook scroll down until you see the Outputs, select the right file and click submit.","e21df98e":"Two columns have missing values:\n\n 1. `t2m_obs`: That is our target. If we don't have that we can't learn. So we should remove these rows.\n 2. `sm_fc_mean`: Quite a lot of soil moisture values are missing. We can't just remove those because there are also missing values in the test set. Plus we would throw away quite a lot of training data. Here we will fill in the missing values.\n\n","14d4622f":"### Normalize input data\n\nMany machine learning algorithms require all the features to have the same order of magnitude for reasons we will see later. The simplest way to achieve this is to subtract the mean and divide by the standard deviation.","53353ad0":"\n## Our first Kaggle submission\n\nLet's just always predict the mean.","8e24b7bb":"Another convenient way to plot tabular data is Seaborn: https:\/\/seaborn.pydata.org\/","5d7fbccc":"### Get function docs\n\nNext, let's create a basic plot and also note a super-handy functionality of the notebook: When using a function and pressing `Shift + Tab` when the cursor is inside the function parenthesis, it will bring up the list of arguments.\n\nSimilarly, executing `?function_name` will bring up the doc string in a separate window. `??function_name` will bring up the source code.","7674323d":"### Your turn\n\nNow comes the most important part in learning ML: doing ML!\n\nYour first task is to reproduce what I have done in this notebook. So open up a new, empty notebook and go through the basic steps\n\n1. Opening up the data\n2. Preprocess\/clean the data and create train\/valid\/test datasets (X\/y)\n3. Submit a first prediction to Kaggle\n\nTry to do as much as possible from memory and look at this notebook only when necessary. It is also not necessary to do everything exactly the same way.\n\nHere are some challenges for you!\n\n1. We will use the mean squared error (MSE) as a metric for our forecast skill. What is the MSE of the raw forecast?\n2. How much can you improve the MSE by simply correcting the bias? Submit your bias-corrected values to kaggle.\n3. Does the bias depend on the time of year?\n4. Is there a geographical pattern?","c07b0364":"Let's load this dataset into a Pandas DataFrame.\n\nThis part differs between the Notebook on Kaggle and the Jupyter Notebook that can be run locally on your computer. Here, we can directly import the data from Kaggle.","9919f7da":"### Split into train\/valid\/test\u00b6\n\nWe have been given two datasets: train and test. `df_train` contains all the data that we are allowed to use to train our models. `df_test` contains only the features for the samples that we actually want to predict. So why do we need a third validation dataset?\n\nIn the real world the testing data would usually be in the future, so we cannot actually use it to verify how good our model is. We also shouldn't use the training data to evaluate our model since we could literally just build a lookup table and get a perfect score. So we want a third validation dataset to check model performance.\n\nIt is up to us which data from df_train to take for validation. Let's assess two options:\n\n1. Randomly pick samples\n2. Pick the last year 2015 from df_train (2007-2015)\n\nWhich one would be a better validation dataset?\n\nMore info: https:\/\/www.fast.ai\/2017\/11\/13\/validation-sets\/\n\nTo get a validation skill that fairly represents operational conditions, we will also split by time.\n","bbc96a8c":"It's important to use the same means and stds to normalize each dataset.","551fb083":"## Exploring and manipulating data with Pandas plus some basic plotting - Post-processing NWP forecasts\n\nOur first dataset will be a postprocessing dataset. Our aim is to correct biases of NWP temperature forecasts. The forecasts are ensemble means of the ECMWF TIGGE archive. The corresponding observations are taken at DWD surface stations. \n\nInformation about variable names, methods and models can be found here: https:\/\/arxiv.org\/pdf\/1805.09091.pdf"}}