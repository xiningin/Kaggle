{"cell_type":{"3868b1d7":"code","b851619f":"code","4dfa3dd9":"code","ada90b25":"code","ad102fd3":"code","a9b627c4":"code","1cd83fc9":"code","9f39f831":"code","8b405461":"code","69305f61":"code","d4b40b7d":"code","54e29a1a":"code","f0312afc":"code","56de08f1":"code","c7e3fc3b":"code","6195aff4":"code","d2d59c84":"code","e2d84222":"code","e21bb509":"code","51e937df":"code","2162346b":"code","ebb44a64":"code","a3d96ce3":"code","971169d2":"code","7b92f63e":"code","e76dddd2":"code","b605d349":"code","fcc6efe9":"code","06125b98":"code","fd81d006":"code","3e5f2354":"code","e1d877cd":"code","fc0119dc":"code","0248e31d":"code","f2b63fd1":"code","69287011":"code","6fa595af":"code","b9a92c39":"code","8b14a82a":"code","cc86b2a1":"code","3e2d720c":"code","e386ca10":"code","4f292c12":"code","7351a1c7":"code","647c26a6":"code","73b4a295":"code","4c8f9818":"code","dbf07154":"code","59f86c75":"code","e0e1faa1":"code","4863fe29":"code","a2bab128":"code","4544836f":"code","9ce0b419":"code","a5d98784":"code","2112f210":"code","e542aef6":"code","386e6301":"markdown","5a347e61":"markdown","c5906e61":"markdown","0f781788":"markdown","1de41d07":"markdown","4cd8eed4":"markdown","d804455d":"markdown","89965588":"markdown","b82785c7":"markdown","d366ec76":"markdown","79e39beb":"markdown","e2c935eb":"markdown","98947438":"markdown","94ad06f2":"markdown","1a22c949":"markdown","8686db63":"markdown","08dd0d98":"markdown","d6bb1e5f":"markdown"},"source":{"3868b1d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load","b851619f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4dfa3dd9":"import gc\nimport os\nimport logging\nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nwarnings.filterwarnings('ignore')","ada90b25":"data_path = \"..\/input\/santander-customer-transaction-prediction\/\"","ad102fd3":"train = pd.read_csv(data_path + 'train.csv')","a9b627c4":"test = pd.read_csv(data_path + 'test.csv')","1cd83fc9":"train.info()","9f39f831":"df_test = test.drop(['ID_code'], axis=1)\ndf_test = df_test.values\n\nunique_samples = []\nunique_count = np.zeros_like(df_test)\nfor feature in tqdm(range(df_test.shape[1])):\n    _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n    unique_count[index_[count_ == 1], feature] += 1\n    \nreal_sample_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\nsynthetic_sample_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n\nprint(len(real_sample_indexes))\nprint(len(synthetic_sample_indexes))","8b405461":"df_test_real = df_test[real_sample_indexes].copy()\n\ngenerator_for_each_synthetic_sample = []\nfor cur_sample_index in tqdm(synthetic_sample_indexes[:20000]):\n    cur_synthetic_sample = df_test[cur_sample_index]\n    potential_generators = df_test_real == cur_synthetic_sample\n    \n    features_mask = np.sum(potential_generators, axis=0) == 1\n    verified_generators_mask = np.any(potential_generators[:, features_mask], axis=1)\n    verified_generators_for_sample = real_sample_indexes[np.argwhere(verified_generators_mask)[:, 0]]\n    generator_for_each_synthetic_sample.append(set(verified_generators_for_sample))","69305f61":"public_LB = generator_for_each_synthetic_sample[0]\nfor x in tqdm(generator_for_each_synthetic_sample):\n    if public_LB.intersection(x):\n        public_LB = public_LB.union(x)\n        \nprivate_LB = generator_for_each_synthetic_sample[1]\nfor x in tqdm(generator_for_each_synthetic_sample):\n    if private_LB.intersection(x):\n        private_LB = private_LB.union(x)\n        \nprint(len(public_LB))\nprint(len(private_LB))","d4b40b7d":"with open(\"public_LB\", \"w\") as public:\n    public.write(str(public_LB))\n    \nwith open(\"private_LB\", \"w\") as private:\n    private.write(str(private_LB))","54e29a1a":"def plot_feature_scatter(df1, df2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(4, 4, figsize=(14, 14))\n    \n    for feature in features:\n        i += 1\n        plt.subplot(4, 4, i)\n        plt.scatter(df1[feature], df2[feature], marker='+')\n        plt.xlabel(feature, fontsize=9)\n    plt.show()","f0312afc":"features = ['var_0', 'var_1','var_2','var_3', 'var_4', 'var_5', 'var_6', 'var_7', \n           'var_8', 'var_9', 'var_10','var_11','var_12', 'var_13', 'var_14', 'var_15', \n           ]\nplot_feature_scatter(train[::20], test[::20], features)","56de08f1":"sns.countplot(train['target'], palette='Set3')\n\n# target\uc758 \ube44\uc728 \uacc4\uc0b0\n# 1. \uac12\uc73c\ub85c \uacc4\uc0b0\n# train.target.value_counts()[1] \/ train.shape[0] * 100\n\n# 2. describe\ud568\uc218 \uc0ac\uc6a9\ntarget_percent = train.target.describe()[1] * 100\nprint(f\"\ud0c0\uac9f \ube44\uc728 {target_percent}\" % target_percent)\n\n# -> \ub370\uc774\ud130 \ube44\uc728\uc774 unbalanced \ud568","c7e3fc3b":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","6195aff4":"t0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]\nfeatures = train.columns.values[2:102]\nplot_feature_distribution(t0, t1, '0', '1', features)","d2d59c84":"# \ud3c9\uade0\uc758 \ud655\ub960\ubc00\ub3c4 \ud568\uc218\nplt.figure(figsize=(16,6))\nfeatures = train.columns.values[2:202]\nplt.title(\"Distribution of mean values per row in the trian and test set\")\nsns.distplot(train[features].mean(axis=1), color=\"green\", kde=True, bins=120, label=\"train\")\nsns.distplot(test[features].mean(axis=1), color=\"blue\", kde=True, bins=120, label=\"test\")\nplt.legend()\nplt.show()","e2d84222":"feat1, feat2 = 'var_81', 'var_139'\n\nfig = plt.subplots(figsize=(15, 5))\n\nplt.subplot(1, 2, 1)\nsns.kdeplot(train[feat1][train['target']==0], shade=True, color='b', label='target = 0')\nsns.kdeplot(train[feat1][train['target']==1], shade=True, color='r', label='target = 0')\nplt.title(feat1)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\nplt.subplot(1, 2, 2)\nsns.kdeplot(train[feat2][train['target']==0], shade=True, color='b', label='target = 0')\nsns.kdeplot(train[feat2][train['target']==1], shade=True, color='r', label='target = 0')\nplt.title(feat2)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')","e21bb509":"# \uc608\uce21 \uc2dc IQR_multiplier, bin_bandwidth_multiplier\ub97c \ubcc0\uc218\ub85c \ud558\uc5ec \uc870\uc808 \uac00\ub2a5\ud558\ub3c4\ub85d PDF\ud568\uc218 \uc791\uc131\n\nfrom sklearn.neighbors import KernelDensity\nfrom operator import itemgetter\n\nfeat = 'var_81'\n\ndef calculate_pdf_difference(feat, df_feature, df_target, IQR_multiplier, bin_bandwidth_multiplier, print_number_bins):\n    #Agreggating feature values in bin format using the Freedman-Diaconis rule\n    IQR = df_feature[feat].quantile([0.75]).values - df_feature[feat].quantile([0.25]).values #Interquartile range (IQR)\n    n = len(df_feature[feat])\n    bin_size = IQR_multiplier*IQR\/n**(1\/3)\n    bin_number = int(np.round((df_feature[feat].max() - df_feature[feat].min())\/bin_size))\n    binvalues = pd.cut(df_feature[feat], bins = bin_number, labels = range(bin_number)).astype('float')\n    \n    if print_number_bins:\n        print('There are {} bins in the feature {}'.format(bin_number, feat))\n\n    #Calculate the PDFs using the df_target\n    pdf_0 = KernelDensity(kernel='gaussian', bandwidth=bin_size*bin_bandwidth_multiplier)\n    pdf_0.fit(np.array(df_target[feat][df_target['target'] == 0]).reshape(-1,1))\n\n    pdf_1 = KernelDensity(kernel='gaussian', bandwidth=bin_size*bin_bandwidth_multiplier)\n    pdf_1.fit(np.array(df_target[feat][df_target['target'] == 1]).reshape(-1,1))\n\n    #Creates an X array with the average feature value for each bin\n    x = np.array(np.arange(min(df_feature[feat]) + bin_size\/2 ,max(df_feature[feat]), bin_size)).reshape(-1,1)\n\n    #gets the pdf values based on the X array\n    log_pdf_0 = np.exp(pdf_0.score_samples(x))\n    log_pdf_1 = np.exp(pdf_1.score_samples(x))\n\n    #creates a dictionary that links the bin number with the PDFs value difference\n    pdf_dict = dict()\n    for i in range(bin_number):\n        pdf_dict[i] = log_pdf_1[i] - log_pdf_0[i] \n\n    #gets the PDF difference for each row of the dataset based on its equivalent bin.\n    bin_pdf_values = np.array(itemgetter(*list(binvalues))(pdf_dict))\n\n    return bin_pdf_values, x, log_pdf_0, log_pdf_1\n","51e937df":"full = pd.concat([train, test], sort = False)","2162346b":"\nfeat1, feat2 = 'var_81', 'var_139'\n\nfig = plt.subplots(figsize=(15, 5))\n\n#plot pdf feat 1\nbin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat1, df_feature = full, df_target = train, IQR_multiplier = 2, bin_bandwidth_multiplier = 1.5, print_number_bins = True)\n\nplt.subplot(1, 2, 1)\n\nsns.kdeplot(train[feat1][train['target'] == 0], shade=False, color=\"b\", label = 'target = 0')\nsns.kdeplot(train[feat1][train['target'] == 1], shade=False, color=\"r\", label = 'target = 1')\nplt.plot(x, log_pdf_0)\nplt.plot(x, log_pdf_1) \nplt.title(feat1)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\n#plot pdf feat 2\nbin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat2, df_feature = full, df_target = train, IQR_multiplier = 2, bin_bandwidth_multiplier = 1.5, print_number_bins = True)\n\nplt.subplot(1, 2, 2)\nsns.kdeplot(train[feat2][train['target'] == 0], shade=False, color=\"b\", label = 'target = 0')\nsns.kdeplot(train[feat2][train['target'] == 1], shade=False, color=\"r\", label = 'target = 1')\nplt.plot(x, log_pdf_0)\nplt.plot(x, log_pdf_1) \nplt.title(feat2)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\nplt.show()","ebb44a64":"var_81_0 = train['var_81'][train['target'] == 0].sort_values().reset_index(drop=True)\nvar_81_1 = train['var_81'][train['target'] == 1].sort_values().reset_index(drop=True)\n\nsns.displot(var_81_0, color=\"b\", label = 'target = 0')\nsns.displot(var_81_1, color=\"r\", label = 'target = 1')","a3d96ce3":"print('-----------------------------------------------')\nIQR_multiplier = 3\nbin_bandwidth_multiplier = 3\nprint('IQR_multiplier', IQR_multiplier)\nprint('bin_bandwidth_multiplier', bin_bandwidth_multiplier)\nprint('-----------------------------------------------')\nfig = plt.subplots(figsize=(15, 5))\n#plot pdf feat 1\nbin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat1, df_feature = full, df_target = train, IQR_multiplier = IQR_multiplier, bin_bandwidth_multiplier = bin_bandwidth_multiplier, print_number_bins = True)\n\nplt.subplot(1, 2, 1)\n\nsns.kdeplot(train[feat1][train['target'] == 0], shade=False, color=\"b\", label = 'target = 0')\nsns.kdeplot(train[feat1][train['target'] == 1], shade=False, color=\"r\", label = 'target = 1')\nplt.plot(x, log_pdf_0)\nplt.plot(x, log_pdf_1) \nplt.title(feat1)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\n#plot pdf feat 2\nbin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat2, df_feature = full, df_target = train, IQR_multiplier = IQR_multiplier, bin_bandwidth_multiplier = bin_bandwidth_multiplier, print_number_bins = True)\n\nplt.subplot(1, 2, 2)\nsns.kdeplot(train[feat2][train['target'] == 0], shade=False, color=\"b\", label = 'target = 0')\nsns.kdeplot(train[feat2][train['target'] == 1], shade=False, color=\"r\", label = 'target = 1')\nplt.plot(x, log_pdf_0)\nplt.plot(x, log_pdf_1) \nplt.title(feat2)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\nplt.show()","971169d2":"train_corr = train.corr()\ntrain_corr","7b92f63e":"train_corr_target = pd.DataFrame(train_corr['target'] > 0.1)\ntrain_corr_target.value_counts()","e76dddd2":"# target\uacfc \uad00\ub828\uc788\ub294 \uc0c1\uc704 10\uac1c\uc758 \ubcc0\uc218\ntrain_corr[train_corr['target'].abs() > 0.1].T.sort_values(['target'], ascending = False).head(10)\n# \uc808\ub300\uac12\uc73c\ub85c \uc21c\uc704 \uc815\ub82c\ud574\uc11c \uc368\uc57c \ud568","b605d349":"# \ubcc0\uc218 \uc11c\ub85c\uac04\uc758 \uc0c1\uad00\uacc4\uc218\ncorrelations = train[features].corr().abs().unstack().sort_values(kind='quicksort').reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\n# correlations.head(10)\ncorrelations.tail(10)","fcc6efe9":"sns.ecdfplot(var_81_0, color=\"b\", label = 'target = 0')\nsns.ecdfplot(var_81_1, color=\"r\", label = 'target = 0')","06125b98":"var_139_0 = train['var_139'][train['target'] == 0]\nvar_139_1 = train['var_139'][train['target'] == 1]\n\nsns.ecdfplot(var_139_0, color=\"b\", label = 'target = 0')\nsns.ecdfplot(var_139_1, color=\"r\", label = 'target = 0')","fd81d006":"# \ud655\ub960\ubc00\ub3c4 \ud568\uc218\uac00 \ube44\uc2b7\ud588\ub358 \ubcc0\uc218\ub294 \ub204\uc801\ubd84\ud3ec\ud568\uc218\ub85c \ud574\ub3c4 \ube44\uc2b7\ud558\uac8c \ub098\uc634\nvar_45_0 = train['var_45'][train['target'] == 0]\nvar_45_1 = train['var_45'][train['target'] == 1]\n\nsns.ecdfplot(var_45_0, color=\"b\", label = 'target = 0')\nsns.ecdfplot(var_45_1, color=\"r\", label = 'target = 0')","3e5f2354":"sns.kdeplot(train['var_45'][train['target']==0], shade=True, color='b', label='target = 0')\nsns.kdeplot(train[\"var_45\"][train['target']==1], shade=True, color='r', label='target = 0')\nplt.title(\"var_45\")\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')","e1d877cd":"var_45_0 = np.log(train['var_45'][train['target'] == 0])\nvar_45_1 = np.log(train['var_45'][train['target'] == 1])\n\nsns.kdeplot(var_45_0, color=\"b\", label = 'target = 0')\nsns.kdeplot(var_45_1, color=\"r\", label = 'target = 0')","fc0119dc":"idx = features = train.columns.values[2:202]\nfor df in [test, train]:\n    df['sum'] = df[idx].sum(axis=1)\n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)\n\n    ","0248e31d":"train[train.columns[202:]].head()","f2b63fd1":"test[test.columns[202:]].head()","69287011":"def plot_new_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(2,4,figsize=(18,8))\n\n    for feature in features:\n        i += 1\n        plt.subplot(2,4,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=11)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","6fa595af":"t0 = train.loc[train['target']==0]\nt1 = train.loc[train['target']==1]\nfeatures = train.columns.values[202:]\nplot_new_feature_distribution(t0, t1, 'target:0', 'target:1',features)","b9a92c39":"train1 = train.copy()\ntrain1.iloc[:, 2:202] = np.log(train1.iloc[:, 2:202])\ntest1 = test.copy()\ntest1.iloc[:, 1:201] = np.log(test1.iloc[:, 1:201])","8b14a82a":"(12-10)\/12","cc86b2a1":"(np.log(12) - np.log(10)) \/ np.log(12)","3e2d720c":"train.head()","e386ca10":"print(train1.isnull().sum())\nprint(test1.isnull().sum())","4f292c12":"train2 = train1.dropna(axis = 1)\ntest2 = test1.dropna(axis = 1)","7351a1c7":"train2 = train2.drop('var_101', axis = 1)\ntest2 = test2.drop('var_114', axis = 1)","647c26a6":"inter = np.intersect1d(train2.columns[2:], test2.columns[1:])\ninter[:]","73b4a295":"train10 = train[train[np.setdiff1d(train.columns, inter)].columns[2:]]\ntest10 = test[test[np.setdiff1d(test.columns, inter)].columns[1:]]\n","4c8f9818":"np.setdiff1d(np.array(train1.columns), inter)","dbf07154":"np.setdiff1d(np.array(test1.columns), inter)","59f86c75":"train = pd.concat([train2, train10], axis = 1)\ntrain","e0e1faa1":"test = pd.concat([test2, test10], axis = 1)\ntest","4863fe29":"idx = features = train.columns.values[2:70]\nfor df in [test, train]:\n    df['sum'] = df[idx].sum(axis=1)\n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)","a2bab128":"train[train.columns[70:]].head()","4544836f":"features = [c for c in train.columns if c not in ['ID_code', 'target']]\ntarget = train['target']","9ce0b419":"# parameter \uc815\uc758\n\nparam = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1\n}","a5d98784":"folds = StratifiedKFold(n_splits=10, shuffle=False, random_state=44000)\noof = np.zeros(len(train))\npredictions = np.zeros(len(train))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 500000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","2112f210":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:150].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()\nplt.savefig('FI.png')","e542aef6":"sub_df = pd.DataFrame({\"ID_code\":test[\"ID_code\"].values})\nsub_df[\"target\"] = predictions\nsub_df.to_csv(\".\/submission2.csv\", index=False)\n","386e6301":"### 4-2). lightGBM\uc0ac\uc6a9","5a347e61":"### 2-5). \uc0c1\uad00\uad00\uacc4 \ubd84\uc11d\n- target\uacfc \uad00\ub828\uc788\ub294, \ud639\uc778 \uc11c\ub85c \uad00\uacc4\uc788\ub294 \ubcc0\uc218\uac00 \uc788\ub294\uc9c0 \ucc3e\uc544\ubcf4\uace0 \uc608\uce21 \uc54c\uace0\ub9ac\uc998\uc5d0 \uc801\uc6a9 \uac00\ub2a5\ud55c\uc9c0 \uace0\ubbfc\ud574\ubcf4\uae30\n- \ud655\uc778 \uacb0\uacfc 0.1 \uc774\uc0c1\uc774 \uc5c6\uc74c. \uc11c\ub85c \uad00\uacc4\uc788\ub294 \ubcc0\uc218\ub294 \uc5c6\uc73c\uba70, \ubcc0\uc218\uac04\uc5d0 \uad00\uacc4\uc131\uc744 \uc774\uc6a9\ud558\uae30\ub294 \uc5b4\ub824\uc6c0\n- corr\ud568\uc218\ub294 \ubcc0\uc218\uac04\uc758 pearson\uc0c1\uad00\uacc4\uc218\ub97c \uacc4\uc0b0","c5906e61":"### 2-2). Feature \ud0d0\uc0c9","0f781788":"- target\uc774 0\uc77c \ub54c, 1\uc77c \ub54c\uc758 \ucc28\uc774\uac00 \uac00\uc7a5 \ud070 81\ubc88, 139\ubc88\uc744 \uae30\uc900\uc73c\ub85c \uc2dc\uac01\ud654","1de41d07":"### 2-4). overfitting \ubc29\uc9c0\n- \ud655\ub960\ubc00\ub3c4\ud568\uc218(PDF) histogram\uc744 \uc0ac\uc6a9 \ud560 \uacbd\uc6b0, overfitting\uc744 \uc5b4\ub5bb\uac8c \ubc29\uc9c0\ud560 \uc218 \uc788\uc744\uae4c?","4cd8eed4":"- log\uc801\uc6a9 \ud6c4 \uc9d1\uacc4\uac12 \ub2e4\uc2dc \uacc4\uc0b0\ud558\uc5ec \ucd94\uac00","d804455d":"# 1. \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30","89965588":"1. \ub370\uc774\ud130 columns : 200\uac1c\n2. train, test \uac01 200,000\uac1c\n2. \ub370\uc774\ud130 \uc0c1\uad00\uad00\uacc4 \ubd84\uc11d, \ucc28\uc6d0\uac10\uc18c\uae30\ubc95 \ub4f1 \ud65c\uc6a9\ud574\uc57c \ud560 \ub4ef..\n4. id_code\ub85c target\uc608\uce21\n5. target\uc740 0, 1\ub85c \uad6c\ubd84(mean \ucc0d\uc73c\uba74 0.100490\ub098\uc634. 10%\uc815\ub3c4?)\n- binary classification\n","b82785c7":"# 4. \uc608\uce21 \ubaa8\ub378 \ub9cc\ub4e4\uae30\n### 4-1). \ubaa8\ub4e0 \uac12\uc5d0 log\ub97c \uc801\uc6a9\n- log\uac00 \uc801\uc6a9\ub418\uc9c0 \uc54a\ub294 column\uc740 \uc6d0\ubcf8\uac12\uc744 \uc0ac\uc6a9(train, test\ub3d9\uc77c\ud558\uac8c \uc801\uc6a9 \uac00\ub2a5\ud55c column\ub9cc \uc801\uc6a9)","d366ec76":"### 2-3). \ucc28\uc6d0\ucd95\uc18c\uae30\ubc95\n- \ud655\ub960\ubc00\ub3c4\ud568\uc218(PDF)\ub97c \uc774\uc6a9\ud558\uc5ec \ub370\uc774\ud130\ub97c histogram\ud615\ud0dc\ub85c \ud45c\ud604\n- 200\uac1c\uc758 \ubcc0\uc218, 200000\uac1c\uc758 \ub0b4\uc5ed\uc740 \ub370\uc774\ud130 \uc591\uc774 \ud06c\uae30\ub54c\ubb38\uc5d0, \ub370\uc774\ud130\ub97c \ucd95\uc18c\ud560 \ud544\uc694\uac00 \uc788\uc74c.\n","79e39beb":"- \uc544\ub798\uc640 \uac19\uc774 IQR_multiplier, bin_bandwidth_multiplier\ub97c \uc870\uc808\ud558\uc5ec overfitting\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc74c","e2c935eb":"![image.png](attachment:9f3fc549-b391-4029-84d7-06a4f7191948.png)","98947438":"### 3-2). \ub85c\uadf8\uac12 \uc0ac\uc6a9\n- \ub370\uc774\ud130\uc5d0 \ub85c\uadf8\ub97c \ucde8\ud558\uba74, \ub370\uc774\ud130 scale\uc774 \uc791\uc544\uc9c0\uace0, \uc0c1\ub300\uc801\uc778 \ucc28\uc774\uac00 \ucee4\uc9c0\uae30 \ub54c\ubb38\uc5d0, target 0\uc77c\ub54c\uc640 1\uc77c\ub54c\uc758 \ucc28\uc774\ub97c \ub354 \ud06c\uac8c \uad6c\ubd84\ud560 \uc218 \uc788\uc744 \uac83\uc73c\ub85c \ubcf4\uc784\n- \ud655\uc778 \uacb0\uacfc \ucc28\uc774\uac00 \ub354 \ud06c\uac8c \uad6c\ubd84\ub418\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\uc74c","94ad06f2":"# 3. \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc704\ud55c \uac1c\uc120\uc548\n### 3-1). \ub204\uc801\ubd84\ud3ec\ud568\uc218 \uc0ac\uc6a9\n - \uc120\ud589\uc5f0\uad6c\uc790\ub4e4\uc740 \ud655\ub960\ubc00\ub3c4\ud568\uc218(PDF)\ub97c \uc0ac\uc6a9\ud558\uc5ec 1, 0\uc77c\ub54c\ub97c \uad6c\ubd84\ud568\n - \ub204\uc801\ubd84\ud3ec\ud568\uc218(CDF)\ub97c \uc368\ubcf4\uc790! -> \ub204\uc801\ubd84\ud3ec\ud568\uc218\ub294 \ud655\ub960\ubc00\ub3c4\ud568\uc218\ubcf4\ub2e4 \uad6c\ubd84\uc774 \uc5b4\ub824\uc6c0","1a22c949":"# 3-3). \uc9d1\uacc4\uac12\uc744 \ucd94\uac00 \ubcc0\uc218\ub85c \uc0ac\uc6a9\n- \uac01 \ub370\uc774\ud130\ub4e4\uc758 \ud569, \ucd5c\ub300, \ucd5c\uc18c \ub4f1 \uc9d1\uacc4\uac12\uc744 \ucd94\uac00 \ubcc0\uc218\ub85c \uc0ac\uc6a9","8686db63":"# 2. EDA\n### 2-1). Fake\ub370\uc774\ud130 \ucc3e\uae30\n- test\ub370\uc774\ud130 200000\uac1c \uc911 100000\uac1c\ub294 \uac00\uacf5\ub41c \ub370\uc774\ud130\uc784\uc744 \uc54c \uc218 \uc788\uc74c\n- unique\ud568\uc218\ub97c \uc368\uc11c \ub370\uc774\ud130\uc758 \uc885\ub958\uc640 \uac2f\uc218\ub97c \ube44\uad50\ud558\uc5ec \ub3d9\uc77c\ud55c \ud568\uc218\ub97c \uc0c9\uc778","08dd0d98":"- \uc704\uc758 \uc608\uce21 \uadf8\ub798\ud504\uc5d0\uc11c target\uc774 1\uc77c \ub54c IQR, bin_bandwidth_multiplier\ub97c \uc870\uc808\ud574\uc8fc\uc9c0 \uc54a\uc73c\uba74, overfitting\uc774 \ubc1c\uc0dd\n- \uadf8 \uc774\uc720\ub294 target\uc774 1\uc77c \ub54c\uc758 \ub370\uc774\ud130 \uc218\uac00 \uc801\uae30 \ub54c\ubb38\uc5d0 \ud2b9\uc815 \ubd80\ubd84\uc5d0\uc11c \uac11\uc790\uae30 \ud280\ub294 \uac12\uc774 \uc788\ub294 \uacbd\uc6b0 \uc774\ub97c \uc608\uce21\ud558\uae30 \uc704\ud568\n- \uc544\ub798\uc758 \uadf8\ub798\ud504\ub85c \ud655\uc778 \uac00\ub2a5","d6bb1e5f":"# 5. \uacb0\uacfc\n- \ub370\uc774\ud130\ub97c \uac00\uacf5\ud558\uc9c0 \uc54a\uace0 \uc608\uce21\ud588\uc744 \uacbd\uc6b0, 0.89971\ub2ec\uc131(Public score\uae30\uc900)\n- log\uac00 \uc801\uc6a9\uac00\ub2a5\ud55c \ub370\uc774\ud130(70 columns)\ub9cc \uc0ac\uc6a9\ud560 \uacbd\uc6b0 0.78610\ub2ec\uc131\n- log \uc801\uc6a9(70 columns) + \uc6d0\ubcf8 \ub370\uc774\ud130(130 columns) \uc0ac\uc6a9\ud560 \uacbd\uc6b0 0.89979\ub2ec\uc131(\uc6d0\ubcf8\ub370\uc774\ud130 \uc0ac\uc6a9\ubcf4\ub2e4 0.0008 \ud5a5\uc0c1)\n\n-> \uac00\ub2a5\ud55c \uc6d0\ubcf8\ub370\uc774\ud130\uc758 \uac2f\uc218\ub97c \uc720\uc9c0\ud558\uba70 \uc608\uce21\ud558\ub294 \uac83\uc774 \uc131\ub2a5 \ud5a5\uc0c1\uc774 \ub428"}}