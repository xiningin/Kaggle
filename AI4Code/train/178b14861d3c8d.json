{"cell_type":{"34d14bf9":"code","d15b47b7":"code","6e6f6572":"code","0d825a0f":"code","956d9c42":"code","659d35dc":"code","9efa91a8":"code","23dbb721":"code","651b3236":"code","04156437":"code","ffe74550":"code","a3fa9527":"code","ae187a88":"code","7d6b9fa4":"code","c7c70d31":"code","1116ed20":"code","2a55ac13":"code","c929a233":"code","133df882":"code","c5b9d0e8":"code","3989d591":"code","f1065845":"code","287245a1":"code","56c62210":"code","1206346a":"code","fca93d84":"code","370a9d66":"markdown","d56ddda1":"markdown","ffd12e19":"markdown","70e10f67":"markdown","178b93fa":"markdown","6893a9dd":"markdown","a37df90d":"markdown","e11cd6c2":"markdown","e56809ea":"markdown","082ddbc6":"markdown","21b7e4ce":"markdown","f76e8bb7":"markdown","92ebc040":"markdown","d3bb2bb3":"markdown","993ed61e":"markdown","e0807899":"markdown","dbd89beb":"markdown","e6799404":"markdown","c4be1a79":"markdown","4dc910cf":"markdown","f2bd51f2":"markdown","d8f8b2a2":"markdown","75735d0b":"markdown","1b01d4ce":"markdown","6ef608c2":"markdown","d6aa3cd8":"markdown","751450e0":"markdown","d9809161":"markdown"},"source":{"34d14bf9":"import warnings\nwarnings.filterwarnings(\"ignore\")","d15b47b7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","6e6f6572":"df = pd.read_csv('\/kaggle\/input\/mushroom-classification\/mushrooms.csv')\ndf","0d825a0f":"df.info()","956d9c42":"plt.figure(figsize = (6,6))\nplt.pie(df['class'].value_counts(), startangle = 90, autopct = '%.1f', labels = ['Edible', 'Poisonous'], shadow = True)\nplt.show()","659d35dc":"X = df.iloc[:, 1:].values\ny = df.iloc[:, 0].values","9efa91a8":"df.notnull().all()","23dbb721":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nX = df.iloc[:, 1:].apply(le.fit_transform).values\n","651b3236":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny = np.array(le.fit_transform(y))\ny.reshape(len(y), 1)","04156437":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","ffe74550":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","a3fa9527":"accuracies = dict()","ae187a88":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(random_state = 0)\nlr.fit(X_train, y_train)\n\n#Make Prediction\ny_pred = lr.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\naccuracies['Logistic Regression'] = accuracy_score(y_test, y_pred)\nprint('Accuracy is: ' + str(accuracy_score(y_test, y_pred)))","7d6b9fa4":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(lr, X_test, y_test,display_labels=['Edible', 'Posionous'],cmap= plt.cm.PuRd, normalize= 'true')\nplt.title(\"Normalized Confusion Matrix of Mushroom Dataset\")\nplt.show()","c7c70d31":"from sklearn.svm import SVC\n\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, y_train)\n\n#Make Prediction\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\naccuracies['Kernel SVM'] = accuracy_score(y_test, y_pred)\nprint('Accuracy is: ' + str(accuracy_score(y_test, y_pred)))","1116ed20":"from sklearn.metrics import plot_confusion_matrix\n\ndisp = plot_confusion_matrix(classifier, X_test, y_test, display_labels= ['Edible', 'Posionous'], cmap = plt.cm.PuRd, normalize= 'true')\nplt.title(\"Normalized Confusion Matrix of Mushroom Dataset\")\nplt.show()","2a55ac13":"from sklearn.naive_bayes import GaussianNB\n\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n#Make Prediction\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\naccuracies['Naive Bayes'] = accuracy_score(y_test, y_pred)\nprint('Accuracy is: ' + str(accuracy_score(y_test, y_pred)))","c929a233":"from sklearn.metrics import plot_confusion_matrix\n\ndisp = plot_confusion_matrix(classifier, X_test, y_test, display_labels= ['Edible', 'Posionous'], cmap = plt.cm.PuRd, normalize= 'true')\nplt.title(\"Normalized Confusion Matrix of Mushroom Dataset\")\nplt.show()","133df882":"from sklearn.tree import DecisionTreeClassifier\n\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state= 0)\nclassifier.fit(X_train, y_train)\n\n#Make Prediction\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\naccuracies['Decision Tree Classification'] = accuracy_score(y_test, y_pred)\nprint('Accuracy is: ' + str(accuracy_score(y_test, y_pred)))","c5b9d0e8":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(classifier, X_test, y_test, display_labels= ['Edible', 'Poisonous'], cmap = plt.cm.PuRd, normalize= 'true')\nplt.title(\"Normalized Confusion Matrix of Mushroom Dataset\")\nplt.show()","3989d591":"from sklearn.ensemble import RandomForestClassifier\n\nclassifier = RandomForestClassifier(n_estimators= 2, random_state= 0)\nclassifier.fit(X_train, y_train)\n\n#Make Prediction\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\naccuracies['Random Tree Classification'] = accuracy_score(y_test, y_pred)\nprint('Accuracy is: ' + str(accuracy_score(y_test, y_pred)))","f1065845":"from sklearn.metrics import plot_confusion_matrix\ndisp = plot_confusion_matrix(classifier, X_test, y_test, display_labels= ['Edible', 'Poisonous'], cmap = plt.cm.PuRd, normalize= 'true')\nplt.title(\"Normalized Confusion Matrix of Mushroom Dataset\")\nplt.show()","287245a1":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)\n\n#Make Prediction\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\naccuracies['K-Nearest Neighbors'] = accuracy_score(y_test, y_pred)\nprint('Accuracy is: ' + str(accuracy_score(y_test, y_pred)))","56c62210":"from sklearn.metrics import plot_confusion_matrix\ndisp = plot_confusion_matrix(classifier, X_test, y_test, display_labels= ['Edible', 'Poisonous'], cmap = plt.cm.PuRd, normalize= 'true')\nplt.title(\"Normalized Confusion Matrix of Mushroom Dataset\")\nplt.show()","1206346a":"accuracy_df  = pd.DataFrame(list(accuracies.items()),columns = ['Model Name', 'Accuracy Score']) \naccuracy_df","fca93d84":"f, ax = plt.subplots(figsize=(8, 6))\nsns.set_color_codes(\"pastel\")\nsns.barplot(y=\"Model Name\", x=\"Accuracy Score\", data = accuracy_df, color=\"pink\")\nplt.show()","370a9d66":"All columns are object. So we need to encode them.","d56ddda1":"### Confusion Matrix","ffd12e19":"<a id = 5> <\/a>\n# Train Test Split","70e10f67":"<a id = 4> <\/a>\n# Encoding Categorical Data\nI am going to use Label Encoding for categorical data.","178b93fa":"<a id = 14> <\/a>\n# Results","6893a9dd":"<a id = 2> <\/a>\n# Load the Data","a37df90d":"### Plot Confusion Matrix","e11cd6c2":"As you can see above, there is no null value in our dataset.","e56809ea":"<a id = 12> <\/a>\n# Random Tree Classification\n\nA random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. [Click to see detailed information.](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)","082ddbc6":"### Encode Independent Variable","21b7e4ce":"### Confusion Matrix","f76e8bb7":"<a id = 10> <\/a>\n# Naive Bayes\n\nNaive Bayes methods are a set of supervised learning algorithms based on applying Bayes\u2019 theorem with the \u201cnaive\u201d assumption of conditional independence between every pair of features given the value of the class variable.  [Click to see detailed information.](https:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html)","92ebc040":"### Confusion Matrix","d3bb2bb3":"# Introduction \nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms in 1981. Each species can be identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one.  \nSo there 2 classes. Poisonous and edible.  \n\nI will use 6 classification models to predict.\n\nI also added explanations to models so that I can learn better. I would be glad if you say my mistakes.\n\n\n## Content:\n\n1. [Import Libraries](#1)\n2. [Load the Data](#2)\n3. [Variable Descriptions](#3)\n4. [Encoding Categorical Data](#4)\n5. [Train Test Split](#5)\n6. [Feature Scaling](#6)\n7. [Classification Models](#7)\n  * [Logistic Regression Model](#8)\n  * [Kernel SVM Model](#9)\n  * [Naive Bayes Model](#10)\n  * [Decision Tree Classification Model](#11)\n  * [Random Tree Classification Model](#12)\n  * [K-Nearest Neighbors(K-NN) Model](#13)\n8. [Results](#14)\n","993ed61e":"### Encode Dependent Variable","e0807899":"<a id = 8> <\/a>\n## Logistic Regression","dbd89beb":"<a id = 7> <\/a>\n# Classification Models\n\nIn these models, I will use formula below. This formula exists as accuracy_score method in sklearn.metric.\n![Accuracy](https:\/\/lawtomated.com\/wp-content\/uploads\/2019\/10\/Accuracy_1.png)","e6799404":"<a id = 13> <\/a>\n# K-Nearest Neighbors(K-NN)\n\nIn k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\nIn k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.\n[Click here to see detailed information.](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm)  \n\nHere I used Euclidean distance. You can find its formula below:\n![output-onlinepngtools%20%282%29.png](attachment:output-onlinepngtools%20%282%29.png)\n\nmetric = 'minkowski', p = 2 means Euclidean distance.","c4be1a79":"### Confusion Matrix","4dc910cf":"<a id = 6> <\/a>\n# Feature Scaling\nI will use standard scaler.\n## [Standard Scaling](https:\/\/machinelearningmastery.com\/standardscaler-and-minmaxscaler-transforms-in-python\/)\nStandardization scales each input variable separately by subtracting the mean (called centering) and dividing by the standard deviation to shift the distribution to have a mean of zero and a standard deviation of one.  \n\nDifferences in the scales across input variables may increase the difficulty of the problem being modeled. An example of this is that large input values (e.g. a spread of hundreds or thousands of units) can result in a model that learns large weight values. A model with large weight values is often unstable, meaning that it may suffer from poor performance during learning and sensitivity to input values resulting in higher generalization error.","f2bd51f2":"## Check if there is null value.","d8f8b2a2":"<a id = 11> <\/a>\n# Decision Tree Classification","75735d0b":"<a id = 9> <\/a>\n# Kernel SVM\n\nSupport vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. [Click to see detailed information.](https:\/\/scikit-learn.org\/stable\/modules\/svm.html)\n","1b01d4ce":"<a id = 1> <\/a>\n# Import Libraries","6ef608c2":"<a id = 3> <\/a>\n# Variable Descriptions\n\n\nAttribute Information: (classes: edible = e, poisonous = p)\n1. cap-shape: bell = b, conical = c,convex = x, flat = f, knobbed = k, sunken = s\n2. cap-surface: fibrous = f, grooves = g, scaly = y, smooth = s\n3. cap-color: brown = n, buff = b, cinnamon = c, gray = g, green = r, pink = p, purple = u, red = e, white = w, yellow = y\n4. bruises: yes = t, no = f\n5. odor: almond = a, anise = l, creosote = c, fishy = y, foul = f, musty = m, none = n, pungent = p, spicy = s\n6. gill-attachment: attached = a, descending = d, free = f, notched = n\n7. gill-spacing: close = c, crowded = w, distant = d\n8. gill-size: broad = b, narrow = n\n9. gill-color: black = k, brown = n, buff = b, chocolate = h, gray = g, green = r, orange = o, pink = p, purple = u, red = e, white = w ,yellow = y\n10. stalk-shape: enlarging = e, tapering = t\n11. stalk-rootbulbous = b, club = c, cup = u, equal = e, rhizomorphs = z, rooted = r, missing = ?\n12. stalk-surface-above-ring: fibrous = f, scaly = y, silky = k, smooth = s\n13. stalk-surface-below-ring: fibrous = f, scaly = y, silky = k, smooth = s\n14. stalk-color-above-ring: brown = n, buff = b, cinnamon = c, gray = g, orange = o, pink = p, red = e, white = w, yellow = y \n15. stalk-color-below-ring: brown = n, buff = b, cinnamon = c, gray = g, orange = o, pink = p, red = e, white = w, yellow = y\n16. veil-type: partial = p, universal = u\n17. veil-color: brown = n, orange = o, white = w, yellow = y \n18. ring-number: none = n, one = o, two = t\n19. ring-type: cobwebby = c, evanescent = e, flaring = f, large = l, none = n, pendant = p, sheathing = s, zone = z\n20. spore-print-color: black = k, brown = n, buff = b, chocolate = h, green = r, orange = o,purple = u, white = w, yellow = y\n21. population: abundant = a, clustered = c, numerous = n, scattered = s, several = v, solitary = y\n22. habitat: grasses = g, leaves = l, meadows = m, paths = p, urban = u, waste = w, woods = d\n\n\n\n","d6aa3cd8":"### Confusion Matrix","751450e0":"You can see percentages of edible and poisonous mushrooms.","d9809161":"## Set dependent and independent variable"}}