{"cell_type":{"7479a01c":"code","d28cfef3":"code","f3f4d031":"code","7b25cc66":"code","d85df0b5":"code","bbfb1909":"code","da88e0c8":"code","2944b989":"code","5fc40b54":"code","4fe975cc":"code","d287f5c5":"code","8df2b139":"code","6076c6c6":"code","0594e6b3":"code","052c7740":"code","d0da67b5":"code","bedaa27e":"code","9f88bf8d":"code","38e1e6ef":"code","7f75c458":"code","30fbd5b8":"code","8ca2004e":"code","2c818f8d":"code","0eef5309":"code","32e526f1":"code","0833eee9":"code","31400d4e":"code","64aeee6a":"code","b22cbaff":"code","1eb6535e":"code","ce2c9480":"code","10d95166":"code","8436ccc0":"code","03f10dc4":"code","783f6ab6":"code","0603833e":"code","2439fda8":"code","14711dc8":"code","a52226bb":"code","463d0c86":"code","4b4f373a":"code","cdf572ca":"code","1ba0736c":"code","3bf31a6a":"code","cbb0d2c2":"code","50eed13f":"code","593b1375":"code","04d93cc3":"code","4fa1c7f4":"code","467e4138":"code","14be00fc":"code","023b9d77":"code","22ef1639":"code","d00611a4":"code","d9efca0e":"code","a61888b6":"code","75d142a0":"code","50981dfe":"code","0f6485da":"code","6fa69f49":"code","027fe020":"code","4c9dfd72":"code","68b54a59":"code","7e406e7c":"code","ef39af1f":"code","6fbba9a4":"code","eb442f19":"code","127cd9c4":"code","95154b94":"code","b5d701ea":"code","87acbef7":"code","596b53f2":"code","d197b97a":"code","7d286902":"code","3be2996b":"code","a6397604":"code","8fbfe628":"code","12cd27c2":"code","95263b07":"markdown","824223d8":"markdown","c9e29832":"markdown","c2eb82b6":"markdown","bc09fd0c":"markdown","309b1849":"markdown","02d182a2":"markdown","7522e832":"markdown","8d920e23":"markdown","8790ba9a":"markdown","648d0490":"markdown","21c4d851":"markdown"},"source":{"7479a01c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n","d28cfef3":"train = pd.read_csv(\"train_titanic.csv\")\ntest = pd.read_csv(\"test_titanic.csv\")","f3f4d031":"len(train), len(test)","7b25cc66":"# we need to clean the data, convert strings to numbers, and manage null values\n\ntrain.isna().sum()","d85df0b5":"train[\"Age\"].fillna(train[\"Age\"].mean(),inplace = True)\n# test[\"Age\"].fillna(test[\"Age\"].mean(), inplace = True)","bbfb1909":"test.isna().sum(), train.isna().sum()","da88e0c8":"train.Embarked.fillna(value =\"S\", inplace =True)\n# test.Embarked.fillna(value=\"S\", inplace = True)","2944b989":"test.isna().sum()","5fc40b54":"train.Fare.fillna(train[\"Fare\"].mean(), inplace = True)\n# test.Fare.fillna(test[\"Fare\"].mean(), inplace = True)","4fe975cc":"train.isna().sum()","d287f5c5":"train[\"Cabin_No\"] = train[\"Cabin\"].notnull().astype(int)\n# test[\"Cabin_No\"] = test[\"Cabin\"].notnull().astype(int)\n\n","8df2b139":"train.drop([\"Name\",\"Ticket\", \"Cabin\"], axis = 1, inplace = True)\n# test.drop([\"Name\",\"Ticket\", \"Cabin\"], axis = 1, inplace = True)","6076c6c6":"train[\"Fare\"] = train['Fare'].round(2)\n# test[\"Fare\"] = test[\"Fare\"].round(2)\ntrain.head()","0594e6b3":"sns.barplot(x= train[\"Sex\"], y = train[\"Survived\"]);","052c7740":"sns.violinplot(x= \"Survived\", y= \"Sex\", data = train)","d0da67b5":"sns.barplot(x= \"Pclass\", y = \"Survived\", data = train)","bedaa27e":"sns.boxplot(x= train[\"Survived\"], y =train[\"Age\"], hue= train[\"Sex\"]);","9f88bf8d":"sns.barplot(x = \"SibSp\", y = \"Survived\", data = train)","38e1e6ef":"sns.barplot(x = \"SibSp\", y = \"Survived\", data = train)","7f75c458":"sns.lmplot(x = \"Survived\", y = \"Age\", hue = \"Sex\", data = train)","30fbd5b8":"train.head()","8ca2004e":"sns.lmplot(x = \"Survived\", y = \"Age\", hue = \"Sex\", col = \"Pclass\", data = train);","2c818f8d":"sns.lmplot(x = \"Survived\", y = \"Age\", hue = \"Sex\", col = \"Embarked\", data = train);","0eef5309":"sns.pairplot(train, hue= \"Survived\")","32e526f1":"sns.jointplot(x = \"Age\", y = \"Survived\", data = train, kind = \"reg\")","0833eee9":"sns.jointplot(x = \"Fare\", y = \"Survived\", data = train, kind = \"reg\")","31400d4e":"sns.jointplot(x = \"Pclass\", y = \"Survived\", data = train, kind = \"reg\")","64aeee6a":"def age_buckets(x): \n    if x < 1: return 'Infants' \n    elif x < 11: return 'Children' \n    elif x < 17: return 'Teen' \n    elif x < 30: return 'Young' \n    elif x < 65: return 'Adult' \n    elif x >=65: return 'Elderly' \n    \n\ntrain[\"Age_Group\"] =train.Age.apply(age_buckets)\n# test[\"Age_Group\"] =test.Age.apply(age_buckets)\n\n","b22cbaff":"from sklearn.preprocessing import LabelEncoder\nnumber = LabelEncoder()\n\ntrain[\"Sex\"] = number.fit_transform(train[\"Sex\"].astype(str))\ntrain[\"Embarked\"] = number.fit_transform(train[\"Embarked\"].astype(str))\ntrain[\"Age_Group\"] = number.fit_transform(train[\"Age_Group\"].astype(str))\n\n# test[\"Sex\"] = number.fit_transform(test[\"Sex\"].astype(str))\n# test[\"Embarked\"] = number.fit_transform(test[\"Embarked\"].astype(str))\n# test[\"Age_Group\"] = number.fit_transform(test[\"Age_Group\"].astype(str))\n","1eb6535e":"train = train.drop([\"Age\"], axis = 1)","ce2c9480":"train","10d95166":"train.info()","8436ccc0":"plt.figure(figsize=(15,10))\nsns.heatmap(train.corr(), annot=True, cmap=\"coolwarm\")","03f10dc4":"train.head()","783f6ab6":"x = train.drop([\"PassengerId\", \"Survived\"], axis = 1)\ny= train[\"Survived\"]","0603833e":"len(x), len(y)","2439fda8":"X_train, X_test, y_train, y_test = train_test_split(x,y, test_size = 0.2)\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nlr.score(X_test, y_test)","14711dc8":"rf_classifier = RandomForestClassifier()\nrf_classifier.fit(X_train, y_train)\nrf_classifier.score(X_test, y_test)","a52226bb":"\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)","463d0c86":"Accuracy_scores = {\"Logistic\":lr.score(X_test, y_test), \"KNN\": knn.score(X_test, y_test), \"Random_forest\": rf_classifier.score(X_test, y_test)}\nAccuracy_scores","4b4f373a":"Accuracy_scores=pd.DataFrame(Accuracy_scores, index = [\"Accuracy\"])\n","cdf572ca":"Accuracy_scores.T.plot.bar() ","1ba0736c":"#Logistic regression\n\nlr_random_grid = {\"C\": np.arange(4,48,4), \"solver\": [\"liblinear\"]} \n\nlr_randomsearchCV = RandomizedSearchCV(LogisticRegression(), param_distributions= lr_random_grid, cv=5, n_iter=20, verbose=True )\nlr_randomsearchCV.fit(X_train, y_train);\nlr_randomsearchCV.best_params_\nlr = LogisticRegression(C=lr_randomsearchCV.best_params_[\"C\"], solver=lr_randomsearchCV.best_params_[\"solver\"] )\nlr_tuned = lr.fit(X_train, y_train)\nlr_tuned.score(X_test, y_test)","3bf31a6a":"# KNN\n\nknn_grid = {\"n_neighbors\": list(range(1,50))}\n\nknn_randomsearchCV = RandomizedSearchCV(KNeighborsClassifier(), param_distributions=knn_grid,cv=5,  verbose=True  )\nknn_randomsearchCV.fit(X_train, y_train);\n\nknn_randomsearchCV.best_params_\nknn_tuned = KNeighborsClassifier(n_neighbors=knn_randomsearchCV.best_params_[\"n_neighbors\"])\n\nknn_tuned.fit(X_train, y_train)\nknn_tuned.score(X_test, y_test)","cbb0d2c2":"#Random Forest\n\nrf_grid = {\"n_estimators\": np.arange(10,1000,50), \"max_depth\": [None, 3,5,10], \n            \"min_samples_split\":np.arange(2,20,2), \"min_samples_leaf\": np.arange(1,20,2)}\n\nrf_randomsearchCV = RandomizedSearchCV(RandomForestClassifier(), param_distributions=rf_grid, cv=15, n_iter=2, verbose=True)\nrf_randomsearchCV.fit(X_train, y_train)\n\nrf_randomsearchCV.best_params_\n\nrf_tuned = RandomForestClassifier(max_depth=rf_randomsearchCV.best_params_[\"max_depth\"], \n                                  min_samples_leaf = rf_randomsearchCV.best_params_[\"min_samples_leaf\"],\n                                 min_samples_split = rf_randomsearchCV.best_params_[\"min_samples_split\"],\n                                 n_estimators =rf_randomsearchCV.best_params_[\"n_estimators\"])\n\nrf_tuned.fit(X_train, y_train)\nrf_tuned.score(X_test, y_test)","50eed13f":"Tuned_scores = {\"Logistic\":lr_tuned.score(X_test, y_test), \n                \"Random Forest\": rf_tuned.score(X_test, y_test), \n                \"KNN\": knn_tuned.score(X_test, y_test)}","593b1375":"Tuned_score = pd.DataFrame(Tuned_scores, index = [\"Accuracy Scores\"])\nTuned_score","04d93cc3":"Accuracy_scores.T.plot.bar()","4fa1c7f4":"# logistic regression tunning parameters\n\nnp.random.seed (21)\n\nlr_grid_params = {\"C\": np.logspace(-4,4,30), \"solver\": [\"liblinear\"]} \n\nlr_gridsearchcv = GridSearchCV(LogisticRegression(), param_grid=lr_grid_params, cv=5, verbose= True)\nlr_gridsearchcv.fit(X_train, y_train)\nlr_gridsearchcv.best_params_\n# do model based on best parameters\n\nLR_tuned = LogisticRegression(C= lr_gridsearchcv.best_params_[\"C\"], solver=lr_gridsearchcv.best_params_[\"solver\"])\n\nLR_tuned.fit(X_train, y_train)\nLR_Accuracy = LR_tuned.score(X_test, y_test)\nLR_Accuracy","467e4138":"# random forest tunning parameters\n\nRF_grid = {\"n_estimators\": np.arange(10,50,10), \"max_depth\": [None, 3,5,10], \n            \"min_samples_split\":np.arange(2,20,2), \"min_samples_leaf\": np.arange(1,4,2)}\n\nRF_gridsearch = GridSearchCV(RandomForestClassifier(), param_grid=RF_grid, cv=2, verbose=True)\n\nRF_gridsearch.fit(X_train, y_train)\nRF_gridsearch.best_params_\n\nRF_tuned = RandomForestClassifier(max_depth= RF_gridsearch.best_params_[\"max_depth\"], min_samples_leaf=RF_gridsearch.best_params_[\"min_samples_leaf\"], min_samples_split= RF_gridsearch.best_params_[\"min_samples_split\"], n_estimators=RF_gridsearch.best_params_[\"n_estimators\"])\nRF_tuned\nRF_tuned.fit(X_train, y_train)\nRF_tuned.score(X_test, y_test)","14be00fc":"# KNN tuning parameters\n\nKNN_grid = {\"n_neighbors\": list(range(1,50)), \"leaf_size\": np.arange(2,10,2)}\nKNN_gridsearch = GridSearchCV(KNeighborsClassifier(), param_grid= KNN_grid, cv=5,  verbose=True  )\nKNN_gridsearch.fit(X_train, y_train);\n\nKNN_gridsearch.best_params_\nKNN_tuned = KNeighborsClassifier(n_neighbors=knn_randomsearchCV.best_params_[\"n_neighbors\"], leaf_size=KNN_gridsearch.best_params_[\"leaf_size\"])\n\nKNN_tuned.fit(X_train, y_train)\nKNN_tuned.score(X_test, y_test)","023b9d77":"Tuned_scores_Grid = {\"Logistic\":LR_tuned.score(X_test, y_test), \n                \"Random Forest\": RF_tuned.score(X_test, y_test), \n                \"KNN\": KNN_tuned.score(X_test, y_test)}\n\nTuned_scores_Grid = pd.DataFrame(Tuned_scores_Grid, index=[\"Accuracy Score\"])\nTuned_scores_Grid","22ef1639":"Tuned_scores_Grid.T.plot.bar()","d00611a4":"\nfrom sklearn.model_selection import cross_val_score\n\n#check best param\n\nrf_randomsearchCV.best_params_\n\n#cross validated auracy\nrf_tuned = RandomForestClassifier(max_depth=rf_randomsearchCV.best_params_[\"max_depth\"], \n                                  min_samples_leaf = rf_randomsearchCV.best_params_[\"min_samples_leaf\"],\n                                 min_samples_split = rf_randomsearchCV.best_params_[\"min_samples_split\"],\n                                 n_estimators =rf_randomsearchCV.best_params_[\"n_estimators\"])\n\n\ncv_acc = cross_val_score(rf_tuned, x, y, cv=5, verbose=True, scoring=\"accuracy\")\ncv_acc = np.mean(cv_acc)\ncv_acc","d9efca0e":"#cross validated Precision\n\ncv_precision = cross_val_score(rf_tuned, x, y, cv=5, verbose=True, scoring=\"precision\")\ncv_precision = np.mean(cv_precision)\ncv_precision\n\n","a61888b6":"#cross validated recall\n\ncv_recall = cross_val_score(rf_tuned, x, y, cv=5, verbose=True, scoring=\"recall\")\ncv_recall = np.mean(cv_recall)\ncv_recall\n","75d142a0":"#cross validated f1\n\ncv_f1 = cross_val_score(rf_tuned, x, y, cv=5, verbose=True, scoring=\"f1\")\ncv_f1 = np.mean(cv_f1)\ncv_f1\n\n","50981dfe":"#visualize as metrics\n\nmetrics_compare = pd.DataFrame({\"Cross validated Accuracy\":cv_acc,\n                  \"Cross validated Precision\": cv_precision,\n                  \"Cross validated Recall\": cv_recall,\n                  \"Cross validated F1\": cv_f1}, index =[1])\nmetrics_compare","0f6485da":"metrics_compare.T.plot.bar(title = \"Evaluation of Matrices using cross validated data\")","6fa69f49":" # Attach a text label above each bar displaying its height\n    \nfor rect in rects:\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width()\/2., 1.05*height,\n                '%d' % int(height),\n                ha='center', va='bottom')\n\nautolabel(rects1)\nautolabel(rects2)\n\nplt.show()","027fe020":"X_test","4c9dfd72":"test = pd.read_csv(\"test_titanic.csv\")\n\ntest[\"Age\"].fillna(test[\"Age\"].mean(), inplace = True)\ntest.Embarked.fillna(value=\"S\", inplace = True)\ntest.Fare.fillna(test[\"Fare\"].mean(), inplace = True)\ntest[\"Cabin_No\"] = test[\"Cabin\"].notnull().astype(int)\ntest.drop([\"Name\",\"Ticket\", \"Cabin\"], axis = 1, inplace = True)\ntest[\"Fare\"] = test[\"Fare\"].round(2)\ntest[\"Age\"] = test[\"Age\"].round(2)\n\n\ndef age_buckets(x): \n    if x < 1: return 'Infants' \n    elif x < 11: return 'Children' \n    elif x < 17: return 'Teen' \n    elif x < 30: return 'Young' \n    elif x < 65: return 'Adult' \n    elif x >=65: return 'Elderly' \n    \n\ntest[\"Age_Group\"] =test.Age.apply(age_buckets)\n\ntest = test.drop([\"Age\"], axis =1)\n\n\nnumber = LabelEncoder()\n\ntest[\"Sex\"] = number.fit_transform(test[\"Sex\"].astype(str))\ntest[\"Embarked\"] = number.fit_transform(test[\"Embarked\"].astype(str))\ntest[\"Age_Group\"] = number.fit_transform(test[\"Age_Group\"].astype(str))","68b54a59":"test","7e406e7c":"PassengerId = test[\"PassengerId\"]\ntest.drop([\"PassengerId\"], axis = 1, inplace = True)","ef39af1f":"# Logistic Regression\n\ny_pred_LR = LR_tuned.predict(test)\ny_pred_LR","6fbba9a4":"# Random_Forest\n\ny_pred_RF = RF_tuned.predict(test)\ny_pred_RF","eb442f19":"# KNN\n\ny_pred_KNN = KNN_tuned.predict(test)\ny_pred_KNN","127cd9c4":"Pred_Output = pd.DataFrame({\"Passenger ID\": PassengerId, \n                            \"Logistic Regression\":y_pred_LR, \n                            \"Random_forest\": y_pred_RF, \"KNN\": y_pred_KNN })\nPred_Output","95154b94":"Pred_Output.to_csv(\"Submission_titanic_Ayu.csv\")","b5d701ea":"len(Pred_Output)","87acbef7":"y_pred_rf = rf_tuned.predict(test)\ny_pred_rf","596b53f2":"submission_Ayu = pd.DataFrame({\"PassengerId\": PassengerId, \"Survived\": y_pred_rf})\nsubmission_Ayu","d197b97a":"submission_Ayu.to_csv(\"submssion_Ayu.csv\", index = False)","7d286902":"submission_Ayu.head()","3be2996b":"#Random Forest\n\n\n#with best hyperparameter result: rf_randomsearchCV.best_params_\n\nrf_tuned = RandomForestClassifier(max_depth=rf_randomsearchCV.best_params_[\"max_depth\"], \n                                  min_samples_leaf = rf_randomsearchCV.best_params_[\"min_samples_leaf\"],\n                                 min_samples_split = rf_randomsearchCV.best_params_[\"min_samples_split\"],\n                                 n_estimators =rf_randomsearchCV.best_params_[\"n_estimators\"])\n\nrf_tuned.fit(X_train, y_train)\nrf_tuned.score(X_test, y_test)","a6397604":"rf_tuned.feature_importances_","8fbfe628":"Feature_importance = dict(zip(test.columns, list(rf_tuned.feature_importances_)))\nFeature_importance","12cd27c2":"Feature = pd.DataFrame(Feature_importance, index =[0])\n\nFeature.T.plot.bar()\n\n","95263b07":"## Modelling","824223d8":"# Modelling\n\n\nThree models based on Skealrn macine learning map or cheat sheet\n\n    Logistic Regression\n    K-nearest classification\n    Random classifier","c9e29832":"## Test Data","c2eb82b6":"##  Visualizations","bc09fd0c":"## Tuning hyperparameters","309b1849":"## Feature importance\n\n  which featurs are more important in predicting the label.","02d182a2":"We are following the following approach:\n    1. problem statement\n   \n       * we have a regression problem\n       \n    2. data\n    \n       * data comes from kaggle competition: https:\/\/www.kaggle.com\/c\/titanic\/data\n                       \n    3. evaluation\n         * If we achieve 0.8% of accuracy in predicting whether a passenger has survived or not, we will pursue this project.\n         \n    4. features\n         \n                Variable\tDefinition\tKey\n                survival\tSurvival\t0 = No, 1 = Yes\n                pclass\t    Ticket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n                sex\t        Sex\t\n                Age\t        Age in years\t\n                sibsp\t    # of siblings \/ spouses aboard the Titanic\t\n                parch\t    # of parents \/ children aboard the Titanic\t\n                ticket\t    Ticket number\t\n                fare\t    Passenger fare\t\n                cabin\t    Cabin number\t\n                embarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southamp\n                \n               Variable Notes \n                    pclass: A proxy for socio-economic status (SES) \n                    1st = Upper \n                    2nd = Middle \n                    3rd = Lower\n \n                    age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n \n                    sibsp: The dataset defines family relations in this way... \n                    Sibling = brother, sister, stepbrother, stepsister \n                    Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n \n                    parch: The dataset defines family relations in this way... \n                    Parent = mother, father \n                    Child = daughter, son, stepdaughter, stepson \n                    Some children travelled only with a nanny, therefore parch=0 for them.\n    5. modelling\n    * Random Forest Regressor\n    \n    5. experimentation\n \n ","7522e832":"   ### Random forest hyperparameters \n    n_estimators = number of trees in the foreset\n    max_features = max number of features considered for splitting a node\n    max_depth = max number of levels in each decision tree\n    min_samples_split = min number of data points placed in a node before the node is split\n    min_samples_leaf = min number of data points allowed in a leaf node\n    bootstrap = method for sampling data points (with or without replacement)","8d920e23":"## Grid Search","8790ba9a":"### let's calculate evaluation matrices using cross validation\n","648d0490":"## Prediction","21c4d851":"Althogh I have applied both RandomSeacrhCV and GridSeach to hyperparameter tunning, \nI have chosen Random forest classifier for the competiion (tuned by RandomSearchCV) as it revealed the highest score."}}