{"cell_type":{"ec5d5bf3":"code","e49f3645":"code","7ec6e955":"code","024d14f6":"code","023a8e4f":"code","b7e0c2ce":"code","09f51f5f":"markdown","a0d5f88c":"markdown","eda5df38":"markdown","b9bdd2b1":"markdown","887e521f":"markdown","9fb1b208":"markdown"},"source":{"ec5d5bf3":"from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import roc_auc_score\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.optimizers import Adadelta, Adam, rmsprop\n\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport time\nimport sys\n\nseed = 42 # Set seed for reproducibility purposes\nmetric = 'accuracy' # See other options https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html\nkFoldSplits = 5\n\nnp.random.seed(seed) # Set numpy seed for reproducibility\n\n# Create a toy-dataset using make_classification function from scikit-learn\nX,Y=make_classification(n_samples=10000,\n                        n_features=30,\n                        n_informative=2,\n                        n_redundant=10,\n                        n_classes=2,\n                        random_state=seed)\n\n# Split in train-test-validation datasets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\nX_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=0.25, random_state=seed) # 0.25 x 0.8 = 0.2\n\n# Check on created data\nprint(\"Training features size:   %s x %s\\nTesting features size:    %s x %s\\nValidation features size: %s x %s\\n\" % (X_train.shape[0],X_train.shape[1], \n                                                                                                                     X_test.shape[0],X_test.shape[1], \n                                                                                                                     X_validation.shape[0],X_validation.shape[1]))\n\n# Create a function to print variable name\ndef namestr(obj, namespace = globals()):\n    return [name for name in namespace if namespace[name] is obj]\n\n# Check on class distribution\nfor x in [Y_train, Y_test, Y_validation]:\n    print(namestr(x)[0])\n    counter = Counter(x)\n    for k,v in counter.items():\n        pct = v \/ len(x) * 100\n        print(\"Class: %1.0f, Count: %3.0f, Percentage: %.1f%%\" % (k,v,pct))\n    print(\"\")","e49f3645":"X = X_train\ny = Y_train\nX_val = X_test\ny_val = Y_test","7ec6e955":"units_options = np.arange(32, 1024 + 1, 32, dtype=int)\ndropout_options = np.arange(.20,.75 + 0.01, 0.025, dtype=float)\nbatchsize_options = np.arange(32, 128 + 1, 32, dtype=int)","024d14f6":"space = {'choice': hp.choice('num_layers',\n                            [ {'layers':'two', },\n                              {'layers':'three',\n                                    'units3': hp.choice('units3', units_options), \n                                    'dropout3': hp.choice('dropout3', dropout_options)}\n                            ]),\n\n            'units1': hp.choice('units1', units_options),\n            'units2': hp.choice('units2', units_options),\n\n            'dropout1': hp.choice('dropout1', dropout_options),\n            'dropout2': hp.choice('dropout2', dropout_options),\n\n            'batch_size' : hp.choice('batch_size', batchsize_options),\n\n            'nb_epochs' :  10,\n            'optimizer': hp.choice('optimizer',['adadelta','adam','rmsprop']),\n            'activation': 'relu'\n        }","023a8e4f":"def f_nn(params):   \n\n    model = Sequential()\n    model.add(Dense(units=params['units1'], input_dim = X.shape[1])) \n    model.add(Activation(params['activation']))\n    model.add(Dropout(params['dropout1']))\n\n    model.add(Dense(units=params['units2'], kernel_initializer = \"glorot_uniform\")) \n    model.add(Activation(params['activation']))\n    model.add(Dropout(params['dropout2']))\n\n    if params['choice']['layers']== 'three':\n        model.add(Dense(units=params['choice']['units3'], kernel_initializer = \"glorot_uniform\")) \n        model.add(Activation(params['activation']))\n        model.add(Dropout(params['choice']['dropout3']))    \n\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=params['optimizer'])\n\n    model.fit(X, y, epochs=params['nb_epochs'], batch_size=params['batch_size'], verbose = 0)\n\n    pred_auc = model.predict_proba(X_val, batch_size = 128, verbose = 0)\n    acc = roc_auc_score(y_val, pred_auc)\n    print(\"AUC: %.5f\" % (acc))\n\n    return {'loss': -acc, 'status': STATUS_OK}","b7e0c2ce":"trials = Trials()\nbest = fmin(f_nn, space, algo=tpe.suggest, max_evals=5, trials=trials)\nprint('\\nBest params found:\\n', best)","09f51f5f":"Example from https:\/\/stackoverflow.com\/questions\/43533610\/how-to-use-hyperopt-for-hyperparameter-optimization-of-keras-deep-learning-netwo","a0d5f88c":"### Step 3: Run Hyperopt function","eda5df38":"## Neural Network\n### Step 1: Initialize space or a required range of values","b9bdd2b1":"### Step 2: Define objective function","887e521f":"# Hyperparameter tuning on Neural Network","9fb1b208":"### Step 0: Load required packages and create a toy-dataset"}}