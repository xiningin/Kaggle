{"cell_type":{"327acd5b":"code","ac2fa9ec":"code","a3f5202b":"code","160e7824":"code","fa4c47a6":"code","d17fea5f":"code","763032ce":"code","d69c29e0":"code","d5ae468e":"code","b4ffc7f3":"code","cf006f5c":"code","37f645d2":"code","e86a4e5a":"code","d622468e":"code","2503e12d":"code","60accfba":"code","c4afcd71":"code","6c8a9dfb":"markdown","3aee72ba":"markdown","294fe22f":"markdown","5e26a1a0":"markdown","105b5ca0":"markdown","208ca6de":"markdown","45fdbc27":"markdown","0dfbbe5b":"markdown"},"source":{"327acd5b":"#@title Base Imports\n\nfrom json import load, dump\n\nimport numpy as np\nimport pandas as pd","ac2fa9ec":"#@title Base Utilities\n\nclass AttrDict(dict):\n    \"\"\"Convenience class\"\"\"\n    __getattr__ = dict.__getitem__\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n\n\ndef softmax(x):\n    \"\"\"Numerically stable softmax\"\"\"\n    e_x = np.exp(x - x.max(1)[:, np.newaxis])\n    return e_x \/ e_x.sum(1)[:, np.newaxis]\n\n\ndef mAP(label, prediction):\n    \"\"\"Mean Average Precision at top k = 2\"\"\"\n    hit0 = np.equal(label, prediction[..., 0])\n    hit1 = np.equal(label, prediction[..., 1])\n    return (hit0 + hit1 * .5)[label > -1].mean()\n\n\ndef show_preview(data, run):\n    return run(data) if data is not None else 'No data selected'","a3f5202b":"#@title Loading Utilities { display-mode: \"form\" }\n\nPROJECT_DIR = '.'\nDATA_INFO_DIR = '..\/input' #@param {type:\"string\"}\n\n\ndef avals(values):\n    sorted_values = sorted((i, v) for v, i in values.items())\n    for i, value in enumerate(sorted_values):\n        assert i == value[0]\n        yield value\n\n\ndef load_data(name, path='%s\/%s' % (PROJECT_DIR, DATA_INFO_DIR)):\n    \"\"\"Store all info related to a dataset\"\"\"\n    _dn = (path, name)\n    train = pd.read_csv('%s\/%s_data_info_train_competition.csv' % _dn)\n    submit = pd.read_csv('%s\/%s_data_info_val_competition.csv' % _dn)\n    with open('%s\/%s_profile_train.json' % _dn) as f:\n        profile = load(f)\n\n    attribute_types = list(train.columns[3:])\n    attribute_values = { a: tuple(avals(v)) for a, v in profile.items() }\n\n    labels = { a: train[a].values for a in attribute_types }\n    train_size = len(train.index)\n    submit_size = len(submit.index)\n    return AttrDict(locals())\n\n\ndef get_counts(data):\n    for attr in data.attribute_values:\n        yield attr, data.train[attr].value_counts().drop(-1)","160e7824":"NDSC = AttrDict({ n: load_data(n) for n in ('beauty', 'fashion', 'mobile') })\n\njpg_stripped = NDSC.fashion.train.image_path.str.replace(r'\\.jpg$', '')\nNDSC.fashion.train.image_path = jpg_stripped + '.jpg'\n\nfor d in NDSC.values():\n    d.train.fillna(-1., inplace=True, downcast='infer')\n    d.counts = dict(get_counts(d))","fa4c47a6":"#@title Preview { run: \"auto\", display-mode: \"form\" }\n\npreview_category = \"beauty\" #@param [\"Choose one...\", \"beauty\", \"fashion\", \"mobile\"]\npreview_set = \"train\" #@param [\"train\", \"submit\"]\nshow_preview(NDSC.get(preview_category), lambda d: d[preview_set].head())","d17fea5f":"#@title Default Thresholds\n\nDEFAULT_THRESHOLDS = AttrDict()\nDEFAULT_THRESHOLDS.beauty = {'Brand': 2000} #@param {type:\"raw\"}\nDEFAULT_THRESHOLDS.mobile = {'Phone Model': 500} #@param {type:\"raw\"}\nDEFAULT_THRESHOLDS.fashion = {} #@param {type:\"raw\"}","763032ce":"#@title `FilteredValues`\n\nclass FilteredValues(dict):\n    \"\"\"Filter attribute values that occur below a certain count\"\"\"\n\n    base_default = 10\n\n    def __init__(self, data, thresholds=None):\n        super().__init__(self.get_values(data))\n        self.data = data\n        self.map = dict(self.get_map(data.counts))\n        self.index = dict(self.get_index(data.counts))\n\n        self.default = self.base_default\n        self.thresholds = thresholds or DEFAULT_THRESHOLDS[data.name]\n\n\n    @staticmethod\n    def get_values(data):\n        for attr, avs in data.attribute_values.items():\n                yield attr, tuple(avs[i] for i in data.counts[attr].index)\n\n    @staticmethod\n    def get_index(counts):\n        for attr, count in counts.items():\n            yield attr, count.index.values\n\n    @staticmethod\n    def get_map(counts):\n        for attr, ct in counts.items():\n            index = ct.index.values\n            mapper = np.full((index.max() + 1,), -1)\n            mapper[index] = np.arange(len(index))\n            yield attr, mapper\n\n    def items(self, threshold=None):\n        default = self.default\n\n        if threshold is None:\n            threshold = self.thresholds\n        elif isinstance(threshold, int):\n            default = threshold\n            threshold = {}\n\n        for attribute in self.data.attribute_types:\n            ct = self.data.counts[attribute]\n            keeps = ct[ct >= threshold.get(attribute, default)]\n            values = self.data.attribute_values[attribute]\n            yield attribute, tuple(v for v in values if v[0] in keeps)\n\n\nfor d in NDSC.values():\n    d.filtered = FilteredValues(d)","d69c29e0":"#@title Attribute Value Occurence { run: \"auto\", display-mode: \"form\" }\n\npreview_category = \"beauty\" #@param [\"beauty\", \"fashion\", \"mobile\"]\npreview_attribute = \"\" #@param {type:\"string\"}\n\nshow = NDSC[preview_category].counts.get(preview_attribute)\nshow_preview(show, lambda d: d.plot.bar(figsize=(12, 6)))","d5ae468e":"#@title Scoring utilities\n\ndef blank_scores(data):\n    for attribute, avs in data.attribute_values.items():\n        kw = {\n            'dtype': float,\n            'index': data.submit.itemid,\n            'columns': tuple(value for k, value in avs)\n        }\n        yield attribute, pd.DataFrame(**kw).fillna(0.)\n\n\ndef baseline_random(data, n=1):\n    yield 'method', 'random'\n    for attr, avs in data.attribute_values.items():\n        l = len(avs)\n        s = data.train_size\n        labels = data.labels[attr]\n        \n        def pred():\n            return np.array(tuple(np.random.permutation(l) for _ in range(s)))\n        # Note: generating random values takes a lot of time\n        # prec = np.array(tuple(mAP(labels, pred()) for i in range(n)))\n        # yield attr, n \/ np.sum(1. \/ prec)\n        yield attr, 0.\n\n\ndef baseline_frequency(data, weight=1):\n    yield 'method', 'frequency'\n    for attr, df in data.scores.items():\n        counts = data.train[attr].replace(-1, np.nan).value_counts(True)\n        precision = mAP(data.labels[attr], counts.index.values)\n        for idx, frequency in counts.items():\n            df.iloc[:, int(idx)] += frequency * precision * weight\n        yield attr, precision","b4ffc7f3":"for d in NDSC.values():\n    d.scores = dict(blank_scores(d))\n\n    cols = ['method']\n    cols.extend(d.attribute_types)\n    rand = dict(baseline_random(d))\n    freq = dict(baseline_frequency(d))\n    d.mAP = pd.DataFrame.from_records((rand, freq), columns=cols)","cf006f5c":"#@title Preview mAP { run: \"auto\", display-mode: \"form\" }\n\npreview_category = \"beauty\" #@param [\"Choose one...\", \"beauty\", \"fashion\", \"mobile\"]\nshow_preview(NDSC.get(preview_category), lambda d: d.mAP.head())","37f645d2":"#@title Title Matching\n\ndef extract_values(column, values):\n    \"\"\"Match attribute value name and return length\"\"\"\n    for key, name in values:\n        print('\\r\\t%5d - %s' % (key, name), end='')\n        reg = '\\\\b(?P<match>%s)\\\\b' % name.replace(' ', '.*')\n        match = column.str.extract(reg, expand=False).str.len()\n        yield name, match\n\n\ndef train_title(data):\n    yield 'method', 'title regex'\n    for attr, avs in data.filtered.items():\n        print('\\r +-- %s' % attr)\n        df = pd.DataFrame(data=dict(extract_values(data.train.title, avs)))\n        predicted = df.dropna(0, 'all')\n        labels = data.labels[attr][predicted.index]\n        pred = predicted.fillna(np.inf).values.argsort()\n        prec = mAP(labels, data.filtered.index[attr][pred])\n        yield attr, prec\n    print('\\r ')\n\n\ndef test_title(data, weight=1):\n    title_mAP = data.mAP[data.mAP.method == 'title regex']    \n    for attr, avs in data.filtered.items():\n        print('\\r +-- %s' % attr)\n        w = title_mAP[attr].values[0] * weight\n        matches = dict(extract_values(data.submit.title, avs))\n        df = pd.DataFrame(data=matches).fillna(np.inf).rdiv(w)\n        data.scores[attr][df.columns] += df.values\n    print('\\r ')","e86a4e5a":"for d in NDSC.values():\n    print(d.name)\n    d.mAP = d.mAP.append(dict(train_title(d)), ignore_index=True).fillna(0.)","d622468e":"for d in NDSC.values():\n    print(d.name)\n    test_title(d)","2503e12d":"#@title Preview scores { run: \"auto\", display-mode: \"form\" }\n\npreview_category = \"mobile\" #@param [\"beauty\", \"fashion\", \"mobile\"]\npreview_attribute = \"Operating System\" #@param {type:\"string\"}\n\nshow = NDSC[preview_category].scores.get(preview_attribute)\nshow_preview(show, lambda d: d.head())","60accfba":"#@title Generate submission file\n\nsubmission_name = 'title_regex_only' #@param {type:\"string\"}\ntop_k = 2 #@param {type:\"slider\", min:0, max:5, step:1}\n\nsubmission_filepath = '%s\/submission_%s.csv' % (PROJECT_DIR, submission_name)\nfmt = '%d_{},' + ' '.join('%d' for _ in range(top_k))\n\nprint('Saving to %s' % submission_filepath)\nwith open(submission_filepath, 'w') as f:\n    print('id,tagging', file=f)\n    for d in NDSC.values():\n        print('%s ...' % d.name, end='')\n        itemids = d.submit.itemid.values[:, np.newaxis]\n        for attribute, df in d.scores.items():\n            predictions = (-df).values.argsort()[:, :top_k]\n            output = np.hstack((itemids, predictions))\n            np.savetxt(f, output, fmt=fmt.format(attribute))\n        print(' OK')","c4afcd71":"#@title Preview submission\n!head $submission_filepath\nprint('...')\n!tail $submission_filepath","6c8a9dfb":"## Kaggle Submission","3aee72ba":"## Text","294fe22f":"### RegEx Matching of Attribute Value names\n\nWe rank shorter matches as more likely","5e26a1a0":"## Scoring system\n\n- Determining the AV for a particular AT is a multiclass classification problem\n- Since we can predict 2 AV for each AT, we can rank them and output the top 2\n- For each AT, we store a score for each AV (column) for each item (row)\n\n### Baselines\n\nWe use some baselines to evaluate the effectiveness of our methods\n- Random guessing\n- Relative frequency in training set","105b5ca0":"# National Data Science Challenge\n\n### Team Name: `git push -f`\n\nNote: This notebook was lazily ported over from a Google Colaboratory notebook, which explains the spurious `#@title` stuff","208ca6de":"### Filtering\n\n - Training is not effective when examples are too few or classes too many\n - `n => 10` was chosen as a default sweet spot\n - Overrides were used when the noise was too great (e.g Mobile - Phone Model)\n - A custom class was implemented to allow setting thresholds on the fly","45fdbc27":"## Load and pre-process data\n\n- An object is used to hold all the info in one place\n- Fix `image_path` in the Fashion dataset (some have missing `.jpg` extensions)\n- Fill in missing labels with `-1` and cast to `int`\n- Count frequency of attribute values ","0dfbbe5b":"## Imports and Utilities"}}