{"cell_type":{"0e4b95e8":"code","7b4f56c5":"code","1f9c7f69":"code","72a384a9":"code","51d61b6d":"code","5787c315":"code","1aec7c36":"code","11e25836":"code","bcafa8fc":"code","8cf29b12":"code","795945b1":"code","796bbbaa":"code","35de3d13":"code","72ffe70f":"code","57edda14":"code","3c3ca7c3":"code","9b0d17d9":"code","3e3dad85":"code","0d5915e7":"code","74d7eb68":"code","9b6e93f2":"markdown","4421a254":"markdown","7f5d11f7":"markdown"},"source":{"0e4b95e8":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy\nfrom scipy import stats\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport xgboost as xgb","7b4f56c5":"features = pd.read_csv('..\/input\/jane-street-yeo-data\/features.csv')","1f9c7f69":"outcomes = pd.read_csv('..\/input\/jane-street-yeo-data\/outcomes.csv')","72a384a9":"def reduce_memory_usage(df):\n    \n    start_memory = df.memory_usage().sum() \/ 1024**2\n    print(f\"Memory usage of dataframe is {start_memory} MB\")\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != 'object':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    pass\n        else:\n            df[col] = df[col].astype('category')\n    \n    end_memory = df.memory_usage().sum() \/ 1024**2\n    print(f\"Memory usage of dataframe after reduction {end_memory} MB\")\n    print(f\"Reduced by {100 * (start_memory - end_memory) \/ start_memory} % \")\n    return df\n\n# https:\/\/www.kaggle.com\/sbunzini\/reduce-memory-usage-by-75","51d61b6d":"reduce_memory_usage(features)","5787c315":"reduce_memory_usage(outcomes)","1aec7c36":"print(features.info())","11e25836":"###Create Real Score for Scoring\noutcomes.loc[outcomes['resp'] >= 0,'real_score'] = int(1)\noutcomes.loc[outcomes['resp']< 0,'real_score'] = int(0)","bcafa8fc":"#Include date in features\nfeatures['date']=outcomes['date']\nfeatures['date'].isnull().sum()\n","8cf29b12":"features[['date']].describe()","795945b1":"#Split Features Into Quarters\nFirst_25_Percent_features=features[features['date']<=203]\nSecond_25_Percent_features=features[(features['date']>203)&(features['date']<309)]\nThird_25_Percent_features=features[(features['date']>=309)&(features['date']<409)]\nFourth_25_Percent_features=features[features['date']>=409]","796bbbaa":"\n#Split Outcome Into Quarters\nFirst_25_Percent_outcomes=outcomes[outcomes['date']<=203]\nSecond_25_Percent_outcomes=outcomes[(outcomes['date']>203)&(outcomes['date']<309)]\nThird_25_Percent_outcomes=outcomes[(outcomes['date']>=309)&(outcomes['date']<409)]\nFourth_25_Percent_outcomes=outcomes[outcomes['date']>=409]","35de3d13":"#Note:The function reindexed the data. Remove index column. \n\nX_Q1=np.array(First_25_Percent_features.iloc[::,1:-1])\ny_Q1=np.array(First_25_Percent_outcomes['action'])\nX_Q2=np.array(Second_25_Percent_features.iloc[::,1:-1])\ny_Q2=np.array(Second_25_Percent_outcomes['action'])\nX_Q3=np.array(Third_25_Percent_features.iloc[::,1:-1])\ny_Q3=np.array(Third_25_Percent_outcomes['action'])\nX_Q4=np.array(Fourth_25_Percent_features.iloc[::,1:-1])\ny_Q4=np.array(Fourth_25_Percent_outcomes['action'])","72ffe70f":"# Quarterly Model","57edda14":"#xg_clas= xgb.XGBClassifier(objective= 'binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,\n#                max_depth = 3, gamma=.1, subsample=0.8, alpha = 10, scale_pos_weight = 1.22, n_estimators=1000, verbosity=2, tree_method ='gpu_hist')\nxg_clas= xgb.XGBClassifier(objective= 'binary:logistic', colsample_bytree = 0.3, learning_rate = 0.05,\n               max_depth = 2, gamma=.1, subsample=0.8, min_child_weight=4, alpha = 10, scale_pos_weight = 1.22, n_estimators=1000, tree_method ='gpu_hist', verbosity=2)","3c3ca7c3":"#data_dmatrix = xgb.DMatrix(data=X,label=y)\n#model.fit(X_train, Y_train, eval_metric=\"rmse\", eval_set=[(X_train, Y_train), (X_cv, Y_cv)], verbose=True, early_stopping_rounds = 10)","9b0d17d9":"xg_clas.fit(X_Q1, y_Q1)","3e3dad85":"# Performance of Q1\nq1_y_pred = xg_clas.predict(X_Q1)\nauc = roc_auc_score(y_Q1, q1_y_pred)\nprint(\"Q1 AUC Performance:\", auc)\n\n# Performance on Q2\nq2_y_pred = xg_clas.predict(X_Q2)\nauc = roc_auc_score(y_Q2, q2_y_pred)\nprint(\"Q2 AUC  Test Performance : \", auc)\n\n# Performance on Q3\nq3_y_pred = xg_clas.predict(X_Q3)\nauc = roc_auc_score(y_Q3, q3_y_pred)\nprint(\"Q3 AUC  Test Performance : \", auc)\n\n# Performance on Q4\nq4_y_pred = xg_clas.predict(X_Q4)\nauc = roc_auc_score(y_Q4, q4_y_pred)\nprint(\"Q4 AUC  Test Performance : \", auc)","0d5915e7":"from sklearn.metrics import confusion_matrix\n#q2_real_y=np.array(Second_25_Percent_outcomes['real_score'])\n\ntn, fp, fn, tp = confusion_matrix(y_Q2, xg_clas.predict(X_Q2)).ravel()\n# Q2 Error rate : \nerr_rate = (fp + fn) \/ (tp + tn + fn + fp)\nprint(\"Error rate  : \", err_rate)\n# Q2 Accuracy : \nacc_ = (tp + tn) \/ (tp + tn + fn + fp)\nprint(\"Accuracy  : \", acc_)\n# Q2 Sensitivity : \nsens_ = tp \/ (tp + fn)\nprint(\"Sensitivity  : \", sens_)\n# Q2 Specificity \nsp_ = tn \/ (tn + fp)\nprint(\"Specificity  : \", sens_)\n# Q2 False positive rate (FPR)\nFPR = fp \/ (tn + fp)\nprint(\"False positive rate  : \", FPR)","74d7eb68":"xgb.plot_importance(xg_clas, max_num_features=20)\nplt.rcParams['figure.figsize'] = [10, 10]\nplt.show()","9b6e93f2":"## Parameters Meaning\n\n#### learning_rate: \n\nstep size shrinkage to avoid overfitting. **Common Starting Points:** .1\n\n#### max depth:\ncontrols depth of trees. More shallow = reduced complexity = more underfit. **Common Starting Points:** 4-6\n\n#### subsample: \npercentage of samples used per tree. Lower value = more under fit. **Common Starting Points:** .8\n\n#### col sample_bytree: \n% of features used per tree. High value = more overfitting. **Common Starting Points:** .5-.9\n\n#### n_estimators\/num_boost_round: \nnumber of trees to build. **Common Starting Points:** 500-1000\n\n#### Objective: Loss Functions\n\n-reg:linear : predict continuous values\n-reg:logistic: single decision classification\n-reg:binary : probability based classification\n\n#### scale_pos_weight:\nUsed for class imbalance to adjust postive class. **#neg \/ #pos**\n\n#### min child weight: \nMinimum number of samples if all samples have weight 1 required to create a new node. A small number means that the alg will create new leafs even when only a few samples are left to distinguish. This leads to more complexity but can also increase overfitting.  **Common Starting Points:** 1\n\n### Regularizers:\n\n#### gamma: \nadjusts propensity of node to split in tree based learners based on reduction in loss. higher gamma = less splits. **Common Starting Points:** 0-.2\n\n#### reg_alpha: \nL1 regularizer of leaf weights. \n\n\n#### reg_lambda: \nL2 regularizer of leaf weights. \n\n\n","4421a254":"# XGBoost Model","7f5d11f7":"# Jane Street: Effect of Delays in Time When Using XGboost? \n\n**Goal:** \n\nI wanted to test a few questions. First, I wanted to see if training on part of the Jane Street data effected AUC. I also wanted to see how time effects my XGBoost models ability to predict. I split training data into 4 quarters. Preprocessing included filling NaN's with conditional means based on day. Yeo Johnson Transformation to fix skew. Removal of outliers by keeping data between the 5th and 97th quantiles. The AUC remains low. \n\n**Findings:** \n\n1) Time lag did not appear to effect predictive ability of model. \n\n2) Training on part of the data had a similar AUC to training on all of the data.\n\n**Next Steps:**\n\n1) Make model useable for one test observation at a time.\n\n2) Perform cross validation on model. \n\n3) Find GPU and Tune Parameters. \n\n**Helpful Links:**\n\n1) https:\/\/www.kaggle.com\/dstuerzer\/optimization-of-xgboost\n\n2) https:\/\/www.datacamp.com\/community\/tutorials\/xgboost-in-python\n\n3) https:\/\/www.kaggle.com\/saxinou\/imbalanced-data-xgboost-tunning\n\n"}}