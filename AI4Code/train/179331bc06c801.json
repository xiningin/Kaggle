{"cell_type":{"aac00fff":"code","c41a3f21":"code","4bf3a5cc":"code","93993e6c":"code","6da0bbc4":"code","dd460903":"code","69341d6a":"code","49ce006f":"code","7ea88de9":"code","143f39d9":"code","c9f63113":"code","40b1caed":"code","9513da89":"code","be6ce16f":"code","99d60c1a":"code","a3e1a32a":"code","a1ed2270":"code","941d209a":"code","7897184d":"code","a52a2b67":"code","743a67c3":"code","85670eef":"code","49f29cfd":"code","f3c92277":"code","e9f24dc5":"markdown","36eb36ce":"markdown","43dff227":"markdown","3761e10d":"markdown","8eb91544":"markdown","8323631a":"markdown","16449d34":"markdown","9508156a":"markdown","7e8571c9":"markdown","3f16f1ad":"markdown","2e759ea1":"markdown","1e821687":"markdown","e357ab88":"markdown","526bc8d4":"markdown","f4003255":"markdown","03ae6fc5":"markdown","849760da":"markdown","74aa57b6":"markdown","7476a8f0":"markdown","86cb7bb9":"markdown","2b94e20e":"markdown","8eb1122b":"markdown","60b1dd05":"markdown","90b6fa7f":"markdown","7a60b665":"markdown","985a2190":"markdown","df511dfe":"markdown"},"source":{"aac00fff":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c41a3f21":"# know the environment for your current notebook\n\nimport sys\nprint(sys.executable)","4bf3a5cc":"# uncomment\/comment down below line to install\/uninstall hugging-face transformers\n\n!pip install transformers","93993e6c":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport torch\nimport transformers as ppb # pytorch-transformers by huggingface\nimport warnings\nimport time\nwarnings.filterwarnings('ignore')","6da0bbc4":"path = '..\/input\/stanford-sentiment-treebank-v2-sst2\/datasets\/'\n\ndf = pd.read_csv(path + 'tsv-format\/train.tsv', delimiter='\\t')\n\n\n# to read via CSV files...\n# df = pd.read_csv(path + 'csv-format\/train.csv')","dd460903":"# Lets find out the shape of dataset.\ndf.shape","69341d6a":"batch_1 = df[:2000]","49ce006f":"batch_1.head(5)","7ea88de9":"batch_1['Ratings'].value_counts()","143f39d9":"model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n\n## Want BERT instead of distilBERT? Uncomment the following line:\n#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')","c9f63113":"# Load pretrained model\/tokenizer\nprint(time.ctime())\n\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\nmodel = model_class.from_pretrained(pretrained_weights)\n\nprint(time.ctime())","40b1caed":"print(time.ctime())\ntokenized = batch_1['Reviews'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\nprint(time.ctime())","9513da89":"tokenized.shape","be6ce16f":"print(time.ctime())\n\nmax_len = 0\nfor i in tokenized.values:\n    if len(i) > max_len:\n        max_len = len(i)\n\npadded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n\nprint(time.ctime())","99d60c1a":"np.array(padded).shape","a3e1a32a":"attention_mask = np.where(padded != 0, 1, 0)\nattention_mask.shape","a1ed2270":"print(time.ctime())\n\ninput_ids = torch.tensor(padded)  \nattention_mask = torch.tensor(attention_mask)\n\nwith torch.no_grad():\n    last_hidden_states = model(input_ids, attention_mask=attention_mask)\n    \n\nprint(time.ctime())","941d209a":"features = last_hidden_states[0][:,0,:].numpy()","7897184d":"labels = batch_1['Ratings']","a52a2b67":"train_features, test_features, train_labels, test_labels = train_test_split(features, labels)","743a67c3":"\n# parameters = {'C': np.linspace(0.0001, 100, 20)}\n# grid_search = GridSearchCV(LogisticRegression(), parameters)\n# grid_search.fit(train_features, train_labels)\n\n# print('best parameters: ', grid_search.best_params_)\n# print('best scrores: ', grid_search.best_score_)","85670eef":"lr_clf = LogisticRegression()\nlr_clf.fit(train_features, train_labels)","49f29cfd":"lr_clf.score(test_features, test_labels)","f3c92277":"from sklearn.dummy import DummyClassifier\nclf = DummyClassifier()\n\nscores = cross_val_score(clf, train_features, train_labels)\nprint(\"Dummy classifier score: %0.3f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","e9f24dc5":"Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence. The way BERT does sentence classification, is that it adds a token called **[CLS]** (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n\n![slices](https:\/\/camo.githubusercontent.com\/009eabd7b7697055256ab3655f048d5c31967e1b\/68747470733a2f2f6a616c616d6d61722e6769746875622e696f2f696d616765732f64697374696c424552542f626572742d6f75747075742d74656e736f722d73656c656374696f6e2e706e67)\n\n\nWe'll save those in the features variable, as they'll serve as the **features to our logitics regression model**.\n\n### This is how we slice that 3d tensor to get the 2d tensor we\u2019re interested in:","36eb36ce":"#### Our dataset is now in the padded variable, we can view its dimensions below:","43dff227":"### Importing necessary Libraries.","3761e10d":"![DISTILLBERT-INTRO](https:\/\/camo.githubusercontent.com\/0c9b76326c1d9bd7d001a3cdac1837b6b4c6b9e8\/68747470733a2f2f6a616c616d6d61722e6769746875622e696f2f696d616765732f64697374696c424552542f626572742d64697374696c626572742d73656e74656e63652d636c617373696669636174696f6e2e706e67)\n\n\n#### We gonna try a Pre-trained deep learning model to process some text. We will then use the output of that model to classify the text. The text is a list of sentences from film reviews. And we will calssify each sentence as either speaking \"positively\" about its subject or \"negatively\".","8eb91544":"## Padding\nAfter tokenization, tokenized is a list of sentences -- each sentences is represented as a list of tokens. We want BERT to process our examples all at once (as one batch). It's just faster that way. For that reason, we need to pad all lists to the same size, so we can represent the input as one 2-d array, rather than a list of lists (of different lengths).","8323631a":"## Masking\nIf we directly send padded to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what attention_mask is:","16449d34":"### If you have come this far, I consider you are liking this kernel.\n\n## <span style=\"color:red;\">DO UPVOTE if you like this kernel. It always motivates me to do more and bring such informative Kernels.Leave your feedbacks down below in the comment section.<\/span>\n\n# Thanks !","9508156a":"![tokenize](https:\/\/camo.githubusercontent.com\/c9d55621f1c4518e88ffe47c0a914e1e71db836c\/68747470733a2f2f6a616c616d6d61722e6769746875622e696f2f696d616765732f64697374696c424552542f626572742d64697374696c626572742d746f6b656e697a6174696f6e2d322d746f6b656e2d6964732e706e67)","7e8571c9":"## Now DEEP LEARNING !\n\nNow that we have our model and inputs ready, let's run our model!\n\nThe **model()** function runs our sentences through BERT. The results of the processing will be returned into **last_hidden_states**.","3f16f1ad":"### <span style=\"color:blue\">you need to toggle ON the Internet button on right to install external package.<\/span>","2e759ea1":"### Note: You can read the data from this dataset in two formats : CSV and TSV.","1e821687":"# Movie Review Sentiment Classification with DISTILLBERT","e357ab88":"#### How good is this score? What can we compare it against? Let's first look at a dummy classifier:","526bc8d4":"#### <span style=\"color:green;\">So our model clearly does better than a dummy classifier. But how does it compare against the best models?<\/span>","f4003255":"## we train the Logistic Regression model on the training set.","03ae6fc5":"## Loading the Pre-trained DISTILLBERT model\n#### Let's now load a pre-trained BERT model.","849760da":"## [Bonus] Grid Search for Parameters\u00b6\nWe can dive into Logistic regression directly with the Scikit Learn default parameters, but sometimes it's worth searching for the best value of the C parameter, which determines regularization strength.","74aa57b6":"## Model #1: Preparing the Dataset\nBefore we can hand our sentences to BERT, we need to so some minimal processing to put them in the format it requires.\n\n## Tokenization\nOur first step is to tokenize the sentences -- break them up into word and subwords in the format BERT is comfortable with.","7476a8f0":"### <span style=\"color:red;\">Please! consider UPVOTING the kernel and leave your feedback down below in the comment section. It boosts moral to do more and more.<\/span>","86cb7bb9":"#### Right now, the variable model holds a pretrained distilBERT model -- a version of BERT that is smaller, but much faster and requiring a lot less memory.","2b94e20e":"### Proper SST2 scores\u00b6\nFor reference, the highest accuracy score for this dataset is currently <span style=\"color:blue;\">**96.8**<\/span>. DistilBERT can be trained to improve its score on this task \u2013 a process called fine-tuning which updates BERT\u2019s weights to make it achieve a better performance in this sentence classification task (which we can call the downstream task). The fine-tuned DistilBERT turns out to achieve an accuracy score of <span style=\"color:blue;\">**90.7**<\/span>. The full size BERT model achieves <span style=\"color:blue;\">**94.9**<\/span>.\n\n\n<span style=\"color:brown;\">And that\u2019s it! That\u2019s a good first contact with BERT. The next step would be to head over to the documentation and try your hand at fine-tuning. You can also go back and switch from distilBERT to BERT and see how that works.<\/span>","8eb1122b":"## Models: Sentence Sentiment Classification\n#### Our goal is to create a model that takes a sentence (just like the ones in our dataset) and produces either 1 (indicating the sentence carries a positive sentiment) or a 0 (indicating the sentence carries a negative sentiment). We can think of it as looking like this:\n\n![1](https:\/\/camo.githubusercontent.com\/13383b5db2198b3c4002854fd5a7fbb51f8e6e14\/68747470733a2f2f6a616c616d6d61722e6769746875622e696f2f696d616765732f64697374696c424552542f73656e74696d656e742d636c61737369666965722d312e706e67)\n\n### NOTE : Under the hood, the model is actually made up of two model.\n\n#### DistilBERT processes the sentence and passes along some information it extracted from it on to the next model. DistilBERT is a smaller version of BERT developed and open sourced by the team at HuggingFace. It\u2019s a lighter and faster version of BERT that roughly matches its performance.\n\n#### The next model, a basic Logistic Regression model from scikit learn will take in the result of DistilBERT\u2019s processing, and classify the sentence as either positive or negative (1 or 0, respectively).\n\nThe data we pass between the two models is a vector of size 768. We can think of this of vector as an embedding for the sentence that we can use for classification.\n\n![2](https:\/\/camo.githubusercontent.com\/a294fd2ef0f04be622a71bc77afed77041238cd2\/68747470733a2f2f6a616c616d6d61722e6769746875622e696f2f696d616765732f64697374696c424552542f64697374696c626572742d626572742d73656e74696d656e742d636c61737369666965722e706e67)\n\n#### Dataset\nThe dataset we will use in this example is SST2, which contains sentences from movie reviews, each labeled as either positive (has the value 1) or negative (has the value 0).\n\n","60b1dd05":"![tokenized-pic](http:\/\/jalammar.github.io\/images\/distilBERT\/sst2-text-to-tokenized-ids-bert-example.png)","90b6fa7f":"## References:\n\n1. [A Visual Guide to Using BERT for the First Time](http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/)","7a60b665":"#### For performance reasons, we'll only use 2,000 sentences from the dataset","985a2190":"#### And now features is a 2d numpy array containing the sentence embeddings of all the sentences in our dataset.\n\n![image-features](http:\/\/jalammar.github.io\/images\/distilBERT\/bert-output-cls-senteence-embeddings.png)","df511dfe":"## Dataset for Logistic Regression\nNow that we have the output of BERT, we have assembled the dataset we need to train our logistic regression model. The 768 columns are the features, and the labels we just get from our initial dataset.\n\n![LR-data](http:\/\/jalammar.github.io\/images\/distilBERT\/logistic-regression-dataset-features-labels.png)\n\nThe labeled dataset we use to train the Logistic Regression. The features are the output vectors of BERT for the [CLS] token (position #0) that we sliced in the previous figure. Each row corresponds to a sentence in our dataset, each column corresponds to the output of a hidden unit from the feed-forward neural network at the top transformer block of the Bert\/DistilBERT model."}}