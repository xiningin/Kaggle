{"cell_type":{"0270bd55":"code","c1bd7129":"code","5c52c07b":"code","ca562b50":"code","8b005a0e":"code","5f3e5e53":"code","58a59d0a":"code","32265dbc":"code","c4086d4a":"code","72f2b27f":"code","bf04cc3f":"code","5ffceb3f":"code","83dfa35b":"code","526b5fe3":"code","4852ffec":"code","dc48a220":"code","24075a84":"code","253a06f0":"code","df4bcdd9":"code","138648ef":"code","2e844893":"code","e5ede4a9":"code","3ed534fa":"code","adbfa62b":"code","e7970c73":"code","34f9448e":"code","e1bb36b0":"code","5dccb7a1":"code","bc3e7a29":"code","ee479163":"code","b86caef8":"code","9265e598":"code","f7f01f6d":"code","a9165748":"code","683fd3ae":"code","4d0c740c":"code","e5361309":"code","14d330e9":"code","babe2a6c":"code","495a4fb6":"code","52b2cb9c":"code","392dc87a":"code","47871ebd":"code","8a6cf037":"code","e2f88de1":"code","8c34598c":"code","6382037a":"code","b7e420db":"code","c04fee8e":"code","c96dfbde":"code","49788591":"code","2060f8ed":"code","97c4dee8":"code","431654c3":"code","810df5a0":"code","ae289500":"code","9c05c339":"code","64a566c8":"code","aecc4b0b":"code","b1749c3e":"code","b52ec463":"code","c15b9e97":"markdown","f55d4eb8":"markdown","b3e9868f":"markdown","71aa865c":"markdown","349c875a":"markdown","0ea50b99":"markdown","e2d4588f":"markdown","45c33d68":"markdown","75b538fc":"markdown","04edcf39":"markdown","df528adc":"markdown","595c857d":"markdown","030115c7":"markdown","84868e7b":"markdown","cde7fd65":"markdown","d20fc839":"markdown","6b137160":"markdown","5e74a96d":"markdown","0175d588":"markdown","58335667":"markdown","f72db47d":"markdown","e24c3881":"markdown","39fa16e8":"markdown","6e141ed8":"markdown","75d9b714":"markdown","060db65d":"markdown","829c5ee1":"markdown","037a0c43":"markdown","11de0c27":"markdown","fae2dcbd":"markdown","a4b22c49":"markdown","889f506e":"markdown","e85b3054":"markdown","32121339":"markdown","cdf23d7f":"markdown","b3d20eb2":"markdown","f841fdce":"markdown","12ee2d93":"markdown"},"source":{"0270bd55":"import pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c1bd7129":"cac_df = pd.read_csv('\/kaggle\/input\/cac1data\/cac40.csv')\ncac_df.head()","5c52c07b":"cac_df.info() # give the complacte inforamtion of dataset including datatypes null values ","ca562b50":"cac_df.describe() # give the statistical informaion of our dataset","8b005a0e":"cac_df['Date'] = pd.to_datetime(cac_df['Date']) \ncac_df.Date.head()","5f3e5e53":"print('There are {} number of days in the dataset.'.format(cac_df.shape[0]))","58a59d0a":"cac_df.set_index('Date', inplace=True)","32265dbc":"cac_df.columns","c4086d4a":"def get_technical_indicators(dataset): #function to generate feature technical indicators\n    \n    \n    # Create 7 and 21 days Moving Average\n    dataset['ma7'] = dataset['Close'].rolling(window = 7).mean()\n    dataset['ma21'] = dataset['Close'].rolling(window = 21).mean()\n    \n    #Create MACD\n    dataset['26ema'] = dataset['Close'].ewm(span=26).mean()\n    dataset['12ema'] = dataset['Close'].ewm(span=12).mean()\n    dataset['MACD'] = (dataset['12ema']-dataset['26ema'])\n    \n    #Create Bollinger Bands\n    dataset['20sd'] = dataset['Close'].rolling(window = 20).std()\n    dataset['upper_band'] = (dataset['Close'].rolling(window = 20).mean()) + (dataset['20sd']*2)\n    dataset['lower_band'] = (dataset['Close'].rolling(window = 20).mean()) - (dataset['20sd']*2)\n    \n    \n    #Create Exponential moving average\n    dataset['ema'] = dataset['Close'].ewm(com=0.5).mean()\n    \n    #Create Momentum\n    dataset['momentum'] = (dataset['Close']\/100)-1\n    #Create ARIMA\n    dataset['ARIMA'] = 0\n    \n    return dataset","72f2b27f":"cac1_df = get_technical_indicators(cac_df)\ncac1_df.head()","bf04cc3f":"cac1_df[['Open','Close']].plot()\nplt.show()","5ffceb3f":"train_data, test_data = cac1_df[0:int(len(cac1_df)*0.7)], cac1_df[int(len(cac1_df)*0.7):]\ntraining_data = train_data['Close'].values\ntest_data = test_data['Close'].values","83dfa35b":"training_data1=pd.Series(training_data)","526b5fe3":"#plot training_data\ntraining_data1.plot(figsize=(15, 6))\nplt.show()","4852ffec":"cac1_df['First Order Difference'] = cac1_df['Close'] - cac1_df['Close'].shift(1)","dc48a220":"cac1_df['First Order Difference'].plot(figsize=(12, 6))\nplt.show()","24075a84":"from statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf","253a06f0":"import statsmodels.api as sm\n\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(training_data1, lags=40, ax=ax1) # \nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(training_data1, lags=40, ax=ax2)# , lags=40","df4bcdd9":"fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(cac1_df.Open, lags=40, ax=ax1) # \nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(cac1_df.Open, lags=40, ax=ax2)# , lags=40","138648ef":"from statsmodels.tsa.arima_model import ARIMA\nfrom pandas import DataFrame\nfrom pandas import datetime\n\nseries = cac1_df['Close']\nmodel = ARIMA(series, order=(5, 1, 0))\nmodel_fit = model.fit(disp=0)\nprint(model_fit.summary())\n","2e844893":"from pandas.plotting import autocorrelation_plot\nautocorrelation_plot(series)\nplt.figure(figsize=(10, 7), dpi=80)\nplt.show()","e5ede4a9":"from statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\n\nX = series.values\ntrain_data, test_data = X[0:int(len(X)*0.7)], X[int(len(X)*0.7):]\nhistory = [x for x in train_data]\npredictions = list()\nfor t in range(len(test_data)):\n    model = ARIMA(history, order=(5, 1, 0))\n    model_fit = model.fit(disp=0)\n    output = model_fit.forecast()\n    yhat = output[0]\n    predictions.append(yhat)\n    obs = test_data[t]\n    history.append(obs)\n\ncac1_df['ARIMA'] = pd.DataFrame(predictions)","3ed534fa":"error = mean_squared_error(test_data, predictions)\nprint('Test RMSE: %.3f' % error)","adbfa62b":"# Plot the predicted (from ARIMA) and real prices\n\nplt.figure(figsize=(12, 6), dpi=100)\nplt.plot(test_data, color='black', label='Real')\nplt.plot(predictions, color='yellow', label='Predicted')\nplt.xlabel('Days')\nplt.ylabel('USD')\nplt.title('ARIMA model on CAC')\nplt.legend()\nplt.show()","e7970c73":"cac1_df.head(8)","34f9448e":"print('Total dataset has {} samples, and {} features.'.format(cac1_df.shape[0], \\\n                                                              cac1_df.shape[1]))","e1bb36b0":"cac1_df.head()","5dccb7a1":"print('Total dataset has {} samples, and {} features.'.format(cac1_df.shape[0], \\\n                                                              cac1_df.shape[1]))","bc3e7a29":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten","ee479163":"#creating test, train and validate trains\ntrain, validate, test = np.split(cac1_df.sample(frac=1), [int(.6*len(cac1_df)), int(.8*len(cac1_df))])","b86caef8":"open_training = train.iloc[:, 1:2].values","9265e598":"#normalise\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range = (0, 1))\nopen_training = scaler.fit_transform(open_training)\n#convert to right shape\nfeatures_set_1 = []\nlabels_1 = []\nfor i in range(60,450): \n    features_set_1.append(open_training[i-60:i, 0])\n    labels_1.append(open_training[i, 0])\n    \n","f7f01f6d":"features_set_1, labels_1 = np.array(features_set_1), np.array(labels_1)\nfeatures_set_1 = np.reshape(features_set_1, (features_set_1.shape[0], features_set_1.shape[1], 1))","a9165748":"#training it\nmodel = Sequential()\nmodel.add(LSTM(units=50, return_sequences=True, input_shape=(features_set_1.shape[1],1)))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(units=50, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(units = 1))\nmodel.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['mean_absolute_error'])\nmodel.fit(features_set_1, labels_1, epochs = 100, batch_size = 32,validation_data = (features_set_1, labels_1))\n","683fd3ae":"#TESTING THE MODEL\nopen_testing_processed = test.iloc[:, 1:2].values","4d0c740c":"#convert test data to right format\nopen_total = pd.concat((train['Open'], test['Open']), axis=0)","e5361309":"test_inputs = open_total[len(open_total) - len(test) - 60:].values","14d330e9":"#scaling data\ntest_inputs = test_inputs.reshape(-1,1)\ntest_inputs = scaler.transform(test_inputs)","babe2a6c":"test_features = []\nfor i in range(60, 151):\n    test_features.append(test_inputs[i-60:i, 0])","495a4fb6":"test_features = np.array(test_features)\ntest_features.shape\ntest_features = np.reshape(test_features, (test_features.shape[0], test_features.shape[1], 1))","52b2cb9c":"#make predictions\npredictions = model.predict(test_features)","392dc87a":"predictions = scaler.inverse_transform(predictions)","47871ebd":"plt.figure(figsize=(10,6))\nplt.plot(open_testing_processed, color='pink', label='Actual Stock Price')\nplt.plot(predictions , color='yellow', label='Predicted Stock Price')\nplt.title('Actual Value vs Predicted')\nplt.xlabel('Date')\nplt.ylabel('Predicted Price')\nplt.legend()\nplt.show()\n","8a6cf037":"dataset = cac1_df[['Open','High','Low','Close','Turnover']]\ndataset.head()","e2f88de1":"# FUNCTION TO CREATE 1D DATA INTO TIME SERIES DATASET\ndef new_dataset(dataset, step_size):\n\tdata_X, data_Y = [], []\n\tfor i in range(len(dataset)-step_size-1):\n\t\ta = dataset[i:(i+step_size), 0]\n\t\tdata_X.append(a)\n\t\tdata_Y.append(dataset[i + step_size, 0])\n\treturn np.array(data_X), np.array(data_Y)","8c34598c":"# IMPORTING IMPORTANT LIBRARIES\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import LSTM","6382037a":"# FOR REPRODUCIBILITY\nnp.random.seed(7)","b7e420db":"# IMPORTING DATASET \ndataset = dataset.reindex(index = dataset.index[::-1])\n# CREATING OWN INDEX FOR FLEXIBILITY\nobs = np.arange(1, len(dataset) + 1, 1)\n# TAKING DIFFERENT INDICATORS FOR PREDICTION\nOHLC_avg = dataset.mean(axis = 1)\nHLC_avg = dataset[['High', 'Low', 'Close']].mean(axis = 1)\nclose_val = dataset[['Close']]","c04fee8e":"# PLOTTING All INDICATORS IN PLOT\nplt.plot(OHLC_avg, 'yellow', label = 'OHLC avg')\nplt.plot(close_val, 'blue', label = 'Closing price')\nplt.xlabel('Days')\nplt.ylabel('OHLC average')\nplt.show()\n","c96dfbde":"plt.plot(HLC_avg, 'red', label = 'HLC avg')\nplt.xlabel('Days')\nplt.ylabel('HLC average')\nplt.show()","49788591":"plt.plot(close_val, 'blue', label = 'Closing price')\nplt.xlabel('Days')\nplt.ylabel('Closing Values')\nplt.show()","2060f8ed":"# PREPARATION OF TIME SERIES DATASE\nOHLC_avg = np.reshape(OHLC_avg.values, (len(OHLC_avg),1)) \nscaler = MinMaxScaler(feature_range=(0, 1))\nOHLC_avg = scaler.fit_transform(OHLC_avg)\n# TRAIN-TEST SPLIT\ntrain_OHLC = int(len(OHLC_avg) * 0.75)\ntest_OHLC = len(OHLC_avg) - train_OHLC\ntrain_OHLC, test_OHLC = OHLC_avg[0:train_OHLC,:], OHLC_avg[train_OHLC:len(OHLC_avg),:]\n# TIME-SERIES DATASET (FOR TIME T, VALUES FOR TIME T+1)\ntrainX, trainY = new_dataset(train_OHLC, 1)\ntestX, testY = new_dataset(test_OHLC, 1)\n# RESHAPING TRAIN AND TEST DATA\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\nstep_size = 1\n","97c4dee8":"# LSTM MODEL\nmodel = Sequential()\nmodel.add(LSTM(32, input_shape=(1, step_size), return_sequences = True))\nmodel.add(LSTM(16))\nmodel.add(Dense(1))\nmodel.add(Activation('linear'))","431654c3":"# MODEL COMPILING AND TRAINING\nmodel.compile(loss='mean_squared_error', optimizer='adagrad',metrics = ['mae']) # Try mae, adam, adagrad and compare!!!\nmodel.fit(trainX, trainY, epochs=50, batch_size=1, verbose=2)","810df5a0":"mae = model.evaluate(testX, testY, batch_size=16)\nprint('Mean Absolute Error for Y:', mae)","ae289500":"# PREDICTION\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)","9c05c339":"# DE-NORMALIZING FOR PLOTTING\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])\n# TRAINING RMSE\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train RMSE: %.2f' % (trainScore))","64a566c8":"# TEST RMSE\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test RMSE: %.2f' % (testScore))","aecc4b0b":"# CREATING SIMILAR DATASET TO PLOT TRAINING PREDICTIONS\ntrainPredictPlot = np.empty_like(OHLC_avg)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[step_size:len(trainPredict)+step_size, :] = trainPredict\n# CREATING SIMILAR DATASSET TO PLOT TEST PREDICTIONS\ntestPredictPlot = np.empty_like(OHLC_avg)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(step_size*2)+1:len(OHLC_avg)-1, :] = testPredict","b1749c3e":"# PLOT OF MAIN OHLC VALUES, TRAIN PREDICTIONS AND TEST PREDICTIONS\nplt.plot(trainPredictPlot, 'r', label = 'training set')\nplt.plot(testPredictPlot, 'b', label = 'predicted stock price\/test set')\nplt.legend(loc = 'upper right')\nplt.xlabel('Time in Days')\nplt.ylabel('Trend of training and prediction data')\nplt.show()\n","b52ec463":"# PREDICT FUTURE VALUES\nlast_val = testPredict[-1]\nlast_val_scaled = last_val\/last_val\nnext_val = model.predict(np.reshape(last_val_scaled, (1,1,1)))\nprint(\"Last Day Value:\", np.asscalar(last_val))\nprint(\"Next Day Value:\", np.asscalar(last_val*next_val))\n# print np.append(last_val, next_val)\n","c15b9e97":"### Reference Material","f55d4eb8":"Start by uploading data from HDFS or Local machine","b3e9868f":"**Plot the prediction model for the number of test days and train days**","71aa865c":"# CAC 40 Stock Price Forecast with ARIMA & LSTM","349c875a":"## Part 3. CAC 40 Stock Price Forecast with LSTM [optional]","0ea50b99":"### 2.4. Use ARIMA Model to Predict CAC 40 Stock Price After 2016 (Weekly Forecast)","e2d4588f":"### 2.2. Draw ACF&PACF Chart and Select Hyperparameter q&p","45c33d68":"**Normalize data:\nThe data is not normalized and the range for each column varies, especially Volume. Normalizing data helps the algorithm in converging i.e. to find local\/ global minimum efficiently. I will use MinMaxScaler from Sci-kit Learn. Use a range to keep values similar for that much range**\n\n**Keep a window for the length 2000 for your data between 50 and 500...since our length is slightly more than 2000 ill make it 60 to 450**","75b538fc":"#### instead of using the ARIMA model use LSTM and do the same steps as ARIMA one. You may also need to add a normalisation step in the pre processing part","04edcf39":"**Start predictions: Reshape, scale and then oredict the model**","df528adc":"#### 2.4.2. Evaluation the arima predicted model using RMSE","595c857d":"### 1.2 Display a summary of statistical measure of this data","030115c7":"**Following steps are done:**\n\n**1.  Clean up the data-Remove any NAs**\n\n**2.   Create a test, train and validate set**\n\n**3.   Create train for Open**\n\n**4.   Normalize data** \n\n**5.Create feature and label set**\n\n**6. Train, test data and  check with validation set**\n\n**7. Make a prediction**\n\n**8. Based on this prediction find if the feature extraction method of LSTM works**\n\n","84868e7b":"---","cde7fd65":"#### 2.2.1. Draw ACF Chart and Chose Hyperparameter q in MA Model","d20fc839":"**Split dataset into train,test and validate sets**","6b137160":"### 1.3. Change String Date to Datetime Format","5e74a96d":"### 1.1. Import Dataset from HDFS","0175d588":"### 1.4. Set Datetime to Index","58335667":"### 2.3. Define then train the ARIMA Model","f72db47d":"---","e24c3881":"* If you are not using HDFS, import the csv file directly from the local disk.\n* Ignore this step if you have imported csv files from HDFS.","39fa16e8":">**This wasn't a great result with one feature so let's try using more features and then train them on LSTM model**","6e141ed8":"* CAC 40 data Donwload: https:\/\/www.euronext.com\/en\/products\/indices\/FR0003500008-XPAR\n* Jupyter Notebook Connect with HDFS: http:\/\/nbviewer.jupyter.org\/github\/ofermend\/IPython-notebooks\/blob\/master\/blog-part-1.ipynb\n* Time Series Prediction with LSTM: https:\/\/machinelearningmastery.com\/time-series-prediction-lstm-recurrent-neural-networks-python-keras\/\n* Data Science with Apache Hadoop: Predicting Airline Delays: https:\/\/fr.hortonworks.com\/blog\/data-science-apacheh-hadoop-predicting-airline-delays\/","75d9b714":"#### 2.1.1. Make First Order Difference","060db65d":"---","829c5ee1":"### 1.7. Plot Training Data & Observation Data Trends","037a0c43":"## PART 1. Data pre processing","11de0c27":"#### 2.2.2. Plot the Training Data After the First Order Difference","fae2dcbd":"**USING 5 FEATURES :**","a4b22c49":"#### 2.4.1. Use ARIMA Model to Predict","889f506e":"#### 2.4.3. Plot the Predict Result","e85b3054":"### 1.5. Feature Selection & Data Resampling","32121339":"### 1.6. Split Dataset to train and test data","cdf23d7f":"## Part 2. CAC 40 Stock Price Forecast with ARIMA","b3d20eb2":"### 2.1. Make First Order Difference or Second Order Difference","f841fdce":"#### 2.2.2. Draw PACF Chart and Chose Hyperparameter p in AR Model","12ee2d93":"### 1.1. Import Dataset from Local Hard Drive"}}