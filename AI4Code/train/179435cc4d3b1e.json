{"cell_type":{"8c3c2dff":"code","373c29a1":"code","84c976f4":"code","23649f33":"code","3e9faa84":"code","254a5c51":"code","89324ad4":"code","77bd084e":"code","8590ad9a":"code","f8cea1d3":"code","4fb49a55":"code","770f3b8f":"code","3fc2f845":"code","8089cfd1":"code","e676acf0":"code","9328a4e9":"code","4e361a63":"code","3d599027":"code","e7c1faa4":"code","b9273085":"code","fbeeb51a":"code","19bb7bbc":"code","f39ba44e":"code","78718f35":"code","f1df9ab1":"code","21ed069e":"code","bb7cdacb":"code","85bcf29f":"markdown","2ce6e4f6":"markdown","8ae51e77":"markdown","fbc1d8a1":"markdown","0f3e7520":"markdown","e71847aa":"markdown","079df186":"markdown","49047d7b":"markdown","cff16e39":"markdown","2d61dfee":"markdown","029ca18f":"markdown","4888656f":"markdown","44363847":"markdown","265c08ed":"markdown","2ebce0cd":"markdown","2767e0f4":"markdown","88dec043":"markdown","23964697":"markdown"},"source":{"8c3c2dff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","373c29a1":"df=pd.read_csv('..\/input\/diamonds\/diamonds.csv')","84c976f4":"df.head()","23649f33":"#column name Unnamed:0 is not a valid columns. so, we will drop the column name Unnamed:0\ndf.drop('Unnamed: 0', axis=1, inplace=True)\ndf=df[df['x']!=0]\ndf=df[df['y']!=0]\ndf=df[df['z']!=0]","3e9faa84":"df.info()","254a5c51":"df.isnull().sum()","89324ad4":"df.describe()","77bd084e":"import matplotlib.pyplot as plt\nimport seaborn as sns\ncolors=['#003f5c','#2f4b7c','#665191','#a05195','#d45087','#f95d6a','#ff7c43','#ffa600']\nsns.set(palette=colors, font='San', style='white', rc={'axes.facecolor':'whitesmoke', 'figure.facecolor':'whitesmoke'})\nsns.despine(left=False, right=False)\nsns.palplot(colors)\nplt.title(\"Theme for EDA\", family='Sherif', size=15, weight=50)","8590ad9a":"int_cols = df.select_dtypes(exclude='object').columns.to_list()\n#print(int_cols)\nint_cols.remove('price')\nj=0\nfig=plt.figure(figsize=(15,10), constrained_layout =True)\nplt.suptitle(\"Regression of the Numeric variables\", family='Sherif', size=20, weight='bold')\nfor i in int_cols:\n    ax=plt.subplot(331+j)\n    #ax.set_title('Title')\n    #print(df[i])\n    ax=sns.regplot(data=df, x=i, y='price', color=colors[1], line_kws={'color':'#ffa600'})\n    ax.set_title(\"Price and {} comparision analysis\".format(i), family='Sherif')\n    for s in ['left','right','top','bottom']:\n        ax.spines[s].set_visible(False)\n    \n    j=j+1\n\n    ","f8cea1d3":"# let us find the distribution of integer variables\nint_cols = df.select_dtypes(exclude='object').columns.to_list()\nj=0\nfig=plt.figure(figsize=(15,10), constrained_layout =True)\nplt.suptitle(\"Distribution of the Numeric variables\", family='Sherif', size=20, weight='bold')\nfor i in int_cols:\n    ax=plt.subplot(331+j)\n    #ax.set_title('Title')\n    #print(df[i])\n    ax=sns.kdeplot(data=df, x=i, color=colors[0], fill=True, edgecolor=colors[-1], alpha=1)\n    ax.set_title(\"Distribution of Numeric variables - {}\".format(i), family='Sherif')\n    for s in ['left','right','top','bottom']:\n        ax.spines[s].set_visible(False)\n    \n    j=j+1\n","4fb49a55":"j=0\nfig=plt.figure(figsize=(15,10))\nplt.suptitle(\"Box plot for Numeric variables\", family='Sherif', size=20, weight='bold')\nfor i in int_cols:\n    ax=plt.subplot(331+j)\n    #ax.set_title('Title')\n    #print(df[i])\n    ax=sns.boxplot(data=df, x=i,color=colors[0])\n    ax.set_title(\"Box plot for {}\".format(i))\n    for s in ['left','right','top','bottom']:\n        ax.spines[s].set_visible(False)\n    j=j+1\nax=plt.subplot(331+j)\nax.text(x=0,y=0.5, s='Obviously there are outliers in the data, we need to examine the outliers to verify if it is extreme value or data error')\nfor s in ['left','right','top','bottom']:\n        ax.spines[s].set_visible(False)","770f3b8f":"#Correlation with Price column\nfig=plt.figure(figsize=(15,8))\nsns.heatmap(df.corr(), linewidths=3, annot=True)\nplt.title(\"Correlation matrics\", family='Sherif', size=20, weight='bold')","3fc2f845":"fig=plt.figure(figsize=(15,10), constrained_layout=True)\n# let us find the target variable relationship with Categorical variables\nplt.suptitle(\"Categorical feature comparison with Price\", family='Sherif', size=20, weight='bold')\ncat_cols = df.select_dtypes(include='object').columns.to_list()\nax=fig.subplot_mosaic(\"\"\"\n                        AAB\n                        AAC\n                        AAD\n                        \"\"\")\nsns.kdeplot(df['price'], fill=True, edgecolor=colors[-1], linewidth=2, color=colors[0], ax=ax['A'], alpha=0.8)\nax['A'].text(x=2000,y=0.00025, s=\"Target Feature Price is not normally distributed\", family='San', fontweight='bold')\nax['A'].text(x=2000,y=0.00023, s=\"Comparing Price with Categorical feature we can see the Median is more or less same\",family='San', fontweight='bold')\nsns.boxplot(data=df, x=cat_cols[0],y='price', ax=ax['B'])\nsns.boxplot(data=df, x=cat_cols[1],y='price', ax=ax['C'])\nsns.boxplot(data=df, x=cat_cols[2],y='price', ax=ax['D'])\nfor i in 'ABCD':\n    for s in ['left','right','top','bottom']:\n        ax[i].spines[s].set_visible(False)","8089cfd1":"cat_cols=df.select_dtypes(include='object').columns.to_list()\n\nfig=plt.figure(figsize=(15,5))\nplt.suptitle(\"Distribution of Categorical variable\",family='Sherif', size=20, weight='bold')\nax1=plt.subplot(131)\nsns.countplot(data=df, x=cat_cols[0], ax=ax1, linewidth=2, edgecolor=colors[-1])\nfor s in ['left','right','top','bottom']:\n        ax1.spines[s].set_visible(False)\nax2=plt.subplot(132, sharey=ax1)\nsns.countplot(data=df, x=cat_cols[1], ax=ax2,linewidth=2, edgecolor=colors[-1])\nfor s in ['left','right','top','bottom']:\n        ax2.spines[s].set_visible(False)\nax3=plt.subplot(133, sharey=ax1)\nsns.countplot(data=df, x=cat_cols[2], ax=ax3,linewidth=2, edgecolor=colors[-1])\nfor s in ['left','right','top','bottom']:\n        ax3.spines[s].set_visible(False)","e676acf0":"import statsmodels.api as stats\nfrom statsmodels.stats.anova import anova_lm\nfrom   statsmodels.formula.api import ols","9328a4e9":"formula='price ~ C(clarity)'\nmodel=ols(formula, df).fit()\nprint(np.round(anova_lm(model, typ=2),3))\nprint(model.summary())\nif np.round(model.f_pvalue,2)<0.05:\n    print(\"Reject Null Hypothesis and accept the alternate hypothesis\")\nelse:\n    print(\"Accept the Null Hypothesis\")","4e361a63":"formula='price ~ C(color)'\nmodel=ols(formula, df).fit()\nprint(np.round(anova_lm(model, typ=2),3))\nprint(model.summary())\nif np.round(model.f_pvalue,2)<0.05:\n    print(\"Reject Null Hypothesis and accept the alternate hypothesis\")\nelse:\n    print(\"Accept the Null Hypothesis\")","3d599027":"formula='price ~ C(cut)'\nmodel=ols(formula, df).fit()\nprint(np.round(anova_lm(model, typ=2),3))\nprint(model.summary())\nif np.round(model.f_pvalue,2)<0.05:\n    print(\"Reject Null Hypothesis and accept the alternate hypothesis\")\nelse:\n    print(\"Accept the Null Hypothesis\")","e7c1faa4":"formula='price ~ C(cut)+C(color)+C(clarity)'\nmodel=ols(formula, df).fit()\nprint(np.round(anova_lm(model, typ=2),3))\nprint(model.summary())\nif np.round(model.f_pvalue,2)<0.05:\n    print(\"Reject Null Hypothesis and accept the alternate hypothesis\")\nelse:\n    print(\"Accept the Null Hypothesis\")","b9273085":"import scipy.stats as st\nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR=Q3-Q1\ndf_clean=df[~((df<(Q1-1.5*IQR))|(df>(Q3+1.5*IQR))).any(axis=1)]","fbeeb51a":"int_cols=df_clean.select_dtypes(exclude='object').columns.to_list()\nj=0\nfig=plt.figure(figsize=(15,10))\nplt.suptitle(\"Box plot for Numeric variables after Outlier removal\", family='Sherif', size=20, weight='bold')\nfor i in int_cols:\n    ax=plt.subplot(331+j)\n    #ax.set_title('Title')\n    #print(df[i])\n    ax=sns.boxplot(data=df_clean, x=i,color=colors[0])\n    ax.set_title(\"Box plot for {}\".format(i))\n    for s in ['left','right','top','bottom']:\n        ax.spines[s].set_visible(False)\n    j=j+1\nax=plt.subplot(331+j)\nax.text(x=0,y=0.5, s='Outliers are handled with IQR method')\nfor s in ['left','right','top','bottom']:\n        ax.spines[s].set_visible(False)","19bb7bbc":"df1=pd.get_dummies(df_clean, columns=cat_cols, drop_first=True)","f39ba44e":"df1.head()","78718f35":"X=df1.drop('price', axis=1)\ny=df1['price']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)","f1df9ab1":"from sklearn.preprocessing import StandardScaler, Normalizer, PolynomialFeatures\n#creating Polynomial features as there is some degree of variation in the linear relationship\nscaler = PolynomialFeatures(degree=2, interaction_only=True)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","21ed069e":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\npred=model.predict(X_test)\nprint()\nprint()\n","bb7cdacb":"fig=plt.figure(figsize=(15,8))\nresidual = y_test - pred\nplt.suptitle(\"Comparing y_test and Predicted value\", family='Sherif', size=20, weight='bold')\nax=fig.subplot_mosaic(\"\"\"AA\n                          BB\n                          CC\"\"\")\nsns.scatterplot(y_test, residual, ax=ax['A'])\nax['A'].axhline(y=0, ls='--', c=colors[-1], linewidth=3)\nsns.kdeplot(residual, ax=ax['B'], fill=True, color=colors[0], edgecolor=colors[-1], linewidth=2)\n\nfrom sklearn.metrics import mean_squared_error\nax['C'].text(x=0.2,y=0.2,s=\"Root squared mean error: {}\".format(np.round(mean_squared_error(y_test, pred, squared=False),2)), ha='left',family='cursive' ,weight='bold', size=15, style='italic')\nax['C'].text(x=0.2,y=0.4,s=\"Accuracy of model with Train data: {}\".format(np.round(model.score(X_train, y_train),2)), ha='left',family='cursive' ,weight='bold', size=15, style='italic')\nax['C'].text(x=0.2,y=0.6,s=\"Accuracy of model with Test data: {}\".format(np.round(model.score(X_test, y_test),2)), ha='left',family='cursive' ,weight='bold', size=15, style='italic')\nax['C'].text(x=0.2,y=0.8,s=\"Result:\", ha='left',family='cursive' ,weight='bold', size=15, style='italic')\n\nax['C'].axis('off')\n\nfor i in 'ABC':\n    for s in ['left','right','top','bottom']:\n        ax[i].spines[s].set_visible(False)","85bcf29f":"Obviously there are outliers in the data, we need to examine the outliers to verify if it is extreme value or data error","2ce6e4f6":"# Train test split","8ae51e77":"# Conclusion\nModel can predict the Price of the diamond with 98% accuracy. and with the Root Square mean error of 462.62\n\n**Please review and provide your inputs <br>\nBest wishes**","fbc1d8a1":"# Statistical Analysis","0f3e7520":"Price feature is not normally distribured","e71847aa":"# Model Creation","079df186":"# Outlier handling","49047d7b":"# One hot encoding for Categorical variables","cff16e39":"# Read Dataset","2d61dfee":"# Standardization","029ca18f":"# Exploratory Data Analysis","4888656f":"# Feature Details\n1. price price in US dollars (\\$326--\\$18,823)\n\n2. carat weight of the diamond (0.2--5.01)\n\n3. cut quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n\n4. color diamond colour, from J (worst) to D (best)\n\n5. clarity a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n\n6. x length in mm (0--10.74)\n\n7. y width in mm (0--58.9)\n\n8. z depth in mm (0--31.8)\n\n9. depth total depth percentage = z \/ mean(x, y) = 2 * z \/ (x + y) (43--79)\n\n10. table width of top of diamond relative to widest point (43--95)","44363847":"# Hypothesis Testing\n**comparing Price value with Categorical feature and check if the mean has significant difference**\n1. H0 = there is no significant difference \n2. H1 = there are significant difference","265c08ed":"**Above chart shows the linear relationship with the Target variable, however, there are outliers**","2ebce0cd":"**Target variable is \"price\"** so let us check the relationship with price with other variables.","2767e0f4":"# Conclusion on Hypothesis testing\nPrice value has significant ***(CI=95%)*** impact on the Cut, Clarity & Color of the Dimond","88dec043":"# Data Types","23964697":"## Dimond Rate Prediction\n![](https:\/\/news.mit.edu\/sites\/default\/files\/styles\/news_article__image_gallery\/public\/images\/202010\/MIT-Metallic-Diamond-01-Press_0.jpg?itok=386hZmMI)"}}