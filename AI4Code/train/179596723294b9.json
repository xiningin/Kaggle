{"cell_type":{"4fdccbce":"code","7826f142":"code","9bca52e8":"code","22a98a3d":"code","5662b49d":"code","de2fb8ab":"code","74a3998d":"code","a522d210":"code","33ae29ce":"code","bedc7a54":"code","f49acceb":"code","0f922552":"code","3ac717ed":"code","5a7df4de":"code","75b5133f":"code","d9b9baed":"code","964928cd":"code","206da3ae":"code","d28ad8c9":"code","e151b7e5":"code","67610996":"code","5f5891f9":"code","15a4af58":"code","fde7124e":"code","3d51e744":"code","e48aa99d":"code","d696ef5d":"code","d2912e56":"code","d1ca8e7d":"code","ed1ae5ce":"code","f0980180":"code","ecf770e8":"code","fa126baa":"code","28697d35":"code","5309f253":"code","a189c139":"code","025c7eca":"code","3a0b6873":"code","2e164f1f":"code","bf0f27bb":"code","efda1545":"code","63fbc69f":"code","c41b8599":"code","76234db8":"code","2046a988":"code","1e03fffc":"code","7750e407":"code","8a1bf97b":"code","00f9bc9f":"code","0ab05633":"code","11078f45":"code","ee15bf7a":"code","7f133f19":"code","c4a9e356":"code","2eb0c281":"code","245ff298":"code","b9c7009f":"code","0cdd0f8b":"code","7089cba5":"markdown","a4d5cd1f":"markdown","75081141":"markdown","7b81af49":"markdown","5150b961":"markdown","516cbc47":"markdown","fe733668":"markdown","c0dd33ff":"markdown","2f8b23e1":"markdown","bbe3b45c":"markdown","3564bc9b":"markdown","527388b3":"markdown","d1ef52f8":"markdown","208fd662":"markdown","dc015a2c":"markdown","35f5e8ef":"markdown"},"source":{"4fdccbce":"import warnings\nwarnings.simplefilter(\"ignore\")","7826f142":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nsns.set()\n%matplotlib inline \n# machine learning library import \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n# Model Evaluations\nfrom sklearn.model_selection import train_test_split , cross_val_score ,RandomizedSearchCV ,GridSearchCV\nfrom sklearn.metrics import confusion_matrix ,classification_report,recall_score,precision_score,f1_score","9bca52e8":"df = pd.read_csv('..\/input\/heartdisease\/heart-disease.csv')","22a98a3d":"df.shape  # rows and columns","5662b49d":"df.sample(8)","de2fb8ab":"df.target.value_counts() ","74a3998d":"df.target.value_counts().plot(kind='bar' ,figsize=(10,6),color=[\"red\",\"green\"])\nplt.xticks(rotation=0)\nplt.title('count how many people effected ')\nplt.xlabel(\"0 is not effected and 1 is effected\")\nplt.ylabel(\"Amount\")\nplt.show()","a522d210":"df.info()","33ae29ce":"sns.heatmap(df.isnull(),yticklabels=False,cbar=False)","bedc7a54":"df.isna().sum()","f49acceb":"df.sex.value_counts()","0f922552":"pd.crosstab(df.target,df.sex)","3ac717ed":"pd.crosstab(df.target,df.sex).plot(kind='bar',figsize=(10,6),color=['red','green'])\nplt.title(\"Heart Disease freqency according to gender\")\nplt.xlabel(\"0 is no disease and 1 is disease\")\nplt.ylabel(\"Amount\")\nplt.legend(['Female','Male'])\nplt.xticks(rotation = 0 )\nplt.show()","5a7df4de":"plt.figure(figsize=(10,6))\n\nplt.scatter(df.age[df.target == 1],df.thalach[df.target == 1], color='red')\nplt.scatter(df.age[df.target == 0],df.thalach[df.target == 0], color='green')\n\nplt.title(\"Age vs. Max Heart Rate for Heart Disease\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"max heart rate\")\nplt.legend(['Disease','No Disease'])\nplt.show()","75b5133f":"df.age.plot.hist()","d9b9baed":"pd.crosstab(df.cp,df.target)","964928cd":"pd.crosstab(df.cp,df.target).plot(kind=\"bar\",figsize=(10,6),color=['green','red'])\n\nplt.title(\"Heart Disease Frequency per Chest Pain Type\")\nplt.xlabel(\"chest pain type\")\nplt.ylabel(\"amount\")\nplt.xticks(rotation=0)\nplt.legend(['no Disease','Disease'])\nplt.show()","206da3ae":"df.corr()","d28ad8c9":"fig,ax = plt.subplots(figsize=(20,10))\nax = sns.heatmap(df.corr(),annot=True,linewidths=0.5,fmt='.2f',cmap='YlGnBu')\nbottom ,top = ax.get_ylim()\nax.set_ylim(bottom + 0.5 , top - 0.5)","e151b7e5":"x = df.drop('target',axis=1)\ny = df['target']\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\nX_train","67610996":"models = {\"Logistic Regression\": LogisticRegression(),\n          \"KNN\": KNeighborsClassifier(),\n          \"Random Forest\": RandomForestClassifier(),\n          \"SVM\":SVC(),\n           \"naive bayes\":GaussianNB(),\n            \"tree\":DecisionTreeClassifier()}","5f5891f9":"def fit_and_score(modeling,X_train,y_train,X_test,y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of differetn Scikit-Learn machine learning models\n    X_train : training data (no labels)\n    X_test : testing data (no labels)\n    y_train : training labels\n    y_test : test labels\n    \"\"\"\n    np.random.seed(42)\n    model_score = {}\n    for name,model in modeling.items():\n        model.fit(X_train,y_train)\n        model_score[name] = model.score(X_test,y_test)  \n    \n    \n    return model_score\n\n\nmodel_result = fit_and_score(modeling=models,X_train=X_train,y_train=y_train,X_test=X_test,y_test=y_test)\nmodel_result","15a4af58":"model_compare = pd.DataFrame(model_result,index=['Accuracy'])\nmodel_compare.T.plot.bar()\nplt.title(\"Logistic Regression And KNN And Random Forest\")\nplt.xlabel(\"Model Name\")\nplt.ylabel(\"Accuracy Number\")\nplt.show()","fde7124e":"train_score = []\ntest_score = []\n\nneigbhour = range(1,21)\n\nknn = KNeighborsClassifier()\n\nfor i in neigbhour:\n    knn.set_params(n_neighbors=i)\n    \n    knn.fit(X_train,y_train)\n    train_score.append(knn.score(X_train,y_train))\n    test_score.append(knn.score(X_test,y_test))","3d51e744":"train_score","e48aa99d":"test_score","d696ef5d":"plt.plot(neigbhour,train_score,label=\"Train Score\")\nplt.plot(neigbhour,test_score,label=\"Test Score\")\nplt.xticks(np.arange(1,21,1))\nplt.xlabel(\"Number of Neigbour\")\nplt.ylabel(\"Model Score\")\nplt.legend()\nplt.show()\nprint(\"test\",max(test_score))\nprint(\"train\",max(train_score))","d2912e56":"# Create a hyperparameter grid for LogisticRegression\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Create a hyperparameter grid for RandomForestClassifier\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}","d1ca8e7d":"np.random.seed(42)\n\n# Setup random hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n\n# Fit random hyperparameter search model for LogisticRegression\nrs_log_reg.fit(X_train, y_train)","ed1ae5ce":"rs_log_reg.best_params_","f0980180":"rs_log_reg.score(X_test, y_test)","ecf770e8":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(), \n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fit random hyperparameter search model for RandomForestClassifier()\nrs_rf.fit(X_train, y_train)","fa126baa":"rs_rf.best_params_","28697d35":"rs_rf.score(X_test, y_test)","5309f253":"\n# Different hyperparameters for our LogisticRegression model\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 30),\n                \"solver\": [\"liblinear\"]}\n\n# Setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid=log_reg_grid,\n                          cv=5,\n                          verbose=True)\n\n# Fit grid hyperparameter search model\ngs_log_reg.fit(X_train, y_train);","a189c139":"gs_log_reg.best_params_","025c7eca":"gs_log_reg.score(X_test, y_test)","3a0b6873":"y_preds = gs_log_reg.predict(X_test)","2e164f1f":"y_preds","bf0f27bb":"y_test","efda1545":"# Confusion matrix\nprint(confusion_matrix(y_test, y_preds))","63fbc69f":"\nsns.set(font_scale=1.5)\n\ndef plot_conf_mat(y_test, y_preds):\n    \"\"\"\n    Plots a nice looking confusion matrix using Seaborn's heatmap()\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True,\n                     cbar=False)\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predicted label\")\n    \n    bottom, top = ax.get_ylim()\n    ax.set_ylim(bottom + 0.5, top - 0.5)\n    \nplot_conf_mat(y_test, y_preds)","c41b8599":"print(classification_report(y_test, y_preds))","76234db8":"# Check best hyperparameters\ngs_log_reg.best_params_","2046a988":"# Create a new classifier with best parameters\nclf = LogisticRegression(C=0.20433597178569418,\n                         solver=\"liblinear\")","1e03fffc":"\n# Cross-validated accuracy\ncv_acc = cross_val_score(clf,\n                         x,\n                         y,\n                         cv=5,\n                         scoring=\"accuracy\")\ncv_acc","7750e407":"cv_acc = np.mean(cv_acc)\ncv_acc","8a1bf97b":"# Cross-validated precision\ncv_precision = cross_val_score(clf,\n                         x,\n                         y,\n                         cv=5,\n                         scoring=\"precision\")\ncv_precision=np.mean(cv_precision)\ncv_precision","00f9bc9f":"\n# Cross-validated recall\ncv_recall = cross_val_score(clf,\n                         x,\n                         y,\n                         cv=5,\n                         scoring=\"recall\")\ncv_recall = np.mean(cv_recall)\ncv_recall","0ab05633":"\n# Cross-validated f1-score\ncv_f1 = cross_val_score(clf,\n                         x,\n                         y,\n                         cv=5,\n                         scoring=\"f1\")\ncv_f1 = np.mean(cv_f1)\ncv_f1","11078f45":"# Visualize cross-validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                           \"Precision\": cv_precision,\n                           \"Recall\": cv_recall,\n                           \"F1\": cv_f1},\n                          index=[0])\n\ncv_metrics.T.plot.bar(title=\"Cross-validated classification metrics\",\n                      legend=False);","ee15bf7a":"\n# Fit an instance of LogisticRegression\nclf = LogisticRegression(C=0.20433597178569418,\n                         solver=\"liblinear\")\n\nclf.fit(X_train, y_train);","7f133f19":"# Check coef_\nclf.coef_","c4a9e356":"df.head()\n","2eb0c281":"\n# Match coef's of features to columns\nfeature_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict","245ff298":"# Visualize feature importance\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title=\"Feature Importance\", legend=False);","b9c7009f":"pd.crosstab(df[\"sex\"], df[\"target\"])","0cdd0f8b":"pd.crosstab(df[\"slope\"], df[\"target\"])","7089cba5":"### Heart Disease Frequency per Chest Pain Type\n1 cp - chest pain type\n * 0- Typical angina: chest pain related decrease blood supply to the heart\n * 1 - Atypical angina: chest pain not related to heart\n * 2- Non-anginal pain: typically esophageal spasms (non heart related)\n * 3- Asymptomatic: chest pain not showing signs of disease","a4d5cd1f":"# Feature Importance","75081141":"# Hyperparameter tuning with RandomizedSearchCV","7b81af49":"# Age vs. Max Heart Rate for Heart Disease","5150b961":"# target values counts and graph","516cbc47":"# Load data ","fe733668":"# Hyperparameter tuning (by hand)","c0dd33ff":"# Describe Data ","2f8b23e1":"# Modeling ","bbe3b45c":"# Hyperparamter Tuning with GridSearchCV","3564bc9b":"### Heart Disease Data Dictionary\n\nA data dictionary describes the data you're dealing with. Not all datasets come with them so this is where you may have to do your research or ask a **subject matter expert** (someone who knows about the data) for more.\n\nThe following are the features we'll use to predict our target variable (heart disease or no heart disease).\n\n1. age - age in years \n2. sex - (1 = male; 0 = female) \n3. cp - chest pain type \n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n    * anything above 130-140 is typically cause for concern\n5. chol - serum cholestoral in mg\/dl \n    * serum = LDL + HDL + .2 * triglycerides\n    * above 200 is cause for concern\n6. fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false) \n    * '>126' mg\/dL signals diabetes\n7. restecg - resting electrocardiographic results\n    * 0: Nothing to note\n    * 1: ST-T Wave abnormality\n        - can range from mild symptoms to severe problems\n        - signals non-normal heart beat\n    * 2: Possible or definite left ventricular hypertrophy\n        - Enlarged heart's main pumping chamber\n8. thalach - maximum heart rate achieved \n9. exang - exercise induced angina (1 = yes; 0 = no) \n10. oldpeak - ST depression induced by exercise relative to rest \n    * looks at stress of heart during excercise\n    * unhealthy heart will stress more\n11. slope - the slope of the peak exercise ST segment\n    * 0: Upsloping: better heart rate with excercise (uncommon)\n    * 1: Flatsloping: minimal change (typical healthy heart)\n    * 2: Downslopins: signs of unhealthy heart\n12. ca - number of major vessels (0-3) colored by flourosopy \n    * colored vessel means the doctor can see the blood passing through\n    * the more blood movement the better (no clots)\n13. thal - thalium stress result\n    * 1,3: normal\n    * 6: fixed defect: used to be defect but ok now\n    * 7: reversable defect: no proper blood movement when excercising \n14. target - have disease or not (1=yes, 0=no) (= the predicted attribute)\n\n**Note:** No personal identifiable information (PPI) can be found in the dataset.\n\nIt's a good idea to save these to a Python dictionary or in an external file, so we can look at them later without coming back here.","527388b3":"### We're going to try 3 different machine learning models:\n\n* 1 Logistic Regression\n* 2 K-Nearest Neighbours Classifier\n* 3 Random Forest Classifier","d1ef52f8":"# Calculate evaluation metrics using cross-validation","208fd662":"# dataframe information and null values checked ","dc015a2c":"# import libraries","35f5e8ef":"# Data Exploration "}}