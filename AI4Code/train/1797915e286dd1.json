{"cell_type":{"0a9ed2b8":"code","55c918f4":"code","c2ec77ae":"code","dc047f40":"code","2e8e3fc6":"code","3a472d18":"code","febc08e4":"code","0433ac9b":"code","b26dcaf0":"code","259bf932":"code","1a662974":"code","2788a448":"code","1bb98969":"code","77225fdb":"code","5123a072":"code","8d3e276d":"code","34c70e0f":"markdown","e736e640":"markdown","35ed500e":"markdown","fd15cec7":"markdown","273163a3":"markdown","9d7b2d6f":"markdown","12b368f3":"markdown","11367ebe":"markdown","b06431d8":"markdown","6bd11f96":"markdown","2a4508b2":"markdown"},"source":{"0a9ed2b8":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd \nfrom sklearn import preprocessing\nimport seaborn as sns\nfrom sklearn.metrics import classification_report \nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nmaster=pd.read_csv(\"..\/input\/meteorological-model-versus-real-data\/vigo_model_vs_real.csv\",index_col=\"datetime\",parse_dates=True)\nmaster.columns","55c918f4":"import plotly.express as px\ndf = pd.DataFrame()\ndf[\"direction\"]=master.dir_o[master.visibility_o<1000]\ndf[\"strength\"]=master.mod_o[master.visibility_o<1000]\ndf['freq'] = df.groupby('direction')['direction'].transform('count')\n\nfig = px.scatter_polar(df,r= \"freq\",theta=\"direction\")\nfig","c2ec77ae":"import plotly.express as px\ndf = px.data.wind()\nfig = px.scatter_polar(df, r=\"frequency\", theta=\"direction\",\n                       color=\"strength\", symbol=\"strength\", size=\"frequency\",)\n                       \nfig","dc047f40":"df = px.data.wind()\nfig = px.line_polar(df, r=\"frequency\", theta=\"direction\", color=\"strength\", line_close=True,\n                                  template=\"plotly_dark\",)\nfig.show()","2e8e3fc6":"labels=[\"NE\",\"SE\",\"SW\",\"NW\"]\nmaster[\"dir_o_l\"]=pd.cut(master.dir_o[master.dir_o!=-1], len(labels),labels=labels)\nmaster[\"dir_4K_l\"]=pd.cut(master.dir_4K[master.dir_o!=-1], len(labels),labels=labels)\nmaster[\"dir_36K_l\"]=pd.cut(master.dir_36K[master.dir_o!=-1], len(labels),labels=labels)\n\ntable_4K=pd.crosstab(master.dir_o_l, master.dir_4K_l, margins=True,)\ntable_36K=pd.crosstab(master.dir_o_l, master.dir_36K_l, margins=True,)\ntable_two_model=pd.crosstab(master.dir_4K_l, master.dir_36K_l, margins=True,)\n\n\nfig, axs = plt.subplots(3,figsize = (15,15))\nsns.heatmap(table_4K,annot=True,cmap=\"YlGnBu\",ax=axs[0],fmt='.0f',linewidths=5)\nsns.heatmap(table_36K,annot=True,cmap=\"YlGnBu\",ax=axs[1],fmt='.0f',linewidths=5)\nsns.heatmap(table_two_model,annot=True,cmap=\"YlGnBu\",ax=axs[2],fmt='.0f',linewidths=5)\n\n\nfig, bxs = plt.subplots(3,figsize = (15,15))\nmaster[\"dir_o_l\"].value_counts(normalize=True).plot.pie(autopct='%1.0f%%',ax=bxs[0])\nmaster[\"dir_4K_l\"].value_counts(normalize=True).plot.pie(autopct='%1.0f%%',ax=bxs[1])\nmaster[\"dir_36K_l\"].value_counts(normalize=True).plot.pie(autopct='%1.0f%%',ax=bxs[2])","3a472d18":"df4K=pd.concat([master[\"dir_o_l\"],master[\"dir_4K_l\"]],axis=1).dropna()\ndf36K=pd.concat([master[\"dir_o_l\"],master[\"dir_36K_l\"]],axis=1).dropna()\nprint(\"Wind direction model 4 Km \")\nprint(classification_report(df4K.dir_o_l, df4K.dir_4K_l, labels=labels,)) \nprint(\"Wind direction model 36 Km \")\nprint(classification_report(df36K.dir_o_l, df36K.dir_36K_l, labels=labels,)) ","febc08e4":"#creating dynamic variables\nmaster[\"mslp_36K_V3H\"]=master[\"mod_36K\"].pct_change(freq=\"3H\")\nmaster['HGT500_4K-V3H']=master[\"HGT500_4K\"].pct_change(freq=\"3H\")\n#X and Y definition\nX=master[[ \"dir_4K\", \"dir_36K\", \"mod_4K\", \"mod_36K\", \"wind_gust_4K\",\"wind_gust_36K\",\"mslp_36K_V3H\",'HGT500_4K-V3H',]]\ndf=pd.concat([X,master[\"dir_o_l\"]],axis=1).dropna()\nX=StandardScaler().fit_transform(df[df.columns[:-1]])\nlb = preprocessing.LabelBinarizer()\nlb.fit(df.dir_o_l)\nY=lb.transform(df.dir_o_l)\n","0433ac9b":"#neural network\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix ,classification_report \nfrom sklearn.model_selection import cross_val_score,cross_validate\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model, load_model, Sequential\nfrom tensorflow.keras.layers import Input, Dense, Dropout, AlphaDropout\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n\n\nx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.2,)\nmlp = Sequential()\nmlp.add(Input(shape=(x_train.shape[1], )))\nmlp.add(Dense(24, activation='relu'))\n#mlp.add(Dropout(0.5))\nmlp.add(Dense(24, activation='relu'))\n#mlp.add(Dropout(0.5))\nmlp.add(Dense(len(labels), activation='softmax'))\nmlp.summary()\nmlp.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),\n            loss='categorical_crossentropy',\n            metrics=['accuracy',]\n           )\n\nhistory = mlp.fit(x=x_train,\n                  y=y_train,\n                  batch_size=48,\n                  epochs=100,\n                  validation_data=(x_test, y_test),\n                  verbose=0).history\npd.DataFrame(history).plot(grid=True,figsize=(12,12),yticks=np.linspace(0.0, 1.0, num=11))\ny_pred=mlp.predict(x_test)\n","b26dcaf0":"y_pred_l=lb.inverse_transform(y_pred)\ny_test_l=lb.inverse_transform(y_test)\nprint(classification_report(y_test_l, y_pred_l, )) \nresult=pd.crosstab(y_test_l, y_pred_l, margins=True,)\nfig = plt.figure(figsize = (18,18)) # width x height\nax1 = fig.add_subplot(3, 3, 1) \ng=sns.heatmap(result,annot=True,cmap=\"YlGnBu\",ax=ax1,fmt='.0f',linewidths=4)","259bf932":"#variable winds detector\nvar=master[[\"dir_o\", \"dir_4K\", \"dir_36K\", \"mod_4K\", \"mod_36K\", \"wind_gust_4K\",\"wind_gust_36K\",]].dropna()\nX=var[[\"dir_4K\", \"dir_36K\", \"mod_4K\", \"mod_36K\", \"wind_gust_4K\",\"wind_gust_36K\",]]\nY=pd.DataFrame({\"datetime\":var.index,\n                     \"var_o\":[\"var\" if c<=-1 else \"no_var\" for c in var.dir_o]}).set_index(\"datetime\")","1a662974":"#decission tree\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(max_depth=3,criterion=\"entropy\") #max_depth is maximum number of levels in the tree\nclf.fit(X, Y)","2788a448":"from sklearn import tree\nimport graphviz\ndot_data = tree.export_graphviz(clf, out_file=None, \n                     feature_names=var.columns[1:],  \n                     class_names=[\"no_var\",\"var\"],  \n                     filled=True, rounded=True,  \n                     special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph ","1bb98969":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix ,classification_report \nfrom sklearn.model_selection import cross_val_score,cross_validate\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nimport graphviz\n\nvar_wind=master[[\"dir_o_l\", \"dir_4K\", \"dir_36K\", \"mod_4K\", \"mod_36K\", \"wind_gust_4K\",\"wind_gust_36K\",\"mslp_36K_V3H\",'HGT500_4K-V3H',]].dropna()\nY=var_wind[\"dir_o_l\"]\nX=var_wind[[\"dir_4K\", \"dir_36K\", \"mod_4K\", \"mod_36K\", \"wind_gust_4K\",\"wind_gust_36K\",\"mslp_36K_V3H\",'HGT500_4K-V3H',]]\n\n#we do not scale!!\nx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.2,)\nclf = DecisionTreeClassifier(max_depth=10,criterion=\"gini\").fit(x_train,y_train) \ny_pred=clf.predict(x_test)\n#plot results\nprint(classification_report(y_test.values,y_pred))\nprint(\"**** Confusion matrix ****\")\nprint(confusion_matrix(y_test,y_pred))\n\n#plot tree\ndot_data = tree.export_graphviz(clf, out_file=None, \n                     feature_names=var_wind.columns[1:],  \n                     class_names=[\"NE\",\"NW\",\"SE\",\"SW\"],  \n                     filled=True, rounded=True,  \n                     special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph ","77225fdb":"\nfrom sklearn.model_selection import cross_validate\nresults=cross_validate(clf, X, Y, cv=10,scoring=\"accuracy\")\nprint(\"Mean:\",\"{0:.3f}\".format(results['test_score'].mean()),\",Standard deviation:\",\"{0:.3f}\".format(results['test_score'].std()))","5123a072":"from sklearn.ensemble import RandomForestClassifier\nvar_wind=master[[\"dir_o_l\", \"dir_4K\", \"dir_36K\", \"mod_4K\", \"mod_36K\", \"wind_gust_4K\",\"wind_gust_36K\",\"mslp_36K_V3H\",'HGT500_4K-V3H',]].dropna()\nY=var_wind[\"dir_o_l\"]\nX=var_wind[[\"dir_4K\", \"dir_36K\", \"mod_4K\", \"mod_36K\", \"wind_gust_4K\",\"wind_gust_36K\",\"mslp_36K_V3H\",'HGT500_4K-V3H',]]\n\n#we do not scale!!\nx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.2,)\nclf =RandomForestClassifier (n_estimators=1500).fit(x_train,y_train) \ny_pred=clf.predict(x_test)\n#plot results\nprint(classification_report(y_test.values,y_pred))\nprint(\"**** Confusion matrix ****\")\nprint(confusion_matrix(y_test,y_pred))\n","8d3e276d":"from sklearn.model_selection import cross_validate\nresults=cross_validate(clf, X, Y, cv=10,scoring=\"accuracy\")\nprint(\"Mean:\",\"{0:.3f}\".format(results['test_score'].mean()),\",Standard deviation:\",\"{0:.3f}\".format(results['test_score'].std()))\n","34c70e0f":"**We built a neural network with the specifications of the code below**","e736e640":"****","35ed500e":"**We aim to improve the performance of the meteorological models. This kernel will be focused on wind direction variable. The wind direction name is dir_o for the actual data. dir_4K and dir_36K will be the predicted variable by the model (4K and 36K mean the model resolution in kilometers). We start loading the dataset at the variable master and displaying columns**","fd15cec7":"**Random forest**","273163a3":"**We select independent variables related with the wind in both models (\"dir_4K\", \"dir_36K\", \"mod_4K\", \"mod_36K\", \"wind_gust_4K\",\"wind_gust_36K\"). We filter all the cases with the variable real wind. Then, we normalize them. Lastly, we label as an array the four wind directions of the dependent variable dir_o**","9d7b2d6f":"**We evaluate the performance of the two meteorological models applying the same techniques to evaluate  machine learning algorithms. The model\u00b4s accuracies are 0.49 and 0.53,respectively. It seems a bad result. Perhaps the model with less resolution performs better **","12b368f3":"**Decision tree applied in wind direction**","11367ebe":"**We built a decision tree function to predict variable winds (without any direction defined). These winds are coded with -1 in dir_o variable**","b06431d8":"**Now, we split the wind direction variable into four labels: \"NE\", \"SE\", \"SW\", \"NW\". We avoid winds observed without direction defined, named \"variable winds\" coded at the variable dir_o as -1. We plot three crosstabulations (or confusion matrix). First, compare the real direction observed and direction predicted by the model of 4 Km. Second, the real direction and direction predicted by the model of 36 Km. The third table compares two models.\nFinally, we plot a plot pie with the actual direction and the model forecast (4Km and 36Km) \n**","6bd11f96":"**Cross validation**","2a4508b2":"**We evaluate the results obtained from the neural network trained. Accuracy 0.70 seems better than the model accuracies 0.49 and 0.53, respectively. Probably neural network can defeat the raw variable from the meteorological model **"}}