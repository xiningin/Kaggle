{"cell_type":{"dc32d0c8":"code","0cc53bff":"code","d1bbf429":"code","3e618c7e":"code","52f66a9c":"code","6e6b3f61":"code","4ee03df5":"code","408a32eb":"code","d9272769":"code","3fa6fde9":"code","171a24b6":"code","d88fc145":"code","360b5227":"code","9d5d86e5":"code","c90495f8":"code","48c77f8a":"code","4a164ed2":"code","8077be78":"code","c6725247":"code","d6cdaf2d":"code","d439ae23":"code","027eba18":"code","ca93cc51":"code","247e59fc":"code","39f940ea":"code","29ca8c33":"code","bf299f2f":"code","fd968229":"code","b411aa30":"code","e0918d95":"code","deaedcab":"code","1aa9b830":"code","9f3e621b":"code","187c6edf":"code","95b9e35c":"code","1011bc72":"code","76567ffd":"code","eb6d3d8b":"code","780575c9":"code","1a54ba8d":"code","1fc7d7fe":"code","9911848f":"code","8a6d8f00":"code","41d16595":"code","310098ba":"code","411af366":"code","a4644447":"code","eac4eb30":"code","65ce46ce":"code","23b60a09":"code","1b18164f":"code","1857f5ca":"code","074912cf":"code","8687b25f":"code","98b41a23":"code","c21e58ac":"code","6082dfb9":"code","63928f12":"code","5ecb5962":"code","2d598edd":"code","465a5ed9":"code","c46791ad":"code","c14c98e5":"code","ace3cb40":"code","5c0769e5":"code","e655fa4a":"code","2b17c332":"code","586ea264":"code","27d8b173":"code","3e3d9d43":"code","a14e2ae2":"code","cc26f461":"code","f8a790e5":"code","8349f871":"code","915cf3d4":"code","31e86905":"code","e9f63eb7":"code","2b3e060f":"code","c068c51d":"code","b8944df3":"code","a7714274":"code","79ec9252":"code","03f436c0":"code","0c43d64c":"code","14b65050":"code","2a168875":"code","77d4cc23":"code","2dff1e57":"code","bdbc9ac3":"code","6bce9914":"code","e94fd053":"code","6aead915":"code","a6a4a92c":"code","c5dbd506":"code","8e503cae":"code","7ed06387":"code","83445c36":"code","4cff207d":"markdown","4ac767c1":"markdown","4ea19d1e":"markdown","0826ab65":"markdown","ce6d8c53":"markdown","955d7da0":"markdown","4fde12fa":"markdown","af8c963d":"markdown","24090b41":"markdown","f233bb2f":"markdown","abdb9ed4":"markdown","bf36224b":"markdown","54c7e749":"markdown","f0719c4d":"markdown","bacb4f56":"markdown","cb7ad08b":"markdown","1529298d":"markdown","9fc8d4ea":"markdown","e9fbca0a":"markdown","d5b36a8e":"markdown","1113fb40":"markdown","1a8aa263":"markdown","365e35e9":"markdown","40afc59b":"markdown","bb24067b":"markdown","6523d36a":"markdown","cdd81f4a":"markdown","1fe192f6":"markdown","1c5a5674":"markdown","74193080":"markdown","2cbe604b":"markdown","2e42935f":"markdown","61aec92b":"markdown","4b007323":"markdown","d1c84528":"markdown","fdbe33d5":"markdown","cc3714ea":"markdown","cff2228b":"markdown","7a72d0fd":"markdown","e51f4008":"markdown","eb2f3ae3":"markdown","ea7f4471":"markdown","58475564":"markdown","bb49c608":"markdown","8059721c":"markdown","9f06bc4e":"markdown","11944a8b":"markdown","59b04deb":"markdown","e73df740":"markdown","6b9a57d7":"markdown","f6f3bbb5":"markdown","98ff4431":"markdown","d1aa721c":"markdown","93835403":"markdown","41533b9a":"markdown","f86b4d0d":"markdown","650b897e":"markdown","b2e069e3":"markdown","939e5e61":"markdown","635f531e":"markdown","6aee9cc0":"markdown","6cc89d94":"markdown","3027cd72":"markdown","53cd6ec3":"markdown"},"source":{"dc32d0c8":"from IPython.display import Image\nImage(url= \"https:\/\/static1.squarespace.com\/static\/5006453fe4b09ef2252ba068\/5095eabce4b06cb305058603\/5095eabce4b02d37bef4c24c\/1352002236895\/100_anniversary_titanic_sinking_by_esai8mellows-d4xbme8.jpg?format=1000w\")","0cc53bff":"import pandas as pd\nimport numpy as np\nfrom pandas.plotting  import scatter_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection \nfrom sklearn.metrics import classification_report \nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom collections import Counter\nimport seaborn as sns\nimport warnings#ignore alertes\nwarnings.filterwarnings('ignore')","d1bbf429":"\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\n\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\n","3e618c7e":"\ntrain_df.head(10)","52f66a9c":"\ntest_df.head(10)","6e6b3f61":"\ntrain_df.describe()","4ee03df5":"train_df.info()","408a32eb":"test_df.info()","d9272769":"#Detection of possible correlations between class and other attributes\ng = sns.heatmap(train_df[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","3fa6fde9":"#Defined a function that represents the Sex, Pclass in barplot\ndef bar_chart(feature):\n    surv\u00e9cu = train_df[train_df['Survived']==1][feature].value_counts()\n    mort = train_df[train_df['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([surv\u00e9cu,mort])\n    df.index = ['Survived','dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))","171a24b6":"bar_chart('Sex')","d88fc145":"#Explanation\ntrain_df[[\"Sex\",\"Survived\"]].groupby('Sex').mean()","360b5227":"#display data by seaborn barplot\ng = sns.barplot(x=\"Sex\",y=\"Survived\",data=train_df)\ng = g.set_ylabel(\"Survival Probability\")","9d5d86e5":"bar_chart('Pclass')","c90495f8":"\ng = sns.factorplot(x=\"Pclass\",y=\"Survived\",data=train_df,kind=\"bar\", size = 6 , palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Survival Probability\")\n","48c77f8a":"#more details\ng = sns.factorplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train_df,size=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"probabilit\u00e9 de survie par class ticket et Sex\")","4a164ed2":"bar_chart('SibSp')","8077be78":"g  = sns.factorplot(x=\"SibSp\",y=\"Survived\",data=train_df,kind=\"bar\", size = 6 , palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Survival Probability\")","c6725247":"bar_chart('Parch')","d6cdaf2d":"g  = sns.factorplot(x=\"Parch\",y=\"Survived\",data=train_df,kind=\"bar\", size = 6 , palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Survival Probability\")","d439ae23":"g = sns.FacetGrid(train_df, col='Survived')\ng = g.map(sns.distplot, \"Age\")","027eba18":"# C = Cherbourg(France),Q = Queenstown(New-Zelande),S = Southampton(England)\nbar_chart('Embarked')","ca93cc51":"g = sns.factorplot(x=\"Embarked\", y=\"Survived\",  data=train_df,size=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Survival Probability\")","247e59fc":"g = sns.factorplot(\"Pclass\", col=\"Embarked\",  data=train_df,size=6, kind=\"count\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Count\")","39f940ea":"from IPython.display import Image\nImage(url= \"https:\/\/static1.squarespace.com\/static\/5006453fe4b09ef2252ba068\/t\/5090b249e4b047ba54dfd258\/1351660113175\/TItanic-Survival-Infographic.jpg?format=1500w\")","29ca8c33":"from collections import Counter\ndef detect_outliers(df,n,features):\n\n    outlier_indices = []\n    \n\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n\n        outlier_step = 1.5 * IQR\n\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n\n        outlier_indices.extend(outlier_list_col)\n\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\nOutliers_to_drop = detect_outliers(train_df,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])","bf299f2f":"train_df.loc[Outliers_to_drop] ","fd968229":"# I defined outliers as being above of 99% percentile here\n# get lists of people above 99% percentile for each feature\nimport pprint\nhighest = {}\nfor column in train_df.columns:\n    if train_df[column].dtypes != \"object\": # exclude string data typed columns\n        highest[column]=[]\n        q = train_df[column].quantile(0.99)\n        highest[column] = train_df[train_df[column] > q].index.tolist()\n    \npprint.pprint(highest)","b411aa30":"# delete 'PassengerId' from dictionary highest\nhighest.pop('PassengerId', 0)","e0918d95":"# summarize the previous dictionary, highest\n# create a dictionary of outliers and the frequency of being outlier\nhighest_count = {}\nfor feature in highest:\n    for person in highest[feature]:\n        if person not in highest_count:\n            highest_count[person] = 1\n        else:\n            highest_count[person] += 1\n             \nhighest_count","deaedcab":"# This time, I defined outliers as being below of 1% percentile here\n# get lists of people below 1% percentile for each feature\nlowest = {}\nfor column in train_df.columns:\n    if train_df[column].dtypes != \"object\": # exception string \n        lowest[column]=[]\n        q = train_df[column].quantile(0.01)\n        lowest[column] = train_df[train_df[column] < q].index.tolist()\n\n# supp 'PassengerId' \nlowest.pop('PassengerId', 0)\n\npprint.pprint(lowest)","1aa9b830":"for person in lowest['Age']:\n    if person not in highest_count:\n        highest_count[person] = 1\n    else:\n        highest_count[person] += 1\n \nhighest_count","9f3e621b":"# fare >99%\ntrain_df.loc[highest['Fare'],['Fare', 'Survived']]","187c6edf":"# age above 99% percentile\ntrain_df.loc[highest['Age'],['Age', 'Survived']]","95b9e35c":"# age below 1% percentile\ntrain_df.loc[lowest['Age'],['Age','Survived']]","1011bc72":"\ntrain_df.isnull().sum()","76567ffd":"\ntest_df.isnull().sum()","eb6d3d8b":"\ntest_df[\"Fare\"].isnull().sum()","780575c9":"\ntest_df[\"Fare\"] = test_df[\"Fare\"].fillna(test_df[\"Fare\"].median())","1a54ba8d":"\ntest_df[\"Fare\"].isnull().sum()","1fc7d7fe":"\ntrain_df[\"Embarked\"].isnull().sum()","9911848f":"\nPclass1 = train_df[train_df['Pclass']==1]['Embarked'].value_counts()\nPclass2 = train_df[train_df['Pclass']==2]['Embarked'].value_counts()\nPclass3 = train_df[train_df['Pclass']==3]['Embarked'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5));","8a6d8f00":"train_test_data = [train_df, test_df]\nfor dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')","41d16595":"train_df[\"Embarked\"].isnull().sum()","310098ba":"train_df.head()","411af366":"train_df[\"Cabin\"] = (train_df[\"Cabin\"].notnull().astype('int'))\ntest_df[\"Cabin\"] = (test_df[\"Cabin\"].notnull().astype('int'))\n\n\nprint(\"survival % in cabins  = 1 :\", train_df[\"Survived\"][train_df[\"Cabin\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"survival % in cabins  = 0 :\", train_df[\"Survived\"][train_df[\"Cabin\"] == 0].value_counts(normalize = True)[1]*100)\n\nsns.barplot(x=\"Cabin\", y=\"Survived\", data=train_df)\nplt.show()","a4644447":"\ntrain_df = train_df.drop(['Cabin'], axis = 1)\ntest_df = test_df.drop(['Cabin'], axis = 1)","eac4eb30":"#train\nprint('pourcentage of missing values in \"Age\" is %.2f%%' %((train_df['Age'].isnull().sum()\/train_df.shape[0])*100))\ntrain_df[\"Age\"].isnull().sum()","65ce46ce":"#test\nprint('pourc of missing values in \"Age\" is %.2f%%' %((test_df['Age'].isnull().sum()\/test_df.shape[0])*100))\ntest_df[\"Age\"].isnull().sum()","23b60a09":"g = sns.factorplot(y=\"Age\",x=\"Sex\",data=train_df,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"Sex\",hue=\"Pclass\", data=train_df,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"Parch\", data=train_df,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"SibSp\", data=train_df,kind=\"box\")","1b18164f":"\ng = sns.heatmap(train_df[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(),cmap=\"BrBG\",annot=True)","1857f5ca":"dataset =  train_df \n\n# NaN values\nindex_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = dataset[\"Age\"].median()#Median of age\n    age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        dataset['Age'].iloc[i] = age_pred\n    else :\n        dataset['Age'].iloc[i] = age_med","074912cf":"g = sns.factorplot(x=\"Survived\", y = \"Age\",data = train_df, kind=\"box\")","8687b25f":"\ntrain_df[\"Age\"].isnull().sum()","98b41a23":"\n\ndataset =  test_df \n\n# Nan values\nindex_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = dataset[\"Age\"].median()#Median of age\n    age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        dataset['Age'].iloc[i] = age_pred\n    else :\n        dataset['Age'].iloc[i] = age_med","c21e58ac":"test_df[\"Age\"].isnull().sum()","6082dfb9":"train_df.isnull().sum()","63928f12":"\ntest_df.isnull().sum()","5ecb5962":"train_df[\"Name\"].head()","2d598edd":"train_test_data = [train_df, test_df]\n\nfor dataset in train_test_data:\n    dataset['Titre'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)","465a5ed9":"\ntrain_df['Titre'].value_counts()","c46791ad":"train_len = len(train_df)\ndataset =  pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)\n\ng = sns.countplot(x=\"Titre\",data=dataset)\ng = plt.setp(g.get_xticklabels(), rotation=45) ","c14c98e5":"\ndataset[['Titre', 'Survived']].groupby(['Titre'], as_index=False).mean()","ace3cb40":"\nbar_chart('Titre')","5c0769e5":"#mapping list\nmapping_nom = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \"Master\": 3, \"Dr\": 3, \"Rev\": 3, \"Col\": 3, \"Major\": 3, \"Mlle\": 3,\"Countess\": 3,\n                 \"Ms\": 3, \"Lady\": 3, \"Jonkheer\": 3, \"Don\": 3, \"Dona\" : 3, \"Mme\": 3,\"Capt\": 3,\"Sir\": 3 }\n#application au dataset(train+test)\nfor dataset in train_test_data:\n    dataset['Titre'] = dataset['Titre'].map(mapping_nom)\n","e655fa4a":"train_df.head()","2b17c332":"\ntest_df.head()","586ea264":"# M:0 et F:1\nsexe_mapping = {\"male\": 0, \"female\": 1}\nfor dataset in train_test_data:\n    dataset['Sex'] = dataset['Sex'].map(sexe_mapping)","27d8b173":"bar_chart('Sex')","3e3d9d43":"\ntrain_df.drop('Name', axis=1, inplace=True)\ntest_df.drop('Name', axis=1, inplace=True)","a14e2ae2":"\nembarked_mapping = {\"S\": 0, \"C\": 1, \"Q\": 2}\nfor dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)","cc26f461":"train_df.head()","f8a790e5":"train_df[\"taillefamille\"] = train_df[\"SibSp\"] + train_df[\"Parch\"] + 1\ntest_df[\"taillefamille\"] = test_df[\"SibSp\"] + test_df[\"Parch\"] + 1","8349f871":"\ng = sns.factorplot(x=\"taillefamille\",y=\"Survived\",data = train_df)\ng = g.set_ylabels(\"Probabilit\u00e9 de survie\")","915cf3d4":"#solo= alone \/ pfamille=smallfamily \/ Mfamille=Medium \/ Gfamille=Bigfamily\ntrain_df['Solo'] = train_df['taillefamille'].map(lambda s: 1 if s == 1 else 0)\ntrain_df['Pfamille'] = train_df['taillefamille'].map(lambda s: 1 if s == 2  else 0)\ntrain_df['Mfamille'] = train_df['taillefamille'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ntrain_df['Gfamille'] = train_df['taillefamille'].map(lambda s: 1 if s >= 5 else 0)","31e86905":"\ng = sns.factorplot(x=\"Solo\",y=\"Survived\",data=train_df,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"Pfamille\",y=\"Survived\",data=train_df,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"Mfamille\",y=\"Survived\",data=train_df,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"Gfamille\",y=\"Survived\",data=train_df,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")","e9f63eb7":"train_df.head()","2b3e060f":"#for test data\ntest_df['Solo'] = test_df['taillefamille'].map(lambda s: 1 if s == 1 else 0)\ntest_df['Pfamille'] = test_df['taillefamille'].map(lambda s: 1 if s == 2  else 0)\ntest_df['Mfamille'] = test_df['taillefamille'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ntest_df['Gfamille'] = test_df['taillefamille'].map(lambda s: 1 if s >= 5 else 0)","c068c51d":"test_df.head()","b8944df3":"\nfeatures_drop = ['Ticket', 'SibSp', 'Parch']\n\ntrain_df = train_df.drop(features_drop, axis=1)\ntest_df = test_df.drop(features_drop, axis=1)","a7714274":"\ntrain_df.head(10)","79ec9252":"\ntest_df.head(5)","03f436c0":"train_df.info()","0c43d64c":"from sklearn.model_selection import train_test_split\n\nX_all = train_df.drop(['Survived', 'PassengerId'], axis=1)\nY_all = train_df['Survived']\n\nnum_test = 0.20 #20% for test\nX_train, X_test, Y_train, Y_test = train_test_split(X_all, Y_all, test_size=num_test, random_state=23)","14b65050":"#KNN\n\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\n# Choose the type of classifier. \nknn = KNeighborsClassifier()\n\n# Choose some parameter combinations to try\nparameters = {'n_neighbors':[3, 5, 7],\n              'weights':['uniform'], \n              'algorithm':['auto'], \n              'leaf_size':[30]\n             }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(knn, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(X_train, Y_train)\n\n# Set the clf to the best combination of parameters\nclf = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nclf.fit(X_train, Y_train)","2a168875":"predictions = clf.predict(X_test)\nprint(accuracy_score(Y_test, predictions))","77d4cc23":"\n# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n# Choose the type of classifier. \nGBC = GradientBoostingClassifier()\n\n# Choose some parameter combinations to try\nparameters = {\n              'max_depth': [1, 2, 3, 4, 5],\n              'max_features': [1, 2, 3, 4]}\n             \n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(GBC, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(X_train, Y_train)\n\n# Set the clf to the best combination of parameters\nclf = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nclf.fit(X_train, Y_train)","2dff1e57":"predictions = clf.predict(X_test)\nprint(accuracy_score(Y_test, predictions))","bdbc9ac3":"#Random Forest Classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n# Choose the type of classifier. \nRFC = RandomForestClassifier()\n\n# Choose some parameter combinations to try\nparameters = {\n              'max_depth': [1, 2, 3, 4, 5],\n              'max_features': [1, 2, 3, 4]}\n             \n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(RFC, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(X_train, Y_train)\n\n# Set the clf to the best combination of parameters\nclf = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nclf.fit(X_train, Y_train)","6bce9914":"predictions = clf.predict(X_test)\nprint(accuracy_score(Y_test, predictions))","e94fd053":"#CART\n\n# Choose the type of classifier. \nCART = DecisionTreeClassifier()\n\n# Choose some parameter combinations to try\nparameters = {'max_depth': [1, 2, 3, 4, 5],\n              'max_features': [1, 2, 3, 4]}\n             \n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(CART, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(X_train, Y_train)\n\n# Set the clf to the best combination of parameters\nclf = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nclf.fit(X_train, Y_train)","6aead915":"predictions = clf.predict(X_test)\nprint(accuracy_score(Y_test, predictions))","a6a4a92c":"\n\npredictions = clf.predict(X_test)\nprint(accuracy_score(Y_test, predictions))","c5dbd506":"#Confusion matrix\n\npredictions = clf.predict(X_test)\nprint(confusion_matrix(Y_test, predictions))","8e503cae":"predictions = clf.predict(X_test)\nprint(classification_report(Y_test, predictions))","7ed06387":"ids = test_df['PassengerId']\npredictions = clf.predict(test_df.drop('PassengerId', axis=1))\n\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('test_result.csv', index = False)\noutput.head()","83445c36":"from IPython.display import Image\nImage(url= \"https:\/\/static1.squarespace.com\/static\/5006453fe4b09ef2252ba068\/t\/5090b249e4b047ba54dfd258\/1351660113175\/TItanic-Survival-Infographic.jpg?format=1500w\")","4cff207d":"<div class=\"alert alert-block alert-info\">\n<b>6.Extraction of Test_results<\/b><br>\n<\/div>","4ac767c1":"_la taille de famille semble avoir un role important dans la survie des passagers pour ceci on a d\u00e9cid\u00e9 de les classer encore en 4 cat\u00e9gories_","4ea19d1e":"<div class=\"alert alert-block alert-warning\">\n<b>Note:<\/b> \nWe can see that the age distributions are not the same in the surviving and non-surviving subpopulations. Indeed, there is a peak corresponding to the young passengers who survived. We also find that passengers between 60 and 80 years have survived less.\n\nThus, even if \"Age\" does not correlate with \"Survived\", we can see that there are age categories of passengers that are more or less likely to survive.\n\nIt seems that very young passengers (between 0-5 years old) are more likely to survive.<\/div>\n","0826ab65":"so i decided to use the Decision tree classifier model for the testing data. ","ce6d8c53":"<div class=\"alert alert-block alert-success\">\n<b>Hypothesis result supported by data:<\/b> <br>\nIndeed, the third class is the most frequent for passengers coming from Southampton (S) and Queenstown (Q), while the passengers of C (Cherbourg-France) are mostly first class and have higher survival rate .\n\nAt this point, we can not explain why the first class has a higher survival rate. our hypothesis is that first-class passengers were given priority during the evacuation due to their influence. the graph below represents this hypothesis.<\/div>","955d7da0":"<div class=\"alert alert-block alert-warning\">\n    <b>Note : <\/b>\"Women and children first\",It is interesting to note that passengers with a rare title are more likely to survive.<\/div>","4fde12fa":"<div class=\"alert alert-block alert-success\">\n<b>3) Data Understanding<\/b> \n<\/div>","af8c963d":"<div class=\"alert alert-block alert-success\">\n<b>b.Missing Values :<\/b> \n<\/div>\n","24090b41":"<div class=\"alert alert-block alert-warning\">\n<b>Note:<\/b> \n   the chart confirms that a person having more than two borthers\/sisters or spouse is more likely to survive.moreover the chart confirms that a person on board without borthers\/sisters\/spouse is more likely to die<\/div>","f233bb2f":"<div class=\"alert alert-block alert-warning\">\n<b>Note:<\/b> \n  The graph confirms that 1st class people are more likely to survive than other classes. The chart confirms that the 3rd class people are more likely to die than the others || the rich people first<\/div>\n","abdb9ed4":"<div class=\"alert alert-block alert-info\">\n<b>Final Step <\/b> \n<\/div>","bf36224b":"_On pense que les personnes ayant un num\u00e9ro de cabine enregistr\u00e9 appartiennent \u00e0 une classe socio\u00e9conomique sup\u00e9rieure sont donc plus susceptibles de survivre._","54c7e749":"\n<div class=\"alert alert-block alert-warning\">\n<b>Note:<\/b> \n    The chart confirms that <b> women are more likely <\/b> to survive than <b> men <\/b> || women and children first<\/div>","f0719c4d":"<div class=\"alert alert-block alert-info\">\n<b>FAMILY SIZE<\/b> \n<\/div>","bacb4f56":"<div class=\"alert alert-block alert-info\">\n<b>SEXE<\/b> \n<\/div>","cb7ad08b":"<div class=\"alert alert-block alert-success\">\n<b>for this we have defined our own method of outliers detection<\/b> by defining the Max and Min for each attribute\n<\/div>","1529298d":"<div class=\"alert alert-block alert-warning\">\n<b>Note:<\/b> <br>\nThe graph confirms that a person onboard C is slightly more likely to survive<br> \nThe graph confirms that a person embarked from Q,S is more likely to die <br><\/div>","9fc8d4ea":"<div class=\"alert alert-block alert-success\">\n<b>6) Feature engeneering<\/b><\/div>","e9fbca0a":"**Berroug Med Amine**","d5b36a8e":"<div class=\"alert alert-block alert-warning\">\nThere is 17 titles in the dataset, most of them are very rare and we can group them in 4 categories.<\/div>","1113fb40":"<div class=\"alert alert-block alert-success\">\n<b>Outliers detection :<\/b> Age\n<\/div>","1a8aa263":"<div class=\"alert alert-block alert-warning\">\n<b>Note 2:<\/b> <br>\nsince we already analyzed survival probability by Pclass (1st class more likely to survive),the next step is to see the passengers distribution by PClass<br> \n<\/div>","365e35e9":"<div class=\"alert alert-block alert-warning\"><b>Note :<\/b><br>\npeople with registred cabin have more chance of survival (66,6% vs 29,9%)<br>\nso it would be legit to delete cabin from dataset.\n<\/div>","40afc59b":"<div class=\"alert alert-block alert-warning\">\n<b>the strategie :<\/b>is to fill in Age with the median age of similar rows according to Pclass, Parch and SibSp.<\/div>","bb24067b":"<div class=\"alert alert-block alert-success\">\n<b>Missing Values :<\/b> Age\n<\/div>\n","6523d36a":"**Pclass**","cdd81f4a":"<div class=\"alert alert-block alert-info\">\n<b>a.Outliers Treatement :<\/b> \n<\/div>","1fe192f6":"<div class=\"alert alert-block alert-info\">\n<b>End of this nb<\/b>\n<\/div>","1c5a5674":"**Parch**","74193080":"<div class=\"alert alert-block alert-success\">\n<b>Missing values :<\/b> Cabin\n<\/div>","2cbe604b":"<div class=\"alert alert-block alert-success\">\n<b>Outliers Detection:<\/b> Fare\n<\/div>","2e42935f":"<div class=\"alert alert-block alert-warning\">\n<b>Note :<\/b>\nThe mean fare is 32 units but there are outliers who paid 262, 263, or 512 units, which are 8 to 16 times higher than the mean fare. I am going to keep these outliers because this might help to classify survival as extreme cases in such decision tree algorithm.<\/div>","61aec92b":"_there are several methods of detecting outliers:_<br>\n\n- Modified Z-score method\n-  Z-score Method\n+ IQR interquartille","4b007323":"<div class=\"alert alert-block alert-warning\">\n    Factorplots of family size categories show that <b>Small and Medium<\/b> families have more chance to survive than single passenger and large families.<\/div>","d1c84528":"<div class=\"alert alert-block alert-success\">\n<b>4) Data Visualisation<\/b> \n<\/div>","fdbe33d5":"For the visualization of the data, it is important to note the type of variables:\n\n**Nominal:** Survived, Sex, Embarked, SibSp, Parch<br>\n**Ordinal:** Pclass<br>\n**Numerical:** Age, Fare<br>","cc3714ea":"**Sexe**","cff2228b":"_Overall, there is no outlier that are repeatedly shown among the features._\n\n_We can focus on age and Fare for continous values and Parch and SibSp for integer values to further outliers detection._","7a72d0fd":"<div class=\"alert alert-block alert-success\">\n<b>Missing values:<\/b> Embarked\n<\/div>\n","e51f4008":"### First ML Notebook || TITANIC SURVIVAL","eb2f3ae3":"**SibSp**","ea7f4471":"<div class=\"alert alert-block alert-info\">\n<b>EMBARKED <\/b> \n<\/div>","58475564":"<div class=\"alert alert-block alert-warning\">\n <b>Two interesting correlations are:<\/b>\npositive (0.41) for <b>SibSp and Parch<\/b>\nnegative (-0.31) pour <b>SibSp and Age<\/b>\n<\/div>","bb49c608":"Testing Models:\n- k-Nearest Neighbors KNN\n+ Decision Tree Classifier\n+ Random Forest Classifier\n+ Gradient Boosting Classifier\n","8059721c":"<div class=\"alert alert-block alert-warning\">\n<b>Note :<\/b>The average age is 29, but there are outliers who are 66 to 80 years old, which are 2.3 to 2.8 times higher than the average age, and who are younger than one year old. I am going to keep these outliers for now because the extreme cases of age might be usefull.<\/div>","9f06bc4e":"<div class=\"alert alert-block alert-warning\">\n<b>Note :<\/b>\nNo difference between median value of age in survived and not survived subpopulation.<br>\n\nBut in the violin plot of survived passengers, we still notice that very young passengers have higher survival rate.<\/div>\n","11944a8b":"<div class=\"alert alert-block alert-success\">\n<b>2) Importing data Set <\/b> \n<\/div>","59b04deb":"<div class=\"alert alert-block alert-success\">\n<b>7) Modeling Section<\/b> <\/div>","e73df740":"<div class=\"alert alert-block alert-info\">\n<b> NAME<\/b> \n<\/div>","6b9a57d7":"<div class=\"alert alert-block alert-warning\">\n<b>Note:<\/b> \nThe graph confirms that a person on board with more than 2 parents or children is more likely to survive\n <\/div>\n","f6f3bbb5":"<div class=\"alert alert-block alert-success\">\n<b>1) Importing libraries<\/b> \n<\/div>","98ff4431":"<div class=\"alert alert-block alert-success\">\n<b>Note <\/b>: \nSince some subpopulations are more likely to survive (eg children), it is best to keep the age function and impute the missing values.\n<b>Solution :<\/b> To solve this problem, we examined the most correlated characteristics with <b> Age (Sex, Parch, Pclass and SibSP)<\/b>,i guess this solution is more logical  because it is based on correlated data ,moreover a baby or child he will be in company with their parents or their brothers\n<\/div>\n","d1aa721c":"<div class=\"alert alert-block alert-warning\">\n<b><\/b> \nwe will display some attributes according to the attribute label = 'Survived'<\/div>","93835403":"_outliers repeatedly shown among the features_","41533b9a":"<div class=\"alert alert-block alert-success\">\n<b>Missing values :<\/b> Fare\n<\/div>","f86b4d0d":"<div class=\"alert alert-block alert-warning\">\n<b>Note :<\/b><br>The age distribution seems to be the same in the subpopulations of men and women, so <b> sex is not informative for age predicting.<\/b>\nHowever, <b> 1st class passengers are older than 2nd class passengers <\/b> and are also older than 3rd class passengers.\nMoreover, the more a passenger has parents\/children the older he is and the more a passenger has siblings\/spouses the younger he is .<\/div>","650b897e":"<div class=\"alert alert-block alert-info\">\n<b><\/b> Removing unnecessary attributes\n<\/div>","b2e069e3":"_**IQR interquartille** ._\n","939e5e61":"<div class=\"alert alert-block alert-info\">\n<b>5.Test du Mod\u00e8le:<\/b> d'apr\u00e8s ce qui prec\u00e8de on va continuer par le mod\u00e8le CART\n<\/div>","635f531e":"Import all libraries required\n- pandas is used for data manipulation \n+ numpy is used for mathematacial functions\n+ matplotlib and seaborn are for Data Visualisation","6aee9cc0":"**Age**","6cc89d94":"**Embarked**","3027cd72":"<div class=\"alert alert-block alert-success\">\n<b>5) Missing values<\/b><\/div>","53cd6ec3":"\n_The Name feature contains information on passenger's title.\nSince some passenger with distingused title may be preferred during the evacuation, it is interesting to add them to the model._\n"}}