{"cell_type":{"03f83ede":"code","09712745":"code","08c01539":"code","4819c822":"code","e3857bb0":"code","105f5d5c":"code","69ef1c28":"code","16f7fade":"code","3d37b539":"code","21753a3b":"code","f1faf143":"code","0f726a23":"code","f85320a5":"code","af128402":"code","e6d628af":"code","3385b5bd":"code","575ab90a":"code","0fbb2297":"code","bee14ac3":"code","7f1ceff4":"code","392f03ab":"markdown","6a441d1a":"markdown","3b0b9bf3":"markdown","f901cbab":"markdown"},"source":{"03f83ede":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","09712745":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers, models, Model, Sequential\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\nimport tensorflow as tf\nimport json\nimport os","08c01539":"image_size = (180,180)\nbatch_size = 32","4819c822":"image_path = \"..\/input\/animal-faces\/afhq\/\" #path_to_animal-faces dataset\ntrain_dir = image_path + \"train\"  #train_directory\nvalidation_dir = image_path + \"val\" #validation_directory\n\ntrain_image_generator = ImageDataGenerator( rescale=1.\/255, \n                                            rotation_range=40, \n                                            width_shift_range=0.2,\n                                            height_shift_range=0.2, \n                                            shear_range=0.2,\n                                            zoom_range=0.2,\n                                            horizontal_flip=True, \n                                            fill_mode='nearest')  #image_data_generator_for_train_data\n\ntrain_data_gen = train_image_generator.flow_from_directory(directory=train_dir,\n                                                           batch_size=batch_size,\n                                                           shuffle=True,\n                                                           target_size= image_size,\n                                                           class_mode='categorical')  #applying_data_generator_on_train_data\n    \ntotal_train = train_data_gen.n #getting_total_number_of_train_images\n\n\nvalidation_image_generator = ImageDataGenerator(rescale=1.\/255) #image_data_generator_for_val_data\n\nval_data_gen = validation_image_generator.flow_from_directory(directory=validation_dir,\n                                                              batch_size=batch_size,\n                                                              shuffle=False,\n                                                              target_size=image_size,\n                                                              class_mode='categorical') #applying_data_generator_on_val_data\n    \ntotal_val = val_data_gen.n #getting_total_number_of_val_images","e3857bb0":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPool2D, Flatten, Dense, InputLayer, BatchNormalization, Dropout\n\n# build a sequential model\nmodel = Sequential()\nmodel.add(InputLayer(input_shape=(180, 180, 3)))\n\n# 1st conv block\nmodel.add(Conv2D(25, (5, 5), activation='relu', strides=(1, 1), padding='same'))\nmodel.add(MaxPool2D(pool_size=(2, 2), padding='same'))\n# 2nd conv block\nmodel.add(Conv2D(50, (5, 5), activation='relu', strides=(2, 2), padding='same'))\nmodel.add(MaxPool2D(pool_size=(2, 2), padding='same'))\nmodel.add(BatchNormalization())\n# 3rd conv block\nmodel.add(Conv2D(70, (3, 3), activation='relu', strides=(2, 2), padding='same'))\nmodel.add(MaxPool2D(pool_size=(2, 2), padding='valid'))\nmodel.add(BatchNormalization())\n# ANN block\nmodel.add(Flatten())\nmodel.add(Dense(units=100, activation='relu'))\nmodel.add(Dense(units=100, activation='relu',name='feature_generator'))\nmodel.add(Dropout(0.25))\n# output layer\nmodel.add(Dense(units=3, activation='softmax'))\n","105f5d5c":"from tensorflow import keras\nepochs = 30\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.h5\"),\n]\nmodel.compile(\n    optimizer=keras.optimizers.Adam(1e-3),\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.fit_generator(\n    train_data_gen, epochs=epochs, callbacks=callbacks, validation_data=val_data_gen,\n)","69ef1c28":"from keras.models import Model\nencoder = Model(inputs=model.input, outputs=model.get_layer('feature_generator').output)","16f7fade":"images_path = '..\/input\/animals-dataset\/dataset\/' #path to image dataset","3d37b539":"from keras.preprocessing import image #function for extracting features of a single image using the trained model\ndef extract_features(image_path):\n    img = image.load_img(image_path, target_size=(180,180))\n    img = image.img_to_array(img)\n    img = img\/255\n    img = img.reshape(1,180,180,3)\n    return encoder.predict(img)","21753a3b":"def batch_extractor(path): #function for generating features for batches of iamges\n    files = [os.path.join(images_path, p) for p in sorted(os.listdir(images_path))]\n    \n    result = {} #dictionary for storing image_id and the feature of patricular image\n    for f in files:\n        print ('Extracting features from image %s' % f)\n        name = f.split('\/')[-1].lower()\n        result[name] = extract_features(f)\n    return result\n    ","f1faf143":"features_images = batch_extractor(images_path) #here features of all the 5000 images are extracted and stored.","0f726a23":"features_images","f85320a5":"def query_image_features_extractor(image_path):#extracting the featutres of query images\n    img = image.load_img(image_path, target_size=(180,180))\n    img = image.img_to_array(img)\n    img = img\/255\n    img = img.reshape(1,180,180,3)\n    return encoder.predict(img)\n    ","af128402":"query_image_features = query_image_features_extractor('..\/input\/animals-dataset\/dataset\/2893.jpg')","e6d628af":"query_image_features","3385b5bd":"import numpy as np\n#calculating the eucledian distance\ndef euclidean_distance(a,b):\n    distance = np.linalg.norm(a - b)\n    return distance\n    ","575ab90a":"def search(query_features,limit=10): #search function for calculating the distance between \n                                        #the features of all images to one query image features\n    results = {}\n    for key in features_images:\n        features = features_images[key]\n        d = euclidean_distance(features,query_features)\n        results[key] = d\n    results = sorted([(v, k) for (k, v) in results.items()])\n    \n    return results[:limit]","0fbb2297":"results_search = search(query_image_features) #returning the search function's results","bee14ac3":"results_search","7f1ceff4":"#displaying the search results\n\nresults_final = []\n\nfor (score, resultID) in results_search:\n    # load the result image and display it\n    results_final.append(cv2.imread('..\/input\/animals-dataset\/dataset\/'+ resultID))\nfor i in range(1,len(results_final)):\n    plt.subplot(5,4,i)\n    plt.imshow(results_final[i])\nplt.show()","392f03ab":"* The last but one layer is used to generate the feature vectors for the image data provided in the task.\n![Image](https:\/\/pyimagesearch.com\/wp-content\/uploads\/2019\/05\/transfer_learning_keras_feature_extract.png)* Main thought of generating the feature vectors from a trained CNN was after seeing this image ","6a441d1a":"**Training Data Generator**\n\n* Here the ImageDataGenerator() function from the keras preprocessing module is used.\n* ImageDataGenerator Generate batches of tensor image data with real-time data augmentation.\n* .flow_from_directory() function takes the path to a directory & generates batches of augmented data.\n","3b0b9bf3":"**Model Training**\n* Here a sequential model is built in which we have 3 convolutional blocks\n* The last ANN block is used to normalize the output of the rest 3 convolutional blocks\n* The Convolutional Networks are the best to play around with image data, there fore I chose to built a simple convolutional network to train the model\n* Main source of reading  about convolutional neural networks (CNN) adopted while doing the task are [here](https:\/\/keras.io\/examples\/vision\/mnist_convnet\/) and [here](https:\/\/www.analyticsvidhya.com\/blog\/2020\/02\/learn-image-classification-cnn-convolutional-neural-networks-3-datasets\/)","f901cbab":"# **Importing Necessary Packages**"}}