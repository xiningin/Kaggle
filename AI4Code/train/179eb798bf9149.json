{"cell_type":{"a7312ec3":"code","66d1d69a":"code","a1299855":"code","31d0a850":"code","68ced1c8":"code","51933095":"code","2ff720d2":"code","8b71d1cf":"code","0c4de7cd":"code","155bbee2":"code","0b139d0b":"code","36ecefa6":"code","c350ec89":"code","1894799c":"code","a47c67e9":"code","b1db9767":"code","85b76be6":"code","c401c339":"code","7f32acd4":"code","de8757ef":"code","277f53f4":"code","73f83de5":"code","06e3f196":"code","0c0ffcd1":"code","0298feb6":"code","aba54208":"code","0dae05c7":"code","a2a12042":"code","b3ed4541":"markdown","2bf32d65":"markdown","71eec9f7":"markdown","085e1126":"markdown","31618351":"markdown","54046bd7":"markdown","009a53ba":"markdown","0acbda28":"markdown","d7b19738":"markdown","a596998a":"markdown","2811faa8":"markdown","7169a424":"markdown","e97211e0":"markdown","4f9933d9":"markdown","d1a6ec7c":"markdown","c39ea364":"markdown","030915fe":"markdown","e46f0ac3":"markdown","f8d7ad3f":"markdown","12853524":"markdown","7d92af6d":"markdown","7cba6dec":"markdown","71c71ad6":"markdown","76422a55":"markdown","c0a4052c":"markdown"},"source":{"a7312ec3":"import numpy as np\nimport pandas as pd","66d1d69a":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","a1299855":"##display the first five rows of the train dataset.\ntrain.head(5)","31d0a850":"##display the first five rows of the test dataset.\ntest.head(5)","68ced1c8":"##display shapes of datasets\ntrain.shape, test.shape","51933095":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(corrmat, square=True, annot=True)","2ff720d2":"var = 'Year_of_Release'\ntarget = 'JP_Sales'\n\ndata = pd.concat([train[target], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=target, data=data)\nfig.axis(ymin=0, ymax=max(train[target]))\nplt.xticks(rotation=90)","8b71d1cf":"from scipy import stats\n\narr = np.log1p(train['JP_Sales'])\nsns.distplot(arr, fit=stats.norm)\nfig = plt.figure()\nstats.probplot(arr, plot=plt);","0c4de7cd":"from sklearn import preprocessing as prep\n\ntarget = 'JP_Sales'\nid_col = 'Id'\n\ndef trans(y):\n    return np.log1p(y)\n\ndef inv_trans(y):\n    return np.exp(y) - 1\n\ndef replace_na_on_mean(data, column):\n    return all_data.replace({column: pd.NaT}, \n                            all_data[column].dropna().mean())\n\ndef str_to_int(data, column):\n    enc = prep.LabelEncoder()\n    return enc.fit_transform(data[column])\n\ndef str_to_columns(data, column):\n    for value in set(data[column]):\n        data[f\"{column}_{value}\"] = pd.Series(1 if el == value else 0 for el in data[column])\n    data = data.drop(columns=column)\n    return data","155bbee2":"y = trans(train[target])\ntrain = train.drop(columns=target)\nId = test[id_col]\ntest = test.drop(columns=id_col)\n\nall_data = pd.concat((train, test), sort=False).reset_index(drop=True)","0b139d0b":"all_data_na = (all_data.isnull().mean()) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data","36ecefa6":"all_data = all_data.drop(columns='Name')","c350ec89":"all_data = all_data.drop(columns='User_Count')\nall_data = replace_na_on_mean(all_data, 'Critic_Count')","1894799c":"all_data = replace_na_on_mean(all_data, 'User_Score')\nall_data = replace_na_on_mean(all_data, 'Critic_Score')\n\nall_data.head()","a47c67e9":"all_data['Total_Score'] = (all_data['User_Score'] + all_data['Critic_Score']\/10) \/ 2","b1db9767":"all_data = all_data.replace({'Developer': pd.NaT}, 'None')\nall_data = all_data.replace({'Publisher': pd.NaT}, 'None')\n\nall_data['Developer'] = str_to_int(all_data, 'Developer')\nall_data['Publisher'] = str_to_int(all_data, 'Publisher')","85b76be6":"all_data = replace_na_on_mean(all_data, 'Year_of_Release')","c401c339":"all_data = str_to_columns(all_data, 'Rating')\nall_data = str_to_columns(all_data, 'Platform')\nall_data = str_to_columns(all_data, 'Genre')","7f32acd4":"train, test = all_data[:train.shape[0]], all_data[train.shape[0]:]","de8757ef":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.33, random_state=42)","277f53f4":"from sklearn.ensemble import GradientBoostingRegressor","73f83de5":"model = GradientBoostingRegressor(subsample=0.9, loss='lad',\n                                 n_estimators=300, learning_rate=0.27,\n                                 max_depth=7, max_features='sqrt')\nmodel.fit(X_train, y_train)","06e3f196":"def error(y_true, y_pred):\n    return np.mean(np.abs(np.log1p(y_true) - np.log1p(y_pred)))\n\nprint(f\"On train: {error(inv_trans(y_train), inv_trans(model.predict(X_train)))}\")\nprint(f\"On test: {error(inv_trans(y_test), inv_trans(model.predict(X_test)))}\")","0c0ffcd1":"class ManyModels(object):\n    def __init__(self, models):\n        self.models = models\n        \n    def fit(self, X, y):\n        for model in self.models:\n            model.fit(X, y)\n            \n    def predict(self, X):\n        preds = [model.predict(X) for model in self.models]\n        return sum(preds) \/ len(preds)","0298feb6":"models = ManyModels([GradientBoostingRegressor(subsample=0.9, loss='lad',\n                                               n_estimators=300, learning_rate=0.27,\n                                               max_depth=7, max_features='sqrt')\n                     for _ in range(10)])\n\nmodels.fit(X_train, y_train)","aba54208":"print(f\"On train: {error(inv_trans(y_train), inv_trans(models.predict(X_train)))}\")\nprint(f\"On test: {error(inv_trans(y_test), inv_trans(models.predict(X_test)))}\")","0dae05c7":"models = ManyModels([GradientBoostingRegressor(subsample=0.9, loss='lad',\n                                               n_estimators=300, learning_rate=0.27,\n                                               max_depth=7, max_features='sqrt')\n                     for _ in range(25)])\n\nmodels.fit(train, y)","a2a12042":"res = pd.DataFrame({id_col: Id, \n                    target: inv_trans(models.predict(test))})\nres.to_csv(\"res.csv\", index=False)","b3ed4541":"Covariance matrix helps us to understand relations between features.","2bf32d65":"# Submission","71eec9f7":"* **Year_of_Release**: I understand, that this isn't a good idea, but let's we replace missing values on mean.","085e1126":"* **User_Count**, **Critic_Cout**: We drop first feature, because this feature has so many missing values and bad correlate with target, and replace second feature on mean.","31618351":"Let's train 25 models!","54046bd7":"* **Developer**, **Publisher**: There are string features, because we replace missing values on 'None'. We also match each unique value with an integer.","009a53ba":"# Visualisation","0acbda28":"We see, that features, which contain 'Score' and 'Sales', are associated with target feature.","d7b19738":"We should fix functions and names, which we will use many times.","a596998a":"* **User_Score**, **Critic_Score**: These features are important for us, because we leave them and replace missing data on means.","2811faa8":"One model is good, but 10 models are better.","7169a424":"The more models we use, the better result we get, because set of trees are very differently. Many models counterbalance errors all of them.","e97211e0":"I hope you will agree with me, if I say, that best model for us problem is **decision tree** or rather **many trees**!","4f9933d9":"We get train and test datasets. Perfectly! Let's visualize it.","d1a6ec7c":"# Features engineering","c39ea364":"Let's look at the data","030915fe":"This competition is very interested, because the dependencies between the data are quite complex. In this notebook we will use:\n* **visualisation**\n* **feature engineering**\n* **bagging of models**","e46f0ac3":"I don't know about you, but I love graphs. I think, there is a good feature for decision trees.\nLet's see on distribution of goal column. We should predict logarithm from sales.","f8d7ad3f":"Good work! Let's return back train and test data.","12853524":"# Models bagging to predict sales","7d92af6d":"We can create new feature i.e. 'Total_Score', which equals mean of 'Critic_Score' and 'User_Score', but we should devide Critic_Score by 10.","7cba6dec":"* **Name**: I think, this is useless feature, if you don't want to do semantic analysis","71c71ad6":"# Test and train model","76422a55":"We want, that our transformations of data will be applied to train and test data.","c0a4052c":"* **Rating**, **Platform**, **Genre**: I suggest create new features like as \"column_value\""}}