{"cell_type":{"b7e74563":"code","424f2405":"code","47854587":"code","43fa37be":"code","835566f4":"code","26ca5b14":"code","b230d394":"code","2edc8dae":"code","8381a6a8":"code","bf81299c":"code","11b69c84":"code","6a96cc93":"code","17396330":"code","01c4391e":"markdown","4f17b524":"markdown","7ba69c29":"markdown","90744085":"markdown","ab9aeaf3":"markdown","689fd846":"markdown","069ab99e":"markdown"},"source":{"b7e74563":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import *\nfrom sklearn.experimental import enable_hist_gradient_boosting\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\ntrain = pd.read_csv(\"..\/input\/data-without-drift\/train_clean.csv\")\ntest = pd.read_csv(\"..\/input\/data-without-drift\/test_clean.csv\")\ndisplay(train.head())\n\nn = 500000\nsets = [train.iloc[i:i + n] for i in range(0, len(train), n)]\ntestsets = [test.iloc[i:i + n] for i in range(0, len(test), n)]\n\n#train.plot.scatter(\"time\", \"signal\", c=\"open_channels\", colormap=\"viridis\", alpha=0.1)","424f2405":"plt.scatter(sets[7].time, sets[7].signal, c=sets[7].open_channels, cmap=\"viridis\")\ntrain = train.drop(sets[7].index)","47854587":"fig, ax = plt.subplots(2, 1, figsize=(15, 10))\nax[0].plot(train.time, train.signal)\nax[1].plot(test.time - 500, test.signal)\nletters = {\"train\": [\"Ar\", \"Ar\", \"As\", \"B\", \"C\", \"D\", \"As\", \"\", \"D\", \"C\"], \"test\": [\"As\", \"B\", \"D\", \"Ar\", \"As\", \"C\", \"D\", \"C\", \"Ar\", \"B\", \"Ar\", \"Ar\", \"Ar\", \"Ar\", \"Ar\", \"Ar\", \"Ar\", \"Ar\", \"Ar\", \"Ar\"]}\n\nfor i in range(len(sets)):\n    ax[0].axvline(i * 50, color=\"r\")\n    ax[0].text(i * 50 + 20, -2, letters[\"train\"][i], color=\"w\")\n    ax[0].set_title(\"train\")\n\nfor i in range(len(testsets)):\n    for j in range(5):\n        ax[1].axvline(i * 50 + j * 10, color=\"r\")\n        ax[1].text(i * 50 + j * 10 + 5, -2.5, letters[\"test\"][5 * i + j], color=\"w\")\n    ax[1].set_title(\"test\")","43fa37be":"def get_name(model):\n    return str(model).split(\"(\")[0]\nprint(get_name(ensemble.GradientBoostingRegressor()))","835566f4":"trainsets = {}\nmodels = {}\nmodeltypes = [ensemble.GradientBoostingRegressor, ensemble.HistGradientBoostingRegressor]#, ensemble.RandomForestClassifier]#, neighbors.KNeighborsClassifier, tree.DecisionTreeClassifier]\nf1_sum = 0.0\n\nfor i in range(len(sets)):\n    pattern = letters[\"train\"][i]\n    if pattern not in trainsets.keys():\n        trainsets[pattern] = sets[i]\n    else:\n        trainsets[pattern] = trainsets[pattern].append(sets[i])","26ca5b14":"traindata = trainsets[\"C\"]\ndisplay(type(traindata))\n\naverage = traindata.signal.rolling(5, center=True, win_type=\"barthann\").mean()\n#average = average.fillna(traindata.signal.rolling(5, center=True, win_type=\"barthann\").mean())\naverage = average.fillna(traindata.signal)\ntraindata[\"rollavg\"] = average\n\ndisplay(traindata.rollavg.head(), traindata.rollavg.tail())\nfig, ax = plt.subplots(2)\nheader = traindata.head(1000)\n\nax[0].plot(header.time, header.signal, label=\"signal\")\nax[0].plot(header.time, header.rollavg, label=\"average\")\n\nax[1].plot(header.time, header.open_channels, label=\"open_channels\")\nax[1].plot(header.time, header.rollavg + 3.5, label=\"average\")\nplt.legend()\n\ndisplay(traindata.corr())","b230d394":"#forward = traindata.signal.shift(3).fillna(traindata.signal)\n#backward = traindata.signal.shift(-3).fillna(traindata.signal)\nforward = traindata.rollavg.shift(3).fillna(traindata.signal)\nbackward = traindata.rollavg.shift(-3).fillna(traindata.signal)\ngrad = np.gradient(traindata.signal)\n\ntraindata[\"lag\"] = backward\ntraindata[\"lead\"] = forward\ntraindata[\"grad\"] = grad","2edc8dae":"pattern = \"C\"\nstartc = datetime.now()\n\nprint(len(traindata.signal), len(traindata.rollavg), len(traindata.lag), len(traindata.lead))\nx = np.array([traindata.signal, traindata.rollavg, traindata.lag, traindata.lead, traindata.grad]).T\nprint(x.shape)\ny = traindata.open_channels\n#newmodel = ensemble.RandomForestRegressor(n_jobs=-1)\nlgbtrain = lgb.Dataset(x, label=y, free_raw_data=False)\nparam = {\"max_bin\": 2048}\n\nprint(\"Training:\", pattern)\n#newmodel.fit(x, y)\nnewmodel = lgb.train(param, lgbtrain)\nprint(\"Predicting:\", pattern)\n#print(newmodel.predict(x))\npred = np.round(newmodel.predict(x))\nprint(metrics.f1_score(y, pred, average=\"micro\"))\nprint(\"Finished in\", datetime.now() - startc)","8381a6a8":"for trainset in trainsets.values():\n    avg = trainset.signal.rolling(5, center=True).mean()\n    avg = avg.fillna(trainset.signal)\n    \n    #forward = trainset.signal.shift(3).fillna(trainset.signal)\n    #backward = trainset.signal.shift(-3).fillna(trainset.signal)\n    forward = avg.shift(3).fillna(trainset.signal)\n    backward = avg.shift(-3).fillna(trainset.signal)\n    \n    grad = np.gradient(trainset.signal)\n    \n    trainset[\"lag\"] = backward\n    trainset[\"lead\"] = forward\n    trainset[\"rollavg\"] = avg\n    trainset[\"grad\"] = grad","bf81299c":"for pattern in trainsets.keys():\n    if pattern == \"\": continue\n    mscore = 0.0\n    traindata = trainsets[pattern]\n    #numchannels = len(traindata.open_channels.unique())\n    \n    x = np.array([traindata.signal, traindata.rollavg, traindata.lag, traindata.lead, traindata.grad]).T\n    y = traindata.open_channels\n    lgbtrain = lgb.Dataset(x, label=y, free_raw_data=False)\n    param = {\"max_bin\": 2048}\n    #param['metric'] = 'auc'\n    \n    print(\"Training:\", pattern)\n    newmodel = lgb.train(param, lgbtrain)\n    print(\"Predicting:\", pattern)\n    pred = np.round(newmodel.predict(x))\n    print(metrics.f1_score(y, pred, average=\"micro\"))\n    models[pattern] = newmodel","11b69c84":"n = 100000\ntestdata = [test.iloc[i:i+n] for i in range(0, len(test), n)]\nfor testd in testdata:\n    avg = testd.signal.rolling(5, center=True).mean()\n    avg = avg.fillna(testd.signal)\n    \n    #forward = testd.signal.shift(3).fillna(testd.signal)\n    #backward = testd.signal.shift(-3).fillna(testd.signal)\n    \n    forward = avg.shift(3).fillna(testd.signal)\n    backward = avg.shift(-3).fillna(testd.signal)\n    \n    grad = np.gradient(testd.signal)\n    \n    testd[\"lag\"] = backward\n    testd[\"lead\"] = forward\n    testd[\"rollavg\"] = avg\n    testd[\"grad\"] = grad\nprint(testdata[0].shape)","6a96cc93":"data = np.array([testdata[0].signal, testdata[0].rollavg, testdata[0].lag, testdata[0].lead, testdata[0].grad]).T\npred = models[letters[\"test\"][0]].predict(data)\n\nfor i in range(1, len(testdata)):\n    data = np.array([testdata[i].signal, testdata[i].rollavg, testdata[i].lag, testdata[i].lead, testdata[0].grad]).T\n    pred = np.append(pred, models[letters[\"test\"][i]].predict(data))\n\n#print(data.shape)\npred = np.round(pred).astype(int)\nout = pd.DataFrame({\"time\": test.time, \"open_channels\": pred})\nout.to_csv(\"LGB_AvgMultiFeature.csv\", float_format='%0.4f', index=False)","17396330":"plt.scatter(test.time, test.signal, c=out.open_channels, cmap=\"viridis\", alpha=0.1)","01c4391e":"<a id=\"data\"><\/a>\n## 1. Data Processing:\nI started with trying to clean data, but there are clearly people who are much more experienced. I found a dataset that removed drift, which made the classification much simpler. I then cut the data up into the batches as per the competition description which stated that the data was grouped into 50 second intervals. Also, the 8th batch had some abnormalities in the middle (shown below), so I chose to exclude that set from training because I felt it would confuse the models.","4f17b524":"Creating a rolling average feature","7ba69c29":"# Predicting Open Ion Channels with Cleaned Signal Data\nThis is my first actual competition, and it intrigued me from the start. I've learned lots about classification and data analysis by reading as well as trial and error. I've gotten a decent score with the help of talented people cleaning the data and coming up with new ways to process it, and I wanted to share my progress.\n1. [Data Processing](#data)\n2. [Grouping by Pattern](#patterns)\n3. [Model Training and Selection](#models)\n4. [Predictions and Submission](#predictions)","90744085":"<a id=\"predictions\"><\/a>\n## 4. Predictions and Submission:\nWith five quality models, I looped through each \"set\" of training data and used the labels I assigned earlier to make predictions with the respective model. I then concatenated all these predictions and outputted it as a csv.","ab9aeaf3":"<a id=\"models\"><\/a>\n## 3. Model Training and Selection:\nMy approach was to create a list of model types, and then train a model of each type on each category of data, and see which one performed the best. Then, the best performing model was used for predictions on data of that category. After this process, I had a model for each category. I hope to further develop this into an ensemble where multiple models can contribute to the classifications, instead of just one.\n\n*Note: I switched from classification to regression models, rounding their result to the nearest integer because it was much faster.*","689fd846":"Previously used code to test multiple model types\n```\nfor pattern in trainsets.keys():\n    if pattern == \"\": continue\n    mscores = []\n    traindata = trainsets[pattern]\n    numchannels = len(traindata.open_channels.unique())\n    trainingmodels = []\n    \n    x = np.array(traindata.signal).reshape(-1, 1)\n    y = traindata.open_channels\n    initialtime = datetime.now()\n    print(initialtime, \"Starting:\", pattern)\n    \n    for i in range(len(modeltypes)):\n        model = modeltypes[i]()\n        name = get_name(model)\n        trainingmodels.append(model)\n        start = datetime.now()\n        \n        print(\"\\t\", start, \"Fitting:\", pattern, name)\n        model.fit(x, y)\n        \n        print(\"\\t\", datetime.now(), \"Predicting:\", pattern, name)\n        pred = np.round(model.predict(x))\n        score = metrics.f1_score(y, pred, average=\"macro\")\n        print(\"\\t\", datetime.now(), pattern, name, score)\n        mscores.append(score)\n        print()\n    \n    bestindex = 0\n    for i in range(len(mscores)):\n        if mscores[i] > mscores[bestindex]:\n            bestindex = i\n\n    f1_sum += mscores[bestindex]\n    \n    print(pattern, \"completed in\", datetime.now() - initialtime)\n    print(\"Best model: \", get_name(trainingmodels[bestindex]), mscores[bestindex])\n    print()\n    \n    models[pattern] = trainingmodels[bestindex]\nprint(\"Best F1 Average:\", f1_sum \/ len(models))\n```","069ab99e":"<a id=\"patterns\"><\/a>\n## 2. Grouping by Pattern:\nThe next step was to isolate similar patterns in the train and test sets, which was inspired by reading from other published notebooks. My approach was to manually label each as As (smooth), Ar (rough), B, C, or D. Then, I clumped the train data with the same label into a \"set\"."}}