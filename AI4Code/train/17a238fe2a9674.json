{"cell_type":{"212d4292":"code","ceb15384":"code","8e7a4a40":"code","75cc5d98":"code","d099fa91":"code","65072259":"code","f3584514":"code","59aace01":"code","fc6112d5":"code","69e1c8eb":"code","18384f98":"code","81e1e383":"code","62999c79":"code","d4d6f66c":"code","063fd49c":"code","5513be50":"code","60f11c89":"code","d03f34cb":"code","48124670":"markdown","6c05940b":"markdown","41c075d7":"markdown"},"source":{"212d4292":"import pandas as pd\ndata = pd.read_csv(\"\/kaggle\/input\/petfinder-pawpularity-score\/train.csv\", sep=',')\ndata\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ceb15384":"data['Id'] = data['Id'] + '.jpg'\ndata = data.rename(columns={'Id': 'filename'})\ndata","8e7a4a40":"from sklearn.model_selection import train_test_split\ntrain, val = train_test_split(data, test_size=0.2, random_state=1)  # val 20%\ntrain, test = train_test_split(train, test_size=0.01, random_state=1)  # test 1%\nprint(\"shape train: \", train.shape)\nprint(\"shape val: \", val.shape)\nprint(\"shape test: \", test.shape)","75cc5d98":"dataset_dir = '\/kaggle\/input\/petfinder-pawpularity-score\/train'\nwidth, height = 512, 512\nbatch_size = 32","d099fa91":"import os\nfrom PIL import Image\nimport numpy as np\nwidths, heights = [], []\nfor image_name in os.listdir(dataset_dir):\n    image_path = os.path.join(dataset_dir, image_name)\n    img = Image.open(image_path)\n    img_width, img_height = img.size\n    widths.append(img_width)\n    heights.append(img_height)\nprint(\"max width:\", max(widths), \"\\nmax height:\", max(heights),\n      \"\\nmin width:\", min(widths), \"\\nmin height:\", min(heights),\n      \"\\nmean width:\", np.array(widths).mean(),\n      \"\\nmean height:\", np.array(heights).mean())","65072259":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ntrain_generator = ImageDataGenerator(\n    rescale=1.0 \/ 255,\n    rotation_range=5,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    brightness_range=(0.75, 1),\n    shear_range=0.1,\n    zoom_range=[0.75, 1],\n    horizontal_flip=True,\n    validation_split=0.2\n)\nvalidation_generator = ImageDataGenerator(\n    rescale=1.0 \/ 255\n)\ntest_generator = ImageDataGenerator(\n    rescale=1.0 \/ 255\n)","f3584514":"import matplotlib.pyplot as plt\n\nseries = data.iloc[2]\ndata_augmentation_viz = pd.concat([series, series], axis=1).transpose()\niterator_visualizations = train_generator.flow_from_dataframe(\n    dataframe=data_augmentation_viz,\n    directory=dataset_dir,\n    x_col=\"filename\",\n    y_col=\"Pawpularity\",\n    class_mode=\"raw\",\n    target_size=(width, height),\n    batch_size=1,  # 1 seule image pour v\u00e9rifier\n)\nplt.figure(figsize=(10, 12))\nfor i in range(9):\n    ax = plt.subplot(3, 3, i + 1)  # 3x3 grid\n    batch = next(iterator_visualizations)  # toujours la m\u00eame image\n    img = batch[0]\n    img = img[0, :, :, :]\n    plt.imshow(img)\nplt.show()\nplt.close()","59aace01":"train_generator = train_generator.flow_from_dataframe(\n    dataframe=train,\n    directory=dataset_dir,\n    x_col=\"filename\",\n    y_col=\"Pawpularity\",\n    class_mode=\"raw\",  # \"raw\" pour les regressions\n    target_size=(width, height),\n    batch_size=batch_size\n)\nvalidation_generator = validation_generator.flow_from_dataframe(\n    dataframe=val,\n    directory=dataset_dir,\n    x_col=\"filename\",\n    y_col=\"Pawpularity\",\n    class_mode=\"raw\",\n    target_size=(width, height),\n    batch_size=batch_size\n)\ntest_generator = test_generator.flow_from_dataframe(\n    dataframe=test,\n    directory=dataset_dir,\n    x_col=\"filename\",\n    y_col=\"Pawpularity\",\n    class_mode=\"raw\",\n    target_size=(width, height),\n    batch_size=batch_size\n)","fc6112d5":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nearly_stopping_callback = EarlyStopping(\n    monitor=\"val_mean_absolute_percentage_error\",\n    min_delta=1,  # sous les 1% de mieux, on patiente\n    patience=10,  # on patiente max 10 epochs\n    verbose=2,\n    mode=\"min\",\n    restore_best_weights=True\n)\nmodel_checkpoint_callback = ModelCheckpoint(\n    'efficientNet_reg.h5',\n    monitor=\"val_mean_absolute_percentage_error\",\n    verbose=0,\n    save_best_only=True,\n    mode=\"min\",\n    save_freq=\"epoch\"\n)\ncallbacks = [early_stopping_callback, model_checkpoint_callback]","69e1c8eb":"import sys\nimport os\nsys.path.insert(0, \"\/kaggle\/input\/efnetv2src\/efficientnet-v2-keras-main\")\nsys.path.append('..\/input\/tfkeras-efficientnetsv2\/')\nfrom efficientnet_v2 import EfficientNetV2XL","18384f98":"from tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Dropout\nfrom tensorflow.keras.metrics import MeanAbsoluteError, MeanAbsolutePercentageError\nimport tensorflow_addons as tfa\n\nefficientnet = EfficientNetV2XL(\n    include_top=False,\n    weights='..\/input\/tfkeras-efficientnetsv2\/21_ft1k_notop\/efficientnetv2-xl-21k-ft1k_notop.h5', \n    input_shape=(height, width, 3)\n)\n\nfor layer in efficientnet.layers:\n        layer.trainable = False\n\nx = GlobalAveragePooling2D(name=\"avg_pool\")(efficientnet.output)\nx = BatchNormalization()(x)\nx = Dropout(0.2, name=\"top_dropout\")(x)\noutputs = Dense(1, name=\"pred\")(x)\n\nefficientnet = Model(inputs=efficientnet.inputs, outputs=outputs)\n\nradam = tfa.optimizers.RectifiedAdam(learning_rate=0.001)\noptimizer = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)\n\nefficientnet.compile(\n    optimizer=optimizer,\n    loss=\"mean_absolute_error\",\n    metrics=[MeanAbsoluteError(), MeanAbsolutePercentageError()]\n)","81e1e383":"epochs = 100\nbatch_size = 8\nhistory_efficientnet = efficientnet.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    callbacks=callbacks,\n    workers=6  # guess\n)","62999c79":"train[\"Pawpularity\"].mean()","d4d6f66c":"mean_baseline = MeanAbsolutePercentageError()\nmean_baseline = mean_baseline(\n    val[\"Pawpularity\"], train[\"Pawpularity\"].mean()\n).numpy()","063fd49c":"dict1 = {\n    \"MAPE\": history_efficientnet.history[\"mean_absolute_percentage_error\"],\n    \"type\": \"training\"\n}\ndict2 = {\n    \"MAPE\": history_efficientnet.history[\"val_mean_absolute_percentage_error\"],\n    \"type\": \"validation\"\n}\ns1 = pd.DataFrame(dict1)\ns2 = pd.DataFrame(dict2)\ndf = pd.concat([s1, s2], axis=0).reset_index()\nimport seaborn as sns\ngrid = sns.relplot(\n    data=df,\n    x=df[\"index\"],\n    y=\"MAPE\",\n    col=\"type\",\n    kind=\"line\",\n    legend=False\n)\ngrid.set(ylim=(20, 100))\nfor ax in grid.axes.flat:\n    ax.axhline(\n        y=mean_baseline, color=\"lightcoral\", linestyle=\"dashed\"\n    )\n    ax.set(xlabel=\"Epoch\")\nplt.legend(labels=[\"efficientNet_reg\", \"mean_baseline\"])\nplt.show()","5513be50":"import tensorflow as tf\ndef preprocess(image):  \n    return (tf.cast(image, dtype=tf.float32) - 128.00) \/ 128.00","60f11c89":"import numpy as np\nplt.figure(figsize=(10, 12))\nbatch = next(test_generator)  # renvoie 32 images\nfor i in range(12):\n    ax = plt.subplot(4, 3, i + 1)  # 4x3 grid\n    image = batch[0][i, :, :, :]  # i-\u00e8me image\n    img = preprocess(image.reshape(1, width, height, 3))\n    pawpularity = batch[1][i]  # i-\u00e8me pawpularity\n    preds = efficientnet.predict(img)\n    prediction = preds.flatten()[0]\n    print('pawpularity=', pawpularity, '\\tprediction=', round(prediction, 2))\n    diff = prediction - pawpularity\n    percentDiff = (diff \/ pawpularity) * 100\n    absPercentDiff = np.abs(percentDiff)\n    plt.title(\"Pawpularity: \" + str(pawpularity) +\\\n              \"\\npred: \" + str(round(prediction, 2)) +\\\n              \" (err=\" + str(int(absPercentDiff)) + \"%)\")\n    plt.imshow(image)\n    plt.axis(\"off\")\nplt.show()\nplt.close()","d03f34cb":"import os\nfrom PIL import Image\ntest_dir = '\/kaggle\/input\/petfinder-pawpularity-score\/test'\nids = []\npawpularities = []\nfor test_image in os.listdir(test_dir):\n    image_path = os.path.join(test_dir, test_image)\n    id_image = test_image.split('.')[0]\n    ids.append(id_image)\n    img = Image.open(image_path) \n    img = img.resize((width, height))\n    img = preprocess(np.array(img).reshape(1, width, height, 3))\n    preds = efficientnet.predict(img)\n    prediction = preds.flatten()[0]\n    pawpularities.append(prediction)\n\nsubmission_dict = {\n    'Id': ids,\n    'Pawpularity': pawpularities\n}\nsubmission_df = pd.DataFrame(submission_dict)\nprint(submission_df)\nsubmission_df.to_csv('submission.csv', index=False, sep=',')","48124670":"Par curiosit\u00e9, je regarde les dimensions des images du dataset","6c05940b":"Le notebook doit fonctionner sans acc\u00e8s internet","41c075d7":"Je d\u00e9coupe en train \/ val \/ test"}}