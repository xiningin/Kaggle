{"cell_type":{"018f398b":"code","61f64d99":"code","7a0433ef":"code","c775d0d7":"code","a14b52f4":"code","b5bb0c34":"code","72b50842":"code","1fa8f76a":"code","3d1b6bdd":"code","f445f407":"code","ead20810":"code","03e22fb9":"code","0205e817":"code","f598dcdd":"code","a995f3a1":"code","9ab047bf":"code","575cd033":"code","6143c066":"code","f9dea992":"code","ef5cdc22":"code","4516df05":"code","760d56dc":"code","f854b245":"code","41165d2d":"code","d32bce10":"code","6f8da3b4":"code","03c7ba24":"code","48829c2b":"code","e958f3a3":"code","dfc4d388":"code","9167dbba":"code","d5be4065":"code","d290a741":"code","5421f40b":"code","6d5a8907":"code","bd16d18a":"code","3088ff98":"code","089fedd6":"code","fea2de0b":"code","4d664dd6":"code","6ce2a2a9":"markdown","6c19ffbe":"markdown","77b0d613":"markdown","25d7dc56":"markdown","b1920d24":"markdown","f9e27c91":"markdown","9b8b31a6":"markdown","a8218071":"markdown","c74a3bba":"markdown","c6912418":"markdown","3f1477d0":"markdown","f4ebec90":"markdown","05f96596":"markdown","7a8901a1":"markdown","2e75e687":"markdown","dfae1678":"markdown","c0cd8805":"markdown","babe1636":"markdown","44f522e6":"markdown","809c94af":"markdown","c85b9f4b":"markdown","effb7149":"markdown","b42a2ed1":"markdown","67c59661":"markdown","e7d1c926":"markdown","d40e5998":"markdown","a38195b4":"markdown","4dae9c63":"markdown","3258c7e7":"markdown"},"source":{"018f398b":"# INSTALLATION:\n\n!pip install pyldavis","61f64d99":"from collections import defaultdict, Counter\nfrom configparser import ConfigParser, ExtendedInterpolation\nfrom IPython.core.display import display, HTML\nfrom IPython.display import Image\nfrom IPython.lib.display import YouTubeVideo\nfrom gensim import corpora, models\nfrom gensim.models.ldamodel import LdaModel\nfrom gensim.models.callbacks import DiffMetric\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pyLDAvis  # conda install pyldavis\nimport pyLDAvis.gensim\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport spacy\nfrom spacy.matcher import Matcher, PhraseMatcher\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\n%matplotlib inline","7a0433ef":"# PCA\n# SOURCE: https:\/\/intoli.com\/blog\/pca-and-svd\/\n# Image(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/pca.png\", width=700)","c775d0d7":"# create an example dataframe\nX_df = pd.DataFrame([[-1, 1], [-2, 1], [-3, 2], [1, -1], [2, -1], [3, -2]])\nX_df","a14b52f4":"# plot the data\nX_df.plot.scatter(x=0, y=1)","b5bb0c34":"# import PCA\nfrom sklearn.decomposition import PCA\n\n# run PCA to create 2 principal components\npca = PCA(n_components=2)\nfit_pca = pca.fit_transform(X_df)\n\n# create the transformed matrix\npca_df = pd.DataFrame(fit_pca, columns=['component 1','component 2'])\npca_df","72b50842":"# plot the prinipal components\npca_df.plot.scatter(x='component 1', y='component 2')\n\n# increase range of y-axis to match the original visualization\nplt.ylim(-2,2)","1fa8f76a":"print('Singular values: {}'.format(pca.singular_values_.round(2)))\n\n# Singular Values are the l2 norm of each component\n# e.g. np.sqrt(sum([i**2 for i in pca_df['component 1']]))\nprint('Singluar Value 1: {}'.format(np.linalg.norm(pca_df['component 1'], ord=2).round(2)))\nprint('Singluar Value 2: {}'.format(np.linalg.norm(pca_df['component 2'], ord=2).round(2)))\n\nprint('\\nExplained variance ratio: {}'.format(pca.explained_variance_ratio_))","3d1b6bdd":"# view the first prinicipal component\npca_df['component 1']","f445f407":"# square all values\npca_df['component 1'] ** 2","ead20810":"# take the sum of squares\nsum(pca_df['component 1'] ** 2)","03e22fb9":"# take the sqaure root of the sum of squares\nnp.sqrt(sum(pca_df['component 1'] ** 2))","0205e817":"print('Components:\\n {}'.format(pca.components_.round(2)))","f598dcdd":"# recreate PCA using SVD (we will explain this calculation in the next section)\nu,s,vt = np.linalg.svd(X_df, full_matrices=False)\n\nprint(\"singular values =\\n {} \\n\".format(np.round(s, 2)))\nprint(\"components =\\n {} \\n\".format(np.round(vt, 2)))\n\n# pca\npd.DataFrame(u*s)","a995f3a1":"# SVD\n# Image(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/svd_graph.png\", width=400, height=200)","9ab047bf":"# SVD\n# Image(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/svd3.png\", width=400)","575cd033":"# reduced SVD\n# Image(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/reduced_svd.png\", width=500, height=500)","6143c066":"# SVD\n# Image(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/svd2.png\", width=700)","f9dea992":"# Truncated SVD\n# SOURCE: https:\/\/www.researchgate.net\/figure\/Singular-value-decomposition-SVD-and-a-truncated-SVD-matrix_fig1_227060873\n# Image(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/truncated_svd.png\", width=500)","ef5cdc22":"# SVD\nImage(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/svd4.png\", width=700)","4516df05":"# reduced SVD equation\nImage(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/svd_truncated_equation.png\", \n      width=500, height=200)","760d56dc":"# SVD vs PCA\nImage(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/svd_vs_pca.png\", width=1000)","f854b245":"# SVD vs PCA\n# Image(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/svd_vs_pca2.png\", width=700)","41165d2d":"from sklearn.feature_extraction.text import CountVectorizer","d32bce10":"# GENSIM_DICTIONARY_PATH = r'gensim_dictionary_path.txt' \n# GENSIM_CORPUS_PATH = r'gensim_corpus_path.txt'\n\n# CLEANED_TEXT_PATH = r'https:\/\/raw.githubusercontent.com\/Alexjmsherman\/nlp_practicum_cohort3_student\/master\/raw_data\/cleaned_text\/cleaned_text.txt?token=ABXRUPVMSIR3QOBGSWUI5SS5CA6AY'\nCLEANED_PATH = \"..\/input\/datafornlp\/text.csv\"\ntexts = pd.read_csv(CLEANED_PATH, sep='\\t',header=None)\ntexts = [line[0].split() for line in texts.values]\nprint(texts[0])","6f8da3b4":"# combine tokens from the first few lists into sentences\nsvd_data = [' '.join(text) for text in texts[0:8]]\n\n# create a document term matrix of the token counts\nvect = CountVectorizer(max_features=10, stop_words='english')\ndtm = vect.fit_transform(svd_data)\n\n# create a dataframe\nvocab = vect.get_feature_names()\ndf = pd.DataFrame(dtm.toarray(), columns=vocab)\ndf","03c7ba24":"# decompose the matrix using SVD\nU, s, VT = np.linalg.svd(df, full_matrices=False)\nS = np.diag(s)","48829c2b":"# what are U, S and V\nprint(\"U =\\n\", np.round(U, decimals=2), \"\\n\")\nprint(\"S =\\n\", np.round(S, decimals=2), \"\\n\")\nprint(\"V^T =\\n\", np.round(VT, decimals=2), \"\\n\")","e958f3a3":"# U is othonormal\n# These vectors are orthogonal to one another; form a basis for the reduced space\n\n# each vector is normalized (unit vector)\n# multiply by itself returns 1\ncol1 = np.array([i[0] for i in U])\nprint(col1, '\\n')\nprint('vector 1: {}'.format(round(col1.dot(col1), 2), '\\n'))\n\ncol2 = np.array([i[1] for i in U])\nprint('vector 2: {}'.format(round(col2.dot(col2), 2)))\n\n# and each vector is orthogonal to the other vectors\n# multiply different vectors returns 0\nprint('dot product: {}'.format(round(col1.dot(col2), 2)))","dfc4d388":"# rebuild the original matrix from U,S, and V^T\nA2 = np.dot(U, np.dot(S, VT))\nprint(\"A2 =\\n\", A2.round(2))","9167dbba":"# example of np.zero_like\nnp.zeros_like(S)","d5be4065":"# S_reduced is the same as S but with only the top n elements kept\nS_reduced = np.zeros_like(S)\n\n# only keep top few eigenvalues\neigen_num = 3\nS_reduced[:eigen_num, :eigen_num] = S[:eigen_num,:eigen_num]\n\n# show S_rediced which has less info than original S\nprint(\"S_reduced =\\n\", S_reduced.round(2))","d290a741":"# reduce VT by S_reduced\nS_reduced_VT = np.dot(S_reduced, VT)\nprint(\"S_reduced_VT = \\n\", S_reduced_VT.round(2))","5421f40b":"# each Singular Value vector is a linear combination of original words\nU_S_reduced = np.dot(U, S_reduced)\ndf = pd.DataFrame(U_S_reduced.round(2))\n\n# show colour coded so it is easier to see significant word contributions to a topic\ndf.style.background_gradient(cmap=plt.get_cmap('Blues'))","6d5a8907":"# recreate using sklearn (explained below)\nfrom sklearn.decomposition import TruncatedSVD\ntsvd = TruncatedSVD(n_components=3)\ntsvd.fit_transform(df).round(2)","bd16d18a":"from sklearn.decomposition import TruncatedSVD","3088ff98":"# review data\nsvd_data[0]","089fedd6":"# vectorize the text with TFIDF\nvect = TfidfVectorizer(max_features=10)\nfit_vect = vect.fit_transform(svd_data)\npd.DataFrame(fit_vect.toarray(), columns=vect.get_feature_names())","fea2de0b":"# retain one component\ntsvd = TruncatedSVD(n_components=1)\ntsvd.fit_transform(fit_vect)","4d664dd6":"# retain two components\ntsvd = TruncatedSVD(n_components=2)\ntsvd.fit_transform(fit_vect)","6ce2a2a9":"1. ![](https:\/\/s3.amazonaws.com\/nlp.practicum\/svd_truncated_equation.png)","6c19ffbe":"1. ![](https:\/\/s3.amazonaws.com\/nlp.practicum\/svd2.png)","77b0d613":"1. ![](https:\/\/s3.amazonaws.com\/nlp.practicum\/svd4.png)","25d7dc56":"## Dimensionality Reduction and Semantic Transformations","b1920d24":"1. ![](https:\/\/s3.amazonaws.com\/nlp.practicum\/svd_vs_pca.png)","f9e27c91":"![](https:\/\/s3.amazonaws.com\/nlp.practicum\/reduced_svd.png)","9b8b31a6":"### Principal Component Analysis (PCA)\n\nThe sheer size of data in the modern age is not only a challenge for computer hardware but also a main bottleneck for the performance of many machine learning algorithms. The main goal of a PCA analysis is to identify patterns in data; PCA aims to detect the correlation between variables. If a strong correlation between variables exists, the attempt to reduce the dimensionality only makes sense. In a nutshell, this is what PCA is all about: Finding the directions of maximum variance in high-dimensional data and project it onto a smaller dimensional subspace while retaining most of the information.\n\n- Principal Component Analysis (PCA) is a dimension-reduction tool that can be used to reduce a large set of variables to a small set that still contains most of the information in the large set.\n- Principal component analysis (PCA) is a mathematical procedure that transforms a number of (possibly) correlated variables into a (smaller) number of uncorrelated variables called principal components. \n- PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance\n- The first principal component accounts for as much of the variability in the data as possible, and each succeeding component accounts for as much of the remaining variability as possible.\n\nSOURCE: \n- http:\/\/sebastianraschka.com\/Articles\/2015_pca_in_3_steps.html\n- ftp:\/\/statgen.ncsu.edu\/pub\/thorne\/molevoclass\/AtchleyOct19.pdf","a8218071":"##### Recreate PCA through SVD","c74a3bba":"##### Coursera Course on SVD\n\nSOURCE: https:\/\/www.coursera.org\/learn\/matrix-factorization\/lecture\/K5NBy\/singular-value-decomposition","c6912418":"##### Limitations of the vector space model:\n\nWe generally do not want to feed a large number of features directly into a machine learning algorithm because:\n- They are expensive to store.\n- They slow down computations (e.g. in algorithms like k nearest neighbors)\n- Large samples are required to avoid overfitting.\n\n\n**Synonymy:** the characteristic of language to have several terms that mean essentially the same thing\n-  In the SAS technical support data set the terms \u201cfrozen\u201d and \u201changs\u201d often refer to the same situation where the program has reached a point where nothing is happening and yet the user cannot continue working\n\n**Polysemy:** is the tendency for the same term to mean different things in different contexts.\n- The term \u201cmonitor\u201d in technical support data is a good example of this. At times it refers to the computer screen, sometimes it refers to a piece of software that displays a graphical result and still other times it refers to the user \u201cwatching\u201d or \u201cobserving\u201d an event\n\n**Term dependence:** refers to the tendency for certain terms to be highly correlated with one another. This problem is not unique to text but also occurs with most other sets of data as well. \n- The terms \u201cerror\u201d and \u201cmessage\u201d are strongly correlated in the technical support collection. When one occurs, the other also tends to occur. A pair of documents, each containing these two terms, may have their similarity overrated in this case\n\nSOURCE: \n- https:\/\/davidrosenberg.github.io\/ml2015\/docs\/13.Lab.PCA-SVD-LDA.pdf\n- ftp:\/\/ftp.sas.com\/techsup\/download\/EMiner\/TamingTextwiththeSVD.pdf","3f1477d0":"### Latent Semantic Indexing (LSI)\nLatent semantic indexing (LSI) is an indexing and retrieval method that uses a mathematical technique called singular value decomposition (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text. LSI is based on the principle that words that are used in the same contexts tend to have similar meanings. A key feature of LSI is its ability to extract the conceptual content of a body of text by establishing associations between those terms that occur in similar contexts.\n\nThe method, also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches. Queries, or concept searches, against a set of documents that have undergone LSI will return results that are conceptually similar in meaning to the search criteria even if the results don\u2019t share a specific word or words with the search criteria.\n\n##### Overview\nLSA can use a term-document matrix which describes the occurrences of terms in documents; it is a sparse matrix whose rows correspond to terms and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is tf-idf (term frequency\u2013inverse document frequency): the weight of an element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance.\n\nThis matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used.\n\n##### Rank Lowering\nAfter the construction of the occurrence matrix, LSA finds a low-rank approximation to the term-document matrix. There could be various reasons for these approximations:\n\n**The original term-document matrix is presumed too large for the computing resources:** in this case, the approximated low rank matrix is interpreted as an approximation (a \"least and necessary evil\").\n**The original term-document matrix is presumed noisy:** for example, anecdotal instances of terms are to be eliminated. From this point of view, the approximated matrix is interpreted as a de-noisified matrix (a better matrix than the original).\nThe original term-document matrix is presumed overly sparse relative to the \"true\" term-document matrix. That is, the original matrix lists only the words actually in each document, whereas we might be interested in all words related to each document\u2014generally a much larger set due to synonymy.\nThe consequence of the rank lowering is that some dimensions are combined and depend on more than one term:\n\n{(car), (truck), (flower)} --> {(1.3452 * car + 0.2828 * truck), (flower)}\n\nThis mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also mitigates the problem with polysemy, since components of polysemous words that point in the \"right\" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense.\n\n\n##### Use Cases\n- Compare the documents in the low-dimensional space (data clustering, document classification).\n- Find similar documents across languages, after analyzing a base set of translated documents (cross language retrieval).\n- Find relations between terms (synonymy and polysemy).\n- Given a query of terms, translate it into the low-dimensional space, and find matching documents (information retrieval).\n- Find the best similarity between small groups of terms, in a semantic way (i.e. in a context of a knowledge corpus), as for example in multi choice questions MCQ answering model.\n- Expand the feature space of machine learning \/ text mining systems \n- Analyze word association in text corpus \n\nSynonymy and polysemy are fundamental problems in natural language processing:\n* **Synonymy** is the phenomenon where different words describe the same idea. Thus, a query in a search engine may fail to retrieve a relevant document that does not contain the words which appeared in the query. For example, a search for \"doctors\" may not return a document containing the word \"physicians\", even though the words have the same meaning.\n\n* **Polysemy** is the phenomenon where the same word has multiple meanings. So a search may retrieve irrelevant documents containing the desired words in the wrong meaning. For example, a botanist and a computer scientist looking for the word \"tree\" probably desire different sets of documents.\n\n##### Limitations\nLSA cannot capture polysemy (i.e., multiple meanings of a word) because each occurrence of a word is treated as having the same meaning due to the word being represented as a single point in space. For example, the occurrence of \"chair\" in a document containing \"The Chair of the Board\" and in a separate document containing \"the chair maker\" are considered the same. The behavior results in the vector representation being an average of all the word's different meanings in the corpus, which can make it difficult for comparison. However, the effect is often lessened due to words having a predominant sense throughout a corpus (i.e. not all meanings are equally likely).\n\nSOURCE: https:\/\/en.wikipedia.org\/wiki\/Latent_semantic_analysis#cite_note-38","f4ebec90":"1. ![](https:\/\/s3.amazonaws.com\/nlp.practicum\/truncated_svd.png)","05f96596":"### Singular Value Decomposition\n\nIn linear algebra, the singular-value decomposition (SVD) is a factorization of a real or complex matrix.\n\nLet A be an m \u00d7 n term-document frequency matrix with rank r, r \u2264 n. Without loss of generality let m \u2265 n hold so that there are more terms than documents. The singular value decomposition of A can be stated succinctly as A = U*\u03a3*VT,\n\n- U is an m \u00d7 r orthogonal matrix whose columns make up the left singular vectors\n- \u03a3 is an r \u00d7 r dimensional diagonal matrix whose diagonal elements are termed singular values\n- V is an r \u00d7 n orthogonal matrix whose columns form the right singular vectors of A.","7a8901a1":"### SVD Example","2e75e687":"The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the \u201ccore\u201d of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes.","dfae1678":"##### When is the SVD Technique Appropriate?\nFor most Text Mining problems, the SVD will be entirely appropriate to use. Without a data reduction technique, there will be more variables (terms) available than one can use in a data mining model. Some method must be applied to select an appropriate set from which a text mining solution can be built. Unlike term elimination, the SVD technique allows one to derive significantly fewer variables from the original variables. There are some drawbacks to using the SVD, however. Computationally, the SVD is fairly resource intensive and requires a large amount of RAM. The user must have access to these resources in order for the decomposition to be obtained.\n\n##### How Many Dimensions Should be Used?\nThe choice for the number of dimensions k to use can be a crucial aspect of many text mining solutions. With too few dimensions, the model will fail to explain prominent relationships in the text. On the other hand, using too many dimensions will add unnecessary noise to the model and make training an effective model nearly impossible. In practice, there is an upper bound of at most a few hundred dimensions from which to build a model. So the user should not need to consider more than this","c0cd8805":"# Dimensionality Reduction \n\n##### Author: Alex Sherman | alsherman@deloitte.com ; Sai Revannth Vedala | revedala@deloitte.com\n\n\n##### Agenda\n- PCA\n- SVD\n- Latent semantic indexing (LSI\/LSA)\n- Latent dirichlet allocation (LDA)","babe1636":"![](https:\/\/s3.amazonaws.com\/nlp.practicum\/svd_graph.png)","44f522e6":"##### SVD approximation equation\nEquation 1 implies that one can get a rough approximation to A by taking the product of the first singular value with the matrix formed from the outer product of the first column of U with the first column of V . The matrix formed, A1, will be m \u00d7 n but will be of only rank one. Of all possible matrices, B, of rank one, ||A \u2212 B||2 will be smallest when B = A1 holds. One can improve the approximation by forming the product of the second singular value with the outer product of the second columns of U and V , and then adding this result to A1. The resultant matrix, A2, will be the the best rank-two approximation to A. The approximations can be successively improved by repeating the process until k = r holds and the original matrix is produced.","809c94af":"### Reduced SVD\n\nThe SVD comes in two forms, a full SVD, and a reduced SVD. In NLP, we tend to focus on the reduced SVD, using SVD for dimensionality reduction.\n\nSVD can be viewed as a sum of rank one matrices.The matrix A can then be approximated by choosing any k \u2264 r. This generates a rank k matrix, Ak, that is the best rank-k approximation to A in terms of least-squares best fit\n\nSOURCE: ftp:\/\/ftp.sas.com\/techsup\/download\/EMiner\/TamingTextwiththeSVD.pdf","c85b9f4b":"# LDA (Latent dirichlet allocation)","effb7149":"###### IF TIME PERMITS ","b42a2ed1":"![](https:\/\/s3.amazonaws.com\/nlp.practicum\/pca.png)","67c59661":"The normal distribution is a probability distribution over all the real numbers. It is described by a mean and a variance. The mean is the expected value of this distribution, and the variance tells us how much we can expect samples to deviate from the mean. If the variance is very high, then you\u2019re going to see values that are both much smaller than the mean and much larger than the mean. If the variance is small, then the samples will be very close to the mean. If the variance goes close to zero, all samples will be almost exactly at the mean.\n\nThe dirichlet distribution is a probability distribution as well - but it is not sampling from the space of real numbers. Instead it is sampling over a probability simplex.\n\nAnd what is a probability simplex? It\u2019s a bunch of numbers that add up to 1. For example:\n\n- (0.6, 0.4)\n- (0.1, 0.1, 0.8)\n- (0.05, 0.2, 0.15, 0.1, 0.3, 0.2)\n\nThese numbers represent probabilities over K distinct categories. In the above examples, K is 2, 3, and 6 respectively. That\u2019s why they are also called categorical distributions.\n\nWhen we are dealing with categorical distributions and we have some uncertainty over what that distribution is, simplest way to represent that uncertainty as a probability distribution is the Dirichlet.\n\nSOURCE: \n- [What is an intuitive explanation of the Dirichlet distribution?](https:\/\/www.quora.com\/What-is-an-intuitive-explanation-of-the-Dirichlet-distribution)\n- [VIDEO: Digging into the Dirichlet Distribution](https:\/\/www.hakkalabs.co\/articles\/the-dirichlet-distribution)\n- [Introduction to Latent Dirichlet Allocation:](http:\/\/blog.echen.me\/2011\/08\/22\/introduction-to-latent-dirichlet-allocation\/)\n- [Gensim LDA: Tips and Tricks](https:\/\/miningthedetails.com\/blog\/python\/lda\/GensimLDA\/)","e7d1c926":"Dimensionality reduction using TruncatedSVD (aka LSA).\n\nThis transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently.\n\nIn particular, truncated SVD works on term count\/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA).\n\nSOURCE: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.TruncatedSVD.html","d40e5998":"##### Manually create the first singular value","a38195b4":"### SVD in comparision to PCA\nUsing the below image displaying several documents that contain only two terms (A&B)\n\n**SVD:** draw a line through the points in such a way that the sum of the distances from each point to the line is minimized. The documents can then be perpendicularly projected onto this new line. The circles indicate the locations of the projected documents. This line and the new locations for the documents, can be obtained by using the SVD\n\n**PCA:** Spread the points out as much as possible on this new line to maximize the variance of the points that are projected onto the line. This line is formed by performing Principal Component Analysis\n\nAlthough based on equivalent procedures, since PCA and TM\u2019s SVD approach operate on different data, they do not produce the same results. Depending on whether the raw data is used or the covariance matrix is used, different vectors will be found as basis vectors for the reduced space\n\nWhile the PCA component maximizes the variance, the SVD finds the best fitting line in the least-squares sense. Depending on the nature of the data, these two lines may or may not be fairly close to one another. ","4dae9c63":"\"Singlular Value Decomposition (SVD) allows us to reduce the dimensionality of a matrix. Instead of analyzing a full document-term matrix with all documents and all terms, we can reduce the matrix into a lower rank representation. In this, we combine the meaning of terms by compressing the number of columns.\n\nTo reduce the size of our matrix without losing much quality, we can perform a low-rank approximation on matrix C. This is done by keeping the top k values of \u03a3 and setting the rest to zero, where k is the new rank. Since \u03a3 contains eigenvalues in descending order, and the effect of small eigenvalues on matrix products is small, the zeroing of the lowest values will leave the reduced matrix C' approximate to C. How to retrieve the most optimal k is not an easy task, since we want k top large enough to include as much variety as possible from our original matrix C, but small enough to exclude sampling errors and redundancy. To do this in a formal way, the Frobenius norm can be applied to measure the discrepancy between C and C_k. A less extensive way is just to try out a couple of different k-values and see what generates the best results.\"\n\nSOURCE: https:\/\/simonpaarlberg.com\/post\/latent-semantic-analyses\/","3258c7e7":"1. ![](https:\/\/s3.amazonaws.com\/nlp.practicum\/svd_vs_pca2.png)"}}