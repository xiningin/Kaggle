{"cell_type":{"213e0561":"code","e5a7675c":"code","bef0ae98":"code","3c84057a":"code","444002d1":"code","beeca070":"code","679542e6":"code","ac3d9376":"code","4a3c6367":"code","f4e496c4":"code","bf9f4aa8":"code","968b612e":"code","f7a715f8":"code","3b4cb970":"code","272510fd":"code","89395818":"code","00543f88":"code","b50519cc":"code","1b7c7562":"code","93035e4e":"code","5af9e490":"code","5e114e99":"code","40cf8d4b":"code","f1534d2b":"code","e568aab1":"code","aa202191":"code","adfc9602":"code","ee2f99be":"code","8694f43b":"code","5602f84f":"code","91f2952b":"code","88fed0b7":"code","6740f03a":"code","9c03b371":"code","d0b3fce0":"code","26ab5592":"code","66b65e07":"code","18917d00":"code","1797dab8":"code","e9211f31":"code","1b53e527":"code","addd4442":"code","5a89a3a5":"code","31e5e95e":"code","7829a58b":"code","03349722":"code","a904fd65":"code","bd60d74b":"code","34f533a2":"code","6941af72":"code","4e69c654":"markdown","ffc8119d":"markdown","43fc0378":"markdown","eb10bfe7":"markdown","da5abf9c":"markdown","d0208c89":"markdown","af7731e2":"markdown","1d52ea56":"markdown","6c312464":"markdown","fd1b1806":"markdown","1169c63e":"markdown","cc7ec551":"markdown","8d37ca1b":"markdown","6156f9d5":"markdown"},"source":{"213e0561":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\n\npd.options.display.max_colwidth = 200\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames[:5]:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e5a7675c":"!pip install https:\/\/med7.s3.eu-west-2.amazonaws.com\/en_core_med7_lg.tar.gz","bef0ae98":"# We read all the medical notes from the directory\ndir = '\/kaggle\/input\/nlp-specialization-data\/Medical_Notes\/Medical_Notes'\nprint (\"Total {} files in directory\".format(len(os.listdir(dir))))","3c84057a":"labels = pd.read_csv(\"\/kaggle\/input\/nlp-specialization-data\/Labels_Medical_Notes.csv\",header=None)\nlabels.columns = ['file','label']","444002d1":"labels.head(5)","beeca070":"labels.shape[0]","679542e6":"labels['label'].value_counts()","ac3d9376":"\"-\".join([\"Rahul\",\"Aggarwal\",\"HERE\"])","4a3c6367":"# read each medical notes and the corresponding label (disease category)\ntexts = []\nclasses = []\n\nfor i in tqdm(range(labels.shape[0])):\n    filename = os.path.join(dir,labels.iloc[i]['file'])\n    text = \" \".join(open(filename,'r',errors='ignore').readlines())\n    texts.append(text)\n    classes.append(labels.iloc[i]['label'])\n    \ndata = pd.DataFrame()\ndata['text'] = texts\ndata['label'] = classes","f4e496c4":"print (data.shape)","bf9f4aa8":"data.head(5)","968b612e":"data.sample(10)","f7a715f8":"sample_text = data.text.iloc[3]\nprint (sample_text)","3b4cb970":"import re","272510fd":"def remove_html(text):\n    text = text.replace(\"\\n\",\" \")\n    pattern = re.compile('<.*?>') #all the HTML tags\n    return pattern.sub(r'', text)","89395818":"sample_text_processed = remove_html(sample_text)\nprint (sample_text_processed)","00543f88":"def remove_headings(text):\n    pattern = re.compile('\\w+:')\n    return pattern.sub(r'', text)","b50519cc":"sample_text_processed = remove_headings(sample_text_processed)\nprint (sample_text_processed)","1b7c7562":"def replace_mult_spaces(text):\n    text = text.replace(\"&quot\",\"\")\n    pattern = re.compile(' +')\n    text = pattern.sub(r' ', text)\n    text = text.strip()\n    return text","93035e4e":"replace_mult_spaces(\"Rahul     Aggarwal\")","5af9e490":"string=\" Rahul \"\nstring","5e114e99":"string.strip()","40cf8d4b":"sample_text_processed = replace_mult_spaces(sample_text_processed)\nprint (sample_text_processed)","f1534d2b":"# read.csv(r\"download\/txct.csv\")","e568aab1":"def replace_other_chars(text):\n    pattern = re.compile(r'[()!@&;]')\n    text = pattern.sub(r'', text)\n    return text","aa202191":"sample_text_processed = replace_other_chars(sample_text_processed)\nprint (sample_text_processed)","adfc9602":"def clean_text(text):\n    text = remove_html(text)\n    text = remove_headings(text)\n    text = replace_mult_spaces(text)\n    text = replace_other_chars(text)\n    text = text.lower()\n    return text","ee2f99be":"data","8694f43b":"data['clean_text'] = data.text.apply(clean_text)\ndata","5602f84f":"import matplotlib.pyplot as plt\n\ndata.clean_text.apply(len).plot.hist()\ndata.text.apply(len).plot.hist()\nplt.title(\"Distribution of total number of characters in the clinical notes\")\nplt.legend([\"before cleaning\",\"after cleaning\"])\nplt.show()","91f2952b":"data.clean_text.apply(lambda x: len(x.split())).plot.hist()\ndata.text.apply(lambda x: len(x.split())).plot.hist()\nplt.title(\"Distribution of total number of words in the clinical notes\")\nplt.legend([\"before cleaning\",\"after cleaning\"])\nplt.show()","88fed0b7":"sample_text = data.clean_text.iloc[1]\nprint (sample_text)","6740f03a":"import nltk\n\ndef simple_stemmer(text):\n    ps = nltk.stem.SnowballStemmer('english')\n    text = ' '.join([ps.stem(word) for word in text.split()])\n    return text","9c03b371":"stemmed_text = simple_stemmer(sample_text)\nprint (stemmed_text)","d0b3fce0":"\" \".join([\"Rahul\",\"Aggarwal\",\"is\",\"a\",\"trainer\"])","26ab5592":"import spacy\nimport en_core_med7_lg #en_core_web_sm\n\nnlp = en_core_med7_lg.load()\n\ndef simple_lemmatizer(text):\n    text = nlp(text)\n    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n    return text","66b65e07":"test_text=nlp(\"preoper ganglion of the left wrist. postop ganglion of the left wrist. excis of ganglion. general. estim blood less than 5 ml\")\ntest_text","18917d00":"for word in test_text:\n    print(word.lemma_)","1797dab8":"sample_text","e9211f31":"lemmatized_text = simple_lemmatizer(sample_text)\nprint (lemmatized_text)","1b53e527":"sample_text = data.clean_text.iloc[1]\ndoc = nlp(sample_text)\nfor token in doc:\n    print(token.text, token.pos_)","addd4442":"pd.Series(\" \".join(data.clean_text.values).split()).value_counts().head(20)","5a89a3a5":"stopword_list = nltk.corpus.stopwords.words('english')\n\nprint (stopword_list[:10])","31e5e95e":"def lemmatize_and_remove_stopwords(text):\n    doc = nlp(text)\n    tokens = [word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in doc]\n    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text","7829a58b":"sample_text","03349722":"sample_text_processed = lemmatize_and_remove_stopwords(sample_text)\nprint (sample_text_processed)","a904fd65":"data.clean_text = data.clean_text.apply(lemmatize_and_remove_stopwords)","bd60d74b":"data.clean_text.apply(lambda x: len(x.split())).plot.hist()\nimport matplotlib.pyplot as plt\nplt.title(\"Distribution of total number of words in the texts\")\nplt.show()","34f533a2":"data","6941af72":"#data.to_csv(\"clinical_notes_cleaned.csv\",index=False)","4e69c654":"<a id='datasetreading'><\/a>\n\n### Read the dataset\n\nThe dataset is collected from https:\/\/www.kaggle.com\/c\/medical-notes\/data. It contains 800 anonymised transcribed medical reports with the disease category (specialty). For more information browse original source - https:\/\/www.mtsamples.com\/","ffc8119d":"<a id='stopword'><\/a>\n\n### Stop word removal\n\nLet us first see the most frequent words in the dataset","43fc0378":"### References for further reading\n\n<strong> NLP overview - <\/strong> https:\/\/towardsdatascience.com\/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n\n<strong> Regular Expressions - <\/strong> https:\/\/regex101.com\/ \n\n<strong> Spacy - <\/strong> https:\/\/spacy.io\/usage\/spacy-101\n\n<strong> NLTK - <\/strong> https:\/\/www.nltk.org\/book\/\n\n","eb10bfe7":"Top 10 words based on frequency are english words like - articles, conjuctions, prepositions etc. These words often do not play in significant roles in the downstream applications. We need to remove these words to reduce the model complexity.","da5abf9c":"<a id='stemming'><\/a>\n\n### Stemming and Lemmatization\n\nStemming changes word into its root stem. \n\n<img src = https:\/\/miro.medium.com\/max\/359\/1*l65c30sY9fQsWPKIckqmCQ.png>\n\nHowever, the root stem may not be lexicographically a correct word. Lemmatization on the other hand standardizes a word into its root word. Lemmatization deals with higher level of abstraction.\n\n<img src = https:\/\/devopedia.org\/images\/article\/227\/6785.1570815200.png>\n","d0208c89":"<a id='regex'><\/a>\n\n### Basic data cleaning\n\nNatural language in its pure form can bring lot of noise. We need to clean the data in order to use any statistical\/machine learning model. Below are the few techniques for cleaning the text data.\n\n* Using RegEx (regular expressions) to identify the irrelevant text sections for removal\n* Standardizing\/normalizing texts like - abbreviations, spelling mistakes\n* For social media data - remove smileys, email ids if these information are not relevant for downstream analysis\n","af7731e2":"<a id='eda'><\/a>\n\n### Basic descriptive analysis on the texts","1d52ea56":"<a id='tokenization'><\/a>\n\n### Tokenization\n\nTokenization splits a text into tokens or, words. Typically, words are splitted based on blank spaces. But tokenizations can also split words joined by other characters.","6c312464":"We remove all the special characters like - \"\\n\", HTML tags from the texts","fd1b1806":"Putting everything together in a function and apply the cleaning on all the texts. Further, convert everything into lower case.","1169c63e":"Remove &quot marks and other characters. Replace multiple spaces with single space","cc7ec551":"## Contents\n\n* [Read the dataset](#datasetreading)\n* [Data Cleaning](#regex)\n* [Stemming & Lemmatization](#stemming)\n* [Tokenization](#tokenization)\n* [Stop word removal](#stopword)","8d37ca1b":"remove multiple consecutive spaces and replace with single space","6156f9d5":"Remove all the headings from text"}}