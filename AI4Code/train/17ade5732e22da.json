{"cell_type":{"d9c3c0e2":"code","0e4019f4":"code","209f3731":"code","b6538701":"code","fab879a8":"code","65d5606f":"code","3093c251":"code","e07a81f5":"code","2766a903":"code","e2bfacc0":"code","ccf40af2":"code","38285b94":"code","093f4a3a":"code","6c255119":"code","f77c74e6":"code","335b04cf":"code","78a976b2":"code","cb86d7ec":"code","91c2b5b5":"code","4f5b4360":"code","0b809228":"code","91caf16d":"code","289bbfbb":"code","37620f21":"code","99682b31":"code","81590137":"code","72be45c3":"code","9a80ead5":"code","e421b3ab":"code","2dbd55d8":"code","80e7e989":"code","cc3ac901":"code","09e10c4d":"code","8c293522":"code","f2053ffe":"code","5265214e":"code","6888bed6":"code","13280e98":"code","3d11641a":"code","eba2aa98":"code","1daf9ccf":"code","4200accd":"code","9a94e7e9":"code","522a8c57":"code","b7cef872":"code","10ef64a0":"code","c8fd51c9":"code","85f53b7f":"code","e9061089":"code","f033aba0":"code","4e269876":"code","783af343":"code","d9201f7f":"code","f824ebb9":"code","30490736":"code","e9df57c2":"code","973667c5":"code","ab0b9207":"markdown","c2f6aef1":"markdown","08c8417b":"markdown","a984c0b7":"markdown","ba7ba1be":"markdown","e613b114":"markdown","47558b59":"markdown","57f7d6c4":"markdown","62709970":"markdown","87f3e931":"markdown","98d27d5f":"markdown","4c226fa6":"markdown","115aaa9d":"markdown","57e45906":"markdown","42caac01":"markdown","1d99e26e":"markdown","e2d66e8c":"markdown","36b272ad":"markdown","50df1b4f":"markdown","a0ec39e4":"markdown","69bd4f14":"markdown","fde88c4b":"markdown","3c8cfaaf":"markdown","4311869c":"markdown","42160002":"markdown","e4ca1b66":"markdown","149a053f":"markdown"},"source":{"d9c3c0e2":"# Import all the libraries which will be needed later\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error, median_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.utils import shuffle\n\n%matplotlib inline","0e4019f4":"df_listings = pd.read_csv(\"..\/input\/munich-airbnb-data\/listings.csv\");\ndf_calendar = pd.read_csv(\"..\/input\/munich-airbnb-data\/calendar.csv\");\ndf_reviews = pd.read_csv(\"..\/input\/munich-airbnb-data\/reviews.csv\");","209f3731":"# Take a look at a concise summary of the DataFrame 'calendar'\ndf_calendar.info()","b6538701":"# Show the first rows of the dataframe\ndf_calendar.head()","fab879a8":"# List all features in this data set and show the number of missing values\nobj = df_calendar.isnull().sum()\nfor key,value in obj.iteritems():\n    percent = round((value * 100 \/ df_calendar['listing_id'].index.size),3)\n    print(key,\", \",value, \"(\", percent ,\"%)\")","65d5606f":"# Show the shape of the dataframe\ndf_calendar.shape","3093c251":"# Take a closer look at a concise summary of the DataFrame 'listings'\ndf_listings.info()","e07a81f5":"# Show the first rows of the dataframe\ndf_listings.iloc[:,100:120].head()","2766a903":"# List all features in this data set and show the number of missing values\nobj = df_listings.isnull().sum()\nfor key,value in obj.iteritems():\n    percent = round((value * 100 \/ df_listings['id'].index.size),3)\n    print(key,\", \",value, \"(\", percent ,\"%)\")","e2bfacc0":"# Show distinct observations per feature and absolute frequency\ndf_listings[\"zipcode\"].value_counts()","ccf40af2":"# Show the shape of the dataframe\ndf_listings.shape","38285b94":"# Count distinct observations per feature\ndf_listings.nunique()","093f4a3a":"# Take a look at a concise summary of the DataFrame 'reviews'\ndf_reviews.info()","6c255119":"# Show the first rows of the dataframe\ndf_reviews.head()","f77c74e6":"# List all features in this data set and show the number of missing values\nobj = df_reviews.isnull().sum()\nfor key,value in obj.iteritems():\n    percent = round((value * 100 \/ df_reviews['id'].index.size),3)\n    print(key,\", \",value, \"(\", percent ,\"%)\")","335b04cf":"# Show the shape of the dataframe\ndf_reviews.shape","78a976b2":"# Copy the data to a new DataFrame for further clean up\ndf_listings_clean = df_listings.copy(deep=True)","cb86d7ec":"# Clean up the data set \"listings\" as the previous analysis pointed out\n\n# Drop features which are not used further \nfeatures_to_drop = ['listing_url', 'picture_url','host_url', 'host_thumbnail_url', 'host_picture_url',\n                    'name', 'summary', 'space', 'neighborhood_overview', 'transit', 'interaction', 'description',\n                    'host_name', 'host_location', 'host_neighbourhood', 'street', 'last_scraped',\n                    'calendar_last_scraped', 'first_review', 'last_review', 'host_since', 'calendar_updated',\n                    'experiences_offered', 'state', 'country', 'country_code', 'city', 'market',\n                    'host_total_listings_count', 'smart_location']\ndf_listings_clean.drop(features_to_drop, axis=1, inplace=True)","91c2b5b5":"# Remove constant features by finding unique values per feature \ndf_listings_clean = df_listings_clean[df_listings_clean.nunique().where(df_listings_clean.nunique()!=1).dropna().keys()]\n\n# Drop features with 50% or more missing values\nmore_than_50 = list(df_listings_clean.columns[df_listings_clean.isnull().mean() > 0.5])\ndf_listings_clean.drop(more_than_50, axis=1, inplace=True)\n\n# Clean up the format values. Maybe not the best solution - but will do the job.\ndf_listings_clean['price'] = df_listings_clean['price'].replace('[\\$,]', '', regex=True).astype(float)\ndf_listings_clean['extra_people'] = df_listings_clean['extra_people'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\ndf_listings_clean['cleaning_fee'] = df_listings_clean['cleaning_fee'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n        \n# Convert rates type from string to float and remove the % sign\ndf_listings_clean['host_response_rate'] = df_listings_clean['host_response_rate'].str.replace('%', '').astype(float)\ndf_listings_clean['host_response_rate'] = df_listings_clean['host_response_rate'] * 0.01\n    \n# Covert boolean data from string data type to boolean\nboolean_features = ['instant_bookable', 'require_guest_profile_picture', \n                'require_guest_phone_verification', 'is_location_exact', 'host_is_superhost', 'host_has_profile_pic', \n                'host_identity_verified']\ndf_listings_clean[boolean_features] = df_listings_clean[boolean_features].replace({'t': True, 'f': False})\n\n## Fill numerical missing data with mean value\nnumerical_feature = df_listings_clean.select_dtypes(np.number)\nnumerical_columns = numerical_feature.columns\n\nimp_mean = SimpleImputer(missing_values = np.nan, strategy = 'mean')\nimp_mean = imp_mean.fit(numerical_feature)\n\ndf_listings_clean[numerical_columns] = imp_mean.transform(df_listings_clean[numerical_columns])\n     \n# Remove all remaining missing values  \ndf_listings_clean.dropna(inplace=True)","4f5b4360":"# Show mean price for each neighbourhood_cleansed\ndf_listings_clean.groupby([\"neighbourhood_cleansed\"])[\"price\"].describe().sort_values(\"mean\", ascending=False)","0b809228":"# Create new feature 'mean' with the mean price per neighbourhood\ndf_listings_clean['mean'] = df_listings_clean.groupby('neighbourhood_cleansed')['price'].transform(lambda r : r.mean())","91caf16d":"# Create plot for mean price per neighbourhood \ndf_listings_plot = df_listings_clean\ndf_listings_plot = df_listings_plot.groupby('neighbourhood_cleansed')[['price']].mean()\ndf_listings_plot = df_listings_plot.reset_index()\ndf_listings_plot = df_listings_plot.sort_values(by='price',ascending=False)\ndf_listings_plot.plot.bar(x='neighbourhood_cleansed', y='price', color='blue', rot=90, figsize = (20,10)).set_title('Mean Price per city district (Neighbourhood)');","289bbfbb":"# Since we also have the geo data (latitude and longitude) of the apartments we can create a map\nfig = px.scatter_mapbox(df_listings_clean, color=\"mean\", lat='latitude', lon='longitude',\n                        center=dict(lat=48.137154, lon=11.576124), zoom=10,\n                        mapbox_style=\"open-street-map\",width=1000, height=800);\nfig.show()","37620f21":"# Create bins for mean price\nbins = [0,25,50,75,100,125,150,175,200,225,250,275,300,325,350,375,400]\ndf_listings_clean['binned'] = pd.cut(df_listings_clean['mean'], bins)\ndf_listings_clean","99682b31":"df_listings_clean[\"latitude\"].describe()","81590137":"df_listings_clean[\"longitude\"].describe()","72be45c3":"# Create squares for langitude and latitude\nstep = 0.0025\nto_bin = lambda x: np.floor(x \/ step) * step\ndf_listings_clean[\"latbin\"] = df_listings_clean.latitude.map(to_bin)\ndf_listings_clean[\"longbin\"] = df_listings_clean.longitude.map(to_bin)\n#groups = df_listings_clean.groupby((\"latbin\", \"longbin\"))","9a80ead5":"# Create df_squares\ndf_squares = df_listings_clean[['neighbourhood_cleansed','zipcode','latitude','longitude','latbin','longbin','mean']]\ndf_squares","e421b3ab":"# Show creates squares on map\nfig = px.scatter_mapbox(df_listings_clean, color=\"mean\", lat='latbin', lon='longbin',\n                        center=dict(lat=48.137154, lon=11.576124), zoom=10, \n                        mapbox_style=\"open-street-map\",width=1000, height=800);\nfig.show()","2dbd55d8":"# Create dataset with octoberfest coordinates  \noct_data = {\n        'id': ['Octoberfest',],\n        'latitude': ['48.131'],\n        'longitude': ['11.550']}\ndf_oct = pd.DataFrame(oct_data, columns = ['id', 'latitude', 'longitude'])\ndf_oct","80e7e989":"# Show place where octoberfest is located    \nfig = px.scatter_mapbox(df_oct, lat='latitude', lon='longitude',\n                        center=dict(lat=48.137154, lon=11.576124), zoom=10,\n                        mapbox_style=\"open-street-map\",width=1000, height=800);\nfig.show()","cc3ac901":"### Getting distance between two points based on latitude\/longitude\nfrom math import sin, cos, sqrt, atan2, radians\n\n# approximate radius of earth in km\nR = 6373.0\n\n# places\nlat_oct = radians(48.131)\nlon_oct = radians(11.550)\nlat_room = radians(48.14)\nlon_room = radians(11.58)\n\ndlon = lon_room - lon_oct\ndlat = lat_room - lat_oct\n\na = sin(dlat \/ 2)**2 + cos(lat_oct) * cos(lat_room) * sin(dlon \/ 2)**2\nc = 2 * atan2(sqrt(a), sqrt(1 - a))\n\ndistance = R * c * 1000\ndistance = round(distance, 2)\n\nprint(\"Result:\", distance,\"meters\")","09e10c4d":"# create copy\ndf_squares_oct = df_squares.copy()\ndf_squares_oct","8c293522":"### Getting distance between two points based on latitude\/longitude\nfrom math import sin, cos, sqrt, atan2, radians\n\n# approximate radius of earth in km\nR = 6373.0\n\n# insert octoberfest coordinates and \ndf_squares_oct['lat_oct'] = 48.131\ndf_squares_oct['long_oct'] = 11.550\n\n# convert to radians\ndf_squares_oct['latbin_rad'] = np.radians(df_squares_oct['latbin'])\ndf_squares_oct['longbin_rad'] = np.radians(df_squares_oct['longbin'])\ndf_squares_oct['lat_oct_rad'] = np.radians(df_squares_oct['lat_oct'])\ndf_squares_oct['long_oct_rad'] = np.radians(df_squares_oct['long_oct'])\n\ndf_squares_oct['dlon'] = df_squares_oct['longbin_rad'] - df_squares_oct['long_oct_rad']\ndf_squares_oct['dlat'] = df_squares_oct['latbin_rad'] - df_squares_oct['lat_oct_rad']\n\ndf_squares_oct['a']  = np.sin(df_squares_oct['dlat'] \/ 2)**2 + np.cos(df_squares_oct['lat_oct_rad']) * np.cos(df_squares_oct['latbin_rad']) * np.sin(df_squares_oct['dlon'] \/ 2)**2\n\ndf_squares_oct['c'] = 2 * np.arctan2(np.sqrt(df_squares_oct['a']), np.sqrt(1 - df_squares_oct['a']))\n\ndf_squares_oct['distance_meter'] = R * df_squares_oct['c'] * 1000\ndf_squares_oct['distance_meter'] = round(df_squares_oct['distance_meter'], 2)\n\ndf_squares_oct","f2053ffe":"# Show mean price for apartments\nfig = px.scatter_mapbox(df_squares_oct, color=\"mean\", lat='latbin', lon='longbin',\n                        center=dict(lat=48.137154, lon=11.576124), zoom=10, \n                        mapbox_style=\"open-street-map\",width=1000, height=800);\nfig.show()\n\n# Show distance to octoberfest\nfig = px.scatter_mapbox(df_squares_oct, color=\"distance_meter\", lat='latbin', lon='longbin',\n                        center=dict(lat=48.137154, lon=11.576124), zoom=10, \n                        mapbox_style=\"open-street-map\",width=1000, height=800);\nfig.show()","5265214e":"# analyze distance\ndf_squares_oct['distance_meter'].describe()","6888bed6":"# bincount distance using linspace\ny = np.linspace(0, 14000, 29) # 500 meter sections\ndf_squares_oct['distance_bins'] = pd.cut(df_squares_oct['distance_meter'], bins=y)\ndf_squares_oct","13280e98":"# Analyze price\ndf_squares_oct['mean'].describe()","3d11641a":"# Bincount price using linspace\nx = np.linspace(80, 170, 19) # 5 euro sections\ndf_squares_oct['mean_bins'] = pd.cut(df_squares_oct['mean'], bins=x)\ndf_squares_oct","eba2aa98":"### Offline-Quoten: 2cat\/1num\n# Create heatmap\ncat_var_1 = \"neighbourhood_cleansed\"\ncat_var_2 = \"distance_bins\"\nnum_var_1 = \"mean\"\n\n# Create heatmap\nplt.figure(figsize=(30, 15))\ncat_means = df_squares_oct.groupby([cat_var_1, cat_var_2]).mean()[num_var_1]\ncat_means = cat_means.reset_index(name = 'avg')\ncat_means = cat_means.pivot(index = cat_var_2, columns = cat_var_1,\n                            values = 'avg')\nsns.heatmap(cat_means, annot = True, fmt = '.4f', cmap=\"coolwarm\",\n           cbar_kws = {'label' : 'mean(avg)'})\n\nplt.title(\"Fig.A: Compare distance to octoberfest in meters, average price for a room according to different districts\", \n          fontsize=12, loc='center', pad=10)","1daf9ccf":"# Relative H\u00e4ufigkeit von Offline-Nacharbeiten i.A. des Modells\nplt.figure(figsize=(20, 10))\nsns.lineplot(x=\"distance_meter\", y=\"mean\", data=df_squares_oct, err_style=None)\nplt.title(\"Fig.B: Compare distance to octoberfest and average price for a room\", fontsize=12, loc='center', pad=10)","4200accd":"# Relative H\u00e4ufigkeit von Offline-Nacharbeiten i.A. des Modells\nplt.figure(figsize=(20, 15))\nsns.lineplot(x=\"distance_meter\", y=\"mean\", hue=\"neighbourhood_cleansed\", data=df_squares_oct, err_style=None)\nplt.title(\"Fig.C: Compare distance to octoberfest and average price for a room with regards to district\", fontsize=12, loc='center', pad=10)","9a94e7e9":"# Copy the data to a new DataFrame for encoding \ndf_listings_encoded = df_listings_clean.copy(deep=True)","522a8c57":"df_listings_encoded","b7cef872":"# Filter dataframe according to TOP 5 city districts in munich in terms of distance-price-ratio \nfilter = ['Sendling','Sendling-Westpark','Untergiesing-Harlaching','Laim','Neuhausen-Nymphenburg']\ndf_listings_encoded = df_listings_encoded[df_listings_encoded.neighbourhood_cleansed.isin(filter)]\ndf_listings_encoded","10ef64a0":"# control filter activities, correct!\ndf_listings_encoded.neighbourhood_cleansed.unique()","c8fd51c9":"# Shape of dataset\ndf_listings_encoded.shape","85f53b7f":"# Encode features for use in machine learing model\n\n# Encode feature 'amenities' and concat the data\ndf_listings_encoded.amenities = df_listings_encoded.amenities.str.replace('[{\"\"}]', \"\")\ndf_amenities = df_listings_encoded.amenities.str.get_dummies(sep = \",\")\ndf_listings_encoded = pd.concat([df_listings_encoded, df_amenities], axis=1) \n\n# Encode feature 'host_verification' and concat the data\ndf_listings_encoded.host_verifications = df_listings_encoded.host_verifications.str.replace(\"['']\", \"\")\ndf_verification = df_listings_encoded.host_verifications.str.get_dummies(sep = \",\")\ndf_listings_encoded = pd.concat([df_listings_encoded, df_verification], axis=1)\n    \n# Encode feature 'host_response_time'\ndict_response_time = {'within an hour': 1, 'within a few hours': 2, 'within a day': 3, 'a few days or more': 4}\ndf_listings_encoded['host_response_time'] = df_listings_encoded['host_response_time'].map(dict_response_time)\n\n# Encode the remaining categorical feature \nfor categorical_feature in ['neighbourhood_cleansed', 'property_type', 'room_type', 'bed_type', 'neighbourhood', \n                            'cancellation_policy']:\n    df_listings_encoded = pd.concat([df_listings_encoded, \n                                     pd.get_dummies(df_listings_encoded[categorical_feature])],axis=1)\n        \n# Drop features\ndf_listings_encoded.drop(['amenities', 'neighbourhood_cleansed', 'property_type', 'room_type', 'bed_type', \n                          'host_verifications', 'neighbourhood','cancellation_policy','security_deposit',\n                          'id', 'host_id', 'mean', 'latitude', 'longitude'],\n                         axis=1, inplace=True)","e9061089":"# Last check if there are any missing values in the data set\nsum(df_listings_encoded.isnull().sum())","f033aba0":"# Shuffle the data to ensure a good distribution\ndf_listings_encoded = shuffle(df_listings_encoded)\n\nX = df_listings_encoded.drop(['price',\"zipcode\",\"binned\"], axis=1)\ny = df_listings_encoded['price']\n\n# Split the data into random train and test subsets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","4e269876":"# Control shape after train-test-split, correct!\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","783af343":"# Initalize the model\nmodel = RandomForestRegressor(max_depth=15, n_estimators=100, criterion='mse', random_state=42)\nmodel","d9201f7f":"# Fit the model on training data\nmodel.fit(X, y)","f824ebb9":"# Predict results\nprediction = model.predict(X_test)\nprediction","30490736":"# Evaluate the result - compare r squared of the training set with the test set\n\n# Find R^2 on training set\nprint(\"Training Set:\")\nprint(\"R_squared:\", round(model.score(X_train, y_train) ,2))\n\n# Find R^2 on testing set\nprint(\"\\nTest Set:\")\nprint(\"R_squared:\", round(model.score(X_test, y_test), 2))","e9df57c2":"# Scatter plot of th actual vs predicted data\nplt.figure(figsize=(10, 10))\nplt.grid()\nplt.xlim((0, 200))\nplt.ylim((0, 200))\nplt.plot([0,200],[0,200], color='#AAAAAA', linestyle='dashed')\nplt.scatter(y_test, prediction, alpha=0.5)\ncoef = np.polyfit(y_test,prediction,1)\npoly1d_fn = np.poly1d(coef) \nplt.plot(y_test, poly1d_fn(y_test))\nplt.title('Actual vs. Predicted data');\nplt.xlabel(\"Actual values\")\nplt.ylabel(\"Predicted values\")\nplt.show()","973667c5":"# Sort the importance of the features\nimportances = model.feature_importances_\n    \nvalues = sorted(zip(X_train.columns, model.feature_importances_), key=lambda x: x[1] * -1)\nfeature_importances = pd.DataFrame(values, columns = [\"feature\", \"score\"])\nfeature_importances = feature_importances.sort_values(by = ['score'], ascending = False)\n\nfeatures = feature_importances['feature'][:10]\ny_feature = np.arange(len(features))\nscore = feature_importances['score'][:10]\n\n# Plot the importance of a feature to the price\nplt.figure(figsize=(20,10));\nplt.bar(y_feature, score, align='center');\nplt.xticks(y_feature, features, rotation='vertical');\nplt.xlabel('Features');\nplt.ylabel('Score');\nplt.title('Importance of features (TOP 10)');","ab0b9207":"### 3.2.1.1 Area = Stadtbezirke\n\n\n### 3.2.1.2 Section = Stadtbezirksteile\n\n\n### 3.2.1.3 Section = Postleitzahlen","c2f6aef1":"**Insights after analysing the dataset \"calendar\"**  \n\nThe data set consists of 7 features and a total rows of 4.190.565.\n\nThe overall quality is good, only the features 'price' and 'adjusted_price' have missing data (both 171).\n\nFor further analysis the following data cleaning is recommended:\n- Drop feature 'adjusted_price'.\n- Remove the rows with missing data in 'price' (number of affected rows is low ~0.004%).\n- The feature 'price' needs to be converted to numerical value\n- The feature 'date' needs to be converted to datetime format\n- The feature 'available' needs to be converted into bool data type","08c8417b":"**Insights after analysing the dataset \"listings\"**  \n\nThe dataset consists of 106 features and a total rows of 11.481.  \n\nThe overall quality is not for all features good. It must be considered how to deal with the remaining missing values. \n\nFilling does not look very promising in all cases, so the features should be droped from a threshold.\n\nSome features have only constant values and don't help us any further.\n\nFor further analysis the following data cleaning is recommended:  \n- Drop features with constant values \n- Drop features with more than 50% missing data\n- Fill missing numerical data with mean value\n- Convert features to useable data type (e.g. price)\n- Drop features that do not provide us with any useful information (for our specific questions) ","a984c0b7":"**Insights after analysing the dataset \"reviews\"**  \n\nThe dataset consists of 6 features and a total of 175.562 rows.   \n\nThe overall quality is good, only the feature 'comments' has missing data (74).  \n\nAt first sight the data set cannot be used to answer the questions. Nevertheless, the number of reviews shows an interesting pattern which should also be examined. \n\nMaybe the feature 'id' can be connected to the other data sets 'calendar' or 'listings'. \n\nIf I used the data set for my further analysis, I would drop missing data in 'comments' (number of affected rows is low ~0.04%) and convert the feature 'date' in to the 'DateTime' data type.","ba7ba1be":"### 2.2.2. Dataset \"listing.csv\"","e613b114":"# 2. Data Understanding\n\n## 2.1. Load the relevant dataset  \nThe available csv data sets from Inside Airbnb (http:\/\/insideairbnb.com\/munich\/) was stored in a separately dataset (https:\/\/www.kaggle.com\/chriskue\/munich-airbnb-data) for further analysis.\n\nThe data set contains the following files:\n*     **listings.csv**:  descriptions and review score\n*     **calendar.csv**:  listing id, price and availability for the upcoming year\n*     **reviews.csv**:   unique id for each reviewer and detailed comments","47558b59":"To analyze question 3 we filter our dataset to the TOP 5 city districts in terms of the best possible distance-price-ratio (findings in question 3). For these TOP 5 location we wanna figure out, what factors most influence the pricing for our TOP city ditricts.","57f7d6c4":"### 2.2.3 Dataet \"reviews.csv\"\n","62709970":"### Summary for Question 3: \"What are the TOP 3 factors influence the price for a stay near the area of the octoberfest?\"\n\nWith a coefficient of determination (R^2 of 0.92 for our testset), the model and the prediction seems accurate enough to predict the price of an apartment. Moreover, the coefficient of determination of the training data is the same (R^2 of 0.90 for our training set) .\n\n**Answer:**  \nThe TOP 3 factors of an apartment that have the greatest influence on the price are:\n- Accommodates\n- Bedrooms\n- Extra people","87f3e931":"### 3.3.1 Octoberfest, Munich, Bavaria, Germany Geographic Information\n- Latitude = 48.131\n- Longitude = 11.550","98d27d5f":"# 6. Deployment\n\nThe deployment is the final phase of the CRISP-DM process.   \n\nIn this phase, the results obtained are processed in order to present them and feed them into the client's decision-making process. \n\nAnd in the last step of the CRISP-DM model, the results obtained are summarized, processed and presented in an understandable way\n\nThis blog post and the Repo on Github (https:\/\/github.com\/noema-git\/airbnb-analysis) is the deployment of this work. To improve the quality of the model the CRSIP-DM cycle should be with adjusted parameters run through again.\n\n![https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/b\/b9\/CRISP-DM_Process_Diagram.png\/240px-CRISP-DM_Process_Diagram.png](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/b\/b9\/CRISP-DM_Process_Diagram.png\/240px-CRISP-DM_Process_Diagram.png)","4c226fa6":"# 1. Business Understanding\n\n**Airbnb background information:**\nAirbnb is a community-based platform that supports magical travel that is local, authentic and unique. Airbnb has more than 5 million listings and operates in 191 countries and more than 81,000 cities. To date, the Airbnb community has hosted nearly half-a-billion guests through a model designed to support healthy travel.\n\n**Specific questions to answer:**\n1. What are the TOP 3 most expensive city districts for a stay in Munich in general?\n2. What are the TOP 5 city districts in munich with the best distance-price-ratio for a stay near the are of the octoberfest?\n3. What are the TOP 3 factors influence the price for a stay near the area of the octoberfest?","115aaa9d":"### Summary for Question 1: \"What are the TOP 3 most expensive city districts for a stay in Munich in general?\"\n\nFrom the analysis, there is a clear difference in the costs between the different neighbourhoods in Munich.   \n\nIn general - the closer the apartment is to the city center of Munich, the higher the price.\n\n**Answer:**  \nThe TOP 3 expensive districts in Munich (on average price) are   \n- Altstadt-Lehel \n- Trudering-Riem\n- Allach-Untermenzing","57e45906":"## 3.3 Question 2: \"What are the TOP 5 city districts in munich with the best distance-price-ratio for a stay near the are of the octoberfest?\"","42caac01":"This notebook uses data from the Munich, Germany area of Airbnb and has been analyzed to answer the following questions. \n\n### Question 1: \"What are the TOP 3 most expensive city districts for a stay in Munich in general?\"  \n**Answer:**  \nThe TOP 3 expensive districts in Munich (on average price) are   \n- Altstadt-Lehel \n- Trudering-Riem\n- Allach-Untermenzing\n\n### Question 2: \"What are the TOP 5 city districts in munich with the best distance-price-ratio for a stay near the area of the octoberfest?\" \n**Answer:**\u00a0  \nThe TOP 5 locations (district) with the best price-distance-ration are:  \n- Sendling\n- Sendling-Westpark\n- Untergie\u00dfing-Harlaching\n- Laim\n- Neuhausen-Nymphenburg\n\n### Question 3: \"What are the TOP 3 factors influence the price for a stay near the area of the octoberfest?\"\n**Answer:**\nThe TOP 3 factors of an apartment that have the greatest influence on the price are:\n- Accommodates\n- Bedrooms\n- Extra people\n\nThis analysis was done as a project within the Udacity Data Science Nanodegree.   \nAny kind of optimization is very welcome. I really appreciate any feedback to improvement.  \n\nThe underlying Jupyter Notebook to this evaluation can be found on GitHub (https:\/\/github.com\/noema-git\/airbnb-analysis)","1d99e26e":"### 2.2.1. Dataset \"calendar.csv\"","e2d66e8c":"# 5. Evaluation\nFor the evaluation we look at the coefficient of determination (r squared value) of the training set and the test set, so we can compare the quality of the model.","36b272ad":"### 3.2.1 Additional information for munich districts\nTo better understand how munich is structured, we take a closer look at the specific districts in munich. The information and images provided can be found at wikipedia (https:\/\/de.wikipedia.org\/wiki\/Stadtbezirke_M%C3%BCnchens).","50df1b4f":"## 4.1 Question 3: \"What are the TOP 3 factors influence the price for a stay near the area of the octoberfest?\"","a0ec39e4":"## 3.1 Final feature selection \nSelect final features necessary to answer our three defined questions. \n\nFinally we only need some specific and meaningfulls features to answer our business questions. \n\nThe features can be devided in five sections:\n- Location\n- Room\n- Price\n- Score\n\nTo answer this question we use following sections and thier assoziated features:\n\n**Location:**\n- neighbourhood_cleansed\n- zipcode\n- latitude\n- longitude\n\n**Room:**\n- room_type = Zimmer oder Apartment\n- square_feet = Gr\u00f6\u00dfe in QM\n- minimum_nights\t\n- maximum_nights\n- bathroome\n\n**Price:**\n- price\t\n- weekly_price\t\n- monthly_price\t\n- security_deposit(...)\t\n- cleaning_fee\t\n- guests_included\t\n- extra_people\t\n\n**Score:**\n- number_of_reviews\n- review_scores_rating\n- review_scores_accuracy\n- review_scores_cleanliness\n- review_scores_checkin\n- review_scores_communication\n- review_scores_location\n- review_scores_value \n- reviews_per_month","69bd4f14":"### Summary for Question 2: \"What are the TOP 5 city districts in munich with the best distance-price-ratio for a stay near the area of the octoberfest?\"\n\nAs we can see in Fig.A and in Fig.C there is a clear winner in terms of the prefect distance-price-ratio, which is the \"Sendling\"-district with an average distance of round about 2000 meters and an average price of 103 euros. \nFig.B shows that the average price tend to become smaller in general, but for some places it is also possible to pay a cheap price for a distance under 2000 meteres\n\n**Answer:**  \nThe TOP 5 locations (district) with the best price-distance-ration are:  \n- Sendling\n- Sendling-Westpark\n- Untergie\u00dfing-Harlaching\n- Laim\n- Neuhausen-Nymphenburg","fde88c4b":"# 3. Data Preparation","3c8cfaaf":"# Analysis of Airbnb Data for Munich Octoberfest\n\n\n![munich_octoberfest.jpg](attachment:munich_octoberfest.jpg)\n\n**Motivation:** I am still trying to understand a bit more about the current real estate market in munich, especially the area around the octoberfest (theresienwiese). Therefore I think the Airbnb dataset for munich could be one possible source of information to gain usefull insights in the current situation.\n\n**Dataset:** The used data sets were created on November 25th, 2019 and contain detailed listings data, review data and calendar data of current Airbnb listings in Munich, Germany. This data was created by Murray Cox and his Inside Airbnb project which can be found here:  http:\/\/insideairbnb.com\/get-the-data.html\n\nThe data set can also be found here: https:\/\/www.kaggle.com\/chriskue\/munich-airbnb-data\n\n**Methodology:** For the analysis I will use the *CRISP-DM* Methodology. CRIPS-DM stands for \"cross-industry process for data mining\". The process consists of six steps:\n\n1. Business Understanding\n1. Data Understanding\n1. Data Preparation\n1. Modeling\n1. Evaluation\n1. Deployment\n\nThe underlying Jupyter notebook can be found on Github (https:\/\/github.com\/noema-git\/airbnb-analysis)","4311869c":"## 3.2 Question 1: \"What are the TOP 3 most expensive city districts for a stay in Munich in general?\"","42160002":"## 2.2. Understand the underlaying data\nTo get a better understanding of the data we will look at the features and the overall quality of the data ( e.g. missing values).","e4ca1b66":"# Summary","149a053f":"# 4. Modeling"}}