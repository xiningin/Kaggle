{"cell_type":{"4337af02":"code","8c16fe8b":"code","0e3abad9":"code","a648833a":"code","4015bf4b":"code","837d9165":"code","06e28e48":"code","b49faf77":"code","8c746f5b":"code","94b09769":"code","5e521f10":"code","50ecf672":"code","fb251dce":"code","f8e6803a":"code","f02269e6":"code","52b84ca1":"code","1fbe135c":"code","9700ebec":"code","ec56c575":"code","b36cd3db":"code","da98bd41":"code","95fbf495":"code","f3827746":"code","256ea050":"code","7ee1f458":"code","164f7d61":"code","af58c992":"code","edb355fc":"code","8369038a":"code","ffa166d8":"code","48ddd996":"code","88a1f37a":"code","0342af51":"code","1df2f3ef":"code","21efe746":"code","ff06f3bd":"code","9f844349":"code","1e3556ef":"code","7b9c5462":"code","af1fcc19":"code","4909d69f":"code","a87276e6":"code","542c16a1":"code","d0d54936":"code","3fb28312":"code","a07c1393":"code","cd0581a5":"code","69947af9":"code","553c4299":"code","98388ecb":"code","23de34a6":"code","57e3e3cf":"code","452781e1":"code","f5dc7179":"code","81388173":"code","935af423":"code","a8e88d13":"code","0c003215":"code","f752b0b0":"code","64733085":"code","b9206e24":"code","d1d7bce7":"markdown","be67280f":"markdown","22e8b687":"markdown","9d2879ce":"markdown","223de594":"markdown","a7c9adb5":"markdown","700fedcd":"markdown","e2c248ce":"markdown","2e0e47fc":"markdown","deb1a6f1":"markdown"},"source":{"4337af02":"!pip install timm filetype","8c16fe8b":"# !ln -s \/kaggle\/input\/korpus-ml-2\/* .\/\n# !ln -s \/kaggle\/input\/diagrams-dataset .\/\n# !mv \/kaggle\/working\/diagrams-dataset\/growth_diagram \/kaggle\/working\/diagrams-dataset\/growth_chart\n# !ln -s \/kaggle\/input\/diagrams-extend\/just_image\/just_image\/* .\/diagrams-dataset\/just_image\n# !ln -s \/kaggle\/input\/diagrams-extend\/synthetic\/* .\/diagrams-dataset\/\n# !rm -r .\/diagrams-dataset\/\n# !unlink v2\n# !ln -s \/kaggle\/input\/diagrams-dataset\/v2 .\/","0e3abad9":"# !ls -l diagrams-dataset\/bar_chart\/|grep ^d\n# !ls -l train\/train\/train\/bar_chart\/","a648833a":"# !mv  \/kaggle\/working\/diagrams-dataset\/growth_chart\/growth_diagram\/* \/kaggle\/working\/diagrams-dataset\/growth_chart\/\n# !ls -l \/kaggle\/working\/diagrams-dataset\/growth_chart\/ | grep ^d\n# !ls -l \/kaggle\/working\/diagrams-dataset\/growth_chart\/growth_diagram\/\n# !rm -r \/kaggle\/working\/diagrams-dataset\/growth_chart\/growth_diagram\/\n# !ls kaggle\/working\/diagrams-dataset\/growth_chart\/growth_diagram","4015bf4b":"import os\nimport time\nfrom glob import glob\n\nimport albumentations\nimport cv2\nimport filetype\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport plotly.express as px\nimport torch\n\nfrom albumentations import pytorch as AT\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\n\nname_to_label = {\n    'bar_chart' : 1, 'diagram' : 2, 'flow_chart' : 3,\n    'graph' : 4, 'growth_chart' : 5, 'pie_chart' : 6,\n    'table' : 7, 'just_image' : 0\n}\nlabel_to_name = {\n    1 : 'bar_chart', 2 : 'diagram', 3 : 'flow_chart',\n    4 : 'graph', 5 : 'growth_chart', 6 : 'pie_chart',\n    7 : 'table', 0 : 'just_image'\n}\n\n\n","837d9165":"!rm -r submission test","06e28e48":"def mount_base_dataset(inp, out):\n    !mkdir {out}\n    for name in name_to_label:\n#         print(name)\n        !mkdir {out}\/{name}\n        !ln -s {inp}\/{name}* {out}\/{name}\n    print('done', out)\nmount_base_dataset('\/kaggle\/input\/korpus-ml-2\/train\/train\/train', 'test')\n# mount_base_dataset('\/kaggle\/input\/korpus-ml-2\/test\/test\/test', 'submission')","b49faf77":"import pandas as pd\n\n","8c746f5b":"class Diagrams(Dataset):\n    def __init__(self, folder, transform=None, train_stage=True):\n        self.train_stage = train_stage        \n        self.transform = transform\n        self.files_list = []\n        \n        if train_stage:\n            self.y = []\n            class_weights = Diagrams.prepare_sampler(folder)\n            self.sampler_weights = []\n            best_names = best_submission['image_name'].tolist() \n            \n            for class_name in name_to_label.keys():\n                file_names = glob(os.path.join(folder, class_name, \"*\"))\n                for file_name in file_names:\n                    if not Diagrams.check_valid_image(file_name):\n                        file_names.remove(file_name)\n                        \n                label = name_to_label[class_name]\n                self.y.extend([label for _ in file_names])\n                weights = []\n                for file_name in file_names:\n                    w = class_weights[name_to_label[class_name]]\n                    if file_name in best_names:\n                        w *= 3\n                    weights.append(w)\n                self.sampler_weights.extend(weights)\n                self.files_list.extend(file_names)\n        else:\n            self.files_list.extend(glob(os.path.join(folder, \"*\")))\n        \n    def __len__(self):\n        return len(self.files_list)\n\n    def __getitem__(self, idx):\n        image = Diagrams.read_image(self.files_list[idx])\n        if self.transform:\n            image = self.transform(image=image)['image']\n        \n        if self.train_stage:\n            return image, self.y[idx], idx\n        else:\n            return image, os.path.basename(self.files_list[idx])\n    \n    @staticmethod\n    def read_image(path):\n        # MIME: image\/jpeg, image\/gif\n        kind = filetype.guess(path)\n        if kind is None or not kind.MIME.startswith(\"image\"):\n            return None\n        if kind.MIME.endswith(\"gif\"):\n            cap = cv2.VideoCapture(path)\n            status, image = cap.read()\n        else:\n            image = cv2.imread(path)\n\n        return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    @staticmethod\n    def check_valid_image(path):\n        kind = filetype.guess(path)\n        if kind is None or not kind.MIME.startswith(\"image\"):\n            return False\n        else:\n            return True\n    \n    @staticmethod\n    def prepare_sampler(folder):\n        num_samples = {}\n        submission_indexes = []\n        for class_name in name_to_label.keys():\n            file_names = glob(os.path.join(folder, class_name, \"*\"))\n            num_samples[class_name] = len(file_names) \n        reverse_weights = [0. for _ in range(8)]\n        sorted_num_samples = [0 for _ in range(8)]\n        for class_name, num in num_samples.items():\n            sorted_num_samples[name_to_label[class_name]] = num\n\n        all_count = sum(sorted_num_samples)\n        for i in range(8):\n            reverse_weights[i] = 1 \/ (sorted_num_samples[i] \/ all_count)\n        return reverse_weights\n\n# def make_weights_for_balanced_classes(images, nclasses):                        \n#     count = [0] * nclasses                                                      \n#     for item in images:                                                         \n#         count[item[1]] += 1                                                     \n#     weight_per_class = [0.] * nclasses                                      \n#     N = float(sum(count))                                                   \n#     for i in range(nclasses):                                                   \n#         weight_per_class[i] = N\/float(count[i])                                 \n#     weight = [0] * len(images)                                              \n#     for idx, val in enumerate(images):                                          \n#         weight[idx] = weight_per_class[val[1]]                                  \n#     return weight  ","94b09769":"def train_model(model_conv, train_loader, valid_loader, criterion, optimizer, scheduler, n_epochs, prefix):\n    model_conv = model_conv.to(device)\n    valid_loss_min = np.Inf\n    patience = 10\n    # \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u044d\u043f\u043e\u0445 \u0436\u0434\u0435\u043c \u0434\u043e \u043e\u0442\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f\n    p = 0\n    # \u0438\u043d\u0430\u0447\u0435 \u043e\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\n    stop = False\n    accuracy_best = 0.94\n    # \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445\n    for epoch in range(1, n_epochs+1):\n        print(time.ctime(), 'Epoch:', epoch)\n\n        train_loss = []\n        model_conv.train()\n        all_outputs = []\n        all_target = []\n        accuracy_per_batch = []\n        for batch_i, (data, target, indexes) in tqdm(enumerate(train_loader), desc='train', total=len(train_loader)):\n            all_target.extend(target.tolist())\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            output = model_conv(data)\n            all_outputs.extend(output.argmax(dim=1).tolist())\n            loss = criterion(output, target)\n            accuracy_per_batch.append(accuracy_score(target.tolist(), output.argmax(dim=1).tolist()))\n            train_loss.append(loss.item())\n            loss.backward()\n            optimizer.step()\n            # scheduler.step(accuracy_score(target.tolist(), output.argmax(dim=1).tolist()))\n            scheduler.step()\n        # scheduler.step()\n        print(\"lr\", optimizer.param_groups[0]['lr'])\n        \n        # fig = px.line(y=train_loss)\n        # fig.show()\n        plt.plot(train_loss)\n        plt.show()\n        plt.plot(accuracy_per_batch, )\n        plt.show()\n        print(\"=\"*10,\"train metrics\",\"=\"*10)\n        print(classification_report(all_target, all_outputs, labels=list(range(8)), target_names=labels_sorted))\n        print(\"=\"*10,\"train metrics\",\"=\"*10)\n\n        # \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e\n        with torch.no_grad():\n            model_conv.eval()\n            val_loss = []\n            all_outputs = []\n            all_target = []\n            for batch_i, (data, target, indexes) in tqdm(enumerate(valid_loader), desc='validation', total=len(valid_loader)):\n                all_target.extend(target.tolist())\n                data, target = data.to(device), target.to(device)\n                output = model_conv(data)\n                all_outputs.extend(output.argmax(dim=1).tolist())\n                loss = criterion(output, target)\n                val_loss.append(loss.item()) \n\n            print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}.')\n            accuracy = accuracy_score(all_target, all_outputs)\n            print(classification_report(all_target, all_outputs))\n            # accuracy = test_accuracy(model_conv, valid_loader)\n        if accuracy > accuracy_best:\n            accuracy_best = accuracy\n            torch.save(model_conv.state_dict(), f'.\/models_checkpoints\/{prefix}model_{accuracy}.pt')\n        else:\n            torch.save(model_conv.state_dict(), f'.\/models_checkpoints\/{prefix}_{epoch}.pt')\n        valid_loss = np.mean(val_loss)\n        # scheduler.step()\n        if valid_loss <= valid_loss_min:\n            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n            valid_loss_min,\n            valid_loss))\n            torch.save(model_conv.state_dict(), prefix+'model.pt')\n            valid_loss_min = valid_loss\n            p = 0\n\n        # \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u043a\u0430\u043a \u0434\u0435\u043b\u0430 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438\n        if valid_loss > valid_loss_min:\n            p += 1\n            print(f'{p} epochs of increasing val loss')\n            if p > patience:\n                print('Stopping training')\n                stop = True\n                break        \n\n        if stop:\n            break\n    return model_conv, train_loss, val_loss","5e521f10":"!ls -l diagrams-dataset\/pie_chart\/","50ecf672":"from timm.models import efficientnet_b3\nmodel = efficientnet_b3(pretrained=True)\nmodel.classifier = nn.Linear(model.classifier.in_features, 10)","fb251dce":"!cp -r \/kaggle\/input\/diagrams-dataset .\/","f8e6803a":"img_size = 300\nbatch_size = 256\ndevice = torch.device(0)\n\n\ndata_transforms = albumentations.Compose([\n    albumentations.HorizontalFlip(),\n#     albumentations.RandomBrightness(),\n    albumentations.ShiftScaleRotate(rotate_limit=0, scale_limit=(0.1, 0.4), shift_limit=0.3),\n    # albumentations.HueSaturationValue(),\n\n    albumentations.Resize(img_size, img_size),\n    albumentations.Cutout(4, max_h_size=int(img_size\/4), max_w_size=int(img_size\/4), p=0.3),\n\n#     albumentations.Normalize([0.8296, 0.8338, 0.8162], [0.1995, 0.1896, 0.2008]),\n    albumentations.Normalize(),\n#     albumentations.CLAHE(),\n\n    # (tensor([0.8184, 0.8207, 0.8030]), tensor([0.2010, 0.1915, 0.2026]))\n    # (tensor([0.8296, 0.8338, 0.8162]), tensor([0.1995, 0.1896, 0.2008])) without just_image\n    AT.ToTensor()\n    ])\n\ndata_transforms_test = albumentations.Compose([\n    albumentations.Resize(img_size, img_size),\n#     albumentations.CLAHE(),\n    # albumentations.ToGray(),\n    # albumentations.Normalize([0.8184, 0.8207, 0.8030], [0.2010, 0.1915, 0.2026]),\n#     albumentations.Normalize([0.8296, 0.8338, 0.8162], [0.1995, 0.1896, 0.2008]),\n    albumentations.Normalize(),\n#     albumentations.CLAHE(),\n\n    # (tensor([0.8184, 0.8207, 0.8030]), tensor([0.2010, 0.1915, 0.2026]))\n    AT.ToTensor()\n])\n\ntrain_set = Diagrams(\"diagrams-dataset\", transform=data_transforms)\n# t = make_weights_for_balanced_classes(train_set, 8)\n# weights = torch.DoubleTensor(t)                                       \nsampler = torch.utils.data.sampler.WeightedRandomSampler(train_set.sampler_weights, len(train_set.sampler_weights)) \nvalid_set = Diagrams(\"test\", transform=data_transforms_test)\n# # valid_size = int(len(train_set) * 0.1)\n# # train_set, valid_set = torch.utils.data.random_split(train_set, \n# #                                     (len(train_set)-valid_size, valid_size))\ntrain_loader = DataLoader(train_set, pin_memory=False, sampler=sampler, batch_size=batch_size)\ntest_loader = DataLoader(valid_set, pin_memory=False, batch_size=batch_size, shuffle=True)","f02269e6":"!ls diagrams-dataset\/growth_chart\/","52b84ca1":"# t[9000]\nfrom tqdm import tqdm\n# mean = torch.zeros((3))\n# std = torch.zeros((3))\n# # t = transforms.Normalize(None, None)\n# # train_set\n# # mean = 0 \n# # mean\n# for x, y, idx in tqdm(train_set):\n#     if y != 0:\n#         mean += x.mean(dim=(1,2))\n#         std += x.std(dim=(1,2))\n\n# #     mean += x.mean(dim=(1,2))\n# # train_set[0][0].std(dim=(1,2))\n# num = len(train_set) - num_samples['just_image']\n# mean \/ num, std \/ num \n# t(mean) tensor([0.9140, 1.0681, 1.7540])","1fbe135c":"# len(valid_set), len(train_set)","9700ebec":"!ls .\/diagrams-dataset\/pie_chart\/pie_chart","ec56c575":"import torchvision\nsamples, labels, indexes = next(iter(train_loader))\nplt.figure(figsize=(16,24))\ngrid_imgs = torchvision.utils.make_grid(samples[:32])\nnp_grid_imgs = grid_imgs.numpy()\nprint(labels)\nplt.imshow(np.transpose(np_grid_imgs, (1,2,0)))","b36cd3db":"from sklearn.metrics import classification_report, accuracy_score\nimport random\n\nlabels_sorted = [label for _, label in sorted(label_to_name.items(), key=lambda i: i[0])] \n\n\ndef test_accuracy(model_conv, loader, verbose=True):\n    model_conv.eval()\n    predictions = []\n    ground_truth = []       \n    for data, target in loader:\n        data, target = data.to(device), target.to(device)\n        output = model_conv(data)\n        # print(classification_report(target.cpu(), output.argmax(dim=1).cpu()))\n        # predictions.append()\n        predictions.extend(output.argmax(dim=1).tolist())\n        ground_truth.extend(target.tolist())\n    if verbose:\n        print(ground_truth)\n        print(classification_report(ground_truth, predictions, labels=list(range(8)), target_names=labels_sorted))\n    return accuracy_score(ground_truth, predictions)\n# test_accuracy(model, test_loader)\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)","da98bd41":"def unfreeze(model, num_last_layers=6):\n    last_blocks = ['conv_head', 'bn2','act2','global_pool','classifier']\n    \n    for name, child in model.named_children():\n        # print(name)\n        if name in last_blocks:\n            for name_inside, param in child.named_parameters():\n                # print(name, name_inside, True)\n                param.requires_grad = True\n        else:\n            for name_inside, param in child.named_parameters():\n                # print(name_inside, False)\n                param.requires_grad = False\n        #     print(name)\n        if name == 'blocks':\n            assert num_last_layers < len(child), \"num_last_layers more than layers in block\"\n            # print(len(child))\n\n            # freeze all inside block\n            for block in child:\n                for layer in block:\n                    for name, param in layer.named_parameters():\n                        param.requires_grad = False\n            \n            # unfreeze all layers after conv_pwl and conv_pwl\n            for block in child[num_last_layers:]:\n                for layer in block:\n                    last = False\n                    for name_inside, param in layer.named_parameters():\n                        if 'conv_pwl' in name_inside:\n                            last = True\n                        if last:\n                            param.requires_grad = True\n                            # print(name_inside, param.requires_grad)\n     \n\ndef unfreeze_block(model, num_last_layers=6):\n    print(\"::::unfreeze::::\")\n#     last_blocks = ['conv_head', 'bn2','act2','global_pool','classifier']\n    last_blocks = ['classifier']\n#     print(last_blocks)\n    \n    \n    for name, child in model.named_children():\n        # print(name)\n        if name in last_blocks:\n            for name_inside, param in child.named_parameters():\n                print(name, name_inside, True)\n                param.requires_grad = True\n        else:\n            for name_inside, param in child.named_parameters():\n                # print(name_inside, False)\n                param.requires_grad = False\n        #     print(name)\n        if name == 'blocks':\n            assert num_last_layers <= len(child), \"num_last_layers more than layers in block\"\n            print(len(child))\n            \n\n            # freeze all inside block\n            for block in child:\n                for layer in block:\n                    for name, param in layer.named_parameters():\n                        param.requires_grad = False\n            \n            # unfreeze all layers after conv_pwl and conv_pwl\n            for block in child[num_last_layers:]:\n                for layer in block:\n                    for name_inside, param in layer.named_parameters():\n                        param.requires_grad = True\n                        print(name_inside, param.requires_grad)\n\n# unfreeze_block(model, 6)\n","95fbf495":"seed_everything(42)\ncriterion = nn.CrossEntropyLoss()\nunfreeze_block(model, 6)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=0.0001, momentum=0.9, nesterov=True)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n!mkdir models_checkpoints\nmodel, train_loss, val_loss = train_model(model, train_loader, test_loader, criterion, \n                            optimizer,scheduler,  n_epochs=20, prefix=f'6_and_classifier')\ntorch.save(model.state_dict(), f'models_checkpoints\/{num_unfreeze}.pt')","f3827746":"for num_unfreeze in [5, 4, 3, 2]:\n    print(num_unfreeze, \"==\"*100)\n    from timm.models import efficientnet_b0\n    # import random\n    del model\n    torch.cuda.empty_cache()\n    model = efficientnet_b0(pretrained=True)\n    # model.\n    model.classifier = nn.Linear(model.classifier.in_features, 10)\n        # nn.Dropout(0.2),\n        \n    seed_everything(42)\n    criterion = nn.CrossEntropyLoss()\n    unfreeze_block(model, num_unfreeze)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.005, weight_decay=0.0001, momentum=0.9, nesterov=True)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n\n    model, train_loss, val_loss = train_model(model, train_loader, test_loader, criterion, \n                                optimizer,scheduler,  n_epochs=20, prefix=f'deepCOsine_fully_unfreeze{num_unfreeze}')\n    torch.save(model.state_dict(), f'models_checkpoints\/{num_unfreeze}duf_Cosine.pt')","256ea050":"for num_unfreeze in [6, 5, 4, 3, 2]:\n    print(num_unfreeze, \"==\"*100)\n    from timm.models import efficientnet_b0\n    # import random\n    del model\n    torch.cuda.empty_cache()\n    model = efficientnet_b0(pretrained=True)\n    # model.\n    model.classifier = nn.Linear(model.classifier.in_features, 10)\n        # nn.Dropout(0.2),\n        \n    seed_everything(42)\n    criterion = nn.CrossEntropyLoss()\n    unfreeze_block(model, num_unfreeze)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.003, weight_decay=0.0005, momentum=0.9, nesterov=True)\n    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=160, verbose=True, mode='max')\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=35)\n    \n    model, train_loss, val_loss = train_model(model, train_loader, test_loader, criterion, \n                                optimizer,scheduler,  n_epochs=20, prefix=f'deep_fully_unfreeze{num_unfreeze}')\n    torch.save(model.state_dict(), f'{num_unfreeze}duf_Cosine_3e-3lr5e-4wd.pt')","7ee1f458":"# import gc\n# gc.collect()\ntorch.cuda.empty_cache()\n!nvidia-smi","164f7d61":"torch.cuda.empty_cache()\n# model.load_state_dict(torch.load('\/home\/jovyan\/work\/deepCOsine_fully_unfreeze4model.pt'))\n\noptimizer = torch.optim.SGD(model.parameters(), lr=0.0003, weight_decay=0.0005, momentum=0.9, nesterov=True)\n    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=160, verbose=True, mode='max')\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)\n\nmodel, train_loss, val_loss = train_model(model, test_loader, test_loader, criterion, \n                            optimizer,scheduler,  n_epochs=8, prefix=f'fine_unfreeze{num_unfreeze}')\n","af58c992":"model.load_state_dict(torch.load('.\/models_checkpoints\/finetune_from947model_0.9912087912087912.pt'))","edb355fc":"torch.cuda.empty_cache()\nsubmission_set = Diagrams('test\/test\/test', transform=data_transforms_test, train_stage=False)\nsubmission_loader = DataLoader(submission_set, batch_size=64, shuffle=False)\nall_filenames = []\nall_predictions = []\nwith torch.no_grad():\n    model.eval()\n    for images, filenames in tqdm(submission_loader):\n        # print(filenames)\n        all_filenames.extend(filenames)\n        output = model(images.to(device))\n        all_predictions.extend(output.argmax(dim=1).tolist())\n","8369038a":"\nimport pandas as pd\nsample_submission = pd.read_csv(\"sample_submission.csv\")\nsample_submission.image_name = all_filenames\nsample_submission.label = all_predictions\nsample_submission.to_csv('overfit_noisy_new_dataset_6and_only_Last.csv', index=False)","ffa166d8":"finetest_loader = DataLoader(valid_set, pin_memory=False, batch_size=32, shuffle=True)","48ddd996":"!ls models_checkpoints\/","88a1f37a":"optimizer = torch.optim.Adagrad(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=240)\ntorch.cuda.empty_cache()\nmodel, train_loss, val_loss = train_model(model, finetest_loader, finetest_loader, criterion, \n                                          optimizer,scheduler,  n_epochs=3 ,prefix='finetune_from_weightednoisy')","0342af51":"import pandas as pd\nbest_submission = pd.read_csv(\"\/kaggle\/input\/public96576\/oferfit_test_submission_new_dataset_6and_only_Last.csv\")\n\n","1df2f3ef":"!ln --help","21efe746":"# best_submission\n# !cp -r diagrams-dataset\/ .\/diagrams-dataset-expanded\n# best_submission['image_name'] = best_submission['image_name'].astype('str')\n\ndirs_files = {}\nfor filename in tqdm(best_submission.iterrows(), total=best_submission.count()[0]):\n    dirs_files[label_to_name[filename[1][1]]] = dirs_files.get(label_to_name[filename[1][1]], [])\n    dirs_files[label_to_name[filename[1][1]]].append(filename[1][0])\n#     !ln -f .\/test\/test\/test\/{filename[1][0]} .\/diagrams-dataset-expanded\/{label_to_name[filename[1][1]]}\/\n#     !rm .\/diagrams-dataset-expanded\/{label_to_name[filename[1][1]]}\/{filename[1][0]}\n    \n#     print(filename[1][0], )#['label'])\n#     print(filename[1][1])\n#     break\n    ","ff06f3bd":"\n!mkdir temp\nfor label, filenames in dirs_files.items():\n    with open(f'temp\/{label}', 'w') as fh:\n        for filename in filenames:\n            fh.write('test\/test\/test\/'+filename+' ')\n    ","9f844349":"# !mv diagrams-dataset\/pie_chart\/pie_chart\/* diagrams-dataset\/pie_chart\n# !ls -l diagrams-dataset\/pie_chart\/pie_chart\n# !rm -r diagrams-dataset\/pie_chart\/pie_chart\n!ls -l diagrams-dataset\/pie_chart\/\n# !rm -r diagrams-dataset-expanded\/\n# !cp -r diagrams-dataset\/ diagrams-dataset-expanded\n# !mv diagrams-dataset\/growth_chart\/growth_diagram\/* diagrams-dataset\/growth_chart\/ \n# !rm -r diagrams-dataset\/growth_chart\/growth_diagram\/\n\n# !mkdir test\n# for name in name_to_label:\n#     print(name)\n#     !mkdir test\/{name}\n#     !ln -s \/kaggle\/input\/korpus-ml-2\/train\/train\/train\/{name}* test\/{name}\n# !mkdir submission\n# for name in name_to_label:\n#     print(name)\n#     !mkdir test\/{name}\n#     !ln -s \/kaggle\/input\/korpus-ml-2\/test\/test\/test\/{name}* submission\/{name}\n# #     break\n#     !ls -l diagrams-dataset-expanded\/{name} |grep ^d\n    ","1e3556ef":"!cp -r \/kaggle\/input\/diagrams-dataset\/ .\/","7b9c5462":"\n!cp -r diagrams-dataset\/ diagrams-dataset-expanded\/\nfor class_name in name_to_label:\n    !cp $(cat temp\/{class_name}) diagrams-dataset-expanded\/{class_name}\/","af1fcc19":"!ls -l .\/diagrams-dataset-expanded\/diagram\/|grep ^d","4909d69f":"# diagrams-dataset-expanded\n# (455, 17939)\nimg_size = 300\nbatch_size = 256\ndevice = torch.device(0)\n\n\ndata_transforms = albumentations.Compose([\n    albumentations.HorizontalFlip(),\n    albumentations.RandomBrightness(),\n    albumentations.ShiftScaleRotate(rotate_limit=0, scale_limit=(0.1, 0.4), shift_limit=0.3),\n    # albumentations.HueSaturationValue(),\n\n    albumentations.Resize(img_size, img_size),\n    albumentations.Cutout(4, max_h_size=int(img_size\/4), max_w_size=int(img_size\/4), p=0.3),\n\n#     albumentations.Normalize([0.8296, 0.8338, 0.8162], [0.1995, 0.1896, 0.2008]),\n    albumentations.Normalize(),\n#     albumentations.CLAHE(),\n\n    # (tensor([0.8184, 0.8207, 0.8030]), tensor([0.2010, 0.1915, 0.2026]))\n    # (tensor([0.8296, 0.8338, 0.8162]), tensor([0.1995, 0.1896, 0.2008])) without just_image\n    AT.ToTensor()\n    ])\n\ndata_transforms_test = albumentations.Compose([\n    albumentations.Resize(img_size, img_size),\n#     albumentations.CLAHE(),\n    # albumentations.ToGray(),\n    # albumentations.Normalize([0.8184, 0.8207, 0.8030], [0.2010, 0.1915, 0.2026]),\n#     albumentations.Normalize([0.8296, 0.8338, 0.8162], [0.1995, 0.1896, 0.2008]),\n    albumentations.Normalize(),\n#     albumentations.CLAHE(),\n\n    # (tensor([0.8184, 0.8207, 0.8030]), tensor([0.2010, 0.1915, 0.2026]))\n    AT.ToTensor()\n])\n\n# train_set = Diagrams(\"diagrams-dataset-expanded\", transform=data_transforms)\n# t = make_weights_for_balanced_classes(train_set, 8)\n# weights = torch.DoubleTensor(t)                                       \n# sampler = torch.utils.data.sampler.WeightedRandomSampler(train_set.sampler_weights, len(train_set.sampler_weights)) \nvalid_set = Diagrams(\"\/input\/korpus-ml-2\/train\/train\/train\", transform=data_transforms_test)\n# # valid_size = int(len(train_set) * 0.1)\n# # train_set, valid_set = torch.utils.data.random_split(train_set, \n# #                                     (len(train_set)-valid_size, valid_size))\n# train_loader = DataLoader(train_set, pin_memory=False, sampler=sampler, batch_size=batch_size)\ntest_loader = DataLoader(valid_set, pin_memory=False, batch_size=batch_size, shuffle=True)","a87276e6":"len(valid_set), len(train_set) # (455, 17939)","542c16a1":"import gc\n# del test_loader\n# del train_loader\n# del model\ngc.collect()\ntorch.cuda.empty_cache()\n!nvidia-smi","d0d54936":"from timm.models import efficientnet_b3\ndel model\ngc.collect()\ntorch.cuda.empty_cache()\nmodel = efficientnet_b3(pretrained=True)\nmodel.classifier = nn.Linear(model.classifier.in_features, 10)\nseed_everything(42)\ncriterion = nn.CrossEntropyLoss()\nunfreeze_block(model, 6)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=0.0001, momentum=0.9, nesterov=True)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=140)\n!mkdir models_checkpoints\nmodel, train_loss, val_loss = train_model(model, train_loader, test_loader, criterion, \n                            optimizer,scheduler,  n_epochs=3, prefix=f'noisy2ndit_6_and_classifier')\ntorch.save(model.state_dict(), f'models_checkpoints\/noisy2ndit_6.pt')","3fb28312":"optimizer = torch.optim.Adagrad(model.parameters(), lr=0.003)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=240)\ngc.collect()\ntorch.cuda.empty_cache()\nmodel, train_loss, val_loss = train_model(model, finetest_loader, finetest_loader, criterion, \n                                          optimizer,scheduler,  n_epochs=3 ,prefix='finetune_from_noisy2ndit')","a07c1393":"!wget http:\/\/83.146.107.37:8000\/overfit_noisy2ndit_new_dataset_6and_only_Last.csv","cd0581a5":"!wget http:\/\/83.146.107.37:8000\/weighted_overfit_from_noisy2ndit_new_dataset_6and_only_Last.csv","69947af9":"# !rm -r diagrams-dataset-expanded\/\n# !cp -r diagrams-dataset\/ diagrams-dataset-expanded\/\n\nimport pandas as pd\nbest_submission = pd.read_csv(\"weighted_overfit_from_noisy2ndit_new_dataset_6and_only_Last.csv\")\n\ndirs_files = {}\nfor filename in tqdm(best_submission.iterrows(), total=best_submission.count()[0]):\n    dirs_files[label_to_name[filename[1][1]]] = dirs_files.get(label_to_name[filename[1][1]], [])\n    dirs_files[label_to_name[filename[1][1]]].append(filename[1][0])\n\n!mkdir temp\nfor label, filenames in dirs_files.items():\n    with open(f'temp\/{label}', 'w') as fh:\n        for filename in filenames:\n            fh.write('\/kaggle\/input\/korpus-ml-2\/test\/test\/test\/'+filename+' ')\n\n# !cp -r diagrams-dataset\/ diagrams-dataset-expanded\/\nfor class_name in name_to_label:\n    !ln -s $(cat temp\/{class_name}) diagrams-dataset\/{class_name}\/","553c4299":"torch.cuda.empty_cache()\nsubmission_set = Diagrams('test\/test\/test', transform=data_transforms_test, train_stage=False)\nsubmission_loader = DataLoader(submission_set, batch_size=64, shuffle=False)\nall_filenames = []\nall_predictions = []\nwith torch.no_grad():\n    model.eval()\n    for images, filenames in tqdm(submission_loader):\n        # print(filenames)\n        all_filenames.extend(filenames)\n        output = model(images.to(device))\n        all_predictions.extend(output.argmax(dim=1).tolist())\nimport pandas as pd\nsample_submission = pd.read_csv(\"sample_submission.csv\")\nsample_submission.image_name = all_filenames\nsample_submission.label = all_predictions\nsample_submission.to_csv('overfit_noisy2ndit_new_dataset_6and_only_Last.csv', index=False)","98388ecb":"!zip -r diagrams-noisy.zip diagrams-dataset-expanded","23de34a6":"# !du --block-size=m \/kaggle\/working\/ \nmount -t tmpfs -o size=500m tmpfs \/mountpoint","57e3e3cf":"# del model\n# gc.collect()\n# torch.cuda.empty_cache()\n","452781e1":"!pip install ttach","f5dc7179":"from timm.models import efficientnet_b3\nmodel = efficientnet_b3(pretrained=True)\n# unfreeze_block(model, 6)\nmodel.classifier = nn.Linear(model.classifier.in_features, 10)\nmodel.load_state_dict(torch.load('\/kaggle\/input\/public97289\/finetune_from_noisy2nditmodel.pt'))\nmodel = model.to(device)","81388173":"submission_set = Diagrams('\/kaggle\/input\/korpus-ml-2\/test\/test\/test', transform=data_transforms_test, train_stage=False)\nsubmission_loader = DataLoader(submission_set, batch_size=64, shuffle=False)","935af423":"# !pip install ttach\n# !pip install --upgrade git+https:\/\/github.com\/qubvel\/ttach\nfrom ttach import Compose, HorizontalFlip, Scale, ClassificationTTAWrapper, FiveCrops\nfrom ttach.aliases import ten_crop_transform\ntta_transforms = Compose(\n    [\n        Scale(scales=[1, 1.5]),\n        HorizontalFlip(),\n    ]\n)\n\n\nbetter_model = ClassificationTTAWrapper(model, tta_transforms, merge_mode='max')\nbetter_model = better_model.to(device)","a8e88d13":"torch.cuda.empty_cache()\n# submission_set = Diagrams('test\/test\/test', transform=data_transforms_test, train_stage=False)\n# submission_loader = DataLoader(submission_set, batch_size=64, shuffle=False)\nall_filenames = []\nall_predictions = []\nwith torch.no_grad():\n    better_model.eval()\n    for images, filenames in tqdm(submission_loader):\n        # print(filenames)\n        all_filenames.extend(filenames)\n        output = better_model(images.to(device))\n        all_predictions.extend(output.argmax(dim=1).tolist())\nimport pandas as pd\nsample_submission = pd.read_csv(\"\/kaggle\/input\/korpus-ml-2\/sample_submission.csv\")\nsample_submission.image_name = all_filenames\nsample_submission.label = all_predictions\nsample_submission.to_csv('TTA_MAX_weighted_overfit_from_noisy2ndit_new_dataset_6and_only_Last.csv', index=False)","0c003215":"torch.cuda.empty_cache()\n# submission_set = Diagrams('test\/test\/test', transform=data_transforms_test, train_stage=False)\n# submission_loader = DataLoader(submission_set, batch_size=64, shuffle=False)\nall_filenames = []\nall_predictions = []\nmodel = model.to(device)\nwith torch.no_grad():\n    model.eval()\n    for images, filenames in tqdm(submission_loader):\n        # print(filenames)\n        all_filenames.extend(filenames)\n        output = model(images.to(device))\n        all_predictions.extend(output.argmax(dim=1).tolist())\nimport pandas as pd\nsample_submission = pd.read_csv(\"\/kaggle\/input\/korpus-ml-2\/sample_submission.csv\")\nsample_submission.image_name = all_filenames\nsample_submission.label = all_predictions\nsample_submission.to_csv('fine_weighted_overfit_from_noisy2ndit_new_dataset_6and_only_Last.csv', index=False)","f752b0b0":"!mkdir models_checkpoints","64733085":"# optimizer = torch.optim.Adagrad(model.parameters(), lr=0.001)\n# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=40)\n# criterion = nn.CrossEntropyLoss()\n# # torch.cuda.empty_cache()\n# unfreeze_block(model, 6)\n# !mkdir models_checkpoints\n# seed_e\nmodel, train_loss, val_loss = train_model(model, train_loader, test_loader, criterion, \n                                          optimizer,scheduler,  n_epochs=3 ,prefix='weighted_finetune_from_noisy2ndit')","b9206e24":"model.load_state_dict(torch.load('models_checkpoints\/finetune_from_noisy2nditmodel_0.9912087912087912.pt'))","d1d7bce7":"# noisy training expand dataset with best public leaderboard predictions","be67280f":"### unfreeze last block and linear layer at the end","22e8b687":"## next fine tune\n","9d2879ce":"| _  _  Base model  _ _  | resolution|\n|----------------|-----|\n| EfficientNetB0 | 224 |\n| EfficientNetB1 | 240 |\n| EfficientNetB2 | 260 |\n| EfficientNetB3 | 300 |\n| EfficientNetB4 | 380 |\n| EfficientNetB5 | 456 |\n| EfficientNetB6 | 528 |\n| EfficientNetB7 | 600 |","223de594":"# 97.289 \ud83d\ude03\ud83d\ude03\ud83d\ude03\n","a7c9adb5":"## copy data","700fedcd":"## 2 epoch 90% (lr 0.1 no sheduler), ","e2c248ce":"## Test time augmentation","2e0e47fc":"## test unfreezed more layers\n","deb1a6f1":"## eval, make submission\n"}}