{"cell_type":{"054ef0ca":"code","b0536264":"code","5113da72":"code","826ae0da":"code","aa6d03d0":"code","9cea111a":"code","34138a5b":"code","3d0887e5":"code","ceb2a3f2":"code","a8bbc3ca":"code","18897489":"code","d8f0fbfd":"code","a1a4a3bd":"code","bfb0e864":"code","62d58117":"code","4a041b78":"code","ce4df433":"code","aa5ab0e8":"code","c0235d53":"code","75dd562f":"code","7bef3432":"code","9aaf40e3":"code","4fb3c4ad":"code","dbe2e751":"code","17be193f":"code","b08a1252":"code","86232825":"code","224edd94":"code","3302baef":"code","07641efc":"code","b0a7095d":"code","f128d97b":"code","065a019d":"code","eec1923b":"code","41a621ae":"code","15cb8320":"code","e9430669":"code","170d3cf3":"code","3b915417":"code","fb2cd3c9":"code","287e6d8e":"code","e83569e5":"code","46517fa4":"code","7d524fff":"code","a8017218":"code","dfe0aeac":"code","7298a896":"code","162d478d":"code","6119de29":"code","1622ecdc":"code","18b13713":"code","0aac9b53":"code","932f9fe7":"code","3f8ba21b":"code","ed2f4650":"code","a5aff089":"code","c98ec648":"code","b3d84bd0":"code","aa860257":"code","2a445f54":"code","fb6c3c89":"code","3e85f9bc":"code","ea5c78aa":"code","a1934967":"markdown","0907292b":"markdown","50bcffa3":"markdown","4dce9c40":"markdown","dee0f0af":"markdown","8885f3f4":"markdown","b7a2f17b":"markdown","8efceba8":"markdown","6023f76f":"markdown","ae6da3ae":"markdown","91a12469":"markdown","b21c827a":"markdown","fc0e869e":"markdown","07dcff57":"markdown","70331995":"markdown","eba3fb75":"markdown","59fad8ef":"markdown","656f5416":"markdown","48e4b7f1":"markdown","c0eed998":"markdown","9c7816ec":"markdown","043ead2f":"markdown","71cbcfac":"markdown","b432aa71":"markdown","ca37397a":"markdown","e687fc73":"markdown","3c3c960d":"markdown","0e074722":"markdown","4be65868":"markdown","0328acda":"markdown","9abaf31f":"markdown","a9ef54f5":"markdown","81639352":"markdown","097505ba":"markdown","28d6597c":"markdown","f3d4ba72":"markdown","d2e1b8c7":"markdown","40362ba1":"markdown","86a4ca00":"markdown","6a2db42e":"markdown","48c17847":"markdown","d0011552":"markdown","139a6383":"markdown","356c6925":"markdown","4ceda47d":"markdown","720ed4ab":"markdown","add6a011":"markdown","07313f0e":"markdown","1d074918":"markdown","cf503a60":"markdown","468ba7f8":"markdown","2cdf8971":"markdown","063cec5c":"markdown","6b51ed31":"markdown","827b48ec":"markdown","b510b21c":"markdown","529d3292":"markdown","72c71946":"markdown","e7d7de41":"markdown"},"source":{"054ef0ca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b0536264":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nplt.style.use('fivethirtyeight')\nfrom scipy import stats\nfrom scipy.stats import rankdata, norm\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, KFold, GroupKFold, GridSearchCV, StratifiedKFold\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n\n\nfrom sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier, OrthogonalMatchingPursuit\nfrom sklearn.svm import SVC, NuSVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import *\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, IterativeImputer\nimport time, os, warnings, random, string, re, gc, sys\n\nimport category_encoders as ce\n\nimport lightgbm as lgb\nimport catboost as cb\n\n\n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n    \nfrom IPython.display import display\npd.set_option('display.max_colwidth', 300)\n\ndef set_seed(seed=4242):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\nset_seed()\n\n","5113da72":"train = pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ntest = pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')\nanswer = np.load('\/kaggle\/input\/job-change-dataset-answer\/jobchange_test_target_values.npy')\n\ntrain.shape, test.shape, answer.shape","826ae0da":"train.info()","aa6d03d0":"cats = [c for c in train.columns if train[c].dtypes=='object']\ncats","9cea111a":"train.columns = ['enrollee_id', 'city_nom', 'city_development_index', 'gender_nom',\n       'relevent_experience_nom', 'enrolled_university_nom', 'education_level_ord',\n       'major_discipline_nom', 'experience_ord', 'company_size_ord', 'company_type_nom',\n       'last_new_job_ord', 'training_hours', 'target']\n\ntest.columns = ['enrollee_id', 'city_nom', 'city_development_index', 'gender_nom',\n       'relevent_experience_nom', 'enrolled_university_nom', 'education_level_ord',\n       'major_discipline_nom', 'experience_ord', 'company_size_ord', 'company_type_nom',\n       'last_new_job_ord', 'training_hours']\n\nordinals= [c for c in train.columns if 'ord' in c]\nnominals = [c for c in train.columns if 'nom' in c]\nordinals","34138a5b":"tr_orig = train.copy()\nts_orig = test.copy()\n\nsns.countplot(train.target)","3d0887e5":"fig, axs = plt.subplots(ncols=2, figsize=(25, 7))\nsns.distplot(train.city_development_index,bins=50,  fit=norm,kde=True,kde_kws={\"shade\": True}, norm_hist=True,  color='darkcyan', ax=axs[0])\nsns.distplot(train.training_hours,bins=50,  fit=norm,kde=True,kde_kws={\"shade\": True}, norm_hist=True,  color='darkcyan', ax=axs[1])\naxs[0].set_title('Train Vs Test')\nsns.distplot(test.city_development_index,bins=50,  fit=norm,kde=True,kde_kws={\"shade\": True}, norm_hist=True,  color='k', ax=axs[0])\nsns.distplot(test.training_hours,bins=50,  fit=norm,kde=True,kde_kws={\"shade\": True},norm_hist=True,  color='k', ax=axs[1])\naxs[1].set_title('Train Vs Test')","ceb2a3f2":"fig, axs = plt.subplots(ncols=2, figsize=(25, 7))\nsns.distplot(train[train.target==1].city_development_index,bins=50,  fit=norm,kde=True,kde_kws={\"shade\": True}, norm_hist=True,  color='teal', ax=axs[0])\nsns.distplot(train[train.target==0].city_development_index,bins=50,  fit=norm,kde=True,kde_kws={\"shade\": True}, norm_hist=True,  color='darkred', ax=axs[0])\naxs[0].set_title('Distribution of target values')\n\nsns.distplot(train[train.target==1].training_hours,bins=50,  fit=norm,kde=True,kde_kws={\"shade\": True}, norm_hist=True,  color='teal', ax=axs[1])\nsns.distplot(train[train.target==0].training_hours,bins=50,  fit=norm,kde=True,kde_kws={\"shade\": True},norm_hist=True,  color='darkred', ax=axs[1])\naxs[1].set_title('Distribution of target values')\nplt.show()","a8bbc3ca":"del train['enrollee_id']\ndel test['enrollee_id']","18897489":"cats = nominals+ordinals\n\n\ndef analyse_cats(df, cat_cols):\n    d = pd.DataFrame()\n    cl = [];u = [];s =[]; nans =[]\n    for c in cat_cols:\n        #print(\"column:\" , c ,\"--Uniques:\" , train[c].unique(), \"--Cardinality:\", train[c].unique().size)\n        cl.append(c); u.append(df[c].unique());s.append(df[c].unique().size);nans.append(df[c].isnull().sum())\n        \n    d['\"feat\"'] = cl;d[\"uniques\"] = u; d[\"cardinality\"] = s; d[\"nans\"] = nans\n    return d\n\nplt.style.use('fivethirtyeight')\ncatanadf = analyse_cats(train, cats)\ncatanadf","d8f0fbfd":"pd.DataFrame(train.isna().sum(axis=0)\/len(train), columns=['missing percent']).sort_values('missing percent', ascending=False)","a1a4a3bd":"def null_analysis(df):\n  '''\n  desc: get nulls for each column in counts & percentages\n  arg: dataframe\n  return: dataframe\n  '''\n  null_cnt = df.isnull().sum() # calculate null counts\n  null_cnt = null_cnt[null_cnt!=0] # remove non-null cols\n  null_percent = null_cnt \/ len(df) * 100 # calculate null percentages\n  null_table = pd.concat([pd.DataFrame(null_cnt), pd.DataFrame(null_percent)], axis=1)\n  null_table.columns = ['counts', 'percentage']\n  null_table.sort_values('counts', ascending=False, inplace=True)\n  return null_table\n\nnull_table = null_analysis(train)\n#plt.bar(null_table.reset_index(), x='index', y='percentage', text='counts', height=500)\nnull_table","bfb0e864":"null_table.reset_index()","62d58117":"plt.figure(figsize=(14, 5))\nsns.barplot(data= null_table.reset_index()[['index', 'counts']], x= 'counts', y='index', palette='gray')","4a041b78":"train = tr_orig.copy()\ntest = ts_orig.copy()\ndel train['enrollee_id']\ndel test['enrollee_id']","ce4df433":"for c in ordinals:\n    print(c,list(train[c].unique()))","aa5ab0e8":"def OrdMapping(df):\n    \n    \n    education_level_ord_mapping = {'Primary School': 0,'High School': 1,'Graduate': 2 , 'Masters':3,'Phd': 4 }\n    experience_ord_mapping= {'<1':0, '1':2, '2':3, '3':4,'4':5, '5':6, '6':7, '7':8, '8':9,'9':10, '10':11, '12':13, '13':14,'14':15, '15':16, '16':17,  '17':18,'18':19, '19':20,'20':21,'>20':22}\n    company_size_ord_mapping = {'<10':0 ,'10\/49':1,'50-99':2, '100-500':3, '500-999':4, '1000-4999':5,'5000-9999':6, '10000+':7}\n    \n    last_new_job_ord_mapping = {'never':0,'1':1,'2':2,'3':3,'4':4,'>4':5}\n    df['education_level_ord'] = df.education_level_ord.map(education_level_ord_mapping)\n    df['experience_ord'] = df.experience_ord.map(experience_ord_mapping)\n    df['company_size_ord'] = df.company_size_ord.map(company_size_ord_mapping)\n    df['last_new_job_ord'] = df.last_new_job_ord.map(last_new_job_ord_mapping)\n     \n    return df\n\ntrain = OrdMapping(train)\ntest  = OrdMapping(test)\n\n\ntrain.head()","c0235d53":"for c in nominals:\n    le = LabelEncoder()\n    le.fit(list(train[c].astype(str)) + list(test[c].astype(str)))\n    train[c] = le.transform(train[c].astype(str))\n    test[c] = le.transform(test[c].astype(str))\n    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n    print('target mapping :  ',c ,  le_name_mapping)","75dd562f":"cats = ordinals + nominals\n\nimp = SimpleImputer(strategy='most_frequent')\ntrain[cats] = imp.fit_transform(train[cats])\ntest[cats]  = imp.transform(test[cats])\n\ntrain.isnull().sum()\n","7bef3432":"target = train.pop('target')\nnums = [c for c in train.columns if c not in  cats]\n\n\nss = StandardScaler()\ntrain[nums]= ss.fit_transform(train[nums])\ntest[nums]= ss.fit_transform(test[nums])","9aaf40e3":"score_auc = []\n\noof_lr = np.zeros(len(train))\npred_lr = np.zeros(len(test))\n\nfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n\nfor fold_ , (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print('fold:', fold_, '  - Starting ...')\n    trn_data, val_data = train.iloc[train_ind], train.iloc[val_ind]\n    y_train, y_val = target.iloc[train_ind], target.iloc[val_ind]\n    \n    lr = LogisticRegression(C=0.1, solver='newton-cg',max_iter=100, random_state=2020, class_weight={0: 1, 1:3})\n    lr.fit(trn_data, y_train)\n    oof_lr[val_ind] = lr.predict_proba(val_data)[:, 1]\n    y = lr.predict_proba(trn_data)[:, 1]\n    print('train auc:' , roc_auc_score(y_train, y),'val auc:' , roc_auc_score(y_val, oof_lr[val_ind]))\n    \n    score_auc.append(roc_auc_score(y_val, oof_lr[val_ind]))\n    pred_lr += lr.predict_proba(test)[:, 1]\/folds.n_splits\n    \nprint('-'*50)\nprint(' Model auc:  ', np.mean(score_auc))\n","4fb3c4ad":"roc_auc_score(answer, pred_lr)","dbe2e751":"score_auc = []\n\noof_qd = np.zeros(len(train))\npred_qd = np.zeros(len(test))\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold_ , (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print('fold:', fold_, '  - Starting ...')\n    trn_data, val_data = train.iloc[train_ind], train.iloc[val_ind]\n    y_train, y_val = target[train_ind], target[val_ind]\n    \n    qd =   QuadraticDiscriminantAnalysis()\n    qd.fit(trn_data, y_train)\n    oof_qd[val_ind] = qd.predict(val_data)\n    y = qd.predict(trn_data)\n    print('train auc:' , roc_auc_score(y_train, y))\n    \n    print('val auc:' , roc_auc_score(y_val, oof_qd[val_ind]))\n       \n    score_auc.append(roc_auc_score(y_val, oof_qd[val_ind]))\n                            \n    pred_qd += qd.predict(test)\/folds.n_splits\n    \nprint('-'*50)\nprint(' Model auc:  ', np.mean(score_auc))\n","17be193f":"roc_auc_score(answer, pred_qd)","b08a1252":"train = tr_orig.copy()\ntest = ts_orig.copy()\n\n\ntarget = train.pop('target')\ndel train['enrollee_id']\ndel test['enrollee_id']\n\ntrain.fillna('UnKnown', inplace=True)\ntest.fillna('UnKnown', inplace=True)\n\n\n\ndata = pd.concat([train, test], axis=0)\ndata = pd.get_dummies(data)\ntrain = data.iloc[:len(train), ]\ntest = data.iloc[:len(test), ]\n\n","86232825":"missing = test.isnull().sum()\nprint(missing[missing>0])\n\ntrain.head()","224edd94":"train.shape, test.shape","3302baef":"\nscore_auc = []\n\noof_svm = np.zeros(len(train))\npred_svm = np.zeros(len(test))\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold_ , (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print('fold:', fold_, '  - Starting ...')\n    trn_data, val_data = train.iloc[train_ind], train.iloc[val_ind]\n    y_train, y_val = target[train_ind], target[val_ind]\n    \n    lsvm =   LinearSVC(C=.0001, random_state=2020)\n    lsvm.fit(trn_data, y_train)\n    oof_svm[val_ind] = lsvm.predict(val_data)\n    y = lsvm.predict(trn_data)\n    print('train auc:' , roc_auc_score(y_train, y))\n    \n    print('val auc:' , roc_auc_score(y_val, oof_svm[val_ind]))\n    score_auc.append(roc_auc_score(y_val, oof_svm[val_ind]))\n                           \n    pred_svm += lsvm.predict(test)\/folds.n_splits\n    \nprint('-'*50)\nprint(' Model auc:  ', np.mean(score_auc))\n","07641efc":"roc_auc_score(answer, pred_svm)","b0a7095d":"train = tr_orig.copy()\ntest = ts_orig.copy()\n\ntarget = train.pop('target')\ndel train['enrollee_id']\ndel test['enrollee_id']\n","f128d97b":"daset = pd.concat([train, test], axis=0)\n\n\nfor c in (cats):\n    daset[c+'_freq'] = daset[c].map(daset.groupby(c).size() \/ daset.shape[0])\n    indexer = pd.factorize(daset[c], sort=True)[1]\n    daset[c] = indexer.get_indexer(daset[c])\n\ntrain= daset.iloc[:len(train) , ]\ntest= daset.iloc[len(train): , ]\ncols=train.columns\ntrain.shape, test.shape","065a019d":"train.isnull().sum()","eec1923b":"imp = SimpleImputer(strategy='constant')\ntrain = imp.fit_transform(train)\ntest  = imp.transform(test)\n\nss = StandardScaler()\ntrain = ss.fit_transform(train)\ntest  = ss.transform(test)","41a621ae":"score_auc = []\n\noof_lr = np.zeros(len(train))\npred_lr = np.zeros(len(test))\n\nfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n\nfor fold_ , (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print('fold:', fold_, '  - Starting ...')\n    trn_data, val_data = train[train_ind], train[val_ind]\n    y_train, y_val = target.iloc[train_ind], target.iloc[val_ind]\n    \n    lr = LogisticRegression(C= 1,max_iter=200, random_state=2020, class_weight={0: 1, 1:3})\n    lr.fit(trn_data, y_train)\n    oof_lr[val_ind] = lr.predict_proba(val_data)[:, 1]\n    y = lr.predict_proba(trn_data)[:, 1]\n    print('train auc:' , roc_auc_score(y_train, y))\n    \n    print('val auc:' , roc_auc_score(y_val, oof_lr[val_ind]))\n    score_auc.append(roc_auc_score(y_val, oof_lr[val_ind]))\n    \n                        \n    pred_lr += lr.predict_proba(test)[:, 1]\/folds.n_splits\n    \nprint('-'*50)\nprint(' Model auc:  ', np.mean(score_auc))\n","15cb8320":"roc_auc_score(answer, pred_lr)","e9430669":"train = tr_orig.copy()\ntest = ts_orig.copy()\n\ntarget = train.pop('target')\ndel train['enrollee_id']\ndel test['enrollee_id']","170d3cf3":"def add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https:\/\/kaggle2.blob.core.windows.net\/forum-message-attachments\/225952\/7441\/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior  \n    \"\"\" \n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 \/ (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) \/ smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","3b915417":"\nsmoothing =1\nimport category_encoders as ce\noof = pd.DataFrame([])\nfrom sklearn.model_selection import StratifiedKFold\nfor tr_idx, oof_idx in StratifiedKFold(n_splits=5, random_state=2020, shuffle=True).split(train, target):\n    ce_target_encoder = ce.TargetEncoder(cols = cats, smoothing=smoothing)\n    ce_target_encoder.fit(train.iloc[tr_idx, :], target.iloc[tr_idx])\n    oof = oof.append(ce_target_encoder.transform(train.iloc[oof_idx, :]), ignore_index=False)\nce_target_encoder = ce.TargetEncoder(cols = cats, smoothing=smoothing)\nce_target_encoder.fit(train, target);  train = oof.sort_index(); test = ce_target_encoder.transform(test)\n","fb2cd3c9":"score_auc = []\n\noof_lr = np.zeros(len(train))\npred_lr = np.zeros(len(test))\n\nfolds = KFold(n_splits=3, shuffle=True, random_state=42)\n\nfor fold_ , (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print('fold:', fold_, '  - Starting ...')\n    trn_data, val_data = train.iloc[train_ind], train.iloc[val_ind]\n    y_train, y_val = target.iloc[train_ind], target.iloc[val_ind]\n    \n    lr = LogisticRegression(C= 0.1,max_iter=100, random_state=2020, class_weight={0: 1, 1:3})\n    lr.fit(trn_data, y_train)\n    oof_lr[val_ind] = lr.predict_proba(val_data)[:, 1]\n    y = lr.predict_proba(trn_data)[:, 1]\n    print('train auc:' , roc_auc_score(y_train, y))\n    \n    print('val auc:' , roc_auc_score(y_val, oof_lr[val_ind]))\n    \n    score_auc.append(roc_auc_score(y_val, oof_lr[val_ind]))\n                           \n    pred_lr += lr.predict_proba(test)[:, 1]\/folds.n_splits\n    \nprint(' Model auc: -------> ', np.mean(score_auc))\n","287e6d8e":"roc_auc_score(answer, pred_lr)","e83569e5":"train = tr_orig.copy()\ntest = ts_orig.copy()\n\ntarget = train.pop('target')\ndel train['enrollee_id']\ndel test['enrollee_id']\n\ndata = pd.concat([train, test], axis=0)\n","46517fa4":"#Create object for hash encoder\nencoder=ce.HashingEncoder(cols=cats,n_components=6)\ndata = encoder.fit_transform(data)\ntrain = data.iloc[:len(train), ]\ntest = data.iloc[len(train):, ]\n","7d524fff":"score_auc = []\n\noof_lr = np.zeros(len(train))\npred_lr = np.zeros(len(test))\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold_ , (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print('fold:', fold_, '  - Starting ...')\n    trn_data, val_data = train.iloc[train_ind], train.iloc[val_ind]\n    y_train, y_val = target.iloc[train_ind], target.iloc[val_ind]\n    \n    lr = LogisticRegression(C= 0.1,max_iter=200, random_state=2020, class_weight={0: 1, 1:3})\n    lr.fit(trn_data, y_train)\n    oof_lr[val_ind] = lr.predict_proba(val_data)[:, 1]\n    y = lr.predict_proba(trn_data)[:, 1]\n    print('train auc:' , roc_auc_score(y_train, y))\n    \n    print('val auc:' , roc_auc_score(y_val, oof_lr[val_ind]))\n      \n    score_auc.append(roc_auc_score(y_val, oof_lr[val_ind]))\n    \n                        \n    pred_lr += lr.predict_proba(test)[:, 1]\/folds.n_splits\n    \nprint(' Model auc: -------> ', np.mean(score_auc))\n","a8017218":"roc_auc_score(answer, pred_lr)","dfe0aeac":"train = tr_orig.copy()\ntest = ts_orig.copy()\ntarget = train.pop('target')\ndel train['enrollee_id']\ndel test['enrollee_id']","7298a896":"import category_encoders as ce\n\nencoder = ce.WOEEncoder(cols=cats, drop_invariant=False, return_df=True, handle_unknown='value', handle_missing='value', random_state=42, randomized=False, sigma=0.05, regularization=1.0)\n\nencoder.fit(train, target)\ntrain = encoder.transform(train)\ntest = encoder.transform(test)\n\n\n\nss = StandardScaler()\ntrain = ss.fit_transform(train)\ntest  = ss.transform(test)","162d478d":"score_auc = []\n\noof_lr = np.zeros(len(train))\npred_lr = np.zeros(len(test))\n\nfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n\nfor fold_ , (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print('fold:', fold_, '  - Starting ...')\n    trn_data, val_data = train[train_ind], train[val_ind]\n    y_train, y_val = target.iloc[train_ind], target.iloc[val_ind]\n    \n    lr = LogisticRegression(C= 1,max_iter=100, random_state=2020, class_weight={0: 1, 1:3})\n    lr.fit(trn_data, y_train)\n    oof_lr[val_ind] = lr.predict_proba(val_data)[:, 1]\n    y = lr.predict_proba(trn_data)[:, 1]\n    print('train auc:' , roc_auc_score(y_train, y))\n    \n    print('val auc:' , roc_auc_score(y_val, oof_lr[val_ind]))\n    score_auc.append(roc_auc_score(y_val, oof_lr[val_ind]))\n                            \n    pred_lr += lr.predict_proba(test)[:, 1]\/folds.n_splits\n    \nprint(' Model auc: -------> ', np.mean(score_auc))\n","6119de29":"roc_auc_score(answer, pred_lr)","1622ecdc":"score_auc = []\n\noof_qd = np.zeros(len(train))\npred_qd = np.zeros(len(test))\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold_ , (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print('fold:', fold_, '  - Starting ...')\n    trn_data, val_data = train[train_ind], train[val_ind]\n    y_train, y_val = target[train_ind], target[val_ind]\n    \n    qd =   QuadraticDiscriminantAnalysis()\n    qd.fit(trn_data, y_train)\n    oof_qd[val_ind] = qd.predict(val_data)\n    y = qd.predict(trn_data)\n    print('train auc:' , roc_auc_score(y_train, y))\n    \n    print('val auc:' , roc_auc_score(y_val, oof_qd[val_ind]))\n      \n    score_auc.append(roc_auc_score(y_val, oof_qd[val_ind]))\n                           \n    pred_qd += qd.predict(test)\/folds.n_splits\n    \nprint('-'*50)\nprint(' Model auc:  ', np.mean(score_auc))\n","18b13713":"roc_auc_score(answer, pred_qd)","0aac9b53":"train = tr_orig.copy()\ntest = ts_orig.copy()\n\ntarget = train.pop('target')\ndel train['enrollee_id']\ndel test['enrollee_id']\n\ntrain.fillna('UnKnown', inplace=True)\ntest.fillna('UnKnown', inplace=True)\n\ncategorical_features_indices = np.where(train.dtypes == 'object')[0]\ncategorical_features_indices","932f9fe7":"\ncat_score = []\n# Split data with kfold\nkfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=2018)\ntrain_features = train.columns\n# Make importance dataframe\nimportances = pd.DataFrame()\n\noof_preds = np.zeros(train.shape[0])\nsub_preds = np.zeros(test.shape[0])\n\nfor n_fold, (trn_idx, val_idx) in enumerate(kfolds.split(train, target)):\n    X_train, y_train = train.iloc[trn_idx], target.iloc[trn_idx]\n    X_valid, y_valid = train.iloc[val_idx], target.iloc[val_idx]\n    \n    # CatBoost Regressor estimator\n    model = cb.CatBoostClassifier(\n        learning_rate = 0.01,\n        iterations = 2000,\n        eval_metric = 'AUC',\n        allow_writing_files = False,\n        od_type = 'Iter',\n        bagging_temperature = 0.8,\n        class_weights = [0.3, 1], \n        depth = 6,\n        od_wait = 20,\n        silent = False\n    )\n    \n    # Fit\n    model.fit(\n        X_train, y_train,\n        cat_features=categorical_features_indices,\n        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n        verbose=100,\n        early_stopping_rounds=100\n    )\n    \n    # Feature importance\n    imp_df = pd.DataFrame()\n    imp_df['feature'] = train_features\n    imp_df['gain'] = model.get_feature_importance()\n    imp_df['fold'] = n_fold + 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n    \n    oof_preds[val_idx] = model.predict_proba(X_valid)[:, 1]\n    cat_score.append(roc_auc_score(y_valid, oof_preds[val_idx]))\n    test_preds = model.predict_proba(test)[:, 1]\n    sub_preds += test_preds \/ kfolds.n_splits\n    \nprint(np.mean(cat_score))\n","3f8ba21b":"roc_auc_score(answer, sub_preds)","ed2f4650":"importances['gain_log'] = importances['gain']\nmean_gain = importances[['gain', 'feature']].groupby('feature').mean()\nimportances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(20, 8))\nsns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False))","a5aff089":"oof_cb_rnd = np.where(oof_preds>.50, 1, 0)\ncf_matrix = confusion_matrix(target, (oof_cb_rnd)) \ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nplt.figure(figsize=(8, 6))\nsns.set(font_scale=1.4)\nplt.style.use('seaborn-poster')\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='vlag_r')","c98ec648":"train = tr_orig.copy()\ntest = ts_orig.copy()\n\ntarget = train.pop('target')\ndel train['enrollee_id']\ndel test['enrollee_id']","b3d84bd0":"cats = [c for c in train.columns if train[c].dtypes =='object']","aa860257":"for c in cats:\n    le=LabelEncoder()\n    le.fit(list(train[c].astype('str')) + list(test[c].astype('str')))\n    train[c] = le.transform(list(train[c].astype(str))) \n    test[c] = le.transform(list(test[c].astype(str))) \ntrain.head()","2a445f54":"\nlgb_params = {\n    \n 'objective': 'binary', \n 'boosting': 'gbdt', \n 'bagging_fraction': 0.9,\n 'bagging_frequency': 1,\n 'cat_smooth': 70,\n 'feature_fraction': 0.9,\n 'learning_rate': 0.01,\n 'min_child_samples': 20,\n 'min_data_per_group': 100,\n 'num_leaves': 18,\n #'reg_alpha': 1.,\n #'reg_lambda': 3., \n 'metric':'auc', \n 'unbalance':True}\n    \n    \n    \n    \n\n\noof_lgb = np.zeros(len(train))\npred_lgb = np.zeros(len(test))\n\nscores = []\n\nfeature_importances_gain = pd.DataFrame()\nfeature_importances_gain['feature'] = train.columns\n\nfeature_importances_split = pd.DataFrame()\nfeature_importances_split['feature'] = train.columns\n\n\nfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n\nfor fold_, (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print(\"fold : ---------------------------------------\", fold_)\n    trn_data = lgb.Dataset(train.iloc[train_ind], label=target.iloc[train_ind], categorical_feature=cats) #-------> Specify Categorical feature for lgb\n    val_data= lgb.Dataset(train.iloc[val_ind], label=target.iloc[val_ind], categorical_feature=cats)  #-------> Specify Categorical feature for lgb\n    \n    lgb_clf = lgb.train(lgb_params, trn_data, num_boost_round=1000, valid_sets=(trn_data, val_data), verbose_eval=100, early_stopping_rounds=100)\n    oof_lgb[val_ind] = lgb_clf.predict(train.iloc[val_ind], num_iteration= lgb_clf.best_iteration)\n    print(\"fold:\", fold_, \"roc_auc ==\", roc_auc_score(target.iloc[val_ind], oof_lgb[val_ind]))\n    scores.append(roc_auc_score(target.iloc[val_ind], oof_lgb[val_ind]))\n    \n    feature_importances_gain['fold_{}'.format(fold_ + 1)] = lgb_clf.feature_importance(importance_type='gain')\n    feature_importances_split['fold_{}'.format(fold_ + 1)] = lgb_clf.feature_importance(importance_type='split')\n    \n    pred_lgb += lgb_clf.predict(test, num_iteration=lgb_clf.best_iteration)\/folds.n_splits\n    \nprint(' \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ model roc_auc \/\/\/\/\/\/\/\/\/\/\/\/\/\/ : ' , np.mean(scores))\n    \nnp.save('oof_lgb', oof_lgb)\nnp.save('pred_lgb', pred_lgb)","fb6c3c89":"roc_auc_score(answer, pred_lgb)","3e85f9bc":"feature_importances_gain['average'] = feature_importances_gain[['fold_{}'.format(fold + 1) for fold in range(folds.n_splits)]].mean(axis=1)\nfeature_importances_gain.to_csv('feature_importances.csv')\n\nplt.figure(figsize=(20, 8))\nsns.barplot(data=feature_importances_gain.sort_values(by='average', ascending=False).head(100),palette='Reds_r',  x='average', y='feature');\nplt.title('TOP n feature importance over {} folds average'.format(folds.n_splits));","ea5c78aa":"oof_lgb_rnd = np.where(oof_lgb>.50, 1, 0)\ncf_matrix = confusion_matrix(target, (oof_lgb_rnd)) \ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nplt.figure(figsize=(8, 6))\nsns.set(font_scale=1.4)\nplt.style.use('seaborn-poster')\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='vlag_r')","a1934967":"![oh1](https:\/\/www.renom.jp\/notebooks\/tutorial\/preprocessing\/onehot\/ConvertCategorical_en.png)","0907292b":"Dummy encocoding scheme is similar to one-hot encoding. This categorical data encoding method transforms the categorical variable into a set of binary variables (also known as dummy variables). In the case of one-hot encoding, for N categories in a variable, it uses N binary variables. The dummy encoding is a small improvement over one-hot-encoding. Dummy encoding uses N-1 features to represent N labels\/categories.\n\nTo understand this better let\u2019s see the image below. Here we are coding the same data using both one-hot encoding and dummy encoding techniques. While one-hot uses 3 variables to represent the data whereas dummy encoding uses 2 variables to code 3 categories.","50bcffa3":"To understand Hash encoding it is necessary to know about hashing. Hashing is the transformation of arbitrary size input in the form of a fixed-size value. We use hashing algorithms to perform hashing operations i.e to generate the hash value of an input. Further, hashing is a one-way process, in other words, one can not generate original input from the hash representation.\n\nHashing has several applications like data retrieval, checking data corruption, and in data encryption also. We have multiple hash functions available for example Message Digest (MD, MD2, MD5), Secure Hash Function (SHA0, SHA1, SHA2), and many more.\n\nJust like one-hot encoding, the Hash encoder represents categorical features using the new dimensions. Here, the user can fix the number of dimensions after transformation using n_component argument. Here is what we mean \u2013 A feature with 5 categories can be represented using N new features similarly, a feature with 100 categories can also be transformed using N new features. \n\nBy default, the Hashing encoder uses the md5 hashing algorithm but a user can pass any algorithm of his choice. ","4dce9c40":"Changin column name to be identified nominals and ordinals:","dee0f0af":">In this noteboo we will review some encoding techniques along side with applying some algorithms and base models, mostly with class weight as a technique to handle imbamced data. In most models using class weight parameter prevented overfiting and boosted the performance .  All models are as base and boosting is possible by applying advanced feat eng, feat selection, model tuning, applying other techniques for handling imbalanced data , ensemblibg and so on.\n\nMorever different imputing techniques applied according the applied algorithm . Some linear models required scaling before feeding to the models. You can apply your ideas on based models to boost performance.    \n","8885f3f4":"Advantages of one-hot encoding\n\n- Does not assume the distribution of categories of the categorical variable.\n- Keeps all the information of the categorical variable.\n- Not so Suitable for tree based models.\n\nLimitations of one-hot encoding\n- Expands the feature space.\n- Does not add extra information while encoding.\n- Many dummy variables may be identical, and this can introduce redundant information.","b7a2f17b":"Here we will add frequency encoded features to labeled encoded features:","8efceba8":"### Cats","6023f76f":"Folowing we specified categorical feats for lgb .  As lgb is using target encoding i used 2 additive parameters 'min_data_per_group' and 'cat_smooth' and changed default values. Thease parameters help to prevent overfitting, similar to what we did with target encoding for Logistic regression throug KFold. \n\n","ae6da3ae":"## **7. LightGBM and Cats**","91a12469":"## **6. CatBoost and Cats**","b21c827a":"## **4. Target Encoding - Mean Likelihood Encoding ,\"The Right Way !\"**","fc0e869e":"## **5. Hash Encoding**","07dcff57":"## **2. One-Hot Encoding, Dummy Encoding**","70331995":"## **Quick EDA**","eba3fb75":">## **Content:**\n>    \n>#### 1. Quick EDA\n>#### 2. Label Encoding, Ordinal Encoding\n>#### 3. One-Hot Encoding, Dummy Encoding\n>#### 4. Frequency Encoding\n>#### 5. Target Encoding\n>#### 6. Hash Encoding\n>#### 7. Weight of Evidence\n>#### 8. CatBoost and Cats\n>#### 9. LightGBM and Cats\n","59fad8ef":"![gif](https:\/\/lh3.googleusercontent.com\/YyC6zhZXIRLiLa87ISWdqwujDaNENXkf1ZqLV6JSKB8a-NGn5aVD0WdRYuookPv9Bfr9-danuXOhfweUGJBMfY9QS59KdxXHspfWRj9e26x9HgiMjEcXP2Jp8IlibnDiLst_sr5P)","656f5416":"Label encoding  includes replacing the categories with digits from 1 to n (or 0 to n-1, depending on the implementation),where n is the number of the variable\u2019s distinct categories (the cardinality), and these numbers are assigned arbitrarily.","48e4b7f1":"![we](https:\/\/miro.medium.com\/max\/412\/1*tCDfdEZA9G7zHZV3bGvCfA.png)","c0eed998":"Advantages of Mean encoding\n\n- Does not expand the feature space.\n- Creates a monotonic relationship between categories and the target.\n\nLimitations of Mean encoding\n- May lead to overfitting.\n- May lead to a possible loss of value if two categories have the same mean as the target\u2014in these cases, the same number replaces the original.","9c7816ec":"### **Logistic Regression**","043ead2f":"### **Conclusion**\n- The results shows we gained acceptable results as these models are base models.In most cases we handled imbalanced data with class weight. Potential improvement exists for Logistic Regression, Lightgbm and CatBoost.\n- It is interesting we could get a competitive result from LogisticRegression by applying weght of evidence and target encoding (.7942 auc and .7915 ) which in comaprision of CatBoost and LightGBM (.7992 auc and .7971) are good performance\n- According to CB and LGB feature importance we can see 'city_development_index', 'company_size' and 'education_level' along side with 'city' are most important factor in hob changes. Data scientists in cities with better development index which work in higher size of company don't tend to change their jobs (corr is negative)","71cbcfac":"The performance of a machine learning model not only depends on the model and the hyperparameters but also on how we process and feed different types of variables to the model. Since most machine learning models only accept numerical variables, preprocessing the categorical variables becomes a necessary step. We need to convert these categorical variables to numbers such that the model is able to understand and extract valuable information.\n\n\n\nUsually there are 2 kinds of categorical data:\n\n- Ordinal Data: The categories have an inherent order\nin Job Change dataset are:[ 'education_level', 'experience','company_size','last_new_job']\n- Nominal Data: The categories do not have an inherent order \nin Job chanege dataset are:['city','gender','enrolled_university','major_discipline', 'company_type','relevent_experience',]\n*(binary data could be nominal or ordinal)*\n\nGenerally:\nIn Ordinal data, while encoding, one should retain the information regarding the order in which the category is provided. \n\nWhile encoding Nominal data, we have to consider the presence or absence of a feature. In such a case, no notion of order is present. \n\n\n\n","b432aa71":"### **Logistic Regression**","ca37397a":"According to: \n - https:\/\/maxhalford.github.io\/blog\/target-encoding\/\n - https:\/\/medium.com\/@pouryaayria\/k-fold-target-encoding-dfe9a594874b \n \nwe have better implement target encoding through KFold and  with smoothing.\n\nmin_samples_leaf define a threshold where prior and target mean (for a given category value) have the same weight. Below the threshold prior becomes more important and above mean becomes more important.How weight behaves against value counts is controlled by smoothing parameter","e687fc73":"### **Logistic Regression**","3c3c960d":"## **1. Label Encoding, Ordinal Encoding**","0e074722":"Advantages of integer (label) encoding\n- Straightforward to implement.\n- Does not expand the feature space.\n- Can work well enough with tree-based algorithms.\n- Allows agile benchmarking of machine learning models.\n\nLimitations of integer (label) encoding\n- Does not add extra information while encoding.\n- Not suitable for linear models.\n- Does not handle new categories in the test set automatically.\n- Creates an order relationship between the categories.","4be65868":"I will use dummy encoding:","0328acda":"## **Introduction**","9abaf31f":"## **6. Encoding using \u201cWeight of Evidence\u201d**\n\n","a9ef54f5":"using https:\/\/contrib.scikit-learn.org\/category_encoders\/index.html#","81639352":"Advantages of Count or Frequency encoding\n- Straightforward to implement.\n- Does not expand the feature space.\n- Can work well with tree-based algorithms.\n\nLimitations of Count or Frequency encoding\n\n- Does not handle new categories in the test set automatically.\n- We can lose valuable information if there are two different categories with the same amount of observations count\u2014this is because we replace them with the same number.","097505ba":"Advantages of the Weight of evidence encoding\n- Creates a monotonic relationship between the target and the variables.\n- Orders the categories on a \u201clogistic\u201d scale, which is natural for logistic regression.\n- We can compare the transformed variables because they are on the same scale. Therefore, it\u2019s possible to determine which one is more predictive.\n\nLimitations of the Weight of evidence encoding\n- May lead to overfitting.\n- Not defined when the denominator is 0.","28d6597c":"seems we lost informations by dummy encoding","f3d4ba72":"Label encoding nominals:","d2e1b8c7":"![dum](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/08\/Screenshot-from-2020-08-12-18-28-24-768x452.png)","40362ba1":"### **LinearSVC**","86a4ca00":"As lgb is inherently tree based we can convert ordinals by LabelEncoder like nominals and then specify them for lgb: ","6a2db42e":"Mean encoding means replacing the category with the mean target value for that category. We start by grouping each category alone, and for each group, we calculate the mean of the target in the corresponding observations. Then we assign that mean to that category. Thus, we encoded the category with the mean of the target.\nHere\u2019s a detailed illustration of mean encoding:\n\n\n![te](https:\/\/miro.medium.com\/max\/653\/1*gbKFmnAGdnaatRm011RLxA.png)\n\n\n","48c17847":"## **3. Frequency Encoding**","d0011552":"# **Categorical Encodings Made Easy: An Overview of Techniques**\n\n### **Note: More complete notebook on Categorical Encoding :**\n\n**Cats on a Hot Tin Roof: Cats Encoding Methods**\n\nhttps:\/\/www.kaggle.com\/arashnic\/cats-on-a-hot-tin-roof-cats-encoding-methods","139a6383":"### LogisticRegression","356c6925":"using https:\/\/contrib.scikit-learn.org\/category_encoders\/index.html#","4ceda47d":"In One-Hot method, we map each category to a vector that contains 1 and 0 denoting the presence of the feature or not. The number of vectors depends on the categories which we want to keep. For high cardinality features, this method produces a lot of columns that slows down the learning significantly. There is a buzz between one hot encoding and dummy encoding and when to use one. They are much alike except one hot encoding produces the number of columns equal to the number of categories and dummy producing is one less. This should ultimately be handled by the modeler accordingly in the validation process.\n\n","720ed4ab":"### QuadraticDiscriminantAnalysis","add6a011":"Lgb sorts the categories according to the training objective at each split. More specifically, LightGBM sorts the histogram (for a categorical feature) according to its accumulated values (sum_gradient \/ sum_hessian) and then finds the best split on the sorted histogram. So the split can be made based on the variable being of one specific level or any subset of levels, so you have 2^N splits available in comparision with e.g of 4 for OHE.\n\nThe algorithm behind above mechanism is Fisher (1958) to find the optimal split over categories. http:\/\/www.csiss.org\/SPACE\/workshops\/2004\/SAC\/files\/fisher.pdf","07313f0e":"So how should we select encoding methods is depends algorithm(s) we apply :\n        \n- Some algorithms can work with categorical data directly e.g LightGBM , CatBoost , or For example, a decision tree can be learned directly from categorical data with no data transform required (this depends on the specific implementation).\n\n- Many machine learning algorithms cannot operate on label data directly. They require all input variables and output variables to be numeric.\n\n- Some implementations of machine learning algorithms require all data to be numerical. For example, scikit-learn has this requirement.\n\n- If we categorize algorithms to linear and tree based models we sholuld consider that generally linear models are sensitive to order of ordinal data so we should select appropriate encoding methods. \n\n\n\n","1d074918":"### **Missing Values**","cf503a60":"It is a way to utilize the frequency of the categories as labels. In the cases where the frequency is related somewhat with the target variable, it helps the model to understand and assign the weight in direct and inverse proportion, depending on the nature of the data.\nReplace the categories by the count of the observations that show that category in the dataset. Similarly, we can replace the category by the frequency -or percentage- of observations in the dataset. \n\n","468ba7f8":"### **QuadraticDiscriminantAnalysis**","2cdf8971":"Note:\n1. Lgb will handle nan values automatically\n2. 'categorical_feature' parameter get cats directly (Numeric)\n3. We used 'unbalance'parameter for handling imbalanced data by lgb (see confusion matrix below), You can use scale_pos_weight too like XGBoost.","063cec5c":"in following implementation:\n- label encoder  will  map nans as a new category so imputation is not a must, So i wont impute nominals and will build new category of nans with label encoder\n\n\n- through the rest of note book we will find out why **generally** label encoding\/ordinal encoding is not suitable for linear model (although it depends on data). Anyway i prefer to encode ordinals by manual mapping and nominal by Scikit.LabelEncoder() if i don't want to apply other techniaues for cat encoding.Because LabelEncoder wont preserve order during mapping and this will confuse  linear models e.g. Logistic Regression.\n","6b51ed31":"Weight of evidence (WOE) is a technique used to encode categorical variables for classification.\nThe rule is simple; WOE is the natural logarithm (ln) of the probability that the target equals 1 divided by the probability of the target equals 0.\nHere is a mathematic formula : WOE = ln (p(1) \/ p(0)).\nWhere p(1) is the probability of the target being 1, and p(0) is the probability of the target being 0.\nIf the result is negative, you can change the sign of the output.\nThe WOE is bigger than 0 if the probability of the target being 0 is more significant and smaller than 0 when the probability of the target being 1 is more significant.\nThis way, the WOE transformation creates an excellent visual representation of the variable. By looking at the WOE encoded variable, you can see which category favors the target being 0 or 1.","827b48ec":"### Numerics","b510b21c":"Good score for logistic regression","529d3292":"When running machine learning algorithms, simply assigning numbers to categorical variables work if a category has only two levels. This is the case for gender (male\/female), bought a product (yes\/no), attended a course (yes\/no). When a category has several levels, as with nationality, assigning numbers to each level implies an order of the levels. This means that one level of the category has a lower rank than another level. While this makes sense for ordinal variables (e.g., preferences of food items or educational degree), it is a wrong assumption for nominal variables such as color preferences, nationality, residential city specially when we use linear Algorithms. Algorithms like CatBoost have different perspective to solve this problem.  \n\nWe can use CatBoost without any explicit pre-processing to convert categories into numbers. CatBoost converts categorical values into numbers using various statistics on combinations of categorical features and combinations of categorical and numerical features.\n\nIn detail , Catboost calculates for every category of a nominal variable , a value (target-based statistic). This is done using a number of steps:\nWe begin with one categorical feature (e.g., Nationality). This is called x.\nIn one randomly chosen row (k-th row in the training data set), we exchange one random level of this categorical feature (i-th level of x) with a number (e.g., Dutch by 5)\nThis number (in our example 5) is usually based on the target variable (the one we want to predict) conditional on the category level. In other words, the target number is based on the expected outcome variable.\nA splitting attribute is used to create two sets of the training data: One set that has all categories (e.g., German, French, Indian etc) who will have greater target variable than the one computed in step 3, and the other set with smaller target variables.\n\n![catb](https:\/\/developer-blogs.nvidia.com\/wp-content\/uploads\/2018\/12\/catboost_hero.png)\n\nIn their [paper](http:\/\/learningsys.org\/nips17\/assets\/papers\/paper_11.pdf) authors describe how catboost is dealing with categorical features. The standard way is to compute some statistics, such as median, based on the label values of the category. However, this creates problems if there is only one example for a label value. In this case, the numerical value of the category would be the same than the label value. For example if in our example with nationalities, the category Belgian is assigned the value 2, and there is only 1 Belgian student, this student would get the value 2 for nationality. This can create problems of overfitting.\n\nTo avoid this problem, the authors designed a solution which involves randomly changing the order of rows in the complete data set. We perform a random permutation of the data set and for each example we compute average label value for the example with the same category value placed before the given one in the permutation .In their paper they also describe how different features are combined to create a new feature. Think about it, every individual observations of categorical and numerical data points describe one observation. The chances that two observations are exactly identical is slim. Hence, different categorical values and numerical values could be combined to create a unique merged categorical variable which contains all the different individual choices. While this might sound easy, doing this for all potential types of combinations will be computational intensive.\nAnother way to combine different features is to do a greedy search at every tree split. Catboost does this by combining all categorical and numerical values at the current tree with all categorical values in the data set.\n\nTransforming categorical features to numerical features methods are:\n\n\n- Borders\n- Buckets\n- BinarizedTargetMeanValue\n- Counter\n\n\n You can read more about it [here](https:\/\/catboost.ai\/docs\/concepts\/algorithm-main-stages_cat-to-numberic.html).","72c71946":"Since Hashing transforms the data in lesser dimensions, it may lead to loss of information. Another issue faced by hashing encoder is the collision. Since here, a large number of features are depicted into lesser dimensions, hence multiple values can be represented by the same hash value, this is known as a collision.\n\nMoreover, hashing encoders have been very successful in some Kaggle competitions. It is great to try if the dataset has high cardinality features.","e7d7de41":"### **Logistic Regression**"}}