{"cell_type":{"5d2a528e":"code","426db881":"code","d6eda1b9":"markdown","b11e204d":"markdown","4f4ab211":"markdown","dcf5bcb1":"markdown"},"source":{"5d2a528e":"# import matplotlib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,RandomForestRegressor\nfrom sklearn import datasets\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import RFE\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix,accuracy_score,r2_score\n\n#Reading in my student information file\nSpO=pd.read_csv(\"..\/input\/StudentsPerformance.csv\")\n#Computing the Total score as a summation of the Math, Reading and Writing scores\nSpO[\"TotalScore\"]=SpO[[\"math score\",\"reading score\",\"writing score\"]].sum(axis=1)\n#Copy of SpOriginal file\nSp=SpO\n\n#Setting the Level of Student criteria \nSp[\"Level\"]=np.NaN\n\nfor ind,TS in zip(Sp.index,Sp[\"TotalScore\"]):\n    if (TS<=175.000000):\n        Sp[\"Level\"][ind]=0\n        \n    elif (TS>175.000000 and TS<=205.000000):\n        Sp[\"Level\"][ind]=1\n        \n    elif (TS>205.000000 and TS<=233.000000):\n        Sp[\"Level\"][ind]=2\n    else:\n        Sp[\"Level\"][ind]=3\n        \n#Using Label Encoder on the Categorical Columns\n#Gender\nle=LabelEncoder()\nle.fit(Sp[\"gender\"])\nSp[\"gender\"]=le.transform(Sp[\"gender\"])\n\n#Race\nle.fit(Sp[\"race\/ethnicity\"])\nSp[\"race\/ethnicity\"]=le.transform(Sp[\"race\/ethnicity\"])\n\n#Parents level of edu\nle.fit(Sp[\"parental level of education\"])\nSp[\"parental level of education\"]=le.transform(Sp[\"parental level of education\"])\n\n#Lunch\nle.fit(Sp[\"lunch\"])\nSp[\"lunch\"]=le.transform(Sp[\"lunch\"])\n\n#Test prep\nle.fit(Sp[\"test preparation course\"])\nSp[\"test preparation course\"]=le.transform(Sp[\"test preparation course\"])\n\n#Print Sp with the label Encoder values now converted \n#print(Sp)\n\n#Plot\nplt.scatter(Sp[\"TotalScore\"],Sp[\"Level\"],c=Sp[\"Level\"])\nplt.show()\n\n#Implementing Random Forest Classifier on Sp\nX=Sp[[\"gender\",\"race\/ethnicity\",\"parental level of education\",\"lunch\",\"test preparation course\",\"math score\",\"reading score\",\"writing score\",\"TotalScore\"]]\ny=Sp[\"Level\"]\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=25)\nrandom_forest=RandomForestClassifier(n_estimators=200)\nrandom_forest.fit(X_train,y_train)\nY_pred=random_forest.predict(X_test)  ## predicting Level of students in test data\n\n#ACCESSING PERFORMANCE\n#Accuracy Score\naccscore=accuracy_score(y_test,Y_pred)\nprint(\"Accuracy score of RFC based on all predictors using LabelEncoder:\")\nprint(accscore)\n\n\n\n#Confusion matrix\nprint(\"Confusion matrix for RFC based on all predictors using LabelEncoder:\")\ncf=confusion_matrix(y_test,Y_pred)\nprint(cf)\n#Classification Report\nreport = classification_report(y_test, Y_pred)\nprint(\"Classification Report for RFC based on all predictors using LabelEncoder:\")\nprint(report)\nsns.heatmap(X.corr(), vmin=0, vmax=1)\nplt.title(\"HeatMap of feature correlation with Label Encoder\",fontsize=15)\nplt.show()\n\n##Feature selection using RFE and ExtratreesClassifier\n#RFE\n\nrfe=RFE(random_forest,1)\nfit=rfe.fit(X_train,y_train)\nprint(fit.n_features_)\nprint(fit.support_)\nprint(fit.ranking_)\nprint(\"\\n\\nFeature importance using RFE:\")\nfor col,rank in zip(X.columns,fit.ranking_):\n    print(col,rank)\n\n#Extra Trees Classifiers\nforest=ExtraTreesClassifier(n_estimators=200)\nforest.fit(X_train,y_train)\nprint(\"Feature importance using ExtraTreesClassifiers:\")\nfor col1,imp in zip(X.columns,forest.feature_importances_):\n    print(col1,imp)\n    \n\n#####Feature importance plotting\nfeatimp=pd.DataFrame()\nfeatimp[\"feat_name\"]=pd.Series(X.columns)\nfeatimp[\"feature_imp\"]=pd.Series(forest.feature_importances_)\nfeatimp.sort_values(by=['feature_imp'], inplace=True)\n\n\n#Plot Cumulative feat imp:\n\nfeatimp[\"Impcumsum\"]=featimp[\"feature_imp\"].cumsum()\nprint(featimp)\n\nfig,ax=plt.subplots()\nfig.set_size_inches(9,7, forward=True)\nax.plot(featimp[\"feat_name\"],featimp[\"Impcumsum\"])\nax.set_title(\" Cumulative Importances for the features after Label Encoding\",fontsize=12)\nax.set_xticklabels(featimp[\"feat_name\"],rotation=90)\nplt.grid()\nplt.hlines(y = 0.90, xmin=0, xmax=len(featimp[\"feat_name\"])-1, color = 'r', linestyles = 'dashed')\nplt.savefig(\"featimp-labelencoder.png\")\nfig.tight_layout(pad=5.0)\nplt.show()\n\n#Conducting RFE on Selected Features\n#Without Total Score\nX=Sp[[\"math score\",\"reading score\",\"writing score\",\"gender\",\"race\/ethnicity\",\"parental level of education\",\"lunch\",\"test preparation course\"]]\n\ny=Sp[\"Level\"]\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=25)\nrandom_forest=RandomForestClassifier(n_estimators=200)\nrandom_forest.fit(X_train,y_train)\nY_pred=random_forest.predict(X_test)  ## predicting Level of students in test data\n\n#ACCESSING PERFORMANCE\n#Accuracy Score\naccscore=accuracy_score(y_test,Y_pred)\nprint(\"\\n\\nAccuracy score of RFC without Total Score as predictor,using LabelEncoder:\")\nprint(accscore)\n\n#Confusion matrix\nprint(\"\\nConfusion matrix for RFC without Total Score as predictor,using LabelEncoder:\")\ncf=confusion_matrix(y_test,Y_pred)\nprint(cf)\n#Classification Report\nreport = classification_report(y_test, Y_pred)\nprint(\"\\nClassification Report for RFC without Total Score as predictor,using LabelEncoder:\")\nprint(report)\n\n#Conducting RFE on Selected Features\n#Without Total Score, Math score,Writing Score and Reading score\nX=Sp[[\"gender\",\"race\/ethnicity\",\"parental level of education\",\"lunch\",\"test preparation course\"]]\n\ny=Sp[\"Level\"]\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=25)\nrandom_forest=RandomForestClassifier(n_estimators=200)\nrandom_forest.fit(X_train,y_train)\nY_pred=random_forest.predict(X_test)  ## predicting Level of students in test data\n\n#ACCESSING PERFORMANCE\n#Accuracy Score\naccscore=accuracy_score(y_test,Y_pred)\nprint(\"\\n\\nAccuracy score of RFC without Total Score,Math Score,Writing Score and Reading Score as predictors,using LabelEncoder:\")\nprint(accscore)\n\n#Confusion matrix\nprint(\"Confusion matrix for RFC without Total Score,Math Score,Writing Score and Reading Score as predictors,using LabelEncoder:\")\ncf=confusion_matrix(y_test,Y_pred)\nprint(cf)\n#Classification Report\nreport = classification_report(y_test, Y_pred)\nprint(\"Classification Report for RFC without Total Score,Math Score,Writing score and Reading Score as predictors,using LabelEncoder:\")\nprint(report)\n\n#PLOT without scores\ndef grosscol(col):\n    if col<=175:\n        return(\"indianred\")\n    elif (col>175 and col<=205):\n        return(\"tomato\")\n    elif (col>205 and col<=233):\n        return (\"red\")\n    else:\n        return(\"gold\")\n\n\nfig,ax=plt.subplots()\n\nfor ind,lev in zip(X_test.index,y_test):\n    ax.scatter(ind,lev,color=grosscol(lev))\nax.scatter(X_test.index,Y_pred,c=Y_pred,s=Y_pred*2)\n\nplt.show()","426db881":"\nimport pandas as pd\nfrom sklearn.preprocessing import LabelBinarizer\nSp=pd.read_csv(\"..\/input\/StudentsPerformance.csv\")\n#Computing the Total score as a summation of the Math, Reading and Writing scores\nSp[\"TotalScore\"]=SpO[[\"math score\",\"reading score\",\"writing score\"]].sum(axis=1)\nSp\n\n#print(Sp)\n\n#Setting the Level of Student criteria \nSp[\"Level\"]=np.NaN\n\nfor ind,TS in zip(Sp.index,Sp[\"TotalScore\"]):\n    if (TS<=175.000000):\n        Sp[\"Level\"][ind]=0\n        \n    elif (TS>175.000000 and TS<=205.000000):\n        Sp[\"Level\"][ind]=1\n        \n    elif (TS>205.000000 and TS<=233.000000):\n        Sp[\"Level\"][ind]=2\n    else:\n        Sp[\"Level\"][ind]=3\n        \n        \n#Gender\na=pd.get_dummies(Sp['gender'], prefix='Gender')\nSp1 = pd.concat([Sp, a], axis=1).drop([\"gender\"], axis=1)\n\n#Race\na=pd.get_dummies(Sp1['race\/ethnicity'], prefix='Race')\nSp1 = pd.concat([Sp1, a], axis=1).drop([\"race\/ethnicity\"], axis=1)\n\n#Parental level of Education\nb=pd.get_dummies(Sp1['parental level of education'], prefix='PEd')\nSp1 = pd.concat([Sp1, b], axis=1).drop([\"parental level of education\"], axis=1)\n\n#Lunch\nc=pd.get_dummies(Sp1['lunch'], prefix='lun')\nSp1 = pd.concat([Sp1, c], axis=1).drop([\"lunch\"], axis=1)\n\n#Test Prep\n\nd=pd.get_dummies(Sp1['test preparation course'], prefix='TP')\nSp1 = pd.concat([Sp1, d], axis=1).drop([\"test preparation course\"], axis=1)\n\n\n\n#Implementing Random Forest Classifier on Sp1 [get_dummies]\nX=Sp1[[\"math score\", \"reading score\", \"writing score\", \"TotalScore\", \"Level\",\n       \"Gender_female\", \"Gender_male\", \"Race_group A\", \"Race_group B\",\n       \"Race_group C\", \"Race_group D\", \"Race_group E\",\n       \"PEd_associate's degree\", \"PEd_bachelor's degree\", \"PEd_high school\",\n       \"PEd_master's degree\", \"PEd_some college\", \"PEd_some high school\",\n       \"lun_free\/reduced\", \"lun_standard\", \"TP_completed\", \"TP_none\"]]\ny=Sp1[\"Level\"]\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=25)\nrandom_forest=RandomForestClassifier(n_estimators=200)\nrandom_forest.fit(X_train,y_train)\nY_pred=random_forest.predict(X_test)  ## predicting Level of students in test data\n\n#ACCESSING PERFORMANCE\n#Accuracy Score\naccscore=accuracy_score(y_test,Y_pred)\nprint(\"Accuracy score of RFC based on all predictors using get_dummies:\")\nprint(accscore)\n\n#Confusion matrix\nprint(\"Confusion matrix for RFC based on all predictors using get_dummies:\")\ncf=confusion_matrix(y_test,Y_pred)\nprint(cf)\n#Classification Report\nreport = classification_report(y_test, Y_pred)\nprint(\"Classification Report for RFC based on all predictors using get_dummies:\")\nprint(report)\nsns.heatmap(X.corr(), vmin=0, vmax=1)\nplt.title(\"HeatMap of feature correlation with get_dummies\",fontsize=15)\nplt.show()\n\n\n\n#####Feature importance plotting\nfeatimp=pd.DataFrame()\nfeatimp[\"feat_name\"]=pd.Series(X.columns)\nfeatimp[\"feature_imp\"]=pd.Series(forest.feature_importances_)\nfeatimp.sort_values(by=['feature_imp'], inplace=True)\n\n\n#Plot Cumulative feat imp:\n\nfeatimp[\"Impcumsum\"]=featimp[\"feature_imp\"].cumsum()\nprint(featimp)\n\nfig,ax=plt.subplots()\nfig.tight_layout(pad=4.0)\nfig.set_size_inches(10,8, forward=True)\nax.plot(featimp[\"feat_name\"],featimp[\"Impcumsum\"])\nax.set_title(\"Cumulative Importances for the features with get_dummies\",fontsize=12)\nax.set_xticklabels(featimp[\"feat_name\"],rotation=90)\nplt.grid()\nplt.hlines(y = 0.90, xmin=0, xmax=len(featimp[\"feat_name\"])-1, color = 'r', linestyles = 'dashed')\nplt.savefig(\"featimp-dummies.png\")\nplt.show()\n\n##Feature selection using RFE and ExtratreesClassifier\n#RFE\n\nrfe=RFE(random_forest,1)\nfit=rfe.fit(X_train,y_train)\nprint(fit.n_features_)\nprint(fit.support_)\nprint(fit.ranking_)\nprint(\"\\n\\nFeature importance using RFE for get_dummies table:\")\nfor col,rank in zip(X.columns,fit.ranking_):\n    print(col,rank)\n\n#Extra Trees Classifiers\nforest=ExtraTreesClassifier(n_estimators=200)\nforest.fit(X_train,y_train)\nprint(\"Feature importance using ExtraTreesClassifiers for get_dummies table:\")\nfor col1,imp in zip(X.columns,forest.feature_importances_):\n    print(col1,imp)\n\n#Conducting RFE on Selected Features\n#Without Total Score\nX=Sp1[[\"math score\", \"reading score\", \"writing score\", \"Level\",\"Gender_female\", \"Gender_male\", \"Race_group A\", \"Race_group B\",\"Race_group C\", \"Race_group D\", \"Race_group E\",\"PEd_associate's degree\", \"PEd_bachelor's degree\", \"PEd_high school\",\"PEd_master's degree\", \"PEd_some college\", \"PEd_some high school\",\"lun_free\/reduced\", \"lun_standard\", \"TP_completed\", \"TP_none\"]]\ny=Sp1[\"Level\"]\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=25)\nrandom_forest=RandomForestClassifier(n_estimators=200)\nrandom_forest.fit(X_train,y_train)\nY_pred=random_forest.predict(X_test)  ## predicting Level of students in test data\n\n#ACCESSING PERFORMANCE\n#Accuracy Score\naccscore=accuracy_score(y_test,Y_pred)\nprint(\"Accuracy score of RFC without Total Score predictor using get_dummies:\")\nprint(accscore)\n\n#Confusion matrix\nprint(\"Confusion matrix for RFC without Total Score predictor using get_dummies:\")\ncf=confusion_matrix(y_test,Y_pred)\nprint(cf)\n#Classification Report\nreport = classification_report(y_test, Y_pred)\nprint(\"Classification Report for RFC without Total Score predictor using get_dummies:\")\nprint(report)\nsns.heatmap(X.corr(), vmin=0, vmax=1)\nplt.show()\n\n\n#Conducting RFE on Selected Features\n#Without Total Score,Math Score,Writing Score and Reading Score as predictors\nX=Sp1[[\"Level\",\"Gender_female\", \"Gender_male\", \"Race_group A\", \"Race_group B\",\"Race_group C\", \"Race_group D\", \"Race_group E\",\"PEd_associate's degree\", \"PEd_bachelor's degree\", \"PEd_high school\",\"PEd_master's degree\", \"PEd_some college\", \"PEd_some high school\",\"lun_free\/reduced\", \"lun_standard\", \"TP_completed\", \"TP_none\"]]\n\ny=Sp1[\"Level\"]\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=25)\nrandom_forest=RandomForestClassifier(n_estimators=200)\nrandom_forest.fit(X_train,y_train)\nY_pred=random_forest.predict(X_test)  ## predicting Level of students in test data\n\n#ACCESSING PERFORMANCE\n#Accuracy Score\naccscore=accuracy_score(y_test,Y_pred)\nprint(\"Accuracy score of RFC without Total Score,Math Score,Writing Score and Reading Score as predictors using get_dummies:\")\nprint(accscore)\n\n#Confusion matrix\nprint(\"Confusion matrix for RFC without Total Score,Math Score,Writing Score and Reading Score as predictors using get_dummies:\")\ncf=confusion_matrix(y_test,Y_pred)\nprint(cf)\n#Classification Report\nreport = classification_report(y_test, Y_pred)\nprint(\"Classification Report for RFC without Total Score,Math Score,Writing Score and Reading Score as predictors using get_dummies:\")\nprint(report)\nsns.heatmap(X.corr(), vmin=0, vmax=1)\nplt.show()\n\n\n\n#Plot\ndef grosscol(col):\n    if col<=175:\n        return(\"indianred\")\n    elif (col>175 and col<=205):\n        return(\"tomato\")\n    elif (col>205 and col<=233):\n        return (\"red\")\n    else:\n        return(\"gold\")\nfor ind,lev in zip(X_test.index,y_test):\n    plt.scatter(ind,lev,color=grosscol(lev))\nplt.scatter(X_test.index,Y_pred,c=Y_pred)\nplt.show()","d6eda1b9":"## RF Classifier using get_dummies","b11e204d":"# Overview:\nSimple processing a dataset and fitting it to Regression or Classification models, some of whose columns are \u2018categorical\u2019 i.e. textual throws an error. This led me to investigate how to work with categorical data.\n\nOne of the most obvious solutions to tackling categorical data is to convert them into numerical representations. Say, a column representing \u2019Colours\u2019 with values like Red, Blue, Green and Black could instead be represented by the number 1,2,3,4, each number representing a colour.\nWhen you convert all your categorical columns into numeric data, the code snippet runs with your chosen algorithm.\n\nHowever, some may not realise that the predicted output may have been tweaked as a result of the newly converted numerical data columns.  For example, if your Column consists of Overall Ranks[Overall Ranks:\u2019first\u2019,\u2019second\u2019,\u2019third\u2019], \u2019first\u2019 to 1, \u2018second\u2019 to 2 and \u2018third\u2019 to 3 would be fine since the Ranks in fact do have an order amongst themselves. For columns like Colours, there seems to be no particular order and therefore it makes more sense to interpret them differently.\nThis process of assigning a series of numbers to categorical values is called Integer Coding, performed using Label Encoder in Python. The process, in hindsight, assigns an order 0<1<2<3.\n\nThe get_dummies function, which comes in the pandas package is used to convert categorical variables to dummy variables. A new column is formed for each unique value. The Column Colours would be split into 4 columns: Red, Blue, Green and Black. A row value with colour Red will be represented as 1,0,0,0; a row value with colour Green will be represented as 0,0,1,0, and so on. \n\nI fit my model to two versions of the dataset: Integer Coded using Label Encoder, and binary coded using get_dummies from pandas.\n\n## Take-Aways:\n1.On plotting the cumulative features importances with each kind of encoding, and I noticed that it is very much possible even though the overall influence of a Category may be low, specific values\/subtypes within that category could still be more influential.\n\n2.Use Integer Encoding when your feature is ordinal, and Binary Encoding when it is not ordinal.\n","4f4ab211":"## RFClassifier using Label Encoder","dcf5bcb1":"# Comparison between LabelEncoder and get_dummies"}}