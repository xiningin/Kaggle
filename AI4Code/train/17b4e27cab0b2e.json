{"cell_type":{"04402827":"code","a9c86a61":"code","b4d914b4":"code","77e24a8c":"code","6026f997":"code","677e2791":"code","6f8d2a36":"code","759e882d":"code","90c469e2":"code","0ffa2c0a":"code","84df216a":"code","c6013299":"code","5074d08e":"code","58dd7371":"code","c2402013":"code","3f98b9b1":"code","9da656e7":"code","8ba3690c":"code","2c5552e4":"code","ac2552d1":"code","cbb96103":"code","401fe87d":"code","950be0a1":"code","66318886":"code","5112b5fd":"code","2b5ea022":"code","a1093933":"code","28915518":"code","db0e57f2":"code","bd0607ca":"code","cb53d398":"code","1ff5a822":"code","06587a16":"code","a71b289e":"code","baa82da6":"code","4b5b4d12":"code","2daeb79d":"code","eba8d88f":"code","b68c30b3":"code","ed17ccbe":"code","398e718f":"code","8803b170":"code","49ebf203":"code","9779ba8c":"code","51fe452b":"code","09aa50a1":"code","03096129":"code","3184c051":"code","6e73d1e7":"code","4166674c":"code","e675a215":"code","6a5d643e":"code","91d84f30":"code","105d05ea":"code","7c7c6acf":"markdown","a834173a":"markdown","42947022":"markdown","164639cb":"markdown","3661a21c":"markdown","d5cd326e":"markdown","f1669109":"markdown","be88c3df":"markdown"},"source":{"04402827":"# Python 3 environment with analytics libraries installed\n# as defined by the kaggle\/python Docker \n\nimport numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a9c86a61":"#assigning CSV files as pandas dataframes\nsample = pd.read_csv('..\/input\/trell-social-media-usage-data\/sample_submission.csv')\ntestDF = pd.read_csv('..\/input\/trell-social-media-usage-data\/test_age_dataset.csv')\ntrain = pd.read_csv('..\/input\/trell-social-media-usage-data\/train_age_dataset.csv')","b4d914b4":"#The dataframe with the bulk of the data\n##Use this to train a prediction model\ntrain","77e24a8c":"#use this dataset to test model, make predictions, \n#and compare to the train dataset prediction\ntestDF","6026f997":"#making sure the data contains no NaNs\ntrain.isnull().values.any()","677e2791":"testDF.isnull().values.any()","6f8d2a36":"#Explore the data\ntrain.describe()","759e882d":"testDF.describe()","90c469e2":"#get to know age_group\ntrain['age_group'].describe()","0ffa2c0a":"train['age_group'].value_counts()","84df216a":"#visualize age_group column\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntrain.age_group.value_counts().plot(kind='bar', color='dodgerblue')","c6013299":"correlation=train.corr()\ncorrelation","5074d08e":"import seaborn as sns\nsns.heatmap(correlation, cmap=\"Reds\")","58dd7371":"# logistic regression for multi-class classification using built-in one-vs-rest\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# define dataset\nX, y = make_classification(n_samples=2000, n_features=26, n_informative=13, n_redundant=13, n_classes=4, random_state=1)\n# define model\nmodel = LogisticRegression(multi_class='ovr')\n# fit model\nmodel.fit(X, y)\n# make predictions\nyhat = model.predict(X)","c2402013":"#accuracy from training dataset\nmodel.score(X, y)","3f98b9b1":"#accuracy from test dataset\nmodel.score(X, yhat)","9da656e7":"from sklearn.model_selection import train_test_split as split\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score","8ba3690c":"# evaluate the model by splitting the data-set into train and test sets\nX_train, X_test, y_train, y_test = split(X, y, test_size=0.3)\n\nmodel2 = LogisticRegression()\nmodel2.fit(X_train, y_train)","2c5552e4":"predicted = model2.predict(X_test)\nprint(y_test)\npredicted","ac2552d1":"prediction1= pd.DataFrame(predicted)\nprediction1.rename(columns = {0:'prediction'})","cbb96103":"#a histogram of the residuals from the model\nplt.hist(y_test - predicted, color='salmon')","401fe87d":"# generate class probabilities\nprobs = model2.predict_proba(X_test)\nprobs","950be0a1":"# generate evaluation metrics\nprint(metrics.accuracy_score(y_test, predicted))","66318886":"conf_matrix = metrics.confusion_matrix(y_test, predicted)\nsns.heatmap(conf_matrix, annot=True,cmap='Greens')","5112b5fd":"print(metrics.classification_report(y_test, predicted))","2b5ea022":"# evaluate the model using 10-fold cross-validation\nscores = cross_val_score(LogisticRegression(), X, y, scoring='accuracy', cv=10)\nscores, scores.mean()","a1093933":"##adjusted model\n#lowered the n_samples and random_state to 0\n\nX, y = make_classification(n_samples=1000, n_features=26, n_informative=13, n_redundant=13, n_classes=4, random_state=0)\n# define model\nmodel = LogisticRegression(multi_class='ovr')\n# fit model\nmodel.fit(X, y)\n# make predictions\nyhat = model.predict(X)","28915518":"model.score(X, y)","db0e57f2":"##the one above fits less\n#redo the adjustments\n#same n_samples and random_state to 1\n\nX, y = make_classification(n_samples=1000, n_features=26, n_informative=13, n_redundant=13, n_classes=4, random_state=1)\n# define model\nmodel = LogisticRegression(multi_class='ovr')\n# fit model\nmodel.fit(X, y)\n# make predictions\nyhat = model.predict(X)","bd0607ca":"model.score(X, y)","cb53d398":"##the one above fits better but not better than the 1st model\n#redo the adjustments\n\n#larger n_samples and random_state to 1\n\nX, y = make_classification(n_samples=3000, n_features=26, n_informative=13, n_redundant=13, n_classes=4, random_state=1)\n# define model\nmodel = LogisticRegression(multi_class='ovr')\n# fit model\nmodel.fit(X, y)\n# make predictions\nyhat = model.predict(X)","1ff5a822":"model.score(X, y)","06587a16":"#redo the adjustments\n\n#same n_samples and random_state to 2\n\nX, y = make_classification(n_samples=3000, n_features=26, n_informative=13, n_redundant=13, n_classes=4, random_state=2)\n# define model\nmodel = LogisticRegression(multi_class='ovr')\n# fit model\nmodel.fit(X, y)\n# make predictions\nyhat = model.predict(X)","a71b289e":"model.score(X, y)","baa82da6":"#the model above is better than the previous ones\n#still adjust to seek better scores\n\n#same n_samples and random_state to 3\n\nX, y = make_classification(n_samples=3000, n_features=26, n_informative=13, n_redundant=13, n_classes=4, random_state=3)\n# define model\nmodel = LogisticRegression(multi_class='ovr')\n# fit model\nmodel.fit(X, y)\n# make predictions\nyhat = model.predict(X)","4b5b4d12":"model.score(X, y)","2daeb79d":"#The model above did not fit better\n#retry with a larger sample but leave random_state at 2\nX, y = make_classification(n_samples=4000, n_features=26, n_informative=13, n_redundant=13, n_classes=4, random_state=2)\n# define model\nmodel = LogisticRegression(multi_class='ovr')\n# fit model\nmodel.fit(X, y)\n# make predictions\nyhat = model.predict(X)","eba8d88f":"model.score(X, y)","b68c30b3":"#the previous model specification did not improve the score\n#the model works better at 2000 sample size and random_state at 2\n#as below\n\nX, y = make_classification(n_samples=2000, n_features=26, n_informative=13, n_redundant=13, n_classes=4, random_state=2)\n# define model\nmodel = LogisticRegression(multi_class='ovr')\n# fit model\nmodel.fit(X, y)\n# make predictions\nyhat = model.predict(X)\nmodel.score(X, y)","ed17ccbe":"# evaluate the model by splitting the data-set into train and test sets\n#increased the test_size\nX_train, X_test, y_train, y_test = split(X, y, test_size=0.4)\n\nmodel3 = LogisticRegression()\nmodel3.fit(X_train, y_train)","398e718f":"predicted = model3.predict(X_test)\nprint(y_test)\npredicted","8803b170":"#a histogram of the residuals from the model\nplt.hist(y_test - predicted, color='purple')","49ebf203":"# generate evaluation metrics\nprint(metrics.accuracy_score(y_test, predicted))","9779ba8c":"print(metrics.classification_report(y_test, predicted))","51fe452b":"conf_matrix = metrics.confusion_matrix(y_test, predicted)\nsns.heatmap(conf_matrix, annot=True,cmap='magma')","09aa50a1":"#increased the test_size one more time\n#for the model that fit best of the above tries\nX_train, X_test, y_train, y_test = split(X, y, test_size=0.5)\n\nmodel4 = LogisticRegression()\nmodel4.fit(X_train, y_train)","03096129":"predicted = model4.predict(X_test)\nprint(y_test)\npredicted","3184c051":"#a histogram of the residuals from the model\nplt.hist(y_test - predicted, color='gray')","6e73d1e7":"# generate class probabilities\nprobs = model4.predict_proba(X_test)\nprobs","4166674c":"# generate evaluation metrics\nprint(metrics.accuracy_score(y_test, predicted))","e675a215":"print(metrics.classification_report(y_test, predicted))","6a5d643e":"conf_matrix = metrics.confusion_matrix(y_test, predicted)\nsns.heatmap(conf_matrix, annot=True,cmap='mako')","91d84f30":"# evaluate the model using 10-fold cross-validation\nscores = cross_val_score(LogisticRegression(), X, y, scoring='accuracy', cv=10)\nscores, scores.mean()","105d05ea":"prediction2= pd.DataFrame(predicted)\nprediction2.rename(columns = {0:'prediction'})\nprediction2.to_csv('predictionDF.csv',index=False)","7c7c6acf":"The **'age_group' column** in the train dataset is the classifications stardard for the prediction model. The column data is a float datatype, with 4 possible classes: 1, 2, 3, 4. The dataset does not specify the definition of each group.   \n\nThe bulk of Trell users fall under age_group 1.","a834173a":"Neither dataset had NaNs.       \nClassification data is in numeric value.   \nThe train dataset includes the age group classification. The test does not. This means that the datasets are ready for use.","42947022":"# generate class probabilities\nprobs = model3.predict_proba(X_test)\nprobs","164639cb":"Correlation among columns:","3661a21c":"First: Explore and evaluate the datasets by cheking for NaNs, get a descrpition of the data, explore the age_group column in the train dataset.   ","d5cd326e":"After several adjustments to the first model ran, prediction2 results fit best at 71% weighted f1 score.      \nOther coders could persue other approaches than logistic regression or classification approach.","f1669109":"A logistic regression multiple classification model using the **One-Vs-Rest** heuristic method to split the multiple classes into multiple binary classification datasets and train a binary classification each one.\n\n","be88c3df":"**Task**:   \nTo help Trell predict the age group of users based on their activity on social media by creating a classification prediction algorithm and conducting a performance evaluation metric of the model.    \n\nI use a **logistic regression** using **one-vs-rest multiple-class** classification algorithm and **weighted F1 score** to evaluate the model."}}