{"cell_type":{"3a8c33e3":"code","3468813c":"code","f5f7169e":"code","0a350dd2":"code","fb7ea9ac":"code","5f4fa79b":"code","92df4a26":"code","accb61b4":"code","585357ad":"code","9cb79967":"code","8e829956":"code","30d56588":"code","472ffc08":"code","eea89486":"code","f395f8ea":"code","074b071e":"code","86e15857":"code","776b5a34":"code","63ade55f":"code","904f0214":"code","58085901":"code","27ca7890":"code","8a2b9663":"code","03692c45":"code","3b738bad":"code","0a593b8c":"code","0d0b8680":"code","016f91d0":"code","1d6725ad":"code","e2c54f36":"code","e6d83682":"code","c80e5725":"code","a329c594":"code","cd4d3c26":"code","a554c013":"code","751080d8":"code","f7022efe":"code","df14329a":"code","a72e57e3":"code","e4921b59":"code","18db9a5e":"code","8b355daa":"code","74d49f77":"code","cf219b0f":"code","497031db":"code","4a14245c":"code","41bc7b7b":"code","46bc70c9":"code","2d6fbbf2":"code","91ad2953":"code","ec77b467":"code","4d9a6d82":"code","900484a7":"code","71d970b8":"code","16a0045b":"code","77587504":"code","e7e45e02":"code","94ff480a":"code","74135980":"code","622525e2":"code","2b1d4d6f":"code","d22e7376":"code","9ab6a455":"code","94cc87ea":"code","99fcb7e2":"code","7f5cb174":"code","95e12a5c":"code","ac361106":"code","2865b12f":"code","9d735c8a":"code","e357d253":"code","2d8418eb":"code","272f3ec9":"code","0a4a9954":"code","e364959b":"code","34177301":"code","ce0f297c":"code","c3f946a5":"code","f28c589d":"code","03e67568":"code","3169b8d6":"code","59069b2b":"code","084dc23d":"code","50b90f47":"code","579404f9":"code","a5674ebe":"markdown","aeb7402a":"markdown","32d26c52":"markdown","f4ccd2aa":"markdown","dee1f5f4":"markdown","d2c5bb32":"markdown","f83d15b4":"markdown","5f14d0f0":"markdown","d8f054e7":"markdown","7f1df7f7":"markdown","f3baf746":"markdown","b65901fb":"markdown","62745bbe":"markdown","453c0110":"markdown","0afdd31e":"markdown","646c2f33":"markdown","de1f9ae9":"markdown","2ce7d51d":"markdown","8c5b2a86":"markdown","e2061c09":"markdown","e5811ca9":"markdown","6b0a1f7a":"markdown","3416663b":"markdown","cdcff940":"markdown","1a49acc9":"markdown","6905216f":"markdown","c7d54dda":"markdown","93c521dc":"markdown","9cf19129":"markdown","20acbf8b":"markdown","e566ee4c":"markdown","3b6e00c8":"markdown","3df9f042":"markdown","7c3b3d04":"markdown","18225744":"markdown","88919fe5":"markdown","8e7eb918":"markdown","2a3f3afc":"markdown","6d23ad36":"markdown","9376d650":"markdown","3aa09a8a":"markdown","7636b4a1":"markdown","62fcf8f7":"markdown","63aeb832":"markdown","0ec493c6":"markdown","e352e8b9":"markdown","ca72aec5":"markdown","183fa22d":"markdown","8909187f":"markdown","50e4f138":"markdown","cc020e99":"markdown","ca9a2378":"markdown","45c51df5":"markdown","2f019670":"markdown","a76be356":"markdown","e3842e4e":"markdown","5786ed84":"markdown","fc472db5":"markdown","df9b5a67":"markdown","2ee90c94":"markdown","74595989":"markdown","4d6c5205":"markdown","49338b24":"markdown","d3f17f62":"markdown","84ac9924":"markdown","44d409db":"markdown","5e628ccb":"markdown","84e14130":"markdown","a1917ee0":"markdown","0c7f338f":"markdown","9c795823":"markdown","53f0b5e2":"markdown","72a6defa":"markdown","611a2e02":"markdown","cfdb91ce":"markdown","fa40d13a":"markdown","a2b6233b":"markdown","a450c69d":"markdown","10d9db7f":"markdown","06603271":"markdown","54f78a91":"markdown","1c8dbb17":"markdown","183fbf4a":"markdown"},"source":{"3a8c33e3":"# fix for autocomplete intellisense\n#%config Completer.use_jedi = False\n\n#for printing all the outputs of a cell in the same output window\nfrom IPython.core.interactiveshell import InteractiveShell  \nInteractiveShell.ast_node_interactivity = \"all\"\n#InteractiveShell.ast_node_interactivity = \"last_expr\"\n\n\n# Basic Libraries\n\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling\nimport re\nimport string\nimport random\nimport math\nimport time\nimport json\nimport os\nfrom os import listdir\nimport itertools\nimport collections\nfrom collections import Counter, defaultdict\nfrom tqdm import tqdm\nfrom sklearn import utils\n\n\n# Visualization\n\n# matplotlib\n%matplotlib inline\n#%matplotlib notebook\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\n# seaborn\nimport seaborn as sns\nsns.set_style(style='whitegrid')\n\n# plotly\n!pip3 install chart_studio\nimport chart_studio.plotly as py\n\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly import tools\n\n# cufflinks\n!pip3 install cufflinks\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\n\n# nltk wordcloud\nfrom wordcloud import WordCloud,STOPWORDS\n\n# dimensionality reduction\nfrom sklearn.decomposition import PCA, TruncatedSVD, SparsePCA\nfrom sklearn.manifold import TSNE\n\n\n\n# Preprocessing\n!pip3 install beautifulsoup4\nfrom bs4 import BeautifulSoup\n\nimport nltk\nimport spacy\n\nfrom nltk import word_tokenize\nfrom nltk.probability import FreqDist\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nfrom nltk.stem import WordNetLemmatizer\n\nnlp_spcy = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\",\"ner\"])\nfrom spacy.lang.en.stop_words import STOP_WORDS\nSTOP_WORDS = list(set(STOP_WORDS))\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.model_selection import train_test_split, cross_val_score,  cross_val_predict\nfrom sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV\n\n# from imblearn.over_sampling import ADASYN, SMOTE\n# from imblearn.under_sampling import NearMiss\n\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier as xgb\nfrom lightgbm import LGBMClassifier as lgbm\n\n\n# Gensim models\nfrom smart_open import open\nfrom gensim.models import Word2Vec, FastText, KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\n#from gensim.models.fasttext import load_facebook_vectors\n#from gensim.models.fasttext import load_facebook_model\n\n\n# Evaluation\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.metrics import  accuracy_score, log_loss, classification_report\nfrom sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve\nfrom sklearn.pipeline import make_pipeline\nfrom lime import lime_text\nfrom lime.lime_text import LimeTextExplainer\n\n\n# model serialization\nimport pickle\n\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings(action = \"ignore\")\n\n\n# Settings for pretty nice plots\nplt.style.use('fivethirtyeight')\nplt.show()\n\n\n## for deep learning\n\n##% tensorflow_version 2.x\nimport tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow import keras\n\nfrom keras.models import Sequential\nfrom keras import models, layers, preprocessing as KP\nfrom keras.layers.embeddings import Embedding\nfrom keras import backend as KB\n\n## for bert language model\n# import transformers","3468813c":"train_df = pd.read_json('..\/input\/github-bugs-prediction\/embold_train.json')\nprint(\"Train Dataframe:\")\ntrain_df.head(3)\nprint(f'Train dataframe contains {train_df.shape[0]} samples.')\nprint('Number of features in train data : ', train_df.shape[1])\nprint('Train Features : ', train_df.columns.values)","f5f7169e":"train_extra_df = pd.read_json('..\/input\/github-bugs-prediction\/embold_train_extra.json')\ntrain_extra_df.head()\nprint('')\nprint(f'Extra train dataset contains {train_extra_df.shape[0]} samples.')\nprint('Number of features in the extra train dataset : ', train_extra_df.shape[1])\nprint('Features in the extra train dataset : ', train_extra_df.columns.values)","0a350dd2":"train_df.profile_report()","fb7ea9ac":"# Null values and Data types\nprint('Train Set:\\n')\nprint(train_df.info())\nprint('')","5f4fa79b":"# check the data for null values\nprint('Train data Null values :')\ntrain_df.isnull().sum()","92df4a26":"# check the basic stats\nprint('Train set basic stats:')\ntrain_df.describe(include='all')","accb61b4":"train_df['text'] = train_df.title + ' ' + train_df.body\nprint(\"Train Dataframe with combined title and body text:\")\ntrain_df.head(3)","585357ad":"#Check the percetage of data points in each category\n\n(train_df.label.value_counts(normalize=True).sort_index())*100","9cb79967":"label_counts = train_df.label.value_counts(normalize=True).sort_index()\nplt.figure(figsize=(7,6))\nlabel_counts.plot(kind='bar', color=['r','g','b'])\n\nB = mpatches.Patch(color='r', label='Bug')\nF = mpatches.Patch(color='g', label='Feature')\nQ = mpatches.Patch(color='b', label='Question')\n\nplt.legend(handles=[B,F,Q], loc='best')\n\nplt.xlabel('Type of Labels')\nplt.ylabel('Count of Data per Label Category')\nplt.title('Distribution of labels')\nplt.show()","8e829956":"pd.DataFrame(train_df.text.value_counts())","30d56588":"print(\"Total count of standard stop words list from SpaCy :\",len(STOP_WORDS))\nprint(\"\\nStandard stop words list from SpaCy :\\n\", STOP_WORDS)","472ffc08":"#add some redundant words like 'elif' in the stop words list\nSTOP_WORDS = STOP_WORDS + ['elif']\nprint('elif' in STOP_WORDS)\n\n#Discard negative words like 'not'and 'no' from this list if required\n\n#STOP_WORDS.remove('not')\n#STOP_WORDS.discard('no')\n#print(len(STOP_WORDS))","eea89486":"# Creating a sigle Generic Class for text cleaning.\n\nclass PreprocessText:\n    \n    #cleaning abbreviated words\n    @staticmethod\n    def remove_contractions(data):\n        data = re.sub(r\"he's\", \"he is\", data)\n        data = re.sub(r\"there's\", \"there is\", data)\n        data = re.sub(r\"We're\", \"We are\", data)\n        data = re.sub(r\"That's\", \"That is\", data)\n        data = re.sub(r\"won't\", \"will not\", data)\n        data = re.sub(r\"they're\", \"they are\", data)\n        data = re.sub(r\"Can't\", \"Cannot\", data)\n        data = re.sub(r\"wasn't\", \"was not\", data)\n        data = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", data)\n        data= re.sub(r\"aren't\", \"are not\", data)\n        data = re.sub(r\"isn't\", \"is not\", data)\n        data = re.sub(r\"What's\", \"What is\", data)\n        data = re.sub(r\"haven't\", \"have not\", data)\n        data = re.sub(r\"hasn't\", \"has not\", data)\n        data = re.sub(r\"There's\", \"There is\", data)\n        data = re.sub(r\"He's\", \"He is\", data)\n        data = re.sub(r\"It's\", \"It is\", data)\n        data = re.sub(r\"You're\", \"You are\", data)\n        data = re.sub(r\"I'M\", \"I am\", data)\n        data = re.sub(r\"shouldn't\", \"should not\", data)\n        data = re.sub(r\"wouldn't\", \"would not\", data)\n        data = re.sub(r\"i'm\", \"I am\", data)\n        data = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", data)\n        data = re.sub(r\"I'm\", \"I am\", data)\n        data = re.sub(r\"Isn't\", \"is not\", data)\n        data = re.sub(r\"Here's\", \"Here is\", data)\n        data = re.sub(r\"you've\", \"you have\", data)\n        data = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", data)\n        data = re.sub(r\"we're\", \"we are\", data)\n        data = re.sub(r\"what's\", \"what is\", data)\n        data = re.sub(r\"couldn't\", \"could not\", data)\n        data = re.sub(r\"we've\", \"we have\", data)\n        data = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", data)\n        data = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", data)\n        data = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", data)\n        data = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", data)\n        data = re.sub(r\"who's\", \"who is\", data)\n        data = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", data)\n        data = re.sub(r\"y'all\", \"you all\", data)\n        data = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", data)\n        data = re.sub(r\"would've\", \"would have\", data)\n        data = re.sub(r\"it'll\", \"it will\", data)\n        data = re.sub(r\"we'll\", \"we will\", data)\n        data = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", data)\n        data = re.sub(r\"We've\", \"We have\", data)\n        data = re.sub(r\"he'll\", \"he will\", data)\n        data = re.sub(r\"Y'all\", \"You all\", data)\n        data = re.sub(r\"Weren't\", \"Were not\", data)\n        data = re.sub(r\"Didn't\", \"Did not\", data)\n        data = re.sub(r\"they'll\", \"they will\", data)\n        data = re.sub(r\"they'd\", \"they would\", data)\n        data = re.sub(r\"DON'T\", \"DO NOT\", data)\n        data = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", data)\n        data = re.sub(r\"they've\", \"they have\", data)\n        data = re.sub(r\"i'd\", \"I would\", data)\n        data = re.sub(r\"should've\", \"should have\", data)\n        data = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", data)\n        data = re.sub(r\"where's\", \"where is\", data)\n        data = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", data)\n        data = re.sub(r\"we'd\", \"we would\", data)\n        data = re.sub(r\"i'll\", \"I will\", data)\n        data = re.sub(r\"weren't\", \"were not\", data)\n        data = re.sub(r\"They're\", \"They are\", data)\n        data = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", data)\n        data = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", data)\n        data = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", data)\n        data = re.sub(r\"let's\", \"let us\", data)\n        data = re.sub(r\"it's\", \"it is\", data)\n        data = re.sub(r\"can't\", \"cannot\", data)\n        data = re.sub(r\"don't\", \"do not\", data)\n        data = re.sub(r\"you're\", \"you are\", data)\n        data = re.sub(r\"i've\", \"I have\", data)\n        data = re.sub(r\"that's\", \"that is\", data)\n        data = re.sub(r\"i'll\", \"I will\", data)\n        data = re.sub(r\"doesn't\", \"does not\",data)\n        data = re.sub(r\"i'd\", \"I would\", data)\n        data = re.sub(r\"didn't\", \"did not\", data)\n        data = re.sub(r\"ain't\", \"am not\", data)\n        data = re.sub(r\"you'll\", \"you will\", data)\n        data = re.sub(r\"I've\", \"I have\", data)\n        data = re.sub(r\"Don't\", \"do not\", data)\n        data = re.sub(r\"I'll\", \"I will\", data)\n        data = re.sub(r\"I'd\", \"I would\", data)\n        data = re.sub(r\"Let's\", \"Let us\", data)\n        data = re.sub(r\"you'd\", \"You would\", data)\n        data = re.sub(r\"It's\", \"It is\", data)\n        data = re.sub(r\"Ain't\", \"am not\", data)\n        data = re.sub(r\"Haven't\", \"Have not\", data)\n        data = re.sub(r\"Could've\", \"Could have\", data)\n        data = re.sub(r\"youve\", \"you have\", data)  \n        data = re.sub(r\"don\u00e5\u00abt\", \"do not\", data)\n        \n        return data\n    \n    \n    #cleaning Urls\n    @staticmethod\n    def remove_urls(data):\n        clean_url_regex = re.compile(r\"http\\S+|www\\.\\S+\")\n        data = clean_url_regex.sub(r\" \", data)\n        return data\n    \n    \n    #cleaning noisy data\n    @staticmethod\n    def remove_noisy_char(data):\n        data = data.replace(\"\\\\r\", \" \").strip()\n        data = data.replace(\"\\r\", \" \").strip()\n        return data\n    \n    \n    #cleaning HTML tags\n    @staticmethod\n    def remove_HTML_tags(data):\n        soup = BeautifulSoup(data, 'html.parser') \n        return soup.get_text()\n        \n        \n    #cleaning emojis\n    @staticmethod\n    def remove_emojis(data):\n        emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n        \n        data = emoji_clean.sub(r\" \",data)\n        return data\n    \n    \n    #cleaning unicode characters\n    \"\"\"\n    @staticmethod\n    def remove_unicode_chars(data):\n        data = (data.encode('ascii', 'ignore')).decode(\"utf-8\")\n        return data\n    \"\"\"\n    \n    #cleaning punctuations\n    @staticmethod\n    def remove_punctuations(data):\n        \n        #clean_punct_regex = re.compile(r\"[^\\w\\s\\d]+\")\n        clean_punct_regex = re.compile(r\"[^a-zA-Z0-9\\s]+\")\n        data = clean_punct_regex.sub(r\" \", data)\n                        \n        #credits - https:\/\/stackoverflow.com\/questions\/265960\/best-way-to-strip-punctuation-from-a-string\n        #data = data.translate(str.maketrans('', '', string.punctuation))\n                \n        return data\n    \n    \n    #cleaning numeric characters\n    @staticmethod\n    def remove_numerics(data):\n        #clean_alphanum_regex = re.compile(r\"\\S*\\d\\S*\")\n        #data = clean_alphanum_regex.sub(r\"\", data)\n        \n        clean_num_regex = re.compile(r\"\\b[0-9]+\\b\")\n        data = clean_num_regex.sub(r\"\", data)\n        return data\n    \n    \n    #cleaning single characters\n    @staticmethod\n    def remove_single_chars(data):\n        #credits - https:\/\/stackoverflow.com\/questions\/42066352\/python-regex-to-replace-all-single-word-characters-in-string\n        clean_single_len_regex = re.compile(r\"\\b[a-zA-Z]\\b\")\n        data = clean_single_len_regex.sub(r\"\", data)\n        return data\n    \n    \n    #cleaning unwanted whitespaces\n    @staticmethod\n    def remove_redundant_whiteSpaces(data):\n        clean_redundant_whitespaces_regex = re.compile(r\"\\s\\s+\") #check for more consecutive spaces\n        data = clean_redundant_whitespaces_regex.sub(r\" \", data) #replace with single space\n        return data\n    \n    \n    #cleaning stopwords\n    @staticmethod\n    def remove_stopwords(data):\n        data = ' '.join(word.lower() for word in data.split() if word.lower() not in STOP_WORDS)\n        data = data.strip()\n        return data\n    \n    #cleaning long length words greater than 25 chars\n    @staticmethod\n    def remove_long_length_tokens(data):\n        data = ' '.join(word.lower() for word in data.split() if len(word) <= 25)\n        data = data.strip()\n        return data\n    \n    #lemmatizing the text data\n    @staticmethod\n    def lemmatize_corpus(data, method = 'wordnet'):\n        if method == 'spacy':\n            out_data = \" \".join([token.lemma_ for token in nlp_spcy(data)])\n        else:\n            lemmatizer=WordNetLemmatizer()\n            out_data = ' '.join(lemmatizer.lemmatize(word) for word in data.split())\n        \n        return out_data\n    ","f395f8ea":"#apply text pre-processing\n\ndef text_cleaning(df,col,clean_col):\n    \n    df[clean_col]= df[col].apply(PreprocessText.remove_contractions)\n    df[clean_col]= df[clean_col].apply(PreprocessText.remove_urls)\n    df[clean_col]= df[clean_col].apply(PreprocessText.remove_noisy_char)\n    df[clean_col]= df[clean_col].apply(PreprocessText.remove_HTML_tags)\n    df[clean_col]= df[clean_col].apply(PreprocessText.remove_emojis)\n    #df[clean_col]= df[clean_col].apply(PreprocessText.remove_unicode_chars)\n    df[clean_col]= df[clean_col].apply(PreprocessText.remove_punctuations)\n    df[clean_col]= df[clean_col].apply(PreprocessText.remove_numerics)\n    df[clean_col]= df[clean_col].apply(PreprocessText.remove_single_chars)\n    df[clean_col]= df[clean_col].apply(PreprocessText.remove_redundant_whiteSpaces)\n    df[clean_col]= df[clean_col].apply(PreprocessText.remove_stopwords)\n    df[clean_col]= df[clean_col].apply(PreprocessText.remove_long_length_tokens)\n    \n    return df\n\nstart_time = time.clock()\ntrain_df = text_cleaning(train_df, 'text', 'clean_text')\nprint(\"time required text_cleaning :\", time.clock() - start_time, \"sec.\")","074b071e":"i = 4\n#check the text data before text-preprocessing\nprint(f\"text data at index {i} before text pre-processing : \\n\\n {train_df.text.iloc[i]}\")\nprint(\"\\n\\n\")\n\n#check the cleaned text data post text-preprocessing\nprint(f\"text data at index {i} post text pre-processing : \\n\\n {train_df.clean_text.iloc[i]}\")","86e15857":"#apply lemmatization\nstart_time = time.clock()\ntrain_df['clean_text'] = train_df['clean_text'].apply(PreprocessText.lemmatize_corpus, method='wordnet')\nprint(\"time required lemmatizing text :\", time.clock() - start_time, \"sec.\\n\")\n\n#check the cleaned text data post Lemmatization\nprint(f\"text data at index {i} post text Lemmatization : \\n\\n {train_df.clean_text.iloc[i]}\")","776b5a34":"lemmatizer=WordNetLemmatizer()\nlemmatizer.lemmatize('os')","63ade55f":"train_df['clean_text'] = train_df['clean_text'].apply(PreprocessText.remove_single_chars)\ntrain_df['clean_text'] = train_df['clean_text'].apply(PreprocessText.remove_redundant_whiteSpaces)\nprint(f\"text data at index {i} post single char text removal : \\n\\n {train_df.clean_text.iloc[i]}\")","904f0214":"def BOW(data):\n    bow_vectorizer = CountVectorizer(lowercase=True,stop_words= stop_words,token_pattern=r'\\w+',ngram_range=(1,2),\n                                     analyzer='word',max_features=50000)\n    train_bow = bow_vectorizer.fit_transform(data)\n    return bow_vectorizer,train_bow","58085901":"#BOW Vectorized Data\n\nbow_vectorizer,train_bow = BOW(train_df.clean_text)","27ca7890":"print(\"Type of BOW count vectorizer :\",type(train_bow))\nprint(\"Shape of BOW count vectorizer :\",train_bow.get_shape())\nprint(\"Number of unique words (uni-grams and bi-grams) :\", train_bow.get_shape()[1])","8a2b9663":"# document wise count of each n-gram term that appears in the BOW transformed corpus\nprint(train_bow[0:2])","03692c45":"#check some of the created features using BOW approach\n\nprint(\"some sample features(unique words in the corpus) :\\n\",bow_vectorizer.get_feature_names()[10000:10020])","3b738bad":"# Number of times a particular n-gram feature appears in the corpus using BOW vectorization\n\nlist(zip(bow_vectorizer.get_feature_names()[10000:10020], train_bow.sum(0).getA1()[10000:10020]))","0a593b8c":"# Top words in BOW embedding\n\nprint(dict(itertools.islice(bow_vectorizer.vocabulary_.items(), 25)))\n#print(dict(list(bow_vectorizer.vocabulary_.items())[0: 25]) )","0d0b8680":"def tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer(lowercase=True,stop_words=stop_words,analyzer='word',\n                                       token_pattern=r'\\w+',ngram_range=(1,2),min_df= 3,\n                                       max_features=50000,use_idf=True, smooth_idf=True, sublinear_tf=True)\n    train_tfidf = tfidf_vectorizer.fit_transform(data)\n    return tfidf_vectorizer,train_tfidf","016f91d0":"#TFIDF Vectorized Data\n\ntfidf_vectorizer,train_tfidf = tfidf(train_df.clean_text)","1d6725ad":"print(\"Type of TFIDF count vectorizer :\",type(train_tfidf))\nprint(\"Shape of TFIDF count vectorizer :\",train_tfidf.get_shape())\nprint(\"Number of unique words including uni-grams and bi-grams  :\", train_tfidf.get_shape()[1])","e2c54f36":"# document wise TF-IDF score of each n-gram term that appears in the TFIDF transformed corpus\n\nprint(train_tfidf[0:2])","e6d83682":"print(\"some sample features(unique words) in the TF-IDF vectorized corpus :\\n\\n\",tfidf_vectorizer.get_feature_names()[10000:10020])","c80e5725":"# Total TF-IDF score of each n-gram term that appears in the entire TFIDF transformed corpus \n\nlist(zip(tfidf_vectorizer.get_feature_names()[10000:10020], train_tfidf.sum(0).getA1()[10000:10020]))","a329c594":"# Top words in TF_IDF embedding\n\nprint(dict(itertools.islice(tfidf_vectorizer.vocabulary_.items(), 25)))\n#print(dict(list(tfidf_vectorizer.vocabulary_.items())[0: 25]) )","cd4d3c26":"list_of_sentances = list(train_df.clean_text.str.split())\nprint(f\"First sentence : {list_of_sentances[0]}\")","a554c013":"np.sort([len(sent) for sent in list_of_sentances])","751080d8":"train_df.iloc[[[len(sent) for sent in list_of_sentances].index(0)]]","f7022efe":"train_df.iloc[146131]","df14329a":"train_df.drop([146131],axis=0,inplace=True)\ntrain_df.reset_index(drop=True, inplace = True)","a72e57e3":"list_of_sentances = [sent for sent in list_of_sentances if len(sent) > 0]","e4921b59":"# vectorize the words using Word2Vec skip-gram based\n\nif os.path.isfile(\"..\/input\/github-w2vec-300-trained\/w2v_model.bin\"):\n    # load the Word2Vec model from saved Word2Vec model\n    w2v_model = KeyedVectors.load(\"..\/input\/github-w2vec-300-trained\/w2v_model.bin\")\n    print(\"Trained 300-dim word2vec model:\", w2v_model)\nelse:\n    start_time = time.clock()\n    w2v_model = Word2Vec(list_of_sentances,vector_size=300,window=8,min_count=5,workers=4,sg=1,hs=0,epochs=10)\n    print(\"time required for Word2Vec model trainin :\", time.clock() - start_time, \"sec.\")\n    # save the modeled words produced from Word2Vec\n    w2v_model.save('w2v_model.bin')","18db9a5e":"list_of_words_w2v = list(w2v_model.wv.key_to_index)\nprint(f\"Total number of words in trained word2vec : {len(list_of_words_w2v)}\")\nprint(list_of_words_w2v[:50])","8b355daa":"# check for the top-10 most similar words \n\nprint(w2v_model.wv.most_similar('useful',topn=10))\nprint('=='*50)\nprint(w2v_model.wv.most_similar('buggy', topn=10))","74d49f77":"#View the embedding vector for word 'buggy' and 'useful' using Word2Vec\n\nprint(w2v_model)\nprint(type(w2v_model))\n# print(\"buggy:\",w2v_model.wv['buggy'])\nprint(\"300-dim vector repreentation for word 'useful' using word2vec embeddings:\\n\",w2v_model.wv['useful'])","cf219b0f":"#Measure Cosine distance\n\ndistance = w2v_model.wv.similarity('useful','buggy')\nprint(distance)","497031db":"#plot the embedding vector for word 'buggy\n\nplt.figure(figsize = (8,6))\nplt.plot(w2v_model.wv['buggy'])\nplt.plot(w2v_model.wv['useful'])\nplt.show()","4a14245c":"%%time\n\n# Visualization of word2Vec embedded words in 2D using T-SNE transform  \n\ntsne = TSNE(n_components=2, random_state=0)\nw2v_data = w2v_model.wv[w2v_model.wv.key_to_index]\nw2v_tsne_transformed = tsne.fit_transform(w2v_data[:100])\n\n# create a scatter plot for projecting glove vetors in 2D\n\nfig = px.scatter(x=w2v_tsne_transformed[:,0], y=w2v_tsne_transformed[:,1],text=list_of_words_w2v[:100])\nfig.update_traces(textposition='bottom center')\nfig.update_layout(\n     width=1100,\n    height=900,\n    title_text='Visualization of Word2Vec embedded words in 2D using T-SNE transform'\n)\nfig.show()","41bc7b7b":"#load the pretrained 100-dim glove vector embeddings file and convert it into word2vec format598.\n\n\nglove_pretrained_input = '..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt'\nglove_to_word2vec_converted = 'glove.6B.100d.txt.word2vec'\n\nglove_loaded = glove2word2vec(glove_pretrained_input, glove_to_word2vec_converted)\nprint(glove_loaded)","46bc70c9":"# load the word2vec formatted glove vector embeddings model\n\nglove_model = KeyedVectors.load_word2vec_format(glove_to_word2vec_converted, binary=False)","2d6fbbf2":"list_of_words_glove = list(glove_model.key_to_index)\nprint(\"Total number of words in trained glove embeddings :\", len(list_of_words_glove))\nprint(\"\\nThe top 50 words in glove_vectors:\\n\", list_of_words_glove[:50])","91ad2953":"# check for the top-10 most similar words using glove embeddings\n\nprint(glove_model.most_similar('useful',topn=10,))\nprint('='*50)\nprint(glove_model.most_similar('buggy', topn=10,))","ec77b467":"#View the embedding vector for the words 'buggy' and 'useful' using glove embeddings\n\nprint(glove_model)\nprint(type(glove_model))\nprint(\"buggy:\",glove_model['buggy'])\nprint(\"useful:\",glove_model['useful'])","4d9a6d82":"# Measure cosine similarity for the words 'buggy' and 'useful' using glove embeddings\n\ndistance_g = glove_model.similarity('useful','buggy')\nprint(distance_g)","900484a7":"#plot the embedding vectors for the words 'buggy' and 'useful' using glove embeddings\n\nplt.figure(figsize = (8,6))\nplt.plot(glove_model['buggy'])\nplt.plot(glove_model['useful'])\nplt.show()","71d970b8":"%%time\n\n# Visualization of glove embedded words in 2D using T-SNE transform  \n\ntsne = TSNE(n_components=2, random_state=0)\nglove_data = glove_model[glove_model.key_to_index]\nglove_tsne_transformed = tsne.fit_transform(glove_data[:200])\n\n# create a scatter plot for projecting glove vetors in 2D\nfig = px.scatter(x=glove_tsne_transformed[:,0], y=glove_tsne_transformed[:,1],text=list_of_words_glove[:200])\nfig.update_traces(textposition='bottom center')\nfig.update_layout(\n     width=1100,\n    height=900,\n    title_text='Visualization of glove embedded words in 2D using T-SNE transform'\n)\nfig.show()","16a0045b":"# vectorize the words using FastText skip-gram based\nif os.path.isfile(\"..\/input\/fasttext300trained\/fasttext_model.bin\"):\n    # load the model from saved fasttext model\n    fasttext_model = KeyedVectors.load(\"..\/input\/fasttext300trained\/fasttext_model.bin\")\n    print(\"Pre-trained 300-dim fasttext model:\",fasttext_model)\nelse:\n    start_time = time.clock()\n    fasttext_model = FastText(list_of_sentances,vector_size=300,window=8,min_count=5,workers=4,sg=1,hs=0,epochs=20)\n    print(\"time required for FastText model training :\", time.clock() - start_time, \"sec.\")\n    # save the modeled words produced from fasttext\n    fasttext_model.save('github_fasttext_300_model.bin')","77587504":"list_of_words_fasttext = list(fasttext_model.wv.key_to_index)\nprint(\"Total number of words in trained fasttext embeddings :\", len(list_of_words_fasttext))\nprint(\"\\nThe top 50 words in fasttext_vectors:\\n\", list_of_words_fasttext[:50])","e7e45e02":"# check for the top-10 most similar words using fasttext embeddings\n\nprint(fasttext_model.wv.most_similar('useful',topn=10,))\nprint('='*50)\nprint(fasttext_model.wv.most_similar('buggy', topn=10,))","94ff480a":"#View the embedding vector for the words 'buggy' and 'useful' using fasttext embeddings\n\nprint(fasttext_model)\nprint(type(fasttext_model))\n# print(\"buggy:\",fasttext_model.wv['buggy'])\n# print(\"useful:\",fasttext_model.wv['useful'])\nprint(\"300-dim vector repreentation for word 'useful' using fasttext embeddings:\\n\",fasttext_model.wv['useful'])","74135980":"# Measure cosine similarity for the words 'buggy' and 'useful' using fasttext embeddings\n\ndistance_g = fasttext_model.wv.similarity('useful','buggy')\nprint(distance_g)","622525e2":"#plot the embedding vectors for the words 'buggy' and 'useful' using fasttext embeddings\n\nplt.figure(figsize = (8,6))\nplt.plot(fasttext_model.wv['buggy'])\nplt.plot(fasttext_model.wv['useful'])\nplt.show()","2b1d4d6f":"%%time\n\n# Visualization of glove embedded words in 2D using T-SNE transform  \n\ntsne = TSNE(n_components=2, random_state=0)\nfasttext_data = fasttext_model.wv[fasttext_model.wv.key_to_index]\nfasttext_tsne_transformed = tsne.fit_transform(fasttext_data[:200])\n\n# create a scatter plot for projecting fasttext vetors in 2D\nfig = px.scatter(x=fasttext_tsne_transformed[:,0], y=fasttext_tsne_transformed[:,1],text=list_of_words_fasttext[:200])\nfig.update_traces(textposition='bottom center')\nfig.update_layout(\n     width=1100,\n    height=900,\n    title_text='Visualization of fasttext embedded words in 2D using T-SNE transform'\n)\nfig.show()","d22e7376":"def avg_w2v_embeddings(data,model):\n  \n    # create a sent vector of size 300 as our word2vec model is trained with a vect_size of 300\n    sent_vec = np.zeros(300)\n    \n    # num of words present in each sentence having a valid vector present in the word2vec vocab\n    cnt_words =0; \n    \n    # loop through each word in a sentence\n    for word in (data.split()):\n        # check if the word is present in the word2vec vocab, if yes then append its vector representation to sent_Vec array\n        if word in model.wv.key_to_index:\n            vec = model.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n\n    # check if the count of words is not zero\n    if cnt_words != 0:\n        sent_vec \/= cnt_words\n    \n    return sent_vec","9ab6a455":"start_time = time.clock()\ntrain_df['Vectorized_avg_w2v'] = train_df['clean_text'].apply(avg_w2v_embeddings, model=w2v_model)\nprint(f\"Time required for average word2vec : {time.clock() - start_time}\")\n#train_df.to_csv('Avg-W2V-Baseline.csv',index=True)","94cc87ea":"def tfidf_w2v_embeddings(data,model,idf_dict,tfidf_feat):\n    \n    # create a sent vector of size 300 as our word2vec model is trained with a vect_size of 300\n    sent_vec = np.zeros(300)\n    \n    # sum of tfidf values for words present in each sentence\n    weight_sum =0\n    # loop through each word in a sentence\n    for word in data.split():\n        # check if the word is present in the word2vec vocab and in the tf-idf vocab\n        if word in model.wv.key_to_index and word in tfidf_feat:\n            w2v_vec = model.wv[word]\n\n            # to reduce the tf-idf computation we are computing as follows\n            # (data.split().count(word)\/len(data.split())) = tf value of word in this sentence\n            # tfidf_dict[word] = idf value of word in whole courpus\n            tf_idf = (data.split().count(word)\/len(data.split())) * idf_dict[word]\n            \n            sent_vec += (w2v_vec * tf_idf)\n            weight_sum += tf_idf\n    \n    if weight_sum != 0:\n        sent_vec \/= weight_sum\n    \n    return sent_vec","99fcb7e2":"start_time = time.clock()\n# train a tf-idf model on cleaned text data\ntf_idf_vect = TfidfVectorizer(stop_words= stop_words, ngram_range=(1,1), lowercase=True, analyzer='word', token_pattern=r'\\w+', \n                               min_df= 1, use_idf=True, smooth_idf=True, sublinear_tf=True)\n\n# fit the tf-idf model on the cleaned text data\ntf_idf_vect.fit(train_df.clean_text)\n\n# create a dictionary with word as a key, and the idf as a value\nidf_dictonary = dict(zip(tf_idf_vect.get_feature_names(), list(tf_idf_vect.idf_)))\n\n# tf-idf features\ntfidf_featues = tf_idf_vect.get_feature_names()\n\ntrain_df['Vectorized_tfidf_w2v'] = train_df['clean_text'].apply(tfidf_w2v_embeddings, model=w2v_model, idf_dict=idf_dictonary, tfidf_feat=tfidf_featues)\nprint(f\"Time required for tf-idf word2vec : {time.clock() - start_time}\")\n#train_df.to_csv('TfIdf-W2V-Baseline.csv',index=False)","7f5cb174":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV","95e12a5c":"def model_evaluate(train_x,train_y,test_x,test_y,emb_name,scoring,scoring_param,scoring_method):\n    \"\"\"\n    train_x,train_y = train input and output\n    test_x,test_y = test input and output\n    emb_name = 'Word2Vec'\n    scoring = 'roc_auc_ovr'\n    scoring_param ='AUC'\n    scoring_method ='roc_auc_score'\n    \"\"\"\n    \n    models=[]\n    model_name = []\n    model_training_result = []\n    model_validation_result = []\n    \n    models.append(('LogisticRregression',LogisticRegression(C=1.0, penalty='l2',class_weight='balanced')))\n#     models.append(('SVM',SGDClassifier(alpha=0.001, penalty='l2', loss='hinge',class_weight='balanced',random_state=42)))\n    models.append(('ExtraTreesClassifier',ExtraTreesClassifier(n_estimators=100, max_depth=None, min_samples_split=2,random_state=42)))\n    models.append(('BagClassifier',BaggingClassifier(DecisionTreeClassifier(criterion='gini'),max_samples=0.5, max_features=0.5,random_state=42)))\n#     models.append(('GradientBoostClassifier',GradientBoostingClassifier(learning_rate=1e-2, loss='deviance',n_estimators=100,random_state=42)))\n#     models.append(('AdaBoostClassifier',AdaBoostClassifier(learning_rate=1e-2,algorithm='SAMME.R',n_estimators=50,random_state=42)))\n#     models.append(('XGBoosting',xgb(n_estimators=50,max_depth=5,objective='multi:softmax',num_class=3,booster='gbtree',random_state=42,n_jobs=-1)))\n    models.append(('LightGBM',lgbm(n_estimators=500,boosting_type='gbdt',class_weight='balanced',random_state=42,n_jobs=-1)))\n\n    print(f\"Statistical Models Evaluation using {emb_name} Baseline\")\n\n    for name, model in tqdm(models):\n        \n#         if name == \"SVM\":\n#             model_svm = model.fit(np.vstack(train_x),train_y)\n#             model = CalibratedClassifierCV(cv=\"prefit\",method=\"sigmoid\",base_estimator=model_svm)\n        \n        skfold=StratifiedKFold(n_splits=6, shuffle=True, random_state=42)       \n        results = cross_val_score(model, np.vstack(train_x), train_y, cv=skfold, scoring = scoring)\n        print(\"=======================\"*4)\n        print(f\"Classifier: {name} has a training {scoring_param} score of {round(results.mean(), 3)}\")\n        model_training_result.append(results.mean())\n        \n        predictions = cross_val_predict(estimator = model, X = np.vstack(test_x), y = test_y, method = 'predict_proba')\n        pred_score = scoring_method(y_true=test_y, y_score=predictions, average='weighted', multi_class='ovr')\n        model_validation_result.append(pred_score)\n        model_name.append(name)\n\n    final_outcomes = pd.DataFrame(columns=['Embeddings_Baseline','Model',f'Training_{scoring_param}_Score',f'Validation_{scoring_param}_Score'])\n    final_outcomes['Embeddings_Baseline'] = [emb_name]*len(model_training_result)\n    final_outcomes['Model'] = model_name\n    final_outcomes[f'Training_{scoring_param}_Score'] = model_training_result\n    final_outcomes[f'Validation_{scoring_param}_Score'] = model_validation_result\n    return final_outcomes","ac361106":"#Split the dataset into training and test sets\ny = train_df['label'].values\nX = train_df['Vectorized_avg_w2v'].values\ntrain_x,test_x,train_y,test_y = train_test_split(X,y,test_size=0.2,random_state=42)\ntrain_x.shape,train_y.shape,test_x.shape,test_y.shape\nlen(train_x), len(train_y), len(test_x), len(test_y)\n\nResult_df = pd.DataFrame(columns=['Embeddings_Baseline','Model','Training_AUC_Score','Validation_AUC_Score'])\nstart_time = time.clock()\nResult_df = model_evaluate(train_x,train_y,test_x,test_y,'Word2Vec','roc_auc_ovr','AUC',roc_auc_score)\nprint(f\"Time required for avg word2vec modelling and evaluation : {time.clock() - start_time}\")\nResult_df","2865b12f":"#Split the dataset into training and test sets\n\ny = list(train_df['label'].values)\nX = list(train_df['Vectorized_tfidf_w2v'].values)\ntrain_x_tfidfw2v, test_x_tfidfw2v, train_y_tfidfw2v, test_y_tfidfw2v = train_test_split(X,y,test_size=0.2,random_state=42)\n#train_x_tfidfw2v.shape,train_y_tfidfw2v.shape,test_x_tfidfw2v.shape,test_y_tfidfw2v.shape\nlen(train_x_tfidfw2v), len(train_y_tfidfw2v), len(test_x_tfidfw2v), len(test_y_tfidfw2v)\n\n\nResult_df = pd.DataFrame(columns=['Embeddings_Baseline','Model','Training_AUC_Score','Validation_AUC_Score'])\nstart_time = time.clock()\nResult_df = model_evaluate(train_x_tfidfw2v,train_y_tfidfw2v,test_x_tfidfw2v,test_y_tfidfw2v,'Word2Vec','roc_auc_ovr','AUC',roc_auc_score)\nprint(f\"Time required for avg word2vec modelling and evaluation : {time.clock() - start_time}\")\nResult_df","9d735c8a":"num_of_words_in_seq = np.sort([len(sent) for sent in list_of_sentances])\n\nsns.distplot(num_of_words_in_seq, bins = 30)","e357d253":"max_sent_length = np.max(np.sort([len(sent) for sent in list_of_sentances]))\nprint(f\"Maximum number of words in sequence post cleanup : {max_sent_length}\")\nmin_sent_length = np.min(np.sort([len(sent) for sent in list_of_sentances]))\nprint(f\"Minimum number of words in sequence post cleanup : {min_sent_length}\")\navg_sent_length = np.mean(np.sort([len(sent) for sent in list_of_sentances]))\nprint(f\"Average length of the number of words in a sequence post cleanup : {avg_sent_length}\")\nmedian_sent_length = np.median(np.sort([len(sent) for sent in list_of_sentances]))\nprint(f\"Median length of the number of words in sequence post cleanup : {median_sent_length}\")","2d8418eb":"## tokenize text\ntokenizer = KP.text.Tokenizer(lower=True, split=' ', oov_token=\"NaN\", filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n')\ntokenizer.fit_on_texts(list_of_sentances)\nvocab_dict = tokenizer.word_index","272f3ec9":"print(dict(list(vocab_dict.items())[0:10]))\nprint(\"Total length of vocabulary:\", len(list(vocab_dict.items())))","0a4a9954":"text_seq = tokenizer.texts_to_sequences(list_of_sentances)","e364959b":"seq_corpus = KP.sequence.pad_sequences(text_seq, maxlen=50, padding=\"post\", truncating=\"post\")","34177301":"#Split the dataset into training and test sets\ny = train_df['label'].values\nX = seq_corpus\ntrain_x,test_x,train_y,test_y = train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)\ntrain_x.shape,train_y.shape,test_x.shape,test_y.shape\nlen(train_x), len(train_y), len(test_x), len(test_y)","ce0f297c":"train_y = keras.utils.to_categorical(train_y, 3)\ntest_y_conv = keras.utils.to_categorical(test_y, 3)","c3f946a5":"## create an embeddings matrix (length of vocabulary x vector size) and initialize with all 0s\nembeddings_mat = np.zeros((len(vocab_dict)+1, 300))\n\nfor word,idx in vocab_dict.items():\n    ## update the row with vector\n    try:\n        embeddings_mat[idx] =  w2v_model.wv[word]\n    ## if word not in model then skip and the row stays all 0s\n    except:\n        pass","f28c589d":"print(f\"Shape of Embeddings Matrix: {embeddings_mat.shape}\")","03e67568":"model = Sequential()\nmodel.add(layers.Embedding(input_dim=embeddings_mat.shape[0], output_dim=embeddings_mat.shape[1],weights=[embeddings_mat], input_length=50, trainable=False))\nmodel.add(layers.Bidirectional(layers.LSTM(units=50,return_sequences=True)))\nmodel.add(layers.Bidirectional(layers.LSTM(units=50)))\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(3, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy','AUC'])\nprint(model.summary())","3169b8d6":"## train\ntraining = model.fit(x=train_x, y=train_y, batch_size=128, epochs=20, shuffle=True, verbose=1, validation_split=0.2)","59069b2b":"score = model.evaluate(test_x, test_y_conv, verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))\n\nprint('Test loss:', score[0]) \nprint('Test accuracy:', score[1]*100)\nprint('Test auc:', score[2])","084dc23d":"## test\npredicted_prob = model.predict(test_x)\npredicted = [np.argmax(pred) for pred in predicted_prob]","50b90f47":"print(\"Confusion Matrix :\")\nsns.heatmap(confusion_matrix(y_true = test_y, y_pred = predicted), cmap='BuPu', annot=True, linewidths=1,fmt='1g')","579404f9":"print(\"Clssification Report:\")\nprint(classification_report(y_true = test_y, y_pred = predicted))","a5674ebe":"## **7. Model Building and Evaluation**\n\n- In this part, we will build statistical models using the word2vec embeddings that we have trained on our local corpus and evaluate the models using 'auc_score'.","aeb7402a":"### Inference:\n\n- Extra train data contains twice the number of data point compared to original train data.\n- If we see any problems of 'Overfitting' while evaluating model on the original train data, this additional train data will be useful in such scenarios.\n- We will continue our analysis with the given train data only and will use the extra train data when requierd for further analysis.","32d26c52":"## 2. Import Libraries and Data","f4ccd2aa":"- In a nutshell TF-IDF works by pennalizing the common words in the corpus by assigning them lower weights while giving more importance to rarer words which are rare in the entire document corpus but appear good numbers in a few documents.\n\n![image.png](attachment:image.png)","dee1f5f4":"### **5.6 FastText**\n\n- Word2vec treats each word in corpus like an atomic entity and generates a vector for each word. In this sense Word2vec is very much like Glove - both treat words as the smallest unit to train on.\n- Fasttext (which is essentially an extension of word2vec model), treats each word as composed of character ngrams. \n- So the vector for a word is made of the sum of this character n grams. For example, the word vector \u201capple\u201d is a sum of the vectors of the n-grams \u201c<ap\u201d, \u201capp\u201d, \u201dappl\u201d, \u201dapple\u201d, \u201dapple>\u201d, \u201cppl\u201d, \u201cpple\u201d, \u201dpple>\u201d, \u201cple\u201d, \u201dple>\u201d, \u201dle>\u201d (assuming hyperparameters for smallest ngram (min_n = 3) and largest ngram (max_n = 6) ).\n- Some of the advantages offered by FastText compared to the Word2Vec and Glove are as follows:\n    1. FasteText Generate better word embeddings for rare words ( even if words are rare their character n-grams are still shared with other words - hence the embeddings can still be good).\n    2. Out of vocabulary words - FastText can construct the vector for a word from its character n-grams even if word doesn't appear in training corpus. Both Word2vec and Glove don't support this.\n\n\nIn this case we will train the FastText model from Gensim using the corpus specific to our problem. We can also use the pre-trained FastText model for evaluating performance on our corpa.","d2c5bb32":"#### Reducing the Dimensionality to Visualize the Word2Vec word vectors in 2D\n\n- Transform the word2vec word vectors in 2D using T-SNE.\n- Visualize the word2vec transfomred features in 2D.","f83d15b4":"### Static Semantic Embeddings\n\nIn this vectorization approach we will implement static embeddings using different techniques like:\n\n1. Word2Vec\n2. Glove\n3. FastText","5f14d0f0":"Now we will pad our list of text sequences with zeros's and max padding length of 50 such all of sequences of same size.","d8f054e7":"### Dataset Complete Information at a glance Using Pandas Profiling:\n\nPandas profiling is a python package which helps us understand our data. It is a simple and fast way to perform exploratory data analysis of a Pandas Dataframe. The Pandas Profiling function extends the pandas DataFrame with df.profile_report() for quick data analysis. It displays a lot of information with a single line of code and that too in an interactive HTML report.","7f1df7f7":"#### Inferences:\n- We can see there are **323 (150000 - 149677)** duplicate entries present for the 'title' feature in train set.\n- **'add unit tests'** is the most repeated 'title' which is repeated 15 times in the train_data.\n- We can also see there are no duplicate entries present for the 'body' feature.\n- Since the 'body' text is an unique entry for each issue, we can conclude that the underlying 'body' text associated with a GitHub issue is primarily responsible for categorising the issue into either of the bug\/feature\/question however 'title' is also useful to as it briefly sets the context of the issue.\n- Thus for our analysis purpose we will merge both the title and text into a single feature.\n- No null values are present across the train_data and extra train_data.","f3baf746":"We can observe that \"Lght Gradient Boosting\" has outperformed compared to other modeling methods. We can hyperparameter tuning and feature selection to further improve the performance.","b65901fb":"#### Inference:\n- cosine similarity - ranges between 0 to 1,  0 being not similar and 1 being exactly similar\n- word vectors for 'buggy' and 'useful' are not siimilar as the cosine similarity of these vectors is only 0.2.\n- let's visulaize to confirm this observation.","62745bbe":"Let's create tokenized sequences from our list of sentences.","453c0110":"## **6. Defining methods for Document to Vector conversion and Statistical Modelling and Evaluation**\n\n- In this part, we will define methods for converting individual word embeddings to sentence embeddings and for evaluating our data using different statistical models.","0afdd31e":"# **Table of contents**\n\n1. Problem Statement\n2. Import Libraries and Data\n3. Dataset Analysis\n4. Data Preprocessing\n5. Data transformation - Word Embeddings\n    * Non-Semantic Embeddings\n        * Bag of Words (BOW)\n        * TF-IDF \n    * Static Semantic Embeddings \n        * Word2Vec\n        * Glove\n        * FastText\n6. Defining methods for Document to Vector conversion and Statistical Modelling and Evaluation\n    * Word Vectors to Sentence conversion using average word2vec embedding\n    * Word Vectors to Sentence conversion using Tf-Idf weighted word2vec embeddings\n    * Statistical Models Building and Evaluation using word2vec embeddings\n7. Model Building and Evaluation using Statical Models\n8. Model Building and Evaluation using Neural Network LSTM based model\n9. Conclusion","646c2f33":"Before we move to modelling let's understand some statistics about our list of sentences. This will be useful for generating the embeddings matrix.","de1f9ae9":"As we have created the train and test datasets, now we will need to create an embeddings matrix that we will use as weight matrix in our neural network architecture. ","2ce7d51d":"Let's remove the single chars again post lemmatization to handle this scenario","8c5b2a86":"#### Inference:\n\nIt can be observed that post data clean up we can see some compound words are present like 'controlbuttonflag' which is actually a composition of 3 semantic words like 'control', 'button' and 'flag'.\nWe can see there are lots of such compound words which are present in the corpus like 'alarmnumber', 'alarmaction', 'setonoffstandby', etc.","e2061c09":"### **6.1 Word Vectors to Sentence conversion using average word2vec embedding**","e5811ca9":"- these are the 300 dimensional word vectors using word2vec embeddings. \n- we can also observe that different word embedding techniques like glove and fasttext generates unique word vector the same words.","6b0a1f7a":"Thus I will take the sequence max_length as 50 for text to sequence conversion.","3416663b":"### **5.4 Word2Vec**\n\n- Word2vec is actually a collection of two different methods: continuous bag-of-words (CBOW) and skip-gram. \n\n![image.png](attachment:image.png)\n\n- Given a word in a sentence, CBOW uses the context or surrounding words as input for predicting the 'target word w(t)' (also called the center word) . For instance, if the context (sliding) window C is set to C=5, then the input would be words at positions w(t-2), w(t-1), w(t+1), and w(t+2). Basically the two words before and after the center word w(t). Given this information, CBOW then tries to predict the target word w(t).\n\n- Skip-gram is the exact opposite. Instead of inputting the context words and predicting the center word, we feed in the center word and predict the context words. This means that w(t) becomes the input while w(t-2), w(t-1), w(t+1), and w(t+2) are the ideal output.\n\n- Both the models (CBOW and Skip-gram) are focused on learning about words given their local usage context, where the context is defined by a window of neighboring words. This window is a configurable parameter of the model.\n\n- The size of the sliding window has a strong effect on the resulting vector similarities. Large windows tend to produce more topical similarities, while smaller windows tend to produce more functional and syntactic similarities.\n\n- Gensim provides a great way to use and start with Word2Vec. The Word2Vec algorithm can be built by using the Skipgram model as well as the Common Bag of Words Model. ","cdcff940":"### Dataset Description :\n\n\n<p>  Data set contains only three fields as follows :\n    <ul>\n        <li>Title - the title of the GitHub bug\/feature\/question<\/li>\n        <li>Body - the body of the GitHub bug\/feature\/question<\/li>\n        <li>Label - the label we are trying to Predict for the given GitHub Issue. It contains the various classes of as follows:\n            <ol>\n                <li>Bug - 0<\/li>\n                <li>Feature - 1<\/li>\n                <li>Question - 2<\/li>\n            <ol>\n        <\/li>\n    <\/ul>\n<\/p>","1a49acc9":"## 4. Data Preprocessing","6905216f":"### Load the Dataset","c7d54dda":"### **5.5 Glove**\n\n\n- The Global Vectors for Word Representation (GloVe), algorithm is an extension to the word2vec method for efficiently learning word vectors.\n\n- Glove embeddings rely on global vector representations mechanism, which is an unsupervised algorithm. \n\n- The Global Vector (GloVe) model aims to combine the count-based matrix factorization and the context-based skip-gram model together.\n\n- The GloVe approach combines both, the global statistics of matrix factorization techniques like LSA (Latent Semantic Analysis) and  the local context-based learning in word2vec.\n\n- Rather than using a window to define local context, GloVe constructs an explicit word-context or word co-occurrence matrix using statistics across the whole text corpus. The result is a learning model that may result in generally better word embeddings.\n\n- GloVe, is a global log-bilinear regression model for the unsupervised learning of word representations that outperforms other models on word analogy, word similarity, and named entity recognition tasks.","93c521dc":"At last, let's plot the confustion matrix and classifcation report to check the model performance.","9cf19129":"In this approach, I will use our trained Word2Vec model for generating embedding layer for Neural Network architecture. In this implementation, I will use the bi-directional LSTM layers to model the order of words in our sequence in both the directions.","20acbf8b":"Let's check the vocabulary post tokenization","e566ee4c":"### **5.3 Converting the preprocessed text data to list**\n\nThis phase is helpful for investigating the individual word embeddings or sentence embeddings. Word based embeddings techniques requires a list of individual features(words) of each of the documents(sentences) of pre-processed 'text' data as input. Differentiating the individual rows of text makes it easier to pass into static and dynamic embedding models.","3b6e00c8":"#### Inference:\n\n- we can see that the vectors representations of words 'buggy' and 'useful' are non-overlapping and out of phase thus representing non-similar words.","3df9f042":"Next, we will create train and test datasets from the sequence corpus.","7c3b3d04":"Since these are the pre-trained word vectors using fasttext embeddings, few of the similar words for 'buggy' and 'useful'  are different compared to word2vec.","18225744":"### Semantic Vectorization:\n\nIn this approach, numeric vectors are generated for each word considering the semantic meaning of the word in the   document corpus. These techniques represent a word with a low-dimensional vector (e.g. 100 dimensions). \n     \nThese techniques can be further divided into the 2 parts, static and dynamic embeddings. \n\n- **Static Word  Embeddings:**  Static means the same word will have same vector representation regardless of the context where it occurs.\n**Examples** : Word2Vec, Glove, Fasttext.\n\n- **Dynamic Word Embeddings:**   It considers the context of the word and the same word can have different vector representations based on the context where it occurs. Dynamic word embeddings also known as contextualized embeddings. **Examples** : BERT, GPT, ELMO, ULMFiT, etc.\n\n    \nSemantic embeddings rely on pretrained word vectors where a probabilistic score is attributed to each word in the corpus. These probabilities are plotted in a low dimensional plane and the \"meaning\" of the words are inferred from these vectors.\n\nSeveral metrices can be applied on Words vectors, like finding similarity, distance measurement between the vectors, numerical transforms of the vectors.\n\nWith word vectors, we can specify semantic similarity between different words or collection of words. Generally, cosine distance is taken as the major metric of similarity measurement between word and sentence vectors to infer similarity.","88919fe5":"### Lemmatization :\n\nTill now we have cleaned the data and removed all the redundant text and noise from it. This reduced the dimensionality of the data to certain extent. Next up, we will prune some words to their roots which will again reduce the length of sentences. Here we are applying Lemmatization to reduce the words to their morphological roots so as to retain the symantics of the text. We can also apply 'Stemming' in this case, but we want to retain symantic meaning of the words, so we are not considering that approach.","8e7eb918":"### Check the Stop Words list","2a3f3afc":"#### Glove algorithm implementation strategy:\n\n- For implementing the Glove vectors, we will first load the pretrained glove vectors (100 - dimension representation) and convert these from \"txt\" format to Word2Vec format by using scripts provided in the Gensim library.\n- This allows us to manipulate the glove embeddings in a manner similar to Word2Vec and apply the similarity metric.","6d23ad36":"<h1 style=\"text-align:center\">Git Hub Bug Prediction<\/h1>\n\n\nThis work demonstrates the implementation of static semantic word embedding techniques like Word2Vec, Glove and FastText and the performance analysis is carried here using Word2Vec baseline for statistical Machine Learning models. I have also implemented and analyzed non-semantic word embedding techniques like BOW and TFIDF. This is my 2nd notebook in the end-to-end implementation approach for solving GitHub Bug Prediction problem series. You can also refer my [1st](http:\/\/www.kaggle.com\/gauravharamkar\/eda-github-bug-prediction) notebook from this series where I have carried out detailed EDA for this problem.","9376d650":"## **8.Model Building and Evaluation using Neural Network LSTM based model**","3aa09a8a":"### Check the basic stats of the data\n\nThe pandas df.describe() and df.info() functions are normally used as a first step in the EDA process and it gives us a basic overview of the data.","7636b4a1":"### **7.1. Model Building and Evaluation using Average Word2Vec on trained corpus**","62fcf8f7":"### Non-Semantic (Count based) Vectorization:\n \n- In this approach, numeric vectors are generated for each word in the document corpus based on the frequency of occurrence of the word in the corpus.  \n- No semantic meaning of the word is taken into consideration while transforming into vectors. \n- These techniques usually represent a word with a very high-dimensional sparse vector. \n- These techniques leverage statistical co-occurence probabilities and log likelihoods for determining the frequently occuring sentences or group of words in a corpus.\n\n**Examples** : Bag-Of-Words(BOW), TF-IDF, One-Hot Encoding, etc.","63aeb832":"#### Inference:\n\n- we can see that the vectors representations of words 'buggy' and 'useful' are non-overlapping and out of phase thus representing non-similar words.","0ec493c6":"### **6.2 Word Vectors to Sentence conversion using Tf-Idf weighted word2vec embeddings**","e352e8b9":"### Combining Title and Body into a Single Feature for further analysis","ca72aec5":"### 7.2. Model Building and Evaluation using Tf-Idf weighted Word2Vec on trained corpus","183fa22d":"we can observe that as the number of epochs are increasing, the validation score is decreasing but the training score is increasing. This means the model is fitting on the train data. This problem can be solved by adding dropout layers in our architecture.","8909187f":"Word net lemmatizer has pruned the word 'os' to 'o' by removing the ending 's' character, let's cross-verify once.","50e4f138":"### **5.1 BOW (with Bi-grams)**\n\n- This is a simpler vectorization technique which relies on frequency of occurence of a particular term in a document or corpus. ","cc020e99":"#### In this kernel we will analyze only the static semantic embeddings.","ca9a2378":"### Distribution of data points amongst output labels","45c51df5":"## Plot the distribution of data points amongst output labels","2f019670":"## Reducing the Dimensionality to Visualize the FastText word vectors in 2D\n\n- Transform the fasttext word vectors in 2D using T-SNE.","a76be356":"### Load the Extra Train data","e3842e4e":"Since we have created such an input which is a list of lists of individual words each datapoint in the cleaned text, now let's start with building static semantic embeddings models like word2vec using Gensim library.","5786ed84":"#### Inferences:\n\n- we can observe that the T-SNE approach has better summarized the data for high dimensional w2v vectors.\n- we can also see word2vec has learned the local semantic relationship for the words.  \n    - Example 1 -  words like 'expected' and 'behavior' are close to each other in the vector space.\n    - Example 2 -  words like 'reproduce' and 'step' are close to each other in the vector space.\n- thus we can conclude that the word2vec has capured the local semantic information for the word vectors based on the contextual information for the words.","fc472db5":"Since, such an input text has no meaning, semantically or otherwise, from the model building and evaluation standpoint, we will need to remove this entry from our input which we will be using for generating word\/sentence embeddings.","df9b5a67":"## 3. Dataset Analysis","2ee90c94":"#### Get the vocabulary of BOW embedding\n\n- Vocabulary gives the mapping of words to feature indices.","74595989":"#### Inference : \n- The output of BOW contains the features which are present for each document and the count of occurence of that feature in that document.\n- for example : (0, 49969)\t2  --> meaning the feature(word) with feature index '49969' appears 2 times in the first (0th) document.","4d6c5205":"#### Inference:\n\n- we can see that the vectors representations of words 'buggy' and 'useful' are non-overlapping and out of phase thus representing non-similar words.","49338b24":"## 1. Problem Statement :\n<p> For an issue posted on the GitHub, predict whether that issue is a bug or a feature or a question based on the issue title and the body text.<\/p> ","d3f17f62":"### **5.2 TF-IDF (with Bi-Grams)**\n\n- This vectorization technique relies not only on frequency of occurence of a particular term in a document or but also considers if that term is present in how many documents across the corpus.\n- The TF-IDF formulation is as per below:\n\n![image.png](attachment:image.png)","84ac9924":"We can see for our cleaned corpus, sentences lengths varies from a minimum of '0' words to a maximum of '6981' words. We can clearly observe that there is actually one record in the cleaned text with length '0', meaning there is not a single word present for that entry. Let's check the cleaned text for this record which has zero length.","44d409db":"### Inferences:\n\n- Due to the character level n-gram approach, we can observe that the t-SNE has better approximated the word vectors and all the words are placed closed to each other in vector space.\n- We can also see that FastText has learned the semantic relationship for the words very well. \n  - Example -  words like 'application'-'app'-'android', 'actual'-'expected'-'behavior' etc. resides in the vicinity of each other in the vector space.\n- Thus we can conclude that the FastText has capured the semantic information for the word vectors at the granular levels based on the contextual information of the words in our corpus.","5e628ccb":"Next up we will train a model using bi-directional LSTM's and evaluate it using test set.","84e14130":"### **6.3 Statistical Models Building and Evaluation using word2vec embeddings**\n\nIn this part, we will use different statistical models for evaluating the performance of the of our word2vec embeddings. Increasing the number of different models increases the time complexity of the model evaluation method here. Thus I have used only few methods for model evaluation here for demonstration purpose. Based on your requirement you can use different statistical models for evaluation.","a1917ee0":"Let's closely analyze the cleaned_text. We will check the minimum and maximum number of words in a sentence.","0c7f338f":"#### Get the vocabulary of TF-IDF embedding\n\n- Vocabulary gives the mapping of words to feature indices.","9c795823":"#### Inference:\n\n- since these are pre-trained word vectors using glove embeddings, the similar words for 'buggy' and 'useful'  are different compared to original word2vec","53f0b5e2":"#### Inference:\n- As expected, words 'useful' and 'buggy' are not similar at all.","72a6defa":"#### Inference:\n\n- We can see that the distribution of data is well balanced between 'Bug' and 'Feature' label categories whereas the 'Question' types labels are comparitively very few. Thus, we can observe data imbalance here.","611a2e02":"#### Inferences:\n\n- We can observe that the T-SNE approach has better summarized the data for 100 dimensional Glove vectors. \n- We can see related words like 'former'-'president', 'governement'-'minister'-'party' are placed closed to each other in vector space.\n- We can also see Glove has learned the global and local semantic relationship for the words. \n  - Example -  words like 'united'-'states', 'new'-'york' resides in the same region.\n  - Example -  all the days like 'monday', 'tuesday',etc. resides in the same region.\n  \n- Thus we can conclude that the Glvoe has capured the local as well as global semantic information for the word vectors based on the contextual information for the words.","cfdb91ce":"### Text Pre-processing  -- Cleaning Redundant Data\n\n- We can observe that, the 'text' feature contains a lot of redundant entities like punctuations, stop words, url, html tags,etc. \n- We can also observe that there is some noisy text such **\"\\\\r\"** is present widely in 'text' feature across all the label categories. This noisy data needs to be cleaned explicitly.\n- We will need to clean such data before we proceed with the word embedding and vector transformations. \n\nRemoving below will sufficiently clean the text and will remove redundancies.\n\n1. HTML codes\n2. URLs\n3. Emojis\n4. Stopwords\n5. Punctuations\n6. Expanding Abbreviations","fa40d13a":"#### Let's take a closer look at the 'text' data","a2b6233b":"This work demonstrates the application of different statistical methods for evaluating performance of our Word2Vec implementaion. In this kernel I have implemented end-to-end model evaluation using the \"Word2Vec\" static semantic embeddings. I have also demonstrated other static semantic embeddings strategies like \"Glove\" and \"FastText\" and non-semantic embeddings strategies like \"BOW\" and \"Tf-Idf\". \nI have carried our the word vectors to sentence conversion using \"Averaging or Mean pooling\" the word vectors. The word vectors to sentence conversion is also demonstrated using the \"Tf-IDF weighted Word2Vec\" strategy, based on the use case and application one can choose to implement any one of the two methods. The same word to sentence conversion methods which are used for Word2Vec in this implementation can be extended to the \"Glove\" and \"FastText\" embeddings.\nAt last, I have trained and evaluated a LSTM based neural networks model using the Word2Vec embeddings and evaluated the model performance.","a450c69d":"## **9. Conclusion**","10d9db7f":"## 5. Data transformation - Word Embeddings\n\n\n- Word Embeddings means the process of converting words into real valued numeric vectors. It is also called Vectorization. Infact, word vectors and word embeddings are basically the same terminologies and can be used interchangeably. \n\n- Broadly the Word Embedding methods are categorised into two parts:\n\n    * Non-Semantic (Count based) Vectorization\n \n    * Semantic Vectorization\n","06603271":"\n<h3 style=\"text-align:center;color:red\"><b>if you like this work, please do an upvote<\/b><\/h3>","54f78a91":"Convert class labels to binary class matrices.","1c8dbb17":"#### Inference:\n- As expected, words 'useful' and 'buggy' are not similar at all.","183fbf4a":"#### Reducing the Dimensionality to Visualize the Glove word vectors in 2D"}}