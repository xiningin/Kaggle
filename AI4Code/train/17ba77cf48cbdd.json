{"cell_type":{"ff82396a":"code","9a4809b3":"code","4fdcd01c":"code","a5e9a52e":"code","6d44d376":"code","5c347840":"code","e744e958":"code","0d340e23":"code","f0b5afa3":"code","e855e0e4":"code","35281673":"code","7147a52e":"code","2c63cca1":"code","844fb613":"code","df4a2a9b":"code","09a75600":"code","d695364f":"code","521111f8":"code","c00d0c18":"code","c537c1f2":"code","92ff709f":"code","4f511532":"code","abd4bbb4":"code","31ce6c4d":"code","2e1aa096":"code","c0cbdafa":"code","e310519e":"code","e1705ccd":"code","bc4b8d4f":"code","79db61f7":"code","726e9f93":"code","1ae99dc5":"code","4a58ad41":"code","23983261":"code","eb138683":"code","16d81fc5":"code","9c1cb42d":"code","0755b0b7":"code","3a9b46a2":"code","d07d2009":"code","f0c3b3a1":"code","920007c3":"code","b0fc5635":"code","0acac8ce":"code","9ac9a646":"code","d1a572c1":"code","f2ee8d6f":"markdown","c382088d":"markdown","a756a30c":"markdown","970aeec2":"markdown","41d1ff6e":"markdown","1f85f4ec":"markdown","938de183":"markdown","9aa22ff3":"markdown"},"source":{"ff82396a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%pylab inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9a4809b3":"quora_train = pd.read_csv(\"..\/input\/train.csv\")\nquora_train.head()","4fdcd01c":"quora_train.info()","a5e9a52e":"print(f\"Number of neitral texts: {len(quora_train[quora_train['target']==0])}\")\nprint(f\"Number of neitral texts: {len(quora_train[quora_train['target']==1])}\")","6d44d376":"import nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","5c347840":"from nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer()\nsklearn_tokenizer = vect.build_tokenizer()","e744e958":"sklearn_tokenizer(quora_train.loc[2][\"question_text\"])","0d340e23":"word_tokenize(quora_train.loc[2][\"question_text\"])","f0b5afa3":"from nltk.corpus import stopwords\n\nstopwords.words(\"english\")","e855e0e4":"from sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\n\nvect = TfidfVectorizer(stop_words=stopwords.words(\"english\"))\nclf = SGDClassifier(loss=\"modified_huber\", max_iter=6, class_weight = \"balanced\")\nmodel = Pipeline([('vect', vect), ('clf', clf)])","35281673":"%%time\npreds = cross_val_predict(model, quora_train.question_text.values, quora_train.target.values,\n                          cv=StratifiedKFold(4), n_jobs=-1,\n                          method='predict_proba')","7147a52e":"from sklearn.metrics import roc_auc_score, classification_report, f1_score\n\nroc_auc_score(quora_train.target.values, preds[:,1])","2c63cca1":"thresholds = np.arange(0.05, 0.95, 0.05)\npred_arr = []\nfor threshold in thresholds:\n    pred_arr.append(list(map(lambda x: 1 if x[1]>threshold else 0, preds)))","844fb613":"i = 0\nfor pred in pred_arr:\n    print(f\"F1-score = {f1_score(quora_train.target.values, np.array(pred))} with threshold = {thresholds[i]}\")\n    i += 1","df4a2a9b":"best_predictions = list(map(lambda x: 1 if x[1]>0.7 else 0, preds))\nprint(classification_report(quora_train.target.values, best_predictions))","09a75600":"import eli5\nmodel.fit(quora_train.question_text.values, quora_train.target.values)\neli5.show_weights(model, top=20)","d695364f":"from nltk.stem import SnowballStemmer, WordNetLemmatizer, LancasterStemmer\nfrom functools import lru_cache","521111f8":"@lru_cache(maxsize=2048)\ndef lemmatize_word(word):\n    parts = ['a','v','n','r']\n    lemmatizer = WordNetLemmatizer()\n    for part in parts:\n        temp = lemmatizer.lemmatize(word, part)\n        if temp != word:\n            return temp\n    return word    ","c00d0c18":"stemmer = SnowballStemmer('english')\nprint(lemmatize_word('evening'))\nprint(stemmer.stem('evening'))","c537c1f2":"def lemmatize_sentence(sentence, tokinizer):\n    return list(map(lemmatize_word, tokinizer(sentence)))\n\ndef stemmatize_sentence(sentence, tokinizer):\n    stemmer = SnowballStemmer('english')\n    return list(map(stemmer.stem, tokinizer(sentence)))","92ff709f":"%%time\n\nlemmatized_data = list(map(lambda t: lemmatize_sentence(t, sklearn_tokenizer), quora_train.question_text.values))\nstemmatized_data = list(map(lambda t: stemmatize_sentence(t, sklearn_tokenizer), quora_train.question_text.values))","4f511532":"%%time\n\ninv_lemmatized_data = list(map(lambda t: \" \".join(t), lemmatized_data))\ninv_stemmatized_data = list(map(lambda t: \" \".join(t), stemmatized_data))","abd4bbb4":"stemming_preds = cross_val_predict(model, inv_stemmatized_data, quora_train.target.values,\n                          cv=StratifiedKFold(4), n_jobs=4, verbose=1,\n                          method='predict_proba')","31ce6c4d":"lemming_preds = cross_val_predict(model, inv_lemmatized_data, quora_train.target.values,\n                          cv=StratifiedKFold(4), n_jobs=4, verbose=1,\n                          method='predict_proba')","2e1aa096":"lemming_pred_arr = []\nstemming_pred_arr = []\nfor threshold in thresholds:\n    lemming_pred_arr.append(list(map(lambda x: 1 if x[1]>threshold else 0, lemming_preds)))\n    stemming_pred_arr.append(list(map(lambda x: 1 if x[1]>threshold else 0, stemming_preds)))","c0cbdafa":"i = 0\nprint(\"Lemmatization\")\nfor pred in lemming_pred_arr:\n    print(f\"F1-score = {f1_score(quora_train.target.values, pred)} with threshold = {thresholds[i]}\")\n    i += 1","e310519e":"i = 0\nprint(\"Stemming\")\nfor pred in stemming_pred_arr:\n    print(f\"F1-score = {f1_score(quora_train.target.values, pred)} with threshold = {thresholds[i]}\")\n    i += 1","e1705ccd":"model.get_params().keys()","bc4b8d4f":"# \u0437\u0430\u0434\u0430\u0434\u0438\u043c \u0441\u0435\u0442\u043a\u0443 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0434\u043b\u044f \u043f\u0435\u0440\u0435\u0431\u043e\u0440\u0430 \u043c\u043e\u0434\u0435\u043b\u044c\u043a\u0438\n\"\"\"\u042f \u0443\u0436\u0435 \u043f\u0435\u0440\u0435\u0431\u0440\u0430\u043b(207 \u043c\u0438\u043d\u0443\u0442!!!), \u043f\u043e\u043b\u0443\u0447\u0438\u043b \u0442\u0430\u043a\u0438\u0435 \u043b\u0443\u0447\u0448\u0438\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b, \n{'clf__l1_ratio': 0.15,\n 'clf__loss': 'modified_huber',\n 'clf__max_iter': 3,\n 'vect__max_df': 0.8,\n 'vect__min_df': 0,\n 'vect__stop_words': None}\"\"\"\n\"\"\"\n\u0412\u043e\u0442 \u0441\u0442\u0430\u0440\u0430\u044f \u0441\u0435\u0442\u043a\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432. \u0422\u0435\u043f\u0435\u0440\u044c \u043f\u0435\u0440\u0435\u0431\u0435\u0440\u0443 \u0431\u044b\u0441\u0442\u0440\u0435\u0435, \u0447\u0442\u043e\u0431\u044b \u0434\u0435\u0431\u0438\u043b\u044c\u043d\u044b\u0439 kernel \u0441\u043c\u043e\u0433 \u0437\u0430\u043a\u043e\u043c\u043c\u0438\u0442\u0438\u0442\u044c\nparam_grid = {\n    'clf__l1_ratio': [0.15, 0.35],\n    'clf__loss': ['log', 'modified_huber'],\n    'clf__max_iter': [3, 5, 7],\n    'vect__stop_words': [None, stopwords.words(\"english\")],\n    'vect__max_df': [0.8, 1.0],\n    'vect__min_df': [0, 100]\n}\"\"\"\n\nparam_grid = {\n    'clf__max_iter': [2, 3],\n    'vect__max_df': [0.7, 0.8]\n}","79db61f7":"from sklearn.model_selection import GridSearchCV\n\nnew_vect = TfidfVectorizer(lowercase=True)\nnew_clf = SGDClassifier(class_weight = \"balanced\", loss='modified_huber')\nnew_model = Pipeline([('vect', new_vect), ('clf', new_clf)])\nrealy_long_search = GridSearchCV(new_model, param_grid, cv = StratifiedKFold(4), verbose=1, n_jobs=4, scoring='f1')","726e9f93":"realy_long_search.fit(inv_stemmatized_data, quora_train.target.values)","1ae99dc5":"realy_long_search.best_params_","4a58ad41":"best_sgd = realy_long_search.best_estimator_","23983261":"best_predictions = cross_val_predict(best_sgd, inv_stemmatized_data, quora_train.target.values,\n                          cv=StratifiedKFold(4), n_jobs=4, verbose=1,\n                          method='predict_proba')","eb138683":"best_pred_arr = []\nfor threshold in thresholds:\n    best_pred_arr.append(list(map(lambda x: 1 if x[1]>threshold else 0, best_predictions)))","16d81fc5":"i = 0\nprint(\"Best model scores\")\nfor pred in best_pred_arr:\n    print(f\"F1-score = {f1_score(quora_train.target.values, pred)} with threshold = {thresholds[i]:.2f}\")\n    i += 1","9c1cb42d":"best_threshold = 0.75","0755b0b7":"submission_example = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmission_example.head()","3a9b46a2":"test = pd.read_csv(\"..\/input\/test.csv\")\ntest.head()","d07d2009":"%%time\n\ntest_stemmatized_data = list(map(lambda t: stemmatize_sentence(t, sklearn_tokenizer), test.question_text.values))\ntest_inv_stemmatized_data = list(map(lambda t: \" \".join(t), test_stemmatized_data))","f0c3b3a1":"test_predictions = best_sgd.predict_proba(test_inv_stemmatized_data)","920007c3":"out_predictions = list(map(lambda x: 1 if x[1]>best_threshold else 0, test_predictions))\nout_predictions[200:220]","b0fc5635":"answers = pd.DataFrame(np.transpose([test[\"qid\"].values, out_predictions]), columns=[\"qid\", \"prediction\"])\nanswers.head()","0acac8ce":"answers.to_csv('submission.csv',index=False)","9ac9a646":"len(answers)","d1a572c1":"\"\"\"from IPython.display import HTML\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\n\n# create a link to download the dataframe\ncreate_download_link(answers.iloc[30000:])\"\"\"","f2ee8d6f":"## Preprocessing","c382088d":"## Learning","a756a30c":"## \u0428\u0442\u0443\u043a\u0438 \u0441 \u0432\u0435\u043a\u0442\u043e\u0440\u0430\u043c\u0438. \u0411\u0443\u0434\u0443\u0442 \u043f\u043e\u0442\u043e\u043c","970aeec2":"## \u0422\u0432\u043e\u0440\u0447\u0435\u0441\u0442\u0432\u043e \u0433\u044b\u0433\u044b\u0433\u044b","41d1ff6e":"## \u0421\u0442\u0435\u043c\u043c\u0438\u043d\u0433, \u041b\u0435\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f","1f85f4ec":"## \u0412\u0440\u043e\u0434\u0435 \u043a\u0430\u043a \u0441\u0442\u0435\u043c\u043c\u0438\u043d\u0433 \u0434\u0430\u043b \u043b\u0443\u0447\u0448\u0438\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442, \u0447\u0435\u043c \u0438\u0437\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u043b\u0438 \u043b\u0435\u043c\u043c\u0438\u043d\u0438\u0437\u0430\u0446\u0438\u044f. \u0422\u0430\u043a \u0447\u0442\u043e \u0431\u0443\u0434\u0435\u043c \u0434\u0430\u043b\u044c\u0448\u0435 \u0443\u0447\u0438\u0442\u044c\u0441\u044f \u043d\u0430 \u043d\u0438\u0445 ","938de183":"## \u042f \u043f\u043e\u043a\u0430 \u0441\u043e\u0445\u0440\u0430\u043d\u044e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b, \u0438\u0431\u043e \u0443\u0441\u0442\u0430\u043b\u044c, \u043f\u043e\u0437\u0436\u0435 \u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0443","9aa22ff3":"## \u041e\u0431\u044a\u044f\u0441\u043d\u0438\u043c \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e eli5"}}