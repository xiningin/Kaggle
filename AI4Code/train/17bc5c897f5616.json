{"cell_type":{"dc29baa8":"code","da8519ad":"code","ab7f83f0":"code","fc29e1ed":"code","cd2236fb":"code","96d81b4c":"code","0d60cb89":"code","2312d806":"code","7ccb20a3":"code","c7ab07f0":"code","623be4c9":"code","23675b9d":"code","3b05e199":"code","4fab169c":"code","0a87877f":"code","387613c8":"code","4cfa71da":"code","84305b6c":"code","24cf8f55":"code","67b08b42":"code","1b59387d":"code","c798c7b4":"code","50351077":"code","28ee9621":"code","286f3973":"code","15336ec4":"code","b53f5f29":"code","b60ed041":"code","ce4072a1":"code","9f04bf06":"code","d1e65643":"code","6738d17e":"code","62a0c9f4":"code","e4bd7f49":"code","ce7a1fea":"code","0852dfc4":"code","73279def":"code","ac467430":"code","9e0ab9c6":"code","0d60ce28":"code","a086b160":"code","d6bd3c0b":"code","0aa37d1b":"code","36b423b4":"markdown","09046b6b":"markdown","3c8ee37f":"markdown","decd8c10":"markdown","4a4e3ff1":"markdown","5172ff88":"markdown","5ee157e7":"markdown","511340da":"markdown","d0d34bf5":"markdown","5195fd80":"markdown","a08a5226":"markdown","128ff491":"markdown","61fb1ef2":"markdown","549b8dec":"markdown","d5a98855":"markdown","e5e53e9f":"markdown","24d1d8d9":"markdown","c7a1c611":"markdown","0a319653":"markdown","74ccddc0":"markdown","905f0c74":"markdown","4b7273c4":"markdown","bef4fa44":"markdown","1a48d8ef":"markdown","982f2e4f":"markdown","12f63353":"markdown","a4668f26":"markdown","54cee578":"markdown"},"source":{"dc29baa8":"import pandas as pd\npd.options.display.max_colwidth = 80\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.svm import SVC # SVM model with kernels\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","da8519ad":"student_por = pd.read_csv('\/kaggle\/input\/student-performance-data-set\/student-por.csv')\nstudent_por.head()","ab7f83f0":"student_por.describe()","fc29e1ed":"# check missing values in variables\n\nstudent_por.isnull().sum()","cd2236fb":"student_por.isnull().any()","96d81b4c":"student_por.info()","0d60cb89":"copied = student_por.copy()\n\nmean = 5.7\nmax_min = 75\n\ndef mean_normalization(x):\n    return((x-mean)\/max_min)\n\ncopied['absences'] = copied['absences'].apply(mean_normalization)\ncopied['health'] = copied['health'].apply(mean_normalization)\n\ncorr_matrix = copied.corr()\n\ncorr_matrix[\"absences\"].sort_values(ascending=False)","2312d806":"corr_matrix = student_por.corr()\n\ncorr_matrix[\"absences\"].sort_values(ascending=False)","7ccb20a3":"corr_matrix[\"G3\"].sort_values(ascending=False)","c7ab07f0":"from pandas.plotting import scatter_matrix\n\n# I don't take G2 and G1 into account, because they are an obvious choice\nattributes = [\"G3\", \"studytime\", \"Fedu\", \"failures\", \"Dalc\", \"Walc\"] \n\nscatter_matrix(student_por[attributes], figsize=(16, 12))","623be4c9":"import seaborn as sns\n\ncorr_matrix = student_por.corr()\n\nplt.figure(figsize=(20,20))\nsns.heatmap(corr_matrix, annot=True, cmap=\"Blues\")\nplt.title('Correlation Heatmap', fontsize=20)","23675b9d":"#comparing sex with G3\nsns.boxplot(x=\"sex\", y=\"G3\", data=student_por)","3b05e199":"#comparing school with G3\nsns.boxplot(x=\"school\", y=\"G3\", data=student_por)","4fab169c":"#comparing adress with G3\nsns.boxplot(x=\"address\", y=\"G3\", data=student_por)","0a87877f":"#comparing parent's jobs with G3\nsns.boxplot(x=\"Mjob\", y=\"G3\", data=student_por)\nsns.boxplot(x=\"Fjob\", y=\"G3\", data=student_por)","387613c8":"#comparing famsize with G3\nsns.boxplot(x=\"famsize\", y=\"G3\", data=student_por)","4cfa71da":"#comparing Pstatus with G3\nsns.boxplot(x=\"Pstatus\", y=\"G3\", data=student_por)","84305b6c":"#comparing reason with G3\nsns.boxplot(x=\"reason\", y=\"G3\", data=student_por)","24cf8f55":"#comparing guardian with G3\nsns.boxplot(x=\"guardian\", y=\"G3\", data=student_por)","67b08b42":"#comparing schoolsup with G3\nsns.boxplot(x=\"schoolsup\", y=\"G3\", data=student_por)","1b59387d":"#comparing famsup with G3\nsns.boxplot(x=\"famsup\", y=\"G3\", data=student_por)","c798c7b4":"#comparing paid with G3\nsns.boxplot(x=\"paid\", y=\"G3\", data=student_por)","50351077":"#comparing activities with G3\nsns.boxplot(x=\"activities\", y=\"G3\", data=student_por)","28ee9621":"#comparing nursery with G3\nsns.boxplot(x=\"nursery\", y=\"G3\", data=student_por)","286f3973":"#comparing higher with G3\nsns.boxplot(x=\"higher\", y=\"G3\", data=student_por)","15336ec4":"#comparing internet with G3\nsns.boxplot(x=\"internet\", y=\"G3\", data=student_por)","b53f5f29":"#comparing romantic with G3\nsns.boxplot(x=\"romantic\", y=\"G3\", data=student_por)","b60ed041":"# making dataframe I'm gonna work with + target G3\n\nfeatures_chosen = ['studytime', 'failures', 'Dalc', 'Walc', 'traveltime', 'freetime',  'Medu', 'Fedu', \n                   'sex', 'school', 'address', 'Mjob', 'Fjob', 'reason', 'guardian', 'schoolsup', \n                   'higher', 'internet', 'G1', 'G2', 'G3']\n\nstudent_reduced = student_por[features_chosen].copy()\n\nstudent_reduced","ce4072a1":"student_reduced.hist(bins=20, figsize=(20,15))\nplt.show()","9f04bf06":"features_cat = ['sex','school','address','Mjob','Fjob','reason','schoolsup','guardian','higher','internet']\n\nstudent_reduced_cat = pd.get_dummies(student_reduced, columns = features_cat)\nstudent_reduced_cat","d1e65643":"student_reduced_cat.columns","6738d17e":"X = np.array(student_reduced_cat.drop(['G3'],1))\ny = np.array(student_reduced_cat['G3'])  ","62a0c9f4":"scaler = StandardScaler()\n\nX = scaler.fit_transform(X)","e4bd7f49":"X.shape","ce7a1fea":"X_train, X_test,y_train, y_test = train_test_split(X, y, test_size=0.24, random_state=42)","0852dfc4":"X_train.shape, X_test.shape","73279def":"from sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(penalty=\"l2\") # specifying Ridge Regression\n\nsgd_reg.fit(X_train, y_train)","ac467430":"accuracy=sgd_reg.score(X_test,y_test)  \naccuracy","9e0ab9c6":"def plot_learning_curves(model, X, y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2) \n    train_errors, val_errors = [], []\n    \n    for m in range(1, len(X_train)):\n        model.fit(X_train[:m], y_train[:m])\n        y_train_predict = model.predict(X_train[:m])\n        y_val_predict = model.predict(X_val) \n        train_errors.append(mean_squared_error(y_train[:m], y_train_predict)) \n        val_errors.append(mean_squared_error(y_val, y_val_predict))\n        \n    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\") \n    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")","0d60ce28":"sgd_reg_curves = SGDRegressor(penalty='l2') \n\nplot_learning_curves(sgd_reg_curves, X, y)","a086b160":"scores = cross_val_score(sgd_reg, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10) \n\nsgd_reg_scores = np.sqrt(-scores)","d6bd3c0b":"sgd_reg_scores","0aa37d1b":"def display_scores(scores):\n    print('Scores:', scores)\n    print('Std.  :', scores.std())\n    print('Mean  :', scores.mean())\n    \ndisplay_scores(sgd_reg_scores)","36b423b4":"## So, as a conclusion I must say, that LinearRegression with regularisation works fine","09046b6b":"#### Scaling numerical variables","3c8ee37f":"#### Estimating SGDRegressor's geberalization performance","decd8c10":"### A little bit about correlation. \n\n<p>Since the dataset is not too large we can easily compute standard correlation coefficient (also called Pearson\u2019s r) between every pair of attributes using the *corr()* method.\n    \nThe correlation coefficient ranges from \u20131 to 1. When it is close to 1, it means that there is a strong positive correlation; when the coefficient is close to \u20131, it means that there is a strong negative correlation.Finally, coefficients close to 0 mean that there is no linear correlation. <\/p>\n\n<p>The correlation coefficient only measures linear correlations (\u201cif x goes up, then y generally goes up\/down\u201d). It may completely miss out on nonlinear relationships (e.g., \u201cif x is close to 0, then y generally goes up\u201d)<\/p>\n\n#### Let\u2019s look at how much each *numerical* attributes correlates with *G3* value:","4a4e3ff1":"### All needed imports","5172ff88":"#### Predict and Target variables","5ee157e7":"### Loading and Exploring Data\n\nThere are two files of students performance in two subjects: math and portugues (Portugal is the country the dataset is from). Important notice : description (later on, as DESCR) tells that \"there are several (382) students that belong to both datasets\", so since data set about portugues is twice larger than about math lessons, I will be taking former.","511340da":"http:\/\/archive.ics.uci.edu\/ml\/datasets\/Student+Performance#","d0d34bf5":"### I have given it a lot of thoughts and here is what I'm thinking.\n#### The point of this notebook is to find *G3* , of course by selecting the best model and the best features for that. And we are visualizing, analysing these features, such as *traveltime* from home to school or possible drinking problems or romantic affairs, family statuses and so on and so on ... we are basically thinking of the things, that influence our grades. So, based on these thoughts, it would've been better to get rid off *G1* and *G2*, since these are grades for first and second halves of the year respectively. And they are, as much as *G3* reflections of the features choses. Instead of having three grades, we should make one mean *G* out of them.\n\n<code>\n    # mean\n    student_reduced[\"G\"]=(student_reduced[\"G1\"]+student_reduced[\"G2\"]+student_reduced[\"G3\"])\/3\n    # dropping initial grades and leaving mean \n    student_reduced.drop(['G1', 'G2', 'G3'], axis=1, inplace=True)\n<\/code>\n\n#### But for now, I will leave them be","5195fd80":"#### Apparently, *G3* has correlation not only with *G1* and *G2* but also with *studytime*, *failures*, *Dalc*, *Walc*, *traveltime*, *freetime*, *age*, *Medu* (mother's education) and *Fedu* (father's education)\n\n#### Another way to check for correlation between attributes is to use the pandas *scatter_matrix()* function, which plots every numerical attribute against every other numerical attribute. Since there are 16 numerical attributes, we would get 16x16 = 256 plots, which would not fit on a page\u2014so let\u2019s just focus on a few promising attributes that seem most correlated with *G3*","a08a5226":"### From what I can understand, looking at the Learning curve, the model is fine","128ff491":"#### Looking at the data we can see string-valued features. \n\n##### They are not arbitrary texts: these are a limited number of possible values, each of which represents a category. So these attributes are categorical attributes. Most Machine Learning algorithms prefer to work with numbers, so let\u2019s convert these categories from text to numbers. For this, we can use Scikit-Learn\u2019s OneHotEncoder class, because it's one of the best when working with categorical nominal variables. And for numerical values I will use StandardScaler. These two function I will put in one pipeline.\n\n<p>As far as I know, all but the last estimator must be transformers (i.e., they must have a fit_transform() method)<\/p>\n\n<code>\nfrom sklearn.preprocessing import OneHotEncoder\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.compose import ColumnTransformer\nfeatures_cat = ['sex','school','address','Mjob','Fjob','reason','schoolsup','guardian','higher','internet']\nfeatures_num = ['studytime', 'failures', 'Dalc', 'Walc', 'traveltime', 'freetime', 'Medu', 'Fedu']\n\nfull_pipeline = ColumnTransformer([\n    (\"num\", StandardScaler(), features_num), \n    (\"encoder\", OneHotEncoder(), features_cat),\n])\n\nX_train_prepared = full_pipeline.fit_transform(X_train)\n<\/code>\n\n### UPD: insted of this pipeline I thought of better way to transform my features. Anyways, for the sake of my experiments, I will be leaving the above discussed pipeline here in code-block:","61fb1ef2":"#### Before looking at the data any further, I need to create a test set, put it aside, and never look at it. (c) Aur\u00e9lien Geron","549b8dec":"### Accuracy of 0.86 is really good. \n\n#### But perhaps the model underfits or overfits.\n\n#### There are a few ways to find that out:\n<ul>\n    <li>Learning curves - these are plots of the model\u2019s performance on the training set and the validation set as a function of the training set size <\/li>\n    <li>Cross-validation - if a model performs well on the training data but generalizes poorly according to the cross-validation metrics, then your model is overfitting. If it performs poorly on both, then it is underfitting.<\/li>\n<\/ul>","d5a98855":"#### And yet another way to check numeric data for correlations","e5e53e9f":"#### After inspecting the dataset description I'm curious how *health* and *absences* values corelate. Perhaps, I could made one feature out of them. But before looking for correlation we should normalize these features, cause their ranges differ very much.\n\n### UPD: normalizing values didn't help. Seems that normalizing or not, nothing changes... I should look it up. Nonetheless, I leave the code in one cell below just as a reminder to myself","24d1d8d9":"### After examining boxplots, I've come to a conclusion that the following numerical and categorical features have an inpact on *G3* : \n\n\n<ul>\n    <li>Numerical: studytime, failures, Dalc, Walc, traveltime, freetime, Medu and Fedu, G1, G2<\/li>\n    <li>Categorical: Sex, School, Address, Mjob + FJob, Reason, Guardian, Schoolsup, Higher, Internet<\/li>\n<\/ul>\n\n<p>See dataset description for info about each feature <\/p>","c7a1c611":"### *get_dummies()* method from pandas yields every values from every categorical feature as a column name and assigns 1 to instances where this value is True and  0 to instances where it is not. This method affects only categorical features","0a319653":"#### Judging by this heatmap and also by previous correlations matrices, *studytime, failures, Dalc, Walc, traveltime, freetime, age, Medu and Fedu* might really have an impact on *G1-G3*\n\n### Let's now analyze categorical variables","74ccddc0":"#### Let's look at the results","905f0c74":"### Better Evaluation Using Cross-Validation","4b7273c4":"### Learning Curves","bef4fa44":"## Choosing features. The goal is to predict *G3*","1a48d8ef":"#### I know from DESCR, that *G1* and *G2* are grades for midterm exams, so they are a consequence of the last exam and they correlate a great deal with our target variable *G3*, so  I won't be making another column of average value for these three","982f2e4f":"#### I guess we have a sufficient number of instances in dataset for each stratum, so no need in *Stratified sampling*","12f63353":"### Selecting and Training the Model\n\n#### I'll try Linear Regression with regularization","a4668f26":"#### Another quick way to get a feel of the type of data we are dealing with is to plot a histogram for each numerical attribute. A histogram shows the number of instances (on the vertical axis) that have a given value range (on the horizontal axis). ","54cee578":"####  Scikit-Learn\u2019s cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better), so the scoring function is actually the opposite of the MSE (i.e., a negative value), which is why the preceding code computes -scores before calculating the square root."}}