{"cell_type":{"881041e0":"code","4c3e2d9b":"code","b4831ba7":"code","834c1f5d":"code","7d645e19":"code","24eadb55":"code","b025eea7":"code","a6d60308":"code","30c06b90":"code","f24ef676":"code","344e7f87":"code","aa53f5a6":"code","de182136":"code","d5bcd4a0":"code","50f251e8":"code","5b997595":"code","ea7e03b2":"code","f1d3eefd":"code","344caa9e":"code","80f91ab0":"code","c24e9909":"code","108f631c":"code","96f96696":"code","bc732f02":"code","2d0893ea":"code","6deb5dca":"code","c4ecb438":"code","6f56b143":"code","174e844a":"code","d1d8f097":"code","d9e7b119":"code","2d9f479e":"code","152d2f44":"code","f48cb1a2":"code","2d4f8bf9":"markdown","00110a4f":"markdown","cb6f53e4":"markdown","932066e8":"markdown","0e426abd":"markdown"},"source":{"881041e0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4c3e2d9b":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.decomposition import PCA # dimensionality reduction\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.metrics import median_absolute_error\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","b4831ba7":"data=pd.read_csv(\"..\/input\/logistics-ease-across-different-states-india-2021\/Leads Report 2021 Numeric Data.csv\",index_col='State wise scores of individual parameters').copy()","834c1f5d":"data.head()\n","7d645e19":"data.shape","24eadb55":"data.describe()","b025eea7":"data.columns","a6d60308":"data.info()","30c06b90":"data['Final Score']","f24ef676":"data_score=data['Final Score']\ndata=data.drop(['Final Score'],axis=1)\n","344e7f87":"data.head()","aa53f5a6":"sns.distplot(data['Capability of Logistics Service Providers'])\nfig = plt.figure()\nres = stats.probplot(data['Capability of Logistics Service Providers'], plot=plt)","de182136":"sns.scatterplot(x=data_score,y=data['Quality of Logistics Services'])","d5bcd4a0":"sns.scatterplot(x=data_score,y=data['Ease of Obtaining all Approvals '])","50f251e8":"data.corr()","5b997595":"f, ax = plt.subplots(figsize=(17,10))\nax=sns.heatmap(data,annot=True)\nplt.show","ea7e03b2":"#ppling PCA\npca = PCA(n_components=10)\nprincipalComponents = pca.fit_transform(data)\nprincipalDf = pd.DataFrame(data = principalComponents\n            , columns = ['PCA1', 'PCA2','PCA3','PCA4','PCA5','PCA6','PCA7','PCA78','PCA9','PCA10'],index=data.index)\n","f1d3eefd":"principalDf.head()","344caa9e":"kmeans = KMeans(n_clusters=4)\nkmeans.fit(principalDf)\n\nprincipalDf['PCA_SCORE'] = kmeans.predict(principalDf)","80f91ab0":"print(principalDf['PCA_SCORE'][:10])","c24e9909":"principalDf.head()","108f631c":"y_score=principalDf['PCA_SCORE']","96f96696":"print(y_score[1:10])","bc732f02":"print(data_score[1:10])","2d0893ea":"#Calculating Mean Squared Error\nMSEValue = mean_squared_error(y_score, data_score, multioutput='uniform_average') # it can be raw_values\nprint('Mean Squared Error Value is : ', MSEValue)","6deb5dca":"#Calculating Median Absolute Error\nMdSEValue = median_absolute_error(y_score, data_score)\nprint('Median Absolute Error Value is : ', MdSEValue )","c4ecb438":"# create dendrogram\nfigure,ax = plt.subplots(1,1,figsize=(15,9))\ndendrogram = sch.dendrogram(sch.linkage(data, method='complete'),labels=data.index,ax=ax)\n# create clusters\nhc = AgglomerativeClustering(n_clusters=4, affinity = 'euclidean', linkage = 'ward')\n# save clusters for chart\ny_hc = hc.fit_predict(data)","6f56b143":"y_hc","174e844a":"#Calculating Median Absolute Error\nMdSEValue = median_absolute_error(y_hc, data_score)\nprint('Median Absolute Error Value is : ', MdSEValue )","d1d8f097":"data.head()","d9e7b119":"data_score.head()","2d9f479e":"#Splitting data\n\nX_train, X_test, y_train, y_test = train_test_split(data, data_score, test_size=0.4, random_state=44, shuffle =True)\n\n#Splitted Data\nprint('X_train shape is ' , X_train.shape)\nprint('X_test shape is ' , X_test.shape)\nprint('y_train shape is ' , y_train.shape)\nprint('y_test shape is ' , y_test.shape)","152d2f44":"#Applying Ridge Regression Model \n\n'''\nsklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False,\n                           copy_X=True, max_iter=None, tol=0.001, solver='auto',\n                           random_state=None)\n'''\n\nRidgeRegressionModel = Ridge(alpha=.2,random_state=33)\nRidgeRegressionModel.fit(X_train, y_train)\n\n#Calculating Details\nprint('Ridge Regression Train Score is : ' , RidgeRegressionModel.score(X_train, y_train))\nprint('Ridge Regression Test Score is : ' , RidgeRegressionModel.score(X_test, y_test))\nprint('Ridge Regression Coef is : ' , RidgeRegressionModel.coef_)\nprint('Ridge Regression intercept is : ' , RidgeRegressionModel.intercept_)\n#print('----------------------------------------------------')\n\n#Calculating Prediction\ny_pred = RidgeRegressionModel.predict(X_test)\nprint('Predicted Value for Ridge Regression is : ' , y_pred[:10])","f48cb1a2":"#Applying SGDRegressor Model \n\n'''\nsklearn.linear_model.SGDRegressor(loss='squared_loss\u2019, penalty=\u2019l2\u2019, alpha=0.0001,\n                                  l1_ratio=0.15, fit_intercept=True, max_iter=None,\n                                  tol=None, shuffle=True, verbose=0, epsilon=0.1,\n                                  random_state=None, learning_rate='invscaling\u2019,\n                                  eta0=0.01, power_t=0.25, early_stopping=False,\n                                  validation_fraction=0.1, n_iter_no_change=5,\n                                  warm_start=False, average=False, n_iter=None)\n'''\n\nSGDRegressionModel = SGDRegressor(alpha=.004,random_state=33,penalty='l2',loss = 'huber')\nSGDRegressionModel.fit(X_train, y_train)\n\n#Calculating Details\nprint('SGD Regression Train Score is : ' , SGDRegressionModel.score(X_train, y_train))\nprint('SGD Regression Test Score is : ' , SGDRegressionModel.score(X_test, y_test))\nprint('SGD Regression Coef is : ' , SGDRegressionModel.coef_)\nprint('SGD Regression intercept is : ' , SGDRegressionModel.intercept_)\n#print('----------------------------------------------------')\n\n#Calculating Prediction\ny_pred = SGDRegressionModel.predict(X_test)\nprint('Predicted Value for SGD Regression is : ' , y_pred[:10])","2d4f8bf9":"# conclogn","00110a4f":"**import librey","cb6f53e4":"read data","932066e8":"sobervesd Machine ","0e426abd":"The thought of using learning Unsopervision was absolutely bad. Most of the results were disappointing, but the results of science with supervision were somewhat favorable to me."}}