{"cell_type":{"6fc74bd7":"code","4e149a5e":"code","33ec5975":"code","690df1e6":"code","ffb3b5c8":"code","1e9479f9":"code","97a8b54f":"code","4627bedf":"code","148f653d":"code","cf0a7451":"code","81f952eb":"code","bf1fe729":"code","39518ee1":"code","59bdd5cf":"code","99c4c2ed":"code","af6ba2ac":"code","15b99f45":"code","01199922":"code","3feffbd6":"code","a54bf069":"code","040cd8a2":"code","37323bac":"markdown","c8179f75":"markdown","47e49cb3":"markdown","d73f56a3":"markdown","a6c54cdc":"markdown","0de909a3":"markdown","7c92992c":"markdown","4a0b6171":"markdown","267dd23f":"markdown","9f3eb46c":"markdown","a50e6c11":"markdown","3fd75a41":"markdown","b5b94b25":"markdown"},"source":{"6fc74bd7":"!pip -q install timm\n!pip -q install pytorch_lightning","4e149a5e":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau, OneCycleLR\nfrom torch.optim.optimizer import Optimizer\nimport torchvision.utils as vutils\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import seed_everything\nfrom pytorch_lightning.metrics.functional import accuracy, f1, auroc\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\nimport timm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2","33ec5975":"SEED = 42\nN_FOLDS = 5\nTRAIN_FOLD = 0\nTARGET_COL = 'target'\nN_EPOCHS = 10\nBATCH_SIZE = 32\nIMG_SIZE = 640\nLR = 1e-4\nMAX_LR = 5e-4\nPRECISION = 16\nMODEL = 'efficientnet_b0'","690df1e6":"def set_seed(seed = int):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random_state = np.random.RandomState(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    seed_everything(seed)\n    return random_state\n\n\nrandom_state = set_seed(SEED)","ffb3b5c8":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"GPU is available.\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, going to use CPU instead.\")","1e9479f9":"train = pd.read_csv('..\/input\/seti-breakthrough-listen\/train_labels.csv')\ntest = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')\n\ndef get_train_file_path(image_id):\n    return \"..\/input\/seti-breakthrough-listen\/train\/{}\/{}.npy\".format(image_id[0], image_id)\n\ndef get_test_file_path(image_id):\n    return \"..\/input\/seti-breakthrough-listen\/test\/{}\/{}.npy\".format(image_id[0], image_id)\n\ntrain['file_path'] = train['id'].apply(get_train_file_path)\ntest['file_path'] = test['id'].apply(get_test_file_path)\n\ndisplay(train.sample(5))","97a8b54f":"dist = train.target.map({0:'haystack', 1:'needle'})\ndist = dist.value_counts()\nfig = px.pie(dist,\n             values='target',\n             names=dist.index,\n             hole=.4,title=\"Target Distribution\")\nfig.update_traces(textinfo='percent+label', pull=0.05)\nfig.show()","4627bedf":"plt.figure(figsize=(12, 8))\nfor i in range(10):\n    if i < 4:\n        image = np.load(train.loc[i, 'file_path'])\n    else:\n        image = np.load(train[train['target']==1].reset_index().loc[i, 'file_path'])\n    image = image.astype(np.float32)\n    image = np.vstack(image).T\n    plt.subplot(5, 2, i + 1)\n    plt.imshow(image)\nplt.suptitle(\"Raw Training Data\")\nplt.tight_layout()","148f653d":"class TrainDataset(Dataset):\n    def __init__(self, df, test=False, transform=None):\n        self.df = df\n        self.test = test\n        self.file_names = df['file_path'].values\n        if not self.test:\n            self.labels = df[TARGET_COL].values\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_path = self.file_names[idx]\n        \n        image = np.load(file_path)[[0, 2, 4]] \n        image = image.astype(np.float32)\n        image = np.vstack(image).T \n        if self.transform:\n            image = self.transform(image=image)['image']\n        else:\n            image = image[np.newaxis,:,:]\n            image = torch.from_numpy(image).float()\n        if not self.test:\n            label = torch.unsqueeze(torch.tensor(self.labels[idx]).float(),-1)\n            return image, label\n        else:\n            return image","cf0a7451":"def get_transforms(*, data):\n    \n    if data == 'train':\n        return A.Compose([\n            A.Resize(IMG_SIZE,\n                    IMG_SIZE),\n            A.VerticalFlip(p=0.5),\n            A.HorizontalFlip(p=0.5),\n            ToTensorV2(),\n        ])\n\n    elif data == 'valid':\n        return A.Compose([\n            A.Resize(IMG_SIZE, IMG_SIZE),\n            ToTensorV2(),\n        ])","81f952eb":"def train_dataloader():\n    return DataLoader(train_dataset, batch_size=4, num_workers=2,\n                      drop_last=False, shuffle=False, pin_memory=True)\ntrain_dataset = TrainDataset(train[train['target']==0], transform=None)\nbatch, targets = next(iter(train_dataloader()))\n\nplt.figure(figsize=(16, 16))\nplt.axis(\"off\")\nplt.title(\"Target = 0\")\nplt.imshow(vutils.make_grid(\n    batch, nrow=1, padding=10, normalize=True).permute(1,2,0).cpu().numpy())\nplt.show()","bf1fe729":"train_dataset = TrainDataset(train[train['target']==1], transform=None)\nbatch, targets = next(iter(train_dataloader()))\n\nplt.figure(figsize=(16, 16))\nplt.axis(\"off\")\nplt.title(\"Target = 1\")\nplt.imshow(vutils.make_grid(\n    batch, nrow=1, padding=10, normalize=True).permute(1,2,0).cpu().numpy())\nplt.show()","39518ee1":"class DataModule(pl.LightningDataModule):\n\n    def __init__(self, train_df, val_df, test_df,  batch_size=8):\n        super().__init__()\n        self.batch_size = batch_size\n        self.train_df = train_df\n        self.val_df = val_df\n        self.test_df = test_df\n\n    def setup(self, stage=None):\n        self.train_dataset = TrainDataset(\n          self.train_df,\n          transform=get_transforms(data='train')\n            \n        )\n        \n        self.val_dataset = TrainDataset(          \n          self.val_df,\n          transform=get_transforms(data='valid')\n        )\n\n        self.test_dataset = TrainDataset(\n          self.test_df,\n          transform=get_transforms(data='valid'),\n          test = True\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n          self.train_dataset,\n          batch_size=self.batch_size,\n          shuffle=True,\n          num_workers=4,\n          pin_memory=True\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n          self.val_dataset,\n          batch_size=self.batch_size,\n          num_workers=4,\n          pin_memory=True\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n          self.test_dataset,\n          batch_size=self.batch_size,\n          num_workers=4,\n          pin_memory=True\n        )","59bdd5cf":"def mixup_data(x, y, alpha=1.0):\n    '''Returns mixed inputs, pairs of targets, and lambda'''\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n\n    index = torch.randperm(batch_size)\n\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)","99c4c2ed":"import math\nfrom typing import TYPE_CHECKING, Any, Callable, Optional\n\nif TYPE_CHECKING:\n    from torch.optim.optimizer import _params_t\nelse:\n    _params_t = Any\n\nclass MADGRAD(Optimizer):\n\n    def __init__(\n        self, params: _params_t, lr: float = 1e-2, momentum: float = 0.9, weight_decay: float = 0, eps: float = 1e-6,\n    ):\n        if momentum < 0 or momentum >= 1:\n            raise ValueError(f\"Momentum {momentum} must be in the range [0,1]\")\n        if lr <= 0:\n            raise ValueError(f\"Learning rate {lr} must be positive\")\n        if weight_decay < 0:\n            raise ValueError(f\"Weight decay {weight_decay} must be non-negative\")\n        if eps < 0:\n            raise ValueError(f\"Eps must be non-negative\")\n\n        defaults = dict(lr=lr, eps=eps, momentum=momentum, weight_decay=weight_decay)\n        super().__init__(params, defaults)\n\n    @property\n    def supports_memory_efficient_fp16(self) -> bool:\n        return False\n\n    @property\n    def supports_flat_params(self) -> bool:\n        return True\n\n    def step(self, closure: Optional[Callable[[], float]] = None) -> Optional[float]:\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        if 'k' not in self.state:\n            self.state['k'] = torch.tensor([0], dtype=torch.long)\n        k = self.state['k'].item()\n\n        for group in self.param_groups:\n            eps = group[\"eps\"]\n            lr = group[\"lr\"] + eps\n            decay = group[\"weight_decay\"]\n            momentum = group[\"momentum\"]\n\n            ck = 1 - momentum\n            lamb = lr * math.pow(k + 1, 0.5)\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                state = self.state[p]\n\n                if \"grad_sum_sq\" not in state:\n                    state[\"grad_sum_sq\"] = torch.zeros_like(p.data).detach()\n                    state[\"s\"] = torch.zeros_like(p.data).detach()\n                    if momentum != 0:\n                        state[\"x0\"] = torch.clone(p.data).detach()\n\n                if momentum != 0.0 and grad.is_sparse:\n                    raise RuntimeError(\"momentum != 0 is not compatible with sparse gradients\")\n\n                grad_sum_sq = state[\"grad_sum_sq\"]\n                s = state[\"s\"]\n\n                # Apply weight decay\n                if decay != 0:\n                    if grad.is_sparse:\n                        raise RuntimeError(\"weight_decay option is not compatible with sparse gradients\")\n\n                    grad.add_(p.data, alpha=decay)\n\n                if grad.is_sparse:\n                    grad = grad.coalesce()\n                    grad_val = grad._values()\n\n                    p_masked = p.sparse_mask(grad)\n                    grad_sum_sq_masked = grad_sum_sq.sparse_mask(grad)\n                    s_masked = s.sparse_mask(grad)\n\n                    # Compute x_0 from other known quantities\n                    rms_masked_vals = grad_sum_sq_masked._values().pow(1 \/ 3).add_(eps)\n                    x0_masked_vals = p_masked._values().addcdiv(s_masked._values(), rms_masked_vals, value=1)\n\n                    # Dense + sparse op\n                    grad_sq = grad * grad\n                    grad_sum_sq.add_(grad_sq, alpha=lamb)\n                    grad_sum_sq_masked.add_(grad_sq, alpha=lamb)\n\n                    rms_masked_vals = grad_sum_sq_masked._values().pow_(1 \/ 3).add_(eps)\n\n                    s.add_(grad, alpha=lamb)\n                    s_masked._values().add_(grad_val, alpha=lamb)\n\n                    # update masked copy of p\n                    p_kp1_masked_vals = x0_masked_vals.addcdiv(s_masked._values(), rms_masked_vals, value=-1)\n                    # Copy updated masked p to dense p using an add operation\n                    p_masked._values().add_(p_kp1_masked_vals, alpha=-1)\n                    p.data.add_(p_masked, alpha=-1)\n                else:\n                    if momentum == 0:\n                        # Compute x_0 from other known quantities\n                        rms = grad_sum_sq.pow(1 \/ 3).add_(eps)\n                        x0 = p.data.addcdiv(s, rms, value=1)\n                    else:\n                        x0 = state[\"x0\"]\n\n                    # Accumulate second moments\n                    grad_sum_sq.addcmul_(grad, grad, value=lamb)\n                    rms = grad_sum_sq.pow(1 \/ 3).add_(eps)\n\n                    # Update s\n                    s.data.add_(grad, alpha=lamb)\n\n                    # Step\n                    if momentum == 0:\n                        p.data.copy_(x0.addcdiv(s, rms, value=-1))\n                    else:\n                        z = x0.addcdiv(s, rms, value=-1)\n\n                        # p is a moving average of z\n                        p.data.mul_(1 - ck).add_(z, alpha=ck)\n\n\n        self.state['k'] += 1\n        return loss","af6ba2ac":"class Predictor(pl.LightningModule):\n\n    def __init__(self, n_classes: int, n_training_steps=None, steps_per_epoch=None):\n        super().__init__()\n        self.n_classes = n_classes\n        self.model = timm.create_model(MODEL, pretrained=True, in_chans=1)\n        self.n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Linear(self.n_features, n_classes)\n        self.n_training_steps = n_training_steps\n        self.steps_per_epoch = steps_per_epoch\n\n    def forward(self, x):\n        output = self.model(x)\n        return output\n\n\n    def training_step(self, batch, batch_idx):\n\n        x, y = batch\n        \n        x, y_a, y_b, lam = mixup_data(x, y.view(-1, 1))\n        \n        output = self(x)\n        labels = y\n        loss = mixup_criterion(criterion, output, y_a, y_b, lam)\n        try:\n            auc=roc_auc_score(labels.detach().cpu(), output.sigmoid().detach().cpu())        \n            self.log(\"auc\", auc, prog_bar=True, logger=True)\n        except:\n            pass\n        return {\"loss\": loss, \"predictions\": output, \"labels\": labels}\n\n    def training_epoch_end(self, outputs):\n\n        preds = []\n        labels = []\n        \n        for output in outputs:\n          \n          preds += output['predictions']\n          labels += output['labels']\n\n        labels = torch.stack(labels)\n        preds = torch.stack(preds)\n\n\n        train_auc=roc_auc_score(labels.detach().cpu(), preds.sigmoid().detach().cpu())\n        self.log(\"mean_train_auc\", train_auc, prog_bar=True, logger=True)\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        output = self(x)\n        labels = y\n        loss = criterion(output, y)\n        self.log('val_loss', loss, prog_bar=True, logger=True)\n        return {\"predictions\": output, \"labels\": labels}\n      \n\n    def validation_epoch_end(self, outputs):\n\n        preds = []\n        labels = []\n        \n        for output in outputs:\n          \n          preds += output['predictions']\n          labels += output['labels']\n\n        labels = torch.stack(labels)\n        preds = torch.stack(preds)\n\n        val_auc=roc_auc_score(labels.detach().cpu(), preds.sigmoid().detach().cpu())\n        self.log(\"val_auc\", val_auc, prog_bar=True, logger=True)\n        \n\n    def test_step(self, batch, batch_idx):\n        x = batch        \n        output = self(x).sigmoid()\n        return output   \n\n\n\n    def configure_optimizers(self):\n\n        optimizer = MADGRAD(self.parameters(), lr=LR)\n\n        scheduler = OneCycleLR(\n          optimizer,\n          epochs = N_EPOCHS,\n          max_lr = MAX_LR,\n          total_steps = self.n_training_steps,\n          steps_per_epoch = self.steps_per_epoch\n        )\n\n        return dict(\n          optimizer=optimizer,\n          lr_scheduler=scheduler\n        )","15b99f45":"t_steps_per_epoch=(len(train)\/\/ N_EPOCHS) \/\/ BATCH_SIZE\ntotal_training_steps = t_steps_per_epoch * N_EPOCHS\ncriterion=nn.BCEWithLogitsLoss()","01199922":"dummy_model = nn.Linear(2, 1)\n\noptimizer = MADGRAD(params=dummy_model.parameters(), lr=LR)\n\n\n\nscheduler = OneCycleLR(\n          optimizer,\n          epochs = N_EPOCHS,\n          max_lr = MAX_LR,\n          total_steps = total_training_steps,\n          steps_per_epoch = t_steps_per_epoch\n        )\n\nlearning_rate_history = []\n\nfor step in range(total_training_steps):\n\n    optimizer.step()\n\n    scheduler.step()\n\n    learning_rate_history.append(optimizer.param_groups[0]['lr'])\n\nplt.plot(learning_rate_history, label=\"Learning Rate\")\n\nplt.axvline(x=total_training_steps*0.3, color=\"red\", linestyle=(0, (2, 4)), label=\"Warmup\")\n\nplt.legend()\n\nplt.xlabel(\"Step\")\n\nplt.ylabel(\"Learning rate\")\n\nplt.tight_layout()","3feffbd6":"skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\ntrain[\"fold\"] = -1\nfor fold_id, (_, val_idx) in enumerate(skf.split(train[\"id\"], train[\"target\"])):\n    train.loc[val_idx, \"fold\"] = fold_id\n    \n    \nmodel = Predictor(steps_per_epoch=t_steps_per_epoch,\n                  n_training_steps=total_training_steps ,\n                  n_classes=1\n    )\n\n\ndata_module = DataModule(\n  train[train['fold']!=TRAIN_FOLD], # train fold\n  train[train['fold']==TRAIN_FOLD], # val fold\n  train[train['fold']==TRAIN_FOLD], # test data, same as val for now\n  batch_size=BATCH_SIZE,\n)\nearly_stopping_callback = EarlyStopping(monitor='val_auc',mode=\"max\", patience=2)\ncheckpoint_callback = ModelCheckpoint(\n  #dirpath=\"checkpoints\",\n  #filename=\"best-checkpoint-ep{epoch:02d}-{val_auc:.3f}\",\n  save_top_k=N_EPOCHS,\n  verbose=True,\n  monitor=\"val_auc\",\n  mode=\"max\"\n)\ntrainer = pl.Trainer(\n  checkpoint_callback=checkpoint_callback,\n  callbacks=[early_stopping_callback],\n  max_epochs=N_EPOCHS,\n  gpus=1,\n  precision=PRECISION,\n  progress_bar_refresh_rate=5\n)\ntrainer.fit(model, data_module)","a54bf069":"predictions = []\ntrained_model = Predictor.load_from_checkpoint(\n  trainer.checkpoint_callback.best_model_path,\n  n_classes=1\n)\ntrained_model.eval()\ntrained_model.freeze()\ntrained_model = trained_model.to(device)\nval_dataset = TrainDataset(\n      test,\n      transform=get_transforms(data='valid'),\n      test = True\n    )\ndataloader=DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)\nfor item in tqdm(dataloader, position=0, leave=True):\n    prediction = trained_model(item.to(device))\n    predictions.append(prediction.flatten().sigmoid())\npredictions = torch.cat(predictions).detach().cpu()\nfinal_preds=predictions.squeeze(-1).numpy()","040cd8a2":"submission = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')\nsubmission['target'] = final_preds\nsubmission.to_csv('sub.csv', index=False)\nsubmission.head()","37323bac":"### That all folks, I hope you find it useful. I might have missed some points like markdowns in general or some code parts,I'm planning to fill those parts ehenever I have free time. Anyways if you have any questions you can leave a comment below and if you liked the notebook please don't forget to upvote. Happy coding!","c8179f75":"### Let's check if our dataloaders are working fine...","47e49cb3":"# Taking First Look into Data","d73f56a3":"# Model","a6c54cdc":"# Data Module","0de909a3":"# Training","7c92992c":"# Owerview of the Competition:\n## **Task:**\n\nIn this competition we are tasked with looking for technosignature signals in cadence snippets taken from the Green Bank Telescope (GBT)\n\n## **What do we have at our hands:**\n\n* **train** - a training set of cadence snippet files stored in numpy float16 format, one file per cadence snippet id, with corresponding labels found in the train_labels.csv file. Each file has dimension (6, 273, 256), with the 1st dimension representing the 6 positions of the cadence, and the 2nd and 3rd dimensions representing the 2D spectrogram.\n* **test**- the test set cadence snippet files; you must predict whether or not the cadence contains a \"needle\", which is the target for this competition.\n* **sample_submission.csv** - a sample submission file in the correct format.\n* **train_labels** - targets corresponding (by id) to the cadence snippet files found in the train\/ folder.\n","4a0b6171":"# Config","267dd23f":"# Mixup","9f3eb46c":"# Inference","a50e6c11":"# Dataset","3fd75a41":"# MADGRAD","b5b94b25":"# Submission"}}