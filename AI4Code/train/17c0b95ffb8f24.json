{"cell_type":{"fd033223":"code","af687925":"code","76538fb5":"code","61b0bca1":"code","6d0a0d21":"code","894a364e":"code","904a9d6f":"code","bb4b6277":"code","7acfdb2e":"code","037b0c08":"code","701d8f3a":"code","53525872":"code","a902b03c":"code","949ddb7d":"code","f9a5ae3e":"code","d4a44281":"code","b7210b22":"code","3e1dad85":"code","b198112d":"code","0cb003ec":"code","3c90ad39":"markdown","10b89f15":"markdown","8d4b5175":"markdown","b70d1c7a":"markdown","de70052a":"markdown","be2dc955":"markdown","e94b2e4b":"markdown","dabcd215":"markdown","4be22642":"markdown","8d80dbe8":"markdown","80e9e832":"markdown","31e580bf":"markdown","e61280f3":"markdown"},"source":{"fd033223":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","af687925":"plt.style.use('seaborn')\nplt.rcParams['figure.figsize'] = 12, 7","76538fb5":"from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()","61b0bca1":"(market_train_df, news_train_df) = env.get_training_data()","6d0a0d21":"del news_train_df","894a364e":"df = (\n    market_train_df.\n    reset_index().\n    sort_values(['assetCode', 'time']).\n    set_index(['assetCode','time'])\n)\n\ndf['implied_mkt_return'] = (\n    df.\n    groupby('assetCode').\n    apply(lambda x: x.returnsClosePrevRaw1 - x.returnsClosePrevMktres1).\n    reset_index(0, drop=True)\n)","904a9d6f":"plt.scatter(\n    df.loc['AAPL.O'].implied_mkt_return,\n    df.loc['NFLX.O'].implied_mkt_return,\n    alpha=0.6\n);\nplt.xlabel('Implied Market Return from AAPL');\nplt.ylabel('Implied Market Return from NFLX');","bb4b6277":"returnsClosePrevRaw1 = (\n    df['returnsClosePrevRaw1'].\n    swaplevel().\n    unstack()\n)\n\nreturnsClosePrevMktres1 = (\n    df['returnsClosePrevMktres1'].\n    swaplevel().\n    unstack()\n)","7acfdb2e":"num_days = 260*5  # Take 5 years\nnum_stocks = 200  # Try for 200 stocks but we will get many less due to NaNs\n\nreturnsClosePrevRaw1 = \\\n    returnsClosePrevRaw1.iloc[-num_days:, 0:num_stocks].dropna(axis=1)\nnum_stocks = len(returnsClosePrevRaw1.columns)\nprint(num_stocks)","037b0c08":"returnsClosePrevMktres1 = (\n    returnsClosePrevMktres1.\n    loc[returnsClosePrevRaw1.index][returnsClosePrevRaw1.columns].\n    clip(lower=-0.15, upper=0.15)\n)","701d8f3a":"from sklearn.decomposition import PCA\npca = PCA(n_components=15, svd_solver='full')\npca.fit(returnsClosePrevRaw1)","53525872":"plt.bar(range(15),pca.explained_variance_ratio_);\nplt.title('Principal Components Sorted by Variance Explain');\nplt.ylabel('% of Total Variance Explained');\nplt.xlabel('PC factor number');","a902b03c":"pcs = pca.transform(returnsClosePrevRaw1)","949ddb7d":"# It's always tricky keeping the dimensions right, so I am going to print them for reference.\nprint(num_stocks)\nprint(num_days)\nprint(np.shape(pca.components_))\nprint(np.shape(pcs))","f9a5ae3e":"# the market return is the first PC\nmkt_return = pcs[:,0].reshape(num_days,1)\n\n# the betas of each stock to the market return are in\n# the first column of the components\nmkt_beta = pca.components_[0,:].reshape(num_stocks,1)\n\n# the market portion of returns is the projection of one onto the other\nmkt_portion = mkt_beta.dot(mkt_return.T).T\n\n# ...and the residual is just the difference\nresidual = returnsClosePrevRaw1 - mkt_portion","d4a44281":"print(mkt_return.shape)\nprint(mkt_portion.shape)","b7210b22":"from sklearn.covariance import LedoitWolf\n\ndef get_corr_from_cov(covmat):\n    d = np.diag(np.sqrt(np.diag(lw.covariance_)))\n    return np.linalg.inv(d).dot(lw.covariance_).dot(np.linalg.inv(d))\n\nlw = LedoitWolf()\n\nlw.fit(returnsClosePrevMktres1)\ncorr = get_corr_from_cov(lw.covariance_)\n\nlw.fit(residual)\ncorr2 = get_corr_from_cov(lw.covariance_)","3e1dad85":"from scipy.spatial import distance\nfrom scipy.cluster import hierarchy\n\ndef plot_side_by_side_hm(corr, corr2, title1, title2):\n    row_linkage = hierarchy.linkage(\n        distance.pdist(corr), method='average')\n    row_order = list(map(int, hierarchy.dendrogram(row_linkage, no_plot=True)['ivl']))\n    \n    col_linkage = hierarchy.linkage(\n        distance.pdist(corr.T), method='average')\n    col_order = list(map(int, hierarchy.dendrogram(col_linkage, no_plot=True)['ivl']))\n    \n    corr_swapped = np.copy(corr)\n    corr_swapped[:, :] = corr_swapped[row_order, :]\n    corr_swapped[:, :] = corr_swapped[:, col_order]\n\n    corr_swapped2 = np.copy(corr2)\n    corr_swapped2[:, :] = corr_swapped2[row_order, :]\n    corr_swapped2[:, :] = corr_swapped2[:, col_order]\n\n    fig, (ax1, ax2) = plt.subplots(ncols=2)\n    fig.tight_layout()\n    cs1 = sns.heatmap(corr_swapped, square=True, xticklabels=False, yticklabels=False, cbar=False, ax=ax1, cmap='OrRd')\n    cs1.set_title(title1)\n    cs2 = sns.heatmap(corr_swapped2, square=True, xticklabels=False, yticklabels=False, cbar=False, ax=ax2, cmap='OrRd')\n    cs2.set_title(title2);\n\nplot_side_by_side_hm(\n    corr,\n    corr2,\n    'Hierarchical Correlation Matrix: returnsClosePrevMktres1',\n    'Correlation Matrix: Our Residual Est (mapped to <-- hierarchy)'\n)","b198112d":"lw.fit(returnsClosePrevRaw1)\ncorr = get_corr_from_cov(lw.covariance_)\n\nlw.fit(returnsClosePrevMktres1)\ncorr2 = get_corr_from_cov(lw.covariance_)","0cb003ec":"plot_side_by_side_hm(\n    corr,\n    corr2,\n    'Hierarchical Correlation Matrix: returnsClosePrevRaw1',\n    'Correlation Matrix: returnsClosePrevMktres1 (mapped to <-- hierarchy)'\n)","3c90ad39":"## Is it the CAPM beta-adjusted return?\nAlternatively, I suspect that this adjustment is just a [CAPM $\\beta$](https:\/\/en.wikipedia.org\/wiki\/Beta_(finance). In other words,\n\n$$mktres = raw - \\beta r_{\\text{market}}$$\n\nHow can we check this? First, let's extract dataframes, T rows of dates by N columns of stocks, for the `returnsClosePrevRaw1` and `returnsClosePrevMktres`.","10b89f15":"# EDA: What does Mktres mean?\nBy @marketneutral\n\nThere have been a few Discussion Forum questions about what transformation is used to go from `Raw` to `Mktres` returns. Using just the data, let's see if we can figure this out. ","8d4b5175":"We can see that the implied market returns here for AAPL and NFLX are **not the same**. As such, the adjustment is **not** simply the difference to market return. If the implied market returns were the same for each stock, then this plot would just be a straight line.","b70d1c7a":"## Is it just the raw minus the market return?\nOne possibility is that the `Raw` return simply subtracts the \"market return\" for the day. If that's true then the implied market return will be the same for each stock for each day. We can check this easily as:","de70052a":"This is my first Kaggle kernel! I hope you liked it. ","be2dc955":"Because not all assets exist for all days, there will be many columns with `NaN` in them for long periods. For this EDA, I need complete returns. As such, I pick a segment of time, and only take stocks that have complete data over that time.","e94b2e4b":"And lastly for the EDA fun. Of course our residuals will not match `returnsCloseMktres1` exactly even if it were known for sure that there is a one factor model at play. There are many ways to calculate a market beta and many assumptions to make (not the least of which is the lookback period in days for the regression). We are after a general view though of what's going on. Hence we will do the following: compare the correlation matrices of `returnsCloseMktres1` with our residuals and see if they **have the same structure**. If they do, I would argue that the `Mktres` adjustment is just a single factor beta adjustment. If they don't, what else could be happening? Well, perhaps there is a multi-factor beta adjustment (e.g., in our case we could use more than 1 PC). Let's see.\n\nTo calculate the correlation matrices, I use the `LedoitWolf` estimator in `sklearn.covariance` which applies Bayesian shrinkage to reduce esitmation error. The extraction of the correlation matrix from the covariance matrix is an exercise in linear algebra.","dabcd215":"Now let's extract the implied market return and then calculate **our own residualized returns**. We will compare these to `returnsCloseMktres1`.","4be22642":"I'll sync up these two DataFrames and clip outliers at some large numbers (large for what daily returns should be).","8d80dbe8":"### Hierarchical Plot","80e9e832":"\n**The two correlations matrices appear to be roughly equivalent.** Conclusion: the `Mktres` adustment is the subtraction of a per-stock beta adjusted market return from the `raw` return.\n\n\nYou might wonder what the correlation matrices would look like if they were **not** implying equivalence. For fun, we can compare the correlation matrices of the `returnsPrevCloseRaw1` and `returnsPrevCloseMktres` and see.[](http:\/\/)","31e580bf":"Let's take a look and see what the PCA reduction found. As we see, there is one big feature and we will assume this is the **market** factor. The first PC explains almost 30% of the total variance.","e61280f3":"### Extracting the hidden market return\n\nWe can use `scikit-learn` *Principal Components Analysis* to extract the latent features in the returns data. One accepted stylized fact about stock markets is the first PC (i.e., the PC which explains the most variance) is the **market**. For the following we will\n\n- Fit the PCA model to the `returnsClosePrevRaw1`\n- Project the PCs on the returns to extract the time series of the hidden market factor\n- Calcualte *our own* residual return, `residual`\n- And lasty, plot the *heirarchical correlation matrix* of `returnsClosePrevMktres1` and compare that to the hierarchical correlation matrix of `residual`\n- If these two are similar, then we can conclude that the `returnsClosePrevMktres1` is just the single factor beta-adjusted return.\n\n### Sidebar: why does PCA apply here?\n\nPCA finds an axis rotation that has maximum variance explanation. In this case, we want to find out if there are significant common drivers of stock returns across the universe of stocks. There is a great StackOverflow post [here](https:\/\/stats.stackexchange.com\/questions\/2691\/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues) that talks about PCA in general and incudes one of my favorite all time GIFs: searching and finding PCA factors in 2 dimensions:\n\n<img src=\"https:\/\/i.stack.imgur.com\/lNHqt.gif\">"}}