{"cell_type":{"fd899d43":"code","bd09312b":"code","0a69a359":"code","ebc21d53":"code","8ab6710d":"code","a76a2e17":"code","fb5743e4":"code","9be29646":"code","1e8bafa5":"code","7e29bfb7":"code","8b409fed":"code","99873220":"code","aefb5dec":"code","c0fd0dc4":"code","415157a9":"code","3d000e74":"code","73151b08":"code","4242725e":"code","5b3943dd":"code","e6deba54":"code","ed433760":"code","927077a9":"code","547f72c6":"code","9120c42a":"code","92ede0c4":"code","7dab47ef":"code","bcc759c5":"code","b60770ff":"code","30c12e9b":"code","b3fe7b41":"code","73c114cd":"code","fa7ef9a8":"code","85423b2d":"code","776a08b7":"code","72fb0613":"code","1c1bdcc9":"code","3e84610f":"code","00d74036":"code","fdbd6827":"code","36f09109":"code","ef10a9ef":"code","128acfd8":"code","622a2880":"code","5dd4a51c":"code","dd2fba6e":"markdown","ee73c3a5":"markdown","99a630e1":"markdown","0204fe6f":"markdown","43611e72":"markdown","4af5522f":"markdown","c8cd921f":"markdown","7eb2f3a0":"markdown","51937c9a":"markdown","539e1780":"markdown","d0d19500":"markdown","383debea":"markdown","8f38d67a":"markdown","d6f5f311":"markdown","197546fa":"markdown","e6aa70cb":"markdown","cd644c9f":"markdown","6d1d06b6":"markdown","48d64d29":"markdown","b1a95705":"markdown","16fbbaa5":"markdown","f884d532":"markdown","a823ff73":"markdown","773e6ee2":"markdown","90554513":"markdown","91a517cc":"markdown","f48ffe9b":"markdown","4d7533dd":"markdown","7f18e810":"markdown","7cf7e308":"markdown","a8ff25af":"markdown","bdcc42e9":"markdown","15515a81":"markdown","50567559":"markdown","9d281ddf":"markdown","3ed8f4d5":"markdown","481e09c5":"markdown","7e0ba11e":"markdown","5c9e693a":"markdown","56545476":"markdown","17e6870b":"markdown","4c46065a":"markdown","0721cf27":"markdown","561db6a9":"markdown","460735c2":"markdown","0260ab6b":"markdown","2099e5af":"markdown","94f0b630":"markdown","77d64c34":"markdown","ab4b697f":"markdown","7a579c19":"markdown","bf4ab5a0":"markdown","943b75ef":"markdown","aae3c4c3":"markdown"},"source":{"fd899d43":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bd09312b":"# Main\nimport pandas as pd\nimport numpy as np\nimport re\nimport pickle\nfrom collections import Counter\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\n# For naive bayes\nfrom sklearn.base import BaseEstimator\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline","0a69a359":"# Reading the dataset with no columns titles and with latin encoding \ndf_raw = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding = \"ISO-8859-1\", header=None)\n\n # As the data has no column titles, we will add our own\ndf_raw.columns = [\"sentiment\", \"time\", \"date\", \"query\", \"username\", \"tweet\"]\n\n# Show the first 10 rows of the dataframe.\ndf_raw.head()","ebc21d53":"# Ommiting every column except for the text and the label, as we won't need any of the other information\ndf = df_raw[['sentiment', 'tweet']]\n\n# Replacing the label 4 with 1.\ndf['sentiment'] = df['sentiment'].replace(4,1)\n\ndf.head(10)","8ab6710d":"# Checking the data's output balance\n# Label '4' denotes positive sentiment and '0' denotes negative sentiment\nax = df.groupby('sentiment').count().plot(kind='bar', title='Distribution of data',\n                                               legend=False)\nax = ax.set_xticklabels(['Negative','Positive'], rotation=0)","a76a2e17":"trim_df = False # If you set this to true -> trim dataframe to 1\/80 for efficiency \n\nis_trimmed = False # This should always be initialized to false. Will be set to true if trimming occurs \n\nif trim_df:\n    print(\"Trimming the dataset to 1\/80\")\n    print(\"Nr rows before trim:\", len(df))\n    df_pos = df[df['sentiment'] == 1]\n    df_neg = df[df['sentiment'] == 0] \n    df_pos = df_pos.iloc[:int(len(df_pos)\/80)]\n    df_neg = df_neg.iloc[:int(len(df_neg)\/80)]\n    df = pd.concat([df_pos, df_neg])\n    trim_df = False # prevent running more than once  \n    is_trimmed = True\n    print(\"Nr rows after trim:\", len(df))\nelse:\n    print(\"No trimming done\")\n    \n# Checking the data's output balance\n# Label '4' denotes positive sentiment and '0' denotes negative sentiment\nax = df.groupby('sentiment').count().plot(kind='bar', title='Distribution of data',\n                                               legend=False)\nax = ax.set_xticklabels(['Negative','Positive'], rotation=0)","fb5743e4":"df.head(10)","9be29646":"# Reading contractions.csv and storing it as a dict.\ncontractions = pd.read_csv('..\/input\/contractions\/contractions.csv', index_col='Contraction')\ncontractions.index = contractions.index.str.lower()\ncontractions.Meaning = contractions.Meaning.str.lower()\ncontractions_dict = contractions.to_dict()['Meaning']\n\n# Defining regex patterns.\nurlPattern        = r\"((http:\/\/)[^ ]*|(https:\/\/)[^ ]*|(www\\.)[^ ]*)\"\nuserPattern       = '@[^\\s]+'\nhashtagPattern    = '#[^\\s]+'\nalphaPattern      = \"[^a-z0-9<>]\"\nsequencePattern   = r\"(.)\\1\\1+\"\nseqReplacePattern = r\"\\1\\1\"\n\n# Defining regex for emojis\nsmileemoji        = r\"[8:=;]['`\\-]?[)d]+\"\nsademoji          = r\"[8:=;]['`\\-]?\\(+\"\nneutralemoji      = r\"[8:=;]['`\\-]?[\\\/|l*]\"\nlolemoji          = r\"[8:=;]['`\\-]?p+\"\n\ndef preprocess_apply(tweet):\n\n    tweet = tweet.lower()\n\n    # Replace all URls with '<url>'\n    tweet = re.sub(urlPattern,'<url>',tweet)\n    # Replace @USERNAME to '<user>'.\n    tweet = re.sub(userPattern,'<user>', tweet)\n    \n    # Replace 3 or more consecutive letters by 2 letter.\n    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n\n    # Replace all emojis.\n    tweet = re.sub(r'<3', '<heart>', tweet)\n    tweet = re.sub(smileemoji, '<smile>', tweet)\n    tweet = re.sub(sademoji, '<sadface>', tweet)\n    tweet = re.sub(neutralemoji, '<neutralface>', tweet)\n    tweet = re.sub(lolemoji, '<lolface>', tweet)\n\n    for contraction, replacement in contractions_dict.items():\n        tweet = tweet.replace(contraction, replacement)\n\n    # Remove non-alphanumeric and symbols\n    tweet = re.sub(alphaPattern, ' ', tweet)\n\n    # Adding space on either side of '\/' to seperate words (After replacing URLS).\n    tweet = re.sub(r'\/', ' \/ ', tweet)\n    return tweet","1e8bafa5":"%%time\ndf['processed_tweet'] = df.tweet.apply(preprocess_apply)","7e29bfb7":"df.head(10)","8b409fed":"processedtext = list(df['processed_tweet'])\nif is_trimmed: \n    data_pos = processedtext[10000:]\n    data_neg = processedtext[:10000]\nelse: \n    data_pos = processedtext[800000:]\n    data_neg = processedtext[:800000]","99873220":"plt.figure(figsize = (15,20))\nwc = WordCloud(max_words = 1000 , width = 1600 , height = 800,  background_color ='white', min_font_size = 25,\n               collocations=False).generate(\" \".join(data_pos))      \nplt.axis(\"off\") \n\nplt.imshow(wc , interpolation = 'bilinear')","aefb5dec":"plt.figure(figsize = (15,20))\nwc = WordCloud(max_words = 1000 , width = 1600 , height = 800,  background_color ='white', min_font_size = 25,\n               collocations=False).generate(\" \".join(data_neg))            \nplt.axis(\"off\") \nplt.imshow(wc , interpolation = 'bilinear')","c0fd0dc4":"from sklearn.model_selection import train_test_split\n\n#X_data, y_data = np.array(df['processed_tweet']), np.array(df['sentiment'])\n\ndf_train, df_test = train_test_split(df, test_size=0.10, random_state=42)\nX_train, y_train = np.array(df_train['processed_tweet']), np.array(df_train['sentiment'])\nX_test, y_test = np.array(df_test['processed_tweet']), np.array(df_test['sentiment'])\n\n#X_train, X_test, y_train, y_test = train_test_split(X_data, y_data,test_size = 0.05, random_state = 42)\n\nprint(\"Nr of samples in each set after split:\\n\")\nprint(\"  TRAIN size:\", len(X_train))\nprint(\"  TEST size:\", len(X_test), \"\\n\")\n\nprint('Data Split done.')\n\npos = 0\nneg = 0\nfor val in y_train: \n    if val == 0:\n        neg +=1 \n    else: pos +=1\nprint(\"pos\", pos, \"neg\", neg)","415157a9":"from time import time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n                \nstart_time = time()\n\npipe = Pipeline([('vect', CountVectorizer()),\n                     ('clf', MultinomialNB())])\n\n\n# Parameters of pipelines can be set using \u2018__\u2019 separated parameter names:\nparam_grid = {\n    'vect__binary': [True, False], # If True, all non zero counts are set to 1. This is what how interpreted \"set of words\"\n    'vect__ngram_range': [(1,1), (1,2)], # unigram, unigram + bigram\n    'clf__alpha': [1, 0.1]\n}\n\nsearch = GridSearchCV(pipe, param_grid, n_jobs=-1) # defaults to 5-fold \nsearch.fit(df_train['processed_tweet'], df_train['sentiment']) # Pipeline requires pd \n\nprint(\"Training complete in\", time() - start_time, \"seconds\")\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)","3d000e74":"from sklearn.metrics import classification_report\n\ny_pred_mnb = search.predict(df_test['processed_tweet'])\nprint(classification_report(df_test['sentiment'], y_pred_mnb,))","73151b08":"from sklearn.metrics import confusion_matrix, classification_report\n\ndef ConfusionMatrix(y_pred, y_test):\n    # Compute and plot the Confusion matrix\n    cf_matrix = confusion_matrix(y_test, y_pred)\n\n    categories  = ['Negative','Positive']\n    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() \/ np.sum(cf_matrix)]\n\n    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n\n    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n                xticklabels = categories, yticklabels = categories)\n\n    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)","4242725e":"# Printing out the Evaluation metrics. \nConfusionMatrix(y_pred_mnb, df_test['sentiment'])","5b3943dd":"def label_to_str(label):\n    if label == 0: return \"Negative :\"\n    return \"Positive :\"\n\ndef nb_predict(df_custom_tweets):\n    df_custom_tweets['tweet_processed'] = df_custom_tweets.tweet.apply(preprocess_apply)\n    sentiments = search.predict(df_custom_tweets['tweet_processed'])\n    for index, row in enumerate(df_custom_tweets['tweet']):\n        print(label_to_str(sentiments[index]), row)\n   ","e6deba54":"\ntweet1 = \"Lunch at McDonalds today was not tasty\"\ntweet2 = \"I am finally done with my homwork, feeling great\" \ntweet3 = \"Could have been better\"\n\ntweet4 = \"The song was good!\"\ntweet5 = \"The song was not good!\"\ntweet6 = \"This song was great!\"\ntweet7 = \"This song wasn't great!\"\n\ntweet8 = \"That's just what I needed today..\"\ntweet9 = \"I love being ignored\" \ntweet10 = \"Just broke my phone, awesome day thus far!\"\n\ntweet_list = [tweet1, tweet2, tweet3, tweet4, tweet5, tweet6, tweet7, tweet8, tweet9, tweet10]\ncustom_tweets = pd.DataFrame({'tweet': tweet_list})\n\nnb_predict(custom_tweets)\n","ed433760":"from gensim.models import Word2Vec\n\nEmbedding_dimensions = 100\n\n# Creating Word2Vec training dataset.\nWord2vec_train_data = list(map(lambda x: x.split(), X_train))","927077a9":"%%time\n\n# Defining the model and training it.\nword2vec_model = Word2Vec(Word2vec_train_data,\n                 size=Embedding_dimensions,\n                 workers=8,\n                 min_count=5)\n\nword2vec_model.wv.save('Word2Vec-twitter-100-dims')\nprint(\"Finished training word2vec\")\nprint(\"Vocabulary Length:\", len(word2vec_model.wv.vocab))","547f72c6":"word2vec_model.most_similar(\"lol\")","9120c42a":"word2vec_model.most_similar(\"like\")","92ede0c4":"word2vec_model.most_similar(\"stress\")","7dab47ef":"# Defining the model input length.\ninput_length = 60\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","bcc759c5":"tokenizer = Tokenizer(filters=\"\", lower=False, oov_token=\"<oov>\")\ntokenizer.fit_on_texts(df['processed_tweet'])\n\nvocab_length = len(tokenizer.word_index) + 1\nprint(\"Tokenizer vocab length:\", vocab_length)","b60770ff":"X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=input_length)\nX_test  = pad_sequences(tokenizer.texts_to_sequences(X_test) , maxlen=input_length)\n\nprint(\"X_train.shape:\", X_train.shape)\nprint(\"X_test.shape :\", X_test.shape)","30c12e9b":"embedding_matrix = np.zeros((vocab_length, Embedding_dimensions))\n\nfor word, token in tokenizer.word_index.items():\n    if word2vec_model.wv.__contains__(word):\n        embedding_matrix[token] = word2vec_model.wv.__getitem__(word)\n\nprint(\"Embedding Matrix Shape:\", embedding_matrix.shape)","b3fe7b41":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding\n","73c114cd":"def getModel():\n    embedding_layer = Embedding(input_dim = vocab_length, \n                                output_dim = Embedding_dimensions,\n                                weights=[embedding_matrix], \n                                input_length=input_length,\n                                trainable=False)\n\n    model = Sequential([\n        embedding_layer,\n        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n        Conv1D(100, 5, activation='relu'),\n        GlobalMaxPool1D(),\n        Dense(16, activation='relu'),\n        Dense(1, activation='sigmoid'),\n    ],\n    name=\"Sentiment_Model\")\n    return model","fa7ef9a8":"# instantiating the model in the strategy scope creates the model on the TPU\n#with tpu_strategy.scope():\ntraining_model = getModel()\ntraining_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\ntraining_model.summary()","85423b2d":"skip_training = False","776a08b7":"checkpoint_path = '..\/input\/lstm-weights\/variables'\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n!ls {checkpoint_dir}","72fb0613":"found_prev_weights = False\n\nif skip_training: \n    # store weights before loading pre-trained weights\n    preloaded_layers = training_model.layers.copy()\n    preloaded_weights = []\n    for pre in preloaded_layers:\n        preloaded_weights.append(pre.get_weights())\n\n    # load pre-trained weights\n    try: \n        training_model.load_weights(checkpoint_path)\n\n        print(\"Loading weights from prev training\\n\")\n\n        # compare previews weights vs loaded weights\n        for layer, pre in zip(training_model.layers, preloaded_weights):\n            weights = layer.get_weights()\n\n            if weights:\n                if np.array_equal(weights, pre):\n                    print('not loaded', layer.name)\n                else:\n                    print('loaded', layer.name)\n                    found_prev_weights = True\n\n    except:\n        found_prev_weights = False\n        print(\"No previous training done\")\nelse:\n    print(\"No prev weights will be used\")\n","1c1bdcc9":"from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n\ncallbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n     EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]","3e84610f":"if not found_prev_weights: \n    print(\"Training from scratch\")\n    history = training_model.fit(\n    X_train, y_train,\n    batch_size=1000,\n    epochs=15,\n    validation_split=0.1,\n    callbacks=callbacks,\n    verbose=1)\nelse:\n    print(\"Weights already loaded, no training will occur\")","00d74036":"acc,  val_acc  = history.history['accuracy'], history.history['val_accuracy']\nloss, val_loss = history.history['loss'], history.history['val_loss']\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","fdbd6827":"# Predicting on the Test dataset.\ny_pred = training_model.predict(X_test)\n\n# Converting prediction to reflect the sentiment predicted.\ny_pred = np.where(y_pred>=0.5, 1, 0)\n\n# Printing out the Evaluation metrics. \nConfusionMatrix(y_pred, y_test)","36f09109":"# Print the evaluation metrics for the dataset.\nprint(classification_report(y_test, y_pred))","ef10a9ef":"def lstm_predict(df_custom_tweets):\n    df_custom_tweets.head()\n    X_custom = np.array(df_custom_tweets['tweet_processed'])\n    X_custom = pad_sequences(tokenizer.texts_to_sequences(X_custom), maxlen=input_length)\n    y_pred = training_model.predict(X_custom)\n    pred = np.where(y_pred>=0.5, 1, 0)\n    for index, row in enumerate(df_custom_tweets['tweet']):\n        print(y_pred[index][0], \"=\", label_to_str(pred[index]), row)","128acfd8":"lstm_predict(custom_tweets)","622a2880":"correct = 0\n                \nfor i in range(len(X_test)):\n    \n    if(y_pred[i] != y_test[i]):\n        print('Expected', label_to_str(y_test[i]), \"\\nInput:\", df_test['tweet'].iloc[i], \"\\nProcessed input: \", df_test[\"processed_tweet\"].iloc[i], \"\\n\")\n        correct += 1\n        \n    if correct > 30:\n        break","5dd4a51c":"# Saving Word2Vec-Model\nword2vec_model.wv.save_word2vec_format('Word2Vec-twitter-100-dim')\n\n# Saving the tokenizer\nwith open('Tokenizer.pickle', 'wb') as file:\n    pickle.dump(tokenizer, file)\n\n# Saving the TF-Model.\ntraining_model.save('Twitter-Sentiment-LSTM')","dd2fba6e":"### Confusion Matrix","ee73c3a5":"### Check if we can load weights from prev training","99a630e1":"## 1.3 Check balance","0204fe6f":"From the training curve we can conclude that our model doesn't have bias nor is it overfitting. As such, the **regularization** introduced in the model seem to work perfectly well. For example we have have two drop out layers embeded in the LSTM layers of the model. The accuracy curve has flattened but is still rising, which means that training for more epochs probably will yield even better results.\n\nThe **validation loss** is lower than the **training loss** because the dropouts in LSTM aren't active while evaluating the model.\n\n**Over-Fitting** is basically the phenomenon where the model's performance on validation data starts degrading, while still achieving great progress on the test set. In other words, the model is doing exceptionally well on learning specific examples it has been trained on, but is failing to generalize to data it never saw in its training phase.\n\n**Dropout** is one of the many regularization techniques, and also one of the simplest to implement and most commonly used. Basically, what dropout does is that it randomly eliminates several (based on a parametrized percentage rate) neurons connections in the network, rendering the model less complex, and forcing the model to only look at part of a given example. The random elimination of connections in the model is repeated randomly for each example training data.\n\n\nFor example, let's consider the following sentences, with a dropout layer with a rate of 0.5 (50% of connections will be eliminated): \n\n> \"Another kind of regularization can be directly applied to the cost function\"\n>\n> \"This is my first ever notebook. Hope you're enjoying it so far!\"\n\nThe output of the dropout layer could look like the following:\n\n> \"kind of regularization be to function\"\n>\n> \"This my notebook. you enjoying it far!\"\n\nThus, the model will only have information on a part of the input example, and should be able to escape over-fitting particular characteristics of the training data.","43611e72":"### Predicting on custom tweets\nNow let\u2019s have some fun testing our new and all-powerful model on some custom data!\n\nThis model will return values between 0 and 1, representing it\u2019s confidence on whether a tweet holds a negative or a positive sentiment. The closer the value is to 0, the more confident the model is that this tweet is negative. The closer the value is to 1, the more confident the model is that this tweet is negative.","4af5522f":"Skimming through the above missclassified output, we get some insight into why the model won't properly classify several tweets. Some examples seem wrongly labeled in the first place, meaning that the emojis used as labels may noy always reflect the sentiment of the tweet. Many tweets seem to be ironic or sarcastic, and perhaps emojis often are used in the same manner. Ie, people use a happy emoji in combination with a negative tweet.\n\nHowever, some other tweets are simply hard to classify without further context, and perhaps should have been labeled neutral or been removed from the dataset to start with. \n\nSo all in all, I would say that the current model is relatively robust in classifying the sentiment in a given sentence. ","c8cd921f":"### Inspecting wrongly predicted data\nLet's take a look at some of the wrongly classified data from the model.","7eb2f3a0":"### Clasification report","51937c9a":"### Model Architecture\n1) Embedding Layer: Layer responsible for converting the tokens into their vector representation that is generated by Word2Vec model. We're using the predefined layer from Tensorflow in out model.\n\nArguments:\n\n- input_dim: Size of the vocabulary.\n- output_dim: Dimension of the dense embedding.\n- weights: Initiazises the embedding matrix.\n- trainable: Specifies whether the layer is trainable or not.\n\n2) Bidirectional: Bidirectional wrapper for RNNs. It means the context are carried from both left to right and right to left in the wrapped RNN layer.\n\n3) LSTM: Long Short Term Memory, its a variant of RNN which has memory state cell to learn the context of words which are at further along the text to carry contextual meaning rather than just neighbouring words as in case of RNN.\n\nArguments:\n\n- units: Positive integer, dimensionality of the output space.\n- dropout: Fraction of the units to drop for the linear transformation of the inputs.\n- return_sequence: Whether to return the last output in the output sequence, or the full sequence.\n\n4) Conv1D: This layer creates a convolution kernel that is convolved with the layer input over a single dimension to produce a tensor of outputs.\n\nArguments:\n\n- filters: The dimensionality of the output space (i.e. the number of output filters in the convolution).\n- kernel_size: Specifies the length of the 1D convolution window.\n- activation: Specifies the activation function to use.\n\n5) GlobalMaxPool1D: Downsamples the input representation by taking the maximum value over the different dimensions.\n\n6) Dense: Dense layer adds a fully connected layer in the model. The argument passed specifies the number of nodes in that layer.\n\nThe last dense layer has the activation \"Sigmoid\", which is used to transform the input to a number between 0 and 1. Sigmoid activations are generally used when we have 2 categories to output in.","539e1780":"We now have 2 options, either train the model from scratch (except our embeddding layer which consists of our word2vec layer), or load the weights from a previous training run which I have already run. ","d0d19500":"## 5.2 Analyse some words in word space","383debea":"### Model Callbacks\u00b6\n\nCallbacks are objects that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc).\n\nWe can use callbacks to write TensorBoard logs after every batch of training, periodically save our model, stop training early or even to get a view on internal states and statistics during training.\n\nReduceLROnPlateau: Reduces Learning Rate whenever the gain in performance metric specified stops improving.\n\n- monitor: quantity to be monitored.\n- patience: number of epochs with no improvement after which learning rate will be reduced.\n- cooldown: number of epochs to wait before resuming normal operation after lr has been reduced.\n\nEarlyStopping: Stop training when a monitored metric has stopped improving.\n\n- monitor: Quantity to be monitored.\n- min_delta: Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\n- patience: Number of epochs with no improvement after which training will be stopped.","8f38d67a":"## 1.4 Trim size of dataframe (optional)\nDuring development it can be handy to work with just a fraction of the whole dataset. Choose if you want to trim dataset by setting trim_df below. \n\n","d6f5f311":"## 1.1 Load dataframe\nFirst make sure you have downloaded the dataset from [here](https:\/\/www.kaggle.com\/kazanova\/sentiment140) and put the csv file in the \"input\" folder. If you are running it on kaggle make sure the data is in the kaggle input folder.","197546fa":"# 7. Naive Bayes","e6aa70cb":"### Classification Report","cd644c9f":"Tokenizing the X_train and X_test dataset and padding them to the length 'input_length'.\n\nThe tokenized list is pre-padded, i.e padding tokens are added to the start. After padding, the length of the data would be equal to 'input_length'.","6d1d06b6":"## 3.2 Analyze most common words","48d64d29":"## 3.1 Inspect some data","b1a95705":"Now that our data is somewhat clean, we can use it to build our classification model. One of the most commonly used classification models in Natural Language Processing (NLP) is the Naive Bayesian.\nNaive Bayesian classifiers are a collection of classification algorithms based on Bayes\u2019 Theorem. It is not a single algorithm but rather a family of algorithms where all of them make the following naive assumptions:\n\n- All features are independent from each other.\n- Every feature contributes equally to the output.\n\nIn our case, these two assumptions can be interpreted as:\n\n- Each word is independent from the other words, no relation between any two words of a given sentence.\n- Each word contributes equally, throughout all sentences, to the decision of our model, regardless of its relative position in the sentence.\n\nExample: \"This is bad\" \/ \"This is very bad\" or \"Such a kind person\" \/ \"This kind of chocolate is disgusting\", in both cases the Naive Bayesian classifier would give the same importance for the words 'bad' and 'kind', albeit them having a stronger meaning and a different meaning respectively in first and second sentences.\n\n\nThe Bayes' Theorem describes the probability of an event $A$, based on prior knowledge of conditions $B$ that might be related to the event: \n\n$P(A | B) = \\frac{P(B | A)P(A)}{P(B)}$\n\nIn our case, this can be intuitively interpreted as the probability of a tweet being positive, based on prior knowledge of the words inside the input text. In a nutshell, this probability is: the probability of the first word occuring in a positive tweet, times, the probability of the second word occuring in a positive tweet, ..., times, the probability of a tweet being positive. \n\n\n","16fbbaa5":"As the model only evaluates sentences word by word, independently, it performs  poorly when it comes to negations and other multi-words constructs. For example, the input:\n\n- The song was good!\n\nThe model will simply take each individual word (after preprocessing) and calculate each word's probability to be either positive and negative, and then finally multiply everything together. Therefore it is expected that the model will perform poorly on examples such:\n\n- The concert was not good! \n\nAnd as we can see in the output above both of these examples came out as positive. \n\nThe model will encounter similar problems in sarcastic and ironic sentences, as we can see in the example:\n\n- That's just what I needed today..","f884d532":"## 3.4 Word cloud of most common negative words","a823ff73":"## 2.1 Inspect data \nFirst take a look at some tweets in the df shown below. As you can see, there exists a lot of user names, digits and punctutaions, hyperlinks etc that we can clean up before any tokenizing or lemmatizing. ","773e6ee2":"## 5.4 Designing the Model\nWe are going to build a deeplearning Sequence model. Sequence model are very good at getting the context of a sentence, since it can understand the meaning rather than employ techniques like counting positive or negative words like in a Bag-of-Words model.","90554513":"## 5.7 Evaluating the Model\n\nSince our dataset is not **skewed**, i.e. it has equal number of **Positive and Negative Predictions**. We're choosing **Accuracy** as our evaluation metric. Furthermore, we're plotting the **Confusion Matrix** to get an understanding of how our model is performing on both classification types.\n\n### Learning curve\n\nLearning curves show the relationship between training set size and your chosen evaluation metric (e.g. RMSE, accuracy, etc.) on your training and validation sets. They can be an extremely useful tool when diagnosing your model performance, as they can tell you whether your model is suffering from bias or variance.\n\n**This section requires that training was run in this notebook**","91a517cc":"From the confusion matrix, it can be concluded that the model makes more False Negative predictions than positive. This means that the model is somewhat biased towards predicting negative sentiment.","f48ffe9b":"## 1.2 Drop uneccessary columns ","4d7533dd":"## 3.5 Comments\nIt seems like the preproccessing is quite good! Some words were left on purpose, for example usernames were replaced by $<user>$. This will help the LSTM model to understand the context in a sentence. The positive words feel like they have a strong motivatoin behind them, while some of the negative words are not as intuative. However, there is some overlop between positive and negative words, eg. \"work\", \"get\", \"go\" and \"day\" are very common in both classes. ","7f18e810":"### Training the model\nWe'll now train our model using the fit method and store the output learning parameters in history, which can be used to plot out the learning curve.\n\nArguements:\n\n- batch_size: Number of samples per gradient update. Increasing the batch_size speeds up the training.\n- epochs: Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided.\n- validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch\n- callbacks: List of callbacks to apply during training process.\n- verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.","7cf7e308":"## 5.1 Creating Word Embeddings using Word2Vec model\n\nWord embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc. Loosely speaking, word embeddings are vector representations of a particular word.\n\nWord2Vec was developed by Google and is one of the most popular technique to learn word embeddings using shallow neural network. Word2Vec can create word embeddings using two methods (both involving Neural Networks): Skip Gram and Common Bag Of Words (CBOW).","a8ff25af":"Cleaning up the tweet text and storing it in **\"processed_tweet\"** field in the dataframe.","bdcc42e9":"### Saving the model\n\nWe're saving the **tokenizer, Word2Vec and Tensorflow model** for use later.","15515a81":"Other than that, the model seems quite nice! But it is worth mentioning that a naive bayes model perhaps does not generalize well.\n\nThe model will perform good on data similar to what the model was trained on. For example, if the model learned that I love music and I love dancing carry a positive sentiment, it would be pretty easy for it to classify I love studying as a sentence carrying a positive sentiment. However, if the model were to classify I adore engineers, it would most probably miss-classify it. As it never encountered any of these words before, therefore it is unable to properly classify it, and would output a random choice.","50567559":"## 5.5 Initialize the model\n\n\n\nThe Model must be compiled to define the loss, metrics and optimizer. Defining the proper loss and metric is essential while training the model.\n\n- Loss: We're using Binary Crossentropy. It is used when we have binary output categories. Check out this article on losses.\n\n- Metric: We've selected Accuracy as it is one of the common evaluation metrics in classification problems when the category data is equal. Learn more about metrics here.\n\n- Optimizer: We're using Adam, optimization algorithm for Gradient Descent. You can learn more about Adam here.","9d281ddf":"# 3. Analyze the data","3ed8f4d5":"## 5.3 Creating Embedding Matrix\nEmbedding Matrix is a maxtrix of all words and their corresponding embeddings. We use embedding matrix in an Embedding layer in our model to embedded a token into it's vector representation, that contains information regarding that token or word.\n\nWe get the embedding vocabulary from the tokenizer and the corresponding vectors from the Embedding Model, which in this case is the Word2Vec model.\n\nShape of Embedding matrix is usually the Vocab Length * Embedding Dimension.","481e09c5":"### Confusion matrix","7e0ba11e":"## 2.3 Preprocessing all tweets\nNow let's define a function that applies all the preprocessing steps mentioned above\n\n**Text Preprocessing** is traditionally an important step for **Natural Language Processing (NLP)** tasks. It transforms text into a more digestible form so that deep learning algorithms can perform better.\n\nTweets usually contains a lot of information apart from the text, like mentions, hashtags, urls, emojis or symbols. Since normally, NLP models cannot parse those data, we need to clean up the tweet and replace tokens that actually contains meaningful information for the model.\n\n**The Preprocessing steps taken are:**\n1. **Lower Casing:** Each text is converted to lowercase.\n2. **Replacing URLs:** Links starting with **'http' or 'https' or 'www'** are replaced by **'<url\\>'**.\n3. **Replacing Usernames:** Replace @Usernames with word **'<user\\>'**. [eg: '@Kaggle' to '<user\\>'].\n4. **Replacing Consecutive letters:** 3 or more consecutive letters are replaced by 2 letters. [eg: 'Heyyyy' to 'Heyy']\n5. **Replacing Emojis:** Replace emojis by using a regex expression. [eg: ':)' to '<smile\\>']\n6. **Replacing Contractions:** Replacing contractions with their meanings. [eg: \"can't\" to 'can not']\n7. **Removing Non-Alphabets:** Replacing characters except Digits, Alphabets and pre-defined Symbols with a space.\n\nAs much as the preprocessing steps are important, the actual sequence is also important while cleaning up the text. For example, removing the punctuations before replacing the urls means the regex expression cannot find the urls. Same with mentions or hashtags. So make sure, the actual sequence of cleaning makes sense.","5c9e693a":"### Predicting on custom tweets\n\nLet's test the model on some custom tweets","56545476":"We notice that the LSTM model can in fact understand the relationship between words. For example, it managed to identify that adding a not before a \"good\",  completely changes its meaning from positive to negative! And it is very confindent in it's decision, reflected by the numbers. \n","17e6870b":"# 1. Importing the dataset\nhttps:\/\/medium.com\/@paritosh_30025\/natural-language-processing-text-data-vectorization-af2520529cf7\n\n**Context**\n\nIn this project, the \"sentiment140\" dataset will be used. It contains 1,600,000 tweets extracted using the twitter api. The tweets have been annotated (0 = negative, 4 = positive) and they can therefore be used to detect sentiment. The training data was automatically created, as opposed to having humans manual annotate tweets. In the approach, it was assumed that any tweet with positive emoticons, like :), were positive, and tweets with negative emoticons, like :(, were negative. They used the Twitter Search API to collect these tweets by using keyword search. \n\n**Content**\n\nIt contains the following 6 fields:\n\n1. target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n2. ids: The id of the tweet ( 2087)\n3. date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n4. flag: The query (lyx). If there is no query, then this value is NO_QUERY\n5. user: the user that tweeted (robotickilldozr)\n6. text: the text of the tweet (Lyx is cool)\n\nA link to kaggle regarding the dataset with resources about how it was generated is available [here](https:\/\/www.kaggle.com\/kazanova\/sentiment140).\n\nCitation: Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12.","4c46065a":"Word2Vec() function creates and trains the word embeddings using the data passed.\n\n**Training Parameters**:\n\n- size: The number of dimensions (N) that the Word2Vec maps the words onto. Bigger size values require more training data, but can lead to better (more accurate) models.\n- workers: Specifies the number of worker threads for training parallelization, to speed up training.\n- min_count: min_count is for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there\u2019s not enough data to make any meaningful training on those words, so it\u2019s best to ignore them.","0721cf27":"## 3.3 Word cloud of most common positive words\n\nWord clouds are a beautiful way of visualizing words frequencies in text documents.\nIt produces an image with frequently-appearing words in the text document, where the most frequent words are showcased with bigger font sizes and less frequent words with smaller font sizes.","561db6a9":"# 2. Cleaning and pre-processing the data","460735c2":"81% accuracy is quite nice. And without the 5-fold cross validation it only takes about 30 seconds to train the model, which is impressive. Let's try the model out with some custom tweets.  ","0260ab6b":"## 5.2 Tokenizing and Padding datasets\nTokenization is a common task in Natural Language Processing (NLP). It\u2019s a fundamental step in both traditional NLP methods like Count Vectorizer and Advanced Deep Learning-based architectures like Transformers.\n\nTokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types \u2013 word, character, and subword (n-gram characters) tokenization.\n\nAll the neural networks require to have inputs that have the same shape and size. However, when we pre-process and use the texts as inputs for our model e.g. LSTM, not all the sentences have the same length. We need to have the inputs with the same size, this is where the padding is necessary.\n\nPadding is the process by which we can add padding tokens at the start or end of a sentence to increase it's length upto the required size. If required, we can also drop some words to reduce to the specified length.\n\n- Tokenizer: Tokenizes the dataset into a list of tokens.\n- pad_sequences: Pads the tokenized data to a certain length.\n- The input_length has been set to 60. This will be the length after the data is tokenized and padded.","2099e5af":"# 4. Split the data into train & test\n\nThe preprocessed data is divided into 2 sets of data:\n    \n - Training Data: The sample of data used to fit the model. Contains 90% of dataset.\n - Test Data:  The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset. Contains 10% of dataset.","94f0b630":"Defining the Tokenizer and fitting it on the training data. Here, we are tokenzing the data by spliting it up with the delimiter space ' '.\n\nArgs in Tokenizer():\n\n- filters: Characters to filter out from the sentences to tokenize.\n- lower: True\/False. Whether to lowerCase the sentence or not.\n- oov_token: Out of Vocabulary token to put in for words which aren't in the tokenizer vocab.\n- Filters and lower has been turned off because we've already done those steps during the preprocessing step.","77d64c34":"# 1. Import dependencies","ab4b697f":"As we can see above, we got a CV score of 0.0.805 using parameters:\n- alpha = 1\n- vect__binary = True -> set-of-of words\n- vect_ngram_range = (1,2) -> unigram + bigram","7a579c19":"# 5. LSTM \n\nIn order to feed our text data to our LSTM model, we'll have to go through several extra preprocessing steps.\n\nMost neural networks expect numbers as inputs. Thus, we'll have to convert our text data to numerical data.","bf4ab5a0":"## 4.2 Comments\nThe small unbalance between positive and negative tweets in the training data is insignificant.","943b75ef":"## 5.6 Training the Model (or load weigths to skip training)","aae3c4c3":"Using the classification report, we can see that that the model achieves more than **84% Accuracy** after training just **15 epochs.** This is really good and better than most other models trained on the sentiment140 dataset achieve."}}