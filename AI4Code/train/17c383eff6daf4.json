{"cell_type":{"8dc11a29":"code","d6c3549a":"code","f2b8fb6e":"code","add3c219":"code","ba31155c":"code","bbe63e13":"code","28ff5826":"code","f2eaf6d8":"code","d59dd6ab":"code","7984ab68":"code","7bee9ec6":"code","5a7e598e":"code","3ef9c1d9":"code","c78e00f0":"code","1249e42c":"code","8f228a22":"code","3f7c5df3":"code","6043f189":"code","b2d256a5":"code","1715d7cd":"code","4e4c2c86":"code","d8fa926a":"code","d2c9ea65":"code","7f52f4f3":"code","e03df83e":"code","1227bd9f":"code","b2f3d93a":"code","13d8e40c":"code","e2b77671":"code","4bead7df":"markdown","8c8049e2":"markdown","88d62d71":"markdown","676d0cb0":"markdown","37921796":"markdown","6172f4a9":"markdown","15967ed3":"markdown","91206ef7":"markdown","43c8f412":"markdown","41e2845f":"markdown","bb8abaf1":"markdown","ca04ccb9":"markdown","ba0ac4c5":"markdown","9aa99a3c":"markdown","52ae72c8":"markdown","c54883d2":"markdown","4538c757":"markdown","67b4000c":"markdown","c8ecfc09":"markdown","de0cb6d0":"markdown"},"source":{"8dc11a29":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow\nimport keras\nfrom keras import (layers, models, regularizers)\nfrom keras import backend as K\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix","d6c3549a":"data1 = pd.read_csv(\"\/kaggle\/input\/paris-housing-classification\/ParisHousingClass.csv\")\ndata2 = pd.read_csv(\"\/kaggle\/input\/paris-housing-classification\/ParisHousingClass99.88.csv\")","f2b8fb6e":"print(data1.shape)\nprint(data2.shape)","add3c219":"data1.head()","ba31155c":"data2.head()","bbe63e13":"(data1.columns == data2.columns).all() # checking if columns are equal","28ff5826":"print(data1.isna().sum().sum()) # checking for NaN values\nprint(data2.isna().sum().sum())","f2eaf6d8":"data = pd.concat([data1,data2]).reset_index(drop=True)\ndata.shape","d59dd6ab":"category = {'Basic':0, 'Luxury': 1}\ndata['category'] = data['category'].map(category)","7984ab68":"plt.figure(figsize=(8,5))\nsns.countplot(data.category)\nplt.title('Class distribution')\nplt.show()","7bee9ec6":"corr = data.corr()\nplt.figure(figsize=(15,8))\nsns.heatmap(corr)\nplt.show()","5a7e598e":"sns.distplot(data['numberOfRooms'])\nplt.show()","3ef9c1d9":"plt.figure(figsize=(8,5))\nsns.countplot(data['made'])\nplt.xticks(rotation=50)\nplt.show()","c78e00f0":"sns.distplot(data['price'])","1249e42c":"numeric_features = ['squareMeters', 'numberOfRooms', 'floors',\n       'cityCode', 'cityPartRange', 'numPrevOwners', 'made', 'basement', 'attic', 'garage', 'hasGuestRoom', 'price']","8f228a22":"for feat in numeric_features:\n    mean = data[feat].mean()\n    std = data[feat].std()\n    data[feat] -= mean\n    data[feat] \/= std","3f7c5df3":"data = data.sample(frac=1).reset_index(drop=True)\n\ntest_frac = .15\nval_frac = .15\n\ntest_data = data[len(data)-int(len(data)*test_frac):len(data)]\nval_data = data[test_data.index[0]-int(len(data)*val_frac):test_data.index[0]]\ntrain_data = data[:val_data.index[0]]","6043f189":"train_labels = train_data['category']\ntrain_data = train_data.iloc[:,:-1]\nval_labels = val_data['category']\nval_data = val_data.iloc[:,:-1]\ntest_labels = test_data['category']\ntest_data = test_data.iloc[:,:-1]","b2d256a5":"lr = LogisticRegression()\nlr.fit(train_data, train_labels)\ny_pred_val = lr.predict(val_data)\nnp.round(f1_score(val_labels, y_pred_val),4)","1715d7cd":"confusion_matrix(val_labels, y_pred_val)","4e4c2c86":"xg = XGBClassifier()\nxg.fit(train_data, train_labels)\ny_pred_val = xg.predict(val_data)\nnp.round(f1_score(val_labels, y_pred_val),4)","d8fa926a":"def recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","d2c9ea65":"model = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(17, )))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='softmax'))\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy',  metrics=['acc',f1_m, precision_m, recall_m])","7f52f4f3":"history = model.fit(train_data, train_labels, epochs=100, batch_size=128, validation_data=(val_data, val_labels), verbose=0)","e03df83e":"y_pred_val = model.predict(val_data)\nnp.round(f1_score(val_labels, y_pred_val),4)","1227bd9f":"sv = SVC()\nsv.fit(train_data, train_labels)\ny_pred_val = sv.predict(val_data)\nnp.round(f1_score(val_labels, y_pred_val),4)","b2f3d93a":"rf = RandomForestClassifier()\nrf.fit(train_data, train_labels)\ny_pred_val = rf.predict(val_data)\nnp.round(f1_score(val_labels, y_pred_val),4)","13d8e40c":"y_lr = lr.predict(test_data) #logistic regression\ny_sv = sv.predict(test_data) #support vector machines\ny_rf = rf.predict(test_data) #random forest\ny_xg = xg.predict(test_data) #xgboost\ny_nn = model.predict(test_data) #neural network","e2b77671":"print('\\nLogical Regression f1-score:\\n')\nprint(np.round(f1_score(test_labels, y_lr),4))\nprint('\\nSVM f1-score:\\n')\nprint(np.round(f1_score(test_labels, y_sv),4))\nprint('\\nRandom f1-score:\\n')\nprint(np.round(f1_score(test_labels, y_rf),4))\nprint('\\nXGBoost f1-score:\\n')\nprint(np.round(f1_score(test_labels, y_xg),4))\nprint('\\nNN f1-score:\\n')\nprint(np.round(f1_score(test_labels, y_nn),4))","4bead7df":"# Distribution of target class","8c8049e2":"Concatenating the two dataframes along index","88d62d71":"The simple neural network is the worst performer. Linear models have outperformed non-linear models. ","676d0cb0":"# Neural Network","37921796":"# Import libraries","6172f4a9":"# Correlation between features and target","15967ed3":"# Encoding target values","91206ef7":"# Splitting data into train, validation and test sets","43c8f412":"# Logistic Regression","41e2845f":"# Random Forest Classifier","bb8abaf1":"## No of rooms","ca04ccb9":"# Prediction on Test data","ba0ac4c5":"# XGBoost","9aa99a3c":"## Distribution of price","52ae72c8":"## Distribution of year of manufacture","c54883d2":"# Load data","4538c757":"# Support Vector Machines","67b4000c":"Confusion Matrix","c8ecfc09":"## Normalizing the numeric features","de0cb6d0":"This is a set of data created from imaginary data of house prices in an urban environment - Paris. There are 2 files, one is with 100% of correctly classified instances - ParisHousing.csv, the other one is with 99.88% and it's name is ParisHousing99.88.csv. This is a binary classification problem with two classes - basic, luxury. Given data about a particular house, we have to predict whether it is a basic or a luxury house.\n\nI will use this toy data to evaluate various classification models and compare them against a three-layer neural network."}}