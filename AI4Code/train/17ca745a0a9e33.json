{"cell_type":{"ba0524a5":"code","df41e4e6":"code","f4f48d08":"code","dcc5c99e":"code","10bd5348":"code","e8927cae":"code","238cb7a3":"code","6e27cecf":"code","9676d245":"markdown"},"source":{"ba0524a5":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nimport gc\n\n\nprint('loading files...')\ntrain = pd.read_csv('..\/input\/train.csv', na_values=-1)\ntest = pd.read_csv('..\/input\/test.csv', na_values=-1)\ncol_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\ntrain = train.drop(col_to_drop, axis=1)  \ntest = test.drop(col_to_drop, axis=1)  \n\nfor c in train.select_dtypes(include=['float64']).columns:\n    train[c]=train[c].astype(np.float32)\n    test[c]=test[c].astype(np.float32)\nfor c in train.select_dtypes(include=['int64']).columns[2:]:\n    train[c]=train[c].astype(np.int8)\n    test[c]=test[c].astype(np.int8)    \n\nprint(train.shape, test.shape)\n","df41e4e6":"def gini(y, pred):\n    g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n    gs = g[:,0].cumsum().sum() \/ g[:,0].sum()\n    gs -= (len(y) + 1) \/ 2.\n    return gs \/ len(y)\n\ndef gini_xgb(pred, y):\n    y = y.get_label()\n    return 'gini', gini(y, pred) \/ gini(y, y)\n\ndef gini_lgb(preds, dtrain):\n    y = list(dtrain.get_label())\n    score = gini(y, preds) \/ gini(y, y)\n    return 'gini', score, True","f4f48d08":"params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, \n          'objective': 'binary:logistic', 'eval_metric': 'auc', 'silent': True}","dcc5c99e":"X = train.drop(['id', 'target'], axis=1)\nfeatures = X.columns\nX = X.values\ny = train['target'].values\nsub=test['id'].to_frame()\nsub['target']=0\n\nnrounds=400  # need to change to 2000\nkfold = 5  # need to change to 5\nskf = StratifiedKFold(n_splits=kfold, random_state=0)","10bd5348":"X[test_index]","e8927cae":"# import time\n# tic = time.time()\n\n# for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n#     print(' xgb kfold: {}  of  {} : '.format(i+1, kfold))\n#     X_train, X_valid = X[train_index], X[test_index]\n#     y_train, y_valid = y[train_index], y[test_index]\n#     d_train = xgb.DMatrix(X_train, y_train)\n#     #\ud559\uc2b5 \ub370\uc774\ud130 set, label\n#     d_valid = xgb.DMatrix(X_valid, y_valid) \n#     #\ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 set\n#     watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n#     # score\ub97c \uacc4\uc18d \uac10\uc2dc -> \uc131\ub2a5\uc774 \uc88b\uc544\uc84c\uc744\uacbd\uc6b0 early_stopping_rounds\uc5d0\uc11c \ub04a\uc74c\n#     xgb_model = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=100,feval=gini_xgb, maximize=True, verbose_eval=100)\n#     #nrounds \ub9cc\ud07c \ubc18\ubcf5, feval= 'eval_metric'\uc758 'auc'\uc5ec\uc57c \ud558\uc9c0\ub9cc, \uc5ec\uae30\uc120 Matrix\uac00 gini\uacc4\uc218 \uc774\uae30\ub54c\ubb38\uc5d0 gini_xgb\uac00 \ub4e4\uc5b4\uac10\n#     sub['target'] += xgb_model.predict(xgb.DMatrix(test[features].values),ntree_limit=xgb_model.best_ntree_limit+50) \/ (kfold)\n#     #ntree_limit = \uc2e4\uc81c \ubaa8\ub378\uc758 \ucd5c\uace0\uc810\uc218 \uc5d0\uc11c \ub04a\uc5b4\uc8fc\uaca0\ub2e4, +50\uc744 \ud574\uc8fc\uba74 \ucd5c\uace0\uc810\uc218\uc5d0\uc11c 50\ubc88 \ub354\uac00\uc11c \ub04a\uc5b4\uc900\ub2e4. \/(2*kfold)\ub97c \ud55c \uc774\uc720\ub294 lgb\ub3c4 \uc4f0\uae30\ub54c\ubb38\uc5d0 \ucd1d 4\uac1c\uc758 \ubaa8\ub378\uc744 \ub9cc\ub4e4\uae30\ub54c\ubb38\uc5d0 fold=4\ub85c \ub098\ub220\uc90c\n# gc.collect()\n# sub.head(2)\n# print(\"time in seconds: \", time.time() - tic)\n\n","238cb7a3":"learning_rate = 0.1\nnum_leaves = 15\nmin_data_in_leaf = 2000\nfeature_fraction = 0.6\nnum_boost_round = 10000\ntrain_label = train['target']\n\nparams = {'objective': 'binary',\n         'boosting_type':'gbdt',\n         'learning_rate':learning_rate,\n         'num_leaves': num_leaves,\n         'max_bin':256,\n         'feature_fraction': feature_fraction,\n         'verbosity':0,\n         'drop_rate':0.1,\n         'is_unbalance':False,\n         'max_drop':50,\n         'min_child_samples':10,\n         'min_child_weight':150,\n         'min_split_gain':0,\n         'subsample':0.9,\n         'metric':'auc',\n         'application':'binary'}\n\nskf = StratifiedKFold(n_splits=kfold, random_state=1)\nfor i, (train_index, test_index) in enumerate(skf.split(X, y)):\n    print(' lgb kfold: {}  of  {} : '.format(i+1, kfold))\n    X_train, X_validate, label_train, label_validate = \\\n                X[train_index, :], X[test_index, :], train_label[train_index], train_label[test_index]\n    dtrain = lgb.Dataset(X_train, label_train)\n    dvalid = lgb.Dataset(X_validate, label_validate, reference=dtrain)\n    lgb_model = lgb.train(params,dtrain,nrounds,valid_sets=[dtrain,dvalid], verbose_eval=100, feval=gini_lgb, \n                          early_stopping_rounds=400)\n    sub['target'] += lgb_model.predict(test[features].values,\n    num_iteration=lgb_model.best_iteration) \/ (2*kfold)\n\n# sub.to_csv('sub10.csv', index=False, float_format='%.5f')\n# gc.collect()\n# sub.head(2)","6e27cecf":"# params = {'metric': 'auc', 'learning_rate' : 0.01, 'max_depth':10, 'max_bin':10,  'objective': 'binary',\n#           'feature_fraction': 0.8,'bagging_fraction':0.9,'bagging_freq':10,  'min_data': 500}\n\n# skf = StratifiedKFold(n_splits=kfold, random_state=1)\n# for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n#     print(' lgb kfold: {}  of  {} : '.format(i+1, kfold))\n#     X_train, X_eval = X[train_index], X[test_index]\n#     y_train, y_eval = y[train_index], y[test_index]\n#     lgb_model = lgb.train(params, lgb.Dataset(X_train, label=y_train), nrounds,lgb.Dataset(X_eval, label=y_eval), verbose_eval=100, feval=gini_lgb, early_stopping_rounds=100)\n#     sub['target'] += lgb_model.predict(test[features].values,\n#     num_iteration=lgb_model.best_iteration) \/ (2*kfold)\n\n# sub.to_csv('sub10.csv', index=False, float_format='%.5f')\n# gc.collect()\n# sub.head(2)","9676d245":"- xgboost\ub97c 2\uac1c\uc758 \ud3f4\ub4dc\ub85c \ub098\ub220\uc11c \ubaa8\ub378 2\uac1c\ub85c 0.5 percent\ub97c \ucc44\uc6c0\n- lightgbm\ub97c 2\uac1c\uc758 \ud3f4\ub4dc\ub85c \ub098\ub220\uc11c \ubaa8\ub378 2\uac1c\ub85c 0.5 percent\ub97c \ucc44\uc6c0\n- 1.0 \uc804\uccb4 submission\uc774 \ub098\uc624\uba74 \uc81c\ucd9c"}}