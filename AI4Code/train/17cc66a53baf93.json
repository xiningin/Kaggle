{"cell_type":{"847b9572":"code","73d3c082":"code","9696e51a":"code","65e90657":"code","ab1be8b8":"markdown","a66a71e7":"markdown"},"source":{"847b9572":"import csv\nimport re\nimport math\nfrom math import sqrt\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport category_encoders as ce\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV, train_test_split\n\nfrom scipy.sparse import hstack, coo_matrix\n\nimport nltk\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk import pos_tag\n","73d3c082":"df_train= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_test=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ndf_train_all = df_train.append(df_test, sort=False)","9696e51a":"# Preprocess for Neural Networks:\ndef preprocess_description_NN(line):\n    stop_words = stopwords.words('english')\n    pattern_url = 'http[^\\s\\n\\t\\r]+'\n    pattern_user = '@[^\\s\\n\\t\\r]+'\n    pattern_tag = '#([^\\s\\n\\t\\r]+)'\n    \n    if re.match(pattern_url, line):\n        p = re.compile(pattern_url)\n        l = p.sub(' URLS ', line)\n        line = l\n    '''   \n    if re.match(pattern_user, line):\n        p_user = re.compile(pattern_user)\n        l_user = p_user.sub(' USERNAME ', line)\n        line = l_user\n        \n    \n    if re.match(pattern_tag, line):\n        p_user = re.compile('#')\n        l_user = p_user.sub(' tag_', line)\n        line = l_user    \n    '''        \n    \n    tokens_pos_all = nltk.pos_tag(word_tokenize(line))\n    \n    meaningful_pos_tokens = [x[0] for x in tokens_pos_all]\n    meaningful_tokens = [x.lower() for x in meaningful_pos_tokens if x.isalpha()]\n    \n    result = \" \".join(meaningful_tokens)\n    return result\n\n# Preprocess for other algorithms than neural networks:\ndef preprocess_description(line):\n    stemmer = SnowballStemmer('english')\n    stop_words = stopwords.words('english')\n    \n    pattern_url = 'http[^\\s\\n\\t\\r]+'\n    pattern_user = '@[^\\s\\n\\t\\r]+'\n    \n    if re.match(pattern_url, line):\n        p = re.compile(pattern_url)\n        l = p.sub(' URLS ', line)\n        line = l\n    \n    if re.match(pattern_user, line):\n        p = re.compile(pattern_user)\n        l_user = p.sub(' USERNAME ', line)\n        line = l_user\n    \n    tokens_pos_all = nltk.pos_tag(word_tokenize(line))\n    meaningful_pos_tokens = [x[0] for x in tokens_pos_all if re.match('(JJ|VB)', x[1]) or re.search('^(NN|NNS|NNP)$', x[1])]\n    meaningful_tokens = [x.lower() for x in meaningful_pos_tokens if not x in stop_words and len(x)>3  and x.isalpha()]\n    meaningful_stems = [stemmer.stem(x) for x in meaningful_tokens]\n    result = \" \".join(meaningful_stems)\n    return result\n\ndef parse_train(df_train, df_train_all, df_test, algo_type, max_words, add_keyword, parse_pca, parse_ce):\n    n_components = 0\n    limit = 10000\n    \n    # Prepare the input training data, dataset and labels :\n    df2 = df_train\n    target = df2['target']\n    Y = target.to_numpy()\n    \n    ids = df2['id']\n    \n    dfYid = pd.concat([ids, target], axis=1, sort=False)\n    \n    ar = np.reshape(dfYid.to_numpy(), (dfYid.shape[0], 2))\n    \n    # Check if the classes are unballanced and ballance if necessary:\n    nb_docs_per_class = {cl:np.count_nonzero(ar[:,cl]) for cl in np.array([0,1])}\n    nb_docs_per_class_zero = nb_docs_per_class[0] - nb_docs_per_class[1]\n    nb_docs_per_class[0] = nb_docs_per_class_zero\n    \n    #if nb_docs_per_class[0] > nb_docs_per_class[1]:\n    diff = nb_docs_per_class[0] - nb_docs_per_class[1]\n    df_diff_1 = df_train[df_train['target']==1]\n    df_diff = df_diff_1[df_diff_1['keyword']!=\"\"].sample(n=diff)\n    df_train = df_train.append(df_diff)\n    \n    # Category encoding\n    parse_keyword = 1\n    parse_kw_how = 1\n    \n    if add_keyword:\n        kw = df_train['keyword']\n        kw_all = df_train_all['keyword']\n        \n        if parse_kw_how == 0: # Target Encoder\n            #cat_ce = TargetEncoder()\n            pass\n        elif parse_kw_how == 1: # Label Encoder\n            kw = kw.fillna(\"0\")\n            kw_all = kw_all.fillna(\"0\")\n            le = LabelEncoder()\n            le.fit(kw_all)\n            kw_enc = le.transform(kw)\n            kw_enc = np.reshape(kw_enc, (kw_enc.shape[0], 1))\n        elif parse_kw_how == 2: # get_dummies    \n            kw_enc = pd.get_dummies(kw).to_numpy()\n        else: # OHE\n            kw = kw.fillna(\"0\")\n            kw_all = kw_all.fillna(\"0\")\n            le = LabelEncoder()\n            le.fit(kw_all)\n            ohe = OneHotEncoder(handle_unknown='ignore')\n            ohe.fit(kw_all)\n            \n            kw_lab = le.transform(kw)\n            kw_lab = np.reshape(kw_lab, (kw_lab.shape[0], 1))\n            kw_enc = ohe.transform(kw_lab)\n            \n        \n    target = df_train['target']\n    Y = target.to_numpy()\n    \n    descriptions = df_train['text']\n    descriptions_all = df_train_all['text']\n    \n    if algo_type == 'nn':\n        descriptions = descriptions.apply(lambda x : preprocess_description_NN(x)).to_numpy()\n        descriptions_all = descriptions_all.apply(lambda x : preprocess_description_NN(x)).to_numpy()\n    else:\n        descriptions = descriptions.apply(lambda x : preprocess_description(x)).to_numpy()\n        descriptions_all = descriptions_all.apply(lambda x : preprocess_description(x)).to_numpy()\n    \n    \n    # Vectorize dataset with frequences and tfidf\n    des = descriptions\n    des_all = descriptions_all\n    tf_vectorizer = CountVectorizer(max_df=0.95, max_features=max_words-1)\n    tf_vectorizer1 = CountVectorizer(max_df=0.9, min_df=2, max_features=5932)\n    tf_vectorizer.fit(des_all)\n    \n    X = tf_vectorizer.transform(des)\n    nb_features = X.shape[1]\n    #print(tf_vectorizer.vocabulary_)\n    tf_transformer = TfidfTransformer().fit(X)\n    X = tf_transformer.transform(X)\n    \n    if add_keyword:\n        X = hstack((kw_enc, X))\n    \n    if parse_ce :\n        cat_ce = ce.TargetEncoder()\n        \n        Xar = X.toarray()\n        y_ce =  Xar[:,0]\n        y1_ce = y_ce.reshape(y_ce.shape[0], 1)\n        X = cat_ce.fit_transform(Xar, y1_ce)\n        \n        X = Xar\n    elif parse_pca :\n        pca = PCA(whiten=True)\n        pca.fit(X.toarray())\n        X = pca.transform(X.toarray())\n    \n    descriptions_test = df_test['text']\n    ids_test = df_test['id']\n    \n    if algo_type == 'nn':\n        descriptions_test = descriptions_test.apply(lambda x : preprocess_description_NN(x)).to_numpy()\n    else:\n        descriptions_test = descriptions_test.apply(lambda x : preprocess_description(x)).to_numpy()\n    \n    X_test = tf_vectorizer.transform(descriptions_test)\n    X_test = tf_transformer.transform(X_test)\n    \n    if add_keyword:\n        kw_test = df_test['keyword']\n        if parse_kw_how == 1: # Label Encoder\n            kw_test = kw_test.fillna(\"0\")\n            kw_enc_test = le.transform(kw_test)\n            kw_enc_test = np.reshape(kw_enc_test, (kw_enc_test.shape[0], 1)) \n            \n        elif parse_kw_how == 2: # get_dummies  \n            kw_test = kw_test.fillna(\"0\")\n            kw_enc_test = pd.get_dummies(kw_test).to_numpy()\n        else: # OHE\n            kw_test = kw_test.fillna(\"0\")\n            kw_enc_lab = le.transform(kw_test)\n            kw_enc_lab = np.reshape(kw_enc_lab, (kw_enc_lab.shape[0], 1)) \n            #ohe = OneHotEncoder(handle_unknown='ignore')\n            kw_enc_test = ohe.transform(kw_enc_lab)  \n            \n        X_test = hstack((kw_enc_test, X_test))\n    \n    if parse_ce :\n        cat_ce = ce.TargetEncoder()\n        Xar = X_test.toarray()\n        y_ce =  Xar[:,0]\n        y1 = y_ce.reshape(y_ce.shape[0], 1)\n        X_test = cat_ce.fit_transform(Xar, y1)\n        \n        X_test = Xar\n    elif parse_pca:\n        X_test = pca.transform(X_test.toarray())\n    \n    return (X, Y, X_test)\n\n\n# Description; Ballance data for a pd.DataFrame, from scratch\n# Input: df : pd.DataFrame without the Id column, containing the label column\n# Output: X_res: pd.Dataframe for training ballanced containing the label column\ndef ballance_data_with_y(df):\n    df_1 =  df[df[\"target\"]==1]\n    df_0 =  df[df[\"target\"]==0]\n    len1 = df_1.shape[0]\n    len0 = df_0.shape[0]\n    \n    vmax = 0\n    vmin = 1\n    if len1 > len0:\n        vmax = 1\n        vmin = 0\n        df_max = df_1\n        df_min = df_0\n    elif len1 < len0:\n        vmax = 0\n        vmin = 1\n        df_max = df_0\n        df_min = df_1\n    else:\n        return (df, Y)\n    \n    len_max = df_max.shape[0]\n    len_min = df_min.shape[0]\n    \n    to_multiply = int(round(len_max\/len_min))\n    df_to_append = pd.concat([df_min] * to_multiply, ignore_index=True)\n    \n    len_append = df_to_append.shape[0]\n    \n    X_res = pd.concat([df_max, df_to_append], ignore_index=True)\n    \n    to_add = len_max - len_append\n    if to_add > 0:\n        df_to_add = df_min.sample(n=to_add, random_state=1)\n        X_res = pd.concat([X_res, df_to_add], ignore_index=True)\n    \n    X_res = X_res.reset_index(drop=True)\n    return X_res   \n\ndef print_into_file(y_pred, df_test, algo_name=None):\n    l = []\n    for myindex in range(y_pred.shape[0]):\n        Y0 = y_pred[myindex]\n        l.insert(myindex, Y0)\n    \n    df_result = df_test\n    df_result = df_result.assign(target=pd.Series(l).values)\n    df_result = df_result.drop(columns=['location'])\n    df_result = df_result.drop(columns=['keyword'])\n    df_result = df_result.drop(columns=['text'])\n    \n    f = open('submission.csv', 'w')\n    r = df_result.to_csv(index=False, path_or_buf=f)\n    f.close()","65e90657":"num_classes = 2\nmax_words = max_words = 701\n\nadd_keyword = True \nparse_pca = False\nparse_ce = True\n\n(X, Y, X_test) = parse_train(df_train, df_train_all, df_test, 'll', max_words, add_keyword, parse_pca, parse_ce)\n\nx_train = X\ny_train = Y\nx_test = X_test\n\nclf = RandomForestClassifier(max_depth=1000, n_estimators=5000, n_jobs=4, verbose=1)\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_test)\n\nprint_into_file(y_pred, df_test)","ab1be8b8":"Here is a rather rough text classification : no LSTM, no RNN, just Random Forest and a lot of pre-processing. \nThe result is around 0.8 (FScore), which might mean 0.9 precision  and 0.9 recall, which is not bad.\n\n","a66a71e7":"Read the data "}}