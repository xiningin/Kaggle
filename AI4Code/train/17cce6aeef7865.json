{"cell_type":{"cee827ca":"code","715d29a3":"code","fac43579":"code","2ac2edd2":"code","7eb63845":"code","59d95054":"code","1db02523":"code","03e4c3df":"code","ecff89f3":"code","8f102145":"code","4a7e6023":"code","f0ff3814":"code","359fea73":"code","68e353e9":"code","f8f39756":"code","827d842f":"code","3cbec469":"markdown","f5e41d8a":"markdown","73de159c":"markdown","7c9e547b":"markdown","c9e2a40c":"markdown","4eb6155f":"markdown","ca2daf50":"markdown","bb183aff":"markdown","a3a664f5":"markdown","c1469dd7":"markdown","6f7e0044":"markdown","f107a23a":"markdown","5b0a9476":"markdown","71634057":"markdown","b388df25":"markdown","e280c95a":"markdown","ec7b3acb":"markdown","e8c0fa6c":"markdown","c1fe891d":"markdown","d2af2332":"markdown","6684d4d7":"markdown","2e696e17":"markdown"},"source":{"cee827ca":"%matplotlib inline\nfrom fastai.basics import *","715d29a3":"n=100","fac43579":"x = torch.ones(n,2) \nx[:,0].uniform_(-1.,1)\nx[:5]","2ac2edd2":"a = tensor(3.,2); a","7eb63845":"y = x@a + torch.rand(n)","59d95054":"plt.scatter(x[:,0], y);","1db02523":"def mse(y_hat, y): return ((y_hat-y)**2).mean()","03e4c3df":"a = tensor(-1.,1)\na","ecff89f3":"y_hat = x@a\nmse(y_hat, y)","8f102145":"plt.scatter(x[:,0],y)\nplt.scatter(x[:,0],y_hat);","4a7e6023":"a = nn.Parameter(a); a","f0ff3814":"def update():\n    y_hat = x@a\n    loss = mse(y, y_hat)\n    if t % 10 == 0: print(loss)\n    loss.backward()\n    with torch.no_grad():\n        a.sub_(lr * a.grad)\n        a.grad.zero_()","359fea73":"lr = 1e-1\nfor t in range(100): update()","68e353e9":"plt.scatter(x[:,0],y)\nplt.scatter(x[:,0],x@a);","f8f39756":"from matplotlib import animation, rc\nrc('animation', html='jshtml')","827d842f":"a = nn.Parameter(tensor(-1.,1))\n\nfig = plt.figure()\nplt.scatter(x[:,0], y, c='orange')\nline, = plt.plot(x[:,0], x@a)\nplt.close()\n\ndef animate(i):\n    update()\n    line.set_ydata(x@a)\n    return line,\n\nanimation.FuncAnimation(fig, animate, np.arange(0, 100), interval=20)","3cbec469":"`tensor` simply means array.\n`torch.ones(x,y)` means create a tensor with x rows and y columns with all the values in every column being one. So `torch.ones(2,2)` will create `[[1,1][1,1]]`.\n`x[:,0].uniform_(-1.,1)` means that for every single row (: represents the rows), column 0 (`x[:,0]`), replace the value of column 0 with a uniform random number between (-1.,1). We add a `.` at the end of `-1` because we want Python to understand that it is of type `float` not of type `int`.","f5e41d8a":"\nSo far we have specified the *model* (linear regression) and the *evaluation criteria* (or loss function). Now we need to handle *optimization*; that is, how do we find the best values for a? How do we find the best *fitting* linear regression.\n","73de159c":"There are 4 possibilities and then we can calculate the loss for each of those 4 possibilities and see what works. Did lifting it up or down make it better? Did tilting it more positive or more negative make it better? And then all we do is we say, okay, whichever one of those made it better, that's what we're going to do.\n\nWe don't actually have to move it up and down, and round about. We can actually calculate the derivative. The derivative is the thing that tells us would moving it up or down make it better, or would rotating it this way or that way make it better.\nThe derivative tells us how changing a<sub>1<\/sub> up or down would change our MSE, how changing a<sub>2<\/sub> up or down would change our MSE, and this does it more quickly than actually moving it up and down.","7c9e547b":"Let's now take a look at this as an animation. This is one of the nice things that you can do with matplotlib. We can take any plot and turn it into an animation. So we can now actually see it updating each step.","c9e2a40c":"In practice, we don't calculate on the whole file at once, but we use mini-batches.\n\n## Vocab\n\n\n- **Learning rate**: A thing we multiply our gradient by to decide how much to update the weights by.\n\n- **Epoch**: One complete run through all of our data points (e.g. all of our images). So for the non-stochastic gradient descent we just did, every single loop, we did the entire dataset. But if we've got a dataset with a thousand images and our mini-batch size is 100, then it would take us 10 iterations to see every image once. So that would be one epoch. Epochs are important because if we do lots of epochs, then we are looking at our images lots of times, so every time we see an image, there's a bigger chance of overfitting. So we generally don't want to do too many epochs.\n\n- **Mini-batch**: A random bunch of points that we use to update our weights.\n\n- **SGD**: Stochastic gradient descent using mini-batches.\n\n- **Model \/ Architecture**: They kind of mean the same thing. In this case, our architecture is <img src=\"http:\/\/latex.codecogs.com\/gif.latex?\\vec{y}&space;=&space;X\\vec{a}\" title=\"\\vec{y} = X\\vec{a}\" \/>\ufe63 the architecture is the mathematical function that we're fitting the parameters to.\n\n- **Parameters \/ Coefficients \/ Weights**: Numbers that we are updating.\n\n- **Loss function**: The thing that's telling us how far away or how close we are to the correct answer. For classification problems, we use *cross entropy loss*, also known as *negative log likelihood loss*. This penalizes incorrect confident predictions, and correct unconfident predictions.\n\nFor classification problems, we use *cross entropy loss*, also known as *negative log likelihood loss*. This penalizes incorrect confident predictions, and correct unconfident predictions.","4eb6155f":"Gradient Descent is basically about taking this guess and trying to make it a little bit better. How do we make it a little better? Well, there are only two numbers and the two numbers are the two numbers are the intercept of the orange line a<sub>2<\/sub> and the gradient of the orange line a<sub>1<\/sub>. So what we are going to do with gradient descent is we're going to simply say:\n\n    What if we changed those two numbers a little bit?\n    What if we made the intercept a little bit higher or a little bit lower?\n    What if we made the gradient a little bit more positive or a little bit more negative?\n","ca2daf50":"In this part of the lecture we explain Stochastic Gradient Descent (SGD) which is an **optimization** method commonly used in neural networks. We will illustrate the concepts with concrete examples.","bb183aff":"These models \/ predictors \/ teddy bear classifiers are functions that take pixel values and return probabilities. They start with some functional form like <img src=\"http:\/\/latex.codecogs.com\/gif.latex?\\vec{y}&space;=&space;X\\vec{a}\" title=\"\\vec{y} = X\\vec{a}\" \/> and they fit the parameter `a` using SGD to try and do the best to calculate our predictions.","a3a664f5":"## Gradient Descent\n\nWe would like to find the values of a that minimize mse_loss.\n\n**Gradient descent** is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function. This iterative minimization is achieved by taking steps in the negative direction of the function gradient.\n\nHere is gradient descent implemented in [PyTorch](https:\/\/pytorch.org).","c1469dd7":"We create a scatter plot `fig = plt.figure(); plt.scatter(x[:,0], y, c='orange')`, but rather than having a loop, we used matplotlib's `FuncAnimation` to call 100 times the `animate` function, which calls the `update` function we created earlier and then updates the y data in our line `line, = plt.plot(x[:,0], x@a)`. Repeat that 100 times `np.arange(0, 100)`, waiting 20 milliseconds after each one `interval=20`.","6f7e0044":"You want to find **parameters** (weights) a such that you minimize the error between the points and the line x@a. Note that here a is unknown. For a regression problem the most common error function or loss function is the **mean squared error**.","f107a23a":"`mse` is a loss function. This is something that tells us how good our line is. Now we have to come up with what is the line that fits through the scatterplot (line of best fit). We are going to pretend we don't know. So what we actually have to do is guess what the values of a<sub>1<\/sub> and a<sub>2<\/sub> in a = tensor(a<sub>1<\/sub>, a<sub>2<\/sub>) are. So let's say we guess that a<sub>1<\/sub> and a<sub>2<\/sub> are -1 and 1 respectively i.e. `a = (-1.0,1.0)` then we can compute `y_hat` which is our prediction and then compute our error.","5b0a9476":" a = tensor(a<sub>1<\/sub>, a<sub>2<\/sub>) is an array that represents the values of the slope(gradient) and y-intercept in y<sub>i<\/sub> = a<sub>1<\/sub>x<sub>i,1<\/sub> + a<sub>2<\/sub>x<sub>i,2<\/sub> where x<sub>i,2<\/sub> (i.e. the equation of a straight line `y = mx + c`).","71634057":"For this regression, our loss is 6.7842.\n\nSo we can now plot a scatter plot of x against y and we can plot the scatter plot of x against y_hat.","b388df25":"We now calculate our loss by first obtaining our predictions `y_hat` and then getting the **mean squared error**","e280c95a":"`y = x@a + torch.rand(n)` means perform matrix multiplication and then add some noise to each matrix multiplication result.","ec7b3acb":"[Lesson Video Link](https:\/\/course.fast.ai\/videos\/?lesson=2)\n\n[Lesson resources and updates](https:\/\/forums.fast.ai\/t\/lesson-2-official-resources-and-updates\/28630)\n\n[Lesson chat](https:\/\/forums.fast.ai\/t\/lesson-2-chat\/28722)\n\n[Further discussion thread](https:\/\/forums.fast.ai\/t\/lesson-2-further-discussion\/28706)\n\nNote: This is a mirror of the FastAI Lesson 2 Nb. \nPlease thank the amazing team behind fast.ai for creating these, I've merely created a mirror of the same here\nFor complete info on the course, visit course.fast.ai","e8c0fa6c":"## Linear Regression problem\nThe goal of linear regression is to fit a line to a set of points.","c1fe891d":"`x@a` represents matrix multiplication. \n\n`torch.rand(n)` returns a tensor filled with `n`  tensor filled with random numbers from a uniform distribution on the interval `[0,1)` e.g. `torch.rand(4)`\nreturns `tensor([ 0.5204,  0.2503,  0.3525,  0.5673])`.","d2af2332":"`plt.scatter(x[:,0], y);` plots every x column 0 on the x-axis against y on the y-axis.","6684d4d7":"We calculate the gradient using `.grad`.\nWe are going to create a loop. We're going to loop through 100 times, and we're going to call a function called `update`. That function is going to:\n\n    Calculate y_hat (i.e. our prediction)\n\n    Calculate loss (i.e. our mean squared error)\n\n    From time to time, it will print that out so we can see how we're going\n\n    Calculate the gradient. In PyTorch, calculating the gradient is done by using a method called `backward`. Mean squared error (`loss = mse(y, y_hat)`) is just a simple standard mathematical function. PyTorch keeps track of how it was calculated and lets us calculate the derivative (`loss.backward()`). So if we do a mathematical operation on a tensor in PyTorch, we can call backward to calculate the derivative and the derivative gets stuck inside an attribute called `.grad`.\n\n    We will take our coefficient `a`and subtract from them our gradient (sub_). There is an underscore there because that's going to do it in-place. It's going to actually update those coefficients a to subtract the gradients from them. Why do we subtract? Because the gradient tells us if we move the whole thing downwards, the loss goes up. If we move the whole thing upwards, the loss goes down. So we want to do the opposite of the thing that makes it go up. We want our loss to be small. That's why we subtract.\n\n    lr is our learning rate. All it is is the thing that we multiply by the gradient.\n","2e696e17":"## **Animate it!**"}}