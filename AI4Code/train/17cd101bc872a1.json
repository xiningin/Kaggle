{"cell_type":{"f5ded1ff":"code","06cfc30d":"code","041fb184":"code","38917bee":"code","60b2aa4b":"code","0a483e6d":"code","1de230bf":"code","ef231bda":"code","d27698b1":"code","ddd1e897":"code","2ccd7aad":"code","008d668f":"code","2c6083f5":"code","083cc758":"code","298b867e":"code","4aa00510":"code","84455e95":"code","6fe881b2":"code","13cf8602":"code","c544da71":"code","b3f6bbe4":"code","20485d37":"code","9f87f4e0":"code","947de860":"code","8f172fbf":"code","0f659278":"code","bd45b494":"code","5489dc12":"code","0df87593":"code","4702ebb7":"code","bbdc46b6":"code","dceb0cff":"code","a4240917":"code","c7db09a6":"code","f1cfda87":"code","853f59a3":"code","89d75727":"code","385429c0":"code","78364054":"code","1218ea75":"code","20041ec7":"code","7b1b35e7":"code","74c6a244":"code","fd5922a2":"code","06c14132":"code","a8cb72bc":"code","7f311175":"code","e8e912c2":"code","af65ab8c":"code","00e41919":"code","db228597":"code","2b1d1b1d":"code","2602b5f1":"code","59529062":"code","59306430":"code","34e521c4":"code","14818293":"code","20e579cc":"code","c7bc177c":"code","5d87023f":"code","220f2024":"code","1a55f55a":"code","76f81fb2":"code","477b7237":"code","8820f642":"code","000e33c2":"code","498ec387":"code","162131da":"code","7d201a6f":"code","26d579ae":"code","72d22e2b":"code","3e4d5181":"code","44b12ee3":"code","10768f93":"code","35a91a11":"code","1b3b7262":"code","00592891":"code","cf1e74a3":"code","b0aa8880":"code","2186de27":"code","bde59749":"code","6530765f":"code","cce541f1":"code","d3ba54c0":"code","8171f6f5":"code","20c99fb4":"code","20ac8369":"code","21620af0":"code","b7eae9fc":"code","0b760e75":"code","73a2ad58":"code","2202fdfc":"code","fd2b592e":"markdown","b2228599":"markdown","4a335f88":"markdown","51ef50bf":"markdown","5dedf5cb":"markdown","df098044":"markdown","916bd36b":"markdown","6d2fafbc":"markdown","6028f165":"markdown","1b998f33":"markdown","c0078f8a":"markdown","8b56a292":"markdown","b468f4b2":"markdown","1921c633":"markdown","1d66e393":"markdown","0f208aea":"markdown","dd560624":"markdown","49e08835":"markdown","67bfe589":"markdown","bf507707":"markdown","b16e30e0":"markdown","8a727ce3":"markdown","1c6902d4":"markdown","ed14aee1":"markdown","bab00d8d":"markdown","4130836e":"markdown","babc1a02":"markdown","a8556c03":"markdown","3fe1b816":"markdown","739f35ce":"markdown","42b39316":"markdown","1cbc7a9f":"markdown","8c68aaab":"markdown","111e352a":"markdown","b3793c43":"markdown","52ee76c6":"markdown","7327379f":"markdown"},"source":{"f5ded1ff":"import gc\ngc.collect()","06cfc30d":"import pandas as pd \nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier  \nfrom xgboost.sklearn import XGBRegressor\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer\n\nfrom scipy.special import boxcox1p, inv_boxcox1p\nfrom scipy.stats import skew,norm\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n\nimport warnings\nwarnings.filterwarnings('ignore')","041fb184":"def countOutlier(df_in, col_name):\n    if df_in[col_name].nunique() > 2:\n        orglength = len(df_in[col_name])\n        q1 = df_in[col_name].quantile(0.00)\n        q3 = df_in[col_name].quantile(0.95)\n        iqr = q3-q1 #Interquartile range \n        fence_low  = q1-1.5*iqr \n        fence_high = q3+1.5*iqr \n        df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n        newlength = len(df_out[col_name])\n        return round(100 - (newlength*100\/orglength),2)  \n    else:\n        return 0\n\ndef drop_columns(dataframe, axis =1, percent=0.3):\n    '''\n    * drop_columns function will remove the rows and columns based on parameters provided.\n    * dataframe : Name of the dataframe  \n    * axis      : axis = 0 defines drop rows, axis =1(default) defines drop columns    \n    * percent   : percent of data where column\/rows values are null,default is 0.3(30%)\n    '''\n    df = dataframe.copy()\n    ishape = df.shape\n    if axis == 0:\n        rownames = df.transpose().isnull().sum()\n        rownames = list(rownames[rownames.values > percent*len(df)].index)\n        df.drop(df.index[rownames],inplace=True) \n        print(\"\\nNumber of Rows dropped\\t: \",len(rownames))\n    else:\n        colnames = (df.isnull().sum()\/len(df))\n        colnames = list(colnames[colnames.values>=percent].index)\n        df.drop(labels = colnames,axis =1,inplace=True)        \n        print(\"Number of Columns dropped\\t: \",len(colnames))\n        \n    print(\"\\nOld dataset rows,columns\",ishape,\"\\nNew dataset rows,columns\",df.shape)\n\n    return df\n\ndef correlation(df, dftest, threshold):\n    col_corr = set() # Set of all the names of deleted columns\n    corr_matrix = df.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n                colname = corr_matrix.columns[i] # getting the name of column\n                col_corr.add(colname)\n                if colname in df.columns:\n                    del df[colname] # deleting the column from the train dataset\n                    del dftest[colname] # deleting the column from the test dataset\n\n    print(df.shape)\n    print(dftest.shape)","38917bee":"df_all = pd.read_csv('..\/input\/prudential-life-insurance-assessment\/train.csv')\ntest_all = pd.read_csv('..\/input\/prudential-life-insurance-assessment\/test.csv')","60b2aa4b":"df_all.head()","0a483e6d":"df_all.shape","1de230bf":"df_all.columns","ef231bda":"df_all['Response'].value_counts()","d27698b1":"df_all.describe()","ddd1e897":"df_all.dtypes","2ccd7aad":"df_all.shape","008d668f":"#Checking null values in each cols.\n(df_all.isnull().sum()*100\/df_all.shape[0]).sort_values(ascending=False)","2c6083f5":"#Checking null values in each cols.\n(test_all.isnull().sum()*100\/df_all.shape[0]).sort_values(ascending=False)","083cc758":"# Plotting null values for first 13 columns\nisna_train = df_all.isnull().sum().sort_values(ascending=False)\nisna_train[:13].plot(kind='bar')","298b867e":"# Plotting null values for first 13 columns\nisna_train = test_all.isnull().sum().sort_values(ascending=False)\nisna_train[:13].plot(kind='bar')","4aa00510":"df_all = drop_columns(df_all, axis =1, percent=0.3)\ntest_all = drop_columns(test_all, axis =1, percent=0.3)","84455e95":"#Checking null values in each cols.\n(df_all.isnull().sum()*100\/df_all.shape[0]).sort_values(ascending=False)","6fe881b2":"#Checking null values in each cols.\n(test_all.isnull().sum()*100\/test_all.shape[0]).sort_values(ascending=False)","13cf8602":"df_all.Medical_History_1.value_counts()","c544da71":"df_all.Employment_Info_4.value_counts()","b3f6bbe4":"df_all.Employment_Info_6.value_counts()","20485d37":"df_all.Employment_Info_1.value_counts()","9f87f4e0":"df_all.Employment_Info_1.fillna(0, inplace=True)\ndf_all.Employment_Info_6.fillna(1, inplace=True)\ndf_all.Employment_Info_4.fillna(0, inplace=True)\ndf_all.Medical_History_1.fillna(1, inplace=True)\n\ntest_all.Employment_Info_1.fillna(0, inplace=True)\ntest_all.Employment_Info_6.fillna(1, inplace=True)\ntest_all.Employment_Info_4.fillna(0, inplace=True)\ntest_all.Medical_History_1.fillna(1, inplace=True)","947de860":"#Checking null values in each cols.\n(df_all.isnull().sum()*100\/df_all.shape[0]).sort_values(ascending=False)","8f172fbf":"#Checking null values in each cols.\n(test_all.isnull().sum()*100\/test_all.shape[0]).sort_values(ascending=False)","0f659278":"df_all.shape","bd45b494":"test_all.shape","5489dc12":"print(df_all.isnull().values.any())\nprint(test_all.isnull().values.any())","0df87593":"sns.set_color_codes()\nplt.figure(figsize=(8,8))\nsns.countplot(df_all.Response).set_title('Dist of Response variables')","4702ebb7":"f, axes = plt.subplots(1, 2, figsize=(10,5))\nsns.boxplot(x = 'BMI', data=df_all,  orient='v' , ax=axes[0])\nsns.distplot(df_all['BMI'],  ax=axes[1])","bbdc46b6":"countOutlier(df_all,'BMI')","dceb0cff":"f, axes = plt.subplots(1, 2, figsize=(10,5))\nsns.boxplot(x = 'Ins_Age', data=df_all,  orient='v' , ax=axes[0])\nsns.distplot(df_all['Ins_Age'],  ax=axes[1])","a4240917":"countOutlier(df_all,'Ins_Age')","c7db09a6":"f, axes = plt.subplots(1, 2, figsize=(10,5))\nsns.boxplot(x = 'Ht', data=df_all,  orient='v' , ax=axes[0])\nsns.distplot(df_all['Ht'],  ax=axes[1])","f1cfda87":"countOutlier(df_all,'Ht')","853f59a3":"f, axes = plt.subplots(1, 2, figsize=(10,5))\nsns.boxplot(x = 'Wt', data=df_all,  orient='v' , ax=axes[0])\nsns.distplot(df_all['Wt'],  ax=axes[1])","89d75727":"countOutlier(df_all,'Wt')","385429c0":"#1 Multiplication of BMI and Age - higher the factor, higher the risk\ndf_all['bmi_age'] = df_all['BMI'] * df_all['Ins_Age']\n\n#2 Multiplication of Weight and Age - higher the factor, higher the risk\ndf_all['age_wt'] = df_all['Ins_Age'] * df_all['Wt']\n\n#3 Multiplication of Height and Age - higher the factor, higher the risk\ndf_all['age_ht'] = df_all['Ins_Age'] * df_all['Ht']\n\n#4 Add all values of Medical Keywords Columns\nmed_keyword_columns = df_all.columns[df_all.columns.str.startswith('Medical_Keyword_')]\ndf_all['med_keywords_count'] = df_all[med_keyword_columns].sum(axis=1)\ndf_all.drop(med_keyword_columns, axis=1, inplace=True)\n\n#5 BMI Categorization\nconditions = [\n    (df_all['BMI'] <= df_all['BMI'].quantile(0.25)),\n    (df_all['BMI'] > df_all['BMI'].quantile(0.25)) & (df_all['BMI'] <= df_all['BMI'].quantile(0.75)),\n    (df_all['BMI'] > df_all['BMI'].quantile(0.75))]\nchoices = ['under_weight', 'average', 'overweight']\ndf_all['bmi_wt'] = np.select(conditions, choices)\n\n#6 Age Categorization\nconditions = [\n    (df_all['Ins_Age'] <= df_all['Ins_Age'].quantile(0.25)),\n    (df_all['Ins_Age'] > df_all['Ins_Age'].quantile(0.25)) & (df_all['Ins_Age'] <= df_all['Ins_Age'].quantile(0.75)),\n    (df_all['Ins_Age'] > df_all['Ins_Age'].quantile(0.75))]\nchoices = ['young', 'average', 'old']\ndf_all['age_cat'] = np.select(conditions, choices)\n\n#7 Height Categorization\nconditions = [\n    (df_all['Ht'] <= df_all['Ht'].quantile(0.25)),\n    (df_all['Ht'] > df_all['Ht'].quantile(0.25)) & (df_all['Ht'] <= df_all['Ht'].quantile(0.75)),\n    (df_all['Ht'] > df_all['Ht'].quantile(0.75))]\nchoices = ['short', 'average', 'tall']\ndf_all['ht_cat'] = np.select(conditions, choices)\n\n#8 Weight Categorization\nconditions = [\n    (df_all['Wt'] <= df_all['Wt'].quantile(0.25)),\n    (df_all['Wt'] > df_all['Wt'].quantile(0.25)) & (df_all['Wt'] <= df_all['Wt'].quantile(0.75)),\n    (df_all['Wt'] > df_all['Wt'].quantile(0.75))]\nchoices = ['thin', 'average', 'fat']\ndf_all['wt_cat'] = np.select(conditions, choices)","78364054":"#1 Multiplication of BMI and Age - higher the factor, higher the risk\ntest_all['bmi_age'] = test_all['BMI'] * test_all['Ins_Age']\n\n#2 Multiplication of Weight and Age - higher the factor, higher the risk\ntest_all['age_wt'] = test_all['Ins_Age'] * test_all['Wt']\n\n#3 Multiplication of Height and Age - higher the factor, higher the risk\ntest_all['age_ht'] = test_all['Ins_Age'] * test_all['Ht']\n\n#4 Add all values of Medical Keywords Columns\nmed_keyword_columns = test_all.columns[test_all.columns.str.startswith('Medical_Keyword_')]\ntest_all['med_keywords_count'] = test_all[med_keyword_columns].sum(axis=1)\ntest_all.drop(med_keyword_columns, axis=1, inplace=True)\n\n#5 BMI Categorization\nconditions = [\n    (test_all['BMI'] <= test_all['BMI'].quantile(0.25)),\n    (test_all['BMI'] > test_all['BMI'].quantile(0.25)) & (test_all['BMI'] <= test_all['BMI'].quantile(0.75)),\n    (test_all['BMI'] > test_all['BMI'].quantile(0.75))]\nchoices = ['under_weight', 'average', 'overweight']\ntest_all['bmi_wt'] = np.select(conditions, choices)\n\n#6 Age Categorization\nconditions = [\n    (test_all['Ins_Age'] <= test_all['Ins_Age'].quantile(0.25)),\n    (test_all['Ins_Age'] > test_all['Ins_Age'].quantile(0.25)) & (test_all['Ins_Age'] <= test_all['Ins_Age'].quantile(0.75)),\n    (test_all['Ins_Age'] > test_all['Ins_Age'].quantile(0.75))]\nchoices = ['young', 'average', 'old']\ntest_all['age_cat'] = np.select(conditions, choices)\n\n#7 Height Categorization\nconditions = [\n    (test_all['Ht'] <= test_all['Ht'].quantile(0.25)),\n    (test_all['Ht'] > test_all['Ht'].quantile(0.25)) & (test_all['Ht'] <= test_all['Ht'].quantile(0.75)),\n    (test_all['Ht'] > test_all['Ht'].quantile(0.75))]\nchoices = ['short', 'average', 'tall']\ntest_all['ht_cat'] = np.select(conditions, choices)\n\n#8 Weight Categorization\nconditions = [\n    (test_all['Wt'] <= test_all['Wt'].quantile(0.25)),\n    (test_all['Wt'] > test_all['Wt'].quantile(0.25)) & (test_all['Wt'] <= test_all['Wt'].quantile(0.75)),\n    (test_all['Wt'] > test_all['Wt'].quantile(0.75))]\nchoices = ['thin', 'average', 'fat']\ntest_all['wt_cat'] = np.select(conditions, choices)","1218ea75":"def new_target(row):\n    if (row['bmi_wt']=='overweight') or (row['age_cat']=='old')  or (row['wt_cat']=='fat'):\n        val='extremely_risky'\n    else:\n        val='not_extremely_risky'\n    return val\n\ndf_all['extreme_risk'] = df_all.apply(new_target,axis=1)\ntest_all['extreme_risk'] = test_all.apply(new_target,axis=1)","20041ec7":"df_all.extreme_risk.value_counts()","7b1b35e7":"# Risk Categorization\nconditions1 = [\n    (df_all['bmi_wt'] == 'overweight') ,\n    (df_all['bmi_wt'] == 'average') ,\n    (df_all['bmi_wt'] == 'under_weight') ]\nconditions2 = [\n    (test_all['bmi_wt'] == 'overweight') ,\n    (test_all['bmi_wt'] == 'average') ,\n    (test_all['bmi_wt'] == 'under_weight') ]\nchoices = ['risk', 'non-risk', 'risk']\ndf_all['bmi_risk'] = np.select(conditions1, choices)\ntest_all['bmi_risk'] = np.select(conditions2, choices)","74c6a244":"df_all.bmi_risk.value_counts()","fd5922a2":"def new_target(row):\n    if (row['bmi_wt']=='average') or (row['age_cat']=='average')  or (row['wt_cat']=='average') or (row['ht_cat']=='average'):\n        val='average'\n    else:\n        val='non_average'\n    return val\n\ndf_all['average_risk'] = df_all.apply(new_target,axis=1)\ntest_all['average_risk'] = test_all.apply(new_target,axis=1)","06c14132":"df_all.average_risk.value_counts()","a8cb72bc":"def new_target(row):\n    if (row['bmi_wt']=='under_weight') or (row['age_cat']=='young')  or (row['wt_cat']=='thin') or (row['ht_cat']=='short'):\n        val='low_end'\n    else:\n        val='non_low_end'\n    return val\ndf_all['low_end_risk'] = df_all.apply(new_target,axis=1)\ntest_all['low_end_risk'] = test_all.apply(new_target,axis=1)","7f311175":"df_all.low_end_risk.value_counts()","e8e912c2":"def new_target(row):\n    if (row['bmi_wt']=='overweight') or (row['age_cat']=='old')  or (row['wt_cat']=='fat') or (row['ht_cat']=='tall'):\n        val='high_end'\n    else:\n        val='non_high_end'\n    return val\ndf_all['high_end_risk'] = df_all.apply(new_target,axis=1)\ntest_all['high_end_risk'] = test_all.apply(new_target,axis=1)","af65ab8c":"df_all.high_end_risk.value_counts()","00e41919":"df_all.shape","db228597":"test_all.shape","2b1d1b1d":"print(df_all.isnull().values.any())\nprint(test_all.isnull().values.any())","2602b5f1":"df_all.columns","59529062":"#Delting all values of Medical History Columns having Value is biased more than 81%\nmed_keyword_columns = test_all.columns[test_all.columns.str.startswith('Medical_History_')]\nfor col in med_keyword_columns:\n    print(\"Dropping column: \" +col)\n    print(df_all[col].value_counts(normalize=True) * 100)\n    \ndrop_columns = ['Medical_History_3','Medical_History_5','Medical_History_6','Medical_History_7','Medical_History_9',\n               'Medical_History_11','Medical_History_12','Medical_History_13','Medical_History_14',\n               'Medical_History_16','Medical_History_17','Medical_History_18','Medical_History_19',\n               'Medical_History_20','Medical_History_22','Medical_History_27','Medical_History_30',\n               'Medical_History_31','Medical_History_33','Medical_History_34','Medical_History_35',\n               'Medical_History_36','Medical_History_37','Medical_History_38','Medical_History_39',]     \ndf_all.drop(drop_columns, axis=1, inplace=True)\ntest_all.drop(drop_columns, axis=1, inplace=True)\nprint(df_all.shape)\nprint(test_all.shape)","59306430":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'extreme_risk', hue = 'Response', data = df_all)","34e521c4":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'average_risk', hue = 'Response', data = df_all)","14818293":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'low_end_risk', hue = 'Response', data = df_all)","20e579cc":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'high_end_risk', hue = 'Response', data = df_all)","c7bc177c":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'bmi_wt', hue = 'Response', data = df_all)","5d87023f":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'age_cat', hue = 'Response', data = df_all)","220f2024":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'wt_cat', hue = 'Response', data = df_all)","1a55f55a":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'ht_cat', hue = 'Response', data = df_all)","76f81fb2":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'bmi_risk', hue = 'Response', data = df_all)","477b7237":"df_all.shape","8820f642":"test_all.shape","000e33c2":"print(df_all.isnull().values.any())\nprint(test_all.isnull().values.any())","498ec387":"df_train = df_all\ndf_test = test_all","162131da":"df_train.head()","7d201a6f":"df_train.drop(['Id'], axis=1, inplace=True)","26d579ae":"df_train.dtypes","72d22e2b":"df_train.columns","3e4d5181":"# Categorical boolean mask\ncategorical_feature_mask = df_train.dtypes=='object'\n# filter categorical columns using mask and turn it into a list\ncategorical_cols = df_train.columns[categorical_feature_mask].tolist()\ncategorical_cols","44b12ee3":"# import labelencoder\nfrom sklearn.preprocessing import LabelEncoder\n# instantiate labelencoder object\nle = LabelEncoder()","10768f93":"# apply le on categorical feature columns\ndf_train[categorical_cols] = df_train[categorical_cols].apply(lambda col: le.fit_transform(col))\ndf_train[categorical_cols].head(10)","35a91a11":"df_test[categorical_cols] = df_test[categorical_cols].apply(lambda col: le.fit_transform(col))\ndf_test[categorical_cols].head(10)","1b3b7262":"correlation(df_train,df_test,0.90)","00592891":"X = df_train.drop(['Response'], axis=1)\ny = df_train['Response']\n\n# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)","cf1e74a3":"gc.collect()","b0aa8880":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [8,12,16]\n    #'min_samples_leaf': range(30, 50),\n    #'min_samples_split': range(20, 40),\n    #'n_estimators': [300,500], \n    #'max_features': [24,48]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\nrf = GridSearchCV(rf, param_grid, cv=5, scoring=\"accuracy\", return_train_score = True)\n\nrf.fit(X_train, y_train)\nprint('We can get accuracy of ',rf.best_score_,' using ',rf.best_params_)\nscores = rf.cv_results_\n\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n       scores[\"mean_train_score\"], \n        label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","2186de27":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    #'max_depth': [8,12,16],\n    'min_samples_leaf': range(20,100,10)\n    #'min_samples_split': range(20, 40),\n    #'n_estimators': [300,500], \n    #'max_features': [24,48]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\nrf = GridSearchCV(rf, param_grid, cv=5, scoring=\"accuracy\", return_train_score = True)\n\nrf.fit(X_train, y_train)\nprint('We can get accuracy of ',rf.best_score_,' using ',rf.best_params_)\nscores = rf.cv_results_\n\nplt.figure()\nplt.plot(scores[\"param_min_samples_leaf\"], \n       scores[\"mean_train_score\"], \n        label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","bde59749":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    #'max_depth': [8,12,16],\n    #'min_samples_leaf': range(20,100,10),\n    'min_samples_split': range(20,200,10)\n    #'n_estimators': [300,500], \n    #'max_features': [24,48]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\nrf = GridSearchCV(rf, param_grid, cv=5, scoring=\"accuracy\", return_train_score = True)\n\nrf.fit(X_train, y_train)\nprint('We can get accuracy of ',rf.best_score_,' using ',rf.best_params_)\nscores = rf.cv_results_\n\nplt.figure()\nplt.plot(scores[\"param_min_samples_split\"], \n       scores[\"mean_train_score\"], \n        label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_split\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","6530765f":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    #'max_depth': [8,12,16],\n    #'min_samples_leaf': range(20,100,10),\n    #'min_samples_split': range(20,100,10)\n    #'n_estimators': [200,300,400,500]\n    'max_features': [10,20,30,40,50]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\nrf = GridSearchCV(rf, param_grid, cv=5, scoring=\"accuracy\", return_train_score = True)\n\nrf.fit(X_train, y_train)\nprint('We can get accuracy of ',rf.best_score_,' using ',rf.best_params_)\nscores = rf.cv_results_\n\nplt.figure()\nplt.plot(scores[\"param_max_features\"], \n       scores[\"mean_train_score\"], \n        label=\"training accuracy\")\nplt.plot(scores[\"param_max_features\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_features\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","cce541f1":"rfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=12,\n                             min_samples_leaf=30, \n                             min_samples_split=80,\n                             max_features=30,\n                             n_estimators=600)\nrfc.fit(X_train,y_train)\npredictions = rfc.predict(X_test)\nprint(classification_report(y_test,predictions))\nprint(confusion_matrix(y_test,predictions))","d3ba54c0":"gc.collect()","8171f6f5":"y_train.unique()","20c99fb4":"# As labeles need to be from (0,7] instead of (1 to 8) else xgb.train will fail\ny_train_new = y_train - 1\ny_test_new = y_test - 1","20ac8369":"y_train_new.unique()","21620af0":"n_classes = len(np.unique(y_train))\nn_classes","b7eae9fc":"params1 = {\n    'learning_rate': 0.02,\n    'max_depth': 16, \n    'n_estimators':500,\n    'subsample':0.9,\n    'objective':'multi:softmax',\n    'num_class': n_classes\n}","0b760e75":"params2 = {\n    'silent': False, \n    'learning_rate': 0.6,  \n    'colsample_bytree': 0.4,\n    'subsample': 0.8,\n    'objective': 'multi:softmax', \n    'num_class': n_classes,\n    'n_estimators': 1000, \n    'reg_alpha': 0.3,\n    'max_depth': 32, \n    'gamma': 10\n}","73a2ad58":"dtrain = xgb.DMatrix(data=X_train, label=y_train_new)\ndtest = xgb.DMatrix(data=X_test)\n\n\n# fit model on training data\nxgb_model = xgb.train(params1, dtrain)\n\npred = xgb_model.predict(dtest)\nprint(pred)\n\nprint(classification_report(y_test_new, pred))\nprint(confusion_matrix(y_test_new,pred))","2202fdfc":"dtrain = xgb.DMatrix(data=X_train, label=y_train_new)\ndtest = xgb.DMatrix(data=X_test)\n\n\n# fit model on training data\nxgb_model = xgb.train(params2, dtrain)\n\npred = xgb_model.predict(dtest)\nprint(pred)\n\nprint(classification_report(y_test_new, pred))\nprint(confusion_matrix(y_test_new,pred))","fd2b592e":"- Under average body type ('bmi_wt'=='average' or 'age_cat'=='average'  or 'wt_cat'=='average' or 'ht_cat'=='average'), more polices are issued.","b2228599":"## Common Function","4a335f88":"\"Response\" is the target variable in the data. Let's see the value counts of the target variable","51ef50bf":"## Loading Libraries and Data\n\nLet's first load few libraries","5dedf5cb":"## Data preparation","df098044":"- Under old age category less policies are issued","916bd36b":"## Introduction\n\nIn this Notebook, we will use Prudential Life Insurance's data of its policyholders and build a classifier which will try to predict different classes of its policyholders depending on the underwriting and risk assessment.","6d2fafbc":"- We can see that Class 8 has the highest distribution.","6028f165":"## Feature Engineering","1b998f33":"- Under risky category more often less policies are issued","c0078f8a":"#### Weight Distribution","8b56a292":"Let's plot few variables. These will be helpful in doing some very important feature engineering.","b468f4b2":"Let's drop ID from the data","1921c633":"- Under overweight less policies are issued.","1d66e393":"#### Height Distribution","0f208aea":"#### BMI Distribution","dd560624":"# Data\n\nLet's see how does our data look like.\n\nWe will see first few entries, its shape and its statistical description","49e08835":"## Data Cleaning","67bfe589":"This is perhaps the most important part of this notebook.\n\nBased on industry knowledge, we know that these are high risk policies:\n\n1. Old Age\n2. Obese persons\n3. High BMI\n4. Extremely short or tall persons\n\nWe will therefore create few features such as:\n\n1. Person very old or very young or in middle\n2. Person very short or tall or in middle\n3. Person with very high BMI or low BMI or in middle\n4. Persons with obesity or are very thin or in middle\n\nWe will also create few more features such as:\n\n1. Multiplication of BMI and Age - higher the factor, higher the risk\n2. Multiplication of Weight and Age - higher the factor, higher the risk\n3. Multiplication of Height and Age - higher the factor, higher the risk\n4. Add all values of Medical Keywords Columns\n5. BMI Categorization\n6. Age Categorization\n7. Height Categorization\n8. Weight Categorization","bf507707":"- Under high-end-risk category ('bmi_wt'=='overweight' or 'age_cat'=='old' or 'wt_cat'=='fat' or 'ht_cat'=='tall'), less policies are issued.","b16e30e0":"There are around 128 features and on a very broad level, these can be categorized into:\n\n1. Product Information (boundary conditions)\n2. Age\n3. Height\n4. Weight\n5. BMI\n6. Employment Information\n7. Other insured information\n8. Family History\n9. Medical History\n10. Medical Keywords","8a727ce3":"- Under \"extreme risk\" category (overweight, old & fat), less polices are issued.","1c6902d4":"- Under low-end risk category ('bmi_wt'=='under_weight' or 'age_cat'=='young' or 'wt_cat'=='thin' or 'ht_cat'=='short'), more policies are issued.","ed14aee1":"## Model Building 1 - Rain Forest","bab00d8d":"Let's see if these feature engineering makes sense","4130836e":"- Under tall category less policies are issued","babc1a02":"## EDA","a8556c03":"## Model Building - XgBoost","3fe1b816":"#### Checking null","739f35ce":"#### Filling Null Values ","42b39316":"#### Age Distribution","1cbc7a9f":"- We can see that Class 8 has the highest distribution. We will assume this as clean and accepted policies on standard underwriting terms. Rest other classes can be considered as policies rejected or accepted at extra terms and conditions","8c68aaab":"- Under fat category less policies are issued","111e352a":"#### Checking null again","b3793c43":"#### Response Distribution","52ee76c6":"#### Dropping null values more than 30%","7327379f":"## Removing highly correlated features"}}