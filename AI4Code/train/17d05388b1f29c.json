{"cell_type":{"ee78c295":"code","6e95bea8":"code","49265d67":"code","3bcd99d2":"code","4448cf85":"code","bb65d8ae":"code","d0736278":"code","561b093b":"code","33fe49aa":"code","5d0e3ace":"code","0084f14f":"code","9197814c":"code","8b960dd5":"code","1ed3882b":"markdown","07630e68":"markdown","05b660d6":"markdown","7b2ba936":"markdown","d0db178c":"markdown"},"source":{"ee78c295":"import math\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport seaborn as sns\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, SequentialSampler\nimport pickle\nimport pandas as pd\nimport numpy as np\nimport math\nimport pickle","6e95bea8":"data_path = '..\/input\/pytorch-models-getting-data'","49265d67":"data = pickle.load(open(data_path+'\/preprocessed_data','rb'))\ntrain = data['train']\ntest=data['test']","3bcd99d2":"train.head()","4448cf85":"class Data(Dataset):\n    def __init__(self, df, flip =0):\n        super().__init__()\n        self.df = df.values.tolist()\n        self.flip = flip\n    def __getitem__(self, idx):\n        row = self.df[idx]\n        tensors = torch.as_tensor(row[:-2], dtype = torch.float)\n        segment = torch.as_tensor(row[-3], dtype=torch.long)\n        target = torch.as_tensor(row[-2], dtype=torch.float)\n        if np.random.rand() < self.flip:\n            tensors = tensors.flip(-1)\n        return {\n            'tensors':tensors,\n            'segment':segment,\n            'target':target,\n        }\n    def __len__(self):\n        return len(self.df)","bb65d8ae":"# we can use either the Sine or the Cosine funtion\nclass TimeToVector(nn.Module):\n    def __init__(self, n_inputs, seq_len, n_outputs, act=torch.cos):\n        super(TimeToVector, self).__init__()\n        self.n_inputs = n_inputs\n        self.n_outputs = n_outputs\n        self.seq_len = seq_len\n        self.w_weiht = nn.parameter.Parameter(torch.randn(self.n_inputs, n_outputs-1))\n        self.b_weight = nn.parameter.Parameter(torch.randn(self.seq_len, n_outputs-1))\n        \n        self.w_bias = nn.parameter.Parameter(torch.randn(self.n_inputs, 1))\n        self.b_bias = nn.parameter.Parameter(torch.randn(self.seq_len, 1))\n        self.act  = act\n    def forward(self, inputs):\n        bias = torch.matmul(inputs, self.w_bias) + self.b_bias\n        weights = self.act(torch.matmul(inputs, self.w_weiht) + self.b_weight)\n        return torch.cat([weights, bias], -1)","d0736278":"class Encoder(nn.Module):\n    def __init__(self, n_inputs, d_emb, d_time):\n        super(Encoder, self).__init__()\n        self.act = nn.ReLU()\n        self.rnn = nn.LSTM(n_inputs + d_time , d_emb\/\/2, 3,batch_first=True, bidirectional=True)\n        self.time = TimeToVector(35,80,d_time)\n    def forward(self, input):\n        time = self.time(input)\n        input = torch.cat([input, time], -1)\n        out, hidden = self.rnn(input)\n        return out","561b093b":"#here we will add 10 extra features as time representation\nencoder = Encoder(n_inputs=35, d_emb=512, d_time=10)","33fe49aa":"loader = DataLoader(Data(train), 64)\nfeatures = next(iter(loader))['tensors'].permute(0,2,1)\nfeatures.shape","5d0e3ace":"#here is our outputs that we can feed to the next stage of the model\nencoder(features).shape","0084f14f":"class PositionalEncoder(nn.Module):\n    def __init__(self, d_model: int, max_len: int = 80):\n        super().__init__()\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) \/ d_model))\n        pe = torch.zeros(1, max_len, d_model)\n        pe[0, :, 0::2] = torch.sin(position * div_term)\n        pe[0, :, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n    def forward(self, x):\n        x = x + self.pe[:x.size(0)]\n        return x","9197814c":"class Encoder2(nn.Module):\n    def __init__(self, n_inputs, d_emb):\n        super(Encoder2, self).__init__()\n        self.d_emb = d_emb\n        self.act = nn.ReLU(True)\n        self.position = PositionalEncoder(d_emb)\n        self.rnn = nn.LSTM(n_inputs, d_emb\/\/2, 3,batch_first=True, bidirectional=True)\n        self.norm = nn.LayerNorm(d_emb)\n        \n    def forward(self, input):\n        out, hidden = self.rnn(input)\n        out = out * math.sqrt(self.d_emb) # here we giving the main features -or the main embedding- \n        #extra importance befor adding the position embedding\n        out = self.position(out)\n        out =self.norm(out)\n        return out","8b960dd5":"#here is the output before sending to the next stage\nencoder2 = Encoder2(35, 512)\nencoder2(features).shape","1ed3882b":"# one of the methods by with we can feed our models by benefitable demintion of time is to transform the timesteps to learnable vector that can learn the meaning of the time \n# please check this paper  \nhttps:\/\/openreview.net\/pdf?id=rklklCVYvB","07630e68":"### Here is an exapmle of encoding our features concatenated with the time vector","05b660d6":"## Another way for step representation is using the position embedding like what transformers models use to add to the words embedding to give each word extra meaning depends on its position in the sentence \n","7b2ba936":"# please check my notebook for data preprocessing\nhttps:\/\/www.kaggle.com\/ahmedmoabbas\/pytorch-models-getting-data","d0db178c":"### we can concatenate this time vector with our features befor feeding them to the model or in any stage of the model"}}