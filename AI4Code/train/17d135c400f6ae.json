{"cell_type":{"947fc20e":"code","6c0e48b1":"code","eb8b87a8":"code","88f4001e":"code","b96d9b19":"code","b12b7df1":"code","d59559ce":"code","461ba84b":"code","e82cb078":"code","4e8ee967":"code","882d9b8e":"code","3f4be524":"code","c691fe26":"code","55d1fece":"code","3d9c4ee9":"code","bb9a52d1":"code","942651c3":"code","3459351f":"code","56eb1575":"code","394f5ba4":"code","73c154f8":"code","4039602d":"code","12d4158c":"code","60cfe8ec":"code","e94bad98":"code","387aa7ac":"code","52e38aeb":"code","081f0751":"code","9fd0c41c":"code","89e172aa":"code","b840f786":"code","4990a230":"code","14abad79":"code","40d4f346":"code","2586ac83":"code","0be41b3e":"code","5d78b14a":"code","682de01d":"code","a264ec11":"code","386fb557":"code","f505a54e":"code","7b9483eb":"code","54caa4da":"code","010defa3":"code","89866f60":"code","3ab27a9b":"code","a3e36303":"code","306315b8":"code","f9a7e337":"code","904a6533":"code","ab5b62a1":"code","ec3b7bb7":"code","b40e1300":"code","f04aa28f":"code","252b336c":"code","9cd55268":"code","fc5196a2":"code","12165e51":"code","d17ee777":"code","ceaa4e6a":"markdown","ccd93269":"markdown","f6739ab1":"markdown","4debbe2a":"markdown","8040a230":"markdown","15eb47d2":"markdown","043c0c28":"markdown","172580b6":"markdown","4de0fddc":"markdown","6b0bdc60":"markdown"},"source":{"947fc20e":"import pandas as pd\nimport numpy as np\n\n# for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# for the Q-Q plots\nimport scipy.stats as stats\n\n# to split the datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import preprocessing\n# from feature-engine\nfrom feature_engine import missing_data_imputers as mdi\n\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.preprocessing import StandardScaler,RobustScaler\n","6c0e48b1":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eb8b87a8":"data=pd.read_csv('\/kaggle\/input\/hitters\/Hitters.csv')\ndata","88f4001e":"data.info()","b96d9b19":"data.Salary.unique()","b12b7df1":"fig = data.Salary.hist(bins=50)\n\nfig.set_title(\"Salaries\")\nfig.set_xlabel(\"Salary mount\")\nfig.set_ylabel(\"Number of Salaries\")","d59559ce":"data.groupby(\"Division\")[\"Salary\"].mean()","461ba84b":"data.groupby(\"League\")[\"Salary\"].mean()","e82cb078":"data.groupby(\"League\")[\"Salary\"].median()","4e8ee967":"# we call the imputer from feature-engine\n#we specify the imputation strategy,median in this case\n\nimputer = mdi.MeanMedianImputer(imputation_method='mean',\n                                       variables=['Salary'])\n\nimputer.fit(data)\n\nimputer.variables\n\ntmp = imputer.transform(data)\ntmp.head()","882d9b8e":"data.describe().T","3f4be524":"tmp.describe().T","c691fe26":"from sklearn.neighbors import LocalOutlierFactor\ny=tmp.Salary\nx=tmp.drop(['Salary', 'League','NewLeague','Division'], axis=1, inplace=False)\ncolumns=x.columns.tolist()\nclf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\ny_pred=clf.fit_predict(x)\n\ny_pred\ny_inlier= np.count_nonzero(y_pred == -1)\ny_inlier  #outlier olmayanlar inlier\n\n\nX_score=clf.negative_outlier_factor_\noutlier_score=pd.DataFrame()\noutlier_score['score']=X_score\nwith pd.option_context(\"display.max_rows\", 1000):\n    display(outlier_score.sort_values(by='score', ascending=True))\n","55d1fece":"threshold= -2.129953\nfiltre=outlier_score['score']<threshold\noutlier_index=outlier_score[filtre].index.tolist()","3d9c4ee9":"outlier_index_x=x.drop(outlier_index)\n\noutlier_index_y=y.drop(outlier_index)","bb9a52d1":"tmp=outlier_index_x\ntmp['Salary'] = outlier_index_y\ntmp[\"League\"] = data[\"League\"]\ntmp[\"Division\"] = data[\"Division\"]\ntmp[\"NewLeague\"] = data[\"NewLeague\"]\ntmp","942651c3":"# function to create histogram, Q-Q plot and\n# boxplot. We learned this in section 3 of the course\n\n\ndef diagnostic_plots(df, variable):\n    # function takes a dataframe (df) and\n    # the variable of interest as arguments\n\n    # define figure size\n    plt.figure(figsize=(16, 4))\n\n    # histogram\n    plt.subplot(1, 3, 1)\n    sns.distplot(df[variable], bins=30)\n    plt.title('Histogram')\n\n    # Q-Q plot\n    plt.subplot(1, 3, 2)\n    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n    plt.ylabel('Variable quantiles')\n\n    # boxplot\n    plt.subplot(1, 3, 3)\n    sns.boxplot(y=df[variable])\n    plt.title('Boxplot')\n\n    plt.show()","3459351f":"\ndef find_skewed_boundaries(df, variable):\n    # Let's calculate the boundaries outside which sit the outliers\n    # for skewed distributions\n\n    # distance passed as an argument, gives us the option to\n    # estimate 1.5 times or 3 times the IQR to calculate\n    # the boundaries.\n\n    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n\n    lower_boundary = df[variable].quantile(0.25) - (IQR * distance)\n    upper_boundary = df[variable].quantile(0.75) + (IQR * distance)\n\n\n    return upper_boundary, lower_boundary","56eb1575":"def find_boundaries_with_quantiles(df, variable):\n   \n    #Better Result\n    #the boundaries are the quantiles\n    \n    lower_boundary = df[variable].quantile(0.05)\n    upper_boundary = df[variable].quantile(0.95)\n        \n\n    return upper_boundary, lower_boundary","394f5ba4":"#Find limits for Salary\nRM_upper_limit, RM_lower_limit = find_boundaries_with_quantiles(tmp, 'Salary')\nRM_upper_limit, RM_lower_limit\n\n# let's trimm the dataset\ntmp[tmp[\"Salary\"] > RM_upper_limit] = RM_upper_limit\ntmp[tmp[\"Salary\"] < RM_lower_limit] = RM_lower_limit","73c154f8":"RM_upper_limit, RM_lower_limit = find_boundaries_with_quantiles(tmp, 'Years')\nRM_upper_limit, RM_lower_limit\n\ntmp[tmp[\"Years\"] > RM_upper_limit] = RM_upper_limit\ntmp[tmp[\"Years\"] < RM_lower_limit] = RM_lower_limit","4039602d":"RM_upper_limit, RM_lower_limit = find_boundaries_with_quantiles(tmp, 'Errors')\nRM_upper_limit, RM_lower_limit\n\n# let's trimm the dataset\ntmp[tmp[\"Errors\"] > RM_upper_limit] = RM_upper_limit\ntmp[tmp[\"Errors\"] < RM_lower_limit] = RM_lower_limit\n","12d4158c":"RM_upper_limit, RM_lower_limit = find_boundaries_with_quantiles(tmp, 'PutOuts')\nRM_upper_limit, RM_lower_limit\n\n# let's trimm the dataset\ntmp[tmp[\"PutOuts\"] > RM_upper_limit] = RM_upper_limit\ntmp[tmp[\"PutOuts\"] < RM_lower_limit] = RM_lower_limit\n","60cfe8ec":"CRM_upper_limit, RM_lower_limit = find_boundaries_with_quantiles(tmp, 'CHmRun')\nRM_upper_limit, RM_lower_limit\n\n\ntmp[tmp[\"CHmRun\"] > RM_upper_limit] = RM_upper_limit\ntmp[tmp[\"CHmRun\"] < RM_lower_limit] = RM_lower_limit","e94bad98":"RM_upper_limit, RM_lower_limit = find_boundaries_with_quantiles(tmp, 'HmRun')\nRM_upper_limit, RM_lower_limit\n\n# let's trimm the dataset\n\ntmp[tmp[\"HmRun\"] > RM_upper_limit] = RM_upper_limit\ntmp[tmp[\"HmRun\"] < RM_lower_limit] = RM_lower_limit","387aa7ac":"RM_upper_limit, RM_lower_limit = find_boundaries_with_quantiles(tmp, 'Assists')\nRM_upper_limit, RM_lower_limit\n\n# let's trimm the dataset\ntmp[tmp[\"Assists\"] > RM_upper_limit] = RM_upper_limit\ntmp[tmp[\"Assists\"] < RM_lower_limit] = RM_lower_limit\n","52e38aeb":"RM_upper_limit, RM_lower_limit = find_boundaries_with_quantiles(tmp, 'Runs')\nRM_upper_limit, RM_lower_limit\n\n# let's trimm the dataset\ntmp[tmp[\"Runs\"] > RM_upper_limit] = RM_upper_limit\ntmp[tmp[\"Runs\"] < RM_lower_limit] = RM_lower_limit","081f0751":"RM_upper_limit, RM_lower_limit = find_boundaries_with_quantiles(tmp, 'Errors')\nRM_upper_limit, RM_lower_limit\n\n# let's trimm the dataset\ntmp[tmp[\"Errors\"] > RM_upper_limit] = RM_upper_limit\ntmp[tmp[\"Errors\"] < RM_lower_limit] = RM_lower_limit","9fd0c41c":"RM_upper_limit, RM_lower_limit = find_boundaries_with_quantiles(tmp, 'CRBI')\nRM_upper_limit, RM_lower_limit\n\n# let's trimm the dataset\ntmp[tmp[\"CRBI\"] > RM_upper_limit] = RM_upper_limit\ntmp[tmp[\"CRBI\"] < RM_lower_limit] = RM_lower_limit","89e172aa":"RM_upper_limit, RM_lower_limit = find_boundaries_with_quantiles(tmp, 'Hits')\nRM_upper_limit, RM_lower_limit\n\n# let's trimm the dataset\ntmp[tmp[\"Hits\"] > RM_upper_limit] = RM_upper_limit\ntmp[tmp[\"Hits\"] < RM_lower_limit] = RM_lower_limit","b840f786":"tmp = pd.get_dummies(tmp, columns = ['League', 'Division', 'NewLeague'], drop_first = True)\ntmp.head()","4990a230":"# Separate into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n            tmp.drop(['Salary'], axis=1),\n            tmp['Salary'], test_size=0.2, random_state=46)\n\nX_train.shape,X_test.shape,","14abad79":"train_t= X_train\ntest_t= X_test\n","40d4f346":"train_t.shape,test_t.shape","2586ac83":"from sklearn.preprocessing import StandardScaler,RobustScaler\n\n# let's scale the features\nscaler = RobustScaler()\nscaler.fit(train_t)","0be41b3e":"# for linear regression\n\n# model build using the natural distributions\n\n# call the model\nlinreg = LinearRegression()\n\n# fit the model\nlinreg.fit(scaler.transform(train_t), y_train)\n\n# make predictions and calculate the mean squared\n# error over the train set\nprint('Train set')\npred = linreg.predict(scaler.transform(train_t))\nprint('Linear Regression mse: {}'.format(mean_squared_error(y_train, pred)))\n\n# make predictions and calculate the mean squared\n# error over the test set\nprint('Test set')\npred = linreg.predict(scaler.transform(test_t))\nprint('Linear Regression mse: {}'.format(mean_squared_error(y_test, pred)))\nprint(np.sqrt(mean_squared_error(y_test, pred)))","5d78b14a":"from sklearn.neighbors import KNeighborsRegressor\n\nknn_model=KNeighborsRegressor().fit(train_t,y_train)\ny_pred=knn_model.predict(test_t)\nnp.sqrt(mean_squared_error(y_test,y_pred))","682de01d":"from sklearn.svm import SVR\n\nsvr_model=SVR().fit(train_t,y_train)\ny_pred=svr_model.predict(test_t)\nnp.sqrt(mean_squared_error(y_test,y_pred))","a264ec11":"from sklearn.neural_network import MLPRegressor\n\nmlp_model = MLPRegressor().fit(train_t, y_train)\ny_pred = mlp_model.predict(test_t)\nnp.sqrt(mean_squared_error(y_test, y_pred))","386fb557":"from sklearn.tree import DecisionTreeRegressor\n\ncart_model=DecisionTreeRegressor().fit(train_t,y_train)\ny_pred = mlp_model.predict(test_t)\nnp.sqrt(mean_squared_error(y_test, y_pred))","f505a54e":"#Random Forest\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf_model = RandomForestRegressor().fit(train_t, y_train)\ny_pred = mlp_model.predict(test_t)\nnp.sqrt(mean_squared_error(y_test, y_pred))","7b9483eb":"#GBM\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngbm_model=GradientBoostingRegressor().fit(train_t,y_train)\ny_pred=gbm_model.predict(test_t)\nnp.sqrt(mean_squared_error(y_test, y_pred))","54caa4da":"# XGB\n\nimport xgboost\nfrom xgboost import XGBRegressor\n\nxgb = XGBRegressor().fit(train_t, y_train)\ny_pred = xgb.predict(test_t)\nnp.sqrt(mean_squared_error(y_test, y_pred))","010defa3":"#Light GBM\n#!pip install lightgbm\n\nfrom lightgbm import LGBMRegressor\nlgb_model = LGBMRegressor().fit(train_t, y_train)\ny_pred = lgb_model.predict(test_t)\nnp.sqrt(mean_squared_error(y_test, y_pred))","89866f60":"# Cat Boost\n\nfrom catboost import CatBoostRegressor\ncatb_model = CatBoostRegressor(verbose = False).fit(train_t, y_train)\ny_pred = catb_model.predict(test_t)\nnp.sqrt(mean_squared_error(y_test, y_pred))","3ab27a9b":"import lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nmodels = []\n\nmodels.append(('Regression', LinearRegression()))\nmodels.append(('Ridge', Ridge()))\nmodels.append(('Lasso', Lasso()))\nmodels.append(('ElasticNet', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('SVR', SVR()))\nmodels.append(('GBM', GradientBoostingRegressor()))\nmodels.append((\"XGBoost\", XGBRegressor()))\nmodels.append((\"LightGBM\", LGBMRegressor()))\nmodels.append((\"CatBoost\", CatBoostRegressor(verbose = False)))\n\nfor name, model in models:\n    model.fit(train_t,y_train)\n    y_pred=model.predict(test_t)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(name,rmse)","a3e36303":"lgbm_params = {\"learning_rate\": [0.01,0.001, 0.1, 0.5, 1],\n              \"n_estimators\": [200,500,1000,5000],\n              \"max_depth\": [2,4,6,7,10],\n              \"colsample_bytree\": [1,0.8,0.5,0.4]}\n\nlgb_model = LGBMRegressor()\n\nclf=GridSearchCV(lgb_model,lgbm_params,cv=n_folds,scoring=\"neg_mean_squared_error\",refit=True)\nclf.fit(train_t,y_train)\nscores=clf.cv_results_[\"mean_test_score\"]\nscores_std= clf.cv_results_[\"std_test_score\"]\n\ny_predicted_dummy=clf.predict(test_t)\nmse=mean_squared_error(y_test, y_predicted_dummy)\nnp.sqrt(mse)","306315b8":"# Ridge Regression Tuning\n\nfrom sklearn.linear_model import RidgeCV\n\nalpha = [0.01,0.002,0.5,0.7,1]\n\n\nridge=Lasso(random_state=46,max_iter=1000)\nalphas=np.linspace(0,1,1000)\ntuned_parameters=[ {\"alpha\":alpha} ]\nn_folds=10\n\nclf=GridSearchCV(ridge,tuned_parameters,cv=n_folds,scoring=\"neg_mean_squared_error\",refit=True)\nclf.fit(train_t,y_train)\nscores=clf.cv_results_[\"mean_test_score\"]\nscores_std= clf.cv_results_[\"std_test_score\"]\nclf.best_estimator_.coef_\n\nridge= clf.best_estimator_\ny_predicted_dummy=clf.predict(test_t)\nmse=mean_squared_error(y_test, y_predicted_dummy)\nprint(np.sqrt(mse))\n\n","f9a7e337":"ridge=Lasso(random_state=46,max_iter=1000)\nalphas = [0.01,0.002,0.5,0.7,1]\ntuned_parameters=[ {\"alpha\":alphas} ]\nn_folds=10\n\nclf=GridSearchCV(ridge,tuned_parameters,cv=n_folds,scoring=\"neg_mean_squared_error\",refit=True)\nclf.fit(train_t,y_train)\nscores=clf.cv_results_[\"mean_test_score\"]\nscores_std= clf.cv_results_[\"std_test_score\"]\nclf.best_estimator_.coef_\n\nridge= clf.best_estimator_\ny_predicted_dummy=clf.predict(test_t)\nmse=mean_squared_error(y_test, y_predicted_dummy)\nnp.sqrt(mse)","904a6533":"ridge=ElasticNet(random_state=46,max_iter=1000)\nenet_params = {\"l1_ratio\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n              \"alpha\":[0.1,0.01,0.001,0.2,0.3,0.5,0.8,0.9,1]}\ntuned_parameters=[ {\"alpha\":enet_params} ]\nn_folds=10\n\nclf=GridSearchCV(ridge,enet_params,cv=n_folds,scoring=\"neg_mean_squared_error\",refit=True)\nclf.fit(train_t,y_train)\nscores=clf.cv_results_[\"mean_test_score\"]\nscores_std= clf.cv_results_[\"std_test_score\"]\nclf.best_estimator_.coef_\n\nridge= clf.best_estimator_\ny_predicted_dummy=clf.predict(test_t)\nmse=mean_squared_error(y_test, y_predicted_dummy)\nnp.sqrt(mse)","ab5b62a1":"# KNN Tuning\n\n\nknn_params = {\"n_neighbors\": np.arange(2,30,1)}\n\n\nknn_model = KNeighborsRegressor()\nclf=GridSearchCV(knn_model,knn_params,cv=n_folds,scoring=\"neg_mean_squared_error\",refit=True)\nclf.fit(train_t,y_train)\nscores=clf.cv_results_[\"mean_test_score\"]\nscores_std= clf.cv_results_[\"std_test_score\"]\n\n\nridge= clf.best_estimator_\ny_predicted_dummy=clf.predict(test_t)\nmse=mean_squared_error(y_test, y_predicted_dummy)\nnp.sqrt(mse)","ec3b7bb7":"# SVR\nsvr_params={\"C\":[1,2,3,5,10]}\n\n\nsvr = SVR()\nclf=GridSearchCV(svr,svr_params,cv=n_folds,scoring=\"neg_mean_squared_error\",refit=True)\nclf.fit(train_t,y_train)\nscores=clf.cv_results_[\"mean_test_score\"]\nscores_std= clf.cv_results_[\"std_test_score\"]\n\ny_predicted_dummy=clf.predict(test_t)\nmse=mean_squared_error(y_test, y_predicted_dummy)\nnp.sqrt(mse)","b40e1300":"#ANN\n\nmlp_params = {\"alpha\": [0.1, 0.01, 0.02, 0.001, 0.0001], \n             \"hidden_layer_sizes\": [(10,20), (5,5), (100,100), (1000,100,10)]}\n\nmlp_model = MLPRegressor()\nmlp_cv = GridSearchCV(mlp_model, mlp_params, cv = 10, verbose = 2, n_jobs = -1)\nclf.fit(train_t,y_train)\nscores=clf.cv_results_[\"mean_test_score\"]\nscores_std= clf.cv_results_[\"std_test_score\"]\n\ny_predicted_dummy=clf.predict(test_t)\nmse=mean_squared_error(y_test, y_predicted_dummy)\nnp.sqrt(mse)","f04aa28f":"#CART\n\ncart_params= {\"max_depth\": [5,6,10,15,20,50,100,1000],\n             \"min_samples_split\":[20,30,40,50]}\n\ncart_model=DecisionTreeRegressor()\nclf=GridSearchCV(cart_model,cart_params,cv=n_folds,scoring=\"neg_mean_squared_error\",refit=True)\nclf.fit(train_t,y_train)\nscores=clf.cv_results_[\"mean_test_score\"]\nscores_std= clf.cv_results_[\"std_test_score\"]\n\ny_predicted_dummy=clf.predict(test_t)\nmse=mean_squared_error(y_test, y_predicted_dummy)\nnp.sqrt(mse)","252b336c":"#RF\n\nrf_model=RandomForestRegressor()\nrf_params = {\"max_depth\": [5,8,10,3],\n            \"max_features\": [2,5,10,15,17],\n            \"n_estimators\": [100,200, 500, 1000],\n            \"min_samples_split\": [10,20,30,40,50]}\n\n\nclf=GridSearchCV(rf_model,rf_params,cv=n_folds,scoring=\"neg_mean_squared_error\",refit=True)\nclf.fit(train_t,y_train)\nscores=clf.cv_results_[\"mean_test_score\"]\nscores_std= clf.cv_results_[\"std_test_score\"]\n\ny_predicted_dummy=clf.predict(test_t)\nmse=mean_squared_error(y_test, y_predicted_dummy)\nnp.sqrt(mse)","9cd55268":"# GBM\n\ngbm_params = {\"learning_rate\": [0.001,0.1,0.01, 0.05],\n             \"max_depth\": [3,5,8,9,10],\n             \"n_estimators\": [200,500,1000,1500],\n             \"subsample\": [1,0.4,0.5,0.7],\n             \"loss\": [\"ls\",\"lad\",\"quantile\"]}\n\n\ngbm_model = GradientBoostingRegressor()\n\n\nclf=GridSearchCV(gbm_model,gbm_params,cv=n_folds,scoring=\"neg_mean_squared_error\",refit=True)\nclf.fit(train_t,y_train)\nscores=clf.cv_results_[\"mean_test_score\"]\nscores_std= clf.cv_results_[\"std_test_score\"]\n\ny_predicted_dummy=clf.predict(test_t)\nmse=mean_squared_error(y_test, y_predicted_dummy)\nnp.sqrt(mse)\n","fc5196a2":"#XGBoost\n\nxgb_params = {\"learning_rate\": [0.1,0.01,0.5],\n             \"max_depth\": [2,3,4,5,8],\n             \"n_estimators\": [100,200,500,1000],\n             \"colsample_bytree\": [0.4,0.7,1]}\n\nxgb = XGBRegressor()\n\n\nclf=GridSearchCV(xgb,xgb_params,cv=n_folds,scoring=\"neg_mean_squared_error\",refit=True)\nclf.fit(train_t,y_train)\nscores=clf.cv_results_[\"mean_test_score\"]\nscores_std= clf.cv_results_[\"std_test_score\"]\n\ny_predicted_dummy=clf.predict(test_t)\nmse=mean_squared_error(y_test, y_predicted_dummy)\nnp.sqrt(mse)\n\n\n","12165e51":"#CatBoost\n\ncatb_params = {\"iterations\": [200,500,100],\n              \"learning_rate\": [0.01,0.1],\n              \"depth\": [3,6,8]}\n\ncatb_model = CatBoostRegressor(verbose = False)\n\nclf=GridSearchCV(catb_model,catb_params,cv=n_folds,scoring=\"neg_mean_squared_error\",refit=True)\nclf.fit(train_t,y_train)\nscores=clf.cv_results_[\"mean_test_score\"]\nscores_std= clf.cv_results_[\"std_test_score\"]\n\ny_predicted_dummy=clf.predict(test_t)\nmse=mean_squared_error(y_test, y_predicted_dummy)\nnp.sqrt(mse)\n\n\n\n","d17ee777":"models = []\n\nmodels.append(('Ridge', Ridge(alpha=0.01)))\nmodels.append(('Lasso', Lasso(alpha=0.01)))\nmodels.append(('ElasticNet', ElasticNet(alpha=0.001, l1_ratio=0.95)))\nmodels.append(('KNN', KNeighborsRegressor(n_neighbors=5)))\nmodels.append(('CART', DecisionTreeRegressor(max_depth= 4, min_samples_split= 50)))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('SVR', SVR(C=1000)))\nmodels.append(('GBM', GradientBoostingRegressor()))\nmodels.append((\"XGBoost\", XGBRegressor()))\nmodels.append((\"LightGBM\", LGBMRegressor()))\nmodels.append((\"CatBoost\", CatBoostRegressor(verbose = False)))\n\nfor name, model in models:\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(name,rmse)","ceaa4e6a":"## Continuous Variables","ccd93269":"# Local Outlier Factor","f6739ab1":"# Set Limits For Features","4debbe2a":"# Mean Sample Imputer","8040a230":"## Missing Data Imputation for Missing Value","15eb47d2":"# MODEL TUNING","043c0c28":"# Outlier Handling Techniques","172580b6":"# Imputation Important  \n\nImputation should be done over the traning set,and then propagated to the test set.This means that the mean\/median to be used to fill mising values both in train and test set,should be extracted from the train set only.Add this is to overfitting","4de0fddc":"# Non-Linear Models (KNN\/SVR\/CART\/ ANN\/ Random Forest\/GBM\/XGBoost\/Light GBM\/CatBoost)","6b0bdc60":"# Select Threshold Value"}}