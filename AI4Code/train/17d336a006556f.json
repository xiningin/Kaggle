{"cell_type":{"911908d0":"code","bca8158c":"code","41a035ee":"code","86d70cbe":"code","53c15c85":"code","3b8348c8":"code","c96bd487":"code","c8d613e9":"code","aafbedec":"code","cb2a694f":"code","2485454c":"code","75f1ded2":"code","f60599c5":"code","69038700":"code","920e7dae":"code","cba549b9":"code","2f05088e":"code","8e239f00":"code","5be47fba":"code","12598ed5":"code","b98794a9":"code","ec86b22f":"code","2756ec2f":"code","15998bb8":"code","d9a4b0b1":"code","deb6ed32":"code","e90053ea":"markdown","c7c7b570":"markdown","887eb8ee":"markdown","594243af":"markdown","9ceb6de1":"markdown","fbd4e0c1":"markdown","40059065":"markdown","4b53857a":"markdown","c25cb8cf":"markdown","aef81d5b":"markdown","4584354a":"markdown"},"source":{"911908d0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","bca8158c":"# Print out a quick overview of the data\ndataset=pd.read_csv(\"..\/input\/dataset.csv\")\ndataset.head()","41a035ee":"# Quick statistical summary of data\ndataset.describe(include='all')","86d70cbe":"# Drop the URL column since that is a unique column for training\ndataset.drop('URL', axis=1, inplace=True)","53c15c85":"# Take a look at any null values to clean up data\n# Likely need to do something with these empty datasets\nprint(dataset.isnull().sum())\ndataset[pd.isnull(dataset).any(axis=1)]","3b8348c8":"# Interpolate our data to get rid of null values\ndataset = dataset.interpolate()\nprint(dataset.isnull().sum())","c96bd487":"# For some reason there's still a isnull in the SERVER column\ndataset['SERVER'].fillna('RARE_VALUE', inplace=True)","c8d613e9":"dataset.describe(include='all')","aafbedec":"# Convert categorical columns to numbered categorical columns\ndataset_with_dummies = pd.get_dummies(dataset,prefix_sep='--')\nprint(dataset_with_dummies.head())","cb2a694f":"# Separate predictors and response\nX = dataset_with_dummies.drop('Type',axis=1) #Predictors\ny = dataset_with_dummies['Type']\n\nX.head()","2485454c":"# Get a training and test dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","75f1ded2":"# Train a Random Forest Regressor\nfrom sklearn.ensemble import RandomForestClassifier\n\n# n_estimators is the number of random forests to use\n# n_jobs says to use all processors available\nrf = RandomForestClassifier(n_estimators=100, n_jobs=-1, max_depth=30, criterion = 'entropy')\nrf.fit(X_train, y_train)\n\nprint('Training Accuracy Score: {}'.format(rf.score(X_train, y_train)))","f60599c5":"y_pred = rf.predict(X_test)","69038700":"from sklearn.model_selection import cross_val_predict, cross_val_score\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n\n# Visualize our results\ndef print_score(classifier,X_train,y_train,X_test,y_test,train=True):\n    if train == True:\n        print(\"Training results:\\n\")\n        print('Accuracy Score: {0:.4f}\\n'.format(accuracy_score(y_train,classifier.predict(X_train))))\n        print('Classification Report:\\n{}\\n'.format(classification_report(y_train,classifier.predict(X_train))))\n        print('Confusion Matrix:\\n{}\\n'.format(confusion_matrix(y_train,classifier.predict(X_train))))\n        res = cross_val_score(classifier, X_train, y_train, cv=10, n_jobs=-1, scoring='accuracy')\n        print('Average Accuracy:\\t{0:.4f}\\n'.format(res.mean()))\n        print('Standard Deviation:\\t{0:.4f}'.format(res.std()))\n    elif train == False:\n        print(\"Test results:\\n\")\n        print('Accuracy Score: {0:.4f}\\n'.format(accuracy_score(y_test,classifier.predict(X_test))))\n        print('Classification Report:\\n{}\\n'.format(classification_report(y_test,classifier.predict(X_test))))\n        print('Confusion Matrix:\\n{}\\n'.format(confusion_matrix(y_test,classifier.predict(X_test))))\n\nprint_score(rf,X_train,y_train,X_test,y_test,train=False)","920e7dae":"from sklearn.tree import export_graphviz\n\ndef create_graph(forest, feature_names):\n    estimator = forest.estimators_[5]\n\n    export_graphviz(estimator, out_file='tree.dot',\n                    feature_names = feature_names,\n                    class_names = ['benign', 'malicious'],\n                    rounded = True, proportion = False, precision = 2, filled = True)\n\n    # Convert to png using system command (requires Graphviz)\n    from subprocess import call\n    call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=200'])\n    \ncreate_graph(rf, list(X))\n# Display in jupyter notebook\nfrom IPython.display import Image\nImage(filename = 'tree.png')\n","cba549b9":"# View our feature importances\nfeature_importance_zip = zip(list(X), rf.feature_importances_)\n\n# Sort the feature_importance_zip\nsorted_importance = sorted(feature_importance_zip, key=lambda x: x[1], reverse=True)\n\nfor feature in sorted_importance[:15]:\n    print(feature)","2f05088e":"# Look at our feature importances without the dummy variables\noriginal_feature_dict = {}\nfor feature, importance in zip(list(X), rf.feature_importances_):\n    # Check for our dummy variable delimeter --\n    if '--' in feature:\n        original_feature_name = feature.split('--')[0]\n    else:\n        original_feature_name = feature\n        \n    # Add to our original_feature_dict, incrememnt if it's already there\n    if original_feature_name in original_feature_dict:\n        original_feature_dict[original_feature_name] += importance\n    else:\n        original_feature_dict[original_feature_name] = importance\n      \n# Sort the original_feature_dict\nsorted_importance = sorted(original_feature_dict.items(), key=lambda x: x[1], reverse=True)\n\nfor feature, importance in sorted_importance:\n    print(feature, importance)\n    ","8e239f00":"# Copy over our data so we don't overwrite the original results if we want\n#    to tweak above\ndataset_preprocessed = dataset\n\n# Converting server types with only 1 unique count to a RARE_VALUE for classification\ntest = dataset_preprocessed ['SERVER'].value_counts()\ncol = 'SERVER'\ndataset_preprocessed.loc[dataset_preprocessed[col].value_counts()[dataset_preprocessed[col]].values < 2, col] = \"RARE_VALUE\"","5be47fba":"# Function to extract the registration year\ndef extract_reg_year(x):\n    # If no year was reported, leave it as None\n    if str(x) == 'None':\n        return(x)\n    \n    # Try different parses for different date formats\n    parse_error = False\n    try:\n        date = x.split(' ')[0]\n        year = date.split('\/')[2]\n    except:\n        parse_error = True\n    \n    # One more date format to try if there's a parse error\n    if parse_error:\n        try:\n            date = x.split('T')[0]\n            year = date.split('-')[0]\n            parse_error = False\n        except:\n            parse_error = True\n            raise ValueError('Error parsing {}'.format(x))\n\n    return(year)\n\ndataset_preprocessed['WHOIS_REGDATE'] = dataset_preprocessed['WHOIS_REGDATE'].apply(extract_reg_year)\ndataset_preprocessed['WHOIS_UPDATED_DATE'] = dataset_preprocessed['WHOIS_UPDATED_DATE'].apply(extract_reg_year)","12598ed5":"# State without country doesn't make sense\ndataset_preprocessed['WHOIS_STATEPRO'] = dataset_preprocessed[['WHOIS_COUNTRY','WHOIS_STATEPRO']].apply(lambda x : '{}-{}'.format(x[0],x[1]), axis=1)","b98794a9":"dataset_preprocessed.describe(include='all')","ec86b22f":"# As above, create our random forest classifier the same way\n\n# Convert categorical columns to numbered categorical columns\ndataset_pp_with_dummies = pd.get_dummies(dataset_preprocessed, prefix_sep='--')\n\n# Separate predictors and response\nX_pp = dataset_pp_with_dummies.drop('Type',axis=1) #Predictors\ny_pp = dataset_pp_with_dummies['Type']\n\n# Get a training and test dataset\nfrom sklearn.model_selection import train_test_split\nX_pp_train, X_pp_test, y_pp_train, y_pp_test = train_test_split(X_pp, y_pp, test_size=0.3, random_state=42)\n\n# Train a Random Forest Regressor\nfrom sklearn.ensemble import RandomForestClassifier\n\n# n_estimators is the number of random forests to use\n# n_jobs says to use all processors available\n# Properties we can play with for the RandomForestClassifier function:\n#    max_depth=int( default None, n_estimators=int(default 10), min_samples_split=int(default 2)\nrf_pp = RandomForestClassifier(n_estimators=100, n_jobs=-1, max_depth=30, criterion = 'entropy')\nrf_pp.fit(X_pp_train, y_pp_train)\n\nprint('Training Accuracy Score: {}'.format(rf_pp.score(X_pp_train, y_pp_train)))","2756ec2f":"y_pp_pred = rf_pp.predict(X_pp_test)\nprint_score(rf_pp,X_pp_train,y_pp_train,X_pp_test,y_pp_test,train=False)","15998bb8":"# View our feature importances\nfeature_importance_zip = zip(list(X_pp), rf_pp.feature_importances_)\n\n# Sort the feature_importance_zip\nsorted_importance = sorted(feature_importance_zip, key=lambda x: x[1], reverse=True)\n\nfor feature in sorted_importance[:15]:\n    print(feature)","d9a4b0b1":"# Look at our feature importances without the dummy variables\noriginal_feature_dict = {}\nfor feature, importance in zip(list(X_pp), rf_pp.feature_importances_):\n    # Check for our dummy variable delimeter --\n    if '--' in feature:\n        original_feature_name = feature.split('--')[0]\n    else:\n        original_feature_name = feature\n        \n    # Add to our original_feature_dict, incrememnt if it's already there\n    if original_feature_name in original_feature_dict:\n        original_feature_dict[original_feature_name] += importance\n    else:\n        original_feature_dict[original_feature_name] = importance\n      \n# Sort the original_feature_dict\nsorted_importance = sorted(original_feature_dict.items(), key=lambda x: x[1], reverse=True)\n\nfor feature, importance in sorted_importance:\n    print(feature, importance)","deb6ed32":"create_graph(rf_pp, list(X_pp))\n# Display in jupyter notebook\nfrom IPython.display import Image\nImage(filename = 'tree.png')","e90053ea":"Import the data and take a look at the first 5 lines.","c7c7b570":"Our classification is based on the \"Type\" column, so separate that out into two different dataframes.","887eb8ee":"Look at if there are any null values, this will be a problem with our model.","594243af":"**Perform some data preprocessing and see how we can improve our dataset.**\n\nFirst, there's a lot of SERVER types.  To reduce our dimesionality, we'll make that assumption that having a rare server type is more interesting then the specific server type for a decision tree classification.  It's possible we could extract interesting features of a server type (such as old versions, the \"base\" type such as nginx, the specific version type, etc.) which would be an interesting way to extend the research.","9ceb6de1":"Separate out our training and test data.  Our split will be a 70\/30 split, and the random_state just ensures that someone else can recreate our test\/training split and recreate our results.","fbd4e0c1":"Convert columns into \"dummy columns\" so that our categorical data becomes discrete for model creation.","40059065":"Tweaking parameters of the random forest classifier","4b53857a":"From above, we see that URL is totally unique.  Also, per the glimpse at the first 5 lines above we can see it's some sort of identifier that seems to be some sort of mapping key that does not describe the URL at all.","c25cb8cf":"To get a glimpse of what one of our Random Forest Decision Tree Classifiers look like, we'll print one out.","aef81d5b":"We see that there are quite a bit where CONTENT_LENGTH is empty.  There's also one DNS_QUERY_TIMES that's empty and one SERVER that's empty.  We should interpolate the CONTENT_LENGTH since that is a large chunks of our data, but we could consider dropping the entry (or entries) that contain those two null.","4584354a":"We've reduced our number of unique values in our column sets quite a bit.\n* SERVER (web server types) are now 98 vs. the original 240\n* WHOIS_REGDATE is now down to 30 vs. 891\n* WHOIS_UPDATED_DATE is now down to 10 vs. 594"}}