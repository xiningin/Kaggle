{"cell_type":{"bd20c2e2":"code","d69e0e42":"code","3e0a56b6":"code","d60bb1e5":"code","7fce5314":"code","1d833d5d":"code","bc31fa2f":"code","5c0502c4":"code","1f6829f8":"code","bba1cf6e":"code","9c445509":"code","cc3acc92":"code","652b18f7":"code","40f99cdb":"code","52ec6106":"code","eedfd9e3":"code","f36441ae":"code","3b150e60":"code","2d2cb9ad":"code","2432fae6":"code","53bae6c0":"code","84c23941":"markdown","6cac4027":"markdown","c32743ec":"markdown","704be3b1":"markdown","ce56c06c":"markdown","29b8a2ab":"markdown","e52a0522":"markdown","8ce88d9e":"markdown"},"source":{"bd20c2e2":"#!pip install seedir","d69e0e42":"import os\nimport copy\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn import model_selection\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport shap\n#import seedir as sd","3e0a56b6":"#data_dir = '\/kaggle\/input\/optiver-realized-volatility-prediction'\n#sd.seedir(data_dir, style='emoji')","d60bb1e5":"data_dir = '\/kaggle\/input\/optiver-realized-volatility-prediction'\ndf_train = pd.read_csv(f'{data_dir}\/train.csv')\ndf_train.head()","7fce5314":"plt.rcParams[\"figure.figsize\"] = (12,8)\nfig, axes = plt.subplots(2,2)\n\ndf_train['target'].hist(bins=100, ax=axes[0][0])\ndf_train['target'].to_frame().boxplot(ax=axes[1][0])\n\ndf_train['stock_id'].hist(bins=100, ax=axes[0][1])\ndf_train['stock_id'].to_frame().boxplot(ax=axes[1][1])\n\naxes[0][0].set_title('Target distribution')\naxes[1][0].set_title('Target box plot')\naxes[0][1].set_title('Stock id distribution')\naxes[1][1].set_title('Stock id box plot')","1d833d5d":"print('Gaps in the stock_id histogram above:')\n\nprint(sorted(list(set(range(120)).difference(df_train['stock_id'].unique()))))","bc31fa2f":"df_test = pd.read_csv(f'{data_dir}\/test.csv')\ndf_test.head()","5c0502c4":"book_train_0 = pd.read_parquet(f'{data_dir}\/book_train.parquet')\nbook_train_0 = book_train_0[book_train_0['stock_id'] == 0]\nbook_train_0.head()","1f6829f8":"book_train_0.hist(figsize = (15, 15), bins=50)\nplt.show()","bba1cf6e":"trade_train_0 = pd.read_parquet(f'{data_dir}\/trade_train.parquet')\ntrade_train_0 = trade_train_0[trade_train_0['stock_id'] == 0]\ntrade_train_0.head()","9c445509":"trade_train_0.hist(figsize = (15, 15), bins=50)\nplt.show()","cc3acc92":"def realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndef prepare_book_features(file_path, raw_features, agg_f):\n    \n    df_book_data = pd.read_parquet(file_path)\n    df_book_data = df_book_data.rename(columns = {'seconds_in_bucket':'seconds_in_bucket_book'})\n    \n    df_book_data['wap'] =(df_book_data['bid_price1'] * df_book_data['ask_size1']+\n                          df_book_data['ask_price1'] * df_book_data['bid_size1'])\/(\n        df_book_data['bid_size1']+ df_book_data['ask_size1'])\n        \n    df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n    df_book_data = df_book_data[~df_book_data['log_return'].isnull()]\n    \n    df_stat = df_book_data.groupby('time_id').agg({ckey:agg_f for ckey in raw_features})\n    df_stat.columns = df_stat.columns.map('_'.join)\n    df_stat = df_stat.reset_index()\n    \n    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['log_return'].agg(realized_volatility)).reset_index()\n    df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':'realized_volatility'})\n    \n    df_realized_vol_per_stock = df_realized_vol_per_stock.merge(df_stat, how='left')\n    stock_id = file_path.split('=')[1]\n    df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    \n    return df_realized_vol_per_stock\n\ndef prepare_trade_features(file_path, raw_features, agg_f):\n    \n    df_trade_data = pd.read_parquet(file_path)\n    df_trade_data = df_trade_data.rename(columns = {'seconds_in_bucket':'seconds_in_bucket_trade'})\n    df_trade_data = df_trade_data.groupby('time_id').agg({ckey:agg_f for ckey in raw_features})\n    df_trade_data.columns = df_trade_data.columns.map('_'.join)\n    df_trade_data = df_trade_data.reset_index()\n    \n    stock_id = file_path.split('=')[1]\n    df_trade_data['row_id'] = df_trade_data['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    \n    return df_trade_data\n\ndef process_book_files(files_dir, book_raw_features, agg_f):\n    \n    df_features = pd.DataFrame()\n    list_file = glob.glob(files_dir)\n    \n    for file in tqdm(list_file):\n        df_features = df_features.append(prepare_book_features(file, book_raw_features, agg_f))\n            \n    return df_features\n\ndef process_trade_files(files_dir, trade_raw_features, agg_f):\n    \n    df_features = pd.DataFrame()\n    list_file = glob.glob(files_dir)\n    \n    for file in tqdm(list_file):\n        df_features = df_features.append(prepare_trade_features(file, trade_raw_features, agg_f))\n            \n    return df_features","652b18f7":"agg_f = ['min', 'max', 'mean','std', 'median']\nbook_raw_features = ['seconds_in_bucket_book', 'bid_price1', 'ask_price1', 'bid_price2', 'ask_price2',\n               'bid_size1', 'ask_size1', 'bid_size2', 'ask_size2', 'wap', 'log_return']\n\ntrade_raw_features = ['seconds_in_bucket_trade', 'price', 'size', 'order_count']\n\nbook_features = process_book_files(f'{data_dir}\/book_train.parquet\/*', book_raw_features, agg_f)\ntrade_features = process_trade_files(f'{data_dir}\/trade_train.parquet\/*', trade_raw_features, agg_f)","40f99cdb":"book_features.head()","52ec6106":"trade_features.head()","eedfd9e3":"book_features = book_features.drop('time_id', axis=1)\ntrade_features = trade_features.drop('time_id', axis=1)\ndf_features = pd.merge(book_features, trade_features, left_on=['row_id'], right_on=['row_id'], how='left')\ndf_features = df_features.fillna(0)\n\ndf_train = pd.read_csv(f'{data_dir}\/train.csv')\ndf_train['row_id'] = df_train[['stock_id', 'time_id']].apply(lambda row: f'{row[0]}-{row[1]}',axis=1)\ndf_train = df_train.drop('stock_id', axis=1)\ndf_train = df_train.drop('time_id', axis=1)\n\ndf_train = pd.merge(df_train, df_features, left_on=['row_id'], right_on=['row_id'], how='left')\ndf_train.head()","f36441ae":"# define indices for 5-fold CV\n\ndf_train.loc[:, 'kfold'] = -1\ndf_train.sample(frac=1).reset_index(drop=True)\ny = df_train['target'].values\nskf = model_selection.KFold(n_splits=5, shuffle=True)\n\nfor f, (t_, v_) in enumerate(skf.split(X=df_train, y=y)):\n    df_train.loc[v_, 'kfold'] = f\n\ndf_train.head()","3b150e60":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\nall_features = list(df_train.columns)\nfor f in ['target', 'row_id', 'kfold']:\n    all_features.remove(f)\n\nfor fold in range(5):\n    \n    df_tr = df_train[df_train.kfold != fold].reset_index(drop=True)\n    df_val = df_train[df_train.kfold == fold].reset_index(drop=True)\n    \n    x_tr = df_tr[all_features].values\n    x_val = df_val[all_features].values\n\n    y_tr = df_tr['target'].values\n    y_val = df_val['target'].values\n    \n    model = xgb.XGBRegressor(n_estimators=50)\n    model.fit(x_tr, y_tr)\n    pred = model.predict(x_val)\n    r = rmspe(y_val, pred)\n    \n    print(f'Fold {fold}, RMSPE:{r}')","2d2cb9ad":"explainer = shap.Explainer(model, df_val[all_features])\nshap_values = explainer.shap_values(df_val[all_features])\nshap.summary_plot(shap_values, df_val[all_features], title='SHAP XGB summary plot', show=False)","2432fae6":"book_features_test = process_book_files(f'{data_dir}\/book_test.parquet\/*', book_raw_features, agg_f)\ntrade_features_test = process_trade_files(f'{data_dir}\/trade_test.parquet\/*', trade_raw_features, agg_f)\ndf_features_test = pd.merge(book_features_test, trade_features_test, left_on=['row_id'], right_on=['row_id'], how='left')\ndf_features_test = df_features_test.fillna(0)\n\nmodel = xgb.XGBRegressor(n_estimators=50)\nmodel.fit(df_train[all_features].values, df_train['target'].values)\n\npred = model.predict(df_features_test[all_features].values)\n\ndf_features_test['target']=pred\n\ndf_test = pd.read_csv(f'{data_dir}\/test.csv')\ndf_test = pd.merge(df_test, df_features_test, left_on=['row_id'], right_on=['row_id'], how='left')\ndf_test = df_test.fillna(0)\n\ndf_test[['row_id', 'target']].to_csv('submission.csv',index = False)","53bae6c0":"df_test[['row_id', 'target']].head()","84c23941":"References:\n* https:\/\/www.kaggle.com\/jiashenliu\/introduction-to-financial-concepts-and-data,\n* https:\/\/www.kaggle.com\/sohier\/working-with-parquet,\n* https:\/\/www.kaggle.com\/konradb\/we-need-to-go-deeper","6cac4027":"**Train the XGBoost model using all training data and make submission**","c32743ec":"### Parquet files\n\nFrom the notebook https:\/\/www.kaggle.com\/sohier\/working-with-parquet, \n\nApache Parquet is an efficient columnar storage format. Compared to saving this dataset in csvs using parquet:\n\n* Greatly reduces the necessary disk space\n* Loads the data into Pandas with memory efficient datatypes\n* Enables fast reads from disk\n* Allows us to easily work with partitions of the data\n\nPandas has a parquet integration that makes loading data into a dataframe trivial\n","704be3b1":"# Feature engineering\n\nFeatures will be constructed using book_train \/ and trade_train \/ files. Besides `realized_volatility`, features encompass statistical features as min, max, mean, median, and std for each of the variable.","ce56c06c":"**Show features importance and efect of the last CV fold using SHAP summary plot**","29b8a2ab":"### Data structure\n\n\ud83d\udcc1 input\/<br>\n\u2514\u2500\u2500\ud83d\udcc1 optiver-realized-volatility-prediction\/<br>\n      &emsp;&emsp;\u251c\u2500\u2500\ud83d\udcc1 trade_train.parquet\/<br>\n      &emsp;&emsp;\u2502&emsp; \u251c\u2500\u2500\ud83d\udcc1 stock_id=97\/<br>\n      &emsp;&emsp;\u2502&emsp; \u2502&emsp; \u2514\u2500\u2500\ud83d\udcc4 888f813404d8417ca8d6b8aebd5f2951.parquet<br>\n      &emsp;&emsp;\u2502&emsp; \u251c\u2500\u2500\ud83d\udcc1 stock_id=43\/<br>\n      &emsp;&emsp;\u2502&emsp; \u2502&emsp;\u2514\u2500\u2500\ud83d\udcc4 bb0efa57f511470e817880842e3e2afa.parquet<br>\n      &emsp;&emsp;\u2502&emsp; \u251c\u2500\u2500\ud83d\udcc1 ...<br>\n      &emsp;&emsp;\u2502&emsp; \u251c\u2500\u2500\ud83d\udcc1 ...<br>\n      &emsp;&emsp;\u251c\u2500\u2500\ud83d\udcc1 book_train.parquet\/<br>\n      &emsp;&emsp;\u2502&emsp; \u251c\u2500\u2500\ud83d\udcc1 stock_id=97\/<br>\n      &emsp;&emsp;\u2502&emsp; \u2502&emsp; \u2514\u2500\u2500\ud83d\udcc4 52e74e4ef0d84c5c989fc4704e46b527.parquet<br>\n      &emsp;&emsp;\u2502&emsp; \u251c\u2500\u2500\ud83d\udcc1 stock_id=43\/<br>\n      &emsp;&emsp;\u2502&emsp; \u2502&emsp; \u2514\u2500\u2500\ud83d\udcc4 ce8f54442b1142338d2e4b02b9dc578a.parquet<br>\n      &emsp;&emsp;\u2502&emsp; \u251c\u2500\u2500\ud83d\udcc1 ...<br>\n      &emsp;&emsp;\u2502&emsp; \u251c\u2500\u2500\ud83d\udcc1 ...<br>\n      &emsp;&emsp;\u251c\u2500\u2500\ud83d\udcc1 trade_test.parquet\/<br>\n      &emsp;&emsp;\u2502&emsp; \u2514\u2500\u2500\ud83d\udcc1 stock_id=0\/<br>\n      &emsp;&emsp;\u2502&emsp; \u2502&emsp; \u2514\u2500\u2500\ud83d\udcc4 31c83a67d81349208e7d5eace9dbbac8.parquet<br>\n      &emsp;&emsp;\u251c\u2500\u2500\ud83d\udcc1 book_test.parquet\/<br>\n      &emsp;&emsp;\u2502&emsp; \u2514\u2500\u2500\ud83d\udcc1 stock_id=0\/<br>\n      &emsp;&emsp;\u2502&emsp; \u2502&emsp; \u2514\u2500\u2500\ud83d\udcc4 7832c05caae3489cbcbbb9b02cf61711.parquet<br>\n      &emsp;&emsp;\u251c\u2500\u2500\ud83d\udcc4 sample_submission.csv<br>\n      &emsp;&emsp;\u251c\u2500\u2500\ud83d\udcc4 train.csv<br>\n      &emsp;&emsp;\u2514\u2500\u2500\ud83d\udcc4 test.csv","e52a0522":"### **Goal: predict column target from train.csv using features from book_train\/ and trade_train\/**\n\nMore info about data can be found here https:\/\/www.kaggle.com\/jiashenliu\/introduction-to-financial-concepts-and-data","8ce88d9e":"**Train and validate XGBoost using all features and 5-fold CV**"}}