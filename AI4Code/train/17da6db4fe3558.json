{"cell_type":{"9d64a381":"code","2c413d9c":"code","1d685540":"code","b76ed303":"code","4c889324":"code","2d343de0":"code","46eb07d0":"code","64f15ef9":"code","cedaf7e4":"code","4bc55878":"code","0a170c2f":"code","13cf6634":"code","0ecc7eef":"code","1f35a9ac":"code","d6c0fbb5":"code","a14d56e2":"code","a118689e":"code","12b2eddf":"code","41d25241":"code","2e179686":"markdown","fa38b4f7":"markdown","c8ea4d68":"markdown","20338bec":"markdown","015ce5f2":"markdown","1ccb387c":"markdown","5dc26ce6":"markdown","441add1b":"markdown","c288263c":"markdown","fbd013ea":"markdown"},"source":{"9d64a381":"import pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline \nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\nfrom numpy import array\nfrom numpy import argmax\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_selection import chi2\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation\nfrom keras.utils import np_utils\nimport re\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier","2c413d9c":"names=['URL','Category']\ndf=pd.read_csv('..\/input\/website-classification-using-url\/URL Classification.csv',names=names, na_filter=False)\ndataset = df[:]","1d685540":"\nadult = dataset[1:2000]\narts = dataset[50000:52000]\nbusiness = dataset[520000:522000]\ncomputers = dataset[535300:537300]\ngames = dataset[650000:652000]\nhealth = dataset[710000:712000]\nhome =  dataset[764200:766200]\nkids =  dataset[793080:795080]\nnews =  dataset[839730:841730]\nrecreation =  dataset[850000:852000]\nreference =  dataset[955250:957250]\nscience =  dataset[1013000:1015000]\nshopping =  dataset[1143000:1145000]\nsociety =  dataset[1293000:1295000]\nsports =  dataset[1492000:1494000]\n\ntest_data = pd.concat([adult, arts, business, computers, games, health, home, \n              kids, news, recreation, reference,science, shopping, society, sports], axis=0)\n\ndataset.drop(dataset.index[1:2000],inplace= True)\ndataset.drop(dataset.index[50000:52000],inplace= True)\ndataset.drop(dataset.index[520000:522000],inplace= True)\ndataset.drop(dataset.index[535300:537300],inplace= True)\ndataset.drop(dataset.index[650000:652000],inplace= True)\ndataset.drop(dataset.index[710000:712000],inplace= True)\ndataset.drop(dataset.index[764200:766200],inplace= True)\ndataset.drop(dataset.index[793080:795080],inplace= True)\ndataset.drop(dataset.index[839730:841730],inplace= True)\ndataset.drop(dataset.index[850000:852000],inplace= True)\ndataset.drop(dataset.index[955250:957250],inplace= True)\ndataset.drop(dataset.index[1013000:1015000],inplace= True)\ndataset.drop(dataset.index[1143000:1145000],inplace= True)\ndataset.drop(dataset.index[1293000:1295000],inplace= True)\ndataset.drop(dataset.index[1492000:1494000],inplace= True)\ndataset.tail()","b76ed303":"dataset.Category.value_counts().plot(figsize=(12,5),kind='bar',color='green');\nplt.xlabel('Category')\nplt.ylabel('Total Number Of Individual Category')","4c889324":"import seaborn as sns\nax = sns.countplot(y=\"Category\",  data=df )\nplt.title(\"Visualization of the dataset\", y=1.01, fontsize=20)\nplt.ylabel(\"Name of the Category\", labelpad=15)\nplt.xlabel(\"Number of Categories of URLs\", labelpad=15)\ndf[:2]","2d343de0":"ax = sns.countplot(y = \"Category\",  data = dataset )\nplt.title(\"Visualization of the train dataset\", y=1.01, fontsize=20)\nplt.ylabel(\"Name of the Category\", labelpad=15)\nplt.xlabel(\"Number of Categories of URLs\", labelpad=15)","46eb07d0":"test_data.Category.value_counts().plot(figsize=(12,5),kind='bar',color='green');\nplt.xlabel('Category')\nplt.ylabel('Total Number Of Individual Category')","64f15ef9":"ax = sns.countplot(y = \"Category\",  data = test_data , color = 'gray')\nplt.title(\"Visualization of the test dataset\", y=1.01, fontsize=20)\nplt.ylabel(\"Name of the Category\", labelpad=15)\nplt.xlabel(\"Number of Categories of URLs\", labelpad=15)","cedaf7e4":"X_train=dataset['URL']\ny_train=dataset['Category']\n#print(X_train)\nX_train.shape","4bc55878":"X_test=test_data['URL']\ny_test=test_data['Category']\n#print(X_test)\nX_test.shape","0a170c2f":"#from sklearn.pipeline import Pipeline\n#from sklearn.linear_model import SGDClassifier\n#text_clf = Pipeline([\n#                   ('vect', CountVectorizer( ngram_range = (2,2))),\n#                    ('tfidf', TfidfTransformer()),\n#                    ('clf', SGDClassifier(loss='perceptron', penalty='l2',\n#                     alpha =1e-3 , max_iter=20 ,tol=None)),\n#                    ])\n#gs_clf = text_clf.fit(X_train , y_train)","13cf6634":"import re\nfrom sklearn.pipeline import Pipeline\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\nclass StemmedCountVectorizer(CountVectorizer):\n    def build_analyzer(self):\n        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\nstemmed_count_vect = StemmedCountVectorizer(stop_words='english', ngram_range=(3,3))\n\n#count_vect = CountVectorizer(stop_words='english', ngram_range=(1,1))\ngs_clf = Pipeline([('vect', stemmed_count_vect),\n                   ('tfidf', TfidfTransformer()),\n                   ('clf', SGDClassifier(loss='perceptron', penalty='l2',\n                    alpha =1e-4 , max_iter=20 ,tol=None)),\n   ])\ngs_clf = gs_clf.fit(X_train, y_train)\n","0ecc7eef":"from sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import confusion_matrix\n#grid_mean_scores = [result.mean_validation_score for result in gs_clf.grid_scores_]\n#print(grid_mean_scores)\ny_pred=gs_clf.predict(X_test)\nprecision_recall_fscore_support(y_test, y_pred, average='weighted')","1f35a9ac":"y_pred=gs_clf.predict(X_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred, digits = 4))","d6c0fbb5":"import seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\narray = confusion_matrix(y_test, y_pred)\ncm=np.array(array)\ncm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\ndf_cm = pd.DataFrame(cm, index = [i for i in \"0123456789ABCDE\"],\n                  columns = [i for i in \"0123456789ABCDE\"])\nplt.figure(figsize = (20,15))\nsn.heatmap(df_cm, annot=True)","a14d56e2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_cm(y_true, y_pred, figsize=(20,10)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n\nplot_cm(y_test, y_pred)","a118689e":"print('Naive Bayes Train Accuracy = ',metrics.accuracy_score(y_train,gs_clf.predict(X_train)))\nprint('Naive Bayes Test Accuracy = ',metrics.accuracy_score(y_test,gs_clf.predict(X_test)))","12b2eddf":"print(gs_clf.predict(['http:\/\/www.businesstoday.net\/']))\nprint(gs_clf.predict(['http:\/\/www.gamespot.net\/']))","41d25241":"import tensorflow as tf\nimport pickle\n\n# save the model to disk\nfilename = 'svm_stem(3,3).sav'\npickle.dump(gs_clf, open(filename, 'wb'))\n \n# some time later...\n \n# load the model from disk\nloaded_model = pickle.load(open(filename, 'rb'))\nresult = loaded_model.score(X_test, y_test)\nresult","2e179686":"### Author : Sanjoy Biswas\n### Project : Website Url Feature Data Classification & Data Visualization\n### Email : sanjoy.eee32@gmail.com","fa38b4f7":"### Analysis Website Sentiment Using Natural Language Processing","c8ea4d68":"### Use Naive Base Classifier & SVM Algorithm","20338bec":"### Importing Libraries","015ce5f2":"### Load Website Feature DataSet ","1ccb387c":"### Accuracy Output- 93%","5dc26ce6":"### Website Feature Information","441add1b":"Hello Kagglers,\nHere is the solution of Website Url base Data Feature Classification and Data Visualization.\n* For Visualization I Use matplotlib & Seaborn Library\n* Use Sklearn library for data scaling & Matrix form\n* Use Naive bayes & SVM Algorithm\n* Use Deep Learning Library Keras & Tensorflow backend\n* Use NLP For sentiment Analysis","c288263c":"### Site Core Data Visualization Using Features Accuracy","fbd013ea":"### Total Individual Category Data Visualization"}}