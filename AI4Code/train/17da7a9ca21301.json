{"cell_type":{"a7a38fe8":"code","e8a3d6cc":"code","1a130fd8":"code","01e48fef":"code","50613c62":"markdown","95fdd4a8":"markdown","2511ab84":"markdown","27aa990e":"markdown"},"source":{"a7a38fe8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nMAX_FEATURE = 100000\nMAX_LEN = 100\nEMBEDDING_DIM = 300\nBATCH_SIZE = 512\nEPOCHS = 2\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ntrain_comments = train[\"comment_text\"].astype(str)\ntrain_targets = train[\"target\"].astype(\"float32\")\ntest_comments = test[\"comment_text\"].astype(str)\ntest_ids = test[\"id\"].astype(str)\n\ntokenizer = Tokenizer(MAX_LEN)\ntokenizer.fit_on_texts(train_comments)\nsequences = tokenizer.texts_to_sequences(train_comments)\ntrain_x = pad_sequences(sequences, MAX_LEN)\n\ntokenizer.fit_on_texts(test_comments)\nsequences = tokenizer.texts_to_sequences(test_comments)\ntest_x = pad_sequences(sequences, MAX_LEN)\ntrain_y = np.where(train_targets >= 0.5, 1, 0)\n\n\n# Any results you write to the current directory are saved as output.","e8a3d6cc":"from keras import models\nfrom keras import layers\nfrom keras import Input\n\n\ninput_tensor = Input(shape=(MAX_LEN,))\nx = layers.Embedding(MAX_FEATURE, EMBEDDING_DIM, input_length=MAX_LEN)(input_tensor)\nx = layers.Bidirectional(layers.LSTM(32 ,return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n#attention = Attention.Attention(max_len)(x)\n#attention = SeqSelfAttention(attention_activation='sigmoid')(x)\n#attention = layers.GlobalMaxPooling1D()(attention)\nx = layers.GlobalMaxPooling1D()(x)\n#x = layers.concatenate([attention, x])\n#x = layers.Dense(32, activation=\"relu\")(x)\noutput_tensor = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = models.Model(input_tensor, output_tensor)\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\nmodel.summary()","1a130fd8":"model.fit(train_x, train_y, epochs=EPOCHS, verbose=1, batch_size=BATCH_SIZE)","01e48fef":"predictions = model.predict(test_x, batch_size=BATCH_SIZE, verbose=1)\npredictions = predictions.ravel()\n\nsubmission = pd.DataFrame.from_dict({\n    'id': test_ids,\n    'prediction': predictions\n})\n\nsubmission.to_csv('submission.csv', index=False)","50613c62":"start to train model","95fdd4a8":"featrue engineering","2511ab84":"prediction","27aa990e":"create model"}}