{"cell_type":{"3c2129a1":"code","9bdee34d":"code","76a71e33":"code","4f20464e":"code","89016d8f":"code","349124ae":"code","f4b5f222":"code","0e7ebf09":"code","ac407311":"code","f91ad592":"code","61a2faae":"code","e25aadb7":"code","c78eda50":"code","cd23c242":"code","f0d50cda":"code","b0b38762":"code","f0bdfc9b":"code","104df830":"code","c803a349":"code","a9286baf":"code","4a835a10":"code","00f9303e":"code","d819f435":"code","389a95cf":"code","947d46fb":"code","cb71a99c":"code","a4415303":"code","ec932a36":"code","2114bc0e":"code","adaba94e":"code","55940f0e":"code","9a236d0e":"code","91423086":"code","352a5173":"code","ea999187":"code","07e9d3fa":"code","b9131f1b":"code","9d28be29":"code","d8b78ea5":"code","734e96b3":"code","5a0f89ff":"code","465e4c7a":"code","5f5c1e4e":"code","37faa039":"code","84d75a4c":"code","2def9545":"code","506f1ae7":"code","cdbeb2ad":"code","efee8ef1":"code","8f6076b3":"code","737a3cc3":"code","bd3a53d6":"code","e62e7dd6":"code","af124610":"code","db3f56e4":"code","8eade86a":"code","9f9baca0":"code","710e1bd9":"code","0d9d8fe7":"code","7d2a46b5":"code","7dc72f72":"code","1af67e98":"code","4d07a7f6":"code","4b9e49cc":"code","91b093bc":"code","e3503d8a":"code","b5edded6":"code","fc28f0c1":"code","85fa88d1":"code","cb2bacf4":"code","2d40304b":"code","f683c1c8":"code","6f5988b5":"code","42fd2eee":"code","7934ed80":"markdown","6986f8d3":"markdown","bea6aca3":"markdown","1e18bf16":"markdown","8a4c4199":"markdown","d179e461":"markdown","fdf1677d":"markdown","f22858ac":"markdown","8d498559":"markdown","9c95537a":"markdown","07c231b9":"markdown","5654e206":"markdown","d6799c61":"markdown","e4ba00df":"markdown","3dc74b32":"markdown","657b6ab0":"markdown","c070c985":"markdown","99b376fb":"markdown","67a5a88e":"markdown","5fc28f77":"markdown","0419a265":"markdown","e4bb6956":"markdown","4368a14c":"markdown","0d0bd38b":"markdown","265a18ff":"markdown","0db8fe8b":"markdown","325d6acd":"markdown","fdb0b293":"markdown","1e7504f0":"markdown","317ea529":"markdown","b24ab2d5":"markdown","1d38b3bc":"markdown","1cecce28":"markdown","7a3f1c0c":"markdown"},"source":{"3c2129a1":"# here we are importing libraries for doing further analysis.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom scipy import stats\nfrom scipy.stats import norm, skew","9bdee34d":"Train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nTest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","76a71e33":"# here we are printing first 5 lines of our train dataset\nTrain.head()","4f20464e":"# Here we are printing first 5 line of our test dataset\nTest.head()","89016d8f":"# here we are exploring outliers\nfig, ax = plt.subplots()\nax.scatter(x = Train['GrLivArea'], y = Train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","349124ae":"# here we are deleting outliers\nTrain = Train.drop(Train[(Train['GrLivArea']>4000) & (Train['SalePrice']<300000)].index)\n\n#Checking graphic again\nfig, ax = plt.subplots()\nax.scatter(Train['GrLivArea'], Train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","f4b5f222":"sns.distplot(Train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(Train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Plot the QQ-plot\nfig = plt.figure()\nres = stats.probplot(Train['SalePrice'], plot=plt)\nplt.show()","0e7ebf09":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\nTrain[\"SalePrice\"] = np.log1p(Train[\"SalePrice\"])\n\n#Checking the new distribution \nsns.distplot(Train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(Train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Plot the QQ-plot\nfig = plt.figure()\nres = stats.probplot(Train['SalePrice'], plot=plt)\nplt.show()","ac407311":"# here we are printing sum of all null values in train dataset\nprint(Train.isnull().sum())\nprint(sum(Train.isnull().sum()))","f91ad592":"# here we are printing sum of all null values in test dataset\nprint(Test.isnull().sum())\nprint(sum(Test.isnull().sum()))","61a2faae":"# here we are printing shape of our train and test dataset\nprint(\"Train shape:\", Train.shape)\nprint(\"Test shape:\", Test.shape)","e25aadb7":"Train.info()\n","c78eda50":"Test.info()","cd23c242":"# Saving the 'Id' column\ntrain_ID = Train['Id']\ntest_ID = Test['Id']\n# now we are droping id column because this feature is not neccesary for prediction process\nTrain = Train.drop(\"Id\",axis=1)\nTest = Test.drop(\"Id\",axis=1)","f0d50cda":"# here we are again printing shape of our dataset to see that id column is replace or not\nprint(\"Train shape:\", Train.shape)\nprint(\"Test shape:\", Test.shape)","b0b38762":"# here we are printing all the name of columns that are present in our dataset\nfor col in Train.columns: \n    print(col) ","f0bdfc9b":"# here we are printing summary of train dataset\nTrain.describe()","104df830":"# here we are printing summary of test dataset\nTest.describe()","c803a349":"# here we are visualising null value in our train dataset\nsns.heatmap(Train.isnull())","a9286baf":"# here we are visualising null value in our test dataset\nsns.heatmap(Test.isnull())","4a835a10":"# here we are concating our train and test dataset\nntrain = Train.shape[0]\nntest = Test.shape[0]\ny_train = Train.SalePrice.values\ntrain_test_data = pd.concat((Train, Test)).reset_index(drop=True)\n","00f9303e":"# here we are droping salesprices columns from our concat dataset\ntrain_test_data.drop([\"SalePrice\"],axis=1,inplace=True)\n","d819f435":"# now we are printing shape of our concat dataset\nprint(\"train_test_data size is: {}\".format(train_test_data.shape))","389a95cf":"# here we are counting the null value of each column\nnull = pd.DataFrame(train_test_data.isnull().sum().sort_values(ascending=False)[:25])\nnull.columns = [\"Null counts\"]\nnull.index.name = \"Feature\"\nnull","947d46fb":"# here we are counting null value in percent\nall_data_na = (train_test_data.isnull().sum() \/ len(train_test_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","cb71a99c":"# here we are plotting the upper tabular form into graph\nplt.style.use(style='ggplot')\nplt.rcParams['figure.figsize'] = (10, 6)\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=10)\nplt.ylabel('Percent of missing values', fontsize=10)\nplt.title('Percent missing data by feature', fontsize=13)","a4415303":"# here we are making correlation graph to see how features are correlated with saleprice\nplt.figure(figsize=(10,8))\ncorrMatrix=Train.corr()\nsns.heatmap(corrMatrix,vmax=0.9,square =True)\nplt.show()","ec932a36":"# firstly we are filling null value of column PoolQC\n# data description says null means \"No Pool\". That make sense,\n# given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general,\n# so we are going to fill null values with none\ntrain_test_data[\"PoolQC\"] = train_test_data[\"PoolQC\"].fillna(\"None\")","2114bc0e":"# secondly we are filling null values of column MiscFeature\n# data description says null means \"no misc feature\",as given that the huge ratio of missing value,\n# so we are going to fill null values with none\ntrain_test_data[\"MiscFeature\"] = train_test_data[\"MiscFeature\"].fillna(\"None\")","adaba94e":"# now we are filling null values of column Alley \n# data description says NA means \"no alley access\", as given that the huge ratio of missing value,\n# so we are going to fill null values with none\ntrain_test_data[\"Alley\"] = train_test_data[\"Alley\"].fillna(\"None\")","55940f0e":"# now we are filling null values of column Fence \n# data description says NA means \"no fence\", as given that the huge ratio of missing value,\n# so we are going to fill null values with none\ntrain_test_data[\"Fence\"] = train_test_data[\"Fence\"].fillna(\"None\")","9a236d0e":"# now we are filling null values of column FireplaceQu\n# data description says NA means \"no fireplace\",as given that the huge ratio of missing value,\n# so we are going to fill null values with none\ntrain_test_data[\"FireplaceQu\"] = train_test_data[\"FireplaceQu\"].fillna(\"None\")","91423086":"# now we are filling null values of column LotFrontage\n# we are going to Group by neighborhood and fill in missing value \n# by the median LotFrontage of all the neighborhood\ntrain_test_data[\"LotFrontage\"] = train_test_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","352a5173":"# now we are filling null values of columns GarageYrBlt, GarageArea and GarageCars  \n# Replacing missing data with 0 Since No garage = no cars in such garage.\ntrain_test_data[\"GarageYrBlt\"] = train_test_data[\"GarageYrBlt\"].fillna(0)\ntrain_test_data[\"GarageArea\"] = train_test_data[\"GarageArea\"].fillna(0)\ntrain_test_data[\"GarageCars\"] = train_test_data[\"GarageCars\"].fillna(0)\n\n","ea999187":"# now we are filling null values of columns GarageType, GarageFinish, GarageQual and GarageCond \n# replacing missing data with none \ntrain_test_data[\"GarageType\"] = train_test_data[\"GarageType\"].fillna(\"None\")\ntrain_test_data[\"GarageFinish\"] = train_test_data[\"GarageFinish\"].fillna(\"None\")\ntrain_test_data[\"GarageQual\"] = train_test_data[\"GarageQual\"].fillna(\"None\")\ntrain_test_data[\"GarageCond\"] = train_test_data[\"GarageCond\"].fillna(\"None\")","07e9d3fa":"# now we are filling null values of columns BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath \n# and BsmtHalfBath\n# Replacing missing values with 0 for having no basement\ntrain_test_data[\"BsmtFinSF1\"] = train_test_data[\"BsmtFinSF1\"].fillna(0)\ntrain_test_data[\"BsmtFinSF2\"] = train_test_data[\"BsmtFinSF2\"].fillna(0)\ntrain_test_data[\"BsmtUnfSF\"] = train_test_data[\"BsmtUnfSF\"].fillna(0)\ntrain_test_data[\"TotalBsmtSF\"] = train_test_data[\"TotalBsmtSF\"].fillna(0)\ntrain_test_data[\"BsmtFullBath\"] = train_test_data[\"BsmtFullBath\"].fillna(0)\ntrain_test_data[\"BsmtHalfBath\"] = train_test_data[\"BsmtHalfBath\"].fillna(0)\n","b9131f1b":"# now we are filling null values in these categorical columns\n# BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2\n# replacing missing values with none for having on basement\ntrain_test_data[\"BsmtQual\"] = train_test_data[\"BsmtQual\"].fillna(\"none\")\ntrain_test_data[\"BsmtCond\"] = train_test_data[\"BsmtCond\"].fillna(\"none\")\ntrain_test_data[\"BsmtExposure\"] = train_test_data[\"BsmtExposure\"].fillna(\"none\")\ntrain_test_data[\"BsmtFinType1\"] = train_test_data[\"BsmtFinType1\"].fillna(\"none\")\ntrain_test_data[\"BsmtFinType2\"] = train_test_data[\"BsmtFinType2\"].fillna(\"none\")","9d28be29":"# now we are filling null values of columns MasVnrArea and MasVnrType\n# replacing missing value with none in column MasVnrType and 0 in column MasVnrArea \n# for having no masonry veneer for these houses\ntrain_test_data[\"MasVnrArea\"] = train_test_data[\"MasVnrArea\"].fillna(0)\ntrain_test_data[\"MasVnrType\"] = train_test_data[\"MasVnrType\"].fillna(\"None\")","d8b78ea5":"# now we are filling null values of column MSZoning\n# as 'RL' is by far the most common value.\n# so we are replacing missing values with RL using mode\ntrain_test_data[\"MSZoning\"] = train_test_data[\"MSZoning\"].fillna(train_test_data['MSZoning'].mode()[0])","734e96b3":"# here we are removing Utilities column because it will not play important role in doing prediction\ntrain_test_data = train_test_data.drop(['Utilities'], axis=1)","5a0f89ff":"# here we are filling null values of column Functional\n# data description says NA means typical so we are filling missing values with TYP\ntrain_test_data[\"Functional\"] = train_test_data[\"Functional\"].fillna(\"Typ\")","465e4c7a":"# here we are filling null values of column Electrical\n# It has one NA value. Since this feature has mostly 'SBrkr', \n# so we are filling missing values with \"SBrkr\" using mode\ntrain_test_data[\"Electrical\"] = train_test_data[\"Electrical\"].fillna(train_test_data[\"Electrical\"].mode()[0])","5f5c1e4e":"# here we are filling null values of column KitchenQual  \n# Only one NA value, and same as Electrical,since this feature has mostly \"TA\",\n# so we are filling missing values with \"TA\" using mode\ntrain_test_data[\"KitchenQual\"] = train_test_data[\"KitchenQual\"].fillna(train_test_data[\"KitchenQual\"].mode()[0])","37faa039":"# here we are filling null values of columns Exterior1st and Exterior2nd\n# Again Both Exterior 1 & 2 have only one missing value,\n# so we are filling missing values using mode\ntrain_test_data[\"Exterior1st\"] = train_test_data[\"Exterior1st\"].fillna(train_test_data[\"Exterior1st\"].mode()[0])\ntrain_test_data[\"Exterior2nd\"] = train_test_data[\"Exterior2nd\"].fillna(train_test_data[\"Exterior2nd\"].mode()[0])","84d75a4c":"# here we are filling null values of columns SaleType\n# Since this feature has mostly 'WD'\n# so we are filling missing values with \"WD\" using mode\ntrain_test_data[\"SaleType\"] = train_test_data[\"SaleType\"].fillna(train_test_data[\"SaleType\"].mode()[0])","2def9545":"# here we are filling null values of columns MSSubClass\n# data discription Na most likely means No building class\n# so we are going to replace missing values with none\ntrain_test_data[\"MSSubClass\"] = train_test_data[\"MSSubClass\"].fillna(\"none\")","506f1ae7":"# now checking ones again if there is any missing value or not\nall_data_na = (train_test_data.isnull().sum() \/ len(train_test_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","cdbeb2ad":"#MSSubClass The building class\ntrain_test_data['MSSubClass'] = train_test_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\ntrain_test_data['OverallCond'] = train_test_data['OverallCond'].astype(str)\n\n\n#Changing Year and month sold are into categorical features.\ntrain_test_data['YrSold'] = train_test_data['YrSold'].astype(str)\ntrain_test_data['MoSold'] = train_test_data['MoSold'].astype(str)","efee8ef1":"# here we are imporitng libraries for performing label encoder\nfrom sklearn.preprocessing import LabelEncoder\ncolumns = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\nfor c in columns:\n    lb = LabelEncoder()\n    lb.fit(list(train_test_data[c].values))\n    train_test_data[c] = lb.transform(list(train_test_data[c].values))\n# here we are printing shape of our dataset\ntrain_test_data.shape","8f6076b3":"# here we are adding one more feature\n# adding total sqfootage feature this feature is combination of total area of basement,first\n# and second floor area of each house\ntrain_test_data[\"TotalSF\"] = train_test_data[\"TotalBsmtSF\"]+train_test_data[\"1stFlrSF\"]+train_test_data[\"2ndFlrSF\"]","737a3cc3":"from scipy import stats\nfrom scipy.stats import norm, skew\nnumeric_feats = train_test_data.dtypes[train_test_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = train_test_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","bd3a53d6":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    train_test_data[feat] = boxcox1p(train_test_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","e62e7dd6":"# creating dummy variable of categorical features\ntrain_test_data = pd.get_dummies(train_test_data)\nprint(train_test_data.shape)","af124610":"# here we are getting new set of train and test\nx = train_test_data[:ntrain]\ny = train_test_data[ntrain:]","db3f56e4":"# here we are imporitng important libraries which we are needed in doing prediction\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge,LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","8eade86a":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x.values)\n    rmse= np.sqrt(-cross_val_score(model, x.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","9f9baca0":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","710e1bd9":"# here we are calculating base model score\nscore = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n","0d9d8fe7":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","7d2a46b5":"# calculating Base model score\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","7dc72f72":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","1af67e98":"# calculating base model score\nscore = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","4d07a7f6":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n","4b9e49cc":"# calculating base model score\nscore = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","91b093bc":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","e3503d8a":"averaged_models = AveragingModels(models = (ENet, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","b5edded6":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)\n","fc28f0c1":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet,KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","85fa88d1":"def rmsle(Y, y_pred):\n    return np.sqrt(mean_squared_error(Y, y_pred))","cb2bacf4":"stacked_averaged_models.fit(x.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(x.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(y.values))\nprint(rmsle(y_train, stacked_train_pred))","2d40304b":"model_lgb.fit(x, y_train)\nlgb_train_pred = model_lgb.predict(x)\nlgb_pred = np.expm1(model_lgb.predict(y.values))\nprint(rmsle(y_train, lgb_train_pred))","f683c1c8":"# RMSE on the entire Train data when averaging\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               + lgb_train_pred*0.15 ))","6f5988b5":"ensemble = stacked_pred*0.70+ lgb_pred*0.15\nensemble","42fd2eee":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)\nprint(sub)","7934ed80":"This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline.","6986f8d3":"#  Base Models","bea6aca3":"**Stacking Averaged models Score**","1e18bf16":"Here we have completed our prediction . If there is need of any improvement that please feel free to share it.\nIf you found this notebook helpful or you just liked it , some upvotes would be very much appreciated That will keep me motivated to update it on a regular basis.","8a4c4199":"# Feature Engineering...","d179e461":"# Conclusion","fdf1677d":"**Lasso regrssion**","f22858ac":"# Data pre-processing","8d498559":"# Introduction","9c95537a":"# Stacking models","07c231b9":"# Building Model","5654e206":"In this compition we are going to do prediction of house price.Prediction house prices are expected to help people who plan to buy a house so they can know the price range in the future, then they can plan their finance well. In addition, house price predictions are also beneficial for property investors to know the trend of housing prices in a certain location.\nwe are using advanced machine learning techniques for doing prediction.","d6799c61":"Traget Variable.\nSalePrice is the variable we need to predict, So let's do some analysis on this variable first.","e4ba00df":"**Averaged base models class**","3dc74b32":"**Box Cox Transformation of (highly) skewed features**","657b6ab0":"**Imputing missing value**","c070c985":"# Reading dataset...","99b376fb":"**Submission**","67a5a88e":"# Data visualization","5fc28f77":"# Contents","0419a265":"**Stacking averaged Models Class**","e4bb6956":"**Skewed features**","4368a14c":"**Transforming some numerical variables that are really categorical**","0d0bd38b":"**Averaged base models score**","265a18ff":"1. Importing important libraries\n2. Reading dataset\n3. Data visualization\n4. Feature Engineering\n5. Data pre-processing\n6. Building models\n7. Base models\n8. Stacking models\n9. Submission\n10. Conclusion","0db8fe8b":"**Kernel Ridge Regression**","325d6acd":"**LightGBM**","fdb0b293":"here we can see that at the bottom right two with extremely otliers","1e7504f0":"**Elastic Net Regression**","317ea529":"again for outliers","b24ab2d5":"**Final Training and Prediction**","1d38b3bc":"# Importing important libraries...","1cecce28":"here we are doing Log-transformation of the target variable.","7a3f1c0c":"**Performing Label Encoding**"}}