{"cell_type":{"30eb68f2":"code","28a1d966":"code","efb58f9c":"code","5e3ce96b":"code","9b7adba3":"code","359fe358":"code","bd307b71":"code","1db49eac":"code","bcfba4ca":"code","6b7de7b9":"code","ec6e49db":"code","7a9c8ba8":"code","1f944aab":"code","13887937":"code","4143cc83":"code","63419b00":"code","7409f012":"code","222f6dcd":"code","edd990a7":"code","721365f3":"code","2a86b7da":"code","346c55d6":"code","9636481e":"code","9e9f68c5":"code","25dac2c5":"code","9040518c":"code","ebff1e75":"code","b041ec4d":"code","d1f87d02":"code","c54bb0e1":"code","45909381":"code","172b8fb0":"code","d189cf33":"code","4ca98f8b":"code","c7a9060f":"code","d04b09eb":"code","141f687d":"code","68bd4b4c":"code","c0fd2486":"code","f33bb348":"code","9e399e48":"code","d7d11b37":"code","bb09d691":"code","e51205cb":"code","f805c590":"code","de9eb9cd":"code","9e0430b4":"code","c451d4b2":"code","d6db3cc6":"code","c333c287":"code","412bcc64":"code","6af31c25":"code","30c0da32":"code","36ba1ab0":"code","7b65b958":"code","7fcc5d12":"code","6b925e2f":"code","3574b138":"code","3a7542c7":"code","8646e8ec":"code","f1ce4330":"code","ace8165f":"code","4c1eef2e":"code","cd091d51":"code","e153804f":"code","82c16ab0":"code","7da77add":"code","8d732db0":"code","e88ada3b":"code","0e18c5b6":"code","75de43ca":"code","eedce933":"code","b60f4c1c":"code","044bcb32":"code","95416d61":"code","f0666d94":"code","89754dc0":"code","7b2734f4":"code","866845a4":"code","ca60ec04":"code","b5ebbfb9":"code","d65ac73b":"code","5fa8932a":"code","05311831":"code","fee85889":"code","48f7113b":"code","d22ab443":"code","336db2f5":"code","6c372a60":"code","6cbfbf09":"code","dd0ea1b9":"code","2f886a13":"code","2c7a5865":"code","8b22a34d":"code","830404d0":"code","bbb95fcc":"code","a92aea0f":"code","56fdfee0":"code","57ed4b53":"code","6feef194":"code","ec01ad3e":"code","b0a3aaff":"code","d99734d1":"code","b627ac83":"code","e6b06e87":"code","509a6ee1":"code","67d77e16":"code","9a50d18e":"code","3e3dec8b":"code","0eea728e":"code","a87f9e48":"code","cf5dfcdb":"code","6105bb48":"code","807be908":"code","51c0ccc9":"code","b5288d36":"code","db5a70a4":"code","a529a9c7":"code","5625f67d":"code","fc97ea79":"code","c531acdd":"code","c6214c6e":"code","f05af967":"code","833c378e":"code","561da610":"code","5e135700":"code","78a2d781":"code","95b651ee":"code","7c8b5148":"code","9b210bd6":"code","5153b54c":"code","43c4ae54":"code","31000d10":"code","a1e9937e":"code","0b3dac6e":"code","127cfe76":"code","9c277aec":"code","9b0e30ab":"code","2ef83022":"code","9eeeaa46":"code","5a75854a":"code","01918fab":"code","78e1592f":"code","eab5423e":"code","3c6da3fb":"code","1cec18e0":"code","1e9c35d7":"code","c821ff39":"code","1285b729":"code","e46b67f6":"code","3365b809":"code","18135cf3":"code","793b7073":"code","68e13609":"code","5397683b":"code","8171a2f9":"code","e2796fc4":"code","6a660b72":"code","84667993":"code","36e619a3":"code","88b861f5":"code","bead800c":"code","2b9ffc2b":"code","01c53d16":"code","8a9c8e5d":"code","4e691698":"code","a52e4326":"code","2e8e8824":"code","b376ae78":"code","664f6f44":"code","a145871b":"code","ef23e2da":"code","24496745":"code","e939a687":"code","60a50262":"code","8e80e054":"code","99315917":"code","57bc1ccf":"code","7e3c5c31":"code","60cd22a8":"code","0dc09775":"code","6c0b8526":"code","8e83a93b":"code","8f6cbedc":"code","960923ca":"code","60710140":"code","10ff57e5":"code","a410c4bc":"code","3a33d456":"code","37d5510b":"code","c4b42ed2":"code","4adc4912":"code","1ec15c5f":"code","1465eda9":"code","620102a0":"code","3e8b7eae":"code","f6c5fd65":"code","d8c7247b":"code","e06ca670":"code","d36eb6b0":"code","939ff111":"code","3288eec2":"code","4cba67ad":"code","9dda13fe":"code","24da9d44":"code","1119008c":"code","023d7255":"code","9ca07cf3":"code","ddc08202":"code","71fbe0a7":"code","c1f36083":"code","f65f42d3":"code","40092cdb":"code","fdfa589d":"code","b7e21eab":"code","5c7245fa":"code","641fc5b7":"code","6ec099cc":"code","7f7c31fb":"code","35afbf3f":"code","2a98dfc3":"code","8ac994bf":"code","73295b0f":"code","9f8c0657":"code","fb941944":"code","032340fd":"code","f77581af":"code","bdb56072":"code","57111905":"code","e20f621d":"code","b93b8ae2":"code","57321664":"code","2a18a50a":"code","b247ec5f":"code","63d421ab":"code","b4a48ad2":"code","1dd55ef1":"code","1729f7d8":"code","52a5f1f6":"code","44fcc7c5":"code","7af60d99":"code","89c24a25":"code","1337b7d5":"code","331c3ece":"code","f072cfb8":"code","bc84ff95":"code","4065838e":"code","4778828d":"code","39bfa705":"code","68e7c190":"code","566af29a":"code","371a0122":"code","deaec8e8":"code","6dd00176":"code","9ee57a69":"code","defff6c0":"code","85dd955e":"code","371aea1d":"code","fec4bae1":"code","3b473c71":"code","5cd90104":"code","76960627":"code","a90100bf":"code","1080cd9a":"code","4782777d":"code","0f0c06a5":"code","b5b57b6c":"code","c36806b7":"code","d649816b":"code","f5327594":"code","a654536b":"code","7ae81ed1":"code","da760ee1":"code","3bb82dc5":"code","8433b57f":"code","7e1e5417":"code","ab3d2d5f":"code","75c1771e":"code","6dd976ce":"code","b7bd1d63":"code","31076276":"code","d737b335":"markdown","c9332731":"markdown","04ba530c":"markdown","327af032":"markdown","d35394c8":"markdown","2c841b72":"markdown","465965e7":"markdown","9b735fbb":"markdown","8714962c":"markdown","e53bfd8d":"markdown","c13fde01":"markdown","84e81fc3":"markdown","7e77b483":"markdown","d94013a3":"markdown","8be19730":"markdown","208abcf8":"markdown","1e0bed1b":"markdown","3650b921":"markdown","49ad2662":"markdown","c15c6320":"markdown","4f708826":"markdown","c69156a8":"markdown","473854fb":"markdown","44c2b566":"markdown","4cd211e1":"markdown","33514c68":"markdown","953cce28":"markdown","72a703d0":"markdown","07c84f5d":"markdown","b22ac9ba":"markdown","49da180f":"markdown","cc5967d0":"markdown","132135c1":"markdown","5e7153f5":"markdown","c7aaecf7":"markdown","edfd6b9a":"markdown","3788dbc9":"markdown","6497c5af":"markdown","5b94bbe5":"markdown","5bed257d":"markdown","4dd349b3":"markdown","ba65e23b":"markdown","6b5e6c60":"markdown","a168ea59":"markdown","1afddf29":"markdown","60ef9d2e":"markdown","fd467afd":"markdown","aca096a6":"markdown","976d5c92":"markdown","c10ab0dc":"markdown","4a906352":"markdown","2a41ee82":"markdown","e36f14c5":"markdown","1493d6b1":"markdown","63365a17":"markdown","5e898abf":"markdown","6943dda8":"markdown","32b21d97":"markdown","abc61724":"markdown","b32c2bd2":"markdown","5f7122c0":"markdown","0ddbeec3":"markdown","b1f73574":"markdown","f9589145":"markdown","89d129d9":"markdown","e6143a1a":"markdown","f7c29658":"markdown","76b601fd":"markdown","8e1b3354":"markdown","d582510f":"markdown","da44aa51":"markdown","01e6c71a":"markdown","c2ec8662":"markdown","9b1d088e":"markdown","ff23f83e":"markdown","85cb527d":"markdown","9f8814e1":"markdown","24b44344":"markdown","bd5d1e44":"markdown","942e656d":"markdown","adf11ff3":"markdown","1135a727":"markdown","55c5935f":"markdown","4318c959":"markdown","6c9880f4":"markdown","0e1ea04d":"markdown","43a1ffbf":"markdown","c1421292":"markdown","95323364":"markdown","94f5b2b6":"markdown","a1e45bc0":"markdown","caa25434":"markdown","b87e5ae5":"markdown","60529fad":"markdown","769005cc":"markdown","3929f6a9":"markdown","105dda2c":"markdown","238ef151":"markdown","162a579e":"markdown","f78fad2b":"markdown","db23fdce":"markdown","cb8033ff":"markdown","726c7f94":"markdown","dd0d7dae":"markdown","5adb1a55":"markdown","99979e10":"markdown","704c934e":"markdown","3370c863":"markdown","5dd004f9":"markdown","dbf5176c":"markdown","c5ba1e3c":"markdown","061425f1":"markdown","c8d2698e":"markdown","d3b0d29b":"markdown","24e6383f":"markdown","f2d6c584":"markdown","0b85c6c1":"markdown","394ca39d":"markdown","9ded5b79":"markdown","4b35a8b3":"markdown","07b372d0":"markdown","54478b20":"markdown","8ace3ba2":"markdown","802edbcf":"markdown","98844666":"markdown","a82518a7":"markdown","e55cb0e4":"markdown","9662e7c3":"markdown","0bf236c0":"markdown","3aa3d8d0":"markdown","68c01772":"markdown","9ec749b8":"markdown","7bac10a8":"markdown","78aad921":"markdown","50486877":"markdown"},"source":{"30eb68f2":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))  ","28a1d966":"from warnings import filterwarnings\nfilterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nads = pd.read_csv('..\/input\/advertising\/Advertising.csv',usecols=[1,2,3,4])\ndata = ads.copy()\ndata.head() # This is datasets","efb58f9c":"# Data's information\ndata.info()","5e3ce96b":"# Transposition of descriptive statistics\ndata.describe().T","9b7adba3":"# We check for missing values\ndata.isnull().any()","359fe358":"# Correlation map between variables\ndata.corr()","bd307b71":"# Let's make the same connection situation with the help of visuals\nimport seaborn as sns\nsns.pairplot(data,kind='reg');","1db49eac":"sns.jointplot(x='TV',y='sales',data=data,kind='reg');","bcfba4ca":"import statsmodels.api as sm\nx = data[['TV']] # TV Values\nx[:5] # First 5","6b7de7b9":"x = sm.add_constant(x) # We have to add a column of 1.0\nx[:5] # Like this","ec6e49db":"y = data[['sales']] # The value of y becomes our dependent variable\ny[:5]","7a9c8ba8":"linear = sm.OLS(y,x) # We create a model\nmodel = linear.fit() # We fit a model\nmodel.summary() # This is summary","1f944aab":"model.params # Parameters of the model","13887937":"model.summary().tables[1] # Location of model parameters in the table","4143cc83":"model.fittedvalues[0:5] # Let's reach the predicted values","63419b00":"y[:5] # Real values in these","7409f012":"print(\"Sales = \" +  str(\"%.2f\" % model.params[0]) + \" + TV\" + \"*\" + str(\"%.2f\" % model.params[1]))\n# Mathematical representation of the established prediction model.","222f6dcd":"g = sns.regplot(data[\"TV\"], data[\"sales\"], ci=None, scatter_kws={'color':'g', 's':15})\ng.set_title(\"Model Equation: Sales = 7.03 + TV*0.05\")\ng.set_ylabel(\"Sales\")\ng.set_xlabel(\"TV Spending\")\nimport matplotlib.pyplot as plt\nplt.xlim(-10,310)\nplt.ylim(bottom=0);","edd990a7":"from sklearn.linear_model import LinearRegression\nx = data[['TV']]\ny = data['sales']\nregression = LinearRegression()\nmodel = regression.fit(x,y)\nprint('Model Intercept:',model.intercept_)\nprint('Model Coef:',model.coef_)","721365f3":"model.score(x,y) # Model Square Score","2a86b7da":"model.predict(x)[:10] # Predict with sklearn","346c55d6":"7.03 + 30 * 0.04 # b0 + x * b1","9636481e":"model.predict([[30]]) # Can also be done this way","9e9f68c5":"MULTI = [[5],[90],[200]] # Can be made in 3 pieces\nmodel.predict(MULTI)","25dac2c5":"from sklearn.metrics import mean_squared_error, r2_score\nimport statsmodels.formula.api as smf\nlinear = smf.ols(\"sales ~ TV\", data)\nmodel = linear.fit()","9040518c":"mse = mean_squared_error(y,model.fittedvalues) # Mean Squared Error\nrmse = np.sqrt(mse) # Root-mean-square deviation\nprint('MSE:',mse)\nprint('RMSE:',rmse)","ebff1e75":"print('Predict Values:',regression.predict(x)[:10]) # Predict Values\nprint('Real Values',y[:10]) # Real Values","b041ec4d":"remains = pd.DataFrame({'real_values':y[:10],\n                       'predict_values':regression.predict(x)[:10]})\nremains","d1f87d02":"remains['error'] = remains['real_values'] - remains['predict_values'] # We subtract predicted values from real values\nremains","c54bb0e1":"remains['error_square'] = remains['error']**2 # Square Of Errors\nremains","45909381":"np.sum(remains['error_square']) # Error squares sum","172b8fb0":"np.mean(remains[\"error_square\"]) # Error squares mean","d189cf33":"model.resid[0:10] # These residues appear with the function","4ca98f8b":"plt.plot(model.resid,color='red');","c7a9060f":"ads = pd.read_csv('..\/input\/advertising\/Advertising.csv',usecols=[1,2,3,4])\ndata = ads.copy()\ndata.head()","d04b09eb":"from sklearn.model_selection import train_test_split,cross_val_predict,cross_val_score\n# Train and test library for seperating\n# library for cross validation predict\n# librar for cross validation score\nx = data.drop('sales',axis=1) # I synchronized variables other than the Sales variable to X.\ny = data['sales'] # Our sales dependent variable in y.\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=42) #We divided it as train and test\n# Test_size allowed me to divide the argument by 80% to 20%\n# random_state has always split the same values \u200b\u200bwithout producing different values each time.\ntrainig = data.copy() # I put it in a variable so that a data set may be needed","141f687d":"linear = sm.OLS(y_train,x_train)\nmodel = linear.fit()\nmodel.summary()","68bd4b4c":"linear = LinearRegression()\nmodel = linear.fit(x_train,y_train) # We building model","c0fd2486":"print('Model Intercept:',model.intercept_) # constant coefficient\nprint('Model Coef:',model.coef_) # Other independent variable coefficients","f33bb348":"pre_value = [[30],[10],[40]]\npre_value = pd.DataFrame(pre_value).T\nprint('Result:',model.predict(pre_value))","9e399e48":"print('Train Error Score:',np.sqrt(mean_squared_error(y_train,model.predict(x_train)))) # Accessing train error\nprint('Test Error Score:',np.sqrt(mean_squared_error(y_test,model.predict(x_test)))) # Accessing test error","d7d11b37":"data.head() # The dataset","bb09d691":"x = data.drop(['sales'],axis=1)\ny = data['sales']\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30,random_state=42)\nlinear = LinearRegression()\nmodel = linear.fit(x,y)\nprint('Train Error:',np.sqrt(mean_squared_error(y_train,model.predict(x_train))))\nprint('Model Score:',model.score(x_train,y_train))\nprint('Our verified score',cross_val_score(model,x_train,y_train,cv=10,scoring='r2').mean())","e51205cb":"hit = pd.read_csv('..\/input\/hitters\/Hitters.csv')\ndata = hit.copy()\ndata.dropna(inplace=True)\ndata.head()","f805c590":"dms = pd.get_dummies(data[['League','Division','NewLeague']])\ndms.head()\n# Now we get which one of this transformation to be 1","de9eb9cd":"y = data['Salary']\nx_ = data.drop(['Salary','League','Division','NewLeague'],axis=1).astype('float64')\nx_.head() # we extracted the variables and the dependent variable that we will make dummy from our dataset","9e0430b4":"x = pd.concat([x_,dms[['League_N','Division_W','NewLeague_N']]],axis=1)\n# We added the categorical variables we converted\nx.head()","c451d4b2":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=42)\nprint('x_train:',x_train.shape)\nprint('x_test:',x_test.shape)\nprint('y_train:',y_train.shape)\nprint('y_test:',y_test.shape)","d6db3cc6":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import scale\npca = PCA()","c333c287":"x_reduced_train = pca.fit_transform(scale(x_train))\nx_reduced_train[0:1,:]\n# Yes, our reduction process is ok","412bcc64":"np.cumsum(np.round(pca.explained_variance_ratio_,decimals=4)*100)[:10]\n# Explained variance","6af31c25":"from sklearn.linear_model import LinearRegression\nlinear = LinearRegression()\npcr_model = linear.fit(x_reduced_train,y_train)\n# Here is our reduced model","30c0da32":"print('Reduced Model Intercept:',pcr_model.intercept_)\nprint('Reduced Model Coef:',pcr_model.coef_)","36ba1ab0":"y_pred = pcr_model.predict(x_reduced_train)\nprint('First 5 Predict:',y_pred[:5])","7b65b958":"from sklearn.metrics import mean_squared_error,r2_score\nprint('Mean Square Error:',np.sqrt(mean_squared_error(y_train,y_pred)))\nprint('R2 Score:',r2_score(y_train,y_pred))","7fcc5d12":"PCATEST = PCA()\nx_reduced_test = PCATEST.fit_transform(scale(x_test))\ny_pred = pcr_model.predict(x_reduced_test)\nprint('Mean Square Error:',np.sqrt(mean_squared_error(y_test,y_pred)))","6b925e2f":"linear = LinearRegression()\npcr_model = linear.fit(x_reduced_train[:,0:1],y_train)\ny_pred = pcr_model.predict(x_reduced_test[:,0:1])\nprint('Mean Square Error:',np.sqrt(mean_squared_error(y_test,y_pred)))\n# One component","3574b138":"linear = LinearRegression()\npcr_model = linear.fit(x_reduced_train[:,0:2],y_train)\ny_pred = pcr_model.predict(x_reduced_test[:,0:2])\nprint('Mean Square Error:',np.sqrt(mean_squared_error(y_test,y_pred)))\n# Two component","3a7542c7":"from sklearn import model_selection\ncv_10 = model_selection.KFold(n_splits=10,shuffle=True,random_state=42)\nlinear = LinearRegression()\nRMSE=[]\nfor i in np.arange(1,x_reduced_train.shape[1]+1):\n    score = np.sqrt(-1*model_selection.cross_val_score(linear,\n                                                      x_reduced_train[:,:i],\n                                                      y_train.ravel(),\n                                                      cv=cv_10,\n                                                      scoring='neg_mean_squared_error').mean())\n    RMSE.append(score)\n\nimport matplotlib.pyplot as plt\nplt.plot(RMSE,'-v')\nplt.xlabel('Component Quantity')\nplt.ylabel('RMSE')\nplt.title('PCR Model Tuning');","8646e8ec":"linear = LinearRegression()\npcr_model = linear.fit(x_reduced_train[:,0:4],y_train)\ny_pred = pcr_model.predict(x_reduced_train[:,0:4])\nprint('Mean Square Error:',np.sqrt(mean_squared_error(y_train,y_pred)))\n# This is our ideal mistake","f1ce4330":"from sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split,cross_val_score\nimport pandas as pd\nimport numpy as np\nhit = pd.read_csv('..\/input\/hitters\/Hitters.csv')\ndata = hit.copy()\ndata = data.dropna()\ndms = pd.get_dummies(data[['League','Division','NewLeague']])\ny = data['Salary']\nx_ = data.drop(['Salary','League','Division','NewLeague'],axis=1).astype('float64')\nx = pd.concat([x_,dms[['League_N','Division_W','NewLeague_N']]],axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state= 42)","ace8165f":"from sklearn.cross_decomposition import PLSRegression, PLSSVD\npls_model = PLSRegression(n_components=2).fit(x_train,y_train)\npls_model.coef_ # We have the coefficients of the model.","4c1eef2e":"print('Predict Values(Train Set):',pls_model.predict(x_train)[:5])\ny_pred = pls_model.predict(x_train)\nprint('Mean Square Error for Train Set:',np.sqrt(mean_squared_error(y_train,y_pred)))\nprint('R2 Error for Train Set:',r2_score(y_train,y_pred))","cd091d51":"y_pred = pls_model.predict(x_test)\nprint('Mean Square Error for Test Set:',np.sqrt(mean_squared_error(y_test,y_pred)))","e153804f":"import matplotlib.pyplot as plt\nfrom sklearn import model_selection\ncv_10 = model_selection.KFold(n_splits=10,shuffle=True,random_state=42)\nRMSE=[]\nfor i in np.arange(1,x_train.shape[1] + 1):\n    pls = PLSRegression(n_components=i)\n    score=np.sqrt(-1*cross_val_score(pls,x_train,y_train,cv=10,scoring='neg_mean_squared_error').mean())\n    RMSE.append(score)\nplt.plot(np.arange(1,x_train.shape[1]+1),np.array(RMSE),'-v',c='r')\nplt.xlabel('Component Quantity')\nplt.ylabel('RMSE')\nplt.title('Salary');","82c16ab0":"hit = pd.read_csv('..\/input\/hitters\/Hitters.csv')\ndata = hit.copy()\ndata.dropna(inplace=True)\ndms = pd.get_dummies(data[['League','Division','NewLeague']])\ny = data['Salary']\nx_ = data.drop(['Salary','League','Division','NewLeague'],axis=1).astype('float64')\nx = pd.concat([x_,dms[['League_N','Division_W','NewLeague_N']]],axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=42)","7da77add":"from sklearn.linear_model import Ridge\nridge_model = Ridge(alpha=0.1).fit(x_train,y_train)\nprint('Ridge Model Coef:',ridge_model.coef_)","8d732db0":"lambdas = 10**np.linspace(10,-2,100)*0.5\nridge_model = Ridge()\ncoef = []\nfor i in lambdas:\n    ridge_model.set_params(alpha=i)\n    ridge_model.fit(x_train,y_train)\n    coef.append(ridge_model.coef_)\nax = plt.gca()\nax.plot(lambdas,coef)\nax.set_xscale('Log')\nplt.xlabel('Lambda(Alpha) Value')\nplt.ylabel('Coef\/Heavy')\nplt.title('Ridge Coefficients as a Function of Regulation');","e88ada3b":"y_pred = ridge_model.predict(x_test)\nprint('Ridge Model Predict:',np.sqrt(mean_squared_error(y_test,y_pred)))","0e18c5b6":"lambdas = 10**np.linspace(10,-2,100)*0.5\nfrom sklearn.linear_model import RidgeCV\nridge_cv = RidgeCV(alphas=lambdas,scoring='neg_mean_squared_error',normalize=True)\nridge_cv.fit(x_train,y_train)","75de43ca":"print('Ridge Alphas:',ridge_cv.alpha_)","eedce933":"ridge_tuned = Ridge(alpha=ridge_cv.alpha_,normalize=True).fit(x_train,y_train)\nprint('Verified Model Error:',np.sqrt(mean_squared_error(y_test,ridge_tuned.predict(x_test))))","b60f4c1c":"hit = pd.read_csv('..\/input\/hitters\/Hitters.csv')\ndata = hit.copy()\ndata.dropna(inplace=True)\ndms = pd.get_dummies(data[['League','Division','NewLeague']])\ny = data['Salary']\nx_ = data.drop(['Salary','League','Division','NewLeague'],axis=1).astype('float64')\nx = pd.concat([x_,dms[['League_N','Division_W','NewLeague_N']]],axis=1)\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30,random_state=42)","044bcb32":"from sklearn.linear_model import Lasso\nlasso_model = Lasso(alpha=0.01).fit(x_train,y_train)","95416d61":"lambdas = 10**np.linspace(10,-2,100)*0.5\nlasso = Lasso()\ncoef = []\nfor i in lambdas:\n    lasso.set_params(alpha=i)\n    lasso.fit(x_train,y_train)\n    coef.append(lasso.coef_)\nax = plt.gca()\nax.plot(lambdas**2,coef)\nax.set_xscale('log')\nplt.axis('tight')\nplt.xlabel('Lambda(Alpha)')\nplt.ylabel('Weights');","f0666d94":"print('Predict Values:',lasso_model.predict(x_test))","89754dc0":"from sklearn.linear_model import LassoCV\nlasso_model_cv = LassoCV(alphas=None,\n                        cv=10,\n                        max_iter=1000,\n                        normalize=True)\nlasso_model_cv.fit(x_train,y_train)\nprint('Lasso Model Alpha:',lasso_model_cv.alpha_)","7b2734f4":"lasso_tuned_model = Lasso(alpha=lasso_model_cv.alpha_).fit(x_train,y_train)\nprint('Verified Score:',np.sqrt(mean_squared_error(y_test,lasso_tuned_model.predict(x_test))))","866845a4":"hit = pd.read_csv('..\/input\/hitters\/Hitters.csv')\ndata = hit.copy()\ndata.dropna(inplace=True)\ndms = pd.get_dummies(data[['League','Division','NewLeague']])\ny = data['Salary']\nx_ = data.drop(['Salary','League','Division','NewLeague'],axis=1).astype('float64')\nx = pd.concat([x_,dms[['League_N','Division_W','NewLeague_N']]],axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=42)","ca60ec04":"from sklearn.linear_model import ElasticNet\nelastic_model = ElasticNet().fit(x_train,y_train)\nprint('Elastic Net Coef:',elastic_model.coef_)\nprint('Elastic Net Intercept:',elastic_model.intercept_)","b5ebbfb9":"print('Elastic Net Predict Values:',elastic_model.predict(x_test))","d65ac73b":"y_pred = elastic_model.predict(x_test)\nprint('Simple Error Score:',np.sqrt(mean_squared_error(y_test,y_pred)))","5fa8932a":"from sklearn.linear_model import ElasticNetCV\nelastic_cv_model = ElasticNetCV(cv=10,random_state=42).fit(x_train,y_train)","05311831":"print('Optimum Alpha Score:',elastic_cv_model.alpha_)","fee85889":"elastic_tuned = ElasticNet(alpha=elastic_cv_model.alpha_).fit(x_train,y_train)\ny_pred = elastic_tuned.predict(x_test)\nprint('Verified Score:',np.sqrt(mean_squared_error(y_test,y_pred)))","48f7113b":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split,ShuffleSplit,GridSearchCV\nfrom sklearn.metrics import mean_squared_error,r2_score\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import BaggingRegressor","d22ab443":"hit = pd.read_csv('..\/input\/hitters\/Hitters.csv')\ndata = hit.copy()\ndata.dropna(inplace=True)\ndms = pd.get_dummies(data[['League','Division','NewLeague']])\ny = data['Salary']\nx_ = data.drop(['Salary','League','Division','NewLeague'],axis=1).astype('float64')\nx = pd.concat([x_,dms[['League_N','Division_W','NewLeague_N']]],axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=42)","336db2f5":"knn_model = KNeighborsRegressor().fit(x_train,y_train)","6c372a60":"y_pred = knn_model.predict(x_test)\nprint('KNN Model Predict Values:',np.sqrt(mean_squared_error(y_test,y_pred)))","6cbfbf09":"from sklearn.model_selection import GridSearchCV\nknn_parameter = {\n    'n_neighbors':np.arange(1,50,1) # We do the dictionary naming in accordance with the parameter of the algorithm. not random.\n}\nKNN = KNeighborsRegressor()\nknn_cv_model = GridSearchCV(KNN,knn_parameter,cv=10)\nknn_cv_model.fit(x_train,y_train)","dd0ea1b9":"print('Best Parameter:',knn_cv_model.best_params_['n_neighbors'])","2f886a13":"# We are re-modeling with our new parameter.\nknn_tuned = KNeighborsRegressor(n_neighbors=knn_cv_model.best_params_['n_neighbors'])\nknn_tuned.fit(x_train,y_train)\ny_pred = knn_tuned.predict(x_test)\nprint('Verified Score:',np.sqrt(mean_squared_error(y_test,y_pred)))","2c7a5865":"hit = pd.read_csv('..\/input\/hitters\/Hitters.csv')\ndata = hit.copy()\ndata  = data.dropna()\ndms = pd.get_dummies(data[['League','Division','NewLeague']])\ny = data['Salary']\nx_ = data.drop(['Salary','League','Division','NewLeague'],axis = 1).astype('float64')\nx = pd.concat([x_,dms[['League_N','Division_W','NewLeague_N']]],axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,\n                                                test_size = 0.25,\n                                                random_state=42)\nx_train = pd.DataFrame(x_train['Hits'])\nx_test = pd.DataFrame(x_test['Hits'])","8b22a34d":"from sklearn.svm import SVR","830404d0":"svr_model  = SVR('linear').fit(x_train,y_train)\nprint('SVR Model Predict:',svr_model.predict(x_test)[:5])\ny_pred = svr_model.predict(x_train)\nprint('Model Equation->','y = {0} + {1} x '.format(svr_model.intercept_[0], # Model Equation\n                               svr_model.coef_[0][0]))","bbb95fcc":"plt.scatter(x_train,y_train)\nplt.plot(x_train,y_pred,color='r');","a92aea0f":"svr_model.predict([[54]]) # Estimates are created this way","56fdfee0":"# Test Error Calculation\ny_pred = svr_model.predict(x_test)\nprint('SVR Model Error Score:',np.sqrt(mean_squared_error(y_test,y_pred)))","57ed4b53":"svr_params = {'C':np.arange(0.1,2,3)}\nsvr = SVR()\nsvr_cv_model = GridSearchCV(svr,svr_params,cv=10).fit(x_train,y_train)\n# The model is set up this way","6feef194":"print('Best Parametre for model:',svr_cv_model.best_params_['C'])","ec01ad3e":"svr_tuned = SVR('linear',C=svr_cv_model.best_params_['C']).fit(x_train,y_train)\ny_pred = svr_tuned.predict(x_test)\nprint('Verified Error Score:',np.sqrt(mean_squared_error(y_test,y_pred)))","b0a3aaff":"np.random.seed(3)\nx_sim = np.random.uniform(2,10,145)\ny_sim = np.sin(x_sim) + np.random.normal(0,0.4,145)\nx_outliers = np.arange(2.5,5,0.5)\ny_outliers = -5*np.arange(5)\nx_sim_idx = np.argsort(np.concatenate([x_sim,x_outliers]))\nx_sim = np.concatenate([x_sim,x_outliers])[x_sim_idx]\ny_sim = np.concatenate([y_sim,y_outliers])[x_sim_idx]","d99734d1":"from sklearn.linear_model import LinearRegression\nols = LinearRegression()\nols.fit(np.sin(x_sim[:,np.newaxis]),y_sim)\nols_pred = ols.predict(np.sin(x_sim[:,np.newaxis]))\nfrom sklearn.svm import SVR\neps = 0.1\nsvr = SVR('rbf',epsilon=eps)\nsvr.fit(x_sim[:,np.newaxis],y_sim)\nsvr_pred = svr.predict(x_sim[:,np.newaxis])","b627ac83":"plt.scatter(x_sim,y_sim,alpha=0.5,s=26)\nplt_ols = plt.plot(x_sim,ols_pred,color ='g')\nplt_svr = plt.plot(x_sim,svr_pred,color='r')\nplt.xlabel('independent variable')\nplt.ylabel('dependent variable')\nplt.ylim(-5.2,2.2)\nplt.legend(['LSE','SVR']);","e6b06e87":"hit = pd.read_csv('..\/input\/hitters\/Hitters.csv')\ndata = hit.copy()\ndata  = data.dropna()\ndms = pd.get_dummies(data[['League','Division','NewLeague']])\ny = data['Salary']\nx_ = data.drop(['Salary','League','Division','NewLeague'],axis = 1).astype('float64')\nx = pd.concat([x_,dms[['League_N','Division_W','NewLeague_N']]],axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,\n                                                test_size = 0.30,\n                                                random_state=42)","509a6ee1":"svr_rbf = SVR('rbf',gamma='auto').fit(x_train,y_train) # The 'rbf' here means non-linear SVR.","67d77e16":"y_pred = svr_rbf.predict(x_test) # Estimate for x test\nprint('Prediction First 5 Values:',y_pred[:5])\nprint('Simple Error:',np.sqrt(mean_squared_error(y_test,y_pred)))","9a50d18e":"svr_params = {\n    'C':[0.1,0.4,5,10,20,30,40,50]\n}\nsvr_rbf_model = SVR('rbf')\nsvr_cv_model = GridSearchCV(svr_rbf_model,svr_params,cv=10)\nsvr_cv_model.fit(x_train,y_train)","3e3dec8b":"print('Best C Parameter:',svr_cv_model.best_params_['C'])","0eea728e":"svr_tuned = SVR('rbf',gamma='auto',C=svr_cv_model.best_params_['C']).fit(x_train,y_train)","a87f9e48":"y_pred = svr_tuned.predict(x_test)\nprint('Verified Score:',np.sqrt(mean_squared_error(y_test,y_pred)))","cf5dfcdb":"hit = pd.read_csv('..\/input\/hitters\/Hitters.csv')\ndata = hit.copy()\ndata  = data.dropna()\ndms = pd.get_dummies(data[['League','Division','NewLeague']])\ny = data['Salary']\nx_ = data.drop(['Salary','League','Division','NewLeague'],axis = 1).astype('float64')\nx = pd.concat([x_,dms[['League_N','Division_W','NewLeague_N']]],axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,\n                                                test_size = 0.25,\n                                                random_state=42)","6105bb48":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(x_train)\nx_train_scaled = scaler.transform(x_train)\nx_test_scaled = scaler.transform(x_test)","807be908":"from sklearn.neural_network import MLPRegressor","51c0ccc9":"MLP_model = MLPRegressor(hidden_layer_sizes=(100,20)).fit(x_train_scaled,y_train)\nMLP_model.n_layers_ # Our number of layers","b5288d36":"y_pred = MLP_model.predict(x_test_scaled)\nprint('Simple Score Error:',np.sqrt(mean_squared_error(y_test,y_pred)))","db5a70a4":"mlp_params = {\n    'alpha':[0.1,0.01,0.02,0.0005],\n    'hidden_layer_sizes':[(20,20),(100,50,150),(300,200,150)],\n    'activation':['relu','logistic']\n}\nmlp_model = MLPRegressor()\nmlp_cv_model = GridSearchCV(mlp_model,mlp_params,cv=10)\nmlp_cv_model.fit(x_train_scaled,y_train)","a529a9c7":"print('Best Alpha Parameter:',mlp_cv_model.best_params_['alpha'])\nprint('Best Hidden Layer Sizes:',mlp_cv_model.best_params_['hidden_layer_sizes'])\nprint('Best Activation Function:',mlp_cv_model.best_params_['activation'])","5625f67d":"mlp_tuned = MLPRegressor(alpha=mlp_cv_model.best_params_['alpha'],\n                        hidden_layer_sizes=mlp_cv_model.best_params_['hidden_layer_sizes'],\n                        activation=mlp_cv_model.best_params_['activation'])\nmlp_tuned.fit(x_train_scaled,y_train)\ny_pred = mlp_tuned.predict(x_test_scaled)\nprint('Verified Error Score:',np.sqrt(mean_squared_error(y_test,y_pred)))","fc97ea79":"hit = pd.read_csv('..\/input\/hitters\/Hitters.csv')\ndata = hit.copy()\ndata  = data.dropna()\ndms = pd.get_dummies(data[['League','Division','NewLeague']])\ny = data['Salary']\nx_ = data.drop(['Salary','League','Division','NewLeague'],axis = 1).astype('float64')\nx = pd.concat([x_,dms[['League_N','Division_W','NewLeague_N']]],axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,\n                                                test_size = 0.25,\n                                                random_state=42)\nx_train = pd.DataFrame(x_train['Hits'])\nx_test = pd.DataFrame(x_test['Hits'])","c531acdd":"cart_model = DecisionTreeRegressor(max_leaf_nodes=10) # I gave Max leaf randomly.\ncart_model.fit(x_train,y_train)","c6214c6e":"x_grid = np.arange(min(np.array(x_train)),max(np.array(x_train)),0.01)\nx_grid = x_grid.reshape((len(x_grid),1))\nplt.scatter(x_train,y_train,color='lime')\nplt.plot(x_grid,cart_model.predict(x_grid),color='Green')\nplt.title('Cart Regression Tree')\nplt.xlabel('Hits')\nplt.ylabel('Salary')\nplt.show()","f05af967":"y_pred = cart_model.predict(x_test)\nprint('Simple Predict Error:',np.sqrt(mean_squared_error(y_test,y_pred)))","833c378e":"params = {'min_samples_split':range(2,100),\n         'max_leaf_nodes':range(2,10)}\ncart_model_cv = GridSearchCV(cart_model,params,cv=10)\ncart_model_cv.fit(x_train,y_train)","561da610":"print('Best Parameter For Model',cart_model_cv.best_params_)","5e135700":"cart_tuned = DecisionTreeRegressor(max_leaf_nodes=cart_model_cv.best_params_['max_leaf_nodes'],min_samples_split=cart_model_cv.best_params_['min_samples_split'])\ncart_tuned.fit(x_train,y_train)\ny_pred = cart_tuned.predict(x_test)\nprint('Verified Error Score:',np.sqrt(mean_squared_error(y_test,y_pred)))","78a2d781":"hit = pd.read_csv('..\/input\/hitters\/Hitters.csv')\ndata = hit.copy()\ndata  = data.dropna()\ndms = pd.get_dummies(data[['League','Division','NewLeague']])\ny = data['Salary']\nx_ = data.drop(['Salary','League','Division','NewLeague'],axis = 1).astype('float64')\nx = pd.concat([x_,dms[['League_N','Division_W','NewLeague_N']]],axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,\n                                                test_size = 0.25,\n                                                random_state=42)","95b651ee":"from sklearn.ensemble import RandomForestRegressor\nrf_model = RandomForestRegressor(random_state=42)\nrf_model.fit(x_train,y_train)\ny_pred = rf_model.predict(x_test)\nprint('Simple Test Error:',np.sqrt(mean_squared_error(y_test,y_pred)))","7c8b5148":"rf_params = {'max_depth':list(range(1,10)),\n            'max_features':[3,5,10,15],\n            'n_estimators':[100,200,500,1000,2000]}\n\nrf_model = RandomForestRegressor(random_state=42)\nrf_model_cv = GridSearchCV(rf_model,rf_params,cv=10,n_jobs=-1,verbose=2) # If n_jobs -1, the number of parameters to be searched will take the processors to full performance.\nrf_model_cv.fit(x_train,y_train)","9b210bd6":"print('Best Max Depth:',rf_model_cv.best_params_['max_depth'])\nprint('Best Max Features:',rf_model_cv.best_params_['max_features'])\nprint('Best N Estimators:',rf_model_cv.best_params_['n_estimators'])","5153b54c":"rf_model_tuned = RandomForestRegressor(max_depth=rf_model_cv.best_params_['max_depth'],\n                                      max_features=rf_model_cv.best_params_['max_features'],\n                                      n_estimators=rf_model_cv.best_params_['n_estimators']).fit(x_train,y_train)","43c4ae54":"y_pred = rf_model_tuned.predict(x_test)\nprint('Verified Error Score:',np.sqrt(mean_squared_error(y_test,y_pred)))","31000d10":"Importance = pd.DataFrame({'Importance':rf_model_tuned.feature_importances_*100},\n                         index=x_train.columns)\n# We check this for which variables are more important model.\n\nImportance.sort_values(by='Importance',axis=0,ascending=True).plot(kind='barh',color='lime')\nplt.xlabel('Significance of variables');","a1e9937e":"from sklearn.ensemble import GradientBoostingRegressor\ngbm_model = GradientBoostingRegressor()\ngbm_model.fit(x_train,y_train)","0b3dac6e":"y_pred = gbm_model.predict(x_test)\nprint('Simple Test Error:',np.sqrt(mean_squared_error(y_test,y_pred)))","127cfe76":"gbm_params = {\n    'learning_rate':[0.001,0.01,0.1,0.2],\n    'max_depth':[3,5,8,50,100],\n    'n_estimators':[200,500,1000,2000],\n    'subsample':[1,0.5,0.75]\n}\ngbm = GradientBoostingRegressor()\ngbm_cv_model = GridSearchCV(gbm,gbm_params,cv=10,n_jobs=-1,verbose=2)\ngbm_cv_model.fit(x_train,y_train)","9c277aec":"print('Best Paramter For Model:',gbm_cv_model.best_params_)","9b0e30ab":"gbm_tuned = GradientBoostingRegressor(learning_rate=gbm_cv_model.best_params_['learning_rate'],\n                                     max_depth=gbm_cv_model.best_params_['max_depth'],\n                                     n_estimators=gbm_cv_model.best_params_['n_estimators'],\n                                     subsample=gbm_cv_model.best_params_['subsample']).fit(x_train,y_train)","2ef83022":"y_pred = gbm_tuned.predict(x_test)\nprint('Verified Test Error Score:',np.sqrt(mean_squared_error(y_test,y_pred)))","9eeeaa46":"Importance = pd.DataFrame({'Importance':gbm_tuned.feature_importances_*100},\n                         index=x_train.columns)\nImportance.sort_values(by='Importance',axis=0,ascending=True).plot(kind='barh',color='Red')\nplt.xlabel('Significance Level of Variables');","5a75854a":"hit = pd.read_csv('..\/input\/hitters\/Hitters.csv')\ndata = hit.copy()\ndata  = data.dropna()\ndms = pd.get_dummies(data[['League','Division','NewLeague']])\ny = data['Salary']\nx_ = data.drop(['Salary','League','Division','NewLeague'],axis = 1).astype('float64')\nx = pd.concat([x_,dms[['League_N','Division_W','NewLeague_N']]],axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,\n                                                test_size = 0.25,\n                                                random_state=42)","01918fab":"import xgboost as xgb","78e1592f":"DM_train = xgb.DMatrix(data= x_train,label=y_train)\nDM_test = xgb.DMatrix(data=x_test,label=y_test)","eab5423e":"from xgboost import XGBRegressor","3c6da3fb":"xgb = XGBRegressor().fit(x_train,y_train)","1cec18e0":"y_pred = xgb.predict(x_test)\nprint('Simple Test Error:',np.sqrt(mean_squared_error(y_test,y_pred)))","1e9c35d7":"xgb_grid = {\n    'colsample_bytree':[0.4,0.5,0.6,0.9,1],\n    'n_estimators':[100,200,500,1000],\n    'max_depth':[2,3,4,5,6],\n    'learning_rate':[0.1,0.01,0.5]\n}\nxgb = XGBRegressor()\nxgb_cv_model = GridSearchCV(xgb,param_grid=xgb_grid,cv=10,n_jobs=-1,verbose=2)\nxgb_cv_model.fit(x_train,y_train)","c821ff39":"print('XGB Best Paramter:',xgb_cv_model.best_params_)","1285b729":"xgb_tuned = XGBRegressor(colsample_bytree = xgb_cv_model.best_params_['colsample_bytree'],\n                        learning_rate=xgb_cv_model.best_params_['learning_rate'],\n                        max_depth=xgb_cv_model.best_params_['max_depth'],\n                        n_estimators=xgb_cv_model.best_params_['n_estimators']).fit(x_train,y_train)","e46b67f6":"y_pred = xgb_tuned.predict(x_test)\nprint('Verified Test Error Score:',np.sqrt(mean_squared_error(y_test,y_pred)))","3365b809":"import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,cross_val_predict\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report,log_loss\nfrom sklearn.metrics import roc_auc_score,roc_curve\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","18135cf3":"diabetes = pd.read_csv('..\/input\/diabetes\/diabetes.csv')\ndata = diabetes.copy()\ndata.dropna(inplace=True)\ndata.head()","793b7073":"y = data['Outcome']\nx = data.drop(['Outcome'],axis=1)","68e13609":"logistic = sm.Logit(y,x)\nlogistic_model = logistic.fit()\nlogistic_model.summary()\n# We are not strangers to this output we have seen before.","5397683b":"from sklearn.linear_model import LogisticRegression\nlogistic = LogisticRegression(solver='liblinear')\nlogistic_model = logistic.fit(x,y)","8171a2f9":"y_pred = logistic_model.predict(x)\nprint(confusion_matrix(y,y_pred))","e2796fc4":"print('Classification Ratio:',accuracy_score(y,y_pred))\nprint('******************************')\nprint(classification_report(y,y_pred))","6a660b72":"logistic_model.predict(x)[:10] # Predict Values","84667993":"print('Probability Of Forecast Values:',logistic_model.predict_proba(x)[:10][:,0:2])","36e619a3":"print('Reel Values:',y[:10])","88b861f5":"logit_roc_auc = roc_auc_score(y,logistic_model.predict(x))\nfpr,tpr,thresholds = roc_curve(y,logistic_model.predict_proba(x)[:,1])\nplt.figure()\nplt.plot(fpr,tpr,label='AUC(area=%0.2f)' % logit_roc_auc)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Ratio')\nplt.ylabel('True Positive Ratio')\nplt.title('ROC')\nplt.show()","bead800c":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30,random_state=42)\n# Cross Val. We perform train test separation to calculate the score.","2b9ffc2b":"logistic = LogisticRegression(solver='liblinear')\nlogistic_model = logistic.fit(x,y)","01c53d16":"print('Test Succes Score:',accuracy_score(y_test,logistic_model.predict(x_test)))\nprint('The Most Accurate Success Rate',cross_val_score(logistic_model,x_test,y_test,cv=10).mean())","8a9c8e5d":"diabetes = pd.read_csv('..\/input\/diabetes\/diabetes.csv')\ndata = diabetes.copy()\ndata.dropna(inplace=True)\ny = data['Outcome']\nx = data.drop(['Outcome'],axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=42)","4e691698":"from sklearn.naive_bayes import GaussianNB\nGuasNB = GaussianNB()\nGuasNB_model = GuasNB.fit(x_train,y_train)","a52e4326":"print('Guassian Naive Bayes Prediction Values:',GuasNB_model.predict(x_test)[:10])\nprint('************************')\nprint('Reel Dataset Values',y[:10])","2e8e8824":"y_pred = GuasNB_model.predict(x_test)\nprint('Accuracy Score:',accuracy_score(y_test,y_pred))","b376ae78":"print('Verified Score:',cross_val_score(GuasNB,x_test,y_test,cv=10).mean())","664f6f44":"diabetes = pd.read_csv('..\/input\/diabetes\/diabetes.csv')\ndata = diabetes.copy()\ndata.dropna(inplace=True)\ny = data['Outcome']\nx = data.drop(['Outcome'],axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=10,random_state=42)","a145871b":"KNN = KNeighborsClassifier()\nKNN_model= KNN.fit(x_train,y_train)","ef23e2da":"y_pred = KNN_model.predict(x_test)\nprint('Simple Accuracy Score:',accuracy_score(y_test,y_pred))","24496745":"print(classification_report(y_test,y_pred))\n# We came out in more detail.","e939a687":"knn_params = {\n    'n_neighbors':np.arange(1,50)\n}\nknn = KNeighborsClassifier()\nknn_cv_model = GridSearchCV(knn,knn_params,cv=10)\nknn_cv_model.fit(x_train,y_train)","60a50262":"print('Best Parameter for model:', str(knn_cv_model.best_params_['n_neighbors']))\nprint('Best Score for model:',knn_cv_model.best_score_)","8e80e054":"knn_tuned = KNeighborsClassifier(n_neighbors=knn_cv_model.best_params_['n_neighbors']).fit(x_train,y_train)\nprint('Score Funciton Value:',knn_tuned.score(x_test,y_test))","99315917":"y_pred = knn_tuned.predict(x_test)\nprint('Verified Score:',accuracy_score(y_test,y_pred))","57bc1ccf":"diabetes = pd.read_csv('..\/input\/diabetes\/diabetes.csv')\ndata = diabetes.copy()\ndata.dropna(inplace=True)\ny = data['Outcome']\nx = data.drop(['Outcome'],axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=42)","7e3c5c31":"svm_model = SVC(kernel='linear').fit(x_train,y_train)","60cd22a8":"y_pred = svm_model.predict(x_test)\nprint('Simple Accuracy Score:',accuracy_score(y_test,y_pred))","0dc09775":"svc_params = {\n    'C':np.arange(1,10)\n}\nsvc = SVC(kernel='linear')\nsvc_cv_model = GridSearchCV(svc,svc_params,cv=10,n_jobs=-1,verbose=2)\nsvc_cv_model.fit(x_train,y_train)","6c0b8526":"print('Best Parameter for model:',svc_cv_model.best_params_['C'])","8e83a93b":"svc_tuned = SVC(kernel='linear',C=svc_cv_model.best_params_['C']).fit(x_train,y_train)\ny_pred = svc_tuned.predict(x_test)\nprint('Verified Score:',accuracy_score(y_test,y_pred))","8f6cbedc":"diabetes = pd.read_csv('..\/input\/diabetes\/diabetes.csv')\ndata = diabetes.copy()\ndata.dropna(inplace=True)\ny = data['Outcome']\nx = data.drop(['Outcome'],axis=1)\nx_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.30,random_state=42)","960923ca":"svc_rbf = SVC(kernel='rbf').fit(x_train,y_train)\ny_pred = svc_rbf.predict(x_test)\nprint('Simple Accuracy Score:',accuracy_score(y_test,y_pred))","60710140":"svc_params = {\n    'C':[0.0001,0.001,0.1,1,5,10,50,100],\n    'gamma':[0.0001,0.001,0.1,1,5,10,50,100]\n}\nsvc = SVC(kernel='rbf')\nsvc_cv_model = GridSearchCV(svc,svc_params,cv=10,n_jobs=-1,verbose=2)\nsvc_cv_model.fit(x_train,y_train)","10ff57e5":"print('Best Parameters For model:',svc_cv_model.best_params_)","a410c4bc":"svc_tuned = SVC(kernel='rbf',\n               C=svc_cv_model.best_params_['C'],\n               gamma=svc_cv_model.best_params_['gamma']).fit(x_train,y_train)","3a33d456":"y_pred = svc_tuned.predict(x_test)\nprint('Verified Accuracy Score:',accuracy_score(y_test,y_pred))","37d5510b":"import pandas as pd\nfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,cross_val_predict\nfrom sklearn.metrics import accuracy_score\ndiabetes = pd.read_csv('..\/input\/diabetes\/diabetes.csv')\ndata = diabetes.copy()\ndata.dropna(inplace=True)\ny = data['Outcome']\nx = data.drop(['Outcome'],axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=42)","c4b42ed2":"# We need a small standardization process\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(x_train)\nx_train_scaled = scaler.transform(x_train)\nx_test_scaled = scaler.transform(x_test)","4adc4912":"from warnings import filterwarnings\nfilterwarnings('ignore')\nfrom sklearn.neural_network import MLPClassifier\nMLPC = MLPClassifier().fit(x_train_scaled,y_train)","1ec15c5f":"y_pred = MLPC.predict(x_test_scaled)\nprint('Simple Test Error:',accuracy_score(y_test,y_pred))","1465eda9":"mlpc_params = {\n    'alpha':[0.1,0.01,0.02,0.005,0.0001,0.00001],\n    'hidden_layer_sizes':[(10,10,10),\n                         (100,100,100),\n                         (100,100),\n                         (3,5),\n                         (5,3)],\n    'solver':['lbgfs','adam','sgd'],\n    'activation':['relu','logistic']\n}\n\nmlpc = MLPClassifier()\nmlpc_cv_model = GridSearchCV(mlpc,mlpc_params,cv=10,n_jobs=-1,verbose=2)\nmlpc_cv_model.fit(x_train_scaled,y_train)","620102a0":"print('Best Parameters for model:',mlpc_cv_model.best_params_)","3e8b7eae":"mlpc_tuned = MLPClassifier(activation=mlpc_cv_model.best_params_['activation'],\n                          alpha=mlpc_cv_model.best_params_['alpha'],\n                          hidden_layer_sizes=mlpc_cv_model.best_params_['hidden_layer_sizes'],\n                          solver=mlpc_cv_model.best_params_['solver']).fit(x_train_scaled,y_train)","f6c5fd65":"y_pred = mlpc_tuned.predict(x_test_scaled)\nprint('Verified Test Score:',accuracy_score(y_test,y_pred))","d8c7247b":"diabetes = pd.read_csv('..\/input\/diabetes\/diabetes.csv')\ndata = diabetes.copy()\ndata.dropna(inplace=True)\ny = data['Outcome']\nx = data.drop(['Outcome'],axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=42)","e06ca670":"from sklearn.tree import DecisionTreeClassifier\ncart = DecisionTreeClassifier()\ncart_model = cart.fit(x_train,y_train)","d36eb6b0":"!pip install astor","939ff111":"!pip install skompiler","3288eec2":"from skompiler import skompile\nprint(skompile(cart_model.predict).to('python\/code'))","4cba67ad":"y_pred = cart_model.predict(x_test)\nprint('Simple Test Error:',accuracy_score(y_test,y_pred))","9dda13fe":"cart_grid = {\n    'max_depth':list(range(1,20)),\n    'min_samples_split':list(range(2,100))\n}\ncart = DecisionTreeClassifier()\ncart_cv_model = GridSearchCV(cart,cart_grid,n_jobs=-1,verbose=2).fit(x_train,y_train)","24da9d44":"print('Best parameters for model:' + str(cart_cv_model.best_params_))","1119008c":"cart_tuned = DecisionTreeClassifier(max_depth=cart_cv_model.best_params_['max_depth'],\n                                   min_samples_split=cart_cv_model.best_params_['min_samples_split']).fit(x_train,\n                                                                                                         y_train)","023d7255":"y_pred = cart_tuned.predict(x_test)\nprint('Verified Test Error Score:',accuracy_score(y_test,y_pred))","9ca07cf3":"diabetes = pd.read_csv('..\/input\/diabetes\/diabetes.csv')\ndata = diabetes.copy()\ndata.dropna(inplace=True)\ny = data['Outcome']\nx = data.drop(['Outcome'],axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=42)","ddc08202":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf_model = rf.fit(x_train,y_train)\ny_pred = rf_model.predict(x_test)\nprint('Simple Test Error:',accuracy_score(y_test,y_pred))","71fbe0a7":"rf_params = {\n    'max_depth':[2,3,5,8,10],\n    'max_features':[2,5,8],\n    'n_estimators':[10,500,1000,2000],\n    'min_samples_split':[2,5,10]\n}\nrf = RandomForestClassifier()\nrf_cv_model = GridSearchCV(rf,rf_params,cv=10,n_jobs=-1,verbose=2).fit(x_train,y_train)","c1f36083":"print('Best parameters for model:',rf_cv_model.best_params_)","f65f42d3":"rf_tuned = RandomForestClassifier(max_depth=rf_cv_model.best_params_['max_depth'],\n                                 max_features=rf_cv_model.best_params_['max_features'],\n                                 min_samples_split=rf_cv_model.best_params_['min_samples_split'],\n                                 n_estimators=rf_cv_model.best_params_['n_estimators']).fit(x_train,y_train)","40092cdb":"y_pred = rf_tuned.predict(x_test)\nprint('Verified Error Score:',accuracy_score(y_test,y_pred))","fdfa589d":"# Let's take a look at the importance levels of the parameters.\n\nimport matplotlib.pyplot as plt\nImportance = pd.DataFrame({'Importance':rf_tuned.feature_importances_*100},\n                         index=x_train.columns)\nImportance.sort_values(by='Importance',axis=0,ascending=True).plot(kind='barh',color='Lime')\nplt.xlabel('Importance Variable');","b7e21eab":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import accuracy_score\ndiabetes = pd.read_csv('..\/input\/diabetes\/diabetes.csv')\ndata = diabetes.copy()\ndata.dropna(inplace=True)\ny = data['Outcome']\nx = data.drop(['Outcome'],axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=42)","5c7245fa":"from sklearn.ensemble import GradientBoostingClassifier\nGradient = GradientBoostingClassifier()\ngbm_model = Gradient.fit(x_train,y_train)","641fc5b7":"y_pred = gbm_model.predict(x_test)\nprint('Simple Test Error:',accuracy_score(y_test,y_pred))","6ec099cc":"gbm_params = {\n    'learning_rate':[0.001,0.01,0.1,0.5],\n    'n_estimators':[100,500,1000],\n    'max_depth':[3,5,10],\n    'min_samples_split':[2,5,10]\n}\ngbm = GradientBoostingClassifier()\ngbm_cv_model = GridSearchCV(gbm,gbm_params,cv=10,n_jobs=-1,verbose=2)\ngbm_cv_model.fit(x_train,y_train)","7f7c31fb":"print('Best parameters of model:',gbm_cv_model.best_params_)","35afbf3f":"gbm_tuned = GradientBoostingClassifier(learning_rate=gbm_cv_model.best_params_['learning_rate'],\n                                      max_depth=gbm_cv_model.best_params_['max_depth'],\n                                      min_samples_split=gbm_cv_model.best_params_['min_samples_split'],\n                                      n_estimators=gbm_cv_model.best_params_['n_estimators']).fit(x_train,y_train)","2a98dfc3":"y_pred = gbm_tuned.predict(x_test)\nprint('Verified Test Error Score:',accuracy_score(y_test,y_pred))","8ac994bf":"diabetes = pd.read_csv('..\/input\/diabetes\/diabetes.csv')\ndata = diabetes.copy()\ndata.dropna(inplace=True)\ny = data['Outcome']\nx = data.drop(['Outcome'],axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=42)","73295b0f":"from xgboost import XGBClassifier\nxgb_model = XGBClassifier().fit(x_train,y_train)\ny_pred = xgb_model.predict(x_test)\nprint('Simple Test Error:',accuracy_score(y_test,y_pred))","9f8c0657":"xgb_params = {\n    'n_estimators':[100,500,1000,2000],\n    'subsample':[0.6,0.8,1.0],\n    'max_depth':[3,4,5,6],\n    'learning_rate':[0.1,0.01,0.02,0.05]\n}\nxgb = XGBClassifier()\nxgb_cv_model = GridSearchCV(xgb,xgb_params,cv=10,n_jobs=-1,verbose=2).fit(x_train,y_train)","fb941944":"print('Best parameters of model:',xgb_cv_model.best_params_)","032340fd":"xgb_tuned = XGBClassifier(learning_rate=xgb_cv_model.best_params_['learning_rate'],\n                         max_depth=xgb_cv_model.best_params_['max_depth'],\n                         n_estimators=xgb_cv_model.best_params_['n_estimators'],\n                         subsample=xgb_cv_model.best_params_['subsample']).fit(x_train,y_train)","f77581af":"y_pred = xgb_tuned.predict(x_test)\nprint('Verified Test Error Score:',accuracy_score(y_test,y_pred))","bdb56072":"from warnings import filterwarnings\nfilterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nfrom sklearn.cluster import KMeans\nUSA = pd.read_csv('..\/input\/usadata\/USA.csv')\ndata = USA.copy()\ndata.head()","57111905":"# I'll take a few steps to fix the part that says Unnamed.\ndata.index = data.iloc[:,0]\ndata = data.iloc[:,1:5]\ndata.index.name = None\ndata.head() # Done","e20f621d":"# We need control to dataset\ndata.isnull().any()\n# Done","b93b8ae2":"kmeans = KMeans(n_clusters=4)\nk_fit = kmeans.fit(data)","57321664":"print('Cluster:',k_fit.n_clusters)\nprint('************************************************************************************************')\nprint('Cluster Center:',k_fit.cluster_centers_)\nprint('************************************************************************************************')\nprint('Class Labels:',k_fit.labels_)","2a18a50a":"kmeans = KMeans(n_clusters=2)\nk_fit = kmeans.fit(data)\ncluster = k_fit.labels_\nplt.scatter(data.iloc[:,0],data.iloc[:,1],c=cluster,cmap='viridis')\ncenters = k_fit.cluster_centers_\nplt.scatter(centers[:,0],centers[:,1],c='black',s=200,alpha=0.5);","b247ec5f":"from mpl_toolkits.mplot3d import Axes3D\nkmeans = KMeans(n_clusters=3)\nk_fit = kmeans.fit(data)\nclusters = k_fit.labels_\ncenters = k_fit.cluster_centers_\nplt.rcParams['figure.figsize'] = (16,9)\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(data.iloc[:,0],data.iloc[:,1],data.iloc[:,2])\nax.scatter(centers[:,0],centers[:,1],centers[:,2],marker='*',c='#050505',s=1000);","63d421ab":"kmeans = KMeans(n_clusters=3)\nk_fit = kmeans.fit(data)\nclusters = k_fit.labels_\npd.DataFrame({'States':data.index,\n             'Cluster':clusters})[:10]","b4a48ad2":"data['Clusters_NO'] = clusters\ndata.head()","1dd55ef1":"from yellowbrick.cluster import KElbowVisualizer\nkmeans = KMeans()\nvisualizer = KElbowVisualizer(kmeans,k=(2,20))\nvisualizer.fit(data)\nvisualizer.poof();","1729f7d8":"kmeans = KMeans(n_clusters=6) # Best cluster num. = 6\nk_fit = kmeans.fit(data)\nclusters = k_fit.labels_\npd.DataFrame({'States':data.index,\n             'Clusters':clusters})[:10] # New Segmentation Datasets","52a5f1f6":"import pandas as pd\nimport numpy as np\ndata = pd.read_csv('..\/input\/usadata\/USA.csv').copy()\ndata.index = data.iloc[:,0]\ndata = data.iloc[:,1:5]\ndata.index.name = None\ndata.head()","44fcc7c5":"from scipy.cluster.hierarchy import linkage\nhc_complete = linkage(data,'complete')\nhc_average = linkage(data,'average')\nhc_single = linkage(data,'single')\n# We need to include these methods.","7af60d99":"import matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram\nplt.figure(figsize=(15,10))\nplt.title('Hierarchical Clustering - Dendogram')\nplt.xlabel('Indexes')\nplt.ylabel('Distance')\ndendrogram(hc_complete,\n          leaf_font_size=10);","89c24a25":"from scipy.cluster.hierarchy import dendrogram\nplt.figure(figsize=(15,10))\nplt.title('Hierarchical Clustering - Dendogram')\nplt.xlabel('Indexes')\nplt.ylabel('Distance')\ndendrogram(hc_complete,\n          leaf_font_size=10);","1337b7d5":"from sklearn.cluster import AgglomerativeClustering","331c3ece":"cluster = AgglomerativeClustering(n_clusters=4,affinity='euclidean',linkage='ward')\ncluster.fit_predict(data)","f072cfb8":"pd.DataFrame({\n    'States':data.index,\n    'Clusters':cluster.fit_predict(data)\n})[:10]","bc84ff95":"data['Clusters_NO'] = cluster.fit_predict(data)\ndata.head()","4065838e":"data = pd.read_csv('..\/input\/usadata\/USA.csv').copy()\ndata.index = data.iloc[:,0]\ndata = data.iloc[:,1:5]\ndata.index.name = None\ndata.head()","4778828d":"# To implement a PCA, we first need to standardize variables.\nfrom sklearn.preprocessing import StandardScaler\ndata = StandardScaler().fit_transform(data)\ndata[:5]","39bfa705":"from sklearn.decomposition import PCA\npca = PCA(n_components=2) # 2 component\npca_fit = pca.fit_transform(data)","68e7c190":"compo_data = pd.DataFrame(data=pca_fit,columns=['1st_component','2nd_component'])\ncompo_data[:5]","566af29a":"pca.explained_variance_ratio_  # Variance explained for components\n# Add these two together tells us that an average of 85% is represented.","371a0122":"# Visual answer to the question of how many components should we use.\npca = PCA().fit(data)\nplt.plot(np.cumsum(pca.explained_variance_ratio_));","deaec8e8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier,NeighborhoodComponentsAnalysis,LocalOutlierFactor\nfrom sklearn.decomposition import PCA\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","6dd00176":"cancer = pd.read_csv('..\/input\/cancer\/cancer.csv')\ndata = cancer.copy()\ndata.drop(['Unnamed: 32','id'],axis=1,inplace=True)\ndata.head()","9ee57a69":"# I want to change the diagnosis variable name to target.\ndata = data.rename(columns= {'diagnosis':'target'})\ndata.head()","defff6c0":"# How many M and how many B's are we examining them.\nsns.countplot(data['target']);\nprint(data.target.value_counts())","85dd955e":"# I need to convert the M and B in the target variable to 0 and 1.\n# .strip () removes spaces in string expressions.\ndata['target'] = [1 if i.strip() == 'M'else 0 for i in data.target]","371aea1d":"# 1 malignant so M\n# 0 benign so B\ndata.head()","fec4bae1":"data.info()","3b473c71":"data.describe()\n# strictly standardization process is required for this data","5cd90104":"# EDA\n# Since all variables we have are numeric, we look at corr ().\nax = plt.figure(figsize=(15,8))\nax = sns.heatmap(data.corr());","76960627":"# Since this is so confusing, I'll set a Threshold and cover those above it.\ncor_mat = data.corr()\nthreshold = 0.75\nfilters = np.abs(cor_mat['target']) > threshold\ncorr_features = cor_mat.columns[filters].tolist()\nax = plt.figure(figsize=(15,8))\nax = sns.heatmap(data[corr_features].corr(),annot=True,linewidths=.3)\nplt.title('Correlation Between Features w Corr Threshold 0.75');","a90100bf":"# box plot\ndata_melted = pd.melt(data,id_vars='target',var_name='features',value_name='value')\nax = plt.figure(figsize=(15,8))\nax = sns.boxplot(x='features',y='value',hue='target',data=data_melted)\nplt.xticks(rotation=90);\n# Because the data is not standardized here, it becomes a strange table, we will use it later.","1080cd9a":"# Pair plot is one of the most effective methods used in numerical data.\n# This will not look nice either, because the data needs to be standardized.\nsns.pairplot(data[corr_features],diag_kind='kde',markers='+',hue='target');\n# But I just used corr_features for images.","4782777d":"# Outliers\ny = data['target']\nx = data.drop(['target'],axis=1)\ncolumns = x.columns.tolist()","0f0c06a5":"clf = LocalOutlierFactor()\ny_pred = clf.fit_predict(x)\nx_score = clf.negative_outlier_factor_\noutlier_score = pd.DataFrame()\noutlier_score['score'] = x_score\noutlier_score.head()","b5b57b6c":"plt.figure(figsize=(8,5))\nplt.scatter(x.iloc[:,0],x.iloc[:,1],color='b',s=3,label='Data Points')\nplt.legend();","c36806b7":"radius = (x_score.max() - x_score) \/ (x_score.max() - x_score.min())","d649816b":"plt.figure(figsize=(8,5))\nplt.scatter(x.iloc[:,0],x.iloc[:,1],color='k',s=3,label='Data Points')\nplt.scatter(x.iloc[:,0],x.iloc[:,1],s=1000*radius,edgecolors='r',facecolors='none',label='Outlier Scores')\nplt.legend()\nplt.show()","f5327594":"# We are looking at contradictory observations.\nthreshold = -2.5\nfiltre = outlier_score['score'] < threshold\noutlier_index = outlier_score[filtre].index.tolist()\nplt.figure(figsize=(8,5))\nplt.scatter(x.iloc[outlier_index,0],x.iloc[outlier_index,1],color='b',s=50,label='Outlier')\nplt.scatter(x.iloc[:,0],x.iloc[:,1],color='k',s=3,label='Data Points')\nplt.scatter(x.iloc[:,0],x.iloc[:,1],s=1000*radius,edgecolors='r',facecolors='none',label='Outlier Scores')\nplt.legend()\nplt.show()","a654536b":"# Drop outliers\nx = x.drop(outlier_index)\ny = y.drop(outlier_index).values\n# All of these, there are a lot of other variables waiting for outlier observations for columns 0 and 1.","7ae81ed1":"# Train Test seperation\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n# Standart\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","da760ee1":"# Let's visualize boxplot that was not visualized before.\nx_train_df = pd.DataFrame(x_train,columns=columns)\nx_train_df['target'] = y_train\ndata_melted = pd.melt(x_train_df,id_vars='target',var_name='features',value_name='value')\nplt.figure(figsize=(15,8))\nsns.boxplot(x='features',y='value',hue='target',data=data_melted)\nplt.xticks(rotation=90)\nplt.show()","3bb82dc5":"knn = KNeighborsClassifier(n_neighbors=2)\nknn.fit(x_train,y_train)\ny_pred = knn.predict(x_test)\ncm = confusion_matrix(y_test,y_pred)\nacc = accuracy_score(y_test,y_pred)\nscore = knn.score(x_test,y_test)\nprint('Score:',score)\nprint('Confusion Matrix:',cm)\nprint('Basic Accuracy Score:',acc)","8433b57f":"def knn_best_params(x_train,x_test,y_train,y_test):\n    k_range = list(range(1,31))\n    weight_options = ['uniform','distance']\n    print()\n    param_grid = dict(n_neighbors = k_range,weights=weight_options)\n    knn = KNeighborsClassifier()\n    grid = GridSearchCV(knn,param_grid,cv=10,scoring='accuracy')\n    grid.fit(x_train,y_train)\n    print('Best Training Score {} with parameters: {}'.format(grid.best_score_,grid.best_params_))\n    print()\n    \n    knn = KNeighborsClassifier(**grid.best_params_)\n    knn.fit(x_train,y_train)\n    \n    y_pred_test = knn.predict(x_test)\n    y_pred_train = knn.predict(x_train)\n    cm_test = confusion_matrix(y_test,y_pred_test)\n    cm_train = confusion_matrix(y_train,y_pred_train)\n    acc_test = accuracy_score(y_test,y_pred_test)\n    acc_train = accuracy_score(y_train,y_pred_train)\n    print('Test Score: {},Train Score: {}'.format(acc_test,acc_train))\n    print()\n    print('Confusion Matrix Test: {}'.format(cm_test))\n    print('Confusion Matrix Train: {}'.format(cm_train))\n    \n    return grid","7e1e5417":"grid = knn_best_params(x_train,x_test,y_train,y_test)","ab3d2d5f":"scaler = StandardScaler()\nx_scaled = scaler.fit_transform(x)\npca = PCA(n_components=2)\npca.fit(x_scaled)\nx_reduced_pca = pca.transform(x_scaled)\npca_data = pd.DataFrame(x_reduced_pca,columns=['p1','p2'])\npca_data['target'] = y\nplt.figure(figsize=(8,5))\nsns.scatterplot(x='p1',y='p2',hue='target',data=pca_data)\nplt.title('PCA: p1 vs p2')\nplt.show()\n# We reduced 30 dimensional data to 2 dimensions with PCA.","75c1771e":"# Now we will do a chnn using 2 dimensional data.\nx_train_pca,x_test_pca,y_train_pca,y_test_pca = train_test_split(x_reduced_pca,y,test_size=0.3,random_state=42)\ngrid_pca = knn_best_params(x_train_pca,x_test_pca,y_train_pca,y_test_pca)","6dd976ce":"# We use a visualization to see how the split is decided.\ncmap_light = ListedColormap(['orange','cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange','darkblue'])\nh = .05\nX = x_reduced_pca\nx_min,x_max = X[:,0].min() -1,X[:,0].max() + 1\ny_min,y_max = X[:,1].min() -1 ,X[:,1].max() + 1\nxx,yy = np.meshgrid(np.arange(x_min,x_max,h),\n                   np.arange(y_min,y_max,h))\nZ = grid_pca.predict(np.c_[xx.ravel(),yy.ravel()])\nZ = Z.reshape(xx.shape)\nplt.figure(figsize=(10,8))\nplt.pcolormesh(xx,yy,Z,cmap=cmap_light)\nplt.scatter(X[:,0],X[:,1],c=y,cmap=cmap_bold,\n           edgecolors='k',s=20)\nplt.xlim(xx.min(),xx.max())\nplt.ylim(yy.min(),yy.max())\nplt.title(\"%i-Class Classification (k= %i,weights = '%s')\"%(len(np.unique(y)),grid_pca.best_estimator_.n_neighbors,grid_pca.best_estimator_.weights))","b7bd1d63":"nca = NeighborhoodComponentsAnalysis(n_components=2,random_state=42)\nnca.fit(x_scaled,y)\nx_reduced_nca = nca.transform(x_scaled)\nnca_data = pd.DataFrame(x_reduced_nca,columns=['p1','p2'])\nnca_data['target'] = y\nplt.figure(figsize=(10,8))\nsns.scatterplot(x='p1',y='p2',hue='target',data=nca_data)\nplt.title('NCA : p1 vs p2')\nplt.show()","31076276":"x_train_nca,x_test_nca,y_train_nca,y_test_nca = train_test_split(x_reduced_nca,y,test_size=0.3,random_state=42)\ngrid_nca = knn_best_params(x_train_nca,x_test_nca,y_train_nca,y_test_nca)","d737b335":"**I quickly did what we did above.**","c9332731":"# KNN Model","04ba530c":"**Let's take a look at the order of importance of variables.**","327af032":"<a id=\"6\"><\/a> <br>\n\n# Real Life\nHere we will consider all what we have learned as a real life problem on a different data set.How algorithms are implemented etc. I expect you to make sense by examining the codes on issues such as\n\nGood reading.","d35394c8":"**There is a distortion, we need to fix it.**","2c841b72":"# Nonlinear SVR","465965e7":"# Predict","9b735fbb":"# K-Means Model And Visualization","8714962c":"**This was valid for the train dataset, now let's do it in the test.**","e53bfd8d":"# Sets And Observation Units","c13fde01":"Yes we got a very high score as you can see.In real life problems these methods and algorithms work more or less like this.Here we just covered a problem for KNN, but I showed you a lot of algorithms, you can use them all.You just need to understand the problem correctly and make the right decisions.I did not explain too much while addressing the problem because I explained everything in detail in the tutorial.I hope this real life problem works for you too","84e81fc3":"- The success will occur if I leave the red line randomly if I don't do any assignment work.\n- If blue is our success rate.","7e77b483":"<a id=\"4\"><\/a> <br>\n# Classification Models\n\n- [KNN](#72)\n\n- [Support Vector Machines](#73) \n\n- [ANN](#74)\n\n- [Classification and Regression Trees (CART)](#75)\n\n- Bagging (Bootstrap Aggregation)\n\n- [Random Forests (RF)](#76)\n\n- [Gradient Boosting Machines (GBM)](#77)\n\n- [Extreme Gradient Boosting (XGBoost)](#78)\n\n- [Logistic Regression](#70)\n\n- [Naive Bayes](#71)","d94013a3":"# Modeling with Scikit-Learn","8be19730":"# Model Tuning For XGB","208abcf8":"**We re-model with our verified parameter.**","1e0bed1b":"<a id=\"62\"><\/a> <br>\n# Artificial Neural Networks\n\nIt is one of the powerful machine learning algorithms that can be used for classification and regression problems that refer to the way the human brain processes information.","3650b921":"**Re-modeling with new parameters.**","49ad2662":"**We will do a small process to use xgb more efficiently.**","c15c6320":"# Predict","4f708826":"# Model Tuning for CART Model","c69156a8":"# PLS - Partial Least Squares Regression\n\nIt is based on the idea of \u200b\u200breducing the variables to less number of components that do not have multiple linear connection problems and establishing a regression model.\n\n- Multi-dimensional curse p > n\n- Multiple linear connection problem\n- PLS also finds linear combinations of the independent variable like PCR. These linear combinations are called components or latent variables.\n- PLS is a special version of NIPLAS, it tries to find latent relationship between iteratively dependent variable and variables with high correlation.\n\n\n## PCR vs. PLS\n\n\n- In PCR, Linear combinations, ie components, form the maximum sum of variability far from the independent variable.\n- This causes lack of ability to declare the dependent variable.\n- In PLS, components form the covariance with the dependent variable in a maximum way.\n- If variables are not desired to be discarded and if explanatory is sought -> PLS\n- PLS can be viewed as the supervised size reduction procedure, PCR unattended size reduction procedure.\n- The parameter that can be set in two methods is the number of components.\n- The CV method is used to determine the optimum number of components. ","473854fb":"**Let's see our success rate by drawing the ROC curve.**","44c2b566":"<a id=\"56\"><\/a> <br>\n# ElasticNet Regression\n\nThe aim is to find the coefficients that minimize the sum of error squares by applying a penalty to these coefficients. ElasticNet combines L1 and L2 approaches.","4cd211e1":"# Predict","33514c68":"# Determining the Optimum Set Number","953cce28":"# Principal Component Analysis (PCA)\n\nThe basic idea is to represent the main properties of multivariate data with fewer variables \/ components.\n\nIn other words: to reduce the size of the variable by risking a small amount of data loss.\n","72a703d0":"<a id=\"76\"><\/a> <br>\n# Random Forests Classification\n\nIts basis is based on the aggregation of estimates produced by multiple decision trees.","07c84f5d":"<a id=\"78\"><\/a> <br>\n# eXtreme Gradient Boosting ( XGBoost )","b22ac9ba":"# Model Tuning For ANN Model","49da180f":"**Re-modeling with new parameters.**","cc5967d0":"<h1><center>Content<\/center><\/h1>\n<p>&#160;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p>\n \n\n\n[<center>What is the Machine Learning?<\/center>](#1)\n\n[<center>Linear Regression and Others<\/center>](#2)\n\n[<center>Nonlinear Regression Models<\/center>](#3)\n\n[<center>Classification Models<\/center>](#4)\n\n[<center>Unsupervised Learning and Others<\/center>](#5)\n\n[<center>Real Life<\/center>](#6)\n\n\n\n","132135c1":"# PCA","5e7153f5":"**Now we need to reduce the dataset.**","c7aaecf7":"# Model Tuning For GBM Model","edfd6b9a":"# Hierarchical Clustering\n\nThe aim is to divide the observations into subsets according to their similarity to each other.\n\n\n### Combiner Clustering\n\n- There are two closest observations in the data set.\n- A new observation is created by bringing these two points together, so now the data set consists of the observations in the first junction.\n- The same process is repeated until it goes upwards. In other words, these new clusters made up of the combination of two clusters are combined again, since they are similar to each other in the same way. This process is repeated until all observations are collected in a single cluster.\n\nPoints close to each other are determined using distance measurements.\nEuclidean distance, manhattan distance, correlation etc.\n\n\n### Partitioner Clustering\n\nThere is 1 cluster at the beginning, room is the whole data set.\n\n- The cluster in which all observations are together is divided into two subsets.\n- The new clusters that are formed are divided into subsets that are dissimilar.\n- The same process is continued until the number of observations is reached.","3788dbc9":"<a id=\"74\"><\/a> <br>\n# Artifical Neural Networks\n\nIt is one of the powerful machine learning algorithms that can be used for classification and regression problems that refer to the way the human brain processes information.","6497c5af":"# 3D Visualization","5b94bbe5":"<a id=\"2\"><\/a> <br>\n# Linear Regression and Others\n\n[1. Simple Linear Regression](#50)\n\n[2. Multiple Linear Regression](#51)\n\n[3. Partial Least Squares Regression](#53)\n\n[4. Ridge Regression](#54)\n\n[5. Lasso Regression](#55)\n\n[6. Elastic Net Regression](#56)\n\n7. For a all model:\n    - Model\n    - Predict\n    - Model Tuning\n    \n    \n    \n**Let's start with the first algorithm.**\n","5bed257d":"**We are re-modeling with the information we have.**","4dd349b3":"**As can be seen, there is a 70% connection, so regression can be established for these.**\n\n# Modeling with Statsmodel","ba65e23b":"# KNN Parameter optimization","6b5e6c60":"**Looking at the model, we can easily say that the best value to choose is 4,so let's build a model again according to the 4 components.**","a168ea59":"# Model Tuning for SVM Model","1afddf29":"So if we spend 30 units on TV and 10 units on newspaper, 40 units on TV, our sales value will be 6.28.\n\n**But what is Prediction success?**","60ef9d2e":"<a id=\"64\"><\/a> <br>\n# Random Forests\n\nIts basis is based on the gathering and evaluation of forecasts produced by multiple decision trees.\n\n- It is a combination of Bagging and Random Subspace methods.\n- Observations for trees are selected using the random sample selection method of bootstrap, and random subspace method.\n- In each node of the decision tree, the best branching (knowledge gain) variable is selected among the randomly fewer variables selected from all variables.\n- 2\/3 of the data is used in tree creation. The data left outside is used to evaluate the performance of the trees and determine the variable importance.\n- Random variables are selected from each node.\n- While estimating values \u200b\u200bare requested from the trees for the final estimation, the trees are weighted by considering the error rates calculated for each tree beforehand.","fd467afd":"# Predict","aca096a6":"### We Transform Our Train and Test Sets.","976d5c92":"We need to convert the categorical variables we have into dummy format.","c10ab0dc":"**Re-modeling with new parameters.**","4a906352":"**PCA Normally the number of components takes the argument, but we are not entering it right now, so there are as many components as there are variables.**","2a41ee82":"The highest and lowest places of the lines in the table indicate residues.","e36f14c5":"**Re-modeling with new parameters**","1493d6b1":"[<h1><center>Machine Learning Tutorial For Beginners<\/center><\/h1>](https:\/\/www.instagram.com\/omercannsvgn)\n\n\n<center><img src=\"https:\/\/www.iberdrola.com\/wcorp\/gc\/prod\/en_US\/comunicacion\/machine_learning_key_res\/machine_learning_792x214.jpg\"><\/center>","63365a17":"# Elastic Net Model Tuning","5e898abf":"<a id=\"54\"><\/a> <br>\n# Ridge Regression\n\nRidge regression is one of the deviation prediction methods. When there is multiple linear dependency, it can be preferred because the least squares estimators give smaller variance estimates than their variances.\n\n\n- Resistant to Over Learning.\n- It is biased but its variance is low. (Sometimes we prefer biased models more.)\n- It offers a solution against the multi-dimensional curse.\n- It is effective when there is multiple linear connection problem.\n- Builds a model with all variables. It does not exclude the unrelated variables from the model, approximates its coefficients to zero.","6943dda8":"<a id=\"73\"><\/a> <br>\n# Support Vector Machines (SVM)\n\nThe aim is to find the hyper-plane that will ensure the optimum separation between the two classes.\n","32b21d97":"# Why Statsmodel?\nIt has a nice output for statistical comments,but other than that, it is not used much, we usually use scikit-learn.\n\n\n# Modeling with Scikit-Learn","abc61724":"<a id=\"67\"><\/a> <br>\n# eXtreme Gradient Boosting (XGBoost)\n\nXGBoost has been optimized to increase GBM's speed and forecast performance; It is scalable and can be integrated into different platforms.","b32c2bd2":"<a id=\"3\"><\/a> <br>\n# Nonlinear Regression Models\n\n- [K-Nearest Neighborhood(KNN)](#60)\n\n- [Support Vector Regression](#61)\n\n- [Artifical Neural Network(ANN)](#62)\n\n- [Classification and Regression Tress(CART)](#63)\n\n- Bagging (Bootstrap Aggregation)\n\n- [Random Forest(RF)](#64)\n\n- [Gradient Boosting Machines (GBM)](#66)\n\n- [Extreme Gradient Boosting (XGBoost)](#67)\n\n- LightGBM\n\n- CatBoost","5f7122c0":"# RBF SVC Model\n\n### Model & Predict & Tuning","0ddbeec3":"# Logistic Regression with Scikit-Learn","b1f73574":"# Model Tuning For ANN Model\nparameters to be corrected here, ALPHA, HIDEN LAYER, ACTIVAT\u0130ON","f9589145":"# Model Tuning for XGB Model","89d129d9":"# Predict","e6143a1a":"**Now let's do an operation by separating our model as train and test.**","f7c29658":"**The optimum number of components seems to be 2, I already used the value 2**","76b601fd":"# Multiple Linear Regression Model Tuning \/ Model Verification","8e1b3354":"**Yes, as seen below, this algorithm creates such a system on the python side.**\n\n**The following output help can help us when integrating this model into a system**","d582510f":"# Nonlinear Regression Models Libraries","da44aa51":"We need to separate the data set as Train and Test.","01e6c71a":"<a id=\"1\"><\/a> <br>\n# What is the Machine Learning?\nIt is the scientific field of study to develop various algorithms and techniques in order to enable computers to learn in a similar way to people.\n\n## Concepts\n**Variable Types:**\n1. The dependent variable\n2. Independent variable\n\n\n**Learning Types:**\n1. Supervised learning\n2. Unsupervised learning\n3. Semi-supervised learning\n\n\n**Problem Types:**\n1. Regression\n2. Classification\n\n\n**Model Seperation:**\n1. Train - Test seperation\n\n\n**Model Selection:**\n\n**There are two different situations when choosing a model.**\n1. Choosing the best model among the models created with combinations of variables that can occur.\n2. Model selection among the different models installed.\n# What model to choose?\n\n- Revelation rate and a similar value for RMSE for regression.\n- The correct classification rate for classification is similar.\n\n# Overfitting\n\n- Data over-learning of the model.\n\n# Model Verification Methods\n\n1. **Holdout Method** -> The original data set is divided into two as a training and test set, the performance of the model set up with the training set is measured with the test set.\n\n2. **K Fold Cross Validation** -> The original data set is divided into two as a training and test set, but the training set is divided into K sub-parts, one of the specified sub-sets is excluded and the model is created with the remaining parts and tested with the remaining part.\n\n3. **Leave One Out** -> In K-like or similar here, the number of clusters is equal to the number of observations.\n\n4. **Bootstrap** -> Samples are taken from the original dataset with the replacement sampling method.\n\n\n# Model Success Evaluation Methods\n\n\n### Regression:\n- MSE (Mean Squares Error)\n- RMSE (Square root of MSE)\n- MAE (Absolute Error Average)\n\n### Classification:\n- Confusion Matrix\n- ROC Curve\n\n# Bias - Variance Tradeoff\n1. Underfitting(Model not learned): It has high bias.\n2. True Model: Low bias,low variance.\n3. Overfitting (Over-learned model): It has high variance.\n\n# Parameter,Hyperparameter,Parameter Tuning,Model Tuning\n\n1. Parameter: Measurable size, numerical value, indicator, measure, size. (Internal coefficients that the model will produce in itself).\n2. Hyperpameter: The external parameters of the machine learning algorithms used. (Used to optimize the model).\n3. Parameter Tuning: 1. To adjust the parameters of the model itself. 2. To adjust the Model Hyperparameters.\n4. Model Tuning: Covers all.","c2ec8662":"**Re-modeling for new parameter.**","9b1d088e":"**Let's take a look at the variance described immediately.**","ff23f83e":"# Predict","85cb527d":"<a id=\"75\"><\/a> <br>\n# Classification and Regression Trees ( CART )\n\nThe aim is to transform the complex structures in the dataset into simple decision structures. The heterogeneous data sets are divided into homogeneous subgroups according to a specified target variable.","9f8814e1":"<a id=\"77\"><\/a> <br>\n# Gradient Boosting Machines\n\nIt is based on the idea of \u200b\u200bbringing together weak learners and creating a strong learners.","24b44344":"# Model Tuning For KNN","bd5d1e44":"# Predict","942e656d":"<a id=\"60\"><\/a> <br>\n# K-Nearest Neighborhood(KNN)\n\nEstimates are made based on observation similarity.\n\n\n## KNN Steps:\n\n- Determine the number of neighbors (K).\n- Calculate the distances between an unknown point and all other points.\n- Sort the distances and choose the closest k-observation based on the specified k number.\n- Classification is the most common class, and regression is the average value estimate.","adf11ff3":"# Predict","1135a727":"<a id=\"50\"><\/a> <br>\n# Simple Linear Regression\n![](data:image\/png;base64,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)","55c5935f":"## K-Means Segmentation Problems","4318c959":"**Now we will decide how many components it is with the K-Fold technique.**","6c9880f4":"# Model Tuning For Random Forests Model","0e1ea04d":"**There is a nice connection between sales and TV, let's look at it privately.**","43a1ffbf":"# Predict","c1421292":"# Residues and Their Importance in Machine Learning\n\nResidues are the difference between real and predicted values,and that determines our mistake.","95323364":"**Same thing I did for the statsmodel in the linear regression section.**","94f5b2b6":"## Predict\n\nModel Equation:\n- Sales = 7.03 + TV * 0.04\n\nExamples: Sales values estimation for 30 units of television expenditure","a1e45bc0":"# Model Tuning For SVR Model\n\nThe parameter to be corrected is the C complexity parameter.","caa25434":"<a id=\"71\"><\/a> <br>\n# Naive Bayes\n\nIt is a probabilistic modeling technique. The aim is to calculate the probability of a particular sample belonging to each class based on conditional probability.\n\n## Guassian Naive Bayes","b87e5ae5":"<a id=\"63\"><\/a> <br>\n# Classification and Regression Tree (CART)\nThe aim is to transform the complex structures in the dataset into simple decision structures. The heterogeneous data sets are divided into homogeneous subgroups according to a specified target variable.","60529fad":"#### Multilayer sensor","769005cc":"**We can see the difference between the algorithms in this table**","3929f6a9":"# Model Tuning for Random Forests Model","105dda2c":"# Model Tuning for GBM Model","238ef151":"**Let's calculate our mistake for the train set.**","162a579e":"**Re-modeling with new parameter.**","f78fad2b":"**We are building a new model according to this result we obtained.**","db23fdce":"# Model Tuning","cb8033ff":"# Predict","726c7f94":"**the same as this is in the model validation process PCR.**","dd0d7dae":"First of all, we should examine the basic information and problems about our dataset.","5adb1a55":"# Predict","99979e10":"# Lasso Regression Model Tuning","704c934e":"<a id=\"51\"><\/a> <br>\n# Multiple Linear Regression\nThe main purpose is to find the linear function that expresses the relationship between dependent and independent variables.\n\n\n\n![](https:\/\/www.researchgate.net\/profile\/Gabriel_Jacome2\/post\/How_to_interpret_a_Multiple_Linear_Regression_plot\/attachment\/59d624556cda7b8083a1f977\/AS%3A380311306293248%401467684689374\/image\/MLR+PLOT.png)\n\n\n\n\n### Assumptions of Linear Regression\n\n\n- Errors are normally distributed\n- Errors are independent of each other and there is no autocorrelation between them.\n- Variances of error terms are fixed for each observation.\n- There is no relationship between the variables and the error term.\n- There is no multiple linear relationship problem between independent variables.\n\n\n### Advantages and Disadvantages of Regression Models\n\n**Benefits:**\n\n- If it is understood well, other ML and DL issues are very easily comprehended.\n- Linearity enables causality interpretations, which allows actionable and strategic modeling.\n- The effect levels and significance of variables can be evaluated.\n- The success of disclosure of variability in the dependent variable can be measured.\n\n**Drawbacks:**\n\n- There are assumptions.\n- It is sensitive to contradictory observations.","3370c863":"<a id=\"53\"><\/a> <br>\n# Principal Component Regression (PCR)\nIn statistics, principal component regression is a regression analysis technique based on principal component analysis.\n\n![](data:image\/png;base64,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)\n\n\n**We will use a different data set in this data set there is a problem of determining the salary of a basketball player**\n\n\n**Again we will create a guess.**\n\n\n\n**Note:** *I will not do data set cleaning, I showed it at first, I will apply directly*\n\n\n\n\n\n\n\n\n","5dd004f9":"<a id=\"55\"><\/a> <br>\n# Lasso Regression\n\nThe aim is to find the coefficients that minimize the sum of error squares by applying a penalty to these coefficients.\n\n- Ridge regression has been proposed to overcome the disadvantage of leaving all relevant-unrelated variables in the model.\n- Converts coefficients to zero in Lasso.\n- But when the L1 norm Lambda is big enough, it makes some coefficients zero.\n- It is very important to choose Lambda correctly, CV is used here too.\n- Ridge and Lasso methods are not superior to each other.\n\n\n## Determination of Lambda Parameter\n\n- A set containing certain values \u200b\u200bfor Lambda is selected and cross validation test error is calculated for each.\n- Lambda, which gives the smallest cross validation, is selected as the setting parameter.\n- With this lastly selected lambda, the model is again fit for all observations.","dbf5176c":"**Now let's re-model with our reduced train set.**","c5ba1e3c":"<a id=\"70\"><\/a> <br>\n# Logistic Regression\n\nThe aim is to establish a linear model that defines the relationship between dependent independent variables for the classification problem.\n![](https:\/\/miro.medium.com\/max\/2508\/1*aDeaim7khcLZrhPcDlYUPg.png)\n\n\n- Dependent variable is categorical.\n- It takes its name from the logit transform applied to the dependent variable.\n- Since the assumptions sought in Linear Regression are not sought here, they have more flexible usability.\n- The probability of realization of the value of the dependent variable defined as 1 is calculated.\n- Thanks to the logistic function, the values produced are between 0 and 1.\n\n\n**Note: dataset we are interested in is a dataset related to classification of diabetes.**\n\n*Dics. for classification*","061425f1":"<a id=\"66\"><\/a> <br>\n# Gradient Boosting Machines\n\nIt is a generalized version of AdaBoost that can be easily adapted to classification and regression problems. A series of models, which are in the form of a single predictive model, is now built on it.\n\n- It is based on the idea of bringing together weak learners and creating a strong learners.\n- (AdaBoost) It is an algorithm that brings the idea of \u200b\u200bweak classifiers to come together and form a strong classifier.\n- Gradient Boosting creates a series of models in the form of a single predictive model.\n- A model in the series is created by building (feet) of the prediction residues \/ residuals of the previous model in the series.\n- GBM uses the Gradient descent algorithm that can optimize any differential function that can be differentiated.\n- GB can use many basic types of learners.\n- Cost functions and link functions can be modified.\n- Boosting + Gradient Descent.\n","c8d2698e":"# Predict\n\nModel Equation:\n\nSales = 2.70 + TV0.04 + RADIO0.19 + NEWSPAPER0.006\n\nFor example, what would be the estimated value of sales when 30 units of TV spend, 10 units of radio spend, and 40 units of newspaper spend?","d3b0d29b":"## Thank you for reading, don't forget to give upvote if you find it really useful, I wish you a pleasant day\n<center><img src=\"data:image\/png;base64,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\"><\/center>\n","24e6383f":"# Model Tuning For Non-linear SVR\n\nThe parameter to be corrected here is the C complexity parameter.","f2d6c584":"# Model","0b85c6c1":"# Ridge Regression Model Tuning","394ca39d":"<a id=\"61\"><\/a> <br>\n# Support Vector Regression (SVR)\n\nThe aim is to determine the line or curve so that you can get the maximum point to a margin range with the smallest error.\n\n## For simple svr","9ded5b79":"# Optimum number of clusters","4b35a8b3":"**Let's do the above checks for the test set.**","07b372d0":"**Choosing a cluster is based entirely on business information, but if you look here, it may make sense to put 4 segments according to the table.**","54478b20":"**Based on this information, we re-model.**","8ace3ba2":"# Prediction and Verified","802edbcf":"# Predict","98844666":"<a id=\"72\"><\/a> <br>\n# KNN Algorithm\n\nEstimates are made based on Observation similarity.","a82518a7":"# Classification with KNN","e55cb0e4":"<a id=\"5\"><\/a> <br>\n# Unsupervised Learning\n\nUnattended learning, unlike supervised learning, is to learn the relationships and structures existing in the data without labeling the data as cause-effect or input-output.\n\n### Steps:\n\n1. Cluster number is determined.\n2. Random k center is selected.\n3. For each observation, distances to k centers are calculated and observations are assigned to the nearest k center.\n4. Each observation is assigned to its closest center, the cluster.\n5. Central calculations are made again for clusters that occur after assignments.\n6. This process is repeated for a specified number of iterations, and the cluster structure of the observations where the total sum of within-cluster variations (minimum within cluster variation) is minimum is selected as the final cluster.\n","9662e7c3":"# Model Tuning For KNN Model","0bf236c0":"# Model Tuning","3aa3d8d0":"# Load Data Set","68c01772":"**How to add clusters?**","9ec749b8":"**As always, let's model a statsmodel to see statistical terms.**\n\n# Logistic Regression with Statsmodel","7bac10a8":"# Modeling With Statsmodel\nIf you want to comment on the data, model it with statsmodel.","78aad921":"# NCA","50486877":"# Visualization"}}