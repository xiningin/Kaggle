{"cell_type":{"a6327d4c":"code","6394b7b7":"code","4a44108e":"code","ba016ef4":"code","fdb3bb70":"code","7aab8922":"code","00279d50":"code","3db63d00":"code","7e3a3015":"code","0a64eb3a":"code","473df6fa":"code","d22e63f1":"code","c69d39b0":"code","4a299a33":"code","37023edf":"code","74d55637":"code","c36bb06e":"code","5b6e142b":"code","b23a4104":"code","8afcd89f":"code","b90adc39":"code","43ceead5":"code","1bd7ea8c":"code","7a141923":"code","f1410eb6":"code","5f30ed97":"code","f56add32":"code","b7dbc83c":"code","c36f6497":"code","75f8ffa6":"code","a2dc0301":"code","fdc35e64":"code","53c39311":"code","6af48ab2":"code","c18b0dd1":"code","d023987a":"code","a25aea0b":"markdown","3c6adfcc":"markdown","1cdf54e6":"markdown","080e5111":"markdown","be3bd503":"markdown","aff59feb":"markdown","be6661f4":"markdown","425ef270":"markdown","f53593be":"markdown","07fa9662":"markdown","9d0b11ae":"markdown","db855e0f":"markdown","726b50cd":"markdown","07d951ea":"markdown","a835621e":"markdown","908602d9":"markdown","ce7fe73c":"markdown","d63f767a":"markdown","569b2674":"markdown","55ad15ce":"markdown","d2068e7a":"markdown","d60b17a8":"markdown","e0ce0ef0":"markdown","0c68c0b1":"markdown","73ba448a":"markdown","ec77ebeb":"markdown","71893e55":"markdown","89d23384":"markdown","e0cd3c57":"markdown"},"source":{"a6327d4c":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn import preprocessing\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn import metrics","6394b7b7":"df=pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","4a44108e":"df.T","ba016ef4":"columns=df.columns\ncolumns_new=[]\nfor i in columns:\n    columns_new.append(any(df[i].isnull()|df[i].isnull()))\ndf=df.drop(columns[columns_new],axis=1)","fdb3bb70":"df.shape","7aab8922":"sns.countplot(x=\"target\", data=df)\nplt.show()","00279d50":"pd.crosstab(df.sex,df.target).plot(kind=\"bar\",figsize=(15,6),color=['blue','orange' ],alpha=0.7)\nplt.title('Heart Disease Frequency for Sex')\nplt.xlabel('Sex (0 = Female, 1 = Male)')\nplt.xticks(rotation=0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency')\nplt.show()","3db63d00":"pd.crosstab(df.age,df.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.savefig('heartDiseaseAndAges.png')\nplt.show()","7e3a3015":"ax = sns.boxplot( palette=\"Set2\", orient=\"h\",data=df[df.target==1])","0a64eb3a":"ax = sns.boxplot( palette=\"Set2\", orient=\"h\",data=df[df.target==0])","473df6fa":"from keras.utils import to_categorical\n# one hot encode\nencoded1 = pd.DataFrame(to_categorical(df.restecg),columns=['restecg0','restecg1','restecg2'])\nencoded2 = pd.DataFrame(to_categorical(df.cp),columns=['cp0','cp1','cp2','cp3'])\nencoded3 = pd.DataFrame(to_categorical(df.thal),columns=['thal0','thal1','thal2','thal3'])\nencoded4 = pd.DataFrame(to_categorical(df.ca),columns=['ca0','ca1','ca2','ca3','ca4'])\nencoded5 = pd.DataFrame(to_categorical(df.slope),columns=['slope0','slope1','slope2'])\n\ndf = pd.concat([df,encoded1,encoded2,encoded3,encoded4,encoded5],axis=1).drop(['restecg','cp','thal','ca','slope'],axis=1)","d22e63f1":"df.T","c69d39b0":"X_train, X_test, y_train, y_test=train_test_split(\n    df.drop(['target'], axis=1),\n    df[['target']],\n    test_size=0.3,\n    random_state=41)","4a299a33":"print(X_train.shape)\nprint(X_test.shape)","37023edf":"for column in X_train.columns:\n    \n    df_train1 = X_train[(y_train.target==0) & (X_train[column]<np.mean(X_train.loc[y_train.target==0,column])+3*np.std(X_train.loc[y_train.target==0,column]))]\n    df_test1 = X_test[(y_test.target==0) & (X_test[column]<np.mean(X_train.loc[y_train.target==0,column])+3*np.std(X_train.loc[y_train.target==0,column]))]\n    \n    label_train1 = y_train[(y_train.target==0) & (X_train[column]<np.mean(X_train.loc[y_train.target==0,column])+3*np.std(X_train.loc[y_train.target==0,column]))]\n    label_test1 = y_test[(y_test.target==0) & (X_test[column]<np.mean(X_train.loc[y_train.target==0,column])+3*np.std(X_train.loc[y_train.target==0,column]))]\n    \n    df_train2 = X_train[(y_train.target==1) & (X_train[column]<np.mean(X_train.loc[y_train.target==1,column])+3*np.std(X_train.loc[y_train.target==1,column]))]\n    df_test2 = X_test[(y_test.target==1) & (X_test[column]<np.mean(X_train.loc[y_train.target==1,column])+3*np.std(X_train.loc[y_train.target==1,column]))]\n    \n    label_train2 = y_train[(y_train.target==1) & (X_train[column]<np.mean(X_train.loc[y_train.target==1,column])+3*np.std(X_train.loc[y_train.target==1,column]))]\n    label_test2 = y_test[(y_test.target==1) & (X_test[column]<np.mean(X_train.loc[y_train.target==1,column])+3*np.std(X_train.loc[y_train.target==1,column]))]","74d55637":"X_train=pd.concat([df_train1,df_train2])\ny_train=pd.concat([label_train1,label_train2])\n\nX_test=pd.concat([df_test1,df_test2])\ny_test=pd.concat([label_test1,label_test2])","c36bb06e":"print(X_train.shape)\nprint(X_test.shape)","5b6e142b":"corrMatrix = X_train.corr()\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(corrMatrix, annot=True,ax=ax)\nplt.show()","b23a4104":"correlated_features = set()\nfor i in range(len(corrMatrix .columns)):\n    for j in range(i):\n        if abs(corrMatrix.iloc[i, j]) > 0.85:\n            colname = corrMatrix.columns[i]\n            correlated_features.add(colname)\nprint(correlated_features)","8afcd89f":"X_train.drop(labels=correlated_features, axis=1, inplace=True)\nX_test.drop(labels=correlated_features, axis=1, inplace=True)","b90adc39":"print(X_train.shape)\nprint(X_test.shape)","43ceead5":"constant_filter = VarianceThreshold(threshold=0.0)\nconstant_filter.fit(X_train)\nX_train = constant_filter.transform(X_train)\nX_test = constant_filter.transform(X_test)\n\nX_train.shape, X_test.shape","1bd7ea8c":"mm_scaler = preprocessing.StandardScaler()\nX_train = pd.DataFrame(mm_scaler.fit_transform(X_train))\nX_test=pd.DataFrame(mm_scaler.transform(X_test))","7a141923":"# Random Forest Classification\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(class_weight=\"balanced\",n_estimators=200,random_state = 1)\nrf.fit(X_train, y_train.values.ravel())\ny_pred=rf.predict(X_test)\nacc = metrics.accuracy_score(y_pred,y_test.values.ravel())*100\nprint(\"Random Forest Algorithm Accuracy Score : {:.2f}%\".format(acc))","f1410eb6":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, y_train.values.ravel())\n\ny_pred=nb.predict(X_test)\nacc = metrics.accuracy_score(y_pred,y_test.values.ravel())*100\n\nprint(\"Accuracy of Naive Bayes: {:.2f}%\".format(acc))","5f30ed97":"from sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(X_train, y_train.values.ravel())\n\ny_pred=svm.predict(X_test)\nacc = metrics.accuracy_score(y_pred,y_test.values.ravel())*100\n\nprint(\"Test Accuracy of SVM Algorithm: {:.2f}%\".format(acc))","f56add32":"# KNN Model\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# try ro find best k value\nscore = []\n\nfor i in range(1,20):\n    knn = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n    knn.fit(X_train, y_train.values.ravel())\n    score.append(knn.score(X_test, y_test.values.ravel()))\n    \nplt.plot(range(1,20), score)\nplt.xticks(np.arange(1,20,1))\nplt.xlabel(\"K neighbors\")\nplt.ylabel(\"Score\")\nplt.show()\n\nacc = max(score)*100\nprint(\"Maximum KNN Score is {:.2f}%\".format(acc))","b7dbc83c":"knn = KNeighborsClassifier(n_neighbors = 7)  # n_neighbors means k\nknn.fit(X_train, y_train.values.ravel())    ","c36f6497":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(max_iter=50)\nlogreg.fit(X_train, y_train.values.ravel())\ny_pred=logreg.predict(X_test)\nacc = metrics.accuracy_score(y_pred,y_test.values.ravel())*100\nprint(\"Test Accuracy of Logistic Regression Algorithm: {:.2f}%\".format(acc))","75f8ffa6":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\n\n# define the keras model\nmodel = Sequential()\nmodel.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the keras model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit the keras model on the dataset\nmodel.fit(X_train, y_train, epochs=100, batch_size=32)\n# evaluate the keras model\n_, accuracy = model.evaluate(X_test, y_test)","a2dc0301":"def conf_matrix(matrix,pred):\n    class_names= [0,1]# name  of classes\n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    # create heatmap\n    sns.heatmap(pd.DataFrame(matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n    ax.xaxis.set_label_position(\"top\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    plt.show()","fdc35e64":"# make class predictions with the model\ny_pred = rf.predict(X_test)\ncnf_matrix = metrics.confusion_matrix(y_pred,y_test)\nconf_matrix(cnf_matrix,y_test)\n# calculate prediction\nreport = classification_report(y_pred,y_test)\nprint(report)","53c39311":"# make class predictions with the model\ny_pred = nb.predict(X_test)\ncnf_matrix = metrics.confusion_matrix(y_pred,y_test)\nconf_matrix(cnf_matrix,y_test)\n# calculate prediction\nreport = classification_report(y_pred,y_test)\nprint(report)","6af48ab2":"# make class predictions with the model\ny_pred = svm.predict(X_test)\ncnf_matrix = metrics.confusion_matrix(y_pred,y_test)\nconf_matrix(cnf_matrix,y_test)\n# calculate prediction\nreport = classification_report(y_pred,y_test)\nprint(report)","c18b0dd1":"# make class predictions with the model\ny_pred = knn.predict(X_test)\ncnf_matrix = metrics.confusion_matrix(y_pred,y_test)\nconf_matrix(cnf_matrix,y_test)\n# calculate prediction\nreport = classification_report(y_pred,y_test)\nprint(report)","d023987a":"# make class predictions with the model\ny_pred = model.predict_classes(X_test)\ncnf_matrix = metrics.confusion_matrix(y_pred,y_test)\nconf_matrix(cnf_matrix,y_test)\nreport = classification_report(y_pred,y_test)\nprint(report)","a25aea0b":"# Neural Network","3c6adfcc":"# KNN model","1cdf54e6":"# Neural Network","080e5111":"# Checking for outliers","be3bd503":"# Random forest model","aff59feb":"Seems that there are no outliers","be6661f4":"# Naive Bayes","425ef270":"# Evaluating models","f53593be":"More prevalence of the desease for woman","07fa9662":"# Support vector machine model","9d0b11ae":"# Removing outliers","db855e0f":"# Removing features with 0 variance","726b50cd":"I see some outliers but not a lot of them, we keep them and try the model again without outliers maybe","07d951ea":"# Random forest","a835621e":"# K Neighbors","908602d9":"Best model is K neighbors with 88% accuracy precision and recall","ce7fe73c":"# Suport Vector Machine","d63f767a":"# Data exploration","569b2674":"the data is almost balanced","55ad15ce":"one 0 variance features","d2068e7a":"# Logistic Regression","d60b17a8":"There is one highly correlated features in the data","e0ce0ef0":"# Naive Bayes ","0c68c0b1":"# Scaling the data","73ba448a":"# Train test splinting","ec77ebeb":"No columns with missing values ","71893e55":"# Results","89d23384":"# Looking for correlated features","e0cd3c57":"# Checking for missing values"}}