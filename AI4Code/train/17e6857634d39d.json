{"cell_type":{"d92e8e40":"code","e103a440":"code","ce2f5c40":"code","9d486656":"code","48d3e770":"code","cb6057fe":"code","bd0cd488":"code","61e2c3f4":"code","44a4715f":"code","4b07d41f":"code","5ddbf5fa":"code","a7fcb576":"code","5388444c":"code","e6a0bdfc":"code","1edb6b08":"code","87a9a1f3":"markdown"},"source":{"d92e8e40":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pandas_profiling import ProfileReport\nfrom lightgbm import LGBMRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nimport datetime\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport gc\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e103a440":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ce2f5c40":"train = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')\nsub = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv')","9d486656":"columns = test.columns[1:]\ncolumns","48d3e770":"target = train['target'].values\n","cb6057fe":"cat_features = columns[:10]\ncat_features","bd0cd488":"for feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train[feature])\n    train[feature] = le.transform(train[feature])\n    test[feature] = le.transform(test[feature])","61e2c3f4":"train.head()\n","44a4715f":"test.head()\n","4b07d41f":"train_oof = np.zeros((300000,))\ntest_preds = 0\ntrain_oof.shape","5ddbf5fa":"params = {'reg_alpha': 6.147694913504962,\n 'reg_lambda': 0.002457826062076097,\n 'colsample_bytree': 0.3,\n 'subsample': 0.8,\n 'learning_rate': 0.0005,\n 'max_depth': 20,\n 'num_leaves': 111,\n 'min_child_samples': 285,\n 'random_state': 48,\n 'n_estimators': 320000,\n 'metric': 'rmse',\n 'cat_smooth': 39}","a7fcb576":"NUM_FOLDS = 10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, target))):\n        #print(f'Fold {f}')\n        train_df, val_df = train.iloc[train_ind][columns], train.iloc[val_ind][columns]\n        train_target, val_target = target[train_ind], target[val_ind]\n        \n        model = LGBMRegressor(**params)\n        model.fit(train_df, train_target, eval_set=[(val_df,val_target)],early_stopping_rounds=1600,verbose=False)\n        temp_oof = model.predict(val_df)\n        temp_test = model.predict(test[columns])\n\n        train_oof[val_ind] = temp_oof\n        test_preds += temp_test\/NUM_FOLDS\n        \n        print(mean_squared_error(temp_oof, val_target, squared=False))","5388444c":"mean_squared_error(train_oof, target, squared=False)\n","e6a0bdfc":"np.save('train_oof', train_oof)\nnp.save('test_preds', test_preds)","1edb6b08":"sub['target'] = test_preds\nsub.to_csv('submission.csv', index=False)","87a9a1f3":"A ten-fold version of the LightGBM Rrgressors with hyperparameters obtained in the following kernel: https:\/\/www.kaggle.com\/hamzaghanmi\/lgbm-hyperparameter-tuning-using-optuna "}}