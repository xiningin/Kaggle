{"cell_type":{"a06b7896":"code","ef626b95":"code","2834da0b":"code","8cde331e":"code","743d0d51":"code","d7d255ae":"code","d88fe5bb":"code","7c0fd132":"code","c86b7208":"code","d6b0d20e":"code","5f792945":"code","44ca1e71":"code","266b1dca":"code","76bc35a2":"code","bbbbc858":"code","5e1a465c":"code","95fe369b":"code","31c1bde9":"code","efb637d3":"code","bffe8d2a":"code","c1c86536":"code","5e738625":"code","64000160":"code","2a50b446":"code","d4c363a6":"code","08de441c":"code","19b4a9ab":"code","a00f809d":"code","91942c07":"code","a8e89e37":"code","ef1da225":"code","616c585d":"code","92758695":"code","ec8071a8":"code","f25b5182":"code","716ad731":"code","94a13ed1":"code","bfeb6c8d":"code","c379fbc6":"code","a3f0a3c7":"code","7ece85e4":"code","7c82ae7e":"code","f2903f06":"code","4e3f0867":"code","fae72015":"code","7a20351d":"code","4ad5b353":"code","91de2286":"code","909034c3":"code","9da3bdaf":"code","4909fa13":"code","112e4005":"code","fcb9cd77":"code","0cb89047":"code","f6da20ff":"code","4a61df53":"markdown","13176bc4":"markdown"},"source":{"a06b7896":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef626b95":"##import all necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport sklearn\nimport re\nimport string\nimport pandas as pd\nimport seaborn as sns","2834da0b":"#training data\ntrain_data=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\n","8cde331e":"train_data.head()","743d0d51":"#drop columns\ntrain_data.drop(['id','keyword','location'],axis=1,inplace=True)","d7d255ae":"##null values\ntrain_data.isnull().sum()","d88fe5bb":"train_data['target'].value_counts().plot.pie()","7c0fd132":"sns.countplot(train_data['target'])\nplt.show()","c86b7208":"##divide into x and y\nX=train_data.drop('target',axis=1)","d6b0d20e":"y=train_data['target']","5f792945":"##remove punctuations,stop words\nfrom nltk.stem import WordNetLemmatizer\n#ps = PorterStemmer()\nwn=WordNetLemmatizer()\ncorpus=[]\nfor i in range(0,len(X)):\n    review=re.sub('[^a-zA-Z]',' ',X['text'][i])\n    review=review.lower()\n    review=review.split()\n    review=[wn.lemmatize(word) for word in review if word not in stopwords.words('english')]\n    review= ' '.join(review)\n    corpus.append(review)","44ca1e71":"len(corpus),X.shape,len(y)","266b1dca":"corpus[2]","76bc35a2":"##train_test_split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(corpus,y,test_size=0.2,random_state=42)","bbbbc858":"len(X_train),len(y_train)","5e1a465c":"##using tokenizer of bert\nfrom transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')","95fe369b":"##tokenize function\ndef tokenize_text(data):\n    encoded=tokenizer(data,padding=True,truncation=True,return_tensors='np')\n    return encoded.data\n        \n        ","31c1bde9":"##tokenize train_data\ntrain_data=tokenize_text(X_train)","efb637d3":"\ntrain_data['input_ids'].shape,y_train.shape","bffe8d2a":"#tokenize test_data\ntest_data=tokenize_text(X_test)","c1c86536":"test_data['input_ids'].shape,y_test.shape","5e738625":"##import pretrained model\nfrom transformers import TFDistilBertForSequenceClassification\n\nmodel = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')","64000160":"##training \nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nimport tensorflow as tf\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\nmodel.compile(optimizer=optimizer, loss=model.compute_loss,metrics=['accuracy'])\nmodel.fit(\n    train_data,\n    np.array(y_train), \n    validation_data=(\n        test_data,\n        np.array(y_test),\n    ),\n    batch_size=32,epochs=10\n)","2a50b446":"##saving model\nmodel.save_pretrained('bert_79')","d4c363a6":"preds=model.predict(test_data)","08de441c":"classes=np.argmax(preds['logits'],axis=1)","19b4a9ab":"##accuracy\nfrom sklearn import metrics\nmetrics.accuracy_score(classes,y_test)","a00f809d":"#confusion matrix\nmetrics.confusion_matrix(classes,y_test)","91942c07":"##roc auc\nmetrics.roc_auc_score(classes,y_test)","a8e89e37":"from sklearn.metrics import classification_report\nprint(classification_report(classes,y_test))","ef1da225":"##lets reduce lr during traininig\nfrom tensorflow.keras.optimizers.schedules import PolynomialDecay\nbatch_size = 32\nnum_epochs = 3\n# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n# by the total number of epochs\nnum_train_steps = (len(train_data)\/\/ batch_size) * num_epochs\nlr_scheduler = PolynomialDecay(\n    initial_learning_rate=5e-5,\n    end_learning_rate=0.,\n    decay_steps=num_train_steps\n    )\nfrom tensorflow.keras.optimizers import Adam\nopt = Adam(learning_rate=lr_scheduler)","616c585d":"import tensorflow as tf\nmodel2 = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n#loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","92758695":"##training\nmodel2.compile(\n    optimizer=opt,\n    loss=model.compute_loss,\n    metrics=['accuracy'],\n)\nmodel.fit(train_data,\n    np.array(y_train), \n    validation_data=(test_data,\n        np.array(y_test),\n    ),\n    batch_size=32,epochs=10\n)","ec8071a8":"##save\nmodel2.save_pretrained('bert_80')","f25b5182":"preds=model.predict(test_data)","716ad731":"classes=np.argmax(preds['logits'],axis=1)","94a13ed1":"##confusion matrix\nmetrics.confusion_matrix(classes,y_test)","bfeb6c8d":"metrics.roc_auc_score(classes,y_test)","c379fbc6":"from sklearn.metrics import classification_report\nprint(classification_report(classes,y_test))","a3f0a3c7":"##making predictions\npred_data=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","7ece85e4":"pred_data.head()","7c82ae7e":"pred_data.drop(['id','keyword','location'],axis=1,inplace=True)","f2903f06":"pred_data.columns","4e3f0867":"##null values\npred_data.isnull().sum()","fae72015":"##remove stop words and punctuations\nfrom nltk.stem import WordNetLemmatizer\n#ps = PorterStemmer()\nwn=WordNetLemmatizer()\ncorpus=[]\nfor i in range(0,len(pred_data)):\n    review=re.sub('[^a-zA-Z]',' ',pred_data['text'][i])\n    review=review.lower()\n    review=review.split()\n    review=[wn.lemmatize(word) for word in review if word not in stopwords.words('english')]\n    review= ' '.join(review)\n    corpus.append(review)","7a20351d":"len(corpus)","4ad5b353":"pred_data=tokenize_text(corpus)","91de2286":"pred_data['input_ids'].shape","909034c3":"##predictions\npreds=model.predict(pred_data)","9da3bdaf":"classes=np.argmax(preds['logits'],axis=1)\nclasses","4909fa13":"##submission file\nsubmisssions=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","112e4005":"submisssions.head()","fcb9cd77":"##pass our predicted values\nsubmisssions['target']=classes","0cb89047":"submisssions['target'].value_counts()","f6da20ff":"##csv file\nsubmisssions.to_csv('submission1.csv',index = False);","4a61df53":"<a href=\".\/submission1.csv\"> Download File <\/a>","13176bc4":"## let's do some EDA"}}