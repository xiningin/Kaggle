{"cell_type":{"928cd812":"code","09fedd73":"code","d30029e5":"code","ed4729f0":"code","42988091":"code","5961b3a3":"code","f5d3998f":"code","4bb34b66":"code","fca1d6aa":"code","b67b3b49":"code","0a7d10a9":"code","f3b422da":"code","e4fff908":"code","3fd2ffbc":"code","6f0edf41":"code","5afa7c1f":"code","8f2f2228":"code","206d69b8":"markdown"},"source":{"928cd812":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib as plt\nimport seaborn as sns\n# Importing AutoGluon\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgbm\n# Scikit Learn\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\n# Metrics for models evaluation\nfrom sklearn.metrics import mean_absolute_error\n\n### YOUR FEATURE ENGINEERING GOES HERE\nimport optuna\nfrom sklearn.decomposition import FactorAnalysis\nfrom sklearn.preprocessing import StandardScaler\nimport gc\nimport os\nimport sys\nfrom tqdm import tqdm\nsns.set_style('whitegrid')\nplt.style.use(\"fivethirtyeight\")\n%matplotlib inline","09fedd73":"# Reduce mem usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_feather(file).drop('index',axis=1)\n    df = reduce_mem_usage(df)\n    return df\n\ndef free_mem(objects):\n    \"\"\"free's mem\"\"\"\n    start_mem = sum([sys.getsizeof(obj) for obj in objects])\/ 1024\n    print('Memory usage before cleanup {:.2f} KB'.format(start_mem))\n    for obj in objects:\n        del obj\n    gc.collect()    \n    end_mem = sum([sys.getsizeof(obj) for obj in objects])\/ 1024\n    print('Memory usage after cleanup: {:.2f} KB'.format(end_mem))","d30029e5":"train = import_data('..\/input\/folds-just-added-in-feather-format\/train_folds_10.ftr') \ntest = import_data('..\/input\/folds-just-added-in-feather-format\/test_stratfold.ftr')","ed4729f0":"all_pressure = np.sort(train.pressure.unique())\nprint('The first 25 unique pressures...')\nPRESSURE_MIN = all_pressure[0].item()\nPRESSURE_MAX = all_pressure[-1].item()\nprint(all_pressure[:25])\n","42988091":"print('The differences between first 25 pressures...')\nPRESSURE_STEP = ( all_pressure[1] - all_pressure[0] ).item()\nall_pressure[1:26] - all_pressure[:25]","5961b3a3":"free_mem([all_pressure])","f5d3998f":"TARGET_VAR='pressure'\nFOLDS=5\nsubmission_file = '..\/input\/ventilator-pressure-prediction\/sample_submission.csv'","4bb34b66":"def add_features(df):\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n    df = df.fillna(0)\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n    \n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n    df['cross']= df['u_in']*df['u_out']\n    df['cross2']= df['time_step']*df['u_out']\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    return df\n\n#train = add_features(train)\n#test = add_features(test)\n#useful_features = [col for col in train.columns if col not in ['id', 'breath_id', 'pressure','kfold']]\n#RS = RobustScaler()\n#train[useful_features] = RS.fit_transform(train[useful_features])\n#test[useful_features] = RS.transform(test[useful_features])","fca1d6aa":"#targets = train[['pressure']].to_numpy().reshape(-1, 80)\n#display(targets.shape)\n#train.drop(['pressure', 'id', 'breath_id'], axis=1, inplace=True)\n#test = test.drop(['id', 'breath_id'], axis=1)\n\n#RS = RobustScaler()\n#train = RS.fit_transform(train[useful_features])\n#test = RS.transform(test[useful_features])\n\n#train = train.reshape(-1, 80, train.shape[-1])\n#test = test.reshape(-1, 80, train.shape[-1])","b67b3b49":"train","0a7d10a9":"#def trainlegacy(train=train,test=test):\n# rewritten calculation of lag features from this notebook: https:\/\/www.kaggle.com\/patrick0302\/add-lag-u-in-as-new-feat\n# some of ideas from this notebook: https:\/\/www.kaggle.com\/mst8823\/google-brain-lightgbm-baseline\ntrain['last_value_u_in'] = train.groupby('breath_id')['u_in'].transform('last')\n#https:\/\/machinelearningmastery.com\/time-series-forecasting-supervised-learning\/\ntrain['u_in_lag1'] = train.groupby('breath_id')['u_in'].shift(1)\ntrain['u_out_lag1'] = train.groupby('breath_id')['u_out'].shift(1)\ntrain['u_in_lag_back1'] = train.groupby('breath_id')['u_in'].shift(-1)\ntrain['u_out_lag_back1'] = train.groupby('breath_id')['u_out'].shift(-1)\ntrain['u_in_lag2'] = train.groupby('breath_id')['u_in'].shift(2)\ntrain['u_out_lag2'] = train.groupby('breath_id')['u_out'].shift(2)\ntrain['u_in_lag_back2'] = train.groupby('breath_id')['u_in'].shift(-2)\ntrain['u_out_lag_back2'] = train.groupby('breath_id')['u_out'].shift(-2)\ntrain = train.fillna(0)\n\ntrain['R__C'] = train[\"R\"].astype(str) + '__' + train[\"C\"].astype(str)\n\n# max value of u_in and u_out for each breath\ntrain['breath_id__u_in__max'] = train.groupby(['breath_id'])['u_in'].transform('max')\ntrain['breath_id__u_out__max'] = train.groupby(['breath_id'])['u_out'].transform('max')\n\n# difference between consequitive values\ntrain['u_in_diff1'] = train['u_in'] - train['u_in_lag1']\ntrain['u_out_diff1'] = train['u_out'] - train['u_out_lag1']\ntrain['u_in_diff2'] = train['u_in'] - train['u_in_lag2']\ntrain['u_out_diff2'] = train['u_out'] - train['u_out_lag2']\n# from here: https:\/\/www.kaggle.com\/yasufuminakama\/ventilator-pressure-lstm-starter\ntrain.loc[train['time_step'] == 0, 'u_in_diff'] = 0\ntrain.loc[train['time_step'] == 0, 'u_out_diff'] = 0\n\n# difference between the current value of u_in and the max value within the breath\ntrain['breath_id__u_in__diffmax'] = train.groupby(['breath_id'])['u_in'].transform('max') - train['u_in']\ntrain['breath_id__u_in__diffmean'] = train.groupby(['breath_id'])['u_in'].transform('mean') - train['u_in']\n\n# https:\/\/www.kaggle.com\/c\/ventilator-pressure-prediction\/discussion\/273974\ntrain['u_in_cumsum'] = train.groupby(['breath_id'])['u_in'].cumsum()\ntrain['time_step_cumsum'] = train.groupby(['breath_id'])['time_step'].cumsum()\n# https:\/\/www.kaggle.com\/yasufuminakama\/ventilator-pressure-lstm-starter\ntrain['breath_time'] = train['time_step'] - train.groupby('breath_id')['time_step'].shift(1)\n\n\n# OHE\ntrain = train.merge(pd.get_dummies(train['R'], prefix='R'), left_index=True, right_index=True).drop(['R'], axis=1)\ntrain = train.merge(pd.get_dummies(train['C'], prefix='C'), left_index=True, right_index=True).drop(['C'], axis=1)\ntrain = train.merge(pd.get_dummies(train['R__C'], prefix='R__C'), left_index=True, right_index=True).drop(['R__C'], axis=1)\n\n\n# all the same for the test data\ntest['last_value_u_in'] = test.groupby('breath_id')['u_in'].transform('last')\ntest['u_in_lag1'] = test.groupby('breath_id')['u_in'].shift(1)\ntest['u_out_lag1'] = test.groupby('breath_id')['u_out'].shift(1)\ntest['u_in_lag_back1'] = test.groupby('breath_id')['u_in'].shift(-1)\ntest['u_out_lag_back1'] = test.groupby('breath_id')['u_out'].shift(-1)\ntest['u_in_lag2'] = test.groupby('breath_id')['u_in'].shift(2)\ntest['u_out_lag2'] = test.groupby('breath_id')['u_out'].shift(2)\ntest['u_in_lag_back2'] = test.groupby('breath_id')['u_in'].shift(-2)\ntest['u_out_lag_back2'] = test.groupby('breath_id')['u_out'].shift(-2)\ntest = test.fillna(0)\n\ntest['R__C'] = test[\"R\"].astype(str) + '__' + test[\"C\"].astype(str)\n\ntest['breath_id__u_in__max'] = test.groupby(['breath_id'])['u_in'].transform('max')\ntest['breath_id__u_out__max'] = test.groupby(['breath_id'])['u_out'].transform('max')\n\ntest['u_in_diff1'] = test['u_in'] - test['u_in_lag1']\ntest['u_out_diff1'] = test['u_out'] - test['u_out_lag1']\ntest['u_in_diff2'] = test['u_in'] - test['u_in_lag2']\ntest['u_out_diff2'] = test['u_out'] - test['u_out_lag2']\ntest.loc[test['time_step'] == 0, 'u_in_diff'] = 0\ntest.loc[test['time_step'] == 0, 'u_out_diff'] = 0\n\ntest['breath_id__u_in__diffmax'] = test.groupby(['breath_id'])['u_in'].transform('max') - test['u_in']\ntest['breath_id__u_in__diffmean'] = test.groupby(['breath_id'])['u_in'].transform('mean') - test['u_in']\n\ntest['u_in_cumsum'] = test.groupby(['breath_id'])['u_in'].cumsum()\ntest['time_step_cumsum'] = test.groupby(['breath_id'])['time_step'].cumsum()\n\ntest['breath_time'] = test['time_step'] - test.groupby('breath_id')['time_step'].shift(1)\n\n\n# OHE\ntest = test.merge(pd.get_dummies(test['R'], prefix='R'), left_index=True, right_index=True).drop(['R'], axis=1)\ntest = test.merge(pd.get_dummies(test['C'], prefix='C'), left_index=True, right_index=True).drop(['C'], axis=1)\ntest = test.merge(pd.get_dummies(test['R__C'], prefix='R__C'), left_index=True, right_index=True).drop(['R__C'], axis=1)\n\n#trainlegacy()    \nuseful_features = [col for col in train.columns if col not in ['id', 'breath_id', 'pressure','kfold']]\n\nRS = RobustScaler()\ntrain[useful_features] = RS.fit_transform(train[useful_features])\ntest[useful_features] = RS.transform(test[useful_features])\nuseful_features","f3b422da":"train","e4fff908":"prep =lambda: ColumnTransformer(\n    transformers=[ \n        #('num', RobustScaler(), useful_features) \n    ], remainder='passthrough')","3fd2ffbc":"# utility functions\n\ndef optunaOpt(modelName,model,folds,n_trials,\n              params,\n              direction=\"minimize\",\n              metric = lambda x,y:mean_absolute_error(x,y),\n              callbacks=(lambda trial: [])):\n    \"\"\" Best model eval util using Optuna\n    \"\"\"\n    def run(trials):\n        \"\"\" Optima trials lambda\"\"\"\n        fold = folds\n        X_m = train\n        preprocessor = prep()\n        trial_params = {param:param_fn(trials) for param,param_fn in params.items()}\n        l_model = model(trial_params)\n        X_train = X_m[X_m.kfold != fold].reset_index(drop=True)\n        X_valid = X_m[X_m.kfold == fold].reset_index(drop=True)\n        train_data = preprocessor.fit_transform(X_train[useful_features])\n        val_data =  preprocessor.transform(X_valid[useful_features])\n        train_labels, val_labels = X_train[TARGET_VAR].to_numpy(), X_valid[TARGET_VAR].to_numpy()\n        l_model.fit(train_data, train_labels ,early_stopping_rounds=300,eval_set=[(val_data, val_labels)],\n                    verbose=False,\n                    callbacks = callbacks(trials))\n        preds_valid = l_model.predict(val_data)\n        score = metric(val_labels, preds_valid)\n        free_mem([X_m,X_train,X_valid,val_data])\n        return score\n    \n    study = optuna.create_study(direction=direction,\n                                study_name=f\"{modelName}-study\")\n    study.optimize(run, n_trials)\n    print('\\n Best Trial:')\n    print(study.best_trial)\n    print('\\n Best value')\n    print(study.best_value)\n    print('\\n Best hyperparameters:')\n    print(study.best_params)\n    return study\n\ndef model_trainer(name,reg,folds,_X=train,_Y=train[TARGET_VAR],X_test_m=test,\n                  useful_features= useful_features,\n                  prep=prep,\n                  metric = lambda x,y:mean_absolute_error(x,y),\n                  earlyStoppingRounds = 400,\n                  usePreprocessor=True):\n    \"\"\"trains model\"\"\"\n    final_predictions = []\n    final_test_predictions = []\n    final_valid_predictions = {}\n    final_valid_preds = []\n    final_valid_labels = []\n    history = []\n    models = []\n    scores = [] \n    X_m = _X\n    print(\"training model ...\",name)\n    for fold in tqdm(range(FOLDS)): \n        tempModel = reg(fold)\n        preprocessor = prep()\n        X_train = X_m[X_m.kfold != fold].reset_index(drop=True)\n        X_valid = X_m[X_m.kfold == fold].reset_index(drop=True)\n        train_data = preprocessor.fit_transform(X_train[useful_features])\n        val_data =  preprocessor.transform(X_valid[useful_features])\n        train_labels, val_labels = X_train[TARGET_VAR].to_numpy(), X_valid[TARGET_VAR].to_numpy()\n        # Training \n        if earlyStoppingRounds:\n            tempModel.fit(train_data, train_labels,early_stopping_rounds=earlyStoppingRounds,eval_set=[(val_data, val_labels)],verbose=False)\n        else:\n            tempModel.fit(train_data, train_labels)\n        valid_ids = X_valid.id.values.tolist()\n\n        test_preds = tempModel.predict(preprocessor.transform(X_test_m[useful_features]))\n        preds_valid = tempModel.predict(val_data)\n        final_predictions.append(test_preds)\n        foldStat = metric(val_labels,preds_valid)\n        final_test_predictions.append(test_preds)\n        final_valid_labels.append(val_labels)\n        final_valid_preds.append(test_preds)\n        final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n        scores.append(foldStat)\n        print(f\"Fold {fold} mae {foldStat}\") \n        # GC collect\n        free_mem([train_labels,val_labels,X_train,X_valid,train_data,val_data])\n        gc.collect()\n    print(f\"score : mean {np.mean(scores)} median {np.median(scores)} std  {np.std(scores)}\")    \n    # https:\/\/www.kaggle.com\/c\/ventilator-pressure-prediction\/discussion\/276138 \n    # Plot ROC Curve\n    final_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\n    final_valid_predictions.columns = [\"id\", f\"pred_{name}\"]\n    final_valid_predictions.to_csv(f\"train_{name}_pred.csv\", index=False)\n    sample_submission_temp = pd.read_csv(submission_file)\n    sample_submission_temp[TARGET_VAR] = np.median(np.column_stack(final_test_predictions), axis=1)\n    sample_submission_temp.columns = [\"id\", f\"pred_{name}\"]\n    sample_submission_temp.to_csv(f\"test_{name}_pred.csv\", index=False)\n    free_mem([sample_submission_temp])\n    gc.collect()\n    return  final_predictions  \n\ndef make_submission(name,final_predictions):\n    \"\"\"Makes submission for testing\"\"\"\n    sample_submission = pd.read_csv(submission_file)\n    try:\n        os.remove(f\"submission_{name}.csv\")\n    except (OSError, IOError) as e:    \n        #gulp\n        print(f\"Gulp {name}\")\n    # https:\/\/www.kaggle.com\/c\/ventilator-pressure-prediction\/discussion\/276138    \n    preds = np.column_stack(final_predictions)\n    sample_submission[TARGET_VAR] = np.mean(preds, axis=1)\n    sample_submission.to_csv(f\"submission_mean_{name}.csv\", index=False)\n    \n    sample_submission[TARGET_VAR] = np.median(preds, axis=1)\n    sample_submission.to_csv(f\"submission_median_{name}.csv\", index=False)\n    \n    # ENSEMBLE FOLDS WITH MEDIAN AND ROUND PREDICTIONS\n    sample_submission[TARGET_VAR] =  np.round( (sample_submission[TARGET_VAR] - PRESSURE_MIN)\/PRESSURE_STEP ) * PRESSURE_STEP + PRESSURE_MIN\n    sample_submission[TARGET_VAR] = np.clip(sample_submission[TARGET_VAR], PRESSURE_MIN, PRESSURE_MAX)\n    sample_submission.to_csv(f\"submission.csv\", index=False)","6f0edf41":"# Doing tuning with LGBM .\nlgbm_params = {\n    \"n_estimators\":lambda trial :trial.suggest_int(\"n_estimators\", 5000, 10000),\n    \"learning_rate\":lambda trial :trial.suggest_loguniform(\"learning_rate\", 0.01,1),\n    \"reg_lambda\":lambda trial :trial.suggest_loguniform(\"reg_lambda\", 1e-9, 100.0),\n    \"reg_alpha\":lambda trial :trial.suggest_loguniform(\"reg_alpha\", 1e-9, 100.0),\n    \"colsample_bytree\":lambda trial :trial.suggest_uniform(\"colsample_bytree\", 0.01, 1),\n    \"subsample\":lambda trial :trial.suggest_uniform(\"subsample\", 0.01, 1.0),\n    \"subsample_freq\":lambda trial :trial.suggest_int(\"subsample_freq\", 0,10),\n    #\"cat_l2\":lambda trial :trial.suggest_float(\"cat_l2\", 0.01, 0.4, log=True),\n    \"min_child_samples\":lambda trial :trial.suggest_int(\"min_child_samples\", 1, 256),\n    \"min_child_weight\":lambda trial :trial.suggest_uniform(\"min_child_weight\", 0.01, 10.0),\n    #\"scale_posn_weight\":lambda trial :trial.suggest_uniform(\"scale_posn_weight\", 1.0, 500.0),\n    #\"min_data_in_leaf\":lambda trial :trial.suggest_int(\"min_data_in_leaf\", 90, 110),\n    \"num_leaves\":lambda trial :trial.suggest_int(\"num_leaves\", 400,  1024),\n    \"max_bin\":lambda trial :trial.suggest_int(\"max_bin\", 150,  256),\n    \"max_depth\":lambda trial :trial.suggest_int(\"max_depth\", -1, 256),\n    #\"seed\":lambda trial :trial.suggest_categorical(\"seed\", [0,42]),\n    #\"feature_fraction\":lambda trial :trial.suggest_float(\"feature_fraction\", 0.1, 1.0, log=True),\n}\n\ndef tune():\n    gbmStudy = optunaOpt(\"lgbmModel\",lambda xargs : LGBMRegressor(boosting_type = 'gbdt',n_jobs=-1,  metric = 'mae',\n                                                     #device = 'gpu', gpu_platform_id=0,gpu_device_id=0, \n                                                     **xargs) ,0,100,lgbm_params,callbacks=\n                                                     lambda trial : [optuna.integration.LightGBMPruningCallback(trial,'l1')])\n    \n#tune()    ","5afa7c1f":"best_params = {'objective': 'regression',\n          'learning_rate': 0.25,\n          \"boosting_type\": \"gbdt\",\n          'min_data_in_leaf':600,\n          'max_bin': 196,\n          #'device':'gpu',\n          'feature_fraction':0.4,\n          'lambda_l1':36, 'lambda_l2':80,\n          'max_depth':16,\n          'num_leaves':1000,         \n         }\nbest_params = {'n_estimators': 6310, 'learning_rate': 0.04365620149019579, 'reg_lambda': 0.037260392798849885, 'reg_alpha': 5.16437079538232e-06,\n               'colsample_bytree': 0.9528265222160901, 'subsample': 0.6508127220019502,\n               'subsample_freq': 7, 'min_child_samples': 88, 'min_child_weight': 3.9179766017460653, 'num_leaves': 731, 'max_bin': 161, 'max_depth': 107}\nbest_params = {'n_estimators': 7022, 'learning_rate': 0.07852321810528877, 'reg_lambda': 1.7862446759505994e-07, 'reg_alpha': 1.2765906103250663e-06, 'colsample_bytree': 0.7566430939237064, \n               'subsample': 0.7338768565837517, 'subsample_freq': 0, 'min_child_samples': 99, 'min_child_weight': 4.008859124270689, 'num_leaves': 697, 'max_bin': 239, 'max_depth': 197}\nfinal_predictions_lgbm = model_trainer(\"lgbm_1\",lambda fold : LGBMRegressor( metric='mae', n_jobs= -1,\n                                                # device = 'gpu', gpu_platform_id=0,gpu_device_id=0,  #num_boost_round=100000,\n                                                  random_state=fold,**best_params),5,earlyStoppingRounds=220\n                                                    #int(best_params['n_estimators']*0.10\n                                                          )\n# 0.515 score","8f2f2228":"make_submission(\"lgbm_1\",final_predictions_lgbm)","206d69b8":"# Reduce memory usage imported from https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage"}}