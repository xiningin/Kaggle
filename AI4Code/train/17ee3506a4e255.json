{"cell_type":{"4a243371":"code","475ec6b3":"code","bc7b1813":"code","9cf7e1ad":"code","a0f5c272":"code","d3c8f896":"code","2071d79e":"code","949388df":"code","bb0fa399":"code","774ed51a":"code","f0c8601d":"code","551c1457":"code","d42d8dbf":"code","6315161e":"code","5ab84f4b":"code","a99c4066":"code","38ad3813":"code","4ac48d48":"code","c4ec7c55":"code","092ff5dd":"markdown"},"source":{"4a243371":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","475ec6b3":"hr_data = pd.read_csv('..\/input\/human-resource\/HR_comma_sep.csv')\nhr_data.head()","bc7b1813":"# we should probably rename this to something more intuitive like 'department'\nhr_data['sales'].unique()","9cf7e1ad":"hr_data['department'] = hr_data['sales']\nhr_data.drop('sales', axis=1, inplace=True)","a0f5c272":"hr_data.describe()","d3c8f896":"# Let's plot an understanding of the categorical features\n\nsns.set_style('darkgrid')\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15,7))\nfig.tight_layout()\nsns.countplot(x='salary', hue='left', data=hr_data, ax=ax1, palette='bright')\nax1.set_xticklabels(ax1.get_xticklabels(), fontsize=12);\nsns.countplot(x='department', hue='left', data=hr_data, ax=ax2, palette='bright')\nax2.set_xticklabels(ax2.get_xticklabels(), fontsize=12, rotation=45);\nax2.set_ylabel(' ');","2071d79e":"hr_num_data = hr_data[['satisfaction_level', 'last_evaluation', 'number_project',\n                       'average_montly_hours', 'time_spend_company', 'Work_accident',\n                       'left', 'promotion_last_5years']]","949388df":"# Let's get an understanding of the numerical features for employees who left the company\n\nleft_nums = hr_num_data[hr_num_data.left==1]\n\nleft_nums.describe()","bb0fa399":"# source: https:\/\/stackoverflow.com\/questions\/29432629\/plot-correlation-matrix-using-pandas\n\ndef CorrelationMatrix(df):\n    f = plt.figure(figsize=(8, 10))\n    plt.matshow(df.corr(), fignum=f.number)\n    plt.xticks(range(df.shape[1]), df.columns, fontsize=14, rotation=90)\n    plt.yticks(range(df.shape[1]), df.columns, fontsize=14)\n    cb = plt.colorbar()\n    cb.ax.tick_params(labelsize=14)","774ed51a":"CorrelationMatrix(hr_num_data)","f0c8601d":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_recall_fscore_support, precision_recall_curve","551c1457":"X_categorical = hr_data[['department', 'salary']]","d42d8dbf":"# Encode values\nX_categorical = pd.concat([pd.get_dummies(X_categorical['department']), \n                           pd.get_dummies(X_categorical['salary'])], axis=1)\nX_categorical.head()","6315161e":"# Scale values\ny = hr_num_data['left']\n\nhr_data_to_scale = hr_num_data.drop(['left', 'Work_accident'], axis=1)\n\nscaler = StandardScaler()\n\nX_numerical = pd.DataFrame(scaler.fit_transform(hr_data_to_scale), columns=hr_data_to_scale.columns)","5ab84f4b":"# Work_accident is binary and needn't be scaled\n\nX = pd.concat([X_categorical, X_numerical, hr_num_data['Work_accident']], axis=1)\nprint(X.shape)","a99c4066":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1867)","38ad3813":"# Baseline models\nlogreg = LogisticRegression()\n\nlogreg.fit(X_train, y_train)\n\nlr_preds = logreg.predict(X_test)\n\nlr_results = pd.DataFrame(precision_recall_fscore_support(y_test, lr_preds)).T\nlr_results.rename(index={0:'LR_0', 1:'LR_1'},\n                  columns={0:'Precision', 1:'Recall', \n                                 2:'F-Score', 3:'Support'}, inplace=True)\n\n\nrandfor = RandomForestClassifier()\n\nrandfor.fit(X_train, y_train)\n\nrf_preds = randfor.predict(X_test)\n\nrf_results = pd.DataFrame(precision_recall_fscore_support(y_test, rf_preds)).T\nrf_results.rename(index={0:'RF_0', 1:'RF_1'},\n                  columns={0:'Precision', 1:'Recall', \n                                 2:'F-Score', 3:'Support'}, inplace=True)\nbaseline_results = pd.concat([lr_results, rf_results], axis=0)\nbaseline_results","4ac48d48":"l_prec, l_rec, _ = precision_recall_curve(y_test, lr_preds)\nr_prec, r_rec, _ = precision_recall_curve(y_test, rf_preds)\n\nplt.figure(figsize=(10, 5))\nplt.plot(l_prec, l_rec, label='LogisticRegression')\nplt.plot(r_prec, r_rec, label='RandomForest')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n\nplt.legend();","c4ec7c55":"feat_names = X.columns\nimportances = randfor.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(10,5))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [feat_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","092ff5dd":"## Findings\n\nRandom Forest outperformed the LogisticRegression model significantly, and it carries the advantage of providing feature_importances from the prediction. If we plot these, we can see that **employee satisfaction level, number of projects, time spent with the company, average monthly hours, and the performance at the last evaluation** all influenced whether the model would predict an employee as staying, or leaving the company.\n\nDepartments, and various salary levels were not considered as heavily in the prediction process.\n\n"}}