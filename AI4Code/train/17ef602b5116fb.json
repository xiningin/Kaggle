{"cell_type":{"177265ea":"code","74d6e0a8":"code","c63fbc1d":"code","7b26b96b":"code","76d0616c":"code","00428af7":"code","a0ea568a":"code","736dc688":"code","f3cab81a":"code","977c0313":"code","ad8b36ae":"code","7c9c9d1d":"code","ed480fcc":"code","c5f95f90":"code","4a44fbb8":"code","5fa94234":"code","4cc5d1de":"code","3da53300":"code","4fcd4ef0":"code","45ea93a7":"code","23f04b25":"code","c2de3df6":"code","97efece2":"code","536bc5cf":"code","9fa4e0c4":"code","a16c054a":"code","45df35e1":"code","92af5e09":"code","390c6956":"code","e14c4cb8":"code","89ceebfb":"code","23ca6753":"code","ecace346":"code","7936edc1":"code","61340d0c":"code","3419b2cb":"code","e23a0be2":"code","71eb0a3e":"code","bfc61cbb":"code","a16416ff":"code","8b1bca08":"code","c0fe9c6d":"code","77a3690e":"code","c9f9a6bb":"code","18199d7f":"code","2744d357":"code","3d77a7d3":"code","073e65f3":"code","d368680c":"code","7c03c70a":"code","57ba2786":"code","bbc9654c":"code","7db216ec":"code","4a06aa2d":"code","0fc2fd4d":"code","03b1da68":"code","712102ff":"code","d39447b8":"code","4d75cbc7":"code","f4faeaa0":"code","625a9edd":"code","208cf260":"code","b05af720":"code","bd44fda0":"code","dd520831":"code","439843b1":"code","b91ac7b8":"code","881b3141":"code","bf58805c":"code","7729a641":"code","2609c036":"code","ed94fd4c":"code","a0730239":"code","ae5ad502":"code","7b5064bb":"code","b32b4e02":"code","655ca0df":"code","6fd5b047":"code","b02fdb70":"code","2c084120":"code","7da3ae83":"code","bdce4e95":"code","1a1cb39d":"code","df048503":"code","a90a37d8":"code","aaea2105":"code","58d05468":"code","a6a29e62":"code","b53f7608":"code","15367e26":"code","ebf7bb9d":"code","210f1764":"code","f0279ab4":"code","c2e88390":"code","cf76f2f2":"code","47d7a034":"code","f0e77ffd":"code","4f99c9da":"code","7d6b174e":"code","320b0cf7":"code","b4f166c0":"code","854eabfe":"code","c2dadc36":"code","c76c5709":"code","dacd076c":"code","62b33151":"code","89da1409":"code","b3f90fcb":"code","97a37a00":"code","e1eaf268":"code","7aa220bc":"code","cb23ae0b":"code","c9dac844":"code","49d6816f":"markdown","3c22eed7":"markdown","48c83098":"markdown","e7d45b25":"markdown","103063eb":"markdown","f88b7be2":"markdown","e306187b":"markdown","b71ceff2":"markdown","270f2824":"markdown","3cdfa4d6":"markdown","9dae9551":"markdown","6419af57":"markdown","0e973d16":"markdown","c2ad7850":"markdown","b028b37e":"markdown","2b18a49a":"markdown","c37d0e27":"markdown","60f4ae40":"markdown","88db7037":"markdown","ea4de67b":"markdown","7ad834d6":"markdown","c87b1575":"markdown","43327cea":"markdown","7ee8a2fe":"markdown","93df49b0":"markdown","cc7cd0b4":"markdown","76aed046":"markdown","3eaccf91":"markdown","c3a69f64":"markdown","cea022a7":"markdown","ebb23cc3":"markdown","df6debe2":"markdown","7fc78997":"markdown","17b16307":"markdown","cd35a084":"markdown","7a75ca63":"markdown","dd27a1c9":"markdown"},"source":{"177265ea":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport tensorflow as tf \nimport sklearn\nimport os \nimport pathlib\nfrom PIL import Image\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","74d6e0a8":"training_data_path = \"..\/input\/titanic\/train.csv\"\ntesting_data_path = \"..\/input\/titanic\/test.csv\"","c63fbc1d":"train_data = pd.read_csv(training_data_path)\ntest_data = pd.read_csv(testing_data_path)","7b26b96b":"#Training data contains 11 features and 1 target (Survived)\ntrain_data.head()","76d0616c":"# The Test set does nit contain the if the passenger  was Survived or not \ntest_data.head()","00428af7":"num_of_training_examples = train_data.shape[0]\nnum_of_training_examples","a0ea568a":"copy_of_train_data = train_data.copy()","736dc688":"#missing values, Age:177, Cabin:687, Embarked: 2\ncopy_of_train_data.isnull().sum()","f3cab81a":"sex_series = copy_of_train_data['Sex']\n\nnum_of_males = sex_series.value_counts()[0]\nnum_of_females = sex_series.value_counts()[1]\ntotal_passengers = num_of_males + num_of_females\n\nmale_prcentage = num_of_males \/ total_passengers\nfemale_prcentage = num_of_females \/ total_passengers\n\nprint(\"{:.2F}% of the passengers in the dataset are males and {:.2F}% are females\".format(\n    male_prcentage * 100, female_prcentage * 100))","977c0313":"def plot_histogram(column):\n    survived_passengers = copy_of_train_data[copy_of_train_data['Survived']==1][column].value_counts()\n    dead_passengers  = copy_of_train_data[copy_of_train_data['Survived']==0][column].value_counts()\n    df = pd.DataFrame([survived_passengers,dead_passengers])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True, title=str(column))","ad8b36ae":"# As we cas see the number of dead male is alot larger that the dead females\nplot_histogram('Sex')","7c9c9d1d":"ages = copy_of_train_data['Age']","ed480fcc":"ages.describe()","c5f95f90":"ages.plot(kind='box')","4a44fbb8":"copy_of_train_data['Age'].hist()","5fa94234":"age_categories = pd.cut(copy_of_train_data['Age'], [0, 14, 24, 64, 80], labels=['children', 'youth', 'adults', 'senior'])","4cc5d1de":"age_categories_survived = pd.DataFrame({'Survived': copy_of_train_data['Survived'], 'Age_Categories': age_categories})\nage_categories_survived","3da53300":"survived_passengers = age_categories_survived[age_categories_survived['Survived']==1]['Age_Categories'].value_counts()\ndead_passengers  = age_categories_survived[age_categories_survived['Survived']==0]['Age_Categories'].value_counts()\ndf = pd.DataFrame([survived_passengers,dead_passengers])\ndf.index = ['Survived','Dead']\ndf.plot(kind='bar',stacked=True, title='Age_Categories')","4fcd4ef0":"age_categories_survived['Age_Categories'].value_counts()","45ea93a7":"copy_of_train_data[\"Pclass\"].value_counts()","23f04b25":"copy_of_train_data[\"Pclass\"].hist()","c2de3df6":"plot_histogram('Pclass')","97efece2":"fare = copy_of_train_data['Fare']\nfare.hist()","536bc5cf":"embarked = copy_of_train_data[\"Embarked\"]\nembarked.value_counts()","9fa4e0c4":"embarked.hist()","a16c054a":"plot_histogram('Embarked')","45df35e1":"sibsp = copy_of_train_data[\"SibSp\"]\nsibsp.value_counts()","92af5e09":"parch = copy_of_train_data[\"Parch\"]\nparch.value_counts()","390c6956":"copy_of_train_data[copy_of_train_data['SibSp']==0]['Parch'].value_counts()","e14c4cb8":"# Create  a new featue (alone)\n\nalone_index = np.array(copy_of_train_data[copy_of_train_data['SibSp']==0][copy_of_train_data['Parch']==0].index)\n# 1:alone, 0:not alone\nnew_alone_column = np.zeros(shape=(len(copy_of_train_data),))\n\nnew_alone_column[alone_index] = 1","89ceebfb":"alone_survived = pd.DataFrame({'Survived': copy_of_train_data['Survived'], 'alone': new_alone_column})\nalone_survived","23ca6753":"survived_passengers = alone_survived[alone_survived['Survived']==1]['alone'].value_counts()\ndead_passengers  = alone_survived[alone_survived['Survived']==0]['alone'].value_counts()\ndf = pd.DataFrame([survived_passengers,dead_passengers])\ndf.index = ['Survived','Dead']\ndf.plot(kind='bar',stacked=True, title='alone_survived')","ecace346":"copy_of_train_data.head()","7936edc1":"#add the alone feature \ncopy_of_train_data[\"Alone\"] = new_alone_column\n\n#deleting the SibSp column\ndel(copy_of_train_data[\"SibSp\"])\n#deleting the Parch column\ndel(copy_of_train_data[\"Parch\"])\n\ncopy_of_train_data.head()","61340d0c":"#replace the Nmae column with a Title column\ntitles = []\nfor (colname,colval) in  copy_of_train_data[\"Name\"].iteritems():\n    titles.append(colval.split(',')[1].split('.')[0].strip())","3419b2cb":"title_column = pd.Series(titles)\n\n#replacing the rare titles with \"others\"\ntitle_column = title_column.replace(['Lady', 'the Countess','Capt', 'Col','Mlle', 'Don',\\\n                 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Ms', 'Mme'], 'Others')\n\n\ncopy_of_train_data[\"Title\"] = title_column\n#deleting the Name column\ndel(copy_of_train_data[\"Name\"])\n#deleting the Ticket column\ndel(copy_of_train_data[\"Ticket\"])\n#deleting the PassengerId column\ndel(copy_of_train_data[\"PassengerId\"])\n\n#deleting the Cabin column due to the large number of missing values\ndel(copy_of_train_data[\"Cabin\"])\n\n\ncopy_of_train_data.head()","e23a0be2":"copy_of_train_data[\"Title\"].value_counts()","71eb0a3e":"title_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \"Master\": 3, \"Others\": 4}\nsex_mapping = {\"male\": 0, \"female\": 1}\nembarked_mapping = {\"C\": 0, \"Q\": 1, \"S\": 1}\n\n\ncopy_of_train_data[\"Title\"].replace(title_mapping, inplace=True)\ncopy_of_train_data[\"Sex\"].replace(sex_mapping, inplace=True)\ncopy_of_train_data[\"Embarked\"].replace(embarked_mapping, inplace=True)\n\n\ncopy_of_train_data.head()","bfc61cbb":"# Finding the indices Embarked attribute where is null\nembarked_null_indices = np.where(copy_of_train_data['Embarked'].isnull())[0]\nembarked_null_indices","a16416ff":"# Drop the rows 61 and 829\ncopy_of_train_data.drop([61, 829], inplace=True)","8b1bca08":"copy_of_train_data = copy_of_train_data.fillna(copy_of_train_data.median())","c0fe9c6d":"copy_of_train_data.isnull().sum()","77a3690e":"copy_of_train_data.head()","c9f9a6bb":"features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Alone', 'Title']\n\nx_train = copy_of_train_data[features]\ny_train = np.array(copy_of_train_data[\"Survived\"])","18199d7f":"num_features = ['Age', 'Fare']\ncat_features = ['Pclass', 'Sex', 'Embarked', 'Alone', 'Title']\n\npreprocessing_pipeline = ColumnTransformer([\n    ('num', StandardScaler(), num_features),\n    ('cat', OneHotEncoder(), cat_features)\n])\n\nx_train_prepered = preprocessing_pipeline.fit_transform(x_train)","2744d357":"LR_model = LogisticRegression(max_iter=1000)\nLR_model.fit(x_train_prepered, y_train)\nprint (\"Training Accuracy: \" + str(round(LR_model.score(x_train_prepered, y_train) * 100, ndigits=2)) + '%')","3d77a7d3":"str_shuffle_split = StratifiedShuffleSplit(train_size=0.75, test_size=0.25, n_splits=5)\nscores = cross_val_score(LogisticRegression(max_iter=1000),\n                        x_train_prepered, y_train, cv=str_shuffle_split)\n\nprint(\"Cross Validation Scores: {}\".format(scores))\nprint(\"The Mean of Cross Validation Scores: {}\".format(scores.mean()))","073e65f3":"logistic_regression_score = round(scores.mean(), ndigits=3)\nlogistic_regression_score","d368680c":"SVC_model = SVC(probability=True)\nSVC_model.fit(x_train_prepered, y_train)\nprint (\"Training Accuracy: \" + str(round(SVC_model.score(x_train_prepered, y_train) * 100, ndigits=2)) + '%')","7c03c70a":"str_shuffle_split = StratifiedShuffleSplit(train_size=0.75, test_size=0.25, n_splits=5)\nscores = cross_val_score(SVC(),\n                        x_train_prepered, y_train, cv=str_shuffle_split)\n\nprint(\"Cross Validation Scores: {}\".format(scores))\nprint(\"The Mean of Cross Validation Scores: {}\".format(scores.mean()))","57ba2786":"SVM_score = round(scores.mean(), ndigits=3)\nSVM_score","bbc9654c":"model = LinearSVC(max_iter=10000)\nmodel.fit(x_train_prepered, y_train)\nprint (\"Training Accuracy: \" + str(round(model.score(x_train_prepered, y_train) * 100, ndigits=2)) + '%')","7db216ec":"str_shuffle_split = StratifiedShuffleSplit(train_size=0.75, test_size=0.25, n_splits=5)\nscores = cross_val_score(LinearSVC(max_iter=10000),\n                        x_train_prepered, y_train, cv=str_shuffle_split)\n\nprint(\"Cross Validation Scores: {}\".format(scores))\nprint(\"The Mean of Cross Validation Scores: {}\".format(scores.mean()))","4a06aa2d":"linear_SVM_score = round(scores.mean(), ndigits=3)\nlinear_SVM_score","0fc2fd4d":"model = DecisionTreeClassifier()\nmodel.fit(x_train_prepered, y_train)\nprint (\"Training Accuracy: \" + str(round(model.score(x_train_prepered, y_train) * 100, ndigits=2)) + '%')","03b1da68":"str_shuffle_split = StratifiedShuffleSplit(train_size=0.75, test_size=0.25, n_splits=5)\nscores = cross_val_score(DecisionTreeClassifier(),\n                        x_train_prepered, y_train, cv=str_shuffle_split)\n\nprint(\"Cross Validation Scores: {}\".format(scores))\nprint(\"The Mean of Cross Validation Scores: {}\".format(scores.mean()))","712102ff":"DT_score = round(scores.mean(), ndigits=3)\nDT_score","d39447b8":"model = RandomForestClassifier(n_estimators=100)\nmodel.fit(x_train_prepered, y_train)\nprint (\"Training Accuracy: \" + str(round(model.score(x_train_prepered, y_train) * 100, ndigits=2)) + '%')","4d75cbc7":"str_shuffle_split = StratifiedShuffleSplit(train_size=0.75, test_size=0.25, n_splits=5)\nscores = cross_val_score(RandomForestClassifier(n_estimators=100),\n                        x_train_prepered, y_train, cv=str_shuffle_split)\n\nprint(\"Cross Validation Scores: {}\".format(scores))\nprint(\"The Mean of Cross Validation Scores: {}\".format(scores.mean()))","f4faeaa0":"RF_score = round(scores.mean(), ndigits=3)\nRF_score","625a9edd":"model = Perceptron(penalty='l2', max_iter=10)\nmodel.fit(x_train_prepered, y_train)\nprint (\"Training Accuracy: \" + str(round(model.score(x_train_prepered, y_train) * 100, ndigits=2)) + '%')","208cf260":"str_shuffle_split = StratifiedShuffleSplit(train_size=0.75, test_size=0.25, n_splits=5)\nscores = cross_val_score(Perceptron(penalty='l2', max_iter=10),\n                        x_train_prepered, y_train, cv=str_shuffle_split)\n\nprint(\"Cross Validation Scores: {}\".format(scores))\nprint(\"The Mean of Cross Validation Scores: {}\".format(scores.mean()))","b05af720":"perceptron_score = round(scores.mean(), ndigits=3)\nperceptron_score","bd44fda0":"model = SGDClassifier()\nmodel.fit(x_train_prepered, y_train)\nprint (\"Training Accuracy: \" + str(round(model.score(x_train_prepered, y_train) * 100, ndigits=2)) + '%')","dd520831":"str_shuffle_split = StratifiedShuffleSplit(train_size=0.75, test_size=0.25, n_splits=5)\nscores = cross_val_score(SGDClassifier(),\n                        x_train_prepered, y_train, cv=str_shuffle_split)\n\nprint(\"Cross Validation Scores: {}\".format(scores))\nprint(\"The Mean of Cross Validation Scores: {}\".format(scores.mean()))","439843b1":"SGD_score = round(scores.mean(), ndigits=3)\nSGD_score","b91ac7b8":"model = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(16,),\n                               kernel_regularizer=tf.keras.regularizers.L2()))\nmodel.add(tf.keras.layers.BatchNormalization())\n\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n             optimizer='rmsprop',\n             metrics=['acc'])","881b3141":"history = model.fit(x_train_prepered, y_train,\n                    epochs=250, batch_size=64,\n                    validation_split=0.2)","bf58805c":"train_acc = history.history['acc']\nval_acc = history.history['val_acc']\n\nplt.plot(history.epoch, train_acc, label='Training Accuracy')\nplt.plot(history.epoch, val_acc, label='Validation Accuracy')\n\nplt.legend()\nplt.grid(True)","7729a641":"train_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.plot(history.epoch, train_loss, label='Training Loss')\nplt.plot(history.epoch, val_loss, label='Validation Loss')\nplt.legend()\nplt.grid(True)","2609c036":"np.max(history.history['acc'])","ed94fd4c":"np.max(history.history['val_acc'])","a0730239":"class stop_training(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if (logs.get('val_acc')>0.90):\n            print(\"\\nReached 90% accuracy so canceling training!\")\n            self.model.stop_training = True\n\nmy_callbacks = stop_training()\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss',\n                                                  patience=10,\n                                                  restore_best_weights=True)\n\n\nmodel = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(16,),\n                               kernel_regularizer=tf.keras.regularizers.L2()))\nmodel.add(tf.keras.layers.BatchNormalization())\n\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n             optimizer='rmsprop',\n             metrics=['acc'])\n\nhistory = model.fit(x_train_prepered, y_train,\n                    epochs=250, batch_size=64,\n                    validation_split=0.2,\n                    callbacks=[early_stopping, my_callbacks])","ae5ad502":"train_acc = history.history['acc']\nval_acc = history.history['val_acc']\n\nplt.plot(history.epoch, train_acc, label='Training Accuracy')\nplt.plot(history.epoch, val_acc, label='Validation Accuracy')\n\nplt.legend()\nplt.grid(True)","7b5064bb":"train_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.plot(history.epoch, train_loss, label='Training Loss')\nplt.plot(history.epoch, val_loss, label='Validation Loss')\nplt.legend()\nplt.grid(True)","b32b4e02":"DNN_loss, DNN_acc = model.evaluate(x_train_prepered, y_train)","655ca0df":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'Logistic Regression', \n              'Random Forest',  'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'simple Dence NN'],\n    'Score': [SVM_score, logistic_regression_score, RF_score, \n              perceptron_score, SGD_score, linear_SVM_score, \n              DT_score, DNN_acc]})\nmodels","6fd5b047":"test_data.head()","b02fdb70":"copy_of_test_data = test_data.copy()","2c084120":"alone_index = np.array(copy_of_test_data[copy_of_test_data['SibSp']==0][copy_of_test_data['Parch']==0].index)\n# 1:alone, 0:not alone\nnew_alone_column = np.zeros(shape=(len(copy_of_test_data),))\n\nnew_alone_column[alone_index] = 1","7da3ae83":"#add the alone feature \ncopy_of_test_data[\"Alone\"] = new_alone_column\n\n#deleting the SibSp column\ndel(copy_of_test_data[\"SibSp\"])\n#deleting the Parch column\ndel(copy_of_test_data[\"Parch\"])\n\ncopy_of_test_data.head()","bdce4e95":"#replace the Nmae column with a Title column\ntitles = []\nfor (colname,colval) in  copy_of_test_data[\"Name\"].iteritems():\n    titles.append(colval.split(',')[1].split('.')[0].strip())","1a1cb39d":"title_column = pd.Series(titles)\n\n#replacing the rare titles with \"others\"\ntitle_column = title_column.replace(['Lady', 'the Countess','Capt', 'Col','Mlle', 'Don',\\\n                 'Dr', 'Major', 'Rev', 'Sir', 'Dona' 'Jonkheer', 'Ms', 'Mme'], 'Others')\n\n\ncopy_of_test_data[\"Title\"] = title_column\n#deleting the Name column\ndel(copy_of_test_data[\"Name\"])\n#deleting the Ticket column\ndel(copy_of_test_data[\"Ticket\"])\n#deleting the PassengerId column\ndel(copy_of_test_data[\"PassengerId\"])\n\n#deleting the Cabin column due to the large number of missing values\ndel(copy_of_test_data[\"Cabin\"])\n\n\ncopy_of_test_data.head()","df048503":"title_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \"Master\": 3, \"Others\": 4}\nsex_mapping = {\"male\": 0, \"female\": 1}\nembarked_mapping = {\"C\": 0, \"Q\": 1, \"S\": 1}\n\n\ncopy_of_test_data[\"Title\"].replace(title_mapping, inplace=True)\ncopy_of_test_data[\"Sex\"].replace(sex_mapping, inplace=True)\ncopy_of_test_data[\"Embarked\"].replace(embarked_mapping, inplace=True)\n\n\ncopy_of_test_data.head()","a90a37d8":"# Finding the indices Embarked attribute where is null\nFare_null_indices = np.where(copy_of_test_data['Fare'].isnull())[0]\nFare_null_indices","aaea2105":"copy_of_test_data = copy_of_test_data.fillna(copy_of_test_data.median())","58d05468":"copy_of_test_data.head()","a6a29e62":"features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Alone', 'Title']\nx_test = copy_of_test_data[features]","b53f7608":"x_test.shape","15367e26":"x_test['Title'][414] = 4","ebf7bb9d":"num_features = ['Age', 'Fare']\ncat_features = ['Pclass', 'Sex', 'Embarked', 'Alone', 'Title']\n\npreprocessing_pipeline = ColumnTransformer([\n    ('num', StandardScaler(), num_features),\n    ('cat', OneHotEncoder(), cat_features)\n])\n\nx_test_prepered = preprocessing_pipeline.fit_transform(x_test)","210f1764":"x_test_prepered.shape","f0279ab4":"DNN_predictions = model.predict(x_test_prepered).reshape(-1)","c2e88390":"SVC_predictions = SVC_model.predict_proba(x_test_prepered)[:, 1]","cf76f2f2":"LR_SVC_predictions = LR_model.predict_proba(x_test_prepered)[:, 1]","47d7a034":"final_prediction_proba = (DNN_predictions + SVC_predictions + LR_SVC_predictions) \/ 3","f0e77ffd":"# I will pich predictions with a higher certainty thatn 0.5 to be the class 1\nfinal_prediction = final_prediction_proba > 0.6","4f99c9da":"predictions = np.zeros(shape=(418,))\npredictions[final_prediction] = 1","7d6b174e":"predictions =  predictions.astype('int32')","320b0cf7":"Submission  = pd.DataFrame({\"PassengerId\" : test_data['PassengerId'],\n                           \"Survived\" : predictions})","b4f166c0":"Submission","854eabfe":"Submission.to_csv(r\"C:\\Users\\Naser\\OneDrive\\Desktop\\Jupytor new\\naser.csv\", index=False, header = 1)","c2dadc36":"x = np.array(copy_of_train_data.drop('Survived', axis=1))\ny = np.array(copy_of_train_data['Survived'])\nx_test_2 = np.array(copy_of_test_data)","c76c5709":"x_test_2[414, 6] = 4","dacd076c":"g_boosting_model = GradientBoostingClassifier(n_estimators=500,\n                                             learning_rate=0.01,\n                                             random_state=0,\n                                             max_depth=5)\n\ng_boosting_model.fit(x, y)\ng_boosting_train_score = g_boosting_model.score(x, y)\n\nprint(\"Gradient Boosting Classifier Training Score: {:.3F}\".format(g_boosting_train_score))\n\n\n# GradientBoostingClassifier evaluated using shuffle-split cross-validation \ng_boosting_shuffle_split = StratifiedShuffleSplit(train_size=0.8, test_size=0.2, n_splits=3, random_state=0)\ng_boosting_val_scores = cross_val_score(g_boosting_model, x, y, cv=g_boosting_shuffle_split)\nprint(\"Gradient Boosting Classifier Cross validation Score: {:.3F}\".format(np.mean(g_boosting_val_scores)))\n\n# overfitting","62b33151":"confusion_matrix(y, g_boosting_model.predict(x))","89da1409":"print(classification_report(y,\n                           g_boosting_model.predict(x)))","b3f90fcb":"print(classification_report(y,\n                           g_boosting_model.predict_proba(x)[:,1] > 0.84))","97a37a00":"confusion_matrix(y, g_boosting_model.predict_proba(x)[:,1] > 0.84)","e1eaf268":"predictions_2 = g_boosting_model.predict_proba(x_test_2)[:,1] > 0.84","7aa220bc":"Submission  = pd.DataFrame({\"PassengerId\" : test_data['PassengerId'],\n                           \"Survived\" : predictions_2})","cb23ae0b":"Submission[\"Survived\"].replace({True: 1, False: 0}, inplace=True)\nSubmission","c9dac844":"Submission.to_csv(r\"C:\\Users\\Naser\\OneDrive\\Desktop\\Jupytor new\\naser_2.csv\", index=False, header = 1)","49d6816f":"# 2. Exploratory data analysis","3c22eed7":"# prepering the test data","48c83098":"Most of the passengers have not any parents or children  on board","e7d45b25":"Most of the passengers have not any siblings or spouses on board","103063eb":"Logistic Regression\n\nSupport Vector Machines\n\nDecision Tree\n\nRandom Forest\n\nPerceptron\n\nSGDClassifier\n\nDence NN ","f88b7be2":"## Sex","e306187b":"## Embarked","b71ceff2":"## Logistic Regression","270f2824":"  The mean age of passengers ages is 29.699118 years and the standard deviation is\nhigh 14.526497 years that's an indication of large deviation between passenger\nages as shown in the box plot there are alot of outliers which probably will\naffect the training algorithms performance, least old passengers in this \ndataset in 0.42 (about 5 months) and the oldest is 80 years old \nand as can we see from the histogram most of the passenger ages are \ndistributed between 20 years and 50 years.","3cdfa4d6":"## simple Dence NN","9dae9551":"## Linear SVM","6419af57":"Now we need to convert the categorical titles, Sex and Embarked to ordinal\nleaving the null values to handling it in the next section.","0e973d16":"## Random Forest","c2ad7850":"# Classification ","b028b37e":"as we can see the best model is the simple DNN ","2b18a49a":"# 1. Loading the Data","c37d0e27":"## Pclass","60f4ae40":"Data preperation Pipeline, standarize the numerical data and one-hot\nencode the categorical data","88db7037":"# Feature engineering and data preprocessing","ea4de67b":"As we can see from the first histogram most of the passenger classes were 3rd class (491 passenger) in this dataset.\nThe second histogram shows that the most of the deaths were from the 3rd class passengers,most of 3rd class passengers \nwere in the broadtail and that was the first sank part of the ship.  \n    \nAnd the number of Survived passengersfrom the 1st class is larger that the deths, 1st class passengers\nwere in the middle of the ship as we can see from the picture below their chance of survival is higher\nthat the 3rd class passengers.\n","7ad834d6":"   As we cas see from the Histogram mod of the dead passengers are adults and \nmost of the survived passengers are are also adults then youth passengers\nand that's beacause most of the passengers are adults and youth.","c87b1575":"## Fare","43327cea":"## Stochastic Gradient Decent","7ee8a2fe":"survival: Survival, 0 = No, 1 = Yes\n\npclass: pclass, 1 = 1st, 2 = 2nd, 3 = 3rd\n\nsex: male\/ female\n\nAge: Age in years\n\nsibsp: number of siblings \/ spouses aboard the Titanic\n\nparch: number of parents \/ children aboard the Titanic\n\nticket: Ticket number\n\nfare: Passenger fare\n\ncabin: Cabin number\n\nembarked: Port of Embarkation, C = Cherbourg, Q = Queenstown, S = Southampton","93df49b0":"## SibSp and Parch","cc7cd0b4":"## Decision Tree","76aed046":"Five Categorical Variables: Pclass, Sex, Embarked, Title, and Alone\n\nTwo Continuous Variables: Age and Fare","3eaccf91":"As we can see from the histogram the number of low fare passengers is the highest, and this reflects\nthe large number of  3rd class passengers.","c3a69f64":"## The number of the Dead passengers that was alone with no siblings, spouses, parents or children is higher than passengers with either siblings\/spouses or parents\/children, the lonly passengers are more likely to die.","cea022a7":"## Age","ebb23cc3":"Using the models with highest accuracy to make the final prediction","df6debe2":"# Handling missing values","7fc78997":"537 of the passengers was alone with no siblings, spouses, parents or children","17b16307":"The two plots above tesll us that the model starts to get worse after the frist\n100 epochs, one of the major reasons is the small amout of data, even when we \nuse a shallow neural network with just 1 layer so I decide to use callbacks to stop training \nif the model does not improve after a certain number of epochs ans restore the best one ","cd35a084":"## Support Vector Machine ","7a75ca63":"As we can see most of the passengers aboarded from  Southampton port and most of\nthe deaths was passengers aboarded from  Southampton port, they are more likely to die.\n","dd27a1c9":"## Perceptron\n"}}