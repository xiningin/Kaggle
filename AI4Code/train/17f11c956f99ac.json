{"cell_type":{"6dfbb5cf":"code","4f56052d":"code","d6731e3a":"code","9bcbc4ff":"code","f760e40f":"code","3a4430e8":"code","4ff4fa3d":"code","7107afd4":"code","b2ef7a31":"code","3a9cf30d":"code","725c97b0":"code","9e21414b":"code","fbbd0d2c":"code","1ad0553f":"code","ab38e5fd":"code","491fcd28":"code","dd9d29a8":"code","d5dbdf33":"code","32cc7486":"code","c66925b1":"code","54f5b31f":"code","3c5f3e03":"code","29d5d34b":"code","3cd946be":"code","f8ac2cec":"code","2e9c6c58":"code","69baef22":"code","39b290eb":"code","64433183":"code","b3d9bf7e":"code","5c2d3064":"code","b724675d":"code","eaa90871":"code","e9333b1d":"code","8efc1dd2":"code","62c4d0c9":"code","fb82819a":"code","d24e4225":"code","5baef48d":"code","73bbd9fd":"code","202690bc":"code","d49a76c1":"code","7a3da437":"code","fac4c7c3":"code","7b021260":"code","03672582":"code","31ac24e2":"code","c585b272":"code","ba2592cf":"code","d742a754":"code","d0eac693":"code","5307df62":"code","11d4fcbb":"code","5d47a2cf":"code","0f98f99e":"code","cd6c81a4":"code","9864f4a1":"markdown","f9d939fd":"markdown","05b1931c":"markdown","844a3314":"markdown","400424e8":"markdown","74b8eb71":"markdown","3f7f339c":"markdown","62a3175a":"markdown","56dffc05":"markdown","e9833a0a":"markdown","dd2c5287":"markdown","c9376433":"markdown","31623b2f":"markdown","78ab5eec":"markdown","5d1e3e25":"markdown","74163f45":"markdown","1637693a":"markdown","2791b2c5":"markdown","fa77a58c":"markdown","46285d75":"markdown","f8e3911d":"markdown","0382819a":"markdown","051fa548":"markdown","6f4d99a4":"markdown","59d3201e":"markdown","d9180815":"markdown","53fb849c":"markdown","08fedd20":"markdown","b7b25f44":"markdown","1c0e2007":"markdown","4c7db47f":"markdown","a1caf71b":"markdown","9c5ba4e2":"markdown","c85cd07f":"markdown","dbb0fbd8":"markdown","25afdb2b":"markdown","e7f7db56":"markdown","e917b692":"markdown","bdc613fe":"markdown","84f02a82":"markdown","b3646494":"markdown","5bd1ff5e":"markdown","0bda51ad":"markdown","29494ad7":"markdown","a26b47a7":"markdown","b62873d2":"markdown","2729fb50":"markdown","00ea9fca":"markdown","28df6ead":"markdown","71ac85f5":"markdown","02602240":"markdown","a6901e1f":"markdown","c1f10869":"markdown","e0ad3662":"markdown","540d4bdf":"markdown","6ce16e16":"markdown"},"source":{"6dfbb5cf":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom collections import OrderedDict\nimport pickle\n\nimport numpy as np\n\nimport IPython","4f56052d":"class MyLinear(nn.Module):\n    \"\"\"Linear layer with equalized learning rate and custom learning rate multiplier.\"\"\"\n    def __init__(self, input_size, output_size, gain=2**(0.5), use_wscale=False, lrmul=1, bias=True):\n        super().__init__()\n        he_std = gain * input_size**(-0.5) # He init\n        # Equalized learning rate and custom learning rate multiplier.\n        if use_wscale:\n            init_std = 1.0 \/ lrmul\n            self.w_mul = he_std * lrmul\n        else:\n            init_std = he_std \/ lrmul\n            self.w_mul = lrmul\n        self.weight = torch.nn.Parameter(torch.randn(output_size, input_size) * init_std)\n        if bias:\n            self.bias = torch.nn.Parameter(torch.zeros(output_size))\n            self.b_mul = lrmul\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        bias = self.bias\n        if bias is not None:\n            bias = bias * self.b_mul\n        return F.linear(x, self.weight * self.w_mul, bias)\n","d6731e3a":"gain = 2**(0.5)\ngain","9bcbc4ff":"he_std = gain*(512**(-0.5)) # input_size = 512\nhe_std","f760e40f":"lrmul = 1\ninit_std = 1.0\/lrmul\nprint(init_std)\nprint('w_mul when use wscale :',he_std*init_std)","3a4430e8":"torch.randn(512,512)*init_std","4ff4fa3d":"weight = torch.nn.Parameter(torch.randn(512,512)*init_std) # Parameter(..) ==> requires_grad=True\nweight","7107afd4":"bias = torch.nn.Parameter(torch.zeros(512,512))\nbias","b2ef7a31":"w_mul = lrmul\nb_mul = lrmul\n\nF.linear(torch.randn(512,512), weight*w_mul, bias*b_mul)","3a9cf30d":"class MyConv2d(nn.Module):\n    \"\"\"Conv layer with equalized learning rate and custom learning rate multiplier.\"\"\"\n    def __init__(self, input_channels, output_channels, kernel_size, gain=2**(0.5), use_wscale=False, lrmul=1, bias=True,\n                intermediate=None, upscale=False):\n        super().__init__()\n        if upscale:\n            self.upscale = Upscale2d()\n        else:\n            self.upscale = None\n        he_std = gain * (input_channels * kernel_size ** 2) ** (-0.5) # He init\n        self.kernel_size = kernel_size\n        if use_wscale:\n            init_std = 1.0 \/ lrmul\n            self.w_mul = he_std * lrmul\n        else:\n            init_std = he_std \/ lrmul\n            self.w_mul = lrmul\n        self.weight = torch.nn.Parameter(torch.randn(output_channels, input_channels, kernel_size, kernel_size) * init_std)\n        if bias:\n            self.bias = torch.nn.Parameter(torch.zeros(output_channels))\n            self.b_mul = lrmul\n        else:\n            self.bias = None\n        self.intermediate = intermediate\n\n    def forward(self, x):\n        bias = self.bias\n        if bias is not None:\n            bias = bias * self.b_mul\n        \n        have_convolution = False\n        if self.upscale is not None and min(x.shape[2:]) * 2 >= 128:\n            # this is the fused upscale + conv from StyleGAN, sadly this seems incompatible with the non-fused way\n            # this really needs to be cleaned up and go into the conv...\n            w = self.weight * self.w_mul\n            w = w.permute(1, 0, 2, 3)\n            # probably applying a conv on w would be more efficient. also this quadruples the weight (average)?!\n            w = F.pad(w, (1,1,1,1))\n            w = w[:, :, 1:, 1:]+ w[:, :, :-1, 1:] + w[:, :, 1:, :-1] + w[:, :, :-1, :-1]\n            x = F.conv_transpose2d(x, w, stride=2, padding=(w.size(-1)-1)\/\/2)\n            have_convolution = True\n        elif self.upscale is not None:\n            x = self.upscale(x)\n    \n        if not have_convolution and self.intermediate is None:\n            return F.conv2d(x, self.weight * self.w_mul, bias, padding=self.kernel_size\/\/2)\n        elif not have_convolution:\n            x = F.conv2d(x, self.weight * self.w_mul, None, padding=self.kernel_size\/\/2)\n        \n        if self.intermediate is not None:\n            x = self.intermediate(x)\n        if bias is not None:\n            x = x + bias.view(1, -1, 1, 1)\n        return x","725c97b0":"class NoiseLayer(nn.Module):\n    \"\"\"adds noise. noise is per pixel (constant over channels) with per-channel weight\"\"\"\n    def __init__(self, channels):\n        super().__init__()\n        self.weight = nn.Parameter(torch.zeros(channels))\n        self.noise = None\n    \n    def forward(self, x, noise=None):\n        if noise is None and self.noise is None:\n            noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device, dtype=x.dtype)\n        elif noise is None:\n            # here is a little trick: if you get all the noiselayers and set each\n            # modules .noise attribute, you can have pre-defined noise.\n            # Very useful for analysis\n            noise = self.noise\n        x = x + self.weight.view(1, -1, 1, 1) * noise\n        return x","9e21414b":"class StyleMod(nn.Module):\n    def __init__(self, latent_size, channels, use_wscale):\n        super(StyleMod, self).__init__()\n        self.lin = MyLinear(latent_size,\n                            channels * 2,\n                            gain=1.0, use_wscale=use_wscale)\n        \n    def forward(self, x, latent):\n        style = self.lin(latent) # style => [batch_size, n_channels*2]\n        shape = [-1, 2, x.size(1)] + (x.dim() - 2) * [1]\n        style = style.view(shape)  # [batch_size, 2, n_channels, ...]\n        x = x * (style[:, 0] + 1.) + style[:, 1]\n        return x","fbbd0d2c":"lin = MyLinear(512, 3*2, 1.0, use_wscale=True)","1ad0553f":"latent1 = torch.from_numpy(np.random.randn(3,512).astype(np.float64))\nlatent2 = torch.from_numpy(np.random.randn(3,512).astype(np.float64))\nlatent = torch.cat([latent1,latent2], axis=0)","ab38e5fd":"latent.size()","491fcd28":"class PixelNormLayer(nn.Module):\n    def __init__(self, epsilon=1e-8):\n        super().__init__()\n        self.epsilon = epsilon\n    def forward(self, x):\n        return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)","dd9d29a8":"class BlurLayer(nn.Module):\n    def __init__(self, kernel=[1, 2, 1], normalize=True, flip=False, stride=1):\n        super(BlurLayer, self).__init__()\n        kernel=[1, 2, 1]\n        kernel = torch.tensor(kernel, dtype=torch.float32)\n        kernel = kernel[:, None] * kernel[None, :]\n        kernel = kernel[None, None]\n        if normalize:\n            kernel = kernel \/ kernel.sum()\n        if flip:\n            kernel = kernel[:, :, ::-1, ::-1]\n        self.register_buffer('kernel', kernel)\n        self.stride = stride\n    \n    def forward(self, x):\n        # expand kernel channels\n        kernel = self.kernel.expand(x.size(1), -1, -1, -1)\n        x = F.conv2d(\n            x,\n            kernel,\n            stride=self.stride,\n            padding=int((self.kernel.size(2)-1)\/2),\n            groups=x.size(1)\n        )\n        return x\n","d5dbdf33":"def upscale2d(x, factor=2, gain=1):\n    assert x.dim() == 4\n    if gain != 1:\n        x = x * gain\n    if factor != 1:\n        shape = x.shape\n        x = x.view(shape[0], shape[1], shape[2], 1, shape[3], 1).expand(-1, -1, -1, factor, -1, factor)\n        x = x.contiguous().view(shape[0], shape[1], factor * shape[2], factor * shape[3])\n    return x\n\nclass Upscale2d(nn.Module):\n    def __init__(self, factor=2, gain=1):\n        super().__init__()\n        assert isinstance(factor, int) and factor >= 1\n        self.gain = gain\n        self.factor = factor\n    def forward(self, x):\n        return upscale2d(x, factor=self.factor, gain=self.gain)","32cc7486":"class G_mapping(nn.Sequential):\n    def __init__(self, nonlinearity='lrelu', use_wscale=True):\n        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n        layers = [\n            ('pixel_norm', PixelNormLayer()),\n            ('dense0', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n            ('dense0_act', act),\n            ('dense1', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n            ('dense1_act', act),\n            ('dense2', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n            ('dense2_act', act),\n            ('dense3', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n            ('dense3_act', act),\n            ('dense4', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n            ('dense4_act', act),\n            ('dense5', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n            ('dense5_act', act),\n            ('dense6', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n            ('dense6_act', act),\n            ('dense7', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n            ('dense7_act', act)\n        ]\n        super().__init__(OrderedDict(layers))\n        \n    def forward(self, x):\n        x = super().forward(x)\n        # Broadcast\n        x = x.unsqueeze(1).expand(-1, 18, -1)\n        return x","c66925b1":"class Truncation(nn.Module):\n    def __init__(self, avg_latent, max_layer=8, threshold=0.7):\n        super().__init__()\n        self.max_layer = max_layer\n        self.threshold = threshold\n        self.register_buffer('avg_latent', avg_latent)\n    def forward(self, x):\n        assert x.dim() == 3\n        interp = torch.lerp(self.avg_latent, x, self.threshold)\n        do_trunc = (torch.arange(x.size(1)) < self.max_layer).view(1, -1, 1)\n        return torch.where(do_trunc, interp, x)","54f5b31f":"class LayerEpilogue(nn.Module):\n    \"\"\"Things to do at the end of each layer.\"\"\"\n    def __init__(self, channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n        super().__init__()\n        layers = []\n        if use_noise:\n            layers.append(('noise', NoiseLayer(channels)))\n        layers.append(('activation', activation_layer))\n        if use_pixel_norm:\n            layers.append(('pixel_norm', PixelNorm()))\n        if use_instance_norm:\n            layers.append(('instance_norm', nn.InstanceNorm2d(channels)))\n        self.top_epi = nn.Sequential(OrderedDict(layers))\n        if use_styles:\n            self.style_mod = StyleMod(dlatent_size, channels, use_wscale=use_wscale)\n        else:\n            self.style_mod = None\n    def forward(self, x, dlatents_in_slice=None):\n        x = self.top_epi(x)\n        if self.style_mod is not None:\n            x = self.style_mod(x, dlatents_in_slice)\n        else:\n            assert dlatents_in_slice is None\n        return x","3c5f3e03":"class InputBlock(nn.Module):\n    def __init__(self, nf, dlatent_size, const_input_layer, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n        super().__init__()\n        self.const_input_layer = const_input_layer\n        self.nf = nf\n        if self.const_input_layer:\n            # called 'const' in tf\n            self.const = nn.Parameter(torch.ones(1, nf, 4, 4))\n            self.bias = nn.Parameter(torch.ones(nf))\n        else:\n            self.dense = MyLinear(dlatent_size, nf*16, gain=gain\/4, use_wscale=use_wscale) # tweak gain to match the official implementation of Progressing GAN\n        self.epi1 = LayerEpilogue(nf, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n        self.conv = MyConv2d(nf, nf, 3, gain=gain, use_wscale=use_wscale)\n        self.epi2 = LayerEpilogue(nf, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n        \n    def forward(self, dlatents_in_range):\n        batch_size = dlatents_in_range.size(0)\n        if self.const_input_layer:\n            x = self.const.expand(batch_size, -1, -1, -1)\n            x = x + self.bias.view(1, -1, 1, 1)\n        else:\n            x = self.dense(dlatents_in_range[:, 0]).view(batch_size, self.nf, 4, 4)\n        x = self.epi1(x, dlatents_in_range[:, 0])\n        x = self.conv(x)\n        x = self.epi2(x, dlatents_in_range[:, 1])\n        return x","29d5d34b":"class GSynthesisBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, blur_filter, dlatent_size, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n        # 2**res x 2**res # res = 3..resolution_log2\n        super().__init__()\n        if blur_filter:\n            blur = BlurLayer(blur_filter)\n        else:\n            blur = None\n        self.conv0_up = MyConv2d(in_channels, out_channels, kernel_size=3, gain=gain, use_wscale=use_wscale,\n                                 intermediate=blur, upscale=True)\n        self.epi1 = LayerEpilogue(out_channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n        self.conv1 = MyConv2d(out_channels, out_channels, kernel_size=3, gain=gain, use_wscale=use_wscale)\n        self.epi2 = LayerEpilogue(out_channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n            \n    def forward(self, x, dlatents_in_range):\n        x = self.conv0_up(x)\n        x = self.epi1(x, dlatents_in_range[:, 0])\n        x = self.conv1(x)\n        x = self.epi2(x, dlatents_in_range[:, 1])\n        return x","3cd946be":"class G_synthesis(nn.Module):\n    def __init__(self,\n        dlatent_size        = 512,          # Disentangled latent (W) dimensionality.\n        num_channels        = 3,            # Number of output color channels.\n        resolution          = 1024,         # Output resolution.\n        fmap_base           = 8192,         # Overall multiplier for the number of feature maps.\n        fmap_decay          = 1.0,          # log2 feature map reduction when doubling the resolution.\n        fmap_max            = 512,          # Maximum number of feature maps in any layer.\n        use_styles          = True,         # Enable style inputs?\n        const_input_layer   = True,         # First layer is a learned constant?\n        use_noise           = True,         # Enable noise inputs?\n        randomize_noise     = True,         # True = randomize noise inputs every time (non-deterministic), False = read noise inputs from variables.\n        nonlinearity        = 'lrelu',      # Activation function: 'relu', 'lrelu'\n        use_wscale          = True,         # Enable equalized learning rate?\n        use_pixel_norm      = False,        # Enable pixelwise feature vector normalization?\n        use_instance_norm   = True,         # Enable instance normalization?\n        dtype               = torch.float32,  # Data type to use for activations and outputs.\n        blur_filter         = [1,2,1],      # Low-pass filter to apply when resampling activations. None = no filtering.\n        ):\n        \n        super().__init__()\n        def nf(stage):\n            return min(int(fmap_base \/ (2.0 ** (stage * fmap_decay))), fmap_max)\n        self.dlatent_size = dlatent_size\n        resolution_log2 = int(np.log2(resolution))\n        assert resolution == 2**resolution_log2 and resolution >= 4\n\n        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n        num_layers = resolution_log2 * 2 - 2\n        num_styles = num_layers if use_styles else 1\n        torgbs = []\n        blocks = []\n        for res in range(2, resolution_log2 + 1):\n            channels = nf(res-1)\n            name = '{s}x{s}'.format(s=2**res)\n            if res == 2:\n                blocks.append((name,\n                               InputBlock(channels, dlatent_size, const_input_layer, gain, use_wscale,\n                                      use_noise, use_pixel_norm, use_instance_norm, use_styles, act)))\n                \n            else:\n                blocks.append((name,\n                               GSynthesisBlock(last_channels, channels, blur_filter, dlatent_size, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, act)))\n            last_channels = channels\n        self.torgb = MyConv2d(channels, num_channels, 1, gain=1, use_wscale=use_wscale)\n        self.blocks = nn.ModuleDict(OrderedDict(blocks))\n        \n    def forward(self, dlatents_in):\n        # Input: Disentangled latents (W) [minibatch, num_layers, dlatent_size].\n        # lod_in = tf.cast(tf.get_variable('lod', initializer=np.float32(0), trainable=False), dtype)\n        batch_size = dlatents_in.size(0)       \n        for i, m in enumerate(self.blocks.values()):\n            if i == 0:\n                x = m(dlatents_in[:, 2*i:2*i+2])\n            else:\n                x = m(x, dlatents_in[:, 2*i:2*i+2])\n        rgb = self.torgb(x)\n        return rgb","f8ac2cec":"g_all = nn.Sequential(OrderedDict([\n    ('g_mapping', G_mapping()),\n    ('g_synthesis', G_synthesis())    \n]))\n","2e9c6c58":"import os\nos.listdir('..\/input\/ffhq-1024x1024-pretrained')","69baef22":"g_all.load_state_dict(torch.load('..\/input\/ffhq-1024x1024-pretrained\/karras2019stylegan-ffhq-1024x1024.for_g_all.pt'))","39b290eb":"device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\ng_all.eval()\ng_all.to(device)","64433183":"nb_rows = 3\nnb_cols = 3\nnb_samples = nb_rows * nb_cols","b3d9bf7e":"latents = torch.randn(nb_samples, 512, device=device)","5c2d3064":"latents","b724675d":"latents.shape","eaa90871":"import torchvision\nimport matplotlib.pyplot as plt","e9333b1d":"with torch.no_grad():\n    imgs = g_all(latents)\n    imgs = (imgs.clamp(-1, 1)+1)\/2.0  # normalization to 0~1 range\nimgs = imgs.cpu()\n\nimgs = torchvision.utils.make_grid(imgs, nrow=nb_cols)\n\nplt.figure(figsize=(15,6))\nplt.imshow(imgs.permute(1,2,0).detach().numpy())\nplt.axis('off')\nplt.show()","8efc1dd2":"latent1 = torch.randn(1, 512, device=device)\nimg1 = g_all(latent1)\nimg1 = img1.clamp(-1,1)+1\/2.0\nimg1 = img1.cpu()\n\nimg1.shape","62c4d0c9":"plt.imshow(img1.squeeze().permute(1,2,0).detach().numpy()) # drop batch (4dim -> 3dim)\nplt.axis('off')\nplt.show()","fb82819a":"latent2 = torch.randn(1, 512, device=device)\nimg2 = g_all(latent2)\nimg2 = img2.clamp(-1,1)+1\/2.0\nimg2 = img2.cpu()\n\nimg2.shape","d24e4225":"plt.imshow(img2.squeeze().permute(1,2,0).detach().numpy()) # drop batch (4dim -> 3dim)\nplt.axis('off')\nplt.show()","5baef48d":"new_img = g_all(latent1*0.5 + latent2*0.5)\nnew_img = new_img.clamp(-1,1)+1\/2.0\nnew_img = new_img.cpu()\n\nnew_img.shape","73bbd9fd":"plt.imshow(new_img.squeeze().permute(1,2,0).detach().numpy()) \nplt.axis('off')\nplt.show()","202690bc":"g_mapping = g_all[0] # We can extract mapping network like this.\ng_mapping","d49a76c1":"g_synthesis = g_all[1]# Similarly, synthesis network can be extracted like this.","7a3da437":"w_1 = g_mapping(latent1)\nw_2 = g_mapping(latent2)","fac4c7c3":"print(latent1.size(), w_1.size())\nprint(latent2.size(), w_2.size())","7b021260":"img3 = g_synthesis(w_1*0.5 + w_2*0.5)\nimg3 = img3.clamp(-1,1)+1\/2.0\nimg3 = img3.cpu()\n\nimg3.shape","03672582":"plt.imshow(img3.squeeze().permute(1,2,0).detach().numpy()) \nplt.axis('off')\nplt.show()","31ac24e2":"itp_imgs = []\n\nwith torch.no_grad():\n    for a in np.linspace(0, 1, 10):\n        z = ((1-a) * latent1) + (a * latent2)\n        result = g_all(z)\n        result = result.clamp(-1,1)+1\/2.0\n        result = result.cpu()\n        itp_imgs.append(result)","c585b272":"itp_imgs[0].size()","ba2592cf":"itp_imgs = torch.cat(itp_imgs)\nitp_imgs.size()","d742a754":"grid_img = torchvision.utils.make_grid(itp_imgs, nrow=5)\ngrid_img.size()","d0eac693":"plt.imshow(grid_img.permute(1,2,0).detach().numpy())\nplt.axis('off')\nplt.show()","5307df62":"itp_imgs2 = []\n\nwith torch.no_grad():\n    for a in np.linspace(0, 1, 10):\n        w = ((1-a) * w_1) + (a * w_2)\n        result2 = g_synthesis(w)\n        result2 = result2.clamp(-1,1)+1\/2.0\n        result2 = result2.cpu()\n        itp_imgs2.append(result2)","11d4fcbb":"itp_imgs2[0].size()","5d47a2cf":"itp_imgs2 = torch.cat(itp_imgs2)\nitp_imgs2.size()","0f98f99e":"grid_img2 = torchvision.utils.make_grid(itp_imgs2, nrow=5)\ngrid_img2.size()","cd6c81a4":"plt.imshow(grid_img2.permute(1,2,0).detach().numpy())\nplt.axis('off')\nplt.show()","9864f4a1":"**6-c. half `z` + half `z`**","f9d939fd":"**2-g. Upscaling Layer**","05b1931c":"**5-b. input setting - grid**","844a3314":"**6-e. Image Interpolation Comparison**","400424e8":"### Step 4. Define the Model (Image Generator)","74b8eb71":"> Copying the styles corresponding to coarse spatial resolutions brings high-level aspects such as pose, general hair style, face shape, and eyeglasses from source B, while all colors (eyes, hair, lighting) and finer facial features resemble source A.","3f7f339c":"![image](https:\/\/bloglunit.files.wordpress.com\/2019\/02\/1_gwchaliormc1xlj7bh0zmg.png)","62a3175a":"### Step 5. Test the Model","56dffc05":"- The noise layer receives the channels and returns the channels to which the noise is applied.\n- The noise layer adds gaussian noise of learnable standard deviation ","e9833a0a":"**5-c. input setting - latent z**","dd2c5287":"![image](https:\/\/bloglunit.files.wordpress.com\/2019\/02\/e18489e185b3e1848fe185b3e18485e185b5e186abe18489e185a3e186ba-2019-02-24-e1848be185a9e18492e185ae-3.43.31.png)","c9376433":"> The generator in a traditional GAN vs the one used by NVIDIA in the StyleGAN","31623b2f":"In this kernel, I will focus on trying out a pre-trained stylegan model.\n\nTherefore, understanding of the background knowledge of gan should be preceded, and it is good to check in advance how stylegan is implemented through the official TensorFlow code. (We'll use PyTorch more simply.) So let's get started.\n\n- [PyTorch\ud83d\udd25 GAN Basic Tutorial for beginner](https:\/\/www.kaggle.com\/songseungwon\/pytorch-gan-basic-tutorial-for-beginner)\n\n\nAnd, this kernel uses [lernapparat's Jupyter notebook (which recreates the StyleGAN for use with the pretrained weights)](https:\/\/github.com\/sw-song\/lernapparat\/blob\/master\/style_gan\/pytorch_style_gan.ipynb) as the base code. I added some test code and refined the structure. Thanks to lernapparat for the nice code sharing.","78ab5eec":"**6-d. half `w` + half `w`**","5d1e3e25":"> If latent z is put into g_mapping network, w is returned, and if the returned w is put into g_synthesis, an image is created. This process is chained sequentially and occurs one after another.","74163f45":"**6-a. first random latent vector + generate first image**","1637693a":"![image](https:\/\/bloglunit.files.wordpress.com\/2019\/02\/e18489e185b3e1848fe185b3e18485e185b5e186abe18489e185a3e186ba-2019-02-24-e1848be185a9e18492e185ae-5.42.19.png?w=1222)","2791b2c5":"### Step 3. Design Networks","fa77a58c":"**3-a. Generator Mapping Network**","46285d75":"![image](https:\/\/miro.medium.com\/max\/1400\/1*mDA1ms7D5NrwKXp4r2CXQQ.png)","f8e3911d":"By the way, we actually have a w vector that passed through the G_mapping network. Let's try it.","0382819a":"**4-a. data flow : z to image**","051fa548":"### Step 1. Import Libraries","6f4d99a4":"> Using the same metric(targeted initialization)","59d3201e":"![image](https:\/\/bloglunit.files.wordpress.com\/2019\/02\/0_uqn4slmhrfykfmjs.png)","d9180815":"**6-b. second random latent vector + generate second image**","53fb849c":"## Preview","08fedd20":"> For each block, 2 noises and 2 styles are continuously injected.","b7b25f44":"Yes. It's so much more natural! Here we can see the strengths of stylegan. The traditional image generation model immediately generates an image from a random vector(gaussian distribution) z. That have showed how high the degree of freedom is, in other words, the low degree of feature separation(**It is said to be entangled**). stylegan captured this core 'style' through the mapping network, and we confirmed this through the interpolation results. \n\nin Abstract..\n> The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. ","1c0e2007":"**2-a. Linear Layer**","4c7db47f":"# Image Generation using Stylegan pre-trained model","a1caf71b":"![image](https:\/\/www.researchgate.net\/publication\/343021405\/figure\/fig3\/AS:915394470625280@1595258457162\/Generator-architecture-of-the-StyleGAN-neural-network-1.png)","9c5ba4e2":"**3-c. Generator Synthesis Network**","c85cd07f":"**2-c. Noise Layer**","dbb0fbd8":"Let's look at the schematic again at this point. ","25afdb2b":"**5-d. show samples**","e7f7db56":"[-->paper](https:\/\/openaccess.thecvf.com\/content_CVPR_2019\/papers\/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.pdf)\n\n![-->paper](https:\/\/openaccess.thecvf.com\/content_CVPR_2019\/papers\/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.pdf)","e917b692":"> TEST CODE :","bdc613fe":"**5-a. gpu setting**","84f02a82":"## Main Reference\n1. [PyTorch-GAN | Github\/lernapparat | PyTorch implementation of the StyleGAN Generator](https:\/\/github.com\/sw-song\/lernapparat\/blob\/master\/style_gan\/pytorch_style_gan.ipynb)\n2. [stylegan | Github\/NVlabs | Official](https:\/\/github.com\/NVlabs\/stylegan)\n3. [StyleGAN: Use machine learning to generate and customize realistic images](https:\/\/heartbeat.fritz.ai\/stylegans-use-machine-learning-to-generate-and-customize-realistic-images-c943388dc672)","b3646494":"**2-b. Convolution Layer**","5bd1ff5e":"## Index\n```\nStep 1. Import Libraries\nStep 2. Design Layers\n     2-a. linear layer\n     2-b. convolution layer\n     2-c. noise layer\n     2-d. style modification layer\n     2-e. pixel normalization layer\n     2-f. blur layer\n     2-g. upscaling layer\nStep 3. Design Networks\n     3-a. generator mapping network\n     3-b. generator synthesis blocks\n     3-c. generator synthesis network\nStep 4. Define the Model (Image Generator)\n     4-a. data flow : z to image\n     4-b. load pre-trained weight\nStep 5. Test the Model\n     5-a. gpu setting\n     5-b. input setting - grid\n     5-c. input setting - latent z\n     5-d. show samples\nStep 6. Control Latent Vector\n     6-a. first random latent vector + generate first image\n     6-b. second random latent vector + generate second image\n     6-c. half `z` + half `z`\n     6-d. half `w` + half `w`\n     6-e. Image Interpolation Comparison\n```\n---","0bda51ad":"Yes! I think this looks more like a `half+half`\n\nAnd It is a really surprising result that it is estimated to be in the middle even by age.","29494ad7":"then, let's convert to image (half + half)","a26b47a7":"### Step 2. Design Layers","b62873d2":"> sampling latent `z`(gaussian distribution) --> return `w` vector\n\n> style information is contained in `w`\n","2729fb50":"**2-e. Pixel Normalization Layer**","00ea9fca":"The results through the MLP mapping network are as follows.","28df6ead":"**2-d. Style Modification Layer**","71ac85f5":"**3-b. Generator Synthesis Blocks**","02602240":"## Structure","a6901e1f":"> TEST CODE :","c1f10869":"**4-b. load pre-trained weight**","e0ad3662":"### Step 6. Control Latent Vector","540d4bdf":"> With this Class, Targeted initialization is performed for each layer. \nIt allows generator to follow the targeted style distribution.","6ce16e16":"**2-f. Blur Layer**"}}