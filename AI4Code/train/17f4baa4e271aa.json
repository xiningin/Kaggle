{"cell_type":{"76377251":"code","6f075881":"code","b14b8644":"code","ca3ce97d":"code","cbff4af5":"code","7aac8806":"code","470d1764":"code","cea6aac0":"code","fb017c63":"code","817abd61":"code","f3eb8ef8":"code","15b8bd58":"code","dce8f635":"code","cfd04a45":"code","4af4b940":"code","4f20a3f3":"code","bd865a54":"code","16d4c9de":"code","6d22a528":"code","14267b58":"code","5b22a101":"code","7c23971d":"code","e3d97370":"code","101fb772":"code","01a973b5":"code","109fe971":"code","31647d01":"code","36f47f73":"code","0778e276":"code","a2401042":"code","1127f674":"code","35d48cbf":"code","6201883b":"code","06ae7c0c":"code","fd33c422":"code","34cb9b3f":"code","3ef7a03a":"code","c559e93d":"code","42acaf7f":"code","f6af5ad2":"code","a3190143":"code","f7e282b7":"code","e1b85f6d":"code","dd856cc7":"code","e3b4cf74":"code","9e1cccd2":"code","6205d643":"code","0b9f21fc":"code","6f9f4876":"code","c69c98f7":"code","9b12e31b":"code","c1332e70":"code","4918f76f":"code","0ee64f35":"code","61721f89":"code","04be2b8e":"code","f28236f8":"code","62d4bb6f":"code","b051a125":"code","2c52d137":"code","899db1a9":"code","1f03a772":"code","e65cae43":"code","18d056f2":"code","8c0566ff":"code","865d02e3":"code","488b940d":"code","062800a8":"code","79dc9605":"code","665e5ca0":"code","7603deee":"code","420431e8":"code","e9897e4f":"code","e3e76f53":"code","7e364272":"code","a0841a61":"code","463087fd":"code","5ceea7a1":"code","9393848c":"code","de71f999":"code","e4b15380":"code","ce3f4cca":"code","5629e72e":"code","2ed314a1":"code","553d9d9a":"code","1e77fed1":"markdown","27f4b8c4":"markdown","1900b30d":"markdown","45a63b7e":"markdown","1142f41d":"markdown","f1054034":"markdown","3d08c390":"markdown","68cc86c2":"markdown","faaa6654":"markdown","3832ecd0":"markdown","00a198fc":"markdown","e1898913":"markdown","7f057c2f":"markdown","85e5da8e":"markdown","91c4ac90":"markdown","b6cf733b":"markdown","bbd52d1a":"markdown","5ad1e63b":"markdown","c28e1cca":"markdown","b7afe561":"markdown","a1f65640":"markdown","929ca53c":"markdown","071a319d":"markdown","ab6a5f1a":"markdown","3ffd3b81":"markdown","697b6c09":"markdown","72c1637a":"markdown","062d384f":"markdown","0f488710":"markdown","a43ae4ef":"markdown","532aa05d":"markdown","e411905a":"markdown","bebd0b83":"markdown","8a12f206":"markdown","489db3ed":"markdown","0edd7d3a":"markdown","f2ba86d0":"markdown","0edca9da":"markdown","b926ff8b":"markdown","047563a2":"markdown","084a0337":"markdown","8f1a20f5":"markdown","bfd97934":"markdown","bbf2ed0d":"markdown","99e1b691":"markdown","4ccfb8a6":"markdown","c08392a1":"markdown","735529b5":"markdown","886c0085":"markdown","cb4e66db":"markdown","d25dc82e":"markdown"},"source":{"76377251":"import datetime\nd = datetime.datetime.now()\nprint(\"Updated : \", d.strftime(\"%Y-%m-%d %H:%M:%S\"))","6f075881":"########\n# Import\n#\n%matplotlib inline\n\n# for figure\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nplt.style.use('seaborn-whitegrid')\nimport seaborn as sns\nsns.set(color_codes=True, font_scale=1.33)\n# data management\nimport pandas as pd\nimport numpy as np\n\n# utilities\nimport shutil\nimport os\nimport datetime\nimport re\nimport math \nimport json \nimport copy\n# for geo lat\/lon\nfrom geopy.geocoders import Nominatim\n\n# HELPER FUNCTIONS\ndef add_days(str_date_0, nb_days_CV):\n    '''\n    add days to string dates\n    '''\n    date_format = \"%Y-%m-%d\"\n    date_last = datetime.datetime.strptime(str_date_0, date_format)\n    date_start = date_last + datetime.timedelta(days=nb_days_CV)\n    str_date_start = date_start.strftime(date_format)\n    return str_date_start\n\n\n#################\n# DATA WORLD\n#################\n# confirmed cases\nPATH_WORLD_CONF_US = 'https:\/\/raw.githubusercontent.com\/CSSEGISandData\/' + \\\n    'COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/' + \\\n    'time_series_covid19_confirmed_US.csv'\ndf_us = pd.read_csv(PATH_WORLD_CONF_US)\n\n# fix problem no data at all for one date\ndf_us.dropna(axis='columns', how='all', inplace=True)\n\n# adapt col names\ndf_us.rename(columns={\"Province_State\": \"Province\/State\", \n                           \"Country_Region\": \"Country\/Region\",\n                          \"Long_\": \"Long\"}, inplace=True)\n\n# remove lat\/long null\ndf_us = df_us[df_us[\"Lat\"] != 0]\ndf_us = df_us[df_us[\"Long\"] != 0]\n# reduce by USA states\ndict_pivot = {'Lat': np.mean, 'Long': np.mean}\nfor date_curr in df_us.columns[11:]:\n    dict_pivot[date_curr] = np.sum\n    \ndf_us_pivot = pd.pivot_table(df_us, values=['Lat', 'Long'] +\\\n                             df_us.columns[11:].tolist(), \n               index=['Province\/State'],\n                    aggfunc=dict_pivot)\ndf_us_pivot['Province\/State'] = df_us_pivot.index\ndf_us_pivot[\"Country\/Region\"] = \"US\"\ndf_us_pivot = df_us_pivot[[\"Country\/Region\", 'Province\/State', 'Lat', 'Long'] +\\\n            df_us.columns[11:].tolist()]\n\n# re-format with all dates in 1 column\n\ndf_us_melt = df_us_pivot.melt(id_vars=[\"Province\/State\", \"Country\/Region\", \n                                       \"Lat\", \"Long\"],\n                            value_vars=df_us_pivot.columns[4:], \n                            var_name=\"date\", value_name=\"nb_cases\")\n\ndf_us_melt[\"Province\/State\"] = df_us_melt[\"Province\/State\"].fillna(\" \")\n\ndf_us_melt[\"area\"] = df_us_melt[\"Country\/Region\"] + \" : \" +\\\n    df_us_melt[\"Province\/State\"]  \n\ndf_us_melt[\"date\"] = df_us_melt[\"date\"].astype(np.datetime64)\n\ndf_us_melt.sort_values(by=['date'], inplace=True)\n\ndf_us_melt[\"nb_cases\"] = df_us_melt[\"nb_cases\"].fillna(0)\n# path because sometimes, value are negative !\ndf_us_melt[\"nb_cases\"] = df_us_melt[\"nb_cases\"].apply(math.fabs)\n\n","b14b8644":"# confirmed cases\nPATH_WORLD_CONF = 'https:\/\/raw.githubusercontent.com\/CSSEGISandData\/' + \\\n    'COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/' + \\\n    'time_series_covid19_confirmed_global.csv'\ndf_world = pd.read_csv(PATH_WORLD_CONF)\n\n# fix problem no data at all for one date\ndf_world.dropna(axis='columns', how='all', inplace=True)\n\ndf_world_melt = df_world.melt(id_vars=[\"Province\/State\", \"Country\/Region\", \n                                       \"Lat\", \"Long\"],\n                            value_vars=df_world.columns[4:], \n                            var_name=\"date\", value_name=\"nb_cases\")\n\ndf_world_melt[\"Province\/State\"] = df_world_melt[\"Province\/State\"].fillna(\" \")\n\ndf_world_melt[\"area\"] = df_world_melt[\"Country\/Region\"] + \" : \" +\\\n    df_world_melt[\"Province\/State\"]  \n\ndf_world_melt[\"date\"] = df_world_melt[\"date\"].astype(np.datetime64)\n\ndf_world_melt.sort_values(by=['date'], inplace=True)\n\ndf_world_melt[\"nb_cases\"] = df_world_melt[\"nb_cases\"].fillna(0)\n# path because sometimes, value are negative !\ndf_world_melt[\"nb_cases\"] = df_world_melt[\"nb_cases\"].apply(math.fabs)\n\n# remove USA\ndf_world_melt = df_world_melt[df_world_melt[\"Country\/Region\"] != 'US']\n# append USA\ndf_world_melt = df_world_melt.append(df_us_melt, ignore_index=True)\n\ndef extract_data_world(df_world_melt, df_death_melt, str_filter, str_value):\n    '''\n    Extract data cases & death for one country by date from df_world_melt\n    \n    '''\n\n    df_cases = df_world_melt[df_world_melt[str_filter] == str_value]\n    \n\n    s_cases = df_cases.groupby(\"date\")[\"nb_cases\"].sum()\n    df_cases_out = pd.DataFrame(columns=[\"date\", \"nb_cases\"])\n    df_cases_out[\"nb_cases\"] = s_cases.values\n    df_cases_out[\"date\"] = s_cases.index\n    \n    if df_death_melt is not None:\n        df_death = df_death_melt[df_death_melt[str_filter] == str_value]\n        s_death = df_death.groupby(\"date\")[\"nb_death\"].sum()\n        df_death_out = pd.DataFrame(columns=[\"date\", \"nb_death\"])\n        df_death_out[\"nb_death\"] = s_death.values\n        df_death_out[\"date\"] = s_death.index\n        return df_cases_out, df_death_out \n    else:\n        return df_cases_out ","ca3ce97d":"s_world = df_world_melt.groupby(\"date\")[\"nb_cases\"].sum()\n\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n#fig = go.Figure()\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n# Create and style traces\nfig.add_trace(go.Scatter(x=s_world.index, \n                         y=s_world.values,\n                    mode='lines+markers',\n                    line_shape='linear',\n                    connectgaps=True, name=\"Total cases\"))\n# add delta \/ diff data\nfig.add_trace(go.Bar(x=s_world.index[1:], \n                    y=np.diff(s_world.values), name=\"Daily cases\"), \n             secondary_y=True)\n# Edit the layout\nfig.update_layout(legend_orientation=\"h\", legend=dict(x=0, y=1.1))\nfig.update_layout(title='COVID-19 Confirmed cases (World)',\n                   yaxis_title='Total cases')\nfig.update_yaxes(title_text=\"Daily cases\", \n                 range=[0, np.diff(s_world.values).max()*2], secondary_y=True)\nfig.show()\n","cbff4af5":"import plotly.express as px\nimport plotly.graph_objects as go\n\n# limit to last 30 days : \ndf_w_map = df_world_melt[df_world_melt[\"date\"] > \\\n              add_days(df_world_melt[\"date\"].astype(str).max(), -30)].copy()\n\n# add animate bubble\nfig = px.scatter_geo(df_w_map, lat=\"Lat\", lon=\"Long\", color=\"nb_cases\",\n                     range_color=[0, df_w_map[\"nb_cases\"].max()], \n                     text=\"nb_cases\",\n                     hover_name=\"area\", \n                     hover_data = [\"nb_cases\"],\n                     size=df_w_map[\"nb_cases\"].apply(math.sqrt), \n                     size_max=40, \n                     animation_frame=df_w_map[\"date\"].\\\n                         astype(np.datetime64).dt.strftime('%b %d'), \n                     projection=\"natural earth\",\n            title=\"COVID-19 Confirmed cases (Areas Animation)\")\n\n# center map\nfig.update_geos(landcolor = 'rgb(217, 217, 217)', \n                showocean=True, oceancolor=\"LightBlue\")\n                \ngeo_world = dict(scope = 'world',\n                landcolor = 'rgb(217, 217, 217)', \n                showocean=True, \n                oceancolor=\"LightBlue\")\ngeo_africa = dict(scope = 'africa',\n                landcolor = 'rgb(217, 217, 217)', \n                showocean=True, \n                oceancolor=\"LightBlue\")  \ngeo_asia = dict(scope = 'asia',\n                landcolor = 'rgb(217, 217, 217)', \n                showocean=True, \n                oceancolor=\"LightBlue\")  \ngeo_europe = dict(scope = 'europe',\n                landcolor = 'rgb(217, 217, 217)', \n                showocean=True, \n                oceancolor=\"LightBlue\")    \ngeo_usa = dict(scope = 'north america',\n                landcolor = 'rgb(217, 217, 217)', \n                showocean=True, \n                oceancolor=\"LightBlue\")  \ngeo_sa = dict(scope = 'south america',\n                landcolor = 'rgb(217, 217, 217)', \n                showocean=True, \n                oceancolor=\"LightBlue\")  \n\nfig.update_layout(\n    updatemenus=[\n        dict(\n            type=\"buttons\",\n            active=0,\n            xanchor=\"left\",\n            y=0.2,\n            buttons=list([\n                dict(label=\"Play\",\n                          method=\"animate\",\n                          args=[None]),\n                dict(label=\"World\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_world}]),\n                dict(label=\"World\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_world}]),\n                dict(label=\"Africa\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_africa}]),\n                dict(label=\"Asia\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_asia}]),\n                dict(label=\"Europe\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_europe}]),\n                dict(label=\"S. Am.\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_sa}]),\n                dict(label=\"USA\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_usa}]),\n            ]),\n        )\n    ])\n\nfig.update_layout(height=500, margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n\n'''fig['data'][0].update(\n    hovertemplate='<b>%{hovertext}<\/b><br><br>date=%{animation_frame}<br>nb_cases=%{marker.color}')'''\nfig.show()","7aac8806":"# confirmed cases\nURL_W_DEATH_US = 'https:\/\/raw.githubusercontent.com\/CSSEGISandData\/' + \\\n    'COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/' + \\\n    'time_series_covid19_deaths_US.csv'\ndf_death_us = pd.read_csv(URL_W_DEATH_US)\n\n# fix problem no data at all for one date\ndf_death_us.dropna(axis='columns', how='all', inplace=True)\n\n# adapt col names\ndf_death_us.rename(columns={\"Province_State\": \"Province\/State\", \n                           \"Country_Region\": \"Country\/Region\",\n                          \"Long_\": \"Long\"}, inplace=True)\n\n# remove lat\/long null\ndf_death_us = df_death_us[df_death_us[\"Lat\"] != 0]\ndf_death_us = df_death_us[df_death_us[\"Long\"] != 0]\n\n# reduce by USA states\ndict_pivot = {'Lat': np.mean, 'Long': np.mean}\nfor date_curr in df_death_us.columns[12:]:\n    dict_pivot[date_curr] = np.sum\n    \ndf_us_pivot = pd.pivot_table(df_death_us, values=['Lat', 'Long'] +\\\n                             df_death_us.columns[12:].tolist(), \n               index=['Province\/State'],\n                    aggfunc=dict_pivot)\ndf_us_pivot['Province\/State'] = df_us_pivot.index\ndf_us_pivot[\"Country\/Region\"] = \"US\"\ndf_us_pivot = df_us_pivot[[\"Country\/Region\", 'Province\/State', 'Lat', 'Long'] +\\\n            df_death_us.columns[12:].tolist()]\n\n\n# re-format date in one column\ndf_death_us_melt = df_us_pivot.melt(id_vars=[\"Province\/State\", \"Country\/Region\", \n                                       \"Lat\", \"Long\"],\n                            value_vars=df_us_pivot.columns[12:], \n                            var_name=\"date\", value_name=\"nb_death\")\n\ndf_death_us_melt[\"Province\/State\"] = df_death_us_melt[\"Province\/State\"].\\\n    fillna(\" \")\n\ndf_death_us_melt[\"area\"] = df_death_us_melt[\"Country\/Region\"] + \" : \" +\\\n    df_death_us_melt[\"Province\/State\"]  \n\ndf_death_us_melt[\"date\"] = df_death_us_melt[\"date\"].astype(np.datetime64)\n\ndf_death_us_melt.sort_values(by=['date'], inplace=True)\n\ndf_death_us_melt[\"nb_death\"] = df_death_us_melt[\"nb_death\"].fillna(0)\n# path because sometimes, value are negative !\ndf_death_us_melt[\"nb_death\"] = df_death_us_melt[\"nb_death\"].apply(math.fabs)","470d1764":"# Deaths\nURL_W_DEATH = 'https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/' + \\\n    '\/master\/csse_covid_19_data\/'  + \\\n    'csse_covid_19_time_series\/time_series_covid19_deaths_global.csv'\ndf_death = pd.read_csv(URL_W_DEATH)\n  \n# fix problem no data at all for one date\ndf_death.dropna(axis='columns', how='all', inplace=True)\n\ndf_death_melt = df_death.melt(id_vars=[\"Province\/State\", \"Country\/Region\", \n                                       \"Lat\", \"Long\"],\n                            value_vars=df_death.columns[4:], \n                            var_name=\"date\", value_name=\"nb_death\")\n\ndf_death_melt[\"Province\/State\"] = df_death_melt[\"Province\/State\"].fillna(\" \")\n\ndf_death_melt[\"area\"] = df_death_melt[\"Country\/Region\"] + \" : \" +\\\n    df_death_melt[\"Province\/State\"]  \n\ndf_death_melt[\"date\"] = df_death_melt[\"date\"].astype(np.datetime64)#.dt.strftime('%b %d')\n\n# remove USA\ndf_death_melt = df_death_melt[df_death_melt[\"Country\/Region\"] != 'US']\n# append USA\ndf_death_melt = df_death_melt.append(df_death_us_melt, ignore_index=True)\n\n\ndf_death_melt.sort_values(by=['date'], inplace=True)\n\ndf_death_melt[\"nb_death\"] = df_death_melt[\"nb_death\"].fillna(0)\n# path because sometimes, value are negative !\ndf_death_melt[\"nb_death\"] = df_death_melt[\"nb_death\"].apply(math.fabs)","cea6aac0":"s_death = df_death_melt.groupby(\"date\")[\"nb_death\"].sum()\n\nimport plotly.graph_objects as go\n\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n# Create and style traces\nfig.add_trace(go.Scatter(x=s_death.index, \n                         y=s_death.values,\n                    mode='lines+markers',\n                    line_shape='linear',\n                    connectgaps=True, name=\"Total death\"))\n# add delta \/ diff data\nfig.add_trace(go.Bar(x=s_death.index[1:], \n                    y=np.diff(s_death.values), name=\"Daily death\"),\n             secondary_y=True)\n# Edit the layout\nfig.update_layout(legend_orientation=\"h\", legend=dict(x=0, y=1.1))\nfig.update_layout(title='COVID-19 Death cases (World)',\n                   yaxis_title='Total death')\nfig.update_yaxes(title_text=\"Daily death\", \n                 range=[0, np.diff(s_death.values).max()*2], secondary_y=True)\nfig.show()","fb017c63":"import plotly.express as px\nimport plotly.graph_objects as go\n# limit to last 30 days : \ndf_w_d_map = df_death_melt[df_death_melt[\"date\"] > \\\n              add_days(df_death_melt[\"date\"].astype(str).max(), -30)].copy()\n# add animate bubble\nfig = px.scatter_geo(df_w_d_map, lat=\"Lat\", lon=\"Long\", color=\"nb_death\",\n                     range_color=[0, df_w_d_map[\"nb_death\"].max()], \n                     text=\"nb_death\",\n                     hover_name=\"area\", \n                     hover_data = [\"nb_death\"],\n                     size=\"nb_death\", \n                     size_max=80, \n                     animation_frame=df_w_d_map[\"date\"].\\\n                         astype(np.datetime64).dt.strftime('%b %d'), \n                     projection=\"natural earth\",\n            title=\"COVID-19 Death cases (Areas Animation)\")\n\n# center map\nfig.update_geos(landcolor = 'rgb(217, 217, 217)', \n                showocean=True, oceancolor=\"LightBlue\")\n# buttons                \nfig.update_layout(\n    updatemenus=[\n        dict(\n            type=\"buttons\",\n            active=0,\n            xanchor=\"left\",\n            y=0.2,\n            buttons=list([\n                dict(label=\"Play\",\n                          method=\"animate\",\n                          args=[None]),\n                dict(label=\"World\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_world}]),\n                dict(label=\"World\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_world}]),\n                dict(label=\"Africa\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_africa}]),\n                dict(label=\"Asia\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_asia}]),\n                dict(label=\"Europe\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_europe}]),\n                dict(label=\"S. Am.\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_sa}]),\n                dict(label=\"USA\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_usa}]),\n            ]),\n        )\n    ])\n\nfig.update_layout(height=500, margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n\n'''fig['data'][0].update(\n    hovertemplate='<b>%{hovertext}<\/b><br><br>date=%{animation_frame}<br>nb_cases=%{marker.color}')'''\nfig.show()","817abd61":"# Recovered cases\nURL_W_REC = 'https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19' + \\\n    '\/master\/csse_covid_19_data\/csse_covid_19_time_series' + \\\n    '\/time_series_covid19_recovered_global.csv'\n\ndf_rec = pd.read_csv(URL_W_REC)\n    \n# fix problem no data at all for one date\ndf_rec.dropna(axis='columns', how='all', inplace=True)\n\ndf_rec_melt = df_rec.melt(id_vars=[\"Province\/State\", \"Country\/Region\", \n                                       \"Lat\", \"Long\"],\n                            value_vars=df_rec.columns[4:], \n                            var_name=\"date\", value_name=\"nb_recovered\")\n\ndf_rec_melt[\"Province\/State\"] = df_rec_melt[\"Province\/State\"].fillna(\" \")\n\ndf_rec_melt[\"area\"] = df_rec_melt[\"Country\/Region\"] + \" : \" +\\\n    df_rec_melt[\"Province\/State\"] \n\ndf_rec_melt[\"date\"] = df_rec_melt[\"date\"].astype(np.datetime64)#.dt.strftime('%b %d')\n\ndf_rec_melt.sort_values(by=['date'], inplace=True)\n\ndf_rec_melt[\"nb_recovered\"] = df_rec_melt[\"nb_recovered\"].fillna(0)\n# path because sometimes, value are negative !\ndf_rec_melt[\"nb_recovered\"] = df_rec_melt[\"nb_recovered\"].apply(math.fabs)","f3eb8ef8":"import plotly.express as px\nimport plotly.graph_objects as go\n# limit to last 30 days : \ndf_w_r_map = df_rec_melt[df_rec_melt[\"date\"] > \\\n              add_days(df_rec_melt[\"date\"].astype(str).max(), -30)].copy()\n# add animate bubble\nfig = px.scatter_geo(df_w_r_map, lat=\"Lat\", lon=\"Long\", color=\"nb_recovered\",\n                     range_color=[0, df_w_r_map[\"nb_recovered\"].max()], \n                     text=\"nb_recovered\",\n                     hover_name=\"area\", \n                     hover_data = [\"nb_recovered\"],\n                     size=df_w_r_map[\"nb_recovered\"].apply(math.sqrt), \n                     size_max=80, \n                     animation_frame=df_w_r_map[\"date\"].\\\n                         astype(np.datetime64).dt.strftime('%b %d'), \n                     projection=\"natural earth\",\n            title=\"COVID-19 Recovered cases (Areas Animation)\")\n\n# center map\nfig.update_geos(landcolor = 'rgb(217, 217, 217)', \n                showocean=True, oceancolor=\"LightBlue\")\n# buttons               \nfig.update_layout(\n    updatemenus=[\n        dict(\n            type=\"buttons\",\n            active=0,\n            xanchor=\"left\",\n            y=0.2,\n            buttons=list([\n                dict(label=\"Play\",\n                          method=\"animate\",\n                          args=[None]),\n                dict(label=\"World\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_world}]),\n                dict(label=\"World\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_world}]),\n                dict(label=\"Africa\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_africa}]),\n                dict(label=\"Asia\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_asia}]),\n                dict(label=\"Europe\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_europe}]),\n                dict(label=\"S. Am.\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_sa}]),\n                dict(label=\"USA\",\n                     method=\"relayout\",\n                     args=[{\"geo\": geo_usa}]),\n            ]),\n        )\n    ])\n\nfig.update_layout(height=500, margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n\n'''fig['data'][0].update(\n    hovertemplate='<b>%{hovertext}<\/b><br><br>date=%{animation_frame}<br>nb_cases=%{marker.color}')'''\nfig.show()","15b8bd58":"s_rec = df_rec_melt.groupby(\"date\")[\"nb_recovered\"].sum()\n\nnb_date = min(s_world.shape[0],s_rec.shape[0])\ns_rec = s_rec[:nb_date]\ns_world = s_world[:nb_date]\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\n# Create and style traces\n\nfig.add_trace(go.Scatter(x=s_world.index, \n                         y=s_world.values,\n                    mode='lines+markers',\n                    name=\"confirmed\",\n                    line_shape='linear',\n                    connectgaps=True))\n\nfig.add_trace(go.Scatter(x=s_rec.index, \n                         y=s_world.values - s_rec.values,\n                    mode='lines+markers',\n                    name=\"active\",\n                    line_shape='linear',\n                    connectgaps=True))\n\nfig.add_trace(go.Scatter(x=s_rec.index, \n                         y=s_rec.values,\n                    mode='lines+markers',\n                    name=\"recovered\",\n                    line_shape='linear',\n                    connectgaps=True))\n\n# add delta \/ diff data\nfig.add_trace(go.Bar(x=s_world.index[1:], \n                     y=np.diff(s_world.values - s_rec.values), \n                     name=\"delta active\"))\n\n# Edit the layout\nfig.update_layout(title='COVID-19 Actived cases (World)',\n                   yaxis_title='nb cases')\nfig.show()","dce8f635":"%%capture\n########\n# Import\n#\n%matplotlib inline\n\n# for figure\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nplt.style.use('seaborn-whitegrid')\nimport seaborn as sns\nsns.set(color_codes=True, font_scale=1.33)\n# data management\nimport pandas as pd\nimport numpy as np\n\n# utilities\nimport shutil\nimport os\nimport datetime\nimport re\nimport math \nimport json \nimport copy\n# for geo lat\/lon\nfrom geopy.geocoders import Nominatim\n\n\ntry:\n    import scrapy\nexcept:\n    !pip install scrapy\n    import scrapy\n    \nimport scrapy.crawler as crawler\nfrom multiprocessing import Process, Queue\nfrom twisted.internet import reactor\n\n###############\n## Definitions\n#\n\npd.options.display.max_rows = 100\n# your path folder to save results\nPATH_FOLDER_SAVE = '..\/..\/data'\n\nPATH_CSV_DATA = PATH_FOLDER_SAVE + '\/data.csv'\nPATH_CSV_DF_GEO_KOR = 'df_geo_kor.csv'\n\nPATH_PAGES_KCDC_UPDATES = os.getcwd() + '\/pages_kcdc_updates.json'\nPATH_PAGES_KCDC_UPDATES_TMP = os.getcwd() + '\/pages_kcdc_updates_temp.json' \nPATH_TABLES_KCDC_UPDATES = os.getcwd() + '\/tables_kcdc_updates.json'\nPATH_TABLES_KCDC_UPDATES_TMP = os.getcwd() + '\/tables_kcdc_updates_temp.json' \nPATH_CSV_DF_KCDC = os.getcwd() + '\/df_kcdc.csv'\nFLAG_CSV_KCDC = os.path.isfile(PATH_CSV_DF_KCDC)\nMODE_SCRAPY_KCDC_EN = not(FLAG_CSV_KCDC) # if CSV exists, \n# don't try to scrap again ENglish version\n\nPATH_PAGES_KCDC_UPDATES_KR = os.getcwd() + '\/pages_kcdc_updates_kr.json'\nPATH_PAGES_KCDC_UPDATES_KR_TMP = os.getcwd() + \\\n    '\/pages_kcdc_updates_kr_temp.json' \nPATH_TABLES_KCDC_UPDATES_KR = os.getcwd() + '\/tables_kcdc_updates_kr.json'\nPATH_TABLES_KCDC_UPDATES_KR_TMP = os.getcwd() + \\\n    '\/tables_kcdc_updates_kr_temp.json' \nMODE_SCRAPY_KCDC_KR = True # scrap korean version if available\n\nPATH_QA_KCDC = os.getcwd() + '\/train_data_qa_kcdc.json'\n\nLIST_AREA = [\"Seoul\",\n\"Busan\",\n\"Daegu\",\n\"Incheon\",\n\"Gwangju\",\n\"Daejeon\",\n\"Ulsan\",\n\"Sejong\",\n\"Gyeonggi\",\n\"Gangwon\",\n\"Chungbuk\",\n\"Chungnam\",\n\"Jeonbuk\",\n\"Jeonnam\",\n\"Gyeongbuk\",\n\"Gyeongnam\",\n\"Jeju\"]\n\nDICT_AREA_SPECIAL = {'Gyeonggi': [\"Gyeong-\", \"gi\"],\n                    'Gangwon':[\"Gang-\", \"won\"],\n                    'Chungbuk':[\"Chung-\", \"buk\"],\n                    'Chungnam':[\"Chung-\", \"nam\"],\n                    'Jeonbuk': [\"Jeon-\", \"buk\"],\n                    'Jeonnam': [\"Jeon-\", \"nam\"],\n                    'Gyeongbuk': [\"Gyeong-\", \"buk\"],\n                    'Gyeongnam': [\"Gyeong-\", \"nam\"],\n                    'Daejeon' : [\"Dae-\", \"jeon\"]}\n\nDICT_AREA_SPECIAL_2 ={'Gyeonggi': [\"Gyeong\", \"gi\"],\n                    'Gangwon':[\"Gang\", \"won\"],\n                    'Chungbuk':[\"Chung\", \"buk\"],\n                    'Chungnam':[\"Chung\", \"nam\"],\n                    'Jeonbuk': [\"Jeon\", \"buk\"],\n                    'Jeonnam': [\"Jeon\", \"nam\"],\n                    'Gyeongbuk': [\"Gyeong\", \"buk\"],\n                    'Gyeongnam': [\"Gyeong\", \"nam\"],\n                    'Daejeon' : [\"Dae\", \"jeon\"]}\n\nLIST_STR_TO_FIND = [\"COVID\",\n                    \"Coronavirus\",\n                    \"coronavirus\",\n                   \"additional cases\"]\n\n##################\n# Helper functions\n#\n\n# save before scraping\ndef clean_file(path_file_name):\n    '''\n    Clean file already traited : rename file with date\n    '''\n    try:\n        d = datetime.datetime.now()\n        str_date = '_' + d.strftime(\"%Y%m%d_%H_%M_%S\")\n       \n        res_re = re.search('\\.\\w+$', path_file_name)\n        \n        path_file_name_saved = \\\n            path_file_name[0:res_re.start()] + str_date + res_re.group(0)\n         \n        shutil.move(path_file_name, path_file_name_saved) \n        print('File {} moved!'.format(path_file_name_saved))\n    except:\n        print('File {} does not exist!'.format(path_file_name))\n\n\n# the wrapper to make it run more times\ndef run_spider(spider):\n    '''\n    DOESNT WORK WITH LAST VERSION OF SCRAPY OR TWISTED ??? \n    BUG\n    function to run several times scraping process\n    '''\n    def f(q):\n        try:\n            runner = crawler.CrawlerRunner()\n            deferred = runner.crawl(spider)\n            deferred.addBoth(lambda _: reactor.stop())\n            reactor.run()\n            q.put(None)\n        except Exception as e:\n            q.put(e)\n\n    q = Queue()\n    p = Process(target=f, args=(q,))\n    p.start()\n    result = q.get()\n    p.join()\n\n    if result is not None:\n        raise result\n        \n\ndef convert_int(val_curr):\n    '''\n    To convert in integer if not NaN\n    '''\n    if np.isnan(val_curr):\n        return val_curr\n    else:\n        return int(val_curr)\n    \n\ndef correct_addtive_value_2(df_in):\n    ''' \n    Try to find \"add-format\" data if \"total-format\" data DOES NOT EXIST\n     - if only additive : try to extrapolate from day after\n     - loop over days from most recent to oldest\n    '''\n    \n    df_kcdc = df_in.copy()\n    \n    df_kcdc_out = df_kcdc.groupby(\"date_published\").max()\n    list_day  = df_kcdc_out.index.unique().tolist()\n    \n    for I_day in range(len(list_day)-1, -1, -1):\n        day_curr = list_day[I_day]\n        #print(day_curr)\n        df_add = df_kcdc[\\\n            (df_kcdc[\"date_published\"] == day_curr) & \\\n            (df_kcdc[\"flag_add\"] == True)]. \\\n            groupby(\"date_published\").max()\n\n        df_tot = df_kcdc[\\\n            (df_kcdc[\"date_published\"] == day_curr) & \\\n            (df_kcdc[\"flag_add\"] == False)]. \\\n            groupby(\"date_published\").max()\n        \n        #print(df_kcdc_out.at[day_curr, \"Seoul\"])\n        \n        if pd.notna(df_kcdc_out.at[day_curr, \"Seoul\"]) & \\\n           (df_kcdc_out.at[day_curr, \"flag_add\"] == False):\n            continue\n        \n        if df_add.shape[0] == 0:\n            print(\"ERROR : no add data for day : \", day_curr) \n            continue\n            \n        # if only add part found, subtract from day after in final df\n        if pd.notna(df_add.at[day_curr, \"Seoul\"]):\n            \n            day_curr_after = list_day[I_day+1]\n            \n            df_kcdc_out.loc[day_curr, LIST_AREA] = \\\n                    df_kcdc_out.loc[day_curr_after, LIST_AREA] - \\\n                    df_add.loc[day_curr, LIST_AREA]\n            print(\"Correction OK for day : \", day_curr) \n            \n        \n    return df_kcdc_out\n\ndef correct_interpol_value(df_in):\n    # interpolation between before - after days : \n    # loop over days\n    list_day = df_in.index\n    df_kcdc = df_in.copy()\n    list_col = LIST_AREA.copy()\n    list_col.append(\"total\")\n    for I_day in range(1, len(list_day)-1):\n        day_curr = list_day[I_day]\n        if pd.isna(df_kcdc.at[day_curr, \"total\"]):\n            print(\"Correction needed for day : \", day_curr)\n            day_curr_before = list_day[I_day-1]\n            day_curr_after = list_day[I_day+1]\n            \n            # Value day curr = mean(value before & after)\n            df_kcdc.loc[day_curr, list_col] = \\\n                (0.5*df_kcdc.loc[day_curr_before, list_col] + \\\n                0.5*df_kcdc.loc[day_curr_after, list_col]).astype(np.int64, \n                                                             errors='ignore')\n        if pd.isna(df_kcdc.at[day_curr, \"Seoul\"]):\n            print(\"Correction needed for day (Seoul = nan): \", day_curr)\n            day_curr_before = list_day[I_day-1]\n            day_curr_after = list_day[I_day+1]\n            \n            # Value day curr = mean(value before & after)\n            df_kcdc.loc[day_curr, LIST_AREA] = \\\n                (0.5*df_kcdc.loc[day_curr_before, LIST_AREA] + \\\n                0.5*df_kcdc.loc[day_curr_after, LIST_AREA]).astype(np.int64, \n                                                             errors='ignore')\n    for area_curr in list_col:\n        df_kcdc[area_curr] = df_kcdc[area_curr].apply(convert_int)\n    return df_kcdc\n\ndef correct_data_date_range(df_kcdc_corr): \n    '''\n    Correct date range of data\n    - interpolate if missing dates\n    '''\n    #### Correction time continuity\n    # if no row for a day, add a row by linear interpolation needed\n    range_days = pd.date_range(start=df_kcdc_corr.index[0], \n                                end=df_kcdc_corr.index[-1])\n\n    df_kcdc_corr.index = df_kcdc_corr.index.astype('datetime64[ns]')\n\n    df_kcdc_corr = df_kcdc_corr.reindex(range_days)\n\n\n    df_kcdc_interp = correct_interpol_value(df_kcdc_corr) \n\n    # add date as column\n    df_kcdc_interp[\"date\"] = df_kcdc_interp.index\n    \n    return df_kcdc_interp\n\ndef export_kcdc_csv(df_kcdc_interp, path_csv=PATH_CSV_DF_KCDC):\n    '''\n    Re-order columns and select columns and save in CSV\n    '''\n    list_col = df_kcdc_interp.columns.tolist()\n    list_col.remove(\"flag_add\")\n    list_col.remove(\"body\")\n    list_col.remove(\"source\")\n    list_col.remove(\"date\")\n    list_col.insert(0,\"date\")\n    list_col.remove(\"total\")\n    list_col.insert(1,\"total\")\n    list_col.remove(\"url\")\n    list_col.append(\"url\")\n    df_kcdc_interp = df_kcdc_interp.filter(list_col)\n    \n    clean_file(path_csv)\n    \n    df_kcdc_interp.to_csv(path_csv, index=False)\n\ndef load_kcdc_csv(path_csv=PATH_CSV_DF_KCDC):\n    df_kcdc_interp = pd.read_csv(path_csv)\n    df_kcdc_interp[\"date\"] = df_kcdc_interp[\"date\"].astype('datetime64[ns]')\n    df_kcdc_interp.index = df_kcdc_interp[\"date\"].tolist()\n    return df_kcdc_interp\n\ndef clean_kcdc_date(df_tables_kcdc_updates):\n    ################\n    ### clean dates\n    #\n    #### Keep only row with at least one non-Nan value in Total or Areas\n    list_col = LIST_AREA.copy()\n    list_col.append(\"total\")\n    df_tables_kcdc_updates.dropna(how='all', inplace=True, subset=list_col)\n    print(\"Nb articles left : \", df_tables_kcdc_updates.shape[0])\n    return df_tables_kcdc_updates","cfd04a45":"%%capture\n%%writefile kcdcspider.py\n# -*-coding:utf-8 -*\n\n#\n# Class to scrap KCDC Tables values for each town\/area in South Korea\n#\n\n#import\nimport re\nimport scrapy\n\n# definitions\nURL_ROOT = 'http:\/\/www.kdca.go.kr\/'\nNUM_MAX_PAGES = 100\nPATH_OUTPUT = 'pages_kcdc_updates_temp.json'\nURL_FIRST_PAGE = URL_ROOT + '\/board.es?mid=a30402000000&bid=0030&nPage=1'\n\nLIST_STR_TO_FIND = [\"COVID\",\n                    \"Coronavirus\",\n                    \"coronavirus\",\n                   \"additional cases\"]\n\n# Spider Class\nclass KCDCPageSpider(scrapy.Spider):\n    '''\n    Spider to scrap all KCDC pages press articles\n    Configure : \n    - KCDCPageSpider.custom_settings : save location \n    - num_max_pages : the number of next page to scrap\n    - url_first_page : web page to start with\n    '''\n    name = \"kcdc_updates_page\"\n    \n    custom_settings = {\n      'FEED_FORMAT': 'json',\n      'FEED_URI': PATH_OUTPUT\n    }\n    \n    num_max_pages = NUM_MAX_PAGES\n    \n    url_first_page = URL_FIRST_PAGE\n    \n    def start_requests(self):\n        urls = [\n                self.url_first_page,\n        ]\n        for url in urls:\n            yield scrapy.Request(url=url, callback=self.parse)\n    \n    def parse(self, response):\n        str_pattern = '\/\/div[@class=\"dbody\"]\/ul\/li\/a[contains(@title,\"{}\")]'\n        str_search = ''\n        for str_curr in LIST_STR_TO_FIND:\n            str_search = str_search + str_pattern.format(str_curr) + '|'\n        str_search = str_search[0:-1]\n        #'\/\/div[@class=\"dbody\"]\/ul\/li\/a[contains(@title,\"updates\")]|' + \\\n        #    '\/\/div[@class=\"dbody\"]\/ul\/li\/a[contains(@title,\"Updates\")]'\n        flag_end = False\n        for post in response.xpath(str_search):\n            link_curr = post.xpath('@href').extract_first()\n            # check if page is article of 2020-01-30 : 'list_no=365901'\n            # to stop scan pages up to 2020-01-30\n            if re.search('&list_no=365901&', link_curr):\n                flag_end = True\n            yield {\n                'link': URL_ROOT + link_curr\n            }\n        \n        # https:\/\/www.cdc.go.kr\/board.es?mid=a30402000000&bid=0030&nPage=2\n        #\/\/a[@class=\"pageNext\"]\/@href\n        next_page = response.xpath('\/\/a[@class=\"pageNext\"]\/@href').get()\n        \n        if (next_page is not None) & (flag_end==False):\n            try:\n                num_next_page = int(re.search(\"(?<=\\&nPage\\=)\\d+$\", \n                         next_page).group(0))\n                \n                if (num_next_page < self.num_max_pages):\n                    #next_page = response.urljoin(next_page)\n                    next_page = URL_ROOT + next_page\n                    yield scrapy.Request(next_page, callback=self.parse)\n            except:\n                next_page = None","4af4b940":"%%capture\n%%writefile kcdcspiderKR.py\n# -*-coding:utf-8 -*\n\n#\n# Class to scrap KCDC Tables values for each town\/area in South Korea\n# KOREAN VERSION\n#\n\n#import\nimport re\nimport scrapy\n\n# definitions\nURL_ROOT = 'http:\/\/www.kdca.go.kr\/'\nNUM_MAX_PAGES = 100\nPATH_OUTPUT = 'pages_kcdc_updates_kr_temp.json'\n\n#URL_FIRST_PAGE = URL_ROOT + '\/board.es?mid=a20501000000&bid=0015&nPage=1'\nURL_FIRST_PAGE = URL_ROOT + '\/board.es?mid=a20501010000&bid=0015&nPage=1'\n\nPAGE_ID_STOP = '368440'\n\nLIST_STR_TO_FIND = [\"\ucf54\ub85c\ub098\ubc14\uc774\ub7ec\uc2a4\uac10\uc5fc\uc99d-19\", \"\ucf54\ub85c\ub09819 \uad6d\ub0b4 \ubc1c\uc0dd\"]\n\n# Spider Class\nclass KCDCPageSpider(scrapy.Spider):\n    '''\n    Spider to scrap all KCDC pages press articles\n    Configure : \n    - KCDCPageSpider.custom_settings : save location \n    - num_max_pages : the number of next page to scrap\n    - url_first_page : web page to start with\n    '''\n    name = \"kcdc_updates_page\"\n    \n    custom_settings = {\n      'FEED_FORMAT': 'json',\n      'FEED_URI': PATH_OUTPUT\n    }\n    \n    num_max_pages = NUM_MAX_PAGES\n    \n    url_first_page = URL_FIRST_PAGE\n\n    \n    def __init__(self, page_id_stop=PAGE_ID_STOP):\n        self.page_id_stop = page_id_stop\n        print(\"self.page_id_stop = \", self.page_id_stop)\n\n    def start_requests(self):\n        urls = [\n                self.url_first_page,\n        ]\n        for url in urls:\n            yield scrapy.Request(url=url, callback=self.parse)\n    \n    def parse(self, response):\n        str_pattern = '\/\/div[@class=\"dbody\"]\/ul\/li\/a[contains(@title,\"{}\")]'\n        str_search = ''\n        for str_curr in LIST_STR_TO_FIND:\n            str_search = str_search + str_pattern.format(str_curr) + '|'\n        str_search = str_search[0:-1]\n        #'\/\/div[@class=\"dbody\"]\/ul\/li\/a[contains(@title,\"updates\")]|' + \\\n        #    '\/\/div[@class=\"dbody\"]\/ul\/li\/a[contains(@title,\"Updates\")]'\n        flag_end = False\n        for post in response.xpath(str_search):\n            link_curr = post.xpath('@href').extract_first()\n            # to stop scan pages up to 2020-09-22 = article n\u00b0 368440\n            if re.search(f'&list_no={self.page_id_stop}&', link_curr):\n                flag_end = True\n            yield {\n                'link': URL_ROOT + link_curr\n            }\n        \n        # https:\/\/www.cdc.go.kr\/board.es?mid=a30402000000&bid=0030&nPage=2\n        #\/\/a[@class=\"pageNext\"]\/@href\n        next_page = response.xpath('\/\/a[@class=\"pageNext\"]\/@href').get()\n        \n        if (next_page is not None) & (flag_end==False):\n            try:\n                num_next_page = int(re.search(\"(?<=\\&nPage\\=)\\d+$\", \n                         next_page).group(0))\n                \n                if (num_next_page < self.num_max_pages):\n                    #next_page = response.urljoin(next_page)\n                    next_page = URL_ROOT + next_page\n                    yield scrapy.Request(next_page, callback=self.parse)\n            except:\n                next_page = None\n","4f20a3f3":"%%capture\n%%writefile run_kcdc_pages_updates.py\n# -*-coding:utf-8 -*\n\n#\n# Run scrapy for retrieve updates pages links from KCDC\n#\n\n# import\n\n# built-in\nimport os \n# third party libs\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom kcdcspider import KCDCPageSpider\n\n# definition \nPATH_OUTPUT = os.getcwd() + '\/pages_kcdc_updates_temp.json'\n\n# remove old is exist\nif os.path.isfile(PATH_OUTPUT):\n    os.remove(PATH_OUTPUT)\n\nprocess = CrawlerProcess()\n\nprocess.crawl(KCDCPageSpider)\nprocess.start() # the script will block here until the crawling is finished","bd865a54":"%%capture\n%%writefile run_kcdc_pages_updates_kr.py\n# -*-coding:utf-8 -*\n\n#\n# Run scrapy for retrieve updates pages links from KCDC\n# KOREAN VERSION\n#\n\n# import\n\n# built-in\nimport os\nimport sys\n# third party libs\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom kcdcspiderKR import KCDCPageSpider # KOREAN VERSION\n\n# definition \nPATH_OUTPUT = os.getcwd() + '\/pages_kcdc_updates_kr_temp.json'\n\n# remove old is exist\nif os.path.isfile(PATH_OUTPUT):\n    os.remove(PATH_OUTPUT)\n\nprocess = CrawlerProcess()\n\n\nif len(sys.argv) > 1:\n    print(\"sys.argv : \", sys.argv)\n    page_id_stop = sys.argv[1]\nelse:\n    page_id_stop = None\n\nprint(\"page_id_stop : \", page_id_stop)\n\nif page_id_stop is None:\n    print(\"no args\")\n    process.crawl(KCDCPageSpider)\nelse:\n    #process.crawl(KCDCPageSpider, page_id_stop=\"711076\")\n    print(\"args : page_id_stop : \", page_id_stop)\n    process.crawl(KCDCPageSpider, page_id_stop=page_id_stop)\n\nprocess.start() # the script will block here until the crawling is finished","16d4c9de":"%%capture capture_out\n%%time\n\n####################################\n# South Korea : Scrap Data from KCDC\n#\n\n####################\n## Scrap pages links\n#\n\ntry:\n    if MODE_SCRAPY_KCDC_EN:\n        # clean (move file if exist)\n        clean_file(PATH_PAGES_KCDC_UPDATES)\n        # execute external process (FALLBACK for scrapy problem)\n        !python run_kcdc_pages_updates.py \n        # retrieve new data into file\n        shutil.copyfile(PATH_PAGES_KCDC_UPDATES_TMP, PATH_PAGES_KCDC_UPDATES)\nexcept:\n    print(\"Error run_kcdc_pages_updates !\")","6d22a528":"%%capture\nwith open('run_kcdc_pages_updates_log.txt',\"w\") as f:\n    f.write(capture_out.stdout)","14267b58":"%%capture capture_out\n%%time\n\n####################################\n# South Korea : Scrap Data from KCDC\n# KOREAN VERSION\n\n####################\n## Scrap pages links\n#\n\n#str_ = \"ipynb\"\n#str_cmd = f\"ls *.{str_}\"\n#!{str_cmd}\n\ntry:\n    if MODE_SCRAPY_KCDC_KR:\n        # check if CSV exist\n        if FLAG_CSV_KCDC:\n            df_kcdc_interp = load_kcdc_csv()\n            idx_nan = df_kcdc_interp[\"total\"].isna()\n            if df_kcdc_interp[idx_nan].shape[0] > 0:\n                last_date_ok = add_days(df_kcdc_interp[idx_nan][\"date\"].min(). \\\n                                        strftime(\"%Y-%m-%d\"), -1)\n            else:\n                last_date_ok = df_kcdc_interp[\"date\"].max()\n\n            url_last_date = df_kcdc_interp[df_kcdc_interp[\"date\"] == \\\n                                           last_date_ok][\"url\"][0]\n\n            page_id_stop = re.search(r\"(?<=list\\_no\\=)\\d+\", url_last_date)[0]\n            str_cmd = f\"python run_kcdc_pages_updates_kr.py {page_id_stop}\"\n        else:\n            str_cmd = \"python run_kcdc_pages_updates_kr.py\"\n        \n        # clean (move file if exist)\n        clean_file(PATH_PAGES_KCDC_UPDATES_KR)\n        \n        # execute external process (FALLBACK for scrapy problem)\n        #!python run_kcdc_pages_updates_kr.py\n        !{str_cmd}\n        \n        # retrieve new data into file\n        shutil.copyfile(PATH_PAGES_KCDC_UPDATES_KR_TMP, \n                        PATH_PAGES_KCDC_UPDATES_KR)\nexcept:\n    print(\"Error run_kcdc_pages_updates_kr !\")","5b22a101":"%%capture\nwith open('run_kcdc_pages_updates_kr_log.txt',\"w\") as f:\n    f.write(capture_out.stdout)","7c23971d":"if MODE_SCRAPY_KCDC_EN:\n    df_pages_kcdc_updates = pd.read_json(PATH_PAGES_KCDC_UPDATES)\n    print(\"Nb South Korea news (EN): \", df_pages_kcdc_updates.shape[0])","e3d97370":"if MODE_SCRAPY_KCDC_KR:\n    df_pages_kcdc_updates_kr = pd.read_json(PATH_PAGES_KCDC_UPDATES_KR)\n    print(\"Nb South Korea news (KR): \", df_pages_kcdc_updates_kr.shape[0])","101fb772":"%%capture\n%%writefile kcdctablespider.py\n# -*-coding:utf-8 -*\n\n#\n# Module for scrapy over tables in KCDC\n#\n\n# import\nimport re\nimport scrapy\nfrom scrapy.loader import ItemLoader\nfrom scrapy.loader.processors import MapCompose, Join, TakeFirst\nfrom w3lib.html import remove_tags, replace_tags, replace_escape_chars\n\n\n# definitions \nURL_ROOT = 'http:\/\/www.kdca.go.kr\/'\nPATH_TABLES_OUTPUT = 'tables_kcdc_updates_temp.json'\nURL_FIRST_PAGE = URL_ROOT + '\/board.es?mid=a30402000000&bid=0030&nPage=1'\n\nLIST_AREA = [\"Seoul\",\n\"Busan\",\n\"Daegu\",\n\"Incheon\",\n\"Gwangju\",\n\"Daejeon\",\n\"Ulsan\",\n\"Sejong\",\n\"Gyeonggi\",\n\"Gangwon\",\n\"Chungbuk\",\n\"Chungnam\",\n\"Jeonbuk\",\n\"Jeonnam\",\n\"Gyeongbuk\",\n\"Gyeongnam\",\n\"Jeju\"]\n\nDICT_AREA_SPECIAL = {'Gyeonggi': [\"Gyeong-\", \"gi\"],\n                    'Gangwon':[\"Gang-\", \"won\"],\n                    'Chungbuk':[\"Chung-\", \"buk\"],\n                    'Chungnam':[\"Chung-\", \"nam\"],\n                    'Jeonbuk': [\"Jeon-\", \"buk\"],\n                    'Jeonnam': [\"Jeon-\", \"nam\"],\n                    'Gyeongbuk': [\"Gyeong-\", \"buk\"],\n                    'Gyeongnam': [\"Gyeong-\", \"nam\"],\n                    'Daejeon' : [\"Dae-\", \"jeon\"]}\n\nDICT_AREA_SPECIAL_2 ={'Gyeonggi': [\"Gyeong\", \"gi\"],\n                    'Gangwon':[\"Gang\", \"won\"],\n                    'Chungbuk':[\"Chung\", \"buk\"],\n                    'Chungnam':[\"Chung\", \"nam\"],\n                    'Jeonbuk': [\"Jeon\", \"buk\"],\n                    'Jeonnam': [\"Jeon\", \"nam\"],\n                    'Gyeongbuk': [\"Gyeong\", \"buk\"],\n                    'Gyeongnam': [\"Gyeong\", \"nam\"],\n                    'Daejeon' : [\"Dae\", \"jeon\"]}\n\ndef convert_int(str_in):\n    # suppress \",\"\n    str_in = re.sub(',', '', str_in)\n    # convert in integer\n    return int(str_in)\n\ndef find_in_table(response, n_row, n_col):\n    # example : n_row=4, n_col=3\n    # \/\/tbody\/tr\/td\/p\/span[contains(., \"Confirmed\")]\/..\/..\/..\/..\/\/tr[4]\/td[3]\/p\/span[re:test(.,\"^\\d\")]\/\/text()\n    search_str = '\/\/tbody\/tr\/td\/p\/' + \\\n        'span[contains(., \"Confirmed\")]' + \\\n        '\/..\/..\/..\/..\/\/tr[{}]\/td[{}]'.format(n_row, n_col) + \\\n        '\/p\/span[re:test(.,\"^\\d\")]\/\/text()'\n\n    text_found = response.xpath(search_str).get()\n    return text_found \n\n\ndef find_in_table_rel_row(response, delta_row, n_col):\n    # example : delta_row=2, n_col=3\n    # \/\/tbody\/tr\/td\/p\/span[contains(., \"Confirmed\")]\/..\/..\/..\/following-sibling::tr[2]\/td[3]\/p\/span[re:test(.,\"^\\d\")]\/\/text()\n    search_str = '\/\/tbody\/tr\/td\/p\/' + \\\n        'span[contains(., \"Confirmed\")]' + \\\n        '\/..\/..\/..\/following-sibling::tr[{}]\/td[{}]'.format(delta_row,n_col) + \\\n        '\/p\/span[re:test(.,\"^\\d\")]\/\/text()'\n\n    text_found = response.xpath(search_str).get()\n    return text_found \n\ndef simplify_names(str_in):\n    str_out = str_in\n    for area_curr in DICT_AREA_SPECIAL_2.keys():\n        str_search = '-{0,1}\\s{0,1}(\\r\\n){0,1}\\s{0,1}-{0,1}'.join(\\\n                                            DICT_AREA_SPECIAL_2[area_curr])\n        str_out = re.sub(str_search, area_curr, str_out)\n    \n    return str_out\n\ndef replace_spaces(text):\n    return re.sub(\"\\s+\", \" \", text) \n\ndef replace_tags_by(text, token=' line '):\n    return replace_tags(text, token=token)\n\ndef remove_tags_keep(text, keep=('tr',)):\n    return remove_tags(text, keep=keep)\n                 \ndef replace_escape_chars_by(text, replace_by=\" \"):\n    return replace_escape_chars(text, replace_by=replace_by)\n    \ndef find_int_in_table(response, n_row, n_col):\n    text_found = find_in_table(response, n_row, n_col)\n    if isinstance(text_found, str):\n        return convert_int(text_found)\n    else:\n        return text_found\n\ndef create_str_area(area):\n    '''\n    Create a search string to find area \n    with take into account special namming\n    '''\n    if (area in DICT_AREA_SPECIAL_2.keys()):\n        str_1 = DICT_AREA_SPECIAL_2[area][0]\n        str_2 = DICT_AREA_SPECIAL_2[area][1]\n        if area == \"Gyeonggi\":\n            return  '[((contains(., \"{}\") and '.format(str_1) + \\\n                'contains(., \"{}\"))'.format(str_2) + \\\n                'or (contains(., \"Gyeon\") and contains(., \"ggi\")))]'\n        \n        return  '[contains(., \"{}\")]'.format(str_1) + \\\n                '[contains(., \"{}\")]'.format(str_2)\n    else:\n        return  '[contains(., \"{}\")]'.format(area) \n\ndef find_int_area_in_table(response, area):\n    \n    flag_add = None\n    \n    # Regions table search \n    \n    # new table since 2020-04-05 : \n    # Regions in column (vertical)\n    # example : \n    # \/\/tbody\/tr\/td[contains(., \"Seoul\")]\/parent::tr\/parent::tbody\/tr[1]\/td[2][contains(., \"Confirmed cases\")]\/parent::tr\/following-sibling::tr\/td[contains(., \"Total\")]\/parent::tr\/following-sibling::tr\/td[contains(., \"Seoul\")]\/following-sibling::td[1]\n    str_area = create_str_area(area)\n    search_str = '\/\/tbody\/tr\/td' + str_area + \\\n    '\/parent::tr\/parent::tbody\/tr[1]\/td[2][contains(., \"Confirmed cases\")]' + \\\n    '\/parent::tr\/following-sibling::tr\/td[contains(., \"Total\")]' + \\\n    '\/parent::tr\/following-sibling::tr\/td' + str_area + \\\n    '\/following-sibling::td[1]\/p\/span[re:test(.,\"^\\d\")]\/\/text()'\n    text_found = response.xpath(search_str).get()\n    if text_found != None:\n        return flag_add, convert_int(text_found)\n    \n    # example : same than before but starting at 3th row (9th june for example)\n    # \/\/tbody\/tr\/td[contains(., \"Seoul\")]\/parent::tr\/parent::tbody\/tr[3]\/td[2][contains(., \"Confirmed cases\")]\/parent::tr\/following-sibling::tr\/td[contains(., \"Total\")]\/parent::tr\/following-sibling::tr\/td[contains(., \"Seoul\")]\/following-sibling::td[1]\n    search_str = '\/\/tbody\/tr\/td' + str_area + \\\n    '\/parent::tr\/parent::tbody\/tr[3]\/td[2][contains(., \"Confirmed cases\")]' + \\\n    '\/parent::tr\/following-sibling::tr\/td[contains(., \"Total\")]' + \\\n    '\/parent::tr\/following-sibling::tr\/td' + str_area + \\\n    '\/following-sibling::td[1]\/p\/span[re:test(.,\"^\\d\")]\/\/text()'\n    text_found = response.xpath(search_str).get()\n    if text_found != None:\n        return flag_add, convert_int(text_found)\n        \n    # check if new table with 2 vertical list of Region : \n    search_str = '\/\/tbody\/tr[1]\/td[contains(., \"Region\")]'\n    text_found = response.xpath(search_str).getall()\n    # list the tables (first one with 2 columns \"Region\") : \n    search_str = '\/\/tbody\/tr[1]\/td[1][contains(., \"Region\")]'\n    list_tables = response.xpath(search_str).getall()\n\n    search_str_2 ='\/\/tbody\/tr[2]\/td[contains(., \"Region\")]' + \\\n        '\/following-sibling::td[1][contains(., \"Confirmed\")]'\n    text_found_2 = response.xpath(search_str_2).get()\n\n    # '\/\/tbody\/tr[2]\/td[contains(., \"Region\")]\/following-sibling::td[1][contains(., \"Confirmed\")]\/parent::tr\/following-sibling::tr\/td[1][contains(., \"Seoul\")]\/following-sibling::td[1]'\n    # if only one vertical list, ok : \n    if (len(text_found) == 1) | \\\n        ((len(text_found) == 3) & (len(list_tables) == 2)) | \\\n        (text_found_2 is not None):\n        print(\"Region Tables found...\")\n        \n        if text_found_2 is not None:\n            print('1 table with only one col \"Region\" but into a Table')\n            str_xp = search_str_2 + '\/..\/..\/'\n        \n        elif (len(text_found) == 1):\n            print('1 table with only one col \"Region\"')\n            str_xp = '\/\/tbody\/tr[1]\/td[1][contains(., \"Region\")]\/..\/..\/'\n        else:\n            print('2 tables (first table contains 2 col \"region\")')\n            str_xp = '\/\/tbody\/tr[1]\/td[1][contains(., \"Region\")]' + \\\n                '\/..\/td[3][not(contains(., \"%\"))]' + '\/..\/..\/'\n            \n        print(\"Region vertical table: \", area)\n        ## classical with whole name of area and in vertical :\n        search_str = str_xp + \\\n            'tr\/td\/p\/span[contains(., \"{}\")]'.format(area) + \\\n            '\/..\/..\/..\/td[2]\/p\/span[re:test(.,\"^\\d\")]\/\/text()'\n        text_found = response.xpath(search_str).get()\n        print(\"text_found: \", text_found)\n        # if can be in two parts : \n        if (text_found is None) & (area in DICT_AREA_SPECIAL.keys()):\n            print(\"Region vertical table special : \", area)\n            str_1 = DICT_AREA_SPECIAL[area][0]\n            str_2 = DICT_AREA_SPECIAL[area][1]\n            \n            text_found = response.xpath(str_xp + '\/tr\/td[1]' + \\\n                           '[contains(., \"{}\")]'.format(str_1) + \\\n                           '[contains(.,\"{}\")]'.format(str_2) + \\\n                           '\/..\/td[2]\/p\/span[re:test(.,\"^\\d\")]\/\/text()').get()\n                  \n            print(\"special Text : \", text_found)\n\n    else: # if no or more than one vert. list, look somewhere else\n        text_found = None\n    ## special seperate name : \"firstPart-secondPart\"\n    if text_found is None:\n        search_str = '\/\/tbody\/tr[1]\/td[1][contains(., \"Regions\")]'\n        table_found = response.xpath(search_str)\n        \n        # if found, try with table in horizontal\n        if (table_found != None):\n            text_row = ''\n            I_row = 1\n            # look for \"area\" into row of Regions table \n            while (text_row != None): \n                search_str = '\/\/tbody\/tr[1]\/td[1][contains(., \"Regions\")]' + \\\n                '\/..\/following-sibling::tr[{}]\/td[1]\/p\/\/text()'.format(I_row)\n\n                text_row = response.xpath(search_str).getall()\n                # if list so it is written in special format (several rows)\n                # text_row is a list (it can be empty)\n                if len(text_row) > 0:\n                    text_row = ''.join(text_row).replace(\"-\",\"\").replace(\" \",\"\")\n                else:\n                    text_row = None\n\n                if text_row == area:\n                    break  \n\n                I_row = I_row + 1\n\n            if text_row != None:\n                search_str = '\/\/tbody\/tr[1]\/td[1][contains(., \"Regions\")]' + \\\n                '\/..\/following-sibling::tr[{}]'.format(I_row) + \\\n                '\/td[2]\/p\/span[re:test(.,\"^\\d\")]\/\/text()'\n                text_found = response.xpath(search_str).get()\n        \n    # if old style table (area in columns)\n    if text_found is None:\n        # search # of columns\n        type_table = None\n        text_col = ''\n        I_col_area = 1\n        \n        # to detect if flag_add \n        # \/\/table\/preceding-sibling::p[1][contains(.,\"total confirmed cases\")]\n        \n        # detection table type : Place or City ?\n        \n        search_place = '\/\/tbody\/tr[1]\/td[1][contains(., \"Place\")]'\n        search_city = '\/\/tbody\/tr[1]\/td[2][contains(., \"City\")]'\n        if response.xpath(search_place).get() != None:\n            type_table = \"Place\"\n        elif response.xpath(search_city).get() != None:\n            type_table = \"City\"\n        else:\n            type_table = None\n            text_found = None\n            flag_add = None\n            return flag_add, text_found\n        \n        # until end of table columns, look for area string\n        #print(response.url)\n        \n        while text_col != None:\n            # look for into \"Place\" type table to retrieve col number of area\n            #print(\"type_table:\", type_table)\n            #print(\"I_col_area:\", I_col_area)\n            \n            # if Place table search into table the column number of the area\n            if type_table == \"Place\":\n                search_str = '\/\/tbody\/tr[1]\/td[1][contains(., \"Place\")]' + \\\n                    '\/..\/..\/tr[2]\/td[{}]\/p\/span\/text()'.format(I_col_area)\n                text_col = response.xpath(search_str).get()\n                if text_col == area:\n                    break\n                I_col_area = I_col_area + 1\n                continue\n                  \n            # if not found, try into \"City\" type table\n            #\/\/tbody\/tr[1]\/td[2][contains(., \"City\")]\/..\/..\/tr[2]\/td[1]\/p\/span\/text()\n            #if text_col is None:\n            search_str = '\/\/tbody\/tr[1]\/td[2][contains(., \"City\")]' + \\\n                '\/..\/..\/tr[2]\/td[{}]\/p\/span\/text()'.format(I_col_area)\n            text_col = response.xpath(search_str).get()\n            if text_col == area:\n                break  \n                \n            # if not found, try into \"Province\" type table\n            #if text_col is None:\n            search_str = '\/\/tbody\/tr\/td[2]\/p\/span[contains(.,\"Province\"' + \\\n                    ')]\/..\/..\/..\/following-sibling::tr[1]' + \\\n                    '\/td[{}]\/p\/\/text()'.format(I_col_area)\n            text_col = response.xpath(search_str).getall()\n            # if list so it is written in special format (several rows)\n            # tet_col is a list (it can be empty)\n            if len(text_col) > 0:\n                text_col = ''.join(text_col).replace(\"-\",\"\").replace(\" \",\"\")\n            else:\n                text_col = None \n     \n            if text_col == area:\n                type_table = \"Province\"\n                break\n                \n            I_col_area = I_col_area + 1\n       \n        if type_table == \"Place\":\n            search_str_int = '\/\/tbody\/tr[1]\/td[1][contains(., \"Place\")]\/..\/' + \\\n            '..\/tr\/td\/p\/span[contains(., \"{}\")]'.format(area) + \\\n            '\/..\/..\/..\/..\/tr[3]\/td[{}]'.format(I_col_area + 1) + \\\n            '\/p\/span[re:test(.,\"^\\d\")]\/\/text()'  \n            text_found = response.xpath(search_str_int).get()\n            if text_found is not None:\n                flag_add = True\n        else:\n            # \/\/tbody\/tr\/td[2][contains(.,\"City\")]\/..\/following-sibling::tr[2]\/td[2]\/p\/span\/\/text()\n            search_str_int = '\/\/tbody\/tr\/td[2]' + \\\n                '[contains(., \"{}\")]\/..\/'.format(type_table) + \\\n                'following-sibling::tr[2]\/td[{}]'.format(I_col_area + 1) + \\\n                '\/p\/span[re:test(.,\"^\\d\")]\/\/text()'  \n            text_found = response.xpath(search_str_int).get()\n            if text_found is not None:\n                flag_add = False\n    else:\n        flag_add = False\n        \n    if isinstance(text_found, str):\n        return flag_add, convert_int(text_found)\n    else:\n        return flag_add, text_found\n    \n\nclass KCDCitem(scrapy.Item):\n    '''\n    Class item to declare different information to scrap\n    and how to process (as input or output)\n    '''\n    # define the fields for your item here like:\n    url = scrapy.Field(output_processor=TakeFirst())\n    \n    source = scrapy.Field(output_processor=TakeFirst())\n    \n    date_published = scrapy.Field(\n        input_processor=MapCompose(remove_tags),\n        output_processor=TakeFirst()\n    )\n    \n    total = scrapy.Field(output_processor=TakeFirst())\n    \n    flag_add = scrapy.Field(output_processor=TakeFirst())\n    \n    Seoul = scrapy.Field(output_processor=TakeFirst())   \n\n    Busan = scrapy.Field(output_processor=TakeFirst())  \n    Daegu = scrapy.Field(output_processor=TakeFirst())  \n    Incheon = scrapy.Field(output_processor=TakeFirst())  \n    Gwangju = scrapy.Field(output_processor=TakeFirst())  \n    Daejeon = scrapy.Field(output_processor=TakeFirst())  \n    Ulsan = scrapy.Field(output_processor=TakeFirst())  \n    Sejong = scrapy.Field(output_processor=TakeFirst())  \n    Gyeonggi = scrapy.Field(output_processor=TakeFirst())  \n    Gangwon = scrapy.Field(output_processor=TakeFirst())  \n    Chungbuk = scrapy.Field(output_processor=TakeFirst())  \n    Chungnam = scrapy.Field(output_processor=TakeFirst())  \n    Jeonbuk = scrapy.Field(output_processor=TakeFirst())  \n    Jeonnam = scrapy.Field(output_processor=TakeFirst())  \n    Gyeongbuk = scrapy.Field(output_processor=TakeFirst())  \n    Gyeongnam = scrapy.Field(output_processor=TakeFirst())  \n    Jeju = scrapy.Field(output_processor=TakeFirst())\n    \n    #output_processor=Join('\\r\\nTable:\\r\\n'),\n    body = scrapy.Field(\n        input_processor=MapCompose(remove_tags_keep, replace_tags_by, \n                                   simplify_names, replace_escape_chars_by, \n                                  replace_spaces),\n        output_processor=Join('Table:'),\n    )\n\nclass KCDCTablesSpider(scrapy.Spider):\n    '''\n    Spider to scrap tables webpages : \n    - how to find information for scraping\n    - which field names to store\n    '''\n    # Your spider definition\n    name = 'tables_kcdc_spider'\n    # output definition :\n    custom_settings = {\n      'FEED_FORMAT': 'json',\n      'FEED_URI': PATH_TABLES_OUTPUT\n      }\n    # urls to scrap\n    start_urls = [URL_FIRST_PAGE]\n   \n    def parse(self, response):\n        '''\n        Parse definition with xpath which define all patterns to use\n        for retrieve information into HTML strings\n        '''\n        #url\tsource\tauthor\ttitle\ttheme\tdescription\tdate_published\tbody   \n        l = ItemLoader(item=KCDCitem(), selector=response)\n        \n        l.add_xpath('body', '\/\/tbody')\n        \n        l.add_value('url', response.url)\n        \n        l.add_value('source', \"KCDC\")\n        \n        # '\/\/span[re:test(.,\"^Update$\")]\/..\/b\/\/text()', \n        l.add_xpath(\"date_published\", \n            '\/\/ul[@class=\"head info\"]\/li\/span[contains(.,\"Date\")]\/..\/b\/\/text()',\n                    re=\"[0-9]+-[0-9]+-[0-9]+\")\n        \n        # scrap TOTAL\n        # test if new 2020-07-20 table\n        tot_2020_07 = response.xpath('\/\/tbody\/tr\/td\/p\/' + \\\n            'span[contains(., \"Confirmed\")]\/parent::p\/parent::td\/' + \\\n            'preceding-sibling::td\/p\/span[contains(.,\"Region\")]\/..\/..\/..\/' + \\\n            'following-sibling::tr\/td[1]\/p\/span[contains(.,\"Total\")]\/' + \\\n            'parent::p\/parent::td\/following-sibling::td[1]\/p\/' + \\\n            'span[re:test(.,\"^\\d\")]\/\/text()').get()\n        \n        tot_2020_03_15 = response.xpath('\/\/tbody\/tr\/td\/p\/' + \\\n            'span[contains(., \"Epidemiological\")]\/parent::p\/parent::td\/' + \\\n            'preceding-sibling::td\/p\/span[contains(.,\"Region\")]\/..\/..\/..\/' + \\\n            'following-sibling::tr\/td[1]\/p\/span[contains(.,\"Total\")]\/' + \\\n            'parent::p\/parent::td\/following-sibling::td[1]\/p\/' + \\\n            'span[re:test(.,\"^\\d\")]\/\/text()').get()\n\n        if tot_2020_03_15 is not None:\n            print(\"Total found by tot 2020_07\")\n            l.add_value(\"total\", convert_int(tot_2020_03_15))\n            flag_2020_03_15 = True\n        else:\n            flag_2020_03_15 = False        \n\n        if tot_2020_07 is not None:\n            print(\"Total found by tot 2020_07\")\n            l.add_value(\"total\", convert_int(tot_2020_07))\n            flag_07_2020 = True\n        else:\n            flag_07_2020 = False\n\n        if not(flag_07_2020 | flag_2020_03_15):\n            # check if more than 1 row of data\n            #flag_total = find_in_table(response, 4, 3)\n            flag_total = find_in_table_rel_row(response, 2, 3)\n            print(\"flag_total : \", flag_total)\n            \n            # if not try to find into 3rd row \n            if flag_total is not None:\n                print(\"Total continue...\")\n                # check if new table Tested positive : \n                # example : \/\/tbody\/tr\/td\/p\/span[contains(., \"Tested\")]\/..\/..\/..\/..\/\/tr[2]\/td[1]\/p\/span[re:test(.,\"^Confirmed\")]\/\/text()\n                # \n                # OLD\n                #flag_tested = response.xpath('\/\/tbody\/tr\/td\/p\/' + \\\n                #    'span[contains(., \"Tested\")]\/..\/..\/..\/..\/\/tr[2]\/td[1]' + \\\n                #    '\/p\/span[re:test(.,\"^Confirmed\")]\/\/text()').get()\n                \n                # \/\/tbody\/tr\/td\/p\/span[contains(., \"Tested\")]\/..\/..\/..\/following-sibling::tr[1]\/td[1]\/p\/span[contains(.,\"Confirmed\")]\/\/text()\n                flag_tested = response.xpath('\/\/tbody\/tr\/td\/p\/' + \\\n                    'span[contains(., \"Tested\")]' + \\\n                    '\/..\/..\/..\/following-sibling::tr[1]\/td[1]' + \\\n                    '\/p\/span[re:test(.,\"^Confirmed\")]\/\/text()').get()\n                \n                # Test new table : \n                # ex: \n                # \/\/tbody\/tr[1]\/td\/p\/span[contains(., \"Total reported\")]\/..\/..\/..\/..\/\/tr[5]\/td[3]\/p\/span\/\/text()\n                flag_new = response.xpath('\/\/tbody\/tr[1]\/td\/p\/span[contains(.,' + \\\n                    '\"Total reported\")]\/..\/..\/..\/..\/\/tr[5]\/td[3]' + \\\n                    '\/p\/span\/\/text()').get()\n                \n                # if Tested positive table\n                if flag_tested is not None:\n                    print(\"Total found by flag_tested in flag_total\")\n                    l.add_value(\"total\", convert_int(flag_total))\n                elif flag_new is not None:\n                    print(\"Total found by flag_new\")\n                    l.add_value(\"total\", convert_int(flag_new))\n                else:\n                    \n                    # check if Sub Total exist \n                    # ex : \n                    # \/\/tbody\/tr\/td\/p\/span[contains(., \"Confirmed\")]\/..\/..\/..\/..\/\/tr[2]\/td[1]\/p\/span[re:test(.,\"^Sub\")]\/\/text()\n                    flag_sub = response.xpath('\/\/tbody\/tr\/td\/p\/' + \\\n                        'span[contains(., \"Confirmed\")]\/..\/..\/..\/..\/\/tr[2]\/td[1]' + \\\n                        '\/p\/span[re:test(.,\"^Sub\")]\/\/text()').get()\n\n                    # if sub total exist (don't sum up)\n                    if flag_sub is not None:\n                        print(\"Total continue by flag_sub\")\n                        int_total_2 = find_int_in_table(response, 3, 3)\n                        int_total_3 = find_int_in_table(response, 4, 3)\n\n                    else:\n                        print(\"Total continue by not flag_sub\")\n                        int_2_1 = find_int_in_table(response, 3, 3)\n                        int_2_2 = find_int_in_table(response, 3, 4)\n                        int_total_2 = int_2_1 + int_2_2\n                        int_3_1 = find_int_in_table(response, 4, 3)\n                        int_3_2 = find_int_in_table(response, 4, 4)\n                        int_total_3 = int_3_1 + int_3_2\n                    # sometimes, last total is in 2nd or 3rd\n                    if int_total_3 > int_total_2:\n                        print(\"Total found by int_total_3 > int_total_2\")\n                        l.add_value(\"total\", int_total_3)\n                    else:\n                        print(\"Total found by NOT int_total_3 > int_total_2\")\n                        l.add_value(\"total\", int_total_2)\n\n            else: # take 2nd row (because only one row of data)\n                print(\"Total found by only one row of data\")\n                int_total = find_int_in_table(response, 3, 3)\n                l.add_value(\"total\", int_total)\n            \n        flag_add = False\n        \n        # SCRAP AREAS\n        for area in LIST_AREA:\n            \n            flag_add_curr, int_area = find_int_area_in_table(response, area)\n            \n            if flag_add_curr == True:\n                flag_add = True\n                \n            if int_area is not None:\n                l.add_value(area, int_area)\n                \n        l.add_value(\"flag_add\", flag_add)\n                \n        yield l.load_item()\n","01a973b5":"%%capture\n%%writefile kcdctablespiderKR.py\n# -*-coding:utf-8 -*\n\n#\n# Module for scrapy over tables in KCDC\n#\n\n# import\nimport re\nimport scrapy\nfrom scrapy.loader import ItemLoader\nfrom scrapy.loader.processors import MapCompose, Join, TakeFirst\nfrom w3lib.html import remove_tags, replace_tags, replace_escape_chars\n\n\n# definitions \nURL_ROOT = 'http:\/\/www.kdca.go.kr'\nPATH_TABLES_OUTPUT = 'tables_kcdc_updates_kr_temp.json'\n\n#\n#URL_FIRST_PAGE = URL_ROOT + '\/board\/board.es?mid=a20501000000&bid=0015'\nURL_FIRST_PAGE = URL_ROOT + '\/board\/board.es?mid=a20501010000&bid=0015'\n\nLIST_AREA = [\"\uc11c\uc6b8\",\n    \"\ubd80\uc0b0\",\n    \"\ub300\uad6c\",\n    \"\uc778\ucc9c\",\n    \"\uad11\uc8fc\",\n    \"\ub300\uc804\",\n    \"\uc6b8\uc0b0\",\n    \"\uc138\uc885\",\n    \"\uacbd\uae30\",\n    \"\uac15\uc6d0\",\n    \"\ucda9\ubd81\",\n    \"\ucda9\ub0a8\",\n    \"\uc804\ubd81\",\n    \"\uc804\ub0a8\",\n    \"\uacbd\ubd81\",\n    \"\uacbd\ub0a8\",\n    \"\uc81c\uc8fc\"]\n\nDICT_AREA_EN = {\"\uc11c\uc6b8\": \"Seoul\",\n    \"\ubd80\uc0b0\": \"Busan\",\n    \"\ub300\uad6c\": \"Daegu\",\n    \"\uc778\ucc9c\": \"Incheon\",\n    \"\uad11\uc8fc\": \"Gwangju\",\n    \"\ub300\uc804\": \"Daejeon\",\n    \"\uc6b8\uc0b0\": \"Ulsan\",\n    \"\uc138\uc885\": \"Sejong\",\n    \"\uacbd\uae30\": \"Gyeonggi\",\n    \"\uac15\uc6d0\": \"Gangwon\",\n    \"\ucda9\ubd81\": \"Chungbuk\",\n    \"\ucda9\ub0a8\": \"Chungnam\",\n    \"\uc804\ubd81\": \"Jeonbuk\",\n    \"\uc804\ub0a8\": \"Jeonnam\",\n    \"\uacbd\ubd81\": \"Gyeongbuk\",\n    \"\uacbd\ub0a8\": \"Gyeongnam\",\n    \"\uc81c\uc8fc\": \"Jeju\"}\n\nDICT_AREA_SPECIAL = {'Gyeonggi': [\"Gyeong-\", \"gi\"],\n                    'Gangwon':[\"Gang-\", \"won\"],\n                    'Chungbuk':[\"Chung-\", \"buk\"],\n                    'Chungnam':[\"Chung-\", \"nam\"],\n                    'Jeonbuk': [\"Jeon-\", \"buk\"],\n                    'Jeonnam': [\"Jeon-\", \"nam\"],\n                    'Gyeongbuk': [\"Gyeong-\", \"buk\"],\n                    'Gyeongnam': [\"Gyeong-\", \"nam\"],\n                    'Daejeon' : [\"Dae-\", \"jeon\"]}\n\nDICT_AREA_SPECIAL_2 ={'Gyeonggi': [\"Gyeong\", \"gi\"],\n                    'Gangwon':[\"Gang\", \"won\"],\n                    'Chungbuk':[\"Chung\", \"buk\"],\n                    'Chungnam':[\"Chung\", \"nam\"],\n                    'Jeonbuk': [\"Jeon\", \"buk\"],\n                    'Jeonnam': [\"Jeon\", \"nam\"],\n                    'Gyeongbuk': [\"Gyeong\", \"buk\"],\n                    'Gyeongnam': [\"Gyeong\", \"nam\"],\n                    'Daejeon' : [\"Dae\", \"jeon\"]}\n\ndef convert_int(str_in):\n    # suppress \",\"\n    #str_in = re.sub(',', '', str_in)\n    str_in = re.sub('[\\W]', '', str_in)\n    # convert in integer\n    return int(str_in)\n\ndef simplify_names(str_in):\n    str_out = str_in\n    for area_curr in DICT_AREA_SPECIAL_2.keys():\n        str_search = '-{0,1}\\s{0,1}(\\r\\n){0,1}\\s{0,1}-{0,1}'.join(\\\n                                            DICT_AREA_SPECIAL_2[area_curr])\n        str_out = re.sub(str_search, area_curr, str_out)\n    \n    return str_out\n\ndef replace_spaces(text):\n    return re.sub(\"\\s+\", \" \", text) \n\ndef replace_tags_by(text, token=' line '):\n    return replace_tags(text, token=token)\n\ndef remove_tags_keep(text, keep=('tr',)):\n    return remove_tags(text, keep=keep)\n                 \ndef replace_escape_chars_by(text, replace_by=\" \"):\n    return replace_escape_chars(text, replace_by=replace_by)\n\nclass KCDCitem(scrapy.Item):\n    '''\n    Class item to declare different information to scrap\n    and how to process (as input or output)\n    '''\n    # define the fields for your item here like:\n    url = scrapy.Field(output_processor=TakeFirst())\n    \n    source = scrapy.Field(output_processor=TakeFirst())\n    \n    date_published = scrapy.Field(\n        input_processor=MapCompose(remove_tags),\n        output_processor=TakeFirst()\n    )\n    \n    total = scrapy.Field(output_processor=TakeFirst())\n    \n    flag_add = scrapy.Field(output_processor=TakeFirst())\n    \n    Seoul = scrapy.Field(output_processor=TakeFirst())   \n\n    Busan = scrapy.Field(output_processor=TakeFirst())  \n    Daegu = scrapy.Field(output_processor=TakeFirst())  \n    Incheon = scrapy.Field(output_processor=TakeFirst())  \n    Gwangju = scrapy.Field(output_processor=TakeFirst())  \n    Daejeon = scrapy.Field(output_processor=TakeFirst())  \n    Ulsan = scrapy.Field(output_processor=TakeFirst())  \n    Sejong = scrapy.Field(output_processor=TakeFirst())  \n    Gyeonggi = scrapy.Field(output_processor=TakeFirst())  \n    Gangwon = scrapy.Field(output_processor=TakeFirst())  \n    Chungbuk = scrapy.Field(output_processor=TakeFirst())  \n    Chungnam = scrapy.Field(output_processor=TakeFirst())  \n    Jeonbuk = scrapy.Field(output_processor=TakeFirst())  \n    Jeonnam = scrapy.Field(output_processor=TakeFirst())  \n    Gyeongbuk = scrapy.Field(output_processor=TakeFirst())  \n    Gyeongnam = scrapy.Field(output_processor=TakeFirst())  \n    Jeju = scrapy.Field(output_processor=TakeFirst())\n    \n    #output_processor=Join('\\r\\nTable:\\r\\n'),\n    body = scrapy.Field(\n        input_processor=MapCompose(remove_tags_keep, replace_tags_by, \n                                   simplify_names, replace_escape_chars_by, \n                                  replace_spaces),\n        output_processor=Join('Table:'),\n    )\n\nclass KCDCTablesSpider(scrapy.Spider):\n    '''\n    Spider to scrap tables webpages : \n    - how to find information for scraping\n    - which field names to store\n    '''\n    # Your spider definition\n    name = 'tables_kcdc_spider'\n    # output definition :\n    custom_settings = {\n      'FEED_FORMAT': 'json',\n      'FEED_URI': PATH_TABLES_OUTPUT\n      }\n    # urls to scrap\n    start_urls = [URL_FIRST_PAGE]\n   \n    def parse(self, response):\n        '''\n        Parse definition with xpath which define all patterns to use\n        for retrieve information into HTML strings\n        '''\n        #url\tsource\tauthor\ttitle\ttheme\tdescription\tdate_published\tbody   \n        l = ItemLoader(item=KCDCitem(), selector=response)\n        \n        l.add_xpath('body', '\/\/tbody')\n        \n        l.add_value('url', response.url)\n        \n        l.add_value('source', \"KCDA_KR\")\n        \n        # '\/\/span[re:test(.,\"^Update$\")]\/..\/b\/\/text()', \n        l.add_xpath(\"date_published\", \n            '\/\/ul[@class=\"head info\"]\/li\/span[contains(.,\"\uc791\uc131\uc77c\")]\/..\/b\/\/text()',\n                    re=\"[0-9]+-[0-9]+-[0-9]+\")\n        \n        # scrap TOTAL\n        xp_first = '\/\/tbody\/tr[1]\/td[1][contains(., \"\uad6c\ubd84\")]\/' + \\\n            'following-sibling::td[2][contains(., \"\uc11c\uc6b8\")]'\n        xp_list_area = xp_first + \\\n            '\/parent::tr\/td[1]\/following-sibling::td\/p\/span\/\/text()'\n        xp_tot = xp_first + '\/parent::tr\/following-sibling::tr[2]\/td[1]\/'  + \\\n            'following-sibling::td\/p\/span[1]\/\/text()'\n\n        list_area = response.xpath(xp_list_area).getall()\n        list_tot = response.xpath(xp_tot).getall()\n\n        for I, area_curr in enumerate(list_area):\n\n            tot_curr = list_tot[I]\n\n            if area_curr == \"\ud569\uacc4\":\n                int_tot = convert_int(tot_curr)\n                xp_incoming = '\/\/tbody\/tr[1]\/td[1][contains(., \"\uad6c\ubd84\")]\/' + \\\n                'following-sibling::td[2][contains(., \"\uc720\uc785\uad6d\uac00\")]'  + \\\n                '\/parent::tr\/following-sibling::tr[3]\/td[1]\/' + \\\n                'following-sibling::td[1]\/p\/span[1]\/\/text()'\n                tot_incoming = response.xpath(xp_incoming).get()\n                int_tot = int_tot + convert_int(tot_incoming)\n\n                l.add_value(\"total\", int_tot)\n                continue\n            \n            l.add_value(DICT_AREA_EN[area_curr], convert_int(tot_curr))\n            \n            \n        flag_add = False\n                \n        l.add_value(\"flag_add\", flag_add)\n                \n        yield l.load_item()\n","109fe971":"str_in = \"68,200 1)2)\"\nre.sub('[\\W]', '', str_in)","31647d01":"6820012","36f47f73":"str_in = \"6062 1)\" \nre.sub('[\\W]', '', str_in)","0778e276":"%%capture\n%%writefile run_kcdc_tables_updates.py\n# -*-coding:utf-8 -*\n\n#\n# Run scrapy for retrieve updates tables from KCDC\n#\n\n# import\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom kcdctablespider import KCDCTablesSpider\nimport pandas as pd\nimport os\n\n# definitions\nPATH_PAGES_KCDC_UPDATES = os.getcwd() + '\/pages_kcdc_updates_temp.json'\nPATH_TABLES_OUTPUT = os.getcwd() + '\/tables_kcdc_updates_temp.json'\n\n# get urls pages links\ndf_pages_kcdc_updates = pd.read_json(PATH_PAGES_KCDC_UPDATES)\n# define url for scrapy\nKCDCTablesSpider.start_urls = df_pages_kcdc_updates[\"link\"].tolist()\n\n# remove old is exist\nif os.path.isfile(PATH_TABLES_OUTPUT):\n    os.remove(PATH_TABLES_OUTPUT)\n# start scraping process\nprocess = CrawlerProcess()\nprocess.crawl(KCDCTablesSpider)\nprocess.start() # the script will block here until the crawling is finished\n","a2401042":"%%capture\n%%writefile run_kcdc_tables_updates_kr.py\n# -*-coding:utf-8 -*\n\n#\n# Run scrapy for retrieve updates tables from KCDC\n#\n\n# import\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom kcdctablespiderKR import KCDCTablesSpider # KOREAN VERSION\nimport pandas as pd\nimport os\n\n# definitions\nPATH_PAGES_KCDC_UPDATES = os.getcwd() + '\/pages_kcdc_updates_kr_temp.json'\nPATH_TABLES_OUTPUT = os.getcwd() + '\/tables_kcdc_updates_kr_temp.json'\n\n# get urls pages links\ndf_pages_kcdc_updates = pd.read_json(PATH_PAGES_KCDC_UPDATES)\n# define url for scrapy\nKCDCTablesSpider.start_urls = df_pages_kcdc_updates[\"link\"].tolist()\n\n# remove old is exist\nif os.path.isfile(PATH_TABLES_OUTPUT):\n    os.remove(PATH_TABLES_OUTPUT)\n# start scraping process\nprocess = CrawlerProcess()\nprocess.crawl(KCDCTablesSpider)\nprocess.start() # the script will block here until the crawling is finished\n","1127f674":"%%capture capture_out\n%%time\n#######################\n## Scrap updates tables\n#\ntry:\n    if MODE_SCRAPY_KCDC_EN:\n        # clean (move file if exist)\n        clean_file(PATH_TABLES_KCDC_UPDATES)\n        # write into temp file to be proceeed as input\n        shutil.copyfile(PATH_PAGES_KCDC_UPDATES, PATH_PAGES_KCDC_UPDATES_TMP)\n        # execute external process (FALLBACK for scrapy problem)\n        !python run_kcdc_tables_updates.py\n        shutil.copyfile(PATH_TABLES_KCDC_UPDATES_TMP, PATH_TABLES_KCDC_UPDATES)\nexcept:\n    print(\"Error run_kcdc_tables_updates !\")","35d48cbf":"%%capture\nwith open('run_kcdc_tables_updates_log.txt',\"w\") as f:\n    f.write(capture_out.stdout)","6201883b":"%%capture capture_out\n%%time\n#######################\n## Scrap updates tables\n# KOREAN VERSION\n#\ntry:\n    if MODE_SCRAPY_KCDC_KR:\n        # clean (move file if exist)\n        clean_file(PATH_TABLES_KCDC_UPDATES_KR)\n        # write into temp file to be proceeed as input\n        shutil.copyfile(PATH_PAGES_KCDC_UPDATES_KR, \n                        PATH_PAGES_KCDC_UPDATES_KR_TMP)\n        # execute external process (FALLBACK for scrapy problem)\n        !python run_kcdc_tables_updates_kr.py\n        shutil.copyfile(PATH_TABLES_KCDC_UPDATES_KR_TMP, \n                        PATH_TABLES_KCDC_UPDATES_KR)\nexcept:\n    print(\"Error run_kcdc_tables_updates_kr !\")","06ae7c0c":"%%capture\nwith open('run_kcdc_tables_updates_kr_log.txt',\"w\") as f:\n    f.write(capture_out.stdout)","fd33c422":"%%capture\n# 1 : No-CSV EN-scrap KR-scrap (from scratch)\n#   - process EN data\n#   - process KR data\n#   - merge EN data \/ KR data\n#   - save CSV\n# 2 : CSV no-EN-scrap KR-scrap\n#   - process KR data\n#   - load CSV\n#   - merge CSV data \/ KR data\n#   - save CSV\n# 3 : CSV no-EN-scrap no-KR-scrap (no new data)\n#   - load CSV\n\n# process EN data   \nif MODE_SCRAPY_KCDC_EN:\n    df_tables_kcdc_updates = pd.read_json(PATH_TABLES_KCDC_UPDATES)\n    df_tables_kcdc_updates = \\\n    df_tables_kcdc_updates.sort_values(by=['date_published'])\n    print(\"Nb. South Korea articles (EN): \", df_tables_kcdc_updates.shape[0])\n\n    # Some articles do not have information => clean\n    df_tables_kcdc_updates = clean_kcdc_date(df_tables_kcdc_updates)\n    \n    # NEW : for each area data : \n    # try to find add format data to create if total format data DOES NOT EXIST \n    df_kcdc_corr = correct_addtive_value_2(df_tables_kcdc_updates) \n\n    #### Correction time continuity\n    df_kcdc_interp = correct_data_date_range(df_kcdc_corr)\n\n    # patch : error into data online\n    df_kcdc_interp.at[\"2020-03-29\",\"Seoul\"] = 410\n\n    # validate\n    # if total is ok and more than 90% of region, it is ok : TODO\n\n# check new data\nif MODE_SCRAPY_KCDC_KR:\n    df_tables_kcdc_updates_kr = pd.read_json(PATH_TABLES_KCDC_UPDATES_KR)\n    \n    if df_tables_kcdc_updates_kr.shape[0] > 1:\n        flag_new_kr_data = True\n    else:\n        flag_new_kr_data = False\nelse:\n    flag_new_kr_data = False\n\n# process KR data    \nif flag_new_kr_data:\n    df_tables_kcdc_updates_kr = \\\n    df_tables_kcdc_updates_kr.sort_values(by=['date_published'])\n    print(\"Nb. South Korea articles (KR): \", \n          df_tables_kcdc_updates_kr.shape[0])\n    # Some articles do not have information => clean\n    df_tables_kcdc_updates_kr = clean_kcdc_date(df_tables_kcdc_updates_kr)\n\n    # NEW : for each area data : \n    # try to find add format data to create\n    # if total format data DOES NOT EXIST \n    df_kcdc_corr_kr = correct_addtive_value_2(df_tables_kcdc_updates_kr) \n\n    #### Correction time continuity\n    df_kcdc_interp_kr = correct_data_date_range(df_kcdc_corr_kr)\n\n# MERGE EN \/ KR data\nif not(FLAG_CSV_KCDC) & flag_new_kr_data:\n    print(\"add new EN data & new KR data\")\n    # MERGE ?\n    date_min_kr = df_tables_kcdc_updates_kr['date_published'].min()\n    df_kcdc_interp = df_kcdc_interp[df_kcdc_interp[\"date\"] < date_min_kr]\n    df_kcdc_interp = df_kcdc_interp.append(df_kcdc_interp_kr)\nelif FLAG_CSV_KCDC & flag_new_kr_data:\n    print(\"add to CSV new KR data\")\n    # no EN scrapy data but new KR scrapy data to append to CSV\n    df_kcdc_interp = load_kcdc_csv()\n    date_min_kr = df_tables_kcdc_updates_kr['date_published'].min()\n    df_kcdc_interp = df_kcdc_interp[df_kcdc_interp[\"date\"] < date_min_kr]\n    df_kcdc_interp = df_kcdc_interp.append(df_kcdc_interp_kr)\nelif FLAG_CSV_KCDC & (not flag_new_kr_data) & (not MODE_SCRAPY_KCDC_EN):\n    print(\"no new KR data\")\n    df_kcdc_interp = load_kcdc_csv()\nelse: # no CSV and only KR data ? (NOT A GOOD SITUATION)\n    print(\"no CSV but new KR data\")\n    df_kcdc_interp = df_kcdc_interp_kr\n\ndf_kcdc_interp = df_kcdc_interp.sort_values(by=['date'])\n\nprint(\"Nb. South Korea articles (TOTAL): \", \n      df_kcdc_interp.shape[0])\n    \nif (MODE_SCRAPY_KCDC_EN | MODE_SCRAPY_KCDC_KR):\n    # save in CSV format\n    export_kcdc_csv(df_kcdc_interp)\n\n\n\n#####################\n## Latitude-Longitude\n#\n\ntry:\n    df_geo_kor = pd.read_csv(PATH_CSV_DF_GEO_KOR)\n    \nexcept:\n    try:\n        geolocator = Nominatim(user_agent=\"ScrapDataFromKoreaNews\")\n        df_geo_kor = pd.DataFrame(index=range(len(LIST_AREA)), columns=[\"area\"], \n                          data=LIST_AREA)\n        df_geo_kor[\"lat\"] = np.nan\n        df_geo_kor[\"lon\"] = np.nan\n        for area_curr in LIST_AREA:\n            location = geolocator.geocode(area_curr)\n            print(' ')\n            print(area_curr)\n            print(location.address)\n            print((location.latitude, location.longitude))\n            df_geo_kor.loc[df_geo_kor[\"area\"] == area_curr, \"lat\"] = \\\n                location.latitude\n            df_geo_kor.loc[df_geo_kor[\"area\"] == area_curr, \"lon\"] = \\\n                location.longitude\n        # save in CSV format\n        df_geo_kor.to_csv(PATH_CSV_DF_GEO_KOR, index=False)\n        print(\"Saved here : \", PATH_CSV_DF_GEO_KOR)\n    except:\n        df_geo_kor = pd.read_csv(\n            'https:\/\/raw.githubusercontent.com\/jeugregg\/coronavirusModel' + \\\n            '\/master\/df_geo_kor.csv')","34cb9b3f":"df_kcdc_interp.filter(items = [\"total\"]).tail(1)","3ef7a03a":"import plotly.graph_objects as go\nimport pandas as pd\nfrom plotly.colors import n_colors\nimport numpy as np\n\n# limit to last 30 days : \ndf_kcdc_table = df_kcdc_interp[df_kcdc_interp[\"date\"] > \\\n              add_days(df_kcdc_interp[\"date\"].astype(str).max(), -30)].copy()\n\ndf_kcdc_table[\"date\"] = df_kcdc_table[\"date\"].dt.strftime('%b %d')\ndf_kcdc_table_color = df_kcdc_table.fillna(0)\ndf_kcdc_table_color.dropna(inplace=True, how='all', subset=LIST_AREA)\ndf_kcdc_table = df_kcdc_table.fillna(\"?\")\ndf_kcdc_table.dropna(inplace=True, how='all', subset=LIST_AREA)\n\nnb_colors = 20\ncolors = n_colors('rgb(255, 240, 240)', 'rgb(200, 0, 0)', \n                  nb_colors, colortype='rgb')\n\ncolors_font = n_colors('rgb(0, 0, 0)', 'rgb(194, 198, 255)', \n                  nb_colors, colortype='rgb')\nscale_color = df_kcdc_interp[\"total\"].max() \/ (nb_colors - 1)\nlist_fill_colors = []\nlist_font_colors = []\nlist_col_table = [\"date\", \"total\"] + LIST_AREA \nlist_table=[]\nfor col_curr in list_col_table:\n    if col_curr == \"date\":\n        list_fill_colors.append(\\\n            np.array(colors)[np.zeros(df_kcdc_table_color.shape[0]).\\\n                             astype(np.int64)])\n        list_font_colors.append(\\\n            np.array(colors_font)[np.zeros(df_kcdc_table_color.shape[0]).\\\n                             astype(np.int64)])\n    else:\n        list_color_curr = (df_kcdc_table_color[col_curr]\/scale_color).\\\n            astype(np.int64).values\n        list_fill_colors.append(np.array(colors)[list_color_curr])\n        list_font_colors.append(np.array(colors_font)[list_color_curr])\n      \n    list_table.append(df_kcdc_table[col_curr])\n\n    \nfig = go.Figure(data=[go.Table(\n    \n    columnwidth=80,\n    \n    header=dict(values=list_col_table,\n                fill_color='paleturquoise',\n                align='left'),\n    cells=dict(values=list_table,\n               fill_color=list_fill_colors, #'lavender',\n               align='left',\n               font=dict(color=list_font_colors)))\n])\nfig.update_layout(width=80*len(list_col_table), \n                  height=int((df_kcdc_table.shape[0] + 1)*300\/12),\n                  title=\"Confirmed cases in South Korea\",\n                  margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n\n\nfig.show()","c559e93d":"import plotly.graph_objects as go\nimport plotly.express as px\n\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Create and style traces\nfig.add_trace(go.Scatter(x=df_kcdc_interp.index, \n                         y=df_kcdc_interp[\"total\"],\n                    mode='lines+markers',\n                    line_shape='linear',\n                    connectgaps=True, name=\"Total cases\"))\n# add delta \/ diff data\nfig.add_trace(go.Bar(x=df_kcdc_interp.index[1:], \n                    y=np.diff(df_kcdc_interp[\"total\"]), name=\"Daily cases\"), \n             secondary_y=True)\n# Edit the layout\nfig.update_layout(title='COVID-19 Confirmed cases (South Korea)',\n                   yaxis_title='Total cases')\nfig.update_layout(legend_orientation=\"h\", legend=dict(x=0, y=1.1))\n\nfig.update_yaxes(title_text=\"Daily cases\", range=[0, \n            np.nanmax(np.diff(df_kcdc_interp[\"total\"]))*2], secondary_y=True)\nfig.show()\n","42acaf7f":"df_kcdc_interp[\"country\"] = 'Korea, Rep.'\ndf_kcdc_interp[\"iso_alpha\"] = 'KOR'\n\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n# Create and style traces\nfig.add_trace(go.Scatter(x=df_kcdc_interp.index, \n                         y=df_kcdc_interp[\"Seoul\"],\n                    mode='lines+markers',\n                    line_shape='linear',\n                    connectgaps=True, name=\"Total cases\"))\n# add delta \/ diff data\nfig.add_trace(go.Bar(x=df_kcdc_interp.index[1:], \n                    y=np.diff(df_kcdc_interp[\"Seoul\"]), name=\"Daily cases\"),\n             secondary_y=True)\n# Edit the layout\nfig.update_layout(legend_orientation=\"h\", legend=dict(x=0, y=1.1))\nfig.update_layout(title='Seoul : COVID-19 Confirmed cases',\n                   yaxis_title='Total cases')\n\n# range on valid dates\nfig.update_xaxes(range=['2020-02-19', df_kcdc_interp.index[-1] + \\\n                        (df_kcdc_interp.index[-1]-df_kcdc_interp.index[-2])])\n\nfig.update_yaxes(title_text=\"Daily cases\", range=[0, \n            np.nanmax(np.diff(df_kcdc_interp[\"Seoul\"]))*2], \n                 secondary_y=True)\n\nfig.show()","f6af5ad2":"list_regions = LIST_AREA.copy()\nlist_regions.remove(\"Seoul\")\n#list_regions.remove(\"Daegu\")\n#list_regions.remove(\"Gyeongbuk\")\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\n# Create and style traces\nfor area_curr in list_regions:\n    \n    fig.add_trace(go.Scatter(x=df_kcdc_interp.index, \n                         y=df_kcdc_interp[area_curr],\n                    mode='lines+markers',\n                    name=area_curr,\n                    line_shape='linear',\n                    connectgaps=True))\n\n# Edit the layout\nfig.update_layout(title='Other areas of South Korea : COVID-19 Confirmed cases',\n                   yaxis_title='nb confirmed cases')\n# range on valid dates\nfig.update_xaxes(range=['2020-02-19', df_kcdc_interp.index[-1] + \\\n                        (df_kcdc_interp.index[-1]-df_kcdc_interp.index[-2])])\n\nfig.update_layout(legend_title='<b> Regions <\/b>')\nfig.update_layout(legend_orientation=\"h\")\nfig.show()","a3190143":"# limit to last 30 days : \ndf_regions = df_kcdc_interp[df_kcdc_interp[\"date\"] > \\\n              add_days(df_kcdc_interp[\"date\"].astype(str).max(), -30)].copy()\n#df_regions = df_kcdc_interp.copy()\n\ndf_regions = df_regions.melt(id_vars=['date'], value_vars=LIST_AREA, \n                             var_name=\"area\", value_name=\"nb_cases\")\n\ndf_regions = df_regions.join(df_geo_kor.set_index('area'), on='area')\ndf_regions.dropna(inplace=True, subset=[\"nb_cases\"])\ndf_regions = df_regions.fillna(0)\n\n#df_regions = df_regions[df_regions[\"date\"] > '2020-02-19']\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n# add animate bubble\nfig = px.scatter_geo(df_regions, lat=\"lat\", lon=\"lon\", color=\"nb_cases\",\n                     range_color=[0, df_kcdc_interp[\"total\"].max()+1], \n                     text=\"nb_cases\",\n                     hover_name=\"area\", size=\"nb_cases\", size_max=40, \n                     animation_frame=df_regions[\"date\"].dt.strftime('%b %d'), \n                     projection=\"natural earth\",\n            title=\"COVID-19 Confirmed cases in South Korea (Areas Animation)\")\n\n# add text \nfig['data'][0].update(mode='markers+text', textposition='bottom center', \n                     textfont={'color': \"red\"})\n\n# center map\nfig.update_geos(lonaxis_range=[ -6.0, 6.0] \\\n                + df_geo_kor[df_geo_kor[\"area\"] == \"Daegu\"][\"lon\"].values[0],\n                lataxis_range=[ -3, 2.5] + \\\n                df_geo_kor[df_geo_kor[\"area\"] == \"Daegu\"][\"lat\"].values[0],\n                landcolor = 'rgb(217, 217, 217)', \n                showocean=True, oceancolor=\"LightBlue\",\n               resolution=50)\n\n# colorize country\nfig.add_trace(go.Choropleth(\n        locationmode='country names',\n        locations=df_kcdc_interp[\"country\"],\n        z=df_kcdc_interp[\"total\"],\n        text=df_kcdc_interp['country'],\n        colorscale=[[0,'rgb(239, 239, 239)'],[1,'rgb(239, 239, 239)']],\n        autocolorscale=False,\n        showscale=False,\n        hoverinfo='skip',\n        geo='geo'\n    ))\n\nfig.update_layout(height=500, margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n\nfig.show()","f7e282b7":"%%capture\n########\n# import\n#\n\n# built-in libs\nimport shutil\nimport os\nimport io\nimport re\nimport math\nimport datetime\nimport locale\nimport platform\n# third-party libs\nimport requests\nimport pandas as pd\nimport numpy as np\n# for geo lat\/lon\nfrom geopy.geocoders import Nominatim\n# plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n# scrapy\ntry:\n    import scrapy\nexcept:\n    !pip install scrapy\n    import scrapy\n    \n########\n# PATCHS\n#\n\n# patch Month in french\n#locale.setlocale(locale.LC_ALL, 'fr_FR')\n# patch VSCODE issue : https:\/\/github.com\/Microsoft\/vscode-python\/issues\/5468\ncwd = os.getcwd()\nif cwd == '\/Users\/gregory':\n    os.chdir('\/Users\/gregory\/Documents\/CloudStationSinchon\/Applications\/python\/CoronaVirus\/code\/coronavirusModel')\n\n#############\n# DEFINITIONS\n#\n\n# for LOCAL mode : \nPATH_INPUT = 'sources'\nPATH_TO_SAVE_DATA = \".\"\n\nPATH_INPUT_KAGGLE = '\/kaggle\/input\/covid19-france-sources\/sources' #for kaggle\nPATH_TO_SAVE_DATA_KAGGLE = '\/kaggle\/working' #for kaggle\nPATH_GOOGLE_COLAB = '' #IF YOU USE GOOGLE COLAB, ENTER YOUR WORKING FOLDER\n# get computer name\nCOMPUTERNAME = platform.node()\n# select current platform\nMY_PLATFORM = platform.system()\n# check if Google colab need Drive ?\nif re.match(\"^\/content\", os.getcwd()):\n    MODE_PLATFORM = \"GOOGLE_COLAB\"\n    #print(\"GOOGLE COLAB MODE\")\n    from google.colab import drive\n    drive.mount('\/content\/drive', force_remount=True)\n    os.chdir(PATH_GOOGLE_COLAB)\n\nelif re.match(\"^\/kaggle\", os.getcwd()):\n    MODE_PLATFORM = \"KAGGLE\"\n    #print(\"KAGGLE COLAB MODE\")\n    PATH_INPUT = PATH_INPUT_KAGGLE \n    PATH_TO_SAVE_DATA = PATH_TO_SAVE_DATA_KAGGLE \n    !pip install pdfminer.six\nelse:\n    MODE_PLATFORM = \"LOCAL\"\n    \nLIST_METROPOLE = [\n'Auvergne-Rh\u00f4ne-Alpes',\n'Bourgogne-Franche-Comt\u00e9',\n'Bretagne',\n'Centre-Val de Loire',\n'Corse',\n'Grand Est',\n'Hauts-de-France',\n'Ile-de-France',\n'Normandie',\n'Nouvelle-Aquitaine',\n'Occitanie',\n'Pays de la Loire',\n\"Provence-Alpes-C\u00f4te d\u2019Azur\"]\n\nTOT_METROPOLE = \"Total M\u00e9tropole\"\n\nLIST_OUTREMER = [\n\"Guadeloupe\",\n\"Guyane\",\n\"Martinique\",\n\"Mayotte\",\n\"La R\u00e9union\",\n\"Saint-Barth\u00e9l\u00e9my\",\n\"Saint-Martin\" ]\n\nTOT_OUTREMER = \"Total Outre Mer\"\n\nLIST_SUBTOT = [\nTOT_METROPOLE,\nTOT_OUTREMER]\n\nLIST_AREA_FR = LIST_METROPOLE + LIST_OUTREMER\n\n# list of formatted area for scrapy Items\nLIST_ITEM = []\nfor area_curr in LIST_AREA_FR:\n    LIST_ITEM.append(area_curr.replace(\"-\",\"_\").replace(\"\u00f4\",\"o\").\\\n    replace(\"\u00e9\",\"e\").replace(\"\u2019\",\"_\").replace(\" \",\"_\"))\n\nPATH_CSV = PATH_INPUT + '\/csv_fr'\nLIST_FILE_CSV = ['spf_web_front_2020_02_28.csv', 'spf_web_front_2020_03_02.csv']\n\nPATH_PDF = PATH_INPUT + '\/pdf_fr'\nPATH_SPF_FRONT_OUTPUT = PATH_TO_SAVE_DATA + '\/' + 'table_spf_front_update.json'\n\nURL_SPF_FRONT = \\\n    'https:\/\/www.santepubliquefrance.fr\/maladies-et-traumatismes\/' + \\\n    'maladies-et-infections-respiratoires\/infection-a-coronavirus\/articles\/' + \\\n    'infection-au-nouveau-coronavirus-sars-cov-2-covid-19-france-et-monde'\n\nLIST_COL_FRA = [\"date\", \"url\", \"source\", \"country\",\"iso_alpha\", \"total\"] + \\\n    LIST_SUBTOT + \\\n    LIST_METROPOLE + LIST_OUTREMER + [\"tot_metrop_calc\", \"tot_outre_calc\", \n        \"check_metrop\", \"check_outre\", \"check_tot\"]\n\nNB_POS_DATE_MIN_DF_FEAT = 140227 # on 12\/05\/2020\n\nURL_CSV_GOUV_FR = \"https:\/\/www.data.gouv.fr\" + \\\n    \"\/fr\/datasets\/r\/406c6a23-e283-4300-9484-54e78c8ae675\"\n\nPATH_DF_GOUV_FR_RAW = PATH_TO_SAVE_DATA + '\/' + 'df_gouv_fr_raw.csv'\n\nPATH_DF_FR = PATH_TO_SAVE_DATA + '\/' + 'df_fr.csv'\nURL_DF_FR = \"https:\/\/raw.githubusercontent.com\/\" + \\\n    \"jeugregg\/coronavirusModel\/master\/df_fr.csv\"\nPATH_CSV_DF_GEO_FRA = PATH_TO_SAVE_DATA + '\/' + 'df_geo_fra.csv'\n\nPATH_DF_POS_FR = PATH_TO_SAVE_DATA + '\/' + 'df_pos_fr.csv'\nPATH_DF_TEST_FR = PATH_TO_SAVE_DATA + '\/' + 'df_test_fr.csv'\n\n# RETRIEVED DATA FROM WORLDOMETERS\nPATH_TABLES_WORLDO = PATH_TO_SAVE_DATA + '\/' + 'table_worldo_fr.json'\n\n\n\n##################\n# useful functions\n#\n\n# save before scraping\ndef clean_file(path_file_name):\n    '''\n    Clean file already traited : rename file with date\n    '''\n    try:\n        d = datetime.datetime.now()\n        str_date = '_' + d.strftime(\"%Y%m%d_%H_%M_%S\")\n       \n        res_re = re.search('\\.\\w+$', path_file_name)\n        \n        path_file_name_saved = \\\n            path_file_name[0:res_re.start()] + str_date + res_re.group(0)\n         \n        shutil.move(path_file_name, path_file_name_saved) \n        print('File {} moved!'.format(path_file_name_saved))\n    except:\n        print('File {} does not exist!'.format(path_file_name))\n\n# manage accents\ndef conv_noaccent(chaine):\n    '''\n    Convert string without accents\n\n    ex: chaine = \"m\u00e0 ch\u00e0\u00eene \u00e2v\u00e8c d\u00e8s c\u00e0r\u00e4ct\u00e8res sp\u00e9ci\u00e2\u00fcx\"\n        print(conv_noaccent(chaine))\n        returns : 'ma chaine avec des caracteres speciaux'\n    '''\n    import unicodedata\n    \n    return unicodedata.normalize('NFKD', \n                                 chaine).encode('ASCII', \n                                                'ignore').decode('ASCII')\n    \n#chaine = \"m\u00e0 ch\u00e0\u00eene \u00e2v\u00e8c d\u00e8s c\u00e0r\u00e4ct\u00e8res sp\u00e9ci\u00e2\u00fcx\"\n#print(conv_noaccent(chaine))\n\ndef create_df_tables_fr():\n    #\n    # Create a dataFrame to load data\n    #\n    return pd.DataFrame(columns=LIST_COL_FRA, index=[0])\n\ndef update_df_fr(df_add):\n    #\n    # Update dataFrame save date by date for areas of France\n    #\n    # First : \n    # If save file doesn't exsit create it\n    # else  load it.\n    # After : \n    # if new date, add the row\n    # else update the row.\n    # \n    if not os.path.exists(PATH_DF_FR):\n        if MODE_PLATFORM == \"KAGGLE\":\n            df_fr = pd.read_csv(URL_DF_FR)\n        else:\n            df_fr = df_add.copy()\n    else:\n        df_fr = pd.read_csv(PATH_DF_FR)\n        # add \/ update \n        for iloc_idx in range(df_add.shape[0]):\n            date_curr = df_add.iloc[iloc_idx][\"date\"]\n            # if date exist, replace\n            if date_curr in df_fr[\"date\"].values:\n                # num row in df_fr \n                num_row_df_fr = df_fr[\"date\"].isin([date_curr]).values.argmax()\n                # update this row\n                df_fr.iloc[num_row_df_fr] = df_add.iloc[iloc_idx]\n            # if new date, add\n            else: \n                df_fr = df_fr.append(df_add.iloc[iloc_idx])\n    # sort by date\n    df_fr.sort_values(by=['date'], inplace=True)\n    # reindex\n    df_fr.reset_index(drop=True, inplace=True)\n    # save old csv file\n    clean_file(PATH_DF_FR)\n    # save csv\n    df_fr.to_csv(PATH_DF_FR, index=False)\n\n    return df_fr\n\n###################################\n# CSV file for old datas (from spf)\n#\ndf_tables_csv_fr = create_df_tables_fr()\n\n\ndf_tables_csv_fr.at[0, \"date\"] = '2020-02-28'\ndf_tables_csv_fr.at[0, \"url\"] = 'https:\/\/web.archive.org\/web\/20200229212809\/https:\/\/www.santepubliquefrance.fr\/maladies-et-traumatismes\/maladies-et-infections-respiratoires\/infection-a-coronavirus\/articles\/covid-19-situation-epidemiologique-en-france'\n\ndf_tables_csv_fr.at[1, \"date\"] = '2020-03-02'\ndf_tables_csv_fr.at[1, \"url\"] = 'https:\/\/web.archive.org\/web\/20200303124836\/https:\/\/www.santepubliquefrance.fr\/maladies-et-traumatismes\/maladies-et-infections-respiratoires\/infection-a-coronavirus\/articles\/covid-19-situation-epidemiologique-en-france'\n\ndf_tables_csv_fr[\"source\"] = \"santepubliquefrance.fr\"\ndf_tables_csv_fr[\"country\"] = \"France\"\ndf_tables_csv_fr[\"iso_alpha\"] = \"FRA\"\n\n\n\nfor idx_csv in range(len(LIST_FILE_CSV)):\n    df_csv = pd.read_csv(PATH_CSV + '\/' + LIST_FILE_CSV[idx_csv])\n    # total\n    df_tables_csv_fr.at[idx_csv, 'total'] = df_csv[df_csv[\"area\"] == \"Total\"][\"nb_cases\"].values[0]\n    # total metropole\n    df_tables_csv_fr.at[idx_csv, TOT_METROPOLE] =  \\\n        df_csv[df_csv[\"area\"] == 'France m\u00e9tropolitaine'][\"nb_cases\"].values[0]\n    for area_curr in LIST_METROPOLE + LIST_OUTREMER:\n        nb_cases_found = df_csv[df_csv[\"area\"] == area_curr][\"nb_cases\"].values\n        if nb_cases_found.shape[0] == 0:\n            nb_cases_found = 0\n        else:\n            nb_cases_found = nb_cases_found[0]\n        df_tables_csv_fr.at[idx_csv, area_curr] = nb_cases_found\n    \n    # tot outremer\n    df_tables_csv_fr[TOT_OUTREMER] = df_tables_csv_fr.filter(items=LIST_OUTREMER).sum(axis=1)\n\n# add csv data to final dataframe\ndf_fr = update_df_fr(df_tables_csv_fr)   \n\n\n###################\n# PDF SOURCES FILES\n# \n\n# check DATA input folder\nlist_pdf = []\nfor dirname, _, filenames in os.walk(PATH_PDF):\n    for filename in filenames:\n        list_pdf.append(os.path.join(dirname, filename))\n        \ndf_pdf_fr = pd.DataFrame(columns=[\"url\", \"source\", \"path\"], index=range(len(list_pdf)))\ndf_pdf_fr[\"source\"] = \"santepubliquefrance.fr\"\ndf_pdf_fr[\"path\"] = list_pdf\n\n###############\n# FROM JHU CSSE\n#\ndf_cases_fr, df_death_fr = extract_data_world(df_world_melt, df_death_melt,\n                                              \"Country\/Region\", \"France\")","e1b85f6d":"%%capture\n%%writefile pdfminer_to_text.py\nimport io\n\nfrom pdfminer.converter import TextConverter\nfrom pdfminer.converter import HTMLConverter\nfrom pdfminer.pdfinterp import PDFPageInterpreter\nfrom pdfminer.pdfinterp import PDFResourceManager\nfrom pdfminer.pdfpage import PDFPage\n\ndef extract_text_from_pdf(pdf_path):\n    resource_manager = PDFResourceManager()\n    fake_file_handle = io.StringIO()\n    converter = TextConverter(resource_manager, fake_file_handle)\n    page_interpreter = PDFPageInterpreter(resource_manager, converter)\n    \n    with open(pdf_path, 'rb') as fh:\n        for page in PDFPage.get_pages(fh, \n                                      caching=True,\n                                      check_extractable=True):\n            page_interpreter.process_page(page)\n            \n        text = fake_file_handle.getvalue()\n    \n    # close open handles\n    converter.close()\n    fake_file_handle.close()\n    \n    if text:\n        return text\n\ndef extract_html_from_pdf(pdf_path):\n    resource_manager = PDFResourceManager()\n    fake_file_handle = io.StringIO()\n    converter = HTMLConverter(resource_manager, fake_file_handle)\n    page_interpreter = PDFPageInterpreter(resource_manager, converter)\n    \n    with open(pdf_path, 'rb') as fh:\n        for page in PDFPage.get_pages(fh, \n                                      caching=True,\n                                      check_extractable=True):\n            page_interpreter.process_page(page)\n            \n        text = fake_file_handle.getvalue()\n    \n    # close open handles\n    converter.close()\n    fake_file_handle.close()\n    \n    if text:\n        return text\n    \nif __name__ == '__main__':\n    print(extract_text_from_pdf('w9.pdf'))","dd856cc7":"%%capture\n###########\n# READ PDFs\n#\n\nfrom pdfminer_to_text import *\n\ndef find_pdf_fr_total(str_text):\n    #\n    # Find french total into text of PDF\n    #\n    \n    # \"Cas confirm\u00e9s Nombre de cas 949\"\n    sub_pre = \"Cas confirm\u00e9s Nombre de cas\"\n    str_pattern = \"(?<={})\\s+\\d+\".format(sub_pre)\n    list_found = re.search(str_pattern, str_text)\n    if list_found != None:\n        return np.int(list_found[0].strip())\n    else:\n        return np.nan\n\ndef find_pdf_fr_date(str_text):\n    #\n    # Find french date into texte of PDF \n    #\n    re_found = re.search(\"(?<=Point\\s\u00e9pid\u00e9miologique\\s-\\sSituation\\sau\\s)\\d+\\s\\w+\\s\\d\\d\\d\\d\", str_text)\n    if re_found != None:\n        str_date_fr = re_found[0]\n        print(str_date_fr)\n        str_date_fr = str_date_fr.replace(\"mars\", \"March\") # DIRTY PATCH... TODO \n        return datetime.datetime.strptime(str_date_fr,'%d %B %Y').strftime(\"%Y-%m-%d\")\n    else:\n        return None\n\n\nfor idx_pdf in range(df_pdf_fr.shape[0]):\n\n    pdf_file = df_pdf_fr.at[idx_pdf, \"path\"]\n    str_text = extract_text_from_pdf(pdf_file)\n    if idx_pdf == 0:\n        df_tables_pdf_fr = create_df_tables_fr()\n\n    df_tables_pdf_fr.at[idx_pdf, \"date\"] = find_pdf_fr_date(str_text)\n    df_tables_pdf_fr.at[idx_pdf, \"country\"] = \"France\"\n    df_tables_pdf_fr.at[idx_pdf, \"iso_alpha\"] = \"FRA\"\n    df_tables_pdf_fr.at[idx_pdf, \"total\"] = find_pdf_fr_total(str_text)\n    df_tables_pdf_fr.at[idx_pdf, \"source\"] = 'santepubliquefrance.fr'\n    #R\u00e9gion   Auvergne-Rh\u00f4ne-Alpes 102 Bourgogne-Franche-Comt\u00e9 129 Bretagne 40 \n    #Centre-Val de Loire  16 Corse 5 Grand Est 250 Hauts-de-France 173 \n    #Ile-de-France 104 Normandie 11 Nouvelle-Aquitaine 17 Occitanie 36 \n    #Pays de la Loire 18 Provence-Alpes-C\u00f4te d\u2019Azur 38 Total M\u00e9tropole  939 \n    #Guadeloupe 0 Guyane 5 Martinique 2 Mayotte 0 La R\u00e9union  0 Saint-Barth\u00e9lemy\n    #1 Saint-Martin 2 Total Outre Mer 10\n\n    for reg_curr in LIST_METROPOLE + LIST_OUTREMER + LIST_SUBTOT:\n        str_pattern = \"(?<={})\\s+\\d+\".format(conv_noaccent(reg_curr))\n        list_found = re.search(str_pattern, conv_noaccent(str_text))\n        if list_found != None:\n            df_tables_pdf_fr.at[idx_pdf, reg_curr] = \\\n                np.int(list_found[0].strip())\n        else:\n            df_tables_pdf_fr.at[idx_pdf, reg_curr] = 0\n    # check total\n    if df_tables_pdf_fr.at[idx_pdf, \"total\"] is np.nan:\n        df_tables_pdf_fr.at[idx_pdf, \"total\"] = \\\n            df_tables_pdf_fr.at[idx_pdf, TOT_METROPOLE] + \\\n            df_tables_pdf_fr.at[idx_pdf, TOT_OUTREMER]\n    # check total metropole\n    df_tables_pdf_fr.at[idx_pdf, \"tot_metrop_calc\"] = 0\n    for reg_curr in LIST_METROPOLE:\n        df_tables_pdf_fr.at[idx_pdf, \"tot_metrop_calc\"] = \\\n            df_tables_pdf_fr.at[idx_pdf, \"tot_metrop_calc\"] + \\\n            df_tables_pdf_fr.at[idx_pdf, reg_curr]\n    df_tables_pdf_fr.at[idx_pdf, \"check_metrop\"] = \\\n        df_tables_pdf_fr.at[idx_pdf, \"tot_metrop_calc\"] == \\\n        df_tables_pdf_fr.at[idx_pdf, TOT_METROPOLE]\n    # check total outre mer\n    df_tables_pdf_fr.at[idx_pdf, \"tot_outre_calc\"] = 0\n    for reg_curr in LIST_OUTREMER:\n        df_tables_pdf_fr.at[idx_pdf, \"tot_outre_calc\"] = \\\n            df_tables_pdf_fr.at[idx_pdf, \"tot_outre_calc\"] + \\\n            df_tables_pdf_fr.at[idx_pdf, reg_curr]\n    df_tables_pdf_fr.at[idx_pdf, \"check_outre\"] = \\\n        df_tables_pdf_fr.at[idx_pdf, \"tot_outre_calc\"] == \\\n        df_tables_pdf_fr.at[idx_pdf, TOT_OUTREMER]\n    # check Total\n    df_tables_pdf_fr.at[idx_pdf, \"check_tot\"] = \\\n        df_tables_pdf_fr.at[idx_pdf, \"total\"] == \\\n        df_tables_pdf_fr.at[idx_pdf, TOT_METROPOLE] + \\\n        df_tables_pdf_fr.at[idx_pdf, TOT_OUTREMER] \n    # final flag OK\n    df_tables_pdf_fr.at[idx_pdf, \"flag_ok\"] = \\\n        df_tables_pdf_fr.at[idx_pdf, \"check_outre\"] & \\\n        df_tables_pdf_fr.at[idx_pdf, \"check_metrop\"] & \\\n        df_tables_pdf_fr.at[idx_pdf, \"check_tot\"]\n\n# add csv data to final dataframe\ndf_fr = update_df_fr(df_tables_pdf_fr)\n","e3b4cf74":"%%capture\n%%writefile spfwebspider.py\n# -*-coding:utf-8 -*\n\n#\n# Module for scrapy over tables in santepublique webpage\n#\n\n# import\nimport os\nimport re\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport locale\n# patch mois en francais\n#locale.setlocale(locale.LC_ALL, 'fr_FR')\nimport scrapy\nfrom scrapy.loader import ItemLoader\nfrom scrapy.loader.processors import MapCompose, Join, TakeFirst\nfrom w3lib.html import remove_tags\n\n# definitions \nURL_SPF_FRONT = \\\n    'https:\/\/www.santepubliquefrance.fr\/maladies-et-traumatismes\/' + \\\n    'maladies-et-infections-respiratoires\/infection-a-coronavirus\/articles\/' + \\\n    'infection-au-nouveau-coronavirus-sars-cov-2-covid-19-france-et-monde'\n\nPATH_SPF_FRONT_OUTPUT = os.getcwd() + '\/' + 'table_spf_front_update.json'\n\nURL_FIRST_PAGE = URL_SPF_FRONT\n\nLIST_METROPOLE = [\n'Auvergne-Rh\u00f4ne-Alpes',\n'Bourgogne-Franche-Comt\u00e9',\n'Bretagne',\n'Centre-Val de Loire',\n'Corse',\n'Grand Est',\n'Hauts-de-France',\n'Ile-de-France',\n'Normandie',\n'Nouvelle-Aquitaine',\n'Occitanie',\n'Pays de la Loire',\n\"Provence-Alpes-C\u00f4te d\u2019Azur\"]\n\nLIST_OUTREMER = [\n\"Guadeloupe\",\n\"Guyane\",\n\"Martinique\",\n\"Mayotte\",\n\"La R\u00e9union\",\n\"Saint-Barth\u00e9l\u00e9my\",\n\"Saint-Martin\" ]\n\nLIST_AREA_FR = LIST_METROPOLE + LIST_OUTREMER\n\n# list of formatted area for scrapy Items\nLIST_ITEM = []\nfor area_curr in LIST_AREA_FR:\n    LIST_ITEM.append(area_curr.replace(\"-\",\"_\").replace(\"\u00f4\",\"o\").\\\n    replace(\"\u00e9\",\"e\").replace(\"\u2019\",\"_\").replace(\" \",\"_\"))\n\n# date\n#'\/\/div[@class=\"article__main-content\"]\/\/h4[contains(.,\"Nombre de cas rapport\u00e9s par r\u00e9gion\")]'\n# \"Nombre de cas rapport\u00e9s par r\u00e9gion au 10\/03\/2020 \u00e0 15h (donn\u00e9es Sant\u00e9 publique France)\"\ndef find_sp_date_fr(str_found):\n    #\n    #  Parse texte from scrapy for date of data for web page santepublique.fr\n    #\n    # ex : str_found = \"Nombre de cas rapport\u00e9s par r\u00e9gion au 10\/03\/2020 \u00e0 15h (donn\u00e9es Sant\u00e9 publique France)\"\n    #   find_sp_date_fr(str_found)\n    #   returns : '2020-03-10'\n\n    re_found = re.search(\\\n        \"(?<=Nombre de cas rapport\u00e9s par r\u00e9gion au )\\d\\d\/\\d\\d\/\\d\\d\\d\\d\", \n        str_found)\n    if re_found is None:\n        return None\n    else:\n        str_date_fr = re_found[0]\n        return datetime.datetime.strptime(str_date_fr,\n                                          '%d\/%m\/%Y').strftime(\"%Y-%m-%d\")\n\n# number value    \ndef convert_str_int(str_curr):\n    '''\n    To convert from dirty string to number\n    \n    exemple : \"dzd 345**\" --> 345 integer\n    '''\n    # first test : it is not string ?\n    if isinstance(str_curr, str):\n        str_clean = ''.join(re.findall('[\\d]', str_curr))\n        try:\n            return int(str_clean)\n        except:\n            return np.nan\n    \n        return val_curr\n    \n    else:\n        return np.nan\n\n# Scrapy item class\nclass SantePubliqueitem(scrapy.Item):\n    '''\n    Class item to declare different information to scrap\n    and how to process (as input or output)\n    '''\n    # define the fields for your item here like:\n    url = scrapy.Field(output_processor=TakeFirst())\n    \n    source = scrapy.Field(output_processor=TakeFirst())\n    \n    date_published = scrapy.Field(output_processor=TakeFirst())\n    \n    total = scrapy.Field(output_processor=TakeFirst())\n    \n    Auvergne_Rhone_Alpes = scrapy.Field(output_processor=TakeFirst())\n    Bourgogne_Franche_Comte = scrapy.Field(output_processor=TakeFirst())\n    Bretagne = scrapy.Field(output_processor=TakeFirst())\n    Centre_Val_de_Loire = scrapy.Field(output_processor=TakeFirst())\n    Corse = scrapy.Field(output_processor=TakeFirst())\n    Grand_Est = scrapy.Field(output_processor=TakeFirst())\n    Hauts_de_France = scrapy.Field(output_processor=TakeFirst())\n    Ile_de_France = scrapy.Field(output_processor=TakeFirst())\n    Normandie = scrapy.Field(output_processor=TakeFirst())\n    Nouvelle_Aquitaine = scrapy.Field(output_processor=TakeFirst())\n    Occitanie = scrapy.Field(output_processor=TakeFirst())\n    Pays_de_la_Loire = scrapy.Field(output_processor=TakeFirst())\n    Provence_Alpes_Cote_d_Azur = scrapy.Field(output_processor=TakeFirst())\n    Guadeloupe = scrapy.Field(output_processor=TakeFirst())\n    Guyane = scrapy.Field(output_processor=TakeFirst())\n    Martinique = scrapy.Field(output_processor=TakeFirst())\n    Mayotte = scrapy.Field(output_processor=TakeFirst())\n    La_Reunion = scrapy.Field(output_processor=TakeFirst())\n    Saint_Barthelemy = scrapy.Field(output_processor=TakeFirst())\n    Saint_Martin = scrapy.Field(output_processor=TakeFirst()) \n    \n# Scrapy Spider class\nclass SantePubliqueTablesSpider(scrapy.Spider):\n    '''\n    Spider to scrap tables webpages : \n    - how to find information for scraping\n    - which field names to store\n    '''\n    # Your spider definition\n    name = 'tables_santepublique_spider'\n    # output definition :\n    custom_settings = {\n      'FEED_FORMAT': 'json',\n      'FEED_URI': PATH_SPF_FRONT_OUTPUT\n  }\n    # urls to scrap\n    start_urls = [URL_FIRST_PAGE]\n   \n    def parse(self, response):\n        '''\n        Parse definition with xpath which define all patterns to use\n        for retrieve information into HTML strings\n        '''\n        #url\tsource\tauthor\ttitle\ttheme\tdescription\tdate_published\tbody   \n        l = ItemLoader(item=SantePubliqueitem(), selector=response)\n        \n        l.add_value('url', response.url)\n        \n        l.add_value('source', 'santepubliquefrance.fr')\n        \n        # date\n        # first try with new structure with h2 tag\n        str_xp_date_1 = '\/\/div[@class=\"article__main-content\"]\/\/h2[contains' + \\\n            '(.,\"Nombre de cas rapport\u00e9s par r\u00e9gion\")]\/span[1]\/text()'\n        xp_found = response.xpath(str_xp_date_1).get() \n        # if first try not found, try old date format with h4 tag\n        if xp_found is None:\n            str_xp_date = '\/\/div[@class=\"article__main-content\"]' + \\\n                '\/\/h4[contains(.,\"Nombre de cas rapport\u00e9s par r\u00e9gion\")]\/\/text()'\n            xp_found = response.xpath(str_xp_date).get()\n        if xp_found is None:\n            l.add_value('date_published', np.nan)\n        else:\n            l.add_value('date_published', find_sp_date_fr(xp_found))\n        \n        # total \n        #exemple : Depuis le 21 janvier 2020, 2876 cas COVID-19 ont \u00e9t\u00e9 confirm\u00e9s, incluant 61 d\u00e9c\u00e8s\n        #str_xp_tot = '\/\/div[@class=\"article__main-content\"]\/div' + \\\n        #    '\/p[contains(.,\"cas COVID-19 ont \u00e9t\u00e9 confirm\u00e9s\")]\/\/text()'\n        # exemple 25\/03\/2020 : \"22 302 cas de COVID-19\"\n        \n        #list_str_tot = [\"cas COVID-19 ont \u00e9t\u00e9 confirm\u00e9s\", \n        #                \"cas de COVID-19  ont \u00e9t\u00e9 diagnostiqu\u00e9s\"]\n        dict_str_tot = dict()\n        \n        dict_str_tot[0] = dict()\n        dict_str_tot[0][\"str_re_text\"] = [\"cas COVID-19 ont \u00e9t\u00e9 confirm\u00e9s\"]\n        dict_str_tot[0][\"str_re_num\"] = \"cas COVID-19 ont \u00e9t\u00e9 confirm\u00e9s\"\n        \n        dict_str_tot[1] = dict()\n        dict_str_tot[1][\"str_re_text\"] = [\"cas de COVID-19\", \n                                          \"ont \u00e9t\u00e9 diagnostiqu\u00e9s\"]\n        dict_str_tot[1][\"str_re_num\"] = \"cas de COVID-19\"\n        \n        re_found = None\n        \n        for key_curr in dict_str_tot.keys():\n            str_xp_tot = '\/\/p'\n            for str_curr in dict_str_tot[key_curr][\"str_re_text\"]:\n                str_xp_tot += '[contains(.,\"' + str_curr + '\")]'\n            str_xp_tot += '\/\/text()'\n            print(\"str_xp_tot : \", str_xp_tot)\n            xp_found = response.xpath(str_xp_tot).get()\n            if xp_found != None:\n                re_found = \\\n                    re.search(\"\\d*\\s*\\d+(?=\\s\" + \\\n                              dict_str_tot[key_curr][\"str_re_num\"] + \")\", \n                              xp_found)\n                break\n                \n        if re_found is None:\n            l.add_value('total', np.nan)\n        else:\n            l.add_value('total', convert_str_int(re_found[0]))\n                \n        # areas\n        for item_curr, area_curr in zip(LIST_ITEM, LIST_AREA_FR):\n            str_xp_area = '\/\/div[@class=\"article__main-content\"]' + \\\n                '\/\/td[contains(.,\"{}\")]'.format(area_curr) + \\\n                '\/following-sibling::td[1]\/\/text()'\n            xp_found = response.xpath(str_xp_area).get()\n            if xp_found is None:\n                l.add_value(item_curr, np.nan)\n            else:\n                l.add_value(item_curr, convert_str_int(xp_found))\n                #l.add_value(item_curr, int(xp_found.replace(\" \", \"\"))) \n\n        yield l.load_item()\n","9e1cccd2":"%%capture\n%%writefile run_spfwebspider.py\n# -*-coding:utf-8 -*\n\n#\n# Run scrapy for retrieve table from santepubliquefrance.fr (1 page)\n#\n\n# import\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom spfwebspider import SantePubliqueTablesSpider\n\n# definitions\n\n# get urls pages links\n\n# define url for scrapy\n\n# start scraping process\nprocess = CrawlerProcess()\nprocess.crawl(SantePubliqueTablesSpider)\nprocess.start() # the script will block here until the crawling is finished","6205d643":"%%capture\n%%time\n#######################\n## Scrap spf front table\n#\ntry:\n    # clean (move file if exist)\n    clean_file(PATH_SPF_FRONT_OUTPUT)\n    # execute external process (FALLBACK for scrapy problem)\n    !python run_spfwebspider.py \nexcept:\n    print(\"Error run_spfwebspider !\")","0b9f21fc":"%%capture\n##############################\n# SPF FRONT PAGE : UPDATE DATA\n#\n\n# add data from front web page to dataFrame france\n\ndf_spf_front_fr = pd.read_json(PATH_SPF_FRONT_OUTPUT)\ndf_spf_front_fr = df_spf_front_fr.sort_values(by=['date_published'])\n\n#\n# convert in final dataFrame format\n#\ndf_tables_web_fr = create_df_tables_fr()\ndf_tables_web_fr[\"date\"] = df_spf_front_fr['date_published'].copy()\ndf_tables_web_fr[\"url\"] = df_spf_front_fr['url'].copy()\ndf_tables_web_fr[\"source\"] = df_spf_front_fr['source'].copy()\ndf_tables_web_fr[\"country\"] = \"France\"\ndf_tables_web_fr[\"iso_alpha\"] = \"FRA\"\ndf_tables_web_fr[\"total\"] = df_spf_front_fr['total'].copy()\n# Nan : 'tot_metrop_calc', 'tot_outre_calc', 'check_metrop', 'check_outre'\n\n# for areas : \nfor idx_col in range(len(LIST_AREA_FR)):\n\n    df_tables_web_fr[LIST_AREA_FR[idx_col]] = df_spf_front_fr[LIST_ITEM[idx_col]].copy()\n\n# add sub-total \ndf_tables_web_fr[TOT_METROPOLE] = \\\n    df_tables_web_fr.filter(items=LIST_METROPOLE).sum(axis=1)[0]\ndf_tables_web_fr[TOT_OUTREMER] = \\\n    df_tables_web_fr.filter(items=LIST_OUTREMER).sum(axis=1)[0]\n\n# add web front data to final dataframe\n# clean nan \nif df_tables_web_fr[df_tables_web_fr[\"date\"].notna() & \\\n                 df_tables_web_fr[\"total\"].notna()].shape[0] > 0:\n    df_fr = update_df_fr(df_tables_web_fr)\n","6f9f4876":"##############################\n# OPENCOVID19_FR : UPDATE DATA \n#\n\n# add data from front web page to dataFrame france\nURL_JSON_OPENCOVID19_FR = 'https:\/\/raw.githubusercontent.com\/' + \\\n    'opencovid19-fr\/data\/master\/dist\/chiffres-cles.csv'\n\ndf_open_fr = pd.read_csv(URL_JSON_OPENCOVID19_FR)\ndf_open_fr = df_open_fr.sort_values(by=['date'])\n# adaptation\ndf_open_fr.loc[df_open_fr[\"maille_nom\"] == '\u00cele-de-France', \"maille_nom\"] = \\\n    'Ile-de-France' \ndf_open_fr.loc[df_open_fr[\"maille_nom\"] == \"Provence-Alpes-C\u00f4te d'Azur\", \\\n    \"maille_nom\"] = 'Provence-Alpes-C\u00f4te d\u2019Azur'\ndf_open_fr.loc[df_open_fr[\"maille_nom\"] == 'Saint-Barth\u00e9lemy', \\\n    \"maille_nom\"] = 'Saint-Barth\u00e9l\u00e9my'\n\n# check date \n\ndef apply_date_format(str_in):\n    #\"2020-11-19\"\n    return re.sub('\\D', '-',str_in)\n\ndf_open_fr[\"date\"] = df_open_fr[\"date\"].apply(apply_date_format)\n\n# check regions\nlist_area_open = df_open_fr[\"maille_nom\"].unique().tolist()\nlist_date = df_open_fr[\"date\"].unique().tolist()\nfor area_curr in LIST_METROPOLE + LIST_OUTREMER:\n    if area_curr not in list_area_open:\n        #print(\"{} not found!\".format(area_curr))\n        continue\n    \n    for date_curr in list_date:\n        nb_curr = df_open_fr[(df_open_fr[\"date\"] == date_curr) & \\\n                  (df_open_fr[\"maille_nom\"] == area_curr)].shape[0]\n\nrange_days = pd.date_range(start=df_fr[\"date\"].max(), \n                            end=df_open_fr[\"date\"].max())\nrange_days = range_days.strftime(\"%Y-%m-%d\")\nif range_days.shape[0] > 1:\n    #pass\n    range_days = range_days[1:]\nelse:\n    range_days = None\n\nif range_days is not None:\n    df_tables_open_fr = create_df_tables_fr()\n    for K, day_curr in enumerate(range_days):\n        df_tables_open_fr.at[K, \"date\"] = day_curr\n    \n    for area_curr in LIST_METROPOLE + LIST_OUTREMER:\n        \n        s_conf_curr = df_open_fr[df_open_fr[\"maille_nom\"] == area_curr] \\\n            .groupby(\"date\")[\"cas_confirmes\"].max()\n        #print(\"area_curr : {} \/ n= {}\".format(area_curr, s_conf_curr.shape[0]))\n        for day_curr in range_days:\n            if day_curr in s_conf_curr.index.tolist():\n                #print(\"day_curr={}, n_conf={}\".format(day_curr, s_conf_curr.loc[day_curr]))\n                df_tables_open_fr.at[df_tables_open_fr[\"date\"] == day_curr,\n                                     area_curr] = s_conf_curr.loc[day_curr]\n            \ndf_tables_open_fr[\"source\"] = \"OPENCOVID19_FR\"\ndf_tables_open_fr[\"country\"] = \"France\"\ndf_tables_open_fr[\"iso_alpha\"] = \"FRA\"\n","c69c98f7":"##############################\n# GOUV FR : UPDATE DATA \n#http:\/\/localhost:8888\/notebooks\/Documents\/CloudStationSinchon\/Applications\/python\/CoronaVirus\/code\/coronavirusModel\/coronavirus-visualization-modeling.ipynb#\n# New source : \"https:\/\/www.data.gouv.fr\/fr\/datasets\/r\/406c6a23-e283-4300-9484-54e78c8ae675\"\n# https:\/\/www.data.gouv.fr\/fr\/datasets\/donnees-relatives-aux-resultats-des-tests-virologiques-covid-19\/#_\n\ndef get_data_gouv_fr():\n    '''\n    Get from Gouv  SFP page data cases in France \n    Clean & Save\n    '''\n    # patch 29\/07\/2020 : SSL error patch\n    req = requests.get(URL_CSV_GOUV_FR).content\n    df_gouv_fr_raw = pd.read_csv(io.StringIO(req.decode('utf-8')), sep=\";\", \n        low_memory=False) # patch dtype 2020-09-08\n\n    # past treat data upper cases -> lower cases\n    if \"t\" not in df_gouv_fr_raw.columns:\n        df_gouv_fr_raw[\"t\"] =  df_gouv_fr_raw[\"T\"]\n    if \"p\" not in df_gouv_fr_raw.columns:\n        df_gouv_fr_raw[\"p\"] =  df_gouv_fr_raw[\"P\"]\n    # patch : clear data in double !!!\n    df_gouv_fr_raw = df_gouv_fr_raw[df_gouv_fr_raw[\"cl_age90\"] != 0]\n\n    df_gouv_fr_raw.to_csv(PATH_DF_GOUV_FR_RAW, index=False)\n\n    return df_gouv_fr_raw\n\ndef compute_sum_dep(df_in):\n    df_in[\"daily\"] = 0\n    list_dep = []\n    for col_curr in df_in.columns:\n        if re.search(\"^\\d\", col_curr):\n            list_dep.append(col_curr)\n    for dep_curr in list_dep:\n        df_in[\"daily\"]  += df_in[dep_curr]\n    return df_in\n\ndef precompute_data_pos(df_gouv_fr_raw, nb_pos_start=NB_POS_DATE_MIN_DF_FEAT,\n        path_df_pos_fr=PATH_DF_POS_FR, path_df_test_fr=PATH_DF_TEST_FR):\n    '''Pre-compute data from Sante Publique France'''\n    # creation of table data : 't':tested 'p':positive\n    # data =  f(line : date, dep \/ col: t) => f(line : date \/ col: dep = f(t)) \n    pt_fr_test = pd.pivot_table(df_gouv_fr_raw, values=['t', 'p'], \n                                index=[\"jour\"],\n                                columns=[\"dep\"], aggfunc=np.sum) \n    pt_fr_test[\"date\"] = pt_fr_test.index\n\n    # age (new feature)\n    # date \/ dep age pos test\n    # =>  date \/ pos mean(age) \n    df_gouv_fr_raw_0 = df_gouv_fr_raw.copy()\n    df_gouv_fr_raw_0[\"prod_p_age\"] = \\\n        df_gouv_fr_raw_0[\"p\"] * df_gouv_fr_raw_0[\"cl_age90\"]\n    df_gouv_fr_raw_0[\"prod_t_age\"] = \\\n        df_gouv_fr_raw_0[\"t\"] * df_gouv_fr_raw_0[\"cl_age90\"]\n    ser_p_age = df_gouv_fr_raw_0.groupby(\"jour\") \\\n        [\"prod_p_age\"].sum() \/ df_gouv_fr_raw_0.groupby(\"jour\")[\"p\"].sum()\n    df_age = pd.DataFrame(index=ser_p_age.index, columns=[\"pos_mean_age\"], \n                        data=ser_p_age.values)\n    ser_t_age = df_gouv_fr_raw_0.groupby(\"jour\") \\\n        [\"prod_t_age\"].sum() \/ df_gouv_fr_raw_0.groupby(\"jour\")[\"t\"].sum()\n    df_age[\"test_mean_age\"] = ser_t_age\n\n    # prepare data positive\n    df_pos_fr = pt_fr_test[\"p\"].copy()\n    df_pos_fr.index = pt_fr_test[\"date\"].index\n    # add date\n    df_pos_fr[\"date\"] = df_pos_fr.index\n    # add age\n    df_pos_fr[\"age\"] = df_age[\"pos_mean_age\"].copy()\n    # add positive cases sum   \n    df_pos_fr = compute_sum_dep(df_pos_fr)\n    # add nb_cases confirmed cummulative sum\n    arr_nb_cases = df_pos_fr[\"daily\"].cumsum().values\n    df_pos_fr[\"nb_cases\"] = nb_pos_start + arr_nb_cases\n    # save data pos\n    df_pos_fr.to_csv(path_df_pos_fr, index=False)\n\n    # prepare data tested\n    df_test_fr = pt_fr_test[\"t\"].copy()\n    df_test_fr.index = pt_fr_test[\"date\"].index\n    # add date\n    df_test_fr[\"date\"] = df_test_fr.index\n    # add age\n    df_test_fr[\"age\"] = df_age[\"test_mean_age\"].copy()\n    # add cases sum\n    df_test_fr = compute_sum_dep(df_test_fr)\n    # save data tested\n    df_test_fr.to_csv(path_df_test_fr, index=False)\n\n    return df_pos_fr, df_test_fr\n\n\ndf_gouv_fr_raw = get_data_gouv_fr()\n\ndf_dep_pos, df_dep_test = precompute_data_pos(df_gouv_fr_raw)","9b12e31b":"%%capture\n############################\n# Create data last 14 days : FRANCE Tested and Positive\n# output : pt_fr_test_last DataFrame\n\n# find last date \ndate_format = \"%Y-%m-%d\"\nstr_date_last = df_dep_pos.index.max() #df_gouv_fr_raw[\"jour\"].max() #pt_fr_test[\"date\"].max()\n\n# find start cumulative sum of confirmed cases \/ test\ndate_last = datetime.datetime.strptime(str_date_last, date_format)\ndate_start = date_last - datetime.timedelta(days=14-1)\nstr_date_start = date_start.strftime(date_format)\n\n# create table of nb_cases of last date : sum of all last 14 days\n# sum all from date_start :\nbol_date_last14d = df_gouv_fr_raw[\"jour\"] >= str_date_start\n#bol_date_last14d = df_dep_pos.index >= str_date_start\n\npt_fr_test_last = pd.pivot_table(df_gouv_fr_raw[bol_date_last14d], \n                                 values=['t', 'p'], \n                            index=[\"dep\"], aggfunc=np.sum) \npt_fr_test_last.index.name = ''\npt_fr_test_last[\"dep\"] = pt_fr_test_last.index","c1332e70":"%%capture\n###########\n# GEOJSON : dep france : source : https:\/\/france-geojson.gregoiredavid.fr\/\n#\n\n#URL_GEOJSON_DEP_FR = 'sources\/geojson-departements.json'\n# source : https:\/\/github.com\/gregoiredavid\/france-geojson\n\nif MODE_PLATFORM == \"KAGGLE\":\n    URL_GEOJSON_DEP_FR = 'https:\/\/raw.githubusercontent.com\/jeugregg\/' + \\\n        'coronavirusModel\/master\/sources\/departements-avec-outre-mer_simple.json'\n    import json\n    import urllib.request\n    # download raw json object\n    data = urllib.request.urlopen(URL_GEOJSON_DEP_FR).read().decode()\n    # parse json object\n    dep_fr = json.loads(data)\nelse:\n    URL_GEOJSON_DEP_FR = 'sources\/departements-avec-outre-mer_simple.json'\n    with open(URL_GEOJSON_DEP_FR) as f:\n        dep_fr = json.load(f)\n\n# example : \n# dep_fr['features'][0]['geometry']['type']\n# dep_fr['features'][0]['geometry'][\"coordinates\"]\n# dep_fr['features'][0][\"properties\"][\"code\"]\n# dep_fr['features'][0][\"properties\"][\"nom\"]\n\n# get list dep \/ code\nlist_code = \\\n    [feat_curr[\"properties\"][\"code\"] for feat_curr in dep_fr['features']]\nlist_name = \\\n    [feat_curr[\"properties\"][\"nom\"] for feat_curr in dep_fr['features']]\ndf_code_dep = pd.DataFrame(data=list_code, columns=[\"code\"])\ndf_code_dep[\"name\"] = list_name","4918f76f":"%%capture\n\n##############\n# R0 evolution\n#\n\ndef sum_between(ser_val, str_date_start, str_date_end):\n    '''\n    sum up values in series between 2 dates (index = date)\n    '''\n    b_range = (ser_val.index >= str_date_start) & \\\n        (ser_val.index <= str_date_end) \n    \n    return ser_val[b_range].sum()\n\ndef mdl_R0_estim(nb_cases, nb_cases_init=1, nb_day_contag=14, delta_days=14):\n    '''\n    R0 Model with exact formulation : \n    Nb_cases(D) - Nb_cases(D-1) = Nb_cases(D-1) * R_0_CV \/ NB_DAY_CONTAG_CV\n    \n    Nb_cases(D) = Nb_cases(D0) * exp(R_0_CV \/ NB_DAY_CONTAG_CV * (D - DO))\n    \n    => \n    R_0_CV = NB_DAY_CONTAG_CV \/ (D - DO) * ln( Nb_cases(D) \/ Nb_cases(D0))\n    \n    return R0\n    \n    '''\n    if type(nb_cases) == np.float64:\n        if nb_cases == 0:\n            return 0\n        return nb_day_contag \/ delta_days * math.log(nb_cases \/ \\\n                                                     max(1, nb_cases_init))\n    else:\n        list_out = []\n        for I in range(len(nb_cases)):\n            try:\n                if nb_cases[I] == 0:\n                    list_out.append(0)\n                else:\n                    list_out.append(nb_day_contag \/ delta_days * \\\n                        math.log(nb_cases[I] \/ max(1, nb_cases_init[I])))\n            except:\n                print(\"I = \",I)\n                print(\"nb_day_contag = \", nb_day_contag)\n                print(\"nb_cases[I] = \", nb_cases[I])\n                print(\"nb_cases_init[I] = \", nb_cases_init[I])\n                raise\n        return list_out\n\n\ndef sum_mobile(ser_val, ser_start, ser_end):\n    '''\n    mobile sums between dates start & end for ser_val (index = date)\n    '''\n    ser_sum = ser_val.copy()*np.NaN\n    # for each date range\n    for date_end, date_start in zip(ser_end, ser_start):\n        # calculate sum \n        sum_curr = sum_between(ser_val, date_start, date_end)\n        # store at date\n        ser_sum.loc[date_end] = sum_curr\n\n    return ser_sum\n\n\ndef find_close_date(ser_dates, str_date_0, str_date_min):\n    '''\n    find closest date but not developped\n    '''\n    return str_date_0\n\n# limitation to validated days\ndef create_date_ranges(ser_dates, nb_days_CV):\n    '''\n    Find first and last dates in \"ser_dates\" for last \"nb_days_CV\" days \n    '''\n    ser_start = []\n    ser_end = []\n    date_format = \"%Y-%m-%d\"\n    # find first date : \n    str_date_min = ser_dates.min()\n    str_date_max = ser_dates.max()\n    print(\"str_date_min: \", str_date_min)\n    print(\"str_date_max: \", str_date_max)\n    ser_end.append(str_date_max)\n    str_date_start = add_days(str_date_max, -(nb_days_CV-1))\n    next_date = find_close_date(ser_dates, str_date_start, str_date_min)\n    ser_start.append(next_date)\n    while ser_start[-1] > str_date_min:\n        ser_end.append(add_days(ser_end[-1], -1))\n        ser_start.append(add_days(ser_end[-1], -(nb_days_CV-1)))\n    return ser_start, ser_end\n\nnb_days_CV = 14\n\nser_start , ser_end = create_date_ranges(df_gouv_fr_raw[\"jour\"], nb_days_CV)\nprint(\"ser_start : \", ser_start)\nprint(\"ser_end : \", ser_end)\n\n\n#df_dep_pos = pt_fr_test[\"p\"]\n#df_dep_pos = df_dep_pos.filter(regex='^\\d', axis=1)\n\ndf_dep_sum = pd.DataFrame(index=df_dep_pos.index, columns=[\"date\"],\n                        data=df_dep_pos.index.tolist())\nfor dep_curr in df_dep_pos.columns:\n    if dep_curr != \"date\":\n        df_dep_sum[dep_curr] = sum_mobile(df_dep_pos[dep_curr], ser_start, \n                                          ser_end)\n\n\n\ndf_dep_r0 = pd.DataFrame(index=df_dep_pos.index, columns=[\"date\"],\n                        data=df_dep_pos.index.tolist())\n\n\n\nfor dep_curr in df_dep_sum.columns[1:]:\n    ser_val = df_dep_sum[dep_curr].copy()\n    date_min = add_days(ser_val.index.min(), nb_days_CV) \n    #list_sum_0 = []\n    #list_sum_1 = []\n    ser_r0 = ser_val.copy()*np.nan\n    for date_curr in ser_val[ser_val.index >= date_min].index:\n        date_0 = add_days(date_curr, -nb_days_CV)\n        if not(np.isnan(ser_val.loc[date_0])):\n            sum_0 = ser_val.loc[date_0]\n            sum_1 = ser_val.loc[date_curr]\n            ser_r0.loc[date_curr] = sum_1 \/ sum_0\n            '''ser_r0.loc[date_curr] = mdl_R0_estim(nb_cases=sum_1, \n                                                 nb_cases_init=sum_0,\n                                                 nb_day_contag=nb_days_CV, \n                                                 delta_days=nb_days_CV)'''\n    df_dep_r0[dep_curr] = ser_r0\n    \ndf_dep_r0.dropna(inplace=True)","0ee64f35":"#################\n# last R0 for MAP\n#\n\n# add departement name\npt_fr_test_last = pt_fr_test_last.merge(df_code_dep, left_on='dep', \n                                        right_on='code')\n# find last date \ndate_format = \"%Y-%m-%d\"\ndate_p0 = date_start - datetime.timedelta(days=14)\nstr_date_p0 = date_p0.strftime(date_format)\n\n# Nb_cases 14 days before: p_0\n# sum cases 14 days period before current 14 days period \n# => period : 28 days -> 14 days before last date:\nbol_date_p0 = (df_gouv_fr_raw[\"jour\"] < str_date_start) & \\\n    (df_gouv_fr_raw[\"jour\"] >= str_date_p0)\npt_fr_test_p0 = pd.pivot_table(df_gouv_fr_raw[bol_date_p0], \n                                 values=['p'], \n                            index=[\"dep\"], aggfunc=np.sum) \npt_fr_test_p0.index.name = ''\npt_fr_test_p0[\"dep\"] = pt_fr_test_p0.index\npt_fr_test_p0.columns = [\"p_0\", \"dep\"]\npt_fr_test_last = pt_fr_test_last.merge(pt_fr_test_p0, left_on='dep', \n                                        right_on='dep')\n\n# R0 Estimation :\n# Nb_cases(T0) sum of confirmed cases with T0=T-14days = between T0-14days -> T0 \n#   <=> (28 days before T -> 14 days before T)\n#\n# Nb cases(T):  sum of confirmed cases between T-28days -> T\npt_fr_test_last[\"R0\"] = pt_fr_test_last[\"p\"]  \/ pt_fr_test_last[\"p_0\"]\n\n'''pt_fr_test_last[\"R0\"] = mdl_R0_estim(nb_cases=pt_fr_test_last[\"p_0\"] + \\\n                                     pt_fr_test_last[\"p\"] , \n                                     nb_cases_init=pt_fr_test_last[\"p_0\"], \n                                     nb_day_contag=14, delta_days=14)'''","61721f89":"####################################\n# Rt evolution plots by departements in France\n#\n# data : \n# - df_dep_r0 (date,  date \/ dep. Rt)\n# - df_code_dep (-, dep. code \/ dep name )\n# - pt_fr_test_last (-, p \/ t \/ dep \/ code \/ name \/ p_0 \/ R0)\n\nlist_num_dep = []\nfor col_curr in df_dep_r0.columns:\n    if re.search(\"^\\d\", col_curr):\n        list_num_dep.append(col_curr)\n            \n#list_num_dep = df_dep_r0.columns[1:].tolist()\n# path 975 & 977 & 978 doesn't exist in dep name data\nlist_num_dep.remove('975')\nlist_num_dep.remove('977')\nlist_num_dep.remove('978')\n\nlist_name_dep = [f'{dep_num_curr} - ' + \\\n                 df_code_dep.loc[df_code_dep[\"code\"] == dep_num_curr,\n                                 \"name\"].values[0] + \\\n                 \"<br>Rt=<b>{:.2f}<\/b>\".format(df_dep_r0[dep_num_curr][-1]) + \\\n                 \" cases={}\".format(pt_fr_test_last.loc[ \\\n                 pt_fr_test_last.dep == dep_num_curr, \"p\"].values[0]) \\\n                 for dep_num_curr in list_num_dep]\n\nnb_dep = len(list_num_dep)\nnb_col = 4\nnb_row = math.ceil(nb_dep\/nb_col)\nfig = make_subplots(rows=nb_row, cols=nb_col, shared_xaxes=True,\n                    shared_yaxes=True,\n                    subplot_titles=list_name_dep)\nI_dep = 0\n#list_color = []\nfor row in range(nb_row):\n    for col in range(nb_col):   \n        dep_num_curr = list_num_dep[I_dep]\n        dep_curr = df_code_dep.loc[df_code_dep[\"code\"] == dep_num_curr, \n                                   \"name\"].values[0]\n       \n        if (df_dep_r0[dep_num_curr][-1] > 1) & \\\n             (pt_fr_test_last.loc[ \\\n                 pt_fr_test_last.dep == dep_num_curr, \"p\"].values[0] > 400):\n            color_curr = \"red\"\n        elif (df_dep_r0[dep_num_curr][-1] > 1):\n            color_curr = \"orange\"\n        else:\n            color_curr = \"blue\"\n    \n        \n        fig.add_trace(go.Scatter(x=df_dep_r0[\"date\"], y=df_dep_r0[dep_num_curr],\n                      mode='lines', name=dep_curr, line=dict(color=color_curr),\n                      fill='tozeroy'), \n                      row=row+1, col=col+1)\n        \n        fig.add_trace(go.Scatter(x=[df_dep_r0[\"date\"][0], \n                                    df_dep_r0[\"date\"][-1]], \n                                 y=[1,1],\n                                 mode='lines', \n                                 line=dict(color=\"red\", dash='dash'),\n                                 hoverinfo=\"skip\"), \n                      row=row+1, col=col+1)\n        I_dep +=1\n        \n        if I_dep >= nb_dep: #nb_dep:\n            break\n    if I_dep >= nb_dep: #nb_dep:\n            break \n\n#fig.update_traces(patch=dict(font=dict(size=6)))\nfor I, subplot_title_curr in enumerate(fig['layout']['annotations']):\n    subplot_title_curr['font'] = dict(size=10)\n    subplot_title_curr['xshift'] = 0\n    subplot_title_curr['yshift'] = -10\n\nfig.update_layout(\n    height=1800,\n    title=\"Rt: Estimated Reproduction Nb. in France ( until {} )\".format( \\\n        df_dep_r0['date'].max()),\n    showlegend=False,\n    font=dict(\n        size=12,\n    )\n)\n\n#fig.update_yaxes({\"color\": \"red\"})\nfig.show()\n","04be2b8e":"#####################\n# Graph Rt map France\n#\n# data : \n# - dep_fr (geo json )\n# - pt_fr_test_last\n\nlat_lon_fr =  {'lat':  47, 'lon': 2}\nzoom_fr = 4.5\nmapbox_args_fr = {'center': lat_lon_fr, \n                   'style': 'carto-positron', 'zoom': zoom_fr}\n\nmapbox_args_idf = {'center': {'lat':  48.86, 'lon': 2.33}, \n                   'style': 'carto-positron', 'zoom': 7}\nmapbox_args_dom = {'center': {'lat':  17, 'lon': -2}, \n                   'style': 'carto-positron', 'zoom': 2}\n\n# Initialize figure\n\nfig = go.Figure()\n\n\n# Add Traces\n\nfig.add_trace(\n    go.Choroplethmapbox(geojson=dep_fr, name=\"positive\",\n                                    locations=pt_fr_test_last[\"name\"], \n                                    featureidkey=\"properties.nom\",\n                                    z=pt_fr_test_last[\"p\"],\n                                    marker_opacity=0.7, marker_line_width=0))\n\nfig.add_trace(\n    go.Choroplethmapbox(geojson=dep_fr, name=\"tested\",\n                                    locations=pt_fr_test_last[\"name\"], \n                                    featureidkey=\"properties.nom\",\n                                    z=pt_fr_test_last[\"t\"],\n                                    marker_opacity=0.7, marker_line_width=0,\n                                    visible=False))\n\nfig.add_trace(\n    go.Choroplethmapbox(geojson=dep_fr, name=\"Rt\",\n                                    locations=pt_fr_test_last[\"name\"], \n                                    featureidkey=\"properties.nom\",\n                                    z=pt_fr_test_last[\"R0\"], zmin=.5, zmax=1.5,\n                                    marker_opacity=0.7, marker_line_width=0,\n                                    visible=False))\n\nannot_conf=[dict( \\\n    text=\"France : <b>Confirmed<\/b> cases (Sum during last 14 days before \" + \\\n    f\"{str_date_last})\", \n                 x=0, xref=\"paper\", y=1, yref=\"paper\",\n                             align=\"left\", showarrow=False,\n                bgcolor=\"#FFFFFF\")]\n\nannot_test=[dict( \\\n    text=\"France : <b>Tested<\/b> cases (Sum during last 14 days before \" + \\\n    f\"{str_date_last})\", x=0, xref=\"paper\", y=1, yref=\"paper\",\n                             align=\"left\", showarrow=False,\n                bgcolor=\"#FFFFFF\")]\n\nannot_r0=[dict( \\\n    text=\"France : <b>Rt<\/b> estimated on last 14 days before \" + \\\n    f\"{str_date_last})\", x=0, xref=\"paper\", y=1, yref=\"paper\",\n                             align=\"left\", showarrow=False,\n                bgcolor=\"#FFFFFF\")]\n\nfig.update_layout(\n    updatemenus=[\n        dict(\n            type=\"buttons\",\n            direction=\"right\",\n            xanchor=\"left\",\n            y=0.95,\n            x=0,\n            active=0,\n            showactive=True,\n            buttons=list([\n                dict(label=\"Confirmed\",\n                     method=\"update\",\n                     args=[{\"visible\": [True, False, False]},\n                           {\"annotations\": annot_conf}]),\n                \n                dict(label=\"Tested\",\n                     method=\"update\",\n                     args=[{\"visible\": [False, True, False]},\n                           {\"annotations\": annot_test}]),\n                \n                dict(label=\"Rt\",\n                     method=\"update\",\n                     args=[{\"visible\": [False, False, True]},\n                           {\"annotations\": annot_r0}]), \n                \n                dict(label=\"Zoom : IdF\",\n                     method=\"relayout\",\n                     args=[{\"mapbox\": mapbox_args_idf}]),\n                \n               dict(label=\"France\",\n                     method=\"relayout\",\n                     args=[{\"mapbox\": mapbox_args_fr}]), \n                \n               dict(label=\"DOM-TOM\",\n                     method=\"relayout\",\n                     args=[{\"mapbox\": mapbox_args_dom}]),\n            ]),\n        )\n    ])\n\nfig.update_layout(mapbox_style=\"carto-positron\",\n                 mapbox_zoom=zoom_fr, mapbox_center = lat_lon_fr)\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.update_layout(annotations=annot_conf)\n# Set title\n#fig.update_layout(title_text=\"Confirmed cases in France\")\nfig.show()","f28236f8":"#df_fr.filter(items = [\"date\", \"total\"]).tail(1)\ndf_cases_fr.filter(items = [\"date\", \"nb_cases\"]).tail(1)","62d4bb6f":"# Table per area in France\n\nimport plotly.graph_objects as go\nimport pandas as pd\nfrom plotly.colors import n_colors\nimport numpy as np\n#locale.setlocale(locale.LC_ALL, 'en_US')\n\ndf_fr_table = df_fr.copy()\ndf_fr_table[\"date\"] = df_fr_table[\"date\"].astype(np.datetime64).dt.strftime('%b %d')\ndf_fr_table_color = df_fr_table.fillna(0)\ndf_fr_table_color.dropna(inplace=True, how='all', subset=LIST_AREA_FR)\n\ndf_fr_table = df_fr_table.fillna(\"?\")\ndf_fr_table.dropna(inplace=True, how='all', subset=LIST_AREA_FR)\n\nnb_colors = 20\ncolors = n_colors('rgb(255, 240, 240)', 'rgb(200, 0, 0)', \n                  nb_colors, colortype='rgb')\n\ncolors_font = n_colors('rgb(0, 0, 0)', 'rgb(194, 198, 255)', \n                  nb_colors, colortype='rgb')\nscale_color = df_fr_table[\"total\"].max() \/ (nb_colors - 1)\n\nlist_font_colors = []\nlist_fill_colors = []\nlist_col_table = [\"date\", \"total\"] + LIST_AREA_FR\nlist_table=[]\nfor col_curr in list_col_table:\n    if col_curr == \"date\":\n        list_fill_colors.append(\\\n            np.array(colors)[np.zeros(df_fr_table_color.shape[0]).\\\n                             astype(np.int64)])\n        list_font_colors.append(\\\n            np.array(colors_font)[np.zeros(df_fr_table_color.shape[0]).\\\n                             astype(np.int64)])\n    else:\n        list_color_curr = (df_fr_table_color[col_curr]\/scale_color).\\\n            astype(np.int64).values\n        list_fill_colors.append(np.array(colors)[list_color_curr])\n        list_font_colors.append(np.array(colors_font)[list_color_curr])\n        \n    list_table.append(df_fr_table[col_curr])\n\nlist_header = list_col_table \nfor idx in range(len(list_header)):\n    list_header[idx] = list_header[idx].replace('-',' -')\n\nfig = go.Figure(data=[go.Table(\n    \n    columnwidth=90,\n    \n    header=dict(values=list_header,\n                fill_color='paleturquoise',\n                align='left'),\n    cells=dict(values=list_table,\n               fill_color=list_fill_colors, #'lavender',\n               align='left',\n               font=dict(color=list_font_colors)))\n])\nfig.update_layout(width=80*len(list_col_table), \n                  height=int((df_fr_table.shape[0] + 4)*300\/12),\n            title=\"Confirmed cases in France Areas (only begining of outbreak)\",\n                  margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nfig.show()","b051a125":"# OLD METHOS FROM SANTE PUBLIQUE FRANCE : NOT UPDATED ANYMORE BY REGIONS !\n# WHY ?? \n\n\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Create and style traces\nfig.add_trace(go.Scatter(x=df_cases_fr[\"date\"].astype(np.datetime64), \n                         y=df_cases_fr[\"nb_cases\"],\n                    mode='lines+markers',\n                    line_shape='linear',\n                    connectgaps=True, name=\"Total cases\"))\n# add delta \/ diff data\ns_delta_t = df_cases_fr[\"date\"].astype(np.datetime64) - \\\n    np.datetime64(df_cases_fr.iloc[0][\"date\"])\ndelta_sec = np.diff(s_delta_t.dt.total_seconds())\n\nfig.add_trace(go.Bar(x=df_cases_fr[\"date\"][1:].astype(np.datetime64), \n                     y=np.diff(df_cases_fr[\"nb_cases\"]) * (3600*24) \/ delta_sec, \n                     name=\"Daily cases\"), secondary_y=True)\n# Edit the layout\nfig.update_layout(legend_orientation=\"h\", legend=dict(x=0, y=1.1))\n#fig.update_yaxes(title_text=\"daily cases\", range=[0, 5000], secondary_y=True)\nfig.update_layout( \\\n            title='COVID-19 Confirmed cases in France (estimated by JHU CSSE)',\n                   yaxis_title='Total cases')\nfig.update_layout(legend_orientation=\"h\", legend=dict(x=0, y=1.1))\nfig.show()\n","2c52d137":"%%capture\n# Time evolution by Areas in France Metropole\n# USELESS\n'''list_regions = LIST_METROPOLE.copy()\n#list_regions.remove(\"Seoul\")\n\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\n# Create and style traces\nfor area_curr in list_regions:\n    \n    fig.add_trace(go.Scatter(x=df_fr[\"date\"], \n                         y=df_fr[area_curr],\n                    mode='lines+markers',\n                    name=area_curr,\n                    line_shape='linear',\n                    connectgaps=True))\n\n# Edit the layout\nfig.update_layout(\\\n    title='France Metropole by Areas : COVID-19 Confirmed cases ' + \\\n                  '(until 25\/03\/2020)', yaxis_title='nb confirmed cases')\n\nfig.update_layout(legend_title='<b> Areas <\/b>')\nfig.update_layout(legend_orientation=\"h\")\nfig.show()'''\n\n# USELESS","899db1a9":"%%capture\n# USELESS\n'''# Time evolution by Areas in France Outre Mer\n\nlist_regions = LIST_OUTREMER.copy() + [TOT_OUTREMER]\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\n# Create and style traces\nfor area_curr in list_regions:\n    \n    fig.add_trace(go.Scatter(x=df_fr[\"date\"], \n                         y=df_fr[area_curr],\n                    mode='lines+markers',\n                    name=area_curr,\n                    line_shape='linear',\n                    connectgaps=True))\n\n# Edit the layout\nfig.update_layout(title='France Outre Mer by Areas : ' + \\\n                  'COVID-19 Confirmed cases (until 25\/03\/2020)',\n                   yaxis_title='nb confirmed cases')\n\nfig.update_layout(legend_title='<b> Areas <\/b>')\nfig.update_layout(legend_orientation=\"h\")\nfig.show()'''\n# USELESS","1f03a772":"%%capture\n#####################\n## Latitude-Longitude\n#\n\ntry:\n    df_geo_fra = pd.read_csv(PATH_CSV_DF_GEO_FRA)\n    \nexcept:\n    try:\n        geolocator = Nominatim(user_agent=\"ScrapDataFromKoreaNews\")\n        df_geo_fra = pd.DataFrame(index=range(len(LIST_AREA_FR)), \n                                  columns=[\"area\"], data=LIST_AREA_FR)\n        df_geo_fra[\"lat\"] = np.nan\n        df_geo_fra[\"lon\"] = np.nan\n        for area_curr in LIST_AREA_FR:\n            if area_curr == \"Saint-Martin\":\n                location = geolocator.geocode(\"\u00celes de Saint-Martin\")\n            else:\n                location = geolocator.geocode(area_curr)\n            print(' ')\n            print(area_curr)\n            print(location.address)\n            print((location.latitude, location.longitude))\n            df_geo_fra.loc[df_geo_fra[\"area\"] == area_curr, \"lat\"] = \\\n                location.latitude\n            df_geo_fra.loc[df_geo_fra[\"area\"] == area_curr, \"lon\"] = \\\n                location.longitude\n        df_geo_fra_add = df_geo_fra.copy().tail(1)\n        df_geo_fra_add[\"area\"] = \"EHPAD\"\n        #geolocator = Nominatim(user_agent=\"ScrapDataFromKoreaNews\")\n        #location = geolocator.geocode(\"FRANCE\")\n        df_geo_fra.loc[df_geo_fra.area ==\"EHPAD\", \"lon\"] = -3.5\n        df_geo_fra.loc[df_geo_fra.area ==\"EHPAD\", \"lat\"] = 46\n        df_geo_fra = df_geo_fra.append(df_geo_fra_add)\n        # save in CSV format\n        df_geo_fra.to_csv(PATH_CSV_DF_GEO_FRA, index=False)\n        print(\"Saved here : \", PATH_CSV_DF_GEO_FRA)\n    except:\n        df_geo_fra = pd.read_csv(\n            'https:\/\/raw.githubusercontent.com\/' + \\\n            'jeugregg\/coronavirusModel\/master\/df_geo_fra.csv')","e65cae43":"df_regions = df_fr.copy()\ndf_regions = df_fr.melt(id_vars=['date'], value_vars=LIST_AREA_FR, \n                             var_name=\"area\", value_name=\"nb_cases\")\n\ndf_regions = df_regions.join(df_geo_fra.set_index('area'), on='area')\ndf_regions.dropna(inplace=True, subset=[\"nb_cases\"])\ndf_regions = df_regions.fillna(0)\n\n#df_regions = df_regions[df_regions[\"date\"] > '2020-02-19']\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n# add animate bubble\nfig = px.scatter_geo(df_regions, lat=\"lat\", lon=\"lon\", color=\"nb_cases\",\n                     range_color=[0, df_fr[\"total\"].max()+1], \n                     text=\"nb_cases\",\n                     hover_name=\"area\", size=\"nb_cases\", size_max=40, \n                     animation_frame=df_regions[\"date\"].\\\n                         astype(np.datetime64).dt.strftime('%b %d'), \n                     projection=\"natural earth\",\n                     title=\"COVID-19 Confirmed cases in France Metropole \" +\\\n                     \"(->25\/03\/2020) [Click on Play]\")\n\n# add text \nfig['data'][0].update(mode='markers+text', textposition='bottom center', \n                     textfont={'color': \"red\"})\n\nfig.update_layout(height=500, margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n\n# center map\nfig.update_geos(lonaxis_range=[ -7.0, 8.0] \\\n                + df_geo_fra[df_geo_fra[\"area\"] == \\\n                             'Centre-Val de Loire'][\"lon\"].values[0],\n                lataxis_range=[ -7, 4] + \\\n                df_geo_fra[df_geo_fra[\"area\"] == \\\n                           'Centre-Val de Loire'][\"lat\"].values[0],\n                landcolor = 'rgb(217, 217, 217)', \n                showocean=True, oceancolor=\"LightBlue\",\n               resolution=50)\n\n# colorize country\nfig.add_trace(go.Choropleth(\n        locationmode='country names',\n        locations=df_fr[\"country\"],\n        z=df_fr[\"total\"],\n        text=df_fr['country'],\n        colorscale=[[0,'rgb(239, 239, 239)'],[1,'rgb(239, 239, 239)']],\n        autocolorscale=False,\n        showscale=False,\n        hoverinfo='skip',\n        geo='geo'\n    ))\n\nfig.show()","18d056f2":"#df_france = df_world_melt[df_world_melt[\"Country\/Region\"] == \"France\"]\ndf_italy = df_world_melt[df_world_melt[\"Country\/Region\"] == \"Italy\"]\n#s_france = df_france.groupby(\"date\")[\"nb_cases\"].sum()\ns_italy = df_italy.groupby(\"date\")[\"nb_cases\"].sum()\ns_italy.index = s_italy.index +  pd.Timedelta('8 day')\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Create and style traces\nfig.add_trace(go.Scatter(x=s_italy.index, \n                         y=s_italy.values,\n                    mode='lines+markers',\n                    line_shape='linear',\n                    connectgaps=True, name=\"Italy Total\"))\n# add delta \/ diff data\nfig.add_trace(go.Bar(x=s_italy.index[1:], \n                    y=np.diff(s_italy.values), name=\"Italy Daily\"), \n             secondary_y=True)\n\nfig.add_trace(go.Scatter(x=df_cases_fr[\"date\"].astype(np.datetime64), \n                         y=df_cases_fr[\"nb_cases\"],\n                    mode='lines+markers',\n                    line_shape='linear',\n                    connectgaps=True, name=\"France Total\"))\n# add delta \/ diff data\ns_delta_t = df_cases_fr[\"date\"].astype(np.datetime64) - \\\n    np.datetime64(df_cases_fr.iloc[0][\"date\"])\ndelta_sec = np.diff(s_delta_t.dt.total_seconds())\n\nfig.add_trace(go.Bar(x=df_cases_fr[\"date\"][1:].astype(np.datetime64), \n                     y=np.diff(df_cases_fr[\"nb_cases\"]) * (3600*24) \/ delta_sec, \n                     name=\"France Daily\"), \n             secondary_y=True)\n# Edit the layout\nfig.update_layout(title='COVID-19 Confirmed cases France \/ 8days delayed Italy',\n                   yaxis_title='Total')\nfig.update_layout(legend_orientation=\"h\", legend=dict(x=0, y=1.1))\n# range on valid dates\nfig.update_xaxes(range=['2020-03-01', s_italy.index[-1] + \\\n                        (s_italy.index[-1]-s_italy.index[-2])])\n\nfig.update_yaxes(title_text=\"daily cases\", secondary_y=True)\n\nfig.show()","8c0566ff":"%%capture\n# DATA NOT GOOD FOR FRANCE\n'''df_france = df_death_melt[df_death_melt[\"Country\/Region\"] == \"France\"]\ndf_italy = df_death_melt[df_death_melt[\"Country\/Region\"] == \"Italy\"]\n\ns_france = df_france.groupby(\"date\")[\"nb_death\"].sum()\n\ns_italy = df_italy.groupby(\"date\")[\"nb_death\"].sum()\ns_italy.index = s_italy.index +  pd.Timedelta('8 day')\n\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nfig = go.Figure()\n# Create and style traces\nfig.add_trace(go.Scatter(x=s_italy.index, \n                         y=s_italy.values,\n                    mode='lines+markers',\n                    line_shape='linear',\n                    connectgaps=True, name=\"Italy Total deaths\"))\n# add delta \/ diff data\nfig.add_trace(go.Bar(x=s_italy.index[1:], \n                    y=np.diff(s_italy.values), name=\"Italy New deaths\"))\n\nfig.add_trace(go.Scatter(x=s_france.index, \n                         y=s_france.values,\n                    mode='lines+markers',\n                    line_shape='linear',\n                    connectgaps=True, name=\"France Total deaths\"))\n# add delta \/ diff data\nfig.add_trace(go.Bar(x=s_france.index[1:], \n                    y=np.diff(s_france.values), name=\"France New deaths\"))\n\n# Edit the layout\nfig.update_layout(title='COVID-19 Death cases France \/ Italy 8 days delayed',\n                   yaxis_title='nb deaths cases')\n\n# range on valid dates\nfig.update_xaxes(range=['2020-03-01', s_italy.index[-1] + \\\n                        (s_italy.index[-1]-s_italy.index[-2])])\nfig.show()'''","865d02e3":"%%capture\n%%writefile worldometerspider.py\n# -*-coding:utf-8 -*\n\n#\n# Module for scrapy over tables in worldometers FRANCE\n#\n\n# import\nimport re\nimport scrapy\nfrom scrapy.loader import ItemLoader\nfrom scrapy.loader.processors import MapCompose, Join, TakeFirst\nfrom w3lib.html import remove_tags\nimport json\n\n# definitions \nURL_ROOT = 'https:\/\/www.worldometers.info\/coronavirus'\nPATH_TABLES_OUTPUT = 'table_worldo_fr.json'\nURL_FIRST_PAGE = URL_ROOT + '\/country\/france\/'\n\n\nclass WorldoTablesItem(scrapy.Item):\n    '''\n    Class item to declare different information to scrap\n    and how to process (as input or output)\n    '''\n    # define the fields for your item here like:\n    url = scrapy.Field(output_processor=TakeFirst())\n    \n    source = scrapy.Field(output_processor=TakeFirst())\n    \n    dates = scrapy.Field(output_processor=TakeFirst())\n    \n    deaths = scrapy.Field(output_processor=TakeFirst())\n    \n\nclass WorldoTablesSpider(scrapy.Spider):\n    '''\n    Spider to scrap tables webpages : \n    - how to find information for scraping\n    - which field names to store\n    '''\n    # Your spider definition\n    name = 'tables_Worldo_spider'\n    # output definition :\n    custom_settings = {\n      'FEED_FORMAT': 'json',\n      'FEED_URI': PATH_TABLES_OUTPUT\n  }\n    # urls to scrap\n    start_urls = [URL_FIRST_PAGE]\n   \n    def parse(self, response):\n        '''\n        Parse definition with xpath which define all patterns to use\n        for retrieve information into HTML strings\n        '''\n        #url\tsource\tauthor\ttitle\ttheme\tdescription\tdate_published\tbody   \n        l = ItemLoader(item=WorldoTablesItem(), selector=response)\n        \n        l.add_value('url', response.url)\n        \n        l.add_value('source', \"worldometers\")\n        \n        # death table\n        # Method : \n        # https:\/\/stackoverflow.com\/questions\/33503643\/get-data-from-script-tag-in-html-using-scrapy\n        \n        # script text\n        data = response.xpath( \\\n            '\/\/script[contains(.,\"coronavirus-deaths-linear\")]').get()\n        # filter dates\n        # text : categories: [\"Feb 15, 2020\",\"Feb 16, 2020\",...,\"Feb 18, 2020\"] \n        pattern = re.compile(\\\n            r'(?<=categories\\: \\[)(\"\\D\\D\\D \\d\\d, \\d\\d\\d\\d\",*)+',\n            re.MULTILINE | re.DOTALL)\n        str_list_date = re.search(pattern, data)[0]                                                                                         \n        # create list dates\n        #pattern_date = re.compile('\\D\\D\\D \\d\\d')\n        #list_date_clean = re.findall(pattern_date, str_list_date)\n        l.add_value('dates', str_list_date)\n        \n        # filter death\n        # text: data: [1,1,1,1,1,1,1,...,372]\n        pattern_death = re.compile('(?<=data\\: \\[)(\\d+,*)+', \n                                   re.MULTILINE | re.DOTALL)\n        str_list_death = re.search(pattern_death, data)[0]\n        l.add_value('deaths', str_list_death)\n        \n        yield l.load_item()\n","488b940d":"%%capture\n%%writefile run_worldo_tables.py\n# -*-coding:utf-8 -*\n\n#\n# Run scrapy for retrieve updates tables from worldometers\n#\n\n# import\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom worldometerspider import WorldoTablesSpider\nimport pandas as pd\nimport os\n\n# start scraping process\nprocess = CrawlerProcess()\nprocess.crawl(WorldoTablesSpider)\nprocess.start() # the script will block here until the crawling is finished","062800a8":"%%capture\n%%time\n#######################\n## Scrap updates tables\n#\ntry:\n    # clean (move file if exist)\n    clean_file(PATH_TABLES_WORLDO)\n    # execute external process (FALLBACK for scrapy problem)\n    !python run_worldo_tables.py \nexcept:\n    print(\"Error run_worldo_tables !\")","79dc9605":"range_days = pd.date_range(start='2020-03-05', \n                            end=df_open_fr[\"date\"].max())\nrange_days = range_days.strftime(\"%Y-%m-%d\")\nif range_days.shape[0] > 1:\n    #pass\n    range_days = range_days[1:]\nelse:\n    range_days = None\n\nif range_days is not None:\n    df_death_tables_open_fr = create_df_tables_fr()\n    for K, day_curr in enumerate(range_days):\n        df_death_tables_open_fr.at[K, \"date\"] = day_curr\n    \n    for area_curr in LIST_METROPOLE + LIST_OUTREMER:\n        \n        s_conf_curr = df_open_fr[df_open_fr[\"maille_nom\"] == area_curr] \\\n            .groupby(\"date\")[\"deces\"].max()\n        #print(\"area_curr : {} \/ n= {}\".format(area_curr, s_conf_curr.shape[0]))\n        for day_curr in range_days:\n            if day_curr in s_conf_curr.index.tolist():\n                #print(\"day_curr={}, n_conf={}\".format(day_curr, s_conf_curr.loc[day_curr]))\n                df_death_tables_open_fr.at[df_death_tables_open_fr[\"date\"] \\\n                    == day_curr, area_curr] = s_conf_curr.loc[day_curr]\n    \n    # EHPAD nb. death:\n    area_curr = \"EHPAD\"\n    df_death_tables_open_fr[area_curr] = 0\n    s_conf_curr = df_open_fr[df_open_fr[\"maille_nom\"] == \"France\"] \\\n            .groupby(\"date\")[\"deces_ehpad\"].max()\n    for day_curr in range_days:\n            if day_curr in s_conf_curr.index.tolist():\n                df_death_tables_open_fr.at[df_death_tables_open_fr[\"date\"] \\\n                    == day_curr, area_curr] = s_conf_curr.loc[day_curr]\n            \ndf_death_tables_open_fr[\"source\"] = \"OPENCOVID19_FR\"\ndf_death_tables_open_fr[\"country\"] = \"France\"\ndf_death_tables_open_fr[\"iso_alpha\"] = \"FRA\"\n\ndf_regions = df_death_tables_open_fr.copy()\ndf_regions = df_death_tables_open_fr.melt(id_vars=['date'], \n                                          value_vars=LIST_AREA_FR + [\"EHPAD\"], \n                             var_name=\"area\", value_name=\"nb_cases\")\n\ndf_regions = df_regions.join(df_geo_fra.set_index('area'), on='area')\ndf_regions.dropna(inplace=True, subset=[\"nb_cases\"])\ndf_regions = df_regions.fillna(0)","665e5ca0":"str_date = df_regions[\"date\"].max()\nval_tot = df_regions.groupby(\"area\")[\"nb_cases\"].max().sum()\nprint(\"{}  : {:.0f} death cases\".format( str_date, val_tot))","7603deee":"# Time evolution by Areas in France Metropole\n\nlist_regions = LIST_METROPOLE.copy()\nlist_regions = list_regions + [\"EHPAD\"]\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\n# Create and style traces\nfor area_curr in list_regions:\n    \n    fig.add_trace(go.Scatter(x=df_regions[df_regions.area == area_curr ]\\\n                             [\"date\"], \n                         y=df_regions[df_regions.area == area_curr ]\\\n                             [\"nb_cases\"],\n                    mode='lines+markers',\n                    name=area_curr,\n                    line_shape='linear',\n                    connectgaps=True))\n\n# Edit the layout\nfig.update_layout(\\\n    title='France Metropole by Areas : COVID-19 Death cases ',\n                  yaxis_title='nb death cases')\n\nfig.update_layout(legend_title='<b> Areas <\/b>')\nfig.update_layout(legend_orientation=\"h\")\nfig.show()","420431e8":"# Time evolution by Areas in France Metropole\n\nlist_regions = LIST_OUTREMER.copy()\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\n# Create and style traces\nfor area_curr in list_regions:\n    \n    fig.add_trace(go.Scatter(x=df_regions[df_regions.area == area_curr ]\\\n                             [\"date\"], \n                         y=df_regions[df_regions.area == area_curr ]\\\n                             [\"nb_cases\"],\n                    mode='lines+markers',\n                    name=area_curr,\n                    line_shape='linear',\n                    connectgaps=True))\n\n# Edit the layout\nfig.update_layout(\\\n    title='France OUTREMER by Areas : COVID-19 Death cases ',\n                  yaxis_title='nb death cases')\n\nfig.update_layout(legend_title='<b> Areas <\/b>')\nfig.update_layout(legend_orientation=\"h\")\nfig.show()","e9897e4f":"#df_regions = df_regions[df_regions[\"date\"] > '2020-02-19']\n# limit to last 30 days : \ndf_regions = df_regions[df_regions[\"date\"] > \\\n              add_days(df_regions[\"date\"].astype(str).max(), -30)].copy()\n\ndf_regions = df_regions.sort_values(by=\"date\")\n\n# number of max death by region\nnb_death_max = df_regions[\"nb_cases\"].max() \n\nimport plotly.express as px\nimport plotly.graph_objects as go\n# add animate bubble\nfig = px.scatter_geo(df_regions, lat=\"lat\", lon=\"lon\", color=\"nb_cases\",\n                     range_color=[0, nb_death_max +1], \n                     text=\"nb_cases\",\n                     hover_name=\"area\", size=\"nb_cases\", size_max=40, \n                     animation_frame=df_regions[\"date\"]. \\\n                         astype(np.datetime64).dt.strftime('%b %d'), \n                     projection=\"natural earth\",\n    title=\"COVID-19 Deatch cases in France Metropole + EHPAD [Click on Play]\")\n\n# add text \nfig['data'][0].update(mode='markers+text', textposition='bottom center', \n                     textfont={'color': \"red\"})\n\nfig.update_layout(height=500, margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n\n# center map\nfig.update_geos(lonaxis_range=[ -7.0, 8.0] \\\n                + df_geo_fra[df_geo_fra[\"area\"] == \\\n                             'Centre-Val de Loire'][\"lon\"].values[0],\n                lataxis_range=[ -7, 4] + \\\n                df_geo_fra[df_geo_fra[\"area\"] == \\\n                           'Centre-Val de Loire'][\"lat\"].values[0],\n                landcolor = 'rgb(217, 217, 217)', \n                showocean=True, oceancolor=\"LightBlue\",\n               resolution=50)\n\n# colorize country\nfig.add_trace(go.Choropleth(\n        locationmode='country names',\n        locations=df_fr[\"country\"],\n        z=df_fr[\"total\"],\n        text=df_fr['country'],\n        colorscale=[[0,'rgb(239, 239, 239)'],[1,'rgb(239, 239, 239)']],\n        autocolorscale=False,\n        showscale=False,\n        hoverinfo='skip',\n        geo='geo'\n    ))\n\nfig.show()","e3e76f53":"# load data\ndf_brut = pd.read_json(PATH_TABLES_WORLDO)\n\n# take dates\nstr_list_date = df_brut[\"dates\"].values[0]\npattern_date = re.compile('\\D\\D\\D \\d\\d, \\d\\d\\d\\d')\nlist_date = re.findall(pattern_date, str_list_date)\n\n# take death\nstr_list_death = df_brut[\"deaths\"].values[0]\nlist_death_str = re.split(\",\", str_list_death)\nlist_death = []\nfor str_curr in list_death_str:\n    list_death.append(int(str_curr))\n\n# create dataframe\ndf_death_w_fr = pd.DataFrame(columns=[\"date\", \"nb_death\"])\ndf_death_w_fr[\"date\"] = list_date\ndf_death_w_fr[\"nb_death\"] = list_death\n\n# update date format\ndef fun_adapt_date(str_curr):\n    return datetime.datetime.strptime(str_curr,'%b %d, %Y').strftime(\"%Y-%m-%d\")\ndf_death_w_fr[\"date\"] = df_death_w_fr[\"date\"].apply(fun_adapt_date)\n\n# prepare data italy for plot\ndf_italy = df_death_melt[df_death_melt[\"Country\/Region\"] == \"Italy\"]\ns_italy = df_italy.groupby(\"date\")[\"nb_death\"].sum()\ns_italy.index = s_italy.index +  pd.Timedelta('8 day')\n\n# plot \nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n# Create and style traces\nfig.add_trace(go.Scatter(x=s_italy.index, \n                         y=s_italy.values,\n                    mode='lines+markers',\n                    line_shape='linear',\n                    connectgaps=True, name=\"Italy Total\"))\n# add delta \/ diff data\nfig.add_trace(go.Bar(x=s_italy.index[1:], \n                    y=np.diff(s_italy.values), name=\"Italy Daily\"),\n             secondary_y=True)\n\n\nfig.add_trace(go.Scatter(x=df_death_w_fr[\"date\"].astype(np.datetime64), \n                         y=df_death_w_fr[\"nb_death\"],\n                    mode='lines+markers',\n                    line_shape='linear',\n                    connectgaps=True, name=\"France Total\"))\n# add delta \/ diff data\nfig.add_trace(go.Bar(x=df_death_w_fr[\"date\"].astype(np.datetime64)[1:], \n                    y=np.diff(df_death_w_fr[\"nb_death\"]), \n                     name=\"France New deaths\"),\n             secondary_y=True)\n\n# Edit the layout\nfig.update_layout(title='COVID-19 Death cases France \/ Italy 8 days delayed',\n                   yaxis_title='Total deaths')\nfig.update_layout(legend_orientation=\"h\", legend=dict(x=0, y=1.1))\n# range on valid dates\nfig.update_xaxes(range=['2020-03-01', s_italy.index[-1] + \\\n                        (s_italy.index[-1]-s_italy.index[-2])])\nfig.update_yaxes(title_text=\"Daily deaths\", secondary_y=True)\nfig.show()","7e364272":"R_0_CV = 2.5 # number of infected cases by one infected case during his contagious state.\nNB_DAY_CONTAG_CV = 14 #  number of contagious days for infected cases\n\ndef model_r0_cv_forecast(r_0, nb_day_contag, nb_cases_0, date_0, nb_days):\n    '''\n    Fixed RO model to calculate epidemic cases\n    '''\n    # range for days\n    arr_days = pd.date_range(start=date_0, periods=nb_days)\n    # range of cases : initialization\n    nb_conf = np.zeros(arr_days.shape[0])\n    nb_conf[0] = nb_cases_0\n    # for each days, calculate next days cases\n    for day_curr in np.arange(1, arr_days.shape[0]):\n        nb_conf[day_curr] = nb_conf[day_curr-1] + \\\n            nb_conf[day_curr-1]*(r_0\/nb_day_contag)\n    # return range days and cases\n    return arr_days, nb_conf","a0841a61":"def create_fig_compare_model(df_cases, col_val, arr_days, nb_conf_CV, r_0_cv,\n                             n_0, title):\n    '''\n    Create comparison fig with data & model\n    '''\n    import plotly.graph_objects as go\n    import plotly.express as px\n    fig = go.Figure()\n    # Create and style traces\n    fig.add_trace(go.Scatter(x=arr_days, \n                         y=nb_conf_CV,\n                    mode='lines+markers',\n                    line_shape='linear',\n                    connectgaps=True, name=\"Model R0 CV\"))\n\n    # Create and style traces\n    fig.add_trace(go.Scatter(x=df_cases[\"date\"].astype(np.datetime64), \n                         y=df_cases[col_val], \n                    mode='lines+markers',\n                    line_shape='linear',\n                    connectgaps=True, name=\"Total cases\"))\n    # Edit the layout\n    fig.update_layout(\n    title=title + '<br>' + \n    'Start {}, R_0={:.2f}, '.format(arr_days[0], r_0_cv) + \n    'N_0={:.2f}'.format(n_0), yaxis_title=col_val)\n\n    return fig\n\n\ndef add_annotation_lockdown(fig, df_cases, col_val, date_lockdown, \n                            date_slowdown=None, str_lock=None):\n    '''\n    Add annotations to figure\n    '''\n    if str_lock is None:\n        str_lock = \"Lockdown\"\n    nb_cases_lockdown = \\\n    df_cases[df_cases[\"date\"] == date_lockdown][col_val].values[0]\n\n    # Annotations\n    fig.add_annotation(\n            x=date_lockdown,\n            y=nb_cases_lockdown,\n            text=str_lock)\n    \n    if date_slowdown != None:\n        date_format = \"%Y-%m-%d\"\n        a = datetime.datetime.strptime(date_lockdown, date_format)\n        b = datetime.datetime.strptime(date_slowdown, date_format)\n        delta = b - a\n        nb_cases_slowdown = \\\n            df_cases[df_cases[\"date\"] == date_slowdown][col_val].values[0]\n        fig.add_annotation(\n            x=date_slowdown,\n            y=nb_cases_slowdown,\n            text=\"Slowdown (after {} days)\".format(delta.days) )\n    \n    fig.update_annotations(dict(\n            xref=\"x\",\n            yref=\"y\",\n            showarrow=True,\n            arrowhead=7,\n            ax=0,\n            ay=-40))\n    \n    return fig\n\n\ndef fun_days_between(date_0):\n    # compute number of days from start until tomorrow\n    delta = datetime.datetime.now() + datetime.timedelta(days=1) - date_0\n    nb_days = delta.days\n    return nb_days","463087fd":"# Find R_0_CV from Hubei\n\n###############\n# CHINA : HUBEI\n#\n\n# retrieve data\ndf_hubei, df_hubei_death = extract_data_world(df_world_melt, df_death_melt,\n                                              \"Province\/State\", \"Hubei\")\n\n######################\n# HUBEI PROVINCE CHINA\n#\n\n# param Country\nr_0_cv_hb = R_0_CV*0.9\n# init time\nn_0_hb = df_hubei[\"nb_cases\"][0] * 6\ndate_0_hb = df_hubei[\"date\"].astype(np.datetime64)[0]\nnb_days = 25\n\n# forecast\narr_days, nb_conf_CV = model_r0_cv_forecast(r_0=r_0_cv_hb, \n                                         nb_day_contag=NB_DAY_CONTAG_CV,\n                                         date_0=date_0_hb,\n                                         nb_cases_0=n_0_hb,\n                                         nb_days=nb_days)\n\n\n# plot result (compare)\nfig = create_fig_compare_model(df_hubei, \"nb_cases\", arr_days, nb_conf_CV, \n                               r_0_cv_hb, n_0_hb, \n                        \"Hubei (China) : COVID-19 Confirmed cases vs Model R_0\")\n\ndate_lockdown = '2020-01-23'\ndate_slowdown = '2020-02-07'\nfig = add_annotation_lockdown(fig, df_hubei, \"nb_cases\", date_lockdown, \n                              date_slowdown, str_lock = \"Lockdown\")\n\nfig.update_layout(legend_orientation=\"h\", legend=dict(x=0, y=1))\n\nfig.show()","5ceea7a1":"# south korea\n# adaptation \ndf_kcdc_interp[\"nb_cases\"] = df_kcdc_interp[\"total\"]\n\n# data country\nr_0_kr = R_0_CV *1.4\n#r_0_kr = R_0_CV \n# init time\nn_0_kr = df_kcdc_interp[df_kcdc_interp[\"date\"] == \"2020-02-08\"][\"nb_cases\"][0]\n#date_0_kr = df_kcdc_interp.index[0]\ndate_0_kr = df_kcdc_interp[df_kcdc_interp[\"date\"] == \"2020-02-08\"][\"date\"][0]\nnb_days_kr = 30\n#nb_days_kr = fun_days_between(date_0_kr)\n\narr_days, nb_conf_CV = model_r0_cv_forecast(r_0=r_0_kr, \n                                         nb_day_contag=NB_DAY_CONTAG_CV,\n                                         date_0=date_0_kr,\n                                         nb_cases_0=n_0_kr,\n                                         nb_days=nb_days_kr)\n    \n\n    \n# plot result (compare)\nfig = create_fig_compare_model(df_kcdc_interp, \"nb_cases\", arr_days, nb_conf_CV, \n                               r_0_kr, n_0_kr, \n                        \"South Korea : COVID-19 Confirmed cases vs Model R_0\")\n\ndate_lockdown = '2020-02-20'\ndate_slowdown = '2020-03-02'\nfig = add_annotation_lockdown(fig, df_kcdc_interp, \"nb_cases\", date_lockdown, \n                              date_slowdown, \n                              str_lock = \"Social distancing<br>Daegu\")\nfig.update_layout(legend_orientation=\"h\", legend=dict(x=0, y=1))\nfig.show()","9393848c":"'''#########\n# FRANCE\n#\n\n# data adaptation\ndf_fr[\"nb_cases\"] = df_fr[\"total\"]\n\n# param Country\nr_0_cv_fr = R_0_CV\n# init time\nn_0_fr = df_fr[\"total\"][0] * 10\ndate_0_fr = df_fr[\"date\"].astype(np.datetime64)[0]\nnb_days = 30\n\n# forecast\narr_days, nb_conf_CV = model_r0_cv_forecast(r_0=r_0_cv_fr, \n                                         nb_day_contag=NB_DAY_CONTAG_CV,\n                                         date_0=date_0_fr,\n                                         nb_cases_0=n_0_fr,\n                                         nb_days=nb_days)\n\n# plot result (compare)\nfig = create_fig_compare_model(df_fr, \"nb_cases\", arr_days, nb_conf_CV, \n                               r_0_cv_fr, n_0_fr, \n                               \"France : COVID-19 Confirmed cases vs Model R_0\")\n\ndate_lockdown = '2020-03-17'\nfig = add_annotation_lockdown(fig, df_fr,\"nb_cases\", date_lockdown,\n                             str_lock = \"Lockdown\")\nfig.show()'''\n\n\n#########\n# FRANCE from df_world\n#\n\n# retrieve data\ndf_cases_fr, df_death_fr = extract_data_world(df_world_melt, df_death_melt,\n                                              \"Country\/Region\", \"France\")\n# param Country\nr_0_cv_fr = R_0_CV * 0.85\n\n# init param model\nn_0_fr = df_cases_fr[df_cases_fr[\"nb_cases\"]>1].iloc[1][\"nb_cases\"] * 1.85\ndate_0_fr = df_cases_fr[df_cases_fr[\"nb_cases\"]>1].iloc[1][\"date\"] \nnb_days = 73\n#nb_days = fun_days_between(date_0_fr)\n# forecast with model\narr_days, nb_conf_CV = model_r0_cv_forecast(r_0=r_0_cv_fr, \n                                         nb_day_contag=NB_DAY_CONTAG_CV,\n                                         date_0=date_0_fr,\n                                         nb_cases_0=n_0_fr,\n                                         nb_days=nb_days)\n\n# plot result (compare)\nfig = create_fig_compare_model(df_cases_fr, \"nb_cases\", arr_days, nb_conf_CV, \n                               r_0_cv_fr, n_0_fr, \n                               \"France : COVID-19 Confirmed cases vs Model R_0\")\n\n# lockdown \ndate_lockdown = '2020-03-17'\ndate_slowdown = '2020-03-29'\nfig = add_annotation_lockdown(fig, df_cases_fr,\"nb_cases\", date_lockdown,\n                             date_slowdown, str_lock = \"Lockdown\")\nfig.update_layout(legend_orientation=\"h\", legend=dict(x=0, y=1))\nfig.show()","de71f999":"#########\n# Italy\n#\n#\n\n# retrieve data\ndf_cases_it, df_death_it = extract_data_world(df_world_melt, df_death_melt,\n                                              \"Country\/Region\", \"Italy\")\n# param Country\nr_0_cv = R_0_CV * 0.85\n\n# init param model\nn_0 = df_cases_it[df_cases_it[\"nb_cases\"]>1].iloc[1][\"nb_cases\"] * 28\ndate_0 = df_cases_it[df_cases_it[\"nb_cases\"]>1].iloc[1][\"date\"] \nnb_days = 60\n#nb_days = fun_days_between(date_0)\n# forecast with model\narr_days, nb_conf_CV = model_r0_cv_forecast(r_0=r_0_cv, \n                                         nb_day_contag=NB_DAY_CONTAG_CV,\n                                         date_0=date_0,\n                                         nb_cases_0=n_0,\n                                         nb_days=nb_days)\n\n# plot result (compare)\nfig = create_fig_compare_model(df_cases_it, \"nb_cases\", arr_days, nb_conf_CV, \n                               r_0_cv, n_0, \n                               \"Italy : COVID-19 Confirmed cases vs Model R_0\")\n\n# ITALY : 3 step in lockdown \n# - step 1 : 2020-02-27 (north cities = 50000 people) \n# - step 2 : 2020-03-08 (north of country), \n# - step 3 : 2020-03-10 (whole country) \n\n\nfig = add_annotation_lockdown(fig, df_cases_it, \"nb_cases\", '2020-02-27', \n                            str_lock = \"Lockdown cities\")\n\nfig = add_annotation_lockdown(fig, df_cases_it, \"nb_cases\", '2020-03-08', \n                             str_lock = \"L. province\")\n\nfig = add_annotation_lockdown(fig, df_cases_it, \"nb_cases\", '2020-03-10',\n                              '2020-03-22', str_lock = \"L. country\")\nfig.update_layout(legend_orientation=\"h\", legend=dict(x=0, y=1))\nfig.show()\n","e4b15380":"# Find death rate with Germany data \ndf_cases_germ = df_world_melt[df_world_melt[\"Country\/Region\"] == \"Germany\"]\ndf_death_germ = df_death_melt[df_death_melt[\"Country\/Region\"] == \"Germany\"]\nrate_death = df_death_germ['nb_death'].values[-1] \/ \\\n    df_cases_germ[\"nb_cases\"].values[-1] \nprint(\"rate_death (Germany): {:.3f} %\".format( 100*rate_death))\nprint(\" \")\nprint(\"So, number of real confirmed cases  = {:.1f} * nb deaths\" \\\n      .format(1\/ rate_death))","ce3f4cca":"######################\n# HUBEI PROVINCE CHINA (DEATH MODEL)\n#\n\n# param Country\nr_0_cv_hb = R_0_CV *0.8\n# init time\nn_0_hb = df_hubei_death[\"nb_death\"][0]*5\n\ndate_0_hb = df_hubei_death[\"date\"].astype(np.datetime64)[0]\nnb_days = 30\n\n# forecast\narr_days, nb_conf_CV = model_r0_cv_forecast(r_0=r_0_cv_hb, \n                                         nb_day_contag=NB_DAY_CONTAG_CV,\n                                         date_0=date_0_hb,\n                                         nb_cases_0=n_0_hb,\n                                         nb_days=nb_days)\n\n# plot result (compare)\nfig = create_fig_compare_model(df_hubei_death, \"nb_death\", arr_days, nb_conf_CV, \n                               r_0_cv_hb, n_0_hb, \n                            \"Hubei (China) : COVID-19 Death cases vs Model R_0\")\n\ndate_lockdown = '2020-01-23'\ndate_slowdown = '2020-02-10'\nfig = add_annotation_lockdown(fig, df_hubei_death, \"nb_death\", date_lockdown, \n                              date_slowdown, str_lock = \"Lockdown\")\nfig.update_layout(legend_orientation=\"h\", legend=dict(x=0, y=1))\nfig.show()\n","5629e72e":"#########\n# FRANCE\n#\n\n# param Country\nr_0_cv_fr = R_0_CV * 1.25\n# init time\n#n_0_fr = df_fr[\"total\"][0] * 10\nstr_date_0 = '2020-02-18'\n#date_0_fr = df_death_w_fr[\"date\"].astype(np.datetime64)[10]\ndate_0_fr = pd.Timestamp(str_date_0)\n#n_0_fr = df_death_w_fr[\"nb_death\"][0] *1\nn_0_fr = df_death_w_fr[df_death_w_fr[\"date\"] == str_date_0] \\\n    [\"nb_death\"].values[0]\n\n#nb_days = fun_days_between(date_0_fr)\nnb_days = 52\n# forecast\narr_days, nb_death_CV_fr = model_r0_cv_forecast(r_0=r_0_cv_fr, \n                                         nb_day_contag=NB_DAY_CONTAG_CV,\n                                         date_0=date_0_fr,\n                                         nb_cases_0=n_0_fr,\n                                         nb_days=nb_days)\n\n# plot result (compare)\nfig = create_fig_compare_model(df_death_w_fr, \"nb_death\", arr_days, \n                               nb_death_CV_fr, r_0_cv_fr, n_0_fr, \n                               \"France : COVID-19 Death vs Death R0 Model\")\n\ndate_lockdown = '2020-03-17'\ndate_slowdown = '2020-03-29'\nfig = add_annotation_lockdown(fig, df_death_w_fr,\"nb_death\", date_lockdown,\n                             date_slowdown, str_lock = \"Lockdown\")\nfig.update_layout(legend_orientation=\"h\", legend=dict(x=0, y=1))\nfig.show()","2ed314a1":"#########\n# FRANCE\n#\n\n# plot   \nimport plotly.graph_objects as go\nimport plotly.express as px\nfig = go.Figure()\n# Create and style traces\nfig.add_trace(go.Scatter(x=arr_days,\n                         y=nb_death_CV_fr * 1 \/ rate_death,\n                    mode='lines+markers',\n                    line_shape='linear',\n                    connectgaps=True, name=\"infected cases (Model R0)\"))\n\nfig.add_trace(go.Scatter(x=df_cases_fr[\"date\"].astype(np.datetime64), \n                         y=df_cases_fr[\"nb_cases\"],\n                    mode='lines+markers',\n                    line_shape='linear',\n                    connectgaps=True, name=\"confirmed cases\"))\n\n# Edit the layout\nfig.update_layout(\n    title=\\\n    'FRANCE : COVID-19 Infected cases estimated from by death R_0 Model :<br>' +\n    'start={}, R_0={:.2f}, ' \\\n    .format(str_date_0, r_0_cv_fr) + 'N_0={:.2f}'.format(n_0_fr),\n                   yaxis_title='nb infected cases')\nfig.update_layout(legend_orientation=\"h\", legend=dict(x=0, y=1))\nfig.show()\n","553d9d9a":"#########\n# Italy\n#\n#\n# param Country\nr_0_cv = R_0_CV * 1\n\n# init param model\nn_0 = df_death_it[df_death_it[\"nb_death\"]>1].iloc[1][\"nb_death\"] * 18\ndate_0 = df_death_it[df_death_it[\"nb_death\"]>1].iloc[1][\"date\"] \nnb_days = 40\n#nb_days = fun_days_between(date_0)\n# forecast with model\narr_days, nb_conf_CV = model_r0_cv_forecast(r_0=r_0_cv, \n                                         nb_day_contag=NB_DAY_CONTAG_CV,\n                                         date_0=date_0,\n                                         nb_cases_0=n_0,\n                                         nb_days=nb_days)\n\n# plot result (compare)\nfig = create_fig_compare_model(df_death_it, \"nb_death\", arr_days, nb_conf_CV, \n                               r_0_cv, n_0, \n                               \"Italy : COVID-19 Death vs Death Model R_0\")\n\n# ITALY : 3 step in lockdown \n# - step 1 : 2020-02-27 (north cities = 50000 people) \n# - step 2 : 2020-03-08 (north of country), \n# - step 3 : 2020-03-10 (whole country) \n\n\nfig = add_annotation_lockdown(fig, df_death_it, \"nb_death\", '2020-02-27', \n                            str_lock = \"Lockdown cities\")\n\nfig = add_annotation_lockdown(fig, df_death_it, \"nb_death\", '2020-03-08', \n                             str_lock = \"L. province\")\n\nfig = add_annotation_lockdown(fig, df_death_it, \"nb_death\", '2020-03-10',\n                              '2020-03-24', str_lock = \"L. country\")\nfig.update_layout(legend_orientation=\"h\", legend=dict(x=0, y=1))\nfig.show()","1e77fed1":"### Fitting Death cases \n\nComfirmed cases doesn't reflect the real number of cases. Some countries like Italy or France test only very sick people...\n\nWe calculate  the evaluation of confirmed cases using the number of deaths.\n\nFor that, we use the lowest death rate from Germany.\n\nAssuming that :\n - Death rate 0 = nb death (Germany) \/ nb cases (Germany) \n - Number of cases =  Nb_death (country) \/ Death Rate 0\n - We know NB_DAY_CONTAG_CV = 14\n - BUT we ajust R_0_CV  (2.5 for China) \n - We know the number of first cases, n_0 \n - BUT we don't know when exactly the first case appeared : so we estimate the date of first case to fit the curve on actual data\n\n","27f4b8c4":"URLs News article from KCDC are retrieved.","1900b30d":"# Models (experimental)","45a63b7e":"The most simple model : Nb_cases(D) - Nb_cases(D-1) = N_cases(D-1) * R_0_CV \/ NB_DAY_CONTAG_CV\n- R_0 : number of people infected by one people during his contagious state.\n- NB_DAY_CONTAG : number of contagious days for infected cases : \n\nThis model is very simplify because it is not time-synchronized (real infected case are difficult to evaluate : data shows only detected cases).\nActually, R_0 is dynamic not fixed!\n\nsources for model : \n- https:\/\/en.wikipedia.org\/wiki\/Basic_reproduction_number\nsources for data : \n- https:\/\/ici.radio-canada.ca\/nouvelle\/1648395\/covid-19-maladies-infectieuses-contagion-letalite-guy-boivin\n- https:\/\/www.europe1.fr\/sante\/coronavirus-pendant-combien-de-temps-sommes-nous-contagieux-3955894","1142f41d":"### Compare death France vs. Italy","f1054034":"### Table by areas","3d08c390":"#### Italy","68cc86c2":"### Map France : Confirmed Tested & Rt","faaa6654":"South Korea have done a lot of test if you compare with other countries (except Germany).\n\nWe can see how they manage to stop the increase by : \n    - Social distanciation\n    - Mask\n    - Transparency about all cases (mobile app to follow paths of new cases)","3832ecd0":"### Time evolution of death cases in France","00a198fc":"- <a href='#Table-by-areas'>Table of confirmed cases<\/a> \n\n- <a href='#Time-evolution-of-total-confirmed-cases-in-South-Korea'>Time-evolution of total confirmed cases<\/a> \n\n- <a href='#Confirmed-cases-by-Area'>Confirmed cases by Area<\/a>\n\n- <a href='#Map-of-South-Korea-regions'>Map with animation by Area<\/a>","e1898913":"#### Hubei province","7f057c2f":"### Table by areas in France","85e5da8e":"### South Korea","91c4ac90":"### Map of France regions of death cases","b6cf733b":"### Time-evolution of total confirmed cases in France","bbd52d1a":"### Rt in France : Effective Reproduction Number","5ad1e63b":"#### Italy","c28e1cca":"# World confirmed cases","b7afe561":"### France","a1f65640":"## Confirmed cases in France","929ca53c":"## Death cases","071a319d":"#### France","ab6a5f1a":"# World death cases","3ffd3b81":"Information about number of cases are read from inside KCDC articles. ","697b6c09":"## Confirmed cases in South Korea\n","72c1637a":"### Time-evolution of total confirmed cases in South Korea","062d384f":"### Confirmed cases by Area","0f488710":"### Fitting Number cases \nAssuming that :\n - number of case are almost real number of cases.\n - we don't know the number of first cases : n_0\n - we know R_0_CV = 2.5 and NB_DAY_CONTAG_CV = 14","a43ae4ef":"# World recovered cases","532aa05d":"This notebook shows the evolution of COVID-19 virus all over the world.\n\nI added a very simple model to estimate actual cases number, and a very close-time forecast\n\nYou can see data by areas\/state\/regions in USA, South Korea & France.\n\nAnimated Maps are Available for World,  (USA, South Korea, and France by states).\n\nCode to retrieved Data is into this notebook.\nSo, this notebook scrap data from French and Korean Health official websites.\nIf you discover the code, you can see how or in my github : https:\/\/github.com\/jeugregg\/coronavirusModel \n\n\n- DATA : \n\nThe world data is taken from https:\/\/github.com\/CSSEGISandData\/COVID-19 provided by JHU CSSE\n\nSouth Korea areas data are retrieved with scrapy from online KCDC Press Release articles  http:\/\/www.kdca.go.kr\/board.es?mid=a20501010000&bid=0015 (before : https:\/\/www.cdc.go.kr\/board\/board.es?mid=a30402000000&bid=0030).\n\nFrance areas data are taken with scrapy from online santepubliquefrance.fr Press articles at https:\/\/www.santepubliquefrance.fr\/maladies-et-traumatismes\/maladies-et-infections-respiratoires\/infection-a-coronavirus\/articles\/infection-au-nouveau-coronavirus-sars-cov-2-covid-19-france-et-monde\nand https:\/\/www.worldometers.info\/coronavirus\/country\/france\/\nbut until 25th March 2020.\n\nFrance departements last status map : data taken from : https:\/\/www.data.gouv.fr\/fr\/datasets\/donnees-de-certification-electronique-des-deces-associes-au-covid-19-cepidc\/\n\nFrance GeoJson data from : https:\/\/github.com\/gregoiredavid\/france-geojson simplified with https:\/\/mapshaper.org\/\n\nFor Global France, data are from https:\/\/www.data.gouv.fr\/fr\/datasets\/donnees-relatives-aux-resultats-des-tests-virologiques-covid-19\/\n\nFor Global Italy, Germany, Hubei data are from https:\/\/www.worldometers.info\/coronavirus\/\n\n(Code available on my github : https:\/\/github.com\/jeugregg\/coronavirusModel)","e411905a":"### Map of France regions","bebd0b83":"#### France","8a12f206":"# World active cases","489db3ed":"#### Hubei province","0edd7d3a":"### Compare France vs. Italy","f2ba86d0":"## France  (Confirmed \/ Tested \/ Rt) (last status)","0edca9da":"#  Coronavirus Visualization & Modeling","b926ff8b":"Try to estimate the actual number of infected people in France :\n\nWe can see that we estimate that :\n    - only 10% of infected people are detected \n    - => 90% sick people are not tested\n    - But According to doctors, 80% of people are not sick or just a little...\n    - So 10% of people are maybe not tested but really sick \/ or the death rate is lower...","047563a2":"- <a href='#Rt-in-France-:-Effective-Reproduction-Number'>France : Curves Last status Rt (Effective Reproduction Number) NEW!!!<\/a> \n- <a href='#Map-France-:-Confirmed-Tested-&-Rt'>France : Map Last status (Confirmed-Tested-Rt) NEW!!!<\/a> \n\n\n- <a href='#Time-evolution-of-total-confirmed-cases-in-France'>Confirmed cases total Time-evolution<\/a> \n\n- <a href='#Time-evolution-of-death-cases-in-France'>Death cases Time-evolution by Regions<\/a> \n\n- <a href='#Map-of-France-regions-of-death-cases'>Death cases Map with animation by Regions<\/a>\n\n- <a href='#Compare-death-France-vs.-Italy'>Death cases Compare France vs. Italy<\/a> \n\n\nUntil 25th March 2020 : \n\n- <a href='#Table-by-areas-in-France'>Table of confirmed cases by Regions until 25th March 2020<\/a> \n\n- <a href='#Confirmed-cases-in-France'>Confirmed cases by Regions until 25th March 2020<\/a>\n\n- <a href='#Map-of-France-regions'>Map with animation by Regions until 25th March 2020<\/a>\n\n\n","084a0337":"## Direct links ","8f1a20f5":"- <a href='#World-confirmed-cases'>World Confirmed cases (Map with animation)<\/a> \n\n- <a href='#World-death-cases'>World Death cases (Map with animation)<\/a> \n\n- <a href='#World-recovered-cases'>World Recovered cases (Map with animation)<\/a> \n\n- <a href='#World-active-cases'>World Active cases<\/a>\n","bfd97934":"#### South Korea","bbf2ed0d":"# South Korea cases","99e1b691":"## R0 model","4ccfb8a6":"### Models","c08392a1":"### Map of South Korea regions","735529b5":"Reproduction number higher than other country at first.\n\nMaybe because the biggest cluster in Daegu (Shincheonji Church meeting)","886c0085":"# France cases","cb4e66db":"### World","d25dc82e":"- <a href='#Models-(experimental)'>Models (experimental)<\/a> "}}