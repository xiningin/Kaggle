{"cell_type":{"36b75f1b":"code","8543ed25":"code","abd1f2b3":"code","7f1f2845":"code","d9dcc4e4":"code","738257de":"code","f117a7bd":"code","3dacaa07":"code","69dc786a":"code","0f0f1997":"code","00fd9fcc":"code","2da24e69":"code","3ee1def8":"code","04d963a4":"code","ea5a7eda":"code","6782291c":"code","92d0b26f":"markdown","f2455f1e":"markdown","6b4fe33d":"markdown","c4d0e5d4":"markdown","64bcc586":"markdown","bc52481e":"markdown","c6852d80":"markdown","f078cdea":"markdown","27b2c498":"markdown","e1be7410":"markdown","e828be73":"markdown","0c4746c1":"markdown","e15a6cc1":"markdown","bea9db68":"markdown","27192c6c":"markdown","a13179d8":"markdown","f166d55e":"markdown","8debfd2c":"markdown","da27f47e":"markdown","18375b56":"markdown","62e60113":"markdown"},"source":{"36b75f1b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode('utf8'))\n\n# Any results you write to the current directory are saved as output.","8543ed25":"# Load datasets\nx_data = np.load('..\/input\/Sign-language-digits-dataset\/X.npy')\ny_data = np.load('..\/input\/Sign-language-digits-dataset\/Y.npy')\nimg_size = 64  # pixel size\n\n # for sign zero\nplt.subplot(1,2,1)  \nplt.imshow(x_data[260])  # Get 260th index\nplt.axis(\"off\")\n\n# for sign one\nplt.subplot(1,2,2)\nplt.imshow(x_data[900])  # Get 900th index\nplt.axis(\"off\")","abd1f2b3":"# From 0 to 204 is zero sign, from 205 to 410 is one sign\nX = np.concatenate((x_data[204:409], x_data[822:1027] ), axis=0)\n\n# We will create their labels. After that, we will concatenate on the Y.\nz = np.zeros(205)\no = np.ones(205)\nY = np.concatenate((z,o), axis=0).reshape(X.shape[0],1)\nprint(\"X shape: \", X.shape)\nprint(\"Y shape: \", Y.shape)","7f1f2845":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, random_state=42)\n# random_state = Use same seed while randomizing\nprint(x_train.shape)\nprint(y_train.shape)","d9dcc4e4":"x_train_flatten = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\nx_test_flatten = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\nprint('x_train_flatten: {} \\nx_test_flatten: {} '.format(x_train_flatten.shape, x_test_flatten.shape))","738257de":"# Here we will change the location of our samples and features. '(328,4096) -> (4096,328)' \nx_train = x_train_flatten.T\nx_test = x_test_flatten.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x train: \", x_train.shape)\nprint(\"x test: \", x_test.shape)\nprint(\"y train: \", y_train.shape)\nprint(\"y test: \", y_test.shape)","f117a7bd":"# Now let's create the parameter and sigmoid function. \n# So what we need is dimension 4096 that is number of pixel as a parameter for our initialize method(def)\n\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension, 1), 0.01)\n    b = 0.0\n    return w,b\n\n# Sigmoid function\n# z = np.dot(w.T, x_train) +b\ndef sigmoid(z):\n    y_head = 1\/(1 + np.exp(-z))  # sigmoid function finding formula\n    return y_head\nsigmoid(0)  # 0 should result in 0.5","3dacaa07":"def initialize_parameters_and_layer_sizes_NN(x_train, y_train):\n    parameters = {\n        \"weight1\": np.random.randn(3,x_train.shape[0]) * 0.1, \n# the reason we say 3, 4096: The number of rows in our weight has to be 3 because we have 3 nodes\n        \"bias1\": np.zeros((3,1)), # That is same for bias \n        \"weight2\": np.random.randn(y_train.shape[0], 3) * 0.1,\n        \"bias2\": np.zeros((y_train.shape[0],1))\n    }\n    return parameters","69dc786a":"# We only process 2 times because we use tanh function.\ndef forward_NN(x_train, parameters):\n    Z1 = np.dot(parameters[\"weight1\"], x_train) + parameters[\"bias1\"]\n    A1 = np.tanh(Z1)  # We can do this easily with the numpy library\n    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n    \n    cache = {\n        \"Z1\": Z1,\n        \"A1\": A1,\n        \"Z2\": Z2,\n        \"A2\": A2\n    }\n    return A2, cache","0f0f1997":"# Compute cost\n# We take A2 as input and use it here\ndef compute_cost_NN(A2, Y, parameters):\n# We multiply the y_head value (A2) with the actual single value (Y).\n    logprobs = np.multiply(np.log(A2),Y)\n# cost : We collect all our losts\n    cost = - np.sum(logprobs) \/ Y.shape[1]\n    return cost","00fd9fcc":"def backward_NN(parameters, cache, X, Y):\n    # We are doing derivative transactions\n    dZ2 = cache[\"A2\"] - Y\n    dW2 = np.dot(dZ2, cache[\"A1\"].T) \/ X.shape[1]\n    # keepdims : He's holding as an Array. We are writing in array, even though the result of our collection is different (consten).\n    db2 = np.sum(dZ2, axis=1, keepdims=True) \/ X.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T, dZ2) * (1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1, X.T) \/ X.shape[1]\n    db1 = np.sum(dZ1, axis=1, keepdims=True) \/ X.shape[1]\n    # grads : We're storing grads. Changes in Weight1, bias1, Weight2, bias2. We store the derivatives of these according to them.\n    grads = {\n        \"dweight1\": dW1,\n        \"dbias1\": db1,\n        \"dweight2\": dW2,\n        \"dbias2\": db2,\n    }\n    return grads","2da24e69":"# Learning_rate: It is a hyper parameter. A parameter we'll find by trying.\ndef update_NN(parameters, grads, learning_rate = 0.003):\n    parameters = {\n        \"weight1\": parameters[\"weight1\"] - learning_rate * grads[\"dweight1\"],\n        \"bias1\": parameters[\"bias1\"] - learning_rate * grads[\"dbias1\"],\n        \"weight2\": parameters[\"weight2\"] - learning_rate * grads[\"dweight2\"],\n        \"bias2\": parameters[\"bias2\"] - learning_rate * grads[\"dbias2\"],\n    }\n    return parameters","3ee1def8":"def predict_NN(parameters, x_test):\n    #x_test is a input for forward propagation\n    A2, cache = forward_NN(x_test, parameters)\n    Y_prediction = np.zeros((1, x_test.shape[1]))\n    # if z is bigger than 0.5, our predictioan is sign one(y_head=1),\n    # if z is smaller than 0.5, our predictioan is sign zero(y_head=0),    \n    for i in range(A2.shape[1]):\n        if A2[0,i] <= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    return Y_prediction","04d963a4":"# Layer Neural Network\ndef layer_neural_network(x_train, y_train, x_test, y_test, num_iterations):\n    # We store Cost and Indexes for analysis.\n    cost_list = []\n    index_list = []\n    # initialize parameters and layer sizes\n    # We determine how many nodes in our layer.\n    parameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)\n    \n    for i in range(0, num_iterations):\n        # forward propagation\n        A2, cache = forward_NN(x_train, parameters)\n        # compute cost\n        cost = compute_cost_NN(A2, y_train, parameters)\n        # backward propagation\n        grads  = backward_NN(parameters, cache, x_train, y_train)\n        # update parameters\n        parameters = update_NN(parameters, grads)\n        \n        # It will store cost in cost_list per hundred steps. Same goes for index.\n        if i % 100 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print(\"Cost after iteration %i: %f\" %(i,cost))\n    \n    plt.plot(index_list, cost_list)\n    plt.xticks(index_list, rotation='vertical')\n    plt.xlabel(\"Number of iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    # predict\n    y_prediction_test = predict_NN(parameters, x_test)\n    y_prediction_train = predict_NN(parameters, x_train)\n    \n    # Print test\/train errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))    \n\n    return parameters\n\nparameters = layer_neural_network(x_train, y_train, x_test, y_test, num_iterations = 2500) ","ea5a7eda":"# We need to get transposing when using Keras. Because it's easier.\n# reshaping\nx_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T  ","6782291c":"#Evaluating the ANN\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # build our layers library\n# build classifier: We are building a structure that will form the neural network.\ndef build_classifier():\n    classifier = Sequential()  # initialize neural network\n    \n    # Dense: It's building the layers.\n    #   - units=8: We have eight node.\n    #   - kernel_initializer: The values we first define in weights. It will be randomly distributed with uniform.  \n    #   - relu: If input < 0, x = 0 indicates(sample: relu(-6)=0). If input > 0, x = x indicates(sample: relu(6)=6).\n    #   - input_dim: 4096 px.\n    classifier.add(Dense(units=8, kernel_initializer='uniform', activation='relu', input_dim = x_train.shape[1]))\n    classifier.add(Dense(units=4, kernel_initializer='uniform', activation='relu'))\n    classifier.add(Dense(units=2, kernel_initializer='uniform', activation='relu'))\n    # - sigmoid: We're adding our last output layer. That will be our output layer.\n    classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))            \n    \n    # We will find loss and cost.\n    # - adam: Adaptive momentum. If we use Adam, learning_rate is not fixed. It updates Learning_rate and enables us to learn more quickly.\n    # - loss: The same lost function that we use in Linear Regression.\n    # - metrics: Evaluation metric. We choose Accuracy.\n    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return classifier\n\n# - build_fn: This parameter calls the neural network we built.\n# epochs: number of iteration\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 100)\n    \n# cross_val_score: It gives us more than one accuracy and we get a more effective result by taking averages of them. \n# estimator: We determine the classifier to use.\n# cv : Find 3 times accuracy and then we'll average it, after that we find a more effective and more accurate result.\naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv =3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean:\" + str(mean))\nprint(\"Accuracy variance:\" + str(variance))","92d0b26f":"<a id=\"7\"><\/a> <br>\n### E) Update parameters","f2455f1e":"#### Implementing with keras library","6b4fe33d":"* Now x and y 2D\n* 4096 = 64 * 64","c4d0e5d4":"<a id=\"6\"><\/a> <br>\n### D) Backward propagation","64bcc586":"* Now we reserve 80% of the values as 'train' and 20% as 'test'.\n* Then let's create x_train, y_train, x_test, y_test arrays\n\n","bc52481e":"<a id=\"10\"><\/a> <br>\n## 4) L Layer Neural Network","c6852d80":"<a id=\"8\"><\/a> <br>\n### F) Prediction\n---\n* Our trained model is ready. \n* We will prediction our trained model.\n* We will check whether it is.","f078cdea":"* Activation Functions\n    * Sigmoid : The sigmoid function, which is a little more advanced in the step function, is usually used as an activation function at the output node when binary estimates are made.\n    * Relu : It replaces negative values with zero and is the most used activation function.\n    * Tanh : It takes a real-value input and takes it to the [-1, 1] range.","27b2c498":"* Now we will concatenate our pictures consisting of 0 and 1.\n* We have image of 255 one sign, 255 zero sign","e1be7410":"<a id=\"11\"><\/a> <br>\n> # CONCLUSION \n* If you want a more detailed kernel. Check out DATAI TEAM's Deep Learning Tutorial for Beginners Kernel.  https:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners\n---\n<br> **Thank you for your votes and comments.**                                                                                                                                             \n<br>**If you have any suggest, May you write for me, I will be happy to hear it.**","e828be73":"<a id=\"9\"><\/a> <br>\n### G) Creat Model\n---\n* Now we're gonna combine them all.","0c4746c1":"  * Since our data in X is 3D, we need to flatten it to 2D to use Deep Learning.\n  * Since our data in Y is 2D, we don't need to flatten.","e15a6cc1":"* The shape of the X is (410, 64, 64)\n    * 410 means that we have 410 images (zero and one signs)\n    * 64 means that our image size is 64x64 (64x64 pixels)\n* The shape of the Y is (410,1)\n    * 410 means that we have 410 labels (0 and 1)","bea9db68":"<a id=\"0\"><\/a> <br>\n## 1) Introduction\n* We will be working on this kernel Sign Language data. We'll introduce 80% of the sign language we have, and we will try to predict the remaining 20%.\n* In this Kernel we will do the Artificial Neural Network (ANN) step by step.\n* Let's start by creating our libraries\n","27192c6c":"<a id=\"2\"><\/a> <br>\n## 3) Artificial Neural Network (ANN)","a13179d8":"# Cihan Yatbaz\n###  04 \/ 12 \/ 2018\n---\n\n\n1.  [Introduction:](#0)\n2. [Preparing Dataset :](#1)\n3. [Artificial Neural Network (ANN):](#2)\n    1. [Initializing parameters  :](#3)\n    1. [Forward propagation  :](#4)\n    1. [Loss and Cost Function  :](#5)\n    1. [Backward propagation  :](#6)\n    1. [Update parameters  :](#7)\n    1. [Prediction  :](#8)\n    1. [Creat Model  :](#9)\n\n5. [L Layer Neural Network :](#10)\n6. [CONCLUSION :](#11)","f166d55e":"<a id=\"3\"><\/a> <br>\n### A) Initializing parameters","8debfd2c":"<a id=\"1\"><\/a> <br>\n## 2) Preparing Dataset\n---\n* Now we'll upload our library and then let's see the 0 and 1 signs we'll work on.","da27f47e":"* Hidden Layer : As the number increases, it starts to explore more complex things. And discovers the data better. Can define objects more easily as their numbers increase.\n* Relu is also faster than others because the derivative is easy to take.","18375b56":"<a id=\"5\"><\/a> <br>\n### C) Loss and Cost Function","62e60113":"<a id=\"4\"><\/a> <br>\n### B) Forward propagation"}}