{"cell_type":{"812c2847":"code","833ad5e2":"code","65bbdf71":"code","8be372f3":"code","2642a19e":"code","00980c30":"code","8e5cfadd":"code","91786fa4":"code","8a80452f":"code","96a909b0":"code","53dfd3ec":"code","653e8fe7":"code","9f9351c2":"code","e15d945d":"code","02fe9a3a":"code","ff1bba64":"code","3ec9f044":"code","2f181d0a":"code","319ff3f9":"code","3c8d558e":"code","685bbee4":"code","44cefb88":"code","65faeb1a":"code","0a22f700":"code","e0cd4b29":"code","4c190dcc":"code","552ef1fb":"code","4a621d57":"code","99472e3f":"code","e9cf33bb":"code","fa6647dc":"code","6f5e8eb3":"code","c24efa51":"code","552d84b7":"code","a82b8f6f":"code","b241a9ac":"code","9cbe142d":"code","933f5a72":"code","dd183afd":"code","f1e4ebb5":"code","6bddc700":"code","433b10fc":"code","f1cb0322":"code","d2350323":"code","8f8725be":"code","a3d54f77":"code","223c29f9":"code","389a61d8":"code","4dd01c2c":"code","3e323790":"code","5da2ec55":"code","ec4a4064":"code","16dc5b84":"code","3ad09dad":"code","e7d68be6":"code","c6f279c1":"code","db7a15ea":"code","7c818927":"code","75dc833b":"code","c3ec0f90":"markdown","ce8c2f40":"markdown","dfa546e4":"markdown","af1728fd":"markdown","8a52f722":"markdown","7cb0ec04":"markdown","5ebdba03":"markdown","c5754994":"markdown","e3a7f600":"markdown","7b74b0d5":"markdown","23c76e4b":"markdown","52d51a29":"markdown","4870d26f":"markdown"},"source":{"812c2847":"from google.colab import drive\ndrive.mount('\/content\/gdrive')","833ad5e2":"import os\nos.environ['KAGGLE_CONFIG_DIR'] = \"\/content\/gdrive\/My Drive\/Kaggle\"","65bbdf71":"#changing the working directory\n%cd \/content\/gdrive\/My Drive\/Kaggle\n\n#Check the present working directory using pwd command","8be372f3":"!pip install transformers","2642a19e":"import torch\nimport torch.optim as optim\n\nimport random\nimport string\n\n# fastai\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.callbacks import *\n\nfrom transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\nfrom transformers import BertForSequenceClassification, BertTokenizer, BertConfig\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\nfrom transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\nfrom transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig","00980c30":"!kaggle competitions download -c nlp-getting-started","8e5cfadd":"path=Path(\"\/content\/gdrive\/My Drive\/Kaggle\")","91786fa4":"train_df= pd.read_csv(path\/\"train.csv\")\ntest_df= pd.read_csv(path\/\"test.csv\")\nsample_df= pd.read_csv(path\/\"sample_submission.csv\")","8a80452f":"ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ntrain_df.loc[train_df['id'].isin(ids_with_target_error),'target'] = 0","96a909b0":"train_df = train_df[[\"text\",\"target\"]]\ntest_df = test_df[['text','id']]","53dfd3ec":"train_df = train_df.dropna()\ntest_df = test_df.dropna()","653e8fe7":"def clean(text):\n    \"\"\"\n    Text preprocessing function.\n    \"\"\"\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","9f9351c2":"train_df['clean_text'] = train_df['text'].apply(lambda x:clean(x))\ntest_df['clean_text'] = test_df['text'].apply(lambda x:clean(x))","e15d945d":"MODEL_CLASSES = {\n    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)\n}","02fe9a3a":"# Parameters\nseed = 42\nepochs=15\nuse_fp16=True\nbs=32\ndiscriminative=False\nmax_seq_len=128\n\nmodel_type = 'roberta'\npretrained_model_name = 'roberta-base'\n\n# model_type = 'bert'\n# pretrained_model_name='bert-base-uncased'\n\n# model_type = 'distilbert'\n# pretrained_model_name = 'distilbert-base-uncased'\n\n#model_type = 'xlm'\n#pretrained_model_name = 'xlm-clm-enfr-1024'\n\n# model_type = 'xlnet'\n# pretrained_model_name = 'xlnet-base-cased'","ff1bba64":"model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]","3ec9f044":"def seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False","2f181d0a":"seed_all(seed)","319ff3f9":"class TransformersBaseTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n        self._pretrained_tokenizer = pretrained_tokenizer\n        self.max_seq_len = pretrained_tokenizer.max_len\n        self.model_type = model_type\n\n    def __call__(self, *args, **kwargs): \n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n        CLS = self._pretrained_tokenizer.cls_token\n        SEP = self._pretrained_tokenizer.sep_token\n        if self.model_type in ['roberta']:\n            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n            tokens = [CLS] + tokens + [SEP]\n        else:\n            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n            if self.model_type in ['xlnet']:\n                tokens = tokens + [SEP] +  [CLS]\n            else:\n                tokens = [CLS] + tokens + [SEP]\n        return tokens","3c8d558e":"transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\ntransformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\nfastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])","685bbee4":"class TransformersVocab(Vocab):\n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        super(TransformersVocab, self).__init__(itos = [])\n        self.tokenizer = tokenizer\n    \n    def numericalize(self, t:Collection[str]) -> List[int]:\n        \"Convert a list of tokens `t` to their ids.\"\n        return self.tokenizer.convert_tokens_to_ids(t)\n        #return self.tokenizer.encode(t)\n\n    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n        \"Convert a list of `nums` to their tokens.\"\n        nums = np.array(nums).tolist()\n        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\n    \n    def __getstate__(self):\n        return {'itos':self.itos, 'tokenizer':self.tokenizer}\n\n    def __setstate__(self, state:dict):\n        self.itos = state['itos']\n        self.tokenizer = state['tokenizer']\n        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})","44cefb88":"transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\nnumericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\ntokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\ntransformer_processor = [tokenize_processor, numericalize_processor]","65faeb1a":"pad_first = bool(model_type in ['xlnet'])\npad_idx = transformer_tokenizer.pad_token_id","0a22f700":"databunch = (TextList.from_df(train_df, cols='clean_text', processor=transformer_processor)\n             .split_by_rand_pct(0.01,seed=seed)\n             .label_from_df(cols= 'target')\n             .add_test(test_df)\n             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))","e0cd4b29":"# defining our model architecture \nclass CustomTransformerModel(nn.Module):\n    def __init__(self, transformer_model: PreTrainedModel):\n        super(CustomTransformerModel,self).__init__()\n        self.transformer = transformer_model\n        \n    def forward(self, input_ids, attention_mask=None):\n        \n        # attention_mask\n        # Mask to avoid performing attention on padding token indices.\n        # Mask values selected in ``[0, 1]``:\n        # ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        attention_mask = (input_ids!=pad_idx).type(input_ids.type()) \n        \n        logits = self.transformer(input_ids,\n                                  attention_mask = attention_mask)[0]   \n        return logits","4c190dcc":"config = config_class.from_pretrained(pretrained_model_name)\nconfig.num_labels = 2\nconfig.use_bfloat16 = use_fp16\nconfig.max_length = max_seq_len","552ef1fb":"transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\ncustom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)","4a621d57":"from fastai.callbacks import *\nfrom transformers import AdamW\nfrom functools import partial\n\nCustomAdamW = partial(AdamW, correct_bias=False)","99472e3f":"learner = Learner(databunch, \n                  custom_transformer_model, \n                  opt_func = CustomAdamW, \n                  metrics=[accuracy, error_rate])","e9cf33bb":"# Show graph of learner stats and metrics after each epoch.\nlearner.callbacks.append(ShowGraph(learner))\n\n# Put learn in FP16 precision mode. --> Seems to not working\nif use_fp16: learner = learner.to_fp16()","fa6647dc":"# For roberta-base\nlist_layers = [learner.model.transformer.roberta.embeddings,\n              learner.model.transformer.roberta.encoder.layer[0],\n              learner.model.transformer.roberta.encoder.layer[1],\n              learner.model.transformer.roberta.encoder.layer[2],\n              learner.model.transformer.roberta.encoder.layer[3],\n              learner.model.transformer.roberta.encoder.layer[4],\n              learner.model.transformer.roberta.encoder.layer[5],\n              learner.model.transformer.roberta.encoder.layer[6],\n              learner.model.transformer.roberta.encoder.layer[7],\n              learner.model.transformer.roberta.encoder.layer[8],\n              learner.model.transformer.roberta.encoder.layer[9],\n              learner.model.transformer.roberta.encoder.layer[10],\n              learner.model.transformer.roberta.encoder.layer[11],\n              learner.model.transformer.roberta.pooler]","6f5e8eb3":"learner.split(list_layers)\nnum_groups = len(learner.layer_groups)\nprint('Learner split in',num_groups,'groups')\nprint(learner.layer_groups)","c24efa51":"learner.save('bert-untrained')","552d84b7":"seed_all(seed)\nlearner.load('bert-untrained')","a82b8f6f":"learner.freeze_to(-1)","b241a9ac":"learner.lr_find()","9cbe142d":"learner.recorder.plot(suggestion=True)","933f5a72":"learner.fit_one_cycle(1, max_lr=1.10E-04)","dd183afd":"learner.save('bert-first-cycle')","f1e4ebb5":"learner.freeze_to(-2)","6bddc700":"learner.lr_find()","433b10fc":"learner.recorder.plot(suggestion=True)","f1cb0322":"learner.fit_one_cycle(1, max_lr=4E-05)","d2350323":"learner.save('bert-second-cycle')","8f8725be":"learner.freeze_to(-3)","a3d54f77":"learner.lr_find()","223c29f9":"learner.recorder.plot(suggestion=True)","389a61d8":"learner.fit_one_cycle(1, max_lr=6.92E-06)","4dd01c2c":"learner.save('bert-third-cycle')","3e323790":"learner.unfreeze()","5da2ec55":"learner.lr_find()","ec4a4064":"learner.recorder.plot(suggestion=True)","16dc5b84":"learner.fit_one_cycle(1, max_lr=1.32E-06)","3ad09dad":"learner.save('bert-fourth-cycle')","e7d68be6":"seed_all(seed)\nlearner.load('bert-third-cycle');","c6f279c1":"sub_df=test_df","db7a15ea":"sub_df['target'] = sub_df['clean_text'].apply(lambda x:int(learner.predict(x)[1]))","7c818927":"final_sub = sub_df[['id','target']]\nfinal_sub.to_csv(path\/\"roberta.csv\", index=False)","75dc833b":"!kaggle competitions submit -c nlp-getting-started -f roberta.csv -m \"RoBERTa\"","c3ec0f90":"# 7 Submission to Kaggle","ce8c2f40":"# Intro\n\n### Credits\nThis notebook is based on the famous [Fastai with \ud83e\udd17 Transformers (BERT, RoBERTa, ...)](https:\/\/www.kaggle.com\/maroberti\/fastai-with-transformers-bert-roberta) article from [Maximilien Roberti](https:\/\/www.kaggle.com\/maroberti). I have implemented a few minor tweaks.\n\nThe data cleaning part is from [Alexander Tesemnikov](https:\/\/www.kaggle.com\/alexandertesemnikov\/bert-pytorch-pretrained-fine-tuning). Erroneous classification cleanups are from [Sokol Heavy](https:\/\/www.kaggle.com\/sokolheavy\/multi-dropout-aug-oof).\n\n**If you find this notebook helpful, please upvote their input! \ud83d\ude4f\ud83d\ude42**\n\n### Short Info\n\n* **fast.ai** Databunch and Learner\n* **Hugging Face** transformers Library\n* **RoBERTa** (multiple other architectures are also included)\n* **Google Colab** Environment\n\n","dfa546e4":"# 5 Model Creation","af1728fd":"# 4 Databunch Creation","8a52f722":"#2 Load Data","7cb0ec04":"# 6 Optimization of Model","5ebdba03":"# 0 Google Drive Connection","c5754994":"# 1 Import Libraries\n\n","e3a7f600":"## 6.3 Round 3: Layer Freeze -3","7b74b0d5":"## 6.4 Round 4: Unfreeze all Layers","23c76e4b":"## 6.1 Round 1: Layer Freeze -1","52d51a29":"## 6.2 Round 2: Layer Freeze -2","4870d26f":"# 3 Definition of Models"}}