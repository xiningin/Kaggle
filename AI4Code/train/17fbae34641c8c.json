{"cell_type":{"a2cce95c":"code","41fd3873":"code","4cc371ad":"code","cc0fb325":"code","193fac31":"code","f5a3ba66":"code","d35fb935":"code","efaeced3":"code","e036b2bc":"code","ae9bf08e":"code","fbf71b45":"code","6c84f797":"code","ade8d74e":"code","9db52bc4":"code","f8ebece8":"code","a267b13b":"code","1a2749ba":"code","2f3ca780":"code","64788e16":"code","69fa246f":"code","ee0eaf5a":"code","000a9360":"code","4b1846ef":"code","6685f1d6":"code","45863b5e":"code","cc6181f0":"code","908c3cf6":"code","33a912b5":"code","ca4e28ad":"code","c74d2798":"code","05781cc7":"code","3953f240":"code","8166739d":"code","f0b6eb52":"code","a69e86ff":"code","ff80d69f":"code","8a805741":"code","4a3fc2a0":"code","f677a680":"code","cfd117a8":"code","6bfe32a5":"code","ea9ec06f":"code","8e8ef34b":"code","2d309de0":"code","f79bfe7e":"code","abdae771":"code","68e8f46f":"code","e612e373":"code","cecda7fc":"code","9a0bf008":"code","9404aa7d":"code","daeac80d":"code","92c22f6d":"code","66d32254":"code","15868891":"code","0a21ec85":"code","d2be4e27":"code","0fce5400":"code","809f73cd":"code","8003c752":"code","257a971b":"code","b22f08b8":"code","3850edb3":"code","230d7043":"code","78907f4a":"code","074ad0a1":"code","09b443cf":"code","76cadfce":"code","1a1f6b29":"code","ecfa5dd9":"code","113458dc":"code","aa115cbc":"code","a2a51e28":"code","b6ba5b10":"code","bc209289":"code","74307465":"code","4ba69956":"code","576160ab":"code","bcc387bd":"code","4909087e":"code","f54ed935":"code","4f868d91":"code","65f2cb08":"code","123d72c2":"code","6c6cc252":"code","284c2d65":"code","12734da4":"code","4bc7d0f6":"code","9d07eb9b":"code","e9afe33e":"code","27a03d5b":"code","5ac163de":"code","5e73e85b":"code","7f80063e":"code","8c9cd629":"code","c666b22f":"code","3a3d97a2":"code","11d2e35e":"code","e08b72d2":"code","0473a72d":"code","5fd5163d":"code","c67fbecb":"code","79e2da9a":"markdown","af152e79":"markdown","2673080f":"markdown","dece7dd3":"markdown"},"source":{"a2cce95c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","41fd3873":"# Initial importing libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport category_encoders as ce\nfrom sklearn.feature_selection import f_regression, SelectKBest, f_classif\nfrom sklearn.linear_model import Ridge, LogisticRegression, LogisticRegressionCV\nfrom sklearn.metrics import mean_absolute_error, accuracy_score, classification_report\nfrom sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier, XGBRegressor","4cc371ad":"# Enabling the entire df to be viewed when it goes beyond the normal 80 cols\/rows\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.width', 1000)","cc0fb325":"# Suppressing warning messages\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn","193fac31":"# Reading in the csv for the training features\ndf = pd.read_csv('..\/input\/train_features.csv')\ndf.head().T","f5a3ba66":"# Reading in the csv for the training labels\ndf_labels = pd.read_csv('..\/input\/train_labels.csv')\ndf_labels.head()","d35fb935":"# Reading in the csv for the test data\ndf_test = pd.read_csv('..\/input\/test_features.csv')\ndf_test.head().T","efaeced3":"# verifying shape of these dataframes\ndf.shape, df_labels.shape, df_test.shape","e036b2bc":"# Verifying the value counts of the target value, status_group\ndf_labels.status_group.value_counts(normalize=True)","ae9bf08e":"# Verifying the data types of each column in our dataframe\ndf.dtypes","fbf71b45":"# Just selecting the numeric features from the features dataframe\ndf_num = df.select_dtypes(include=['number'])\nprint(df_num.shape)\ndf_num.head()","6c84f797":"# Just selecting the numeric features from the test dataframe\ndf_num_test = df_test.select_dtypes(include=['number'])\nprint(df_num_test.shape)\ndf_num_test.head()","ade8d74e":"# Initializing our X and y variables for regression \nX = df_num\ny = df_labels.status_group","9db52bc4":"# Using Logistic Regression model to make baseline prediction\nmodel = LogisticRegression(solver='lbfgs', multi_class='auto')\nmodel.fit(X, y)\ny_pred = model.predict(X)\nprint(classification_report(y, y_pred))\nprint('accuracy', accuracy_score(y, y_pred))","f8ebece8":"# Fitting and scoring our Logistic Regression model\nlog_reg = LogisticRegression(solver='lbfgs').fit(X, y)\nlog_reg.score(X, y)","a267b13b":"# Predicting outcomes on our test data\npred_test = log_reg.predict(df_num_test)","1a2749ba":"# creating a dataframe for the predicted outcomes\npred = pd.DataFrame(pred_test)\npred.head()","2f3ca780":"# Adding the 'id' column to the prediction dataframe\npred = pd.concat([df_num_test.id, pred], axis=1)\npred.head()","64788e16":"# Remaning the status_group column to the correct name\npred.columns.values[1]= 'status_group'\npred.head()","69fa246f":"# Verifying the dataframe has the correct shape for submission\npred.shape","ee0eaf5a":"# ***PRINT TO CSV***\npred.to_csv('pred_test.csv', sep=',', encoding='utf-8', index=False)\n# Or use this to print to csv\n#pd.DataFrame(pred).to_csv(\"submission_pd.csv\", index = False)","000a9360":"# Find columns with unique categorical observations\n# I did this to try to see how many categorical features \n# could\/should be used for One Hot encoding\ndf_unique = df.select_dtypes(exclude=['number'])\ndf_unique.nunique()","4b1846ef":"# Check data types of this dataframe to ensure non-numerics\ndf_unique.dtypes","6685f1d6":"# Counting total number of unique cat observations\ndf_unique.nunique().sum()","45863b5e":"# Checking to describe of unique dataframe\ndf_unique.describe()","cc6181f0":"# Dropping columns I dont find useful(too many unique values\/odd observations)\ndf_unique_drop = df_unique.drop(columns=['date_recorded', 'funder', 'installer', 'wpt_name', \n                            'lga', 'ward', 'recorded_by', 'scheme_name', \n                            'subvillage', 'public_meeting', 'scheme_management', 'permit',\n                            'funder', 'installer', 'scheme_name'])\n  \n# Head check for dropped unique\ndf_unique_drop.head()","908c3cf6":"# Checking unique dataframe after drop\ndf_unique_drop.nunique()","33a912b5":"# Recount of unique observations after drop\ndf_unique_drop.nunique().sum()\n","ca4e28ad":"# The unique dataframe was not as easy to work with\n# so back to working with the original data\n# verifying the numeric columns\ndf.describe()","c74d2798":"# Dropping columns I dont find useful(too many unique values\/odd observations)\ndf_drop = df.drop(columns=['id', 'longitude', 'latitude', 'num_private', \n                            'date_recorded', 'funder', 'installer', 'wpt_name', \n                            'lga', 'ward', 'recorded_by', 'scheme_name', \n                            'subvillage', 'public_meeting', 'scheme_management', 'permit',\n                            'funder', 'installer', 'scheme_name'])\n  \ndf_drop.head()","05781cc7":"# Verifying nulls\ndf_drop.isnull().sum()","3953f240":"# Another null check\ndef no_nulls(df):\n    return not any(df.isnull().sum())\n\nno_nulls(df_drop)","8166739d":"# Verifying head of dataframe after dropping columns\ndf_drop.head()","f0b6eb52":"# Verifying numeric features\ndf_drop.describe()","a69e86ff":"# Verifying which columns are numeric and which are strings\/objects\ndf_drop.dtypes","ff80d69f":"# 1hot encode non-numeric columns\ndf_one_hot = pd.get_dummies(df_drop)\ndf_one_hot.head()","8a805741":"# Dropping same columns on the test dataframe\ndf_drop_test = df_test.drop(columns=['id', 'longitude', 'latitude', 'num_private', \n                            'date_recorded', 'funder', 'installer', 'wpt_name', \n                            'lga', 'ward', 'recorded_by', 'scheme_name', \n                            'subvillage', 'public_meeting', 'scheme_management', 'permit',\n                            'funder', 'installer', 'scheme_name'])\n                           \ndf_drop_test.head()","4a3fc2a0":"# Null verification\ndf_drop_test.isnull().sum()","f677a680":"# More null verification\ndef no_nulls(df):\n    return not any(df.isnull().sum())\n\nno_nulls(df_drop_test)","cfd117a8":"# checking numeric features\ndf_drop_test.describe()","6bfe32a5":"# Verifying shape to ensure we have correct feature and observation counts\ndf_drop_test.shape","ea9ec06f":"# 1hot encode non-numeric columns for test data\ndf_one_hot_test = pd.get_dummies(df_drop_test)\ndf_one_hot_test.head()","8e8ef34b":"# Swapping '\/' characters for '_' from data as it was causing errors, in test dataframe\ndf_one_hot_test.columns = [x.strip().replace('\/', '_') for x in df_one_hot_test.columns]\n\ndf_one_hot_test.head().T","2d309de0":"# Checking head after one hot encoding\ndf_one_hot.head().T","f79bfe7e":"# Checking numeric cols after one hot\ndf_one_hot.describe()","abdae771":"# Fixing '\/' to '_', this time on train dataframe\ndf_one_hot.columns = [x.strip().replace('\/', '_') for x in df_one_hot.columns]\ndf_one_hot.head().T","68e8f46f":"# Had one column in train that was not in test so I had to drop it from train dataframe so \n# the number of features in test and train match\ndf_one_hot = df_one_hot.drop(columns=['extraction_type_other - mkulima_shinyanga'])\ndf_one_hot.head()","e612e373":"# Setting X and y variables\nX = df_one_hot\ny = df_labels.status_group","cecda7fc":"# Fit Logistic Regression model on X and y\nmodel = LogisticRegression(solver='lbfgs')\nmodel.fit(X, y)\ny_pred = model.predict(X)\nprint(classification_report(y, y_pred))\nprint('accuracy', accuracy_score(y, y_pred))","9a0bf008":"# Fit and score Logistic Regression model\nlog_reg = LogisticRegression().fit(X, y)\nlog_reg.score(X, y)","9404aa7d":"# Scoring prediction with test data \npred_test = log_reg.predict(df_one_hot_test)","daeac80d":"# Sending prediction to a dataframe and verifying correct shape\npred = pd.DataFrame(pred_test)\npred.shape","92c22f6d":"# Adding 'id' column to dataframe\npred = pd.concat([df_test.id, pred], axis=1)\npred.head()","66d32254":"# Renaming status_group column\npred.columns.values[1]= 'status_group'\npred.head()","15868891":"# Verifying data looks to be in acceptable ranges\npred.describe()","0a21ec85":"# ***PRINT TO CSV***\npred.to_csv('pred_test_again.csv', sep=',', encoding='utf-8', index=False)","d2be4e27":"# Reread in data to have fresh dataframes\ndf_train = pd.read_csv('..\/input\/train_features.csv')\ndf_labels = pd.read_csv('..\/input\/train_labels.csv')\ndf_test = pd.read_csv('..\/input\/test_features.csv')","0fce5400":"# Working towards Random Forest and XGBoost here\n# Verify head of training data\ndf_train.head()","809f73cd":"# verify head of test data\ndf_test.head()","8003c752":"# verify head of feature labels\ndf_labels.head()","257a971b":"# Checking shape of each dataframe\ndf_train.shape, df_labels.shape, df_test.shape","b22f08b8":"# merge train and labels datasets\ndf_merged = pd.merge(df_train, df_labels, on='id')\ndf_merged.head().T","3850edb3":"# set terget \ntarget=df_merged.pop('status_group')","230d7043":"# Verify head of target\ntarget.head()","78907f4a":"# Verify head of merged sataframe\ndf_merged.head().T","074ad0a1":"# Check shapes of each dataframe\ndf_train.shape, df_labels.shape, df_test.shape, df_merged.shape, target.shape","09b443cf":"# Verifying merged dataframe feature info\ndf_merged.info()","76cadfce":"# Adding 'train' column to training and test dataframes and adding value of \n# 1 if from training dataframe and 0 if from test dataframe\ndf_merged['train']=1\ndf_test['train']=0\ndf_merged.info()","1a1f6b29":"# Verifying test dataframe feature info\ndf_test.info()","ecfa5dd9":"# Dataframe shape verification\ndf_merged.shape, df_test.shape","113458dc":"# concatenating the train and test dateframes for munging\ncombined = pd.concat([df_merged, df_test])\ncombined.head()","aa115cbc":"# verifying dataframe feature info\ncombined.info()","a2a51e28":"# Dropping features I do not feel fit the prediction model we will be using\ncombined.drop('construction_year',axis=1,inplace=True)\ncombined.drop('date_recorded',axis=1,inplace=True)\ncombined.drop('wpt_name',axis=1,inplace=True)\ncombined.drop('num_private',axis=1,inplace=True)\ncombined.drop('subvillage',axis=1,inplace=True)\ncombined.drop('region_code',axis=1,inplace=True)\ncombined.drop('ward',axis=1,inplace=True)\ncombined.drop('public_meeting',axis=1,inplace=True)\ncombined.drop('recorded_by',axis=1,inplace=True)\ncombined.drop('scheme_name',axis=1,inplace=True)\ncombined.drop('permit',axis=1,inplace=True)\ncombined.drop('extraction_type_group',axis=1,inplace=True)\ncombined.drop('extraction_type_class',axis=1,inplace=True)\ncombined.drop('management_group',axis=1,inplace=True)\ncombined.drop('payment',axis=1,inplace=True)\ncombined.drop('quality_group',axis=1,inplace=True)\ncombined.drop('quantity_group',axis=1,inplace=True)\ncombined.drop('source_type',axis=1,inplace=True)\ncombined.drop('source_class',axis=1,inplace=True)\ncombined.drop('waterpoint_type_group',axis=1,inplace=True)\ncombined.drop('installer',axis=1,inplace=True)\ncombined.info()","b6ba5b10":"# Factorizing the remaining categorical features\n# factorize swaps unique categorical observations into a unique numeric\ncombined['funder'] = pd.factorize(combined['funder'])[0]\ncombined['scheme_management'] = pd.factorize(combined['scheme_management'])[0]\ncombined['extraction_type'] = pd.factorize(combined['extraction_type'])[0]\ncombined['management'] = pd.factorize(combined['management'])[0]\ncombined['payment_type'] = pd.factorize(combined['payment_type'])[0]\ncombined['water_quality'] = pd.factorize(combined['water_quality'])[0]\ncombined['quantity'] = pd.factorize(combined['quantity'])[0]\ncombined['source'] = pd.factorize(combined['source'])[0]\ncombined['waterpoint_type'] = pd.factorize(combined['waterpoint_type'])[0]\ncombined['basin'] = pd.factorize(combined['basin'])[0]\ncombined['region'] = pd.factorize(combined['region'])[0]\ncombined['lga'] = pd.factorize(combined['lga'])[0]\ncombined['district_code'] = pd.factorize(combined['district_code'])[0]\ncombined.district_code.head(5)","bc209289":"# Splitting the combined dataframe back into test and train using the 'train' feature we added above\ntrain_df = combined[combined[\"train\"] == 1]\ntest_df = combined[combined[\"train\"] == 0]","74307465":"# Verifying the test dataframe \ntest_df.head()","4ba69956":"# checking to ensure the train dataframe has only training data \ntrain_df.train.value_counts()","576160ab":"# checking to ensure the test dataframe has only test data \ntest_df.train.value_counts()","bcc387bd":"# verifying shapes are correct\ntrain_df.shape, test_df.shape","4909087e":"# Dropping the 'train' column we added above\ntrain_df.drop([\"train\"], axis=1, inplace=True)\ntrain_df.head()","f54ed935":"# verifying shape is correct\ntrain_df.shape","4f868d91":"# Dropping the 'train' column we added above\ntest_df.drop([\"train\"], axis=1, inplace=True)\ntest_df.head()","65f2cb08":"# verifying shape is correct\ntest_df.shape","123d72c2":"#defining X as the entire train dataframe\nX = train_df\n# defining the target as 'y'\ny = target","6c6cc252":"# Setting RendomForestClassifier estimators\nmodel_rfc = RandomForestClassifier(n_estimators=1000)","284c2d65":"# Setting cross validation score inputs\ncross_val_score(model_rfc, X, y, cv=3)","12734da4":"# Fitting the Random Forest model\nmodel_rfc.fit(X,y)\nX.info() # Just printing the features to aid in matching features to score below\nimportances = model_rfc.feature_importances_\nimportances # list of feature scores, can match using above print out","4bc7d0f6":"# Printing and sorting the most important features from Random Forest model\nimportances = model_rfc.feature_importances_\nimportances\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\nfor f in range(X.shape[1]):\n    print(X.columns[indices[f]],end=', ')","9d07eb9b":"# Random Forest model fitting\nmodel_rfc.fit(X,y)","e9afe33e":"# Model set up for XGBoost classification model\nmodel = XGBClassifier(objective = 'multi:softmax', booster = 'gbtree', nrounds = 'min.error.idx', \n                      num_class = 4, maximize = False, eval_metric = 'merror', eta = .2,\n                      max_depth = 14, colsample_bytree = .4)","27a03d5b":"# XGBoost fit and scoring\nprint(cross_val_score(model, X, y, cv=3))\nmodel.fit(X,y)\nimportances = model.feature_importances_\nimportances\nindices = np.argsort(importances)[::-1]","5ac163de":"# Print the feature ranking without score\nprint(\"Feature ranking:\")\n\nfor f in range(X.shape[1]):\n    print(X.columns[indices[f]],end=', ')","5e73e85b":"# Printing feature ranking with rank and score\nfor f in range(X.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))","7f80063e":"# Setting X_test initial value\nX_test=test_df","8c9cd629":"# Verify head of X_test looks good\nX_test.head()","c666b22f":"# Run test dataframe through model to receive score\na=X_test['id']\n#X_test.drop(['id'],axis=1, inplace=True)\ny_pred = model.predict(X_test)","3a3d97a2":"# Place prediction into dataframe\ny_pred=pd.DataFrame(y_pred)\ny_pred.head()","11d2e35e":"# adding 'id feature to dataframe'\ny_pred['id']=a\ny_pred.head()","e08b72d2":"# Renaming columns in dataframe\ny_pred.columns=['status_group','id']\ny_pred.head()","0473a72d":"# Swapping columns in dataframe to fit the Kaggle submission requirements\ny_pred=y_pred[['id','status_group']]\ny_pred.head()","5fd5163d":"# Verify shape of prediciton\ny_pred.shape","c67fbecb":"# Output dataframe to CSV for Kaggle submission\npd.DataFrame(y_pred).to_csv(\"fifth_try.csv\", index=False)","79e2da9a":"The XGBoost model was the best score, barely beating out the Random Forest model.","af152e79":"Submission number 2 was a success!","2673080f":"First baseline test completed and submitted","dece7dd3":"Large improvement with Random Forest model on prediction score over Logistic Regression"}}