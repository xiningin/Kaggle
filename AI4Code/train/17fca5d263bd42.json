{"cell_type":{"700431bd":"code","107c6b4d":"code","b5a9548b":"code","99c6da10":"code","13cc7693":"code","1a070799":"code","146078e1":"code","c767c0d5":"code","23b652d1":"markdown","c07a31e1":"markdown","777721c4":"markdown","d0613786":"markdown","a3375673":"markdown","369c450a":"markdown","8c234eff":"markdown"},"source":{"700431bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier #our algorithm library.\nfrom sklearn.model_selection import train_test_split #Separates for learning and prediction \n\nfrom sklearn.metrics import accuracy_score #Accuracy Score, what is our success rate ?\nfrom sklearn import tree\nimport numpy\n\n\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","107c6b4d":"df = pd.read_csv(\"\/kaggle\/input\/iris-flower-dataset\/IRIS.csv\")\ndf.head()\n# We printed our first 5 lines.\n\n\nX = df.iloc[:, 0:4]\nY = df.iloc[:, 4]\n# We specified our class column.\n\n\n","b5a9548b":"X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.3)","99c6da10":"clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,\n                               max_depth=10, min_samples_leaf=8)","13cc7693":"clf_gini.fit(X_train, y_train)","1a070799":"y_pred = clf_gini.predict(X_test)\nprint(y_pred)","146078e1":"print(clf_gini.predict([[5,2,3,1]]))\n# sepal_length = 5\n# sepal_width=2\n# petal_length=3\t\n# petal_width=1\n# Which flower can have these values?","c767c0d5":"\nprint (\"Accuracy is \", accuracy_score(y_test,y_pred)*100)","23b652d1":"Let's run the algorithm","c07a31e1":"Now I call the gini algorithm for decision tree study.","777721c4":"To get high accuracy from decision trees, we need to divide our data into two, prediction and learn. I divided it as 70 percent learning and 30 percent as prediction. I used the code below for this.\n","d0613786":"\nLet's explain our hyperparameters.\n* max_depth = The depth of our tree. We indicate how many 'if' structures will be nested. \n* min_samples_leaf =It refers to the minimum number of instances required to split an internal node.","a3375673":"Let's print out the algorithm's predictions now.","369c450a":"Our algorithm worked.Now we will enter random numbers ourselves for prediction. And the algorithm will predict which flower these values belong to.","8c234eff":"Let's measure the accuracy of our algorithm."}}