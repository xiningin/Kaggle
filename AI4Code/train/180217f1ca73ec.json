{"cell_type":{"8919a39f":"code","b54ad1a9":"code","efab0e04":"code","da3cbad8":"code","27c1829e":"code","9dd332ba":"code","fc6a0767":"code","ad30fc5b":"code","3f23b6b0":"markdown","fedecf5b":"markdown","75272422":"markdown","8227712f":"markdown","d964e4b4":"markdown","c260ce41":"markdown","5aaed93b":"markdown","278bf4c7":"markdown","01c27473":"markdown"},"source":{"8919a39f":"import tensorflow as tf\nimport os\nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport pandas as pd\n\nprint(tf.__version__)\nprint(os.listdir('..\/input\/'))","b54ad1a9":"name_file_train = 'sign_mnist_train.csv'\nname_file_test ='sign_mnist_test.csv'\ndirectory = '..\/input\/sign-language-mnist\/'\n\nfile_train = os.path.join(directory,name_file_train)\nfile_test = os.path.join(directory, name_file_test)\n\nprint(directory)\nprint(file_train)\nprint(file_test)","efab0e04":"def get_data(filename):\n    df = pd.read_csv(filename, header=0)\n    labels= np.array(df.iloc[:,0].values)\n    imgs = df.iloc[:,1:].values\n    data = []\n    for img in imgs:\n        tmp =np.array( np.array_split(img,28))\n        data.append(tmp)\n    \n    data = np.array(data).astype('float')    \n    return data, labels\n\ntraining_images, training_labels = get_data(file_train)\ntesting_images, testing_labels = get_data(file_test)\n\nprint(training_images.shape)\nprint(training_labels.shape)\nprint(testing_images.shape)\nprint(testing_labels.shape)","da3cbad8":"training_images = np.expand_dims(training_images,axis=3)\ntesting_images = np.expand_dims(testing_images,axis=3)\n\nprint(training_images.shape)\nprint(testing_images.shape)","27c1829e":"# Create an ImageDataGenerator \n# and do Image Augmentation\ntrain_datagen = ImageDataGenerator(\n    rescale=1.0\/255.0,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2, \n    horizontal_flip=True,\n    fill_mode='nearest'\n    )\n\nvalidation_datagen = ImageDataGenerator(\n    rescale=1.0\/255.0)","9dd332ba":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32,(3,3), activation='relu', input_shape=(28,28,1)),\n    tf.keras.layers.MaxPool2D(2,2),\n    tf.keras.layers.Conv2D(64,(3,3), activation='relu'),\n    tf.keras.layers.MaxPool2D(2,2),\n    tf.keras.layers.Conv2D(128,(3,3), activation='relu'),\n    tf.keras.layers.MaxPool2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1024, activation='relu'),\n    tf.keras.layers.Dense(26, activation='softmax')    \n])\n\nmodel.summary()","fc6a0767":"# Compile Model. \nmodel.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the Model\nhistory = model.fit_generator(\n    train_datagen.flow(training_images,training_labels,batch_size=32),\n    steps_per_epoch=len(training_images)\/32,\n    epochs=50,\n    validation_data=validation_datagen.flow(testing_images, testing_labels, batch_size=32),\n    validation_steps=len(testing_images)\/32\n                             )\n\nmodel.evaluate(testing_images, testing_labels)","ad30fc5b":"import matplotlib.pyplot as plt\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","3f23b6b0":"### Get_data function\n It is function that recieve the filename path and **return two objets**.\n 1.  training images\n 2.  label images\n \nThe task it was to created:\n 1. a dataframe from file using pandas library\n 2. isolate the column label.\n 3. Get the array images\n 4. Transform the list on np.array and gives them it a 28 to 28 shape for each line\n","fedecf5b":"The idea of this notebook it to predict Sign language competion using TF 2.0 (keras) and Data augmentation","75272422":"<html>\n     <body>\n         <p><font size=\"6\" color=\"white\">Sign Language MNIST with CNN in Tensorflow 2.0 & Image Aumentation.<\/font><\/p>\n     <\/body>     ![tensorflow](https:\/\/www.gstatic.com\/devrel-devsite\/v0bcab1972c3b1579b82e221c810ad3eb061cbd75c8520328bda82087ceb07528\/tensorflow\/images\/lockup.svg)","8227712f":"### Image Generation\n\n We use the ImageDataGenerator, this class not override the information of the folder. Only expand the information on memory. \n We rotate, shift and zoom. And we fill the picture on the transformation with the neares picture.","d964e4b4":"### visualization\n We visualize the training and validation history","c260ce41":"### Expand Dimension\n\n We define the Matrix on Training and Label. We need to include the color channel (a new dimension) and the because is black and white the value of the channel it is one.","5aaed93b":"### Model Compilation and Monitoring variables\n The compilation that we use is 'adam' or we can consider other that work we momentun. (not GD)","278bf4c7":"##  Define the Directory and Files","01c27473":"### Sequential Model\n We create 3 convolutional layers each one\u00b4s with their on MaxPooling. Before to create the Dense Layer we Dropout a twenty percent to avoid overfitting"}}