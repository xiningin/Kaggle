{"cell_type":{"15cf2c2c":"code","ff01ba8d":"code","2903a5cd":"code","53fd81c8":"code","1bbf9bb3":"code","6df8fe46":"code","62e8c527":"code","f8403bc4":"code","83468ab7":"code","fe22b914":"code","9c770662":"code","c7585062":"code","590d09f4":"code","f1caeba1":"code","dad9f62f":"code","16181858":"code","4d69b44b":"code","b1ab99e0":"code","0fa56dd0":"code","0768fe13":"code","c38d8ca9":"code","d7aff331":"code","17dae74b":"code","bf82655d":"code","ccac4869":"code","cad7ca91":"code","75c46c5a":"code","7478a120":"code","02d9e987":"code","9ded2843":"code","f5c06efe":"code","0a93041f":"code","f15a608a":"code","f62a300f":"code","de485726":"code","d06f78d9":"code","3ef829c3":"code","3455d9d4":"code","813db530":"code","f4da7629":"code","fc54fb54":"code","dd659ff4":"code","64dc5e37":"code","f66f625d":"code","f8353834":"code","dcc33842":"markdown","0b1d7225":"markdown","6f38adb7":"markdown","054210ed":"markdown","fd1acff7":"markdown","f514f7b7":"markdown","3bf141f8":"markdown","1ef15fb3":"markdown","8ad2c54a":"markdown","1abc2774":"markdown","7c04ed1c":"markdown","e734ab77":"markdown"},"source":{"15cf2c2c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ff01ba8d":"import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom xgboost import XGBClassifier\nfrom pandas import DataFrame\nfrom sklearn.svm import SVC\n# Set seed for reproducibility\nSEED = 123","2903a5cd":"data = pd.read_csv('\/kaggle\/input\/cascadecup\/train_age_dataset.csv')","53fd81c8":"data.head()","1bbf9bb3":"Y = data['age_group']\nX = data.drop('age_group',1)","6df8fe46":"# Split dataset into 70% train, 30% test\nX_train, X_test, y_train, y_test= train_test_split(X, Y, test_size=0.1, random_state=SEED)","62e8c527":"# fit scaler on training data\nnorm = MinMaxScaler().fit(X_train)\n\n# transform training data\nX_train_norm = norm.transform(X_train)\n\n# transform testing dataabs\nX_test_norm = norm.transform(X_test)","f8403bc4":"DataFrame(X_train_norm).describe()","83468ab7":"# fit scaler on training data\nstdscale = StandardScaler().fit(X_train)\n\n# transform training data\nX_train_std = stdscale.transform(X_train)\n\n# transform testing dataabs\nX_test_std = stdscale.transform(X_test)","fe22b914":"DataFrame(X_train_std).describe()","9c770662":"# Instantiate individual classifiers\nlr = LogisticRegression(max_iter = 500, n_jobs=-1, random_state=SEED)\nknn = KNN()\ndt = DecisionTreeClassifier(random_state=SEED)\nsvc = SVC(kernel='rbf', probability = True, random_state=SEED)\nrf = RandomForestClassifier(random_state=SEED)\n\n# Define a list called classifier that contains the tuples (classifier_name, classifier)\nclassifiers = [('Logistic Regression', lr),\n('K Nearest Neighbours', knn),\n('SVM', svc),\n('Random Forest Classifier', rf),\n('Decision Tree', dt)]              ","c7585062":"# Iterate over the defined list of tuples containing the classifiers\nfor clf_name, clf in classifiers:\n    #fit clf to the training set\n    clf.fit(X_train, y_train)\n    # Predict the labels of the test set\n    y_pred = clf.predict(X_test)\n    # Evaluate the accuracy of clf on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))","590d09f4":"# Iterate over the defined list of tuples containing the classifiers\nfor clf_name, clf in classifiers:\n    #fit clf to the training set\n    clf.fit(X_train_norm, y_train)\n    # Predict the labels of the test set\n    y_pred = clf.predict(X_test_norm)\n    # Evaluate the accuracy of clf on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))","f1caeba1":"# Iterate over the defined list of tuples containing the classifiers\nfor clf_name, clf in classifiers:\n    #fit clf to the training set\n    clf.fit(X_train_std, y_train)\n    # Predict the labels of the test set\n    y_pred = clf.predict(X_test_std)\n    # Evaluate the accuracy of clf on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))","dad9f62f":"from keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, LSTM, Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline","16181858":"# encode class values as integers\nencoder = LabelEncoder()\nencoder.fit(y_train)\nencoded_Y = encoder.transform(y_train)\n# convert integers to dummy variables (i.e. one hot encoded)\ndummy_y = np_utils.to_categorical(encoded_Y)","4d69b44b":"# encode class values as integers\nencoder = LabelEncoder()\nencoder.fit(y_test)\nencoded_Y = encoder.transform(y_test)\n# convert integers to dummy variables (i.e. one hot encoded)\ndummy_y1 = np_utils.to_categorical(y_test)","b1ab99e0":"input_shape = [X.shape[1]]","0fa56dd0":"earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\nmcp_save = ModelCheckpoint('best', save_best_only=True, monitor='val_accuracy', mode='max')\nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')","0768fe13":"model = Sequential()\nmodel.add(BatchNormalization(input_shape=input_shape))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(4, activation='softmax'))\n# Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',callbacks=[earlyStopping, mcp_save, reduce_lr_loss], metrics=['accuracy'])","c38d8ca9":"model.fit(X_train, dummy_y,validation_split = 0.1,batch_size=64,epochs=30)","d7aff331":"y_pred = model.predict_classes(X_test)\ny_pred = y_pred+1\n# print accuracy\nprint(\"Accuracy: \", accuracy_score(y_test, y_pred))\n\n# print precision, recall, F1-score per each class\/tag\nprint(classification_report(y_test, y_pred))\n\n# print confusion matrix, check documentation for sorting rows\/columns\nprint(confusion_matrix(y_test, y_pred))","17dae74b":"model = XGBClassifier()","bf82655d":"eval_set = [(X_train, y_train), (X_test, y_test)]","ccac4869":"model.fit(X_train, y_train, eval_metric=[\"merror\", \"mlogloss\"], eval_set=eval_set, verbose=True)\n# make predictions for test data\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","cad7ca91":"model = XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.01,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      n_estimators=1000, \n                      reg_alpha = 0.3,\n                      max_depth=4, \n                      gamma=10)","75c46c5a":"model.fit(X_train,y_train, eval_metric=[\"merror\", \"mlogloss\"], eval_set=eval_set, verbose=True)","7478a120":"# make predictions for test data\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","02d9e987":"model = XGBClassifier(  learning_rate=0.1,  \n               colsample_bytree = 0.4,\n                      subsample = 0.3,\n                      n_estimators=200,\n                      max_depth=5)","9ded2843":"model.fit(X_train,y_train, eval_metric=[\"merror\", \"mlogloss\"], eval_set=eval_set, verbose=True)","f5c06efe":"# make predictions for test data\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","0a93041f":"model =  XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n nthread=4, scale_pos_weight=1, seed=27)","f15a608a":"model.fit(X_train,y_train, eval_metric=[\"merror\", \"mlogloss\"], eval_set=eval_set, verbose=True)\n# make predictions for test data\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","f62a300f":"for n in range(3,10,2):\n        model =  XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=n,\n         min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n         nthread=4, scale_pos_weight=1, seed=27)\n        model.fit(X_train,y_train, eval_metric=[\"merror\", \"mlogloss\"], eval_set=eval_set, verbose=True)\n        # make predictions for test data\n        y_pred = model.predict(X_test)\n        predictions = [round(value) for value in y_pred]\n        # evaluate predictions\n        accuracy = accuracy_score(y_test, predictions)\n        print(\"n\",n,\"Accuracy: %.2f%%\" % (accuracy * 100.0))","de485726":"for n in range(1,6,2):\n        model =  XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=9,\n         min_child_weight=n, gamma=0, subsample=0.8, colsample_bytree=0.8,\n         nthread=4, scale_pos_weight=1, seed=27)\n        model.fit(X_train,y_train, eval_metric=[\"merror\", \"mlogloss\"], eval_set=eval_set, verbose=True)\n        # make predictions for test data\n        y_pred = model.predict(X_test)\n        predictions = [round(value) for value in y_pred]\n        # evaluate predictions\n        accuracy = accuracy_score(y_test, predictions)\n        print(\"n\",n,\"Accuracy: %.2f%%\" % (accuracy * 100.0))","d06f78d9":"for n in range(100,400,50):\n        model =  XGBClassifier( learning_rate =0.1, n_estimators=n, max_depth=9,\n         min_child_weight=5, gamma=0, subsample=0.8, colsample_bytree=0.8,\n         nthread=4, scale_pos_weight=1, seed=27)\n        model.fit(X_train,y_train, eval_metric=[\"merror\", \"mlogloss\"], eval_set=eval_set, verbose=True)\n        # make predictions for test data\n        y_pred = model.predict(X_test)\n        predictions = [round(value) for value in y_pred]\n        # evaluate predictions\n        accuracy = accuracy_score(y_test, predictions)\n        print(\"n\",n,\"Accuracy: %.2f%%\" % (accuracy * 100.0))","3ef829c3":"model = XGBClassifier(\n learning_rate =0.08,\n n_estimators=1500,\n max_depth=9,\n min_child_weight=6,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n reg_alpha=0.005,\n nthread=4,\n scale_pos_weight=1,\n seed=27)","3455d9d4":"model.fit(X_train, y_train, eval_metric=[\"merror\", \"mlogloss\"], eval_set=eval_set, verbose=True)\n# make predictions for test data\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","813db530":"model.fit(X, Y, eval_metric=[\"merror\", \"mlogloss\"], eval_set=eval_set, verbose=True)\n# make predictions for test data\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","f4da7629":"model = XGBClassifier(\n learning_rate =0.06,\n n_estimators=1500,\n max_depth=10,\n min_child_weight=8,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n reg_alpha=0.005,\n nthread=4,\n scale_pos_weight=1,\n seed=27)","fc54fb54":"model.fit(X_train, y_train, eval_metric=[\"merror\", \"mlogloss\"], eval_set=eval_set, verbose=True)\n# make predictions for test data\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","dd659ff4":"model.fit(X, Y, eval_metric=[\"merror\", \"mlogloss\"], eval_set=eval_set, verbose=True)\n# make predictions for test data\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","64dc5e37":"model = XGBClassifier(\n learning_rate =0.05,\n n_estimators=2000,\n max_depth=10,\n min_child_weight=8,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n reg_alpha=0.01,\n nthread=4,\n scale_pos_weight=1,\n seed=27)","f66f625d":"model.fit(X_train, y_train, eval_metric=[\"merror\", \"mlogloss\"], eval_set=eval_set, verbose=True)\n# make predictions for test data\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","f8353834":"d = pd.read_csv('\/kaggle\/input\/cascadecup\/test_age_dataset.csv')\ny_pred = model.predict(d)\nsubmission = pd.read_csv('\/kaggle\/input\/cascadecup\/sample_submission.csv')\nsubmission.prediction = y_pred\nsubmission.to_csv('submission.csv', index=False)","dcc33842":"# Feature Scaling","0b1d7225":"### Normalize the data","6f38adb7":"Most of the times, our dataset will contain features highly varying in magnitudes, units and range. \nBut since, most of the machine learning algorithms use Eucledian distance between two data points in their computations. \nWe need to bring all features to the same level of magnitudes. This can be achieved by scaling. \nThis means that you\u2019re transforming your data so that it fits within a specific scale, like 0\u2013100 or 0\u20131.","054210ed":"### Models prediction with Standardized data","fd1acff7":"# Introduction\nIn this notebook, I tried to focus on finding the Best Machine Learning (ML) model for Breast Cancer Dataset.","f514f7b7":"### Standardize the data","3bf141f8":"### Creating a test set and a training set\n\nSince this data set is not ordered, we will to do a simple 70:30 split to create a training data set and a test data set.","1ef15fb3":"From above, we can see that after standardizing the data all the columns have standard deviation of 1.","8ad2c54a":"### Models prediction without any normalization or standardization","1abc2774":"From above, we can see that after normalizing the data all the columns have min and max values between 0 and 1 respectively.","7c04ed1c":"### Models prediction with Normalized data","e734ab77":"# Model Selection"}}