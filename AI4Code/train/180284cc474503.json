{"cell_type":{"fc6ff6a9":"code","25c07983":"code","445b193c":"code","28c0ed29":"code","cb6bbe9a":"code","37014977":"code","a29a7e23":"code","bc3e884c":"code","31260152":"code","260498f8":"code","cdf79a1a":"code","2605af17":"code","a066a9c8":"code","172ef22a":"code","f9e117c7":"code","b084d5bf":"code","78c47170":"code","bacd62a2":"code","80581bca":"code","95f204b8":"code","15d147fc":"code","5df9e6f2":"code","98a211a1":"code","32cd8986":"code","3ae85df0":"code","351103c8":"code","e17da27b":"code","180ab6c7":"code","2481b5d2":"code","f90987c7":"code","6c2018ea":"code","e456773a":"code","23aa2e5e":"code","4f8da1ea":"code","555be57a":"code","3aea1102":"code","4b9c7a19":"code","74c6d716":"code","fec16c50":"code","1845ab2e":"code","8b608a2f":"markdown","0c069d2e":"markdown","b22a0700":"markdown","4ab5ab12":"markdown","b09d9ac8":"markdown","aa6a11ff":"markdown"},"source":{"fc6ff6a9":"### This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","25c07983":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split","445b193c":"tweet= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest= pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","28c0ed29":"tweet[tweet['target']==1]['text'].values[0]","cb6bbe9a":"tweet.head(10)","37014977":"test.head(10)","a29a7e23":"tweet.shape","bc3e884c":"x= tweet.target.value_counts()","31260152":"print(x)","260498f8":"sns.barplot(x.index,x)","cdf79a1a":"y= tweet[tweet['target']==1]['text'].str.split()\nprint(y)","2605af17":"word_len=tweet[tweet['target']==1]['text'].str.split().str.len()\nprint(word_len)","a066a9c8":"plt.hist(word_len)","172ef22a":"def create_corpus(target):\n    corpus= []\n    \n    for l in y:\n        for q in l:\n            corpus.append(q)\n    return(corpus)","f9e117c7":"corpus= create_corpus(1)\n\ndic= defaultdict(int)\n\nfor i in corpus:\n    if i in stop:\n        dic[i]= dic[i]+1\ntop = sorted(dic.items(), key= lambda x: x[1], reverse= True)[:10]\nm,n =zip(*top)\nplt.bar(m,n)","b084d5bf":"df= pd.concat([tweet, test])\ndf.shape","78c47170":"def sterilization(data):\n    \n    data = re.sub('https?:\/\/\\S+|www\\.\\S+', '', data)\n    data = re.sub('<.*?>', '', data)\n    emoj = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    emoj.sub(r'', data)\n    data = data.lower()\n    data = data.translate(str.maketrans('','', string.punctuation))\n    data = re.sub(r'\\[.*?\\]', '', data)\n    data = re.sub(r'\\w*\\d\\w*','', data)\n    \n\n    return data\n    \n    ","bacd62a2":"df['text']=df['text'].apply(lambda x : sterilization(x))\ndf.head(10)","80581bca":"from nltk.tokenize import RegexpTokenizer\n\ntokenizer = RegexpTokenizer(r'\\w+')\n\ndf[\"tokens\"] = df[\"text\"].apply(tokenizer.tokenize)\ndf.head()","95f204b8":"word2vec_path = \"..\/input\/googles-trained-word2vec-model-in-python\/GoogleNews-vectors-negative300.bin\"\nword2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)","15d147fc":"def filtered(text):\n    words= [w for w in text if w not in stop]\n    \n    return words","5df9e6f2":"df['tokens']= df['tokens'].apply(lambda x: filtered(x))\ndf.head()","98a211a1":"def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n    if len(tokens_list)<1:\n        return np.zeros(k)\n    if generate_missing:\n        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n    else:\n        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n   \n    length = len(vectorized)\n    summed = np.sum(vectorized, axis=0)\n    averaged = np.divide(summed, length)\n    return averaged\n\ndef get_word2vec_embeddings(vectors, df, generate_missing=False):\n    embeddings = df['tokens'].apply(lambda x: get_average_word2vec(x, vectors, generate_missing=generate_missing))\n    return list(embeddings)","32cd8986":"tweet.shape","3ae85df0":"X_train= df[:7613]\ny_train= X_train['target']","351103c8":"X_test= df[7613:]","e17da27b":"embeddings = get_word2vec_embeddings(word2vec, X_train)","180ab6c7":"X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(embeddings, y_train, test_size= 0.2, random_state=40)","2481b5d2":"from sklearn.linear_model import LogisticRegression\nclf_w2v = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n                         multi_class='multinomial', random_state=40)\nclf_w2v.fit(X_train_word2vec, y_train_word2vec)\ny_predicted_word2vec = clf_w2v.predict(X_test_word2vec)","f90987c7":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n\ndef get_metrics(y_test, y_predicted):  \n    # true positives \/ (true positives+false positives)\n    precision = precision_score(y_test, y_predicted, pos_label=None,\n                                    average='weighted')             \n    # true positives \/ (true positives + false negatives)\n    recall = recall_score(y_test, y_predicted, pos_label=None,\n                              average='weighted')\n    \n    # harmonic mean of precision and recall\n    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n    \n    # true positives + true negatives\/ total\n    accuracy = accuracy_score(y_test, y_predicted)\n    return accuracy, precision, recall, f1\n\naccuracy_word2vec, precision_word2vec, recall_word2vec, f1_word2vec = get_metrics(y_test_word2vec, y_predicted_word2vec)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_word2vec, precision_word2vec, \n                                                                       recall_word2vec, f1_word2vec))","6c2018ea":"test_embeds= get_word2vec_embeddings(word2vec, X_test)","e456773a":"from xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline","23aa2e5e":"clf = XGBClassifier(colsample_bytree=0.7, learning_rate= 0.03, max_depth= 10,\n                    min_child_weight=11, missing= -999, n_estimators= 1200,\n                    nthread= 4, objective='binary:logistic', seed=1337, silent=1, subsample=0.8)","4f8da1ea":"XTa= np.array(X_train_word2vec)\nyTa= np.array(y_train_word2vec)","555be57a":"clf.fit(XTa,yTa)","3aea1102":"y_predicted_word2vec = clf.predict(X_test_word2vec)","4b9c7a19":"y_predicted_word2vec","74c6d716":"accuracy_word2vec, precision_word2vec, recall_word2vec, f1_word2vec = get_metrics(y_test_word2vec, y_predicted_word2vec)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_word2vec, precision_word2vec, \n                                                                       recall_word2vec, f1_word2vec))","fec16c50":"submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\n\nsubmission[\"target\"] = clf.predict(test_embeds)\n\nsubmission.to_csv(\"submission.csv\", index=False)","1845ab2e":"from IPython.display import FileLink\nFileLink(r'submission.csv')","8b608a2f":"## Logistic Regression","0c069d2e":"**IMPORTING WORD2VEC pretrained model**","b22a0700":"## Download the generated CSV","4ab5ab12":"## Taking average of all the vector weights of words present in a single tweet","b09d9ac8":"## XGBoost","aa6a11ff":"**'filtered' function is for removing stop words**"}}