{"cell_type":{"286976b9":"code","7bbbbaa3":"code","79bf8f2c":"code","32df648e":"code","85f0eb31":"code","0a40f63d":"code","d6d14829":"code","03c240eb":"code","650c0d81":"code","2ff42e20":"code","5ed5e935":"code","8bdebef6":"code","bdc02fb4":"code","098f9ed9":"code","863dc2e6":"code","fae8a294":"code","5f6b172b":"code","28c0c609":"code","0ebc4611":"code","f4851610":"code","6cd5aa4a":"code","f070e680":"code","336d0d3d":"code","aea035e5":"code","25691497":"code","61f86e82":"code","15f3407e":"code","fa2661f5":"code","ba399222":"code","605a29f0":"code","6dfe67de":"code","85254c04":"code","bfe1f7e2":"code","7305f6b1":"code","c116123b":"code","96c05479":"markdown","f321e21c":"markdown","cba81d0d":"markdown","4e179037":"markdown","4e81b892":"markdown","ccdd65cb":"markdown","ddaa53fc":"markdown","63050c4b":"markdown","6e541082":"markdown","10879668":"markdown","5aabb67c":"markdown","580db5bf":"markdown","cf930643":"markdown","e4d476ff":"markdown","008810b3":"markdown","0ff18c8f":"markdown","75f9f249":"markdown","5b0f086a":"markdown"},"source":{"286976b9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7bbbbaa3":"global obs_cnt\nobs_cnt = 0\nact_cnt = 0 \ndef observation(comment,_type=0):\n    global obs_cnt\n    global act_cnt\n    \n    if _type == 0:\n        obs_cnt= obs_cnt+1\n        print(\"\\nObservation-\",obs_cnt,\"->\",comment,\"\\n\")\n    elif _type == 1:\n        act_cnt= act_cnt+1\n        print(\"\\nActioned-\",act_cnt,\"->\",comment,\"\\n\")","79bf8f2c":"bp=\"\/kaggle\/input\/goodbooks-10k\/\" # base path","32df648e":"booksdf=pd.read_csv(bp+\"books.csv\")\nbooksdf.head(5)","85f0eb31":"observation(\"2278 records have duplicate rating\")\nratingdf=pd.read_csv(bp+\"ratings.csv\")\nprint(ratingdf.shape)\nratingdf[['book_id','user_id']][ratingdf[['book_id','user_id']].duplicated()].shape # recodes with duplicate ratings\n","0a40f63d":"observation(\"\"\"Before removing duplicate - rating - shape is (981756, 3) \n              After removing duplicate  - rating shape is (979478, 3)\n            \"\"\",1\n          )\nx=ratingdf[['book_id','user_id']].drop_duplicates().index\nratingdf= ratingdf.loc[x,:]\n#ratingdf.shape","d6d14829":"## unisue users, books\nn_users, n_books = len(ratingdf.user_id.unique()), len(ratingdf.book_id.unique())\n\n\nf'The dataset includes {len(ratingdf)} ratings by {n_users} unique users on {n_books} unique books.'","03c240eb":"from sklearn.model_selection import train_test_split\n\n## split the data to train and test dataframes\ntrain, test = train_test_split(ratingdf, test_size=0.1)\n\nf\"The training and testing data include {len(train), len(test)} records.\"","650c0d81":"## import keras models, layers and optimizers\nfrom keras.models import Sequential, Model\nfrom keras.layers import Embedding, Flatten, Dense, Dropout, concatenate, multiply, Input\nfrom keras.layers.merge import Dot, multiply, concatenate\nfrom keras.optimizers import Adam","2ff42e20":"import tensorflow as tf","5ed5e935":"## specify learning rate (or use the default)\n#opt_adam = Adam(lr = 0.002)\n\n## compile model\n#model_mf.compile(optimizer = opt_adam, loss = ['mse'], metrics = ['mean_absolute_error'])\n#model_mf.compile(optimizer='adam',loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True)) \n","8bdebef6":"# History = model_mf.fit([train.user_id, train.book_id],\n#                           train.rating,\n#                           batch_size = 100,\n#                           validation_split = 0.005,\n#                           epochs = 4,\n#                           verbose = 0)","bdc02fb4":"# creating book embedding path\nbook_input = Input(shape=[1], name=\"Book-Input\")\nbook_embedding = Embedding(n_books+1, 5, name=\"Book-Embedding\")(book_input)\nbook_vec = Flatten(name=\"Flatten-Books\")(book_embedding)\n\n# creating user embedding path\nuser_input = Input(shape=[1], name=\"User-Input\")\nuser_embedding = Embedding(n_users+1, 5, name=\"User-Embedding\")(user_input)\nuser_vec = Flatten(name=\"Flatten-Users\")(user_embedding)\n\n# performing dot product and creating model\nprod = Dot(name=\"Dot-Product\", axes=1)([book_vec, user_vec])\nmodel = Model([user_input, book_input], prod)\nmodel.compile('adam', 'mean_squared_error')","098f9ed9":"## fit model\n# from keras import backend as K\nHistory = model.fit([train.user_id, train.book_id],\n                          train.rating,\n                          batch_size = 100,\n                          validation_split = 0.5,\n                          epochs = 4,\n                          verbose = 1)\n#History = model.fit([train.user_id, train.book_id], train.rating, epochs=5, verbose=1)","863dc2e6":"\n# creating user embedding path\nuser_input = Input(shape=[1], name=\"User-Input\")\nuser_embedding = Embedding(n_users+1, 25, name=\"User-Embedding\")(user_input)\nuser_vec = Flatten(name=\"Flatten-Users\")(user_embedding)\nuser_vec=Dropout(0.40)(user_vec)\n\n# creating book embedding path\nbook_input = Input(shape=[1], name=\"Book-Input\")\nbook_embedding = Embedding(n_books+1, 25, name=\"Book-Embedding\")(book_input)\nbook_vec = Flatten(name=\"Flatten-Books\")(book_embedding)\nbook_vec=Dropout(0.40)(book_vec)\n\n\n# model configuration part 3\nsim=Dot(name=\"Dot-Product\", axes=1)([user_vec,book_vec])\nnn_inp=Dense(96,activation='relu')(sim)\nnn_inp=Dropout(0.4)(nn_inp)\n# nn_inp=BatchNormalization()(nn_inp)\nnn_inp=Dense(1,activation='relu')(nn_inp)\n\n# Ensemle Part 1, Part 2, Part 3\nnn_model =Model([user_input, book_input],nn_inp)\nnn_model.summary()\n","fae8a294":"## fit model\n# from keras import backend as K\nnn_model.compile(optimizer=Adam(lr=1e-3),loss='mse')\nHistory = nn_model.fit([train.user_id, train.book_id],\n                          train.rating,\n                          batch_size = 100,\n                          validation_split = 0.5,\n                          epochs = 4,\n                          verbose = 1)\n#History = model.fit([train.user_id, train.book_id], train.rating, epochs=5, verbose=1)","5f6b172b":"def get_model_3(max_work, max_user):\n    dim_embedddings = 30\n    bias = 1\n    # inputs - part 1\n    w_inputs = Input(shape=(1,), dtype='int32')\n    w = Embedding(max_work+1, dim_embedddings, name=\"work\")(w_inputs)\n    w_bis = Embedding(max_work + 1, bias, name=\"workbias\")(w_inputs)\n\n    # context - part 2\n    u_inputs = Input(shape=(1,), dtype='int32')\n    u = Embedding(max_user+1, dim_embedddings, name=\"user\")(u_inputs)\n    u_bis = Embedding(max_user + 1, bias, name=\"userbias\")(u_inputs)\n    \n    # dot product to find similarity - part 3\n    o = multiply([w, u])\n    #o = dot([w,u],name='Simalarity-Dot-Product',axes=1)\n    o = Dropout(0.5)(o)\n    o = concatenate([o, u_bis, w_bis])\n    o = Flatten()(o)\n    o = Dense(10, activation=\"relu\")(o)\n    o = Dense(1)(o)\n\n    # Ensembling part 1 , part 2, part 3 to make final Model\n    rec_model = Model(inputs=[w_inputs, u_inputs], outputs=o)\n    #rec_model.summary()\n    \n    # compile Model\n    rec_model.compile(loss='mae', optimizer='adam', metrics=[\"mae\"])\n\n    return rec_model\nmodel=get_model_3(n_users,n_books)","28c0c609":"#nn_model.compile(optimizer=Adam(lr=1e-3),loss='mse')\nHistory = model.fit([train.user_id, train.book_id],\n                          train.rating,\n                          batch_size = 100,\n                          validation_split = 0.5,\n                          epochs = 4,\n                          verbose = 1)","0ebc4611":"## show loss at each epoch\npd.DataFrame(History.history)","f4851610":"from pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\nimport matplotlib.pyplot as plt\nplt.plot(History.history['loss'] , 'g')\nplt.plot(History.history['val_loss'] , 'b')\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.grid(True)\nplt.show()","6cd5aa4a":"## define the number of latent factors (can be different for the users and books)\ndim_embedding_user = 50\ndim_embedding_book = 30\n\n## book embedding\nbook_input= Input(shape=[1], name='Book')\nbook_embedding = Embedding(n_books + 1, dim_embedding_book, name='Book-Embedding')(book_input)\nbook_vec = Flatten(name='Book-Flatten')(book_embedding)\nbook_vec = Dropout(0.2)(book_vec)\n\n## user embedding\nuser_input = Input(shape=[1], name='User')\nuser_embedding = Embedding(n_users + 1, dim_embedding_user, name ='User-Embedding')(user_input)\nuser_vec = Flatten(name ='User-Flatten')(user_embedding)\nuser_vec = Dropout(0.2)(user_vec)\n\n## concatenate flattened values \nconcat = concatenate([book_vec, user_vec])\nconcat_dropout = Dropout(0.2)(concat)\n\n## add dense layer (can try more)\ndense_1 = Dense(20, name ='Fully-Connected1', activation='relu')(concat)\n\n## define output (can try sigmoid instead of relu)\nresult = Dense(1, activation ='relu',name ='Activation')(dense_1)\n\n## define model with 2 inputs and 1 output\nmodel_tabular = Model([user_input, book_input], result)\n\n## show model summary\nmodel_tabular.summary()\n\n## specify learning rate (or use the default by specifying optimizer = 'adam')\nopt_adam = Adam(lr = 0.002)\n\n## compile model\nmodel_tabular.compile(optimizer= opt_adam, loss= ['mse'], metrics=['mean_absolute_error'])\n\n## fit model\nhistory_tabular = model_tabular.fit([train['user_id'], train['book_id']],\n                                    train['rating'],\n                                    batch_size = 256,\n                                    validation_split = 0.20,\n                                    epochs = 10,\n                                    verbose = 1)\n\n# This model performs better that the first one after 4 epochs. Part of the experimentation is to train both \n# for more epochs, \n# tune the hyper parameters or modify the architecture.\n\n","f070e680":"from pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\nimport matplotlib.pyplot as plt\nplt.plot(history_tabular.history['loss'] , 'g')\nplt.plot(history_tabular.history['val_loss'] , 'b')\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.grid(True)\nplt.show()","336d0d3d":"## show loss at each epoch\npd.DataFrame(history_tabular.history)\n\n","aea035e5":"## import libraries\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n## define a function to return arrays in the required form \ndef get_array(series):\n    return np.array([[element] for element in series])\n\n## predict on test data  \npredictions = model_tabular.predict([get_array(test['user_id']), get_array(test['book_id'])])\n\nf'mean squared error on test data is {mean_squared_error(test[\"rating\"], predictions)}'","25691497":"## get weights of the books embedding matrix\nbook_embedding_weights = model_tabular.get_layer('Book-Embedding').get_weights()[0]\nbook_embedding_weights.shape","61f86e82":"## import PCA\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\n\npca = PCA(n_components = 2) ## use 3 components\npca_result1 = pca.fit_transform(book_embedding_weights)\nsns.scatterplot(x=pca_result1[:,0], y=pca_result1[:,1])","15f3407e":"from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\ntnse_results = tsne.fit_transform(book_embedding_weights)\nsns.scatterplot(x=tnse_results[:,0], y=tnse_results[:,1])","fa2661f5":"## import PCA\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\n\npca = PCA(n_components = 3) ## use 3 components\nbook_embedding_weights_t = np.transpose(book_embedding_weights) ## pass the transpose of the embedding matrix\nbook_pca = pca.fit(book_embedding_weights_t) ## fit\n\n## display the resulting matrix dimensions\nprint(book_pca.components_.shape)\n\n# We can look at the percentage of variance explained by each of the selected components. \n## display the variance explained by the 3 components\nprint(book_pca.explained_variance_ratio_)\n## If the variance explained is very low, we might not be able to see a good interpretation. \n##However, for demo purposes, we will just extract the first component\/factor that explains the highest \n## percentage of the variance. The array we get can be mapped to the books names as follows.\n","ba399222":"## create a dictionary out of bookid, book original title\nbooks_dict = booksdf.set_index('book_id')['original_title'].to_dict()\nbooks_dict","605a29f0":"from operator import itemgetter\n\n## extract first PCA\npca0 = book_pca.components_[0]\n\n## get the value (pca0, book title)\nbook_comp0 = [(f, books_dict[i]) for f,i in zip(pca0, list(books_dict.keys()))]\nbook_comp0","6dfe67de":"## books corresponding to the highest values of pca0\nsorted(book_comp0, key = itemgetter(0), reverse = True)[:10]","85254c04":"## books corresponding to the lowest values of pca0\nsorted(book_comp0, key = itemgetter(0))[:10]","bfe1f7e2":"# Creating dataset for making recommendations for the first user\nbook_data = np.array(list(set(ratingdf.book_id)))\nuser = np.array([1 for i in range(len(book_data))])\npredictions = model_tabular.predict([user, book_data])\npredictions = np.array([a[0] for a in predictions])\nrecommended_book_ids = (-predictions).argsort()[:5]\nprint(recommended_book_ids)\nprint(predictions[recommended_book_ids])","7305f6b1":"ratingdf[ratingdf.user_id == 1]","c116123b":"booksdf[booksdf['id'].isin(recommended_book_ids)]","96c05479":"Then we will extract the book embedding weights, and we can see it has the expected dimensions.","f321e21c":"### 3- Other methods\nAlthough we saw preliminary models we could get the idea behind each of the methods and we have a starting point to experiment more. There\u2019s also another approach which almost combines both methods. It was published in a paper entitled Neural Collaborative Filtering. I haven\u2019t experimented with it but I thought it was worth mentioning as we discuss similar methods.","cba81d0d":"### 3. What is a Matrix Decomposition or Factorization?\nA matrix decomposition is a way of reducing a matrix into its constituent parts.\n\nIt is an approach that can simplify more complex matrix operations that can be performed on the decomposed matrix rather than on the original matrix itself.\n\nIt is called matrix factorization because of a common analogy of factoring a number, such as the factoring number 10 into 2 x 5. For this reason, matrix decomposition is also called matrix factorization. \n\nOriginal matrix is decomposed into low rank matrix, so we can also call it low rank factorization or decomposition.\n\nLU, PLU, QR are different methods of matrix Factorization\n\n\nhttps:\/\/machinelearningmastery.com\/introduction-to-matrix-decompositions-for-machine-learning\/","4e179037":"### 1. Recommendation system \nFundamentally a recommendation system seeks to predict the rating or preference a user would give to a new item when  his old item ratings or preferences data is givien for other items. Recommendation systems are used by pretty much every major company in order to enhance the quality of their services.\n\n### About the dataset\nThis data contains the rating given different users to different book and ask to create a \"book recommendation system\" where it can recomments user would like to buy or like or provide rating based on the preference.","4e81b892":"## Low rank matrix- and tensor-based models are especially\nsuitable for this task and are widely used in industry.","ccdd65cb":"### Coding Approach for building RD -> Deep Learning-KERAS framework \n\nkeras offer Embedding layer for Matrix factorisation and is very easy to build the recommendation engine.\nIt easy calculate the dot product of user and books matrics and use it as basic building block of the Keras deep NN model and then perform further tuning.\n\n### Content vs Context\n\n    Content is a permanent characterization of users or items; \n        - imposes structure on the lower dimensional latent space.\n    Context characterizes transient relations between users and items;\n        - expands interaction space.\n\n\n\n### 2 Theortical appoach on building recommendation engine\n\n\n\n\n    \u2013 Content Filtering \n    \u2013 Collaborative Filtering (CF) \n        \u2013 CF: Neighborhood Methods \n        \u2013 CF: Latent Factor Methods\n        - CF: Dimensionality reduction (matrix Factorization)\n\n    \n### 2.1 Content based Filtering\n\nThis algorithm recommends products which are similar to the ones that a user has liked in the past.\n\nconsider user A , has watched and liked movies M1, M2, M3 all of these have same genre G1, Now lets say 5 moviews launched today and out of these five MT1,MT2 have genre G1 and you intuitively suggest MT1 and MT2 to user A and you should not recomment MT3,MT4, MT5 because the genre of these moviews are not matching and user A is will less likely to watch these movies.\n\nThe point of content-based is that we have to know the content of both user and item. Usually we need to construct user-profile and item-profile using the content of shared attribute space.\n\nIts different from Item Based collaborative filtering that you will study in the next section.\n\nExample: pandora.com\nCons: Assumes access to side information about items (e.g. properties of asong)\nPros: Got a new item to add? No problem, just be sure to include the side information\n\n\n### 2.2 Collobarative Filtering\n\nPros: Does not assume access to side information about items (e.g. does not need to know about movie genres)\nCons: Does not work on new items that have no ratings.\nExample: netflix\n\n   This is advance method and there are two different cases or thoughts by which you can make recommendation such as\n   \n    \u2013 Bestseller lists\n    \u2013 Top 40 music lists\n    \u2013 The \u201crecent returns\u201d shelf at the library\n    \u2013 Unmarked but well-used paths thru the woods\n    \u2013 The printer room at work\n    \u2013 \u201cRead any good books lately?\u201d\n\n\n    2.2.1: - User-based collaborative filtering: \n   This algorithm first finds the similarity score between users. Based on this similarity score, it then picks out the most similar users and recommends products which these similar users have liked or bought previously.\n   \n \n   Let us understand this with an example. If person A likes 3 movies, say Interstellar, Inception and Predestination, and person B likes Inception, Predestination and The Prestige, then they have almost similar interests. We can say with some certainty that A should like The Prestige and B should like Interstellar. The collaborative filtering algorithm uses \u201cUser Behavior\u201d for recommending items. This is one of the most commonly used algorithms in the industry as it is not dependent on any additional informat\n   \n   Let say, you are user u, and you want to watch some moview\n   Now on other hand, system has understood by your behaviour of liking and found that your behavior similary to user v\n   So system needs to calculate and have already calculated similarity(u,v)\n   \n   Now system can easily find recommendation for you with following formulae\n   \n   \n   ![image.png](attachment:image.png)\n   ","ddaa53fc":"## STEP 2 (MATRIX FACTORIZATION METHOD)\n![image.png](attachment:image.png)","63050c4b":"To compress these these vectors into fewer dimensions, we can use Principal Component Analysis(PCA) to reduce the dim_embedddings to three components.","6e541082":"## Step 1 (MATRIX FACTORIZATION)\n![image.png](attachment:image.png)","10879668":"# using TSNE","5aabb67c":"# Making Recommendations","580db5bf":"### Matrix factorization method\nThe core of this methods is having:\n\nusers and books embedding layers with the same number of factors.\nuser and books bias layers which can be considered as representation of the unique inherent characteristic of each user\/book.","cf930643":"## Item-Based Filtering technique \n### Matrix Factorization, Latent Factors and Embeddings-> implementation using KERAS\nUsing this approach, we learn about the similarities between the books from the ratings we already have. Starting with tabular data, the first step is to convert our table to a matrix with one row per user as shown in the following figure.\n","e4d476ff":"## Visualizing embeddings\nEmbeddings can be used to visualize concepts such as the relation of different books in our case. To visualize these concepts, we need to further reduce dimensionality using dimensionality reduction techniques like principal component analysis (PSA) or t-distributed stochastic neighbor embedding (TSNE).\nStarting with 10000 dimensions (one for each book), we map them to 5 dimensions using embedding and then further map them to 2 dimensions using PCA or TSNE.\nFirst of we need to extract the embeddings using the get_layer function:","008810b3":"    2.2.2: - Item-based collaborative filtering: \n    \n    \n   It also user interaction based approach but in little different way.\n   However, you might have studied similar approach called content based which explored the contents of items, here it explores the interaction of items.\n   In this algorithm, we compute the similarity between each pair of items.\n   \n   So in our case we will find the similarity between each movie pair and based on that, we will recommend similar movies which are liked by the users in the past. This algorithm works similar to user-user collaborative filtering with just a little change \u2013 instead of taking the weighted sum of ratings of \u201cuser-neighbors\u201d, we take the weighted sum of ratings of \u201citem-neighbors\u201d. \n   \n   These systems identify similar items based on users\u2019 previous ratings. \n   For example if users A, B and C gave a 5 star rating to books X and Y then if another user D buys book Y, Now system can also give user D the recommendation to purchase book X because the system identifies book X and Y as similar based on the ratings of users A, B and C.\n\n\n### Dimensionality Reduction\nIn the user-item matrix, there are two dimensions:\n\n - The number of users\n - The number of items\n\n\none of the algo is UMF-UNCONSTRAINED MATRIX FACTORIZATION that uses \n- SGD\n\u2013 SGD with Regularization\n\u2013 Alternating Least Squares\n\u2013 User\/item bias terms (matrix trick)\n\n\nAnother popular algorithms to factorize a matrix is the singular value decomposition (SVD) algorithm, it has no regularization. SVD came into the limelight when matrix factorization was seen performing well in the Netflix prize competition. \n\n\nOther algorithms include PCA and its variations, NMF(Non-negative Matrix Factorization), and so on. \n\nAutoencoders can also be used for dimensionality reduction in case you want to use Neural Networks.\n\nCollaborative filtering by Matrix Factorization (MF) is an efficient and effective approach in building recommendation engine","0ff18c8f":"# Tabular data method\n\nIn this method, we will just have users and books embedding layers that can have different dimensions. We will concatenate them together as if we have a table with dim_embedding_user+dim_embedding_book features. Then we can add dense layers, or even use these weights as features with other algorithms","75f9f249":"## Interpretation of embedding\/bias values\nIdeally, if we have trained the model enough to learn about the latent factors of the users\/books, the values of the bias or embedding matrices should have a meaning about the underlying characteristics of what they represent. Although this model is preliminary, we will see how we could interpret these values with a good model.\n\nFirst, we will create a dictionary with the book id and title.","5b0f086a":"# Using PCA"}}