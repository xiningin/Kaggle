{"cell_type":{"115eab26":"code","65d87208":"code","1a4dfd0b":"code","5b27a6b9":"code","361e8576":"code","ea900a73":"code","19070b33":"code","bd4f615f":"code","72c602a3":"code","16ff684d":"code","da709038":"code","0f310c72":"code","630c6d57":"code","b106d6a4":"code","4683c7fd":"code","370b0e59":"code","e621a88f":"code","98f3283c":"code","4c92228c":"code","def6aec1":"code","252dd025":"code","32f12c4c":"code","0a489daf":"code","f1f00107":"code","f4529b47":"code","61538305":"code","e4872502":"code","5efeb902":"markdown","747bfd6e":"markdown","0e561668":"markdown","578859e6":"markdown","393ed2e4":"markdown","fb60ff1a":"markdown","103a769d":"markdown","979c7922":"markdown","9352a871":"markdown","44541f7f":"markdown"},"source":{"115eab26":"# Familiar imports\nimport numpy as np\nimport pandas as pd\n\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\n# For training random forest model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error","65d87208":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()","1a4dfd0b":"train.describe()","5b27a6b9":"train.info()","361e8576":"train.isnull().sum()","ea900a73":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target'], axis=1)\n\n# Preview features\nfeatures.head()","19070b33":"y.value_counts().unique()","bd4f615f":"for i in features.columns:\n    print(\" col {} has unique value : {}\".format(i,features[i].value_counts()))","72c602a3":"# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\n\n\n# ordinal-encode categorical columns\nX = features.copy()\nX_test = test.copy()\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n\n# Preview the ordinal-encoded features\nX.head()","16ff684d":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)","da709038":"# Columns that can be safely ordinal encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_valid[col]).issubset(set(X_train[col]))]\n\n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n\nprint('Categorical columns that will be ordinal encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)","0f310c72":"X.describe()","630c6d57":"X.info()","b106d6a4":"X.isnull().sum()","4683c7fd":"def get_mse(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = RandomForestRegressor(max_leaf_nodes, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mse=mean_squared_error(val_y, preds_val, squared=False)\n    return(mse)","370b0e59":"#test.info()","e621a88f":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train.columns if\n                    X_train[cname].nunique() < 10 and \n                    X_train[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X.columns if \n                X_train[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train_p = X_train[my_cols].copy()\nX_valid_p = X_valid[my_cols].copy()\n#X_test_p = test[my_cols].copy()","98f3283c":"from sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor\n\ndef get_score(n_estimators):\n    \"\"\"Return the average MAE over 5 CV folds of random forest model.\n    \n    Keyword argument:\n    n_estimators -- the number of trees in the forest\n    \"\"\"\n    # Replace this body with your own code\n    my_pipeline_1 = Pipeline(steps=[\n    ('preprocessor', SimpleImputer()),\n    ('model', XGBRegressor(n_estimators=n_estimators, random_state=0))\n])\n    scores = -1 * cross_val_score(my_pipeline_1, X_train_p, y_train,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n    return scores.mean()","4c92228c":"results = {} # Your code here\nfor r in range(50,450,50):\n    results[r]=get_score(r)\n# Check your answer","def6aec1":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(list(results.keys()), list(results.values()))\nplt.show()","252dd025":"n_estimators_best = min(results, key=results.get)\nn_estimators_best","32f12c4c":"my_pipeline_2 = Pipeline(steps=[\n    ('preprocessor', SimpleImputer()),\n    ('model', XGBRegressor(n_estimators=n_estimators_best, random_state=0))\n])\nscores = -1 * cross_val_score(my_pipeline_2,  X_train_p, y_train,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\nsc=scores.mean()\n","0a489daf":"sc","f1f00107":"my_pipeline_2.fit(X_train_p, y_train)","f4529b47":"predictions = my_pipeline_2.predict(X_valid_p)","61538305":"from sklearn.metrics import mean_absolute_error\n\n# Calculate MAE\nmae_1 = mean_absolute_error(predictions, y_valid) \n\n# print MAE\nprint(\"Mean Absolute Error:\" , mae_1)","e4872502":"# Use the model to generate predictions\npredictions = my_pipeline_2.predict(X_test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","5efeb902":"Welcome to the **[30 Days of ML competition](https:\/\/www.kaggle.com\/c\/30-days-of-ml\/overview)**!  In this notebook, you'll learn how to make your first submission.\n\nBefore getting started, make your own editable copy of this notebook by clicking on the **Copy and Edit** button.\n\n# Step 1: Import helpful libraries\n\nWe begin by importing the libraries we'll need.  Some of them will be familiar from the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** course and the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course.","747bfd6e":"Once you have run the code cell above, follow the instructions below to submit to the competition:\n1. Begin by clicking on the **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n2. Ensure that the **Save and Run All** option is selected, and then click on the **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\nIf you want to keep working to improve your performance, select the **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.","0e561668":"# Step 3: Prepare the data\n\nNext, we'll need to handle the categorical columns (`cat0`, `cat1`, ... `cat9`).  \n\nIn the **[Categorical Variables lesson](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables)** in the Intermediate Machine Learning course, you learned several different ways to encode categorical variables in a dataset.  In this notebook, we'll use ordinal encoding and save our encoded features as new variables `X` and `X_test`.","578859e6":"# **By Using Pipeline:**","393ed2e4":"The next code cell separates the target (which we assign to `y`) from the training features (which we assign to `features`).","fb60ff1a":"# Step 6: Keep Learning!\n\nIf you're not sure what to do next, you can begin by trying out more model types!\n1. If you took the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course, then you learned about **[XGBoost](https:\/\/www.kaggle.com\/alexisbcook\/xgboost)**.  Try training a model with XGBoost, to improve over the performance you got here.\n\n2. Take the time to learn about **Light GBM (LGBM)**, which is similar to XGBoost, since they both use gradient boosting to iteratively add decision trees to an ensemble.  In case you're not sure how to get started, **[here's a notebook](https:\/\/www.kaggle.com\/svyatoslavsokolov\/tps-feb-2021-lgbm-simple-version)** that trains a model on a similar dataset.","103a769d":"# Step 4: Train a model\n\nNow that the data is prepared, the next step is to train a model.  \n\nIf you took the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** courses, then you learned about **[Random Forests](https:\/\/www.kaggle.com\/dansbecker\/random-forests)**.  In the code cell below, we fit a random forest model to the data.","979c7922":"In the code cell above, we set `squared=False` to get the root mean squared error (RMSE) on the validation data.\n\n# Step 5: Submit to the competition\n\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.","9352a871":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  \n\nWe set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.  (*If you're not sure how this works, try temporarily removing `index_col=0` and see how it changes the result.*)","44541f7f":"Next, we break off a validation set from the training data."}}