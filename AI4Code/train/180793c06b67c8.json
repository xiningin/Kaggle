{"cell_type":{"54f67c75":"code","9b409539":"code","d58c7589":"code","99c87418":"code","e3cc618b":"code","63b75f56":"code","dc9e8d51":"code","51c4abfa":"markdown","eca74e4b":"markdown","79e7201f":"markdown","51fee334":"markdown","30029b23":"markdown","72ba6066":"markdown","c55ca6a4":"markdown"},"source":{"54f67c75":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom numpy.random import default_rng\nrng = default_rng()\n# set the RNG seed in case we wish to test the stability of our predictions\nnp.random.seed(8)","9b409539":"df               = pd.DataFrame(np.arange(0,100,0.5), columns={\"mph\"})\ndf['kmph']       = df['mph'] * 1.609344\ndf['n_unicorns'] = np.random.randint(low= 1, high=  4, size=df.shape[0])\ndf['RNG']        = np.random.uniform(low= 0, high= 50, size=df.shape[0])\n# we shall also add a little Gaussian noise to the target\ndf['fuel']       = df['mph'] * (1 + np.random.normal(0,0.02, size=df.shape[0]))\nX = df.iloc[:,[0,1,2,3]]\ny = df.iloc[:,[4]]\n# take a look\ndf","d58c7589":"sns.pairplot(X);","99c87418":"# using scikit-learn\nfrom sklearn import linear_model\nregression = linear_model.LinearRegression()\n# ground truth: X=mph, y=fuel\nregression.fit(df.iloc[:,[0]] , df.iloc[:,[4]])\nprint(\"Ground truth regression coefficient:\",regression.coef_)\nprint()\n\n# full regression, this time using statsmodels\nimport statsmodels.api as sm\nmodel   = sm.OLS(y,X)\nresults = model.fit()\nprint(results.summary())","e3cc618b":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nVIF             = pd.DataFrame()\nVIF['feature']  = X.columns\nVIF['VIF']      = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n# take a look\nVIF","63b75f56":"X = X.drop(['kmph'], axis=1)\n\nVIF            = pd.DataFrame()\nVIF['feature'] = X.columns\nVIF['VIF']     = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nVIF","dc9e8d51":"results = sm.OLS(y,X).fit()\nprint(results.summary())","51c4abfa":"When we perform the linear regression for fuel with respect to **mph** we obtain our ground truth coefficient of **1**. However, the full regression suggests that \n\n$$ \\mathit{fuel} = 0.2786 ~ \\mathit{mph} + 0.4483 ~ \\mathit{kmph}$$\n\nwhich is indeed correct, but is far less interpretable. We can also see that neither the random numbers, nor the unicorns, seem to have any noticeable effect on the fuel consumption at all(!)\n\n### The variance inflation factor (VIF)\nSo how do we detect and deal with collinearity? One way is to calculate the [variance inflation factor](https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.stats.outliers_influence.variance_inflation_factor.html) for each feature. The VIF is given by\n$$ \\mathrm{VIF}_i = \\frac{1}{1-R_i^2} $$\n\nwhere $R^2$ is the [coefficient of determination](https:\/\/en.wikipedia.org\/wiki\/Coefficient_of_determination). It is generally deemed that a feature that has a VIF value greater than 5 is highly collinear with other features in the data.","eca74e4b":"Let us make a [pairplot](https:\/\/seaborn.pydata.org\/generated\/seaborn.pairplot.html) of the data","79e7201f":"Now we see that all of the VIF values are comfortably below 5. Let us now re-calculate our linear regression","51fee334":"we can see that (unsurprisingly) **mph** and **kmph** are both features with extremely high collinearity. Now let us see what happens if we drop the **kmph** column and recalculate the VIF","30029b23":"# Explainability, collinearity and the variance inflation factor (VIF)\nAn important part of data science is the aspect of explainability, and to get a feeling for what is going on there is nothing better than performing a linear regression. However, if we do not take some basic precautions we can be caught out by what is known as [collinearity](https:\/\/en.wikipedia.org\/wiki\/Multicollinearity). Ideally our linear regression works best when all of our features are independent (orthogonal) to each other, but that is not always (*i.e.* almost never) the case.\n\nHere we shall create a very simple example having a bivariate correlation; our 'ground truth' will simply be the **fuel** consumed with respect to the speed in miles per hour (**mph**). \n$$  \\mathit{fuel ~ consumed} = 1 ~ \\times ~ \\mathit{mph} $$\n\nTo this data we shall add a perfectly collinear feature which will be the speed, but now converted to kilometers per hour (**kmph**). We shall also add a couple of orthogonal and completely useless features; a column of random numbers, and a column of the number of unicorns grazing in Hyde Park at any one time. ","72ba6066":"Now we see that we obtain a coefficient of **1** for the relationship between **mph** and **fuel**.\n\n### Conclusion\nAlthough collinearity does not generally have too much effect on the quality of our machine learning predictions, nevertheless it is good practice during the exploratory data analysis to check for collinearity as this will make our results more stable (for example, try changing the RNG seed in the first code cell of this notebook to see how the results can vary when colinearity is present$^*$) and will greatly aid in interpreting the underlying relationship between our features and the target.\n\n\n(\\* The core of the problem is the numerical [inversion](https:\/\/en.wikipedia.org\/wiki\/Invertible_matrix) of an [ill-coditioned](https:\/\/en.wikipedia.org\/wiki\/Condition_number) [design matrix](https:\/\/en.wikipedia.org\/wiki\/Design_matrix)).","c55ca6a4":"We can clearly see the collinearity between **mph** and **kmph**.\n\nNow for our [linear regression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html)"}}