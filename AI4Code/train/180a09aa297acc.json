{"cell_type":{"74627935":"code","164cc729":"code","08158542":"code","b246990c":"code","a99016a4":"code","f1506771":"code","4b27daab":"code","4de2fb76":"code","1d5961fd":"code","d10fddb2":"code","67f19a8a":"code","2a6bfea0":"code","9a4b6acb":"code","153cb2da":"code","3791543d":"code","1c2c4911":"code","6b3f1fa3":"code","af805d91":"code","608ffe8f":"markdown"},"source":{"74627935":"#Libraries\nimport os\nfrom time import time\nimport math\nimport random\nimport gc\nimport numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import spearmanr, rankdata\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import MultiTaskElasticNet\nfrom sklearn.linear_model import Ridge\n\nimport tensorflow as tf\nimport keras.backend as K\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, Lambda\nfrom keras.optimizers import Adam\nfrom keras.callbacks import Callback\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import TensorBoard\nimport keras.backend.tensorflow_backend as KTF\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.callbacks import TensorBoard\n\nfrom time import gmtime, strftime\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nseed(42)\nrandom.seed(42)","164cc729":"train = pd.read_csv('..\/input\/trends-assessment-prediction\/train_scores.csv', dtype={'Id':str})\\\n            .dropna().reset_index(drop=True) # to make things easy\nreveal_ID = pd.read_csv('..\/input\/trends-assessment-prediction\/reveal_ID_site2.csv', dtype={'Id':str})\nICN_numbers = pd.read_csv('..\/input\/trends-assessment-prediction\/ICN_numbers.csv')\nloading = pd.read_csv('..\/input\/trends-assessment-prediction\/loading.csv', dtype={'Id':str})\nfnc = pd.read_csv('..\/input\/trends-assessment-prediction\/fnc.csv', dtype={'Id':str})\nsample_submission = pd.read_csv('..\/input\/trends-assessment-prediction\/sample_submission.csv', dtype={'Id':str})","08158542":"# Config\nOUTPUT_DICT = ''\nID = 'Id'\nTARGET_COLS = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\nSEED = 42","b246990c":"sample_submission['ID_num'] = sample_submission[ID].apply(lambda x: int(x.split('_')[0]))\ntest = pd.DataFrame({ID: sample_submission['ID_num'].unique().astype(str)})\ndel sample_submission['ID_num']; gc.collect()","a99016a4":"# merge\ntrain = train.merge(loading, on=ID, how='left')\ntrain = train.merge(fnc, on=ID, how='left')\n\ntest = test.merge(loading, on=ID, how='left')\ntest = test.merge(fnc, on=ID, how='left')","f1506771":"len(loading.columns)","4b27daab":"len(fnc.columns)","4de2fb76":"len(train.columns)","1d5961fd":"def outlier_2s(df):\n    for i in range(1, len(df.columns)-1):\n        col = df.iloc[:,i]\n        average = np.mean(col)\n        sd = np.std(col)\n        outlier_min = average - (sd) * 2.2\n        outlier_max = average + (sd) * 2.2\n        col[col < outlier_min] = outlier_min\n        col[col > outlier_max] = outlier_max\n    return df\n\nfrom sklearn import preprocessing\ndef scaler(df):\n    for i in range(5, len(df.columns)-5):\n        col = df.iloc[:,i]\n        col = preprocessing.minmax_scale(col)\n    return df\n\ndef mean_diff1(df):\n    for i in range(7, 7+len(loading.columns)):\n        dfa = df.iloc[:,7:7+len(loading.columns)]\n        average = np.mean(np.mean(dfa))\n        col = df.iloc[:,i]\n        for j in range(1,len(train)):\n            val = df.iloc[j]\n            val = col - average\n    return df\n\ndef mean_diff2(df):\n    for i in range(7+len(loading.columns), 7+len(loading.columns)+len(fnc.columns)-7):\n        dfa = df.iloc[:,7+len(loading.columns):7+len(loading.columns)+len(fnc.columns)]\n        average = np.mean(np.mean(dfa))\n        col = df.iloc[:,i]\n        for j in range(1,len(train)):\n            val = df.iloc[j]\n            val = col - average\n    return df","d10fddb2":"#diff1 = mean_diff1(train)\n#diff2 = mean_diff2(train)\n#train = train.merge(diff1, on=ID, how='left')\n#train = train.merge(diff2, on=ID, how='left')\n\n\n#diff1 = mean_diff1(test)\n#diff2 = mean_diff2(test)\n#test = test.merge(diff1, on=ID, how='left')\n#test = test.merge(diff2, on=ID, how='left')","67f19a8a":"train = outlier_2s(train)\ntrain = scaler(train)\ntrain = train.dropna(how='all').dropna(how='all', axis=1)","2a6bfea0":"X_train = train.drop('Id', axis=1).drop(TARGET_COLS, axis=1)\ny_train = train.drop('Id', axis=1)[TARGET_COLS]\nX_test = test.drop('Id', axis=1)","9a4b6acb":"np.random.seed(1964)\nepochs= 16\nbatch_size = 128\nverbose = 1\nvalidation_split = 0.25\ninput_dim = X_train.shape[1]\nn_out = y_train.shape[1]\n\nmodel_1 = Sequential([\n               #input\n               Dense(2048, input_shape=(input_dim,)),\n               Activation('relu'),\n               Dropout(0.1),\n               Dense(2048),\n               Activation('relu'),\n               Dropout(0.1),\n#               Dense(256),\n#               Activation('relu'),\n#               Dropout(0.1),\n#               Dense(128),\n#               Activation('relu'),\n#               Dropout(0.1),\n               #output\n               Dense(n_out),\n               Activation('relu'),\n        ])\n\nmodel_1.compile(loss='mse',\n                 optimizer='adam',\n                 metrics=['mse'])\nhist_1 = model_1.fit(X_train, y_train,\n                        batch_size = batch_size, epochs = epochs,\n                        callbacks = [],\n                        verbose=verbose, validation_split=validation_split)","153cb2da":"prediction_dict = model_1.predict(X_test)\nprediction_dict = pd.DataFrame(prediction_dict)\nprediction_dict.columns = y_train.columns\nprediction_dict.head(10)","3791543d":"pred_df = pd.DataFrame()\n\nfor TARGET in TARGET_COLS:\n    tmp = pd.DataFrame()\n    tmp[ID] = [f'{c}_{TARGET}' for c in test[ID].values]\n    tmp['Predicted'] = prediction_dict[TARGET]\n    pred_df = pd.concat([pred_df, tmp])\n\nprint(pred_df.shape)\nprint(sample_submission.shape)\n\npred_df.head()","1c2c4911":"submission = pd.merge(sample_submission, pred_df, on = 'Id')\nsubmission","6b3f1fa3":"submission = pd.merge(sample_submission, pred_df, on = 'Id')[['Id', 'Predicted_y']]\nsubmission.columns = ['Id', 'Predicted']","af805d91":"submission.to_csv('submission.csv', index=False)\nsubmission.head()","608ffe8f":"# Simple NN baseline using Keras\nIn this notebook, we aim to show a very simple startline using Keras.  \n\nThis note cointains:  \n\u3000\u3000Making a starting model by simple NN using keras  \nand does not handle:  \n\u3000\u3000EDAs  \n\u3000\u3000The files formatted in \"mat.\"   \n\nThis not is based on these kernels about other than modeling part:  \n    https:\/\/www.kaggle.com\/yasufuminakama\/trends-lgb-baseline  \n    https:\/\/www.kaggle.com\/rohitsingh9990\/trends-eda-visualization-simple-baseline  \nBIG THANKS to all kagglers!!"}}