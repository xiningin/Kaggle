{"cell_type":{"55c3de06":"code","5d65fce5":"code","fbac81ad":"code","d729d87a":"code","3ad58212":"code","b6e7aab1":"code","cb83541b":"code","e3bab1c6":"code","c78e2681":"code","bb411bb4":"code","a4c69e4a":"code","da485418":"code","7fac9335":"code","4e83d87e":"code","583a5b87":"code","3bdcc7ed":"code","7272cdb0":"code","efe1abe8":"code","54481056":"code","9515b761":"code","e2f2b0de":"code","58e31f90":"code","66fd90cd":"code","83b73ec0":"code","bccaa80d":"code","aff3adbb":"code","be55c9aa":"code","2ee93ebc":"code","dc5379be":"code","11eec610":"markdown","c95b1efc":"markdown","7a49ee7e":"markdown","ae83de9e":"markdown","1a4b95c7":"markdown","9eb61314":"markdown","2ffe56e4":"markdown","54cad0a8":"markdown","98f1df02":"markdown","e8177841":"markdown","64d20b9b":"markdown","1b7c39a0":"markdown","93679d87":"markdown","5ce3e55b":"markdown","5ab7e451":"markdown","6ac39155":"markdown","31a2dbf7":"markdown","75742103":"markdown","af6e65c7":"markdown","4b5f9560":"markdown","c19206e1":"markdown","4ead764c":"markdown","5d785f16":"markdown","50fb4549":"markdown","52876583":"markdown","30c44e5b":"markdown","e873cb6f":"markdown","678db226":"markdown","9d41cc2b":"markdown","72a0450b":"markdown","2aabab55":"markdown","22c0e01d":"markdown","ea53a798":"markdown","15279f2f":"markdown"},"source":{"55c3de06":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\nimport itertools\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5d65fce5":"df=pd.read_csv('\/kaggle\/input\/a-fake-news-dataset-around-the-syrian-war\/FA-KES-Dataset.csv',encoding='latin1')\ndf.head()","fbac81ad":"df.isnull().sum().sum()","d729d87a":"print('There are {} rows and {} columns in train'.format(df.shape[0],df.shape[1]))","3ad58212":"print(df.article_content.describe())","b6e7aab1":"ddf = df[df.duplicated()]\nprint(ddf)","cb83541b":"df.drop_duplicates(keep=False, inplace=True)","e3bab1c6":"ddf = df[df.duplicated()]\nprint(ddf)","c78e2681":"#Show Labels distribution\n\ndf['labels'].value_counts(normalize=True)\n","bb411bb4":"sns.countplot(x='labels', data=df)","a4c69e4a":"df['source'].value_counts().plot(kind='barh')","da485418":"\ndf.groupby(['source','labels']).size().unstack().plot(kind='bar',stacked=False)\nplt.figure(figsize=(20,10))\nplt.show()","7fac9335":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntrue_len=df[df['labels']==1]['article_content'].str.len()\nax1.hist(true_len,color='green')\nax1.set_title('Real News')\nfake_len=df[df['labels']==0]['article_content'].str.len()\nax2.hist(fake_len,color='red')\nax2.set_title('Fake News')\nfig.suptitle('Characters in an article')\nplt.show()","4e83d87e":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntrue_len=df[df['labels']==1]['article_content'].str.split().map(lambda x: len(x))\nax1.hist(true_len,color='green')\nax1.set_title('Real News')\nfake_len=df[df['labels']==0]['article_content'].str.split().map(lambda x: len(x))\nax2.hist(fake_len,color='red')\nax2.set_title('Fake News')\nfig.suptitle('Words in an article')\nplt.show()","583a5b87":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=df[df['labels']==1]['article_content'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='green')\nax1.set_title('Real')\nword=df[df['labels']==0]['article_content'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')\nax2.set_title('Fake')\nfig.suptitle('Average word length in each article')","3bdcc7ed":"mfreq = pd.Series(' '.join(df[df['labels']==1]['article_content']).split()).value_counts()[:25]\nmfreq","7272cdb0":"vect = TfidfVectorizer(use_idf=True,max_df=0.40,min_df=0.1,stop_words='english').fit(df[df['labels']==1]['article_content'])\nlen(vect.get_feature_names())","efe1abe8":"list(vect.vocabulary_.keys())[:10]","54481056":"true_tfidf=list(vect.vocabulary_.keys())\nwordcloud = WordCloud(width=1600, height=800).generate(str(true_tfidf))\n#  plot word cloud image.\n\nplt.figure( figsize=(20,10), facecolor='k')\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","9515b761":"mfreq = pd.Series(' '.join(df[df['labels']==0]['article_content']).split()).value_counts()[:25]\nmfreq","e2f2b0de":"vect = TfidfVectorizer(use_idf=True,max_df=0.40,min_df=0.1,stop_words='english').fit(df[df['labels']==0]['article_content'])\nlen(vect.get_feature_names())","58e31f90":"fake_tfidf=list(vect.vocabulary_.keys())\nwordcloud = WordCloud(width=1600, height=800).generate(str(fake_tfidf))\n#  plot word cloud image.\n\nplt.figure( figsize=(20,10), facecolor='k')\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","66fd90cd":"#Intialize TfidfVectorizer\ntfidf_vect=TfidfVectorizer(stop_words='english',max_df=0.4,min_df=0.1).fit(df['article_content'])\nlen(tfidf_vect.get_feature_names())","83b73ec0":"txt_tfidf=list(tfidf_vect.vocabulary_.keys())\nwordcloud = WordCloud(width=1600, height=800).generate(str(txt_tfidf))\n#  plot word cloud image.\n\nplt.figure( figsize=(20,10), facecolor='k')\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","bccaa80d":"tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n\nfeatures = tfidf.fit_transform(df.article_content).toarray()\nlabels = df.labels\nfeatures.shape","aff3adbb":"X_train, X_test, y_train, y_test = train_test_split(df['article_content'], df['labels'], random_state = 0)\ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(X_train)\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n\nclf = MultinomialNB().fit(X_train_tfidf, y_train)","be55c9aa":"print(clf.predict(count_vect.transform([\"The Syrian army has taken control of a strategic northwestern crossroads town, its latest gain in a weeks-long offensive against the country's last major rebel bastion.\"])))","2ee93ebc":"models = [\n    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n    LinearSVC(),\n    MultinomialNB(),\n    LogisticRegression(random_state=0)]\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\nentries = []\nfor model in models:\n  model_name = model.__class__.__name__\n  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n  for fold_idx, accuracy in enumerate(accuracies):\n    entries.append((model_name, fold_idx, accuracy))\ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n\n\nsns.boxplot(x='model_name', y='accuracy', data=cv_df)\nsns.stripplot(x='model_name', y='accuracy', data=cv_df, \n              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\nplt.show()","dc5379be":"cv_df.groupby('model_name').accuracy.mean()","11eec610":"**Conclusion**","c95b1efc":"Wordcloud for words in real news after some cleaning and deleting stop words using TfidfVectorizer ","7a49ee7e":"**Classifier: Features and Design**","ae83de9e":"**Exploratory Data Analysis of News**","1a4b95c7":"**Data Exploration****","9eb61314":"Awesome!!!!!!!!!!!!!!!!!!!! That's correct","2ffe56e4":"We will do very basic analysis,that is character level,word level and sentence level analysis.","54cad0a8":"It's better to strat with understaning how our dataset distributed according to the label(labels 0\/1)","98f1df02":"Let's try predicting on recent news ???","e8177841":"**Model Selection**\n\n\nWe are now ready to experiment with different machine learning models, evaluate their accuracy and find the source of any potential issues.\n\nWe will benchmark the following four models:\n\n* Logistic Regression \n* (Multinomial) Naive Bayes \n* Linear Support Vector Machine \n* Random Forest","64d20b9b":"We have duplicated rows in our dataset","1b7c39a0":"Let's see down wordclod for the whole articles(real and fake) from article_content","93679d87":"**Preliminary text exploration**\n\n\nBefore we proceed with any text pre-processing, it is advisable to quickly explore the dataset in terms of word counts, most common and most uncommon words.","5ce3e55b":"**Loading the data**","5ab7e451":"Our dataset is a bit unbalanced towords real news(1)","6ac39155":"Naive Bayes Classifier: the one most suitable for word counts is the multinomial variant:","31a2dbf7":"Find Duplicate Rows based on all columns","75742103":"Number of words in a article","af6e65c7":"We will now visualize the text  to get insights on the most frequently used words.","4b5f9560":"We will use TfidfVectorizer for some text pre-processing like removing stop words and to get the vocabularies in our articles","c19206e1":"It showes here how each source is contributing in real or fake news","4ead764c":"Average word length in a article","5d785f16":"\nThe accuracy of these models on predicting is low.in this case I think it's better to go and collect more data rathar than trying another model to get better accuracy","50fb4549":"**About the dataset**\n\nThe dataset consists of news articles from several media outlets representing mobilisation press, loyalist press, and diverse print media.Also,consists of a set of articles\/news labeled by 0 (fake) or 1 (credible).\nThe dataset consists of 804 articles labeled as true or fake and that is ideal for training machine learning models to predict the credibility of news articles.\n\nCredibility of articles are computed with respect to a ground truth information obtained from the Syrian Violations Documentation Center (VDC). This dataset is collected by researchers at American University of Beirut(AUB).","52876583":"* To train supervised classifiers, we first transformed the \u201carticle_content\u201d into a vector of numbers. We explored vector representations such as TF-IDF weighted vectors.\n\n* After having this vector representations of the text we can train supervised classifiers to train unseen \u201carticle_content\u201d and predict the \u201clabels\u201d(0\/1) on which they fall.\n\nAfter all the above data transformation, now that we have all the features and labels, it is time to train the classifiers. There are a number of algorithms we can use for this type of problem.\n\nNaive Bayes Classifier: the one most suitable for word counts is the multinomial variant:","30c44e5b":"Wordcloud for words in fake news after some cleaning and deleting stop words using TfidfVectorizer ","e873cb6f":"Count NaN or missing values in DataFrame","678db226":"**The Most common words in Real news**","9d41cc2b":"**Background**\n\nMost currently available fake news datasets revolve around US politics, entrainment news or satire. They are typically scraped from fact-checking websites, where the articles are labeled by human experts.\nThis dataset around the Syrian war. Given the specific nature of news reporting on incidents of wars and the lack of available sources from which manually-labeled news articles can be scraped.","72a0450b":"We can see here sources of news in an ascending order","2aabab55":"Now we can move forward in our task!","22c0e01d":"Duplicated rows might affect on our results, So, we should remove them.","ea53a798":"And let's see the most common words in fake news","15279f2f":"As we can see, the dataset contains the articl title, article content, media source, date of incident,where and the incident happend and the labels(real or fake)."}}