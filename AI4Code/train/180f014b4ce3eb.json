{"cell_type":{"6ba666cb":"code","826daa5d":"code","eaa6dba7":"code","f7f14dfa":"code","4ed0b1e2":"code","025445c2":"code","1a6a9106":"code","56b164b8":"code","8177c843":"code","990bdbcf":"code","6be703b8":"code","02f9e682":"code","52dc929a":"code","ad8ce32c":"code","1b1d0084":"code","15f2981d":"code","c529a31e":"code","98068939":"code","a00f2d33":"code","d6f564b5":"code","57ee194f":"code","03ab81eb":"code","8cda8342":"code","b61c2348":"code","05acb40f":"code","89bf4795":"code","616de397":"code","43169aa3":"code","18d23d65":"code","ea84c4a0":"code","79c5e6b2":"code","19503cc3":"code","c1bd646e":"code","bb6ac32f":"code","5c318aa7":"code","bdd48567":"code","a0366b7b":"code","8c8050c9":"code","d2e70839":"code","e549ed08":"code","65baa5b3":"code","e62d790b":"code","2bd45539":"code","960c5bf4":"code","964db67a":"code","3a40290f":"code","d84907d0":"code","6bf99c94":"code","aa190666":"code","aa8aaf98":"code","d385c7f2":"code","01cf9f3f":"code","4a4cd1ed":"code","ce4b4e00":"code","72a6da88":"code","3a8967c1":"code","5dedce27":"code","51b7ef86":"code","0c2df8fa":"code","4cf36c01":"code","c6cf4c19":"code","201f1eed":"code","5c671249":"code","43d15647":"code","02239ae9":"code","727a5443":"code","b174d172":"code","f2d30c49":"code","728f33a5":"code","db23b197":"code","f01c50ef":"code","780276de":"code","f6ddaa81":"markdown","09a57e61":"markdown","abb55539":"markdown","f59893d9":"markdown","7667b043":"markdown","14bf8593":"markdown","e6bb3ce8":"markdown","d81fb935":"markdown","54ef6870":"markdown"},"source":{"6ba666cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","826daa5d":"# Import all the tools we need\n\n# Regular EDA (exploratory data analysis) and plotting libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# we want our plots to appear inside the notebook\n%matplotlib inline \n\n# Models from Scikit-Learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model Evaluations\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","eaa6dba7":"df = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")\ndf.shape # (rows, columns)","f7f14dfa":"df.head()","4ed0b1e2":"df.tail()","025445c2":"# Let's find out how many of each class there\ndf[\"target\"].value_counts()","1a6a9106":"# Normalized value counts\ndf.target.value_counts(normalize=True)","56b164b8":"df[\"target\"].value_counts().plot(kind = \"bar\", color = [\"salmon\", \"lightblue\"]);","8177c843":"df.info()","990bdbcf":"# Are there any missing data\ndf.isna().sum()","6be703b8":"df.describe()","02f9e682":"df.sex.value_counts()","52dc929a":"# Compare target column with sex column\n\npd.crosstab(df.target, df.sex)","ad8ce32c":"# Create a plot of crosstab \npd.crosstab(df.target, df.sex).plot(kind= \"bar\", figsize=(10, 6),color = [\"salmon\", \"lightblue\"])\n\nplt.title(\" Heart Disease Frequency for Sex\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.ylabel(\"Amount\")\n\nplt.legend([\"Female\", \"Male\"])\n\nplt.xticks(rotation = 0);\n","1b1d0084":"df.head()","15f2981d":"df[\"thalach\"].value_counts()","c529a31e":"# Create another figure\nplt.figure(figsize=(10, 6))\n\n# Scatter with positive examples\nplt.scatter(df.age[df.target ==1],\n           df.thalach[df.target ==1],\n           c= \"salmon\")\n\n# Scatter with negative example\n\nplt.scatter(df.age[df.target == 0],\n           df.thalach[df.target == 0],\n           c= \"lightblue\")\n# Add some helpful info\nplt.title(\"Heart Disease in function of Age and Max Heart Rate\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max Heart Rate\")\nplt.legend([\"Disease\", \"No Disease\"]);","98068939":"# Check the distribution of the age column with a histgram\n\ndf.age.plot.hist();","a00f2d33":"pd.crosstab(df.cp, df.target)","d6f564b5":"# Make the crosstab more visual\npd.crosstab(df.cp, df.target).plot(kind=\"bar\",\n                                   figsize=(10, 6),\n                                   color=[\"salmon\", \"lightblue\"])\n\n# Add some communication\nplt.title(\"Heart Disease Frequency Per Chest Pain Type\")\nplt.xlabel(\"Chest Pain Type\")\nplt.ylabel(\"Amount\")\nplt.legend([\"No Disease\", \"Disease\"])\nplt.xticks(rotation=0);","57ee194f":"df.head(2)","03ab81eb":"# Make a correlation matrix\ndf.corr()","8cda8342":"# Let's make it look a little prettier\ncorr_matrix = df.corr()\nplt.figure(figsize=(15, 10))\nsns.heatmap(corr_matrix, \n            annot=True, \n            linewidths=0.5, \n            fmt= \".2f\", \n            cmap=\"YlGnBu\");","b61c2348":"df.head()","05acb40f":"# Everything except target variable\nX = df.drop(\"target\", axis=1)\n\n# Target variable\ny = df.target.values","89bf4795":"# Independent variables (no target column)\nX.head()","616de397":"# Targets\ny","43169aa3":"# Random seed for reproducibility\nnp.random.seed(42)\n\n# Split into train & test set\nX_train, X_test, y_train, y_test = train_test_split(X, # independent variables \n                                                    y, # dependent variable\n                                                    test_size = 0.2) # percentage of data to use for test set","18d23d65":"X_train.head()","ea84c4a0":"y_train, len(y_train)","79c5e6b2":"X_test.head()","19503cc3":"y_test, len(y_test)","c1bd646e":"# Put models in a dictionary\n\nmodels = {\"KNN\":KNeighborsClassifier(),\n          \"Logistic Regression\": LogisticRegression(),\n          \"Random Forest\":RandomForestClassifier()}\n\n# Create function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of different Scikit-Learn machine learning models\n    X_train : training data\n    X_test : testing data\n    y_train : labels assosciated with training data\n    y_test : labels assosciated with test data\n    \"\"\"\n    # Random seed for reproducible results\n    np.random.seed(42)\n    # Make a list to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores\n    ","bb6ac32f":"model_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test)\nmodel_scores","5c318aa7":"model_compare = pd.DataFrame(model_scores, index=['accuracy'])","bdd48567":"model_compare","a0366b7b":"model_compare.T","8c8050c9":"model_compare.T.plot.bar();","d2e70839":"# Create a list of train scores\ntrain_scores = []\n\n# Create a list of test scores\ntest_scores = []\n\n# Create a list of different values for n_neighbors\nneighbors = range(1, 21) # 1 to 20\n\n# Setup algorithm\nknn = KNeighborsClassifier()\n\n# Loop through different neighbors values\nfor i in neighbors:\n    knn.set_params(n_neighbors = i) # set neighbors value\n    \n    # Fit the algorithm\n    knn.fit(X_train, y_train)\n    \n    # Update the training scores\n    train_scores.append(knn.score(X_train, y_train))\n    \n    # Update the test scores\n    test_scores.append(knn.score(X_test, y_test))","e549ed08":"train_scores","65baa5b3":"plt.plot(neighbors, train_scores, label=\"Train score\")\nplt.plot(neighbors, test_scores, label=\"Test score\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")","e62d790b":"# Different LogisticRegression hyperparameters\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Different RandomForestClassifier hyperparameters\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}","2bd45539":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for LogisticRegression\n\nrs_log_reg = RandomizedSearchCV(LogisticRegression(), param_distributions= log_reg_grid,\n                               cv= 5,\n                               n_iter=20, verbose=True)\n\n# Fit random hyperparameter search model\nrs_log_reg.fit(X_train, y_train);","960c5bf4":"rs_log_reg.best_params_","964db67a":"rs_log_reg.score(X_test, y_test)","3a40290f":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\n\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fit random hyperparameter search model\nrs_rf.fit(X_train, y_train);","d84907d0":"# Find the best parameters\nrs_rf.best_params_","6bf99c94":"# Evaluate the randomized search random forest model\nrs_rf.score(X_test, y_test)","aa190666":"# Different LogisticRegression hyperparameters\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid=log_reg_grid,\n                          cv=5,\n                          verbose=True)\n\n# Fit grid hyperparameter search model\ngs_log_reg.fit(X_train, y_train);","aa8aaf98":"# Check the best parameters\ngs_log_reg.best_params_","d385c7f2":"# Evaluate the model\ngs_log_reg.score(X_test, y_test)","01cf9f3f":"# Make preidctions on test data\ny_preds = gs_log_reg.predict(X_test)","4a4cd1ed":"y_preds","ce4b4e00":"y_test","72a6da88":"# Import ROC curve function from metrics module\nfrom sklearn.metrics import plot_roc_curve\n\n# Plot ROC curve and calculate AUC metric\nplot_roc_curve(gs_log_reg, X_test, y_test);","3a8967c1":"# Display confusion matrix\nprint(confusion_matrix(y_test, y_preds))","5dedce27":"# Import Seaborn\nimport seaborn as sns\nsns.set(font_scale=1.5) # Increase font size\n\ndef plot_conf_mat(y_test, y_preds):\n    \"\"\"\n    Plots a confusion matrix using Seaborn's heatmap().\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True, # Annotate the boxes\n                     cbar=False)\n    plt.xlabel(\"true label\")\n    plt.ylabel(\"predicted label\")\n    \nplot_conf_mat(y_test, y_preds)","51b7ef86":"# Show classification report\nprint(classification_report(y_test, y_preds))","0c2df8fa":"# Check best hyperparameters\ngs_log_reg.best_params_","4cf36c01":"# Import cross_val_score\nfrom sklearn.model_selection import cross_val_score\n\n# Instantiate best model with best hyperparameters (found with GridSearchCV)\nclf = LogisticRegression(C=0.23357214690901212,\n                         solver=\"liblinear\")","c6cf4c19":"# Cross-validated accuracy score\ncv_acc = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5, # 5-fold cross-validation\n                         scoring=\"accuracy\") # accuracy as scoring\ncv_acc","201f1eed":"cv_acc = np.mean(cv_acc)\ncv_acc","5c671249":"# Cross-validated precision score\ncv_precision = np.mean(cross_val_score(clf,\n                                       X,\n                                       y,\n                                       cv=5, # 5-fold cross-validation\n                                       scoring=\"precision\")) # precision as scoring\ncv_precision","43d15647":"# Cross-validated recall score\ncv_recall = np.mean(cross_val_score(clf,\n                                    X,\n                                    y,\n                                    cv=5, # 5-fold cross-validation\n                                    scoring=\"recall\")) # recall as scoring\ncv_recall","02239ae9":"# Cross-validated F1 score\ncv_f1 = np.mean(cross_val_score(clf,\n                                X,\n                                y,\n                                cv=5, # 5-fold cross-validation\n                                scoring=\"f1\")) # f1 as scoring\ncv_f1","727a5443":"# Visualizing cross-validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                            \"Precision\": cv_precision,\n                            \"Recall\": cv_recall,\n                            \"F1\": cv_f1},\n                          index=[0])\ncv_metrics.T.plot.bar(title=\"Cross-Validated Metrics\", legend=False);","b174d172":"# Fit an instance of LogisticRegression (taken from above)\nclf.fit(X_train, y_train);","f2d30c49":"# Now Check coef_\nclf.coef_","728f33a5":"# Match features to columns\nfeatures_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeatures_dict","db23b197":"# Visualize feature importance\nfeatures_df = pd.DataFrame(features_dict, index=[0])\nfeatures_df.T.plot.bar(title=\"Feature Importance\", legend=False);","f01c50ef":"pd.crosstab(df[\"sex\"], df[\"target\"])","780276de":"# Contrast slope (positive coefficient) with target\npd.crosstab(df[\"slope\"], df[\"target\"])","f6ddaa81":"### Age vs. Max Heart Rate for Heart Disease","09a57e61":"### Heart Disease Frequency per Chest Pain Type\n\ncp - chest pain type\n\n    * 0: Typical angina: chest pain related decrease blood supply to the heart    \n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)    \n    * 3: Asymptomatic: chest pain not showing signs of disease","abb55539":"### Heart Disease Frequency according to Sex","f59893d9":"### Data Exploration (exploratory data analysis or EDA)","7667b043":"### Tuning models with with RandomizedSearchCV","14bf8593":"### Model Comparison","e6bb3ce8":"### Now Tuning a model with GridSearchCV","d81fb935":"Looking at the graph, n_neighbors = 11 seems best.\n\nEven knowing this, the KNN's model performance didn't get near what LogisticRegression or the RandomForestClassifier did.\n\nBecause of this, we'll discard KNN and focus on the other two.\n\nWe've tuned KNN by hand but let's see how we can LogisticsRegression and RandomForestClassifier using RandomizedSearchCV.","54ef6870":"### Evaluating a classification model, beyond accuracy"}}