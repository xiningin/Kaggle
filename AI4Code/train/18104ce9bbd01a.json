{"cell_type":{"b2093688":"code","c120a66a":"code","1e824989":"code","f3f8b33b":"code","28bf0b2a":"code","9a65e946":"code","4ddca572":"code","eaa48e48":"code","19e7a9ac":"code","3925e45f":"code","d95fab66":"code","8bd68c9a":"code","b6e77f29":"code","cbea6781":"code","75a15add":"code","f87efcd8":"code","a712a453":"code","0b38c4ae":"code","2087d8e6":"code","82fb15bc":"code","a638b924":"code","d373ef2a":"code","7bed8c60":"code","e7425d6f":"code","005add02":"code","a0bbabc3":"code","606355cb":"code","6c025c01":"code","4aec6d30":"code","b8d69cc5":"code","86c07e02":"code","00c111a9":"code","d43d3f10":"code","800f717e":"code","8bbb106e":"code","b3242127":"code","43da214c":"code","6479a6f6":"code","76e1090f":"code","b93cddef":"code","b88bcfc9":"code","bf2078dd":"code","928531b7":"code","b37fba40":"code","54eec123":"code","91f6a6d4":"code","e6cbaa5d":"code","4040ce03":"markdown","b47f232a":"markdown","dfdb2c4e":"markdown","93f2343e":"markdown","ce91f95f":"markdown","0a009fe3":"markdown","5800bad3":"markdown","cd208ed5":"markdown","36a1566e":"markdown","a7a7c587":"markdown","d618920d":"markdown","739cb13f":"markdown","328f79f8":"markdown","2baeb16a":"markdown","7e38a4f0":"markdown"},"source":{"b2093688":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\n\n\nplt.style.use('ggplot')\nplt.rcParams.update({'font.size': 14})\n\n#ML Models\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n#Metrics \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\n\n#Plot\nimport scikitplot as skplt\n\nimport warnings\nwarnings.filterwarnings('ignore')","c120a66a":"df = pd.read_csv('..\/input\/passenger-list-for-the-estonia-ferry-disaster\/estonia-passenger-list.csv')\ndf.head()","1e824989":"#Checking nan values\ndf.isnull().mean() * 100","f3f8b33b":"#Plot How many Survived\nax = df['Survived'].value_counts().plot(kind='bar', figsize=(12,6), color=['skyblue', 'violet'], rot=0,\n                                  title='How many Survived?')","28bf0b2a":"df['Sex'].value_counts().plot(kind='bar', color=['skyblue', 'violet'], rot=0, title='Numbers of Passagers by sex', figsize=(16,6));","9a65e946":"df.groupby(['Sex', 'Survived'])['Survived'].count().unstack().plot(kind='bar', color=['skyblue', 'violet'], rot=0, title='Survived by Sex',\n                                                                      figsize=(16,6));","4ddca572":"df.groupby(['Country', 'Survived'])['Survived'].count().unstack().plot(kind='bar', color=['skyblue', 'violet'], rot=45, title='Survived by Country',\n                                                                      figsize=(16,6));\nprint(f'How many Countrys the data have: {df[\"Country\"].nunique()}')","eaa48e48":"df['Country'] = df['Country'].apply(lambda x: 'Other' if x not in df['Country'].value_counts()[:3].index.to_list() else x)","19e7a9ac":"#Plot again\ndf.groupby(['Country', 'Survived'])['Survived'].count().unstack().plot(kind='bar', color=['skyblue', 'violet'], rot=45, title='Survived by Country',\n                                                                      figsize=(16,6));\nprint(f'How many Countrys the data have: {df[\"Country\"].nunique()}')","3925e45f":"age_bins = [0, 10, 18, 30, 55, 100]\ngroup_names = ['child', 'teenager', 'young adult', 'adult', 'elderly']\ndf['cat_age'] = pd.cut(df['Age'], age_bins, right=False, labels=group_names)","d95fab66":"df['cat_age'].value_counts().plot(kind='bar', color='skyblue', rot=45, title='Passengers by category age', figsize=(16,6));","8bd68c9a":"df.groupby(['cat_age', 'Survived'])['Survived'].count().unstack().plot(kind='bar', color=['skyblue', 'violet'], rot=45, figsize=(16, 6),\n                                                                  title='Survived by category Age');","b6e77f29":"df_clean = df.drop(['PassengerId', 'Firstname', 'Lastname'], axis=1)","cbea6781":"df_clean.head()","75a15add":"X = df_clean.drop(['Survived', 'cat_age'], axis=1)\nX = pd.get_dummies(X, columns=['Country', 'Sex', 'Category'], drop_first=True)\ny = df_clean['Survived']\nX.head()","f87efcd8":"Seed = 12\nknn = KNeighborsClassifier()\ndt = DecisionTreeClassifier(random_state=Seed)\nsvc = SVC(gamma='auto', random_state=Seed)\nada = AdaBoostClassifier()\nrf = RandomForestClassifier()\nlr = LogisticRegression(max_iter=1000)\nls = LinearSVC()\nbc = BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=Seed)","a712a453":"model_list = [('KNeighborsClassifier', knn),\n              ('DecisionTree', dt),\n              ('SVC', svc),\n              ('AdaBoost', ada),\n              ('RandomForest', rf),\n              ('LogisticRegression', lr),\n              ('LinearSVC', ls),\n              ('BaggingClassifier', bc)]","0b38c4ae":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=Seed)","2087d8e6":"y_test","82fb15bc":"%%time \nfor name, model in model_list:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    print(f'Model: {name} test_acc: {acc * 100:.2f}%')","a638b924":"predict_all_died = accuracy_score(y_test, np.zeros(len(y_test)))\nprint(f'Accuracy score if we predict everyone died in your test data: {predict_all_died *100:.2f}%')","d373ef2a":"predict_all_died = roc_auc_score(y_test, np.zeros(len(y_test)))\nprint(f'ROC AUC SCORE if we predict everyone died in your test data: {predict_all_died*100:.2f}%')","7bed8c60":"%%time \nfor name, model in model_list:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    roc_auc = roc_auc_score(y_test, y_pred)\n    print(f'Model: {name} test_acc: {acc * 100:.2f}% roc_auc_test: {roc_auc * 100:.2f}%')","e7425d6f":"# Oversampling\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.over_sampling import SMOTENC","005add02":"smote = SMOTE()\nadasyn = ADASYN()\nbl = BorderlineSMOTE()\nsmote_nc = SMOTENC(categorical_features=[0, 1], random_state=Seed)","a0bbabc3":"oversampling_list = [('SMOTE', smote),\n                     ('ADASYN', adasyn),\n                     ('BorderlineSMOTE', bl),\n                     ('SMOTENC', smote_nc)]","606355cb":"#Create your validade set.\nX, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=Seed)","6c025c01":"#See our target\ny.value_counts().plot(kind='bar', color='skyblue', rot=0, title='Data without oversampling');","4aec6d30":"# Now we will resample our data to make equal.\nX_resampled, y_resampled = smote.fit_resample(X, y)\ny_resampled.value_counts().plot(kind='bar', color='skyblue', rot=0, title='Data with oversampling');","b8d69cc5":"%%time\nresults = []\nfor imblearn, method in oversampling_list:\n    X_resampled, y_resampled = method.fit_resample(X, y)\n    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=Seed)\n    for name, model in model_list:\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        y_pred_val = model.predict(X_val)\n        acc = round(accuracy_score(y_test, y_pred),4)\n        acc_val = round(accuracy_score(y_val, y_pred_val),4)\n        roc_test = round(roc_auc_score(y_test, y_pred), 4)\n        roc_val = round(roc_auc_score(y_val, y_pred_val), 4)\n        results.append({'Method': imblearn,'Model': name, 'test_acc': acc, 'val_acc': acc_val, 'roc_test': roc_test, 'roc_val': roc_val})","86c07e02":"results = pd.DataFrame(results)","00c111a9":"results.sort_values(by='roc_val', ascending=False).head(10)","d43d3f10":"#Undersampling\n\nfrom imblearn.under_sampling import ClusterCentroids\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.under_sampling import EditedNearestNeighbours \nfrom imblearn.under_sampling import RepeatedEditedNearestNeighbours \nfrom imblearn.under_sampling import AllKNN\nfrom imblearn.under_sampling import CondensedNearestNeighbour\nfrom imblearn.under_sampling import OneSidedSelection\nfrom imblearn.under_sampling import NeighbourhoodCleaningRule","800f717e":"cc = ClusterCentroids(random_state=Seed)\nrus = RandomUnderSampler(random_state=Seed)\nenn = EditedNearestNeighbours()\nrenn = RepeatedEditedNearestNeighbours()\nallknn = AllKNN()\ncnn = CondensedNearestNeighbour(random_state=Seed)\noss = OneSidedSelection(random_state=Seed)\nncr = NeighbourhoodCleaningRule()","8bbb106e":"undersampling_list = [('ClusterCentroids', cc),\n                      ('RandomUnderSampler', rus),\n                      ('EditedNearestNeighbours', enn),\n                      ('RepeatedEditedNearestNeighbours', renn),\n                      ('AllKNN', allknn),\n                      ('CondensedNearestNeighbour', cnn),\n                      ('OneSidedSelection', oss),\n                      ('NeighbourhoodCleaningRule', ncr)]","b3242127":"y.value_counts().plot(kind='bar', color='skyblue', rot=0, title='Data without undersampling');","43da214c":"X_resampled, y_resampled = cc.fit_resample(X, y)\ny_resampled.value_counts().plot(kind='bar', color='skyblue', rot=0, title='Data with undersampling');","6479a6f6":"%%time\nresults_under = []\nfor imblearn, method in undersampling_list:\n    X_resampled, y_resampled = method.fit_resample(X, y)\n    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=Seed)\n    for name, model in model_list:\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        y_pred_val = model.predict(X_val)\n        acc = round(accuracy_score(y_test, y_pred),4)\n        acc_val = round(accuracy_score(y_val, y_pred_val),4)\n        roc_test = round(roc_auc_score(y_test, y_pred), 4)\n        roc_val = round(roc_auc_score(y_val, y_pred_val), 4)\n        results_under.append({'Method': imblearn,'Model': name, 'test_acc': acc, 'val_acc': acc_val, 'roc_test': roc_test, 'roc_val': roc_val})","76e1090f":"test_under = pd.DataFrame(results_under)\ntest_under.head()","b93cddef":"test_under.sort_values(by=['roc_val', 'roc_test'], ascending=[False, False])[:10]","b88bcfc9":"%%time\nmixed = []\nfor imblearn, method in oversampling_list:\n    X_resampled, y_resampled = method.fit_resample(X, y)\n    for imblearn2, method2 in undersampling_list:\n        X_resampled1, y_resampled1 = method2.fit_resample(X_resampled, y_resampled)\n        X_train, X_test, y_train, y_test = train_test_split(X_resampled1, y_resampled1, test_size=0.2, random_state=Seed)\n        for name, model in model_list:\n            model.fit(X_train, y_train)\n            y_pred = model.predict(X_test)\n            y_pred_val = model.predict(X_val)\n            acc = round(accuracy_score(y_test, y_pred),4)\n            acc_val = round(accuracy_score(y_val, y_pred_val),4)\n            roc_test = round(roc_auc_score(y_test, y_pred), 4)\n            roc_val = round(roc_auc_score(y_val, y_pred_val), 4)\n            mixed.append({'Method 1': imblearn,'Method 2': imblearn2, 'Model': name, 'test_acc': acc, 'val_acc': acc_val, 'roc_test': roc_test, 'roc_val': roc_val})","bf2078dd":"mixed = pd.DataFrame(mixed)\nmixed.head()","928531b7":"mixed.sort_values(by=['roc_val', 'roc_test'], ascending=[False, False])[:10]","b37fba40":"fig, (axes1, axes2) = plt.subplots(1, 2, figsize=(16, 6))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=Seed)\ndt.fit(X_train, y_train)\ny_probas = dt.predict_proba(X_val)\ny_pred = dt.predict(X_val)\nacc = accuracy_score(y_val, y_pred )\nskplt.metrics.plot_roc(y_val, y_probas, cmap='cool', plot_micro=False, plot_macro=False,ax= axes1)\nskplt.metrics.plot_confusion_matrix(y_val, y_pred, ax=axes2)\nfig.suptitle(f'Results without sampling Data and DecisionTree  Accuracy Validation Test: {acc *100:.2f}%\\n', fontsize=20) \nplt.show()","54eec123":"fig, (axes1, axes2) = plt.subplots(1, 2, figsize=(16, 6))\n\nX_resampled, y_resampled = smote_nc.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=Seed)\nada.fit(X_train, y_train)\ny_probas = ada.predict_proba(X_val)\ny_pred = ada.predict(X_val)\nacc = accuracy_score(y_val, y_pred )\nskplt.metrics.plot_roc(y_val, y_probas, cmap='cool', plot_micro=False, plot_macro=False,ax= axes1)\nskplt.metrics.plot_confusion_matrix(y_val, y_pred, ax=axes2)\nfig.suptitle(f'Results with SMOTENC and AdaBoost  Accuracy Validation Test: {acc *100:.2f}%\\n', fontsize=20)\nplt.show()","91f6a6d4":"fig, (axes1, axes2) = plt.subplots(1, 2, figsize=(16, 6))\n\nX_resampled, y_resampled = renn.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=Seed)\nada.fit(X_train, y_train)\ny_probas = ada.predict_proba(X_val)\ny_pred = ada.predict(X_val)\nacc = accuracy_score(y_val, y_pred )\nskplt.metrics.plot_roc(y_val, y_probas, cmap='cool', plot_micro=False, plot_macro=False,ax= axes1)\nskplt.metrics.plot_confusion_matrix(y_val, y_pred, ax=axes2)\nfig.suptitle(f'Results with RepeatedEditedNearestNeighbours and AdaBoost  Accuracy Validation Test: {acc *100:.2f}%\\n', fontsize=20)\nplt.show()","e6cbaa5d":"\nfig, (axes1, axes2) = plt.subplots(1, 2, figsize=(16, 6))\n\nX_resampled, y_resampled = smote_nc.fit_resample(X, y)\nX_resampled, y_resampled = enn.fit_resample(X_resampled, y_resampled)\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=Seed)\nlr.fit(X_train, y_train)\ny_probas = lr.predict_proba(X_val)\ny_pred = lr.predict(X_val)\nacc = accuracy_score(y_val, y_pred)\nskplt.metrics.plot_roc(y_val, y_probas, cmap='cool', plot_micro=False, plot_macro=False,ax= axes1)\nskplt.metrics.plot_confusion_matrix(y_val, y_pred, ax=axes2)\nfig.suptitle(f'Results with SMOTENC, EditedNearestNeighbours and LogisticRegression  Accuracy Validation Test: {acc *100:.2f}%\\n', fontsize=20)\nplt.show()","4040ce03":"![](https:\/\/pbs.twimg.com\/media\/D47GZ7tU4AAf4yI.jpg)\nAgain we didn't beat the baseline, even after we change the metric score.","b47f232a":"Much better measure for your model, lets run again our models.","dfdb2c4e":"## Comparing our Scores <a id=\"7\"><\/a>","93f2343e":"# Dealing with imbalanced data\n\n\n![](https:\/\/image.freepik.com\/vetores-gratis\/balanca-da-justica-equilibrio-de-peso_29937-3252.jpg)\n\n## What is Imbalanced Data?\nImbalanced data typically refers to a problem with classification problems where the classes are not represented equally.\n\nFor example, you may have a 2-class (binary) classification problem with 100 instances (rows). A total of 80 instances are labeled with Class-1 and the remaining 20 instances are labeled with Class-2.\n\nThis is an imbalanced dataset and the ratio of Class-1 to Class-2 instances is 80:20 or more concisely 4:1.\n\nYou can have a class imbalance problem on two-class classification problems as well as multi-class classification problems. Most techniques can be used on either. [Continue reading...](https:\/\/machinelearningmastery.com\/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset\/)\n\n\n## How we will deal with it?\n\n- Changing metric\n- Oversampling \n- Undersampling \n\n\nThese are some methods that we will use. There's other methods that can be used.\n\n\n* [Importing Packages & EDA](#1)\n* [First Model](#2)\n* [ROC AUC SCORE](#3)\n* [Oversampling our Data](#4)\n* [Undersampling our Data](#5)\n* [Mix Oversampling with Undersampling](#6)\n* [Comparing our Scores](#7)","ce91f95f":"Our best score was 87.54% not bad, however if we predict everyone died, we have the same score 87.54% so our model is not that good.","0a009fe3":"The accuracy has decreased but we increase our roc auc score.","5800bad3":"## Oversampling our Data <a id=\"4\"><\/a>\n\n### What is Oversampling?\nWhen one class of data is the underrepresented minority class in the data sample, over sampling techniques maybe used to duplicate these results for a more balanced amount of positive results in training. Over sampling is used when the amount of data collected is insufficient. A popular over sampling technique is SMOTE (Synthetic Minority Over-sampling Technique), which creates synthetic samples by randomly sampling the characteristics from occurrences in the minority class.\n[Continue reading...](https:\/\/whatis.techtarget.com\/definition\/over-sampling-and-under-sampling)\n\nSo for work with Oversampling we will import a package name imbalanced is based in sklearn for work with imbalanced dataset.<br>\nRead their doc:<br>\nhttps:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/","cd208ed5":"As we can see, the numbers of people that died is very high.","36a1566e":"## First Model <a id=\"2\"><\/a>\n\nWe will test our data with differents models","a7a7c587":"## Mix Oversampling with Undersampling <a id=\"6\"><\/a>\nWe can mix our imbalanced methods.","d618920d":"## Importing Packages & EDA <a id=\"1\"><\/a>","739cb13f":"16 Countrys is a large number for countrys that don't give to much information, we will reduce to 3 countrys only.","328f79f8":"We got a better result, but undersampling reduced our data too much, maybe our results are more luck than a good model.","2baeb16a":"## Undersampling our Data <a id= \"5\"><\/a>\n\n### What is undersampling?\n\nUndersampling is the opposite of oversampling is simple as that, we will decrease the target with more values, it's not recommended for small dataset like ours, but we will try.","7e38a4f0":"## ROC AUC SCORE <a id=\"3\"><\/a>\n\n### What is AUC - ROC Curve?\nAUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s.\n[Continue reading...](https:\/\/towardsdatascience.com\/understanding-auc-roc-curve-68b2303cc9c5)"}}