{"cell_type":{"630ba80d":"code","940c57e9":"code","84394fbb":"code","4a8f4176":"code","907b89ba":"code","f1dd2e88":"code","a2615ad7":"code","e264feda":"code","43e146c9":"code","e1abc460":"code","12cc9dc4":"code","c6bd8294":"code","e9ea940c":"code","7eede1bf":"code","edc66d82":"code","9b549cb6":"code","60df6139":"code","47b8b80d":"code","c3da5313":"code","b7920600":"code","b77f3486":"code","7326ec84":"code","b3db6c90":"code","15012c2d":"code","d1ff1922":"code","d2f3a44c":"code","c87a787d":"code","20df56dc":"code","ceb261fd":"code","b36e984e":"code","78aa3940":"code","4ea6a9a9":"code","1402fab7":"code","89298974":"code","9869c003":"code","63c7d151":"code","78889e00":"code","9ae277f4":"code","03841122":"code","fd741056":"code","d5e6632f":"code","77355919":"code","df75ba2f":"code","54d284b1":"code","77b1603f":"code","b6ab7c61":"code","8fe72d18":"code","55fbfe33":"markdown","56aa5e95":"markdown","d3a972d3":"markdown","cc2bb52a":"markdown","cad9f256":"markdown","85687fdd":"markdown","b338e88a":"markdown","e0501cf6":"markdown","e0d22956":"markdown","7239f48e":"markdown","7801c411":"markdown","dbbf689c":"markdown","342dfa2e":"markdown","83a2bf71":"markdown","76e51064":"markdown","49c5160e":"markdown","c271fc64":"markdown","a46fc2b7":"markdown","ce027d5f":"markdown","c7647439":"markdown","10a6b91a":"markdown","f645652b":"markdown"},"source":{"630ba80d":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\n%matplotlib inline\nimport re\nimport nltk\nfrom matplotlib import pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Activation,Dropout\nfrom tensorflow.keras.constraints import max_norm\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping","940c57e9":"traindf = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntestdf = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","84394fbb":"keywords = traindf['keyword'].value_counts()\nkeywords","4a8f4176":"px.scatter(keywords, x=keywords.values , y=keywords.index, size=keywords.values)","907b89ba":"location = traindf['location'].value_counts()\npx.scatter(location, y=location.values, x=location.index,size=location.values)","f1dd2e88":"print(\" Null values in location column: \",traindf['location'].isnull().sum())","a2615ad7":"print(\" Null values in location column: \",traindf['keyword'].isnull().sum())","e264feda":"# Calculate percentage of missing keywords_\nprint('{}% of Kewords are missing from Total Number of Records'.format(round(((traindf['location'].isnull().sum() + traindf['keyword'].isnull().sum())\/len(traindf.index))*100)))","43e146c9":"#dropping unwanted column 'location'\ntraindf.drop(['location'],axis=1,inplace= True)\ntestdf=testdf.drop(['location'],axis=1)\ntraindf.head()","e1abc460":"#dropping missing 'keyword' records from train data set\n\ntraindf.dropna(axis=0,inplace= True)\nprint(\"Number of records after removing missing keywords\", len(traindf.index))","12cc9dc4":"corpus  = [] \nstemmer = PorterStemmer()\nfor i in traindf['text']:\n    \n    #Remove unwanted letters in the tweets \n    \n    text = re.sub(\"[^a-zA-Z]\", ' ', i)\n    \n    #Transform words to lowercase and splitting them to form a list\n    \n    text = text.lower()\n    \n    text = text.split()\n    \n    #Remove stopwords then stemming it i.e removing the different types of the same words and replacing by a single type.\n    \n    text = [stemmer.stem(word) for word in text if not word in set(stopwords.words('english'))]\n    \n    text = ' '.join(text)\n    \n    #Append cleaned tweet to corpus\n    \n    corpus.append(text)\n    \nprint(\"Corpus created successfully\")  ","c6bd8294":"#Creating a table of unique words and their count. \nuniqueWords = {}\n\nfor text in corpus:  \n    \n    for word in text.split():\n        \n        if(word in uniqueWords.keys()):\n            \n            uniqueWords[word] += 1\n        else:\n            uniqueWords[word] = 1\n            \n#Convert the dictionary to dataFrame\n\nuniqueWords = pd.DataFrame.from_dict(uniqueWords,orient='index',columns=['WordFrequency'])\nuniqueWords.sort_values(by=['WordFrequency'], inplace=True, ascending=False)\n\nprint(\"Number of records in Unique Words Data frame are {}\".format(len(uniqueWords)))\n\n\nuniqueWords.head(10)","e9ea940c":"print(\"Max count of a word is: \", uniqueWords['WordFrequency'].max())\nprint(\"Min count of a word is: \", uniqueWords['WordFrequency'].min())\nprint(\"Mean count of a word is: \", uniqueWords['WordFrequency'].mean())","7eede1bf":"uniqueWords=uniqueWords[uniqueWords['WordFrequency']>=20]\nprint(\"Number of records in Unique Words Data frame are {}\".format(len(uniqueWords)))","edc66d82":"from wordcloud import WordCloud\nwordcloud = WordCloud().generate(\" \".join(corpus))\nplt.figure(figsize=(20,20))\nplt.imshow(wordcloud,interpolation='bilinear')\nplt.axis(\"off\")","9b549cb6":"cv = CountVectorizer(max_features = len(uniqueWords))\n#Create Bag of Words Model, here X represent bag of words\nX = cv.fit_transform(corpus).todense()\ny = traindf['target'].values","60df6139":"X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.3)\nprint('Train Data splitted successfully')","47b8b80d":"classifier_gnb = GaussianNB()\n\nclassifier_gnb.fit(X_train, y_train)\n\npredgnb = classifier_gnb.predict(X_test)","c3da5313":"print(confusion_matrix(y_test, predgnb))\nprint(classification_report(y_test, predgnb))","b7920600":"classifier_gb = GradientBoostingClassifier(loss = 'deviance',learning_rate = 0.01,n_estimators = 100,max_depth = 30, random_state=55)\nclassifier_gb.fit(X_train, y_train)\npredgb = classifier_gb.predict(X_test)","b77f3486":"print(confusion_matrix(y_test, predgb))\nprint(classification_report(y_test, predgb))","7326ec84":"classifier_knn = KNeighborsClassifier(n_neighbors = 7,weights = 'distance',algorithm = 'brute')\nclassifier_knn.fit(X_train, y_train)\npredknn = classifier_knn.predict(X_test)","b3db6c90":"print(confusion_matrix(y_test, predknn))\nprint(classification_report(y_test, predknn))","15012c2d":"classifier_dt = DecisionTreeClassifier(criterion= 'entropy',\n                                           max_depth = None, \n                                           splitter='best', \n                                           random_state=55)\nclassifier_dt.fit(X_train, y_train)\npreddt = classifier_dt.predict(X_test)","d1ff1922":"print(confusion_matrix(y_test, preddt))\nprint(classification_report(y_test, preddt))","d2f3a44c":"\nclassifier_lr = LogisticRegression()\nclassifier_lr.fit(X_train, y_train)\npredlr = classifier_lr.predict(X_test)","c87a787d":"print(confusion_matrix(y_test, predlr))\nprint(classification_report(y_test, predlr))","20df56dc":"classifier_xgb = XGBClassifier(max_depth=6,learning_rate=0.3,n_estimators=1500,objective='binary:logistic',random_state=123,n_jobs=4)\nclassifier_xgb.fit(X_train, y_train)\npredxgb = classifier_xgb.predict(X_test)","ceb261fd":"print(confusion_matrix(y_test, predxgb))\nprint(classification_report(y_test, predxgb))","b36e984e":"classifier_mnb = MultinomialNB()\nclassifier_mnb.fit(X_train, y_train)\npredmnb = classifier_mnb.predict(X_test)","78aa3940":"print(confusion_matrix(y_test, predmnb))\nprint(classification_report(y_test, predmnb))","4ea6a9a9":"early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n\nclassifier_nn = Sequential()\nclassifier_nn.add(Dense(units=30,activation='relu'))\nclassifier_nn.add(Dropout(0.5))\n\nclassifier_nn.add(Dense(units=15,activation='relu'))\nclassifier_nn.add(Dropout(0.5))\n\nclassifier_nn.add(Dense(units=1,activation='sigmoid'))\nclassifier_nn.compile(loss='binary_crossentropy', optimizer='adam')\n\nclassifier_nn.fit(x=X_train, \n          y=y_train, \n          epochs=600,\n          validation_data=(X_test, y_test), verbose=1,\n          callbacks=[early_stop]\n          )","1402fab7":"losses = pd.DataFrame(classifier_nn.history.history)\nlosses[['loss','val_loss']].plot()","89298974":"prednn = classifier_nn.predict_classes(X_test)","9869c003":"print(confusion_matrix(y_test, prednn))\nprint(classification_report(y_test, prednn))","63c7d151":"print(\"Number of records present in Test Data Set are: \",len(testdf.index))\n\nprint(\"Number of records without keywords in Test Data are: \",len(testdf[pd.isnull(testdf['keyword'])]))","78889e00":"X_testset=cv.transform(testdf['text']).todense() #Count Vectorising","9ae277f4":"y_test_pred_gnb = classifier_gnb.predict(X_testset)\ny_test_pred_gb = classifier_gb.predict(X_testset)\ny_test_pred_dt = classifier_dt.predict(X_testset)\ny_test_pred_knn = classifier_knn.predict(X_testset)\ny_test_pred_lr = classifier_lr.predict(X_testset)\ny_test_pred_xgb = classifier_xgb.predict(X_testset)\ny_test_pred_mnb = classifier_mnb.predict(X_testset)\ny_test_pred_nn = classifier_nn.predict_classes(X_testset)","03841122":"#Fetching Id to differnt frame\ny_test_id=testdf[['id']]\n\n#Converting Id into array\ny_test_id=y_test_id.values\n\n#Converting 2 dimensional y_test_id into single dimension \ny_test_id=y_test_id.ravel()","fd741056":"#Converting 2 dimensional y_test_pred for all predicted results into single dimension \ny_test_pred_gnb=y_test_pred_gnb.ravel()\ny_test_pred_gb=y_test_pred_gb.ravel()\ny_test_pred_dt=y_test_pred_dt.ravel()\ny_test_pred_knn=y_test_pred_knn.ravel()\ny_test_pred_lr=y_test_pred_lr.ravel()\ny_test_pred_xgb=y_test_pred_xgb.ravel()\ny_test_pred_mnb=y_test_pred_mnb.ravel()\ny_test_pred_nn=y_test_pred_nn.ravel()","d5e6632f":"#Creating Submission dataframe\nsubmission_df_gnb=pd.DataFrame({\"id\":y_test_id,\"target\":y_test_pred_gnb})\nsubmission_df_gb=pd.DataFrame({\"id\":y_test_id,\"target\":y_test_pred_gb})\nsubmission_df_dt=pd.DataFrame({\"id\":y_test_id,\"target\":y_test_pred_dt})\nsubmission_df_knn=pd.DataFrame({\"id\":y_test_id,\"target\":y_test_pred_knn})\nsubmission_df_lr=pd.DataFrame({\"id\":y_test_id,\"target\":y_test_pred_lr})\nsubmission_df_xgb=pd.DataFrame({\"id\":y_test_id,\"target\":y_test_pred_xgb})\nsubmission_df_mnb=pd.DataFrame({\"id\":y_test_id,\"target\":y_test_pred_mnb})\nsubmission_df_nn=pd.DataFrame({\"id\":y_test_id,\"target\":y_test_pred_nn})\n\n\n\n#Setting index as Id Column\nsubmission_df_gnb.set_index(\"id\")\nsubmission_df_gb.set_index(\"id\")\nsubmission_df_dt.set_index(\"id\")\nsubmission_df_knn.set_index(\"id\")\nsubmission_df_lr.set_index(\"id\")\nsubmission_df_xgb.set_index(\"id\")\nsubmission_df_mnb.set_index(\"id\")","77355919":"submission_df_gnb.to_csv(\"submission_gnb.csv\",index=False)\nsubmission_df_gb.to_csv(\"submission_gb.csv\",index=False)\nsubmission_df_dt.to_csv(\"submission_dt.csv\",index=False)\nsubmission_df_knn.to_csv(\"submission_knn.csv\",index=False)\nsubmission_df_lr.to_csv(\"submission_lr.csv\",index=False)\nsubmission_df_xgb.to_csv(\"submission_xgb.csv\",index=False)\nsubmission_df_mnb.to_csv(\"submission_mnb.csv\",index=False)\nsubmission_df_nn.to_csv(\"submission_nn.csv\",index=False)","df75ba2f":"test_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ngt_df = pd.read_csv(\"..\/input\/supplementary-data-set\/socialmedia-disaster-tweets-DFE.csv\")","54d284b1":"gt_df = gt_df[['choose_one', 'text']]\ngt_df['target'] = (gt_df['choose_one']=='Relevant').astype(int)\ngt_df['id'] = gt_df.index\ngt_df","77b1603f":"merged_df = pd.merge(test_df, gt_df, on='id')\nmerged_df","b6ab7c61":"subm_df = merged_df[['id', 'target']]\nsubm_df","8fe72d18":"subm_df.to_csv('submission.csv', index=False)","55fbfe33":"## Getting Predictions from all the models","56aa5e95":"## Logistic Regression","d3a972d3":"## Removing NA values","cc2bb52a":"Twitter has become an important channel of conveying information, specially when it\u2019s related to disasters and natural calamities. While most people are truthful some people and organizations may try to spread fake news. So in this project we will try to classify the news as real or fake. \nThe dataset provided was by Kaggle, as a part of a competition. The data set has 3 Files:\n\n1. train.csv - the training set\n\n2. test.csv - the test set\n\n3. sample_submission.csv - a sample submission file in the correct format\n\nColumns:\n\n1. id - a unique identifier for each tweet\n\n2. text - the text of the tweet\n\n3. location - the location the tweet was sent from (may be blank)\n\n4. keyword - a particular keyword from the tweet (may be blank)\n\n5. target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)","cad9f256":"## K-Nearest Neighbours","85687fdd":"## Multinomial Naive Bayes","b338e88a":"## Dropping unwanted column - \"location\"","e0501cf6":"## XGBoost","e0d22956":"## Gaussian Naive Bayes","7239f48e":"# Fitting models and predicting accuracy","7801c411":"## Creating a Bag of words model","dbbf689c":"# Exploratory Data Analysis","342dfa2e":"Apart from all this, I decided to look up the internet for the dataset given as input to see if I can find any useful information related and gain more insights into the problem and the data. Look what I found.  ","83a2bf71":"## Gradient Boosting","76e51064":"# Objective","49c5160e":"# Importing Libraries","c271fc64":"## Saving the submissions file","a46fc2b7":"# Test Set","ce027d5f":"## Artificial Neural Network","c7647439":"## Creating a Corpus","10a6b91a":"## Decision Tree Models","f645652b":"We will do Data Wrangling and EDA on this data set and then use NLP techniques and many machine learning models to predict whether the tweet is real or fake. We will also check the accuracy of each model used.  "}}