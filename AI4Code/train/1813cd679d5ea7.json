{"cell_type":{"c3a135fb":"code","3f848e47":"code","d2e9224b":"code","c234ad95":"code","a27c789c":"code","7ba2f1dd":"code","051c02de":"code","9c6d7ca3":"code","5b8f4bdd":"code","1159b515":"code","914f926c":"code","dcedc5d7":"code","eee9384e":"code","687d8ef8":"code","5bc186bb":"code","1060ac93":"code","d7a0fa2c":"code","3ea29334":"code","12b21a69":"code","585a0ef9":"code","0f035ae8":"code","7ccee7e9":"code","db092b2c":"code","844eb856":"code","dc4b3928":"code","d129b46c":"code","edec0d9e":"code","8330ec58":"code","bea7716b":"code","8312ec4f":"code","b101fddc":"code","9fe68986":"code","1f457f90":"code","6627e72f":"code","d4efe84a":"code","004ac29c":"code","928c9411":"code","f4246487":"code","4462fad4":"code","eba0627f":"code","674f7f7e":"code","c6605004":"code","94e6bfdc":"code","25814200":"code","7bc92fb4":"code","1cc9ae8e":"code","f4b8af9d":"code","2758cf08":"code","13c8917b":"markdown","7837ce18":"markdown","b6890401":"markdown","78e859a0":"markdown","9be1f983":"markdown","e18b881e":"markdown","37bcf060":"markdown","f13b7552":"markdown","01ef7900":"markdown","ff1c23e5":"markdown","926c3a44":"markdown","44654bcf":"markdown","2f124d18":"markdown","ed6d90c2":"markdown","84c44556":"markdown","ba6e6b87":"markdown","6ef8497f":"markdown","26cd84bf":"markdown"},"source":{"c3a135fb":"import gc\nimport os\nimport json\nimport wandb\nimport torch\nimport shutil\nimport random\nimport operator\nimport numpy as np\nimport pandas as pd\n\nfrom torch import nn\nfrom scipy import stats\nfrom pathlib import Path\nfrom itertools import chain\nfrom sklearn.pipeline import make_pipeline\nfrom torch.optim.lr_scheduler import CyclicLR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.metrics import accuracy_score, mean_squared_error\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom transformers import AdamW, AutoModel, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup\n\npd.options.display.max_colwidth = None","3f848e47":"class UnoTextDataset(Dataset):\n    def __init__(self, text_excerpts, targets):\n        self.text_excerpts = text_excerpts\n        self.targets = targets\n    \n    def __len__(self):\n        return len(self.text_excerpts)\n    \n    def __getitem__(self, idx):\n        sample = {'text_excerpt': self.text_excerpts[idx],\n                  'target': self.targets[idx]}\n        return sample","d2e9224b":"class DuoTextDataset(Dataset):\n    \"\"\"\n    If the first input is easier to read then the target is 1\n    If the second input is easier to read then the target is -1\n    \"\"\"\n    def __init__(self, text_excerpts_left, text_excerpts_right, targets):\n        self.text_excerpts_left = text_excerpts_left\n        self.text_excerpts_right = text_excerpts_right\n        self.targets = targets\n        \n    def __len__(self):\n        return len(self.targets)\n    \n    def __getitem__(self, idx):\n        sample = {'text_excerpt_left': self.text_excerpts_left[idx],\n                  'text_excerpt_right': self.text_excerpts_right[idx],\n                  'target': self.targets[idx]}\n        return sample","c234ad95":"def get_category_weighted_sampler(categories):\n    category_frequency = Counter(categories)\n    weights = [1\/category_frequency.get(category) for category in categories]\n    category_weighted_sampler = WeightedRandomSampler(weights=weights, num_samples=len(categories), replacement=True)\n    return category_weighted_sampler","a27c789c":"def create_uno_text_dataloader(data, batch_size, shuffle, sampler, apply_preprocessing=True, num_workers=4, pin_memory=True, drop_last=False):\n    # Preprocessing\n    if apply_preprocessing:\n        data['excerpt'] = data['excerpt'].apply(lambda x: x.replace('\\n', ' '))\n        data['excerpt'] = data['excerpt'].apply(lambda x: ' '.join(x.split()))\n    \n    text_excerpts = data['excerpt'].tolist()\n    targets = data['target'].to_numpy().astype(np.float32).reshape(-1, 1)\n    dataset = UnoTextDataset(text_excerpts=text_excerpts, targets=targets)\n    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, sampler=sampler,\n                            num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last)\n    return dataloader","7ba2f1dd":"def create_duo_text_dataloader(data, batch_size, shuffle, sampler, apply_preprocessing=True, num_workers=4, pin_memory=True, drop_last=False):\n    if apply_preprocessing:\n        data['easy_text'] = data['easy_text'].apply(lambda x: x.replace('\\n', ' '))\n        data['easy_text'] = data['easy_text'].apply(lambda x: ' '.join(x.split()))\n\n        data['difficult_text'] = data['difficult_text'].apply(lambda x: x.replace('\\n', ' '))\n        data['difficult_text'] = data['difficult_text'].apply(lambda x: ' '.join(x.split()))\n    \n    text_excerpts_left = data['easy_text'].tolist()\n    text_excerpts_right = data['difficult_text'].tolist()\n    targets = len(data) * [1]\n    targets = np.asarray(targets).astype(np.float32).reshape(-1, 1)\n\n    dataset = DuoTextDataset(text_excerpts_left=text_excerpts_left,\n                             text_excerpts_right=text_excerpts_right,\n                             targets=targets)\n    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, sampler=sampler,\n                            num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last)\n    return dataloader","051c02de":"class AttentionHead(nn.Module):\n    def __init__(self, input_dim, head_hidden_dim):\n        super(AttentionHead, self).__init__()\n        head_hidden_dim = input_dim if head_hidden_dim is None else head_hidden_dim\n        self.W = nn.Linear(input_dim, head_hidden_dim)\n        self.V = nn.Linear(head_hidden_dim, 1)\n        \n    def forward(self, x):\n        attention_scores = self.V(torch.tanh(self.W(x)))\n        attention_scores = torch.softmax(attention_scores, dim=1)\n        attentive_x = attention_scores * x\n        attentive_x = attentive_x.sum(axis=1)\n        return attentive_x","9c6d7ca3":"class MaskFilledAttentionHead(nn.Module):\n    def __init__(self, input_dim, head_hidden_dim):\n        super(MaskFilledAttentionHead, self).__init__()\n        head_hidden_dim = input_dim if head_hidden_dim is None else head_hidden_dim\n        self.W = nn.Linear(input_dim, head_hidden_dim)\n        self.V = nn.Linear(head_hidden_dim, 1)\n        \n    def forward(self, x, attention_mask):\n        attention_scores = self.V(torch.tanh(self.W(x)))\n        attention_scores[attention_mask==0] = -10\n        attention_scores = torch.softmax(attention_scores, dim=1)\n        attentive_x = attention_scores * x\n        attentive_x = attentive_x.sum(axis=1)\n        return attentive_x","5b8f4bdd":"class MaskAddedAttentionHead(nn.Module):\n    def __init__(self, input_dim, head_hidden_dim):\n        super(MaskAddedAttentionHead, self).__init__()\n        head_hidden_dim = input_dim if head_hidden_dim is None else head_hidden_dim\n        self.W = nn.Linear(input_dim, head_hidden_dim)\n        self.V = nn.Linear(head_hidden_dim, 1)\n        \n    def forward(self, x, attention_mask):\n        attention_scores = self.V(torch.tanh(self.W(x)))\n        attention_scores = attention_scores + attention_mask\n        attention_scores = torch.softmax(attention_scores, dim=1)\n        attentive_x = attention_scores * x\n        attentive_x = attentive_x.sum(axis=1)\n        return attentive_x","1159b515":"class RobertaPoolerOutputRegressor(nn.Module):\n    def __init__(self, model_path, dropout_prob=0.1,  roberta_hidden_dropout_prob=0.1, roberta_attention_probs_dropout_prob=0.1, **kwargs):\n        super(RobertaPoolerOutputRegressor, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_path,\n                                                 hidden_dropout_prob=roberta_hidden_dropout_prob,\n                                                 attention_probs_dropout_prob=roberta_attention_probs_dropout_prob, **kwargs)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n        \n    def forward(self, inputs):\n        roberta_outputs = self.roberta(**inputs)\n        pooler_output = roberta_outputs['pooler_output']\n        pooler_output = self.dropout(pooler_output)\n        logits = self.regressor(pooler_output)\n        return logits","914f926c":"class RobertaLastHiddenStateRegressor(nn.Module):\n    def __init__(self, model_path, head_hidden_dim=None, dropout_prob=0.1, roberta_hidden_dropout_prob=0.1, roberta_attention_probs_dropout_prob=0.1, **kwargs):\n        super(RobertaLastHiddenStateRegressor, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_path,\n                                                 hidden_dropout_prob=roberta_hidden_dropout_prob,\n                                                 attention_probs_dropout_prob=roberta_attention_probs_dropout_prob, **kwargs)\n        self.head = AttentionHead(input_dim=self.roberta.config.hidden_size, head_hidden_dim=head_hidden_dim)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n        \n    def forward(self, inputs):\n        roberta_outputs = self.roberta(**inputs)\n        last_hidden_state = roberta_outputs['last_hidden_state']\n        attentive_vector = self.head(last_hidden_state)\n        attentive_vector = self.dropout(attentive_vector)\n        logits = self.regressor(attentive_vector)\n        return logits","dcedc5d7":"class RobertaMaskedLastHiddenStateRegressor(nn.Module):\n    def __init__(self, model_path, head_hidden_dim=None, dropout_prob=0.1, roberta_hidden_dropout_prob=0.1, roberta_attention_probs_dropout_prob=0.1, **kwargs):\n        super(RobertaMaskedLastHiddenStateRegressor, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_path,\n                                                 hidden_dropout_prob=roberta_hidden_dropout_prob,\n                                                 attention_probs_dropout_prob=roberta_attention_probs_dropout_prob, **kwargs)\n        self.head = AttentionHead(input_dim=self.roberta.config.hidden_size, head_hidden_dim=head_hidden_dim)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n        \n    def forward(self, inputs):\n        roberta_outputs = self.roberta(**inputs)\n        last_hidden_state = roberta_outputs['last_hidden_state']\n        masked_last_hidden_state = last_hidden_state * torch.unsqueeze(inputs['attention_mask'], dim=2)\n        attentive_vector = self.head(masked_last_hidden_state)\n        attentive_vector = self.dropout(attentive_vector)\n        logits = self.regressor(attentive_vector)\n        return logits","eee9384e":"class RobertaMaskFilledAttentionHeadRegressor(nn.Module):\n    def __init__(self, model_path, dropout_prob=0.1, head_hidden_dim=None, roberta_hidden_dropout_prob=0.1, roberta_attention_probs_dropout_prob=0.1, **kwargs):\n        super(RobertaMaskFilledAttentionHeadRegressor, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_path,\n                                                 hidden_dropout_prob=roberta_hidden_dropout_prob,\n                                                 attention_probs_dropout_prob=roberta_attention_probs_dropout_prob, **kwargs)\n        self.head = MaskFilledAttentionHead(input_dim=self.roberta.config.hidden_size, head_hidden_dim=head_hidden_dim)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n        \n    def forward(self, inputs):\n        roberta_outputs = self.roberta(**inputs)\n        last_hidden_state = roberta_outputs['last_hidden_state']\n        attentive_vector = self.head(last_hidden_state, torch.unsqueeze(inputs['attention_mask'], dim=2))\n        attentive_vector = self.dropout(attentive_vector)\n        logits = self.regressor(attentive_vector)\n        return logits","687d8ef8":"class RobertaMaskAddedAttentionHeadRegressor(nn.Module):\n    def __init__(self, model_path, dropout_prob=0.1, head_hidden_dim=None, roberta_hidden_dropout_prob=0.1, roberta_attention_probs_dropout_prob=0.1, **kwargs):\n        super(RobertaMaskAddedAttentionHeadRegressor, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_path,\n                                                 hidden_dropout_prob=roberta_hidden_dropout_prob,\n                                                 attention_probs_dropout_prob=roberta_attention_probs_dropout_prob, **kwargs)\n        self.head = MaskAddedAttentionHead(input_dim=self.roberta.config.hidden_size, head_hidden_dim=head_hidden_dim)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n        \n    def forward(self, inputs):\n        roberta_outputs = self.roberta(**inputs)\n        last_hidden_state = roberta_outputs['last_hidden_state']\n        attentive_vector = self.head(last_hidden_state, torch.unsqueeze(inputs['attention_mask'], dim=2))\n        attentive_vector = self.dropout(attentive_vector)\n        logits = self.regressor(attentive_vector)\n        return logits","5bc186bb":"class RobertaBigLinearMaskAddedAttentionHeadRegressor(nn.Module):\n    def __init__(self, model_path, dropout_prob=0.1, head_hidden_dim=None, roberta_hidden_dropout_prob=0.1, roberta_attention_probs_dropout_prob=0.1, **kwargs):\n        super(RobertaBigLinearMaskAddedAttentionHeadRegressor, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_path,\n                                                 hidden_dropout_prob=roberta_hidden_dropout_prob,\n                                                 attention_probs_dropout_prob=roberta_attention_probs_dropout_prob, **kwargs)\n        self.head = MaskAddedAttentionHead(input_dim=self.roberta.config.hidden_size, head_hidden_dim=head_hidden_dim)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.regressor = nn.Sequential(\n            nn.Linear(self.roberta.config.hidden_size, self.roberta.config.hidden_size\/\/2),\n            nn.ReLU(),\n            nn.Linear(self.roberta.config.hidden_size\/\/2, 1), \n        )\n        \n    def forward(self, inputs):\n        roberta_outputs = self.roberta(**inputs)\n        last_hidden_state = roberta_outputs['last_hidden_state']\n        attentive_vector = self.head(last_hidden_state, torch.unsqueeze(inputs['attention_mask'], dim=2))\n        attentive_vector = self.dropout(attentive_vector)\n        logits = self.regressor(attentive_vector)\n        return logits","1060ac93":"class RobertaNHiddenStateRegressor(nn.Module):\n    def __init__(self, model_path, dropout_prob=0.1, head_hidden_dim=None, roberta_hidden_dropout_prob=0.1, roberta_attention_probs_dropout_prob=0.1, **kwargs):\n        super(RobertaNHiddenStateRegressor, self).__init__()\n        self.num_last_hidden_states = kwargs.pop('num_last_hidden_states')\n        self.roberta = AutoModel.from_pretrained(model_path,\n                                                 hidden_dropout_prob=roberta_hidden_dropout_prob,\n                                                 attention_probs_dropout_prob=roberta_attention_probs_dropout_prob, **kwargs)\n        self.head = MaskAddedAttentionHead(input_dim=self.roberta.config.hidden_size * self.num_last_hidden_states, head_hidden_dim=head_hidden_dim)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.regressor = nn.Linear(self.roberta.config.hidden_size *  self.num_last_hidden_states, 1)\n        \n    def forward(self, inputs):\n        roberta_outputs = self.roberta(**inputs, output_hidden_states=True)\n        last_hidden_states = torch.cat((roberta_outputs['hidden_states'][- self.num_last_hidden_states:]), axis=2)\n        attentive_vector = self.head(last_hidden_states, torch.unsqueeze(inputs['attention_mask'], dim=2))\n        attentive_vector = self.dropout(attentive_vector)\n        logits = self.regressor(attentive_vector)\n        return logits","d7a0fa2c":"class RobertaLastHiddenStateMeanPooler(nn.Module):\n    def __init__(self, model_path, dropout_prob=0.1, head_hidden_dim=None,  roberta_hidden_dropout_prob=0.1, roberta_attention_probs_dropout_prob=0.1, **kwargs):\n        super(RobertaLastHiddenStateMeanPooler, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_path,\n                                                 hidden_dropout_prob=roberta_hidden_dropout_prob,\n                                                 attention_probs_dropout_prob=roberta_attention_probs_dropout_prob, **kwargs)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n        \n    def forward(self, inputs):\n        roberta_outputs = self.roberta(**inputs)\n        last_hidden_state = roberta_outputs['last_hidden_state']\n        masked_last_hidden_state = last_hidden_state * torch.unsqueeze(inputs['attention_mask'], dim=2)\n        num_tokens = torch.unsqueeze(inputs['attention_mask'], dim=2)\n        num_tokens = torch.clamp(num_tokens, min=1e-9)\n        mean_embeddings = masked_last_hidden_state.sum(axis=1) \/ num_tokens.sum(axis=1)\n        mean_embeddings = self.dropout(mean_embeddings)\n        logits = self.regressor(mean_embeddings)\n        return logits","3ea29334":"class SquiveldMeanPooling(nn.Module):\n    def __init__(self, model_path, dropout_prob=0.1, head_hidden_dim=None,  roberta_hidden_dropout_prob=0.1, roberta_attention_probs_dropout_prob=0.1, **kwargs):\n        super().__init__()\n        config = AutoConfig.from_pretrained(model_path)\n        self.roberta = AutoModel.from_pretrained(model_path, config=config)\n        self.regressor = nn.Linear(1024, 1)\n        \n    def forward(self, inputs):\n        outputs = self.roberta(**inputs)\n        last_hidden_state = outputs[0]\n        attention_mask = inputs['attention_mask']\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings \/ sum_mask\n        logits = self.regressor(mean_embeddings)\n        return logits","12b21a69":"class BertLastHiddenStateRegressor(nn.Module):\n    def __init__(self, model_path, dropout_prob=0.1, **kwargs):\n        super(BertLastHiddenStateRegressor, self).__init__()\n        self.bert = AutoModel.from_pretrained(model_path, **kwargs)\n        self.head = AttentionHead(self.bert.config.hidden_size)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)\n        \n    def forward(self, inputs):\n        bert_outputs = self.bert(**inputs)\n        last_hidden_state = bert_outputs['last_hidden_state']\n        attentive_vector = self.head(last_hidden_state)\n        attentive_vector = self.dropout(attentive_vector)\n        logits = self.regressor(attentive_vector)\n        return logits","585a0ef9":"class MultiModel(nn.Module):\n    def __init__(self, model_1_path, model_1_class, model_2_path, model_2_class):\n        super(MultiModel, self).__init__()\n        self.examiner_1 = model_1_class(model_path=model_1_path)\n        self.examiner_2 = model_2_class(model_path=model_2_path)\n        self.bradley_terry = nn.Linear(2, 1)\n    \n    def forward(self, inputs_1, inputs_2):\n        logits_1 = self.examiner_1(inputs_1)\n        logits_2 = self.examiner_2(inputs_2)\n        concatenated_outputs = torch.hstack((logits_1, logits_2))\n        bradley_terry_score = self.bradley_terry(concatenated_outputs)\n        return bradley_terry_score","0f035ae8":"def compute_predictions(text_excerpts, tokenizer, model, max_length, device, **kwargs):\n    if max_length is None:\n        # Sequence bucketing\n        inputs = tokenizer(text=text_excerpts, padding=True, truncation=True, return_tensors='pt')\n    else:\n        inputs = tokenizer(text=text_excerpts, padding='max_length', truncation=True, max_length=max_length,  return_tensors='pt')\n    inputs = {key:value.to(device) for key, value in inputs.items()}\n    predictions = model(inputs)\n    return predictions","7ccee7e9":"mse_loss_fn = nn.MSELoss()\n\ndef compute_mse_loss(outputs, targets, device, **kwargs):\n    predictions = outputs['predictions']\n    targets = targets.to(device)\n    loss = mse_loss_fn(predictions, targets)\n    return loss\n\ndef compute_rmse_loss(outputs, targets, device, **kwargs):\n    predictions = outputs['predictions']\n    targets = targets.to(device)\n    loss = torch.sqrt(mse_loss_fn(predictions, targets))\n    return loss\n\ndef compute_weighted_rmse_loss(outputs, targets, device, eps=1e-6, **kwargs):\n    predictions = outputs['predictions']\n    targets = targets.to(device)\n    if 'weights' in kwargs.keys():\n        weights = kwargs['weights']\n    else:\n        weights = torch.ones_like(targets)\n    loss = torch.sqrt(torch.mean(weights * (predictions - targets) ** 2) + eps)\n    return loss","db092b2c":"ranking_loss_fn = nn.MarginRankingLoss()\n\ndef compute_ranking_loss(outputs, targets, device, **kwargs):\n    predictions_left = outputs['predictions_left']\n    predictions_right = outputs['predictions_right']\n    targets = targets.to(device)\n    \n    predictions_left = predictions_left.reshape(len(predictions_left))\n    predictions_right = predictions_right.reshape(len(predictions_right))\n    targets = targets.reshape(len(targets))\n    \n    loss = ranking_loss_fn(predictions_left, predictions_right, targets)\n    return loss\n\ndef compute_hard_ranking_loss(outputs, targets, device, **kwargs):\n    predictions = outputs['predictions']\n    targets = targets.to(device)\n    \n    predictions = predictions.reshape(len(predictions))\n    targets = targets.reshape(len(targets))\n    \n    # Sort targets based on difficulty\n    sorted_targets, indices = torch.sort(targets)\n    shifted_sorted_targets = torch.roll(sorted_targets, -1, 0)\n    \n    # Sort the corresponding predictions\n    sorted_predictions = predictions[indices]\n    shifted_sorted_predictions = torch.roll(sorted_predictions, -1, 0)\n    \n    targets = torch.sign(sorted_targets - shifted_sorted_targets)\n    outputs = {'predictions_left': sorted_predictions, 'predictions_right': shifted_sorted_predictions}\n    loss = compute_ranking_loss(outputs, targets, device)\n    return loss\n\ndef compute_random_pair_ranking_loss(outputs, targets, device, **kwargs):\n    predictions = outputs['predictions']\n    targets = targets.to(device)\n    \n    predictions = predictions.reshape(len(predictions))\n    targets = targets.reshape(len(targets))\n    \n    # Sort targets and predictions randomly\n    indices = torch.randperm(len(targets))\n    sorted_targets = targets[indices]\n    sorted_predictions = predictions[indices]\n    \n    ranking_targets = torch.sign(targets - sorted_targets)\n    outputs = {'predictions_left': predictions, 'predictions_right': sorted_predictions}   \n    loss = compute_ranking_loss(outputs, ranking_targets, device)\n    return loss\n\n\ndef compute_pairwise_ranking_loss(outputs, targets, device, **kwargs):\n    predictions = outputs['predictions']\n    targets = targets.to(device)\n    \n    predictions = predictions.reshape(len(predictions))\n    targets = targets.reshape(len(targets))\n    \n    predictions_combinations = torch.combinations(predictions, r=2)\n    targets_combinations = torch.combinations(targets, r=2) \n       \n    predictions_left  = predictions_combinations[:, 0]\n    predictions_right = predictions_combinations[:, 1]\n    \n    targets_left  =  targets_combinations[:, 0]\n    targets_right = targets_combinations[:, 1]\n\n    targets = torch.sign(targets_left - targets_right)\n    outputs = {'predictions_left': predictions_left, 'predictions_right': predictions_right}   \n    loss = compute_ranking_loss(outputs, targets, device)\n    return loss","844eb856":"from torch import nn\n\nclass LogCoshLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, y_t, y_prime_t):\n        ey_t = y_t - y_prime_t\n        return torch.mean(torch.log(torch.cosh(ey_t + 1e-12)))\n\n\nclass XTanhLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, y_t, y_prime_t):\n        ey_t = y_t - y_prime_t\n        return torch.mean(ey_t * torch.tanh(ey_t))\n\n\nclass XSigmoidLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, y_t, y_prime_t):\n        ey_t = y_t - y_prime_t\n        return torch.mean(2 * ey_t \/ (1 + torch.exp(-ey_t)) - ey_t)","dc4b3928":"log_cosh_loss = LogCoshLoss()\n\ndef compute_log_cosh_loss(outputs, targets, device, **kwargs):\n    predictions = outputs['predictions']\n    targets = targets.to(device)\n    loss = log_cosh_loss(predictions, targets)\n    return loss","d129b46c":"xtanh_loss = XTanhLoss()\n\ndef compute_xtanh_loss(outputs, targets, device, **kwargs):\n    predictions = outputs['predictions']\n    targets = targets.to(device)\n    loss = xtanh_loss(predictions, targets)\n    return loss","edec0d9e":"xsigmoid_loss = XSigmoidLoss()\n\ndef compute_xsigmoid_loss(outputs, targets, device, **kwargs):\n    predictions = outputs['predictions']\n    targets = targets.to(device)\n    loss = xsigmoid_loss(predictions, targets)\n    return loss","8330ec58":"class UnoStacker:\n    def __init__(self):\n        self.predictions = []\n        self.targets = []\n        \n    def update(self, batch_outputs, batch_targets):\n        self.predictions.append(batch_outputs['predictions'])\n        self.targets.append(batch_targets)\n    \n    def get_stack(self):\n        predictions = torch.vstack(self.predictions)\n        targets = torch.vstack(self.targets)\n        predictions = {'predictions': predictions}\n        return predictions, targets","bea7716b":"class DuoStacker:\n    def __init__(self):\n        self.predictions_left = []\n        self.predictions_right = []\n        self.targets = []\n        \n    def update(self, batch_outputs, batch_targets):\n        self.predictions_left.append(batch_outputs['predictions_left'])\n        self.predictions_right.append(batch_outputs['predictions_right'])\n        self.targets.append(batch_targets)\n    \n    def get_stack(self):\n        predictions_left = torch.vstack(self.predictions_left)\n        predictions_right = torch.vstack(self.predictions_right)\n        targets = torch.vstack(self.targets)\n        predictions = {'predictions_left': predictions_left,\n                       'predictions_right': predictions_right}\n        return predictions, targets","8312ec4f":"def compute_rmse_score(outputs, targets, **kwargs):\n    predictions = outputs['predictions']\n    predictions = predictions.detach().cpu().numpy()\n    targets = targets.detach().cpu().numpy()\n    rmse_score = mean_squared_error(targets, predictions, squared=False)\n    return rmse_score","b101fddc":"def compute_rmse_score_from_ranker_predictions(outputs, targets, **kwargs):\n    predictions = outputs['predictions']\n    predictions = predictions.detach().cpu().numpy()\n    targets = targets.detach().cpu().numpy()\n        \n    regressor = Ridge(fit_intercept=True, normalize=False)    \n    predictions = predictions.reshape(len(predictions), 1)\n    targets = targets.reshape(len(targets))\n        \n    scores = cross_val_score(regressor, predictions, targets, cv=kwargs['cv'], scoring='neg_root_mean_squared_error')\n    rmse_score = np.abs(np.mean(scores))\n    return rmse_score","9fe68986":"def compute_correlation_coefficient(outputs, targets, **kwargs):\n    predictions = outputs['predictions']\n    predictions = predictions.detach().cpu().numpy()\n    targets = targets.detach().cpu().numpy()\n    predictions = predictions.reshape(len(predictions), )\n    targets = targets.reshape(len(targets), )\n    correlation_coefficient, _ = stats.pearsonr(predictions, targets)\n    return correlation_coefficient","1f457f90":"def compute_rmse_score(outputs, targets, **kwargs):\n    predictions = outputs['predictions']\n    predictions = predictions.detach().cpu().numpy()\n    targets = targets.detach().cpu().numpy()\n    rmse_score = mean_squared_error(targets, predictions, squared=False)\n    return rmse_score","6627e72f":"def scale_and_fit_regressor(outputs, targets, **kwargs):\n    predictions = outputs['predictions']\n    predictions = predictions.detach().cpu().numpy()\n    targets = targets.detach().cpu().numpy()\n    \n    targets = targets.reshape(len(targets), 1)\n    predictions = predictions.reshape(len(predictions), 1)\n    \n    scaler = kwargs['scaler']\n    \n    targets = scaler.inverse_transform(targets)\n    predictions = scaler.inverse_transform(predictions)\n    targets = targets.reshape(len(targets), )\n    \n    regressor = make_pipeline(StandardScaler(), Ridge(fit_intercept=True, normalize=False))    \n    scores = cross_val_score(regressor, predictions, targets, cv=kwargs['cv'], scoring='neg_root_mean_squared_error')\n    rmse_score = np.abs(np.mean(scores))\n    return rmse_score","d4efe84a":"def compute_ranking_accuracy(outputs, targets, **kwargs):\n    predictions_left = outputs['predictions_left']\n    predictions_right = outputs['predictions_right']\n    \n    predictions_left = predictions_left.reshape(len(predictions_left))\n    predictions_right = predictions_right.reshape(len(predictions_right))\n    targets = targets.reshape(len(targets))\n\n    predictions_left = predictions_left.detach().cpu().numpy()\n    predictions_right = predictions_right.detach().cpu().numpy()\n    targets = targets.detach().cpu().numpy()\n\n    predictions = np.sign(predictions_left - predictions_right)\n    ranking_accuracy = accuracy_score(targets, predictions)\n    return ranking_accuracy","004ac29c":"def forward_pass_uno_text_batch(batch, tokenizer, model, compute_loss_fn, max_length, device, **kwargs):\n    predictions = compute_predictions(text_excerpts=batch['text_excerpt'], tokenizer=tokenizer, model=model, max_length=max_length, device=device, **kwargs)\n    outputs = {'predictions': predictions}\n    loss = compute_loss_fn(outputs=outputs, targets=batch['target'], device=device, **kwargs) if compute_loss_fn is not None else None\n    outputs['loss'] = loss\n    return outputs\n\n\ndef forward_pass_duo_text_batch(batch, tokenizer, model, compute_loss_fn, max_length, device, **kwargs):\n    predictions_left = compute_predictions(text_excerpts=batch['text_excerpt_left'], tokenizer=tokenizer, model=model, max_length=max_length, device=device, **kwargs)\n    predictions_right = compute_predictions(text_excerpts=batch['text_excerpt_right'], tokenizer=tokenizer, model=model, max_length=max_length, device=device, **kwargs)\n    outputs = {'predictions_left': predictions_left,  'predictions_right': predictions_right}\n    loss = compute_loss_fn(outputs=outputs, targets=batch['target'], device=device, **kwargs) if compute_loss_fn is not None else None\n    outputs['loss'] = loss\n    return outputs","928c9411":"def make_cyclic_scheduler(optimizer, **kwargs):\n    base_lr = kwargs['base_lr']\n    step_size_up = kwargs['step_size_up']\n    max_lr = kwargs['max_lr']\n    cycle_momentum = kwargs['cycle_momentum']\n    scheduler = CyclicLR(optimizer=optimizer, base_lr=base_lr, max_lr=max_lr, step_size_up=step_size_up, cycle_momentum=cycle_momentum)\n    return scheduler\n\ndef make_cosine_schedule_with_warmup(optimizer, **kwargs):\n    num_warmup_steps = kwargs['num_warmup_steps']\n    num_training_steps = kwargs['num_training_steps']\n    scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer,\n                                                num_warmup_steps=num_warmup_steps,\n                                                num_training_steps=num_training_steps)\n    return scheduler\n\ndef make_linear_schedule_with_warmup(optimizer, **kwargs):\n    num_warmup_steps = kwargs['num_warmup_steps']\n    num_training_steps = kwargs['num_training_steps']\n    scheduler = get_linear_schedule_with_warmup(optimizer=optimizer,\n                                                num_warmup_steps=num_warmup_steps,\n                                                num_training_steps=num_training_steps)\n    return scheduler\n    \ndef make_cosine_with_hard_restarts_schedule_with_warmup(optimizer, **kwargs):\n    num_warmup_steps = kwargs['num_warmup_steps']\n    num_training_steps = kwargs['num_training_steps']\n    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer=optimizer, \n                                                                   num_warmup_steps=num_warmup_steps,\n                                                                   num_training_steps=num_training_steps,\n                                                                   num_cycles=3)\n    return scheduler\n\ndef make_polynomial_decay_schedule_with_warmup(optimizer, **kwargs):\n    num_warmup_steps = kwargs['num_warmup_steps']\n    num_training_steps = kwargs['num_training_steps']\n    scheduler = get_polynomial_decay_schedule_with_warmup(optimizer=optimizer, \n                                                          num_warmup_steps=num_warmup_steps,\n                                                          num_training_steps=num_training_steps,\n                                                          power=3.0)\n    return scheduler\n    \ndef get_scheduler(scheduler_type, optimizer, **kwargs):\n    if scheduler_type == 'cyclic':\n        scheduler = make_cyclic_scheduler(optimizer, **kwargs)\n    elif scheduler_type == 'cosine_schedule_with_warmup':\n        scheduler = make_cosine_schedule_with_warmup(optimizer, **kwargs)\n    elif scheduler_type == 'get_linear_schedule_with_warmup':\n        scheduler = make_linear_schedule_with_warmup(optimizer, **kwargs)\n    elif scheduler_type == 'cosine_with_hard_restarts_schedule_with_warmup':\n        scheduler = make_cosine_with_hard_restarts_schedule_with_warmup(optimizer, **kwargs)\n    elif scheduler_type == 'polynomial_decay_schedule_with_warmup':\n        scheduler = make_polynomial_decay_schedule_with_warmup(optimizer, **kwargs)\n    else:\n        scheduler = None\n    return scheduler","f4246487":"def split_into_wd_groups(param_group, weight_decay):\n    # Applies weight decay\n    weight_parameters = {'params': [param_group['params'][index] for index, name in enumerate(param_group['param_names']) if 'weight' in name and 'LayerNorm' not in name],\n                         'param_names': [param_group['param_names'][index] for index, name in enumerate(param_group['param_names']) if 'weight' in name and 'LayerNorm' not in name],\n                         'lr': param_group['lr'],\n                         'weight_decay': weight_decay,\n                         'name': param_group['name']+'_weight'}\n    # Does not apply weight decay\n    bias_ln_parameters = {'params': [param_group['params'][index] for index, name in enumerate(param_group['param_names']) if 'bias' in name or 'LayerNorm' in name],\n                          'param_names': [param_group['param_names'][index] for index, name in enumerate(param_group['param_names']) if 'bias' in name or 'LayerNorm' in name],\n                          'lr': param_group['lr'],\n                          'weight_decay': 0.0,\n                          'name': param_group['name']+'_bias_ln'}\n    parameters = [weight_parameters, bias_ln_parameters]\n    return parameters","4462fad4":"def get_optimizer_parameters(group_mode, lr, model, **kwargs):\n    \n    param_optimizer = list(model.named_parameters())\n    non_bert = [(n,p) for n,p in model.named_parameters() if 'roberta' not in n]\n    no_decay = ['bias', 'gamma', 'beta']\n\n    if group_mode == 'i':\n        optimizer_parameters = [{'params': [p for n,p in model.named_parameters() if ('bias' not in n) and ('pooler' not in n)],\n                                 'lr': lr, 'weight_decay':0.01, 'name': 'weights'}, \n                                {'params': [p for n,p in model.named_parameters() if ('bias' in n) and ('pooler' not in n)],\n                                 'lr': lr, 'weight_decay':0.00, 'name': 'bias'}]\n        \n    elif group_mode == 'j':\n        optimizer_parameters = [{'params': [p for n,p in model.named_parameters() if ('bias' not in n)],\n                                 'lr': lr, 'weight_decay':0.01, 'name': 'weights'}, \n                                {'params': [p for n,p in model.named_parameters() if ('bias' in n)],\n                                 'lr': lr, 'weight_decay':0.00, 'name': 'bias'}]\n\n    elif group_mode == 'k':\n        # Finetuning task specific layers\n        optimizer_parameters = [{'params': [p for n,p in non_bert if 'bias' not in n],\n                                 'param_names': [n for n,p in non_bert if 'bias' not in n],\n                                 'lr': lr, 'weight_decay':0.01, 'name': 'non_roberta_weights'}, \n                                {'params': [p for n,p in non_bert if 'bias' in n],\n                                 'param_names': [n for n,p in non_bert if 'bias' in n],\n                                 'lr': lr, 'weight_decay':0.00, 'name': 'non_roberta_bias'}]\n        \n    elif group_mode  == 's':\n        multiplicative_factor = kwargs['multiplicative_factor']\n        optimizer_parameters = [{'params': [p for n,p in model.roberta.named_parameters() if (not any(nd in n for nd in no_decay)) and ('pooler' not in n)],\n                                 'lr': lr, 'weight_decay' : 0.01, 'name': 'roberta_weights'},\n                                {'params': [p for n,p in model.roberta.named_parameters() if (any(nd in n for nd in no_decay))  and ('pooler' not in n)],\n                                 'lr': lr,'weight_decay': 0.0, 'name': 'roberta_bias'},\n                                {'params': [p for n,p in model.named_parameters() if all(nd not in n for nd in ['roberta','bias'])],\n                                 'lr': lr * multiplicative_factor, 'weight_decay':0.01, 'name': 'non_roberta_weights'}, \n                                {'params': [p for n,p in non_bert if 'bias' in n],\n                                 'lr': lr * multiplicative_factor, 'weight_decay':0.00, 'name': 'non_roberta_bias'}]\n    elif group_mode == 'b':\n        multiplicative_factor = kwargs['multiplicative_factor']\n        model_parameters = {name: param for name, param in model.named_parameters()}\n        model_parameters_names = [name for name, param in model.named_parameters()]\n        \n        # LR for task specific layers\n        if kwargs['train_pooler']:\n            task_specific_layer_names = [name for name, param in model.named_parameters() if 'regressor' in name or 'head' in name or 'pooler' in name or 'layer_norm' in name]\n        else:\n            task_specific_layer_names = [name for name, param in model.named_parameters() if 'regressor' in name or 'head' in name or 'layer_norm' in name]\n        task_specific_layer_params = [model_parameters.get(name) for name in task_specific_layer_names]\n        task_specific_optimizer_parameters = [{'params': task_specific_layer_params,\n                                               'param_names': task_specific_layer_names,\n                                               'lr': lr,\n                                               'name': 'task_specific_layers'}]\n        # LR for roberta layers\n        # Freeze embeddings\n        roberta_layer_names = [name for name, param in model.named_parameters() if 'roberta' in name]\n        max_num_layers = model.roberta.config.num_hidden_layers\n        roberta_layers_groups = {layer_num: {'params': [],\n                                             'param_names': [],\n                                             'lr': lr * multiplicative_factor ** (max_num_layers - layer_num),\n                                             'name': f'layer_{layer_num}'} for layer_num in range(max_num_layers)}\n        for layer_num in range(max_num_layers):\n            for layer_name in roberta_layer_names:\n                if f'layer.{layer_num}.' in layer_name:\n                    roberta_layers_groups[layer_num]['param_names'].append(layer_name)\n                    roberta_layers_groups[layer_num]['params'].append(model_parameters.get(layer_name))\n        roberta_layers_optimizer_parameters = list(roberta_layers_groups.values())\n        # Combine task specific layers and roberta layers\n        optimizer_parameters = roberta_layers_optimizer_parameters + task_specific_optimizer_parameters\n\n    elif group_mode == 'b_wd':\n        multiplicative_factor = kwargs['multiplicative_factor']\n        model_parameters = {name: param for name, param in model.named_parameters()}\n        model_parameters_names = [name for name, param in model.named_parameters()]\n        \n        # LR for task specific layers\n        if kwargs['train_pooler']:\n            task_specific_layer_names = [name for name, param in model.named_parameters() if 'regressor' in name or 'head' in name or 'pooler' in name or 'layer_norm' in name]\n        else:\n            task_specific_layer_names = [name for name, param in model.named_parameters() if 'regressor' in name or 'head' in name or 'layer_norm' in name]\n        task_specific_layer_params = [model_parameters.get(name) for name in task_specific_layer_names]\n        task_specific_optimizer_parameters = [{'params': task_specific_layer_params,\n                                               'param_names': task_specific_layer_names,\n                                               'lr': lr,\n                                               'name': 'task_specific_layers'}]\n        # LR for roberta layers\n        # Freeze embeddings\n        roberta_layer_names = [name for name, param in model.named_parameters() if 'roberta' in name]\n        max_num_layers = model.roberta.config.num_hidden_layers\n        roberta_layers_groups = {layer_num: {'params': [],\n                                             'param_names': [],\n                                             'lr': lr * multiplicative_factor ** (max_num_layers - layer_num),\n                                             'name': f'layer_{layer_num}'} for layer_num in range(max_num_layers)}\n        for layer_num in range(max_num_layers):\n            for layer_name in roberta_layer_names:\n                if f'layer.{layer_num}.' in layer_name:\n                    roberta_layers_groups[layer_num]['param_names'].append(layer_name)\n                    roberta_layers_groups[layer_num]['params'].append(model_parameters.get(layer_name))\n        roberta_layers_optimizer_parameters = list(roberta_layers_groups.values())\n        # Combine task specific layers and roberta layers\n        optimizer_parameters_without_wd = roberta_layers_optimizer_parameters + task_specific_optimizer_parameters\n\n        # Assign weight decay\n        weight_decay = kwargs['weight_decay']\n        optimizer_parameters = []\n        for layer_parameters in optimizer_parameters_without_wd:\n            weight_parameters = {'params': [], 'param_names': [], 'lr': layer_parameters['lr'], 'name': layer_parameters['name']+'_weights', 'weight_decay': weight_decay}\n            bias_parameters = {'params': [], 'param_names': [], 'lr': layer_parameters['lr'], 'name': layer_parameters['name']+'_bias', 'weight_decay': 0.0}\n            layer_norm_parameters = {'params': [], 'param_names': [], 'lr': layer_parameters['lr'], 'name': layer_parameters['name']+'_layer_norm', 'weight_decay': 0.0}\n            for param, param_name in zip(layer_parameters['params'], layer_parameters['param_names']):\n                if 'LayerNorm' in param_name:\n                    layer_norm_parameters['params'].append(param)\n                    layer_norm_parameters['param_names'].append(param_name)\n                elif 'bias' in param_name:\n                    bias_parameters['params'].append(param)\n                    bias_parameters['param_names'].append(param_name)\n                else:\n                    weight_parameters['params'].append(param)\n                    weight_parameters['param_names'].append(param_name)\n            optimizer_parameters.append(weight_parameters)\n            optimizer_parameters.append(bias_parameters)\n            optimizer_parameters.append(layer_norm_parameters)\n            \n    elif group_mode == 'be_wd':\n        multiplicative_factor = kwargs['multiplicative_factor']\n        weight_decay = kwargs['weight_decay']\n        max_num_layers = model.roberta.config.num_hidden_layers\n\n        # Task Specific Layer group\n        tsl_param_group = [{'params': [param for name, param in model.named_parameters() if 'roberta' not in name],\n                            'param_names': [name for name, param in model.named_parameters() if 'roberta' not in name],\n                            'lr': lr,\n                            'name': 'tsl'}]\n\n        # Roberta Layer group\n        roberta_layers_param_groups = []\n        for layer_num in reversed(range(max_num_layers)):\n            roberta_layer_param_groups = {'params': [param for name, param in model.named_parameters() if f'roberta.encoder.layer.{layer_num}.' in name],\n                                          'param_names': [name for name, param in model.named_parameters() if f'roberta.encoder.layer.{layer_num}.' in name],\n                                          'lr': lr * (multiplicative_factor ** (max_num_layers - layer_num)),\n                                          'name': f'layer_{layer_num}'}\n            roberta_layers_param_groups.append(roberta_layer_param_groups)\n\n        # Embeddding group\n        embedding_lr = lr * (multiplicative_factor ** (max_num_layers + 1))\n        embedding_param_group = [{'params': [param for name, param in model.named_parameters() if 'embedding' in name],\n                                  'param_names': [name for name, param in model.named_parameters() if 'embedding' in name],\n                                  'lr': embedding_lr,\n                                  'name': 'embedding'}]\n\n        param_groups = tsl_param_group + roberta_layers_param_groups + embedding_param_group\n        optimizer_parameters = list(chain(*[split_into_wd_groups(param_group, weight_decay=weight_decay) for param_group in param_groups]))\n\n    elif group_mode == 'a':\n        group1 = ['layer.0.','layer.1.','layer.2.','layer.3.']\n        group2 = ['layer.4.','layer.5.','layer.6.','layer.7.']    \n        group3 = ['layer.8.','layer.9.','layer.10.','layer.11.']\n        group_all = group1 + group2 + group3\n        optimizer_parameters = [\n            {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay_rate': 0.01, 'lr': lr\/2.6, 'name': 'roberta_group_1_weights'},\n            {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay_rate': 0.01, 'lr': lr, 'name': 'roberta_group_2_weights'},\n            {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay_rate': 0.01, 'lr': lr*2.6, 'name': 'roberta_group_3_weights'},\n            {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay_rate': 0.0, 'lr': lr\/2.6, 'name': 'roberta_group_1_bias'},\n            {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay_rate': 0.0, 'lr': lr, 'name': 'roberta_group_2_bias'},\n            {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay_rate': 0.0, 'lr': lr*2.6, 'name': 'roberta_group_3_bias'},\n                \n            {'params': [p for n, p in non_bert if  'bias' not in n and 'head' in n], 'lr':lr*10, \"momentum\" : 0.99,'weight_decay_rate':0.01, 'name': 'head_weights'},\n            {'params': [p for n, p in non_bert if  'bias' in n and 'head' in n], 'lr':lr*10, \"momentum\" : 0.99,'weight_decay_rate':0.0, 'name': 'head_bias'},\n\n            {'params': [p for n, p in non_bert if  'bias' not in n and 'regressor' in n], 'lr':lr*10, \"momentum\" : 0.99,'weight_decay_rate':0.01, 'name': 'regressor_weights'},\n            {'params': [p for n, p in non_bert if \"bias\"  in n and 'regressor' in n], 'lr':lr*10, \"momentum\" : 0.99,'weight_decay_rate':0.00, 'name': 'regressor_bias'},\n        ]\n    return optimizer_parameters","eba0627f":"class Saver:\n    def __init__(self, metric_name, is_lower_better, config, save_name, should_save=True):\n        self.metric_name = metric_name\n        self.save_path = Path(f'{save_name}\/{metric_name}')\n        self.operator_function = operator.le if is_lower_better else operator.ge\n        self.best_score = np.inf if is_lower_better else 0\n        self.best_iteration_num = 0\n        self.config = config\n        self.save_config()\n        self.should_save = should_save\n    \n    def update(self, current_iteration_num, current_score, model, tokenizer):\n        if self.operator_function(current_score, self.best_score):\n            self.best_score = current_score\n            self.best_iteration_num = current_iteration_num\n            if self.should_save:\n                self.save(model, tokenizer)\n            print(f'{self.metric_name} attained best score: {current_score:.3f}. Saving the model')\n            \n    def save_config(self):\n        shutil.rmtree(self.save_path, ignore_errors=True)\n        os.makedirs(self.save_path)\n        with open(self.save_path \/ 'config.json', 'w') as fp:\n            json.dump(self.config, fp, sort_keys=True, indent=4)\n    \n    def save(self, model, tokenizer):\n        shutil.rmtree(self.save_path, ignore_errors=True)\n        os.makedirs(self.save_path)\n        torch.save(model.state_dict(), self.save_path \/ 'model.pth')\n        tokenizer.save_pretrained(self.save_path)\n        \n    def get_best_score(self):\n        return {'best_score': self.best_score, 'best_iteration_num': self.best_iteration_num}","674f7f7e":"def train_one_batch(iteration_num, batch, tokenizer, model, optimizer, scheduler, forward_pass_fn, compute_loss_fn, max_length, accumulation_steps, device, **kwargs):\n    model.train()\n    if iteration_num == 0:\n        optimizer.zero_grad()\n    batch_loss = forward_pass_fn(batch=batch, tokenizer=tokenizer, model=model, \n                                 compute_loss_fn=compute_loss_fn, max_length=max_length, \n                                 device=device, **kwargs)['loss']                                            # Forward pass\n    batch_loss = batch_loss \/ accumulation_steps                                                   # Normalize our loss (if averaged)\n    batch_loss.backward()                                                                          # Backward pass\n    if (iteration_num + 1) % accumulation_steps == 0:                                              # Wait for several backward steps\n        optimizer.step()                                                                           # Now we can do an optimizer step\n        optimizer.zero_grad()\n        if scheduler is not None:\n            scheduler.step()\n    return model, batch_loss * accumulation_steps","c6605004":"def evaluate(dataloader, tokenizer, model, forward_pass_fn, compute_loss_fn, compute_metric_fn, stacker_class, max_length, device, **kwargs):\n    epoch_loss = 0\n    model.eval()\n    stacker = stacker_class()\n    with torch.no_grad():\n        for batch_num, batch in enumerate(dataloader):\n            batch_outputs = forward_pass_fn(batch=batch, tokenizer=tokenizer, model=model, \n                                            compute_loss_fn=compute_loss_fn, max_length=max_length, \n                                            device=device, **kwargs)\n            batch_loss = batch_outputs['loss']\n            epoch_loss += batch_loss.item()\n            stacker.update(batch_outputs=batch_outputs, batch_targets=batch['target'])\n    average_epoch_loss = epoch_loss\/(batch_num+1)\n    outputs, targets = stacker.get_stack()\n    metric_score = compute_metric_fn(outputs=outputs, targets=targets, **kwargs)\n    return average_epoch_loss, metric_score","94e6bfdc":"def predict(dataloader, tokenizer, model, forward_pass_fn, stacker_class, max_length, device, **kwargs):\n    model.eval()\n    stacker = stacker_class()\n    with torch.no_grad():\n        for batch_num, batch in enumerate(dataloader):\n            batch_outputs = forward_pass_fn(batch=batch, tokenizer=tokenizer, model=model, \n                                            compute_loss_fn=None, max_length=max_length, \n                                            device=device, **kwargs)\n            stacker.update(batch_outputs=batch_outputs, batch_targets=batch['target'])\n    outputs, _ = stacker.get_stack()\n    return outputs","25814200":"def train_and_evaluate(num_epochs, train_dataloader, valid_dataloader, tokenizer, model, optimizer, scheduler,\n                       forward_pass_fn_train, forward_pass_fn_valid, compute_loss_fn_train, compute_loss_fn_valid,\n                       compute_metric_fn, stacker_class, max_length, accumulation_steps, validate_every_n_iteraion, \n                       valid_loss_saver, valid_score_saver, device, **kwargs):\n    iteration_num = 0\n    for epoch_num in range(num_epochs):\n        for batch in train_dataloader:\n            for param_group in optimizer.param_groups:\n                wandb.log({param_group['name']: {\"lr\": param_group['lr']}})\n            model, iteration_train_loss = train_one_batch(iteration_num=iteration_num, batch=batch, tokenizer=tokenizer, model=model,\n                                                          optimizer=optimizer, scheduler=scheduler, forward_pass_fn=forward_pass_fn_train,\n                                                          compute_loss_fn=compute_loss_fn_train, max_length=max_length,\n                                                          accumulation_steps=accumulation_steps, device=device, **kwargs)\n            wandb.log({'Epoch_num': epoch_num, 'iteration_num': iteration_num, 'iteration_train_loss': iteration_train_loss})\n            \n            if 'validate_after_n_iteration' in kwargs:\n                validate_after_n_iteration = kwargs['validate_after_n_iteration']\n            else:\n                validate_after_n_iteration = -1\n\n            if ((iteration_num + 1) % validate_every_n_iteraion == 0) and (iteration_num > validate_after_n_iteration):\n                valid_loss, valid_score = evaluate(dataloader=valid_dataloader, tokenizer=tokenizer, model=model,\n                                                   forward_pass_fn=forward_pass_fn_valid, compute_loss_fn=compute_loss_fn_valid,\n                                                   compute_metric_fn=compute_metric_fn, stacker_class=stacker_class,\n                                                   max_length=max_length, device=device, **kwargs)\n                valid_loss_saver.update(current_iteration_num=iteration_num,current_score=valid_loss, model=model, tokenizer=tokenizer)\n                valid_score_saver.update(current_iteration_num=iteration_num, current_score=valid_score, model=model, tokenizer=tokenizer)\n                wandb.log({\"iteration_num\": iteration_num, \"valid_loss\": valid_loss, \"valid_score\": valid_score})\n                print(f'Epoch_num: {epoch_num}, iteration_num: {iteration_num}, iteration_train_loss: {iteration_train_loss}')\n                print(f'Epoch_num: {epoch_num}, iteration_num: {iteration_num}, valid_loss: {valid_loss}, valid_score: {valid_score}')\n                wandb.run.summary[\"best_valid_loss_iteration_num\"] = valid_loss_saver.get_best_score()['best_iteration_num']\n                wandb.run.summary[\"best_valid_loss\"] = valid_loss_saver.get_best_score()['best_score']\n                wandb.run.summary[\"best_valid_score_iteration_num\"] = valid_score_saver.get_best_score()['best_iteration_num']\n                wandb.run.summary[\"best_valid_score\"] = valid_score_saver.get_best_score()['best_score']\n            iteration_num += 1\n            \n    if 'final_model_saver' in kwargs:\n        final_model_saver = kwargs['final_model_saver']\n        valid_score = 1\n        final_model_saver.update(current_iteration_num=iteration_num, current_score=valid_score, model=model, tokenizer=tokenizer)\n    \n    wandb.run.summary[\"best_valid_loss_iteration_num\"] = valid_loss_saver.get_best_score()['best_iteration_num']\n    wandb.run.summary[\"best_valid_loss\"] = valid_loss_saver.get_best_score()['best_score']\n    wandb.run.summary[\"best_valid_score_iteration_num\"] = valid_score_saver.get_best_score()['best_iteration_num']\n    wandb.run.summary[\"best_valid_score\"] = valid_score_saver.get_best_score()['best_score']\n\n    output = {'model': model, 'best_score': valid_score_saver.get_best_score()['best_score'], 'best_loss': valid_loss_saver.get_best_score()['best_score']}\n    return output","7bc92fb4":"def train_and_evaluate_crazily(num_epochs, train_dataloader, valid_dataloader, tokenizer, model, optimizer, scheduler,\n                               forward_pass_fn_train, forward_pass_fn_valid, compute_loss_fn_train, compute_loss_fn_valid,\n                               compute_metric_fn, stacker_class, max_length, accumulation_steps, validate_every_n_iteraion, \n                               valid_loss_saver, valid_score_saver, device, **kwargs):\n    iteration_num = 0\n    valid_score = np.inf\n    for epoch_num in range(num_epochs):\n        for batch in train_dataloader:\n            for param_group in optimizer.param_groups:\n                wandb.log({param_group['name']: {\"lr\": param_group['lr']}})\n            model, iteration_train_loss = train_one_batch(iteration_num=iteration_num, batch=batch, tokenizer=tokenizer, model=model,\n                                                          optimizer=optimizer, scheduler=scheduler, forward_pass_fn=forward_pass_fn_train,\n                                                          compute_loss_fn=compute_loss_fn_train, max_length=max_length,\n                                                          accumulation_steps=accumulation_steps, device=device, **kwargs)\n            wandb.log({'Epoch_num': epoch_num, 'iteration_num': iteration_num, 'iteration_train_loss': iteration_train_loss})\n            \n            if 'validate_after_n_iteration' in kwargs:\n                validate_after_n_iteration = kwargs['validate_after_n_iteration']\n            else:\n                validate_after_n_iteration = -1\n                \n            if valid_score < 0.51:\n                validate_every_n_iteraion = 2\n            else:\n                validate_every_n_iteraion = 10                \n\n            if ((iteration_num + 1) % validate_every_n_iteraion == 0) and (iteration_num > validate_after_n_iteration):\n                valid_loss, valid_score = evaluate(dataloader=valid_dataloader, tokenizer=tokenizer, model=model,\n                                                   forward_pass_fn=forward_pass_fn_valid, compute_loss_fn=compute_loss_fn_valid,\n                                                   compute_metric_fn=compute_metric_fn, stacker_class=stacker_class,\n                                                   max_length=max_length, device=device, **kwargs)\n                valid_loss_saver.update(current_iteration_num=iteration_num,current_score=valid_loss, model=model, tokenizer=tokenizer)\n                valid_score_saver.update(current_iteration_num=iteration_num, current_score=valid_score, model=model, tokenizer=tokenizer)\n                wandb.log({\"iteration_num\": iteration_num, \"valid_loss\": valid_loss, \"valid_score\": valid_score})\n                print(f'Epoch_num: {epoch_num}, iteration_num: {iteration_num}, iteration_train_loss: {iteration_train_loss}')\n                print(f'Epoch_num: {epoch_num}, iteration_num: {iteration_num}, valid_loss: {valid_loss}, valid_score: {valid_score}')\n                wandb.run.summary[\"best_valid_loss_iteration_num\"] = valid_loss_saver.get_best_score()['best_iteration_num']\n                wandb.run.summary[\"best_valid_loss\"] = valid_loss_saver.get_best_score()['best_score']\n                wandb.run.summary[\"best_valid_score_iteration_num\"] = valid_score_saver.get_best_score()['best_iteration_num']\n                wandb.run.summary[\"best_valid_score\"] = valid_score_saver.get_best_score()['best_score']\n            iteration_num += 1\n            \n    if 'final_model_saver' in kwargs:\n        final_model_saver = kwargs['final_model_saver']\n        valid_score = 1\n        final_model_saver.update(current_iteration_num=iteration_num, current_score=valid_score, model=model, tokenizer=tokenizer)\n    \n    wandb.run.summary[\"best_valid_loss_iteration_num\"] = valid_loss_saver.get_best_score()['best_iteration_num']\n    wandb.run.summary[\"best_valid_loss\"] = valid_loss_saver.get_best_score()['best_score']\n    wandb.run.summary[\"best_valid_score_iteration_num\"] = valid_score_saver.get_best_score()['best_iteration_num']\n    wandb.run.summary[\"best_valid_score\"] = valid_score_saver.get_best_score()['best_score']\n\n    output = {'model': model, 'best_score': valid_score_saver.get_best_score()['best_score'], 'best_loss': valid_loss_saver.get_best_score()['best_score']}\n    return output","1cc9ae8e":"def train_and_evaluate_swa(num_epochs, train_dataloader, valid_dataloader, tokenizer, model, optimizer, scheduler,\n                           forward_pass_fn_train, forward_pass_fn_valid, compute_loss_fn_train, compute_loss_fn_valid,\n                           compute_metric_fn, stacker_class, max_length, accumulation_steps, validate_every_n_iteraion, \n                           valid_loss_saver, valid_score_saver, device, swa_freq, **kwargs):\n    iteration_num = 0\n    for epoch_num in range(num_epochs):\n        data_in_epoch = len(train_dataloader.dataset)\n        epoch_step = 0\n        n_data = 0\n        for batch in train_dataloader:\n            for param_group in optimizer.param_groups:\n                wandb.log({param_group['name']: {\"lr\": param_group['lr']}})\n            model, iteration_train_loss = train_one_batch(iteration_num=iteration_num, batch=batch, tokenizer=tokenizer, model=model,\n                                                          optimizer=optimizer, scheduler=scheduler, forward_pass_fn=forward_pass_fn_train,\n                                                          compute_loss_fn=compute_loss_fn_train, max_length=max_length,\n                                                          accumulation_steps=accumulation_steps, device=device, **kwargs)\n            wandb.log({'Epoch_num': epoch_num, 'iteration_num': iteration_num, 'iteration_train_loss': iteration_train_loss})\n            epoch_step += 1\n            n_data += len(batch['target'])\n            eps = int(n_data\/data_in_epoch)\n            if (epoch_step>=int(0.75*len(train_dataloader))) and (epoch_step%swa_freq==0):\n                    print(f\"taking swa snapshot @ {epoch_step}\")\n                    optimizer.update_swa()\n            if ((iteration_num + 1) % validate_every_n_iteraion == 0) or (eps!=0):\n                if eps!=0:\n                    print(f\"swap swa weights @ {epoch_step}\")\n                    optimizer.swap_swa_sgd()\n                valid_loss, valid_score = evaluate(dataloader=valid_dataloader, tokenizer=tokenizer, model=model,\n                                                   forward_pass_fn=forward_pass_fn_valid, compute_loss_fn=compute_loss_fn_valid,\n                                                   compute_metric_fn=compute_metric_fn, stacker_class=stacker_class,\n                                                   max_length=max_length, device=device, **kwargs)\n                valid_loss_saver.update(current_iteration_num=iteration_num,current_score=valid_loss, model=model, tokenizer=tokenizer)\n                valid_score_saver.update(current_iteration_num=iteration_num, current_score=valid_score, model=model, tokenizer=tokenizer)\n                wandb.log({\"iteration_num\": iteration_num, \"valid_loss\": valid_loss, \"valid_score\": valid_score})\n                print(f'Epoch_num: {epoch_num}, iteration_num: {iteration_num}, iteration_train_loss: {iteration_train_loss}')\n                print(f'Epoch_num: {epoch_num}, iteration_num: {iteration_num}, valid_loss: {valid_loss}, valid_score: {valid_score}')\n                wandb.run.summary[\"best_valid_loss_iteration_num\"] = valid_loss_saver.get_best_score()['best_iteration_num']\n                wandb.run.summary[\"best_valid_loss\"] = valid_loss_saver.get_best_score()['best_score']\n                wandb.run.summary[\"best_valid_score_iteration_num\"] = valid_score_saver.get_best_score()['best_iteration_num']\n                wandb.run.summary[\"best_valid_score\"] = valid_score_saver.get_best_score()['best_score']\n                \n                if eps!=0:\n                    print(\"swap weights back\")\n                    optimizer.swap_swa_sgd()\n            iteration_num += 1\n    if 'final_model_saver' in kwargs:\n        final_model_saver = kwargs['final_model_saver']\n        final_model_saver.update(current_iteration_num=iteration_num, current_score=valid_score, model=model, tokenizer=tokenizer)\n    wandb.run.summary[\"best_valid_loss_iteration_num\"] = valid_loss_saver.get_best_score()['best_iteration_num']\n    wandb.run.summary[\"best_valid_loss\"] = valid_loss_saver.get_best_score()['best_score']\n    wandb.run.summary[\"best_valid_score_iteration_num\"] = valid_score_saver.get_best_score()['best_iteration_num']\n    wandb.run.summary[\"best_valid_score\"] = valid_score_saver.get_best_score()['best_score']\n    output = {'model': model, 'best_score': valid_score_saver.get_best_score()['best_score'], 'best_loss': valid_loss_saver.get_best_score()['best_score']}\n    return output","f4b8af9d":"def clear_cuda():\n    gc.collect()\n    torch.cuda.empty_cache()","2758cf08":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","13c8917b":"# Sampler","7837ce18":"# Train and Evaluate functions","b6890401":"# Saver","78e859a0":"# Forward pass functions","9be1f983":"# Metrics","e18b881e":"# Compute predictions","37bcf060":"# DataLoader","f13b7552":"# Dataset","01ef7900":"# Crazy evaluation","ff1c23e5":"# New loss functions","926c3a44":"# Objective\nThe objective of this notebook is to build a pipeline combining all the components that we have developed until now. In the end it should be possible to convert this notebook into a script so that we can train a Ranker or Regressor on External or internal data.","44654bcf":"# Scheduler\n\n``` python\ndef lr_scheduler(lrs, optimizer, lr, train_dataloader):\n    if lrs == 'cyclic':\n        scheduler = CyclicLR(optimizer, base_lr=lr, max_lr=4.5e-5, step_size_up=len(train_dataloader)\/2, cycle_momentum=False)\n    \n    elif lrs == 'cosine_schedule_with_warmup':\n        scheduler = get_cosine_schedule_with_warmup(optimizer,\n                                                    num_training_steps=config['epochs'] * len(train_dataloader),\n                                                    num_warmup_steps=0.2*len(train_dataloader))\n    else:\n        scheduler = None\n    return scheduler\n```","2f124d18":"# Model","ed6d90c2":"# Loss functions","84c44556":"# Stackers","ba6e6b87":"# SWA","6ef8497f":"# Helper functions","26cd84bf":"# Optimizer"}}