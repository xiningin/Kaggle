{"cell_type":{"67fd4987":"code","8e2d1463":"code","797e23ae":"code","87e26b44":"code","7471c994":"code","2cb63dc9":"code","409eab74":"code","5ae59d72":"code","64905070":"code","9470a8a8":"code","06030cc5":"code","33f02417":"code","e5670d71":"code","a56496e7":"code","7c6af9c0":"code","77cf4a86":"code","76c1ba07":"code","05a30daa":"code","ac3e8a55":"code","ed3b140b":"markdown","259be94d":"markdown","8175c761":"markdown","ea590b9c":"markdown","cd5bdfb6":"markdown","1839856c":"markdown","e23f04b2":"markdown","64e0fe12":"markdown","d5133fba":"markdown","a945b740":"markdown","fac20899":"markdown","60031b82":"markdown"},"source":{"67fd4987":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8e2d1463":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn import tree\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split , GridSearchCV\nfrom sklearn.metrics import plot_confusion_matrix ,accuracy_score","797e23ae":"df=pd.read_csv('..\/input\/drug-classification\/drug200.csv')\ndf.head()","87e26b44":"df['Drug'].value_counts()","7471c994":"categorical_cols=[col for col in df.columns if df[col].dtype=='object']\ncategorical_cols","2cb63dc9":"label_encoder=LabelEncoder()\nlabelled_df=df.copy()\nfor col in categorical_cols:\n    labelled_df[col]=label_encoder.fit_transform(df[col])\nlabelled_df.head()    ","409eab74":"X=labelled_df.drop('Drug',axis=1)\ny=labelled_df['Drug']\ny.head()","5ae59d72":"scaler=StandardScaler()\nfeature_set=scaler.fit_transform(X)\nfeature_set.shape","64905070":"X_train,X_val,y_train,y_val=train_test_split(feature_set,y,test_size=0.2,random_state=0)\nX_train.shape","9470a8a8":"from sklearn.neighbors import KNeighborsClassifier\n\nKs=10\nmean_acc=np.zeros((Ks-1))\nstd_acc=np.zeros((Ks-1))\nConfusionMx=[];\nfor n in range(1,Ks):\n    #Train Model and Predict\n    neigh=KNeighborsClassifier(n_neighbors=n).fit(X_train,y_train)\n    yhat=neigh.predict(X_val)\n    mean_acc[n-1]=metrics.accuracy_score(y_val,yhat)\n    \n    std_acc[n-1]=np.std(yhat==y_val)\/np.sqrt(yhat.shape[0])\nmean_acc","06030cc5":"plt.plot(range(1,Ks),mean_acc,'g')\nplt.fill_between(range(1,Ks),mean_acc-1*std_acc,mean_acc+1*std_acc,alpha=0.10)\nplt.legend(('Accuracy','+\/-3xstd'))\nplt.ylabel('Accuracy')\nplt.xlabel('Number of neighbors(K)')\nplt.show()","33f02417":"knn_acc=mean_acc.max()\nprint('The best accuracy = ',knn_acc,'with K = ',mean_acc.argmax()+1)","e5670d71":"from sklearn.naive_bayes import GaussianNB\n\nnb=GaussianNB()\nnb.fit(X_train,y_train)\npred=nb.predict(X_val)\nnb_acc=accuracy_score(pred,y_val)\nnb_acc","a56496e7":"from sklearn.linear_model import LogisticRegression\n\nlr=LogisticRegression(C=0.01,solver='liblinear')\nlr.fit(X_train,y_train)\npred=lr.predict(X_val)\nlr_acc=accuracy_score(pred,y_val)\nlr_acc","7c6af9c0":"from sklearn.tree import DecisionTreeClassifier\n\nmodel=DecisionTreeClassifier()\nparameters=[{'max_depth':[1,2,3,4,5,6,7,8,9,10]}]\nclf=GridSearchCV(model,parameters,cv=5,scoring='accuracy')\nclf.fit(X_train,y_train)\nprint(clf.best_params_)","77cf4a86":"clf=DecisionTreeClassifier(criterion='entropy',max_depth=4)\nclf.fit(X_train,y_train)\npred=clf.predict(X_val)\ntree_acc=accuracy_score(pred,y_val)\ntree_acc","76c1ba07":"plot_confusion_matrix(clf,X_val,y_val,cmap=plt.cm.Reds,display_labels=['Drug A','Drug B','Drug C','Drug X',\n                                                     'Drug Y'])","05a30daa":"scores=pd.DataFrame({\n    'Models':['KNearest Neighbors','Gaussian Naive Bayes','Logistic Regression','Decision Tree'],\n    'Accuracy':[knn_acc,nb_acc,lr_acc,tree_acc]\n})\nscores.sort_values(by='Accuracy',ascending=False)","ac3e8a55":"plt.figure(figsize=(14,7))\ntree.plot_tree(clf, rounded=True, filled=True)","ed3b140b":"# Train Test Split","259be94d":"# Logistic Regression","8175c761":"**So, max_depth is 4**","ea590b9c":"**100% accuracy**","cd5bdfb6":"**So, We use Decision Tree Classification Model to predict Drugs with 100% accuracy**","1839856c":"# Importing Libraries","e23f04b2":"# Visualization","64e0fe12":"# Decision Tree","d5133fba":"# Encoding Categorical Variables","a945b740":"# KNearest Neighbors Classification","fac20899":"# Gaussian Naive Bayes","60031b82":"# Accuracy Scores"}}