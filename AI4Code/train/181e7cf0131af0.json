{"cell_type":{"95960267":"code","cd996385":"code","cce6b07e":"code","f13cd46f":"code","41293e6d":"code","838b31a3":"code","7af66647":"code","fc19b796":"code","ef2d66ce":"code","4abeab19":"code","9a226625":"code","315f9e93":"code","3a3076e9":"code","6ff21974":"code","50d8842b":"code","a611e530":"code","89469403":"code","529eef81":"code","df455ff6":"code","b906672c":"code","cc700227":"code","068fab57":"code","518d32d4":"code","99b297e8":"code","a92c5c56":"code","1b40dc41":"code","e2cf0316":"code","83221420":"code","e740772c":"code","e868e29a":"code","fd004933":"code","1b16916b":"code","9b5ee5a7":"code","e22a79fc":"code","0c2af5b7":"code","b593dabe":"code","d806dd76":"code","6d036b47":"code","a77d12bc":"code","bd16c96d":"code","498904d3":"code","b4f06af4":"code","ace98ab8":"code","78a3639c":"code","8a8208b6":"code","d4d40bee":"code","b31a9851":"code","fbb153b6":"code","a9be1403":"code","302748e7":"code","91cefe4b":"code","98f22ae0":"code","7dcc7f9e":"code","c5704dd4":"code","abadbe86":"code","5b338b16":"code","97533337":"code","78d49d6f":"markdown","ddb3e4a0":"markdown","67303dcb":"markdown","5b49543f":"markdown","042de182":"markdown","5677dc9d":"markdown","23279f81":"markdown","20c86b06":"markdown","eb11b1d1":"markdown","05e2ac0c":"markdown","270ca3cb":"markdown","f81973cd":"markdown","31985cec":"markdown","35b149b0":"markdown","1dd667a3":"markdown","1f67552b":"markdown","88fd9a36":"markdown","babe5d2d":"markdown","10b7f516":"markdown","12977854":"markdown","eaaf2d5e":"markdown","c6008ec9":"markdown","5dc739f9":"markdown","71a567cf":"markdown","9a258a77":"markdown","7accb349":"markdown","1043ccd9":"markdown","3d165be3":"markdown","345a2c55":"markdown","1eeaa320":"markdown","20b9014c":"markdown","5fe3b315":"markdown","7f8f3ed9":"markdown","cefe6343":"markdown","b60ea419":"markdown","e6daff39":"markdown","67cd5251":"markdown"},"source":{"95960267":"! pip install pandas_market_calendars numpy_indexed mplfinance","cd996385":"%matplotlib inline\n\nfrom typing import Iterable, List, Optional, Tuple, Union\n\nimport numpy as np\nimport pandas as pd","cce6b07e":"%%bash\nshopt -s extglob\ncd \/kaggle\/input\/daily-and-intraday-stock-price-data\nls !(Data)","f13cd46f":"%%bash\nshopt -s extglob\ncd \/kaggle\/input\/daily-and-intraday-stock-price-data\nIFS=$'\\n'\nfor d in `ls -1d !(1 Hour|Data)\/Stocks`; do\n    echo \"$d:\"\n    ls \"$d\" | head -5 | sed -E 's\/^\/    \/g'\n    echo '    ...'\ndone","41293e6d":"from pathlib import Path\n\nSTOCK_DATA = Path('\/kaggle\/input\/daily-and-intraday-stock-price-data\/')\n\nsfx = '.us.txt'\nsfx_len = len(sfx)\n\nfive_min_tickers = set(p.name[:-sfx_len] for p in (STOCK_DATA \/ '5 Min\/Stocks').iterdir()\n                       if p.name.endswith(sfx))\none_day_tickers = set(p.name[:-sfx_len] for p in (STOCK_DATA \/ '1 Day\/Stocks').iterdir()\n                      if p.name.endswith(sfx))","838b31a3":"five_min_tickers.symmetric_difference(one_day_tickers)","7af66647":"tickers = five_min_tickers & one_day_tickers","fc19b796":"len(tickers)","ef2d66ce":"with (STOCK_DATA \/ '5 Min\/Stocks\/a.us.txt').open() as f:\n    df = pd.read_csv(f)\ndf.head()","4abeab19":"with (STOCK_DATA \/ '1 Day\/Stocks\/a.us.txt').open() as f:\n    df = pd.read_csv(f)\ndf.head()","9a226625":"import pandas_market_calendars as mcal\n\nNYSE = mcal.get_calendar('NYSE')\n\ndef read_df_5m(ticker: str) -> pd.DataFrame:\n    with (STOCK_DATA \/ f'5 Min\/Stocks\/{ticker}.us.txt').open() as f:\n        df = pd.read_csv(f, index_col='Datetime',\n                         usecols=['Date', 'Time', 'Open', 'High', 'Low', 'Close', 'Volume'],\n                         parse_dates={'Datetime': ['Date', 'Time']})\n    return df.set_index(\n        df.index.tz_localize('CET').tz_convert(NYSE.tz).tz_localize(None))\n\ndef read_df_1d(ticker: str) -> pd.DataFrame:\n    with (STOCK_DATA \/ f'1 Day\/Stocks\/{ticker}.us.txt').open() as f:\n        return pd.read_csv(f, usecols=['Date', 'Open', 'High', 'Low', 'Close', 'Volume'],\n                           parse_dates=['Date'])","315f9e93":"read_df_5m('civbp').loc['2017-02-16']","3a3076e9":"read_df_5m('akg').loc['2017-11-24'].tail()","6ff21974":"from dataclasses import dataclass\nfrom functools import lru_cache\n\n@dataclass\nclass DailySchedule:\n    valid_days: np.ndarray\n    early_closes: np.ndarray\n\n@lru_cache()\ndef nyse_daily_schedule(min_date: np.datetime64, max_date: np.datetime64) -> DailySchedule:\n    if max_date < min_date:\n        raise ValueError('max_date is before min_date.')\n\n    schedule = NYSE.schedule(min_date, max_date).tz_localize(NYSE.tz).tz_localize(None)\n    return DailySchedule(schedule.index.to_numpy(),\n                         NYSE.early_closes(schedule).index.to_numpy())","50d8842b":"freq_5m = pd.Timedelta(minutes=5)\nmarket_open = pd.Timedelta(NYSE.open_time.isoformat())\n\nFULL_5M_CANDLES = pd.timedelta_range(\n    start=market_open + freq_5m, end=pd.Timedelta(NYSE.close_time.isoformat()), freq=freq_5m\n).to_numpy()\n\nNUM_5M_CANDLES = len(FULL_5M_CANDLES)\nNUM_5M_CANDLES_EARLY = (\n    pd.Timedelta(NYSE.regular_early_close.isoformat()) - market_open) \/\/ freq_5m\n\ndef encode_time(num_candles: int) -> np.ndarray:\n    points = 2 * np.pi * np.linspace(0., 1., num_candles, endpoint=False)[:, None]\n    return np.concatenate([np.sin(points), np.cos(points)], axis=1)\n\nENCODED_5M_TIME = encode_time(NUM_5M_CANDLES)\nENCODED_5M_TIME_EARLY = encode_time(NUM_5M_CANDLES_EARLY)\nFREQ_5M = freq_5m.to_numpy()\nDAY_DTYPE = np.datetime64('', 'D')\n\ndef candle_num(datetime: np.datetime64) -> int:\n    return (datetime - datetime.astype(DAY_DTYPE) - FULL_5M_CANDLES[0]) \/\/ FREQ_5M\n\n@dataclass\nclass FiveMinSchedule:\n    candles: np.ndarray\n    encoded_time: np.ndarray\n\n@lru_cache()\ndef nyse_five_min_schedule(min_date: np.datetime64, max_date: np.datetime64) -> FiveMinSchedule:\n    sched = nyse_daily_schedule(min_date, max_date)\n    earlies_mask = np.isin(sched.valid_days, sched.early_closes, assume_unique=True)\n\n    num_early = len(sched.early_closes)\n    num_full = len(sched.valid_days) - num_early\n    candles = np.full(num_full * NUM_5M_CANDLES + num_early * NUM_5M_CANDLES_EARLY,\n                      np.datetime64(), sched.valid_days.dtype)\n    encoded_time = np.full((len(candles), 2), np.nan)\n\n    start_idx = 0\n    for d, e in zip(sched.valid_days, earlies_mask):\n        num_candles = NUM_5M_CANDLES_EARLY if e else NUM_5M_CANDLES\n        stop_idx = start_idx + num_candles\n        day_slice = slice(start_idx, stop_idx)\n        candles[day_slice] = d + FULL_5M_CANDLES[:num_candles]\n        encoded_time[day_slice, :] = ENCODED_5M_TIME_EARLY if e else ENCODED_5M_TIME\n        start_idx = stop_idx\n\n    assert not np.isnat(candles).any() or not np.isnan(encoded_time).any(), \\\n           'Not all values have been filled.'\n\n    return FiveMinSchedule(candles, encoded_time)","a611e530":"@dataclass\nclass ConsecutiveSlices:\n    slices: List[slice]\n    datetime: np.ndarray\n    encoded_time: np.ndarray\n    ohlcv: np.ndarray\n\ndef consecutive_slices(df_5m: pd.DataFrame, min_consec_candles: int) -> ConsecutiveSlices:\n    assert df_5m.index.is_unique and df_5m.index.is_monotonic_increasing, \\\n           'searchsorted will not work with datetime.'\n    assert (df_5m.columns == pd.Index(['Open', 'High', 'Low', 'Close', 'Volume'])).all()\n\n    datetime = df_5m.index.to_numpy()\n    sched = nyse_five_min_schedule(datetime[0].astype(DAY_DTYPE), datetime[-1].astype(DAY_DTYPE))\n\n    # Remove extraneous outside-the-trading-hours candles.\n    extra_idx = datetime.searchsorted(\n        np.setdiff1d(datetime, sched.candles, assume_unique=True))\n    datetime = np.delete(datetime, extra_idx)\n    ohlcv = np.delete(df_5m.to_numpy(), extra_idx, axis=0)\n    assert len(datetime) == len(ohlcv)\n\n    missing = np.setdiff1d(sched.candles, datetime, assume_unique=True)\n    missing_idx = sched.candles.searchsorted(missing)\n    encoded_time = np.delete(sched.encoded_time, missing_idx, axis=0)\n    assert len(encoded_time) == len(datetime)\n\n    slices = []\n    start = 0\n    for stop in np.append(datetime.searchsorted(missing, 'right'), len(datetime)):\n        if stop - start > min_consec_candles:\n            slices.append(slice(start, stop))\n        start = stop\n    return ConsecutiveSlices(slices, datetime, encoded_time, ohlcv)","89469403":"from numba import njit\n\n@njit\ndef argrelextrema_eq(x: np.ndarray, relmax: bool, order: int) -> np.ndarray:\n    if order < 1:\n        raise ValueError('Order must be >= 1')\n\n    result = np.full(len(x), True)\n    for shift in range(1, order + 1):\n        any_true = False\n        for i in range(len(x)):\n            plus, minus = x[min(i + shift, len(x) - 1)], x[max(i - shift, 0)]\n            if relmax:\n                result[i] &= (x[i] >= plus) & (x[i] >= minus)\n            else:\n                result[i] &= (x[i] <= plus) & (x[i] <= minus)\n            any_true |= result[i]\n        if not any_true:\n            break\n    return result.nonzero()[0]\n\n@njit\ndef argrelmax_eq(x: np.ndarray, order: int) -> np.ndarray:\n    return argrelextrema_eq(x, True, order)\n\n@njit\ndef argrelmin_eq(x: np.ndarray, order: int) -> np.ndarray:\n    return argrelextrema_eq(x, False, order)","529eef81":"from numpy.random import default_rng\nfrom scipy.signal import argrelextrema\n\nRNG = default_rng()\n\ntest_data = RNG.random(10_000)\n\nargrelmax_eq(test_data, 3);  # JIT compile, discard output.","df455ff6":"%timeit argrelmax_eq(test_data, 3)","b906672c":"%timeit argrelextrema(test_data, np.greater_equal, order=3)","cc700227":"import numpy_indexed as npi\n\ndef price_levels(smooth_high: np.ndarray, smooth_low: np.ndarray,\n                 threshold_pct=.2, max_levels=5) -> np.ndarray:\n    from sklearn.cluster import AgglomerativeClustering\n\n    extrema = np.concatenate([smooth_high[argrelmax_eq(smooth_high, 3)],\n                              smooth_low[argrelmin_eq(smooth_low, 3)]])\n\n    # A level is usually tested multiple times, i.e. it comes close\n    # to other price levels. Here, close means within threshold_pct\n    # of the entire price range.\n    diff = np.abs(np.concatenate([smooth_high, smooth_low]) - extrema[:, None])\n    threshold = (smooth_high.max() - smooth_low.min()) * threshold_pct\n\n    # Order by the number of times level was tested.\n    # argsort - indices that sort the array.\n    # ordering - position in the sorted array.\n    argsort = (diff < threshold).sum(axis=1).argsort()\n    ordering = np.empty_like(argsort)\n    ordering[argsort] = np.arange(ordering.size)\n\n    # Coalesce all levels that are apart by less than threshold.\n    labels = AgglomerativeClustering(\n        n_clusters=None, distance_threshold=threshold\n    ).fit(extrema[:, None]).labels_\n\n    # Pick from each cluster the level most frequently tested,\n    # and then leave max_levels most frequently tested overall.\n    choice = npi.group_by(labels).reduce(ordering, np.maximum)\n    if choice.size > max_levels:\n        choice = np.partition(choice, -max_levels)[-max_levels:]\n\n    # Convert indices into the sorted array\n    # to indices into the unsorted array.\n    return extrema[np.isin(ordering, choice, assume_unique=True)]","068fab57":"test_data = RNG.choice(np.arange(30), 30, replace=False)\ntest_grp = RNG.integers(5, size=30)\ntest_series = pd.Series(test_data)","518d32d4":"%timeit npi.group_by(test_grp).reduce(test_data, np.maximum)","99b297e8":"%timeit test_series.groupby(test_grp).max()","a92c5c56":"from scipy.signal import savgol_coeffs\n\nSAVGOL_COEFFS = savgol_coeffs(5, 3, use='dot')[None, None]\n\ndef rolling_window(x: np.ndarray, window: int) -> np.ndarray:\n    from numpy.lib.stride_tricks import as_strided\n\n    return as_strided(x, x.shape[:-1] + (x.shape[-1] - window + 1, window),\n                      x.strides + (x.strides[-1],), writeable=False)\n\ndef rolling_price_levels(high: np.ndarray, low: np.ndarray, window: int,\n                         threshold_pct=.2, max_levels=5) -> np.ndarray:\n    from scipy.ndimage import correlate\n\n    if len(high) != len(low):\n        raise ValueError('Lengths of high and low do not match.')\n\n    smooth_high_low = correlate(\n        rolling_window(np.concatenate([high[None], low[None]]), window),\n        SAVGOL_COEFFS, mode='nearest')\n\n    levels = np.full((len(high), max_levels), np.nan)\n    high, low = smooth_high_low[0], smooth_high_low[1]\n    for i in range(len(high)):\n        lvl = price_levels(high[i], low[i], threshold_pct, max_levels)\n        levels[window - 1 + i, :lvl.size] = lvl\n\n    assert window - 1 + i == len(levels) - 1\n    return levels","1b40dc41":"import mplfinance as mpf\n\nCANDLE_STYLE = mpf.make_mpf_style(base_mpf_style='yahoo', gridstyle='', y_on_right=False)\n\ndef candlestick_plot(df: pd.DataFrame, levels: Optional[Iterable[float]] = None,\n                     target_idx: Optional[np.ndarray] = None, title: Optional[str] = None):\n    kwargs = dict(figratio=(15, 6), type='candle', style=CANDLE_STYLE, tight_layout=True)\n\n    if levels is not None:\n        kwargs['hlines'] = dict(hlines=tuple(levels), linestyle='--', linewidths=1.2)\n    if target_idx is not None:\n        targ = np.full(len(df), np.nan)\n        targ[target_idx] = df['Close'].to_numpy()[target_idx]\n        kwargs['addplot'] = mpf.make_addplot(targ, scatter=True, markersize=200)\n    if title is not None:\n        kwargs['title'] = title\n\n    mpf.plot(df, **kwargs)","e2cf0316":"tks = tuple(tickers)\nwindow = 30\n\nwhile True:\n    tk = RNG.choice(tks)\n    df = read_df_1d(tk).set_index('Date')\n    if len(df) > window:\n        break\nstop = RNG.integers(window, len(df))\ndf = df.iloc[stop - window : stop]\n\nsup_res = rolling_price_levels(df['High'].to_numpy(), df['Low'].to_numpy(), window)[-1]\nsup_res = sup_res[~np.isnan(sup_res)]\n\ncandlestick_plot(df, levels=sup_res, title=tk)","83221420":"@njit\ndef ewma(x: np.ndarray, span: float = None,\n         alpha: float = None, adjust=True) -> np.ndarray:\n    if not ((span is None) ^ (alpha is None)):\n        raise ValueError('Exactly one of span and alpha must be provided.')\n    if span is not None:\n        alpha = 2. \/ (1. + span)\n\n    out = np.empty_like(x)\n    mult = 1. - alpha\n\n    if not adjust:\n        out[0] = x[0]\n        for i in range(1, out.size):\n            out[i] = alpha * x[i] + mult * out[i - 1]\n        return out\n\n    num, denum = 0., 0.\n    for i in range(out.size):\n        num = x[i] + mult * num\n        denum = 1 + mult * denum\n        out[i] = num \/ denum\n    return out","e740772c":"test_data = RNG.random(10_000)\ntest_series = pd.Series(test_data)\n\newma(test_data, span=30);  # JIT compile, discard output.","e868e29a":"%timeit ewma(test_data, span=30)","fd004933":"%timeit test_series.ewm(span=30).mean()","1b16916b":"@njit\ndef average_true_range(high: np.ndarray, low: np.ndarray, y_close: np.ndarray) -> np.ndarray:\n    window = 14\n    tr = np.maximum(high, y_close) - np.minimum(y_close, low)\n    start_ewma = window - 1\n    tr[start_ewma] = tr[:start_ewma + 1].mean()\n    tr[start_ewma:] = ewma(tr[start_ewma:], alpha=1 \/ window, adjust=False)\n    tr[:start_ewma] = np.nan\n    return tr","9b5ee5a7":"import bottleneck as bn\n\nDF_1D_COLS = (('Early Close', 'ADV', 'ATR', 'Premarket Gap', 'PCL',\n               'Y High', 'Y Low', 'YY High', 'YY Low')\n              + tuple(f'Day Level {i}' for i in range(1, 6)))\n\ndef process_daily(df_1d: pd.DataFrame, min_date: np.datetime64, max_date: np.datetime64,\n                  history: int, window=30) -> Optional[pd.DataFrame]:\n    if max_date < min_date:\n        raise ValueError('max_date is before min_date.')\n    if history < 0:\n        raise ValueError('history must be non-negative.')\n    if window < 14:\n        raise ValueError('window must be at least 14 to compute ATR.')\n\n    dates = df_1d['Date'].to_numpy()\n    assert (dates[:-1] < dates[1:]).all(), 'searchsorted will not work with dates.'\n    # At the beginning of a day only yesterday's data is available.\n    # Shift by one accounts for it.\n    y_stop = dates.searchsorted(max_date, 'right') - 1\n    min_req_candles = history - 1 + window\n    if y_stop < min_req_candles:\n        return None\n\n    # Where computation of indicators starts.\n    window_start = np.max([0, dates.searchsorted(min_date) - min_req_candles])\n    assert y_stop - window_start >= min_req_candles\n    window_slice = slice(window_start, y_stop)\n    # Where actual data starts: all indicators are not NaN.\n    y_start = window_start + window - 1\n\n    if window_start > 0:\n        atr_y_slice = window_slice\n        atr_yy_slice = slice(window_start - 1, y_stop - 1)\n    else:\n        # ATR requires previous day close. If it's not available\n        # and window is not large enough to prime ATR,\n        # an additional date is consumed.\n        atr_y_slice = slice(window_start + 1, y_stop)\n        atr_yy_slice = slice(window_start, y_stop - 1)\n        if window <= 14:\n            y_start += 1\n            window_slice = atr_y_slice\n    assert atr_y_slice.stop - atr_y_slice.start == atr_yy_slice.stop - atr_yy_slice.start\n    assert atr_y_slice.stop - atr_y_slice.start >= 14\n\n    if y_stop <= y_start:\n        return None\n    sched = nyse_daily_schedule(dates[window_start + 1], dates[y_stop])\n    gaps = np.setxor1d(sched.valid_days, dates[window_start + 1 : y_stop + 1], assume_unique=True)\n    if gaps.size:\n        return None\n\n    high, low, close = df_1d['High'].to_numpy(), df_1d['Low'].to_numpy(), df_1d['Close'].to_numpy()\n    levels = rolling_price_levels(\n        high[window_slice], low[window_slice], window)[window - 1 :]\n    atr = average_true_range(high[atr_y_slice], low[atr_y_slice], close[atr_yy_slice])\n\n    y_slice = np.s_[y_start:y_stop, None]\n    yy_slice = np.s_[y_start - 1 : y_stop - 1, None]\n    as_col = np.s_[y_start - y_stop :, None]\n\n    today_slice = slice(y_start + 1, y_stop + 1)\n    dates = dates[today_slice]\n    # Actual pre-market data is not available.\n    premarket = df_1d['Open'].to_numpy()[today_slice, None] - close[y_slice]\n\n    earlies = np.isin(dates, sched.early_closes, assume_unique=True)\n    return pd.DataFrame(\n        np.concatenate([\n            earlies[as_col],\n            bn.move_mean(df_1d['Volume'].to_numpy()[window_slice], window)[as_col],\n            atr[as_col], premarket, close[y_slice], high[y_slice], low[y_slice],\n            high[yy_slice], low[yy_slice], levels\n        ], axis=1),\n        pd.DatetimeIndex(dates, name='Date'),\n        DF_1D_COLS\n    )","e22a79fc":"def targets_and_eta(close: np.ndarray, extremum_order=5) -> np.ndarray:\n    # Because of *_eq, a point may end up in both mins and maxs simultaneously,\n    # if prices to the left and\/or to the right of it are the same (illiquid stock).\n    # Use of strict comparison, however, will throw all such points away.\n    mins = argrelmin_eq(close, order=extremum_order)\n    maxs = argrelmax_eq(close, order=extremum_order)\n    extrema = np.unique(np.concatenate([mins, maxs]))\n    min_mask = np.isin(extrema, mins, assume_unique=True)\n    max_mask = np.isin(extrema, maxs, assume_unique=True)\n\n    # The first point might be a min and a max simultaneously, see above.\n    # Resolving the tie in favour of a max is a bias towards short sells,\n    # which seem more appropriate for an infrequently traded stock.\n    is_max = max_mask[0]\n    consec_start = 0\n    prev_extremum = 0\n    targ_eta = np.full((len(close), 2), np.nan)\n    for i in range(1, extrema.size + 1):\n        # Process only if the very end is reached or min changed to max or vice versa.\n        # Consecutive mins\/maxs are ignored.\n        if i < extrema.size and (max_mask if is_max else min_mask)[i]:\n            continue\n\n        if i == consec_start + 1:\n            # Consecutive mins\/maxs have only one point.\n            extremum = extrema[consec_start]\n        else:\n            # A lowest min\/highest max is to be chosen among consecutive mins\/maxs.\n            consec = extrema[consec_start:i]\n            extremum = consec[(np.argmax if is_max else np.argmin)(close[consec])]\n\n        # Current extremum is a target for all points since the last extremum.\n        since_prev_extremum = slice(prev_extremum, extremum)\n        targ_eta[since_prev_extremum, 0] = close[extremum]\n        eta = np.arange(extremum - prev_extremum, 0, -1)\n        # Short sells (transition from max to min) are indicated by a negative ETA.\n        targ_eta[since_prev_extremum, 1] = eta if is_max else -eta\n\n        prev_extremum = extremum\n        consec_start = i\n        is_max = not is_max\n\n    # Closing price is the target for the rest of the day.\n    targ_eta[prev_extremum:, 0] = close[-1]\n    eta = np.arange(len(close) - prev_extremum - 1, -1, -1)\n    targ_eta[prev_extremum:, 1] = eta if close[-1] >= close[prev_extremum] else -eta\n\n    return targ_eta","0c2af5b7":"df = read_df_5m('bv')\ndf = df[df.index.floor('D') == '2017-11-17']\n\ntarg_eta = targets_and_eta(df['Close'].to_numpy())\ntarg_idx = (np.abs(targ_eta[:, 1]) == 1).nonzero()[0] + 1\n\ncandlestick_plot(df, target_idx=targ_idx)","b593dabe":"close = df['Close'].to_numpy()\nmins, maxs = argrelmin_eq(close, order=5), argrelmax_eq(close, order=5)\nnp.unique(np.concatenate([mins, maxs]))","d806dd76":"np.union1d(argrelextrema(close, np.less, order=5),\n           argrelextrema(close, np.greater, order=5))","6d036b47":"while True:\n    tk = RNG.choice(tks)\n    df = read_df_5m(tk)\n    day = RNG.choice(df.index.floor('D')).astype(DAY_DTYPE)\n    df = df[df.index.floor('D') == day]\n    if len(df) == NUM_5M_CANDLES:\n        break\n\ntarg_eta = targets_and_eta(df['Close'].to_numpy())\ntarg_idx = (np.abs(targ_eta[:, 1]) == 1).nonzero()[0] + 1\n\ncandlestick_plot(df, target_idx=targ_idx, title=f'{tk}, {day}')","a77d12bc":"@dataclass\nclass IntradayData:\n    day_codes: np.ndarray\n    cumvol_day_high_low_vwap: np.ndarray\n    targ_eta: np.ndarray\n\ndef intraday_data(datetime: np.ndarray, ohlcv: np.ndarray) -> IntradayData:\n    day_codes = np.full(len(datetime), -1)\n    cumvol_day_high_low_vwap = np.full((len(datetime), 4), np.nan)\n    targ_eta = np.full((len(datetime), 2), np.nan)\n\n    volume = ohlcv[:, -1]\n    stops = npi.group_by(datetime.astype(DAY_DTYPE)).count.cumsum()\n    for i in range(len(stops)):\n        slc = slice(stops[i - 1] if i != 0 else 0, stops[i])\n        slc_len = slc.stop - slc.start\n\n        day_codes[slc] = np.full(slc_len, i)\n\n        cumvol = volume[slc].cumsum()\n        cumvol_day_high_low_vwap[slc, 0] = cumvol\n        cumvol_day_high_low_vwap[slc, 1] = np.maximum.accumulate(ohlcv[slc, 1])\n        cumvol_day_high_low_vwap[slc, 2] = np.minimum.accumulate(ohlcv[slc, 2])\n        cumvol_day_high_low_vwap[slc, 3] = (\n            volume[slc] * ohlcv[slc, 1:-1].mean(axis=1)).cumsum() \/ cumvol\n\n        targ_eta[slc] = targets_and_eta(ohlcv[slc, 3])\n\n    assert not np.isnan(cumvol_day_high_low_vwap).any()\n    assert (day_codes != -1).all()\n    assert not np.isnan(targ_eta).any()\n\n    return IntradayData(day_codes, cumvol_day_high_low_vwap, targ_eta)","bd16c96d":"def closest_levels(x: np.ndarray, candidate_levels: np.ndarray,\n                   min_gap: Union[float, np.ndarray]) -> np.ndarray:\n    diff = candidate_levels - x\n\n    # Discretionary support\/resistance levels are sometimes NaN.\n    nonan_mask = ~np.isnan(diff)\n    lower_mask = np.less_equal(diff, -min_gap, out=np.full(diff.shape, False), where=nonan_mask)\n    higher_mask = np.greater_equal(diff, min_gap, out=np.full(diff.shape, False), where=nonan_mask)\n\n    # Use x \u00b1 min_gap only if no candidate level is available.\n    lower = np.amax(diff, axis=1, keepdims=True, initial=-np.inf, where=lower_mask)\n    lower = np.where(np.isfinite(lower), lower, -min_gap)\n\n    higher = np.amin(diff, axis=1, keepdims=True, initial=np.inf, where=higher_mask)\n    higher = np.where(np.isfinite(higher), higher, min_gap)\n\n    return x + np.concatenate([lower, higher], axis=1)\n\ndef worthy_trades(close: np.ndarray, sup_res: np.ndarray,\n                  targ: np.ndarray, longs: np.ndarray) -> np.ndarray:\n    profit = targ - close\n    stop_loss = np.where(longs, sup_res[:, 0:1], sup_res[:, 1:])\n    loss = close - stop_loss\n    ratio = np.divide(profit, loss, out=np.zeros(profit.shape), where=loss != 0)\n    return ratio >= 2","498904d3":"@njit\ndef next_half(x: np.ndarray) -> np.ndarray:\n    # Shift by .05 to handle exact .5.\n    return np.ceil((x + .05) * 2) \/ 2\n\n@njit\ndef prev_half(x: np.ndarray) -> np.ndarray:\n    # Shift by .05 to handle exact .5.\n    return np.floor((x - .05) * 2) \/ 2\n\ndef smma14(x: np.ndarray) -> np.ndarray:\n    return ewma(x, alpha=1 \/ 14)\n\ndef rsi(x: np.ndarray) -> np.ndarray:\n    up = np.ediff1d(x)\n    down = up.copy()\n    up[up < 0] = 0\n    down[down > 0] = 0\n    denum = smma14(np.abs(down))\n    rs = np.true_divide(smma14(up), denum,\n                        out=np.full_like(denum, np.inf), where=denum != 0)\n    return np.append(np.nan, 100 - 100 \/ (1 + rs))","b4f06af4":"MIN_CANDLES = 200  # For MA(200).\nDF_5M_COLS = ('sin(t)', 'cos(t)', 'RSI', 'Volume', 'Day Volume',\n              'Open', 'High', 'Low', 'Close', 'ATR', 'Day High', 'Day Low',\n              'VWAP', 'EMA(9)', 'EMA(20)', 'MA(50)', 'MA(200)',\n              'Full $ Above', 'Half $ Above', 'Half $ Below', 'Full $ Below',\n              'Support', 'Resistance', 'Indicator Worth', 'True Worth',\n              'Indicator Target', 'True Target', 'True ETA')\n\ndef process_five_min(datetime: np.ndarray, encoded_time: np.ndarray, ohlcv: np.ndarray,\n                     df_1d: pd.DataFrame, history_1d: int) -> pd.DataFrame:\n    if history_1d < 0:\n        raise ValueError('history_1d must be non-negative.')\n\n    dates = df_1d.index.to_numpy()\n    # First 199 candles will be NaN due to MA(200), but daily data might start even later.\n    start = max(datetime.searchsorted(dates[history_1d - 1]), MIN_CANDLES - 1)\n    # Full day must be present in order to compute cumulative volume correctly.\n    if candle_num(datetime[start - candle_num(datetime[start])]) != 0:\n        raise ValueError(f'Missing beginning of day data for {datetime[start]}.')\n\n    stop = datetime.searchsorted(dates[-1] + np.timedelta64(1, 'D'), 'right')\n    non_nan_slice = slice(start, stop)\n    # If first days are within MA(200) NaN, they need to be dropped.\n    dates_start = dates.searchsorted(datetime[start].astype(DAY_DTYPE))\n\n    intraday = intraday_data(datetime, ohlcv)\n    close, as_col = ohlcv[:, 3], np.s_[:, None]\n    five_min_levels = np.concatenate([\n        intraday.cumvol_day_high_low_vwap[:, 1:],\n        ewma(close, span=9)[as_col],\n        ewma(close, span=20)[as_col],\n        bn.move_mean(close, 50)[as_col],\n        bn.move_mean(close, 200)[as_col],\n        # Round half-dollars and full dollars can serve as support\/resistance too.\n        next_half(close + .5)[as_col],\n        next_half(close)[as_col],\n        np.maximum(prev_half(close), 0.)[as_col],\n        np.maximum(prev_half(close - .5), 0.)[as_col]\n    ], axis=1)[non_nan_slice]\n\n    assert all(df_1d.columns[4:] == ['PCL', 'Y High', 'Y Low', 'YY High', 'YY Low']\n                                  + [f'Day Level {i}' for i in range(1, 6)])\n    df_1d_np = df_1d.to_numpy()\n    day_codes = intraday.day_codes[non_nan_slice] - intraday.day_codes[start]\n    daily_levels = df_1d_np[dates_start:, 4:][day_codes]\n\n    # Choose distance to support\/resistance of twice the ATR not to be stopped out immediately.\n    # ATR consumes previous close, indices are shifted by one to account for that.\n    atr = average_true_range(\n        ohlcv[1:, 1], ohlcv[1:, 2], close[:-1])[start - 1 : stop - 1, None]\n    close, targ_eta = close[non_nan_slice, None], intraday.targ_eta[non_nan_slice]\n    assert atr.shape == close.shape\n\n    candidate_levels = np.concatenate([five_min_levels, daily_levels], axis=1)\n    sup_res = closest_levels(close, candidate_levels, 2 * atr)\n\n    longs = targ_eta[:, 1:] > 0  # Short sells are encoded as negative ETA.\n    targ_eta[:, 1] = np.abs(targ_eta[:, 1])\n\n    indicator_targ = closest_levels(targ_eta[:, 0:1], candidate_levels, 0)\n    assert (indicator_targ[:, 0] <= indicator_targ[:, 1]).all()\n    # Use estimate from below for longs, estimate from above for shorts.\n    indicator_targ = np.where(longs, indicator_targ[:, 0:1], indicator_targ[:, 1:])\n\n    worth = np.concatenate([\n        worthy_trades(close, sup_res, indicator_targ, longs),\n        worthy_trades(close, sup_res, targ_eta[:, 0:1], longs)\n    ], axis=1)\n\n    return pd.DataFrame(\n        np.concatenate([\n            encoded_time[non_nan_slice],\n            rsi(ohlcv[:, 3])[non_nan_slice, None],\n            ohlcv[non_nan_slice, -1:],\n            intraday.cumvol_day_high_low_vwap[non_nan_slice, 0:1],\n            ohlcv[non_nan_slice, :-1],\n            atr, five_min_levels,\n            sup_res, worth, indicator_targ, targ_eta\n        ], axis=1),\n        pd.DatetimeIndex(datetime[non_nan_slice], name='Datetime'),\n        DF_5M_COLS\n    )","ace98ab8":"HISTORY_1D = 20\n\ndef process_ticker(ticker: str) -> List[Tuple[str, pd.DataFrame, pd.DataFrame]]:\n    whole_df_5m = read_df_5m(ticker)\n    consec = consecutive_slices(whole_df_5m, MIN_CANDLES)\n    if not consec.slices:\n        return []\n    whole_df_1d = read_df_1d(ticker)\n\n    result = []\n    for i, slc in enumerate(consec.slices):\n        min_date = consec.datetime[slc.start + MIN_CANDLES - 1].astype(DAY_DTYPE)\n        max_date = consec.datetime[slc.stop - 1].astype(DAY_DTYPE)\n        df_1d = process_daily(whole_df_1d, min_date, max_date, HISTORY_1D)\n        if (df_1d) is None:\n            continue\n        df_5m = process_five_min(consec.datetime[slc], consec.encoded_time[slc],\n                                 consec.ohlcv[slc], df_1d, HISTORY_1D)\n        assert df_1d.index[HISTORY_1D - 1] == df_5m.index[0].floor('D')\n        assert df_1d.index[-1] == df_5m.index[-1].floor('D')\n        result.append((ticker if len(consec.slices) == 1 else f'{ticker}{i}', df_1d, df_5m))\n    return result","78a3639c":"process_ticker('a');  # JIT compile, initialize caches, discard output.","8a8208b6":"%timeit process_ticker('a')","d4d40bee":"%load_ext line_profiler","b31a9851":"%lprun -f process_ticker -f process_daily -f process_five_min -f intraday_data -f targets_and_eta -f rolling_price_levels -f price_levels process_ticker(\"a\")","fbb153b6":"%%time\n\nimport multiprocessing as mp\nfrom itertools import chain\n\nconcat_ticker, concat_1d, concat_5m = [], [], []\nnum_proc = mp.cpu_count()\nchunk_size = (len(tickers) + num_proc - 1) \/\/ num_proc\n\nwith mp.Pool(num_proc) as pool:\n    processed = pool.imap_unordered(process_ticker, tickers, chunk_size)\n    for tk, df_1d, df_5m in chain.from_iterable(processed):\n        concat_ticker.append(tk)\n        concat_1d.append(df_1d)\n        concat_5m.append(df_5m)\n\ndf_1d = pd.concat(concat_1d, keys=concat_ticker, names=['Ticker']).sort_index()\ndf_5m = pd.concat(concat_5m, keys=concat_ticker, names=['Ticker']).sort_index()","a9be1403":"df_1d.to_parquet('\/kaggle\/working\/df_1d.parquet')\ndf_5m.to_parquet('\/kaggle\/working\/df_5m.parquet')","302748e7":"df_5m","91cefe4b":"df_1d","98f22ae0":"worthy = df_5m[df_5m['True Worth'] == 1]\nlongs = worthy['True Target'] > worthy['Close']\nmax_loss = worthy['Close'] - worthy['Support'].where(longs, worthy['Resistance'])\n((worthy['True Target'] - worthy['Close']) \/ max_loss).sort_values()","7dcc7f9e":"df_5m.loc[('irbt', '2017-11-22'), :].head()","c5704dd4":"df_5m['Indicator Worth'].mean()","abadbe86":"df_5m['True Worth'].mean()","5b338b16":"hours = df_5m.index.get_level_values('Datetime').hour.rename('Hour')\ndf_5m['Indicator Worth'].groupby(hours).mean()","97533337":"df_5m['True Worth'].groupby(hours).mean()","78d49d6f":"In this example, closing price doesn't change for numerous consecutive candles,\nwhich makes na&iuml;ve extrema finder produce too many targets.","ddb3e4a0":"It's clear from the above that data for a particular ticker is stored in two files:\n\n- `5 Min\/Stocks\/{ticker}.us.txt`,\n- `1 Day\/Stocks\/{ticker}.us.txt`.\n\nLet's find out, which tickers have data available.","67303dcb":"We've seen many indicators that can serve as support or resistance. For the stop loss,\nwe need the closest one. However, it cannot be too close: there is little value in being\nstopped out of a trade immediately. The following function determines the closest level below\n(support) and the closest level above (resistance) that are at least `min_gap` away from\nthe current price (or current price &plusmn; `min_gap` if a better level cannot be found).\nA natural candidate for `min_gap` is twice the five minute ATR: price is expected to move by ATR in\nany direction, but a large unfavourable move means that direction of a trade was predicted\nincorrectly, and the trade must be exited from for a small loss.\n\nRemember, day traders estimate profit targets using the same indicators they use to determine\nstop losses. It is assumed to be easier to predict what level is likely to be crossed instead of\npredicting the actual price. `closest_levels` can be used to round profit targets (local minimums\nand maximums) to the same support\/resistance levels used to determine stop losses:\nthe closest price level below the maxima is used as a target for longs and the closest level\nabove the minima is used as a target for shorts. When rounding targets, `min_gap` must be set\nto zero.\n\nHaving profit target and support\/resistance we can finally determine trades worth entering into,\nand that's exactly what `worthy_trades` is doing.","5b49543f":"Things to notice:\n\n- Open interest column is useless.\n- Date is split from time in five minute dataframes.\n- Trading starts at 9:30 New York time. First candle should be 9:35, but it's 15:35.\n  There's a time zone difference.\n\nData reading functions below account for the above.\n\nDaylight saving time makes it tricky to convert time from one time zone to another. Looks like\nthe guy who published the dataset is from Eastern Europe, and happily there is only one time zone\nout there.\n\nNYSE market calendar is used for all stocks from the dataset, even though many of them\naren't actually NYSE. As far as I know, other major US exchanges (NASDAQ, AMEX, etc.) are in\nthe same time zone and have the same schedule.","042de182":"The use of strict comparison leads to almost no targets being found.","5677dc9d":"## Computing daily indicators\n\nHaving dealt with data misses, we can start computing indicators. Remember the goal:\nfinding trades with a reward\/loss ratio of at least 2. Loss is the distance from the current price\nto the closest support\/resistance. Since indicators computed on a daily chart can serve as\na support\/resistance on a five minute chart, daily chart should be processed first.\n\nHardest indicators to compute are discretionary support\/resistance levels on a daily chart.\nThey are vaguely defined as a level at which the price changed its direction multiple times:\nrises turned into falls and falls turned into rises or, to put it simply, the level was tested.\n\nDirection changes are relative extrema; functions below compute them. Note that comparisons\nare non-strict: if price doesn't change (e.g. stock is illiquid), it's still considered\nan extremum.","23279f81":"## Computing five minute indicators\n\nFinally, we can start processing five minute charts. First we'll discuss the reward part of trades\nwith a reward to loss ratio of at least 2. To squeeze the most out of a trade, all longs must be\nentered into when the price hits the bottom and exited when the price reaches the top.\nDitto for shorts with tops and bottoms changing their roles. All that means that the nearest local\nextremum is the most natural choice for a profit target.\n\nThis is the idea behind `targets_and_eta`: find all local extrema and then zig-zag through them,\nfrom a minimum to a maximum and vice versa, designating the next extremum as the profit target\nfor all points since the previous extremum. `extremum_order` is the number of candles on both\nsides that must be lower\/higher than the extremum. It controls how prominent price peaks and\ntroughs should be to be considered tradable. The default value was chosen to be five candles or\n25 minutes. Day trading is usually very fast, and trades lasting a couple of minutes or\neven seconds are not that uncommon, but faster trading requires one minute charts,\nwhich are unavailable.\n\nThe number of candles till the next extremum (ETA) is returned as well, indicating how long\nthe trade would take, which might be useful to predict along with the target. In addition\nETA encodes the trend direction: transitions from a minimum to a maximum (longs) have\npositive ETA, transitions from a maximum to a minimum (shorts)&mdash;negative.","20c86b06":"There are ~7k tickers to process.","eb11b1d1":"The entire dataset is processed in parallel. Dataframes for different tickers are concatenated\ninto one, with ticker as a level of a multi-index.","05e2ac0c":"## Putting it all together\n\nCombining daily processing with five minute processing is very simple. The following function\nreturns a list of (ticker, daily dataframe, five minute dataframe) for every consecutive slice\nof five minute data. If there are no gaps in data, list will have only one entry.\nOtherwise there'll be multiple entries disambiguated by appending a number to the ticker,\ne.g. `z0`, `z1`.","270ca3cb":"Fine-grained performance analysis can be done with line profiler. As it was pointed out\npreviously, `AgglomerativeClustering` is the bottleneck, which cannot be easily optimized.","f81973cd":"Let's finally take a look at the actual data.","31985cec":"## Basic data analysis\n\nHere are the processed dataframes. Worth, Target, and ETA columns must be predicted by the model,\nthe rest are features.","35b149b0":"As pointed out in comments, there are degenerate cases, which are best illustrated graphically.","1dd667a3":"Let's concentrate on day trading stocks, and ignore ETFs and hourly candles completely.\nThe primary timeframe for day trading is five minutes (one minute candles are not available).\nDaily candles provide the overall picture for the stock.","1f67552b":"Having finished with support\/resistance, let's turn to other indicators. First of all,\na fast implementation of EWMA is necessary; Pandas is an order of magnitude slower, again.","88fd9a36":"The following code picks random 30 candlesticks for a random stock, and plots support\/resistance\nlevels. As can be seen, the algorithm is not perfect, but it produces sensible results\nmost of the time.","babe5d2d":"Finally, we can start processing actual daily data. Recall the list of indicators\nto be computed:\n\n- Average Daily Volume (ADV). `bottleneck` provides a fast implementation of a moving average.\n- Average True Range (ATR).\n- Premarket Gap. Actual premarket data is unavailable, difference between the open and\n  the previous close is used as a proxy.\n- Previous Close (PCL).\n- Yesterday's and two days ago low and high (Y\/YY Low\/High).\n- Discretionary support\/resistance levels, which were extensively discussed above.\n\nThe most complicated part of `process_daily` is date handling. First of all, OHLC is data\nat the end of a day. At the beginning of a day, only open and yesterday's data is available,\nand that's what one is looking at before deciding to enter into a day trade. Shifting `Date`\ncolumn in a dataframe is slow. Instead, date shifting is done by using different index slices.\n\nRecall that there can be gaps in five minute data (function handles gaps in daily data as well,\nthough I haven't checked whether such gaps are actually present). Since the function could\nbe called multiple times for different contiguous date ranges of the same ticker, it does not\nread daily data itself to avoid reading it twice. Instead, daily dataframe must be read only once\nexternally and passed as an argument.\n\nThe function tries to make sure that for every date in a given date range there's at least\n`history` candles available in the output. Things get even more complicated since the computation\nof ADV, ATR and discretionary daily levels requires at least  `window` candles to prime\nthe indicators; before that number of candles is consumed, indicator value is NaN.\n\nIt is not guaranteed, however, that both `history` and `window` are available for all the dates;\na caller must be prepared to receive an incomplete dataframe, which starts later than\nthe requested `min_date`. Function returns `None` if history cannot be found even\nfor a single date.","10b7f516":"Having the information about market schedule, five minute dataframe can be split into\nconsecutive pieces. Data misses skew indicator calculations, so I decided to re-compute indicators\nfor each consecutive piece as if it's an entirely different stock.\n\nA piece must be at least 200 candles in order to compute MA(200). MA calculation always produces\nthe same result regardless of where it's started. Other indicators, such as EMA, do depend on\nthe starting point, but contribution of historical candles decays exponentially. EMA etc. computed\non > 200 candles should not differ much from true EMA computed from the beginning of time.\n\nDue to Pandas being slow, the function returns a struct of arrays with contiguous slices\nfor indexing purposes instead of returning contiguous pieces of a dataframe.","12977854":"Let's put it all together. Already discussed intraday levels are augmented with EMA(9), EMA(20),\nMA(50), MA(200), and discretionary support\/resistance levels. Together with daily levels and\nfive minute ATR they determine the stop loss and the target in a way described above, and,\nin turn, trades with reward to loss ratio of at least 2. RSI, used in intraday stock scanners,\nis also computed, then everything is combined into a single dataframe.\n\nI decided to keep both true targets and targets rounded to support\/resistance levels\n(indicator target). After all, an ML model might be better than humans at predicting the future.\nETA, which is a time a trade would take, might not be very useful for indicator targets since\na level below\/above the target is reached faster than the target itself.\n\nDatetime handling is much simpler compared to the daily case; basically we need to drop candles\nfor which either MA(200) is NaN or daily data is absent, taking into account that some of\ndaily data is kept for historic purposes only.","eaaf2d5e":"Let's return to the issue of filtering noise with Savitzky-Golay filter. SciPy's implementation\ndoes not cache coefficients; it's faster to request them upfront and correlate manually.\n\nBoundary effects are a bigger issue. SciPy provides the following modes to deal with them:\n`mirror`, `constant`, `nearest`, `wrap`, and `interp`. Subjectively, `interp` and `nearest` are\nthe best for support\/resistance computations, but `interp` wastes time on polynomial\ninterpolations, so `nearest` was chosen instead.\n\nStock predictions are made given the recent stock history. An ML model then moves to the next point\nin time and stock history window advances as well. Support\/resistance levels are computed within\nthat window, but instead of smoothing stock data per window it's more performance-friendly\nto smooth everything at once, and move window along already smoothed data. That's the reason why\n`price_levels` above expects its arguments to be smoothed, rather than doing the smoothing itself.\n\nHowever, in a global smoothing, boundary effects are present only at the very beginning and\nthe very end of data. At the end of every window, cross-correlation leaks data from\nthe next window. Such looks into the future must be avoided for the model to have any\nreal predictive power. To gain in performance yet avoid data leaks NumPy trickery is used to create\nan array of windows that are operated upon by a broadcasting `correlate`.","c6008ec9":"From the performance perspective, three points in the implementation of `price_levels`\nare worth mentioning.\n\nFirst, before trying to find extrema, prices are usually smoothed with Savitzky-Golay filter\nto reduce noise. `price_levels` assumes its arguments has already been smoothed. It speeds up\nwindowed computations of support\/resistance, see below for details.\n\nSecond, picking most frequently tested level from each cluster requires group by, and Pandas\ngroup by is super slow; proof follows. A fast implementation of group by is provided\nby `numpy_indexed`.\n\nThird, profiling shows that Scikit's `AgglomerativeClustering` dominates indicator computation.\nA custom implementation in Numba or Cython might possibly speed things up, but that wasn't tried.","5dc739f9":"A chance of winning at market open is higher, though.","71a567cf":"Here's an algorithm for finding discretionary support\/resistance levels:\n\n1. Levels where price changed direction: relative extrema.\n2. Tested multiple times: close to other prices in the same region. First, count the number\n   of prices whose difference with extrema is less than the threshold, which is specified as\n   a percentage of the total price range. Second, order extrema by these counts: the higher\n   the number the more the level was tested.\n3. Previous steps produce too many price levels. The number is reduced by coalescing close extrema\n   into clusters, and then picking the most frequently tested level from each cluster.\n\nThe use of high and low prices instead of closing prices is unconventional.\nTo quote from Andrew Aziz:\n\n> For day trading, it is better to draw support and resistance lines across the extreme prices\n  or wicks on daily levels&hellip; This is the complete opposite of swing trading. &hellip;\n  The close price of a stock on a daily chart is the price that the market makers and professional\n  traders have agreed on. Previous extreme high and low wicks have been made by day traders,\n  so you should look at those.","9a258a77":"On November 24, 2017 market closed early at 13:00 due to Thanksgiving. Everything after that\nis utterly incomplete post-market data with zero volume.\n\nSo not only misses should be filtered out, extra candles need to be filtered out as well.\nThe presence of early closes makes it problematic, since you can't just compare\nthe number of candles to 78.\n\n## Dealing with missing and extraneous data\n\nWe'll use `pandas_market_calendar` to find trading days and early closes in a given date range.\n\nA note on performance: `pandas_market_calendar` is unbelievably slow. But date ranges\nfor many tickers are identical, which makes market schedule functions eligible for caching.","7accb349":"Highest reward\/loss is for a stock that sold from \\\\$73 to \\\\$66, which looks plausible.\nSince all other reward\/loss ratios are lower, there are no outliers.","1043ccd9":"## The Goal\n\nMost ML stock predictors are trying to infer the next price from the previous ones.\nSuch an approach is considered flawed: since the next price does not differ much from the current,\nthe model simply learns to predict the current price as the next one. See e.g. [How (not) to use\nMachine Learning for time series forecasting: Avoiding the pitfalls](\nhttps:\/\/towardsdatascience.com\/how-not-to-use-machine-learning-for-time-series-forecasting-avoiding-the-pitfalls-19f9d7adf424).\n\nIf attempts to predict the next price fail, the natural question to ask is how do human day traders\nwork? There are many books on this subject, I've picked the following two, which will be used as\na reference:\n\n- A. Aziz - How to Day Trade for a Living\n- A. Aziz - Advanced Techniques in Day Trading\n\nHuman traders operate with a pair of (stop loss, profit target), which are usually based on\nindicators. Trades take place only if:\n\n- A reward (target &minus; current price) to loss (current price &minus; stop loss) ratio\n  is at least two.\n- There is a very high confidence that the stock will move in the predicted direction.\n\nAll these means, that instead of trying to accurately predict the next price, ML model should look\nfor high reward\/loss opportunities. High confidence means high precision, not accuracy: model can\nand should ignore potentially good trades to reduce the number of false positives. Exceptional\nprecision, however, isn't necessary, since for a reward\/loss ratio of at least two, breakeven\nis just 34%. Human level performance is ~60%.\n\nThe number of indicators used in day trading is small compared to e.g. swing trading, since you\ncan't attend quickly to all of them. Computerized systems are much faster than humans, but let's\nstart with indicators that humans use. Andrew Aziz uses the following indicators as potential\nstop losses or profit targets:\n\n- VWAP\n- Moving averages: EMA(9), EMA(20), MA(50), MA(200)\n- Day Low\/High\n- Closest half dollars and full dollars\n- Previous Close (PCL)\n- Yesterday's and two days ago low and high (Y\/YY Low\/High)\n- Discretionary support and resistance levels on daily charts\n\nIn addition, he uses some indicators in market scanners:\n\n- Average Daily Volume (ADV). Used to filter out illiquid stocks. Can be used as\n  a trading indicator as well, if the volume surges above the average.\n- Average True Range (ATR): expected price change during the day. Stocks whose prices stand still\n  are filtered out.\n- Premarket Gap. Stocks with good day trading opportunities are usually active in premarket.\n- Relative Strength Index (RSI) on five minute charts. Extreme values (> 90, < 10) show stocks\n  whose prices are either soaring or falling very rapidly.\n\nComputing these indicators on a large dataset (~7k tickers) is a performance problem, especially\nin Python. The goal of this notebook is the efficient processing of stock data, not machine\nlearning on stock data.\n\nThe primary performance offender is Pandas. Stock data is usually represented as Pandas dataframes\nwith a column for each indicator. Pandas operations, however, are very slow, whether it's indexing\nor column insertion or computations. Because of that, Pandas is used only for reading CSVs.\nAll data is converted into raw NumPy arrays, which are operated upon and reassembled back into\na dataframe only when computations are finished. Other performance points are in the rest of\nthe text.\n\n## Initial data exploration\n\nHere's the directory structure (`Data` subfolder contains duplicates and should be ignored):","3d165be3":"With a reward\/loss ratio of least 2, a predictor must have a minimal precision of 34% just to\nbreak even. Trading opportunities do not occur very often.","345a2c55":"Having an algorithm for profit target, let's turn to indicators. The following indicators\nare intraday, i.e. when a new day begins, computation is restarted:\n\n- Cumulative Volume. Indicator of trading activity: if the volume surpassed the average\n  daily volume mid-day, the stock is very active.\n- Day High\/Low. Can serve as a support\/resistance level.\n- VWAP. The most important support\/resistance level.\n- Profit Target & ETA are both intraday only, by the end of the day all positions are closed.\n  This is due to Andrew Aziz Rule \\#3: Never turn a day trade into a swing trade.\n\nIn addition to indicators, `intraday_data` returns `day_codes`, which mark candles belonging\nto the same day with the same integer. `day_codes` are used to broadcast levels computed\non daily charts into the same shape as the five minute levels, see `process_five_min` below.","1eeaa320":"A stock whose price changed from \\\\$1400 to \\\\$70 was filtered out due to data gaps, but we need\nto ensure that there are no others. Outliers in data must produce extreme reward\/loss ratios,\nlet's check them.","20b9014c":"The only way to evaluate the quality of a support\/resistance algorithm is pictorial.\n`mplfinance` is used to plot candlesticks.","5fe3b315":"Similarly to discretionary support\/resistance levels, targets finder can be evaluated\non a randomly picked stock on a random day if the full set of candles is available\n(remember, five minute data has gaps).","7f8f3ed9":"As it is easy to see, not only data is missing, but there is a huge drop from \\\\$1400 to \\\\$70\nin just fifteen minutes, which is obviously an outlier.\n\nI haven't found other stocks with outliers, but data misses are plentiful: you can find\n(stock, date) pairs with any number of candles from 0 to 78, the latter being the healthy amount\nof five minute candles.\n\nMissing data doesn't go well with indicator calculations, especially if the number of misses\nis large. Stocks with missing data must be filtered out, but the problem is subtler than it looks.\nTake a look at another example:","cefe6343":"Five minute data is trickier to handle. We need to construct a full set of five minute candles\nthat ought to be present between the given dates, and then take a set difference\nwith times in the dataframe to find missing or extraneous candles.\n\nTime of day is, by itself, an indicator: most activity occurs either in the first\nor in the last trading hour. A standard way to encode time is to use a\n$(\\sin(\\frac{2 \\pi t}{T}), \\cos(\\frac{2 \\pi t}{T}))$ pair, where $T$ is different\nfor regular trading days and early closes.","b60ea419":"Actually, SciPy has a function for computing relative extrema, but a Numba version is ~3x faster.\nRemember, these functions will be called multiple times for ~7k stocks.","e6daff39":"Not all tickers have both five minute and daily charts available. We'll limit out attention\nonly to those, which have both.","67cd5251":"Dataframes for `'a'` presented above show that daily data goes back as far as 1999\n(for that particular stock) while five minute data cover less than a month of 2017: from the middle\nof November to the beginning of December. So, even though the dataset is huge, actually there isn't\nthat much when it comes to day trading.\n\nIn addition, neither post-market nor premarket data is available. They are desirable to have\nnot only because of data continuity, but also because premarket price action is considered\nvery important for day trading.\n\n## Problems with the dataset\n\nLet's take a look at a particularly outrageous example:"}}