{"cell_type":{"6e9c7ac4":"code","1dec6761":"code","a95ec2fe":"code","abcef597":"code","93b50673":"code","645b1223":"code","edfbb64c":"code","68412265":"code","c95acb97":"code","8f61b00a":"code","7191db74":"code","19ba5f26":"code","498a4e7b":"code","09a8824d":"code","88bca337":"code","cee62893":"code","543579b5":"code","9d18d14d":"code","248579a8":"code","8e3e6110":"code","67f9f4c7":"code","62350752":"code","016c0070":"code","49307a8e":"code","5115a710":"code","eabc0451":"code","f3a17254":"code","2f80eeed":"code","977d08b9":"code","33c46003":"code","210c0c29":"code","e0dab88f":"code","158cea58":"code","402e71a1":"code","804c830b":"code","ebf59e3d":"code","1721b354":"code","824730c8":"code","27f0a09d":"code","d702f779":"code","c7bdc4ca":"markdown","ee48821a":"markdown","0442a38f":"markdown"},"source":{"6e9c7ac4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","1dec6761":"df = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")","a95ec2fe":"df.head()","abcef597":"df.shape","93b50673":"df.columns","645b1223":"df.info()","edfbb64c":"df.describe(include = \"all\")","68412265":"#Finding missing values\ndf.isnull().sum()","c95acb97":"df = df.drop([\"Unnamed: 32\"],axis = 1)","8f61b00a":"diag_group = df.groupby(\"diagnosis\")","7191db74":"print(\"Total number of rows diagnosed malignant: \", diag_group.get_group(\"M\").shape[0])","19ba5f26":"print(\"Total number of rows diagnosed benign: \",diag_group.get_group(\"B\").shape[0])","498a4e7b":"df = df.drop([\"id\"],axis = 1)\n#Encoding categorical data values\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ndf.diagnosis = le.fit_transform(df.diagnosis)\ndf.head()","09a8824d":"#Finding correlation between the features\nimport seaborn as sns\ncorr = df.corr()\nsns.heatmap(corr)","88bca337":"#fill true value array with shape of corr.shape[0]\n#print(np.full(corr.shape[0],True, dtype=bool))","cee62893":"#Compare correlation between the features and remove features that have correlation higher than 0.9\ncolumns = np.full((corr.shape[0],), True, dtype=bool)\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >= 0.9:\n            if columns[j]:\n                columns[j] = False\nselected_columns = df.columns[columns]\ndf = df[selected_columns]","543579b5":"df.shape","9d18d14d":"#Now we will calculate p-value for every predictor variable in regression model for given dataset and we can conclude from \n#p-value that if p-value is greater than 0.05 then that feature don't have any significant contribution to predict \n#cancer type . Hence the feature will be removed.\n\nX = df.iloc[:,1:]\nY = df.iloc[:,0]\nimport statsmodels.api as sm\nmod = sm.OLS(Y,X)\nfii = mod.fit()\np_values = fii.summary2().tables[1]['P>|t|']\np_values = pd.DataFrame({'feature': p_values.index, 'PVal': p_values.values})","248579a8":"p_values","8e3e6110":"threshold = 0.05\nfor i in range(len(p_values)):\n    if p_values.iloc[i].PVal > threshold:\n        df = df.drop(p_values.iloc[i].feature, axis = 1)","67f9f4c7":"df.shape","62350752":"df.columns","016c0070":"import seaborn as sns\n#radius_mean\nsns.boxplot(x = df['concavity_mean'])","49307a8e":"outlier = df[df['concavity_mean'] > 0.25]","5115a710":"df = df[df['concavity_mean'] < 0.25]","eabc0451":"sns.boxplot(x = df['concavity_mean'])","f3a17254":"df.shape","2f80eeed":"sns.boxplot(x = df['radius_mean'])","977d08b9":"outlier = df[df['radius_mean'] > 21]\ndf = df[df['radius_mean'] < 21]\nsns.boxplot(x = df['radius_mean'])","33c46003":"#splitting the dataset\nX = df.drop('diagnosis', axis = 1)\nY = df.diagnosis\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.6, random_state = 0)","210c0c29":"#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","e0dab88f":"#Logistic Regression Algorithm \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nclf = LogisticRegression(random_state = 0)\nclf.fit(X_train, Y_train)\nY_pred = clf.predict(X_test)\nclf_scores = []\ncm = confusion_matrix(Y_test, Y_pred)\nacc_logreg = accuracy_score(Y_test, Y_pred)\nclf_scores.append(acc_logreg * 100)\nprint(cm)\nprint(acc_logreg)","158cea58":"#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nlist1 = []\nfor neighbors in range(1,7):\n    clf = KNeighborsClassifier(n_neighbors=neighbors, metric='minkowski')\n    clf.fit(X_train, Y_train)\n    Y_pred = clf.predict(X_test)\n    list1.append(accuracy_score(Y_test,Y_pred))\nplt.plot(list(range(1,7)), list1)\nplt.show()","402e71a1":"clf = KNeighborsClassifier(n_neighbors = 6)\nclf.fit(X_train, Y_train)\nY_pred = clf.predict(X_test)\ncm = confusion_matrix(Y_test, Y_pred)\nacc_logreg = accuracy_score(Y_test, Y_pred)\nclf_scores.append(acc_logreg * 100)\nprint(cm)\nprint(acc_logreg)","804c830b":"#SVM\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor c in [0.5,0.6,0.7,0.8,0.9,1.0]:\n    clf = SVC(C = c, random_state=0, kernel = 'rbf')\n    clf.fit(X_train, Y_train)\n    Y_pred = clf.predict(X_test)\n    list1.append(accuracy_score(Y_test,Y_pred))\nplt.plot([0.5,0.6,0.7,0.8,0.9,1.0], list1)\nplt.show()","ebf59e3d":"from sklearn.svm import SVC\nclf = SVC(C = 0.8, random_state=0, kernel = 'rbf')\nclf.fit(X_train, Y_train)\nY_pred = clf.predict(X_test)\ncm = confusion_matrix(Y_test, Y_pred)\nacc_logreg = accuracy_score(Y_test, Y_pred)\nclf_scores.append(acc_logreg * 100)\nprint(cm)\nprint(acc_logreg)","1721b354":"#Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor leaves in range(2,10):\n    clf = DecisionTreeClassifier(max_leaf_nodes = leaves, random_state=0, criterion='entropy')\n    clf.fit(X_train, Y_train)\n    Y_pred = clf.predict(X_test)\n    list1.append(accuracy_score(Y_test,Y_pred))\n#print(mylist)\nplt.plot(list(range(2,10)), list1)\nplt.show()","824730c8":"clf = DecisionTreeClassifier(max_leaf_nodes = 8, random_state=0, criterion='entropy')\nclf.fit(X_train, Y_train)\nY_pred = clf.predict(X_test)\ncm = confusion_matrix(Y_test, Y_pred)\nacc_logreg = accuracy_score(Y_test, Y_pred)\nclf_scores.append(acc_logreg * 100)\nprint(cm)\nprint(acc_logreg)","27f0a09d":"clf_scores","d702f779":"import matplotlib.pyplot as plt\nx = clf_scores\ny = [\"LogisticRegression\", \"KNN\", \"SVM\", \"DecisionTree\"]\nplt.bar( y,x,color=['aqua', 'coral', 'gold', 'orchid'])","c7bdc4ca":"# Machine Learning Application","ee48821a":"# Feature Selection","0442a38f":"# Outliers  Identification"}}