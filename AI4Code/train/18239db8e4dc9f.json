{"cell_type":{"b326a899":"code","fa92cad0":"code","39e80d0a":"code","2e9bef77":"code","a548cf71":"code","e59693aa":"code","c3f1e669":"code","03e629d5":"markdown","ca88bb6c":"markdown","0a203a97":"markdown","2478eb90":"markdown","8134c234":"markdown","12798578":"markdown"},"source":{"b326a899":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.cluster import OPTICS, cluster_optics_dbscan\nimport matplotlib.pyplot as plt","fa92cad0":"dataset_dir = \"\/kaggle\/input\/kdddmproject\/data\/\"\n\nfilename = \"009_UCR_Anomaly_4000.txt\"\ndata_path = dataset_dir + filename\ndata = pd.read_csv(data_path, header=None)","39e80d0a":"# if data given in a row, convert to one data point per row\nif len(data) > 1: # time series data given in many rows\n    X = data\nelif len(data) == 1: # time series data given in one line\n    data = pd.read_csv(data_path, sep=\"  \", header=None, engine='python')\n    X = data.T\nelse:\n    print('bug')\n\n# display time series dataset\nplt.rcParams['figure.figsize'] = [20, 5]\nplt.figure()\nplt.plot(X)\nplt.show()","2e9bef77":"# Standardize X values\nX_mean = X.mean()\nX_std = X.std()\nX_norm = (X - X_mean) \/ X_std\nX_norm = X_norm.to_numpy() # convert to numpy array\n\n# Detrend by simple differencing\ndiff = []\nfor i in range(1, len(X_norm)):\n    value = X_norm[i] - X_norm[i - 1]\n    diff.append(value)\nX_norm = np.array(diff)\n\nplt.figure()\nplt.plot(X_norm)\nplt.show()","a548cf71":"# MinPts\nM = [5, 10, 50]\n# Percentile to choose eps\nP = [5, 10, 30, 50, 70, 90, 95]","e59693aa":"## Find distances for set M\nMinPts_eps = {} # hyperparameters (minpts, eps) candidates for each component in ensemble\nmodel_count = 0 # number of components in ensemble\n# generate hyperparameter pairs (minpts, eps)\nfor m in M:\n    D_q = []\n    # find average distance d_i from point i to its immediate neighbors in the window\n    # (the neighbor MinPts on one side and MinPts on the other side along the time axis)\n    for i in range(m, len(X_norm) - m, 1):\n        d_i = np.mean(abs(X_norm[i-m:i+m+1]-X_norm[i]))\n        D_q.append(d_i)\n    # 3.2. Find eps as P percentiles of D_q\n    for percentile in P:\n        eps = np.percentile(D_q, percentile)\n        if eps > 0:\n            model_count += 1\n            if m not in MinPts_eps.keys():\n                MinPts_eps[m] = [eps]\n            else:\n                MinPts_eps[m].append(eps)\n\n## Apply density-based clustering to all derived pairs of (MinPts,eps) and obtain binary tokens for each observation\nbinary_tokens_agg = np.zeros(len(X_norm))\nfor m in MinPts_eps.keys():\n    eps_s = MinPts_eps[m] # list of eps candidates for this minpts\n    # build the clustering model\n    model = OPTICS(min_samples=m, max_eps=eps_s[-1])\n    # train the model\n    model.fit(X_norm)\n    for eps in eps_s:\n        # produce the labels according to the DBSCAN technique with eps in eps_s\n        labels = cluster_optics_dbscan(reachability=model.reachability_,\n                                       core_distances=model.core_distances_,\n                                       ordering=model.ordering_, eps=eps)\n        # outlier points are labelled as -1,\n        # convert outlier points -1 to 1, and cluster points to 0\n        binary_tokens = abs(labels * (labels < 0))\n## Calculate abnormality estimate by combining results from individual components\n        binary_tokens_agg += binary_tokens\n        print('done clustering on MinPts_eps pair {}'.format((m,eps)))\nprob_anomaly = binary_tokens_agg \/ model_count # abnormality estimate","c3f1e669":"pred_outlier = 0\nbreakpoint = int(filename.split(\"_\")[-1].split(\".\")[0]) # split point for test data\noutlier_prob = max(prob_anomaly[breakpoint:]) # find the maximum abnormality estimate of this dataset\noutlier_index = np.where(prob_anomaly == outlier_prob) # find where does this maximum occur\n# if maximum occur at more than one point, pick first point in the test set\nif len(outlier_index)>0:\n    for i in outlier_index[0]:\n        if i >= breakpoint:\n            pred_outlier = i\n            break\n            \nimport warnings \nwarnings.filterwarnings('ignore')\n            \nplt.figure()\nplt.plot(X_norm, '-bo', markevery=[pred_outlier])\nplt.show()","03e629d5":"# Results\nFrom the model ensemble, each point has a predicted abnormality estimate. The point where the maximum abnormality estimate occurs is found and labelled as anomaly. According to KDD Cup 2021, there is only one time range in the test set in which anomaly occured. \n\nThe result is also visualized below.","ca88bb6c":"### Standardize and Detrend Data\nA standardized time series data has zero mean and one standard deviation. Each dataset was standardized using the equation\n$$\\hat{x} = {{x- \\mu } \\over {\\sigma} }$$\nwhere $\\mu$ is the mean of the time series and $\\sigma$ is the standard deviation of the time series.\n\n<br\/>\n\nThe data was also detrended using the simple differencing method. For each dataset, the value at the previous timestep is deducted from the current timestep.\n$$x'=x(t)-x(t-1)$$","0a203a97":"## Build Ensemble\nThe main idea is to use multiple models to predict the probability of a point being an anomaly. For example, if 10 models are used, and 7 of them predict Point P as an outlier, the abnormality estimate of Point P is 0.7. Details can be found in the original paper by Chesnokov (2019).","2478eb90":"# Model\n\nAn ensemble of OPTICS models is used. The ensembling method is an implementation of [Chesnokov, M.Y. Time Series Anomaly Searching Based on DBSCAN Ensembles. Sci. Tech. Inf. Proc. 46, 299\u2013305 (2019)][1]. The model predicts the probability of each point being an outlier. For faster running time using the scikit-learn library, OPTICS algorithm is used instead of DBSCAN.\n\n[1]: https:\/\/doi.org\/10.3103\/S0147688219050010 \"Chesnokov, M.Y. Time Series Anomaly Searching Based on DBSCAN Ensembles. Sci. Tech. Inf. Proc. 46, 299\u2013305 (2019).\"\n\n## Hyperparameters\n\nThe hyperparameters used is the same as those proposed in the original paper.","8134c234":"# Data\n\nKDD Cup 2021 details can be found at https:\/\/compete.hexagon-ml.com\/practice\/competition\/39\/. It contains 250 univariate time series datasets. Here only one dataset is used as an example for the anomaly detection algorithm.","12798578":"## Data Preprocessing\n### Managing data in different formats\nMost datasets are given in multiple rows with one value in each row (column datasets). However, there are a few datasets given in a single row with multiple values, separated by an empty space (row datasets). The row datasets are first converted into column datasets to ensure similar handling by the model."}}