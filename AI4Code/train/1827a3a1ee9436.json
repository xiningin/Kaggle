{"cell_type":{"da1a20dc":"code","8c96c53c":"code","1e52e1fc":"code","e93856d9":"code","27d32bc5":"code","7863a8b7":"code","a03bd973":"code","63fa110a":"code","5b043166":"code","74748524":"code","abcb1d2a":"code","4314274d":"code","a678b8ce":"code","b2b37cad":"code","ee125213":"code","3393de1c":"code","cf86f3b9":"code","5865f71c":"code","3b182536":"code","8cd9739c":"code","eeee742e":"code","b046b3eb":"code","85b33706":"code","1f07fd54":"code","90bb74e7":"code","a46ee49c":"code","caec1f75":"code","aeee09e4":"code","1c5349cc":"code","3017dc1f":"code","35736f04":"code","4224b1cf":"code","fa79660e":"code","52879c39":"code","9efa6452":"code","bdf199fe":"code","c62248dd":"code","34a20e84":"code","e1f1f157":"code","40d9f329":"code","88750803":"code","c446bd66":"code","e20c8ffa":"code","47675ce0":"code","4478a94d":"code","5bdcdfe9":"code","1f5d0440":"code","4a52ce51":"code","50338607":"code","106b5c3e":"code","363da8f0":"code","ce66424a":"code","f146ba65":"code","af4f2715":"code","0a1e5377":"code","cde87cc9":"code","cd6e456a":"code","c178809a":"code","a6b78b3b":"code","8bf01008":"code","824836a5":"code","5ec77227":"code","44a4f022":"code","30938998":"code","d7bd4604":"code","2370d8ba":"code","a15b509c":"code","ae502e50":"code","79a9b0d0":"code","06d5f564":"code","f5496114":"code","5256a948":"code","292dd1c3":"code","f2671653":"code","d93f34a7":"code","3cbf5a92":"code","bc44d044":"code","1be18691":"code","619110ac":"code","cfd4d4fb":"code","6f74382d":"code","e1e90932":"code","b39c952f":"code","131c8a90":"code","ab683132":"code","cf3176f7":"code","ffae9f96":"code","ee621fb0":"code","4dd458e2":"code","9826fee2":"code","75e4dbc4":"code","851e973a":"code","33ce128e":"code","209b8a1d":"code","ce02e762":"code","6ec67b40":"code","c851d68f":"code","e54fc9a5":"code","3acfcde9":"code","b800f420":"code","1c81c2e7":"code","a0060cf9":"code","87ba857f":"code","4b77bc9c":"code","f73d5f58":"code","df73cf9f":"code","e21fdcfe":"code","0a5acd8d":"code","f52d200a":"code","1231a1f4":"code","e8ddd74c":"code","ad11abd5":"code","e6a6fd1e":"code","f6bf89ba":"code","974bcb65":"code","5cb7740f":"code","f68cc442":"code","a3e3276a":"code","13627ba6":"code","cae195a4":"code","ca85a400":"code","c33c4b91":"code","56603446":"code","2da91976":"code","af495758":"code","2c932709":"code","6719f778":"code","a987f742":"code","28816ac4":"code","e0f17454":"code","1df9d9a7":"code","defe887d":"code","bc11982d":"code","58bc7be1":"code","54e8ee14":"code","93f1bf4c":"code","0880c7c1":"code","833eab0d":"code","dec72fd5":"code","19524ff1":"code","bf49fc54":"code","d1de740d":"code","bce8c537":"code","3a366842":"code","1d2b91d1":"code","21a4394c":"code","3876d9ec":"code","f7872eba":"code","21196d7e":"code","1cae4fb2":"code","30c5cada":"code","03ed5883":"code","fdbe50a1":"code","402fdeba":"code","042198ac":"code","a0791270":"code","365111ea":"code","9c2fe0f6":"markdown","cd3a068c":"markdown","07f3578e":"markdown","f4295639":"markdown","5cbd0e04":"markdown","c7a79913":"markdown","e94b5287":"markdown","03f25f23":"markdown","9b6d92cb":"markdown","e64eeff2":"markdown","e02119bb":"markdown","0f855325":"markdown","eabe9ce1":"markdown","12531622":"markdown","9b1e7454":"markdown","79014cae":"markdown","c4495f1d":"markdown","871eb957":"markdown","2290891e":"markdown","d060a3dc":"markdown","c4d8e73d":"markdown","e211f941":"markdown","7cd4158d":"markdown","4adfae2e":"markdown","dd030566":"markdown","ee858d8b":"markdown","64d4a243":"markdown","525ba16e":"markdown","264a9e2c":"markdown","d5f86ae9":"markdown","69c91727":"markdown","b4dd6c6d":"markdown","ba0a4ab5":"markdown","5445c3ed":"markdown","266c1c75":"markdown","8f9ba838":"markdown","8eb6cf0c":"markdown","eae16dc0":"markdown","17dc0199":"markdown","ee05b4cb":"markdown","932a3eb1":"markdown","9c15b560":"markdown","8b674c9a":"markdown","2c118d5d":"markdown","607625a8":"markdown","59bd1ee9":"markdown","9f30bac4":"markdown","121bfe75":"markdown","7c5aa785":"markdown","8c8c62fc":"markdown","c60a0622":"markdown","aae65d4f":"markdown","cd2a332b":"markdown","83ab4553":"markdown","74c6da25":"markdown","337b0639":"markdown","36fed894":"markdown","f3ce6583":"markdown","b9da5356":"markdown","542f8214":"markdown","f02c9579":"markdown","9f161a3f":"markdown","a2dfa8c8":"markdown","33d05c57":"markdown","cb7cb090":"markdown","a56fb4ea":"markdown","cd15c340":"markdown","7f5f3160":"markdown","9b77035f":"markdown","0f064610":"markdown","3ab5aafa":"markdown","f1cd2e69":"markdown","bc9d1093":"markdown","0b7e8108":"markdown","7910ad03":"markdown","81af419c":"markdown","631ae707":"markdown","feb8a79d":"markdown","39f00887":"markdown","8b783a5f":"markdown","155546c3":"markdown","4117ecfa":"markdown","ec1a7b84":"markdown","a454a2ff":"markdown","ed9ad3c5":"markdown","45ac86a0":"markdown","0f57f7d8":"markdown","8a62aafd":"markdown","c861a50a":"markdown","1db64c41":"markdown","0c6c4ff2":"markdown","748a4bb8":"markdown","7d811d6e":"markdown","a0204253":"markdown","564bd1f8":"markdown","bfa56e8b":"markdown","c571f4e8":"markdown","fe5ab24f":"markdown","3e8dca7d":"markdown","89f01dad":"markdown","65c8dd68":"markdown","c698f730":"markdown","15338cd6":"markdown","75f0c303":"markdown","dcf09f5f":"markdown","23dcc5f9":"markdown","9e88c3b7":"markdown","77c7a34a":"markdown","44120d21":"markdown","8364221a":"markdown","c6ef7f99":"markdown","2713cf7d":"markdown","9c824c55":"markdown","cd8603a8":"markdown","a4e4776a":"markdown","c50a84a5":"markdown","0d5b5782":"markdown","fc4b9878":"markdown","0f6293f3":"markdown","868ab931":"markdown","c1d28f5a":"markdown","27a09552":"markdown","0bb04d18":"markdown","a4ec655b":"markdown","83ca64ad":"markdown","b3d311c1":"markdown","bef9ad49":"markdown","31ea7468":"markdown","af744068":"markdown"},"source":{"da1a20dc":"import os\nimport sklearn\nimport pandas as pd\nimport pandas_profiling\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm\nimport catboost as cb\nimport xgboost as xgb\nimport optuna\nfrom datetime import datetime\nimport warnings\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.feature_selection import f_classif, mutual_info_classif\n\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.ensemble import StackingRegressor\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import f1_score, accuracy_score, roc_curve, roc_auc_score,plot_roc_curve\nfrom sklearn.metrics import classification_report, precision_score, recall_score, plot_precision_recall_curve\nfrom sklearn.metrics import confusion_matrix\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\n\n\n\nfrom sklearn.model_selection import cross_val_score\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.inspection import permutation_importance\n\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","8c96c53c":"#fix the version of the packages so that the experiments are reproducible:\n!pip freeze > requirements.txt","1e52e1fc":"# Lock a RANDOM SEED to keep experiments reproducible.\nRANDOM_SEED = 42","e93856d9":"# Set project colors\ncolors = ['#001c57', '#50248f', '#a6a6a6', '#38d1ff','#cc3181']\nsns.palplot(sns.color_palette(colors))","27d32bc5":"INPUT_PATH = ''\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        INPUT_PATH = dirname\n\nif INPUT_PATH == '':\n    raise Exception('The input path is empty!')\nelse:\n    print('\\nWorking dir:', INPUT_PATH)","7863a8b7":"def pandas_report(data):\n    ''' Function is called for generating of dataset profile-report'''\n\n    profile = data.profile_report(\n        title='Credit Scoring',\n        progress_bar=False,\n        correlations={\n            'pearson': {'calculate': True},\n            'spearman': {'calculate': True},\n            'kendall': {'calculate': False},\n            'phi_k': {'calculate': False},\n            'cramers': {'calculate': False}        \n        },\n        \n        interactions={\n            'continuous': True,\n            'targets': []\n        },\n        missing_diagrams={\n            'heatmap': True,\n            'dendrogram': True,\n            'matrix': True\n        },\n        vars={\n            'cat' : {'n_obs':10}\n        }\n    )\n    \n    return profile","a03bd973":"def get_boxplot(data, col1, col2, hue=None):\n    '''Function is called to plot boxplots'''\n    fig, ax = plt.subplots(figsize=(7, 5))\n    sns.boxplot(x=col1, y=col2, hue=hue, data=data, palette=colors)\n    plt.xticks(rotation=45)\n    ax.set_title(f'Boxplot for {col1} and {col2}', fontsize=14)\n    plt.show()","63fa110a":"class Preprocessing:\n    def __init__(self, data):\n        self.data = data\n\n    def label_encoder(self, column):\n        le = LabelEncoder()\n        self.data[column] = le.fit_transform(self.data[column])\n\n    def hot_enc(self, column):\n        ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n        aux_df = pd.DataFrame(ohe.fit_transform(self.data[[column]]))\n        aux_df.columns = ohe.get_feature_names([f'hot_{column}'])\n        self.data = self.data.drop(col, axis=1)\n        self.data = pd.concat([self.data, aux_df], axis=1)\n        return self.data     ","5b043166":"def missing_vars(data, col='education', random_proba=True):\n    '''Function is called for filling of missing data'''\n    # With using probability and random choise\n\n    if random_proba:\n        col_name = data[col].value_counts().index.to_list(\n        )  # get list of values\n        col_distr = data[col].value_counts(\n            normalize=True).values  # get l;ist of probs\n        missing = data[col].isnull()  # flag of missing val\n        # substitute values from the list of names in accordance with the probability of meeting the name\n        data.loc[missing, [col]] = np.random.choice(col_name,\n                                                    size=len(data[missing]),\n                                                    p=col_distr)\n\n    # Using  most common in  column\n    data[col] = data[col].fillna(data[col].value_counts().index[0])","74748524":"def corr_matrix(data, det=True, pltx=10, plty=10):\n    '''Funcion is called for making correlation matrix'''\n    \n    X = data.corr()\n    if det:\n        \n        evals,evec = np.linalg.eig(X)\n        ev_product = np.prod(evals)\n    \n        print(f'Rank of Matrix: {np.linalg.matrix_rank(X)}')\n        print(f'Determinant of matrix: {np.round(ev_product,4)}')\n        print(f'Shape of matrix: {np.shape(X)}')\n    \n    plt.figure(figsize=(pltx,plty))\n    sns.heatmap(X,vmin=0,vmax=.9,annot=True,square=True)\n    plt.show()","abcb1d2a":"def get_num_info(col, title=None):\n    '''Function is called to plot feture distribution'''\n\n    title = title if title is not None else f\"Distribution for '{col}\"\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5),)\n    fig = sm.qqplot(col, fit=True, line='45', ax=ax1)\n    fig.suptitle(title, fontsize=20)\n\n    sns.distplot(col.values, bins=20, color=colors[1], ax=ax2)\n    sns.violinplot(col.values, color=colors[3], bw=.3, cut=1, linewidth=4)\n\n    ax1.set_title('QQ-plot')\n    ax2.set_title('Distribution')\n    ax3.set_title('Violinplot')\n\n    plt.show()","4314274d":"def detect_outliers(data):\n    '''Function is called to detect outliers'''\n    q1, q3 = np.percentile(sorted(data), [25, 75])\n\n    IQR = q3 - q1\n\n    l_b = q1 - (1.5 * IQR) # lower bound\n    u_b = q3 + (1.5 * IQR) # upper bound\n    outl_count = len(data[data < l_b]) + len(data[data > u_b])\n\n    print(\n        f'Lower Bound: {round(l_b,3)}, Upper Bound {round(u_b,3)}, Outliers Count: {outl_count}')","a678b8ce":"'''Function that returns the ordinal day of the year'''\ndef get_days_count(x):  \n    day = ((x.month-1) * 30)+x.day\n    return day\n'''A function that returns the number of days between the order date and today's date'''\ndef get_days_beetwen(x):\n    curr_date = datetime.today()\n    count = (curr_date-x).days\n    return count\n    \n'''Function that returns the flag of filing an application on a weekend'''\ndef if_weekend(x):\n    if x.weekday() in [5,6]:\n        return 1\n    else:\n        return 0\n    \n'''Function that returns month'''\ndef month(x):\n    month = x.month\n    return month","b2b37cad":"def make_confusion_matrix(cf,\n                          group_names=['TN', 'FP', 'FN', 'TP'],\n                          categories='auto',\n                          sum_stats=True,\n                          count=True,\n                          cbar=True,\n                          percent=True,\n                          cmap='BuPu'):\n    '''Function is called for making a confusion matrix\n    args\n    ------\n    cf - confusion matrix\n    group_names - Names for each group\n    categories -  categories to be displayed on the x,y axis. Default is 'auto'\n    sum_stats -   shows Accuracies. Deafult is TRUE\n    c_bar -       If True, show the color bar. The cbar values are based off the values in the confusion matrix. \n                  Default is True\n    percent -     to be displayed on the x,y axis. Default is True\n    '''\n    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n    blanks = ['' for i in range(cf.size)]\n\n    group_labels = [\"{}\\n\".format(value) for value in group_names]\n\n    if count:\n        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n    else:\n        group_counts = blanks\n\n    if percent:\n        group_percentages = [\n            \"{0:.2%}\".format(value) for value in cf.flatten() \/ np.sum(cf)\n        ]\n    else:\n        group_percentages = blanks\n\n    box_labels = [\n        f\"{v1}{v2}{v3}\".strip()\n        for v1, v2, v3 in zip(group_labels, group_counts, group_percentages)\n    ]\n    box_labels = np.asarray(box_labels).reshape(cf.shape[0], cf.shape[1])\n\n    # Metrics\n    if sum_stats:\n        # Accuracy is sum of diagonal divided by total observations\n        accuracy = np.trace(cf) \/ float(np.sum(cf))\n        ball_accuracy = .5 * (cf[1, 1] \/ sum(cf[1, :]) +\n                              cf[0, 0] \/ sum(cf[0, :]))\n\n        # if it is a binary confusion matrix, show some more stats\n        if len(cf) == 2:\n            # pr = how many real true\n            precision = cf[1, 1] \/ sum(cf[:, 1])\n            # How many positives from all positives\n            recall = cf[1, 1] \/ sum(cf[1, :])\n            # F1 score\n            f1_score = 2 * precision * recall \/ (precision + recall)\n            stats_text = \"\\n\\nAccuracy={:0.3f}\\nBallancedAcc={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n                accuracy, ball_accuracy, precision, recall, f1_score)\n        else:\n            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n    else:\n        stats_text = \"\"\n\n    plt.rcParams.get('figure.figsize')\n    plt.figure(figsize=(5, 5))\n    sns.heatmap(cf,\n                annot=box_labels,\n                fmt=\"\",\n                cmap=cmap,\n                cbar=cbar,\n                xticklabels=categories,\n                yticklabels=categories)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label' + stats_text)","ee125213":"def make_roc_auc(model, X, y):\n    '''Plot ROC-AUC and PR curves'''\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n    plot_precision_recall_curve(model, X, y, ax=ax1)\n    plot_roc_curve(model, X, y, ax=ax2)\n    plt.show()","3393de1c":"def get_scores(report_df, model, X_test, y_test, name):\n    '''Create and add metrics into a pandas DF after experiment'''\n\n    report = pd.DataFrame(columns={'ROC-AUC'}, data=[0])\n    report['ROC-AUC'] = roc_auc_score(y_test,\n                                      model.predict_proba(X_test)[:, 1])\n    report['PR-AUC'] = precision_score(y_test,model.predict(X_test))\n    report['F1'] = f1_score(y_test, model.predict(X_test))\n    report['precision_Neg'] = precision_score(\n        y_test, model.predict(X_test), pos_label=0)\n    report['precision_Pos'] = precision_score(\n        y_test, model.predict(X_test), pos_label=1)\n    report['recall_Neg'] = recall_score(\n        y_test, model.predict(X_test), pos_label=0)\n    report['recall_Pos'] = recall_score(\n        y_test, model.predict(X_test), pos_label=1)\n\n    report.index = [name]\n    report_df = report_df.append(report)\n    return report_df","cf86f3b9":"def plot_cross_validate(model, X_train, X_test, y_train, y_test, cv=15):\n    '''Check overfitting of a model'''\n    \n    train_score = cross_val_score(model,\n                                  X_train,\n                                  y_train,\n                                  cv=cv,\n                                  scoring='f1',\n                                  verbose=False)\n    test_score = cross_val_score(model,\n                                 X_test,\n                                 y_test,\n                                 cv=cv,\n                                 scoring='f1',\n                                 verbose=False)\n\n    avg_f1_train, std_f1_train = train_score.mean(), train_score.std()\n    avg_f1_test, std_f1_test = test_score.mean(), test_score.std()\n    plt.figure(figsize=(10, 4))\n    plt.plot(\n        train_score,\n        label=f'[Train] F1-score: {avg_f1_train:.2f} $\\pm$ {std_f1_train:.2f}',\n        marker='.')\n    plt.plot(\n        test_score,\n        label=f'[Test] F1-score: {avg_f1_test:.2f} $\\pm$ {std_f1_test:.2f}',\n        marker='.')\n    plt.ylim([0.02, 1.])\n    plt.xlabel('CV iteration', fontsize=15)\n    plt.ylabel('F1-score', fontsize=15)\n    plt.legend(fontsize=15)","5865f71c":"def permutation_imp(model, X_test, y_test, num_f):\n    '''Function is called to show feature importance'''\n    perm = PermutationImportance(model, scoring='f1').fit(X_test, y_test)\n#     eli5.show_weights(perm, feature_names=X_test.columns.tolist())\n    perm_importance = eli5.explain_weights_df(perm).sort_values(\n        by='weight', ascending=False)\n    perm_importance = perm_importance[perm_importance['weight'] > 0]\n    perm_importance['f'] = perm_importance['feature'].apply(\n        lambda x: int(x[1:]))\n    cols_perm = list(X_test.columns[perm_importance['f']])\n    perm_importance['feature'] = cols_perm\n    \n    return perm_importance[:num_f]","3b182536":"df_train = pd.read_csv(INPUT_PATH + '\/train.csv')\ndf_test = pd.read_csv(INPUT_PATH + '\/test.csv')","8cd9739c":"# For the correct processing of features, combine train and test sets into a one dataset\n\ndf_train['sample'] = 1  # train\ndf_test['sample'] = 0  # test\n\ndf = df_test.append(df_train, sort=False).reset_index(\n    drop=True)  # combine sets","eeee742e":"display(df.sample(3))\ndf.info()","b046b3eb":"dtype_df = df.dtypes.reset_index()\ndtype_df.columns = ['Count', 'Column Type']\ndtype_df.groupby('Column Type').agg('count').reset_index()","85b33706":"for i, j in enumerate(df.columns):\n    print(j, type(df.loc[1][i]))","1f07fd54":"data_profile = pandas_report(df_train)\ndata_profile.to_file('result.html') # Check your folder)","90bb74e7":"bool_cols = [\n    'sex', 'car', 'car_type', \n    'good_work', 'foreign_passport'\n]\n\ncat_cols = [\n    'education', 'region_rating',\n    'home_address', 'work_address',\n    'sna','first_time'\n]\n\nnum_cols = [\n    'age', 'decline_app_cnt', 'income', \n    'bki_request_cnt', 'score_bki'\n]","a46ee49c":"plt.figure(figsize=[20, 20])\ni = 1\n\nfor k in bool_cols:\n    plt.subplot(4, 3, i)\n    sns.barplot(x=k,\n                y='proportion',\n                hue='default',\n                data=df_train[[k, 'default']].value_counts(\n                    normalize=True).rename('proportion').reset_index(),\n                palette=[colors[3], colors[4]])\n    plt.title('Binary Feature Name\\n' + k, fontsize=15)\n    i += 1\nplt.tight_layout()\nplt.show()","caec1f75":"# create an instance of  class\nencoder = Preprocessing(df_train)","aeee09e4":"for col in bool_cols:\n    encoder.label_encoder(col)\n\ndf_train.sample(3)","1c5349cc":"imp_bol = pd.Series(mutual_info_classif(df_train[bool_cols], df_train['default'],\n                                        discrete_features=True), index=[bool_cols])\nimp_bol.sort_values(inplace=True)\nimp_bol.plot(kind='barh')","3017dc1f":"corr_matrix(df_train[bool_cols])","35736f04":"# fill up missing vlues\nmissing_vars(df_train)","4224b1cf":"plt.figure(figsize=[20, 20])\ni = 1\n\nfor k in cat_cols:\n    plt.subplot(4, 3, i)\n    sns.barplot(x=k, y='proportion', hue='default',  data=df_train[[\n                k, 'default']].value_counts(normalize=True).rename('proportion').reset_index(), palette=[colors[3], colors[4]])\n    plt.title('Binary Feature Name\\n' + k, fontsize=15)\n    i += 1\nplt.tight_layout()\nplt.show()","fa79660e":"get_boxplot(df_train, 'education', 'region_rating')","52879c39":"get_boxplot(df_train, 'default', 'region_rating')","9efa6452":"get_boxplot(df_train, 'education', 'sna', hue='default')","bdf199fe":"# Encode categorical column for better interpretation of results.\nfor col in cat_cols:\n    encoder.label_encoder(col)","c62248dd":"imp_cat = pd.Series(mutual_info_classif(df_train[cat_cols], df_train['default'],\n                                        discrete_features=True), index=[cat_cols])\nimp_cat.sort_values(inplace=True)\nimp_cat.plot(kind='barh')","34a20e84":"corr_matrix(df_train[cat_cols])","e1f1f157":"for col in num_cols:\n    get_num_info(df_train[col], title=col)\n    detect_outliers(df_train[col])","40d9f329":"imp_num = pd.Series(f_classif(df_train[num_cols], df_train['default'])[\n                    0], index=[num_cols])\nimp_num.sort_values(inplace=True)\nimp_num.plot(kind='barh')","88750803":"corr_matrix(df_train[num_cols])","c446bd66":"df_train['app_date'] = pd.to_datetime(\n    df_train['app_date'])  # convert to  datetime\n\ndf_train['days_numb'] = df_train['app_date'].apply(\n    lambda x: (get_days_count(x)))  # Number of day in year\ndf_train['days_beetwen'] = df_train['app_date'].apply(\n    lambda x: (get_days_beetwen(x)\n               ))  # Number of days between application and current date\ndf_train['month'] = df_train['app_date'].apply(lambda x: (month(x)))","e20c8ffa":"# Drop app_date\ndf_train = df_train.drop('app_date', axis=1)","47675ce0":"df_train['month'].value_counts()","4478a94d":"# Add cols with dates to existing list of dedicated cols\nnum_cols = num_cols + ['days_beetwen'] + ['days_numb']\ncat_cols = cat_cols + ['month']","5bdcdfe9":"imp_num = pd.Series(f_classif(df_train[num_cols], df_train['default'])[0],\n                    index=[num_cols])\nimp_num.sort_values(inplace=True)\nimp_num.plot(kind='barh');","1f5d0440":"imp_cat = pd.Series(mutual_info_classif(df_train[cat_cols],\n                                        df_train['default'],\n                                        discrete_features=True),\n                    index=[cat_cols])\nimp_cat.sort_values(inplace=True)\nimp_cat.plot(kind='barh');","4a52ce51":"corr_matrix(df_train[cat_cols])","50338607":"corr_matrix(df_train.drop(['sample'], axis=1), det=False, pltx=20, plty=20)","106b5c3e":"# Split dataset\nX_n = df_train.drop(['default'], axis=1)\ny_n = df_train['default']\n\n# Build train and test sets\n\nX_train_n, X_test_n, y_train_n, y_test_n = train_test_split(X_n,\n                                                    y_n,\n                                                    test_size=.2,\n                                                    random_state=RANDOM_SEED)","363da8f0":"lr = LogisticRegression()\nlr.fit(X_train_n, y_train_n)\ny_pred = lr.predict(X_test_n)\ncf_matrix = confusion_matrix(y_test_n, y_pred)","ce66424a":"categories = ['Regular guy', 'DEFAULT']","f146ba65":"# make confusion matrix\nmake_confusion_matrix(cf_matrix,categories=categories,sum_stats=True)\n# make roc-auc curves\nmake_roc_auc(lr,X_test_n,y_test_n)","af4f2715":"f1_1 = f1_score(y_test_n,y_pred)\nprint(f1_1)","0a1e5377":"# fill up missing vars\nmissing_vars(df)\n\n# Drop client_id\ndf = df.drop(['client_id'], axis=1)\n\n# Encode binary features\nencoder = Preprocessing(df)\nfor i in list(['car', 'car_type']):\n    encoder.label_encoder(i)\n    \n# reduce size. Combine car and car_type. Encode it.\n# 0 - Has no car, 1-Has semi-car (Zhiguli), 3- has a car\ndf['car_comb'] = df['car'] + df['car_type']\ndf = df.drop(['car', 'car_type'], axis=1)\ndf['car_comb'] = df['car_comb'].astype('category')","cde87cc9":"df.sample(3)","cd6e456a":"# age\n# Devide age into categories: young,'not-young','midle','oldfag'\ndef age_to_cat(age):\n    if age <= 28:\n        cat_age = 'young'\n        return cat_age             \n    if 28 < age <= 35:\n        cat_age = 'not-young'\n        return cat_age\n    if 35 < age <= 50:\n        cat_age = 'midle'\n        return cat_age\n    if age > 50:\n        cat_age = 'oldfag'\n        return cat_age","c178809a":"df['age_cat'] = 0 # create age_categorical column\ndf['age_cat'] = df['age'].apply(lambda x:age_to_cat(x))\ndf = df.drop('age',axis=1)","a6b78b3b":"df.sample(3)","8bf01008":"# Check age distribution\nsns.countplot(df['age_cat'],palette=colors)\ndf['age_cat'].value_counts()","824836a5":"df['decline_cat'] = df['decline_app_cnt'].apply(lambda x: 4 if x >= 4 else x) \n\ndf['bki_request_cat'] = df['bki_request_cnt'].apply(lambda x: 7 if x >= 7 else x) # option 1\ndf = df.drop('decline_app_cnt',axis=1)\ndf = df.drop('bki_request_cnt',axis=1)","5ec77227":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# extract two columns from dataset\ndata = df[['work_address', 'home_address']].values\n\n# create Scaler instance\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(data)\n\n# We have two vectors. Reduce plane to line, taking most important info.\npca = PCA(n_components=1)\npca.fit(scaled_data)\npca_data = pca.transform(scaled_data)\ndf['pca_address'] = pca_data\n\n# drop used columns\ndf = df.drop(['home_address','work_address'],axis=1)","44a4f022":" # convert yo datetime\ndf['app_date'] = pd.to_datetime(\n    df['app_date'])\n\n # Num of days between application and current date\ndf['days_beetwen'] = df['app_date'].apply(\n    lambda x: (get_days_beetwen(x)\n               ))\n\n# Extract month\ndf['month'] = df['app_date'].apply(lambda x: (month(x)))\ndf = df.drop('app_date', axis=1)","30938998":"def has_no_garant(edu, grnt):\n    if edu == 'PGR' or edu == 'ACD':\n        grnt = 1\n        return grnt\n    else:\n        grnt = 0\n        return grnt","d7bd4604":"df['has_no_guarantor'] = 0\ndf['has_no_guarantor'] = df[['education', 'has_no_guarantor']].apply(\n    lambda x: has_no_garant(*x), axis=1)","2370d8ba":"df.sample(3)","a15b509c":"# Let's save a dataset for boosting models\ndf_boost = df.copy()","ae502e50":"# instance of preprocessing class\nencoder = Preprocessing(df)","79a9b0d0":"# label Encoder\nfor i in list(['sex', 'foreign_passport', 'good_work', 'has_no_guarantor']):\n    encoder.label_encoder(i)","06d5f564":"cols_to_hot = ['education','region_rating',\n               'sna','first_time','car_comb',\n               'decline_cat','bki_request_cat',\n              'month','age_cat'] \n\nfor col in cols_to_hot:\n    df = encoder.hot_enc(col)","f5496114":"cols_to_drop = ['hot_decline_cat_3','hot_region_rating_30',\n                'hot_education_PGR','hot_decline_cat_2',\n                'hot_region_rating_20','hot_sna_3',\n                'hot_decline_cat_4','has_no_guarantor',\n                'hot_bki_request_cat_6','hot_bki_request_cat_5', \n                'hot_bki_request_cat_2','hot_region_rating_60',\n                'hot_bki_request_cat_1', 'hot_bki_request_cat_4',\n                'hot_age_cat_not-young']\ndf = df.drop(cols_to_drop, axis=1)","5256a948":"cols_to_drop_b = ['month','foreign_passport','has_no_guarantor']\ndf_boost = df_boost.drop(cols_to_drop_b, axis=1)","292dd1c3":"# to avoid infinite or none val, let's add 5 to column\ndf['pca_address'] = df['pca_address'] + 5\n\ncols_to_log = ['days_beetwen','pca_address','income']","f2671653":"for col in cols_to_log:\n    df[col] = df[col].apply(lambda x: np.log(x) +1)","d93f34a7":"df.sample(3)","3cbf5a92":"# Extract the part of dataframe for testing\ndf_train = df.query('sample == 1').drop(['sample'], axis=1)\ndf_valid = df.query('sample == 0').drop(['sample'], axis=1)\n\n# Extract data and label\nX = df_train.drop(['default'], axis=1)\ny = df_train[['default']]\n\n# Extract data for validation\nX_valid = df_valid.drop(['default'], axis=1)","bc44d044":"scaler = RobustScaler()\ncols_to_scal = ['income', 'days_beetwen', 'pca_address']\n\nX[cols_to_scal] = scaler.fit_transform(X[cols_to_scal])\nX_valid[cols_to_scal] = scaler.transform(X_valid[cols_to_scal])","1be18691":"print(\n    f'Shape of X_train:{X.shape} \\nShape of X_Valid:{X_valid.shape} \\nShape of Target:{y.shape} '\n)","619110ac":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, shuffle=False, random_state=RANDOM_SEED)","cfd4d4fb":"# Build a model\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\n\n# make a prediction\ny_pred = lr.predict(X_test)\n\n# Make a confusion matrix\ncf_matrix = confusion_matrix(y_test, y_pred)","6f74382d":"# Set names for confusion matrix\ncategories = ['Regular guy', 'DEFAULT']","e1e90932":"# Plot confusion matrix\nmake_confusion_matrix(cf_matrix,categories=categories,sum_stats=True)\n# Plot ROC-AUC curves\nmake_roc_auc(lr,X_test,y_test)","b39c952f":"df_report = pd.DataFrame(data=None)","131c8a90":"df_report = get_scores(df_report, lr, X_test,\n                       y_test, 'LogisticRegression_Naive')","ab683132":"df_report","cf3176f7":"# class weigh - ballanced is to compensate unbalanced data\nlr_skf = LogisticRegression(class_weight='balanced')\nskf = StratifiedShuffleSplit(n_splits=6, random_state=RANDOM_SEED)\n\nparam = {'C': np.linspace(0.001, 10, 10), 'penalty': ['l1', 'l2']}\n\n# refit - True. Use best parameter when predicting\nclf_lr = GridSearchCV(\n    lr_skf, param, scoring='roc_auc', refit=True, cv=skf)\nclf_lr.fit(X_train, y_train)\n\ny_pred = clf_lr.predict(X_test)\ncf_matrix = confusion_matrix(y_test,y_pred)\n\nprint('Best roc_auc: {:.4}, with best C: {}'.format(\n    clf_lr.best_score_, clf_lr.best_params_))","ffae9f96":"# Plot confusion matrix\nmake_confusion_matrix(cf_matrix,categories=categories,sum_stats=True)\n\n# Plot ROC-AUC curves\nmake_roc_auc(clf_lr,X_test,y_test)","ee621fb0":"# write down results to our table\ndf_report = get_scores(df_report, clf_lr, X_test,\n                       y_test, 'LogisticRegression_skf')","4dd458e2":"df_report","9826fee2":"# For oversampling use whole dataset excluding validation set.\noversample = RandomOverSampler(sampling_strategy=1)\nX_, y_ = oversample.fit_resample(X, y)","75e4dbc4":"# Plot label distribution\nsns.displot(y_);","851e973a":"X_over, X_test_ov, y_over, y_test_ov = train_test_split(\n    X_, y_, test_size=.2, shuffle=True, random_state=RANDOM_SEED\n)","33ce128e":"lr_skf_over = LogisticRegression()\nskf = StratifiedShuffleSplit(n_splits=5, random_state=RANDOM_SEED)\n\nparam = {'C': np.linspace(0.001, 10, 10), 'penalty': ['l1', 'l2']}\n\n# verbose - show system info\n\nclf_lr_over = GridSearchCV(\n    lr_skf_over, param, scoring='roc_auc', refit=True, cv=skf, verbose=False)\nclf_lr_over.fit(X_over, y_over)\n\ny_pred_skf_over = clf_lr_over.predict(X_test_ov)\ncf_matrix = confusion_matrix(y_test_ov, y_pred_skf_over)\n\nprint('Best roc_auc: {:.4}, with best C: {}'.format(clf_lr_over.best_score_,\n                                                    clf_lr_over.best_params_))","209b8a1d":"make_confusion_matrix(cf_matrix,categories=categories,sum_stats=True)\nmake_roc_auc(clf_lr_over,X_test_ov,y_test_ov)","ce02e762":"df_report = get_scores(df_report, clf_lr_over, X_test_ov,\n                       y_test_ov, 'LogisticRegression_skf_over')","6ec67b40":"df_report","c851d68f":"plot_cross_validate(clf_lr_over,\n                    X_over,\n                    X_test_ov,\n                    y_over,\n                    y_test_ov,\n                    cv=5)","e54fc9a5":"permutation_imp(clf_lr_over, X_test_ov, y_test_ov, 15)","3acfcde9":"# For undrsampling use whole dataset excluding validation set.\nundersample = RandomUnderSampler()\nX_u, y_u = undersample.fit_resample(X, y)","b800f420":"# Plot label distribution\nsns.displot(y_u);","1c81c2e7":"X_under, X_test_un, y_under, y_test_un = train_test_split(\n    X_u, y_u, test_size=.2, shuffle=True, random_state=RANDOM_SEED\n)","a0060cf9":"lr_skf_under = LogisticRegression()\nskf = StratifiedShuffleSplit(n_splits=5, random_state=RANDOM_SEED)\n\nparam = {\n    'C': np.linspace(0.001, 10, 10),\n    'penalty': ['l1', 'l2', 'elasticnet']\n}\n\nclf_lr_under = GridSearchCV(lr_skf_under,\n                            param,\n                            scoring='roc_auc',\n                            refit=True,\n                            cv=skf,\n                            verbose=False)\nclf_lr_under.fit(X_under, y_under)\n\ny_pred_skf_under = clf_lr_under.predict(X_test_un)\ncf_matrix = confusion_matrix(y_test_un, y_pred_skf_under)\n\nprint('Best roc_auc: {:.4}, with best C: {}'.format(clf_lr_under.best_score_,\n                                                    clf_lr_under.best_params_))","87ba857f":"make_confusion_matrix(cf_matrix,categories=categories,sum_stats=True)\nmake_roc_auc(clf_lr_under,X_test_un,y_test_un)","4b77bc9c":"df_report = get_scores(df_report, clf_lr_under, X_test_un,\n                       y_test_un, 'LogisticRegression_skf_under')","f73d5f58":"df_report","df73cf9f":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\n\ny_pred_rf = rf.predict(X_test)\ncf_matrix = confusion_matrix(y_test,y_pred_rf)","e21fdcfe":"make_confusion_matrix(cf_matrix,categories=categories,sum_stats=True)\nmake_roc_auc(rf,X_test,y_test)","0a5acd8d":"df_report = get_scores(df_report, rf, X_test, y_test, 'RandomForestClassifier')","f52d200a":"df_report","1231a1f4":"# Grid Search\nrf_skf = RandomForestClassifier(class_weight='balanced')\nskf = StratifiedShuffleSplit(n_splits=5, random_state=42)\n\nparam = {\n    'bootstrap': [True],\n    'max_depth': [10, 30],\n    'n_estimators': [600, 1000]\n}\n\nclf_rf = GridSearchCV(rf_skf,\n                      param,\n                      scoring='roc_auc',\n                      refit=True,\n                      cv=skf,\n                      verbose=3,\n                      n_jobs=-1)\nclf_rf.fit(X_train, y_train)\n\ny_pred_rf_skf = clf_rf.predict(X_test)\ncf_matrix = confusion_matrix(y_test, y_pred_rf_skf)\n\nprint('Best roc_auc: {:.4}, with best C: {}'.format(clf_rf.best_score_,\n                                                    clf_rf.best_params_))","e8ddd74c":"make_confusion_matrix(cf_matrix,categories=categories,sum_stats=True)\nmake_roc_auc(clf_rf,X_test,y_test)","ad11abd5":"df_report = get_scores(df_report, clf_rf, X_test, y_test, 'RandomForestClassifier_skf')","e6a6fd1e":"df_report","f6bf89ba":"# Grid Search\nrf_skf = RandomForestClassifier()\nskf = StratifiedShuffleSplit(n_splits=5, random_state=42)\n\nparam = {\n    'bootstrap': [True],\n    'max_depth': [5, 10], #  depth of the tree\n    'n_estimators': [100, 200], # The number of trees in the forest\n    'max_features' : [4] # The number of features to consider when looking for the best split\n}\n\nclf_rf_over = GridSearchCV(rf_skf,\n                           param,\n                           scoring='roc_auc',\n                           refit=True,\n                           cv=skf,\n                           verbose=3,\n                           n_jobs=-1)\nclf_rf_over.fit(X_over, y_over)\n\ny_pred_rf_over = clf_rf_over.predict(X_test_ov)\ncf_matrix = confusion_matrix(y_test_ov, y_pred_rf_over)\n\nprint('Best roc_auc: {:.4}, with best C: {}'.format(clf_rf_over.best_score_,\n                                                    clf_rf_over.best_params_))","974bcb65":"make_confusion_matrix(cf_matrix,categories=categories,sum_stats=True)\nmake_roc_auc(clf_rf_over,X_test_ov,y_test_ov)","5cb7740f":"df_report = get_scores(df_report, clf_rf_over, X_test_ov, y_test_ov,\n                       'RandomForestClassifier_skf_over')","f68cc442":"df_report","a3e3276a":"# Check overfitting\nplot_cross_validate(clf_rf_over,\n                    X_over,\n                    X_test_ov,\n                    y_over,\n                    y_test_ov,\n                    cv=5)","13627ba6":"# Check permutation\npermutation_imp(clf_rf_over, X_test_ov, y_test_ov, 15)","cae195a4":"# Grid Search\nrf_skf = RandomForestClassifier()\nskf = StratifiedShuffleSplit(n_splits=5, random_state=42)\n\nparam = {\n    'bootstrap': [True],\n    'max_depth': [5, 10], #  depth of the tree\n    'n_estimators': [100, 200], # The number of trees in the forest\n    'max_features' : [4] # The number of features to consider when looking for the best split\n}\n\nclf_rf_under = GridSearchCV(rf_skf,\n                           param,\n                           scoring='roc_auc',\n                           refit=True,\n                           cv=skf,\n                           verbose=3,\n                           n_jobs=-1)\nclf_rf_under.fit(X_under, y_under)\n\ny_pred_rf_under = clf_rf_under.predict(X_test_un)\ncf_matrix = confusion_matrix(y_test_un, y_pred_rf_under)\n\nprint('Best roc_auc: {:.4}, with best C: {}'.format(clf_rf_under.best_score_,\n                                                    clf_rf_under.best_params_))","ca85a400":"make_confusion_matrix(cf_matrix,categories=categories,sum_stats=True)\nmake_roc_auc(clf_rf_under,X_test_un,y_test_un)","c33c4b91":"df_report = get_scores(df_report, clf_rf_under, X_test_un, y_test_un,\n                       'RandomForestClassifier_skf_under')","56603446":"df_report","2da91976":"# Extract the part of dataframe for testing\ndf_train_b = df_boost.query('sample == 1').drop(['sample'], axis=1)\ndf_valid_b = df_boost.query('sample == 0').drop(['sample'], axis=1)\n\nX_b = df_train_b.drop(['default'], axis=1)\nX_val = df_valid_b.drop(['default'], axis=1)\ny_b = df_train_b.default.values","af495758":"# convert object columns to Category\nfor c in X_b.columns:\n    col_type = X_b[c].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        X_b[c] = X_b[c].astype('category')","2c932709":"# Split training and test set\nX_train_b, X_test_b, y_train_b, y_test_b = train_test_split(\n    X_b, y_b, test_size=.2, random_state=RANDOM_SEED)","6719f778":"# prepare data for oversampling\noversample = RandomOverSampler(sampling_strategy=1)\nX_b_, y_b_ = oversample.fit_resample(X_b, y_b)","a987f742":"sns.displot(y_b_);","28816ac4":"# Split training and test oversampled set\nX_over_b, X_test_ov_b, y_over_b, y_test_ov_b = train_test_split(\n    X_b_, y_b_, test_size=.2, random_state=RANDOM_SEED)","e0f17454":"# Naive Model\nlgb = LGBMClassifier()\nlgb.fit(X_train_b, y_train_b, categorical_feature='auto')\n\ndf_report = get_scores(df_report, lgb, X_test_b,\n                       y_test_b, 'LGBMClassifier')\n\ny_pred_lgb = lgb.predict(X_test_b)\ncf_matrix = confusion_matrix(y_test_b,y_pred_lgb)","1df9d9a7":"make_confusion_matrix(cf_matrix,categories=categories,sum_stats=True)\nmake_roc_auc(lgb,X_test_b,y_test_b)","defe887d":"df_report","bc11982d":"lgb_skf = LGBMClassifier(class_weight='balanced',\n                         categorical_feature=[0, 1, 2, 4, 6, 7, 8, 9]) # specify indexes of cat_cols\nskf = StratifiedShuffleSplit(n_splits=5, random_state=RANDOM_SEED)  #5\n\nparam = {\n    'learning_rate': [0.005, 0.1],\n    'num_leaves': [30, 70],\n    'n_estimators': [50, 100],\n}\n\nclf_lgb = GridSearchCV(lgb_skf,\n                       param,\n                       scoring='roc_auc',\n                       cv=skf,\n                       verbose=3,\n                       n_jobs=-1)\n\nclf_lgb.fit(X_over_b, y_over_b)\n\ny_pred_lgb_clf = clf_lgb.predict(X_test_ov_b)\ncf_matrix = confusion_matrix(y_test_ov_b, y_pred_lgb_clf)\n\nprint('Best roc_auc: {:.4}, with best C: {}'.format(clf_lgb.best_score_,\n                                                    clf_lgb.best_params_))","58bc7be1":"df_report = get_scores(df_report, clf_lgb, X_test_ov_b, y_test_ov_b,\n                       'LGBMClassifier_skf_over')\nmake_confusion_matrix(cf_matrix,categories=categories,sum_stats=True)\nmake_roc_auc(clf_lgb,X_test_ov_b,y_test_ov_b)","54e8ee14":"df_report","93f1bf4c":"# Check overfitting\nplot_cross_validate(clf_lgb,\n                    X_over_b,\n                    X_test_ov_b,\n                    y_over_b,\n                    y_test_ov_b,\n                    cv=5)","0880c7c1":"lgb_skf_1 = LGBMClassifier(class_weight='balanced',\n                           # specify index of cat_feature\n                         categorical_feature = [0, 1, 2, 4, 6, 7, 8, 9],\n                         learning_rate = 0.1,\n                         num_leaves = 70,\n                         n_estimators = 100\n                        )\nlgb_skf_1.fit(X_over_b, y_over_b)","833eab0d":"# Check permutation\nlightgbm.plot_importance(lgb_skf_1, ax=None, height=0.2, xlim=None, ylim=None,\n                         title='Feature importance', xlabel='Feature importance', \n                         ylabel='Features', importance_type='split', max_num_features=None,\n                         ignore_zero=True, figsize=None, dpi=None, grid=True, precision=3)","dec72fd5":"def objective(trial):\n    '''Function to find best tuning params'''\n\n    # Copy test and trainsets\n    X_train_cb = X_over_b\n    y_train_cb = y_over_b\n    valid_x = X_test_ov_b\n    valid_y = y_test_ov_b\n\n    # Specify which parameters we would search\n    param = {\n        'cat_features': [0,1,8,9,10],\n        \n        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"used_ram_limit\": \"3gb\"\n    }\n    \n    gbm = cb.CatBoostClassifier(**param)\n    # Set early_stop rounds to avoid overfitting\n    gbm.fit(X_train_cb, y_train_cb, eval_set=[\n        (valid_x, valid_y)], verbose=0, early_stopping_rounds=20)\n        \n    preds = gbm.predict(valid_x)\n    pred_labels = np.rint(preds)\n    # We will maximize ROC_AUC metric. But you can select any other\n    roc_auc_score = sklearn.metrics.roc_auc_score(valid_y, pred_labels)\n    \n    return roc_auc_score","19524ff1":"if __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=20, timeout=600)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","bf49fc54":"cat_features = [0,1,8,9,10]\n\ncb = CatBoostClassifier(\n    objective = 'CrossEntropy',\n    iterations=50,\n    random_seed=42,\n    learning_rate=0.33,\n    colsample_bylevel = 0.09954484897533786,  #test good\n    depth= 12, #12\n    custom_loss=['AUC'],\n    early_stopping_rounds=20,\n    boosting_type='Plain'\n)\ncb.fit(\n    X_over_b, y_over_b,\n    cat_features=cat_features,\n    eval_set=(X_test_ov_b, y_test_ov_b),\n    verbose=False,\n    plot=False\n)","d1de740d":"y_pred_cb = cb.predict(X_test_ov_b)\ncf_matrix = confusion_matrix(y_test_ov_b, y_pred_cb)","bce8c537":"df_report = get_scores(df_report, cb, X_test_ov_b, y_test_ov_b,\n                       'CatBoost')\nmake_confusion_matrix(cf_matrix,categories=categories,sum_stats=True)\nmake_roc_auc(cb,X_test_ov_b,y_test_ov_b)","3a366842":"df_report","1d2b91d1":"def objective(trial):\n    '''Function to find best tuning params for XGboost'''\n\n    # Copy test and trainsets\n    X_train_ = X_over\n    y_train_ = y_over\n    valid_x = X_test_ov\n    valid_y = y_test_ov\n    dtrain = xgb.DMatrix(X_train_, label=y_train_)\n    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n    \n    param = {\n        \"verbosity\": 0,\n        \"objective\": \"binary:logistic\",\n        # use exact for small dataset.\n        \"tree_method\": \"exact\",\n        # defines booster, gblinear for linear functions.\n        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n        # L2 regularization weight.\n        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n        # L1 regularization weight.\n        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n        # sampling ratio for training data.\n        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n        # sampling according to each tree.\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n    }\n    \n    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n        # maximum depth of the tree, signifies complexity of the tree.\n        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n        # minimum child weight, larger the term more conservative the tree.\n        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n        # defines how selective algorithm is.\n        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n\n    if param[\"booster\"] == \"dart\":\n        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n        \n    bst = xgb.train(param, dtrain)\n    preds = bst.predict(dvalid)\n    pred_labels = np.rint(preds)\n    metric = sklearn.metrics.roc_auc_score(valid_y, pred_labels)\n    return metric\n","21a4394c":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=40, timeout=600)","3876d9ec":"param_xb = study.best_params","f7872eba":"clf_xb = xgb.XGBClassifier(param = param_xb)\n\nclf_xb.fit(X_over,y_over)","21196d7e":"y_pred_xb = clf_xb.predict(X_test_ov)\ncf_matrix = confusion_matrix(y_test_ov, y_pred_xb)","1cae4fb2":"df_report = get_scores(df_report, clf_xb, X_test_ov, y_test_ov,\n                       'XGBoost')\nmake_confusion_matrix(cf_matrix,categories=categories,sum_stats=True)\nmake_roc_auc(clf_xb,X_test_ov,y_test_ov)","30c5cada":"df_report","03ed5883":"#test good.\nlgbm_ = LGBMClassifier(learning_rate=0.1, n_estimators=100, num_leaves=70)\nrf_ = RandomForestClassifier(bootstrap=True, max_depth=10, n_estimators=200, max_features=4)\nlr_ = LogisticRegression(C=1.1119999999999999, penalty='l2')\ncb_ = CatBoostClassifier(\n    objective = 'CrossEntropy',\n    iterations=50,\n    random_seed=42,\n    learning_rate=0.33,\n    colsample_bylevel = 0.0995448,\n    depth= 3,\n    custom_loss=['AUC'],\n    early_stopping_rounds=20,\n    boosting_type='Plain')\n\nclf_st = VotingClassifier(estimators=[('lr', lr_), ('rf', rf_),\n                                      ('lgb', lgbm_),('cb', cb_)],\n                          voting='soft')\nclf_st.fit(X_over, y_over)","fdbe50a1":"y_pred_clf_st = clf_st.predict(X_over)\ncf_matrix = confusion_matrix(y_over,y_pred_clf_st)","402fdeba":"df_report = get_scores(df_report,clf_st, X_over,y_over, 'Voting')\nmake_confusion_matrix(cf_matrix, categories=categories, sum_stats=True)\nmake_roc_auc(clf_st,X_over, y_over)","042198ac":"df_report","a0791270":"id_test = df_test['client_id']\nid_test","365111ea":"submission = pd.DataFrame({'client_id': id_test,\n                           'default': clf_st.predict_proba(X_valid)[:, 1]})\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission","9c2fe0f6":"---\n\nWell, the model started to predict better than before. Sensitivity (recall) is noticebly grown. F1 score also grown. Proportion of positive samples (precision) that were correctly classified dropped. Our model incorrectly classified default clients (FP=28%), while TP is only 8.5%\n\nSure it is better, but not good enough\n\nLet's try to add objects of the missing class for balancing dataset\n\n---","cd3a068c":"It's interesting, are we able to observe some relationships between education level and the connection of borrowers with another clients of a bank? Let's see that.","07f3578e":"client_id -  client ID\n\neducation - the level of education\n\nsex - borrowe's gender\n\nage - borrowers age\n\ncar - has(has no) car\n\ncar_type - flag of car type\n\ndecline_app_cnt - number of rejected past applications\n\ngood_work - good job flag\n\nbki_request_cnt - number of requests to Credit bureau\n\nhome_address - home address categorizer \n\nwork_address - work address categorizer \n\nincome - borrower's income\n\nforeign_passport - has (has no) traveling passport\n\nsna - Relationships of borrower with the bank's clients\n\nfirst_time - how old the information about the borrower was\n\nscore_bki - score based on data from Credit bureau\n\nregion_rating - region rating\n\napp_date - application date\n\ndefault - has (has no) default","f4295639":"In this and next section (9.4) we try to balance our datasets using RandomOverSampler.\n\nLet's prepare dataset for oversampling","5cbd0e04":"<a id=\"sec9.2\"><\/a>\n## [9.2 LOG REGRESSION (CrosValid & GridSearch)](#sec9.2)\n[(INDEX)](#sec2)","c7a79913":"Well, most significant feature to client's default is ***foreign_passport*** and ***car_type***","e94b5287":"The Main target of the project is to classify default clients (who tend to overdue payment). The model shall return probability of classified client.","03f25f23":"<a id=\"sec9.9.2\"><\/a>\n## [9.9.2 LightGBM](#sec9.9.2)\n[(INDEX)](#sec2)","9b6d92cb":"### [DECLINE & bki_request count TO CATEGORICAL]","e64eeff2":"We will perform same workflow for boosters. Booster produces a prediction model in the form of an ensemble of weak prediction models. \n\nIn other words, it is learning on its own errors which it made in a previouse iteration.\n\nBoosters work much faster than descicion trees.","e02119bb":"---\n\nWell, definatelly we are.\nPeople, mostly with a low level of education, have some kind of relationships with the bank's clients. Perhaps this can be explained by the fact that people with low education grade are working in large teams (factories, plants) and the exchange of information is better, **or** people with higher education do not need guarantors.\n\n---","0f855325":"Proceed with date columns.","eabe9ce1":"<a id=\"sec9.5\"><\/a>\n## [9.5 Random Forest_Naive](#sec9.5)\n[(INDEX)](#sec2)","12531622":"---\n\nIn plots we can see that people with low-grade education fail to return loans more often than people with high-grade education. But in other hand, they borrow more oftenly.\n\nThe feature ***region_rating*** gives an interesting insight. The lower rating of the region, the higher share of overdue loan payments. Except two low grade. Perhaps it is some industrial regions.\n\n***Home address*** and ***work addres*** are almost the same. We just can see that share of failed borrowers different from each address. Probably, it has some insights. We will check later.\n\nThe category 4 in ***sna*** feature has a higher ratio of failed borrowers. And it falls by reducing the category from 4 to 1.\n\n***First time*** columns also shows us the fall of share of failed borrowers.\n\n---","9b1e7454":" - Dataset has 19 features (excluding feature ''Sample)\n \n - Total Number Of observations is 110 148\n \n - Missing values is about 0.04%\n \n - There are no duplicates\n \n \n - ***Client_id*** has 100% unique values \n \n - ***app_date*** has only 120 distinct values (0.1%). Most of presented dates concentrated in MARCH2014, APRIL2014, FEB2014. We need to process this column in further.\n \n - ***education*** Consists 5 distinct categories: \n     \n - - SCH (52%) - School;\n \n - - GRD (31%) - Graduated (Master degree);\n \n - - UGR (13%) - UnderGraduated (Bachelor degree);\n \n - - PGR (1.7%) - PostGraduated;\n \n - - ACD (0.3%) - Academic Degree;\n \n - - Missing (0.4%)\n \n \n We will see how it is impact on a result.'\n \n - ***sex*** Consists 2 distinct categories:\n \n - - Female (56%);\n \n - - Male (44%);\n \n \n - ***age*** Is a real Number variable. The distribution is a left-biased.\n \n -- Minimum \t21\n \n -- median\t37\n \n -- Mean\t39.2\n \n -- Maximum \t72\n \n -- Interquartile range (IQR)\t18\n \n - ***car*** Is a boolean variable. 67% of borrowers has no car.\n \n \n - ***car_type*** Is a boolean variable which shows whethere the borrower has domestic or foreign car. 81% of borrowers has domestic cars.\n \n \n - ***decline_app_cnt*** Is a real number value. Distribution is shifted to the left. Most of values (83%) has zeros which is reasonable. Overwhelimng majority of the observations are located between 0 and 6. We might convert this feature to categorical.\n \n -- Maximum\t33\n\n\n - ***good_work*** Most of borrowers have not good job (83%). It is interesting it here any relations between good job and education? We also want to know, does someone has poor education, bad job and foreign car (Just for fun...Maybe...)\n \n \n - ***score_bki*** Is a real number variable. It consists 93% distinct values. We may note, that it has approximately normal distribution and also consists negative values. Most likely it is already scaled by Standard Scaler.\n \n \n - ***bki_request_cnt*** Is a real number variable with max.requests - 53, min - 0 and median - 1. Most of values scattered from 0 to 8 attempts.\n \n - ***region_rating*** It has kind of grade from 20 up to 80. We would say it is more similar with categoreical feature. Most common rating is 50 (37%)\n \n - ***home_address, work_address*** Is a categorical data with 3 categories.\n \n - ***income*** Biased distribution with large scatter of values - from 1000 to 1 000 000. Here we can try to convert it in a categorical value, assigning the income values to the categories - low income, medium, above medium, etc. \n \n - ***sna \/ first_time***  Are categorical features and have 4 categories.  Nothing intersting seems.\n \n - ***foreign_passport*** Is a boolean feature. 67% of borrowers have foreign passport\n \n \n - ***default*** is our target which we have to classify. It is a boolean var, with overwhelming majority of 'regular guys' who always returns their loans. We may note that it is unballanced and the ratio is 1:6.8. In the feature engineering we will try apply under and oversampling and will results.","79014cae":"Let's see is there any relationships between target data and dates.","c4495f1d":"<a id=\"sec9.1\"><\/a>\n## [9.1 LOG REGRESSION (naive)](#sec9.1)","871eb957":"<a id=\"sec6.1\"><\/a>\n## [6.1 Binary feature analysis](#sec6.1)\n[(INDEX)](#sec2)","2290891e":"We can see that for this particullar dataset LGBM classify better that catboos. Precision and recall more balanced. Right after LGBM is Random forest. \u0421at Boost disappointed us a litle. \n\nLet's try one more Algorithm. XG_boost","d060a3dc":"<a id=\"sec5.2\"><\/a>\n## [5.2 Pandas Profiling](#sec5.2)\n[(INDEX)](#sec2)","c4d8e73d":"---\n\n**Binary:**\n\n - ***Car*** and ***car_type*** strongly correlated.\n\n - The column ***car*** indicates the presence of a car, and in the column ***car_type*** the presence of a foreign car. Combine them further\n\n - Quantity of male and female default borrowers almost the same. However, female tend to loan money more oftenly.\n\n**Category:**\n\n - People with low-grade education fail to return loans more often than people with high-grade education.\n\n - More relationships of borrower with another clients - more overduie payments. Feature ***has no guarantors*** can be added to people with high-level education\n\n - People with high level of education prefer to live in a region with higher level\n\n - The higher rating of the city, the lower chance of default\n\n - Correlation between ***sna*** and ***first_time***, ***Home*** and ***work addresses***.\n \n**Numerical:**\n\n - ***score_bki*** has a distribution close to normal. It is already scaled by STD (most likely)\n \n - Data has outliers. However, it is better to convert some features into a categorical type\n \n - There are no strong correlations between features\n \n - ***car_type*** has correlation with income\n \n - ***sna*** has quite strong correlation with ***foreign_passport***\n \n \n***Statistically most significant:***\n\n - foreign_passport\n\n - car_type\n\n - sna\n\n - first_time\n\n - region_rating\n \n - Score_bki \n \n - decline_app_cnt.\n \nAs we have lot's of no obvious correlation and connection between features, it is better to use ***descicion tree models, or logistic regressions.***\n\n---","e211f941":"---\n\nRandomForest shows good results.We increased F1 and PR-AUC. Precision and Recall for positive classes also increased. So far we got besy results.\n\nCross validation shows that our model is not overfitted.\n\nFeature importance has some differences with LogRegression but not significant. It is shows us that these algorithms works not in the same way. For example, random forest takes randomly 4 features (n_features=4) and build a tree. So, for this method feature imp. may differ with logregressor\n\n---","7cd4158d":"Check a significance of features","4adfae2e":"---\n\nWe can see that our accuracy is about 88% Cool. Let's submit our prediction...\n\nActually it is a bit early to make such decision. Accuracy depends on how many correct classifications our model made. But, our dataset is not balanced, and we are able to classify only negative class (client's who would return loan) and we are not able to predict default clients for reason that our model learned only negitive classes. \n\n\nWith a reference to a business target, we have to classify positive class, so then we need to take care about Precision and Recall. \n\nF1 score is 0.\n\nArea under Curve (AUC) is 0.58. Although, in the ideal case, the value should tend to 1.\n\nPrecision-Recall Area is very low.\n\nWell, let's try to improve results. We will do some feature engineering in the next section.\n\n---\n\n","dd030566":"Firstly, we use label encoder to convert binary features","ee858d8b":"As we remember, in DATA analysis we found, that ***home_address*** and ***work address*** are correlated (.7). Do vector space reducion using sklearn.decomposition module. Lets try principal component analyse method.","64d4a243":"---\n\nWe have quite stong correlation between ***sna*** and ***first_time***. Also, these two columns has best significance to a target variable. We may guess, that these two features can be explained as follows: This person has long been a client of the bank and, as a result, has acquired connections with other clients (possibly guarantors ).\n\nHome and work addresses also correlated. We need to think what can we do with these features.\n\n---","525ba16e":"After some of experiments it is decided to drop following cols ","264a9e2c":"Well, let's skip one step and try to apply parameter's search in oversampled set with cross-validation.\n\nAll do in one cell","d5f86ae9":"Let's see, how the region rating depends on education","69c91727":"Sort out **decline_app_cnt** and ***bki_request_cnt***  by groups:","b4dd6c6d":"<a id=\"sec9.3\"><\/a>\n## [9.3 LOG REGRESSION Oversampling](#sec9.3)\n[(INDEX)](#sec2)","ba0a4ab5":"---\n\nWith lack of data RF shows bit worse results than in a oversampled dataset. \n\nAlso, there is a risk to be *overfitted* when we have small datasets.\n\n---","5445c3ed":"Let's start with Logistic Regression.\nBuild a naive model to check what we have and what can we do. Also, creation of naive model helps us to evaluate metrics after we improve or modify dataset.","266c1c75":"Some of boosters can work with categorical features. Then we try to use LightGBM. \n\nOne of the advantages of such models is that you do not need to encode features by yourself.\n\nSome of them like LightGBM will encode it automatically.\n\nLet's prepare dataset for trainig which we saved for this reason in the feature engineering section exactly for this reason. It has not encoded columns.","8f9ba838":"<a id=\"sec8\"><\/a>\n# [8. Feature Engineering](#sec8)\n[(INDEX)](#sec2)","8eb6cf0c":"### [CAR_CAR_TYPE]","eae16dc0":"---\n\nAs we may see, results are much better. However, due to lack of the data (our dataset presents only 4 month), our model is still not able to classify positive class perfectly.\n\nIn this Notebook we will try to find better solution with small amount of data.\n\nLet's check Overfitting of our model and Feature importance.\n\n---","17dc0199":"With a reference to EDA section we made assumption, that people with high degree don't need to have guarantors.Let's add new feature - has no guarantors.","ee05b4cb":"### [AGE TO CATEGORICAL]","932a3eb1":"As we might notice, using sklearn GridSearch is not best option for tuning our model for ***several reasons***:\n\n- time consumption. It is required lots of time. \n- CPU resources. It is consumption a lot of CPU capacity and as a result - energy waste.\n- Limited Search range. Sometimes we don't know in what in what range shall we search numbers for one or the other parameter. For example number of estimators in RandomForest. By preparing this project we got ***overfitting*** and ***underfitting*** infinite number of times ( and plus all above it need to wait huge amount of ***time*** while GridSearch veify all set parameters. It is annoying a lot)\n\n***GridSearch*** is ok for people who has plenty of time :)\n\nIn this small section we try to use better solution for tuning model. Let's use ***Optuna*** and other Booster  - ***CatBooster***\n\nWe do tuning with oversampled set. While we prepared this project, we tried to use all types of data such as unbalanced, undersampled etc.","9c15b560":"Try to do the same with undersampling","8b674c9a":"---\n\nWell, Score+bki is most important column for our regression model. Second place takes PCA_address. This column what we created by combination information from two correlated columns work and home address. \n\nAs we suspected, the region rating also important feature. Higher (80)region takes 3d place in importance feature table.\n\nCar_comb also quite important. People without any car somehow impact on the classification.\n\nWill **will delete** unimportant features. (In above dection already deleted)\n\n---","2c118d5d":"Let's see full correlation matrix","607625a8":"<a id=\"sec11\"><\/a>\n# [SUBMISSION](#sec11)\n[(INDEX)](#sec2)","59bd1ee9":"<a id=\"sec6.5\"><\/a>\n## [6.5 Draw a conclusion](#sec6.5)\n[(INDEX)](#sec2)","9f30bac4":"---\n\nWe can see the complete correlation of the **client_id** and **days_numb** features (the ordinal number of the day in the year). This indicates a certain system of assigning a customer number. Most likely, these signs will give us anything useful.\n\n***car_type*** has correlation with income. It can be explained by assumption that if borrower has more income and as a result he has better car and vice versa.\n\n***sna*** has quite strong correlation with ***foreign_passport***\n\n---\n","121bfe75":"Added feature **days_beetwen** has no significant impact. However it has more than age. Keep this feature.","7c5aa785":"Let's check feture importance","8c8c62fc":"#### Link for the  **[the dataset profile report](.\/result.html)**.","c60a0622":"<a id=\"sec9.7\"><\/a>\n## [9.7 Random Forest_OverSampling](#sec9.7)\n[(INDEX)](#sec2)","aae65d4f":"Let's try to apply GridSearch of parameters with crossvalidation","cd2a332b":"Nope. Seems all right>>>","83ab4553":"<a id=\"sec9.9.1\"><\/a>\n## [9.9.1 Prepare Data](#sec9.9.1)","74c6da25":"<a id=\"sec6.3\"><\/a>\n## [6.3 Numerical Feature analysis](#sec6.3)\n[(INDEX)](#sec2)","337b0639":"<a id=\"sec6\"><\/a>\n# [6. ANALYZE DATA](#sec6)\n[(INDEX)](#sec2)","36fed894":"# **Identify a deafault client (CREDIT SCORING)**\n\nThis work has made in cooperation with Vetak8\n#### [GitHub](https:\/\/github.com\/vetak8)\n#### [Kaggle](https:\/\/www.kaggle.com\/vitaliyburlakov)","f3ce6583":"<img src = 'https:\/\/media.makeameme.org\/created\/analyze-the-data.jpg' width = '400px' >","b9da5356":"### [drop cols]","542f8214":"<a id=\"sec6.4\"><\/a>\n## [6.4 Date_Time analysis](#sec6.4)\n[(INDEX)](#sec2)","f02c9579":"### [DATES]","9f161a3f":"<a id=\"sec4\"><\/a>\n# [Functions and Classes](#sec4)\n[(INDEX)](#sec2)","a2dfa8c8":"<a id=\"sec9.9.5\"><\/a>\n## [9.9.5 XGboost & OPTUNA parameters Search](#sec9.9.5)\n[(INDEX)](#sec2)","33d05c57":"<a id=\"sec8.1\"><\/a>\n## [8.1 Create New features](#sec8.1)\n[(INDEX)](#sec2)","cb7cb090":"We will work with combined dataset (Train and validation). Before modeling we split them out.","a56fb4ea":"<a id=\"sec3\"><\/a>\n# [IMPORT LIBRARRIES](sec3)\n[(INDEX)](#sec2)","cd15c340":"Let's check the how our features statistically significant?\nTo do so, we need to encode it to binary entities.","7f5f3160":"Let's save a copy of df with current processing. We need to use that copy for gradient boosting later on this Notebook","9b77035f":"<a id=\"sec5.1\"><\/a>\n## [5.1 Data Types](#sec5.1)\n[(INDEX)](#sec2)","0f064610":"Let's try to fit our model with best parameters and check results by Cross-Validation using sklearn modul StratifieldShuffleSplit.\n\nIt can give us more correct metrics due to splitting our dataset by more than just two sets. In this case, the data inside may be more balanced. Also, it may help us to avoid overfitting.\n\nOverfiting - a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data","3ab5aafa":"<a id=\"sec8.3\"><\/a>\n## [8.3 LOGARITHM](#sec8.3)\n[(INDEX)](#sec2)","f1cd2e69":"Data set is unbalanced. Oversampling showed better results for getting scores in this competition.\n\nLogistic regression as a classical algorithm showed not bad results.\n\nRandom Forest is very slow algorithm especially when we select big number of estimators. Add to above gridsearch and you will get Infinity. \n\nHowever, with a reference to a confusion matrix it gives us better results than Logistic regression.\n\nWe also get a look to three Gradient Boosters.  They works faster than Random forest and LogRegression. It has friendly interface and understandable tuning parameters (except CatBooster). \n\nXGboost Gives best scores from all algorithms but has no impact on final score. WHY? IHNI.\n\nSecond place takes LGBM Classifier. \n\nCat Booster gives us more or less same results (at least in this dataset) but it has a lot of tuning parameters and a little bit strange fit method. Interface is not quite frendly. I would not prefer to work with it again. However, CatBoost has some advantages. It has integrated plot_curves module and we can visualize learning progress. But I failed to run it in kaggle. Run it in JupyterNotebook.\n\nDuring the preparation of the project we met OverFitting and UnderFitting of models and solved it by hyperparameters. Need to be very careful with this issue in future. Always need to verify results with testing dataset.\n\nChecked the feature permutation importance. Usless columns has been deleted.","bc9d1093":"---\nBy looking at plots, we may note that female borrowers tend to be default slightly more than males.\n\nBorrowers, who has a car could be considered as more reliable.Borrowers who do not have a car tend not to repay their loans twice as often as those who have a car.\n\nThose borrowers who have a car and that car is made abroad,tend to be more reliable. However, if we want to see the distribution more clear, we need to create a new category in this col: Has a domestic car, has a foreign car, has no car.\n\nPeople, who have good jobs and foreign passport tend to return loans more often than those, who have bad jobs.\n\n---","0b7e8108":"Well, it seems not to be **underfitted**. Model classify objects in the training and test sets more or less good.","7910ad03":"---\n\nWe have strong correlation between car and car_type. This can be explained by the fact that the column car indicates the presence of a car, and in the column car_type the presence of a foreign car, however, the absence of a car or whether the car is domestic is not indicated in car_type. In feature engineering section we will combine information from these to columns into a one. It allows us reduce matrix of features without loosing information.\n\nIn addition, car is correlated with sex a bit.\n\n---","81af419c":"<a id=\"sec9.6\"><\/a>\n## [9.6 Random Forest_GridSearch](#sec9.6)\n[(INDEX)](#sec2)","631ae707":"<img src = 'https:\/\/external-content.duckduckgo.com\/iu\/?u=http%3A%2F%2Fferriermath.weebly.com%2Fuploads%2F1%2F3%2F8%2F2%2F13820853%2Ffirst-attempt-at-learning_1_orig.jpg&f=1&nofb=1' width = '400px' >","feb8a79d":"Let's try some other algorithms. For example - Forests...","39f00887":"<a id=\"sec1\"><\/a>\n# [Description](#sec1)","8b783a5f":"Repeat same workflow for RandomFores as we did for LogRegressor with oversampling.\n\nApply gridSearch for parameters","155546c3":"<a id=\"sec1.1\"><\/a>\n## [Features Defination](sec1.1)","4117ecfa":"### [DECOMPOSITION]","ec1a7b84":"---\n\nWell, there is no significant changes. But area under precision-recall curve is bit higher. F1 also looks better. F1 increased because we increased recall_pos, While precision for positive class dropped a little.\n\nLet's try other algorithms\n\n---","a454a2ff":"<a id=\"sec9.9\"><\/a>\n## [9.9 Gradient BOOSTERS](#sec9.9)\n[(INDEX)](#sec2)","ed9ad3c5":"<a id=\"sec9.4\"><\/a>\n## [9.4 LOG REGRESSION Undersampling](#sec9.4)\n[(INDEX)](#sec2)","45ac86a0":"<a id=\"sec9\"><\/a>\n# [9. Models](#sec9)\n[(INDEX)](#sec2)","0f57f7d8":"We see that XGboost shows best metrics from all above models. That is unexpected. Don't trust Advertising :)\n\nWe got best precision and recall for positive class. ROC-AUC also most high.","8a62aafd":"People with high level of education prefer to live in a region with higher levels and vice versa.","c861a50a":"---\n\nWell, we can see that our dataset has more people with middle age. Nothing is strange\n\n---","1db64c41":"And use One_hot encoder to convert categorical features","0c6c4ff2":"Let's create a table where we can save the computation results","748a4bb8":"Use optuna for best params","7d811d6e":"It is ready for further experiments.","a0204253":"---\n\nWe may see that default clients lives in mid level regions. The higher rating of the city, the lower the chance to meet a bad guy (default one). It must be a good insight for a model.\n\n---","564bd1f8":"Let's start with distribution of binary cols to unreliable guys","bfa56e8b":"### [Guarantors]","c571f4e8":"<a id=\"sec9.9.4\"><\/a>\n## [9.9.4 CATBOOST & OPTUNA parameters Search](#sec9.9.4)\n[(INDEX)](#sec2)","fe5ab24f":"Now, get a look into a correlation matrix.","3e8dca7d":"<a id=\"sec6.2\"><\/a>\n## [6.2 Categorical Feature analysis](#sec6.2)\n[(INDEX)](#sec2)","89f01dad":"Let's separate columns into a different categories. ","65c8dd68":"---\n\nTrue-positive - Is correctly classified as default client\n\nFalse positive - Not default clients incorrectly classified as default\n\nFalse Negative - Default clients incorectly classified as not default(regular guy)\n\n\nAs ROC-AUC is not good to apply when classes disballanced, let's check Precision-Recall curve. **Only PRC changes with the ratio of positives and negatives**.  (first pic.)\n#### Link :  **[The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets](https:\/\/journals.plos.org\/plosone\/article?id=10.1371\/journal.pone.0118432)**.\n\nPR-REC AUC is close to (0.5), f1_score also extreamely low(0.05) which means that the algorithm is not yet able to distinguish between classes in any way, most of all it assigns class 0 (Not Default) to objects. We will try to choose the best parameters, and also we will make a stratification, which is often used when class disbalanced for validation.\n\nAs our Business target is to identify positive class ( Default Client), then we need to look at sensitivity metric (recall) and Precision. In other words, check the proportion of default clients were correctly classified\n\nIt is only 0.03 now.\n\n---\n","c698f730":"<a id=\"sec8.3\"><\/a>\n## [8.4 Feature Scaling](#sec8.4)\n[(INDEX)](#sec2)","15338cd6":"Check the significance and correlation","75f0c303":"<img src = 'https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Fbestvacuumresource.com%2Fwp-content%2Fuploads%2FRobot-Holding-Vacuum-Cleaner.jpg&f=1&nofb=1' width = '400px' >","dcf09f5f":"<img src = 'https:\/\/www.nationalvanguard.org\/wp-content\/uploads\/2016\/03\/debt-slave.jpg' width='400px' >","23dcc5f9":"Same problem. Unbalanced classes. Lack of data and as a result we cannot get good metreics of ROC-AUC and F1, as well as psitive class precision.\n\nHowever, We got best PR-AUC and best F1 score. It happened because our precicion and recall became a little bit balanced.","9e88c3b7":"OK, and now let's see the distribution of the failed borrowers by ***region_rating***","77c7a34a":"Nothing special here. Booster also dislike unbalanced data :(","44120d21":"Well, our dataset has only four (4) monts.","8364221a":"---\n\nMost statistically significant variables are **Score_bki**, **decline_app_cnt**. There is no strong correlations between features. It is confirmed by rank of matrix. The value of determinant is good, there must be an inverse matrix.\n\n---","c6ef7f99":"Random forest without additionla tunings shows worse results.","2713cf7d":"<a id=\"sec9.8\"><\/a>\n## [9.8 Random Forest_UnderSampling](#sec9.8)\n[(INDEX)](#sec2)","9c824c55":"Here we see that education has no significant impact on the model. Skore_bki is most important feature","cd8603a8":"<a id=\"sec5\"><\/a>\n# [5.Load and Chek the Dataset](#sec5)\n[(INDEX)](#sec2)","a4e4776a":"<a id=\"sec2\"><\/a>\n# [INDEX](#sec2)\n\n<a id=\"sec1\"><\/a>\n1. [Description](#sec1)\n   * [1.1 Features Defination](#sec1.1)\n2. [INDEX](#sec2)\n3. [IMPORT LIBRARRIES](#sec3)\n4. [Functions and Classes](#sec4)\n5. [Load and Chek the Dataset](#sec5)\n    * [Data Types](#sec5.1)\n    * [Pandas Profiling](#sec5.2)\n6. [ANALYZE DATA](#sec6)\n    * [Bolean Feature analysis](#sec6.1)\n    * [Categorical Feature analysis](#sec6.2)\n    * [Numerical Feature analysis](#sec6.3)\n    * [Date_Time analysis](#sec6.4)\n    * [Draw a conclusion](#sec6.5)\n7. [Naive Model](#sec7)\n8. [Feature Engineering](#sec8)\n    * [Create New features](#sec8.1)\n    * [ENCODING](#sec8.2)\n    * [LOGARITHM](#sec8.3)\n    * [Feature Scaling](#sec8.3)\n9. [Models](#sec9)\n    * [9.1 LOG REGRESSION (naive)](#sec9.1)\n    * [9.2 LOG REGRESSION (CrosValid & GridSearch)](#sec9.2)\n    * [9.3 LOG REGRESSION Oversampling](#sec9.3)\n    * [9.4 LOG REGRESSION Undersampling](#sec9.4)\n    * [9.5 Random Forest_Naive](#sec9.5)\n    * [9.6 Random Forest_GridSearch](#sec9.6)\n    * [9.7 Random Forest_OverSampling](#sec9.7)\n    * [9.8 Random Forest_UnderSampling](#sec9.8)\n    * [9.9 Gradient BOOSTERS](#sec9.9)\n        * [9.9.1 Prepare Data](#sec9.9.1)\n        * [9.9.2 LightGBM](#sec9.9.2)\n        * [9.9.3 LightGBM_Gridsearch](#sec9.9.3)\n        * [9.9.4 CATBOOST & OPTUNA parameters Search](#sec9.9.4)\n    * [9.10 Stacking_OverSample](#sec9.10)\n10. [Conclusions](#sec10)\n11. [SUBMISSION](#sec11)\n\n","c50a84a5":"<a id=\"sec9.9.3\"><\/a>\n## [9.9.3 LightGBM_Gridsearch](#sec9.9.3)\n[(INDEX)](#sec2)","0d5b5782":"Before we do, let's fill missing values up","fc4b9878":"Let's try to apply stacking on models, combine different models with f1 over 60%\n\nLet's see if we can improve performance by combining soft voting model predictions.\n\nSoft voting is argmax of the sum of the predicted probabilities. \n\n***NOTE_1***: For this competition best score could be obtained if we make oversampling for whole X.\n\n***NOTE_2***: Including XG_boost in voter decrease sore for competition.","0f6293f3":"---\n\nLooking at QQ plots, we may note, that only column **score_bki** has a distribution close to normal. Moreover, as was mentioned above, seems this feature already scaled. **Age** has left-biased distribution. Data has outliers, but later we will decide what to do with it.\n\nWe have several options: 1. Apply logarithm function do data, 2convert some data to categorical features, Apply combination of logarithm and converting to categorical features.\n\n---","868ab931":"<a id=\"sec8.2\"><\/a>\n## [8.2 ENCODING](#sec8.2)\n[(INDEX)](#sec2)","c1d28f5a":"Split our train data into two parts: Train and test","27a09552":"As we can see, the voiting gives us best solution for such dataset. However, including XGboost in voiting significantly impact on score for competition.  We won't use XGboost in voting for submission.","0bb04d18":"<a id=\"sec10\"><\/a>\n# [Conclusions](#sec10)\n[(INDEX)](#sec2)","a4ec655b":"Questions For Menthor:\n\n1. Is there fast methods to check exactly in what state my model (overfitted or underfitted)\n2. How to increase score? What did I do wrong?\n3. Why XGboost did not increase score in Kaggle? \n3. Did my conclussions are correct? After each model?\n","83ca64ad":"The checking of feature importance via permutations within one feature (for example, a line): if after the permutation the model error increased, then the feature is \"important\", if the error occurred unchanged, then it is \"unimportant\" ","b3d311c1":"---\n\nFeature **Month** has lower statistic significance. Somehow the month is correlated with work_address. \n\nProbably, in this period of time larger proportion of people came to bank from specific region.\n\n---","bef9ad49":"<a id=\"sec7\"><\/a>\n# [7. Naive Model](#sec7)\n[(INDEX)](#sec2)","31ea7468":"<a id=\"sec9.10\"><\/a>\n## [9.10 Stacking_OverSample](#sec9.10)\n[(INDEX)](#sec2)","af744068":"---\n\nFinally we got what we waanted. We can predict whith high accuracy positive class. At the same time we are not able to classiffy negative class like we did it before. But here we need to refer to the target of project. WHat is more important to us? Predict Negative or positive class?\n\n\nIt is notable, that Boosters works much faster and gives us bettrer results and balanced metrics for the bad data. But we need to remember the rule 'Garbage in --> Garbage out'. (Under the hood of this project we made huge amount of fails with it. Feeded to poor models by lots of garbage:)\n\nIt is prefarable to work with boosters in further.\n\nHowever, Maybe our model is overfitted? Chek it out.\n\n---"}}