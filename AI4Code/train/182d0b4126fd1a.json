{"cell_type":{"9f505c70":"code","50468304":"code","82ac2513":"code","d28568b6":"code","7ab1a470":"code","815a8385":"code","c110461c":"code","fde19b85":"code","c5930b42":"code","0d33d7d2":"code","cc682cc6":"code","415e00d8":"code","fecb0136":"code","ab80d422":"code","0eb407ff":"code","b5a4413a":"code","b990c116":"code","0a86a819":"code","620a9d89":"code","4fc29e39":"code","20ae7f03":"code","06f2d706":"code","ec094b8b":"code","6a417321":"code","fae76997":"code","6677d527":"code","ebecc6a5":"code","bf825abf":"code","b5869439":"code","21c3e6e7":"code","66f2905e":"code","6d8087aa":"markdown","8b32e7d5":"markdown","0bdb05de":"markdown","e38cd6aa":"markdown","57852483":"markdown","4068f5c4":"markdown","75ed27aa":"markdown","9abfc4fa":"markdown","c3b91804":"markdown","332254fe":"markdown","1195d733":"markdown","d8054bbc":"markdown","2c370d77":"markdown","50b89b50":"markdown","6f4def61":"markdown","69ffa08a":"markdown","d073b437":"markdown","c8425ea4":"markdown","ec2c6a52":"markdown","bf02fd10":"markdown","52f2ece1":"markdown"},"source":{"9f505c70":"#Import libraries\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\nfrom sklearn.metrics import roc_auc_score,  roc_curve, auc\n\nimport lightgbm as lgb\n\nfrom IPython.display import display\n\n# Suppr warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline  ","50468304":"#Useful settings\nplt.style.use('ggplot')\n\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 1000)\npd.options.mode.use_inf_as_na = True","82ac2513":"#Load data\ndata_path = '..\/input\/learn-together\/'\n\ntrain_df = pd.read_csv(data_path + 'train.csv', index_col = 'Id')\ntest_df = pd.read_csv(data_path + 'test.csv', index_col = 'Id')","d28568b6":"original_features = [f for f in list(train_df) if f!='Cover_Type']\ntarget = 'Cover_Type'","7ab1a470":"stony_level = np.array([3,2,0,0,0,1,0,0,2,0,0,1,0,0,0,0,0,2,0,0,0,0,0,3,3,2,3,3,3,3,3,3,3,3,0,3,3,3,3,3])","815a8385":"def feat_eng(df):\n    \n    df['Dist_To_Hydro'] = np.sqrt(df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)\n    df['Log_Dist_To_Hydro'] = np.log( np.sqrt(df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2) +1)\n    \n    df['Hydro_Fire_p'] = np.abs(df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points'])\n    df['Hydro_Fire_n'] = np.abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points'])\n\n    df['Hydro_Road_p'] = np.abs(df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways'])\n    df['Hydro_Road_n'] = np.abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways'])\n\n    df['Fire_Road_p'] = np.abs(df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Roadways'])\n    df['Fire_Road_n'] = np.abs(df['Horizontal_Distance_To_Fire_Points'] - df['Horizontal_Distance_To_Roadways'])\n    \n    horiz_grp = [f for f in original_features if f.startswith('Horizontal')]\n    df['Horiz_Dist_Mean'] = df[horiz_grp].mean(axis = 1).round(3)\n    df['Horiz_Dist_Std'] = df[horiz_grp].std(axis = 1).round(3)\n    \n    df['Is_Overwater'] = df['Vertical_Distance_To_Hydrology'] > 0\n    \n    hill_grp = [f for f in original_features if f.startswith('Hill')]\n    df['Hillshade_Mean'] = df[hill_grp].mean(axis = 1).round(3)\n    df['Hillshade_Std'] = df[hill_grp].std(axis = 1).round(3)\n    \n    soil_grp = [f for f in original_features if f.startswith('Soil_Type')]\n    df['Stony_Level'] =  df[soil_grp]@stony_level\n    \n    df['Elevation_Adjusted'] = df['Elevation'] - df['Vertical_Distance_To_Hydrology']\n    df['Elevation'] = np.log(df['Elevation'])\n    \n    df['Aspect'] = df['Aspect'].astype(int) % 360\n    #df['Aspect_Bins'] = pd.cut(df['Aspect'], bins=20, labels=range(20))\n    \n    df['Sen_Aspect'] = np.sin(np.radians(df['Aspect']))\n    df['Cos_Aspect'] = np.cos(np.radians(df['Aspect']))\n    \n    df['Sen_Slope'] = np.sin(np.radians(df['Slope']))\n    df['Cos_Slope'] = np.cos(np.radians(df['Slope']))\n    \n    return df\n    \ntrain_df = feat_eng(train_df)\ntest_df = feat_eng(test_df)","c110461c":"features = [f for f in list(train_df) if f != 'Cover_Type']\ntarget = 'Cover_Type'","fde19b85":"def plot_auc(labels, probs):\n    ## Compute the false positive rate, true positive rate\n    ## and threshold along with the AUC\n    fpr, tpr, threshold = roc_curve(labels, probs)\n    auc_score = auc(fpr, tpr)\n    \n    ## Plot the result\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, color = 'orange', label = 'Auc score = %0.2f' % auc_score)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n","c5930b42":"n = len(test_df)\ntest_df = test_df.sample(frac=1) #shuffles the test\n\n#create mask of similar size to train_df\nmask = np.random.randint(low=0, high=20, size=n)","0d33d7d2":"#Create the new target we try to predict\ntrain_df['is_test'] = 0\ntest_df['is_test'] = 1","cc682cc6":"acc_rf = []\nauc_rf = []\nfeature_importance_rf = pd.DataFrame()\n\nfor m in range(0,20):\n    fold = m\n    df = pd.concat([train_df.drop(target, axis=1), test_df[mask == m]])\n    \n    X_train, X_test, y_train, y_test = train_test_split(df[features], df['is_test'], \n                                                        test_size = 0.25, stratify = df['is_test'],\n                                                        shuffle = True)\n\n\n        # Fitting Random Forest Classification to the Training set\n    rfc = RandomForestClassifier(n_estimators=20,\n                                 max_depth=None,\n                                 min_samples_split=15,\n                                 min_samples_leaf=5,\n                                 max_features=0.5,\n                                 n_jobs=-1,\n                                 random_state=42)\n    rfc.fit(X_train, y_train)\n\n    # Predicting the Test set results\n    y_pred = rfc.predict(X_test)\n    \n    #print(pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Prediction']))\n    \n    acc_rf.append(accuracy_score(y_test, y_pred))\n    \n    y_pred_prob = rfc.predict_proba(X_test)\n    fpr, tpr, t = roc_curve(y_test, y_pred_prob[:,1])\n    auc_s = auc(fpr, tpr)\n    auc_rf.append(auc_s)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = rfc.feature_importances_\n    fold_importance_df[\"fold\"] = fold + 1\n    feature_importance_rf = pd.concat([feature_importance_rf, fold_importance_df], axis=0)","415e00d8":"print(f\"---------- Accuracy ------------ \\n\\\n      Max: {np.max(acc_rf)} \\n\\\n      Min: {np.min(acc_rf)} \\n\\\n      Mean: {np.mean(acc_rf)}\\n\\\n      Std: {np.std(acc_rf)}\")\n\nprint(f\"---------- AUC Score ------------ \\n\\\n      Max: {np.max(auc_rf)} \\n\\\n      Min: {np.min(auc_rf)} \\n\\\n      Mean: {np.mean(auc_rf)}\\n\\\n      Std: {np.std(auc_rf)}\")","fecb0136":"sns.set_style('whitegrid')\n\ncols = (feature_importance_rf[[\"feature\", \"importance\"]]\n    .groupby(\"feature\")\n    .mean()\n    .sort_values(by=\"importance\", ascending=False)[:30].index)\n\nbest_features = feature_importance_rf.loc[feature_importance_rf['feature'].isin(cols)]\n\nplt.figure(figsize=(10,10))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False),\n        edgecolor=('black'), linewidth=2, palette=\"colorblind\")\nplt.title('RFC Features importance (averaged\/folds)', fontsize=15)\nplt.tight_layout()","ab80d422":"from sklearn.linear_model import LogisticRegression as LR\n\nacc_lr = []\nauc_lr = []\nfeature_coeff_lr = pd.DataFrame()\n\nfor m in range(0,20):\n    fold = m\n    df = pd.concat([train_df, test_df[mask == m]])\n\n    X_train, X_test, y_train, y_test = train_test_split(df[features], df['is_test'], \n                                                        test_size=0.25,\n                                                        stratify=df['is_test'],\n                                                        shuffle=True)\n    \n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.transform(X_test)\n    \n    lr = LR(C=2)\n    lr.fit(X_train, y_train)\n\n    y_pred_prob = lr.predict_proba(X_test)\n    y_pred = lr.predict(X_test)\n    acc_lr.append(accuracy_score(y_test, y_pred))\n    \n    fpr, tpr, t = roc_curve(y_test, y_pred_prob[:,1])\n    auc_s = auc(fpr, tpr)\n    auc_lr.append(auc_s)\n    \n    fold_coeff_lr = pd.DataFrame()\n    fold_coeff_lr[\"feature\"] = features\n    fold_coeff_lr[\"coeff\"] = np.abs(lr.coef_[0,:])\n    fold_coeff_lr[\"fold\"] = fold + 1\n    feature_coeff_lr = pd.concat([feature_coeff_lr, fold_coeff_lr], axis=0)\n","0eb407ff":"print(f\"---------- Accuracy ------------ \\n\\\n      Max: {np.max(acc_lr)} \\n\\\n      Min: {np.min(acc_lr)} \\n\\\n      Mean: {np.mean(acc_lr)}\\n\\\n      Std: {np.std(acc_lr)}\")\n\nprint(f\"---------- AUC Score ------------ \\n\\\n      Max: {np.max(auc_lr)} \\n\\\n      Min: {np.min(auc_lr)} \\n\\\n      Mean: {np.mean(auc_lr)}\\n\\\n      Std: {np.std(auc_lr)}\")","b5a4413a":"sns.set_style('whitegrid')\n\ncols = (feature_coeff_lr[[\"feature\", \"coeff\"]]\n    .groupby(\"feature\")\n    .mean()\n    .sort_values(by=\"coeff\", ascending=False)[:30].index)\n\nbest_features = feature_coeff_lr.loc[feature_coeff_lr['feature'].isin(cols)]\n\nplt.figure(figsize=(10,10))\nsns.barplot(x=\"coeff\", y=\"feature\", data=best_features.sort_values(by=\"coeff\",ascending=False),\n        edgecolor=('black'), linewidth=2, palette=\"colorblind\")\nplt.title('Logistic Regression Absolute Value of Coefficients (averaged\/folds)', fontsize=15)\nplt.tight_layout()","b990c116":"import lightgbm as lgb\n\nacc_gbm = []\nauc_gbm = []\nfeature_importance_gbm = pd.DataFrame()\n\nparam = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.30,\n    'boost_from_average':'false',\n    #'boost': 'gbdt',\n    'max_bin': 32,\n    'feature_fraction': 0.15,\n    'learning_rate': 0.012,\n    'max_depth': 23,\n    'metric': 'auc',\n    'min_data_in_leaf': 100,\n    'lambda_l1': 10,\n    'lambda_l2': 10,\n    #'min_gain_to_split': 3,\n    #'min_sum_hessian_in_leaf': 12,\n    'num_leaves': 3,\n    #'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary',\n    'verbosity': 0,\n    'n_jobs' : 6}\n\nnum_round = 2000 \n\nfor m in range(0,20):\n    fold = m\n    df = pd.concat([train_df, test_df[mask == m]])\n\n    X_train, X_test, y_train, y_test = train_test_split(df[features], df['is_test'], \n                                                        test_size = 0.25, stratify = df['is_test'],\n                                                        shuffle = True)\n    \n    train_data = lgb.Dataset(data = X_train, label = y_train)\n    val_data = lgb.Dataset(data = X_test, label = y_test)\n    \n    lgbm = lgb.train(param, train_data, num_round, \n                 valid_sets = [train_data, val_data],\n                 verbose_eval = 2000, early_stopping_rounds = 50)\n\n    y_pred_prob = lgbm.predict(X_test, num_iteration = lgbm.best_iteration)\n\n    y_pred = (y_pred_prob > 0.5).astype(int)\n    acc_gbm.append(accuracy_score(y_test, y_pred))\n    \n    fpr, tpr, t = roc_curve(y_test, y_pred_prob)\n    auc_s = auc(fpr, tpr)\n    auc_gbm.append(auc_s)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = lgbm.feature_importance()\n    fold_importance_df[\"fold\"] = fold + 1\n    feature_importance_gbm = pd.concat([feature_importance_gbm, fold_importance_df], axis=0)\n    ","0a86a819":"print(f\"---------- Accuracy ------------ \\n\\\n      Max: {np.max(acc_gbm)} \\n\\\n      Min: {np.min(acc_gbm)} \\n\\\n      Mean: {np.mean(acc_gbm)}\\n\\\n      Std: {np.std(acc_gbm)}\")\n\nprint(f\"---------- AUC Score ------------ \\n\\\n      Max: {np.max(auc_gbm)} \\n\\\n      Min: {np.min(auc_gbm)} \\n\\\n      Mean: {np.mean(auc_gbm)}\\n\\\n      Std: {np.std(auc_gbm)}\")","620a9d89":"sns.set_style('whitegrid')\n\ncols = (feature_importance_gbm[[\"feature\", \"importance\"]]\n    .groupby(\"feature\")\n    .mean()\n    .sort_values(by=\"importance\", ascending=False)[:30].index)\n\nbest_features = feature_importance_gbm.loc[feature_importance_gbm['feature'].isin(cols)]\n\nplt.figure(figsize=(10,10))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False),\n        edgecolor=('black'), linewidth=2, palette=\"colorblind\")\nplt.title('LGBM Features importance (averaged\/folds)', fontsize=15)\nplt.tight_layout()","4fc29e39":"def pca_plot(df, n_components=3, plot_dim=2):\n    \n    from sklearn.decomposition import PCA\n    \n    pca = PCA(n_components=n_components)\n\n    # Run PCA on scaled dataframe\n    pca_fit = pca.fit_transform(df)    \n\n    pca_df = pd.DataFrame(pca_fit,\n                          columns=[\"PCA\"+str(i+1) for i in range(pca_fit.shape[1])])\n    \n    if plot_dim==2:\n        fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n        \n        #Plot the explained variance\n        axes[0].plot(pca.explained_variance_ratio_, \n                     \"--o\", linewidth=2,\n                    label=\"Explained variance ratio\")\n    \n        # Plot the cumulative explained variance\n        axes[0].plot(pca.explained_variance_ratio_.cumsum(),\n                     \"--o\", linewidth=2,\n                     label=\"Cumulative explained variance ratio\")\n        \n        axes[0].legend(loc=\"best\", frameon=True)\n        \n        #Plot PCA Components\n        cmap = sns.cubehelix_palette(as_cmap=True)\n        temp = axes[1].scatter(pca_df['PCA1'], pca_df['PCA2'], \n                               c=df['is_test'], s=50, cmap=cmap, alpha=.3)\n        \n        fig.colorbar(temp)\n        plt.show()\n    else:\n        from mpl_toolkits.mplot3d import Axes3D\n\n        cmap = sns.cubehelix_palette(as_cmap=True)\n        fig = plt.figure(figsize = (12,12))\n        #tp, ax = plt.subplots(figsize=(20,15))\n        axes = Axes3D(fig)\n\n        axes.scatter(pca_df['PCA1'], pca_df['PCA2'], pca_df['PCA3'], \n                   c=df['is_test'], cmap=cmap, s=50, alpha=.3)\n\n        # label the axes\n        axes.set_xlabel('Principal Component 1', size = 18)\n        axes.set_ylabel('Principal Component 2', size = 18)\n        axes.set_zlabel('Principal Component 2', size = 18)\n\n        #ax.set_title('3D Principal Component Analysis', size = 26)\n        plt.show()\n","20ae7f03":"original = False\n\nif original:\n    pca_train = train_df[original_features + ['is_test']].copy()\n    pca_test = test_df[original_features + ['is_test']].copy()\n    pca_features = original_features\nelse: \n    pca_train = train_df[features + ['is_test']].copy()\n    pca_test = test_df[features + ['is_test']].copy()\n    pca_features = features\n\n\n#create mask of similar size to train_df\nn = len(pca_test)\nmask = np.random.randint(low=0, high=20, size=n)","06f2d706":"for m in range(0,20):\n    \n    df = pd.concat([pca_train, pca_test[mask == m]])\n    df = df.sample(frac=1).copy() #Shuffles the df\n        \n    sc = StandardScaler()\n    df[pca_features] = sc.fit_transform(df[pca_features])\n        \n    pca_plot(df, 20)","ec094b8b":"df = pd.concat([pca_train, pca_test])\ndf = df.sample(frac=1).copy() #Shuffles the df\nsc = StandardScaler()\ndf[pca_features] = sc.fit_transform(df[pca_features])\n    \npca_plot(df, 20)","6a417321":"pca_plot(df,20,3) ","fae76997":"from sklearn.manifold import TSNE\ndef tsne_plot(x1, y1, \n              perplexity=30,\n              learning_rate=200,\n              init='random',\n              name=None,\n              save=True):\n    \n    tsne = TSNE(n_components=2, \n                perplexity=perplexity,\n                learning_rate=learning_rate,\n                init=init,\n                random_state=42)\n    \n    X_t = tsne.fit_transform(x1)\n    \n    cmap = sns.cubehelix_palette(as_cmap=True)\n    tp, ax = plt.subplots(figsize=(20,15))\n    temp = ax.scatter(X_t[:,0], X_t[:,1], c=y1, s=50, cmap=cmap)\n    tp.colorbar(temp)\n    \n    plt.title(f'tsne with perplexity {perplexity} and learning rate {learning_rate}')\n    #plt.legend(loc = 'best');\n    if save and name: \n        plt.savefig(name);\n    plt.show();","6677d527":"original = False\n\nif original:\n    tsne_train = train_df[original_features + ['is_test']].copy()\n    tsne_test = test_df[original_features + ['is_test']].copy()\n    tsne_features = original_features\nelse: \n    tsne_train = train_df[features + ['is_test']].copy()\n    tsne_test = test_df[features + ['is_test']].copy()\n    tsne_features = features\nn = len(tsne_test)\n\n#create mask of similar size to train_df\nmask = np.random.randint(low=0, high=40, size=n)\n","ebecc6a5":"#sample_size = 10_000\n#\n#for m in range(0,40):\n#    \n#    df = pd.concat([tsne_train, tsne_test[mask == m]])\n#    df = df.sample(frac=1)\n#    \n#    sc = StandardScaler()\n#    df = sc.fit_transform(df[tsne_features])\n#    \n#    tsne_plot(df.iloc[100*m:sample_size+100*m,][tsne_features], df.iloc[100*m:sample_size+100*m,]['is_test'], \n#              perplexity = 100,\n#              learning_rate = 1000,\n#              save = False)","bf825abf":"continuous_features = ['Elevation',\n                       'Aspect',\n                       'Slope',\n                       'Horizontal_Distance_To_Hydrology',\n                       'Vertical_Distance_To_Hydrology',\n                       'Horizontal_Distance_To_Roadways',\n                       'Hillshade_9am',\n                       'Hillshade_Noon',\n                       'Hillshade_3pm',\n                       'Horizontal_Distance_To_Fire_Points',\n                       'Dist_To_Hydro',\n                       'Log_Dist_To_Hydro',\n                       'Hydro_Fire_p',\n                       'Hydro_Fire_n',\n                       'Hydro_Road_p',\n                       'Hydro_Road_n',\n                       'Fire_Road_p',\n                       'Fire_Road_n',\n                       'Horiz_Dist_Mean',\n                       'Horiz_Dist_Std',\n                       'Elevation_Adjusted',\n                       'Sen_Aspect',\n                       'Cos_Aspect',\n                       'Sen_Slope',\n                       'Cos_Slope']","b5869439":"from scipy.stats import ks_2samp\n\nDStat = []\npvals = []\nfor f in continuous_features:\n    \n    D,p = ks_2samp(train_df[f],test_df[f])\n    \n    DStat.append(D)\n    pvals.append(p)\n\nrejected_list = [(f,p,d) for f,p,d in zip(continuous_features,pvals, DStat) if p<0.025 ]","21c3e6e7":"def plot_ks2(df1, df2, label1, label2, rejected_list):\n    \n    sns.set_palette('colorblind')\n    sns.set_style('whitegrid')\n    \n    i = 0\n    plt.figure()\n    fig, ax = plt.subplots(5,5,figsize=(30,20))\n\n    for col,stat,pval in rejected_list:\n        i += 1\n        plt.subplot(5,5,i)\n        plt.title(\"KS Test for feature: {}, \\n\"\n              \"statistics: {:.5f}, pvalue: {:3f}\".format(col, stat, pval))\n        sns.kdeplot(train_df[col], color='blue', shade=True, label='Train')\n        sns.kdeplot(test_df[col], color='green', shade=True, label='Test')\n    plt.show();","66f2905e":"plot_ks2(train_df, test_df, 'train', 'test', rejected_list)","6d8087aa":"### Plot Distributions of features that did not pass the test","8b32e7d5":"This is a two-sided test for the null hypothesis that 2 independent samples are drawn from the same continuous distribution. KS test returns a D statistic and a p-value corresponding to the D statistic. The D statistic is the absolute max distance (supremum) between the CDFs of the two samples. The closer this number is to 0 the more likely it is that the two samples were drawn from the same distribution. The p-value returned by the KS test has the same interpretation as other p-values. You reject the null hypothesis that the two samples were drawn from the same distribution if the p-value is less than your significance level. You can find tables online for the conversion of the D statistic into a p-value if you are interested in the procedure.","0bdb05de":"<a id=\"section-two\"><\/a>\n# Random Forest Classifier","e38cd6aa":"<a id=\"subsection-four\"><\/a>\n### Light GBM Features Importance\n","57852483":"Some fast remarks:\n* I didn't try to tune any algorithm hyperparameter\n* I limited Light GBM to 2000 rounds with 50 rounds of not improving for early stopping: it stopped because it reached 2000 with very few exceptions;\n* To make predictions (not probabilities) in Light GBM I used a threshold of .5, that is also a hyperparameter to try out;\n* We can see how different algorithms yield similar results.","4068f5c4":"### Logistic Regression Scores","75ed27aa":"### Random Forest Score\n","9abfc4fa":"<a id=\"subsection-two\"><\/a>\n### Random Forest Features Importance\n","c3b91804":"This appears to be pretty bad since no one among the continuous features passed the KS test.\nI don't know how to deal with it. Any ideas? Cheers!","332254fe":"<a id=\"subsection-three\"><\/a>\n### Coeff Weights","1195d733":"<a id=\"section-five\"><\/a>\n# PCA Analysis\n","d8054bbc":"<a id=\"section-four\"><\/a>\n# Light GBM","2c370d77":"# Adversarial Validation - Train vs Test Distribution\n","50b89b50":"All train and test together","6f4def61":"<a id=\"section-three\"><\/a>\n# Logistic Regression","69ffa08a":"I also tried T-SNE with poor results (I leave the code hidden, if anyone is interested to try or has better insights)\n","d073b437":"The goal of this notebook is to investigate differences between Train and Test data distribution by mean of simple models. \nThis necessity arises because many of us noticed a drop from cross-validated accuracy to public leaderboard score - sometimes around 10% lower.\n\n\n* [Feature Engineering](#section-one)\n* [Random Forest Classifier](#section-two)\n    - [Feature Importance](#subsection-two)\n* [Logistic Regression](#section-three)\n    - [Coeff Weights](#subsection-three)\n* [Light GBM](#section-four)\n    - [Feature Importance](#subsection-four)\n* [PCA Analysis](#section-five)\n* [Kolmogorov\u2013Smirnov test](#section-six)","c8425ea4":"Here I create 20 randomly shuffled dataset from the test set: this allows to have less imbalanced data and faster training and testing. (However similar results yield if we cross-validate for the entire test set at once\n","ec2c6a52":"### Light GBM Scores\n","bf02fd10":"<a id=\"section-one\"><\/a>\n# Feature Engineering","52f2ece1":"<a id=\"section-six\"><\/a>\n# Kolmogorov\u2013Smirnov test"}}