{"cell_type":{"68739120":"code","85a11832":"code","31506e8f":"code","5764f6bb":"code","a20c778b":"code","5599885d":"code","dfd2b4ff":"code","6b15b7ee":"code","ee7696dd":"code","7aa8262d":"code","b9f9cb46":"code","e340aa5a":"code","b1e238f7":"code","819818fb":"code","7017813b":"code","902fa06c":"code","5777191c":"code","8b1b0233":"code","a2a8cd92":"code","1f5bebed":"code","d549c208":"code","d987df98":"code","1e9a425a":"code","fa3af748":"code","6d2d92c9":"markdown","f797303d":"markdown","f329b438":"markdown","87e60798":"markdown","979ebeac":"markdown","cc1c6c9b":"markdown","aa242cd7":"markdown","cdc1db0c":"markdown","28c1e1b3":"markdown","6b7ca15f":"markdown","e63b42ba":"markdown","c06c20a3":"markdown","d1058b84":"markdown","09797cc5":"markdown","64d14d40":"markdown","691006d2":"markdown","113990c3":"markdown","785130ab":"markdown","366df16b":"markdown","a325519d":"markdown","cf8ffa9f":"markdown","5f9dd957":"markdown"},"source":{"68739120":"import pandas as pd\n\nmain_file_path = '..\/input\/house-prices-advanced-regression-techniques\/train.csv' # this is the path to the Iowa data that you will use\ndata = pd.read_csv(main_file_path)\n\n# Run this code block with the control-enter keys on your keyboard. Or click the blue botton on the left\nprint('Some output from running this cell')\nprint(data.columns)\n\ndata_price = data.SalePrice\nprint(data_price.head())","85a11832":"column_names = ['LotArea', 'SalePrice']\ncondition_and_price = data[column_names]\n\ncondition_and_price.describe()","31506e8f":"# data = data.dropna(axis=0)\ny = data.SalePrice\n\nestimators = [\"LotArea\", \"YearBuilt\", \"1stFlrSF\", \"2ndFlrSF\", \"FullBath\", \"BedroomAbvGr\", \"TotRmsAbvGrd\"]\nX = data[estimators]","5764f6bb":"from sklearn.tree import DecisionTreeRegressor\n\nmodel = DecisionTreeRegressor()\nmodel.fit(X, y)\n\nprint(\"making predictions\")\nprint(X.head())\nprint(\"the predictions are\")\nprint(model.predict(X.head()))","a20c778b":"from sklearn.metrics import mean_absolute_error\n\npredicted_home_prices = model.predict(X)\nmean_absolute_error(y, predicted_home_prices)","5599885d":"from sklearn.model_selection import train_test_split\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n\nmodel = DecisionTreeRegressor()\nmodel.fit(train_X, train_y)\n\nval_predictions = model.predict(val_X)\nprint(mean_absolute_error(val_y, val_predictions))","dfd2b4ff":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(predictors_train, targ_train)\n    preds_val = model.predict(predictors_val)\n    mae = mean_absolute_error(targ_val, preds_val)\n    return mae\n\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d \\t\\t Mean Absolute Error: %f\" %(max_leaf_nodes, my_mae))","6b15b7ee":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model = RandomForestRegressor()\nforest_model.fit(train_X, train_y)\nforest_preds = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y, forest_preds))","ee7696dd":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\n\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\ntrain_y = train.SalePrice\n\npredictor_cols = ['LotArea', 'OverallQual', 'YearBuilt', 'TotRmsAbvGrd']\n\ntrain_X = train[predictor_cols]\n\nmy_model = RandomForestRegressor()\nmy_model.fit(train_X, train_y)","7aa8262d":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntest_X = test[predictor_cols]\n\npredicted_prices = my_model.predict(test_X)\n\nprint(predicted_prices)","b9f9cb46":"my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})\n\nmy_submission.to_csv('submission.csv', index=False)","e340aa5a":"import pandas as pd\n\ndata = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\ntarget = data.SalePrice\npredictors = data.drop(['SalePrice'], axis=1)\n\n# only consider numeric columns\nnumeric_predictors = predictors.select_dtypes(exclude=['object'])","b1e238f7":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(numeric_predictors, \n                                                    target,\n                                                    train_size=0.7,\n                                                    test_size=0.3,\n                                                    random_state=0)\n\ndef score_dataset(X_train, X_test, y_train, y_test):\n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return mean_absolute_error(y_test, preds)","819818fb":"cols_with_missing = [col for col in X_train.columns\n                                 if X_train[col].isnull().any()]\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_test = X_test.drop(cols_with_missing, axis=1)\nprint(\"Mean absolute error from dropping columsn with missing values:\")\nprint(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))","7017813b":"from sklearn.impute import SimpleImputer\n\nmy_imputer = SimpleImputer()\nimputed_X_train = my_imputer.fit_transform(X_train)\nimputed_X_test = my_imputer.transform(X_test)\nprint(\"Mean absolute error from imputation:\")\nprint(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))","902fa06c":"imputed_X_train_plus = X_train.copy()\nimputed_X_test_plus = X_test.copy()\n\ncols_with_missing = (col for col in X_train.columns\n                                 if X_train[col].isnull().any())\n\nfor col in cols_with_missing:\n    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n    \nmy_imputer = SimpleImputer()\nimputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)\nimputed_X_test_plus = my_imputer.transform(imputed_X_test_plus)\n\nprint(\"Mean absolute error from imputation while track imputed cols:\")\nprint(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))","5777191c":"import pandas as pd\ntrain_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n# Drop data samples where SalePrice is empty.\ntrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\ntarget = train_data.SalePrice\n\ncols_with_missing = [col for col in train_data.columns if train_data[col].isnull().any()]\ncand_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\ncand_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)\n\nlow_card_cols = [cname for cname in cand_train_predictors.columns if \n                     cand_train_predictors[cname].nunique() < 10 and\n                     cand_train_predictors[cname].dtype == \"object\"]\nnumeric_cols = [cname for cname in cand_train_predictors.columns if\n                   cand_train_predictors[cname].dtype in ['int64', 'float64']]\nmy_cols = low_card_cols + numeric_cols\ntrain_predictors = cand_train_predictors[my_cols]\ntest_predictors = cand_test_predictors[my_cols]","8b1b0233":"train_predictors.dtypes.sample(10)","a2a8cd92":"one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)","1f5bebed":"from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef get_mae(X, y):\n    return -1 * cross_val_score(RandomForestRegressor(50),\n                                X,\n                                y,\n                                scoring='neg_mean_absolute_error').mean()\n\npredictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\nmae_without_categoricals = get_mae(predictors_without_categoricals, target)\nmae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)\n\nprint(\"Mean absolute error when dropping categoricals: \" + str(int(mae_without_categoricals)))\nprint(\"Mean absolute error with one hot encoding: \" + str(int(mae_one_hot_encoded)))","d549c208":"one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\none_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\nfinal_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                    join='left',\n                                                                    axis=1)","d987df98":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\n\ntrain_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\ntarget = train_data.SalePrice\n\n# drop unnecessary columns\ncandidate_train_predictors = train_data.drop(['Id', 'SalePrice'], axis=1)\ncandidate_test_predictors = test_data.drop(['Id'], axis=1)\n\n# only consider low cardinality and numeric columns for now\nlow_card_cols = [cname for cname in candidate_train_predictors.columns if \n                     candidate_train_predictors[cname].nunique() < 10 and\n                     candidate_train_predictors[cname].dtype == \"object\"]\nnumeric_cols = [cname for cname in candidate_train_predictors.columns if\n                   candidate_train_predictors[cname].dtype in ['int64', 'float64']]\nmy_cols = low_card_cols + numeric_cols\n\ntrain_predictors = candidate_train_predictors[my_cols]\ntest_predictors = candidate_test_predictors[my_cols]\n\n# one hot encoding\none_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\none_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\n\n# we need to align after doing one hot encoding\naligned_train, aligned_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                    join='left', \n                                                                    axis=1)\n\n# imputation\nmy_imputer = SimpleImputer()\nfinal_train_predictors = my_imputer.fit_transform(aligned_train)\nfinal_test_predictors = my_imputer.transform(aligned_test)\n\n# train model\nmy_model = RandomForestRegressor(50)\nmy_model.fit(final_train_predictors, target)\n\n# run model on test data\npredicted_prices = my_model.predict(final_test_predictors)\n# print(predicted_prices)\n\n# prepare submission\nmy_submission = pd.DataFrame({'Id': test_data.Id, 'SalePrice': predicted_prices})\nmy_submission.to_csv('submission2.csv', index=False)","1e9a425a":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\n\ntrain_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\ntarget = train_data.SalePrice\n\n# drop unnecessary columns\ncandidate_train_predictors = train_data.drop(['Id', 'SalePrice'], axis=1)\ncandidate_test_predictors = test_data.drop(['Id'], axis=1)\n\n# only consider low cardinality and numeric columns for now\nlow_card_cols = [cname for cname in candidate_train_predictors.columns if \n                     candidate_train_predictors[cname].nunique() < 10 and\n                     candidate_train_predictors[cname].dtype == \"object\"]\nnumeric_cols = [cname for cname in candidate_train_predictors.columns if\n                   candidate_train_predictors[cname].dtype in ['int64', 'float64']]\nmy_cols = low_card_cols + numeric_cols\n\ntrain_predictors = candidate_train_predictors[my_cols]\ntest_predictors = candidate_test_predictors[my_cols]\n\n# one hot encoding\none_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\none_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\n\n# we need to align after doing one hot encoding\naligned_train, aligned_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                    join='left', \n                                                                    axis=1)\n\n# imputation\nmy_imputer = SimpleImputer()\nfinal_train_predictors = my_imputer.fit_transform(aligned_train)\nfinal_test_predictors = my_imputer.transform(aligned_test)\n\n# split as it's required by XGBoost\ntrain_X, test_X, train_y, test_y = train_test_split(final_train_predictors, \n                                                    target,\n                                                    test_size=0.25)\n\n# using XGBoost\nmy_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(train_X, train_y, early_stopping_rounds=10,\n             eval_set=[(test_X, test_y)], verbose=False)\n\n# evaluation\npredictions = my_model.predict(test_X)\nprint(\"Mean aboslute error: \" + str(mean_absolute_error(predictions, test_y)))\n\n# run model on test data\npredicted_prices = my_model.predict(final_test_predictors)\n\n# prepare submission\nmy_submission = pd.DataFrame({'Id': test_data.Id, 'SalePrice': predicted_prices})\nmy_submission.to_csv('submission3.csv', index=False)","fa3af748":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.pipeline import make_pipeline\nfrom xgboost import XGBRegressor\n\n\ntrain_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\ntarget = train_data.SalePrice\n\n# drop unnecessary columns\ncandidate_train_predictors = train_data.drop(['Id', 'SalePrice'], axis=1)\ncandidate_test_predictors = test_data.drop(['Id'], axis=1)\n\n# only consider low cardinality and numeric columns for now\nlow_card_cols = [cname for cname in candidate_train_predictors.columns if \n                     candidate_train_predictors[cname].nunique() < 10 and\n                     candidate_train_predictors[cname].dtype == \"object\"]\nnumeric_cols = [cname for cname in candidate_train_predictors.columns if\n                   candidate_train_predictors[cname].dtype in ['int64', 'float64']]\nmy_cols = low_card_cols + numeric_cols\n\ntrain_predictors = candidate_train_predictors[my_cols]\ntest_predictors = candidate_test_predictors[my_cols]\n\n# one hot encoding\none_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\none_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\n\n# we need to align after doing one hot encoding\naligned_train, aligned_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                        join='left', \n                                                                        axis=1)\n\n# making pipeline\nmy_pipeline = make_pipeline(SimpleImputer(), RandomForestRegressor())\nscores = cross_val_score(my_pipeline, aligned_train, target, scoring='neg_mean_absolute_error')\n\n# imputation\n# my_imputer = SimpleImputer()\n# final_train_predictors = my_imputer.fit_transform(aligned_train)\n# final_test_predictors = my_imputer.transform(aligned_test)\n\n# split as it's required by XGBoost\n# train_X, test_X, train_y, test_y = train_test_split(final_train_predictors, \n#                                                     target,\n#                                                     test_size=0.25)\n\n# using XGBoost\n# my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n# my_model.fit(train_X, train_y, early_stopping_rounds=10,\n#              eval_set=[(train_X, train_y)], verbose=False)\n\n# evaluation\n# predictions = my_model.predict(test_X)\nprint(\"Mean aboslute error: %2f \" %(-1 * scores.mean()))\n\n\n\n# run model on test data\nmy_pipeline.fit(aligned_train, target)\npredicted_prices = my_pipeline.predict(aligned_test)\nprint(predicted_prices)\n\n# # prepare submission\n# my_submission = pd.DataFrame({'Id': test_data.Id, 'SalePrice': predicted_prices})\n# my_submission.to_csv('submission3.csv', index=False)","6d2d92c9":"Comparing: \n1. one hot encoded categoricals + numeric predictors\n2. numerical predictors only\n\nSolution 1 works better we can use this trick to improve our house prediction.","f797303d":"We don't want to evaluate error on training data.\n\nScikit learn has built in way to split data into training data and validation data.","f329b438":"### Imputation","87e60798":"With the help of XGBoost we're now at **38%** (1801\/4755). Pretty significant improvement!","979ebeac":"Using **RandomForest** model to see some performance improvements.","cc1c6c9b":"Without any tuning, RandomForest gives better results (24322) than best results of decision tree (27825).","aa242cd7":"### Imputation with extra columns showing what was imputed","cdc1db0c":"### Convert to using pipelines & cross validation\n* Pipelines makes the code cleaner and easier to change.\n* Cross validation gives better estimate of the model than simple train-test-split, though it takes longer to run.","28c1e1b3":"With imputation and categorical handling, we're jumped from `81%` to `53%`, amazing!","6b7ca15f":"### Helper functions to measure quality","e63b42ba":"### Partial Dependence Plots (feature engineering, skipped)","c06c20a3":"Read data into pandas DataFrames.","d1058b84":"### Dropping columns with missing values","09797cc5":"Ensure test data is encoded in the same manner as training data with **align** command.\n\n`join='left'` works the same way as SQL's left join, the same is true for `join='inner'`.","64d14d40":"# Learning Machine Learning\n**Workspace for the [Machine Learning course](https:\/\/www.kaggle.com\/learn\/machine-learning).**","691006d2":"Add two helper functions to determine best value for parameter: max_leaf_nodes.","113990c3":"## Submitting a model\n\nFirst train the model on training data.","785130ab":"Then evaluate model on test data.","366df16b":"## Handling missing values\n\n### Problem setup","a325519d":"Let's put what we've just learnt into practice and improve our previous submission.\n- add imputation\n- handle of categorical data with one hot encoding","cf8ffa9f":"### Handling categorical data","5f9dd957":"We come up with a list of important features. How to best determine those features is a difficult problem in itself."}}