{"cell_type":{"d1182a72":"code","813d0a34":"code","beedd90e":"code","9701bf6c":"code","91fcb975":"code","48d8e46c":"code","b36888a5":"code","a7e63461":"code","8b408b8c":"code","bbf5aa86":"code","f73cdcd7":"code","1dca0c90":"code","10fc1598":"code","4e1d3c91":"code","8b0dbc8a":"code","d64ef663":"code","3171f9f7":"code","af1bb91b":"code","d381a6c4":"code","229897a0":"code","046d0838":"code","5a45a012":"code","7d5fb397":"code","268eeb4b":"code","06246187":"code","7f84f0e5":"code","fd270d27":"code","94609c77":"code","0ff90642":"code","2e045b7a":"code","8e21cf80":"code","869e4605":"code","86cc1f00":"code","7022692a":"code","c8d0f032":"markdown","9b098e59":"markdown","0b94541b":"markdown","864d6831":"markdown","9ca3bcfc":"markdown","b3a696ba":"markdown","ce4e9c98":"markdown","4ea6d791":"markdown","ca0a243b":"markdown","23d3ef0e":"markdown","6c8355d3":"markdown","df9864bf":"markdown","d3e0329d":"markdown","7e128e2c":"markdown","dfff950d":"markdown","fda16db8":"markdown","d88a2235":"markdown","9066fa5f":"markdown"},"source":{"d1182a72":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename)) \n\n# Any results you write to the current directory are saved as output.","813d0a34":"import warnings\nwarnings.filterwarnings('ignore')","beedd90e":"import os\ndef log(*args):\n    os.system(f'echo \\\"{args}\\\"')\n    print(*args)","9701bf6c":"! pip install tensorflow_datasets","91fcb975":"import tensorflow as tf\nimport tensorflow_datasets.public_api as tfds\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nimport pickle\nimport pandas as pd\nimport numpy as np","48d8e46c":"df_stve = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv')\ndf_cal = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\ndf_price = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')","b36888a5":"# We keep our encoders in a dictionary in case we need it later\nencoders = {}","a7e63461":"# sales from this first day to 1941st day are represented by columns d_1 to d_1941  with a row representing an items in a store\ndf_stve.head(2)","8b408b8c":"# We will not use the id column\ndf_stve.drop(['id','dept_id','state_id'],axis=1,inplace=True)","bbf5aa86":"# No missing values\ndf_stve.isna().sum().sum()","f73cdcd7":"# Now we encode categorical features to numerical values\nfor col in df_stve.columns:\n    if not np.issubdtype(df_stve[col].dtype, np.number):\n        if col not in encoders:\n            encoders[col] = LabelEncoder().fit(df_stve[col])\n        df_stve[col] = encoders[col].transform(df_stve[col])\ndf_stve.head(2)","1dca0c90":"df_cal.head(2)","10fc1598":"print(df_cal.shape)\ndf_cal.isna().sum()","4e1d3c91":"# event columns are missing we will fill them with NA\ncols = ['event_name_1','event_type_1'] # We will after drop event_name_2 and event_type_2: on 1969 records 1964 are null\ndf_cal[cols] = df_cal[cols].fillna('NA')","8b0dbc8a":"df_cal.head(2)","d64ef663":"# We don't want to include these columns in our final dataset so we drop them\ndf_cal.drop(['date','weekday','event_name_2', 'event_type_2','snap_CA','snap_TX','snap_WI'],axis=1, inplace=True)\n# We remove  the d_ prefix in d column\ndf_cal[\"d\"] = df_cal[\"d\"].apply(lambda x : int(x.split(\"_\")[-1])).astype(int)","3171f9f7":"# encoding \nfor col in df_cal.columns:\n    log(col,df_cal[col].dtype)\n    if not np.issubdtype(df_cal[col].dtype, np.number):\n        if col not in encoders:\n            encoders[col] = LabelEncoder().fit(df_cal[col])\n        df_cal[col] = encoders[col].transform(df_cal[col]).astype(np.int32)\n\ndf_cal.head()","af1bb91b":"df_price.head(2)","d381a6c4":"df_price.isna().sum() # No missing","229897a0":"# encoding \nfor col in df_price.columns:\n    log(col,df_price[col].dtype)\n    if not np.issubdtype(df_price[col].dtype, np.number):\n        if col not in encoders:\n            encoders[col] = LabelEncoder().fit(df_price[col])\n        df_price[col] = encoders[col].transform(df_price[col]).astype(np.int32)","046d0838":"# Thanks for the guy who wrote this function \ud83d\ude02\ufe0f.\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: log('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","5a45a012":"df_stve = df_stve.astype('int16')\ndf_cal = reduce_mem_usage(df_cal)\ndf_price = reduce_mem_usage(df_price)\ngc.collect()","7d5fb397":"df_price.head(1)","268eeb4b":"df_cal.head(1) # We can merge them on wm_yr_wk","06246187":"%%time\ndf_cal_price = df_cal.merge(df_price,on=\"wm_yr_wk\")\ndf_cal_price.sort_values(['store_id','item_id','d'],inplace=True);\n# For this challenge, memory is precious so we never miss an opportunity to collect garbage\ndel df_cal   \ndel df_price\ngc.collect()","7f84f0e5":"# Saturday and Sunday are respectively encoded 1,2\ndf_cal_price['weekend']= df_cal_price['wday'].apply(lambda x: 1 if x ==1 or x==2 else 0)\ndf_cal_price.head()","fd270d27":"# Mean\ndef sell_price_rolling_mean(item_data):\n    windows = [7,14,28]\n    for rolling in windows:\n        item_data[f'spma-{rolling}'] = item_data[\"sell_price\"].rolling(window=rolling,min_periods=1).mean()\n        item_data[f'spma-{rolling}'].fillna(0.,inplace=True)\n    return item_data\n\n#MIN\ndef sell_price_rolling_min(item_data):\n    windows = [28]\n    for rolling in windows:\n        item_data[f'spmin-{rolling}'] = item_data[\"sell_price\"].rolling(window=rolling,min_periods=1).min()\n        item_data[f'spmin-{rolling}'].fillna(0.,inplace=True)\n    return item_data\n\n#MAX\ndef sell_price_rolling_max(item_data):\n    windows = [28]\n    for rolling in windows:\n        item_data[f'spmax-{rolling}'] = item_data[\"sell_price\"].rolling(window=rolling,min_periods=1).max()\n        item_data[f'spmax-{rolling}'].fillna(0.,inplace=True)\n    return item_data","94609c77":"df_cal_price = df_cal_price[df_cal_price['d'] <= 1941] #We only have 1941 days sales data ","0ff90642":"df_cal_price_group = df_cal_price.groupby(['item_id','store_id'])\n# Now we are ready to build our final dataset \ud83d\ude0e\ufe0f","2e045b7a":"# The columns in our final dataset\ncolumns = ['item_id','cat_id','store_id','n_sales',] # These are from the sales df\n# We will build these later (means and stds of sales)\ncolumns += ['smean14', 'smean28', 'ssum28', 'smax28', 'smin28']\n# calendar and price data\ncolumns += ['wday', 'month', 'year', 'd', 'event_name_1','event_type_1', 'sell_price', 'weekend','spma-7', 'spma-14', 'spma-28']\n\nlen(columns)","8e21cf80":"# We save metadata\npickle.dump(columns, open(\"columns.pkl\",\"wb\"))\npickle.dump(encoders, open(\"encoders.pkl\",\"wb\"))\ngc.collect()","869e4605":"class MyFullDataset(tfds.core.GeneratorBasedBuilder):\n    VERSION = tfds.core.Version('0.1.0')\n    \n    def _split_generators(self, dl_manager):\n        return [\n            tfds.core.SplitGenerator(\n                    name=f'train',\n                    gen_kwargs={\n                    },\n            )\n        ]\n    \n    def _info(self):\n        # Each record's feature matrice shape will be like (days, features)\n        shape = (1941,len(columns))\n        return tfds.core.DatasetInfo(\n            builder=self,\n            description=(\"\"),\n            features=tfds.features.FeaturesDict({\n                \"input\": tfds.features.Tensor(shape=shape,dtype=tf.float32),\n                \"key\": tfds.features.Tensor(shape=(),dtype=tf.int32),\n            }),\n        )\n    \n    def _generate_examples(self,**args):\n        days = np.arange(1,1942)\n        # We yield the time series one by one\n        for i in range(df_stve.shape[0]):\n            data = df_stve.iloc[i]\n            item_id = data['item_id']\n            store_id= data['store_id']\n            data = data.values\n            # repeat the first 5 cols for all days\n            ids = np.repeat(data[:3].reshape(1,-1),1941,axis=0)\n            \n            # Feature engineering: we can brig some previous sales signals in present \n            df_sales = pd.DataFrame({'n_sales':data[3:]})\n            \n            df_sales['smean7'] = df_sales['n_sales'].rolling(window=7,min_periods=1).mean()\n            df_sales['smean28'] = df_sales['n_sales'].rolling(window=28,min_periods=1).mean()\n            \n            df_sales['ssum28'] = df_sales['n_sales'].rolling(window=28,min_periods=1).sum()\n            \n            df_sales['smax28'] = df_sales['n_sales'].rolling(window=28,min_periods=1).max()\n            df_sales['smin28'] = df_sales['n_sales'].rolling(window=28,min_periods=1).min()\n            \n            df_sales.fillna(0,inplace=True)\n            #print(df_sales.columns)\n            \n            # We get calendar and price data\n            cal_data = df_cal_price_group.get_group((item_id,store_id))\n            # There is no price and calendar data for some days so we have to check and fill with -1\n            if cal_data.shape[0] != 1941:\n                missing_days = np.setxor1d(days, cal_data['d'].values)\n                cal_data = cal_data.append(pd.DataFrame({'d':missing_days}),ignore_index=True)\n            cal_data.sort_values(['d'],inplace=True);\n            cal_data.fillna(-1,inplace=True)\n            \n            cal_data = sell_price_rolling_mean(cal_data)\n            #cal_data = sell_price_rolling_max(cal_data)\n            #cal_data = sell_price_rolling_min(cal_data)\n            #print(cal_data.columns)\n            cal_data = cal_data[['wday', 'month', 'year', 'd', 'event_name_1','event_type_1', 'sell_price', 'weekend','spma-7', 'spma-14', 'spma-28']]\n            \n            \n            # We combine everything in one feature matrice with a shape like (days, features)\n            # The key is the record position and will be useful later to generate the test samples\n            input_ = np.c_[ids,df_sales.values,cal_data].astype(np.float32)\n            yield i, {\n                'input':input_,\n                'key':i,\n            }","86cc1f00":"%%time\n### Create a local copy \n\ndata_dir='.\/' \nbuilder = MyFullDataset(data_dir=data_dir)\n# The following line creates the dataset folder containing the tf records files in \/kaggle\/input\nbuilder.download_and_prepare() ","7022692a":"%%time\n### Copy to GS Bucket\n# To not worry about permission issues the bucket is publicly available in both reads and writes but will be deleted at the end of the competition\ngs_path = 'gs:\/\/bucket59'\nbuilder = MyFullDataset(data_dir=gs_path)\n# If you opted for a bucket, change the gs_path to your bucket's path and uncomment this line\n#builder.download_and_prepare()","c8d0f032":"* The memory doesn't allow us to directly merge the sales data frame(df_stve) with the one containing the price and calendar info(df_cal_price).\n* For items in a stores, we will use the following group object to access the price and calendar data\n","9b098e59":"## Feature engineering\nWe will generate more features from existing ones","0b94541b":"### We install the TFDS library and import required modules:","864d6831":"### Fill missing values & Encode features categorical features","9ca3bcfc":"### Merge calendar and price data","b3a696ba":"* Price rolling MEANs and STDs ...\n\nWe will later use these function and try to bring the previous sell_prices signals  into the present","ce4e9c98":"**One last thing. Please don't forget to upvote the notebook and let a comment \ud83e\udd17\ufe0f**","4ea6d791":"* Price data","ca0a243b":"Now we are no longer limited by memory but by storage space. The max output of a kernel is around 5GB so I limit the number of columns to stay within this limit.\n\n I think one option is store in the temp folder or a GS bucket if you want to use TPU acceleration or generate more features","23d3ef0e":"## Build the final dataset","6c8355d3":"* **Sales data**","df9864bf":"We use TensorFlow datasets public API to prepare our final dataset. For more information on how this API works, you can check this [link](https:\/\/github.com\/tensorflow\/datasets\/blob\/master\/docs\/add_dataset.md)\n\nIn short, our dataset is a class extending tfds.core.GeneratorBasedBuilder and defining the following function:\n* **_split_generators**: Defines dataset splits(eg: training,validation,testing)\n* **_info**:  Builds the tfds.core.DatasetInfo object describing the dataset\n* **_generate_examples**: Yields a record. A (key, example) tuples in the dataset from the source data\n\n\nIn our case, a record or example will be an item with its corresponding data in the store so to build our records we get a row containing time series data from df_stve and combine the sale and calendar information from df_cal_price_group","d3e0329d":"* **Calendar data**","7e128e2c":"# **M5 forecasting -Accuracy:** \n**This is my first Kaggle challenge as a student**\n\nIn this kernel, we will prepare the data from the different CSV files in a more efficient format for our training.\n\n**Desperate times call for desperate measures!** \n\nThe data huge size makes some preprocessing steps and the use of certain memory-intensive forecasting techniques difficult. \nExisting notebooks, limit the number of features used in the dataset to ensure that it fits in memory. And also a certain memory margin must be available for the training process.\nIn this one, we will instead use the maximum features that we consider important to train our model. Instead of keeping our final dataset in memory, we will rather store it in several tfrecord files on the disc using TFDS.\n\nI think Tensorflow's  tf.data API offers great tools that help to avoid memory issues when we have to deal with a huge dataset that cannot fit in memory. The API supports writing descriptive and efficient input pipelines by following a common pattern:\n1. Create a source dataset from your input data.\n1. Apply dataset transformations to preprocess the data.\n1. Iterate over the dataset and process the elements.\n\nIt is the best choice to go with if you opted for a top-down approach and want to train memory and computationally expensive models like LSTM using TensorFlow and Keras.\n\nTo benefit from some advantages of the tf.data API and TUP,GPU accelerators during training, we must imperatively serialize your data and store it in a set of [TFRecord](https:\/\/www.tensorflow.org\/tutorials\/load_data\/tfrecord) files (100-200MB each storing a sequence of binary records of our dataset) that can each be read linearly. This process which can give headaches is greatly simplified by the [TFDS](https:\/\/github.com\/tensorflow\/datasets\/blob\/master\/docs\/add_dataset.md) external module.\n\nOnce our data prepared we could train any TensorFlow model whether it be on CPU, GPU, or TPU without worrying much about memory. So this step is worth it.","dfff950d":"### We load our 3 CSV files\n\n* df_stve  : Contain the sales quantity per item and store\n* df_cal   : Packed time-related information like the date the events ...\n* df_price : The prices of items in stores","fda16db8":"* weekend","d88a2235":"### Let reduce data frames memory before we continue","9066fa5f":"Congratulation now visite my differents training notebooks and see your eyes will be amazed \ud83d\ude48\ufe0f. Training will be much easier, faster, and more memory-efficient.\n\n1. [LSTM model](https:\/\/www.kaggle.com\/tchaye59\/m5-acc-lstm-model)\n1. [Boosting models](https:\/\/www.kaggle.com\/tchaye59\/m5-acc-boosting)"}}