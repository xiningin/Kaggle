{"cell_type":{"8f256f3a":"code","6523afb3":"code","d5663532":"code","64c42385":"code","27edc34a":"code","f4ce5b76":"code","355d4e76":"code","e31a07b6":"code","570b5974":"code","2d699dbe":"code","66b31ae4":"code","6987367e":"code","02937a24":"code","225a4cc3":"code","bf71a975":"code","fcbc778d":"code","750a4007":"code","67eb26a3":"code","03d24d16":"code","b88c4326":"code","fce3c4d9":"code","8ecdc055":"code","1a02b767":"code","04fe1629":"code","60d2d8e1":"code","fd6e95b3":"code","b48f62da":"code","58255024":"code","576ca349":"code","ff6744f5":"code","06f2a497":"code","af66ff5b":"code","dc9135b8":"code","5ed66158":"code","70d394d9":"code","f0d80c55":"code","d8062535":"code","cdd6c008":"markdown","716a1cc3":"markdown","df1be8a5":"markdown","55d58ce5":"markdown","20e24265":"markdown","3f3eca4d":"markdown","d6ff29fa":"markdown"},"source":{"8f256f3a":"# SVC try \n# OneHotEncoding\n# GridSearch and class_weights\n# grid search for the best kernel\n# limiting data to 10,000\n# run with the results from grid search\n# using best results {'C': 1.0, 'degree': 3.0, 'gamma': 4.0}\n# Reducing the sample size to 50,000 entries for Grid Search\n# accuracy round 4\n# final","6523afb3":"%env JOBLIB_TEMP_FOLDER=\/tmp\n%matplotlib inline\n%reload_ext autoreload\n%autoreload 2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import train_test_split , GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.metrics import accuracy_score, f1_score\n\nfrom fastai.structured import *\nfrom fastai.column_data import *\nfrom sklearn.svm import SVC\nfrom sklearn import preprocessing\n\nimport warnings\nwarnings.filterwarnings('ignore')","d5663532":"# Importing the data\ndata = pd.read_csv('..\/input\/globalterrorismdb_0718dist.csv',engine = 'python')","64c42385":"# There appears to be a lot of string and binary data\ndata.head(2)","27edc34a":"# So there are close to 2 Lakh entries and 135 features\ndata.shape","f4ce5b76":"# There are a lot of null values which will be eliminated.\nprint(data.isnull().sum().to_frame().sort_values(0,ascending=False))","355d4e76":"# Get the count of attacks from that event\ndata['count'] = data['eventid'].astype(str).str.slice(-2,).map(lambda x : int(x))\n\n# Standardizing the year\ndata.iyear = data.iyear.apply(lambda x: 2018-x)","e31a07b6":"# Categorical variable list : Nomial - 0 or 1 answer\ncat_no = ['extended','crit1','crit2','crit3','doubtterr','multiple','country','region','specificity','attacktype1','success',\n          'suicide','weaptype1','weapsubtype1','targtype1','targsubtype1','natlty1','guncertain1',\n         'claimed','property','ishostkid','ransom','INT_LOG','INT_IDEO']\n\n#Continous variable list\n#nperpcap is removed due to sparsity \ncont = ['count', 'iyear','imonth','iday','latitude','longitude','nperps','nkill','nkillus']\n\ntarget = ['gname']\n\nids = ['eventid']\n\n# Converting it into int\n# -9 is replacing NA values as the same was followed when fillig the data\nfor v in cat_no:\n    data[v] = data[v].fillna(-9).astype('int32')\n\n# Replacing the negative categories\ndata[cat_no] = data[cat_no].replace(-9,0).astype('int32')\n\n# Coverting it into continous variables\nfor v in cont:\n    data[v] = data[v].fillna(0).astype('float32')\n\n# Replacing missing values with zero, the unknown values will be replaced with 0\ndata[['nperps','nkill','nkillus']] = data[['nperps','nkill','nkillus']].replace(to_replace=[-99,-9],value=0).astype('float32')\n\n# Consolidating it\ndata = data[ids+cat_no+cont+target]","570b5974":"# Test set split\ntest = data[data['gname']=='Unknown']\ntest['gname'] = -999\n\n# Training set split\ntrain = data[data['gname']!='Unknown']\n\n# Label encoding gname\nle = LabelEncoder()\nle.fit(train['gname'])\ntrain['gname'] = le.transform(train['gname'])","2d699dbe":"train = pd.concat([train,test],axis=0)\ntrain = train.reset_index(drop=True)","66b31ae4":"# Let's look at the distribution \n# The class distribution is not even and most of the data is missing\nsns.distplot(train.gname)","6987367e":"# Lets look at the correlation plot\n# We can see that the variables iyear and count have multicollinearity and so does nkill and nkillus.\n# The correlation index is not high so we can keep them.\ncorrmat = train.corr()\nsns.heatmap(corrmat,vmin=0)","02937a24":"# 10000 nperps is a large value by any stretch of imagination, so we will be dropping it\n# removing entries with nkill > 500 Y& nkillus > 500\ntrain[['nkill','nkillus','nperps']].describe()","225a4cc3":"train = train[(train.nperps<10000)]\ntest = test[(test.nperps<10000)]\n\ntrain = train[(train.nkill<500)]\ntest = test[(test.nkill<500)]\n\ntrain = train[(train.nkillus<500)]\ntest = test[(test.nkillus<500)]","bf71a975":"# Creating a df to get the size\nsize_train = train.groupby('gname').size().to_frame().sort_values(0,ascending=False)\nsize_train.describe()","fcbc778d":"# Keeping only the values that have incidents below 50\ntrain = train.drop(list(size_train[size_train[0]<50].index),axis=0)","750a4007":"# The data is highly imbalanced, if the compute limit for kaggle was not limited to 6 hrs then Oversampling would have\n# been performed for the minority class\ntrain[train.gname != -999].gname.hist(bins=100)","67eb26a3":"# Using the proc_df to scales the continous variables, they will be combined again\ntrain, train_y, nas, mapper = proc_df(train, 'gname', do_scale=True,ignore_flds=['eventid','extended','crit1','crit2',\n                                                                                 'crit3','doubtterr','multiple','country','region',\n                                                                                 'specificity','attacktype1','success','suicide',\n                                                                                 'weaptype1','weapsubtype1','targtype1','targsubtype1',\n                                                                                 'natlty1','guncertain1','claimed','property','ishostkid',\n                                                                                 'ransom','INT_LOG','INT_IDEO'])\n\ntrain = train.reset_index(drop=True)\ntrain = pd.concat([train,pd.DataFrame(train_y)],axis=1,join_axes=[train.index])\ntrain=train.rename(index=str,columns={0:'gname'})\ntrain = train.reset_index(drop=True)","03d24d16":"# Creating a function to oneHotEncode all the categorical variables since there are not ordinal\ndef ohe(train,features):\n    \"\"\"\n    The functions takes the df with the list of arguments to be one hot encoded and returns a df\n    train is the df and features is a list\n    \"\"\"\n    for v in features:\n        df = train[v].values\n        train = train.drop([v],axis=1)\n        oh = OneHotEncoder(sparse=False)\n        df = df.reshape(len(df),1)\n        df = oh.fit_transform(df)\n        df = df[:,1:]\n        train = pd.concat([train,pd.DataFrame(df)],axis=1,sort=False)\n    return train","b88c4326":"train = ohe(train,['country','region','specificity','attacktype1','weaptype1','weapsubtype1','targtype1','targsubtype1','natlty1'])","fce3c4d9":"train.shape","8ecdc055":"# Looking at the final DataFrame\ntrain.head()","1a02b767":"# Test set split\ntest = train[train.gname == -999]\n\n# Training set split\ntrain = train[train.gname!= -999]","04fe1629":"test.shape","60d2d8e1":"# Limiting the data so as to keep the compute time low\n# Kaggle kernel has a limit of 6 hrs.\ntrain = train.iloc[:50000,:]","fd6e95b3":"# To check if any null values have crept in\ntrain.isnull().sum().sum()","b48f62da":"# Dropping the event ID\ntrain.drop(['eventid'],axis='columns',inplace=True)\n\n# Creating the split for validation\ntrainData, validData = train_test_split(train,test_size=0.3)\n\ntrain_X, train_y, nas = proc_df(trainData, 'gname', do_scale=False)\nvalid_X, valid_y, nas = proc_df(validData, 'gname', do_scale=False)","58255024":"train_X.shape,train_y.shape,valid_X.shape,valid_y.shape","576ca349":"# The SVC model had the best overall accruaracy score among the models tested\n# Using the best results from GridSearch\nest = SVC(C=1, gamma=4, kernel='poly', degree=3, max_iter=10000, class_weight='balanced')","ff6744f5":"# Fitting the training data\nest.fit(train_X,train_y)","06f2a497":"# Making the prediction\ny_pred = est.predict(valid_X)","af66ff5b":"# Checking the accruracy and F1 score\n# The F1 score is considered as it paints a better picture when compared to accuracy as accuracy can be high for a highly imbalanced\n# problem like ours.\naccuracy_score(valid_y,y_pred)","dc9135b8":"f1_score(valid_y,y_pred,average='macro')","5ed66158":"# The model was run 4 times to get the average accuracy and f1 score, this is done as the time complexity would be high for performing \n# K-fold cross validation. The limit set for Kaggle notebooks in 6 hrs\n# avg accuracy = 68 % which is the best for among the models tested like lightGBM and KNN & Random forest.","70d394d9":"test = test.reset_index(drop=True) # resetting the index of the df\ntest_X = test.drop(['eventid','gname'],axis='columns') # Dropping the event ID\ntest_pred = est.predict(test_X) # making the prediction\ntest_pred = le.inverse_transform(test_pred) # inverse transform\npred = pd.DataFrame(test_pred) # converting the array into a df","f0d80c55":"# The indexes are same so they can be combined.\ntest.index,pred.index","d8062535":"final = pd.concat([test.eventid,pred],axis=1,join_axes=[test.index]) # combining the final data\n\nfinal.to_csv('Final_pred.csv') # Exporting it to csv file ","cdd6c008":"# Making the final prediction","716a1cc3":"# Training the model","df1be8a5":"# Validation split","55d58ce5":"# List of Steps\n1. Pre Processing\n2. Visualizing \n3. Removing outliers\n4. Validation split\n5. Training the model\n6. Making the final prediction","20e24265":"# Pre Processing ","3f3eca4d":"# Visualizing ","d6ff29fa":"## Removing Outliers\n"}}