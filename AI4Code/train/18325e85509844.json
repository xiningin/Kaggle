{"cell_type":{"443f0de5":"code","aa38cd97":"code","9909a42f":"code","990f329e":"code","0a12c84a":"code","8e8e218f":"code","0dc5006e":"code","88cfb300":"code","ca42c6cd":"code","5056ce4b":"code","905cdd43":"markdown","e11aa777":"markdown","77af68ea":"markdown","3e4de66d":"markdown","2cd6edcd":"markdown","c7093ef4":"markdown","c9345338":"markdown"},"source":{"443f0de5":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.metrics import mean_absolute_error\nimport lightgbm as lgbm\nimport mlb\nimport gc","aa38cd97":"BASE_DIR = Path('..\/input\/mlb-player-digital-engagement-forecasting')\nTRAIN_DIR = Path('..\/input\/mlb-pdef-train-dataset')","9909a42f":"# loading data\nplayers = pd.read_csv(BASE_DIR \/ 'players.csv')\nrosters = pd.read_pickle(TRAIN_DIR \/ 'rosters_train.pkl')\ntargets = pd.read_pickle(TRAIN_DIR \/ 'nextDayPlayerEngagement_train.pkl')\nscores = pd.read_pickle(TRAIN_DIR \/ 'playerBoxScores_train.pkl')\nscores = scores.groupby(['playerId', 'date']).max().reset_index()\n\nplayer_target_stats = pd.read_csv(\"..\/input\/player-target-stats\/player_target_stats.csv\")","990f329e":"# making data for events file, but it dosen't make a score better...\nevents = pd.read_pickle(TRAIN_DIR \/ 'events_train.pkl')\n\ndef make_events_df(df_events=events):\n    \n    \n    game_info = [\"date\"]\n    pitch_cols = [\"pitcherId\", \"pitchType\", \"spinRate\", \"endSpeed\", \"nastyFactor\"]\n    bat_cols = [\"hitterId\", \"totalDistance\", \"launchSpeed\"]\n    \n    # \u6295\u624b\n    pitch = df_events[df_events.gameType==\"R\"][game_info+pitch_cols].copy()\n    ## \u901f\u7403\u306b\u3064\u3044\u3066\n    pitch_fb = pitch[pitch.pitchType.isin([\"FF\", \"FT\"])][[\"date\", \"pitcherId\", \"spinRate\", \"endSpeed\"]]\n    pitch_fb_mean = pitch_fb.groupby([\"date\", \"pitcherId\"], as_index=False).mean()\n    pitch_fb_mean.rename(columns={\"spinRate\":\"spinRate_mean\", \"endSpeed\":\"endSpeed_mean\"}, inplace=True)\n    pitch_fb_max = pitch_fb.groupby([\"date\", \"pitcherId\"], as_index=False).max()\n    pitch_fb_max.rename(columns={\"spinRate\":\"spinRate_max\", \"endSpeed\":\"endSpeed_max\"}, inplace=True)\n    pitch_fb_min = pitch_fb.groupby([\"date\", \"pitcherId\"], as_index=False).min()\n    pitch_fb_min.rename(columns={\"spinRate\":\"spinRate_min\", \"endSpeed\":\"endSpeed_min\"}, inplace=True)\n    ## nastyFactor\u306b\u3064\u3044\u3066\n    pitch_nasty = pitch[[\"date\", \"pitcherId\", \"nastyFactor\"]]\n    pitch_nasty_mean = pitch_nasty.groupby([\"date\", \"pitcherId\"], as_index=False).mean()\n    pitch_nasty_mean.rename(columns={\"nastyFactor\":\"nastyFactor_mean\"}, inplace=True)\n    pitch_nasty_max = pitch_nasty.groupby([\"date\", \"pitcherId\"], as_index=False).max()\n    pitch_nasty_max.rename(columns={\"nastyFactor\":\"nastyFactor_max\"}, inplace=True)\n    pitch_nasty_min = pitch_nasty.groupby([\"date\", \"pitcherId\"], as_index=False).min()\n    pitch_nasty_min.rename(columns={\"nastyFactor\":\"nastyFactor_min\"}, inplace=True)\n    ## \u7d50\u5408\n    events_pitch = pitch_fb_mean.merge(pitch_fb_max, on=[\"date\", \"pitcherId\"])\n    events_pitch = events_pitch.merge(pitch_fb_min, on=[\"date\", \"pitcherId\"])\n    events_pitch = events_pitch.merge(pitch_nasty_mean, on=[\"date\", \"pitcherId\"])\n    events_pitch = events_pitch.merge(pitch_nasty_max, on=[\"date\", \"pitcherId\"])\n    events_pitch = events_pitch.merge(pitch_nasty_min, on=[\"date\", \"pitcherId\"])\n    ## \u540d\u524d\u5909\u66f4\n    events_pitch.rename(columns={\"pitcherId\":\"playerId\"}, inplace=True)\n\n    \n    # \u6253\u8005\n    bat = df_events[(df_events.gameType==\"R\")&(df_events.launchAngle>5)][game_info+bat_cols]\n    events_bat = bat.groupby([\"date\", \"hitterId\"], as_index=False).max()\n    events_bat.rename(columns={\"hitterId\":\"playerId\"}, inplace=True)\n    \n    del df_events, pitch, pitch_fb, pitch_fb_mean, pitch_fb_max, pitch_fb_min, \n    del pitch_nasty, pitch_nasty_mean, pitch_nasty_max, pitch_nasty_min\n    del bat\n    gc.collect()\n    \n    \n    return events_pitch.merge(events_bat, on=[\"playerId\", \"date\"], how=\"outer\")\n\nevents_train = make_events_df(events)","0a12c84a":"# make chosen columns\ntargets_cols = ['playerId', 'target1', 'target2', 'target3', 'target4', 'date']\nplayers_cols = ['playerId', 'primaryPositionName']\nrosters_cols = ['playerId', 'teamId', 'status', 'date']\nscores_cols = ['playerId', 'battingOrder', 'gamesPlayedBatting', 'flyOuts',\n       'groundOuts', 'runsScored', 'doubles', 'triples', 'homeRuns',\n       'strikeOuts', 'baseOnBalls', 'intentionalWalks', 'hits', 'hitByPitch',\n       'atBats', 'caughtStealing', 'stolenBases', 'groundIntoDoublePlay',\n       'groundIntoTriplePlay', 'plateAppearances', 'totalBases', 'rbi',\n       'leftOnBase', 'sacBunts', 'sacFlies', 'catchersInterference',\n       'pickoffs', 'gamesPlayedPitching', 'gamesStartedPitching',\n       'completeGamesPitching', 'shutoutsPitching', 'winsPitching',\n       'lossesPitching', 'flyOutsPitching', 'airOutsPitching',\n       'groundOutsPitching', 'runsPitching', 'doublesPitching',\n       'triplesPitching', 'homeRunsPitching', 'strikeOutsPitching',\n       'baseOnBallsPitching', 'intentionalWalksPitching', 'hitsPitching',\n       'hitByPitchPitching', 'atBatsPitching', 'caughtStealingPitching',\n       'stolenBasesPitching', 'inningsPitched', 'saveOpportunities',\n       'earnedRuns', 'battersFaced', 'outsPitching', 'pitchesThrown', 'balls',\n       'strikes', 'hitBatsmen', 'balks', 'wildPitches', 'pickoffsPitching',\n       'rbiPitching', 'gamesFinishedPitching', 'inheritedRunners',\n       'inheritedRunnersScored', 'catchersInterferencePitching',\n       'sacBuntsPitching', 'sacFliesPitching', 'saves', 'holds', 'blownSaves',\n       'assists', 'putOuts', 'errors', 'chances', 'date']\n\nfeature_cols = [\n    'label_playerId', 'label_primaryPositionName', 'label_teamId',\n    'label_status', 'battingOrder', 'gamesPlayedBatting', 'flyOuts',\n    'groundOuts', 'runsScored', 'doubles', 'triples', 'homeRuns',\n    'strikeOuts', 'baseOnBalls', 'intentionalWalks', 'hits', 'hitByPitch',\n    'atBats', 'caughtStealing', 'stolenBases', 'groundIntoDoublePlay',\n    'groundIntoTriplePlay', 'plateAppearances', 'totalBases', 'rbi',\n    'leftOnBase', 'sacBunts', 'sacFlies', 'catchersInterference',\n    'pickoffs', 'gamesPlayedPitching', 'gamesStartedPitching',\n    'completeGamesPitching', 'shutoutsPitching', 'winsPitching',\n    'lossesPitching', 'flyOutsPitching', 'airOutsPitching',\n    'groundOutsPitching', 'runsPitching', 'doublesPitching',\n    'triplesPitching', 'homeRunsPitching', 'strikeOutsPitching',\n    'baseOnBallsPitching', 'intentionalWalksPitching', 'hitsPitching',\n    'hitByPitchPitching', 'atBatsPitching', 'caughtStealingPitching',\n    'stolenBasesPitching', 'inningsPitched', 'saveOpportunities',\n    'earnedRuns', 'battersFaced', 'outsPitching', 'pitchesThrown', 'balls',\n    'strikes', 'hitBatsmen', 'balks', 'wildPitches', 'pickoffsPitching',\n    'rbiPitching', 'gamesFinishedPitching', 'inheritedRunners',\n    'inheritedRunnersScored', 'catchersInterferencePitching',\n    'sacBuntsPitching', 'sacFliesPitching', 'saves', 'holds', 'blownSaves',\n    'assists', 'putOuts', 'errors', 'chances','target1_mean',\n    'target1_median',\n    'target1_std',\n    'target1_min',\n    'target1_max',\n    'target1_prob',\n    'target2_mean',\n    'target2_median',\n    'target2_std',\n    'target2_min',\n    'target2_max',\n    'target2_prob',\n    'target3_mean',\n    'target3_median',\n    'target3_std',\n    'target3_min',\n    'target3_max',\n    'target3_prob',\n    'target4_mean',\n    'target4_median',\n    'target4_std',\n    'target4_min',\n    'target4_max',\n    'target4_prob',\n    \n    'spinRate_mean',\n    'endSpeed_mean',\n    'spinRate_max',\n    'endSpeed_max',\n    'spinRate_min',\n    'endSpeed_min',\n    'nastyFactor_mean',\n    'nastyFactor_max',\n    'nastyFactor_min',\n    'totalDistance',\n    'launchSpeed'\n]","8e8e218f":"# creat dataset\ntrain = targets[targets_cols].merge(players[players_cols], on=['playerId'], how='left')\ntrain = train.merge(rosters[rosters_cols], on=['playerId', 'date'], how='left')\ntrain = train.merge(scores[scores_cols], on=['playerId', 'date'], how='left')\ntrain = train.merge(player_target_stats, how='inner', left_on=[\"playerId\"],right_on=[\"playerId\"])\n\ndel targets, rosters, scores\ngc.collect()\n\n\ntrain = train.merge(events_train, on=[\"date\", \"playerId\"], how=\"left\")\n\ndel events_train\ngc.collect()\n\n\n\n# label encoding\nplayer2num = {c: i for i, c in enumerate(train['playerId'].unique())}\nposition2num = {c: i for i, c in enumerate(train['primaryPositionName'].unique())}\nteamid2num = {c: i for i, c in enumerate(train['teamId'].unique())}\nstatus2num = {c: i for i, c in enumerate(train['status'].unique())}\ntrain['label_playerId'] = train['playerId'].map(player2num)\ntrain['label_primaryPositionName'] = train['primaryPositionName'].map(position2num)\ntrain['label_teamId'] = train['teamId'].map(teamid2num)\ntrain['label_status'] = train['status'].map(status2num)","0dc5006e":"# split train and valid\ntrain_X = train[feature_cols]\ntrain_y = train[['target1', 'target2', 'target3', 'target4']]\n\n\n_index = (train['date'] < 20210401)\nx_train = train_X.loc[_index].reset_index(drop=True)\ny_train = train_y.loc[_index].reset_index(drop=True)\nx_valid = train_X.loc[~_index].reset_index(drop=True)\ny_valid = train_y.loc[~_index].reset_index(drop=True)\n\ndel train, train_X, train_y\ngc.collect()","88cfb300":"def fit_lgbm(x_train, y_train, x_valid, y_valid, x_test, params: dict=None, verbose=100, t_num=None):\n    oof_pred = np.zeros(len(y_valid), dtype=np.float32)\n    model = lgbm.LGBMRegressor(**params)\n    model.fit(x_train, y_train, \n        eval_set=[(x_valid, y_valid)],  \n        early_stopping_rounds=verbose, \n        verbose=verbose)\n    oof_pred = model.predict(x_test)\n    score = mean_absolute_error(oof_pred, y_valid)\n    print('mae:', score)\n    return oof_pred, model, score\n\n\n# training lightgbm\n\nparams1 = {\n    'objective':'mae',\n    'reg_alpha': 0.14947461820098767, \n    'reg_lambda': 0.10185644384043743, \n    'n_estimators': 3633, \n    'learning_rate': 0.08046301304430488, \n    'num_leaves': 674, \n    'feature_fraction': 0.9101240539122566, \n    'bagging_fraction': 0.9884451442950513, \n    'bagging_freq': 8, \n    'min_child_samples': 51,\n    'random_state': 42,\n}\n\nparams2 = {\n 'objective':'mae',\n 'reg_alpha': 0.1,\n 'reg_lambda': 0.1, \n 'n_estimators': 1000,#80,\n 'learning_rate': 0.1,\n 'random_state': 42,\n \"num_leaves\": 22,\n}\n\nparams3 = {\n 'objective':'mae',\n 'reg_alpha': 0.1,\n 'reg_lambda': 0.1, \n 'n_estimators': 10000,\n 'learning_rate': 0.1,\n 'random_state': 42,\n \"num_leaves\": 100,\n}\n\nparams4 = {\n    'objective':'mae',\n    'reg_alpha': 0.016468100279441976, \n    'reg_lambda': 0.09128335764019105, \n    'n_estimators': 9868, \n    'learning_rate': 0.10528150510326864, \n    'num_leaves': 157, \n    'feature_fraction': 0.5419185713426886, \n    'bagging_fraction': 0.2637405128936662, \n    'bagging_freq': 19, \n    'min_child_samples': 71,\n    'random_state': 42,\n}\n\n\noof1_eval, model1_eval, score1_eval = fit_lgbm(\n    x_train, y_train['target1'],\n    x_valid, y_valid['target1'],\n    x_valid,\n    params1\n)\n\nx_train2 = x_train.copy()\nx_train2[\"target1\"] = y_train[\"target1\"]\nx_valid2, x_test2 = x_valid.copy(), x_valid.copy()\nx_valid2[\"target1\"] = y_valid[\"target1\"]\nx_test2[\"target1\"] = oof1_eval\n\noof2_eval, model2_eval, score2_eval = fit_lgbm(\n    x_train2, y_train['target2'],\n    x_valid2, y_valid['target2'],\n    x_test2,\n    params2\n)\n\n\nx_train3 = x_train2.copy()\nx_train3[\"target2\"] = y_train[\"target2\"]\nx_valid3, x_test3 = x_valid2.copy(), x_valid2.copy()\nx_valid3[\"target2\"] = y_valid[\"target2\"]\nx_test3[\"target2\"] = oof2_eval\n\ndel x_train2, x_valid2, x_test2\ngc.collect()\n\noof3_eval, model3_eval, score3_eval = fit_lgbm(\n    x_train3, y_train['target3'],\n    x_valid3, y_valid['target3'],\n    x_test3,\n    params3\n)\n\nx_train4 = x_train3.copy()\nx_train4[\"target3\"] = y_train[\"target3\"]\nx_valid4, x_test4 = x_valid3.copy(), x_valid3.copy()\nx_valid4[\"target3\"] = y_valid[\"target3\"]\nx_test4[\"target3\"] = oof3_eval\n\ndel x_train3, x_valid3, x_test3\ngc.collect()\n\noof4_eval, model4_eval, score4_eval = fit_lgbm(\n    x_train4, y_train['target4'],\n    x_valid4, y_valid['target4'],\n    x_test4,\n    params4\n)\n\nparams_dict = {\n    \"target1\":params1, \"target2\":params2, \"target3\":params3, \"target4\":params4, \n}\nmodels_eval = {\n    \"target1\":model1_eval, \"target2\":model2_eval, \"target3\":model3_eval, \"target4\":model4_eval, \n}\n\n\ndel x_train4, x_valid4, x_test4\ngc.collect()\n\n\nscore_eval = (score1_eval+score2_eval+score3_eval+score4_eval) \/ 4\nprint(f'score: {score_eval}')","ca42c6cd":"# models trained by all data\nfor i in range(1, 4+1):\n    target_name = \"target\" + str(i)\n    params_dict[target_name][\"n_estimators\"] = models_eval[target_name].best_iteration_\n\n    \ndel oof1_eval, model1_eval, score1_eval, oof2_eval, model2_eval, score2_eval\ndel oof3_eval, model3_eval, score3_eval, oof4_eval, model4_eval, score4_eval\ngc.collect()\n\n    \nx_train_all = x_train.append(x_valid)\ny_train_all = y_train.append(y_valid)\n\ndel x_train, y_train, x_valid, y_valid\ngc.collect()\n\n\noof1, model1, score1 = fit_lgbm(\n    x_train_all, y_train_all['target1'],\n    x_train_all, y_train_all['target1'],\n    x_train_all,\n    params_dict['target1'],\n    verbose=100\n)\n\nx_train_all2,  x_test_all2 = x_train_all.copy(), x_train_all.copy()\nx_train_all2[\"target1\"] = y_train_all[\"target1\"]\nx_test_all2[\"target1\"] = oof1\n\n\noof2, model2, score2 = fit_lgbm(\n    x_train_all2, y_train_all['target2'],\n    x_train_all2, y_train_all['target2'],\n    x_test_all2,\n    params_dict['target2'],\n    verbose=100\n)\n\n\nx_train_all3,  x_test_all3 = x_train_all2.copy(), x_train_all2.copy()\nx_train_all3[\"target2\"] = y_train_all[\"target2\"]\nx_test_all3[\"target2\"] = oof2\n\ndel x_train_all2, x_test_all2\ngc.collect()\n\noof3, model3, score3 = fit_lgbm(\n    x_train_all3, y_train_all['target3'],\n    x_train_all3, y_train_all['target3'],\n    x_test_all3,\n    params_dict['target3'],\n    verbose=100\n)\n\n\nx_train_all4,  x_test_all4 = x_train_all3.copy(), x_train_all3.copy()\nx_train_all4[\"target3\"] = y_train_all[\"target3\"]\nx_test_all4[\"target3\"] = oof3\n\ndel x_train_all3, x_test_all3\ngc.collect()\n\noof4, model4, score4 = fit_lgbm(\n    x_train_all4, y_train_all['target4'],\n    x_train_all4, y_train_all['target4'],\n    x_test_all4,\n    params_dict['target4'],\n    verbose=100\n)\n\ndel x_train_all4, x_test_all4\ngc.collect()\n\nscore = (score1 + score2 + score3 + score4) \/ 4\nprint(f'score: {score}')","5056ce4b":"players_cols = ['playerId', 'primaryPositionName']\nrosters_cols = ['playerId', 'teamId', 'status']\nscores_cols = [\"date\", 'playerId', 'battingOrder', 'gamesPlayedBatting', 'flyOuts',\n       'groundOuts', 'runsScored', 'doubles', 'triples', 'homeRuns',\n       'strikeOuts', 'baseOnBalls', 'intentionalWalks', 'hits', 'hitByPitch',\n       'atBats', 'caughtStealing', 'stolenBases', 'groundIntoDoublePlay',\n       'groundIntoTriplePlay', 'plateAppearances', 'totalBases', 'rbi',\n       'leftOnBase', 'sacBunts', 'sacFlies', 'catchersInterference',\n       'pickoffs', 'gamesPlayedPitching', 'gamesStartedPitching',\n       'completeGamesPitching', 'shutoutsPitching', 'winsPitching',\n       'lossesPitching', 'flyOutsPitching', 'airOutsPitching',\n       'groundOutsPitching', 'runsPitching', 'doublesPitching',\n       'triplesPitching', 'homeRunsPitching', 'strikeOutsPitching',\n       'baseOnBallsPitching', 'intentionalWalksPitching', 'hitsPitching',\n       'hitByPitchPitching', 'atBatsPitching', 'caughtStealingPitching',\n       'stolenBasesPitching', 'inningsPitched', 'saveOpportunities',\n       'earnedRuns', 'battersFaced', 'outsPitching', 'pitchesThrown', 'balls',\n       'strikes', 'hitBatsmen', 'balks', 'wildPitches', 'pickoffsPitching',\n       'rbiPitching', 'gamesFinishedPitching', 'inheritedRunners',\n       'inheritedRunnersScored', 'catchersInterferencePitching',\n       'sacBuntsPitching', 'sacFliesPitching', 'saves', 'holds', 'blownSaves',\n       'assists', 'putOuts', 'errors', 'chances']\nevents_cols =[\n    'date', 'playerId', 'spinRate_mean', 'endSpeed_mean', 'spinRate_max',\n    'endSpeed_max', 'spinRate_min', 'endSpeed_min', 'nastyFactor_mean',\n    'nastyFactor_max', 'nastyFactor_min', 'totalDistance', 'launchSpeed'\n]\n\n\nnull = np.nan\ntrue = True\nfalse = False\n\nenv = mlb.make_env() # initialize the environment\niter_test = env.iter_test() # iterator which loops over each date in test set\n\nfor (test_df, sample_prediction_df) in iter_test: # make predictions here\n    \n    sample_prediction_df = sample_prediction_df.reset_index(drop=True)\n    \n    # creat dataset\n    sample_prediction_df['playerId'] = sample_prediction_df['date_playerId']\\\n                                        .map(lambda x: int(x.split('_')[1]))\n    # Dealing with missing values\n    if test_df['rosters'].iloc[0] == test_df['rosters'].iloc[0]:\n        test_rosters = pd.DataFrame(eval(test_df['rosters'].iloc[0]))\n    else:\n        test_rosters = pd.DataFrame({'playerId': sample_prediction_df['playerId']})\n        for col in rosters_cols:\n            if col == 'playerId': continue\n            test_rosters[col] = np.nan\n            \n    if test_df['playerBoxScores'].iloc[0] == test_df['playerBoxScores'].iloc[0]:\n        test_scores = pd.DataFrame(eval(test_df['playerBoxScores'].iloc[0]))\n        test_scores[\"date\"] = test_df.index[0]\n    else:\n        test_scores = pd.DataFrame({'playerId': sample_prediction_df['playerId']})\n        for col in scores_cols:\n            if col == 'playerId':\n                pass\n            elif col == \"date\":\n                test_scores[\"date\"] = test_df.index[0]\n            else:\n                test_scores[col] = np.nan\n    \n    \n    if test_df['events'].iloc[0] == test_df['events'].iloc[0]:\n        test_events = pd.DataFrame(eval(test_df['events'].iloc[0]))\n        test_events[\"date\"] = test_df.index[0]\n        test_events = make_events_df(test_events)\n    else:\n        test_events = pd.DataFrame({'playerId': sample_prediction_df['playerId']})\n        for col in events_cols:\n            if col == 'playerId':\n                pass\n            elif col == \"date\":\n                test_events[\"date\"] = test_df.index[0]\n            else:\n                test_events[col] = np.nan\n    \n    \n    \n    test_scores = test_scores.groupby(['playerId', \"date\"]).max().reset_index()\n    test = sample_prediction_df[['playerId']].copy()\n    test = test.merge(players[players_cols], on='playerId', how='left')\n    test = test.merge(test_rosters[rosters_cols], on='playerId', how='left')\n    test = test.merge(test_scores[scores_cols], on='playerId', how='left')\n    test = test.merge(player_target_stats, on=[\"playerId\"], how='left')\n    \n    test = test.merge(test_events, on=[\"date\", \"playerId\"], how='left')\n    del test_events\n    gc.collect()\n\n    \n    test['label_playerId'] = test['playerId'].map(player2num)\n    test['label_primaryPositionName'] = test['primaryPositionName'].map(position2num)\n    test['label_teamId'] = test['teamId'].map(teamid2num)\n    test['label_status'] = test['status'].map(status2num)\n    \n    test_X = test[feature_cols]\n    \n    \n    \n    # predict\n    pred1 = model1.predict(test_X)\n    \n    test_X2 = test_X.copy()\n    test_X2[\"target1\"] = pred1\n    pred2 = model2.predict(test_X2)\n    \n    test_X3 = test_X2.copy()\n    test_X3[\"target2\"] = pred2\n    del test_X2\n    gc.collect()\n    pred3 = model3.predict(test_X3)\n    \n    test_X4 = test_X3.copy()\n    test_X4[\"target3\"] = pred3\n    del test_X3\n    gc.collect()\n    pred4 = model4.predict(test_X4)\n    \n    del test_X4\n    gc.collect()\n    \n    # merge submission\n    sample_prediction_df['target1'] = np.clip(pred1, 0, 100)\n    sample_prediction_df['target2'] = np.clip(pred2, 0, 100)\n    sample_prediction_df['target3'] = np.clip(pred3, 0, 100)\n    sample_prediction_df['target4'] = np.clip(pred4, 0, 100)\n    sample_prediction_df = sample_prediction_df.fillna(0.)\n    del sample_prediction_df['playerId']\n    \n    env.predict(sample_prediction_df)","905cdd43":"# OverView\n- This is a code of my submission.\n- This is not my last submission but almost the same.\n    - My last submission is little more complex, so I choose this simple one. Scores are not so different.\n- My last submission is leaked and score is about 1.31, but definitely leaked.\n- I think my submission score is about 1.35, same as this one.\n\n\n# My Original Effort (only one thing)\n- I trained model for target1 from dataset, and model for target2 from dataset+target1, model for target3 from dataset+target1+target2, model for target4 from dataset+target1+target2+target3\n- I predicted targets as above by predicted ones.\n\n# About More Efforts and My Skills\n- I wanted to make things better, but I couldn't do it because I couldn't make enough time, and I don't have enough skills.\n- I'm just a beginner and this is my first submission for a (ongoing) competition. So I know almost nothing about machine learning, kaggle and python. Please comment anything helpful or useful. Thank you!\n\n# Sorry for my poor English...\n- I'm sorry for my poor English. If you tell me better way of making English comments, I'm verry happy!","e11aa777":"---","77af68ea":"## Make Dataset","3e4de66d":"## Make Predictions","2cd6edcd":"- As my first comment above, my effort is\n    - I trained model for target1 from dataset, and model for target2 from dataset+target1, model for target3 from dataset+target1+target2, model for target4 from dataset+target1+target2+target3\n    - I predict targets as above by predicted ones.\n- It may be bad way for validation, but I don't know much about it. **Please tell me how did you make it or\/and tell me good information or books for making good validation.**\n- Parameters are not optimized by these models. These parameters are from other persons ones.","c7093ef4":"# Make Models","c9345338":"At first, I made dataset. There is nothing special, but remark that it contains some target encodings. It makes a score little better."}}