{"cell_type":{"89c7bb88":"code","538e3f07":"code","f2ac06a0":"code","7d0a3671":"code","72f23c9c":"code","70467d6f":"code","87fdb627":"code","3e8bfc8e":"code","0f1324f0":"code","1adb9b29":"code","242cd3fd":"code","18710b70":"code","bb7d1087":"code","87a1fe46":"code","ba00da66":"code","0a3aed25":"code","76664419":"code","26d8bdeb":"code","251768b4":"code","17df3b6d":"code","4874a266":"code","c5f0edbd":"code","1115e961":"code","a3bc4713":"code","4feb5b0c":"code","6378cea5":"markdown","8e247229":"markdown","db60d963":"markdown","c40fa05d":"markdown","5f90aa55":"markdown","b3eeacb9":"markdown","564b104b":"markdown","277e3a2e":"markdown","305554ad":"markdown","11d54c46":"markdown","ef09b36c":"markdown","35c3ce3b":"markdown","41858db8":"markdown","fae86bd5":"markdown","42e7a10a":"markdown","d82487fd":"markdown","5492118f":"markdown","7b86dc18":"markdown"},"source":{"89c7bb88":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt","538e3f07":"bc = pd.read_csv(r'..\/input\/data.csv')\nbc.head(10)","f2ac06a0":"bc.isnull().sum()","7d0a3671":"bc.dtypes","72f23c9c":"drop_cols = ['Unnamed: 32','id']\nbc = bc.drop(drop_cols, axis = 1)","70467d6f":"bc.shape","87fdb627":"bc['diagnosis'] = bc['diagnosis'].map({'M': 1, 'B': 0})\nbc.head()","3e8bfc8e":"bc['diagnosis'].value_counts()","0f1324f0":"# plotting the labels with the frequency \nLabels = ['Benign','Malignant']\nclasses = pd.value_counts(bc['diagnosis'], sort = True)\nclasses.plot(kind = 'bar', rot=0)\nplt.title(\"Transaction class distribution\")\nplt.xticks(range(2), Labels)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","1adb9b29":"# Plotting the features with each other.\ngroups = bc.groupby('diagnosis')\nfig, ax = plt.subplots()\nfor name, group in groups:\n    ax.plot(group.perimeter_mean, group.texture_mean, marker='o', ms=3.5, linestyle='', \n            label = 'Malignant' if name == 1 else 'Benign')\nax.legend()\nplt.xlabel(\"perimeter_mean\")\nplt.ylabel(\"texture_mean\")\nplt.show()    \n","242cd3fd":"groups = bc.groupby('diagnosis')\nfig, ax = plt.subplots()\nfor name, group in groups:\n    ax.plot(group.radius_mean, group.texture_mean, marker='o', ms=3.5, linestyle='', \n            label = 'Malignant' if name == 1 else 'Benign')\nax.legend()\nplt.ylabel(\"texture_mean\")\nplt.xlabel(\"radius_mean\")\nplt.show()    ","18710b70":"groups = bc.groupby('diagnosis')\nfig, ax = plt.subplots()\nfor name, group in groups:\n    ax.plot(group.area_mean, group.texture_mean, marker='o', ms=3.5, linestyle='', \n            label = 'Malignant' if name == 1 else 'Benign')\nax.legend()\nplt.ylabel(\"texture_mean\")\nplt.xlabel(\"area_mean\")\nplt.show()    ","bb7d1087":"groups = bc.groupby('diagnosis')\nfig, ax = plt.subplots()\nfor name, group in groups:\n    ax.plot(group.smoothness_mean, group.compactness_mean, marker='o', ms=3.5, linestyle='', \n            label = 'Malignant' if name == 1 else 'Benign')\nax.legend()\nplt.ylabel(\"compactness_mean\")\nplt.xlabel(\"smoothness_mean\")\nplt.show()   ","87a1fe46":"import seaborn as sns\nplt.figure(figsize=(12,12)) \nsns.heatmap(bc.corr(),annot=True,cmap='cubehelix_r') \nplt.show()","ba00da66":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split","0a3aed25":"bc_labels = pd.DataFrame(bc['diagnosis'])\nbc_features = bc.drop(['diagnosis'], axis = 1)","76664419":"X_train,X_test,y_train,y_test = train_test_split(bc_features,bc_labels,test_size=0.20)","26d8bdeb":"#Performing cross validation\nneighbors = []\ncv_scores = []\nfrom sklearn.model_selection import cross_val_score\n#perform 10 fold cross validation\nfor k in range(1,10):\n    neighbors.append(k)\n    knn = KNeighborsClassifier(n_neighbors = k)\n    scores = cross_val_score(knn,X_train,y_train,cv=10, scoring = 'accuracy')\n    cv_scores.append(scores.mean())\n    ","251768b4":"#Misclassification error versus k\nMSE = [1-x for x in cv_scores]\n\n#determining the best k\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint('The optimal number of neighbors is %d ' %optimal_k)\n\n#plot misclassification error versus k\n\nplt.figure(figsize = (10,6))\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of neighbors')\nplt.ylabel('Misclassification Error')\nplt.show()","17df3b6d":"knn=KNeighborsClassifier(n_neighbors=optimal_k)\nknn.fit(X_train,y_train)\n","4874a266":"from sklearn.metrics import classification_report\ny_pred = knn.predict(X_test)","c5f0edbd":"# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\n\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","1115e961":"#calculating confusion matrix for knn\ntn,fp,fn,tp=confusion_matrix(y_test,y_pred).ravel()","a3bc4713":"print(\"K-Nearest Neighbours\")\nprint(\"Confusion Matrix\")\nprint(\"tn =\",tn,\"fp =\",fp)\nprint(\"fn =\",fn,\"tp =\",tp)","4feb5b0c":"Labels = ['Benign','Malignant']\nconf_matrix = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(12, 12))\nsns.heatmap(conf_matrix, xticklabels=Labels, yticklabels=Labels, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","6378cea5":"<a id=\"14\"><\/a> \n# 4-Evaluation","8e247229":"<a id=\"7\"><\/a> \n## A-Dropping variables\nBy looking at the dataset, we can see that variable 'id' and variable 'Unnamed: 32' will have no impact on the model building. So we can drop the unneccessary variable to prevent data leakage.\n","db60d963":"<a id=\"6\"><\/a> \n# 2- Feature Extracting","c40fa05d":"<a id=\"9\"><\/a> \n## C-Data visualization","5f90aa55":"<a id=\"12\"><\/a>\n## B-Cross-validation\nTrying to find the optimal number of  neighbors with the lowest Misclassification error. ","b3eeacb9":"<a id=\"3\"><\/a>\n## B-Reading Data","564b104b":"<a id=\"15\"><\/a> \n## A-Model Prediction\n","277e3a2e":"<a id=\"10\"><\/a> \n# 3-Building Model","305554ad":"<a id=\"2\"><\/a> \n## A-Importing libraries\nI am using pandas to manipulate data and matplotlib and seaborn for the data visualization","11d54c46":"<a id=\"11\"><\/a> \n## A-Splitting Data\n","ef09b36c":"<a id=\"4\"><\/a> \n## C-Null values\nChecking for missing values.  ","35c3ce3b":"<a id=\"8\"><\/a> \n## B-Convertion\nFor the model building the dataset need to be in a numerical format. The only variable that is not in numerical format is 'diagnosis'. Since it is a categorical variable, we can take 1 for M & 0 for B","41858db8":"<a id=\"1\"><\/a> \n# 1-Introduction\nFor this dataset I am using K-Nearest Neighbors Algorithm. The dataset contains 33 columns\/features. The target variable is 'diagnosis'. You can find more info on dataset from the Data tab.","fae86bd5":"Draws  heatmap with input as the correlation matrix calculted by(bc.corr()) using seaborn","42e7a10a":"<a id=\"5\"><\/a> \n## D-Data types","d82487fd":"<a id=\"0\"><\/a> <br>\n**Notebook Content**\n1. [Introduction](#1)\n    1. [Importing libraries](#2)\n    1. [Reading Data](#3)\n    1. [Null values](#4)\n    1. [Data types](#5)\n1. [Feature Extracting](#6)\n    1. [Dropping variables](#7)\n    1. [Convertion](#8)\n    1. [Data visualization](#9)\n1.  [Building Model](#10)\n    1. [Splitting Data](#11)\n    1. [Cross-validation](#12)\n    1. [KNNClassifier](#13)\n1. [Evaluation](#14)\n    1. [Model prediction](#15)\n    1. [Report generation](#16)","5492118f":"<a id=\"13\"><\/a> \n## C-KNNClassifier","7b86dc18":"<a id=\"15\"><\/a> \n## B-Report generation\n"}}