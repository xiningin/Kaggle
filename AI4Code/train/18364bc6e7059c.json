{"cell_type":{"0a628daa":"code","10770b12":"code","cdd571b7":"code","26590c3f":"code","4ef68a5f":"code","4fa70b11":"code","d8074a63":"code","07bb92b1":"code","6be288fe":"code","a84d7129":"code","c7fd1a60":"code","7172aaf0":"code","ecf9e867":"code","683384cc":"code","1d9b5309":"code","049b9600":"code","8a8bc14a":"code","c4014de1":"code","7bf77eec":"code","677660f5":"code","642d241d":"code","41aefc94":"code","74c2683c":"code","8ee7673e":"code","4238698c":"code","4c52d326":"code","5b389586":"markdown","b6298e2c":"markdown","68a3216d":"markdown","1aece44a":"markdown","aba97863":"markdown","52c97975":"markdown","f45aae0d":"markdown","4946d5f2":"markdown","2e57d531":"markdown","998b99e3":"markdown","15d0e580":"markdown","2bb2cf5b":"markdown","63c8bddb":"markdown","2e4c32ef":"markdown","b4f9f210":"markdown","16b4b91b":"markdown","2b265641":"markdown"},"source":{"0a628daa":"import pandas as pd\nimport numpy as np\npd.options.display.float_format = '{:.2f}'.format","10770b12":"#load items data \nitems = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv\")\n#load items category data\nitemscat = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\n#load shops data\nshops = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\n#load sales train data  \ntrain = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\n#load sales test data  \ntest = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv\")","cdd571b7":"print(\"************************** items table **************************\\n\", items.head(1),\"\\n******************************************************************************\\n\")\nprint(\"************************** items category table **************************\\n\",itemscat.head(1),\"\\n******************************************************************************\\n\")\nprint(\"************************** shops table **************************\\n\",shops.head(1),\"\\n******************************************************************************\\n\")\nprint(\"************************** train table **************************\\n\",train.head(1),\"\\n******************************************************************************\\n\")\nprint(\"************************** test table **************************\\n\",test.head(1),\"\\n******************************************************************************\\n\")","26590c3f":"train_itms= train.join(items, on='item_id', rsuffix='_')","4ef68a5f":"train_itms_shps = train_itms.join(shops, on='shop_id', rsuffix='_')","4fa70b11":"train_itms_shps_cat = train_itms_shps.join(itemscat, on='item_category_id', rsuffix='_')","d8074a63":"#drop duplicate columns \ntrain_itms_shps_cat = train_itms_shps.join(itemscat, on='item_category_id', rsuffix='_')","07bb92b1":"train_data = train_itms_shps_cat.drop([\"item_id_\",\"shop_id_\",\"item_category_id_\" ], axis=1)","6be288fe":"train_data=train.join(items, on='item_id', rsuffix='_').join(shops, on='shop_id', rsuffix='_').join(itemscat, on='item_category_id', rsuffix='_').drop([\"item_id_\",\"shop_id_\",\"item_category_id_\" ], axis=1)","a84d7129":"# lets convert date data type to date field.\ntrain_data[\"date\"]=pd.to_datetime(train_data[\"date\"])\n#create year and year-month field so we can summarise the data by dates\ntrain_data[\"date_yyyy\"]=pd.to_datetime(train_data[\"date\"]).dt.strftime('%Y') \ntrain_data[\"date_yyyymm\"]=pd.to_datetime(train_data[\"date\"]).dt.strftime('%Y-%m') ","c7fd1a60":"#all duplcates will be stored in duplicate dataframe\nduplicate = train_data[train_data.duplicated()]","7172aaf0":"#we will get rid of the duplicates using \ntrain_final= train_data.drop_duplicates()","ecf9e867":"# train_final is the data without any duplicates\ntrain_final.shape","683384cc":"import seaborn as sns\nsns.boxplot(x=train_final['item_price'])","1d9b5309":"#train_final[train_final[\"item_count\"]>40000].head(30).reset_index() \ntrain_final[train_final[\"item_cnt_day\"]>40000].head(30).reset_index() ","049b9600":"sns.boxplot(x=train_final['item_cnt_day'])","8a8bc14a":"train_final[train_final[\"item_cnt_day\"]>500].head(30).reset_index()","c4014de1":"#WIP, \ntrain_final['item_cnt_day'] = np.where(train_final['item_cnt_day']>400 , 400,train_final['item_cnt_day'] )\n\n#cap item count >500 to 500 \n#cap item price >40000 to 40000 \ntrain_final['item_price'] = np.where(train_final['item_price']>40000 , 40000,train_final['item_price'] )\n\n#another way to do same in python, try . there are tons of other ways!!! \n#train_final['item_cnt_day'] = [400 if x > 400 else x for x in train_final['item_cnt_day']]","7bf77eec":"sns.boxplot(x=train_final['item_cnt_day'])","677660f5":"sns.boxplot(x=train_final['item_price'])","642d241d":"train_data.isnull().sum()","41aefc94":"train_year = train_data.groupby(['date_yyyy'])[\"item_price\",\"item_cnt_day\"].sum().reset_index().sort_values('date_yyyy', ascending=True)\ntrain_year.T","74c2683c":"train_month = train_data.groupby(['date_yyyymm'])[\"item_price\",\"item_cnt_day\"].sum().reset_index().sort_values('date_yyyymm', ascending=True)\ntrain_month.T","8ee7673e":"import seaborn as sns  #import seaborn\nsns.set(rc={'figure.figsize':(5.7,5.27)})\nimport matplotlib.pylab as plt \nplt.xticks(rotation=45)\nsns.barplot(x='date_yyyy', y='item_price', data=train_year)","4238698c":"#Historic sales items per day\nsns.set(rc={'figure.figsize':(20,6)})\nimport matplotlib.pylab as plt\nplt.xticks(rotation=45)\nsns.barplot(x='date_yyyymm', y='item_price', data=train_month)","4c52d326":"#Historic sales items count per day\nsns.set(rc={'figure.figsize':(20,6)})\nimport matplotlib.pylab as plt\nplt.xticks(rotation=45)\nsns.barplot(x='date_yyyymm', y='item_cnt_day', data=train_month)","5b389586":"<h5>All above variables are showing zero's that means there are no variables with missing values. This is a good thing, <span style=\"color:red\"> We are in luck, we can skip missing value treatment step !!!<\/h5><h5>lets summarize all sales by date variable to see if we can learn something. ","b6298e2c":"<h3>Are there any outliers in the dataset","68a3216d":"<h3 style=\"color:red;text-align:center\"> * * *  to be continued * * * <\/h3>","1aece44a":"<h4>Before we begin data analysis, we first need a complete dataset that contains all variables we have for prediction. \n <h4>Here the datasets we are given are scattered and needs to be merged. So lets merge all datasets one by one and create a final dataset by merging all variables.","aba97863":"<h5>We can clearly see 2014 has been the best year in terms of sales. Lets deep dive further and look in to monthly trend","52c97975":"<div style='background-color: #6c3d75; align:center;color:white;border: 4px solid ; text-align:center;margin: 5px;padding:5px;font-size:20px'>Sales Price Prediction Challenge - Step 2 - Analyze data<\/div>\n<div style=\"text-align: center;\">\n<img height=\"150\" width=\"800\"  src=\"https:\/\/sp-ao.shortpixel.ai\/client\/q_glossy,ret_img,w_1000\/https:\/\/www.leadsquared.com\/wp-content\/uploads\/2019\/02\/banner-4.png\" alt=\"C1 Technologies\"><\/div>","f45aae0d":"### Here we will treat the outliers and will cap them around 99th percentile. \n\n<h3><span style=\"color:red\"> Please be warned before doing this. <\/span> ALWAYS ALWAYS study outliers, they give insight in to process, business and outliers provide good information that will help in building statistical models aka ML algos.","4946d5f2":"<h5>but this doesnt look very intuitive, a visual is the easiest way to share the story. <br><br>\nLets display the same details on bar plot and see if it looks better. First lets look at yearly trend of total sales","2e57d531":"<h3> Also, let's change data type for date field same way as first kernel.","998b99e3":"<h4>lets quickly take a look at individual datasets. ","15d0e580":"<h5>As you can see, there are two peaks in the data and they are present in last two months of each year. <BR><BR>This is because of festival season and people are buying a lot in the last couple of months. this is also attributed to a lot of discounts that are provided during festival season. \n<H5> does sales item show the same trend. Lets see ","2bb2cf5b":"<h4 style=\"color:red;text-align:center\"> * * * Please upvote if you like this kernel. * * *<\/h4>  \n  <h4>This is step 2, Please visit <a href='https:\/\/www.kaggle.com\/zenstat\/notebook-for-beginners-step-1-load-the-data' > step 1 kernal code <\/a>to understand the basics of loading data. <\/h4><h4>In this kernal, we will work on second step of the anaysis which is to analyze and understand all variables. We will learn <br><br>\n    * merge and join <br><br>\n    * data quality checks - missing data, outliers <br><br>\n    * data transformation - MVI - missing value imputations<br><br>\n    We will load the data same as in <a href='https:\/\/www.kaggle.com\/zenstat\/notebook-for-beginners-step-1-load-the-data' >step 1 kernel<\/a> Be sure to check it out if you haven't.\n<\/h4>","63c8bddb":"<h3>Here is a cool thing about python. You can do all above merge steps in one line","2e4c32ef":"<h3>if you look carefully at item price column, there is a single item price which is $307,980. while mostly item price are less than 50k. there are only 3 items greater than 50k.\n<br><br>Same way, the count seem to have some exceptionally high values. this time we will treat them both. \n","b4f9f210":"<h5>How to check if there any missing value in any of the variable? ","16b4b91b":"<h3>Check duplicate dataset and you will find 6 records that are duplicate. Below is how we can find and then get rid of duplicates","2b265641":"<h3>How to check if there any duplicates in the datasets? "}}