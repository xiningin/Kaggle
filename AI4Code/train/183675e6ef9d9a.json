{"cell_type":{"120881b6":"code","3ba8e4e9":"code","17ce7750":"code","26b35c5a":"code","cc5bce67":"code","cff18a08":"code","5ae5d763":"code","0c0fa6b9":"code","5b5f78ec":"code","7fb29491":"code","44fac986":"code","a948ee93":"code","f8cb080b":"code","ab7bf862":"code","ece4ae03":"code","b54b2c93":"markdown","04975741":"markdown","191ec0e7":"markdown","0e365485":"markdown","930e7f05":"markdown","2cc4d5c1":"markdown","54ea5364":"markdown","92dbebb3":"markdown","e5724436":"markdown","c32529aa":"markdown","674013ee":"markdown","9824c005":"markdown","40a23784":"markdown","a00fe6cd":"markdown","acc64f11":"markdown","d7433bcf":"markdown","8fa487ed":"markdown","adb2ee0d":"markdown","3d7e746d":"markdown","597f96a3":"markdown","baa052c0":"markdown","7546ab35":"markdown","fffc6d45":"markdown","72f50be0":"markdown","b1b6af98":"markdown","6aa292bc":"markdown"},"source":{"120881b6":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom mlxtend.preprocessing import minmax_scaling\nimport seaborn as sns\nfrom sklearn.decomposition import PCA,SparsePCA,KernelPCA,NMF\nfrom sklearn.datasets import make_circles","3ba8e4e9":"summer_products_path = \"..\/input\/summer-products-and-sales-in-ecommerce-wish\/summer-products-with-rating-and-performance_2020-08.csv\"\nunique_categories_path = \"..\/input\/summer-products-and-sales-in-ecommerce-wish\/unique-categories.csv\"\nunique_categories_sort_path = \"..\/input\/summer-products-and-sales-in-ecommerce-wish\/unique-categories.sorted-by-count.csv\"\n\nsummer_products = pd.read_csv(summer_products_path)\nunique_categories = pd.read_csv(unique_categories_path)\nunique_categories_sort = pd.read_csv(unique_categories_sort_path)\n\ndf = summer_products\n\nC = (df.dtypes == 'object')\nCategoricalVariables = list(C[C].index)\nInteger = (df.dtypes == 'int64') \nFloat   = (df.dtypes == 'float64') \nNumericVariables = list(Integer[Integer].index) + list(Float[Float].index)\n\ndf[NumericVariables]=df[NumericVariables].fillna(0)\ndf=df.drop('has_urgency_banner', axis=1) # 70 % NA's\n\ndf[CategoricalVariables]=df[CategoricalVariables].fillna('Unknown')\ndf=df.drop('urgency_text', axis=1) # 70 % NA's\ndf=df.drop('merchant_profile_picture', axis=1) # 86 % NA's\n\nC = (df.dtypes == 'object')\nCategoricalVariables = list(C[C].index)\nInteger = (df.dtypes == 'int64') \nFloat   = (df.dtypes == 'float64') \nNumericVariables = list(Integer[Integer].index) + list(Float[Float].index)\n\nSize_map  = {'NaN':1, 'XXXS':2,'Size-XXXS':2,'SIZE XXXS':2,'XXS':3,'Size-XXS':3,'SIZE XXS':3,\n            'XS':4,'Size-XS':4,'SIZE XS':4,'s':5,'S':5,'Size-S':5,'SIZE S':5,\n            'M':6,'Size-M':6,'SIZE M':6,'32\/L':7,'L.':7,'L':7,'SizeL':7,'SIZE L':7,\n            'XL':8,'Size-XL':8,'SIZE XL':8,'XXL':9,'SizeXXL':9,'SIZE XXL':9,'2XL':9,\n            'XXXL':10,'Size-XXXL':10,'SIZE XXXL':10,'3XL':10,'4XL':10,'5XL':10}\n\ndf['product_variation_size_id'] = df['product_variation_size_id'].map(Size_map)\ndf['product_variation_size_id']=df['product_variation_size_id'].fillna(1)\nOrdinalVariables = ['product_variation_size_id']\n\nColor_map  = {'NaN':'Unknown','Black':'black','black':'black','White':'white','white':'white','navyblue':'blue',\n             'lightblue':'blue','blue':'blue','skyblue':'blue','darkblue':'blue','navy':'blue','winered':'red',\n             'red':'red','rosered':'red','rose':'red','orange-red':'red','lightpink':'pink','pink':'pink',\n              'armygreen':'green','green':'green','khaki':'green','lightgreen':'green','fluorescentgreen':'green',\n             'gray':'grey','grey':'grey','brown':'brown','coffee':'brown','yellow':'yellow','purple':'purple',\n             'orange':'orange','beige':'beige'}\n\ndf['product_color'] = df['product_color'].map(Color_map)\ndf['product_color']=df['product_color'].fillna('Unknown')\n\nNominalVariables = [x for x in CategoricalVariables if x not in OrdinalVariables]\nLvl = df[NominalVariables].nunique()\n\nToDrop=['title','title_orig','currency_buyer', 'theme', 'crawl_month', 'tags', 'merchant_title','merchant_name',\n              'merchant_info_subtitle','merchant_id','product_url','product_picture','product_id']\ndf = df.drop(ToDrop, axis = 1)\nFinalNominalVariables = [x for x in NominalVariables if x not in ToDrop]\n\ndf_dummy = pd.get_dummies(df[FinalNominalVariables], columns=FinalNominalVariables)\n\ndf_clean = df.drop(FinalNominalVariables, axis = 1)\ndf_clean = pd.concat([df_clean, df_dummy], axis=1)\n\nNumericVariablesNoTarget = [x for x in NumericVariables if x not in ['units_sold']]\ndf_scale=df_clean\ndf_scale = minmax_scaling(df_clean, columns=df_clean.columns)\n\nprint(\"The number of categorical variables: \" + str(len(FinalNominalVariables)+len(OrdinalVariables)) +\"; where 1 ordinal variable and 35 dummy variables\")\nprint(\"The number of numeric variables: \" + str(len(NumericVariables)))\ndf_scale.describe()","17ce7750":"pca = PCA().fit(df_scale)\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5), dpi=80, facecolor='w', edgecolor='k')\nax0, ax1 = axes.flatten()\n\nax0.plot(np.cumsum(pca.explained_variance_ratio_))\nax0.set_xlabel('Number of components')\nax0.set_ylabel('Cumulative explained variance');\n\nax1.bar(range(59),pca.explained_variance_)\nax1.set_xlabel('Number of components')\nax1.set_ylabel('Explained variance');\n\nplt.show()","26b35c5a":"n_PCA_50 = np.size(np.cumsum(pca.explained_variance_ratio_)>0.5) - np.count_nonzero(np.cumsum(pca.explained_variance_ratio_)>0.5)\nn_PCA_80 = np.size(np.cumsum(pca.explained_variance_ratio_)>0.8) - np.count_nonzero(np.cumsum(pca.explained_variance_ratio_)>0.8)\nn_PCA_90 = np.size(np.cumsum(pca.explained_variance_ratio_)>0.9) - np.count_nonzero(np.cumsum(pca.explained_variance_ratio_)>0.9)\nprint(\"Already: \" + format(n_PCA_50) + \" Cover 50% of variance.\")\nprint(\"Already: \" + format(n_PCA_80) + \" Cover 80% of variance.\")\nprint(\"Already: \" + format(n_PCA_90) + \" Cover 90% of variance.\")","cc5bce67":"pca = PCA(12).fit(df_scale)\n\nX_pca=pca.transform(df_scale) \n\nplt.matshow(pca.components_,cmap='viridis')\nplt.yticks([0,1,2,3,4,5,6,7,8,9,10,11,12],['1st Comp','2nd Comp','3rd Comp','4th Comp','5th Comp','6th Comp','7th Comp','8th Comp','9th Comp','10th Comp','11th Comp','12th Comp'],fontsize=10)\nplt.colorbar()\nplt.xticks(range(len(df_scale.columns)),rotation=0)\nplt.tight_layout()\nplt.show()","cff18a08":"CompOne = pd.DataFrame(list(zip(df_scale.columns,pca.components_[0])),columns=('Name','Contribution to Component 1'),index=range(1,60,1))\nCompOne = CompOne[(CompOne['Contribution to Component 1']>0.05) | (CompOne['Contribution to Component 1']< -0.05)]\nCompOne","5ae5d763":"def ExtractColumn(lst,j): \n    return [item[j] for item in lst] \n\nPCA_vars = [0]*len(df_scale.columns)\n\nfor i, feature in zip(range(len(df_scale.columns)),df_scale.columns):\n    x = ExtractColumn(pca.components_,i)\n    if ((max(x) > 0.1) | (min(x) < -0.1)):\n        if abs(max(x)) > abs(min(x)):\n            PCA_vars[i] = max(x)\n        else:\n            PCA_vars[i] = min(x)                 \n    else:\n        PCA_vars[i] = 0\n\nPCA_vars = pd.DataFrame(list(zip(df_scale.columns,PCA_vars)),columns=('Name','Max absolute contribution'),index=range(1,60,1))      \nPCA_vars = PCA_vars[(PCA_vars['Max absolute contribution']!=0)]\nPCA_vars","0c0fa6b9":"SPCA = SparsePCA(n_components=12)\nSPCA_fit = SPCA.fit(df_scale)\n\nplt.matshow(SPCA_fit.components_,cmap='viridis')\nplt.yticks([0,1,2,3,4,5,6,7,8,9,10,11,12],['1st Comp','2nd Comp','3rd Comp','4th Comp','5th Comp','6th Comp','7th Comp','8th Comp','9th Comp','10th Comp','11th Comp','12th Comp'],fontsize=10)\nplt.colorbar()\nplt.xticks(range(len(df_scale.columns)),rotation=0)\nplt.tight_layout()\nplt.show()","5b5f78ec":"SPCA_vars = [0]*len(df_scale.columns)\n\nfor i, feature in zip(range(len(df_scale.columns)),df_scale.columns):\n    x = ExtractColumn(SPCA_fit.components_,i)\n    if ((max(x) > 0.1) | (min(x) < -0.1)):\n        if abs(max(x)) > abs(min(x)):\n            SPCA_vars[i] = max(x)\n        else:\n            SPCA_vars[i] = min(x)                 \n    else:\n        SPCA_vars[i] = 0\n\nSPCA_vars = pd.DataFrame(list(zip(df_scale.columns,SPCA_vars)),columns=('Name','Max absolute contribution'),index=range(1,60,1))      \nSPCA_vars = SPCA_vars[(SPCA_vars['Max absolute contribution']!=0)]\nSPCA_vars","7fb29491":"KPCA = KernelPCA(n_components = len(df_scale.columns), kernel=\"rbf\", fit_inverse_transform=True, gamma=10)\nKPCA_fit = KPCA.fit(df_scale)\nX_KPCA = KPCA.fit_transform(df_scale)\nX_KPCA_back = KPCA.inverse_transform(X_KPCA)","44fac986":"NNMF = NMF(n_components=12)\nNMF_fit = NNMF.fit(df_scale)\n\nGMax = 0\nfor i in range(len(NMF_fit.components_)):\n    Lmax = max(NMF_fit.components_[i])\n    if Lmax > GMax:\n        GMax = Lmax\n    else:\n        GMax = GMax\n        \nScaledList = NMF_fit.components_ \/ GMax\n\nplt.matshow(ScaledList,cmap='viridis')\nplt.yticks([0,1,2,3,4,5,6,7,8,9,10,11,12],['1st Comp','2nd Comp','3rd Comp','4th Comp','5th Comp','6th Comp','7th Comp','8th Comp','9th Comp','10th Comp','11th Comp','12th Comp'],fontsize=10)\nplt.colorbar()\nplt.xticks(range(len(df_scale.columns)),rotation=0)\nplt.tight_layout()\nplt.show()","a948ee93":"NMF_vars = [0]*len(df_scale.columns)\n\nfor i, feature in zip(range(len(df_scale.columns)),df_scale.columns):\n    x = ExtractColumn(ScaledList,i)\n    if ((max(x) > 0.1) | (min(x) < -0.1)):\n        if abs(max(x)) > abs(min(x)):\n            NMF_vars[i] = max(x)\n        else:\n            NMF_vars[i] = min(x)                 \n    else:\n        NMF_vars[i] = 0\n\nNMF_vars = pd.DataFrame(list(zip(df_scale.columns,NMF_vars)),columns=('Name','Max absolute contribution'),index=range(1,60,1))      \nNMF_vars = NMF_vars[(NMF_vars['Max absolute contribution']!=0)]\nNMF_vars","f8cb080b":"All_Features = np.unique(list(PCA_vars['Name'])+list(SPCA_vars['Name'])+list(NMF_vars['Name']))\n\nAll_Features_df =  pd.DataFrame(zip(All_Features,[False]*len(All_Features),[False]*len(All_Features),\n                                [False]*len(All_Features)),columns=['Feature','Is in PCA','Is in SPCA','Is in NMF'])\n\nAll_Features_df['Is in PCA'] = [True if x in list(PCA_vars['Name']) else False for x in All_Features]\nAll_Features_df['Is in SPCA'] = [True if x in list(SPCA_vars['Name']) else False for x in All_Features]\nAll_Features_df['Is in NMF'] = [True if x in list(NMF_vars['Name']) else False for x in All_Features]\n\nAll_Features_df=All_Features_df.sort_values('Feature')\n\nAll_Features_df","ab7bf862":"print(format(sum(All_Features_df['Is in PCA'])) + \" features by PCA; \" + format(sum(All_Features_df['Is in SPCA'])) + \" features by SPCA; \" +\n     format(sum(All_Features_df['Is in NMF'])) + \" features by NMF. \")","ece4ae03":"fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(15, 10), dpi=80, facecolor='w', edgecolor='k')\nax0, ax1, ax2 = axes.flatten()\n\nScaledListPCA = abs(pca.components_)\nScaledListSPCA = abs(SPCA_fit.components_)\n\nax0.matshow(ScaledListPCA,cmap='viridis')\nax1.matshow(ScaledListSPCA,cmap='viridis')\nax2.matshow(ScaledList,cmap='viridis')\n\nplt.show()","b54b2c93":"Principial component analysis, the basic algorithm accepted the most variables. SPCA dropped the most of them being focused just on the most significant elements. For NMF I made scaling to keep results comparable. All these three methods can be boosted by calibration or solvers' applciation. The final plots for: PCA & SPCA were standardized by absolute value to make them comparable with NMF.","04975741":"The idea behind the algorithm is similar, but enhanced. As defined on [scikit website](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.SparsePCA.html):\n> Finds the set of sparse components that can optimally reconstruct the data. The amount of sparseness is controllable by the coefficient of the L1 penalty, given by the parameter alpha.\n\nI see then a use similar to this behind LASSO, namely using L1 metric for penalty. I start analysis choosing 12 components, this value was proposed by solo PCA. The sparsity-inducing norm also prevents learning components from noise when few training samples are available.\n\nFirst, what we expect is sparsity of our results, which in contrast to dense structures (matrices, vectors,..) mean mostly zeros as elements. Namely, components are mapped just to particular parts of data. It simplifies interpretability comapring to regular PCA. Sparse principal components yields a more parsimonious, interpretable representation, clearly emphasizing which of the original features contribute to the differences between samples.","191ec0e7":"This step I take from my [another notebook on this topic](https:\/\/www.kaggle.com\/jjmewtw\/clustering-k-means-hierarchical-debscan-ema). I apply following steps:\n* NA's cleaning\n* categorical variables encoding (nominal & ordinal)\n* numeric variables cleaning\n* scaling","0e365485":"Fast observations: algorithm did not eiminate correlated 'rating' varables; a lot of color dummy variables remained.","930e7f05":"The results of NMF are not standardized. Hence, we apply min-max scaling to have above results between 0 and 1. Still the previously used method would work in any case of it.","2cc4d5c1":"I will compare these results altogether at the end.","54ea5364":"The big difference between solo PCA vs SPCA, NMF is set f features: 'rating'. SPCA and NMF drop these highly-correlated features, not bad. However, they keep different dummy levels for color, second time good job because indeed these features point at another interesting groups.","92dbebb3":"This algorithm is based on process requirement that elements of matricis used have non-negative elements. Paradoxiacally, this non-negativity makes the resulting matrices easier to inspect. Its performance tcomapred to PCA depends on the case and hard to assess it generally.\n\n**Some relations to other technics:** NMF is not the direct part of PCA family, but is considered to be alternative method. Some types of NMF are an instance of a more general probabilistic model called \"multinomial PCA\". What's interesting, NMF with the least-squares objective is equivalent to a relaxed form of K-means clustering. Furthermore, NMF is an instance of nonnegative quadratic programming (NQP), just like the support vector machine (SVM).\n\n","e5724436":"The often question: **is it possible to use KPCA for fetaure selection** can be found [here](https:\/\/stats.stackexchange.com\/questions\/8182\/is-it-possible-to-use-kernel-pca-for-feature-selection) and sounds as follows:\n> (...) in kernel PCA each principal component is a linear combination of features in the target space, and for e.g. Gaussian kernel (which is often used) the target space is infinite-dimensional. So the concept of \"loadings\" does not really make sense for kPCA,(...)","c32529aa":"# Add.Further reading\n* [Data cleaning step by step, notebook by me](https:\/\/www.kaggle.com\/jjmewtw\/prices-cleaning-analysis-estimation-in-stages)\n* [Correlation and the method's relevance, notebook by me](https:\/\/www.kaggle.com\/jjmewtw\/yt-pearson-spearman-distance-corr-rv-coef)\n* [Comprehensive PCA, notebook by Andrea Sindico](https:\/\/www.kaggle.com\/asindico\/customer-segments-with-pca)\n* [Kernel PCA, notebook by \"bronson\"](https:\/\/www.kaggle.com\/jsultan\/visualizing-classifier-boundaries-using-kernel-pca)\n* [PCA, KPCA, KNN, notebook by \"nic\"](https:\/\/www.kaggle.com\/nicw102168\/trying-out-some-pca-nmf-and-knn)","674013ee":"# 2.Principial Component Analysis (PCA)","9824c005":"# 1a.Introduction","40a23784":"# 3.Sparse Principial Component Analysis (Sparse PCA)","a00fe6cd":"Alright, 1st component is vastly dominated by 'uses_ad_boosts', ads are power, ok. But for so many features, so many components, I need something iterative, I will define the function which will choose features which contribute at 0.10 or less than -0.10 to any of components.","acc64f11":"I decide to go further with 12 components, since 80% of variance explained satisfy my arbitrary threshold. For this I plot our 59 variables in columns and principial components in rows. Due to this, I can see how much particular variable contributes to aprticular component and what is its sign (-\/+).","d7433bcf":"The ultimate goal of this notebook is to **explore powerful method called \"Principal component analysis\" and its extensions**. The main use of PCA is dimensionality reduction, what allows for understanding what part of data really contributes to explaining of the variance. PCA is clearly connected with Singular Value Decomposition (SVD) and they both overlap on many layers ([more info](https:\/\/mlfromscratch.com\/principal-component-analysis-pca-svd\/)).\n\nContent:\n* In '1a' and '1b' I introduce the topic, load the data and perform key data cleaning and preparation\n* Chapter '2' corresponds to main **PCA** definition introduction\n* Part '3' presents **Sparse PCA**, in other words PCA with sparsity constraint imposed\n* In chapter '4' I describe **Kernel PCA** which is a repsonse for biggest PCA's shortcoming: linearity assumption\n* Next, in '5' I go through **Non-negative matrix factorization** analysing it as alternative dimensions' reduction method\n* Last, in '6' methods are summarised\n\n**For the clairty all the code was hidden, exception is chapter 4th, where the code is the point of material.**","8fa487ed":"# 4.Kernel Principial Component Analysis (Kernel PCA)","adb2ee0d":"# 1b.Data preparation","3d7e746d":"A lot of variables, let's look closer:","597f96a3":"Indeed, results seem to be correct at a glance. Output matrix meets sparsity requirements. Next I repeat the stage from the previous chapter, where top contributing variables are used.","baa052c0":"My favorite extension of PCA is Kernel PCA (KPCA), which deals epicly with lineariy requirement. Once more: PCA detects only linear connections, KPCA is the amazing answer and crucial generalization of PCA. Using a kernel, the originally linear operations of PCA are performed in a reproducing kernel Hilbert space, more in [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Kernel_principal_component_analysis). I will use the [example from scikit](https:\/\/scikit-learn.org\/stable\/auto_examples\/decomposition\/plot_kernel_pca.html#sphx-glr-auto-examples-decomposition-plot-kernel-pca-py) as basic methodology and repeat the use of this fantastic method for our data.\n\nTo keep it clear, KPCA is not another method for variability detection, but it offers the possibility of data transformation such that it can be used in linear case.","7546ab35":"I will summarize the results comparing the output of PCA, Sparse PCA and NMF. As I mentioned 12 components were used for all methods as on the basis of PCA they contribute to 80% of variability. First I look at the list of variables chosen by our algorithms.","fffc6d45":"# 6.Break it down altogether","72f50be0":"The values were calibrated to receive similar number of features.","b1b6af98":"# 5.Non-negative matrix factorization (NMF)","6aa292bc":"First, I apply aforeentioned PCA to the given data. I do it for whole data set, namely both numeric and categorical data. For this, it is very important to remember to scale and clean data, cause in other case variables with bigger numbers will dominate the analysis. For the theory [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis) should suffice, chapter: \"Details\".\n\nBasically, I would like to assess how many components suffice to cover the majority of variance in the data set. For this I look at cumulative variance explained:"}}