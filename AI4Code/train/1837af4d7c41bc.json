{"cell_type":{"52f7bb32":"code","2dd4ee61":"code","e7b41bc3":"code","d41f4049":"code","48914239":"code","26559ae7":"code","e01fee3b":"code","5eb5403a":"code","72a65848":"code","7b511ada":"code","ecf23749":"code","37c01bbd":"code","c57f1118":"code","e67584e9":"code","14c2afe8":"code","b5af4f4c":"code","721b55be":"code","b1536468":"code","825e9a4a":"code","f1f4b12c":"code","609afffa":"code","1bcde469":"code","8b39837b":"code","c34995c6":"code","051fbae0":"code","fa4ffb99":"code","80af880a":"code","d4a09767":"code","a7a0de65":"code","2d3dcd16":"code","af6e8375":"code","f247f11c":"code","03ab0cee":"code","50ab2cbd":"code","5b203437":"code","7f742d8d":"code","b1e21571":"code","8d5a6cdc":"code","32048c1c":"code","d041ebcc":"markdown"},"source":{"52f7bb32":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","2dd4ee61":"df = pd.read_csv('..\/input\/Consumer_Complaints.csv', nrows = 50000)\nprint(df.shape)\ndf.head()","e7b41bc3":"_ = sns.countplot(y = df['Product'])","d41f4049":"df['Product'].value_counts()","48914239":"# Data Processing: Drop the fatures that are not used for Neural Networks Training\nsel_df = df.iloc[:, [1,5]]\nsel_df = sel_df.dropna()\nprint(sel_df.info())\nprint(sel_df.head())\n_ = sns.countplot(y = sel_df['Product'])","26559ae7":"sel_df.Product.value_counts()","e01fee3b":"# Category selection\n# categories = ['Debt collection', 'Mortgage', 'Credit reporting', 'Credit card', 'Student loan', 'Consumer Loan']\nexclude = [\n    'Credit reporting, credit repair services, or other personal consumer reports', \n    'Credit card or prepaid card', \n    'Money transfer, virtual currency, or money service ',\n    'Payday loan, title loan, or personal loan'\n]\nsel_cat = sel_df.Product.isin(exclude)\nsel_df_cat = sel_df[~sel_cat] # select category not in the exclude categories \nsel_df_cat['Product'].value_counts()","5eb5403a":"from io import StringIO\ncol = ['Product', 'Consumer complaint narrative']\nsel_df_cat = sel_df_cat[col]\n\nsel_df_cat['category_id'] = sel_df_cat['Product'].factorize()[0]\ncategory_id_df = sel_df_cat[['Product', 'category_id']].drop_duplicates().sort_values('category_id')\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['category_id', 'Product']].values)\nsel_df_cat.head()","72a65848":"# NLTK\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\nimport re\nimport string\n\n# Text Normalization\ndef clean_str(text):\n    \n    ## Remove puncuation\n    text = text.translate(string.punctuation)\n    \n    ## Convert words to lower case and split them\n    text = text.lower().split()\n    \n    ## Remove stop words\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops and len(w) >= 3]\n    \n    text = \" \".join(text)\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    \n        \n    text = re.sub(r\"\\$\", \" $ \", text) #isolate $\n    text = re.sub(r\"\\%\", \" % \", text) #isolate %\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    #text = re.sub(r\" e g \", \" eg \", text)\n    #text = re.sub(r\" b g \", \" bg \", text)\n    #text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    #text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    #text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    \n    #removing xxx since it will be treated as importand words by tf-idf vectorization\n    text = re.sub(r\"x{2,}\", \" \", text)\n    \n    # fixing XXX and xxx like as word\n    #text = re.sub(r'\\S*(x{2,}|X{2,})\\S*',\"xxx\",text)\n    # removing non ascii\n    text = re.sub(r'[^\\x00-\\x7F]+', \"\", text) \n    \n    # Stemming is important to reduce the number of features (variation from a single word), why stemming?\n    # Lemmatization takes way longer time to process\n    text = text.split()\n    stemmer = SnowballStemmer('english')\n    stemmed_words = [stemmer.stem(word) for word in text]\n    text = \" \".join(stemmed_words)\n\n    return text","7b511ada":"X = sel_df_cat['Consumer complaint narrative']\ny = sel_df_cat['Product']\nprint(X.shape)\nprint(y.shape)","ecf23749":"from time import time\nt0 = time()\nX = X.map(lambda x: clean_str(x))\nprint (\"\\nCleaning time: \", round(time()-t0, 1), \"s\")","37c01bbd":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\nfeatures = tfidf.fit_transform(X).toarray()\nlabels = sel_df_cat.category_id\nfeatures.shape\nprint(features)\nprint(category_to_id)","c57f1118":"from sklearn.feature_selection import chi2\n\nN = 2\nfor Product, category_id in sorted(category_to_id.items()):\n  features_chi2 = chi2(features, labels == category_id)\n  indices = np.argsort(features_chi2[0])\n  feature_names = np.array(tfidf.get_feature_names())[indices]\n  print(feature_names)\n  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n  print(\"# '{}':\".format(Product))\n  print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n  print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))","e67584e9":"from sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split\n\nmodel = LinearSVC()\nX_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, sel_df_cat.index, test_size=0.33, random_state=0)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)","14c2afe8":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nprint(accuracy_score(y_test, y_pred))\n\nconf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(conf_mat, annot=True, fmt='d',\n            xticklabels=category_id_df.Product.values, yticklabels=category_id_df.Product.values)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","b5af4f4c":"print(classification_report(y_test, y_pred, target_names=sel_df_cat['Product'].unique()))","721b55be":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\n\nX_train, X_test, y_train, y_test = train_test_split(sel_df_cat['Consumer complaint narrative'], sel_df_cat['Product'], random_state = 0)\ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(X_train)\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\nclf = MultinomialNB().fit(X_train_tfidf, y_train)","b1536468":"sel_df_cat.head(10)","825e9a4a":"debt_collection = sel_df[sel_df['Product'] == \"Debt collection\"].head(10)\nprint(debt_collection)","f1f4b12c":"print(clf.predict(count_vect.transform(debt_collection['Consumer complaint narrative'])))","609afffa":"student_loan = sel_df[sel_df['Product'] == \"Student loan\"].head(10)\nprint(student_loan)","1bcde469":"print(clf.predict(count_vect.transform(student_loan['Consumer complaint narrative'])))","8b39837b":"# Deep Learning libs import\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom keras.models import Sequential, Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Conv1D, GlobalMaxPooling1D, MaxPooling1D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\n\n%matplotlib inline","c34995c6":"df = pd.read_csv('..\/input\/Consumer_Complaints.csv', nrows=500000)\nprint(df.shape)\ndf.head()","051fbae0":"sel_df = df.iloc[:, [1,5]]\nsel_df = sel_df.dropna()\nexclude = [\n    'Credit reporting, credit repair services, or other personal consumer reports', \n    'Credit card or prepaid card', \n    'Money transfer, virtual currency, or money service ',\n    'Payday loan, title loan, or personal loan'\n]\nsel_cat = sel_df.Product.isin(exclude)\nsel_df_cat = sel_df[~sel_cat] # select category not in the exclude categories \nsel_df_cat['Product'].value_counts()","fa4ffb99":"X = sel_df_cat['Consumer complaint narrative']\ny = sel_df_cat['Product']\nprint(X.shape)\nprint(y.shape)","80af880a":"from time import time\nt0 = time()\nX = X.map(lambda x: clean_str(x))\nprint (\"\\nCleaning time: \", round(time()-t0, 1), \"s\")","d4a09767":"from tensorflow.contrib import learn\n\n# Preprocessing to encode the text to sequences\nmax_doc_len = max([len(x.split(\" \")) for x in X])\nvocab_processor = learn.preprocessing.VocabularyProcessor(max_doc_len)\nvocab_processor.fit_transform(X)\nvocab_size=len(vocab_processor.vocabulary_)","a7a0de65":"print(max_doc_len)\nprint(vocab_size)","2d3dcd16":"token = Tokenizer(num_words=vocab_size, filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~ ', lower=True, split=' ')\ntoken.fit_on_texts(X)\nX_train_seq = token.texts_to_sequences(X)\nX_train_seq = sequence.pad_sequences(X_train_seq, maxlen=max_doc_len)\n\n\nprint(X_train_seq.shape)","af6e8375":"le = LabelEncoder()\ny_en = le.fit_transform(y)\nprint(np.unique(y_en, return_counts=True))\n\ny_en = to_categorical(y_en, num_classes= 15)\nprint(y_en)\nprint(y_en.shape)","f247f11c":"X_train,X_test,y_train,y_test = train_test_split(X_train_seq, y_en,test_size=0.15)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","03ab0cee":"max_features = vocab_size\nmaxlen = max_doc_len\nembedding_dims = 50\nfilters = 250\nkernel_size = 5\nhidden_dims = 250\n\ndef ProductClassifier():\n    model = Sequential()\n    # Add embedding layer\n    model.add(Embedding(max_features,\n                        embedding_dims,\n                        input_length=maxlen))\n    model.add(Dropout(0.2))\n    \n    # Conv1D for filtering layer\n    model.add(Conv1D(filters,\n                     kernel_size,\n                     padding='valid',\n                     activation='relu',\n                     strides=1))\n    # max pooling:\n    model.add(GlobalMaxPooling1D())\n\n    # add a hidden layer:\n    model.add(Dense(hidden_dims))\n    model.add(Dropout(0.2))\n    model.add(Activation('relu'))\n\n    # Using Softmax for multiclass classifications\n    # model.add(Dense(18))\n    model.add(Dense(15))\n    model.add(Activation('softmax'))\n    return model","50ab2cbd":"model = ProductClassifier()\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","5b203437":"fit = model.fit(X_train, y_train,\n          batch_size=50,\n          epochs=5,\n          #shuffle=True,\n          validation_data=(X_test, y_test))","7f742d8d":"print(fit.history.keys())\n# summarize history for accuracy\nplt.plot(fit.history['acc'])\nplt.plot(fit.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(fit.history['loss'])\nplt.plot(fit.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","b1e21571":"def sentences_to_sequences(X):\n    token = Tokenizer(num_words=vocab_size, filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~ ', lower=True, split=' ')\n    token.fit_on_texts(X)\n    X_seq = token.texts_to_sequences(X)\n    X_seq = sequence.pad_sequences(X_seq, maxlen=max_doc_len)\n    return X_seq","8d5a6cdc":"index = 228\nx_test = np.array([sel_df_cat.iloc[index, 1]])\nx_result = np.array([sel_df_cat.iloc[index, 0]])\nX_test_indices = sentences_to_sequences(x_test)\nle = LabelEncoder()\nle.fit_transform(sel_df_cat['Product'])\nprint('Narrative: ' + x_test[0] + ', Expected Product: ' + x_result[0] + ', Prediction Product: '+  le.inverse_transform([np.argmax(model.predict(X_test_indices))]))","32048c1c":"x_test = np.array(['I have a problem with my credit. This is really sad.'])\nX_test_indices = sentences_to_sequences(x_test)\nle = LabelEncoder()\nle.fit_transform(sel_df_cat['Product'])\nprint('Narrative: ' + x_test[0] + ', Prediction Product: '+  le.inverse_transform([np.argmax(model.predict(X_test_indices))]))","d041ebcc":"### Text Normalization\n- converting all letters to lower or upper case\n- converting numbers into words or removing numbers\n- removing punctuations, accent marks and other diacritics\n- removing white spaces\n- expanding abbreviations\n-removing stop words, sparse terms, and particular words\n- text canonicalization"}}