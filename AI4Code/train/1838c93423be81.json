{"cell_type":{"6e534e88":"code","de8e6de4":"code","ca81e5dd":"code","456e1712":"code","d0b0b95b":"code","c37f9243":"code","9993c536":"code","120b25a9":"code","424cc8a7":"code","ddabf727":"code","a8afe34f":"code","2674cb31":"code","df7fc769":"code","71581e2a":"code","9ebfc6b4":"code","eca6627e":"code","0ad4b5de":"code","3c7f48c7":"code","c0dbc762":"code","122b5747":"code","99ab4f56":"code","6fcb50cf":"code","a930a4f2":"code","535b2612":"code","61778294":"code","389f14c6":"code","eac87fd3":"code","734898ed":"markdown"},"source":{"6e534e88":"\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.layers import Convolution2D, Dense, MaxPooling2D, Dropout, Flatten\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.datasets import mnist\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n# One-hot encoding\nfrom keras.utils import to_categorical\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator\n\n","de8e6de4":"train_csv = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest_csv = pd.read_csv('..\/input\/digit-recognizer\/train.csv')","ca81e5dd":"train_csv.isnull().sum().any()","456e1712":"test_csv.isnull().sum().any()","d0b0b95b":"training_data = np.array(train_csv)\ntesting_data = np.array(test_csv)\ntraining_data.shape, testing_data.shape","c37f9243":"feature = training_data[:, 1:]\nlabel = training_data[:, :1]\nfeature.shape, label.shape\n","9993c536":"# keras dataset\n(keras_train_X, keras_train_y), (keras_test_X, keras_test_y) = mnist.load_data()\nkeras_train_X = keras_train_X.reshape(-1, 28*28)\nkeras_test_X = keras_test_X.reshape(-1, 28*28)\nprint(keras_train_X.shape)\n","120b25a9":"keras_train_X[0]","424cc8a7":"keras_train_X.shape, keras_test_X.shape","ddabf727":"# concat\nall_feature = np.r_[feature, keras_train_X, keras_test_X]\nall_label = np.r_[label.ravel(), keras_train_y, keras_test_y]\nall_feature.shape, all_label.shape\n# numpy. r_ = <numpy.lib.index_tricks.RClass object>  Tramslates slice obj to concatenation\n# alog the first axis. this is a simple way to build up arrays quickly","a8afe34f":"X_train, X_test, y_train, y_test = train_test_split(all_feature, all_label, test_size = 0.1, shuffle=True)\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n","2674cb31":"import seaborn as sns\nsns.set(style='white')\ng = sns.countplot(train_csv['label'])\n","df7fc769":"\n# KNN = KNeighborsClassifier(n_neighbors=10)\n# KNN.fit(X_train, y_train)\n# print(KNN.predict(X_test[:10]).shape,  y_test[:10].flatten())\n# print(X_test[:10].shape)\n# prediction = KNN.predict(X_test)\n# from sklearn.metrics import accuracy_score\n# print(accuracy_score(y_test, prediction))\n\n# # output\n# # (10,) [2 3 4 3 1 6 1 8 9 2]\n# # (10, 784)\n# # 0.9755357142857143\n","71581e2a":"# from sklearn.svm import SVC\n# SVC = SVC(kernel = 'poly').fit(X_train, y_train.ravel())\n# SVC.score(X_test[:1000], y_test[:1000])\n# # 0.982","9ebfc6b4":"# from sklearn.naive_bayes import MultinomialNB\n# NB = MultinomialNB().fit(X_train, y_train)\n# NB.score(X_test[:1000], y_test[:1000])\n# # 0.805","eca6627e":"# Load the data\ntrain_data = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest_data = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\n","0ad4b5de":"train_data.head()","3c7f48c7":"test_data.head()","c0dbc762":"Y_train = train_data[\"label\"]\nX_train = train_data.drop(labels = [\"label\"],axis = 1) \n# Normalize the data\nX_train = X_train \/ 255.0\ntest_data = test_data \/ 255.0\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(test_data.shape)\nX_train = X_train.values.reshape(-1,28,28,1)\ntest_data = test_data.values.reshape(-1,28,28,1)\nX_train.shape , test_data.shape , Y_train.shape\n","122b5747":"Y_train = to_categorical(Y_train, num_classes = 10)\n","99ab4f56":"# Split the train and the validation set for the fitting\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=2)\n","6fcb50cf":"\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\ndatagen.fit(X_train)\n","a930a4f2":"model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))\nmodel.summary()\n","535b2612":"model.compile(optimizer='RMSprop', loss = 'categorical_crossentropy', metrics=['accuracy'])\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.000001)\n","61778294":"history=model.fit(X_train, Y_train,epochs=25,validation_split=0.25,callbacks=[learning_rate_reduction])\n","389f14c6":"\nplt.plot(history.history['accuracy'],'r',label='training acc')\nplt.plot(history.history['val_accuracy'],label='validation acc')\nplt.xlabel('# epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(history.history['loss'],'r',label='training loss')\nplt.plot(history.history['val_loss'],label='validation loss')\nplt.xlabel('# epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()\n","eac87fd3":"# predict results\nresults = model.predict(test_data)\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"digit_recognizeSEQ.csv\",index=False)\n","734898ed":"# NN"}}