{"cell_type":{"44da3915":"code","7bfffda0":"code","137726d7":"code","44a32e69":"code","bda8d550":"code","52ad8192":"code","e551cb9b":"code","553ec2bd":"code","1f7837de":"code","b3461f8d":"code","a15f0d48":"code","d9b2a341":"code","4e259af6":"code","7317fac9":"code","2f2ebd43":"code","4823922a":"code","0e03aa34":"code","6d67d9f1":"code","216e554c":"code","5c20f74c":"code","b6ba8997":"code","e4e3c18f":"code","08d21fcd":"code","7fb972f4":"code","c5897863":"code","9387d873":"code","67116f05":"code","976a79fb":"code","73f236bc":"code","21b80549":"markdown","63d3cb06":"markdown","694dd529":"markdown","fe671c94":"markdown","af325bac":"markdown","d400dac7":"markdown","2380d0fb":"markdown","108ba5c8":"markdown","0fdea93b":"markdown","b8bae2a0":"markdown","c0f21edb":"markdown","584ae473":"markdown","cafe4435":"markdown","f73e7cd0":"markdown","2e03d097":"markdown","b0a5e9e4":"markdown","e5789735":"markdown","f64dd120":"markdown","5ef0cdc5":"markdown","f5c8ab5c":"markdown","bf9d68cb":"markdown","71e9d442":"markdown","6b8ea9fe":"markdown","92809547":"markdown","4b16a544":"markdown","cfc11cf4":"markdown","86080b8e":"markdown","2750d8e2":"markdown","1722a1e0":"markdown","221db53d":"markdown","6a7143c9":"markdown","d67bde8e":"markdown","c6336e8c":"markdown","a771bdcd":"markdown","cfdd50a8":"markdown","bd122347":"markdown"},"source":{"44da3915":"import os\nimport pandas as pd # for reading our csv files\nimport matplotlib.pyplot as plt # for the amazing plots which will increase the notebook beauty\nimport seaborn as sns # a more detailed plotting tool used mostly for making complex plots just by simples lines","7bfffda0":"print(os.listdir(\"..\/input\"))","137726d7":"df_red = pd.read_csv(\"..\/input\/wineQualityReds.csv\")\ndf_white = pd.read_csv(\"..\/input\/wineQualityWhites.csv\")\nprint(df_red.shape,df_white.shape)\ndf = pd.concat([df_red,df_white])\ndf = df.iloc[:,1:]\ndf = df.drop(\"quality\",axis=1)\ndf","44a32e69":"Type = {'Red': 1,'White': 0}\n\ndf[\"Type\"] = [Type[item] for item in df[\"Type\"]]\ndf","bda8d550":"# The info() function is used to print a concise summary of a DataFrame. \n#This method prints information about a DataFrame including the index dtype and column dtypes,\n#non-null values and memory usage. \ndf.info()","52ad8192":"#The describe() method is used for calculating some statistical data like percentile, \n#mean and std of the numerical values of the Series or DataFrame. It analyzes both \n#numeric and object series and also the DataFrame column sets of mixed data types.\ndf.describe()","e551cb9b":"new_df = df.drop(\"Type\",axis=1) # i am dropinf this because dur to value between 0 and 1 for type \n#its throwing an error of one hot encoding, so i am not taking one hot encodinf here because its for learning of you people how skewness look\n# but remmember we will train and perform all analysis with type coloum included\nfig, axes = plt.subplots(ncols=new_df.shape[1],figsize=(40,10))\nfor ax, col in zip(axes, new_df.columns):\n    sns.distplot(new_df[col], ax=ax)\n\nplt.show()","553ec2bd":"# # Some of you may get this error as i got it and posted here if anyone else have please have look\n#RuntimeWarning: invalid value encountered in log1p this occurs due to negative value in \n#total.sulfur.dioxide coloumn\nimport numpy as np\nnew_df['total.sulfur.dioxide'] = new_df[new_df['total.sulfur.dioxide']>=0]\nnew_df['free.sulfur.dioxide'] = new_df[new_df['free.sulfur.dioxide']>=0]\nnew_df['residual.sugar'] = np.log1p(new_df['residual.sugar'])\nnew_df['chlorides'] = np.log1p(new_df['chlorides'])\nnew_df['total.sulfur.dioxide'] = np.log1p(new_df['total.sulfur.dioxide'])\nnew_df['free.sulfur.dioxide'] = np.log1p(new_df['free.sulfur.dioxide'])\nnew_df['sulphates'] = np.log1p(new_df['sulphates'])","1f7837de":"fig, axes = plt.subplots(ncols=new_df.shape[1],figsize=(40,10))\nfor ax, col in zip(axes, new_df.columns):\n    sns.distplot(new_df[col], ax=ax)\nplt.show()","b3461f8d":"_,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(df.corr(),annot=True,ax=ax)","a15f0d48":"x = df.iloc[:,:-1].values\nx","d9b2a341":"y = df.iloc[:,-1].values\ny","4e259af6":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=16)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","7317fac9":"from sklearn.linear_model import LogisticRegression,LinearRegression\nmodel1 = LogisticRegression(random_state=0)\nmodel1.fit(X_train, y_train)","2f2ebd43":"y_pred1 = model1.predict(X_test)\ny_pred1.shape","4823922a":"from sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import classification_report\nprint(r2_score(y_test, y_pred1))\nprint(metrics.accuracy_score(y_pred1,y_test))\nprint(classification_report(y_test, y_pred1))","0e03aa34":"model2 = LinearRegression()\nmodel2.fit(X_train, y_train)","6d67d9f1":"y_pred2 = model1.predict(X_test)\ny_pred2.shape","216e554c":"from sklearn.metrics import r2_score\nprint(r2_score(y_test, y_pred2))\nprint(metrics.accuracy_score(y_pred2,y_test))\nprint(classification_report(y_test, y_pred2))","5c20f74c":"#most important SVR parameter is Kernel type. It can be \n#linear,polynomial or gaussian SVR. We have a non-linear condition \n#so we can select polynomial or gaussian but here we select RBF(agaussian type) kernel.\nfrom sklearn.svm import SVR\nregressor = SVR(kernel = 'rbf')\nregressor.fit(X_train, y_train)","b6ba8997":"y_pred3 = regressor.predict(X_test)\nprint(r2_score(y_test, y_pred3))\n","e4e3c18f":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\nclf = clf.fit(X_train,y_train)\ny_pred4 = clf.predict(X_test)","08d21fcd":"from sklearn import metrics\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred4))\nprint(classification_report(y_test, y_pred4))","7fb972f4":"# from sklearn.tree import export_graphviz\n# from sklearn.externals.six import StringIO  \n# from IPython.display import Image  \n# import pydotplus\n\n# dot_data = StringIO()\n# export_graphviz(clf, out_file=dot_data,  \n#                 filled=True, rounded=True,\n#                 special_characters=True)\n# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n# graph.write_png('diabetes.png')\n# Image(graph.create_png())","c5897863":"from sklearn.ensemble import RandomForestRegressor","9387d873":"import numpy as np\n# n_estimators :-The number of trees in the forest.\nclf1 = RandomForestRegressor(n_estimators=10)\nclf1.fit(X_train,y_train)\npredicted_Y = clf1.predict(X_test)\npredicted = np.round(predicted_Y)\nprint(metrics.accuracy_score(predicted,y_test))\nprint(classification_report(y_test, predicted))","67116f05":"import tensorflow as tf # the libary for building our model you can use pytorch also but that would be much advanced.\nann = tf.keras.models.Sequential()\nann.add(tf.keras.layers.Dense(units=6, activation='relu'))\nann.add(tf.keras.layers.Dense(units=6, activation='relu'))\nann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\nann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","976a79fb":"ann.fit(X_train, y_train, batch_size = 32, epochs = 10)\ny_pred = ann.predict(X_test)","73f236bc":"comparison_df = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})\ncomparison_df","21b80549":"And here it is guys we have reached an accuracy of 97 % with random forests and i am very happy to share this score with you :)\n\n## What a Journey to attain this score, Well done!!!!","63d3cb06":"Making a new coloum here which describes Red :1, White :0\n\n**What is dictionary ?**\n\nDictionaries are Python's implementation of a data structure that is more generally known as an associative array. A dictionary consists of a collection of key-value pairs. Each key-value pair maps the key to its associated value.","694dd529":"# Handling Skewed Data\n\n#### **What is distplot, why its important distplot ?**\n\nSeaborn distplot lets you show a histogram with a line on it. This can be shown in all kinds of variations. We use seaborn in combination with matplotlib, the Python plotting module.\n\nA distplot plots a univariate distribution of observations. The distplot() function combines the matplotlib hist function with the seaborn kdeplot() and rugplot() functions.\n\nThis is bascially used to determine the skewness in your data(coloums), if it exists and you can also remove that skewness by using various methos.\n\n![image.png](attachment:image.png)","fe671c94":"A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.","af325bac":"#### Support Vector regression is a type of Support vector machine that supports linear and non-linear regression.The mission is to fit as many instances as possible between the lines while limiting the margin violations. In SVR, the goal is to make sure that the errors do not exceed the threshold.","d400dac7":"# 5. RandomForestRegressor","2380d0fb":"# 6. Artificial neural networks ","108ba5c8":"### what is precision, Recall, F1-Score ?\n\n**Precision \u2013 What percent of your predictions were correct?**\n\nPrecision is the ability of a classifier not to label an instance positive that is actually negative. For each class it is defined as the ratio of true positives to the sum of true and false positives.\n\n* TP \u2013 True Positives\n* FP \u2013 False Positives\n\nRemmember :-\n1. Precision \u2013 Accuracy of positive predictions.\n2. Precision = TP\/(TP + FP)\n\n**Recall \u2013 What percent of the positive cases did you catch? **\n\nRecall is the ability of a classifier to find all positive instances. For each class it is defined as the ratio of true positives to the sum of true positives and false negatives.\n\n* FN \u2013 False Negatives\n\n* Recall: Fraction of positives that were correctly identified.\n* Recall = TP\/(TP+FN)\n\n**F1 score \u2013 What percent of positive predictions were correct? **\n\nThe F1 score is a weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0. Generally speaking, F1 scores are lower than accuracy measures as they embed precision and recall into their computation. As a rule of thumb, the weighted average of F1 should be used to compare classifier models, not global accuracy.\n\n1. F1 Score = 2*(Recall * Precision) \/ (Recall + Precision)","0fdea93b":"As you can see above the coloums like residual.sugar, chlorides, sulphates are skewed so i am using here log transformation for that and also in our coloum total.sulfur.dioxide and free.sulfur.dioxide the values are -ve which give run time error if you go for log transformation(the error is mentioned below) ","b8bae2a0":"# 4. DecisionTreeClassifier","c0f21edb":"## Please UPVOTE if you like my work and will soon catch you !! :)","584ae473":"Decision Tree is one of the most powerful and popular algorithm. Decision-tree algorithm falls under the category of supervised learning algorithms. It works for both continuous as well as categorical output variables.\nHere is a beautiful way of explaination for this:- https:\/\/towardsdatascience.com\/decision-tree-in-python-b433ae57fb93","cafe4435":"Ok Great! But how to correct it if there is skew data in our dataset. See this small changs might look small but they make they make difference also small : PJ(poor joke)\n\n**Methods to Handle Skewed Data**\n\n**1. Log Transform**\n\nLog transformation is most likely the first thing you should do to remove skewness from the predictor.It can be easily done via Numpy, just by calling the log() function on the desired column.\n\n**2. Square Root Transform**\n\nThe square root sometimes works great and sometimes isn\u2019t the best suitable option. You can apply a square root transformation via Numpy, by calling the sqrt() function.\n\n**3. Box-Cox Transform**\n\nThis is the last transformation method I want to explore today. As I don\u2019t want to drill down into the math behind.You should only know that it is just another way of handling skewed data. To use it, your data must be positive \u2014 so that can be a bummer sometimes.\n\nSkewed data can mess up the power of your predictive model if you don\u2019t address it correctly.\nThis should go without saying, but you should remember what transformation you\u2019ve performed on which attribute, because you\u2019ll have to reverse it once when making predictions, so keep that in mind.\nNevertheless, these three methods should suit you well.","f73e7cd0":"Now here actually i am going to classify the red and white wines and going to predict which will be more healthy for your health.\n\nOur original dataset have first coloumn as Unnamed which i think is of no use so i m drooping that off and also I m not predicting the quality here so no need for that too also and thereby droping that too.\n\nOk,Lets make our hand dirty and go deep into this wonderful journey.\n![image.png](attachment:image.png)","2e03d097":"#### Intro about decision tree parameters \n\n1. criterion: optional (default=\u201dgini\u201d) or Choose attribute selection measure: This parameter allows us to use the different-different attribute selection measure. Supported criteria are \u201cgini\u201d for the Gini index and \u201centropy\u201d for the information gain.\n2. max_depth : int or None, optional (default=None) or Maximum Depth of a Tree: The maximum depth of the tree. If None, then nodes are expanded until all the leaves contain less than min_samples_split samples. The higher value of maximum depth causes overfitting, and a lower value causes underfitting","b0a5e9e4":"![image.png](attachment:image.png)","e5789735":"So as you can see the above coloums have shifted somewhat this is what i want you people to see that how this affect our data while predictions.","f64dd120":"These are just very basic models guys just have a small look on their documentation there is nothing much here to explain about logistic regression if you have interest here is link where you find best of best https:\/\/en.wikipedia.org\/wiki\/Logistic_regression#:~:text=Logistic%20regression%20is%20a%20statistical,a%20form%20of%20binary%20regression).","5ef0cdc5":"# Model Evaluations\n\nOK OK Guys!!, we are very near to our destination and will be soon ending this journey.\n\nI have tried here many models guys and you will look with what we got the best results just right below.\n\n","f5c8ab5c":"Well with both above model we have got a good accuracy of 95.5% appox which is good but never forgot the things which looks good have chance of much improvements and with that we go for next !! ;)","bf9d68cb":"# 2.LinearRegression","71e9d442":"### What is classification report ?\n\nA Classification report is used to measure the quality of predictions from a classification algorithm. How many predictions are True and how many are False. More specifically, True Positives, False Positives, True negatives and False Negatives are used to predict the metrics of a classification report.\n\nThe report shows the main classification metrics precision, recall and f1-score on a per-class basis. The metrics are calculated by using true and false positives, true and false negatives. Positive and negative in this case are generic names for the predicted classes. There are four ways to check if the predictions are right or wrong:\n\n* TN \/ True Negative: when a case was negative and predicted negative\n\n* TP \/ True Positive: when a case was positive and predicted positive\n\n* FN \/ False Negative: when a case was positive but predicted negative\n\n* FP \/ False Positive: when a case was negative but predicted positive","6b8ea9fe":"Ok, folks this ROLAR COASTER ride has came to its end and was really wonderful from my point of view if you have any doubts and wants to clarrify or even wanna correct just jump yourself with me in this big ocean of learning and lets help each other.\n\nThank You for your patience ;)","92809547":"# How does the Decision Tree algorithm work?\n\nThe basic idea behind any decision tree algorithm is as follows:\n\n* Select the best attribute using Attribute Selection Measures(ASM) to split the records.\n\n* Make that attribute a decision node and breaks the dataset into smaller subsets.\n\n* Starts tree building by repeating this process recursively for each child until one of the condition will match:\n\n    1. All the tuples belong to the same attribute value.\n    2. There are no more remaining attributes.\n    3. There are no more instances.","4b16a544":"Hi Folks, Welcome back again. Today we are having a great dataset on which we are going to analyze which wine is more good its either red or white.\n\nI have covered here many things realted to ML and Dl as well where you will see the beauty of working of these model in depth. Hope that you will like my work !! :)","cfc11cf4":"# 3. Support Vector Machine","86080b8e":"## Question: Any one know for random_state in train test split, if yes commet below to tell answer i want you also to take actively participate. :)","2750d8e2":"# HeatMaps\nA heatmap is a two-dimensional graphical representation of data where the individual values that are contained in a matrix are represented as colors. The seaborn python package allows the creation of annotated heatmaps which can be tweaked using Matplotlib tools as per the your requirements.","1722a1e0":"Well here due to some reasons, I dont know why some libaries issue is there i m not able to show the tree but you can look into google and it will defanitely help you. ;(","221db53d":"## 1.LogisticRegression\n","6a7143c9":"**This whole below is the method to visualize the decision tree that how our decision tree has made the decision and then which definately help us to see how our algo worked. In this dont change anything if you try just replace clf with your model name and thats all:)**","d67bde8e":"**What is r2_score** ?\n\nR-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression.\n\nThe definition of R-squared is fairly straight-forward; it is the percentage of the response variable variation that is explained by a linear model. Or:\n\nR-squared = Explained variation \/ Total variation\n\nR-squared is always between 0 and 100%:\n\n0% indicates that the model explains none of the variability of the response data around its mean.\n100% indicates that the model explains all the variability of the response data around its mean.\nIn general, the higher the R-squared, the better the model fits your data.","c6336e8c":"So our model is a very simple, actually we no need to apply these models on these data but just for your learning i have put it here.So here a brief explaination about layers:\n\n1. **A input layer** which conatins your input features which will be fed to the first hidden layer i.e dense layer\n\n2. **A dense layer** is just a regular layer of neurons in a neural network. Each neuron recieves input from all the neurons in the previous layer, thus densely connected. Here units means the no of neurons you want, well actually that all uopn your choice you can play with that number also.\n\n## Important Note:- \nThe last layer you add must have the units equal to no of oulets u need like in this example we have output as \"1\" or \"0\" which can be represented by 1 neuron itself so only last layer have unit =1 and we used sigmoid function for the activation.\n\nIf you dont know about the functions please have a look onto my other kernals, you will find them in detail. ;)","a771bdcd":"So at last we come to know that the RandomForestRegressor was the best model for this kind of predictions and well you may try out this guys and definately i think if any of you got more, it would make me happy ;)\n\nHappy Learning","cfdd50a8":"* Ok folks, we are driving now into deep and dense models ","bd122347":"#### Batch Size ?\nThe batch size below denotes that you are training your model in batches of 32 which helps us to reduce the time for the training and helps us to perform good over 10 epochs.\n\n#### 10 Epochs ?\nOne Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE and thus 10 means we are passing it for 10 times which helps us in the covergence of our loss."}}