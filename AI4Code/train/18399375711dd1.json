{"cell_type":{"d964e985":"code","66080b9e":"code","f8641b97":"code","eee1b083":"code","fed8c4c8":"code","d943263a":"code","77183f95":"code","f97e8ef4":"code","666e898c":"code","e5ba8c6a":"code","8799ff1f":"code","f31cc558":"code","fc443bb1":"code","d9a8ca39":"code","8a7c9378":"code","3f77491f":"code","39ead51e":"code","2fab1444":"code","b501389a":"code","f082f223":"code","608c12c0":"code","b1d5f223":"code","b71cefd3":"code","54f67471":"code","67c71aa2":"code","1b5dc400":"code","1558eea4":"code","300824f3":"code","6d112dc5":"code","7c6d1421":"code","a68ccde3":"code","bfbbb9ad":"code","0fd4655a":"code","3db5513a":"code","02c64d6d":"markdown","82f62e07":"markdown","b4557094":"markdown","52ffe17f":"markdown","20bc689c":"markdown","c16a12d6":"markdown","16429408":"markdown","204d9c73":"markdown","ccbf9c45":"markdown","ed93e42c":"markdown","1a35c90d":"markdown","761fffa3":"markdown","bc79bb9c":"markdown","5d762a9b":"markdown","fc05c9c1":"markdown","30017b79":"markdown","8bc6a179":"markdown","b119b80a":"markdown","0e36b209":"markdown","805993c1":"markdown","9f98bfc0":"markdown","b61d69fb":"markdown","fda90b94":"markdown","6d61ce8a":"markdown","5e90d2e8":"markdown","f66d273e":"markdown","a931f423":"markdown","90d10425":"markdown","ec249799":"markdown","1035535c":"markdown","08708eef":"markdown","100de1e5":"markdown","8b773f14":"markdown","856993c8":"markdown","34ac74da":"markdown","ec59fbf6":"markdown"},"source":{"d964e985":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.neighbors import LocalOutlierFactor","66080b9e":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.width', 500)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f8641b97":"df = pd.read_csv(\"\/kaggle\/input\/bank-customers\/Churn Modeling.csv\")\ndf.head()","eee1b083":"def check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n\n    print(\"##################### Describe #####################\")\n    print(dataframe.describe().T)\n\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\ncheck_df(df)","fed8c4c8":"del df[\"RowNumber\"]","d943263a":"def grab_col_names(dataframe, cat_th=10, car_th=20): #cat_th ve car_th de\u011ferleri projeye g\u00f6re de\u011fi\u015febilir.\n    \n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\n","77183f95":"num_cols","f97e8ef4":"cat_but_car = cat_but_car + [\"CustomerId\"]\nnum_cols.remove(\"CustomerId\")","666e898c":"def cat_summary(dataframe, col_name, plot=False):\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() \/ len(dataframe)}))\n    print(\"##########################################\")\n    if plot:\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.show()\n\n\nfor col in cat_cols:\n    cat_summary(df, col, plot=True)","e5ba8c6a":"for col in num_cols:\n    print(df.groupby(\"Exited\").agg({col: \"mean\"}))","8799ff1f":"def outlier_thresholds(dataframe, col_name, q1=0.5, q3=0.95):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\n\nfor col in num_cols:\n    print(col, \"-\", check_outlier(df, col))","f31cc558":"for col in num_cols:\n    sns.boxplot(x=df[col])\n    plt.show()","fc443bb1":"low_th, up_th = outlier_thresholds(df,\"CreditScore\")\ndf.loc[(df[\"CreditScore\"]<low_th) | (df[\"CreditScore\"]>up_th),\"CreditScore\"]","d9a8ca39":"def replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\nreplace_with_thresholds(df,\"CreditScore\")","8a7c9378":"clf = LocalOutlierFactor(n_neighbors=5)\nclf.fit_predict(df[num_cols])\ndf_scores = clf.negative_outlier_factor_\nnp.sort(df_scores)[0:5]\nscores = pd.DataFrame(np.sort(df_scores))\nscores.plot(stacked=True, xlim=[0, 50], style='.-')\nplt.show()","3f77491f":"th = np.sort(df_scores)[4]\ndf[df_scores < th]\ndf.drop(df[df_scores < th].index, inplace=True)","39ead51e":"def missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n\n    if na_name:\n        return na_columns\n\n\nmissing_values_table(df)","2fab1444":"corr = df.corr()\n\nsns.heatmap(corr,\n        xticklabels=corr.columns,\n        yticklabels=corr.columns)\nplt.show()","b501389a":"df[\"Age\"].describe()","f082f223":"df.loc[(df['Age'] <= 22 ), 'NEW_AGE_CAT'] = 'Z'\ndf.loc[(df['Age'] >= 23) & (df['Age'] <= 38), 'NEW_AGE_CAT'] = 'Millennials'\ndf.loc[(df['Age'] >= 39) & (df['Age'] <= 54), 'NEW_AGE_CAT'] = 'X'\ndf.loc[(df['Age'] >= 55), 'NEW_AGE_CAT'] = 'Boomers'\ndf[\"NEW_AGE_CAT\"].value_counts()","608c12c0":"df[\"NEW_SALARY_SCALA\"] = pd.qcut(df[\"EstimatedSalary\"], 5, labels=[1,2,3,4,5])","b1d5f223":"df.loc[(df[\"IsActiveMember\"]==0), \"NEW_CUSTOMER_SEGMENT\"] = \"InActive\"\ndf.loc[(df[\"HasCrCard\"]==0) & (df[\"NumOfProducts\"]<=1) & (df[\"IsActiveMember\"]==1), \"NEW_CUSTOMER_SEGMENT\"] = \"Active_limited_usage1\"\ndf.loc[(df[\"HasCrCard\"]==0) & (df[\"NumOfProducts\"]>1) & (df[\"IsActiveMember\"]==1), \"NEW_CUSTOMER_SEGMENT\"] = \"Active_limited_usage2\"\ndf.loc[(df[\"HasCrCard\"]==1) & (df[\"NumOfProducts\"]<=1) & (df[\"IsActiveMember\"]==1), \"NEW_CUSTOMER_SEGMENT\"] = \"Active_limited_usage3\"\ndf.loc[(df[\"HasCrCard\"]==1) & (df[\"NumOfProducts\"] == 2) & (df[\"IsActiveMember\"]==1), \"NEW_CUSTOMER_SEGMENT\"] = \"Active_constantly_usage\"\ndf.loc[(df[\"HasCrCard\"]==1) & (df[\"NumOfProducts\"]>=3) & (df[\"IsActiveMember\"]==1), \"NEW_CUSTOMER_SEGMENT\"] = \"Active_loyal\"","b71cefd3":"df[\"NEW_CREDITSCORE_AGE\"] = df[\"CreditScore\"] \/ df[\"Age\"]\ndf[\"NEW_BALANCE_TENURE\"] = df.apply(lambda x: x[\"Balance\"] \/ x[\"Tenure\"] if x[\"Tenure\"] != 0 else 0, axis = 1)\ndf[\"NEW_ESALARY_BALANCE\"] = df.apply(lambda x: x[\"EstimatedSalary\"] \/ x[\"Balance\"] if x[\"Balance\"] != 0 else 0, axis = 1)","54f67471":"cat_cols, num_cols, cat_but_car = grab_col_names(df)\ncat_but_car = cat_but_car + [\"CustomerId\"]\nnum_cols.remove(\"CustomerId\")","67c71aa2":"for col in cat_cols:\n      df = pd.get_dummies(df, columns=[col], drop_first=True)\n\ndf.head()","1b5dc400":"cat_cols, num_cols, cat_but_car = grab_col_names(df)\ncat_but_car = cat_but_car + [\"CustomerId\"]\nnum_cols.remove(\"CustomerId\")","1558eea4":"scaler = RobustScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])\ndf.head()","300824f3":"y = df[\"Exited_1\"] # Dependent Variable\nX = df.drop([\"Exited_1\",\"CustomerId\",\"Surname\"], axis=1) # Independent Variable\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=17)","6d112dc5":"# Random Forest:\nfrom sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier(random_state=20).fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\naccuracy_score(y_pred, y_test)","7c6d1421":"# Artificial Neural Network\nfrom sklearn.neural_network import MLPClassifier\nmlpc_model = MLPClassifier(random_state=20).fit(X_train, y_train)\ny_pred = mlpc_model.predict(X_test)\naccuracy_score(y_pred, y_test)","a68ccde3":"# Gradient Boosting Machines\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbm_model = GradientBoostingClassifier(random_state=20).fit(X_train, y_train)\ny_pred = gbm_model.predict(X_test)\naccuracy_score(y_pred, y_test)","bfbbb9ad":"# XGBoost\nfrom xgboost import XGBClassifier\nxgb_model = XGBClassifier(random_state=20, eval_metric='mlogloss').fit(X_train, y_train)\ny_pred = xgb_model.predict(X_test)\naccuracy_score(y_pred, y_test)","0fd4655a":"# LightGBM\nfrom lightgbm import LGBMClassifier\nlgbm_model = LGBMClassifier(random_state=20).fit(X_train, y_train)\ny_pred = lgbm_model.predict(X_test)\naccuracy_score(y_pred, y_test) ","3db5513a":"def plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                      ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')\n\nplot_importance(lgbm_model, X_train)","02c64d6d":"* CreditScore, Balance and EstimatedSalary are most important variables in this model. \n* NEW_CREDITSCORE_AGE and NEW_BALANCE_TENURE what I created are more important too. \n* NEW_AGE_CAT and NEW_SALARY_SCALA_5 variables don't appear very important, they can be deleted. ","82f62e07":"Thank you for your reading. \nFor more: [Github](https:\/\/github.com\/omeryasirkucuk) & [Medium](http:\/\/omeryasirkucuk.medium.com)","b4557094":"# **Model & Accuray Score**","52ffe17f":"# **Analysis Numeric and Categoric Variables**","20bc689c":"**Importing Data**","c16a12d6":"There is an elbow in 4. index. We should delete these values.","16429408":"Now let's look outliers with Local Outlier Factor. ","204d9c73":"# **Defining Variables Types**","ccbf9c45":"We looked the outliers values in the dataset with BoxPlot method. \nThere are outliers values in CreditScore variable.\nLet's check which values are outlier. ","ed93e42c":"We can add CustomerId variables from numeric cols to categoric but cardinal cols.","1a35c90d":"# **Categoric Columns Encoding Process w\/One Hot Encoding**","761fffa3":"![](https:\/\/sc04.alicdn.com\/kf\/Hdd80e4daec0d4942b5314ad74342be0aa.png)","bc79bb9c":"**Pressure outliers value with thresholds**","5d762a9b":"We can predict customers will be churned or not with LightGBM in %85 accuracy score.  \nLet's see which variable is more important for LightGBM model.","fc05c9c1":"**Defining Variables Types with NEW Variables**","30017b79":"**Import Libraries and Setting Console Settings**","8bc6a179":"# **Exploratory Data Analysis**","b119b80a":"# **Summary**","0e36b209":"* There are 10000 observation unit and 14 different variables. \n* All variables' types are true.\n* There are no NaN values. \n* There are zero values in Tenure variables, so these customers who have zero tenure values have attended newly.\n* Some categorical variables are numeric type. (Ex. IsActiveMember)\n* We can delete RowNumber variables. ","805993c1":"# **Correlation Analysis**","9f98bfc0":"***First Glance***","b61d69fb":"**Analysis of numerical variables according to customers' churn status**","fda90b94":"# **Variables**\n\n* RowNumber: \n* CustomerId: \n* Surname: Surname of relevant customer\n* CreditScore: It changes 350-850 in the dataset, express credit eligibility\n* Geography: There are 3 country in the dataset, show county where Customer live in.\n* Gender: Customer Gender\n* Age: Customer Age\n* Tenure: How many years customer work with this bank\n* Balance: Customer's total money in the account\n* NumOfProducts: Product number customer use \n* HasCrCard: Whether has credit card or not (Customer has credit card is 1)\n* IsActiveMember: Customer is active or not \n* EstimatedSalary: Customers salary (yearly)\n* Exited: Churn or not (Churn situation is 1)","6d61ce8a":"* When the correlation of the numerical variables in the dataset with each other is examined, \n* it is observed that the correlations are mostly low. \n* There is a correlation of 0.285 between Age and Exited, and 0.119 between Balance and Exited.","5e90d2e8":"# **Purpose**\n\nThe purpose of this project is to establish machine learning models on whether customers are churn with the dataset we have, \nto display accuracy scores and to complete some preliminary preparations to establish this model.","f66d273e":"# **Outliers Analysis**","a931f423":"* Initially we had 10000 observations of 14 variables.\n* We got to know the variables by doing Exploratory Data Analysis for these variables, and then we examined our outliers and missing values.\n* We tried to create new categorical and numeric variables in order to strengthen our algorithm with the existing variables.\n* We encoded categorical variables with One Hot Encoding and got the drop_first True despite the dummy variable trap.\n* We scaled numerical variables using Robust Scaling.\n* By separating the data we have with 20% test set, we created Random Forest, Artificial Neural Network, GBM, XGBoost, LightGBM models and observed accuracy scores.\n* The highest accuracy score was achieved in the LightGBM model with %85 and the importance of the variables for this model was visualized.","90d10425":"* There are so low number of customers who use 4 products.\n* Most customers are from France and distribution rates of customers who live in Germany and Spain is so close each other.\n* %70 of customers have least one credit card. \n* %20.37 of all customers in the dataset was churned.","ec249799":"Average credit score of customers who churned is low than customers are alive.","1035535c":"# **Missing Value Analysis**","08708eef":"**Defining Variables Types after encoding**","100de1e5":"Special Thanks,\n\n*  [Veri Bilimi Okulu](https:\/\/miuul.com\/)\n*  [Vahit Keskin](https:\/\/www.linkedin.com\/in\/vahitkeskin\/)\n*  [Mehmet Akt\u00fcrk](https:\/\/www.kaggle.com\/mathchi)\n*  [Ozlem Ilgun Cagirici](https:\/\/www.kaggle.com\/ozlemilgun) - [The best studies on this subject can be found in Ozlem's codes](https:\/\/www.kaggle.com\/ozlemilgun\/diabetics-prediction-with-automated-ml)\n*  [Arif Eker](https:\/\/www.kaggle.com\/arifeker)\n*  [Ozan Guner](https:\/\/www.kaggle.com\/oktayozangner)","8b773f14":"# **Defining New Variables**","856993c8":"# **Robust Scaling**","34ac74da":"# **EDA, Feature Engineering and Calculation Accuracy Score with ML in Bank Churn Dataset**","ec59fbf6":"# **Dataset Story**\n\n* This dataset was taken from Kaggle. (https:\/\/www.kaggle.com\/kmalit\/bank-customer-churn-prediction\/data)\n* The dataset contains bank's customers information and churn status."}}