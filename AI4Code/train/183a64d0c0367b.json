{"cell_type":{"01fb068d":"code","46df7f45":"code","2c6d7b5c":"code","20e1434a":"code","4a3db9d5":"code","24674349":"code","8fe23c24":"code","9a3c48c7":"code","a7086280":"code","261e737a":"code","f9e211f9":"code","8950920d":"code","91f0998f":"code","c8cb53e3":"code","c2336fd7":"code","3a302e23":"code","f7fbc791":"code","03e12a10":"code","ee82abca":"code","f2ddf20f":"code","85716545":"code","5a0e649a":"code","08524c9e":"code","a32ea188":"code","39bce29e":"code","b903f0f6":"code","00b4f56b":"code","a3e387b4":"code","3e8099de":"code","8976db87":"code","23808426":"code","b0dcb522":"code","6e67e238":"code","d2236911":"code","886f979d":"code","e1c90323":"code","6d00a057":"code","9cb30d8a":"code","e12004da":"code","cabf9717":"code","203f250f":"code","6ab9907a":"code","10e92a43":"code","5424ff9b":"code","d624ec31":"code","6301836b":"code","f2b4a32a":"code","1ef10df6":"code","7f6f60f3":"code","0300946f":"code","d1734aae":"code","39cf3630":"code","71fe008e":"code","6bae8d47":"code","8fcfd0bf":"code","e7f45ea5":"code","762dd127":"code","d1843921":"code","19b72910":"code","a05d3f2f":"code","be78a182":"code","6d70d2cb":"code","67d2d350":"code","c54294c8":"code","fb015979":"code","c2598838":"code","e0f50966":"code","4dba928d":"code","df8ae449":"code","4fc5aff7":"code","3535fbb8":"code","99787387":"code","379b6dc1":"code","e90830c6":"markdown","d0cfc327":"markdown","6fa47a26":"markdown","3d48daec":"markdown","b455b2a8":"markdown","f8bc94df":"markdown","8585f25b":"markdown","9c29ca68":"markdown","2d455eb4":"markdown","828142d2":"markdown","7aa5cb58":"markdown","d1fa29de":"markdown","8b2fee51":"markdown","a5c0d4ba":"markdown","e56fbfc6":"markdown","bba4b0a5":"markdown","ad8127ff":"markdown","665db1bf":"markdown","071d6ef4":"markdown"},"source":{"01fb068d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","46df7f45":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport scipy.stats as ss\nfrom statsmodels.formula.api import ols\nfrom scipy.stats import zscore\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns\n%timeit \n%matplotlib inline\nsns.set_style('darkgrid')","2c6d7b5c":"dftrain=pd.read_csv(\"..\/input\/train.csv\")\ndftest=pd.read_csv(\"..\/input\/test.csv\")\ntest=dftest.copy()","20e1434a":"dftrain.info()","4a3db9d5":"dftrain.head()","24674349":"dftrain.info()","8fe23c24":"# Missing data\nimport missingno as msno\nmsno.bar(dftrain.sample(890))","9a3c48c7":"#Missing data in form of white lines \nmsno.matrix(dftrain)","a7086280":"#The feature correlation in form of visualisation\nsns.heatmap(dftrain.corr())","261e737a":"df=dftrain.copy()","f9e211f9":"#Survival rates of MALES and FEMALES\nmale1=df.loc[(df.Survived==1) &(df.Sex=='male'),:].count()\nprint(\"MALE: \\n\",male1) \nfemale1=df.loc[(df.Survived==1) & (df.Sex=='female'),:].count()\nprint(\"\\nFEMALE:\\n\",female1)","8950920d":"# Stats and Visualisation of Survival Rate\nsns.factorplot(x=\"Sex\",col=\"Survived\", data=df , kind=\"count\",size=6, aspect=.7,palette=['crimson','lightblue'])\nmalecount=pd.value_counts((df.Sex == 'male') & (df.Survived==1))\nfemalecount=pd.value_counts((df.Sex=='female') & (df.Survived==1))\ntotalmale,totalfemale=pd.value_counts(df.Sex)\nprint(\"male survived {} , female survived {}\".format(malecount\/totalmale,femalecount\/totalfemale))","91f0998f":"#Clear representation of Ages of passengers and to which Class they belonged\nplt.figure(figsize=(12,12))\nsns.swarmplot(x=\"Sex\",y=\"Age\",hue='Pclass',data=df,size=10 ,palette=['orange','brown','purple'])","c8cb53e3":"plt.figure(figsize=(12,12))\nsns.swarmplot(x=\"Sex\",y=\"Age\",hue='Survived',data=df,size=10,palette='viridis')","c2336fd7":"sns.factorplot(x=\"Sex\", hue = \"Pclass\" , col=\"Survived\", data=df , kind=\"count\",size=7, aspect=.7,palette=['crimson','orange','green'])","3a302e23":"pd.crosstab([df.Sex,df.Survived],df.Pclass, margins=True).style.background_gradient(cmap='autumn_r')","f7fbc791":"sns.factorplot(x=\"Survived\",col=\"Embarked\",data=df ,hue=\"Pclass\", kind=\"count\",size=8, aspect=.7,palette=['crimson','darkblue','purple'])","03e12a10":"sns.factorplot(x=\"Sex\", y=\"Survived\",col=\"Embarked\",data=df ,hue=\"Pclass\",kind=\"bar\",size=7, aspect=.7)","ee82abca":"# Correlation Heatmap \ncontext1 = {\"female\":0 , \"male\":1}\ncontext2 = {\"S\":0 , \"C\":1 , \"Q\":2}\ndf['Sex_bool']=df.Sex.map(context1)\ndf[\"Embarked_bool\"] = df.Embarked.map(context2)\nplt.figure(figsize=(20,20))\ncorrelation_map = df[['PassengerId', 'Survived', 'Pclass', 'Sex_bool', 'Age', 'SibSp',\n       'Parch', 'Fare' , 'Embarked_bool']].corr()\nsns.heatmap(correlation_map,vmax=.7, square=True,annot=True,fmt=\".2f\",cmap='YlGnBu')","f2ddf20f":"df.groupby(\"Pclass\").Age.mean()","85716545":"df.isnull().sum()","5a0e649a":"for x in [dftrain, dftest,df]:\n    x['Age_bin']=np.nan\n    for i in range(8,0,-1):\n        x.loc[ x['Age'] <= i*10, 'Age_bin'] = i\ndf[['Age','Age_bin']].head(20)","08524c9e":"plt.figure(figsize=(20,20))\nsns.set(font_scale=1)\nsns.factorplot('Age_bin','Survived', col='Pclass' , row = 'Sex',kind=\"bar\", data=df)","a32ea188":"df.describe()","39bce29e":"for x in [dftrain, dftest , df]:\n    x['Fare_bin']=np.nan\n    for i in range(12,0,-1):\n        x.loc[ df['Fare'] <= i*50, 'Fare_bin'] = i\nfig, axes = plt.subplots(2,1)\nfig.set_size_inches(20, 18)\nsns.kdeplot(df.Age_bin , shade=True, color=\"green\" , ax= axes[0])\nsns.kdeplot(df.Fare , shade=True, color=\"green\" , ax= axes[1])","b903f0f6":"df.isnull().sum()","00b4f56b":"dftrain.info()","a3e387b4":"dftest.info()","3e8099de":"dftrain.loc[[61,829],\"Embarked\"] = 'C'","8976db87":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\naxis1.set_title('Original Age values - Titanic')\naxis2.set_title('New Age values - Titanic')\n\n# plot original Age values\n# NOTE: drop all null values, and convert to int\ndftrain['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n\n# get average, std, and number of NaN values\naverage_age = dftrain[\"Age\"].mean()\nstd_age = dftrain[\"Age\"].std()\ncount_nan_age = dftrain[\"Age\"].isnull().sum()\n\n# generate random numbers between (mean - std) & (mean + std)\nrand_age = np.random.randint(average_age - std_age, average_age + std_age, size = count_nan_age)\n\n# fill NaN values in Age column with random values generated\nage_slice = dftrain[\"Age\"].copy()\nage_slice[np.isnan(age_slice)] = rand_age\n\n# plot imputed Age values\nage_slice.astype(int).hist(bins=70, ax=axis2)","23808426":"dftrain[\"Age\"] = age_slice\ndftrain=dftrain.drop('Age_bin',axis=1)\ndftrain.info()","b0dcb522":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\naxis1.set_title('Original Age values - Titanic')\naxis2.set_title('New Age values - Titanic')\n\n# plot original Age values\n# NOTE: drop all null values, and convert to int\ndftest['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n\n# get average, std, and number of NaN values\naverage_age = dftest[\"Age\"].mean()\nstd_age = dftest[\"Age\"].std()\ncount_nan_age = dftest[\"Age\"].isnull().sum()\n\n# generate random numbers between (mean - std) & (mean + std)\nrand_age = np.random.randint(average_age - std_age, average_age + std_age, size = count_nan_age)\n\n# fill NaN values in Age column with random values generated\nage_slice = dftest[\"Age\"].copy()\nage_slice[np.isnan(age_slice)] = rand_age\n\n# plot imputed Age values\nage_slice.astype(int).hist(bins=70, ax=axis2)","6e67e238":"dftest[\"Age\"] = age_slice\ndftest=dftest.drop('Age_bin',axis=1)","d2236911":"family_df = dftrain.loc[:,[\"Parch\", \"SibSp\", \"Survived\"]]\n\n# Create a family size variable including the passenger themselves\nfamily_df[\"Fsize\"] = family_df.SibSp + family_df.Parch + 1\n\nfamily_df.head()\n","886f979d":"plt.figure(figsize=(15,5))\n\n# visualize the relationship between family size & survival\nsns.countplot(x='Fsize', hue=\"Survived\", data=family_df)","e1c90323":"dftrain['Fsize']=family_df['Fsize']","6d00a057":"family_df_t= dftest.loc[:,[\"Parch\", \"SibSp\", \"Survived\"]]\n\n# Create a family size variable including the passenger themselves\nfamily_df_t[\"Fsize\"] = family_df_t.SibSp + family_df_t.Parch + 1\ndftest['Fsize']=family_df_t['Fsize']\nfamily_df_t.head()\ndftest['Fsize']=family_df_t['Fsize']","9cb30d8a":"dftest.loc[[152],\"Fare\"] = 10","e12004da":"family_df_tr= dftrain.loc[:,[\"Parch\", \"SibSp\", \"Survived\"]]\n\n# Create a family size variable including the passenger themselves\nfamily_df_tr[\"Fsize\"] = family_df_tr.SibSp + family_df_tr.Parch + 1\n\nfamily_df_tr.head()\ndftrain['Fsize']=family_df_tr['Fsize']","cabf9717":"import scipy.stats as stats\nfrom scipy.stats import chi2_contingency\n\nclass ChiSquare:\n    def __init__(self, dataframe):\n        self.df = dataframe\n        self.p = None #P-Value\n        self.chi2 = None #Chi Test Statistic\n        self.dof = None\n        \n        self.dfObserved = None\n        self.dfExpected = None\n        \n    def _print_chisquare_result(self, colX, alpha):\n        result = \"\"\n        if self.p<alpha:\n            result=\"{0} is IMPORTANT for Prediction\".format(colX)\n        else:\n            result=\"{0} is NOT an important predictor. (Discard {0} from model)\".format(colX)\n\n        print(result)\n        \n    def TestIndependence(self,colX,colY, alpha=0.05):\n        X = self.df[colX].astype(str)\n        Y = self.df[colY].astype(str)\n        \n        self.dfObserved = pd.crosstab(Y,X) \n        chi2, p, dof, expected = stats.chi2_contingency(self.dfObserved.values)\n        self.p = p\n        self.chi2 = chi2\n        self.dof = dof \n        \n        self.dfExpected = pd.DataFrame(expected, columns=self.dfObserved.columns, index = self.dfObserved.index)\n        \n        self._print_chisquare_result(colX,alpha)\n\n#Initialize ChiSquare Class\ncT = ChiSquare(dftrain)\n\n#Feature Selection\ntestColumns = ['Embarked','Cabin','Pclass','Age','Name','Fare','Fare_bin','Fsize']\nfor var in testColumns:\n    cT.TestIndependence(colX=var,colY=\"Survived\" )","203f250f":"# Make a copy of the titanic data frame\ndftrain['Title'] = dftrain['Name']\n# Grab title from passenger names\ndftrain[\"Title\"].replace(to_replace='(.*, )|(\\\\..*)', value='', inplace=True, regex=True)\n\nrare_titles = ['Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer']\ndftrain['Title'].replace(rare_titles, \"Rare title\", inplace=True)\n# Also reassign mlle, ms, and mme accordingly\ndftrain['Title'].replace([\"Mlle\",\"Ms\", \"Mme\"], [\"Miss\", \"Miss\", \"Mrs\"], inplace=True)","6ab9907a":"cT = ChiSquare(dftrain)\n\n#Feature Selection\ntestColumns = ['Embarked','Cabin','Pclass','Age','Name','Fare','Fare_bin','Fsize','Title','SibSp','Parch']\nfor var in testColumns:\n    cT.TestIndependence(colX=var,colY=\"Survived\" )  ","10e92a43":"dftest=dftest.drop(['Ticket','PassengerId'],axis=1)","5424ff9b":"dftest['Title'] = dftest['Name']\n# Grab title from passenger names\n\ndftest[\"Title\"].replace(to_replace='(.*, )|(\\\\..*)', value='', inplace=True, regex=True)","d624ec31":"dftrain=dftrain.drop('Name',axis=1)","6301836b":"dftrain.head()","f2b4a32a":"context1 = {\"female\":0 , \"male\":1}\ncontext2 = {\"S\":0 , \"C\":1 , \"Q\":2}\ncontext3= {\"Mr\":0 , \"Mrs\":1 , \"Miss\":2,'Master':3}\n\ndftrain['Sex_bool']=dftrain.Sex.map(context1)\ndftrain[\"Embarked_bool\"] = dftrain.Embarked.map(context2)\ndftrain['Title']=dftrain.Title.map(context3)","1ef10df6":"dftrain=dftrain.drop(['PassengerId','Cabin','Ticket'],axis=1)\ndftrain=dftrain.drop(['Embarked','Sex'],axis=1)\ndftrain.head()","7f6f60f3":"dftest.head()","0300946f":"context1 = {\"female\":0 , \"male\":1}\ncontext2 = {\"S\":0 , \"C\":1 , \"Q\":2}\ncontext3= {\"Mr\":0 , \"Mrs\":1 , \"Miss\":2,'Master':3}\n\ndftest['Sex_bool']=dftest.Sex.map(context1)\ndftest[\"Embarked_bool\"] = dftest.Embarked.map(context2)\ndftest['Title']=dftest.Title.map(context3)","d1734aae":"dftest=dftest.drop(['Name','Sex','Embarked'],axis=1)","39cf3630":"for x in [dftrain, dftest,df]:\n    x['Age_bin']=np.nan\n    for i in range(8,0,-1):\n        x.loc[ x['Age'] <= i*10, 'Age_bin'] = i","71fe008e":"for x in [dftrain, dftest,df]:\n    x['Fare_bin']=np.nan\n    for i in range(12,0,-1):\n        x.loc[ x['Fare'] <= i*10, 'Fare_bin'] = i","6bae8d47":"dftrain=dftrain.drop('Age',axis=1)\ndftest=dftest.drop('Age',axis=1)","8fcfd0bf":"dftrain=dftrain.convert_objects(convert_numeric=True)","e7f45ea5":"def change_type(df):\n    float_list=list(df.select_dtypes(include=[\"float\"]).columns)\n    print(float_list)\n    for col in float_list:\n        df[col]=df[col].fillna(0).astype(np.int64)\n        \n    return df    \nchange_type(dftrain)   \ndftrain.dtypes","762dd127":"dftrain.head()","d1843921":"x=dftrain.iloc[:,1:].values\ny=dftrain.iloc[:,0].values\nprint(dftrain.columns)\nprint(dftest.columns)\n\nX_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=101)","19b72910":"dftest=dftest.convert_objects(convert_numeric=True)\nchange_type(dftest)    \ndftest.dtypes","a05d3f2f":"MLA = []\nZ = [LinearSVC() , DecisionTreeClassifier() , LogisticRegression() , KNeighborsClassifier() , GaussianNB() ,\n    RandomForestClassifier() , GradientBoostingClassifier()]\nX = [\"LinearSVC\" , \"DecisionTreeClassifier\" , \"LogisticRegression\" , \"KNeighborsClassifier\" , \"GaussianNB\" ,\n    \"RandomForestClassifier\" , \"GradientBoostingClassifier\"]\n\nfor i in range(0,len(Z)):\n    model = Z[i]\n    model.fit( X_train , y_train )\n    pred = model.predict(X_test)\n    MLA.append(accuracy_score(pred , y_test))\nMLA    ","be78a182":"d = { \"Accuracy\" : MLA , \"Algorithm\" : X }\ndfm = pd.DataFrame(d)\ndfm","6d70d2cb":"sns.kdeplot(MLA , shade=True, color=\"green\")","67d2d350":"#Logistic Regression \nparams={'C':[1,100,0.01,0.1,1000],'penalty':['l2','l1']}\nlogreg=LogisticRegression()\ngscv=GridSearchCV(logreg,param_grid=params,cv=10)\n%timeit gscv.fit(x,y)","c54294c8":"gscv.predict(X_test)","fb015979":"print(gscv.best_params_)\nlogregscore=gscv.best_score_\nprint(logregscore)","c2598838":"gscv.score(X_test,y_test)","e0f50966":"#KNN\nparam={'n_neighbors':[3,4,5,6,8,9,10],'metric':['euclidean','manhattan','chebyshev','minkowski'] }       \nknn = KNeighborsClassifier()\ngsknn=GridSearchCV(knn,param_grid=param,cv=10)\ngsknn.fit(x,y) \n\nprint(gsknn.best_params_)\ngsknn.best_score_","4dba928d":"Survived=gsknn.predict(X_test)\nprint(Survived)","df8ae449":"gsknn.score(X_test,y_test)","4fc5aff7":"#Random Forest\nrfcv=RandomForestClassifier(n_estimators=500,max_depth=6)\nrfcv.fit(X_train,y_train)\nrfcv.predict(X_test)\nrfcv.score(X_test,y_test)","3535fbb8":"#Gradient Boosting\ngbcv=GradientBoostingClassifier(learning_rate=0.001,n_estimators=2000,max_depth=5)\ngbcv.fit(X_train,y_train)\ngbcv.predict(X_test)\ngbcv.score(X_test,y_test)","99787387":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, gscv.predict_proba(X_test)[:,1])\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, rfcv.predict_proba(X_test)[:,1])\nknn_fpr, knn_tpr, knn_thresholds = roc_curve(y_test, gsknn.predict_proba(X_test)[:,1])\ngbc_fpr, gbc_tpr, ada_thresholds = roc_curve(y_test, gbcv.predict_proba(X_test)[:,1])\n\nplt.figure(figsize=(9,9))\nlog_roc_auc = roc_auc_score(y_test, gscv.predict(X_test))\nprint (\"logreg model AUC = {} \" .format(log_roc_auc))\nrf_roc_auc = roc_auc_score(y_test, rfcv.predict(X_test))\nprint (\"random forest model AUC ={}\" .format(rf_roc_auc))\nknn_roc_auc = roc_auc_score(y_test, gsknn.predict(X_test))\nprint (\"KNN model AUC = {}\" .format(knn_roc_auc))\ngbc_roc_auc = roc_auc_score(y_test, gbcv.predict(X_test))\nprint (\"GBC Boost model AUC = {}\" .format(gbc_roc_auc))\n# Plot Logistic Regression ROC\nplt.plot(fpr, tpr, label='Logistic Regression')\n\n# Plot Random Forest ROC\nplt.plot(rf_fpr, rf_tpr, label='Random Forest')\n\n# Plot Decision Tree ROC\nplt.plot(knn_fpr, knn_tpr, label=' KnnClassifier')\n\n# Plot GradientBooseting Boost ROC\nplt.plot(gbc_fpr, gbc_tpr, label='GradientBoostingclassifier')\n\n# Plot Base Rate ROC\nplt.plot([0,1], [0,1],label='Base Rate' 'k--')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Graph')\nplt.legend(loc=\"lower right\")\nplt.show()","379b6dc1":"\ntest_PassengerId = pd.read_csv('..\/input\/gender_submission.csv')['PassengerId']\nsubmission = pd.concat([pd.DataFrame(test_PassengerId), pd.DataFrame({'Survived':Survived})], axis=1)\nsubmission.to_csv('submission.csv', index=False)","e90830c6":"# Inferences from the above graph\n* Most passengers were aged from 20-40\n* Most passengers paid nearly 40 units $\/Rs\n## As the graph is left-skewed we can use log scale or sqrt scale to change this","d0cfc327":"> # If you find this notebook helpful , some upvotes would be very much appreciated - That will keep me motivated \ud83d\udc4d","6fa47a26":"### Making some changes in the titles ","3d48daec":"## It is clear from visualisation that most of the survivors were children and women ","b455b2a8":"### Most of the embarkments were from class : S\n### Least embarkments were from class : Q","f8bc94df":"# Applying different types of ML algorithms to find the best accuracies ","8585f25b":"**Importing the necessary libraries**","9c29ca68":"# Analysis of the data ","2d455eb4":"### Using Chi- Squared test at 5% we get the above results telling us which are important  features\n","828142d2":"# Tuning the parameters","7aa5cb58":"\n# Feature Engineering","d1fa29de":"## Most of the people who died were from Passenger Class 3 irrespective of Gender","8b2fee51":"## In this kernel we will see how different people either survived or lost their lives who were present on the great RMS Titanic. \n![Titanic](https:\/\/www.printwand.com\/blog\/media\/2012\/01\/titanic-sinking.jpg)","a5c0d4ba":"## From above statistics it is clear that Women were given more preference than Men while evacuation  ","e56fbfc6":"### The above stats show us survival of each class and its clear the ones in better class had a better chance of survival\n# Power of Money","bba4b0a5":"## The above graph makes it clear that most of the people were aged 20-50","ad8127ff":"### Above we are creating boolean values for the model to understand ","665db1bf":"### Bigger the family lesser the chance of survival\n","071d6ef4":"# Inferences from the above heatmap\n*  PassengerId is a redundant column as its very much less related to all other attributes , we can remove it .\n* Also , Survived is related indirectly with Pclass and also we earlier proved that as Pclass value increases Survival decreases\n* Pclass and Age are also inversely related and can also be proven by the following cell that as Pclass decreases , the mean of the Age      increases , means the much of the older travellers are travelling in high class .\n* Pclass and fare are also highly inversely related as the fare of Pclass 1 would obviously be higher than corresponding Pclass 2 and 3 .\n* Also , people with lower ages or children are travelling with their sibling and parents more than higher aged people (following an                inverse relation) , which is quite a bit obvious .\n* Parch and SibSp are also highly directly related\n* Sex_bool and Survived people are highly inversely related , i.e. females are more likely to survive than men"}}