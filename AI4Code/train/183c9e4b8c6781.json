{"cell_type":{"e5ac31de":"code","d159ca56":"code","1e0f117d":"code","8e6fbc92":"code","082e1ef4":"code","8406acd3":"code","2e5b4d2a":"code","bafbae4e":"code","ece4275c":"code","c2cce9f4":"code","8084d6eb":"code","dda15b84":"code","ab3467cf":"code","55062880":"code","295127e2":"code","d6f696b3":"code","e4381bf9":"code","9e96ad0b":"code","7bbd600e":"code","b0e809b7":"code","415cf3dd":"code","1aca5bfd":"code","595e1ee2":"code","0796d5fe":"code","8e3c5cca":"code","e107d5f8":"code","f3f6c192":"code","7440ec18":"code","d48927d0":"code","c0140d10":"code","be2a539c":"code","20c15295":"code","0293963f":"code","8f79a79a":"code","bf0cfc85":"code","bf8c5f26":"code","34822817":"code","4fccf180":"code","41fcbe93":"code","debb5436":"code","eb596021":"code","e9fddae3":"code","6c160068":"code","746f8b4b":"code","569b27a1":"code","a9e19ed7":"code","8e95566b":"code","191907a8":"code","84a6a4fa":"code","7f70a083":"code","50ddefbd":"code","9016a97b":"code","8be922f0":"code","ca7bf75c":"code","4f4f8013":"code","2f17540b":"markdown","cf87fa61":"markdown","00f1d676":"markdown","5b8108db":"markdown","4910fe7d":"markdown","2c5c6f79":"markdown","362a0189":"markdown","6d8264a8":"markdown","0214a1e8":"markdown","4099e6a1":"markdown","e6b3b326":"markdown","25773af6":"markdown","d8799150":"markdown","92076811":"markdown","942b8214":"markdown","4a45ffa8":"markdown","8ce3e1c0":"markdown","7af96a77":"markdown","6fcf6051":"markdown","1494d856":"markdown","78b8c499":"markdown","f46012db":"markdown","5c8c1cc9":"markdown","5194b9f5":"markdown","c4980252":"markdown","17e3f482":"markdown","efe2feaa":"markdown","6daedecc":"markdown","0e51f1b7":"markdown","4291a86a":"markdown","51ecf7db":"markdown","8f0667cf":"markdown","74836659":"markdown","29cf6b1f":"markdown","3ed6f988":"markdown","d7897c76":"markdown","a6f5cdcf":"markdown","e5fa9cee":"markdown","e095b2f3":"markdown","a6066d1e":"markdown","16a3c254":"markdown","b15bb589":"markdown","64b5b78f":"markdown","2d66f024":"markdown","1e6468b4":"markdown","c5513157":"markdown","83248b62":"markdown","5bc298c6":"markdown"},"source":{"e5ac31de":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nstop = set(stopwords.words('english'))\n\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score","d159ca56":"# official way to get the data\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\nprint('Done!')","1e0f117d":"(market_train_df, news_train_df) = env.get_training_data()","8e6fbc92":"print(f'{market_train_df.shape[0]} samples and {market_train_df.shape[1]} features in the training market dataset.')","082e1ef4":"market_train_df.tail()","8406acd3":"data = []\nfor asset in np.random.choice(market_train_df['assetName'].unique(), 10):\n    asset_df = market_train_df[(market_train_df['assetName'] == asset)]\n#     print(asset_df)\n#     print()\n\n    data.append(go.Scatter(\n        x = asset_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = asset_df['close'].values,\n        name = asset\n    ))\nlayout = go.Layout(dict(title = \"Closing prices of 10 random assets\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","2e5b4d2a":"dd= market_train_df.groupby('time')['close']","bafbae4e":"price_df = market_train_df.groupby('time')['close'].quantile(0.95).reset_index()\nx = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values\nlen(x)","ece4275c":"data = []\nfor i in [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    price_df = market_train_df.groupby('time')['close'].quantile(i).reset_index()\n\n    data.append(go.Scatter(\n        x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = price_df['close'].values,\n        name = f'{i} quantile'\n    ))\nlayout = go.Layout(dict(title = \"Trends of closing prices by quantiles\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),\n    annotations=[\n        dict(\n            x='2008-09-01 22:00:00+0000',\n            y=82,\n            xref='x',\n            yref='y',\n            text='Collapse of Lehman Brothers',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2011-08-01 22:00:00+0000',\n            y=85,\n            xref='x',\n            yref='y',\n            text='Black Monday',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2014-10-01 22:00:00+0000',\n            y=120,\n            xref='x',\n            yref='y',\n            text='Another crisis',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=-20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2016-01-01 22:00:00+0000',\n            y=120,\n            xref='x',\n            yref='y',\n            text='Oil prices crash',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        )\n    ])\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","c2cce9f4":"market_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']\ngrouped = market_train_df.groupby('time').agg({'price_diff': ['std', 'min']}).reset_index()\ngrouped.describe()","8084d6eb":"print(f\"Average standard deviation of price change within a day in {grouped['price_diff']['std'].mean():.4f}.\")","dda15b84":"g = grouped.sort_values(('price_diff', 'std'), ascending=False)[:10]\ng['min_text'] = 'Maximum price drop: ' + (-1 * g['price_diff']['min']).astype(str)\ng","ab3467cf":"g = grouped.sort_values(('price_diff', 'std'), ascending=False)[:10]\ng['min_text'] = 'Maximum price drop: ' + (-1 * g['price_diff']['min']).astype(str)\ntrace = go.Scatter(\n    x = g['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = g['price_diff']['std'].values,\n    mode='markers',\n    marker=dict(\n        size = g['price_diff']['std'].values,\n        color = g['price_diff']['std'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = g['min_text'].values\n    #text = f\"Maximum price drop: {g['price_diff']['min'].values}\"\n    #g['time'].dt.strftime(date_format='%Y-%m-%d').values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Top 10 months by standard deviation of price change within a day',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'price_diff',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","55062880":"aa = market_train_df.sort_values('price_diff')[:10]\n\naa[['assetCode','assetName','close','open','time','price_diff']]","295127e2":"market_train_df['close_to_open'] =  np.abs(market_train_df['close'] \/ market_train_df['open'])","d6f696b3":"aa = market_train_df.sort_values('close_to_open',ascending=False)[:10]\n\naa[['assetCode','assetName','close','open','time','price_diff','close_to_open']]","e4381bf9":"print(f\"{1+1}\") #\u5167\u90e8\u904b\u7b97","9e96ad0b":"print(f\"In {(market_train_df['close_to_open'] >= 1.2).sum()} lines price increased by 20% or more.\")\nprint(f\"In {(market_train_df['close_to_open'] <= 0.8).sum()} lines price decreased by 20% or more.\")","7bbd600e":"print(f\"In {(market_train_df['close_to_open'] >= 2).sum()} lines price increased by 100% or more.\")\nprint(f\"In {(market_train_df['close_to_open'] <= 0.5).sum()} lines price decreased by 100% or more.\")","b0e809b7":"bb = market_train_df.groupby('assetName')['open'].transform('mean')\nbb = bb.sort_values(ascending=False)\nbb.reset_index()\nbb.head()","415cf3dd":"market_train_df['assetName_mean_open'] = market_train_df.groupby('assetName')['open'].transform('mean')\nmarket_train_df['assetName_mean_close'] = market_train_df.groupby('assetName')['close'].transform('mean')\n\n# if open price is too far from mean open price for this company, replace it. Otherwise replace close price.\n# \u5b9a\u4f4d\u5230\u90a3\u4e9b\u6536\u958b\u76e4\u6bd4\u4f8b\u503c\u904e\u5927\u7684dataframe\u503c\n# \u82e5(\u5747\u958b-\u958b) > (\u5747\u6536-\u6536): \nfor i, row in market_train_df.loc[market_train_df['close_to_open'] >= 2].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        market_train_df.iloc[i,5] = row['assetName_mean_open']\n    else:\n        market_train_df.iloc[i,4] = row['assetName_mean_close']\n# \u5b9a\u4f4d\u5230\u90a3\u4e9b\u6536\u958b\u76e4\u6bd4\u4f8b\u503c\u904e\u5c0f\u7684dataframe\u503c        \nfor i, row in market_train_df.loc[market_train_df['close_to_open'] <= 0.5].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        market_train_df.iloc[i,5] = row['assetName_mean_open']\n    else:\n        market_train_df.iloc[i,4] = row['assetName_mean_close']","1aca5bfd":"market_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']\ngrouped = market_train_df.groupby(['time']).agg({'price_diff': ['std', 'min']}).reset_index()\ng = grouped.sort_values(('price_diff', 'std'), ascending=False)[:10]\ng['min_text'] = 'Maximum price drop: ' + (-1 * np.round(g['price_diff']['min'], 2)).astype(str)\ntrace = go.Scatter(\n    x = g['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = g['price_diff']['std'].values,\n    mode='markers',\n    marker=dict(\n        size = g['price_diff']['std'].values * 5,\n        color = g['price_diff']['std'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = g['min_text'].values\n    #text = f\"Maximum price drop: {g['price_diff']['min'].values}\"\n    #g['time'].dt.strftime(date_format='%Y-%m-%d').values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Top 10 months by standard deviation of price change within a day',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'price_diff',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","595e1ee2":"data = []\nfor i in [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    price_df = market_train_df.groupby('time')['returnsOpenNextMktres10'].quantile(i).reset_index()\n\n    data.append(go.Scatter(\n        x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = price_df['returnsOpenNextMktres10'].values,\n        name = f'{i} quantile'\n    ))\nlayout = go.Layout(dict(title = \"Trends of returnsOpenNextMktres10 by quantiles\",\n                  xaxis = dict(title = '     Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),)\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","0796d5fe":"data = []\nmarket_train_df = market_train_df.loc[market_train_df['time'] >= '2010-01-01 22:00:00+0000']\n\nprice_df = market_train_df.groupby('time')['returnsOpenNextMktres10'].mean().reset_index()\n\ndata.append(go.Scatter(\n    x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = price_df['returnsOpenNextMktres10'].values,\n    name = f'{i} quantile'\n))\nlayout = go.Layout(dict(title = \"Treand of returnsOpenNextMktres10 mean\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),)\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","8e3c5cca":"data = []\nfor col in ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n       'returnsOpenNextMktres10']:\n    df = market_train_df.groupby('time')[col].mean().reset_index()\n    data.append(go.Scatter(\n        x = df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = df[col].values,\n        name = col\n    ))\n    \nlayout = go.Layout(dict(title = \"Treand of mean values\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),)\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","e107d5f8":"news_train_df.head()","f3f6c192":"show_df = news_train_df[news_train_df.columns[:10]]\nshow_df.head()","7440ec18":"print(f'{news_train_df.shape[0]} samples and {news_train_df.shape[1]} features in the training news dataset.')","d48927d0":"len(show_df['headline'].str.lower().values)\nlen(show_df['headline'])\nlen(show_df['headline'].str.lower().values[-1000000:]) # \u53d6\u5f8c\u97621000000\u7b46\u7684headline","c0140d10":"# \u5c07\u5f8c\u97621000000\u7b46\u7684headline\u6587\u5b57\u4e32\u806f\u4ee5\u751f\u6210\u6587\u5b57\u96f2\ntext = ' '.join(news_train_df['headline'].str.lower().values[-1000000:])\nwordcloud = WordCloud(max_font_size=None, stopwords=stop, background_color='white',\n                      width=1200, height=1000).generate(text)\nplt.figure(figsize=(12, 8))\nplt.imshow(wordcloud)\nplt.title('Top words in headline')\nplt.axis(\"off\")\nplt.show()","be2a539c":"# Let's also limit the time period\nnews_train_df = news_train_df.loc[news_train_df['time'] >= '2010-01-01 22:00:00+0000']","20c15295":"news_train_df['urgency'].value_counts()","0293963f":"(news_train_df['urgency'].value_counts() \/ 1000000).plot('bar');\nplt.xticks(rotation=0);\nplt.title('Urgency counts (mln)');","8f79a79a":"news_train_df['sentence_word_count'] =  news_train_df['wordCount'] \/ news_train_df['sentenceCount']\nplt.boxplot(news_train_df['sentence_word_count'][news_train_df['sentence_word_count'] < 40]);","bf0cfc85":"# \nee = news_train_df['sentence_word_count'].head()\nee.sort_values(ascending=False).reset_index()","bf8c5f26":"dd = news_train_df[['wordCount','sentenceCount','sentence_word_count']]\ndd.sort_values('sentence_word_count',ascending=False).head()\n# dd.describe()","34822817":"# \u5217\u51fa\u5404\u500b\u65b0\u805e\u63d0\u4f9b\u8005\u6578\u91cf\nnews_train_df['provider'].value_counts().head(10)","4fccf180":"# \u5217\u51fa\u5404\u500b\u65b0\u805e\u982d\u689d\u6a19\u7c64\u6578\u91cf\nee = news_train_df['headlineTag'].value_counts()[:10]\nee.reset_index()","41fcbe93":"# \u5217\u51fa\u5404\u500b\u65b0\u805e\u982d\u689d\u6a19\u7c64\u6578\u91cf\uff0c\u4e26\u756b\u51fa\u503c\u65b9\u5716\n(news_train_df['headlineTag'].value_counts() \/ 1000)[:10].plot('barh');\nplt.title('headlineTag counts (thousands)');","debb5436":"news_train_df['sentimentClass'].value_counts()","eb596021":"# \u5c07\u9019\u4e9b\u8cc7\u7522\u516c\u53f8\u540d\u7a31\u4f9d\u60c5\u7dd2\u5206\u985e\u70ba[\u8ca0\u8a55,\u81ea\u7136,\u6b63\u8a55]\nfor i, j in zip([-1, 0, 1], ['negative', 'neutral', 'positive']):\n#     print(\"{} , {}\".format(i,j)) ZIP\u5c07\u5169\u500bList\u4e2d\u7684item\u5305\u6210\u5143\u7d44\n    df_sentiment = news_train_df.loc[news_train_df['sentimentClass'] == i, 'assetName']\n    print(f'Top mentioned companies for {j} sentiment are:')\n    print(df_sentiment.value_counts().head(5))\n    print('')","e9fddae3":"#%%time\n# code mostly takes from this kernel: https:\/\/www.kaggle.com\/ashishpatel26\/bird-eye-view-of-two-sigma-xgb\n\ndef data_prep(market_df,news_df):\n    market_df['time'] = market_df.time.dt.date\n    market_df['returnsOpenPrevRaw1_to_volume'] = market_df['returnsOpenPrevRaw1'] \/ market_df['volume']\n    market_df['close_to_open'] = market_df['close'] \/ market_df['open']\n    market_df['volume_to_mean'] = market_df['volume'] \/ market_df['volume'].mean()\n    \n    news_df['time'] = news_df.time.dt.hour\n    news_df['sourceTimestamp']= news_df.sourceTimestamp.dt.hour\n    news_df['firstCreated'] = news_df.firstCreated.dt.date\n    news_df['assetCodesLen'] = news_df['assetCodes'].map(lambda x: len(eval(x)))\n    news_df['assetCodes'] = news_df['assetCodes'].map(lambda x: list(eval(x))[0])\n    news_df['headlineLen'] = news_df['headline'].apply(lambda x: len(x))\n    news_df['assetCodesLen'] = news_df['assetCodes'].apply(lambda x: len(x))\n    news_df['asset_sentiment_count'] = news_df.groupby(['assetName', 'sentimentClass'])['time'].transform('count')\n    news_df['asset_sentence_mean'] = news_df.groupby(['assetName', 'sentenceCount'])['time'].transform('mean')\n    lbl = {k: v for v, k in enumerate(news_df['headlineTag'].unique())} # \u5efa\u7acbheadlineTag\u4e2d\u552f\u4e00\u7684Key pair\n    news_df['headlineTagT'] = news_df['headlineTag'].map(lbl) # headlineTagT\u70baheadlineTag\u5c0d\u61c9\u7684\u6578\u5b57\n    kcol = ['firstCreated', 'assetCodes']\n    news_df = news_df.groupby(kcol, as_index=False).mean() # \u5c07\u65b0\u805e\u4ee5\u65e5\u671f\u548c\u5efa\u7acb\u65e5\u671f\u70ba\u5206\u7fa4\uff0c\u5c07\u5404column\u503c\u505a\u5e73\u5747\n\n    market_df = pd.merge(market_df, news_df, how='left', left_on=['time', 'assetCode'], \n                            right_on=['firstCreated', 'assetCodes'])\n\n    lbl = {k: v for v, k in enumerate(market_df['assetCode'].unique())}\n    market_df['assetCodeT'] = market_df['assetCode'].map(lbl)\n    \n    market_df = market_df.dropna(axis=0)\n    \n    return market_df\n\nmarket_train_df.drop(['price_diff', 'assetName_mean_open', 'assetName_mean_close'], axis=1, inplace=True)\nmarket_train = data_prep(market_train_df, news_train_df)\nprint(market_train.shape)\nup = market_train.returnsOpenNextMktres10 >= 0\n\nfcol = [c for c in market_train.columns if c not in ['assetCode', 'assetCodes', 'assetCodesLen', 'assetName', 'assetCodeT', 'volume_to_mean', 'sentence_word_count',\n                                             'firstCreated', 'headline', 'headlineTag', 'marketCommentary', 'provider', 'returnsOpenPrevRaw1_to_volume',\n                                             'returnsOpenNextMktres10', 'sourceId', 'subjects', 'time', 'time_x', 'universe','sourceTimestamp']]\n\nX = market_train[fcol].values # X :\u6574\u9ad4\u8a13\u7df4\u8cc7\u6599\u7126\u9edecolumn\u7684\u503c\nup = up.values\nr = market_train.returnsOpenNextMktres10.values\n\n# Scaling of X values\nmins = np.min(X, axis=0) # \u6b78\u4e00\u6700\u5c0fvector\nmaxs = np.max(X, axis=0) # \u6b78\u4e00\u6700\u5927vector\nrng = maxs - mins # \u6b78\u4e00\u6700\u5c0f\u6700\u5927\u5dee\u503c\u7bc4\u570dvector\nX = 1 - ((maxs - X) \/ rng) # \u6574\u9ad4\u8a13\u7df4\u8cc7\u6599\u6b78\u4e00\u5316","6c160068":"# fcol\nX.shape","746f8b4b":"df = news_train_df[['assetCodes','assetCodesLen','headline','headlineLen','headlineTag','headlineTagT']].reset_index()\ndf[df['headlineTag']!=''].head()","569b27a1":"df = news_train_df[['assetName','sentimentClass','asset_sentiment_count']].sort_values('assetName')\ndf.drop_duplicates()[:10]","a9e19ed7":"df = news_train_df[['assetName','sentenceCount','asset_sentence_mean']].sort_values('assetName')\ndf.drop_duplicates()[:10]","8e95566b":"market_train[fcol].head()","191907a8":"X_train, X_test, up_train, up_test, r_train, r_test = model_selection.train_test_split(X, up, r, test_size=0.1, random_state=99)\n\n# xgb_up = XGBClassifier(n_jobs=4,\n#                        n_estimators=300,\n#                        max_depth=3,\n#                        eta=0.15,\n#                        random_state=42)","84a6a4fa":"params = {'learning_rate': 0.05, 'max_depth': 5, 'boosting': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'is_training_metric': True, 'seed': 42}\nmodel = lgb.train(params, train_set=lgb.Dataset(X_train, label=up_train), num_boost_round=2000,\n                  valid_sets=[lgb.Dataset(X_train, label=up_train), lgb.Dataset(X_test, label=up_test)],\n                  verbose_eval=50, early_stopping_rounds=30)","7f70a083":"df = pd.DataFrame({'imp': model.feature_importance(), 'col':fcol})\ndf = df.sort_values(['imp','col'], ascending=[True, False])\ndf.head()","50ddefbd":"model.feature_importance()","9016a97b":"def generate_color():\n    color = '#{:02x}{:02x}{:02x}'.format(*map(lambda x: np.random.randint(0, 255), range(3)))\n    return color\n\ndf = pd.DataFrame({'imp': model.feature_importance(), 'col':fcol})\ndf = df.sort_values(['imp','col'], ascending=[True, False])\ndata = [df]\nfor dd in data:  \n    colors = []\n    for i in range(len(dd)):\n         colors.append(generate_color())\n\n    data = [\n        go.Bar(\n        orientation = 'h',\n        x=dd.imp,\n        y=dd.col,\n        name='Features',\n        textfont=dict(size=20),\n            marker=dict(\n            color= colors,\n            line=dict(\n                color='#000000',\n                width=0.5\n            ),\n            opacity = 0.87\n        )\n    )\n    ]\n    layout= go.Layout(\n        title= 'Feature Importance of LGB',\n        xaxis= dict(title='Columns', ticklen=5, zeroline=False, gridwidth=2),\n        yaxis=dict(title='Value Count', ticklen=5, gridwidth=2),\n        showlegend=True\n    )\n\n    py.iplot(dict(data=data,layout=layout), filename='horizontal-bar')","8be922f0":"days = env.get_prediction_days()\nimport time\n\nn_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    temp_market_obs_df = market_obs_df # \u66ab\u5b58\u8b58\u5225\n    temp_news_obs_df = news_obs_df  # \u66ab\u5b58\u8b58\u5225\n    temp_predictions_template_df = predictions_template_df  # \u66ab\u5b58\u8b58\u5225\n    n_days +=1\n    if n_days % 50 == 0:\n        print(n_days,end=' ')\n    \n    t = time.time()\n    market_obs_df = data_prep(market_obs_df, news_obs_df)  # \u91cd\u65b0\u6574\u4f75\u9810\u6e2c\u5929\u6578\u7684\u9810\u8a13\u7df4df\u8cc7\u6599\n    market_obs_df = market_obs_df[market_obs_df.assetCode.isin(predictions_template_df.assetCode)]\n    X_live = market_obs_df[fcol].values # X_live :\u9810\u6e2c\u5929\u6578\u4e2d\u6574\u9ad4\u8a13\u7df4\u8cc7\u6599\u7126\u9edecolumn\u7684\u503c\n    X_live = 1 - ((maxs - X_live) \/ rng) # \u9810\u6e2c\u5929\u6578\u4e2d\u6574\u9ad4\u8a13\u7df4\u8cc7\u6599\u6b78\u4e00\u5316\n    prep_time += time.time() - t # \u7d2f\u52a0\u9810\u6e2c\u8cc7\u6599\u524d\u8655\u7406\u6642\u9593\n    \n    t = time.time()\n    lp = model.predict(X_live) # \u4ee5\u6a21\u578b\u9810\u6e2c\u672a\u4f86\u5929\u6578\u4e26\u66f4\u65b0\n    prediction_time += time.time() -t # \u7d2f\u52a0\u9810\u6e2c\u8cc7\u6599\u9810\u6e2c\u6642\u9593\n    \n    t = time.time()\n    confidence = 2 * lp -1 # \u5c07\u6b78\u4e00\u5316\u9084\u539f\u70ba\u4fe1\u5fc3\u503c\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':confidence}) # preds : \u6574\u4f75\u9810\u6e2c\u7684\u4fe1\u5fc3\u503c\u548c\u5176assetCode\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n#     predictions_template_df \u8207 preds \u9032\u884cleft join\u4e26\u4e1f\u68c4\u820a\u6709\u7684confidenceValue\uff0c\u5c07merge\u5f97\u5230\u7684confidence\u547d\u540d\u70ba\u65b0\u7684confidenceValue column\n    env.predict(predictions_template_df) # \u7531\u539f\u751f\u74b0\u5883\u9810\u6e2c\u672a\u4f86\u7684\u4fe1\u5fc3\u503c\n    packaging_time += time.time() - t\n    \nenv.write_submission_file() # \u5c07\u6574\u9ad4\u9810\u6e2c\u7684\u4fe1\u5fc3\u503c\u5beb\u5165\u63d0\u4ea4\u6a94","ca7bf75c":"# \u8b58\u5225\u9810\u6e2c\u5929\u6578\u8cc7\u6599df\n# temp_market_obs_df.head()\n# temp_news_obs_df.head()\n# # \u67e5\u770b\u9810\u6e2c\u65e5\u7684\u4fe1\u5fc3\u503c\n# predictions_template_df[predictions_template_df.confidenceValue!=0.0].head()\n# lp\n# preds.head()\npredictions_template_df.head()","4f4f8013":"# \u5c0b\u627e\u5e02\u5834\u8cc7\u6599\u4e2dassetCode\u7684\u503c\u6709\u51fa\u73fe\u5728predictions_template_df\u7684assetCode\u88e1\n# isin() \u9078\u64c7\u67d0\u5217\u7b49\u65bc\u591a\u500b\u6578\u503c\u6216\u8005\u5b57\u7b26\u4e32\u6642\nmarket_obs_df.assetCode.isin(predictions_template_df.assetCode).head()\n# market_obs_df.assetCode\n# market_obs_df.head()\n# n_days #639","2f17540b":"Another case is with cost equal to 999, such numbers are usually suspicious. Let's look at Archrock Inc - no spikes there as well.\n\n![](https:\/\/i.imgur.com\/KYZKkSd.png)","cf87fa61":"![](http:\/\/fintechnews.ch\/wp-content\/uploads\/2016\/11\/Deutsche-Bank-Survey-87-of-Financial-Market-Participants-Say-Blockchain-Will-Disrupt-The-Industry-1440x564_c.jpg)","00f1d676":"The file is too huge to work with text directly, so let's see a wordcloud of the last 100000 headlines.","5b8108db":"\u5c07\u5e02\u5834\u8cc7\u6599\u7684\u6536\u76e4\u50f9\uff0c\u4ee5\u6642\u9593\u5206\u7fa4\u70ba7\u500b\u4e3b\u8981\u5340\u9593\n\u4e26\u5217\u51fa\u90197\u500b\u5340\u9593\u7684\u8da8\u52e2\u5716\n","4910fe7d":"\u96a8\u6a5f\u9078\u51fa10\u5bb6\u516c\u53f8\uff0c\u5c55\u793a\u4ed6\u5011\u7684\u8cc7\u7522\u6536\u76e4\u50f9\u8da8\u52e2\u5716","2c5c6f79":"Well, for me it is difficult to interpret this, but it seems that returns for previous 10 days fluctuate the most.\n\u4f3c\u4e4e\u524d10\u5929\u7684\u56de\u5831\u7387\u6ce2\u52d5\u6700\u5927\u3002","362a0189":"There are some big outliers, but sentences mostly have 15-25 words in them.\n\u6709\u4e00\u4e9b\u5927\u7684\u96e2\u7fa4\u503c\uff0c\u4f46\u662f\u53e5\u5b50\u5927\u591a\u670915-25\u500b\u55ae\u8a5e\u3002","6d8264a8":"## General information\n\nTwo Sigma Financial News Competition is a unique competitions: not only it is a Kernel-only competition, but we aren't supposed to download data and during stage two our solutions will be used to predict future real data.\n\nI'll try to do an extensive EDA for this competition and try to find some interesting things about the data.\n\nP. S. I'l learning to use plotly, so there will be interactive charts at last!\n\n*The work is in progress.*","0214a1e8":"Training and Valid http:\/\/lightgbm.apachecn.org\/cn\/latest\/Python-Intro.html#id4\n* train_set : \u52a0\u8f7d numpy \u6570\u7ec4\u5230 Dataset \u4e2d\n* valid_sets\n* early_stopping_rounds : \u63d0\u524d\u505c\u6b62\u627e\u5230\u6700\u4f73\u6570\u91cf\u7684 boosting rounds\uff08\u68af\u5ea6\u6b21\u6570\uff09\n* num_boost_round (int, optional (default=100)) \u2013 Number of boosting iterations\n* verbose_eval : \u6bcf\u6b21\u7d93\u904e\u5e7e\u500b\u8dcc\u4ee3\u5c31\u5370\u51fa\u8a13\u7df4\u7684\u6e96\u78ba\u7387\u7d50\u679c(valid_0's : Test auc\t, valid_1's: Valid auc)","4099e6a1":"Well, this isn't much considering we have more than 4 million lines and a lot of these cases are due to price falls during market crash. Well just need to deal with outliers.","e6b3b326":"\u8a08\u7b97\u53e5\u5b50\u4e2d\u7684\u6587\u5b57\u6578\u91cf\uff0c\u4e26\u5283\u51fa\u80fd\u4ee3\u8868\u5206\u5e03\u7684\u76d2\u65b9\u5716","25773af6":"At first I was sad that we don't have access to the texts of the news, but I have realized that we won't be able to use them anyway due to kernel memory limitations.","d8799150":"\u5217\u51fa\u6587\u7ae0\u985e\u578b\u6240\u6709\u7d71\u8a08\u6578\u91cf","92076811":"Now let's remember the description:\n```\nThe marketdata contains a variety of returns calculated over different timespans. All of the returns in this set of marketdata have these properties:\n\n    Returns are always calculated either open-to-open (from the opening time of one trading day to the open of another) or close-to-close (from the closing time of one trading day to the open of another).\n    Returns are either raw, meaning that the data is not adjusted against any benchmark, or market-residualized (Mktres), meaning that the movement of the market as a whole has been accounted for, leaving only movements inherent to the instrument.\n    Returns can be calculated over any arbitrary interval. Provided here are 1 day and 10 day horizons.\n    Returns are tagged with 'Prev' if they are backwards looking in time, or 'Next' if forwards looking.\n```\n\nLet's have a look at means of these variables.","942b8214":"\u8a08\u7b97\u6536\u76e4\u50f9\u8207\u958b\u76e4\u50f9\u4e4b\u6bd4\u503c","4a45ffa8":"At first let's take 10 random assets and plot them.","8ce3e1c0":"It is cool to be able to see how markets fall and rise again.\nI have shown 4 events when there were serious stock price drops on the market.\nYou could also notice that higher quantile prices have increased with time and lower quantile prices decreased.\nMaybe the gap between poor and rich increases... on the other hand maybe more \"little\" companies are ready to go to market and prices of their shares isn't very high.","7af96a77":"\u8a08\u7b97price_diff (Feature)\u70ba \u6536\u76e4\u50f9\u6e1b\u958b\u76e4\u50f9\n\u4e26\u4ee5\u6642\u9593\u5206\u7fa4\uff0c\u8a08\u7b97\u80a1\u50f9\u5dee\u7570\u7684std(\u6a19\u6e96\u5e73\u5747) \u4ee5\u53ca min (\u6700\u5c0f\u503c)","6fcf6051":"So, let's try to find strange cases.","1494d856":"Let's look at the target variable now.\n\u5c07\u6b77\u53f2\u56de\u5831\u7387\u904e\u6ffe\u63892010\u4ee5\u524d\u7684\u7d00\u9304","78b8c499":"### News data","f46012db":"## Market data\n\nWe have a really interesting dataset which contains stock prices for many companies over a decade!\n\nFor now let's have a look at the data itself and not think about the competition. We can see long-term trends, appearing and declining companies and many other things.","5c8c1cc9":"\u5217\u51fa\u6240\u6709\u65b0\u805e\u982d\u689d","5194b9f5":"We have two datasets, let's explore them separately.","c4980252":"* Well, it seems that in fact urgency \"2\" is almost never used.","17e3f482":"Now let's take a look at out target variable.","efe2feaa":"\u627e\u51fa\u524d10\u7b46\u6536\u958b\u76e4\u50f9\u5dee\u6700\u5927\u7684\u8cc7\u7522\u516c\u53f8","6daedecc":"There were no spikes.","0e51f1b7":"So price of \"Towers Watson & Co\" shares was almost 10k... I think this is simply an error in data.\n\nBut what about Bank of New York Mellon Corp?\n\nLet's see data by Yahoo:","4291a86a":"![](https:\/\/i.imgur.com\/C3COWfe.png)","51ecf7db":"## Modelling\n\nIt's time to build a model!\nI think that in this case we should build a binary classifier - we will simply predict whether the target goes up or down.","8f0667cf":"I plot data for all periods because I'd like to show long-term trends.\nAssets are sampled randomly, but you should see that some companies' stocks started trading later, some dissappeared. Disappearence could be due to bankruptcy, acquisition or other reasons.","74836659":"### Possible data errors\n\nAt first let's simply sort data by the difference between open and close prices.","29cf6b1f":"Now the graph is much more reasonable.","3ed6f988":"Now, let's look at these price drops in details.","d7897c76":"\u53d6\u524d10\u500b\u80a1\u50f9\u6a19\u6e96\u5dee\u5dee\u7570\u6700\u5927\u7684\u8cc7\u7522\u516c\u53f8\uff0c\u4ee5\u5713\u9ede\u5716\u5c55\u793a\u5dee\u7570\u5927\u5c0f","a6f5cdcf":"Well, most news are tagless.\n\u5927\u591a\u6578\u65b0\u805e\u90fd\u662f\u7121\u6a19\u7c64\u7684\u3002","e5fa9cee":"Well, these were some random companies. But it would be more interesting to see general trends of prices.","e095b2f3":"I think it is quite funny that Apple is a company with most both negative and positive sentiments.\n\u6211\u8a8d\u70ba\u860b\u679c\u516c\u53f8\u662f\u4e00\u5bb6\u65e2\u6709\u6d88\u6975\u60c5\u7dd2\u53c8\u6709\u7a4d\u6975\u60c5\u7dd2\u7684\u516c\u53f8\uff0c\u9019\u5f88\u6709\u8da3\u3002","a6066d1e":"For a quick fix I'll replace outliers in these lines with mean open or close price of this company.\n\u8655\u7406outliers\u503c\u7684\u65b9\u5f0f\u662f\u7528 \"\u4ee5\u8cc7\u7522\u540d\u7a31\"\u5206\u7fa4\u5c07\u958b\u76e4\u50f9\u8207\u6536\u76e4\u50f9\u8f49\u70ba\u8cc7\u7522\u5e73\u5747\u503c","16a3c254":"### Getting data and importing libraries","b15bb589":"Now let's try to build that graph again.","64b5b78f":"http:\/\/lightgbm.apachecn.org\/cn\/latest\/Parameters.html\n* LightBGM\u662f\u4e00\u500b\u53ef\u4ee5\u627e\u51fa\u6700\u4f73\u985e\u5225\u7279\u5fb5\u5207\u5272\u7684\u6c7a\u7b56\u6a39\uff0c\u5373many-vs-many\u7684\u5207\u5206\u65b9\u5f0f\u3002\u5e76\u4e14\u6700\u4f18\u5206\u5272\u7684\u67e5\u627e\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u53ef\u4ee5\u5728\u7ebf\u6027\u65f6\u95f4\u5b8c\u6210\uff0c\u548c\u539f\u6765\u7684one-vs-other\u7684\u590d\u6742\u5ea6\u51e0\u4e4e\u4e00\u81f4\n* boosting : gbdt, \u50b3\u7d71\u7684\u68af\u5ea6\u63d0\u5347\u6c7a\u7b56\u6a39\n* objective(\u640d\u5931\u51fd\u6578) : binary, binary log loss classification application\n* metric(\u5ea6\u91cf\u53c3\u6578) : AUC (Area under the curve) \u6bd4\u8f03\u66f2\u7dda\u4e0b\u9762\u7a4d\u505a\u70ba\u6a21\u578b\u512a\u52a3\u7684\u6307\u6a19\u3002 https:\/\/zh.wikipedia.org\/wiki\/ROC%E6%9B%B2%E7%BA%BF\n* is_training_metric \u5982\u679c\u4f60\u9700\u8981\u8f38\u51fa\u8a13\u7df4\u7684\u5ea6\u91cf\u7d50\u679c\u5247\u8a2d\u7f6e true\n* seed : \u5b78\u7fd2\u6578\u64da\u5206\u9694\u4e2d\u7684\u96a8\u6a5f\u7a2e\u5b50","2d66f024":"It isn't surprising that Reuters is the most common provider :)","1e6468b4":"Collapse of Lehman Brothers : 2008\u5e749\u670815\u65e5\uff0c\u5728\u7f8e\u570b\u8ca1\u653f\u90e8\u3001\u7f8e\u570b\u9280\u884c\u4ee5\u53ca\u82f1\u570b\u5df4\u514b\u840a\u9280\u884c\u76f8\u7e7c\u653e\u68c4\u6536\u8cfc\u8ac7\u5224\u5f8c\uff0c\u96f7\u66fc\u5144\u5f1f\u516c\u53f8\u5ba3\u4f48\u7533\u8acb\u7834\u7522\u4fdd\u8b77\nBlack Monday :\n\u9ed1\u8272\u661f\u671f\u4e00\u6307\u80a1\u5e02\u5927\u8dcc\u7d93\u5e38\u51fa\u73fe\u5728\u661f\u671f\u4e00\u7684\u73fe\u8c61\u3002\u6700\u8457\u540d\u7684\u9ed1\u8272\u661f\u671f\u4e00\u662f1987\u5e7410\u670819\u65e5\u7f8e\u570b\u80a1\u5e02\u767c\u751f\u7684\u5927\u8dcc\uff0c\u7576\u65e5\u9053\u74ca\u65af\u6307\u6578\u4e0b\u8dcc\u4e8622%\u3002\u9053\u74ca\u65af\u5de5\u696d\u5e73\u5747\u6307\u6578\u4e0b\u8dcc\u81f3508\u9ede\u3002\u9ed1\u8272\u661f\u671f\u4e00\uff0c\u662f\u6700\u4ee4\u4eba\u96e3\u4ee5\u7f6e\u4fe1\u7684\u4e00\u5929\u3002\u56e0\u70ba\u90a3\u5152\u6c92\u6709\u5e02\u5834\uff0c\u88ab\u7a31\u4f5c\u201c\u81ea\u7531\u4e0b\u8dcc\u201d\u3002\u81ea\u7531\u4e0b\u8dcc\uff0c\u7c21\u55ae\u5730\u8aaa\u5c31\u662f\u50f9\u683c\u4e00\u76f4\u4e0b\u8dcc\uff0c\u6c92\u6709\u4efb\u4f55\u8cb7\u5bb6\u3002\u50c5\u50c5\u4e00\u5929\u9053\u74ca\u65af\u55aa\u5931\u5176\u50f9\u503c\u768423%\uff0c\u6578\u767e\u5104\u7f8e\u5143\u6d88\u5931\u4e86\u3002\u88dd\u4e86\u7279\u6b8a\u7a0b\u5e8f\u7684\u8a08\u7b97\u6a5f\u4e0d\u505c\u5730\u5728\u8ce3\uff0c\u4efb\u4f55\u8a66\u5716\u4f7f\u5176\u7a69\u5b9a\u4e0b\u4f86\u7684\u52aa\u529b\u90fd\u5931\u6557\u4e86\u3002\n\n\u3000\u3000\u5c0d\u9ed1\u8272\u661f\u671f\u4e00\u73fe\u8c61\u7684\u4e00\u7a2e\u89e3\u91cb\u662f\u56e0\u70ba\u5468\u672b\u5bb9\u6613\u516c\u4f48\u4e00\u4e9b\u91cd\u5927\u8ca0\u9762\u65b0\u805e\uff0c\u6240\u4ee5\u5468\u4e00\u958b\u76e4\u5f8c\u6295\u8cc7\u8005\u6703\u6709\u5287\u70c8\u53cd\u61c9\u3002\u4f46\u6b77\u53f2\u4e0a\u8a31\u591a\u9ed1\u8272\u661f\u671f\u4e00\u4e4b\u524d\u7684\u5468\u672b\u4e26\u6c92\u6709\u51fa\u73fe\u4ec0\u9ebc\u91cd\u5927\u6d88\u606f\uff0c\u6240\u4ee5\uff0c\u5f88\u591a\u60c5\u6cc1\u4e0b\u662f\u7531\u65bc\u5e02\u5834\u8b20\u8a00\u5f15\u767c\u7684\u6050\u614c\u6027\u62cb\u76e4\u3002\u4e8b\u5be6\u4e0a\uff0c\u5168\u7403\u6b77\u53f2\u4e0a\u7684\u80a1\u5e02\u5927\u8dcc\u5e7e\u4e4e\u5728\u5468\u4e00\u81f3\u9031\u4e94\u90fd\u6709\u767c\u751f\u3002\u5982\u679c\u6295\u8cc7\u8005\u5728\u7121\u5be6\u8cea\u6027\u5229\u7a7a\u4e0b\u7206\u767c\u7684\u6050\u614c\u6027\u4e0b\u8dcc\u4e2d\u80fd\u4fdd\u6301\u7406\u6027\uff0c\u901a\u5e38\u6703\u7372\u5f97\u6975\u4f73\u7684\u8cb7\u5165\u6a5f\u6703\uff0c\u5982\u7f8e\u570b87\u5e74\u80a1\u707d\u5f8c1\u5e74\u591a\u6642\u9593\u5c31\u6536\u5fa9\u5931\u5730\u3002\n\nAnother crisis \u53e6\u4e00\u5834\u91d1\u878d\u6d77\u562f\nOil prices crash \u6cb9\u50f9\u66b4\u8dcc","c5513157":"We can see that quantiles have a high deviation, but mean value doesn't change much.\n\nNow I think it is time to throw an old part of dataset. Let's leave only data since 2010 year, this way we will get rid of the data of the biggest crisis.","83248b62":"\u6ce2\u52d5\u4f3c\u4e4e\u5f88\u9ad8\uff0c\u4f46\u5be6\u969b\u4e0a\u5b83\u5011\u6bd48%\u8981\u4f4e\u3002\u4e8b\u5be6\u4e0a\uff0c\u5b83\u770b\u8d77\u4f86\u50cf\u4e00\u500b\u96a8\u6a5f\u7684\u566a\u97f3\u2026\u2026","5bc298c6":"We can see huge price fluctiations when market crashed. Just think about it... **But this is wrong!** There was no huge crash on January 2010... Let's dive into the data!"}}