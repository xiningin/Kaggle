{"cell_type":{"0fea1d69":"code","cf54b8b3":"code","9b992056":"code","1cf8a8c5":"code","52d663fd":"code","4539afe7":"code","545865c1":"code","7489ec12":"code","e8cb31cd":"code","c46648ca":"code","917cda43":"code","68797ac2":"code","b7164030":"code","69e1db0d":"markdown","67db5c0f":"markdown","6175f72e":"markdown","188f1af4":"markdown","39d33c3b":"markdown","e935e65d":"markdown","84853740":"markdown","02b54e21":"markdown","5e511848":"markdown","43dc097a":"markdown","5f489c5a":"markdown","1b0fec53":"markdown","31151ee5":"markdown","ab5d7596":"markdown","15894dd5":"markdown","4588c809":"markdown"},"source":{"0fea1d69":"import numpy as np  \nimport pandas as pd  \n\nimport plotly.express as px\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import r2_score, mean_squared_log_error\nimport category_encoders as ce\n\nimport multiprocessing\n\nDEP_VAR = 'SalePrice'\n\ntrain_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv').set_index('Id')\n\n# split the dependent variable from the features\ny_train = train_df[DEP_VAR]\ntrain_df.drop(DEP_VAR, axis=1, inplace=True)\n\ntest_df =  pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv').set_index('Id')","cf54b8b3":"print(train_df.shape)\n\nfeature_types = train_df.dtypes.astype(str).value_counts().to_frame('count').rename_axis('datatype').reset_index()\n\n\npx.bar(feature_types, x='datatype', y='count', color='datatype')\\\n    .update_layout(showlegend=False)\\\n    .update_layout(title={'text': 'Ames Dtypes', 'x': .5})","9b992056":"select_numeric_features = make_column_selector(dtype_include=np.number)\n\nnumeric_features = select_numeric_features(train_df)\n\nprint(f'N numeric_features: {len(numeric_features)} \\n')\nprint(', '.join(numeric_features))","1cf8a8c5":"train_df.fillna(np.nan, inplace=True)\ntest_df.fillna(np.nan, inplace=True)\n\nnumeric_pipeline = make_pipeline(SimpleImputer(strategy='median', add_indicator=True))  ","52d663fd":"MAX_OH_CARDINALITY = 10\n\ndef select_oh_features(df):\n    \n    hc_features =\\\n        df\\\n        .select_dtypes(['object', 'category'])\\\n        .apply(lambda col: col.nunique())\\\n        .loc[lambda x: x <= MAX_OH_CARDINALITY]\\\n        .index\\\n        .tolist()\n        \n    return hc_features\n\noh_features = select_oh_features(train_df)\n\nprint(f'N oh_features: {len(oh_features)} \\n')\nprint(', '.join(oh_features))","4539afe7":"oh_pipeline = make_pipeline(SimpleImputer(strategy='constant'), OneHotEncoder(handle_unknown='ignore'))","545865c1":"def select_hc_features(df):\n    \n    hc_features =\\\n        df\\\n        .select_dtypes(['object', 'category'])\\\n        .apply(lambda col: col.nunique())\\\n        .loc[lambda x: x > MAX_OH_CARDINALITY]\\\n        .index\\\n        .tolist()\n        \n    return hc_features\n\n\nhc_features = select_hc_features(train_df)\n\nprint(f'N hc_features: {len(hc_features)} \\n')\nprint(', '.join(hc_features))","7489ec12":"hc_pipeline = make_pipeline(ce.GLMMEncoder())","e8cb31cd":"column_transformer = ColumnTransformer(transformers=\\\n                                       [('numeric_pipeline', numeric_pipeline, select_numeric_features),\\\n                                        ('oh_pipeline', oh_pipeline, select_oh_features),\\\n                                        ('hc_pipeline', hc_pipeline, select_hc_features)],\n                                       n_jobs = multiprocessing.cpu_count(),\n                                       remainder='drop')","c46648ca":"X_train = column_transformer.fit_transform(train_df, y_train)\nX_test = column_transformer.transform(test_df)\n\nprint(X_train.shape)\nprint(X_test.shape)","917cda43":"model = GradientBoostingRegressor(learning_rate=0.025, n_estimators=1000, subsample=0.25, max_depth=5,\\\n                                 min_samples_split=50, max_features='sqrt')\nmodel.fit(X_train, y_train)\n\ny_train_pred = model.predict(X_train)","68797ac2":"print(f'R-squared: {r2_score(y_train, y_train_pred)}')\nprint(f'RMSLE: {np.sqrt(mean_squared_log_error(y_train, y_train_pred))}')","b7164030":"submission = pd.DataFrame(dict(Id=test_df.index, \n                               SalePrice=model.predict(X_test)))\nsubmission.to_csv(\"submission.csv\", index=False)","69e1db0d":"With an R-squared value close to 1, our features explain nearly all of the variation in the training set's dependent variable.\nThe root mean-squared log error was nearly 0.07.","67db5c0f":"## The Dataset\n\n\nThe Ames training dataset has a relatively small number of observations and a decent amount of features at 79. 43 of these features are categorical, and 36 are numeric. I recommend reading [this notebook](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) if you are interested in some exploratory data analysis on the dataset.\n\n","6175f72e":"# Putting It All Together","188f1af4":"Feature engineering can be a time consuming part of the machine learning process, especially if you are dealing with many features and different types of features. Over the course of my projects, I've developed some heuristics that allow me to construct a reasonably effective Scikit-Learn ColumnTransformer quickly and dynamically. \n\nIn my post, I will demonstrate 2 techniques. First, I'll show how to select features with logical conditions instead of listing every single column in the code. Second, I will explain the transformer pipelines that I use as my \"defaults\" when training a new model. I will demonstrate my technique on the Ames, IA house prices dataset, which you can find on [Kaggle](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data).\n\n\nBefore proceeding, I should note that my post assumes that you have worked with Scikit-Learn and Pandas before and are familiar with how [ColumnTransformer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.compose.ColumnTransformer.html), [Pipeline](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.pipeline) & [preprocessing classes](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.preprocessing) facilitate reproducible feature engineering processes. If you need a refresher, checkout this Scikit-Learn [example](https:\/\/scikit-learn.org\/stable\/modules\/compose.html#columntransformer-for-heterogeneous-data).\n\nLet's start by importing the required packages, classes, functions and data.\n\n\n\n<!-- \n \nLastly, I will collapse all the code into a function that will rely on my default settings and instantiate the ColumnTransformer. \n\nThese rules of thumb work particularly well for tree-based models, which have fewer feature-engineering requirements.\n -->\n","39d33c3b":"## Categorical with high cardinality\n\nTo select the features with high cardinality, I've created a similar function that selects the `object` and `category` features with unique value counts greater than the threshold. It selects three features that meet these criteria.","e935e65d":"Let's see how are engineered features perform on an GBM regressor without rigorous hyperparameter tuning.","84853740":"# Results\n\nAfter fitting the ColumnTransformer and transforming the data, the OH encodings increased the number of columns from 79 to 254. If we hadn't used the `GLMMEncoder`, we would be dealing with over 300 columns.","02b54e21":"![img](https:\/\/images.unsplash.com\/photo-1560574188-6a6774965120?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80)","5e511848":"Finally, let's put all the pieces together and instantiate our ColumnTransformer:\n\n- The `transformer` parameter accepts a list of 3-element tuples. Each tuple contains the name of the transformer\/pipeline, the instantiated pipelines and the selector functions that we created. \n\n- If you are dealing with a significant number of features and mulit-thread capability, I would definitely set the `n_jobs` parameter, so that the pipelines can be run in parallel. I've used `multiprocessing.cpu_count` to use all available threads.\n\n- Lastly, I want to call attention to the `remainder` parameter. By default, ColumnTransformer drops any columns not included in the `transformers` list. Alternatively, if you have features that require no transformations, you could set this argument to \"passthrough\" and not drop any remaining features.","43dc097a":"I have two default transformations for categorical features with low-to-moderate cardinality: `SimpleImputer` and [OneHotEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html)\n\nIn the `SimpleImputer`, using the \"constant\" strategy sets the missing values to \"missing_value.\" (I don't set the `add_indicator` parameter to `True` since this would create duplicated columns.) In the OH encoder, I like to set the `handle_unknown` parameter to \"ignore\" instead of using the default \"error,\" so that this transformer won't throw an error if it encounters an unknown value in the test dataset. Instead, it sets all of the OH columns to zero if this situation occurs. Because the Ames test dataset contains categorical values not in the training dataset, our ColumnTransformer will fail on the test dataset without using this setting. If you are planning to use a linear model, you will want to set the `drop` parameter so that the features are not perfectly collinear.\n\n\n<!-- with a \"constant\" strategy -->","5f489c5a":"To transform our features with high cardinality, I could have gone with a more basic approach and used Scikit-Learn's native [LabelEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder) or [OrdinalEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder) preprocessor. However, in many cases, these methods are likely to [perform suboptimally](https:\/\/towardsdatascience.com\/choosing-the-right-encoding-method-label-vs-onehot-encoder-a4434493149b) in your model unless you are dealing with ordinal data. I prefer to use the [Category Encoder](http:\/\/contrib.scikit-learn.org\/category_encoders) package, which has more than a dozen ways of intelligently encoding highly cardinal features. [This post](https:\/\/towardsdatascience.com\/all-about-categorical-variable-encoding-30) provides an overview of several of these methods. Most of these are supervised techniques, which use the dependent variable to transform the nominal values into numerical ones. The [TargetEncoder](http:\/\/contrib.scikit-learn.org\/category_encoders\/targetencoder.html) is probably the easiest method to understand, but I prefer to use the [Generalized Linear Mixed Model Encoder](http:\/\/contrib.scikit-learn.org\/category_encoders\/glmm.html), which has \"solid statistical theory behind [it]\" and \"no hyperparameters to tune.\" Without diving into the [details of GLMMs](https:\/\/stats.idre.ucla.edu\/other\/mult-pkg\/introduction-to-generalized-linear-mixed-models\/), at its core, this method encodes the nominal values as the coefficents from a one-hot-encoded linear model. The Category Encoder methods handle missing and unknown values by setting them to zero or the mean of the dependent variable. (If these features in the Ames training dataset had any missing values, we would also want to create missing indicators.)\n\n","1b0fec53":"# Types of Features\n\n\nIf you are anything like me, the thought of listing 79 features in the code or a configuration file seems like a tedious and unnecessary task. What if there was a way to logically bucket these features by their characteristics?\n\n\nThe key insight that allows you to dynamically construct a ColumnTransformer is understanding that there are 3 broad types of features in non-textual, non-time series datasets:\n\n1. numerical \n2. categorical with moderate-to-low cardinality\n3. categorical with high cardinality\n\nLet's take a look at how to dynamically select each feature type and the default transformer pipeline I use with it.\n\n\n## Numerical Features\n\nThe sklearn.compose module comes with a handy class called [make_column_selector](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.compose.make_column_selector.html), and it provides some limited functionality to dynamically select columns. You can list dtypes to include or exclude or use a regex pattern to select the column names. To select numeric features, we will instantiate a function to select columns with the np.number datatype, which will match any integer or float columns. When we call the `select_numeric_features` on the training dataset, we see that it correctly selects the 36 `int64` and `float64` columns.\n\n\n\n<!-- (Datetime would be a fourth type, but we won't address that here.) -->","31151ee5":"## Categorical with moderate-to-low cardinality\n\nNext, let's discuss how to select and transform the nominal data into numeric form. \n\n[One-hot (OH) encoding](https:\/\/machinelearningmastery.com\/why-one-hot-encode-data-in-machine-learning\/), where an indicator column is created for each unique value, is the most common method. However, the OH transformation may not be suitable for features with high [cardinality](https:\/\/en.wikipedia.org\/wiki\/Cardinality). OH encoding features with many unique values may create too many columns with very low variance, which may take up too much memory or have a negative impact on the performance of linear models. Hence, we may want to limit the features we select for this encoding to ones below a certain threshold of unique values. For the sake of illustration, I'm going to set my limit at 10 values. In reality, we would probably select the threshold to a higher value depending upon the size of your dataset.\n\nSince the [`make_column_selector` isn't capable of detecting cardinality](https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15873), I've developed my own `select_oh_features` custom function. It consists of a piping of pandas methods that do the following:\n\n\n- Selects the `object` and `category` dtypes from the pandas `DataFrame`\n\n- Counts the number of unique values for those columns\n\n- Subsets the unique value counts if they are less than or equal to `MAX_OH_CARDINALITY` using an anonymous `lambda` function within the `loc` method\n\n- Extracts the column names from the index and returns them as a list\n\n\nWhen we call the function on the training dataset, we see that it selects 40 of the 43 categorical features.\n","ab5d7596":"However, the test RMSLE turned out to be 0.13249 when I submitted my test predictions to Kaggle. The gap between the training and test RMSLE indicates that the model is overfit and that it would benefit from regularization and hyperparameter tuning.\n\nStay tuned for further posts on training & regularizing models with Scikit-Learn ColumnTransformers and Pipelines. Let me know if you found this post helpful or have any ideas for improvement. Thanks!","15894dd5":"My default numeric feature transformation involves using the [SimpleImputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html). I impute missing values with the median and set the `add_indicator` parameter to `True`. Using the median instead of the imputer's mean default guards against the influence of outliers. Using the `add_indicator` functionality calls the [MissingIndicator class](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.MissingIndicator.html#sklearn.impute.MissingIndicator), which creates binary missing-indicator columns for each feature with missing values. In my experience, these columns can be moderately important to the model when the data is not missing at random.\n\nA few things to note:\n\n- When I construct transformer pipelines, I prefer to use the [make_pipeline](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.make_pipeline.html) function as opposed to the Pipeline class. This function replaces the requirement to explicitly name each Pipeline step by automatically using the lower-cased version of the class name, e.g. SimpleImputer is named 'simpleimputer'.\n\n- Scikit-Learn imputers require that the missing values are represented with `np.nan` -- hence, my use of the `fillna` method.\n\n- If you are going to use a linear model, you are going to want to insert one of the [preprocessors](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.preprocessing) to center and scale before the imputer.\n\n- More sophisticated alternatives to the SimpleImputer include the [KNNImputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer), which requires centering and scaling, or the experimental [IterativeImputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer).\n\n\n\n\n\n","4588c809":"# Building a Scikit-Learn ColumnTransformer Dynamically\n\n## Using logical conditions to select types of features for transformation"}}