{"cell_type":{"f94f7cba":"code","f975c06e":"code","4044efdc":"code","a2078144":"code","fd161a71":"code","2e36b065":"code","6c9ac749":"code","6fa5626e":"code","4f7aac12":"code","a4dc1f43":"code","0b903e7d":"code","93645090":"code","5dc7697e":"code","8c8e5037":"code","b26cdbef":"code","7ced88ab":"code","053c0ba8":"code","db58185c":"markdown","5045fb7b":"markdown","8e9f86da":"markdown","06350883":"markdown","6c79d8f2":"markdown","20c22ca4":"markdown","1526d0a8":"markdown","9d67d7d8":"markdown","f4d44a9b":"markdown","108e418e":"markdown","f733bcfa":"markdown","1bcadfcc":"markdown","58d60c43":"markdown","b9babe19":"markdown","da88d865":"markdown","fd945358":"markdown"},"source":{"f94f7cba":"# Import the libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","f975c06e":"# Import the data\npath = '..\/input\/dataset\/bank_customers.csv'\ndata_raw = pd.read_csv(path)\n\n# View the first 5 observations of the data\ndata_raw.head()","4044efdc":"# View a more detailed summary of the data\ndata_raw.describe(include='all')","a2078144":"# Drop the 'RowNumber', 'CustomerId', and 'Surname' columns\ndata = data_raw.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\ndata.head()","fd161a71":"# Declare the features and labels\nX = data.drop('Exited', axis=1)  # Features\ny = data['Exited']  # Label","2e36b065":"X.head()","6c9ac749":"# Create dummy variables\ngeo = pd.get_dummies(X['Geography'], drop_first=True)\ngender = pd.get_dummies(X['Gender'], drop_first=True)\n\n# Concatenate the dummy columns\nX = pd.concat([X, geo, gender], axis=1)\n\n# Drop the columns that are no longer required\nX = X.drop(['Geography', 'Gender'], axis=1)\nX.head()","6fa5626e":"# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","4f7aac12":"# Scale our data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","a4dc1f43":"# Build the model\nmodel = Sequential()\nmodel.add(Dense(input_dim=11, units=6, activation='relu'))\nmodel.add(Dense(units=6, activation='relu', name='hid'))\nmodel.add(Dense(units=1, activation='sigmoid'))","0b903e7d":"# Get the architecture of our model\nmodel.summary()","93645090":"# Compile the model\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","5dc7697e":"# Fit the model\nhistory = model.fit(X_train, y_train, batch_size=10, epochs=100)","8c8e5037":"# Visualize the loss for each epoch\nplt.plot(history.history['loss'])\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()","b26cdbef":"# Make predictions\ny_pred = model.predict(X_test)\n\n# Convert into Boolean values\ny_pred = y_pred > 0.5","7ced88ab":"# Evaluate the model\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint(f'The loss on the test set is: {test_loss}')\nprint(f'The accuracy on the test set is: {test_acc}')","053c0ba8":"# Create the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Visualize the confusion matrix\nsns.heatmap(cm, annot=True)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()","db58185c":"## Preprocessing the Data\nAs we can see above, we don't have any missing values in our data. However, there are some columns that might not be useful for building our model e.g `RowNumber`, `CustomerId`, and `Surname`.","5045fb7b":"We can observe that our model is pretty good at classifying 0 values (customers leaving the bank) but not that good in the case where the values are 1 (customers that are actually staying). This is probably due to the imbalance amount of label in our data where our data contains a lot more 0 values than the 1 values.","8e9f86da":"As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.86 (or 86%) on the training set. We can also visualize the plot for the loss for each epoch.","06350883":"One last step is we need to do feature scaling on our data before building our model. To avoid any data leakage, we will fit the scaler on our training set only, then standardise the test set with that scaler.","6c79d8f2":"It turns out that the accuracy on the test dataset is a slightly higher than the accuracy on the training data. As another evaluation metric, we can create a confusion matrix to further examine our model's performace.","20c22ca4":"With the model trained, we can now use it to make predictions.","1526d0a8":"## Building our Model\nAfter cleaning and preprocessing our data, we are now ready to build our ANN model. Our data will consist of 2 dense (fully-connected) layers with ReLu activation functions and another 1 dense layer as our output layer with Sigmoid activation function.","9d67d7d8":"Before training the model, we need to compile the model with the following settings:\n\n- Loss function: This measures how accurate the model is during training. For the loss function, we are going to use the binary cross entropy.\n- Optimizer: This is how the model is updated based on the data it sees and its loss function. We are going to use Adam, which is an optimization algorithm based on adaptive estimation of first-order and second-order moments.\n- Metrics: Used to monitor the training and testing steps. Here we will use accuracy, the fraction of the images that are correctly classified.","f4d44a9b":"Finally, we can fit the training set to our model.","108e418e":"# Artificial Neural Network on Bank Customers Dataset\n\nThis project is one of my Deep Learning projects. For this project, we have a [bank customers](https:\/\/github.com\/richardcsuwandi\/datasets\/blob\/master\/bank_customers.csv) dataset that consists of a randomly sampled population of a banking customers detailing demographics and whether a customer left (or stayed at) the bank within the last 6 months.\n\nThe goal of this project is create an Artificial Neural Network (ANN) model to classify whether a customer will stay or leave the bank.","f733bcfa":"Now, we can split our data into training and test sets. Following the Pareto principle, we are going to split the data with a ratio of 80:20.","1bcadfcc":"## Loading the Data\nFirst, we need to load the data from the `bank_customers.csv` file and convert it into a Pandas DataFrame.","58d60c43":"We notice that we have two columns that contain categorical values: `Geography` and `Gender`. So, we are going to create dummy variables to need to convert those categorical values into numerical values.","b9babe19":"Now, let's take a look at our features (X) and determine whether we need to further preprocess it or not.","da88d865":"## Evaluating the Model\nLastly, let's evaluate our model on the test set.\n","fd945358":"Next, we can declare the features and label in our dataset. Here, our label (column to predict) is the `Exited` column which contains either 0 (left) or 1 (stayed). All the other columns will be our features."}}