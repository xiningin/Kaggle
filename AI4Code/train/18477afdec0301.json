{"cell_type":{"65419cbd":"code","f5510c28":"code","4515e6e8":"code","59bd8774":"code","5f374235":"code","ffd0e153":"code","1d42e101":"code","96748b01":"code","70c707e1":"code","84f443a6":"code","274bdda5":"code","b8e76277":"code","6c7144e2":"code","b6261668":"code","cd1fc582":"code","3ef2fdeb":"code","3c306c96":"code","434c0cb9":"code","036362b5":"code","c6a4192d":"code","e0bf2c80":"code","a40503cb":"code","b9846c62":"code","e1ebb323":"code","df6c8da8":"code","29178d44":"code","c87b49d1":"code","d213dff1":"code","905deaf7":"markdown","db8d80a6":"markdown","ee867b70":"markdown","72749a8a":"markdown","8f741bd6":"markdown","42512114":"markdown"},"source":{"65419cbd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f5510c28":"import numpy as np\nimport pandas as pd\nimport datatable as dt\nimport psutil\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import train_test_split\nimport riiideducation\nfrom pandas_profiling import ProfileReport\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nimport lightgbm as lgb\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom scipy.stats import ttest_ind\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nfrom sklearn.metrics import mean_absolute_error\nfrom matplotlib import pyplot as plt \n%matplotlib inline\nimport seaborn as sns","4515e6e8":"train_df = pd.DataFrame()\ncounter = 1\nfor chunk in pd.read_csv('..\/input\/riiid-test-answer-prediction\/train.csv', chunksize=1000000, low_memory=False, nrows=10000000):\n    print('Reading chunck {}'.format(counter))\n    train_df = pd.concat([train_df, chunk], ignore_index=True)\n    counter += 1","59bd8774":"test_df = pd.read_csv('..\/input\/riiid-test-answer-prediction\/example_test.csv')\nquestions_df = pd.read_csv('..\/input\/riiid-test-answer-prediction\/questions.csv')\nlectures_df = pd.read_csv('..\/input\/riiid-test-answer-prediction\/lectures.csv')","5f374235":"train_df","ffd0e153":"print(train_df['answered_correctly'].isna().sum())\nprint(train_df.shape[0])\nprint('There are {:0.4f}% of missing data in answered_correctly. '.format(train_df['answered_correctly'].isna().sum()\/train_df.shape[0]))","1d42e101":"print(train_df['prior_question_elapsed_time'].isna().sum())\nprint(train_df.shape[0])\nprint('There are {:0.4f}% of missing data in prior_question_elapsed_time. '.format(train_df['prior_question_elapsed_time'].isna().sum()\/train_df.shape[0]))","96748b01":"print(train_df['prior_question_had_explanation'].isna().sum())\nprint(train_df.shape[0])\nprint('There are {:0.4f}% of missing data in prior_question_had_explanation. '.format(train_df['prior_question_had_explanation'].isna().sum()\/train_df.shape[0]))","70c707e1":"# Code from https:\/\/www.kaggle.com\/dmikar\/baseline-for-riiid-lightgbm\nmean_prior = train_df['prior_question_elapsed_time'].astype(\"float64\").mean()\nprint(f'{mean_prior} is filled for the missing data in prior_question_elapsed_time. ')\n\ntrain_df['prior_question_elapsed_time'].fillna(mean_prior, inplace = True)\ntrain_df['prior_question_had_explanation'].fillna(False, inplace = True)","84f443a6":"test_df","274bdda5":"test_df['prior_question_had_explanation'].dtype","b8e76277":"questions_df","6c7144e2":"questions_df['tag'] = questions_df['tags'].str.split()","b6261668":"questions_df","cd1fc582":"print(f\"There are {train_df['user_id'].unique().shape[0]} users in training data.\") \nprint(f\"There are {test_df['user_id'].unique().shape[0]} users in testing data.\") \ntotal_users = pd.concat([train_df['user_id'], test_df['user_id']])\nprint(f\"There are {total_users.unique().shape[0]} users in either data frames.\" )","3ef2fdeb":"y = train_df.loc[0:1000000,['answered_correctly']]\nX = train_df.loc[0:1000000,['task_container_id', 'content_type_id','prior_question_elapsed_time', 'prior_question_had_explanation']]","3c306c96":"mini_max_scaler = MinMaxScaler()\nX = mini_max_scaler.fit_transform(X)\n","434c0cb9":"X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X, y, test_size =0.3, shuffle=False)","036362b5":"# X_train_df['prior_question_had_explanation'].isna().sum()","c6a4192d":"X_train_df = X_train_df.astype('float64')\nX_val_df = X_val_df.astype('float64')\ny_train_df = y_train_df.astype('float64')\ny_val_df = y_val_df.astype('float64')","e0bf2c80":"X_train_df","a40503cb":"from keras import Sequential\nfrom keras.layers import Dense\nimport lightgbm as lgbm\n# \u5b9a\u4e49\u6a21\u578b\nmodel=lgbm.LGBMClassifier(num_leaves=60,learning_rate=0.05,n_estimators=40)\nmodel.fit(X_train_df,y_train_df)\ny_pre=model.predict(X_val_df)\ncounty = 0\ncount = 0\ny_val_df = y_val_df.values\nprint(y_val_df)\nfor i in range(len(y_pre)):\n    count+=1\n    if(y_pre[i] == y_val_df[i]):\n        county+=1\nprint(county\/count)","b9846c62":"# model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","e1ebb323":"# nb_epoch = 5\n# model.fit(X_train_df, y_train_df, epochs=nb_epoch, validation_data=(X_val_df, y_val_df), batch_size=16)","df6c8da8":"ex = pd.read_csv('..\/input\/riiid-test-answer-prediction\/example_test.csv')\nex = ex[ex['content_type_id'] == 0]\nex1 = ex[['user_id', 'content_type_id', 'prior_question_elapsed_time', 'prior_question_had_explanation']]\n# \u8fd9\u91cc\u505a\u4f60\u7684\u5904\u7406\nmean_prior_test = ex1['prior_question_elapsed_time'].astype(\"float64\").mean()\nex1['prior_question_elapsed_time'].fillna(mean_prior_test, inplace=True)\nex1['prior_question_had_explanation'].fillna(False, inplace=True)\nex1 = ex1.astype('float64')\nex1 = mini_max_scaler.fit_transform(ex1)\nprint(ex1)","29178d44":"res = model.predict(ex1)\nex['answered_correctly']=res\n# res = sum(res, [])\nress = ex[['row_id', 'answered_correctly']]\nprint(ress)","c87b49d1":"import riiideducation\nenv = riiideducation.make_env()\niter_test = env.iter_test()","d213dff1":"for (sample_test,sample_prediction_df) in iter_test:\n    sample_test = sample_test[sample_test['content_type_id'] == 0]\n    sample_test1 = sample_test[['user_id', 'content_type_id', 'prior_question_elapsed_time', 'prior_question_had_explanation']]\n    # \u8fd9\u91cc\u505a\u4f60\u7684\u5904\u7406\n    mean_prior_test = sample_test1['prior_question_elapsed_time'].astype(\"float64\").mean()\n    sample_test1['prior_question_elapsed_time'].fillna(mean_prior_test, inplace=True)\n    sample_test1['prior_question_had_explanation'].fillna(False, inplace=True)\n    sample_test1 = sample_test1.astype('float64')\n    sample_test1 = mini_max_scaler.fit_transform(sample_test1)\n    \n    res = model.predict(sample_test1)\n    sample_test['answered_correctly']=res\n    \n    env.predict(sample_test[['row_id','answered_correctly']])","905deaf7":"# \u5206\u6790\u6570\u636e-train_df","db8d80a6":"# \u8bfb\u5165\u6570\u636e","ee867b70":"# \u5206\u6790\u6570\u636e-questions_df","72749a8a":"# \u5f15\u5165\u5e93","8f741bd6":"### \u586b\u8865Nan","42512114":"# \u5206\u6790\u6570\u636e-test_df"}}