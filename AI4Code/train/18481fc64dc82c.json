{"cell_type":{"af46ac75":"code","b38f40a8":"code","1e1ff724":"code","c2732450":"code","60fa184d":"code","27ff3539":"code","d058b65a":"code","dfce3f1e":"code","30af75d1":"code","25493b6e":"code","a9daedf9":"code","b22949a1":"code","185c7793":"code","34f9b033":"code","89e2ffc0":"code","f906de62":"code","252c74a4":"code","2fa26179":"code","8ab14f51":"code","1dd7b34e":"code","47f9139a":"code","d9c42eb9":"code","aa5d468d":"code","6ab78701":"code","610b5ccb":"code","a99b1488":"code","1906376c":"code","63dfd147":"code","a03a427e":"code","cb7d0441":"code","bfc63c68":"code","db781756":"code","6dce4b03":"code","bb64afad":"code","6b87a79b":"code","2fd2ee74":"code","de118db5":"code","a6ca4333":"code","530ddfaa":"code","179e4542":"markdown","03521f37":"markdown","eb5fc462":"markdown","535fa910":"markdown","6820c08e":"markdown","487e0443":"markdown","5c0a04fc":"markdown","860038b3":"markdown","2efc8fe2":"markdown","2d9a692f":"markdown","42943745":"markdown"},"source":{"af46ac75":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","b38f40a8":"cancerdata = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")\ncancerdata.head()","1e1ff724":"cancerdata.info()","c2732450":"cancerdata.isnull().sum()","60fa184d":"summary = cancerdata.describe()\nsummary = summary.transpose()\nsummary","27ff3539":"count = cancerdata.diagnosis.value_counts()\ncount","d058b65a":"count.plot.bar()\nplt.title('diagnosis of M and B'),plt.xlabel('diagnosis'),plt.ylabel('count')","dfce3f1e":"cancerdata.drop(cancerdata.columns[[-1, 0]], axis=1, inplace=True)\ncancerdata.info()","30af75d1":"cancerdata_corr = cancerdata.corr()\ncancerdata_corr","25493b6e":"f,ax = plt. subplots(figsize = (10,10))\n#sns.heatmap(cancerdata_corr,  annot = True, cmap =plt.cm.Reds)\nsns.heatmap(cancerdata_corr)","a9daedf9":"featureMeans = list(cancerdata.columns[1:10])","b22949a1":"correlationData = cancerdata[featureMeans].corr()\nsns.pairplot(cancerdata[featureMeans].corr(), diag_kind='kde', size=2);","185c7793":"x = cancerdata.drop(\"diagnosis\", axis = 1)\ny = cancerdata.diagnosis","34f9b033":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, train_size = 0.7)","89e2ffc0":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]\nvif[\"features\"] = x.columns\nprint(vif.round(1))","f906de62":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit_transform(x)","252c74a4":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(x_train,y_train)","2fa26179":"log_pred = log_reg.predict(x_test)","8ab14f51":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report","1dd7b34e":"print(\"confusion_matrix :\")\nprint(confusion_matrix(y_test, log_pred))\nprint(\"classification_report :\")\nprint(classification_report(y_test, log_pred))\nprint(\"acc_score :\")\nprint(accuracy_score(y_test, log_pred))","47f9139a":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\ndtree.fit(x_train, y_train)","d9c42eb9":"dtree_pred = dtree.predict(x_test)","aa5d468d":"print(\"confusion_matrix :\")\nprint(confusion_matrix(y_test, dtree_pred))\nprint(\"classification_report :\")\nprint(classification_report(y_test, dtree_pred))\nprint(\"acc_score :\")\nprint(accuracy_score(y_test, dtree_pred))","6ab78701":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(x_train, y_train)","610b5ccb":"rf_pred = rf.predict(x_test)","a99b1488":"print(\"confusion_matrix :\")\nprint(confusion_matrix(y_test, rf_pred))\nprint(\"classification_report :\")\nprint(classification_report(y_test, rf_pred))\nprint(\"acc_score :\")\nprint(accuracy_score(y_test, rf_pred))","1906376c":"from sklearn.neighbors import KNeighborsClassifier\nkNN = KNeighborsClassifier()\nkNN.fit(x_train, y_train)","63dfd147":"knn_pred = kNN.predict(x_test)","a03a427e":"print(\"confusion_matrix :\")\nprint(confusion_matrix(y_test, knn_pred))\nprint(\"classification_report :\")\nprint(classification_report(y_test, knn_pred))\nprint(\"acc_score :\")\nprint(accuracy_score(y_test, knn_pred))","cb7d0441":"from sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(x_train, y_train)","bfc63c68":"svm_pred = svc.predict(x_test) ","db781756":"print(\"confusion_matrix :\")\nprint(confusion_matrix(y_test, svm_pred))\nprint(\"classification_report :\")\nprint(classification_report(y_test, svm_pred))\nprint(\"acc_score :\")\nprint(accuracy_score(y_test, svm_pred))","6dce4b03":"svc2 = SVC(kernel='linear')\nsvc2.fit(x_train, y_train)","bb64afad":"svm2_pred = svc.predict(x_test)","6b87a79b":"print(\"confusion_matrix :\")\nprint(confusion_matrix(y_test, svm2_pred))\nprint(\"classification_report :\")\nprint(classification_report(y_test, svm2_pred))\nprint(\"acc_score :\")\nprint(accuracy_score(y_test, svm2_pred))","2fd2ee74":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)","de118db5":"nb_pred = nb.predict(x_test)","a6ca4333":"print(\"confusion_matrix :\")\nprint(confusion_matrix(y_test, nb_pred))\nprint(\"classification_report :\")\nprint(classification_report(y_test, nb_pred))\nprint(\"acc_score :\")\nprint(accuracy_score(y_test, nb_pred))","530ddfaa":"print(\"log_regresion_acc_score :\")\nprint(accuracy_score(y_test, log_pred))\nprint(\"Decision_tree_acc_score :\")\nprint(accuracy_score(y_test, dtree_pred))\nprint(\"random_forest_acc_score :\")\nprint(accuracy_score(y_test, rf_pred))\nprint(\"KNN_acc_score :\")\nprint(accuracy_score(y_test, knn_pred))\nprint(\"SVM_acc_score :\")\nprint(accuracy_score(y_test, svm_pred))\nprint(\"SVM2_acc_score :\")\nprint(accuracy_score(y_test, svm2_pred))\nprint(\"NB_acc_score :\")\nprint(accuracy_score(y_test, nb_pred))","179e4542":"# LogisticRegression Model","03521f37":"# SVMClassifier Model","eb5fc462":"# NaiveBayes Model","535fa910":"SVM model2 kernel = \"linear\"","6820c08e":"SVM model kernel = \"rbf\"","487e0443":"Logistic regression assumptions\nThe logistic regression method assumes that:\n\nThe outcome is a binary or dichotomous variable like yes vs no, positive vs negative, 1 vs 0.\nThere is a linear relationship between the logit of the outcome and each predictor variables. Recall that the logit function is logit(p) = log(p\/(1-p)), where p is the probabilities of the outcome (see Chapter @ref(logistic-regression)).\nThere is no influential values (extreme values or outliers) in the continuous predictors\nThere is no high intercorrelations (i.e. multicollinearity) among the predictors.\n\nTo improve the accuracy of your model, you should make sure that these assumptions hold true for your data.","5c0a04fc":"# RandomForestClassifier Model","860038b3":"# Models Comparing","2efc8fe2":"As expected, all features have a high variance inflation factor because they \"explain\" the same variance within this dataset. We would need to discard one of these variables before moving on to model building or risk building a model with high multicolinearity.","2d9a692f":"# kNNClassifier Model","42943745":"# DecisionTree Model"}}