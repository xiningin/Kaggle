{"cell_type":{"226518ed":"code","21c7674c":"code","8bc4bd0a":"code","9b400054":"code","9e49a919":"code","d8f04630":"code","7bbf7a70":"code","0e6d0e4a":"code","e293b02a":"code","9f3995bb":"code","056c9aa7":"code","e2185c5d":"code","a8e60286":"markdown","9f113e92":"markdown","d0d54129":"markdown","217b095f":"markdown","09ca679e":"markdown","daf2211e":"markdown","b44cf07c":"markdown","509b396b":"markdown","05bcead6":"markdown","9003aefb":"markdown","a93a3666":"markdown","5dac6e2f":"markdown","68867113":"markdown","7b7950f1":"markdown"},"source":{"226518ed":"import os\nimport glob\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn.utils import shuffle\nfrom PIL import Image\nimport random\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, BatchNormalization, Conv2D, MaxPooling2D","21c7674c":"d = {}\nwith open('..\/input\/iam-handwriting-top50\/forms_for_parsing.txt') as f:\n    for line in f:\n        key = line.split(' ')[0]\n        writer = line.split(' ')[1]\n        d[key] = writer\nprint(len(d.items()))\nprint(list(d.items())[0:10])","8bc4bd0a":"tmp = []\ntarget_list = []\n\npath_to_files = '..\/input\/iam-handwriting-top50\/data_subset\/data_subset\/*.png'\nfor filename in sorted(glob.glob(path_to_files)):\n    tmp.append(filename)\n    image_name = filename.split('\/')[-1]\n    file, extension = os.path.splitext(image_name)\n    parts = file.split('-')\n    form = parts[0] + '-' + parts[1]\n    for key in d:\n        if key == form:\n            target_list.append(str(d[form]))\n\nimg_files = np.asarray(tmp)\nimg_targets = np.asarray(target_list)\nprint(img_files.shape)\nprint(img_targets.shape)\nprint(img_files[0:5])\nprint(img_targets[0:5])","9b400054":"for filename in img_files[:3]:\n    img=mpimg.imread(filename)\n    plt.figure(figsize=(10,10))\n    plt.imshow(img, cmap ='gray')","9e49a919":"encoder = LabelEncoder()\nencoder.fit(img_targets)\nencoded_Y = encoder.transform(img_targets)\n\nprint(\"Writer ID        : \", np.unique(img_targets))\nprint(\"Encoded writer ID: \", np.unique(encoded_Y))","d8f04630":"#Split dataset into training and test sets\ntrain_files, set_files, train_targets, set_targets = train_test_split(\n        img_files, encoded_Y, train_size=0.8, random_state=52, shuffle= True)\n\n#Further split test set into test and validation sets\nvalidation_files, test_files, validation_targets, test_targets = train_test_split(\n        set_files, set_targets, train_size=0.5, random_state=22, shuffle=True)\n\nprint(train_files.shape, validation_files.shape, test_files.shape)\nprint(train_targets.shape, validation_targets.shape, test_targets.shape)","7bbf7a70":"crop_size = 113\nbatch_size = 16\nnum_classes = 50\n# Start with train generator shared in the class and add image augmentations\ndef generate_data(samples, target_files,  batch_size=batch_size, factor = 0.1 ):\n    num_samples = len(samples)\n    while 1: # Loop forever so the generator never terminates\n        #we generate batches\n        for offset in range(0, num_samples, batch_size):\n            batch_samples = samples[offset:offset+batch_size]\n            batch_targets = target_files[offset:offset+batch_size]\n\n            images = []\n            targets = []\n            #for every image\n            for i in range(len(batch_samples)):\n                batch_sample = batch_samples[i]\n                batch_target = batch_targets[i]\n                im = Image.open(batch_sample)\n                cur_width = im.size[0]\n                cur_height = im.size[1]\n\n                height_fac = crop_size \/ cur_height\n                new_width = int(cur_width * height_fac)\n                size = new_width, crop_size\n\n                imresize = im.resize((size), Image.ANTIALIAS)  # Resize so height = 113 while keeping aspect ratio\n                now_width = imresize.size[0]\n                now_height = imresize.size[1]\n                # Generate crops of size 113x113 from this resized image and keep random 10% of crops\n                #Image.crop((left, top, right, bottom)) top and bottom are always going to be 0 and 113(height)\n                #we need to randomly choose left and right(keeping width=113)\n                #so we can choose from 0 to width -113\n                avail_x_points = list(range(0, now_width - crop_size ))# total x start points are from 0 to width -113\n\n                #10% of all point\n                pick_num = int(len(avail_x_points)*factor)\n\n                # Now pick\n                random_startx = random.sample(avail_x_points,  pick_num)\n\n                for start in random_startx:\n                    imcrop = imresize.crop((start, 0, start+crop_size, crop_size))\n                    images.append(np.asarray(imcrop))\n                    targets.append(batch_target)\n\n            X_train = np.array(images)\n            y_train = np.array(targets)\n\n            #reshape X_train for feeding in later\n            X_train = X_train.reshape(X_train.shape[0], crop_size, crop_size, 1)\n            #convert to float and normalize\n            X_train = X_train.astype('float32')\n            X_train \/= 255\n\n            #One hot encode y\n            y_train = to_categorical(y_train, num_classes)\n            yield shuffle(X_train), shuffle(y_train)","0e6d0e4a":"train_generator = generate_data(train_files, train_targets, batch_size=batch_size, factor = 0.3)\nvalidation_generator = generate_data(validation_files, validation_targets, batch_size=batch_size, factor = 0.3)\ntest_generator = generate_data(test_files, test_targets, batch_size=batch_size, factor = 0.1)","e293b02a":"model = Sequential()\n\nmodel.add(Conv2D(filters= 32, kernel_size =(5,5), strides= (2,2),activation='relu', name='conv1', input_shape=(crop_size,crop_size,1)))\nmodel.add(Conv2D(filters= 64, kernel_size =(3,3), strides= (2,2),activation='relu', name='conv2'))\nmodel.add(Conv2D(filters= 128,kernel_size =(3,3), strides= (2,2),activation='relu', name='conv3'))\nmodel.add(Conv2D(filters= 64, kernel_size =(3,3), strides= (2,2),activation='relu', name='conv4'))\nmodel.add(Conv2D(filters= 32,kernel_size =(3,3), strides= (2,2),activation='relu', name='conv5'))\n\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(512, activation='relu', name='dense1'))  #1024\n# model.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(256, activation='relu', name='dense2'))  #1024\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(num_classes,activation='softmax',name='output'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())","9f3995bb":"print(len(train_files)\/\/batch_size+1)\nprint(len(validation_files)\/\/batch_size+1)","056c9aa7":"'''\n#save every model using Keras checkpoint\nfrom keras.callbacks import ModelCheckpoint\nfilepath=\"checkpoint2\/check-{epoch:02d}-{val_loss:.4f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath= filepath, verbose=1, save_best_only=False)\ncallbacks_list = [checkpoint]\n'''\n#Model fit generator\nhistory_object = model.fit(train_generator, steps_per_epoch= len(train_files)\/\/batch_size+1, epochs=3,\n                           validation_data=validation_generator,\n                           validation_steps= len(validation_files)\/\/batch_size+1)","e2185c5d":"scores = model.evaluate_generator(test_generator,round(len(test_files)\/batch_size)) \nprint(\"Accuracy = \", scores[1])","a8e60286":"# trainning","9f113e92":"# Reading Data","d0d54129":"Encode writers with a value between 0 and n_classes-1:","217b095f":"Writer identification has been applied in anti-crime and historic document analysis fields.\nThis project addresses the problem of automatic writer identification using handwritten images.\n\nThis model is built on handwriting from 50 different writers. This model is language independent as we dont consider the letters or words in particular, but patches of image size 113x113 are extracted and fit into the model for learning.","09ca679e":"# Model","daf2211e":"# Visualization of images","b44cf07c":"# Introduction","509b396b":"Create training, validation, and test generators","05bcead6":"The input to the model are not unique sentences but rather random patches cropped from each sentence by resizing each sentence's height to 113 pixels, and its width such that original aspect ratio is maintained. Finally, from the resized image, patches of 113x113 are randomly cropped.","9003aefb":"create a dictionary which will map each form ID (sentence) to a writer","a93a3666":"# Preparing data","5dac6e2f":"Split dataset into train, validation, and tests sets:","68867113":"# Imports","7b7950f1":"Create arrays of file inputs (a form) and their respective targets (a writer id)(img -> writer)"}}