{"cell_type":{"55966cd2":"code","ecf54c1e":"code","031410d7":"code","a74d3022":"code","6f401dbc":"code","6eb8fcb0":"code","efe9ac83":"code","bcdc2777":"code","67394f9d":"code","2cdb4175":"code","9a767b31":"code","f028cdcc":"code","8945bb5c":"code","b24688bf":"code","afb59d2e":"code","865b0288":"code","4510c4b1":"code","f607ea68":"code","cca715fb":"code","f10454c6":"code","24546489":"code","dc641cad":"code","6cfe1ca8":"code","bc03e055":"code","bb5cecc6":"code","be319f81":"code","1a0fe88f":"code","4e1661d9":"code","61b0b65b":"code","1f173045":"code","0237ccc8":"code","cc6271e6":"code","19b4c4da":"code","0c60eaa7":"code","3a0361da":"code","b22853b8":"code","4ff2d1ed":"code","29643199":"code","603e9748":"code","f2db894d":"code","32b4ed8c":"code","249cdb2e":"code","d9fef0fa":"code","559f0157":"code","20b5f31c":"code","6944899a":"code","82fc343b":"code","1123f935":"code","d4223fb3":"code","0d248062":"code","ba77f1a9":"code","91f05434":"code","18ed564f":"code","d899750c":"code","a86a47da":"code","29cd3099":"markdown","85fb3e45":"markdown","588bc4a7":"markdown","f94492af":"markdown","65ab1c8f":"markdown","0c74f470":"markdown","9e943dd9":"markdown","f0b96d17":"markdown","ee6ef5a1":"markdown","09520e30":"markdown","0edac4dc":"markdown","9d197c09":"markdown","d9eeda2f":"markdown","d19f60c2":"markdown","26927dea":"markdown","06661e1c":"markdown","f8140ee0":"markdown","6e89684e":"markdown","2fb66800":"markdown","55eccc55":"markdown"},"source":{"55966cd2":"import numpy as np \nimport pandas as pd \nimport ast\nfrom tqdm import tqdm\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import normalize\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cat\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nprint(os.listdir(\"..\/input\"))\nrandom_seed = 0\ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","ecf54c1e":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain.index = train['id']\ntest.index = test['id']","031410d7":"print(\"Train : \" + str(train.shape))\nprint(\"Teset : \"+ str(test.shape))","a74d3022":"train.head()","6f401dbc":"print(\"Types columns : \\n\" + str(train.dtypes))","6eb8fcb0":"train.isna().sum().sort_values(ascending=False)","efe9ac83":"missing=train.isna().sum().sort_values(ascending=False)\nsns.barplot(missing[:8],missing[:8].index)\nplt.show()","bcdc2777":"train['belongs_to_collection'][:5]","67394f9d":"train['homepage'][:5]","2cdb4175":"train['tagline'][:5]","9a767b31":"train['Keywords'][:5]","f028cdcc":"train['production_companies'][:5]","8945bb5c":"train['production_countries'][:5]","b24688bf":"train['spoken_languages'][:5]","afb59d2e":"train['crew'][:5]","865b0288":"train['cast'][:5]","4510c4b1":"train['overview'][:5]","f607ea68":"train['genres'][:5]","cca715fb":"train['runtime'][:5]\n","f10454c6":"train['poster_path'][:5]","24546489":"train.loc[train['id'] == 16,'revenue'] = 192864         \ntrain.loc[train['id'] == 90,'budget'] = 30000000                  \ntrain.loc[train['id'] == 118,'budget'] = 60000000       \ntrain.loc[train['id'] == 149,'budget'] = 18000000       \ntrain.loc[train['id'] == 313,'revenue'] = 12000000       \ntrain.loc[train['id'] == 451,'revenue'] = 12000000      \ntrain.loc[train['id'] == 464,'budget'] = 20000000       \ntrain.loc[train['id'] == 470,'budget'] = 13000000       \ntrain.loc[train['id'] == 513,'budget'] = 930000         \ntrain.loc[train['id'] == 797,'budget'] = 8000000        \ntrain.loc[train['id'] == 819,'budget'] = 90000000       \ntrain.loc[train['id'] == 850,'budget'] = 90000000       \ntrain.loc[train['id'] == 1007,'budget'] = 2              \ntrain.loc[train['id'] == 1112,'budget'] = 7500000       \ntrain.loc[train['id'] == 1131,'budget'] = 4300000        \ntrain.loc[train['id'] == 1359,'budget'] = 10000000       \ntrain.loc[train['id'] == 1542,'budget'] = 1             \ntrain.loc[train['id'] == 1570,'budget'] = 15800000       \ntrain.loc[train['id'] == 1571,'budget'] = 4000000        \ntrain.loc[train['id'] == 1714,'budget'] = 46000000       \ntrain.loc[train['id'] == 1721,'budget'] = 17500000       \ntrain.loc[train['id'] == 1865,'revenue'] = 25000000      \ntrain.loc[train['id'] == 1885,'budget'] = 12             \ntrain.loc[train['id'] == 2091,'budget'] = 10             \ntrain.loc[train['id'] == 2268,'budget'] = 17500000       \ntrain.loc[train['id'] == 2491,'budget'] = 6              \ntrain.loc[train['id'] == 2602,'budget'] = 31000000       \ntrain.loc[train['id'] == 2612,'budget'] = 15000000       \ntrain.loc[train['id'] == 2696,'budget'] = 10000000      \ntrain.loc[train['id'] == 2801,'budget'] = 10000000       \ntrain.loc[train['id'] == 335,'budget'] = 2 \ntrain.loc[train['id'] == 348,'budget'] = 12\ntrain.loc[train['id'] == 470,'budget'] = 13000000 \ntrain.loc[train['id'] == 513,'budget'] = 1100000\ntrain.loc[train['id'] == 640,'budget'] = 6 \ntrain.loc[train['id'] == 696,'budget'] = 1\ntrain.loc[train['id'] == 797,'budget'] = 8000000 \ntrain.loc[train['id'] == 850,'budget'] = 1500000\ntrain.loc[train['id'] == 1199,'budget'] = 5 \ntrain.loc[train['id'] == 1282,'budget'] = 9              \ntrain.loc[train['id'] == 1347,'budget'] = 1\ntrain.loc[train['id'] == 1755,'budget'] = 2\ntrain.loc[train['id'] == 1801,'budget'] = 5\ntrain.loc[train['id'] == 1918,'budget'] = 592 \ntrain.loc[train['id'] == 2033,'budget'] = 4\ntrain.loc[train['id'] == 2118,'budget'] = 344 \ntrain.loc[train['id'] == 2252,'budget'] = 130\ntrain.loc[train['id'] == 2256,'budget'] = 1 \ntrain.loc[train['id'] == 2696,'budget'] = 10000000","dc641cad":"test.loc[test['id'] == 3033,'budget'] = 250 \ntest.loc[test['id'] == 3051,'budget'] = 50\ntest.loc[test['id'] == 3084,'budget'] = 337\ntest.loc[test['id'] == 3224,'budget'] = 4  \ntest.loc[test['id'] == 3594,'budget'] = 25  \ntest.loc[test['id'] == 3619,'budget'] = 500  \ntest.loc[test['id'] == 3831,'budget'] = 3  \ntest.loc[test['id'] == 3935,'budget'] = 500  \ntest.loc[test['id'] == 4049,'budget'] = 995946 \ntest.loc[test['id'] == 4424,'budget'] = 3  \ntest.loc[test['id'] == 4460,'budget'] = 8  \ntest.loc[test['id'] == 4555,'budget'] = 1200000 \ntest.loc[test['id'] == 4624,'budget'] = 30 \ntest.loc[test['id'] == 4645,'budget'] = 500 \ntest.loc[test['id'] == 4709,'budget'] = 450 \ntest.loc[test['id'] == 4839,'budget'] = 7\ntest.loc[test['id'] == 3125,'budget'] = 25 \ntest.loc[test['id'] == 3142,'budget'] = 1\ntest.loc[test['id'] == 3201,'budget'] = 450\ntest.loc[test['id'] == 3222,'budget'] = 6\ntest.loc[test['id'] == 3545,'budget'] = 38\ntest.loc[test['id'] == 3670,'budget'] = 18\ntest.loc[test['id'] == 3792,'budget'] = 19\ntest.loc[test['id'] == 3881,'budget'] = 7\ntest.loc[test['id'] == 3969,'budget'] = 400\ntest.loc[test['id'] == 4196,'budget'] = 6\ntest.loc[test['id'] == 4221,'budget'] = 11\ntest.loc[test['id'] == 4222,'budget'] = 500\ntest.loc[test['id'] == 4285,'budget'] = 11\ntest.loc[test['id'] == 4319,'budget'] = 1\ntest.loc[test['id'] == 4639,'budget'] = 10\ntest.loc[test['id'] == 4719,'budget'] = 45\ntest.loc[test['id'] == 4822,'budget'] = 22\ntest.loc[test['id'] == 4829,'budget'] = 20\ntest.loc[test['id'] == 4969,'budget'] = 20\ntest.loc[test['id'] == 5021,'budget'] = 40 \ntest.loc[test['id'] == 5035,'budget'] = 1 \ntest.loc[test['id'] == 5063,'budget'] = 14 \ntest.loc[test['id'] == 5119,'budget'] = 2 \ntest.loc[test['id'] == 5214,'budget'] = 30 \ntest.loc[test['id'] == 5221,'budget'] = 50 \ntest.loc[test['id'] == 4903,'budget'] = 15\ntest.loc[test['id'] == 4983,'budget'] = 3\ntest.loc[test['id'] == 5102,'budget'] = 28\ntest.loc[test['id'] == 5217,'budget'] = 75\ntest.loc[test['id'] == 5224,'budget'] = 3 \ntest.loc[test['id'] == 5469,'budget'] = 20 \ntest.loc[test['id'] == 5840,'budget'] = 1 \ntest.loc[test['id'] == 5960,'budget'] = 30\ntest.loc[test['id'] == 6506,'budget'] = 11 \ntest.loc[test['id'] == 6553,'budget'] = 280\ntest.loc[test['id'] == 6561,'budget'] = 7\ntest.loc[test['id'] == 6582,'budget'] = 218\ntest.loc[test['id'] == 6638,'budget'] = 5\ntest.loc[test['id'] == 6749,'budget'] = 8 \ntest.loc[test['id'] == 6759,'budget'] = 50 \ntest.loc[test['id'] == 6856,'budget'] = 10\ntest.loc[test['id'] == 6858,'budget'] =  100\ntest.loc[test['id'] == 6876,'budget'] =  250\ntest.loc[test['id'] == 6972,'budget'] = 1\ntest.loc[test['id'] == 7079,'budget'] = 8000000\ntest.loc[test['id'] == 7150,'budget'] = 118\ntest.loc[test['id'] == 6506,'budget'] = 118\ntest.loc[test['id'] == 7225,'budget'] = 6\ntest.loc[test['id'] == 7231,'budget'] = 85\ntest.loc[test['id'] == 5222,'budget'] = 5\ntest.loc[test['id'] == 5322,'budget'] = 90\ntest.loc[test['id'] == 5350,'budget'] = 70\ntest.loc[test['id'] == 5378,'budget'] = 10\ntest.loc[test['id'] == 5545,'budget'] = 80\ntest.loc[test['id'] == 5810,'budget'] = 8\ntest.loc[test['id'] == 5926,'budget'] = 300\ntest.loc[test['id'] == 5927,'budget'] = 4\ntest.loc[test['id'] == 5986,'budget'] = 1\ntest.loc[test['id'] == 6053,'budget'] = 20\ntest.loc[test['id'] == 6104,'budget'] = 1\ntest.loc[test['id'] == 6130,'budget'] = 30\ntest.loc[test['id'] == 6301,'budget'] = 150\ntest.loc[test['id'] == 6276,'budget'] = 100\ntest.loc[test['id'] == 6473,'budget'] = 100\ntest.loc[test['id'] == 6842,'budget'] = 30","6cfe1ca8":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","bc03e055":"train.head()","bb5cecc6":"test.head()","be319f81":"train.describe()","1a0fe88f":"train.columns.values","4e1661d9":"#Transform Text to Dictionary. Because these columns are in json format.\nimport ast\ndict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\ndef text_to_dict(df):\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n    return df\n        \ndfx = text_to_dict(train)\nfor col in dict_columns:\n    train[col]=dfx[col]","61b0b65b":"train.head(5)","1f173045":"dfx = text_to_dict(test)\nfor col in dict_columns:\n       test[col]=dfx[col]   ","0237ccc8":"def date(x):\n    x=str(x)\n    year=x.split('\/')[2]\n    if int(year)<19:\n        return x[:-2]+'20'+year\n    else:\n        return x[:-2]+'19'+year\n    \ntrain['release_date'] =train['release_date'].fillna('1\/1\/90').apply(lambda x: date(x))\ntest['release_date']  =test['release_date'].fillna('1\/1\/90').apply(lambda x: date(x))\n\n#from datetime import datetime\ntrain['release_date'] = train['release_date'].apply(lambda x: datetime.strptime(x,'%m\/%d\/%Y'))\ntest['release_date']  = test['release_date'].apply(lambda x: datetime.strptime(x,'%m\/%d\/%Y'))\n\ntrain['release_day']   = train['release_date'].apply(lambda x:x.weekday())\ntrain['release_month'] = train['release_date'].apply(lambda x:x.month)\ntrain['release_year']  = train['release_date'].apply(lambda x:x.year)\n\ntest['release_day']   = test['release_date'].apply(lambda x:x.weekday())\ntest['release_month'] = test['release_date'].apply(lambda x:x.month)\ntest['release_year']  = test['release_date'].apply(lambda x:x.year)\n\nday=train['release_day'].value_counts().sort_index()\nsns.barplot(day.index,day)\nplt.gca().set_xticklabels([\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"],rotation='45')\nplt.ylabel('No of releases')","cc6271e6":"def data_preprocessing(df):\n    df['belongs_to_collection']= df['belongs_to_collection'].apply(lambda x: 1 if x != {} else 0)\n    df['has_homepage'] = 1\n    df.loc[ pd.isnull(df['homepage']),'has_homepage'] = 0\n    \n    df['num_genres'] = df['genres'].apply(lambda x: len(x) if x != {} else 0)\n\n    df['budget'] = np.log1p(df['budget'])\n    df['isTaglineNA'] = df['tagline'].apply( lambda x: 0 if pd.isnull(x) else 1)\n    df['original_language'] = le.fit_transform(df['original_language'])\n    \n    \n    df['isOriginalLanguageEng'] = 0 \n    df.loc[ df['original_language'].astype(str) == \"en\" ,\"isOriginalLanguageEng\"] = 1\n    \n    df['ismovie_released']=1\n    df.loc[(df['status']!='Released'),'ismovie_released']=0\n    \n    df['no_spoken_languages']=df['spoken_languages'].apply(lambda x: len(x))\n    df['original_title_letter_count'] = df['original_title'].str.len() \n    df['original_title_word_count'] = df['original_title'].str.split().str.len() \n\n\n    df['title_word_count'] = df['title'].str.split().str.len()\n    df['overview_word_count'] = df['overview'].str.split().str.len()\n    df['tagline_word_count'] = df['tagline'].str.split().str.len()\n    \n    \n    df['production_countries_count'] = df['production_countries'].apply(lambda x : len(x))\n    df['production_companies_count'] = df['production_companies'].apply(lambda x : len(x))\n    df['cast_count'] = df['cast'].apply(lambda x : len(x))\n    df['crew_count'] = df['crew'].apply(lambda x : len(x))\n    \n    \n    #Gender\n    df['genders_0_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n    df['genders_1_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n    df['genders_2_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n    \n    \n    cols_to_normalize=['runtime','popularity','budget','_budget_runtime_ratio','_budget_year_ratio','_budget_popularity_ratio','_releaseYear_popularity_ratio',\n    '_releaseYear_popularity_ratio2','_num_Keywords','_num_cast','no_spoken_languages','original_title_letter_count','original_title_word_count',\n    'title_word_count','overview_word_count','tagline_word_count','production_countries_count','production_companies_count','cast_count','crew_count',\n    'genders_0_crew','genders_1_crew','genders_2_crew']\n    for col in cols_to_normalize:\n        if (col in df):\n            print(col)\n            x_array=[]\n            x_array=np.array(df[col].fillna(0))\n            X_norm=normalize([x_array])[0]\n            df[col]=X_norm\n        \n    \n    df = df.drop(['homepage','tagline','poster_path','overview','genres',\n                  'imdb_id','original_title','cast','Keywords','title',\n                 'production_companies','production_countries',\n                  'release_date','spoken_languages','crew','status'],axis=1)\n    \n    df.fillna(-1,inplace=True)\n    return df","19b4c4da":"train.head()","0c60eaa7":"'''\n#ref: https:\/\/www.kaggle.com\/zero92\/tmdb-prediction with some minors changes\ndef  data_preprocessing2(df):\n    df['_budget_runtime_ratio'] = (df['budget']\/df['runtime']).replace([np.inf,-np.inf,np.nan],0)\n    df['_budget_popularity_ratio'] = df['budget']\/df['popularity']\n    df['_budget_year_ratio'] = df['budget'].fillna(0)\/(df['release_year']*df['release_year'])\n    df['_releaseYear_popularity_ratio'] = df['release_year']\/df['popularity']\n    df['_releaseYear_popularity_ratio2'] = df['popularity']\/df['release_year']\n    df['budget']=np.log1p(df['budget'])\n    \n    df['collection_name']=df['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else 0)\n    df['has_homepage'] = 0\n    df.loc[(pd.isnull(df['homepage'])),'has_homepage'] = 1\n    \n    le = LabelEncoder()\n    le.fit(list(df['collection_name'].fillna('')))\n    df['collection_name']=le.transform(df['collection_name'].fillna('').astype(str))\n    \n    le = LabelEncoder()\n    le.fit(list(df['original_language'].fillna('')))\n    df['original_language']=le.transform(df['original_language'].fillna('').astype(str))\n    \n    df['_num_Keywords'] = df['Keywords'].apply(lambda x: len(x) if x != {} else 0)\n    df['_num_cast'] = df['cast'].apply(lambda x: len(x) if x != {} else 0)\n    \n    df['isbelongto_coll']=0\n    df.loc[pd.isna(df['belongs_to_collection']),'isbelongto_coll']=1\n    \n    df['isTaglineNA'] = 0\n    df.loc[df['tagline'] == 0 ,\"isTaglineNA\"] = 1 \n\n    df['isOriginalLanguageEng'] = 0 \n    df.loc[ df['original_language'].astype(str) == \"en\" ,\"isOriginalLanguageEng\"] = 1\n    \n    df['ismovie_released']=1\n    df.loc[(df['status']!='Released'),'ismovie_released']=0\n    \n    df['no_spoken_languages']=df['spoken_languages'].apply(lambda x: len(x))\n    df['original_title_letter_count'] = df['original_title'].str.len() \n    df['original_title_word_count'] = df['original_title'].str.split().str.len() \n\n\n    df['title_word_count'] = df['title'].str.split().str.len()\n    df['overview_word_count'] = df['overview'].str.split().str.len()\n    df['tagline_word_count'] = df['tagline'].str.split().str.len()\n    \n    \n    df['collection_id'] = df['belongs_to_collection'].apply(lambda x : np.nan if len(x)==0 else x[0]['id'])\n    df['production_countries_count'] = df['production_countries'].apply(lambda x : len(x))\n    df['production_companies_count'] = df['production_companies'].apply(lambda x : len(x))\n    df['cast_count'] = df['cast'].apply(lambda x : len(x))\n    df['crew_count'] = df['crew'].apply(lambda x : len(x))\n\n    df['genders_0_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n    df['genders_1_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n    df['genders_2_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n\n    for col in  ['genres', 'production_countries', 'spoken_languages', 'production_companies'] :\n        df[col] = df[col].map(lambda x: sorted(list(set([n if n in train_dict[col] else col+'_etc' for n in [d['name'] for d in x]])))).map(lambda x: ','.join(map(str, x)))\n        temp = df[col].str.get_dummies(sep=',')\n        df = pd.concat([df, temp], axis=1, sort=False)\n  \n    \n    cols_to_normalize=['runtime','popularity','budget','_budget_runtime_ratio','_budget_year_ratio','_budget_popularity_ratio','_releaseYear_popularity_ratio',\n    '_releaseYear_popularity_ratio2','_num_Keywords','_num_cast','no_spoken_languages','original_title_letter_count','original_title_word_count',\n    'title_word_count','overview_word_count','tagline_word_count','production_countries_count','production_companies_count','cast_count','crew_count',\n    'genders_0_crew','genders_1_crew','genders_2_crew']\n    for col in cols_to_normalize:\n        print(col)\n        x_array=[]\n        x_array=np.array(df[col].fillna(0))\n        X_norm=normalize([x_array])[0]\n        df[col]=X_norm\n    \n    df = df.drop(['belongs_to_collection','genres','homepage','imdb_id','overview','id'\n    ,'poster_path','production_companies','production_countries','release_date','spoken_languages'\n    ,'status','title','Keywords','cast','crew','original_language','original_title','tagline', 'collection_id'\n    ],axis=1)\n    \n    df.fillna(value=0.0, inplace = True) \n\n    return df\n'''","3a0361da":"test['revenue'] = np.nan\nall_data=data_preprocessing((pd.concat([train,test]))).reset_index(drop=True)","b22853b8":"all_data.head(10)","4ff2d1ed":"train=all_data.loc[:train.shape[0]-1,:]\ntest=all_data.loc[train.shape[0]:,:]\nprint(\"Train shape\",train.shape)\nprint(\"Test shape\",train.shape)","29643199":"train.info()","603e9748":"features = list(train.columns)\nfeatures =  [i for i in features if i != 'id' and i != 'revenue']","f2db894d":"from sklearn.metrics import mean_squared_error\ndef score(data, y):\n    validation_res = pd.DataFrame(\n    {\"id\": data[\"id\"].values,\n     \"transactionrevenue\": data[\"revenue\"].values,\n     \"predictedrevenue\": np.expm1(y)})\n\n    validation_res = validation_res.groupby(\"id\")[\"transactionrevenue\", \"predictedrevenue\"].sum().reset_index()\n    return np.sqrt(mean_squared_error(np.log1p(validation_res[\"transactionrevenue\"].values), \n                                     np.log1p(validation_res[\"predictedrevenue\"].values)))","32b4ed8c":"from sklearn.model_selection import GroupKFold\n\nclass KFoldValidation():\n    def __init__(self, data, n_splits=5):\n        unique_vis = np.array(sorted(data['id'].astype(str).unique()))\n        folds = GroupKFold(n_splits)\n        ids = np.arange(data.shape[0])\n        \n        self.fold_ids = []\n        for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n            self.fold_ids.append([\n                    ids[data['id'].astype(str).isin(unique_vis[trn_vis])],\n                    ids[data['id'].astype(str).isin(unique_vis[val_vis])]\n                ])\n            \n    def validate(self, train, test, features, model, name=\"\", prepare_stacking=False, \n                 fit_params={\"early_stopping_rounds\": 500, \"verbose\": 100, \"eval_metric\": \"rmse\"}):\n        model.FI = pd.DataFrame(index=features)\n        full_score = 0\n        \n        if prepare_stacking:\n            test[name] = 0\n            train[name] = np.NaN\n        \n        for fold_id, (trn, val) in enumerate(self.fold_ids):\n            devel = train[features].iloc[trn]\n            y_devel = np.log1p(train[\"revenue\"].iloc[trn])\n            valid = train[features].iloc[val]\n            y_valid = np.log1p(train[\"revenue\"].iloc[val])\n                       \n            print(\"Fold \", fold_id, \":\")\n            model.fit(devel, y_devel, eval_set=[(valid, y_valid)], **fit_params)\n            \n            if len(model.feature_importances_) == len(features):  \n                model.FI['fold' + str(fold_id)] = model.feature_importances_ \/ model.feature_importances_.sum()\n\n            predictions = model.predict(valid)\n            predictions[predictions < 0] = 0\n            print(\"Fold \", fold_id, \" error: \", mean_squared_error(y_valid, predictions)**0.5)\n            \n            fold_score = score(train.iloc[val], predictions)\n            full_score += fold_score \/ len(self.fold_ids)\n            print(\"Fold \", fold_id, \" score: \", fold_score)\n            if prepare_stacking:\n                train[name].iloc[val] = predictions\n                \n                test_predictions = model.predict(test[features])\n                test_predictions[test_predictions < 0] = 0\n                test[name] += test_predictions \/ len(self.fold_ids)\n                \n        print(\"Final score: \", full_score)\n        return full_score","249cdb2e":"Kfolder = KFoldValidation(train)","d9fef0fa":"lgbmodel = lgb.LGBMRegressor(n_estimators=10000, \n                             objective='regression', \n                             metric='rmse',\n                             max_depth = 5,\n                             num_leaves=30, \n                             min_child_samples=100,\n                             learning_rate=0.01,\n                             boosting = 'gbdt',\n                             min_data_in_leaf= 10,\n                             feature_fraction = 0.9,\n                             bagging_freq = 1,\n                             bagging_fraction = 0.9,\n                             importance_type='gain',\n                             lambda_l1 = 0.2,\n                             bagging_seed=random_seed, \n                             subsample=.8, \n                             colsample_bytree=.9,\n                             use_best_model=True)","559f0157":"Kfolder.validate(train, test, features , lgbmodel, name=\"lgbfinal\", prepare_stacking=True) ","20b5f31c":"#lgbmodel.FI.mean(axis=1).sort_values()[180:250].plot(kind=\"barh\",title = \"Features Importance\", figsize = (10,10))","6944899a":"xgbmodel = xgb.XGBRegressor(max_depth=5, \n                            learning_rate=0.01, \n                            n_estimators=10000, \n                            objective='reg:linear', \n                            gamma=1.45, \n                            seed=random_seed, \n                            silent=True,\n                            subsample=0.8, \n                            colsample_bytree=0.7, \n                            colsample_bylevel=0.5)","82fc343b":"Kfolder.validate(train, test, features, xgbmodel, name=\"xgbfinal\", prepare_stacking=True)","1123f935":"catmodel = cat.CatBoostRegressor(iterations=10000, \n                                 learning_rate=0.01, \n                                 depth=5, \n                                 eval_metric='RMSE',\n                                 colsample_bylevel=0.8,\n                                 bagging_temperature = 0.2,\n                                 metric_period = None,\n                                 early_stopping_rounds=200,\n                                 random_seed=random_seed)","d4223fb3":"Kfolder.validate(train, test, features , catmodel, name=\"catfinal\", prepare_stacking=True,\n               fit_params={\"use_best_model\": True, \"verbose\": 100})","0d248062":"train['Revenue_lgb'] = train[\"lgbfinal\"]\nprint(\"RMSE model lgb :\" ,score(train, train.Revenue_lgb),)\n\ntrain['Revenue_xgb'] = train[\"xgbfinal\"]\nprint(\"RMSE model xgb :\" ,score(train, train.Revenue_xgb))\n\ntrain['Revenue_cat'] = train[\"catfinal\"]\nprint(\"RMSE model cat :\" ,score(train, train.Revenue_cat))\n\ntrain['Revenue_Dragon1'] = 0.4 * train[\"lgbfinal\"] + \\\n                               0.2 * train[\"xgbfinal\"] + \\\n                               0.4 * train[\"catfinal\"]\n\nprint(\"RMSE model Dragon1 :\" ,score(train, train.Revenue_Dragon1))\ntrain['Revenue_Dragon2'] = 0.35 * train[\"lgbfinal\"] + \\\n                               0.3 * train[\"xgbfinal\"] + \\\n                               0.35 * train[\"catfinal\"]\n\nprint(\"RMSE model Dragon2 :\" ,score(train, train.Revenue_Dragon2))","ba77f1a9":"test['revenue'] =  np.expm1(test[\"lgbfinal\"])\ntest[['id','revenue']].to_csv('submission_lgb.csv', index=False)\ntest[['id','revenue']].head()","91f05434":"test['revenue'] =  np.expm1(test[\"xgbfinal\"])\ntest[['id','revenue']].to_csv('submission_xgb.csv', index=False)\ntest[['id','revenue']].head()","18ed564f":"test['revenue'] =  np.expm1(test[\"catfinal\"])\ntest[['id','revenue']].to_csv('submission_cat.csv', index=False)\ntest[['id','revenue']].head()","d899750c":"test['revenue'] =  np.expm1(0.4 * test[\"lgbfinal\"]+ 0.4 * test[\"catfinal\"] + 0.2 * test[\"xgbfinal\"])\ntest[['id','revenue']].to_csv('submission_Dragon1.csv', index=False)\ntest[['id','revenue']].head()","a86a47da":"test['revenue'] =  np.expm1((test[\"lgbfinal\"] + test[\"catfinal\"] + test[\"xgbfinal\"])\/3)\ntest[['id','revenue']].to_csv('submission_Dragon2.csv', index=False)\ntest[['id','revenue']].head()","29cd3099":"**CAT** :","85fb3e45":"**Dragon1** :","588bc4a7":"<a id=\"id4\"><\/a> <br> \n# **4. Data Pre-processing**  ","f94492af":"# TMDB Box Office Prediction\n\n<h3> Simple, Without External Files, EDA<\/h3>\n","65ab1c8f":"**LGB** :","0c74f470":"<a id=\"ref\"><\/a> <br> \n# **8. References** \n\nThis Kernel is only possible because theses kernels:<br>\n\n- https:\/\/www.kaggle.com\/adrianoavelar\/template-analisys-en\n- https:\/\/www.kaggle.com\/kamalchhirang\/eda-feature-engineering-lgb-xgb-cat\n- https:\/\/www.kaggle.com\/praxitelisk\/tmdb-box-office-prediction-eda-ml\n- https:\/\/www.kaggle.com\/shahules\/eda-feature-engineering-and-keras-model\n\n# Thank you for upvoted if it was useful for you\n","9e943dd9":"**XGB** :","f0b96d17":"**Dragon2** :","ee6ef5a1":"It is clear from the graph below that revenue depends a lot on budget and total_vote","09520e30":"<a id=\"id3\"><\/a> <br> \n# **3. Load the Dataset** \n<h3>3.1 Heads of the data<\/h3><br>","0edac4dc":"<a id=\"id2\"><\/a> <br> \n# **2. Get the Data (Collect \/ Obtain):** \n","9d197c09":"<a id=\"id5\"><\/a> <br> \n# **5. Model** ","d9eeda2f":"<a id=\"id6\"><\/a> <br> \n# **6. Visualization and Analysis of Results** ","d19f60c2":"## Ok, now we're good to go","26927dea":"\n\n# Table of Contents:\n\n**1. [Problem Definition](#id1)** <br>\n**2. [Get the Data (Collect \/ Obtain)](#id2)** <br>\n**3. [Load the Dataset](#id3)** <br>\n**4. [Data Pre-processing](#id4)** <br>\n**5. [Model](#id5)** <br>\n**6. [Visualization and Analysis of Results](#id6)** <br>\n**7. [Submittion](#id7)** <br>\n**8. [References](#ref)** <br>","06661e1c":"Let's investigate all NaN Values:\n- belongs_to_collection    2396\n- homepage                 2054\n- tagline                   597\n- Keywords                  276\n- production_companies      156\n- production_countries       55\n- spoken_languages           20\n- crew                       16\n- cast                       13\n- overview                    8\n- genres                      7\n- runtime                     2\n- poster_path                 1\n","f8140ee0":"\n<img src=\"http:\/\/wyeside.co.uk\/event-assets\/images\/avengers-endgame-20190401025200-tmdb5jqncbp8.jpg\" width=\"800\"><\/img>\n\n<br>","6e89684e":"## Cheking for missing or null values","2fb66800":"<a id=\"id1\"><\/a> <br> \n# **1. Problem Definition:** \n\nIn a world... where movies made an estimated $41.7 billion in 2018, the film industry is more popular than ever. But what movies make the most money at the box office? How much does a director matter? Or the budget? For some movies, it's \"You had me at 'Hello.'\" For others, the trailer falls short of expectations and you think \"What we have here is a failure to communicate.\"\n\nIn this competition, you're presented with metadata on over 7,000 past films from The Movie Database to try and predict their overall worldwide box office revenue. Data points provided include cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries. You can collect other publicly available data to use in your model predictions, but in the spirit of this competition, use only data that would have been available before a movie's release.","55eccc55":"<a id=\"id7\"><\/a> <br> \n# **7. Submittion** "}}