{"cell_type":{"02e33c42":"code","57b571dc":"code","f9f90d48":"code","cb108a87":"code","af5ad49b":"code","69b279fe":"code","456d1c9a":"code","4cdce3af":"code","971806b3":"code","ae44c411":"code","24df626d":"code","c3587608":"code","68c8245d":"code","54d1d4c8":"code","ac2054f3":"code","b7937eef":"code","9aff22df":"code","11f3e7ce":"code","e28e465f":"code","82aa1a34":"code","1aa1513f":"code","475c627d":"code","deaed689":"code","f03b3970":"code","b832ddd8":"code","f0e5406f":"markdown","b84190c8":"markdown","d5de36fe":"markdown","637049a6":"markdown","19d8280a":"markdown","fe992c5d":"markdown","c4fff49c":"markdown"},"source":{"02e33c42":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n \nfrom tqdm import tqdm\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n%matplotlib inline\nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid', color_codes=True)\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D, MaxPool2D, BatchNormalization, GlobalAveragePooling2D\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n\nimport random\nimport uuid\nimport shutil\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","57b571dc":"random.seed(42)\nnp.random.seed(42)","f9f90d48":"PATH = \"..\/input\"\nIMAGE_SIZE = (32, 32)\n\ntrain_dir=f'{PATH}\/train\/train'\ntest_dir=f'{PATH}\/test\/test'\n\ntrain_img = os.listdir(train_dir)\ntest_img = os.listdir(test_dir)\n\ntrain_df =pd.read_csv(f'{PATH}\/train.csv')\ntest_df = pd.read_csv(f'{PATH}\/sample_submission.csv')\n\n# Create new images for the imbalanced data\ntrain_new_dir = 'train'\n\n# Make sure the folder is empty\nif os.path.exists(train_new_dir):\n    shutil.rmtree(train_new_dir)\nelse:\n    os.makedirs(train_new_dir)\n\nprint(f\"The number of rows in train and test set are {len(train_df)} and {len(test_df)}\")","cb108a87":"train_df.head()","af5ad49b":"test_df.head()","69b279fe":"sns.countplot(train_df['has_cactus'])","456d1c9a":"fig = plt.figure(figsize=(10, 8))\nfor idx, img in enumerate(np.random.choice(train_img, 20)):\n    ax = fig.add_subplot(4, 20\/\/4, idx+1, xticks=[], yticks=[])\n    im = cv2.imread(f'{train_dir}\/{img}')\n    plt.imshow(im)\n    lab = train_df.loc[train_df['id'] == img, 'has_cactus'].values[0]\n    ax.set_title(f'has_cactus: {lab}')","4cdce3af":"train_new = []\nif len(os.listdir(train_new_dir))==0:\n    for idx, row in train_df[train_df['has_cactus']==0].iterrows():\n        # get image\n        img = cv2.imread(f'{train_dir}\/{row[\"id\"]}')\n        # flip image\n        for i in range(2):\n            f = cv2.flip(img, i)\n            img_id = uuid.uuid4().hex + '.jpg'\n            cv2.imwrite(f'{train_new_dir}\/{img_id}', f)\n            train_new.append({'id': img_id, 'has_cactus': 0}) ","971806b3":"print(f'{len(os.listdir(train_new_dir))} new images generated')","ae44c411":"# copy the read-only files to new dir\n!cp ..\/input\/train\/train\/*.jpg train\/","24df626d":"print(f\"The number of rows in train and test set are {len(os.listdir(train_new_dir))} and {len(test_df)}\")","c3587608":"if len(train_df) == len(os.listdir(train_dir)):\n    train_df=train_df.append(train_new, ignore_index=True)\n    sns.countplot(train_df['has_cactus'])","68c8245d":"# splitting data into train and validation\ntrain, valid = train_test_split(train_df, stratify=train_df.has_cactus, test_size=0.33, random_state=2019)","54d1d4c8":"BATCH_SIZE = 8\n\ntrain_gen=ImageDataGenerator(\n    rescale=1.\/255, \n    rotation_range=10,  \n    zoom_range = 0.1, \n    width_shift_range=0.1,  \n    height_shift_range=0.1,  \n    fill_mode='nearest'\n)  \n\ntrain_generator=train_gen.flow_from_dataframe(\n    x_col='id',                                  \n    y_col='has_cactus',\n    dataframe=train, \n    directory=train_new_dir, \n    class_mode='other',\n    color_mode='rgb',\n    batch_size=BATCH_SIZE,\n    target_size=IMAGE_SIZE,\n    shuffle=True,\n    seed=2019\n)\n\nvalid_generator=train_gen.flow_from_dataframe(\n    x_col='id',                                  \n    y_col='has_cactus',\n    dataframe=valid, \n    directory=train_new_dir, \n    class_mode='other',\n    color_mode='rgb',\n    batch_size=BATCH_SIZE,\n    target_size=IMAGE_SIZE,\n    shuffle=True,\n    seed=2019\n)","ac2054f3":"drop_rate=0.2\n\nmodel = Sequential()\n\nmodel.add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(32,32,3)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32,kernel_size=3,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(drop_rate))\n\nmodel.add(Conv2D(64,kernel_size=3,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64,kernel_size=3,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(drop_rate))\n\nmodel.add(GlobalAveragePooling2D())\nmodel.add(BatchNormalization())\nmodel.add(Dropout(drop_rate))\n\nmodel.add(Dense(1,activation='sigmoid'))","b7937eef":"model.summary()","9aff22df":"model.compile(loss='binary_crossentropy',optimizer=Adam(), metrics=['accuracy'])","11f3e7ce":"file_path = 'best_weights.h5'\n\ncallbacks = [\n    ModelCheckpoint(file_path, monitor = 'val_acc', verbose = 1, save_best_only = True, mode = 'max'),\n    ReduceLROnPlateau(monitor = 'val_loss', factor = 0.8, patience = 4, verbose = 1, mode = 'min', min_lr = 1e-8),\n    EarlyStopping(monitor = 'val_loss', min_delta = 1e-10, patience = 32, verbose = 1, restore_best_weights = True)\n]","e28e465f":"epochs=128\nhistory=model.fit_generator(train_generator,\n                            steps_per_epoch=train_generator.n\/\/train_generator.batch_size,\n                            epochs=epochs,\n                            verbose = 1,\n                            shuffle=True,\n                            validation_data=valid_generator,\n                            validation_steps=valid_generator.n\/\/valid_generator.batch_size,\n                            callbacks = callbacks)","82aa1a34":"history_df = pd.DataFrame(history.history)\nhistory_df[['loss', 'val_loss']].plot()\nhistory_df[['acc', 'val_acc']].plot()","1aa1513f":"# Make sure not output too many files\nif os.path.exists(train_new_dir):\n    shutil.rmtree(train_new_dir)","475c627d":"model.load_weights(file_path)","deaed689":"test_gen=ImageDataGenerator(\n    rescale=1.\/255, \n)  \n\ntest_generator=test_gen.flow_from_dataframe(\n    x_col='id',                                  \n    y_col='has_cactus',\n    dataframe=test_df, \n    directory=test_dir, \n    class_mode='other',\n    color_mode='rgb',\n    batch_size=1,\n    target_size=IMAGE_SIZE,\n    shuffle=False\n)","f03b3970":"test_generator.reset()\npred = model.predict_generator(test_generator, verbose=1, steps=test_generator.n)\n\npred[pred>0.99]=1\npred[pred<0.01]=0","b832ddd8":"test_df['has_cactus'] = pred\ntest_df.to_csv('submission.csv', index = False)","f0e5406f":"## Virtualize Training","b84190c8":"## Fit Model","d5de36fe":"## Define Model","637049a6":"## Investigate Data","19d8280a":"## Predict","fe992c5d":"## Add images","c4fff49c":"## Prepare Data"}}