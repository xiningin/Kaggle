{"cell_type":{"ced1dc4d":"code","2c4a79d8":"code","5b7eacbc":"code","1fe4e3de":"code","d0cd2354":"code","972e23c5":"code","f2952754":"code","94fa335a":"code","a802d4ad":"code","3c5e6664":"code","bbf3a820":"code","f1151f65":"code","45d2728d":"code","9d4c3d9a":"code","643d24ac":"code","5ee34a11":"code","85d1c4d9":"code","30604e74":"markdown","0941f6f6":"markdown","de165cb2":"markdown","89c2a50c":"markdown","bd8ac263":"markdown","cfd15f41":"markdown","a08dfd51":"markdown","58139cb5":"markdown","ea9c5688":"markdown","22277c01":"markdown","31e0a0ba":"markdown","6b57f6a0":"markdown","7d8c88f0":"markdown","14f32060":"markdown"},"source":{"ced1dc4d":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score,KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.tree import _tree\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')","2c4a79d8":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nprint(\"Train Shape: {}\".format(train.shape))\nprint(\"Test Shape: {}\".format(test.shape))","5b7eacbc":"test = pd.read_csv('..\/input\/test.csv')\nprint(\"Test Shape: {}\".format(test.shape))","1fe4e3de":"train[\"LogSalePrice\"] = np.log1p(train['SalePrice'])\n\nprint(\"Mean(std) of Sale Price: {0:.0f}({1:.0f})\".format(train[\"SalePrice\"].mean(), train[\"SalePrice\"].std()))\nprint(\"Mean(std) of Log Sale Price: {:.2f}({:.2f})\".format(train[\"LogSalePrice\"].mean(), train[\"LogSalePrice\"].std()))\n\nf, axes = plt.subplots(1, 2, figsize=(12,4))\nax1 = sns.distplot(train.SalePrice, ax=axes[0])\nax2 = sns.distplot(train.LogSalePrice, ax=axes[1])\nax1.set(xlabel='Sale Price', ylabel='Proportion', title='Sale Price')\nax2.set(xlabel='Log Sale Price', ylabel='Proportion', title='Log Sale Price')\nplt.show()","d0cd2354":"print(\"Numerical Variables:\")\ntrain.select_dtypes(exclude=['object']).columns.values","972e23c5":"area_vars = [\"LogSalePrice\",\"LotArea\",\"TotalBsmtSF\",'1stFlrSF','2ndFlrSF','GrLivArea','GarageArea']\narea_train = train[area_vars]\narea_train.dropna()\nsns.pairplot(area_train)","f2952754":"f, axes = plt.subplots(1, 2, figsize=(12,4))\nax1 = sns.violinplot(x=\"OverallQual\", y=\"LogSalePrice\", data=train, ax=axes[0])\nax2 = sns.violinplot(x=\"OverallCond\", y=\"LogSalePrice\", data=train, ax=axes[1])\nax1.set(xlabel='Quality', ylabel='Log Sale Price', title='Overall Quality')\nax2.set(xlabel='Condition', ylabel='Log Sale Price', title='Overall Condition')\nplt.show()","94fa335a":"plt.figure(figsize=(20,8))\nax = sns.boxplot(x=\"YearBuilt\", y=\"LogSalePrice\", data=train)\nax.set_xlabel(xlabel='Year Sold')\nax.set_ylabel(ylabel='Log Sale Price')\nax.set_title(label='House Price by Year Sold')\nplt.xticks(rotation=90)\nplt.show()","a802d4ad":"plt.figure(figsize=(20,8))\nax = sns.boxplot(x=\"YearRemodAdd\", y=\"LogSalePrice\", data=train)\nax.set_xlabel(xlabel='Year Renovated')\nax.set_ylabel(ylabel='Log Sale Price')\nax.set_title(label='House Price by Year Renovated')\nplt.xticks(rotation=90)\nplt.show()","3c5e6664":"plt.figure(figsize=(6,4))\nax = sns.boxplot(x=\"MoSold\", y=\"LogSalePrice\", data=train)\nax.set_xlabel(xlabel='Sale Month')\nax.set_ylabel(ylabel='Log Sale Price')\nax.set_title(label='Month of Sale')\nplt.show()","bbf3a820":"print(\"Categorical Variables:\")\ntrain.select_dtypes(include=['object']).columns.values","f1151f65":"# Function to plot categical data against the target variable \"SalePrice\"\ndef category_boxplot(table, var):\n    grouped = table.groupby(var)['SalePrice'].mean().sort_values(ascending=False)\n    sns.boxplot(x=var, y='LogSalePrice', data=table, order=grouped.index)\n    \ncategory_boxplot(train, \"SaleCondition\")","45d2728d":"train['MSSubClass'] = train['MSSubClass'].apply(str)\ntrain['OverallCond'] = train['OverallCond'].astype(str)\ntrain['MoSold'] = train['MoSold'].astype(str)","9d4c3d9a":"# Function to plot missing data percentage\ndef missing_plot(table):\n    f, ax = plt.subplots()\n    plt.xticks(rotation='90')\n    sns.barplot(x=table.index, y=table)\n    plt.xlabel('Features')\n    plt.ylabel('Percentage of Missing Values')\n    plt.title('Percentage of Missing Data by Features')\n\ntrain_miss = (train.isnull().sum() \/ len(train)) * 100\ntrain_miss = train_miss.drop(train_miss[train_miss == 0].index).sort_values(ascending=False)\nmissing_plot(train_miss)","643d24ac":"train_drop = train_miss.drop(train_miss[train_miss < 50].index).index.values\ntrain_drop","5ee34a11":"print(\"Pre Drop Shape: {}\".format(train.shape))\n\nfor i, col in enumerate(train_drop):\n    if i==1:\n        train2 = train.drop(col, axis=1)\n    elif i>1:\n        train2 = train2.drop(col, axis=1)\n    \nprint(\"Post Drop Shape: {}\".format(train2.shape))","85d1c4d9":"cats = train2.select_dtypes(include=['object'])\ncols = cats.columns.values\ndf2=pd.DataFrame([0], columns=['count'], index=['Test'])\n\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(cats[c].values))\n    cats[c] = lbl.transform(list(cats[c].values))\n    df = pd.DataFrame([len(cats[c].unique())], columns=['count'], index=[c])\n    df2 = df2.append(df)\ndf2 = df2.drop(df2[df2['count'] == 0].index)\ndf2 = df2.sort_values(by=['count'], ascending=False)\n","30604e74":"## 1.3 Target Variable\nThe target variable for this problem is the log of the \"SalePrice\" which is not currently on the \"train\" dataset. For this reason we will have to add it to the dataset manually. Notice in the plots that LogSalePrice is more normally distributed that SalePrice.","0941f6f6":"## 1.2 Importing Data\nThe data that will be used in this notebook is the \"House Price: Advanced Regression Techniques\" data.","de165cb2":"### 2.1.2 Time of Sale and Condition\nThe second type of variable that will be considered in the model is the time of the Sale and the condition of the properies. As a result of this analysis we should be able to understand the following:\n\n- Whether Overall Condition and Quality impacts house price\n- Whether house prices are dependent on the year they were sold\n- Whether there is any seasonality in sale price","89c2a50c":"The \"test\" dataset has one less colunm because there is no \"SalePrice\" variable as this needs to be predicted.","bd8ac263":"### Impute Missing Data \nAfter dropping all of the variable where more than 50% of values are missing, we can not impute the remaining variables with missing values using a basic group average approach. Note that we could also consider more advanced imputation approaches (e.g. KNN), however the objective of this notebook is to understand the difference in modelling techniques on the same data. Optimising the imputation is therefore not a key priority.","cfd15f41":"# Introduction\nThe purpose of the notebook is to provide an introduction into deveopling a regression model using the \"House Price: Advanced Regression Techniques\" data set. The overall modelling approaches which will be considered in this notebook will include Linear Regression (Lasso, Ridge and Net Elastic), Random Forest (RF) and Gradient Boosting Machine (GBM).\n\nThe key points which will be covered in detail in this notebook include:\n\n1. Setting up the Python environment\n2. Data exploration;\n3. Handling missing data (including missing data imputation);\n4. Feature engineering;\n5. Setting up a validation environent;\n6. Fitting different regressions including feature engineering;\n7. Comparing modelling approaches and stacking models.","a08dfd51":"### 2.1.1 Area Related Variables\nFirst type of numeric variable which will be investigated is the area related variables.","58139cb5":"For simplicity, let us consider a threashold of 50%. Therefore any variables with more than 50% missing values will be excluded from the data and variables with less than 50% missing will be imputed.\n","ea9c5688":"## Missing Data \nUnderstanding missing data is another important step. Where variables have missing information, we will have to either drop the variables or consider imputing the missing values. The chart below shows the percentage of missing values in each of the variables.","22277c01":"### Converting Categorical Numeric Data\nBy investigating each of the numeric variables in the above way, two numeric variables were identified as ordinal and one as categorical. For simplicity we will treat these as categorical for now by converting to a string.","31e0a0ba":"# 1. Python Environment Setup\n\n## 1.1 Importing Python Packages\nBefore we begin importing the data we will first import any Python packages which will be useful. Below is a very high level description for each of the packages and how they will be used in this notebook:\n\n__pandas:__ will provide a data structure and basic data analysis functionality such as merging, grouping and shaping the data.\n\n__numpy & scipy:__ will be used to handle most of the mathematical operations on the data.\n\n__seaborn:__ will be used as a framework for plotting data.\n\n__sklearn:__ will provide the framework for the validation environment and different regression classifiers. All of the regressions in this notebook will leaverage the sklearn package.","6b57f6a0":"# Evaluation\nThe first step intacking any modelling problem is to fully understand the target variable you plan to predict. For Kaggle problems in particular, it is equally as important to fully understand the evaluation metric. In order to achieve the best prediction for the Kaggle problem, we will need to set up the validation environment with the evaluation metric in mind.\n\nBelow is the official evaluation metric for this problem:\n\n_\"Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\"_\n\nThe evaluation metric in mathematical notation is therefore:\n\n\\begin{equation}\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^{n}(\\hat{y_i} - y_i)}{n}}\n\\end{equation}\n\nwhere:\n\n- $y_i$ is the $i^{th}$ actual log of the house sale price, , $\\log{(SalePrice)}$;\n- $\\hat{y_i}$ is the $i^{th}$ prediction of the log of the house sale price, $\\log{(SalePricePred)}$;\n- $n$ is the number of sale prices predicted.","7d8c88f0":"# 2. Data Exploration\nThis section is dedicated to undetstanding the data and providing a framework for data visualisations. This data visualisation framework will provide some useful tools to visually inspect different data types including numeric and and categorical.\n\n## 2.1 Visualising Numeric Data\nA list of numeric variables in the \"train\" dataset is provided below. Notice that both \"SalePrice\" and \"LogSaePrice\" are in this list.","14f32060":"The key points to take away from this analysis are:\n\n- Overall quality is more predictive of price than condition. However, both of these variables seem to be relitively predictive\n- Higher quality houses tend to be built more recently\n- Only a slight increase in house price with time. This could be a result of higher wuality houses being built\n- No seasonality effect in house price"}}