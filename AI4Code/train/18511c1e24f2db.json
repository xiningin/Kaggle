{"cell_type":{"d79d1ac3":"code","d3d6eb2e":"code","582d8ebd":"code","142b2119":"code","13c5ab64":"code","dae1e34c":"code","eb1fa4b3":"code","c64d7142":"code","c84116e5":"code","d8442ba6":"code","570c71ac":"code","5846b648":"code","ced765fe":"code","ac8c1034":"code","17a0d319":"code","d24d8fd1":"code","a59fed9d":"code","2a665924":"code","8222f178":"code","fb266e11":"code","5d72606e":"code","69eff09f":"code","fce478ec":"code","beaf5903":"code","e6e15559":"code","d3a2c8ae":"code","ae802133":"code","2776980b":"code","61bd3621":"code","92a73a74":"code","a67be696":"code","ae284fd9":"code","1004e46b":"code","73a126e8":"code","1436a965":"code","48cb6958":"code","4022875a":"code","b8c06592":"code","568742e6":"code","971aa4c3":"code","aa776712":"code","757baa86":"code","e6acacde":"code","04419a45":"code","a08760e7":"code","aef68999":"code","eb058c66":"code","4998b165":"code","a45c67b5":"code","6c0813bb":"code","bc7178dd":"code","57c2f054":"code","b837da03":"code","3be76944":"code","3ed2fbbd":"code","4cd4c46c":"code","18fa9434":"code","0bc1fc0c":"code","8db9c0b5":"code","8621af73":"code","e1974a7c":"code","1bd4e96e":"code","611b36e3":"code","a6b35bd9":"code","f0626363":"code","1c126ec1":"code","282d528c":"code","bcb99b26":"markdown","fa47811f":"markdown","3051f95c":"markdown","df4b103c":"markdown","a386d41e":"markdown","109e11ac":"markdown","725a5423":"markdown","a0e290a0":"markdown","38e4dc65":"markdown","4776f456":"markdown","fcd662b6":"markdown","848e6e44":"markdown","e46c9381":"markdown"},"source":{"d79d1ac3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d3d6eb2e":"data = pd.read_csv(\"..\/input\/data.csv\", index_col=0)\ndata.head()","582d8ebd":"data.head(100).info()","142b2119":"data.columns","13c5ab64":"# filter variables\nnew_columns = ['Overall', 'Potential', 'Crossing',\n       'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling',\n       'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration',\n       'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower',\n       'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression',\n       'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure',\n       'Marking', 'StandingTackle', 'SlidingTackle','Release Clause']","dae1e34c":"data[new_columns].head(500).info()","eb1fa4b3":"data = data[new_columns].head(500)\ndata.dropna(inplace=True)\ndata.info()","c64d7142":"data.head()","c84116e5":"data['Release Clause'].head()","d8442ba6":"data['Release Clause'].tail()","570c71ac":"## convert to float dtype\ndata['Release Clause'] = data['Release Clause'].str.strip('\u20acM').astype(float)\ndata['Release Clause'].tail()","5846b648":"data.info()","ced765fe":"data.corr().iloc[-1]","ac8c1034":"data.corr().iloc[-1][:-1].max()","17a0d319":"data.corr().iloc[-1][:-1].min()","d24d8fd1":"import seaborn as sns\nimport matplotlib.pylab as plt\n\nfig, ax = plt.subplots(figsize=(14,10))         # Sample figsize in inches\nsns.heatmap(data.corr(),  linewidths=.5, ax=ax, square=True, vmin=-1, vmax=1)\nplt.show()","a59fed9d":"data.plot.scatter(x='Overall', y='Release Clause')","2a665924":"data.plot.scatter(x='Potential', y='Release Clause')","8222f178":"from sklearn.linear_model import LinearRegression","fb266e11":"model = LinearRegression()","5d72606e":"X = data['Potential']\ny = data['Release Clause']","69eff09f":"model.fit(X.values.reshape(-1,1), y)","fce478ec":"model.predict(X.values.reshape(-1,1))","beaf5903":"data.plot.scatter(x='Potential', y='Release Clause', figsize=[14,10])\nplt.plot(X , model.predict(X.values.reshape(-1,1)), c='r')","e6e15559":"model.coef_","d3a2c8ae":"model.intercept_","ae802133":"f = lambda x : model.intercept_ + model.coef_*x","2776980b":"f(85)","61bd3621":"y_predict = model.predict(X.values.reshape(-1,1))","92a73a74":"y[0]","a67be696":"y_predict[0]","ae284fd9":"y[0] - y_predict[0]","1004e46b":"(y[0] - y_predict[0])**2","73a126e8":"y[1] - y_predict[1]","1436a965":"(y[1] - y_predict[1])**2","48cb6958":"(y - y_predict)[:2]","4022875a":"((y - y_predict)**2)[:2]","b8c06592":"((y - y_predict)**2)","568742e6":"((y - y_predict)**2).sum()","971aa4c3":"ss_r = ((y - y_predict)**2).sum()","aa776712":"ss_t = ((y - y.mean())**2).sum()","757baa86":"r_squared = 1 - (ss_r \/ ss_t)\nr_squared","e6acacde":"model.score(X.values.reshape(-1,1), y)","04419a45":"data.iloc[:,:-1].head()","a08760e7":"new_X = data.iloc[:,:-1]","aef68999":"np.dot(np.linalg.inv(np.dot(new_X.values.T, new_X.values)), np.dot(new_X.T, y))","eb058c66":"np.dot(np.linalg.pinv(new_X.values), y)","4998b165":"np.linalg.lstsq(new_X, y)","a45c67b5":"## using scikit\n\nmodel_2 = LinearRegression()","6c0813bb":"model_2.fit(new_X, y)","bc7178dd":"model_2.score(new_X, y)","57c2f054":"model_2.predict(new_X)","b837da03":"model_2.intercept_ + (model_2.coef_ * new_X.iloc[0].values).sum()","3be76944":"model_2.intercept_","3ed2fbbd":"for i in range(10):\n    print((new_X.iloc[i].values * np.dot(np.linalg.pinv(new_X.head(31).values), y.head(31))).sum(), y[i])","4cd4c46c":"_x = np.array([0, 1, 2, 3, 4, 5])\n\n_y = np.array([0, .8, .9, .1, -.8, -1])","18fa9434":"p1 = np.polyfit(_x, _y, 1)\np2 = np.polyfit(_x, _y, 2)\np3 = np.polyfit(_x, _y, 3)\np4 = np.polyfit(_x, _y, 4)","0bc1fc0c":"plt.figure(figsize=(14,10))\nplt.plot(_x, _y, 'o')\n_x = np.linspace(-2,6, 100)\nplt.plot(_x, np.polyval(p1, _x), 'r-')\nplt.plot(_x, np.polyval(p2, _x), 'b--')\nplt.plot(_x, np.polyval(p3, _x), 'm:')\nplt.plot(_x, np.polyval(p4, _x))\nplt.legend(['dado',1,2,3,4])","8db9c0b5":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score","8621af73":"X_train, X_test, y_train, y_test = train_test_split(new_X, y, random_state=1)","e1974a7c":"# .fit()\nX_train.shape","1bd4e96e":"# .score()\nX_test.shape","611b36e3":"new_X.shape","a6b35bd9":"model_3 = LinearRegression()\nscores = cross_val_score(model_3, new_X, y, cv=4, scoring=\"mean_squared_error\")","f0626363":"scores","1c126ec1":"sklearn.metrics.SCORERS.keys()","282d528c":"import sklearn","bcb99b26":"# Polynomial Regression\n\nThe curvilinear relationship is captured when you transform the training data by adding polynomial terms. ","fa47811f":"The regression hyperplane has one dimension, cause there is one dimension for the response variable and another dimension for the explanatory variable, making a total of two dimensions. A hyperplane is a subspace that has one dimension less than tha ambient space that contains it.\n\nFormula of the model:\n\n$$ y = -616.803538622231 + 7.88520305 \\cdot x$$","3051f95c":"### R-squared\n\nR-square measures how well the observed values of the response variables are predicted by the model. \n\nIt is the proportion of the variance in the response variable that is explained by the model.\n\nR-squared == 1 indicates that the response variable can be predicted without any error using the model.\n\nR-squared == 0.5 indicates that half of the variance in the response variable can be predicted using the model. \n\nIn linear regression, r-squared is equal to the square of the Pearson product momento correlation coefficient, or Pearson's r.\n\n$$ SS_{tot} = \\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2} $$","df4b103c":"Both columns have a positive relationship with Release Clause","a386d41e":"# Multiple Linear Regression\n\n- Maybe others atributes are related to the value of the player.\n\nMultiple Linear Regression follow the model:\n\n$$ y = \\alpha  + \\beta_{1} x_{1} + \\beta_{2} x_{2} + ... + \\beta_{n} x_{n} $$\n\nHmmm..\n\nIt makes more sense:\n\n![mlr.png](attachment:mlr.png)\n\n$$ Y = X\\beta $$\n\nY -> response variables\n\n$\\beta$ -> model'sparameter\n\nX -> design matrix\n\n$$ \\beta = (X^{T} X)^{-1} X^{T} Y $$","109e11ac":"## Fitting and evaluating the model\n\nLearning the parameters of a prediction function and testing it on the same data is a methodological mistake. We have to predict the unseen data.\n\nRepeat the labels of the samples that it has just seen is called **overfitting**.","725a5423":"## Evaluating the fitness of a model with a cost function\n\n- A cost function, also called a loss function is used to define and measure the error of a model. Describes how well the current response surface f(x) fits the available data (judge the performance of the model). Smaller values of the cost function correspond to a better fit.\n\n- Machine Learning goal: contruct h(x) such that the cost function is minimized.\n\n- **Residuals** or **training errors** is the differences between the values predicted by the model and the observed values. (show in the blackboard)\n\nLater, we will evaluate a model on a separate set of test data; the differences between the predicted and observed values in the test data are called **prediction erros** or **test erros**\n\nMinimizing the sum of the residuals we can produce the best predictor.\n\nResidual sum of squares formula:\n\n$$ SS_{res} = \\sum_{i=1}^{n} (y_{i}-f(x_{i}))^{2} $$","a0e290a0":"### 1.1 Know the price of the player","38e4dc65":"### 1. Linear Regression\n\nThe model can be used to predict the value of the response variable for values of the explanatory variable that have not been previously observed.\n\nThe **goal** in regression problems is to **predict the value of a continuous response variable.** ","4776f456":"## Regularization\n\nRegularization is a collection of techniques that can be used to prevent over-fitting. It adds information to a problem, often in the penalty against complexity. \n\nRegularization attempts to find the simplest model that explains the data.\n\n**Ridge regression** (Tikhonov regularization) penalizes model parameters that become too large. ","fcd662b6":"$$ y = \\alpha * \\beta_{1}x * \\beta_{2}x^{2} $$","848e6e44":"# Linear Regreesion\n\n## Objectives:\n\n1. Examine Simple Linear Regression - which models the relationship between a response variable and single explanatory variable.\n\n2. Examine Multiple Linear Regression - a generalization of simple linear regression that can support more than ond exploratory variable.\n\n3. Examine Polynomial regression, a special case of multiple regression that can effectively model nonlinear relationships.\n\n4. Discuss how to train our model by finding the values od their parameters that minimize a cost function.","e46c9381":"An r-squared score of 0.6133 indicates that a large proportion of the variance is explained by the model."}}