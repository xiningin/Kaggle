{"cell_type":{"0e31ad7d":"code","8c8559a2":"code","8cf86805":"code","ece24ccf":"code","9fc3eec5":"code","938bca7b":"code","3f07a555":"code","cfb86e0b":"code","9cf275b6":"code","250ac6a4":"code","62bc9c27":"code","26294e84":"code","20e732e6":"code","8f50deb8":"code","32603339":"code","a1c3a41a":"code","ae34578d":"code","d82ee358":"code","c0d8d6f5":"code","4cb12028":"code","de3f5e12":"code","3ec32aeb":"code","ce0aa5dc":"code","7181558b":"code","952f2969":"code","2bb0813b":"code","db6bb72d":"code","2e909cb0":"code","72b306f5":"code","002f126e":"code","9bac70cf":"code","4a8e153a":"code","08ad211b":"code","105b568c":"code","4b35d48c":"code","9fbc7c03":"code","2bdd8458":"code","8e890c74":"code","103d16e9":"code","59b25e4f":"code","af2f2ff5":"code","65d61e25":"markdown","78eb218a":"markdown","9e53f0f0":"markdown","9a3725eb":"markdown","38596a6e":"markdown","8b5ab722":"markdown","58bae3ed":"markdown","6c678421":"markdown","9ab5d910":"markdown","6196b18d":"markdown","474a9f20":"markdown","fda0b745":"markdown","765884a7":"markdown","ccdb62b8":"markdown","26023f55":"markdown","6f658d1d":"markdown","900cc043":"markdown","f9b83575":"markdown","802d93d5":"markdown","8ec15d37":"markdown","4e7b3bac":"markdown","3d83af5e":"markdown","d375b0eb":"markdown","732383d9":"markdown","bad3b39a":"markdown","e12b9ff8":"markdown","54117411":"markdown"},"source":{"0e31ad7d":"\n#import libraries\nfrom __future__ import division\n\nfrom datetime import datetime, timedelta,date\nimport pandas as pd\n%matplotlib inline\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\n\n\nimport plotly as py\nimport plotly.offline as pyoff\nimport plotly.graph_objs as go\n\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\n\nimport xgboost as xgb\n","8c8559a2":"#Read data\ntx_data = pd.read_csv('..\/input\/customer_segmentation\/customer_segmentation.csv', encoding='cp1252')","8cf86805":"#initate plotly\npyoff.init_notebook_mode()\n\n#read data from csv and redo the data work we done before\ntx_data.head()","ece24ccf":"#converting the type of Invoice Date Field from string to datetime.\ntx_data['InvoiceDate'] = pd.to_datetime(tx_data['InvoiceDate'])","9fc3eec5":"#creating YearMonth field for the ease of reporting and visualization\ntx_data['InvoiceYearMonth'] = tx_data['InvoiceDate'].map(lambda date: 100*date.year + date.month)","938bca7b":"tx_data.describe()","3f07a555":"tx_data['Country'].value_counts()","cfb86e0b":"#we will be using only UK data\ntx_uk = tx_data.query(\"Country=='United Kingdom'\").reset_index(drop=True)\n","9cf275b6":"#create a generic user dataframe to keep CustomerID and new segmentation scores\ntx_user = pd.DataFrame(tx_data['CustomerID'].unique())\ntx_user.columns = ['CustomerID']\ntx_user.head()","250ac6a4":"tx_uk.head()","62bc9c27":"#get the max purchase date for each customer and create a dataframe with it\ntx_max_purchase = tx_uk.groupby('CustomerID').InvoiceDate.max().reset_index()\ntx_max_purchase.columns = ['CustomerID','MaxPurchaseDate']\ntx_max_purchase.head()","26294e84":"# Compare the last transaction of the dataset with last transaction dates of the individual customer IDs.\ntx_max_purchase['Recency'] = (tx_max_purchase['MaxPurchaseDate'].max() - tx_max_purchase['MaxPurchaseDate']).dt.days\ntx_max_purchase.head()","20e732e6":"#merge this dataframe to our new user dataframe\ntx_user = pd.merge(tx_user, tx_max_purchase[['CustomerID','Recency']], on='CustomerID')\ntx_user.head()","8f50deb8":"#plot a recency histogram\n\nplot_data = [\n    go.Histogram(\n        x=tx_user['Recency']\n    )\n]\n\nplot_layout = go.Layout(\n        title='Recency'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)\n","32603339":"from sklearn.cluster import KMeans\n\nsse={} # error\ntx_recency = tx_user[['Recency']]\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(tx_recency)\n    tx_recency[\"clusters\"] = kmeans.labels_  #cluster names corresponding to recency values \n    sse[k] = kmeans.inertia_ #sse corresponding to clusters\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.show()","a1c3a41a":"#build 4 clusters for recency and add it to dataframe\nkmeans = KMeans(n_clusters=4)\ntx_user['RecencyCluster'] = kmeans.fit_predict(tx_user[['Recency']])\n","ae34578d":"tx_user.head()","d82ee358":"tx_user.groupby('RecencyCluster')['Recency'].describe()","c0d8d6f5":"#function for ordering cluster numbers\ndef order_cluster(cluster_field_name, target_field_name,df,ascending):\n    new_cluster_field_name = 'new_' + cluster_field_name\n    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()\n    df_new = df_new.sort_values(by=target_field_name,ascending=ascending).reset_index(drop=True)\n    df_new['index'] = df_new.index\n    df_final = pd.merge(df,df_new[[cluster_field_name,'index']], on=cluster_field_name)\n    df_final = df_final.drop([cluster_field_name],axis=1)\n    df_final = df_final.rename(columns={\"index\":cluster_field_name})\n    return df_final\n\ntx_user = order_cluster('RecencyCluster', 'Recency',tx_user,False)","4cb12028":"tx_user.head()","de3f5e12":"tx_user.groupby('RecencyCluster')['Recency'].describe()","3ec32aeb":"#get order counts for each user and create a dataframe with it\ntx_frequency = tx_uk.groupby('CustomerID').InvoiceDate.count().reset_index()\ntx_frequency.columns = ['CustomerID','Frequency']","ce0aa5dc":"tx_frequency.head() #how many orders does a customer have","7181558b":"#add this data to our main dataframe\ntx_user = pd.merge(tx_user, tx_frequency, on='CustomerID')\n\ntx_user.head()","952f2969":"#plot the histogram\nplot_data = [\n    go.Histogram(\n        x=tx_user['Frequency']\n    )\n]\n\nplot_layout = go.Layout(\n        title='Frequency'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","2bb0813b":"#Now we are getting maximum frequencies below 1000. But if I want to zoom and see frequencies below 1000, then what should I do?\n#Let me try to plot histogram with frequencies below 1000.\n\n#plot the histogram\nplot_data = [\n    go.Histogram(\n        x=tx_user.query('Frequency < 1000')['Frequency']\n    )\n]\n\nplot_layout = go.Layout(\n        title='Frequency',\n    xaxis_title = 'Frequencies'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","db6bb72d":"from sklearn.cluster import KMeans\n\nsse={} # error\ntx_recency = tx_user[['Frequency']]\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(tx_recency)\n    tx_recency[\"clusters\"] = kmeans.labels_  #cluster names corresponding to recency values \n    sse[k] = kmeans.inertia_ #sse corresponding to clusters\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.show()","2e909cb0":"# Applying k-Means\nkmeans=KMeans(n_clusters=4)\ntx_user['FrequencyCluster']=kmeans.fit_predict(tx_user[['Frequency']])\n\n#order the frequency cluster\ntx_user = order_cluster('FrequencyCluster', 'Frequency', tx_user, True )\ntx_user.groupby('FrequencyCluster')['Frequency'].describe()","72b306f5":"#calculate revenue for each customer\ntx_uk['Revenue'] = tx_uk['UnitPrice'] * tx_uk['Quantity']\ntx_revenue = tx_uk.groupby('CustomerID').Revenue.sum().reset_index()","002f126e":"tx_revenue.head()","9bac70cf":"#merge it with our main dataframe\ntx_user = pd.merge(tx_user, tx_revenue, on='CustomerID')\ntx_user.head()\n","4a8e153a":"#plot the histogram\nplot_data = [\n    go.Histogram(\n        x=tx_user['Revenue']\n    )\n]\n\nplot_layout = go.Layout(\n        title='Monetary Value'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)\n","08ad211b":"\n#plot the histogram\nplot_data = [\n    go.Histogram(\n        x=tx_user.query('Revenue < 10000')['Revenue']\n    )\n]\n\nplot_layout = go.Layout(\n        title='Monetary Value'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)\n\n","105b568c":"from sklearn.cluster import KMeans\n\nsse={} # error\ntx_recency = tx_user[['Revenue']]\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(tx_recency)\n    tx_recency[\"clusters\"] = kmeans.labels_  #cluster names corresponding to recency values \n    sse[k] = kmeans.inertia_ #sse corresponding to clusters\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.show()","4b35d48c":"#apply clustering\nkmeans = KMeans(n_clusters=4)\ntx_user['RevenueCluster'] = kmeans.fit_predict(tx_user[['Revenue']])\n\n#order the cluster numbers\ntx_user = order_cluster('RevenueCluster', 'Revenue',tx_user,True)\n\n#show details of the dataframe\ntx_user.groupby('RevenueCluster')['Revenue'].describe()","9fbc7c03":"#calculate overall score and use mean() to see details\ntx_user['OverallScore'] = tx_user['RecencyCluster'] + tx_user['FrequencyCluster'] + tx_user['RevenueCluster']\ntx_user.groupby('OverallScore')['Recency','Frequency','Revenue'].mean()","2bdd8458":"tx_user['Segment'] = 'Low-Value'\ntx_user.loc[tx_user['OverallScore']>2,'Segment'] = 'Mid-Value' \ntx_user.loc[tx_user['OverallScore']>4,'Segment'] = 'High-Value' ","8e890c74":"tx_user","103d16e9":"#Revenue vs Frequency\ntx_graph = tx_user.query(\"Revenue < 10000 and Frequency < 1000\")\n\nplot_data = [\n    go.Scatter(\n        x=tx_graph.query(\"Segment == 'Low-Value'\")['Frequency'],\n        y=tx_graph.query(\"Segment == 'Low-Value'\")['Revenue'],\n        mode='markers',\n        name='Low',\n        marker= dict(size= 7,\n            line= dict(width=1),\n            color= 'blue',\n            opacity= 0.8\n           )\n    ),\n        go.Scatter(\n        x=tx_graph.query(\"Segment == 'Mid-Value'\")['Frequency'],\n        y=tx_graph.query(\"Segment == 'Mid-Value'\")['Revenue'],\n        mode='markers',\n        name='Mid',\n        marker= dict(size= 9,\n            line= dict(width=1),\n            color= 'green',\n            opacity= 0.5\n           )\n    ),\n        go.Scatter(\n        x=tx_graph.query(\"Segment == 'High-Value'\")['Frequency'],\n        y=tx_graph.query(\"Segment == 'High-Value'\")['Revenue'],\n        mode='markers',\n        name='High',\n        marker= dict(size= 11,\n            line= dict(width=1),\n            color= 'red',\n            opacity= 0.9\n           )\n    ),\n]\n\nplot_layout = go.Layout(\n        yaxis= {'title': \"Revenue\"},\n        xaxis= {'title': \"Frequency\"},\n        title='Segments'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)\n","59b25e4f":"\n#Revenue Recency\n\ntx_graph = tx_user.query(\"Revenue < 10000 and Frequency < 1000\")\n\nplot_data = [\n    go.Scatter(\n        x=tx_graph.query(\"Segment == 'Low-Value'\")['Recency'],\n        y=tx_graph.query(\"Segment == 'Low-Value'\")['Revenue'],\n        mode='markers',\n        name='Low',\n        marker= dict(size= 7,\n            line= dict(width=1),\n            color= 'blue',\n            opacity= 0.8\n           )\n    ),\n        go.Scatter(\n        x=tx_graph.query(\"Segment == 'Mid-Value'\")['Recency'],\n        y=tx_graph.query(\"Segment == 'Mid-Value'\")['Revenue'],\n        mode='markers',\n        name='Mid',\n        marker= dict(size= 9,\n            line= dict(width=1),\n            color= 'green',\n            opacity= 0.5\n           )\n    ),\n        go.Scatter(\n        x=tx_graph.query(\"Segment == 'High-Value'\")['Recency'],\n        y=tx_graph.query(\"Segment == 'High-Value'\")['Revenue'],\n        mode='markers',\n        name='High',\n        marker= dict(size= 11,\n            line= dict(width=1),\n            color= 'red',\n            opacity= 0.9\n           )\n    ),\n]\n\nplot_layout = go.Layout(\n        yaxis= {'title': \"Revenue\"},\n        xaxis= {'title': \"Recency\"},\n        title='Segments'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)\n\n","af2f2ff5":"# Revenue vs Frequency\ntx_graph = tx_user.query(\"Revenue < 50000 and Frequency < 2000\")\n\nplot_data = [\n    go.Scatter(\n        x=tx_graph.query(\"Segment == 'Low-Value'\")['Recency'],\n        y=tx_graph.query(\"Segment == 'Low-Value'\")['Frequency'],\n        mode='markers',\n        name='Low',\n        marker= dict(size= 7,\n            line= dict(width=1),\n            color= 'blue',\n            opacity= 0.8\n           )\n    ),\n        go.Scatter(\n        x=tx_graph.query(\"Segment == 'Mid-Value'\")['Recency'],\n        y=tx_graph.query(\"Segment == 'Mid-Value'\")['Frequency'],\n        mode='markers',\n        name='Mid',\n        marker= dict(size= 9,\n            line= dict(width=1),\n            color= 'green',\n            opacity= 0.5\n           )\n    ),\n        go.Scatter(\n        x=tx_graph.query(\"Segment == 'High-Value'\")['Recency'],\n        y=tx_graph.query(\"Segment == 'High-Value'\")['Frequency'],\n        mode='markers',\n        name='High',\n        marker= dict(size= 11,\n            line= dict(width=1),\n            color= 'red',\n            opacity= 0.9\n           )\n    ),\n]\n\nplot_layout = go.Layout(\n        yaxis= {'title': \"Frequency\"},\n        xaxis= {'title': \"Recency\"},\n        title='Segments'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)\n","65d61e25":"From elbow's method, we find that clusters can be 3 or 4. Lets take 4 as the number of clusters","78eb218a":"Here it looks like 3 is the optimal one. Based on business requirements, we can go ahead with less or more clusters. We will be selecting 4 for this example","9e53f0f0":"**Elbow method to find out the optimum number of clusters for K-Means**","9a3725eb":"Great! cluster 1 earlier is now cluster0, cluster 2 earlier is now cluster 1 and so on. The clusters are arranged according to inactiviuty. Cluster 0 now is most inactive, cluster 3 is most active. ","38596a6e":"Since we are calculating recency, we need to know when last the person bought something. Let us calculate the last date of transaction for a person.","8b5ab722":"**Ordering clusters**\n\nWe have a cluster corresponding to each customerID. But each cluster is randomly assigned. Cluster 2 is not better than cluster 1 for e.g. and so on. We want to give clusters according to most recent transactions.\n\nWe will first find the mean of recency value corresponding to each cluster. Then we will sort these values. Let's say cluster 3 has the most recent transactions mean value. From the above table we see that cluster 1(mean recency 304) > cluster 2 > cluster 3 > cluster 0. That means that cluster 1 is most inactive and cluster 0 is most recent. We will give indices to these clusters as 0,1,2,3. So cluster 1 becomes cluster 0, cluster 2 becomes cluster 1, cluster 3 becomes cluster 2 and so on. Now we will drop the original cluster numbers and replace them with 0,1,2,3. Code is below.","58bae3ed":"**Segmentation Techniques**\n\nYou can do many different segmentations according to what you are trying to achieve. If you want to increase retention rate, you can do a segmentation based on churn probability and take actions. But there are very common and useful segmentation methods as well. Now we are going to implement one of them to our business: RFM.\nRFM stands for Recency - Frequency - Monetary Value. Theoretically we will have segments like below:\n\n* Low Value: Customers who are less active than others, not very frequent buyer\/visitor and generates very low - zero - maybe negative revenue.\n* Mid Value: In the middle of everything. Often using our platform (but not as much as our High Values), fairly frequent and generates moderate revenue.\n* High Value: The group we don\u2019t want to lose. High Revenue, Frequency and low Inactivity.\n\nAs the methodology, we need to calculate Recency, Frequency and Monetary Value (we will call it Revenue from now on) and apply unsupervised machine learning to identify different groups (clusters) for each. Let\u2019s jump into coding and see how to do RFM Clustering.\n\n**1. Recency**\n\nTo calculate recency, we need to find out most recent purchase date of each customer and see how many days they are inactive for. After having no. of inactive days for each customer, we will apply K-means* clustering to assign customers a recency score.\n\nLets go ahead and calculate that.","6c678421":"**Actions to be taken**\n\nWe can start taking actions with this segmentation. The main strategies are quite clear:\n\n* High Value: Improve Retention\n* Mid Value: Improve Retention + Increase Frequency\n* Low Value: Increase Frequency\n","9ab5d910":"<h3> Feature Engineering <\/h3>","6196b18d":"Starting from this part, we will be focusing on UK data only (which has the most records). We can get the monthly active customers by counting unique CustomerIDs. The same analysis can be carried out for customers of other countries as well.","474a9f20":"**3. Revenue**\n\nLet\u2019s see how our customer database looks like when we cluster them based on revenue. We will calculate revenue for each customer, plot a histogram and apply the same clustering method.","fda0b745":"**2. Assigning a recency score**\n\nWe are going to apply K-means clustering to assign a recency score. But we should tell how many clusters we need to K-means algorithm. To find it out, we will apply Elbow Method. Elbow Method simply tells the optimal cluster number for optimal inertia. Code snippet and Inertia graph are as follows:","765884a7":"By Elbow method, clusters number should be 4 as after 4, the graph goes down.","ccdb62b8":"**Frequency clusters**","26023f55":"Clsuter with max frequency is cluster 3, least frequency cluster is cluster 0.","6f658d1d":"Determine the right number of clusters for K-Means by elbow method","900cc043":"**2. Frequency**\n\nTo create frequency clusters, we need to find total number orders for each customer. First calculate this and see how frequency look like in our customer database","f9b83575":"We have some customers with negative revenue as well. Let\u2019s continue and apply k-means clustering:\n","802d93d5":"Since the graph is not visible properly. Let us reduce the sclae and try","8ec15d37":"Score 8 is our best customer, score 0 is our worst  customer.","4e7b3bac":"Cluster 3 has max revenue, cluster 0 has lowest revenue","3d83af5e":"**Overall Score**\n\nWe have scores (cluster numbers) for recency, frequency & revenue. Let\u2019s create an overall score out of them\n","d375b0eb":"We have all the crucial information we need:\nCustomer ID\nUnit Price\nQuantity\nInvoice Date\nRevenue = Active Customer Count * Order Count * Average Revenue per Order\n","732383d9":"<H2>SEGMENTATION<\/H2>","bad3b39a":"<h2> Importing relevant packages and libraries <\/h2>","e12b9ff8":"**Visualisation of segments**\n\nLet\u2019s see how our segments distributed on a scatter plot","54117411":"**Why we do segmentation?**\n\nBecause you can\u2019t treat every customer the same way with the same content, same channel, same importance. They will find another option which understands them better.\n\n**About the project**\n\nWe have a dataset with customer details, inovice dates, order value, quantity etc. and we need to divide people on clusters so that we can make customised strategies for each person. We will follow the Recency, Frequency and Revenue model for dividing customers into clusters. First we will make clusters based on the recency of transactions. People who are most active will be in one cluster and most inactive people for instance will be in another cluster. Similarly we will make clusters according to how frequency people trasact and we will make clusters according to the total revenue value of customers. Finally an overall score will be calculated based on the 3 factors and an overall cluster number will be alloted to each customer ID. Then we can accordingly have different strategies for different clusters.\n\n**What is RFM Clustering**\n\nRecenecy, Frequency and Monetary value. It means overall clusters will be based on these 3 factors."}}