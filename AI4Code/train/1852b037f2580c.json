{"cell_type":{"487bc406":"code","09de597f":"code","c2ae0dc1":"code","61c8a012":"code","ebe159a8":"code","b5008add":"code","8031be68":"code","f340e824":"code","e339ccff":"code","81f5b683":"code","7ec089ea":"code","03bb7f7e":"code","18b21c69":"code","b7e0d8da":"code","a8d604bb":"code","1281b0cd":"code","33388703":"code","50901686":"code","9d19458b":"code","e42618bf":"code","ce9d77fe":"code","d768dca7":"code","f0431377":"code","adc52394":"code","f2a21562":"code","a01cce6e":"code","2d8b5b08":"code","771748e1":"code","f35071a9":"code","4e89b991":"code","b131830c":"code","ec193ec3":"code","ae05c9d5":"code","a01f9adb":"code","24722122":"code","0f71c3e8":"code","a61dba60":"code","dc14c1fc":"code","c1afa926":"code","2e506872":"code","e1d291c4":"code","cf19f307":"code","eb857cd5":"code","bb31816d":"code","9eec661d":"code","7f0fb772":"code","4bbbd94f":"code","5c3cf51a":"code","fdeb928a":"code","5336a60d":"code","5a1619a2":"code","4cb8f441":"code","c499bc53":"code","734b75eb":"code","cfb18cd7":"code","ad165003":"code","3711848f":"code","8f0f79dd":"code","c38a8962":"code","2e0a13ce":"code","1b4c5ccb":"code","0ba965eb":"code","526ef239":"code","1840902a":"code","60120e72":"code","c7acb7b3":"code","4031e7ac":"code","0e158637":"code","b2093709":"code","c9db30da":"code","62253776":"code","052fd978":"code","bfce5413":"code","a53f029a":"code","230ba48f":"code","819dd48e":"code","043c665f":"code","a90a2ee7":"code","ca05dd4d":"code","646f43ce":"code","5c2b6576":"code","6329b2a4":"code","50c9d71c":"markdown","77c58048":"markdown","9ad43ba8":"markdown","2c858659":"markdown","cfb1c3ef":"markdown","e3a9222a":"markdown","bea86b83":"markdown","e46b4dca":"markdown","e36cc3dc":"markdown","cc0cc671":"markdown","c8833dba":"markdown","997a6a08":"markdown","1f4e57ec":"markdown","6ba38c78":"markdown","7f48e52c":"markdown","f199e478":"markdown"},"source":{"487bc406":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","09de597f":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","c2ae0dc1":"# Importing all datasets\n\ndf_train=pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test=pd.read_csv('..\/input\/titanic\/test.csv')\ny_test=pd.read_csv('..\/input\/titanic\/gender_submission.csv')","61c8a012":"df_train.shape","ebe159a8":"df_train.info()","b5008add":"# performing pandas profiling for basic idea of variables\nimport pandas_profiling\npandas_profiling.ProfileReport(df_train)","8031be68":"#checking missing values\ndf_train.isnull().sum()","f340e824":"df_train.columns","e339ccff":"df_test.isnull().sum()","81f5b683":"plt.figure(figsize=(4,5))\nsns.countplot(df_train.Survived)\nplt.xlabel(\"Survived Value\")\nplt.ylabel(\"Count of Survived\")\nplt.title(\"Distribution of Variable-Survived\")\nplt.show()","7ec089ea":"plt.figure(figsize=(15,4))\nax1=plt.subplot(1, 3, 1)\nax1=sns.countplot(df_train.Pclass, hue=df_train.Survived, palette='husl')\nplt.xlabel(\"Passenger Class Value\")\nplt.ylabel(\"Count of Value\")\nplt.title(\"Distribution of Pclass\")\n\nax2=plt.subplot(1, 3, 2)\nax2=sns.countplot(df_train.Sex, hue=df_train.Survived, palette='husl')\nplt.xlabel(\"Sex Value\")\nplt.ylabel(\"Count of Value\")\nplt.title(\"Distribution of Sex\")\n\nax3=plt.subplot(1, 3, 3)\nax3=sns.countplot(df_train.Embarked, hue=df_train.Survived, palette='husl')\nplt.xlabel(\"Embarked Value\")\nplt.ylabel(\"Count of Value\")\nplt.title(\"Distribution of Embarked\")\nplt.show()","03bb7f7e":"plt.figure(figsize=(15,4))\nax1=plt.subplot(1, 3, 1)\nax1=sns.countplot(df_train.SibSp, hue=df_train.Survived, palette='husl')\nplt.xlabel(\"Sibsp Value\")\nplt.ylabel(\"Count of Value\")\nplt.title(\"Distribution of Sibsp\")\n\nax2=plt.subplot(1, 3, 2)\nax2=sns.countplot(df_train.Parch, hue=df_train.Survived, palette='husl')\nplt.xlabel(\"Parch Value\")\nplt.ylabel(\"Count of Value\")\nplt.title(\"Distribution of Parch\")\n\nax3=plt.subplot(1, 3, 3)\nax3=sns.swarmplot(x='Pclass',y='Fare',hue='Survived',data=df_train,palette='husl')\nplt.xlabel(\"Plcass\")\nplt.ylabel(\"Fare\")\nplt.title(\"Distribution of Plcass vs Fare\")\nplt.show()","18b21c69":"survived=df_train[df_train.Survived==1]\nnot_survived=df_train[df_train.Survived==0]","b7e0d8da":"# plots\nplt.figure(figsize=(15,5))\nax1=plt.subplot(1, 2, 1)\nax1=plt.hist(x=survived['Age']);\nplt.xlabel(\"Age Values\")\nplt.ylabel(\"Count of Value\")\nplt.title(\"Distribution of Age for passengers who Survived\")\n\nax2=plt.subplot(1, 2, 2)\nax2=plt.hist(x=not_survived['Age']);\nplt.xlabel(\"Age Values\")\nplt.ylabel(\"Count of Value\")\nplt.title(\"Distribution of Age for passengers who did not Survive\")\n\nplt.show()","a8d604bb":"df_train.columns","1281b0cd":"#Imputng Missing value in Train Data\n\ndf_train['Age'].fillna(df_train.Age.mean(), inplace=True)\ndf_train['Embarked'].fillna(df_train.Embarked.mode()[0], inplace=True)\ndf_train.isnull().sum()","33388703":"#Imputing Missing value in Test Data \n\ndf_test['Age'].fillna(df_test.Age.mean(), inplace=True)\ndf_test['Fare'].fillna(df_test.Fare.mean(), inplace=True)\ndf_test['Embarked'].fillna(df_test.Embarked.mode()[0], inplace=True)\ndf_test.isnull().sum()","50901686":"#Removing Coloumn Cabin from Datasets since it has too many missing values\n\ndf_train = df_train.drop('Cabin', axis=1)\ndf_test = df_test.drop('Cabin', axis=1)","9d19458b":"#Removing Column Name & Ticket as they have high cardinality(too many unique values) and are not very useful\n\ndf_train = df_train.drop(['Name','Ticket'], axis=1)\ndf_test = df_test.drop(['Name','Ticket'], axis=1)","e42618bf":"#Removing Column Passenger ID as it has high cardinality(too many unique values) and is not very useful\ndf_train = df_train.drop(['PassengerId'], axis=1)","ce9d77fe":"df_train.nunique()","d768dca7":"df_test.nunique()","f0431377":"df_train.dtypes","adc52394":"# converting categorical variables\ndf_train['Sex'] = df_train['Sex'].replace('male',0)\ndf_train['Sex'] = df_train['Sex'].replace('female',1)","f2a21562":"df_test['Sex'] = df_test['Sex'].replace('male',0)\ndf_test['Sex'] = df_test['Sex'].replace('female',1)","a01cce6e":"df_train.nunique()","2d8b5b08":"df_test.isnull().sum()","771748e1":"df_train.isnull().sum()","f35071a9":"df_train.head()","4e89b991":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy1 = pd.get_dummies(df_train[['Embarked']], drop_first=True)\n\n# Adding the results to the master dataframe\ndf_train = pd.concat([df_train, dummy1], axis=1)\ndf_train = df_train.drop(['Embarked'], axis=1)\ndf_train.head()","b131830c":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy2 = pd.get_dummies(df_test[['Embarked']], drop_first=True)\n\n# Adding the results to the master dataframe\ndf_test = pd.concat([df_test, dummy2], axis=1)\ndf_test = df_test.drop(['Embarked'], axis=1)\ndf_test.head()","ec193ec3":"# Checking outliers at 25%, 50%, 75%, 90%, 95% and 99%\ndf_train.describe(percentiles=[.25, .5, .75, .90, .95, .99])","ae05c9d5":"# Putting feature variable to X\nX_train = df_train.drop(['Survived'], axis=1)\n\nX_train.head()","a01f9adb":"# Putting response variable to y\ny_train = df_train['Survived']\n\ny_train.head()","24722122":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train[['Age','Fare']] = scaler.fit_transform(X_train[['Age','Fare']])\n\nX_train.head()","0f71c3e8":"import statsmodels.api as sm\n\n# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","a61dba60":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","dc14c1fc":"cols=X_train.columns\ncols","c1afa926":"cols = cols.drop('Embarked_Q', 1)","2e506872":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[cols])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","e1d291c4":"#checking VIF\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['Features'] = X_train[cols].columns\nvif['VIF'] = [variance_inflation_factor(X_train[cols].values, i) for i in range(X_train[cols].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","cf19f307":"cols = cols.drop('Parch', 1)","eb857cd5":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[cols])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","bb31816d":"cols = cols.drop('Fare', 1)","9eec661d":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[cols])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","7f0fb772":"#checking VIF\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['Features'] = X_train[cols].columns\nvif['VIF'] = [variance_inflation_factor(X_train[cols].values, i) for i in range(X_train[cols].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","4bbbd94f":"y_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","5c3cf51a":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","fdeb928a":"y_train_pred_final = pd.DataFrame({'Survival':y_train.values, 'Survival_Prob':y_train_pred})\ny_train_pred_final.head()","5336a60d":"y_train_pred_final['predicted'] = y_train_pred_final.Survival_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","5a1619a2":"from sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Survival, y_train_pred_final.predicted )\nprint(confusion)","4cb8f441":"#Predicted     not_survived    survived\n\n#Actual\n\n#not_survived       458          91\n\n#survived            98         244  ","c499bc53":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Survival, y_train_pred_final.predicted))","734b75eb":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","cfb18cd7":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","ad165003":"# Let us calculate specificity\nTN \/ float(TN+FP)","3711848f":"# Calculate false postive rate - predicting survived when customer did not survive\nprint(FP\/ float(TN+FP))","8f0f79dd":"# positive predictive value \nprint (TP \/ float(TP+FP))","c38a8962":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","2e0a13ce":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","1b4c5ccb":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Survival, y_train_pred_final.Survival_Prob, drop_intermediate = False )\ndraw_roc(y_train_pred_final.Survival, y_train_pred_final.Survival_Prob)","0ba965eb":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Survival_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head(10)","526ef239":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Survival, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","1840902a":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","60120e72":"#### From the curve above, 0.38 is the optimum point to take it as a cutoff probability.","c7acb7b3":"y_train_pred_final['final_predicted'] = y_train_pred_final.Survival_Prob.map( lambda x: 1 if x > 0.36 else 0)\n\ny_train_pred_final.head()","4031e7ac":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Survival, y_train_pred_final.final_predicted)","0e158637":"df_test.head()","b2093709":"# Putting feature variable to X\nX_test = df_test.copy()\n\nX_test.head()","c9db30da":"X_test[['Age','Fare']] = scaler.transform(X_test[['Age','Fare']])","62253776":"X_test = X_test[cols]\nX_test.head()","052fd978":"X_test_sm = sm.add_constant(X_test)","bfce5413":"y_test_pred = res.predict(X_test_sm)","a53f029a":"y_test_pred[:10]","230ba48f":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)\ny_pred_1.head()","819dd48e":"# Putting PassengerId to index\ny_test_df = pd.DataFrame(df_test['PassengerId'])","043c665f":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","a90a2ee7":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","ca05dd4d":"y_pred_final.head()","646f43ce":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Survival_Prob'})\ny_pred_final.head()","5c2b6576":"y_pred_final['final_predicted'] = y_pred_final.Survival_Prob.map(lambda x: 1 if x > 0.36 else 0)\ny_pred_final.head()","6329b2a4":"submission = pd.DataFrame({\n        \"PassengerId\": y_pred_final[\"PassengerId\"],\n        \"Survived\": y_pred_final['final_predicted']\n    })\nsubmission.to_csv('submission.csv', index=False)","50c9d71c":"- There are more entries for Survived=0 than Survived=1 since the number of passengers who died is far more than the number of passengers who survived.","77c58048":"### Making predictions on the test set","9ad43ba8":"**In Train Set**\n- Age has 177 missing values, \n- Cabin has 687 missing values\n- Embarked has 2 missing values","2c858659":"**We will need to treat some of these missing values but first let's do some basic EDA on the dataset**","cfb1c3ef":"### Other Metrics","e3a9222a":"##### Creating new column 'predicted' with 1 if Survival_Prob > 0.5 else 0","bea86b83":"### Finding Optimal Cutoff Point\nOptimal cutoff probability is that prob where we get balanced sensitivity and specificity","e46b4dca":"- In 1st Class, more number of people have survived than not survived\n- In 3rd Class, number of people who did not survive is way more than number of people who survived\n- In case of Males, more number of people have died than survived, while the opposite it True for Females.","e36cc3dc":"Plotting the ROC Curve","cc0cc671":"- Passengers who have no siblings or parents (are traveling alone) seem to have high survival rates.\n- More people in the 1st class with high fares have survived","c8833dba":"## Basic EDA and Data Visualization","997a6a08":"Creating a dataframe with the actual survival flag and the predicted probabilities","1f4e57ec":"## Making predictions on the test set","6ba38c78":"## Treating Missing Values","7f48e52c":"## Model Building","f199e478":"**In Test Set**\n- Age has 86 missing values, \n- Cabin has 327 missing values"}}