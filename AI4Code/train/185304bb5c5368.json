{"cell_type":{"92bc97dc":"code","8d75735f":"code","f9ca1830":"code","5db8e1c9":"code","1a2d0f63":"code","bdfa66d5":"code","a45107f5":"code","08193003":"code","e32b2f41":"code","f9da098d":"code","b82147a9":"markdown","a19bfcee":"markdown","2d87f346":"markdown","8eadeb2f":"markdown","80ec8c59":"markdown","d47e745f":"markdown"},"source":{"92bc97dc":"import matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\nimport time\nimport os\nimport PIL.Image as Image\nfrom IPython.display import display\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nprint(torch.cuda.get_device_name(device))","8d75735f":"dataset_dir = \"..\/input\/car_data\/car_data\/\"\n\ntrain_tfms = transforms.Compose([transforms.Resize((400, 400)),\n                                 transforms.RandomHorizontalFlip(),\n                                 transforms.RandomRotation(15),\n                                 transforms.ToTensor(),\n                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\ntest_tfms = transforms.Compose([transforms.Resize((400, 400)),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ndataset = torchvision.datasets.ImageFolder(root=dataset_dir+\"train\", transform = train_tfms)\ntrainloader = torch.utils.data.DataLoader(dataset, batch_size = 32, shuffle=True, num_workers = 2)\n\ndataset2 = torchvision.datasets.ImageFolder(root=dataset_dir+\"test\", transform = test_tfms)\ntestloader = torch.utils.data.DataLoader(dataset2, batch_size = 32, shuffle=False, num_workers = 2)\n","f9ca1830":"def train_model(model, criterion, optimizer, scheduler, n_epochs = 5):\n    \n    losses = []\n    accuracies = []\n    test_accuracies = []\n    # set the model to train mode initially\n    model.train()\n    for epoch in range(n_epochs):\n        since = time.time()\n        running_loss = 0.0\n        running_correct = 0.0\n        for i, data in enumerate(trainloader, 0):\n\n            # get the inputs and assign them to cuda\n            inputs, labels = data\n            #inputs = inputs.to(device).half() # uncomment for half precision model\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            \n            # forward + backward + optimize\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            # calculate the loss\/acc later\n            running_loss += loss.item()\n            running_correct += (labels==predicted).sum().item()\n\n        epoch_duration = time.time()-since\n        epoch_loss = running_loss\/len(trainloader)\n        epoch_acc = 100\/32*running_correct\/len(trainloader)\n        print(\"Epoch %s, duration: %d s, loss: %.4f, acc: %.4f\" % (epoch+1, epoch_duration, epoch_loss, epoch_acc))\n        \n        losses.append(epoch_loss)\n        accuracies.append(epoch_acc)\n        \n        # switch the model to eval mode to evaluate on test data\n        model.eval()\n        test_acc = eval_model(model)\n        test_accuracies.append(test_acc)\n        \n        # re-set the model to train mode after validating\n        model.train()\n        scheduler.step(test_acc)\n        since = time.time()\n    print('Finished Training')\n    return model, losses, accuracies, test_accuracies\n\n    ","5db8e1c9":"def eval_model(model):\n    correct = 0.0\n    total = 0.0\n    with torch.no_grad():\n        for i, data in enumerate(testloader, 0):\n            images, labels = data\n            #images = images.to(device).half() # uncomment for half precision model\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            outputs = model_ft(images)\n            _, predicted = torch.max(outputs.data, 1)\n            \n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    test_acc = 100.0 * correct \/ total\n    print('Accuracy of the network on the test images: %d %%' % (\n        test_acc))\n    return test_acc","1a2d0f63":"model_ft = models.resnet34(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\n\n# replace the last fc layer with an untrained one (requires grad by default)\nmodel_ft.fc = nn.Linear(num_ftrs, 196)\nmodel_ft = model_ft.to(device)\n\n# uncomment this block for half precision model\n\"\"\"\nmodel_ft = model_ft.half()\n\n\nfor layer in model_ft.modules():\n    if isinstance(layer, nn.BatchNorm2d):\n        layer.float()\n\"\"\"\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n\n\"\"\"\nprobably not the best metric to track, but we are tracking the training accuracy and measuring whether\nit increases by atleast 0.9 per epoch and if it hasn't increased by 0.9 reduce the lr by 0.1x.\nHowever in this model it did not benefit me.\n\"\"\"\nlrscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, threshold = 0.9)","bdfa66d5":"model_ft, training_losses, training_accs, test_accs = train_model(model_ft, criterion, optimizer, lrscheduler, n_epochs=10)","a45107f5":"# plot the stats\n\nf, axarr = plt.subplots(2,2, figsize = (12, 8))\naxarr[0, 0].plot(training_losses)\naxarr[0, 0].set_title(\"Training loss\")\naxarr[0, 1].plot(training_accs)\naxarr[0, 1].set_title(\"Training acc\")\naxarr[1, 0].plot(test_accs)\n\naxarr[1, 0].set_title(\"Test acc\")","08193003":"# tie the class indices to their names\n\ndef find_classes(dir):\n    classes = os.listdir(dir)\n    classes.sort()\n    class_to_idx = {classes[i]: i for i in range(len(classes))}\n    return classes, class_to_idx\nclasses, c_to_idx = find_classes(dataset_dir+\"train\")","e32b2f41":"# test the model on random images\n\n\n# switch the model to evaluation mode to make dropout and batch norm work in eval mode\nmodel_ft.eval()\n\n# transforms for the input image\nloader = transforms.Compose([transforms.Resize((400, 400)),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\nimage = Image.open(dataset_dir+\"test\/Mercedes-Benz C-Class Sedan 2012\/01977.jpg\")\nimage = loader(image).float()\nimage = torch.autograd.Variable(image, requires_grad=True)\nimage = image.unsqueeze(0)\nimage = image.cuda()\noutput = model_ft(image)\nconf, predicted = torch.max(output.data, 1)","f9da098d":"# get the class name of the prediction\ndisplay(Image.open(dataset_dir+\"test\/Mercedes-Benz C-Class Sedan 2012\/01977.jpg\"))\nprint(classes[predicted.item()], \"confidence: \", conf.item())","b82147a9":"### As we can see the model reached 90% training accuracy by epoch 10.\n","a19bfcee":"## Making a car classifier using Pytorch\n\nIn this notebook I'm making a car classifier using the Stanford car dataset, which contains 196 classes.\nI'll be using a pre-trained resnet34 with transfer learning to train the model. All layers will be fine tuned and the last fully connected layer will be replaced entirely.\n\nDataset (196 classes):\n\nTrain folder: 8144 images, avg: 41.5 images per class.\n\nTest folder: 8041 images, avg: 41.0 images per class.","2d87f346":"## Model training function\n\nHere we train our model, after each epoch, we test the model on the test data to see how it's going","8eadeb2f":"### Evaluate the model on single images (e.g for production)\n\nNext we can use the model on our own images. For that we need to tie the class numbers for which the model returns probablities with the names of those classes.\n","80ec8c59":"## Load the data and transform\n\nFirst, lets create some transforms for our data and load the train\/test data+labels from the folders.\n\nHere we are using 300x300 images with random horizontal flip, random rotation and normalization","d47e745f":"### Evaluate on training data\nThis function is called out after each epoch of training on the training data. We then measure the accuracy of the model."}}