{"cell_type":{"f6e9a0b0":"code","1f604769":"code","7bf9fd09":"code","c5b7a6fb":"code","cba1fb57":"code","470a247c":"code","4c51d2ac":"code","d9bed484":"code","7da4a601":"code","fab03e41":"code","a30e97eb":"code","18d08b98":"code","c740c746":"code","c2a4d6f1":"code","a86c17e4":"code","b66d23d6":"code","e07a1997":"code","96207599":"code","81576249":"code","104e067a":"code","a5f7cab1":"code","832fe05a":"code","a4b47163":"markdown","cf83fe65":"markdown","02fadad8":"markdown","87c0bf0c":"markdown","d6d9524f":"markdown","2bb20892":"markdown","8a81ec6e":"markdown","5b21c5d7":"markdown","3bc04f52":"markdown","c7a6b970":"markdown"},"source":{"f6e9a0b0":"import pandas as pd\n%config IPCompleter.use_jedi = False","1f604769":"df = pd.read_csv('\/kaggle\/input\/gufhtugu-publications-dataset-challenge\/GP Orders - 5.csv')\ndf.head()","7bf9fd09":"df.dropna(subset=[\"City\"], inplace=True)\ndf.City.isna().sum()","c5b7a6fb":"df[\"City\"] = df.City.astype(str)\nraw_cities = df.iloc[:,[0,4]].copy()\nraw_cities.columns = [\"oid\",\"city\"]\nraw_cities.city.value_counts()","cba1fb57":"N = 150\n\nbefore_ev = raw_cities.city.nunique()\nbefore_ev_p = raw_cities.city.value_counts()[:N].sum()\/raw_cities.shape[0]\n\nprint(f\"Initially we have {before_ev} unique values for city names.\")\nprint(f\"Top {N} cities with most ordes are {before_ev_p:.2f}% of the whole dataset\")","470a247c":"mask = raw_cities.city.str.contains(\"[a-zA-Z]\")\nurdu_names= raw_cities[~mask].city.unique()\nurdu_names.shape\nprint(\"we have {} unique city names in urdu, a sample is shown below:\\n{}\".format(len(urdu_names), urdu_names[:10]))","4c51d2ac":"from textblob import TextBlob\nfrom time import sleep","d9bed484":"to_en = {}\nfor x in urdu_names:\n    # sleep to not exceed the limit of requests\n    sleep(0.5)\n    try:\n        tr = TextBlob(x).translate().string\n        to_en[x] = tr\n#         print(x,\" - \", tr)\n    except:\n        pass\n#         print(x)\n","7da4a601":"len(to_en), to_en","fab03e41":"tr = raw_cities.city.replace(to_en)\nraw_cities[\"city\"] = tr\n\nafter_tr = raw_cities.city.nunique()\nafter_tr_p = raw_cities.city.value_counts()[:N].sum()\/raw_cities.shape[0]\n\nprint(f\"After translation, we have {after_tr} unique values for city names.\")\nprint(f\"Top {N} cities with most ordes are {after_tr_p:.2f}% of the whole dataset.\")","a30e97eb":"raw_cities[\"city\"] = raw_cities.city.str.strip().str.title()\n\nafter_cn = raw_cities.city.nunique()\nafter_cn_p = raw_cities.city.value_counts()[:N].sum()\/raw_cities.shape[0]\n\nprint(f\"After case normalization, we have {after_cn} unique values for city names.\")\nprint(f\"Top {N} cities with most ordes are {after_cn_p:.2f}% of the whole dataset.\")","18d08b98":"# list names with less than 5 characters\nmask = raw_cities.city.str.len() < 5\nraw_cities[mask].city.unique()","c740c746":"short_full = {\n    \"Rwp\": \"Rawalpindi\",\n    \"Isb\": \"Islamabad\",\n    \"Fsd\": \"Faisalabad\",\n    \"Khi\": \"Karachi\",\n    \"Lhr\": \"Lahore\",\n    \"D I Khan\": \"Dera ismail khan\",\n    \"G G Khan\": \"Dera ghazi khan\"\n}\n\nraw_cities[\"city\"] = raw_cities.city.replace(short_full)","c2a4d6f1":"from fuzzywuzzy import process, fuzz","a86c17e4":"# Create on dataframe for each province\n\nurls = {\n    \"kpk\": \"https:\/\/en.wikipedia.org\/wiki\/List_of_cities_in_Khyber_Pakhtunkhwa_by_population\",\n    \"balochistan\":\"https:\/\/en.wikipedia.org\/wiki\/List_of_cities_in_Balochistan,_Pakistan_by_population\",\n    \"punjab\":\"https:\/\/en.wikipedia.org\/wiki\/List_of_cities_in_Punjab,_Pakistan_by_population\",\n    \"sindh\": \"https:\/\/en.wikipedia.org\/wiki\/List_of_cities_in_Sindh_by_population\",\n    \"gb\": \"https:\/\/en.wikipedia.org\/wiki\/List_of_cities_in_Gilgit-Baltistan_by_population\"\n}\n\ntabel_idx = {\n    \"kpk\": 0,\n    \"balochistan\":0,\n    \"punjab\":0,\n    \"sindh\": 0,\n    \"gb\": 1\n}\n\ndf_list = {}\nfor pname in urls:\n    df_list[pname] = pd.read_html(urls[pname])[tabel_idx[pname]]","b66d23d6":"# Create list of cities and city-to-province mapping\n\ncities = pd.Series(\"Islamabad\",dtype = object)\ncity_prov = {\"Islamabad\":\"Federal\"}\nfor pname in df_list:\n    p = df_list[pname]\n    p.rename(columns = {\"City Name\": \"City\"}, inplace = True)\n    cities = cities.append(p.City, ignore_index=True)\n    city_prov.update({city:pname.title() for city in p.City})","e07a1997":"to_city = {}\nfor city in raw_cities.city.unique():\n    res, score, _ = process.extractOne(city, cities, scorer = fuzz.WRatio)\n    to_city[city] = f\"{res};{score}\"","96207599":"new_names = raw_cities.city.replace(to_city)\nraw_cities[[\"proposed_city_name\", \"similarity_score\"]] = new_names.str.split(\";\", n = 1, expand = True)\nraw_cities[\"similarity_score\"] = raw_cities.similarity_score.astype(int)\n\nraw_cities.head()","81576249":"cols = [\"city\", \"proposed_city_name\", \"similarity_score\"]\nmask_85 = raw_cities.similarity_score > 85\nmask = mask_85 & (raw_cities.similarity_score < 90)\nraw_cities[mask][cols]","104e067a":"raw_cities.loc[mask_85, \"city\"] = raw_cities[mask].proposed_city_name\nraw_cities.loc[mask_85, \"fuzzy_name\"] = True\nraw_cities.loc[~mask_85, \"fuzzy_name\"] = False\n\nprint(\"{} names are mapped to their fuzzy match.\".format(raw_cities.fuzzy_name.sum()))\n\nraw_cities.head()","a5f7cab1":"mask = raw_cities.similarity_score > 85\n\nraw_cities.loc[mask, \"city\"] = raw_cities[mask].proposed_city_name\nraw_cities.loc[mask, \"fuzzy_name\"] = True\nraw_cities.loc[~mask, \"fuzzy_name\"] = False\n\nprint(\"{} names are mapped to their fuzzy match.\".format(raw_cities.fuzzy_name.sum()))\nraw_cities","832fe05a":"final = raw_cities.city.nunique()\nfinal_p = raw_cities.city.value_counts()[:N].sum()\/raw_cities.shape[0]\n\nprint(f\"Finally, we have {final} unique values for city names.\")\nprint(f\"Top {N} cities are {final_p:.2f}% of the whole dataset.\")","a4b47163":"I could not find a way to replace abbreviations of city names with correct names.  \nI will manually replace city names like {\"lhr\", \"khi\"} to their full names.","cf83fe65":"# Translation from Urdu to English","02fadad8":"The real magic happens between the scores of 85 and 90, where spelling mistakes are corrected.  \nResults include some errors, which will cause noise in the data.  \nThreshold can be set according to a particular problem, we will rename all the cities where score is above 85.","87c0bf0c":"#### TextBlob was able to translate 122 out of 156 names. As expected, most of the translations are perfect.\nWe can replace these names in raw dataset.","d6d9524f":"#### Manually translating these names is time consuming, We can use a very famous module TextBlob which provides basic translation.\n","2bb20892":"# Fuzzy matching\n#### In this section I will try to correct the spelling mistakes and extract city names from detailed address.  \nIn particular, I will  \n- Create a List of cities using Wikipedia articles\n- Match city entries from raw data to newly created list","8a81ec6e":"## Results\n\nMore than the fact that we have lesser unique values, it is satisfying to see that almost 88% of the dataset has standard city names.  \nWe have made some errors but those city names were rarely present in our dataset.  \nThe results can be further improved by expanding our search for True values of city names.\n\n\n### If you find this report useful \ud83e\uddd0, please upvote \u261d. Adios. ","5b21c5d7":"#### To match two strings, we will use the weighted ratio score.   (details about Weighted ratio are  [here](https:\/\/stackoverflow.com\/questions\/31806695\/when-to-use-which-fuzz-function-to-compare-2-strings))","3bc04f52":"# In this report,\n### I will present a possible solution to reduce the number of unique city names in dataset.\n\n#### In particular, we will see:\n- How to translate from urdu to english using python modules\n- How to create pandas dataframes from wikipedia tabels\n- What is fuzzy match and how to find a best match for noisy categorical data\n","c7a6b970":"# Case Normalization\n#### We will convert all city names in title format to remove any difference w.r.t case sensitivity."}}