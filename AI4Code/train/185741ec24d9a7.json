{"cell_type":{"07f9fe3a":"code","1988402e":"code","6b63c43f":"code","bc373797":"code","5423f709":"code","ef9040df":"code","aebe5ffd":"code","8c5517f1":"code","761f3800":"code","85cdebd8":"code","d2ca7839":"code","c1614171":"code","2f9133d0":"code","4254e827":"code","8a7dc9e3":"code","c1412739":"code","884e87c1":"code","0317bed0":"code","a49857e3":"code","d9ca76c9":"code","e30aae5e":"code","ac719468":"code","1d63021f":"code","705b8ce6":"code","3ec1f6ef":"code","a282e6dd":"code","799a3672":"code","97b70d8c":"code","ea63d648":"code","9644d8a4":"code","9da824bd":"code","af712bc0":"code","ad217939":"code","5b5644a7":"code","c247c272":"code","cb579847":"code","4ddc0832":"code","a9b16fb0":"code","a77f8924":"code","12c4e526":"code","0b32da67":"code","857e3bef":"code","dd3a7601":"code","3b9e9216":"code","aa5caafc":"code","5c6133ee":"code","27873a15":"code","309dac26":"code","2e2b026d":"code","3fa20955":"code","68f51d6a":"code","c61b17e7":"code","2d033a83":"code","a7a2788c":"code","c9baf845":"code","0d051664":"code","de704364":"code","5ab66e52":"code","aaaf35f3":"code","06abfc6f":"code","3273efd5":"code","64bfbf1e":"code","c95bcb8f":"code","4cac22c2":"code","e1ceb474":"code","7c6fe649":"code","4d5da064":"code","ea82467c":"code","a2e21a73":"code","a9b2f39b":"code","88e4d319":"code","71d49728":"code","67d420f5":"code","098811ad":"code","ee15c74a":"code","f1d70916":"code","dab637b1":"code","eb0fb752":"code","7e53eb6f":"code","af476478":"code","e93eab40":"code","71b03ef0":"code","7f3d75c8":"code","60c88149":"code","ad9d3da4":"code","7f234580":"code","b78317a6":"code","16a2b787":"code","2bf960fe":"code","8b20c08c":"code","b91ead35":"code","331a6993":"code","86b991fd":"code","3879939a":"code","dce2a963":"code","9ba803df":"markdown","e89f88df":"markdown","3fb0e965":"markdown","2ed220d9":"markdown","04e466fd":"markdown","16681b54":"markdown","3d329e7b":"markdown","a3097cae":"markdown","767c078a":"markdown","8505a532":"markdown","433bb2af":"markdown","41b018ec":"markdown","406f4ee4":"markdown","8a3eb3e4":"markdown","d30c6bab":"markdown","c29e2913":"markdown","244d0409":"markdown","f7971243":"markdown","29b33f33":"markdown","35ec90ce":"markdown","229a3a00":"markdown","dcb3ee32":"markdown","e7403479":"markdown","912a2683":"markdown","fc5b37c7":"markdown","08e61fe0":"markdown","300eb39d":"markdown","b12feac3":"markdown","f863d155":"markdown","b3b33b9f":"markdown","ce551cd9":"markdown","45058b33":"markdown","e0e3d541":"markdown","9b74b1be":"markdown","99ec8a75":"markdown","52ddf05f":"markdown","949c5294":"markdown","a7b8598a":"markdown","3c220249":"markdown","68c729b3":"markdown","e085f65f":"markdown","43ff6b1e":"markdown","71eb9edc":"markdown","eb2d71b8":"markdown","0904c7d6":"markdown","cd609614":"markdown","240ff1d1":"markdown","1b0e0be8":"markdown","142925ed":"markdown","1b01484f":"markdown","5bba9c24":"markdown","73c10dd8":"markdown","c1bc8196":"markdown","37490cde":"markdown","9b4237d7":"markdown","5485d838":"markdown","84f7e95e":"markdown","c432a3a7":"markdown","e816056a":"markdown","63690c97":"markdown","dcce6d50":"markdown","c1104bff":"markdown","5a9cc696":"markdown","c5086e92":"markdown","2c55c57a":"markdown","ae59a025":"markdown","a3762faa":"markdown","6292092f":"markdown","7a19c53b":"markdown","8301c0e1":"markdown","b5b618da":"markdown","a8e5a79a":"markdown","362f58d7":"markdown","2a8018b7":"markdown","d428ad87":"markdown"},"source":{"07f9fe3a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport string\nimport random as rd\n\n#%matplotlib inline\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.lines import Line2D\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport math\nimport statistics\nimport sklearn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import manifold\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve,auc\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import fbeta_score, make_scorer\nfrom sklearn import tree as treepl\n\nseed = 45\nrd.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)","1988402e":"def cv_results(X_4_cv, y_4_cv, estimator_4_cv, n_splits_4_cv, seed_4_cv):\n\n    _X_cv = X_4_cv\n    _y_cv = y_4_cv\n    _estimator = sklearn.base.clone(estimator_4_cv)\n    _n_splits_4_cv = n_splits_4_cv\n    _shuff_seed = seed_4_cv\n    \n    _features = 'No names'\n    \n    if isinstance(_X_cv, pd.DataFrame):\n        _features = list(_X_cv.columns)\n        _X_cv = _X_cv.to_numpy()\n            \n    elif isinstance(_X_cv, pd.Series):\n        _features = _X_cv.name\n        _X_cv = _X_cv.to_numpy()\n      \n        \n    if isinstance(_y_cv, pd.DataFrame) or isinstance(_y_cv, pd.Series):\n        _y_cv = _y_cv.to_numpy()   \n    \n    print('cross validating with ', _n_splits_4_cv, ' folds\\n',\n          'model: ', _estimator.__class__.__name__, '\\n',\n          'with parameters:\\n', _estimator.get_params(), '\\n',\n          'dataset size is:\\n', _X_cv.shape[0], ' data points\\n',\n          _X_cv.shape[1], 'features\\n',\n          'feature names: \\n', _features)\n\n    kf = StratifiedKFold(n_splits=_n_splits_4_cv, random_state = _shuff_seed, shuffle = True)\n    \n    _probs = []\n    _mean_c_matr = np.zeros((2,2))\n    _mean_b_acc = []\n    _mean_b_prec = []\n    _mean_b_recall = []\n    _mean_b_f1 = []\n\n    \n    _mean_acc = []\n    _mean_prec = []\n    _mean_recall = []\n    _mean_f1 = []\n    _tprs = []\n    _aucs = []\n    _mean_fpr = np.linspace(0,1,100)   \n    \n    _i = 1\n\n    _T_space = np.linspace(0,1,100)\n    _acc_scores_curve_b = []\n    _acc_scores_curve = []\n    \n    fig, ax = plt.subplots(1,3, figsize = (20,5))\n    \n    for _train_ind, _test_ind in kf.split(_X_cv, _y_cv):\n        \n        _X_cv_train = _X_cv[_train_ind]\n        _X_cv_test = _X_cv[_test_ind]\n        _y_cv_train = _y_cv[_train_ind]\n        _y_cv_test = _y_cv[_test_ind]\n\n        _y_cv_test_weights = compute_sample_weight(class_weight='balanced', y=_y_cv_test)\n\n        _estimator.fit(_X_cv_train, _y_cv_train)\n        _y_cv_pred = _estimator.predict(_X_cv_test)\n        _y_cv_pred_proba = _estimator.predict_proba(_X_cv_test)\n        \n        _probs = _probs + list(_y_cv_pred_proba[:,1])\n        \n        _c_matr = 10*confusion_matrix(_y_cv_test, _y_cv_pred, labels = [1,0], normalize = 'all')\n                                   #sample_weight = _y_cv_test_weights)#)\n        _c_matr = _c_matr.T\n        \n        #c_matr = np.round((c_matr)*100)\n\n        #_mean_c_matr = np.round( _mean_c_matr + _c_matr\/kf.n_splits , 0)\n\n        _mean_c_matr = np.round( _mean_c_matr + _c_matr , 1)\n        \n        _mean_acc.append(accuracy_score(_y_cv_test, _y_cv_pred))\n        _mean_b_acc.append(accuracy_score(_y_cv_test, _y_cv_pred, \n                                        sample_weight = _y_cv_test_weights))\n        \n        _mean_prec.append(precision_score(_y_cv_test, _y_cv_pred))    \n        _mean_b_prec.append(precision_score(_y_cv_test, _y_cv_pred, \n                                          sample_weight = _y_cv_test_weights, labels = [1,0]))\n\n        _mean_recall.append(recall_score(_y_cv_test, _y_cv_pred))        \n        _mean_b_recall.append(recall_score(_y_cv_test, _y_cv_pred, \n                                         sample_weight = _y_cv_test_weights, labels = [1,0]))\n\n        _mean_f1.append(f1_score(_y_cv_test, _y_cv_pred))        \n        _mean_b_f1.append(f1_score(_y_cv_test, _y_cv_pred, \n                                 sample_weight = _y_cv_test_weights, labels = [1,0]))\n \n        _fpr, _tpr, _t = roc_curve(_y_cv_test, _y_cv_pred_proba[:, 1])\n\n        _tprs.append(np.interp(_mean_fpr, _fpr, _tpr))        \n\n        _roc_auc = auc(_fpr, _tpr)\n        _aucs.append(_roc_auc)\n        \n        ax[0].plot(_fpr, _tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (_i, _roc_auc))\n        \n        #print('fold ', _i, 'of ', _n_splits_4_cv)\n        _i= _i+1\n        \n        _acc_scores_b = []\n        _acc_scores = []\n        \n        for _T in _T_space:\n            \n            \n            _acc_scores.append(accuracy_score(_y_cv_test, \n                                              [1 if _m > _T else 0 for _m in _y_cv_pred_proba[:, 1]]))\n            _acc_scores_b.append(accuracy_score(_y_cv_test, \n                                              [1 if _m > _T else 0 for _m in _y_cv_pred_proba[:, 1]],\n                                              sample_weight = _y_cv_test_weights))\n\n        _acc_scores_curve.append(_acc_scores)    \n        _acc_scores_curve_b.append(_acc_scores_b)\n        \n        \n    _mean_acc = round(statistics.mean(_mean_acc), 3)\n    _mean_prec = round(statistics.mean(_mean_prec), 3)\n    _mean_recall = round(statistics.mean(_mean_recall), 3)\n    _mean_f1 = round(statistics.mean(_mean_f1), 3)\n    \n    _mean_b_acc = round(statistics.mean(_mean_b_acc), 3)\n    _mean_b_prec = round(statistics.mean(_mean_b_prec), 3)\n    _mean_b_recall = round(statistics.mean(_mean_b_recall), 3)\n    _mean_b_f1 = round(statistics.mean(_mean_b_f1), 3)\n \n    _mean_tpr = np.mean(_tprs, axis=0)\n    _mean_auc = auc(_mean_fpr, _mean_tpr)\n\n    \n    print()\n    print('cv mean confusion matrix (in % of all, not balanced):')\n    print(_mean_c_matr)\n    print()\n    print('cv balanced accuracy: ', _mean_b_acc )\n    print('cv balanced precision: ', _mean_b_prec )\n    print('cv balanced recall: ', _mean_b_recall )\n    print('cv balanced f1: ', _mean_b_f1 )\n    print()\n\n    print('cv accuracy: ', _mean_acc )\n    print('cv precision: ', _mean_prec )\n    print('cv recall: ', _mean_recall )\n    print('cv f1: ', _mean_f1 )\n    print('cv AUC: ', round(_mean_auc,3) )\n\n      \n    ax[0].plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n\n    ax[0].plot(_mean_fpr, _mean_tpr, color='blue',\n             label=r'Mean ROC (AUC = %0.2f )' % (_mean_auc),lw=2, alpha=1)\n\n    ax[0].set_xlabel('False Positive Rate')\n    ax[0].set_ylabel('True Positive Rate')\n    ax[0].set_title('ROC')\n    ax[0].legend(loc=\"lower right\")\n    ax[0].text(0.32,0.7,'More accurate area',fontsize = 12)\n    ax[0].text(0.63,0.4,'Less accurate area',fontsize = 12)\n    \n    ax[1].set_title('Balanced accuracy vs threshold')\n    ax[1].plot(_T_space, np.mean(_acc_scores_curve_b, axis=0))\n    \n    ax[2].grid(False)\n    ax[2].set_title('Accuracy vs threshold')\n    ax[2].hist(_probs, bins = 20, color='pink')\n    ax[2].set_ylabel('predicted probabilities', color = 'pink')\n    \n    ax2_t = ax[2].twinx()\n    ax2_t.plot(_T_space, np.mean(_acc_scores_curve, axis=0))\n     \n    \n    plt.show()\n    return None","6b63c43f":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\nprint('train_data shape = ', train_data.shape)\nprint('test_data shape = ', test_data.shape)\ntrain_data.head()","bc373797":"#data = all data, y = train targets. Will divide X on train and test after preproccessing data\ny = train_data['Survived']\n\ndata = pd.concat( [train_data.drop(columns = ['Survived']), test_data], axis = 0 )\n\nprint('all data shape = ', data.shape)\nprint('y shape = ', y.shape)\nprint('Fraction of passangers survived: ',\n      round(train_data[train_data['Survived'] == 1]['Survived'].count()\/\n            train_data['Survived'].count(),2))","5423f709":"fig, (ax1,ax2) = plt.subplots(1,2 , figsize = (20,7))\nsns.heatmap(data.corr(method = 'spearman'), annot = True, ax = ax1)\nax1.set_title('spearman')\nsns.heatmap(data.corr(method = 'pearson'), annot = True, ax = ax2)\nax2.set_title('pearson')\nplt.show()","ef9040df":"train_length = train_data.shape[0]\n#dm = data modified\n#concat train data with y in order to perform some plotting\ndm_train = pd.concat( [data.iloc[:train_length], y], axis = 1 )\ndm_test = data.iloc[train_length:]","aebe5ffd":"round(dm_train.groupby(['Pclass'])['Survived'].mean(),2)","8c5517f1":"round(dm_train.groupby(['Pclass', 'Sex'])['Survived'].mean(),2)","761f3800":"round(dm_train.groupby(['Embarked', 'Pclass'])['Survived'].mean(),2)","85cdebd8":"data.isna().sum()","d2ca7839":"def age_est_rec(age):\n    if (age - 0.5).is_integer():\n        return 1\n    elif pd.isna(age):\n        return 1\n    else:\n        return 0\n    \ndata['AgeEstRec'] = data['Age'].map(age_est_rec)","c1614171":"sns.set_style(\"whitegrid\")\nsns.pointplot(x=data['AgeEstRec'].iloc[:train_data.shape[0]], y = y, palette=\"muted\")\nplt.show()","2f9133d0":"data['Title'] = data['Name'].str.split(', ', expand = True)[1].str.split('.', expand = True)[0]","4254e827":"pd.crosstab(data['Title'], data['Sex'])","8a7dc9e3":"data[(data['Title'] == 'Dr') & (data['Sex'] == 'female')]","c1412739":"data.loc[(data['Title'] == 'Dr') & (data['Sex'] == 'female'), ['Title']] = 'Mrs'\n\ndata['Title'] = data['Title'].replace(['Capt', 'Col', 'Major', 'Rev', 'Don', 'Jonkheer','Sir', 'Dr'], 'Mr')\ndata['Title'] = data['Title'].replace(['Mlle'], 'Miss')\ndata['Title'] = data['Title'].replace(['Mme',  'Ms', 'Dona',  'Lady',  'the Countess'], 'Mrs')\n\npd.crosstab(data['Title'], data['Sex'])","884e87c1":"sns.pointplot(x=data['Title'].iloc[:train_data.shape[0]], y = y, palette=\"muted\")\nplt.show()","0317bed0":"data[data.Fare.isna()]","a49857e3":"data.index = range(data.shape[0])\ndata.at[1043, 'Fare'] = data[data['Pclass']==3].median()['Fare']","d9ca76c9":"data['Embarked'].fillna('S', inplace = True)\nsns.countplot(x = 'Embarked', data = data, hue = 'Pclass')\nplt.show()","e30aae5e":"def ticket_split(s):\n    \n    s = s.strip()\n    split = s.split()\n    \n    if len(split) == 1:\n        tnum = split[0]\n        #there are 4 strange ticket numbers\n        #that state 'LINE'. Assign them to 0\n        if tnum == 'LINE':\n            tnum = 0\n        tstr = 'NA'\n\n            \n    elif len(split) == 2:\n        tstr = split[0]\n        tnum = split[1]\n    else:\n        tstr = split[0] + split[1]\n        tnum = split[2]\n        \n    \n    tnum = int(tnum)\n\n    return tstr, tnum","ac719468":"data['TicketStr'], data['TicketNum'] = zip(*data['Ticket'].map(ticket_split))","1d63021f":"plt.figure(figsize = (8,3))\nsns.distplot(data[\"TicketNum\"])\nplt.show()","705b8ce6":"data[data['TicketNum']>2500000].head()","3ec1f6ef":"data[\"LogTicketNum\"] = data[\"TicketNum\"].map(lambda i: np.log(i) if i > 0 else 0)\nplt.figure(figsize = (5,3))\nsns.distplot(data[\"LogTicketNum\"])\nplt.show()","a282e6dd":"print('correlation of LogTicketNum feature with others:')\nround(data.corr().iloc[:,-1],2).sort_values(ascending = False)","799a3672":"print('tickets count: ', (data['TicketNum']).count())\nprint('unique ticket numbers: ', np.unique(data['TicketNum']).shape[0])\n\n#create ticket frequency coloumn\ndata['TicketFreq'] = data.groupby(['TicketNum', 'TicketStr', 'Fare'])['TicketNum'].transform('count')\nprint('Amount of tickets with frequency > 1:', data[data.TicketFreq>1].TicketNum.count())","97b70d8c":"data['FamSize'] = data['SibSp'] + data['Parch']","ea63d648":"fig, ax = plt.subplots(2,2, figsize = (15,8))\n\nplt.subplot(2,2,1)\nsns.countplot(data[\"FamSize\"], color=\"g\", label=\"Famsize\").legend(loc=\"best\")\nplt.subplot(2,2,2)\nsns.countplot(data[\"SibSp\"], color=\"lightblue\",  label=\"SibSp\").legend(loc=\"best\")\nplt.subplot(2,2,4)\nsns.countplot(data[\"Parch\"], color=\"pink\",  label=\"Parch\").legend(loc=\"best\")\nplt.subplot(2,2,3)\nsns.countplot(data[\"TicketFreq\"], color=\"y\",  label=\"TicketFreq\").legend(loc=\"best\")\nplt.show()","9644d8a4":"train_length = train_data.shape[0]\nfig, ax = plt.subplots(2,2, figsize = (15,8))\nplt.subplot(2,2,4)\nsns.pointplot(x = data[:train_length]['Parch'], y = y, palette = 'muted')\nplt.subplot(2,2,2)\nsns.pointplot(x = data[:train_length]['SibSp'], y = y, palette = 'muted')\nplt.subplot(2,2,3)\nsns.pointplot(x = data[:train_length]['FamSize'], y = y, palette = 'muted')\nplt.subplot(2,2,1)\nsns.pointplot(x = data[:train_length]['TicketFreq'], y = y, palette = 'muted')\nplt.show()","9da824bd":"def family_size(fam):\n    if fam < 1:\n        return 0\n    elif fam < 2:\n        return 1\n    elif fam < 3:\n        return 2\n    elif fam < 4:\n        return 3\n    else:\n        return 4   \n    \n    \ndata['ModFamSize'] = data['FamSize'].map(family_size)\ndata['ModTicketFreq'] = data['TicketFreq'] - 1\ndata['ModTicketFreq'] = data['ModTicketFreq'].map(family_size)\ndata['ModSibSp'] = data['SibSp'].map(family_size)\ndata['ModParch'] = data['Parch'].map(family_size)","af712bc0":"fig, ax = plt.subplots(2,2, figsize = (15,8))\nplt.subplot(2,2,1)\nsns.countplot(data[\"ModFamSize\"], color=\"g\", label=\"Famsize\").legend(loc=\"best\")\nplt.subplot(2,2,2)\nsns.countplot(data[\"ModSibSp\"], color=\"lightblue\",  label=\"SibSp\").legend(loc=\"best\")\nplt.subplot(2,2,4)\nsns.countplot(data[\"ModParch\"], color=\"pink\",  label=\"Parch\").legend(loc=\"best\")\nplt.subplot(2,2,3)\nsns.countplot(data[\"ModTicketFreq\"], color=\"y\",  label=\"TicketFreq\").legend(loc=\"best\")\nplt.show()","ad217939":"train_length = train_data.shape[0]\nfig, ax = plt.subplots(2,2, figsize = (15,8))\nplt.subplot(2,2,4)\nsns.pointplot(x = data[:train_length]['ModParch'], y = y, palette = 'muted')\nplt.subplot(2,2,2)\nsns.pointplot(x = data[:train_length]['ModSibSp'], y = y, palette = 'muted')\nplt.subplot(2,2,3)\nsns.pointplot(x = data[:train_length]['ModFamSize'], y = y, palette = 'muted')\nplt.subplot(2,2,1)\nsns.pointplot(x = data[:train_length]['ModTicketFreq'], y = y, palette = 'muted')\nplt.show()","5b5644a7":"data['CabinNA'] = np.where(data['Cabin'].isna(), 1, 0)\n#fill all missing cabin with M\ndata.Cabin.fillna('NA', inplace = True)\n#extract cabin letters\ndata['Floor'] = data['Cabin'].str.extract('([A-Za-z]+)')\nprint('Floor letters:')\nprint(pd.unique(data['Floor']))","c247c272":"data.groupby('Floor').count()['PassengerId']","cb579847":"#join T with A because they vere close, and there are too few T passengers\nT_index = data[data.Floor == 'T'].index\ndata.at[T_index, 'Floor'] = 'A'","4ddc0832":"plt.figure(figsize = (20,5))\nsns.countplot(x='Floor', hue='Pclass', data = data, palette=\"muted\")\nplt.show()","a9b16fb0":"plt.figure(figsize = (20,5))\nsns.countplot(x='Floor', hue='Pclass', data = data[data['Floor']!='NA'], palette=\"muted\")\nplt.show()","a77f8924":"print('we have ', data['Age'].isna().sum(), 'NaN entries in Age')\ngrouped = data.groupby(['Title', 'Pclass', 'ModSibSp', \n                            'ModParch'])['Age']\nfor name, group in grouped:\n    if (group.shape[0] > 5) & (group.isna().sum()>0):\n        data['Age'].iloc[list(group.index)] = group.fillna(group.median())\nprint('grouped by Title, Pclass, ModSibSp, ModParch, replaced NaN with group median for groups > 5')\nprint('we have ', data['Age'].isna().sum(), 'NaN entries in Age now')","12c4e526":"data['Age'] = data.groupby(['Title', 'Pclass',])['Age'].apply(lambda x: x.fillna(x.median()))\nprint('grouped by Title, Pclass replaced NaN with group median')\nprint('we have ', data['Age'].isna().sum(), 'NaN entries in Age now')","0b32da67":"g = sns.FacetGrid(data, col='Title', margin_titles=True, sharey = False, sharex = False)\ng.map(plt.hist, \"Age\", color=\"steelblue\")\nplt.show()","857e3bef":"fig, ax = plt.subplots(1,2, figsize = (12,3))\nax[0].hist(data[ (data['Title'] == 'Mr') & (data['Age'] < 20) & (data['AgeEstRec']==0) ]['Age'])\nax[1].hist(data[ (data['Title'] == 'Mr') & (data['Age'] < 20) & (data['AgeEstRec']==1) ]['Age'])\nax[0].set_title('Mr, Age < 19, age not reconstructed')\nax[1].set_title('Mr, Age < 19, age reconstructed')\nplt.show()","dd3a7601":"fig, ax = plt.subplots(1,3, figsize = (15,3))\nplt.subplot(1,3,1)\nsns.distplot(data[\"Fare\"])\nplt.subplot(1,3,2)\ndata['ModFare'] = data['Fare']\/data['TicketFreq']\nsns.distplot(data[\"ModFare\"])\nplt.subplot(1,3,3)\ndata[\"LogFare\"] = data[\"ModFare\"].map(lambda i: np.log(i) if i > 0 else 0)\nsns.distplot(data[\"LogFare\"])\nplt.show()","3b9e9216":"fig, (ax1,ax2) = plt.subplots(1,2, figsize = (15,3))\n\nax1.scatter(data[:train_length]['LogFare'], data[:train_length]['Pclass'])\nax1.set_title('Train data')\nax1.set_xlabel('Log Fare')\nax1.set_ylabel('Pclass')\n\nax2.scatter(data[train_length:]['LogFare'], data[train_length:]['Pclass'])\nax2.set_title('Test data')\nax2.set_xlabel('Log Fare')\nax2.set_ylabel('Pclass')\nplt.show()","aa5caafc":"data.loc[ ((data['LogFare']>2.5) | (data['LogFare']<1)) & (data['Pclass']==3), ['ModFare'] ] = \\\ndata[data['Pclass']==3]['ModFare'].median()\n\ndata.loc[ (data['LogFare']<1) & (data['Pclass']==2), ['ModFare'] ] = \\\ndata[data['Pclass']==2]['ModFare'].median()\n\ndata.loc[ (data['LogFare']<2) & (data['Pclass']==1), ['ModFare'] ] = \\\ndata[data['Pclass']==1]['ModFare'].median()\n\ndata[\"LogFare\"] = data[\"ModFare\"].map(lambda i: np.log(i) if i > 0 else 0)","5c6133ee":"fig, (ax1,ax2) = plt.subplots(1,2, figsize = (15,3))\n\nax1.scatter(data[:train_length]['LogFare'], data[:train_length]['Pclass'])\nax1.set_title('Train data')\nax1.set_xlabel('Log Fare')\nax1.set_ylabel('Pclass')\n\nax2.scatter(data[train_length:]['LogFare'], data[train_length:]['Pclass'])\nax2.set_title('Test data')\nax2.set_xlabel('Log Fare')\nax2.set_ylabel('Pclass')\nplt.show()","27873a15":"plt.figure(figsize = (5,3))\nsns.distplot(data[\"Age\"])\nplt.show()","309dac26":"data['ModFareCut'] = pd.qcut(data['ModFare'], 7)\ndata['AgeCut'] = pd.qcut(data['Age'], 9)","2e2b026d":"train_length = train_data.shape[0]\n#dm = data modified\n#concat train data with y in order to perform some plotting\ndm_train = pd.concat( [data.iloc[:train_length], y], axis = 1 )\ndm_test = data.iloc[train_length:]","3fa20955":"fig, ax = plt.subplots(4,1, figsize = (10,14))\nfig.subplots_adjust(hspace = 0.4)\nplt.subplot(4,1,1)\nsns.pointplot(x='ModFareCut', y = 'Survived', data = dm_train, palette=\"muted\").set_title('Log fare per passenger')\nplt.subplot(4,1,2)\nsns.pointplot(x='AgeCut', y = 'Survived', data = dm_train, palette=\"muted\").set_title('all Age')\nplt.subplot(4,1,3)\nsns.pointplot(x='AgeCut', y = 'Survived',\n              data = dm_train[dm_train['AgeEstRec'] == 1], palette=\"muted\").set_title('Age reconstructed')\nplt.subplot(4,1,4)\nsns.pointplot(x='AgeCut', y = 'Survived',\n              data = dm_train[dm_train['AgeEstRec'] == 0], palette=\"muted\").set_title('Age known')\nplt.show()","68f51d6a":"fig, ax = plt.subplots(2,1, figsize = (10,7))\nfig.subplots_adjust(hspace = 0.4)\nplt.subplot(2,1,1)\nsns.pointplot(x='AgeCut', y = 'Survived', \n              data = dm_train[ (dm_train['AgeEstRec']==0) &\n                               (dm_train['Title']!='Mr')], palette=\"muted\").set_title('Mr, Age known')\nplt.subplot(2,1,2)\nsns.countplot(x='AgeCut', hue = 'Survived', \n              data = dm_train[ (dm_train['AgeEstRec']==1) &\n                               (dm_train['Title']=='Mr')], palette=\"muted\").set_title('Mr, Age reconstructed')\n\nplt.show()","c61b17e7":"features = ['Pclass', 'Sex', 'Embarked', 'AgeEstRec', \n            'Title', 'CabinNA', 'ModFamSize', 'ModTicketFreq', 'ModFareCut', 'AgeCut', 'Floor']\nfig, ax = plt.subplots(4,3, figsize = (20,15))\nfig.subplots_adjust(hspace = 0.3,wspace = 0.3)\nfig.suptitle('train data survived distribution', fontsize=16)\n\nfor i, feature in enumerate(features):\n\n    i += 1\n    plt.subplot(4,3,i)\n    sns.countplot(x=feature, hue='Survived', data = dm_train, palette=\"muted\")\n    plt.legend(loc='upper right')\n    \nplt.show()","2d033a83":"features = ['Pclass', 'Sex', 'Embarked', 'AgeEstRec', \n            'Title', 'CabinNA', 'ModFamSize', 'ModTicketFreq', 'ModFareCut', 'AgeCut', 'Floor']\nfig, ax = plt.subplots(4,3, figsize = (20,15))\nfig.subplots_adjust(hspace = 0.3,wspace = 0.3)\nfig.suptitle('train data survived percentage with bootstrapped confidence interval', fontsize=16)\n\nfor i, feature in enumerate(features):\n\n    i += 1\n    plt.subplot(4,3,i)\n    sns.pointplot(x=feature, y = 'Survived', data = dm_train, palette=\"muted\")\n    \nplt.show()","a7a2788c":"#encode Title by survival rate\ndata['TitleDigit'] = data['Title'].map({'Mr': 1, 'Master': 2, 'Miss': 3, 'Mrs': 4})","c9baf845":"data['FloorDigit'] = data['Floor'].map({'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F':6, 'G':7, 'NA':8})","0d051664":"train_length = train_data.shape[0]\n#dm = data modified\n#concat train data with y in order to perform some plotting\ndm_train = pd.concat( [data.iloc[:train_length], y], axis = 1 )\ndm_test = data.iloc[train_length:]","de704364":"fig, ax = plt.subplots(1,2,figsize = (13,4))\nplt.subplot(1,2,1)\nsns.pointplot('TitleDigit', 'Survived', data = dm_train, palette = 'muted')\nplt.subplot(1,2,2)\nsns.pointplot('FloorDigit', 'Survived', data = dm_train, palette = 'muted')\nplt.show()","5ab66e52":"features_to_onehot = ['Pclass', 'Embarked', 'Title', 'Floor']\n\nenc = OneHotEncoder(sparse = False)\n\nfor feature in features_to_onehot:\n    \n    encoded = pd.DataFrame( enc.fit_transform(data[feature].to_numpy().reshape(-1,1)) )\n    encoded.columns = feature + ' ' + enc.get_feature_names()\n    \n    data = pd.concat([data, encoded], axis = 1)","aaaf35f3":"feat_to_drop = ['PassengerId','Name', 'Sex', 'Ticket', 'Cabin', 'Embarked', 'Title',\n       'TicketStr','Floor','ModFareCut', 'AgeCut']\ndata.drop(columns = feat_to_drop, inplace=True)\nprint('we have ', len(data.columns), ' features now')\n","06abfc6f":"data.columns","3273efd5":"train_length = train_data.shape[0]\nX = data.iloc[:train_length]\nX_submit = data.iloc[train_length:]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify = y, random_state = seed)","64bfbf1e":"cmap1 = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"red\",\"blue\"])\n\nfig, ax = plt.subplots(1,2, figsize = (20,7))\nfig.suptitle('Survival vs LogFare\/Age')\nscatter_fem = ax[0].scatter(dm_train[dm_train['Sex'] == 'female']['Age'], \n                          dm_train[dm_train['Sex'] == 'female']['LogFare'], \n                          c = y[dm_train['Sex'] == 'female'], cmap = cmap1)\nlegend1 = ax[0].legend(*scatter_fem.legend_elements(), loc=\"upper left\", title=\"Survived women\")\n\nax[0].set_ylabel('LogFare')\nax[0].set_xlabel('Age')\nscatter_men = ax[1].scatter(dm_train[dm_train['Sex'] == 'male']['Age'], \n                          dm_train[dm_train['Sex'] == 'male']['LogFare'], \n                          c = y[dm_train['Sex'] == 'male'], cmap = cmap1)\nlegend2 = ax[1].legend(*scatter_men.legend_elements(), loc=\"upper left\", title=\"Survived men\")\nax[1].set_ylabel('LogFare')\nax[1].set_xlabel('Age')\n\nplt.show()","c95bcb8f":"tsne_features = ['Pclass', 'Age',  'AgeEstRec',\n       'LogTicketNum', 'TicketFreq',\n       'SibSp', 'Parch', 'ModFare', 'FloorDigit',\n       'Embarked x0_C', 'Embarked x0_Q', 'Embarked x0_S', \n       'Title x0_Master','Title x0_Miss', 'Title x0_Mr', 'Title x0_Mrs']\n\nfig, (ax1,ax2) = plt.subplots(1,2 , figsize = (20,7))\nsns.heatmap(data[tsne_features].corr(method = 'spearman'), annot = False, ax = ax1)\nax1.set_title('spearman')\nsns.heatmap(data[tsne_features].corr(method = 'pearson'), annot = False, ax = ax2)\nax2.set_title('pearson')\nplt.show()\n\n","4cac22c2":"data_4tsne = data[tsne_features]\n\n#now scale\n\nscaler = StandardScaler()\nscaler.fit(data_4tsne)\ndata_4tsne_scaled_np = scaler.transform(data_4tsne)\ndata_4tsne_scaled = pd.DataFrame(data = data_4tsne_scaled_np, \n                                 index = data_4tsne.index, columns = data_4tsne.columns) \n\n#tSNE\n\n\ntsne2 = manifold.TSNE( **{'n_components': 2, 'perplexity': 50,\n                          'learning_rate': 230, 'init': 'pca',\n                          'random_state': seed, 'n_jobs': -1} )\n\n\ndata_2d_tsne = tsne2.fit_transform(data_4tsne_scaled)\n\ntrain_length = train_data.shape[0]\nX_train_2d_tsne = data_2d_tsne[:train_length,:]\nX_test_2d_tsne = data_2d_tsne[train_length:,:]\n","e1ceb474":"cmap1 = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"red\",\"blue\"])\n\nfig = plt.figure(figsize = (10,8))\nax = fig.add_subplot(1, 1, 1)\n\ntr = ax.scatter(X_train_2d_tsne[:, 0], X_train_2d_tsne[:, 1], c = y, cmap = cmap1)\nte = ax.scatter(X_test_2d_tsne[:, 0], X_test_2d_tsne[:, 1], c = 'g', alpha = 0.4)\n\n\nlegend_elements = [Line2D([0], [0], marker='o', color='w', label='Dead', markerfacecolor='r'),\n                   Line2D([0], [0], marker='o', color='w', label='Survived', markerfacecolor='b'),\n                   Line2D([0], [0], marker='o', color='w', label='test data', markerfacecolor='g')]\n\nax.legend(handles=legend_elements)\n\nplt.show()","7c6fe649":"knn_data = data_2d_tsne\n\nknn_train_data = knn_data[:train_length]\nknn_test_data = knn_data[train_length:]\n\nKNN = KNeighborsClassifier()\n\nkNN_params = {'n_neighbors' : range(1,30,2),\n              'weights' : ['uniform'],\n              'p' : [2],\n              'n_jobs' : [-1]}\n\ngrid_cv = GridSearchCV(KNN, kNN_params, scoring = 'accuracy', cv = 10, verbose = 1, n_jobs = -1)\n\ngrid_cv.fit(knn_train_data, y)\n\nprint(grid_cv.best_params_, grid_cv.best_score_)\n","4d5da064":"knn_best_pars = {'n_jobs': -1, 'n_neighbors': 19, 'p': 2, 'weights': 'uniform'}\n\n#KNN1 = KNeighborsClassifier(**grid_cv.best_params_)\n\nKNN1 = KNeighborsClassifier(**knn_best_pars)\n\ncv_results(knn_train_data, y, KNN1, 10, 15)","ea82467c":"KNN1.fit(knn_train_data, y)\ny_knn_sub_pr = KNN1.predict_proba(knn_test_data)\ny_knn_sub = [1 if m > 0.6 else 0  for m in y_knn_sub_pr[:,1] ]\n\ntest_data2 = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\noutput = pd.DataFrame({'PassengerId': test_data2.PassengerId, 'Survived': y_knn_sub})\noutput.to_csv('Titanic_submission_KNN.csv', index=False)","a2e21a73":"tree_features = ['Title x0_Mr', 'Pclass x0_3', 'TicketFreq', 'CabinNA', 'LogTicketNum','LogFare', 'Floor x0_E']\n\nX_tree = X[tree_features]\nX_submit_tree = X_submit[tree_features]\n\nX_train_tree = X_train[tree_features]\nX_test_tree = X_test[tree_features] ","a9b2f39b":"fig, (ax1,ax2) = plt.subplots(1,2 , figsize = (20,7))\nsns.heatmap(data[tree_features].corr(method = 'spearman'), annot = True, ax = ax1)\nax1.set_title('spearman')\nsns.heatmap(data[tree_features].corr(method = 'pearson'), annot = True, ax = ax2)\nax2.set_title('pearson')\nplt.show()","88e4d319":"tree = DecisionTreeClassifier(random_state = seed)\n    \nparameters = {'criterion': ['entropy', 'gini'],\n              'splitter' : ['best'],\n              'min_samples_leaf': list(range(3,9,2)),\n              'min_impurity_decrease' : list(np.linspace(0.00001,0.01,100)),             \n              'max_features': [None],\n              'class_weight' : ['balanced'],\n              'random_state' : [seed]}\n\ngrid_cv = GridSearchCV(tree, parameters, scoring = 'accuracy', cv = 5, verbose = 1, n_jobs = -1)\n\ngrid_cv.fit(X_train_tree, y_train)\nprint('best accuracy: ', grid_cv.best_score_)                          \nprint(grid_cv.best_params_)\n\nprint('\\n depth: ', grid_cv.best_estimator_.get_depth())\nbest_tree_params = grid_cv.best_params_","71d49728":"best_tree_params = {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': None,\n                    'min_impurity_decrease': 0.004853636363636364, 'min_samples_leaf': 3, \n                    'random_state': 45, 'splitter': 'best'}","67d420f5":"best_tree_impurity = best_tree_params['min_impurity_decrease']\nbest_tree_min_samples_leaf = best_tree_params['min_samples_leaf']\nbest_tree_crit = best_tree_params['criterion']\n\nbest_tree = DecisionTreeClassifier(**best_tree_params)\nbest_tree.fit(X_train_tree, y_train)\nprint('depth ',best_tree.get_depth())","098811ad":"tree_importances = pd.Series(best_tree.feature_importances_,X_train_tree.columns).sort_values(ascending=False)\nprint(tree_importances.loc[tree_importances > 0])\ntree_importances.loc[tree_importances > 0].index","ee15c74a":"plt.figure(figsize=(6,6))\npd.Series(best_tree.feature_importances_,X_train_tree.columns).sort_values(ascending=True).plot.barh(width=0.8)\nplt.show()","f1d70916":"plt.figure(figsize = (50,50))\nplt.style.use('default')\na = treepl.plot_tree(best_tree, filled = True, rounded = True,\n                     feature_names = tree_features, class_names = ['Dead', 'Survived'])\nsns.set_style(\"whitegrid\")","dab637b1":"imp_list = np.linspace(0.00001,0.01,100)\n\nimp_train_acc_list = []\nimp_test_acc_list = []\n\ndepth_list = []\n\nfor imp in imp_list:\n\n    parameters = {'criterion': best_tree_crit,\n                  'splitter' : 'best',\n                  'min_impurity_decrease' : imp,\n                  #'max_depth': list(range(1,12)),\n                  #'min_samples_split': list(range(2,20,2)),\n                  'min_samples_leaf': best_tree_min_samples_leaf,\n                  'max_features': None,\n                  #'max_leaf_nodes': (list(range(2,60,2)) + [None]),\n                  'class_weight' : 'balanced',\n                  'random_state' : seed}\n\n    imp_tree = DecisionTreeClassifier(**parameters)\n\n    imp_tree.fit(X_train_tree,y_train)\n    \n    acc_sc_train = accuracy_score(y_train, imp_tree.predict(X_train_tree))\n    acc_sc_test = accuracy_score(y_test, imp_tree.predict(X_test_tree))\n\n    imp_train_acc_list.append(acc_sc_train)\n    imp_test_acc_list.append(acc_sc_test)\n    \n    depth_list.append(imp_tree.get_depth())\n    \n\nfig = plt.figure()\nax = fig.add_subplot(1,1,1)\n\nax.plot(imp_list,imp_train_acc_list, label = 'accuracy train')\nax.plot(imp_list,imp_test_acc_list, label = 'accuracy test')\n\nax2 = ax.twinx()\nax2.plot(imp_list,depth_list, color = 'pink', label = 'tree depth')\n\nax.legend()\nax.set_title('accuracy vs min_impurity_decrease')\nax2.legend(loc='upper right', bbox_to_anchor=(1, 0.8))\nplt.show()","eb0fb752":"leaf_list = range(1,10)\n\nleaf_train_acc_list = []\nleaf_test_acc_list = []\n\ndepth_list = []\n\nfor leaf in leaf_list:\n\n    parameters = {'criterion': best_tree_crit,\n                  'splitter' : 'best',\n                  'min_impurity_decrease' : best_tree_impurity,\n                  #'max_depth': list(range(1,12)),\n                  #'min_samples_split': list(range(2,20,2)),\n                  'min_samples_leaf': leaf,\n                  'max_features': None,\n                  #'max_leaf_nodes': (list(range(2,60,2)) + [None]),\n                  'class_weight' : 'balanced',\n                  'random_state' : seed}\n\n\n    \n    leaf_tree = DecisionTreeClassifier(**parameters)\n    \n    leaf_tree.fit(X_train_tree,y_train)\n\n    \n    leaf_train_acc_list.append(accuracy_score(y_train, leaf_tree.predict(X_train_tree)))\n    \n    leaf_test_acc_list.append(accuracy_score(y_test, leaf_tree.predict(X_test_tree)))\n    \n    #depth_list.append(imp_tree.get_depth())\n    \nfig = plt.figure()\nax = fig.add_subplot(1,1,1)\nax.set_title('accuracy vs min_samples_leaf')\n\nplt.plot(leaf_list,leaf_train_acc_list, label = 'train')\nplt.plot(leaf_list,leaf_test_acc_list, label = 'test')\nplt.legend()\nplt.show()","7e53eb6f":"cv_results(X_train_tree, y_train, best_tree, 10, 15)","af476478":"best_tree.fit(X_train_tree, y_train)\n\ny_tree_test_pred_prob = best_tree.predict_proba(X_test_tree)[:,1]\n\ntree_acc_scores = []\n\nfor thr in np.linspace(0,1,100):\n    y_tree_test_pred = [1 if m > thr else 0 for m in y_tree_test_pred_prob]\n    tree_acc_score = accuracy_score(y_test, y_tree_test_pred)\n    tree_acc_scores.append(tree_acc_score)\n\nprint('accuracy with 0.5 threshold', \n      accuracy_score(y_test, [1 if m > 0.5 else 0 for m in best_tree.predict_proba(X_test_tree)[:,1]]))\n\nfig = plt.figure(figsize = (4,3))\nax = fig.add_subplot(1,1,1)\nax.set_title('accuracy vs threshold on test')\nplt.plot(np.linspace(0,1,100), tree_acc_scores)\nplt.show()\n","e93eab40":"best_tree.fit(X_tree, y)\n\ny_tree_sub = best_tree.predict_proba(X_submit_tree)\ny_tree_sub_tr = [1 if m > 0.7 else 0 for m in y_tree_sub[:, 1]]\n\ntest_data2 = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\noutput = pd.DataFrame({'PassengerId': test_data2.PassengerId, 'Survived': y_tree_sub_tr})\noutput.to_csv('Titanic_submission_tree.csv', index=False)","71b03ef0":"forest_features = [\n    \n        'Age',\n\n        'ModSibSp',\n        'ModTicketFreq',\n\n        'LogFare',\n\n        'LogTicketNum',\n\n        'AgeEstRec',\n\n        'TitleDigit',\n\n        'FloorDigit', \n\n        'Pclass']\n\n\n#split data to train and test\nX_forest = X[forest_features]\nX_submit_forest = X_submit[forest_features]\n\nX_train_forest = X_train[forest_features]\nX_test_forest = X_test[forest_features] ","7f3d75c8":"fig, (ax1,ax2) = plt.subplots(1,2 , figsize = (20,7))\nsns.heatmap(data[forest_features].corr(method = 'spearman'), annot = True, ax = ax1)\nax1.set_title('spearman')\nsns.heatmap(data[forest_features].corr(method = 'pearson'), annot = True, ax = ax2)\nax2.set_title('pearson')\nplt.show()","60c88149":"imp_list = np.linspace(0.00001,0.01,30)\n\nimp_train_acc_list = []\nimp_train_oob_list = []\nimp_test_acc_list = []\n\nfor imp in imp_list:\n    \n    #print('min_impurity_decrease: ', round(imp,4))\n    parameters = {'random_state': 45,\n                  'n_estimators': 2700,\n                  'min_samples_leaf': 1, \n                  'min_impurity_decrease': imp,\n                  'max_samples': None,\n                  'max_features': 6,\n                  'criterion': 'entropy',\n                  'class_weight': 'balanced',\n                  'oob_score' : True,\n                  'n_jobs' : -1\n                 }\n    \n    imp_forest = RandomForestClassifier(**parameters)\n    \n    imp_forest.fit(X_train_forest,y_train)\n\n    f_depth = 0\n    for f_tree in imp_forest.estimators_:\n        f_depth = f_depth + f_tree.get_depth()\n        \n    #print('average forest depth: ', round(f_depth\/len(imp_forest.estimators_),1))\n    \n    \n    imp_train_acc_list.append(accuracy_score(y_train, imp_forest.predict(X_train_forest)))\n    \n    imp_train_oob_list.append(imp_forest.oob_score_)\n                              \n    imp_test_acc_list.append(accuracy_score(y_test, imp_forest.predict(X_test_forest)))\n    \n\nfig = plt.figure()\nax = fig.add_subplot(1,1,1)\n\nax.plot(imp_list,imp_train_acc_list, label = 'accuracy on train')\nax.plot(imp_list,imp_train_oob_list, label = 'oob accuracy on train')\nax.plot(imp_list,imp_test_acc_list, label = 'accuracy on test')\n\n\nax.legend()\nax.set_title('accuracy vs min_impurity_decrease')\nplt.show()","ad9d3da4":"\nn_feat = len(forest_features)\nn_feat_start = round(math.sqrt(n_feat),0)\n#print('max_features start: ', n_feat_start)\n\nfeat_list = range(int(n_feat_start),n_feat)\n\nfeat_train_acc_list = []\nfeat_train_oob_list = []\nfeat_test_acc_list = []\n\nfor feat in feat_list:\n    \n    #print('max_features: ', feat)\n    parameters = {'random_state': 45,\n                  'n_estimators': 2700,\n                  'min_samples_leaf': 1, \n                  'min_impurity_decrease': 0.0045,\n                  'max_samples': None,\n                  'max_features': feat,\n                  'criterion': 'entropy',\n                  'class_weight': 'balanced',\n                  'oob_score' : True,\n                  'n_jobs' : -1\n                 }\n    \n    feat_forest = RandomForestClassifier(**parameters)\n\n    feat_forest.fit(X_train_forest,y_train)\n\n    \n    feat_train_acc_list.append(accuracy_score(y_train, feat_forest.predict(X_train_forest)))\n    \n    feat_train_oob_list.append(feat_forest.oob_score_)\n                              \n    feat_test_acc_list.append(accuracy_score(y_test, feat_forest.predict(X_test_forest)))\n    \n\nfig = plt.figure()\nax = fig.add_subplot(1,1,1)\n\nax.plot(feat_list,feat_train_acc_list, label = 'accuracy train')\nax.plot(feat_list,feat_train_oob_list, label = 'oob train')\nax.plot(feat_list,feat_test_acc_list, label = 'accuracy test')\n\n\nax.legend()\nax.set_title('accuracy vs number of features')\nplt.show()","7f234580":"\nbest_params = {'random_state': 45, 'n_estimators': 2700, 'min_samples_leaf': 1, \n 'min_impurity_decrease': 0.0045, 'max_samples': None,\n 'max_features': 6, 'criterion': 'entropy', 'class_weight': 'balanced', 'n_jobs' : -1}\n\nbest_params['oob_score'] = True\nprint(best_params)\n","b78317a6":"best_forest = RandomForestClassifier(**best_params)\n                              \nbest_forest.fit(X_train_forest, y_train)\nprint('oob accuracy score for chosen parameters: ', best_forest.oob_score_)","16a2b787":"f_depth = 0\nfor f_tree in best_forest.estimators_:\n    f_depth = f_depth + f_tree.get_depth()\nprint('average forest depth: ', round(f_depth\/len(best_forest.estimators_),1))","2bf960fe":"cv_results(X_train_forest, y_train, best_forest, 10, 15)","8b20c08c":"forest_importn = pd.Series(best_forest.feature_importances_,X_train_forest.columns).sort_values(ascending=False)\nprint(forest_importn.loc[forest_importn > 0])\nforest_importn.loc[forest_importn > 0].index","b91ead35":"plt.figure(figsize = (5,5))\npd.Series(best_forest.feature_importances_,X_train_forest.columns).sort_values(ascending=True).plot.barh(width=0.8)\nplt.show()","331a6993":"best_forest.fit(X_train_forest, y_train)\n\ny_forest_test_pred_prob = best_forest.predict_proba(X_test_forest)[:,1]\n\nforest_acc_scores = []\n\nfor thr in np.linspace(0,1,100):\n    y_forest_test_pred = [1 if m > thr else 0 for m in y_forest_test_pred_prob]\n    forest_acc_score = accuracy_score(y_test, y_forest_test_pred)\n    forest_acc_scores.append(forest_acc_score)\n\nprint('accuracy score with 0.5 threshold: ',\n      accuracy_score(y_test, [1 if m > 0.5 else 0 for m in best_forest.predict_proba(X_test_forest)[:,1]]))\n\nfig = plt.figure(figsize = (3,3))\nax = fig.add_subplot(1,1,1)\nax.set_title('accuracy vs threshold on test')\nplt.plot(np.linspace(0,1,100), forest_acc_scores)\n\nplt.show()","86b991fd":"best_forest.fit(X_forest, y)","3879939a":"y_forest_sub = best_forest.predict_proba(X_submit_forest)\ny_forest_sub_tr = [1 if m > 0.7 else 0 for m in y_forest_sub[:, 1]]\n\ntest_data2 = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\noutput = pd.DataFrame({'PassengerId': test_data2.PassengerId, 'Survived': y_forest_sub_tr})\noutput.to_csv('Titanic_submission_forest.csv', index=False)","dce2a963":"a = pd.DataFrame(columns=['kNN', 'Tree', 'Forest'], \n                 index = ['accuracy','precision','recall', 'f1', 'ROC AUC', 'public score'])\n\na.loc['accuracy'] = [0.817,0.795,0.828]\na.loc['precision'] = [0.786,0.725,0.787]\na.loc['recall'] = [0.722,0.762,0.758]\na.loc['f1'] = [0.749,0.74,0.771]\na.loc['ROC AUC'] = [0.871,0.85,0.885]\na.loc['public score'] = [0.80861,0.80382,0.80382]\n\na","9ba803df":"**Create feature \"Family size\" by adding SibSp and Parch**","e89f88df":"Ticket numbers have huge variance. Let's logarithm them for the sake of visualisation.","3fb0e965":"![forest.png](attachment:forest.png)","2ed220d9":"This is my first Kaggle project and I am new to ML<br>\nI have not taken proper python courses yet, so the code is not beautiful nor it is well documented.<br>\nMy score on leaderboard shows 0.82, this happened occasionaly when I run someones notebook and submitted. I have never achieved this score. My highest score is 0.808.<br>\nI did not group passengers by their family name and ticket in order to count group survival rates and create a feature based on that.<br>\nSome people say it increaces score by 0.3, but I just don't feel like it.<br>\nInteresting things to be found here are enhanced model metrics including accuracy vs threshold graphs and a funny t-SNE gif.<br>\nOk, let's go!","04e466fd":"**Let's see if passengers can be separated by Age and Fare**","16681b54":"Passengers with unknown age are less likely to survive","3d329e7b":"**Make an overall plot for all features**","a3097cae":"**First create feature AgeEstRec. 0 if age was known, and 1 if age was NaN or estimated (=xx.5 from data description)**","767c078a":"**Join train and test data**","8505a532":"**Cut train data into train and test, X_submit is data with unknown labels that we have to find out**","433bb2af":"There are Misters younger than 19 and even 15. Let's have a look at them.","41b018ec":"Now it looks nice and neat","406f4ee4":"**Survival rate by class, sex and embarkation**<br>","8a3eb3e4":"**Some correlation matrices again.**<br>\nI don't know if those make any sense","d30c6bab":"**Load data**","c29e2913":"**the tree gets 0.80382 public score**","244d0409":"**I do a cv parameters search based on min_impurity_decrease**<br>\nA curious fact:<br>\nThough it is not documented, if max_leaf_nodes is not set, \na DepthFirstTreeBuilder will be used to fit the underlying\ntree object; if it is, then a BestFirstTreeBuilder will be used;\nthis difference results in trees of different depths being generated.<br>\nhttps:\/\/stackoverflow.com\/questions\/56132387\/sklearn-tree-","f7971243":"# Fare","29b33f33":"One Doctor was female, just curious who she was","35ec90ce":"**Group together passengers with 4 or more family members in order to cut off those long tails.**","229a3a00":"It is slightly correlated with Pclass meaning 1 class passengers had smaller ticket numbers then the others, as well as more expencive tickets had smaller numbers. Well, maybe ticket numbers do contain some information.","dcb3ee32":"**Here I obtain feature importances for my tree**","e7403479":"# Floor\nThere were several floors on the Titanic, each with its own letter.","912a2683":"Divide Fare by TicketFrequency in order to get fare per passenger (create new ModFare feature)<br>\nThen create LogFare feature by logarythming ModFare","fc5b37c7":"![tree.png](attachment:tree.png)","08e61fe0":"**Correlation matrices. Pclass, Age and Fare are correlated as well as Sibsp and Parch**<br>\nThe older - the richer","300eb39d":"**Encode Title and Floor with numbers**","b12feac3":"![knn.png](attachment:knn.png)","f863d155":"**The tree itself**","b3b33b9f":"**my metrics function again**<br>\nwe can see that tree is more certain than kNN, it has less probabilities in range (0.2 - 0.7)","ce551cd9":"# Simple tree\nI selected features based on their importance (see below)","45058b33":"# Ticket","e0e3d541":"**best results were shown with threshold = 0.7**","9b74b1be":"**Extract ticket numbers. Somehow descigion trees find this feature useful**","99ec8a75":"**Here are some graphs of model performance depending on parameter values**","52ddf05f":"**There are outliers with Fare = 0 and 3 class passengers with high fare. Let's replace it with median**","949c5294":"# Done with features","a7b8598a":"**kNN on t-SNE gets 0.80861 public score**","3c220249":"What is Accuracy vs threshold?<br>\nI take estimator predicted probabilities (predict_proba) and make predictions in the following way:<br>\nI predict 1 if estimator predicted probability is greater than the theshold an 0 if less<br>\nThis way I can maximize accuracy by choosing a proper threshold<br>\nIn case of kNN I choose 0.6 threshold<br>\nPink bars are counts of estimator predictions with certain probabilities, in other words - predicted probabilities distribution","68c729b3":"**Import libraries**","e085f65f":"**Forest**","43ff6b1e":"## Fill NaN","71eb9edc":"It can seem that I should have chosen min_impurity_decrease based on best performance on test, somewhere close to 0.0028, where I get almost 0.88 accuracy on test, but it is overfitting for test data and will show poor results on public.","eb2d71b8":"**Forest scores 0.80382 on public, the same as tree**","0904c7d6":"**Need to extract title. It is needed to reconstruct missing Age**","cd609614":"**Some ticket numbers are duplicated. It means passengers with same ticket numbers form a family or a group.**<br>\nI create a feature named TicketFreq based on that<br>\nThere are passengers with same Family names and close ticket numbers, but counting their family size based on this information did not improve model performance.<br>","240ff1d1":"# The overall table for kNN, Tree and Forest\n**and pictures too, all obtained on 10 fold cross validation**<br>\nI just collected all the data in one table manually, too lazy to make changes in the code\n","1b0e0be8":"Floor is known for mostly 1 class passengers. But if it is known for 2 and 3 class it can be valuable information.","142925ed":"**drop features not to be used**","1b01484f":"**Tree**","5bba9c24":"# Forest here\n**features were selected based on estimator feature importance**","73c10dd8":"**Fill one empty fare cell with average fare of that class**","c1bc8196":"It seems like all misters with missing Age got their reconstructed Age equal to 25","37490cde":"# Conclusion\nAs we can see, random forest gets best scores on cross validation, but kNN gets lucky on public data.<br>\n","9b4237d7":"Filled two missing Embarked values with info from EncyclopediaTitanica. Yes, it is a cheat.","5485d838":"Misters obviously suck at surviving the Titanic.","84f7e95e":"**Feature correlation matrices**<br>\nAgain, lots of correlated features. Deal with it.","c432a3a7":"**Missing values**","e816056a":"**I choose 0.7 threshold for submission**","63690c97":"**We need something better to separate them. t-SNE!**<br>\nI select some features for tsne and look at their correlation first.<br>\nSome of them are heavily correlatad, but removing correlated features did not improve performance, so I kept them.","dcce6d50":"**Big and ugly function to show model metrics on cross validation, is called \"cv_results\"**<br>\nIt returns accuracy, ROC curves and so on, will see it in action later","c1104bff":"Let's explore those greater than 2 500 000. They all embarked in Southampton and were 3 class.","5a9cc696":"# Age NaN Filled here\nAge is correlated with Title (Miss-Mrs, Mr-Master), Pclass and Age (the older the wealthier), and SibSp and Parch.<br>\nThus I group by those features and count median. For groups with less than 5 members I do it again, but group only by Title and Pclass.","c5086e92":"**Below is a link to a gif for 2D t-SNE with different perplexity values. It was recorded for a different feature set, so it is different from the above picture**<br>\nThe gif is 12Mb, so kaggle won't allow me to include it in the kernel directly.\nhttps:\/\/ibb.co\/2jXkyyw","2c55c57a":"**Here are results for the chosen tree on the test set**","ae59a025":"**11 NN shows best performance on cross validation, but 19 NN gets best public score**<br>\nNow cv_results function defined at the very top of this notebook takes action","a3762faa":"**join small title groups with big ones**","6292092f":"**Forest feature importances**","7a19c53b":"**Best results on cv are shown by 'min_impurity_decrease' = 0.00223<br>**\nBut in order to make the model less overfitted I have chosen a higher value to make my tree smaller<br>\nThis way I get a tree with depth 5 instead of 7<br>\nThe value is chosen almost by guessing","8301c0e1":"**Now let's explore fare and age survival distribution by cutting into quantiles**","b5b618da":"**accuracy score on test data depending on threshold**","a8e5a79a":"# kNN on t-SNE\n**Search parameters grid for best number of neighbours on cross validation**<br>\nI search only for odd NN because we have a two-class classification problem","362f58d7":"**kNN**","2a8018b7":"**One-hot encoding for categorial features**","d428ad87":"**Forest parameters search**<br>\nAs with tree, my stopping criteria for the forest is min_impurity_decrease. It seems more precise than max_depth.<br>\nI have chosen max_features = 6 out of 9 to my forest more random and less overfitted<br>\nAlso, I choose min_samples_leaf = 1, because in the forest case I don't care if a single tree will turn out to be overfitted. The whole forest will be ok.<br>\nThe search takes a lot of time due to 2700 trees in the forest. Go smoke a cigarette."}}