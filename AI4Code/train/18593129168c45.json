{"cell_type":{"b69a9a4f":"code","a983afe8":"code","cc22a55d":"code","af14fa2c":"code","8092c601":"code","950d5218":"code","40f16dad":"code","f94e034c":"code","a1402b33":"code","93c0bfa1":"code","e4423190":"code","5755e729":"code","0096559e":"code","0f4bae1a":"code","591c9c2d":"code","f1a5d4db":"code","3f410d2a":"code","0b9ac24b":"code","bbc100d7":"code","446600fd":"code","eef5cd01":"code","4d509af0":"code","c254fbfc":"code","14613fcf":"code","d2f93a01":"code","933ef191":"code","b260f112":"code","fb34ea21":"code","6e5618e6":"code","264252ad":"code","a17cbe7c":"code","0a1857f6":"code","d7f9280a":"code","a160386f":"code","9373fa6f":"code","9afb544a":"code","83d78636":"code","1e9ee3f9":"code","af5aa1b5":"code","1910eee7":"code","2842c965":"code","d805ea52":"code","0fe34fb7":"code","bbd1769f":"code","8353fd2c":"code","5c48a9a7":"code","58170418":"code","52da9a82":"code","cc8ba6f5":"markdown","e0869b2e":"markdown","79457aa1":"markdown","acaab4fc":"markdown","c3fe5c8e":"markdown","ad945d37":"markdown","48adaf87":"markdown","feb19ea6":"markdown","c6917ef3":"markdown","b28f9b78":"markdown","df4c7109":"markdown","c6a3975f":"markdown","234c03f9":"markdown","d013b08f":"markdown","2fd9e31d":"markdown","705bc8b7":"markdown","a479f3fd":"markdown","43c57e89":"markdown","ab7beeb8":"markdown","72bdcb30":"markdown","598653ce":"markdown","8bc506c3":"markdown","a81713ba":"markdown","61d5b7d2":"markdown","d112752a":"markdown","82972fec":"markdown","96529bda":"markdown"},"source":{"b69a9a4f":"#Importing libraries\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\nimport pprint, time","a983afe8":"# reading the Treebank tagged sentences\ndata = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))","cc22a55d":"# let's check some of the tagged data\nprint(data[:10])","af14fa2c":"# split data into training and validation set in the ratio 95:5\nrandom.seed(1234)\ntrain_set, test_set = train_test_split(data, train_size=0.95, test_size=0.05)\n\nprint(\"Training Set Length -\", len(train_set))\nprint(\"Testing Set Length -\", len(test_set))\nprint(\"-\" * 100)\nprint(\"Training Data -\\n\")\nprint(train_set[:10])","8092c601":"# Getting list of train and test tagged words\ntrain_tagged_words = [tup for sent in train_set for tup in sent]\nprint(\"Train Tagged Words - \", len(train_tagged_words))\n\ntest_tagged_words = [tup[0] for sent in test_set for tup in sent]\nprint(\"Train Tagged Words - \", len(test_tagged_words))","950d5218":"# Let's have a look at the tagged words in the training set\ntrain_tagged_words[:10]","40f16dad":"# tokens in the train set - train_tagged_words\ntrain_tagged_tokens = [tag[0] for tag in train_tagged_words]\ntrain_tagged_tokens[:10]","f94e034c":"# POS tags for the tokens in the train set - train_tagged_words\n\ntrain_tagged_pos_tokens = [tag[1] for tag in train_tagged_words]\ntrain_tagged_pos_tokens[:10]","a1402b33":"# building the train vocabulary to a set\ntraining_vocabulary_set = set(train_tagged_tokens)","93c0bfa1":"# building the POS tags to a set\ntraining_pos_tag_set = set(train_tagged_pos_tokens)","e4423190":"# let's check how many unique tags are present in training data\nprint(len(training_pos_tag_set))","5755e729":"# let's check how many words are present in vocabulary\nprint(len(training_vocabulary_set))","0096559e":"# compute emission probability for a given word for a given tag\ndef word_given_tag(word, tag, train_bag = train_tagged_words):\n    tag_list = [pair for pair in train_bag if pair[1] == tag]\n    tag_count = len(tag_list)    \n    word_given_tag_list = [pair[0] for pair in tag_list if pair[0] == word]    \n    word_given_tag_count = len(word_given_tag_list)    \n    \n    return (word_given_tag_count, tag_count)","0f4bae1a":"# compute transition probabilities of a previous and next tag\ndef t2_given_t1(t2, t1, train_bag = train_tagged_words):\n    tags = [pair[1] for pair in train_bag]\n    \n    t1_tags_list = [tag for tag in tags if tag == t1]\n    t1_tags_count = len(t1_tags_list)\n    \n    t2_given_t1_list = [tags[index+1] for index in range(len(tags)-1) if tags[index] == t1 and tags[index+1] == t2]\n    t2_given_t1_count = len(t2_given_t1_list)\n    \n    return(t2_given_t1_count, t1_tags_count)","591c9c2d":"# computing P(w\/t) and storing in [Tags x Vocabulary] matrix. This is a matrix with dimension\n# of len(training_pos_tag_set) X en(training_vocabulary_set)\n\nlen_pos_tags = len(training_pos_tag_set)\nlen_vocab = len(training_vocabulary_set)","f1a5d4db":"# creating t x t transition matrix of training_pos_tag_set\n# each column is t2, each row is t1\n# thus M(i, j) represents P(tj given ti)\n\ntags_matrix = np.zeros((len_pos_tags, len_pos_tags), dtype='float32')\nfor i, t1 in enumerate(list(training_pos_tag_set)):\n    for j, t2 in enumerate(list(training_pos_tag_set)): \n        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]\/t2_given_t1(t2, t1)[1]","3f410d2a":"# convert the matrix to a df for better readability\ntags_df = pd.DataFrame(tags_matrix, columns = list(training_pos_tag_set), index=list(training_pos_tag_set))","0b9ac24b":"# Let's have a glimpse into the transition matrix\ntags_df","bbc100d7":"# Importing libraries for heatmap\nimport matplotlib.pyplot as plt\nimport seaborn as sns","446600fd":"# heatmap of tags matrix\n# T(i, j) means P(tag j given tag i)\nplt.figure(figsize=(14, 8))\nsns.heatmap(tags_df, annot = True)\nplt.show()","eef5cd01":"# frequent tags\n# filter the df to get P(t2, t1) > 0.5\ntags_frequent = tags_df[tags_df>0.5]\nplt.figure(figsize=(14, 8))\nsns.heatmap(tags_frequent, annot = True)\nplt.show()","4d509af0":"# Vanilla Viterbi Algorithm\ndef Vanilla_Viterbi(words, train_bag = train_tagged_words):\n    state = []\n    \n    T = list(set([pair[1] for pair in train_bag]))\n    \n    for key, word in enumerate(words):\n        #initialise list of probability column for a given observation\n        p = [] \n        for tag in T:\n            if key == 0:\n                transition_p = tags_df.loc['.', tag]\n            else:\n                transition_p = tags_df.loc[state[-1], tag]\n                \n            # compute emission and state probabilities\n            emission_p = word_given_tag(words[key], tag)[0]\/word_given_tag(words[key], tag)[1]\n            state_probability = emission_p * transition_p    \n            p.append(state_probability)\n            \n        pmax = max(p)\n        # getting state for which probability is maximum\n        state_max = T[p.index(pmax)] \n        state.append(state_max)\n    return list(zip(words, state))","c254fbfc":"# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n\nrandom.seed(1234)\n\n# choose random 5 sents\nrndom = [random.randint(1, len(test_set)) for x in range(5)]\n\n# list of sents\ntest_run = [test_set[i] for i in rndom]\n\n# list of tagged words\ntest_run_base = [tup for sent in test_run for tup in sent]\n\n# list of untagged words\ntest_tagged_words = [tup[0] for sent in test_run for tup in sent]","14613fcf":"# tagging the test sentences\nstart = time.time()\ntagged_seq = Vanilla_Viterbi(test_tagged_words)\nend = time.time()\ndifference = end-start\n\nprint(\"Time taken in seconds: \", difference)\n\n# accuracy\nvanilla_viterbi_word_check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \nvanilla_viterbi_accuracy = len(vanilla_viterbi_word_check)\/len(tagged_seq) * 100\nprint('Vanilla Viterbi Algorithm Accuracy: ', vanilla_viterbi_accuracy)","d2f93a01":"# let's check the incorrectly tagged words\nincorrect_tagged_words = [j for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0] != j[1]]\n\nprint(\"Total Incorrect Tagged Words :\", len(incorrect_tagged_words))\nprint(\"\\n\")\nprint(\"Incorrect Tagged Words :\", incorrect_tagged_words)","933ef191":"# Unknown words \n\ntest_vocabulary_set = set([t for t in test_tagged_words])\n\nunknown_words = list(test_vocabulary_set - training_vocabulary_set)\nprint(\"Total Unknown words :\", len(unknown_words))\nprint(\"\\n\")\nprint(\"Unknown Words :\", unknown_words)","b260f112":"# Lexicon (or unigram tagger)\n\nunigram_tagger = nltk.UnigramTagger(train_set)\naccuracy_unigram_tagger = unigram_tagger.evaluate(test_set)\nprint(\"The accuracy of the Unigram Tagger is -\", accuracy_unigram_tagger)","fb34ea21":"# patterns for tagging using a rule based regex tagger -\n\npatterns = [\n    (r'^[aA-zZ].*[0-9]+','NOUN'),  # Alpha Numeric\n    (r'.*ness$', 'NOUN'),\n    (r'.*\\'s$', 'NOUN'),              # possessive nouns\n    (r'.*s$', 'NOUN'),                # plural nouns\n    (r'.*', 'NOUN'),    \n    (r'.*ly$', 'ADV'),\n    (r'^(0|([*|-|$].*))','X'), # Any special character combination\n    (r'.*ould$', 'X'), # modals\n    (r'(The|the|A|a|An|an)$', 'DET'),\n    (r'^([0-9]|[aA-zZ])+\\-[aA-zZ]*$','ADJ'),\n    (r'.*able$', 'ADJ'), # adjective like 100-megabytes 237-Seats\n    (r'[aA-zZ]+(ed|ing|es)$', 'VERB'), # Any word ending with 'ing' or 'ed' is a verb\n    (r'[0-9].?[,\\\/]?[0-9]*','NUM')# Numbers \n    ]","6e5618e6":"# rule based tagger\n\nrule_based_tagger = nltk.RegexpTagger(patterns)\n\n# unigram tagger backed up by the rule-based tagger\nrule_based_unigram_tagger = nltk.UnigramTagger(train_set, backoff = rule_based_tagger)\n\naccuracy_rule_based_unigram_tagger = rule_based_unigram_tagger.evaluate(test_set)\n\nprint(\"The accuracy of the Unigram Tagger backed up by the RegexpTagger is -\", accuracy_rule_based_unigram_tagger)","264252ad":"# Bigram tagger\n\nbigram_tagger = nltk.BigramTagger(train_set, backoff=rule_based_unigram_tagger)\nbigram_tagger.evaluate(test_set)\naccuracy_bigram_tagger = bigram_tagger.evaluate(test_set)\nprint(accuracy_bigram_tagger)","a17cbe7c":"# Trigram tagger\n\ntrigram_tagger = nltk.TrigramTagger(train_set, backoff = bigram_tagger)\ntrigram_tagger.evaluate(test_set)\naccuracy_trigram_tagger = trigram_tagger.evaluate(test_set)\nprint(\"The accuracy of the Trigram Tagger backed up by the bigram_tagger is -\", accuracy_trigram_tagger)","0a1857f6":"# use transition probability of tags when emission probability is zero (in case of unknown words)\n\ndef Vanilla_Viterbi_for_Unknown_Words(words, train_bag = train_tagged_words):\n    state = []\n    T = list(set([pair[1] for pair in train_bag]))\n    \n    for key, word in enumerate(words):\n        #initialise list of probability column for a given observation\n        p = [] \n        p_transition =[] # list for storing transition probabilities\n        for tag in T:\n            if key == 0:\n                transition_p = tags_df.loc['.', tag]\n            else:\n                transition_p = tags_df.loc[state[-1], tag]\n                \n            # compute emission and state probabilities\n            emission_p = word_given_tag(words[key], tag)[0]\/word_given_tag(words[key], tag)[1]\n            state_probability = emission_p * transition_p    \n            p.append(state_probability)\n            p_transition.append(transition_p)\n            \n        pmax = max(p)\n        state_max = T[p.index(pmax)] \n        \n      \n        # if probability is zero (unknown word) then use transition probability\n        if(pmax==0):\n            pmax = max(p_transition)\n            state_max = T[p_transition.index(pmax)]\n                           \n        else:\n            state_max = T[p.index(pmax)] \n        \n        state.append(state_max)\n    return list(zip(words, state))","d7f9280a":"# tagging the test sentences\nstart = time.time()\ntagged_seq = Vanilla_Viterbi_for_Unknown_Words(test_tagged_words)\nend = time.time()\ndifference = end-start\n\nprint(\"Time taken in seconds: \", difference)\n\n# accuracy\ncheck = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \naccuracy = len(check)\/len(tagged_seq)\nprint('Vanilla Viterbi for Unknown Words Accuracy: ',accuracy*100)","a160386f":"# lets create a list containing tuples of POS tags and POS tag occurance probability, based on training data\ntag_prob = []\ntotal_tag = len([tag for word,tag in train_tagged_words])\nfor t in training_pos_tag_set:\n    each_tag = [tag for word,tag in train_tagged_words if tag==t]\n    tag_prob.append((t,len(each_tag)\/total_tag))\n\ntag_prob","9373fa6f":"# use transition probability of tags when emission probability is zero (in case of unknown words)\n\ndef Vanilla_Viterbi_for_Unknown_Words_Modified(words, train_bag = train_tagged_words):\n    state = []\n    T = list(set([pair[1] for pair in train_bag]))\n    \n    for key, word in enumerate(words):\n        #initialise list of probability column for a given observation\n        p = [] \n        p_transition =[] # list for storing transition probabilities\n       \n        for tag in T:\n            if key == 0:\n                transition_p = tags_df.loc['.', tag]\n            else:\n                transition_p = tags_df.loc[state[-1], tag]\n                \n            # compute emission and state probabilities\n            emission_p = word_given_tag(words[key], tag)[0]\/word_given_tag(words[key], tag)[1]\n            state_probability = emission_p * transition_p    \n            p.append(state_probability)\n            \n            # find POS tag occurance probability\n            tag_p = [pair[1] for pair in tag_prob if pair[0]==tag ]\n            \n            # calculate the transition prob weighted by tag occurance probability.\n            transition_p = tag_p[0]*transition_p             \n            p_transition.append(transition_p)\n            \n        pmax = max(p)\n        state_max = T[p.index(pmax)] \n        \n      \n        # if probability is zero (unknown word) then use weighted transition probability\n        if(pmax==0):\n            pmax = max(p_transition)\n            state_max = T[p_transition.index(pmax)]                 \n                           \n        else:\n            state_max = T[p.index(pmax)] \n        \n        state.append(state_max)\n    return list(zip(words, state))","9afb544a":"# tagging the test sentences\nstart = time.time()\ntagged_seq = Vanilla_Viterbi_for_Unknown_Words_Modified(test_tagged_words)\nend = time.time()\ndifference = end-start\n\nprint(\"Time taken in seconds: \", difference)\n\n# accuracy\nviterbi_word_check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \naccuracy_viterbi_modified = len(viterbi_word_check)\/len(tagged_seq) * 100\nprint('Modified Vanilla Viterbi for Unknown Words Accuracy: ', accuracy_viterbi_modified)","83d78636":"# A trigram tagger backed off by a rule based tagger.\n\ndef trigram_tagger(word, train_set = train_set):\n    \n    patterns = [\n    (r'[aA-zZ]+(ed|ing|es)$', 'VERB'), # Any word ending with 'ing' or 'ed' is a verb\n\n    (r'.*ly$', 'ADV'),\n        \n    (r'^([0-9]|[aA-zZ])+\\-[aA-zZ]*$','ADJ'),\n    (r'.*able$', 'ADJ'), \n    (r'.*ful$', 'ADJ'),\n    (r'.*ous$', 'ADJ'),\n        \n    (r'^[aA-zZ].*[0-9]+','NOUN'),     # Alpha Numeric\n    (r'.*ness$', 'NOUN'),\n    (r'.*\\'s$', 'NOUN'),              # possessive nouns - words ending with 's\n    (r'.*s$', 'NOUN'),                # plural nouns\n    (r'.*ers$', 'NOUN'),              # eg.- kinderganteners, autobioghapgers\n    (r'.*ment$', 'NOUN'),\n    (r'.*town$', 'NOUN'),\n        \n    (r'^(0|([*|-|$].*))','X'), # Any special character combination\n    (r'.*ould$', 'X'),\n        \n    (r'(The|the|A|a|An|an|That|that|This|this|Those|those|These|these)$', 'DET'), # That\/this\/these\/those belong to the category of Demonstrative determiners\n    (r'[0-9].?[,\\\/]?[0-9]*','NUM'), # Numbers \n        \n    (r'.*', 'NOUN')\n    ]\n\n    regex_based_tagger = nltk.RegexpTagger(patterns)\n\n    # trigram backed up by the regex tagger\n    trigram_regex_tagger = nltk.TrigramTagger(train_set, backoff = regex_based_tagger)\n    return trigram_regex_tagger.tag_sents([[(word)]])","1e9ee3f9":"# viterbi with handling for unknown words from regex tagger\n\ndef Viterbi_Trigram_Tagger(words, train_bag = train_tagged_words):\n    state = []\n    T = list(set([pair[1] for pair in train_bag]))\n    \n    for key, word in enumerate(words):\n        # unknown words from trigram taggr\n        if word not in training_vocabulary_set:\n            unk_word_tag=trigram_tagger(word)\n            for sent in unk_word_tag:\n                for tup in sent:\n                    state.append(tup[1])\n        # rest remains same            \n        else:            \n            p = [] \n            for tag in T:\n                if key == 0:\n                    transition_p = tags_df.loc['.', tag]\n                else:\n                    transition_p = tags_df.loc[state[-1], tag]\n                \n            # compute emission and state probabilities\n                emission_p = word_given_tag(words[key], tag)[0]\/word_given_tag(words[key], tag)[1]\n                state_probability = emission_p * transition_p    \n                p.append(state_probability)\n            \n            pmax = max(p)\n            # getting state for which probability is maximum\n            state_max = T[p.index(pmax)] \n            state.append(state_max)\n            \n    return list(zip(words, state))","af5aa1b5":"# tagging the test sentences\nstart = time.time()\ntagged_seq = Viterbi_Trigram_Tagger(test_tagged_words)\nend = time.time()\ndifference = end-start\n\nprint(\"Time taken in seconds: \", difference)\n\n# accuracy\nviterbi_trigram_word_check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \nviterbi_trigram_accuracy = len(viterbi_trigram_word_check)\/len(tagged_seq) * 100\nprint('Modified Viterbi Trigram Tagger Accuracy: ', viterbi_trigram_accuracy)","1910eee7":"acccuracy_data = [['Vanilla Viterbi', vanilla_viterbi_accuracy], \n                  ['Vanilla Viterbi Modified', accuracy_viterbi_modified], \n                  ['Unigram Tagger', accuracy_unigram_tagger * 100],\n                  ['Unigram + RegexpTagger', accuracy_rule_based_unigram_tagger * 100],\n                  ['Bigram Tagger + Unigram_tagger', accuracy_bigram_tagger*100],\n                  ['Trigram Tagger + Bigram_tagger', accuracy_trigram_tagger*100],\n                  ['Viterbi + Trigram_tagger', viterbi_trigram_accuracy]]\n\nacccuracy_data_df = pd.DataFrame(acccuracy_data, columns = ['Tagging_Algorithm', 'Tagging_Accuracy'])\n\nacccuracy_data_df.set_index('Tagging_Algorithm', drop = True, inplace = True)\n\nacccuracy_data_df","2842c965":"acccuracy_data_df.plot.line(rot = 90, legend = False)","d805ea52":"from nltk.tokenize import word_tokenize","0fe34fb7":"## Testing\nsentence_test_1 = 'Google and Twitter made a deal in 2015 that gave Google access to Twitter\\'s firehose.'\nwords = word_tokenize(sentence_test_1)\ntagged_seq = Vanilla_Viterbi(words)\nprint(tagged_seq)","bbd1769f":"tagged_seq_modified = Viterbi_Trigram_Tagger(words)\nprint(tagged_seq_modified)","8353fd2c":"sentence_test_2='Android has been the best-selling OS worldwide on smartphones since 2011 and on tablets since 2013.'\nwords = word_tokenize(sentence_test_2)\ntagged_seq = Vanilla_Viterbi(words)\nprint(tagged_seq)","5c48a9a7":"tagged_seq_modified = Viterbi_Trigram_Tagger(words)\nprint(tagged_seq_modified)","58170418":"sentence_test_3='I Instagrammed a Facebook post taken from Android smartphone and uploaded results to Youtube.'\nwords = word_tokenize(sentence_test_3)\ntagged_seq = Vanilla_Viterbi(words)\nprint(tagged_seq)","52da9a82":"tagged_seq_modified = Viterbi_Trigram_Tagger(words)\nprint(tagged_seq_modified)","cc8ba6f5":"We see the new words - Youtube, Facebook, smartphone going from X to NOUN. We alse see the verb of Instagrammed,uploaded getting recognised correctly","e0869b2e":"### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications","79457aa1":"Bigram and Trigram tagger","acaab4fc":"You can split the Treebank dataset into train and validation sets. Please **use a sample size of 95:5** for training: validation sets, i.e. keep the validation size small, else the algorithm will need a very high amount of runtime.\n\nYou need to accomplish the following in this assignment:\n* Write the vanilla Viterbi algorithm for assigning POS tags (i.e. without dealing with unknown words) \n* Solve the problem of unknown words using **at least two techniques**. These techniques can use any of the approaches discussed in the class - lexicon, rule-based, probabilistic etc. Note that to implement these techniques, you can either write separate functions and call them from the main Viterbi algorithm, or modify the Viterbi algorithm, or both.\n* Compare the tagging accuracy after making these modifications with the vanilla Viterbi algorithm.\n* List down at least three cases from the sample test file (i.e. unknown word-tag pairs) which were incorrectly tagged by the original Viterbi POS tagger and got corrected after your modifications.","c3fe5c8e":"### Testing Vanilla Viterbi Algorithm on sampled test data","ad945d37":"### Viterbi Modification-Technique II","48adaf87":"#### Let's try with some Lexicon and Rule-Based Models for POS Tagging","feb19ea6":"### Viterbi Algorithm\n\nThe steps are as follows:\n\n1. Given a sequence of words.\n2. iterate through the sequence\n3. for each word (starting from first word in sequence) calculate the product of emission probabilties and transition probabilties for all possible tags.\n4. assign the tag which has maximum probability obtained in step 3 above.\n5. move to the next word in sequence to repeat steps 3 and 4 above.","c6917ef3":"### Build the vanilla Viterbi based POS tagger","b28f9b78":"Let's now try combining the unigram tagger with a rule based regex tagger.","df4c7109":"We observe that much better accuracy is obtained now.","c6a3975f":"#### Function to compute emission probabilties for a given word","234c03f9":"#### Evaluating tagging accuracy","d013b08f":"In this assignment, you need to modify the Viterbi algorithm to solve the problem of unknown words using **at least two techniques**. Though there could be multiple ways to solve this problem, you may use the following hints:\n\n1. Which tag class do you think most unknown words belong to? Can you identify rules (e.g. based on morphological cues) that can be used to tag unknown words? You may define separate python functions to exploit these rules so that they work in tandem with the original Viterbi algorithm.\n2. Why does the Viterbi algorithm choose a random tag on encountering an unknown word? Can you modify the Viterbi algorithm so that it considers only one of the transition or emission probabilities for unknown words?","2fd9e31d":"#### Function to compute transition probabilties for a given tag and previous tag","705bc8b7":"## POS tagging using modified Viterbi","a479f3fd":"### Solve the problem of unknown words","43c57e89":"### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm","ab7beeb8":"**Adding Tag occurance probability weights:** we will apply weights based on the probability of tag occurance to the transition probabilities of tags and then use the resulting probability for predicting unknown words.","72bdcb30":"Before proceeding for dealing with unknown words, lets first see how many unknown words we have. Unknows words would be\nthose words that are present in the test set but not in the train set. That is the words the algorithm has not seen before.","598653ce":"### Data Preparation","8bc506c3":"We see that the Trigram Tagger backed up by the bigram tagger gives an accuracy of about 93.87%. Let's now try to modify the viterbi algorithm to use this trigram tagger as a back-off.\n\nWhen the viterbi algorithm is not able to tag an unknown word, it uses the rule-based tagger.","a81713ba":"Thus, we see that we have got a much better accuracy by using weighted transition probabilties.","61d5b7d2":"### Viterbi Modification-Technique I","d112752a":"#### Unigram tagger","82972fec":"### Exploratory Data Analysis","96529bda":"## Goals"}}