{"cell_type":{"a83bf8be":"code","6a5c0f41":"code","dcd6787a":"code","bece5ae1":"code","b0815aa4":"code","9048c7a0":"code","5a20e717":"code","3449dc2d":"code","4aa4ec3d":"code","d2ae47ea":"code","33858d66":"code","18911c98":"code","057f75c2":"code","08933a85":"code","89b4e3e2":"code","caff80f5":"code","4ddfe1d3":"code","9b260bf0":"code","c84e6d56":"code","05baa91f":"code","af4eafee":"code","244b3aa2":"code","54eb765d":"code","fb9f69df":"markdown","f5802578":"markdown","ff33b22d":"markdown","d94c6706":"markdown"},"source":{"a83bf8be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6a5c0f41":"data = pd.read_csv(\"..\/input\/database.csv\")","dcd6787a":"# First 10 entries\ndata.head(10)","bece5ae1":"# Last 10 entries\ndata.tail(10)","b0815aa4":"data.columns","9048c7a0":"# Making data columns' name lowercase\ndata.columns = [i.lower() for i in data.columns]","5a20e717":"# Removing spaces in column names\n# There are names with one and two spaces. So we can create this algorithm.\ndata.columns = [i.split()[0]+'_'+i.split()[1] if len(i.split())==2 \n                else i.split()[0]+'_'+i.split()[1]+'_'+i.split()[2] if len(i.split())==3\n                else i \n                for i in data.columns]","3449dc2d":"# All column names are lowercase and without spaces\ndata.head(1)","4aa4ec3d":"# Cause of the seismic activity\nprint(data.type.value_counts(dropna=False))","d2ae47ea":"# Source of the seismic data\nprint(data.source.value_counts(dropna=False))","33858d66":"data.info()","18911c98":"# Converting 'date' column to datetime64[ns] type. So we can use this column in time series\ndata.date = pd.to_datetime(data.date)","057f75c2":"# Creating a new dataframe Earthquakes of Turkey\nLat = np.logical_and(data.latitude>=36, data.latitude<=42)\nLong = np.logical_and(data.longitude>=26, data.longitude<=45)\ndataTR = data[Lat & Long]\n# Basicaly we're filtering the data by setting a rectangle polygon.\n\n# Saving .csv file to computer\n# dataTR.to_csv(\"......\/dataTR.csv\")","08933a85":"# Viewing the new data frame.\ndataTR\n# Because of the filtering index numbers (73, 175, 241...) are disordered. If we're going to use ID we can correct this\n# index numbers.\n# I'd like to use this dataTR for time series so I'll convert the index to date.","89b4e3e2":"# Changing the index to date\ndataTR = dataTR.set_index('date')\ndataTR","caff80f5":"# Filtering the earthquake data for 1999 year\n#dataTR.loc['1999']\n# Filtering the earthquake data for 1999 and 2000 years\ndataTR.loc['1999':'2000']","4ddfe1d3":"# Viewing 'latitude','longitude','type','depth','magnitude' columns\ndataTRsimple = dataTR[['latitude','longitude','type','depth','magnitude']]\ndataTRsimple.head()","9b260bf0":"# Creating a function which divides the depth value by 2. (For study purpose. It doesn't make sense.)\ndef divide(x):\n    return x\/2\ndataTRsimple.depth = dataTRsimple.depth.apply(divide)\n#dataTRsimple.depth = dataTRsimple.depth.apply(lambda x: x\/2)  # (2nd method: lambda function)\ndataTRsimple.head()","c84e6d56":"# To learn the data's index\nprint(dataTRsimple.index.name)","05baa91f":"# We can change index name 'date' to something else.\ndataTRsimple.index.name = 'time_id'\ndataTRsimple.head()","af4eafee":"# Or we can create new column for ID\n#dataTRsimple.info()  # We have total of 123 entries for this data. So we can create ID's from 1 to 123.\n\n# If we set the ID's as a new index we will lose the date(time_id) values.\n# To prevent this we should create a new column for date values\ndataTRsimple['date'] = dataTRsimple.index  # Creating a new column called 'date' and copying the index to this column.\ndataTRsimple['ID'] = range(1,124)  # Creating a new column for ID's\ndataTRsimple = dataTRsimple.set_index('ID')  # Set the index as ID\ndataTRsimple.head()","244b3aa2":"# Hierarchical indexing\n\n# We can set two index for data.\ndataTRsimple['ID'] = dataTRsimple.index  # Creating a new column 'ID' and copying our index to this column\ndataTRsimple = dataTRsimple.set_index([\"ID\",\"date\"])  # Setting two index. ID is outer index, date is inner index\ndataTRsimple.head()","54eb765d":"# Calculating mean value for all columns according to years ('A')\n#dataTR.resample('A').mean() \n# Calculating mean value for all columns according to months ('M')\n#dataTR.resample('M').mean()\n# Linear interpolation\n#dataTR.resample('M').first().interpolate('linear')  # This method can be used for filling empty values between first and last data\n\n# These calculations are for study purposes. Don't make any sense for this data.","fb9f69df":"* Turkey Coordinates : \n*     Latitude (northing) between 36 and 42 degrees\n*     Longitude (easting) between 26 and 45 degrees\n* We can use these coordinates to filter the earthquake data, so we can create a new dataframe of earthquakes that occured in Turkey.\n\n        ","f5802578":"**Summary**\n1. Filtered the data in order the get Turkey's earthquakes by using latitude & longtitude values (numpy logical_and method.) and created a new dataframe from this filtered data.\n2. Changed the data index to date and filtered the new dataframe by using .loc slicing method to view the Turkey's earthquakes in 1999 and 2000. There were two big earthquakes in 1999 (17 August Izmit and 12 November Duzce). We can also see some aftershocks which are very close to the epicenter of major earthquakes. (Unfortunately smaller than Mw 5.5 earthquake data is not avaible in this dataset)\n3. It's possible to make custom filters and create new dataframes from the main data as csv files. Later these csv files can be used in GIS programs like QGIS to add a new layer to maps. (Maybe filtering can be done QGIS platform?)\n4. Learned manipulating data's index.\n","ff33b22d":"**Data null \/ non-null objects**\n* 18951 null value for Depth Error\n* 16315 null value for Depth Seismic Stations  \n* 3 null value for Magnitude Type\n* 23082 null value for Magnitude Error\n* 20848 null value for Magnitude Seismic Stations\n* 16113 null value for Azimuthal Gap\n* 21808 null value for Horizontal Distance\n* 22256 null value for Horizontal Error\n* 6060 nul value for Root Mean Square","d94c6706":"**The purpose of this kernel:**\n* Learning basic pandas methods (importing data from csv file, getting familar with head, tail, column, type, value_counts, info, to_datetime, set_index, loc, resample methods)\n* Learning filtering methods with Pandas and Numpy (logical_and method)"}}