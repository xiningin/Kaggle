{"cell_type":{"40abd18b":"code","e94a5897":"code","d5cd2ad3":"code","eb9ec7cc":"code","64904edb":"code","fc73a210":"code","34271a4b":"code","91ffab2c":"code","ddda4f6f":"code","bbf3e7fb":"code","6b224301":"code","7460adab":"code","ef907e01":"code","1c55f545":"code","8b30d227":"code","8be8d4dc":"code","af0372f0":"code","3ec918c7":"code","af6bda60":"code","255daf43":"code","466b3a64":"code","465abade":"code","dcb2be4b":"code","cb826af5":"code","0421b191":"code","2a3a6af3":"code","4c2c89f8":"code","0052d005":"code","2372c530":"code","a348a343":"code","202eff21":"code","15bc5df7":"code","9e88d779":"code","f9f3574b":"code","030428fa":"code","20d6ae4a":"code","90df5709":"code","c12cea79":"code","ae230924":"code","fd7b1665":"code","e46359f1":"code","cffe9064":"code","6c123a78":"code","820c9a27":"code","7b1bd60c":"code","105b130a":"code","e252765e":"code","bd06dda0":"code","89762fab":"code","b99dc086":"code","25e17952":"code","53dcc5cb":"code","3610b5d4":"code","dde549a4":"code","0854ec0e":"code","83b79fef":"code","240e4103":"code","ab978932":"code","52db41d2":"code","52662f4d":"code","557d3011":"code","20a48976":"code","dda083b2":"code","e0ea770c":"code","615513e7":"code","c83bbcc5":"code","e5c18724":"code","2a10650c":"code","24c5fbd0":"code","28eea10e":"code","afe8462f":"code","aaece9e1":"markdown","e0088ee8":"markdown","5e8a9581":"markdown","1919ef5d":"markdown","8c9bf47b":"markdown","a9050ef4":"markdown","ede4d71e":"markdown","d0dc4417":"markdown","963eab52":"markdown","56587351":"markdown","7529fe85":"markdown","21c248fe":"markdown","328618fa":"markdown","c59d2b92":"markdown","c3a7ea28":"markdown","06edb0b7":"markdown","f5c810ab":"markdown","2ad31b14":"markdown","81536354":"markdown","039a7fb5":"markdown","aa938acf":"markdown","255da677":"markdown","f0279b79":"markdown","aafa46f7":"markdown","56f2fbf3":"markdown","e3d2900c":"markdown","7505736d":"markdown"},"source":{"40abd18b":"# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e94a5897":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","d5cd2ad3":"train = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv', encoding='ISO-8859-1')\ntest = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv', encoding='ISO-8859-1')","eb9ec7cc":"train.head()","64904edb":"train.info()","fc73a210":"train.isnull().sum()","34271a4b":"test.head()","91ffab2c":"test.info()  # same dtypes with train df","ddda4f6f":"test.isnull().sum()  # there are null values again in location column","bbf3e7fb":"train.Location.value_counts(dropna = False)[:20]","6b224301":"train.Location = train.Location.str.split(\",\").str[0]","7460adab":"sns.set_style(\"whitegrid\")\nsns.set(rc={'figure.figsize':(11,4)})\n\nplt.figure(figsize=(12, 6))\nsns.barplot(train[\"Location\"].value_counts().values[:10],\n            train[\"Location\"].value_counts().index[:10]);\nplt.title(\"Top 10 Countries with maximum Covid-19 tweets\", fontsize=14)\nplt.xlabel(\"Number of tweets\", fontsize=14)\nplt.ylabel(\"Country Name\", fontsize=14)\nplt.show()","ef907e01":"train['Sentiment'].value_counts() ","1c55f545":"sns.countplot(x = \"Sentiment\", data = train)","8b30d227":"encoding = {'Extremely Negative': 0,\n            'Negative': 0,\n            'Neutral': 1,\n            'Positive':2,\n            'Extremely Positive': 2\n           }\n\nlabels = ['Negative', 'Neutral', 'Positive']\n           \ntrain[\"Sentiment\"].replace(encoding, inplace=True)\ntest[\"Sentiment\"].replace(encoding, inplace=True)","8be8d4dc":"sns.countplot(x = \"Sentiment\", data = train)","af0372f0":"sns.countplot(x = \"Sentiment\", data = test)","3ec918c7":"loc_with_sentiment = train.iloc[:, [2,5]]\n","af6bda60":"plt.figure(figsize=(15, 6))\nax = sns.countplot(x = \"Location\", hue = \"Sentiment\", data = loc_with_sentiment, \n              order = train.Location.value_counts()[:10].index, orient = \"h\", palette = \"Paired\") \n\nfor p in ax.patches:\n    ax.annotate(format(p.get_height(), '.0f'), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   size=15,\n                   xytext = (0, -12), \n                   textcoords = 'offset points')\nplt.show()","255daf43":"import regex as re\n\ndef extract_hash_tags(s):\n    hashes = re.findall(r\"#(\\w+)\", s)\n    return \" \".join(hashes)\n    \ntrain['hashtags'] = train['OriginalTweet'].apply(lambda x : extract_hash_tags(x))","466b3a64":"from collections import Counter\n\nallHashTags = list(train[(train['hashtags'] != None) & (train['hashtags'] != \"\")]['hashtags'])\nallHashTags = [tag.lower() for tag in allHashTags]\nhash_df = dict(Counter(allHashTags))\ntop_hash_df = pd.DataFrame(list(hash_df.items()),columns = ['word','count']).reset_index(drop=True).sort_values('count',ascending=False)[:15]\ntop_hash_df.head()","465abade":"import plotly.express as px\n\nfig = px.bar(x=top_hash_df['word'],y=top_hash_df['count'],\n       orientation='v',\n       color=top_hash_df['word'],\n       text=top_hash_df['count'],\n       color_discrete_sequence= px.colors.qualitative.Bold)\n\nfig.update_traces(texttemplate='%{text:.2s}', \n                  textposition='outside', \n                  marker_line_color='rgb(8,48,107)', \n                  marker_line_width=1.5, \n                  opacity=0.7)\n\nfig.update_layout(width=1000, \n                  showlegend=False, \n                  xaxis_title=\"Word\",\n                  yaxis_title=\"Count\",\n                  title=\"Top #hashtags in Covid19 Tweets\")\nfig.show()","dcb2be4b":"def get_mentions(s):\n    mentions = re.findall(\"(?<![@\\w])@(\\w{1,25})\", s)\n    return \" \".join(mentions)\ntrain['mentions'] = train['OriginalTweet'].apply(lambda x : get_mentions(x))","cb826af5":"allMentions = list(train[(train['mentions'] != None) & (train['mentions'] != \"\")]['mentions'])\nallMentions = [tag.lower() for tag in allMentions]\nmentions_df = dict(Counter(allMentions))\ntop_mentions_df = pd.DataFrame(list(mentions_df.items()),columns = ['word','count']).reset_index(drop=True).sort_values('count',ascending=False)[:15]\ntop_mentions_df.head()","0421b191":"fig = px.bar(x=top_mentions_df['word'],y=top_mentions_df['count'],\n       orientation='v',\n       color=top_mentions_df['word'],\n       text=top_mentions_df['count'],\n       color_discrete_sequence= px.colors.qualitative.Bold)\n\nfig.update_traces(texttemplate='%{text:.2s}', \n                  textposition='outside', \n                  marker_line_color='rgb(8,48,107)', \n                  marker_line_width=1.5, \n                  opacity=0.7)\n\nfig.update_layout(width=1000, \n                  showlegend=False, \n                  xaxis_title=\"Word\",\n                  yaxis_title=\"Count\",\n                  title=\"Top #mentions in Covid19 Tweets\")\nfig.show()","2a3a6af3":"train.drop([\"hashtags\", \"mentions\"], axis =1, inplace = True)","4c2c89f8":"train[\"TweetAt\"] = pd.to_datetime(train[\"TweetAt\"])\ntrain[\"TweetAt\"].apply(lambda x : x.dayofweek).value_counts()","0052d005":"train[\"TweetAt\"].apply(lambda x : x.dayofweek).value_counts().plot.barh()\nplt.title(\"maximun tweets during 2020\")","2372c530":"train[\"TweetAt\"] = pd.to_datetime(train[\"TweetAt\"])\ntrain[\"day\"] = train[\"TweetAt\"].apply(lambda x : x.dayofweek)\ndmap = {0: 'Mon', 1: 'Tue', 2:'Wed', 3: 'Thu', 4: 'Fri', 5: 'Sat', 6: 'Sun'}\ntrain[\"day\"] = train[\"day\"].map(dmap)\nplt.title(\"Day with maximun tweets\")\nsns.countplot(train[\"day\"])\n","a348a343":"train.drop(\"day\", axis = 1, inplace = True)","202eff21":"train.drop_duplicates(inplace = True)\ntest.drop_duplicates(inplace = True)","15bc5df7":"train_df = train.copy()\ntest_df = test.copy()","9e88d779":"from nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport re","f9f3574b":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom nltk.tokenize import TweetTokenizer","030428fa":"import nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","20d6ae4a":"sentences = train['OriginalTweet'][:5]\n\nfor i in sentences[3:4]:\n    print(\"Original:\\n\")\n    print(i)\n    print('\\nTensorflow Tokenizer\\n:')\n    a = Tokenizer()\n    a.fit_on_texts([i])\n    print(a.word_index)\n    print(\"\\nTweet Tokenizer:\\n\")\n    print(TweetTokenizer().tokenize(i))\n    print('\\nNLTK word_tokenizer:\\n')\n    print(word_tokenize(i))","90df5709":"stop_words = stopwords.words('english')\nlem = WordNetLemmatizer()\n\ndef cleaning(data):\n    #1. Remove urls \n    tweet_without_url = re.sub(r'http\\S+', ' ', data)\n    \n    #2. Remove hashtags\n    tweet_without_hashtag = re.sub(r'#\\w+',' ', tweet_without_url)\n    \n    #3. Remove mentions and characters that not in the English alphabets\n    tweet_without_mentions = re.sub(r'@\\w+',' ', tweet_without_hashtag)\n    precleaned_tweet = re.sub('[^A-Za-z]+', ' ', tweet_without_mentions)\n\n    #2. Tokenize\n    tweet_tokens = TweetTokenizer().tokenize(precleaned_tweet)\n    \n    #3. Remove Puncs\n    tokens_without_punc = [w for w in tweet_tokens if w.isalpha()]\n    \n    #4. Removing Stopwords\n    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n    \n    #5. lemma\n    text_cleaned = [lem.lemmatize(t) for t in tokens_without_sw]\n    \n    #6. Joining\n    return \" \".join(text_cleaned)\n","c12cea79":"train_df['OriginalTweet'] = train_df['OriginalTweet'].apply(lambda x: cleaning(x))","ae230924":"pd.set_option('display.max_colwidth', -1)  \ntrain_df.iloc[:, [4,5]].head()","fd7b1665":"test_df['OriginalTweet'] = test_df['OriginalTweet'].apply(lambda x: cleaning(x))","e46359f1":"pd.set_option('display.max_colwidth', -1)  \ntest_df.iloc[:, [4,5]].head()","cffe9064":"HQ_words = ' '.join([i for i in train_df['OriginalTweet']]).split() \nunigram_HQ = pd.Series(nltk.ngrams(HQ_words, 1)).value_counts()[:15]\nunigram_HQ = pd.DataFrame(unigram_HQ)\nunigram_HQ['idx'] = unigram_HQ.index\nunigram_HQ['idx'] = unigram_HQ.apply(lambda x: '('+x['idx'][0]+')',axis=1)","6c123a78":"import plotly.graph_objs as go\nimport plotly.offline as pyoff\n\nplot_data = [\n    go.Bar(\n        x=unigram_HQ['idx'],\n        y=unigram_HQ[0],\n        marker = dict(\n            color = 'Blue'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 15 uni-grams from Covid-19 Tweets',\n        yaxis_title='Count',\n        xaxis_title='Uni-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","820c9a27":"bigram_HQ = (pd.Series(nltk.ngrams(HQ_words, 2)).value_counts())[:15]\nbigram_HQ = pd.DataFrame(bigram_HQ)\nbigram_HQ['idx'] = bigram_HQ.index\nbigram_HQ['idx'] = bigram_HQ.apply(lambda x: '('+x['idx'][0]+', '+x['idx'][1]+')',axis=1)","7b1bd60c":"plot_data = [\n    go.Bar(\n        x=bigram_HQ['idx'],\n        y=bigram_HQ[0],\n        marker = dict(\n            color = 'Red'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 15 bi-grams from Covid 19 Tweets',\n        yaxis_title='Count',\n        xaxis_title='bi-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","105b130a":"trigram_HQ = (pd.Series(nltk.ngrams(HQ_words, 3)).value_counts())[:20]\ntrigram_HQ = pd.DataFrame(trigram_HQ)\ntrigram_HQ['idx'] = trigram_HQ.index\ntrigram_HQ['idx'] = trigram_HQ.apply(lambda x: '('+x['idx'][0]+', '+x['idx'][1]+', '+x['idx'][2]+')',axis=1)","e252765e":"plot_data = [\n    go.Bar(\n        x=trigram_HQ['idx'],\n        y=trigram_HQ[0],\n        marker = dict(\n            color = 'Green'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 15 Tri-grams from Covid 19 Tweets',\n        yaxis_title='Count',\n        xaxis_title='Tri-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","bd06dda0":"from wordcloud import WordCloud","89762fab":"Positive = ' '.join([tweet for tweet in train_df['OriginalTweet'][train_df['Sentiment'] == 0]])\n\nwordcloud = WordCloud(background_color = \"white\", width = 800, height = 500,\n                      random_state = 21, max_font_size = 110).generate(Positive)\nplt.figure(figsize = (10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","b99dc086":"Negative = ' '.join([tweet for tweet in train_df['OriginalTweet'][train_df['Sentiment'] == 1]])\n\nwordcloud = WordCloud(width = 800, height = 500, random_state = 21, max_font_size = 110).generate(Negative)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","25e17952":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow.keras.layers as Layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers import Dense, Dropout, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Bidirectional, SpatialDropout1D\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","53dcc5cb":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_df[\"OriginalTweet\"])  # fitting tokenizer on training_datase\n\nX = tokenizer.texts_to_sequences(train_df[\"OriginalTweet\"])  # getting text sequences from training dataframe\ny = train_df[\"Sentiment\"]\n\nvocab_size = len(tokenizer.word_index) + 1","3610b5d4":"vocab_size","dde549a4":"print(\"Vocabulary size: {}\".format(vocab_size))\nprint(\"\\n----------Example----------\\n\")\nprint(\"Sentence:\\n{}\".format(train_df[\"OriginalTweet\"][6]))\nprint(\"\\nAfter tokenizing :\\n{}\".format(X[6]))\n\nX = pad_sequences(X, padding='post')  # adding padding of zeros to obtain uniform length for all sequences\nprint(\"\\nAfter padding :\\n{}\".format(X[6]))","0854ec0e":"X.shape","83b79fef":"# hyper parameters\nEPOCHS = 3\nBATCH_SIZE = 32 \nembedding_dim = 16\nunits = 256\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embedding_dim, input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))  # This version performs the same function as Dropout, however, it drops entire 1D feature maps instead of individual elements.\nmodel.add(LSTM(units, dropout = 0.2, recurrent_dropout = 0.2))\nmodel.add(Dense(3,activation = 'softmax'))  # we have 3 categories so we have to use softmax \nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nprint(model.summary())","240e4103":"Y = pd.get_dummies(train['Sentiment']).values  \n# categorical cross entropy requires get_dummies cause of it only accepts [0]s and [1]s\n\nprint(X.shape,Y.shape)\n","ab978932":"model.fit(X, Y, epochs = 5, validation_split = 0.12, batch_size = BATCH_SIZE)","52db41d2":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.plot()","52662f4d":"tf.keras.backend.clear_session()\n\n\nmodel_blstm = tf.keras.Sequential([\n    Layers.Embedding(vocab_size, embedding_dim, input_length = X.shape[1]),\n    Layers.Bidirectional(Layers.LSTM(units, return_sequences = True)),  # recurrent layer with lstm\n    Layers.GlobalMaxPool1D(),  # Downsamples the input representation by taking the maximum value over the target\n    Layers.Dropout(0.2),\n    Layers.Dense(64, activation = \"relu\"),\n    Layers.Dropout(0.2),\n    Layers.Dense(3, activation = 'softmax')\n])","557d3011":"model_blstm.compile(loss = SparseCategoricalCrossentropy(from_logits = True),  # Computes the crossentropy loss between the labels and predictions.\n              optimizer = 'adam', metrics = ['accuracy'])","20a48976":"model_blstm.summary()","dda083b2":"model_blstm.fit(X, y, epochs = EPOCHS, validation_split = 0.12, batch_size = BATCH_SIZE)","e0ea770c":"model_blstm_loss = pd.DataFrame(model_blstm.history.history)\nmodel_blstm_loss.plot()","615513e7":"model_cnn = Sequential()\nmodel_cnn.add(Embedding(vocab_size,embedding_dim,input_length=X.shape[1]))\n\nmodel_cnn.add(Conv1D(64, kernel_size=3, padding='same', activation='relu', strides=1))\n# This layer creates a convolution kernel that is convolved with the layer input over a single spatial \n# (or temporal) dimension to produce a tensor of outputs.\n\nmodel_cnn.add(GlobalMaxPooling1D()) \n# Downsamples the input representation by taking the maximum value over the dimension.\n\nmodel_cnn.add(Dense(128, activation='relu'))\nmodel_cnn.add(Dropout(0.2))\n\nmodel_cnn.add(Dense(3,activation='softmax'))\n\nmodel_cnn.compile(loss='categorical_crossentropy',optimizer= 'adam',metrics=['accuracy'])\n\nmodel_cnn.summary()","c83bbcc5":"model_cnn.fit(X, Y, validation_split = 0.12,epochs=2, batch_size=BATCH_SIZE)","e5c18724":"model_cnn_loss = pd.DataFrame(model_cnn.history.history)\nmodel_cnn_loss.plot()","2a10650c":"X_test = test['OriginalTweet'].copy()\ny_test = test['Sentiment'].copy()\n\nX_test = X_test.apply(cleaning)\n\nX_test = tokenizer.texts_to_sequences(X_test)\n\nX_test = pad_sequences(X_test, padding='post')","24c5fbd0":"pred = model_cnn.predict_classes(X_test)","28eea10e":"print(classification_report(y_test, pred))","afe8462f":"conf = confusion_matrix(y_test, pred)\n\ncm = pd.DataFrame(\n    conf, index = [i for i in labels],\n    columns = [i for i in labels]\n)\n\nplt.figure(figsize = (12,7))\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.show()","aaece9e1":"### checking out dfs","e0088ee8":"## uni grams for train tweets","5e8a9581":"## c. Modeling with CNN","1919ef5d":"## Tri-grams for Tweets","8c9bf47b":"#### Results of Bi-LSTM model","a9050ef4":"### tweet locations","ede4d71e":"## 6. Evaluation","d0dc4417":"#### results of LSTM model","963eab52":"## 3. Text Mining","56587351":"### tweet times","7529fe85":"## 1. Loading Dataframes","21c248fe":"## hashtags","328618fa":"## mentions","c59d2b92":"### making cheack points and dropping duplicated rows","c3a7ea28":"### Model Results","06edb0b7":"### tweet sentiment values","f5c810ab":"## Bi-grams for Tweets","2ad31b14":"## 2. Explotary Data Analysis","81536354":"### a. Modeling with LSTM","039a7fb5":"## 5. Sentiment Analysis Models \n\n\n","aa938acf":"## b. Modeling with Bidirectional LSTM","255da677":"#### As you can see these all yield different results and you have to see which works best for your use case. \n#### For now we will use NLTK Tweet-Tokenizer.","f0279b79":"### Preprocessing test data ","aafa46f7":"### Making Predictions with Bi-LSTM","56f2fbf3":"### regrouping train and test dfs","e3d2900c":"## 4. WordCloud - Repetition of Words\u00b6","7505736d":"### analysis of locations with sentiment"}}