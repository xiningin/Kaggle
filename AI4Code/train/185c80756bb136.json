{"cell_type":{"8ea5fd22":"code","15f29841":"code","96a1b1f1":"code","29e34e54":"code","ae29b4ad":"code","ba3286ab":"code","91580cb0":"code","14ca9019":"code","e998667c":"code","dd272f75":"code","c26aec64":"code","3592dd65":"code","2f012f6a":"code","a219fdd5":"code","977ade23":"code","cc720fe1":"code","66db904e":"code","74465891":"code","5fde806d":"code","1267faab":"code","3ea6ae8d":"code","b0a7a138":"code","84feaccb":"code","2915910a":"code","071d141b":"code","32129ff5":"code","c7a07086":"markdown"},"source":{"8ea5fd22":"!conda install -y gdown \nimport gdown \nurl = 'https:\/\/drive.google.com\/uc?id=1-7WqWH8h2XjXx_4_SCcKYtVfSHojkgfU' \noutput = 'myfile.zip'\ngdown.download(url, output)","15f29841":"mkdir driver_drawsiness","96a1b1f1":"!unzip ..\/input\/drawsiness-detection\/myfile.zip -d  .\/driver_drawsiness","29e34e54":"import cv2\nimport matplotlib.pyplot as plt\n\nimg_bgr = cv2.imread('..\/input\/driver-drawsiness\/40-FemaleNoGlasses-Talking\/frame_000015.png')\nimg_rgb = cv2.cvtColor(img_bgr , cv2.COLOR_BGR2RGB)\nplt.imshow(img_rgb)","ae29b4ad":"import os\nimg_paths = []\ntest_img_paths =[]\nfile_list = os.listdir('..\/input\/driver-drawsiness')\nfor file in file_list :\n    if(file.endswith('Talking') or file.endswith(\"Yawning\") or file.endswith(\"Normal\")):\n        img_paths.append(file)\n    else:\n        test_img_paths.append(file)","ba3286ab":"img_paths[:20]","91580cb0":"test_img_paths[:10]","14ca9019":"from torch.utils.data import Dataset , DataLoader \nfrom torchvision import transforms\nimport torch\nimport numpy as np","e998667c":"collate_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize(224),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean = [0.485, 0.456, 0.406] ,std = [0.229, 0.224, 0.225])\n])\ndef map_label(label):\n    if(label=='Talking'):\n        return 0\n    elif(label==\"Normal\"):\n        return 1\n    elif(label==\"Yawning\"):\n        return 2\n\nbase_dir = '..\/input\/driver-drawsiness'   \ndef my_collate(batch):\n    \n    data_file = batch[0]\n    data_file = os.path.join(base_dir , data_file)\n    img_files = os.listdir(data_file)\n    \n    images=[]\n    for img_file in img_files :\n        img_bgr = cv2.imread(os.path.join(data_file , img_file))\n        img_rgb = cv2.cvtColor(img_bgr , cv2.COLOR_BGR2RGB)\n        images.append(collate_transform(img_rgb))\n    \n    label = map_label(data_file.split(\"-\")[-1])\n    \n    label =  [torch.from_numpy(np.array(label))]\n    \n    data = torch.stack(images , dim=0)\n    data = data.permute(1,0,2,3)\n    label = torch.stack(label , dim=0)\n    \n    return data , label\n    \n    ","dd272f75":"class DrawsinessData(Dataset):\n    def __init__(self , file_path ):\n        super(DrawsinessData , self).__init__()\n        self.file_list = file_path\n        \n    def __len__(self):\n        return len(self.file_list)\n    \n    def __getitem__(self , index):\n        \n        file_path = self.file_list[index]\n        \n        return file_path","c26aec64":"data_set = DrawsinessData(img_paths)\n\ndata_loader = DataLoader(data_set , batch_size=1 , shuffle=True , collate_fn=my_collate)","3592dd65":"data , label = next(iter(data_loader))","2f012f6a":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch","a219fdd5":"class DDD_Model(nn.Module):\n    def __init__(self , in_c , n_classes , seed , lstm_layers , hidden_size):\n        super(DDD_Model , self).__init__()\n        self.in_c = in_c\n        self.seed= torch.manual_seed(seed)\n        self.n_lstm = lstm_layers\n        self.hidden_size = hidden_size\n        self.n_classes = n_classes\n        \n        self.conv_1 = DDD_Model.__conv(self.in_c , 64 )\n        # 224 - 224 - 64  --> 112 - 112 - 128\n        self.conv_2 = DDD_Model.__conv( 64 , 128 , maxpool=True)\n        #112 - 112 - 128 --> 56 - 56 - 256\n        self.conv_3 = DDD_Model.__conv(128 , 256 , maxpool=True)\n        #56 - 56 - 256  --> 28 - 28 - 512\n        self.conv_4 = DDD_Model.__conv(256 , 512 , maxpool=True)\n        #28 - 28 - 512 --> 14 - 14 - 1024\n        self.conv_5 = DDD_Model.__conv(512,1024 , maxpool=True)\n        \n        self.flatten_size = 14 * 14 * 1024\n        \n        self.dropout = nn.Dropout(p=0.4)\n        self.fc1 = nn.Linear(self.flatten_size , 2048)\n        \n        self.lstm_layer=  nn.LSTM(input_size = 2048 , hidden_size=self.hidden_size , num_layers= self.n_lstm ,\n                                  batch_first=True , dropout= 0.3)\n        \n        self.drop_lstm = nn.Dropout(p=0.5)\n        self.fc_out = nn.Linear(self.hidden_size , self.n_classes)\n        \n    def forward(self , x ):\n        \n        hidden = self.init_hidden(1)\n        \n        x = self.conv_1(x)\n        x = self.conv_2(x)\n        x = self.conv_3(x)\n        x = self.conv_4(x)\n        x = self.conv_5(x)\n        \n        x = x.view(-1 , self.flatten_size)\n        x = self.dropout(x)\n        x_fc1 = self.fc1(x)\n        \n        x = x.view(1 , -1 , 2048)\n        \n        x  , _  = self.lstm_layer(x , hidden)\n        x = x.contiguous().view(-1,self.hidden_size)\n        \n        x = x[-1]\n        x = x.view(1,-1)\n        x = self.drop_lstm(x)\n        \n        out = self.fc_out(x)\n        \n        return out\n        \n        \n    \n    def init_hidden(self  , batch_size):\n        weight = next(self.parameters()).data\n        \n        hidden = ( weight.new(self.n_lstm , batch_size , self.hidden_size).zero_().to(device) , \n                   weight.new(self.n_lstm , batch_size , self.hidden_size).zero_().to(device)  )\n        \n        return hidden\n        \n        \n    \n    @staticmethod\n    def __conv(in_c , out_c , kernel_size = 3 , strides = 1 , padding=1 , batch_norm=True , maxpool=False):\n        layers =[]\n        conv_layer = nn.Conv2d(in_c , out_c ,  kernel_size=kernel_size , stride=strides , padding=padding , bias=False)\n        layers.append(conv_layer)\n        \n        if(batch_norm):\n            bn = nn.BatchNorm2d(out_c)\n            layers.append(bn)\n            \n        relu = nn.ReLU(inplace=True)\n        layers.append(relu)\n        \n        if(maxpool):\n            pool_layer = nn.MaxPool2d(kernel_size=4 , stride=2 , padding=1)\n            layers.append(pool_layer)\n            \n        return nn.Sequential(*layers)","977ade23":"from torchvision import models\nddd_model = models.video.r3d_18(pretrained=True , progress=True)","cc720fe1":"layers = list(ddd_model.children())\nfor params in layers[:-5]:\n    params.requires_grad =False\n\nclassifier = nn.Linear(in_features=512 , out_features=3 , bias=True)\nddd_model.fc = classifier","66db904e":"device = 'cuda:0' if torch.cuda.is_available() else 'cpu'","74465891":"#ddd_model = DDD_Model(in_c = 3 , n_classes = 3 , seed = 1422 , lstm_layers = 2 , hidden_size = 512)\nddd_model.to(device)","5fde806d":"#ddd_model.load_state_dict(torch.load('..\/input\/drawsiness-detection\/ddd_model.pth'))","1267faab":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam( params=filter(lambda p: p.requires_grad , ddd_model.parameters())  , lr=0.0001)","3ea6ae8d":"from collections import deque\n\nEPOCHS = 7\ntrain_loss = deque(maxlen=100)\ntotal_train_loss = []\nprint_every = 100\n\nfor i_epoch in range(EPOCHS):\n    \n    epoch_loss = 0\n    epoch_correct =0\n    total =0\n    for idx , (data , label ) in enumerate(data_loader):\n        \n        data , label = data.to(device) , label.to(device)\n        #forward pass thorugh the model\n        \n        outputs = ddd_model(data.unsqueeze(0))\n        #calculate the loss\n        loss = criterion(outputs , label)\n        #append the losses and correct labels\n        epoch_loss += loss.detach().to('cpu').item()\n        epoch_correct += outputs.max(dim=1)[1].eq(label).sum().item()\n        \n        train_loss.append(loss.detach().to('cpu').item())\n        total_train_loss.append(loss.detach().to('cpu').item())\n        total += label.shape[0]\n        \n        #reset the optimizer\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if((idx+1)%print_every ==0):\n            print(\"Epoch : {} Train Loss {:.6f} Correct Precentage : {:.6f}\".format(i_epoch , np.mean(train_loss) , \n                                                                                   100.0 *epoch_correct \/ total ))","b0a7a138":"torch.save(ddd_model.state_dict(),'ddd_model.pth')","84feaccb":"def test_collate(batch):\n    \n    data_file = batch[0]\n    data_file = os.path.join(base_dir , data_file)\n    img_files = os.listdir(data_file)\n    \n    images=[]\n    for img_file in img_files :\n        img_bgr = cv2.imread(os.path.join(data_file , img_file))\n        img_rgb = cv2.cvtColor(img_bgr , cv2.COLOR_BGR2RGB)\n        images.append(collate_transform(img_rgb))\n    \n    label = data_file\n    \n    data = torch.stack(images[:20] , dim=0)\n    data = data.permute(1,0,2,3)\n    return data , label\n\ntest_set = DrawsinessData(test_img_paths)\n\ntest_loader = DataLoader(test_set , batch_size=1 , shuffle=True , collate_fn=test_collate)","2915910a":"data , label = next(iter(test_loader))","071d141b":"import torchvision\nmean = [0.485, 0.456, 0.406] \nstd  = [0.229, 0.224, 0.225]\n\ndef map_class(label):\n    if(label==0):\n        return \"Talking\"\n    elif(label==1):\n        return \"Normal\"\n    elif(label==2):\n        return \"Yawning\"\n    \ndef imshow(img):\n    npimg = img.numpy()\n    np_img=np.transpose(npimg, (1, 2, 0))\n    np_img = np_img*std + mean\n    plt.imshow(np_img)\n    \nddd_model.eval()\nfor data , label  in test_loader :\n    \n    data = data.to(device)\n\n    output = ddd_model(data.unsqueeze(0))\n    output_numpy = output.detach().to('cpu').numpy()\n\n    max_class = np.argmax(output_numpy)\n\n    pred_class = map_class(max_class)\n    \n    print(\"Data Visualization\")\n    fig = plt.figure(figsize=(15, 10))\n    data = data.permute(1,0,2,3)\n    imshow(torchvision.utils.make_grid(data.detach().to('cpu')))\n\n    print(\"Video File Name : {}  predicted class : {} \".format(label,pred_class))\n    \n    break\n ","32129ff5":"torch.save(ddd_model.state_dict(),'res_18_video.pth')","c7a07086":"## Resnet 3D Model"}}