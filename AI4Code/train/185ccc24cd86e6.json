{"cell_type":{"4d650372":"code","4cfa2878":"code","a8c434ec":"code","79ef9bf3":"code","8821cc6c":"code","36917f38":"code","9923e20a":"code","b369ad07":"code","f408e56d":"code","324ac5ea":"code","f0cf46f4":"code","e747d33f":"code","9acb46cd":"code","7b686c44":"code","e0f01f2c":"code","a395940d":"code","5372fbed":"code","0c23d4d4":"code","6b1e5178":"code","8da1578d":"code","9be9af3e":"code","170cd2c1":"code","57f881f3":"code","745996b0":"code","8538376c":"code","7f9b26b0":"code","72a195b9":"code","f5e40f1e":"code","3c00ed2c":"code","cc8645d6":"code","2ed7cbef":"code","c1acf7a0":"code","02f090de":"code","ba19801f":"code","15d1f149":"code","67505da3":"code","61a7f854":"code","6929dbb8":"code","ee2cab17":"code","17f3c967":"code","49d15881":"code","b9c22c28":"code","bd177336":"code","80610a80":"code","03229fc1":"code","f4aef157":"code","95a84f5c":"code","ef452050":"code","b795a828":"code","a5fc64d3":"code","c3478ac9":"code","1b6b036f":"code","3da3a8d8":"code","524b5f59":"code","f154d893":"markdown","3ba0e9ba":"markdown","04aa31fa":"markdown","61f9f746":"markdown","452e694f":"markdown","0ca37ac5":"markdown","7a7fca1f":"markdown","520d5212":"markdown","554f0594":"markdown","74002c24":"markdown","06143056":"markdown","7e9d5624":"markdown","e96dcec3":"markdown","5ecd1fcd":"markdown","64791f9e":"markdown","47b30ba2":"markdown","9a7a5941":"markdown","c03f1898":"markdown","2b84f393":"markdown","d1e37841":"markdown","787c2a8d":"markdown","0b0d3a9e":"markdown","39f6d0a5":"markdown","99cc4f5a":"markdown","8aa59ba3":"markdown","a3080be4":"markdown","ebde6d09":"markdown","5de40c3d":"markdown","7df1449c":"markdown","22dc9321":"markdown","c34b7a02":"markdown","0c7037da":"markdown","500f4cee":"markdown","e36b4e8d":"markdown","2b66b023":"markdown","30e9cbac":"markdown","6d114436":"markdown","ea83dded":"markdown","8f205e9c":"markdown","c721ec33":"markdown"},"source":{"4d650372":"# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#core imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import plot, iplot, init_notebook_mode\nimport plotly_express as px\ninit_notebook_mode(connected=True)\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","4cfa2878":"#load dataset and assign it to a variable\nplayst=pd.read_csv(\"..\/input\/google-play-store-apps\/googleplaystore.csv\")","a8c434ec":"# checking the first 10 rows of our play store apps dataframe\nplayst.head(10)","79ef9bf3":"# shape of the play store apps dataframe\nplayst.shape","8821cc6c":"# getting the insight of the type of feature columns and non-null values in the dataset\nplayst.info()","36917f38":"# Getting the insight of null values present in the feature columns\nplayst.isnull().sum()","9923e20a":"#making a copy of our playst dataframe\nplayst_mb=playst.copy()","b369ad07":"#function to convert size into Bytes by popping 'M' and 'K' from the string size values.\n#converts to float data-type. \ndef clean_size(x):\n    if x.endswith('M'):\n        x=float(x[:-1])*1000000\n    elif x.endswith('k'):\n        x=float(x[:-1])*1000\n    else:\n        x=np.nan\n    return x\n\n#function to convert size into MegaBytes\n#converts to float data-type\ndef clean_size_mb(x):\n    if x.endswith('M'):\n        x=float(x[:-1])\n    elif x.endswith('k'):\n        x=float(x[:-1])\/1000\n    else:\n        x=np.nan\n    return x\n    \n#pop ',' from installs string and remove '+' from the end\n#convert to float\ndef clean_install(x):\n    if len(x)>1:\n        x=x[:-1]\n        x=x.replace(',','')\n    x=float(x)\n    return x\n\n#pop '$' sign at the beginning and convert to float\ndef clean_price(x):\n    if x.startswith('$'):\n        return x[1:]\n    else:\n        return x\n\nplayst['Size']=playst['Size'].apply(clean_size)\nplayst['Size'].fillna(playst.groupby('Category')['Size'].transform('mean'),inplace = True)\nplayst=playst[playst['Size'].notna()]\n#playst.head()\n\nplayst_mb['Size']=playst_mb['Size'].apply(clean_size_mb)\nplayst_mb['Size'].fillna(playst_mb.groupby('Category')['Size'].transform('mean'),inplace = True)\nplayst_mb=playst_mb[playst_mb['Size'].notna()]\n\n\nplayst['Installs']=playst['Installs'].apply(clean_install)\nplayst_mb['Installs']=playst_mb['Installs'].apply(clean_install)\n#playst.info()\n\nplayst['Price']=playst['Price'].apply(clean_price).astype(float)\nplayst_mb['Price']=playst_mb['Price'].apply(clean_price).astype(float)\n\nplayst['Reviews']=playst['Reviews'].astype(float)\nplayst_mb['Reviews']=playst_mb['Reviews'].astype(float)\n#playst.info()\n\nplayst=playst[playst['Rating'].notna()]\nplayst_mb=playst_mb[playst_mb['Rating'].notna()]","f408e56d":"playst.describe()","324ac5ea":"# rating distibution \nplt.figure(figsize=(12,8))\nfig = sns.kdeplot(playst['Rating'], color=\"darkblue\", shade = True)\nfig.set_xlabel(\"Rating\",size=15)\nfig.set_ylabel(\"Frequency\",size=15)\nplt.xticks(size=10)\nplt.yticks(size=10)\nplt.title('Rating Spread in Play Store',size = 25)\nplt.show()","f0cf46f4":"print('Avg. Size[MB]: ',playst_mb['Size'].mean())","e747d33f":"plt.figure(figsize=(12,8))\nfig = sns.kdeplot(playst_mb['Size'], color=\"darkred\", shade = True)\nfig.set_xlabel(\"Size(MB)\",size=15)\nfig.set_ylabel(\"Frequency\",size=15)\nplt.title('Size Spread in Play Store',size = 25)\nplt.show()","9acb46cd":"print('Avg.Reviews: ',playst['Reviews'].mean())","7b686c44":"plt.figure(figsize=(12,8))\nfig = sns.kdeplot(playst['Reviews'], color=\"darkorange\", shade = True)\nfig.set_xlabel(\"Reviews\",size=15)\nfig.set_ylabel(\"Frequency\",size=15)\nplt.title('Reviews Distribution',size = 25)\nplt.show()","e0f01f2c":"plt.figure(figsize=(12,8))\nfig=sns.countplot(playst_mb['Type'],palette='BrBG')\nfig.set_xlabel(\"Type\",size=15)\nfig.set_ylabel(\"Count\",size=15)\nplt.xticks(size=10)\nplt.yticks(size=10)\nplt.title('Count of apps (Free vs Paid)',size = 20)\nplt.show()","a395940d":"plt.figure(figsize=(12,8))\nfig=sns.countplot(playst['Content Rating'],palette='RdGy')\nfig.set_xlabel(\"Content Rating\",size=15)\nfig.set_ylabel(\"Count\",size=15)\nplt.xticks(size=10,rotation=90)\nplt.yticks(size=10)\nplt.title('Count of apps as per Content Rating',size = 20)\nplt.tight_layout()","5372fbed":"plt.figure(figsize=(12,8))\nfig=sns.countplot(playst['Category'],palette='Greens')\nfig.set_xlabel(\"Category\",size=15)\nfig.set_ylabel(\"Count\",size=15)\nplt.xticks(size=10,rotation=90)\nplt.yticks(size=10)\nplt.title('Count of apps as per Category',size = 20)\nplt.tight_layout()","0c23d4d4":"rat_catdf=playst.groupby('Category').describe().reset_index()\n#rat_catdf.head()","6b1e5178":"plt.figure(figsize=(12,8))\nfig=sns.barplot(rat_catdf['Category'],rat_catdf['Rating']['mean'],palette='Reds')\nfig.set_xlabel(\"Category\",size=15)\nfig.set_ylabel(\"Mean Rating\",size=15)\nplt.xticks(size=10,rotation=90)\nplt.yticks(size=10)\nplt.title('Mean Rating of Apps per Category',size = 20)\nplt.tight_layout()","8da1578d":"plt.figure(figsize=(12,8))\nfig=sns.regplot(x='Reviews',y='Rating',data=playst,color='orange')\nfig.set_xlabel(\"Reviews\",size=15)\nfig.set_ylabel(\"Ratings\",size=15)\nplt.xticks(size=10)\nplt.yticks(size=10)\nplt.title('Reviews vs Ratings',size = 20)\nplt.tight_layout()","9be9af3e":"plt.figure(figsize=(12,8))\nfig=sns.regplot(x='Reviews',y='Rating',data=playst[playst['Reviews']<1000000],color='orange')\nfig.set_xlabel(\"Reviews<1000000\",size=15)\nfig.set_ylabel(\"Ratings\",size=15)\nplt.xticks(size=10)\nplt.yticks(size=10)\nplt.title('Reviews vs Ratings- II',size = 20)\nplt.tight_layout()","170cd2c1":"plt.figure(figsize=(12,8))\nfig=sns.regplot(x='Price',y='Rating',data=playst[playst['Reviews']<1000000],color='orange')\nfig.set_xlabel(\"Price\",size=15)\nfig.set_ylabel(\"Ratings\",size=15)\nplt.xticks(size=10)\nplt.yticks(size=10)\nplt.title('Price vs Ratings',size = 20)\nplt.tight_layout()\n","57f881f3":"plt.figure(figsize=(12,8))\nfig=sns.regplot(x='Reviews',y='Size',data=playst_mb[playst_mb['Reviews']<1000000],color='orange')\nfig.set_ylabel(\"Size(MB)\",size=15)\nfig.set_xlabel(\"Reviews<1000000\",size=15)\nplt.xticks(size=10)\nplt.yticks(size=10)\nplt.title('Reviews vs Size',size = 20)\nplt.tight_layout()\n","745996b0":"plt.figure(figsize=(15,12))\nfig=sns.jointplot(x='Size',y='Rating',data=playst_mb,color='red',size=12)\nplt.show()","8538376c":"# Dropping columns that do not contribute numerically to the Regression Model\nplayst.drop(columns=['Current Ver','Android Ver','App','Last Updated'],inplace=True)\n#playst.isnull().sum()","7f9b26b0":"#encoding categorical values\nplayst=pd.get_dummies(playst,columns=['Type','Content Rating'],drop_first=True)\n#playst.head()","72a195b9":"# Splitting our mathematical feature columns and assigning it to 'X'\nX=playst.drop(columns=['Category','Rating','Genres'],axis=1)\n\n#Splitting our target variable 'Rating' and assigning it to 'y'\ny=playst['Rating']","f5e40f1e":"# We're splitting up our data set into groups called 'train' and 'test'\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(0)\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.3)","3c00ed2c":"from sklearn.preprocessing import StandardScaler\n\nscaler= StandardScaler()\n\n\n#scaling the training data(fitting the parameters and transforming the values)\nX_train=scaler.fit_transform(X_train)\n\n#transforming the test data.We avoid fitting the values to prevent data leakage!\nX_test=scaler.transform(X_test)\n","cc8645d6":"# Import xgboost ensemble model\nimport xgboost\n\n#core import for hyperparamter tuning\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import metrics","2ed7cbef":"#creates a xgbRegressor object\nregressor_xgb=xgboost.XGBRegressor()\n\n#fitting without hyperparamter tuning\nregressor_xgb.fit(X_train,y_train)\n\n#predictions\npred_xgb=regressor_xgb.predict(X_test)","c1acf7a0":"#Hyperparamter_Tuning_xgb\nn_estimators = [100, 500, 900, 1100, 1500]\nmax_depth = [2, 3, 5, 10, 15]\nbooster=['gbtree','gblinear']\nlearning_rate=[0.05,0.1,0.15,0.20]\nmin_child_weight=[1,2,3,4]\nbase_score=[0.25,0.5,0.75,1]\n\n# Define the grid of hyperparameters to search\nparameter_grid_xgb = {\n    'n_estimators': n_estimators,\n    'max_depth':max_depth,\n    'learning_rate':learning_rate,\n    'min_child_weight':min_child_weight,\n    'booster':booster,\n    'base_score':base_score\n    }","02f090de":"# Set up the random search with 5-fold cross validation\nregressor=xgboost.XGBRegressor()\n\nrandom_cv_xgb = RandomizedSearchCV(estimator=regressor,\n            param_distributions=parameter_grid_xgb,\n            cv=5, n_iter=50,\n            scoring = 'neg_mean_absolute_error',n_jobs = 3,\n            verbose = 5, \n            return_train_score = True,\n            random_state=42)\n\n#train on the RandomSearchCv object to get best estimators\nrandom_cv_xgb.fit(X_train,y_train)","ba19801f":"#getting the best estimators\nrandom_cv_xgb.best_estimator_","15d1f149":"#getting the best params\nrandom_cv_xgb.best_params_","67505da3":"# reinitializing the regressor object with the best probable estimators\nregressor_xgb=xgboost.XGBRegressor(base_score=1, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.15, max_delta_step=0, max_depth=5,\n             min_child_weight=2, missing=np.nan, monotone_constraints='()',\n             n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)","61a7f854":"# fitting the xgbRegressor on our training data\nregressor_xgb.fit(X_train,y_train)","6929dbb8":"# fetching the predictions on our test data\npredictions_xgb=regressor_xgb.predict(X_test)","ee2cab17":"from sklearn.ensemble import RandomForestRegressor\n# Instantiate model with 1000 decision trees","17f3c967":"rf=RandomForestRegressor(n_estimators = 10, random_state = 42)\n\n#fitting without hyperparamter tuning\nrf.fit(X_train,y_train)","49d15881":"#predictions\npred_rf=rf.predict(X_test)","b9c22c28":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","bd177336":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs =-1,scoring='neg_mean_squared_error')\n# Fit the random search model\nrf_random.fit(X_train, y_train)","80610a80":"#getting the best params\nrf_random.best_params_","03229fc1":"#getting the best estimators\nrf_random.best_estimator_","f4aef157":"rf=RandomForestRegressor(max_depth=50, max_features='sqrt', min_samples_leaf=4,\n                      min_samples_split=10, n_estimators=800,bootstrap=True)","95a84f5c":"rf.fit(X_train,y_train)","ef452050":"# Use the forest's predict method on the test data\npredictions_rf = rf.predict(X_test)","b795a828":"fig,ax = plt.subplots(2,2,figsize=(20,8))\n# Plot-label\nax[0][0].set_title('y_test vs predictions(XGBRegressor Tuned)',size=15)\n# Y-label\nax[0][0].set_ylabel('predcitions',size=10)\nax[0][0].scatter(y_test,predictions_xgb,color='red')\n\n# Plot-label\nax[0][1].set_title('y_test vs predictions(RFRegressor Tuned)',size=15)\nax[0][1].scatter(y_test,predictions_rf,color='red')\n\nax[1][0].set_title('y_test vs predictions(XGBRegressor UnTuned)',size=15)\n# Y-label\nax[1][0].set_ylabel('predcitions',size=10)\n#X-label\nax[1][0].set_xlabel('y_test',size=10)\n\nax[1][0].scatter(y_test,pred_xgb)\n\n# Plot-label\nax[1][1].set_title('y_test vs predictions(RFRegressor UnTuned)',size=15)\n#X-label\nax[1][1].set_xlabel('y_test',size=10)\n\nax[1][1].scatter(y_test,pred_rf)","a5fc64d3":"fig,ax=plt.subplots(2,2,figsize=(20,12))\n  \nsns.distplot((y_test-predictions_xgb),bins=20,ax=ax[0][0],color='red')\n\n#Plot Label\nax[0][0].set_title('Residual Analysis XGB(Tuned)', fontsize = 20)\n\nsns.distplot((y_test-predictions_rf),bins=20,ax=ax[0][1],color='red')\n\n#Plot Label\nax[0][1].set_title('Residual Analysis RF(Tuned)', fontsize = 20)\n\nsns.distplot((y_test-pred_xgb),bins=20,ax=ax[1][0])\n\n#Plot Label\nax[1][0].set_title('Residual Analysis XGB(UnTuned)', fontsize = 20)\n\nsns.distplot((y_test-pred_rf),bins=20,ax=ax[1][1])\n\n#Plot Label\nax[1][1].set_title('Residual Analysis RF(UnTuned)', fontsize = 20)\n\n","c3478ac9":"print('Mean Absolute Error\\tMAE_XGB:', metrics.mean_absolute_error(y_test, predictions_xgb),'\\t\\t MAE_RF:',  metrics.mean_absolute_error(y_test, predictions_rf))\nprint('Mean Squared Error\\tMSE_XGB:', metrics.mean_squared_error(y_test, predictions_xgb),'\\t\\t MSE_RF:',  metrics.mean_squared_error(y_test, predictions_rf))\nprint('Root Mean Squared Error\\tRMSE_XGB:', np.sqrt(metrics.mean_squared_error(y_test, predictions_xgb)),'\\t\\t RMSE_RF:', np.sqrt(metrics.mean_squared_error(y_test, predictions_rf)))","1b6b036f":"print('Mean Absolute Error\\tMAE_XGB:', metrics.mean_absolute_error(y_test, pred_xgb),'\\t\\t MAE_RF:',  metrics.mean_absolute_error(y_test, pred_rf))\nprint('Mean Squared Error\\tMSE_XGB:', metrics.mean_squared_error(y_test, pred_xgb),'\\t\\t MSE_RF:',  metrics.mean_squared_error(y_test, pred_rf))\nprint('Root Mean Squared Error\\tRMSE_XGB:', np.sqrt(metrics.mean_squared_error(y_test, pred_xgb)),'\\t\\t RMSE_RF:', np.sqrt(metrics.mean_squared_error(y_test, pred_rf)))","3da3a8d8":"# Calculate the absolute errors\nerrors_xgb = abs(predictions_xgb - y_test)\nmape_xgb = 100 * (errors_xgb \/ y_test)\n# Calculate and display accuracy\naccuracy_xgb = 100 - np.mean(mape_xgb)\n\nerrors_rf = abs(predictions_rf - y_test)\nmape_rf = 100 * (errors_rf \/ y_test)\n# Calculate and display accuracy\naccuracy_rf = 100 - np.mean(mape_rf)\n\nprint('Accuracy_XGBoost:', round(accuracy_xgb, 2), '%.\\t\\t\\tAccuracy_RandomForest:', round(accuracy_rf, 2), '%.')\n","524b5f59":"# Calculate the absolute errors\nerrors_xgb = abs(pred_xgb - y_test)\nmape_xgb = 100 * (errors_xgb \/ y_test)\n# Calculate and display accuracy\naccuracy_xgb = 100 - np.mean(mape_xgb)\n\nerrors_rf = abs(pred_rf - y_test)\nmape_rf = 100 * (errors_rf \/ y_test)\n# Calculate and display accuracy\naccuracy_rf = 100 - np.mean(mape_rf)\n\nprint('Accuracy_XGBoost:', round(accuracy_xgb, 2), '%.\\t\\t\\tAccuracy_RandomForest:', round(accuracy_rf, 2), '%.')\n","f154d893":"# Imports\n\n\n> 1. Let's get our environment ready with the libraries we'll need and then import the relevant ones beforehand!\n> 2. Pandas is one of the most widely used python libraries in data science. It provides high-performance, easy to use structures and data analysis tools.\n> 3. Matplotlib is a plotting library for the Python programming language\n> 4. Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.","3ba0e9ba":"# EDA,Comparisons and Projections\n\n> Let's create some simple plots to check out the data and relations amongs the feature columns in the Play-Store-Apps dataset!","04aa31fa":"# Feature Engineering\n\n> Performing one-hot encoding on Categorical Columns Type and Content-Rating besides dropping columns which don't contribute to regression models !","61f9f746":"> >### Random Forest Hyperparamter Tuning","452e694f":"## Accuracy: XGBOOST vs RandomForest [UnTuned]","0ca37ac5":"# Fetching the Data\n> Using Pandas to load the dataset into this notebook. Using pandas we can read our datafile (googleplaystore.csv) with the line below. Data-set loaded will be assigned to the variable playst.","7a7fca1f":"- **More Applications tend to score a rating of 4.0+ on the play store. Most of the play store applications are found useful and engaing by its userbase.**\n- **Average Rating is 4.19**","520d5212":"- **Avg # of Reviews seems to be 514049**","554f0594":"> >### Basic Model Fitting and Predictions","74002c24":">>### RF Model Training","06143056":"# Perfomance Evaluation: XGBOOST vs Random Forest\n\nWe shall evaluate and compare the perfomance of XGBOOST Regressor and Random Forest Regressor against each other.\nMetrics involved are-\n> 1. MAS\n> 2. MSE\n> 3. RMSE\n> 4. Accuracy","7e9d5624":"### XGB(UnTuned) vs RF(UnTuned) Error Evaluations","e96dcec3":"> >### XGB Core Imports","5ecd1fcd":"# Checking out the DataSet\n> We will run some exploratory analysis on our playstore dataset to have an insight of the features.We would check for the shape of the dataset, any missing or null values and will try to find out the correlation amongst the dataset features.\n#### Let's Play!","64791f9e":"### Thank You for visiting this Kernel !!\n> - Please do highlight any irregularities found in the kernel below in the comments!\n> - Do hit upvote if you like my work on this project !!\n> - Do check for the updated version soon!","47b30ba2":"- **Most of the applications have reviews' count below 1000000 if we shun the outliers**\n- **Apps scoring high on ratings generally have high Reviews**","9a7a5941":"> >### Basic Model Fitting and Predictions","c03f1898":"> >### XGB Hyperparameter tuning","2b84f393":"#### Average App 'Ratings' from different 'Categories' is more or less same!","d1e37841":"#### More popular the app is more likely it is to be highly rated!","787c2a8d":"#### Lighter Apps seems to be more popular than heavier ones!","0b0d3a9e":"#### Game and Family Category Applications seems to be swarming all over the playstore !!","39f6d0a5":">>### RF Regressor Predictions","99cc4f5a":"#### Applications on the playstore are mostly suitable for Everyone","8aa59ba3":"# Random Forest vs XGBOOST-GooglePlayStore(EDA,Comparisons and Predictions)","a3080be4":"## Accuracy: XGBOOST vs RandomForest [Tuned]","ebde6d09":">>### XGBRegressor() Predictions","5de40c3d":"# Splitting\/Scaling our train\/test Data","7df1449c":"> >### Random Forest Imports","22dc9321":"- **Average Size(MB) of applications in the play store is 22.35MB**","c34b7a02":"> ### 1. XGB Model performs more or less the same even after tuning the hyperparameters\n> ### 2. We see a slight improvement in the Random Forest Regressor with hyperparameters tuning","0c7037da":"**The line of best fit would clearly fit our Tuned Models better than the Untuned Models.**","500f4cee":"> ## Random Forest Regressor","e36b4e8d":"#### Applications in the playstore are mostly free to use.","2b66b023":"> >### XGB Model Training","30e9cbac":"#### High Priced Apps suffer on ratings if they don't perform as they claim","6d114436":"# Tuning and Training our Model\n\n> ## XGBOOST Regressor","ea83dded":"### XGB(Tuned) vs RF(Tuned) Error Evaluations","8f205e9c":"# Conclusion\n\n## 1. MODEL\n\n- **For both Random Forest and XGBOOST Regressor, perfomance is comparable**\n- **Tuned Models performs expectedly better than their untuned versions**\n- **Tuned XGBOOST edges Tuned Random Forest by 0.05% performing better in accuracy**\n- **XGBOOST performs better overall on every Error metric compared to Random Forest Regressor**\n\n## 2. PlayStore Predictions\n\n- Play Store is flodded with Apps that are mostly free to use\n- Store has a high number of apps from **Family and Game Category**\n- Avg Number Reviews on an app is below **1000000**\n- Average size of an app on the play store is **22MB**\n- Average price of a play store app is **0.96** with Highest being **400** in dollars\n- **Lighter Applications** are more popular and highly rated\n- Apps with **high Price** suffer from **low ratings** more readily\n- Apps with **good ratings** have **more** number of user reviews","c721ec33":"# Data Cleaning\n\n> Cleaning and fine-tuning our columns Installs,Price,Reviews and Size"}}