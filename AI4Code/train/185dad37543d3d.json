{"cell_type":{"631a839f":"code","ad4afa7f":"code","dab16930":"code","6dca44d4":"code","4209cc56":"code","af581f47":"code","0244afa4":"code","22cd5d5f":"code","7bb341d1":"code","9a62a91b":"code","d7461b40":"code","2c384215":"markdown","aea6c912":"markdown","9d98e945":"markdown","68a5a866":"markdown","4617832a":"markdown","f7f8407e":"markdown","9fff8ec2":"markdown","7da7c715":"markdown","de978c26":"markdown","a0d6cc91":"markdown","2dfe652a":"markdown","77269b05":"markdown"},"source":{"631a839f":"# importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimport seaborn as sns\nimport json\n\n! cp ..\/input\/indian-products-on-amazon\/amazon_vfl_reviews.csv .\njson_file = 'amazon_vfl_reviews.csv'\ndf = pd.read_csv(json_file)\ndf.info()","ad4afa7f":"df.head()","dab16930":"for tp in df.groupby('name')['asin']:\n    if len(tp[1].unique()) > 1:\n        print(tp[0], tp[1].unique())","6dca44d4":"f, ax = plt.subplots(1, 2, figsize=(18,8))\ncs = ['r', 'dodgerblue', 'orange', 'green', 'pink']\ndf['rating'].value_counts().plot(kind='pie', autopct='%2.2f%%', ax=ax[0], colors=cs)\ndf['rating'].value_counts().plot(kind='barh', ax=ax[1], color=cs)\nax[0].set_title('Share of rating (pie)')\nax[0].set_ylabel('Rating Share')\nax[1].set_title('Share of ratin(bar)')\nplt.show()","4209cc56":"df[df.review.isna()]","af581f47":"# replace NaN-valued review with word NULL\ndf.fillna({'review': 'NULL'}, inplace=True)","0244afa4":"from wordcloud import WordCloud\nimport os\nfrom PIL import Image\nimport urllib\n\n# Control the font for our wordcloud\nif not os.path.exists('Comfortaa-Regular.ttf'):\n    urllib.request.urlretrieve('http:\/\/git.io\/JTqLk', 'Comfortaa-Regular.ttf')\n\nif not os.path.exists('cloud.png'):\n    urllib.request.urlretrieve('http:\/\/git.io\/JTORU', 'cloud.png')\n    \ntext = ' '.join(str(t) for t in df.review)\nmask = np.array(Image.open('cloud.png'))\nwc = WordCloud(max_words=100, background_color='white', \n              font_path='.\/Comfortaa-Regular.ttf', mask=mask,\n#                 max_font_size=100,\n              width=mask.shape[1], height=mask.shape[0]).generate(text)\n\nplt.figure(figsize=(24, 12))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.show()","22cd5d5f":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n\nclass Bayes:\n    def _pipeline(self, df):\n        cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n        review = df[['review']]\n        Xtrain, Xtest, ytrain, ytest = train_test_split(review, df.rating, random_state=2)\n        cv.fit(pd.concat([Xtrain.review, Xtest.review]))\n        Xtrain = cv.transform(Xtrain.review)\n        Xtest  = cv.transform(Xtest.review)\n\n        model = MultinomialNB()\n        model.fit(Xtrain, ytrain)\n        \n        ypred = model.predict(Xtest)\n        print(\"Bayes model accuracy score: \", accuracy_score(ytest, ypred))\n                             \nBayes()._pipeline(df)","7bb341d1":"from xgboost import XGBClassifier\n\nclass Xgb:\n    def _pipeline(self, df):\n        cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n        review = df[['review']]\n        Xtrain, Xtest, ytrain, ytest = train_test_split(review, df.rating, random_state=2)\n        cv.fit(pd.concat([Xtrain.review, Xtest.review]))\n        Xtrain = cv.transform(Xtrain.review)\n        Xtest  = cv.transform(Xtest.review)\n\n        model = XGBClassifier()\n        model.fit(Xtrain, ytrain)\n        \n        ypred = model.predict(Xtest)\n        print(\"Xgboost classifier accuracy score: \", accuracy_score(ytest, ypred))\n                             \nXgb()._pipeline(df)","9a62a91b":"### import keras \nfrom keras import layers, Input\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Flatten, Dense, Embedding, Dropout, LSTM, GRU, Bidirectional\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport gensim.downloader as api\nimport logging\nimport math\n# from tqdm.notebook import tqdm\nimport tensorflow_hub as hub\n\nclass Classifier():\n  def __init__(self):\n    self.train = None\n    self.test = None \n    self.model = None\n    \n  def load_data(self, df):\n      \"\"\" Load train, test csv files and return pandas.DataFrame\n      \"\"\"\n      self.train, self.test = train_test_split(df, test_size=0.2)\n      self.train.rename({'review': 'text', 'rating': 'target'}, axis='columns', inplace=True)\n      self.test.rename({'review': 'text', 'rating': 'target'}, axis='columns', inplace=True)\n\n  \n  def save_predictions(self, y_preds):\n      sub = pd.read_csv(f\"sampleSubmission.csv\")\n      sub['Sentiment'] = y_preds \n      sub.to_csv(f\"submission_{self.__class__.__name__}.csv\", index=False)\n      logging.info(f'Prediction exported to submission_{self.__class__.__name__}.csv')\n  \n\nclass C_NN(Classifier):\n    def __init__(self, max_features=10000, embed_size=128, max_len=300):\n        self.max_features=max_features\n        self.embed_size=embed_size\n        self.max_len=max_len\n    \n    def tokenize_text(self, text_train, text_test):\n        '''@para: max_features, the most commenly used words in data set\n        @input are vector of text\n        '''\n        tokenizer = Tokenizer(num_words=self.max_features)\n        text = pd.concat([text_train, text_test])\n        tokenizer.fit_on_texts(text)\n\n        sequence_train = tokenizer.texts_to_sequences(text_train)\n        tokenized_train = pad_sequences(sequence_train, maxlen=self.max_len)\n        logging.info('Train text tokeninzed')\n\n        sequence_test = tokenizer.texts_to_sequences(text_test)\n        tokenized_test = pad_sequences(sequence_test, maxlen=self.max_len)\n        logging.info('Test text tokeninzed')\n        return tokenized_train, tokenized_test, tokenizer\n      \n    def build_model(self, embed_matrix=[]):\n        text_input = Input(shape=(self.max_len, ))\n        embed_text = layers.Embedding(self.max_features, self.embed_size)(text_input)\n        if len(embed_matrix) > 0:\n            embed_text = layers.Embedding(self.max_features, self.embed_size, \\\n                                          weights=[embed_matrix], trainable=False)(text_input)\n            \n        branch_a = layers.Bidirectional(layers.GRU(32, return_sequences=True))(embed_text)\n        branch_b = layers.GlobalMaxPool1D()(branch_a)\n\n        x = layers.Dense(64, activation='relu')(branch_b)\n        x = layers.Dropout(0.2)(x)\n\n        x = layers.Dense(32, activation='relu')(branch_b)\n        x = layers.Dropout(0.2)(x)\n        branch_z = layers.Dense(6, activation='softmax')(x)\n        \n        model = Model(inputs=text_input, outputs=branch_z)\n        self.model = model\n\n        return model\n        \n    def embed_word_vector(self, word_index, model='glove-wiki-gigaword-100'):\n        glove = api.load(model) # default: wikipedia 6B tokens, uncased\n        zeros = [0] * self.embed_size\n        matrix = np.zeros((self.max_features, self.embed_size))\n          \n        for word, i in word_index.items(): \n            if i >= self.max_features or word not in glove: continue # matrix[0] is zeros, that's also why >= is here\n            matrix[i] = glove[word]\n\n        logging.info('Matrix with embedded word vector created')\n        return matrix\n\n    def run(self, x_train, y_train):\n        checkpoint = ModelCheckpoint('weights_base_best.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n        early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=3)\n\n        self.model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.8, random_state=2020)\n        BATCH_SIZE = max(16, 2 ** int(math.log(len(X_tra) \/ 100, 2)))\n        logging.info(f\"Batch size is set to {BATCH_SIZE}\")\n        history = self.model.fit(X_tra, y_tra, epochs=30, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), \\\n                              callbacks=[checkpoint, early], verbose=0)\n\n        return history\n\n\nc = C_NN(max_features=10000, embed_size=300, max_len=300)\nc.load_data(df)  \nlabels = to_categorical(c.train.target, num_classes=6)\nlabels\n\nvector_train, vector_test, tokenizer = c.tokenize_text(c.train.text, c.test.text)\nembed = c.embed_word_vector(tokenizer.word_index, 'word2vec-google-news-300')\nc.build_model(embed_matrix=embed)\nc.run(vector_train, labels)\n","d7461b40":"model = load_model('weights_base_best.hdf5')\ny_preds = model.predict(vector_test)\nfinal = np.argmax(y_preds, axis=1)\nprint('CNN accuracy score is', accuracy_score(c.test.target, final))\n","2c384215":"## Loading Data & Import Libraries","aea6c912":"Great, with CNN we improved the accuracy score from 0.839 to 0.86. Although the improvement is little, we know CNN could be the right tool for this task.\n\nFor those who are interested, potential strategies help with boosting the score includes:\n1. Embed a different pretrained model, such as 'glove-wiki-gigaword-300'\n2. Use [Bidirectional Encoder Representations from Transformers\uff1a BERT](https:\/\/arxiv.org\/abs\/1810.04805) \n3. Collect more data and feed them into your model.\n\nSentiment analysis is a popular ML topic and lots of work have been done in this field. One great thing about this task is that many pre-trained models on one dataset can be easily transformed to another datasets with few adjustments. Readers interested in this topic can find more great kernels from Kaggle's competition [Sentiment Analysis on Movie Reviews](https:\/\/www.kaggle.com\/c\/sentiment-analysis-on-movie-reviews).\n","9d98e945":"Except for the words (e.g., 'Amazon' and 'Product') that are closely related to the platform, words like 'soap', 'hair', 'smell', 'skin' are frequently mentioned in the reviews, which makes sense since some products pertains to categories such as hair and skin care products. \n\nNote that, the word cloud itself is insufficient to reflect the sentiment towards deals. That being said, a word cloud generated from reviews with rating 1 would be very similar to the one generated from reviews with rating 5. This is because the words (e.g., 'angry', 'suck', 'poor') exhibiting customers' emotion are overwhelmed by other words. Therefore we need to build a model to analyze the sentiment from reviews.","68a5a866":"## `rating`\nFirst step: find out the distribution of ratings.","4617832a":"51.9% customers were very satisfied with the deal (rating 5), while nearly 20% were unsatisfied with the deal (rating 1) because of either the high-price or the product quanlity. 16.7% have considered the order as fine (rating 4). ","f7f8407e":"# Sentiment Analysis\n\nIn this section, we're going to build three models to make predictions on the ratings based on reviews. The first one is Naive Bayes, a simple yet efficient model usually served as a benchmark model, and then we will try a another model: xgboost classifier. And finally, we will implement a conventional neural network, a powerful model that is widely used in various fields. \n\nThe goal here is to provide a hint on how to conduct sentiment analysis on reviews, perfecting the model and improving scores to a certain level is beyond the scope of this kernel and hence will not be discussed.","9fff8ec2":"Each `asin` (product id) corresponds to a product `name`, the reverse, however, is not true. This means products with different ids may have identical names. For example, if we group the dataset by name and extract its unique id(s), we can see 'Cinthol-Lime-Soap-100-Pack' corresponds two ids: ['B01CGESSG0' 'B07V6DQNP5']. \n\nThis subtle difference could have influence in further analysis on our data, but for now we only care about the rating and review given by customers. Thus, we move forward and ignore the difference.","7da7c715":"# Overview\n\nThis is a kernel in progress and more updates will be made in the coming days. Please upvote if you like it, or leave below a comment to elaborate why you don't (I promise I will not hold a grudge \ud83d\ude09).\n\n<img src=\"http:\/\/git.io\/JTYV6\" width=\"500px\" alt=\"amazon.in\">\n\nIndia needs a boost to its economy that can also simultaneously transform local businesses. In order to support Indian brands, 'Vocal for Local' campaign was started in India. Emphasizing on a self-reliant, or \u2018aatmanirbhar\u2019, India, the Indian government has asked that products not just be made in India, but also for the promotion of local brands, manufacturing, and supply chain.\n\nData analysis on millions of reviews through Amazon is a crucial factor for the company. These reviews generates a lot of data -- data that can be analyzed and used for advertising, business decisions, understanding of customers' needs and wishes on the products (and platforms), guiding marketing initiatives, implementation of innovative additional services and much more.\n\nThis dataset comprises of 2500+ reviews of about 100+ Indian Products pertaining to categories like hair and skin care products, clothes, electronic gadgets, etc from Amazon.\n\n","de978c26":"Xgboost classifier works better, the accuracy score is improved to 83.9%. Let's see whether we can further improve this score to a higher value with CNN. ","a0d6cc91":"This simple Bayes model achieved 79.2% accuracy on ratings, which is not so bad. ","2dfe652a":"A quick view of the dataset tells us that:\n- There are 2782 entries, and 5 columns,\n- The column `review` have 4 missing values (we will deal with it later),\n- Only one feature (`rating`) is numeric.\n\n## Understadning, Wrangling and Cleaning Data\n\nColumn description:\n- `asin` : Amazon Standard Identification Number - a unique id for each product on Amazon\n- `name` : name of the product\n- `date` : date on which review was posted\n- `rating` : rating given to the product ( out of 5 )\n- `review`: review given to the product","77269b05":"## `review`\n\nNow let's analyze the reviews. Remember that we have 4 missing values in `review`. We can either drop the corresponding entries or replace the NaN-valued review with a word such as 'NULL'. In this kernel, we take the later strategy. Then we will visualize the reviews with word cloud to reveal the focus of customers."}}