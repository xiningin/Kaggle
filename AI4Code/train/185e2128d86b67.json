{"cell_type":{"9f08bd44":"code","a1291cb0":"code","5b08e77c":"code","c0daf64b":"code","94c0107b":"code","48cc65a5":"code","9191e762":"code","dd52403e":"code","d3bfee5e":"code","64f3ae82":"code","1f75fb24":"code","6df4af0a":"code","2913169d":"code","1d327fae":"code","6cd0a297":"code","218a910b":"code","f4480afc":"code","ff94975f":"code","1b9f0178":"code","5d2706bd":"code","3bd2c52c":"code","96701396":"code","6870edcd":"code","361ca884":"code","09b29a63":"code","8d99424e":"code","1e56c051":"code","f06d573f":"code","bb914124":"code","fd20aacb":"code","9b3b525c":"code","737b1d4e":"code","670fce74":"code","e1d96ced":"code","ee56ae8b":"code","2d521a13":"code","3e78876f":"code","28b76a27":"code","9dea5bb6":"code","99f49aa6":"code","8201e6e8":"code","9089138b":"code","3ea20f79":"code","0750d24c":"code","41258803":"code","afd932fe":"code","33759023":"code","d53da51c":"code","a96581b4":"code","42d2e715":"code","f96bdef4":"code","935574ec":"code","a103bba4":"code","4283b69a":"code","c824c686":"code","888df318":"code","afaba9a0":"code","3c4682b1":"code","f8479ac2":"code","784ad46e":"code","ed5da010":"code","0c2292f1":"code","48892867":"code","aafc04f8":"code","76ea8e14":"code","79c112af":"code","f8411775":"code","ce5fb9d7":"code","c1515771":"code","b4df42db":"code","18a831f8":"code","6fd5d943":"code","27f36e04":"code","6ddec5a8":"code","9b1eead6":"markdown","15ae5f9b":"markdown","9cfebbcd":"markdown","21cd86b5":"markdown","7249282a":"markdown","e0565a5d":"markdown","7a8ca7a5":"markdown","af439a4e":"markdown","f7d6be07":"markdown","a2e15f15":"markdown","597e0cbf":"markdown","da9f397e":"markdown","34ad252f":"markdown","0b1332a3":"markdown","62199102":"markdown","a9adb099":"markdown","0b71f953":"markdown","6bfeb4e8":"markdown","3e79f1e8":"markdown","a48ae002":"markdown","b5a30b0a":"markdown","9da90ba4":"markdown","56bd1040":"markdown","ce5dd474":"markdown","ed6fc358":"markdown","67c8ae05":"markdown","3243f5c6":"markdown","97938cac":"markdown","b88f150c":"markdown","8b029622":"markdown","71b04317":"markdown","8c8e1988":"markdown","1da347ef":"markdown","36c5f318":"markdown","f7977563":"markdown","07ca86aa":"markdown","1448d639":"markdown","e688cf7c":"markdown","19bcbb1a":"markdown","58ab2e82":"markdown","3ff57a64":"markdown","12e07c3a":"markdown","4313ad09":"markdown","0d1d719d":"markdown","895435d0":"markdown","44b58e3f":"markdown","1deea00d":"markdown","d4ead696":"markdown","ab01d50b":"markdown","1db32b50":"markdown","7399d502":"markdown","edb5fce5":"markdown","796a8586":"markdown","37111026":"markdown","4c331fbf":"markdown","813475ac":"markdown","d6001dee":"markdown","608deec3":"markdown","62a77768":"markdown","4c8fab1a":"markdown","4b5cd591":"markdown","e31d0254":"markdown","889fdaaf":"markdown","768817f5":"markdown","05840c1d":"markdown","f523fd09":"markdown","bee50a10":"markdown"},"source":{"9f08bd44":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom fancyimpute import IterativeImputer as MICE\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import optimizers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dropout\n","a1291cb0":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","5b08e77c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","c0daf64b":"df = pd.read_csv('..\/input\/widsdatathon2021\/TrainingWiDS2021.csv')\ndf_raw = df.copy()\ndf_unlabel = pd.read_csv('..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv')\nunlabel_raw = df_unlabel.copy() ","94c0107b":"print('Shape of data training : {}'.format(df.shape)) \nprint('Shape of data unlabel (test): {}'.format(df_unlabel.shape)) \ndf.info()\ndf.describe()\nnp.shape(df)\ndf.head()","48cc65a5":"sns.countplot(x='diabetes_mellitus',data=df)","9191e762":"(df['diabetes_mellitus'].value_counts()\/df['diabetes_mellitus'].count())*100","dd52403e":"plt.figure(figsize=(10,10))\nsns.heatmap(df.isnull(), yticklabels=False,cbar=False,cmap='viridis')","d3bfee5e":"Missing_valuestable = pd.DataFrame(df.isnull().mean() * 100)\nMissing_valuestable.columns={'Missing_perc'}\nMv =Missing_valuestable[Missing_valuestable['Missing_perc']>80]\nMv = Mv.reset_index()\n\nListcolumn_manymissings = Mv['index'].to_list()\nMv","64f3ae82":"plt.figure(figsize=(18,10))\ndf.corr()['diabetes_mellitus'][:-1].sort_values().plot(kind='bar')\nMcorr = pd.DataFrame(df.corr()['diabetes_mellitus'][:-1].sort_values().abs())\n\nMcorr = Mcorr.reset_index()\nMcorr=Mcorr[Mcorr['diabetes_mellitus']<0.05]\nListcolumn_low_corr = Mcorr['index'].tolist()\n\n","1f75fb24":"plt.figure(figsize=(18,18))\nsns.heatmap(df.corr())","6df4af0a":"df['readmission_status'].value_counts()","2913169d":"Listcolumn_to_delete =Listcolumn_manymissings +Listcolumn_low_corr\nListcolumn_to_delete = set(Listcolumn_to_delete)","1d327fae":"df.drop(Listcolumn_to_delete, axis=1, inplace=True)","6cd0a297":"df.drop(['readmission_status' ], axis=1, inplace= True)","218a910b":"np.shape(df)","f4480afc":"plt.figure(figsize=(18,18))\nsns.heatmap(df.corr())","ff94975f":"df.drop(['h1_hemaglobin_max','h1_hemaglobin_min','h1_hematocrit_max', 'd1_hemaglobin_max','d1_hemaglobin_min','d1_hematocrit_max','h1_glucose_max','d1_sysbp_max','d1_bun_max', 'd1_creatinine_max','h1_diasbp_min' ,'h1_potassium_min'], axis=1, inplace= True)","1b9f0178":"df_raw[df_raw['height'].isnull() & df_raw['weight'].notnull() & df_raw['bmi'].notnull()]","5d2706bd":"df_raw[df_raw['height'].notnull() & df_raw['weight'].isnull() & df_raw['bmi'].notnull()]","3bd2c52c":"df.drop(['bmi'], axis=1, inplace= True)","96701396":"df_numericalcolumns= df._get_numeric_data()\ndf_numericalcolumns= df_numericalcolumns.drop(['diabetes_mellitus'], axis=1)","6870edcd":"df_numericalcolumns.iloc[:, :]  = MICE().fit_transform(df_numericalcolumns)","361ca884":"sns.heatmap(df_numericalcolumns.isnull(), yticklabels=False,cbar=False,cmap='viridis')","09b29a63":" df['ethnicity'].fillna('Other\/Unknown', inplace=True)","8d99424e":"cols = df.columns\nnum_cols = df._get_numeric_data().columns\nlist_categoricalcolumns= list(set(cols) - set(num_cols))\ndf_categoricalcolumns= df[list_categoricalcolumns]\ndf_categoricalcolumns.head()","1e56c051":"t = df.groupby(['icu_admit_source', 'hospital_admit_source']).sum().reset_index()[['icu_admit_source', 'hospital_admit_source']]\nt","f06d573f":"icu_admit_source_d = {'Accident & Emergency': 'Accident & Emergency','Floor': 'ICU_Floor', 'Other ICU': 'Other ICU', 'Operating Room \/ Recovery': 'Operating Room \/ Recovery', 'Other Hospital': 'Other Hospital'}","bb914124":"df['icu_admit_source']= df.icu_admit_source.map(icu_admit_source_d)","fd20aacb":"imp = SimpleImputer(strategy=\"most_frequent\")\ndf_categoricalcolumns.iloc[:, :]  =imp.fit_transform(df_categoricalcolumns)","9b3b525c":"df=pd.concat([df_numericalcolumns,df['diabetes_mellitus'],df_categoricalcolumns],axis=1)","737b1d4e":"np.shape(df)","670fce74":"for i in list_categoricalcolumns:\n    i= pd.get_dummies(df[i],drop_first=True)\n    i=pd.DataFrame(i)\n    df_numericalcolumns=pd.concat([df_numericalcolumns,i],axis=1)","e1d96ced":"df = df_numericalcolumns","ee56ae8b":"np.shape(df)","2d521a13":"df_unlabel.drop(Listcolumn_to_delete, axis=1, inplace=True)\ndf_unlabel.drop(['readmission_status'], axis=1, inplace= True)\ndf_unlabel.drop(['d1_hemaglobin_max','d1_hemaglobin_min','d1_hematocrit_max','h1_glucose_max','d1_sysbp_max','d1_bun_max', 'd1_creatinine_max','h1_diasbp_min' ], axis=1, inplace= True)\ndf_unlabel.drop(['bmi' ], axis=1, inplace= True)","3e78876f":"df_unlabel_numcolumns= df_unlabel._get_numeric_data()","28b76a27":"df_unlabel_numcolumns.iloc[:, :]  = MICE().fit_transform(df_unlabel_numcolumns)","9dea5bb6":" df_unlabel['ethnicity'].fillna('Other\/Unknown', inplace=True)","99f49aa6":"df_unlabel['icu_admit_source']= df_unlabel.icu_admit_source.map(icu_admit_source_d)","8201e6e8":"df_unlabel_categoricalcolumns= df_unlabel[list_categoricalcolumns]\ndf_unlabel_categoricalcolumns.iloc[:, :]  =imp.fit_transform(df_unlabel_categoricalcolumns)","9089138b":"df_unlabel=pd.concat([df_unlabel_numcolumns,df_unlabel_categoricalcolumns],axis=1)","3ea20f79":"for i in list_categoricalcolumns:\n    i= pd.get_dummies(df_unlabel[i],drop_first=True)\n    i=pd.DataFrame(i)\n    df_unlabel_numcolumns=pd.concat([df_unlabel_numcolumns,i],axis=1)","0750d24c":"df_unlabel=df_unlabel_numcolumns","41258803":"f_df, f_df_unlabel = df.align(df_unlabel, join='inner', axis=1)\nf_df['diabetes_mellitus']=df_raw['diabetes_mellitus']","afd932fe":"f_df = f_df.loc[:,~f_df.columns.duplicated()]\nf_df_unlabel = f_df_unlabel.loc[:,~f_df_unlabel.columns.duplicated()]","33759023":"plt.figure(figsize=(12,12))\nsns.heatmap(f_df.corr())","d53da51c":"X = np.array(f_df.iloc[:, f_df.columns != 'diabetes_mellitus'])\ny = np.array(f_df.iloc[:, f_df.columns == 'diabetes_mellitus'])\nprint('Shape of X: {}'.format(X.shape))\nprint('Shape of y: {}'.format(y.shape))","a96581b4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, shuffle=True)\n\nprint(\"X_train dataset: \", X_train.shape)\nprint(\"y_train dataset: \", y_train.shape)\nprint(\"X_test dataset: \", X_test.shape)\nprint(\"y_test dataset: \", y_test.shape)","42d2e715":"under = RandomUnderSampler(sampling_strategy=0.5)\nX_train_under, y_train_under = under.fit_resample(X_train, y_train)","f96bdef4":"over = RandomOverSampler(sampling_strategy='minority')\nX_train_res, y_train_res = over.fit_resample(X_train_under, y_train_under)","935574ec":"print(\"Raw Dataset, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Raw Dataset, counts of label '0': {} \\n\".format(sum(y_train==0)))\nprint(\"After UnderSampling, counts of label '1': {}\".format(sum(y_train_under==1)))\nprint(\"After UnderSampling, counts of label '0': {} \\n\".format(sum(y_train_under==0)))\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After OverSampling, counts of label '0': {} \\n\".format(sum(y_train_res==0)))","a103bba4":"std = StandardScaler()\nX_train_std = std.fit_transform(X_train_res)\nX_test_std = std.transform(X_test)","4283b69a":"pca = PCA(n_components = X_train_std.shape[1])\npca_data = pca.fit_transform(X_train_std)\n\npercent_var_explained = pca.explained_variance_\/(np.sum(pca.explained_variance_))\ncumm_var_explained = np.cumsum(percent_var_explained)\n\nplt.plot(cumm_var_explained)\nplt.grid()\nplt.xlabel(\"n_components\")\nplt.ylabel(\"% variance explained\")\nplt.show()","c824c686":"cumm_var_explained\npca = PCA(n_components=30)\npca_train_data = pca.fit_transform(X_train_std)\n\ndf_train_pca = pd.DataFrame(pca_train_data)\n\ncorr = df_train_pca.corr()\nplt.figure(figsize = (12,10))\n\nsns.heatmap(corr, annot = False, vmin=-1, vmax=1, cmap=\"YlGnBu\", linewidths=.5)\nplt.grid(b=True, color='#f68c1f', alpha=0.1)\nplt.show()\n","888df318":"pca_test_data = pca.transform(X_test_std)\ndf_test_pca = pd.DataFrame(pca_test_data)","afaba9a0":"f_df_unlabel_std = std.transform(f_df_unlabel)\npca_unlabel_data = pca.transform(f_df_unlabel_std)\n\nf_df_unlabel_pca = pd.DataFrame(pca_unlabel_data)\n\nX_train_std =pd.DataFrame(X_train_std)\nX_test_std =pd.DataFrame(X_test_std)","3c4682b1":"logmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)\npredictions_log = logmodel.predict(X_test)\nprint(classification_report(y_test,predictions_log))","f8479ac2":"logmodel_rebalanced = LogisticRegression(C = 0.0001)\nlogmodel_rebalanced.fit(X_train_res,y_train_res)\npredictions_logreb = logmodel_rebalanced.predict(X_test)\nprint(classification_report(y_test,predictions_logreb))","784ad46e":"logmodel_reb_pca = LogisticRegression()\nlogmodel_reb_pca.fit(df_train_pca,y_train_res)\n\npredictions_logreb_pca = logmodel_reb_pca.predict(df_test_pca)\nprint(classification_report(y_test,predictions_logreb_pca))\n","ed5da010":"rfc = RandomForestClassifier(n_estimators=100)\n\nrfc.fit(X_train,y_train)\n\npredictions_rfc = rfc.predict(X_test)\n\nprint(classification_report(y_test,predictions_rfc))","0c2292f1":"rfc_reb = RandomForestClassifier(n_estimators=100)\nrfc_reb.fit(X_train_res, y_train_res)\npredictions_rfc_reb= rfc_reb.predict(X_test)\nprint(classification_report(y_test,predictions_rfc_reb))","48892867":"rfc_reb_pca = RandomForestClassifier(n_estimators=100)\n\nrfc_reb_pca.fit(df_train_pca,y_train_res)\n\npredictions_rfc_reb_pca = rfc_reb_pca.predict(df_test_pca)\nprint(classification_report(y_test,predictions_rfc_reb_pca))","aafc04f8":"modelgb_reb = XGBClassifier()\nmodelgb_reb.fit(X_train_res,y_train_res)\npredictions_xgb_reb= modelgb_reb.predict(X_test)\nprint(classification_report(y_test,predictions_xgb_reb))","76ea8e14":"\nmodelgb_reb_pca = XGBClassifier()\nmodelgb_reb_pca.fit(df_train_pca,y_train_res)\npredictions_xgb_reb_pca= modelgb_reb_pca.predict(df_test_pca)\nprint(classification_report(y_test,predictions_xgb_reb_pca))","79c112af":"alg = XGBClassifier(objective='binary:logistic')\n\nparameters = {'nthread':[4], \n              'objective':['binary:logistic'],\n              'learning_rate': [0.05], \n              'max_depth': [2,4,6],\n              'min_child_weight': [11],\n              'silent': [1],\n              'subsample': [0.8],\n              'colsample_bytree': [0.7],\n              'n_estimators': [50,100,200],\n              'missing':[-999],\n              'seed': [1337]}\n\n\ngrid_xgb = GridSearchCV(alg, parameters, n_jobs=5, \n                   cv=StratifiedKFold(n_splits=5, shuffle=True), \n                   scoring='roc_auc',\n                   verbose=2, refit=True)\n\ngrid_xgb.fit(X_train_res, y_train_res)\ngrid_xgb.best_score_, grid_xgb.best_params_","f8411775":"predictions_xgb_gridsearch = grid_xgb.predict(X_test)\nprint(classification_report(y_test,predictions_xgb_gridsearch))","ce5fb9d7":"learning_rate=0.0001\nhidden_layer_act='relu'\noutput_layer_act='sigmoid'\nno_epochs=500","c1515771":"model_dl = Sequential()\nmodel_dl.add(Dense(47, activation=hidden_layer_act))\nmodel_dl.add(Dense(23, activation=hidden_layer_act))\nmodel_dl.add(Dropout(0.5))\nmodel_dl.add(Dense(10, activation=hidden_layer_act))\nmodel_dl.add(Dropout(0.5))\nmodel_dl.add(Dense(1, activation=output_layer_act))","b4df42db":"model_dl.compile(loss='binary_crossentropy',optimizer='adam')","18a831f8":"early_stop = EarlyStopping(monitor ='val_loss',mode='min',verbose=1,patience=25)","6fd5d943":"model_dl.fit(X_train_std, y_train_res, epochs=no_epochs, verbose=2, validation_data = (X_test_std,y_test), callbacks=[early_stop])","27f36e04":"losses = pd.DataFrame(model_dl.history.history)\nlosses.plot()","6ddec5a8":"predictions_dl = model_dl.predict(X_test)\npredictions_dl = [int(round(x[0])) for x in predictions_dl]\nprint(classification_report(y_test,predictions_dl))","9b1eead6":"we can see that this last model requires a lot of improvements","15ae5f9b":"we can see that this specific XGB model is giving slightly better results compared to the logistic regression.","9cfebbcd":"we can see that there are columns that are highly correlated among themselves, we should consider dropping one of them to avoid multicollinearity","21cd86b5":"we can see that the PCA is not improving our model, this is probably because we didnt have a lot of columns so the feature selection was not required. ","7249282a":"# 12) Align the dataset df with unlabel test data to make sure they have the same columns creating f_df and f_df_unlabel","e0565a5d":"# 3) Check if there are missing values","7a8ca7a5":"Oversampling strategy:","af439a4e":"singleImputer:","f7d6be07":"One hot encoding:","a2e15f15":"WiDS2021\n\nAuthor: PG\n","597e0cbf":"# 14) Performing Random Undersampling and then Oversampling to rebalance the dataset","da9f397e":"remove duplicated columns, just to be sure: ","34ad252f":"Undersampling strategy:","0b1332a3":"# 15) Apply PCA to perform feature selection on standardized X dataset ","62199102":"# 18) Creation of a xgboost model on rebalanced dataset and creation of classification report","a9adb099":"weight can't be derived from height and bmi becuase if weight is missing the other variables are missing as well. ","0b71f953":"bmi is derived from the formulas kg\/m2, having height and weight we should remove it from the columns to avoid multicollinearity","6bfeb4e8":"# 9) Use SingleImputer to replace missing values in categorical variables, strategy \"most frequent\", replace missing values in ethnicity with pre-existent category Other\/Unknown","3e79f1e8":"Recreating the dataset with no missing values:","a48ae002":"Apply the PCA to test set:","b5a30b0a":"we can see that readmission status is a white line, if we explore this column we discover that it has all zeros, it is not informative and we can drop it.","9da90ba4":"Normalization for the inputs is mainly used in linear models\/knn\/neural networks and it is not required in Logistic Regressions, Random Forest Classifier and XGBoost because they're not affected by absolute values taken by features.","56bd1040":"Creating df table with only numerical columns, we will use this table to apply MICE and input missing values. ","ce5dd474":"# 5) Correlation map to see if there are variables that are highly correlated with the others, to avoid multicollinearity","ed6fc358":"Simple Random Forest without  Rebalanced Dataset and without PCA:","67c8ae05":"Random Forest with  Rebalanced Dataset and with PCA:","3243f5c6":"SingleImputer:","97938cac":"we see a low recall for minority class","b88f150c":"this model is an improvement of the previous xgboost.","8b029622":"# 10) Use one hot encoding to transform categorical columns into numerical ones. ","71b04317":"# 11) Apply all the pre-processing steps (up to point 11)  for the unlabel test data (df_unlabel).","8c8e1988":"Create one hot encoding for the six categorical variables, dropping the first column created with one hot encoding. I just apply the hot encoding variables to the dataset with numerical columns in order to have our new df. ","1da347ef":"there is no need to drop these specific columns 'Unnamed: 0','encounter_id', 'hospital_id' because they are already inside the listcolumn_to_delete.","36c5f318":"MICE:","f7977563":"# 16) Perform Logistic Regressions and creation of classification report to compare different types of logistic regressions ","07ca86aa":"In df_numericalcolumns all the categorical ones have been converted with one hot encoding so all the column are numerical.","1448d639":"Apply the PCA to unlabel standardised dataset:","e688cf7c":"Logistic Regression with  Rebalanced Dataset and PCA: ","19bcbb1a":"Check missing values and columns where there are more than 80% missing values","58ab2e82":"# 1) Reading the csv file training set as df, df_unlabel and show its information","3ff57a64":"# 4) Check the variables that are highly correlated with the target one and the ones that have a low correlation with the target variable, in absolute value.","12e07c3a":"we want to drop columns that are highly correlated among themselves, to avoid multicollinearity:","4313ad09":"we can clearly see that the category Floor is available in the two columns \"icu_admit_source\" and \"hospital_admit_source\" and this can clearly create issues in the one hot encoding process. We are going to rename the category floor in the Icu_admit_source","0d1d719d":"# 8) Use MICE to replace missing values in numerical columns","895435d0":"yellow is where there are missing values, we can see that some columns have a lot of missing values. ","44b58e3f":"We can see that this model is not giving good results in terms of recall for the minority class, the logistic regression is still performing better.","1deea00d":"height can't be derived from weight and bmi because if height is missing the other two variables are missing as well.","d4ead696":"we can see that the table df_numericalcolumns doesnt contain missing values (no yellow lines)","ab01d50b":"# 6) Remove columns where missing values are more than 80% of the entire column and remove columns that have low correlation with target variable. Remove Readmission because it only  has zeros and other column ids that I don't think are relevant to the analysis. Remove columns that are correlated with other ones to avoid multicollinearity","1db32b50":"# 17) Creation of Random Forest Classifier on  Rebalanced Dataset and creation of a classification report","7399d502":"# 2) Check if target (diabetes_mellitus) is unbalanced","edb5fce5":"# 13) Split our f_df dataset into X and y datasets.","796a8586":"Random Forest with  Rebalanced Dataset and without PCA:","37111026":"# 20) Creation of a deep learning model with Tensoflow to try to improve the results","4c331fbf":"Logistic Regression with Rebalanced Dataset and without PCA: ","813475ac":"we can see that the recall of the minority variable improved a lot","d6001dee":"EXPLORATORY DATA ANALYSIS + BUILDING MODELS\n\n* 1) Reading the csv file training set as df\n* 2) Check if target (diabetes_mellitus) is unbalanced\n* 3) Check if there are missing values\n* 4) Check the variables that are highly correlated with the target one\n* 5) Correlation map to see if there are variables that are highly correlated with the others, to avoid multicollinearity. \n* 6) Remove columns where missing values are more than 80% of the entire column, columns that have low correlation with target variable. Remove Readmission because it has only zeros and other column ids that I don't think are relevant to the analysis. Remove columns that are visible correlated with other ones to avoid multicollinearity.\n* 7) Drop BMI column because it can be derived from weight and height, I want to avoid multicollinearity. Before dropping it I assess that BMI can't be used to derive missing height or missing weight values.\n* 8) Use MICE to replace missing values in numerical columns\n* 9) Use SingleImputer to replace missing values in categorical variables, strategy \"most frequent\"\n* 10) Use one hot encoding to transform categorical columns into numerical ones.\n* 11) Apply all the pre-processing steps (up to point 11)  for the unlabel test data (df_unlabel).\n* 12) Align dataset df with unlabel test data to make sure they have the same columns. \n* 13) Split our df dataset into X and y datasets.\n* 14) Performing Random Undersampling and then Oversampling to rebalance the dataset\n* 15) Apply PCA to perform feature selection on standardized X dataset. \n* 16) Creation of Logistic Regression Models and creation of classification reports to compare different types of logistic regressions \n* 17) Creation of Random Forest Classifier on rebalanced dataset and creation of classification report\n* 18) Creation of a xgboost model on rebalanced dataset and creation of classification report\n* 19) Application of GridSearch to improve parameters for xgboost model creation\n* 20) Creation of a deep learning model with Tensoflow to try to improve the results\n","608deec3":"Simple logistic regression without Rebalanced Dataset and without PCA:","62a77768":"XGboost with  Rebalanced Dataset and without PCA:","4c8fab1a":"# 7) Drop BMI column because it can be derived from weight and height, I want to avoid multicollinearity. Before dropping it I assess that BMI can't be used to derive missing height or missing weight values","4b5cd591":"I will keep the first 30 components that are explaining more than 90% of the variance","e31d0254":"creating table with only categorical variables:","889fdaaf":"Rejoining numerical variables with categorical ones to recreate df without missing values:","768817f5":"# 19) Application of GridSearch to improve parameters for xgboost model creation","05840c1d":"We create the neural network wth 47 inputs (number of columns in the datasets), we use drop out layers and early stopping to avoid overfitting. We check the validation loss to see when if the model is overfitting","f523fd09":"we can see that the training set is highly unbalanced, around 22 percentage is diabetes versus 78 non diabetes\n","bee50a10":"we can see that the recall for the minority variable is very low (0.25) "}}