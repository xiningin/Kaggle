{"cell_type":{"3fc692f2":"code","e66f5e08":"code","a2cce02e":"code","802142d0":"code","3591300e":"code","47dfc33e":"code","cb50acf5":"code","aa3a43f9":"code","de954f1f":"code","ecf725cd":"code","87045b60":"code","5764c3e2":"code","28eb051d":"code","16b113cb":"code","3871761f":"code","8548c8c9":"code","ec5b5b1b":"code","3305258d":"code","eced5c7b":"code","1daf613d":"code","75bc00c2":"code","cc53251b":"code","6e3c1513":"code","d5d2eff3":"code","7fadd811":"code","088feb35":"code","801cb0d7":"code","f57e0906":"code","3058641b":"markdown","8c8d9a36":"markdown","9c740ba5":"markdown","dd3441c5":"markdown","53b7abaa":"markdown","d788d4b5":"markdown","214f55bf":"markdown","a9992fe1":"markdown"},"source":{"3fc692f2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv('..\/input\/asteroid-dataset\/dataset.csv', index_col = 'pdes')","e66f5e08":"pd.set_option('display.max_columns', 500)","a2cce02e":"cols_with_missing_data = [col for col in df.columns\n                         if df[col].isnull().any()]\nmissing_data_count = df[cols_with_missing_data].isnull().sum()\nprint(missing_data_count)","802142d0":"df_input = df.dropna(axis=0,how='any',subset=['pha']).copy()","3591300e":"def print_cols_with_missing_data():\n    cols_with_missing_data = [col for col in df_input.columns\n                             if df_input[col].isnull().any()]\n    missing_data_count = df_input[cols_with_missing_data].isnull().sum()\n    print(missing_data_count)","47dfc33e":"df_input[df_input['neo'].isnull()]","cb50acf5":"print_cols_with_missing_data()\n\ndf_input['neo'] = df_input['neo'].fillna('N')\ndf_input = df_input.dropna(axis=0, subset=['sigma_ad'])\ndf_input = df_input.dropna(axis=0, subset=['ma'])","aa3a43f9":"print_cols_with_missing_data()","de954f1f":"df[(df['pha']=='Y') & (df['H'].isnull())].info()","ecf725cd":"df_input = df_input.dropna(axis=0, subset=['H'])","87045b60":"print_cols_with_missing_data()","5764c3e2":"drop_cols = ['id', 'spkid', 'name', 'full_name', 'prefix', 'equinox', \n             'diameter', 'albedo', 'diameter_sigma', 'class', 'orbit_id']\ndf_input = df_input.drop(drop_cols, axis=1)","28eb051d":"df_input['neo'] = df_input['neo'].replace(('Y','N'), (1,0))\ndf_input['pha'] = df_input['pha'].replace(('Y','N'), (1,0))","16b113cb":"df_input.info()","3871761f":"df_output = df_input['pha'].copy()\ndf_input = df_input.drop(['pha'], axis=1)","8548c8c9":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(df_input, df_output, \n                                                      train_size = 0.8, test_size = 0.2) ","ec5b5b1b":"X_train.head()","3305258d":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler\n\n# Scaler\nscaler = MinMaxScaler()\nscaled_X_train = scaler.fit_transform(X_train)\nscaled_X_train = pd.DataFrame(scaled_X_train, columns = X_train.columns, index = X_train.index)\nscaled_X_valid = scaler.transform(X_valid)\nscaled_X_valid = pd.DataFrame(scaled_X_valid, columns = X_valid.columns, index = X_valid.index)","eced5c7b":"scaled_X_train.head()","1daf613d":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE()\nX_train_res, y_train_res = sm.fit_resample(scaled_X_train, y_train.ravel()) ","75bc00c2":"print('Number of PHA = Y after resampling {}'.format(sum(y_train_res == 1)))\nprint('Number of PHA = N after resampling {}'.format(sum(y_train_res == 0)))","cc53251b":"X_train_res.head()","6e3c1513":"from sklearn import metrics\ndef metricCalculation(y_test, pred):\n    \n    precision_metric = metrics.precision_score(y_test, pred, average = \"macro\")\n    recall_metric = metrics.recall_score(y_test, pred, average = \"macro\")\n    accuracy_metric = metrics.accuracy_score(y_test, pred)\n    f1_metric = metrics.f1_score(y_test, pred, average = \"macro\")\n    print('Precision metric:',round(precision_metric, 8))\n    print('Recall Metric:',round(recall_metric, 8))\n    print('Accuracy Metric:',round(accuracy_metric, 8))\n    print('F1 score:',round(f1_metric, 8))","d5d2eff3":"from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train_res, y_train_res)\npred = model.predict(scaled_X_valid)\nmetricCalculation(y_valid, pred)","7fadd811":"metrics.confusion_matrix(y_valid, pred)","088feb35":"feature_imp = pd.DataFrame(model.feature_importances_,\n                           index=X_train_res.columns,\n                           columns = ['Importance']).sort_values(by='Importance', ascending=False)\nprint(feature_imp)","801cb0d7":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\n\nmodels = [LogisticRegression(max_iter = 5000), LinearSVC(), DecisionTreeClassifier(),\n          RandomForestClassifier(), KNeighborsClassifier()]","f57e0906":"for model in models:\n    model.fit(X_train_res, y_train_res)\n    pred = model.predict(scaled_X_valid)\n    metricCalculation(y_valid, pred)","3058641b":"Big shoutout to: https:\/\/www.kaggle.com\/marissafernandes\nI am currently a beginner and his kernel helped me figure out valuable information about this dataset.\n\nHowever, my kernel differ from his kernel by the usage of 'H'.","8c8d9a36":"You can look up these asteroid on: https:\/\/ssd.jpl.nasa.gov\/sbdb.cgi\nBased on their moid and orbital parameters, it clear these are not near-earth objects.","9c740ba5":"- LogisticRegression, Linear SVC, and KNeighborsClassifier achieve terrible F1 score.\n- RandomForest() have a long running time\n- DecisionTree have good result - and minimal running time compared to RandomForest\n\nFeel free to add models as you like.","dd3441c5":"The result indicate that moid_ld and H are the most importance features - this is expected according to multiple online sources.\nI suspect diameter also play a big role (as it is directly correlate to distance and H).\n\nHowever, given that more than 80% of diameter data is missing, it best to delete the columns entirely.\n\nFinetune comments:\n- Deleting columns such as neo, sigma of orbital parameters is possible -> achiving a more accurate model (removing data noise)\n- it is possible to calculate some missing value of H, diameter and albedo (when 1 unknown and 2 known)using: http:\/\/www.physics.sfasu.edu\/astro\/asteroids\/sizemagnitude.html . However, the amound of data retrieved using this method is minimal. Also, the value is also slightly different from the dataset, for example, pdes == 1, Ceres, whose diameter calculated using given online resource is 954, whereas the dataset gave us 939.\n\nI will not finetune these feature, as they serve little purpose for learning, feel free to finetune these features yourself.","53b7abaa":"We arrived at a full numerical dataframe with no missing values","d788d4b5":"The columns with missing values are:\n- name, prefix: there are identification, their values already stored in pdes => can be removed\n- H: absolute magnitude play a role in determining pha \n- diameter, albedo and diameter_sigma: correlated with H, however, as more of 80% of data is missing, it best to remove these columns","214f55bf":"There are 1->4 problematic indexes, as they do not carry much statistic signicficant, its best to remove them than try to imputer their values.\n\nThere statistic significant is determined subjectively as their's moid is all higher than 0.05 au meaning: pha = N","a9992fe1":"This shown that all data with 'pha' = 'Y' have a known 'H'\n-> removing valua without 'H' is possible as this does not affect our interested feature."}}