{"cell_type":{"edc81349":"code","ee381de1":"code","fbec2dad":"code","69f7ce15":"code","9b706044":"code","f61d51b6":"code","c453507e":"code","b95c959c":"code","55915d21":"code","6cf93c78":"code","011a7ee1":"code","a03e04cd":"code","33d8a621":"code","05990a99":"code","ea322562":"code","6e5d50da":"code","f5be1f56":"code","3810eadd":"code","d7f2f2e5":"code","839d3f91":"markdown","f82e66ba":"markdown","b7bbc9cd":"markdown","e246c53f":"markdown","4d6e2b3f":"markdown"},"source":{"edc81349":"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchtext.datasets import Multi30k\nfrom torchtext.data import Field, BucketIterator\nimport numpy as np\nimport spacy\nimport random\nfrom torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n\n","ee381de1":"# \"\"\"\n# To install spacy languages use:\n#  \n!python -m spacy download en\n!python -m spacy download de\n# \"\"\"\n\nspacy_ger = spacy.load(\"de\")  #loads german language dataset\nspacy_eng = spacy.load(\"en\") #loads english language dataset","fbec2dad":"def tokenize_ger(text):               #takes an sentence as input.\n    return [tok.text for tok in spacy_ger.tokenizer(text)]\n\n'''\nExample ,\nSuppose this is a German sentence: \n'Hello, my name is  magician',\nthen tokenizer function will seperate each word from sentence, adn then save it in form of list of strings\n=> ['Hello', 'my', 'name', 'is' , 'magician']\n\n''' \n\n\ndef tokenize_eng(text):               #takes an sentence as input.\n    return [tok.text for tok in spacy_eng.tokenizer(text)]\n# same as tokenize_ger function\n'''\nExample ,\nSuppose this is an English sentence: \n'Hello, my name is  magician',\nthen tokenizer function will seperate each word from sentence, adn then save it in form of list of strings\n=> ['Hello', 'my', 'name', 'is' , 'magician']\n\n''' \n\n\n#Now use Multi30k Data for training data\n## How pre-processing of text\/ data is done:\n\ngerman = Field(tokenize=tokenize_ger, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n\n\n# Field function use list the tokenizer function made, Lower for making every letter lowercase,\n#                    init_token is for specifing start of sentence(<sos>) , eos_token for end of sentence (<eos>)\n\nenglish = Field(\n    tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\"\n)\n# Field function use list the tokenizer function made, Lower for making every letter lowercase,\n#                    init_token is for specifing start of sentence(<sos>) , eos_token for end of sentence (<eos>)\n\n\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(\".de\", \".en\"), fields=(german, english)\n)\n\n## Spliting data with extension .de for german, .en for english, in field\n##               same order as for exts, use for calling Field functions for each language.\n\ngerman.build_vocab(train_data, max_size=10000, min_freq=2)  #word should be repeated atleast 2 times, then only it will be added to vocab.\nenglish.build_vocab(train_data, max_size=10000, min_freq=2)\n","69f7ce15":"# upar se .build.vocab idhar likho","9b706044":"#implementing model\n","f61d51b6":"# first LSTM\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n                       # input_size= size of vocab.(for german lang) ,\n                       #   embedding_size so that each word is mapped to some D-space\n                       #          num_layer= no. of layers in Encoder\n        super(Encoder, self).__init__()\n                  #  The super() function in Python makes class inheritance more manageable and extensible. The function returns a temporary object that allows reference to a parent class by the keyword super.\n                  # The super() function has two major use cases:\n                  #  To avoid the usage of the super (parent) class explicitly.\n                  #  To enable multiple inheritances\u200b.    \n        self.dropout = nn.Dropout(p)    # p= how many nodes to drop.\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)    # Embedding of some input_size mapped to embedding_size\n\n                       # first we run input on embedding, then we run RNN on those embeddings\n\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)  \n                            #  input of LSTM is embedding_size, ouyputting hidden_size, dropout is key argument.\n\n\n    def forward(self, x):\n        # sending an input of a long vector of the indexes of words in vocab through tokenize func()\n        # x shape: (seq_length, N) where N is batch size\n\n        embedding = self.dropout(self.embedding(x))\n        # embedding shape: (seq_length, N, embedding_size) \n        #      N is shape of X, so that each word is mapped to embedding , by adding embedding size, we added another Dimension to our tensor.\n\n        outputs, (hidden, cell) = self.rnn(embedding)\n        # outputs shape: (seq_length, N, hidden_size)\n            # imp. to us are hidden and cell.\n        return hidden, cell\n\n# Second LSDM\nclass Decoder(nn.Module):\n    def __init__(\n        self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n        \n         # input_size= size of vocab.(for English lang) ,\n         #   embedding_size so that each word is mapped to some D-space\n         #   num_layer= no. of layers in Decoder\n         # output_size =  same as input_size, but gives value of vector for each word in vocab.\n        \n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)     # p= how many nodes to drop.\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)    # Embedding of some input_size mapped to embedding_size\n            # first we run input on embedding, then we run RNN on those embeddings\n\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n                 #  input of LSTM is embedding_size, ouyputting hidden_size, dropout is key argument.\n\n        self.fc = nn.Linear(hidden_size, output_size)\n                 #fc= fully connected : linear for hiden_size, output_size\n\n    def forward(self, x, hidden, cell):\n            # sending an input of a long vector of the indexes of words in vocab through tokenize func()\n\n        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n        # is 1 here because we are sending in a single word and not a sentence\n        x = x.unsqueeze(0)  #adding 1 D.\n\n        embedding = self.dropout(self.embedding(x))\n        # embedding shape: (1, N, embedding_size)\n\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        # outputs shape: (1, N, hidden_size)\n\n        predictions = self.fc(outputs)\n\n        # predictions shape: (1, N, length_target_vocabulary) to send it to\n        # loss function we want it to be (N, length_target_vocabulary) so we're\n        # just gonna remove the first dim\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\n# for combinin g encoder and decoder \nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n                            # teacher_force_ratio  is used so that model will be fed with\n                            # 50% correct sentence\/words and 50 incorrect.\n                            # so that it doesn't create problem when we fed test data to machine.\n                            \n        batch_size = source.shape[1]  #second dimension of source shape\n        \n        target_len = target.shape[0]\n        target_vocab_size = len(english.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n                # Predict one word at a time, and for each word we predict\n                # we gonna do it for entire batch , and then every prediction will\n                # be sort of that vector of entire vocab size.\n        hidden, cell = self.encoder(source)\n\n        # Grab the first input to the Decoder which will be <SOS> token\n        x = target[0]\n\n        for t in range(1, target_len):\n            # Use previous hidden, cell as context from encoder at start\n            output, hidden, cell = self.decoder(x, hidden, cell)\n\n            # Store next output prediction\n            outputs[t] = output\n\n            # Get the best word the Decoder predicted (index in the vocabulary)\n            best_guess = output.argmax(1)\n\n            # With probability of teacher_force_ratio we take the actual next word\n            # otherwise we take the word that the Decoder predicted it to be.\n            # Teacher Forcing is used so that the model gets used to seeing\n            # similar inputs at training and testing time, if teacher forcing is 1\n            # then inputs at test time might be completely different than what the\n            # network is used to. This was a long comment.\n            x = target[t] if random.random() < teacher_force_ratio else best_guess\n\n        return outputs\n","c453507e":"# Training hyperparameters\nnum_epochs = 20\nlearning_rate = 0.001\nbatch_size = 64","b95c959c":"# Model hyperparameters\nload_model = False\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninput_size_encoder = len(german.vocab)\ninput_size_decoder = len(english.vocab)\noutput_size = len(english.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5","55915d21":"# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs\/loss_plot\")\nstep = 0","6cf93c78":"train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data), \n    batch_size=batch_size,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    device=device,\n)\n            # lambda func() in sort_key is used to prioritize examples of similar lenght\n            # so that to minimise number of paddings","011a7ee1":"# all variables sent in function are declared in cell above name hyperparameter\nencoder_net = Encoder(\n    input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n).to(device)\n","a03e04cd":"decoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    num_layers,\n    dec_dropout,\n).to(device)\n","33d8a621":"model = Seq2Seq(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","05990a99":"pad_idx = english.vocab.stoi[\"<pad>\"]\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n","ea322562":"if load_model:\n    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n\n\nsentence = \"ein boot mit mehreren m\u00e4nnern darauf wird von einem gro\u00dfen pferdegespann ans ufer gezogen.\"\n","6e5d50da":"# if you got problem importing other ipynb file from same directory, use\n# pip install import_ipynb\n#import import_ipynb\n#from utils2 import translate_sentence, bleu, save_checkpoint, load_checkpoint\n","f5be1f56":"import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\nimport sys\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])","3810eadd":"for epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} \/ {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, german, english, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n    \n    for batch_idx, batch in enumerate(train_iterator):\n            # Get input and targets and get to cuda\n            inp_data = batch.src.to(device)\n            target = batch.trg.to(device)\n\n            # Forward prop\n            output = model(inp_data, target)\n\n            # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n            # doesn't take input in that form. For example if we have MNIST we want to have\n            # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n            # way that we have output_words * batch_size that we want to send in into\n            # our cost function, so we need to do some reshapin. While we're at it\n            # Let's also remove the start token while we're at it\n            output = output[1:].reshape(-1, output.shape[2])\n            target = target[1:].reshape(-1)\n\n            optimizer.zero_grad()\n            loss = criterion(output, target)\n\n            # Back prop\n            loss.backward()\n\n            # Clip to avoid exploding gradient issues, makes sure grads are\n            # within a healthy range\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n            # Gradient descent step\n            optimizer.step()\n\n            # Plot to tensorboard\n            writer.add_scalar(\"Training loss\", loss, global_step=step)\n            step += 100\n\n\nscore = bleu(test_data[1:100], model, german, english, device)\nprint(f\"Bleu score {score*100:.2f}\")\n","d7f2f2e5":"score = bleu(test_data[1:100], model, german, english, device)\nprint(f\"Bleu score {score*100:.2f}\")","839d3f91":"### Importing Libraries","f82e66ba":"### Building Vocabulary\n","b7bbc9cd":"#### Loading dataset using Spacy.","e246c53f":"#### Importing functions from other .ipynb file for operations.","4d6e2b3f":"### We're ready to define everything we need for training our Seq2Seq model ###"}}