{"cell_type":{"ac007fc0":"code","384e5d79":"code","aaadd082":"code","5c8886a5":"code","4c9910c2":"code","7486cd42":"code","b0b567be":"code","35e11772":"code","025c52a4":"code","a4f59daf":"code","9594410f":"code","170a8ffb":"code","3f20c286":"code","87d3ec0f":"code","c6aaec3e":"code","84951b56":"code","75073dff":"code","f2cf4c93":"code","fa3cad8f":"code","c2f6a67a":"code","6d1f740c":"code","82a594e4":"code","e617b9b4":"code","0df1eb50":"code","066daea2":"code","360d5fa2":"code","a6707b21":"code","9caa859f":"code","c6336886":"code","8504cbde":"code","00b74013":"code","3ee06738":"code","14097916":"code","675af2a6":"code","a38a40ad":"code","3617a7ef":"code","c023940a":"code","2c1944b7":"code","90329fae":"code","639741a8":"code","c6b8daff":"code","dddf82ec":"code","de31f951":"code","b6b02417":"code","b88260e7":"code","c0794f8d":"code","b12e6a6c":"code","300cf86f":"code","a46c39ff":"code","07323ae7":"code","604121a9":"code","11bbcc87":"code","002cfb71":"code","aa1869a6":"code","ccc8601b":"code","721e996d":"code","71c72ad9":"code","8f682f37":"code","5cc2e178":"code","dd69cded":"code","581af39c":"code","0bc8e416":"code","92cc24ff":"code","f1441f5c":"code","521f5fa0":"code","c5817076":"markdown","2f6998cf":"markdown","3932c70b":"markdown","eff9dec0":"markdown","3736d805":"markdown"},"source":{"ac007fc0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","384e5d79":"import pandas as pd\nimport numpy as np\nimport warnings \nwarnings.filterwarnings('ignore')","aaadd082":"base_data_train=pd.read_csv(\"..\/input\/train.csv\")","5c8886a5":"base_data_train","4c9910c2":"base_data_test=pd.read_csv(\"..\/input\/test.csv\")","7486cd42":"base_data_test.shape","b0b567be":"cont=base_data_train.describe().columns","35e11772":"cat=base_data_test.describe(include='object').columns","025c52a4":"def nullvalue_function(base_dataset,percentage):\n    \n    # Checking the null value occurance\n    \n    print(base_dataset.isna().sum())\n\n    # Printing the shape of the data \n    \n    print(base_dataset.shape)\n    \n    # Converting  into percentage table\n    \n    null_value_table=pd.DataFrame((base_dataset.isna().sum()\/base_dataset.shape[0])*100).sort_values(0,ascending=False )\n    \n    null_value_table.columns=['null percentage']\n    \n    # Defining the threashold values \n    \n    null_value_table[null_value_table['null percentage']>percentage].index\n    \n    # Drop the columns that has null values more than threashold \n    base_dataset.drop(null_value_table[null_value_table['null percentage']>percentage].index,axis=1,inplace=True)\n    \n    # Replace the null values with median() # continous variables \n    for i in base_dataset.describe().columns:\n        base_dataset[i].fillna(base_dataset[i].median(),inplace=True)\n    # Replace the null values with mode() #categorical variables\n    for i in base_dataset.describe(include='object').columns:\n        base_dataset[i].fillna(base_dataset[i].value_counts().index[0],inplace=True)\n  \n    print(base_dataset.shape)\n    \n    return base_dataset","a4f59daf":"base_data_train_null=nullvalue_function(base_data_train,30)","9594410f":"from sklearn import preprocessing\n\ndef variables_creation(base_dataset,unique):\n    \n    cat=base_dataset.describe(include='object').columns\n    \n    cont=base_dataset.describe().columns\n    \n    x=[]\n    \n    for i in base_dataset[cat].columns:\n        if len(base_dataset[i].value_counts().index)<unique:\n            x.append(i)\n    \n    dummies_table=pd.get_dummies(base_dataset[x])\n    encode_table=base_dataset[x]\n    \n    le = preprocessing.LabelEncoder()\n    lable_encode=[]\n    \n    for i in encode_table.columns:\n        le.fit(encode_table[i])\n        le.classes_\n        lable_encode.append(le.transform(encode_table[i]))\n        \n    lable_encode=np.array(lable_encode)\n    lable=lable_encode.reshape(base_dataset.shape[0],len(x))\n    lable=pd.DataFrame(lable)\n    return (lable,dummies_table,cat,cont)","170a8ffb":"import numpy as np\nlable,dummies_table,cat,cont=variables_creation(base_data_train_null,15)","3f20c286":"lable","87d3ec0f":"lable.shape","c6aaec3e":"base_data_train_null.shape","84951b56":"for i in lable.columns:\n    base_data_train_null[i]=lable[i]","75073dff":"base_data_train_null.drop(cat,axis=1,inplace=True)","f2cf4c93":"base_data_train_null","fa3cad8f":"def outliers(df):\n    import numpy as np\n    import statistics as sts\n\n    for i in df.describe().columns:\n        x=np.array(df[i])\n        p=[]\n        Q1 = df[i].quantile(0.25)\n        Q3 = df[i].quantile(0.75)\n        IQR = Q3 - Q1\n        LTV= Q1 - (1.5 * IQR)\n        UTV= Q3 + (1.5 * IQR)\n        for j in x:\n            if j <= LTV or j>=UTV:\n                p.append(sts.median(x))\n            else:\n                p.append(j)\n        df[i]=p\n    return df","c2f6a67a":"base_data_anly=outliers(base_data_train_null)","6d1f740c":"base_data_anly[\"Id\"]","82a594e4":"y=base_data_anly['SalePrice']\nx=base_data_anly.drop('SalePrice',axis=1)","e617b9b4":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)","0df1eb50":"print(X_train.shape ,X_test.shape, y_train.shape, y_test.shape)","066daea2":"X_train","360d5fa2":"X_test","a6707b21":"y_train","9caa859f":"y_test","c6336886":"# liinear regression \nfrom sklearn.linear_model import LinearRegression\n\nlm=LinearRegression()\n\nlm.fit(X_train,y_train)\n\npredicted=lm.predict(X_test)","8504cbde":"actual=y_test.values","00b74013":"errors=pd.DataFrame(predicted,actual).reset_index()","3ee06738":"errors","14097916":"errors.columns=['acutal','predicted']","675af2a6":"errors.plot()","a38a40ad":"errors[0:100].plot()","3617a7ef":"errors[100:200].plot()","c023940a":"errors[200:300].plot()","2c1944b7":"from sklearn.metrics import r2_score\nr2_score(lm.predict(X_train),y_train)","90329fae":"# now apply variable reduction methods to improve the accuracy i.,e Rsquared  value","639741a8":"import statsmodels.api as sm\nimport statsmodels.formula.api as sfa ","c6b8daff":"variance_table=pd.DataFrame(X_train.var().sort_values(ascending=False))","dddf82ec":"variance_table","de31f951":"# low variance method\nmodel=sm.OLS(y_train,X_train[variance_table.index[0:20]]) \nlm=model.fit()\nlm.summary()\nlm.rsquared","b6b02417":"# forward selection\nfor i in range(1,40):\n    model=sm.OLS(y_train,X_train[variance_table.index[0:i]]) \n    lm=model.fit()\n    lm.summary()\n    print(variance_table.index[0:i],lm.rsquared)","b88260e7":"# backward selection\nfor i in range(40,1,-1):\n    model=sm.OLS(y_train,X_train[variance_table.index[0:i]]) \n    lm=model.fit()\n    lm.summary()\n    print(lm.rsquared)","c0794f8d":"x=list(X_train.columns)","b12e6a6c":"from random import shuffle\nshuffle(x)","300cf86f":"x","a46c39ff":"for i in range(1,40):\n        shuffle(x)\n        model=sm.OLS(y_train,X_train[x[0:i]]) \n        lm=model.fit()\n        lm.summary()\n        print(lm.rsquared)","07323ae7":"X_train.corr()","604121a9":"x=[]\nfor i in X_train.describe().columns:\n    for j in X_train.describe().columns:\n        if i!=j:\n            d=  {\n                'X' :i,\n                'y' :j ,\n                'Corr' : X_train[[i,j]].corr().values[0][1]\n                }\n        \n            x.append(d)","11bbcc87":"pd.DataFrame(x).sort_values('Corr',ascending=False).head()","002cfb71":"import statsmodels.api as sm\nimport statsmodels.formula.api as sfa \n\nmodel=sm.OLS(y_train,X_train) \nlm=model.fit()\nlm.summary()","aa1869a6":"lm.params.sort_values(ascending=False).head()","ccc8601b":"lm.params.sort_values(ascending=True).head()","721e996d":"lm.params","71c72ad9":"import matplotlib.pyplot as plt\nplt.plot(lm.params.values)","8f682f37":"pd.DataFrame(lm.params).reset_index()","5cc2e178":"coeff_tables=pd.DataFrame(lm.params).reset_index()","dd69cded":"coeff_tables.head()","581af39c":"coeff_tables.tail()","0bc8e416":"coeff_tables.values","92cc24ff":"from sklearn.metrics import mean_squared_error\nmean_squared_error(actual, predicted)","f1441f5c":"from sklearn.metrics import mean_squared_error\nnp.sqrt(mean_squared_error(actual, predicted))","521f5fa0":"sum(abs(actual-predicted))\/len(actual)","c5817076":"## to create dummy variables  and label encodong variables ","2f6998cf":"# variable Reduction methods\n# 1.Low variace method\n# 2.correlation method\n# 3.Forward selection method\n# 4.Backward selection method\n# 5.Stepwise Method","3932c70b":"# append these lable data with the base_data_train_null data","eff9dec0":"# now perform ouliers treatment","3736d805":"# Printing MSE RMSE MAE"}}