{"cell_type":{"ef837143":"code","650ccbe3":"code","efab00af":"code","97fd2f66":"code","054c7d4c":"code","df9a589f":"code","9b3cb85b":"code","d3cc3929":"code","1e2ea041":"code","9f19fc35":"code","6f4660ab":"code","906689a8":"code","c775cca4":"code","89bb4228":"code","151e449d":"code","65174092":"code","bca76e83":"code","c773e18d":"code","c9a9b129":"code","e21c5b37":"code","68f34976":"code","55dfe515":"code","fdd5bb1c":"code","ad69bef8":"code","e9cc0fe5":"code","50eb80fc":"code","2cc45967":"code","12e0c33a":"code","21a60a44":"code","408527b9":"code","22f81380":"code","03a9cb78":"code","4b432ee5":"code","af35db6c":"code","2a5b1c5d":"code","feac723d":"code","890c82b6":"code","1556225a":"code","36705ac5":"code","a3c00c25":"code","2838b9fe":"code","fc651e7b":"code","df26090a":"code","e3656b2d":"code","ee9ea745":"code","81a137db":"code","2e72f47b":"code","f4852ac8":"code","c3f52070":"code","cdf33c3e":"code","282d825c":"code","f03079e0":"code","101ce8c7":"code","7e546665":"code","c1f628ac":"code","a10bd7bf":"code","daf23244":"code","d2d8d780":"code","fc97369f":"code","2f3a19d3":"code","903ab913":"code","44232226":"markdown","4a7eb33c":"markdown","0fb253bb":"markdown","8f56205f":"markdown","97b2dbf9":"markdown","78262b1b":"markdown","aa38edb7":"markdown","a6c1a69f":"markdown","49a1c846":"markdown","41e48d12":"markdown","d7a9ca9b":"markdown","822de674":"markdown","f94621d5":"markdown","3a0623d6":"markdown","99ec9072":"markdown","be66a475":"markdown","729c9b65":"markdown","1785de7f":"markdown","52098199":"markdown","36390d54":"markdown","96d44940":"markdown","e297bb65":"markdown","8f0cf061":"markdown","71b40628":"markdown","6c73c9e9":"markdown","320d3076":"markdown","c8399b25":"markdown","16510433":"markdown","b8ae55ec":"markdown","af4dbfea":"markdown","f1b055bb":"markdown","a3bcb1a5":"markdown","9e90e3ee":"markdown","42add912":"markdown","161f4beb":"markdown"},"source":{"ef837143":"import pandas as pd                 # data manipulation\nimport numpy as np                  # linear algebra\nimport matplotlib.pyplot as plt     # basic plotting \nimport seaborn as sns               # plotting libriary\nimport datetime as dt                  # basic date and time types                    \nimport warnings    \nimport math\nimport time\nimport re\nimport os\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\nwarnings.filterwarnings('ignore')\nplt.style.use(style = 'tableau-colorblind10')\nplt.rcParams['figure.figsize'] = (15, 10)\nos.getcwd()\nos.listdir(os.getcwd())","650ccbe3":"file_path = '\/kaggle\/input\/ecommerce-data\/data.csv'\n\ndtypes = {'InvoiceNo' : str, 'StockCode' : str, 'Description' : str, 'Quantity' : int,\n          'UnitPrice' : float, 'CustomerID' : str, 'Country' : str}","efab00af":"online = pd.read_csv(file_path, encoding = 'latin1', dtype = dtypes)\nonline.head()","97fd2f66":"online.info()","054c7d4c":"online[online.Description.isna()]","df9a589f":"online.dropna(inplace = True)\nonline.info()","9b3cb85b":"online['InvoiceDate'] = pd.to_datetime(online['InvoiceDate'])","d3cc3929":"online.dtypes","1e2ea041":"online.head()","9f19fc35":"# Check the date variations in the DataFrame\nprint(\"Min Date: {} \\t Max Date: {}\".format(online['InvoiceDate'].min(), online['InvoiceDate'].max()))","6f4660ab":"# Check how many customers the DataFrame have and how many purchases they made\nlen(online['CustomerID'].value_counts())\n# So there are 4372 unique customers that have made the purchases during the period of 1 year.","906689a8":"# Check quantity of positions \nprint(\"The number of sell positions are {}.\\n\".format(len(online['Description'].value_counts())))\n\n# So cutomers spread over 3896 position. I am interested what product have the most popularity.\nprint(\"The most popular merchandise in the E-commerce shop: \\n\\n{}.\".format(online['Description'].value_counts()[:10]))","c775cca4":"online.describe()","89bb4228":"online = online[online['Quantity'] > 0]\nonline = online[online['UnitPrice'] > 0.05]\nonline.describe()","151e449d":"online['Country'].value_counts()","65174092":"online = online[online['Country'] == 'United Kingdom']","bca76e83":"online.head()","c773e18d":"online.head()","c9a9b129":"# Define a function that will parse the dates from datetime object\ndef extract_days(x):\n    \"\"\"\n    The function extract_dates receives a datetype object from a specified column and return\n    splitted values into year, month and day.\n    \n    Usage:\n    extract_dates(df['datetime object column'])\n    or\n    df['datetime_object_column'].apply(extract_dates)\n    \n    Returns:\n    Series object:\n    2020-02-01\n    2020-02-02\n    ..........\n    2020-02-29\n    \n    Exceptions:\n    This function is applied by the .apply() method if dataframe datetime column is passed.\n    \"\"\"\n    return dt.datetime(x.year, x.month, x.day)","e21c5b37":"# Create InvoiceDay column that contains invoice dates issue by applying extract_dates function\nonline['InvoiceDay'] = online['InvoiceDate'].apply(extract_days)\n\n# Groupby InvoiceDay column\ngrouping = online.groupby('CustomerID')['InvoiceDay'] \n\n# Assign a minimum InvoiceDay value to the dataset\nonline['CohortDay'] = grouping.transform('min')\n\n# View the top 5 rows\nprint(online.head())","68f34976":"# Let do the another column that contains month cohort and invoice month for cohorts\n# I will not describe what the function does as you can read similar to this above\ndef extract_month_int(x):\n    return dt.datetime(x.year, x.month, 1)\n\n# Create a column InvoiceMonth\nonline['InvoiceMonth'] = online['InvoiceDate'].apply(extract_month_int)\n\n# Group\ngrouping = online.groupby('CustomerID')['InvoiceMonth']\n\n# Assign new column\nonline['CohortMonth'] = grouping.transform('min')\n\n# Show several rows of transformed data\nonline.head()","55dfe515":"def extract_dates_int(df, column):\n    \"\"\"\n    The function extract_dates_int help to divide datetime column into several columns based\n    on splitting date into year, month and day.\n    \n    Usage:\n    extract_dates_int(dataFrame, dataFrame['DateTime_Column'])\n    \n    Returns:\n    tuple object that contains unique years, months and days.\n    \n    ((416792    2011\n      482904    2011)\n      ..............\n      482904    11\n      263743     7\n      ..............\n      482904    11\n      263743     7\n    \n    Type:\n    function\n    \n    \"\"\"\n    # extract years from datetime column\n    year = df[column].dt.year\n    \n    # extract months from datetime column\n    month = df[column].dt.month\n    \n    # extract days from datetime column\n    day = df[column].dt.day\n    \n    return year, month, day","fdd5bb1c":"# Get the integers for date parts from 'InvoiceDay' column and 'CohortDay' column\n\n# InvoiceDay column manipulation\ninvoice_year, invoice_month, _ = extract_dates_int(online, 'InvoiceMonth')\n\n# CohortDay column manipulation\ncohort_year, cohort_month, _ = extract_dates_int(online, 'CohortMonth')","ad69bef8":"# calculation of the difference in years\nyears_difference = invoice_year - cohort_year\n\n# calculation of the difference in months\nmonths_difference = invoice_month - cohort_month","e9cc0fe5":"# Extract the difference in days from all the previous extracted values above and create \n# new column called CohortIndex\n\n# ~365 days in one year, ~30 days in one month and plus 1 day to differ from zero value\nonline['CohortIndex'] = years_difference * 12 + months_difference + 1\nonline.head()","50eb80fc":"grouping = online.groupby(['CohortMonth', 'CohortIndex'])\n\n# Count the number of unique values per CustomerID\ncohort_data = grouping['CustomerID'].apply(pd.Series.nunique).reset_index()\n\n# Creating cohort pivot table \ncohort_counts = cohort_data.pivot(index = 'CohortMonth', columns = 'CohortIndex', values = 'CustomerID')\n\n# Review the results\ncohort_counts","2cc45967":"# Select the first column and store value in cohort_sizes\ncohort_sizes = cohort_counts.iloc[:, 0]\n\n# Calculate Retention table by dividing the cohort count by cohort sizes along the rows\nretention = cohort_counts.divide(cohort_sizes, axis = 0)\n\n# Review the retention table\nretention.round(3) * 100","12e0c33a":"grouping_avg_quantity = online.groupby(['CohortMonth', 'CohortIndex'])\n\n# Extract Quantity column from grouping and calculate its mean value\ncohort_data_avg_quantity = grouping_avg_quantity['Quantity'].mean().reset_index()\n\n# average quantity table similar to retention but showing the change in quantity of products purchased\naverage_quantity = cohort_data_avg_quantity.pivot(index = 'CohortMonth', columns = 'CohortIndex', values = 'Quantity')\naverage_quantity.round(1).fillna('')","21a60a44":"# Build a figure\nplt.figure(figsize = (10, 8))\nplt.title('Retentoin rate for customers')\n\n# Initialize a heatmap grapgh \nsns.heatmap(data = retention, annot = True, fmt = '.0%', vmin = 0.01, vmax = 0.5, cmap = 'BuGn')\n\n# show the retention graph\nplt.show()","408527b9":"# Build a figure\nplt.figure(figsize = (10, 8))\nplt.title('average_quantity for customers')\n\n# Initialize a heatmap grapgh \nsns.heatmap(data = average_quantity, annot = True, vmin = 0.01, vmax = 0.5, cmap = 'BuGn')\n\n# show the retention graph\nplt.show()","22f81380":"# Creating TotalSum Column in order to define a total amount spent by customers during the period\nonline['TotalSum'] = online['Quantity'] * online['UnitPrice']\nonline.head()","03a9cb78":"print('Min_date {} \\nMax_date {}'.format(min(online.InvoiceDate), max(online.InvoiceDate)))","4b432ee5":"snapshot_date = max(online.InvoiceDate) + dt.timedelta(days = 1)","af35db6c":"# Calculate Recency, Frequency and Monetary Values for each customer in the dataFrame\nrfm_data = online.groupby(['CustomerID']).agg({\n                                                'InvoiceDate' : lambda x: (snapshot_date - x.max()).days,\n                                                'InvoiceNo' : 'count',\n                                                'TotalSum' : 'sum'\n                                                })\n\n# Rename the created data columns in order to interpritate the obtained results\nrfm_data.rename(columns = {\n                            'InvoiceDate' : 'Recency',\n                            'InvoiceNo' : 'Frequency',\n                            'TotalSum' : 'MonetaryValue'\n                            }, inplace = True)\n\n# Check the obtained results\nrfm_data.head()","2a5b1c5d":"# Labels for Recency, Frequenct and Monetary values metrics\nr_labels = range(4, 0, -1)\nf_labels = range(1, 5)\nm_labels = range(1, 5)\n\n# Recency metric quartiles\nr_quartiles = pd.qcut(rfm_data['Recency'], 4, labels = r_labels)\nrfm_data = rfm_data.assign(R = r_quartiles.values)\n\n# Frequency metric quartiles\nf_quartiles = pd.qcut(rfm_data['Frequency'], 4, labels = r_labels)\nrfm_data = rfm_data.assign(F = f_quartiles.values)\n\n# Monetary Value metric quartiles\nm_quartiles = pd.qcut(rfm_data['MonetaryValue'], 4, labels = m_labels)\nrfm_data = rfm_data.assign(M = m_quartiles.values)","feac723d":"rfm_data.head()","890c82b6":"# Define function concat_rfm that will concatenate integer to string value\ndef concat_rfm(x):\n    \"\"\"\n    Function which return a concatenated string from integer values.\n    \"\"\"\n    return str(x['R']) + str(x['F']) + str(x['M'])\n\n# Calculate the RFM segment \nrfm_data['RFM_Segment'] = rfm_data.apply(concat_rfm, axis = 1)\n\n# Calculate the RFM score which is the sum of RFM values\nrfm_data['RFM_Score'] = rfm_data[['R', 'F', 'M']].sum(axis = 1)","1556225a":"rfm_data.head()","36705ac5":"# Explore the RFM score in the rfm_data\nrfm_data.RFM_Score.value_counts().sort_index()","a3c00c25":"# Explore the RFM segment in the rfm_data\nrfm_data.RFM_Segment.value_counts().sort_values(ascending = False)[:10]","2838b9fe":"# Function that assigns a humanlike label to each of the RFM segment based on the RFM scores\ndef auto_rfm_level(df):\n    \"\"\"\n    Function that auto assigns humanlike segment to each RFM Segments.\n    \"\"\"\n    if df['RFM_Score'] >= 8:\n        return 'Top'\n    elif ((df['RFM_Score'] >= 6) and (df['RFM_Score'] < 8)):\n        return 'Middle'\n    else:\n        return 'Low'\n\n\n\n# In order to save the RFM labels create a new column where to store it\nrfm_data['RFM_Level'] = rfm_data.apply(auto_rfm_level, axis = 1)\n\n# Explore the obtained results\nrfm_data.head()","fc651e7b":"# Explore the obtained RFM levels\nprint(\"Absolute values \\n{} \\n\\nRelative values \\n{}\".format(rfm_data.RFM_Level.value_counts(),rfm_data.RFM_Level.value_counts(normalize = True)))","df26090a":"# Let's dig deeper into the RFM Score\nrfm_data.groupby('RFM_Score').agg({\n                                    'Recency' : 'mean',\n                                    'Frequency' : 'mean',\n                                    'MonetaryValue' : ['mean', 'count']\n}).round(1)","e3656b2d":"# Let's dig deeper into the RFM Score\nrfm_data.groupby('RFM_Level').agg({\n                                    'Recency' : 'mean',\n                                    'Frequency' : 'mean',\n                                    'MonetaryValue' : ['mean', 'count']\n}).round(1)","ee9ea745":"rfm_data.head()","81a137db":"# So to identify what type of data we have let's call describe method to show basic stats info\nrfm_data.describe()","2e72f47b":"# The average values of the variables in the rfm_data dataset\nprint(np.mean(rfm_data))","f4852ac8":"# The standard deviation of the variables in the dataset\nprint(np.std(rfm_data))","c3f52070":"# Visual Exploration of skeweness in the data\nplt.subplot(3, 1, 1);\nsns.distplot(rfm_data['Recency']);\n\nplt.subplot(3, 1, 2);\nsns.distplot(rfm_data['Frequency']);\n\nplt.subplot(3, 1, 3);\nsns.distplot(rfm_data['MonetaryValue'])\n\nplt.show();","cdf33c3e":"# Log Transform the Recency metric\nrfm_data['Recency_log'] = np.log(rfm_data['Recency'])\n\n# Log Transform the Frequency metric\nrfm_data['Frequency_log'] = np.log(rfm_data['Frequency'])\n\n# Log Transform the MonetaryValue metric\nrfm_data['MonetaryValue_log'] = np.log(rfm_data['MonetaryValue'])","282d825c":"# Visual Exploration of log transformed data\nplt.subplot(3, 1, 1);\nsns.distplot(rfm_data['Recency_log']);\n\nplt.subplot(3, 1, 2);\nsns.distplot(rfm_data['Frequency_log']);\n\nplt.subplot(3, 1, 3);\nsns.distplot(rfm_data['MonetaryValue_log'])\n\nplt.show();","f03079e0":"rfm_data.head()","101ce8c7":"# Choose clean values to perform again step by step transoframtion using StandardScaler\nraw_rfm = rfm_data.iloc[:,:3]\nraw_rfm.head()","7e546665":"# Log Transformation\nlog_transformed_rfm = np.log(raw_rfm)\n\n# Initializing a standard scaler and fitting it\nscaler = StandardScaler()\nscaler.fit(log_transformed_rfm)\n\n# Scale and center data\nrfm_normalized = scaler.transform(log_transformed_rfm)\n\n# Create the final dataframe to work with a Clustering problem\nrfm_normalized = pd.DataFrame(data = rfm_normalized, index = raw_rfm.index, columns = raw_rfm.columns)","c1f628ac":"# Visualise the obtained results\nplt.subplot(3, 1, 1)\nsns.distplot(rfm_normalized['Recency'])\n\nplt.subplot(3, 1, 2)\nsns.distplot(rfm_normalized['Frequency'])\n\nplt.subplot(3, 1, 3)\nsns.distplot(rfm_normalized['MonetaryValue'])\n\nplt.show()","a10bd7bf":"# Mean\nprint('Mean value of the data: \\n\\n{}'.format(rfm_normalized.mean(axis = 0).round(2)))\n\n# Standard Deviation\nprint('\\nStandard Deviation value of the data: \\n\\n{}'.format(rfm_normalized.std(axis = 0).round(2)))","daf23244":"# Initialisation of KMeans algorithm with number of cluster 4 (why I choose this I explain further)\nkmeans = KMeans(n_clusters = 4, random_state = 42)\n\n# Fit k-means cluster algorithm on the normalized data (rfm_normalized)\nkmeans.fit(rfm_normalized)\n\n# Extract the obtained cluster labels\ncluster_labels = kmeans.labels_","d2d8d780":"# Create a DataFrame by adding a new cluster label column\nrfm_cluster_k4 = rfm_data.assign(Cluster = cluster_labels)\n\n# Group by cluster label\ngrouped_clusters_rfm = rfm_cluster_k4.groupby(['Cluster'])\n\ngrouped_clusters_rfm.agg({\n    'Recency': 'mean',\n    'Frequency': 'mean',\n    'MonetaryValue': ['mean', 'count']\n  }).round(1)\n","fc97369f":"# Fit KMeans and calculate SSE for each k\nsse = {}\n\nfor k in range(1, 25):\n  \n    # Initialize KMeans with k clusters\n    kmeans = KMeans(n_clusters = k, random_state = 1)\n    \n    # Fit KMeans on the normalized dataset\n    kmeans.fit(rfm_normalized)\n    \n    # Assign sum of squared distances to k element of dictionary\n    sse[k] = kmeans.inertia_\n\n# Add the plot title \"The Elbow Method\"\nplt.title('The Elbow Method')\n\n# Add X-axis label \"k\"\nplt.xlabel('k')\n\n# Add Y-axis label \"SSE\"\nplt.ylabel('SSE')\n\n# Plot SSE values for each key in the dictionary\nsns.pointplot(x = list(sse.keys()), y = list(sse.values()))\nplt.show();","2f3a19d3":"# Calculate average RFM values for each cluster\ncluster_avg = rfm_cluster_k4.groupby(['Cluster']).mean() \n\n# Calculate average RFM values for the total customer population\npopulation_avg = raw_rfm.mean()\n\n# Calculate relative importance of cluster's attribute value compared to population\nrelative_imp = cluster_avg \/ population_avg - 1\n\n# Print relative importance scores rounded to 2 decimals\nprint(relative_imp.iloc[:, [0, 2, 5]].round(2))","903ab913":"# Initialize a plot with a figure size of 8 by 2 inches \nplt.figure(figsize = (8, 4))\n\n# Add the plot title\nplt.title('Relative importance of attributes')\n\n# Plot the heatmap\nsns.heatmap(data = relative_imp.iloc[:, [0, 2, 5]], annot = True, fmt='.2f', cmap='RdYlGn')\nplt.show()","44232226":"<h3> **Part 2. Cohort Analysis of customers based on Time Cohorts. ** <\/h3>\n","4a7eb33c":"So that's it to Part 2. Now you can create a retention cohorts. ","0fb253bb":"![image.png](attachment:image.png)","8f56205f":"The scope of the Elbow Method is to choose the number of cluster where the graph breaks. Ususally it is number 4 so 4 clusters must be in the algorithm. Further I want to show you how to calculate base metrics to identify the 'fit' of the Kmeans algorithm. ","97b2dbf9":"Defining a cohort is the first step to cohort analysis. Let's try to create daily (or maybe other time types cohorts) cohorts based on the day each customer has made their first transaction (received an InvoiceNo due to the InvoiceDate).\n\nTo do this I will create a function that accepts a datetime datatype (object) and returns such values as year, month and day from specified column.","78262b1b":"** <h2> E-commerce customer's data exploration and segmentation <\/h2> **\n\nThe DataFrame contains the information about customer's purchases across United Kingdom.\nThere are nearly about 4000 customers and their purchases during the period of one year (**from 2010-12-01 to 2011-12-09**). \n\nThe analysis will be divided into several parts:\n1. Data Importing and Data Cleaning.\n2. Cohort Analysis of customers based on Time Cohorts.\n3. Recency, Frequency and Monetary Value Analysis (RFM).\n4. Customer Segmentation using K-Means Algorithm. \n5. Conclusion.\n\n\nToday Customer and Cohort Analysis is the must have knowledge for any data-scientist or data-analyst. Cohort analysis is one of the most effective way to gather information about customer's behaviour. Before dive in I want to clarify some points of the analysis. Further you will not find any super complex concepts of machine learnign models or neural networks. The main goal of this notebook is simply try to show you how to build a full pipeline to analyze customer's behaviour on the basis of E-commerce data. And maybe some part of this notebook will clarify your mind on some simple points of the customer analysis.\n\nSo, terminology:\n* **Cohort** - group of people (users, individuals and etc.) who have a common characteristic(-s) during a period of time.\n* **Cohort Analysis** - a subset of behavioral analytics that researches groups of people who have taken a common action during a selected period of time\n* **Retention** - the continued possesion, use, or control something\n* **Churn** - the user (customer) who has stopped using the service\n\nFurthermore, outside from this notebook you can face with such types of cohorts as:\n* Time Cohorts (So called Retention)\n* Behavior Cohorts (RFM, LFL and other variations of analysis)\n* Size Cohorts (Clustering, Behavior Patterns Segmentation and etc.)\n\n\nSo from this point I will stop. Let's go dive dipper into analytics.\n","aa38edb7":"<h3> ** Part 5. Conclusion. ** <\/h3>\n\nSo we have finished. It is was very interesting work. I showed to you one of the main work of marketing analytics and data science in business - identify behavioural and time cohorts of users and customers. It is a very useful technique in identifying business and marketing strategy. So if you have any questions or suggestion, so you are welcome comment. \n\nIf you want to know more about users identification you can read more information here:\n1. [6 Strategies to Increase Customer Loyalty and Retention](https:\/\/medium.com\/gobeyond-ai\/6-strategies-to-increase-customer-loyalty-and-retention-136544b9ad87)\n2. [Marketing Analytics in SQL and Python](https:\/\/towardsdatascience.com\/where-are-the-best-customers-marketing-analytics-in-sql-and-python-a20ca16968ea)\n3. [Analyzing Marketing Data in Python ](https:\/\/medium.com\/@tl_dr\/analyzing-marketing-data-fast-in-python-d24b4387af45)\n\nThank You! See you next time. Good Luck!","a6c1a69f":"Let's check if we have a missing values in the DataFrame. \nIf so I think I will delete it because I will not build a complex machine learning model and the \ndata quantity in such case does not matter.","49a1c846":"Another useful metric for behavioural analytics and customer analytics is to calculate tha average quantity of products that customers buy in the E-Commerce store and visualise it in similar cohort table like above for the retention analytics.","41e48d12":"The next step is optional. If you will present the obtained result to your boss you will need to interpret the results into humanlike language. Because not all people understand the language of math on which speaks a lot of poeple in our industry (Data Science).\n\nSo the better way to interpret the obtained results is to assign a humanlike value to each of the RFM segment based on the RFM score. What I mean here? For example the RFM Score which is greater than 10 will be called Top,\nthe RFM Score between 6 and 10 will be called Middle and the final RFM Score lower than 6 will be called Low. So let's do it.","d7a9ca9b":"So the next step in order to build an appropriate cohorts is to calculate time offsets for each customer transaction. It will allow us to report the metrics for each cohort in an appropriate way. To create time offsets I will create the function that splits the dates into year, month, day columns. It will help to easily do calculations based on time dates.","822de674":"As can be seen from the visual represenation of the RFM data. All metrics are right skewed and we need to get rid from skewness by applying either Log Transformation or Z-Transformation. I prefer to do the log transformation in such case because the data has no negative values and ideally prepared for this kind of method.","f94621d5":"So it seems that the dataFrame is cleaned. Data Types are correct and logically spreaded. So let's dive in into the second part of the Analysis - Cohort Analysis of customers based on the Time Cohorts.\n\nBefore it lets select only United Kingdom customers in order to avoid Time Zone Collapsing and other inappropriate data problems. The list of countries can be found below.","3a0623d6":"That's it we have created useful RFM (behavioural segmentation) based on user's data (customer's data). The interpritation is simple the lower Recency the better and the higher Frequency and Monetary Value the better. Finally we divided our RFM into 3 customer cohorts - Top, Middle and Low segments. Based on this data we can make different business decisions on these groups of customers. Next Step is a part of machine learning. Yeap the interesting is last. I will show you how to preprocess data to implement a simple segmentation ML algorithm called K-Means Clustering.","99ec9072":"Hmmmmmmm....Seems normally distributed from statistics point of view. Ok.","be66a475":"So from this point it is available to calculate an appropriate time offsets in days (or other time periods). We have six different data sets with year, month and day values for such columns as InvoiceDay and CohortDay:\n* invoice_year\n* invoice_month\n* invoice_day\n* cohort_year\n* cohort_month\n* cohort_day\n\nSo lets try to calculate the difference between the Invoice and Cohort dates in years, month and days separately and then calculate the total days difference between this two. And this will be days offsets which will be used in the customer cohort building.","729c9b65":"The dataFrame starting with only latest 12 month of data. Create a hypothetical snapshot_day data as we are doing analysis recently.","1785de7f":"So we are in the final of Part 2. The last action to build a complex cohort and retention analysis is to visualise results that we have calculated above. The best choice to show the obtained results is to draw a heatmap that will show us a retention graph of customer data (cohorts and retention). So lets do it!","52098199":"A further data exploration task is to identify skewness. Skewness is a measure of symmetry (or lack of symmetry). A distirbution of a dataset is symmetric if it looks the same to the left and right of the center point. The histogram is an effective graph technique for showing skewness of data. \n\nMath formula for skewness detection is: \n![image.png](attachment:image.png)\n\nUsually there are 3 types of skewness:\n1. Left Skewed data\n2. Normal Distributed data\n3. Right Skewed data\n\nHere we will explore skewness graphically.","36390d54":"So the final and most important part of our big work. Now we will implement a segmentation algorithm with k-means clustering using data we have prepared. Key step for the K-Means clustering problem:\n* Data Preprocessing (done)\n* Identify the number of clusters\n* Run Kmeans on the preprocessed data\n* Analyze average RFM values for each obtained cluster\n\nProblem: Identifying the number of clusters. In KMeans algorithm there are several ways to identify the quantity of clusters:\n* Visual method - so called elbow method (or elbow criteria and etc.)\n* Quantitative method called [silhouette coefficient](https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html) \n* Experimentation and imagination\n\n","96d44940":"So far so good. Here I will show you a couple of concepts that is used in data-preprocessing step and other considerations. After building and preprocessing pipeline I will show you how to build the popular Machine Learning Algorithm called K-Means Clustering which will be based on our calculated RFM Scores. This will help us to clarify and indentify users based on their customer behaviour metrics. \n\nWhat is K-Means Clustering and why to use this algorithm?\n* K-Means is one of the most popular unsupervised learning method to identify different patterns\n* K-Means is simple and fast\n* Works well on big datasets \n\nThere are some critical assumptions with K-Means algorithm before building it:\n1. All variable must have symmetrical distribution and should not be skewed.\n2. All variables should have the same or almost the same average values.\n3. All variables should have the same level of variance.\n\nTips and tricks:\n* We have several ways to get rid skewness from data. The first one is logarithmic transformation. But log transformation works only with positive values (I am not going to show the math formulas to explain why, just admit it as the fact).\n* Other method to get rid of skewness is widely used Z-Transformation which works well on the mixed (negative and positive) values in the data.\n\nFurther I will explain different methods during the work. Here is enough information to dive in to the work. So let's do some work!","e297bb65":"<h3> **Part 3. Recency, Frequency and Monetary Value Analysis (RFM).** <\/h3>","8f0cf061":"<h3> **Part 1. Data Importing And Data Cleaning** <\/h3>","71b40628":"So the next step is creation of the retention table and research the retention table from customers data. Customer retention is very powerful metric to understand how many of all the customers are still 'alive' (or active). Before that Retention shows us a percentage of active customers out of total customers.","6c73c9e9":"Hmmm. Okay, I still do not like Recency visual representation of data. Let discover statistic values of the distribution here:","320d3076":"Now it is high time to build so called RFM Segment and RFM Score. RFM score is a simply sum of the Recency, Frequency and Monetary Values and this sum is a result of integer values like 10, 9, 8 and etc. This score will indicate the value of RFM score that will allow us to make decisions on a business product or on our customers. This is very important metric due to future decision-making process concerning the users or customers. ","c8399b25":"Now let's create three separate groups based on Recency, Frequency and Monetary Value of customers.","16510433":" Not Ideal especially with Recency value. We can see that recency_log transformed metric is still do not have a good normal shape of distribution. There are a lot some other specific methods to use such as manually perform Z-Transformation or use sklearn method such as StandardScaler.\n \n Let's try StandardScaler on log transformed data as it much more easier than to do it manually:","b8ae55ec":"So the columns Description and CustomerID have missing values. \nDelete them.","af4dbfea":"So now we are able to analyze the obtained results with RFM values. Let's assign labes to our data.","f1b055bb":"**RFM Segmentation** is another type of customer segmentation based on three factors:\n\n* Recency (R) - measures how recent was each customer's last purchase\n* Frequency (F) - measures how many purchases customer has done in the last N periods \n* Monetary Value (M) - measures how much has customer spent in the last N periods\n\nAs soon as values are calculated the next step is grouping them into categories (or some sort of it) such as high, medium and low. The RFM values can be grouped in such ways as:\n\n* Percentiles (I will implement this method)\n* Pareto law (split metrics on high and low values)\n* Custom (based on specific domain knowledge which you can use further)","a3bcb1a5":"<h3> ** Part 4. Customer Segmentation using K-Means Algorithm. ** <\/h3>","9e90e3ee":"The E-Commerce DataFrame contains the following columns:\n* **InvoiceNo** - datatype string\n* **Stock Code** - datatype string\n* **Description** - datatype string\n* **Quantity** - datatype integer\n* **InvoiceDate** - datatype datetime\n* **UnitPrice** - datatype float\n* **CustomerID** - datatype string\n* **Country** - datatype string","42add912":"![image.png](attachment:image.png)","161f4beb":"I found one interesting thing in the dataFrame. The **Quantity** column has a negative value in it. But logically it should not, because the customer can not buy a negative amount of merchant. It is impossible. But maybe it is a way that store register such sales as installment plan. But maybe I am wrong.\n\nAlso the **UnitPrice** column have zero price rows. It means that we sell for free. But it is also impossible. Seems like there are a mistake in data or something is wrong.\n\nSo I will find such rows and just delete them in order to fully clean the dataFrame."}}