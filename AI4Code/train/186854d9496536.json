{"cell_type":{"e0ab15b8":"code","1ab252df":"code","d37cbd68":"code","a874cf09":"code","0939b24f":"code","d511977a":"code","288f63ea":"code","3ff98739":"code","e89a420f":"code","fd2d595a":"code","cc899bb9":"code","831f3617":"code","c9314499":"code","543f7629":"code","07ba6250":"code","ebebb56c":"code","50e6aa25":"code","150850f1":"code","7252f285":"code","f95f0a10":"code","0b113403":"code","d05a407b":"code","23a46d1c":"code","b27a882f":"code","a1e92585":"code","fcb6a95f":"code","cffc6585":"code","7a786cc1":"code","c7ddf23a":"code","35017e45":"code","19d814b3":"code","07be51c0":"code","83de39ff":"code","187c3721":"code","c4210294":"code","dd0a851b":"code","585b79dc":"code","b27a3c4e":"code","d1acbbba":"code","0bcbe150":"markdown","7e77b3e6":"markdown","6b74571d":"markdown","c6988e1e":"markdown","9d573b2b":"markdown","4a5f409e":"markdown","2ccc9cb7":"markdown","86ea23f3":"markdown","cc5eaf79":"markdown","10683586":"markdown","e005f522":"markdown","d1ea277f":"markdown","4d576821":"markdown","ca351aa4":"markdown"},"source":{"e0ab15b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1ab252df":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns","d37cbd68":"df = pd.read_csv('\/kaggle\/input\/faulty-steel-plates\/faults.csv')","a874cf09":"df.head()","0939b24f":"df.info()","d511977a":"df.isnull().sum()","288f63ea":"#check missing values\nplt.figure(figsize=(12,4))\nsns.heatmap(df.isnull(),cbar=False,cmap='Wistia',yticklabels=False)\nplt.title('Missing value in the dataset');","3ff98739":"df.iloc[:,27:34]","e89a420f":"y=df.iloc[:,27:34];\nX=df.iloc[:,0:27];\nfrom sklearn.preprocessing import MinMaxScaler\n\nsc = MinMaxScaler(feature_range =(0,1))\n\nX_std = pd.DataFrame(sc.fit_transform(X), columns = X.columns)\nprint(X_std.shape)\nX_std.boxplot(figsize = (12.8,8), rot = 90)\nplt.boxplot\nplt.show()","fd2d595a":"targets = df.iloc[:, 27:]\ndf.drop(targets.columns, axis=1, inplace=True) # this will drop the all the defects column\ndf['Target'] = targets.idxmax(1) #this will add a 'Target' column to our dataset describing which record has a particular defect.\ndf.head()","cc899bb9":"target_counts= df['Target'].value_counts()\n\nfig, ax = plt.subplots(1, 2, figsize=(15,7))\ntarget_counts_barplot = sns.barplot(x = target_counts.index,y = target_counts.values, ax = ax[0])\ntarget_counts_barplot.set_ylabel('Number of classes in the dataset')\n\ncolors = ['#8d99ae','#ffe066', '#f77f00','#348aa7','#bce784','#ffcc99',  '#f25f5c']\ntarget_counts.plot.pie(autopct=\"%1.1f%%\", ax=ax[1])","831f3617":"plt.figure(figsize=(20,15))\nsns.heatmap(df.corr(), cmap='hsv')","c9314499":"df['Target'] = pd.Categorical(df['Target'])\ndf['Target_Code'] = df.Target.cat.codes","543f7629":"df.head()","07ba6250":"y=df.iloc[:,27];\ny.head()","ebebb56c":"plt.figure(figsize=(8,5))\nsns.boxplot(x='Target', y='X_Maximum', data=df)","50e6aa25":"df.corr()","150850f1":"df.describe()","7252f285":"from sklearn.cluster import DBSCAN\nfrom collections import Counter\neps_r = 0.1\nwhile eps_r < 10:\n    dbscan_model = DBSCAN(eps=eps_r, min_samples=7).fit(X_std) \n    \n    if Counter(dbscan_model.labels_)[-1] < 0.1*len(X_std):\n        print('The neighbourhood distance considered for removal of less than 10% outliers is: {}'.format(eps_r))\n        break\n        \n    eps_r = eps_r + 0.1","f95f0a10":"dbscan_model = DBSCAN(eps=eps_r, min_samples=7).fit(X_std)","0b113403":"outliers = X_std[dbscan_model.labels_ == -1]\noutliers.shape","d05a407b":"X_std.drop(outliers.index, axis=0, inplace=True)\ny.drop(outliers.index, axis=0, inplace=True)","23a46d1c":"X_std.head()","b27a882f":"X_std.boxplot(figsize = (12.8,8), rot = 90)\nplt.boxplot\nplt.show()","a1e92585":"X_std.info()","fcb6a95f":"y","cffc6585":"# Splitting the dataset into the Training set and Test set\nX_train, X_test, y_train, y_test = train_test_split(X_std, y, train_size = 0.90, random_state = 15)","7a786cc1":"y","c7ddf23a":"# LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\nLogReg = LogisticRegression(max_iter=1000)\nLogReg.fit(X_train, y_train)\n\ny_pred_LogReg = LogReg.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred_LogReg))\nprint(confusion_matrix(y_test, y_pred_LogReg))\n# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred_LogReg,y_test))","35017e45":"#Plotting the confusion matrix.\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred_LogReg)\n\nprint(cm)\nplt.figure(figsize=(15,10))\ncategories = np.unique(y)\ndf_cm = pd.DataFrame(cm, index = [i for i in categories], columns = [i for i in categories])\nsns.heatmap(df_cm,annot=True,cmap='Reds')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","19d814b3":"from sklearn.tree import DecisionTreeClassifier\n\nDTC = DecisionTreeClassifier()\n\nDTC.fit(X_train, y_train)\n\ny_pred_DTC = DTC.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred_DTC))\nprint(confusion_matrix(y_test, y_pred_DTC))\n# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred_DTC,y_test))","07be51c0":"#Plotting the confusion matrix.\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred_DTC)\n\nprint(cm)\nplt.figure(figsize=(15,10))\ncategories = np.unique(y)\ndf_cm = pd.DataFrame(cm, index = [i for i in categories], columns = [i for i in categories])\nsns.heatmap(df_cm,annot=True,cmap='Reds')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","83de39ff":"# Support Vector Machine's \nfrom sklearn.svm import SVC\n\nSVM_C = SVC(kernel='linear')\nSVM_C.fit(X_train, y_train)\n\ny_pred_svm = SVM_C.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred_svm))\nprint(confusion_matrix(y_test, y_pred_svm))\n# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred_svm,y_test))","187c3721":"#Plotting the confusion matrix.\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred_svm)\n\nprint(cm)\nplt.figure(figsize=(15,10))\ncategories = np.unique(y)\ndf_cm = pd.DataFrame(cm, index = [i for i in categories], columns = [i for i in categories])\nsns.heatmap(df_cm,annot=True,cmap='Reds')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","c4210294":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators=30, random_state=68, max_depth = 10)\nrfc.fit(X_train, y_train)\ny_pred_rfc = rfc.predict(X_test)\ny_pred_train = rfc.predict(X_train)\nprint('The training accuracy of the model is {}'.format(accuracy_score(y_train,y_pred_train)))\nprint('The testing accuracy of the model is {}'.format(accuracy_score(y_test,y_pred_rfc)))","dd0a851b":"#Plotting the confusion matrix.\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred_rfc)\n\nprint(cm)\nplt.figure(figsize=(15,10))\ncategories = np.unique(y)\ndf_cm = pd.DataFrame(cm, index = [i for i in categories], columns = [i for i in categories])\nsns.heatmap(df_cm,annot=True,cmap='Reds')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","585b79dc":"acc=[accuracy_score(y_pred_DTC,y_test),accuracy_score(y_pred_svm,y_test),accuracy_score(y_pred_LogReg,y_test),accuracy_score(y_pred_rfc,y_test)]\nmodels = ['Decision Tree','Support Vector Machine','Logistic Regression','Random Forest']\nacc_ = np.array(acc)*100\nsns.barplot(acc_, models, color=\"g\")\nplt.xlim([70,100])\nplt.xlabel('Accuracy %')\nplt.title('Classifier Accuracy')\nplt.show()","b27a3c4e":"y_rfc = rfc.predict(X_std)\n\nprint('accuracy is {:.2f}%'.format(accuracy_score(y_rfc,y)*100))\n\n","d1acbbba":"submission = pd.DataFrame({\n        \n        \"Result\": y_rfc\n    })\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","0bcbe150":"Let us look at the classification of each of these defects.","7e77b3e6":"We will use the DBSCAN method. Before we dig in, we will try to remove the 10% outliers and we need to identify the distance to remove those outleirs. A small code will work for us to identify the distance and we fix the number of samples to be 7.","6b74571d":"Check the shape of outliers. It has to be less than 10% of 1941 (i.e., less than 194)","c6988e1e":"The last seven columns are the defects.","9d573b2b":"We will be developing the prediction model with various algorithms viz., Logistic Regression, Decision Tree Classifier and Support Vector machine and pick the one with better accuracy.","4a5f409e":"Now lets remove the outliers. There are many ways to remove the outliers. Few of them are as follows:\n1. Using the Z-scores, removing the ones which has z-score of less than -3 and greater than 3, if the  data distribution follows the normal distribution.\n1. Using the quartile ranges, if the value falls below the 1.5 times of IQR (inter quartile range) from Q1 (25 percentile) or above the 1.5 times of IQR from Q3 (75 percentile) it can be done if the data is skewed. \n1. The other method is by using the clusters and providing the minimum distance and minimum samples to be in the cluster.\n\nBut before we remove any data, we must ensure we dont remove the data which may carry important information. As can be seen if we apply the 1st and 2nd method we will be removing all the values for K_scratch defect, which may undertrain our model. So we will go with 3rd method\n","2ccc9cb7":"lets drop these 144 records.","86ea23f3":"We will use this distance of 0.6 on our data and minimum samples of 7 in each cluster to remove less than 105 outliers.","cc5eaf79":"Before we dig in further. We will try to put all the seven columns of the label in to one 'Target' column.","10683586":"Lets convert the categorical values of Target column in to numerical values.","e005f522":"In the above columns , it can be found that all the Dtypes of the columns are int64 or float64 meaning there are no NULL values in any of the columns. Anyhow lets check it with the heatmap.","d1ea277f":"First 27 columns are the features of the dataset.","4d576821":"Now lets check the distribution of data after removing the outliers. Still the data goes beyond the IQR ranges.If we try to remove those we will be missing the important information needed for the model training.\n\nNB: Remember the figure below is for new data, here the median and mean will be changed and not the same as before with all the entries.","ca351aa4":"Lets check if there is any correlation between the features."}}