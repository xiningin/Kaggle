{"cell_type":{"437abec3":"code","c53a139c":"code","5827fdee":"code","f1235610":"code","efdd4f9f":"code","9d66af9c":"code","c63eb36a":"code","961f9a85":"code","da6c7b34":"code","f6dfb5a3":"code","e9fc8145":"code","caab241e":"code","01e9fddc":"code","3efd93fb":"code","70b343b1":"code","79497bcb":"markdown","2c3b4bba":"markdown","bffb429d":"markdown","9bf35e8a":"markdown","22d59315":"markdown","ee286fad":"markdown","a014c253":"markdown","2f477879":"markdown"},"source":{"437abec3":"import json\nimport os\nimport re\nimport string\nimport numpy as np\n\nfrom gensim.models import Word2Vec","c53a139c":"def extract_text(filename, field):\n    \n    extracted_field=[]\n    \n    with open(os.path.join(filename), 'r') as f:\n        articles=json.load(f)\n    \n    for article in articles['articles']:\n        extracted_field.append(article[field].strip())\n    \n    return extracted_field","5827fdee":"def replace_strings(texts, replace):\n    new_texts=[]\n    \n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)\n    \n    for text in texts:\n        for r in replace:\n            text=text.replace(r[0], r[1])\n        text=emoji_pattern.sub(r'', text)\n        text=english_pattern.sub(r'', text)\n        text=re.sub(r'\\s+', ' ', text).strip()\n        new_texts.append(text)\n\n    return new_texts","f1235610":"def remove_punc(sentences):\n    # import ipdb; ipdb.set_trace()\n    new_sentences=[]\n    exclude = list(set(string.punctuation))\n    exclude.extend([\"\u2019\", \"\u2018\", \"\u2014\"])\n    for sentence in sentences:\n        s = ''.join(ch for ch in sentence if ch not in exclude)\n        new_sentences.append(s)\n    \n    return new_sentences","efdd4f9f":"ebala_body=extract_text('..\/input\/news articles\/News Articles\/ebala_articles.txt', 'body')\n\nprint(\"\\x1b[31mCrawled Unprocessed Text\\x1b[0m\")\nprint(ebala_body[43])\n\nreplace=[('\\u200c', ' '),\n         ('\\u200d', ' '),\n        ('\\xa0', ' '),\n        ('\\n', ' '),\n        ('\\r', ' ')]\n\nebala_body=remove_punc(ebala_body)\n\nprint(\"\\x1b[31mSentences after removing all punctuations\\x1b[0m\")\nprint(ebala_body[43])\n\nebala_body=replace_strings(ebala_body, replace)\n\nprint(\"\\x1b[31mSentences after replacing strings\\x1b[0m\")\nprint(ebala_body[43])","9d66af9c":"abz_body=extract_text('..\/input\/news articles\/News Articles\/anandabazar_articles.txt', 'body')\n\nabz_body=remove_punc(abz_body)\nabz_body=replace_strings(abz_body, replace)","c63eb36a":"zee_body=extract_text('..\/input\/news articles\/News Articles\/zeenews_articles.txt', 'body')\n\nzee_body=remove_punc(zee_body)\nzee_body=replace_strings(zee_body, replace)","961f9a85":"body=[]\nbody.extend(zee_body)\nbody.extend(abz_body)\nbody.extend(ebala_body)\n\nprint(f\"Total Number of training data: {len(body)}\")","da6c7b34":"body=[article.split('\u0964') for article in body]\nbody=[item for sublist in body for item in sublist]\nbody=[item.strip() for item in body if len(item.split())>1]\n\nbody=[item.split() for item in body]\n\nprint(body[:10])","f6dfb5a3":"model = Word2Vec(body, size=200, window=5, min_count=1)","e9fc8145":"print(\"What are the words most similar to chele\")\ngirl=model.wv.most_similar('\u099b\u09c7\u09b2\u09c7', topn=5)[0][0]\nprint(model.wv.most_similar('\u099b\u09c7\u09b2\u09c7', topn=5)[0][0])","caab241e":"print(\"What is Father + Girl - Boy =?\")\nmodel.wv.most_similar(positive=['\u09ac\u09be\u09ac\u09be', girl], negative=['\u099b\u09c7\u09b2\u09c7'], topn=5)","01e9fddc":"print('Find the odd one out')\nmodel.wv.doesnt_match(\"\u0995\u09b2\u0995\u09be\u09a4\u09be \u099a\u09c7\u09a8\u09cd\u09a8\u09be\u0987 \u09a6\u09bf\u09b2\u09cd\u09b2\u09bf \u09b0\u09ac\u09c0\u09a8\u09cd\u09a6\u09cd\u09b0\u09a8\u09be\u09a5\".split())","3efd93fb":"print(\"How similar are bengali and sweet?\")\nmodel.wv.similarity('\u09ac\u09be\u0999\u09be\u09b2\u09bf', '\u09ae\u09bf\u09b7\u09cd\u099f\u09bf')","70b343b1":"print(\"What about Bihari and Sweets?\")\nmodel.wv.similarity('\u09ac\u09bf\u09b9\u09be\u09b0\u09bf', '\u09ae\u09bf\u09b7\u09cd\u099f\u09bf')","79497bcb":"# Training Bengali News Word Vectors\n\nIn this notebook, we will use the data we scraped from news websites to train a Word2Vec model for Bengali.\n\nThen we will test the model to see how well it is performing.\n\nFirst we import the packages we need","2c3b4bba":"We also need to remove all the punctuations in our data. The `remove_pun` function removes all common punctuations found in text.","bffb429d":"Let's define a function that will read the data file and extract the fields we want.\n\nIn our case, we will be using the article body for training","9bf35e8a":"Finally, we need to split the articles into sentences and extract each word from those sentences.\n\nOur final training data looks like this","22d59315":"Let's extract some of the data from Ebala and print them to see how the data changes throughout the process.","ee286fad":"We do the same thing for the other data too","a014c253":"Now we define a function to preprocess our data.\n\nThe function does the following:\n- It replaces common texts found in the data and replaces that with our custom text\n- It removes all emoji's and emoticons from the text\n- It removes all English text","2f477879":"Now that we have our preprocessed training data, we can start training our model.\n\nWe will generate embeddings for each word of size 200 and use 5 words in its vicinity to figure out the meaning of the word"}}