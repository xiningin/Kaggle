{"cell_type":{"9e8b6498":"code","cd19463f":"code","f9a8a98e":"code","92a56089":"code","581b12e7":"code","7212afe0":"code","060304d8":"code","71f5097d":"code","73dfccb9":"code","4575d787":"code","6582f8c8":"code","8a8c9129":"code","73dcc138":"code","22eb8756":"code","49e7e51a":"code","9d83db32":"code","49afb50a":"code","f5950259":"code","faecf6e2":"code","7ddc5edf":"code","69c9b11f":"code","16c729ab":"code","43dff926":"code","855dc4bf":"code","d3fd84a2":"code","3010344d":"code","d336227a":"code","197d9910":"code","ae867355":"code","a5cff20d":"code","268ebf77":"code","2293c74a":"code","443b8256":"code","b5f21c50":"code","4d5524c8":"code","98041be7":"code","c16fdfe7":"code","9d682649":"code","fa34335e":"code","88ea00c3":"code","78bac1cf":"code","b004cb67":"code","31dfb9fb":"code","fbd164c5":"code","ffbc0d48":"code","e2e22303":"code","c712cc91":"code","ef54b87d":"code","eddd73f2":"code","ad8cbada":"code","4a48b6fc":"code","5d504477":"code","a8fc9854":"code","150cd50a":"code","9583c35d":"code","d64be5f2":"code","e4994f46":"code","4a14b9e4":"code","afbb40d5":"code","9fa5625d":"code","81eae4d7":"code","630ac409":"code","210276c4":"code","13d65387":"code","24c2b4cc":"code","0e4e46ab":"code","c9cebfdb":"code","e2b912b6":"code","fec4421d":"code","e59517b2":"code","d1c314d3":"code","381b9b18":"code","e1b85af5":"code","70687be0":"code","8c40d0ed":"code","8eb4ce9e":"code","3808023d":"markdown","5ec1ec07":"markdown","d613ae39":"markdown","651ecb79":"markdown","2bda2a2f":"markdown","e4fb30ec":"markdown","f3fa0178":"markdown","ae847431":"markdown","fa9f90fd":"markdown","8d82eae2":"markdown","608753cd":"markdown","1f549249":"markdown","a17b4b2d":"markdown","1d3c9873":"markdown","197cf46c":"markdown","c03361fc":"markdown","7c5bb4c7":"markdown","34d832b6":"markdown","cfe78037":"markdown","e41b00e5":"markdown","d49b87d5":"markdown","ef51fde9":"markdown","9c642a42":"markdown","d12aee71":"markdown","00abcdfa":"markdown","ee67f4b3":"markdown","880f8879":"markdown","33090327":"markdown","1d242a1e":"markdown","1c8c8c09":"markdown","195ca27b":"markdown","3b3b19cb":"markdown","05c153d2":"markdown","303f1505":"markdown","813f196e":"markdown","b818b24f":"markdown","8d9f6e56":"markdown","3876c304":"markdown","fd6851ff":"markdown","6f5752e2":"markdown","005fa8b8":"markdown","be6b4f08":"markdown","6e588d4e":"markdown","1881c7d9":"markdown","6c3bde9f":"markdown","1ada442b":"markdown","4dd7d229":"markdown","cd29bb65":"markdown","302150e0":"markdown","3c2d1326":"markdown","fe78987d":"markdown","7d7d0121":"markdown","835abf2a":"markdown","ba964bc4":"markdown","5e819ed1":"markdown","9bae93a1":"markdown","d135ec9b":"markdown","b9ed6d93":"markdown","63f5e7c0":"markdown","57072f6b":"markdown","93d77a57":"markdown","5f15dc82":"markdown","5f610d35":"markdown","2408c136":"markdown","fa38eb8f":"markdown","8f1bcdd7":"markdown","917ea48f":"markdown","888feaec":"markdown","ea95d21f":"markdown","bc6c5126":"markdown","a90556d2":"markdown","825080a7":"markdown","6339b9e3":"markdown","433c63b8":"markdown","23a23e12":"markdown","e01e395d":"markdown","d01abf53":"markdown","f0edf07f":"markdown"},"source":{"9e8b6498":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')","cd19463f":"# load the train dataset.\ntitanic_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\n# summary\nprint('This dataset has {} observations with {} variables each.' .format(titanic_train.shape[0], titanic_train.shape[1]))\n\n# print first 10 observations\ntitanic_train.head(10)","f9a8a98e":"#Get the number of missing observations for each variable\ntitanic_train.isna().sum()","92a56089":"# Generate a new variable (Cabin_known) which equals one if the cabin of the passenger is known.\ntitanic_train[\"Cabin_known\"] = np.where(titanic_train[\"Cabin\"].isna() == False, 1, 0)\nsurvived_cabinknown = pd.crosstab(index=titanic_train[\"Survived\"], \n                           columns=titanic_train[\"Cabin_known\"], margins = True)\nsurvived_cabinknown.columns = [\"Unkown\",\"Known\",\"Row total\"]\nsurvived_cabinknown.index= [\"Died\",\"Survived\",\"Column total\"]\nsurvived_cabinknown\/survived_cabinknown.loc[\"Column total\"] #gives us relative figures","581b12e7":"titanic_train[\"Cabin_letter\"] = titanic_train[\"Cabin\"].str.slice(start=0,stop=1) # Gets the cabin letter\nsurvived_cabinletter = pd.crosstab(index=titanic_train[\"Survived\"], \n                           columns=titanic_train[\"Cabin_letter\"], margins = True)\nsurvived_cabinletter.columns = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"T\", \"Row total\"]\nsurvived_cabinletter.index = [\"Died\", \"Survived\", \"Column total\"]\nsurvived_cabinletter # We need the absolute frequencies to see whether the differences are statistically significant","7212afe0":"survived_cabinletter\/survived_cabinletter.loc[\"Column total\"]","060304d8":"titanic_train.loc[titanic_train['Cabin_letter'].isin([\"A\",\"C\",\"F\",\"G\",\"T\"]), 'Cabin_letter'] = 'known low'\ntitanic_train.loc[titanic_train['Cabin_letter'].isin([\"B\",\"D\",\"E\"]), 'Cabin_letter'] = 'known high'\ntitanic_train.loc[titanic_train['Cabin_letter'].isna(), 'Cabin_letter'] = 'unkown'","71f5097d":"# drop Cabin_known\ntitanic_train.drop(columns='Cabin', inplace = True)\n# drop Cabin_letter\ntitanic_train.drop(columns='Cabin_known', inplace = True)","73dfccb9":"titanic_train[\"Age_known\"] = np.where(titanic_train[\"Age\"].isna() == False, 1, 0)\nsurvived_ageknown = pd.crosstab(index = titanic_train[\"Survived\"], \n                                columns = titanic_train[\"Age_known\"])\nsurvived_ageknown","4575d787":"# titanic_train[\"Age\"].max() # check max age (80)\nbins = [0, 5, 12, 18, 35, 55, 100]\ntitanic_train[\"Age_group\"] = pd.cut(titanic_train[\"Age\"], bins)\nsurvived_agegroup = pd.crosstab(index = titanic_train[\"Survived\"], \n                           columns = titanic_train[\"Age_group\"])\nsurvived_agegroup","6582f8c8":"round(titanic_train[\"Age\"].mean())","8a8c9129":"titanic_train[\"Age\"].fillna(round(titanic_train[\"Age\"].mean()), inplace=True)\n\n# drop Age_group\ntitanic_train.drop(columns='Age_group', inplace = True)","73dcc138":"titanic_train[titanic_train['Embarked'].isna()]","22eb8756":"survived_embarked = pd.crosstab(index = titanic_train[\"Survived\"], \n                           columns = titanic_train[\"Embarked\"])\nsurvived_embarked","49e7e51a":"titanic_train.loc[titanic_train['Embarked'].isna(), 'Embarked'] = 'C'","9d83db32":"# Have a look at the first rows.\ntitanic_train.head(10)","49afb50a":"#Statistical description of numerical varibles\nprint('\\nStatistical description of dataset:')\ntitanic_train.describe() # It will only show up numerical variables","f5950259":"# drop 'Name'\ntitanic_train.drop(['Name'], axis=1, inplace=True)\n\n# 'Ticket_letter' = 1 if 'Ticket' starts with a letter\ntitanic_train['Ticket_letter'] = np.where(titanic_train['Ticket'].str.isnumeric() == False, 1, 0)\n\n# check survival rate\nsurvived_Ticketletter = pd.crosstab(index = titanic_train[\"Survived\"], \n                                columns = titanic_train[\"Ticket_letter\"])\nsurvived_Ticketletter","faecf6e2":"# extract first digit from 'Ticket'\ntitanic_train.loc[titanic_train['Ticket_letter'] == 0, 'Ticket_number'] = titanic_train['Ticket'][titanic_train['Ticket_letter'] == 0].str.slice(start=0,stop=1)\ntitanic_train.loc[titanic_train['Ticket_letter'] == 1, 'Ticket_number'] = titanic_train['Ticket'][titanic_train['Ticket_letter'] == 1].str.split().str[1].str.slice(start=0,stop=1)\n\n# check survival rate\nsurvived_Ticketletter = pd.crosstab(index = titanic_train[\"Survived\"], \n                                columns = titanic_train[\"Ticket_number\"])\nsurvived_Ticketletter","7ddc5edf":"titanic_train.loc[titanic_train['Ticket_number']=='B']","69c9b11f":"titanic_train.loc[titanic_train['Ticket'].str.split().str.len() == 3, 'Ticket_number'] = titanic_train['Ticket'][titanic_train['Ticket'].str.split().str.len() == 3].str.split().str[2].str.slice(start=0,stop=1)\n\n# check survival rate\nsurvived_Ticketletter = pd.crosstab(index = titanic_train[\"Survived\"], \n                                columns = titanic_train[\"Ticket_number\"])\nsurvived_Ticketletter","16c729ab":"titanic_train.head()","43dff926":"# convert 'Ticket_number' into numeric format \ntitanic_train['Ticket_number'] = pd.to_numeric(titanic_train['Ticket_number'])\n\n# check whether 'Ticket_number' == 'Pclass'\ntitanic_train['Pclass_Ticket_equal'] = np.where(titanic_train['Pclass'] == titanic_train['Ticket_number'], 1, 0)\n\nsum(titanic_train['Pclass_Ticket_equal'])\/titanic_train.shape[0]","855dc4bf":"# drop 'Ticket', Ticket_letter' and 'Pclass_Ticket_equal'\ntitanic_train.drop(['Ticket','Ticket_letter','Pclass_Ticket_equal'], axis=1, inplace=True)","d3fd84a2":"sum(titanic_train['Ticket_number'].isna()) # 4 missing values\n\ntitanic_train.loc[np.isnan(titanic_train['Ticket_number']),'Ticket_number'] = 4 ","3010344d":"n_records = titanic_train.shape[0]\nn_survived = titanic_train[titanic_train.Survived==1].shape[0]\npercent_survivied = (n_survived*100)\/n_records\n\nprint('Total number of passengers: {}'.format(n_records))\nprint('The number of survivors: {}'.format(n_survived))\nprint('The number of non-survivors: {}'.format(n_records - n_survived))\nprint('Survival probability: {:.2f}%'.format(percent_survivied))\n\nsns.countplot(x='Survived',label='Count',data=titanic_train)","d336227a":"#SibSp\nplt.subplot(221)\ntitanic_train.boxplot(column = 'SibSp')\n#Parch\nplt.subplot(222)\ntitanic_train.boxplot(column = 'Parch')\n#Fare\nplt.subplot(223)\ntitanic_train.boxplot(column = 'Fare')","197d9910":"# divide 'Fare' into different groups\nbins = [-1, 10, 30, 70, 550]  # the interval starts in -1 to include passengers with a fare of 0 (workers?)   \ntitanic_train[\"Fare_group\"] = pd.cut(titanic_train[\"Fare\"], bins)\nsurvived_faregroup = pd.crosstab(index=titanic_train[\"Survived\"], \n                           columns=titanic_train[\"Fare_group\"], margins = True)\nsurvived_faregroup","ae867355":"pclass_faregroup = pd.crosstab(index = titanic_train[\"Pclass\"], columns = titanic_train[\"Fare_group\"], margins = True)\npclass_faregroup","a5cff20d":"survived_pclass = pd.crosstab(index = titanic_train[\"Survived\"], columns = titanic_train[\"Pclass\"], margins = True)\nsurvived_pclass","268ebf77":"titanic_train.drop([\"Fare_group\"], axis = 1, inplace = True)","2293c74a":"titanic_train.groupby('Survived').mean()","443b8256":"titanic_train.drop(['PassengerId'], axis=1, inplace=False).corr()","b5f21c50":"pd.crosstab(titanic_train.Survived,titanic_train.SibSp).plot(kind='bar')","4d5524c8":"pd.crosstab(titanic_train.Survived,titanic_train.SibSp)","98041be7":"pd.crosstab(titanic_train.Survived,titanic_train.Parch).plot(kind='bar')","c16fdfe7":"pd.crosstab(titanic_train.Survived,titanic_train.Parch)","9d682649":"titanic_train['Family_members'] = titanic_train['SibSp'] + titanic_train['Parch']\n\npd.crosstab(titanic_train.Survived,titanic_train.Family_members).plot(kind='bar')","fa34335e":"titanic_train[\"Family_members\"] = titanic_train[\"Family_members\"].map({0:'none', 1:'few', 2:'few', 3:'few', 4:'many', 5:'many', 6:'many', 7:'many', 10:'many'})\n\ntitanic_train.drop(columns=['SibSp','Parch'], inplace = True)","88ea00c3":"pd.crosstab(titanic_train.Embarked,titanic_train.Survived).plot(kind='bar')","78bac1cf":"pd.crosstab(titanic_train.Embarked,titanic_train.Survived)","b004cb67":"pd.crosstab(titanic_train.Sex,titanic_train.Survived).plot(kind='bar')","31dfb9fb":"pd.crosstab(titanic_train.Sex,titanic_train.Survived)","fbd164c5":"titanic_train.Pclass = pd.Categorical(titanic_train.Pclass)\ntitanic_train.Sex = pd.Categorical(titanic_train.Sex)\ntitanic_train.Embarked = pd.Categorical(titanic_train.Embarked)\ntitanic_train.Cabin_letter = pd.Categorical(titanic_train.Cabin_letter)\ntitanic_train.Age_known = pd.Categorical(titanic_train.Age_known)\ntitanic_train.Family_members = pd.Categorical(titanic_train.Family_members)","ffbc0d48":"titanic_train.info()","e2e22303":"# Save file\ntitanic_train.to_pickle('.\/train.clean.pkl')","c712cc91":"titanic_test = pd.read_csv('..\/input\/titanic\/test.csv')","ef54b87d":"titanic_test.head()","eddd73f2":"titanic_test[\"Cabin_letter\"] = titanic_test[\"Cabin\"].str.slice(start=0,stop=1) # Gets the cabin letter\ntitanic_test.loc[titanic_test['Cabin_letter'].isin([\"A\",\"C\",\"F\",\"G\",\"T\"]), 'Cabin_letter'] = 'known low'\ntitanic_test.loc[titanic_test['Cabin_letter'].isin([\"B\",\"D\",\"E\"]), 'Cabin_letter'] = 'known high'\ntitanic_test.loc[titanic_test['Cabin_letter'].isna(), 'Cabin_letter'] = 'unkown'\n\n# drop Cabin\ntitanic_test.drop(columns='Cabin', inplace = True)","ad8cbada":"titanic_test[\"Age_known\"] = np.where(titanic_test[\"Age\"].isna() == False, 1, 0)\n\ntitanic_test[\"Age\"].fillna(round(titanic_test[\"Age\"].mean()), inplace=True)","4a48b6fc":"titanic_test.loc[titanic_test['Embarked'].isna(), 'Embarked'] = 'C'","5d504477":"# 'Ticket_letter' = 1 if 'Ticket' starts with a letter\ntitanic_test['Ticket_letter'] = np.where(titanic_test['Ticket'].str.isnumeric() == False, 1, 0)\n\n# extract first digit from 'Ticket'\ntitanic_test.loc[titanic_test['Ticket_letter'] == 0, 'Ticket_number'] = titanic_test['Ticket'][titanic_test['Ticket_letter'] == 0].str.slice(start=0,stop=1)\ntitanic_test.loc[titanic_test['Ticket_letter'] == 1, 'Ticket_number'] = titanic_test['Ticket'][titanic_test['Ticket_letter'] == 1].str.split().str[1].str.slice(start=0,stop=1)\n\n# extract first digit from 'Ticket'\ntitanic_test.loc[titanic_test['Ticket_letter'] == 0, 'Ticket_number'] = titanic_test['Ticket'][titanic_test['Ticket_letter'] == 0].str.slice(start=0,stop=1)\ntitanic_test.loc[titanic_test['Ticket_letter'] == 1, 'Ticket_number'] = titanic_test['Ticket'][titanic_test['Ticket_letter'] == 1].str.split().str[1].str.slice(start=0,stop=1)\n\n# extract first digit from 'Ticket' in case of three parts ticket number \ntitanic_test.loc[titanic_test['Ticket'].str.split().str.len() == 3, 'Ticket_number'] = titanic_test['Ticket'][titanic_test['Ticket'].str.split().str.len() == 3].str.split().str[2].str.slice(start=0,stop=1)\n\n# convert 'Ticket_number' into numeric format \ntitanic_test['Ticket_number'] = pd.to_numeric(titanic_test['Ticket_number'])","a8fc9854":"titanic_test.drop(['Name','Ticket','Ticket_letter'],1,inplace=True)","150cd50a":"titanic_test['Family_members'] = titanic_test['SibSp'] + titanic_test['Parch']\n\ntitanic_test['Family_members'].value_counts()","9583c35d":"titanic_test[\"Family_members\"] = titanic_test[\"Family_members\"].map({0:'none', 1:'few', 2:'few', 3:'few', 4:'many', 5:'many', 6:'many', 7:'many', 10:'many'})\n\n# drop 'Parch' and 'SibSp'\ntitanic_test.drop(columns=['SibSp','Parch'], inplace = True)","d64be5f2":"titanic_test.isna().sum()","e4994f46":"titanic_test.loc[titanic_test['Fare'].isna(),'Fare'] = titanic_test[\"Fare\"].median()","4a14b9e4":"titanic_test.Pclass = pd.Categorical(titanic_test.Pclass)\ntitanic_test.Sex = pd.Categorical(titanic_test.Sex)\ntitanic_test.Embarked = pd.Categorical(titanic_test.Embarked)\ntitanic_test.Cabin_letter = pd.Categorical(titanic_test.Cabin_letter)\ntitanic_test.Age_known = pd.Categorical(titanic_test.Age_known)\ntitanic_test.Family_members = pd.Categorical(titanic_test.Family_members)","afbb40d5":"titanic_test.to_pickle('.\/test.clean.pkl')","9fa5625d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# LPM and logistic regression\nfrom sklearn import linear_model\n\n# lDA and QDA\nfrom sklearn import discriminant_analysis\n\n# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# grid search crossvalidation\nfrom sklearn.model_selection import GridSearchCV\n\n# decision trees\nfrom sklearn.tree import DecisionTreeClassifier\n\n# bagging\nfrom sklearn.ensemble import BaggingClassifier\n\n# random forests\nfrom sklearn.ensemble import RandomForestClassifier\n\n# gradient boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# neural networks\nfrom sklearn.neural_network import MLPClassifier\n\n# data preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\n# SVM\nfrom sklearn.svm import SVC\n\n# model selection\nfrom sklearn.model_selection import cross_val_score\n\n# voting classifier\nfrom sklearn.ensemble import VotingClassifier","81eae4d7":"# load clean trainset\ntrain_clean = pd.read_pickle('.\/train.clean.pkl')\n\n# obtain covariates and dependent variable\ntrain_clean_X = train_clean.loc[:,['Pclass','Sex','Age','Fare','Embarked','Cabin_letter','Age_known','Ticket_number','Family_members']]\ntrain_clean_Y = train_clean.loc[:,'Survived']\n\n# get dummies\ntrain_clean_X = pd.get_dummies(train_clean_X, drop_first=True)","630ac409":"train_clean_X.head()","210276c4":"# create vector containing classifiers\n\nclassifiers = []\n\n# logistic regression\nclassifiers.append(linear_model.LogisticRegression())\n\n# linear discriminant analysis (LDA)\nclassifiers.append(discriminant_analysis.LinearDiscriminantAnalysis())\n\n# quadratic discriminant analysis (QDA)\nclassifiers.append(discriminant_analysis.QuadraticDiscriminantAnalysis())\n\n# KNN\nclassifiers.append(KNeighborsClassifier())\n\n# decision trees\nclassifiers.append(DecisionTreeClassifier())\n\n# bagging\nclassifiers.append(BaggingClassifier())\n\n# random forests\nclassifiers.append(RandomForestClassifier())\n\n# gradient boosting\nclassifiers.append(GradientBoostingClassifier())\n\n# multilayer perceptron (NN)\nclassifiers.append(MLPClassifier())\n\n# SVM\nclassifiers.append(SVC())\n\n# store results from cross-validation\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier,train_clean_X,train_clean_Y,scoring=\"accuracy\",cv=10,n_jobs=-1))\n\n# compute average performance & std deviation\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())","13d65387":"# create dataframe with results\ncv_res = pd.DataFrame({\"average performance\":cv_means,\"standard deviation\": cv_std,\"algorithm\":[\"logistic regression\",\"LDA\",\"QDA\",\n\"KNN\",\"decision trees\",\"bagging\",\"random forests\",\"gradient boosting\",\"NN\",\"SVM\"]})\n\ng = sns.barplot(\"average performance\",\"algorithm\",data = cv_res,orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"average performance\")\ng = g.set_title(\"cross validation scores\")","24c2b4cc":"# grid parameters\ngrid = {\n    'max_iter' : [100,200,500,1000,2000]\n}\n\n# grid search\nlogistic = GridSearchCV(\n    linear_model.LogisticRegression(),\n    grid,\n    cv=10 \n)\n\n# hyperparameter tuning using crossvalidation\nlogistic = logistic.fit(train_clean_X,train_clean_Y)","0e4e46ab":"logistic.best_estimator_","c9cebfdb":"# grid parameters\ngrid = {\n    'n_estimators' : [10,20,50,100,500,1000]\n}\n\n# grid search\nbagging = GridSearchCV(\n    BaggingClassifier(),\n    grid,\n    cv=10 \n)\n\n# hyperparameter tuning using crossvalidation\nbagging = bagging.fit(train_clean_X,train_clean_Y)","e2b912b6":"bagging.best_estimator_","fec4421d":"grid = {\n    'max_depth' : [3,5,7,10,15,20],\n}\n\n# grid search\nforests = GridSearchCV(\n    RandomForestClassifier(n_estimators=1000,n_jobs=-1),\n    grid,\n    cv=10,    \n    refit=True\n) \n\n# hyperparameter tuning using crossvalidation\nforests.fit(train_clean_X,train_clean_Y)","e59517b2":"forests.best_estimator_","d1c314d3":"# grid parameters\ngrid = {'loss': ['deviance', 'exponential'],\n        'max_depth': [1, 3, 5, 10]\n}\n\n# grid search\nboosting = GridSearchCV(\n    GradientBoostingClassifier(n_estimators = 1000),\n    grid,\n    cv=10,    \n    refit=True,\n)\n\n# hyperparameter tuning using crossvalidation\nboosting.fit(train_clean_X,train_clean_Y)","381b9b18":"boosting.best_estimator_","e1b85af5":"# voting classifier\nvoting = VotingClassifier(estimators=[('logistic',logistic.best_estimator_),('lda',discriminant_analysis.LinearDiscriminantAnalysis()),\n('bagging',bagging.best_estimator_),('forests',forests.best_estimator_),('boosting',boosting.best_estimator_)],voting='hard',n_jobs=-1)\n\nvoting = voting.fit(train_clean_X,train_clean_Y)","70687be0":"# load test set\ntest_clean = pd.read_pickle('.\/test.clean.pkl')\n\n# get covariates\ntest_clean_X = test_clean.loc[:,['Pclass','Sex','Age','Fare','Embarked','Cabin_letter','Age_known','Ticket_number','Family_members']]\n\n# get dummies\ntest_clean_X = pd.get_dummies(test_clean_X, drop_first=True)\n\n# predict using test set \npredictions = voting.predict(test_clean_X)","8c40d0ed":"submission = pd.DataFrame({'Survived': predictions}, index = test_clean.PassengerId)\nsubmission.to_csv('.\/submission.csv')","8eb4ce9e":"submission.value_counts()","3808023d":"## Decision trees ","5ec1ec07":"## Logistic regression","d613ae39":"Finally, let's have a look at the two observations for which the port of embark is missing.","651ecb79":"We see that passengers whose cabin name started with a \"B\", \"D\" or \"E\" apparently had a higher survival probability than passengers whose cabin name started with an \"A\", \"C\" or \"F\". Yet, these passengers had a higher survival probability than passengers whose cabin name was unknown. Due to the small sample size, we cannot make any claim for cabin names starting with \"G\" and \"T\". \n\nA sensible approach could be to discretised the variable $Cabin\\_letter$ with three different values: 'unkown' for passengers whose cabin name was unknown, 'known low' for passengers whose cabin name started with a \"A\", \"C\", \"F\", \"G\", or \"T\" and 'known high' otherwise (cabin name started with a \"B\", \"D\" or \"E\"). ","2bda2a2f":"Those passengers for whom their cabins were known had a greater probability of survival (66.7% vs 30%). If we have a further look at the variable $Cabin$ we will see that the location of each cabin is given by a letter followed by a number. It might be the case that the survival rate was different for cabins with different letters (the letters may indicates the class or the floor in which the cabin was located).","e4fb30ec":"print best hyperparameters:","f3fa0178":"Before undertaking any further statistical analysis, it is necessary to identify any outliers in our variables that could threaten the validity of our model. The simplest way of detecting outliers is by plotting boxplots of the continuous variables. \n","ae847431":"Additionally, $Age$ is also missing some of its values (~20%). Let's check whether passangers whose age is unkown had a lower probability of survival.","fa9f90fd":"## Categorical variables","8d82eae2":"### Gradient boosting","608753cd":"## Data exploration","1f549249":"A drawback of decision trees is that a small change in the dataset can lead to a very different tree (to see this run the code above several times and check the optimal depth obtained each time). To obtain more stable solutions, bagging can be applied. The idea is to average the predictions obtained from several trees fitted to bootstrapped data sets from the training set. ","a17b4b2d":"Create $Age\\_known$ and replace missing values of $Age$ with mean.","1d3c9873":"## Linear discriminant analysis","197cf46c":"Notice that the group that contains the average age (18 to 35 years old) and the group whose age is unkown have not too different survival probabilities. Thus, relationship between $Age$ and $Survived$ is not changed too much after the imputation. We still keep the variable $Age\\_known$ since it does gives us information about the survival probability of the passenger. ","c03361fc":"# Part 1: Data cleansing & exploration","7c5bb4c7":"Gradient boosting is similar to bagging but now every time a model is fitted, it uses the gradient to determine the direction in which the model needs to improve the most. The next model is then fitted taking into consideration this information, leading to gradually better models. ","34d832b6":"## Missing values & data cleansing ","cfe78037":"Create $Ticket\\_number$ variable.","e41b00e5":"Finally, we store the predictions in a CSV file to upload them to Kaggle.","d49b87d5":"First, we load the clean training data and create dummy variables from the categorical variables. We will do model selection by applying 10-fold cross-validation.  ","ef51fde9":"There are outliers in all the cases. In the first two cases ($SibSp$ and $Parch$), the outliers are passengers with many family members aboard. For $Fare$, the outliers are tickets with very high prices (compared to the price of the other tickets). Notice that it is very likely that it is related to $Pclass$. Let's examine this. ","9c642a42":"It seems as if passangers that embarked in Cherbourg (C) had a higher survival rate than those that did it in Queenstown (Q) and, especially, in Southampton (S). This could be due to the way passengers were allocated as they embarked the ship. Alternatively, this could also be explained if most of the crew members (with lower survival probability) embarked in Southampton. Moreover, female passangers had a higher survival rate than male passengers.","d12aee71":"Even though when we apply bagging we are averaging over predictions obtained with different models, these predictions may be correlated. In other words, the same predictors may be always included in the models resulting in very similar predictions. As a result, the reduction in variance resulting from adding predictions from different models is hindered (the variance of the sum of random variables increases when the variables are positively correlated). Similarly as bagging, random forests average the predictions obtained from decision trees fitted to bootstrapped samples but, unlike bagging, the trees are fitted using different subsets of the predictors. ","00abcdfa":"It seems that when $SibSp$ and $Parch$ are either 1 or 2 (or 3 in the case of $Parch$), the passenger has a larger probability of survival. Conversely, when they are 0 or greater to 2 (or 3 in the case of $Parch$) the probability of survival decreases. Since both variables express similar things (the number of siblings or spouse aboard and the number of parents or children aboad, respectively), we opt for creating a new variable called $Family\\_members$ that is made of the sum of both variables.  ","ee67f4b3":"Given that there are only two observations with missing value for $Embarked$, it is impossible to extract any conclusion. Since both passengers survived, we will impute the missing values with the port for which the survival probability was higher.","880f8879":"We now turn our attention to decision trees. This algorithm divides the predictor space in $m$ non-overlaping regions such that the resulting regions preferably contain observations that belong to only one of the classes (survived or not survivived in our case). In practice, perfect classification is unattainable and we instead minimise the classification error rate. \n\nIt should be noted that the classification error rate can be reduced by increasing the number of regions. In fact, it will be zero when there are as many regions as observations. Yet, performing well in the training set does not necessarily translate into a good performance in a different set (overfitting). For this reason, we will use cross-validation to determine the optimal depth (number of regions).","33090327":"Finally, notice that there are missing values for our new variable 'Ticket_number'. We will impute the missing values with the value 4. ","1d242a1e":"The higher the ticket number, the smaller the probability of survival (except for 9). Furthermore, one of the observations include a letter as first digit. Let's locate this observation.","1c8c8c09":"This notebook proposes a solution to Titanic Kaggle competition (https:\/\/www.kaggle.com\/c\/titanic). It comprises of two parts: a data cleansing & exploration part and a data modelling part. Comments and explanations are provided throughout the notebook. The following models are considered:\n\n- Logistic regression.\n- Linear discriminant analysis.\n- Quadratic discriminant analysis.\n- K-nearest neighbours (knn).\n- Decision trees.\n- Bagging.\n- Random forests. \n- Gradient boosting.\n- Neural networks.\n- Support vector machines (SVM). ","195ca27b":"Impute missing values of $Embarked$ in case there are missing values.","3b3b19cb":"Create variable $Cabin\\_letter$ and drop variable $Cabin$","05c153d2":"\nAt this point, we can draw several conclusions:\n\n- Our dependent variable is a binary one. Survival rate was $38.25\\%$.\n\n- The variable $Name$ may appear to have no value for the prediction at first sight. However, this is probably not the case. For instance, the last name may reveal the socioeconomic status of the passenger which, in turn, is likely to be related to his\/her survival probability. For the purpose of this notebook, we will not dig into this.\n\n- $Pclass$, $Ticket$ and $Fare$ measure similar things. Yet, they may still give us different information that can be useful for prediction. For instance, variations in fare ($Fare$) whithin a given class ($Pclass$) might be signaling differences in, for instance, the location of the cabin (level) which can influence survival rate. With respecto to the variable $Ticket$, it may be the case that the presence of letters is related to higher\/lower survival probabilities. Let's examine this. ","303f1505":"Once we have obtained the hyperparameters that result in the best performance for our models, we will use a voting classifier (ensemble modelling) to obtain our predictions for the test dataset. The reason for doing this is the reduction in the standard deviation of the performance that is obtained when combining the predictions from several models compared to just using the predictions from one single model. ","813f196e":"# Part 2: Statistical modelling","b818b24f":"It can be seen that the models with the best performance are logistic regression, LDA, bagging, random forests and gradient boosting. Moreover, the performance is not always the same. This can be seen by the standard deviation (black line). We will know carry out hyperparameter tuning for the best five models.","8d9f6e56":"Less than 75% of the observations share the same values for $Pclass$ and $Ticket\\_number$. Thus, given that it has predictive power, we keep $Ticket\\_number$. $Ticket$, $Ticket\\_letter$ and $Pclass\\_Ticket\\_equal$ are dropped. ","3876c304":"Finally, to be able to apply the statistical learning models, we have to change the data type of the categorical variables  $Pclass$, $Sex$, $Embarked$, $Cabin\\_letter$, $Age\\_known$ and $Family\\_members$ into factor variables.","fd6851ff":"$Survived$ is negatively correlated with $Pclass$ (smaller $Pclass$ is associated with a higher travel class) and positively correlated with $Cabin\\_known$. The correlation between these two last variables is, as we would expect, negative. It seems that there is very little (linear) correlation between $Survived$ and $SibSp$ and $Parch$. It may be convenient to have a further look to it.","6f5752e2":"## Modify the test dataset","005fa8b8":"$Ticket\\_number$ and $Pclass$ are probably measuring similar things. Indeed if we look at the first rows, we can see that for all except one row the two variables have the same values. ","be6b4f08":"Save file.","6e588d4e":"We drop $Cabin$ and $Cabin\\_known$.","1881c7d9":"Delete variables $Name$, $Ticket$ and $Ticket\\_letter$.","6c3bde9f":"Naturally, the same relationship is observed as with $SibSp$ and $Parch$. We transform $Family\\_members$ into a categorical variable. $SibSp$ and $Parch$ are dropped.","1ada442b":"print best hyperparameters:","4dd7d229":"print best hyperparameters:\n\n","cd29bb65":"### Conclusions:\n\n1. Higher class was associated to higher survival rate ($Pclass$ & $Fare$).\n2. Having a few of your family members aboard had also a positive effect ($SibSp$ & $Parch$). Conversely, having none or many family members aboard was associated with a lower survival probability.  \n3. Younger passangers were more likely to survive.\n4. Passengers who embarked in Cherbourg had a higher survival rate.\n5. Female passengers had a considerable higher survival rate.","302150e0":"Ideally, we want the proportion of 'Survived' and 'dead' to be as close as possible to the actual population proportion.\n","3c2d1326":"Load test dataset, create dummies and predict using the voting classifier.","fe78987d":"Finally, we want to have a quick view at the relationship between the explanatory variables and the dependent variable. This relationship can be seen in a covariance matrix.\n","7d7d0121":"We will impute its missing value with the median (since the distribution has a long right tail) value for Fare.","835abf2a":"For PassengerId 474, her ticket contains three parts instead of too. Let's extract the first digit:","ba964bc4":"### Relation between variables","5e819ed1":"There are no significant differences in survival rate among passengers with tickets starting with and without letters. Let's see if the first digit in the ticket's number is related to the survival probability.  ","9bae93a1":"We now turn our attention to linear discriminant analysis (LDA). Instead of modelling directly $P(Survived=1|X)$ we first model the distribution of the regressors separately for passengers who survived and for passengers who did not survived, that is, $P(X|Survived=1)$ and $P(X|Survived=0)$. Then, we apply the Bayes' theorem to obtain $P(Survived=1|X)$. Key assumptions for this model are that the regressors have a multivariate Gaussian distribution and that the covariance matrices are identical for both classes (survived and not survived). Notice that this model does not need hyperparameter tuning. ","d135ec9b":"The survival rate appears to be higher for young passengers, especially for those in the range (0, 5] and (5,12]. The question now is how to treat the missing values, as missing values cannot be introduced in our predictive model. One option is to replace the missing values with the mean of the variable $age$ (imputation).","b9ed6d93":"Now we turn our attention to the variables $Sex$ and $Embarked$.","63f5e7c0":"## Ensemble modelling","57072f6b":"print best hyperparameters:","93d77a57":"Change type to categorical.","5f15dc82":"### Random forests","5f610d35":"### Outliers","2408c136":"\n'Cabin' seems to be missing most of its observations. Let's make some quick comprobations to check whether a passenger whose cabin is known had a higher probability (or lower) of surviving.\n","fa38eb8f":"### Dependent variable","8f1bcdd7":"### Bagging","917ea48f":"Let's create a graph showing the results: ","888feaec":"Have a look at our predictors.","ea95d21f":"Notice that there is a missing value for $Fare$.","bc6c5126":"We start with the logistic model. It models the probability of survival conditional on the covariates, $P(Survived=1|X)$, using the logistic function:\n\n\\begin{equation}\n    P(Survived=1|X) = \\frac{e^{X\\beta}}{1+e^{X\\beta}}\n\\end{equation}","a90556d2":"Import necessary libraries","825080a7":"It is a good idea to have a look at the similarity between these two variables to decide whether to keep or not $Ticket\\_number$.   ","6339b9e3":"We can extract several conclusions from the frequency tables above:\n\n1. The more the passenger pays, the higher the probability that he survived.\n2. A higher class (1>2>3) is generally associated with a higher fare as we would expect.\n3. $Pclass$ is related with $Survived$ in the same way as $Fare\\_group$.\n   \nWe opt for leaving $Fare$ as it is even though it is correlated with $Pclass$. It can still reflect differences within a given $Pclass$ that are not captured by this variable. ","433c63b8":"Import necessary libraries","23a23e12":"Notice that we save the pandas dataframe in .pkl format since it preserves both the data and the variable types.","e01e395d":"It can be seen that passengers whose age was unkown had a lower survival probability $(\\frac{52}{125+52}\\approx 0.29)$ than passengers whose age was known $(\\frac{290}{424+290}\\approx 0.41)$. Furthermore, for passengers whose age was known, we can look at the probability of survival for each age group. ","d01abf53":"We take the estimated survival probability $38.25\\%$ as our best guess for the true unconditional survival probability. ","f0edf07f":"Create $Family\\_members$ and drop $Parch$ and $SibSp$."}}