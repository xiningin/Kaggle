{"cell_type":{"65a47e50":"code","66c2bab5":"code","0dee3900":"code","d431d6b8":"code","79e81cba":"code","3d2248fb":"code","82cbcc28":"code","45b74f47":"code","9bc9019e":"code","44bb4868":"code","50e477d6":"code","d82dea65":"code","633f1e30":"code","018f805f":"code","ca47867d":"code","4fa832c2":"code","431f4935":"markdown","4b1f4f7a":"markdown","bfd2e1ae":"markdown","e6e243b0":"markdown","dd17b1b4":"markdown","31828c42":"markdown","3f8243e5":"markdown","17b26abf":"markdown","5eb9c02e":"markdown","232b4a68":"markdown"},"source":{"65a47e50":"%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.max_columns', None)\n# import advertools as adv\n\ncx = 'CUSTOM_SEARCH_ENGINE_ID'  # used to get data from Google's custom search engine\nkey = 'API_KEY'                 ","66c2bab5":"candidates = pd.read_csv('..\/input\/midterm_election_candidates_2018_0921.csv')\nprint(candidates.shape)\ncandidates.head(2)","0dee3900":"candidates.url.isna().agg(['mean','sum'])","d431d6b8":"cand_summary = (candidates['party']\n                .value_counts()\n                .to_frame()\n                .assign(perc=candidates['party']\n                        .value_counts(normalize=True))\n                .assign(cum_perc=lambda df: df['perc'].cumsum()))\nprint('Top 10 parties\\ntotal candidates:', candidates.shape[0])\ncand_summary.head(10).style.format({'perc': '{:.1%}', 'cum_perc': '{:.1%}'})","79e81cba":"fig, ax = plt.subplots(facecolor='#ebebeb')\nfig.set_size_inches(10, 6)\nax.bar(cand_summary.index[:10], cand_summary['party'][:10], color='olive')\nax.set_frame_on(False)\nax.grid()\nax.set_title('US Midterm Elections Candidates Per Party - Top 10', fontsize=18)\nax.tick_params(labelsize=14)\nax.set_xlabel('Party', fontsize=14)\nax.set_ylabel('Number of Candidates', fontsize=14)\nplt.show()","3d2248fb":"cand_summary_state = (candidates['state']\n                      .value_counts()\n                      .to_frame()\n                      .assign(perc=candidates['state']\n                              .value_counts(normalize=True))\n                      .assign(cum_perc=lambda df: df['perc'].cumsum()))\nprint('Top 10 states (582 out of 1,230 candidates)')#, cand_summary_state['state'].sum())\ncand_summary_state.head(10).style.format({'perc': '{:.1%}', 'cum_perc': '{:.1%}'})","82cbcc28":"fig, ax = plt.subplots(facecolor='#ebebeb')\nfig.set_size_inches(10, 6)\nax.bar(cand_summary_state.index[:10], \n       cand_summary_state['state'][:10], color='olive')\nax.set_frame_on(False)\nax.grid()\nax.set_title('US Midterm Elections Candidates Per State - Top 10', fontsize=18)\nax.tick_params(labelsize=14)\nax.set_xlabel('State', fontsize=14)\nax.set_ylabel('Number of Candidates', fontsize=14)\nplt.show()","45b74f47":"serp_candidates = pd.read_csv('..\/input\/serp_candidates_oct_21.csv')\nprint(serp_candidates.shape)\nserp_candidates.head(2)","9bc9019e":"top_domains = (serp_candidates\n               .displayLink.str.replace('.*(house.gov)', 'house.gov')\n               .value_counts())\ntop_domains.to_frame()[:25]","44bb4868":"fig, ax = plt.subplots(facecolor='#ebebeb')\nfig.set_size_inches(8,8)\nax.set_frame_on(False)\nax.barh(top_domains.index.str.replace('www.', '')[:15], top_domains.values[:15])\nax.invert_yaxis()\nax.grid()\nax.tick_params(labelsize=14)\nax.set_xticks(range(0, 1000, 100))\nax.set_title('Top Domains Search Ranking - 2018 Midterm Elections', pad=20, fontsize=20)\nax.text(0.5, 1, 'Searching for 1,230 Candidates\\' Names', fontsize=16,\n        transform=ax.transAxes, ha='center')\nax.set_xlabel('Number of appearances on SERPs', fontsize=14)\nplt.show()","50e477d6":"top_df = (serp_candidates\n          [serp_candidates['displayLink'].str.replace('.*(house.gov)', 'house.gov')\n          .isin(top_domains.index[:15])].copy())\ntop_df['displayLink'] = top_df['displayLink'].str.replace('.*(house.gov)', 'house.gov')\n\n# similar to top_df, but containing all domains:\nall_serp_can = (pd.merge(serp_candidates, candidates, how='left', left_on='searchTerms', \n                         right_on='clean_name')\n                .sort_values(['searchTerms', 'rank'])\n                .reset_index(drop=True))\n\ntop_serp_cand = pd.merge(top_df, candidates, how='left', left_on='searchTerms', right_on='clean_name').sort_values(['searchTerms'])\ntop_serp_cand.head(1)","d82dea65":"(all_serp_can\n .pivot_table('rank', ['displayLink'], aggfunc=['count', 'mean'])\n .reset_index()\n .sort_values([('count', 'rank')], ascending=False)\n .reset_index(drop=True)\n .head(15).style.format({('mean', 'rank'): '{:.2f}'}))","633f1e30":"for i, party in enumerate(['DEM', 'REP']):\n    fig, ax = plt.subplots(1, 1, facecolor='#ebebeb')\n    fig.set_size_inches(12, 8)\n\n    ax.set_frame_on(False)\n    ax.scatter((top_serp_cand[top_serp_cand['party']==party]\n                   .sort_values('displayLink')['displayLink']\n                   .str.replace('www.', '')), \n                  (top_serp_cand[top_serp_cand['party']==party]\n                   .sort_values('displayLink')['rank']), \n                  s=850, alpha=0.02, edgecolor='k', lw=2, color='navy' if party == 'DEM' else 'darkred')\n    ax.grid(alpha=0.25)\n    ax.invert_yaxis()\n    ax.yaxis.set_ticks(range(1, 11))\n    ax.tick_params(labelsize=15, rotation=30, labeltop=True,\n                   labelbottom=False, length=8)\n    ax.xaxis.set_ticks_position('top')\n    ax.set_ylabel('Search engine results page rank', fontsize=16)\n    ax.set_title('Midterm Election Search Ranking for Candidate Names - ' + party, pad=95, fontsize=24)\n    fig.tight_layout()\n    plt.show()","018f805f":"%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt\n\n\nfor i, state in enumerate(cand_summary_state.index[:5]):\n    fig, ax = plt.subplots(facecolor='#ebebeb')\n    fig.set_size_inches(12, 8)\n    ax.set_frame_on(False)\n    ax.scatter((top_serp_cand[top_serp_cand['state']==state]\n                   .sort_values('displayLink')['displayLink']\n                   .str.replace('www.', '')), \n                  (top_serp_cand[top_serp_cand['state']==state]\n                   .sort_values('displayLink')['rank']), \n                  s=850, alpha=0.1, edgecolor='k', lw=2, color='olive')\n    ax.grid(alpha=0.25)\n    ax.invert_yaxis()\n    ax.yaxis.set_ticks(range(1, 11))\n    ax.tick_params(labelsize=15, rotation=30, labeltop=True,\n                   labelbottom=False, length=8)\n    ax.xaxis.set_ticks_position('top')\n    ax.set_ylabel('Search engine results page rank', fontsize=16)\n    ax.set_title('Midterm Election Search Ranking for Candidate Names - ' + state, pad=95, fontsize=20)\n    fig.tight_layout()\n    plt.show()","ca47867d":"own_domain = (all_serp_can\n [all_serp_can.url.str.replace('https?:\/\/(www.)?|\/$', '')\n  .eq(all_serp_can.displayLink)][['rank','party']].copy())\n\nown_domain_summary = (own_domain\n                      .pivot_table('rank', 'party',\n                                   aggfunc=['count', 'mean', 'std'])\n                      .sort_values([('count', 'rank')], ascending=False))\nown_domain_summary","4fa832c2":"fig, ax = plt.subplots(facecolor='#ebebeb')\nfig.set_size_inches(14, 7)\nfor party in own_domain['party'].unique():\n    ax.scatter('party', 'rank', data=own_domain.query('party == @party'), s=800, alpha=0.1)\n    ax.set_frame_on(False)\n    ax.grid()\n    ax.tick_params(labelsize=13)\n    ax.set_yticks(range(1, 11))\n    ax.invert_yaxis()\n    ax.set_title('Search Results Position for Candidates\\' Own Domain', fontsize=18, pad=30)\nplt.show()","431f4935":"### Generating Search Data\nI used the name from the `clean_name` column for the queries. \nThis is the call to import the data from Google:  \n`serp_candidates = adv.serp_goog(q=candidates['clean_name'].tolist(), cx=cx, key=key, gl='us')`  \nThis will make 1,230 requests, put each set of results in a DataFrame, and merge all results together. For such a number of requests, it's better to split them into smaller requests, in case you get an issue midway.  \nThe search results will be merged with the `candidates`  dataset later for further analysis.\n\nSERP is the acronym used frequenty by SEO's for \"search engine results pages\". ","4b1f4f7a":"Again, no major differences. This confirms that the top sites are quite consistent, and seem to be influential when it comes to what people know about their candidates. \n\nNow we can take a look at the candidates' own personal websites, and how they rank on SERPs, split by party. ","bfd2e1ae":"# US Midterm Elections 2018 on Google Search\n### Who are the most influential websites?\n#### Search engine results pages data for 1,230 candidates\n\nThe upcoming election is going to be determined by (sorry to state the obvious) who people vote for. People vote based on what they know about their candidates. People go online to know more about those candidates to make up their minds. Quite often, they use a search engine called Google!\n\nThis is what they see...\n\n### The Keywords\n\nAlthough the majority of searches are probably for \"election\", \"midterm elections\", or \"elections 2018\", the keywords that will ultimately determine the results, are the ones that are about the candidates that people will vote for. \n\nI got the list of updated candidates from [ProPublica](https:\/\/www.propublica.org\/datastore\/dataset\/2018-midterm-election-congressional-candidates). They have included and added some great meta data about candidates (state, party, URL, and more).  \n\n\n### Methodology\n\nI ran a Google query for each of the candidates' names, which returned ten results each. Each result contains the visible information on the search results page (title, domain, snippet, etc.), as well as some additional meta data like the total number of results and rank.  \nFor more information on how this works, you can check out my tutorial on how to use [Google Custom Search Engine to get search data with Python.](https:\/\/www.kaggle.com\/eliasdabbas\/search-engine-results-pages-serps-research)\n\nKeep in mind that this approach treats all candidates equally, which is known not to be the case. Several states are pretty much determined which way they will vote, and some are not. There are differences across states in terms of population and representation. Even across equal states, there might be a different number of candidates for each.   \nThis is not a problem, because we are not trying to predict the outcome of the election here. We are trying to find out which websites are the most influential based on their strength in SEO. It will be clear that there is a set of sites that are consistently prominent on search results pages. \n\nLet's first load the main libraries, and quickly explore the ProPublica data. ","e6e243b0":"Checking for missing data on URLs it seems that 259 candidates don't have a personal website, which is around 21% of the candidates. I reached out to ProPublica and they confirmed that indeed these candidates do not have personal websites. That's a lot, and could be a hint as to how serious the candidate is.\n\nLet's quickly take a look at the candidates, and visualize them by state and by party. ","dd17b1b4":"### Visualizing the different domains \nAlthough we now know the average rank from the above table, we can get a better perspective to visualize how those results are distributed.  It would also be interesting to see if there are any differences across the two major parties.  \nThe darker the circle the more frequent that domain appears in that position. ","31828c42":"### Appearances on SERPs and average rank per domain","3f8243e5":"No surprise with the top social media sites and Wikipedia. The election-specific ones that seem to be big are also prominent (Ballotpedia, VoteSmart, and OpenSecrets).  house.gov is mainly for incumbent candidates who already have a profile page on this website in the form of `<name>.house.gov`.\nI would have expected a much stronger presence by the news sites.  \nWe can now define a DataFrame that contains the top sites (15 in this case - `top_df`), and then merge it with the `candidates` DataFrame: ","17b26abf":"It doesn't seem there is a noticeable difference across the two major parties when it comes to SEO presence. The dominant domains are the same, and their distribution also looks similar. \n\nWe can do the same for the top five states for example (including all parties).","5eb9c02e":"The columns that look interesting are `party`, `state`, and `url`. The first two for segmenting candidates, and the URL for comparing their personal URL with actual results and seeing how these sites rank for the main keyword, the name of the candidate. ","232b4a68":"The majority rank high, but not as strongly and consistently as for example house.gov. \n\n\nSo, this was a quick summary of how influential sites can be uncovered, especially when you have a long list of keywords \/ topics. The same keywords could be run in different ways, for example, by explicitly saying \"name last_name congress\" or \"name last_name state\". The visualizations can be filtered to show several possible combinations of party, state, even city, zip code and district. \n\nFor details on how to produce similar data, you can check the [tutorial](https:\/\/www.kaggle.com\/eliasdabbas\/search-engine-results-pages-serps-research) mentioned above. Feel free to share any [feedback](https:\/\/twitter.com\/eliasdabbas), or [bugs.](https:\/\/github.com\/eliasdabbas\/advertools) "}}