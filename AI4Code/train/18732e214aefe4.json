{"cell_type":{"2db96d62":"code","78ea9bdc":"code","9aed1615":"code","e5aff932":"code","39f77b8e":"code","b82346ef":"code","10b47a2b":"code","1e91e1c8":"code","ec038de0":"code","a13d1172":"code","07414f90":"code","72a03dda":"code","b10eefcb":"code","344dd39d":"code","142f635f":"code","164700ac":"code","11ec19b5":"code","14fa74b6":"code","10d095be":"markdown","99c6ef2c":"markdown","bea19ec7":"markdown","ef83cf21":"markdown","4d1c64d2":"markdown","0fe9aad1":"markdown","80f00e66":"markdown","1a44a920":"markdown","04896a85":"markdown","9507b86b":"markdown","4cf36c6d":"markdown","0c6c6bcf":"markdown","06c5e66d":"markdown","240d6c33":"markdown","d8f7a778":"markdown","5f59e005":"markdown","cc721fbb":"markdown"},"source":{"2db96d62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","78ea9bdc":"data=pd.read_csv(\"\/kaggle\/input\/ufcdata\/preprocessed_data.csv\")\ndata.head()","9aed1615":"for col in data.columns: \n    print(col) ","e5aff932":"from sklearn import preprocessing\nfrom sklearn.preprocessing import label_binarize\n\nle = preprocessing.LabelEncoder()\nfor i in range(0,len(data.columns)):\n    data.iloc[:,i] = le.fit_transform(data.iloc[:,i])\ndata.head()\n","39f77b8e":"nrows=len(data.index)\npercentage=round((nrows*90)\/100)\ndata=data.sample(frac=1, random_state=69)\ntrainingData=data.iloc[:percentage,:]\ntestData=data.iloc[percentage:,:]\n\nprint(\"Number of training data examples \"+str(len(trainingData.index)))\nprint(\"Number of test examples \"+str(len(testData.index)))","b82346ef":"train_x=trainingData[[\"B_wins\", \"B_losses\",\"B_draw\", \"R_wins\", \"R_losses\",\"R_draw\"]]\n#train_x=trainingData[trainingData.columns.difference(['b'])]\ntrain_y=trainingData[\"Winner\"]\n\ntest_x=testData[[\"B_wins\", \"B_losses\",\"B_draw\", \"R_wins\", \"R_losses\",\"R_draw\"]]\n#test_x=testData[testData.columns.difference(['b'])]\ntest_y=testData[\"Winner\"]","10b47a2b":"train_x.head()","1e91e1c8":"train_y.head","ec038de0":"class AbstractPerceptron :\n    weights=np.array([])\n    learningRate=1\n    def __init__(self, learningRate):\n        self.learningRate=learningRate\n        \n    def predict(self,x):\n        xtemp=np.append(1,x)\n        return self.weights.dot(xtemp)\n\n    def getWeights(self):\n        return self.weights","a13d1172":"class Perceptron(AbstractPerceptron):\n    def train(self,x,y):\n        nFeatures=x.shape[1]\n        nExamples=x.shape[0]\n        onesColumn=np.ones([nExamples,1],dtype=int)\n        xtemp=np.append(onesColumn,x,axis=1)\n        np.random.seed(69)\n        self.weights=np.random.rand(nFeatures+1)\n        for i in range(0,nExamples):\n            output=self.predict(x[i][:])\n            adjustment=(self.learningRate*(y[i]-output))*xtemp[i][:]\n            self.weights=(self.weights+adjustment)\n    pass","07414f90":"class SignPerceptron(Perceptron):\n    def  predict(self,x):\n        predictions=super().predict(x)\n        return np.sign(predictions)\n    pass","72a03dda":"class SigmoidPerceptron(Perceptron):\n    def  predict(self,x):\n        predictions=super().predict(x)\n        sigmoid=lambda x : 1\/(np.exp(x*-1)+1)\n        return sigmoid(predictions)\n    pass","b10eefcb":"class GradientDescentPerceptron(AbstractPerceptron):\n    def train(self,x,y):\n        nFeatures=x.shape[1]\n        nExamples=x.shape[0]\n        onesColumn=np.ones([nExamples,1],dtype=int)\n        xtemp=np.append(onesColumn,x,axis=1)\n        np.random.seed(69)\n        self.weights=np.random.uniform(-0.5,0.5,nFeatures+1)\n        deltas=np.zeros(len(self.weights))\n        for i in range(0,nExamples):\n            output=self.predict(x[i][:])\n            for j in range(0,len(deltas)):\n                deltas[j]=deltas[j]+(self.learningRate*(y[i]-output))*xtemp[i][j]\n        self.weights=(self.weights+deltas)\n    pass","344dd39d":"class SigmoidGradientDescentPerceptron(AbstractPerceptron):  \n    def train(self,x,y):\n        sigmoid=lambda x : 1\/(np.exp(x*-1)+1)\n        nFeatures=x.shape[1]\n        nExamples=x.shape[0]\n        onesColumn=np.ones([nExamples,1],dtype=int)\n        xtemp=np.append(onesColumn,x,axis=1)\n        np.random.seed(69)\n        self.weights=np.random.uniform(-1*10^-10,-1*10^-10,nFeatures+1)\n        deltas=np.zeros(len(self.weights))\n        for i in range(0,nExamples):\n            output=self.predict(x[i][:])\n            for j in range(0,len(deltas)):\n                deltas[j]=deltas[j]+self.learningRate*(y[i]-output)*sigmoid(y[i])*(1-sigmoid(y[i]))*xtemp[i][j]\n            self.weights=(self.weights+deltas)\n    def predict(self,x):\n        prediction=super().predict(x)\n        sigmoid=lambda x : 1\/(np.exp(x*-1)+1)\n        return sigmoid(prediction)","142f635f":"from sklearn.metrics import accuracy_score\nperceptron=SignPerceptron(1)\nperceptron.train(train_x.to_numpy(),train_y.to_numpy())\n\nnExamples=test_x.shape[0]\npredictions=np.empty([nExamples,1])\nfor i in range(0,nExamples):\n    predictions[i]=perceptron.predict(test_x.iloc[i][:])\n\nprint(\"The accuracy score of the prediction on the test data is\",accuracy_score(test_y, predictions))\n\nnExamples=train_x.shape[0]\npredictions=np.empty([nExamples,1])\nfor i in range(0,nExamples):\n    predictions[i]=perceptron.predict(train_x.iloc[i][:])\nprint(\"The accuracy score of the prediction on the train data is\",accuracy_score(train_y, predictions))\n\nprint()\n\nprint(\"The calculated weights are \")\nprint(perceptron.getWeights())","164700ac":"perceptron=SigmoidPerceptron(1)\nperceptron.train(train_x.to_numpy(),train_y.to_numpy())\n\nnExamples=test_x.shape[0]\npredictions=np.empty([nExamples,1])\nfor i in range(0,nExamples):\n    predictions[i]=perceptron.predict(test_x.iloc[i][:])\nprint(\"The accuracy score of the prediction on the test data is\",accuracy_score(test_y, predictions.round()))\n\nnExamples=train_x.shape[0]\npredictions=np.empty([nExamples,1])\nfor i in range(0,nExamples):\n    predictions[i]=perceptron.predict(train_x.iloc[i][:])\nprint(\"The accuracy score of the prediction on the train data is\",accuracy_score(train_y, predictions.round()))\n\nprint()\n\nprint(\"The calculated weights are \")\nprint(perceptron.getWeights())","11ec19b5":"perceptron=SigmoidGradientDescentPerceptron(1)\nperceptron.train(train_x.to_numpy(),train_y.to_numpy())\n\nnExamples=test_x.shape[0]\npredictions=np.empty([nExamples,1])\nfor i in range(0,nExamples):\n    predictions[i]=perceptron.predict(test_x.iloc[i][:])\nprint(\"The accuracy score of the prediction on the test data is\",accuracy_score(test_y, predictions.round()))\n\nnExamples=train_x.shape[0]\npredictions=np.empty([nExamples,1])\nfor i in range(0,nExamples):\n    predictions[i]=perceptron.predict(train_x.iloc[i][:])\nprint(\"The accuracy score of the prediction on the train data is\",accuracy_score(train_y, predictions.round()))\n\nprint()\n\nprint(\"The calculated weights are \")\nprint(perceptron.getWeights())","14fa74b6":"from sklearn.linear_model import Perceptron\nperceptron = Perceptron(alpha=1)\n\nperceptron.fit(train_x, train_y)\n\npredictions=perceptron.predict(test_x)\nprint(\"The accuracy score of the prediction on the test data is\",accuracy_score(test_y, predictions.round()))\n\npredictions=perceptron.predict(train_x)\nprint(\"The accuracy score of the prediction on the train data is\",accuracy_score(train_y, predictions.round()))","10d095be":"# Analizing Data","99c6ef2c":"# Dividing into training data and test data","bea19ec7":"Come si pu\u00f2 notare l'accuracy ottenuta \u00e8 piuttosto bassa ciononostante ritengo che sia piuttosto normale ottenere tali risultati con un singolo perceptron. Per aspirare a risultati migliori sarebbe necessario implementare un'apposita rete neurale multistrato.","ef83cf21":"Gradient descent version of the perceptron","4d1c64d2":"Extention of the base perceptron that uses the sigmoid function as activation function","0fe9aad1":"Most basic perceptron version without any kind of activation function","80f00e66":"Analizziamo prima la versione che ha come funzione di attivazione la funzione segno","1a44a920":"Analizziamo la versione sigmoide gradient descent","04896a85":"# Preprocessing\nSklearn implementation of the decision tree algoritmh doesn't support discrete values, but only binary or real values, so we need to encode each label into a numerical value","9507b86b":"Extention of the base perceptron which uses the sign function as activation function","4cf36c6d":"Gradient descent version of the perceptron with a sigmoid function as activation function","0c6c6bcf":"AbstractPerceptron class which contains all the attributes and operations common to each type of perceptron","06c5e66d":"Analizziamo ora la versione alternativa in cui utilizziamo come funzione di attivazione la funzione sigmoide","240d6c33":"# Training the perceptron","d8f7a778":"# Comparing results with the sklearn implementation of the perceptron","5f59e005":"# Creating the perceptron algorithm","cc721fbb":"List of all feature available in the dataset"}}