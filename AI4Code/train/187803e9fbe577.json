{"cell_type":{"e9237948":"code","038f78ca":"code","d3285f1a":"code","26c8084d":"code","6814516a":"code","bdd9b816":"code","0a47516b":"code","aa97f093":"code","9654644c":"code","f8c5084c":"code","715c62a6":"code","ad35045f":"code","898594a3":"code","e041518c":"code","82aa2b04":"code","9ab140cd":"code","eaedf513":"code","c8e56bba":"code","c6fe4201":"code","4eb1714a":"code","ae52af29":"code","0e38d212":"code","933993a8":"code","27911e8a":"code","160d22be":"code","df066362":"code","3212ffb6":"code","803a8b57":"code","e9b550e3":"code","519c03b6":"code","e3e325f5":"code","7b9fa3ad":"code","c7e72d4f":"markdown","dca4c923":"markdown","99b52a7d":"markdown","47841c43":"markdown","49287bd0":"markdown","b5f3ed72":"markdown","e40d3bbb":"markdown","8cbe1b74":"markdown","7f732d1e":"markdown","a03df9c1":"markdown","0306c5dc":"markdown","292c54cb":"markdown","e60f029d":"markdown","73564f1d":"markdown","eb18b8ce":"markdown","8eb806e7":"markdown","34193f3f":"markdown","50f92ce2":"markdown","f0648556":"markdown","92f51955":"markdown"},"source":{"e9237948":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import tree\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport graphviz\nfrom seaborn import heatmap\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nimport plotly.express as px\nfrom matplotlib import rcParams\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\n","038f78ca":"df = pd.read_csv('..\/input\/world-happiness-report\/2020.csv')\ndf.head(10)\n","d3285f1a":"columns = df.columns\ncolumns_df = pd.DataFrame({\"names\":columns})\nprint(\"All columns in our dataset\")\ncolumns_df","26c8084d":"print(\"There are {} rows and {} columns in the dataset.\".format(df.shape[0], df.shape[1]))","6814516a":"df.describe()","bdd9b816":"fig = px.bar(data_frame = df.nlargest(10,\"Ladder score\"),\n             y=\"Country name\",\n             x=\"Ladder score\",\n             orientation='h',\n             color=\"Country name\",\n             text=\"Ladder score\",\n             color_discrete_sequence=px.colors.qualitative.D3)\nprint(\"Top 10 happiest countries\")\nfig.show()","0a47516b":"fig = px.bar(data_frame = df.nsmallest(10,\"Ladder score\"),\n             y=\"Country name\",\n             x=\"Ladder score\",\n             orientation='h',\n             color=\"Country name\",\n             text=\"Ladder score\",\n             color_discrete_sequence=px.colors.qualitative.D3)\nprint(\"Top 10 unhappiest countries\")\nfig.show()","aa97f093":"rcParams[\"figure.figsize\"] = 20,10\nplt.title(\"Corellation between different features\")\nsns.heatmap(df.corr(),annot=True,cmap=\"YlGnBu\")\n","9654644c":"cols=['Explained by: Log GDP per capita', 'Explained by: Social support',\n       'Explained by: Healthy life expectancy',\n       'Explained by: Freedom to make life choices',\n       'Explained by: Generosity', 'Explained by: Perceptions of corruption',\n       'Dystopia + residual']\nfor a in cols:\n    plt.figure(figsize=(10,5))\n    sns.regplot(x=a,y='Ladder score',data=df,color='b')\n    plt.show()","f8c5084c":" def feature_analysis(df, feature):               \n    grouped_df = df.groupby([\"Regional indicator\"]).agg({feature : np.mean}).reset_index()\n    template='%{text:0.2f}'\n    tickformat = None\n    if grouped_df[feature].min() < 1:\n        template='%{text:0.3%}'\n        tickformat = \".3%\"\n    \n    fig = px.bar(grouped_df,\n                 x=\"Regional indicator\",\n                 y=feature,\n                 color=\"Regional indicator\",\n                 text=feature,\n                 color_discrete_sequence=px.colors.qualitative.D3\n                )\n    fig.update_traces(textposition='outside') \n    fig.show()\nfeature_names = [\"Logged GDP per capita\",\n                 \"Social support\",\n                 \"Healthy life expectancy\",\n                 \"Freedom to make life choices\",\n                 \"Generosity\",\n                 \"Perceptions of corruption\"]\nfor feature in feature_names:\n    feature_analysis(df, feature)","715c62a6":"y=df['Ladder score']\nX=df[['Logged GDP per capita', 'Social support', 'Healthy life expectancy',\n       'Freedom to make life choices', 'Generosity',\n       'Perceptions of corruption', 'Ladder score in Dystopia']]\n\nX_train,X_test,Y_train,Y_test= train_test_split(X,y,test_size=0.2,random_state=1)\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","ad35045f":"lin_reg = LinearRegression()\nlin_reg.fit(X_train,Y_train)\ny_hat = lin_reg.predict(X_test)\nscore=lin_reg.score(X_test, Y_test)\nmse = mean_squared_error(Y_test, y_hat)\nmae = mean_absolute_error(Y_test, y_hat)\nr2 = r2_score(Y_test, y_hat)\nr2, mse","898594a3":"coefficients = pd.DataFrame({\"Feature\":X.columns,\"Coefficients\":np.transpose(lin_reg.coef_)})\nfig = px.pie(data_frame = coefficients,\n             names=\"Feature\",\n             values=\"Coefficients\",\n             color=\"Feature\",\n             color_discrete_sequence=px.colors.sequential.RdBu)\nprint(\"Feature weights\")\nfig.show()","e041518c":"model = Ridge()\nmodel.fit(X_train, Y_train)\ny_hat = model.predict(X_test)\nR2_train = model.score(X_train, Y_train)\nR2_test = model.score(X_test, Y_test)\nmse = mean_squared_error(Y_test, y_hat)\nR2_test, mse","82aa2b04":"coefficients = pd.DataFrame({\"Feature\":X.columns,\"Coefficients\":np.transpose(model.coef_)})\nfig = px.pie(data_frame = coefficients,\n             names=\"Feature\",\n             values=\"Coefficients\",\n             color=\"Feature\",\n             color_discrete_sequence=px.colors.sequential.RdBu)\nprint(\"Feature weights\")\nfig.show()","9ab140cd":"model = Lasso()\nmodel.fit(X_train, Y_train)\ny_hat = model.predict(X_test)\nR2_train = model.score(X_train, Y_train)\nR2_test = model.score(X_test, Y_test)\nmse = mean_squared_error(Y_test, y_hat)\nR2_test, mse","eaedf513":"model = ElasticNet()\nmodel.fit(X_train, Y_train)\ny_hat = model.predict(X_test)\nR2_train = model.score(X_train, Y_train)\nR2_test = model.score(X_test, Y_test)\nmse = mean_squared_error(Y_test, y_hat)\nR2_test, mse","c8e56bba":"coefficients = pd.DataFrame({\"Feature\":X.columns,\"Coefficients\":np.transpose(model.coef_)})\nfig = px.pie(data_frame = coefficients,\n             names=\"Feature\",\n             values=\"Coefficients\",\n             color=\"Feature\",\n             color_discrete_sequence=px.colors.sequential.RdBu)\nprint(\"Feature weights\")\nfig.show()","c6fe4201":"dtr= DecisionTreeRegressor()\ndtr.fit(X_train,Y_train)\ny_pred = dtr.predict(X_test)\ntest_mse = mean_squared_error(Y_test, y_pred)\ny_pred_train = dtr.predict(X_train)\ntrain_mse = mean_squared_error(Y_train, y_pred_train)\ndtr.score(X_test, Y_test), test_mse ","4eb1714a":"rf = RandomForestRegressor(n_estimators = 1000)\nrf.fit(X_train, Y_train)\ny_hat = rf.predict(X_test)\nerrors = abs(y_hat - Y_test)\nacc = 1 - errors\nrf.score(X_test, Y_test), np.mean(acc)","ae52af29":"svr = SVR(kernel='linear')\nsvr.fit(X_train, Y_train)\ny_hat = svr.predict(X_test)\nprint(r2_score(Y_test,y_hat))","0e38d212":"coefficients = pd.DataFrame({\"Feature\":X.columns,\"Coefficients\":np.transpose(svr.coef_[0])})\nfig = px.pie(data_frame = coefficients,\n             names=\"Feature\",\n             values=\"Coefficients\",\n             color=\"Feature\",\n             color_discrete_sequence=px.colors.sequential.RdBu)\nprint(\"Feature weights\")\nfig.show()","933993a8":"folds = KFold(n_splits=5, shuffle=True, random_state=13579)\nscore = 0\nY_train = pd.DataFrame(data=Y_train, columns = {\"Ladder score\"})\nX_train = pd.DataFrame(data=X_train, columns = {'Logged GDP per capita', 'Social support', 'Healthy life expectancy',\n       'Freedom to make life choices', 'Generosity',\n       'Perceptions of corruption', 'Ladder score in Dystopia'})\nfor i, (x_index, y_index) in enumerate(folds.split(X_train, Y_train['Ladder score'])):\n    print('-' * 22, i, '-' * 22)\n    svr = SVR(kernel='linear')\n    svr.fit(X_train.iloc[x_index], Y_train['Ladder score'].iloc[x_index])\n    score += svr.score(X_train.iloc[y_index], Y_train['Ladder score'].iloc[y_index])\n    print('score ', svr.score(X_train.iloc[y_index], Y_train['Ladder score'].iloc[y_index]))\n    \nprint('Average Accuracy', score \/ folds.n_splits)\n","27911e8a":"coefficients = pd.DataFrame({\"Feature\":X.columns,\"Coefficients\":np.transpose(svr.coef_[0])})\nfig = px.pie(data_frame = coefficients,\n             names=\"Feature\",\n             values=\"Coefficients\",\n             color=\"Feature\",\n             color_discrete_sequence=px.colors.sequential.RdBu)\nprint(\"Feature weights\")\nfig.show()","160d22be":"folds = KFold(n_splits=5, shuffle=True, random_state=13579)\nscore = 0\nY_train = pd.DataFrame(data=Y_train, columns = {\"Ladder score\"})\nX_train = pd.DataFrame(data=X_train, columns = {'Logged GDP per capita', 'Social support', 'Healthy life expectancy',\n       'Freedom to make life choices', 'Generosity',\n       'Perceptions of corruption', 'Ladder score in Dystopia'})\nfor i, (x_index, y_index) in enumerate(folds.split(X_train, Y_train['Ladder score'])):\n    print('-' * 22, i, '-' * 22)\n    dtr= DecisionTreeRegressor()\n    dtr.fit(X_train.iloc[x_index], Y_train['Ladder score'].iloc[x_index])\n    score += dtr.score(X_train.iloc[y_index], Y_train['Ladder score'].iloc[y_index])\n    print('score ', dtr.score(X_train.iloc[y_index], Y_train['Ladder score'].iloc[y_index]))\n    \nprint('Average Accuracy', score \/ folds.n_splits)\n","df066362":"folds = KFold(n_splits=5, shuffle=True, random_state=13579)\nscore = 0\nY_train = pd.DataFrame(data=Y_train, columns = {\"Ladder score\"})\nX_train = pd.DataFrame(data=X_train, columns = {'Logged GDP per capita', 'Social support', 'Healthy life expectancy',\n       'Freedom to make life choices', 'Generosity',\n       'Perceptions of corruption', 'Ladder score in Dystopia'})\nfor i, (x_index, y_index) in enumerate(folds.split(X_train, Y_train['Ladder score'])):\n    print('-' * 22, i, '-' * 22)\n    model = ElasticNet()\n    model.fit(X_train.iloc[x_index], Y_train['Ladder score'].iloc[x_index])\n    score += model.score(X_train.iloc[y_index], Y_train['Ladder score'].iloc[y_index])\n    print('score ', model.score(X_train.iloc[y_index], Y_train['Ladder score'].iloc[y_index]))\n    \nprint('Average Accuracy', score \/ folds.n_splits)\n","3212ffb6":"coefficients = pd.DataFrame({\"Feature\":X.columns,\"Coefficients\":np.transpose(model.coef_)})\nfig = px.pie(data_frame = coefficients,\n             names=\"Feature\",\n             values=\"Coefficients\",\n             color=\"Feature\",\n             color_discrete_sequence=px.colors.sequential.RdBu)\nprint(\"Feature weights\")\nfig.show()","803a8b57":"folds = KFold(n_splits=5, shuffle=True, random_state=13579)\nscore = 0\nY_train = pd.DataFrame(data=Y_train, columns = {\"Ladder score\"})\nX_train = pd.DataFrame(data=X_train, columns = {'Logged GDP per capita', 'Social support', 'Healthy life expectancy',\n       'Freedom to make life choices', 'Generosity',\n       'Perceptions of corruption', 'Ladder score in Dystopia'})\nfor i, (x_index, y_index) in enumerate(folds.split(X_train, Y_train['Ladder score'])):\n    print('-' * 22, i, '-' * 22)\n    model = Lasso()\n    model.fit(X_train.iloc[x_index], Y_train['Ladder score'].iloc[x_index])\n    score += model.score(X_train.iloc[y_index], Y_train['Ladder score'].iloc[y_index])\n    print('score ', model.score(X_train.iloc[y_index], Y_train['Ladder score'].iloc[y_index]))\n    \nprint('Average Accuracy', score \/ folds.n_splits)\n","e9b550e3":"folds = KFold(n_splits=5, shuffle=True, random_state=13579)\nscore = 0\nY_train = pd.DataFrame(data=Y_train, columns = {\"Ladder score\"})\nX_train = pd.DataFrame(data=X_train, columns = {'Logged GDP per capita', 'Social support', 'Healthy life expectancy',\n       'Freedom to make life choices', 'Generosity',\n       'Perceptions of corruption', 'Ladder score in Dystopia'})\nfor i, (x_index, y_index) in enumerate(folds.split(X_train, Y_train['Ladder score'])):\n    print('-' * 22, i, '-' * 22)\n    model = Ridge()\n    model.fit(X_train.iloc[x_index], Y_train['Ladder score'].iloc[x_index])\n    score += model.score(X_train.iloc[y_index], Y_train['Ladder score'].iloc[y_index])\n    print('score ', model.score(X_train.iloc[y_index], Y_train['Ladder score'].iloc[y_index]))\n    \nprint('Average Accuracy', score \/ folds.n_splits)\n","519c03b6":"coefficients = pd.DataFrame({\"Feature\":X.columns,\"Coefficients\":np.transpose(model.coef_)})\nfig = px.pie(data_frame = coefficients,\n             names=\"Feature\",\n             values=\"Coefficients\",\n             color=\"Feature\",\n             color_discrete_sequence=px.colors.sequential.RdBu)\nprint(\"Feature weights\")\nfig.show()","e3e325f5":"folds = KFold(n_splits=5, shuffle=True, random_state=13579)\nscore = 0\nY_train = pd.DataFrame(data=Y_train, columns = {\"Ladder score\"})\nX_train = pd.DataFrame(data=X_train, columns = {'Logged GDP per capita', 'Social support', 'Healthy life expectancy',\n       'Freedom to make life choices', 'Generosity',\n       'Perceptions of corruption', 'Ladder score in Dystopia'})\nfor i, (x_index, y_index) in enumerate(folds.split(X_train, Y_train['Ladder score'])):\n    print('-' * 22, i, '-' * 22)\n    rfr = RandomForestRegressor(n_estimators=200, n_jobs=-1)\n    rfr.fit(X_train.iloc[x_index], Y_train['Ladder score'].iloc[x_index])\n    score += rfr.score(X_train.iloc[y_index], Y_train['Ladder score'].iloc[y_index])\n    print('score ', rfr.score(X_train.iloc[y_index], Y_train['Ladder score'].iloc[y_index]))\n    \nprint('Average Accuracy', score \/ folds.n_splits)\n\n","7b9fa3ad":"testing_vals = pd.DataFrame(data=[{2.11085868,0.94068184,1.17120687,0.30922594,-0.4856469,-0.58227342,2}], columns = {'Logged GDP per capita', 'Social support', 'Healthy life expectancy', \n                                                                                                                   'Freedom to make life choices', 'Generosity','Perceptions of corruption', \n                                                                                                                   'Ladder score in Dystopia'})\n\nprint(rfr.predict(testing_vals))\n","c7e72d4f":"|           Model          \t| Accuracy \t|\n|:------------------------:\t|:--------:\t|\n|  Baseline Model (linear) \t|   0.58   \t|\n|     Ridge Regression     \t|   0.58   \t|\n|     Lasso Regression     \t|  -0.004  \t|\n|   ElasticNet Regression  \t|   0.36   \t|\n|  DecisionTree Regression \t|   0.29   \t|\n| Random Forest Regression \t|   0.53   \t|\n|      SVM Regression      \t|   0.59   \t|\n\n\n|     Model with KFold     \t| Accuracy \t|\n|:------------------------:\t|:--------:\t|\n|     Ridge Regression     \t|   0.68   \t|\n|     Lasso Regression     \t|   -0.08  \t|\n|   ElasticNet Regression  \t|   0.39   \t|\n|  DecisionTree Regression \t|   0.71   \t|\n| Random Forest Regression \t|   0.77   \t|\n|      SVM Regression      \t|   0.65   \t|\n\n\n<br\/><br\/>\n<br\/>\n<br\/>\n\n\n|           Model          \t| Logged GDP \t| Social Support \t| Healthy life expectancy \t| Freedom to make life choices \t| Generosity \t| Ladder score in Dystopia \t|\n|:------------------------:\t|:----------:\t|:--------------:\t|:-----------------------:\t|:----------------------------:\t|:----------:\t|:------------------------:\t|\n|  Baseline Model (linear) \t|    24.8%   \t|      33.3%     \t|          20.8%          \t|             14.7%            \t|    6.31%   \t|            0%            \t|\n|     Ridge Regression     \t|    24.9%   \t|      33.1%     \t|           21%           \t|             14.8%            \t|    6.26%   \t|            0%            \t|\n|     Lasso Regression     \t|     0%     \t|       0%       \t|            0%           \t|              0%              \t|     0%     \t|            0%            \t|\n|   ElasticNet Regression  \t|    32.9%   \t|      36.4%     \t|          30.7%          \t|              0%              \t|     0%     \t|            0%            \t|\n|  DecisionTree Regression \t|     N\/A    \t|       N\/A      \t|           N\/A           \t|              N\/A             \t|     N\/A    \t|            N\/A           \t|\n| Random Forest Regression \t|     N\/A    \t|       N\/A      \t|           N\/A           \t|              N\/A             \t|     N\/A    \t|            N\/A           \t|\n|      SVM Regression      \t|    21.6%   \t|      32.5%     \t|          24.7%          \t|             16.5%            \t|    4.67%   \t|            0%            \t|\n\n<br\/>\n\n|     Model with KFold     \t| Logged GDP \t| Social Support \t| Healthy life expectancy \t| Freedom to make life choices \t| Generosity \t| Ladder score in Dystopia \t|\n|:------------------------:\t|:----------:\t|:--------------:\t|:-----------------------:\t|:----------------------------:\t|:----------:\t|:------------------------:\t|\n|     Ridge Regression     \t|    26.9%   \t|      32.1%     \t|          17.2%          \t|             16.1%            \t|    7.73%   \t|            0%            \t|\n|     Lasso Regression     \t|     0%     \t|       0%       \t|            0%           \t|              0%              \t|     0%     \t|            0%            \t|\n|   ElasticNet Regression  \t|     29%    \t|      42.5%     \t|          27.1%          \t|             1.47%            \t|     0%     \t|            0%            \t|\n|  DecisionTree Regression \t|     N\/A    \t|       N\/A      \t|           N\/A           \t|              N\/A             \t|     N\/A    \t|            N\/A           \t|\n| Random Forest Regression \t|     N\/A    \t|       N\/A      \t|           N\/A           \t|              N\/A             \t|     N\/A    \t|            N\/A           \t|\n|      SVM Regression      \t|    19.8%   \t|      25.5%     \t|          22.7%          \t|             17.8%            \t|    14.2%   \t|            0%            \t|","dca4c923":"# KFold with Lasso Regression","99b52a7d":"# Basic Baseline model","47841c43":"# Results so far","49287bd0":"# Final Summary","b5f3ed72":"# KFold with Ridge Regression","e40d3bbb":"|           Model          \t| Accuracy \t|\n|:------------------------:\t|:--------:\t|\n|  Baseline Model (linear) \t|   0.58   \t|\n|     Ridge Regression     \t|   0.58   \t|\n|     Lasso Regression     \t|  -0.004  \t|\n|   ElasticNet Regression  \t|   0.36   \t|\n|  DecisionTree Regression \t|   0.29   \t|\n| Random Forest Regression \t|   0.53   \t|\n|      SVM Regression      \t|   0.59   \t|\n\n\n\n\n|           Model          \t| Logged GDP \t| Social Support \t| Healthy life expectancy \t| Freedom to make life choices \t| Generosity \t| Ladder score in Dystopia \t|\n|:------------------------:\t|:----------:\t|:--------------:\t|:-----------------------:\t|:----------------------------:\t|:----------:\t|:------------------------:\t|\n|  Baseline Model (linear) \t|    24.8%   \t|      33.3%     \t|          20.8%          \t|             14.7%            \t|    6.31%   \t|            0%            \t|\n|     Ridge Regression     \t|    24.9%   \t|      33.1%     \t|           21%           \t|             14.8%            \t|    6.26%   \t|            0%            \t|\n|     Lasso Regression     \t|     0%     \t|       0%       \t|            0%           \t|              0%              \t|     0%     \t|            0%            \t|\n|   ElasticNet Regression  \t|    32.9%   \t|      36.4%     \t|          30.7%          \t|              0%              \t|     0%     \t|            0%            \t|\n|  DecisionTree Regression \t|     N\/A    \t|       N\/A      \t|           N\/A           \t|              N\/A             \t|     N\/A    \t|            N\/A           \t|\n| Random Forest Regression \t|     N\/A    \t|       N\/A      \t|           N\/A           \t|              N\/A             \t|     N\/A    \t|            N\/A           \t|\n|      SVM Regression      \t|    21.6%   \t|      32.5%     \t|          24.7%          \t|             16.5%            \t|    4.67%   \t|            0%            \t|","8cbe1b74":"# WORLD HAPPINESS REPORT\n \nName: Tarek Saidee\n\nNID: ts3732\n\nThe idea of happiness is subjective among everyone. Each person gets a sense of happiness from their own experiences and ideas. There are certain general metrics that can be used to predict the overall happiness of a certain community or society. This is where the dataset I'm using comes in. https:\/\/www.kaggle.com\/mathurinache\/world-happiness-report contains over 20 columns of data each being a metric derived from a certain country. Those metrics are used to determine the overall happiness rank for that compared to others. Some of those columns include life expectancy, generosity, GDP per capita and so on. Each country is then given a rank based on those metrics. \n\nThe goal here is to use that data to see which metrics or features are the most important in determining the happiness rank of a country and how tweeking a certain data point such as life expectancy would have an effect on the happiness level. This could be used by leaders or politicians everywhere to see which aspects of life would make their citizens the happiest. They can use that data to build their platforms. For example, someone could input some data into the model to get a happiness level then that same person could tweek the life expectancy which should change the happiness level. By doing so, that person can determine how important focusing on each aspect of life is to the people of that country. There's many other applications that could be used here. ","7f732d1e":"# Data Analysis","a03df9c1":"# KFold with SVM regression","0306c5dc":"# ElasticNet Regression","292c54cb":"# KFold with Decision Tree Regression","e60f029d":"# Transforming the data","73564f1d":"# KFold with ElasticNet Regression","eb18b8ce":"# Lasso Regression","8eb806e7":"# DecisionTree Regression","34193f3f":"# Ridge Regression","50f92ce2":"# SVM Regression","f0648556":"# KFold with RandomForestRegressor","92f51955":"# Random Forest Regression"}}