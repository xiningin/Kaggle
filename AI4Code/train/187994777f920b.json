{"cell_type":{"e76e0940":"code","a0e20682":"code","071b0f33":"code","20fc0141":"code","7aa67551":"code","20862f60":"code","d8793846":"code","81afb04c":"code","d82e711d":"code","99127fb1":"code","f86e09be":"code","1bee8df6":"code","a80d1a9a":"code","24fff090":"markdown","286a61c4":"markdown","0c340392":"markdown","49be93d6":"markdown","d52a7c05":"markdown","dda1f5e4":"markdown","ed5208d8":"markdown","3d13e20b":"markdown","e575b77f":"markdown","031227b0":"markdown","2f49b34b":"markdown","258000cd":"markdown","edadce8b":"markdown","f67c41a9":"markdown","6dbc8e25":"markdown","28a4478b":"markdown","4f4e4cc7":"markdown","dde548c0":"markdown"},"source":{"e76e0940":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',None)","a0e20682":"df = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")","071b0f33":"df.head(2)","20fc0141":"df.shape","7aa67551":"df.describe().T","20862f60":"# Percentage of missing values\n(df.isnull().sum()\/df.shape[0]*100)","d8793846":"# Correlation with the target\ndf.corr()[['claim']]","81afb04c":"fig, ax = plt.subplots(6, 1, figsize = (30,20))\nsns.boxplot(data = df.iloc[:, 1:20], ax = ax[0])\nsns.boxplot(data = df.iloc[:, 20:40], ax = ax[1])\nsns.boxplot(data = df.iloc[:, 40:60], ax = ax[2])\nsns.boxplot(data = df.iloc[:, 60:80], ax = ax[3])\nsns.boxplot(data = df.iloc[:, 80:100], ax = ax[4])\nsns.boxplot(data = df.iloc[:, 100:], ax = ax[5])","d82e711d":"features = df.columns[1:-1]\nfeat_df=df[features]\nn_df=((feat_df - feat_df.min())\/(feat_df.max() - feat_df.min()))","99127fb1":"fig, ax = plt.subplots(6, 1, figsize = (30,20))\nsns.boxplot(data = n_df.iloc[:, 1:20], ax = ax[0])\nsns.boxplot(data = n_df.iloc[:, 20:40], ax = ax[1])\nsns.boxplot(data = n_df.iloc[:, 40:60], ax = ax[2])\nsns.boxplot(data = n_df.iloc[:, 60:80], ax = ax[3])\nsns.boxplot(data = n_df.iloc[:, 80:100], ax = ax[4])\nsns.boxplot(data = n_df.iloc[:, 100:], ax = ax[5])","f86e09be":"nrows = 30\nncols = 4\ni = 0\nfig, ax = plt.subplots(nrows, ncols, figsize = (40,120))\nfor row in range(nrows):\n    for col in range(ncols):\n        if i==118:\n            break\n        else:\n            sns.histplot(data = n_df.iloc[:, i], bins = 30, ax = ax[row, col]).set(ylabel = '')\n            i += 1","1bee8df6":"i = 0\nfig, ax = plt.subplots(nrows, ncols, figsize = (40,120))\nfor row in range(nrows):\n    for col in range(ncols):\n        if i==118:\n            break\n        else:\n            sns.kdeplot(x = n_df.iloc[:, i], ax = ax[row, col]).set(ylabel = '')\n            i += 1","a80d1a9a":"# !pip install sweetviz\nimport sweetviz\nmy_report = sweetviz.analyze([df,'Train'], target_feat='claim')\nmy_report.show_html('FinalReport.html')","24fff090":"## 3. D-Tale\n### quickest of the lot. It seemed like it hosted report on the machine and created the visulaizations as requested through the browser interface. You should definitely check it out! ","286a61c4":"## Observations so far\n* Dataset is huge - 957919 rows, 120 columns\n* We have 118 feature columns excluding the id and target - claim column\n* All the feature columns have varied range of values\n* Missing values are constantly around 1.6% in each column! hmm, Should we drop?(I'm not for it) should we impute them somehow?? we'll ponder over it later..\n* As seen there is also not much of a correlation with the target variable. o-O","0c340392":"## 2. Pandas Profiling\n\n### Since the data is huge pandas profiling took a lot of time and the report generated was also over 500mb on my local machine.Code is as below:","49be93d6":"# b) Load Dataset","d52a7c05":"> * import sweetviz\n> * my_report  = sweetviz.analyze([df,'Train'], target_feat='claim')\n> * my_report.show_html('FinalReport.html')","dda1f5e4":"> * from pandas_profiling import ProfileReport\n> * profile = ProfileReport(df, title=\"Pandas Profiling Report\", explorative=True)\n> * profile.to_file(\"PandasProfilingReport.html\")","ed5208d8":"# 2. Summarize Data\n# a) Descriptive Statistics","3d13e20b":"## 1. Sweetviz","e575b77f":"## [sweetviz report](.\/FinalReport.html)","031227b0":"### Observations from visualizations\n* We can clearly see from the visualisations that there are quite a few outliers - we will need a strategy to handle them\n* Also, we see that the distributions are non-gaussian, all kinds of skewness exists in the data. we have to work towards staandardizing this too\n* Yet, there are some columns that are following a gaussian pattern. to handle this we could use Scalers viz - Standard, Robust or MinMax and check with them later.","2f49b34b":"# Problem Statement- \nTo predict whether a customer made a claim upon an insurance policy. The ground truth claim is binary valued, but a prediction may be any number from 0.0 to 1.0, representing the probability of a claim. The features in this dataset have been anonymized and may contain missing values.","258000cd":"# b) Data visualizations","edadce8b":"# a) Load libraries","f67c41a9":"# 1. Preparation","6dbc8e25":"### **** Thats it for this version! Do let me know your critiques through comments and\/or appreciation through upvotes. TIA ****","28a4478b":"### This just confirms what we observed earlier that the ranges are so varied we cannot see the plots clearly.. So lets normalize the data for the visualization","4f4e4cc7":"> * !pip install dtale\n> * import dtale\n> * d = dtale.show(df)\n> * d.open_browser()","dde548c0":"### Now an introduction to some tools that provide quick EDA - I'll be only sharing the codes on how to use them. Some take a lot of time and are also slow to use with the dataset being so huge. Take your pick"}}