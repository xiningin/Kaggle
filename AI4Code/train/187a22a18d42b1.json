{"cell_type":{"2d123b3f":"code","c7ee7832":"code","737e1d2f":"code","4dea554e":"code","30311122":"code","7a6147a2":"code","fcda091a":"code","3f71348c":"code","cd1946f3":"code","a375a6ea":"code","2b69c099":"code","11731c1d":"code","c2c84a82":"code","f885c41a":"markdown","bb207dda":"markdown","d941435b":"markdown","a4efd5a2":"markdown","eaa20c6c":"markdown","74380b30":"markdown","4933c7b7":"markdown"},"source":{"2d123b3f":"import pandas as pd\nimport numpy as np\nimport gc","c7ee7832":"if False:\n    train = pd.read_csv('..\/input\/ubiquant-market-prediction\/train.csv')","737e1d2f":"%%time\ntrain = pd.read_parquet('..\/input\/ubiquant-parquet\/train.parquet')","4dea554e":"train.info()","30311122":"train.dtypes","7a6147a2":"del train\ngc.collect()","fcda091a":"%%time\ntrain = pd.read_parquet('..\/input\/ubiquant-parquet\/train_low_mem.parquet')","3f71348c":"train.info()","cd1946f3":"train.dtypes","a375a6ea":"%%time\nexample = pd.read_parquet('..\/input\/ubiquant-parquet\/investment_ids\/529.parquet')","2b69c099":"example.info()","11731c1d":"%%time\ncol_subset = ['time_id','investment_id','target','f_1','f_2','f_3']\ntrain = pd.read_parquet('..\/input\/ubiquant-parquet\/train.parquet',\n               columns=col_subset)","c2c84a82":"train.info()","f885c41a":"# Reading as CSV (Slow)\n- **18GB in size**\n- Don't Do this. It may cause the kaggle notebooks to crash.","bb207dda":"# Reading a Subset of Columns","d941435b":"# Read just a single `investment_id`\n- If you only want to work with a single transaction load them like this","a4efd5a2":"# Reading as Parquet (Fast)\n- **5.5GB** in size.\n- This is faster and keeps the dtypes of the original dataset.","eaa20c6c":"## Thanks!","74380b30":"# Reading as Parquet Low Memory (Fast & Low Mem Use)\n- **3.63GB** in size\n- Even better! Uses less memory and loads even faster!","4933c7b7":"# Speed Up Loading The Data By Importing from the Parquet Dataset\n\nDataset Link here: https:\/\/www.kaggle.com\/robikscube\/ubiquant-parquet\n\nRead about parquet files here: https:\/\/databricks.com\/glossary\/what-is-parquet\n\nExcerpt from the above website:\n\n**What is Parquet?**\n\n*Parquet is an open source file format available to any project in the Hadoop ecosystem. Apache Parquet is designed for efficient as well as performant flat columnar storage format of data compared to row based files like CSV or TSV files.*\n\n*Parquet uses the record shredding and assembly algorithm which is superior to simple flattening of nested namespaces. Parquet is optimized to work with complex data in bulk and features different ways for efficient data compression and encoding types.  This approach is best especially for those queries that need to read certain columns from a large table. Parquet can only read the needed columns therefore greatly minimizing the IO.*"}}