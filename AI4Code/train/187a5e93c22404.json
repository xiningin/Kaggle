{"cell_type":{"0a729248":"code","1076e0e0":"code","539877e8":"code","36d4dfbd":"code","5676a024":"code","f2d5abc6":"code","e6c8f17d":"code","7027e40e":"code","b61ec214":"code","340ff9a8":"code","47a49ce1":"code","f416ea00":"code","4d03b180":"code","1ec343f3":"code","48d986e7":"code","1badc947":"code","379561fd":"code","540695a1":"code","ffbef6cb":"code","a72f0d99":"code","e0f168df":"code","73f8026a":"code","6911d26e":"markdown","b41ea00a":"markdown","522f5b70":"markdown","01cf023a":"markdown","9e0a7163":"markdown","a3be1d98":"markdown","483c0c2e":"markdown","022ed40a":"markdown","517bb8ac":"markdown","50d7913f":"markdown","35319505":"markdown","8e2b34a3":"markdown","9f3b24e2":"markdown","ed761468":"markdown","5479c69c":"markdown"},"source":{"0a729248":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn.model_selection\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1076e0e0":"df=pd.read_csv('\/kaggle\/input\/cusersmarildownloadsgermancsv\/german.csv',encoding ='ISO-8859-1',sep=\";\")\ndf.head()","539877e8":"!pip install feyn","36d4dfbd":"import feyn","5676a024":"df.Creditability.value_counts()","f2d5abc6":"train, test = sklearn.model_selection.train_test_split(df, stratify=df[\"Creditability\"], train_size=.66, random_state=1)","e6c8f17d":"#Allocate a QLattice\n\nql = feyn.connect_qlattice()","7027e40e":"ql.reset(random_seed=1)","b61ec214":"#Code by Liv Toft https:\/\/www.kaggle.com\/livtoft\/explainable-model-for-hf-using-the-qlattice\n\nql.reset(random_seed=1)\nmodels = ql.auto_run(train, output_name=\"Creditability\", kind=\"classification\", max_complexity=10, criterion='aic')","340ff9a8":"models[0]","47a49ce1":"models[0].plot(train, test)","f416ea00":"#Code by Liv Toft https:\/\/www.kaggle.com\/livtoft\/explainable-model-for-hf-using-the-qlattice\n\nrf = feyn.reference.RandomForestClassifier(train, output_name=\"Creditability\")\ngb = feyn.reference.GradientBoostingClassifier(train, output_name=\"Creditability\")\nlr = feyn.reference.LogisticRegressionClassifier(train, output_name=\"Creditability\", max_iter=10000)","4d03b180":"#Code by Liv Toft https:\/\/www.kaggle.com\/livtoft\/explainable-model-for-hf-using-the-qlattice\n\nmodels[0].plot_roc_curve(test, label='QLattice')\nrf.plot_roc_curve(test, label=\"Random Forest\")\ngb.plot_roc_curve(test, label=\"Gradient Boosting\")\nlr.plot_roc_curve(test, label=\"Logistic Regression\")","1ec343f3":"models[0].plot_confusion_matrix(test)","48d986e7":"rf.plot_confusion_matrix(test)","1badc947":"from feyn.plots.interactive import interactive_activation_flow","379561fd":"interactive_activation_flow(models[0], train)","540695a1":"models[0].plot_probability_scores(test)","ffbef6cb":"#Code by Liv Toft https:\/\/www.kaggle.com\/livtoft\/the-qlattice-shows-how-3-features-predict-toxicity\/notebook\n\ntrain, test = sklearn.model_selection.train_test_split(df, stratify=df[\"Creditability\"], train_size=.66, random_state=1)\ntest, holdout = sklearn.model_selection.train_test_split(test, stratify=test[\"Creditability\"], test_size=.5, random_state=1)","a72f0d99":"predictions = models[0].predict(holdout)","e0f168df":"predictions","73f8026a":"holdout[\"Creditability\"]","6911d26e":"#Compare this to three other Machine Learning algorithms: Random Forest, Gradient Boost, and Logistic Regression","b41ea00a":"#The QLattice works with both categorical and numerical data, but needs to be told which entries are categorical (i.e. it assumes they are numerical). ","522f5b70":"\"We can see here that the QLattice outperforms Random Forest and Logistic Regression. The ROC curve for the QLattice and Gradient Boosting seem to resemble one another.\"\n\nFor me they are all very close.","01cf023a":"#The actual QLattice is a Quantum Simulator that runs on Abzu's hardware, but we can allocate one with a single line of code. ","9e0a7163":"##See the number of false negative predictions of Random Forest models make using a confusion matrix.","a3be1d98":"#Visualize their relative performances using their respective ROC curves","483c0c2e":"#Performance on train vs. test\n\nLet's see how the model performs on the train versus test dataset. Looking for high accuracy and AUC, but similar values across the two datasets (Don't Overfit!)","022ed40a":"#Splitting the data\n\nLet's split the data into train and test sets. Stratify by Creditability and take 2\/3 of the entire dataset for training\n\nThe line below doesn't work with few data (class_count)","517bb8ac":"#Models\n\nmodels is a list of graphs sorted by accuracy. Each model shows how the selected features, or inputs, interact to achieve the output. We can access the best graph by calling:","50d7913f":"#Code by Liv Toft https:\/\/www.kaggle.com\/livtoft\/the-qlattice-shows-how-3-features-predict-toxicity\/notebook","35319505":"#See the number of false negative predictions of the QLattice models make using a confusion matrix.","8e2b34a3":"#Understanding the model\n\nSee how each feature contributes to the model using plot_flow_interactive","9f3b24e2":"#Reset","ed761468":"#Search for the best model","5479c69c":"We can see here that the QLattice model has a Little (32\/31) lower false negative rate. It predicts that 31 Credit subjects are fine compared to the 32 predicted by Random Forest. From a Credit perspective, the QLattice model is therefore better. (In this case the difference is very small)"}}