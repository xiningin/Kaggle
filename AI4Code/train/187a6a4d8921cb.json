{"cell_type":{"be523597":"code","8acd83d1":"code","ba7795ab":"code","cb1cb0ec":"code","d3baed02":"code","0fe74ee5":"code","df4c4158":"code","79e05b10":"code","38468286":"code","1abc455e":"code","3c4e3711":"code","6ff5e672":"code","f7f25072":"code","ce83d0d0":"code","44216575":"code","966bb30b":"code","60a7a808":"code","9b22a19d":"code","00c67d86":"code","74806bbc":"code","0d09bcea":"code","d5f189e9":"code","e0457b1c":"code","8ec11342":"code","14da6b00":"code","29aec4fd":"code","3c19ff3a":"code","b4b2f408":"code","6e50fbb3":"code","e83612f8":"code","66782d7d":"code","b7574706":"code","fceb4e25":"code","b6dac523":"code","3dcb8cb0":"code","2c87c0d0":"code","0a1982e8":"code","bfd7b04c":"code","8c69111a":"code","07ee595e":"code","86c70b5e":"code","fc70703c":"code","3357678f":"code","568642be":"code","7d87d367":"code","68861270":"code","5191b0cb":"code","13c48892":"code","cd84f926":"code","91dd1542":"code","cf42e3f9":"code","ea668af0":"markdown","45c6a941":"markdown","225614ea":"markdown","6350f9f3":"markdown","c903db30":"markdown","a05224af":"markdown","1de5f282":"markdown","cef8035b":"markdown","0032736f":"markdown","41408d5a":"markdown","10054cd9":"markdown","62495312":"markdown","9941c6e1":"markdown","c61bb6ae":"markdown","2f7c3e5a":"markdown","bcddd37d":"markdown","f50ab1de":"markdown","fd1d90c2":"markdown","dbce598a":"markdown","7793b992":"markdown","02195ef9":"markdown","02b85f11":"markdown","c8716152":"markdown","deba4a32":"markdown","dfa0ff80":"markdown","e469425e":"markdown"},"source":{"be523597":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8acd83d1":"import warnings\nwarnings.filterwarnings('ignore')","ba7795ab":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns","cb1cb0ec":"df = pd.read_csv(\"..\/input\/diamonds\/diamonds.csv\")\ndf.head()","d3baed02":"df.isna().sum()","0fe74ee5":"df.shape","df4c4158":"df=df.drop(['Unnamed: 0'],axis=1)","79e05b10":"df.describe()","38468286":"df.drop(df[df['x']==0].index,inplace=True)\ndf.drop(df[df['y']==0].index,inplace=True)\ndf.drop(df[df['z']==0].index,inplace=True)","1abc455e":"df.describe()","3c4e3711":"fig = plt.figure(figsize=(14,6))\nsns.regplot(x=df.price,y=df.carat)\nplt.title('Price vs Carat')\nplt.show()","6ff5e672":"fig = plt.figure(figsize=(14,6))\nsns.regplot(x=df.price,y=df.depth,color='violet')\nplt.title('Price vs Depth')\nplt.show()","f7f25072":"fig = plt.figure(figsize=(14,6))\nsns.regplot(x=df.price,y=df.table,color='orange')\nplt.title('Price vs Table')\nplt.show()","ce83d0d0":"fig = plt.figure(figsize=(10,6))\n\nsns.displot(df.depth,kde=True)\nplt.title('Depth density plot')\nplt.show()","44216575":"fig = plt.figure(figsize=(10,6))\nsns.boxplot(df.depth)\nplt.title('Depth Boxplot')\nplt.show()","966bb30b":"fig = plt.figure(figsize=(10,6))\n\nsns.displot(df.table,kde=True)\nplt.title('Table Depth density plot')\nplt.show()","60a7a808":"fig = plt.figure(figsize=(10,6))\nsns.boxplot(df.table)\nplt.title('Table Boxplot')\nplt.show()","9b22a19d":"fig = plt.figure(figsize=(10,6))\nsns.boxplot(df.x)\nplt.title('x dimension Boxplot')\nplt.show()","00c67d86":"fig = plt.figure(figsize=(10,6))\nsns.boxplot(df.y)\nplt.title('y dimension Boxplot')\nplt.show()","74806bbc":"fig = plt.figure(figsize=(10,6))\nsns.boxplot(df.z)\nplt.title('z dimension Boxplot')\nplt.show()","0d09bcea":"fig = plt.figure(figsize=(14,8))\nsns.boxplot(y=df.price,x=df.cut)\nplt.title(\"Diamond cut vs Price\")\nplt.show()","d5f189e9":"fig = plt.figure(figsize=(14,8))\nsns.boxplot(y=df.price,x=df.color)\nplt.title(\"Diamond color vs Price\")\nplt.show()","e0457b1c":"fig = plt.figure(figsize=(14,8))\nsns.boxplot(y=df.price,x=df.color)\nplt.title(\"Clarity vs Price\")\nplt.show()","8ec11342":"q_hi_depth = df[\"depth\"].quantile(0.99)\nq_low_depth=df[\"depth\"].quantile(0.01)\nq_hi_depth\nq_low_depth","14da6b00":"q_hi_table = df[\"table\"].quantile(0.99)\nq_low_table=df[\"table\"].quantile(0.01)\nq_low_table","29aec4fd":"q_hi_x = df[\"x\"].quantile(0.99)\nq_hi_x","3c19ff3a":"q_hi_y = df[\"y\"].quantile(0.99)\nq_hi_y","b4b2f408":"q_hi_z = df[\"z\"].quantile(0.99)\nq_low_z=df[\"z\"].quantile(0.01)\nq_low_z","6e50fbb3":"df_filtered=df[(df['depth']<q_hi_depth) &  (df['depth']>q_low_depth)]","e83612f8":"df_filtered=df[(df['table']<q_hi_table) &  (df['table']>q_low_table)]\ndf_filtered=df[df['x']<q_hi_x]\ndf_filtered=df[df['y']<q_hi_y]\ndf_filtered=df[(df['z']<q_hi_z) &  (df['z']>q_low_z)]","66782d7d":"df_filtered.shape","b7574706":"pd.plotting.scatter_matrix(df_filtered, figsize=(20, 20));","fceb4e25":"plt.figure(figsize=(14,9))\n\ncorrMatrix=df_filtered.corr()\nsns.heatmap(corrMatrix,annot=True)\nplt.show()\n","b6dac523":"categorical_col=df_filtered.select_dtypes(include=['object'])\ncategorical_col","3dcb8cb0":"df_final=df.apply(LabelEncoder().fit_transform)","2c87c0d0":"df_final.head()","0a1982e8":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nfrom sklearn.preprocessing import MinMaxScaler","bfd7b04c":"X = df_final.drop(['price'], axis=1)\ny = df_final['price']","8c69111a":" X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","07ee595e":"mms=MinMaxScaler()","86c70b5e":"X_train=mms.fit_transform(X_train)\nX_test=mms.transform(X_test)","fc70703c":"X_train","3357678f":"lr=LinearRegression()\nlr.fit(X_train,y_train)\n\n\ny_pred=lr.predict(X_test)\n\naccuracy_lr=cross_val_score(estimator = lr, X = X_train, y = y_train, cv = 8,verbose = 1)\nscore_lr=lr.score(X_test,y_test)\nr2_lr=r2_score(y_test,y_pred)\nmse_lr=mean_squared_error(y_test,y_pred)\n\nprint(f'Accuracy is {accuracy_lr}')\nprint(f'Score is {score_lr}')\nprint(f'r2_Score is {r2_lr}')\nprint(f'Mean Squared error is {mse_lr}')","568642be":"ada_regr = AdaBoostRegressor(random_state=0, n_estimators=100)\nada_regr.fit(X_train,y_train)\n\n\ny_pred=ada_regr.predict(X_test)\n\naccuracy_ada_regr=cross_val_score(estimator = ada_regr, X = X_train, y = y_train, cv = 8,verbose = 1)\nscore_ada_regr=ada_regr.score(X_test,y_test)\nr2_ada_regr=r2_score(y_test,y_pred)\nmse_ada_regr=mean_squared_error(y_test,y_pred)\n\nprint(f'Accuracy is {accuracy_ada_regr}')\nprint(f'Score is {score_ada_regr}')\nprint(f'r2_Score is {r2_ada_regr}')\nprint(f'Mean Squared error is {mse_ada_regr}')","7d87d367":"regr_rf = RandomForestRegressor(max_depth=2, random_state=0)\nregr_rf.fit(X_train,y_train)\n\n\ny_pred=regr_rf.predict(X_test)\n\naccuracy_regr_rf=cross_val_score(estimator = regr_rf, X = X_train, y = y_train, cv = 8,verbose = 1)\nscore_regr_rf=regr_rf.score(X_test,y_test)\nr2_regr_rf=r2_score(y_test,y_pred)\nmse_regr_rf=mean_squared_error(y_test,y_pred)\n\nprint(f'Accuracy is {accuracy_regr_rf}')\nprint(f'Score is {score_regr_rf}')\nprint(f'r2_Score is {r2_regr_rf}')\nprint(f'Mean Squared error is {mse_regr_rf}')","68861270":"kneigh=KNeighborsRegressor(n_neighbors=2)\nkneigh.fit(X_train,y_train)\n\n\ny_pred=kneigh.predict(X_test)\n\naccuracy_kneigh=cross_val_score(estimator = kneigh, X = X_train, y = y_train, cv = 8,verbose = 1)\nscore_kneigh=kneigh.score(X_test,y_test)\nr2_kneigh=r2_score(y_test,y_pred)\nmse_kneigh=mean_squared_error(y_test,y_pred)\n\nprint(f'Accuracy is {accuracy_kneigh}')\nprint(f'Score is {score_kneigh}')\nprint(f'r2_Score is {r2_kneigh}')\nprint(f'Mean Squared error is {mse_kneigh}')","5191b0cb":"gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls',verbose = 1)\n\ngbr.fit(X_train,y_train)\n\n\ny_pred=gbr.predict(X_test)\n\naccuracy_gbr=cross_val_score(estimator = gbr, X = X_train, y = y_train, cv = 8,verbose = 1)\nscore_gbr=gbr.score(X_test,y_test)\nr2_gbr=r2_score(y_test,y_pred)\nmse_gbr=mean_squared_error(y_test,y_pred)\n\nprint(f'Accuracy is {accuracy_gbr}')\nprint(f'Score is {score_gbr}')\nprint(f'r2_Score is {r2_gbr}')\nprint(f'Mean Squared error is {mse_gbr}')","13c48892":"dtree = DecisionTreeRegressor(random_state = 0)\n\ndtree.fit(X_train,y_train)\n\n\ny_pred=dtree.predict(X_test)\n\naccuracy_dtree=cross_val_score(estimator = dtree, X = X_train, y = y_train, cv = 8,verbose = 1)\nscore_dtree=dtree.score(X_test,y_test)\nr2_dtree=r2_score(y_test,y_pred)\nmse_dtree=mean_squared_error(y_test,y_pred)\n\nprint(f'Accuracy is {accuracy_dtree}')\nprint(f'Score is {score_dtree}')\nprint(f'r2_Score is {r2_dtree}')\nprint(f'Mean Squared error is {mse_dtree}')\n ","cd84f926":"models=pd.DataFrame({'Model':['Linear Regression','AdaBoost Regression','RandomForest Regression','KNeighbour Regression','GradientBoosting Regression','Decision Tree Regressor'],\n                     'Score':[r2_lr,r2_ada_regr,r2_regr_rf,r2_kneigh,r2_gbr,r2_dtree]\n                    })","91dd1542":"models","cf42e3f9":"fig = plt.figure(figsize=(15,10))\nplt.plot(models.Model,models.Score, linestyle = 'dashed',marker='H',mfc = 'r',ms = 10)\nplt.xticks(rotation=90)\nplt.show()","ea668af0":"*Please do upvote this notebook if you found it useful.Thank You for your review!*","45c6a941":"**1)Linear Regression**","225614ea":"# Box Plots and Density plots to analyze outliers","6350f9f3":"**Dropping the unnamed columns as it is not of much use**","c903db30":"# Linear Regression Model fit between numerical features and target feature","a05224af":"# Correlation Matrix","1de5f282":"# Determining the quantiles","cef8035b":"# Removing the Outliers with quantiles","0032736f":"**2)AdaBoost Regression**","41408d5a":"**3)Random Forest Regression**","10054cd9":"**Decision Tree Regressor seems to have the best R2 score**","62495312":"# Filtering out the categorical columns","9941c6e1":"**Looking for Missing values in the dataset**","c61bb6ae":"**Dropping rows that have 0 as one of the dimensions**","2f7c3e5a":"# Converting categorical variable to numeric","bcddd37d":"**5)Gradient Boosting Regressor**","f50ab1de":"# Chemistry behind a diamond","fd1d90c2":"# Comparing the performance of different models","dbce598a":"**A diamond is a precious stone consisting of a clear and colorless crystalline form of pure carbon, the hardest naturally occurring substance.**","7793b992":"# Models","02195ef9":"**4)KNeighbours Regressor**","02b85f11":"**6)Decision Tree Regressor**","c8716152":"# Box Plot for categorical variables","deba4a32":"# Splitting the dataset for train\/test","dfa0ff80":"# Few Facts about diamonds:\n1. For ages, diamonds have been an indication of influence, riches and status\n2. Two diamonds of a similar lucidity, weight, and cut can vary in esteem dependent on shading alone\n3. Diamonds come in many sizes, shapes, colors, and with different inward qualities\n4. Even the slightest hint of color can make a dramatic difference in value","e469425e":"![image.png](attachment:e7a03634-0c5c-44b5-966a-54ba9ad47b93.png)"}}