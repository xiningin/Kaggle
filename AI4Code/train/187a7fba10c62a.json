{"cell_type":{"c07a394f":"code","6e511d03":"code","3fc95d6f":"code","a6c7e16e":"code","ff579ea7":"code","7e1a5e84":"code","dc7131db":"code","2f2e4874":"code","76369f65":"code","7d037e29":"code","a6ac34b6":"code","d41253af":"code","6184d920":"code","bc49cb71":"code","8c74574f":"code","80193c98":"code","570d30a0":"code","5b79b1fe":"code","b0585c4c":"code","572e1261":"code","9f41cd4e":"code","3a8467a4":"code","a4f16440":"code","8aec12b5":"code","073a10f0":"code","f0d4f2c6":"code","195ef901":"code","4f25ea5a":"code","6de13137":"code","3749d141":"code","dbad8a2a":"code","89d2ee2d":"code","39afbd70":"code","c8e6c59b":"code","00c3f7f9":"code","7af8f6e9":"code","a23a1fe8":"code","576cb893":"code","82d98ca4":"code","790ff369":"code","a87bd3af":"code","ee2c03d1":"code","c403d8b0":"code","cb836e05":"code","dd4ddcce":"code","53ffb342":"code","7d2ad1f6":"code","d73bc7fa":"code","8bccd391":"code","35b3e19d":"code","5a227c68":"code","b986ad22":"code","00eb5671":"markdown","479aedcb":"markdown","e207a35d":"markdown","799902b2":"markdown","d0ddc95f":"markdown","fbb10531":"markdown","5f49f4ed":"markdown","2186f98a":"markdown","c1e5007d":"markdown","e7bb3704":"markdown","615d8cc0":"markdown","f5d216ef":"markdown","6960aff3":"markdown","358cdcbf":"markdown","3f1cbdea":"markdown","56d656f5":"markdown","919e8456":"markdown","f60b0949":"markdown","3238527c":"markdown","2367d588":"markdown","9de2ddec":"markdown","9414b4e6":"markdown","4407a2b8":"markdown","4d59c42a":"markdown","39b4cfc3":"markdown","774140e4":"markdown","2f1ea4df":"markdown","d94c3d10":"markdown","42102b98":"markdown","3e8c6c1c":"markdown","3af33a52":"markdown","c897ce25":"markdown","5e61f87f":"markdown","c08f8275":"markdown","0a05de73":"markdown","25af8388":"markdown","19aabf0b":"markdown","f2bc5b21":"markdown","85a99da6":"markdown","56b5043b":"markdown","cf9de454":"markdown","6274ebcf":"markdown","186fab1a":"markdown","f8564786":"markdown","6e2dfbec":"markdown","2cf9215e":"markdown","66830002":"markdown","2a606af7":"markdown","da449275":"markdown","7f675b27":"markdown","33b8415a":"markdown","d4b1b442":"markdown","4f475ca4":"markdown","a3207efc":"markdown","055830bd":"markdown"},"source":{"c07a394f":"import numpy as np\nimport pandas as pd\nimport os\nimport string\nimport re\nimport warnings \nwarnings.filterwarnings('ignore')\n\n#plotting libraries!\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom shapely.geometry import Point\nimport geopandas as gpd\nfrom geopandas import GeoDataFrame\n%matplotlib inline\n\n\n#PLOTLY\nimport plotly\nimport plotly.plotly as py\nimport plotly.offline as offline\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport cufflinks as cf\nfrom collections import defaultdict\nfrom plotly import tools\nfrom plotly.graph_objs import Scatter, Figure, Layout\ncf.set_config_file(offline=True)\nfrom textblob import TextBlob\nfrom nltk.corpus import stopwords\neng_stopwords = set(stopwords.words(\"english\"))\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pyLDAvis.sklearn\nfrom pylab import bone, pcolor, colorbar, plot, show, rcParams, savefig\nimport squarify\n\nprint(os.listdir('..\/input'))","6e511d03":"twitter_data = pd.read_csv('..\/input\/auspol2019.csv',parse_dates=['created_at','user_created_at'])\ngeo_data = pd.read_csv('..\/input\/location_geocode.csv')","3fc95d6f":"twitter_data.head()","a6c7e16e":"geo_data.head()","ff579ea7":"twitter_data.shape","7e1a5e84":"geo_data.shape","dc7131db":"#merging two data frames based on user location\ntwitter_data = twitter_data.merge(geo_data, how='inner', left_on='user_location', right_on='name')","2f2e4874":"twitter_data.head()","76369f65":"twitter_data = twitter_data.drop('name',axis =1)","7d037e29":"#lets check for null values\ntwitter_data.isnull().mean()*100","a6ac34b6":"print(f\" Data Available since {twitter_data.created_at.min()}\")\nprint(f\" Data Available upto {twitter_data.created_at.max()}\")","d41253af":"#lets check latest and oldest twitter members in the dataframe\nprint(f\" Data Available since {twitter_data.user_created_at.min()}\")\nprint(f\" Data Available upto {twitter_data.user_created_at.max()}\")","6184d920":"print('The oldest user in the data was',twitter_data.loc[twitter_data['user_created_at'] == '2006-03-21 21:04:12', 'user_name'].values)","bc49cb71":"print('The newest user in the data was',twitter_data.loc[twitter_data['user_created_at'] == '2019-05-19 10:49:59', 'user_name'].values)","8c74574f":"#lets explore created_at column\ntwitter_data['created_at'] =  pd.to_datetime(twitter_data['created_at'])\ncnt_srs = twitter_data['created_at'].dt.date.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\nplt.xticks(rotation='vertical')\nplt.xlabel('Date', fontsize=12)\nplt.ylabel('Number of tweets', fontsize=12)\nplt.title(\"Number of tweets according to dates\")\nplt.show()","80193c98":"#lets explore user_created_at column\ncount_  = twitter_data['user_created_at'].dt.date.value_counts()\ncount_ = count_[:10,]\nplt.figure(figsize=(10,5))\nsns.barplot(count_.index, count_.values, alpha=0.8)\nplt.title('Most accounts created according to date')\nplt.xticks(rotation='vertical')\nplt.ylabel('Number of accounts', fontsize=12)\nplt.xlabel('Date', fontsize=12)\nplt.show()","570d30a0":"#lets derive some columns from date colums\ntwitter_data['tweeted_day_of_week'] = twitter_data['created_at'].dt.weekday_name\ntwitter_data['created_day_of_week'] = twitter_data['user_created_at'].dt.weekday_name","5b79b1fe":"cnt_ = twitter_data['tweeted_day_of_week'].value_counts()\ncnt_ = cnt_.sort_index() \nfig = {\n  \"data\": [\n    {\n      \"values\": cnt_.values,\n      \"labels\": cnt_.index,\n      \"domain\": {\"x\": [0, .5]},\n      \"name\": \"Number of tweets per day\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .3,\n      \"type\": \"pie\"\n    },],\n  \"layout\": {\n        \"title\":\"Percentage of tweets per days of the week\",\n        \"annotations\": [\n            { \"font\": { \"size\": 20},\n              \"showarrow\": False,\n             \"text\": \"Percentage of Tweets according to days of the week\",\n                \"x\": 0.50,\n                \"y\": 1\n            },\n        ]\n    }\n}\niplot(fig)\ncnt_","b0585c4c":"\nx = 0.\ny = 0.\nwidth = 50.\nheight = 50.\ntype_list = list(twitter_data['tweeted_day_of_week'].unique())\nvalues = [len(twitter_data[twitter_data['tweeted_day_of_week'] == i]) for i in type_list]\n\nnormed = squarify.normalize_sizes(values, width, height)\nrects = squarify.squarify(normed, x, y, width, height)\n\ncolor_brewer = ['#2D3142','#4F5D75','#BFC0C0','#F2D7EE','#EF8354','#839788','#EEE0CB']\nshapes = []\nannotations = []\ncounter = 0\n\nfor r in rects:\n    shapes.append( \n        dict(\n            type = 'rect', \n            x0 = r['x'], \n            y0 = r['y'], \n            x1 = r['x']+r['dx'], \n            y1 = r['y']+r['dy'],\n            line = dict( width = 2 ),\n            fillcolor = color_brewer[counter]\n        ) \n    )\n    annotations.append(\n        dict(\n            x = r['x']+(r['dx']\/2),\n            y = r['y']+(r['dy']\/2),\n            text = \"{}-{}\".format(type_list[counter], values[counter]),\n            showarrow = False\n        )\n    )\n    counter = counter + 1\n    if counter >= len(color_brewer):\n        counter = 0\n\n# For hover text\ntrace0 = go.Scatter(\n    x = [ r['x']+(r['dx']\/2) for r in rects ], \n    y = [ r['y']+(r['dy']\/2) for r in rects ],\n    text = [ str(v) for v in values ], \n    mode = 'text',\n)\n        \nlayout = dict(\n    height=700, \n    width=700,\n    xaxis=dict(showgrid=False,zeroline=False),\n    yaxis=dict(showgrid=False,zeroline=False),\n    shapes=shapes,\n    annotations=annotations,\n    hovermode='closest',\n    font=dict(color=\"#FFFFFF\")\n)\n\n# With hovertext\nfigure = dict(data=[trace0], layout=layout)\niplot(figure, filename='squarify-treemap')","572e1261":"cnt_ = twitter_data['created_day_of_week'].value_counts()\ncnt_ = cnt_.sort_index() \nfig = {\n  \"data\": [\n    {\n      \"values\": cnt_.values,\n      \"labels\": cnt_.index,\n      \"domain\": {\"x\": [0, .5]},\n      \"name\": \"Number of tweets per day\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .3,\n      \"type\": \"pie\"\n    },],\n  \"layout\": {\n        \"title\":\"Percentage of created accounts per day\",\n        \"annotations\": [\n            { \"font\": { \"size\": 20},\n              \"showarrow\": False,\n             \"text\": \"Percentage of accounts created according to days of the week\",\n                \"x\": 0.50,\n                \"y\": 1\n            },\n        ]\n    }\n}\niplot(fig)\ncnt_","9f41cd4e":"x = 0.\ny = 0.\nwidth = 50.\nheight = 50.\ntype_list = list(twitter_data['created_day_of_week'].unique())\nvalues = [len(twitter_data[twitter_data['created_day_of_week'] == i]) for i in type_list]\n\nnormed = squarify.normalize_sizes(values, width, height)\nrects = squarify.squarify(normed, x, y, width, height)\n\ncolor_brewer = ['#99B2DD','#F9DEC9','#3A405A','#494949','#FF5D73','#7C7A7A']\nshapes = []\nannotations = []\ncounter = 0\n\nfor r in rects:\n    shapes.append( \n        dict(\n            type = 'rect', \n            x0 = r['x'], \n            y0 = r['y'], \n            x1 = r['x']+r['dx'], \n            y1 = r['y']+r['dy'],\n            line = dict( width = 2 ),\n            fillcolor = color_brewer[counter]\n        ) \n    )\n    annotations.append(\n        dict(\n            x = r['x']+(r['dx']\/2),\n            y = r['y']+(r['dy']\/2),\n            text = \"{}-{}\".format(type_list[counter], values[counter]),\n            showarrow = False\n        )\n    )\n    counter = counter + 1\n    if counter >= len(color_brewer):\n        counter = 0\n\n# For hover text\ntrace0 = go.Scatter(\n    x = [ r['x']+(r['dx']\/2) for r in rects ], \n    y = [ r['y']+(r['dy']\/2) for r in rects ],\n    text = [ str(v) for v in values ], \n    mode = 'text',\n)\n        \nlayout = dict(\n    height=700, \n    width=700,\n    xaxis=dict(showgrid=False,zeroline=False),\n    yaxis=dict(showgrid=False,zeroline=False),\n    shapes=shapes,\n    annotations=annotations,\n    hovermode='closest',\n    font=dict(color=\"#FFFFFF\")\n)\n\n# With hovertext\nfigure = dict(data=[trace0], layout=layout)\niplot(figure, filename='squarify-tree')","3a8467a4":"#lets extract the hours from the created_at and user_created_at column\ntwitter_data['created_at_hour'] = twitter_data['created_at'].dt.hour\ntwitter_data['user_created_at_hour'] = twitter_data['user_created_at'].dt.hour","a4f16440":"cnt_ = twitter_data['created_at_hour'].value_counts()\ncnt_ = cnt_.sort_index() \ntrace1 = go.Scatter(\n                    x = cnt_.index,\n                    y = cnt_.values,\n                    mode = \"lines\",\n                    name = \"citations\",\n                    marker = dict(color = 'rgba(16, 112, 2, 0.8)')\n                    )\n\ndata = [trace1]\nlayout = dict(title = 'Number of tweets per hour',\n              xaxis= dict(title= 'Tweets per hour',ticklen= 5,zeroline= False)\n             )\nfig = dict(data = data, layout = layout)\niplot(fig)","8aec12b5":"cnt_ = twitter_data['user_created_at_hour'].value_counts()\ncnt_ = cnt_.sort_index() \ntrace1 = go.Scatter(\n                    x = cnt_.index,\n                    y = cnt_.values,\n                    mode = \"lines\",\n                    name = \"citations\",\n                    marker = dict(color = 'rgba(210, 113, 25, 0.8)')\n                    )\n\ndata = [trace1]\nlayout = dict(title = 'Number of Accounts Created per hour ',\n              xaxis= dict(title= 'Accounts per hour',ticklen= 5,zeroline= False)\n             )\nfig = dict(data = data, layout = layout)\niplot(fig)","073a10f0":"#most favourite and retweeted tweet\nprint(f\" Maximum number of retweets {twitter_data.retweet_count.max()}\")\nprint(f\" Maximum number of favorites {twitter_data.favorite_count.max()}\")","f0d4f2c6":"#lets see the tweet which has the maximum retweet count\ntwitter_data.loc[twitter_data['retweet_count']==6622.0,'full_text'].values","195ef901":"twitter_data.loc[twitter_data['favorite_count']==15559.0,['full_text','user_name','user_description']].values","4f25ea5a":"#most number of occurances of a person\ntwitter_data.user_name.value_counts()[:5,]","6de13137":"#wordcloud\n\nwordcloud__ = WordCloud(\n                          background_color='white',\n                          stopwords=set(STOPWORDS),\n                          max_words=250,\n                          max_font_size=40, \n                          random_state=1705\n                         ).generate(str(twitter_data['user_screen_name'].dropna()))\ndef cloud_plot(wordcloud):\n    fig = plt.figure(1, figsize=(20,15))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\ncloud_plot(wordcloud__)","3749d141":"#wordcloud\nwordcloud_ = WordCloud(\n                          background_color='black',\n                          stopwords=set(STOPWORDS),\n                          max_words=250,\n                          max_font_size=40, \n                          random_state=1705\n                         ).generate(str(twitter_data['user_description'].dropna()))\ndef cloud_plot(wordcloud):\n    fig = plt.figure(1, figsize=(20,15))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\ncloud_plot(wordcloud_)","dbad8a2a":"twitter_data['sentiment'] = twitter_data['full_text'].map(lambda text: TextBlob(text).sentiment.polarity)","89d2ee2d":"print(\"5 random tweets with highest positive sentiment polarity: \\n\")\ncL = twitter_data.loc[twitter_data.sentiment==1, ['full_text']].sample(5).values\nfor c in cL:\n    print(c[0])\n    print()","39afbd70":"print(\"5 random tweets with highest nagative sentiment polarity: \\n\")\ncL = twitter_data.loc[twitter_data.sentiment==-1, ['full_text']].sample(5).values\nfor c in cL:\n    print(c[0])\n    print()","c8e6c59b":"print(\"5 random tweets with neutral sentiment polarity: \\n\")\ncL = twitter_data.loc[twitter_data.sentiment==0, ['full_text']].sample(5).values\nfor c in cL:\n    print(c[0])\n    print()","00c3f7f9":"trace1 = go.Histogram(\n    x = twitter_data['sentiment'],\n    opacity=0.75,\n    name = \"Sentiment\",\n    marker=dict(color='rgba(122, 75, 196, 0.6)'))\n\ndata = [trace1]\nlayout = go.Layout(barmode='overlay',\n                   title='Histogram plot of sentiment',\n                   xaxis=dict(title='Sentiment'),\n                   yaxis=dict( title='Count'),\n)\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","7af8f6e9":"cut = pd.cut(\n    twitter_data['sentiment'],\n    [-np.inf, -.01, .01, np.inf],\n    labels=['negative', 'neutral', 'positive']\n)\ntwitter_data['polarity'] = cut.values\ntwitter_data[['polarity','sentiment']][:20]","a23a1fe8":"twitter_data['polarity'].value_counts()","576cb893":"data = [go.Scatterpolar(\n  r = [twitter_data['polarity'].value_counts()[0],twitter_data['polarity'].value_counts()[1],twitter_data['polarity'].value_counts()[2]],\n  theta = list(twitter_data['polarity'].unique()),\n  fill = 'toself'\n)]\n\nlayout = go.Layout(\n  polar = dict(\n    radialaxis = dict(\n      visible = True,\n      range = [0, 60000]\n    )\n  ),\n  showlegend = False,\n  title ='Radar chart of polarities'\n)\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename = \"Single Pokemon stats\")","82d98ca4":"twitter_data['count_sent']=twitter_data[\"full_text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n#Word count in each comment:\ntwitter_data['count_word']=twitter_data[\"full_text\"].apply(lambda x: len(str(x).split()))\n#Unique word count\ntwitter_data['count_unique_word']=twitter_data[\"full_text\"].apply(lambda x: len(set(str(x).split())))\n#Letter count\ntwitter_data['count_letters']=twitter_data[\"full_text\"].apply(lambda x: len(str(x)))\n#punctuation count\ntwitter_data[\"count_punctuations\"] =twitter_data[\"full_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n#upper case words count\ntwitter_data[\"count_words_upper\"] = twitter_data[\"full_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n#title case words count\ntwitter_data[\"count_words_title\"] = twitter_data[\"full_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n#Number of stopwords\ntwitter_data[\"count_stopwords\"] = twitter_data[\"full_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n#Average length of the words\ntwitter_data[\"mean_word_len\"] = twitter_data[\"full_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","790ff369":"twitter_data.describe().T","a87bd3af":"sample_df = twitter_data[['count_sent','count_word','count_unique_word','count_letters','count_punctuations','count_words_upper','count_words_title','count_stopwords','mean_word_len' ]]\nsns.pairplot(sample_df,palette=\"husl\")\ndel sample_df","ee2c03d1":"\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n\nfreq_dict = defaultdict(int)\nfor sent in twitter_data[\"full_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n\nfig = tools.make_subplots(rows=1, cols=1, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words\"\n                                          ])\nfig.append_trace(trace0, 1, 1)\n\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\niplot(fig, filename='word-plots.html')\n","c403d8b0":"freq_dict = defaultdict(int)\nfor sent in twitter_data[\"full_text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\nfig = tools.make_subplots(rows=1, cols=1, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent bigrams\"\n                                          ])\nfig.append_trace(trace0, 1, 1)\nfig['layout'].update(height=1200, width=1000, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\niplot(fig, filename='word-plots')","cb836e05":"freq_dict = defaultdict(int)\nplotly.tools.set_credentials_file(username='Ratan2513', api_key='Pxh1iyluGYYjRbo13n0G')\nfor sent in twitter_data[\"full_text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'green')\n\nfig = tools.make_subplots(rows=1, cols=1, vertical_spacing=0.04, horizontal_spacing=0.2,\n                          subplot_titles=[\"Frequent trigrams\", \n                                          ])\nfig.append_trace(trace0, 1, 1)\nfig['layout'].update(height=1200, width=1500, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\npy.iplot(fig, filename='word-plots')","dd4ddcce":"cnt_ = twitter_data['user_location'].value_counts()\ncnt_.reset_index()\ncnt_ = cnt_[:20,]\ntrace1 = go.Bar(\n                x = cnt_.index,\n                y = cnt_.values,\n                name = \"Number of tweets on Australia polls by state.\",\n                marker = dict(color = 'rgba(200, 74, 55, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\n\ndata = [trace1]\nlayout = go.Layout(barmode = \"group\",title = 'Number of tweets on Australia polls by state.')\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","53ffb342":"data = [go.Scattermapbox(\n            lat= twitter_data['lat'] ,\n            lon= twitter_data['long'],\n            mode='markers',\n            marker=dict(\n                size= 4,\n                color = 'orange',\n                opacity = .8,\n            ),\n          )]\nlayout = go.Layout(\n    title = go.layout.Title(\n        text = 'Tweets on Australia polls by state'\n    ),\n    geo = go.layout.Geo(\n        scope = 'world',\n        projection = go.layout.geo.Projection(type = 'albers usa'),\n        showlakes = True,\n        lakecolor = 'rgb(255, 255, 255)'),\n)\n\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig, filename = 'd3-cloropleth')","7d2ad1f6":"trace1 = go.Scattermapbox(\n            lat= twitter_data.loc[twitter_data['polarity'] == 'negative','lat'] ,\n            lon= twitter_data.loc[twitter_data['polarity'] == 'negative','long'],\n            mode='markers',\n            marker=dict(\n                size= 4,\n                color = 'black',\n                opacity = .5,\n            ),\n          )\ntrace2= go.Scattermapbox(\n            lat= twitter_data.loc[twitter_data['polarity'] == 'neutral','lat'] ,\n            lon= twitter_data.loc[twitter_data['polarity'] == 'neutral','long'],\n            mode='markers',\n            marker=dict(\n                size= 4,\n                color = 'blue',\n                opacity = .3,\n            ),\n          )\ntrace3= go.Scattermapbox(\n            lat= twitter_data.loc[twitter_data['polarity'] == 'positive','lat'] ,\n            lon= twitter_data.loc[twitter_data['polarity'] == 'positive','long'],\n            mode='markers',\n            marker=dict(\n                size= 4,\n                color = 'gold',\n                opacity = .2,\n            ),\n          )\n\n\ndata = [trace1,trace2,trace3]\nlayout = go.Layout(\n    title = go.layout.Title(\n        text = 'Tweets on Australia polls according to polarity by state '\n    ),\n    geo = go.layout.Geo(\n        scope = 'world',\n        projection = go.layout.geo.Projection(type = 'albers usa'),\n        showlakes = True,\n        lakecolor = 'rgb(200, 125, 255)'),\n)\n\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig, filename = 'd3-cloropleth-ma')","d73bc7fa":"vectorizer_ = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\ntweets_vectorized = vectorizer_.fit_transform(twitter_data['full_text'])","8bccd391":"lda_ = LatentDirichletAllocation(n_components=10, max_iter=5, learning_method='online',verbose=True)\ntweets_lda = lda_.fit_transform(tweets_vectorized)","35b3e19d":"def selected_topics(model, vectorizer, top_n=10):\n    for idx, topic in enumerate(model.components_):\n        print(\"Topic %d:\" % (idx))\n        print([(vectorizer.get_feature_names()[i], topic[i])\n                        for i in topic.argsort()[:-top_n - 1:-1]]) ","5a227c68":"print(\"Tweets LDA Model:\")\nselected_topics(lda_, vectorizer_)","b986ad22":"pyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda_, tweets_vectorized, vectorizer_, mds='tsne')\ndash","00eb5671":"### Getting the newest member name in the data","479aedcb":"## Plotting the number of accounts created by days of the week.","e207a35d":"## Plotting number of accounts created per hour","799902b2":"**Thanks for reading this. I welcome suggestions to improve this kernel further.**","d0ddc95f":"### Getting the oldest and newest twitter member in the dataframe","fbb10531":"## Merging the two dataframes on user_location","5f49f4ed":"## Applying Latent Dirichlet Allocation models","2186f98a":"## Radar chart of polarities","c1e5007d":"## Topic modeling\n**Topic modeling is a type of statistical modeling for discovering the abstract \u201ctopics\u201d that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.**","e7bb3704":"## Checking the head of the dataframe","615d8cc0":"**user description has null values in it.**","f5d216ef":"## Checking most retweeted and favourite tweet","6960aff3":"## Checking different tweets with different ploarities","358cdcbf":"# Exploratory Data Analysis of tweets on Australian Election","3f1cbdea":"## Plotting number of tweets by state(Top 20)","56d656f5":"### Getting the oldest member name in the data.","919e8456":"## Extracting hours from the date columns","f60b0949":"Only 10 days data was available.","3238527c":"most of the tweets are with zero polarity.","2367d588":"There are 11153 rows and 3 columns","9de2ddec":"## Printing keywords","9414b4e6":"We can see most of the tweets was on saturdays and sundays.","4407a2b8":"## Treemap of number of accounts created by days of the week.","4d59c42a":"## Word cloud of user description","39b4cfc3":"## Number of tweets according to Dates","774140e4":"## Importing the dataframes","2f1ea4df":"## Dates on which the most accounts was created (Top10)","d94c3d10":"### Plotting the number of tweets by days of the week.","42102b98":"## Histogram plot of sentiment","3e8c6c1c":"## Ngrams Visualisations of tweets","3af33a52":"## Pairplot of text based features","c897ce25":"## Getting the sentiment of the tweets","5e61f87f":"## Wordcloud of user screen name","c08f8275":"### Getting the name of the day from the dates.","0a05de73":"## Mapping sentiment to polarities(positive,neutral,negative)","25af8388":"Most of the accounts created at 2AM.","19aabf0b":"## Importing libraries","f2bc5b21":"### Getting the minimum and maximum dates in dataframe","85a99da6":"Most members tweeted around 10-11AM.","56b5043b":"The more tweets was on 2019-05-18.","cf9de454":"We can see the most accounts was created on 2011-09-14.","6274ebcf":"## Visualizing number of tweets by state","186fab1a":"#### Table of contents:\n1. [Importing libraries](#import)\n2. [Imporing dataframes](#data)\n3. [Checking the head and shape of the dataframes](#heads)\n4. [Merging the two dataframes](#merge)\n5. [Checking for null values](#nulls)\n6. [Getting the oldest and newest members in the dataframe](#old)\n7. [Plotting number of tweets according to dates](#no)\n8. [Plotting dates on which most accounts are created](#most)\n9. [Getting the name of the day in the week](#week)<br>\n      a. [Plotting the number of tweets by days of the week](#plot)<br>\n      b. [Treemap of number of tweets by days of the week](#treemap1)<br>\n      c. [Plotting the number of accounts created by days of the wee.](#plot2)<br>\n      d. [Treemap of number of accounts created by days of the week](#treemap2)<br>\n10. [Extracting hours from the date columns](#hour)<br>\n      a. [Plotting number of tweets per hour](#hours1)<br>\n      b. [Plotting number of accounts created per hour](#hours2)<br>\n11. [Checking most retweeted and favourite tweet](#re)\n12. [Wordclouds](#cloud)<br>\n      a. [Wordcloud of user screen name](#cloud1)<br>\n      b. [Wordcloud of user description](#cloud2)<br>\n13. [Getting the sentiment of the tweets](#sentiment)\n14. [Checking different tweets with different ploarities](#printing)\n15. [Histogram plot of sentiment](#histse)\n16. [Radar chart of polarities](#radar)\n17. [Extracting features from tweets](#ext)<br>\n      a. [Pairplot of text based features](#pair)\n18. [Ngrams Visualisations of tweets](#n)\n19. [Plotting number of tweets by state(Top 20)](#tweetsbystate)\n20. [Visualizing number of tweets by state](#state)\n21. [Tweets on Australia polls according to polarity by state](#bysate)\n22. [Topic modeling](#model)<br>\n       a. [Count Vectorizers for the data](#count)<br>\n       b. [Applying Latent Dirichlet Allocation models](#lda)<br>\n       c. [Visualizing LDA results of tweets with pyLDAvis](#py)<br>\n    ","f8564786":"## Extracting features from tweets\n1. count of sentences\n2. count of words\n3. count of unique words\n4. count of letters\n5. count of punctuations\n6. count of uppercase words\/letters\n7. count of stop words\n8. Avg length of each word","6e2dfbec":"## Checking the shapes of the dataframes","2cf9215e":"The most retweeted and favorite tweet was same and was tweeted by Sara A. Carter.","66830002":"## Plotting number of tweets per hour","2a606af7":"## Checking for nulls","da449275":"There are 183379 rows and 11 features ","7f675b27":"## Treemap of number of tweets by days of the week","33b8415a":"## Visualizing LDA results of tweets with pyLDAvis","d4b1b442":"## Count Vectorizers for the data","4f475ca4":"## Tweets on Australia polls according to polarity by state ","a3207efc":"## Checking the head of the dataframes","055830bd":"The dataset comprises of over 180,000 tweets on australian elections between the time 10.05.2019 and 20.05.2019.The dataset columns are \n* created_at: Date and time of tweet creation\n* id: Unique ID of the tweet\n* full_text: Full tweet text\n* retweet_count: Number of retweets\n* favorite_count: Number of likes\n* user_id: User ID of tweet creator\n* user_name: Username of tweet creator\n* user_screen_name: Screen name of tweet creator\n* user_description: Description on tweet creator's profile\n* user_location: Location given on tweet creator's profile\n* user_location: Location given on tweet creator's profile <br>\n\nAnd loacation_geocode.csv contains latitude and longitude of the user."}}