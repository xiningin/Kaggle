{"cell_type":{"271e3d19":"code","9caea839":"code","48749858":"code","94c69231":"code","532e4711":"code","c0df84be":"code","0ff26f81":"code","3e2d74f5":"code","85c80625":"code","715fa88a":"code","d848d3d7":"code","f0130fbe":"code","68b39dff":"code","50964425":"code","453d9f5c":"code","767745c2":"code","1ab9f27a":"code","fc1e143a":"code","7d91efee":"code","37036909":"code","1dc9a49d":"code","0068f864":"code","1912c1d2":"code","6934c834":"code","5f0f2f8a":"code","b047c8ec":"code","b35357b3":"code","52123904":"code","05271a3a":"code","0eb16eec":"code","a4a3f0fc":"code","220323ba":"code","e777c508":"code","396b74c5":"code","68b330c8":"code","6953bd8d":"code","6056d13a":"code","9dfdc4d5":"code","b3a8fcf0":"code","adc3c623":"code","ead54382":"code","8ab7805b":"code","033f25be":"code","72064c7a":"code","251bc422":"code","92c1bba9":"code","1aa200f5":"code","8c4b1b11":"code","cdbe26e8":"code","54267d76":"code","6c91b922":"code","9fd1d988":"code","85d7a0ee":"code","5be794d1":"code","d1cf3e87":"code","b886cae6":"code","728b5e0b":"code","c979dab4":"code","d323d88b":"code","46d35b65":"code","98d473f7":"code","249110d0":"code","199ba01e":"code","24e3fcfa":"code","1afbb7ed":"code","51a79aef":"code","b0bf8edd":"code","0ea4873b":"code","677a6f06":"code","34965277":"code","82140e5d":"code","73f78bff":"code","9b601dd1":"code","f52abc91":"code","ea559878":"code","59912c27":"code","1f10dd80":"code","571b2c3a":"markdown","c65a1449":"markdown","90a5f7a8":"markdown","e8067f80":"markdown","05520319":"markdown","5d4481b1":"markdown","00ba1398":"markdown","7371de6b":"markdown","a657d5a7":"markdown","f139f343":"markdown","54e46c6f":"markdown","2d5fba7f":"markdown","138f95e3":"markdown","452483de":"markdown","c440b0f2":"markdown","9d8bae2d":"markdown","1fc3af20":"markdown","665e768e":"markdown","5b7d2670":"markdown","371587fd":"markdown","230ab73f":"markdown","dc40f285":"markdown","0ebbe2fa":"markdown","0d62589b":"markdown","fd6bb9f2":"markdown","29f36350":"markdown","e4da0c0c":"markdown","11c09b76":"markdown","77e425b9":"markdown","84589bf2":"markdown","4907e9f3":"markdown","f77489e3":"markdown","270603ff":"markdown","a0963b06":"markdown","9e8ea0d0":"markdown","2103e58d":"markdown","a9d9c619":"markdown","c6d3a426":"markdown","c46f39e4":"markdown","d10a458a":"markdown","23cdbb68":"markdown","05538aab":"markdown","552a1170":"markdown","728d25f1":"markdown","93dbd17c":"markdown","0bc251ff":"markdown","780774f2":"markdown","f650e062":"markdown","393edd7f":"markdown","63bf219d":"markdown","2319396d":"markdown","9c3ca40c":"markdown","c719e540":"markdown","fa498830":"markdown","66807b86":"markdown","c64dfcfa":"markdown","f133fc9a":"markdown","86665797":"markdown","73f18c58":"markdown","bda48e6d":"markdown","60956d01":"markdown","271e61fa":"markdown","c1dfa20d":"markdown","d01d60d5":"markdown","ffe78f1e":"markdown","02b34cac":"markdown","c914ea91":"markdown","69d79912":"markdown","1a295d23":"markdown","8e950623":"markdown","0cd22931":"markdown","d3baee2b":"markdown"},"source":{"271e3d19":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import time\nimport datetime\nfrom sklearn.metrics import roc_curve\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv('..\/input\/telecom_churn_data.csv')","9caea839":"# Function to Return Monthwise ColumnsList. Returns arrays of columns belonging to 6,7,8,9 month separately.\n# Also returns an array of columns that are not month specific as common columns.\ndef returnColumnsByMonth(df):\n    column_Month_6 = []\n    column_Month_7 = []\n    column_Month_8 = []\n    column_Month_9 = []\n    column_Common = []\n    for eachColumns in df.columns:\n        if((eachColumns.find(\"_6\") >=0) | (eachColumns.find(\"jun_\") >=0)):\n            column_Month_6.append(eachColumns)\n        elif((eachColumns.find(\"_7\") >=0) | (eachColumns.find(\"jul_\") >=0)):\n            column_Month_7.append(eachColumns)\n        elif((eachColumns.find(\"_8\") >= 0) | (eachColumns.find(\"aug_\") >=0)):\n            column_Month_8.append(eachColumns)\n        elif((eachColumns.find(\"_9\") >=0) | (eachColumns.find(\"sep_\") >=0)):\n            column_Month_9.append(eachColumns)\n        else:\n            column_Common.append(eachColumns)\n    return column_Month_6, column_Month_7, column_Month_8, column_Month_9, column_Common\n\n# Function to Get Columns Based on Null %. \n#Returns columns that have % of null values higher or lower than nullPercentLimit\ndef getColumnsBasedOnNullPercent(df, nullPercentLimit, limitType = 'Upper'):\n    col2NullPercent_df = pd.DataFrame(round((df.isnull().sum()\/len(df.index))* 100, 2), columns=['NullPercent'])\n    col2NullPercent_df = pd.DataFrame(round((df.isnull().sum()\/len(df.index))* 100, 2), columns=['NullPercent'])\n    if(limitType == 'Upper'):\n        columnsList = np.array(col2NullPercent_df.apply(lambda x: x['NullPercent'] > nullPercentLimit , axis=1))\n    if(limitType == 'Lower'):\n        columnsList = np.array(col2NullPercent_df.apply(lambda x: ((x['NullPercent'] < nullPercentLimit) & (x['NullPercent'] > 0)) , axis=1))\n    return np.array(df.loc[:, columnsList].columns)\n\n# Function to get Days Since Last Recharge for 6\/7\/8\/9 months\ndef daysSinceLastRechargeMonthwise(df, month):\n    if(month == 6):\n        return pd.to_datetime(df['last_date_of_month_6']) - pd.to_datetime(df['date_of_last_rech_6'])\n    elif(month == 7):\n        return pd.to_datetime(df['last_date_of_month_7']) - pd.to_datetime(df['date_of_last_rech_7'])\n    elif(month == 8):\n        return pd.to_datetime(df['last_date_of_month_8']) - pd.to_datetime(df['date_of_last_rech_8'])\n    elif(month == 9):\n        return pd.to_datetime(df['last_date_of_month_9']) - pd.to_datetime(df['date_of_last_rech_9'])\n\ndef plotCategoricalChurn_NotChurn(df, columnsList, flag = 0):\n    for eachMonth in columnsList:\n    #flag = 1        \n    #eachMonth = \"days_from_LastRechage_6\"\n        col = eachMonth\n        X1 = df.groupby('churn')[col].agg(['mean']).reset_index()\n        X1.rename(columns={'mean':col}, inplace=True)\n        if(flag == 1):\n            seventhMonth = eachMonth[:-1] + \"7\"\n            X2 = df.groupby('churn')[seventhMonth].agg(['mean']).reset_index()    \n            X2.rename(columns={'mean':seventhMonth}, inplace=True)\n            X2 = pd.merge(X1,X2, on = ['churn'])\n            newCol = eachMonth[:-1] + \"goodPeriod_Avg\"\n            print(newCol)\n            X2[newCol] = (X2[eachMonth] + X2[seventhMonth])\/2\n            p = sns.barplot(x='churn', y=newCol, data=X2)\n            p.set_xticklabels(['Not-Churn','churn'], fontsize= 12)\n            plt.ylabel(newCol,fontsize = 12)\n            plt.xlabel('churn', fontsize = 12)\n            plt.show()\n            X2.head()\n\n        else:\n            print(eachMonth)\n            p = sns.barplot(x='churn', y=col, data=X1)\n            p.set_xticklabels(['Not-Churn','churn'], fontsize= 12)\n            plt.ylabel(col,fontsize = 12)\n            plt.xlabel('churn', fontsize = 12)\n            plt.show()\n            X1.head()\n\n#Function to Show Howmuch % usage done for Churn Subscriber with respect to total usage on that month, for tht particular feature\n# e.g: arpu (Averag Revenue Per User) ==> How much is the average arpu for churn and non-churn subscriber in month 6 7 and 8.\n# Then check for churn subscriber how much % usage on total mean Usage in each month\ndef churnSubscriberUsageChangePercentage():\n    for count, eachFeature in enumerate(column_Month_6):\n        col = eachFeature\n        X1 = df.groupby(['churn'])[col].agg(['mean']).reset_index()\n        X1.rename(columns={'mean': \"mean_\"+col}, inplace=True)\n        if(col == 'jun_vbc_3g'):\n            col = 'jul_vbc_3g'\n        else:\n            col = col[:-1] + \"7\"\n        X2 = df.groupby(['churn'])[col].agg(['mean']).reset_index()\n        X2.rename(columns={'mean': \"mean_\"+col}, inplace=True)\n        if(col == 'jul_vbc_3g'):\n            col = 'aug_vbc_3g'\n        else:\n            col = col[:-1] + \"8\"\n        X3 = df.groupby(['churn'])[col].agg(['mean']).reset_index()\n        X3.rename(columns={'mean': \"mean_\"+col}, inplace=True)\n\n        X1 = pd.merge(X1, X2, on = ['churn'])\n        X1 = pd.merge(X1,X3, on = ['churn'])\n        X1.head()\n        X1 = X1.transpose().reset_index()\n        X1 = X1.loc[1:]\n        X1.columns = ['Feature', 'Not-Churn', 'Churn']\n        #X1.head()\n        X1['Usage%_During_Churn'] = round((X1['Churn']\/(X1['Not-Churn'] + X1['Churn']))*100,2)\n        print(X1.head())\n        plt.figure(figsize=(12, 6))\n        ax = plt.subplot(111)\n        p = sns.barplot(x='Feature', y='Usage%_During_Churn', data=X1)\n        p.set_xticklabels(p.get_xticklabels(),rotation=45)\n        plt.title('Churn subscriber usage to Total Usage % for {}'.format(col[:-2]), fontsize = 12)\n        X1.rename(columns={'Usage%_During_Churn':'Churn_Subscriber_Usage_Trend'}, inplace=True)\n        plt.plot(X1['Churn_Subscriber_Usage_Trend'], 'r-')\n        #plt.title(title, fontsize = 12)\n        ax.legend(loc='upper center', bbox_to_anchor=(0.8, 1.00), shadow=True, ncol=2, fontsize = 10)\n        plt.grid(True)\n        plt.show()\n        \n# Method to draw AUC curve\ndef draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(6, 6))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True)\n    plt.show()\n\n    return fpr, tpr, thresholds\n\n# Common Method for Hyperparameter Tuning using Random Forest\ndef randomforestHyperparameterTuning(parameters, X_train, y_train, n_folds = 5, n_jobs = 4):\n    from sklearn.model_selection import GridSearchCV\n    from sklearn.model_selection import KFold\n    from sklearn.ensemble import RandomForestClassifier\n\n    # instantiate the model\n    param = list(parameters.keys())[0]\n    if((param == 'max_features') | (param == 'n_estimators')):\n        rfc = RandomForestClassifier(max_depth=4)\n    else:\n        rfc = RandomForestClassifier()\n    # fit tree on training data\n    rfc = GridSearchCV(rfc, parameters, \n                        cv=n_folds, \n                       scoring=\"accuracy\",\n                      return_train_score=True,\n                      n_jobs = n_jobs)\n    rfc.fit(X_train, y_train)\n    #print(\"Best Parameter ==> {}\".format(rfc.best_params_))\n    # printing the optimal accuracy score and hyperparameters\n    print('We can get accuracy of',rfc.best_score_,'using Best Parameter',rfc.best_params_)\n    \n    if(len(list(parameters.keys())) == 1):\n        scores = rfc.cv_results_\n        scoreParam = \"param_\" + list(parameters.keys())[0]\n\n        plt.figure()\n        plt.plot(scores[scoreParam], \n                 scores[\"mean_train_score\"], \n                 label=\"training accuracy\")\n        plt.plot(scores[scoreParam], \n                 scores[\"mean_test_score\"], \n                 label=\"test accuracy\")\n        plt.xlabel(param)\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n\ndef convertCategorical(contVal, threshold):\n    if(contVal > threshold):\n        return 1\n    else:\n        return 0\nconvertCategorical = np.vectorize(convertCategorical)\n\n# Function to Check the outlier in each Feature for Non-Churn and Churn Subscribers\ndef featurewiseOutlierBetweenChurnAndNonChurn():\n    columnList = (list(df.columns[3:]))\n    columnList.remove('churn')\n    for count, eachColumn in enumerate(columnList):\n        if(count%2 == 0):\n            plt.figure(count, figsize=(18,6))\n            p = plt.subplot(121)\n            sns.boxplot(y = df[eachColumn], x = df['churn'])\n            p.set_xticklabels(['Not-Churn','churn'], fontsize= 12)\n            p.grid(True)\n        else:\n            q = plt.subplot(122)\n            sns.boxplot(y = df[eachColumn], x = df['churn'])\n            q.set_xticklabels(['Not-Churn','churn'], fontsize= 12)\n            q.grid(True)\n\n# Common Function to Do the Model Evalution\ndef modelEvaluation(y_test, y_pred, model, flag = 0):\n    print(confusion_matrix(y_test,y_pred))\n    print(\"Accuracy Score ==> {}\".format(accuracy_score(y_test,y_pred)))\n    print(\"AUC Score ==> {}\".format(roc_auc_score(y_test,y_pred)))\n    if flag == 1: #For PCA\n        pred_probs_test = model.predict_proba(df_test_pca)[:,1]\n    elif flag == 2: #For XGBoost For Imbalance Data\n        pred_probs_test = model.predict_proba(np.array(X_test))[:,1]\n    elif flag == 3: #For Lasso\n        pred_probs_test = lasso.predict(X_test)\n    else:\n        pred_probs_test = model.predict_proba(X_test)[:,1]\n    print(\"ROC_AUC Score ==> {:2.2}\".format(metrics.roc_auc_score(y_test, pred_probs_test)))\n    TP = (confusion_matrix(y_test,y_pred))[0][0]\n    FP = (confusion_matrix(y_test,y_pred))[0][1]\n    FN = (confusion_matrix(y_test,y_pred))[1][0]\n    TN = (confusion_matrix(y_test,y_pred))[1][1]\n    print(\"Not-Churn Accuracy Rate:(Specificity) ==> {}\".format(TP\/(TP+FP)))\n    print(\"Churn Accuracy Rate:(Sensitivity) ==> {}\".format(TN\/(TN+FN)))\n    draw_roc(y_test, y_pred)\n    ","48749858":"df.head()","94c69231":"df.columns.values","532e4711":"column_Month_6, column_Month_7, column_Month_8, column_Month_9, column_Common = returnColumnsByMonth(df)\n\nprint(\"Month 6 Columns Count ==> {}\".format(len(column_Month_6)))\nprint(\"Month 7 Columns Count ==> {}\".format(len(column_Month_7)))\nprint(\"Month 8 Columns Count ==> {}\".format(len(column_Month_8)))\nprint(\"Month 9 Columns Count ==> {}\".format(len(column_Month_9)))\nprint(\"Common Columns Count ==> {}\".format(len(column_Common)))","c0df84be":"# All Months are having same type of columns So lets see the columns in general\nprint (\"\\nMonth based Columns:\\n \\t\\t==> {}\".format(np.array(column_Month_6)))\nprint (\"\\nCommon Columns:\\n \\t\\t==> {}\".format(np.array(column_Common)))","0ff26f81":"df['Total_Recharge_Amount'] = df['total_rech_amt_6'] + df['total_rech_amt_7']\n\n# Get 70% of \"Total Recharge Amount\" to identify the recharge Amount Range for High value customer\nprint(df['Total_Recharge_Amount'].describe(percentiles = [0.7]))\nprint(\"\\n70% of Total Recharge Amount of first 2 months are {}\".format(df['Total_Recharge_Amount'].describe(percentiles = [0.7])[5]))","3e2d74f5":"df = df[df['Total_Recharge_Amount'] > 737].reset_index(drop=True)\nprint(\"\\nTotal High Value Customer Count ==> {}\".format(df.shape[0]))\ndf.drop(columns=['Total_Recharge_Amount'], inplace=True)","85c80625":"#Get Null Percentage in dataFrame and Filter\nnullPercentageLimit = 50\ncolumns_More_Than_50_PercentNull = getColumnsBasedOnNullPercent(df,nullPercentageLimit)\n#Drop Columns with More than 50% NUll\ndf = df.loc[:, ~df.columns.isin(columns_More_Than_50_PercentNull)]\n\nprint(\"\\nColumn List Dropped with More than 50% of Null Value:==>\\n {}\\n\".format(columns_More_Than_50_PercentNull))","715fa88a":"# Get Columns which have only one value for all the rows.\nsingleCategoryColumns = df.loc[:, np.array(df.apply(lambda x: x.nunique() == 1))].columns\n\n#Print these single value column names, and the value that they contain.\nfor eachSingleCatgory in singleCategoryColumns:\n    print(\"{}: {}\".format(eachSingleCatgory, df[eachSingleCatgory].unique()))\nprint(\"\\n<=== Drop Single Category Columns, Other than last_date_of_month_6\/7\/8\/9, as it will be used for Derive Columns ===>\\n\")\nsingleCategoryColumns = [x for x in singleCategoryColumns if x not in list(['last_date_of_month_6', 'last_date_of_month_7', 'last_date_of_month_8', 'last_date_of_month_9'])]\nsingleCategoryColumns = np.array(singleCategoryColumns)\ndf = df.loc[:, ~df.columns.isin(singleCategoryColumns)]\n\n# Fill the NA value of the last_date_of_month column\ndf['last_date_of_month_7'] = df['last_date_of_month_7'].fillna('7\/31\/2014')\ndf['last_date_of_month_8'] = df['last_date_of_month_8'].fillna('8\/31\/2014')\ndf['last_date_of_month_9'] = df['last_date_of_month_9'].fillna('9\/30\/2014')","d848d3d7":"#Get the columns where the number of null valued rows are less than 50%\ncolumns_Less_Than_50_PercentNull = getColumnsBasedOnNullPercent(df,nullPercentageLimit, limitType='Lower')\ndf_temp = df.loc[:, columns_Less_Than_50_PercentNull]\n\n#Get the % of rows that are having null values in each of these columns\nround(df_temp.isnull().sum()\/len(df_temp.index) * 100,2)","f0130fbe":"column_Month_6, column_Month_7, column_Month_8, column_Month_9, column_Common = returnColumnsByMonth(df_temp)\n\nprint(\"Month 6 Columns Count ==> {}\".format(len(column_Month_6)))\nprint(\"Month 7 Columns Count ==> {}\".format(len(column_Month_7)))\nprint(\"Month 8 Columns Count ==> {}\".format(len(column_Month_8)))\nprint(\"Month 9 Columns Count ==> {}\".format(len(column_Month_9)))\nprint(\"Common Columns Count ==> {}\".format(len(column_Common)))\nprint(\"==> All Months are having same columns with less% of Null Value\")\nprint(np.array(column_Month_7))\ndf_temp.loc[:, column_Month_7].head()","68b39dff":"# 4 Derive Columns for each month, which will tell before how many days from month end, \n#recharge happened by subscriber.\ndf['days_from_LastRechage_6'] = daysSinceLastRechargeMonthwise(df, 6).apply(lambda x: x.days)\ndf['days_from_LastRechage_7'] = daysSinceLastRechargeMonthwise(df, 7).apply(lambda x: x.days)\ndf['days_from_LastRechage_8'] = daysSinceLastRechargeMonthwise(df, 8).apply(lambda x: x.days)\ndf['days_from_LastRechage_9'] = daysSinceLastRechargeMonthwise(df, 9).apply(lambda x: x.days)\ndf['days_from_LastRechage_6'] = df['days_from_LastRechage_6'].fillna(30)\ndf['days_from_LastRechage_7'] = df['days_from_LastRechage_7'].fillna(30)\ndf['days_from_LastRechage_8'] = df['days_from_LastRechage_8'].fillna(30)\ndf['days_from_LastRechage_9'] = df['days_from_LastRechage_9'].fillna(30)","50964425":"#Drop the last Recharge Date and End date of the month columns for all the months\ndateColumns = ['last_date_of_month_6', 'last_date_of_month_7', 'last_date_of_month_8','last_date_of_month_9',\n              'date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8','date_of_last_rech_9']\ndf = df.loc[:, ~df.columns.isin(dateColumns)]","453d9f5c":"\ndf = df.fillna(0).reset_index()","767745c2":"# Label churn and non-churn customers\ndf['churn'] = np.where(\n            (\n                (df['total_ic_mou_9'] == 0.0) | \n                (df['total_og_mou_9'] == 0.0)\n            ) & \n            (\n                (df['vol_2g_mb_9'] == 0.0) & \n                (df['vol_3g_mb_9'] == 0.0)\n            ),1,0\n        )","1ab9f27a":"# Remove columns with '9'\ndf = df.drop(df.filter(like = '9').columns, axis=1)","fc1e143a":"df.groupby(['churn'])['churn'].count()","7d91efee":"df['loc_og_mou_Percent_6'] = round((df['loc_og_mou_6']\/df['total_og_mou_6']) * 100,2)\ndf['std_og_mou_Percent_6'] = round((df['std_og_mou_6']\/df['total_og_mou_6']) * 100,2)\ndf['spl_og_mou_Percent_6'] = round((df['spl_og_mou_6']\/df['total_og_mou_6']) * 100,2)\ndf['og_others_Percent_6'] = round((df['og_others_6']\/df['total_og_mou_6']) * 100,2)\ndf['loc_ic_mou_Percent_6'] = round((df['loc_ic_mou_6']\/df['total_ic_mou_6']) * 100,2)\ndf['std_ic_mou_Percent_6'] = round((df['std_ic_mou_6']\/df['total_ic_mou_6']) * 100,2)\ndf['spl_ic_mou_Percent_6'] = round((df['spl_ic_mou_6']\/df['total_ic_mou_6']) * 100,2)\ndf['ic_others_Percent_6'] = round((df['ic_others_6']\/df['total_ic_mou_6']) * 100,2)\n\ndf['loc_og_mou_Percent_7'] = round((df['loc_og_mou_7']\/df['total_og_mou_7']) * 100,2)\ndf['std_og_mou_Percent_7'] = round((df['std_og_mou_7']\/df['total_og_mou_7']) * 100,2)\ndf['spl_og_mou_Percent_7'] = round((df['spl_og_mou_7']\/df['total_og_mou_7']) * 100,2)\ndf['og_others_Percent_7'] = round((df['og_others_7']\/df['total_og_mou_7']) * 100,2)\ndf['loc_ic_mou_Percent_7'] = round((df['loc_ic_mou_7']\/df['total_ic_mou_7']) * 100,2)\ndf['std_ic_mou_Percent_7'] = round((df['std_ic_mou_7']\/df['total_ic_mou_7']) * 100,2)\ndf['spl_ic_mou_Percent_7'] = round((df['spl_ic_mou_7']\/df['total_ic_mou_7']) * 100,2)\ndf['ic_others_Percent_7'] = round((df['ic_others_7']\/df['total_ic_mou_7']) * 100,2)\n\ndf['loc_og_mou_Percent_8'] = round((df['loc_og_mou_8']\/df['total_og_mou_8']) * 100,2)\ndf['std_og_mou_Percent_8'] = round((df['std_og_mou_8']\/df['total_og_mou_8']) * 100,2)\ndf['spl_og_mou_Percent_8'] = round((df['spl_og_mou_8']\/df['total_og_mou_8']) * 100,2)\ndf['og_others_Percent_8'] = round((df['og_others_8']\/df['total_og_mou_8']) * 100,2)\ndf['loc_ic_mou_Percent_8'] = round((df['loc_ic_mou_8']\/df['total_ic_mou_8']) * 100,2)\ndf['std_ic_mou_Percent_8'] = round((df['std_ic_mou_8']\/df['total_ic_mou_8']) * 100,2)\ndf['spl_ic_mou_Percent_8'] = round((df['spl_ic_mou_8']\/df['total_ic_mou_8']) * 100,2)\ndf['ic_others_Percent_8'] = round((df['ic_others_8']\/df['total_ic_mou_8']) * 100,2)\n\n# Fill All Nan Value because of 0 division set to 0.\ndf = df.fillna(0).reset_index()","37036909":"column_Month_6, column_Month_7, column_Month_8, column_Month_9, column_Common = returnColumnsByMonth(df)\n\nprint(\"Month 6 Columns Count ==> {}\".format(len(column_Month_6)))\nprint(\"Month 7 Columns Count ==> {}\".format(len(column_Month_7)))\nprint(\"Month 8 Columns Count ==> {}\".format(len(column_Month_8)))\nprint(\"Month 9 Columns Count ==> {}\".format(len(column_Month_9)))\nprint(\"Common Columns Count ==> {}\".format(len(column_Common)))\nprint(\"==> All Months are having same columns with less% of Null Value\")\nprint(np.array(column_Month_7))\ndf.loc[:, column_Month_7].head()","1dc9a49d":"churnSubscriberUsageChangePercentage()","0068f864":"X1 = df.groupby('churn')['aon'].agg(['mean']).reset_index()\np = sns.barplot(x='churn', y='mean', data=X1)\np.set_xticklabels(['Not-Churn', 'Churn'],rotation=30)\np.set_ylabel('Average Age in Network')\nplt.title('Average Age in Network between Churn and Not-Churn subscriber')\nplt.show()","1912c1d2":"featurewiseOutlierBetweenChurnAndNonChurn()","6934c834":"df = df.drop(df.loc[(df['churn'] == 0) & (\n    (df['arpu_6'] > 15000) | (df['arpu_7'] > 20000) | (df['arpu_8'] > 20000) | (df['onnet_mou_8'] > 8000) | \n    (df['offnet_mou_7'] > 9000) | (df['offnet_mou_8'] > 10000) | (df['loc_og_t2t_mou_6'] > 6000) | (df['loc_og_t2t_mou_7'] > 5000) |\n    (df['loc_og_t2t_mou_8'] > 6000) | (df['loc_og_t2m_mou_6'] > 4000) | (df['loc_og_t2f_mou_7'] > 600) | (df['loc_og_t2f_mou_8'] > 600) |\n    (df['loc_og_t2c_mou_8'] > 250) | (df['loc_og_mou_6'] > 8000) | (df['loc_og_mou_7'] > 6000) | (df['loc_og_mou_8'] > 6000) |\n    (df['std_og_t2m_mou_8'] > 8000) | (df['std_og_t2f_mou_6'] > 400) | (df['std_og_t2f_mou_7'] > 400) | (df['std_og_t2f_mou_8'] > 400) |\n    (df['std_og_mou_8'] > 10000) | (df['spl_og_mou_7'] > 800) | (df['spl_og_mou_8'] > 600) |(df['total_og_mou_8'] > 8000) |\n    (df['loc_ic_t2m_mou_8'] > 3000) | (df['loc_ic_t2f_mou_6'] > 1000) |(df['loc_ic_t2f_mou_8'] > 1000) |(df['loc_ic_mou_8'] > 4000) |\n    (df['std_ic_t2m_mou_8'] > 3000) | (df['std_ic_t2f_mou_8'] > 800) | (df['std_ic_mou_8'] > 3000) | (df['total_ic_mou_8'] > 4000) |\n    (df['isd_ic_mou_7'] > 3000) | (df['isd_ic_mou_8'] > 2000) | (df['ic_others_8'] > 400) | (df['sachet_2g_8'] > 30) | (df['sachet_3g_8'] > 30)\n)].index)","5f0f2f8a":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nplt.figure(figsize = (40,20))        # Size of the figure\nsns.heatmap(df.corr(),annot = True)","b047c8ec":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import RFE\n# Importing classification report and confusion matrix from sklearn metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\nfrom sklearn import metrics","b35357b3":"X = (df.iloc[:,3:])\nX = X.loc[:,X.columns != 'churn']\ny = df.loc[:, 'churn']\n\n#Standardization of Data\nscaler = StandardScaler()\nscaler.fit(X)\n#Using a Train : Test Split of 80:20\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)","52123904":"print(\"Number of Features ==> {}\".format(len(X.columns)))","05271a3a":"lr = LogisticRegression()\nlr.fit(X_train, y_train) #Use Balanced Data for Logistic Regression\ny_pred = lr.predict(X_test)","0eb16eec":"modelEvaluation(y_test, y_pred, lr)","a4a3f0fc":"#Improting the PCA module\nfrom sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=42)\n#Doing the PCA on the train data\npca.fit(X_train)\n\n#Making the screeplot - plotting the cumulative variance against the number of components\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.title(\"Scree Plot\")\nplt.show()","220323ba":"pca.components_","e777c508":"#Using incremental PCA for efficiency\nfrom sklearn.decomposition import IncrementalPCA\npca_final = IncrementalPCA(n_components=25)\ndf_train_pca = pca_final.fit_transform(X_train)\n#creating correlation matrix for the principal components\ncorrmat = np.corrcoef(df_train_pca.transpose())\n#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (20,10))\nsns.heatmap(corrmat,annot = True)\ncorrmat_nodiag = corrmat - np.diagflat(corrmat.diagonal())\nprint(\"max corr:==> {}\".format(corrmat_nodiag.max()), \"\\n min corr: ==> {}\".format(corrmat_nodiag.min()))","396b74c5":"df_test_pca = pca_final.transform(X_test)\nlr = LogisticRegression()\nmodel_pca = lr.fit(df_train_pca, y_train)\n\n# Making prediction on the test data\ny_pred = model_pca.predict(df_test_pca)","68b330c8":"# draw_roc(y_test, y_pred)\nmodelEvaluation(y_test, y_pred, model_pca, 1)","6953bd8d":"pca_again = PCA(0.95)\ndf_train_pca = pca_again.fit_transform(X_train)\ndf_train_pca.shape","6056d13a":"learner_pca = LogisticRegression()\nmodel_pca = learner_pca.fit(df_train_pca,y_train)\n\ndf_test_pca = pca_again.transform(X_test)\ndf_test_pca.shape\n# #Making prediction on the test data\ny_pred = model_pca.predict(df_test_pca)","9dfdc4d5":"# draw_roc(y_test, y_pred)\nmodelEvaluation(y_test, y_pred, model_pca, 1)","b3a8fcf0":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)","adc3c623":"# Making predictions\ny_pred = rfc.predict(X_test)\nmodelEvaluation(y_test, y_pred, rfc)","ead54382":"parameters = {'max_depth': range(2,30, 5)}\nrandomforestHyperparameterTuning(parameters, X_train, y_train)","8ab7805b":"# Number of Trees\nparameters = {'n_estimators': range(100, 1500, 400)}\n#randomforestHyperparameterTuning(parameters)\nrandomforestHyperparameterTuning(parameters, X_train, y_train)","033f25be":"# Maximum Features to split in a node\nparameters = {'max_features': [4, 8, 14, 20, 24, 28, 32]}\n#randomforestHyperparameterTuning(parameters)\nrandomforestHyperparameterTuning(parameters, X_train, y_train)","72064c7a":"parameters = {'min_samples_leaf': range(50, 400, 40)}\n#randomforestHyperparameterTuning(parameters)\nrandomforestHyperparameterTuning(parameters, X_train, y_train)","251bc422":"parameters = {'min_samples_split': range(200, 500, 50)}\n#randomforestHyperparameterTuning(parameters)\nrandomforestHyperparameterTuning(parameters, X_train, y_train)","92c1bba9":"# param_grid = {\n#     'max_depth': [4,8,10],\n#     'min_samples_leaf': [30, 50 , 70],\n#     'min_samples_split': [150, 170, 200],\n#     'n_estimators': [100,200, 300], \n#     'max_features': [15,20,25]\n# }\n# # Create a based model\n# rf = RandomForestClassifier()\n# # Instantiate the grid search model\n# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n#                           cv = 3, n_jobs = -1,verbose = 1)","1aa200f5":"# grid_search.fit(X_train, y_train)","8c4b1b11":"# # printing the optimal accuracy score and hyperparameters\n# print('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","cdbe26e8":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=8,\n                             min_samples_leaf=50, \n                             min_samples_split=150,\n                             max_features=20,\n                             n_estimators=100)\n# fit\nrfc.fit(X_train,y_train)\n\n# Making predictions\ny_pred = rfc.predict(X_test)","54267d76":"modelEvaluation(y_test, y_pred, rfc)","6c91b922":"from sklearn.svm import SVC","9fd1d988":"# #SVM\n# from sklearn.svm import SVC\n\n# folds = KFold(n_splits = 5, shuffle = True, random_state = 4)\n# # specify range of parameters (C) as a list\n# params = {\"C\": [0.1, 1, 10]}\n\n# model = SVC()\n\n# # set up grid search scheme\n# # note that we are still using the 5 fold CV scheme we set up earlier\n# model_cv = GridSearchCV(estimator = model, param_grid = params, \n#                         scoring= 'accuracy', \n#                         cv = folds, \n#                         verbose = 1,\n#                        return_train_score=True,\n#                        n_jobs = -1)   \n# # fit the model - it will fit 5 folds across all values of C\n# model_cv.fit(X_train, y_train)  \n\n# best_score = model_cv.best_score_\n# best_C = model_cv.best_params_['C']\n\n# print(\" The highest test accuracy is {0} at C = {1}\".format(best_score, best_C))","85d7a0ee":"# model_svm = SVC(C = 0.1, probability=True)\n# model_svm.fit(X_train, y_train)\n# y_pred = model_svm.predict((X_test))","5be794d1":"# modelEvaluation(y_test, y_pred, model_svm)","d1cf3e87":"print(\"Non-Churn Percentage ==> {}\".format(df[df['churn'] == 0].shape[0]\/df.shape[0]))\nprint(\"Churn Percentage ==> {}\".format(df[df['churn'] == 1].shape[0]\/df.shape[0]))","b886cae6":"import xgboost as xgb\nfrom xgboost import XGBClassifier\n\n# hyperparameter tuning with XGBoost\n\n# creating a KFold object \nfolds = 3\n\n# specify range of hyperparameters\nparam_grid = {'learning_rate': [0.2, 0.6], \n             'subsample': [0.3, 0.6, 0.9]}          \n\n\n# specify model\nxgb_model = XGBClassifier(max_depth=2, n_estimators=200)\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = xgb_model, \n                        param_grid = param_grid, \n                        scoring= 'roc_auc', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True,\n                        n_jobs = -1)    \nmodel_cv.fit(X_train, y_train)","728b5e0b":"model_cv.best_params_","c979dab4":"#Execute XGBoost using the best parameter value got from GridSearch Cross  Validation\nparams = {'learning_rate': 0.2,\n          'max_depth': 2, \n          'n_estimators':200,\n          'subsample':0.9,\n         'objective':'binary:logistic'}\n\n# fit model on training data\nmodel = XGBClassifier(params = params)\nmodel.fit(X_train, y_train)\n\n# # Making predictions\ny_pred = model.predict((X_test))","d323d88b":"modelEvaluation(y_test, y_pred, model)","46d35b65":"from imblearn.over_sampling import SMOTE\n\nprint('Before OverSampling, the shape of train_X: {}'.format(X_train.shape))\nprint('Before OverSampling, the shape of train_y: {} \\n'.format(y_train.shape))\nprint(\"Before OverSampling, y_train count: '1': Churn ==> {}\".format(sum(y_train == 1)))\nprint(\"Before OverSampling, y_train count: '0': Not-Churn ==> {}\".format(sum(y_train == 0)))\n\n\nsm = SMOTE()\nX_train_sam, y_train_sam = sm.fit_sample(X_train, y_train.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_sam.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_sam.shape))\n\nprint(\"After OverSampling, count: '1': Churn ==> {}\".format(sum(y_train_sam==1)))\nprint(\"After OverSampling, count: '0': Not-Churn ==>{}\".format(sum(y_train_sam==0)))","98d473f7":"lr = LogisticRegression()\nlr.fit(X_train_sam, y_train_sam) #Use Balanced Data for Logistic Regression\ny_pred = lr.predict(X_test)\nmodelEvaluation(y_test, y_pred,lr)","249110d0":"#Use the Same Hyper parameter got in last execution\nrfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=8,\n                             min_samples_leaf=50, \n                             min_samples_split=150,\n                             max_features=20,\n                             n_estimators=100)\n# fit\nrfc.fit(X_train_sam,y_train_sam)\n\n# Making predictions\ny_pred = rfc.predict(X_test)\nmodelEvaluation(y_test, y_pred,rfc)","199ba01e":"# model_svm = SVC(C = 0.1, probability=True)\n# model_svm.fit(X_train_sam, y_train_sam)\n# y_pred = model_svm.predict((X_test))\n# modelEvaluation(y_test, y_pred,model_svm)","24e3fcfa":"pca_again = PCA(0.99)\ndf_train_pca = pca_again.fit_transform(X_train_sam)\ndf_train_pca.shape","1afbb7ed":"learner_pca = LogisticRegression()\nmodel_pca = learner_pca.fit(df_train_pca,y_train_sam)\n\ndf_test_pca = pca_again.transform(X_test)\ndf_test_pca.shape\n# #Making prediction on the test data\ny_pred = model_pca.predict(df_test_pca)","51a79aef":"modelEvaluation(y_test, y_pred, model_pca, 1)","b0bf8edd":"from sklearn.linear_model import Lasso\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\nlasso = Lasso()\n\n# cross validation\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = 5, \n                        return_train_score=True,\n                        verbose = 1,\n                        n_jobs = -1)            \n\nmodel_cv.fit(X_train, y_train)","0ea4873b":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","677a6f06":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","34965277":"print(\"Best Alpha Value ==> {} \".format(model_cv.best_params_))","82140e5d":"alpha =0.01\nlasso = Lasso(alpha=alpha)\nlasso.fit(X_train, y_train)\n\n\ny_pred = lasso.predict(X_test)\ny_pred = convertCategorical(y_pred, 0.5)","73f78bff":"modelEvaluation(y_test, y_pred, lasso, 3)","9b601dd1":"#Get Features list and co-efficient values from the Lasso Regression and make a single dataframe\ns1 = pd.DataFrame(np.insert(np.array(X.columns),0,\"constant\"), columns=['feature'])\ns1.reset_index(drop = True, inplace = True)\n\ns2 = pd.DataFrame(np.insert(np.array(lasso.coef_), 0, lasso.intercept_), columns=['Values'])\ns2.reset_index(drop = True, inplace= True)\n\ns2['Values'] = s2['Values'].apply(lambda x: round(x,3))\ndrivingFeaturedf = pd.concat([s1,s2], axis=1)\ndrivingFeaturedf = drivingFeaturedf.iloc[1:]\ndrivingFeaturedf.reset_index(drop= True, inplace = True)","f52abc91":"# Draw the complete date frame of features and coefficient.\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(26,10))\nplt.subplot(111)\nax1 = sns.barplot(x = drivingFeaturedf['feature'], y = drivingFeaturedf['Values'])\nax1.set_xticklabels(ax1.get_xticklabels(),rotation=90, fontsize= 9)\nax1.set_yticklabels(ax1.get_yticklabels(), fontsize = 15)\nplt.ylabel('Co-efficient', fontsize = 10)\nplt.show()","ea559878":"#Drop features with 0 coefficient.\ndrivingFeaturedf = drivingFeaturedf.loc[drivingFeaturedf['Values'] != 0]","59912c27":"#Draw the plot for the features with non-zero co-efficient\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(15,8))\nplt.subplot(111)\nax1 = sns.barplot(x = drivingFeaturedf['feature'], y = drivingFeaturedf['Values'])\nax1.set_xticklabels(ax1.get_xticklabels(),rotation=90, fontsize=10)\nplt.ylabel('Co-efficient')\nplt.show()","1f10dd80":"print(\"Driving Feature List: ==>\\n \\t\\t\\t{}\".format(np.array(drivingFeaturedf['feature'])))","571b2c3a":"<a id=\"correlation_trend\"><\/a>\n#### * EDA For all the features (Monthly Featues) to find how much % usage for each feature monthwise for Churn Subscriber on total usage","c65a1449":"<a id=\"drop_nulls\"><\/a>\n#### * Null Value Checking and Drop High Null Value Columns","90a5f7a8":"#### * GridSearch with different value of Alpha for Lasso regression","e8067f80":"### Problem Statement\nIn the telecom industry, customers switch their operators when they come across better deals. Since it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention is more important than customer acquisition. In this project, we predict the churn for pre-paid customers who are predominant in the Indian and Southeast Asian market. The business objective is to predict the churn in the ninth month using the data  from the first three months.","05520319":"<a id=\"xgboost\"><\/a>\n#### * GridSearch with different Hyperparameter for XGBoost","5d4481b1":"<a id=\"common_functions\"><\/a>\n# Common Functions","00ba1398":"* Column Analysis for Derive Columns\n      - From the data provided following are the overall calculation and relation between below 5 set of columns.\n      - Total Outgoing MOU is sum of all kind of local, std, special and other outgoing MOU.\n          i.e:\n              total_og_mou_6\/7\/8 = loc_og_mou_6\/7\/8 +  std_og_mou_6\/7\/8 + spl_og_mou_6\/7\/8 + og_others_6\/7\/8\n      - Total Incoming MOU is sum of local, Std, special and others incoming MOU.\n           i.e:\n              total_ic_mou_6\/7\/8 = loc_ic_mou_6\/7\/8 + std_ic_mou_6\/7\/8 + spl_ic_mou_6\/7\/8 + ic_others_6\/7\/8\n* Derive Columns\n  - In place of using direct columns, as we have total value and its distribution, lets get the % of usage in each distribution.\n    \n    e.g: How much local outgoing MOU with respect to total outgoing and so on for all month data\n    \n             loc_og_mou_Percent_6\/7\/8, std_og_mou_Percent_6\/7\/8, spl_og_mou_Percent_6\/7\/8, og_others_Percent_6\/7\/8\n             loc_ic_mou_Percent_6\/7\/8, std_ic_mou_Percent_6\/7\/8, spl_ic_mou_Percent_6\/7\/8, ic_others_Percent_6\/7\/8","7371de6b":"<a id=\"null_less_than_50\"><\/a>\n#### * Analyze Null Value for Less than 50%","a657d5a7":"<a id=\"pca\"><\/a>\n### <font color='#0000FF'> * Logistic Regression Using PCA <\/font>","f139f343":"#### `` 2. n_estimators `` ","54e46c6f":"<a id=\"eda\"><\/a>\n# Basic Data Analysis and Null Value Imputation","2d5fba7f":"<a id=\"lasso\"><\/a>\n### <font color='#0000FF'> * Lasso Regression to find the major Determining Parameters <\/font>","138f95e3":"### Solution Abstract\nWe start with exploratory data analysis, impute null values and drop columns that do not add additional information. We then filter for high value customers who have recharged with an amount more than or equal the 70th percentile of the average recharge amount in the first two months. We also derived 6 columns. We then prepared the data and applied multiple algorithms to generate a model by hyperparameter tuning. \n\nWe first applied four machine learning algorithms and measured the effectiveness of the models by using ROC AUC score and stratified accuracy scores for Churn and Non Churn Customers.\n* Basic Logistic Regression \n* Logistic Regression with PCA \n* Random Forest with Hyperparameter tuning \n* SVM with Hyperparameter tuning \n\nSince the data set is highly imbalanced with the churn customers being 9.6% of the dataset, the above algorithms did not produce production quality results.\n\nHence we applied the following two techniques for Handling this highly imbalanced dataset.\n\n* XGBoost \n* SMOTE followed by Logistic Regression, Random Forest, SVM and PCA \n\nHere is the summary of the Model Performance of all the models that we tried.\n\nNo | Algorithm | Accuracy Score |  AUC Score | ROC_AUC Score | Non-Churn Accuracy Rate(Specificity) | Churn Accuracy Rate (Sensitivity) |\n-|----------|----------------|------------|---------------|-------------------------|---------------------|\n1|Basic Logistic Regression | 0.92 | 0.73 | 0.91 | 0.97 | 0.48 |\n2|Logistic Regression with PCA | 0.91 | 0.60 | 0.88 | 0.99 | 0.22 |\n3|Random Forest with Hyperparameter tuning | 0.93 | 0.70 | 0.89 | 0.98 | 0.41 | \n4|Linear SVM with Hyperparameter tuning | 0.89 | 0.5 | 0.5 | 1 | 0 | \n5|Using XGBoost | 0.93 | 0.75 | 0.94 | 0.97 | 0.52 | \n6|Using SMOTE and Logistic Regression | 0.87 | 0.83 | 0.9 | 0.88 | 0.78 |\n7|<b><font color='#0000FF'>Using SMOTE and Random Forest <\/font><\/b> | 0.89 | 0.84 | <b><font color='#00FF00'>0.93 <\/font><\/b>| 0.91 | <b><font color='#FF0000'>0.77 <\/font><\/b>|\n8|Using SMOTE and Linear SVM | 0.89 | 0.5 | 0.5 | 1 | 0 | \n9|<b><font color='#0000FF'>Using SMOTE and PCA <\/font><\/b> | 0.81 | 0.81 | <b><font color='#00FF00'>0.88<\/font><\/b> | 0.80 | <b><font color='#00FF00'>0.83 <\/font><\/b> | \n\nWith SMOTE Data Sampling all the algorithms perform better than the unsampled data. Among these with SMOTE followed by PCA, we achieved the best churn prediction model with AUC 81% and Churn Predicton Accuracy more than 82.5%. This is the best model among all the six models that we applied. Linear SVM fared as the worst model.\nWe summarize the results alongwith metrics at the end of this project.\n\n","452483de":"<a id=\"smote_svm\"><\/a>\n#### *Applying SVM with SMOTE","c440b0f2":"### Insight","9d8bae2d":"#### `` 5. min_samples_split ``","1fc3af20":"#### * Drop Outlier for Non-Churn Subscriber","665e768e":"#### * Fit test data with the same PCA model and execute Logistic Regression","5b7d2670":"#### * Applying 25 features in final PCA","371587fd":"* As data is imbalanced, none of the algorithms is providing effective result for Churn Subscriber % in above steps.\n* Lets apply 2 methods for imbalanced data set handling\n     - Use XGBoost directly\n     - Use SMOTE (Synthetic Minority Over Sampling Technique) and apply various algorithms\n         - SMOTE with Logistic Regression\n         - SMOTE with Random Forest\n         - SMOTE with SVM\n         - SMOTE with PCA","230ab73f":"<a id=\"exploratory_data_analysis\"><\/a>\n# EDA and Derive Columns","dc40f285":"<a id=\"random_forest\"><\/a>\n### <font color='#0000FF'> * Using Random Forest <\/font>","0ebbe2fa":"<a id=\"data_standardization\"><\/a>\n### <font color='#0000FF'> * Data standardization and preparation <\/font>","0d62589b":"#### * Random Forest with Grid Search and Hyperparameter Tuning","fd6bb9f2":"#### * PCA Again","29f36350":"# Telecom Churn [](http:\/\/) Project","e4da0c0c":"#### * Drop 9th Month Columns","11c09b76":"<a id=\"outlier_handling\"> <\/a>\n### Outlier Handling\n","77e425b9":"* As Non-Churn Count is more, need to check outlier for each feature by comparing Churn and Non-Churn subscriber ","84589bf2":"<a id=\"eda_insights\"><\/a>\n### Insights from EDA","4907e9f3":"<a id=\"summary\"><\/a>\n# Summary","f77489e3":"<a id=\"svm\"><\/a>\n### <font color='#0000FF'> * Using SVM <\/font>","270603ff":"<a id=\"monthwise\"><\/a>\n####  * Get Columns Monthwise & Basic Understanding of Columns","a0963b06":"<a id=\"data_imbalance\"><\/a>\n### <font color='#0000FF'> * Imbalance dataset Handling<\/font>","9e8ea0d0":"* As Non-Churn subscriber % is more, some as per the feature value spread, some of the records can delete in following condition for Non-Churn subscriber.\n    1. Drop Record with arpu_6 more than 15000\n    2. Drop arpu_7 more than 20000\n    3. Drop arpu_8 more than 20000\n    4. onnet_mou_8 more than 8000\n    5. offnet_mou_7 more than 9000\n    6. offnet_mou_8 more than 10000\n    7. loc_og_t2t_mou_6 more tan 6000\n    8. loc_og_t2t_mou_7 more than 5000\n    9. loc_og_t2t_mou_8 more than 6000\n    10. loc_og_t2m_mou_6 more than 4000\n    11. loc_og_t2f_mou_7 more than 600\n    12. loc_og_t2f_mou_8  more than 600\n    13. loc_og_t2c_mou_8 more than 250\n    14. loc_og_mou_6 more than 8000\n    15. loc_og_mou_7 more than 6000\n    16. loc_og_mou_8  more than 6000\n    17. std_og_t2m_mou_8 more than 8000\n    18. std_og_t2f_mou_6 more than 400\n    19. std_og_t2f_mou_7  more than 400\n    20. std_og_t2f_mou_8 more than 400\n    21. std_og_mou_8 more than 10000\n    22. spl_og_mou_7 more than 800\n    23. spl_og_mou_8 more than 600\n    24. total_og_mou_8 more than 8000\n    25. loc_ic_t2m_mou_8 more than 3000\n    26. loc_ic_t2f_mou_6 more than 1000\n    27. loc_ic_t2f_mou_8 more than 1000\n    28. loc_ic_mou_8 more than 4000\n    29. std_ic_t2m_mou_8 more than 3000\n    30. std_ic_t2f_mou_8 more than 800\n    31. std_ic_mou_8 more than 3000\n    32. total_ic_mou_8 more than 4000\n    33. isd_ic_mou_7 more than 3000\n    34. isd_ic_mou_8 more than 2000\n    35. ic_others_8 more than 400\n    36. sachet_2g_8 more than 30 \n    37. sachet_3g_8 more than 30","2103e58d":" * As per PCA 25 features can give 95% accuracy.","a9d9c619":"### Table of Contents\nThis notebook is organized as follows. Please click on the following links to go to the respective section.\n\n### <a href=\"#common_functions\">Common Functions <\/a><\/b>\n\n### <a href = \"#eda\">Basic Data Analysis and Null Value imputation <\/a><\/b>\n   * #### <a href =\"#monthwise\"> Get Columns Monthwise and Basic Understanding of Columns <\/a>\n   * #### <a href = \"#total_recharge\"> Derive Columns to get Total Recharge Amount in Month 6 and 7th <\/a>\n   * #### <a href=\"#high_value_customer\">Filter High Value Customer <\/a>\n   * #### <a href=\"#drop_nulls\"> Null Value Check and Drop Columns High % NULL Values <\/a>\n\n   * #### <a href=\"#single_categorical\">Check Categorical Variables with Single Value columns and Drop them.<\/a>\n   * #### <a href=\"#null_less_than_50\"> Analyze NULL Value with less than 50% NULL data. <\/a>\n   * #### <a href=\"#distribution\"> Check for columns with less than 50% NULL value and distribution in all months. <\/a>\n   * #### <a href=\"#impute\"> Impute other NULL fields with 0. <\/a>\n   * #### <a href=\"#derived_columns\"> Get a new derived columns <\/a>\n\n### <a href=\"#exploratory_data_analysis\">EDA <\/a>\n   * #### <a href=\"#churn_nonchurn\">Get Churn, Non-Churn ratio in complete dataset <\/a>\n   * #### <a href=\"#correlation_trend\">Monthwise trend of each features <\/a>.\n   * #### <a href=\"#age_on_network\"> Age on Network Correlation trend <\/a>\n   * #### <a href=\"#eda_insights\"> Insights from EDA <\/a>\n\n### <a href=\"#outlier_handling\"> Outlier Handling <\/a>\n    \n### <a href=\"#data_modelling\"> Applying Machine Learning Algorithms <\/a>\n   * #### <a href=\"#data_standardization\">Data standardization and preparation.<\/a>\n   * #### <a href = \"#basic_logistic_regression\"> Basic Logistic Regression <\/a>\n   * #### <a href=\"#pca\"> Logistic Regression with PCA <\/a>\n   * #### <a href=\"#random_forest\"> Random Forest with Hyperparameter tuning <\/a>\n   * #### <a href=\"#svm\"> SVM with Hyperparameter tuning <\/a>\n   * #### <a href=\"#data_imbalance\">Imbalance Dataset Handling <\/a>\n       * #### <a href=\"#xgboost\"> Using XGBoost <\/a>\n       * #### <a href =\"#smote_logistic_regression\"> Using SMOTE and Logistic Regression <\/a>\n       * #### <a href =\"#smote_random_forest\"> Using SMOTE and Random Forest <\/a>\n       * #### <a href =\"#smote_svm\"> Using SMOTE and Linear SVM <\/a>\n       * #### <a href =\"#smote\"> Using SMOTE and PCA <\/a>\n   \n### <a href=\"#lasso\"> Lasso Regression to find the major Determining Parameters <\/a>\n\n### <a href= \"#summary\" >Summary <\/a><\/b>\n   * #### <a href = \"#smote_pca\">SMOTE with PCA as Best Balanced Model <\/a>\n   * #### <a href=\"#driving_features\"> Churn Indicators and Business Recommendation <\/a>    ","c6d3a426":"#### * Run XGBoost with Best Parameter got from Hyperparameter Tuning","c46f39e4":"<a id=\"basic_logistic_regression\"><\/a>\n### <font color='#0000FF'> * Basic Logistic Regression <\/font>","d10a458a":"<a id=\"derived_columns\"><\/a>\n#### * Derive Columns","23cdbb68":"#### * One derive column for each month from the date columns and drop the date column, then impute NULL for derive column with 30 days","05538aab":"<a id=\"smote_random_forest\"><\/a>\n#### * Applying RandomForest With SMOTE","552a1170":"<a id=\"data_modelling\"><\/a>\n# Data Modelling","728d25f1":"#### * Final Random Forest Model with all optimal parameter","93dbd17c":"* By Considering 8th Month as Churn Decision month and 6th and 7th as Good Month, following are the insight for all the features\n    * Average Revenue per user (ARPU) is less in 8th month than 6th and 7th month. \n    * Onnet-Monthly-Usage (Onnet_MOU) is less in 8th Month than 6th and 7th Month.\n    * Offnet-Montly-Usage (Offnet_MOU) is less in 8th Month than 6th and 7th Month.\n    * ROAM_Incoming_MontlyUsage (Roam_IC_MOU) is not having any changes in good and decision period.\n    * Roam_Outgoing_MonthlyUsage (Roam_OG_MOU) is not having any changes in good and decision perod.\n    * Local Outgoing t2t Monthly Usage is less in 8th Month than 6th and 7th Month.\n    * Local Outgoing t2m Monthly Usage is less in 8th month than 6th and 7th month.\n    * Local Outgoing t2f Monthly Usage is less in 8th month than 6th and 7th month.\n    * Local Outgoing t2c Monthly Usage is less in 8th month than 6th and 7th month.\n    * Local Outgoing Monthly Usage is less in 8th month than 6th and 7th month.\n    * STD outgoing t2t Monthly Usage is less in 8th month than 6th and 7th month.\n    * STD outgoing t2m Monthly usage is less in 8th month than 6th and 7th month.\n    * STD Outgoinh t2f Monthly Usage is less in 8th month than 6th and 7th month.\n    * STD outgoing Monthly Usage is less in 8th month than 6th and 7th month.\n    * ISD Outgoing Monthly Usage is less in 8th month than 6th and 7th month.\n    * Special Outgoing Monthly Usage is less in 8th month than 6th and 7th month.\n    * Outgoing Others is not having any changes in good and decision period.\n    * Total Outgoing Monthly Usage is less in 8th Month than 6th and 7th month.\n    * Local Incoming t2t Monthly Usage is less in 8th month than 6th and 7th month.\n    * Local incoming t2m Monthly Usage is less in 8th month than 6th and 7th month.\n    * Local incoming t2f Monthly Usage is less in 8th month than 6th and 7th month.\n    * Local incoming usage Monthly Usage is less in 8th month than 6th and 7th month.\n    * STD incoming t2t Monthly Usage is less in 8th month than 6th and 7th month.\n    * STD incoming t2m monthly usage is less in 8th month than 6th and 7th month.\n    * STD incoming t2f monthly usage is less in 8th month than 6th and 7th month.\n    * STD incoming monthly usage is less in 8th month than 6th and 7th month.\n    * Total Incoming Monthly Usage is less in 8th month than 6th and 7th month.\n    * spl_ic_mou is less in 8th month than 6th and 7th month.\n    * isd_ic_mou is less in 8th month than 6th and 7th month.\n    * ic_others is less in 8th month than 6th and 7th month.\n    * total_rech_num is less in 8th month than 6th and 7th month.\n    * toal_rech_amt is less in 8th month than 6th and 7th month.\n    * max_rech_amt is less in 8th month than 6th and 7th month.\n    * last_rech_amt is less in 8th month than 6th and 7th month.\n    * vol_2g_mb is less in 8th month than 6th and 7th month.\n    * vol_3g_mb is less in 8th month than 6th and 7th month.\n    * monthly_2g is less in 8th month than 6th and 7th month.\n    * sachet_2g is less in 8th month than 6th and 7th month.\n    * monhtly_3g is less in 8th month than 6th and 7th month.\n    * sachet_3g is less in 8th month than 6th and 7th month.\n    * vbc_3g is less in 8th month than 6th and 7th month.","0bc251ff":"#### * Identify Outlier for each Feature By comparing data of Churn and Non-Churn","780774f2":"<a id=\"single_categorical\"><\/a>\n#### * Check Categorical Variables and Single Record Variables. Then drop the columns which have a single value.","f650e062":"#### * Driving Parameter Selection","393edd7f":"<a id=\"total_recharge\"><\/a>\n#### * Derive Columns Total_Recharge_Amount from 6th and 7th Month total_rech_amt","63bf219d":"#### * Basic Random Forest with default parameters","2319396d":"### Insight","9c3ca40c":"<a id=\"age_on_network\"><\/a>\n#### * AON feature trend in all the 3 months for churn and non-churn subscribers","c719e540":"* Churn subscriber is having less average AON than Non-Churn Subscriber. Hence subsriber is having high AON, then chances of Churn is less","fa498830":"#### `` 4. min_samples_leaf ``","66807b86":"#### * Fill Other NULL field with value 0","c64dfcfa":"#### * Grid Search to find Optimal Hyperparameter for Random Forest","f133fc9a":"<a id=\"impute\"><\/a>\n#### * As the Null % is very less, lets see if Null Value Can be imputed with some value","86665797":"#### `` 3. max_feature ``","73f18c58":"<a id=\"high_value_customer\"> <\/a>\n#### * Filter High Value Customer from main data frame","bda48e6d":"<a id=\"churn_nonchurn\"><\/a>\n#### * Derive a Column which will tell if subscriber is churn or not churn","60956d01":"* Following columns with less % of NULL value can be imputed with 0, as all these columns represent some kind of special MOU (Minutes of usage)\n* It is possible that one susbcriber may not have done a special number call in one month, or not done a isd call and so on..\n* With this above logic, following columns can be imputed with 0 for NULL values.\n\nColumns List Left with NULL Value (Null % less than 50):\n    \n'onnet_mou_6\/7\/8' 'offnet_mou_6\/7\/8' 'roam_ic_mou_6\/7\/8' 'roam_og_mou_6\/7\/8'\n 'loc_og_t2t_mou_6\/7\/8' 'loc_og_t2m_mou_6\/7\/8' 'loc_og_t2f_mou_6\/7\/8'\n 'loc_og_t2c_mou_6\/7\/8' 'loc_og_mou_6\/7\/8' 'std_og_t2t_mou_6\/7\/8' 'std_og_t2m_mou_6\/7\/8'\n 'std_og_t2f_mou_6\/7\/8' 'std_og_mou_6\/7\/8' 'isd_og_mou_6\/7\/8' 'spl_og_mou_6\/7\/8'\n 'og_others_6\/7\/8' 'loc_ic_t2t_mou_6\/7\/8' 'loc_ic_t2m_mou_6\/7\/8' 'loc_ic_t2f_mou_6\/7\/8'\n 'loc_ic_mou_6\/7\/8' 'std_ic_t2t_mou_6\/7\/8' 'std_ic_t2m_mou_6\/7\/8' 'std_ic_t2f_mou_6\/7\/8'\n 'std_ic_mou_6\/7\/8' 'spl_ic_mou_6\/7\/8' 'isd_ic_mou_6\/7\/8' 'ic_others_6\/7\/8'\n    ","271e61fa":"#### Handle Imbalance training dataset using SMOTE","c1dfa20d":"<a id=\"driving_features\"><\/a>\n** Based on the Lasso Regression results above The major factors to that determine if a subscriber is going to churn are **\n\n    1) Minutes of Usage on 8th Month for outgoing and incoming calls (Mostly Local\/STD\/Special\/Others) and \n    2) Recharge count on 8th and 7th months. \n    \n** Below are the detailed features that have a bearing on whether the customer will churn**\n\n** Recommendation: If the total usage as measured by the total minutes of usage and the recharge amount in 7th and 8th month is declining as compared to 6th month, then it is likely that such a customer will churn. If the Total Outgoing Minutes of Usage falls below 228 minutes in the 8th  73 We recommend the telecom provider to reach out to such customers, and provide them with lockin offers that will prevent their churn. **\n\n* Driving Feature List As per Model ( 9 features)\n      - total_Rech_Num_7 (Total Number of recharge on 7th Month)\n      - total_Rech_Num_8 (Total Number of Reccharge on 8th Month)\n      Derived Columns:\n      - Days_From_LastRecharge_8 (When the last recharge done on 8th Month)\n      - loc_og_mou_Percent_8 (Local Outgoing Minutes of Usage % in 8th month wrt to Total Outgoing usage on 8th Month)\n      - std_og_mou_Percent_8 (STD outgoing Minutes of usage % in 8th month wrt to Total Outgoing usage on 8th Month)\n      - spl_og_mou_Percent_8 (Special Outgoing Minutes of usage % in 8th Month wrt to total Outgoing usage on 8th Month)\n      - loc_ic_mou_Percent_8 (Local Incoming Minutes of Usage % in 8th Month wrt to Total incoming usage on 8th month)\n      - std_ic_mou_Percent_8 (STD Incoming Minutes of Usage % in 8th Month wrt to total incoming Usage on 8th Month)\n      - ic_others_Percent_8 (Others incoming Minutes of Usage % in 8th Month wrt to Toal incoming Usage on 8th Month)\n      \n* Driving Feature List As per Original Features given in Dataset (12 Features)\n\n      - total_Rech_Num_7 (Total Number of Recharge on 7th Month)\n      - total_Rech_Num_8 (Total number of Recharge on 8th Month)\n      - last_date_of_month_8 (Last Date of 8th Month)\n      - date_of_last_rech_8 (Last recharge date on 8th Month)\n      - loc_og_mou_8 (Local Outgoing Minutes of Usage on 8th Month)\n      - std_og_mou_8 (STD Outgoing Minutes of Usage on 8th Month)\n      - spl_og_mou_8 (Special Outgoing Minutes of usage on 8th Month)\n      - loc_ic_mou_8 (Local Incoming Minutes of Usage on 8th Month)\n      - std_ic_mou_8 (STD Incoming Minutes of Usage on 8th Month)\n      - ic_others_8 (Others Incoming Minutes of Usage on 8th Month)\n      - total_ic_mou_8 (Total Incoming Minutes of Usage on 8th Month)\n      - total_og_mou_8 (Toal Outgoing Minutes of Usage on 8th Month)\n","d01d60d5":"<a id=\"smote_logistic_regression\"> <\/a>\n#### * Applying Logistic Regression with SMOTE","ffe78f1e":"#### Correlation matrix","02b34cac":"<a id=\"smote\"><\/a>\n#### * Applying PCA with SMOTE","c914ea91":"<a id=\"smote_pca\"><\/a>\n* With SMOTE (Synthetic Minority Over Sampling Technique) and PCA, we are getting best balance prediction with ROC AUC 88% and Churn Predicton Accuracy more than 82.5%. A close competitor to this model is the Random Forest model with a ROC AUC score of 93%. However the Churn accuracy rate for this model is lower at 77%. Since we are more concerned with the accuracy of prededicting churn, SMOTE with PCA  is the best model among all the six models that we applied. Linear SVM fared as the worst model.","69d79912":"Due to a large number of variables, we cannot visualize the corellation matrix properly. We will address this after PCA.","1a295d23":"#### `` 1. max_depth `` ","8e950623":"#### Get the distribution of churn vs non churn customers. Churn is 10.6% which indicates an unbalanced datasets.","0cd22931":"#### * Run Lasso with best Alpha Parameter","d3baee2b":"### Insights and Actions to be taken on outliers"}}