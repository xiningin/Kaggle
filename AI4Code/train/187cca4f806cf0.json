{"cell_type":{"deb98b7c":"code","4b103bcb":"code","0452b729":"code","4a9e41ad":"code","2c8cdad0":"code","32f5c2f0":"code","a38f243f":"code","7da6246b":"code","cae93fdc":"code","893755ce":"code","db356bd0":"code","c05b75e2":"code","a9902611":"code","c882f12c":"code","e3bfcfc3":"code","cf5cb310":"code","104df184":"code","f1bb675a":"code","5bec56fd":"code","11396e45":"code","29a30c63":"code","0e9e40bd":"code","3d72f3f2":"code","6dc2d5a6":"code","198bfc28":"code","ef652b50":"code","0cfb2cc1":"code","6d4f8250":"code","2057f456":"code","8c19d91b":"code","c1906031":"code","0611419a":"code","1ef599d6":"code","6730c0cb":"code","74367607":"code","ba3d4f63":"code","4ebba79e":"code","601b379c":"code","b7b315fd":"markdown","ef20b635":"markdown","4a4719ec":"markdown","dd5b7ff5":"markdown","34ade745":"markdown","196b173c":"markdown","99bd8c11":"markdown","c7ed3f1d":"markdown","ee0a76cf":"markdown","da7e10c0":"markdown","334d0e4c":"markdown","a36c6286":"markdown","a4ef07d0":"markdown","105398b7":"markdown","655f2523":"markdown"},"source":{"deb98b7c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4b103bcb":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","0452b729":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-prediction-dataset\/Breast_cancer_data.csv')","4a9e41ad":"df.head()","2c8cdad0":"df.describe()","32f5c2f0":"df.info()","a38f243f":"missing_values_count = df.isnull().sum()\n\ntotal_cells = np.product(df.shape)\n\ntotal_missing = missing_values_count.sum()\n\npercentage_missing = (total_missing\/total_cells)*100\nprint(percentage_missing)","7da6246b":"from sklearn.ensemble import RandomForestRegressor","cae93fdc":"x = df.copy()\ny = x.pop('diagnosis')","893755ce":"#Label encoding from categorical featurtes\nfor colname in x.select_dtypes('object'):\n    x[colname],_ = x[colname].factorize()\n    \ndiscrete_features = x.dtypes ==int","db356bd0":"from sklearn.feature_selection import mutual_info_regression\n\ndef make_mi_scores(x, y, discrete_features):\n    mi_scores = mutual_info_regression(x, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=x.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(x, y, discrete_features)\nmi_scores[::1]","c05b75e2":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\n\nplt.figure(dpi=100, figsize=(5,5))\nplot_mi_scores(mi_scores)","a9902611":"sns.relplot(x='mean_perimeter', y='mean_area',hue='diagnosis', data=df)","c882f12c":"sns.lmplot(x='mean_smoothness', y='mean_texture',hue='diagnosis', data=df)","e3bfcfc3":"categorical_val = []\ncontinous_val = []\nfor column in df.columns:\n    if len(df[column].unique()) <= 10:\n        categorical_val.append(column)\n    else:\n        continous_val.append(column)","cf5cb310":"plt.figure(figsize=(20,20))\n\nfor i, column in enumerate(continous_val, 1):\n    plt.subplot(6, 6, i)\n    df[df[\"diagnosis\"] == 0][column].hist(bins=40, color='green', label='Breast Cancer = Benign', alpha=1,width=0.2)\n    df[df[\"diagnosis\"] == 1][column].hist(bins=40, color='red', label='Breast Cancer = Malignant', alpha=1,width=0.2)\n    plt.legend()\n    plt.xlabel(column)","104df184":"def draw_histograms(dataframe, features, rows, cols):\n    fig=plt.figure(figsize=(20,20))\n    for i, feature in enumerate(features):\n        ax=fig.add_subplot(rows,cols,i+1)\n        dataframe[feature].hist(bins=20,ax=ax,facecolor='green')\n        ax.set_title(feature+\" Distribution\",color='Red')\n        \n    fig.tight_layout()  \n    plt.show()\ndraw_histograms(df,df.columns,6,6)","f1bb675a":"corr_matrix = df.corr()\nfig, ax = plt.subplots(figsize=(20, 20))\nax = sns.heatmap(corr_matrix,\n                 annot=True,\n                 linewidths=0.5,\n                 fmt=\".2f\",\n                 cmap=\"YlGnBu\");\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","5bec56fd":"r = np.random.RandomState(0)\ndf1 = pd.DataFrame(r.rand(15,15))\ncorr = df.corr()\ncorr.style.background_gradient(cmap='coolwarm')","11396e45":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score","29a30c63":"regression_model = LinearRegression()","0e9e40bd":"regression_model.fit(x,y)","3d72f3f2":"y_predict = regression_model.predict(x)","6dc2d5a6":"rmse = mean_squared_error(y, y_predict)\nr2 = r2_score(y, y_predict)","198bfc28":"print('slope : ', regression_model.coef_)\nprint('Intercept : ', regression_model.intercept_)\nprint('RMSE : ', rmse)\nprint('R2 :', r2)","ef652b50":"from sklearn.model_selection import  train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix","0cfb2cc1":"x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.3)","6d4f8250":"from sklearn.linear_model import LogisticRegression","2057f456":"model = LogisticRegression(solver='liblinear')","8c19d91b":"model.fit(x_train, y_train)","c1906031":"y_pred = model.predict(x_test)","0611419a":"y_pred","1ef599d6":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report","6730c0cb":"def print_score(clf, x_train, y_train, x_test, y_test, train=True):\n    if train:\n        pred = clf.predict(x_train)\n        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(x_test)\n        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","74367607":"print_score(model, x_train, y_train, x_test, y_test, train=True)\nprint_score(model, x_train, y_train, x_test, y_test, train=False)","ba3d4f63":"test_score = accuracy_score(y_test, model.predict(x_test)) * 100\ntrain_score = accuracy_score(y_train, model.predict(x_train)) * 100\n\nresults_df = pd.DataFrame(data=[[\"Logistic Regression\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df","4ebba79e":"model.predict(x_train[:10])","601b379c":"y_train[:10]","b7b315fd":"### Data Visualisation","ef20b635":"## Model Predictions","4a4719ec":"## show a few features with their Mutual Info scores","dd5b7ff5":"## Information about the Dataset","34ade745":"#### the above graph shows that on increasing perimeter and increasing area person geting chance to cancer is malignant","196b173c":"#### the above graph shows that smoothness does not effect more on cancer but mean texture is increased malignant cases increased","99bd8c11":"## Total percentage of data is missing","c7ed3f1d":"## Model Accuracy","ee0a76cf":"### Correlation Matrix","da7e10c0":"## Data Visualization With most Dependent Features","334d0e4c":"## Apply linear Regression","a36c6286":"## Model Acuracy , confusion matrix and classification report","a4ef07d0":"## Apply Logistic Regression","105398b7":"## Divide the data into train and test","655f2523":"### Label encoding from categorical featurtes"}}