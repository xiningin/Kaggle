{"cell_type":{"187f1303":"code","bcf2ca03":"code","cc9f28f4":"code","06ec57f8":"code","e5e551ab":"code","7fcb794f":"code","43fb4108":"code","5612c646":"code","368f1407":"code","710122dd":"code","90e52e68":"code","da56a7af":"code","3a7cf366":"code","45daa447":"code","2b05df1c":"code","c78af47a":"code","0f9be4c2":"code","8bc1bdb8":"code","88a99391":"code","e02f345f":"code","fbd256bb":"code","bcd94073":"code","cc8a3601":"code","4cfb791d":"code","d319d268":"code","9d4115cd":"code","2d16153e":"code","c71b755f":"code","0a2ccf74":"code","c0ec20bf":"code","055e86b5":"code","2558abfb":"code","86076c65":"code","07786231":"code","a175287a":"code","ee2f6c8d":"code","50ccd1ff":"code","762b8dcd":"code","3a869659":"code","c88e9711":"code","48235986":"code","7bcfa991":"code","d4f88279":"code","97c5790f":"code","75d703c8":"code","83d44c40":"code","455e53ee":"code","0fd64eec":"code","dd91fdde":"code","35807c35":"code","64c5db2a":"code","59ebb98f":"code","a68d9415":"code","a937f018":"code","6bf5a383":"code","12e2aab5":"code","1d9ace8f":"code","babc1538":"code","3eefebc2":"code","be06373f":"code","fade4843":"code","ee597cef":"code","fc4fc841":"code","bf7f7b6b":"code","29521c3c":"code","7bb6cf63":"markdown","4d39d4d8":"markdown","71bd1590":"markdown","fdc6278a":"markdown","dd3c2681":"markdown","c829ab7c":"markdown","8bda077a":"markdown","09ae73c7":"markdown"},"source":{"187f1303":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","bcf2ca03":"#Providing a passingmark criteria which will be used to categorize the students\npassmarks = 40 \n#Reading the Data from a local repo in the system\ndf = pd.read_csv(\"..\/input\/StudentsPerformance.csv\")","cc9f28f4":"# Let's Get some max,min,standard deviation for the Data Frame\ndf.describe()","06ec57f8":"#Also let's check for any missing values if in Data Set\ndf.isnull().sum()\n# We find that no such missing values are there which will not be the case everytime. \n#But for Now since there are None....Let's continue with it.","e5e551ab":"# Let Us Explore Math Score at First Instance# Let U \np = sns.countplot(x=\"math score\" , data=df , palette = \"muted\")\n_ = plt.setp(p.get_xticklabels(),rotation = 90)","7fcb794f":"# Let's find the number of students Passed and failed according to the passing Score:\ndf['MathPassingStatus'] = np.where(df['math score'] < passmarks , 'Failed!' , 'Passed!')\ndf.MathPassingStatus.value_counts()","43fb4108":"#Let's Plot a Graph for Passed Students:\np = sns.countplot(x='parental level of education' , data = df , hue = 'MathPassingStatus' , palette = 'bright')\n_ = plt.setp(p.get_xticklabels(), rotation = 90)\n#Here we plot the graph in context to the Parental level of Education and depending upon that, showing the Number of Students passed or failed.","5612c646":"#Now exploring the Writing Score:\np= sns.countplot(x = \"writing score\" , data = df , palette = \"muted\")\n_ = plt.setp(p.get_xticklabels(),rotation = 90)","368f1407":"#Here We are Analyzing on the Attribute of Lunch:#Here We \np = sns.countplot(x='lunch' , data =df , hue = 'MathPassingStatus' , palette = 'bright')\n_ = plt.setp(p.get_xticklabels(),rotation = 90)","710122dd":"#Similarly going for race\/ethnicity:\np = sns.countplot(x='race\/ethnicity' , data = df , hue = 'MathPassingStatus' , palette = 'bright')\n_ = plt.setp(p.get_xticklabels(), rotation = 90)","90e52e68":"# Now students passing the Writing Exam:\ndf['WritingPassingStatus'] = np.where(df['writing score']<passmarks , 'Failed!','Passed!')\ndf.WritingPassingStatus.value_counts()","da56a7af":"#Plot for the Passed or failed, and seeing the Variation w.r.t Parental Level of Education:\np = sns.countplot(x='parental level of education' , data = df, hue = 'WritingPassingStatus', palette = 'bright')\n_ = plt.setp(p.get_xticklabels(),rotation =90)","3a7cf366":"#Now exploring the Writing Score:\np= sns.countplot(x = \"writing score\" , data = df , palette = \"muted\")\n_ = plt.setp(p.get_xticklabels(),rotation = 65)","45daa447":"#Here We are Analyzing on the Attribute of Lunch:\np = sns.countplot(x='lunch' , data =df , hue = 'WritingPassingStatus' , palette = 'bright')\n_ = plt.setp(p.get_xticklabels(),rotation = 65)","2b05df1c":"#Similarly going for race\/ethnicity:#Similar \np = sns.countplot(x='race\/ethnicity' , data = df , hue = 'WritingPassingStatus' , palette = 'bright')\n_ = plt.setp(p.get_xticklabels(), rotation = 60)","c78af47a":"# Similarly for the Reading Score:\np=sns.countplot(x=\"reading score\" , data =df,palette = \"muted\")\nplt.show()","0f9be4c2":"# Number of Students Passed??\ndf['ReadingPassStatus'] = np.where(df['reading score'] < passmarks , 'Failed!' , 'Passed!')\ndf.ReadingPassStatus.value_counts()","8bc1bdb8":"#Finding % of Marks:\ndf['Total_Marks'] = df['math score'] + df['reading score'] + df['writing score']\ndf['Percent'] = df['Total_Marks']\/3","88a99391":"#Let us Check how many Students totally passed in All Subjects:\ndf['OverAllPassingStatus'] = np.where(df.Total_Marks < 215 , 'Failed' , 'Passed!')\ndf.OverAllPassingStatus.value_counts()","e02f345f":"p =  sns.countplot(x=\"Percent\" , data = df , palette = \"muted\")\n_ = plt.setp(p.get_xticklabels(),rotation = 0)","fbd256bb":"#Let us do the grading for the students now:\ndef GetGrade(Percent,OverAllPassingStatus):\n    if(OverAllPassingStatus == 'Failed!'):\n        return 'Failed'\n    if(Percent >= 80):\n        return 'A'\n    if(Percent >= 70):\n        return 'B'\n    if(Percent >= 60):\n        return 'C'\n    if(Percent >= 50):\n        return 'D'\n    if(Percent >= 40):\n        return 'E'\n    else:\n        return 'Failed!'","bcd94073":"df['Grade'] = df.apply(lambda x: GetGrade(x['Percent'], x['OverAllPassingStatus']),axis =1)\ndf.Grade.value_counts()","cc8a3601":"#Plotting Grades in an Obtained Order\nsns.countplot(x=\"Grade\" , data=df,order = ['A','B','C','D','E','F'] , palette = \"muted\")\nplt.show()","4cfb791d":"#Plotting with variation of Perental Education:\np = sns.countplot(x='parental level of education', data=df,hue='Grade',palette = 'bright')\n_ = plt.setp(p.get_xticklabels(),rotation = 30)","d319d268":"#Lunch Variation\np = sns.countplot(x='lunch', data=df,hue='Grade',palette = 'bright')\n_ = plt.setp(p.get_xticklabels(),rotation = 30)","9d4115cd":"#Test Prep Course Variation\np = sns.countplot(x='test preparation course', data=df,hue='Grade',palette = 'bright')\n_ = plt.setp(p.get_xticklabels(),rotation = 30)","2d16153e":"#Race\/Ethnicity Variation\np = sns.countplot(x='race\/ethnicity', data=df,hue='Grade',palette = 'bright')\n_ = plt.setp(p.get_xticklabels(),rotation = 30)","c71b755f":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Getting Data Points for All Scores availbale\nML_DataPoints = pd.read_csv(filepath_or_buffer = \"..\/input\/StudentsPerformance.csv\",header = 0,\n                           usecols = ['math score','reading score','writing score'])\n#Getting Test Prep Course Values\nML_Labels = pd.read_csv(filepath_or_buffer = \"..\/input\/StudentsPerformance.csv\",header = 0,usecols=['test preparation course'])\n\n#Load MinMaxScaler\nMNScaler = MinMaxScaler()\nMNScaler.fit(ML_DataPoints) #Fitting the Scores\nT_DataPoints = MNScaler.transform(ML_DataPoints) #Transform the Scores\n\n\n#Load Label Encoder#Load La \nLEncoder = LabelEncoder()\nLEncoder.fit(ML_Labels)\nT_Labels = LEncoder.transform(ML_Labels)\n\n#Split the DATA SET\nXTrain,XTest,YTrain,YTest = train_test_split(T_DataPoints,T_Labels,random_state=10)\n\n#Apply Random Forest Classifier:\nRandomForest = RandomForestClassifier(n_estimators = 10,random_state=5)\n\nRandomForest.fit(XTrain,YTrain)","0a2ccf74":"RandomForest.fit(XTest,YTest)","c0ec20bf":"RandomForest.score(XTrain,YTrain)","055e86b5":"RandomForest.score(XTest,YTest)\n#We see the Model is Underfitting..!!","2558abfb":"model_now = LogisticRegression()\nmodel_now.fit(XTrain,YTrain)","86076c65":"y_pred = model_now.predict(XTest)","07786231":"from sklearn.metrics import accuracy_score\nac = accuracy_score(YTest,y_pred)\nprint(ac)","a175287a":"from sklearn.svm import SVC","ee2f6c8d":"model = SVC()\nmodel.fit(XTrain,YTrain)","50ccd1ff":"model.score(XTrain,YTrain)","762b8dcd":"model.score(XTest,YTest)\n#We found Almost Same Accuracy..!!","3a869659":"model_tree = DecisionTreeClassifier()\nmodel_tree.fit(XTrain,YTrain)","c88e9711":"model_tree.score(XTrain,YTrain)","48235986":"model_tree.score(XTest,YTest)","7bcfa991":"df.head()","d4f88279":"df.nunique()","97c5790f":"# Since we need to create dummy variables and from my inference, we don't need MathPassingStatus\/ReadingPassingStatus and WritingPassingStatus, so we will drop these.\n# Next we will dummy code the variables -> gender\/lunch\/test preparation course.\n# Next we will Label Encode the variables -> race\/ethinicity \/ parental level of education \/ Grade\n# The we will standardize the remaining Numerical Variables to bring them onto one scale for modelling.\nmarks_df = df.drop(['MathPassingStatus','WritingPassingStatus','ReadingPassStatus'],1)\nmarks_df.head()","75d703c8":"# Ok, as mentioned we have dropped the non-required columns, now let's perform some visualization for the dataframe\nsns.pairplot(marks_df);","83d44c40":"# So we see a Linear Relationship between the variables, let's plot their correlation\nsns.heatmap(marks_df.corr(),annot=True);\nplt.title('Correaltion for Marks Data Frame');","455e53ee":"marks_df = marks_df.drop('Total_Marks',axis=1)\n# Dropping highly correlated variables","0fd64eec":"# we see that Total Marks has high correlation for all the individual subjects and obviously, it is because it has been derived from the sum of all subjects, so we will keep it for modelling.\n# Next we will convert our categorical variables to Numerical Variables.\ndummy_df = pd.get_dummies(marks_df[['gender','test preparation course','lunch']],drop_first=True)\nmarks_df = pd.concat([marks_df,dummy_df],axis = 1)\nmarks_df.head(50)","dd91fdde":"marks_df.info()\n# So now, we will drop the coulmns from which we have got dummies as they are now insignificant","35807c35":"marks_df = marks_df.drop(['gender','lunch','test preparation course'],axis=1)\nmarks_df.head()","64c5db2a":"# Next we go onto Label Enocding for the Variables -> race\/ethinicity , parental level of education , Grade and Overall Passing Status\nmarks_df['race\/ethnicity'] = marks_df['race\/ethnicity'].astype('category')\nmarks_df['parental level of education'] = marks_df['parental level of education'].astype('category')\nmarks_df['Grade'] = marks_df['Grade'].astype('category')\nmarks_df['OverAllPassingStatus'] = marks_df['OverAllPassingStatus'].astype('category')\nmarks_df.info()","59ebb98f":"#Group A->0,Group B->1,#Group C->2,Group D->3,Group E->4,\nmarks_df['race\/ethnicity'] = marks_df['race\/ethnicity'].cat.codes\n#associate's degree -> 0 , bachelor's degree -> 1, high school -> 2, master's degree - >3 , some college ->4 , some high school ->5\nmarks_df['parental level of education'] = marks_df['parental level of education'].cat.codes\n# A->0 , B->1,C->2,D->3,E->4,Failed ->5\nmarks_df['Grade'] = marks_df['Grade'].cat.codes\nmarks_df.head()","a68d9415":"marks_df.OverAllPassingStatus = marks_df.OverAllPassingStatus.cat.codes\n# Passed->1,Failed - >0\nmarks_df.head()","a937f018":"sns.heatmap(marks_df.corr(),annot=True)","6bf5a383":"# Dropping negatively high correlated variables!\nmarks_df = marks_df.drop(['Grade','Percent'],axis=1)\nmarks_df.head()","12e2aab5":"# Now we will Standardize the untouches variables, which are: match score\/writing score\/readin score\/Total_Marks\/Percent\n# But after splitting the data!\ny = marks_df.OverAllPassingStatus\nX = marks_df.drop('OverAllPassingStatus',axis=1)\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)\nX_train.head() ","1d9ace8f":"cols_to_standardize = ['math score','writing score','reading score']\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train[cols_to_standardize] = scaler.fit_transform(X_train[cols_to_standardize])","babc1538":"X_train.head()","3eefebc2":"X_train.corr()","be06373f":"plt.figure(figsize=(16,9))\nsns.heatmap(X_train.corr(),annot=True);\nplt.title('Correlation for the training set');","fade4843":"#Let's see what is our passing rate\npassed = round(sum(marks_df.OverAllPassingStatus\/len(marks_df.OverAllPassingStatus.index))*100,2)\npassed","ee597cef":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nlogistic_reg = LogisticRegression()\nrfe = RFE(logistic_reg,5)\nrfe = rfe.fit(X_train,y_train)","fc4fc841":"rfe.support_","bf7f7b6b":"cols_we_need = X_train.columns[rfe.support_]\ncols_we_need","29521c3c":"# Let's now access the Stats Model as we have found the columns that are to be used. Also, we have standardised the columns too.\nimport statsmodels.api as sm\nX_train_sm = sm.add_constant(X_train[cols_we_need])\nmodel = sm.GLM(y_train,X_train_sm,family = sm.families.Binomial())\nres = model.fit()","7bb6cf63":"**Using SVM**","4d39d4d8":"## Ok, this is an error of Perfect Sepration, where one of our column is total biased. Looking for solution and will get back. Till then keep Kaggling!","71bd1590":"# Ok....So i did the above exploration as a Beginner, now practicing a lot, I have some specific steps to follow where out objective is to Achieve a **Logistic Regression** model to find out the pass\/fail students based on our chosen cutoff.\n\n## The improvements to be performed are:\n1. Deduce metrics such as \"Total Marks\" and \"Passed\/Failed\" for our dataframe, as our outcome variable is \"Passed\/Failed\".\n2. Next we will create dummy variables for the categorical variables, and then look for correlations, before creating dummy variables and after creating dummy variables.\n3. Apply Logistic Regression and find the variables, if applicable use RFE to find the variables which are TRUE, as they are selected from the RFE process.\n4. Use StatsModels to find the best variables,best R2_Score, Accuracy,Precision and Confusion Matrix.\n5. Provide Conclusion.","fdc6278a":"**Here We find....the Data is Overfitting the Model..!!**","dd3c2681":"**Decsion Tree Implementation..!!**","c829ab7c":"**Now We will Apply Several Machine Learning Algo's based on Understanding and will see the variation of each Algorithm**\nBeing a starter I am trying to bring all the attained and practiced knowledge towards the implementation for this Data Set..!!","8bda077a":"**Using Logistic Regression..!!**","09ae73c7":"Being a Starter....I am stucked here as which appropriate Algo will fit the Data to the best..!!\nThanks for Having a look and suggestions and improvements are always welcomed..!!\n"}}