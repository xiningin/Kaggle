{"cell_type":{"9f45818d":"code","64572a65":"code","2bd1f07a":"code","92555343":"code","1d0d9773":"code","0293e1bd":"code","637a58c3":"code","1e7bc431":"code","ad47cc6d":"code","3c7618cb":"code","b2b217c6":"code","1d591bf1":"code","d2c00dd4":"code","688fdcfe":"code","0c6bb69c":"code","b9ac0e09":"code","e67a58c0":"code","e9a43f2f":"code","265203b4":"code","6cb55ca2":"code","aa1bf40b":"code","b8af427f":"markdown","082ca972":"markdown","d615e489":"markdown","607a955d":"markdown","b0128838":"markdown","ee596e8b":"markdown","803811d1":"markdown","98a61521":"markdown","ce922061":"markdown"},"source":{"9f45818d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","64572a65":"import matplotlib.pyplot as plt\nimport seaborn as sns","2bd1f07a":"train = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\ntrain.head()","92555343":"train.shape","1d0d9773":"train.nunique()","0293e1bd":"train.isnull().sum()","637a58c3":"sns.histplot(train['pressure'], stat = 'probability', bins = 30, kde = 'True')","1e7bc431":"#Save id column to train_id\ntrain_id = train['id']\n# Drop id column from train dataset\ntrain.drop(['id'], axis=1, inplace=True)","ad47cc6d":"#Assign the pressure column as target\ntarget = train['pressure']\n#Drop 'pressure' from the train dataset\ntrain.drop(['pressure'], axis=1, inplace = True)","3c7618cb":"test = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')","b2b217c6":"test.head()","1d591bf1":"test.shape","d2c00dd4":"test.isnull().sum()","688fdcfe":"#Save test id column to test_id\ntest_id = test['id']\ntest.drop(['id'], axis=1, inplace=True)","0c6bb69c":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom sklearn import metrics\n\nX_train, X_test, y_train, y_test = train_test_split(train, target, test_size = 0.1)\n\nreg_lgb = lgb.LGBMRegressor(n_estimators = 2000)\nreg_lgb.fit(X_train,y_train)\npreds = reg_lgb.predict(X_test)\n\nlgb_score = metrics.mean_absolute_error(y_test, preds)\nlgb_score","b9ac0e09":"pip install scikit-optimize","e67a58c0":"# def optimize(params, param_names, x, y):\n#   params = dict(zip(param_names, params))\n#   model = lgb.LGBMRegressor(**params)\n#   kf = model_selection.KFold(n_splits = 5)\n#   accuracies = []\n#   for idx in kf.split(X=x, y=y):\n#     train_idx, test_idx = idx[0], idx[1]\n#     xtrain = x.iloc[train_idx]\n#     ytrain = y.iloc[train_idx]\n\n#     xtest = x.iloc[test_idx]\n#     ytest = y.iloc[test_idx]\n\n#     model.fit(xtrain, ytrain)\n#     preds = model.predict(xtest)\n#     fold_acc = metrics.mean_absolute_error(ytest, preds)\n#     accuracies.append(fold_acc)\n\n#   return np.mean(accuracies)","e9a43f2f":"# from functools import partial\n# from skopt import space\n# from skopt import gp_minimize\n# from sklearn import model_selection\n# from skopt import callbacks\n# from skopt.callbacks import CheckpointSaver\n# from scipy.stats import uniform as sp_uniform\n\n# checkpoint_saver = CheckpointSaver(\".\/checkpoint.pkl\", compress = 9)\n\n# param_space = [\n#                space.Categorical(['regression'], name = 'objective'),\n#                space.Integer(100,5000, name = 'n_estimators'),\n#                space.Integer(2,100, name = 'num_leaves'),\n#                space.Integer(1,50, name = 'min_data_in_leaf'),\n#                space.Integer(1,25, name = 'max_depth'),\n#                space.Real(0.0001, 0.1, name = 'learning_rate'),\n#                space.Real(0.01, 0.99, name = 'bagging_fraction'),\n#                space.Integer(1,20, name = 'bagging_freq'),\n#                space.Integer(1,10, name = 'bagging_seed'),\n#                space.Integer(2,100, name = 'max_bin'),\n#                space.Real(0.01, 0.99, name = 'feature_fraction'),\n#                space.Integer(1, 10, name = 'feature_fraction_seed'),\n#                space.Integer(1, 20, name = 'min_sum_hessian_in_leaf')\n# ]\n\n# param_names = [\n#                'objective',\n#                'n_estimators',\n#                'num_leaves',\n#                'min_data_in_leaf',\n#                'max_depth',\n#                'learning_rate',\n#                'bagging_fraction',\n#                'bagging_freq',\n#                'bagging_seed',\n#                'max_bin',\n#                'feature_fraction',\n#                'feature_fraction_seed',\n#                'min_sum_hessian_in_leaf'\n# ]\n\n# optimization_function = partial(\n#     optimize,\n#     param_names = param_names,\n#     x=train,\n#     y=target\n# )\n\n# result = gp_minimize(\n#     optimization_function,\n#     dimensions = param_space,\n#     n_calls = 50,\n#     n_random_starts = 10,\n#     n_jobs = -1,\n#     callback = [checkpoint_saver],\n#     random_state = 123,\n#     verbose = True,\n# )","265203b4":"# from skopt import load\n\n# result = load('.\/checkpoint.pkl')\n\n# print(\"\"\"Best parameters:\n# objective=%s,\n# n_estimators = %d,\n# num_leaves=%d,\n# min_data_in_leaf=%d,\n# max_depth=%d,\n# learning_rate=%.6f,\n# bagging_fraction=%f,\n# bagging_freq=%d,\n# bagging_seed=%d,\n# max_bin=%d,\n# feature_fraction=%f,\n# feature_fraction_seed=%d,\n# min_sum_hessian_in_leaf=%d\n# \"\"\" % (result.x[0], result.x[1],result.x[2], result.x[3],result.x[4],result.x[5],result.x[6],\n#         result.x[7],result.x[8],result.x[9],result.x[10],result.x[11],result.x[12])\n# )","6cb55ca2":"reg_lgb = lgb.LGBMRegressor(objective='regression',\nn_estimators = 2200,\nnum_leaves=70,\nmin_data_in_leaf=36,\nmax_depth=13,\nlearning_rate=0.078025,\nbagging_fraction=0.412706,\nbagging_freq=12,\nbagging_seed=2,\nmax_bin=41,\nfeature_fraction=0.624771,\nfeature_fraction_seed=4,\nmin_sum_hessian_in_leaf=6)\nreg_lgb.fit(train,target)","aa1bf40b":"preds = reg_lgb.predict(test)\noutput = pd.DataFrame({'id': test_id, 'pressure': preds})\noutput.to_csv('submission.csv', index=False)","b8af427f":"The test dataset has fewer rows than the train dataset.","082ca972":"<a id=\"Make Predictions\"><\/a>\n# Make Predictions","d615e489":"We can see that the target (pressure) is skewed to the right.","607a955d":"<a id=\"Using LightGBM in Google Brain- Ventilator Pressure Competition\"><\/a>\n# Using LightGBM in Google Brain- Ventilator Pressure Competition\n\nThis notebook is a straightforward implementation of LightGBM to make predictions of ventilator pressure in the Google Brain - Ventilator Pressure Competition[1].There are no missing values in the train dataset so those can be used directly for training the LightGBM Model without encoding. \n\nThis notebook includes hyperparameter tuning of the LightGBM model using scikit-optimize. \n\n[1]https:\/\/www.kaggle.com\/c\/ventilator-pressure-prediction\n\n","b0128838":"<a id=\"Train LightGBM Model\"><\/a>\n# Train LightGBM Model","ee596e8b":"<a id=\"Load Data\"><\/a>\n# Load Data","803811d1":"<a id=\"Hyperparameter Tuning\"><\/a>\n# Hyperparameter Tuning","98a61521":"Uncomment the next few cells to execute hyperparameter tuning","ce922061":"<a id=\"Plot the target variable\"><\/a>\n# Plot the target variable"}}