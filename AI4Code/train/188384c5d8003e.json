{"cell_type":{"85db5a73":"code","c842d18b":"code","f048f848":"code","dd5baace":"code","f31be95d":"code","fee2df44":"code","0c5aee07":"code","dd686cf1":"code","c8882ace":"code","9266e950":"code","925f7bae":"code","697c770a":"code","543ae775":"code","802d08c3":"markdown"},"source":{"85db5a73":"from sklearn import datasets\nimport pandas as pd","c842d18b":"iris = datasets.load_iris(return_X_y=False)","f048f848":"#Create numeric classes for species (0,1,2) \niris.loc[iris['Name']=='virginica','species']=0\niris.loc[iris['Name']=='versicolor','species']=1\niris.loc[iris['Name']=='setosa','species'] = 2\niris = iris[iris['species']!=2]","dd5baace":"#Create Input and Output columns\nX = iris[['PetalLength', 'PetalWidth']].values.T\nY = iris[['species']].values.T\nY = Y.astype('uint8'","f31be95d":"#Make a scatter plot\nplt.scatter(X[0, :], X[1, :], c=Y[0,:], s=40, cmap=plt.cm.Spectral);\nplt.title(\"IRIS DATA | Blue - Versicolor, Red - Virginica \")\nplt.xlabel('Petal Length')\nplt.ylabel('Petal Width')\nplt.show()","fee2df44":"def initialize_parameters(n_x, n_h, n_y):\n    \n    np.random.seed(2) # we set up a seed so that our output matches ours although the initialization is random.\n    \n    W1 = np.random.randn(n_h, n_x) * 0.01 #weight matrix of shape (n_h, n_x)\n    b1 = np.zeros(shape=(n_h, 1))  #bias vector of shape (n_h, 1)\n    W2 = np.random.randn(n_y, n_h) * 0.01   #weight matrix of shape (n_y, n_h)\n    b2 = np.zeros(shape=(n_y, 1))  #bias vector of shape (n_y, 1)\n       \n    #store parameters into a dictionary    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters","0c5aee07":"#Function to define the size of the layer\ndef layer_sizes(X, Y):\n    n_x = X.shape[0] # size of input layer\n    n_h = 6# size of hidden layer\n    n_y = Y.shape[0] # size of","dd686cf1":"#retrieve intialized parameters from dictionary    \ndef forward_propagation(X, parameters):\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    \n    \n    # Implement Forward Propagation to calculate A2 (probability)\n    Z1 = np.dot(W1, X) + b1\n    A1 = np.tanh(Z1)  #tanh activation function\n    Z2 = np.dot(W2, A1) + b2\n    A2 = 1\/(1+np.exp(-Z2))  #sigmoid activation function\n    \n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache","c8882ace":"def compute_cost(A2, Y, parameters):   \n    m = Y.shape[1] # number of training examples\n    \n    # Retrieve W1 and W2 from parameters\n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    \n    # Compute the cross-entropy cost\n    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))\n    cost = - np.sum(logprobs) \/ m\n    \n    return cost","9266e950":"def backward_propagation(parameters, cache, X, Y):\n# Number of training examples\n    m = X.shape[1]\n    \n    # First, retrieve W1 and W2 from the dictionary \"parameters\".\nW1 = parameters['W1']\n    W2 = parameters['W2']\n    ### END CODE HERE ###\n        \n    # Retrieve A1 and A2 from dictionary \"cache\".\n    A1 = cache['A1']\n    A2 = cache['A2']\n    \n    # Backward propagation: calculate dW1, db1, dW2, db2. \n    dZ2= A2 - Y\n    dW2 = (1 \/ m) * np.dot(dZ2, A1.T)\n    db2 = (1 \/ m) * np.sum(dZ2, axis=1, keepdims=True)\n    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n    dW1 = (1 \/ m) * np.dot(dZ1, X.T)\n    db1 = (1 \/ m) * np.sum(dZ1, axis=1, keepdims=True)\n    grads = {\"dW1\": dW1,\n             \"db1\": db1,\n             \"dW2\": dW2,\n             \"db2\": db2}\n    \n    return grads","925f7bae":"def update_parameters(parameters, grads, learning_rate=1.2):\n# Retrieve each parameter from the dictionary \"parameters\"\nW1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    \n    # Retrieve each gradient from the dictionary \"grads\"\n    dW1 = grads['dW1']\n    db1 = grads['db1']\n    dW2 = grads['dW2']\n    db2 = grads['db2']\n    \n    # Update rule for each parameter\n    W1 = W1 - learning_rate * dW1\n    b1 = b1 - learning_rate * db1\n    W2 = W2 - learning_rate * dW2\n    b2 = b2 - learning_rate * db2\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters","697c770a":"def nn_model(X, Y, n_h, num_iterations=10000, print_cost=False):\n    np.random.seed(3)\n    n_x = layer_sizes(X, Y)[0]\n    n_y = layer_sizes(X, Y)[2]\n    \n    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n    parameters = initialize_parameters(n_x, n_h, n_y)\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    \n    # Loop (gradient descent)\n    for i in range(0, num_iterations):\n        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n        A2, cache = forward_propagation(X, parameters)\n        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n        cost = compute_cost(A2, Y, parameters)\n        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n        grads = backward_propagation(parameters, cache, X, Y)\n        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n        parameters = update_parameters(parameters, grads)\n        # Print the cost every 1000 iterations\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after iteration %i: %f\" % (i, cost))\n        return parameters,n_h","543ae775":"parameters = nn_model(X,Y , n_h = 6, num_iterations=10000, print_cost=True)\n\ndef plot_decision_boundary(model, X, y):\n    # Set min and max values and give it some padding\n    x_min, x_max = X[0, :].min() - 0.25, X[0, :].max() + 0.25\n    y_min, y_max = X[1, :].min() - 0.25, X[1, :].max() + 0.25\n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    # Predict the function value for the whole grid\n    Z = model(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n    plt.ylabel('x2')\n    plt.xlabel('x1')\n    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y[0,:])\n    plt.title(\"Decision Boundary for hidden layer size \" + str(6))\n    plt.xlabel('Petal Length')\n    plt.ylabel('Petal Width') ","802d08c3":"<h1 align=\"center\">Assignment<\/h1>\n<h3 align=\"center\">Faisal Akhtar<\/h3>\n<h3 align=\"center\">Roll No.: 17\/1409<\/h3>\n<p>Machine Learning - B.Sc. Hons Computer Science - VIth Semester<\/p>\n<p>Compare performance of some classification algorithms like logistic regression and neural networks on Iris dataset and Seeds dataset (available on UCI).<\/p>"}}