{"cell_type":{"bad40526":"code","181e2c56":"code","92af6cf0":"code","1eff56db":"code","c4a03959":"code","000229fb":"code","d5df3d93":"code","667c6353":"code","a64772db":"code","f3a463de":"code","8353c7fa":"code","d3458ef5":"code","a888b44e":"code","b3cbb6db":"code","752951a1":"code","9347ba73":"code","d0e309a7":"code","2300019e":"code","1f579325":"code","0efd6468":"code","40c2b124":"code","876a376e":"code","8603b297":"code","58968eff":"code","4666aa66":"code","477b143a":"code","405b1807":"code","d060c761":"code","88325c18":"code","06d61a9b":"code","0a3b2523":"code","3776d325":"code","b94fc1d7":"code","34bdac7b":"code","a24f3011":"markdown","d8a2f545":"markdown","408f82ad":"markdown","8c919d5e":"markdown","c841f130":"markdown","47570689":"markdown","60ce07bb":"markdown","f2e9caff":"markdown","979de12a":"markdown","22e3d6e0":"markdown","0c17130c":"markdown","e16a0dbf":"markdown","95bfcd86":"markdown","33ceba9f":"markdown","77589a70":"markdown","8cc0038d":"markdown","5ed82d38":"markdown","65130b8c":"markdown","3b0b34fd":"markdown","1533d21d":"markdown","dbbc6856":"markdown","bba874dc":"markdown","9ecbeccf":"markdown","418cdf35":"markdown"},"source":{"bad40526":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","181e2c56":"train=pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/test.csv\")\nsamp_sub=pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv\")\nprint(train.shape,test.shape)","92af6cf0":"train.head(3)","1eff56db":"#checking if there are any empty rows\nprint(train.isnull().sum()),print(test.isnull().sum())","c4a03959":"#one missing value in train lets see what it is and remove it\nprint(train[train.text.isna()])\n\ntrain.dropna(axis=0,inplace=True)\n\nprint(train.shape)","000229fb":"import plotly.express as px\n\ndf1=pd.DataFrame(train[\"sentiment\"].value_counts()).reset_index()\ndf1[\"Label\"]=df1[\"index\"]\ndf1[\"Count of Sentences\"]=df1[\"sentiment\"]\ndf1=df1.sort_values(\"sentiment\", ascending=False)\nfig = px.bar(df1, x=\"Label\", y=\"Count of Sentences\", title=\"No. of sentences per label\",color=\"Count of Sentences\",text=\"Count of Sentences\")\nfig.update_traces( textposition='outside')\nfig.show()","d5df3d93":"#now lets look at sentence lengths\n\ntrain['text_lengths'] = train.apply(lambda row: len(row['text'].split(\" \")), axis=1)\npx.histogram(train['text_lengths'])","667c6353":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\ntrain[\"jaccard_score\"]= [jaccard(train.at[i,\"selected_text\"],train.at[i,\"text\"]) for i in train.index]","a64772db":"\ndf1=train[train[\"sentiment\"]==\"neutral\"]\npx.histogram(df1, x=\"jaccard_score\", color=\"sentiment\")","f3a463de":"#jaccard distribution for positive\ndf1=train[train[\"sentiment\"]==\"positive\"]\npx.histogram(df1, x=\"jaccard_score\", color=\"sentiment\")","8353c7fa":"#jaccard distribution for negative\ndf1=train[train[\"sentiment\"]==\"negative\"]\npx.histogram(df1, x=\"jaccard_score\", color=\"sentiment\")","d3458ef5":"from transformers import RobertaTokenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base',add_prefix_space=True)","a888b44e":"# converting everythin to lower case\ntrain['text'] = train['text'].apply(lambda x : str(x).lower())\ntrain['selected_text'] = train['selected_text'].apply(lambda x : str(x).lower())","b3cbb6db":"train","752951a1":"from sklearn.model_selection import train_test_split\nx_train, x_val , y_train , y_val = train_test_split(train[['text','sentiment']],train['selected_text'],test_size=0.2, random_state=42)\nx_train.shape, x_val.shape , y_train.shape , y_val.shape","9347ba73":"max_length=128\ncount = x_train.shape[0]\ninput_ids = np.zeros((count,max_length),dtype='int32')\nattention_mask = np.zeros((count,max_length),dtype='int32')\nstart_tokens = np.zeros((count,max_length),dtype='int32')\nend_tokens = np.zeros((count,max_length),dtype='int32')\ntoks_all = []\nfrom tqdm import tqdm\n\nfor i,sent in tqdm(enumerate(x_train.values)):\n    \n    #appending sentiment as a word and converting it into a tokenizer\n    nums = tokenizer.encode_plus(sent[0],sent[1],add_special_tokens=True,max_length=128,return_attention_mask=True,pad_to_max_length=True,return_tensors='tf',verbose=False)\n    input_ids[i]=nums[\"input_ids\"]\n    attention_mask[i] = nums['attention_mask']\n    \n    text1 = \" \"+\" \".join(sent[0].split())\n    text2 = \" \".join(y_train.values[i].split())\n    \n    \n    #finding the start index of selected text\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1 #fillining ones where the selected text is in original text\n    if text1[idx-1]==' ':\n        chars[idx-1] = 1\n\n\n    enc = tokenizer.encode(text1) \n    offsets = []\n    idx=0\n    for t in enc:\n        w=tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n\n    toks = []\n    for c,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0:\n            toks.append(c)\n    toks_all.append(toks)    \n    if len(toks)>0:\n        count+=1\n        start_tokens[i,(toks[0])+1] = 1\n        end_tokens[i,(toks[-1])+1] = 1","d0e309a7":"max_length=128\ncount = y_val.shape[0]\ninput_ids_val = np.zeros((count,max_length),dtype='int32')\nattention_mask_val = np.zeros((count,max_length),dtype='int32')\nstart_tokens_val = np.zeros((count,max_length),dtype='int32')\nend_tokens_val = np.zeros((count,max_length),dtype='int32')\ntoks_all = []\n\n\ncount=0\nfor i,each in tqdm(enumerate(x_val.values)):\n    val = tokenizer.encode_plus(each[0],each[1],add_special_tokens=True,max_length=128,return_attention_mask=True,pad_to_max_length=True,return_tensors='tf',verbose=False)\n    input_ids_val[i] = val['input_ids']\n    attention_mask_val[i] = val['attention_mask']\n    \n    \n    text1 = \" \"+\" \".join(each[0].split())\n    text2 = \" \".join(y_val.values[i].split())\n    \n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ':\n        chars[idx-1] = 1\n\n\n    enc = tokenizer.encode(text1) \n    offsets = []; idx=0\n    for t in enc:\n        w=tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n\n    toks = []\n    for c,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0:\n            toks.append(c)\n    toks_all.append(toks)    \n    if len(toks)>0:\n        count+=1\n        start_tokens_val[i,(toks[0])+1] = 1\n        end_tokens_val[i,(toks[-1])+1] = 1","2300019e":"print(input_ids.shape,attention_mask.shape,start_tokens.shape,end_tokens.shape)\nprint(input_ids_val.shape,attention_mask_val.shape,start_tokens_val.shape,end_tokens_val.shape)","1f579325":"from transformers import TFRobertaForQuestionAnswering\nroberta = TFRobertaForQuestionAnswering.from_pretrained('roberta-base')","0efd6468":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input,Softmax,Dense,Activation,Dropout\nimport tensorflow as tf\n\ninput1 = Input(shape=(max_length,),name='input_id',dtype=tf.int32)\ninput2 = Input(shape=(max_length,),name='attention_mask',dtype=tf.int32)\nstart_scores,end_scores = roberta(input1,attention_mask = input2)\ndense1 = Dense(units=max_length,activation='relu',name='dense1',kernel_regularizer = tf.keras.regularizers.L2(l2=0.00001))(start_scores)\ndrop1 = Dropout(0.4)(dense1)\ndense11 = Dense(units=max_length,activation='relu',name='dense11',kernel_regularizer = tf.keras.regularizers.L2(l2=0.00001))(drop1)\nsoftmax1 = Activation('softmax')(dense11)\ndense2 = Dense(units=max_length,activation='relu',name='dense2',kernel_regularizer = tf.keras.regularizers.L2(l2=0.00001))(end_scores)\ndrop2 = Dropout(0.4)(dense2)\ndense22 = Dense(units=max_length,activation='relu',name='dense22',kernel_regularizer = tf.keras.regularizers.L2(l2=0.00001))(drop2)\nsoftmax2 = Activation('softmax')(dense22)\n\nmodel = Model(inputs=[input1,input2],outputs=[softmax1,softmax2])\nmodel.summary()","40c2b124":"tf.keras.utils.plot_model(model, 'Model.png',show_shapes=True)","876a376e":"input_data = (input_ids,attention_mask)\noutput_data = (start_tokens,end_tokens)\n\nval = (input_ids_val,attention_mask_val)\noutput_val = (start_tokens_val,end_tokens_val)\nval_data = (val,output_val)","8603b297":"opt = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\nmodel.compile(optimizer=opt,loss='categorical_crossentropy')","58968eff":"train_dataset = tf.data.Dataset.from_tensor_slices((input_data, output_data)).shuffle(buffer_size=1024).batch(32)\nval_dataset = tf.data.Dataset.from_tensor_slices(val_data).batch(32)","4666aa66":"model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=\"model2.hdf5\",\n    save_weights_only=True,\n    monitor='val_loss',\n    mode=\"min\",\n    save_best_only=True)","477b143a":"model.fit(train_dataset,epochs=5,validation_data=val_dataset,callbacks=model_checkpoint_callback)","405b1807":"test['text'] = test['text'].apply(lambda x : str(x).lower())","d060c761":"max_length=128\ntest1=test[[\"text\",\"sentiment\"]]\ncount = test1.shape[0]\ninput_ids_tes = np.zeros((count,max_length),dtype='int32')\nattention_mask_tes = np.zeros((count,max_length),dtype='int32')\n\nfor i,each in tqdm(enumerate(test1.values)):\n    tes = tokenizer.encode_plus(each[0],each[1],add_special_tokens=True,max_length=128,return_attention_mask=True,pad_to_max_length=True,return_tensors='tf',verbose=False)\n    input_ids_tes[i] = tes['input_ids']\n    attention_mask_tes[i] = tes['attention_mask']","88325c18":"test_data=(input_ids_tes,attention_mask_tes)\nstart_pred , end_pred= model.predict((test_data))\nstart_pred.shape,end_pred.shape","06d61a9b":"strt_tes =[]\nend_tes=[]\nfor i in tqdm(range(start_pred.shape[0])):\n    s = tf.math.argmax(start_pred[i],axis=0).numpy()\n    e = tf.math.argmax(end_pred[i],axis=0).numpy()\n    strt_tes.append(s)\n    end_tes.append(e)\nprint(len(strt_tes),len(end_tes)) \n\npred_values = []\nfor i in tqdm(range(len(strt_tes))):\n    index1 = strt_tes[i]\n    index2 = end_tes[i] +2\n    pred = input_ids_tes[i][index1:index2]\n    mystring = tokenizer.decode(pred)\n    pred_values.append(mystring)","0a3b2523":"pred_values1=[i.split(\"<\/s>\")[0] for i in pred_values] ","3776d325":"pred_values1[0:5]","b94fc1d7":"samp_sub[\"selected_text\"]=pred_values1","34bdac7b":"samp_sub.to_csv(\"sub1.csv\",index=False)","a24f3011":"# Kaggle NLP series - 2nd Task - Tweet Sentiment Extraction\n\n### Hi Everyone, This is the second notebook in my NLP Kaggle series. In previous notebook we saw a simple toxic comment classification (https:\/\/www.kaggle.com\/sasidharturaga\/eda-step-wise-preprocess-and-lstm-classifier). \n### In this one lets find out the \"word or phrase\" from the tweet that exemplifies the provided sentiment\n\n# Do upvote if you find this helpful. Let's get started.\n\n### I am going to perform EDA and discuss different methods of tackling the problem\n\n## Brief description about the problem\n\n* A set of sentences and their sentiment labels (postive, negative and neutral) are given.\n* The task for us is to identify and extract the part of sentences which are mainly responsible for the sentiment associated with sentence.\n\n### Sentiment classification vs Sentiment Extraction\n\n![download.png](attachment:download.png)\n\n### In our task sentiment labels are given and we have to extract phrases responsible for that sentiment. So, Let's get started","d8a2f545":"# Test data preparation","408f82ad":"# Now lets discuss in how many ways this problem can be solved\n\n![image.png](attachment:image.png)","8c919d5e":"# A detailed picture of model and its flow","c841f130":"# Download pretrained RoBERTa QA model","47570689":"## Unlike neutral, \"Positive\" and \"Negative\" sentiments have somewat identical distribution of jaccard scores. \n\n","60ce07bb":"## Data prepration\n\n### Download pretrained RoBERTa tokenizer","f2e9caff":"## Steps:\n### 1) Split the data in to train and validation","979de12a":"## Jaccard distribution for positive sentiment sentences","22e3d6e0":"### Good, most of the sentences are falling within range 1-40. We cannot remove characters or any non-alphanumeric values from sentence beacuse in the challenge it is clearly mentioned that word or phrase should include all characters within that span (i.e. including commas, spaces, etc.)","0c17130c":"### Jaccard score is the evaluating metric for the task. Let's look at an example which explains how jaccard similarity works\n![image.png](attachment:image.png)\n\n## Now let's look at how jaccard distribution looks like for each sentiment","e16a0dbf":"## Model parameters and compiling","95bfcd86":"## Submission","33ceba9f":"## Jaccard distribution for neutral sentiment sentences","77589a70":"## So in conclusion, we observerd a detailed EDA and different ways of tackling this problem and finally a very clear way of finetuning pretrained roberta model. That's it for 2nd task guys. See you again in Task 3.","8cc0038d":"## I ran model only for 5 epochs. As you can see as epochs progress train and validation loss are getting REDUCED. Hyper parameter tuning and more epochs would bring more efficient model. Now lets see the predictions","5ed82d38":"## From above graphs its is evident that most of the \"selected texts\" having \"neutral\" sentiment are the \"entire sentences\" thats why most of the distribution of jaccard score is near 1.","65130b8c":"# Model prediction","3b0b34fd":"## Jaccard distribution for neutral sentiment sentences","1533d21d":"# Model training and validation","dbbc6856":"### 2) Tokenize data - convert string to list of numbers with pretrained tokenizer \n### 3) Prepare Attention mask - helps model in paying attention to actual tokens of sentences rather than padded tokens\n\n## In below two cells steps 2 and 3 are performed on train and validation data","bba874dc":"# Design model with tensorflow","9ecbeccf":"## Here we have text and sentiment column and we have to predict selected_text column. Now let's do some EDA regarding sentiment distribution and sentence lengths","418cdf35":"## **Import basic libraries and let's have a look at the data**"}}