{"cell_type":{"9e596e5a":"code","7cbdf17a":"code","d4845d76":"code","5fca442b":"code","beb17190":"code","47a8cf57":"code","d386bccd":"code","2b8e3110":"code","2c644727":"code","26b18fb3":"code","54ce777f":"code","6a85ec85":"code","160124ff":"code","cc78c26a":"code","1c19aaa6":"code","954aea2d":"code","76e54625":"code","927878f1":"code","1d461a0a":"code","e49d9d79":"code","33fd5780":"code","f3109503":"code","c60fd2cc":"code","79b001fe":"code","d49c107f":"code","d0a0e831":"code","287e9e2b":"code","3d836e1f":"code","90fd583e":"code","12598b54":"code","6e220cd5":"code","41652133":"code","a611d136":"code","5a0bd0d5":"code","bc97f96b":"code","7b684ba7":"code","fb5b0ae1":"code","fdcad312":"code","8f80af99":"code","242c6355":"code","f0b5050d":"code","80f7e33d":"code","acbaa9e4":"code","d8577c07":"code","93456a55":"markdown","29d98f4c":"markdown","b7a16ff6":"markdown","0e69a4b3":"markdown","4a90fdde":"markdown","0410b366":"markdown","2b805256":"markdown","926c4e55":"markdown","d003d26f":"markdown","b305db02":"markdown","5fc81c74":"markdown","e6b9c210":"markdown","aad295de":"markdown","aa5290fd":"markdown","1ace77ed":"markdown"},"source":{"9e596e5a":"!pip install transformers\n!pip install farasapy\n!pip install pyarabic\n!git clone https:\/\/github.com\/aub-mind\/arabert","7cbdf17a":"#Copy or download Farasa to the colab env","d4845d76":"#This command is usefull when the java runtime hangs after a runtime restart (colab issue)\n!pkill \"java\"","5fca442b":"#Checking for GPU\nimport torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n    !nvidia-smi\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","beb17190":"import pandas as pd\n#from py4j.java_gateway import JavaGateway\nfrom farasa.segmenter import FarasaSegmenter\nfrom arabert.preprocess_arabert import preprocess\nfrom sklearn.model_selection import train_test_split\n\n# !pkill \"java\"\n# gateway = JavaGateway.launch_gateway(classpath='.\/FarasaSegmenterJar.jar')\n# farasa_segmenter = gateway.jvm.com.qcri.farasa.segmenter.Farasa()\n\nfarasa_segmenter = FarasaSegmenter(interactive=True)\n\n","47a8cf57":"df_AJGT = pd.read_excel('..\/input\/tunisian-texts\/tun.xlsx',header=0)","d386bccd":"df_AJGT","2b8e3110":"#df_AJGT['data_labels'] = df_AJGT['data_labels'].astype(int)","2c644727":"df_AJGT.dtypes","26b18fb3":"DATA_COLUMN = 'texts'\nLABEL_COLUMN = 'data_labels'\n\ndf_AJGT = df_AJGT[['texts', 'data_labels']]\ndf_AJGT.columns = [DATA_COLUMN, LABEL_COLUMN]","54ce777f":"#label_map = {'Positive' : 0,'Negative' : 1}","6a85ec85":"#label_map ","160124ff":"df_AJGT[DATA_COLUMN] = df_AJGT[DATA_COLUMN].apply(lambda x: preprocess(x, do_farasa_tokenization=True , farasa=farasa_segmenter, use_farasapy = True))","cc78c26a":"df_AJGT[DATA_COLUMN]","1c19aaa6":"#df_AJGT[LABEL_COLUMN] = df_AJGT[LABEL_COLUMN].apply(lambda x: label_map[x])","954aea2d":"df_AJGT[LABEL_COLUMN] ","76e54625":"train_AJGT, test_AJGT = train_test_split(df_AJGT, test_size=0.2,random_state=42)","927878f1":"#%load_ext google.colab.data_table","1d461a0a":"test_AJGT","e49d9d79":"train_df = pd.DataFrame({\n    'id':range(len(train_AJGT)),\n    'label':train_AJGT[\"data_labels\"],\n    'alpha':['a']*train_AJGT.shape[0],\n    'text': train_AJGT[\"texts\"].replace(r'\\n', ' ', regex=True)\n})\n\ndev_df = pd.DataFrame({\n    'id':range(len(test_AJGT)),\n    'label':test_AJGT[\"data_labels\"],\n    'alpha':['a']*test_AJGT.shape[0],\n    'text': test_AJGT[\"texts\"].replace(r'\\n', ' ', regex=True)\n})\n\n!mkdir data\ntrain_df.to_csv(\"data\/train.tsv\",index=False,columns=train_df.columns,sep='\\t',header=False)\ndev_df.to_csv(\"data\/dev.tsv\",index=False,columns=dev_df.columns,sep='\\t',header=False)","33fd5780":"from __future__ import absolute_import, division, print_function\n\nimport glob\nimport logging\nimport os\nimport random\nimport json\nimport csv\nimport sys\nfrom io import open\nfrom multiprocessing import Pool, cpu_count\nfrom tqdm import tqdm\nimport numpy as np\n\nimport torch\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm import tqdm_notebook, trange\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\n\nfrom transformers import BertPreTrainedModel\nfrom transformers import BertModel\nfrom transformers import WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer                                  \nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nfrom arabert.preprocess_arabert import never_split_tokens\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)","f3109503":"class BertForSequenceClassification(BertPreTrainedModel):\n    def __init__(self, config, loss_fn=None):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        \n        self.bert = BertModel(config) #https:\/\/github.com\/huggingface\/transformers\/blob\/master\/src\/transformers\/modeling_bert.py#L594 \n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n        self.loss_fn = loss_fn\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n            Labels for computing the sequence classification\/regression loss.\n            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n            If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n    Returns:\n        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n            Classification (or regression if config.num_labels==1) loss.\n        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n    Examples::\n        from transformers import BertTokenizer, BertForSequenceClassification\n        import torch\n        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n        \"\"\"\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), logits, (hidden_states), (attentions)","c60fd2cc":"#Or you can use\n#from transformers import BertForSequenceClassification\n\n#https:\/\/github.com\/huggingface\/transformers\/blob\/master\/src\/transformers\/modeling_bert.py#L1107","79b001fe":"# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" BERT classification fine-tuning: utilities to work with GLUE tasks \"\"\"\ncsv.field_size_limit(2147483647)\n\nclass InputExample(object):\n    \"\"\"A single training\/test example for simple sequence classification.\"\"\"\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        \"\"\"Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\n\nclass InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id\n\n\nclass DataProcessor(object):\n    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n\n    def get_train_examples(self, data_dir):\n        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n        raise NotImplementedError()\n\n    def get_predict_examples(self, data_dir):\n        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n        raise NotImplementedError()\n\n    def get_labels(self):\n        \"\"\"Gets the list of labels for this data set.\"\"\"\n        raise NotImplementedError()\n\n    @classmethod\n    def _read_tsv(cls, input_file, quotechar=None):\n        \"\"\"Reads a tab separated value file.\"\"\"\n        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n            lines = []\n            for line in reader:\n                if sys.version_info[0] == 2:\n                    line = list(unicode(cell, 'utf-8') for cell in line)\n                lines.append(line)\n            return lines\n\n\nclass BinaryProcessor(DataProcessor):\n    \"\"\"Processor for the binary data sets\"\"\"\n\n    def get_train_examples(self, data_dir , train_file_name):\n        \"\"\"See base class. file should be a tsv\"\"\"\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, train_file_name)), \"train\")\n\n    def get_dev_examples(self, data_dir, dev_file_name):\n        \"\"\"See base class. file should be a tsv\"\"\"\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, dev_file_name)), \"dev\")\n\n    def get_predict_examples(self, data_dir, train_file_name):\n        \"\"\"See base class. file should be a tsv\"\"\"\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, train_file_name)), \"predict\")\n\n    def get_labels(self):\n        \"\"\"See base class.\"\"\"\n        return [\"0\", \"1\"]\n\n    def _create_examples(self, lines, set_type):\n        \"\"\"Creates examples for the training and dev sets.\"\"\"\n        examples = []\n\n        if(set_type!=\"predict\"):\n            for (i, line) in enumerate(lines):\n                guid = \"%s-%s\" % (set_type, i)\n                text_a = line[3]\n                label = line[1]\n                examples.append(\n                    InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        else:\n            for (i, line) in enumerate(lines):\n                guid = \"%s-%s\" % (set_type, i)\n                text_a = line[3]\n                label = '0'\n                examples.append(\n                    InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        return examples\n\n\ndef convert_example_to_feature(example_row, pad_token=0,\nsequence_a_segment_id=0, sequence_b_segment_id=1,\ncls_token_segment_id=1, pad_token_segment_id=0,\nmask_padding_with_zero=True, sep_token_extra=False):\n    example, label_map, max_seq_length, tokenizer, output_mode, cls_token_at_end, cls_token, sep_token, cls_token_segment_id, pad_on_left, pad_token_segment_id, sep_token_extra = example_row\n\n    tokens_a = tokenizer.tokenize(example.text_a)\n\n    tokens_b = None\n    if example.text_b:\n        tokens_b = tokenizer.tokenize(example.text_b)\n        # Modifies `tokens_a` and `tokens_b` in place so that the total\n        # length is less than the specified length.\n        # Account for [CLS], [SEP], [SEP] with \"- 3\". \" -4\" for RoBERTa.\n        special_tokens_count = 4 if sep_token_extra else 3\n        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - special_tokens_count)\n    else:\n        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n        special_tokens_count = 3 if sep_token_extra else 2\n        if len(tokens_a) > max_seq_length - special_tokens_count:\n            tokens_a = tokens_a[:(max_seq_length - special_tokens_count)]\n\n    # The convention in BERT is:\n    # (a) For sequence pairs:\n    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n    #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n    # (b) For single sequences:\n    #  tokens:   [CLS] the dog is hairy . [SEP]\n    #  type_ids:   0   0   0   0  0     0   0\n    #\n    # Where \"type_ids\" are used to indicate whether this is the first\n    # sequence or the second sequence. The embedding vectors for `type=0` and\n    # `type=1` were learned during pre-training and are added to the wordpiece\n    # embedding vector (and position vector). This is not *strictly* necessary\n    # since the [SEP] token unambiguously separates the sequences, but it makes\n    # it easier for the model to learn the concept of sequences.\n    #\n    # For classification tasks, the first vector (corresponding to [CLS]) is\n    # used as as the \"sentence vector\". Note that this only makes sense because\n    # the entire model is fine-tuned.\n    tokens = tokens_a + [sep_token]\n    segment_ids = [sequence_a_segment_id] * len(tokens)\n\n    if tokens_b:\n        tokens += tokens_b + [sep_token]\n        segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n\n    if cls_token_at_end:\n        tokens = tokens + [cls_token]\n        segment_ids = segment_ids + [cls_token_segment_id]\n    else:\n        tokens = [cls_token] + tokens\n        segment_ids = [cls_token_segment_id] + segment_ids\n\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n\n\n    # Zero-pad up to the sequence length.\n    padding_length = max_seq_length - len(input_ids)\n    if pad_on_left:\n        input_ids = ([pad_token] * padding_length) + input_ids\n        input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n        segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n    else:\n        input_ids = input_ids + ([pad_token] * padding_length)\n        input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n        segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n\n    assert len(input_ids) == max_seq_length\n    assert len(input_mask) == max_seq_length\n    assert len(segment_ids) == max_seq_length\n\n    if output_mode == \"classification\":\n        label_id = label_map[example.label]\n    elif output_mode == \"regression\":\n        label_id = float(example.label)\n    else:\n        raise KeyError(output_mode)\n\n    return InputFeatures(input_ids=input_ids,\n                        input_mask=input_mask,\n                        segment_ids=segment_ids,\n                        label_id=label_id)\n  \n\ndef convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer, output_mode,\n                                 cls_token_at_end=False, sep_token_extra=False, pad_on_left=False,\n                                 cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\n                                 cls_token_segment_id=1, pad_token_segment_id=0,\n                                 mask_padding_with_zero=True):\n    \"\"\" Loads a data file into a list of `InputBatch`s\n        `cls_token_at_end` define the location of the CLS token:\n            - False (Default, BERT\/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n            - True (XLNet\/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n    \"\"\"\n\n    label_map = {label : i for i, label in enumerate(label_list)}\n\n    examples = [(example, label_map, max_seq_length, tokenizer, output_mode, cls_token_at_end, cls_token, sep_token, cls_token_segment_id, pad_on_left, pad_token_segment_id, sep_token_extra) for example in examples]\n\n    process_count = cpu_count()\n\n    with Pool(process_count) as p:\n        features = list(tqdm(p.imap(convert_example_to_feature, examples, chunksize=500), total=len(examples)))\n\n    return features\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that's truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n\n\nprocessors = {\n    \"binary\": BinaryProcessor\n}\n\noutput_modes = {\n    \"binary\": \"classification\"\n}","d49c107f":"def load_and_cache_examples(task, tokenizer, mode=\"train\"):\n    processor = processors[task]()\n    output_mode = args['output_mode']\n    \n    cached_features_file = os.path.join(args['data_dir'], f\"cached_{mode}_{args['cache_dir']}_{args['max_seq_length']}_{task}\")\n    \n    if os.path.exists(cached_features_file) and not args['reprocess_input_data']:\n        logger.info(\"Loading features from cached file %s\", cached_features_file)\n        features = torch.load(cached_features_file)\n               \n    else:\n        logger.info(\"Creating features from dataset file at %s\", args['data_dir'])\n        label_list = processor.get_labels()\n        if mode==\"train\":\n          examples=processor.get_train_examples(args['data_dir'],args['train_file_name']);\n        if mode==\"dev\":\n          examples=processor.get_dev_examples(args['data_dir'],args['dev_file_name']);\n        if mode==\"predict\":\n          examples=processor.get_predict_examples(args['data_dir'],args['pred_file_name'])\n        \n        if __name__ == \"__main__\":\n            features = convert_examples_to_features(examples, label_list, args['max_seq_length'], tokenizer, output_mode,\n                cls_token_at_end=bool(args['model_type'] in ['xlnet']),            # xlnet has a cls token at the end\n                cls_token=tokenizer.cls_token,\n                cls_token_segment_id=2 if args['model_type'] in ['xlnet'] else 0,\n                sep_token=tokenizer.sep_token,\n                sep_token_extra=bool(args['model_type'] in ['roberta']),           # roberta uses an extra separator b\/w pairs of sentences, cf. github.com\/pytorch\/fairseq\/commit\/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n                pad_on_left=bool(args['model_type'] in ['xlnet']),                 # pad on the left for xlnet\n                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n                pad_token_segment_id=4 if args['model_type'] in ['xlnet'] else 0)\n        \n        logger.info(\"Saving features into cached file %s\", cached_features_file)\n        torch.save(features, cached_features_file)\n        \n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n    if output_mode == \"classification\":\n        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n    elif output_mode == \"regression\":\n        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n\n    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n    return dataset","d0a0e831":"def train(model, tokenizer):\n    tb_writer = SummaryWriter(log_dir=f\"{args['log_dir']}\/{args['notes']}\/seq_len_{args['max_seq_length']}_BS_{args['train_batch_size']}_lr_{args['learning_rate']}\")\n    \n    train_dataset = load_and_cache_examples(args['task_name'], tokenizer , mode=\"train\")\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args['train_batch_size'])\n    \n    num_training_steps = len(train_dataloader) * args['num_train_epochs']\n    \n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = ['bias', 'LayerNorm.weight']\n\n    #model.named_parameters(): returns the name of the parameter as well as the parameter itself\n    #'bias', 'LayerNorm.weight' should have zero decay\n    optimizer_grouped_parameters = [\n        {'params': [param for name, param in model.named_parameters() if not any(nd in name for nd in no_decay)],\n         'weight_decay': args['weight_decay'],\n        },\n        {'params': [param for name, param in model.named_parameters() if any(nd in name for nd in no_decay)],\n         'weight_decay': 0.0,\n        }\n        ]\n    \n    #correct_bias (bool): can be set to False to avoid correcting bias in Adam (e.g. like in Bert TF repository). Default True.\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], correct_bias=True)\n    # PyTorch scheduler\n    # https:\/\/huggingface.co\/transformers\/main_classes\/optimizer_schedules.html\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args['warmup_steps'], num_training_steps=num_training_steps)     \n\n    if args['fp16']:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\"Please install apex from https:\/\/www.github.com\/nvidia\/apex to use fp16 training.\")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args['fp16_opt_level'])\n        \n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Num examples = %d\", len(train_dataset))\n    logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n    logger.info(\"  Total train batch size  = %d\", args['train_batch_size'])\n    logger.info(\"  Total optimization steps = %d\", num_training_steps)\n\n    global_step = 0\n    tr_loss, logging_loss = 0.0, 0.0\n    model.zero_grad()\n\n    train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")    \n    for epoch in train_iterator:\n        epoch_iterator = tqdm_notebook(train_dataloader, desc=\"Iteration\")\n        for step, batch in enumerate(epoch_iterator):\n\n            #model.train() tells your model that you are training the model.\n            #So effectively layers like dropout, batchnorm etc. which behave different on the train and test procedures know what is going on and hence can behave accordingly\n            model.train()\n\n            batch = tuple(t.to(device) for t in batch)\n\n            inputs = {'input_ids':      batch[0],\n                      'attention_mask': batch[1],\n                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n                      'labels':         batch[3]}\n\n            outputs = model(**inputs)\n\n            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n            print(\"\\r%f\" % loss, end='')\n\n            if args['fp16']:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args['max_grad_norm'])                \n            else:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n\n            tr_loss += loss.item() #trainning loss\n                \n            optimizer.step()\n            scheduler.step()  # Update learning rate schedule\n            model.zero_grad()\n            global_step += 1\n\n            if args['logging_steps'] > 0 and global_step % args['logging_steps'] == 0:\n                # Log metrics\n                tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n                tb_writer.add_scalar('loss', (tr_loss - logging_loss)\/args['logging_steps'], global_step)\n                logging_loss = tr_loss\n\n            if args['save_steps'] > 0 and global_step % args['save_steps'] == 0:\n                # Save model checkpoint\n                output_dir = os.path.join(args['output_dir'], 'checkpoint-{}'.format(global_step))\n                if not os.path.exists(output_dir):\n                    os.makedirs(output_dir)\n                model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed\/parallel training\n                model_to_save.save_pretrained(output_dir)\n                logger.info(\"Saving model checkpoint to %s\", output_dir)\n\n        if args['evaluate_during_training']:  # Only evaluate when single GPU otherwise metrics may not average well\n            results = evaluate(model, tokenizer)\n            for key, value in results.items():\n                tb_writer.add_scalar('eval_{}'.format(key), value, epoch)\n\n    return global_step, tr_loss \/ global_step","287e9e2b":"from sklearn.metrics import (mean_squared_error, matthews_corrcoef, confusion_matrix,\n                             f1_score, precision_score , recall_score , accuracy_score)\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\ndef get_mismatched(labels, preds):\n    mismatched = labels != preds\n    examples = processor.get_dev_examples(args['data_dir'],args['dev_file_name'])\n    wrong = [i for (i, v) in zip(examples, mismatched) if v]    \n    return wrong\n\ndef get_eval_report(labels, preds):\n    mcc = matthews_corrcoef(labels, preds)\n    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n    return {\n        \"mcc\": mcc,\n        \"tp\": tp,\n        \"tn\": tn,\n        \"fp\": fp,\n        \"fn\": fn\n    }, get_mismatched(labels, preds)\n\ndef compute_metrics(task_name, preds, labels):\n\n    assert len(preds) == len(labels)\n    print(classification_report(labels,preds))\n    print(confusion_matrix(labels,preds))\n\n    f1_Positive = f1_score(labels,preds,pos_label=1,average='binary')\n    f1_Negative = f1_score(labels,preds,pos_label=0,average='binary')\n    macro_f1 = f1_score(labels,preds,average='macro')\n    macro_precision = precision_score(labels,preds,average='macro')\n    macro_recall = recall_score(labels,preds,average='macro')\n    acc = accuracy_score(labels,preds)\n    return {\n        'f1_pos': f1_Positive,\n        'f1_neg': f1_Negative,\n        'macro_f1' : macro_f1, \n        'macro_precision': macro_precision,\n        'macro_recall': macro_recall,\n        'accuracy': acc\n    }\n    #return get_eval_report(labels, preds)\n\ndef evaluate(model, tokenizer, prefix=\"\"):\n    \n    eval_output_dir = args['output_dir']\n\n    results = {}\n    EVAL_TASK = args['task_name']\n\n    eval_dataset = load_and_cache_examples(EVAL_TASK, tokenizer, mode=\"dev\")\n    if not os.path.exists(eval_output_dir):\n        os.makedirs(eval_output_dir)\n\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n\n    # Eval!\n    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n    logger.info(\"  Num examples = %d\", len(eval_dataset))\n    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    preds = None\n    out_label_ids = None\n    for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n        model.eval()\n        batch = tuple(t.to(device) for t in batch)\n\n        with torch.no_grad():\n            inputs = {'input_ids':      batch[0],\n                      'attention_mask': batch[1],\n                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n                      'labels':         batch[3]}\n            outputs = model(**inputs)\n            tmp_eval_loss, logits = outputs[:2]\n\n            eval_loss += tmp_eval_loss.mean().item()\n        nb_eval_steps += 1\n\n        if preds is None:\n            preds = logits.detach().cpu().numpy()\n            out_label_ids = inputs['labels'].detach().cpu().numpy()\n        else:\n            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n\n    eval_loss = eval_loss \/ nb_eval_steps\n    if args['output_mode'] == \"classification\":\n        preds = np.argmax(preds, axis=1)\n    elif args['output_mode'] == \"regression\":\n        preds = np.squeeze(preds)\n\n    #result, wrong = compute_metrics(EVAL_TASK, preds, out_label_ids)\n    results = compute_metrics(EVAL_TASK, preds, out_label_ids)\n    \n    return results#, wrong","3d836e1f":"def predict(model, tokenizer, prefix=\"\"):\n    pred_output_dir = args['output_dir']\n  \n    results={}\n    PRED_TASK = args['task_name']\n  \n    pred_dataset = load_and_cache_examples(PRED_TASK, tokenizer, mode='predict')\n    if not os.path.exists(pred_output_dir):\n     os.makedirs(pred_output_dir)\n    \n  \n    pred_sampler = SequentialSampler(pred_dataset)\n    pred_dataloader = DataLoader(pred_dataset, sampler=pred_sampler, batch_size=args['eval_batch_size'])\n  \n    logger.info(\"***** Running prediction {} *****\".format(prefix))\n    logger.info(\"  Num examples = %d\", len(pred_dataset))\n    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n  \n    preds = None\n    for batch in pred_dataloader:\n     with torch.no_grad():\n        batch = tuple(t.to(device) for t in batch)\n        inputs = {'input_ids': batch[0],'attention_mask': batch[1],'token_type_ids': batch[2],'labels': batch[3]}\n      \n        outputs = model(**inputs)\n        _, logits = outputs[:2]\n    if preds is None:\n        preds = logits.detach().cpu().numpy()\n    else:\n        preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n\n    preds = np.argmax(preds, axis=1)\n      \n    return preds","90fd583e":"args = {\n    'data_dir': 'data\/',\n    'train_file_name': 'train.tsv',\n    'dev_file_name': 'dev.tsv',\n    'pred_file_name': 'dev.tsv',\n    'model_type':  'bert',\n    'model_name': 'aubmindlab\/bert-base-arabert',\n    'task_name': 'binary',\n    'output_dir': 'outputs_bert\/',\n    'cache_dir': 'cache',\n    'do_train': True,\n    'do_eval': True,\n    'fp16': False,\n    'fp16_opt_level': 'O1',\n    'max_seq_length': 128,\n    'output_mode': 'classification',\n    'train_batch_size': 16,\n    'eval_batch_size': 32,\n    'num_train_epochs': 1,\n    'weight_decay': 0,\n    'learning_rate': 2e-5,\n    'adam_epsilon': 1e-8,\n    'warmup_steps': 0,\n    'max_grad_norm': 1.0,\n    'log_dir':'\/logs',\n    'logging_steps': 0,\n    'evaluate_during_training': True,\n    'save_steps': 90,\n    'eval_all_checkpoints': True,\n    'overwrite_output_dir': True,\n    'reprocess_input_data': False,\n    'notes': 'AJGT_arabert'\n}\nwith open('args.json', 'w') as f:\n    json.dump(args, f)\n\n!mkdir .\/{args['log_dir']}","12598b54":"MODEL_CLASSES = {\n    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n    #'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n    #'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n    #'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n}\nconfig_class, model_class, tokenizer_class = MODEL_CLASSES[args['model_type']]","6e220cd5":"#https:\/\/github.com\/huggingface\/transformers\/blob\/master\/src\/transformers\/configuration_bert.py#L52\nconfig = config_class.from_pretrained(args['model_name'], num_labels=2, finetuning_task=args['task_name'])\n\nmodel = model_class.from_pretrained(args['model_name'], output_attentions=True)\nmodel.to(device)\n\n#https:\/\/github.com\/huggingface\/transformers\/blob\/master\/src\/transformers\/tokenization_bert.py#L119\ntokenizer = tokenizer_class.from_pretrained(args['model_name'],\n    do_lower_case=False,\n    do_basic_tokenize=True,\n    never_split=never_split_tokens)","41652133":"print(never_split_tokens)","a611d136":"text = \"\u0648\u0644\u0646 \u0646\u0628\u0627\u0644\u063a \u0625\u0630\u0627 \u0642\u0644\u0646\u0627 \u0625\u0646 \u0647\u0627\u062a\u0641 \u0623\u0648 \u0643\u0645\u0628\u064a\u0648\u062a\u0631 \u0627\u0644\u0645\u0643\u062a\u0628 \u0641\u064a \u0632\u0645\u0646\u0646\u0627 \u0647\u0630\u0627 \u0636\u0631\u0648\u0631\u064a\"\ntext_preprocessed = preprocess(text, do_farasa_tokenization= False) #farasa=farasa\ntext_tokenized = tokenizer.tokenize(text_preprocessed)\n\nprint(\"Original text: \",text)\nprint(\"Preprocessed text: \",text_preprocessed)\nprint(\"Tokenized text: \",text_tokenized)","5a0bd0d5":"task = args['task_name']\n\nif task in processors.keys() and task in output_modes.keys():\n    processor = processors[task]()\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\nelse:\n    raise KeyError(f'{task} not found in processors or in output_modes. Please check utils.py.')","bc97f96b":"!rm -rf outputs_bert","7b684ba7":"global_step, tr_loss = train(model, tokenizer)\nlogger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n","fb5b0ae1":"#%load_ext tensorboard\n#%tensorboard --logdir {args['log_dir']}","fdcad312":"preds = predict(model,tokenizer)","8f80af99":"df_predictions = pd.concat((test_AJGT.reset_index(),pd.DataFrame(preds,columns=['predicted'])),axis=1,ignore_index=False,)","242c6355":"df_predictions","f0b5050d":"df_predictions['predicted'].value_counts() \n#chofet kifeh 5arejhom NAN , je comprends pas prq ??? bizaree , ey cha3malna tawa? savi lkhidma w ","80f7e33d":"df_predictions['predicted']","acbaa9e4":"df_predictions.dtypes","d8577c07":"df_arabert_predictions = df_predictions.to_csv('Result.csv',index = False)","93456a55":"##Training Function","29d98f4c":"##Train!!","b7a16ff6":"## Evaluate Function and Metrics","0e69a4b3":"#Installing dependencies","4a90fdde":"#Model Finetuning","0410b366":"##Predict","2b805256":"To do Farasa segmenting you will need FarasaSegmenter.jar in the same directory as the preprocess.py file \n\n(you can get the Farasa segmenter from http:\/\/qatsdemo.cloudapp.net\/farasa\/register.html)","926c4e55":"##Imports","d003d26f":"## Data Loading functions","b305db02":"##Define the Model Paramaters","5fc81c74":"##Creating and Configure Model","e6b9c210":"##Building the model","aad295de":"##Predict function","aa5290fd":"##Visualize Training Logs","1ace77ed":"#Reading and Preparing Data"}}