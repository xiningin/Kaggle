{"cell_type":{"e61279c9":"code","1a589f80":"code","1d4e5307":"code","79b5b424":"code","8d08c53c":"code","0ba9d316":"code","b619ac58":"code","2d1069a1":"code","fa1ebb78":"code","70869c5b":"code","567cebf7":"code","00dda3de":"code","79c5066a":"code","b748f9ec":"code","40acf641":"code","8f44b3f1":"code","4c2f360c":"code","0efd69ed":"code","df4c4f65":"code","19092bb8":"code","f21fdf5c":"code","408eb442":"code","e18eb0df":"code","81b9b687":"code","408319e4":"code","186543c2":"code","85477f65":"code","0d7bf7ed":"code","c682c5bf":"code","d8d98c93":"code","3d7becb9":"code","506f1ac8":"code","413359e7":"code","63082fb7":"code","35977d1f":"code","78cb194a":"code","0ac29ad9":"code","9a4c30a7":"code","31c6916b":"markdown","232e0a99":"markdown","2bfa955a":"markdown","a0bbf6d7":"markdown","64e8f7ff":"markdown","50c83302":"markdown","1a6e414d":"markdown","c0f1580f":"markdown","8e9ac5c1":"markdown","40281ab5":"markdown","23b9693c":"markdown","792bea97":"markdown","7dfa1d20":"markdown","8a244d91":"markdown","7ab30f28":"markdown","b6417660":"markdown","911e9b38":"markdown","0cbca0c5":"markdown","a4190aba":"markdown","e74ef35a":"markdown","9218e71b":"markdown","eba22f0f":"markdown","d74c147b":"markdown","4c149ee3":"markdown","35f0b777":"markdown"},"source":{"e61279c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        df= pd.read_csv(os.path.join(dirname, filename))\n\n\nprint(\"Rows and Columns count:\",df.shape)\ndf.head()\n","1a589f80":"df.info()","1d4e5307":"df.describe().style.format('{:0.2f}')\n# shape.format() - To aviod viewing data in exponential format","79b5b424":"df = pd.read_csv('\/kaggle\/input\/usa-cers-dataset\/USA_cars_datasets.csv',keep_default_na=False,na_values=['#','?','None'])","8d08c53c":"df.isnull().sum()\n#df.isnull().sum().sum() - To find total Null Values","0ba9d316":"#Drop Columns\ndf.drop(['Unnamed: 0'],axis=1,inplace=True)\ndf.head()","b619ac58":"#Replace Columns\ndf.replace({'condition': r' left$'},{'condition': ''},regex=True,inplace=True)\ndf.head(3)","2d1069a1":"#Mapping Datatime to Year column\n\nprint(\"Before mapping:\",df.dtypes['year'])\n\ndef lookup(a):\n    \n    \"\"\"\n    This is an extremely fast approach to datetime parsing through mapping.\n    For large data, the same years are often repeated. Rather than\n    re-parsing every record, we store all unique years, parse them in 'to_datetime', and\n    use a lookup to map all other records through Key-Value pairs.\n    \n    \"\"\"\n    \n    val = {i:pd.to_datetime(i,format='%Y') for i in a.unique()}\n    return a.map(val)\n\ndf['modified_year'] = lookup(df['year'])\n\nprint(\"After mapping: \",df.dtypes['modified_year'])\n","fa1ebb78":"# Changing dtype of 'title_status' to Categorical (ordinal)\n\nprint(\"Mmeory Usage of title_status before: \",df.title_status.memory_usage())\nprint(\"Dtype before:\", df.dtypes['title_status'] ,\", Unique values\",df.title_status.unique())\n","70869c5b":"lst = ['clean vehicle','salvage insurance']\ndf['title_status']=df.title_status.astype(pd.CategoricalDtype(lst,ordered=True))\n\nprint(\"Memory Usage of title_status after: \",df.title_status.memory_usage())\nprint(\"Dtype before:\", df.dtypes['title_status'] ,\", Unique values\",df.title_status.unique())","567cebf7":"df.head(3)","00dda3de":"#Standardization\n\ndf['kilometer'] = 1.609 * df['mileage']\n\n# check our transformed data\ndf.head(3)","79c5066a":"#Normalization\n\ndf['mileage'] = df['mileage'] \/ df['mileage'].max()\ndf['kilometer'] = df['kilometer'] \/ df['kilometer'].max()\ndf['price_mod'] = df['price'] \/ df['price'].max()\ndf[['mileage','kilometer','price_mod']].head()","b748f9ec":"df.year.value_counts().head(10)\n\n##shows significant increase in production after 2010, so lets group 9years together through bining","40acf641":"val = np.linspace(min(df['year']),max(df['year']),6,dtype='int')\nprint(val)\n\ngroup_names = ['1973-1982','1982-1991','1991-2001','2001-2010','2010-2020']","8f44b3f1":"df['year_binned'] = pd.cut(df['year'],bins=val,labels=group_names,include_lowest=True)\ndf.head(3)","4c2f360c":"print(\"shape before :\", df.shape)\ndf_numerical = df.select_dtypes(exclude=['object'])\nprint(\"shape after excluding object columns: \", df_numerical.shape)","0efd69ed":"df_numerical.head(3)\n#Keeping only the required numerical columns\n\ndf_numerical=df_numerical[['price_mod','kilometer']]","df4c4f65":"df_numerical.plot(kind='box',figsize=(6,6))\n\n'''\nIt is understandable as price as gone up with year and hence we could find outliers.\nAlso, kilometer too has huge outliers as better mileage cars are produced with time.\n\n'''","19092bb8":"# Let's compare price and year \n\ndf1=df[['price_mod','year_binned']]\ndf1.boxplot(column='price_mod',by='year_binned',figsize=(6,6))","f21fdf5c":"sns.distplot(df['price'])\nprint(\"Skewness: %f\" % df['price'].skew())","408eb442":"''' We see 95.8% occupancy in production during 2010-2020 '''\nprint(\"Decade wise occupancy\\n\",df.year_binned.value_counts(normalize=True))\n\nprint(\"\\n\")\n\n#Let's see for last 5 years\nprint(\"Last 5 years\\n\",df[df.year>2015].year.value_counts(normalize=True))\n\nresdf=df[df.year>2015].year.value_counts(normalize=True)\n\nresdf.plot.bar(x=resdf.index,y=resdf.values,rot=0,figsize=(8,4),color='c',label=\"Count\")\nplt.title(\"Last 5years Production Rate\",fontsize=15)\nplt.xlabel('Year',fontsize=12)\nplt.ylabel('Count',fontsize=12)\nplt.tight_layout()\nplt.legend()\nplt.show()\n","e18eb0df":"pt=df['brand'].value_counts()\npt.index","81b9b687":"fig = plt.figure(figsize=(12, 5))\nax = fig.add_axes([0,0,1,1])\nsns.countplot(x='brand', data=df, ax=ax)\n\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.tight_layout()\nplt.show()","408319e4":"#Finding sales of Top 5 brands \n\nprint(df.brand.value_counts(normalize=True).head(5))\ndfpt = df.brand.value_counts().reset_index().rename(columns={'index':'brand','brand':'count'}).head(5)","186543c2":"dfpt.plot.barh(x='brand',y='count')\nplt.xlabel('Brand')\nplt.ylabel('Count')\nplt.tight_layout()\nplt.show()\n","85477f65":"# Ford has highest production sales, let's look into it's top 'model' for last decade 2010-2020\n\nresdf=df[(df.brand=='ford') & (df.year_binned=='2010-2020')].groupby(['brand','model','year_binned']).size().to_frame('count').reset_index().sort_values(['count'],ascending=False)\n\n\n#Let's say we want to see top 'model' that has count more than 40, hence updating all lower count below 50 to 'Others'\n\nresdf.loc[resdf['count'] <= 50,'model']='Others'\nresdf=resdf.groupby(['model']).aggregate(np.sum).reset_index()  #suming all counts based on grouping the 'model'\nresdf.head(3)","0d7bf7ed":"#Sketching Bar and Pie Charts\n\nfig = plt.figure(figsize = (8,10))\naxes = fig.subplots(nrows=2)\nfig.subplots_adjust(hspace=0.7)\naxes[0].bar(resdf['model'],resdf['count'])\naxes[0].tick_params('x', labelsize=12)\naxes[0].set_title('Ford-Model', fontsize=18)\naxes[0].set_xlabel('Model Variants', fontsize=14, labelpad=14)\naxes[0].set_ylabel('Count of Model', fontsize=14)\n\naxes[1].pie(resdf['count'], labels=resdf['model'],  autopct='%.1f', textprops={'fontsize':14})\naxes[1].set_title('Percentage Distribution of Models', fontsize=18)","c682c5bf":"df1 = df[df.year>2015].groupby(['year','brand','modified_year']).size().to_frame('Count').reset_index()\n\ndf2=df1[df1['Count']>50] #Brands with more than 50 prduction annually \n\n\n#df3 for Bar Graph\ndf3=pd.pivot_table(df2,values='Count',index=['year'],columns=['brand'],fill_value=0)\n\n#df4 for Line Graph\ndf4=pd.pivot_table(df2,values='Count',index=['modified_year'],columns=['brand'],fill_value=0)","d8d98c93":"df3.plot.bar(rot=0,figsize=(8,4))\nplt.xlabel('Year',fontsize=14)\nplt.ylabel('Count',fontsize=14)","3d7becb9":"df4.plot.line(rot=0,figsize=(8,4))\nplt.xlabel('Year',fontsize=14)\nplt.ylabel('Count',fontsize=14)","506f1ac8":"df1=df.copy()\ndf1['average_year_price']=df1.groupby('year')['price'].transform('mean')\ndf1.head(3)","413359e7":"df1['average_state_price'] = df1.groupby('state')['price'].transform('mean')\ndf1 = df1[df1['country'] != ' canada']\ndf1['average_state_price'].head(3)","63082fb7":"fig = plt.figure(figsize=(12, 5))\nax = fig.add_axes([0,0,1,1])\nsns.barplot(x=df1['state'], y=df1['average_state_price'], ax=ax)\nplt.tight_layout()\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.show()","35977d1f":"df1=df.copy()\ndf1['average_color_price'] = df1.groupby('color')['price'].transform('mean')\ndf1 = df1[df1['country'] != ' canada']\ndf1.head(3)","78cb194a":"fig = plt.figure(figsize=(12, 5))\nax = fig.add_axes([0,0,1,1])\nsns.barplot(x=df1['color'], y=df1['average_color_price'], ax=ax)\nplt.tight_layout()\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.show()","0ac29ad9":" df.plot.scatter(x=\"mileage\", y=\"price\")","9a4c30a7":"matrix = np.triu(df.corr())\nsns.heatmap(df.corr(), annot=True, mask=matrix)","31c6916b":"#### Method 1:\n\nHandling while reading data through read_csv():\n\n1.keep_default_na = True (Default), Keeping False will retain the \u201d , \u2018#N\/A\u2019 , 'N\/A' values as it is, else True for 'Nan'\n\n2.na_values = [\"$Vvalues\"] , Pass values of columns or symbols for 'Nan'","232e0a99":"### E. Data Bining\n\nIt is the process of transforming continous numrical variable into discrete categorical 'bins', for grouped analysis.\n\nExample: in our dataset year is real value ranging from 1973 to 2020, it has 30 unique values.\n\nwhat if we care about the car price difference in two periods before the 2000 year and after 2000 year.\nwe will use the pandas method cut() to segement the year column into two bins: before_2008 and after_2008.","2bfa955a":"#### Inference - \n\ni. 2019 has seen twice the production sales of cars when comapred to 2018\n\nii. 2018 and 2017 almost had same rate of production of cars","a0bbf6d7":"# 2. Data Cleansing\n\n### A. Handling NULL Values","64e8f7ff":"### PLOTTING THE BREAKDOWN OF THE CARS SOLD BY BRAND","50c83302":"### PLOTTING THE DISTRIBUTION OF  CAR PRODUCTION OVER THE YEARS","1a6e414d":"''' We will retain the outliers in this scenario as its in acceptable range. (Could also remove them based on convenience) '''\n\n### Data Skewness:\n\nWhy do we care if the data is skewed? \n\nIf the response variable is skewed, the model will be trained on a much larger number of moderately priced cars, and will be less likely to successfully predict the price for the most expensive cars.\nThe concept is the same as training a model on imbalanced categorical classes. If the values of a certain independent variable (feature) are skewed, depending on the model, skewness may violate model assumptions (e.g. logistic regression) or may impair the interpretation of feature importance.","c0f1580f":"### C. Changing the Datatypes\n","8e9ac5c1":"Positive Skewness can be corrected by:\n\ni. Square Root\n\nii. Logarithmic\n\niii. Reciprocal","40281ab5":"# 1. Data Importing and Analyzing","23b9693c":"### Checking Avg. Price based on State-Wise distribution  - USA","792bea97":"### F. Detecting the Outliers\n\nMost of the times for Exploratory Data Analysis (EDA), outlier detection is an important segment, as, outlier for particular features may distort the true picture, so we need to disregard them. Specifically, outliers can play havoc when we want to apply machine learning algorithm for prediction. At the same time outliers can even help us for anomaly detection.\n\nSeaborn Box Plot: Box plot is a standard way of visualizing distribution of data based on median, quartiles and outliers.\n","7dfa1d20":"#### Inference - \n\ni. Year and Price are relatively correlated, meaning change in Year will impact the Price.\n\nii. Mileage and Year have poor correlation factor.","8a244d91":"#### Method 2:\n\nThe most adapted way is to check ( isnull() ) the Null values('Nan') and perform either of below option:\n\ni. dropna ( subset = ['col1','col2'] , how='all' ('any' Default) ) - with inplace set to True and subset set to a list of column names to drop all rows that contain NaN under those columns\n\nii. fillna (values or method='ffill') - Fill 'Nan' with custom based values ","7ab30f28":"# 3. Data Exploration and Visualization","b6417660":"### Analyzing the Heat Map","911e9b38":"### D. Data Standardization and Normalization:\n\nStandarization : It is the process of transforming data into a common format which allows the researcher to make meaningful comparisons.\n\nEaxmple:\nkilometer = 1.609 * mileage\n\n\n\nNormalization: It is the process of transforming values of several variables into similar range.typical normalization include scaling the variable.average is zero and variance is one, or scaling variable so the variable values range from 0 to 1\n\nExample: To demonstrate normalization,let's say we want scale the following features by dividing each value in the column by the maximum value in the column.\n\nColumns: mileage,kilometer_age,price approach","0cbca0c5":"### Price dependancy on Mileage","a4190aba":"### Checking Avg. Price based on Vehicle Color - USA","e74ef35a":"### Note: How to choose the right Visualization method for Analysis?\n\n1.Well it's important to know the correlation between the columns that you are dealing with. This can be found out using tht above HeatMap and find how closely two columns are related.\n\n2.Depending on their Column Type and also based on how the data is scattered, one can decide on the right visualization chart.","9218e71b":"#### Inference - \ni. Ford has been the leading Brand with more than 1200+ sales\n\nii. Nissan and Chevrolet had equal occupancy in sales but below Dodge ","eba22f0f":"#### Inference - \n\ni. Ford has the leading sales numbers compared to its competitors\n\nii. In 2018, Ford and Nissan almost had same number of models rolling out.","d74c147b":"### Let's compare top Brands","4c149ee3":"### B. Drop \/ Replace \/ Mapping -- UDF Based Actions\n\ni.   Let's Drop columns that are of no use\n\nii.  Let's Replace column values\n\niii. Let's map 'year' column to datetime","35f0b777":"#### Inference -\n\ni. Most cars ranging upto 20000$ have a better mileage between 0.1 to 0.3\n\nii. Cars ranging 20000$ and above have mileage less than 0.2\n\nNote: Mileage column had been normalized ranging from (0 to 1)"}}