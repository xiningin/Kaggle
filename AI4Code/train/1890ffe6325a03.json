{"cell_type":{"58333cde":"code","5398f815":"code","c09bc2b2":"code","11bda127":"code","d88d6f02":"code","b05eb42b":"code","3248af6c":"code","798935d6":"code","600b6008":"code","d02a42cc":"code","86862ff3":"code","e9ed9b77":"code","79ae7188":"code","3a05e1ab":"code","c65b127b":"code","1cdea351":"code","bfbec8ba":"code","d62841c1":"code","c038d2d3":"code","9a86a8de":"code","893cbf8e":"code","360bf94d":"code","a2dda91e":"code","4e149416":"code","d05cb1db":"code","886e7c22":"code","4c1ce030":"code","ca088b99":"code","f2b6c74a":"code","09f0f21a":"code","be6174f0":"code","ecad9356":"code","f46b7cb2":"code","0654039e":"code","731b5115":"code","e44f76d2":"code","c2d438e8":"code","147d0d86":"code","7aea03e7":"code","aa205b9d":"code","5981daeb":"code","fa404a15":"code","86b1430d":"code","4a5e523a":"code","c9dc7cb8":"code","bce05670":"code","28887c6a":"code","11e1870c":"code","09576181":"code","b7f923bc":"code","0745d00b":"code","f150afa0":"code","0f38237f":"code","2fb10559":"code","95e8095b":"code","e8b5c494":"code","df0cfe9f":"code","5fe91f79":"code","417e2048":"code","6a6ac446":"code","20a8a1db":"code","eb6993c5":"code","38c83813":"code","d767811a":"code","ac53466e":"code","88ee8a9f":"code","f3eaae91":"code","39667417":"code","c180a01f":"code","1fc1de7c":"code","622e3949":"code","bb5b2f70":"code","51fd48ee":"code","75bc2479":"code","a217d15d":"code","5832b758":"code","2aa3b9d2":"code","1b13dcfe":"code","a5ad362e":"code","31a551f8":"code","88c2c0f9":"code","0913cb92":"code","fd0ab0d1":"code","c579816a":"code","59bcbcfb":"markdown","a6e5c71c":"markdown","6270bf12":"markdown","25a094e4":"markdown","b0060c08":"markdown","c64b97d2":"markdown","0902c575":"markdown","570aa62e":"markdown","2cf04d3a":"markdown","368ec525":"markdown","cede7bf4":"markdown","35cf949b":"markdown","c8078cd3":"markdown","a068c8ca":"markdown","523c71a2":"markdown","0cb0fc9d":"markdown","e315ba23":"markdown","86a74c5a":"markdown","01043269":"markdown"},"source":{"58333cde":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5398f815":"# import all the tools we need\n\n\n# Regular EDA libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n\n# we want our plots to apear within notebooks\n%matplotlib inline\npd.set_option('display.float_format', lambda x: '%.2f' % x)\nsns.set_style(\"darkgrid\")\n\n\n# Models from scikit-learn & XGboost\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\n# Model Evaluation libraries\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, f1_score, make_scorer, accuracy_score\nfrom sklearn.metrics import plot_roc_curve\n# For Hyperparameter tunning of Xgboost\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe","c09bc2b2":"df = pd.read_csv(\"..\/input\/vehicle-claim-fraud-detection\/fraud_oracle.csv\")\ndf.head()","11bda127":"df.info()","d88d6f02":"df.describe().T","b05eb42b":"df[\"FraudFound_P\"].value_counts()","3248af6c":"plt.figure(figsize=(10,6))\ndf[\"FraudFound_P\"].value_counts().plot(kind=\"bar\", color=['salmon', 'lightblue'])","798935d6":"df[\"Sex\"].value_counts()","600b6008":"plt.figure(figsize=(10,6))\ndf[\"Fault\"].value_counts().plot(kind=\"bar\")\nplt.xticks(rotation = 0);","d02a42cc":"df.AccidentArea.value_counts()","86862ff3":"# Let's visualize the AccidentArea\nplt.figure(figsize=(12,6))\ndf.AccidentArea.value_counts().plot(kind=\"bar\",color=[\"salmon\",\"lightblue\"])\nplt.xticks(rotation=0)","e9ed9b77":"# Let's find out VehicleCategory\nplt.figure(figsize=(10,6))\ndf.VehicleCategory.value_counts().plot(kind=\"bar\",color=[\"green\",\"pink\",\"navy\"])","79ae7188":"df.AgeOfVehicle.value_counts()","3a05e1ab":"# Let's Visualize the age of Vehicle\nplt.figure(figsize=(12,6))\ndf.AgeOfVehicle.value_counts().plot(kind=\"bar\")\nplt.xticks(rotation=0);","c65b127b":"df.WitnessPresent.value_counts()","1cdea351":"# Let's Visulaize the WitnessPresent On Accident Site\nplt.figure(figsize=(12,7))\ndf.WitnessPresent.value_counts().plot(kind=\"bar\", color=[\"pink\",\"red\"])","bfbec8ba":"df.PoliceReportFiled.value_counts()","d62841c1":"# Let's Check out the Police Report Filed with barplot\nplt.figure(figsize=(12,7))\ndf.PoliceReportFiled.value_counts().plot(kind=\"bar\",color=[\"lightblue\",\"darkred\"])","c038d2d3":"df.DriverRating.value_counts()","9a86a8de":"df.VehiclePrice.value_counts()","893cbf8e":"df.corr()","360bf94d":"# Let's viualize the corr between independent variables and with dependant (target variable)\nplt.figure(figsize=(12,7))\nsns.heatmap(df.corr(), annot=True, cmap=\"plasma_r\")","a2dda91e":"# Let's check it with crosstab \n#(crosstab() function takes two or more lists, pandas series or dataframe columns and returns a frequency of each combination by default)\n\npd.crosstab(df.FraudFound_P, df.Sex)","4e149416":"# Create a plot of crosstab for FraudFound and Sex Columns\npd.crosstab(df.FraudFound_P, df.Sex).plot(kind=\"bar\",\n                                          color=[\"salmon\",\"lightblue\"],\n                                          figsize=(12,6))\n\nplt.title(\"Fraud Frequency For Gender\")\nplt.xlabel(\"Sex = Male & Female\")\nplt.ylabel(\"Amount\")\nplt.legend([\"Female\",\"Male\"])\nplt.xticks(rotation = 0);","d05cb1db":"# let's plot \npd.crosstab(df.FraudFound_P, df.Fault).plot(kind=\"bar\",\n                                            color = [\"salmon\",\"lightblue\"],\n                                            figsize=(12,7))\nplt.xticks(rotation = 0);","886e7c22":"# Let's check FraudFound with Age using barplot\ngpd_by_val=df.groupby('Age').agg({'FraudFound_P':'sum'}).reset_index()\n\nfig, (ax1) = plt.subplots(1,1,figsize=(22, 6))\ngrph =sns.barplot(x='Age', y='FraudFound_P', data = gpd_by_val, ax=ax1)\n\ngrph.set_xticklabels(grph.get_xticklabels(),\n                    rotation=0,\n                    horizontalalignment='right'\n                    );","4c1ce030":"# Let's Check the AgentType, Year, and daysPolicy with FraudFound using barplot\ngpd_val1=df.groupby('AgentType').agg({'FraudFound_P':'sum'}).reset_index()\ngpd_val2=df.groupby('Year').agg({'FraudFound_P':'sum'}).reset_index()\ngpd_val3=df.groupby('Days_Policy_Accident').agg({'FraudFound_P':'sum'}).reset_index()\n\n\nfig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(22, 6))\nsns.barplot(x='AgentType', y='FraudFound_P', data = gpd_val1, ax=ax1)\nsns.barplot(x='Year', y='FraudFound_P', data = gpd_val2, ax=ax2)\nsns.barplot(x='Days_Policy_Accident', y='FraudFound_P', data = gpd_val3, ax=ax3)","ca088b99":"#Let's Plot the PoliceReportFiled, Days_Policy_Claim & DayOfWeek with FraudFound using Barplot\ngpd_val4=df.groupby('PoliceReportFiled').agg({'FraudFound_P':'sum'}).reset_index()\ngpd_val5=df.groupby('Days_Policy_Claim').agg({'FraudFound_P':'sum'}).reset_index()\n\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(20, 7))\nsns.barplot(x='PoliceReportFiled', y='FraudFound_P', data = gpd_val4, ax=ax1)\nsns.barplot(x='Days_Policy_Claim', y='FraudFound_P', data = gpd_val5, ax=ax2)","f2b6c74a":"# Let's plot the WitnessPresent, WeekOfMonthClaimed & DayOFWeekClaimed with FraudFound using barplot\ngpd_val8=df.groupby('WeekOfMonthClaimed').agg({'FraudFound_P':'sum'}).reset_index()\ngpd_val9=df.groupby('DayOfWeekClaimed').agg({'FraudFound_P':'sum'}).reset_index()\n\n\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(22, 6))\nsns.barplot(x='WeekOfMonthClaimed', y='FraudFound_P', data = gpd_val8, ax=ax1)\nsns.barplot(x='DayOfWeekClaimed', y='FraudFound_P', data = gpd_val9, ax=ax2)\nplt.xticks(rotation=45);","09f0f21a":"# Let's plot DriverRating, NumberOfCars & WinessPresent with barplot\ngpd_val_10 = df.groupby(\"DriverRating\").agg({\"FraudFound_P\":\"sum\"}).reset_index()\ngpd_val_11 = df.groupby(\"NumberOfCars\").agg({\"FraudFound_P\":\"sum\"}).reset_index()\ngpd_val_12 = df.groupby(\"WitnessPresent\").agg({\"FraudFound_P\":\"sum\"}).reset_index()\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(24,7))\n\nsns.barplot(x=\"DriverRating\", y=\"FraudFound_P\", data = gpd_val_10, ax= ax1)\nsns.barplot(x=\"NumberOfCars\", y=\"FraudFound_P\", data = gpd_val_11, ax= ax2)\nsns.barplot(x='WitnessPresent', y='FraudFound_P', data = gpd_val_12, ax= ax3)     \n","be6174f0":"# Let's Plot the AddressChangeClaim & PastNumberOfClaims using Barplot\ngpd_val_12 = df.groupby(\"AddressChange_Claim\").agg({\"FraudFound_P\": \"sum\"}).reset_index()\ngpd_val_13 = df.groupby(\"PastNumberOfClaims\").agg({\"FraudFound_P\": \"sum\"}).reset_index()\n\nfig , (ax1, ax2) = plt.subplots(1,2, figsize=(20,7))\n\nsns.barplot(x=\"AddressChange_Claim\", y=\"FraudFound_P\", data=gpd_val_12, ax=ax1)\nsns.barplot(x=\"PastNumberOfClaims\", y=\"FraudFound_P\", data=gpd_val_13, ax=ax2)\n","ecad9356":"df.info()","f46b7cb2":"# We have 0 values only in Age column\nprint(df['Age'].unique()==0)\nlen(df[df['Age']==0])","0654039e":"# Make a copy of original dataframe for preprocessing\ndf_temp = df.copy()\n# Finding columns which contains strings\nfor labels, content in df_temp.items():\n    if pd.api.types.is_string_dtype(content):\n        print(labels)","731b5115":"# Now Let's Fill 0 value with median of the column\ndf_temp[\"Age\"] = df_temp[\"Age\"].replace(0,df[\"Age\"].median())","e44f76d2":"# Let's Check if we have 0 in Age\nprint(df_temp['Age'].unique()==0)\nlen(df_temp[df_temp['Age']==0])","c2d438e8":"# Now turn all string into categorical features\nfor labels, content in df_temp.items():\n    if pd.api.types.is_string_dtype(content):\n        df_temp[labels] = content.astype(\"category\").cat.as_ordered()","147d0d86":"# Let's Check how many columns changed into Category\ndf_temp.info()","7aea03e7":"# Turn Categorical Features values into numeric\nfor labels, content in df_temp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        # Add binary column to indicate whether sample had missing values \n       #df_temp[labels+\"_is_missing\"] = pd.isnull(content) # Though we don't have but we could've used if we had missing values\n        df_temp[labels] = pd.Categorical(content).codes","aa205b9d":"df_temp.head()","5981daeb":"df_temp.describe()","fa404a15":"# Split data into X & y\nX = df_temp.drop(\"FraudFound_P\",axis = 1)\ny = df_temp[\"FraudFound_P\"]","86b1430d":"X","4a5e523a":"y","c9dc7cb8":"# Setup random seed for reproduction\nnp.random.seed(42)\n\n# Split data into train and test dataset\nX_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                    test_size=0.2)\n","bce05670":"# Let's Check our Train and Test Split\nfrom collections import Counter\nprint(f\"Training target statistics: {Counter(y_train)}\")\nprint(f\"Testing target statistics: {Counter(y_test)}\")\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","28887c6a":"# Let's import the function\nfrom sklearn.utils import class_weight\nclass_weights = dict(zip(np.unique(y_train), class_weight.compute_class_weight(\n                                                                        class_weight='balanced',\n                                                                        classes= np.unique(y_train), \n                                                                        y = y_train)))\nclass_weights","11e1870c":"# Let's put our models into dictionary \nmodels = {\"Logistic Regression\": LogisticRegression(class_weight=class_weights,solver = 'liblinear'),\n          \"KNN\": KNeighborsClassifier(),\n          \"Random Forest Classifier\": RandomForestClassifier(class_weight=class_weights),\n          \"XGboost\": XGBClassifier()}\n\n# Let's create a function to fit and later score our models\ndef fit_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates the given machine learning models\n    \"\"\"\n    # random seed for reproduction\n    np.random.seed(42)\n    \n    # Let's create a empty dictionary to keep model score\n    model_score = {}\n    \n    # Let's loop through the models dictionary\n    for name, model in models.items():\n        # Fit the model\n        model.fit(X_train, y_train)\n        # Evaluate the score and append it\n        model_score[name] = model.score(X_test,y_test)\n    return model_score","09576181":"model_score = fit_score(models=models,\n                        X_train=X_train,\n                        X_test=X_test,\n                        y_train=y_train,\n                        y_test=y_test)\nmodel_score","b7f923bc":"# Save into DataFrame\nmodel_compare = pd.DataFrame(model_score,index=[\"Accuracy\"])\nmodel_compare","0745d00b":"# Let's Plot The Models and Compare\nmodel_compare.T.plot(kind=\"bar\")","f150afa0":"# First Let's tune KNN\ntrain_score = []\ntest_score  = []\n\n# Let's create a list for different neighbors\nneighbors = range(1, 21)\n\n# Setup knn instance\nknn = KNeighborsClassifier()\n\n# loop through different neighbors\nfor i in neighbors:\n    knn.set_params(n_neighbors = i)\n    \n    # Fit the model\n    knn.fit(X_train, y_train)\n    \n    # Update the training score list\n    train_score.append(knn.score(X_train, y_train))\n    \n    # Update the test score list\n    test_score.append(knn.score(X_test, y_test))","0f38237f":"# Let's Plot And Viusalize The KNN Tunned Model\nplt.plot(neighbors, train_score, label= \"Train score\")\nplt.plot(neighbors, test_score, label= \"Test score\")\nplt.xlabel(\"Neighbors\")\nplt.ylabel(\"Model Accuracy\")\nplt.legend()\n\nprint(f\"Maximum KNN score on Test data: {max(test_score)*100 :.2f}%\")","2fb10559":"# Create hyperparametergrid for LogisticRegression\n\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Create hyperparameterGrid for RandomForestClassifier\nrf_random_grid = {\"n_estimators\": np.arange(10,1000,50),\n                  \"max_depth\": [None, 3, 5, 10],\n                  \"min_samples_split\": np.arange(2, 20, 2),\n                  \"min_samples_leaf\": np.arange(1, 20, 2)} ","95e8095b":"# Let's Tune LogisticRegression\nnp.random.seed(42)\n\n# Setup Random Hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(class_weight=class_weights),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n\n# Fit random hyperparameter search\nrs_log_reg.fit(X_train, y_train)","e8b5c494":"# Let's Check The Best Parmeters\nrs_log_reg.best_params_","df0cfe9f":"# Let's Score the Tunned model\nrs_log_reg.score(X_test, y_test)","5fe91f79":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifer\nrf_rs = RandomizedSearchCV(RandomForestClassifier(class_weight=class_weights),\n                           param_distributions=rf_random_grid,\n                           cv=5,\n                           verbose=True,\n                           n_iter=10)\n\n# Fitting random hyperparamter search\nrf_rs.fit(X_train, y_train)","417e2048":"# Let's Check The Best Parmeters\nrf_rs.best_params_","6a6ac446":"# Let's Score the Tunned model\nrf_rs.score(X_test, y_test)","20a8a1db":"# Let's compare it default Models score\nmodel_score","eb6993c5":"# Setup random seed\nnp.random.seed(42)\n\n# Different  LogisticRegression Hyperparameter\nlog_reg_grid = {\"C\": np.logspace(-4,4,30),\n              \"solver\": [\"liblinear\"]}\n\n# Setup Grid Search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(class_weight=class_weights),\n                          param_grid=log_reg_grid,\n                          cv = 20,\n                          n_jobs=-1,\n                          verbose=True)\n\n# Fit the GridSearch instance\ngs_log_reg.fit(X_test, y_test)","38c83813":"# Let's Check The Best Parameters\ngs_log_reg.best_params_","d767811a":"# Let's score the grid search model of logistic Regression\ngs_log_reg.score(X_test,y_test)","ac53466e":"# Let's Create  a dictionary with random Parameters for all XGBoostClassifier parameters\nspace={\n    'objective': 'binary:logistic', \n    'use_label_encoder': False, \n    'base_score': 0.5,\n    'booster': 'gbtree',\n    'colsample_bylevel' : 1,\n    'colsample_bynode' : 1,\n    'colsample_bytree' : 1,\n    'enable_categorical': False,\n    'gamma': hp.uniform('gamma', 0,10),\n    'gpu_id': -1,\n    'importance_type': None,\n    'interaction_constraints': '', \n    'learning_rate': 0.300000012, \n    'max_delta_step': 0,\n    'max_depth': hp.randint(\"max_depth\", 10)+3,\n    'min_child_weight' : hp.randint('min_child_weight', 4)+1,\n    'monotone_constraints': '()',\n    'n_estimators': hp.randint('n_estimators', 150)+50,\n    'n_jobs': -1,\n    'num_parallel_tree':1, \n    'predictor':'auto', \n    'random_state': 0,\n    'reg_alpha' : hp.randint('reg_alpha', 10),\n    'reg_lambda' : hp.randint('reg_lambda', 10),\n    'scale_pos_weight': 1,\n    'subsample': 1,\n    'tree_method': 'exact',\n    'validate_parameters':1,\n    'verbosity': None,\n    'eval_metric': 'aucpr'\n    }","88ee8a9f":"# Let's Define a function for our Space Dictionary and train our model\ndef objective(space):\n    clf_model= XGBClassifier(**space)\n    \n    evaluation = [( X_train, y_train), ( X_test, y_test)]\n    \n    clf_model.fit(X_train, y_train,\n            eval_set=evaluation, eval_metric=\"auc\",\n            early_stopping_rounds=10,verbose=False)\n    \n\n    pred = clf_model.predict(X_test)\n    accuracy = accuracy_score(y_test, pred>0.5)\n    print (\"SCORE:\", accuracy)\n    return {'loss': -accuracy, 'status': STATUS_OK }","f3eaae91":"trials = Trials()\nbest_hyperparams = fmin(fn = objective,\n                        space = space,\n                        algo = tpe.suggest,\n                        max_evals = 100,\n                        trials = trials)","39667417":"# Let's Check the best hyperparameters\nbest_hyperparams","c180a01f":"# Let's Create The optimized model with best hyperparameters of XGboost Classifier\nclf_model_optimizied = XGBClassifier(\n    objective= 'binary:logistic', \n    use_label_encoder= False, \n    base_score= 0.5, \n    booster= 'gbtree', \n    colsample_bylevel= 1, \n    colsample_bynode= 1, \n    colsample_bytree= 1, \n    enable_categorical= False, \n    gamma= best_hyperparams['gamma'], \n    gpu_id= -1, \n    importance_type= None, \n    interaction_constraints= '', \n    learning_rate= 0.300000012, \n    max_delta_step= 0, \n    max_depth= best_hyperparams['max_depth'], \n    min_child_weight= best_hyperparams['min_child_weight'], \n    monotone_constraints= '()',\n    n_estimators= best_hyperparams['n_estimators'], \n    n_jobs= 4, \n    num_parallel_tree= 1, \n    predictor= 'auto', \n    random_state= 0, \n    reg_alpha= best_hyperparams['reg_alpha'], \n    reg_lambda= best_hyperparams['reg_lambda'], \n    scale_pos_weight= 1, \n    subsample= 1, \n    tree_method= 'exact', \n    validate_parameters= 1, \n    verbosity= None, \n    eval_metric= 'aucpr'\n)\nprint(clf_model_optimizied.get_params())","1fc1de7c":"# Let's Fit our optimized model\nxgb_model = clf_model_optimizied.fit(X_train, y_train)","622e3949":"# Let's Predict on our Optimized model\ny_preds = xgb_model.predict(X_test)\ny_preds","bb5b2f70":"# Let's plot ROC Curve and calculate the AUC metric\nplot_roc_curve(xgb_model, X_test,y_test);","51fd48ee":"print(confusion_matrix(y_test,y_preds))","75bc2479":"print(classification_report(y_test,y_preds));","a217d15d":"# Let's visualize the confusion matrix\n\ndef conf_plot(y_test, y_preds):\n    \"\"\"\n    Plots a nice looking heatmap on seaborn\n    \"\"\"\n    fix, ax = plt.subplots(figsize=(10,6))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True,\n                     cbar=True,\n                     fmt=\"g\");\n    plt.xlabel(\"Predicted Labels\")\n    plt.ylabel(\"True Labels\")\n\nconf_plot(y_test, y_preds)  ","5832b758":"# Checking the best_parameters\nxgb_model.get_params()","2aa3b9d2":"# Creating a new classifier with best parameters\n\nfinal_clf_xgb = XGBClassifier(objective= 'binary:logistic', \n                              use_label_encoder= False, \n                              base_score= 0.5, \n                              booster= 'gbtree', \n                              colsample_bylevel= 1, \n                              colsample_bynode= 1, \n                              colsample_bytree= 1, \n                              enable_categorical= False, \n                              gamma= best_hyperparams['gamma'], \n                              gpu_id= -1, \n                              importance_type= None, \n                              interaction_constraints= '', \n                              learning_rate= 0.300000012, \n                              max_delta_step= 0, \n                              max_depth= 8, \n                              min_child_weight= 0, \n                              monotone_constraints= '()',\n                              n_estimators= 41, \n                              n_jobs= 4, \n                              num_parallel_tree= 1, \n                              predictor= 'auto', \n                              random_state= 0, \n                              reg_alpha= 2, \n                              reg_lambda= 1, \n                              scale_pos_weight= 1, \n                              subsample= 1, \n                              tree_method= 'exact', \n                              validate_parameters= 1, \n                              verbosity= None, \n                              eval_metric= 'aucpr')","1b13dcfe":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Cross-validated accuracy\ncv_acc = cross_val_score(final_clf_xgb,\n                         X,\n                         y,\n                         cv=100,\n                         scoring=\"accuracy\")\n# Let's take over all mean of the accuracy\ncv_acc = np.mean(cv_acc)\nprint(f\"The Accuracy for our XGboost Classifier is: {cv_acc : .2f}%\")\n\n\n# Cross-validated Precision\ncv_precision = cross_val_score(final_clf_xgb,\n                         X,\n                         y,\n                         cv = 100,\n                         scoring=\"precision\")\n# Let's take over all mean of the Precision\ncv_precision = np.mean(cv_precision)\nprint(f\"The Precision for our XGboost Classifier is: {cv_precision : .2f}%\")\n\n\n# Cross-validated Recall\ncv_recall = cross_val_score(final_clf_xgb,\n                         X,\n                         y,\n                         cv = 100,\n                         scoring=\"recall\")\n# Let's take over all mean of the Recall\ncv_recall = np.mean(cv_recall)\nprint(f\"The Recall for our XGboost Classifier is: {cv_recall : .2f}%\")\n\n\n# Cross-validated f1-score\ncv_f1 = cross_val_score(final_clf_xgb,\n                         X,\n                         y,\n                         cv = 100,\n                         scoring=\"f1\")\n# Let's take over all mean of the Precision\ncv_f1 = np.mean(cv_f1)\nprint(f\"The f1-score for our XGboost Classifier is:{cv_f1 :.2f}%\")","a5ad362e":"print(classification_report(y_test,y_preds));","31a551f8":"# Let's Fit the final optimized model\nfinal_clf_xgb.fit(X_train, y_train)","88c2c0f9":"# Check coeffeficient eg.(how the independent variables (X_train) contributes to predict target variable (y))\nfinal_clf_xgb.feature_importances_","0913cb92":"# Here we will plot the F-Score with Features using XGboost built-in function\nplot_importance(final_clf_xgb, max_num_features=20);","fd0ab0d1":"# Helper function for plotting feature importance of our XGboost Classifier\n# We would only plot first top 20 features\ndef plot_features(columns, importances, n=20):\n    df_feat = (pd.DataFrame({\"features\":columns,\n                             \"features_importances\": importances})\n          .sort_values(\"features_importances\",ascending=False)\n          .reset_index(drop=True))\n    \n    # Plot the dataframe we created\n    fig, ax = plt.subplots(figsize=(12,7))\n    ax.barh(df_feat[\"features\"][:n], df_feat[\"features_importances\"][:20])\n    ax.set_ylabel(\"Features\")\n    ax.set_xlabel(\"Features Importance\")\n    ax.invert_yaxis()","c579816a":"plot_features(X_train.columns, final_clf_xgb.feature_importances_)","59bcbcfb":"# EDA (Exploratory Data Analysis)","a6e5c71c":"### Now Out Dataset is Ready For Algorithms ","6270bf12":"* Now We Need To Turn All Strings Into Numerical Values For Machine Learning Model","25a094e4":"## Let's calculate the evaluation matrix using cross-validation\n\n#### We're going to calculate Accuracy, precision, recall, and f1-score of our model using cross-validattion and to do so we'll \n\n#### be using `cross_val_score()`","b0060c08":"# Build Model","c64b97d2":"## As we can see our Target Feature is totally unbalanced so we would apply scikit-learn function to balance the weight of classes","0902c575":"### So far three algorithms performed pretty well \n\n* The first model is not the best model.\nLet's tune all the algorithms if we can get more accuracy","570aa62e":"## Let's work with XGboost Classifier and see if we can get overall best score","2cf04d3a":"## Evaluating our tunned machine learning classifier, beyond accuracy \n\n* ROC curve and AUC Score\n* Confusion matrix\n* Classification report \n* Precision \n* Recall\n* F1-Score\n\nand it would be great if Cross-validation used if possible","368ec525":"#### we need find corr() between features","cede7bf4":"## Let's use GridSearchCV for hyperparameter tunning","35cf949b":"> Now we can say that Logistic Regression is not the best model for our dataset","c8078cd3":"> So far K-nn has not improved so let's jump into another algorithm tunning\n\n> This time we will tune hyperparameter with Scikit-learn libraries `RandomizedSearchCV` & `GridSearchCV`","a068c8ca":"### Splitting Data","523c71a2":"* We used many Algorithms but so far XGboost performed pretty well on this dataset though we have un-balanced data(only Target feature).\n","0cb0fc9d":"## Hyperparameter with RandomizedSearchCV","e315ba23":"* So far Logistic Regression has not improved that much ","86a74c5a":"### The reason Why we have less Accuracy, Precsion, Recall, and f1-score is because we have used cv=100 we can gain more if we increase the number of cv(cross-validation).","01043269":"* First We Need To Remove The 0 Values in Dataset"}}