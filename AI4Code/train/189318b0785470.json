{"cell_type":{"a448895e":"code","78b3ba66":"code","b2ac1493":"code","ec846f53":"code","340e6996":"code","5baa16fc":"code","ca093362":"code","1d5589d2":"code","ae445818":"code","5a8bac2b":"code","62ef236f":"code","20ff45ae":"code","8acb21ae":"code","f6fd6d1c":"code","d8ac1620":"code","7fa4c9ef":"code","3b032168":"code","07f18f17":"code","8095dcb7":"code","1f63303f":"code","5f2df0b2":"code","a13f2393":"code","070c1525":"code","49de7926":"code","e2fdf61c":"code","12fef625":"code","29dfee98":"code","7369e959":"code","c26a7647":"code","7ac4908a":"code","8bed5654":"code","3af60e6e":"code","dec5d556":"code","0e9b3102":"code","2154dfa9":"code","aa030ea5":"code","82c6f373":"code","7accdd87":"code","8be297e5":"code","5f2af47f":"code","3cc3edfb":"code","94d74990":"markdown","caacaf7c":"markdown","1a63c4cf":"markdown","d15c6eed":"markdown","abf47180":"markdown","9b6a24e4":"markdown"},"source":{"a448895e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","78b3ba66":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n#from catboost import CatBoostClassifier,Pool\nfrom IPython.display import display\nimport matplotlib.patches as patch\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import NuSVR\nfrom scipy.stats import norm\nfrom sklearn import svm\nimport lightgbm as lgb\nimport xgboost as xgb\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport time\nimport glob\nimport sys\nimport os\nimport gc\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier","b2ac1493":"train = pd.read_csv('..\/input\/train.csv')","ec846f53":"train.head()","340e6996":"train.shape","5baa16fc":"train.columns","ca093362":"train.describe()","1d5589d2":"def check_missing_data(df):\n    flag=df.isna().sum().any()\n    if flag==True:\n        total = df.isnull().sum()\n        percent = (df.isnull().sum())\/(df.isnull().count()*100)\n        output = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n        data_type = []\n        for col in df.columns:\n            dtype = str(df[col].dtype)\n            data_type.append(dtype)\n        output['Types'] = data_type\n        return(np.transpose(output))\n    else:\n        return(False)","ae445818":"check_missing_data(train)","5a8bac2b":"train['target'].value_counts()","62ef236f":"def check_balance(df,target):\n    check=[]\n    print('size of data is:',df.shape[0] )\n    for i in [0,1]:\n        print('for target  {} ='.format(i))\n        print(df[target].value_counts()[i]\/df.shape[0]*100,'%')","20ff45ae":"check_balance(train,'target')","8acb21ae":"cols=[\"target\",\"ID_code\"]\nX = train.drop(cols,axis=1)\ny = train[\"target\"]","f6fd6d1c":"train_X, val_X, train_y, val_y = train_test_split(X, y,test_size=0.3, random_state=1)\nrfc_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)","d8ac1620":"y_pred_rfc=rfc_model.predict(val_X)","7fa4c9ef":"print(\"Accuracy:\",metrics.accuracy_score(val_y, y_pred_rfc))\nprint(\"precision:\",metrics.precision_score(val_y, y_pred_rfc))","3b032168":"tree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y)","07f18f17":"y_pred_tree = tree_model.predict(val_X)","8095dcb7":"print(\"Accuracy:\",metrics.accuracy_score(val_y, y_pred_tree))\nprint(\"precision:\",metrics.precision_score(val_y, y_pred_tree))","1f63303f":"logreg = LogisticRegression()\nlogreg.fit(train_X,train_y)\ny_pred=logreg.predict(val_X)","5f2df0b2":"print(\"Accuracy:\",metrics.accuracy_score(val_y, y_pred))\nprint(\"Precision:\",metrics.precision_score(val_y, y_pred))","a13f2393":"params = {'num_leaves': 9,\n         'min_data_in_leaf': 42,\n         'objective': 'binary',\n         'max_depth': 16,\n         'learning_rate': 0.0123,\n         'boosting': 'gbdt',\n         'bagging_freq': 5,\n         'bagging_fraction': 0.8,\n         'feature_fraction': 0.8201,\n         'bagging_seed': 11,\n         'reg_alpha': 1.728910519108444,\n         'reg_lambda': 4.9847051755586085,\n         'random_state': 42,\n         'metric': 'auc',\n         'verbosity': -1,\n         'subsample': 0.81,\n         'min_gain_to_split': 0.01077313523861969,\n         'min_child_weight': 19.428902804238373,\n         'num_threads': 4}","070c1525":"fold_n=2\nfolds = StratifiedKFold(n_splits=fold_n, shuffle=True, random_state=10)\n%matplotlib inline\n%precision 4\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')\nnp.set_printoptions(suppress=True)\npd.set_option(\"display.precision\", 15)","49de7926":"y_pred_lgb = np.zeros(len(val_X))\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    train_data = lgb.Dataset(X_train, label=y_train)\n    valid_data = lgb.Dataset(X_valid, label=y_valid)\n        \n    lgb_model = lgb.train(params,train_data,num_boost_round=2000,#change 20 to 2000\n                    valid_sets = [train_data, valid_data],verbose_eval=300,early_stopping_rounds = 200)##change 10 to 200\n            \n    y_pred_lgb += lgb_model.predict(val_X, num_iteration=lgb_model.best_iteration)\/5","e2fdf61c":"model = XGBClassifier().fit(X_train, y_train)\ny_pred = model.predict(val_X)","12fef625":"print(\"Accuracy:\",metrics.accuracy_score(val_y, y_pred))\nprint(\"precision:\",metrics.precision_score(val_y, y_pred))","29dfee98":"from imblearn.over_sampling import SMOTE\nfrom collections import Counter\nsm = SMOTE(random_state=42)\nX_resamp_tr, y_resamp_tr = sm.fit_resample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_resamp_tr))\nX_resamp_tr = pandas.DataFrame(X_resamp_tr)\ny_resamp_tr = pandas.DataFrame({\"target\": y_resamp_tr})","7369e959":"X_resamp_tr.head()","c26a7647":"y_resamp_tr.head()","7ac4908a":"train_X, val_X, train_y, val_y = train_test_split(X_resamp_tr, y_resamp_tr,test_size=0.3, random_state=1)","8bed5654":"logreg = LogisticRegression()\nlogreg.fit(X_train,y_train)\ny_pred=logreg.predict(val_X)","3af60e6e":"print(\"Accuracy:\",metrics.accuracy_score(val_y, y_pred))\nprint(\"Precision:\",metrics.precision_score(val_y, y_pred))","dec5d556":"tree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y)","0e9b3102":"y_pred_tree = tree_model.predict(val_X)","2154dfa9":"print(\"Accuracy:\",metrics.accuracy_score(val_y, y_pred_tree))\nprint(\"precision:\",metrics.precision_score(val_y, y_pred_tree))","aa030ea5":"rfc_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)","82c6f373":"y_pred_rfc=rfc_model.predict(val_X)","7accdd87":"print(\"Accuracy:\",metrics.accuracy_score(val_y, y_pred_rfc))\nprint(\"precision:\",metrics.precision_score(val_y, y_pred_rfc))","8be297e5":"y_pred_lgb = np.zeros(len(val_X))\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X_resamp_tr,y_resamp_tr)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train, X_valid = X_resamp_tr.iloc[train_index], X_resamp_tr.iloc[valid_index]\n    y_train, y_valid = y_resamp_tr.iloc[train_index], y_resamp_tr.iloc[valid_index]\n    \n    train_data = lgb.Dataset(X_train, label=y_train)\n    valid_data = lgb.Dataset(X_valid, label=y_valid)\n        \n    lgb_model = lgb.train(params,train_data,num_boost_round=2000,#change 20 to 2000\n                    valid_sets = [train_data, valid_data],verbose_eval=300,early_stopping_rounds = 200)##change 10 to 200\n            \n    y_pred_lgb += lgb_model.predict(val_X, num_iteration=lgb_model.best_iteration)\/5","5f2af47f":"model = XGBClassifier().fit(X_train, y_train)\ny_pred = model.predict(val_X)","3cc3edfb":"print(\"Accuracy:\",metrics.accuracy_score(val_y, y_pred))\nprint(\"precision:\",metrics.precision_score(val_y, y_pred))","94d74990":"even after applying SMOTE, there is some imbalance in accuracy and precision of the SMOTE aaplied dataset.\nso thus, in future work we try to apply 1-class classifiers like 1-class SVM to handle the imbalance dataset.","caacaf7c":"we will perform SMOTE operation on the dataset to oversample the dataset by oversampling the data","1a63c4cf":"The will apply different classifiers on the dataset","d15c6eed":"This is a imbalance dataset","abf47180":"There is no missing data ","9b6a24e4":"**Santander Customer Transaction Prediction**\nWe apply different classifiers on original dataset and even SMOTE applied dataset. Thus compare the results."}}