{"cell_type":{"c3d453a5":"code","39c94306":"code","20e24217":"code","77bcd4a7":"code","541b739d":"code","fab7f094":"code","33d56538":"code","058d49e0":"code","f9c5cb04":"code","04509d44":"code","011ce648":"code","e8ab9881":"code","4838479b":"code","3b9a98e8":"code","82e77f3c":"code","796c8358":"code","1f73624a":"code","675fa54a":"code","a2a25bc0":"markdown","8fbf7d83":"markdown","db81787b":"markdown","c787aaee":"markdown","8213c747":"markdown"},"source":{"c3d453a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","39c94306":"from sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom skopt import BayesSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom warnings import filterwarnings\n\nfilterwarnings(\"ignore\", category=DeprecationWarning) \nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)","20e24217":"%%time\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")","77bcd4a7":"x_train = train.drop(['id','claim'],axis=1)\ny_train = np.array(train.claim)\nx_cols = x_train.columns","541b739d":"# as all are numerical columns so filling missing values with mean\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer = imputer.fit(x_train)\nx_train = imputer.transform(x_train)","fab7f094":"x_train = pd.DataFrame(data = x_train, columns=x_cols)","33d56538":"scaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_train = pd.DataFrame(data = x_train, columns=x_cols)\nx_train","058d49e0":"X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size = 0.2, random_state = 0, stratify = y_train)","f9c5cb04":"#tuning hyperparameters\nfrom bayes_opt import BayesianOptimization\nfrom skopt  import BayesSearchCV \nimport warnings\n#graph, plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#building models\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nimport time\nimport sys\n\n#metrics \nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport shap\nwarnings.simplefilter(action='ignore', category=FutureWarning)","04509d44":"%%time\ndef bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=3, random_seed=6,n_estimators=20000, output_process=False):\n    # prepare data\n    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n    # parameters\n    def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, min_data_in_leaf,min_sum_hessian_in_leaf,subsample):\n        params = {'application':'binary', 'metric':'auc'}\n        params['learning_rate'] = max(min(learning_rate, 1), 0)\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['max_bin'] = int(round(max_depth))\n        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n        params['subsample'] = max(min(subsample, 1), 0)\n                \n        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n        return max(cv_result['auc-mean'])\n     \n    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.001, 0.2),\n                                            'num_leaves': (20, 50),\n                                            'feature_fraction': (0.1, 1),\n                                            'bagging_fraction': (0.5, 1),\n                                            'max_depth': (5, 30),\n                                            'max_bin':(20,90),\n                                            'min_data_in_leaf': (20, 80),\n                                            'min_sum_hessian_in_leaf':(0,100),\n                                           'subsample': (0.01, 1.0)}, random_state=200)\n\n    \n    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n    \n    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    \n    model_auc=[]\n    for model in range(len( lgbBO.res)):\n        model_auc.append(lgbBO.res[model]['target'])\n    \n    # return best parameters\n    return lgbBO.res[pd.Series(model_auc).idxmax()]['target'],lgbBO.res[pd.Series(model_auc).idxmax()]['params']\n\nopt_params = bayes_parameter_opt_lgb(X_train, Y_train, init_round=5, opt_round=10, n_folds=5, random_seed=6,n_estimators=10000)","011ce648":"opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\nopt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\nopt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\nopt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\nopt_params[1]['objective']='binary'\nopt_params[1]['metric']='auc'\nopt_params[1]['is_unbalance']=True\nopt_params[1]['boost_from_average']=False\nopt_params=opt_params[1]\nopt_params","e8ab9881":"%%time\ny_train = train.claim\nfeatures= [c for c in x_train.columns ]\nfolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=31416)\noof = np.zeros(len(train))\npredictions = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(x_train, y_train)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=y_train.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=y_train.iloc[val_idx])\n\n    num_round = 15000\n    clf = lgb.train(opt_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, early_stopping_rounds = 250)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(y_train, oof)))","4838479b":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:20].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(20,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()\nplt.savefig('Feature_Importance.png')","3b9a98e8":"# explainer = shap.TreeExplainer(clf)\n# shap_values = explainer.shap_values(X_train)\n\n# shap.summary_plot(shap_values, X_train)","82e77f3c":"#tree visualization\ngraph = lgb.create_tree_digraph(clf, tree_index=3, name='Tree3' )\ngraph.graph_attr.update(size=\"500,500\")\ngraph","796c8358":"test_id = test.id\nx_test = test.drop(['id'],axis=1)\n# filling the NaN with mean\nx_test = imputer.transform(x_test)\nx_test = pd.DataFrame(data = x_test, columns=x_cols)\nx_test = scaler.transform(x_test)\nx_test = pd.DataFrame(data = x_test, columns=x_cols)","1f73624a":"def submission(model,filename):\n    pred = model.predict(x_test, num_iteration=model.best_iteration)\n    pred = pd.DataFrame(pred,columns=['claim'])\n    sub = pd.concat([test_id,pred],axis=1)\n    sub.set_index('id',inplace=True)\n    sub.to_csv(f\"Submission_file_{filename}.csv\")","675fa54a":"# creating submission file\nsubmission(clf,\"Tuned_lgbm\")\npred = pd.DataFrame(predictions,columns=['claim'])\nsub = pd.concat([test_id,pred],axis=1)\nsub.set_index('id',inplace=True)\nsub.to_csv(\"Submission_file_Model_raw.csv\")","a2a25bc0":"## Understanding the model","8fbf7d83":"**I hope you guys enjoyed my kernel and do not forget to upvote if you think that it's helpful!**","db81787b":"## Feature Importance","c787aaee":"## Bayesian Optimization","8213c747":"Now we will extract the best features"}}