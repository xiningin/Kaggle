{"cell_type":{"5bdd0108":"code","2b044ab9":"code","ca962ff3":"code","33f21d63":"code","0c37eb73":"code","50fd8928":"code","2f85b072":"code","b086f816":"code","00fc3dab":"code","dc359cc9":"code","9f5a72db":"code","b17b36e5":"code","3be1b78e":"code","17b20f53":"code","273124d9":"code","70064987":"code","c97fda8e":"code","c3b33b1d":"code","ed1b061f":"code","39b35ae3":"code","e59c020e":"code","58729ad0":"code","a9fba52c":"code","8f64b5b3":"code","dc8984b9":"code","60866a27":"code","909a82c6":"code","edaaa4a4":"code","351fe668":"markdown","ae10da42":"markdown","59951af1":"markdown","5384f77d":"markdown","5cee488d":"markdown","b5fdc8da":"markdown","6e41bec3":"markdown","f9854853":"markdown","9e63f4e7":"markdown","375bb8d9":"markdown","f9af84fa":"markdown","ed1a09a6":"markdown","189f461d":"markdown","9b4b4dfd":"markdown","d2e6acdc":"markdown","4cb2e8d3":"markdown","513f25f9":"markdown","8978b1c6":"markdown","d27045d1":"markdown","1ce1bdd8":"markdown","f8de25aa":"markdown","604677bb":"markdown"},"source":{"5bdd0108":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# import for plotting \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2b044ab9":"app_train = pd.read_csv('..\/input\/application_train.csv')\napp_test = pd.read_csv('..\/input\/application_test.csv')\nbureau = pd.read_csv('..\/input\/bureau.csv')\nbureau_balance = pd.read_csv('..\/input\/bureau_balance.csv')\npos_cash_balance = pd.read_csv('..\/input\/POS_CASH_balance.csv')\n\nprevious_app = pd.read_csv('..\/input\/previous_application.csv')\ninstallments_payments = pd.read_csv('..\/input\/installments_payments.csv')\ncredit_card_balance = pd.read_csv('..\/input\/credit_card_balance.csv')","ca962ff3":"print(app_test.shape)","33f21d63":"# Groupby the client id (SK_ID_CURR), count the number of previous loans, and rename the column\nprevious_loan_counts = bureau.groupby('SK_ID_CURR', as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU': 'previous_loan_counts'})\nprevious_loan_counts.head()","0c37eb73":"def normalize_categorical(df, group_var, col_name):\n    \n    \"\"\"Computes counts and normalized counts for each observation\n    of `group_var` for each unique category in every categorical variable\n    \n    Parameters \n    ----------\n    df - DataFrame for which we will calculate count\n    \n    group_var  = string\n        The variable by which to group the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    col_name = string\n            Variable added to the front of column names to keep track of columns\n            \n            \"\"\"\n    # select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('object'))\n    \n    # Make sure to put the identifying id on the column\n    categorical[group_var] = df[group_var]\n    \n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])                                              \n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['count', 'count_norm']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (col_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    return categorical\n    ","50fd8928":"bureau_counts = normalize_categorical(bureau, group_var = 'SK_ID_CURR', col_name = 'bureau')\nbureau_counts.head()","2f85b072":"# Grouping data  so  that we can merge all the files in 1 dataset\n\ndata_bureau_agg=bureau.groupby(by='SK_ID_CURR').mean()\ndata_credit_card_balance_agg=credit_card_balance.groupby(by='SK_ID_CURR').mean()\ndata_previous_application_agg=previous_app.groupby(by='SK_ID_CURR').mean()\ndata_installments_payments_agg=installments_payments.groupby(by='SK_ID_CURR').mean()\ndata_POS_CASH_balance_agg=pos_cash_balance.groupby(by='SK_ID_CURR').mean()\n\ndata_bureau_agg.head()","b086f816":"# we will be plotting gender, occupation, has car, has flat  \n\nplt.figure(1)\nplt.subplot(221)\napp_train['CODE_GENDER'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'Gender')\n\nplt.subplot(222)\napp_train['FLAG_OWN_CAR'].value_counts(normalize=True).plot.bar(title= 'Own Car?')\n\nplt.subplot(223)\napp_train['CNT_CHILDREN'].value_counts(normalize=True).plot.bar(title= 'Count Children')\n\nplt.subplot(224)\napp_train['FLAG_OWN_REALTY'].value_counts(normalize=True).plot.bar(figsize=(24,6), title= 'Has Realty?')\n\n\n\nplt.show()","00fc3dab":"plt.figure(2)\n\nplt.subplot(321)\napp_train['NAME_TYPE_SUITE'].value_counts(normalize=True).plot.bar(figsize=(20,20), title= 'Accompanient')\n\nplt.subplot(322)\napp_train[\"NAME_CONTRACT_TYPE\"].value_counts(normalize=True).plot.pie(figsize=(20,20), title='Loan Type')\n\nplt.subplot(323)\napp_train[\"NAME_FAMILY_STATUS\"].value_counts(normalize=True).plot.pie(figsize=(20,20), title='Family status of applicants')\n\nplt.subplot(324)\napp_train[\"OCCUPATION_TYPE\"].value_counts(normalize=True).plot.bar(figsize=(20,20), title='Occupation')\nplt.show()","dc359cc9":"def merge(df):\n    df = df.join(data_bureau_agg, how='left', on='SK_ID_CURR', lsuffix='1', rsuffix='2') \n    df = df.join(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n    df = df.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'left')\n    df = df.join(data_credit_card_balance_agg, how='left', on='SK_ID_CURR', lsuffix='1', rsuffix='2')    \n    df = df.join(data_previous_application_agg, how='left', on='SK_ID_CURR', lsuffix='1', rsuffix='2')   \n    df = df.join(data_installments_payments_agg, how='left', on='SK_ID_CURR', lsuffix='1', rsuffix='2') \n    \n    return df\n\ntrain = merge(app_train)\ntest = merge(app_test)\ndisplay(train.head())","9f5a72db":"print(train.shape)\nprint(test.shape)","b17b36e5":"#combining the data\nntrain = train.shape[0]\nntest = test.shape[0]\n\ny_train = train.TARGET.values\n\n#train_df = train_df.drop\n\nall_data = pd.concat([train, test]).reset_index(drop=True)\nall_data.drop(['TARGET'], axis=1, inplace=True)","3be1b78e":"# Now we will convert days employed and days registration and days id publish to a positive no. \ndef correct_birth(df):\n    \n    df['DAYS_BIRTH'] = round((df['DAYS_BIRTH'] * (-1))\/365)\n    return df\n\ndef convert_abs(df):\n    df['DAYS_EMPLOYED'] = abs(df['DAYS_EMPLOYED'])\n    df['DAYS_REGISTRATION'] = abs(df['DAYS_REGISTRATION'])\n    df['DAYS_ID_PUBLISH'] = abs(df['DAYS_ID_PUBLISH'])\n    df['DAYS_LAST_PHONE_CHANGE'] = abs(df['DAYS_LAST_PHONE_CHANGE'])\n    return df\n\n# Now we will fill misisng values in OWN_CAR_AGE. \n#Most probably there will be missing values if the person does not own a car. So we will fill with 0\n\ndef missing(df):\n    \n    features = ['previous_loan_counts','NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAPARTMENTS_AVG','NONLIVINGAREA_MEDI','OWN_CAR_AGE']\n    \n    for f in features:\n        df[f] = df[f].fillna(0 )\n    return df\n\ndef transform_app(df):\n    df = correct_birth(df)\n    df = convert_abs(df)\n    df = missing(df)\n    return df\n\n   \n\nall_data = transform_app(all_data)\n\n    ","17b20f53":"# counting no of phones given by the company and delete the irrelevant features\nall_data['NO_OF_CLIENT_PHONES'] = all_data['FLAG_MOBIL'] + all_data['FLAG_EMP_PHONE'] + all_data['FLAG_WORK_PHONE']\nall_data.head()","273124d9":"# add a feature to determine if client's permanent city does not match with contact\/work city\nall_data['FLAG_CLIENT_OUTSIDE_CITY'] = np.where((all_data['REG_CITY_NOT_WORK_CITY']==1) & (all_data['REG_CITY_NOT_LIVE_CITY']==1),1,0)\nall_data.head()","70064987":" # add a feature to determine if client's permanent city does not match with contact\/work region\nall_data['FLAG_CLIENT_OUTSIDE_REGION'] = np.where((all_data['REG_REGION_NOT_LIVE_REGION']==1) & (all_data['REG_REGION_NOT_WORK_REGION']==1),1,0)\nall_data.head()","c97fda8e":"# deleting useless features\ndef delete(df):\n   # useless=['FLAG_MOBIL', 'FLAG_EMP_PHONE' ,'FLAG_WORK_PHONE','REG_CITY_NOT_WORK_CITY','REG_CITY_NOT_LIVE_CITY','REG_REGION_NOT_LIVE_REGION','REG_REGION_NOT_WORK_REGION']\n    #for feature in useless:\n     return df.drop(['FLAG_MOBIL', 'FLAG_EMP_PHONE' ,'FLAG_WORK_PHONE','REG_CITY_NOT_WORK_CITY','REG_CITY_NOT_LIVE_CITY','REG_REGION_NOT_LIVE_REGION','REG_REGION_NOT_WORK_REGION'], axis=1)\ndef transform(df):\n   # df = convert_abs(df)\n    df = delete(df)\n   \n    return df\n\nall_data = transform(all_data)\nall_data.head()","c3b33b1d":"# delete Ids\n\ndef delete_id(df):\n    return df.drop(['SK_ID_CURR', 'SK_ID_PREV','SK_ID_BUREAU'], axis = 1)\n\nall_data = delete_id(all_data)","ed1b061f":"all_data.head()","39b35ae3":"print(all_data.columns)","e59c020e":"# handling missing values\n\ndef miss_numerical(df):\n    \n    features = ['previous_loan_counts','NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAPARTMENTS_AVG','NONLIVINGAREA_MEDI','OWN_CAR_AGE']\n    numerical_features = all_data.select_dtypes(exclude = [\"object\"] ).columns\n    #print(numerical_features)\n    for f in numerical_features:\n        #print(f)\n        if f not in features:\n            df[f] = df[f].fillna(df[f].median())\n      \n    return df\n\ndef miss_categorical(df):\n    \n    categorical_features = all_data.select_dtypes(include = [\"object\"]).columns\n    \n    for f in categorical_features:\n        df[f] = df[f].fillna(df[f].mode()[0])\n        \n    return df\n\ndef transform_feature(df):\n    df = miss_numerical(df)\n    df = miss_categorical(df)\n    #df = fill_cabin(df)\n    return df\n\nall_data = transform_feature(all_data)\n\n\nall_data.head()\n        ","58729ad0":"# Scaling the data \n\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef encoder(df):\n    scaler = MinMaxScaler()\n    numerical = all_data.select_dtypes(exclude = [\"object\"]).columns\n    features_transform = pd.DataFrame(data= df)\n    features_transform[numerical] = scaler.fit_transform(df[numerical])\n    display(features_transform.head(n = 5))\n    return df\n\nall_data = encoder(all_data)\n\n#display(all_data.head())","a9fba52c":"# Converting into categorical features\n\n# Create a label encoder object\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle_count = 0\n\n\n# Iterate through the columns\nfor col in all_data:\n    if all_data[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(all_data[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(all_data[col])\n            # Transform both training and testing data\n            all_data[col] = le.transform(all_data[col])\n            #test[col] = le.transform(test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n           \nprint('%d columns were label encoded.' % le_count)","8f64b5b3":"# dummy variables\nall_data = pd.get_dummies(all_data)\n\ndisplay(all_data.shape)","dc8984b9":"### Splitting features\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]\n\nprint(\"Training shape\", train.shape)\nprint(\"Testing shape\", test.shape)","60866a27":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(train, y_train, test_size = 0.3, random_state = 200)\nprint(\"X Training shape\", X_train.shape)\nprint(\"X Testing shape\", X_test.shape)\nprint(\"Y Training shape\", Y_train.shape)\nprint(\"Y Testing shape\", Y_test.shape)\n\n","909a82c6":"from sklearn.metrics import make_scorer\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.grid_search import GridSearchCV\n\nlogreg = LogisticRegression(random_state=0, class_weight='balanced', C=100)\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict_proba(X_test)[:,1]\n\n#Y_pred_proba = logreg.predict_proba(X_test)\n\nprint('Train\/Test split results:')\n#print(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(Y_test, Y_pred))\nprint(\"ROC\",  roc_auc_score(Y_test, Y_pred))\n#print(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\n\n","edaaa4a4":"pred_test = logreg.predict_proba(test)\n#print(\"ROC\",  roc_auc_score(Y_test, pred_test))\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\n\nsubmission['SK_ID_CURR']=app_test['SK_ID_CURR']\nprint(len(app_test['SK_ID_CURR']))\nsubmission['TARGET']=pred_test\n#converting to csv\n#print(submission['TARGET'])\npd.DataFrame(submission, columns=['SK_ID_CURR','TARGET'],index=None).to_csv('homecreditada.csv')","351fe668":" ## <a id='8'>8.1. Deleting features<\/a>","ae10da42":"# Inference - \n1. We see that most of the applicants were female and without any children.\n2. An interesting fact is that most of the applicants owned a realty but not a car. \n","59951af1":"> # <a id='4'>4 Grouping the data <\/a>","5384f77d":"> # <a id='3'>3. Feature Engineering<\/a>","5cee488d":"> # <a id='5'>5. Exploratory Data Exploration<\/a>","b5fdc8da":"## <a id='8-3'>8.3 Scaling Numerical Features <\/a>","6e41bec3":"## <a id='3-1'>3.1 Creating new feature for bureau<\/a>","f9854853":"## <a id='8-2'>8.2  Handling Missing Values <\/a>","9e63f4e7":"> # <a id='6'>6. Merging the data<\/a>","375bb8d9":"## <a id='5-3'>5.3 Client accompanied by ? <\/a>","f9af84fa":"## <a id='3-2'>3.2 Function to count and normalize values of categorical variables <\/a>","ed1a09a6":"> # <a id='1'>1. Problem Statement<\/a>","189f461d":"Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n\n**Evalutaion**  - Area under the ROC Curve\n\n**Data  ** -   the problem has 7 files. \n\n* **application_train\/application_test**: the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the feature SK_ID_CURR.  \n* **bureau **: All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample). Each previous credit has its own row in bureau, but one loan in the application data can have multiple previous credits.","9b4b4dfd":"> # <a id='7'>7. Combining training and testing data<\/a>","d2e6acdc":"## <a id='5-3'>5.3  Loan is replayed or not? <\/a>","4cb2e8d3":"- <a href='#1'>1. Problem Statement <\/a>  \n- <a href='#2'>2. Reading the data<\/a>\n- <a href='#3'>3. Feature Engineering<\/a>\n    - <a href='#3-1'>3.1 Creating new feature for bureau<\/a>\n    - <a href='#3-2'> 3.2 Function to count and normalize values of categorical variables <\/a>\n- <a href='#4'>4. Grouping the data<\/a>\n- <a href='#5'>5. Exploratory Data Analysis<\/a>\n       - <a href='#5-1'>5.1  Analyzing Target Variable<\/a>\n     - <a href='#5-2'>5.2  Visualizing basic info of the applicant <\/a>\n      - <a href='#5-3'>5.3 Client accompanied by ? <\/a>\n- <a href='#6'>6. Merging the data<\/a>     \n- <a href='#7'>7. Combining Training and Testing data<\/a>     \n- <a href='#7'>8. Feature Engineering Continued<\/a>     \n     -<a href='#8_1'>8.1. Deleting features <\/a>\n     -<a href='#8_2'>8.2  Handling Missing Values <\/a>\n      -<a href='#8_3'>8.3 Scaling Numerical Features <\/a>\n      -<a href='#8_3'>8.4 Converting into Categorical <\/a>\n   \n- <a href='#9'>9.Modelling<\/a>     ","513f25f9":"> # <a id='9'>9. Modelling<\/a>","8978b1c6":">  # <a id='2'>2. Reading the Data<\/a>","d27045d1":"In Progress","1ce1bdd8":"## <a id='8-4'>8.4 Converting into categorical features <\/a>","f8de25aa":"## <a id='5-2'>5.2  Visualizing basic info of the applicant <\/a>","604677bb":"> # <a id='8'>8. Feature Engineering Continued<\/a>"}}