{"cell_type":{"c09df5f2":"code","171e2a80":"code","568df5f3":"code","5092fe83":"code","deef4b38":"code","8f58fb00":"code","ea8b7a85":"code","d5670c0b":"code","99fc16ba":"code","ba381c84":"code","6ab080f5":"code","f40571c8":"code","870085d2":"code","84394ae3":"code","71dbd7d0":"code","60afaf60":"code","cee17526":"code","456a0446":"code","483a3889":"code","f1306310":"code","71662ea5":"code","394b4f44":"markdown","34e2bad1":"markdown","0ecc30bd":"markdown"},"source":{"c09df5f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom datetime import datetime\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfrom os.path import join as pjoin\n\ndata_root = '..\/input\/make-data-ready'\nprint(os.listdir(data_root))\n\n# Any results you write to the current directory are saved as output.","171e2a80":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport xgboost as xgb\n\nfrom pprint import pprint\nimport math\n\nfrom scipy.stats import kurtosis, skew\n\nfrom IPython import embed\nfrom IPython.terminal.embed import InteractiveShellEmbed\n\nfrom sklearn.model_selection import KFold\n\nimport random","568df5f3":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport shap\nplt.rcParams['figure.figsize'] = (12,6)","5092fe83":"def load_data(data='train',n=2):\n    df = pd.DataFrame()\n    for i in range(n) :\n        if data=='train':\n            if i > 8 :\n                break\n            dfpart = pd.read_pickle(pjoin(data_root,f'train_{i}.pkl'))\n        elif data=='test':\n            if i > 2 :\n                break\n            dfpart = pd.read_pickle(pjoin(data_root,f'test_{i}.pkl'))\n        df = pd.concat([df,dfpart])\n        del dfpart\n    return df\n        ","deef4b38":"%%time\ndf_train = load_data(n=9)\ndf_test = load_data('test',n=4)","8f58fb00":"# delete date-related features\ndel_cols=[item for item in df_train.columns if \"Date\" in item]\ndf_train=df_train.drop(del_cols,axis=1).copy()\ndf_test=df_test.drop(del_cols,axis=1).copy()\n","ea8b7a85":"df_all = pd.concat([df_train,df_test]).reset_index(drop=True)","d5670c0b":"print(f'# of columns has na value: {(df_test.isnull().sum().sort_values(ascending=False) > 0).sum()}')","99fc16ba":"# train_v2.csv - from August 1st 2016 to April 30th 2018.\n# test_v2.csv - from May 1st 2018 to October 15th 2018.\n# sample_submission_v2.csv - from December 1st 2018 to January 31st 2019\n\n# divide df_all into\n# df_train_x - from August 1st 2016 to June 30st 2018, 5+12+6=23 months\n# <-> 1.5 month (Jul, Aug * 0.5)\n# df_train_y -  from August 15th 2018 to October 15th 2018, 2 months\n# -> divide new_test into in-sample and out-of-sample to avoid overfitting\n#   (only includes visitors Jan 15st 2018- June 30th 2018)\n\ndf_train_x = df_all[df_all.date <= \"2018-06-30\"].copy()\ndf_train_y = df_all[df_all.date >= \"2018-08-15\"].copy()\n\nid_train = df_all[(df_all.date >= \"2018-01-15\") & (df_all.date <= \"2018-06-30\")].fullVisitorId.drop_duplicates()\ndf_train_x = df_train_x[df_train_x.fullVisitorId.isin(id_train)].copy()\ndf_train_y = df_train_y[df_train_y.fullVisitorId.isin(id_train)].copy()\n\n# Apply the trained model to \n# df_test_x - from November 15th 2016 to October 15th 2018, 23 months\n# <-> 1.5 month (Oct *0.5, Nov)\n# df_test_y - from December 1st 2018 to January 31st 2019, 2 months \n#   (only includes visitors May 1st 2018- Oct 15th 2018)\n\ndf_test_x = df_all[(df_all.date >= \"2016-11-15\") & (df_all.date <= \"2018-10-15\")].copy()\nid_test = df_all[(df_all.date >= \"2018-05-01\") & (df_all.date <= \"2018-10-15\")].fullVisitorId.drop_duplicates()\ndf_test_x = df_test_x[df_test_x.fullVisitorId.isin(id_test)].copy()","ba381c84":"print({\"all\":df_all.shape,\n       \"train_x\":df_train_x.shape,\n       \"train_y\":df_train_y.shape,\n       \"test_x\":df_test_x.shape,\n       \"id_train\":id_train.shape,\n       \"id_test\":id_test.shape})","6ab080f5":"#sns.lineplot(data=df_train_y.groupby(\"date\")[\"fullVisitorId\"].count())","f40571c8":"#df_all.groupby(\"fullVisitorId\")[\"totals_transactionRevenue\"].sum().describe()\n#df_train.groupby(\"fullVisitorId\")[\"totals_transactionRevenue\"].sum().describe()\n#df_test.groupby(\"fullVisitorId\")[\"totals_transactionRevenue\"].sum().describe()\n#df_all.groupby(\"fullVisitorId\")[\"device_deviceCategory\"].agg(lambda x:x.value_counts().index[0])","870085d2":"\ndf_all.head()","84394ae3":"%time\nglobal df_train_x_encoded,df_train_y_encoded, df_all_encoded, df_test_x_encoded\ndf_train_x_encoded = df_train_x.copy()\ndf_train_y_encoded = df_train_y.copy()\ndf_all_encoded = df_all.copy()\ndf_test_x_encoded = df_test_x.copy()\n\n## agg for last\n\n# date\n# the end of day of the dataset 10-15\nimport datetime\ndf_train_x_encoded[\"date\"]=(datetime.datetime(2018,6,30)-df_train_x_encoded.date).dt.days.astype(\"int64\")\/365\ndf_train_y_encoded[\"date\"]=(datetime.datetime(2018,10,15)-df_train_y_encoded.date).dt.days.astype(\"int64\")\/365\ndf_test_x_encoded[\"date\"]=(datetime.datetime(2018,10,15)-df_test_x_encoded.date).dt.days.astype(\"int64\")\/365\n\n# last_cols =  [\"channelGrouping\", \"device_browser\", \n#             \"device_deviceCategory\", \"device_operatingSystem\", \n#             \"geoNetwork_city\", \"geoNetwork_continent\", \n#             \"geoNetwork_country\", \"geoNetwork_metro\",\n#             \"geoNetwork_networkDomain\", \"geoNetwork_region\", \n#             \"geoNetwork_subContinent\", \"trafficSource_adContent\", \n#             \"trafficSource_adwordsClickInfo.adNetworkType\", \n#             \"trafficSource_adwordsClickInfo.gclId\", \n#             \"trafficSource_adwordsClickInfo.slot\", \"trafficSource_campaign\",\n#             \"trafficSource_keyword\", \"trafficSource_medium\", \n#             \"trafficSource_referralPath\", \"trafficSource_source\",\n#             'trafficSource_adwordsClickInfo.isVideoAd',\n#             'trafficSource_isTrueDirect', 'device_isMobile'] + [\"fullVisitorId\",\"date\"]\n\ncat_cols=[\"channelGrouping\",\"device_browser\",\"device_deviceCategory\",\n              \"device_operatingSystem\",\"geoNetwork_city\",\"geoNetwork_continent\",\"geoNetwork_country\",\"trafficSource_medium\",\n              \"trafficSource_adwordsClickInfo.isVideoAd\",\"trafficSource_isTrueDirect\",\"device_isMobile\"]\nlast_cols = cat_cols + [\"fullVisitorId\",\"date\"]\n\ndf_train_x_agg_last = df_train_x_encoded[last_cols].groupby(\"fullVisitorId\",as_index=False).last().sort_values(\"fullVisitorId\").reset_index(drop=True).copy()\ndf_test_x_agg_last = df_test_x_encoded[last_cols].groupby(\"fullVisitorId\",as_index=False).last().sort_values(\"fullVisitorId\").reset_index(drop=True).copy()\ndf_train_y_agg_last = df_train_y_encoded[last_cols].groupby(\"fullVisitorId\",as_index=False).last().sort_values(\"fullVisitorId\").reset_index(drop=True).copy()","71dbd7d0":"import category_encoders as ce\n\nec= ce.OrdinalEncoder(cols=cat_cols,handle_unknown='impute')\n\nec.fit(pd.concat([df_train_x_agg_last,df_train_y_agg_last,df_test_x_agg_last]))\ndf_train_x_agg_last=ec.transform(df_train_x_agg_last)\ndf_train_y_agg_last=ec.transform(df_train_y_agg_last)\ndf_test_x_agg_last=ec.transform(df_test_x_agg_last)","60afaf60":"df_train_x","cee17526":"%%time\n\nnum_cols = [item for item in df_train.columns if \"totals\" in item]\nsum_cols = num_cols + [\"fullVisitorId\"]\n\ndf_train_x_agg_sum = df_train_x_encoded[sum_cols].groupby(\"fullVisitorId\",as_index=False).sum().sort_values(\"fullVisitorId\").reset_index(drop=True).copy()\ndf_test_x_agg_sum = df_test_x_encoded[sum_cols].groupby(\"fullVisitorId\",as_index=False).sum().sort_values(\"fullVisitorId\").reset_index(drop=True).copy()\ndf_train_y_agg_sum = df_train_y_encoded[sum_cols].groupby(\"fullVisitorId\",as_index=False).sum().sort_values(\"fullVisitorId\").reset_index(drop=True).copy()\n\n# totals_transactionRevenue\ndf_train_x_agg_sum['totals_transactionRevenue'] = np.log1p(df_train_x_agg_sum['totals_transactionRevenue'])\ndf_test_x_agg_sum['totals_transactionRevenue'] = np.log1p(df_test_x_agg_sum['totals_transactionRevenue'])\ndf_train_y_agg_sum['totals_transactionRevenue'] = np.log1p(df_train_y_agg_sum['totals_transactionRevenue'])\n\ndf_train_x_agg = pd.merge(df_train_x_agg_sum,df_train_x_agg_last, how='left',on=\"fullVisitorId\").sort_values(\"fullVisitorId\").reset_index(drop=True).copy()\ndf_train_y_agg = pd.merge(df_train_y_agg_sum,df_train_y_agg_last, how='left',on=\"fullVisitorId\").sort_values(\"fullVisitorId\").reset_index(drop=True).copy()\ndf_test_x_agg = pd.merge(df_test_x_agg_sum,df_test_x_agg_last, how='left',on=\"fullVisitorId\").sort_values(\"fullVisitorId\").reset_index(drop=True).copy()\n\ndf_train_y_agg=pd.merge(id_train.to_frame(), df_train_y_agg, how='left',on=\"fullVisitorId\").sort_values(\"fullVisitorId\").reset_index(drop=True).copy()\ndf_train_y_agg[\"totals_transactionRevenue\"]=df_train_y_agg[\"totals_transactionRevenue\"].fillna(0)","456a0446":"print({\"all\":df_all.shape,\n       \"train_x\":df_train_x_agg.shape,\n       \"train_y\":df_train_y_agg.shape,\n       \"test_x\":df_test_x_agg.shape,\n       \"id_train\":id_train.shape,\n       \"id_test\":id_test.shape})","483a3889":"df_train_x_agg[df_train_x_agg.totals_transactionRevenue>0]","f1306310":"# data setting\n\ntrain_x = df_train_x_agg.drop([\"fullVisitorId\"],axis=1).values#[0:10000]\ntrain_y = df_train_y_agg.totals_transactionRevenue.values#[0:10000]\n\ntest_x_id = df_test_x_agg.fullVisitorId.values\ntest_x = df_test_x_agg.drop([\"fullVisitorId\"],axis=1).values\n\n# Grid Search\ngrid_params={'max_depth': [4,5,6],\n        'subsample': [0.95],\n        'colsample_bytree': [1.0]\n}\n\nxgb_model = xgb.XGBClassifier(early_stopping_rounds=50,eval_metric=\"rmse\",verbose=3,n_jobs=-1)\n\ngrid_search = GridSearchCV(xgb_model,\n                  grid_params,\n                  cv=3,\n                  scoring=\"neg_mean_squared_error\",\n                  n_jobs=1,\n                  verbose=3)\ngrid_search.fit(train_x,train_y)\npredict = grid_search.predict(test_x)","71662ea5":"predict_df=pd.DataFrame()\npredict_df[\"fullVisitorId\"]=test_x_id\npredict_df[\"PredictedLogRevenue\"]=predict\npredict_df.to_csv(\"xgboost_cv.csv\", index=False)","394b4f44":"# Submit","34e2bad1":"# Import and load data","0ecc30bd":"# Base model"}}