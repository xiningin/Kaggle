{"cell_type":{"523adc08":"code","5e773ba4":"code","6613c978":"code","32dcfb62":"code","4f8d8199":"code","9f293590":"code","0fef476e":"code","483a058b":"code","e6f09d7a":"code","88d4b86e":"code","4cfedd3b":"code","503cf25a":"code","ffe7483c":"code","a80e96a4":"code","5ddbde0a":"code","4ab36b73":"code","58446575":"code","9d7ed723":"code","681c9ea7":"code","d73f9555":"code","5436f705":"code","678fa304":"code","da676e53":"code","4496b84a":"code","28db3370":"code","04965b37":"markdown","6bbb17fe":"markdown","5f2d395a":"markdown","168da431":"markdown","3acd16d2":"markdown","edfd7594":"markdown","d8f1bbf2":"markdown","a61cdc6b":"markdown","8ae70201":"markdown","aada5770":"markdown","92435a19":"markdown","d8d3781d":"markdown","8fd7048e":"markdown","0843490c":"markdown","d57ddad4":"markdown","a0753f27":"markdown","c2abd9cb":"markdown","529d1d40":"markdown","e286fd3e":"markdown"},"source":{"523adc08":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nstopwords = set(STOPWORDS)\n\n# For Modelling\n\nfrom sklearn.metrics import f1_score\nimport transformers\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport torch\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AdamW\nfrom sklearn import metrics\nfrom transformers import get_linear_schedule_with_warmup\nimport numpy as np\nimport re\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","5e773ba4":"# Let's Load the data\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/\/train.csv')\ntest = pd.read_csv(r'..\/input\/nlp-getting-started\/test.csv')\ntrain.head()","6613c978":"print(\"shape of train data is {}\".format(train.shape))\nprint(\"shape of test data is {}\".format(test.shape))","32dcfb62":"# checking basic info for test and train\ntrain.info()","4f8d8199":"# Data missing information for train\ndata_info=pd.DataFrame(train.dtypes).T.rename(index={0:'column type'})\ndata_info=data_info.append(pd.DataFrame(train.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ndata_info=data_info.append(pd.DataFrame(train.isnull().sum()\/train.shape[0]*100).T.\n                         rename(index={0:'null values (%)'}))\ndisplay(data_info)","9f293590":"# Data missing information for test\ndata_info=pd.DataFrame(test.dtypes).T.rename(index={0:'column type'})\ndata_info=data_info.append(pd.DataFrame(test.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ndata_info=data_info.append(pd.DataFrame(test.isnull().sum()\/test.shape[0]*100).T.\n                         rename(index={0:'null values (%)'}))\ndisplay(data_info)","0fef476e":"# Basic Stats\ntrain.describe()","483a058b":"fig = px.bar(train, x=['Not Real','Real'] , y=train['target'].value_counts(),height=400,template=\"plotly_dark\")\n\nfig.update_layout(\n    autosize=False,\n    width=500,\n    height=500,\n    )\nfig.show()","e6f09d7a":"train.drop(['location','keyword','id'],inplace=True,axis=1)\ntest.drop(['location','keyword','id'],inplace=True,axis=1)","88d4b86e":"def clean_data(data):\n    # Lowercaseing \n    data = data.lower()\n    \n    # Removing alll special char\n    data = re.sub(r\"[@_!#$%^&*()<>?\/\\|}{~:-]\", ' ', data)\n    # Keeping only letters\n    data = re.sub(r\"[^a-zA-Z.!?']\", ' ', data)\n    # Removing additional whitespaces\n    data = re.sub(r\" +\", ' ', data)\n    \n    #Removing all https\n    data = re.sub(r'<.*?>',' ',data)\n    \n    #Removing Emojis\n    data = re.sub(r\"[\"u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                    u\"\\U00002702-\\U000027B0\"\n                    u\"\\U000024C2-\\U0001F251\"\"]+\",'',data)\n    \n    #reoving https and https \n    data = data.replace(\"http\",'')\n    data = data.replace(\"https\",'')\n    data = data.replace(\"co\",'')\n    \n\n\n    return data","4cfedd3b":"train['text'] = train['text'].apply(lambda x : clean_data(x))\ntest['text'] = test['text'].apply(lambda x : clean_data(x))","503cf25a":"# Reoving Stop words\ntrain['text'] = train['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ntest['text'] = test['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))","ffe7483c":"def get_top_ngram_words(corpus, n,ngram):\n    vec = CountVectorizer((ngram,ngram)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_ngram_words(train['text'], 50,1)    \ndf1 = pd.DataFrame(common_words, columns = ['unigram_words' , 'count'])\nfig = px.bar(df1, x= 'unigram_words', y='count',color='count',height=400,template=\"plotly_dark\")\nfig.show()","a80e96a4":"common_words = get_top_ngram_words(train['text'], 50,2)    \ndf1 = pd.DataFrame(common_words, columns = ['bigram_words' , 'count'])\nfig = px.bar(df1, x= 'bigram_words', y='count',color='count',height=400,template=\"plotly_dark\")\nfig.show()","5ddbde0a":"def get_top_trigram_words(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_trigram_words(train['text'], 30)    \ndf1 = pd.DataFrame(common_words, columns = ['trigram_words' , 'count'])\nfig = px.bar(df1, x= 'trigram_words', y='count',color='count',height=400,template=\"plotly_dark\")\nfig.show()","4ab36b73":"def show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=50, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15,7))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","58446575":"show_wordcloud(train['text'],\"Train set Word Cloud\")\n","9d7ed723":"# Define Configuration\nMAX_LEN = 256\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 4\nEPOCHS = 10\nBERT_PATH = \"..\/input\/bertbase-uncase\/\"\nMODEL_PATH = \"model.bin\"\nTOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH,do_lower_case= True)","681c9ea7":"# Model Define\nclass BERTBaseUncased(nn.Module):\n    def __init__(self):\n        super(BERTBaseUncased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n        self.bert_drop = nn.Dropout(0.2)\n        self.out = nn.Linear(768,1)\n        #self.outlayer = nn.Linear(10, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, out2 = self.bert(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n        bert_out= self.bert_drop(out2)        \n        #output1  = self.linear1(bert_out)\n        output = self.out(bert_out)\n        return output","d73f9555":"class BERTDataset:\n    def __init__(self,text,target):\n        self.text = text\n        self.target = target\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n    \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, item):\n        text = str(self.text[item])\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.target[item], dtype=torch.float)\n        }","5436f705":"def lossFunction(output,target):\n    return nn.BCEWithLogitsLoss()(output,target.view(-1,1))","678fa304":"def train_fn(data_loader, model, optimizer, device,scheduler):\n    model.train()\n\n    for batch, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n        ids = data[\"ids\"]\n        token_type_ids = data[\"token_type_ids\"]\n        mask = data[\"mask\"]\n        targets = data[\"targets\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n\n        optimizer.zero_grad()\n        outputs = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        loss = lossFunction(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()","da676e53":"def eval_fn(data_loader, model, device):\n    model.eval()\n    org_targets = []\n    org_outputs = []\n    with torch.no_grad():\n        for batch, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n            ids = data[\"ids\"]\n            token_type_ids = data[\"token_type_ids\"]\n            mask = data[\"mask\"]\n            targets = data[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            outputs = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n            outputs = torch.sigmoid(outputs)\n            org_targets.extend(targets.cpu().detach().numpy().tolist())\n            org_outputs.extend(outputs.cpu().detach().numpy().tolist())\n    return org_outputs, org_targets","4496b84a":"def compile(data):\n    train, valid = train_test_split(data,test_size=0.1,random_state=0,stratify=data.target)\n\n    train_dataset = BERTDataset(\n        text=train['text'].values,\n        target=train['target'].values\n    )\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        num_workers=4\n    )\n\n    valid_dataset = BERTDataset(\n        text=valid.text.values,\n        target=valid.target.values\n    )\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        num_workers=1\n    )\n\n    device = torch.device(\"cuda\")\n    model = BERTBaseUncased()\n    model.to(device)\n    \n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n\n    train_steps = int(len(train) \/ TRAIN_BATCH_SIZE *EPOCHS)\n    optimizer = AdamW(optimizer_parameters, lr=0.00003)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=train_steps\n    )\n\n    best_accuracy = 0\n    for epoch in range(EPOCHS):\n        train_fn(train_data_loader, model, optimizer, device,scheduler)\n        outputs, targets = eval_fn(valid_data_loader, model, device)\n        outputs = np.array(outputs) >= 0.5\n        accuracy = metrics.accuracy_score(targets, outputs)\n        #f1_score = f1_score(targets,outputs,average='macro')\n        print(f\"Accuracy Score = {accuracy}\")\n        #print(f\"F1 Score is = {f1_score}\")\n        if accuracy > best_accuracy:\n            torch.save(model.state_dict(),MODEL_PATH)\n            best_accuracy = accuracy\n","28db3370":"if __name__=='__main--':\n    compile(train)","04965b37":"## End Note\n\n### Thank you!!! Please Upvote kernel if you like the work","6bbb17fe":"# EDA and Data visualization","5f2d395a":"# 2. Basic Understanding of Data","168da431":"# Part A - EDA and Basic Data Analysis","3acd16d2":"# PART B : BERT BASE UNCASE - PYTORCH ","edfd7594":"# n-Gram Analysis","d8f1bbf2":"## Data and Module","a61cdc6b":"### Unigram Words","8ae70201":"* We have Around 80% missing information in keyword feature\n* we have 34% missing information in location feature","aada5770":"## References\n\nFOR MORE INFORMATION\n\n##### For EDA : https:\/\/towardsdatascience.com\/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a\n\n##### For Modelling : Please Watch this Awesome Video on bert by Mr. Abhishek Thakur\n                      https:\/\/www.youtube.com\/watch?v=hinZO--TEk4&list=PL98nY_tJQXZl0WwsJluhc6tGrKWCX2suH&index=3","92435a19":"#### Before Doing N-gram Analysis Let's clean the data ","d8d3781d":"# Real and Not Real Tweet\n\n\n\n\n\n\n\n![twitter-hidden-replies1.png](attachment:twitter-hidden-replies1.png)","8fd7048e":"# Table of Contents\n* <a id='section1'> References<\/a>\n* <a id='section2'>Introduction <\/a>\n* <a id='Section3'> Basic Understanding of data <\/a>\n* <a id='Section4'> EDA<\/a>\n* <a id='section5'> develope a model using BERT BASE <\/a>\n* <a id='section6'> End Note<\/a>","0843490c":"# Bi-gram Words analysis","d57ddad4":"* Basic statistics always help in many ways. this give the first look about data\n* Here we can see we have target mean is .43 which mean we have more non-real tweet compare to real tweet, so let's plot distribtuion","a0753f27":"## So Let's Jump to modelling","c2abd9cb":"### Before Modelling Let's explore the word Cloud also\n","529d1d40":"\n## 1. Introduction\n\n**In past year big social networks like Facebook or Twitter admit that on their networks are fake and duplicate accounts, fake\nnews and fake likes. With these accounts, their creators can distribute false information, support or attack an idea, a product, or an\nelection candidate, influencing real network users in making a decision. In this notbook, my aim present end to end solution for identifying not real and real Tweets**","e286fd3e":"**As we can see we have lot of null values in data set Let's explore that**"}}