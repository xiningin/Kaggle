{"cell_type":{"814de442":"code","ed848529":"code","d2ae6a96":"code","81e6ae58":"code","b24c050a":"code","fbaa9cf5":"code","8d66b8c8":"code","380f4841":"code","bb544dfd":"code","9045c032":"code","a6b1bd77":"code","9007ba09":"code","7596faab":"code","9d5b700c":"code","b5719b0b":"code","29b84ea7":"code","37fb0ede":"code","1828b1a9":"code","b691c814":"code","b5ab24ec":"code","f07681f2":"code","2a5cad27":"code","e6dd9956":"code","d37f756b":"code","8934cf87":"code","f8a1f41e":"code","893ed2f9":"code","c8ad0fd2":"code","b45bd903":"code","1ee966b4":"code","814fe268":"markdown","7c06f6bd":"markdown","ecd21340":"markdown","aff68684":"markdown","2dfe1069":"markdown","15a4bd43":"markdown","2cfa7f5e":"markdown","1491dbde":"markdown","34bc071c":"markdown","30c95145":"markdown","f8f379b7":"markdown","8b59a7bb":"markdown","0d760d6e":"markdown","16a45ca9":"markdown","ed6ead97":"markdown","3f96ec86":"markdown","3848031b":"markdown","27751375":"markdown","6808c3d0":"markdown"},"source":{"814de442":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nimport lightgbm as lgbm\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.ensemble import StackingRegressor,GradientBoostingRegressor,RandomForestRegressor\nimport optuna","ed848529":"data = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/train.csv')\ndata.drop(columns='id',axis=1,inplace=True)\ndata.describe()","d2ae6a96":"all_col_data = [data[col] for col in data.columns]\nall_col_data.pop(-1)\nfig,ax = plt.subplots(figsize=(14,10))\nax.boxplot(all_col_data)\nax.set_xticklabels(data.columns[:-1])\nax.set_title('boxplot before cleaning')\noriginal_len = len(data)","81e6ae58":"fig,ax = plt.subplots(figsize=(8,5))\nax.boxplot(data['target'])\nax.set_title('boxplot of target')","b24c050a":"data = data[data.target!=0]","fbaa9cf5":"IQR_dict = defaultdict(list)\nfor cont in ['cont7','cont9','cont10']:\n    Q1 = data[cont].quantile(0.25)\n    Q3 = data[cont].quantile(0.75)\n    IQR = Q3 - Q1  \n    IQR_dict[cont] = [Q1,Q3,IQR]\n    \nfor key,value in IQR_dict.items():\n    myfilter = (data[key] >= value[0] - 1.5 * value[2]) & (data[key] <= value[1] + 1.5 *value[2])\n    data = data.loc[myfilter] ","8d66b8c8":"all_col_data = [data[col] for col in data.columns]\nall_col_data.pop(-1)\nfig,ax = plt.subplots(figsize=(14,10))\nax.boxplot(all_col_data)\nax.set_xticklabels(data.columns[:-1])\nax.set_title('boxplot after cleaning')\ncleaned_len = len(data)","380f4841":"print('%s rows of data is dropped'% (original_len-cleaned_len))","bb544dfd":"fig,ax = plt.subplots(figsize=(8,5))\nax.boxplot(data['target'])\nax.set_title('boxplot of target')","9045c032":"#x = data.drop(columns='target',axis=1)\ncorr = data.corr()\nfig,ax = plt.subplots(figsize=(12,10))\nsns.heatmap(corr,ax=ax,vmin=-1, vmax=1, cmap='coolwarm', annot=True)\nplt.yticks(rotation=0,fontsize=13)\nplt.xticks(rotation=90,fontsize=13)\nax.set_title('correlation heatmap',fontsize=14)","a6b1bd77":"columns = np.full(shape=(corr.shape[0],), fill_value=True, dtype=bool)\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if abs(corr.iloc[i,j]) >= 0.9:\n            if columns[j]:\n                columns[j] = False\nsel_columns = data.columns[columns]\ndata = data[sel_columns]\ndata.columns","9007ba09":"def xgb_model_pipeline(data):\n    #train_x,test_x,train_y,test_y = train_test_split(data,test_size=0.2,random_state=42)\n    X = data.drop(columns='target',axis=1)\n    y = data['target']\n\n    ##Caculate E_cv (cross validation error)\n    kf = KFold(n_splits=5,shuffle=True,random_state=42)\n    fold_error = []\n    for fold,(train_idx,val_idx) in enumerate(kf.split(X)):\n        train_X,val_X = X.iloc[train_idx,:],X.iloc[val_idx,:]\n        train_y,val_y = y.iloc[train_idx],y.iloc[val_idx]\n        \n        print('Fold %s:'% (fold))\n        xgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.1, max_depth=8,seed=42,verbosity=1)\n        xgb_model.fit(train_X,train_y, eval_metric= 'rmse',\n              eval_set=[(train_X,train_y),(val_X,val_y)], early_stopping_rounds=5,verbose=False)\n        pred_y = xgb_model.predict(val_X)\n        in_fold_rmse = np.sqrt(np.mean((val_y-pred_y)**2))\n        print('Fold %s rmse: %s\\n' % (fold,in_fold_rmse))\n        fold_error.append(in_fold_rmse)\n        \n    oof_rmse = np.sum(fold_error)\/len(fold_error)\n    print('E_cv: %s' % (oof_rmse))\n\n    ##Train on the whole training data\n    xgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.1, max_depth=8,seed=42,verbosity=1)\n    xgb_model.fit(X,y,verbose=False)\n    \n    ##Plot feature importance\n    fig,ax = plt.subplots(figsize=(10,8))\n    xgb.plot_importance(xgb_model, ax=ax, importance_type='gain')\n    plt.yticks(fontsize=14)\n    \n    return xgb_model","7596faab":"xgb_model = xgb_model_pipeline(data)","9d5b700c":"def lightgbm_pipeline(data):\n    X = data.drop(columns='target',axis=1)\n    y = data['target']\n\n    ##Caculate E_cv (cross validation error)\n    kf = KFold(n_splits=5,shuffle=True,random_state=42)\n    fold_error = []\n    for fold,(train_idx,val_idx) in enumerate(kf.split(X)):\n        train_X,val_X = X.iloc[train_idx,:],X.iloc[val_idx,:]\n        train_y,val_y = y.iloc[train_idx],y.iloc[val_idx]\n        \n        print('Fold %s:'% (fold))\n        lgbm_model = LGBMRegressor(n_estimators=1000, learning_rate=0.1, \n                                   max_depth=8,random_state=42,verbosity=-1)\n        lgbm_model.fit(train_X,train_y, eval_metric= 'rmse',\n              eval_set=[(val_X,val_y)], early_stopping_rounds=5,verbose=0)\n        pred_y = lgbm_model.predict(val_X)\n        in_fold_rmse = np.sqrt(np.mean((val_y-pred_y)**2))\n        print('Fold %s rmse: %s\\n' % (fold,in_fold_rmse))\n        fold_error.append(in_fold_rmse)\n        \n    oof_rmse = np.sum(fold_error)\/len(fold_error)\n    print('E_cv: %s' % (oof_rmse))\n\n    ##Train on the whole training data\n    lgbm_model = LGBMRegressor(n_estimators=1000, learning_rate=0.1, \n                               max_depth=8, random_state=42,verbosity=-1)\n    lgbm_model.fit(X,y,verbose=0)\n    \n    ##Plot feature importance\n    fig,ax = plt.subplots(figsize=(10,8))\n    lgbm.plot_importance(lgbm_model, ax=ax, importance_type='gain')\n    plt.yticks(fontsize=14)\n    \n    return lgbm_model","b5719b0b":"lgbm_model = lightgbm_pipeline(data)","29b84ea7":"def xgb_oof(trial,data):\n    params = {\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [250, 300, 350, 400, 450]),\n        \"eta\": trial.suggest_loguniform(\"eta\",1e-2,1e-1),\n        \"max_depth\": trial.suggest_categorical(\"max_depth\",[6,8,10,12]),\n        \"subsample\": trial.suggest_discrete_uniform(\"subsample\", 0.6,1,0.1),\n        \"colsample_bytree\": trial.suggest_discrete_uniform(\"colsample_bytree\", 0.6,1,0.1),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\",5,11),\n        \"random_state\": 42\n    }\n        \n    X = data.drop(columns='target',axis=1)\n    y = data['target']\n\n    kf = KFold(n_splits=5,shuffle=True,random_state=42)\n    fold_error = []\n    for fold,(train_idx,val_idx) in enumerate(kf.split(X)):\n        train_X,val_X = X.iloc[train_idx,:],X.iloc[val_idx,:]\n        train_y,val_y = y.iloc[train_idx],y.iloc[val_idx]\n        \n        xgb_model = XGBRegressor(**params)\n        xgb_model.fit(train_X,train_y)\n        pred_y = xgb_model.predict(val_X)\n        in_fold_rmse = np.sqrt(np.mean((val_y-pred_y)**2))\n\n        fold_error.append(in_fold_rmse)\n        \n    oof_rmse = np.sum(fold_error)\/len(fold_error)\n    return oof_rmse","37fb0ede":"def objective(trial):\n    return xgb_oof(trial,data)","1828b1a9":"study = optuna.create_study(direction='minimize',study_name='XGBoost optimization')\nstudy.optimize(objective, n_trials=10)","b691c814":"study.best_params","b5ab24ec":"best_xgb = XGBRegressor(**(study.best_params))\nbest_xgb.fit(data.drop(columns='target',axis=1),data['target'])","f07681f2":"def lgbm_oof(trial,data):\n    params = {\n        'num_leaves':trial.suggest_int('num_leaves',31,100),\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [250, 300, 350, 400, 450]),\n        \"eta\": trial.suggest_loguniform(\"eta\",1e-2,1e-1),\n        \"max_depth\": trial.suggest_categorical(\"max_depth\",[6,8,10,12]),\n        \"subsample\": trial.suggest_discrete_uniform(\"subsample\", 0.6,1,0.1),\n        \"colsample_bytree\": trial.suggest_discrete_uniform(\"colsample_bytree\", 0.6,1,0.1),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\",5,11),\n        'min_child_sample':trial.suggest_int('min_child_sample',20,50),\n        \"random_state\": 42\n    }\n        \n    X = data.drop(columns='target',axis=1)\n    y = data['target']\n\n    kf = KFold(n_splits=5,shuffle=True,random_state=42)\n    fold_error = []\n    for fold,(train_idx,val_idx) in enumerate(kf.split(X)):\n        train_X,val_X = X.iloc[train_idx,:],X.iloc[val_idx,:]\n        train_y,val_y = y.iloc[train_idx],y.iloc[val_idx]\n        \n        lgbm_model = LGBMRegressor(**params)\n        lgbm_model.fit(train_X,train_y)\n        pred_y = lgbm_model.predict(val_X)\n        in_fold_rmse = np.sqrt(np.mean((val_y-pred_y)**2))\n\n        fold_error.append(in_fold_rmse)\n        \n    oof_rmse = np.sum(fold_error)\/len(fold_error)\n    return oof_rmse","2a5cad27":"def objective(trail):\n    return lgbm_oof(trail,data)","e6dd9956":"study = optuna.create_study(direction='minimize',study_name='LGBM optimization')\nstudy.optimize(objective, n_trials=20)","d37f756b":"study.best_params","8934cf87":"best_lgbm = LGBMRegressor(**(study.best_params))\nbest_lgbm.fit(data.drop(columns='target',axis=1),data['target'])","f8a1f41e":"best_xgb_param = {'n_estimators': 450,\n                  'eta': 0.025241948026570656,\n                  'max_depth': 10,\n                  'subsample': 0.6,\n                  'colsample_bytree': 0.6,\n                  'min_child_weight': 7}\n\nbest_lgbm_param = {'num_leaves': 42,\n                   'n_estimators': 400,\n                   'eta': 0.07349402647118564,\n                   'max_depth': 6,\n                   'subsample': 1.0,\n                   'colsample_bytree': 0.6,\n                   'min_child_weight': 9,\n                   'min_child_sample': 32}","893ed2f9":"def ensemble_pipeline(data,best_xgb,best_lgbm):\n    X = data.drop(columns='target',axis=1)\n    y = data['target']\n\n    ##Caculate E_cv (cross validation error)\n    kf = KFold(n_splits=5,shuffle=True,random_state=42)\n    fold_error = []\n    for fold,(train_idx,val_idx) in enumerate(kf.split(X)):\n        train_X,val_X = X.iloc[train_idx,:],X.iloc[val_idx,:]\n        train_y,val_y = y.iloc[train_idx],y.iloc[val_idx]\n        \n        print('Fold %s:'% (fold))\n        ensemble_model = StackingRegressor(estimators = \n                                           [('best_xgb',XGBRegressor(**best_xgb_param)),\n                                            ('best_lgbm',LGBMRegressor(**best_lgbm_param))],\n                                           final_estimator = \n                                           GradientBoostingRegressor(n_estimators=200,\n                                                                 random_state=42))\n        ensemble_model.fit(train_X,train_y)\n        pred_y = ensemble_model.predict(val_X)\n        in_fold_rmse = np.sqrt(np.mean((val_y-pred_y)**2))\n        print('Fold %s rmse: %s\\n' % (fold,in_fold_rmse))\n        fold_error.append(in_fold_rmse)\n        \n    oof_rmse = np.sum(fold_error)\/len(fold_error)\n    print('E_cv: %s' % (oof_rmse))\n\n    ##Train on the whole training data\n    ensemble_model = StackingRegressor(estimators = \n                                        [('best_xgb',best_xgb),\n                                        ('best_lgbm',best_lgbm)],\n                                        final_estimator = \n                                        GradientBoostingRegressor(n_estimators=200,\n                                                                 random_state=42))\n    ensemble_model.fit(X,y)\n    \n    return ensemble_model","c8ad0fd2":"best_xgb = XGBRegressor(**best_xgb_param)\nbest_lgbm = LGBMRegressor(**best_lgbm_param)\nensemble_model = ensemble_pipeline(data,best_xgb,best_lgbm)","b45bd903":"test = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/test.csv')\n#pred_res = xgb_model.predict(test.loc[:,'cont1':])\n#pred_res = best_xgb.predict(test.loc[:,'cont1':])\n#pred_res = lgbm_model.predict(test.loc[:,'cont1':])\n#pred_res = best_lgbm.predict(test.loc[:,'cont1':])\npred_res = ensemble_model.predict(test.loc[:,'cont1':])\nsubmission = pd.DataFrame({'id':test.id,'target':pred_res})\nsubmission.to_csv('submission.csv',index=False)","1ee966b4":"submission.head()","814fe268":"LightGBM baseline model performs better than XGBoost baseline model. On public leaderboard(0.70140 vs 0.70587).","7c06f6bd":"# Ensemble two best models","ecd21340":"# Baseline Model","aff68684":"Features 'cont1', 'cont6'~'cont13' have relatively high correlation with each other. ","2dfe1069":"## Tune XGBoost","15a4bd43":"# Data Visualization and Cleaning","2cfa7f5e":"**Attention: This cleaning method will lead to bad result.**","1491dbde":"target=0 seems to be an extreme outlier.","34bc071c":"## LightGBM","30c95145":"# Submit Result","f8f379b7":"# Optuna","8b59a7bb":"Let's take a look at the boxplot of each feature and target.","0d760d6e":"Check the correlation matrix.","16a45ca9":"Let's also take a look at the box plot of target after cleaning.","ed6ead97":"Clean data under the rule of 1.5*interquantile range","3f96ec86":"## Tune LightGBM","3848031b":"There are some outliers in features 'cont7', 'cont9', 'cont10'","27751375":"No column are dropped due to high absoulte correlation > 0.9","6808c3d0":"## XGBoost"}}