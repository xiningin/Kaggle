{"cell_type":{"161937ec":"code","c4570bd7":"code","2d81a6af":"code","ed015723":"code","06258278":"code","9de685c8":"code","b8123e4d":"code","843942ae":"code","3760ea46":"code","c878eaae":"code","2eeef828":"code","669e1306":"code","f8fa162c":"code","f5db93ca":"code","91cd9837":"code","134742a0":"code","da66fe4a":"markdown","46344bbf":"markdown","360d6d39":"markdown","5b0b4b03":"markdown","bb9bddb1":"markdown","3e27a568":"markdown","ad857e17":"markdown","df719c06":"markdown","cac06e8c":"markdown","a596cef9":"markdown","182e1212":"markdown","64076fa1":"markdown","064c0d8d":"markdown","12a96bad":"markdown","a776d01d":"markdown","507b26ba":"markdown","136550d4":"markdown","7fedf590":"markdown"},"source":{"161937ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.express as px\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c4570bd7":"df1 = pd.read_csv(\"\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv\", low_memory=False)\nprint(df1.shape)\ndf1.head()","2d81a6af":"print(df1.county_connections_ratio.unique())\nprint(np.mean([df1.county_connections_ratio== '[0.18, 1[']))\ndf1_2=df1.dropna()\nnnadf=df1_2.drop(labels=('county_connections_ratio'),axis=1)\ndists_wdata=nnadf.district_id.values","ed015723":"nnadf['pct_black\/hispanic'].describe()","06258278":"nnadf['pct_black\/hispanic'].value_counts()","9de685c8":"df = pd.read_csv(\"\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/8815.csv\", low_memory=False)\nprint(df.shape)\ndf.head()","b8123e4d":"df.groupby('lp_id').mean().sort_values('pct_access',ascending=False).head(5)","843942ae":"#Create Dictionary For the Zones\ni=0\nzones = {}\nfor zone in nnadf.locale.unique():\n    zones[zone]=i\n    i+=1\n#zones={'Rural':0, 'Town': 1,'Suburb': 2, 'City': 3}\nprint('Area zone category tags:',zones)\n#Make the categorical Data\nnnadf.locale=nnadf.locale.map(zones)\n\n#Create Dictionary For the pp_total\ni=0\npp_ = {}\nfor pp in nnadf.pp_total_raw.unique():\n    pp_[pp]=i\n    i+=1\n    \npp_={'[4000, 6000[':0, '[6000, 8000[':1,'[8000, 10000[':2,'[10000, 12000[':3,'[12000, 14000[':4,'[14000, 16000[':5,\n   '[16000, 18000[':6,'[18000, 20000[':7,'[22000, 24000[':8,'[32000, 34000[':9}\nprint('pp_total bracket cat:',pp_)\nnnadf.pp_total_raw=nnadf.pp_total_raw.map(pp_)\n\n\n#Create Dictionary For the pct_black\/hisp\ni=0\np_bh = {}\nfor p in nnadf['pct_black\/hispanic'].unique():\n    p_bh[p]=i\n    i+=1\np_bh = {'[0, 0.2[': 0, '[0.2, 0.4[': 1, '[0.4, 0.6[': 2, '[0.6, 0.8[': 3,'[0.8, 1[' : 4}\nprint('pct_black\/hispanic bracket category:',p_bh)\nnnadf['pct_black\/hispanic']=nnadf['pct_black\/hispanic'].map(p_bh)\n\n\n#Create Dictionary For the pct_free\/reduced\ni=0\np_fr = {}\nfor p in nnadf['pct_free\/reduced'].unique():\n    p_fr[p]=i\n    i+=1\nprint('pct_free\/reduced bracket category :',p_fr)\nnnadf['pct_free\/reduced']=nnadf['pct_free\/reduced'].map(p_fr)\n\n\n\n\n","3760ea46":"nnadf=nnadf.drop(labels=('state'),axis=1)\nnnadf=nnadf.set_index('district_id')\nDistricts_data=np.empty(shape=(dists_wdata.shape[0],10))\ni=0\nfor dist in dists_wdata:      \n    d=str(dist)\n    df = pd.read_csv(\"\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/{}.csv\".format(d), low_memory=False)\n    dist_top5_Ip= df.groupby('lp_id').mean().sort_values('pct_access',ascending=False).head(5)\n    #Districts_data[i,[0]]=dist\n    Districts_data[i,0]=dist#dsitrict name\n    Districts_data[i,1:5]=nnadf.loc[[dist]].values#district data\n    Districts_data[i,5]=dist_top5_Ip.pct_access.mean() # mean of the mean of the top 5 most accessed ip ( platforms)\n    Districts_data[i,6]=dist_top5_Ip.engagement_index.mean()\n    Districts_data[i,7]=dist_top5_Ip.index[0] #districts most used paltform \n    Districts_data[i,8]=dist_top5_Ip.index[1]#districts second most used paltform \n    Districts_data[i,9]=dist_top5_Ip.index[2]# ...thrid...\n    i+=1\n    \ndata= pd.DataFrame(Districts_data)\ndata=data.dropna()\ndata.columns=['district','locale', 'pct_black\/hispanic', 'pct_free\/reduced',\n        'pp_total_raw','most_usedip_pct_access_mean','most_usedip_eng_mean','most_accessed_ip','second_m_a_ip','third_m_a_ip']\ndata.head()","c878eaae":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\n\ny = data['most_usedip_pct_access_mean']  \nfeature_names=['locale', 'pct_black\/hispanic', 'pct_free\/reduced',\n        'pp_total_raw','most_accessed_ip']\nX = data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestRegressor(random_state=0,max_leaf_nodes=10,min_samples_leaf=5 ).fit(train_X, train_y)","2eeef828":"#Quick test on the Val data\nrow_to_show = 3\ndata_for_prediction = val_X.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\nprint('pred:',my_model.predict(data_for_prediction_array),'True:',val_y.iloc[row_to_show])","669e1306":"#Feature improtances\ndict(zip(my_model.feature_importances_,feature_names))","f8fa162c":"df1['pct_black\/hispanic'].value_counts(),nnadf['pct_black\/hispanic'].value_counts()","f5db93ca":"#see if  overfitting\nprint(my_model.score(train_X,train_y),my_model.score(val_X,val_y))\n# Comapre r2 score VS a stupid mean model  \nfrom sklearn.metrics import r2_score\nval_y_pred= my_model.predict(val_X)\nval_y_mean= [train_y.mean() for x in val_y_pred]\nr2_score(val_y, val_y_pred),r2_score(val_y, val_y_mean)","91cd9837":"import shap  # package used to calculate Shap values\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(my_model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)","134742a0":"shap.initjs()\nshap.force_plot(explainer.expected_value, shap_values, data_for_prediction)","da66fe4a":"# Shap Plot & Deductions","46344bbf":"**With the Data ready let's start with the SHAP !**","360d6d39":"Good to see 'pct_black\/hispanic' bearing such little weight !! Hence the title\nhaving said that, one must remember waht it is we are modelling. It is the access probability of each student from a given district. Now the model shows it does not matter if the district has a higher population of people with black or Hispanic Heritage, the probability that any given student will acess will reamain the same. It does not seem to play a role.","5b0b4b03":"# Here is the connected data \nbetween the two datasets ( district features file and the access files of each district)","bb9bddb1":"Remebering the Mapping :\n\nArea zone category tags: {'Suburb': 0, 'Rural': 1, 'City': 2, 'Town': 3}\npp_total bracket cat: {'[4000, 6000[': 0, '[6000, 8000[': 1, '[8000, 10000[': 2, '[10000, 12000[': 3, '[12000, 14000[': 4, '[14000, 16000[': 5, '[16000, 18000[': 6, '[18000, 20000[': 7, '[22000, 24000[': 8, '[32000, 34000[': 9}\npct_black\/hispanic bracket category: {'[0, 0.2[': 0, '[0.2, 0.4[': 1, '[0.4, 0.6[': 2, '[0.6, 0.8[': 3, '[0.8, 1[': 4}\npct_free\/reduced bracket category : {'[0, 0.2[': 0, '[0.2, 0.4[': 1, '[0.4, 0.6[': 2, '[0.6, 0.8[': 3, '[0.8, 1[': 4}","3e27a568":"DISTRICT DATA overview:\n\ndistrict_id - The unique identifier of the school district\n\nstate - The state where the district resides in\n\nlocale - NCES locale classification that categorizes U.S. territory into four types of areas: City, Suburban, Town, and Rural. See Locale Boundaries User's Manual for more information.\n\npct_black\/hispanic - Percentage of students in the districts identified as Black or Hispanic based on 2018-19 NCES data\n\npct_free\/reduced - Percentage of students in the districts eligible for free or reduced-price lunch based on 2018-19 NCES data\n\ncountyconnectionsratio - ratio that for reasons below i decide not to use\n\npptotalraw - Per-pupil total expenditure (sum of local and federal expenditure) from Edunomics Lab's National Education Resource Database on Schools (NERD$) project. The expenditure data are school-by-school, and we use the median value to represent the expenditure of a given school district.","ad857e17":"to confirm that this feature in the selected dataset (districts without Nans in data) is not represented i do an quick analysis of it's composition vs the original data of all the districts(df1)\nthow the rappresentation is half, the distriution seems to hold perfectly fine,so no issue there.","df719c06":"That's it folks !\n\nthis was written with the best intentions,\nAny comments, signs of appretiation, doubts and suggestions are more than welcome, thank you !","cac06e8c":"**DEDUCTIONS :**\n\nThis district scored higher beacuse of the hgiher amount of pubblic funding per Student (high pp_total) and because it was in the suburbs, Less beacuse it was a subsided area ( payign students more likly to log in i guess) \nAnother infleunecer was the mostused platform in the district that could be may be less user friendly and attractive. ","a596cef9":"# **Feature importance**","182e1212":"We are not overfitting and the model as approximative as it is doing significantly better than simply guessing the mean !!","64076fa1":"In order to unite all the files quickly i resort to nummpy witch is mutch faster for this than directly in pandas. \nThis is a habit, probably not necessary for sutch a small resulting data set. For the Random forest model i need to move away from the object type the columns were in into a numerical rappresentation of the categories so i therefor create the dictionary used for the mapping ","064c0d8d":"I leave the following data feature tags below from the EDA notebooks because of the importance of understanding what the stats are if they are to be analysed in a social demographic context.","12a96bad":"# **Approach :** \n\nIn  this note book i try to explor via SHAP the correlation beetween districts features,and the percentage of students that make access on the palforms. To do this i use the mean pct_access of the 5 most frequently used paltforms in their district. \n\nIn order to do this i take a dive into each district that does not contain any Na stats. No districts with Na's are considered. This is just a first approach, a sort of EDA to get a feel of the data. \n","a776d01d":"ENGAGEMENT DATA per District overview:\n\neach district has its own file that holds the following data:\n\ntime - date in \"YYYY-MM-DD\"\n\nlp_id - The unique identifier of the product\n\npct_access - Percentage of students in the district have at least one page-load event of a given product and on a given day\n\nengagement_index - Total page-load events per one thousand students of a given product and on a given day\n\nSo from this file..","507b26ba":"county_connections_rati nearly all is 0.18,-1  or Nan, \nthe 4 district with Nan as connection seem to disurupt the model, may be interesting to see why; But for Now as there is very little varaince among the districts \nwe have got data on we shall discard this column. ","136550d4":"I will afterwards do the mean over them also. This i s very crude, a fast analysis to see what seems to amtter the most about the districts. \n","7fedf590":"i get the top 5 platforms that scored the highest overall mean pct_access in that district."}}