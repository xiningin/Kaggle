{"cell_type":{"68c6cb19":"code","e105719a":"code","20f3cc0b":"code","868cbed2":"code","21e64743":"code","6f6924ec":"code","fa0d64e7":"code","10ae4635":"code","bb894259":"code","63b1303b":"code","40acadfc":"code","fa61002e":"code","3fc6129b":"code","74620bc7":"code","ab569e8c":"code","53809874":"code","ede625ff":"code","ddbc05a8":"code","157f97a7":"code","ee3f4169":"code","62966138":"code","f35b0d0d":"code","d17b6526":"code","ed09de36":"code","863e9d81":"code","b5f65242":"code","23223748":"code","c4301e7f":"code","62d385c6":"code","2edcae37":"code","a23ea13f":"code","fce7b181":"code","2a6e462c":"code","bc1a1e6b":"code","b9785458":"code","4e92fff9":"code","327032ac":"code","125b4b98":"code","32653b05":"code","da132255":"code","784cfe8c":"code","b3b23c16":"code","7a9d7486":"code","4d93d4e9":"code","01b4bd2e":"code","a9dc7998":"code","44c6a478":"code","d7803e73":"code","0c91ea22":"code","eb362312":"code","2bf097f8":"code","4d61a567":"code","1f2201c1":"code","ca01c25e":"code","fd4a2df9":"code","18127809":"code","2a5f6823":"code","ae48cd38":"code","8ee3d711":"code","c0b5295f":"code","da10a923":"code","7be06dbb":"code","3cb1567f":"code","6fe491e4":"code","f99a52ad":"markdown","6861b5bc":"markdown","0dadb6ef":"markdown","5d8ed69d":"markdown","3b1b7261":"markdown","64202253":"markdown","19bf8318":"markdown","7129ede0":"markdown","c2569f1c":"markdown","71ae5cfc":"markdown","acb0eab7":"markdown","2ff00f31":"markdown","5520cc1f":"markdown","cd265fad":"markdown","839deba0":"markdown","2ec4453e":"markdown","f8fec643":"markdown","e072a27c":"markdown","0c4e9d85":"markdown","35b6777e":"markdown","8662f969":"markdown"},"source":{"68c6cb19":"from IPython.display import clear_output\n!pip install imutils\nclear_output()","e105719a":"import numpy as np\nimport pandas as pd\nimport cv2\nfrom PIL import Image\nimport scipy\nimport os\nimport shutil\nimport itertools\nimport imutils\nimport tensorflow as tf\nfrom tensorflow.keras.applications import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.losses import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.preprocessing.image import *\nfrom tensorflow.keras.utils import *\n# import pydot\n\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\nimport tensorflow.keras.backend as K\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom colorama import Fore\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob\nfrom skimage.io import *\n%config Completer.use_jedi = False\nimport time\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras import layers\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils.vis_utils import plot_model\ninit_notebook_mode(connected=True)\nRANDOM_SEED = 123\n\nprint(\"All modules have been imported\")","20f3cc0b":"!apt-get install tree\nclear_output()\n# create new folders\n!mkdir TRAIN TEST VAL TRAIN\/YES TRAIN\/NO TEST\/YES TEST\/NO VAL\/YES VAL\/NO\n!tree -d","868cbed2":"IMG_PATH = \"..\/input\/brain-tumor-detection-mri\/Brain_Tumor_Detection\"\n\n# split the data by train\/val\/test\nignored = {\"pred\"}\n# split the data by train\/val\/test\nfor CLASS in os.listdir(IMG_PATH):\n    if CLASS not in ignored:\n        if not CLASS.startswith('.'):\n            IMG_NUM = len(os.listdir(IMG_PATH +\"\/\"+ CLASS))\n            for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH +\"\/\"+ CLASS)):\n                img = IMG_PATH+ '\/' +  CLASS + '\/' + FILE_NAME\n                if n < 300:\n                    shutil.copy(img, 'TEST\/' + CLASS.upper() + '\/' + FILE_NAME)\n                elif n < 0.8*IMG_NUM:\n                    shutil.copy(img, 'TRAIN\/'+ CLASS.upper() + '\/' + FILE_NAME)\n                else:\n                    shutil.copy(img, 'VAL\/'+ CLASS.upper() + '\/' + FILE_NAME)","21e64743":"def load_data(dir_path, img_size=(100,100)):\n    \"\"\"\n    Load resized images as np.arrays to workspace\n    \"\"\"\n    X = []\n    y = []\n    i = 0\n    labels = dict()\n    for path in tqdm(sorted(os.listdir(dir_path))):\n        if not path.startswith('.'):\n            labels[i] = path\n            for file in os.listdir(dir_path + path):\n                if not file.startswith('.'):\n                    img = cv2.imread(dir_path + path + '\/' + file)\n                    X.append(img)\n                    y.append(i)\n            i += 1\n    X = np.array(X)\n    y = np.array(y)\n    print(f'{len(X)} images loaded from {dir_path} directory.')\n    return X, y, labels\n\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize = (6,6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    cm = np.round(cm,2)\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","6f6924ec":"TRAIN_DIR = 'TRAIN\/'\nTEST_DIR = 'TEST\/'\nVAL_DIR = 'VAL\/'\nIMG_SIZE = (224,224)","fa0d64e7":"X_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\nX_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)\nX_val, y_val, _ = load_data(VAL_DIR, IMG_SIZE)","10ae4635":"y = dict()\ny[0] = []\ny[1] = []\nfor set_name in (y_train, y_val, y_test):\n    y[0].append(np.sum(set_name == 0))\n    y[1].append(np.sum(set_name == 1))\n\ntrace0 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[0],\n    name='No',\n    marker=dict(color='#33cc33'),\n    opacity=0.7\n)\ntrace1 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[1],\n    name='Yes',\n    marker=dict(color='#ff3300'),\n    opacity=0.7\n)\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title='Count of classes in each set',\n    xaxis={'title': 'Set'},\n    yaxis={'title': 'Count'}\n)\nfig = go.Figure(data, layout)\niplot(fig)","bb894259":"def plot_samples(X, y, labels_dict, n=50):\n    \"\"\"\n    Creates a gridplot for desired number of images (n) from the specified set\n    \"\"\"\n    for index in range(len(labels_dict)):\n        imgs = X[np.argwhere(y == index)][:n]\n        j = 10\n        i = int(n\/j)\n\n        plt.figure(figsize=(15,6))\n        c = 1\n        for img in imgs:\n            plt.subplot(i,j,c)\n            plt.imshow(img[0])\n\n            plt.xticks([])\n            plt.yticks([])\n            c += 1\n        plt.suptitle('Tumor: {}'.format(labels_dict[index]))\n        plt.show()","63b1303b":"plot_samples(X_train, y_train, labels, 30)","40acadfc":"def crop_imgs(set_name, add_pixels_value=0):\n    \"\"\"\n    Finds the extreme points on the image and crops the rectangular out of them\n    \"\"\"\n    set_new = []\n    for img in set_name:\n        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        gray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n        # threshold the image, then perform a series of erosions +\n        # dilations to remove any small regions of noise\n        thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n        thresh = cv2.erode(thresh, None, iterations=2)\n        thresh = cv2.dilate(thresh, None, iterations=2)\n\n        # find contours in thresholded image, then grab the largest one\n        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cnts = imutils.grab_contours(cnts)\n        c = max(cnts, key=cv2.contourArea)\n\n        # find the extreme points\n        extLeft = tuple(c[c[:, :, 0].argmin()][0])\n        extRight = tuple(c[c[:, :, 0].argmax()][0])\n        extTop = tuple(c[c[:, :, 1].argmin()][0])\n        extBot = tuple(c[c[:, :, 1].argmax()][0])\n\n        ADD_PIXELS = add_pixels_value\n        new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n        set_new.append(new_img)\n\n    return np.array(set_new)","fa61002e":"import imutils\nimg = cv2.imread('.\/VAL\/NO\/no852.jpg')\nimg = cv2.resize(\n            img,\n            dsize=IMG_SIZE,\n            interpolation=cv2.INTER_CUBIC\n        )\ngray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\ngray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n# threshold the image, then perform a series of erosions +\n# dilations to remove any small regions of noise\nthresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\nthresh = cv2.erode(thresh, None, iterations=2)\nthresh = cv2.dilate(thresh, None, iterations=2)\n\n# find contours in thresholded image, then grab the largest one\ncnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = imutils.grab_contours(cnts)\nc = max(cnts, key=cv2.contourArea)\n\n# find the extreme points\nextLeft = tuple(c[c[:, :, 0].argmin()][0])\nextRight = tuple(c[c[:, :, 0].argmax()][0])\nextTop = tuple(c[c[:, :, 1].argmin()][0])\nextBot = tuple(c[c[:, :, 1].argmax()][0])\n\n# add contour on the image\nimg_cnt = cv2.drawContours(img.copy(), [c], -1, (0, 255, 255), 4)\n\n# add extreme points\nimg_pnt = cv2.circle(img_cnt.copy(), extLeft, 8, (0, 0, 255), -1)\nimg_pnt = cv2.circle(img_pnt, extRight, 8, (0, 255, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extTop, 8, (255, 0, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extBot, 8, (255, 255, 0), -1)\n\n# crop\nADD_PIXELS = 0\nnew_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()","3fc6129b":"plt.figure(figsize=(15,6))\nplt.subplot(141)\nplt.imshow(img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 1. Get the original image')\nplt.subplot(142)\nplt.imshow(img_cnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 2. Find the biggest contour')\nplt.subplot(143)\nplt.imshow(img_pnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 3. Find the extreme points')\nplt.subplot(144)\nplt.imshow(new_img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 4. Crop the image')\nplt.show()","74620bc7":"X_train_crop = crop_imgs(set_name=X_train)\nX_val_crop = crop_imgs(set_name=X_val)\nX_test_crop = crop_imgs(set_name=X_test)","ab569e8c":"plot_samples(X_train_crop, y_train, labels, 30)","53809874":"def save_new_images(x_set, y_set, folder_name):\n    i = 0\n    for (img, imclass) in zip(x_set, y_set):\n        if imclass == 0:\n            cv2.imwrite(folder_name+'NO\/'+str(i)+'.jpg', img)\n        else:\n            cv2.imwrite(folder_name+'YES\/'+str(i)+'.jpg', img)\n        i += 1","ede625ff":"# saving new images to the folder\n!mkdir TRAIN_CROP TEST_CROP VAL_CROP TRAIN_CROP\/YES TRAIN_CROP\/NO TEST_CROP\/YES TEST_CROP\/NO VAL_CROP\/YES VAL_CROP\/NO\n\nsave_new_images(X_train_crop, y_train, folder_name='TRAIN_CROP\/')\nsave_new_images(X_val_crop, y_val, folder_name='VAL_CROP\/')\nsave_new_images(X_test_crop, y_test, folder_name='TEST_CROP\/')","ddbc05a8":"def preprocess_imgs(set_name, img_size):\n    set_new = []\n    for img in set_name:\n        img = cv2.resize(\n            img,\n            dsize=img_size,\n            interpolation=cv2.INTER_CUBIC\n        )\n        set_new.append(preprocess_input(img))\n    return np.array(set_new)","157f97a7":"X_train_prep = preprocess_imgs(set_name=X_train_crop, img_size=IMG_SIZE)\nX_test_prep = preprocess_imgs(set_name=X_test_crop, img_size=IMG_SIZE)\nX_val_prep = preprocess_imgs(set_name=X_val_crop, img_size=IMG_SIZE)","ee3f4169":"plot_samples(X_train_prep, y_train, labels, 30)","62966138":"TRAIN_DIR = 'TRAIN_CROP\/'\nVAL_DIR = 'VAL_CROP\/'\nRANDOM_SEED = 42\ntrain_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input\n)\n\n\ntest_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input\n)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    TRAIN_DIR,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=32,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)\n\nvalidation_generator = test_datagen.flow_from_directory(\n    VAL_DIR,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=16,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)","f35b0d0d":"#Callbacks\nEPOCHS = 15\nes = EarlyStopping(\n    monitor='val_acc', \n    mode='max',\n    patience=6\n)\n\nnClasses = 1","d17b6526":"model_2 = Sequential()\n\nmodel_2.add(Conv2D(16,(3,3),padding='same',input_shape =X_train_prep[0].shape))\nmodel_2.add(BatchNormalization())\nmodel_2.add(Activation('relu'))\nmodel_2.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_2.add(Conv2D(32,(3,3),padding='same'))\nmodel_2.add(BatchNormalization())\nmodel_2.add(Activation('relu'))\nmodel_2.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_2.add(Flatten())\nmodel_2.add(Dense(nClasses,activation='sigmoid'))\n    \nmodel_2.summary()","ed09de36":"plot_model(model_2)","863e9d81":"model_2.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy','AUC'])","b5f65242":"history = model_2.fit_generator(\n    train_generator,\n    steps_per_epoch=50,\n    epochs=EPOCHS,\n    validation_data=validation_generator,\n    validation_steps=25,\n    callbacks=[es]\n)","23223748":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20 , 6))\n#plotting the training\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['auc'])\nplt.title('training')\nplt.xlabel('epoch')\nplt.legend(['train_loss', 'train_accuracy' , 'auc'], loc='upper right')\n\n#plotting the validation\nplt.subplot(1, 2, 2)\nplt.plot(history.history['val_loss'])\nplt.plot(history.history['val_accuracy'])\nplt.plot(history.history['val_auc'])\nplt.title('validation')\nplt.xlabel('epoch')\nplt.legend(['val_loss', 'val_accuracy' , 'val_auc'], loc='upper right')\nplt.show()\n","c4301e7f":"#get the train, test and validation accuracies\ntrain_predictions = model_2.predict(X_train_prep)\nval_predictions = model_2.predict(X_val_prep)\ntest_predictions = model_2.predict(X_test_prep)\n\n#Binarizing all the outputs by setting the threshold to 0.5\ntrain_predictions = [1 if x>0.5 else 0 for x in train_predictions]\nval_predictions = [1 if x>0.5 else 0 for x in val_predictions]\ntest_predictions = [1 if x>0.5 else 0 for x in test_predictions]\n\n#Calculating the accuracy scores\ntrain_accuracy = accuracy_score(y_train, train_predictions)\nval_accuracy = accuracy_score(y_val, val_predictions)\ntest_accuracy = accuracy_score(y_test, test_predictions)\nprint('Train Accuracy = %.2f' % train_accuracy)\nprint('Val Accuracy = %.2f' % val_accuracy)\nprint('Test Accuracy = %.2f' % test_accuracy)\n\n#Plotting the confusion matrix on the test data\nconfusion_mtx = confusion_matrix(y_train, train_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n\nconfusion_mtx = confusion_matrix(y_val, val_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n\nconfusion_mtx = confusion_matrix(y_test, test_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","62d385c6":"test_prob_pred = model_2.predict_proba(X_test_prep)","2edcae37":"from sklearn import metrics\nprint('Accuracy score is :', metrics.accuracy_score(y_test, test_predictions))\nprint('Precision score is :', metrics.precision_score(y_test, test_predictions, average='weighted'))\nprint('Recall score is :', metrics.recall_score(y_test, test_predictions, average='weighted'))\nprint('F1 Score is :', metrics.f1_score(y_test, test_predictions, average='weighted'))\nprint('ROC AUC Score is :', metrics.roc_auc_score(y_test, test_prob_pred,multi_class='ovo', average='weighted'))\nprint('Cohen Kappa Score:', metrics.cohen_kappa_score(y_test, test_predictions))\n\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test, test_predictions))","a23ea13f":"model_3 = Sequential()\n\nmodel_3.add(Conv2D(16,(3,3),padding='same',input_shape =X_train_prep[0].shape))\nmodel_3.add(BatchNormalization())\nmodel_3.add(Activation('relu'))\nmodel_3.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_3.add(Conv2D(32,(3,3),padding='same'))\nmodel_3.add(BatchNormalization())\nmodel_3.add(Activation('relu'))\nmodel_3.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_3.add(Conv2D(64,(3,3),padding='same'))\nmodel_3.add(BatchNormalization())\nmodel_3.add(Activation('relu'))\nmodel_3.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_3.add(Flatten())\nmodel_3.add(Dense(nClasses,activation='sigmoid'))\n    \nmodel_3.summary()","fce7b181":"plot_model(model_3)","2a6e462c":"model_3.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy','AUC'])\nhistory = model_3.fit_generator(\n    train_generator,\n    steps_per_epoch=50,\n    epochs=EPOCHS,\n    validation_data=validation_generator,\n    validation_steps=25,\n    callbacks=[es]\n)","bc1a1e6b":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20 , 6))\n#plotting the training\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['auc'])\nplt.title('training')\nplt.xlabel('epoch')\nplt.legend(['train_loss', 'train_accuracy' , 'auc'], loc='upper right')\n\n#plotting the validation\nplt.subplot(1, 2, 2)\nplt.plot(history.history['val_loss'])\nplt.plot(history.history['val_accuracy'])\nplt.plot(history.history['val_auc'])\nplt.title('validation')\nplt.xlabel('epoch')\nplt.legend(['val_loss', 'val_accuracy' , 'val_auc'], loc='upper right')\nplt.show()","b9785458":"#get the train, test and validation accuracies\ntrain_predictions = model_3.predict(X_train_prep)\nval_predictions = model_3.predict(X_val_prep)\ntest_predictions = model_3.predict(X_test_prep)\n\n#Binarizing all the outputs by setting the threshold to 0.5\ntrain_predictions = [1 if x>0.5 else 0 for x in train_predictions]\nval_predictions = [1 if x>0.5 else 0 for x in val_predictions]\ntest_predictions = [1 if x>0.5 else 0 for x in test_predictions]\n\n#Calculating the accuracy scores\ntrain_accuracy = accuracy_score(y_train, train_predictions)\nval_accuracy = accuracy_score(y_val, val_predictions)\ntest_accuracy = accuracy_score(y_test, test_predictions)\nprint('Train Accuracy = %.2f' % train_accuracy)\nprint('Val Accuracy = %.2f' % val_accuracy)\nprint('Test Accuracy = %.2f' % test_accuracy)\n\n#Plotting the confusion matrix on the test data\nconfusion_mtx = confusion_matrix(y_train, train_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n\nconfusion_mtx = confusion_matrix(y_val, val_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n\nconfusion_mtx = confusion_matrix(y_test, test_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","4e92fff9":"test_prob_pred = model_3.predict_proba(X_test_prep)\nfrom sklearn import metrics\nprint('Accuracy score is :', metrics.accuracy_score(y_test, test_predictions))\nprint('Precision score is :', metrics.precision_score(y_test, test_predictions, average='weighted'))\nprint('Recall score is :', metrics.recall_score(y_test, test_predictions, average='weighted'))\nprint('F1 Score is :', metrics.f1_score(y_test, test_predictions, average='weighted'))\nprint('ROC AUC Score is :', metrics.roc_auc_score(y_test, test_prob_pred,multi_class='ovo', average='weighted'))\nprint('Cohen Kappa Score:', metrics.cohen_kappa_score(y_test, test_predictions))\n\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test, test_predictions))","327032ac":"model_4 = Sequential()\n\nmodel_4.add(Conv2D(16,(3,3),padding='same',input_shape =X_train_prep[0].shape))\nmodel_4.add(BatchNormalization())\nmodel_4.add(Activation('relu'))\nmodel_4.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_4.add(Conv2D(32,(3,3),padding='same'))\nmodel_4.add(BatchNormalization())\nmodel_4.add(Activation('relu'))\nmodel_4.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_4.add(Conv2D(64,(3,3),padding='same'))\nmodel_4.add(BatchNormalization())\nmodel_4.add(Activation('relu'))\nmodel_4.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_4.add(Conv2D(128,(3,3),padding='same'))\nmodel_4.add(BatchNormalization())\nmodel_4.add(Activation('relu'))\nmodel_4.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_4.add(Flatten())\nmodel_4.add(Dense(16, activation='relu'))\nmodel_4.add(Dense(nClasses,activation='sigmoid'))\n    \nmodel_4.summary()","125b4b98":"plot_model(model_4)","32653b05":"model_4.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy','AUC'])\nhistory = model_4.fit_generator(\n    train_generator,\n    steps_per_epoch=50,\n    epochs=EPOCHS,\n    validation_data=validation_generator,\n    validation_steps=25,\n    callbacks=[es]\n)","da132255":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20 , 6))\n#plotting the training\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['auc'])\nplt.title('training')\nplt.xlabel('epoch')\nplt.legend(['train_loss', 'train_accuracy' , 'auc'], loc='upper right')\n\n#plotting the validation\nplt.subplot(1, 2, 2)\nplt.plot(history.history['val_loss'])\nplt.plot(history.history['val_accuracy'])\nplt.plot(history.history['val_auc'])\nplt.title('validation')\nplt.xlabel('epoch')\nplt.legend(['val_loss', 'val_accuracy' , 'val_auc'], loc='upper right')\nplt.show()","784cfe8c":"#get the train, test and validation accuracies\ntrain_predictions = model_4.predict(X_train_prep)\nval_predictions = model_4.predict(X_val_prep)\ntest_predictions = model_4.predict(X_test_prep)\n\n#Binarizing all the outputs by setting the threshold to 0.5\ntrain_predictions = [1 if x>0.5 else 0 for x in train_predictions]\nval_predictions = [1 if x>0.5 else 0 for x in val_predictions]\ntest_predictions = [1 if x>0.5 else 0 for x in test_predictions]\n\n#Calculating the accuracy scores\ntrain_accuracy = accuracy_score(y_train, train_predictions)\nval_accuracy = accuracy_score(y_val, val_predictions)\ntest_accuracy = accuracy_score(y_test, test_predictions)\nprint('Train Accuracy = %.2f' % train_accuracy)\nprint('Val Accuracy = %.2f' % val_accuracy)\nprint('Test Accuracy = %.2f' % test_accuracy)\n\n#Plotting the confusion matrix on the test data\nconfusion_mtx = confusion_matrix(y_train, train_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n\nconfusion_mtx = confusion_matrix(y_val, val_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n\nconfusion_mtx = confusion_matrix(y_test, test_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","b3b23c16":"test_prob_pred = model_4.predict_proba(X_test_prep)\nfrom sklearn import metrics\nprint('Accuracy score is :', metrics.accuracy_score(y_test, test_predictions))\nprint('Precision score is :', metrics.precision_score(y_test, test_predictions, average='weighted'))\nprint('Recall score is :', metrics.recall_score(y_test, test_predictions, average='weighted'))\nprint('F1 Score is :', metrics.f1_score(y_test, test_predictions, average='weighted'))\nprint('ROC AUC Score is :', metrics.roc_auc_score(y_test, test_prob_pred,multi_class='ovo', average='weighted'))\nprint('Cohen Kappa Score:', metrics.cohen_kappa_score(y_test, test_predictions))\n\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test, test_predictions))","7a9d7486":"model_5 = Sequential()\n\nmodel_5.add(Conv2D(16,(3,3),padding='same',input_shape =X_train_prep[0].shape))\nmodel_5.add(BatchNormalization())\nmodel_5.add(Activation('relu'))\nmodel_5.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_5.add(Conv2D(32,(3,3),padding='same'))\nmodel_5.add(BatchNormalization())\nmodel_5.add(Activation('relu'))\nmodel_5.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_5.add(Conv2D(64,(3,3),padding='same'))\nmodel_5.add(BatchNormalization())\nmodel_5.add(Activation('relu'))\nmodel_5.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_5.add(Conv2D(128,(3,3),padding='same'))\nmodel_5.add(BatchNormalization())\nmodel_5.add(Activation('relu'))\nmodel_5.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_5.add(Conv2D(256,(3,3),padding='same'))\nmodel_5.add(BatchNormalization())\nmodel_5.add(Activation('relu'))\nmodel_5.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_5.add(Flatten())\nmodel_5.add(Dense(16, activation='relu'))\nmodel_5.add(Dense(nClasses,activation='sigmoid'))\n    \nmodel_5.summary()","4d93d4e9":"plot_model(model_5)","01b4bd2e":"model_5.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy','AUC'])\nhistory = model_5.fit_generator(\n    train_generator,\n    steps_per_epoch=50,\n    epochs=EPOCHS,\n    validation_data=validation_generator,\n    validation_steps=25,\n    callbacks=[es]\n)","a9dc7998":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20 , 6))\n#plotting the training\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['auc'])\nplt.title('training')\nplt.xlabel('epoch')\nplt.legend(['train_loss', 'train_accuracy' , 'auc'], loc='upper right')\n\n#plotting the validation\nplt.subplot(1, 2, 2)\nplt.plot(history.history['val_loss'])\nplt.plot(history.history['val_accuracy'])\nplt.plot(history.history['val_auc'])\nplt.title('validation')\nplt.xlabel('epoch')\nplt.legend(['val_loss', 'val_accuracy' , 'val_auc'], loc='upper right')\nplt.show()","44c6a478":"#get the train, test and validation accuracies\ntrain_predictions = model_5.predict(X_train_prep)\nval_predictions = model_5.predict(X_val_prep)\ntest_predictions = model_5.predict(X_test_prep)\n\n#Binarizing all the outputs by setting the threshold to 0.5\ntrain_predictions = [1 if x>0.5 else 0 for x in train_predictions]\nval_predictions = [1 if x>0.5 else 0 for x in val_predictions]\ntest_predictions = [1 if x>0.5 else 0 for x in test_predictions]\n\n#Calculating the accuracy scores\ntrain_accuracy = accuracy_score(y_train, train_predictions)\nval_accuracy = accuracy_score(y_val, val_predictions)\ntest_accuracy = accuracy_score(y_test, test_predictions)\nprint('Train Accuracy = %.2f' % train_accuracy)\nprint('Val Accuracy = %.2f' % val_accuracy)\nprint('Test Accuracy = %.2f' % test_accuracy)\n\n#Plotting the confusion matrix on the test data\nconfusion_mtx = confusion_matrix(y_train, train_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n\nconfusion_mtx = confusion_matrix(y_val, val_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n\nconfusion_mtx = confusion_matrix(y_test, test_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","d7803e73":"test_prob_pred = model_5.predict_proba(X_test_prep)\nfrom sklearn import metrics\nprint('Accuracy score is :', metrics.accuracy_score(y_test, test_predictions))\nprint('Precision score is :', metrics.precision_score(y_test, test_predictions, average='weighted'))\nprint('Recall score is :', metrics.recall_score(y_test, test_predictions, average='weighted'))\nprint('F1 Score is :', metrics.f1_score(y_test, test_predictions, average='weighted'))\nprint('ROC AUC Score is :', metrics.roc_auc_score(y_test, test_prob_pred,multi_class='ovo', average='weighted'))\nprint('Cohen Kappa Score:', metrics.cohen_kappa_score(y_test, test_predictions))\n\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test, test_predictions))","0c91ea22":"model_6 = Sequential()\n\nmodel_6.add(Conv2D(16,(3,3),padding='same',input_shape =X_train_prep[0].shape))\nmodel_6.add(BatchNormalization())\nmodel_6.add(Activation('relu'))\nmodel_6.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_6.add(Conv2D(32,(3,3),padding='same'))\nmodel_6.add(BatchNormalization())\nmodel_6.add(Activation('relu'))\nmodel_6.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_6.add(Conv2D(64,(3,3),padding='same'))\nmodel_6.add(BatchNormalization())\nmodel_6.add(Activation('relu'))\nmodel_6.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_6.add(Conv2D(128,(3,3),padding='same'))\nmodel_6.add(BatchNormalization())\nmodel_6.add(Activation('relu'))\nmodel_6.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_6.add(Conv2D(256,(3,3),padding='same'))\nmodel_6.add(BatchNormalization())\nmodel_6.add(Activation('relu'))\nmodel_6.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_6.add(Conv2D(128,(3,3),padding='same'))\nmodel_6.add(BatchNormalization())\nmodel_6.add(Activation('relu'))\nmodel_6.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_6.add(Flatten())\nmodel_6.add(Dense(16, activation='relu'))\nmodel_6.add(Dense(nClasses,activation='sigmoid'))\n    \nmodel_6.summary()\nplot_model(model_6)","eb362312":"model_6.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy','AUC'])\nhistory = model_6.fit_generator(\n    train_generator,\n    steps_per_epoch=50,\n    epochs=EPOCHS,\n    validation_data=validation_generator,\n    validation_steps=25,\n    callbacks=[es]\n)","2bf097f8":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20 , 6))\n#plotting the training\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['auc'])\nplt.title('training')\nplt.xlabel('epoch')\nplt.legend(['train_loss', 'train_accuracy' , 'auc'], loc='upper right')\n\n#plotting the validation\nplt.subplot(1, 2, 2)\nplt.plot(history.history['val_loss'])\nplt.plot(history.history['val_accuracy'])\nplt.plot(history.history['val_auc'])\nplt.title('validation')\nplt.xlabel('epoch')\nplt.legend(['val_loss', 'val_accuracy' , 'val_auc'], loc='upper right')\nplt.show()","4d61a567":"#get the train, test and validation accuracies\ntrain_predictions = model_6.predict(X_train_prep)\nval_predictions = model_6.predict(X_val_prep)\ntest_predictions = model_6.predict(X_test_prep)\n\n#Binarizing all the outputs by setting the threshold to 0.5\ntrain_predictions = [1 if x>0.5 else 0 for x in train_predictions]\nval_predictions = [1 if x>0.5 else 0 for x in val_predictions]\ntest_predictions = [1 if x>0.5 else 0 for x in test_predictions]\n\n#Calculating the accuracy scores\ntrain_accuracy = accuracy_score(y_train, train_predictions)\nval_accuracy = accuracy_score(y_val, val_predictions)\ntest_accuracy = accuracy_score(y_test, test_predictions)\nprint('Train Accuracy = %.2f' % train_accuracy)\nprint('Val Accuracy = %.2f' % val_accuracy)\nprint('Test Accuracy = %.2f' % test_accuracy)\n\n#Plotting the confusion matrix on the test data\nconfusion_mtx = confusion_matrix(y_train, train_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n\nconfusion_mtx = confusion_matrix(y_val, val_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n\nconfusion_mtx = confusion_matrix(y_test, test_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","1f2201c1":"test_prob_pred = model_6.predict_proba(X_test_prep)\nfrom sklearn import metrics\nprint('Accuracy score is :', metrics.accuracy_score(y_test, test_predictions))\nprint('Precision score is :', metrics.precision_score(y_test, test_predictions, average='weighted'))\nprint('Recall score is :', metrics.recall_score(y_test, test_predictions, average='weighted'))\nprint('F1 Score is :', metrics.f1_score(y_test, test_predictions, average='weighted'))\nprint('ROC AUC Score is :', metrics.roc_auc_score(y_test, test_prob_pred,multi_class='ovo', average='weighted'))\nprint('Cohen Kappa Score:', metrics.cohen_kappa_score(y_test, test_predictions))\n\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test, test_predictions))","ca01c25e":"model_7 = Sequential()\n\nmodel_7.add(Conv2D(16,(3,3),padding='same',input_shape =X_train_prep[0].shape))\nmodel_7.add(BatchNormalization())\nmodel_7.add(Activation('relu'))\nmodel_7.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_7.add(Conv2D(32,(3,3),padding='same'))\nmodel_7.add(BatchNormalization())\nmodel_7.add(Activation('relu'))\nmodel_7.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_7.add(Conv2D(64,(3,3),padding='same'))\nmodel_7.add(BatchNormalization())\nmodel_7.add(Activation('relu'))\nmodel_7.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_7.add(Conv2D(128,(3,3),padding='same'))\nmodel_7.add(BatchNormalization())\nmodel_7.add(Activation('relu'))\nmodel_7.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_7.add(Conv2D(256,(3,3),padding='same'))\nmodel_7.add(BatchNormalization())\nmodel_7.add(Activation('relu'))\nmodel_7.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_7.add(Conv2D(512,(3,3),padding='same'))\nmodel_7.add(BatchNormalization())\nmodel_7.add(Activation('relu'))\nmodel_7.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_7.add(Conv2D(128,(3,3),padding='same'))\nmodel_7.add(BatchNormalization())\nmodel_7.add(Activation('relu'))\nmodel_7.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_7.add(Flatten())\nmodel_7.add(Dense(16, activation='relu'))\nmodel_7.add(Dense(nClasses,activation='sigmoid'))\n\nmodel_7.summary()\nplot_model(model_7)","fd4a2df9":"model_7.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy','AUC'])\nhistory = model_7.fit_generator(\n    train_generator,\n    steps_per_epoch=50,\n    epochs=EPOCHS,\n    validation_data=validation_generator,\n    validation_steps=25,\n    callbacks=[es]\n)","18127809":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20 , 6))\n#plotting the training\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['auc'])\nplt.title('training')\nplt.xlabel('epoch')\nplt.legend(['train_loss', 'train_accuracy' , 'auc'], loc='upper right')\n\n#plotting the validation\nplt.subplot(1, 2, 2)\nplt.plot(history.history['val_loss'])\nplt.plot(history.history['val_accuracy'])\nplt.plot(history.history['val_auc'])\nplt.title('validation')\nplt.xlabel('epoch')\nplt.legend(['val_loss', 'val_accuracy' , 'val_auc'], loc='upper right')\nplt.show()","2a5f6823":"#get the train, test and validation accuracies\ntrain_predictions = model_7.predict(X_train_prep)\nval_predictions = model_7.predict(X_val_prep)\ntest_predictions = model_7.predict(X_test_prep)\n\n#Binarizing all the outputs by setting the threshold to 0.5\ntrain_predictions = [1 if x>0.5 else 0 for x in train_predictions]\nval_predictions = [1 if x>0.5 else 0 for x in val_predictions]\ntest_predictions = [1 if x>0.5 else 0 for x in test_predictions]\n\n#Calculating the accuracy scores\ntrain_accuracy = accuracy_score(y_train, train_predictions)\nval_accuracy = accuracy_score(y_val, val_predictions)\ntest_accuracy = accuracy_score(y_test, test_predictions)\nprint('Train Accuracy = %.2f' % train_accuracy)\nprint('Val Accuracy = %.2f' % val_accuracy)\nprint('Test Accuracy = %.2f' % test_accuracy)\n\n#Plotting the confusion matrix on the test data\nconfusion_mtx = confusion_matrix(y_train, train_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n\nconfusion_mtx = confusion_matrix(y_val, val_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n\nconfusion_mtx = confusion_matrix(y_test, test_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","ae48cd38":"test_prob_pred = model_7.predict_proba(X_test_prep)\nfrom sklearn import metrics\nprint('Accuracy score is :', metrics.accuracy_score(y_test, test_predictions))\nprint('Precision score is :', metrics.precision_score(y_test, test_predictions, average='weighted'))\nprint('Recall score is :', metrics.recall_score(y_test, test_predictions, average='weighted'))\nprint('F1 Score is :', metrics.f1_score(y_test, test_predictions, average='weighted'))\nprint('ROC AUC Score is :', metrics.roc_auc_score(y_test, test_prob_pred,multi_class='ovo', average='weighted'))\nprint('Cohen Kappa Score:', metrics.cohen_kappa_score(y_test, test_predictions))\n\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test, test_predictions))","8ee3d711":"model_8 = Sequential()\n\nmodel_8.add(Conv2D(16,(3,3),padding='same',input_shape =X_train_prep[0].shape))\nmodel_8.add(BatchNormalization())\nmodel_8.add(Activation('relu'))\nmodel_8.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_8.add(Conv2D(32,(3,3),padding='same'))\nmodel_8.add(BatchNormalization())\nmodel_8.add(Activation('relu'))\nmodel_8.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_8.add(Conv2D(64,(3,3),padding='same'))\nmodel_8.add(BatchNormalization())\nmodel_8.add(Activation('relu'))\nmodel_8.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_8.add(Conv2D(128,(3,3),padding='same'))\nmodel_8.add(BatchNormalization())\nmodel_8.add(Activation('relu'))\nmodel_8.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_8.add(Conv2D(256,(3,3),padding='same'))\nmodel_8.add(BatchNormalization())\nmodel_8.add(Activation('relu'))\nmodel_8.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_8.add(Conv2D(512,(3,3),padding='same'))\nmodel_8.add(BatchNormalization())\nmodel_8.add(Activation('relu'))\nmodel_8.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_8.add(Conv2D(128,(3,3),padding='same'))\nmodel_8.add(BatchNormalization())\nmodel_8.add(Activation('relu'))\nmodel_8.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_8.add(Conv2D(64,(3,3),padding='same'))\nmodel_8.add(BatchNormalization())\nmodel_8.add(Activation('relu'))\nmodel_8.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))\n\nmodel_8.add(Flatten())\nmodel_8.add(Dense(16, activation='relu'))\nmodel_8.add(Dense(nClasses,activation='sigmoid'))\n\nmodel_8.summary()\nplot_model(model_8)","c0b5295f":"model_8.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy','AUC'])\nhistory = model_8.fit_generator(\n    train_generator,\n    steps_per_epoch=50,\n    epochs=EPOCHS,\n    validation_data=validation_generator,\n    validation_steps=25,\n    callbacks=[es]\n)","da10a923":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20 , 6))\n#plotting the training\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['auc'])\nplt.title('training')\nplt.xlabel('epoch')\nplt.legend(['train_loss', 'train_accuracy' , 'auc'], loc='upper right')\n\n#plotting the validation\nplt.subplot(1, 2, 2)\nplt.plot(history.history['val_loss'])\nplt.plot(history.history['val_accuracy'])\nplt.plot(history.history['val_auc'])\nplt.title('validation')\nplt.xlabel('epoch')\nplt.legend(['val_loss', 'val_accuracy' , 'val_auc'], loc='upper right')\nplt.show()","7be06dbb":"#get the train, test and validation accuracies\ntrain_predictions = model_8.predict(X_train_prep)\nval_predictions = model_8.predict(X_val_prep)\ntest_predictions = model_8.predict(X_test_prep)\n\n#Binarizing all the outputs by setting the threshold to 0.5\ntrain_predictions = [1 if x>0.5 else 0 for x in train_predictions]\nval_predictions = [1 if x>0.5 else 0 for x in val_predictions]\ntest_predictions = [1 if x>0.5 else 0 for x in test_predictions]\n\n#Calculating the accuracy scores\ntrain_accuracy = accuracy_score(y_train, train_predictions)\nval_accuracy = accuracy_score(y_val, val_predictions)\ntest_accuracy = accuracy_score(y_test, test_predictions)\nprint('Train Accuracy = %.2f' % train_accuracy)\nprint('Val Accuracy = %.2f' % val_accuracy)\nprint('Test Accuracy = %.2f' % test_accuracy)\n\n#Plotting the confusion matrix on the test data\nconfusion_mtx = confusion_matrix(y_train, train_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n\nconfusion_mtx = confusion_matrix(y_val, val_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n\nconfusion_mtx = confusion_matrix(y_test, test_predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","3cb1567f":"test_prob_pred = model_8.predict_proba(X_test_prep)\nfrom sklearn import metrics\nprint('Accuracy score is :', metrics.accuracy_score(y_test, test_predictions))\nprint('Precision score is :', metrics.precision_score(y_test, test_predictions, average='weighted'))\nprint('Recall score is :', metrics.recall_score(y_test, test_predictions, average='weighted'))\nprint('F1 Score is :', metrics.f1_score(y_test, test_predictions, average='weighted'))\nprint('ROC AUC Score is :', metrics.roc_auc_score(y_test, test_prob_pred,multi_class='ovo', average='weighted'))\nprint('Cohen Kappa Score:', metrics.cohen_kappa_score(y_test, test_predictions))\n\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test, test_predictions))","6fe491e4":"# clean up the space\n!rm -rf TRAIN TEST VAL TRAIN_CROP TEST_CROP VAL_CROP","f99a52ad":"<h2> Creating the required Directory structure<\/h2>\n<p> The dataset contains three folders <dl>\n    <dt>yes<\/dt>\n    <dd>This folder contains the MRI scans of the brains which have tumors <\/dd>\n    <dt>no<\/dt>\n    <dd>This folder contains the MRI scans of the brains which do not have a tumor<dd>\n    <dt>pred<\/dt>\n    <dd>This folder  ains unlabelled MRI scans<\/dd>\n    <\/dl><\/p>\n \n<p>For our problem we are not going to use the <b>\"pred\"<\/b> folder. <\/p>\n<p> We are going to create three separate folders<ul>\n    <li>TRAIN<\/li>\n    <li>TEST<\/li>\n    <li>VAL<\/li><\/ul><\/p>\n   \n<p>Each of these folder will have two folders : <dl>\n    <dt>no<\/dt>\n    <dd>Folder containing negative samples (no brain tumor)<\/dd>\n    <dt>yes<\/dt>\n    <dd>Folder containing positive sample (brain tumor is present<\/dd><\/dl><\/p>\n    ","6861b5bc":"<h2> 7 Layered CNN <\/h2>","0dadb6ef":"<h1> Brain tumor detection using custom CNN models <\/h1>\n<p> In this notebook, multiple <b>CNNs<\/b> of <b>varying depths<\/b> have been used to create an <b>image classification network<\/b> to detect Brain Tumors. Keras API and tensorflow have been used in this notebook.  \n","5d8ed69d":"<h2> Let's define some helpful functions <\/h2>","3b1b7261":"<h2>Conclusion<\/h2>\n\n<p> The larger models could have performed better with dropouts, learning rate scheduler, greater number of training samples and more number of epochs. Due to several constraints, I was unable to try all of these out. But will try them all some time in the future !! <p>","64202253":"<h2> 6 Layered CNN <\/h2>","19bf8318":"<h2> Let's visualize the images we are working with <\/h2>","7129ede0":"<h2> Resizing the images <\/h2>\n<p> Now that we have cropped the images, we can resize them without suffering from heavy distortions or resizing artefacts<\/p>","c2569f1c":"<h2> 3 Layered CNN <\/h2>","71ae5cfc":"In all of the previous notebooks, I had used image augmentations to increase the number of training samples. In this notebook, I am not going toe image augmentations to study the effect of data on the performance of a model on a qualitative scale. \n\nIn this notebook, the aim is to classify these images by training CNNs from scratch. I will be using CNNs of varying depths ranging from 2 layered CNNs going all the way up to 8 layered CNNs. The performance of the models will be evaluated on a test set using various metrics like Cohen's Kappa Score, curacy, F1 score, AUC score, etc.\n","acb0eab7":"<h2> 4 layered CNN <\/h2>","2ff00f31":" <h2> Let's define our callbacks for the CNN models <\/h2>","5520cc1f":"<h2> 5 Layered CNN <\/h2>","cd265fad":"<h2> Let's visulaize the cropping process step by step <\/h2>","839deba0":"<h2>Let's visualize the images after being cropped<\/h2>","2ec4453e":"<h2> Importing Necessary Libraries <\/h2>","f8fec643":"<h2> Creating the models <\/h2>\n\n<p> As mentioned earlier, in this notebook I am going to create Convolutional Neural Networks of varyiing depths and train them from scatch. <\/p>\n","e072a27c":"<h2>8 Layered CNN <\/h2>","0c4e9d85":"<h2> Plotting the distribution of data in train, test and validation sets<\/h2>","35b6777e":"<p> Normally at this stage the images would undergo image augmentations. But for now, the image will not go through any image augmentations. This is being done to numerically analyze the impact that the data can have on a model performance<\/p>\n    ","8662f969":"<h2> Cropping the images<\/h2>\n<p>The images we have are of different sizes. But our model accepts images of size (224*224*3) as input. To achienve this ew have to resize the images. Blindly resizing the images can lead to extreme distortions in the images. Hence, We will first crop thie images and then resize them. This will minimize the issue of distortions.<\/p>\n<p>This cropping is done by finding contours in the images using the OpenCV Library<\/p>    "}}