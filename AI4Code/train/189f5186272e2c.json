{"cell_type":{"bbf97cf6":"code","4457daf5":"code","9f66d51d":"code","530788df":"code","7dc81ddf":"code","d5977a36":"code","66f707bd":"code","95c780e2":"code","f941fee7":"code","669ef097":"code","ed25ac33":"code","412b7393":"code","eea79866":"code","b91c159a":"code","993d5f6f":"code","66c399e0":"code","c6bb2bc4":"code","695c05b7":"code","d14e7edc":"code","1f139e31":"code","b5073b0b":"code","f6887650":"code","eab80d6b":"code","2b42ca84":"code","acfbb6da":"code","cfd75626":"code","bf5b9e1e":"code","626cfaab":"code","367f931b":"code","b8ca53e2":"code","4b67751a":"code","650468e9":"code","5f98df9b":"code","538348f7":"code","6d38881a":"code","c29e9a28":"code","91954445":"code","b6aedbb2":"code","cf1096a3":"code","811a0ddb":"code","cc3285cd":"code","6dc9aafd":"code","88a353a6":"code","9ac1a881":"code","c937f0d9":"code","1d2beba8":"code","3fb2e0a4":"code","53d8bda7":"code","b0cecc24":"code","42a7cb26":"code","6d2782a6":"code","656ef1e2":"code","1529359c":"code","cbed222b":"code","5015490d":"code","e4078466":"code","f9ba6670":"code","5e417526":"code","a123d0eb":"code","7b436dd9":"code","b46ed3c9":"code","f0a8d2a5":"code","97878d00":"code","0fbd772d":"code","04b1f071":"code","7dfb072d":"code","3dc2621e":"code","46bf558a":"code","0556f73b":"code","d5fc3796":"code","7d3d83a8":"code","c001e321":"code","8f9a9d3c":"code","6d606a4f":"code","443db83a":"code","4bab46a7":"code","339e7b73":"code","8739fbd9":"code","96000fba":"code","d329c7db":"code","b6bc3118":"code","945f7df7":"code","b2cd9215":"code","72d2732e":"code","032a9150":"code","f9b53714":"code","a28d9162":"code","2717c465":"code","5d789091":"code","4fb186dc":"code","67946ed0":"markdown"},"source":{"bbf97cf6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4457daf5":"train=pd.read_csv('\/kaggle\/input\/amexpert\/train.csv')\ns=pd.read_csv('\/kaggle\/input\/amexpert\/sample_submission_Byiv0dS.csv')\ncoup_item=pd.read_csv('\/kaggle\/input\/amexpert\/coupon_item_mapping.csv')\ntest=pd.read_csv('\/kaggle\/input\/amexpert\/test_QyjYwdj.csv')\ncomp=pd.read_csv('\/kaggle\/input\/amexpert\/campaign_data.csv')\ntran=pd.read_csv('\/kaggle\/input\/amexpert\/customer_transaction_data.csv')\ndemo=pd.read_csv('\/kaggle\/input\/amexpert\/customer_demographics.csv')\nitem=pd.read_csv('\/kaggle\/input\/amexpert\/item_data.csv')\nprint(train.shape,test.shape,coup_item.shape,comp.shape,tran.shape,demo.shape,item.shape)","9f66d51d":"train.head()","530788df":"print(train.shape)\ntrain.redemption_status.value_counts()","7dc81ddf":"print(comp.shape)\ncomp.head()","d5977a36":"df=train.append(test,ignore_index=True)\ndf.head()","66f707bd":"comp['start_date']=pd.to_datetime(comp['start_date'],format='%d\/%m\/%y',dayfirst=True)\ncomp['end_date']=pd.to_datetime(comp['end_date'],format='%d\/%m\/%y',dayfirst=True)\n\n# comp['start_date_d']=comp['start_date'].dt.day.astype('category')\n# comp['start_date_m']=comp['start_date'].dt.month.astype('category')\n# comp['start_date_y']=comp['start_date'].dt.year.astype('category')\n# comp['start_date_w']=comp['start_date'].dt.week.astype('category')\n\n\n# comp['end_date_d']=comp['end_date'].dt.day.astype('category')\n# comp['end_date_m']=comp['end_date'].dt.month.astype('category')\n# comp['end_date_y']=comp['end_date'].dt.year.astype('category')\n# comp['end_date_w']=comp['end_date'].dt.week.astype('category')\n\n\ncomp['diff_d']=(comp['end_date']-comp['start_date'])\/np.timedelta64(1,'D')\ncomp['diff_m']=(comp['end_date']-comp['start_date'])\/np.timedelta64(1,'M')\ncomp['diff_w']=(comp['end_date']-comp['start_date'])\/np.timedelta64(1,'W')\n\n# comp.drop(['start_date','end_date'],axis=1,inplace=True)","95c780e2":"comp.describe(include='all').T","f941fee7":"comp.head()","669ef097":"df=df.merge(comp,on='campaign_id',how='left')\ndf.head()","ed25ac33":"for j in ['brand', 'brand_type', 'category']:\n    print(j,item[j].nunique())","412b7393":"\nfor j in ['brand', 'brand_type', 'category']:\n    item[j]=item[j].astype('category')\n    \ncoup_item=coup_item.merge(item,on='item_id',how='left')\n","eea79866":"coup_item.coupon_id.nunique()","b91c159a":"coup_item.head(),coup_item.shape","993d5f6f":"tran=pd.read_csv('\/kaggle\/input\/amexpert\/customer_transaction_data.csv')\ntran['date']=pd.to_datetime(tran['date'],format='%Y-%m-%d')\ntran['date_d']=tran['date'].dt.day.astype('category')\ntran['date_m']=tran['date'].dt.month.astype('category')\ntran['date_w']=tran['date'].dt.week.astype('category')\n\n# tran.drop('date',axis=1,inplace=True)\ntran.head()","66c399e0":"tran[tran['quantity']==20]","c6bb2bc4":"tran['discount_bin']=tran['coupon_discount'].apply(lambda x: 0 if x>=0 else 1)\ntran['marked_price']=tran['selling_price']-tran['other_discount']-tran['coupon_discount']\ntran['disc_percent']=(tran['marked_price']-tran['selling_price'])\/tran['selling_price']\ntran['price_per_quan']=tran['marked_price']\/tran['quantity']\ntran['marked_by_sale']=tran['marked_price']\/tran['selling_price']\n","695c05b7":"tran.columns","d14e7edc":"tran=tran.merge(coup_item,on='item_id',how='left')\ntran.head()","1f139e31":"print(tran.shape)\ntran=tran[tran.duplicated()==False]\nprint(tran.shape,train.shape)\n# --drop it","b5073b0b":"tran.head()","f6887650":"tran=tran.merge(tran.groupby(['customer_id','date']).agg({'coupon_id':'count','item_id':'count','disc_percent':sum}).reset_index()\n                .rename(columns={'coupon_id':'coupon_aquired','item_id':'item_bought','disc_percent':'tot_disc'}),on=['customer_id','date'],how='left')","eab80d6b":"tran[(tran['customer_id']==1052) & (tran['coupon_id']==21)]","2b42ca84":"tran['coupon_to_item']=tran['item_bought']-tran['coupon_aquired']","acfbb6da":"tran[(tran['customer_id']==413) & (tran['coupon_id']==577)]","cfd75626":"df[(df['customer_id']==413) & (df['coupon_id']==577)]","bf5b9e1e":"# tran.groupby(['customer_id','coupon_id']).agg({'date':set}).reset_index()\ntran.head()","626cfaab":"def func(a,b,c):\n    if c!=0:\n        c=list(c)\n        v=0\n        for k in c:\n            if a<=k and b>k:\n                v+=1\n        return v\n    else:\n        return 0\n# cc['within']=cc.apply(lambda x: func(x['start_date'],x['end_date'],x['date']),axis=1)","367f931b":"# Magic features\n# tran.groupby(['customer_id','date']).agg({'coupon_id':'count','discount_bin':sum,'quantity':sum,'item_id':'count'}).reset_index()","b8ca53e2":"df.head()","4b67751a":"# cc=df.merge(tran.groupby(['customer_id','date']).agg({'coupon_id':'count','discount_bin':sum,'quantity':sum,'item_id':'count'}).reset_index(),on=['customer_id','date'],how='left')\n# cc.sample(10)\ntran.columns","650468e9":"ddf=df.merge(tran.groupby(['customer_id','coupon_id']).agg({'date':set,'discount_bin':sum,'quantity':sum,'item_id':'count',\n                                                            'coupon_aquired':sum,'item_bought':'mean','tot_disc':sum}).reset_index(),on=['customer_id','coupon_id'],how='left')\nddf.sample(10)","5f98df9b":"ddf['coupon_aquired'].fillna(0)","538348f7":"# def new_df(df):\n\nprint(ddf.shape)\nddf['date'].replace(np.nan,0,inplace=True)\nddf['discount_bin'].replace(np.nan,-1,inplace=True)\n# ddf['quantity'].replace(np.nan,0,inplace=True)\n# ddf['item_id'].replace(np.nan,0,inplace=True)\n# df['camp_date_within_count']=ddf.apply(lambda x: func(x['start_date'],x['end_date'],x['date']),axis=1)\n\n\n# df['bin']=ddf['discount_bin'].apply(lambda x: 1 if x!=-1 else 0)\ndf['within_date']=ddf['date'].apply(lambda x: len(x) if x !=0 else 0)\n# df['C1']=ddf['coupon_aquired'].fillna(0)\n# df['C2']=ddf['item_bought'].fillna(0)\n# df['C3']=ddf['tot_disc'].fillna(0)\n\n\n\n\n# df['within_date_discount']=ddf['discount_bin'].apply(lambda x: x if x >=0 else 0)\n#     df['quantity_date']=ddf['quantity']\n# df['item_count']=ddf['item_id']\n\n# -- worked good\n    # df['quantity_date']=ddf['quantity']\n    # df['item_count']=ddf['item_id']\n","6d38881a":"ddf.head()","c29e9a28":"\n# %time\n# within_date=[]\n# from tqdm import tqdm_notebook as tqdm\n# for i in tqdm(range(df.shape[0])):\n#     st_dt=df.loc[i,'start_date']\n#     en_dt=df.loc[i,'end_date']\n#     cust=df.loc[i,'customer_id']\n#     coup=df.loc[i,'coupon_id']\n# #     temp=tran[(tran['date']>=st_dt) & (tran['date']<en_dt) & (tran['customer_id']==cust) & (tran['coupon_id']==coup)]\n# #     temp=temp[temp.duplicated()==False]\n#     if tran[(tran['date']>=st_dt) & (tran['date']<en_dt) & (tran['customer_id']==cust) & (tran['coupon_id']==coup)].shape[0]>0:\n#         within_date.append(1)\n#     else:\n#         within_date.append(0)\n# #     print(temp.shape,df.loc[i,'redemption_status'])\n\n# df['within_date']=within_date\ntran.columns","91954445":"c=['count','nunique']\nn=['mean','max','min','sum','std']\nnn=['mean','max','min','sum','std','quantile']\n# agg_c={'date_d':c,'date_m':c,'date_w':c,'quantity':n,'selling_price':n,'other_discount':n,'coupon_discount':n,'item_id':c,'brand':c,\n#        'category':c,'coupon_id':c,'discount_bin':nn,'marked_price':n,'disc_percent':n,'price_per_quan':n,'brand_type':c,'marked_by_sale':n,\n#        'coupon_aquired':nn, 'item_bought':nn, 'tot_disc':n, 'coupon_to_item':nn}\n\n\nagg_c={'date_d':c,'date_m':c,'date_w':c,'quantity':n,'selling_price':n,'other_discount':n,'coupon_discount':n,'item_id':c,'brand':c,\n       'category':c,'coupon_id':c,'discount_bin':nn,'marked_price':n,'disc_percent':n,'price_per_quan':n,'brand_type':c,'marked_by_sale':n,\n       'coupon_aquired':nn, 'item_bought':nn, 'tot_disc':n, 'coupon_to_item':nn}\ntrans=tran.groupby(['customer_id']).agg(agg_c)\ntrans.head()","b6aedbb2":"trans.columns=['F_' + '_'.join(col).strip() for col in trans.columns.values]\ntrans.reset_index(inplace=True)\ntrans.head()","cf1096a3":"trans.shape","811a0ddb":"df=df.merge(trans,on=['customer_id'],how='left')\n\n\n# -------to uncomment\n\n# df.head()","cc3285cd":"df['campaign_type']=df['campaign_type'].astype('category')","6dc9aafd":"# df['campaign_id']=df['campaign_id'].astype('category')\n# df['coupon_id']=df['coupon_id'].astype('category')\n# df['customer_id']=df['customer_id'].astype('category')\n# df['campaign_type']=df['campaign_type'].astype('category')\n\n# df['within_date_discount'].value_counts()","88a353a6":"df.info()","9ac1a881":"df_train=df[df['redemption_status'].isnull()==False].copy()\ndf_test=df[df['redemption_status'].isnull()==True].copy()\n\nprint(df_train.shape,df_test.shape)","c937f0d9":"df_train.merge(df_train.drop(['id','redemption_status'],axis=1).groupby('campaign_id').mean().reset_index(),on='campaign_id',how='left')","1d2beba8":"df_train=df_train.merge(df_train.drop(['id','redemption_status'],axis=1).groupby('coupon_id').mean().reset_index(),on='coupon_id',how='left')\ndf_test=df_test.merge(df_test.drop(['id','redemption_status'],axis=1).groupby('coupon_id').mean().reset_index(),on='coupon_id',how='left')\n\n# df_train=df_train.merge(df_train.drop(['id','redemption_status'],axis=1).groupby('coupon_id_x').mean().reset_index(),on='coupon_id_x',how='left')\n# df_test=df_test.merge(df_test.drop(['id','redemption_status'],axis=1).groupby('coupon_id_x').mean().reset_index(),on='coupon_id_x',how='left')\n\n\n\n# df_train=new_df(df_train)\n# print(df_train.shape)\n\n# df_train.head()","3fb2e0a4":"df_train[df_train.redemption_status==1]","53d8bda7":"import seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(15,15))\n\n# sns.heatmap(df_train.corr())","b0cecc24":"from catboost import CatBoostClassifier,Pool, cv\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold,train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score","42a7cb26":"df_train.columns","6d2782a6":"# X,y=df_train.drop(['id','redemption_status'],axis=1),df_train['redemption_status']\n# Xtest=df_test.drop(['id','redemption_status'],axis=1)\n# col_to_drop=['id','redemption_status','start_date','end_date','F_quantity_min','F_other_discount_max','F_coupon_discount_max','F_discount_bin_min',\n#              'F_disc_percent_min','F_brand_type_nunique','F_marked_by_sale_min','customer_id','campaign_id','coupon_id']\n\ncol_to_drop=['id','redemption_status','start_date','end_date']\n\nX,y=df_train.drop(col_to_drop,axis=1),df_train['redemption_status']\nXtest=df_test.drop(col_to_drop,axis=1)\n\n# X,y=df_train.drop(['id','redemption_status','start_date','end_date','customer_id','coupon_id','campaign_id'],axis=1),df_train['redemption_status']\n# Xtest=df_test.drop(['id','redemption_status','start_date','end_date','customer_id','coupon_id','campaign_id'],axis=1)\n\n# X=pd.get_dummies(X,drop_first=True)\n\n# from sklearn.ensemble import IsolationForest\n# clf = IsolationForest(contamination = 'auto', random_state=1994,behaviour=\"new\",bootstrap=True)\n# clf.fit(X)\n# df_train['iso_out']=clf.predict(X)\n# print(df_train['iso_out'].value_counts())\n\n\n\n\n\n# print(df_train[df_train['iso_out']==1].shape)\n# print(df_train[df_train['iso_out']==-1].shape)","656ef1e2":"# X['iso_out'].value_counts()\n# X=X[X.iso_out==1].copy()\nprint(X.shape,Xtest.shape)\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.3,random_state = 1994,stratify=y)","1529359c":"X_train.columns","cbed222b":"col=['campaign_id', 'coupon_id', 'customer_id', 'campaign_type','within_date', 'within_date_discount']","5015490d":"# # for j in col:\n# X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.3,random_state = 1994,stratify=y)\n# print('Dropped->',j)\n# X_train.drop(col,inplace=True,axis=1)\n# X_val.drop(col,inplace=True,axis=1)\n# m=LGBMClassifier(n_estimators=1500,random_state=1994,learning_rate=0.03,reg_alpha=0.2,colsample_bytree=0.5,bagging_fraction=0.9)\n# # m=RidgeCV(cv=4)\n# m.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_val, y_val.values)],eval_metric='auc', early_stopping_rounds=100,verbose=200)\n# p=m.predict_proba(X_val)[:,-1]\n\n# print(roc_auc_score(y_val,p))\n# print('---------------------')","e4078466":"from scipy.special import logit\nm=LGBMClassifier(n_estimators=1500,random_state=1994,learning_rate=0.03,reg_alpha=0.2,colsample_bytree=0.5,reg_lambda=20)\n# m=RidgeCV(cv=4)\nm.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_val, y_val.values)],eval_metric='auc', early_stopping_rounds=100,verbose=200)\np=m.predict_proba(X_val)[:,-1]\np1=logit(p)\nprint(roc_auc_score(y_val,p))\nprint(roc_auc_score(y_val,p1))","f9ba6670":"p1","5e417526":"m.feature_importances_","a123d0eb":"confusion_matrix(y_val,p>0.5)","7b436dd9":"sorted(zip(m.feature_importances_,X_train),reverse=True)","b46ed3c9":"err=[]\ny_pred_tot=[]\n\n\nfeature_importance_df = pd.DataFrame()\ngr=X.campaign_id_x.values\nfrom sklearn.model_selection import KFold,StratifiedKFold,GroupKFold\nfold=GroupKFold(n_splits=10)\ni=1\nfor train_index, test_index in fold.split(X,y,gr):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    m=LGBMClassifier(n_estimators=5000,random_state=1994,learning_rate=0.03,reg_alpha=0.2,colsample_bytree=0.5)\n    m.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_test, y_test)],eval_metric='auc', early_stopping_rounds=200,verbose=200)\n    \n    preds=m.predict_proba(X_test,num_iteration=m.best_iteration_)[:,-1]\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = X_train.columns\n    fold_importance_df[\"importance\"] = m.feature_importances_\n    fold_importance_df[\"fold\"] = i + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    \n    print(\"err: \",roc_auc_score(y_test,preds))\n    err.append(roc_auc_score(y_test,preds))\n    p = m.predict_proba(Xtest)[:,-1]\n    i=i+1\n    y_pred_tot.append(p)","f0a8d2a5":"np.mean(err,0)\n","97878d00":"all_features = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)\nall_features.reset_index(inplace=True)\nimportant_features = list(all_features[0:170]['feature'])\nall_features[0:170]","0fbd772d":"df1 = X[important_features]\ncorr_matrix = df1.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nhigh_cor = [column for column in upper.columns if any(upper[column] > 0.98)]\nprint(len(high_cor))\nprint(high_cor)","04b1f071":"features = [i for i in important_features if i not in high_cor]\nprint(len(features))\nprint(features)","7dfb072d":"X=X[features]\nXtest=Xtest[features]","3dc2621e":"err=[]\ny_pred_tot=[]\n\n# feature_importance_df = pd.DataFrame()\n\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfold=GroupKFold(n_splits=15)\ni=1\nfor train_index, test_index in fold.split(X,y,gr):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    m=LGBMClassifier(n_estimators=5000,random_state=1994,learning_rate=0.03,reg_alpha=0.2,colsample_bytree=0.5)\n    m.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_test, y_test)],eval_metric='auc', early_stopping_rounds=200,verbose=200)\n    \n    preds=m.predict_proba(X_test,num_iteration=m.best_iteration_)[:,-1]\n    \n#     fold_importance_df = pd.DataFrame()\n#     fold_importance_df[\"feature\"] = X_train.columns\n#     fold_importance_df[\"importance\"] = m.feature_importances_\n#     fold_importance_df[\"fold\"] = i + 1\n#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    \n    print(\"err: \",roc_auc_score(y_test,preds))\n    err.append(roc_auc_score(y_test,preds))\n    p = m.predict_proba(Xtest)[:,-1]\n    i=i+1\n    y_pred_tot.append(p)","46bf558a":"np.mean(err,0)","0556f73b":"s['redemption_status']=np.mean(y_pred_tot,0)\ns.head()\n","d5fc3796":"sum(s.redemption_status>0.5)","7d3d83a8":"s.to_csv('AV_amex_lgb_folds_v39.csv',index=False)\ns.shape","c001e321":"# print(X.shape,Xtest.shape)\n\n# X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.25,random_state = 1994,stratify=y)\n# categorical_features_indices = np.where(X_train.dtypes =='category')[0]\n# categorical_features_indices","8f9a9d3c":"# m=CatBoostClassifier(n_estimators=2500,random_state=1994,learning_rate=0.03,eval_metric='AUC')\n# # m=RidgeCV(cv=4)\n# m.fit(X_train,y_train,eval_set=[(X_val, y_val.values)], early_stopping_rounds=300,verbose=200,cat_features=categorical_features_indices)\n# p=m.predict_proba(X_val)[:,-1]\n# print(roc_auc_score(y_val,p))\n\n\n# 0:\ttest: 0.7072835\tbest: 0.7072835 (0)\ttotal: 94.6ms\tremaining: 3m 56s\n# 200:\ttest: 0.9846892\tbest: 0.9846892 (200)\ttotal: 6.49s\tremaining: 1m 14s\n# 400:\ttest: 0.9857940\tbest: 0.9857940 (400)\ttotal: 13.1s\tremaining: 1m 8s\n# 600:\ttest: 0.9860940\tbest: 0.9860980 (590)\ttotal: 19.6s\tremaining: 1m 1s\n# 800:\ttest: 0.9861993\tbest: 0.9862259 (737)\ttotal: 25.9s\tremaining: 54.9s\n# 1000:\ttest: 0.9862786\tbest: 0.9863095 (882)\ttotal: 32.2s\tremaining: 48.2s\n# 1200:\ttest: 0.9863154\tbest: 0.9863839 (1056)\ttotal: 38.5s\tremaining: 41.6s\n# 1400:\ttest: 0.9863995\tbest: 0.9864448 (1265)\ttotal: 44.8s\tremaining: 35.1s\n# Stopped by overfitting detector  (300 iterations wait)\n\n# bestTest = 0.9864447541\n# bestIteration = 1265\n\n# Shrink model to first 1266 iterations.\n# 0.9864447540507506","6d606a4f":"# errCB=[]\n# y_pred_tot_cb=[]\n# from sklearn.model_selection import KFold,StratifiedKFold\n# fold=StratifiedKFold(n_splits=15,shuffle=True,random_state=1994)\n# i=1\n# for train_index, test_index in fold.split(X,y):\n#     X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n#     y_train, y_test = y[train_index], y[test_index]\n#     m=CatBoostClassifier(n_estimators=5000,random_state=1994,eval_metric='AUC',learning_rate=0.03)\n#     m.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_test, y_test)], early_stopping_rounds=200,verbose=200,cat_features=categorical_features_indices)\n#     preds=m.predict_proba(X_test)[:,-1]\n#     print(\"err_cb: \",roc_auc_score(y_test,preds))\n#     errCB.append(roc_auc_score(y_test,preds))\n#     p = m.predict_proba(Xtest)[:,-1]\n#     i=i+1\n#     y_pred_tot_cb.append(p)","443db83a":"# np.mean(errCB,0)","4bab46a7":"# s['redemption_status']=np.mean(y_pred_tot_cb,0)\n# s.head()","339e7b73":"# sum(s.redemption_status>0.5)","8739fbd9":"# s.to_csv('AV_amex_cb_folds_v28.csv',index=False)\n# s.shape","96000fba":"# s['redemption_status']=np.mean(y_pred_tot_cb,0)*0.25+np.mean(y_pred_tot,0)*0.75\n# s.head()","d329c7db":"# sum(s.redemption_status>0.5)","b6bc3118":"# s.to_csv('AV_amex_stack2_folds_v28.csv',index=False)\n# # s.shape","945f7df7":"# print(X.shape,Xtest.shape)\n\n# X=pd.get_dummies(X,drop_first=True)\n# Xtest=pd.get_dummies(Xtest,drop_first=True)\n\n# X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.25,random_state = 1994,stratify=y)\n# categorical_features_indices = np.where(X_train.dtypes =='category')[0]\n# categorical_features_indices","b2cd9215":"# from xgboost import XGBClassifier\n\n# errxgb=[]\n# y_pred_tot_xgb=[]\n# from sklearn.model_selection import KFold,StratifiedKFold\n# fold=StratifiedKFold(n_splits=10,shuffle=True,random_state=1994)\n# i=1\n# for train_index, test_index in fold.split(X,y):\n#     X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n#     y_train, y_test = y[train_index], y[test_index]\n#     m=XGBClassifier(n_estimators=5000,random_state=1994,eval_metric='auc',learning_rate=0.03)\n#     m.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_test, y_test)], early_stopping_rounds=200,verbose=200)\n#     preds=m.predict_proba(X_test)[:,-1]\n#     print(\"err_xgb: \",roc_auc_score(y_test,preds))\n#     errxgb.append(roc_auc_score(y_test,preds))\n#     p = m.predict_proba(Xtest)[:,-1]\n#     i=i+1\n#     y_pred_tot_xgb.append(p)","72d2732e":"# np.mean(errxgb,0)","032a9150":"# s['redemption_status']=np.mean(y_pred_tot_xgb,0)\n# s.head()","f9b53714":"# s.to_csv('AV_amex_xgb_folds_v28.csv',index=False)\n# s.shape","a28d9162":"# s['redemption_status']=(np.mean(y_pred_tot_cb,0)+np.mean(y_pred_tot,0)+np.mean(y_pred_tot_xgb,0))\/3\n# s.head()","2717c465":"# s.to_csv('AV_amex_stack3_folds_v28.csv',index=False)","5d789091":"# s['redemption_status']=np.mean(y_pred_tot_xgb,0)*0.5+np.mean(y_pred_tot,0)*0.5\n# s.head()","4fb186dc":"# s.to_csv('AV_amex_stack4_folds_v17.csv',index=False)\n# s.shape","67946ed0":"Problem Statement\nPredicting Coupon Redemption\nXYZ Credit Card company regularly helps it\u2019s merchants understand their data better and take key business decisions accurately by providing machine learning and analytics consulting. ABC is an established Brick & Mortar retailer that frequently conducts marketing campaigns for its diverse product range. As a merchant of XYZ, they have sought XYZ to assist them in their discount marketing process using the power of machine learning. Can you wear the AmExpert hat and help out ABC?\n\n \nDiscount marketing and coupon usage are very widely used promotional techniques to attract new customers and to retain & reinforce loyalty of existing customers. The measurement of a consumer\u2019s propensity towards coupon usage and the prediction of the redemption behaviour are crucial parameters in assessing the effectiveness of a marketing campaign.\n\n \nABC\u2019s promotions are shared across various channels including email, notifications, etc. A number of these campaigns include coupon discounts that are offered for a specific product\/range of products. The retailer would like the ability to predict whether customers redeem the coupons received across channels, which will enable the retailer\u2019s marketing team to accurately design coupon construct, and develop more precise and targeted marketing strategies.\n\n \nThe data available in this problem contains the following information, including the details of a sample of campaigns and coupons used in previous campaigns -\n\nUser Demographic Details\nCampaign and coupon Details\nProduct details\nPrevious transactions\nBased on previous transaction & performance data from the last 18 campaigns, predict the probability for the next 10 campaigns in the test set for each coupon and customer combination, whether the customer will redeem the coupon or not?\n\n \n\nDataset Description\nHere is the schema for the different data tables available. The detailed data dictionary is provided next.\n\n\n \n\nYou are provided with the following files in train.zip:\n\ntrain.csv: Train data containing the coupons offered to the given customers under the 18 campaigns\n\nVariable\tDefinition\nid\tUnique id for coupon customer impression\ncampaign_id\tUnique id for a discount campaign\ncoupon_id\tUnique id for a discount coupon\ncustomer_id\tUnique id for a customer\nredemption_status\t(target) (0 - Coupon not redeemed, 1 - Coupon redeemed) \ncampaign_data.csv: Campaign information for each of the 28 campaigns\n\nVariable\tDefinition\ncampaign_id\tUnique id for a discount campaign\ncampaign_type\tAnonymised Campaign Type (X\/Y)\nstart_date\tCampaign Start Date\nend_date\tCampaign End Date \ncoupon_item_mapping.csv: Mapping of coupon and items valid for discount under that coupon\n\nVariable\tDefinition\ncoupon_id\tUnique id for a discount coupon (no order)\nitem_id\tUnique id for items for which given coupon is valid (no order) \ncustomer_demographics.csv: Customer demographic information for some customers\n\nVariable\tDefinition\ncustomer_id\tUnique id for a customer\nage_range\tAge range of customer family in years\nmarital_status\tMarried\/Single\nrented\t0 - not rented accommodation, 1 - rented accommodation\nfamily_size\tNumber of family members\nno_of_children\tNumber of children in the family\nincome_bracket\tLabel Encoded Income Bracket (Higher income corresponds to higher number) \ncustomer_transaction_data.csv: Transaction data for all customers for duration of campaigns in the train data\n\nVariable\tDefinition\ndate\tDate of Transaction\ncustomer_id\tUnique id for a customer\nitem_id\tUnique id for item\nquantity\tquantity of item bought\nselling_price\tSales value of the transaction\nother_discount\tDiscount from other sources such as manufacturer coupon\/loyalty card\ncoupon_discount\tDiscount availed from retailer coupon \nitem_data.csv: Item information for each item sold by the retailer\n\nVariable\tDefinition\nitem_id\tUnique id for item\nbrand\tUnique id for item brand\nbrand_type\tBrand Type (local\/Established)\ncategory\tItem Category \ntest.csv: Contains the coupon customer combination for which redemption status is to be predicted\n\nVariable\tDefinition\nid\tUnique id for coupon customer impression\ncampaign_id\tUnique id for a discount campaign\ncoupon_id\tUnique id for a discount coupon\ncustomer_id\tUnique id for a customer \n*Campaign, coupon and customer data for test set is also contained in train.zip \n\nsample_submission.csv: This file contains the format in which you have to submit your predictions.\n\nTo summarise the entire process:\n\nCustomers receive coupons under various campaigns and may choose to redeem it.\nThey can redeem the given coupon for any valid product for that coupon as per coupon item mapping within the duration between campaign start date and end date\nNext, the customer will redeem the coupon for an item at the retailer store and that will reflect in the transaction table in the column coupon_discount.\n \nEvaluation Metric\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n \n\nPublic and Private Split\nTest data is further randomly divided into Public (40%) and Private data (60%)\nYour initial responses will be checked and scored on the Public data.\nThe final rankings would be based on your private score which will be published once the competition is over.\nHackathon Rules\nSetting the final submission is mandatory. Without a final submission, the submission corresponding to best public score will be taken as final submission\nUse of external datasets is not allowed\nUse of id variable as a part of making predictions is not allowed\nYou can only make 10 submissions per day\nThe code file is mandatory while setting final submission. For GUI based tools, please upload a zip file of snapshots of steps taken by you, else upload code file.\nThe code file uploaded should be pertaining to your final submission.\nNo submission will be accepted after the contest deadline\n "}}