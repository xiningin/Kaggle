{"cell_type":{"79cded98":"code","33305e6c":"code","64972152":"code","fd47ee36":"code","f198b7ab":"code","a52d2e5d":"code","33f22950":"code","d8f64706":"code","85814d34":"code","c5eefedb":"code","a237079e":"code","85c834b5":"code","0e0b92dd":"code","a0ebcea6":"code","e4bea871":"code","acc069c8":"code","7ebd29a1":"code","cfde35e2":"code","efa9abfd":"code","db44dd0a":"code","e22c96b7":"code","63dc514d":"code","9824bf4d":"markdown","3bb3b170":"markdown","0e533a7e":"markdown","8299bcb8":"markdown","55c2fdb5":"markdown","96109aff":"markdown","911a030e":"markdown","933501a7":"markdown"},"source":{"79cded98":"from __future__ import division, print_function\n# \u043e\u0442\u043a\u043b\u044e\u0447\u0438\u043c \u0432\u0441\u044f\u043a\u0438\u0435 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f Anaconda\nimport warnings\nimport numpy as np\nimport pandas as pd\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = 10, 6\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV","33305e6c":"df_train = pd.read_csv('..\/input\/Kannada-MNIST\/train.csv')\n","64972152":"images = df_train.drop(\"label\", axis=1).values.astype('float32')\nlabels = df_train['label'].values.astype('float32')\ndel df_train","fd47ee36":"df_test = pd.read_csv('..\/input\/Kannada-MNIST\/test.csv')\ndf_test = df_test.drop(\"id\", axis=1).values.astype('float32')","f198b7ab":"# Initialize the stratified breakdown of our dataset for validation\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)","a52d2e5d":"# Initialize our classifier with default parameters\nrfc = RandomForestClassifier(random_state=42, n_jobs=-1, oob_score=True)","33f22950":"# teach on a training dataset\nresults = cross_val_score(rfc, images, labels, cv=skf)","d8f64706":"# evaluate the accuracy of the train dataset\nprint(\"CV accuracy score: {:.2f}%\".format(results.mean()*100))","85814d34":"# Initialize validation\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)","c5eefedb":"# Create lists to maintain accuracy on the training and test dataset\ntrain_acc = []\ntest_acc = []\ntemp_train_acc = []\ntemp_test_acc = []\ntrees_grid = [5, 10, 15, 20, 30, 50, 75, 100]\n\n# Teach on a training dataset\nfor ntrees in trees_grid:\n    rfc = RandomForestClassifier(n_estimators=ntrees, random_state=42, n_jobs=-1, oob_score=True)\n    temp_train_acc = []\n    temp_test_acc = []\n    for train_index, test_index in skf.split(images, labels):\n        X_train, X_test = images[train_index], images[test_index]\n        y_train, y_test = labels[train_index], labels[test_index]\n        rfc.fit(X_train, y_train)\n        temp_train_acc.append(rfc.score(X_train, y_train))\n        temp_test_acc.append(rfc.score(X_test, y_test))\n    train_acc.append(temp_train_acc)\n    test_acc.append(temp_test_acc)\n    \ntrain_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\nprint(\"Best accuracy on CV is {:.2f}% with {} trees\".format(max(test_acc.mean(axis=1))*100, \n                                                        trees_grid[np.argmax(test_acc.mean(axis=1))]))","a237079e":"plt.style.use('ggplot')\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.plot(trees_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\nax.plot(trees_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='cv')\nax.fill_between(trees_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\nax.fill_between(trees_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\nax.legend(loc='best')\nax.set_ylim([0.88,1.02])\nax.set_ylabel(\"Accuracy\")\nax.set_xlabel(\"N_estimators\");","85c834b5":"\n# Create lists to maintain accuracy on the training and test dataset\ntrain_acc = []\ntest_acc = []\ntemp_train_acc = []\ntemp_test_acc = []\nmax_depth_grid = [3, 5, 7, 9, 11, 13, 15, 17, 20, 22, 24]\n\n# Teach on a training dataset\nfor max_depth in max_depth_grid:\n    rfc = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, oob_score=True, max_depth=max_depth)\n    temp_train_acc = []\n    temp_test_acc = []\n    for train_index, test_index in skf.split(images, labels):\n        X_train, X_test = images[train_index], images[test_index]\n        y_train, y_test = labels[train_index], labels[test_index]\n        rfc.fit(X_train, y_train)\n        temp_train_acc.append(rfc.score(X_train, y_train))\n        temp_test_acc.append(rfc.score(X_test, y_test))\n    train_acc.append(temp_train_acc)\n    test_acc.append(temp_test_acc)\n    \ntrain_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\nprint(\"Best accuracy on CV is {:.2f}% with {} max_depth\".format(max(test_acc.mean(axis=1))*100, \n                                                        max_depth_grid[np.argmax(test_acc.mean(axis=1))]))\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.plot(max_depth_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\nax.plot(max_depth_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='cv')\nax.fill_between(max_depth_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\nax.fill_between(max_depth_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\nax.legend(loc='best')\nax.set_ylim([0.88,1.02])\nax.set_ylabel(\"Accuracy\")\nax.set_xlabel(\"Max_depth\");","0e0b92dd":"# Create lists to maintain accuracy on the training and test dataset\ntrain_acc = []\ntest_acc = []\ntemp_train_acc = []\ntemp_test_acc = []\nmin_samples_leaf_grid = [1, 3, 5, 7, 9, 11, 13, 15, 17, 20, 22, 24]\n\n# Teach on a training dataset\nfor min_samples_leaf in min_samples_leaf_grid:\n    rfc = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, \n                                 oob_score=True, min_samples_leaf=min_samples_leaf)\n    temp_train_acc = []\n    temp_test_acc = []\n    for train_index, test_index in skf.split(images, labels):\n        X_train, X_test = images[train_index], images[test_index]\n        y_train, y_test = labels[train_index], labels[test_index]\n        rfc.fit(X_train, y_train)\n        temp_train_acc.append(rfc.score(X_train, y_train))\n        temp_test_acc.append(rfc.score(X_test, y_test))\n    train_acc.append(temp_train_acc)\n    test_acc.append(temp_test_acc)\n    \ntrain_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\nprint(\"Best accuracy on CV is {:.2f}% with {} min_samples_leaf\".format(max(test_acc.mean(axis=1))*100, \n                                                        min_samples_leaf_grid[np.argmax(test_acc.mean(axis=1))]))","a0ebcea6":"fig, ax = plt.subplots(figsize=(8, 4))\nax.plot(min_samples_leaf_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\nax.plot(min_samples_leaf_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='cv')\nax.fill_between(min_samples_leaf_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\nax.fill_between(min_samples_leaf_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\nax.legend(loc='best')\nax.set_ylim([0.88,1.02])\nax.set_ylabel(\"Accuracy\")\nax.set_xlabel(\"Min_samples_leaf\");","e4bea871":"# Create lists to maintain accuracy on the training and test dataset\ntrain_acc = []\ntest_acc = []\ntemp_train_acc = []\ntemp_test_acc = []\nmax_features_grid = [2, 4, 6, 8, 10, 12, 14, 16]\n\n# Teach on a training dataset\nfor max_features in max_features_grid:\n    rfc = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, \n                                 oob_score=True, max_features=max_features)\n    temp_train_acc = []\n    temp_test_acc = []\n    for train_index, test_index in skf.split(images, labels):\n        X_train, X_test = images[train_index], images[test_index]\n        y_train, y_test = labels[train_index], labels[test_index]\n        rfc.fit(X_train, y_train)\n        temp_train_acc.append(rfc.score(X_train, y_train))\n        temp_test_acc.append(rfc.score(X_test, y_test))\n    train_acc.append(temp_train_acc)\n    test_acc.append(temp_test_acc)\n    \ntrain_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\nprint(\"Best accuracy on CV is {:.2f}% with {} max_features\".format(max(test_acc.mean(axis=1))*100, \n                                                        max_features_grid[np.argmax(test_acc.mean(axis=1))]))\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.plot(max_features_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\nax.plot(max_features_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='cv')\nax.fill_between(max_features_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\nax.fill_between(max_features_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\nax.legend(loc='best')\nax.set_ylim([0.88,1.02])\nax.set_ylabel(\"Accuracy\")\nax.set_xlabel(\"Max_features\");","acc069c8":"# \u0421\u0434\u0435\u043b\u0430\u0435\u043c \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432, \u043f\u043e \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u0445\u043e\u0442\u0438\u043c \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u043e\u043b\u043d\u044b\u0439 \u043f\u0435\u0440\u0435\u0431\u043e\u0440\nparameters = {'max_features': [4, 7, 10, 13], 'min_samples_leaf': [1, 3, 5, 7], 'max_depth': [5,10,15,20]}\nrfc = RandomForestClassifier(n_estimators=100, random_state=42, \n                             n_jobs=-1, oob_score=True)\ngcv = GridSearchCV(rfc, parameters, n_jobs=-1, cv=skf, verbose=1)\ngcv.fit(images, labels)","7ebd29a1":"gcv.best_estimator_, gcv.best_score_","cfde35e2":"rfc = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                        max_depth=20, max_features=13, max_leaf_nodes=None,\n                        min_impurity_decrease=0.0, min_impurity_split=None,\n                        min_samples_leaf=1, min_samples_split=2,\n                        min_weight_fraction_leaf=0.0, n_estimators=100,\n                        n_jobs=-1, oob_score=True, random_state=42, verbose=0,\n                        warm_start=False)","efa9abfd":"rfc.fit(images, labels)","db44dd0a":"pred = rfc.predict(df_test).astype(int)","e22c96b7":"def write_preds(preds, fname):\n    pd.DataFrame({\"id\": list(range(0,len(preds))), \"label\": preds}).to_csv(fname, index=False, header=True)","63dc514d":"write_preds(pred, \"samplesubmission.csv\")","9824bf4d":"### Random Forest","3bb3b170":"**In our case, the optimal number of features is 16, it is with this value that the best result is achieved.**\n\n**We looked at how learning curves behave depending on changes in the basic parameters. Now let's use Grid Search to find the optimal parameters for our example.**","0e533a7e":"**Let's try to improve this result and see how the learning curves behave when changing the basic parameters.**","8299bcb8":"**In this case, we do not gain in accuracy on validation, but we can greatly reduce retraining while maintaining accuracy.**\n\n**Consider such a parameter as max_features. For classification problems, the default is $\\large \\sqrt{n}$, where n is the number of features. Let's check whether it is optimal in our case to use 4 signs or not.**","55c2fdb5":"**The best accuracy that we were able to achieve with brute force is 97.66% with 'max_depth': 20,' max_features': 13,' min_samples_leaf': 1.**","96109aff":"**As you can see, when a certain number of trees is reached, our accuracy on the test reaches the asymptote, and you can decide for yourself how many trees are optimal for your task. The figure also shows that in the training sample we were able to achieve 100% accuracy, this tells us about the retraining of our model. To avoid overtraining, we must add regularization parameters to the model.**","911a030e":"**The max_depth parameter does a good job of regularizing the model, and we are not so overtrained. The accuracy of our model has increased slightly.**\n\n**Another important parameter min_samples_leaf, it also serves as a regularizer**","933501a7":"Original [kernel](https:\/\/github.com\/Yorko\/mlcourse.ai\/blob\/master\/jupyter_russian\/topic05_bagging_rf\/topic5_part2_random_forest.ipynb)"}}