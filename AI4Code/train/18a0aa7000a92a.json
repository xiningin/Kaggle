{"cell_type":{"6540391b":"code","d1f1216b":"code","8c7dd827":"code","a2cf38bf":"code","cef36d1e":"code","baeee0af":"code","e6f79a3d":"code","61fc7f18":"code","9c7a842f":"code","5f6386a0":"code","a150d784":"code","968e8949":"code","739e4fbc":"code","861c5e4a":"code","9735a769":"code","9718ad47":"code","f604856f":"code","f302a0ec":"code","8fd89db7":"code","0cc44c1a":"code","f804fee3":"code","cfee6be1":"code","1ec22d6a":"code","00fb1c49":"code","4f6af8d3":"markdown","b4aedc29":"markdown","ae70b78a":"markdown","81e4badb":"markdown","e2b06e22":"markdown"},"source":{"6540391b":"import pandas as pd\n\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\ntrain.head()","d1f1216b":"import numpy as np\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\n\n\nsubmission=test[[\"id\"]].copy()\nsubmission[\"target\"] = -1.0\nperformance_report = pd.DataFrame({\n    \"segment\": range(512),\n    \"mean\": 0,\n    \"std\": 0\n})\n\nfor i in range(10,11):\n    Y = train.loc[train[\"wheezy-copper-turtle-magic\"]==i, \"target\"]\n    X = train.loc[train[\"wheezy-copper-turtle-magic\"]==i, :].drop([\"id\", \"target\", \"wheezy-copper-turtle-magic\"], axis=1)\n    X_te = test.loc[test[\"wheezy-copper-turtle-magic\"]==i, :].drop([\"id\", \"wheezy-copper-turtle-magic\"], axis=1)\n    selected_columns = X.columns[(X.std(axis=0)>2).values]\n    X = X.loc[:, selected_columns]\n    X_te = X_te[selected_columns]\n    \n    model = RandomForestClassifier(n_estimators=100, random_state=1)\n    scores_rf = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\n    print(np.mean(scores_rf), \"+\/-\", np.std(scores_rf))\n    \n    model = svm.SVC(kernel='poly', degree=4, probability=True, gamma='auto', random_state=1)\n    scores_svm = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\n    print(np.mean(scores_svm), \"+\/-\", np.std(scores_svm))\n    \n    model = LogisticRegression(solver='liblinear', penalty=\"l1\", random_state=1)\n    scores_lr = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\n    print(np.mean(scores_lr), \"+\/-\", np.std(scores_lr))\n    \n    model = KNeighborsClassifier(n_neighbors=12)\n    scores_knn = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\n    print(np.mean(scores_knn), \"+\/-\", np.std(scores_knn))\n    \n    print(np.mean(scores_lr+scores_knn+scores_svm+scores_rf)\/4)\n#    performance_report.loc[i, \"mean\"] = np.mean(scores_rf)\n#    performance_report.loc[i, \"std\"] = np.std(scores_rf)\n#    model = RandomForestClassifier(n_estimators=100, n_jobs=-1).fit(X, Y)\n#    submission.loc[test[\"wheezy-copper-turtle-magic\"]==i, \"target\"] =  model.predict_proba(X_te)[:,1]","8c7dd827":"performance_report.to_csv(\"performance.csv\", index=False)\nsubmission.to_csv(\"submission.csv\", index=False)","a2cf38bf":"print(model)","cef36d1e":"Y = train.loc[train[\"wheezy-copper-turtle-magic\"]==i, \"target\"]\nX = train.loc[train[\"wheezy-copper-turtle-magic\"]==i, :].drop([\"id\", \"target\", \"wheezy-copper-turtle-magic\"], axis=1)\nX_te = test.loc[test[\"wheezy-copper-turtle-magic\"]==i, :].drop([\"id\", \"wheezy-copper-turtle-magic\"], axis=1)","baeee0af":"from sklearn import mixture\n\ndpgmm = mixture.BayesianGaussianMixture(n_components=X.shape[1], covariance_type='full').fit(X)\nprint(len(set(dpgmm.predict(X))))","e6f79a3d":"print(dpgmm.weights_)","61fc7f18":"import seaborn as sns\n\nsns.distplot(dpgmm.weights_)","9c7a842f":"print(np.sum(dpgmm.weights_ > np.quantile(dpgmm.weights_, 0.95)))","5f6386a0":"dpgmm = mixture.BayesianGaussianMixture(n_components=X.shape[1], covariance_type='full', random_state=2019).fit(X)\ndpgmm = mixture.BayesianGaussianMixture(n_components=np.sum(dpgmm.weights_ > np.quantile(dpgmm.weights_, 0.95)), \n                                        covariance_type='full', random_state=2019).fit(X)","a150d784":"clusters = pd.DataFrame({\n    \"cluster_name\": dpgmm.predict(X),\n    \"Y\": Y\n})\ndic = clusters.groupby(\"cluster_name\").mean()","968e8949":"dic.iloc[1,0]","739e4fbc":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=3)\nscores_knn = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_knn), \"+\/-\", np.std(scores_knn))","861c5e4a":"model = KNeighborsClassifier(n_neighbors=4)\nscores_knn = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_knn), \"+\/-\", np.std(scores_knn))","9735a769":"model = KNeighborsClassifier(n_neighbors=10)\nscores_knn = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_knn), \"+\/-\", np.std(scores_knn))","9718ad47":"model = KNeighborsClassifier(n_neighbors=15)\nscores_knn = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_knn), \"+\/-\", np.std(scores_knn))","f604856f":"model = KNeighborsClassifier(n_neighbors=12)\nscores_knn = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_knn), \"+\/-\", np.std(scores_knn))","f302a0ec":"model = KNeighborsClassifier(n_neighbors=8)\nscores_knn = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_knn), \"+\/-\", np.std(scores_knn))","8fd89db7":"Y = train.loc[train[\"wheezy-copper-turtle-magic\"]==i, \"target\"]\nX = train.loc[train[\"wheezy-copper-turtle-magic\"]==i, :].drop([\"id\", \"target\", \"wheezy-copper-turtle-magic\"], axis=1)\nX_te = test.loc[test[\"wheezy-copper-turtle-magic\"]==i, :].drop([\"id\", \"wheezy-copper-turtle-magic\"], axis=1)\n\nselected_columns = X.columns[(X.std(axis=0)>2).values]\nX = X.loc[:, selected_columns]\nX_te = X_te[selected_columns]\n\ndpgmm = mixture.BayesianGaussianMixture(n_components=X.shape[1], covariance_type='full', random_state=2019).fit(X)\ndpgmm = mixture.BayesianGaussianMixture(n_components=np.sum(dpgmm.weights_ > np.quantile(dpgmm.weights_, 0.95)), \n                                        covariance_type='full', random_state=2019).fit(X)\ndic = pd.DataFrame({\"cluster_name\": dpgmm.predict(X), \"Y\": Y}).groupby(\"cluster_name\").mean()\n\ncolumn_train = [dic.iloc[i, 0] for i in dpgmm.predict(X)]\ncolumn_test = [dic.iloc[i, 0] for i in dpgmm.predict(X_te)]\n\nX[\"train_column\"] = column_train\nX_te[\"train_column\"] = column_test\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=1)\nscores_rf = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_rf), \"+\/-\", np.std(scores_rf))\n\nmodel = svm.SVC(kernel='poly', degree=4, probability=True, gamma='auto', random_state=1)\nscores_svm = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_svm), \"+\/-\", np.std(scores_svm))\n    \nmodel = LogisticRegression(solver='liblinear', penalty=\"l1\", random_state=1)\nscores_lr = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_lr), \"+\/-\", np.std(scores_lr))\n    \nmodel = KNeighborsClassifier(n_neighbors=12)\nscores_knn = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_knn), \"+\/-\", np.std(scores_knn))","0cc44c1a":"Y = train.loc[train[\"wheezy-copper-turtle-magic\"]==i, \"target\"]\nX = train.loc[train[\"wheezy-copper-turtle-magic\"]==i, :].drop([\"id\", \"target\", \"wheezy-copper-turtle-magic\"], axis=1)\nX_te = test.loc[test[\"wheezy-copper-turtle-magic\"]==i, :].drop([\"id\", \"wheezy-copper-turtle-magic\"], axis=1)\n\ndpgmm = mixture.BayesianGaussianMixture(n_components=X.shape[1], covariance_type='full', random_state=2019).fit(X)\ndpgmm = mixture.BayesianGaussianMixture(n_components=np.sum(dpgmm.weights_ > np.quantile(dpgmm.weights_, 0.95)), \n                                        covariance_type='full', random_state=2019).fit(X)\ndic = pd.DataFrame({\"cluster_name\": dpgmm.predict(X), \"Y\": Y}).groupby(\"cluster_name\").mean()\n\ncolumn_train = [dic.iloc[i, 0] for i in dpgmm.predict(X)]\ncolumn_test = [dic.iloc[i, 0] for i in dpgmm.predict(X_te)]\n\nX[\"train_column\"] = column_train\nX_te[\"train_column\"] = column_test\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=1)\nscores_rf = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_rf), \"+\/-\", np.std(scores_rf))\n\nmodel = svm.SVC(kernel='poly', degree=4, probability=True, gamma='auto', random_state=1)\nscores_svm = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_svm), \"+\/-\", np.std(scores_svm))\n    \nmodel = LogisticRegression(solver='liblinear',random_state=1)\nscores_lr = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_lr), \"+\/-\", np.std(scores_lr))\n    \nmodel = KNeighborsClassifier(n_neighbors=12)\nscores_knn = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_knn), \"+\/-\", np.std(scores_knn))","f804fee3":"Y = train.loc[train[\"wheezy-copper-turtle-magic\"]==i, \"target\"]\nX = train.loc[train[\"wheezy-copper-turtle-magic\"]==i, :].drop([\"id\", \"target\", \"wheezy-copper-turtle-magic\"], axis=1)\nX_te = test.loc[test[\"wheezy-copper-turtle-magic\"]==i, :].drop([\"id\", \"wheezy-copper-turtle-magic\"], axis=1)\n\ndpgmm = mixture.BayesianGaussianMixture(n_components=X.shape[1], covariance_type='full', random_state=2019).fit(X)\ndpgmm = mixture.BayesianGaussianMixture(n_components=np.sum(dpgmm.weights_ > np.quantile(dpgmm.weights_, 0.90)), \n                                        covariance_type='full', random_state=2019).fit(X)\ndic = pd.DataFrame({\"cluster_name\": dpgmm.predict(X), \"Y\": Y}).groupby(\"cluster_name\").mean()\n\ncolumn_train = [dic.iloc[i, 0] for i in dpgmm.predict(X)]\ncolumn_test = [dic.iloc[i, 0] for i in dpgmm.predict(X_te)]\n\nX[\"train_column\"] = column_train\nX_te[\"train_column\"] = column_test\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=1)\nscores_rf = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_rf), \"+\/-\", np.std(scores_rf))\n\nmodel = svm.SVC(kernel='poly', degree=4, probability=True, gamma='auto', random_state=1)\nscores_svm = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_svm), \"+\/-\", np.std(scores_svm))\n    \nmodel = LogisticRegression(solver='liblinear',random_state=1)\nscores_lr = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_lr), \"+\/-\", np.std(scores_lr))\n    \nmodel = KNeighborsClassifier(n_neighbors=12)\nscores_knn = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_knn), \"+\/-\", np.std(scores_knn))","cfee6be1":"Y = train.loc[train[\"wheezy-copper-turtle-magic\"]==i, \"target\"]\nX = train.loc[train[\"wheezy-copper-turtle-magic\"]==i, :].drop([\"id\", \"target\", \"wheezy-copper-turtle-magic\"], axis=1)\nX_te = test.loc[test[\"wheezy-copper-turtle-magic\"]==i, :].drop([\"id\", \"wheezy-copper-turtle-magic\"], axis=1)\n\ndpgmm = mixture.BayesianGaussianMixture(n_components=X.shape[1], covariance_type='full', random_state=2019).fit(X)\ndpgmm = mixture.BayesianGaussianMixture(n_components=np.sum(dpgmm.weights_ > np.quantile(dpgmm.weights_, 0.85)), \n                                        covariance_type='full', random_state=2019).fit(X)\ndic = pd.DataFrame({\"cluster_name\": dpgmm.predict(X), \"Y\": Y}).groupby(\"cluster_name\").mean()\n\ncolumn_train = [dic.iloc[i, 0] for i in dpgmm.predict(X)]\ncolumn_test = [dic.iloc[i, 0] for i in dpgmm.predict(X_te)]\n\nX[\"train_column\"] = column_train\nX_te[\"train_column\"] = column_test\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=1)\nscores_rf = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_rf), \"+\/-\", np.std(scores_rf))\n\nmodel = svm.SVC(kernel='poly', degree=4, probability=True, gamma='auto', random_state=1)\nscores_svm = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_svm), \"+\/-\", np.std(scores_svm))\n    \nmodel = LogisticRegression(solver='liblinear',random_state=1)\nscores_lr = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_lr), \"+\/-\", np.std(scores_lr))\n    \nmodel = KNeighborsClassifier(n_neighbors=12)\nscores_knn = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_knn), \"+\/-\", np.std(scores_knn))","1ec22d6a":"from xgboost import XGBClassifier\n\nmodel = XGBClassifier()\nscores_xgb = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_xgb), \"+\/-\", np.std(scores_xgb))","00fb1c49":"from lightgbm import LGBMClassifier\n\nmodel = LGBMClassifier()\nscores_gbm = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\nprint(np.mean(scores_gbm), \"+\/-\", np.std(scores_gbm))","4f6af8d3":"# Using nearest neighbours","b4aedc29":"# Gradient Boosting\n\nTrying algorithms that uses gradient boosting to assess the performance (without fine-tuning):","ae70b78a":"# Classifiers with new column\n\nNow, comparing all the classifiers with the new column (and trying to fine tune the amount of components to be considered in the clustering):","81e4badb":"Basically, the SVM seems slightly better with the feature. It should added to the others.","e2b06e22":"# Building a model\n\nThis is a fork from [nvnn's kernel](https:\/\/www.kaggle.com\/nvnnghia\/svm-knn-0-943) supplemented with [Chris Deotte's idea for feature selection](https:\/\/www.kaggle.com\/c\/instant-gratification\/discussion\/92930):"}}