{"cell_type":{"8a264437":"code","a6eb6674":"code","29ca2a1d":"code","4bfaea6f":"code","be605793":"code","abfcbac3":"code","fc880cb7":"code","6b767425":"code","670cb930":"code","cb3c649a":"code","54dc7d94":"code","68931ac0":"code","63d50b74":"code","198d6bfa":"code","c8b5e41f":"code","ff2106b0":"code","7bfeb242":"code","a2c4ec0a":"code","92be31ae":"code","b9ec7b46":"code","66549295":"code","0ba34fa6":"code","c0dc7d52":"code","5fd5fa24":"code","bd8c4ee5":"code","b8f6146d":"code","e4b320d3":"markdown","7b3d66a6":"markdown","2d2807c3":"markdown","051bb706":"markdown","feddef5e":"markdown","48918a5b":"markdown","c579eb4b":"markdown","6872ce29":"markdown","f0bcdde6":"markdown","c11ee746":"markdown","9d295ce8":"markdown"},"source":{"8a264437":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a6eb6674":"# Classification Problem - Titanic Survival\nimport numpy  as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import normalize\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler","29ca2a1d":"#Load the dataset\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","4bfaea6f":"# Exploratory data analysis\n    # Target variable analysis\nplt.hist(train['Survived'])\nplt.show();","be605793":" # check for unique values - null or nan can be checked since categorical\n train['Survived'].value_counts(dropna=False)","abfcbac3":"# check for null columns\nnull_columns = train.columns[train.isnull().any()]\ntrain[null_columns].isnull().sum()","fc880cb7":"train['Pclass'].unique()  # Out[10]: array([3, 1, 2], dtype=int64)\ntrain['Pclass'].value_counts(dropna=False)","6b767425":"train['Sex'].unique()\ntrain['Sex'].value_counts(dropna=False)\nplt.hist(train['Sex'])\nplt.show()","670cb930":"train['SibSp'].unique()\ntrain['SibSp'].value_counts(dropna=False)","cb3c649a":"train['Parch'].unique()\ntrain['Parch'].value_counts(dropna=False)","54dc7d94":" # age \nplt.hist(train['Age'])\nplt.show();\ntrain['Age'].max()\ntrain['Age'].describe()  # mean age is 29 - mean replacement for NAN\n","68931ac0":"plt.hist(train['Fare'])\nplt.show()\ntrain['Fare'].describe()","63d50b74":"def transform_data(df):\n    # create dummy variables for Pclass\n    df_Pclass = pd.get_dummies(df['Pclass'], prefix='Pclass', drop_first=True)\n    # Join the dummy variables to the main dataframe\n    df = pd.concat([df, df_Pclass], axis=1)\n    del df['Pclass']\n\n    # transform column sex\n    label_encoder = LabelEncoder()\n    df['Sex'] = label_encoder.fit_transform(df['Sex'])\n\n    # transform age - mean replacement\n\n    mean_age = df['Age'].mean()\n    df['Age'] = df['Age'].apply(lambda x: mean_age if pd.isnull(x) else x)\n    # create buckets or bins to categorize passengers agewise\n    cut_bins = [0, 20, 40, 80]\n    cut_labels = [\"young\", \"working\", \"old\"]\n    df['age_bins'] = pd.cut(df['Age'], bins=cut_bins, labels=cut_labels)\n    # Nan handling\n    df['age_bins'] = df['age_bins'].fillna(\"working\")\n    del df['Age']\n\n    # create dummies for age_bins\n    df_age_bins = pd.get_dummies(df['age_bins'], prefix='age_bins', drop_first=True)\n    df = pd.concat([df, df_age_bins], axis=1)\n    del df['age_bins']\n\n    f_bins = [0, 50, 100, 600]\n    f_labels = [\"low\", \"mid\", \"high\"]\n    df['Fare_bins'] = pd.cut(df['Fare'], bins=f_bins, labels=f_labels)\n    del df['Fare']\n    # create dummies for Fare\n    df_Fare_bins = pd.get_dummies(df['Fare_bins'], prefix='Fare_bins', drop_first=True)\n    df = pd.concat([df, df_Fare_bins], axis=1)\n    del df['Fare_bins']\n\n    # handle cabin\n    # df['Cabin'] = df['Cabin'].apply(lambda x: 'NO_CABIN' if pd.isnull(x) else 'CABIN')\n    # df['Cabin'] = label_encoder.fit_transform(df['Cabin'])\n    del df['Cabin']\n\n    # clean Embarked\n    df['Embarked'].value_counts(dropna=False)\n    df['Embarked'] = df['Embarked'].apply(lambda x: 'S' if pd.isnull(x) else x)\n\n    # create dummies for Embarked\n    df_Embarked = pd.get_dummies(df['Embarked'], prefix='Embarked', drop_first=True)\n    df = pd.concat([df, df_Embarked], axis=1)\n    del df['Embarked']\n\n    # delete not used columns\n    del df['Name']\n    del df['Ticket']\n    del df['PassengerId']\n    if 'Survived' in df:\n        del df['Survived']\n    return df\n","198d6bfa":"#data cleaning and transformation\ntrain_x =  transform_data(train)\ntest_x =    transform_data(test)\ntrain_y = train['Survived']","c8b5e41f":"#xgb model\nmodel = XGBClassifier(n_estimators=100 ,max_depth=9, seed=2017)\nmodel.fit(train_x, train_y)\ny_pred = model.predict(train_x)\ncm = confusion_matrix(train_y, y_pred)\nprint(cm)","ff2106b0":"# generate class probabilities\n# Notice that 2 elements will be returned in probs array,\n# 1st element is probability for negative class,\n# 2nd element gives probability for positive class\nprobs = model.predict_proba(train_x)\ny_pred_prob = probs[:, 1]\n\n# generate evaluation metrics\nprint(\"Accuracy:\", metrics.accuracy_score(train_y, y_pred))\nprint (\"AUC :\", metrics.roc_auc_score(train_y, y_pred_prob))\n","7bfeb242":"# extract false positive, true positive rate\nfpr, tpr, thresholds = metrics.roc_curve(train_y, y_pred_prob)\nroc_auc = metrics.auc(fpr, tpr)\nprint(\"Area under the ROC curve : %f\" % roc_auc)\n","a2c4ec0a":"# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","92be31ae":"i = np.arange(len(tpr)) # index for df\nroc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series(tpr, index = i),'1-fpr' : pd.Series(1-fpr, index = i), 'tf' : pd.Series(tpr - (1-fpr), index = i),'thresholds' : pd.Series(thresholds, index = i)})\nroc.iloc[(roc.tf-0).abs().argsort()[:1]]\n","b9ec7b46":"# Plot tpr vs 1-fpr\nfig, ax = plt.subplots()\nplt.plot(roc['tpr'], label='tpr')\nplt.plot(roc['1-fpr'], color = 'red', label='1-fpr')\nplt.legend(loc='best')\nplt.xlabel('1-False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","66549295":"def Find_Optimal_Cutoff(target, predicted):\n    \"\"\" Find the optimal probability cutoff point for a classification model related to event rate\n    Parameters\n    ----------\n    target : Matrix with dependent or target data, where rows are observations\n\n    predicted : Matrix with predicted data, where rows are observations\n\n    Returns\n    -------\n    list type, with optimal cutoff value\n\n    \"\"\"\n    fpr, tpr, threshold = metrics.roc_curve(target, predicted)\n    i = np.arange(len(tpr))\n    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n\n    return list(roc_t['threshold'])","0ba34fa6":"# Find optimal probability threshold\n# Note: probs[:, 1] will have probability of being positive label\nthreshold = Find_Optimal_Cutoff(train_y, probs[:, 1])\nprint(\"Optimal Probability Threshold: \", threshold)\n\n# Applying the threshold to the prediction probability\ny_pred_optimal = np.where(y_pred_prob >= threshold, 1, 0)","c0dc7d52":"# Applying the threshold to the prediction probability\ny_pred_optimal = np.where(y_pred_prob >= threshold, 1, 0)\n","5fd5fa24":"# Let's compare the accuracy of traditional\/normal approach vs optimal cutoff\nprint (\"\\nNormal - Accuracy: \", metrics.accuracy_score(train_y, y_pred))\nprint (\"Optimal Cutoff - Accuracy: \", metrics.accuracy_score(train_y, y_pred_optimal))\nprint (\"\\nNormal - Confusion Matrix: \\n\", metrics.confusion_matrix(train_y, y_pred))\nprint (\"Optimal - Cutoff Confusion Matrix: \\n\", metrics.confusion_matrix(train_y, y_pred_optimal))","bd8c4ee5":"feature_importance = model.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 2)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, train_x.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","b8f6146d":"test_pred = model.predict(test_x)\ntest['Survived'] = test_pred\ntest\n","e4b320d3":"# Model evaluation","7b3d66a6":"# make importances relative to max importance","2d2807c3":"# Data transformation\n\nTransform data for analysis","051bb706":"#Exploratory data analysis\n\nEDA is important for every data science problem . Data transformation and cleaning are the first step in KDD process. ","feddef5e":"# Test data set Prediction","48918a5b":"# ROC Curve","c579eb4b":"# Classification Model","6872ce29":"# Let's compare the accuracy of traditional\/normal approach vs optimal cutoff","f0bcdde6":"# Titanic - Predict survival \n\nThis notebook is for users who are looking to dive in ML techniques and learn how to predict using xgboost algoritm . This is a classification problem to \npredict survival of the passengers.","c11ee746":"#Load all the requored libraries ","9d295ce8":"# Optimal threshold for probability"}}