{"cell_type":{"3aa191e3":"code","ac8f930c":"code","5de9fc2f":"code","4d6e37a3":"code","0c5cbbb8":"code","cf31c1d1":"code","e0d7c768":"code","5996ff64":"code","f1847de7":"code","e5f12298":"code","a43c36d4":"code","2e5153a8":"code","c419c6e0":"code","9c0ada34":"code","0fe60c06":"code","c61c98c0":"code","57074fc3":"code","b6569eea":"code","381dc6f7":"code","3cba8f59":"code","c91a017c":"code","ef13ee9a":"code","402c404c":"code","94b993ea":"code","7d849b16":"code","9773a5ad":"code","c4c48484":"code","167d366c":"code","49b6378e":"code","5fe5715e":"code","6c8218d7":"code","915c7509":"code","f0ae16ae":"code","bc1243df":"code","26c53b30":"code","3dd80097":"code","bfe815de":"code","ff611cab":"code","8176edcd":"code","d17293e3":"code","c6924c44":"code","8d9a3038":"code","84d97250":"code","5ff565bd":"code","ec578add":"code","4d4485f1":"code","496324f0":"code","547d2ea0":"code","69dbc6ec":"code","0e4c0dc0":"code","609edbc4":"code","4a18b039":"code","9dcbabdc":"code","399bcff9":"code","8d451a0d":"code","5d66ea9e":"code","2eaba337":"code","0dc694b3":"code","072cda2a":"code","90d85aa3":"code","a574657e":"code","2f0d0a62":"code","c0391e76":"code","3c7c52e5":"code","cba645dc":"code","e859df02":"markdown","04f1ac6b":"markdown","59cc41f2":"markdown","93f623a8":"markdown","b9ecae0f":"markdown","ace83fa6":"markdown","706bf245":"markdown","346b3090":"markdown","03cc961c":"markdown","53a6e8c7":"markdown","c5458282":"markdown","03cb12ef":"markdown","52caf435":"markdown","14d0b6c9":"markdown","bd25ad89":"markdown","1918d14d":"markdown","12f72554":"markdown","8c3c0852":"markdown","e457ffaa":"markdown","170c2e34":"markdown","59113abb":"markdown","8b77cbfb":"markdown","e6e7980d":"markdown","302173b8":"markdown","c24036e2":"markdown","c0f3c4c2":"markdown","e67c84b6":"markdown","85a3730c":"markdown","07e379d5":"markdown","94c2341d":"markdown","f34786a5":"markdown","cdf715ef":"markdown","120aaa79":"markdown","720a95e5":"markdown","88a0f673":"markdown","74e9ec0d":"markdown","ff42b9c9":"markdown","a47d7dfb":"markdown","25869c45":"markdown","2445b875":"markdown","6bef60e2":"markdown","fee3d352":"markdown","05e149ed":"markdown","53c8dab9":"markdown","8f451d1e":"markdown","fc6a8fc5":"markdown","c93e06ac":"markdown","68de4fbb":"markdown","b90cd8c4":"markdown","4a1ac9f1":"markdown","075e0e4c":"markdown","c54dd62d":"markdown","e5808f93":"markdown","fa7469d3":"markdown","348bb7a7":"markdown","0059592a":"markdown","dcf7a200":"markdown","72cc6513":"markdown","c3423a96":"markdown","877fc2fc":"markdown","7124300e":"markdown"},"source":{"3aa191e3":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","ac8f930c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sb\nimport matplotlib.pyplot as pl\n\n%matplotlib inline\n\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest  = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","5de9fc2f":"train.head()","4d6e37a3":"test.head()","0c5cbbb8":"train.shape","cf31c1d1":"test.shape","e0d7c768":"train.info()","5996ff64":"test.info()","f1847de7":"train.isnull().sum()","e5f12298":"test.isnull().sum()","a43c36d4":"def grafico(feature):\n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar', stacked=True, figsize=(10,5))","2e5153a8":"grafico('Sex')","c419c6e0":"grafico('Pclass')","9c0ada34":"grafico('SibSp')","0fe60c06":"grafico('Parch')","c61c98c0":"grafico('Embarked')","57074fc3":"train_test = [train, test]\nfor dataset in train_test:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)","b6569eea":"train['Title'].value_counts()","381dc6f7":"test['Title'].value_counts()","3cba8f59":"title_map = {\"Mr\": 0,\n            \"Miss\": 1,\n            \"Mrs\": 2,\n            \"Master\": 3,\n            \"Dr\": 3,\n            \"Rev\": 3,\n            \"Col\": 3,\n            \"Major\": 3,\n            \"Mlle\": 3,\n            \"Ms\": 3,\n            \"Don\": 3,\n            \"Lady\": 3,\n            \"Jonkheer\": 3,\n            \"Countess\": 3,\n            \"Mme\": 3,\n            \"Sir\": 3,\n            \"Capt\": 3}\nfor dataset in train_test:\n    dataset['Title'] = dataset['Title'].map(title_map)","c91a017c":"test[\"Title\"].fillna(0, inplace=True)","ef13ee9a":"train.isnull().sum()","402c404c":"train.head()","94b993ea":"grafico('Title')","7d849b16":"train.drop('Name', axis=1, inplace=True)\ntest.drop('Name', axis=1, inplace=True)","9773a5ad":"test.head()","c4c48484":"train.head()","167d366c":"sex_map = {\"male\": 0, \"female\": 1}\nfor dataset in train_test:\n    dataset['Sex'] = dataset['Sex'].map(sex_map)","49b6378e":"grafico('Sex')","5fe5715e":"train.head(100)","6c8218d7":"train[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\ntest[\"Age\"].fillna(test.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)","915c7509":"facet = sb.FacetGrid(train, hue=\"Survived\", aspect=4)\nfacet.map(sb.kdeplot, 'Age', shade=True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()\npl.show()","f0ae16ae":"for dataset in train_test:\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] =0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 26), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 36), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 62), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 62), 'Age'] = 4","bc1243df":"grafico('Age')","26c53b30":"Pclass1 = train[train['Pclass']==1]['Embarked'].value_counts()\nPclass2 = train[train['Pclass']==2]['Embarked'].value_counts()\nPclass3 = train[train['Pclass']==3]['Embarked'].value_counts()\ndf = pd.DataFrame([Pclass1,Pclass2,Pclass3])\ndf.index = ['1st class', '2nd class', '3rd class']\ndf.plot(kind='bar', stacked=True, figsize=(10,5))","3dd80097":"for dataset in train_test:\n    dataset['Embarked'] =  dataset['Embarked'].fillna('S')","bfe815de":"train.head()","ff611cab":"emb_map = {\"S\": 0,\n           \"C\": 1,\n           \"Q\": 2}\nfor dataset in train_test:\n    dataset['Embarked'] = dataset['Embarked'].map(emb_map)","8176edcd":"train[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntest[\"Fare\"].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)","d17293e3":"for dataset in train_test:\n    dataset.loc[ dataset['Fare'] <= 17, 'Fare'] =0\n    dataset.loc[(dataset['Fare'] > 17) & (dataset['Fare'] <= 30), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 30) & (dataset['Fare'] <= 100), 'Fare'] = 2\n    dataset.loc[(dataset['Fare'] > 100), 'Fare'] = 3","c6924c44":"train.Cabin.value_counts()","8d9a3038":"for dataset in train_test:\n    dataset['Cabin'] = dataset['Cabin'].str[:1]","84d97250":"Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts()\nPclass2 = train[train['Pclass']==2]['Cabin'].value_counts()\nPclass3 = train[train['Pclass']==3]['Cabin'].value_counts()\ndf = pd.DataFrame([Pclass1,Pclass2,Pclass3])\ndf.index = ['1st class', '2nd class', '3rd class']\ndf.plot(kind='bar', stacked=True, figsize=(10,5))","5ff565bd":"cab_map = {\"A\": 0,\n           \"B\": 0.4,\n           \"C\": 0.8,\n           \"D\": 1.2,\n           \"E\": 1.6,\n           \"F\": 2.0,\n           \"G\": 2.4,\n           \"T\": 2.8}\nfor dataset in train_test:\n    dataset['Cabin'] = dataset['Cabin'].map(cab_map)","ec578add":"train[\"Cabin\"].fillna(train.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)\ntest[\"Cabin\"].fillna(test.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)","4d4485f1":"train[\"FamilySize\"] = train[\"SibSp\"] + train[\"Parch\"]  + 1\ntest[\"FamilySize\"] = test[\"SibSp\"] + test[\"Parch\"]  + 1","496324f0":"facet = sb.FacetGrid(train, hue=\"Survived\", aspect=4)\nfacet.map(sb.kdeplot, 'FamilySize', shade= True)\nfacet.set(xlim=(0, train['FamilySize'].max()))\nfacet.add_legend()","547d2ea0":"family_map = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4}\nfor dataset in train_test:\n    dataset['FamilySize'] = dataset['FamilySize'].map(family_map)","69dbc6ec":"train.head()","0e4c0dc0":"test.head()","609edbc4":"features_drop = ['Ticket', 'SibSp', 'Parch']\ntrain = train.drop(features_drop, axis=1)\ntest = test.drop(features_drop, axis=1)\ntrain = train.drop(['PassengerId'], axis=1)\ntrain_data = train.drop('Survived', axis=1)\ntarget = train['Survived']\n\ntrain_data.shape, target.shape","4a18b039":"train_data.head(10)","9dcbabdc":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn import linear_model\nfrom sklearn.svm import SVC\n\nimport numpy as np","399bcff9":"train.info()","8d451a0d":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","5d66ea9e":"clf = KNeighborsClassifier(n_neighbors = 13)\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nround(np.mean(score)*100, 2)","2eaba337":"clf = DecisionTreeClassifier()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nround(np.mean(score)*100, 2)","0dc694b3":"clf = RandomForestClassifier(n_estimators=13)\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nround(np.mean(score)*100, 2)","072cda2a":"clf = GaussianNB()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nround(np.mean(score)*100, 2)","90d85aa3":"clf = SVC()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nround(np.mean(score)*100,2)","a574657e":"clf = QuadraticDiscriminantAnalysis()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nround(np.mean(score)*100,2)","2f0d0a62":"clf = linear_model.LinearRegression()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1)\nprint(score)\nround(np.mean(score)*100,2)","c0391e76":"clf = SVC()\n\nclf.fit(train_data, target)\n\ntest_data = test.drop(\"PassengerId\", axis=1).copy()\n\nprediction = clf.predict(test_data)\n\ntest_data2 = pd.read_csv('\/kaggle\/input\/testes\/teste.csv')\nprediction2 = clf.predict(test_data2)\n\nprint(prediction2)","3c7c52e5":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": prediction\n    })\n\nsubmission.to_csv('\/kaggle\/working\/submission.csv', index=False)","cba645dc":"submission = pd.read_csv('\/kaggle\/working\/submission.csv')\nsubmission.head()","e859df02":"> * Mais do que 50% de cada classe veio do porto 'S', ent\u00e3o est\u00e1 sendo preenchido os vazios com 'S'.","04f1ac6b":"> ### A seguir, foram escolhidos alguns gr\u00e1ficos a serem plotados para a visualiza\u00e7\u00e3o de como as *features* se comportam:","59cc41f2":"> * Abaixo mostra as informa\u00e7\u00f5es de cada *features* do conjunto de teste:","93f623a8":"> ### Ent\u00e3o, criamos classes e classificamos as idades por faixas et\u00e1rias:\n| Faixa et\u00e1ria | Classe |\n|--------------|--------|\n| 0 - 16       | 0      |\n| 17 - 26      | 1      |\n| 27 - 36      | 2      |\n| 37 - 62      | 3      |\n| Maior que 62 | 4      |\n \n","b9ecae0f":"> * Gr\u00e1fico baseado no n\u00famero de pais\/filhos a bordo:","ace83fa6":"> ###  Transformando os t\u00edtulos em representa\u00e7\u00e3o num\u00e9rica:","706bf245":"# 4. MODELING","346b3090":"> ### Desta forma, tornamos o gr\u00e1fico mais limpo e f\u00e1cil de ser interpretado:","03cc961c":"# 1. BUSINESS UNDERSTANDING\n\n## 1.1. Problem\u00e1tica\n>### Os motivos que contribu\u00edram para o naufr\u00e1gio foram: fatores naturais, como o clima; e causas humanas, como neglig\u00eancia, pois n\u00e3o haviam botes salva-vidas suficientes para os passageiros e tripulantes e muitos dos botes salva-vidas n\u00e3o estavam com a sua capacidade m\u00e1xima de pessoas a bordo, e se estivessem seria poss\u00edvel salvar 53% dos passageiros, mas apenas 32% deles sobreviveram.\n>### Embora aqueles que escaparam com vida tiveram sua sorte, alguns grupos de pessoas eram mais propensos a escaparem da morte do que outros. A maioria do sobreviventes eram mulheres, crian\u00e7as e passageiros da 1\u00aa Classe, deixando evidente que existe algum padr\u00e3o que pode ser extra\u00eddo dos dados brutos, que ser\u00e1 apresentado ao longo do projeto.\n\n>### Ent\u00e3o, eis que surge a quest\u00e3o: Quais eram as caracter\u00edsticas das pessoas que sobreviveram ao desastre? Haveria um padr\u00e3o entre as caracter\u00edsticas dos sobreviventes?  Por que certas pessoas sobreviveram e outras n\u00e3o?\n<p align=\"center\">\n  <img width=\"550\" height=\"250\" src=\"https:\/\/digitalks.com.br\/wp-content\/uploads\/2018\/08\/blockchain-marketing-digital.png\">\n<\/p>\n","53a6e8c7":"# 5. EVALUATION\n> * Ser\u00e1 verificado se os resultados foram atingidos baseados no objetivo definido do projeto: **atingir pelo menos 80% de acur\u00e1cia.**","c5458282":"> * Abaixo mostra a dimens\u00e3o do conjunto de dados de teste:\n>> - 418 registros (linhas);\n>> - 11  *features* (colunas).\n\n> Obs: Tem 1 coluna a menos, pois \u00e9 a *feature* que indica se o passageiro sobreviveu (1) ou n\u00e3o (0) e n\u00e3o deve estar no conjunto de testes, pois \u00e9 a previs\u00e3o que o algoritmo nos mostrar\u00e1.\n","03cb12ef":"# 6. DEPLOYMENT\n\n> ### Criamos um pequeno *dataset* com os dados dos integrantes do grupo do projeto para o algoritmo ser executado e testarmos o seu funcionamento.\n","52caf435":"> ### A seguir, \u00e9 preenchido os valores vazios das tarifas com a m\u00e9dia de cada conjunto:","14d0b6c9":"## 1.2. Objetivo do Projeto\n\n>### Construir um algoritmo de *Machine Learning* para prever o \u00edndice de sobreviv\u00eancia dos passageiros do *Titanic*, que tenha pelo menos 80% de acur\u00e1cia, baseadas nas *features* dos *datasets* disponibilizados no desafio do *[Kaggle](https:\/\/www.kaggle.com\/c\/titanic)*.","bd25ad89":"> ### Importando classificadores:","1918d14d":"> * Abaixo mostra a dimens\u00e3o do conjunto de dados de treino:\n>> - 891 registros (linhas);\n>> - 12  *features* (colunas).\n","12f72554":"> * O resultado apresentado acima, mostra que dos 5 integrantes do grupo, baseado nas *features*, apenas 1 integrante sobreviveria.","8c3c0852":"> ### Aqui tamb\u00e9m \u00e9 feito um mapping do tamanho das fam\u00edlias, substituindo elas por valores n\u00famericos que variam de 0.4 em 0.4:","e457ffaa":"> * Assim criando apenas 5 classes, vamos obter o resultado apenas das 5 classes.","170c2e34":"> * **Conjunto de teste**\n>> #### A fun\u00e7\u00e3o abaixo, nos mostra as 5 primeiras linhas do *Test Dataset*.","59113abb":"> ### Analisando o *dataset*:\n\n> * **Conjunto de treinamento**\n>> #### A fun\u00e7\u00e3o abaixo, nos mostra as 5 primeiras linhas do *Train Dataset*.","8b77cbfb":"> * Gr\u00e1fico baseado no n\u00famero de irm\u00e3os\/c\u00f4njuges a bordo:","e6e7980d":"> ### Os *datasets* possuem uma quantidade de dados nulos (sem valor) em algumas *features*.\n> *  Abaixo apresenta a quantidade de dados nulos do conjunto de treino:\n","302173b8":"> ## Naive Bayes","c24036e2":"> ## Florestas Aleat\u00f3rias (Random Forest):","c0f3c4c2":"> ### Abaixo mostra os valores atribu\u00eddos para a *feature* 'Title' do conjunto de treino:","e67c84b6":"> * Gr\u00e1fico baseado no sexo:","85a3730c":"> ### E para finalizar a prepara\u00e7\u00e3o de dados do *dataset*, foi definido que:\n> * As *features* 'SibSP' e 'Parch' v\u00e3o ser *dropadas*, pois  n\u00e3o s\u00e3o mais necess\u00e1rias;\n> * As *features* 'Ticket' e 'PassengerId' n\u00e3o ser\u00e3o importantes para nossa an\u00e1lise;\n> * E no conjunto de treino a *feature* 'Survived' vai ser eliminada.","07e379d5":"> * Abaixo mostra as informa\u00e7\u00f5es de cada *features* do conjunto de treino:","94c2341d":"> ### Foram definidos alguns classificadores que ser\u00e3o utilizados para analisarmos qual ser\u00e1 o melhor m\u00e9todo a ser aplicado no algoritmo, baseado no resultado da apura\u00e7\u00e3o *SCORE*:\n> * Cross Validation com K-fold;\n> * KNN;\n> * \u00c1rvore de Decis\u00e3o; \n> * Florestas Aleat\u00f3rias (Random Forest);\n> * Naive Bayes;\n> * Support Vector Machine (SVM);\n> * QDA; \n> * Regress\u00e3o Linear.","f34786a5":"> ### A seguir, substitu\u00edmos os valores 'male' e 'female' da *feature* 'Sex' por 0 e 1 respectivamente, para criarmos o gr\u00e1fico de sobreviventes entre homens e mulheres:","cdf715ef":"# 3. DATA PREPARATION\n\n> * Criar classes baseados nos pronomes de tratamento;\n> * Preencher os valores vazios das idades, baseados nos nas m\u00e9dias de idade dos pronomes de tratamento;\n> * Substituir valores da *feature* 'Sex' para valores num\u00e9ricos;\n> * Criar classes para as idades.","120aaa79":"## 1.3. Metodologia\n\n> ### Ser\u00e1 utilizada o CRISP-DM (*Cross Industry Standard Process for Data Mining*), \u00e9 uma metodologia de processo de minera\u00e7\u00e3o de dados, capaz de transformar os dados em conhecimento e informa\u00e7\u00f5es para estrat\u00e9gias de neg\u00f3cio.\n> ## Classificador de dados utilizado no *Machine Learning*:\n> * Cross Validation com K-fold;\n> * KNN;\n> * \u00c1rvore de Decis\u00e3o; \n> * Florestas Aleat\u00f3rias (Random Forest);\n> * Naive Bayes;\n> * Support Vector Machine (SVM);\n> * QDA; \n> * Regress\u00e3o Linear.\n\n> ## Tecnologias utilizadas:\n\n> ### Ambiente de desenvolvimento:\n>> * Jupyter Notebook - Servidor do Kaggle\n\n> ### Linguagem de programa\u00e7\u00e3o:\n>> *  Python\n\n> ### Bibliotecas:\n>> *  Pandas\n>> *  Numpy\n>> *  Seaborn\n>> *  Matplotlib\n>> *  SciKit Learn\n\n> ### Formato dos *datasets*:\n>> *  .csv (valores separados por v\u00edrgulas)","720a95e5":"> ### \u00c9 feito um *mapping* das cabines substituindo elas por valores n\u00famericos que variam de 0.4 em 0.4:","88a0f673":"> ## Regress\u00e3o Linear:","74e9ec0d":"> ### Vamos retirar a *feature* 'Name' de ambos os conjuntos:","ff42b9c9":"> * Gr\u00e1fico baseado na classe de embarque:","a47d7dfb":"# 2. DATA UNDERSTANDING\n> * Importar as bibliotecas utilizadas;\n> * Importar os *datasets* utilizando a biblioteca Pandas;\n> * Analisar os *datasets*.","25869c45":"> * O gr\u00e1fico abaixo mostra a quantidade de passageiros em determinadas se\u00e7\u00f5es de cabines baseada nas suas classes:","2445b875":" # PROJETO TITANIC","6bef60e2":"> ### Nesta etapa, utiliza-se a *feature* 'Name' para criar a *feature* 'Title', que nada mais \u00e9 do que a extra\u00e7\u00e3o do t\u00edtulo dos nomes da pessoas,isto \u00e9, os pronomes de tratamento, por exemplo: Mr., Miss., etc.:\n","fee3d352":"> ## KNN:","05e149ed":"> ### Foi criada uma faixa de valores para a tarifa:\n| Faixa valores| Classe |\n|--------------|--------|\n| 0 - 17       | 0      |\n| 18 - 30      | 1      |\n| 31 - 100     | 2      |\n| > 100        | 3      |","53c8dab9":"> * Gr\u00e1fico baseado em qual porto o navio embarcou:","8f451d1e":"> ## QDA:","fc6a8fc5":"> ### Nesta etapa, \u00e9 feito um mapping dos lugares de embarque para n\u00fameros, transformando as letras em representa\u00e7\u00e3o n\u00famerica, isto \u00e9, atribu\u00edmos aos valores da feature 'Embarked' os numeros 0, 1 e 2:","c93e06ac":"> ### Arquivos disponibilizados pelo *[Kaggle](https:\/\/www.kaggle.com\/c\/titanic)*:\n","68de4fbb":"> ## \u00c1rvore de Decis\u00e3o:","b90cd8c4":"> ## Cross Validation com K-fold:","4a1ac9f1":">## Support Vector Machine (SVM):","075e0e4c":"<p align=\"center\">\n  <img width=\"550\" height=\"250\" src=\"https:\/\/canalhistoria.pt\/wp-content\/uploads\/2016\/05\/1.Portada.jpg\">\n<\/p>\n\n### O naufr\u00e1gio do RMS Titanic \u00e9 uma das trag\u00e9dias mais famosas da hist\u00f3ria, originando diversos livros, filmes e afins. \u00c9 v\u00e1lido lembrar que a hist\u00f3ria narrada de forma c\u00e9lebre por James Cameron em seu filme de 1997, ilustra perfeitamente o motivo deste desafio. Vamos come\u00e7ar com uma breve perspectiva sobre o tema: O Titanic foi um navio de passageiros constru\u00eddo nos estaleiros da Harland and Wolff durante o per\u00edodo de mar\u00e7o de 1909 a maio de 1911 em Belfast no Reino Unido. Naquela \u00e9poca, a constru\u00e7\u00e3o do Titanic levou cerca de 2 anos e custou 7,5 milh\u00f5es de d\u00f3lares. A embarca\u00e7\u00e3o partiu em sua viagem inaugural de Southampton para Nova Iorque em 10 de abril de 1912, com passagem em Cherbourg-Octeville na Fran\u00e7a e em Queenstown na Irlanda. Devido a sua excelente projeta\u00e7\u00e3o, gerou boatos de que a embarca\u00e7\u00e3o seria \"inafund\u00e1vel\", por\u00e9m, \u00e0s 23h40min do dia 14 de abril, a embarca\u00e7\u00e3o se chocou contra um iceberg. Em 15 de abril de 1912, o Titanic afundou matando 1.502 dos 2.224 passageiros e tripulantes, ou seja, apenas 32% desses passageiros sobreviveram ao naufr\u00e1gio, tornando assim o maior desastre mar\u00edtimo em tempos de paz da hist\u00f3ria.\n\n","c54dd62d":"> ### Por fim, executamos o algoritmo no conjunto de teste do projeto para prever se cada um dos passeiros do Titanic sobreviveria \u00e0 trag\u00e9dia ou n\u00e3o, baseado na an\u00e1lise, defini\u00e7\u00e3o das t\u00e9cnicas e *datasets* disponibilizados pelo desafio do [Kaggle](https:\/\/www.kaggle.com\/c\/titanic).\n> ### O classificador escolhido para fazer a predi\u00e7\u00e3o do conjunto de dados foi o **Support Vector Machine (SVM)**, no qual obtivemos o *score* mais alto, de 83,5.  ","e5808f93":"> ### Nesta etapa, um gr\u00e1fico \u00e9 plotado para analisar os lugares de embarque das pessoas:","fa7469d3":"> *  Contagem dos t\u00edtulos presentes no conjunto de treino:","348bb7a7":"> * Abaixo apresenta a quantidade de dados nulos do conjunto de teste:\n","0059592a":"> ### A partir disto, vamos criar um algoritmo capaz de dizer se uma pessoa sobreviveria \u00e0 trag\u00e9dia do Titanic, baseado em *Features* que o pr\u00f3prio desafio nos disponibiliza atrav\u00e9s de um algoritmo de *Machine Learning*.","dcf7a200":"> ### Aqui come\u00e7a a an\u00e1lise relacionado ao tamanho da fam\u00edlia dos passeiros:","72cc6513":"> * Contagem de cada titulo presente no conjunto de teste:","c3423a96":"> ### Como existem v\u00e1rias idades presentes nos conjuntos de dados, se cri\u00e1ssemos um gr\u00e1fico n\u00e3o teria como saber qual a idade que mais sobreviveu \u00e0 trag\u00e9dia, pois ter\u00edamos muitos resultados...","877fc2fc":"> ### Aqui foi feito um *substring* utilizando apenas a primeira posi\u00e7\u00e3o da cabine, ou seja, apenas as letras:","7124300e":"> ### Importa\u00e7\u00e3o de bibliotecas e defini\u00e7\u00e3o de v\u00e1riaveis de **treino** e de **teste** com a biblioteca *pandas*, utilizando os caminhos dos arquivos mostrados no c\u00f3digo anterior."}}