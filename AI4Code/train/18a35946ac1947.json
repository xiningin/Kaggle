{"cell_type":{"baff129d":"code","30fa72d6":"code","25d44f36":"code","6b291881":"code","ec984443":"code","65ffbec5":"code","e6e7e32f":"markdown"},"source":{"baff129d":"import random\nfrom functools import lru_cache\nimport os\nimport math\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport sklearn.preprocessing\nfrom sklearn.preprocessing import LabelEncoder","30fa72d6":"# Hack for running on kernels and locally\nRUNNING_ON_KERNELS = 'KAGGLE_WORKING_DIR' in os.environ\ninput_dir = '..\/input' if RUNNING_ON_KERNELS else '..\/input\/movies'\nout_dir = '.' if RUNNING_ON_KERNELS else '..\/input\/movielens_preprocessed'\n\nrating_path = os.path.join(input_dir, 'rating.csv')\ndf = pd.read_csv(rating_path, usecols=['userId', 'movieId', 'rating'])\n# Shuffle (reproducibly)\ndf = df.sample(frac=1, random_state=1).reset_index(drop=True)\n\n# Partitioning train\/val according to behaviour of keras.Model.fit() when called with\n# validation_split kwarg (which is to take validation data from the end as a contiguous\n# chunk)\nval_split = .05\nn_ratings = len(df)\nn_train = math.floor(n_ratings * (1-val_split))\nitrain = df.index[:n_train]\nival = df.index[n_train:]\n\n# Compactify movie ids. \nmovie_id_encoder = LabelEncoder()\n# XXX: Just fitting globally for simplicity. See movie_helpers.py for more 'principled'\n# approach. I don't think there's any realistically useful data leakage here though.\n#orig_movieIds = df['movieId']\ndf['movieId'] = movie_id_encoder.fit_transform(df['movieId'])\n\n# Add centred target variable\ndf['y'] = df['rating'] - df.loc[itrain, 'rating'].mean()\n\nSCALE = 0\nif SCALE:\n    # Add version of target variable scale to [0, 1]\n    yscaler = sklearn.preprocessing.MinMaxScaler()\n    yscaler.fit(df.loc[itrain, 'rating'].values.reshape(-1, 1))\n    df['y_unit_scaled'] = yscaler.transform(df['rating'].values.reshape(-1, 1))\n\npath = os.path.join(out_dir, 'rating.csv')\ndf.to_csv(path, index=False)","25d44f36":"# Save a 10% sample of ratings for exercises (with re-compactified movieIds, and mapping back to canonical movie ids)\nfrom sklearn.model_selection import GroupShuffleSplit\n\nmovie_counts = df.groupby('movieId').size()\nthresh = 1000\npop_movies = movie_counts[movie_counts >= thresh].index\n\npop_df = df[df.movieId.isin(pop_movies)]\n\n# Take approx 10% of the whole dataset\nfrac = 2 * 10**6 \/ len(pop_df)\nprint(frac)\nsplitter = GroupShuffleSplit(n_splits=1, test_size=frac, random_state=1)\nsplits = splitter.split(pop_df, groups=pop_df.userId)\n_, mini = next(splits)\n\nmini_df = pop_df.iloc[mini].copy()\n\nprint(\n    '{:,}'.format(len(mini_df)),\n    len(df.userId.unique()) \/\/ 1000,\n    len(mini_df.userId.unique()) \/\/ 1000,\n    sep='\\n',\n)\n\n# Compactify ids\n\ndef compactify_ids(df, col, backup=True):\n    encoder = LabelEncoder()\n    if backup:\n        df[col+'_orig'] = df[col]\n    df[col] = encoder.fit_transform(df[col])\n    \nfor col in ['movieId', 'userId']:\n    compactify_ids(mini_df, col, backup=col=='movieId')\n    \n# Shuffle\nmini_df = mini_df.sample(frac=1, random_state=1)\n\n# Recalculate y (just to be totally on the level. Very little opportunity for contamination here.)\nval_split = .05\nn_mini_train = math.floor(len(mini_df) * (1-val_split))\nmini_train_rating_mean = mini_df.iloc[:n_mini_train]['rating'].mean()\nmini_df['y'] = mini_df['rating'] - mini_train_rating_mean\n\npath = os.path.join(out_dir, 'mini_rating.csv')\nmini_df.to_csv(path, index=False)\n\nprint(\n    df.userId.max(),\n    mini_df.userId.max(),\n    '\\n',\n    df.movieId.max(),\n    mini_df.movieId.max(),\n)","6b291881":"def munge_title(title):\n    i = title.rfind(' (')\n    if i != -1:\n        title = title[:i]\n    for suff_word in ['The', 'A', 'An']:\n        suffix = ', {}'.format(suff_word)\n        if title.endswith(suffix):\n            title = suff_word + ' ' + title[:-len(suffix)]\n    return title\n\ndef get_year(title):\n    l = title.rfind('(') + 1\n    try:\n        return int(title[l:l+4])\n    except ValueError:\n        print(title, end='\\t')\n        return 0\n\nmovie_path = os.path.join(input_dir, 'movie.csv')\nmovie_df = pd.read_csv(movie_path)\nmdf = movie_df\n\n# XXX: hack\nassert mdf.loc[\n    mdf.movieId==64997,\n    'title'].iloc[0] == 'War of the Worlds (2005)'\nmdf.loc[\n    mdf.movieId==64997,\n    'title'\n] = 'War of the Worlds (2005)x'\n\n#mdf['movieId_orig'] = mdf['movieId']\nn_orig = len(mdf)\n\n# There are some movies listed in movie.csv which have no ratings. Drop them.\nwhitelist = set(movie_id_encoder.classes_)\nmdf = mdf[mdf['movieId'].isin(whitelist)].copy()\nprint(\"Went from {} movies to {} after filtering out movies with no ratings\".format(\n    n_orig, len(mdf)\n))\n\n# New, compact movie Ids\nmdf['movieId'] = movie_id_encoder.transform(mdf['movieId'].values)\n\nmdf = mdf.sort_values(by='movieId').reset_index(drop=True)\n\n# By default use original title field (which includes year of release) as unique key\nmdf['key'] = mdf['title']\n\nmdf['year'] = mdf['title'].map(get_year)\nmdf['title'] = mdf['title'].map(munge_title)\n\n# For movies whose munged title are unique, use it as their key\ntitle_counts = mdf.groupby('title').size()\nunique_titles = title_counts.index[title_counts == 1]\nunique_ids = mdf.index[mdf.title.isin(unique_titles)]\nmdf.loc[unique_ids, 'key'] = mdf.loc[unique_ids, 'title']\n\nmdf['n_ratings'] = df.groupby('movieId').size()\nmean_ratings = df.groupby('movieId')['rating'].mean()\nmdf['mean_rating'] = mean_ratings\n\npath = os.path.join(out_dir, 'movie.csv')\nmdf.to_csv(path)","ec984443":"mdf.head()","65ffbec5":"df.head()","e6e7e32f":"Preprocessing\n\n- Subset dataset to movies\/users appearing at least n\/m times\n- compactify movie ids\n- do train\/test split?\n\nOutput = new versions of...\n\nrating.csv. As before except\n- no timestamp column\n- use compactified movieIds\n- add val\/train flag\n- add 'y' col (centred)\n- add 'yscaled' col\n\nmovie.csv. As before except\n- new compactified movieIds\n- parse out base title and year into separate cols (keeping original as well - maybe as 'key' column)\n- nratings col\n- avg_rating col\n\nAlso, some other file mapping between old and new movie ids (just in case that's useful later?)\nOr maybe just store in movie.csv"}}