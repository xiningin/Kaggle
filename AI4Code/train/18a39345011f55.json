{"cell_type":{"e85c38aa":"code","ef934ba4":"code","27ada41a":"code","9b08f25b":"code","2df5a0d7":"code","ccf4e8d3":"code","b64931c8":"code","2bbf830a":"code","a338e1f7":"code","8fa8808f":"code","6693a49f":"code","0ef3217b":"code","17a8ba54":"code","eee48814":"code","1fccdaf5":"code","edb9b2c3":"code","c61b1785":"code","9a7dfb69":"code","afd45803":"code","5117d804":"code","9dfb9eea":"code","089f6e36":"code","b8caee20":"code","39e870c5":"code","72a6dfde":"code","544cba05":"code","7af3ae22":"code","754ec727":"code","f049111c":"code","8181c73a":"code","f9f2dac1":"code","a10f1836":"code","e4bb6cfe":"code","4b993dac":"code","a88100f4":"code","fa2f8687":"code","dae0851a":"code","085d4699":"code","254ca7b2":"code","93aff1cd":"code","900013fb":"code","140ac424":"code","caabf5c8":"code","52ca6413":"code","4297686e":"code","aab18723":"code","6d689e54":"code","5c959dce":"code","d3cdfeb4":"code","f7c84bb7":"code","03e25dd9":"code","45d48041":"code","80bd8378":"code","bc11108c":"code","937635c5":"code","04a5d0b9":"code","27497bcf":"code","ca8c1f18":"code","da3b0a6c":"code","d93cd935":"code","9bb6d3de":"code","a925a75b":"code","77996e98":"code","70fc9597":"code","2c940a9e":"code","8d795f1a":"code","2eba49bf":"markdown","65c5db75":"markdown","54f8b2fe":"markdown","2ae9b580":"markdown","0816d6a9":"markdown","c6ceb869":"markdown","86cb86bb":"markdown","8f465d1c":"markdown","4ce56457":"markdown","16626558":"markdown","5fa49842":"markdown","6a985924":"markdown","34f06f81":"markdown","efacf414":"markdown","9f2b2623":"markdown","9063e851":"markdown","4fbd681a":"markdown","f3cf6ee0":"markdown","1413979f":"markdown","be97dc62":"markdown","f0621d1f":"markdown","6b2f8ef8":"markdown","b6a828eb":"markdown","be6b2767":"markdown","6da79de3":"markdown"},"source":{"e85c38aa":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom nltk.stem import WordNetLemmatizer\nimport re\nfrom textblob import TextBlob\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom gensim.models import Word2Vec\nfrom gensim.models.keyedvectors import KeyedVectors\nimport time\nfrom keras.layers import Dense, Input, Flatten, Dropout\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Sequential\nfrom keras import losses\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model","ef934ba4":"train_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\nsubmit_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","27ada41a":"train_data[train_data['text'].isna()]","9b08f25b":"train_data.info()","2df5a0d7":"train_data.groupby('target').count()","ccf4e8d3":"%matplotlib inline","b64931c8":"piedata = train_data['target']\nplt.figure(figsize=(6,6))\npiedata.value_counts().plot(kind = 'pie',autopct = '%.2f%%')","2bbf830a":"num_words_0 = train_data[train_data['target']==0]['text'].apply(lambda x: len(x.split()))\nnum_words_1 = train_data[train_data['target']==1]['text'].apply(lambda x: len(x.split()))\nplt.figure(figsize=(12,6))\nsns.kdeplot(num_words_0, shade=True, color = 'b').set_title('Kernel distribution of number of words')\nsns.kdeplot(num_words_1, shade=True, color = 'r')\nplt.legend(labels=['0_no disaster', '1_disaster'])","a338e1f7":"len_word_0 = train_data[train_data['target']==0]['text'].str.split().map(lambda x: [len(i) for i in x])\nave_len_0 = len_word_0.map(lambda x: np.mean(x))\nlen_word_1 = train_data[train_data['target']==1]['text'].str.split().map(lambda x: [len(i) for i in x])\nave_len_1 = len_word_1.map(lambda x: np.mean(x))\nplt.figure(figsize=(12,6))\nsns.kdeplot(ave_len_0, shade=True, color='b').set_title('Kernel distribution of average words lenth')\nsns.kdeplot(ave_len_1, shade=True, color='r')\nplt.legend(labels=['0_no disaster', '1_disaster'])","8fa8808f":"data = pd.concat([train_data, submit_data])\ndata.shape","6693a49f":"data['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'https?\\S+'), '', x))\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'[\\\/\/:,.!?@&\\-\\'\\`\\\"\\_\\n\\#]'), ' ', x))\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'<.*?>'), '', x))\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  \n                           u\"\\U0001F300-\\U0001F5FF\"  \n                           u\"\\U0001F680-\\U0001F6FF\"  \n                           u\"\\U0001F1E0-\\U0001F1FF\"  \n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE), '', x))\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'\\d'), '', x))\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'[^\\w]'), ' ', x))\ndata['text'] = data['text'].str.lower()","0ef3217b":"'''\ntext_series = data.loc[:,'text']\nfor i in range(len(text_series)):\n    content = text_series.iloc[i]\n    textblob = TextBlob(content)\n    text_series.iloc[i] = textblob.correct()\n'''","17a8ba54":"clean_train = data[0:train_data.shape[0]]\nclean_submit = data[train_data.shape[0]:-1]\n\nX_train, X_test, y_train, y_test = train_test_split(clean_train['text'], clean_train['target'],\n                                                   test_size = 0.2, random_state = 4)","eee48814":"def tfidf(words):\n    tfidf_vectorizer = TfidfVectorizer()\n    data_feature = tfidf_vectorizer.fit_transform(words)\n    return data_feature, tfidf_vectorizer\n\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train.tolist())\nX_test_tfidf = tfidf_vectorizer.transform(X_test.tolist())","1fccdaf5":"X_train_tfidf.shape","edb9b2c3":"lr_tfidf = LogisticRegression(class_weight = 'balanced', solver = 'lbfgs', n_jobs = -1)\nlr_tfidf.fit(X_train_tfidf, y_train)\ny_predicted_lr = lr_tfidf.predict(X_test_tfidf)","c61b1785":"def score_metrics(y_test, y_predicted):\n    accuracy = accuracy_score(y_test, y_predicted)\n    precision = precision_score(y_test, y_predicted)\n    recall = recall_score(y_test, y_predicted)\n    print(\"accuracy = %0.3f, precision = %0.3f, recall = %0.3f\" % (accuracy, precision, recall))","9a7dfb69":"score_metrics(y_test, y_predicted_lr)","afd45803":"def plot_confusion_matrix(y_test, y_predicted, title='Confusion Matrix'):\n    cm = confusion_matrix(y_test, y_predicted)\n    plt.figure(figsize=(8,6))\n    sns.heatmap(cm,annot=True, fmt='.20g')\n    plt.title(title)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","5117d804":"plot_confusion_matrix(y_test, y_predicted_lr)","9dfb9eea":"# fail to sort and plot the top 10 most important features in disaster and non-disaster text\n'''\nindex_to_word = [(v,k) for k,v in tfidf_vectorizer.vocabulary_.items()]\nsorted(index_to_word, key=lambda x: x[0], reverse=True)\n'''","089f6e36":"pipeline = Pipeline([\n    ('clf', DecisionTreeClassifier(splitter='random', class_weight='balanced'))\n])\nparameters = {\n    'clf__max_depth':(150,160,165),\n    'clf__min_samples_split':(18,20,23),\n    'clf__min_samples_leaf':(5,6,7)\n}\n\ndf_tfidf = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=-1, scoring='f1')\ndf_tfidf.fit(X_train_tfidf, y_train)\n\nprint(df_tfidf.best_estimator_.get_params())","b8caee20":"y_predicted_dt = df_tfidf.predict(X_test_tfidf)","39e870c5":"score_metrics(y_test, y_predicted_dt)","72a6dfde":"plot_confusion_matrix(y_test, y_predicted_dt)","544cba05":"!pip install gensim -i http:\/\/pypi.douban.com\/simple --trusted-host pypi.douban.com","7af3ae22":"import requests\nurl = 'https:\/\/s3.amazonaws.com\/dl4j-distribution\/GoogleNews-vectors-negative300.bin.gz'\nfilename = url.split('\/')[-1]\nr = requests.get(url)\nwith open(filename, \"wb\") as file:\n        file.write(r.content)\n        \n!ls","754ec727":"stop_words = stopwords.words('english')\nfor word in ['us','no','yet']:\n    stop_words.append(word)\n\ndata_list = []\ntext_series = data['text']\nfor i in range(len(text_series)):\n    content = text_series.iloc[i]\n    cutwords = [word for word in content.split(' ') if word not in  stop_words if len(word) != 0]\n    data_list.append(cutwords)","f049111c":"for i in range(len(data_list)):\n    content = data_list[i]\n    if len(content) <1:\n        print(i)","8181c73a":"data_list[7626]","f9f2dac1":"'''\nstarttime = time.time()\nword2vec_model = Word2Vec(data_list, size=300, iter=10, min_count=10)\nusedtime = time.time() - starttime\nprint('It took %.2fseconds to train word2vec' %usedtime)\n'''","a10f1836":"import gensim\nword2vec_path='.\/GoogleNews-vectors-negative300.bin.gz'\nword2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)","e4bb6cfe":"word2vec_model.wv['earthquake'].shape","4b993dac":"def get_textVector(data_list, word2vec, textsVectors_list):\n    for i in range(len(data_list)):\n        words_perText = data_list[i]\n        if len(words_perText) < 1:\n            words_vector = [np.zeros(300)]\n        else:\n            words_vector = [word2vec.wv[k]  if k in word2vec_model else  np.zeros(300) for k in words_perText]\n        text_vector = np.array(words_vector).mean(axis=0)\n        textsVectors_list.append(text_vector)\n    return textsVectors_list","a88100f4":"textsVectors_list = []\nget_textVector(data_list, word2vec_model, textsVectors_list)\nX = np.array(textsVectors_list)","fa2f8687":"pd.isnull(X).any()","dae0851a":"word2vec_X = X[0:train_data.shape[0]]\ny = data['target'][0:train_data.shape[0]]\nword2vec_submit = X[train_data.shape[0]:-1]\n\nX_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(word2vec_X, y,\n                                                   test_size = 0.2, random_state = 4)","085d4699":"print(X_train_word2vec.shape, y_train_word2vec.shape)","254ca7b2":"word2vec_lr = LogisticRegression(class_weight = 'balanced', solver = 'lbfgs', n_jobs = -1)\nword2vec_lr.fit(X_train_word2vec, y_train_word2vec)\ny_predicted_word2vec_lr = word2vec_lr.predict(X_test_word2vec)","93aff1cd":"score_metrics(y_test_word2vec, y_predicted_word2vec_lr)","900013fb":"plot_confusion_matrix(y_test_word2vec, y_predicted_word2vec_lr)","140ac424":"compare_list = []\nfor (i,j) in zip(y_test_word2vec, y_predicted_word2vec_lr):\n    k = i - j\n    compare_list.append(k)\n\nwrong_num = [i for i,j in enumerate(compare_list) if j != 0]\ntext_series[0:train_data.shape[0]][wrong_num]","caabf5c8":"lenlen = []\nfor i in range(len(data_list)):\n    content = data_list[i]\n    perlen = len(content)\n    lenlen.append(perlen)\nprint(max(lenlen))","52ca6413":"max_sequence_length = 26\nembedding_dim = 300","4297686e":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(data_list)\nsequences = tokenizer.texts_to_sequences(data_list)\nword_index = tokenizer.word_index\ncnn_data = pad_sequences(sequences, maxlen = max_sequence_length)\ncnn_label = to_categorical(np.asarray(train_data['target']))\nprint('len of word_index:', len(word_index))\nprint('shape of data tensor:', cnn_data.shape)\nprint('shape of label tensoe:', cnn_label.shape)","aab18723":"trainCNN_data = cnn_data[0:train_data.shape[0]]\nX_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(trainCNN_data, cnn_label,\n                                                   test_size = 0.2, random_state = 4)\nX_cnn, X_val_cnn, y_cnn, y_val_cnn = train_test_split(X_train_cnn, y_train_cnn,\n                                                   test_size = 0.2, random_state = 4)","6d689e54":"CNNmodel = Sequential()\nCNNmodel.add(Embedding(len(word_index)+1, embedding_dim, input_length = max_sequence_length))\nCNNmodel.add(Conv1D(filters=250, kernel_size=3, strides=1, padding='valid', activation = 'relu'))\nCNNmodel.add(MaxPooling1D(pool_size=3))\nCNNmodel.add(Flatten())\nCNNmodel.add(Dense(embedding_dim, activation='relu'))\nCNNmodel.add(Dropout(0.8))\nCNNmodel.add(Dense(cnn_label.shape[1], activation='sigmoid'))\n\nCNNmodel.summary()","5c959dce":"CNNmodel.compile(optimizer='adam', loss=losses.binary_crossentropy, metrics=['accuracy'])\nhistory = CNNmodel.fit(X_cnn, y_cnn, epochs=3, validation_data=(X_val_cnn, y_val_cnn))","d3cdfeb4":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.5,1])\nplt.legend()\nplt.show()","f7c84bb7":"test_loss, test_acc = CNNmodel.evaluate(X_test_cnn, y_test_cnn, verbose=2)\nprint('test loss:',test_loss)\nprint('test acc:',test_acc)","03e25dd9":"embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\nfor word, i in word_index.items(): \n    if word in word2vec_model:\n        embedding_matrix[i] = np.asarray(word2vec_model.wv[word])","45d48041":"embedding_layer = Embedding(len(word_index)+1,\n                           embedding_dim,\n                           weights = [embedding_matrix],\n                           input_length = max_sequence_length,\n                           trainable = False)","80bd8378":"model = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Conv1D(filters=150, kernel_size=3, strides=1, padding='valid', activation = 'relu'))\nmodel.add(MaxPooling1D(pool_size=3))\nmodel.add(Flatten())\nmodel.add(Dense(embedding_dim, activation='relu'))\nmodel.add(Dropout(0.8))\nmodel.add(Dense(cnn_label.shape[1], activation='sigmoid'))\n\nmodel.summary()","bc11108c":"model.compile(optimizer='adam', loss=losses.binary_crossentropy, metrics=['accuracy'])\nhistory = model.fit(X_cnn, y_cnn, epochs=10, validation_data=(X_val_cnn, y_val_cnn))","937635c5":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.5,1])\nplt.legend()\nplt.show()","04a5d0b9":"test_loss, test_acc = model.evaluate(X_test_cnn, y_test_cnn, verbose=2)\nprint('test loss:',test_loss)\nprint('test acc:',test_acc)","27497bcf":"tf.__version__","ca8c1f18":"import tensorflow_hub as hub\nhub.__version__","da3b0a6c":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\n# get the official tokenization created by the Google team (??)","d93cd935":"import tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization","9bb6d3de":"def bert_encode(texts, bert_layer, max_len=128):\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n    \n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n        text = text[:max_len - 2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        input_ids = tokens + [0]* pad_len\n        all_tokens.append(input_ids)\n\n        masks = [1]*len(input_sequence) + [0]* pad_len\n        all_masks.append(masks)\n        \n        segments = [0]* max_len\n        all_segments.append(segments)\n        \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\n    \ndef build_model(bert_layer, max_len = 128, lr = 1e-5):\n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32,name=\"input_word_ids\")\n    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32,name=\"input_mask\")\n    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32,name=\"segment_ids\")\n        \n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    dense_out = Dense(1,activation=\"relu\")(pooled_output)\n    drop_out = tf.keras.layers.Dropout(0.8)(dense_out)\n    out = Dense(1,activation=\"sigmoid\")(pooled_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    adam = tf.keras.optimizers.Adam(lr)\n    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n        \n    return model\n\n\ndef plot_curve(history):\n    plt.plot(history.history['accuracy'], label='accuracy')\n    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.ylim([0.5,1])\n    plt.legend()\n    plt.show()","a925a75b":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","77996e98":"# read and encode train data\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\n\ntrain_input = bert_encode(train.text.values, bert_layer, max_len=128)\ntrain_labels = np.array(train.target)","70fc9597":"# train model\nmodel = build_model(bert_layer, max_len=128, lr = 1e-5)\nmodel.summary()\n\ncheckpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)\n\nplot_curve(train_history)","2c940a9e":"# predict\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\n\ntest_input = bert_encode(test.text.values, bert_layer, max_len=128)\nmodel.load_weights('model.h5')\ntest_pred = model.predict(test_input)","8d795f1a":"# submit\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission['target'] = np.round(test_pred).astype('int')\nsubmission.to_csv('submission.csv', index=False)\nsubmission.groupby('target').count()","2eba49bf":"Explore the wrong predicted label.","65c5db75":"Let's have a check.","54f8b2fe":"Get rid of stopwords.","2ae9b580":"# **Introduction**\n* This is my first time to join kaggle competion and get sarted in the nlp field. I still have some problems in this notebook, so I would be very gtateful for anyone to help me solve the problems or suggest me improvements.\n* I tried different models here. Including tf-idf + logistic regression\/decision tree, word2vec + logistic regression, embedding + cnn, word2vec + cnn, bert. The result shows that the best accuracy is bert, next is logistic regression, then is word2vec+cnn.\n* When I wrote this kernel, I learned much from kernels below. The ideas from these kernels inspired me a lot, and some of my codes came from them. Thanks a lot to them!\n* [https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert](http:\/\/)\n* [https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub](http:\/\/)\n* [https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove](http:\/\/)\n* [https:\/\/blog.csdn.net\/qq_43522113\/article\/details\/89888036?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522160680506719721942257729%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=160680506719721942257729&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_v1~rank_blog_v1-7-89888036.pc_v1_rank_blog_v1&utm_term=%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5](http:\/\/)","0816d6a9":"*Here I used the data without cleaning.*","c6ceb869":"# **Feature Engineering 2nd. Word2vec**","86cb86bb":"After deleting the stopwords, some context became null. So, when dealing with the next step vectorization, I need to change these text to zero vector. Or, at last I put feature matrix into the model will fail because of different dimension. ","8f465d1c":"# **Model 1st. LogisticRegression**","4ce56457":"Download the trained word2vec vectors.","16626558":"# **Feature Engineering 1st. TF-IDF**","5fa49842":"The reasons for writing  'else  np.zeros(300)' is that some words cannot be vectorized by word2vec then becoming nan.","6a985924":"**Embedding + CNN**","34f06f81":"# **Model 2nd. DecisionTree**","efacf414":"Both the distribution of words number and words average lenth of two  class are similiar. Class 1(disaster) trends to have more words and longer words length. Accord with reality.","9f2b2623":"**Word2Vec + CNN**","9063e851":"# Data Exploring","4fbd681a":"*Obviously, the model is overfitting here. If anyone knows nice tricks to adjust overfitting, please tell me. Thanks!*","f3cf6ee0":"Less 1(disaster) predicted to be 0(no-disaster). But more 0(no-disaster) predicted to be 1.","1413979f":"# Getting Data","be97dc62":"# **Cleaning Data**","f0621d1f":"More tweets with class 0 ( No disaster).  Accord with reality. ","6b2f8ef8":"# **Model 3rd. CNN**","b6a828eb":"* *I have a question: If I do not get the official tokenization script here, 'tokenizer.tokenize' in get_tokens function will returns error:'raise _exceptions.UnparsedFlagAccessError(error_message)--Trying to access flag ... before flags were parsed'.*\n* *If anyone konws the reasons, please tell me. THANKS!*","be6b2767":"# **Model 4th. TFhub+Bert**","6da79de3":"*Ensure tensorflow's version >= 2.0 & tensorflow_hub's version >= 0.5.0*"}}