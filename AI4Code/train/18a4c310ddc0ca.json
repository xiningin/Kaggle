{"cell_type":{"0adf1242":"code","9a9fe41f":"code","e1c20e8d":"code","8fad3881":"code","f7054a85":"code","b300864a":"code","16bf3d1d":"code","c81c0b94":"code","15ddcad4":"code","b3b6f4ad":"code","04d181e3":"code","85997b94":"code","39eb083b":"code","bf28df03":"code","a5cba731":"code","543e7d86":"code","b036f624":"code","2ce4d0c4":"code","74ad2791":"code","3b17ae99":"code","513c7a49":"code","7781e035":"code","5bd05c88":"code","a5289581":"code","1c6853e4":"code","7fe6ba9c":"code","5648afaa":"code","6199fd75":"code","dfd147bb":"code","5cbb3b43":"code","e5ec4c92":"code","8503e26e":"code","25ba8a4e":"code","05d58be9":"code","5ecf6dc2":"code","4524365a":"code","dd03a680":"code","5c2ebd20":"code","969a0ab1":"code","cf9531c5":"code","50db6e72":"code","d4eaccb3":"code","ad93e604":"code","0bfd8f7b":"code","88d20439":"code","f3098dc9":"code","a6b4ca27":"code","ebfcfa55":"code","79979a87":"code","adac80d8":"code","7e8b1c38":"code","ab8e148c":"code","456a5ba1":"code","156f3778":"code","d0a11bfa":"code","8062f4ad":"code","f84946b0":"code","fdb8d488":"code","056c34d4":"code","5908581f":"code","096b95e0":"code","eb415e67":"code","fb150ba9":"code","e5e5d191":"code","35d0433d":"code","56fe102f":"code","93e8ae09":"code","f15768e0":"code","435f0fa3":"code","0cf212f7":"code","e1b4eab8":"code","1264c7f0":"code","ea20f64e":"code","5ea108d0":"code","eb332924":"code","e4c68b8f":"code","0517c264":"code","924738a4":"code","820f3458":"code","4dd9bc61":"code","b6806429":"code","5d511542":"code","68667e6b":"code","15c11cc6":"code","59794d4b":"code","ea1b15f5":"code","65581591":"code","6271c1c1":"code","0e7d5f2e":"code","b59830c7":"code","8b96350c":"code","f64bbe94":"code","50108236":"code","7b524d46":"code","66f7996c":"code","867d5e63":"code","cd8feb2e":"code","d76b41f7":"code","91bceb82":"code","26ab1438":"code","5bad3a8b":"code","66760966":"code","7320abed":"code","0b994caa":"code","3b35bd40":"code","a962b97d":"code","e7dcd219":"code","d52586ec":"code","de05a881":"code","2ea315b1":"code","38c86f7d":"code","93ff72d5":"code","7975141d":"code","e85b8380":"code","2a9675b3":"code","9e5caf3c":"code","0d8bd398":"code","e6e1f738":"code","4243d412":"code","a0294463":"code","80870fae":"code","e8aade5d":"code","072c0613":"code","39280b18":"code","f87669ed":"code","ab7eb05d":"code","50fd861d":"code","1f62cf46":"code","7183b108":"code","831b95c7":"code","03ade8bc":"code","fe9f52b1":"code","480412c5":"code","018b58d6":"code","7533e385":"code","fb8523df":"code","bd1f7e8e":"code","3be01e27":"code","5f156fb7":"code","ba33dc29":"code","040ac7fe":"code","b0c0f79c":"code","d3b6e3c5":"code","e854b58f":"code","627d2df1":"code","3bf5edbd":"code","a0c14c7b":"code","82a8ea94":"code","ef72b567":"code","4d6d909e":"code","919dd529":"code","4399d303":"code","f196fbbb":"code","b6f9d4d6":"code","932202dc":"code","ec1b7a62":"code","4b55be3c":"markdown","95cf702a":"markdown","1cf5b963":"markdown","bd0f5465":"markdown","4aeee05a":"markdown","758525b7":"markdown","0b760802":"markdown","ba09daa8":"markdown","fda26d76":"markdown","626cb2e3":"markdown","5d697e9c":"markdown","d5e62d83":"markdown","152ea41a":"markdown","7e8940e4":"markdown","e7e960d7":"markdown","fa733ba9":"markdown","2d90835c":"markdown","e8c7dcdb":"markdown","c6616221":"markdown","8489a795":"markdown","eb4df958":"markdown","2f4bc59a":"markdown","274fa09a":"markdown","82326fe2":"markdown","9f4d1eb8":"markdown","fa45987f":"markdown","f2591ac0":"markdown","61bd1113":"markdown","2349ffb0":"markdown","f699f109":"markdown","367d8263":"markdown","5d0fa1a0":"markdown","889d49e7":"markdown","c44c5e13":"markdown","b9783e3c":"markdown","11c5ea5e":"markdown","b56600df":"markdown","ce6815ac":"markdown","d47c4335":"markdown","fa9e9bde":"markdown","76f145ce":"markdown","886f12b5":"markdown","dfdbabc4":"markdown","9a4cf666":"markdown","7776c541":"markdown","42e3a078":"markdown","61074795":"markdown","29c1bf69":"markdown","a9b6940c":"markdown","aa567995":"markdown","87f30248":"markdown","6bc1c4ec":"markdown","97403c50":"markdown","9d7596a9":"markdown","74404388":"markdown","c8dd6892":"markdown","58f99e90":"markdown","27aa6d9d":"markdown","b29fff21":"markdown","cfb07753":"markdown"},"source":{"0adf1242":"#Importing all the necessary libraries.\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn import metrics\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, precision_score, recall_score, precision_recall_curve, f1_score, accuracy_score\n\npd.set_option(\"display.max_columns\", 50)","9a9fe41f":"#Reading the data into a dataframe.\n\nleads=pd.read_csv(\"..\/input\/lead-score\/Leads.csv\")\nleads.head()","e1c20e8d":"#Cheking the number of Rows and Columns.\n\nleads.shape","8fad3881":"#Checking the descriptive statistics of all the numerical columns.\n\nleads.describe()","f7054a85":"#Checking the data types of all the columns.\n\nleads.info()","b300864a":"#Analyzing null values.\n\nleads.isnull().sum()","16bf3d1d":"#Understanding the percentage of Null values in the \n\nround(100*(leads.isnull().sum()\/len(leads.index)), 2)","c81c0b94":"#Checking for distinct elements.\n\nleads.nunique()","15ddcad4":"#Now, we need to check if there are any duplicate rows of information for any lead.\n#If it exists, we need to remove the duplicates.\n\nleads.drop_duplicates(subset=['Prospect ID'], inplace=True)\n\n#Checking the shape to see if there were any duplicates present.\n\nleads.shape","b3b6f4ad":"#Now, it is better to drop all the columns which have only one unique value.\n\nleads= leads.drop(['Magazine','Receive More Updates About Our Courses','Update me on Supply Chain Content',\n                   'Get updates on DM Content','I agree to pay the amount through cheque'],axis=1)","04d181e3":"#Also,the value \"Select\" is equivalent to no value provided, so let's go ahead and do that.\n\nleads = leads.replace('Select', np.nan)","85997b94":"#When we observe the percentage of nulls, we take a call to drop columns with 45% or more null values.\n\ncol=leads.columns\n\nfor i in col:\n    if((100*(leads[i].isnull().sum()\/len(leads.index))) >= 45):\n        leads.drop(i, 1, inplace = True)","39eb083b":"#Checking for null values after dropping those columns with > 45% nulls.\n\nleads.isnull().sum()","bf28df03":"#Since Prospect ID and Lead Numbers are not useful for our model building and analysis, it is better to drop them.\n\nleads.drop(['Prospect ID', 'Lead Number'], 1, inplace = True)","a5cba731":"#Checking the values of the column: Lead Source.\n\nleads['Lead Source'].value_counts(dropna=False)","543e7d86":"#Here we can see a lot of values with low occurances, so it is better to club them together.\n\nleads['Lead Source'] = leads['Lead Source'].replace('google','Google')\nleads['Lead Source'] = leads['Lead Source'].replace(np.nan,'Others')\nleads['Lead Source'] = leads['Lead Source'].replace(['bing','Click2call','Press_Release','Live Chat','NC_EDM','testone','youtubechannel',\n                                                     'Pay per Click Ads','welearnblog_Home','WeLearn','blog','Facebook','Social Media'] ,'Others')","b036f624":"#Visualizing the data from this column:\n\nplt.figure(figsize=[20,5])\n\nsns.barplot(x=leads['Lead Source'].value_counts().index, \n            y=leads['Lead Source'].value_counts().values).set_title(\"Lead Source\", fontsize=30, color='Teal', pad = 20)\n\nplt.xlabel('Lead Source', fontsize= 20, color='Brown')\nplt.xticks(rotation=45, size = 12)\nplt.yticks(size = 12)\n\nplt.show()","2ce4d0c4":"#Checking the values of the column: Last Activity.\n\nleads['Last Activity'].value_counts(dropna=False)","74ad2791":"#Here we can see a lot of values with low occurances, so it is better to club them together.\n\nleads['Last Activity'] = leads['Last Activity'].replace(np.nan,'Others')\nleads['Last Activity'] = leads['Last Activity'].replace(['Resubscribed to emails','Visited Booth in Tradeshow','Email Received',\n                                                         'Email Marked Spam','View in browser link Clicked',\n                                                         'Approached upfront','Had a Phone Conversation','Unsubscribed','Unreachable'] ,'Others')","3b17ae99":"#Visualizing the data from this column:\n\nplt.figure(figsize=[22,5])\n\nsns.barplot(x=leads['Last Activity'].value_counts().index, \n            y=leads['Last Activity'].value_counts().values).set_title(\"Last Activity\", fontsize=30, color='Teal', pad = 20)\n\n\nplt.xticks(rotation=45, size = 12)\nplt.yticks(size = 12)\n\nplt.show()","513c7a49":"# Checking how skewed the data in the column(TotalVisits) is:\n\nplt.figure(figsize=[15,5])\n\nsns.distplot(leads['TotalVisits']).set_title(\"TotalVisits\", fontsize=20, color='Teal', pad=20)\nplt.show()","7781e035":"#Futher checking if there are any null values in the column: TotalVisits.\n\nprint(\"Percentage of nulls: \" + str(round(leads.TotalVisits.isnull().sum()\/len(leads.TotalVisits),2)))","5bd05c88":"#Checking the: TotalVisits column.\n\nplt.figure(figsize=[15,8])\n\nsns.set_style('darkgrid')\n\nplt.subplot(2,1,1)\nsns.boxplot(leads['TotalVisits']).set_title(\"TotalVisits\", fontsize=20, color='Teal', pad=20)\n\nplt.subplot(2,1,2)\nsns.distplot(leads['TotalVisits'], color='g')\n\nplt.show()","a5289581":"#Checking the percentile values of the TotalVisits Column\n\nleads.describe(percentiles = [0.05, 0.5, 0.75, 0.90, 0.95, 0.98, 0.99])","1c6853e4":"#Since in the TotalVisits column, we can see that there is a huge jump from 99th percentile to the max. And also below 5,\n#we are removing the top 1 percentile and the bottom 5 percentile.\n\nplt.figure(figsize=[8,5])\n\nQ3 = leads.TotalVisits.quantile(0.99)\nleads = leads[(leads.TotalVisits <= Q3)]\nQ1 = leads.TotalVisits.quantile(0.05)\nleads = leads[(leads.TotalVisits >= Q1)]\nsns.boxplot(y=leads['TotalVisits']).set_title(\"TotalVisits\", fontsize=20, color='Teal', pad=20)\nplt.ylabel(\"Total Visits\",size = 15)\nplt.show()","7fe6ba9c":"# Checking how skewed the data in the column(Page Views Per Visit) is:\n\nplt.figure(figsize=[15,5])\n\nsns.distplot(leads['Page Views Per Visit']).set_title(\"Page Views Per Visit\", fontsize=20, color='Teal', pad=20)\nplt.show()","5648afaa":"#Futher checking if there are any null values in the column: Page Views Per Visit.\n\nprint(\"Percentage of nulls: \" + str(round(leads['Page Views Per Visit'].isnull().sum()\/len(leads['Page Views Per Visit']),2)))","6199fd75":"# Checking the Page Views Per Visit column\n\nplt.figure(figsize=[15,8])\n\nplt.subplot(2,1,1)\nsns.boxplot(leads['Page Views Per Visit']).set_title(\"Page Views Per Visit\", fontsize=20, color='Teal', pad=20)\n\nplt.subplot(2,1,2)\nsns.distplot(leads['Page Views Per Visit'], color='g')\n\nplt.show()","dfd147bb":"#Checking the percentile values of the Page Views Per Visit column\n\nleads.describe(percentiles = [0.05, 0.5, 0.75, 0.90, 0.95, 0.98, 0.99])","5cbb3b43":"#Since in the Page Views Per Visit column, we can see that there is a huge jump from 99th percentile to the max. And also below 5,\n#we are removing the top 1 percentile and the bottom 5 percentile.\n\nplt.figure(figsize=[8,5])\n\nQ3 = leads[\"Page Views Per Visit\"].quantile(0.99)\nleads = leads[(leads[\"Page Views Per Visit\"] <= Q3)]\nQ1 = leads.TotalVisits.quantile(0.05)\nleads = leads[(leads[\"Page Views Per Visit\"] >= Q1)]\nsns.boxplot(y=leads[\"Page Views Per Visit\"]).set_title(\"Page Views Per Visit\", fontsize=20, color='Teal', pad=20)\nplt.ylabel(\"Page Views Per Visit\",size = 15)\nplt.show()","e5ec4c92":"#Checking for all the values and their occurances in the column: Country.\n\nleads['Country'].value_counts(dropna=False)","8503e26e":"#Visualizing the Country column.\n\nplt.figure(figsize=[20,5])\n\nsns.barplot(x=leads['Country'].value_counts().index, \n            y=leads['Country'].value_counts().values).set_title(\"Country\", fontsize=30, color='Teal', pad = 20)\n\nplt.xlabel('Country', fontsize= 20, color='Brown')\nplt.xticks(rotation=45, size = 12)\nplt.yticks(size = 12)\n\nplt.show()","25ba8a4e":"#Dropping the Country column.\n\nleads.drop(columns = \"Country\", inplace = True)","05d58be9":"#Checking for all the values and their occurances in the column: Specialization.\n\nleads['Specialization'].value_counts(dropna=False)","5ecf6dc2":"#Here, we can see that the NaN values occur 3x that of the 2nd highest value in this column. So we create a new category here.\n\nleads['Specialization'] = leads['Specialization'].replace(np.nan, 'Unknown')","4524365a":"# We also notice that there are several Management related Specializations, Hence we can have a single bin for them.\n\nleads['Specialization'] = leads['Specialization'].replace(['Finance Management','Human Resource Management','Marketing Management'\n                                                           ,'Operations Management','IT Projects Management','Supply Chain Management',\n                                                         'Healthcare Management','Hospitality Management','Retail Management'],'Management Specialization')","dd03a680":"#Visualizing the \"Specialization\" column.\n\nplt.figure(figsize=[25,5])\n\nsns.barplot(x=leads['Specialization'].value_counts().index, \n            y=leads['Specialization'].value_counts().values).set_title(\"Specialization\", fontsize=30, color='Teal', pad = 20)\n\nplt.xticks(rotation=45, size = 13)\nplt.yticks(size = 13)\n\nplt.show()","5c2ebd20":"#Checking for all the values and their occurances in the column: \"What is your current occupation\".\n\nleads['What is your current occupation'].value_counts(dropna=False)","969a0ab1":"#Since the occurance of Housewife and Businessman is less than others, we can bin them together with Others.\n\nleads['What is your current occupation'] = leads['What is your current occupation'].replace(np.nan, 'Unemployed')\nleads['What is your current occupation'] = leads['What is your current occupation'].replace(['Housewife','Businessman','Other'] ,'Others')","cf9531c5":"#Visualizing the \"What is your current occupation\" column.\n\nplt.figure(figsize=[20,5])\n\nsns.barplot(x=leads['What is your current occupation'].value_counts().index, \n            y=leads['What is your current occupation'].value_counts().values).set_title(\"What is your current occupation\", \n                                                                                        fontsize=30, color='Teal', pad = 20)\n\nplt.xticks(rotation=45, size = 12)\nplt.yticks(size = 12)\n\nplt.show()","50db6e72":"#Checking for all the values and their occurances in the column: \"What matters most to you in choosing a course\".\n\nleads['What matters most to you in choosing a course'].value_counts(dropna=False)","d4eaccb3":"#Imputing the NaN values with Better Career Prospects\n\nleads['What matters most to you in choosing a course'] = leads['What matters most to you in choosing a course'].replace(np.nan,'Better Career Prospects')","ad93e604":"#Visualizing the \"What matters most to you in choosing a course\" column.\n\nplt.figure(figsize=[20,5])\n\nleads['What matters most to you in choosing a course'].value_counts(normalize=True).plot.barh(color='c').set_title(\"What matters most to you in choosing a course\", \n                                                                                            fontsize=20, color='Teal', pad=20)\n\nplt.xticks(rotation=45, size = 12)\nplt.yticks(size = 12)\n\n\nplt.show()","0bfd8f7b":"#Dropping the \"What matters most to you in choosing a course\" column:\n\nleads.drop(columns = 'What matters most to you in choosing a course', axis=1, inplace=True)","88d20439":"#Checking for all the values and their occurances in the column: \"Tags\".\n\nleads['Tags'].value_counts(dropna=False)","f3098dc9":"#Since the occurance of anything below 100 is less, we can bin them together with Others.\n\nleads['Tags'] = leads['Tags'].replace(np.nan, 'Unknown')\nleads['Tags'] = leads['Tags'].replace(['In confusion whether part time or DLP', 'in touch with EINS','Diploma holder (Not Eligible)',\n                                     'Approached upfront','number not provided', 'opp hangup','Still Thinking',\n                                    'Lost to Others','Shall take in the next coming month','Lateral student','Interested in Next batch',\n                                    'Recognition issue (DEC approval)','Want to take admission but has financial problems',\n                                    'University not recognized','switched off','Already a student','Not doing further education','invalid number','wrong number given',\n                                       'Interested  in full time MBA'] ,'Other_Reasons')","a6b4ca27":"#Visualizing the \"Tags\" column.\n\nplt.figure(figsize=[20,5])\n\nsns.barplot(x=leads['Tags'].value_counts().index, \n            y=leads['Tags'].value_counts().values).set_title(\"Tags\", fontsize=30, color='Teal', pad = 20)\n\nplt.xticks(rotation=45, size=13)\nplt.yticks(size=13)\n\nplt.show()","ebfcfa55":"#Checking for all the values and their occurances in the column: \"City\".\n\nleads['City'].value_counts(dropna=False)","79979a87":"#Imputing the value of NaN as Mumbai here.\n\nleads['City'] = leads['City'].replace(np.nan,'Mumbai')","adac80d8":"#Visualizing the column: City.\n\nplt.figure(figsize=[20,5])\n\nleads['City'].value_counts(normalize=True).plot.barh(color='orchid').set_title(\"City\", fontsize=20, color='Teal', pad=20)\n\nplt.xticks(rotation=45, size=13)\nplt.yticks(size=13)\n\nplt.show()","7e8b1c38":"#Since this column is also highly dominated by one single value, it is better to drop this column.\n\nleads.drop(columns = 'City', axis=1, inplace=True)","ab8e148c":"#Checking if any more null values exist in the dataframe.\n\nleads.isnull().sum()","456a5ba1":"#Checking for all the values and their occurances in the column: \"Lead Origin\".\n\nleads['Lead Origin'].value_counts()","156f3778":"#VALUE COUNTS IN GRAPH \n\nplt.figure(figsize=[20,5])\n\nleads['Lead Origin'].value_counts(normalize=True).plot.barh(color='darkorchid').set_title(\"Lead Origin\", fontsize=20, \n                                                                                          color='Teal', pad=20)\n\nplt.xlabel('Lead Origin', fontsize= 20, color='Brown')\nplt.xticks(rotation=45, size=12)\nplt.yticks(size=12)\n\nplt.show()","d0a11bfa":"#Checking for all the values and their occurances in the column: \"Do Not Email\".\n\nleads['Do Not Email'].value_counts()","8062f4ad":"#Checking for all the values and their occurances in the column: \"Do Not Call\".\n\nleads['Do Not Call'].value_counts()","f84946b0":"#Checking for all the values and their occurances in the column: \"Search\".\n\nleads['Search'].value_counts()","fdb8d488":"#Checking for all the values and their occurances in the column: \"Newspaper Article\".\n\nleads['Newspaper Article'].value_counts()","056c34d4":"#Checking for all the values and their occurances in the column: \"X Education Forums\".\n\nleads['X Education Forums'].value_counts()","5908581f":"#Checking for all the values and their occurances in the column: \"Newspaper\".\n\nleads['Newspaper'].value_counts()","096b95e0":"#Checking for all the values and their occurances in the column: \"Digital Advertisement\".\n\nleads['Digital Advertisement'].value_counts()","eb415e67":"#Checking for all the values and their occurances in the column: \"Through Recommendations\".\n\nleads['Through Recommendations'].value_counts()","fb150ba9":"#Since, for the above columns, we can observe that they are usually dominated by a single entity, it is best to drop them.\n\ncols = ['Do Not Email','Do Not Call','Search','Newspaper Article','Digital Advertisement','Through Recommendations','X Education Forums','Newspaper']\n\nleads.drop(columns=cols, axis=1, inplace=True)","e5e5d191":"#Checking for all the values and their occurances in the column: \"Converted\".\n\nleads['Converted'].value_counts()","35d0433d":"#Visualizing the column: Converted.\n\nplt.figure(figsize=[20,5])\n\nleads['Converted'].value_counts(normalize=True).plot.barh(color='skyblue').set_title(\"Converted\", fontsize=20,\n                                                                                          color='Teal', pad=20)\n\nplt.xticks(rotation=45, size=13)\nplt.yticks(size=13)\n\nplt.show()","56fe102f":"#Checking for any imbalance in the Converted column.\n\nconverted = leads[leads['Converted'] == 1].shape[0]\nnon_converted = leads[leads['Converted'] != 1].shape[0]\n\n# the ratio of converted to non_converted is:\n\nratio = float(converted \/ non_converted)\n\nif ratio > 0.95:\n    print(\"The data is imbalanced.\")\nelse:\n    print(\"The data is balanced.\")","93e8ae09":"#Checking for all the values and their occurances in the column: \"Total Time Spent on Website\".\n\nleads['Total Time Spent on Website'].value_counts().head()","f15768e0":"# Visualizing the column: \"Total Time Spent on Website\"\n\nplt.figure(figsize=[15,8])\n\nplt.subplot(2,1,1)\nsns.boxplot(leads['Total Time Spent on Website']).set_title(\"Total Time Spent on Website\", fontsize=20, color='Teal', pad=20)\n\nplt.subplot(2,1,2)\nsns.distplot(leads['Total Time Spent on Website'], color='g')\n\nplt.show()","435f0fa3":"#Checking the percentile values of the Total Time Spent on Website column.\n\nleads.describe(percentiles = [0.05, 0.5, 0.75, 0.90, 0.95, 0.98, 0.99])","0cf212f7":"#Since in the Total Time Spent on Website column, we can see that there is a huge jump from 99th percentile to the max. And also below 5,\n#we are removing the top 1 percentile and the bottom 5 percentile.\n\nplt.figure(figsize=[8,5])\n\nQ3 = leads[\"Total Time Spent on Website\"].quantile(0.99)\nleads = leads[(leads[\"Total Time Spent on Website\"] <= Q3)]\nQ1 = leads.TotalVisits.quantile(0.05)\nleads = leads[(leads[\"Total Time Spent on Website\"] >= Q1)]\nsns.boxplot(y=leads[\"Total Time Spent on Website\"]).set_title(\"Total Time Spent on Website\", fontsize=20, color='Teal', pad=20)\nplt.ylabel(\"Total Time Spent on Website\",size = 15)\nplt.show()","e1b4eab8":"#Checking for all the values and their occurances in the column: \"A free copy of Mastering The Interview\".\n\nleads['A free copy of Mastering The Interview'].value_counts()","1264c7f0":"#Performing One Hot encoding on the Yes\/No values.\n\nleads['A free copy of Mastering The Interview']=leads['A free copy of Mastering The Interview'].map({\"No\":0,\"Yes\":1})","ea20f64e":"#Visualizing the column: \"A free copy of Mastering The Interview\"\n\nplt.figure(figsize=[20,5])\n\nleads['A free copy of Mastering The Interview'].value_counts(normalize=True).plot.barh(color='wheat').set_title(\"A free copy of Mastering The Interview\", \n                                                                                                                 fontsize=20, color='Teal', pad=20)\n\nplt.xticks(rotation=45, size=13)\nplt.yticks(size=13)\n\nplt.show()","5ea108d0":"#Checking for all the values and their occurances in the column: \"Last Notable Activity\".\n\nleads['Last Notable Activity'].value_counts()","eb332924":"#Clubbing anything below 100 as \"Other_Activities\"\n\nleads['Last Notable Activity'] = leads['Last Notable Activity'].replace(['Email Bounced','Unsubscribed','Unreachable','Had a Phone Conversation','Email Marked Spam','Form Submitted on Website','Resubscribed to emails','Email Received','Approached upfront','View in browser link Clicked'], 'Other_Activities')","e4c68b8f":"#VALUE COUNTS IN GRAPH \n\nplt.figure(figsize=[20,5])\n\nsns.barplot(x=leads['Last Notable Activity'].value_counts().index, \n            y=leads['Last Notable Activity'].value_counts().values).set_title(\"Last Notable Activity\", fontsize=30, color='Teal', pad = 20)\n\nplt.xticks(rotation=45, size=13)\nplt.yticks(size=13)\n\nplt.show()","0517c264":"#Lead Origin Vs Converted\n\nplt.figure(figsize=[20,10])\n\nsns.countplot(data = leads, y= 'Lead Origin', order=leads['Lead Origin'].value_counts().index,\n              hue = 'Converted',palette='magma').set_title(\"Lead Origin Vs Converted \" ,\n                                                                      fontsize=30, color='Teal', pad=20)\n\n\nplt.ylabel(\"Lead Origin\", fontdict={'fontsize': 20, 'fontweight' : 5, 'color' : 'Brown'})\nplt.xlabel(\"Count\", fontdict={'fontsize': 20, 'fontweight' : 5, 'color' : 'Brown'})\nplt.xticks(size=13)\nplt.yticks(size=13)\n\nplt.show()","924738a4":"# Lead Source Vs Converted\n\nplt.figure(figsize=[20,10])\n\nsns.countplot(data = leads, y= 'Lead Source', order=leads['Lead Source'].value_counts().index,\n              hue = 'Converted').set_title(\"Lead Source Vs Converted \" ,\n                                                                      fontsize=30, color='Teal', pad=20)\n\n\nplt.ylabel(\"Lead Source\", fontdict={'fontsize': 20, 'fontweight' : 5, 'color' : 'Brown'})\nplt.xlabel(\"Count\", fontdict={'fontsize': 20, 'fontweight' : 5, 'color' : 'Brown'})\nplt.xticks(size=13)\nplt.yticks(size=13)\n\nplt.show()","820f3458":"#Last Activity Vs Converted\n\nplt.figure(figsize=[20,10])\n\nsns.countplot(data = leads, y= 'Last Activity', order=leads['Last Activity'].value_counts().index,\n              hue = 'Converted',palette='mako').set_title(\"Last Activity Vs Converted \" ,\n                                                                      fontsize=30, color='Teal', pad=20)\n\n\nplt.ylabel(\"Last Activity\", fontdict={'fontsize': 20, 'fontweight' : 5, 'color' : 'Brown'})\nplt.xlabel(\"Count\", fontdict={'fontsize': 20, 'fontweight' : 5, 'color' : 'Brown'})\nplt.xticks(size=13)\nplt.yticks(size=13)\n\nplt.show()","4dd9bc61":"#Specialization Vs Converted\n\nplt.figure(figsize=[20,15])\n\nsns.countplot(data = leads, y= 'Specialization', order=leads['Specialization'].value_counts().index,\n              hue = 'Converted',palette='icefire').set_title(\"Specialization Vs Converted \" ,\n                                                                      fontsize=30, color='Teal', pad=20)\n\n\nplt.ylabel(\"Specialization\", fontdict={'fontsize': 20, 'fontweight' : 5, 'color' : 'Brown'})\nplt.xlabel(\"Count\", fontdict={'fontsize': 20, 'fontweight' : 5, 'color' : 'Brown'})\nplt.xticks(size=13)\nplt.yticks(size=13)\n\nplt.show()","b6806429":"#What is your current occupation Vs Converted\n\nplt.figure(figsize=[20,10])\n\nsns.countplot(data = leads, y= 'What is your current occupation', order=leads['What is your current occupation'].value_counts().index,\n              hue = 'Converted',palette='cubehelix').set_title(\"What is your current occupation Vs Converted \" ,\n                                                                      fontsize=30, color='Teal', pad=20)\n\n\nplt.ylabel(\"What is your current occupation\", fontdict={'fontsize': 20, 'fontweight' : 5, 'color' : 'Brown'})\nplt.xlabel(\"Count\", fontdict={'fontsize': 20, 'fontweight' : 5, 'color' : 'Brown'})\nplt.xticks(size=13)\nplt.yticks(size=13)\n\nplt.show()","5d511542":"#Tags  Vs Converted\n\nplt.figure(figsize=[20,15])\n\nsns.countplot(data = leads, y= 'Tags', order=leads['Tags'].value_counts().index,\n              hue = 'Converted',palette='viridis').set_title(\"Tags Vs Converted \" ,\n                                                                      fontsize=30, color='Teal', pad=20)\n\n\nplt.ylabel(\"Tags\", fontdict={'fontsize': 20, 'fontweight' : 5, 'color' : 'Brown'})\nplt.xlabel(\"Count\", fontdict={'fontsize': 20, 'fontweight' : 5, 'color' : 'Brown'})\nplt.xticks(size=13)\nplt.yticks(size=13)\n\nplt.show()","68667e6b":"#Last Notable Activity Vs Converted\n\nplt.figure(figsize=[20,15])\n\nsns.countplot(data = leads, y= 'Last Notable Activity', order=leads['Last Notable Activity'].value_counts().index,\n              hue = 'Converted',palette='flare').set_title(\"Last Notable Activity Vs Converted \" ,\n                                                                      fontsize=30, color='Teal', pad=20)\n\n\nplt.ylabel(\"Last Notable Activity\\n\\n\", fontdict={'fontsize': 25, 'fontweight' : 5, 'color' : 'Brown'})\nplt.xlabel(\"Count\", fontdict={'fontsize': 20, 'fontweight' : 5, 'color' : 'Brown'})\nplt.xticks(size=13)\nplt.yticks(size=13, rotation=0)\n\nplt.show()","15c11cc6":"#Checking all the columns that we are left with...\n\nprint(list(leads.columns), end=\"\")","59794d4b":"#Checking the percentile of all the numerical values\n\nround(leads.describe(percentiles=[0.10,0.25,0.50,0.60,0.75,0.90,0.95,0.99,0.999]),2)","ea1b15f5":"plt.figure(figsize=[12,12])\n\nf=sns.heatmap(leads.corr(),cmap = \"YlGnBu\" , annot=True).set_title('Correlation between variables\\n', fontsize = 25, color='Teal')\nplt.yticks(rotation=0)\nplt.show()","65581591":"#Before we proceed with the modelling, we need to create dummies for the categorical columns.\n\n#Finding all the categorical columns:\n\ncols_to_drop = leads.select_dtypes(include=[object]).columns\ncols_to_drop","6271c1c1":"#Creating dummies for all the categorical columns one by one:\n\n#For \"Lead Origin\", \"What is your current occupation\" and \"City\":\n\ndummy = pd.get_dummies(leads[['Lead Origin','What is your current occupation']], drop_first=True)\n\nleads = pd.concat([leads,dummy], axis=1)\n\n#For \"Specialization\":\n\ndummy = pd.get_dummies(leads['Specialization'], prefix  = 'Specialization')\n\ndummy = dummy.drop(['Specialization_Unknown'], axis = 1)\n\nleads = pd.concat([leads, dummy], axis = 1)\n\n#For \"Lead Source\":\n\ndummy = pd.get_dummies(leads['Lead Source'], prefix  = 'Lead Source')\n\ndummy = dummy.drop(['Lead Source_Others'], axis = 1)\n\nleads = pd.concat([leads, dummy], axis = 1)\n\n#For \"Last Activity\":\n\ndummy = pd.get_dummies(leads['Last Activity'], prefix  = 'Last Activity')\n\ndummy = dummy.drop(['Last Activity_Others'], axis = 1)\n\nleads = pd.concat([leads, dummy], axis = 1)\n\n#For \"Tags\":\n\ndummy = pd.get_dummies(leads['Tags'], prefix  = 'Tags')\n\ndummy = dummy.drop(['Tags_Other_Reasons'], axis = 1)\n\nleads = pd.concat([leads, dummy], axis = 1)\n\n#For \"Last Notable Activity\":\n\ndummy = pd.get_dummies(leads['Last Notable Activity'], prefix  = 'Last Notable Activity')\n\ndummy = dummy.drop(['Last Notable Activity_Other_Activities'], axis = 1)\n\nleads = pd.concat([leads, dummy], axis = 1)","0e7d5f2e":"#Dropping the categorical columns:\n\nleads.drop(columns = cols_to_drop, axis=1, inplace = True)","b59830c7":"leads.info() #Checking the columns that have been created after the dummy creation","8b96350c":"#Resetting the index of the dataframe\n\nleads.reset_index()","f64bbe94":"#Creating the X and y variables:\n\ny = leads[\"Converted\"]\n\nX = leads.drop(columns=[\"Converted\"])","50108236":"X.head() #verifying","7b524d46":"y.head() #verifying","66f7996c":"#Performing the train_test_split:\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, random_state = 30)","867d5e63":"X_train.shape #Verifying","cd8feb2e":"X_test.shape #Verifying","d76b41f7":"#Here, in this case, we are going to perform Standardization\n\nscale = StandardScaler() #creating an object of the class\n\nnumerical_cols = X_train.select_dtypes(include = ['float64','int64']).columns #Finding the numerical columns\n\n#Now we use the Standardization on the numerical columns:\n\nX_train[numerical_cols] = scale.fit_transform(X_train[numerical_cols])\n\nX_train.head() #Verifying if the scaling happened correctly","91bceb82":"#Creating a object of the Logistic Regression class\n\nregression_ = LogisticRegression()\n\n#Taking the help of RFE to eliminate the less important columns:\n\nrfe = RFE(regression_, 15)             # running RFE with 15 variables as output\n\nrfe = rfe.fit(X_train, y_train)","26ab1438":"rfe.support_","5bad3a8b":"lead_rfe = pd.DataFrame({'PREDICTORS': X_train.columns, 'SELECTED ': rfe.support_, 'RANKS': rfe.ranking_})\nlead_rfe.sort_values(by='RANKS')","66760966":"#Here are the list of RFE supported columns:\n\ncols = X_train.columns[rfe.support_]\ncols","7320abed":"X_train.columns[~rfe.support_]","0b994caa":"#Building the first Logistic Regression Model:\n\nX_train_sm = sm.add_constant(X_train[cols])\n\nmodel1 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n\nresult = model1.fit()\n\nresult.summary()","3b35bd40":"#Checking the Variation Inflation Factor:\n\nvif = pd.DataFrame()\nvif['Features'] = X_train[cols].columns\nvif['VIF'] = [variance_inflation_factor(X_train[cols].values, i) for i in range(X_train[cols].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","a962b97d":"#Dropping the \"Last Activity_SMS Sent\" column from the list of columns from before:\n\ncols = cols.drop('Last Activity_SMS Sent',1)","e7dcd219":"cols #Verifying if the column has been dropped","d52586ec":"#Building our second logistic regression model:\n\nX_train_sm = sm.add_constant(X_train[cols])\n\nmodel2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n\nresult = model2.fit()\n\nresult.summary()","de05a881":"#Since there are no high P values, we will go ahead and find the VIF:\n\nvif = pd.DataFrame()\nvif['Features'] = X_train[cols].columns\nvif['VIF'] = [variance_inflation_factor(X_train[cols].values, i) for i in range(X_train[cols].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","2ea315b1":"#Predicting on the training dataset:\n\ny_train_pred = result.predict(X_train_sm)\n\ny_train_pred.head(10)","38c86f7d":"y_train_pred = y_train_pred.values.reshape(-1) #Reshaping the data","93ff72d5":"#Now it is time to compare the Actual Converted with that of the Predicted Conversion.\n\ny_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Converted_prob':y_train_pred})\n\ny_train_pred_final['Prospect ID'] = y_train.index\n\ny_train_pred_final.head()","7975141d":"# Now we are going to take an arbitrary number \n# If the converted probability is higher than this number, the Lead is converted else not.\n\ny_train_pred_final['Predicted'] = y_train_pred_final.Converted_prob.map(lambda x: 1 if x > 0.5 else 0)\n\ny_train_pred_final.head()","e85b8380":"#Checking the Confusion Matrix of this Logistic Regression Model:\n \ncon_matrix = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted )\n\nprint(con_matrix)","2a9675b3":"#Checking the Accuracy of the model\n\nacc = metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Predicted)\n\nprint(\"Accuracy in the model is: \" + str(round(acc,2)))","9e5caf3c":"#Finding: true positive, true negatives, false positives, false negatives\n\nTP = con_matrix[1,1] \nTN = con_matrix[0,0] \nFP = con_matrix[0,1] \nFN = con_matrix[1,0] ","0d8bd398":"# Checking the sensitivity of the model:\n\nround(TP \/ float(TP+FN),2)","e6e1f738":"# Checking the specificity of the model:\n\nround(TN \/ float(TN+FP),2)","4243d412":"# Checking the False positive rates of the model:\n\nround(FP\/ float(TN+FP),2)","a0294463":"# Calculating Positive Predictive value of the model:\n\nround (TP \/ float(TP+FP),2)","80870fae":"# Calculating the Negative predictive value of the model:\n\nround (TN \/ float(TN+ FN),2)","e8aade5d":"#Now to check the ROC curve:\n\ndef draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(10, 7))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('\\nFalse Positive Rate or [1 - True Negative Rate]', size = 13, color = \"Brown\")\n    plt.ylabel('True Positive Rate\\n', size = 13, color = \"Brown\")\n    plt.title('Receiver operating characteristic example', size = 20, pad = 20, color = \"Teal\")\n    plt.xticks(size = 13)\n    plt.yticks(size = 13)\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","072c0613":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Converted_prob, drop_intermediate = False )","39280b18":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Converted_prob)","f87669ed":"#Finding the different probability cut-offs:\n\nnumbers = [float(x)\/10 for x in range(10)]\n\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Converted_prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","ab7eb05d":"# Calculating the accuracy sensitivity and specificity for various probability cutoffs.\ncutoff = pd.DataFrame( columns = ['Prob','Accuracy','Sensitivity','Specificity'])\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff.loc[i] =[ i ,accuracy,sensi,speci]\n    \nprint(cutoff)","50fd861d":"# Visualizing the graph:\n\ncutoff.plot.line(x='Prob', y=['Accuracy','Sensitivity','Specificity'])\nplt.xlabel('\\nProbabilities', size = 13, color = \"Brown\")\nplt.title('Probabilities of Accuracy, Sensitivity and Specificity', size = 15, pad = 20, color = \"Teal\")\nplt.xticks(size = 13)\nplt.yticks(size = 13)\n\nplt.show()","1f62cf46":"#Finding the final predicted\n\ny_train_pred_final['Final_Predicted'] = y_train_pred_final.Converted_prob.map( lambda x: 1 if x > 0.28 else 0)\n\ny_train_pred_final.head()","7183b108":"# Checking the final confusion matrix:\n\ncon_matrix1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted )\nprint(con_matrix1)","831b95c7":"# Accuracy of the model after selecting optimal cut-off.\n\nround(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted),2)","03ade8bc":"# Sensitivity of the model after selecting the optimal cut-off.\n\nround(TP \/ float(TP+FN),2)","fe9f52b1":"# Specificity of the model after selecting the optimal cut-off.\n\nround(TN \/ float(TN+FP),2)","480412c5":"# Calculating the Precision:\n\nTP \/ TP + FP\n\nround(con_matrix1[1,1]\/(con_matrix1[0,1]+con_matrix1[1,1]),2)","018b58d6":"# Calculating the Recall:\n\nTP \/ TP + FN\n\nround(con_matrix1[1,1]\/(con_matrix1[1,0]+con_matrix1[1,1]),2)","7533e385":"#Finding the precision_score:\n\nround(precision_score(y_train_pred_final.Converted , y_train_pred_final.Final_Predicted),2)","fb8523df":"#Finding the recall_score:\n\nround(recall_score(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted),2)","bd1f7e8e":"#Creating the Recall Curve:\nplt.figure(figsize=[10,10])\n\ny_train_pred_final.Converted, y_train_pred_final.Final_Predicted\np, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Converted_prob)\n\nplt.plot(thresholds, p[:-1], \"y-\")\nplt.plot(thresholds, r[:-1], \"b-\")\nplt.show()","3be01e27":"#Scaling the Test Set:\n\nnumeric_cols=X_test.select_dtypes(include=['float64', 'int64']).columns\n\nX_test[numeric_cols] = scale.transform(X_test[numeric_cols])\n\nX_test.head()","5f156fb7":"# Adding a constant since we are using StatsModel\nX_test = X_test[cols]\n\nX_test_sm = sm.add_constant(X_test)\n\ny_test_pred = result.predict(X_test_sm)","ba33dc29":"y_test_pred.head(10)","040ac7fe":"y_pred_ = pd.DataFrame(y_test_pred)","b0c0f79c":"y_pred_.head()","d3b6e3c5":"# Converting y_test to dataframe\n\ny_test_df = pd.DataFrame(y_test)","e854b58f":"# Making Prospect ID as the index:\n\ny_test_df['Prospect ID'] = y_test_df.index","627d2df1":"# Removing index for both dataframes:\n\ny_pred_.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","3bf5edbd":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_],axis=1)","a0c14c7b":"y_pred_final.head()","82a8ea94":"# Renaming the column:\n\ny_pred_final= y_pred_final.rename(columns={ 0 : 'Converted_probability'})\n\ny_pred_final.head()","ef72b567":"# Arranging the cols:\n\ny_pred_final = y_pred_final[['Prospect ID','Converted','Converted_probability']]\ny_pred_final['Lead_Score'] = y_pred_final.Converted_probability.map( lambda x: round(x*100))\n\ny_pred_final.head()","4d6d909e":"y_pred_final['Final_Predicted'] = y_pred_final.Converted_probability.map(lambda x: 1 if x > 0.28 else 0)\n\ny_pred_final.head()","919dd529":"# Checking the confusion Matrix:\n\ncon_mat_pred = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.Final_Predicted )\nprint(con_mat_pred)","4399d303":"#Checking the Accuracy on the Test Dataset of the model:\n\nround(metrics.accuracy_score(y_pred_final.Converted, y_pred_final.Final_Predicted),2)","f196fbbb":"#Checking the Sensitivity on the Test Dataset of the model:\n\nTP = con_mat_pred[1,1] \nTN = con_mat_pred[0,0] \nFP = con_mat_pred[0,1] \nFN = con_mat_pred[1,0] \n\n\nround(TP \/ float(TP+FN),2)","b6f9d4d6":"#Checking the Specificity on the Test Dataset of the model:\n\nround(TN \/ float(TN+FP),2)","932202dc":"#Finding the precision_score on the Test Dataset:\n\nround(precision_score(y_pred_final.Converted , y_pred_final.Final_Predicted),2)","ec1b7a62":"#Finding the recall_score on the Test Dataset:\n\nround(recall_score(y_pred_final.Converted, y_pred_final.Final_Predicted),2)","4b55be3c":"##### **`INFERENCE`**","95cf702a":"##### **`INFERENCE`**\n\n- It can be observed from the plot above that Leads who are tagged as \"Will revert back after reading the email\" are more likely to be converted followed by \"Closed by Horizzon\" and \"Lost to EINS\".\n- Leads who are still graduating, interested in other course or their phone ringing and not picking up are less likely to get converted.","1cf5b963":"##### **`INFERENCE`** \n\n- Here we can see that there are almost 38% of candidates getting converted.\n- But a majority of more than 60% are not converted","bd0f5465":"### MODEL NUMBER - 2","4aeee05a":"- After building the second model, we can see that the all the Variation Inflation Factors are now sensible.\n- We will use this model to derive our probabilities etc on the Training dataset.","758525b7":"### Prediction on Testing Dataset","0b760802":"##### **`INFERENCE`**","ba09daa8":"- From the ROC curve, we can see that the area under the curve is very high (0.97).\n- A high area under the curve indicates that the model is very good.","fda26d76":"- From the results given by StatsModel, we can see that none of the columns have a very high P value.\n- Hence, we will not drop anything here. Instead we will go ahead and find the Variance Inflation Factor to make further decisions.","626cb2e3":"- What we get to see here is that, the higher the value, the more likely the Lead is going to get converted.","5d697e9c":"#### FINAL MODEL EQUATION\n\n**`Probability of conversion = -4.3841 + ( 1.1248 * Total Time Spent on Website ) + ( 3.0066 * Lead Origin_Lead Add Form ) + ( 1.5997 * Lead Source_Olark Chat ) + ( 2.6224 * Lead Source_Welingak Website ) + ( -1.7582 * Last Activity_Email Bounced) + ( -1.2890 * Last Activity_Olark Chat Conversation ) + ( 3.1471 * Tags_Busy ) + ( 9.1264 * Tags_Closed by Horizzon ) + ( 8.6147 * Tags_Lost to EINS ) + ( -0.9941 * Tags_Ringing ) + ( 2.6034 * Tags_Unknown ) + ( 6.9711 * Tags_Will revert after reading the email ) + ( -0.7132 * Last Notable Activity_Modified ) + ( 2.2050 * Last Notable Activity_SMS Sent )`**","d5e62d83":"##### **`INFERENCE`** \n\n- Here, we can see that Leads, who's origin is from the Add Form section, are more likely to get converted later on.\n- The ratio of Leads converted from the Landing Page Submission and API looks okayish, however not as great as that of Lead Add Form.","152ea41a":"- We have seen that our model is producing a ROC curve with AUC of 0.97.\n- This is really good. The higher the value, the better the model.\n- Also, the Accuracy, Sensitivity and Specificity of the model are 92%, 88% and 96% respectively. ","7e8940e4":"##### **`INFERENCE`**","e7e960d7":"### Evaluating the Model","fa733ba9":"##### **`INFERENCE`** \n\n- From the boxplot we can definitely see that there are outliers in the data.\n- And on the other hand, from the histogram we can see that the data is definitely skewed. With most of the data near the 0 to 10 bin.","2d90835c":"##### **`INFERENCE`** \n\n- From the above bar graph, we can see that people with specialization in Management domains are the most common visitors. \n- However, people from Rural and Agricultural, E-Business and Services Excellence are among the least visited people.","e8c7dcdb":"##### **`INFERENCE`**","c6616221":"- When the Logistic Regression was applied to the Test dataset, we can see that the Accuracy, Sensitivity and Specificity are 92%, 88% and 96% respectively.\n- The precision score is 88%.\n- The recall score is 91%.","8489a795":"##### **`INFERENCE`** \n\n- From the graph we can understand that leads from Management, Business Administration, Banking inverstment and insurance are more likely to get converted.\n- However, people who do not mention their specialization are less likely to be converted.","eb4df958":"- When the Logistic Regression was applied to the Test dataset, we can see that the Accuracy, Sensitivity and Specificity are 93%, 93% and 92% respectively.\n- The precision score is 88%.\n- The recall score is 93%.\n\n","2f4bc59a":"### USING RECURSIVE FEATURE ELIMINATION (RFE) \n#### TO SELECT TOP 15 PREDICTOR VARIABLES ","274fa09a":"### Reading the DataFrame","82326fe2":"##### **`INFERENCE`**\n\n- From the above graph we can see that Leads who come through refrence or from Wellingak website, or any other sources are more likely to get converted.\n- Leads from Google are also quite likely to get converted. ","9f4d1eb8":"##### **`INFERENCE`** \n\n- From the above graph we can see that most of the lead had come from Google or a result of Direct Traffic (directly typing in the URL)\n- It is also worth noticing that the 3rd place is occupied by leads coming from Olark chat rather than Organic Search.","fa45987f":"##### **`INFERENCE`**\n\n- Here, we can notice that the Leads who's last activity is sending SMS are really good to target, as they are more likely to get converted.\n- However, we should avoid leads who's last activities are- Olark Chat Conversation, Email Bounced or already converted leads.","f2591ac0":"##### **`INFERENCE`**\n\n- Since the column, country is highly dominated by the value of India, it is best to leave it out of our model.\n- Dropping this column will be best.","61bd1113":"##### **`INFERENCE`**\n\n- Here we can observe that most of the audience belong are unemployed with a very tiny proportion of people who are working or studying.","2349ffb0":"##### **`INFERENCE ON THE TEST DATASET`**","f699f109":"## Data Cleaning and Visualization","367d8263":"### Importing Libraries","5d0fa1a0":"### DataFrame Inspections","889d49e7":"### Test Train Split","c44c5e13":"### MODEL NUMBER - 1","b9783e3c":"##### **`INFERENCE`**","11c5ea5e":"##### **`INFERENCE`**\n\n- If we observe the boxplot we can see that the there are definitely some outliers in the range of 250. It show that people are visiting the page for 250 times.\n- It can also be observed from the histogram that most of the visits are in the range of 0 to 25. There are very less leads who have visited the page for more than 25 times.","b56600df":"##### **`INFERENCE`**\n\n- Here, we can notice that most of the leads had opened their email to check the news about X Education.\n- Followed by sending SMS and Olark Chat conversation.\n- We can also see that a very less population of leads had either clicked on the email link or submitted the form on the website.","ce6815ac":"##### **`INFERENCE ON THE TRAIN DATASET`**","d47c4335":"##### **`INFERENCE`**","fa9e9bde":"### Preparing the Dataset for Modelling","76f145ce":"# CONCLUSION:","886f12b5":"##### **`INFERENCE`**","dfdbabc4":"### Scaling the Data","9a4cf666":"##### **`INFERENCE`**\n\n- From the bar graph we can clearly see that Leads who are working should be targetted, followed by students.\n- Unemployed leads are the worst category to target.","7776c541":"## Building the Logistic Regression Model","42e3a078":"- The model is doing a great job in prediction! \n- This model can hence, be used, to make sound business descisions.","61074795":"# LEAD SCORING CASE STUDY - (PGDDS - C28)\n\n## BY SEYED JAVIDH & VIVEK CHOWDHURY","29c1bf69":"### Finding the best Cut-Off","a9b6940c":"- From the above graph, we can make out that the optimal cut-off for our model will be 0.28.\n- This is the point where the sensitivity, accuracy and specificity co-exist.","aa567995":" - Comparing Precision, Recall and other metrics value for both train and test. Our model performs well on test set as well.\n - This model explains how exactly the Probability of conversion vary with different features. The management can accordingly manipulate the business strategy to meet the conversion target and meet the business expectations.\n - In business terms, this model can be deployed in the upcoming future to meet the X education's requirements.\n - Focusing on the features of the model will increase their chances of contacting most of the potential buyers for the course.\n - The Marketting team and evaluate the leads based on the top 3 variables and make sound business decisions.\n - The Marketting team can also chase after leads, who spend longer time on their website, orginate from Add form.\n - The team can also come with interesting courses and offers that attract people with specialization in banking, investment and insurance.\n - They can also keep a close watch on Leads originating from Olark Chat.","87f30248":" - **Tags_Closed by Horizzon has the highest coefficient of 9.1264**, which means keeping other variable constant an unit increase results in 9.1264 unit increase in Probability of conversion.\n - **Tags_Closed by Horizzon**, **Tags_Lost to EINS** and **Tags_Will revert after reading the email** are the **top 3 variables** having strong coefficients.\n - Last Activity_Olark Chat Conversation, Tags_Ringing and Last Notable Activity_Modified have **negative coeeficient**, which means increase in values of these variables would result in decrease in value of Probability of conversion.\n - Probability of conversion increases if Tags_Busy, Lead Origin_Lead Add Form, Lead Source_Welingak Website, Tags_Unknown, Last Notable Activity_SMS Sent, Lead Source_Olark Chat, Total Time Spent on Website increases as these variables have **positive coefficients**.\n - **Constant value** - when all other variables are zero the Probability of conversion value will still be **-4.3841**\n","6bc1c4ec":"##### **`INFERENCE`**\n\n- We can observe from the above bar chart that the last activity of the leads are usually- modification, email opened or sending sms.","97403c50":"##### **`INFERENCE`**\n\n- The above Bar graph concludes that most of the people are looking for Better Career Prospects.\n- But since this value is highly dominated by just one value, we can drop this column.","9d7596a9":"##### **`INFERENCE`** \n\n- Here, we can see from the boxplot that the mostly people spend about 1000 seconds on the website.\n- Also, we can see from the histogram that it is skewed and most people spend near about 500 seconds on the website.","74404388":"##### **`INFERENCE`** \n\n- We can observe that most of the leads are from the landing page and the API.","c8dd6892":"### Preparing the Test Set","58f99e90":"- But with this chart alone, we cannot decide.\n- A better way to decide will be to create a graph and plot- accuracy sensitivity and specificity for different probabilities.","27aa6d9d":"##### **`INFERENCE`**\n\n- The heatmap clearly shows us that there is a strong correlation between \"Page Views Per Visit\" and \"Total Visit\" column.\n- Similar positive correlations can be identified between \"Total Time Spent on Website\" against the \"Converted\" value.\n- There is also a positive correlation between \"Total Time Spent on Website\" with both \"Total Visit\" and \"Page Views Per Visit\".","b29fff21":"- Right off the bat, we can see that the Variation Inflation Factor of \"Last Activity_SMS Sent\" and \"Last Notable Activity_SMS Sent\". \n- This goes to show that these features have high correlation among them. \n- In this case, we will be dropping off the feature: \"Last Activity_SMS Sent\" and build the model again.","cfb07753":"##### **`INFERENCE`**\n\n- The result here is very similar to that of Last Activity performed by the Lead."}}