{"cell_type":{"3f36d794":"code","777a8fe8":"code","2a40ff69":"code","cfdf70c1":"code","b8c53212":"code","525b3daf":"code","dc847db5":"code","06d218f7":"code","89bfcf17":"code","65d7d1b4":"code","9ea14d0d":"code","2cef2bcd":"code","6db107f5":"code","b571922f":"code","40992f58":"code","79c0f225":"code","d7e3b6be":"code","6e45c964":"code","b145c4bb":"code","207aeb6e":"code","0ab2898f":"code","b775ce5b":"code","9f93c3e7":"code","9a127441":"code","96703fcf":"code","49f711b9":"code","aefcd964":"code","632e41ab":"code","d71e95d2":"code","64830fe7":"code","ac765832":"code","48e6be08":"code","c8c12e8e":"code","8eb35670":"code","36e7e9c7":"code","5711e269":"code","5b8aba36":"code","3e03f7c1":"code","277b272f":"code","195e6514":"code","3d50e458":"code","aad3049e":"code","1adae03e":"code","51ab09ad":"code","dff75f07":"code","e2073ab3":"code","6704ac70":"code","62e7fdc6":"code","1226dd90":"code","ad9b90fc":"code","21cf2088":"code","ed3076eb":"code","372dc3e9":"code","cc758386":"code","fdfdf8c8":"code","c0d3a5be":"code","e4b6cde9":"code","96a8d312":"code","d2576896":"code","019a61c0":"code","7585bd25":"code","a22e8e84":"code","978b3d1e":"code","84f9589a":"code","e97a8665":"code","eaed958f":"code","86e75301":"code","d6151981":"code","ce5ba463":"code","3a6b9f21":"code","ce65345b":"code","e1d106b3":"code","9431c812":"code","2477ae79":"code","9f482baf":"code","cb8ad919":"code","91a1d484":"code","3cb8c392":"code","67bfbf34":"code","bf8d6801":"code","16d730cc":"code","341246b5":"markdown","6feb83ff":"markdown","a7247b3f":"markdown","ed487a57":"markdown","b43164ee":"markdown","4ffdb9b0":"markdown","d515761d":"markdown","ad7b1785":"markdown","4977a21f":"markdown","5ecff17b":"markdown","9eda692a":"markdown","8d489b44":"markdown","b91e5ab2":"markdown","e518d35b":"markdown","5df11dee":"markdown","310be315":"markdown","e6a8addf":"markdown","888bcd17":"markdown","a084eef6":"markdown","dbf7f0f0":"markdown","5894a1b0":"markdown","a09ed7d3":"markdown","5f996cf5":"markdown","b6649e05":"markdown","d82b2822":"markdown","9002624b":"markdown","b28ba0aa":"markdown","f3114944":"markdown","5c55a0fc":"markdown","a67403c8":"markdown"},"source":{"3f36d794":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","777a8fe8":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2a40ff69":"# Local\n# df = pd.read_csv('Data\/Car_Insurance_Claim.csv')\n\n#Kaggle\ndf = pd.read_csv('\/kaggle\/input\/car-insurance-data\/Car_Insurance_Claim.csv')\ndf.head()","cfdf70c1":"df","b8c53212":"df.info()","525b3daf":"df.isnull().sum()","dc847db5":"plt.figure(figsize=(20,10))\nplt.title('Outcome Count')\nsns.countplot(data=df, x ='OUTCOME');","06d218f7":"df['AGE'].value_counts()","89bfcf17":"plt.figure(figsize=(20,10))\nplt.title('Age Count')\nsns.countplot(data=df, x ='AGE');","65d7d1b4":"plt.figure(figsize=(20,10))\nplt.title('Age Count colored by outcome')\nsns.countplot(data=df, x ='AGE', hue='OUTCOME');","9ea14d0d":"plt.figure(figsize=(20,10))\nplt.title('Gender Count color by outcome')\nsns.countplot(data=df, x ='GENDER', hue='OUTCOME');","2cef2bcd":"plt.figure(figsize=(20,10))\nplt.title('Race Count color by outcome')\nsns.countplot(data=df, x ='RACE', hue='OUTCOME');","6db107f5":"plt.figure(figsize=(20,10))\nplt.title('Driving Experience Count color by outcome')\nsns.countplot(data=df, x ='DRIVING_EXPERIENCE', hue='OUTCOME');","b571922f":"plt.figure(figsize=(20,10))\nplt.title('Education Count color by outcome')\nsns.countplot(data=df, x ='EDUCATION', hue='OUTCOME');","40992f58":"plt.figure(figsize=(20,10))\nplt.title('Income Count color by outcome')\nsns.countplot(data=df, x ='INCOME', hue='OUTCOME');","79c0f225":"plt.figure(figsize=(20,10))\nplt.title('Credit Score histogram')\nsns.histplot(data=df, x='CREDIT_SCORE', kde=True);","d7e3b6be":"plt.figure(figsize=(20,10))\nplt.title('Boxplot of Credit Score')\nsns.boxplot(data=df, x='CREDIT_SCORE')","6e45c964":"df[df['CREDIT_SCORE']<0.1]","b145c4bb":"df[df['CREDIT_SCORE']>0.9]","207aeb6e":"plt.figure(figsize=(20,10))\nplt.title('Vehicle ownership Score histogram')\nsns.histplot(data=df, x='VEHICLE_OWNERSHIP', kde=True);","0ab2898f":"plt.figure(figsize=(20,10))\nplt.title('Vehicle ownership Count colored by outcome')\nsns.countplot(data=df, x='VEHICLE_OWNERSHIP', hue='OUTCOME');","b775ce5b":"plt.figure(figsize=(20,10))\nplt.title('Vehicle Year Count colored by outcome')\nsns.countplot(data=df, x='VEHICLE_YEAR', hue='OUTCOME');","9f93c3e7":"plt.figure(figsize=(20,10))\nplt.title('married Count colored by outcome')\nsns.countplot(data=df, x='MARRIED', hue='OUTCOME');","9a127441":"plt.figure(figsize=(20,10))\nplt.title('Children Count colored by outcome')\nsns.countplot(data=df, x='CHILDREN', hue='OUTCOME');","96703fcf":"df.info()","49f711b9":"plt.figure(figsize=(20,10))\nplt.title('Annual Mileage Score histogram')\nsns.histplot(data=df, x='ANNUAL_MILEAGE', kde=True);","aefcd964":"plt.figure(figsize=(20,10))\nplt.title('Annual Mileage Score histogram')\nsns.histplot(data=df, x='ANNUAL_MILEAGE',hue='OUTCOME', kde=True);","632e41ab":"plt.figure(figsize=(20,10))\nplt.title('Vehicle type Count colored by outcome')\nsns.countplot(data=df, x='VEHICLE_TYPE', hue='OUTCOME');","d71e95d2":"plt.figure(figsize=(20,10))\nplt.title('Speeding Violations histogram colored by outcome')\nsns.histplot(data=df, x='SPEEDING_VIOLATIONS',hue='OUTCOME', kde=True);","64830fe7":"plt.figure(figsize=(20,10))\nplt.title('DUIS count colored by outcome')\nsns.countplot(data=df, x='DUIS',hue='OUTCOME');","ac765832":"plt.figure(figsize=(20,10))\nplt.title('Past Accidents colored by outcome')\nsns.countplot(data=df, x='PAST_ACCIDENTS',hue='OUTCOME');","48e6be08":"df.info()","c8c12e8e":"df['INCOME'].value_counts()","8eb35670":"upper_class_median = df[df['INCOME'] == 'upper class']['CREDIT_SCORE'].median()\nmiddle_class_median = df[df['INCOME'] == 'middle class']['CREDIT_SCORE'].median()\npoverty_class_median = df[df['INCOME'] == 'poverty']['CREDIT_SCORE'].median()\nworking_class_median = df[df['INCOME'] == 'working class']['CREDIT_SCORE'].median()","36e7e9c7":"df[(df['INCOME'] == 'working class') & df['CREDIT_SCORE'].isnull()].index","5711e269":"df.loc[(df[(df['INCOME'] == 'working class') & df['CREDIT_SCORE'].isnull()].index),'CREDIT_SCORE'] = df[df['INCOME'] == 'working class']['CREDIT_SCORE'].fillna(working_class_median)\ndf.loc[(df[(df['INCOME'] == 'poverty') & df['CREDIT_SCORE'].isnull()].index),'CREDIT_SCORE'] = df[df['INCOME'] == 'poverty']['CREDIT_SCORE'].fillna(poverty_class_median)\ndf.loc[(df[(df['INCOME'] == 'middle class') & df['CREDIT_SCORE'].isnull()].index),'CREDIT_SCORE'] = df[df['INCOME'] == 'middle class']['CREDIT_SCORE'].fillna(middle_class_median)\ndf.loc[(df[(df['INCOME'] == 'upper class') & df['CREDIT_SCORE'].isnull()].index),'CREDIT_SCORE'] = df[df['INCOME'] == 'upper class']['CREDIT_SCORE'].fillna(upper_class_median)","5b8aba36":"df.info()","3e03f7c1":"df['ANNUAL_MILEAGE'] = df['ANNUAL_MILEAGE'].fillna(df['ANNUAL_MILEAGE'].median())","277b272f":"df.info()","195e6514":"df.describe()","3d50e458":"X = df.drop(['OUTCOME','ID'], axis=1)\ny = df['OUTCOME']\nX = pd.get_dummies(X, drop_first=True)","aad3049e":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","1adae03e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","51ab09ad":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","dff75f07":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier","e2073ab3":"from warnings import filterwarnings\nfilterwarnings('ignore')","6704ac70":"from sklearn.metrics import classification_report,precision_score, recall_score,f1_score","62e7fdc6":"def fit_and_score(models, X_train, X_test, y_train, y_test):\n    np.random.seed(42)\n    \n    model_scores = {}\n    model_recall = {}\n    model_f1 = {}\n    model_precision = {}\n    \n    for name, model in models.items():\n        model.fit(X_train,y_train)\n        y_preds = model.predict(X_test)\n        print(name)\n        print(classification_report(y_test, y_preds))\n        print('\\n')\n        model_scores[name] = model.score(X_test,y_test)\n        model_recall[name] = recall_score(y_test, y_preds)\n        model_f1[name] = f1_score(y_test, y_preds)\n        model_precision[name] = precision_score(y_test, y_preds)\n\n    model_scores = pd.DataFrame(model_scores, index=['Score']).transpose()\n    model_scores = model_scores.sort_values('Score')\n    model_recall = pd.DataFrame(model_recall, index=['Recall']).transpose()\n    model_recall = model_recall.sort_values('Recall')\n    model_f1 = pd.DataFrame(model_f1, index=['F1']).transpose()\n    model_f1 = model_f1.sort_values('F1')\n    model_precision = pd.DataFrame(model_precision, index=['Precision']).transpose()\n    model_precision = model_precision.sort_values('Precision')\n        \n    return model_scores, model_recall, model_f1, model_precision","1226dd90":"models = {'LogisticRegression': LogisticRegression(max_iter=10000),\n          'KNeighborsClassifier': KNeighborsClassifier(),\n          'SVC': SVC(),\n          'DecisionTreeClassifier': DecisionTreeClassifier(),\n          'RandomForestClassifier': RandomForestClassifier(),\n          'AdaBoostClassifier': AdaBoostClassifier(),\n          'GradientBoostingClassifier': GradientBoostingClassifier(),\n          'XGBClassifier': XGBClassifier(objective='binary:logistic',eval_metric=['logloss']),\n          'XGBRFClassifier': XGBRFClassifier(objective='binary:logistic',eval_metric=['logloss']),\n          'LGBMClassifier':LGBMClassifier(),\n         'CatBoostClassifier': CatBoostClassifier(verbose=0)}","ad9b90fc":"model_scores, model_recall, model_f1, model_precision = fit_and_score(models, X_train, X_test, y_train, y_test)","21cf2088":"model_scores","ed3076eb":"model_recall","372dc3e9":"model_f1","cc758386":"model_precision","fdfdf8c8":"from sklearn.model_selection import RandomizedSearchCV","c0d3a5be":"def randomsearch_cv_scores(models, params, X_train, X_test, y_train, y_test):\n    np.random.seed(42)\n    \n    model_rs_scores = {}\n    model_rs_best_param = {}\n    \n    for name, model in models.items():\n        rs_model = RandomizedSearchCV(model,\n                                     param_distributions=params[name],\n                                      scoring='f1',\n                                      cv=5,\n                                     n_iter=40,\n                                     verbose=0)        \n        rs_model.fit(X_train,y_train)\n        model_rs_scores[name] = rs_model.score(X_test,y_test)\n        model_rs_best_param[name] = rs_model.best_params_\n        y_preds = rs_model.predict(X_test)\n        print('\\n')\n        print(name)\n        print(classification_report(y_test, y_preds))\n        print('\\n')\n        \n        \n    return model_rs_scores, model_rs_best_param","e4b6cde9":"models = {'LGBMClassifier': LGBMClassifier()}\n\nparams = {'LGBMClassifier':{}}","96a8d312":"model_rs_scores_base, model_rs_best_param_base = randomsearch_cv_scores(models, params, X_train, X_test, y_train, y_test)","d2576896":"params = {'LGBMClassifier':{'num_leaves': np.arange(21,42,2),\n                           'learning_rate': np.linspace(0.1,0.9,9),\n                            'n_estimators':[50,100,200,300,500],\n                            'min_split_gain':np.linspace(0.0,0.9,10),\n                            'min_child_weight':np.linspace(0.0,0.9,10),\n                            'min_child_samples': [10,20,40,80,100],\n                            'reg_alpha': np.linspace(0.0,0.9,10),\n                            'reg_lambda': np.linspace(0.0,0.9,10)\n                           }\n         }","019a61c0":"model_rs_scores1, model_rs_best_param1 = randomsearch_cv_scores(models, params, X_train, X_test, y_train, y_test)","7585bd25":"model_rs_scores1","a22e8e84":"model_rs_best_param1","978b3d1e":"params = {'LGBMClassifier':{'num_leaves': np.arange(30,43),\n                           'learning_rate': np.linspace(0.0,0.2,9),\n                            'n_estimators':[250,300,250],\n                            'min_split_gain':np.linspace(0.7,0.9,10),\n                            'min_child_weight':np.linspace(0.0,0.1,10),\n                            'min_child_samples': [5,10,20,30],\n                            'reg_alpha': np.linspace(0.4,0.6,10),\n                            'reg_lambda': np.linspace(0.4,1.6,10)\n                           }\n         }","84f9589a":"model_rs_scores2, model_rs_best_param2 = randomsearch_cv_scores(models, params, X_train, X_test, y_train, y_test)","e97a8665":"model_rs_scores2","eaed958f":"model_rs_best_param2","86e75301":"params = {'LGBMClassifier':{'num_leaves': [41],\n                           'learning_rate': np.linspace(0.001,0.006,9),\n                            'n_estimators':[290,300,310],\n                            'min_split_gain':np.linspace(0.8,0.9,10),\n                            'min_child_weight':[0.05555555555555556],\n                            'min_child_samples': [3,4,5,6,7,8],\n                            'reg_alpha': [0.5333333333333333],\n                            'reg_lambda': [1.6]\n                           }\n         }","d6151981":"model_rs_scores3, model_rs_best_param3 = randomsearch_cv_scores(models, params, X_train, X_test, y_train, y_test)","ce5ba463":"model_rs_scores3","3a6b9f21":"model_rs_best_param3","ce65345b":"model_rs_scores2","e1d106b3":"model_rs_best_param2","9431c812":"from sklearn.metrics import classification_report, plot_confusion_matrix,plot_roc_curve\nfrom sklearn.model_selection import cross_val_score","2477ae79":"model = LGBMClassifier(reg_lambda = 1.6,\n                      reg_alpha = 0.5333333333333333,\n                      num_leaves = 41,\n                      n_estimators = 300,\n                      min_split_gain = 0.8555555555555556,\n                      min_child_weight = 0.05555555555555556,\n                      min_child_samples = 5,\n                      learning_rate = 0.05)","9f482baf":"model.fit(X_train, y_train)\ny_preds = model.predict(X_test)","cb8ad919":"print(classification_report(y_test, y_preds))","91a1d484":"plot_confusion_matrix(model, X_test,y_test)","3cb8c392":"plot_roc_curve(model, X_test,y_test)","67bfbf34":"def get_cv_score(model, X, y, cv=5):\n    \n    \n    cv_accuracy = cross_val_score(model,X,y,cv=cv,\n                         scoring='accuracy')\n    print(f'Cross Validaion accuracy Scores: {cv_accuracy}')\n    print(f'Cross Validation accuracy Mean Score: {cv_accuracy.mean()}')\n    \n    cv_precision = cross_val_score(model,X,y,cv=cv,\n                         scoring='precision')\n    print(f'Cross Validaion precision Scores: {cv_precision}')\n    print(f'Cross Validation precision Mean Score: {cv_precision.mean()}')\n    \n    cv_recall = cross_val_score(model,X,y,cv=cv,\n                         scoring='recall')\n    print(f'Cross Validaion recall Scores: {cv_recall}')\n    print(f'Cross Validation recall Mean Score: {cv_recall.mean()}')\n    \n    cv_f1 = cross_val_score(model,X,y,cv=cv,\n                         scoring='f1')\n    print(f'Cross Validaion f1 Scores: {cv_f1}')\n    print(f'Cross Validation f1 Mean Score: {cv_f1.mean()}')   \n    \n    cv_merics = pd.DataFrame({'Accuracy': cv_accuracy.mean(),\n                         'Precision': cv_precision.mean(),\n                         'Recall': cv_recall.mean(),\n                         'f1': cv_recall.mean()},index=[0])\n    \n    return cv_merics","bf8d6801":"cv_merics = get_cv_score(model, X_train, y_train, cv=5)","16d730cc":"cv_merics","341246b5":"## Model imports","6feb83ff":"we will use the income group to fill the nan for the credit scores","a7247b3f":"As we can see there are some outlier in the credit scores, however we dont think i will effect the overall model","ed487a57":"## Baseline Model Scores","b43164ee":"## Data Exporation","4ffdb9b0":"## Evalution using Cross-Validation","d515761d":"# 2. Data\n\nData from: https:\/\/www.kaggle.com\/sagnik1511\/car-insurance-data\n\n## Context\n\nThe company has shared its annual car insurance data. Now, you have to find out the real customer behaviors over the data.\n\n## Content\n\nThe columns are resembling practical world features.\nThe outcome column indicates 1 if a customer has claimed his\/her loan else 0.\nThe data has 19 features from there 18 of them are corresponding logs which were taken by the company.\n\n## Acknowledgements\n\nMostly the data is real and some part of it is also generated by Sagnik Roy.","ad7b1785":"## RS Model 2","4977a21f":"## Confusion Matirx","5ecff17b":"## Baseline CV scores","9eda692a":"from the count, we can see that the data is in-balanced.","8d489b44":"## RS Model 1","b91e5ab2":"# 4. Features\n\n## Input\/Features\n\n    1. ID\n    2. AGE\n    3. GENDER\n    4. RACE\n    5. DRIVING_EXPERIENCE\n    6. EDUCATION\n    7. INCOME\n    8. CREDIT_SCORE\n    9. VEHICLE_OWNERSHIP\n    10. VEHICLE_YEAR\n    11. MARRIED\n    12. CHILDREN\n    13. POSTAL_CODE\n    14. ANNUAL_MILEAGE\n    15. VEHICLE_TYPE\n    16. SPEEDING_VIOLATIONS\n    17 .DUIS\n    18.PAST_ACCIDENTS\n    \n## Outputs\/Labels\n    \n    19. OUTCOME","e518d35b":"## Random Search CV","5df11dee":"Since the labels are in-balanced, We will choose to use the LGBMClassifier as it provides the best overall scores. we will do a Randome Search CV to find the optimized hyper parameters","310be315":"We will use RS model 2 as that provides the best hyperparameters","e6a8addf":"we will use the median ANNUAL_MILEAGE to fill the the nan values for ANNUAL_MILEAGE","888bcd17":"# Car Insurance Claims Classification","a084eef6":"# 1. Problem Definition\n\nHow we can use various python based Machine Learning Model and the given parameters to predict if a claim was made?","dbf7f0f0":"# 7. Experimentation \/ Improvements\n\nwith a scoring model of Recall 76% and f1 of 76% in the CV and classification, we hope to get a better scoring model.\n\nmaybe we can look into the follow for improvements:\n\n    1. Check for other outliers? or other ways to fill nan values? or dropping the nan instead of filling them?\n    2. Build and looking in to the dataset again to build a better model\n    3. Getting more data to balance out the labels?","5894a1b0":"# 5. Modelling","a09ed7d3":"## Classification Report","5f996cf5":"## ROC Curve","b6649e05":"# 3. Evaluation\n\nAs this is a classification problem, we will use the classification metics for evauluting the model","d82b2822":"with the model, and with the CV evalution, we are able to get the following:\n\n    Accuracy 0.850714\n    Precision 0.762139\n    Recall 0.762747\n    f1 0.762747","9002624b":"## RS Model 3","b28ba0aa":"Going to take the following approach:\n\n1. Problem definition\n2. Data\n3. Evaluation\n4. Features\n5. Modelling\n6. Model Evaluation\n7. Experimentation \/ Improvements","f3114944":"## Reading the Dataset","5c55a0fc":"# 6. Model Evalution","a67403c8":"## Standard Imports"}}