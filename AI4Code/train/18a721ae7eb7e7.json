{"cell_type":{"58219c3e":"code","dd8c2f00":"code","ef583544":"code","1641aeda":"code","3948205b":"code","c795f6b6":"code","bbde748c":"code","493cfca9":"code","3210db5e":"code","e84486ab":"code","c962756a":"code","3ad31b68":"code","aadf356c":"code","aa65e46c":"code","a1c578ba":"code","318a1ca8":"code","d7dd8f56":"code","8f48c8ff":"code","373dbf10":"code","6d41403e":"code","2c571f39":"code","9d2e0ec7":"code","9bb31397":"code","afb2bdc1":"code","d248c304":"code","b368ffa7":"code","eee9a2f7":"code","ef81a22c":"code","a745397f":"code","4750830c":"code","c334f3e7":"code","231ab313":"code","9a3f9b4c":"code","44c634aa":"code","93496914":"code","707272b1":"code","eee3e05f":"code","3a0c91a3":"code","86834492":"code","4e368aa0":"code","61419534":"code","4c158c72":"code","f77c25d8":"code","d331d531":"code","d68aaf90":"code","1bd1669d":"code","b4623171":"code","22568ee2":"code","85b01cf0":"code","99bd034d":"code","1f69b117":"code","2d190b0f":"code","871a9660":"code","10dd7702":"code","d9925257":"code","983d79e8":"code","519a0b09":"code","a95c6edb":"code","3bad2e28":"code","45df85c8":"code","7341623e":"code","f39292b6":"code","6b6f75fa":"code","d3edf6f6":"code","5553eed9":"code","5ed2be7b":"code","c6032c99":"code","9ce8c981":"code","8690bde5":"code","4898f54d":"code","65e6a7ce":"code","5af9d403":"code","7dbfdca2":"code","b13e9477":"code","1d0f4ea4":"code","c5f4f0e6":"code","b0564a28":"code","d1fe3e4e":"code","bc0aff25":"code","78ef3874":"code","bb9b0fe2":"code","bb950d3c":"code","937b5e1e":"code","830491ad":"code","532ac411":"code","165ea36d":"code","4d0ba01b":"code","7a6657c6":"code","be10e8ae":"code","49e6f9ba":"code","6169f858":"code","ff2f32e9":"code","9a8dd671":"code","d4285ee2":"code","3d0ae0b2":"code","0d7c66f3":"code","b27ffc47":"code","b0e16a5e":"code","0bc1a128":"code","ffa0f6c3":"code","7330efb4":"code","63563f9b":"code","cc802f02":"code","4844706b":"code","d393adc7":"code","235ade23":"code","a79ab621":"code","0c40118f":"code","035d6ce4":"code","0f7bfa29":"code","a9a8427c":"code","12cbbb7e":"code","647da010":"code","102621ca":"code","95d2548c":"code","2afc801b":"code","3e97da44":"code","b6f8ed08":"code","a33a6d5a":"code","a79a8f6d":"code","3d9cebd5":"code","97eed990":"code","9a4b624c":"code","67f0cc1f":"code","c2340fbf":"code","4ac9e6fb":"code","47b903ce":"code","3e74876f":"code","4baf2ff1":"code","9a77d76d":"code","3c128f29":"code","024e3365":"code","31e45494":"code","ca523c3b":"code","cc1ca431":"code","61d8d80d":"code","5bb200a1":"code","8e92982d":"code","fdfac354":"code","3a66a7f6":"code","4e7d1369":"code","798302f8":"markdown","209dc159":"markdown","6d196b49":"markdown","e0ce81a1":"markdown","15583f22":"markdown","ec9cba30":"markdown","28352608":"markdown","6f77b842":"markdown","27d9c8cc":"markdown","7ed2f309":"markdown","2964eb7c":"markdown","ea8903d7":"markdown","1ff50e00":"markdown","31178d04":"markdown","fbfe4e85":"markdown"},"source":{"58219c3e":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom datetime import datetime\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.model_selection import GridSearchCV,train_test_split,cross_val_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_curve, auc\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nprint(os.listdir(\"..\/input\"))\n\n\ndata=pd.read_csv('..\/input\/2015.csv')","dd8c2f00":"print(\"Data Head So First 5 rows :\\n\",(data.head()))\n\nprint(\"Data Tail So Last 5 rows :\\n\",(data.tail()))         ","ef583544":"data.sample(5)","1641aeda":"print(\"Data Info:\\n\",(data.info()))","3948205b":"print(\"Data Describe:\\n\",(data.describe()))","c795f6b6":"print('Country Counts unique')\ncountries=data.Country.unique()\nfor country in countries:\n    print(country)","bbde748c":"print(\"Country Counts :\\n\")\nprint(data['Country'].value_counts())","493cfca9":"data.dtypes","3210db5e":"pd.isnull(data).sum()","e84486ab":"data.dropna(how='any',axis='rows')\n# null at any point and should be deleted.","c962756a":"#show random rows in dataset\ndata.sample(5)","3ad31b68":"print('Regions unqiue:\\n')\nregions=data.Region.unique()\nfor reg in regions:\n    print(reg)","aadf356c":"pd.isna(data['Region']).count()","aa65e46c":"for col in data.columns:\n    print(data[data[col].isnull()])","a1c578ba":"print('Regions Counts:\\n')\nprint(data['Region'].value_counts())","318a1ca8":"data.isnull().values.any()","d7dd8f56":"data_filter=data.iloc[:,1:9]\ndata_filter.columns\n#Incomplete detection will be performed.\nfor i, col in enumerate(data_filter.columns.values):\n    plt.subplot(4, 2, i+1)\n    plt.scatter(np.arange(1,159), data[col].values.tolist())\n    plt.title(col)\n    fig, ax = plt.gcf(), plt.gca()\n    fig.set_size_inches(10, 10)\n    plt.tight_layout()\nplt.show()","8f48c8ff":"data.head()","373dbf10":"data_region=data['Region'].value_counts()\ndata_rvalues=data_region.values\ndata_rregion=data_region.index","6d41403e":"\nplt.figure(figsize=(10,10))\nsns.barplot(x=data_rregion,y=data_rvalues)\nplt.xticks(rotation=90)\nplt.xlabel('Region')\nplt.ylabel('Values')\nplt.title('Region VS Values')\nplt.show()","2c571f39":"sns.countplot(data.Region)\nplt.title('Region Count System')\nplt.show()","9d2e0ec7":"sns.swarmplot(data.Region)\nplt.show()","9bb31397":"data.head()","afb2bdc1":"plt.figure(figsize=(10,10))\nax=sns.barplot(x=data_rregion,y=data_rvalues,palette=sns.cubehelix_palette(len(data_rregion)))\nplt.xlabel('Regions')\nplt.ylabel('Values')\nplt.xticks(rotation=90)\nplt.title('Most Common Region of Happy')\nplt.show()","d248c304":"# Draw a violinplot with a narrower bandwidth than the default\nsns.violinplot(data=data.corr(), palette=\"Set3\", bw=.2, cut=1, linewidth=1)\n\n# Finalize the figure\nax.set(ylim=(-.7, 1.05))\nsns.despine(left=True, bottom=True)\nplt.xticks(rotation=90)\nplt.show()","b368ffa7":"data.columns","eee9a2f7":"plt.figure(figsize=(10,10))\nplt.scatter(data['Economy (GDP per Capita)'], data['Freedom'], s=(data['Happiness Score']**3), alpha=0.5)\nplt.grid(True)\n\nplt.xlabel(\"Economy\")\nplt.ylabel(\"Freedom\")\n\nplt.suptitle(\"Health Economy graph with sizes as Happiness score and colors as Region\", fontsize=18)\n\nplt.show()","ef81a22c":"data.columns=['Country','Region','Happiness_Rank','Happiness_Score','Standart_Error','Economy_GPD_Capital','Family','Healt_Life_Expectancy','Freedom','Trust_Goverment_Corruption','Generosity','Dystopia_Residual']","a745397f":"data['Happiness_Rank'].unique()","4750830c":"values_region=data.Region.value_counts().values\nvalues_region","c334f3e7":"Economy_GPD_Capital=[]\nfor i,region in enumerate(data.Region.value_counts().index):\n    Economy_GPD_Capital.append(sum(data[data['Region']==region].Economy_GPD_Capital)\/values_region[i])","231ab313":"Economy_GPD_Capital","9a3f9b4c":"data.head()","44c634aa":"cmap = sns.cubehelix_palette(rot=-.2, as_cmap=True)\nax = sns.scatterplot(x=\"Happiness_Score\", y=\"Standart_Error\",\n                     palette=cmap, sizes=(10, 200),\n                     data=data)\nplt.show()","93496914":"# Pie Chart\ncolors = ['grey','blue','red','yellow','green','brown','lime','pink','orange','purple']\nexplode = [0,0.4,0,0.1,0,0,0,0,0,0.2]\nplt.figure(figsize = (7,7))\nplt.pie(Economy_GPD_Capital, explode=explode, labels=data['Region'].unique(), colors=colors, autopct='%1.1f%%')\nplt.title('Regions Rates for Economy GPD',color = 'blue',fontsize = 15)\nplt.show()","707272b1":"ayrimcol=[]\n\nfor col in data.columns:\n    print(col)\n    ayrimcol.append(col.split())","eee3e05f":"data.columns=['Country','Region','Happiness_Rank','Happiness_Score','Standart_Error','Economy_GPD_Capital','Family','Healt_Life_Expectancy','Freedom','Trust_Goverment_Corruption','Generosity','Dystopia_Residual']\n\nhappiess_ranks=[]\nhappiess_scores=[]\nprint('----------------------------------------------------------------------')\nfor i in data.Region.unique():\n    happiess_ranks.append(sum(data[data['Region']==i].Happiness_Rank))\n    happiess_scores.append(sum(data[data['Region']==i].Happiness_Score))\n\nf,ax=plt.subplots(figsize=(10,10))\nsns.barplot(y=data_rregion,x=happiess_ranks,color='green',alpha=0.5,label='Ranks')\nsns.barplot(y=data_rregion,x=happiess_scores,color='red',alpha=0.7,label='Scores')\nax.legend(loc='lower right',frameon=True)\nax.set(xlabel='Happy of Region',ylabel='Happy of Rate',title='Ranks VS Scores')\nplt.xticks(rotation=90)\nplt.show()","3a0c91a3":"data[data['Freedom']==0]\n#so,the country with the lowest rate of freedom and the country with the highest","86834492":"data[data['Freedom']>0.66]\n#In addition, it seems to be the best country in Switzerland, both health and free.","4e368aa0":"#Now, according to the regions, the result of the vote of confidence vote in the government will be shown.\ndata.groupby('Region')['Trust_Goverment_Corruption'].mean()","61419534":"pal=sns.cubehelix_palette(2,rot=.5,dark=.3)\nsns.violinplot(data=data.groupby('Region')['Trust_Goverment_Corruption'].count().values, palette=pal, inner=\"points\",color='b')\n#sns.violinplot(data=data.groupby('Region')['Freedom'].count().values, palette=pal, inner=\"points\",color='r')\nplt.title('Region State for Trust_Goverment_Corruption ')\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.show()","4c158c72":"sns.pairplot(data)\nplt.show()","f77c25d8":"g = sns.PairGrid(data, diag_sharey=False)\ng.map_lower(sns.kdeplot)\ng.map_upper(sns.scatterplot)\ng.map_diag(sns.kdeplot, lw=3)\nplt.show()","d331d531":"data.head()","d68aaf90":"sns.set(style=\"whitegrid\")\nax = sns.boxplot(x=data[\"Trust_Goverment_Corruption\"])\nplt.show()","1bd1669d":"ax = sns.boxplot(y=\"Happiness_Score\", x=\"Economy_GPD_Capital\",data=data, linewidth=2.5)\nplt.title('Happiness Score vs Economy GDP Capital')\nplt.show()","b4623171":"sns.set(style=\"whitegrid\")\nax = sns.boxplot(x=data[\"Happiness_Score\"])\nplt.show()","22568ee2":"# Pie Chart\ncolors = ['grey','blue','red','yellow','green','brown','lime','pink','orange','purple']\nexplode = [0,0,0,0,0,0,0,0,0,0]\nplt.figure(figsize = (7,7))\nplt.pie(data.groupby('Region')['Trust_Goverment_Corruption'].mean().values, explode=explode, labels=data.groupby('Region')['Trust_Goverment_Corruption'].mean().index, colors=colors, autopct='%1.1f%%')\nplt.title('Region vs Trust Goverment Corr',color = 'blue',fontsize = 15)\nplt.show()","85b01cf0":"generosity=data.sort_values(by=\"Generosity\",ascending=\"True\")[:20].reset_index()\ngenerosity=generosity.drop('index',axis=1)","99bd034d":"#we will now analyze the rates and situations of generosity relative to countries.\ngenerosity","1f69b117":"sns.barplot(x=generosity.Region.value_counts().index,y=generosity.Region.value_counts().values)\nplt.xlabel(\"Region\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=90)\nplt.title(\"Top 20 most generous region rates\")\nplt.show()","2d190b0f":"f,ax1=plt.subplots(figsize=(10,10))\nsns.pointplot(x=data_rregion,y=happiess_ranks,data=data,color='lime',alpha=0.8)\nsns.pointplot(x=data_rregion,y=happiess_scores,data=data,color='red',alpha=0.8)\nplt.xlabel('Region',fontsize=15,color='blue')\nplt.ylabel('Rates',fontsize=15,color='blue')\nplt.title('Region VS Rank and Scores Rates',fontsize=20,color='blue')\nplt.xticks(rotation=90)\nplt.grid()\nplt.show()","871a9660":"colors = ['grey','blue','red','yellow','green','brown','lime','pink','orange','purple']\nexplode = [0,0,0,0,0,0,0,0,0,0]\nplt.figure(figsize = (7,7))\nplt.pie(data_rvalues, explode=explode, labels=data_rregion, colors=colors, autopct='%1.1f%%')\nplt.title('Regions Rates',color = 'blue',fontsize = 15)\nplt.show()","10dd7702":"print(data_rvalues)\nprint(len(data_rregion))","d9925257":"turst_goverment_corr=[]\n\nfor col in data.Region.unique():\n    turst_goverment_corr.append(sum(data[data['Region']==col].Trust_Goverment_Corruption))\n    \n\nplt.figure(figsize=(10,10))\nsns.barplot(x=data_rregion,y=turst_goverment_corr)\nplt.xticks(rotation=90)\nplt.xlabel('Regions')\nplt.ylabel('Goverment Corruptions')\nplt.title('Corruption Rates')\nplt.show()","983d79e8":"plt.figure(figsize=(7,7))\nplt.pie(turst_goverment_corr,explode=explode,labels=data_rregion,colors=colors,autopct='%1.1f%%')\nplt.title('Region VS Goverment Corruption',color='blue',fontsize=15)\nplt.show()","519a0b09":"generosity=[]\n\nfor col in data.Region.unique():\n    generosity.append(sum(data[data['Region']==col].Generosity))\n\nplt.figure(figsize=(10,10))\nsns.barplot(x=data_rregion,y=generosity)\nplt.xticks(rotation=45)\nplt.xlabel('Region')\nplt.ylabel('Generosity Rates')\nplt.title('Region VS Generosity Rates')\nplt.show()","a95c6edb":"min_d=generosity[0]\nmax_d=generosity[0]\ni=0\nmin_i=0\nmax_i=0\nfor d in generosity:\n    if min_d>generosity[i]:\n        min_d=generosity[i]\n        min_i=i\n    elif max_d<generosity[i]:\n        max_d=generosity[i]\n        max_i=i\n    i=i+1\n\nprint(data_rregion[0])\nprint('The most generosity Rates :'+(data_rregion[max_i]))\nprint('The less generosity Rates :'+(data_rregion[min_i]))","3bad2e28":"freedom=[]\nfor c in data.Region.unique():\n    freedom.append(sum(data[data['Region']==c].Freedom))\n\nplt.figure(figsize=(10,10))\nsns.barplot(x=data_rregion,y=freedom)\nplt.xticks(rotation=90)\nplt.xlabel('Reigon')\nplt.ylabel('Freedom Rate')\nplt.title('Region VS Freedom Rates')\nplt.show()","45df85c8":"f2,ax2=plt.subplots(figsize=(9,15))\nsns.barplot(x=freedom,y=data_rregion,label='Freedom',color='green',alpha=0.5)\nsns.barplot(x=generosity,y=data_rregion,label='Generosity',color='red',alpha=0.7)\nsns.barplot(x=turst_goverment_corr,y=data_rregion,label='Trust Goverment Corr',color='blue',alpha=0.9)\nax2.legend(loc='lower right',frameon = True)\nax2.set(xlabel='Percentage of Rates', ylabel='Region',title = \"Rates vs Frequency\")\nplt.show()","7341623e":"economy_gpd_capital=[]\nfor c in data.Region.unique():\n    economy_gpd_capital.append(sum(data[data['Region']==c].Economy_GPD_Capital))\n\n\neconomy_gpd_capital.sort(reverse=True)\n\nplt.figure(figsize=(10,10))\nsns.barplot(x=data_rregion,y=economy_gpd_capital)\nplt.xticks(rotation=90)\nplt.xlabel('Region')\nplt.ylabel('Economy GPD Region')\nplt.title('Region VS Economy')\nplt.show()    \n","f39292b6":"sns.countplot(data.Region)\nplt.title('Data Region Counts')\nplt.xticks(rotation=90)\nplt.show() \n","6b6f75fa":"data.groupby('Region')[['Healt_Life_Expectancy','Freedom','Trust_Goverment_Corruption','Generosity']].mean()\n#It seems that Australia and New Zealand seem to be the most appropriate region.","d3edf6f6":"data.head()","5553eed9":"for i,col in enumerate(data.columns[3:]):\n    plt.subplot(3,3,i+1)\n    ax = sns.distplot(data[col], rug=True, hist=False)\n    plt.title(col)\n    fig, ax = plt.gcf(), plt.gca()\n    fig.set_size_inches(10, 10)\n    plt.tight_layout()\nplt.show()","5ed2be7b":"f, ax = plt.subplots(figsize=(6.5, 6.5))\nsns.despine(f, left=True, bottom=True)\nsns.scatterplot(x=\"Generosity\", y=\"Freedom\",data=data)\nplt.show()","c6032c99":"#        columns=['a','b','c','d','e','f'])\n\nregions=pd.DataFrame(data.Region.unique(),index=range(10),columns=['Region'])\neconomy=pd.DataFrame(economy_gpd_capital,index=range(10),columns=['Economy'])\nfreedoms=pd.DataFrame(freedom,index=range(10),columns=['Freedom'])\ngenerositys=pd.DataFrame(generosity,index=range(10),columns=['Generosity'])\nhappiess_rankss=pd.DataFrame(happiess_ranks,index=range(10),columns=['Happy Ranks'])\nhappiess_scoress=pd.DataFrame(happiess_scores,index=range(10),columns=['Happy Scores'])\n\nnew_data=pd.concat([regions,economy,freedoms,generositys,happiess_rankss,happiess_scoress],axis=1)\n\nsns.pairplot(new_data)\nplt.show()","9ce8c981":"new_data.head()","8690bde5":"sns.boxplot(x='Region',y='Dystopia_Residual',data=data,palette='PRGn')\nplt.xticks(rotation=90)\nplt.show()\n","4898f54d":"print(new_data.Region)","65e6a7ce":"data.head()","5af9d403":"sns.heatmap(data.iloc[:,2:].corr())\nplt.show()","7dbfdca2":"data.isna().values.any()","b13e9477":"data.isna().sum()","1d0f4ea4":"data.isnull().sum()","c5f4f0e6":"Happiness_Score=[]\nfor region in data.Region.unique():\n    Happiness_Score.append(sum(data[data['Region']==region].Happiness_Score))\n    \nfig, ax = plt.subplots()\nax.scatter(x = data['Region'].unique(), y =Happiness_Score)\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.xticks(rotation=90)\nplt.show()","b0564a28":"data.Region.value_counts()\nplt.figure(figsize=(5,5))\nplt.scatter(x=data[data['Region']==\"Sub-Saharan Africa\"].Economy_GPD_Capital,y=data[data['Region']==\"Sub-Saharan Africa\"].Happiness_Score)\nplt.show()","d1fe3e4e":"plt.subplots(figsize=(12, 8))\ntop_corr = data[abs(data['Happiness_Rank']>6.5)].corr()\nsns.heatmap(top_corr, annot=True)\nplt.show()","bc0aff25":"data.columns","78ef3874":"for i,col in enumerate(data.columns[2:]):\n    plt.subplot(5,2 ,i+1)\n    sns.distplot(data[col])\n    plt.title(col)\n    fig, ax = plt.gcf(), plt.gca()\n    fig.set_size_inches(10, 10)\n    plt.tight_layout()\nplt.show()","bb9b0fe2":"data.columns","bb950d3c":"plt.figure(figsize=(14,4))\n\nplt.subplot(1,3,1)\nsns.barplot(x = 'Happiness_Rank', y = 'Healt_Life_Expectancy', data = data[:100])\nplt.xticks(rotation=90)\n\nplt.subplot(1,3,2)\nsns.barplot(x = 'Happiness_Rank', y = 'Healt_Life_Expectancy', data = data[:100])\nplt.xticks(rotation=90)\n\nplt.subplot(1,3,3)\nsns.barplot(x = 'Happiness_Rank', y = 'Healt_Life_Expectancy', data = data[:100])\nplt.xticks(rotation=90)\n\nplt.tight_layout()\nplt.show()","937b5e1e":"sns.heatmap(data.corr(), annot = True, cmap='inferno')\nplt.show()","830491ad":"plt.scatter(x=np.arange(1,159),y=data['Freedom'],color='r')\nplt.scatter(x=np.arange(1,159),y=data['Economy_GPD_Capital'],color='b')\nplt.title('Freedom vs Economy_GPD_Capital')\nplt.show()","532ac411":"plt.figure(figsize=(8,5))\nsns.distplot(data['Freedom'], kde = False, color='m', bins = 30)\nplt.ylabel('Frequency')\nplt.title('Freedom Distribution')\nplt.show()","165ea36d":"plt.figure(figsize=(8,5))\nsns.distplot(data['Economy_GPD_Capital'], kde = False, color='r', bins = 30)\nplt.ylabel('Frequency')\nplt.title('Reading Score Distribution')\nplt.show()","4d0ba01b":"plt.scatter(x=data['Freedom'],y=data['Happiness_Score'],color='r')\nplt.scatter(x=data['Freedom'],y=data['Economy_GPD_Capital'],color='b')\nplt.show()","7a6657c6":"data.columns[2:]","be10e8ae":"plt.scatter(x=data['Trust_Goverment_Corruption'],y=data['Happiness_Score'],color='r')\nplt.scatter(x=data['Trust_Goverment_Corruption'],y=data['Economy_GPD_Capital'],color='b')\nplt.scatter(x=data['Trust_Goverment_Corruption'],y=data['Generosity'],color='y')\nplt.xlabel('Trust_Goverment_Corruption')\nplt.ylabel('Happiness_Score , Economy_GPD_Capital , Generosity')\nplt.show()","49e6f9ba":"#There is no need for Country value. This is because there is only one order of a value and a value.\ndata=data.drop('Country',axis=1)","6169f858":"data.head()","ff2f32e9":"#There is more than one region value. For this, we need to make all the regions sorted.\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ndata['Region']=le.fit_transform(data['Region'])","9a8dd671":"data.head()","d4285ee2":"#Happiness Rank value Because it is an ordered value, we need to delete it.\ndata=data.drop('Happiness_Rank',axis=1)","3d0ae0b2":"#I'll create a value now. For this value we will create an index value among themselves. For this value, 75% will be treated as 1 and 0 will be treated as 0. This value is the value of Healt_Life_Expectancy.\ndata['Healt_Life_Expectancy']=[1 if healt_value>0.75 else 0 for healt_value in data.Healt_Life_Expectancy]","0d7c66f3":"# value>0.75---->1  value<0.75------>0 \ndata.Healt_Life_Expectancy.value_counts()\nsns.countplot(data.Healt_Life_Expectancy)\nplt.show()","b27ffc47":"data_healt_life=data['Healt_Life_Expectancy']\ndata=data.drop('Healt_Life_Expectancy',axis=1)","b0e16a5e":"row_id=1\nfor col in data.columns:\n    if col!='Region':\n        plt.subplot(4,2,row_id)\n        row_id=row_id+1\n        plt.title(col)\n        plt.scatter(x=np.arange(1,159),y=data[col],color='b')\n        fig, ax = plt.gcf(), plt.gca()\n        fig.set_size_inches(10, 10)\n        plt.tight_layout()\n        plt.show()\n\n#As can be seen, the operations at the bottom are outlier data.\n#There are outlier data in a number of regions. It takes this kind of process to better analyze the data.","0bc1a128":"data.columns","ffa0f6c3":"len(data[(data['Region']==8)].sort_values(by='Economy_GPD_Capital',ascending=False).iloc[:,3].values)","7330efb4":"sns.barplot(np.arange(1,41),data[(data['Region']==8)].sort_values(by='Economy_GPD_Capital',ascending=False).iloc[:,3].values)\nplt.xticks(rotation=90)\nplt.xlabel('Ranges')\nplt.ylabel('Values')\nplt.show()","63563f9b":"# Plot miles per gallon against horsepower with other semantics\nsns.relplot(x=\"Economy_GPD_Capital\", y=\"Freedom\",\n            sizes=(40, 400), alpha=.5, palette=\"muted\",\n            height=6, data=data)\nplt.show()","cc802f02":"# Show each distribution with both violins and points\nsns.violinplot(data=data,inner=\"points\")\nplt.xticks(rotation=90)\nplt.show()","4844706b":"g = sns.jointplot(\"Happiness_Score\", \"Family\", data=data, kind=\"reg\",\n                  xlim=(0, 60), ylim=(0, 12), color=\"m\", height=7)\n\nplt.show()","d393adc7":"data.head()","235ade23":"data[data['Region']==9].corr()","a79ab621":"\nfig, axs = plt.subplots(2, 2, figsize=(5, 5))\naxs[0, 0].hist(data['Happiness_Score'])\naxs[1, 0].scatter(data['Standart_Error'], data['Economy_GPD_Capital'])\naxs[0, 1].plot(data['Trust_Goverment_Corruption'], data['Freedom'])\naxs[1, 1].hist2d(data['Happiness_Score'], data['Economy_GPD_Capital'])\n\nplt.show()","0c40118f":"data.columns","035d6ce4":"data.head()","0f7bfa29":"data_new=data.copy()","a9a8427c":"data_new.sample(5)","12cbbb7e":"data_new.corr()","647da010":"data_new[\"NewFeature\"]=data_new.Trust_Goverment_Corruption*data_new.Freedom","102621ca":"sns.heatmap(data_new.corr(),annot=True,fmt='.1f')\nplt.show()","95d2548c":"data[data['Region']==0]","2afc801b":"data[data['Region']==6]","3e97da44":"data[data['Region']==6].groupby('Region')['Economy_GPD_Capital'].mean()","b6f8ed08":"import statsmodels.formula.api as sm\nmodel=sm.OLS(data.iloc[:,-1],data.iloc[:,:-1]).fit()\nmodel.summary()","a33a6d5a":"#I've divided the values to make better analysis of all values. Then, it is necessary to rank data between Healt values.\n#Normally we need to reserve train_test_split values first. Then we need to use the StandardScaler function.\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test=train_test_split(data,data_healt_life,test_size=0.2,random_state=0)","a79a8f6d":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX_train=sc.fit_transform(X_train)\nX_test=sc.transform(X_test)","3d9cebd5":"def plot_roc_(false_positive_rate,true_positive_rate,roc_auc):\n    plt.figure(figsize=(5,5))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],linestyle='--')\n    plt.axis('tight')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n    \ndef plot_feature_importances(gbm):\n    n_features = X_train.shape[1]\n    plt.barh(range(n_features), gbm.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), X_train.columns)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\n    plt.ylim(-1, n_features)","97eed990":"reduced_data_train = pd.DataFrame(X_train, columns=['Dim1', 'Dim2','Dim3','Dim4','Dim5','Dim6','Dim7','Dim8','Dim9'])\nreduced_data_test = pd.DataFrame(X_test, columns=['Dim1', 'Dim2','Dim3','Dim4','Dim5','Dim6','Dim7','Dim8','Dim9'])\nX_train=reduced_data_train\nX_test=reduced_data_test","9a4b624c":"combine_features_list=[\n    ('Dim1','Dim2','Dim3'),\n    ('Dim4','Dim5','Dim5','Dim6'),\n    ('Dim7','Dim8','Dim1'),\n    ('Dim4','Dim8','Dim5','Dim9')\n]","67f0cc1f":"parameters=[\n{\n    'penalty':['l1','l2'],\n    'C':[0.1,0.4,0.5],\n    'random_state':[0]\n    },\n]\n\nfor features in combine_features_list:\n    print(features)\n    print(\"*\"*50)\n    \n    X_train_set=X_train.loc[:,features]\n    X_test_set=X_test.loc[:,features]\n    \n    gslog=GridSearchCV(LogisticRegression(),parameters,scoring='accuracy')\n    gslog.fit(X_train_set,y_train)\n    print('Best parameters set:')\n    print(gslog.best_params_)\n    print()\n    predictions=[\n    (gslog.predict(X_train_set),y_train,'Train'),\n    (gslog.predict(X_test_set),y_test,'Test'),\n    ]\n    \n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1],pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n\n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=LogisticRegression(),X=X_train,y=y_train,cv=12)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50) \n   ","c2340fbf":"from sklearn.linear_model import LogisticRegression\n\nlr=LogisticRegression(C=0.1,penalty='l1',random_state=0)\nlr.fit(X_train,y_train)\n\ny_pred=lr.predict(X_test)\n\n\ny_proba=lr.predict_proba(X_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\n#print('Hata Oran\u0131 :',r2_score(y_test,y_pred))\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"Logistic TRAIN score with \",format(lr.score(X_train, y_train)))\nprint(\"Logistic TEST score with \",format(lr.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","4ac9e6fb":"parameters=[\n{\n    'n_neighbors':np.arange(2,33),\n    'n_jobs':[2,6]\n    },\n]\nprint(\"*\"*50)\nfor features in combine_features_list:\n    print(\"*\"*50)\n    \n    X_train_set=X_train.loc[:,features]\n    X_test_set=X_test.loc[:,features]\n   \n    gsknn=GridSearchCV(KNeighborsClassifier(),parameters,scoring='accuracy')\n    gsknn.fit(X_train_set,y_train)\n    print('Best parameters set:')\n    print(gsknn.best_params_)\n    print(\"*\"*50)\n    predictions = [\n    (gsknn.predict(X_train_set), y_train, 'Train'),\n    (gsknn.predict(X_test_set), y_test, 'Test1')\n    ]\n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n        \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=KNeighborsClassifier(),X=X_train,y=y_train,cv=12)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)","47b903ce":"knn=KNeighborsClassifier(n_jobs=2, n_neighbors=22)\nknn.fit(X_train,y_train)\n\ny_pred=knn.predict(X_test)\n\ny_proba=knn.predict_proba(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"KNN TRAIN score with \",format(knn.score(X_train, y_train)))\nprint(\"KNN TEST score with \",format(knn.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","3e74876f":"n_neighbors = range(1, 17)\ntrain_data_accuracy = []\ntest1_data_accuracy = []\nfor n_neigh in n_neighbors:\n    knn = KNeighborsClassifier(n_neighbors=n_neigh,n_jobs=5)\n    knn.fit(X_train, y_train)\n    train_data_accuracy.append(knn.score(X_train, y_train))\n    test1_data_accuracy.append(knn.score(X_test, y_test))\nplt.plot(n_neighbors, train_data_accuracy, label=\"Train Data Set\")\nplt.plot(n_neighbors, test1_data_accuracy, label=\"Test1 Data Set\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Neighbors\")\nplt.legend()\nplt.show()","4baf2ff1":"n_neighbors = range(1, 17)\nk_scores=[]\nfor n_neigh in n_neighbors:\n    knn = KNeighborsClassifier(n_neighbors=n_neigh,n_jobs=5)\n    scores=cross_val_score(estimator=knn,X=X_train,y=y_train,cv=12)\n    k_scores.append(scores.mean())\nprint(k_scores)","9a77d76d":"plt.plot(n_neighbors,k_scores)\nplt.xlabel('Value of k for KNN')\nplt.ylabel(\"Cross-Validated Accurancy\")\nplt.show()","3c128f29":"parameters = [\n    {\n        'kernel': ['linear'],\n        'random_state': [2]\n    },\n    {\n        'kernel': ['rbf'],\n        'gamma':[0.9,0.06,0.3],\n        'random_state': [0],\n        'C':[1,2,3,4,5,6],\n        'degree':[2],\n        'probability':[True]\n    },\n]\n\nfor features in combine_features_list:\n    print(\"*\"*50)\n    X_train_set=X_train.loc[:,features]\n    X_test_set=X_test.loc[:,features]\n  \n    svc = GridSearchCV(SVC(), parameters,\n    scoring='accuracy')\n    svc.fit(X_train_set, y_train)\n    print('Best parameters set:')\n    print(svc.best_params_)\n    print(\"*\"*50)\n    predictions = [\n    (svc.predict(X_train_set), y_train, 'Train'),\n    (svc.predict(X_test_set), y_test, 'Test1')\n    ]\n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n        \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=SVC(),X=X_train,y=y_train,cv=4)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)","024e3365":"svc=SVC(C=5,degree=2,gamma=0.06,kernel='rbf',probability=True,random_state=0)\nsvc.fit(X_train,y_train)\n\ny_pred=svc.predict(X_test)\n\ny_proba=svc.predict_proba(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"SVC TRAIN score with \",format(svc.score(X_train, y_train)))\nprint(\"SVC TEST score with \",format(svc.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","31e45494":"parameters = [\n{\n    'learning_rate': [0.01, 0.02, 0.002],\n    'random_state': [0],\n    'n_estimators': np.arange(3, 20)\n    },\n]\nfor features in combine_features_list:\n    print(\"*\"*50)\n    X_train_set=X_train.loc[:,features]\n    X_test1_set=X_test.loc[:,features]\n   \n    gbc = GridSearchCV(GradientBoostingClassifier(), parameters, scoring='accuracy')\n    gbc.fit(X_train_set, y_train)\n    print('Best parameters set:')\n    print(gbc.best_params_)\n    print(\"*\"*50)\n    predictions = [\n    (gbc.predict(X_train_set), y_train, 'Train'),\n    (gbc.predict(X_test1_set), y_test, 'Test1')\n    ]\n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n        \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=GradientBoostingClassifier(),X=X_train,y=y_train,cv=4)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)","ca523c3b":"gbc=GradientBoostingClassifier(learning_rate=0.02,n_estimators=18,random_state=0)\ngbc.fit(X_train,y_train)\n\ny_pred=gbc.predict(X_test)\n\ny_proba=gbc.predict_proba(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"GradientBoostingClassifier TRAIN score with \",format(gbc.score(X_train, y_train)))\nprint(\"GradientBoostingClassifier TEST score with \",format(gbc.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","cc1ca431":"plot_feature_importances(gbc)\nplt.show()","61d8d80d":"parameters = [\n    {\n        'max_depth': np.arange(1, 10),\n        'min_samples_split': np.arange(2, 5),\n        'random_state': [3],\n        'n_estimators': np.arange(10, 20)\n    },\n]\n\nfor features in combine_features_list:\n    print(\"*\"*50)\n    \n    X_train_set=X_train.loc[:,features]\n    X_test1_set=X_test.loc[:,features]\n    \n    tree=GridSearchCV(RandomForestClassifier(),parameters,scoring='accuracy')\n    tree.fit(X_train_set, y_train)\n    \n    print('Best parameters set:')\n    print(tree.best_params_)\n    print(\"*\"*50)\n    predictions = [\n        (tree.predict(X_train_set), y_train, 'Train'),\n        (tree.predict(X_test1_set), y_test, 'Test1')\n    ]\n    \n    for pred in predictions:\n        \n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n    \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=RandomForestClassifier(),X=X_train,y=y_train,cv=4)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)","5bb200a1":"rfc=RandomForestClassifier(max_depth=7,min_samples_split=4,n_estimators=19,random_state=3)\nrfc.fit(X_train,y_train)\n\ny_pred=rfc.predict(X_test)\n\ny_proba=rfc.predict_proba(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\nfrom sklearn.metrics import r2_score,accuracy_score\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"RandomForestClassifier TRAIN score with \",format(rfc.score(X_train, y_train)))\nprint(\"RandomForestClassifier TEST score with \",format(rfc.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","8e92982d":"for i in range(1,11):\n    rf = RandomForestClassifier(n_estimators=i, random_state = 3, max_depth=7)\n    rf.fit(X_train, y_train)\n    print(\"TEST set score w\/ \" +str(i)+\" estimators: {:.5}\".format(rf.score(X_test, y_test)))","fdfac354":"plot_feature_importances(rf)\nplt.show()","3a66a7f6":"parameters = [\n{\n    'random_state': [42],\n    },\n]\nfor features in combine_features_list:\n    print(\"*\"*50)\n    X_train_set=X_train.loc[:,features]\n    X_test1_set=X_test.loc[:,features]\n    \n    dtr = GridSearchCV(DecisionTreeClassifier(), parameters, scoring='accuracy')\n    \n    dtr.fit(X_train_set, y_train)\n    print('Best parameters set:')\n    print(dtr.best_params_)\n    print(\"*\"*50)\n    predictions = [\n    (dtr.predict(X_train_set), y_train, 'Train'),\n    (dtr.predict(X_test1_set), y_test, 'Test1')\n    ]\n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n        \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=DecisionTreeClassifier(),X=X_train,y=y_train,cv=4)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)  ","4e7d1369":"parameters = [\n{\n    'random_state': [42],\n    },\n]\nfor features in combine_features_list:\n    print(\"*\"*50)\n    X_train_set=X_train.loc[:,features]\n    X_test1_set=X_test.loc[:,features]\n    \n    dtr = GridSearchCV(SVC(), parameters, scoring='accuracy')\n    \n    dtr.fit(X_train_set, y_train)\n    print('Best parameters set:')\n    print(dtr.best_params_)\n    print(\"*\"*50)\n    predictions = [\n    (dtr.predict(X_train_set), y_train, 'Train'),\n    (dtr.predict(X_test1_set), y_test, 'Test1')\n    ]\n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n        \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=SVC(),X=X_train,y=y_train,cv=4)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)  ","798302f8":"<p id='13'><b><h3>Kernelized SVM<\/h3><\/b><\/p>","209dc159":"<p id='10'><b><h3>Gradient Boosting Machine<\/h3><\/b>","6d196b49":"<p id='7'><b><h3>Logistic Regression<\/h3><\/b>\n<p>First we need parameters to use our data more effectively. Hyperthermatic technique was used for this condition. This technique is used to express different features in the process.<\/p>","e0ce81a1":"<p id='5'><b><h3>Data Cleaning<\/h3><\/b>","15583f22":"<p id='8'><b><h3>K-Nearest Neighbors<\/h3><\/b>","ec9cba30":"<p id='9'><b><h3>Naive Baes<\/h3><\/b>","28352608":"<p>As a result of our initial evaluations, we have used a number of artificial learning algorithms. These are logistic regression, support vector machine (SVM), k close neighborhood (kNN), GradientBoostingClassifier and RandomForestClassifier algorithms. The first algorithm is logistic regression algorithm. To implement this algorithm model, we need to separate dependent and independent variables within our data sets. In addition, we created a combination of features between different features to make different experiments. While creating these parameters, the process of finding the best results was made by giving hyper parameter values.<\/p>","6f77b842":"<p id='1'><b><h3>Indroduction<\/b><\/h3>\n<p>The World Happiness Report is a landmark survey of the state of global happiness. The first report was published in 2012, the second in 2013, the third in 2015, and the fourth in the 2016 Update. The World Happiness 2017, which ranks 155 countries by their happiness levels, was released at the United Nations at an event celebrating International Day of Happiness on March 20th. The report continues to gain global recognition as governments, organizations and civil society increasingly use happiness indicators to inform their policy-making decisions. Leading experts across fields \u2013 economics, psychology, survey analysis, national statistics, health, public policy and more \u2013 describe how measurements of well-being can be used effectively to assess the progress of nations. The reports review the state of happiness in the world today and show how the new science of happiness explains personal and national variations in happiness.<\/p>\n\n<p>Our aim here is to analyze the data set in detail and visualize it with a wide range of visualization tools.<\/p>","27d9c8cc":"<p id='4'><h3><b>Data Visualization<\/b><\/h3><\/p>","7ed2f309":"<p id='2'><b><h3>Import Library<\/b>","2964eb7c":"<p id='11'><b><h3>Random Forest<\/h1><\/b><\/p>\t\n","ea8903d7":"<p><h1><b>World Happiness Report<\/b><\/h1><\/p>\n<p><h2><b>Content<\/b><\/h2><\/p>\n<ul>\n    <a href='#1'><li>Introduction<\/li><\/a>\n    <a href='#2'><li>Import Library<\/li><\/a>\n    <a href='#3'><li>Exploratory Data Analysis<\/li><\/a>\n    <a href='#4'><li>Data Visualization<\/li><\/a>\n     <a href='#5'><li>Data Cleaning<\/li><\/a>\n    <a href='#6'><li>Machine Learning<\/li><\/a>\n        <ul>\n                <a href='#7'><li>Logistic Regression<\/li><\/a>\n            <a href='#8'><li>K-Nearest Neighbors<\/li><\/a>\n            <a href='#9'><li>Naive Bayes<\/li><\/a>\n            <a href='#10'><li>Gradient Boosting Machine<\/li><\/a>\n            <a href='#11'><li>Random Forest<\/li><\/a>\n            <a href='#12'><li>Decision Tree<\/li><\/a>\n            <a href='#13'><li>Kernelized SVM<\/li><\/a>\n        <\/ul>\n<\/ul>\n\n<p>last updated : <b>30.06.2019<\/b><\/p>\n<p><h2><b>If you like it, please <i>upvote.<\/i><\/b><\/h2><\/p>","1ff50e00":"<p id='12'><b><h3>Decision Tree<\/h3><\/b><\/p>","31178d04":"<p id='3'><b><h3>Exploratory Data Analysis<\/b><\/h3>","fbfe4e85":"<p id='6'><b><h3>Machine Learning<\/h3><\/b>"}}