{"cell_type":{"a6646f81":"code","3801a87d":"code","71c71047":"code","36eac0b2":"code","a14d77a1":"code","a4c72c71":"code","8e560f64":"code","dd9cdd04":"code","ceb564e4":"code","db5a92d2":"code","e342109a":"code","32f065ab":"code","75dd6549":"code","50e51a63":"code","1cbb1f06":"code","0a7d565c":"code","8ae17c4a":"code","cd8f41dc":"code","24e95210":"code","23c68e39":"code","7dd621f8":"code","ee427272":"code","3158d955":"code","b87354c6":"code","e16d9fe2":"code","1cb06bd1":"code","88c0e67e":"code","be3a30ae":"code","d1e1260d":"code","10b85ffe":"code","fe5587a3":"code","d3f643c3":"code","36b9849d":"code","ca96058b":"code","40bfe61a":"code","c0c6adc1":"code","f7dcd491":"code","41cdb127":"code","7d3528cf":"code","c488e7ff":"code","93a2c86b":"code","5ca21c9f":"code","ea55e4e6":"code","511b90f6":"code","15846447":"code","164fa17e":"code","8e11d014":"markdown","2ae31c35":"markdown","b6a9916d":"markdown","ce230a9a":"markdown","b95a3df5":"markdown","f2fe87be":"markdown","a67a48ac":"markdown","d29eb5b7":"markdown","2750b2dc":"markdown","c3b45660":"markdown","85530fa5":"markdown","584ab7a8":"markdown","41e80976":"markdown","4fd4ab11":"markdown","8d5f2622":"markdown","d00ce6d1":"markdown","3cfe37ca":"markdown","526443df":"markdown","9bb003d1":"markdown","ab41f391":"markdown","32db652f":"markdown"},"source":{"a6646f81":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport cv2\nfrom tqdm import tqdm\nfrom PIL import Image\nimport io\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import EfficientNetB2\nfrom keras.layers import GlobalAveragePooling2D, Dropout, Dense\nfrom keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard, ModelCheckpoint\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom IPython.display import display, clear_output\nimport ipywidgets as widgets\n","3801a87d":"labels = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\nX_train = []\nY_train  = []\nX_test = []\nY_test = []\nimage_size=150\n\nfor label in labels:\n    trainPath = os.path.join('..\/input\/brain-tumor-classification-mri\/Training',label)\n    for file in tqdm(os.listdir(trainPath)):\n        image = cv2.imread(os.path.join(trainPath, file))\n        image = cv2.resize(image, (image_size, image_size))\n        X_train.append(image)\n        Y_train.append(label)\n    \n    testPath = os.path.join('..\/input\/brain-tumor-classification-mri\/Testing',label)\n    for file in tqdm(os.listdir(testPath)):\n        image = cv2.imread(os.path.join(testPath, file))\n        image = cv2.resize(image, (image_size, image_size))\n        X_test.append(image)\n        Y_test.append(label)\n    \nX_train = np.array(X_train)\nX_test = np.array(X_test)","71c71047":"fig, ax = plt.subplots(1,4, figsize=(20,20))\nk = 0\nfor i in range(0,4):\n    if i==0: idx=0\n    elif i==1: idx=827\n    elif i==2: idx=1649\n    else: idx=2045\n    ax[k].imshow(X_train[idx])\n    ax[k].set_title(Y_train[idx])\n    ax[k].axis('off')\n    k+=1","36eac0b2":"X_train, Y_train = shuffle(X_train, Y_train, random_state=28)","a14d77a1":"X_train.shape","a4c72c71":"sns.countplot(Y_test)","8e560f64":"sns.countplot(Y_train)","dd9cdd04":"y_train_ = []\nfor i in Y_train:\n    y_train_.append(labels.index(i))\nY_train = y_train_\n\nY_train = tf.keras.utils.to_categorical(Y_train)\n\ny_test_ = []\nfor i in Y_test:\n    y_test_.append(labels.index(i))\nY_test = y_test_\n\nY_test = tf.keras.utils.to_categorical(Y_test)","ceb564e4":"X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=28)","db5a92d2":"base_model = EfficientNetB2(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))","e342109a":"model = base_model.output\nmodel = GlobalAveragePooling2D()(model)\nmodel = Dense(4, activation='softmax')(model)\nmodel = Model(inputs = base_model.input, outputs=model)","32f065ab":"model.summary()","75dd6549":"model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])","50e51a63":"reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.3, patience=2, min_denta=0.0001, mode='auto', verbose=1)\ntensorboard = TensorBoard(log_dir='logs')\ncheckpoint = ModelCheckpoint(\"brain_tumor_classification.h5\", monitor='val_accuracy', save_best_only=True, mode='auto', verbose=1)","1cbb1f06":"datagen = ImageDataGenerator(\nfeaturewise_center=False,\nsamplewise_center=False,\nfeaturewise_std_normalization=False,\nsamplewise_std_normalization=False,\nzca_whitening=False,\nrotation_range=10,\nzoom_range=0.2,\nwidth_shift_range=0.2,\nhorizontal_flip=False,\nvertical_flip=False)\n\ndatagen.fit(X_train)","0a7d565c":"history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),\n                              validation_data=(X_val, Y_val), \n                              epochs=50, \n                              verbose=1,\n                            callbacks=[tensorboard, checkpoint, reduce_lr])","8ae17c4a":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","cd8f41dc":"pred = model.predict(X_test)\npred = np.argmax(pred,axis=1)\ny_test_new = np.argmax(Y_test,axis=1)","24e95210":"accuracy = np.sum(pred==y_test_new)\/len(pred)\nprint(\"Accuracy on testing dataset: {:.2f}%\".format(accuracy*100))","23c68e39":"def img_pred(upload):\n    for name, file_info in uploader.value.items():\n        img = Image.open(io.BytesIO(file_info['content']))\n    opencvImage = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n    img = cv2.resize(opencvImage,(150,150))\n    img = img.reshape(1,150,150,3)\n    p = model.predict(img)\n    p = np.argmax(p,axis=1)[0]\n\n    if p==0:\n        p='Glioma Tumor'\n    elif p==1:\n        p='Meningioma Tumor'\n    elif p==2:\n        print('The model predicts that there is no tumor')\n    else:\n        p='Pituitary Tumor'\n\n    if p!=2:\n        print(f'The Model predicts that it is a {p}')","7dd621f8":"uploader = widgets.FileUpload()\ndisplay(uploader)\nbutton = widgets.Button(description='Predict')\nout = widgets.Output()\ndef on_button_clicked(_):\n    with out:\n        clear_output()\n        try:\n            img_pred(uploader)\n        except:\n            print('No Image Uploaded\/Invalid Image File')\nbutton.on_click(on_button_clicked)\nwidgets.VBox([button,out])","ee427272":"new_model = tf.keras.models.load_model('.\/brain_tumor_classification.h5')","3158d955":"test_pred = new_model.predict(X_test)\ntest_pred = np.argmax(test_pred, axis=1)\nY_test_ = np.argmax(Y_test, axis=1)\nprint(\"Accuracy on testing set: {:.2f}%\".format(np.sum(test_pred==Y_test_)\/len(Y_test_)*100))","b87354c6":"train_pred = new_model.predict(X_train)\ntrain_pred = np.argmax(train_pred, axis=1)\nY_train_ = np.argmax(Y_train, axis=1)\nprint(\"Accuracy on training set: {:.2f}%\".format(np.sum(train_pred==Y_train_)\/len(Y_train_)*100))","e16d9fe2":"# tunning the model\nbase_model.trainable = True\n\nfor layer in base_model.layers[:-1]:\n  base_model.trainable = False\n\nmodel.compile(\n    loss = tf.keras.losses.categorical_crossentropy,\n    optimizer = tf.keras.optimizers.Adam(learning_rate= 0.001),\n    metrics = [\"accuracy\"]\n)","1cb06bd1":"model.summary()","88c0e67e":"train_dir = \"..\/input\/brain-tumor-classification-mri\/Training\"\ntest_dir = \"..\/input\/brain-tumor-classification-mri\/Testing\"","be3a30ae":"import tensorflow as tf\n\ntrain_data = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n                                                                 image_size=(150,150),\n                                                                 label_mode = \"categorical\",\n                                                                 batch_size = 32\n                                                                 )\ntest_data =tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n                                                               image_size =(150,150),\n                                                               label_mode = \"categorical\",\n                                                               batch_size = 32\n                                                               )","d1e1260d":"# fitting data to the tuned model\n\nh_t=model.fit(train_data,\n          epochs = 10,\n          steps_per_epoch = len(train_data),\n          validation_data = test_data,\n          validation_steps = len(test_data)\n          )","10b85ffe":"# evaluatinn the model\nmodel_evaluation = model.evaluate(test_data)","fe5587a3":"print(f\"Model Accuracy:{model_evaluation[1] *100: 0.2f} %\")","d3f643c3":"import seaborn as sns\ncolors_dark = [\"#1F1F1F\", \"#313131\", '#636363', '#AEAEAE', '#DADADA']\ncolors_red = [\"#331313\", \"#582626\", '#9E1717', '#D35151', '#E9B4B4']\ncolors_green = ['#01411C','#4B6F44','#4F7942','#74C365','#D0F0C0']\n\nsns.palplot(colors_dark)\nsns.palplot(colors_green)\nsns.palplot(colors_red)","36b9849d":"epochs = [i for i in range(10)]\nfig, ax = plt.subplots(1,2,figsize=(14,7))\ntrain_acc = h_t.history['accuracy']\ntrain_loss = h_t.history['loss']\nval_acc = h_t.history['val_accuracy']\nval_loss = h_t.history['val_loss']\n\nfig.text(s='Epochs vs. Training and Validation Accuracy\/Loss_tunning',size=18,fontweight='bold',\n             fontname='monospace',color=colors_dark[1],y=1,x=0.28,alpha=0.8)\n\nsns.despine()\nax[0].plot(epochs, train_acc, marker='o',markerfacecolor=colors_green[2],color=colors_green[3],\n           label = 'Training Accuracy')\nax[0].plot(epochs, val_acc, marker='o',markerfacecolor=colors_red[2],color=colors_red[3],\n           label = 'Validation Accuracy')\nax[0].legend(frameon=False)\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Accuracy')\n\nsns.despine()\nax[1].plot(epochs, train_loss, marker='o',markerfacecolor=colors_green[2],color=colors_green[3],\n           label ='Training Loss')\nax[1].plot(epochs, val_loss, marker='o',markerfacecolor=colors_red[2],color=colors_red[3],\n           label = 'Validation Loss')\nax[1].legend(frameon=False)\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('Training & Validation Loss')\n\nfig.show()","ca96058b":"import tensorflow as tf\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nmpl.rcParams['figure.figsize'] = (8, 8)\nmpl.rcParams['axes.grid'] = False","40bfe61a":"from sklearn.preprocessing import OneHotEncoder \n\nencoder = OneHotEncoder()\nencoder.fit([[0], [1]]) \n\n","c0c6adc1":"def names(number):\n    if number==0:\n        return 'Its a glioma'\n    else:\n        return 'No, Its a megningima'","f7dcd491":"from matplotlib.pyplot import imshow\nimg = Image.open(r\"..\/input\/brain-tumor-classification-mri\/Training\/glioma_tumor\/gg (10).jpg\")\nx = np.array(img.resize((150,150)))\nx = x.reshape(1,150,150,3)\nres = model.predict_on_batch(x)\nclassification = np.where(res == np.amax(res))[1][0]\nimshow(img)\nprint(str(res[0][classification]*100) + '% Confidence This Is ' + names(classification))","41cdb127":"pretrained_model = tf.keras.applications.efficientnet.EfficientNetB2(include_top=True,\n                                                    weights='imagenet')\npretrained_model.trainable = False\n\n#ImageNet labels\ndecode_predictions = tf.keras.applications.efficientnet.decode_predictions","7d3528cf":"# Helper function to preprocess the image so that it can be inputted in MobileNetV2\ndef preprocess(image):\n  image = tf.cast(image, tf.float32)\n  image = tf.image.resize(image, (260, 260))\n  image = tf.keras.applications.resnet_v2.preprocess_input(image)\n  image = image[None, ...]\n  return image\n\n# Helper function to extract labels from probability vector\ndef get_imagenet_label(probs):\n  return decode_predictions(probs, top=1)[0][0]","c488e7ff":"pretrained_model = tf.keras.applications.efficientnet.EfficientNetB2(include_top=True,\n                                                    weights='imagenet')\npretrained_model.trainable = False\n\n#ImageNet labels\ndecode_predictions = tf.keras.applications.efficientnet.decode_predictions","93a2c86b":"image_raw = tf.io.read_file('..\/input\/brain-tumor-classification-mri\/Training\/glioma_tumor\/gg (10).jpg')\nimage = tf.image.decode_image(image_raw)\n\nimage = preprocess(image)\nimage_probs = pretrained_model.predict(image)","5ca21c9f":"loss_object = tf.keras.losses.CategoricalCrossentropy()\n\ndef create_adversarial_pattern(input_image, input_label):\n  with tf.GradientTape() as tape:\n    tape.watch(input_image)\n    prediction = pretrained_model(input_image)\n    loss = loss_object(input_label, prediction)\n\n  # Get the gradients of the loss w.r.t to the input image.\n  gradient = tape.gradient(loss, input_image)\n  # Get the sign of the gradients to create the perturbation\n  signed_grad = tf.sign(gradient)\n  return signed_grad","ea55e4e6":"# Get the input label of the image.\nlabrador_retriever_index = 208\nlabel = tf.one_hot(labrador_retriever_index, image_probs.shape[-1])\nlabel = tf.reshape(label, (1, image_probs.shape[-1]))\n\nperturbations = create_adversarial_pattern(image, label)\nplt.imshow(perturbations[0] * 0.5 + 0.5);  # To change [-1, 1] to [0,1]","511b90f6":"\npretrained_model = tf.keras.applications.efficientnet.EfficientNetB2(include_top=True,\n                                                    weights='imagenet')\npretrained_model.trainable = False\n\n#ImageNet labels\ndecode_predictions = tf.keras.applications.efficientnet.decode_predictions","15846447":"def display_images(image, description):\n  _, label, confidence = get_imagenet_label(pretrained_model.predict(image))\n  plt.figure()\n  plt.imshow(image[0]*0.5+0.5)\n  plt.title('{} \\n {} : {:.2f}% Confidence'.format(description,\n                                                   label, confidence*100))\n  plt.show()","164fa17e":"epsilons = [0, 0.01, 0.1, 0.15]\ndescriptions = [('Epsilon = {:0.3f}'.format(eps) if eps else 'Input')\n                for eps in epsilons]\n\nfor i, eps in enumerate(epsilons):\n  adv_x = image + eps*perturbations\n  adv_x = tf.clip_by_value(adv_x, -1, 1)\n  display_images(adv_x, descriptions[i])","8e11d014":"## Data Preparation","2ae31c35":"## Define Callbacks","b6a9916d":"## Split the data into training and validation subsets","ce230a9a":"> Essayons ceci pour diff\u00e9rentes valeurs d'epsilon et observons l'image r\u00e9sultante. Vous remarquerez qu'au fur et \u00e0 mesure que la valeur d'epsilon augmente, il devient plus facile de tromper le r\u00e9seau. Cependant, il s'agit d'un compromis qui rend les perturbations plus identifiables.","b95a3df5":"## One-Hot encoding","f2fe87be":"> **Impl\u00e9mentation de la m\u00e9thode de signe de gradient rapide**","a67a48ac":"## Shuffle the train set","d29eb5b7":"# Tunning the model\n","2750b2dc":"### Show the counts of observations in each categorical bin using bars.","c3b45660":"## Import Necessarcy Libraries","85530fa5":"## Transfer Learning with EfficientNetB2","584ab7a8":"Essayons donc de tromper un mod\u00e8le pr\u00e9-entra\u00een\u00e9.  le mod\u00e8le est EfficientNetB2 mod\u00e8le, sur pr\u00e9 - entra\u00een\u00e9 IMAGEnet .","41e80976":"> Les perturbations r\u00e9sultantes peuvent \u00e9galement \u00eatre visualis\u00e9es.","4fd4ab11":"## Data Augmentation","8d5f2622":"## Visusalize model performance","d00ce6d1":"## Show sample images from each label","3cfe37ca":"## Train Model","526443df":"# Exemple contradictoire utilisant FGSM","9bb003d1":"## Evaluate the model","ab41f391":"La premi\u00e8re \u00e9tape consiste \u00e0 cr\u00e9er des perturbations qui seront utilis\u00e9es pour d\u00e9former l'image originale r\u00e9sultant en une image contradictoire. Comme mentionn\u00e9, pour cette t\u00e2che, les gradients sont pris par rapport \u00e0 l'image.","32db652f":"# Cr\u00e9er l'image contradictoire"}}