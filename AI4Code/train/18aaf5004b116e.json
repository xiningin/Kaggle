{"cell_type":{"5ed2cb68":"code","79d8708f":"code","bd0f7567":"code","4c95e82e":"code","3360cbb3":"code","ba473a1d":"code","a7f61c93":"code","8510a67d":"code","da44bf0f":"code","2a15e160":"code","51f42651":"code","769b4b0b":"code","245699ef":"code","0d58afe6":"code","60d9f61e":"code","9a01586b":"code","a6e00479":"code","0fa6de72":"code","2ae37bd7":"code","dbf1ed9f":"code","399c09a0":"code","70a2dca8":"code","42322e8b":"code","dd1a5ea7":"code","4fb5edef":"code","49430579":"code","8346d1f5":"code","8f0648e0":"code","f57c041d":"code","a5cd3503":"code","1cbbd558":"code","7e3e2bc5":"code","312aed16":"code","defb951a":"code","5e976027":"code","fbad775e":"code","13251c49":"code","a441b6e5":"markdown","2453944f":"markdown","c42d3c91":"markdown","461119e2":"markdown","ef2c62d7":"markdown","78fd97bb":"markdown","65bcdb3d":"markdown","7e41c2a2":"markdown","e6bc9057":"markdown","ada278b9":"markdown","923ba081":"markdown","658bba51":"markdown","b99c0437":"markdown","4d5165da":"markdown","5f5ddf7d":"markdown","ec4c46f7":"markdown","f3bed3ef":"markdown"},"source":{"5ed2cb68":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.python.framework import ops\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = \"99\"","79d8708f":"from tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()","bd0f7567":"from __future__ import absolute_import #\ud30c\uc774\uc36c 2,3\ubc84\uc804\ubb38\uc81c\ub85c \uc778\ud574 \uc0dd\uae30\ub294 \uac83\ub4e4\uc744 \ubc29\uc9c0\ud558\uace0\uc790 \nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\nimport warnings\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\nimport matplotlib\n\nfrom sklearn.model_selection import train_test_split\n# \ucd5c\uc18c \ucd5c\ub300\ub97c 0~1\ubc94\uc704\ub85c \ud45c\uc900\ud654 \uc2dc\ud0a4\ub294, \uc804\ucc98\ub9ac \uacfc\uc815\uc5d0\uc11c \ud544\uc694\ud55c \ud568\uc218\nfrom sklearn.preprocessing import MinMaxScaler\n\ntf.logging.set_verbosity(tf.logging.INFO)\nsess = tf.InteractiveSession()\n\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nprint('Shape of the train data with all features:', train.shape)\ntrain = train.select_dtypes(exclude=['object'])\nprint(\"\")\nprint('Shape of the train data with numerical features:', train.shape)\ntrain.drop('Id',axis = 1, inplace = True)\ntrain.fillna(0,inplace=True)","4c95e82e":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest = test.select_dtypes(exclude=['object'])\n\n#\uc22b\uc790\ud615\uc778 \uac83\ub4e4\ub9cc \ub0a8\uaca8\ub193\uace0 \uc5ec\uae30 \uc548\uc5d0\uc11c \ud574\uacb0\ud558\uaca0\ub2e4\ub294 \uc758\uc9c0\n\nID = test.Id\ntest.fillna(0,inplace=True) #\uacb0\uce21\uce58 \uc784\uc758\ub85c 0 \ub123\uae30\ntest.drop('Id',axis = 1, inplace = True)\n\nprint(\"\")\nprint(\"List of features contained our dataset:\",list(train.columns))","3360cbb3":"warnings.filterwarnings('ignore')\nfrom sklearn.ensemble import IsolationForest\n\nclf = IsolationForest(max_samples = 100, random_state = 42)\nclf.fit(train)\ny_noano = clf.predict(train)\ny_noano = pd.DataFrame(y_noano, columns = ['Top'])\ny_noano[y_noano['Top'] == 1].index.values\n\ntrain = train.iloc[y_noano[y_noano['Top'] == 1].index.values]\ntrain.reset_index(drop = True, inplace = True)\nprint(\"Number of Outliers:\", y_noano[y_noano['Top'] == -1].shape[0])\nprint(\"Number of rows without outliers:\", train.shape[0])","ba473a1d":"train.head(10)","a7f61c93":"col_train = list(train.columns)\ncol_train_bis = list(train.columns)\n\n# \uc608\uce21\ud574\uc57c \ud560 \uac12\uc740 \ucc98\ub9ac\ud560 \ud544\uc694 \uc5c6\uc73c\ubbc0\ub85c \uc81c\uac70\ncol_train_bis.remove('SalePrice')\n\nmat_train = np.matrix(train)\nmat_test  = np.matrix(test)\nmat_new = np.matrix(train.drop('SalePrice',axis = 1))\nmat_y = np.array(train.SalePrice).reshape((1314,1))\n\nprepro_y = MinMaxScaler()\nprepro_y.fit(mat_y)\n\nprepro = MinMaxScaler()\nprepro.fit(mat_train)\n\nprepro_test = MinMaxScaler()\nprepro_test.fit(mat_new)\n\ntrain = pd.DataFrame(prepro.transform(mat_train),columns = col_train)\ntest  = pd.DataFrame(prepro_test.transform(mat_test),columns = col_train_bis)\n\ntrain.head()","8510a67d":"# List of features\nCOLUMNS = col_train\nFEATURES = col_train_bis\nLABEL = \"SalePrice\"\n\n# Columns for tensorflow : \ud150\uc11c\ud50c\ub85c\uc5d0\uac8c column\uac12\ub4e4\uc744 \uc81c\uc2dc\nfeature_cols = [tf.contrib.layers.real_valued_column(k) for k in FEATURES]\n\n# Training set and Prediction set with the features to predict : \ubb34\uc5c7\uc744 \ud559\uc2b5\ud574\uc57c \ud558\ub294\uc9c0 input, output \uac12 \uc8fc\uae30\ntraining_set = train[COLUMNS]\nprediction_set = train.SalePrice\n\n# Train and Test : train \uc548\uc5d0\uc11c \ubd84\ub9ac\ud558\uae30 (x_train > training_set, x_test > testing_set)\nx_train, x_test, y_train, y_test = train_test_split(training_set[FEATURES] , prediction_set, test_size=0.33, random_state=42)\ny_train = pd.DataFrame(y_train, columns = [LABEL])\ntraining_set = pd.DataFrame(x_train, columns = FEATURES).merge(y_train, left_index = True, right_index = True)\ntraining_set.head()\n\n# Training for submission\ntraining_sub = training_set[col_train]","da44bf0f":"# Same thing but for the test set : \ub098\uc911\uc5d0 test\uc5d0\ub3c4 \uc2e4\ud589\ub418\uae30 \uc704\ud574\uc11c \ub530\ub85c test \uc14b\ub3c4 \ucc98\ub9ac\ud558\uae30\ny_test = pd.DataFrame(y_test, columns = [LABEL])\ntesting_set = pd.DataFrame(x_test, columns = FEATURES).merge(y_test, left_index = True, right_index = True)\ntesting_set.head()","2a15e160":"# Model  \uc124\uc815\ud558\uae30 \ntf.logging.set_verbosity(tf.logging.ERROR)\nregressor = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols, \n                                          activation_fn = tf.nn.relu, hidden_units=[256, 128, 64, 32, 16])\n#optimizer = tf.train.GradientDescentOptimizer( learning_rate= 0.1 ))\n    \n# Reset the index of training : \uc778\ub371\uc2a4 \uc815\ub9ac\ud558\uae30\ntraining_set.reset_index(drop = True, inplace =True)","51f42651":"#\ndef input_fn(data_set, pred = False):\n    \n    if pred == False:\n        \n        feature_cols = {k: tf.constant(data_set[k].values) for k in FEATURES}\n        labels = tf.constant(data_set[LABEL].values)\n        \n        return feature_cols, labels\n\n    if pred == True:\n        feature_cols = {k: tf.constant(data_set[k].values) for k in FEATURES}\n        \n        return feature_cols","769b4b0b":"# Deep Neural Network Regressor with the training set which contain the data split by train test split\nregressor.fit(input_fn=lambda: input_fn(training_set), steps=2000)","245699ef":"# Evaluation on the test set created by train_test_split\nev = regressor.evaluate(input_fn=lambda: input_fn(testing_set), steps=1)","0d58afe6":"# Display the score on the testing set\n# 0.002X in average\nloss_score1 = ev[\"loss\"]\nprint(\"Final Loss on the testing set: {0:f}\".format(loss_score1))","60d9f61e":"# Predictions\ny = regressor.predict(input_fn=lambda: input_fn(testing_set))\npredictions = list(itertools.islice(y, testing_set.shape[0]))","9a01586b":"def leaky_relu(x):\n    return tf.nn.relu(x) - 0.01 * tf.nn.relu(-x)","a6e00479":"# Model\nregressor = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols, \n                                          activation_fn = leaky_relu, hidden_units=[256, 128, 64, 32, 16])\n    \n# Deep Neural Network Regressor with the training set which contain the data split by train test split\nregressor.fit(input_fn=lambda: input_fn(training_set), steps=2000)\n\n# Evaluation on the test set created by train_test_split\nev = regressor.evaluate(input_fn=lambda: input_fn(testing_set), steps=1)\n# Display the score on the testing set\n# 0.002X in average\nloss_score2 = ev[\"loss\"]\nprint(\"Final Loss on the testing set with Leaky Relu: {0:f}\".format(loss_score2))\n\n# Predictions\ny_predict = regressor.predict(input_fn=lambda: input_fn(test, pred = True))","0fa6de72":"# Model\nregressor = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols, \n                                          activation_fn = tf.nn.elu, hidden_units=[200, 100, 50, 25, 12])\n    \n# Deep Neural Network Regressor with the training set which contain the data split by train test split\nregressor.fit(input_fn=lambda: input_fn(training_set), steps=2000)\n\n# Evaluation on the test set created by train_test_split\nev = regressor.evaluate(input_fn=lambda: input_fn(testing_set), steps=1)\n\nloss_score3 = ev[\"loss\"]","2ae37bd7":"print(\"Final Loss on the testing set with Elu: {0:f}\".format(loss_score3))\n# Predictions\ny_predict = regressor.predict(input_fn=lambda: input_fn(test, pred = True))","dbf1ed9f":"# Import and split : \uc800\ubc88\uacfc \ub3d9\uc77c\ud55c \ubc29\uc2dd\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain.drop('Id',axis = 1, inplace = True)\ntrain_numerical = train.select_dtypes(exclude=['object'])\ntrain_numerical.fillna(0,inplace = True)\n\ntrain_categoric = train.select_dtypes(include=['object'])\ntrain_categoric.fillna('NONE',inplace = True) #numerical\uacfc \ub2e4\ub974\uac8c \uc801\uc6a9\ntrain = train_numerical.merge(train_categoric, left_index = True, right_index = True) \n\n#test\uc14b\ub3c4 \ub3d9\uc77c\ud558\uac8c \uc801\uc6a9\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nID = test.Id\ntest.drop('Id',axis = 1, inplace = True)\ntest_numerical = test.select_dtypes(exclude=['object'])\ntest_numerical.fillna(0,inplace = True)\n\ntest_categoric = test.select_dtypes(include=['object'])\ntest_categoric.fillna('NONE',inplace = True) #numerical\uacfc \ub2e4\ub974\uac8c \uc801\uc6a9\n\ntest = test_numerical.merge(test_categoric, left_index = True, right_index = True) ","399c09a0":"# Removie the outliers: \uc800\ubc88\uacfc \ub3d9\uc77c\ud55c \ubc29\uc2dd\nfrom sklearn.ensemble import IsolationForest\n\nclf = IsolationForest(max_samples = 100, random_state = 42)\nclf.fit(train_numerical)\ny_noano = clf.predict(train_numerical)\ny_noano = pd.DataFrame(y_noano, columns = ['Top'])\ny_noano[y_noano['Top'] == 1].index.values\n\ntrain_numerical = train_numerical.iloc[y_noano[y_noano['Top'] == 1].index.values]\ntrain_numerical.reset_index(drop = True, inplace = True)\n\ntrain_categoric = train_categoric.iloc[y_noano[y_noano['Top'] == 1].index.values]\ntrain_categoric.reset_index(drop = True, inplace = True)\n\ntrain = train.iloc[y_noano[y_noano['Top'] == 1].index.values]\ntrain.reset_index(drop = True, inplace = True)","70a2dca8":"#Preprocessing\ncol_train_num = list(train_numerical.columns)\ncol_train_num_bis = list(train_numerical.columns)\n\ncol_train_cat = list(train_categoric.columns)\n\ncol_train_num_bis.remove('SalePrice')\n\nmat_train = np.matrix(train_numerical)\nmat_test  = np.matrix(test_numerical)\nmat_new = np.matrix(train_numerical.drop('SalePrice',axis = 1))\nmat_y = np.array(train.SalePrice)\n\nprepro_y = MinMaxScaler()\nprepro_y.fit(mat_y.reshape(1314,1))\n\nprepro = MinMaxScaler()\nprepro.fit(mat_train)\n\nprepro_test = MinMaxScaler()\nprepro_test.fit(mat_new)\n\ntrain_num_scale = pd.DataFrame(prepro.transform(mat_train),columns = col_train)\ntest_num_scale  = pd.DataFrame(prepro_test.transform(mat_test),columns = col_train_bis)","42322e8b":"train[col_train_num] = pd.DataFrame(prepro.transform(mat_train),columns = col_train_num)\ntest[col_train_num_bis]  = test_num_scale","dd1a5ea7":"# Model \uc801\uc6a9\nCOLUMNS = col_train_num\nFEATURES = col_train_num_bis\nLABEL = \"SalePrice\"\n\nFEATURES_CAT = col_train_cat\n\nengineered_features = []\n\nfor continuous_feature in FEATURES:\n    engineered_features.append(\n        tf.contrib.layers.real_valued_column(continuous_feature))\n\nfor categorical_feature in FEATURES_CAT:\n    sparse_column = tf.contrib.layers.sparse_column_with_hash_bucket(\n        categorical_feature, hash_bucket_size=1000)\n\n    engineered_features.append(tf.contrib.layers.embedding_column(sparse_id_column=sparse_column, dimension=16,combiner=\"sum\"))\n                                 \n# Training set and Prediction set with the features to predict\ntraining_set = train[FEATURES + FEATURES_CAT]\nprediction_set = train.SalePrice\n\n# Train and Test \nx_train, x_test, y_train, y_test = train_test_split(training_set[FEATURES + FEATURES_CAT] ,\n                                                    prediction_set, test_size=0.20, random_state=42)\ny_train = pd.DataFrame(y_train, columns = [LABEL])\ntraining_set = pd.DataFrame(x_train, columns = FEATURES + FEATURES_CAT).merge(y_train, left_index = True, right_index = True)\n\n# \uc81c\ucd9c\ud558\uae30\ntraining_sub = training_set[FEATURES + FEATURES_CAT]\ntesting_sub = test[FEATURES + FEATURES_CAT]","4fb5edef":"# Same thing but for the test set\ny_test = pd.DataFrame(y_test, columns = [LABEL])\ntesting_set = pd.DataFrame(x_test, columns = FEATURES + FEATURES_CAT).merge(y_test, left_index = True, right_index = True)","49430579":"training_set[FEATURES_CAT] = training_set[FEATURES_CAT].applymap(str)\ntesting_set[FEATURES_CAT] = testing_set[FEATURES_CAT].applymap(str)\n\ndef input_fn_new(data_set, training = True):\n    continuous_cols = {k: tf.constant(data_set[k].values) for k in FEATURES}\n    \n    categorical_cols = {k: tf.SparseTensor(\n        indices=[[i, 0] for i in range(data_set[k].size)], values = data_set[k].values, dense_shape = [data_set[k].size, 1]) for k in FEATURES_CAT}\n\n    # Merges the two dictionaries into one.\n    feature_cols = dict(list(continuous_cols.items()) + list(categorical_cols.items()))\n    \n    if training == True:\n        # Converts the label column into a constant Tensor.\n        label = tf.constant(data_set[LABEL].values)\n\n        # Returns the feature columns and the label.\n        return feature_cols, label\n    \n    return feature_cols\n\n# Model\nregressor = tf.contrib.learn.DNNRegressor(feature_columns = engineered_features, \n                                          activation_fn =leaky_relu, hidden_units=[256, 128, 64, 32, 16])\n","8346d1f5":"categorical_cols = {k: tf.SparseTensor(indices=[[i, 0] for i in range(training_set[k].size)], values = training_set[k].values, dense_shape = [training_set[k].size, 1]) for k in FEATURES_CAT}","8f0648e0":"# Deep Neural Network Regressor with the training set which contain the data split by train test split\nregressor.fit(input_fn = lambda: input_fn_new(training_set) , steps=2000)","f57c041d":"ev = regressor.evaluate(input_fn=lambda: input_fn_new(testing_set, training = True), steps=1)\nloss_score4 = ev[\"loss\"]\nprint(\"Final Loss on the testing set: {0:f}\".format(loss_score4))","a5cd3503":"# Model\nregressor = tf.contrib.learn.DNNRegressor(feature_columns = engineered_features, \n                                          activation_fn = tf.nn.relu, hidden_units=[2500])","1cbbd558":"# Deep Neural Network Regressor with the training set which contain the data split by train test split\nregressor.fit(input_fn = lambda: input_fn_new(training_set) , steps=2000)","7e3e2bc5":"ev = regressor.evaluate(input_fn=lambda: input_fn_new(testing_set, training = True), steps=1)\nloss_score5 = ev[\"loss\"]","312aed16":"print(\"Final Loss on the testing set: {0:f}\".format(loss_score5))","defb951a":"list_score = [loss_score1, loss_score2, loss_score3, loss_score4,loss_score5]\nlist_model = ['Relu_cont', 'LRelu_cont', 'Elu_cont', 'Relu_cont_categ','Shallow_1ku']","5e976027":"import matplotlib.pyplot as plt; plt.rcdefaults()\n\nplt.style.use('ggplot')\nobjects = list_model\ny_pos = np.arange(len(objects))\nperformance = list_score\n \nplt.barh(y_pos, performance, align='center', alpha=0.9)\nplt.yticks(y_pos, objects)\nplt.xlabel('Loss ')\nplt.title('Model compared without hypertuning')\n \nplt.show()","fbad775e":"def to_submit(pred_y,name_out):\n    y_predict = list(itertools.islice(pred_y, test.shape[0]))\n    y_predict = pd.DataFrame(prepro_y.inverse_transform(np.array(y_predict).reshape(len(y_predict),1)), columns = ['SalePrice'])\n    y_predict = y_predict.join(ID)\n    y_predict.to_csv(name_out + '.csv',index=False)","13251c49":"y_predict = regressor.predict(input_fn=lambda: input_fn_new(testing_sub, training = False))    \nto_submit(y_predict, \"submission_shallow\")","a441b6e5":"### 2. Outliers : \uc774\uc0c1\uce58 Isolation Forest","2453944f":"#### Special Thanks to [Neural Network Model for House Prices](https:\/\/www.kaggle.com\/zoupet\/neural-network-model-for-house-prices-tensorflow).\n#### \uc774 \uae00\uc740 [Neural Network Model for House Prices](https:\/\/www.kaggle.com\/zoupet\/neural-network-model-for-house-prices-tensorflow)\uc744 \ucc38\uace0\ud55c \ucee4\ub110\uc785\ub2c8\ub2e4.","c42d3c91":"tf.contrib.learn\uc740 \uae4a\uc740 \uc778\uacf5\uc2e0\uacbd\ub9dd\uc744 \uc2e4\ud589\ud558\uae30 \uc704\ud55c \uac00\uc7a5 \uc88b\uc740 \ubc29\ubc95\uc774\ub2e4. \uc6b0\ub9ac\uc758 \uacbd\uc6b0\uc5d0\ub294 200,100,50,25,12\uc758 \uac01\uac01\uc758 \uc720\ub2db\ub4e4\uc744 \uac00\uc9c4 5\uac1c\uc758 Hidden Layer\ub97c \uac00\uc9c8 \uac83\uc785\ub2c8\ub2e4. + \ud65c\uc131\ud568\uc218\ub294 relu\uac00 \ub420 \uac83\uc785\ub2c8\ub2e4. \n\uc6b0\ub9ac\uc758 \uacbd\uc6b0\uc5d0 \uc774\uc6a9\ud55c \ucd5c\uc801\ud654\ub294 Adagrad(\uae30\ubcf8\uac12)\uc785\ub2c8\ub2e4.","461119e2":"### \uc774 \uae00\uc740 \uc5ec\ub7ec\uac00\uc9c0 \ubd80\ubd84\uc73c\ub85c \ub098\ub269\ub2c8\ub2e4 :\n1. \uc911\uc694\ud55c \uc810 & \uac00\ub2a5\ud55c \uc7a5\uce58\ub4e4\n2. Outliers(\ubcf4\ud1b5\uc744 \ub118\uc5b4\uc120 \uac12\ub4e4)\n3. \uc804\ucc98\ub9ac \uacfc\uc815\n4. \uc5f0\uc18d\uc801\uc778 \ud2b9\uc131\ub4e4\uc5d0 \ub300\ud55c DNN \ud68c\uadc0\n5. \uc608\uce21\n6. Leaky Relu\uc758 \uc608\uc2dc\n7. \uc5f0\uc18d\uc801\uc778 \/ \ubd84\ub958 \ud2b9\uc131\ub4e4\uc5d0 \ub300\ud55c DNN \ud68c\uadc0\n8. \uc608\uce21\n9. \uc778\uacf5\uc2e0\uacbd\ub9dd\n10. \uacb0\ub860","ef2c62d7":"## 1. \uc911\uc694\ud55c \uc810 \ubc0f \uac00\ub2a5\ud55c \ub514\ubc14\uc774\uc2a4\ub4e4\n\ucc98\uc74c\uc5d0 \uc2dc\ub3c4\uc2dc \uac00\ub2a5\ud55c \ub514\ubc14\uc774\uc2a4\ub4e4\uc744 \ud655\uc778\ud558\ub294 \uac83\uc774 \uc911\uc694\ud558\ub2e4. \uc608\ub97c \ub4e4\uc5b4 GPU\uc5d0 \ubc1c\uc0dd\ud558\ub294 \ubb38\uc81c\ub4e4\uc774 \uc885\uc885 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4. ","78fd97bb":"\uc6b0\ub9ac\ub294 \uc9c0\uae08\uae4c\uc9c0 3\uac00\uc9c0 \ub2e4\ub978 \ud65c\uc131\ud568\uc218\ub85c 3\uac00\uc9c0 \uc81c\ucd9c\uc744 \ub0b4\uc5c8\uc2b5\ub2c8\ub2e4. \uc774\uac83\uc740 \ub2e8\uc9c0 \uc5f0\uc18d\uc801\uc778 \ud2b9\uc131\uc744 \uc774\uc6a9\ud558\uc5ec \ub9cc\ub4e0 \uac83\uc778\ub370\uc694, \ubc94\uc8fc\ud615 \ud2b9\uc131\uc744 \ub354\ud558\uba74 \ub3c4\ub2e4\ub978 \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","65bcdb3d":"### Tensorflow \uc815\ub9ac : tf.contrib.learn\uc744 \uc774\uc6a9\ud55c \uac04\ub2e8\ud55c \ub525\ub7ec\ub2dd \uad6c\ud604\n\ub77c\uc774\ube0c\ub7ec\ub9ac \ubd80\ub974\uae30\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom sklearn.model_selection import train_test_split\n\n- feature_cols = [tf.contrib.layers.real_valued_column(k) for k in FEATURES] < \ud150\uc11c\ud50c\ub85c\uc5d0\uac8c \uac12\uc744 \uc81c\uc2dc(\ubaa8\ub4e0 \uac12\uc774 \uc2e4\uc218\uac12\uc744 \uac00\uc9d0\uc73c\ub85c \ud45c\uc2dc) \ubc0f feature_cols = {k: tf.constant(data_set[k].values) for k in FEATURES} < \ud150\uc11c\ud50c\ub85c \uc0c1\uc218 \ud150\uc11c \ub9cc\ub4e4\uae30\n- regressor = *tf.contrib.learn.DNNRegressor*(feature_columns=feature_cols,activation_fn = tf.nn.relu, hidden_units=[256, 128, 64, 32, 16]) < \ubaa8\ub378 \ubd80\ub974\uae30\n- regressor.fit(input_fn=lambda: input_fn(training_set), steps=2000) < \ud559\uc2b5\ud558\uae30\n- ev = regressor.evaluate(input_fn=lambda: input_fn(testing_set), steps=1) <testing_set\uc73c\ub85c \ud3c9\uac00\ud558\uae30","7e41c2a2":"Tensorflow\ub97c \uc774\uc6a9\ud558\uae30 \uc774\ud574\uc11c\ub294 \ud2b9\ubcc4\ud55c \ud615\uc2dd\uc73c\ub85c \uc6b0\ub9ac\uc758 \ub370\uc774\ud130\ub4e4\uc744 \ubc14\uafd4\uc57c \ud569\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \uc5f0\uc18d\uc801\uc778 \ud2b9\uc131\ub4e4\ub9cc \uc788\ub294 \uac83\uc5d0 \ub530\ub77c tf.contrib.layers.real_valued_column \uc774\ub77c\ub294 \ud568\uc218\ub97c \uba3c\uc800 \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4. \ub354 \ub9ce\uc740 \uc815\ubcf4\ub294 \uc5ec\uae30\uc11c : https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/contrib\/layers\/real_valued_column\n\ntf.contrib.layer.real_valued_column : \ubaa8\ub4e0 \uac12\uc774 \uc2e4\uc218\uac12\uc784\uc744 \uc9c0\uc815","e6bc9057":"\uc774\uc0c1\uce58\ub4e4\uc740 [Isolation Forest](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.IsolationForest.html)\ub97c \uc774\uc6a9\ud558\uc5ec \ubd84\ub9ac\ud560 \uac83\uc774\ub2e4.\n>Isolation Forest\ub294 \uc758\uc0ac\uacb0\uc815\ub098\ubb34\ub97c \uc774\uc6a9\ud558\uc5ec \uc774\uc0c1\ud0d0\uc9c0\ub97c \uc218\ud589\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4. \uc5ec\uae30\uc11c\ub294 max_samples\uc5d0\uc11c \uc774\uc6a9\ud560 \ucd5c\ub300 \ud2b9\uc131\uc744 \uccb4\ud06c \ud6c4 \uac78\ub7ec\ub0b4\ub294 \ubc29\uc2dd\uc73c\ub85c \uc774\uc6a9\ub429\ub2c8\ub2e4.\nhttps:\/\/donghwa-kim.github.io\/iforest.html","ada278b9":"### 7. \uc595\uc740 \uc2e0\uacbd\ub9dd\n\uc5ec\uae30\uc11c\ub294 \ud558\ub098\uc758 Hidden Layer\uc5d0 \uc5ec\ub7ec\uac1c\uc758 \uc720\ub2db\ub4e4\uc774 \ud3ec\ud568\ub418\uc5b4\uc788\ub294 \uad6c\uc870\ub97c \ubcfc \uac83\uc785\ub2c8\ub2e4. \uc77c\ub2e8 1000\uac1c \uc815\ub3c4\ub85c \uc2e4\ud5d8\ud574 \ubcf4\uc790.","923ba081":"### 3. \uc804\ucc98\ub9ac MinMaxScaler","658bba51":"### 8. \uacb0\ub860","b99c0437":"### 6. \uc5f0\uc18d\ud615, \ubc94\uc8fc\ud615 \ud2b9\uc131 \ubaa8\ub450 \uc774\uc6a9\ud55c \uc778\uacf5 \uc2e0\uacbd\ub9dd\n\ubc94\uc8fc\ud615 \ud2b9\uc131\uc744 \ub354\ud558\ubbc0\ub85c \uac19\uc740 \ud568\uc218\ub97c \ubc18\ubcf5\ud574\uc57c \ud569\ub2c8\ub2e4.","4d5165da":"Scikit-learn\uc758 MinMaxScaler\ub97c \uc774\uc6a9\ud558\uc5ec 0\uacfc 1\uc0ac\uc774\uac00 \ub418\ub3c4\ub85d \uc6b0\ub9ac\uc758 \ub370\uc774\ud130\ub97c \ub2e4\uc2dc \uc815\ub9ac\ud569\ub2c8\ub2e4.","5f5ddf7d":"### The best solution must be","ec4c46f7":"### 5. \uc5f0\uc18d\uc801\uc778 \ud2b9\uc131\ub4e4 + Leaky Relu\ub97c \uc801\uc6a9\ud55c \uc778\uacf5\uc2e0\uacbd\ub9dd\n\uc6b0\ub9ac\ub294 \ub2e4\ub978 \ud65c\uc131\ud568\uc218 : Leaky Relu \ub97c \uc774\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4.\nLeaky Relu\uc758 \ud568\uc218\ub294 Max(x, delta * x)\uc778\ub370 \uc6b0\ub9ac\uc758 \uacbd\uc6b0 delta = 0.01","f3bed3ef":"### 4. \uc5f0\uc18d\uc801\uc778 \ud2b9\uc131\ub4e4\uc5d0 \ub300\ud55c \uae4a\uc740 \uc778\uacf5\uc2e0\uacbd\ub9dd"}}