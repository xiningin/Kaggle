{"cell_type":{"44f7f720":"code","47aec6c2":"code","366305b2":"code","d2d0a2de":"code","02085c84":"code","d331e888":"code","cffd79fb":"code","b56b78ed":"code","1cf6bd56":"code","620bf3f5":"code","f5ce0dc6":"code","4a997601":"code","2d5284db":"code","a95ebe1a":"code","e156df24":"code","3b6be22e":"code","21bffdb3":"code","c4abc715":"markdown","5d5f66e3":"markdown","a18d231c":"markdown","0148bf1b":"markdown"},"source":{"44f7f720":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\n\nimport shap\nimport gc\n\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\nfrom sklearn.feature_selection import VarianceThreshold\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","47aec6c2":"# import pandas as pd\n# compas_scores_raw = pd.read_csv(\"..\/input\/compass\/compas-scores-raw.csv\")\n# cox_violent_parsed = pd.read_csv(\"..\/input\/compass\/cox-violent-parsed.csv\")\n# cox_violent_parsed_filt = pd.read_csv(\"..\/input\/compass\/cox-violent-parsed_filt.csv\")","366305b2":"TARGET_COL = \"Two_yr_Recidivism\"","d2d0a2de":"df = pd.read_csv(\"..\/input\/compass\/propublicaCompassRecividism_data_fairml.csv\/propublica_data_for_fairml.csv\")\n\nprint(df.shape)\ndisplay(df.columns)\ndf.head()","02085c84":"data = df.drop([TARGET_COL],axis=1)\ny = df[TARGET_COL]","d331e888":"# LightGBM GBDT with KFold or Stratified KFold\n# Overkill function below based on : https:\/\/www.kaggle.com\/hmendonca\/lightgbm-predictions-explained-with-shap-0-796\ndef kfold_lightgbm(train_df, train_target, test_df, num_folds, stratified=False, debug=False):\n    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=47)\n    else:\n        folds = KFold(n_splits= num_folds, shuffle=True, random_state=47)\n    # Create arrays and dataframes to store results\n    oof_preds = np.zeros(train_df.shape[0])\n    sub_preds = np.zeros(test_df.shape[0])\n    feat_importance = pd.DataFrame()\n    scores = []\n    models = []\n    \n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, train_target)):\n        train_x, train_y = train_df.iloc[train_idx], train_target.iloc[train_idx]\n        valid_x, valid_y = train_df.iloc[valid_idx], train_target.iloc[valid_idx]\n\n        # LightGBM parameters found by Bayesian optimization\n        clf = LGBMClassifier(\n#             nthread=4,\n            #is_unbalance=True,\n#             n_estimators=10000,\n            learning_rate=0.04,\n#             num_leaves=32,\n#             colsample_bytree=0.9497036,\n#             subsample=0.8715623,\n#             max_depth=8,\n#             reg_alpha=0.04,\n#             reg_lambda=0.073,\n#             min_split_gain=0.0222415,\n#             min_child_weight=40,\n#             silent=-1,\n#             verbose=-1,\n            #scale_pos_weight=11\n            )\n\n        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n            eval_metric= 'auc', verbose= 200, early_stopping_rounds= 50)\n\n        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n        sub_preds += clf.predict_proba(test_df, num_iteration=clf.best_iteration_)[:, 1] \/ folds.n_splits\n\n        ## feature importance\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = test_df.columns.values\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"shap_values\"] = abs(shap.TreeExplainer(clf).shap_values(valid_x)[:,:test_df.shape[1]]).mean(axis=0).T\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feat_importance = pd.concat([feat_importance, fold_importance_df], axis=0)\n        \n        scores.append(roc_auc_score(valid_y, oof_preds[valid_idx]))\n        print('Fold %2d AUC : %.6f' % (n_fold + 1, scores[n_fold]))\n        models.append(clf)\n        del clf, train_x, train_y, valid_x, valid_y, fold_importance_df\n        gc.collect()\n\n    score = roc_auc_score(train_target, oof_preds)\n    print('Full AUC score %.6f' % score)\n    print('Mean AUC score %.6f' % np.mean(scores))\n    # Write submission file and plot feature importance\n    if not debug:\n        pd.DataFrame(oof_preds).to_csv(\"lgb{:03}_{:.5f}_train_oof.csv\".format(test_df.shape[1], score), index=False)\n        sub_df = pd.read_csv('..\/input\/sample_submission.csv')\n        sub_df['TARGET'] = sub_preds\n        sub_df.to_csv(\"lgb{:03}_{:.5f}.csv\".format(test_df.shape[1], score), index= False)\n    display_shapley_values(feat_importance)\n    return feat_importance, models, scores\n\n# Display\/plot feature importance\ndef display_importances(feat_importance):\n    best_features = feat_importance[[\"feature\", \"importance\"]].groupby(\"feature\")[\"importance\"].agg(['mean', 'std']) \\\n                                                              .sort_values(by=\"mean\", ascending=False).head(40).reset_index()\n    best_features.columns = [\"feature\", \"mean importance\", \"err\"]\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"mean importance\", y=\"feature\", xerr=best_features['err'], data=best_features)\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.show()\n\n# Display\/plot shapley values\ndef display_shapley_values(feat_importance):\n    best_features = feat_importance[[\"feature\", \"shap_values\"]].groupby(\"feature\")[\"shap_values\"].agg(['mean', 'std']) \\\n                                                               .sort_values(by=\"mean\", ascending=False).head(40).reset_index()\n    best_features.columns = [\"feature\", \"mean shapley values\", \"err\"]\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"mean shapley values\", y=\"feature\", xerr=best_features['err'], data=best_features)\n    plt.title('LightGBM shapley values (avg over folds)')\n    plt.tight_layout()\n    plt.show()","cffd79fb":"# # train_df = df[df['TARGET'].notnull()][feats]\n# # train_target = df[df['TARGET'].notnull()]['TARGET']\n# # test_df = df[df['TARGET'].isnull()][feats]\n\n\n# %%time\n# feat_importance, models, scores = kfold_lightgbm(train_df, train_target, test_df, num_folds=5, stratified=False, debug=debug)","b56b78ed":"#Create train and validation set\ntrain_x, valid_x, train_y, valid_y = train_test_split(data, y, test_size=0.25, shuffle=True, stratify=y, random_state=42)\n\n## ----------Dataset for  LightGBM Model  -----------------------\ntrain_data=lgb.Dataset(train_x,label=train_y)\nvalid_data=lgb.Dataset(valid_x,label=valid_y)","1cf6bd56":"params = {'metric' : 'auc',\n          'boosting_type' : 'gbdt',\n          'colsample_bytree' : 0.92,\n#           'max_depth' : -1,\n#           'n_estimators' : 200,\n          'min_child_samples': 4, \n#           'min_child_weight': 0.1,\n          'subsample': 0.85,\n#           'verbose' : -1,\n          'num_threads' : 4\n}","620bf3f5":"lgbm = lgb.train(params,\n                 train_data,\n#                  2500,\n                 valid_sets=valid_data,\n                 early_stopping_rounds= 15,\n                 verbose_eval= 30\n                 )","f5ce0dc6":"y_pred = lgbm.predict(valid_x)\nscore = roc_auc_score(valid_y, y_pred)\nprint(\"Overall AUC on validation (not test!): {:.3f}\" .format(score))","4a997601":"import shap\nlgbm.params['objective'] = 'binary' ## workaround\n%time shap_values = shap.TreeExplainer(lgbm).shap_values(data)","2d5284db":"shap.summary_plot(shap_values, data)\n# shap values over all the data","a95ebe1a":"%time shap_values = shap.TreeExplainer(lgbm).shap_values(valid_x)","e156df24":"shap.dependence_plot(\"African_American\", shap_values[1], valid_x)\n# plot over the validation data","3b6be22e":"shap.dependence_plot(\"score_factor\", shap_values[1], valid_x)\n## we see that the cutoff misses quite a few \"outliers\" at the 0.5 cutoff. ","21bffdb3":"shap.dependence_plot(\"Number_of_Priors\", shap_values[1], valid_x)","c4abc715":"* We see that shap gives the most importance to the model score. Followed by # priors and age, then gender. Race has the wekest effect, mainly if the individual is African american. \n\n\n* Let's look at some Partial dependency plots over the validation data: (Plot a feature vs another variable. e.g. target =1 ). ","5d5f66e3":"## We'll start with the naive fairML subset of the data. Very simple\n* **Target column**: `Two_yr_Recidivism` = recividism (any) within 2 years\n* Note that we have fewer variables and features here.\n","a18d231c":"## Next steps:\n* Try different models\n* try different feature importance metrics - e.g. perturbation or naive model importance\n* Try the full compass dataset and variables , and different targets (e.g. violent recividism vs any recividism).\n* Probe the racial bias. (e.g. FP\/TP and the age+gender+priors as counfounders)","0148bf1b":"### Getting started with feature importance and fairness using ML & SHAP\n\n\n* We'll look at feature importance according to ML models. \n* We can use multiple methods - e.g. permutation importance, shapley, and the  difference in feature importance from different models"}}