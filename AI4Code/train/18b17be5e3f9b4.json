{"cell_type":{"25c9d2da":"code","c0b2569e":"code","5f642d53":"code","29085e18":"code","291ab275":"code","61129d94":"code","4d2bd16d":"code","0ec6b032":"code","a6cffe5f":"code","7a205317":"code","3d55191d":"code","c0ec07b7":"code","59dbdfd3":"code","7dabd25e":"code","12718515":"code","b438cd03":"code","55bbd9c9":"code","93823c71":"code","97aa9600":"code","4645c74e":"code","ba5e5260":"code","f8dd89d5":"code","f3d97589":"code","31342117":"code","45d425a2":"code","b6123234":"code","87a3662f":"code","f2463704":"code","6f71ec6b":"code","fced6cc0":"markdown","bf83959d":"markdown","eccec981":"markdown","5a5cad87":"markdown","5bf87e81":"markdown","290cc9b6":"markdown","93c671a1":"markdown","3b0d738e":"markdown","87e02b17":"markdown","bc8bc0fb":"markdown","0aec3772":"markdown","60f9ca1d":"markdown","d76aa01c":"markdown","fe1d1a7f":"markdown","40b27095":"markdown","57dae3a0":"markdown","328eb1ac":"markdown","595cb7bf":"markdown","1682c10c":"markdown","cb007831":"markdown","2f0351f0":"markdown","51f54846":"markdown","0b001ad7":"markdown"},"source":{"25c9d2da":"from sklearn import set_config\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split as split\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.metrics import roc_curve as roc\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nimport warnings\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.preprocessing import KBinsDiscretizer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nimport category_encoders as ce\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import TimeSeriesSplit\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import precision_score\nimport warnings\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nwarnings.filterwarnings('ignore')\n\nset_config(display='diagram')\n\n","c0b2569e":"\npath= r'..\/input\/loansdata\/RepaymentsData.csv'\npath2= r'..\/input\/loansdata\/LoanData.csv'\n\n\ndf= pd.read_csv(path, parse_dates=[2], infer_datetime_format=True)\n\nloans= pd.read_csv(path2,index_col=1, parse_dates=[11,13]\\\n                   , infer_datetime_format=True)\n\nloans.shape \n","5f642d53":"pd.options.display.float_format = '{:,.2f}'.format\nloans[['Gender','Age','IncomeTotal','LoanDuration','Amount'\n       ,'LiabilitiesTotal']].describe()","29085e18":"\n# loans basic filtering: \n\n#amount bigger than 200:\nloans['LoanDate']= pd.to_datetime(loans['LoanDate'])\nloans= loans[ (loans['Amount']>=200)  & (loans['LoanDate']>'2011-03-01') & (loans['LoanDate']<'2020-01-01')]\n\n# loans per user name up to 5:\nloans_per_user= loans.groupby('UserName')['LoanNumber'].count()\nloans_per_user.value_counts().plot(kind='bar')\nlons_per_user_filter= loans_per_user[loans_per_user<=5]\n\n#age 18+:\nloans= loans[loans['UserName'].isin(lons_per_user_filter.index)]\nloans= loans[loans['Age']>=18]\n\n#eliminate current status\nloans = loans[loans['Status']!='Current']\n\n#incomeTotal\nloans = loans[loans['IncomeTotal']<30000]\n\n#LiabilitesTotal\nloans = loans[loans['LiabilitiesTotal']<15000]\n\nplt.title(\"number of loans taken vs. amount of users\");\n","291ab275":"# data cleaning and uniting rare categories:\n\n# (uniting categories should help with tree regularization)\n\n# gender - change to names. 0='man', 1='woman', 2='unknown'\nloans=loans.replace(\n{'Gender' : { 0.0 : 'male', 1.0 : 'woman', 2.0: 'unknown' }}) \n     \n# country : drop slovakia - only 300 records almost all late\nloans= loans[loans['Country'] !='SK']\n\n# education : unite 4,5 as  higher , unite 1 and 3 as lower\/technical\nloans= loans[loans['Education']>0] # remove nulls (very few)\nloans=loans.replace(\n{'Education' : { 2.0 : 'basic', 4.0 : 'higher', 5.0: 'higher'\n            ,1.0:'lower\/technical', 3.0:'lower\/technical'}}) \n\n# loan_duration: add feature : two years or less (~8,000 values)\nloans['under_2years_loan']= np.where(loans['LoanDuration']<=24,'True','False')\n\n# loan hour sign - add feature -- before 8 AM (more chance for Late)\nloans['sign_after_7AM']= np.where(loans['ApplicationSignedHour']>7, '+', '-')\n\n# employment status -add feature - filled or didn't filled (more chance for Late)?\nloans['filled_details'] = np.where(loans['EmploymentStatus']> 0 , 'yes', 'no')\n\n# 'EmploymentDurationCurrentEmployer': unite rare : 2years, 3years, 4years.  unite trial to 1 year.   5 years or more, others\nloans=loans.replace(\n{'EmploymentDurationCurrentEmployer' : { 'UpTo1Year' : 'trial to 1 year', \n                                        'UpTo2Years' : '2 to 4 years'\n                                        , 'UpTo3Years' : '2 to 4 years' , \n                                        'UpTo4Years':'2 to 4 years'\n                                        , 'TrialPeriod':'trial to 1 year' \n                                        ,'UpTo5Years':'5 years or more', \n                                        'MoreThan5Years':'5 years or more'\n                                        ,'Other':'Others', 'Retiree':'Others'}})\n\n# HomeOwnershipType : unite 1 and 6 to 9 (ownership) drop -1, 0 (homeless)\nloans= loans[loans['HomeOwnershipType']>0]\nloans=loans.replace(\n{'HomeOwnershipType' : { 1.0 : 'owner or partly', 6.0 : 'owner or partly',\n                        7.0 : 'owner or partly' , 8.0: 'owner or partly',\n                        9.0: 'owner or partly'\n                                        ,2.0:'tenant or others', 3.0:'tenant or others',\n                        4.0:'tenant or others', 5.0:'tenant or others', 10.0:'tenant or others'}}) \n\n#rating- : unite the rare ratings- A AA B to C as prime. replace nulls with D (medium)\nloans['Rating'].isnull().sum()# 648 nulls (less than 1%)\nloans['Rating']= loans['Rating'].fillna('D')\nloans=loans.replace(\n{'Rating' : { 'AA' : 'prime', 'A' : 'prime', 'B': 'prime', 'C': 'prime' \n            ,'D':'medium', 'E':'subprime', 'F':'subprime', 'HR': 'subprime'}}) \n\n\n#credit in Finland :['CreditScoreFiAsiakasTietoRiskGrade'] :\n\n# nulls are very few and ignored\n\nloans=loans.rename(columns={'CreditScoreFiAsiakasTietoRiskGrade':'credit_finland'})\nloans=loans.replace({'credit_finland':\n{ 'RL1' : 11, 'RL2' : 12, 'RL3': 13, 'RL4': 14 ,'RL5':15}}) \n\nloans['credit_finland']= loans['credit_finland'].fillna(0) # not from finland \nloans['credit_finland']= loans['credit_finland'].astype('int')\n\n#  drop 7,8,14,15:\nloans= loans[  \n             (loans['credit_finland']!=7) &\n             (loans['credit_finland']!=8) &\n             (loans['credit_finland']!=14) &\n             (loans['credit_finland']!=15) \n             ]\n#unite:\nloans=loans.replace({'credit_finland':\n{ 1 : 'fin_prime', 2 : 'fin_prime', 12 : 'fin_prime', 13: 'fin_prime',\n 11: 'fin_prime',\n 3:'fin_prime',\n 4:'fin_sub', 5:'fin_sub', 6:'fin_sub', 0:'not_finland'}}) \n\n\n\n#credit in spain : drop AA D AAA, nulss will be imputed later on with knn classifier:\nloans= loans[ (loans['CreditScoreEsEquifaxRisk']!='AA') &\n             (loans['CreditScoreEsEquifaxRisk']!='AAA') &\n             (loans['CreditScoreEsEquifaxRisk']!='D') ]\n\n\nloans= loans.rename(columns={\"CreditScoreEsEquifaxRisk\": \"credit_spain\"})\nloans['credit_spain']= np.where((loans['Country']!='ES'), \"not_spain\",\n                                                    loans['credit_spain'] )\n\n\n#credit in estonia -  : fill na with 1000\n# devide to prime and sub\nloans['CreditScoreEeMini']= np.where((loans['Country']=='EE')& (loans['CreditScoreEeMini'].isnull()),\n                                     1000.0, loans['CreditScoreEeMini'])\n\nloans['CreditScoreEeMini']= loans['CreditScoreEeMini'].fillna('not_estonia')                                     \nloans['credit_estonia']= np.where(loans['CreditScoreEeMini']==1000.0,'est_prime',\n                            np.where (loans['CreditScoreEeMini']!='not_estonia', 'est_sub',\n                                      loans['CreditScoreEeMini']))\n","61129d94":"\n\n#filter payments table according to loans table:\ndf= df[df['loan_id'].isin(loans.index) ]\n\nprint(loans['Status'].value_counts())\n\n#order by loan_id:\ndf= df.sort_values(by=['loan_id','Date']).reset_index(drop=True)\n\n# calculate days passed between every payment:\ndf['date2']= df['Date'].shift(periods=1).where(df['loan_id'].eq(df['loan_id'].shift()))\n\ndf['days_passed']= df['Date']-df['date2']\n\n# turn days passed to numeric\ndf['days_numeric']= df['days_passed'].astype('str').str.extract('(\\d+)').astype('float')\n\n# calculat days passed from FirstPaymentDate:\ndf= df.join(loans['FirstPaymentDate'],on= df['loan_id'])\ndf= df.join(loans['LoanDate'] ,on= df['loan_id'])\n\ndf['days_first_pay']= (df['LoanDate']-df['FirstPaymentDate'])\\\n                .astype('str').str.extract('(\\d+)').astype('float')\n\n# add status column :\ndf= df.join(loans['Status'], on= df['loan_id'])\n\n# add amount column (total loan to pay):\ndf= df.join(loans['Amount'], on= df['loan_id'])\n\n# add Monthylypayment\ndf= df.join(loans['MonthlyPayment'], on= df['loan_id'])\n\n# add loan duration\ndf= df.join(loans['LoanDuration'], on= df['loan_id'])\n\n#add claculated monthly payment with pmt function:\nloans['calc_month_payment']= -1* (np.pmt(loans['Interest']\/1200, \n                                         loans['LoanDuration'], loans['Amount']))\n\ndf= df.join(loans['calc_month_payment'], on= df['loan_id'])\n\n# add total monthly payment :\ndf['total_paid_this_month']= df['InterestRepayment']+ df['PrincipalRepayment']\\\n                                                    +df['LateFeesRepayment']\n\n# fill na in monthly payment with calculated payment:\ndf['MonthlyPayment']= df['MonthlyPayment'].combine_first(df['calc_month_payment'])\n\n# replce 0 payment with calculate payment:\ndf['MonthlyPayment']= np.where(df['MonthlyPayment']==0, df['calc_month_payment'],\n                               df['MonthlyPayment'])\n\n# calculate monthly return precantage:\ndf['return_prec']=100*(df['total_paid_this_month']\/df['MonthlyPayment'])\n\n# add overall return as column:\ndf['overall_paid']= df['loan_id'].map(df.groupby('loan_id')['total_paid_this_month'].sum())\n","4d2bd16d":"\nclass DurationCat  (TransformerMixin, BaseEstimator):  # TransformerMixin needed for fit_transform\n    \n    def __init__(self):\n        super().__init__() # needed for heriatance\n        pass\n        \n    def fit (self ,X ,y= None):\n        \n        self.df_paid_lates= X[(X['days_numeric']<183)&\n                                              (X['days_numeric']>27)]\n        self.late_avg = self.df_paid_lates.groupby(['duration_type'])\\\n                                        ['days_numeric'].mean().mean()\n        self.late_std = self.df_paid_lates.groupby(['duration_type'])\\\n                                        ['days_numeric'].std().mean()        \n\n        self.on_time_  = round(self.late_avg)  #35 days\n        self.late_regular_= self.on_time_ + round(self.late_std ) #50\n        self.very_late_ =  self.on_time_ + 2*round(self.late_std ) #65\n        \n        return self\n        \n    def transform(self, X ):\n        X['days_numeric']= X['days_numeric'].fillna(0)\n        X['late_payment_cat']= \\\n        np.where(X['days_numeric']< self.on_time_ ,\n                                        'on_time', \n        np.where((X['days_numeric']>= self.on_time_ ) &\n                        (X['days_numeric']< self.late_regular_), \n                                   'late_regular', \n        np.where((X['days_numeric']>= self.late_regular_)&\n                        (X['days_numeric']<= self.very_late_),\n                                   'very_late'\n                                   , 'Default')))\n\n        return  X #pd.DataFrame(X['late_payment_cat'].T)\n         ","0ec6b032":"\nclass ReturnEvaluation (TransformerMixin, BaseEstimator):  # TransformerMixin needed for fit_transform\n    \n    def __init__(self):\n        super().__init__() \n        pass\n        \n    def fit (self ,X ,y= None):\n        #percentage of loan returned monthly on avergae for every loan_duration_category:\n        df_paid_filter = X[(X['return_prec']<130) & (X['return_prec']>25)]  \n        # filtering to avoid large STD:\n        self.month_re_perc= df_paid_filter.groupby('duration_type')\\\n        ['return_prec']\\\n                        .agg( mean='mean'\n                           , median='median',\n                              std='std',\n                            count='count')\n        #defing limits for every payment-category with avg and std's:\n        self.normal_ = round(self.month_re_perc.loc\\\n                             ['2 years or more','median']-\\\n                     self.month_re_perc.loc['2 years or more','std']) #70%\n        self.partial_ = round(self.month_re_perc.loc\\\n                              ['2 years or more','median']-\\\n                      2* self.month_re_perc.loc\\\n                              ['2 years or more','std'])  #50%\n        self.irregular_ = round(self.month_re_perc.loc\\\n                                ['2 years or more','median']-\\\n                 3* self.month_re_perc.loc['2 years or more','std']) #30%\n \n        return self\n        \n    def transform(self, X ):\n        X['return_perc_category']= \\\n        np.where(X.return_prec<self.irregular_, \\\n                 f'10% -{self.irregular_}%', \n        np.where((X.return_prec>=self.irregular_) & \\\n                 (X.return_prec<self.partial_)\\\n                 , f'{self.irregular_}%-{self.partial_}%',\n        np.where((X.return_prec>=self.partial_) & \\\n                 (X.return_prec<self.normal_)\\\n                 , f'{self.partial_}%-{self.normal_}%',\n        np.where((X.return_prec>=self.normal_) & \\\n                 (X.return_prec<130) \\\n                 , f'{self.normal_}%-130%', 'more or 130% '))))\n        \n        \n        X['adjusted_payment_cat']=np.where((X['return_perc_category']==\n                                         f'10% -{self.irregular_}%') \\\n                             |(X['return_perc_category']==\n                               f'{self.irregular_}%-{self.partial_}%'),\\\n                            'late_regular',  X['late_payment_cat'])\n\n        return  X ","a6cffe5f":"\n        \nclass AllowedLateCat (TransformerMixin, BaseEstimator):  \n    \n    def __init__(self):\n        super().__init__() \n        pass \n    \n    def fit (self ,X ,y= None):\n        # for every loan: count how many records for each \"late\" category              \n        self.late_payment_by_loan_duration_id =  X.groupby(\n            [ 'loan_id','duration_type','adjusted_payment_cat'])\\\n        ['loan_id'].agg(count='count').reset_index()\n \n        #based on averages: the allowed times for each \"late\" category-\n        self.late_payment_by_loan_duration_ =\\\n        self.late_payment_by_loan_duration_id.groupby(\\\n            ['duration_type','adjusted_payment_cat'])['count'].mean()\n\n        return self\n    \n    def transform(self, X ):\n   # for every loan: count how many records for each \"late\" category              \n        eval_payment =  X.groupby([ 'loan_id','duration_type'\n                                   ,'adjusted_payment_cat'])\\\n                    ['loan_id'].agg(count='count').reset_index()             \n    #adding the calculated allowed late payments to agg-by-loan table:\n        eval_payment =eval_payment.set_index(['duration_type',\n                                              'adjusted_payment_cat'])\n        eval_payment=\\\n        eval_payment.join(self.late_payment_by_loan_duration_,\n                                              lsuffix='_origin', \n                                                rsuffix='_calc')        \n    #filter \"on time\" and mark \"default\" when counted is bigger than allowed:\n        default_late_loans=eval_payment.reset_index()\n        default_late_loans =default_late_loans[\n                        default_late_loans['adjusted_payment_cat']\\\n                                                     != 'on_time']\n    #mark default loans in new column:\n        default_late_loans['is_default']= default_late_loans['count_origin']>\\\n        round(default_late_loans['count_calc'])\n    #update default column if at least 1 defualt payment duration exists:\n        default_late_loans['is_default']=\\\n        np.where((\n                default_late_loans['adjusted_payment_cat']=='Default')\\\n                    & (default_late_loans['count_origin']>0),\n                         True, default_late_loans['is_default'])        \n    #late loans deduced from average tendancy:\n        default_late_loans=\\\n        default_late_loans[default_late_loans['is_default']==\n                                    True]['loan_id'].unique()    \n    #late loanes because didnt reach overall sum: \n        default_overall_sum=\\\n        X [ X['Amount'] > X['overall_paid'] ]['loan_id'].unique()\n             \n    #uniting both lates :      \n        self.to_relabel= pd.concat(\n            [pd.Series(default_late_loans),pd.Series(default_overall_sum)])\n    # relabel y (Status) by the list of defaults:\n        X['Status']=np.where(X['loan_id']\\\n                     .isin(self.to_relabel),'Late', X['Status'])\n        return X\n        ","7a205317":"# divide to repaid and late :\n\ndf_paid= df[ df['Status']=='Repaid'  ]\ndf_late= df[ df['Status']=='Late'  ]\n\n#erase payments under 10% from df_paid:\ndf_paid= df_paid[df_paid['total_paid_this_month']>=10]#.reset_index\n\n# #late loanes becuse didnt reach overall sum: (assuming at least 3% income)\ndefault_overall_sum= df_paid [ df_paid['Amount'] > (df_paid['overall_paid']-\n                               0.03*df_paid['Amount'])]['loan_id'].unique()\n\n# categorize by loan duration type:\ndf_paid['duration_type']= \\\nnp.where( df_paid.LoanDuration <12 , 'under 1 year',\nnp.where((df_paid.LoanDuration >=12) & (df_paid.LoanDuration <23)\n                         , '1 year to 2 years', '2 years or more'))\n\n\n# number of days it takes to pay a monthly payment- for label \"late\" and label \"repaid\"\n\nfig, ax = plt.subplots()\n\nplt.hist(df_paid['days_numeric'], bins=1500 )\nplt.xlim(-1,120)\nplt.ylim(0,120000)\n\n\nplt.hist(df_late['days_numeric'], bins=1500, color='red', alpha=0.3)\nplt.xlim(-1,120)\nplt.ylim(0,120000)\nplt.title(\"days between payments- Red: Late, Blue: Repaid\");\nplt.show()\n","3d55191d":"\nloans= loans.reset_index()\n\nloans_new= loans [['LoanDate','LoanId','Age', 'Gender', \n                   'Country','LoanDuration',\n                    'Amount', 'Education', \n            'EmploymentDurationCurrentEmployer', 'HomeOwnershipType',\n                   'IncomeTotal',   \n            'ExistingLiabilities', 'LiabilitiesTotal', \n            'Rating', 'credit_spain',\n            'credit_finland', 'credit_estonia','under_2years_loan',\n              'sign_after_7AM','filled_details', 'Status'    ]]\n\nloans_new= loans_new.set_index('LoanDate',drop=True)\n\nloans_new= loans_new.sort_index(ascending= True) # ordered by date\n\nloans_new= loans_new.set_index('LoanId',drop=True) # get rids of date","c0ec07b7":"\n\nX=loans_new.drop('Status',axis=1)\ny=loans_new.Status\n\n# this makes a time-series split (train= past, test=\"future\"):\nX_train= X.iloc[:int(len(X)*0.9),:]\nX_test= X.iloc[int(len(X)*0.9):,:]\n\ny_train= y[:int(len(X)*0.9)]\ny_test= y[int(len(X)*0.9):]\n\n\n# from df_paid take only the rows that belong to X_train's loans:\ndf_paid_train =df_paid[ df_paid['loan_id'].isin(X_train.index)]\n\n# from df_paid take only the rows that belong to X_tests's loans:\ndf_paid_test = df_paid[ df_paid['loan_id'].isin(X_test.index)]\n","59dbdfd3":"\n\nrelabeling_pipeline =        \\\nPipeline([(\"duration\", DurationCat() ), (\"return\",  ReturnEvaluation() ),\n          (\"allowed\", AllowedLateCat() ) ])\n\ntrain_pipe= relabeling_pipeline.fit_transform(df_paid_train)\\\n[relabeling_pipeline.fit_transform(df_paid_train)['Status']=='Late' ]['loan_id'].unique()\n\ntest_pipe= relabeling_pipeline.transform(df_paid_test)\\\n[relabeling_pipeline.transform(df_paid_test)['Status']=='Late']['loan_id'].unique()\n\nrelabeling_pipeline.fit(df_paid_train)         ","7dabd25e":"\ny_train_new = np.where(y_train.index.isin(train_pipe),'Late',y_train)\n\ny_test_new=  np.where(y_test.index.isin(test_pipe),'Late',y_test  )\n\n\nrelabeling_pipeline.steps[0][1].df_paid_lates.groupby(\n    'return_perc_category')['return_perc_category'].count().plot(kind='bar')\nplt.title(\"trnsformer 2: loan return vs amount of payments\");\n","12718515":"relabeling_pipeline.steps[2][1].late_payment_by_loan_duration_.unstack().drop(\n    \"on_time\",axis=1).plot(kind='bar')\nplt.legend(bbox_to_anchor=(0.98, 1.4), bbox_transform=ax.transAxes)\nplt.title(\"transformer 3: allowed late payments map\");\n\nrelabeling_pipeline.steps[2][1].to_relabel.nunique()","b438cd03":"print(\"new train set's labeling:\")\nprint(pd.DataFrame(y_train_new).iloc[:,0].value_counts())\nprint('\\n')\nprint(\"new test set's labeling:\")\nprint(pd.DataFrame(y_test_new).iloc[:,0].value_counts())\n","55bbd9c9":"\ndef plot_var(col_name, continuous, X_train, y_train_new, fig, l):\n    \"\"\"\n    Visualizing numeric and categorical columns by the loan status.\n    \"\"\"\n    inner = gridspec.GridSpecFromSubplotSpec(1, 2,\n                    subplot_spec=outer[l], wspace=0.2, hspace=0.15)\n    \n    ax1, ax2 = plt.Subplot(fig, inner[0]), plt.Subplot(fig, inner[1])\n    \n    # Plot without loan status\n    if continuous ==1:\n        sns.distplot(X_train.loc[X_train[col_name].notnull(), col_name], kde=False, ax=ax1)\n    else: # catagorical\n        sns.countplot(X_train[col_name], color='#5975A4', saturation=1, ax=ax1)\n    ax1.set_xlabel(col_name)\n    ax1.set_ylabel('Count')\n    ax1.set_title(col_name)\n\n    # Plot with loan status\n    if continuous ==1:\n        limit= X_train[col_name].mean()+2*(X_train[col_name].std() )\n        loans_with_limit= loans[loans[col_name]<=limit]\n        sns.boxplot(x=col_name, y=y_train_new, data=pd.DataFrame(y_train_new)\n                    .join(X_train.reset_index()), ax=ax2)\n        ax2.set_ylabel('')\n        ax2.set_title(col_name+ ' by Loan Status')\n    else: #catagorical\n        y_train_new= y_train_new =='Repaid'\n        X_train=X_train.reset_index()\n        charge_off_rates = pd.Series(y_train_new).groupby(X_train[col_name]).mean()\n        sns.barplot(x=charge_off_rates.index, y=charge_off_rates.values, \n                    color='lightgreen', saturation=1,  ax=ax2)\n        ax2.set_ylabel('Fraction of Loans')\n        ax2.set_title('Repaid Rate by ' + col_name)\n    ax2.set_xlabel( col_name)\n    \n    fig.add_subplot(ax1)\n    fig.add_subplot(ax2)\n    return fig\n    \n   # plt.tight_layout()\n\n\n\ncat_cols= ['Gender', 'Education', 'EmploymentDurationCurrentEmployer', 'Country',\n  'HomeOwnershipType', 'Rating', 'credit_spain',\n        'credit_finland', 'credit_estonia', 'filled_details']\n# 'under_2years_loan', 'sign_after_7AM',\n # 'filled_details']\n\nnum_cols= ['Age', 'IncomeTotal', 'Amount',\n 'ExistingLiabilities',  'LiabilitiesTotal', 'LoanDuration']\n\ncats=[0 for i in range(len(cat_cols))]\n\nnums=[1 for i in range(len(num_cols))]\n\ncols_tuples= list(zip(cat_cols ,cats))\ncols_tuples.extend( list(zip(num_cols, nums)) )\n\nimport matplotlib.gridspec as gridspec\n\nfig = plt.figure(figsize=(25, 25))\nouter = gridspec.GridSpec(8, 2, wspace=0.1, hspace=0.35)\nl=0\n\nfor i in cols_tuples:\n#  'new_rating']\n    f= plot_var (i[0], i[1], X_train, y_train_new, fig, l)\n    l=l+1\n    \nf.tight_layout()\n\nplt.show()\n        \n","93823c71":"X_train= X_train.drop([\"Country\",\"LoanDuration\"],axis=1)\nX_test = X_test.drop([\"Country\", \"LoanDuration\"],axis=1)","97aa9600":"\nclass KnnNullImputingSpainCredit (TransformerMixin, BaseEstimator):\n    \n    def __init__(self):\n        super().__init__()\n        self.ct = ColumnTransformer([\n        ('std', StandardScaler(), ['Age','IncomeTotal', 'Amount',\n                                   'ExistingLiabilities', \n                                         'LiabilitiesTotal'])],\n       remainder='passthrough')\n        \n        self.knn_init= KNeighborsClassifier(n_neighbors=3 )\n        \n\n    def fit(self, X, y=None):\n        X_temp= X[['Age','IncomeTotal', 'Amount','ExistingLiabilities',\n                   'LiabilitiesTotal','credit_spain']]\n        #scaling only needed columns:\n        X_train_scaled= pd.DataFrame(self.ct.fit_transform(X_temp), \n                                     columns= ['Age','IncomeTotal',\n                                             'Amount','ExistingLiabilities', \n                                           'LiabilitiesTotal','credit_spain'])\n        \n        X_train_knn =X_train_scaled.dropna()\n        #!!!REMEMBER : IN Null IMPUTING YOU DONT USE Y! THE COLUMN IN X TO BE IMPUTED WILL BE Y\n        \n        y_train_knn= X_train_knn ['credit_spain']\n        X_train_knn = X_train_knn.drop('credit_spain', axis=1)\n        \n        self.knn_class_= self.knn_init.fit(X_train_knn, y_train_knn)\n\n        return self\n    \n    def transform(self, X):\n# columns= ['weight', 'wingspan', 'radar'] -- whole columns in X\n        X_temp= X[['Age','IncomeTotal', 'Amount','ExistingLiabilities',\n                   'LiabilitiesTotal','credit_spain']]\n        \n        X_scaled= pd.DataFrame(self.ct.fit_transform(X_temp),\n                               columns= ['Age',\n                                          'IncomeTotal', 'Amount',\n                                            'ExistingLiabilities',\n                                         'LiabilitiesTotal'\n                                              ,'credit_spain'])\n        \n        X_scaled['new_credit_spain']= self.knn_class_.predict(\n            X_scaled[['Age','IncomeTotal', \n                      'Amount',\n                    'ExistingLiabilities', \n                     'LiabilitiesTotal']]) \n        \n        X_scaled.index= X.index # beacuse index had auto reset after scaling\n        \n        X['credit_spain']= X_scaled['credit_spain'].combine_first(\n            X_scaled['new_credit_spain'])\n\n        return X\n","4645c74e":"class MyBinsEncoder(TransformerMixin, BaseEstimator):\n    \n    def __init__(self):\n        super().__init__()\n        self.df_numeric_to_bins=KBinsDiscretizer(4, encode='ordinal', strategy='quantile',)\n        \n        \n    def fit(self, X, y=None):\n        #cols = X.select_dtypes([np.number]).columns\n        self.cols=['Age','IncomeTotal', 'Amount','ExistingLiabilities', \n                   'LiabilitiesTotal']\n        self.df_numeric_to_bins.fit(X[self.cols])\n               \n        return self\n    \n    def transform(self, X):\n        X= X.reset_index(drop=True)\n        others_X =  X.drop(self.cols, axis=1)\n         # df_with_col_name_2= X[self.cols]\n        cols_qcut_bins= self.df_numeric_to_bins.transform(X[self.cols])\n        df_with_col_name=pd.DataFrame(cols_qcut_bins)\n        df_with_col_name.columns=self.cols  #give the qcut columnes names\n        df_with_col_name_2= df_with_col_name.apply(\n            lambda x: x.astype('int').astype('str')+'_'+x.name)\n\n        df_with_col_name_2= df_with_col_name_2.reset_index()\n        combined_X= others_X.join (df_with_col_name_2)\n        combined_X= combined_X.drop('index', axis=1)\n        return combined_X\n        #return   cols_qcut_bins\n         # # return df_with_col_name_2\n        #return df_with_col_name_2\n    \n    def edge(self,X):\n        \n        bins_edges=self.df_numeric_to_bins.bin_edges_\n        return bins_edges\n       ","ba5e5260":"class TargetRating  (TransformerMixin, BaseEstimator):  # TransformerMixin needed for fit_transform\n    \n    def __init__(self, class_limit= 0.025):\n        super().__init__() \n        self.class_limit= class_limit\n    \n        \n    def fit (self ,X ,y):\n\n        self.limit= round((y.value_counts()\/ len(y)).values[1],3)\n\n        self.joiner_= pd.DataFrame()\n        columns = X.columns              \n    #combine aggregates to one big agg table:\n        for col in columns :\n            col_agg= y.groupby(X[col]).mean()\n            # print(col_agg)\n            self.joiner_=pd.concat( [self.joiner_ ,col_agg])\n    #drops un-necessary weights from weights table:\n        label= self.joiner_.index[self\n                                  .joiner_.index.astype('str')\n                                  .str.contains('not')]\n        \n        self.joiner_= self.joiner_.drop(labels=\n                                    [label[0],label[1],label[2]])\n    #back to series:\n        self.joiner_=self.joiner_.squeeze()\n\n        return self\n    \n    \n    def transform(self, X ):\n        out= X.copy()\n        X= X.apply(lambda element: element.map(self.joiner_) ,axis=1)\n\n        X['avg_rating']= X.apply(lambda row: row.mean(), axis=1)\n        \n        X['new_rating']= np.where(X['avg_rating']-self.class_limit >self.limit\n                                  ,'A class',\n                                 np.where((X['avg_rating']-self.class_limit <self.limit) \n                                          &\n                                          (X['avg_rating'] >self.limit),\n                                          'B class', \n                                          'C class' ))\n        out['new_rating']= X['new_rating']\n        self.X_encoded= X\n        return out  \n\n    ","f8dd89d5":"class OurOneHotEncoder(TransformerMixin, BaseEstimator):\n    \n    def __init__(self):\n        super().__init__()\n        self.cat_cols= ['Gender', 'Education', 'EmploymentDurationCurrentEmployer',\n       'HomeOwnershipType', 'Rating', 'credit_spain', 'credit_finland',\n       'credit_estonia', 'under_2years_loan', 'sign_after_7AM',\n       'filled_details', 'Age', 'IncomeTotal', 'Amount', 'ExistingLiabilities',\n       'LiabilitiesTotal', 'new_rating']\n        \n        self.hotcoder= ce.OneHotEncoder (cols=self.cat_cols, use_cat_names=True)\n        \n    def fit(self, X, y=None):\n        self.hotcoder.fit(X)\n\n        return self\n    \n    def transform(self, X):\n        X_new= self.hotcoder.transform(X)\n        self.x_new=X_new\n        return X_new\n","f3d97589":"X_train= X_train.reset_index(drop=True)\nX_test= X_test.reset_index(drop=True)\n\ny_train_new =pd.Series(y_train_new )\ny_test_new =pd.Series(y_test_new )\n\ny_train_new= pd.Series(np.where(y_train_new.values== 'Repaid', 1, 0),\n                       name='Status')\ny_test_new= pd.Series(np.where(y_test_new.values== 'Repaid', 1, 0),\n                      name='Status')\n \nmodel1 =xgb.XGBClassifier()\n\nmodel2 =CatBoostClassifier()\n\nclassifiers= [(\"xgb\", model1), (\"cbc\", model2)]\n\nfrom sklearn.ensemble import VotingClassifier\n\nclf_voting= VotingClassifier(estimators=classifiers, voting='soft',\n                             weights=None)\n\nfinal_model =     \\\nPipeline( steps= [  (\"knn_imputer\", KnnNullImputingSpainCredit() )\n                   ,(\"cut\", MyBinsEncoder() )\n                   ,(\"rating\", TargetRating(class_limit= 0.025) ) \n                   ,(\"ohe\", OurOneHotEncoder())\n                   ,(\"voter\",clf_voting)] )\nfinal_model    ","31342117":"test=X_test.join(y_test_new)\ntrain=X_train.join(y_train_new)\n\ntest['Status'].cumsum().plot()\ntrain['Status'].cumsum().plot()\n\nplt.title(\"cumsum plot of repaid loans\")","45d425a2":"\n\nparams_store= final_model.get_params()\n\nparam_search = {\n            'voter__cbc__bagging_temperature': [0.5],\n            'voter__cbc__depth': [4],\n            'voter__cbc__iterations': [100],\n            'voter__cbc__l2_leaf_reg':[25],\n            'voter__cbc__learning_rate': [0.05],\n            'voter__cbc__sampling_frequency': ['PerTreeLevel'],\n            'voter__cbc__leaf_estimation_method': ['Gradient'],\n            'voter__cbc__random_strength': [0.8],\n            'voter__cbc__feature_border_type': ['MaxLogSum'],\n            'voter__cbc__max_ctr_complexity': [2],\n            'voter__cbc__fold_len_multiplier': [2],\n                \"voter__xgb__learning_rate\"    : [0.045] ,\n                \"voter__xgb__max_depth\"        : [ 3],\n                \"voter__xgb__min_child_weight\" : [ 5],\n                \"voter__xgb__gamma\"            : [ 0.0],\n                \"voter__xgb__colsample_bytree\" : [1] ,\n                \"voter__xgb__scale_pos_weight\" : [1],\n                \"voter__xgb__n_estimators\": [100]       }\n    \n\n\nscorers = {\n    'precision_score': make_scorer(precision_score)}\n\ntscv = TimeSeriesSplit(n_splits=3) # this return 3 arrays each for ever time split of the data\ngsearch = GridSearchCV(estimator=final_model, cv=tscv, scoring=scorers,\n                        refit='precision_score',  param_grid=param_search, verbose=0 ) #n_jobs=4\n\ngsearch.fit(X_train, y_train_new);\n\ny_test_pred= gsearch.best_estimator_.predict(X_test);\n\n","b6123234":"\ncv_report= pd.DataFrame(gsearch.cv_results_) # gives accuracy score \n\nconf_matrix= confusion_matrix(y_test_new ,y_test_pred)\n","87a3662f":"pd.options.display.float_format = '{:,.5f}'.format\nreport= pd.DataFrame(classification_report(y_test_new ,y_test_pred ,output_dict=True))\n\n\nreport","f2463704":"pd.DataFrame(conf_matrix)","6f71ec6b":"proba_test = gsearch.best_estimator_.predict_proba(X_test)[:,1]\nproba_train= gsearch.best_estimator_.predict_proba(X_train)[:,1]\n\ntable= pd.DataFrame(y_test_new.to_numpy(), proba_test).reset_index() \ntable.columns=['y_pred','y_original']\n                                                            \ntable2= table[table['y_pred']>0.76]\n              \nprint(\"probabilty for 'Repaid' status\" ,table2['y_original'].sum()\/len(table2))\n\nprint(\"number of loans: \",len(table2))","fced6cc0":"explaining cross-validation with time series split:\n\nthe set was split to 4 parts by their index (ordered by date)\nthe left column in table below is the train set and the right is the test set\nso in the first round the first part predicts on the 2, 3 and 4 parts and so on.\nthis assures the train always contains the \"older\" part of the set.\n\n| train cv periods |  test cv periods|  |\n| --- | --- | --- |\n| 1 | 2+3+4 |  |\n| 1+2 | 3+4|  |\n| 1+2+3 | 4 |  |","bf83959d":"using the repayment table:\n\nthe repayments table describes every transcations that occured in every loan that appears in the loans table\n\nthis table will be used for relabeling with the help of the following steps:\n1. data enrichment of repayment table with loans table info (adding columns like amount and loan duration)\n2. adding calaulated columns (days passed, monthly payment, overall return)\n3. null imputing of the monthly payment column with the calculated one (pmt funcion)\n3. aggregating and labeling every transcations with the 3 transformers below \n","eccec981":"the plot above is the map that we use to alter the loan's label.\na loan is allowed to have as many late payments as this plot shows. more means a \"late\" label.","5a5cad87":"**spliting to X and y as time series** :\n\nfirst we order by date then we can split so the train simulates \"past\" and test is used as \"future\" time","5bf87e81":"**This project was made with the cooperation of Orel Mishael as part of Data Science course assignments**","290cc9b6":"**3. AllowedLateCat transformer**\n\nthis transformer relabels 'Status' column in X as a final step.\n\nfirst it adds an agg table of allowed late payments, joins its info and relabel according to it.\n","93c671a1":" 2. **ReturnEvaluation transformer**\n \n this transformer adds two columns :\n \n-- 'return_perc_category' : after finiding average return and std we categorize the returns for each payment\n\n-- 'adjusted_payment_cat' : low returns change the 'late_payment_category' to late returns\n\n        ","3b0d738e":"fitting and operating the Pipeline:","87e02b17":"---\nto decided if a loan is going to be relabeled \"late\", first we will label the repayments in the following cases :\n\n    1-  they were repaid too late (relative to norm..) -- see transformer 1\n    \n    2-  the payments are too low (also relatively)  --- see transformer 2\n    \n\nthen the  whole loan itself will be labeled \"late\" if:        \n    \n    3-  the amount of payments that were late or incomplete is bigger than the allowed amount --- see transformer 3\n    \n    4-  the total payment is lower than total loan taken","bc8bc0fb":"conclusion: \n1. we got mediocre precision and very poor recall for the \"repaid\" loans. so it seems that it's close to impossible to predict the \"positive\" human behaviour in Bondora site beyond some simple rules (your chances are much better when lending small amounts for short peirods of time for people from estonia but thats about it..) \n\n2. we get a very good prediction for the \"LATE\" loans. one might say it's not a difficult task when we have  about 80% \"LATE\" loans in the dataset, but still  the model managed to find alomost all of them, which might come in handy. \n\n3. this means you can avoid the bad loans easily, but the model can only find vey few loans that he predicts will be repaid. For bondora this is bad news because this means their rating scale is misleading and not usable, and even worse- there are vey few solid options for good invesment.  this \"invetory\" of solid investments  will run out very fast, and the inevitable failures should scare away the coservative investor.\n\n4. for the sole lender that didn't run away-- it's good news. since one only needs few good loans to make a decent revenue.\n   for example -if you want to maximize your precision ,you can reach up to 88%, \n   but this will only be true to around 120 loans. for one loaner it's seems enough.\n\n5. So, to sum it up -- using training dataset of ~80,000 loans yielded  about 120 \"sure bet\" loans - this means that we are left with the problem of getting enough options from bondora to choose from in the first place which are still relevant for today's loaners.  coronavirus case  for instance can transform the training set to something obsolete..\n\nanother issue is the revenue you get from the loans. for better results, we had to tune down the expected revenue to close to zero when we relabeled the dataset. (for example,  even a loan that returned 0.001% interest was labeled \"repaid\").\nso this means not all returns will be as good as you might expect and **surely** not as good as advertised..  but at least you got your money back and helped a person in need! (as long he is from estonia and needs **very** few euro's, anyways..).\n\n-the end-\n\n","0aec3772":"---\n\n1. **DurationCat transformer**\n\nthis transformer adds a column :\n\n'late_payment_cat': add a category for each duration of payment calculated with the limits (based on average and std of loans)\n","60f9ca1d":"![Bondora_capture.PNG](attachment:Bondora_capture.PNG)","d76aa01c":"we can see from the plot above that most payments are in the 70%+ region.\n\nwe wil use this information to set a limit for inedquate payments. \n\npayments that are smaller than the limit minus one standard deviation will be labeled as \"Late regular\"\n\n","fe1d1a7f":"we are loading two tables- loans and repayments (df)\n\nloans has many columns (mostly regarding the target, not the X) -- only 13 are used as features!","40b27095":"explaining this scheme :\n1. splitting loans to test and train to avoid data leakage and to create a time series data-set format\n2. choosing the transcations in the repayment table that appear in the loans table\n3. creating \"df_paid_train\" table which  contains the transactions that appear in the X_train part of loans\n4. fitting the transformers below to \"df_paid_train\" (this will be the X in our transformers)\n5. transforming df_paid_test ,which relabels the loans it contains.\n6. after transforming , df_paid_test is used to relabel y_test  (and df_paid_train is used to relabel y_train)","57dae3a0":"What does TargetRating transformer do?\n\n\n![image.png](attachment:image.png)","328eb1ac":"**X transformers pipeline**:\n\nfor categorical features: we can see from the plots above that some sub-categories are bigger doners to the\n\"repaid\" status than others, but since the differnces are too small,  we must combine all the doners to one agrregated category which will become the new rating for the loan (transformer 2)\n\nfor numeric features: we see the same trend but here we must transform the feature to categorical first with bins-encoder. this will help the model differentiate between the sub-cagories that have more influence on the \"repaid status\" (transformer 1)\n\nthe steps in the pipeline:\n\n1. Null imputing : using knn clasifier to fill nulls in spain_credit (about 50% nulls) using the numeric columns.\n2. BinsEncoder:  basically Qcut on numerics that stores the info where the cuts where done and can be passed to the Test set .\n3. Target encoder :  adding a columns- new rating - based on the average \"repaid\" label ratio in all the features.\n4. One hot encoding : done for all of the column.\n5. last step in this pipeline is the prediction. since this set have a non-linear non-monotonous decision making character we will use tree-like estimators (XGB an Catboost).\n\n   ","595cb7bf":"   **y Transformers scheme**\n   \nloans -> X_test , y_test  (split by time)\n                                                                                            \n    loans--> df + columns-->df_paid-> \n                                    ->df_paid_train-->_paid_test-->y_test_new         \n                            X_train->  \n                                                                       ","1682c10c":"final step: changes y's : ","cb007831":"In this project we tried to predict which loans will be repaid (or not) in a Peer-to-Peer model - Bondora's site.\n\nbeacuse we saw early-on that the site's labeling of \"Repaid\" and \"Late\" was questionable just like it's promise of earnings\n\n(many records had debts, many were long over-dues before finally being paid, and some had strange debt write-offs which were not explaind , but all still were labeld as \"Repaid\")\n\nwe decided to take matters into our hands and so this project became a **relabeling and predicting project**","2f0351f0":"# Classification Project- Peer to Peer Lending- Bondora\n---","51f54846":"the cumsum plot above shows steady accumulation of \"repaid\" loans alnog the timeline that was chosen.\nbut this is not so true for the last coronavirus-inflicted months of 2020 which were not taken into consideration here.","0b001ad7":"lonas basic filtering : age, income, liabilities.  also not using loans after covid-19 pandemic started\n\nloans that have a \"Current\" status, have not been resolved yet so are not usable in our case."}}