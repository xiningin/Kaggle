{"cell_type":{"aebdb087":"code","a173a8f8":"code","e73bd3a7":"code","1069438a":"code","c3caee23":"code","05c063a3":"code","e1112574":"code","1f801e11":"code","af8fa283":"code","0b4ca474":"code","3130a59d":"code","7846e513":"code","56334661":"code","28f4ee6e":"code","c22c962c":"code","0fca2c76":"code","90562678":"code","a78a02c4":"code","b3ce8829":"code","1f987c08":"code","c05e8058":"code","b925de7c":"code","394fd464":"code","755d01ea":"code","623b8012":"code","7f6f87fd":"code","78bd5b3d":"code","dcb4d527":"markdown","375bc8a6":"markdown","c1683398":"markdown","e0ba1fa2":"markdown","98da5a4c":"markdown","b08e0060":"markdown","5ac435a8":"markdown","874167d8":"markdown","0bf5b1c3":"markdown","7b500f95":"markdown","f3feefc2":"markdown"},"source":{"aebdb087":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a173a8f8":"# To print multiple output in a cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'","e73bd3a7":"os.getcwd()","1069438a":"# Load Libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\nfrom keras.layers import Dense,Embedding,LSTM,Dropout,Bidirectional,Input,BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\n","c3caee23":"train = pd.read_csv(\"..\/input\/av-recommendation-systems\/train_mddNHeX\/train.csv\")\ntest = pd.read_csv(\"..\/input\/av-recommendation-systems\/test_HLxMpl7\/test.csv\")\n\ntrain.head()\ntest.head()\n","05c063a3":"train[train.challenge_sequence > 10]","e1112574":"# Create labels\nlabel = train[train.challenge_sequence > 10][['user_id','challenge']]\nlabel.rename(columns={'challenge':'label'},inplace=True)","1f801e11":"label.head()","af8fa283":"# Treat the sequence of challenges as text\ndf = train[train.challenge_sequence <= 10].groupby('user_id').challenge.aggregate(lambda x: ' '.join(x)).reset_index()\ndf.head()","0b4ca474":"# Merge Labels\ndf = df.merge(label)\ndf.head()\ndf.shape","3130a59d":"\n# Validation split for early stopping\ndf_train, df_validation = train_test_split(df.sample(frac=1,random_state=123), test_size=0.05, random_state=123)\n","7846e513":"df_train.head()\ndf_validation.head()\n\ndf_train.shape\ndf_validation.shape","56334661":"# Load all the challenges\nchallenges = pd.read_csv('..\/input\/av-recommendation-systems\/train_mddNHeX\/challenge_data.csv')\n\nchallenges.head()","28f4ee6e":"len(challenges.challenge_ID.unique())","c22c962c":"len(df_train.label.unique())","0fca2c76":"df_train.head()","90562678":"# Encode challenges\nencoder = LabelEncoder()\nencoder.fit(challenges['challenge_ID'])\n\ndf_train['brand_id_encoded'] = encoder.transform(df_train.label)\ndf_validation['brand_id_encoded'] = encoder.transform(df_validation.label)\n\n# Tokenize text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df_train['challenge'])\n","a78a02c4":"len(tokenizer.word_index)","b3ce8829":"# Constants\nNB_WORDS = len(tokenizer.word_index)+1\nMAX_SEQUENCE_LENGTH = 10\nN_CATEGORIES = challenges.shape[0]\n\n# Create sequences\nsequences_train = tokenizer.texts_to_sequences(df_train['challenge'])\nsequences_validation = tokenizer.texts_to_sequences(df_validation['challenge'])\n\n# Pad sequences\nx_train = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\nx_validation = pad_sequences(sequences_validation, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n\n# Set Labels\ny_train = df_train['brand_id_encoded'].values\ny_validation= df_validation['brand_id_encoded'].values","1f987c08":"y_train\ny_validation","c05e8058":"# NN architecture\ndef get_model(path='',lr=0.001):\n    adam = Adam(lr=lr)\n    inp = Input(shape=(MAX_SEQUENCE_LENGTH, ))\n    x = Embedding(NB_WORDS,256)(inp)\n    x = BatchNormalization()(x)\n    x = Bidirectional(LSTM(256, dropout=0.1, recurrent_dropout=0.1))(x)\n    x = Dropout(0.4)(x)\n    x = Dense(N_CATEGORIES, activation=\"softmax\")(x)\n    model = Model(inputs=inp, outputs=x)\n    if path != '':\n        model.load_weights(path)\n    model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n    return model","b925de7c":"# Initialize the model\nmodel = get_model()","394fd464":"# Model callbacks\npath = 'best_model_weights'\nes_callback = EarlyStopping(monitor='val_loss')\nmc_callback = ModelCheckpoint('{}.hdf5'.format(path), monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)\ncallbacks = [es_callback,mc_callback]","755d01ea":"# Fit the model\nmodel.fit(x_train,\n          y_train,\n          epochs=100,\n          batch_size=500,\n          validation_data=(x_validation, y_validation),\n#           callbacks = callbacks\n         )","623b8012":"# Load best weights\n# model = get_model('{}.hdf5'.format(path))\n\nmodel=model","7f6f87fd":"# Test preprocessing\ndef padding(text):\n    return pad_sequences(tokenizer.texts_to_sequences(text), maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\ntest_text = test[test.challenge_sequence <= 10].groupby('user_id').challenge.aggregate(lambda x: ' '.join(x)).reset_index()\nx_test = padding(test_text.challenge)\n\n# Get top 3 predictions for each user\npred = model.predict(x_test,batch_size=2048)\npred = pred.argsort(axis=1)[:,-3:][:,::-1]","78bd5b3d":"# Write Predictions\ndf_list = []\nfor i in range(3):\n    test_11 = test_text[['user_id']]\n    test_11['user_sequence'] = test_11.user_id.astype(str) + '_'+str(i+11)\n    test_11['challenge'] = encoder.inverse_transform(pred[:,i])\n    df_list.append(test_11[['user_sequence','challenge']])\npd.concat(df_list).to_csv('bes_submission8.csv',index=False)\n","dcb4d527":"![](https:\/\/raw.githubusercontent.com\/AIVenture0\/JanataHack-Recommendation-Systems\/master\/data_description2.jpg)","375bc8a6":"## Data Description\nfor data desciption, there are three files:\n\n- ```train.csv```: It contains the set of 13 challenges that were attempted by the same user in a sequence.\n- ```challenge_data.csv```: Contains attributes related to each challenge\n- ```test.csv```: Contains the first 10 challenges solved by a new user set (not in train) in the test set. We need to predict\n","c1683398":"## <center> JantaHack : Recommendation Systems<\/center>\n_______________","e0ba1fa2":"## Note:","98da5a4c":"<center><img src=\"https:\/\/datahack-prod.s3.ap-south-1.amazonaws.com\/__sized__\/contest_cover\/cover_81npx51-thumbnail-1200x1200.png\"\/><\/center>","b08e0060":" If this kernel helped you: \n - Do upvote \n - Do follow \n - In case you have any query use comment section.","5ac435a8":"> - Your client is a fast-growing mobile platform, for hosting coding challenges. They have a unique business model, where they crowdsource problems from various creators(authors). These authors create the problem and release it on the client's platform. The users then select the challenges they want to solve. The authors make money based on the level of difficulty of their problems and how many users take up their challenge.\n\n \n> \n> - The client, on the other hand makes money when the users can find challenges of their interest and continue to stay on the platform. Till date, the client has relied on its domain expertise, user interface and experience with user behaviour to suggest the problems a user might be interested in. You have now been appointed as the data scientist who needs to come up with the algorithm to keep the users engaged on the platform.\n\n## What to recommend\n\n> - The client has provided you with history of last 10 challenges the user has solved, and you need to predict which might be the next 3 challenges the user might be interested to solve. Apply your data science skills to help the client make a big mark in their user engagements\/revenue.","874167d8":"**Credit**: This version of notebook is highly inspired from ybabakhin works, For more detail check his github repo\n\n- https:\/\/github.com\/ybabakhin\/mckinsey\/blob\/master\/Mckinsey.py","0bf5b1c3":"![](https:\/\/raw.githubusercontent.com\/AIVenture0\/JanataHack-Recommendation-Systems\/master\/data_description1.jpg)","7b500f95":"### Problem Statement","f3feefc2":"## Reading the train and test data"}}