{"cell_type":{"9abf5f54":"code","a647cf97":"code","85ef3a6f":"code","e074a932":"code","bb3c36d7":"code","cfb74766":"code","3c9e92a5":"code","ffeb7da1":"code","c95c062d":"code","624c7a54":"code","86dcb138":"code","63fe34f6":"code","19eef6c2":"code","dd4af76e":"code","2e41abfd":"code","0fadf54f":"code","321f297b":"code","2dc90f36":"code","5a379441":"code","9c981d32":"code","150ac4c9":"code","653a7db3":"code","2ac6b8e0":"code","2c725e63":"code","913adb74":"code","47e885cf":"code","4ed6d35c":"code","4c17e101":"code","d3f6c071":"code","1bc5324c":"code","3bf9daed":"code","d1c8f132":"markdown","c083cd0f":"markdown","3446e61e":"markdown","1de26d59":"markdown","f7c7965e":"markdown"},"source":{"9abf5f54":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","a647cf97":"df = pd.read_csv(\"..\/input\/train.csv\",index_col=  0)","85ef3a6f":"df.head()","e074a932":"# To change the names of DataFrame's Columns\ndf.columns = [\"label\",\"message\"]","bb3c36d7":"df.head()","cfb74766":"# To check all the messages are string or not\nall(df['message'].apply(lambda x : type(x)==str))","3c9e92a5":"# Change all the messages to string format\ndf['message'] = df['message'].apply(lambda x : str(x))","ffeb7da1":"import nltk","c95c062d":"df[\"length of message\"] = df[\"message\"].apply(len)","624c7a54":"df.head()","86dcb138":"# To plot a countplot to view how many SPAM messages are there and how many HAM\nsns.countplot(\"label\", data = df)","63fe34f6":"# To visualize the length of the messages based on the label\nasdf = sns.FacetGrid(data = df, col = 'label')\nasdf.map(sns.distplot, 'length of message', kde = False, hist_kws = dict(edgecolor = \"k\"))","19eef6c2":"from nltk.corpus import stopwords\nimport string","dd4af76e":"# These are the most common word which we have to remove from text messages\nstopwords.words(\"english\")","2e41abfd":"# We need to remove punctuations too\nstring.punctuation","0fadf54f":"# function for preprocessing\ndef all_words(msg):\n    no_punctuation = [char for char in msg if char not in string.punctuation]\n    no_punctuation = \"\".join(no_punctuation)\n    word = [word for word in no_punctuation.split() if word.lower() not in stopwords.words(\"english\")]\n    return word","321f297b":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer","2dc90f36":"bag_of_words_transformer = CountVectorizer(analyzer=all_words).fit(df[\"message\"])","5a379441":"len(bag_of_words_transformer.vocabulary_)","9c981d32":"# This will create the sparse matrix of all the messages based on the frequecy of words in that message\nmessage_bow = bag_of_words_transformer.transform(df['message'])","150ac4c9":"# This is the shape of sparse matrix\n# 310 is no. of message\n# 1406 is the no. of words after preprocessing\nmessage_bow.shape","653a7db3":"tfid_transformer = TfidfTransformer().fit(message_bow)","2ac6b8e0":"message_tfid = tfid_transformer.transform(message_bow)","2c725e63":"# Here Naive bayes has been used for traning\nfrom sklearn.naive_bayes import MultinomialNB\nspam_detection_model = MultinomialNB().fit(message_tfid,df['label'])","913adb74":"test = pd.read_csv(\"..\/input\/test.csv\", index_col = 0)","47e885cf":"test.head()","4ed6d35c":"test[\"'text'\"] = test[\"'text'\"].apply(lambda x: str(x))","4c17e101":"test_message_bow = bag_of_words_transformer.transform(test[\"'text'\"])","d3f6c071":"test_message_tfid = tfid_transformer.transform(test_message_bow)","1bc5324c":"# Prediction\ntest[\"'label'\"] = spam_detection_model.predict(test_message_tfid)","3bf9daed":"sns.countplot(test[\"'label'\"])","d1c8f132":"# Traning","c083cd0f":"# Preprocessing","3446e61e":"# Testing","1de26d59":"*So we need to do following things for preprocessing <br>*\n*1. Remove the Punctuations <br>*\n*2. Remove the most common words*","f7c7965e":"*For testing we need to get the Tfidf of all the messages*"}}