{"cell_type":{"417f2981":"code","253e2750":"code","788bba12":"code","959ca16e":"code","7eb3ef51":"code","b66ccf19":"code","f92eedc3":"markdown","a4649a20":"markdown","72dc4dbe":"markdown","7815edc9":"markdown"},"source":{"417f2981":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/cap-4611-2021-fall-assignment-4'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-4\/train.csv\", low_memory = False)\neval_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-4\/eval.csv\", low_memory = False)\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","253e2750":"train_data","788bba12":"train_data.isnull().values.any()","959ca16e":"train_data = train_data.drop(labels = [\"id\"], axis = 1) \neval_data = eval_data.drop(labels = [\"id\"], axis = 1)\ny_train = train_data[\"label\"]\nx_train = train_data.drop(labels = [\"label\"], axis = 1)\n\nx_train = x_train \/ 255.0\neval_data = eval_data \/ 255.0\n\nx_train = x_train.values.reshape(60000, 28, 28, 1)\neval_data = eval_data.values.reshape(10000, 28, 28, 1)\n\ny_train = to_categorical(y_train, num_classes = 10)\n\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, random_state = 2)","7eb3ef51":"model = Sequential()\nmodel.add(Conv2D(filters = 32, kernel_size = 3, activation = 'relu'))\nmodel.add(Conv2D(filters = 32, kernel_size = 3, activation  ='relu'))\nmodel.add(MaxPool2D(pool_size = 2))\nmodel.add(Dropout(0.6))\nmodel.add(Conv2D(filters = 64, kernel_size = 3, activation = 'relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = 3, activation = 'relu'))\nmodel.add(Dropout(0.7))\nmodel.add(MaxPool2D(pool_size = 2))\nmodel.add(Flatten())\nmodel.add(Dense(500, activation='relu'))\nmodel.add(Dropout(0.8))\nmodel.add(Dense(10, activation = 'softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nmodel.fit(x_train, y_train, epochs = 1000, batch_size = 4000, verbose = 1, validation_split = 0.1)","b66ccf19":"results = model.predict(eval_data)\nresults = np.argmax(results, axis = 1)\nresults = pd.Series(results, name = \"label\")\nsubmission = pd.concat([pd.Series(range(0, 10000), name = \"id\"), results], axis = 1)\nsubmission.to_csv(\"submission.csv\", index = False)\nprint(submission.to_string())","f92eedc3":"# Prep the data","a4649a20":"# Submission","72dc4dbe":"# Build the model","7815edc9":"# Check for missing values"}}