{"cell_type":{"636d6a90":"code","9c0605f4":"code","917bf6af":"code","9f4f2390":"code","e0817f10":"code","46f6b1ea":"code","727b40c7":"code","c8349beb":"code","38ec1638":"code","f552c1f2":"code","200aa886":"code","d927835c":"code","9f7c9440":"code","bb7b3c33":"code","228113c0":"code","c6a5428f":"code","359cd409":"code","e9b84f13":"code","92574a6a":"code","2023df92":"code","1a5361ff":"code","d1aa812d":"code","813f2436":"code","5c033b0b":"code","27525b76":"code","a4a71754":"code","3e099bf9":"code","7d9505c1":"code","699cbfc7":"code","f278c9b4":"code","3cd3f1a5":"code","f135fd8f":"code","ecde58bd":"code","9822101c":"code","2fdc08ce":"code","c7e64c42":"code","456206e1":"code","b53b40fa":"code","1b30ca54":"code","bf58cd03":"code","aa9c7d4f":"code","c3b5d6c5":"code","6be8102a":"markdown","88fb040b":"markdown","2f60741e":"markdown","7bd66906":"markdown","a1ddaf46":"markdown","87af2b9d":"markdown","888f9401":"markdown","17278622":"markdown","02514b3b":"markdown","a99b2e0c":"markdown","55a23d65":"markdown","7cfe24d9":"markdown","e930af56":"markdown","b7918abb":"markdown","86c0c758":"markdown","201a92c9":"markdown","26a98505":"markdown","dd3f724e":"markdown","02dd3cbe":"markdown","9a97e861":"markdown","ed748865":"markdown","2484de41":"markdown","431c14db":"markdown","e9ec3347":"markdown","f5b5c9ef":"markdown","27931c24":"markdown","743fd33a":"markdown","57c354cf":"markdown","30e72842":"markdown","b1c146c5":"markdown","da644176":"markdown","587dcb9d":"markdown","1764f078":"markdown","eeba8b1d":"markdown","19f7fda9":"markdown","ef7f2f3b":"markdown","49e9255c":"markdown","c6eb579b":"markdown","9333af70":"markdown","9fbc2324":"markdown","8aa74da5":"markdown","113de8c6":"markdown","d1b6bd5d":"markdown","dd3d7045":"markdown","590e3cf4":"markdown","3dc26df1":"markdown","ee4b863c":"markdown","4a60d1a7":"markdown","be1f388f":"markdown","4afc6785":"markdown","0e5c9537":"markdown","74f3bac0":"markdown","2f9832cd":"markdown"},"source":{"636d6a90":"%%capture\n!pip install kmeanstf\n!pip install tabulate\n!pip install elasticsearch","9c0605f4":"import numpy as np\nimport pandas as pd \nimport json\nimport os\nimport pickle\nimport time\nimport copy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import cdist \nimport sklearn.metrics as metrics\nimport nltk\nnltk.download('stopwords', quiet=True)\nfrom nltk.corpus import stopwords\nimport tensorflow as tf\nfrom kmeanstf import KMeansTF\nfrom tabulate import tabulate\nfrom mpl_toolkits.mplot3d import Axes3D\nimport pylab\nimport matplotlib.cm as cm\nimport tensorflow.compat.v1 as tf\nimport tensorflow_hub as hub\nfrom elasticsearch import Elasticsearch\nfrom elasticsearch.helpers import bulk\nfrom scipy import spatial","917bf6af":"def read_json_data(data_list, input_path): \n    '''\n    Inputs: \n        - data_list: json file paths\n        - input_path: input_path\n        \n    Output:\n        - dataframe containing: \n              'paper_id', \n              'titles', \n              'abstracts', \n              'introductions', \n              'conclusions', \n              'full_bodytext', \n              'bodysections',\n              'body_text_citations', \n              'context_title_list', \n              'cite_start', \n              'cite_end', \n              'cite_mark'\n    This function is used to parse json files to return the output elements\n    '''\n    \n    bibentries_title = []\n    bibentries_token = []\n    for json_file in range(0, len(data_list)):\n        bibentries_token.append(list(data_list[json_file]['bib_entries'].keys()))\n\n    for token_num, token_list in enumerate(bibentries_token):\n        bibentry_title = []\n        for token_len, token in enumerate(token_list):\n            bibentry_title.append(data_list[token_num]['bib_entries'][token]['title'])\n        bibentries_title.append(bibentry_title)\n        \n    titles = []\n    all_info = []\n    paper_id = []\n    search_abstracts = []\n    for json_file in range(0, len(data_list)):\n        paper_id.append(data_list[json_file]['paper_id'])\n        titles.append(data_list[json_file]['metadata']['title'])\n        all_info.append(data_list[json_file]['body_text'])\n        try:\n            search_abstracts.append(data_list[json_file]['abstract'])\n        except IndexError:\n            search_abstracts.append(None)\n        except KeyError:\n            search_abstracts.append(None)\n\n    abstracts = []\n    for texts in search_abstracts:\n        local_abstract = []\n        if texts is not None:\n            for num in range(0, len(texts)):\n                local_abstract.append(texts[num]['text'])\n        abstracts.append(' '.join(local_abstract))\n\n    bodysections = []\n    full_bodytext = []\n    introductions = []\n    conclusions = []\n    cite_tokens = []\n    cite_start = []\n    cite_end = []\n    cite_mark = []\n    \n    for text_info in all_info:\n        local_info = []\n        local_cite_token = []\n        local_cite_start = []\n        local_cite_end = []\n        local_cite_mark = []\n        local_introduction = []\n        local_conclusion = []\n\n        for info_len in range(0, len(text_info)):\n            if text_info[info_len]['section'] == 'Introduction':\n                local_introduction.append(text_info[info_len]['text'])\n            elif text_info[info_len]['section'] == 'Conclusion':\n                local_conclusion.append(text_info[info_len]['text'])\n            local_info.append(text_info[info_len]['text'])\n        for indices in text_info:\n            for cite_spans in indices['cite_spans']:\n                local_cite_token.append(cite_spans['ref_id'])\n                local_cite_start.append(cite_spans['start'])\n                local_cite_end.append(cite_spans['end'])\n                try:\n                    local_cite_mark.append(cite_spans['text'])\n                except KeyError:\n                    local_cite_mark.append(None)\n        introductions.append(''.join(local_introduction))\n        conclusions.append(''.join(local_conclusion))\n        full_bodytext.append(' '.join(local_info))\n        bodysections.append(local_info)\n        cite_tokens.append(local_cite_token)\n        cite_start.append(local_cite_start)\n        cite_end.append(local_cite_end)\n        cite_mark.append(local_cite_mark)\n\n    bib_dict_list = []\n    for bib_ref, bib_ttl in (zip(bibentries_token, bibentries_title)):\n        bib_dict = {}\n        for bib_bib_ref, bib_bib_ttl in zip(bib_ref, bib_ttl):\n            bib_dict[bib_bib_ref] = bib_bib_ttl\n        bib_dict_list.append(bib_dict)\n\n    context_title_list = []\n    for cite_val, bib_val in (zip(cite_tokens, bib_dict_list)):\n        cite_set = cite_val\n        bib_set = set(bib_val)\n        context_title_temp = []\n        for value in cite_set:\n            for val in bib_set:\n                if value == val:\n                    context_title_temp.append(bib_val[value])\n                elif value == None:\n                    context_title_temp.append(None)\n                    break\n        context_title_list.append(context_title_temp)\n        \n    \n    fields = {\n              'paper_id': paper_id[0], \n              'titles': titles[0], \n              'abstracts': abstracts[0], \n              'introductions': introductions[0], \n              'conclusions': conclusions[0], \n              'full_bodytext': full_bodytext[0], \n              'bodysections': bodysections[0],\n              'context_title_list': context_title_list[0], \n              'cite_start': cite_start[0], \n              'cite_end': cite_end[0], \n              'cite_mark': cite_mark[0]\n            }\n    return fields","9f4f2390":"input_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\n\nstart = time.time()\nf_name_list = ['biorxiv_medrxiv\/biorxiv_medrxiv', 'comm_use_subset', 'noncomm_use_subset', 'custom_license\/custom_license']\nfiles = []\nfor f_name in f_name_list:\n    for root, dirs, file in os.walk(input_path + f_name):\n        for fname in file:\n            files.append(os.path.abspath(os.path.join(root, fname)))  \n\nfile_list = []\nfolder_list = []\nfor l in files:\n    with open(l) as json_data_l:\n        data_list = json.load(json_data_l)\n        context_data = read_json_data([data_list], input_path=input_path)\n    file_list.append(context_data)\nfolder_list.append(file_list)\n\nflatten_data = [item for sublist in folder_list for item in sublist]\ncontext_data = pd.DataFrame.from_dict(flatten_data)\n\nprint('Elapsed time: ', time.time() - start)\n\n# pickle.dump(context_data, open('context_data.df', 'wb'))\n# print('Data file size: ', os.path.getsize('context_data.df')\/(1024*1024))","e0817f10":"context_data.head(3)","46f6b1ea":"context_data.tail(3)","727b40c7":"\ndef create_features_tfidf(data, columns=['title'], max_features=None):\n    corpus = pd.DataFrame(columns=['text'])\n    corpus['text'] = data[columns].dropna().agg('\\n'.join, axis=1)\n    \n    vectorizer = TfidfVectorizer(stop_words='english', max_features=max_features)\n\n    x = vectorizer.fit_transform(corpus['text'].values)\n    \n    return x, vectorizer\n","c8349beb":"\ncolumns_to_consider = ['titles', 'abstracts', 'full_bodytext']\nx, vectorizer = create_features_tfidf(context_data, columns=columns_to_consider, max_features=100)\n\nprint(x.shape)\n# pickle.dump(x, open('tfidf_matrix.pkl', 'wb'))","38ec1638":"features = tf.convert_to_tensor(x.toarray(), dtype=tf.float32)\nkm = KMeansTF(n_clusters=10, random_state=1)\nkm.fit(features)\nlabels = km.predict(x.toarray())\ns_score = metrics.silhouette_score(x.toarray(), labels)\ndb_score = metrics.davies_bouldin_score(x.toarray(), labels)\n\nprint(s_score, db_score)\npickle.dump([km, labels], open('clustering_kernel.pkl', 'wb'))","f552c1f2":"'''\n[km, labels] = pickle.load(open('clustering_kernel.pkl', 'rb'))\nx = pickle.load(open('tfidf_matrix.pkl', 'rb'))\n\ncolors = cm.rainbow(np.linspace(0, 10, 100))\n\n\n################### 3D Visualization ######################\n# Use t-SNE to reduce dimensions down to 3\ntsne = TSNE(n_components=3, verbose=0).fit(x)\ntsne_vec_3d = tsne.embedding_\n\nfig = plt.figure(figsize=(10, 10))\nplt.subplots_adjust(bottom=0.1, right=0.8, top=0.9)\nax_3d = fig.add_subplot(2, 2, 1, projection='3d')\nax_3d.scatter(tsne_vec_3d[:, 0], tsne_vec_3d[:, 1], tsne_vec_3d[:, 2], c=colors[labels-1])#c=kernel.labels_*100, cmap=pylab.cm.cool)\nax_3d.set_title('t-SNE')\n\n\n# Use PCA to reduce dimensions down to 3\npca = PCA(n_components=3)\npca.fit(x.toarray())\n#print('Explained variance ratio of the first three PCs: ', pca.explained_variance_ratio_)\npca_vec_3d = pca.transform(x.toarray())\n\nax_3d = fig.add_subplot(2, 2, 2, projection='3d')\nax_3d.scatter(pca_vec_3d[:, 0], pca_vec_3d[:, 1], pca_vec_3d[:, 2], c=colors[labels-1])\nax_3d.set_title('PCA')\n\n\n\n################### 2D Visualization ######################\n# Use t-SNE to reduce dimensions down to 2\ntsne = TSNE(n_components=2, verbose=0).fit(x)\ntsne_vec_2d = tsne.embedding_\n\nax_2d=plt.subplot(2, 2, 3)\nax_2d.scatter(tsne_vec_2d[:, 0], tsne_vec_2d[:, 1], c=colors[labels-1])\nax_2d.set_title('t-SNE')\n\n\n# Use PCA to reduce dimensions down to 2\npca = PCA(n_components=2).fit(x.toarray())\n#print('Explained variance ratio of the first two PCs: ', pca.explained_variance_ratio_)\npca_vec_2d = pca.transform(x.toarray())\n\nax_2d=plt.subplot(2, 2, 4)\nax_2d.scatter(pca_vec_2d[:, 0], pca_vec_2d[:, 1], c=colors[labels-1])\nax_2d.set_title('PCA')\n'''\n","200aa886":"import os\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\nimport gensim\nfrom gensim import corpora, models\nfrom gensim.models.coherencemodel import CoherenceModel\nfrom gensim.models.ldamodel import LdaModel\nfrom gensim.models.phrases import Phrases, Phraser\nfrom gensim.utils import simple_preprocess\nfrom nltk.corpus import stopwords\nimport re\nfrom pprint import pprint","d927835c":"documents = ['Efforts targeted at a universal coronavirus vaccine.',\n'Exploration of use of best animal models and their predictive value for a human vaccine.',\n'Methods evaluating potential complication of Antibody Dependent Enhancement (ADE) in vaccine recipients.',\n'Approaches to evaluate risk for enhanced disease after vaccination',\n'Effectiveness of drugs being developed and tried to treat COVID-19 patients.',\n'Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocycline that may exert effects on viral replication.',\n'Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.',\n'Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.',\n'Efforts to develop animal models and standardize challenge studies',\n'Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers',\n'Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models (in conjunction with therapeutics)',]\n","9f7c9440":"words=[]\nfor c in documents: \n    words.append(c.split(' '))\n\nwords = [row.split() for row in documents]","bb7b3c33":"from gensim.models.phrases import Phrases, Phraser\n\nbigram = Phrases(words, min_count=30, progress_per=10000)\ntrigram = Phrases(bigram[words], threshold=100)\nbigram_mod = Phraser(bigram)\ntrigram_mod = Phraser(trigram)\n\nbigrams = [b for l in documents for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]","228113c0":"stop_words = stopwords.words('english')\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\nwords_nostops = remove_stopwords(words)\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\nwords_bigrams = make_bigrams(words_nostops)","c6a5428f":"id2word = corpora.Dictionary(words_bigrams)\ntexts = words_bigrams\ncorpus = [id2word.doc2bow(text) for text in texts]","359cd409":"lda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                       id2word=id2word,\n                                       num_topics=10, \n                                       random_state=10,\n                                       chunksize=5,\n                                       passes=1,\n                                       per_word_topics=True)\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","e9b84f13":"lda_model.print_topics(20,num_words=15)[:10]\n\ndef format_topics_sentences(ldamodel=None, corpus=corpus, texts=words_bigrams):\n    sent_topics_df = pd.DataFrame()\n    keywords_arr = []\n    for i, row_list in enumerate(ldamodel[corpus]):\n        row = row_list[0] if ldamodel.per_word_topics else row_list            \n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                keywords_arr.append(topic_keywords)\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df, keywords_arr)\ndf_topic_sents_keywords, keywords_arr = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=words_bigrams)\ndf_topic_sents_keywords\n\nkeywords1 = [i.lower().replace(',', '').split() for i in keywords_arr]\nkeywords2= [x for sublist in keywords1 for x in sublist]\nkeywords = keywords2","92574a6a":"from tqdm import tqdm\nimport glob\nimport json\nimport re\n\ndf = pd.DataFrame()\ndf['body'] = context_data['full_bodytext']\ndf['paper_id'] = context_data['paper_id']\ndf['abstract'] = context_data['abstracts']\ndf['title'] = context_data['titles']\n\nDic_title_to_key={}\nfor i in range(len(df.body)):\n    for item in keywords:\n        if item.lower() in df.body[i].lower():\n            Dic_title_to_key[df.title[i]]=item\ndf2=pd.DataFrame(Dic_title_to_key.items(), columns=['titles', 'keyword'])\ndf3=pd.merge(df2, context_data, on='titles')\ndf3 = df3.sort_values('keyword').reset_index().drop(columns=['index'])\ndf3.to_csv (r'export_dataframe.csv', index = False, header=True)\n","2023df92":"\n# [km, labels] = pickle.load(open('clustering_kernel.pkl', 'rb'))\n\nclustering_data = context_data[columns_to_consider].dropna().reset_index()\nclustering_data['label'] = labels\n\ntm_data = pd.read_csv('export_dataframe.csv')\ntm_titles = set(tm_data['title'].values)\n\ntable = []\nfor l in np.unique(labels):\n    label_data = clustering_data[clustering_data['label'] == l]\n    label_titles = set(label_data['title'].values)\n\n    selected_articles_i = tm_titles.intersection(label_titles)\n    selected_articles_u = tm_titles.union(label_titles)\n    table.append([l, len(label_titles), len(selected_articles_i), len(selected_articles_u)])\n\nprint(tabulate(table, headers=[\"Cluster ID\",\"# of Articles\", \"Intersection\", \"Union\"]))\n","1a5361ff":"\nl = 3\nlabel_data = clustering_data[clustering_data['label'] == l]\nlabel_titles = set(label_data['title'].values)\n\nselected_articles_i = set(tm_titles.intersection(label_titles))\nselected_articles_u = set(tm_titles.union(label_titles))\n\nselected_articles = clustering_data[clustering_data['title'].isin(selected_articles_u)]\nselected_articles = selected_articles.filter(items=['title', 'sha'])\n\n# pickle.dump(selected_articles, open('selected_articles.pkl', 'wb'))\n","d1aa812d":"from nltk.tokenize import word_tokenize\nimport string\n\nstop_words = set(stopwords.words('english'))\n\ntokenized_words = []\nfor words in documents:\n    tokenized_words.append(word_tokenize(words))\n    \nwordsFiltered = []\n\nfor l in tokenized_words:\n    temp_list = []\n    for w in l:\n        if w not in stop_words:\n            temp_list.append(w)\n    wordsFiltered.append(temp_list)\n    \nsearch_sentences = []\nfor i in wordsFiltered:\n    search_sentences.append(' '.join(i))\n\nremove_punct = []\nfor s in search_sentences:\n    remove_punct.append(s.translate(str.maketrans('', '', string.punctuation)))","813f2436":"!git clone https:\/\/github.com\/facebookresearch\/InferSent.git\nfrom InferSent.models import InferSent\nimport torch\nnltk.download('punkt')   \n\n!mkdir encoder\n!curl -Lo encoder\/infersent2.pkl https:\/\/dl.fbaipublicfiles.com\/infersent\/infersent2.pkl","5c033b0b":"V = 2\nMODEL_PATH = 'encoder\/infersent%s.pkl' % V\nparams_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\ninfersent = InferSent(params_model)\ninfersent.load_state_dict(torch.load(MODEL_PATH))\n\nW2V_PATH = '\/kaggle\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'\ninfersent.set_w2v_path(W2V_PATH)\n\ninfersent.build_vocab_k_words(K=100000)","27525b76":"string_titles = []\nfor sent in list(context_data['titles']):\n    if isinstance(sent, str):\n        string_titles.append(sent)\n\ninfersent.update_vocab(string_titles)","a4a71754":"for topic in remove_punct[:4]:\n    infersent.visualize(topic, tokenize=True)","3e099bf9":"index_mappings = {\n      \"settings\": {\n          \"number_of_shards\": 2,\n          \"number_of_replicas\": 1\n        },\n       \"mappings\": {\n          \"dynamic\": \"true\",\n           \"_source\": {\n                 \"enabled\": \"true\"\n             },\n          \"properties\": {\n              \"paper_id\": {\n                  \"type\": \"keyword\"\n              },  \n              \"title\": {\n                  \"type\": \"text\"\n              },\n              \"abstract\": {\n                  \"type\": \"text\"\n              },\n              \"title_vector\": {\n                \"type\": \"dense_vector\",\n                \"dims\": 512\n              },\n              \"abstract_vector\": {\n                \"type\": \"dense_vector\",\n                \"dims\": 512\n              }\n            }\n          }\n        }","7d9505c1":"%%capture\nimport sys\n!{sys.executable} -m pip install tensorflow_hub\n!{sys.executable} -m pip install tensorflow","699cbfc7":"# Create graph and finalize (finalizing optional but recommended).\ng = tf.Graph()\nwith g.as_default():\n# We will be feeding 1D tensors of text into the graph.\n    text_input = tf.placeholder(dtype=tf.string, shape=[None])\n    embed = hub.Module(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/2\")\n    embeddings = embed(text_input)\n    init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\ng.finalize()\n# Create session and initialize.\nsession = tf.Session(graph=g)\nsession.run(init_op)","f278c9b4":"# Use tensorflow for the Universal Sentence Encoder\nGPU_LIMIT = 0.5\n\nprint(\"Downloading pre-trained embeddings from tensorflow hub...\")\nembed = hub.Module(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/2\") # You can also download the model and reload it from your path\ntext_ph = tf.placeholder(tf.string)\nembeddings = embed(text_ph)\nprint(\"Done.\")\n\nprint(\"Creating tensorflow session...\")\nconfig = tf.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = GPU_LIMIT\nsession = tf.Session(config=config)\nsession.run(tf.global_variables_initializer())\nsession.run(tf.tables_initializer())\nprint(\"Done.\")","3cd3f1a5":"# Uncomment the following to install the elasticsearch python package\nimport sys\n!{sys.executable} -m pip install elasticsearch","f135fd8f":"articles_list = \"selected_articles.pkl\"\narticles_df = pd.read_pickle(articles_list)\ntitles = articles_df['title'].tolist()\nprint(len(titles))","ecde58bd":"def load_article(context_data, title):\n    doc = context_data[context_data['titles'] == title]\n    try:\n        return doc.iloc[0]\n    except:\n        return None","9822101c":"INDEX_NAME = \"covid19\"\nINDEX_Mappings = index_mappings\n\nBATCH_SIZE = 1000 # The batch size for bulk indexing\n\nclient = Elasticsearch() # Connect to the elasticsearch service we started earlier\nclient.indices.delete(index=INDEX_NAME, ignore=[404]) # Delete the index if it exists\nclient.indices.create(index=INDEX_NAME, body=INDEX_Mappings)","2fdc08ce":"# Embedding method\ndef embed_text(text):\n    vectors = session.run(embeddings, feed_dict={text_ph: text})\n    return [vector.tolist() for vector in vectors]","c7e64c42":"# Extract content and apply sentence embedding, then bulk index\ndef index_batch(docs):\n    titles = [doc[\"titles\"] for doc in docs]\n    title_vectors = embed_text(titles)\n    \n    abstracts = [doc[\"abstracts\"] for doc in docs]\n    abstract_vectors = embed_text(abstracts)\n\n    requests = []\n    for i, doc in enumerate(docs):\n        request = {}\n        request['paper_id'] = doc['paper_id']\n        request['title'] = titles[i] \n        request['abstract'] = abstracts[i]\n        request[\"_op_type\"] = \"index\"\n        request[\"_index\"] = INDEX_NAME\n        request[\"title_vector\"] = title_vectors[i]\n        request[\"abstract_vector\"] = abstract_vectors[i]\n        requests.append(request)\n    bulk(client, requests)","456206e1":"docs = []\ncount = 0\n\nstart_time = time.time()\nprint(len(titles))\nfor title in titles:\n    doc = load_article(data, title)\n    \n    if doc is None:\n        continue\n    docs.append(doc)\n    count += 1\n\n    if count % BATCH_SIZE == 0:\n        print(count)\n        index_batch(docs)\n        docs = []\n        print(\"Indexed {} documents.\".format(count))\n\nif docs:\n    index_batch(docs)\n    print(\"Indexed {} documents.\".format(count))\n\nclient.indices.refresh(index=INDEX_NAME)\nprint(\"Done indexing.\")\nend_time = time.time()\nprint(\"Time to index: \", (end_time - start_time)\/3600)","b53b40fa":"def run_query_loop(SEARCH_SIZE=5):\n    while True:\n        try:\n            handle_query(SEARCH_SIZE)\n        except KeyboardInterrupt:\n            return","1b30ca54":"def get_top_paragraphs(data, titles, query_vector, SEARCH_SIZE):  \n    enhanced_origin = {}\n    for title in titles:\n        doc = load_article(data, title)\n        body_sections = doc['bodysections']\n        citation_titles = doc['context_title_list']\n        cite_mark = doc['cite_mark']\n        mark_title = {}\n        for i, c_mark in enumerate(cite_mark):\n            mark_title[c_mark] = citation_titles[i]\n            \n        for sec in body_sections:\n            enhan = sec\n            for m in mark_title:\n                try:\n                    enhan = enhan.replace(m, mark_title[m])\n                except:\n                    pass\n            enhanced_origin[enhan] = title + \"<TITLE_SEP>\" + sec\n\n    paragraphs_all = list(enhanced_origin.keys())\n    paragraphs_all_vectors = embed_text(paragraphs_all)\n    scores = []\n    score_index = {}\n    for p in paragraphs_all_vectors:\n        score = 1 - spatial.distance.cosine(p, query_vector)\n        scores.append(score)\n    for i, s in enumerate(scores):\n        score_index[s] = i\n    scores.sort(reverse=True)\n    topk_scores = scores[:SEARCH_SIZE]\n    topk_index = [score_index[s] for s in topk_scores]\n    for i, index in enumerate(topk_index):\n        print(\"Top {} Relevant Paragraph: \".format(i))\n        top_enhanced_paragraph = paragraphs_all[index]\n        top_origin_paragraph = enhanced_origin[top_enhanced_paragraph]\n        title, paragraph = top_origin_paragraph.split('<TITLE_SEP>')[0],top_origin_paragraph.split('<TITLE_SEP>')[1]\n        print(\"title: \", title)\n        print(\"paragraph: \", paragraph)\n        print(\"*\" * 50)","bf58cd03":"def handle_query(SEARCH_SIZE):\n    query = input(\"Enter query: \")\n\n    embedding_start = time.time()\n    query_vector = embed_text([query])[0]\n    embedding_time = time.time() - embedding_start\n\n    script_query = {\n        \"script_score\": {\n            \"query\": {\"match_all\": {}},\n            \"script\": {\n                \"source\": \"cosineSimilarity(params.query_vector, doc['title_vector']) + cosineSimilarity(params.query_vector, doc['abstract_vector']) + 1.0\",\n                \"params\": {\"query_vector\": query_vector}\n            }\n        }\n    }\n\n    search_start = time.time()\n    response = client.search(\n        index=INDEX_NAME,\n        body={\n            \"size\": SEARCH_SIZE,\n            \"query\": script_query,\n            \"_source\": {\"includes\": [\"paper_id\",\"title\", \"abstract\", \"body_texts\"]}\n        }\n    )\n    search_time = time.time() - search_start\n\n    print()\n    print(\"{} total hits.\".format(response[\"hits\"][\"total\"][\"value\"]))\n    print(\"embedding time: {:.2f} ms\".format(embedding_time * 1000))\n    print(\"search time: {:.2f} ms\".format(search_time * 1000))\n    print()\n    print(\"The top \" + str(SEARCH_SIZE) + \" relevant abstracts to the question\")\n    count = 0\n    top_titles = []\n    for hit in response[\"hits\"][\"hits\"]:\n        count += 1\n        title = hit[\"_source\"][\"title\"]\n        print(\"Top \", str(count))\n        print(\"Paper Id: \", hit[\"_source\"][\"paper_id\"])\n        print(\"Paper Title: \", title)\n        print(\"Similarity Score to the query: \", hit[\"_score\"])\n        print(\"Abstract Text: \", hit[\"_source\"][\"abstract\"])\n        print(\"=\" * 50)\n        top_titles.append(title)\n    print()\n    print(\"The top \" + str(SEARCH_SIZE) + \" paragraphs from the top abstracts articles\")\n    get_top_paragraphs(data, top_titles, query_vector, SEARCH_SIZE)","aa9c7d4f":"run_query_loop()","c3b5d6c5":"print(\"Closing tensorflow session...\")\nsession.close()","6be8102a":"## Step 2 - Table of Contents\n* [Extract features and apply k-means clustering on all articles](#STEP1-1-bullet)\n* [How to interpret the clutering results?](#STEP1-2-bullet)\n* [Topic Modeling](#STEP1-3-bullet)\n","88fb040b":"# Introduction\nThe problem we are tasked to tackle is to develop text and data mining tools that can help the medical community develop answers to high priority scientific questions.\nWe specifically focus on developing tools to answer questions related to \"What do we know about vaccines and therapeutics?\". We refer to this task as Task #4 in the rest of this document.\n\nBefore we get into too much details of the approaches taken to tackle this problem, we would like to explain the steps we are taking to have more accurate results in order to help the medical community in a systematic way. \n\n### Crowd-Sourcing Expert Labels\nSince the CORD-19 input dataset is unlabeled and many of the Natural Language Processing (NLP) tools rely heavily on labels obtained from domain specific experts, we propose to attack all of these tasks at once in a highly iterative, closed-loop manner by providing a means of engaging Subject Matter Experts (SMEs) in generating gold data. \nIn order to do so, we have created a user-friendly app for medical experts to generate gold data to be used in training a model in a supervised manner.\n**If you are a medical professional who is interested in contributing to generating gold data**, please fill out <a href=\"https:\/\/docs.google.com\/forms\/d\/e\/1FAIpQLSclojRHOqPks_RuNknqfcb-75jngMsltdjwR6m54eGGjn9_LA\/viewform\" target=\"_blank\">this form<\/a> and you will be contacted to provide your feedback on the results we are already obtaining from our model.\nFor a demo of how the app that we have developed works please watch <a href=\"https:\/\/www.loom.com\/share\/27d2ab7eaaf54ada89df22318d59802d\" target=\"_blank\">this video<\/a>.\nHere are a few snapshots of how the app will help us gather SME feedback.\n<img width=\"1821\" alt=\"Picture1\" src=\"https:\/\/user-images.githubusercontent.com\/61748540\/79514357-fddbdd00-8013-11ea-831e-f1b3cd1b3c6f.png\">\n<img width=\"1822\" alt=\"Picture2\" src=\"https:\/\/user-images.githubusercontent.com\/61748540\/79514374-0f24e980-8014-11ea-94d8-39d0f7321252.png\">\n<img width=\"1822\" alt=\"Picture3\" src=\"https:\/\/user-images.githubusercontent.com\/61748540\/79514399-1946e800-8014-11ea-847b-346770468ac6.png\">\n<img width=\"1822\" alt=\"Picture4\" src=\"https:\/\/user-images.githubusercontent.com\/61748540\/79514422-2532aa00-8014-11ea-9840-9c675c5cc3db.png\">","2f60741e":"### Reduce the dimension of the feature space to visualize the results in 2D\/3D dimensions\n\nWe use two different methods to reduce the dimension of the feature space from 4000 down to 2 and 3. These two methods are <a href=\"https:\/\/en.wikipedia.org\/wiki\/T-distributed_stochastic_neighbor_embedding\" target=\"_blank\">t-distributed Stochastic Neighbor Embedding (t-SNE)<\/a> and <a href=\"https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis\" target=\"_blank\">Principal Component Analysis (PCA)<\/a> which are two commonly used approaches for this purpose. Observe that the obtained figures show that the similar articles have been well clustered together. \n\nThe following code has been provided to perform the dimensionality reduction. However, since kaggle is limiting the memory access for each notebook, we leave this section commented and only provide the figures that you will obtain by running this code. Please note that this part has been commented only to accomodate the limitation issues of submission.","7bd66906":"## Step 4 - Table of Contents\n* [Install Elasticsearch and Kibana](#STEP4-1-bullet)\n* [Create Index Mapping](#STEP4-2-bullet)\n* [Download Universal Sentence Encoder for text embedding](#STEP4-3-bullet)\n* [Start Indexing](#STEP4-4-bullet)\n* [Start Querying](#STEP4-5-bullet)","a1ddaf46":"## Start Querying <a class=\"anchor\" id=\"STEP4-5-bullet\"><\/a>\n\nOnce we have indexed the articles of interest, when a query comes in, we will first apply the same sentence embedding method on the query to get a vector representation of it, and use cosine similarity to return the top K texts that are similar to the query.","87af2b9d":"# Step 0: Install the Required Packages\n\nHere, we install all the required packages that are not already installed in this virtualenvironment.","888f9401":"Observe that cluster #3 has the largest number of articles in the intersection with the most important results. Therefore, we choose this cluster and find the union of the articles in this cluster with the most relevant articles to create a set of articles that are the most important for Task #4.","17278622":"** Note: It will take a few minutes to download the universal sentence encoder model from the tensorflow hub. So grab a cup of coffee and come back :) **","02514b3b":"### Import the Required Packages","a99b2e0c":"**Extract content of interest from the original article json file and apply sentence embedding**","55a23d65":"**Tokenize sentences, remove stop words and punctuation**","7cfe24d9":"**Import the Required Packages**","e930af56":"**Create Index**","b7918abb":"Moving forward, we consider this set of articles to use for making the queries.","86c0c758":"# Reference\n\n[1] Arora, Sanjeev, Yingyu Liang, and Tengyu Ma. \"A simple but tough-to-beat baseline for sentence embeddings.\" (2016). <br>\n[2] https:\/\/github.com\/hanxiao\/bert-as-service#install <br>\n[3] Conneau, Alexis, et al. \"Supervised learning of universal sentence representations from natural language inference data.\" arXiv preprint arXiv:1705.02364 (2017). <br>\n[4] Cer, Daniel, et al. \"Universal sentence encoder.\" arXiv preprint arXiv:1803.11175 (2018). <br>\n[5] Arman Cohan, Nazli Goharian \"Scientific Article Summarization Using Citation-Context and Article\u2019s Discourse Structure\" (EMNLP 2015).<br>\n","201a92c9":"# Overview of Our Approach\nWe explain the approach we took to tackle this problem in 4 main steps: \n1. Preprocessing and exploring the data, \n2. Finding the most relevant articles using clustering and topic modeling approaches, \n3. Visualizing the significance of words in important topics\n4. Creating a question answering system using using Universal Sentence Embedding and ElasticSearch. \n\nThe following figure explains these steps visually as well.\nObserve that we combine the output of the clustering approach and the topic modeling approach to select the most relevent articles to Task #4.\nThen, we further analyze the articles by looking at the citations used in each article.\nThe last step is to use the most relevant articles found in the previous steps to create a question answering system which finds the most relevant answer to specific questions within the topic of Task #4.\n\n<figure>\n    <img width=\"600\" alt=\"Pipeline\" src=\"https:\/\/user-images.githubusercontent.com\/61748540\/79469442-2c36c980-7fce-11ea-9bd5-77626334c277.png\">\n<\/figure>","26a98505":"### Store the Data\n\nHere, we store the preprocessed data for further investigation. We have ignored some articles that do not have a valid path to a full text article. ","dd3f724e":"**Visualize a few important topics**\n\nWe can clearly see that based on the vector representations created from using fasttext embeddings on titles in our dataset, and visualizing important terms specific to task-4, **\"vaccine\"**, it's derivative words, and **\"coronavirus\"** are important words contributing towards generating embeddings. This handy function can be used to identify the significance of several other important topics from a model trained in an unsupervised manner to create embeddings on a related corpora, which in this case is titles from the CORD-19 dataset.","02dd3cbe":"## Topic Modeling<a class=\"anchor\" id=\"STEP1-3-bullet\"><\/a>\n\nOne of the well-known generative probabilistic topic modeling algorithms is the <a href=\"https:\/\/en.wikipedia.org\/wiki\/Latent_Dirichlet_allocation\" target=\"_blank\">Latent Dirichlet Allocation (LDA)<\/a> approach which performs well in finding topics in a collection of documents.\nIn LDA, topics are the latent variables, while words are the apparent variables. \nLDA receives words as an input vector and generates topics which are probability distribution over words based on a generative process. LDA uses a joint probability distribution over both the observed and hidden random variables and compute the posterior distribution (conditional distribution) of the hidden variables given the observed variables. \nThe fundamental assumption of LDA is that documents can be assigned to multiple topics. Another assumption is that topics are hidden variables and words in documents are apparent variables. Thus, LDA performs a generative process by receiving words as an input vector to provide topics which are probability distribution over words.  The aim is that we provide a list of keywords that are relvent to Task#4. \nHere, we explain how we can apply this method to our problem.\n\n\n","9a97e861":"### Bigram and Trigram \nWe need to provide Bigram and Trigram from the orginal texts. ","ed748865":"### Download Universal Sentence Encoder for text embedding <a class=\"anchor\" id=\"STEP4-3-bullet\"><\/a>\n\nTo obtain the vector representations for texts, we need to apply the sentence embedding technique. There are various sentence embedding methods such as SIF embedding[1], or using BERT as a service[2], or InferSent[3] etc.. In this work, we choose to use Google's Universal Sentence Encoder[4]. However, investigating in other methods is the priority in our future work, particularly applying BioBERT as a service.","2484de41":"**Top Abstracts Returned**\n<img width=\"1188\" alt=\"AB\" src=\"https:\/\/user-images.githubusercontent.com\/61748194\/79496630-65823000-7ff4-11ea-8e87-a1a772b5fc97.png\">","431c14db":"### Create the Dictionary and Corpus needed for Topic Modeling\nThe LDA function receives *Dictionary* and *Corpus* as inpuut and provides *Topics* as output","e9ec3347":"### NLTK Stop words","f5b5c9ef":"### Install Elasticsearch and Kibana <a class=\"anchor\" id=\"STEP4-1-bullet\"><\/a>\n\nAs mentioned before, we want to create vector representations for text and use them to allow better document scoring when searching answers for a query, we need to install Elasticsearch service with version higher than 7.6. Here is the link to download the package: https:\/\/www.elastic.co\/start.  \n\nOn the download page, there are instructions to install Elasticsearch and Kibana. Getting Kibana is optional but we highly recommend you install it to enable easy visualize and manage your data. Once you download the packages, make sure to run: <br>\n1. `bin\/elasticsearch` to start the elasticsearch service. To make sure the service is up and running, you can open http:\/\/localhost:9200 in your browser, you should get response like the following:\n\n    `{\n      \"name\" : \"luna\",\n      \"cluster_name\" : \"elasticsearch\",\n      \"cluster_uuid\" : \"1MVhoZ9HS4GcGRppF4P-yQ\",\n      \"version\" : {\n        \"number\" : \"7.6.1\",\n        \"build_flavor\" : \"default\",\n        \"build_type\" : \"tar\",\n        \"build_hash\" : \"aa751e09be0a5072e8570670309b1f12348f023b\",\n        \"build_date\" : \"2020-02-29T00:15:25.529771Z\",\n        \"build_snapshot\" : false,\n        \"lucene_version\" : \"8.4.0\",\n        \"minimum_wire_compatibility_version\" : \"6.8.0\",\n        \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n      },\n      \"tagline\" : \"You Know, for Search\"\n    }`\n   \n2. `bin\/kibana` to start the kibana service. Once the service is up and running, you can access it from http:\/\/localhost:5601","27931c24":"**Load pre-trained model and update the vocabulary with most common words**","743fd33a":"## How to interpret the clutering results? <a class=\"anchor\" id=\"STEP1-2-bullet\"><\/a>\n\nAs mentioned earlier, the motivation behind choosing the number of clusters to be 10 is that we would like to work with the articles that are more relevant to Task #4. However, by simply applying a clustering algorithm, it is hard to tell which cluster corresponds to which general topic. Therefore, we propose to perform a Topic Modeling on all parts of the articles to obtain the most relevant articles to Task #4. Next, we find the cluster that has the most overlap with the relavant articles obtained from Topic Modeling. Then, we take the union of these two sets (the chosen cluster and the set of important articles obtained by Topic Modeling) and use those articles for the downstream tasks since we are focusing on answering specif questions stated in Task #4. In the next section, we explain how we obtain the most relevant articles using topic modeling.","57c354cf":"### Start Indexing <a class=\"anchor\" id=\"STEP4-4-bullet\"><\/a>","30e72842":"## Extract features and apply k-means clustering on all articles <a class=\"anchor\" id=\"STEP1-1-bullet\"><\/a>\n\nTo extract features for each aritcle using a set of specific parts (i.e., abstract, intorduction, body text, conclusion) we use the Term Frequency\u2013Inverse Document Frequency (TF-IDF) approach. Use the *TfidfVectorizer* package of *sklearn* to get the TF-IDF vector. We consider all parts of the papers to be considered in this mapping. We also set the maximum number of features to be 4000 to limit the required computational power. In future studies, we will experiment with increasing this number to see if we can also improve the accuracy of the downstream tasks.","b1c146c5":"## Find the cluster that has the most overlap with the most relevant articles\n\nIn order to do so, we find the overlap between the most important articles and each individual cluster. We also find the intersection and union of these two sets for all the 10 different clusters.","da644176":"# Step 3: Visualizing the Significance of Words in Important Topics\n\nWe identify some important topics from the task description of the challenge and after preprocessing (removal of stopwords and punctuation), we are able to visualize it by an InferSent model trained on the titles of our dataset.","587dcb9d":"## Example of query output","1764f078":"**Note: The indexing process will take some time depending on the number of articles you'd like to index, but this is one time effort, once the articles are indexed, the searching process is fast**","eeba8b1d":"**Incorporate the citation titles in the original context to allow better search**","19f7fda9":"**Clone the InferSent repo and download the InferSent model trained with fasttext**","ef7f2f3b":"### Run KMeans algorithm on the extracted features\n\nHere we use a TensorFlow-based implementation of k-means (<a href=\"https:\/\/pypi.org\/project\/kmeanstf\/\" target=\"_blank\">KMeansTF<\/a>) to perform the clustering. We also use two different metrics to see how well these articles are grouped together. We use <a href=\"https:\/\/en.wikipedia.org\/wiki\/Davies\u2013Bouldin_index\" target=\"_blank\">Davies\u2013Bouldin index (DBI)<\/a> and the <a href=\"https:\/\/en.wikipedia.org\/wiki\/Silhouette_(clustering)\" target=\"_blank\">Silhouette score<\/a> to quantify the goodness of these clusters. Although, the scores do not show great performance, we can consider these clusters relatively good.","49e9255c":"# Discussion and Future Work\n\n* Run the clustering algorithm using more features\n* Incorporate the querying results in our annotation tool\n* Explore more sentence embedding methods, particularly utilizing BioBERT","c6eb579b":"**Load articles**","9333af70":"### Function to Read Input Data\n\nThis function reads the input data and collects all the relavant information about the articles in one dataframe.","9fbc2324":"Import a description of task4","8aa74da5":"# Step 4: Find the most Relevant Answers to Specific Queries\n\nBy following the previous steps, we can identify the articles that are most relevant to the questions asked in Task 4, now we need an efficient way to query the articles and return the most relevant ones to answer a question entered by the end user. To achieve that, we select Elastisearch as our search engine and utilize the dense vector data type to store vector representation of the text to allow better document scoring. The details of this part will be introduced as the following table of content.","113de8c6":"**Get articles that are relevant to Task 4**","d1b6bd5d":"### Create Index Mapping <a class=\"anchor\" id=\"STEP4-2-bullet\"><\/a>\n\nWe need to create a mapping schema to tell elasticsearch what information from each article we want to store and what data type we want to use. For this submission, we store the 'paper_id' as the _keyword_ type, the 'title' and 'abstract' as the _text_ type, the vector representation of 'title' and 'abstract' as the <em>dense_vector<\/em> type. ","dd3d7045":"### Function to create feature vectors using TF-IDF vectorizer","590e3cf4":"# Step 2: Find the Most Relevant Articles to Task #4\n\nWe first apply the TF-IDF vectorizer to map the articles (including abstract, introduction, body, and conclusion) to vectors representing their contents. We then use these vectors as features for each article and then apply the k-means clustering algorithm to cluster all the articles in the dataset. For this specifc problem, we choose the number of clusters to be 10 since we have 10 different tasks which have relatively low overlap in terms of the discussed topic.\n\nFurthermore, we also find a set of articles that are most relevant to Task #4 by performing a Topic Modeling (TM) analysis. \nAfter finding the most relavant articles using the TM analysis, we find the cluster (obtained from k-means clustering) that has the most overlap with the selected articles using the TM approach. We use this subset of articles as the input for the next step.","3dc26df1":"### Building the Topic Model\nTo train the LDA model, we need to define 1) the corpus, 2) dictionary and 3)the number of topics. We also need to determine the values of hyperparameters such as alpha and eta. The defult values of these parametters are  1\/#topic\n1\/#topic\n . Another parameter is chunksize that determines the number of documents to be used in each training chunk. Finally passes is the total number of training passes.\n![](http:\/\/)\n","ee4b863c":"**Top Paragraphs Returned**\n\n<img width=\"1188\" alt=\"PARA\" src=\"https:\/\/user-images.githubusercontent.com\/61748194\/79496690-7fbc0e00-7ff4-11ea-9d22-91c7ebdbdafd.png\">","4a60d1a7":"# Step 1: Preprocessing and Data Exploration\nIn this section, we write some auxilary functions to read the CORD-19 dataset and store it in a format that will be used in the rest of this project.","be1f388f":"### Look at the Data\n\nHere, we provide the first and last few rows of the dataframe that we just created to make sure everything looks fine. ","4afc6785":"Run LDA to get some useful keywords related to Task4","0e5c9537":"### Apply the TF-IDF Vectorizer to abstract, introduction, body text, and conclusion of all the articles\n\nPlease note that we set the maximum number of features to 100 only due to the computational time while submitting the notebook for this competition. The rest of the analysis has been done while the *max_features* parameter is set to 4000. Future work also explores setting higher values for this hyperparameter to see if we gain any accuracy while performing the clustering.","74f3bac0":"**Update vocabulary with titles from CORD-19 dataset**","2f9832cd":"![PCA-2D](https:\/\/user-images.githubusercontent.com\/61748540\/79476286-c995fb80-7fd6-11ea-9e62-b7089b6d6552.png)"}}