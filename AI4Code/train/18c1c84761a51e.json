{"cell_type":{"e48a6e6e":"code","12a70f87":"code","3d47a5a4":"code","8fee3d22":"code","2d125652":"code","24a9f176":"code","fb51a6e6":"code","a19341bb":"code","529568b3":"code","a733a6e2":"markdown","386fb574":"markdown","9929d587":"markdown","6364be29":"markdown","399d9ac5":"markdown","4159cb62":"markdown"},"source":{"e48a6e6e":"! pip install -q pytorch-lightning\n! pip install -q \"https:\/\/github.com\/Borda\/kaggle_iMet-collection\/archive\/refs\/heads\/fix\/img-load.zip\"\n# ! pip install -q -U Pillow\n! pip list | grep -E \"torch|kaggle\"\n! nvidia-smi","12a70f87":"%matplotlib inline\n%reload_ext autoreload\n%autoreload","3d47a5a4":"# jsu to see what is the data location\n! ls \/kaggle\/input -l\n! ls \/kaggle\/input\/imet-2021-fgvc8 -l","8fee3d22":"import pandas as pd\n\nPATH_DATASET = \"\/kaggle\/input\/imet-2021-fgvc8\/\"\npd.read_csv(PATH_DATASET + \"train-from-kaggle.csv\").head()","2d125652":"import matplotlib.pyplot as plt\n\nfrom kaggle_imet.data import IMetDataset\n\n\ndataset = IMetDataset(\n    df_data=PATH_DATASET + \"train-from-kaggle.csv\",\n    path_img_dir=PATH_DATASET + \"train-1\/train-1\",\n)\n\n# quick view\nfig = plt.figure(figsize=(12, 8))\nfor i in range(9):\n    img, lb = dataset[i]\n    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])  # \n    ax.imshow(img)\n    ax.set_title(f\"img: {img.size}\\n lb: {lb}\")","24a9f176":"import torch\nimport numpy as np\n\nfrom kaggle_imet.data import IMetDM, TORCHVISION_TRAIN_TRANSFORM, TORCHVISION_VALID_TRANSFORM\n\n\ndm = IMetDM(\n    base_path=PATH_DATASET,\n    batch_size=128,\n    train_transforms=TORCHVISION_TRAIN_TRANSFORM,\n    valid_transforms=TORCHVISION_VALID_TRANSFORM,\n    num_workers=0,\n)\ndm.setup()\n\n# Quick view\nfig = plt.figure(figsize=(3, 7))\nfor imgs, lbs in dm.val_dataloader():\n    batch_lb_sum = torch.sum(lbs, axis=0).numpy()\n    print(f'batch labels: {list(batch_lb_sum[batch_lb_sum > 0])}')\n    print(f'image size: {imgs[0].shape}')\n    for i in range(3):\n        ax = fig.add_subplot(3, 1, i + 1, xticks=[], yticks=[])\n        # print(np.rollaxis(imgs[i].numpy(), 0, 3).shape)\n        ax.imshow(np.rollaxis(imgs[i].numpy(), 0, 3))\n        ax.set_title(lbs[i])\n    break","fb51a6e6":"from kaggle_imet.models import LitResnet, LitMet\n\n# see: https:\/\/pytorch.org\/vision\/stable\/models.html\nnet = LitResnet(arch='resnet50', num_classes=dm.num_classes)\n# print(net)\n\nmodel = LitMet(model=net, num_classes=dm.num_classes, lr=1e-3)","a19341bb":"import pytorch_lightning as pl\n\nlogger = pl.loggers.CSVLogger(save_dir='logs\/', name=model.name)\nswa = pl.callbacks.StochasticWeightAveraging(swa_epoch_start=0.6)\nckpt = pl.callbacks.ModelCheckpoint(\n    monitor='valid_f1',\n    save_top_k=1,\n    save_last=True,\n    # save_weights_only=True,\n    filename='checkpoint\/{epoch:02d}-{valid_acc:.4f}-{valid_f1:.4f}',\n    # verbose=False,\n    mode='max',\n)\n\n# ==============================\n\ntrainer = pl.Trainer(\n    # fast_dev_run=True,\n    gpus=5,\n    callbacks=[ckpt],\n    logger=logger,\n    max_epochs=1,\n    precision=16,\n    #overfit_batches=5,\n    auto_lr_find=True,\n    accumulate_grad_batches=24,\n    val_check_interval=0.5,\n    progress_bar_refresh_rate=1,\n    weights_summary='top',\n)\n\n# ==============================\n\n# lr_find_kwargs = dict(min_lr=1e-5, max_lr=1e-2, num_training=25)\n# trainer.tune(model, datamodule=dm, lr_find_kwargs=lr_find_kwargs)\n# print(f\"LR: {model.learning_rate}\")\n\n# ==============================\n\ndm.batch_size = 128\ntrainer.fit(model=model, datamodule=dm)","529568b3":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nmetrics = pd.read_csv(f'{trainer.logger.log_dir}\/metrics.csv')\ndisplay(metrics.dropna(axis=1, how=\"all\").head())\n\ndel metrics[\"epoch\"]\nmetrics.set_index(\"step\", inplace=True)\ng = sns.relplot(data=metrics, kind=\"line\")\nplt.gcf().set_size_inches(15, 5)","a733a6e2":"## Dataset & DataModule\n\nCreating standard PyTorch dataset to define how the data shall be loaded and set representations. We define the sample pair as:\n- RGB image\n- one-hot lable encding\n\nA DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is consistent data splits, data preparation and transforms across models.","386fb574":"Quick visualization of the training process...","9929d587":"## CNN Model\n\nWe start with some stanrd CNN models taken from torch vision. Then we define Ligthning module including training and validation step and configure optimizer\/schedular.","6364be29":"## Data view\n\nChecking what data do we have available...","399d9ac5":"# iMet Collection 2021 with Lightning \u26a1","4159cb62":"## Training\n\nWe use Pytorch Lightning which allow us to drop all the boilet plate code and simplify all training just to use\/call Trainer..."}}