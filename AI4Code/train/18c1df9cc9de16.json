{"cell_type":{"fef6fe3f":"code","48beacdd":"code","b4ecb415":"code","e282c291":"code","c853ad52":"code","c373190d":"code","907bfd37":"code","f1382013":"code","88e49c0e":"code","6de235da":"code","ffaf164b":"code","15213e28":"code","ed2a5551":"code","a74ab0bf":"code","88dd1a63":"code","9ed70b0c":"markdown","b5a31f04":"markdown","da075f73":"markdown","4e3e9ff9":"markdown","f438e87f":"markdown","411be344":"markdown","52d2309c":"markdown","c4137ffd":"markdown","e4a95d86":"markdown","765e2007":"markdown","e4edf6a7":"markdown","f9b78810":"markdown","258851f3":"markdown","9fc55329":"markdown","f8e0647a":"markdown","7127b69e":"markdown","f00a7a9f":"markdown","b1590d95":"markdown","299a2171":"markdown","d1cf957b":"markdown","5b1f9552":"markdown","4ec85917":"markdown","f723d593":"markdown","eeccee01":"markdown","d554e1b1":"markdown","ff015cd9":"markdown","cbfa2767":"markdown","f709746d":"markdown","37c76390":"markdown"},"source":{"fef6fe3f":"# Data Handling\nfrom pandas import read_csv\nimport os\nimport pandas as pd\nimport numpy as np\nimport time\n\n# Modeling\nimport xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\n\n# Visualization\nimport matplotlib\nfrom matplotlib import pyplot\n\n## Hyperparameter optimization using RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV","48beacdd":"df_basedata_train_0 = pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf_basedata_train_0.head()","b4ecb415":"df_basedata_train_0.info()\n# Converting Total Charges to a numerical data type.\ndf_basedata_train_0.TotalCharges = pd.to_numeric(df_basedata_train_0.TotalCharges, errors='coerce')\n\n# Find Categorical Variables\ns = (df_basedata_train_0.dtypes == 'object')\nobject_cols = list(s[s].index)\nprint(\"Categorical variables:\")\nprint(object_cols)\n# Encode Categorical Variables\ndf_1 = pd.get_dummies(df_basedata_train_0, columns=[\"gender\",\"Partner\",\"Dependents\", \"PhoneService\", \"MultipleLines\", \"InternetService\",\"OnlineSecurity\",\"OnlineBackup\",\"DeviceProtection\", \"TechSupport\",\"StreamingTV\",\"StreamingMovies\",\"Contract\",\"PaperlessBilling\",\"PaymentMethod\", \"Churn\"   ],drop_first=True)\n\n# Rename Target feature\ndf_1 = df_1.rename(columns={'Churn_Yes': 'target'})\n\n## Drop ID Features\ndf_2=df_1.drop(['customerID'],axis=1)\ndf_2.head()\n","e282c291":"# split data into X and y\nX = df_2.drop(\"target\", axis=1)\ny = df_2[\"target\"]\n\n\n# encode string class values as integers\nlabel_encoded_y = LabelEncoder().fit_transform(y)\nnp.unique(label_encoded_y)","c853ad52":"## Hyper Parameter Optimization\n\nlearning_rate = [0.01, 0.03, 0.05, 0.10, 0.15, 0.20, 0.25, 0.3] \nmax_depth = [ 3, 4, 5, 6, 8, 10, 15, 20]\nmin_child_weight = [ 1, 3, 5, 7 ]\ngamma = [ 0.0, 0.1,  0.4,   0.9, 1 , 2, 5 , 10, 15, 20 ]\ncolsample_bytree = [ 0.3, 0.4, 0.5 , 0.7 ]\nn_estimators = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]","c373190d":"# Create Model Object\nclassifier=xgboost.XGBClassifier()\n\n# Initiate start time\nstart_time = time.time()\n\n# Parameters\n#n_estimators = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000] \nparam_grid = dict(n_estimators=n_estimators)\n\n#Cross Validation \nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n#Model creation with GridSearchCV\ngrid_search = GridSearchCV(classifier, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=kfold, verbose = 1)\n#Fit the Model\ngrid_result = grid_search.fit(X, label_encoded_y)\n\n# Check Execution time\nend_time = time.time()\nexecution_time = (end_time - start_time)\nprint(\"Successfully executed in\", round(execution_time\/60,1), \"mins\")\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n\n# plot the results\npyplot.plot(n_estimators, means)\npyplot.title(\"XGBoost n_estimators vs Accuracy\")\npyplot.xlabel('n_estimators')\npyplot.ylabel('Accuracy')\npyplot.show()    ","907bfd37":"# Experiment 2\n\n# Create Model Object\nclassifier=xgboost.XGBClassifier(n_estimators=100)\n\n# Initiate start time\nstart_time = time.time()\n\n# Parameters\n#max_depth = [2, 4, 6, 8, 10, 15, 20]\nparam_grid = dict(max_depth=max_depth)\n\n#Cross Validation \nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n#Model creation with GridSearchCV\ngrid_search = GridSearchCV(classifier, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=kfold, verbose = 1)\n#Fit the Model\ngrid_result = grid_search.fit(X, label_encoded_y)\n\n# Check Execution time\nend_time = time.time()\nexecution_time = (end_time - start_time)\nprint(\"Successfully executed in\", round(execution_time\/60,1), \"mins\")\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n\n# plot\npyplot.plot(max_depth, means)\npyplot.title(\"XGBoost Max Depth vs Accuracy\")\npyplot.xlabel('Tree Size')\npyplot.ylabel('Accuracy')\npyplot.show()    ","f1382013":"# Experiment 3\n# No. of Trees & Max_depth \n\n\n# Create Model Object\nclassifier=xgboost.XGBClassifier()\n\n#Initiate start time\nstart_time = time.time()\n\n# Parameters\n#n_estimators = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n#max_depth = [2, 4, 6, 8, 10]\nparam_grid = dict(max_depth=max_depth, n_estimators=n_estimators)\n\n#Cross Validation \nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n#Model creation with GridSearchCV\ngrid_search = GridSearchCV(classifier, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=kfold, verbose = 1)\n#Fit the Model\ngrid_result = grid_search.fit(X, label_encoded_y)\n\n\n# Check Execution time\nend_time = time.time()\nexecution_time = (end_time - start_time)\nprint(\"Successfully executed in\", round(execution_time\/60,1), \"mins\")\n\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n\n# plot results\nscores = np.array(means).reshape(len(max_depth), len(n_estimators))\nfor i, value in enumerate(max_depth):\n    pyplot.plot(n_estimators, scores[i], label='depth: ' + str(value))\n\n    pyplot.title(\"XGBoost n_estimators vs Accuracy\")\n    pyplot.xlabel('n_estimators')\n    pyplot.ylabel('Accuracy')\n    pyplot.legend()\n\n    \n# Plot results in Heat map\ndf_scores = pd.DataFrame(scores, columns = (n_estimators), index = (max_depth))\ndf_scores\n\n#import for visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf,ax = plt.subplots(figsize=(10, 10))\n\n\nsns.heatmap(df_scores, annot = True, cbar = False, linewidths = 0.2, cmap=\"YlGnBu\",\n           xticklabels=n_estimators, yticklabels=max_depth, fmt='.5f')\n\nplt.title('Comparative summary of Accuracy Trend', fontsize = 15) # title with fontsize 20\nplt.xlabel('Trees-Numbers', fontsize = 15) # x-axis label with fontsize 15\nplt.ylabel('Trees-Size', fontsize = 15) # y-axis label with fontsize 15\n\nplt.show()","88e49c0e":"#Experiment 4\n# Create Model Object\nclassifier=xgboost.XGBClassifier(max_depth=4, n_estimators=100)\n\n# Initiate start time\nstart_time = time.time()\n\n# Parameters\n#learning_rate = [0.01, 0.03, 0.05, 0.10, 0.15, 0.20, 0.25, 0.3] \nparam_grid = dict(learning_rate=learning_rate)\n\n#Cross Validation \nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n#Model creation with GridSearchCV\ngrid_search = GridSearchCV(classifier, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=kfold, verbose = 1)\n#Fit the Model\ngrid_result = grid_search.fit(X, label_encoded_y)\n\n# Check Execution time\nend_time = time.time()\nexecution_time = (end_time - start_time)\nprint(\"Successfully executed in\", round(execution_time\/60,1), \"mins\")\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n\n# plot the results\npyplot.plot(learning_rate, means)\npyplot.title(\"XGBoost learning_rate vs Accuracy\")\npyplot.xlabel('learning_rate')\npyplot.ylabel('Accuracy')\npyplot.show()   ","6de235da":"#Experiment 5\n# Create Model Object\nclassifier=xgboost.XGBClassifier(max_depth=4, n_estimators=100, learning_rate= 0.05)\n\n# Initiate start time\nstart_time = time.time()\n\n# Parameters\n#gamma = [ 0.0, 0.1, 0.2 , 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 ]\nparam_grid = dict(gamma=gamma)\n\n#Cross Validation \nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n#Model creation with GridSearchCV\ngrid_search = GridSearchCV(classifier, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=kfold, verbose = 1)\n#Fit the Model\ngrid_result = grid_search.fit(X, label_encoded_y)\n\n# Check Execution time\nend_time = time.time()\nexecution_time = (end_time - start_time)\nprint(\"Successfully executed in\", round(execution_time\/60,1), \"mins\")\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n\n# plot the results\npyplot.plot(gamma, means)\npyplot.title(\"XGBoost Gamma vs Accuracy\")\npyplot.xlabel('Gamma')\npyplot.ylabel('Accuracy')\npyplot.show()   ","ffaf164b":"# Experiment 6\n# Learning Rate & Gamma\n\n\n# Create Model Object\nclassifier=xgboost.XGBClassifier(max_depth=4, n_estimators=100)\n\n#Initiate start time\nstart_time = time.time()\n\n# Parameters\n# learning_rate = [0.01, 0.03, 0.05, 0.10, 0.15, 0.20, 0.25, 0.3] \n# gamma = [ 0.0, 0.1,  0.4,   0.9, 1 , 2, 5 , 10, 15, 20 ]\nparam_grid = dict(learning_rate=learning_rate, gamma=gamma)\n\n#Cross Validation \nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n#Model creation with GridSearchCV\ngrid_search = GridSearchCV(classifier, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=kfold, verbose = 1)\n#Fit the Model\ngrid_result = grid_search.fit(X, label_encoded_y)\n\n\n# Check Execution time\nend_time = time.time()\nexecution_time = (end_time - start_time)\nprint(\"Successfully executed in\", round(execution_time\/60,1), \"mins\")\n\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n\n# plot results\nscores = np.array(means).reshape(len(gamma), len(learning_rate))\nfor i, value in enumerate(gamma):\n    pyplot.plot(learning_rate, scores[i], label='gamma: ' + str(value))\n\n    pyplot.title(\"XGBoost learning_rate vs Accuracy\")\n    pyplot.xlabel('learning_rate')\n    pyplot.ylabel('Accuracy')\n    pyplot.legend()\n\n# Plot results in Heat map\ndf_scores = pd.DataFrame(scores, columns = (learning_rate), index = (gamma))\ndf_scores\n\n#import for visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf,ax = plt.subplots(figsize=(10, 10))\n\n\nsns.heatmap(df_scores, annot = True, cbar = False, linewidths = 0.2, cmap=\"YlGnBu\",\n           xticklabels=learning_rate, yticklabels=gamma, fmt='.5f')\n\nplt.title('Comparative summary of Accuracy Trend', fontsize = 15) # title with fontsize 20\nplt.xlabel('learning_rate', fontsize = 15) # x-axis label with fontsize 15\nplt.ylabel('gamma', fontsize = 15) # y-axis label with fontsize 15\n\nplt.show()","15213e28":"#Experiment 7\nclassifier=xgboost.XGBClassifier(gamma=10, learning_rate=0.05, max_depth=4,\n              n_estimators=600, verbosity=1)\n\nfrom sklearn.model_selection import cross_val_score\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\nscore=cross_val_score(classifier,X,label_encoded_y,cv=kfold)\nscore.mean()","ed2a5551":"#Experiment 8\n# Create Model Object\nclassifier=xgboost.XGBClassifier()\n\n# Initiate start time\nstart_time = time.time()\n\n# Parameters\nparams = dict(learning_rate = learning_rate, max_depth = max_depth, min_child_weight = min_child_weight, gamma = gamma, colsample_bytree = colsample_bytree, n_estimators = n_estimators )\n\n# Check Best parameters\nrandom_search=RandomizedSearchCV(classifier,param_distributions=params,n_iter=5,scoring='accuracy',n_jobs=-1,cv=5,verbose=1)\nrandom_result = random_search.fit(X,label_encoded_y)\n\n# Check Execution time\nend_time = time.time()\nexecution_time = (end_time - start_time)\nprint(\"Successfully executed in\", round(execution_time\/60,1), \"mins\")\n\nrandom_result.best_estimator_","a74ab0bf":"classifier=xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.3, gamma=5, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.03, max_delta_step=0, max_depth=4,\n              min_child_weight=5,  monotone_constraints='()',\n              n_estimators=1000, n_jobs=0, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=1)\n\nfrom sklearn.model_selection import cross_val_score\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\nscore=cross_val_score(classifier,X,y,cv=kfold)\nscore.mean()","88dd1a63":"\"\"\"\nOptuna example that optimizes a classifier configuration for Telco Churn dataset\nusing XGBoost.\nIn this example, we optimize the validation accuracy of churn detection\nusing XGBoost. We optimize both the choice of booster model and their hyper\nparameters.\n\"\"\"\n\nimport numpy as np\nimport sklearn.datasets\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\nimport optuna\n\n\n# FYI: Objective functions can take additional arguments\n# (https:\/\/optuna.readthedocs.io\/en\/stable\/faq.html#objective-func-additional-args).\n\ndef objective(trial):\n    (data, target) = (X,y)\n\n    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n    dtrain = xgb.DMatrix(train_x, label=train_y)\n    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n\n    param = {\n        \"verbosity\": 0,\n        \"objective\": \"binary:logistic\",\n        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n    }\n\n    if param[\"booster\"] == \"gbtree\" or param[\"booster\"] == \"dart\":\n        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n    if param[\"booster\"] == \"dart\":\n        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n\n    bst = xgb.train(param, dtrain)\n    preds = bst.predict(dvalid)\n    pred_labels = np.rint(preds)\n    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n    return accuracy\n\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100, timeout=600)\n\n    print(\"Number of finished trials: \", len(study.trials))\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n","9ed70b0c":"Find Best Parameters with Random Search.","b5a31f04":"<h1 style=\"color:DodgerBlue;\">Tune with best parameters found till now<\/h1>","da075f73":"<strong style=\"color:Tomato;\">Split the Dataset in Predictor and Target <\/strong>","4e3e9ff9":"<strong style=\"color:Tomato;\">Import Libraries <\/strong>","f438e87f":"<b>Conclusion:<\/b> Max_Depth of 4-5 and #Trees till 400 will give us accuracy more than what we have achieved from max_depth 20 and 1000 Trees. this would save resources and time.\n<ul><b>Both MaxDepth and N_estimators should be considered for better accuracy <\/b><\/ul>\n\n<b>Future consideration:<\/b> Try multiple models with changing Learning Rate, reg_lambda, Gamma and reg_alpha.","411be344":"<b>Conclusion:<\/b> We achieved Accuracy close to parameters found by best estimators with Random Search CV\n<ul><b>Both MaxDepth, N_estimators, Learning Rate & Gamma are key parameters for better accuracy <\/b><\/ul>\n","52d2309c":"Successfully executed in <b>0.2 mins<\/b>\n<ul>\nAccuracy flatuates Gamma below 1. <b>Improves above 1 till 10<\/b> followed by decline.","c4137ffd":"Successfully executed in 0.2 mins\n<ul>\n    Accuracy peaks at <b>0.05<\/b> and continuously declines after that.","e4a95d86":"<h1 style=\"color:DodgerBlue;\">Find Best Parameters in XGBoost with Optuna<\/h1>","765e2007":"<h1 style=\"color:DodgerBlue;\">Tune the Size of Decision Trees in XGBoost <\/h1>\n","e4edf6a7":"<h1 style=\"color:DodgerBlue;\">Tune The Learning Rate in XGBoost<\/h1>","f9b78810":"<h1 style=\"color:DodgerBlue;\">Find Best Parameters in XGBoost with Random Search<\/h1>","258851f3":"<h1 style=\"color:DodgerBlue;\">Tune the Number of Decision Trees in XGBoost<\/h1>","9fc55329":"<strong style=\"color:Tomato;\">Data Preparation <\/strong>","f8e0647a":"\nWe can tune this hyperparameter of XGBoost using the grid search infrastructure in scikit-learn on the Otto dataset. \n<ul>Below we evaluate odd values for<b> max_depth between 3 and 20<\/b> (3, 4, 5, 6, 8, 10, 15, 20).<\/ul>\n\nEach of the 8 configurations is evaluated using 10-fold cross validation, resulting in <b>80 models<\/b> being constructed. The full code listing is provided below for completeness.\n","7127b69e":"We can see that the best result was achieved with a <b>n_estimators=100 and max_depth=4<\/b>. We must look into other scores as well. <b> Depth 20 <\/b> is a big tree and creating <b> 1000 trees <\/b> is also time consuming. It is important to see other trends as well.\n<ul>\n  <li>Accuracy reduces as Tree size increases<\/li>\n  <li>Accuracy reduces as number of trees increase <\/li>\n  <li>Accuracy reduces as with both increasing together<\/li>\n<\/ul>","f00a7a9f":"<strong style=\"color:Tomato;\">Load Data Set <\/strong>","b1590d95":"<h1 style=\"color:DodgerBlue;\">Tune The Learning Rate & Gamma in XGBoost<\/h1>","299a2171":"<h1 style=\"color:DodgerBlue;\">Tune The Gamma in XGBoost<\/h1>","d1cf957b":"There is a relationship between the number of trees in the model and the depth of each tree.\n\nWe would expect that deeper trees would result in fewer trees being required in the model, and the inverse where simpler trees (such as decision stumps) require many more trees to achieve similar results.\n\nWe can investigate this relationship by evaluating a grid of n_estimators and max_depth configuration values. To avoid the evaluation taking too long, we will limit the total number of configuration values evaluated. Parameters were chosen to tease out the relationship rather than optimize the model.\n\nWe will create a grid of 10 different n_estimators values (100-1000) and 8 different max_depth values (3-20) and each combination will be evaluated using 5-fold cross validation. <ul>A total of 10 X 8 X 5 or <b>400 models<\/b> will be trained and evaluated.<\/ul>","5b1f9552":"Using scikit-learn we can perform a grid search of the n_estimators model parameter, evaluating a series of values from <b>100 to 1000 <\/b>with a step size of 100 (100, 200, 300, 400, 500, 600, 700, 800, 900, 1000).\nWe can perform this grid search on the Otto dataset, using 10-fold cross validation, requiring 100 models to be trained (10 configurations * 10 folds).","4ec85917":"<h1 style=\"color:DodgerBlue;\">Tune The Number of Trees and Max Depth in XGBoost<\/h1>","f723d593":"Successfully executed in <b>0.7 mins<\/b>\n<ul>\nAccuracy improves as learning rate increase from 0.05 till 0.15. <b>Accuracy peaks learning rate 0.05 to 0.1. gamma 5 & 10<\/b> followed by slow decline.","eeccee01":"<li>gamma=10, <\/li>\n<li>learning_rate=0.05,  <\/li>\n<li>max_depth=4, <\/li>\n<li>n_estimators=500,  <\/li>\n<li>verbosity=1 <\/li>","d554e1b1":"<strong style=\"color:Tomato;\">Define Hyper-Parameters <\/strong>","ff015cd9":"Here are trends of accuracy for various Tree sizes accross number of trees.","cbfa2767":"<b> Obejective<\/b> : Observe the impact of changing parameter on Accuracy. We would use a Small dataset of Dimension (7043, 31)","f709746d":"The optimal configuration was <b>max_depth=3<\/b> resulting in <b>accuracy of 0.799947<\/b>. Successfully executed in 0.3 mins\n\nReviewing the plot of Accuracy scores, we can see a drop from max_depth=3 to max_depth=10 then pretty even and trending down performance for the rest the values of max_depth.\n\nAlthough the best score was observed for max_depth=3, it is interesting to note that there was <b>practically little difference between using max_depth=8 or max_depth=20.<\/b>\n\nThis suggests a point of diminishing returns in max_depth on a problem that you can tease out using grid search. <b>Using tree with Depth 3 would solve the purpose.<\/b>","37c76390":"The best number of trees was <b>n_estimators=100<\/b> resulting in a <b>Accuracy of 78.67%<\/b>. we can see that accuracy dropped with more number of trees but it stablizes after that. it means we did not get any significant advantage by adding further number of trees. Successfully executed in <b>1.6 mins<\/b>"}}