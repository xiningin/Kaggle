{"cell_type":{"14c938ff":"code","bb420567":"code","f688aeaa":"code","d6d9ca99":"code","25190dc3":"code","2f01075a":"code","04059377":"code","a9070e72":"code","5cbff4d8":"code","3b2b7739":"code","c8eb3d05":"code","7c242e8c":"code","a144d3ad":"code","919b913e":"code","fec90f89":"code","d9bdbf64":"code","328d79d7":"code","8050c7c2":"code","7a738a6c":"code","51572f07":"code","65429551":"code","4351d360":"code","eaf5ad3d":"code","15bf34eb":"code","9da29752":"code","425a40b8":"code","1b5f78c5":"code","6701408d":"code","40673025":"code","1914bdc5":"code","6eff7cdc":"code","40772076":"code","a13314c5":"code","8fecc673":"code","5e008a0a":"code","a3a3d474":"code","c6067537":"markdown","60fc32e8":"markdown","43f8fcb8":"markdown","16068fe5":"markdown","49371002":"markdown","9b1d70a8":"markdown","03c473ea":"markdown","048b7f08":"markdown","5dad5c48":"markdown","b526602f":"markdown","92170458":"markdown","2f2bd968":"markdown","e124b72b":"markdown","bfb1f480":"markdown","63c29100":"markdown","afe9230a":"markdown","6673628d":"markdown","91e4b99e":"markdown","b1cf0a55":"markdown","e911f4a3":"markdown","71cc421b":"markdown","4d180a73":"markdown","b15f0829":"markdown","2d070c27":"markdown","1cae26b4":"markdown","021c9e09":"markdown","72192df3":"markdown","77c7774f":"markdown","ef08a83a":"markdown"},"source":{"14c938ff":"# Work with Data - the main Python libraries\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\n\n# Visualization\nimport matplotlib.pyplot as plt\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, ShuffleSplit, GridSearchCV\n\n# Modelling - sklearn\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Modeling - NN models\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import optimizers\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.callbacks import ReduceLROnPlateau\n#import tensorflow as tf\n\n# Metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error as mse\n\nimport warnings\nwarnings.simplefilter('ignore')","bb420567":"# Download training data\ntrain = pd.read_csv('..\/input\/ammonium-prediction-in-river-water\/train.csv')","f688aeaa":"# Display the first 5 rows of the training dataframe.\ntrain.head()","d6d9ca99":"# Information for training data\ntrain.info()","25190dc3":"# Download test data\ntest = pd.read_csv('..\/input\/ammonium-prediction-in-river-water\/test.csv')","2f01075a":"# Display the 7 last rows of the training dataframe\ntest.tail()","04059377":"test.info()","a9070e72":"# Select the stations with the most data in training dataset\ntrain = train.drop(['Id','3','4','5','6','7'], axis = 1)\ntrain = train.dropna().reset_index(drop=True)\ntrain.info()","5cbff4d8":"# Display the statistics for training data\ntrain.describe()","3b2b7739":"# Selecting a target featute and removing it from training dataset\ntarget = train.pop('target')","c8eb3d05":"# Select the stations with the most data in test dataset\ntest = test.drop(['Id','3','4','5','6','7'], axis = 1)\ntest = test.dropna().reset_index(drop=True)","7c242e8c":"# Display basic information about the test data\ntest.info()","a144d3ad":"# Training data splitting to new training (part of the all training) and validation data\ntrain_all = train.copy()\ntarget_all = target.copy()\ntrain, valid, target_train, target_valid = train_test_split(train_all, target_all, test_size=0.2, random_state=0)","919b913e":"train","fec90f89":"# Display information about new training data\ntrain.info()","d9bdbf64":"# Display information about validation data\nvalid.info()","328d79d7":"def acc(y_true, y_pred):\n    # Calculation accuracy of prediction\n    return r2_score(y_true, y_pred)","8050c7c2":"# Creation the dataframe with the resulting score of all models\nresult = pd.DataFrame({'model' : ['NN Regressor', 'NN Regressor with Dropout', 'MLP Regressor'], \n                       'train_score': 0, 'train_mse': 0, 'valid_score': 0, 'valid_mse': 0})\nresult","7a738a6c":"batch_size_num = 32\n#batch_size_num = int(len(train)\/5)\n#batch_size_num","51572f07":"%%time\ndef build_nn():\n\n    # Initializing the NN with 3 layers including 2 hidden layers\n    model = Sequential()\n\n    # The first hidden layer of the NN with input data\n    model.add(Dense(units=4, activation='relu', input_shape=(len(train.columns),)))\n    \n    # The second hidden layer of the NN\n    model.add(Dense(units=3, activation='relu'))\n    \n    # The output layer\n    model.add(Dense(units=1, activation='sigmoid'))\n\n    # Compiling the NN\n    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n\n    learning_rate_reduction = ReduceLROnPlateau(monitor='val_mse', \n                                    patience=3, \n                                    verbose=1, \n                                    factor=0.5, \n                                    min_lr=0.0001)\n    return model\n\nnn_model = build_nn()\nnn_model.fit(train, target_train, batch_size=batch_size_num, epochs=200, validation_data=(valid, target_valid), verbose=0)\n\n# Drawing metrics plot\nplt.plot(nn_model.history.history['mse'])\nplt.title('Metrics of NN model')\nplt.xlabel('Epochs')\nplt.ylabel('Metrics \"Mean Square Error\"') \nplt.show()\n\n# Prediction for training data\ny_train_nn = nn_model.predict(train)\n\n# Accuracy of model\nacc_pred = round(acc(target_train, y_train_nn), 2)\nprint(f'Accuracy of NN model model training is {acc_pred}')\n\n# Save to result DataFrame\nresult.loc[result['model'] == 'NN Regressor', 'train_score'] = acc_pred\nresult.loc[result['model'] == 'NN Regressor', 'train_mse'] = mse(target_train, y_train_nn)","65429551":"# NN structure and parameters\nnn_model.summary()","4351d360":"# Print rounded acc_pred to 2 decimal values after the text\ny_val_nn = nn_model.predict(valid)\nacc_pred_valid = round(acc(target_valid, y_val_nn),2)\nresult.loc[result['model'] == 'NN Regressor', 'valid_score'] = acc_pred_valid\nresult.loc[result['model'] == 'NN Regressor', 'valid_mse'] = round(mse(target_valid, y_val_nn),2)\nprint(f'Accuracy of NN Regressor model prediction for valid dataset is {acc_pred_valid}')","eaf5ad3d":"%%time\ndef build_nn2():\n\n    # Initializing the NN with 3 layers including 2 hidden layers and Dropout\n    model = Sequential()\n\n    # The first hidden layer of the NN with input data\n    model.add(Dense(units=4, activation='relu', input_shape=(len(train.columns),)))\n    \n    # Dropout\n    model.add(Dropout(0.2))\n    \n    # The second hidden layer of the NN\n    model.add(Dense(units=3, activation='relu'))\n    \n    # The output layer\n    model.add(Dense(units=1, activation='sigmoid'))\n\n    # Compiling the NN\n    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n    \n    learning_rate_reduction = ReduceLROnPlateau(monitor='val_mse', \n                                    patience=3, \n                                    verbose=1, \n                                    factor=0.05, \n                                    min_lr=0.0001)\n    \n    return model\n\nnn_model2 = build_nn2()\nnn_model2.fit(train, target_train, batch_size=batch_size_num, epochs=400, validation_data=(valid, target_valid), verbose=2)\n\n# Drawing metrics plot\nplt.plot(nn_model2.history.history['mse'])\nplt.title('Metrics of NN Regressor with Dropout')\nplt.xlabel('Epochs')\nplt.ylabel('Mean Square Error') \nplt.show()\n\n# Prediction for training data\ny_train_nn2 = nn_model2.predict(train)\n\n# Accuracy of model\nacc_pred = round(acc(target_train, y_train_nn2), 2)\nprint(f'Accuracy of NN Regressor with Dropout training is {acc_pred}')\n\n# Save to result DataFrame\nresult.loc[result['model'] == 'NN Regressor with Dropout', 'train_score'] = acc_pred\nresult.loc[result['model'] == 'NN Regressor with Dropout', 'train_mse'] = round(mse(target_train, y_train_nn),2)","15bf34eb":"# Print rounded acc_pred to 2 decimal values after the text\ny_val_nn2 = nn_model2.predict(valid)\nacc_pred_valid = round(acc(target_valid, y_val_nn2),2)\nresult.loc[result['model'] == 'NN Regressor with Dropout', 'valid_score'] = acc_pred_valid\nresult.loc[result['model'] == 'NN Regressor with Dropout', 'valid_mse'] = mse(target_valid, y_val_nn2)\nprint(f'Accuracy of NN Regressor with Dropout prediction for valid dataset is {acc_pred_valid}')","9da29752":"%%time\n# MLPRegressor\nmlp = MLPRegressor()\nparam_grid = {'hidden_layer_sizes': [i for i in range(2,10)],\n              'solver': ['sgd'],\n              'learning_rate': ['adaptive'],\n              'max_iter': [100]\n              }\n\n# Training model\nmlp_CV = GridSearchCV(mlp, param_grid=param_grid, cv=5, verbose=False)\nmlp_CV.fit(train, target_train)\nprint(mlp_CV.best_params_)\n\n# Prediction for training data\ny_train_mlp = mlp_CV.predict(train)\n\n# Accuracy of model\nacc_pred = round(acc(target_train, y_train_mlp), 2)\nprint(f'Accuracy of MLP Regressor model training is {acc_pred}')\n\n# Save to result dataframe\nresult.loc[result['model'] == 'MLP Regressor', 'train_score'] = acc_pred\nresult.loc[result['model'] == 'MLP Regressor', 'train_mse'] = mse(target_train, y_train_nn)","425a40b8":"# Print rounded acc_pred to 2 decimal values after the text\ny_val_mlp = mlp_CV.predict(valid)\nacc_pred_valid = round(acc(target_valid, y_val_mlp),2)\nresult.loc[result['model'] == 'MLP Regressor', 'valid_score'] = acc_pred_valid\nresult.loc[result['model'] == 'MLP Regressor', 'valid_mse'] = round(mse(target_valid, y_val_mlp),2)\nprint(f'Accuracy of MLP Regressor model prediction for valid dataset is {acc_pred_valid}')","1b5f78c5":"# Prediction of target for test data for all models\ny_test_nn = nn_model.predict(test)\ny_test_nn2 = nn_model2.predict(test)\ny_test_mlp = mlp_CV.predict(test)","6701408d":"def plot_prediction(target, y_nn, y_nn2, y_mlp, data_name, MAV=0.5):\n    # Building plot with target, Maximum allowable value (MAV) and \n    # prediction for the data_name (training, validation or test) data by 3 models\n    \n    x = np.arange(len(y_nn))\n    plt.figure(figsize=(16,10))\n    if target is not None:\n        plt.scatter(x, target, label = \"Target data\", color = 'g')\n    plt.scatter(x, y_nn, label = \"NN prediction\", color = 'b')\n    plt.scatter(x, y_nn2, label = \"NN with Dropout\", color = 'm')\n    plt.scatter(x, y_mlp, label = \"MLP prediction\", color = 'y')\n    plt.plot(x, np.full(len(y_nn), MAV), label = \"Maximum allowable value\", color = 'r')\n    plt.title(f'Prediction for the {data_name} data')\n    plt.legend(loc='best')\n    plt.grid(True)","40673025":"plot_prediction(target_train, y_train_nn, y_train_nn2, y_train_mlp, 'training')","1914bdc5":"plot_prediction(target_valid, y_val_nn, y_val_nn2, y_val_mlp, 'validation')","6eff7cdc":"plot_prediction(None, y_test_nn, y_test_nn2, y_test_mlp, 'test')","40772076":"# Display results of modeling\nresult.sort_values(by=['valid_score', 'train_score'], ascending=False)","a13314c5":"# Select models with minimal overfitting\nresult_best = result[(result['train_score'] - result['valid_score']).abs() < 5]\nresult_best.sort_values(by=['valid_score', 'train_score'], ascending=False)","8fecc673":"# Select the best model\nresult_best.nlargest(1, 'valid_score')","5e008a0a":"# Find a name of the best model (with maximal valid score)\nbest_model_name = result_best.loc[result_best['valid_score'].idxmax(result_best['valid_score'].max()), 'model']","a3a3d474":"print(f'The best model is \"{best_model_name}\"')","c6067537":"I hope you find this notebook useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\n[Go to Top](#0)","60fc32e8":"## 5. Test prediction<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","43f8fcb8":"<a class=\"anchor\" id=\"0\"><\/a>\n# [AI-ML-DS : Training for beginners](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-for-beginners-in-kaggle). Level 3 (difficult). 2020\n## Kaggle GM, Prof. [@vbmokin](https:\/\/www.kaggle.com\/vbmokin)\n### [Vinnytsia National Technical University](https:\/\/vntu.edu.ua\/), Ukraine\n#### [Chair of the System Analysis and Information Technologies](http:\/\/mmss.vntu.edu.ua\/index.php\/ua\/)","16068fe5":"## 2. Download data<a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","49371002":"### It is recommended to start studying this course from notebooks:\n* [AI-ML-DS Training. L1T : Titanic - Decision Tree](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l1t-titanic-decision-tree)\n* [AI-ML-DS Training. L1T : NH4 - linear regression](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l1t-nh4-linear-regression)\n* [AI-ML-DS Training. L2A: NH4 - Tree Regress models](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l2t-nh4-tree-regress-models)\n* [BOD prediction in river - 15 regression models](https:\/\/www.kaggle.com\/vbmokin\/bod-prediction-in-river-15-regression-models)\n\nand then move on to this notebook.","9b1d70a8":"### Example of the NN structure:\n\n![image.png](attachment:image.png)","03c473ea":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download data](#2)\n1. [EDA & FE & Preprocessing data](#3)\n    - [Statistics & FE](#3.1)\n1. [Modeling](#4)\n    - [NN Regressor](#4.1)\n    - [NN Regressor with Dropout](#4.2)\n    - [MLP Regressor](#4.3)    \n1. [Test prediction](#5)\n1. [Results visualization](#6)\n1. [Select the best model](#7)","048b7f08":"### TASK: Experiment with:\n* hidden_layer_sizes (maximum value)\n* max_iter\n* cv (cross-validation)","5dad5c48":"**ADDITIONAL TASK:** Experiment with number of hidden layers and metrics.","b526602f":"### 4.3. MLP Regressor<a class=\"anchor\" id=\"4.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","92170458":"**ADDITIONAL TASK:** Experiment with number of hidden layers and metrics.","2f2bd968":"### Possible Tasks:\n\n* Analysis of data dependences, including EDA.\n\n* Prediction the target data (water quaity in the target station) with the highest accuracy.\n\n* Analysis of impact on the prediction accuracy in target station from the different number of stations (1, 2, ... 7).","e124b72b":"### TASK: Experiment with:\n* batch size\n* units in layers \n* activation functions\n* epochs number\n* patience and factor in ReduceLROnPlateau","bfb1f480":"### 3.1. Statistics & FE<a class=\"anchor\" id=\"3.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","63c29100":"The analysis showed that many values are only available in stations 1 and 2, while others have much less data. I propose select only these two stations.","afe9230a":"## 4. Modeling<a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","6673628d":"### 4.2. NN Regressor with Dropout<a class=\"anchor\" id=\"4.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","91e4b99e":"Dataset has data of the Ammonium ions concentration in river water (the maximum permissible value in Ukraine is 0.5 mg\/cub. dm).\n\nAmmonium ions (NH4) concentration is measured in mg\/cub. dm (ie milligrams in the cubic decimeter).\n\nDatasets has data of river water quality from 8 consecutive stations of the state water monitoring system for Pivdennyi Bug river (from the source of the river to the water intake of the city of Vinnytsia).\n\nTarget is a NH4 concentration in the river crossection with the water intake of the Vinnytsia city.\n\nData for the 1997-2019.","b1cf0a55":"![image.png](attachment:image.png)\n* 1 - the source of the river (see at the station first on the left), \n* ....\n* 8 (target) - the place of water intake in Vinnytsia (see at the station in the lower right corner)","e911f4a3":"### Map of the stations:\nhttp:\/\/monitoring.davr.gov.ua\/EcoWaterMon\/GDKMap\/Index\n\n![image.png](attachment:image.png)\n\nThe upper reaches of the Pivdennyi Bug river","71cc421b":"### TASK: Experiment with:\n* batch size\n* units in layers \n* activation functions\n* epochs number\n* patience and factor in ReduceLROnPlateau\n* Dropout parameter","4d180a73":"## Acknowledgements\n* [Data Science for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/data-science-for-tabular-data-advanced-techniques)\n* [EDA for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/eda-for-tabular-data-advanced-techniques)\n* [Datasets for river water quality prediction](https:\/\/www.kaggle.com\/vbmokin\/datasets-for-river-water-quality-prediction)\n* [Heart Disease - Automatic AdvEDA & FE & 20 models](https:\/\/www.kaggle.com\/vbmokin\/heart-disease-automatic-adveda-fe-20-models)\n* [BOD prediction in river - 15 regression models](https:\/\/www.kaggle.com\/vbmokin\/bod-prediction-in-river-15-regression-models)\n* [The system \"MONITORING AND ENVIRONMENTAL ASSESSMENT OF WATER RESOURCES OF UKRAINE\", State Agency of Water Resources of Ukraine](http:\/\/monitoring.davr.gov.ua\/EcoWaterMon\/GDKMap\/Index)","b15f0829":"### 4.1. NN Regressor<a class=\"anchor\" id=\"4.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","2d070c27":"## 3. EDA & FE & Preprocessing data<a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","1cae26b4":"## 7. Select the best model <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","021c9e09":"## 6. Visualization<a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","72192df3":"## 1. Import libraries<a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","77c7774f":"### 3.3. Training data splitting<a class=\"anchor\" id=\"3.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","ef08a83a":"## Dataset [Ammonium prediction in river water](https:\/\/www.kaggle.com\/vbmokin\/ammonium-prediction-in-river-water)"}}