{"cell_type":{"e0d840fd":"code","57c70f48":"code","6bc20aeb":"code","5a7e48b0":"code","05af6cd5":"code","da288ef8":"code","c95ec0b7":"code","8a604e6a":"code","a6b2e620":"code","e24da330":"code","8e6c6899":"code","a6da9529":"code","6bb13a8a":"code","a2494cad":"code","0a7feba1":"code","a8d3b549":"code","3506f4ef":"code","4a0ea60a":"code","40423241":"code","d8c949f1":"code","7aa238df":"markdown","d43c7c00":"markdown","1d971e6c":"markdown","3b140c17":"markdown","20898355":"markdown","336dd6c5":"markdown","f85067db":"markdown","f65ee4fd":"markdown","fa9ec6eb":"markdown","37e85123":"markdown","bd1fe1a4":"markdown","b254c696":"markdown","8ff77d2e":"markdown","6e43055e":"markdown","5bc07d10":"markdown","89dc2357":"markdown","0b69b724":"markdown","660b759b":"markdown"},"source":{"e0d840fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","57c70f48":"import seaborn as sns\npd.set_option('display.width', 100)\npd.set_option('display.max_columns', 20)\nsns.set_theme(color_codes=True, style='darkgrid', \n              palette='deep', font='sans-serif')","6bc20aeb":"df_train = pd.read_csv ( '\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv' )\ndf_train.drop ('Id', axis = 1, inplace = True )\ndf_train.head().style.set_properties(**{'background-color':'black',\n                                     'color': 'white'})","5a7e48b0":"df_train.isnull().sum()","05af6cd5":"df_train.info()","da288ef8":"# set target variable as category\ndf_train['Risk_Flag']=df_train['Risk_Flag'].astype('category')","c95ec0b7":"#Import ploting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots \ncolors = ['#ffa07a','#00b2ff']\nsns.set(palette=colors, font='Serif', style='white', rc={'axes.facecolor':'#f1f1f1', 'figure.facecolor':'#f1f1f1'})\nsns.palplot(colors)","8a604e6a":"df_train.describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='tab10')","a6b2e620":"#Lets check the Target features first\nfig = plt.figure(figsize=(10,6))\nax=sns.countplot(data=df_train, x='Risk_Flag')\nfor i in ax.patches:\n    ax.text(x=i.get_x()+i.get_width()\/2, y=i.get_height()\/7, s=f\"{np.round(i.get_height()\/len(df_train)*100,0)}%\", ha='center', size=40, weight='bold', rotation=360, color='white')\nplt.title(\"Risk_Flag Feature\", size=20, weight='bold')\nplt.annotate(text=\"No potential default on loans\", xytext=(0.5,150000),xy=(0.2,120000), arrowprops =dict(arrowstyle=\"->\", color='black', connectionstyle=\"angle3,angleA=0,angleB=90\"), color='black')\nplt.annotate(text=\"Potential default on loans\", xytext=(0.8,130000),xy=(1,30000), arrowprops =dict(arrowstyle=\"->\", color='black',  connectionstyle=\"angle3,angleA=0,angleB=90\"), color='black')\nplt.show()","e24da330":"g = sns.PairGrid(df_train)\ng.map(sns.scatterplot)\nplt.show()","8e6c6899":"from sklearn.preprocessing import LabelEncoder\n\nbinary_class = ['Married\/Single', 'Car_Ownership']\nfor column in binary_class:\n    print ( '\\nBefore:', df_train [column].unique () )\n    lab_enc = LabelEncoder()\n    df_train [column] = lab_enc.fit_transform ( df_train [column].values )\n    print ('')\n    print ( 'After:\\n', df_train [column] )\n    print ( '*' * 50 )\n\n# rename column Single\ndf_train.rename(columns = { 'Married\/Single' : 'Single' }, inplace = True)\ndf_train['Single']=df_train['Single'].astype('category')\ndf_train['Car_Ownership']=df_train['Car_Ownership'].astype('category')","a6da9529":"one_hot_class = ['House_Ownership', 'CITY', 'STATE', 'Profession']\nfor column in one_hot_class:\n    one_hot = pd.get_dummies ( df_train [column] ,\n                drop_first = True)\n    df_train = pd.concat([df_train, one_hot], axis=1)\n    df_train.drop (column, axis = 1, inplace = True )\n    \ndf_train.head().style.set_properties(**{'background-color':'black',\n                                     'color': 'white'})","6bb13a8a":"print ( df_train.info() )","a2494cad":"from sklearn.model_selection import train_test_split\nX, y = df_train.drop ('Risk_Flag', axis=1).values , df_train.Risk_Flag.values\nX_train, X_test, y_train, y_test = train_test_split ( X, y,\n                                                     test_size = 0.3,\n                                                     random_state = 1,\n                                                     stratify = y)","0a7feba1":"from imblearn.over_sampling import SMOTE\n\nprint ('Number of observations in the target variable before oversampling of the minority class:', np.bincount (y_train) )\n\nsmt = SMOTE ()\nX_train, y_train = smt.fit_resample (X_train, y_train)\n\nprint ('\\nNumber of observations in the target variable after oversampling of the minority class:', np.bincount (y_train) )","a8d3b549":"from sklearn.preprocessing import StandardScaler\nstd_scaler = StandardScaler()\nX_train_std = std_scaler.fit_transform ( X_train )\nX_test_std = std_scaler.transform ( X_test )","3506f4ef":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\n\ntree = DecisionTreeClassifier ( random_state = 1 )\ntree.fit ( X_train_std, y_train )\ny_pred = tree.predict ( X_test_std )\nprint ( 'Accuracy score: %.2f' %accuracy_score ( y_test, y_pred ) )\nprint ( 'Roc_Auc score: %.2f' %roc_auc_score ( y_test, y_pred ) )","4a0ea60a":"from sklearn.model_selection import GridSearchCV\n# range of parameter values\nsplit_range = [ 8, 10 ]\n# parameters grid\ngrid_param = [\n    { 'criterion' : [ 'entropy' ],\n     'splitter' : [ 'best', 'random' ],\n     'min_samples_split' : split_range }\n]\ngs = GridSearchCV ( estimator = tree,\n                   param_grid = grid_param,\n                   scoring = 'roc_auc',\n                   cv = 3,\n                   refit = True,\n                   n_jobs = 4\n                   )\n\ngs = gs.fit ( X_train, y_train )\n\nprint ( 'Best hyperparameter:', gs.best_params_ )\n\nprint ( 'Best score: %.3f' %gs.best_score_ )\n\ngs = gs.best_estimator_","40423241":"gs.fit ( X_train_std, y_train )\ny_pred_gs = gs.predict ( X_test_std )\nprint ( 'Accuracy score: %.2f' %accuracy_score ( y_test, y_pred_gs ) )\nprint ( 'Roc_Auc score: %.2f' %roc_auc_score ( y_test, y_pred_gs ) )","d8c949f1":"from sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix (  y_test, y_pred_gs )\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n#plot 1\nsns.heatmap(conf_matrix,ax=axes[0],annot=True, cmap='Blues', cbar=False, fmt='d')\naxes[0].set_xlabel('\\nPredicted label', size = 14)\naxes[0].set_ylabel('True label\\n', size = 14)\n\n# plot 2\nsns.heatmap(conf_matrix\/np.sum(conf_matrix),ax=axes[1], annot=True, \n            fmt='.2%', cmap='Blues', cbar=False)\naxes[1].set_xlabel('\\nPredicted label', size = 14)\naxes[1].set_ylabel('True label\\n', size = 14)\naxes[1].yaxis.tick_left()\nplt.show()\n","7aa238df":"# INTRODUCTION\n\n### Task Details\nAn organization wants to predict who possible defaulters are for the consumer loans product. They have data about historic customer behavior based on what they have observed. Hence when they acquire new customers they want to predict who is riskier and who is not.\n\n### What do you have to do?\nYou are required to use the training dataset to identify patterns that predict \u201cpotential\u201d defaulters.\n\n### Expected Submission\nSubmissions should be made in the same format as the Sample Notebook provided. Train\/Test split should be 80% for training & 20% for testing.\n\n### Evaluation\nSubmissions will be evaluated on the basis of roc_auc_score on 20% of train_dataset.\n\n<img src=\"https:\/\/www.onlygfx.com\/wp-content\/uploads\/2020\/05\/alert-stamp-3.png\" width=\"600\" height=\"200\" \/>","d43c7c00":"From the confusion matrices it can be deduced that:\n\n- the model fails 2.30% of the time to classify it as non-potential default\n\n- in general, it is noted that it is more wrong to classify as potential defaulting those who in reality are not (9.88%)","1d971e6c":"#### Import libraries and set options","3b140c17":"# MODEL SELECTION AND EVALUATION OF PERFORMANCE","20898355":"###### The classes are heavily skewed we need to solve this issue later, with algorithm SMOTE (Synthetic Minority Oversampling TEchnique).\n\n###### Class 0 represents 88.00% of the dataset, while class 1 only 12.00%.","336dd6c5":"#### Management of categorical data\n\n**One-Hot coding** for the other categorical columns, otherwise one of the most common mistakes would be made, i.e. the classification algorithm will assume that there is an order of magnitude between the various professions, states or cities.","f85067db":"# CONCLUSIONS AND FINAL CONSIDERATIONS","f65ee4fd":"#### CONFUSION MATRIX","fa9ec6eb":"**Based on the requirements of the task in question, we can conclude that the trained tree model achieved a good roc_auc_score of 0.85.**\n\n**For a more in-depth analysis it is advisable to test other classification algortms, perhaps more performing, or to test some ensemble algorithm.**\n\n**I await comments and \/ or suggestions.**","37e85123":"# EDA","bd1fe1a4":"Following model optimization:\n\n- accuracy has improved (0,87 --> 0,88)\n\n- the rac_auc score is fixed at 0.85.\n\nNow let's see in detail what errors the model makes on the test data through the confusion matrix.","b254c696":"#### HYPERPARAMETERS OPTIMIZATION","8ff77d2e":"# IMPORT DATA AND DATA CLEANSING","6e43055e":"#### Minority class oversampling in the training dataset (SMOTE)","5bc07d10":"# PREPROCESSING\n\n#### Management of binary categorical data","89dc2357":"#### Train and test split","0b69b724":"#### Standardization of variables","660b759b":"###### There are no missing-values \u200b\u200bin the training dataset."}}