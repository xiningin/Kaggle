{"cell_type":{"6775e640":"code","da7f263a":"code","48b56ad2":"code","294fa30d":"code","26ba1f1f":"code","aa81678e":"code","d6620def":"code","87e7bb4f":"code","d0431b08":"code","99adeaa9":"code","99049ba1":"code","d2ccd1ef":"code","06a94e7a":"code","8f12e751":"code","474833ad":"code","c3fb809a":"code","4c6928c4":"code","b842863b":"code","ed9b1ced":"code","53a4015e":"code","07aa009c":"code","5507490c":"code","3ae68c70":"code","0d8095e0":"code","c00d00e9":"code","e6289f00":"code","c09c838d":"code","07c78bcf":"code","c7321aec":"code","29daa5d2":"code","cdde619a":"code","0375e909":"code","2fd641e0":"code","6213d773":"code","63579dd8":"code","a7d5e022":"code","aef03a17":"code","87b053a8":"markdown","96d55ed8":"markdown","6ca25b47":"markdown","bd1d4d09":"markdown","cea7381e":"markdown","be8526cd":"markdown","799e67c3":"markdown","f2e9d3e9":"markdown","1969c761":"markdown","78202252":"markdown","17bb4914":"markdown","300eb60f":"markdown"},"source":{"6775e640":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport os\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\n\nimport numpy as np\nimport matplotlib.pylab as plt\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom tqdm import tqdm\n\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nprint(\"\\u2022 Using TensorFlow Version:\", tf.__version__)\nprint(\"\\u2022 Using TensorFlow Hub Version: \", hub.__version__)\nprint('\\u2022 GPU Device Found.' if tf.test.is_gpu_available() else '\\u2022 GPU Device Not Found. Running on CPU')\n\ntf.config.list_physical_devices('GPU')\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n\n#tf.config.optimizer.set_jit(True)\n\n#from numpy.random import seed\n#seed(16)\n#tf.random.set_seed(32)\n\nmodule_selection = (\"mobilenet_v2\", 224, 1280) #@param [\"(\\\"mobilenet_v2\\\", 224, 1280)\", \"(\\\"inception_v3\\\", 299, 2048)\"] {type:\"raw\", allow-input: true}\nhandle_base, pixels, FV_SIZE = module_selection\nMODULE_HANDLE =\"https:\/\/tfhub.dev\/google\/tf2-preview\/{}\/feature_vector\/4\".format(handle_base)\nIMAGE_SIZE = (pixels, pixels)\nprint(\"Using {} with input size {} and output dimension {}\".format(MODULE_HANDLE, IMAGE_SIZE, FV_SIZE))","da7f263a":"train_dir = '..\/input\/chest-xray-pneumonia\/chest_xray\/train'\nval_dir = '..\/input\/chest-xray-pneumonia\/chest_xray\/val'","48b56ad2":"NORMAL_dir = os.path.join(train_dir, 'NORMAL')\nNORMAL_fnames = os.listdir(NORMAL_dir)\nprint(len(NORMAL_fnames))","294fa30d":"PNEUMONIA_dir = os.path.join(train_dir, 'PNEUMONIA')\nPNEUMONIA_fnames = os.listdir(PNEUMONIA_dir)\nprint(len(PNEUMONIA_fnames))","26ba1f1f":"def format_example(img):\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = x\/255.0\n\n    return x","aa81678e":"from keras.preprocessing import image\npose_name = 'NORMAL'\nfolder = train_dir+'\/'+pose_name","d6620def":"images = []\nimages_org = []\nfolder = folder\nfor img in os.listdir(folder):\n    img_imp = image.load_img(folder+'\/'+img, target_size=(224, 224))\n    x = format_example(img_imp)\n    images.append(x)\n    images_org.append(img_imp)","87e7bb4f":"plt.figure(figsize=(10,8))\nplt.imshow(images_org[11], cmap=plt.cm.binary)","d0431b08":"batch_size = 128\n\n# Add our data-augmentation parameters to ImageDataGenerator\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255.,\n                                   rotation_range = 90,\n                                   width_shift_range = 0.2,\n                                   height_shift_range = 0.2,\n                                   shear_range = 0.1,\n                                   zoom_range = 0.1,\n                                   brightness_range=(0.4, 1.0),\n                                   horizontal_flip = True,\n                                   channel_shift_range=80.0,\n                                   vertical_flip = True)\n\n# Note that the validation data should not be augmented!\nvalid_datagen = ImageDataGenerator( rescale = 1.0\/255.)\n\n# Flow training images in batches of batch_size using train_datagen generator\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    batch_size = batch_size,\n                                                    class_mode = 'binary',\n                                                    target_size = (224, 224),\n                                                    seed=42,\n                                                    shuffle=True)     \n\n# Flow validation images in batches of batch_size using test_datagen generator\nvalidation_generator =  valid_datagen.flow_from_directory(val_dir,\n                                                          batch_size  = batch_size,\n                                                          class_mode  = 'binary', \n                                                          target_size = (224, 224),\n                                                          seed=42,\n                                                          shuffle=True)\n\nlabels_list = list(train_generator.class_indices.keys())\nlabel_dict = train_generator.class_indices","99adeaa9":"print(labels_list)","99049ba1":"print(label_dict)","d2ccd1ef":"class myCallback(tf.keras.callbacks.Callback):\n      def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('val_accuracy')>0.999):\n          print(\"\\nReached 99.9% accuracy so cancelling training!\")\n          self.model.stop_training = True","06a94e7a":"do_fine_tuning = True #@param {type:\"boolean\"}\n\nfeature_extractor = hub.KerasLayer(MODULE_HANDLE, input_shape=IMAGE_SIZE + (3,), \n                                   output_shape=[FV_SIZE],\n                                   trainable=do_fine_tuning)\nnum_classes = 2\n\nprint(\"Building model with\", MODULE_HANDLE)\n\nmodel = tf.keras.Sequential([\n        feature_extractor,\n        tf.keras.layers.Dense(num_classes, activation='softmax')])\n\nmodel.summary()\n\n#@title (Optional) Unfreeze some layers\nNUM_LAYERS = 8 #@param {type:\"slider\", min:1, max:50, step:1}\n      \nif do_fine_tuning:\n    feature_extractor.trainable = True\n    \n    for layer in model.layers[-NUM_LAYERS:]:\n        layer.trainable = True\nelse:\n    feature_extractor.trainable = False","8f12e751":"if do_fine_tuning:\n    model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.005, momentum=0.9),\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                  metrics=['accuracy'])\nelse:\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n\n#We may need to create a model checkpoint \n#for large datasets and also we may require running the py script from the console. \n#Since we do note have GBs of data, we may skip this step\n#checkpoint_filepath = 'XRay_Model\/checkpoint' \n#model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_accuracy', mode='max', save_best_only=True)\n\nEPOCHS = 100 #In this version, we aim at the max val_accuracy with callbacks > 99.9% val_accuracy\ncallbacks = myCallback()\nhist = model.fit(train_generator,\n                 epochs=EPOCHS,\n                 validation_data=validation_generator,\n                 callbacks=[callbacks])","474833ad":"from tensorflow.keras.models import load_model\nSAVED_MODEL = \"XRay_Model\"\n\n# Export the SavedModel\ntf.saved_model.save(model, SAVED_MODEL)\n\n# Convert Using TFLite's Converter\nconverter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n\n#converter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_model = converter.convert()\n\ntflite_model_file = 'XRay_Model.tflite'\n\nwith open(tflite_model_file, \"wb\") as f:\n    f.write(tflite_model)","c3fb809a":"# Load TFLite model and allocate tensors.\nwith open(\"XRay_Model.tflite\", 'rb') as fid:\n    tflite_model = fid.read()\n    \ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\ninterpreter.allocate_tensors()\n\ninput_index = interpreter.get_input_details()[0][\"index\"]\noutput_index = interpreter.get_output_details()[0][\"index\"]","4c6928c4":"test_folder = '..\/input\/chest-xray-pneumonia\/chest_xray\/test'","b842863b":"os.listdir(test_folder)","ed9b1ced":"NORMAL_class_name = 'NORMAL'\nPNEUMONIA_class_name = 'PNEUMONIA'","53a4015e":"test_folder_NORMAL = test_folder+'\/'+NORMAL_class_name\ntest_folder_PNEUMONIA = test_folder+'\/'+PNEUMONIA_class_name","07aa009c":"from keras.preprocessing import image","5507490c":"label_dict[NORMAL_class_name]","3ae68c70":"label_dict[PNEUMONIA_class_name]","0d8095e0":"def get_predictions(folder):\n    images = []\n    images_org = []\n    for img in os.listdir(folder):\n        img_imp = image.load_img(folder+'\/'+img, target_size=(224, 224))\n        x = format_example(img_imp)\n        images.append(x)\n        images_org.append(img_imp)\n    predictions = []\n    for i in images:\n        interpreter.set_tensor(input_index, i)\n        interpreter.invoke()\n        prediction_array = interpreter.get_tensor(output_index)\n        predictions.append(prediction_array)    \n    labels= []\n    for j in predictions:\n        predicted_label = np.argmax(j)\n        labels.append(predicted_label)\n    return labels","c00d00e9":"preds_NORMAL = get_predictions(test_folder_NORMAL)\npreds_PNEUMONIA = get_predictions(test_folder_PNEUMONIA)","e6289f00":"print(preds_NORMAL)","c09c838d":"print(preds_PNEUMONIA)","07c78bcf":"print(len(preds_NORMAL), len(preds_PNEUMONIA))","c7321aec":"from sklearn.metrics import accuracy_score","29daa5d2":"accuracy_score(234*[label_dict[NORMAL_class_name]], preds_NORMAL)","cdde619a":"accuracy_score(390*[label_dict[PNEUMONIA_class_name]], preds_PNEUMONIA)","0375e909":"accuracy_score(234*[label_dict[NORMAL_class_name]] + 390*[label_dict[PNEUMONIA_class_name]], preds_NORMAL + preds_PNEUMONIA)","2fd641e0":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns \n\nplt.figure(figsize=(14,10))\nconf_mat = confusion_matrix(234*[label_dict[NORMAL_class_name]] + 390*[label_dict[PNEUMONIA_class_name]], preds_NORMAL + preds_PNEUMONIA)\nsns.heatmap(conf_mat, annot=True, fmt='d', cmap=\"YlGnBu\",\n            xticklabels=labels_list, yticklabels=labels_list)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')","6213d773":"def set_batch(label):\n    pose_name = label\n    folder = train_dir+'\/'+pose_name\n    images = []\n    images_org = []\n    folder = folder\n    for img in os.listdir(folder):\n        img_imp = image.load_img(folder+'\/'+img, target_size=(224, 224))\n        x = format_example(img_imp)\n        images.append(x)\n        images_org.append(img_imp)\n    return images_org","63579dd8":"def plot_batch(label):\n    images_org = set_batch(label)\n    plt.figure(figsize=(10,10))\n    for n in range(9):\n        ax = plt.subplot(3,3 ,n+1)\n        plt.imshow(images_org[n])\n        plt.title(label)\n        plt.axis(\"off\")","a7d5e022":"plot_batch('NORMAL')\nplt.savefig('NORMAL.png')","aef03a17":"plot_batch('PNEUMONIA')\nplt.savefig('PNEUMONIA.png')","87b053a8":"AI Models working at the Edge might introduce great ease to our daily life, these benefits could be applied to a wide range of domains such as retail & fashion and health. In recent years AI is being used for supporting the medical examination, I believe this could prevent false diagnosis, if we deploy such systems as decision support to doctors. The impact that AI system creates in terms of preserving the human health is huge and it may further accelerate the medical examination processes. \n\nIn this notebook I show how we can achieve to identify whether a Chest X-Ray Scanning contains a PNEUMONIA or not, by training a few latter layers of a pretrained MobileNet, in the TF framework and then we export our model as tf.lite ready to be deployed to our mobile app to serve us as mobile Chest X-Ray Examiner!","96d55ed8":"Checking model performance on images with disease:","6ca25b47":"## Export the Model","bd1d4d09":"## Let's Visualize!","cea7381e":"and the overall score:","be8526cd":"## Training the Model","799e67c3":"# TFLite with MobileNet for Image Classification at the Edge for Chest X-Ray Scanning","f2e9d3e9":"## Data Preprocessing","1969c761":"![Screen Shot 2021-08-07 at 17.24.04.png](attachment:d7535d3d-fb94-4ca8-8208-4bbf8abd1359.png)","78202252":"## Evaluating the Performance on Unseen Data","17bb4914":"## Defining the Model","300eb60f":"Checking model performance on healthy images:"}}