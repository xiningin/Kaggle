{"cell_type":{"8792696d":"code","a16fdf27":"code","b100a9ca":"code","c47e5c71":"code","c02099f6":"code","5b6d7ccb":"code","d05941e1":"code","309bd7c4":"code","608055c0":"code","f8ba9317":"code","401b892c":"code","4fc13296":"code","6e986d73":"code","3e102960":"code","a837c875":"code","f7c24c11":"code","1c27bd8e":"code","6a83aefd":"code","edcf089d":"code","e59e35a2":"code","41860dda":"markdown","9b0ed46c":"markdown","6b545c8b":"markdown","097a7668":"markdown","2e943cb0":"markdown","edd86240":"markdown","e7ff13a0":"markdown","e0fead70":"markdown","37eb659e":"markdown","322f8b9d":"markdown","3aa79d44":"markdown","8d357c44":"markdown","0b238d95":"markdown","a05c5076":"markdown","c44d6ecb":"markdown","01c965d2":"markdown"},"source":{"8792696d":"# make sure to have the latest sns version\n!pip install seaborn --upgrade","a16fdf27":"import seaborn as sns\nsns.__version__","b100a9ca":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c47e5c71":"df = pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","c02099f6":"df","5b6d7ccb":"df.info()","d05941e1":"for feat in df.columns:\n    print(feat)\n    print(df[feat].dtype)\n    print(df[feat].unique())\n    print('#'*30)","309bd7c4":"# change dtype\ndf['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n\n# fill NaNs with mean\ndf['TotalCharges'].fillna(df['TotalCharges'].mean(), inplace=True)","608055c0":"sns.countplot(data=df, x=df['Churn']);","f8ba9317":"# Step 1: remove features\ndf_cross = df.drop(columns=['customerID', 'tenure', 'MonthlyCharges', 'TotalCharges'])\n\n# Step 2: create empty list\ncrosstabs = []\n# Step 3: For loop to create the crosstabs\nfor feat in df_cross.columns:\n    crosstab = pd.crosstab(df_cross[feat], df_cross['Churn'])\n    # Step 4: append them to the list\n    crosstabs.append(crosstab)\n\n# Step 5: concate each of them with the column names as index\ncrosstab_count = pd.concat(crosstabs, keys=df_cross.columns[:-1])\ncrosstab_count\n\n\n# this can be done in one line of code:\n# pd.concat([pd.crosstab(df_cross[x], df_cross['Churn']) for x in df_cross.columns[:-1]], keys=df_cross.columns[:-1])\n# code from udemy course: Data Science & Deep Learning for Business","401b892c":"crosstab_count['percentage'] = (crosstab_count['Yes'] \/ (crosstab_count['Yes'] + crosstab_count['No'])* 100)\ncrosstab_count.sort_values('percentage', ascending=False)","4fc13296":"df_ratio = df[['tenure', 'MonthlyCharges', 'TotalCharges', 'Churn']]\n\nfig, axes = plt.subplots(1, 3, figsize=[12,6])\n# space between the plots \nfig.subplots_adjust(wspace=0.5)\nfor feat, ax in zip(df_ratio.columns[:-1], axes.flatten()):\n    sns.violinplot(data=df_ratio, y=feat, x=df_ratio['Churn'], ax=ax)","6e986d73":"df.drop(columns='customerID', inplace=True)","3e102960":"df_bin = df.select_dtypes('object')\n\n# select only features with two unique values\nlist_bin = [df_bin[x].name for x in df_bin.columns if df_bin[x].nunique() == 2]\n# create dict with values\ndict_bin = {'Yes': 1, 'No': 0, 'Female': 0, 'Male': 1}\n\n# for loop to go through each binary feature and map the values\nfor feat in list_bin:\n    df_bin[feat] = df_bin[feat].map(dict_bin)","a837c875":"# dtype object only includes categorical variables now so i can just filter them\ndf_cat = df_bin.select_dtypes('object')\n\ndf_dummy = pd.get_dummies(df_cat, drop_first=True)","f7c24c11":"# cobine Data Frames\ndf_final = pd.concat([df.select_dtypes(exclude='object'), df_bin.select_dtypes(exclude='object'), df_dummy], axis=1)\ndf_final","1c27bd8e":"X = df_final.drop(columns='Churn')\ny = df_final['Churn']\n\n# we will use the stratify argument to account for the imbalance\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","6a83aefd":"# use Column Transformer to apply scaling only to the specified columns\nct = ColumnTransformer([('scaler', StandardScaler(), ['tenure', 'MonthlyCharges', 'TotalCharges'])], remainder='passthrough')\n# fit and transform X_train\nX_train_sc = ct.fit_transform(X_train)\n# transform X_test \nX_test_sc = ct.transform(X_test)","edcf089d":"# initiate models \nlogreg = LogisticRegression()\nrf = RandomForestClassifier()\nadac = AdaBoostClassifier()\ngbc = GradientBoostingClassifier()\n\nmodel_list = [logreg, rf, adac, gbc]\n\nfor model in model_list:\n    model.fit(X_train_sc, y_train)\n    preds = model.predict(X_test_sc)\n    print('accuracy: ', accuracy_score(y_test, preds))\n    print(confusion_matrix(y_test, preds))\n    print(classification_report(y_test, preds))\n    print('#'*60)","e59e35a2":"col_list = list(df_final.drop(columns = 'Churn').columns)\nimportance = logreg.coef_.flatten()\n\nfeat_importance = pd.DataFrame({'feature': col_list, 'importance': importance})\nfeat_importance.sort_values('importance')","41860dda":"## Look at the churn across the dataset","9b0ed46c":"Now we just need to addres the categorical vairables and transform them into dummy varibales","6b545c8b":"# Why do telco customers leave?\n\n**Customer chrun** is when an existing customer, user, subsctiber or any kind of client stops doing business or ends the relationship with a company\n\n**The goal** of this kernel is to explore the different variables and thier effect on churn. ","097a7668":"### Conlusion\n- As seen in the EDA contract type (`Contract_Two year`, `Contract_One yar`) are influential variables predicting that a customer doesn't churn\n- `Fiber optic` and digital billing (`MonthlyCharges`, `PaperlessBilling`) are strong incentives for customers to churn ","2e943cb0":"### Conclusion\n- `tenure`: the number of month a customer has styed in the company is a good indicator if a person is leaving nor not. \n- `MonthlyCharges`: high monthly charges lead to more churn\n- `TotalCharges`: there seems to be no difference, but you can see that the body for chrun-yes is bigger than for no, which makes sense, because the most people leave in the first months which explains the low total charges","edd86240":"### Conclusion\n- the higest chrun rates or for: `PaymentMethod`-electronic check, `Contract`-month to month, and `InternetService`-fiber optic\n    * this makes sense since one can quicly change the servie when they have a month to month contract and a digital payment method \n- the lowest chrun rate are for customers how have a tow year `Contract` and no internet service \n    * this also is as expected since cutomers who aren't online can't easily compare or change contracts, especially when they comitted for two years","e7ff13a0":"### Crosstab\n- a crosstab is used to compare two (or more) factors by creating a frequancy table, unless an aggregation function is passed\n- this only works with categorical data\n\n#### Steps\n1. remove non-categorical data\n2. create an empty list to store every crosstab for each feature\n3. create the inidvidual crosstabs for each feature with a for loop\n4. save them in the list with `.append`\n5. concat each individual crosstab and pass the coloumns as keys to create the outer index","e0fead70":"## Preprocessing\n- drop `customerID`\n- map binary colummns to 0 and 1\n- creat dummy variabales\n- combine Data Frames\n> - split into x and y for train and test\n- scale features","37eb659e":"## Make predictions","322f8b9d":"As expected Churn is imbalanced. We need to keep that in mind when spliting the data in train and test. Stratify is the key word.","3aa79d44":"## Resources\n- https:\/\/stackoverflow.com\/questions\/38420847\/apply-standardscaler-to-parts-of-a-data-set","8d357c44":"After we created the crosstab we can calculate the percentage for each factor to get a quick overview in which instances churn is especially prevalent","0b238d95":"Logistic Regression performed the best out of evey model tested","a05c5076":"Now that we looked at the categorical and binary data lets us now compare the ratio scaled data ","c44d6ecb":"## Feature importance","01c965d2":"### Transformation\n- `TotalCharges` shouldn't be an object, simple transformation isn't possible, beacuse there are various emoty values which are formatted as a string"}}