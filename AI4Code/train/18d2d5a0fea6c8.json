{"cell_type":{"1685f8df":"code","7331fa67":"code","8cc001e9":"code","7332999a":"code","bbe25172":"markdown","904d1af1":"markdown","9463014f":"markdown","e8fb46a6":"markdown","87aaf8cc":"markdown","4f778bb7":"markdown"},"source":{"1685f8df":" !pip install --upgrade google-cloud-storage","7331fa67":"from ipywidgets import interact, widgets\nfrom IPython.display import display, clear_output\nimport json\nimport os\ntext = widgets.Text(\n    value='my gcs.json auth',\n    placeholder='Paste your gcs json auth file here!',\n    description='Paste your gcs json auth file:',\n    disabled=False\n)\ndisplay(text)\n\ndef callback(text):\n    # replace by something useful\n    text = text.value.replace('\\n', '\\\\n')\n    \n    try:\n        json.loads(text)\n        with open (\"gcs.json\", \"w\") as f:\n            f.write(text) \n        \n    except Exception as e:\n        print(e)\n    clear_output()\n    \n\ntext.on_submit(callback)","8cc001e9":"from google.cloud import storage\n\n# Instantiates a client\nif os.path.isfile('gcs.json') \n    storage_client = storage.Client.from_service_account_json(\n            'gcs.json')\n# -- uncomment the below to create a new bucket--\n# The name for the new bucket\n# bucket_name = 'my-bucket-name'\n\n# # Creates the new bucket\n# bucket = storage_client.create_bucket(bucket_name)\n\n# print('Bucket {} created.'.format(bucket.name))","7332999a":"\ndef upload_blob(storage_client, bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Uploads a file to the bucket.\"\"\"\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n\n    blob.upload_from_filename(source_file_name)\n\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))","bbe25172":"4.use the below to upload, e.g. \n`upload_blob(storage_client, \"my-bucket\", \"models\/stage-2.pth\", \"stage-2.pth\")`","904d1af1":"1. Install google cloud storage python sdk","9463014f":"Kaggle kernels can be a bit of a pain to extract data from sometimes since you can't get the output until you commit the notebook, and if it got some neural network training in, it might take a _verrrrrrry_ long time... and in the meantime, if you leave your notebook for a few hours and it restarts, you lost all your hard work, so I've figure out how to transfer intermediate results to gcs:","e8fb46a6":"2.setup your auth keys as per https:\/\/cloud.google.com\/storage\/docs\/reference\/libraries#client-libraries-install-python, download the authentication json file to your computer, then join all the split lines so it's one long string (the `cmd+J` shortcut if you are using atom is a useful option)","87aaf8cc":"3.Create your bucket for storing stuff","4f778bb7":"save your gcs json into a json file on the kernel env by dumping it in through the jupyter widget text box as per below (the textbox destroys itself after running so you don't need to worry about having keys shown if you share your kernel)"}}