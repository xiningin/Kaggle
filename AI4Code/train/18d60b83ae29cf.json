{"cell_type":{"0be4b72c":"code","d468ad83":"code","b9cc457c":"code","ee621c32":"code","5e2e4b92":"code","a7ece9fb":"code","e17a416d":"code","ed4a37cb":"code","6bf58f4d":"code","eca6f7e6":"code","36808d5b":"code","5688f525":"markdown"},"source":{"0be4b72c":"# !pip install lightgbm==2.3.1\n# import lightgbm\n# lightgbm.__version__","d468ad83":"# load libraries\nimport gc\nimport re\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold","b9cc457c":"# run functions and pre_settings\ndef one_hot_encoder(df, nan_as_category=True):\n    original_columns = list(df.columns)\n    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n    new_columns = [c for c in df.columns if c not in original_columns]\n    return df, new_columns\n\ndef group(df_to_agg, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n    agg_df = df_to_agg.groupby(aggregate_by).agg(aggregations)\n    agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].upper())\n                               for e in agg_df.columns.tolist()])\n    return agg_df.reset_index()\n\ndef group_and_merge(df_to_agg, df_to_merge, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n    agg_df = group(df_to_agg, prefix, aggregations, aggregate_by= aggregate_by)\n    return df_to_merge.merge(agg_df, how='left', on= aggregate_by)\n\ndef do_sum(dataframe, group_cols, counted, agg_name):\n    gp = dataframe[group_cols + [counted]].groupby(group_cols)[counted].sum().reset_index().rename(columns={counted: agg_name})\n    dataframe = dataframe.merge(gp, on=group_cols, how='left')\n    return dataframe\n\ndef reduce_mem_usage(dataframe):\n    m_start = dataframe.memory_usage().sum() \/ 1024 ** 2\n    for col in dataframe.columns:\n        col_type = dataframe[col].dtype\n        if col_type != object:\n            c_min = dataframe[col].min()\n            c_max = dataframe[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    dataframe[col] = dataframe[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    dataframe[col] = dataframe[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    dataframe[col] = dataframe[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    dataframe[col] = dataframe[col].astype(np.int64)\n            elif str(col_type)[:5] == 'float':\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    dataframe[col] = dataframe[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    dataframe[col] = dataframe[col].astype(np.float32)\n                else:\n                    dataframe[col] = dataframe[col].astype(np.float64)\n\n    m_end = dataframe.memory_usage().sum() \/ 1024 ** 2\n    return dataframe\n\nnan_as_category = True","ee621c32":"def application():\n    df = pd.read_csv(r'..\/input\/home-credit-default-risk\/application_train.csv')\n    test_df = pd.read_csv(r'..\/input\/home-credit-default-risk\/application_test.csv')\n    df = df.append(test_df).reset_index()\n\n    # general cleaning procedures\n    df = df[df['CODE_GENDER'] != 'XNA']\n    df = df[df['AMT_INCOME_TOTAL'] < 20000000] # remove a outlier 117M\n    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True) # set null value\n    df['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True) # set null value\n\n    # Categorical features with Binary encode (0 or 1; two categories)\n    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n    \n    # Categorical features with One-Hot encode\n    df, cat_cols = one_hot_encoder(df, nan_as_category)\n\n    # Flag_document features - count and kurtosis\n    docs = [f for f in df.columns if 'FLAG_DOC' in f]\n    df['DOCUMENT_COUNT'] = df[docs].sum(axis=1)\n    df['NEW_DOC_KURT'] = df[docs].kurtosis(axis=1)\n\n    def get_age_label(days_birth):\n        \"\"\" Return the age group label (int). \"\"\"\n        age_years = -days_birth \/ 365\n        if age_years < 27: return 1\n        elif age_years < 40: return 2\n        elif age_years < 50: return 3\n        elif age_years < 65: return 4\n        elif age_years < 99: return 5\n        else: return 0\n    # Categorical age - based on target=1 plot\n    df['AGE_RANGE'] = df['DAYS_BIRTH'].apply(lambda x: get_age_label(x))\n\n    # New features based on External sources\n    df['EXT_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n    df['EXT_SOURCES_WEIGHTED'] = df.EXT_SOURCE_1 * 2 + df.EXT_SOURCE_2 * 1 + df.EXT_SOURCE_3 * 3\n    np.warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n    for function_name in ['min', 'max', 'mean', 'nanmedian', 'var']:\n        feature_name = 'EXT_SOURCES_{}'.format(function_name.upper())\n        df[feature_name] = eval('np.{}'.format(function_name))(\n            df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\n\n    # Some simple new features (percentages)\n    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] \/ df['DAYS_BIRTH']\n    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] \/ df['AMT_CREDIT']\n    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] \/ df['CNT_FAM_MEMBERS']\n    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] \/ df['AMT_INCOME_TOTAL']\n    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] \/ df['AMT_CREDIT']\n\n    # Credit ratios\n    df['CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] \/ df['AMT_GOODS_PRICE']\n    \n    # Income ratios\n    df['INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] \/ df['DAYS_EMPLOYED']\n    df['INCOME_TO_BIRTH_RATIO'] = df['AMT_INCOME_TOTAL'] \/ df['DAYS_BIRTH']\n    \n    # Time ratios\n    df['ID_TO_BIRTH_RATIO'] = df['DAYS_ID_PUBLISH'] \/ df['DAYS_BIRTH']\n    df['CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] \/ df['DAYS_BIRTH']\n    df['CAR_TO_EMPLOYED_RATIO'] = df['OWN_CAR_AGE'] \/ df['DAYS_EMPLOYED']\n    df['PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] \/ df['DAYS_BIRTH']\n\n    # EXT_SOURCE_X FEATURE\n    df['APPS_EXT_SOURCE_MEAN'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n    df['APPS_EXT_SOURCE_STD'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n    df['APPS_EXT_SOURCE_STD'] = df['APPS_EXT_SOURCE_STD'].fillna(df['APPS_EXT_SOURCE_STD'].mean())\n    df['APP_SCORE1_TO_BIRTH_RATIO'] = df['EXT_SOURCE_1'] \/ (df['DAYS_BIRTH'] \/ 365.25)\n    df['APP_SCORE2_TO_BIRTH_RATIO'] = df['EXT_SOURCE_2'] \/ (df['DAYS_BIRTH'] \/ 365.25)\n    df['APP_SCORE3_TO_BIRTH_RATIO'] = df['EXT_SOURCE_3'] \/ (df['DAYS_BIRTH'] \/ 365.25)\n    df['APP_SCORE1_TO_EMPLOY_RATIO'] = df['EXT_SOURCE_1'] \/ (df['DAYS_EMPLOYED'] \/ 365.25)\n    df['APP_EXT_SOURCE_2*EXT_SOURCE_3*DAYS_BIRTH'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['DAYS_BIRTH']\n    df['APP_SCORE1_TO_FAM_CNT_RATIO'] = df['EXT_SOURCE_1'] \/ df['CNT_FAM_MEMBERS']\n    df['APP_SCORE1_TO_GOODS_RATIO'] = df['EXT_SOURCE_1'] \/ df['AMT_GOODS_PRICE']\n    df['APP_SCORE1_TO_CREDIT_RATIO'] = df['EXT_SOURCE_1'] \/ df['AMT_CREDIT']\n    df['APP_SCORE1_TO_SCORE2_RATIO'] = df['EXT_SOURCE_1'] \/ df['EXT_SOURCE_2']\n    df['APP_SCORE1_TO_SCORE3_RATIO'] = df['EXT_SOURCE_1'] \/ df['EXT_SOURCE_3']\n    df['APP_SCORE2_TO_CREDIT_RATIO'] = df['EXT_SOURCE_2'] \/ df['AMT_CREDIT']\n    df['APP_SCORE2_TO_REGION_RATING_RATIO'] = df['EXT_SOURCE_2'] \/ df['REGION_RATING_CLIENT']\n    df['APP_SCORE2_TO_CITY_RATING_RATIO'] = df['EXT_SOURCE_2'] \/ df['REGION_RATING_CLIENT_W_CITY']\n    df['APP_SCORE2_TO_POP_RATIO'] = df['EXT_SOURCE_2'] \/ df['REGION_POPULATION_RELATIVE']\n    df['APP_SCORE2_TO_PHONE_CHANGE_RATIO'] = df['EXT_SOURCE_2'] \/ df['DAYS_LAST_PHONE_CHANGE']\n    df['APP_EXT_SOURCE_1*EXT_SOURCE_2'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2']\n    df['APP_EXT_SOURCE_1*EXT_SOURCE_3'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_3']\n    df['APP_EXT_SOURCE_2*EXT_SOURCE_3'] = df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n    df['APP_EXT_SOURCE_1*DAYS_EMPLOYED'] = df['EXT_SOURCE_1'] * df['DAYS_EMPLOYED']\n    df['APP_EXT_SOURCE_2*DAYS_EMPLOYED'] = df['EXT_SOURCE_2'] * df['DAYS_EMPLOYED']\n    df['APP_EXT_SOURCE_3*DAYS_EMPLOYED'] = df['EXT_SOURCE_3'] * df['DAYS_EMPLOYED']\n\n    # AMT_INCOME_TOTAL : income\n    # CNT_FAM_MEMBERS  : the number of family members\n    df['APPS_GOODS_INCOME_RATIO'] = df['AMT_GOODS_PRICE'] \/ df['AMT_INCOME_TOTAL']\n    df['APPS_CNT_FAM_INCOME_RATIO'] = df['AMT_INCOME_TOTAL'] \/ df['CNT_FAM_MEMBERS']\n    \n    # DAYS_BIRTH : Client's age in days at the time of application\n    # DAYS_EMPLOYED : How many days before the application the person started current employment\n    df['APPS_INCOME_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] \/ df['DAYS_EMPLOYED']\n\n    # other feature from better than 0.8\n    df['CREDIT_TO_GOODS_RATIO_2'] = df['AMT_CREDIT'] \/ df['AMT_GOODS_PRICE']\n    df['APP_AMT_INCOME_TOTAL_12_AMT_ANNUITY_ratio'] = df['AMT_INCOME_TOTAL'] \/ 12. - df['AMT_ANNUITY']\n    df['APP_INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] \/ df['DAYS_EMPLOYED']\n    df['APP_DAYS_LAST_PHONE_CHANGE_DAYS_EMPLOYED_ratio'] = df['DAYS_LAST_PHONE_CHANGE'] \/ df['DAYS_EMPLOYED']\n    df['APP_DAYS_EMPLOYED_DAYS_BIRTH_diff'] = df['DAYS_EMPLOYED'] - df['DAYS_BIRTH']\n\n    print('\"Application_Train_Test\" final shape:', df.shape)\n    return df","5e2e4b92":"def bureau_bb():\n    bureau = pd.read_csv(r'..\/input\/home-credit-default-risk\/bureau.csv')\n    bb = pd.read_csv(r'..\/input\/home-credit-default-risk\/bureau_balance.csv')\n\n    # Credit duration and credit\/account end date difference\n    bureau['CREDIT_DURATION'] = -bureau['DAYS_CREDIT'] + bureau['DAYS_CREDIT_ENDDATE']\n    bureau['ENDDATE_DIF'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_ENDDATE_FACT']\n    \n    # Credit to debt ratio and difference\n    bureau['DEBT_PERCENTAGE'] = bureau['AMT_CREDIT_SUM'] \/ bureau['AMT_CREDIT_SUM_DEBT']\n    bureau['DEBT_CREDIT_DIFF'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_DEBT']\n    bureau['CREDIT_TO_ANNUITY_RATIO'] = bureau['AMT_CREDIT_SUM'] \/ bureau['AMT_ANNUITY']\n    bureau['BUREAU_CREDIT_FACT_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_ENDDATE_FACT']\n    bureau['BUREAU_CREDIT_ENDDATE_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_CREDIT_ENDDATE']\n    bureau['BUREAU_CREDIT_DEBT_RATIO'] = bureau['AMT_CREDIT_SUM_DEBT'] \/ bureau['AMT_CREDIT_SUM']\n\n    # CREDIT_DAY_OVERDUE :\n    bureau['BUREAU_IS_DPD'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 0 else 0)\n    bureau['BUREAU_IS_DPD_OVER120'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 120 else 0)\n\n    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n\n    # Bureau balance: Perform aggregations and merge with bureau.csv\n    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size', 'mean']}\n    for col in bb_cat:\n        bb_aggregations[col] = ['mean']\n\n    #Status of Credit Bureau loan during the month\n    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n\n    # Bureau and bureau_balance numeric features\n    num_aggregations = {\n        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n        'DAYS_CREDIT_UPDATE': ['mean'],\n        'CREDIT_DAY_OVERDUE': ['max', 'mean', 'min'],\n        'AMT_CREDIT_MAX_OVERDUE': ['mean', 'max'],\n        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_OVERDUE': ['mean', 'max', 'sum'],\n        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n        'AMT_ANNUITY': ['max', 'mean', 'sum'],\n        'CNT_CREDIT_PROLONG': ['sum'],\n        'MONTHS_BALANCE_MIN': ['min'],\n        'MONTHS_BALANCE_MAX': ['max'],\n        'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n        'SK_ID_BUREAU': ['count'],\n        'DAYS_ENDDATE_FACT': ['min', 'max', 'mean'],\n        'ENDDATE_DIF': ['min', 'max', 'mean'],\n        'BUREAU_CREDIT_FACT_DIFF': ['min', 'max', 'mean'],\n        'BUREAU_CREDIT_ENDDATE_DIFF': ['min', 'max', 'mean'],\n        'BUREAU_CREDIT_DEBT_RATIO': ['min', 'max', 'mean'],\n        'DEBT_CREDIT_DIFF': ['min', 'max', 'mean'],\n        'BUREAU_IS_DPD': ['mean', 'sum'],\n        'BUREAU_IS_DPD_OVER120': ['mean', 'sum']\n        }\n\n    # Bureau and bureau_balance categorical features\n    cat_aggregations = {}\n    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n\n    # Bureau: Active credits - using only numerical aggregations\n    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n\n    # Bureau: Closed credits - using only numerical aggregations\n    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n\n    print('\"Bureau\/Bureau Balance\" final shape:', bureau_agg.shape)\n    return bureau_agg","a7ece9fb":"def previous_application():\n    prev = pd.read_csv(r'..\/input\/home-credit-default-risk\/previous_application.csv')\n\n    prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n\n    # Days 365.243 values -> nan\n    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n\n    # Add feature: value ask \/ value received percentage\n    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] \/ prev['AMT_CREDIT']\n\n    # Feature engineering: ratios and difference\n    prev['APPLICATION_CREDIT_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n    prev['CREDIT_TO_ANNUITY_RATIO'] = prev['AMT_CREDIT'] \/ prev['AMT_ANNUITY']\n    prev['DOWN_PAYMENT_TO_CREDIT'] = prev['AMT_DOWN_PAYMENT'] \/ prev['AMT_CREDIT']\n\n    # Interest ratio on previous application (simplified)\n    total_payment = prev['AMT_ANNUITY'] * prev['CNT_PAYMENT']\n    prev['SIMPLE_INTERESTS'] = (total_payment \/ prev['AMT_CREDIT'] - 1) \/ prev['CNT_PAYMENT']\n\n    # Days last due difference (scheduled x done)\n    prev['DAYS_LAST_DUE_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_LAST_DUE']\n\n    # from off\n    prev['PREV_GOODS_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_GOODS_PRICE']\n    prev['PREV_ANNUITY_APPL_RATIO'] = prev['AMT_ANNUITY']\/prev['AMT_APPLICATION']\n    prev['PREV_GOODS_APPL_RATIO'] = prev['AMT_GOODS_PRICE'] \/ prev['AMT_APPLICATION']\n\n    # Previous applications numeric features\n    num_aggregations = {\n        'AMT_ANNUITY': ['min', 'max', 'mean', 'sum'],\n        'AMT_APPLICATION': ['min', 'max', 'mean', 'sum'],\n        'AMT_CREDIT': ['min', 'max', 'mean', 'sum'],\n        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean', 'sum'],\n        'AMT_GOODS_PRICE': ['min', 'max', 'mean', 'sum'],\n        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n        'DAYS_DECISION': ['min', 'max', 'mean'],\n        'CNT_PAYMENT': ['mean', 'sum'],\n        'SK_ID_PREV': ['nunique'],\n        'DAYS_TERMINATION': ['max'],\n        'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n        'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean', 'sum'],\n        'DOWN_PAYMENT_TO_CREDIT': ['mean'],\n        'PREV_GOODS_DIFF': ['mean', 'max', 'sum'],\n        'PREV_GOODS_APPL_RATIO': ['mean', 'max'],\n        'DAYS_LAST_DUE_DIFF': ['mean', 'max', 'sum'],\n        'SIMPLE_INTERESTS': ['mean', 'max']\n    }\n\n    # Previous applications categorical features\n    cat_aggregations = {}\n    for cat in cat_cols:\n        cat_aggregations[cat] = ['mean']\n\n    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n\n    # Previous Applications: Approved Applications - only numerical features\n    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n\n    # Previous Applications: Refused Applications - only numerical features\n    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n\n    print('\"Previous Applications\" final shape:', prev_agg.shape)\n    return prev_agg","e17a416d":"def pos_cash():\n    pos = pd.read_csv(r'..\/input\/home-credit-default-risk\/POS_CASH_balance.csv')\n\n    pos, cat_cols = one_hot_encoder(pos, nan_as_category=True)\n\n    # Flag months with late payment\n    pos['LATE_PAYMENT'] = pos['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n    pos['POS_IS_DPD'] = pos['SK_DPD'].apply(lambda x: 1 if x > 0 else 0) # <-- same with ['LATE_PAYMENT']\n    pos['POS_IS_DPD_UNDER_120'] = pos['SK_DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n    pos['POS_IS_DPD_OVER_120'] = pos['SK_DPD'].apply(lambda x: 1 if x >= 120 else 0)\n\n    # Features\n    aggregations = {\n        'MONTHS_BALANCE': ['max', 'mean', 'size', 'min'],\n        'SK_DPD': ['max', 'mean', 'sum', 'var', 'min'],\n        'SK_DPD_DEF': ['max', 'mean', 'sum'],\n        'SK_ID_PREV': ['nunique'],\n        'LATE_PAYMENT': ['mean'],\n        'SK_ID_CURR': ['count'],\n        'CNT_INSTALMENT': ['min', 'max', 'mean', 'sum'],\n        'CNT_INSTALMENT_FUTURE': ['min', 'max', 'mean', 'sum'],\n        'POS_IS_DPD': ['mean', 'sum'],\n        'POS_IS_DPD_UNDER_120': ['mean', 'sum'],\n        'POS_IS_DPD_OVER_120': ['mean', 'sum'],\n    }\n\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n\n    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n\n    # Count pos cash accounts\n    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n\n\n    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n    gp = sort_pos.groupby('SK_ID_PREV')\n    df_pos = pd.DataFrame()\n    df_pos['SK_ID_CURR'] = gp['SK_ID_CURR'].first()\n    df_pos['MONTHS_BALANCE_MAX'] = gp['MONTHS_BALANCE'].max()\n\n    # Percentage of previous loans completed and completed before initial term\n    df_pos['POS_LOAN_COMPLETED_MEAN'] = gp['NAME_CONTRACT_STATUS_Completed'].mean()\n    df_pos['POS_COMPLETED_BEFORE_MEAN'] = gp['CNT_INSTALMENT'].first() - gp['CNT_INSTALMENT'].last()\n    df_pos['POS_COMPLETED_BEFORE_MEAN'] = df_pos.apply(lambda x: 1 if x['POS_COMPLETED_BEFORE_MEAN'] > 0 \\\n                                                                      and x['POS_LOAN_COMPLETED_MEAN'] > 0 else 0, axis=1)\n    # Number of remaining installments (future installments) and percentage from total\n    df_pos['POS_REMAINING_INSTALMENTS'] = gp['CNT_INSTALMENT_FUTURE'].last()\n    df_pos['POS_REMAINING_INSTALMENTS_RATIO'] = gp['CNT_INSTALMENT_FUTURE'].last()\/gp['CNT_INSTALMENT'].last()\n\n    # Group by SK_ID_CURR and merge\n    df_gp = df_pos.groupby('SK_ID_CURR').sum().reset_index()\n    df_gp.drop(['MONTHS_BALANCE_MAX'], axis=1, inplace= True)\n    pos_agg = pd.merge(pos_agg, df_gp, on= 'SK_ID_CURR', how= 'left')\n\n    # Percentage of late payments for the 3 most recent applications\n    pos = do_sum(pos, ['SK_ID_PREV'], 'LATE_PAYMENT', 'LATE_PAYMENT_SUM')\n\n    # Last month of each application\n    last_month_df = pos.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n\n    # Most recent applications (last 3)\n    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n    gp = sort_pos.iloc[last_month_df].groupby('SK_ID_CURR').tail(3)\n    gp_mean = gp.groupby('SK_ID_CURR').mean().reset_index()\n    pos_agg = pd.merge(pos_agg, gp_mean[['SK_ID_CURR', 'LATE_PAYMENT_SUM']], on='SK_ID_CURR', how='left')\n\n    print('\"Pos-Cash\" balance final shape:', pos_agg.shape) \n    return pos_agg","ed4a37cb":"def installment():\n    ins = pd.read_csv(r'..\/input\/home-credit-default-risk\/installments_payments.csv')\n\n    ins, cat_cols = one_hot_encoder(ins, nan_as_category=True)\n\n    # Group payments and get Payment difference\n    ins = do_sum(ins, ['SK_ID_PREV', 'NUM_INSTALMENT_NUMBER'], 'AMT_PAYMENT', 'AMT_PAYMENT_GROUPED')\n    ins['PAYMENT_DIFFERENCE'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT_GROUPED']\n    ins['PAYMENT_RATIO'] = ins['AMT_INSTALMENT'] \/ ins['AMT_PAYMENT_GROUPED']\n    ins['PAID_OVER_AMOUNT'] = ins['AMT_PAYMENT'] - ins['AMT_INSTALMENT']\n    ins['PAID_OVER'] = (ins['PAID_OVER_AMOUNT'] > 0).astype(int)\n\n    # Percentage and difference paid in each installment (amount paid and installment value)\n    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] \/ ins['AMT_INSTALMENT']\n    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n\n    # Days past due and days before due (no negative values)\n    ins['DPD_diff'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n    ins['DBD_diff'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n    ins['DPD'] = ins['DPD_diff'].apply(lambda x: x if x > 0 else 0)\n    ins['DBD'] = ins['DBD_diff'].apply(lambda x: x if x > 0 else 0)\n\n    # Flag late payment\n    ins['LATE_PAYMENT'] = ins['DBD'].apply(lambda x: 1 if x > 0 else 0)\n    ins['INSTALMENT_PAYMENT_RATIO'] = ins['AMT_PAYMENT'] \/ ins['AMT_INSTALMENT']\n    ins['LATE_PAYMENT_RATIO'] = ins.apply(lambda x: x['INSTALMENT_PAYMENT_RATIO'] if x['LATE_PAYMENT'] == 1 else 0, axis=1)\n\n    # Flag late payments that have a significant amount\n    ins['SIGNIFICANT_LATE_PAYMENT'] = ins['LATE_PAYMENT_RATIO'].apply(lambda x: 1 if x > 0.05 else 0)\n    \n    # Flag k threshold late payments\n    ins['DPD_7'] = ins['DPD'].apply(lambda x: 1 if x >= 7 else 0)\n    ins['DPD_15'] = ins['DPD'].apply(lambda x: 1 if x >= 15 else 0)\n\n    ins['INS_IS_DPD_UNDER_120'] = ins['DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n    ins['INS_IS_DPD_OVER_120'] = ins['DPD'].apply(lambda x: 1 if (x >= 120) else 0)\n\n    # Features: Perform aggregations\n    aggregations = {\n        'NUM_INSTALMENT_VERSION': ['nunique'],\n        'DPD': ['max', 'mean', 'sum', 'var'],\n        'DBD': ['max', 'mean', 'sum', 'var'],\n        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n        'AMT_INSTALMENT': ['max', 'mean', 'sum', 'min'],\n        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum', 'min'],\n        'SK_ID_PREV': ['size', 'nunique'],\n        'PAYMENT_DIFFERENCE': ['mean'],\n        'PAYMENT_RATIO': ['mean', 'max'],\n        'LATE_PAYMENT': ['mean', 'sum'],\n        'SIGNIFICANT_LATE_PAYMENT': ['mean', 'sum'],\n        'LATE_PAYMENT_RATIO': ['mean'],\n        'DPD_7': ['mean'],\n        'DPD_15': ['mean'],\n        'PAID_OVER': ['mean'],\n        'DPD_diff':['mean', 'min', 'max'],\n        'DBD_diff':['mean', 'min', 'max'],\n        'DAYS_INSTALMENT': ['mean', 'max', 'sum'],\n        'INS_IS_DPD_UNDER_120': ['mean', 'sum'],\n        'INS_IS_DPD_OVER_120': ['mean', 'sum']\n    }\n\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n\n    # Count installments accounts\n    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n\n    # from oof (DAYS_ENTRY_PAYMENT)\n    cond_day = ins['DAYS_ENTRY_PAYMENT'] >= -365\n    ins_d365_grp = ins[cond_day].groupby('SK_ID_CURR')\n    ins_d365_agg_dict = {\n        'SK_ID_CURR': ['count'],\n        'NUM_INSTALMENT_VERSION': ['nunique'],\n        'DAYS_ENTRY_PAYMENT': ['mean', 'max', 'sum'],\n        'DAYS_INSTALMENT': ['mean', 'max', 'sum'],\n        'AMT_INSTALMENT': ['mean', 'max', 'sum'],\n        'AMT_PAYMENT': ['mean', 'max', 'sum'],\n        'PAYMENT_DIFF': ['mean', 'min', 'max', 'sum'],\n        'PAYMENT_PERC': ['mean', 'max'],\n        'DPD_diff': ['mean', 'min', 'max'],\n        'DPD': ['mean', 'sum'],\n        'INS_IS_DPD_UNDER_120': ['mean', 'sum'],\n        'INS_IS_DPD_OVER_120': ['mean', 'sum']}\n\n    ins_d365_agg = ins_d365_grp.agg(ins_d365_agg_dict)\n    ins_d365_agg.columns = ['INS_D365' + ('_').join(column).upper() for column in ins_d365_agg.columns.ravel()]\n\n    ins_agg = ins_agg.merge(ins_d365_agg, on='SK_ID_CURR', how='left')\n\n    print('\"Installments Payments\" final shape:', ins_agg.shape)\n    return ins_agg","6bf58f4d":"def credit_card():    \n    cc = pd.read_csv(r'..\/input\/home-credit-default-risk\/credit_card_balance.csv')\n\n    cc, cat_cols = one_hot_encoder(cc, nan_as_category=True)\n\n    # Amount used from limit\n    cc['LIMIT_USE'] = cc['AMT_BALANCE'] \/ cc['AMT_CREDIT_LIMIT_ACTUAL']\n    # Current payment \/ Min payment\n    cc['PAYMENT_DIV_MIN'] = cc['AMT_PAYMENT_CURRENT'] \/ cc['AMT_INST_MIN_REGULARITY']\n    # Late payment <-- 'CARD_IS_DPD'\n    cc['LATE_PAYMENT'] = cc['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n    # How much drawing of limit\n    cc['DRAWING_LIMIT_RATIO'] = cc['AMT_DRAWINGS_ATM_CURRENT'] \/ cc['AMT_CREDIT_LIMIT_ACTUAL']\n\n    cc['CARD_IS_DPD_UNDER_120'] = cc['SK_DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n    cc['CARD_IS_DPD_OVER_120'] = cc['SK_DPD'].apply(lambda x: 1 if x >= 120 else 0)\n\n    # General aggregations\n    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n\n    # Count credit card lines\n    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n\n    # Last month balance of each credit card application\n    last_ids = cc.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n    last_months_df = cc[cc.index.isin(last_ids)]\n    cc_agg = group_and_merge(last_months_df,cc_agg,'CC_LAST_', {'AMT_BALANCE': ['mean', 'max']})\n\n    CREDIT_CARD_TIME_AGG = {\n        'AMT_BALANCE': ['mean', 'max'],\n        'LIMIT_USE': ['max', 'mean'],\n        'AMT_CREDIT_LIMIT_ACTUAL':['max'],\n        'AMT_DRAWINGS_ATM_CURRENT': ['max', 'sum'],\n        'AMT_DRAWINGS_CURRENT': ['max', 'sum'],\n        'AMT_DRAWINGS_POS_CURRENT': ['max', 'sum'],\n        'AMT_INST_MIN_REGULARITY': ['max', 'mean'],\n        'AMT_PAYMENT_TOTAL_CURRENT': ['max','sum'],\n        'AMT_TOTAL_RECEIVABLE': ['max', 'mean'],\n        'CNT_DRAWINGS_ATM_CURRENT': ['max','sum', 'mean'],\n        'CNT_DRAWINGS_CURRENT': ['max', 'mean', 'sum'],\n        'CNT_DRAWINGS_POS_CURRENT': ['mean'],\n        'SK_DPD': ['mean', 'max', 'sum'],\n        'LIMIT_USE': ['min', 'max'],\n        'DRAWING_LIMIT_RATIO': ['min', 'max'],\n        'LATE_PAYMENT': ['mean', 'sum'],\n        'CARD_IS_DPD_UNDER_120': ['mean', 'sum'],\n        'CARD_IS_DPD_OVER_120': ['mean', 'sum']\n    }\n\n    for months in [12, 24, 48]:\n        cc_prev_id = cc[cc['MONTHS_BALANCE'] >= -months]['SK_ID_PREV'].unique()\n        cc_recent = cc[cc['SK_ID_PREV'].isin(cc_prev_id)]\n        prefix = 'INS_{}M_'.format(months)\n        cc_agg = group_and_merge(cc_recent, cc_agg, prefix, CREDIT_CARD_TIME_AGG)\n\n\n    print('\"Credit Card Balance\" final shape:', cc_agg.shape)\n    return cc_agg","eca6f7e6":"def Kfold_LightGBM(df):\n    print('===============================================', '\\n', '##### the ML in processing...')\n\n    # loading predicted result \n    df_subx = pd.read_csv(r'..\/input\/homecredit-best-subs\/df_subs_3.csv')\n    df_sub = df_subx[['SK_ID_CURR', '23']]\n    df_sub.columns = ['SK_ID_CURR', 'TARGET']\n\n\n    # split train, and test datasets\n    train_df = df[df['TARGET'].notnull()]\n    test_df = df[df['TARGET'].isnull()]\n    del df\n\n    # Expand train dataset with two times of test dataset including predicted results\n    test_df.TARGET = np.where(df_sub.TARGET > 0.75, 1, 0)\n    train_df = pd.concat([train_df, test_df], axis=0)\n    train_df = pd.concat([train_df, test_df], axis=0)\n    print(f'Train shape: {train_df.shape}, test shape: {test_df.shape} are loaded.')\n\n\n    # Cross validation model\n    folds = KFold(n_splits=5, shuffle=True, random_state=2020)\n\n    # Create arrays and dataframes to store results\n    oof_preds = np.zeros(train_df.shape[0])\n    sub_preds = np.zeros(test_df.shape[0])\n\n\n    # limit number of feature to only 174!!!\n    feats = ['index', 'POS_SK_DPD_DEF_SUM', 'CC_DRAWING_LIMIT_RATIO_MAX', 'INSTAL_PAYMENT_RATIO_MEAN', 'POS_REMAINING_INSTALMENTS', 'CC_LAST_AMT_BALANCE_MEAN', 'CC_PAYMENT_DIV_MIN_MIN', 'CC_LATE_PAYMENT_VAR', 'NEW_DOC_KURT', 'PREV_SK_ID_PREV_NUNIQUE', 'EMERGENCYSTATE_MODE_nan', 'REFUSED_AMT_GOODS_PRICE_MAX', 'ORGANIZATION_TYPE_Industry_type_5', 'CC_AMT_PAYMENT_TOTAL_CURRENT_SUM', 'CC_CNT_DRAWINGS_POS_CURRENT_SUM', 'APPROVED_AMT_GOODS_PRICE_MAX', 'PREV_NAME_SELLER_INDUSTRY_Connectivity_MEAN', 'APPROVED_AMT_ANNUITY_MAX', 'BURO_CREDIT_ACTIVE_Active_MEAN', 'PREV_NAME_PRODUCT_TYPE_walk_in_MEAN', 'BURO_AMT_CREDIT_SUM_MAX', 'CC_CNT_DRAWINGS_ATM_CURRENT_SUM', 'ACTIVE_DAYS_CREDIT_VAR', 'ACTIVE_MONTHS_BALANCE_MAX_MAX', 'NAME_EDUCATION_TYPE_Higher_education', 'CC_CNT_DRAWINGS_POS_CURRENT_VAR', 'PREV_APP_CREDIT_PERC_MIN', 'REGION_RATING_CLIENT_W_CITY', 'NAME_HOUSING_TYPE_House_apartment', 'CLOSED_AMT_CREDIT_SUM_DEBT_SUM', 'APPROVED_DAYS_DECISION_MEAN', 'ANNUITY_INCOME_PERC', 'ORGANIZATION_TYPE_Services', 'FLAG_DOCUMENT_8', 'WALLSMATERIAL_MODE_Panel', 'ORGANIZATION_TYPE_Cleaning', 'ORGANIZATION_TYPE_Military', 'LIVINGAPARTMENTS_AVG', 'APARTMENTS_AVG', 'ELEVATORS_AVG', 'ORGANIZATION_TYPE_School', 'INSTAL_DPD_MEAN', 'FLOORSMIN_AVG', 'INSTAL_DBD_SUM', 'DAYS_BIRTH', 'INSTAL_DPD_MAX', 'ACTIVE_AMT_CREDIT_SUM_DEBT_MAX', 'OCCUPATION_TYPE_High_skill_tech_staff', 'CC_AMT_DRAWINGS_ATM_CURRENT_VAR', 'ACTIVE_DAYS_CREDIT_ENDDATE_MEAN', 'CC_CNT_DRAWINGS_ATM_CURRENT_MIN', 'PREV_NAME_TYPE_SUITE_Spouse_partner_MEAN', 'CC_AMT_DRAWINGS_OTHER_CURRENT_MEAN', 'POS_SK_DPD_DEF_MEAN', 'ACTIVE_DAYS_CREDIT_ENDDATE_MIN', 'PREV_AMT_DOWN_PAYMENT_MAX', 'BURO_AMT_CREDIT_SUM_SUM', 'PREV_NAME_TYPE_SUITE_Group_of_people_MEAN', 'PREV_APP_CREDIT_PERC_MEAN', 'INSTAL_AMT_PAYMENT_MEAN', 'ACTIVE_DAYS_CREDIT_UPDATE_MEAN', 'INSTAL_DAYS_ENTRY_PAYMENT_MAX', 'PREV_NAME_TYPE_SUITE_Unaccompanied_MEAN', 'CC_CNT_DRAWINGS_POS_CURRENT_MIN', 'ACTIVE_AMT_CREDIT_SUM_MEAN', 'OCCUPATION_TYPE_Private_service_staff', 'CC_CNT_DRAWINGS_OTHER_CURRENT_MEAN', 'BURO_AMT_CREDIT_SUM_MEAN', 'CC_AMT_DRAWINGS_OTHER_CURRENT_MIN', 'CC_AMT_DRAWINGS_OTHER_CURRENT_SUM', 'INSTAL_AMT_PAYMENT_SUM', 'PREV_NAME_PRODUCT_TYPE_nan_MEAN', 'CC_CNT_DRAWINGS_ATM_CURRENT_MEAN', 'OCCUPATION_TYPE_HR_staff', 'BURO_AMT_CREDIT_SUM_OVERDUE_MEAN', 'PREV_CODE_REJECT_REASON_LIMIT_MEAN', 'CC_AMT_DRAWINGS_POS_CURRENT_MIN', 'BURO_AMT_CREDIT_MAX_OVERDUE_MEAN', 'FLOORSMAX_MODE', 'ELEVATORS_MEDI', 'CODE_GENDER', 'INSTAL_DBD_MEAN', 'ORGANIZATION_TYPE_Advertising', 'EXT_SOURCE_3', 'FLAG_DOCUMENT_20', 'OCCUPATION_TYPE_Managers', 'FLAG_OWN_REALTY', 'EMERGENCYSTATE_MODE_Yes', 'POS_COUNT', 'LIVINGAREA_MODE', 'YEARS_BUILD_MEDI', 'AMT_CREDIT', 'INCOME_PER_PERSON', 'EMERGENCYSTATE_MODE_No', 'ORGANIZATION_TYPE_Police', 'FLAG_WORK_PHONE', 'LANDAREA_MEDI', 'COMMONAREA_AVG', 'ORGANIZATION_TYPE_University', 'ORGANIZATION_TYPE_Medicine', 'ORGANIZATION_TYPE_Telecom', 'NONLIVINGAPARTMENTS_AVG', 'WALLSMATERIAL_MODE_Block', 'ORGANIZATION_TYPE_Housing', 'FLAG_CONT_MOBILE', 'FLAG_EMAIL', 'WALLSMATERIAL_MODE_Monolithic', 'REGION_POPULATION_RELATIVE', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLOORSMAX_MEDI', 'ORGANIZATION_TYPE_Electricity', 'REGION_RATING_CLIENT', 'YEARS_BUILD_MODE', 'DAYS_ID_PUBLISH', 'TOTALAREA_MODE', 'WALLSMATERIAL_MODE_Mixed', 'EXT_SOURCE_1', 'FLAG_DOCUMENT_16', 'YEARS_BEGINEXPLUATATION_MODE', 'INSTAL_COUNT', 'ORGANIZATION_TYPE_Realtor', 'FLAG_DOCUMENT_6', 'COMMONAREA_MODE', 'FLAG_DOCUMENT_3', 'FLOORSMAX_AVG', 'OCCUPATION_TYPE_Laborers', 'APARTMENTS_MODE', 'ORGANIZATION_TYPE_Security', 'AMT_INCOME_TOTAL', 'ENTRANCES_AVG', 'PAYMENT_RATE', 'FLAG_DOCUMENT_17', 'FLAG_OWN_CAR', 'FLOORSMIN_MODE', 'FLOORSMIN_MEDI', 'FLAG_DOCUMENT_19', 'ORGANIZATION_TYPE_Mobile', 'INSTAL_DBD_MAX', 'LANDAREA_MODE', 'DAYS_EMPLOYED_PERC', 'INCOME_CREDIT_PERC', 'LIVINGAREA_AVG', 'ORGANIZATION_TYPE_Postal', 'BASEMENTAREA_AVG', 'ORGANIZATION_TYPE_Insurance', 'OCCUPATION_TYPE_Accountants', 'BURO_CREDIT_TYPE_Microloan_MEAN', 'NONLIVINGAREA_MEDI', 'INSTAL_NUM_INSTALMENT_VERSION_NUNIQUE', 'FONDKAPREMONT_MODE_nan', 'INS_24M_AMT_BALANCE_MAX', 'ORGANIZATION_TYPE_Agriculture', 'CC_AMT_PAYMENT_TOTAL_CURRENT_MIN', 'CC_CNT_DRAWINGS_OTHER_CURRENT_SUM', 'PREV_NAME_TYPE_SUITE_Other_B_MEAN', 'CC_AMT_DRAWINGS_ATM_CURRENT_MIN', 'CC_NAME_CONTRACT_STATUS_Sent_proposal_MIN', 'NONLIVINGAREA_AVG', 'FLAG_DOCUMENT_11', 'CC_CNT_DRAWINGS_CURRENT_MIN', 'EXT_SOURCE_2', 'NONLIVINGAREA_MODE', 'AMT_ANNUITY', 'BURO_CREDIT_TYPE_Mortgage_MEAN', 'AMT_GOODS_PRICE', 'APPROVED_CNT_PAYMENT_MEAN', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_18', 'NONLIVINGAPARTMENTS_MEDI', 'ACTIVE_AMT_CREDIT_MAX_OVERDUE_MEAN', 'ORGANIZATION_TYPE_Construction', 'INSTAL_AMT_PAYMENT_MIN', 'BURO_AMT_CREDIT_SUM_DEBT_MEAN']\n\n    # print final shape of dataset to evaluate by LightGBM\n    print(f'only {len(feats)} features from a total {train_df.shape[1]} features are used for ML analysis')\n\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n        clf = LGBMClassifier(nthread=-1,\n                            n_estimators=5000,\n                            learning_rate=0.01,\n                            max_depth=11,\n                            num_leaves=58,\n                            colsample_bytree=0.613,\n                            subsample=0.708,\n                            max_bin=407,\n                            reg_alpha=3.564,\n                            reg_lambda=4.930,\n                            min_child_weight=6,\n                            min_child_samples=165,\n                            silent=-1,\n                            verbose=-1,)\n\n        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric='auc', verbose=500, early_stopping_rounds=500)\n\n        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] \/ folds.n_splits\n\n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n        del clf, train_x, train_y, valid_x, valid_y\n\n    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n\n    # create submission file\n    test_df['TARGET'] = sub_preds\n    test_df[['SK_ID_CURR', 'TARGET']].to_csv('submission.csv', index=False)\n    print('a submission file is created')","36808d5b":"df = application()\ndf = df.merge(bureau_bb(), how='left', on='SK_ID_CURR')\nprint('--=> df after merge with bureau:', df.shape)\ndf = df.merge(previous_application(), how='left', on='SK_ID_CURR')\nprint('--=> df after merge with previous application:', df.shape)\ndf = df.merge(pos_cash(), how='left', on='SK_ID_CURR')\nprint('--=> df after merge with pos cash :', df.shape)\ndf = df.merge(installment(), how='left', on='SK_ID_CURR')\nprint('--=> df after merge with installments:', df.shape)\ndf = df.merge(credit_card(), how='left', on='SK_ID_CURR')\nprint('--=> df after merge with credit card:', df.shape)\ndf = reduce_mem_usage(df)\nprint('data types are converted for a reduced memory usage')\ndf = df.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '_', x))\nprint('names of feature are renamed')\nKfold_LightGBM(df)\nprint('--=> all calculations are done!! <=--')","5688f525":"## a Micro Model Study on Home Credit\n\nThe Home Credit Default Risk dataset on the Kaggle is subjected as a final project of my DS\/ML bootcamp, and I have spent a period of three weeks on this project. I developed various models and quite a large number of them having AUC scores better than 0.8 ( highest one +0.804). Unfortunately, I could not run any full version of my models on Kaggle because of insufficient RAM issue even though datasets are zipped to almost 4 times by integer\/float dtype conversion on my datasets. In addition, I made a bleend boosting study to acheive highest AUC score (0.81128, much highers possible) on Kaggle (https:\/\/www.kaggle.com\/hikmetsezen\/blend-boosting-for-home-credit-default-risk).\n\nHere I would like to share my micro model study with you. This micro model has only 174 features and is able to reach better than 0.8 AUC score. Micro model is developed on my base model via successive feature elimination and addition procedure, which is developed by myself. My ambition is that tremendously increasing number of feature is not always necessary to improve performance of model! \n\nMostly I use Colab Pro to compute LigthGBM calculations with 5-fold CV on GPUs. My models have 900-1800 features. \n\nI have a limited knowledge about the credit finance, therefore, I combined many Kaggle notebooks for expending number of features as much as I desire and\/or acceptance of my LigthGBM models harvesting further enhance scores. I would like to thank these contributors. Some of them are listed here:\n* https:\/\/www.kaggle.com\/jsaguiar\/lightgbm-with-simple-features <=-- my models are based on this study\n* https:\/\/www.kaggle.com\/jsaguiar\/lightgbm-7th-place-solution\n* https:\/\/www.kaggle.com\/sangseoseo\/oof-all-home-credit-default-risk <=-- in most cases these hyperparameters are used\n* https:\/\/www.kaggle.com\/ashishpatel26\/different-basic-blends-possible <=-- thank for blending idea\n* https:\/\/www.kaggle.com\/mathchi\/home-credit-risk-with-detailed-feature-engineering\n* https:\/\/www.kaggle.com\/windofdl\/kernelf68f763785\n* https:\/\/www.kaggle.com\/meraxes10\/lgbm-credit-default-prediction\n* https:\/\/www.kaggle.com\/luudactam\/hc-v500\n* https:\/\/www.kaggle.com\/aantonova\/aggregating-all-tables-in-one-dataset\n* https:\/\/www.kaggle.com\/wanakon\/kernel24647bb75c"}}