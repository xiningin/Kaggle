{"cell_type":{"5bb77107":"code","5b1fe9ce":"code","a1c97278":"code","5f7876a1":"code","0a6b6424":"code","980b0b7b":"code","5c506642":"code","03b6eaf3":"code","931b06fc":"code","89f85d3d":"code","6cc3c067":"code","ab68e8c6":"code","458cb244":"code","68a4f1af":"code","ac740e9a":"code","7ef665ab":"code","681d44da":"code","b92afea1":"code","d868fb02":"code","9857dcd1":"code","1c98b17f":"code","8c857f46":"code","aee01bec":"code","43ebbc04":"code","5e829e84":"code","8cb116de":"code","948573c7":"code","b61290c8":"code","014b855c":"code","8cc62eda":"code","efa260b3":"code","f21e3795":"code","a482f459":"code","e8433932":"code","fddd396e":"code","435e9196":"code","1ef719ce":"code","e4c51334":"code","8dd84ec0":"code","f5d1085e":"code","16767afc":"code","168f543c":"code","1052077d":"code","56d7e2f0":"code","ab91d887":"code","50d40443":"code","1cc92801":"code","feae1d1d":"code","f451e5ee":"code","537b2cfd":"code","9e26ef4e":"code","a5e75987":"code","9d856c6e":"code","06e31e42":"code","d2023734":"code","b9bbe834":"code","5765cdf5":"code","ec035be6":"code","4beb5686":"code","68ec9db1":"code","8c1823d4":"markdown","a7b5a204":"markdown","da1e6b4a":"markdown","40dc9440":"markdown","eb0f697b":"markdown","2830bdb2":"markdown","842ae073":"markdown","7343682b":"markdown","714eabc5":"markdown","03a06204":"markdown","8ca1225f":"markdown","68dbb174":"markdown","0c1da4a7":"markdown","c51dd494":"markdown","fdc06d35":"markdown","448aa9a6":"markdown","6a34d484":"markdown","626fd0a8":"markdown","0e793b90":"markdown","2e01ec1a":"markdown","df7de076":"markdown","aeba5202":"markdown","3196323c":"markdown","bb20ea56":"markdown","5b177a42":"markdown","b551bec1":"markdown","a1084741":"markdown","f0b0f55c":"markdown"},"source":{"5bb77107":"# some useful sources\n# fast.ai github\n# https:\/\/github.com\/fastai\/fastai1\/blob\/master\/old\/fastai\/structured.py\n# how to read from kaggle\n# https:\/\/www.youtube.com\/watch?v=tGw-ZACouik","5b1fe9ce":"# Import Libraries\n\nimport pandas as pd\nimport numpy as np\nimport re\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nimport math\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier","a1c97278":"# !pip install kaggle\n# from google.colab import files\n# files.upload()","5f7876a1":"# !mkdir -p ~\/.kaggle\n# !cp kaggle.json ~\/.kaggle \n\n# # change the permission\n# !chmod 600 ~\/.kaggle\/kaggle.json\n# !kaggle competitions download -c titanic","0a6b6424":"# Read Raw Data\n#train_raw = pd.read_csv('train.csv')\ntrain_raw = pd.read_csv('..\/input\/titanic\/train.csv')\n#test_raw = pd.read_csv('test.csv')\ntest_raw = pd.read_csv('..\/input\/titanic\/test.csv')\n#gender_submission = pd.read_csv('gender_submission.csv')","980b0b7b":"train = train_raw.copy(deep= True)\ntest = test_raw.copy(deep= True)","5c506642":"# features\ntrain.columns","03b6eaf3":"# number of null, Data type\ntrain.info()","931b06fc":"train.describe()","89f85d3d":"train.describe(include=['O'])\n# describe category","6cc3c067":"train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","ab68e8c6":"train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","458cb244":"# get title from Name\n\ndef get_title(name):\n    # title_search: a word which ends up with dot\n    title_search = re.search('([A-Za-z]+)\\.', name)\n    # If the title_search exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"","68a4f1af":"train['Title'] = train['Name'].apply(get_title)\ntest['Title'] = test['Name'].apply(get_title)","ac740e9a":"pd.crosstab(train['Title'], train['Sex'])","7ef665ab":"# We can replace many titles with a more common name or classify them as Rare.\n\n\ntrain['Title'] = train['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntrain['Title'] = train['Title'].replace('Mlle', 'Miss')\ntrain['Title'] = train['Title'].replace('Ms', 'Miss')\ntrain['Title'] = train['Title'].replace('Mme', 'Mrs')","681d44da":"test['Title'] = test['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntest['Title'] = test['Title'].replace('Mlle', 'Miss')\ntest['Title'] = test['Title'].replace('Ms', 'Miss')\ntest['Title'] = test['Title'].replace('Mme', 'Mrs')","b92afea1":"train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","d868fb02":"plt.figure(figsize=(12,5))\n\n#Plotting the result\nsns.countplot(x='Title', data=train, palette=\"hls\")\nplt.xlabel(\"Title\", fontsize=16) #seting the xtitle and size\nplt.ylabel(\"Count\", fontsize=16) # Seting the ytitle and size\nplt.title(\"Title Name Count\", fontsize=20) \nplt.xticks(rotation=45)\nplt.show()","9857dcd1":"# FamilySize using SibSp and Parch\n# Note : Most family suvived who are consisting of Four people\n\ntrain['FamilySize'] = train['SibSp'] + train['Parch'] + 1\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1","1c98b17f":"train[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","8c857f46":"train[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","aee01bec":"train[[\"FamilySize\", \"Survived\"]].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","43ebbc04":"sns.factorplot(x=\"FamilySize\",y=\"Survived\", data=train, kind=\"bar\",size=6, aspect=1.6)\nplt.show()","5e829e84":"print(pd.crosstab(train.FamilySize, train.Survived))","8cb116de":"\ntrain.loc[ train['Fare'] <= 7.91, 'Fare'] = int(0)\ntrain.loc[(train['Fare'] > 7.91) & (train['Fare'] <= 14.454), 'Fare'] = int(1)\ntrain.loc[(train['Fare'] > 14.454) & (train['Fare'] <= 31), 'Fare']   = int(2)\ntrain.loc[ train['Fare'] > 31, 'Fare'] = int(3)","948573c7":"test.loc[ test['Fare'] <= 7.91, 'Fare'] = int(0)\ntest.loc[(test['Fare'] > 7.91) & (test['Fare'] <= 14.454), 'Fare'] = int(1)\ntest.loc[(test['Fare'] > 14.454) & (test['Fare'] <= 31), 'Fare']   = int(2)\ntest.loc[ test['Fare'] > 31, 'Fare'] = int(3)","b61290c8":"g = sns.FacetGrid(train, col='Survived')\ng.map(plt.hist, 'Age', bins=25)","014b855c":"\ntrain.loc[ train['Age'] <= 16, 'Age']  = int(0)\ntrain.loc[(train['Age'] > 16) & (train['Age'] <= 32), 'Age'] = int(1)\ntrain.loc[(train['Age'] > 32) & (train['Age'] <= 48), 'Age'] = int(2)\ntrain.loc[(train['Age'] > 48) & (train['Age'] <= 64), 'Age'] = int(3)\ntrain.loc[ train['Age'] > 64, 'Age'] = int(4)","8cc62eda":"test.loc[ test['Age'] <= 16, 'Age']  = int(0)\ntest.loc[(test['Age'] > 16) & (test['Age'] <= 32), 'Age'] = int(1)\ntest.loc[(test['Age'] > 32) & (test['Age'] <= 48), 'Age'] = int(2)\ntest.loc[(test['Age'] > 48) & (test['Age'] <= 64), 'Age'] = int(3)\ntest.loc[ test['Age'] > 64, 'Age'] = int(4)","efa260b3":"train['Cabin_'] = train['Cabin'].apply(lambda x: 0 if type(x) == float else x[0])\ntest['Cabin_'] = test['Cabin'].apply(lambda x: 0 if type(x) == float else x[0])","f21e3795":"col_drop = [\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\", \"SibSp\"]\ntrain = train.drop(col_drop , axis = 1)\ntest = test.drop(col_drop , axis = 1)","a482f459":"def train_cats(df):\n    \"\"\"Change any columns of strings in a panda's dataframe to a column of\n    categorical values. This applies the changes inplace.\n    Parameters:\n    -----------\n    df: A pandas dataframe. Any columns of strings will be changed to\n        categorical values.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    note the type of col2 is string\n    >>> train_cats(df)\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    now the type of col2 is category\n    \"\"\"\n    for n,c in df.items():\n        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()\n\ndef apply_cats(df, trn):\n    \"\"\"Changes any columns of strings in df into categorical variables using trn as\n    a template for the category codes.\n    Parameters:\n    -----------\n    df: A pandas dataframe. Any columns of strings will be changed to\n        categorical values. The category codes are determined by trn.\n    trn: A pandas dataframe. When creating a category for df, it looks up the\n        what the category's code were in trn and makes those the category codes\n        for df.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    note the type of col2 is string\n    >>> train_cats(df)\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    now the type of col2 is category {a : 1, b : 2}\n    >>> df2 = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['b', 'a', 'a']})\n    >>> apply_cats(df2, df)\n           col1 col2\n        0     1    b\n        1     2    a\n        2     3    a\n    now the type of col is category {a : 1, b : 2}\n    \"\"\"\n    for n,c in df.items():\n        if (n in trn.columns) and (trn[n].dtype.name=='category'):\n            df[n] = c.astype('category').cat.as_ordered()\n            df[n].cat.set_categories(trn[n].cat.categories, ordered=True, inplace=True)\n","e8433932":"# Change any columns of strings in a panda's dataframe to a column of\n# categorical values. This applies the changes inplace\n\ntrain_cats(train)\n\n\n# Changes any columns of strings in df into categorical variables using trn as\n# a template for the category codes.\n\napply_cats(test, train)","fddd396e":"# applying proc test need to have same col as train\ntest['Survived'] = np.repeat(0, test.shape[0])","435e9196":"def proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,\n            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n    \"\"\" proc_df takes a data frame df and splits off the response variable, and\n    changes the df into an entirely numeric dataframe. For each column of df \n    which is not in skip_flds nor in ignore_flds, na values are replaced by the\n    median value of the column.\n    Parameters:\n    -----------\n    df: The data frame you wish to process.\n    y_fld: The name of the response variable\n    skip_flds: A list of fields that dropped from df.\n    ignore_flds: A list of fields that are ignored during processing.\n    do_scale: Standardizes each column in df. Takes Boolean Values(True,False)\n    na_dict: a dictionary of na columns to add. Na columns are also added if there\n        are any missing values.\n    preproc_fn: A function that gets applied to df.\n    max_n_cat: The maximum number of categories to break into dummy values, instead\n        of integer codes.\n    subset: Takes a random subset of size subset from df.\n    mapper: If do_scale is set as True, the mapper variable\n        calculates the values used for scaling of variables during training time (mean and standard deviation).\n    Returns:\n    --------\n    [x, y, nas, mapper(optional)]:\n        x: x is the transformed version of df. x will not have the response variable\n            and is entirely numeric.\n        y: y is the response variable\n        nas: returns a dictionary of which nas it created, and the associated median.\n        mapper: A DataFrameMapper which stores the mean and standard deviation of the corresponding continuous\n        variables which is then used for scaling of during test-time.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    note the type of col2 is string\n    >>> train_cats(df)\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    now the type of col2 is category { a : 1, b : 2}\n    >>> x, y, nas = proc_df(df, 'col1')\n    >>> x\n       col2\n    0     1\n    1     2\n    2     1\n    >>> data = DataFrame(pet=[\"cat\", \"dog\", \"dog\", \"fish\", \"cat\", \"dog\", \"cat\", \"fish\"],\n                 children=[4., 6, 3, 3, 2, 3, 5, 4],\n                 salary=[90, 24, 44, 27, 32, 59, 36, 27])\n    >>> mapper = DataFrameMapper([(:pet, LabelBinarizer()),\n                          ([:children], StandardScaler())])\n    >>>round(fit_transform!(mapper, copy(data)), 2)\n    8x4 Array{Float64,2}:\n    1.0  0.0  0.0   0.21\n    0.0  1.0  0.0   1.88\n    0.0  1.0  0.0  -0.63\n    0.0  0.0  1.0  -0.63\n    1.0  0.0  0.0  -1.46\n    0.0  1.0  0.0  -0.63\n    1.0  0.0  0.0   1.04\n    0.0  0.0  1.0   0.21\n    \"\"\"\n    if not ignore_flds: ignore_flds=[]\n    if not skip_flds: skip_flds=[]\n    if subset: df = get_sample(df,subset)\n    else: df = df.copy()\n    ignored_flds = df.loc[:, ignore_flds]\n    df.drop(ignore_flds, axis=1, inplace=True)\n    if preproc_fn: preproc_fn(df)\n    if y_fld is None: y = None\n    else:\n        if not is_numeric_dtype(df[y_fld]): df[y_fld] = pd.Categorical(df[y_fld]).codes\n        y = df[y_fld].values\n        skip_flds += [y_fld]\n    df.drop(skip_flds, axis=1, inplace=True)\n\n    if na_dict is None: na_dict = {}\n    else: na_dict = na_dict.copy()\n    na_dict_initial = na_dict.copy()\n    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n    if len(na_dict_initial.keys()) > 0:\n        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)\n    if do_scale: mapper = scale_vars(df, mapper)\n    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n    df = pd.get_dummies(df, dummy_na=True)\n    df = pd.concat([ignored_flds, df], axis=1)\n    res = [df, y, na_dict]\n    if do_scale: res = res + [mapper]\n    return res\n\n  \ndef fix_missing(df, col, name, na_dict):\n    \"\"\" Fill missing data in a column of df with the median, and add a {name}_na column\n    which specifies if the data was missing.\n    Parameters:\n    -----------\n    df: The data frame that will be changed.\n    col: The column of data to fix by filling in missing data.\n    name: The name of the new filled column in df.\n    na_dict: A dictionary of values to create na's of and the value to insert. If\n        name is not a key of na_dict the median will fill any missing data. Also\n        if name is not a key of na_dict and there is no missing data in col, then\n        no {name}_na column is not created.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n    >>> fix_missing(df, df['col1'], 'col1', {})\n    >>> df\n       col1 col2 col1_na\n    0     1    5   False\n    1     2    2    True\n    2     3    2   False\n    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n    >>> fix_missing(df, df['col2'], 'col2', {})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n    >>> fix_missing(df, df['col1'], 'col1', {'col1' : 500})\n    >>> df\n       col1 col2 col1_na\n    0     1    5   False\n    1   500    2    True\n    2     3    2   False\n    \"\"\"\n    if is_numeric_dtype(col):\n        if pd.isnull(col).sum() or (name in na_dict):\n            df[name+'_na'] = pd.isnull(col)\n            filler = na_dict[name] if name in na_dict else col.median()\n            df[name] = col.fillna(filler)\n            na_dict[name] = filler\n    return na_dict\n\n\n\ndef numericalize(df, col, name, max_n_cat):\n    \"\"\" Changes the column col from a categorical type to it's integer codes.\n    Parameters:\n    -----------\n    df: A pandas dataframe. df[name] will be filled with the integer codes from\n        col.\n    col: The column you wish to change into the categories.\n    name: The column name you wish to insert into df. This column will hold the\n        integer codes.\n    max_n_cat: If col has more categories than max_n_cat it will not change the\n        it to its integer codes. If max_n_cat is None, then col will always be\n        converted.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    note the type of col2 is string\n    >>> train_cats(df)\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    now the type of col2 is category { a : 1, b : 2}\n    >>> numericalize(df, df['col2'], 'col3', None)\n       col1 col2 col3\n    0     1    a    1\n    1     2    b    2\n    2     3    a    1\n    \"\"\"\n    if not is_numeric_dtype(col) and ( max_n_cat is None or len(col.cat.categories)>max_n_cat):\n        df[name] = pd.Categorical(col).codes+1\n","1ef719ce":"df_train, y, nas = proc_df(train, 'Survived')\ndf_test, _, _ = proc_df(test, 'Survived', na_dict=nas)","e4c51334":"def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n\nn_valid = 100  # same as Kaggle's test set size\nn_trn = len(df_train) - n_valid\nX_train, X_valid = split_vals(df_train, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n\nX_train.shape, y_train.shape, X_valid.shape","8dd84ec0":"def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef accuracy(x,y): return (x == y).mean()\n\ndef print_score(m, X_train, y_train, X_valid, y_valid):\n    res = [accuracy(m.predict(X_train), y_train), accuracy(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","f5d1085e":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nprint_score(logreg, X_train, y_train, X_valid, y_valid)","16767afc":"coeff_df = pd.DataFrame(train.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","168f543c":"predict_LR = logreg.predict(df_test)\nrslt_LR = pd.DataFrame(data = {'PassengerId': test_raw['PassengerId'], 'Survived': predict_LR})\nrslt_LR.to_csv('titanic_LR.csv', index = False)","1052077d":"svc = SVC()\nsvc.fit(X_train, y_train)\nprint_score(svc, X_train, y_train, X_valid, y_valid)\n","56d7e2f0":"predict_SVM = svc.predict(df_test)\nrslt_SVM = pd.DataFrame(data = {'PassengerId': test_raw['PassengerId'], 'Survived': predict_SVM})\nrslt_SVM.to_csv('titanic_SVM.csv', index = False)","ab91d887":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nprint_score(knn, X_train, y_train, X_valid, y_valid)\n","50d40443":"predict_knn = knn.predict(df_test)\nrslt_knn = pd.DataFrame(data = {'PassengerId': test_raw['PassengerId'], 'Survived': predict_knn})\nrslt_knn.to_csv('titanic_knn.csv', index = False)","1cc92801":"gaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\nprint_score(gaussian, X_train, y_train, X_valid, y_valid)\n","feae1d1d":"predict_gaussian = gaussian.predict(df_test)\nrslt_gaussian = pd.DataFrame(data = {'PassengerId': test_raw['PassengerId'], 'Survived': predict_gaussian})\nrslt_gaussian.to_csv('titanic_gaussian.csv', index = False)","f451e5ee":"perceptron = Perceptron()\nperceptron.fit(X_train, y_train)\nprint_score(perceptron, X_train, y_train, X_valid, y_valid)\n","537b2cfd":"predict_perceptron = perceptron.predict(df_test)\nrslt_perceptron = pd.DataFrame(data = {'PassengerId': test_raw['PassengerId'], 'Survived': predict_perceptron})\nrslt_perceptron.to_csv('titanic_perceptron.csv', index = False)","9e26ef4e":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\nprint_score(linear_svc, X_train, y_train, X_valid, y_valid)\n","a5e75987":"predict_linear_svc = linear_svc.predict(df_test)\nrslt_linear_svc = pd.DataFrame(data = {'PassengerId': test_raw['PassengerId'], 'Survived': predict_linear_svc})\nrslt_linear_svc.to_csv('titanic_linear_svc.csv', index = False)","9d856c6e":"sgd = SGDClassifier()\nsgd.fit(X_train, y_train)\nprint_score(sgd, X_train, y_train, X_valid, y_valid)\n","06e31e42":"predict_sgd = sgd.predict(df_test)\nrslt_sgd = pd.DataFrame(data = {'PassengerId': test_raw['PassengerId'], 'Survived': predict_sgd})\nrslt_sgd.to_csv('titanic_sgd.csv', index = False)","d2023734":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\nprint_score(decision_tree, X_train, y_train, X_valid, y_valid)\n","b9bbe834":"predict_decision_tree = decision_tree.predict(df_test)\nrslt_decision_tree = pd.DataFrame(data = {'PassengerId': test_raw['PassengerId'], 'Survived': predict_decision_tree})\nrslt_decision_tree.to_csv('titanic_decision_tree.csv', index = False)","5765cdf5":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\nprint_score(random_forest, X_train, y_train, X_valid, y_valid)\n","ec035be6":"predict_random_forest = random_forest.predict(df_test)\nrslt_random_forest = pd.DataFrame(data = {'PassengerId': test_raw['PassengerId'], 'Survived': predict_random_forest})\nrslt_random_forest.to_csv('titanic_random_forest.csv', index = False)","4beb5686":"GradientBoosting = GradientBoostingClassifier(n_estimators=100)\nGradientBoosting.fit(X_train, y_train)\nprint_score(GradientBoosting, X_train, y_train, X_valid, y_valid)","68ec9db1":"predict_GradientBoosting = GradientBoosting.predict(df_test)\nrslt_GradientBoosting = pd.DataFrame(data = {'PassengerId': test_raw['PassengerId'], 'Survived': predict_GradientBoosting})\nrslt_GradientBoosting.to_csv('titanic_GradientBoosting.csv', index = False)","8c1823d4":"# SECTION 4: Data Cleaning\n\n- Dealing with String Value \n\n- Categorical data to numeric and dealing with missing data\n\n- Split train data to train and validation\n\n","a7b5a204":"# 3.1 Pcalss : Ticket class ( 1 = 1st, 2 = 2nd, 3 = 3rd )\nWe observe significant correlation (>0.5) among Pclass=1 and Survived.\n","da1e6b4a":"# 5.7 Stochastic Gradient Descent","40dc9440":"# 5.5 Perceptron","eb0f697b":"# **1.2 Reading Data with Pandas Package:**\n\nThe Python Pandas packages helps us work with our datasets. \nWe start by acquiring the training and testing datasets into Pandas DataFrames.\n","2830bdb2":"# 3.7 Cabin : Cabin number\n\nCabin is an example of alphanumeric.","842ae073":"# SECTION 5: Machine Learning Models\n\nNow we are ready to train a model. \n\n- 5.1 Logistic Regression\n- 5.2 Support Vector Machines\n- 5.3 KNN or k-Nearest Neighbors\n- 5.4 Naive Bayes classifier\n- 5.5 Perceptron\n- 5.6 Linear SVC\n- 5.7 Stochastic Gradient Descent \n- 5.8 Decision Tree\n- 5.9 Random Forrest\n- 5.10 Gradient boosting ","7343682b":"# 3.6 Age : Age in years\t\n\nObservations.\n\n- Infants (Age <=4) had high survival rate.\n- Oldest passengers (Age = 80) survived.\n- Large number of 15-25 year olds did not survive.\n- Most passengers are in 15-35 age range.","714eabc5":"# 3.2 Sex \nwe observe Sex=female had very high survival rate at 74% .","03a06204":"# SECTION 3: Feature Engineering","8ca1225f":"# 5.2 Support Vector Machines\n","68dbb174":"# 4.3 Split data","0c1da4a7":"# 5.9 Random Forest\n","c51dd494":"# 5.6 Linear SVC","fdc06d35":"# 5.1 Logistic Regression\n\n","448aa9a6":"# 3.3 Name\n- Certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer)\n- we can extract titles from Name and use it in our model.\n","6a34d484":"# **Introduction**\n\n  The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development\n\n\n**this notebook hast 5 Sections.**\n\n1. Acquire data \n  - Kaggle to Google colab\n  - Reading Data with Pandas Package\n2. Exploring Data\n3. Feature Engineering\n4. DATA Cleaning\n5. Ml model \n  - applying 10 diffrent Ml model to our data\n\n\n\n\n\n","626fd0a8":"# 5.4 Gaussian Naive Bayes","0e793b90":"# 5.10 Gradient Boosting\n","2e01ec1a":"# 5.8 Decision Tree","df7de076":"# **SECTION 1: Acquire data**\n\n\n\n\n\n\n","aeba5202":"# 4.2 Categorical data to numeric and dealing with missing data\n\n ","3196323c":"# 3.4 Family Size\nSibSp : # of siblings \/ spouses aboard the Titanic\n\nparch : # of parents \/ children aboard the TitanicParch \n\nFamilySize : SibSp + parch + 1","bb20ea56":"# 4.1 Dealing with String Value\n\n","5b177a42":"# **SECTION 2: Exploring data**\n\n# Which features are available in the dataset?\n\nThese feature names are described [here](https:\/\/www.kaggle.com\/c\/titanic\/data).\n\n# Which features are categorical?\n\nThese values classify the samples into sets of similar samples. \n\n**Categorical:** Survived, Sex, and Embarked. Ordinal: Pclass.\n# Which features are numerical?\n\nThese values change from sample to sample.\n\n**Continous:** Age, Fare. Discrete: SibSp, Parch.\n\n# Which features are mixed data types?\n\nNumerical, alphanumeric data within same feature. \n**Ticket** is a mix of numeric and alphanumeric data types. **Cabin** is alphanumeric.\n\n# Which features may contain errors or typos?\n\n\n**Name** feature may contain errors or typos as there are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names.\n\n# Which features contain blank, null or empty values?\n\nThese will require correcting.\n\n**Cabin , Age , Embarked** features contain a number of null values.\n\n# What are the data types for various features?\n\nSeven features are integer or floats.\nFive features are strings (object).","b551bec1":"# **1.1 Kaggle to Google colab:**\n\n**Note:** if you are not using Google colab, you can skip part 1.1\n\ncheck out the [video.]( https:\/\/www.youtube.com\/watch?v=tGw-ZACouik)","a1084741":"# 3.5 Fare : Passenger fare\t\nNote. Higher fare paying passengers had better survival.\n\nConvert the Fare feature to ordinal values based on the FareBand.","f0b0f55c":"# 5.3 k-Nearest Neighbors\n"}}