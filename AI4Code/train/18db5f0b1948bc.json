{"cell_type":{"a93edab8":"code","a8b8b307":"code","d6a86321":"code","fbf7e3e7":"code","a5494a44":"code","68f435af":"code","f52baeb1":"code","ce346f91":"code","39fc12ee":"code","b6c8ee07":"code","61eefa93":"code","27c0fc36":"code","bdb353dc":"code","12052803":"code","6ebbd872":"code","ad050bbc":"code","aba1a08c":"code","c4ce80fb":"code","1ce7dfad":"code","842db2fb":"code","37470fc8":"code","60b38f7a":"code","29f14f84":"code","b4293d64":"code","6029e4b4":"code","f0e31ef7":"code","755751d6":"code","861164c4":"code","7d853ae5":"code","8a62f154":"code","050e482d":"code","e3c72739":"code","68027a18":"code","f69c9044":"code","65424d4a":"code","4f542e5c":"code","abe88f11":"code","44c80870":"code","561a5b27":"code","4414ab21":"code","db773ca7":"code","340c6bd9":"code","b82065c5":"code","adb180d3":"code","b2aa1687":"code","2ad0004b":"code","403a013e":"code","365460f1":"code","3c58f324":"code","7f41036f":"code","f4ed989f":"code","5f1ba4c6":"code","7938160d":"code","1bf8cb54":"code","94d92fca":"code","0afab67d":"code","751b197a":"code","86c31bf3":"code","91b1ef9f":"code","72840aa3":"code","9f329ba2":"markdown","5b8e905b":"markdown","7a9bfba6":"markdown","e4c54620":"markdown","bc3c4b4b":"markdown","d0f9badf":"markdown","4ac7cd5d":"markdown","eefcdcfd":"markdown","41827517":"markdown"},"source":{"a93edab8":"!pip install gensim --upgrade\n!pip install keras --upgrade\n!pip install pandas --upgrade","a8b8b307":"# DataFrame\nimport pandas as pd\n\n# Matplot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\nfrom keras import utils\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n# nltk\nimport nltk\nfrom nltk.corpus import stopwords\nfrom  nltk.stem import SnowballStemmer\n\n# Word2vec\nimport gensim\n\n# Utility\nimport re\nimport numpy as np\nimport os\nfrom collections import Counter\nimport logging\nimport time\nimport pickle\nimport itertools\n\n# Set log\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","d6a86321":"nltk.download('stopwords')","fbf7e3e7":"# DATASET\n\"\"\"\nDATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nDATASET_ENCODING = \"ISO-8859-1\"\nTRAIN_SIZE = 0.8\n\"\"\"\n\n# TEXT CLENAING\nTEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n\n# WORD2VEC \nW2V_SIZE = 300\nW2V_WINDOW = 7\nW2V_EPOCH = 32\nW2V_MIN_COUNT = 10\n\n# KERAS\nSEQUENCE_LENGTH = 300\nEPOCHS = 8\nBATCH_SIZE = 1024\n\n# SENTIMENT\nPOSITIVE = \"POSITIVE\"\nNEGATIVE = \"NEGATIVE\"\nNEUTRAL = \"NEUTRAL\"\nSENTIMENT_THRESHOLDS = (0.4, 0.7)\n\n# EXPORT\nKERAS_MODEL = \"model.h5\"\nWORD2VEC_MODEL = \"model.w2v\"\nTOKENIZER_MODEL = \"tokenizer.pkl\"\nENCODER_MODEL = \"encoder.pkl\"","a5494a44":"raw_data = pd.read_csv(\"..\/input\/hmif-challenge-2019\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/hmif-challenge-2019\/test.csv\")\n\nraw_data.head()                    ","68f435af":"print(len(raw_data)) \nprint(len(test_data))","f52baeb1":"raw_data.dtypes","ce346f91":"for col in raw_data.columns:\n    \n    if (str(col) != \"captions\") and (raw_data[col].dtypes != raw_data[\"post_id\"].dtypes) :\n        print(\"------------------\")\n        print(\"\\n \" + str(col) + \" : \\n\")\n        print( raw_data[col].value_counts())\n        print(\"\\n\\n------------------------\")\n\n","39fc12ee":"\ncaption_string_vec = []\n\n# Subtitute emoji\n\nimport re\n\n\ndef deEmojify(text):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002500-\\U00002BEF\"  # chinese char\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        u\"\\U0001f926-\\U0001f937\"\n        u\"\\U00010000-\\U0010ffff\"\n        u\"\\u2640-\\u2642\" \n        u\"\\u2600-\\u2B55\"\n        u\"\\u200d\"\n        u\"\\u23cf\"\n        u\"\\u23e9\"\n        u\"\\u231a\"\n        u\"\\ufe0f\"  # dingbats\n        u\"\\u3030\"\n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)\n\n\nfor caption in raw_data[\"captions\"]:\n    \n    caption_string_vec.append(str(deEmojify(caption)))\n    \n#caption_string_vec\n\n# Detect language\n\n!pip install langdetect\n\nfrom langdetect import detect \n\n\nlang_vec = []\n\ni= 0\n\nfor caption in caption_string_vec:\n    \n    try :\n        ans = detect(str(caption))\n        lang_vec.append(ans)\n    except :\n        lang_vec.append(\"none\")\n\n# change lang\n\nnew_lang_vec = []\n\nfor lang in lang_vec:\n    if lang in [\"id\", \"en\"]:\n        new_lang_vec.append(lang)\n    else:\n        new_lang_vec.append(\"id\")\n\n\n    \n\ndata_with_string = raw_data\n\ndata_with_string[\"lang\"] = new_lang_vec\n\ndata_with_string = raw_data\n\ndata_with_string[\"string_caption\"] = caption_string_vec\n\ndata_with_string.head()","b6c8ee07":"# change type\n\ndata_coded = data_with_string\n\ndata_coded[\"type\"] = data_coded[\"type\"].astype(\"string\") \ndata_coded[\"is_business_account\"] = data_coded[\"is_business_account\"].astype(\"string\") \ndata_coded[\"business_category_name\"] = data_coded[\"business_category_name\"].astype(\"string\") \ndata_coded[\"gender\"] = data_coded[\"gender\"].astype(\"string\") \ndata_coded[\"user_exposure\"] = data_coded[\"user_exposure\"].astype(\"string\") \ndata_coded[\"lang\"] = data_coded[\"lang\"].astype(\"string\") \n\nprint(data_coded.dtypes)","61eefa93":"# coding\n\nx = data_coded[\"business_category_name\"][0]\n\ndata_coded[\"comments_disabled\"] = data_coded[\"comments_disabled\"].replace({False : 1, True : 0})\ndata_coded[\"is_video\"] = data_coded[\"is_video\"].replace({False : 0, True : 1})\ndata_coded[\"type\"] = data_coded[\"type\"].replace({\"GraphImage\" : 0, \"GraphSidecar\" : 1, \"GraphVideo\" : 2})\ndata_coded[\"has_external_url\"] = data_coded[\"has_external_url\"].replace({False : 0, True : 1})\ndata_coded[\"is_business_account\"] = data_coded[\"is_business_account\"].replace({\"False\" : 0, \"True\" : 1})\ndata_coded[\"business_category_name\"] = data_coded[\"business_category_name\"].replace({\"Creators & Celebrities\" : \"1\", \"Local Events\" : \"0\" , x : \"-1\"})\ndata_coded[\"gender\"] = data_coded[\"gender\"].replace({\"Female\" : \"0\", \"Male\" : \"1\"})\ndata_coded[\"user_exposure\"] = data_coded[\"user_exposure\"].replace({\"National\" : \"0\", \"International\" : \"1\"})\ndata_coded[\"lang\"] = data_coded[\"lang\"].replace({\"en\" : \"1\", \"id\" : \"0\"})\n\ndata_coded.head()","27c0fc36":"# First make a function to delete repetitive alphabet\nimport itertools\n\ndef remove_repeating_characters(text):\n    return ''.join(''.join(s)[:1] for _, s in itertools.groupby(text))\n\nimport re\n\ndef remove_nonalphanumeric(text):\n    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n    return text\n\n\ndef to_lower_case(text):\n    return text.lower()\n\n\ndef preprocessing_text(text):\n    text = remove_repeating_characters(text)\n    text = remove_nonalphanumeric(text)\n    text = to_lower_case(text)\n    \n    return text\n\n# Check our function\npreprocessing_text('Bagus\\n\\n\\nNamun Akan Lebih Baik Apabila Lebih')","bdb353dc":"# Apply function to column 'string_function'\n\nclean_caption_data = data_coded\nclean_caption_data['captions'] = clean_caption_data['captions'].apply(lambda x: preprocessing_text(x))\n\nclean_caption_data.head()","12052803":"# Translate bahasa inggris ke indonesia\n\n#!pip install translate\n\nfrom translate import Translator\n\ntranslator= Translator(to_lang=\"Indonesian\")\n\n\ndef translate_to_id(row):\n    \n    text = row[\"captions\"]\n    \n    if row[\"lang\"] == 0:\n        return text\n    else:\n        return translator.translate(text)\n\ntranslated_data = clean_caption_data\n\ntranslated_data[\"captions\"] = translated_data.apply(lambda row: translate_to_id(row), axis = 1)","6ebbd872":"translated_data[\"type\"] = data_coded[\"type\"]\ntranslated_data.head()","ad050bbc":"def get_len_caption(row):\n    \n    return( len(row[\"captions\"]))\n\ntranslated_data[\"len_caption\"] = translated_data.apply(lambda row :  get_len_caption(row), axis = 1)\n\ntranslated_data.head()","aba1a08c":"translated_data[\"is_business_account\"] = data_coded[\"is_business_account\"]\n\ntranslated_data.head()","c4ce80fb":"# Make a vector to contain all unique word in 'review sangat singkat'\ntranslated_data['captions']\nunique_string = set()\nfor x in translated_data['captions']:\n    for y in x.split():\n        unique_string.add(y)\n        \nlen(unique_string)","1ce7dfad":"#Count statistics of number of word in review\n\nlen_data = [len(x.split()) for x in translated_data['captions']]\nprint(np.mean(len_data))\nprint(np.median(len_data))\nprint(np.std(len_data))\nprint(np.min(len_data))\nprint(np.max(len_data))\nprint(np.percentile(len_data, 98))","842db2fb":"embed_size = 100 # how big is each word vector\nmax_features = 23000 # how many unique words to use\nmaxlen = 20 # max number of words in a comment to use","37470fc8":"# Example\nfrom keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words = 4)\ntokenizer.fit_on_texts([\"ini sebuah kalimat hehehe\"])\nexamples = tokenizer.texts_to_sequences([\"ini contoh kalimat juga\"])\nprint(examples[0])","60b38f7a":"# Real one\n\npreprocessing_data = translated_data\n\ntokenizer = Tokenizer(num_words = max_features)\ntokenizer.fit_on_texts(preprocessing_data['captions'])\nlist_tokenized_train = tokenizer.texts_to_sequences(preprocessing_data['captions'].values)","29f14f84":"list_tokenized_train[0]","b4293d64":"# Example\n\nfrom keras.preprocessing.sequence import pad_sequences\npad_sequences(examples, maxlen = maxlen)","6029e4b4":"# Real one\n\nX_t = pad_sequences(list_tokenized_train, maxlen= maxlen )","f0e31ef7":"X_t[0]","755751d6":"len(X_t) == len(translated_data)","861164c4":"clean_data = translated_data\n\n\nclean_data[\"type\"] = clean_data[\"type\"].astype(\"int64\") \nclean_data[\"is_business_account\"] = clean_data[\"is_business_account\"].astype(\"int64\") \nclean_data[\"business_category_name\"] = clean_data[\"business_category_name\"].astype(\"int64\") \nclean_data[\"gender\"] = clean_data[\"gender\"].astype(\"int64\") \nclean_data[\"user_exposure\"] = clean_data[\"user_exposure\"].astype(\"int64\") \nclean_data[\"lang\"] = clean_data[\"lang\"].astype(\"int64\") \n\nclean_data = clean_data.drop([\"captions\"], axis = 1)\nclean_data = clean_data.drop([\"string_caption\"], axis = 1)\nclean_data = clean_data.drop([\"engagement\"], axis = 1)\n\n\ndf_list = clean_data.values.tolist()\n\ndf_list[0]","7d853ae5":"len(df_list[0])","8a62f154":"\n\n# Create input list\n\nX_new = []\n\nfor i in range(len(X_t)):\n    \n    y = df_list[i]  + X_t[i].tolist()\n    \n    X_new = X_new+[y]\n    \n    i = i +1\n    \nX_new_array = np.array(X_new)\n\nX_new_array","050e482d":"len(X_new_array[0])","e3c72739":"import gensim\n\nDIR_DATA_MISC = \"..\/input\/word2vec-100-indonesian\"\npath = '{}\/idwiki_word2vec_100.model'.format(DIR_DATA_MISC)\nid_w2v = gensim.models.word2vec.Word2Vec.load(path)\nprint(id_w2v.most_similar('itb'))","68027a18":"maxlen","f69c9044":"word_index = tokenizer.word_index\nnb_words = max_features\nembedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\nunknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\nfor word, i in word_index.items():\n    cur = word\n    if cur in index2word_set:\n        embedding_matrix[i] = id_w2v[cur]\n        continue\n        \n    embedding_matrix[i] = unknown_vector","65424d4a":"# Import needed packages\n# And make needed function\n\n\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, SpatialDropout1D, GlobalMaxPooling1D, Concatenate\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras import callbacks\n\nfrom keras import backend as K\n\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n    \n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\ndef get_model():\n    \n    meta_input = Input(shape= (18,))\n    inp = Input(shape=(maxlen,))\n    \n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = SpatialDropout1D(0.3)(x)\n    x1 = Bidirectional(LSTM(32, return_sequences=True))(x)\n    x2 = Bidirectional(GRU(32, return_sequences=True))(x1)\n    max_pool1 = GlobalMaxPooling1D()(x1)\n    max_pool2 = GlobalMaxPooling1D()(x2)\n    conc = Concatenate()([max_pool1, max_pool2])\n    conc1 = Concatenate()([conc, meta_input])\n    #print(\"\\nSampai sini\\n\")\n    #x = Dense(1, activation=\"sigmoid\")(conc)\n    x1 = Dense(256, activation = \"relu\")(conc1)\n    #x15 = Dropout(0.5)(x1)\n    x2 = Dense(256, activation = \"relu\")(x1)\n    #x25 = Dropout(0.5)(x2)\n    x3 = Dense(256, activation = \"relu\")(x2)\n    x = Dense(1, activation=\"linear\")(x3)\n    #print(\"\\nSampai sini woyy\\n\")\n    \n    model = Model(inputs= [meta_input, inp], outputs=x)\n    #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n    model.compile(loss='mse', optimizer='adam', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    return model","4f542e5c":"from sklearn.model_selection import KFold\ndef get_kfold():\n    return KFold(n_splits=5, shuffle=True, random_state=1)","abe88f11":"import tensorflow as tf\n\nX = X_t\n#X_add = np.array(df_list)\nX_add = np.array(df_list_standardize)\n#X = X_new_array\n#y = translated_data[\"engagement\"].values\n\ny = y_result\n\npred_cv = np.zeros(len(y))\ncount = 0\n\nfor train_index, test_index in get_kfold().split(X,X_add, y):\n    count += 1\n    print(count, end='')\n    X_train, X_test = X[train_index], X[test_index]\n    X_add_train, X_add_test = X_add[train_index], X_add[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    es = callbacks.EarlyStopping(monitor='val_rmse', min_delta=0.0001, patience=8,\n                                             verbose=1, mode='max', baseline=None, restore_best_weights=True)\n\n    rlr = callbacks.ReduceLROnPlateau(monitor='val_rmse', factor=0.5,\n                                      patience=3, min_lr=1e-6, mode='max', verbose=1)\n    \n    \n    model = get_model()\n    model.fit([X_add_train,X_train], \n             y_train, batch_size=16, epochs=4,\n             validation_data=([X_add_test, X_test], y_test),\n             callbacks=[es, rlr],\n             verbose=1)\n    \n    pred_cv[[test_index]] += model.predict([X_add_test, X_test])[:,0]","44c80870":"standardize_data.columns","561a5b27":"# Standardize\n\nstandardize_data = clean_data\n\nstandardize_data.head()\n\n#from sklearn import preprocessing\n\n# Create the Scaler object\n#scaler = preprocessing.StandardScaler()\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\"\"\"\nct = ColumnTransformer([\n        ('follow_scaler', StandardScaler(), ['followers', 'following'])\n    ], remainder='passthrough')\n\nstandardize_data = ct.fit_transform(standardize_data)\n\"\"\"\n\nfollow_scaler = StandardScaler()\n\ndf1 = standardize_data\n\nfollow_scaler.fit(df1)\ndf1 = follow_scaler.transform(df1)\n\nstandardize_result = pd.DataFrame(df1,  columns=standardize_data.columns)\n\nstandardize_result.head()","4414ab21":"df_list_standardize = standardize_result.values.tolist()","db773ca7":"y = raw_data[\"engagement\"]\n\ny_angle = np.arctan(y)\n\ny_sin = np.sin(y_angle* np.pi \/ 180)\n\ny_result = np.log(y)\/np.log(2)\n\ny_result","340c6bd9":"np.sin(0.78539816339744828)","b82065c5":"caption_string_test_vec = []\n\n# Subtitute emoji\n\nimport re\n\n\ndef deEmojify(text):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002500-\\U00002BEF\"  # chinese char\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        u\"\\U0001f926-\\U0001f937\"\n        u\"\\U00010000-\\U0010ffff\"\n        u\"\\u2640-\\u2642\" \n        u\"\\u2600-\\u2B55\"\n        u\"\\u200d\"\n        u\"\\u23cf\"\n        u\"\\u23e9\"\n        u\"\\u231a\"\n        u\"\\ufe0f\"  # dingbats\n        u\"\\u3030\"\n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)\n\n\nfor caption in test_data[\"captions\"]:\n    \n    caption_string_test_vec.append(str(deEmojify(caption)))\n    \n#caption_string_vec\n\n# Detect language\n\n!pip install langdetect\n\nfrom langdetect import detect \n\n\nlang_test_vec = []\n\ni= 0\n\nfor caption in caption_string_test_vec:\n    \n    try :\n        ans = detect(str(caption))\n        lang_test_vec.append(ans)\n    except :\n        lang_test_vec.append(\"none\")\n\n# change lang\n\nnew_lang_vec = []\n\nfor lang in lang_test_vec:\n    if lang in [\"id\", \"en\"]:\n        new_lang_vec.append(lang)\n    else:\n        new_lang_vec.append(\"id\")\n\n\n    \n\ndata_test_with_string = test_data\n\ndata_test_with_string[\"lang\"] = new_lang_vec\n\n#data_with_string = raw_data\n\ndata_test_with_string[\"string_caption\"] = caption_string_test_vec\n\ndata_test_with_string.head()","adb180d3":"#data_test_coded.head()","b2aa1687":"# change type\n\ndata_test_coded = data_test_with_string\n\ndata_test_coded[\"type\"] = data_test_coded[\"type\"].astype(\"string\") \ndata_test_coded[\"is_business_account\"] = data_test_coded[\"is_business_account\"].astype(\"string\") \ndata_test_coded[\"business_category_name\"] = data_test_coded[\"business_category_name\"].astype(\"string\") \ndata_test_coded[\"gender\"] = data_test_coded[\"gender\"].astype(\"string\") \ndata_test_coded[\"user_exposure\"] = data_test_coded[\"user_exposure\"].astype(\"string\") \ndata_test_coded[\"lang\"] = data_test_coded[\"lang\"].astype(\"string\") \n\nprint(data_test_coded.dtypes)\n\n# coding\n\nx = data_test_coded[\"business_category_name\"][2]\n\ndata_test_coded[\"comments_disabled\"] = data_test_coded[\"comments_disabled\"].replace({False : 1, True : 0})\ndata_test_coded[\"is_video\"] = data_test_coded[\"is_video\"].replace({False : 0, True : 1})\ndata_test_coded[\"type\"] = data_test_coded[\"type\"].replace({\"GraphImage\" : 0, \"GraphSidecar\" : 1, \"GraphVideo\" : 2})\ndata_test_coded[\"has_external_url\"] = data_test_coded[\"has_external_url\"].replace({False : 0, True : 1})\ndata_test_coded[\"is_business_account\"] = data_test_coded[\"is_business_account\"].replace({\"False\" : 0, \"True\" : 1})\ndata_test_coded[\"business_category_name\"] = data_test_coded[\"business_category_name\"].replace({\"Creators & Celebrities\" : \"1\", \"Local Events\" : \"0\" , x : \"-1\"})\ndata_test_coded[\"gender\"] = data_test_coded[\"gender\"].replace({\"Female\" : \"0\", \"Male\" : \"1\"})\ndata_test_coded[\"user_exposure\"] = data_test_coded[\"user_exposure\"].replace({\"National\" : \"0\", \"International\" : \"1\"})\ndata_test_coded[\"lang\"] = data_test_coded[\"lang\"].replace({\"en\" : \"1\", \"id\" : \"0\"})\n\ndata_test_coded.head()","2ad0004b":"# Apply function to column 'string_function'\n\nclean_caption_data_test = data_test_coded\nclean_caption_data_test['captions'] = clean_caption_data_test['captions'].apply(lambda x: preprocessing_text(x))\n\nclean_caption_data_test.head()","403a013e":"#clean_caption_data_test.head()","365460f1":"from translate import Translator\n\ntranslator= Translator(to_lang=\"Indonesian\")\n\n\ndef translate_to_id(row):\n    \n    text = row[\"captions\"]\n    \n    if row[\"lang\"] == 0:\n        return text\n    else:\n        #return translator.translate(text)\n        return text\n\ntranslated_data_test = clean_caption_data_test\n\ntranslated_data_test[\"captions\"] = translated_data_test.apply(lambda row: translate_to_id(row), axis = 1)\n\ndef get_len_caption(row):\n    \n    return( len(row[\"captions\"]))\n\ntranslated_data_test[\"len_caption\"] = translated_data_test.apply(lambda row :  get_len_caption(row), axis = 1)\n\ntranslated_data_test.head()","3c58f324":"translated_data_test[\"captions\"][0]","7f41036f":"#tokenizer.fit_on_texts(preprocessing_data['captions'])\n\npreprocessing_data_test = translated_data_test\n\nlist_tokenized_train = tokenizer.texts_to_sequences(preprocessing_data_test['captions'].values)\n\nX_t_test = pad_sequences(list_tokenized_train, maxlen= maxlen )\n\n","f4ed989f":"clean_data_test = translated_data_test\n\nclean_data_test[\"type\"] = clean_data_test[\"type\"].astype(\"int64\") \nclean_data_test[\"is_business_account\"] = clean_data_test[\"is_business_account\"].astype(\"int64\") \nclean_data_test[\"business_category_name\"] = clean_data_test[\"business_category_name\"].astype(\"int64\") \n\nclean_data_test[\"business_category_name\"] = data_test_coded[\"business_category_name\"]\n\nclean_data_test[\"gender\"] = clean_data_test[\"gender\"].astype(\"int64\") \nclean_data_test[\"user_exposure\"] = clean_data_test[\"user_exposure\"].astype(\"int64\") \nclean_data_test[\"lang\"] = clean_data_test[\"lang\"].astype(\"int64\") \n\nclean_data_test = clean_data_test.drop([\"captions\"], axis = 1)\nclean_data_test = clean_data_test.drop([\"string_caption\"], axis = 1)\n\n\n\n\ndf_list_test = clean_data_test.values.tolist()\n\n\nX_test_add = np.array(df_list_test)","5f1ba4c6":"standardize_test_data = clean_data_test\n\ndf1 = follow_scaler.transform(standardize_test_data)\n\nstandardize_result_test = pd.DataFrame(df1,  columns=clean_data_test.columns)\n\ndf_list_test_standardize = standardize_result_test.values.tolist()\n\ndf_list_test_standardize\n\nX_test_add_standardize = np.array(df_list_test_standardize)","7938160d":"len(X_test_add[0])","1bf8cb54":"model_test = model\n\n\"\"\"\nmodel_test.fit([X_add,X_t], \n             y, batch_size=16, epochs=4,\n             validation_data=([X_add, X_t], y),\n             callbacks=[es, rlr],\n             verbose=1)\n\"\"\"","94d92fca":"#y_pred = model_test.predict([X_test_add, X_t_test])\n#X_test_add_standardize\ny_pred = model_test.predict([X_test_add_standardize, X_t_test])","0afab67d":"y_pred","751b197a":"y_final = []\n\n\ny_final = np.power(2, y_pred)\n\ny_final","86c31bf3":"sub = pd.read_csv(\"..\/input\/hmif-challenge-2019\/sample-submission.csv\")\n\nsub.head()","91b1ef9f":"sub[\"engagement\"] = y_pred\n\nsub.head()","72840aa3":"submission = sub.to_csv(\"ans_1.csv\", index = False)","9f329ba2":"## Feature Engineering","5b8e905b":"## Extra Preprocessing","7a9bfba6":"## Feature Engineering for NLP","e4c54620":"## Setting","bc3c4b4b":"## Model","d0f9badf":"## Import Dataset","4ac7cd5d":"## Preprocessing","eefcdcfd":"## Create Submission","41827517":"## Prepare Test Data"}}