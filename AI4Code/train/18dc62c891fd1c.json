{"cell_type":{"d6fd7ad3":"code","f725be26":"code","dfdc15f6":"code","8c9e05ea":"code","a15c4b4c":"code","958d968e":"code","85e5b0aa":"code","e2f7d271":"code","ec011913":"code","68c14f21":"code","3002a226":"code","c2f4bf5a":"code","bc0aab95":"code","d8f3e3d3":"code","f384c296":"code","98b07d95":"code","533a017b":"code","278eb848":"code","de7dff60":"code","a7bf59d1":"code","df85f3a9":"code","82e29a32":"code","629d2cd2":"code","b5c8e8cd":"code","8c428188":"code","0d0752ce":"code","17275829":"code","48693769":"code","45ae256e":"code","cdeb41c1":"code","70eeb9b7":"code","6a454b5f":"code","6bb7efaf":"code","c6fa71c9":"code","8e72480d":"code","205fc2e5":"code","363b6238":"code","8fadbf31":"code","aa126404":"code","7f7f3cb5":"code","b05b1436":"code","51acda0f":"code","6febb5d9":"code","d07295d8":"code","2ed04945":"code","8ed4663c":"code","605b50c6":"code","02ecdd08":"code","51e76163":"code","98cbdcf9":"code","018bf5e3":"code","ffd85a17":"markdown","3918fc90":"markdown","1aa5c800":"markdown","8a9e0ea2":"markdown","ec87fa46":"markdown","5c2a07ff":"markdown","1ce23e68":"markdown","2501b68a":"markdown","d8923a1d":"markdown"},"source":{"d6fd7ad3":"from warnings import filterwarnings\nfilterwarnings('ignore')\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom xgboost import plot_importance, plot_tree\nfrom datetime import date\nimport holidays\nfrom sklearn.metrics import r2_score\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport time\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize']=(20,10) # for graphs styling\nplt.style.use('tableau-colorblind10') # for graph stying\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom xgboost.sklearn import XGBRegressor  # wrapper\nimport scipy.stats as st\nimport operator\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score, KFold\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score, KFold\nimport pandas as pd\nimport numpy as np\nimport pickle","f725be26":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\\\n        \n        print(os.path.join(dirname, filename))","dfdc15f6":"data=pd.read_excel('\/kaggle\/input\/timeseriesconvxgboost\/dataset_convxgb.xlsx')\ndata['Date'] = pd.to_datetime(data['Date']).dt.date\ndata = pd.DataFrame(data.groupby(data['Date'])['Units'].sum())\ntest_df = data.groupby(['Date'])['Units'].sum().reset_index()\ndata = test_df\ndata.head()","8c9e05ea":"german_holidays = holidays.Germany(years = [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020])\ntraining_df = data.groupby(['Date'])['Units'].sum().reset_index()\ntraining_df['Date'] = pd.to_datetime(training_df['Date']).dt.date\ntraining_df = training_df[training_df['Date'].isin(german_holidays) == False]\ntraining_df['Date'] = pd.to_datetime(training_df['Date'])\nmain_df = training_df\nmain_df","a15c4b4c":"# Setting features for the XGBoost\ntraining_df['Year'] = training_df['Date'].dt.year\ntraining_df['Week'] = pd.to_datetime(training_df['Date']).dt.week\ntraining_df['Day'] = pd.to_datetime(training_df['Date']).dt.day\ntraining_df['WeekDay'] = pd.to_datetime(training_df['Date']).dt.dayofweek\ntraining_df['Weekend'] = training_df.WeekDay.isin([5, 6]).astype(int)\ntraining_df['quarter'] = pd.to_datetime(training_df['Date']).dt.quarter\ntraining_df['month'] = pd.to_datetime(training_df['Date']).dt.month\ntraining_df['dayofyear'] = pd.to_datetime(training_df['Date']).dt.dayofyear\ntraining_df['dayofmonth'] = pd.to_datetime(training_df['Date']).dt.day\ntraining_df['weekofyear'] = pd.to_datetime(training_df['Date']).dt.weekofyear\ntraining_df","958d968e":"training_df = training_df.reindex(columns=['Date', 'Year', 'Week', 'Day', 'WeekDay', 'Weekend',\n       'quarter', 'month', 'dayofyear', 'dayofmonth', 'weekofyear', 'Units',])\ntraining_df.isnull().sum()","85e5b0aa":"training_df.columns","e2f7d271":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n\n\n# Select subset of predictors\ncols_to_use = ['Year', 'Week', 'Day', 'WeekDay', 'Weekend', 'quarter', 'month',\n       'dayofyear', 'dayofmonth', 'weekofyear']\nX = training_df[cols_to_use]\n\n# Select target\ny = training_df.Units\n\n# Separate data into training and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y)  \ntraining_df = X\ntraining_df","ec011913":"X_train","68c14f21":"X_valid","3002a226":"y_train","c2f4bf5a":"y_valid","bc0aab95":"# Select subset of predictors\ncols_to_use = ['Year', 'Week', 'Day', 'WeekDay', 'Weekend', 'quarter', 'month',\n       'dayofyear', 'dayofmonth', 'weekofyear']\ndf_unseen = training_df[cols_to_use]\ndf_test = training_df[cols_to_use]\n","d8f3e3d3":"\nprint('\\n-----Xgboost on only datetime information---------\\n')\n\ndim = {'train and validation data ': training_df.shape,\n       'test data ': df_test.shape,\n       'forecasting data ': df_unseen.shape}\nprint(pd.DataFrame(list(dim.items()), columns=['Data', 'dimension']))\n\nX_test = xgb.DMatrix(df_test)\ny_test = df_test\nX_unseen = xgb.DMatrix(df_unseen)\ndtrain = xgb.DMatrix(X_train, y_train)\ndval = xgb.DMatrix(X_valid, y_valid)\nwatchlist = [(dtrain, 'train'), (dval, 'validate')]","f384c296":"X_test","98b07d95":"y_test","533a017b":"X_unseen","278eb848":"\n\nparams_sk = {\n    'objective': 'reg:squarederror',\n    'subsample': 0.8,\n    'colsample_bytree': 0.85,\n    'seed': 42}\n\nxgb_reg = XGBRegressor(**params_sk)\n\nxgb_reg.fit(X_train, y_train)\n\nparams_grid = {\"n_estimators\": st.randint(100, 500),\n                               \"colsample_bytree\": st.beta(10, 1),     \n                               \"subsample\": st.beta(10, 1),\n                               \"gamma\": st.uniform(0, 10),\n                               'reg_alpha': st.expon(0, 50),\n                               \"min_child_weight\": st.expon(0, 50),\n                              \"learning_rate\": st.uniform(0.06, 0.12),\n               'max_depth': st.randint(6, 30)\n               }\ngrid = RandomizedSearchCV(\n    xgb_reg, params_grid, cv=5, random_state=1, n_iter=20)  # 5 fold cross validation\ngrid.fit(X_train, y_train)","de7dff60":"gridcv_xgb = grid.best_estimator_\nprint(r2_score(y_valid, gridcv_xgb.predict(X_valid)))\nprint(\"best parameters:\", grid.best_params_); \nprint(\"best score:\", grid.best_score_)","a7bf59d1":"_ = plot_importance(gridcv_xgb)","df85f3a9":"params_new = {**params_sk, **grid.best_params_}\n\nmodel_final = xgb.train(params_new, dval, evals=watchlist, verbose_eval=True)\n","82e29a32":"_ = plot_importance(model_final, height=1.5)","629d2cd2":"Y_hat = model_final.predict(X_test)\nY_hat = pd.DataFrame(Y_hat, index=y_test.index, columns=[\"predicted\"])\nY_hat","b5c8e8cd":"\nunseen_y = model_final.predict(X_unseen)\nforecast = pd.DataFrame(\n    unseen_y, index=df_unseen.index, columns=[\"forecast\"])\nforecast","8c428188":"\nplot_start = '2010-11-24 00:00:00'\nprint('-----Xgboost Using Datetime Features Only------',\n      '\\n---Forecasting from Grid Search---')\n","0d0752ce":"\nxgb_model = xgb.train(params_sk, dtrain,  evals=watchlist)\nY_hat = xgb_model.predict(X_test)\nY_hat = pd.DataFrame(Y_hat, index=y_test.index, columns=[\"test_predicted\"])\nunseen_y = xgb_model.predict(X_unseen)\nforecasts = pd.DataFrame(\n    unseen_y, index=df_unseen.index, columns=[\"forecasts\"])\n","17275829":"forecasts","48693769":"import xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score, KFold\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport os\nimport warnings\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\nfrom collections import UserDict\nfrom glob import glob\nfrom IPython.display import Image\n%matplotlib inline\nfrom keras.models import Model, Sequential\nfrom keras.layers import Conv1D, Dense, Flatten\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\npd.options.display.float_format = '{:,.2f}'.format\nnp.set_printoptions(precision=2)\nwarnings.filterwarnings(\"ignore\")\nfrom keras.layers.convolutional import MaxPooling1D","45ae256e":"german_holidays = holidays.Germany(years = [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020])\ntraining_df = data.groupby(['Date'])['Units'].sum().reset_index()\ntraining_df['Date'] = pd.to_datetime(training_df['Date']).dt.date\ntraining_df = training_df[training_df['Date'].isin(german_holidays) == False]\ntraining_df['Date'] = pd.to_datetime(training_df['Date'])\nmain_df = training_df\nmain_df","cdeb41c1":"# Setting features for the XGBoost\ntraining_df['Year'] = training_df['Date'].dt.year\ntraining_df['Week'] = pd.to_datetime(training_df['Date']).dt.week\ntraining_df['Day'] = pd.to_datetime(training_df['Date']).dt.day\ntraining_df['WeekDay'] = pd.to_datetime(training_df['Date']).dt.dayofweek\ntraining_df['Weekend'] = training_df.WeekDay.isin([5, 6]).astype(int)\ntraining_df['quarter'] = pd.to_datetime(training_df['Date']).dt.quarter\ntraining_df['month'] = pd.to_datetime(training_df['Date']).dt.month\ntraining_df['dayofyear'] = pd.to_datetime(training_df['Date']).dt.dayofyear\ntraining_df['dayofmonth'] = pd.to_datetime(training_df['Date']).dt.day\ntraining_df['weekofyear'] = pd.to_datetime(training_df['Date']).dt.weekofyear\ntraining_df","70eeb9b7":"traini = training_df.reindex(columns=['Year', 'Week', 'Day', 'WeekDay', 'Weekend',\n       'quarter', 'month', 'dayofyear', 'dayofmonth', 'weekofyear'])\ntraini.isnull().sum()","6a454b5f":"traini","6bb7efaf":"temp = pd.DataFrame(traini)","c6fa71c9":"temp","8e72480d":"temp['Year'] = temp['Year'] * forecasts['forecasts']\ntemp['Week'] = temp['Week'] * forecasts['forecasts']\ntemp['Day'] = temp['Day'] * forecasts['forecasts']\ntemp['WeekDay'] = temp['WeekDay'] * forecasts['forecasts']\ntemp['Weekend'] = temp['Weekend'] * forecasts['forecasts']\ntemp['quarter'] = temp['quarter'] * forecasts['forecasts']\ntemp['month'] = temp['month'] * forecasts['forecasts']\ntemp['dayofyear'] = temp['dayofyear'] * forecasts['forecasts']\ntemp['dayofmonth'] = temp['dayofmonth'] * forecasts['forecasts']\ntemp['weekofyear'] = temp['weekofyear'] * forecasts['forecasts']","205fc2e5":"temp","363b6238":"temp.columns","8fadbf31":"temp = temp.join(training_df['Units'])","aa126404":"temp","7f7f3cb5":"data = temp\ndata","b05b1436":"data.columns","51acda0f":"T = data.T","6febb5d9":"x_cols = list(data.columns[:-1])\nX_data = data[x_cols].copy()\n# Adding 0 for easy reshaping\nX_data['F12'] = 0\nX_data.head()","d07295d8":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n\n# Select subset of predictors\ncols_to_use = ['Year', 'Week', 'Day', 'WeekDay', 'Weekend', 'quarter', 'month',\n       'dayofyear', 'dayofmonth', 'weekofyear']\nX = data[cols_to_use]\n\n# Select target\ny = data.Units\n\n# Separate data into training and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y)","2ed04945":"LATENT_DIM = 5\nKERNEL_SIZE = 2\nBATCH_SIZE = 32\nEPOCHS = 10","8ed4663c":"model = Sequential()\nmodel.add(Conv1D(filters=10, kernel_size=2, activation='relu', input_shape=(2400, 10)))\nmodel.add(Conv1D(filters=6, kernel_size=2, activation='relu', input_shape=(1201, 5)))\nmodel.add(Conv1D(filters=3, kernel_size=2, activation='relu', input_shape=(600, 3)))\n#model.add(MaxPooling1D(pool_size=1))\nmodel.add(Dense(2, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(loss='mse', optimizer='adam')\n","605b50c6":"model.summary()","02ecdd08":"model.compile(optimizer='Adam', loss='mse')","51e76163":"earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5)","98cbdcf9":"best_val = ModelCheckpoint('model_{epoch:02d}.h5', save_best_only=True, mode='min', period=1)","018bf5e3":"history = model.fit(X_train,\n          y_train,\n          batch_size=BATCH_SIZE,\n          epochs=EPOCHS,\n          validation_data=(X_valid, y_valid),\n          callbacks=[earlystop, best_val],\n          verbose=0)","ffd85a17":"# best parameters\n","3918fc90":"# forcasts results using itinial model","1aa5c800":"# Initialize XGB and GridSearch","8a9e0ea2":"CrossValidation","ec87fa46":"# predictions to unseen future data","5c2a07ff":"# with new parameters\n","1ce23e68":"# plot forcast results using grid search final model","2501b68a":"# A parameter grid for XGBoost\n# Grid Search","d8923a1d":"# Forcasting\n# prediction to testing data"}}