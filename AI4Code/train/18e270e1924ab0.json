{"cell_type":{"4d0e3609":"code","fe1e0a79":"code","8ae30ba1":"code","a6e8d4dc":"code","3097809a":"code","fdc76788":"code","8838488e":"code","8443a3b6":"markdown","c45f9fea":"markdown","a8a1db62":"markdown","fcc03976":"markdown","c5a2e282":"markdown","0664d1dc":"markdown","bed8205f":"markdown"},"source":{"4d0e3609":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fe1e0a79":"# Load library tools and text\nfrom transformers import pipeline, set_seed, BertTokenizer\nfrom numpy.random import default_rng\n\npath = '..\/input\/texttpu\/cowper.txt'\n\nwith open(path) as txt:    \n    text = txt.readlines() ","8ae30ba1":"LINES = 3\nrng = default_rng()\n\n# Select a random sample from text\nRAND_START = rng.integers(len(text)-LINES)\nSTOP = RAND_START + LINES\n\nprompt = ''.join(text[RAND_START:STOP])\nprint(prompt)","a6e8d4dc":"tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\ntokens = tokenizer.tokenize(prompt)\ntoken_length = len(tokens)\nprint(f'Token Length: {token_length}')","3097809a":"MULTIPLIER = 5\nMAX_LENGTH = MULTIPLIER * token_length\nprint(f'Max Length: {MAX_LENGTH}')\nassert MAX_LENGTH > token_length, f'MAX_LENGTH must be greater than {token_length} tokens'","fdc76788":"SEED = 123\nNUM_SEQ = 3\nseq_list = [] \n\ngenerator = pipeline('text-generation', model='gpt2-medium')\nset_seed(SEED)\n\nfor sequence in range(NUM_SEQ):\n    seq_list.append(generator(prompt, max_length=MAX_LENGTH, num_return_sequences=NUM_SEQ)[sequence]['generated_text'])","8838488e":"for sequence in seq_list:\n    print(sequence)\n    print('-' * 50, end='\\n')","8443a3b6":"A sequence of lines will be randomly selected from the Iliad text. The lines will then be joined into a single string. This will be used as a prompt for the generator.","c45f9fea":"Lastly, a generator pipeline is created. This GPT-2 generator returns a list of dictionaries so indexing is required to get only the text. Here three sequences are returned for comparison. We'll append them to a list and print.","a8a1db62":"In this exercise a language model called the Generative Pre-Trained Transformer 2 ([GPT-2](https:\/\/en.wikipedia.org\/wiki\/GPT-2)) will be used to create synthetic text using a prompt from an epic poem. GPT-2 was born from the laboratory at [OpenAI](https:\/\/en.wikipedia.org\/wiki\/OpenAI). It was scaled up from the original GPT with training on ten times more data, and it excels at predicting the next word from a body of text. To illustrate this adaptive ability we will select a few lines from the [Iliad](https:\/\/www.gutenberg.org\/files\/16452\/16452-h\/16452-h.htm) by Homer. We will then observe how well GPT-2 performs when it takes a prompt from an ancient poem. ","fcc03976":"In conclusion, the GPT-2 generator returned 3 sequences, each with the prompt completed with synthetic text. We can see how this language model performs with an ancient poem that contrasts with modern writing. With prompts from different genres, GPT-2 could be used to create text datasets for classification exercises. Another task might be to identify synthetic text versus text from the original human prompt source. A system to filter out text from artificial intelligence may be needed in the future, but it remains to be seen how technologies like GPT-2 will be truly used by society. ","c5a2e282":"# Epic Text Generation with GPT-2","0664d1dc":"The prompt is then tokenized to find its token length. ","bed8205f":"Since the token length of the prompt must be greater than the generator's `max_length` parameter, we will multiply the token length of the prompt by an arbitrary positive integer. As this integer value increases, so does the amount of synthetic text generated by GPT-2. "}}