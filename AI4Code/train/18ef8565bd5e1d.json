{"cell_type":{"9ead7fd4":"code","1a8db356":"code","a320531d":"code","7a267ea8":"code","a153c266":"code","035a8252":"code","830136fb":"code","707f9874":"code","457bbd81":"code","60bd1ef2":"code","05eee0fb":"code","d720bb4d":"code","39013130":"code","85d5b24d":"code","42a65142":"code","313cb6d8":"code","97104dc8":"code","af510e58":"code","a3c5a509":"code","5fe04677":"code","ce6d8680":"code","0d47942f":"code","a4622440":"code","fa288862":"code","6c91b03d":"code","f83593dd":"code","c6a2b8b2":"code","c515bc9e":"code","048cb448":"code","55d5af90":"code","5bef9b3d":"code","6f744cba":"code","293e2bcd":"code","7d5d1330":"code","2298e617":"code","4545983f":"code","1825ff00":"code","f4a590fb":"code","0e8b2565":"code","00bbe907":"code","b503c2ca":"code","75d284df":"code","cd604d0b":"code","15eec60d":"code","94bfdf1c":"code","4113d097":"code","561b0a12":"code","33ed789b":"code","c8deeabf":"code","f904e0ed":"code","817a4734":"code","11bcc609":"code","593d526a":"code","e1de75a9":"code","0fa498d5":"code","d2132ab9":"code","e1612f4b":"code","931ea841":"code","f13a2eb6":"code","769d4b0a":"code","4b2be3e3":"code","ba507e3e":"code","74acecff":"code","9513ac6c":"code","88addca2":"code","a2b5ea92":"code","cdd8a944":"code","57d77a1e":"code","42437f74":"code","3c462d1c":"code","a99af082":"code","04b404fe":"code","e6915978":"code","82b89d2a":"code","4b4e7874":"markdown","f0fbd972":"markdown","63bcff4d":"markdown","0f388619":"markdown","c6fa5e84":"markdown","56e2d80f":"markdown","90933372":"markdown","723b3593":"markdown","f39456da":"markdown","1edc3b28":"markdown","d6064e8d":"markdown","e1155307":"markdown","0e1e4731":"markdown","3d3218a7":"markdown","3cc889b1":"markdown","3a133bff":"markdown","d698771f":"markdown","2ef5eaff":"markdown","982ec5ae":"markdown","21165495":"markdown","f97601ac":"markdown","f518fc70":"markdown","7d814f9a":"markdown","a428188b":"markdown","b78204dc":"markdown","17585e08":"markdown","e52e4966":"markdown","4e971347":"markdown","b806d404":"markdown","0cd3d5f7":"markdown","69e8370c":"markdown","9ec10f05":"markdown","eaecdc5b":"markdown","860e8e57":"markdown","76ec728f":"markdown","d1af7e19":"markdown"},"source":{"9ead7fd4":"# required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# preprocessing libraries\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# For selecting optimum features\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# Machine Learning algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\n# for evaluating the model performance\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score\n\n# set style for the plots\nsns.set_theme(style=\"darkgrid\")","1a8db356":"# importing the dataset\ndf = pd.read_csv('..\/input\/internet-service-churn\/internet_service_churn.csv')\ndf.head() # see the top 5 rows of the dataset","a320531d":"# Checking the shape of the dataset\ndf.shape","7a267ea8":"# checking basic information about the dataset\n\ndf.info()","a153c266":"# First let's see how much customer are already churned\n\nplt.figure(figsize=(10,6))\nsns.countplot(data = df, x = 'churn', palette= 'ch:start=.2,rot=-.3')\nplt.plot()","035a8252":"df['churn'].value_counts()","830136fb":"# Now let's check if the customer has \"tv_subscription\" who has cancelled the service\n\n# First check the number of TV subscriber and not subscriberr\ndf['is_tv_subscriber'].value_counts()","707f9874":"plt.figure(figsize = (10,6))\nsns.countplot(data = df, x = 'is_tv_subscriber', palette= 'ch:start=.2,rot=-.3')\nplt.title('TV subscriber or Not')\nplt.show()","457bbd81":"# Now let's check if the customers has a \"Movie Package subscription\" or not\n# First we'll see the number of counts and then will see a visual graph for this\ndf['is_movie_package_subscriber'].value_counts()","60bd1ef2":"plt.figure(figsize = (10,6))\nsns.countplot(data =df, x = 'is_movie_package_subscriber', palette= 'ch:start=.2,rot=-.3')\nplt.title('Movie Package Subscription or Not')\nplt.show()","05eee0fb":"# creating subplot\nfig, axes = plt.subplots(1, 2, figsize=(8, 6))\n\n# Tv subscription or not\nsns.countplot(data = df, x = 'is_tv_subscriber', hue = 'churn', palette= 'ch:start=.2,rot=-.3', ax=axes[0])\naxes[0].set_title(\"TV Subscription\")\n\n# Movie package subscription or not\nsns.countplot(data = df, x = 'is_movie_package_subscriber', hue = 'churn', palette= 'ch:start=.2,rot=-.3', ax=axes[1])\naxes[1].set_title('Movie Package Subscription')\nfig.tight_layout()","d720bb4d":"# Now, let's check for  service_failure_count features\n# first check the unique category\nprint(df['service_failure_count'].unique())\nprint(df['service_failure_count'].value_counts())","39013130":"# Let's see how much customer phoned for service more than 6 times in last 3 months\n\ndata = df[df['service_failure_count'] > 6]\nplt.figure(figsize = (10,6))\nsns.countplot(data = data, x = 'service_failure_count', palette= 'ch:start=.2,rot=-.3')\nplt.title('Phoned For Service')\nplt.show()","85d5b24d":"# Here we can see only a few customer phoned more than 6 times within last 3 months.\n# which means most of the customer didn't phone much.\n\n# Let's see how much customer phoned for service less than 6 times in last 3 months\ndata = df[df['service_failure_count'] < 6]\nplt.figure(figsize = (10,6))\nsns.countplot(data = data, x = 'service_failure_count', palette= 'ch:start=.2,rot=-.3')\nplt.title('Phoned For Service')\nplt.show()","42a65142":"# Now let's check the 'subscription age' and how it is distributed\n# I'll use hisagram for that\n\nplt.figure(figsize = (10,6))\nsns.histplot(data = df, x= 'subscription_age', kde = True, hue = 'churn')\nplt.title('Subscription Age vs Count')\nplt.show()","313cb6d8":"# Now let's use a jointplot to see the age and avg bill at time\nplt.figure(figsize = (10,6))\nsns.jointplot(data = df, x = 'subscription_age', y = 'bill_avg', kind =  'scatter', palette= 'ch:start=.2,rot=-.3')\nplt.show()","97104dc8":"plt.figure(figsize = (10,6))\nsns.jointplot(data = df, x = 'download_avg', y = 'upload_avg', kind =  'scatter', palette= 'ch:start=.2,rot=-.3')\nplt.show()","af510e58":"# Let's use boxplot to see if there is outliers in the dataset.\n# for this I'll use a function to make it easy\ndef boxplot_(var):\n    plt.figure(figsize = (10,7))\n    sns.boxplot(y = var, data = df, x = 'churn', palette= 'ch:start=.2,rot=-.3')\n    plt.show()","a3c5a509":"boxplot_('subscription_age')","5fe04677":"# for average bill\nboxplot_('bill_avg')","ce6d8680":"# for download_avg\nboxplot_('download_avg')","0d47942f":"boxplot_('upload_avg')","a4622440":"# pairplot\ndf.dropna()\nsns.pairplot(df.drop('id', axis = 1), hue = 'churn', palette= 'ch:start=.2,rot=-.3') # we don't need id column for our task\nplt.show()","fa288862":"# heatmap to find correlation\n\nplt.figure(figsize = (12,8))\nsns.heatmap(np.round(df.corr(), 3), annot= True,  cmap= 'YlGnBu')\nplt.show()","6c91b03d":"# First I'll take care of the missing values\n# check the missing vlaue\ndf.isnull().mean()","f83593dd":"# Taking care of \"reamining_contract\"\ndf['reamining_contract'] = df['reamining_contract'].fillna(df['reamining_contract'].median())","c6a2b8b2":"# For the rest two fetures I'll use mean to impute them.\ndf['download_avg'] = df['download_avg'].fillna(df['download_avg'].mean())\ndf['upload_avg'] = df['upload_avg'].fillna(df['upload_avg'].mean())","c515bc9e":"df.isnull().mean()","048cb448":"# removing id column\ndf.drop('id', axis = 1, inplace = True)","55d5af90":"# Now first let's seperate the target from the features\n\nx = df.iloc[:, 0:-1]\ny = df.iloc[:, -1]","5bef9b3d":"# checking the shape of training set\nx.shape","6f744cba":"# first let's split the dataset into train & test set after that I'll perform feature selection.\n# This because if we done this step before spliting this may cause bias\n# spliting\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.20, random_state = 41)\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","293e2bcd":"# For this notebook I'll use \"Feature Importance\" method\nmodel = ExtraTreesClassifier()\nmodel.fit(x_train, y_train)\n\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n","7d5d1330":"# Let's use a plot graph of feature importances for better visualization\n\nfeat_importances = pd.Series(model.feature_importances_, index=x_train.columns)\nfeat_importances.nlargest(7).plot(kind='barh')\nplt.show()","2298e617":"# columns of train set\nx_train.columns","4545983f":"# keeping the top 7 features let's drop other features from our dataset\nx_train.drop(['service_failure_count',  'download_over_limit'], \n             axis = 1, inplace = True)","1825ff00":"# Let's do same for the test set\nx_test.drop(['service_failure_count','download_over_limit'], \n             axis = 1, inplace = True)","f4a590fb":"# check the shape of training set\nx_train.shape","0e8b2565":"x_train.head()","00bbe907":"# Before applying Machine Learning Let's scale the data\nscale = StandardScaler()\nx_train = scale.fit_transform(x_train)\nx_test = scale.transform(x_test)","b503c2ca":"log_cls = LogisticRegression(random_state=41, max_iter=75) # creating instance\nlog_cls.fit(x_train, y_train) # fitting the traing data\n\n# Predicting for the test data\nlog_pred = log_cls.predict(x_test)","75d284df":"# let's create confusion matrix to see the accuracy\ncm = confusion_matrix(y_test, log_pred)\ncm","cd604d0b":"# The accuracy from logistic regression classifier\n\nlog_accuracy = (4956+7097)\/(4956+7097+1525+877)\nlog_accuracy","15eec60d":"# precision\nlog_pre = precision_score(y_test, log_pred)\nlog_pre","94bfdf1c":"# recall\nlog_rec = recall_score(y_test, log_pred)\nlog_rec","4113d097":"kn_cls = KNeighborsClassifier(n_neighbors=10, metric= 'minkowski', p =2)\nkn_cls.fit(x_train, y_train) # fitting the model\n\n# predicting output\nkn_pred = kn_cls.predict(x_test)","561b0a12":"# Creating confusion matrix\ncm_kn = confusion_matrix(y_test, kn_pred)\nprint(cm_kn)","33ed789b":"# Accuracy from KNN\n\nknn_accuracy = (5936+7071)\/(5936+545+903+7071)\nknn_accuracy","c8deeabf":"# precision\nknn_pre = precision_score(y_test, kn_pred)\nknn_pre","f904e0ed":"# recall\nknn_rec = recall_score(y_test, kn_pred)\nknn_rec","817a4734":"d_tree = DecisionTreeClassifier(criterion='entropy') # creating instance\nd_tree.fit(x_train, y_train) # fitting the training data\n\n# predicting the \nd_tree_pred = d_tree.predict(x_test)","11bcc609":"cm_d_tree = confusion_matrix(y_test, d_tree_pred)\ncm_d_tree","593d526a":"d_tree_acc = (5868+7289)\/(5868+613+685+7289)\nprint(d_tree_acc)","e1de75a9":"# precision\nd_tree_pre = precision_score(y_test, d_tree_pred)\nd_tree_pre","0fa498d5":"# recall\nd_tree_rec = recall_score(y_test, d_tree_pred)\nd_tree_rec","d2132ab9":"random_forest = RandomForestClassifier(n_estimators= 13, criterion='entropy')\nrandom_forest.fit(x_train, y_train) # fitting the model\n\n# predicting \nrandom_pred = random_forest.predict(x_test)","e1612f4b":"cm_random = confusion_matrix(y_test, random_pred)\ncm_random","931ea841":"random_acc = (6149+7360)\/(6149+332+614+7360)\nprint(\"Accuracy: {}\".format(random_acc))","f13a2eb6":"# precision\nrandom_pre = precision_score(y_test, random_pred)\nrandom_pre","769d4b0a":"# recall\nrandom_rec = recall_score(y_test, random_pred)\nrandom_rec","4b2be3e3":"svc = SVC(kernel= 'sigmoid', random_state= 41)\nsvc.fit(x_train, y_train)\n\n# prediction\nsvc_pred = svc.predict(x_test)","ba507e3e":"cm_svc = confusion_matrix(y_test, svc_pred)\ncm_svc","74acecff":"svc_accuracy = (4583+6161)\/(4583+1898+1813+6161)\nprint(svc_accuracy)","9513ac6c":"# precision\nsvc_pre = precision_score(y_test, svc_pred)\nsvc_pre","88addca2":"# recall\nsvc_rec = recall_score(y_test, svc_pred)\nsvc_rec","a2b5ea92":"nb_log = GaussianNB()\nnb_log.fit(x_train, y_train) # fitting the data\n\n# predicting\nnb_pred = nb_log.predict(x_test)","cdd8a944":"cm_nb = confusion_matrix(y_test, nb_pred)\ncm_nb","57d77a1e":"nb_acc = (4635+6765)\/(4635+1846+1209+6765)\nprint(nb_acc)","42437f74":"# precision\nnb_pre = precision_score(y_test, nb_pred)\nnb_pre","3c462d1c":"# recall\nnb_rec = recall_score(y_test, nb_pred)\nnb_rec","a99af082":"# creating performance dictionary\nperformance = {'Accuracy_Score':[log_accuracy, knn_accuracy, d_tree_acc, random_acc, svc_accuracy, nb_acc],\n              'Precision':[log_pre, knn_pre, d_tree_pre, random_pre, svc_pre, nb_pre],\n              'Recall': [log_rec, knn_rec, d_tree_rec, random_rec, svc_rec, nb_rec]}","04b404fe":"performance_df = pd.DataFrame(data = performance, index= ['Logistic Regression', 'KNeighbors', \n                                                          'DecisionTree', 'RandomForest',\n                                                          'SVC', 'GaussianNB'])","e6915978":"performance_df","82b89d2a":"# Model performance chart\n\nperformance_df.plot(kind=\"bar\", figsize=(12,8))\nplt.title('Models vs (Accuracy_Score, Precision, Recall)', fontdict= {'weight': 'bold',\n        'size': 16})\nplt.xlabel('Models', fontdict= {'size': 14})\nplt.ylabel('Accuracy_Score, Precision, Recall',fontdict= {'size': 14})\nplt.show()","4b4e7874":"Some variable like \"reamining_contract\" has 29% missing values. Other two variables \"download_avg\" & \"upload_av\" has also a little missing value and their percetage is same 0.52%","f0fbd972":"#### DecisionTreeClassifier\n","63bcff4d":"#### KNeighborsClassifier","0f388619":"Here most of the download and upload speed relay within 1000 and 100 and we there are some exceptional data point which may be outliers in the dataset.","c6fa5e84":"KNN algoritm has higher precision & recall value than logistic model","56e2d80f":"Here we can see the top three algorithm for this problem is \"KNeighbour, Decision Tree & Random Forest\" SO, for this problem anyone of these there can be used.","90933372":"Here we can see that most of the customer has \"TV Subscription\" Let's see this with a plot","723b3593":"Removing all NaN values from the dataset we got a complete dataset. Now let's remove id column which is not requried for this task","f39456da":"With KNeighboursClassifier we got 89.9% of accuracy","1edc3b28":"With RandomForestClassifier we've got an accuracy of 93.45% which is good. Let's use two more classifer","d6064e8d":"Here some of the variable are positively correlated like the bill age is related to the download_avg and the download_avg with upload_avg, and also the tv_subscription is related to movie subscription.\n\nOn the other hand some of are negatively correlated like the remaining_contact is highly negatively correlated to churn. That's means decreasing the number of remaining contact may happen to churn.\n\n##### This is the end of our EDA. In next step I'll perform feature engineering or data cleaning part","e1155307":"#### Support Vector Machine Classifier","0e1e4731":"GaussianNB model perform not so good and it's precision & recall value are less than previous models except Support Vector Regressor","3d3218a7":"Here we can see the customers who has 'tv subscription' cancelled the service. But the customer who cancelled the service doesn't have movie pack subscription so much","3cc889b1":"Here we've choice these 7 features that explain almost 99% of the target prediction.","3a133bff":"## Exploratory Data Analysis (EDA)\n","d698771f":"The dataset has 72274 rows and 11 columns at all.","2ef5eaff":"### Performance at a Glance","982ec5ae":"Impressive. Most of the customer didn't phone at least once within last three months","21165495":"With KNeighboursClassifier we got 91.02% of accuracy","f97601ac":"SVC model perform not so good and it's precision & recall value are less than previous models","f518fc70":"Here we can see that most of the customer who has just started the service has higher average bill and most older customer has lower average bill. This is not a very important interpretation because it can be depend on the speed and package the customer has.\n\nNow let's check the average download and upload limit","7d814f9a":"We got a high precision & recall value which is good for a model like this.","a428188b":"From the above visualization and \"counted number\" it can be interpreted that most of customer has already cancelled the service.","b78204dc":"With GaussianNB we've got an accuracy of 78.86%","17585e08":"#### GaussianNB","e52e4966":"### Feature Selection\nFor this notebook I'll use \"Feature Importance\" technique","4e971347":"Here only one third of the customer has \"Movie Package Subscription\" and rest don't have any. Let's see with a countplot\n\n","b806d404":"Decision Tree algoritm has more higher precision & recall value than KNN","0cd3d5f7":"We've selectted top five predictors for this prediction. Now We will implement Machine Learning.\n\nIn this notebook I'll apply several Machine learning algorithm so that I can obtain the best performing algorithm.\n\n## Machine Learning\n\n#### LogisticRegression Classifier","69e8370c":"For the 'subscription_age' there are so many outlier. Let's check for other variables","9ec10f05":"From above we can see for each feature we've some outliers. we'll take care of them in feature engineering section.\n\nNow let's use a pairplot to see all features at a glance and also a heatmap to see the correlation","eaecdc5b":"Here most of the features has integer and float datatype but some may shoule as category which will checked later. For now let's check the null values.","860e8e57":"#### RandomForestClassifier","76ec728f":"## Data Cleaning \/ Feature Engineering","d1af7e19":"With Support Vector Machine Classifier we've got an accuracy of 74.32%"}}