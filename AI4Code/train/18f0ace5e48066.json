{"cell_type":{"98a6efac":"code","c24a5362":"code","07b7b923":"code","5e5c7d06":"code","6c17ed57":"code","8289aef3":"code","b7939ca3":"code","51f09a1a":"code","1c66a135":"code","30ff9398":"code","e2512130":"code","94059921":"code","b91d1f4e":"code","cfed2f11":"code","d719a89a":"code","00acc0e9":"code","bac9d982":"code","cb3e5b73":"code","483d6548":"code","ea101173":"code","97574a55":"code","37eab07a":"code","2c94268c":"code","312072b7":"code","fdb02366":"code","c1aa3605":"code","b7dbed02":"code","84990d7b":"code","c908a821":"code","b4b15e5a":"code","4df557e6":"code","81c896b0":"code","d0d85f78":"code","27ab993b":"code","c631b102":"code","31a69bc5":"code","6a58c99f":"code","b319c306":"code","59b15b7e":"code","99657af1":"code","b43ace8c":"code","0dc36fbd":"code","e8c86ea8":"code","23476324":"code","1da8db1c":"code","9d9ee8ef":"code","4c0e9291":"code","96283066":"code","b9272035":"code","5cf25197":"code","186c6fdb":"markdown","6eaed186":"markdown","fd8fc68a":"markdown","045e9d35":"markdown","cd75ebd6":"markdown","8cf957c6":"markdown","1456479c":"markdown","2841c507":"markdown","8c658e97":"markdown","7a491dc8":"markdown","40e2fe54":"markdown","b97bf2c2":"markdown","f4a2599e":"markdown","55971743":"markdown","99ff9d1d":"markdown","9752bdba":"markdown","cf87b44e":"markdown","d79a471d":"markdown","a187f1a0":"markdown","c6b435a0":"markdown","e2a7884a":"markdown","778122a0":"markdown","ca783d80":"markdown","4b086fc6":"markdown","0c5e0469":"markdown","6defbb30":"markdown","7b4f6e2b":"markdown","538fb3de":"markdown","569a20f2":"markdown","dba41795":"markdown","34c4feb1":"markdown","8602dab5":"markdown","f6cd5329":"markdown","741d898f":"markdown","754dbb28":"markdown","6ec18e10":"markdown","1b40a727":"markdown","09be5d75":"markdown","11a095fc":"markdown","f76bf979":"markdown","9e661971":"markdown","87b2c6d8":"markdown","05b81776":"markdown","a80405df":"markdown","5a9a3ea8":"markdown","d846a70d":"markdown","0d347c22":"markdown","1bb6f7ee":"markdown","c07d6e7f":"markdown","d8c663ee":"markdown"},"source":{"98a6efac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #charting\nfrom scipy.stats import mode #statistics for slope\nfrom sklearn.metrics import mean_squared_error #error metric to optimise when we build a model\nfrom math import sqrt #Other math functions\nimport plotly.express as px #alternative charting function\nimport lightgbm as lgb #popular model choice\nimport seaborn as sns #alternative charting function\nimport gc\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c24a5362":"#change display when printing .head for example the default settings only displays limited number of columns\npd.set_option('display.max_columns', 500)","07b7b923":"#The data we will be using is a foreign exhange rates dataset kindly provided to Kaggle. \nforex_df = pd.read_csv('\/kaggle\/input\/foreign-exchange-rates-per-dollar-20002019\/Foreign_Exchange_Rates.csv',engine = 'python')\n","5e5c7d06":"#let's go ahead and preview the top or bottom n rows\nforex_df.tail(6)","6c17ed57":"#you can see that the currencies are 'objects' which effectively means they are strings. We will need to convert later on to numeric to enable calculations.\nforex_df.info()","8289aef3":"#What is the number of rows, and columns in this dataset?\nforex_df.shape","b7939ca3":"#create a list of all the currency columns in the dataset\ncurrency_list = ['AUSTRALIA - AUSTRALIAN DOLLAR\/US$','EURO AREA - EURO\/US$','NEW ZEALAND - NEW ZELAND DOLLAR\/US$','UNITED KINGDOM - UNITED KINGDOM POUND\/US$','BRAZIL - REAL\/US$','CANADA - CANADIAN DOLLAR\/US$','CHINA - YUAN\/US$','HONG KONG - HONG KONG DOLLAR\/US$','INDIA - INDIAN RUPEE\/US$','KOREA - WON\/US$','MEXICO - MEXICAN PESO\/US$','SOUTH AFRICA - RAND\/US$','SINGAPORE - SINGAPORE DOLLAR\/US$','DENMARK - DANISH KRONE\/US$','JAPAN - YEN\/US$','MALAYSIA - RINGGIT\/US$','NORWAY - NORWEGIAN KRONE\/US$','SWEDEN - KRONA\/US$','SRI LANKA - SRI LANKAN RUPEE\/US$','SWITZERLAND - FRANC\/US$','TAIWAN - NEW TAIWAN DOLLAR\/US$','THAILAND - BAHT\/US$']\n#cleanse data\nfor c in currency_list:\n    #ffill simply takes the previous row and applies it to the next row. We have conditioned this to only be applied to non numeric data.\n    forex_df[c] = forex_df[c].where(~forex_df[c].str.isalpha()).ffill()\n    #we then want to convert the currency columns into numeric so that we can apply functions to it.\n    forex_df[c] = pd.to_numeric(forex_df[c], errors='coerce') ","51f09a1a":"#Let's check that this actually did what we intended.\nforex_df.tail()","1c66a135":"#let's check that the columns are now numeric, yep that worked!\nforex_df.info()","30ff9398":"#generate features\n\n# time features\nforex_df['date'] = pd.to_datetime(forex_df['Time Serie'])\nforex_df['year'] = forex_df['date'].dt.year\nforex_df['month'] = forex_df['date'].dt.month\nforex_df['week'] = forex_df['date'].dt.week\nforex_df['day'] = forex_df['date'].dt.day\nforex_df['dayofweek'] = forex_df['date'].dt.dayofweek\n\n","e2512130":"# lag features\nforex_df['lag_t1'] = forex_df['AUSTRALIA - AUSTRALIAN DOLLAR\/US$'].transform(lambda x: x.shift(1))\nforex_df['lag_t3'] = forex_df['AUSTRALIA - AUSTRALIAN DOLLAR\/US$'].transform(lambda x: x.shift(3))\nforex_df['lag_t7'] = forex_df['AUSTRALIA - AUSTRALIAN DOLLAR\/US$'].transform(lambda x: x.shift(7))\n","94059921":"# lag other country features\nfor c in [x for x in currency_list if x != \"AUSTRALIA - AUSTRALIAN DOLLAR\/US$\"]:\n    forex_df['lag_t1_%s' % c] = forex_df[c].transform(lambda x: x.shift(1))","b91d1f4e":"# ratio lag other country features\nfor c in [x for x in currency_list if x != \"AUSTRALIA - AUSTRALIAN DOLLAR\/US$\"]:\n    forex_df['lag_t1_ratio_%s' % c] = forex_df['lag_t1']  \/ forex_df['lag_t1_' + c] ","cfed2f11":"forex_df.tail()","d719a89a":"#rolling features\n#mean\nforex_df['rolling_mean_t1_t7'] = forex_df['lag_t1'].rolling(7,min_periods=1).mean()\nforex_df['rolling_mean_t1_t14'] = forex_df['lag_t1'].rolling(14,min_periods=1).mean()\nforex_df['rolling_mean_t1_t28'] = forex_df['lag_t1'].rolling(28,min_periods=1).mean()\nforex_df['rolling_mean_t1_t90'] = forex_df['lag_t1'].rolling(90,min_periods=1).mean()\nforex_df['rolling_mean_t1_t180'] = forex_df['lag_t1'].rolling(180,min_periods=1).mean()\nforex_df['rolling_mean_t1_t360'] = forex_df['lag_t1'].rolling(360,min_periods=1).mean()\n\n#max\nforex_df['rolling_max_t1_t7'] = forex_df['lag_t1'].rolling(7,min_periods=1).max()\nforex_df['rolling_max_t1_t14'] = forex_df['lag_t1'].rolling(14,min_periods=1).max()\nforex_df['rolling_max_t1_t28'] = forex_df['lag_t1'].rolling(28,min_periods=1).max()\nforex_df['rolling_max_t1_t90'] = forex_df['lag_t1'].rolling(90,min_periods=1).max()\nforex_df['rolling_max_t1_t180'] = forex_df['lag_t1'].rolling(180,min_periods=1).max()\nforex_df['rolling_max_t1_t360'] = forex_df['lag_t1'].rolling(360,min_periods=1).max()\n\n#min\nforex_df['rolling_min_t1_t7'] = forex_df['lag_t1'].rolling(7,min_periods=1).min()\nforex_df['rolling_min_t1_t14'] = forex_df['lag_t1'].rolling(14,min_periods=1).min()\nforex_df['rolling_min_t1_t28'] = forex_df['lag_t1'].rolling(28,min_periods=1).min()\nforex_df['rolling_min_t1_t90'] = forex_df['lag_t1'].rolling(90,min_periods=1).min()\nforex_df['rolling_min_t1_t180'] = forex_df['lag_t1'].rolling(180,min_periods=1).min()\nforex_df['rolling_min_t1_t360'] = forex_df['lag_t1'].rolling(360,min_periods=1).min()\n\n#standard deviation\nforex_df['rolling_std_t1_t7'] = forex_df['lag_t1'].rolling(7,min_periods=1).std()\nforex_df['rolling_std_t1_t14'] = forex_df['lag_t1'].rolling(14,min_periods=1).std()\nforex_df['rolling_std_t1_t28'] = forex_df['lag_t1'].rolling(28,min_periods=1).std()\nforex_df['rolling_std_t1_t90'] = forex_df['lag_t1'].rolling(90,min_periods=1).std()\nforex_df['rolling_std_t1_t180'] = forex_df['lag_t1'].rolling(180,min_periods=1).std()\nforex_df['rolling_std_t1_t360'] = forex_df['lag_t1'].rolling(360,min_periods=1).std()\n\n#median\nforex_df['rolling_med_t1_t7'] = forex_df['lag_t1'].rolling(7,min_periods=1).median()\nforex_df['rolling_med_t1_t14'] = forex_df['lag_t1'].rolling(14,min_periods=1).median()\nforex_df['rolling_med_t1_t28'] = forex_df['lag_t1'].rolling(28,min_periods=1).median()\nforex_df['rolling_med_t1_t90'] = forex_df['lag_t1'].rolling(90,min_periods=1).median()\nforex_df['rolling_med_t1_t180'] = forex_df['lag_t1'].rolling(180,min_periods=1).median()\nforex_df['rolling_med_t1_t360'] = forex_df['lag_t1'].rolling(360,min_periods=1).median()","00acc0e9":"# exponential moving averages\nforex_df['rolling_ema_t1_t7'] = forex_df['lag_t1'].ewm(span=7,adjust=False).mean()\nforex_df['rolling_ema_t1_t14'] = forex_df['lag_t1'].ewm(span=14,adjust=False).mean()\nforex_df['rolling_ema_t1_t28'] = forex_df['lag_t1'].ewm(span=28,adjust=False).mean()\nforex_df['rolling_ema_t1_t90'] = forex_df['lag_t1'].ewm(span=90,adjust=False).mean()\nforex_df['rolling_ema_t1_t180'] = forex_df['lag_t1'].ewm(span=180,adjust=False).mean()\nforex_df['rolling_ema_t1_t360'] = forex_df['lag_t1'].ewm(span=360,adjust=False).mean()","bac9d982":"#Take a quick look at the data over time now that we have some features to compare against:\n# This is a relatively easy method to plot multiple values on a line chart plus it allows you to dynamically interact with the chart\ndf_long=pd.melt(forex_df, id_vars=['date'], value_vars=['AUSTRALIA - AUSTRALIAN DOLLAR\/US$', 'rolling_ema_t1_t7', 'rolling_mean_t1_t7', 'rolling_ema_t1_t360', 'rolling_med_t1_t360'])\n\n# plotly \nfig = px.line(df_long, x='date', y='value', color='variable')\n\n# Show plot \nfig.show()","cb3e5b73":"#round the value to 0 decimals\nforex_df['lag_t1_round_0'] = forex_df['lag_t1'].round(0)\nforex_df['lag_t3_round_0'] = forex_df['lag_t3'].round(0)\nforex_df['lag_t7_round_0'] = forex_df['lag_t7'].round(0)\n\n#get the decimal place\nforex_df['lag_t1_dec'] = forex_df['lag_t1'] - forex_df['lag_t1_round_0']\nforex_df['lag_t3_dec'] = forex_df['lag_t3'] - forex_df['lag_t3_round_0']\nforex_df['lag_t7_dec'] = forex_df['lag_t7'] - forex_df['lag_t7_round_0']\n\n#round the value to 1 decimals, as the rounded value to 0 decimals is nearly always 1 in the case of AUD\/USD\nforex_df['lag_t1_round_1'] = forex_df['lag_t1'].round(1)\nforex_df['lag_t3_round_1'] = forex_df['lag_t3'].round(1)\nforex_df['lag_t7_round_1'] = forex_df['lag_t7'].round(1)","483d6548":"#rolling mode of rounded figure\nforex_df['lag_t1_mode_7'] = forex_df['lag_t1_round_1'].rolling(window=7,min_periods=1).apply(lambda x: mode(x)[0])\nforex_df['lag_t1_mode_14'] = forex_df['lag_t1_round_1'].rolling(window=14,min_periods=1).apply(lambda x: mode(x)[0])\nforex_df['lag_t1_mode_28'] = forex_df['lag_t1_round_1'].rolling(window=28,min_periods=1).apply(lambda x: mode(x)[0])\nforex_df['lag_t1_mode_90'] = forex_df['lag_t1_round_1'].rolling(window=90,min_periods=1).apply(lambda x: mode(x)[0])\nforex_df['lag_t1_mode_180'] = forex_df['lag_t1_round_1'].rolling(window=180,min_periods=1).apply(lambda x: mode(x)[0])\nforex_df['lag_t1_mode_360'] = forex_df['lag_t1_round_1'].rolling(window=360,min_periods=1).apply(lambda x: mode(x)[0])","ea101173":"#frequency of mode\n","97574a55":"#ranges\nforex_df['rolling_range_t1_t7'] = forex_df['rolling_max_t1_t7'] - forex_df['rolling_min_t1_t7']\nforex_df['rolling_range_t1_t14'] = forex_df['rolling_max_t1_t14'] - forex_df['rolling_min_t1_t14']\nforex_df['rolling_range_t1_t28'] = forex_df['rolling_max_t1_t28'] - forex_df['rolling_min_t1_t28']\nforex_df['rolling_range_t1_t90'] = forex_df['rolling_max_t1_t90'] - forex_df['rolling_min_t1_t90']\nforex_df['rolling_range_t1_t180'] = forex_df['rolling_max_t1_t180'] - forex_df['rolling_min_t1_t180']\nforex_df['rolling_range_t1_t360'] = forex_df['rolling_max_t1_t360'] - forex_df['rolling_min_t1_t360']","37eab07a":"#coefficient of variation - the ratio of standard deviation to mean\nforex_df['rolling_coefvar_t1_t7'] =  forex_df['rolling_std_t1_t7'] \/ forex_df['rolling_mean_t1_t7']\nforex_df['rolling_coefvar_t1_t14'] = forex_df['rolling_std_t1_t14'] \/ forex_df['rolling_mean_t1_t14']\nforex_df['rolling_coefvar_t1_t28'] = forex_df['rolling_std_t1_t28'] \/ forex_df['rolling_mean_t1_t28']\nforex_df['rolling_coefvar_t1_t90'] = forex_df['rolling_std_t1_t90'] \/ forex_df['rolling_mean_t1_t90']\nforex_df['rolling_coefvar_t1_t180'] = forex_df['rolling_std_t1_t180'] \/ forex_df['rolling_mean_t1_t180']\nforex_df['rolling_coefvar_t1_t360'] = forex_df['rolling_std_t1_t360'] \/ forex_df['rolling_mean_t1_t360']","2c94268c":"#ratio of change to standard deviation\n#I like this because if the currency is normally volatile (high std dev), then a change in the rolling mean may be normal. \n#On the other hand if the currency is not normally volatile (low std dev), then it adds weight to any changes observed\nforex_df['rolling_meanstd_t1_t14'] = (forex_df['rolling_mean_t1_t7'] - forex_df['rolling_mean_t1_t14']) \/ forex_df['rolling_std_t1_t14']\nforex_df['rolling_meanstd_t1_t28'] = (forex_df['rolling_mean_t1_t7'] - forex_df['rolling_mean_t1_t28']) \/ forex_df['rolling_std_t1_t28']\nforex_df['rolling_meanstd_t1_t90'] = (forex_df['rolling_mean_t1_t7'] - forex_df['rolling_mean_t1_t90']) \/ forex_df['rolling_std_t1_t90']\nforex_df['rolling_meanstd_t1_t180'] = (forex_df['rolling_mean_t1_t7'] - forex_df['rolling_mean_t1_t180']) \/ forex_df['rolling_std_t1_t180']\nforex_df['rolling_meanstd_t1_t360'] = (forex_df['rolling_mean_t1_t7'] - forex_df['rolling_mean_t1_t360']) \/ forex_df['rolling_std_t1_t360']","312072b7":"#cardinality\nforex_df['lag_t1_card_180'] = forex_df['lag_t1_round_1'].rolling(window=180,min_periods=1).apply(lambda x: np.unique(x).shape[0])\nforex_df['lag_t1_card_360'] = forex_df['lag_t1_round_1'].rolling(window=360,min_periods=1).apply(lambda x: np.unique(x).shape[0])","fdb02366":"#moving average crossover trends, 1 = positive, 0 = negative\nforex_df['lag_t1_trend_7'] = np.where(forex_df['lag_t1'] >= forex_df['rolling_ema_t1_t7'],1,0)\nforex_df['lag_t1_trend_14'] = np.where(forex_df['rolling_ema_t1_t7'] >= forex_df['rolling_ema_t1_t14'],1,0)\nforex_df['lag_t1_trend_28'] = np.where(forex_df['rolling_ema_t1_t7'] >= forex_df['rolling_ema_t1_t28'],1,0)\nforex_df['lag_t1_trend_90'] = np.where(forex_df['rolling_ema_t1_t7'] >= forex_df['rolling_ema_t1_t90'],1,0)\nforex_df['lag_t1_trend_180'] = np.where(forex_df['rolling_ema_t1_t7'] >= forex_df['rolling_ema_t1_t180'],1,0)\nforex_df['lag_t1_trend_360'] = np.where(forex_df['rolling_ema_t1_t7'] >= forex_df['rolling_ema_t1_t360'],1,0)","c1aa3605":"#number of crossovers last n days\nforex_df['lag_t1_no_crossover_7'] = forex_df['lag_t1_trend_7'].rolling(window=7,min_periods=1).sum()\nforex_df['lag_t1_no_crossover_14'] = forex_df['lag_t1_trend_14'].rolling(window=14,min_periods=1).sum()\nforex_df['lag_t1_no_crossover_28'] = forex_df['lag_t1_trend_28'].rolling(window=28,min_periods=1).sum()\nforex_df['lag_t1_no_crossover_90'] = forex_df['lag_t1_trend_90'].rolling(window=90,min_periods=1).sum()\nforex_df['lag_t1_no_crossover_180'] = forex_df['lag_t1_trend_180'].rolling(window=180,min_periods=1).sum()\nforex_df['lag_t1_no_crossover_360'] = forex_df['lag_t1_trend_360'].rolling(window=360,min_periods=1).sum()","b7dbed02":"#decay","84990d7b":"#slope or 1st derivative\nforex_df['lag_t1_slope_7'] = forex_df['lag_t1'].rolling(7).apply(lambda x: np.polyfit(range(7), x, 1)[0]).values\nforex_df['lag_t1_slope_14'] = forex_df['lag_t1'].rolling(14).apply(lambda x: np.polyfit(range(14), x, 1)[0]).values\nforex_df['lag_t1_slope_28'] = forex_df['lag_t1'].rolling(28).apply(lambda x: np.polyfit(range(28), x, 1)[0]).values\nforex_df['lag_t1_slope_90'] = forex_df['lag_t1'].rolling(90).apply(lambda x: np.polyfit(range(90), x, 1)[0]).values\nforex_df['lag_t1_slope_180'] = forex_df['lag_t1'].rolling(180).apply(lambda x: np.polyfit(range(180), x, 1)[0]).values\nforex_df['lag_t1_slope_360'] = forex_df['lag_t1'].rolling(360).apply(lambda x: np.polyfit(range(360), x, 1)[0]).values","c908a821":"#2nd derivative, slope of the 1st derivative, again for detecting trend changes\nforex_df['lag_t1_deriv2_7'] = forex_df['lag_t1_slope_7'].rolling(7).apply(lambda x: np.polyfit(range(7), x, 1)[0]).values\nforex_df['lag_t1_deriv2_14'] = forex_df['lag_t1_slope_7'].rolling(14).apply(lambda x: np.polyfit(range(14), x, 1)[0]).values\nforex_df['lag_t1_deriv2_28'] = forex_df['lag_t1_slope_7'].rolling(28).apply(lambda x: np.polyfit(range(28), x, 1)[0]).values\nforex_df['lag_t1_deriv2_90'] = forex_df['lag_t1_slope_7'].rolling(90).apply(lambda x: np.polyfit(range(90), x, 1)[0]).values\nforex_df['lag_t1_deriv2_180'] = forex_df['lag_t1_slope_7'].rolling(180).apply(lambda x: np.polyfit(range(180), x, 1)[0]).values\nforex_df['lag_t1_deriv2_360'] = forex_df['lag_t1_slope_7'].rolling(360).apply(lambda x: np.polyfit(range(360), x, 1)[0]).values\n","b4b15e5a":"forex_df.shape","4df557e6":"#We have a heap of features:\nlist(forex_df.columns)","81c896b0":"\n#Create a list of the features to drop, as previously mentioned we can't use the feature from today else it would cause target leakage - the model knows something that it can't know in advance.\nuseless_cols = ['Unnamed: 0', \n                \"date\", \n                'AUSTRALIA - AUSTRALIAN DOLLAR\/US$',\n                'Time Serie', \n                'EURO AREA - EURO\/US$',\n                 'NEW ZEALAND - NEW ZELAND DOLLAR\/US$',\n                 'UNITED KINGDOM - UNITED KINGDOM POUND\/US$',\n                 'BRAZIL - REAL\/US$',\n                 'CANADA - CANADIAN DOLLAR\/US$',\n                 'CHINA - YUAN\/US$',\n                 'HONG KONG - HONG KONG DOLLAR\/US$',\n                 'INDIA - INDIAN RUPEE\/US$',\n                 'KOREA - WON\/US$',\n                 'MEXICO - MEXICAN PESO\/US$',\n                 'SOUTH AFRICA - RAND\/US$',\n                 'SINGAPORE - SINGAPORE DOLLAR\/US$',\n                 'DENMARK - DANISH KRONE\/US$',\n                 'JAPAN - YEN\/US$',\n                 'MALAYSIA - RINGGIT\/US$',\n                 'NORWAY - NORWEGIAN KRONE\/US$',\n                 'SWEDEN - KRONA\/US$',\n                 'SRI LANKA - SRI LANKAN RUPEE\/US$',\n                 'SWITZERLAND - FRANC\/US$',\n                 'TAIWAN - NEW TAIWAN DOLLAR\/US$',\n                 'THAILAND - BAHT\/US$']\n\n#define train columns to use in model\ntrain_cols = forex_df.columns[~forex_df.columns.isin(useless_cols)]\n\n\n \n\n","d0d85f78":"#Let's simply use historical data up until Oct 2019\nx_train = forex_df[forex_df['date'] <= '2019-10-31'].copy()\n#The variable we want to predict is AUD to USD rate.\ny_train = x_train['AUSTRALIA - AUSTRALIAN DOLLAR\/US$']\n\n#The LGBM model needs a train and validation dataset to be fed into it, let's use Nov 2019\nx_val = forex_df[(forex_df['date'] > '2019-10-31') & (forex_df['date'] <= '2019-11-30')].copy()\ny_val = x_val['AUSTRALIA - AUSTRALIAN DOLLAR\/US$']\n\n#We shall test the model on data it hasn't seen before or been used in the training process\ntest = forex_df[(forex_df['date'] > '2019-12-01')].copy()\n\n#Setup the data in the necessary format the LGB requires\ntrain_set = lgb.Dataset(x_train[train_cols], y_train)\nval_set = lgb.Dataset(x_val[train_cols], y_val)","27ab993b":"#Set the model parameters\nparams = {\n        \"objective\" : \"regression\", # regression is the type of business case we are running\n        \"metric\" :\"rmse\", #root mean square error is a standard metric to use\n        \"learning_rate\" : 0.05, #the pace at which the model is allowed to reach it's objective of minimising the rsme.\n        'num_iterations' : 2000,\n        'num_leaves': 50, # minimum number of leaves in each boosting round\n        \"early_stopping\": 50, #if the model does not improve after this many consecutive rounds, call a halt to training\n        \"max_bin\": 200,\n        \"seed\":888\n\n}","c631b102":"#Run the model\nm1_lgb = lgb.train(params, train_set, num_boost_round = 2500, valid_sets = [train_set, val_set], verbose_eval = 50)","31a69bc5":"#plot feature importance\nfeature_imp = pd.DataFrame({'Value':m1_lgb.feature_importance(),'Feature':train_cols})\nplt.figure(figsize=(20, 10))\nsns.set(font_scale = 1)\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n                                                    ascending=False)[0:40])\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances-01.png')\nplt.show()","6a58c99f":"#generate predictions on test data\ny_pred = m1_lgb.predict(test[train_cols])\ntest['AUSTRALIA - AUSTRALIAN DOLLAR\/US$_pred'] = y_pred","b319c306":"#view the test data in chart form\ndf_long=pd.melt(test, id_vars=['date'], value_vars=['AUSTRALIA - AUSTRALIAN DOLLAR\/US$', 'AUSTRALIA - AUSTRALIAN DOLLAR\/US$_pred'])\n\n# plotly \nfig = px.line(df_long, x='date', y='value', color='variable')\n\n# Show plot \nfig.show()","59b15b7e":"#RMSE metric\nrms_m1 = sqrt(mean_squared_error(test['AUSTRALIA - AUSTRALIAN DOLLAR\/US$'], test['AUSTRALIA - AUSTRALIAN DOLLAR\/US$_pred']))","99657af1":"rms_m1","b43ace8c":"from sklearn import preprocessing, metrics\nfrom sklearn.model_selection import KFold, TimeSeriesSplit","0dc36fbd":"n_fold = 100\nfolds = TimeSeriesSplit(n_splits=n_fold)\nsplits = folds.split(x_train, y_train)\n\ny_preds = np.zeros(test.shape[0])\ny_oof = np.zeros(x_train.shape[0])\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = train_cols\nmean_score = []","e8c86ea8":"#Set the model parameters\nparams = {\n        \"objective\" : \"regression\", # regression is the type of business case we are running\n        \"metric\" :\"rmse\", #root mean square error is a standard metric to use\n        \"learning_rate\" : 0.05, #the pace at which the model is allowed to reach it's objective of minimising the rsme.\n        'num_iterations' : 2000,\n        'num_leaves': 50, # minimum number of leaves in each boosting round\n        \"early_stopping\": 50, #if the model does not improve after this many consecutive rounds, call a halt to training\n        \"max_bin\": 200,\n        \"seed\":888\n\n}","23476324":"#Include the additional month reserved from validation under the previous method\nx_train_tss = forex_df[forex_df['date'] <= '2019-11-30'].copy() #changed month from Oct to Nov\ny_train_tss = x_train_tss['AUSTRALIA - AUSTRALIAN DOLLAR\/US$'] ","1da8db1c":"for fold_n, (train_index, valid_index) in enumerate(splits):\n    print('Fold:',fold_n+1)\n    #generate the train and validation datasets for each fold\n    X_train1, X_valid1 = x_train_tss[train_cols].iloc[train_index], x_train_tss[train_cols].iloc[valid_index]\n    y_train1, y_valid1 = y_train_tss.iloc[train_index], y_train_tss.iloc[valid_index]\n    #convert the data into lgb format \n    dtrain = lgb.Dataset(X_train1, label=y_train1)\n    dvalid = lgb.Dataset(X_valid1, label=y_valid1)\n    #train lgb model, using same parameters as previous model for more direct comparison\n    clf = lgb.train(params, dtrain, valid_sets = [dtrain, dvalid], verbose_eval=100)\n    #feature importance\n    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n    #make predictions on validation set\n    y_pred_valid = clf.predict(X_valid1,num_iteration=clf.best_iteration)\n    y_oof[valid_index] = y_pred_valid\n    #validation score\n    val_score = np.sqrt(metrics.mean_squared_error(y_pred_valid, y_valid1))\n    print(f'val rmse score is {val_score}')\n    mean_score.append(val_score)\n    y_preds += clf.predict(test[train_cols], num_iteration=clf.best_iteration)\/n_fold\n    del X_train1, X_valid1, y_train1, y_valid1\n    gc.collect()\nprint('mean rmse score over folds is',np.mean(mean_score))","9d9ee8ef":"feature_importances","4c0e9291":"#generate predictions on test data\ny_pred_ts_split = clf.predict(test[train_cols])\ntest['AUSTRALIA - AUSTRALIAN DOLLAR\/US$_pred_ts_split'] = y_pred_ts_split","96283066":"\ndf_long=pd.melt(test, id_vars=['date'], value_vars=['AUSTRALIA - AUSTRALIAN DOLLAR\/US$', 'AUSTRALIA - AUSTRALIAN DOLLAR\/US$_pred','AUSTRALIA - AUSTRALIAN DOLLAR\/US$_pred_ts_split'])\n\n# plotly \nfig = px.line(df_long, x='date', y='value', color='variable')\n\n# Show plot \nfig.show()","b9272035":"#RSME metric\nrms_ts_split = sqrt(mean_squared_error(test['AUSTRALIA - AUSTRALIAN DOLLAR\/US$'], test['AUSTRALIA - AUSTRALIAN DOLLAR\/US$_pred_ts_split']))","5cf25197":"rms_ts_split","186c6fdb":"With rolling features you can set min_periods as 1, that way you don't lose that much data (if you were to drop nulls in a subsequent step) as for example a 7 day rolling average the previous 6 days would be 'n\/a'. This may help especially for longer lags e.g.365 days.","6eaed186":"### 7. TimeSeries Split","fd8fc68a":"#### 7.3 Predictions on Test Data for out of time performance","045e9d35":"It's interesting to try decimal or rounded features if you remember, it can often give a small boost to results","cd75ebd6":"**6.2 Model**","8cf957c6":"### 4. Generate Features","1456479c":"### 2. Install Packages","2841c507":"#### 4.3 Rolling Features","8c658e97":"### 3. Import & Cleanse Data","7a491dc8":"#### 6.4 Predictions on Test Data for out of time performance","40e2fe54":"7.2 Model Interpretation","b97bf2c2":"***","f4a2599e":"So let's get straight into it!","55971743":"Mode is often overlooked, don't forget to give it a try when tackling your next problem","99ff9d1d":"#### 7.1 Apply Time Series Split","9752bdba":"#### 4.5 Trends","cf87b44e":"> **How it Works**<br> \nAlso known as nested cross validation, the aim of this method is to avoid using the values from the future to forecast values in the past. <br>\nIf I simply used regular cross validation then the order of the train and validation data would be random and could incur the above issue. <br>\nThe training data becomes cumulative in order to achieve this e.g. \n\n*     fold 1 : training [1], test [2]\n*     fold 2 : training [1, 2], test [3]\n*     fold 3 : training [1, 2, 3], test [4]\n*     fold 4 : training [1, 2, 3, 4], test [5]\n*     fold 5 : training [1, 2, 3, 4, 5], test [6]\n\nIf it is still not clear, for illustrative purposes if you had data from Jan-Jun 2019, think of fold 1 being training [Jan] test [Feb]; fold 2 being training [Jan,Feb] test [Mar] etc. so that the test set is always occuring after the training set.","d79a471d":"This notebook is useful for absolute beginners who are new to Python, through to intermediate users who I will introduce some more advanced features or a refresher. The code deliberately avoids functions where possible to keep things easy to read for all levels.","a187f1a0":"> **How it Works**<br> \nUsing the date column, we will split the data into the necessary components. <br>\nHow might you decide what is the ideal split of data in training and test? Weigh up having more data in training against the chances of overfitting and vice versa. Run several iterations to compare performance. Also consider what the problem is trying to solve for, e.g. if you are predicting the next month, quarter or year of data for instance, you may want to take that into account for your validation set for example. <br> \n","c6b435a0":"**Advantages**<br> \nThe main benefit of this method is its simplicity and a great deal of control.  <br>\n**Disadvantages**<br> \nThe downside may be lower model performance, but not always.","e2a7884a":"#### 4.1 Date Features","778122a0":"### Summary","ca783d80":"### 2. Timeseries Practice - 2 Training Methods - with Forex Dataset","4b086fc6":"So let's go ahead and create a bunch of rolling features across different days and metrics","0c5e0469":"We may want to add in a ratio as the raw value for GBP may not be as stable as a ratio value","6defbb30":"Ultimately the nature of the business problem will show through trial and error the appropriate method you may want to use. In this example the first method was slightly better against this particular dataset and modelling technique. This is also tradeoff in terms of model run time.","7b4f6e2b":"We can see that the model generally captures trends well, there will always be spikes and volatility that is hard to predict.","538fb3de":"### 6. Method 1 - Train\/ Validation\/ Test Split","569a20f2":"#### 6.3 Model Interpretation","dba41795":"#### 4.4 Other Features - Decimals, Rounding, Mode, Coefficient of Variation","34c4feb1":"Light GBM will be used to build this model.\nLight GBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\n*     Faster training speed and higher efficiency.\n\n*     Lower memory usage.\n\n*     Better accuracy.\n\n*     Support of parallel and GPU learning.\n\n*     Capable of handling large-scale data.\n","8602dab5":"#### 7.4 Model Performance","f6cd5329":"#### 4.2 Lag features","741d898f":"#### 3.1 Cleanse data and Convert to Numeric Format","754dbb28":"**6.1 Manually split data into Train\/ Validation\/ Test datasets based on the date**","6ec18e10":"Thanks for viewing this notebook and wish you a good day :) ","1b40a727":"You can see that there is 'ND' in the above example for 25th December, markets closed for Christmas, so we could either drop this or apply some cleansing to enable us to convert to a numeric field.","09be5d75":"#### 5.1 Drop unnecessary columns","11a095fc":"If the shorter moving average crosses over a longer one, it could be a trend indicator","f76bf979":"We need to shift the data by 1 or more days, so that we can use yesterday's data to predict today and so on. Later on we will remove today's data as we don't want to cause 'data leakage' whereby our model has information available to it that is not known at the time, which would not help it to work in practice.","9e661971":"Purpose is to practice 2 popular methods for splitting training data on timeseries example on an easy dataset that does not run into memory issues.\nI am only going to predict 1 currency, the AUD\/USD rate, and use other countries lagged values as features. <br>\n* 1. Train Validation Test Method \n* 2. Timeseries Split Method\n\n\n","87b2c6d8":"We will also use other countries values from yesterday to aid in predicting today's data","05b81776":"About the dataset - the data is foreign exchange rates from 2000 to 2019 across the major trading pairs. \nThis data is reasonably clean and requires minimal data preparation. We want to be able to make a prediction on the daily AUD\/USD rate at the end of this notebook.\n","a80405df":"#### 6.5 Model Performance","5a9a3ea8":"Generally with timeseries there are two main approaches. One being getting the data into a stationary format, accounting for the trend and seasonality. You then apply more traditional models you may have come across such as 'ARIMA', 'GARCH' etc. In this case, we won't be doing that, but will be applying machine learning models to the data directly without the need for this step which is simpler.","d846a70d":"This shows how many times the feature was used by the model. \n<br>We can see that the model did like a lot of the slope and derivative type features generated at the end.\n<br>The model also liked the ratio of other currencies a fair amount. NB when I ran with these excluded it did not make much difference to performance. \n","0d347c22":"Exponential moving averages provide more weight to recent values, which in finance are generally useful. ","1bb6f7ee":"### 5. Prepare Dataset for Model","c07d6e7f":"note that the value for 24th Dec is carried forward to 25th Dec for those countries, but not for others that did not have this holiday.","d8c663ee":"Cardinality is the number of unique values, e.g. [0,0,0,1,1] has 2 unique values"}}