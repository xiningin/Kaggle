{"cell_type":{"5a875b36":"code","78ec003b":"code","f6c77daa":"code","2fe8c41b":"code","094a5373":"code","07394000":"code","72052253":"code","f0164d2b":"code","ac9b2053":"code","c58b0f72":"code","abc74fa0":"code","1098c661":"code","6a18b3e0":"code","a0364cb4":"code","c435bdc1":"code","dbbba1c2":"code","1471113b":"code","f3dd2fd4":"code","81a5af40":"code","2817e099":"code","54c8472e":"code","10df7d78":"code","6febb594":"code","81bc1377":"code","541cdc1e":"code","df559302":"code","af46d9cc":"code","1cb763a6":"code","381e529a":"code","40bac7b6":"code","8e77d912":"code","5048a397":"code","e39bfaa6":"code","08b7754f":"code","3dd51290":"code","d5b90699":"code","6da10c22":"code","d8b67eb7":"code","4693966c":"code","b3d96025":"code","2f35fb54":"code","1b3fc852":"code","338481a3":"code","cba7ecc0":"code","7d3b63a6":"code","7cbcc066":"code","fb0e8b17":"code","95b7561f":"code","b37d9ac5":"code","136654a7":"markdown","921af292":"markdown","bd7c8949":"markdown","c13e5e1d":"markdown","96b4ba12":"markdown","1f814544":"markdown","51c4e829":"markdown","0cc3127f":"markdown","0e5a3323":"markdown","67851945":"markdown","a211f7a8":"markdown","f420b78c":"markdown","b2a382e1":"markdown","5b329846":"markdown","32dc9505":"markdown","68173a90":"markdown","eb590af1":"markdown","2132734d":"markdown","83199c7b":"markdown","da25b0da":"markdown","c3976963":"markdown","568187d6":"markdown","9bf72d82":"markdown","04724602":"markdown","bfefbf72":"markdown","e6bf8fe1":"markdown","006e5674":"markdown","5a6bee7b":"markdown","8a0d9d1a":"markdown","3853d48e":"markdown","26536d3e":"markdown","04eef987":"markdown","013050f8":"markdown","df33939a":"markdown","3c60a83a":"markdown","781d298d":"markdown","cd42ed44":"markdown","f62168f8":"markdown","23a1cc8a":"markdown","84e35e55":"markdown","fdb324a8":"markdown","a11be679":"markdown","98a2fe9f":"markdown","c8ff36d2":"markdown","4fb372fb":"markdown","eee440b3":"markdown","cba64e76":"markdown","b98548fc":"markdown","73e4b86c":"markdown","73725ae2":"markdown","a68855f8":"markdown","c32cd35e":"markdown","0d031983":"markdown","93a4153e":"markdown","7d8beb35":"markdown","e269b068":"markdown","631ff671":"markdown","0bb56d97":"markdown","669a5a21":"markdown","2f70c433":"markdown","dd2723fa":"markdown","607926b4":"markdown","c0a1fb1a":"markdown","6ac1611b":"markdown","54fa006e":"markdown","f83ff702":"markdown","adc9516c":"markdown","544fca47":"markdown","c0030326":"markdown","f6860b0a":"markdown","e9d7a4a4":"markdown","c463812d":"markdown","7883b2b1":"markdown","e121456c":"markdown","f4ac7ac5":"markdown","1fec8fb9":"markdown","631a12b9":"markdown","d9afde38":"markdown","463c61c6":"markdown","f9243068":"markdown","57c54246":"markdown","e5846cc8":"markdown","96ec04ca":"markdown"},"source":{"5a875b36":"from warnings import filterwarnings\nfilterwarnings('ignore')","78ec003b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\nfrom sklearn.metrics import r2_score,mean_squared_error","f6c77daa":"evaluation = pd.DataFrame({\n    \"Model\":[],\n    \"R2 Score (train)\":[],\n    \"Adjusted R2 Score (train)\":[],\n    \"R2 Score (test)\":[],\n    \"Adjusted R2 Score (test)\":[],\n    \"Root Mean Squared Error(RMSE) (train)\":[],\n    \"Root Mean Squared Error(RMSE) (test)\":[],\n    \"R2 Score (5-Fold Cross Validation)\":[],\n    \"Root Mean Squared Error(RMSE) (5-Fold Cross Validation)\":[]\n})","2fe8c41b":"# Function to calculate adjusted r2 score:\n# where  n  is the number of instances and  k  is the number of features.\ndef adjustedR2(r2_score,n,k):\n    return 1-(((1-r2_score)*(n-1))\/(n-k-1))","094a5373":"# Model Evaluation Function\n\ndef evaluateModel(model,X_train,X_test,y_train,y_test,model_name):\n    \n    if(model_name==\"Polynomial Regression\"):\n        n_train=X_train.shape[1]\n        n_test=X_test.shape[1]\n    else:\n        n_train=len(X_train.columns)\n        n_test=len(X_test.columns)\n        \n    y_predict_test = model.predict(X_test)\n    y_predict_train = model.predict(X_train)\n    \n    r2_score_train = float(format(r2_score(y_train,y_predict_train),'.3f'))\n    \n    r2_score_test = float(format(r2_score(y_test,y_predict_test),'.3f'))\n    \n    rmse_train = float(format(np.sqrt(mean_squared_error(y_train,y_predict_train)),'.3f'))\n    \n    rmse_test = np.sqrt(mean_squared_error(y_test,y_predict_test))\n    \n    ad_r2_score_train = float(format(adjustedR2(r2_score_train,X_train.shape[0],n_train),'.3f'))\n    \n    ad_r2_score_test = float(format(adjustedR2(r2_score_test,X_test.shape[0],n_test),'.3f'))\n                              \n    r2_score_mean = float(format(cross_val_score(model,X_train,y_train,cv=5).mean(),'.3f'))\n                              \n    rmse_mean = -float(format(cross_val_score(model,X_train,y_train,cv=5,scoring=\"neg_root_mean_squared_error\").mean(),'.3f'))\n    \n    r = evaluation.shape[0]\n    evaluation.loc[r]=[model_name,\n                       r2_score_train,ad_r2_score_train,\n                       r2_score_test,ad_r2_score_test,\n                       rmse_train,rmse_test,\n                      r2_score_mean,\n                       rmse_mean]\n    \n    return evaluation.sort_values(by = 'Root Mean Squared Error(RMSE) (5-Fold Cross Validation)', ascending=True)","07394000":"# read and load data\ndf = pd.read_csv(\"..\/input\/used-car-dataset-ford-and-mercedes\/audi.csv\")","72052253":"df.head()","f0164d2b":"df.info()","ac9b2053":"df.describe()","c58b0f72":"df_2=df.copy()","abc74fa0":"\ndf_2=pd.concat((df_2,pd.get_dummies(df_2[\"model\"]),pd.get_dummies(df_2[\"transmission\"]),pd.get_dummies(df_2[\"fuelType\"]))\n               ,axis=1)\n\ndf_2.drop([\"model\",\"transmission\",\"fuelType\"],axis=1,inplace=True)","1098c661":"# correlation\n\nplt.figure(figsize=(30,30))\nsns.heatmap(df_2.corr(),annot=True)\nplt.show()","6a18b3e0":"df_2.drop(\"Petrol\",axis=1,inplace=True)","a0364cb4":"#  Count plot on fuel type\n\nfig, ax1 = plt.subplots(figsize=(5,4))\ngraph = sns.countplot(ax=ax1,x='fuelType', data=df)\ngraph.set_xticklabels(graph.get_xticklabels())\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height\/2,height ,ha=\"center\",fontsize=10)","c435bdc1":"df_2.drop(\"Hybrid\",axis=1,inplace=True)","dbbba1c2":"# Count plot on the transmission\n\nfig, ax1 = plt.subplots(figsize=(5,4))\ngraph = sns.countplot(ax=ax1,x='transmission', data=df)\ngraph.set_xticklabels(graph.get_xticklabels())\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height\/2,height ,ha=\"center\",fontsize=8)","1471113b":"# Pairplotting\n\nsns.pairplot(df)\nplt.show()","f3dd2fd4":"df_2.drop(index=df_2[df_2[\"mileage\"]>160000].index,inplace=True)\ndf_2.drop(index=df_2[df_2[\"year\"]<2000].index,inplace=True)\n\ndf_2.drop(index=df_2[df_2[\"engineSize\"]==0].index,inplace=True)","81a5af40":"stdn_scaler = StandardScaler().fit(df_2[[\"year\",\"mileage\",\"tax\",\"mpg\",\"engineSize\"]])\n\ndf_2[[\"year\",\"mileage\",\"tax\",\"mpg\",\"engineSize\"]] = stdn_scaler.transform(df_2[[\"year\",\"mileage\",\"tax\",\"mpg\",\"engineSize\"]])","2817e099":"train,test = train_test_split(df_2,test_size=0.25,random_state=42)\n\nX_train = train.drop(\"price\",axis=1)\ny_train = train[\"price\"]\n\nX_test = test.drop(\"price\",axis=1)\ny_test = test[\"price\"]","54c8472e":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression().fit(X_train,y_train)","10df7d78":"evaluateModel(lin_reg,X_train,X_test,y_train,y_test,\"Linear Regression\")","6febb594":"from sklearn.linear_model import Ridge\nridge_reg=Ridge().fit(X_train,y_train)","81bc1377":"evaluateModel(ridge_reg,X_train,X_test,y_train,y_test,\"Ridge Regression\")","541cdc1e":"from sklearn.linear_model import Lasso\nlasso_reg = Lasso().fit(X_train,y_train)","df559302":"evaluateModel(lasso_reg,X_train,X_test,y_train,y_test,\"Lasso Regression\")","af46d9cc":"from sklearn.linear_model import ElasticNet\nelastic_reg = ElasticNet().fit(X_train,y_train)","1cb763a6":"evaluateModel(elastic_reg,X_train,X_test,y_train,y_test,\"Elastic Net Regression\")","381e529a":"params={\"alpha\":[0.0001,0.001,0.01,0.1,1,10,100,1000,10000],\"max_iter\":[1000,2500,5000,7500,10000],\n       \"l1_ratio\":[0.3,0.4,0.5,0.6,0.7]}\n\nelastic_gs=GridSearchCV(ElasticNet(random_state=42),param_grid=params,cv=5,scoring=\"neg_root_mean_squared_error\")\nelastic_gs.fit(X_train,y_train)\n\nelastic_reg_best=elastic_gs.best_estimator_.fit(X_train,y_train)\n\nevaluateModel(elastic_reg_best,X_train,X_test,y_train,y_test,\"Best Elastic Net Regression\")","40bac7b6":"from scipy.special import factorial\nfactorial(38)\/(factorial(35)*factorial(3))","8e77d912":"from sklearn.preprocessing import PolynomialFeatures\n\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\n\nX_train_poly = poly_features.fit_transform(X_train[[\"year\",\"mileage\",\"tax\",\"mpg\",\"engineSize\"]])\nX_test_poly= poly_features.fit_transform(X_test[[\"year\",\"mileage\",\"tax\",\"mpg\",\"engineSize\"]])\n\npoly_reg=LinearRegression()\npoly_reg.fit(X_train_poly,y_train)\n\nevaluateModel(poly_reg,X_train_poly,X_test_poly,y_train,y_test,\"Polynomial Regression\")","5048a397":"from sklearn.svm import LinearSVR\n\nsvm_reg = LinearSVR()\nsvm_reg.fit(X_train, y_train)","e39bfaa6":"evaluateModel(svm_reg,X_train,X_test,y_train,y_test,\"Linear SVM Regression\")","08b7754f":"params={\"C\":[100,1000,10000],\n       \"dual\":[True,False],\"epsilon\":[1500,4500,7500],\n       \"fit_intercept\":[True,False],\"max_iter\":[5000,7500,10000]}\n\nsvm_gs=GridSearchCV(LinearSVR(),param_grid=params,cv=5,n_jobs=-1,scoring=\"neg_root_mean_squared_error\")\nsvm_gs.fit(X_train,y_train)\n\nbest_linear_svm_reg=svm_gs.best_estimator_\nbest_linear_svm_reg.fit(X_train,y_train)\nevaluateModel(best_linear_svm_reg,X_train,X_test,y_train,y_test,\"Best Linear SVM Regression\")","3dd51290":"from sklearn.svm import SVR\nnonlinear_svm_reg=SVR()\nnonlinear_svm_reg.fit(X_train,y_train)\nevaluateModel(nonlinear_svm_reg,X_train,X_test,y_train,y_test,\"Nonlinear SVM Regression\")","d5b90699":"best_nonlinear_svm_reg = SVR(C=100000,degree=2,epsilon=1000,gamma=0.1,kernel=\"rbf\",max_iter=10000)\nbest_nonlinear_svm_reg.fit(X_train,y_train)\nevaluateModel(best_nonlinear_svm_reg,X_train,X_test,y_train,y_test,\"Best Nonlinear SVM Regression\")","6da10c22":"from sklearn.tree import DecisionTreeRegressor\ndec_reg=DecisionTreeRegressor()\ndec_reg.fit(X_train,y_train)","d8b67eb7":"evaluateModel(dec_reg,X_train,X_test,y_train,y_test,\"Decision Tree Regressor\")","4693966c":"from sklearn.ensemble import RandomForestRegressor\nrand_reg=RandomForestRegressor().fit(X_train,y_train)\nevaluateModel(rand_reg,X_train,X_test,y_train,y_test,\"Random Forest Regressor\")","b3d96025":"from sklearn.ensemble import AdaBoostRegressor\nadaboost_reg=AdaBoostRegressor().fit(X_train,y_train)","2f35fb54":"evaluateModel(adaboost_reg,X_train,X_test,y_train,y_test,\"Adaboost Regressor\")","1b3fc852":"from sklearn.ensemble import GradientBoostingRegressor\ngb_reg=GradientBoostingRegressor().fit(X_train,y_train)","338481a3":"evaluateModel(gb_reg,X_train,X_test,y_train,y_test,\"Gradient Boosting Regressor\")","cba7ecc0":"from sklearn.metrics import mean_squared_error\ngbrt=GradientBoostingRegressor(max_depth=2,n_estimators=10000)\ngbrt.fit(X_train,y_train)\n\nerrors=[mean_squared_error(y_test,y_pred) for y_pred in gbrt.staged_predict(X_test)]\n\nbest_n_est=np.argmin(errors)+1\n\ngbrt_best=GradientBoostingRegressor(n_estimators=best_n_est,max_depth=2).fit(X_train,y_train)\nevaluateModel(gbrt_best,X_train,X_test,y_train,y_test,\"Gradient Boosting Regressor with optimum n_estimators\")","7d3b63a6":"import xgboost\n\nxgb_reg=xgboost.XGBRegressor()\nxgb_reg.fit(X_train,y_train)\nevaluateModel(xgb_reg,X_train,X_test,y_train,y_test,\"XGB Regressor\")","7cbcc066":"xgb_reg_early_stopping=xgboost.XGBRegressor()\nxgb_reg_early_stopping.fit(X_train,y_train,\n                           eval_set=[(X_test,y_test)],early_stopping_rounds=2)\nevaluateModel(xgb_reg_early_stopping,X_train,X_test,y_train,y_test,\"XGB Regressor with early stopping\")","fb0e8b17":"from sklearn.ensemble import VotingRegressor\n\nvoting_reg=VotingRegressor(\nestimators=[(\"v_lin_reg\",lin_reg),\n            (\"v_rid_reg\",ridge_reg),\n            (\"v_lasso_reg\",lasso_reg),\n            (\"v_elastic_reg_best\",elastic_reg_best),\n            (\"v_elastic_reg\",elastic_reg),\n            (\"v_lin_svm_reg\",svm_reg),\n            (\"v_best_lin_svm_reg\",best_linear_svm_reg),\n            (\"v_nonlinear_svm_reg\",nonlinear_svm_reg),\n            (\"v_best_nonlinear_svm_reg\",best_nonlinear_svm_reg),\n            (\"v_dec_reg\",dec_reg),\n            (\"v_rand_reg\",rand_reg),\n            (\"v_adaboost_reg\",adaboost_reg),\n            (\"v_gb_reg\",gb_reg),\n            (\"v_gb_reg_best\",gbrt_best),\n            (\"v_xgb_reg\",xgb_reg),\n            (\"v_xgb_reg_best\",xgb_reg_early_stopping)\n            ]\n)\n\nvoting_reg.fit(X_train,y_train)","95b7561f":"evaluateModel(voting_reg,X_train,X_test,y_train,y_test,\"Voting Regression\")","b37d9ac5":"evaluation.sort_values(by=\"Root Mean Squared Error(RMSE) (5-Fold Cross Validation)\")","136654a7":"XGBoost is optimized implementation of Gradient Boosting.","921af292":"It is important to scale the data before performing Ridge Regression, as it is sensitive to the scale of the input features. This is true of most regularized models.\n\n-----------","bd7c8949":"# <span id=\"7\"><\/span>**Preprocessing and Visualize**\n#### [Return Contents](#0)\n<hr\/>","c13e5e1d":"Random Forest is an ensemble of Decision Trees,that is, a Random Forest contains more than one Decision Tree.","96b4ba12":"# <span id=\"26\"><\/span>Conclusion","1f814544":"Let's create a VotingRegressor using all the models we have created so far(except Polynomial Regression,because the dataset used to train Polynomial Regression is different from the dataset used to train other models.).","51c4e829":"# <span id=\"22\"><\/span>**Gradient Boosting**\n#### [Return Contents](#0)\n<hr\/>","0cc3127f":"Welcome to my kernel.I'm trying to learn machine learning. I wanted to write a kernel. In this kernel, I used various regression models to predict Audi car prices. Also, I tried to explain briefly the models I used.\n\nIf you have a question or feedback, do not hesitate to write. Thanks \ud83d\ude42\n\n<img src=\"https:\/\/carfromjapan.com\/wp-content\/uploads\/2016\/10\/audi-tax-free-cars.jpg\" title=\"source: https:\/\/carfromjapan.com\/\" \/>","0e5a3323":"# <span id=\"13\"><\/span>Regularized Linear Models\n#### [Return Contents](#0)\n<hr\/>","67851945":"# <span id=\"2\"><\/span> **Importing Modules**\n#### [Return Contents](#0)\n<hr\/>","a211f7a8":"![title](https:\/\/www.researchgate.net\/profile\/Hassen_Bouzgou\/publication\/316351306\/figure\/fig7\/AS:485878301761550@1492853822259\/Example-of-linear-SVM-regression-with-tube.png)\n<center><b>Resource<\/b> :https:\/\/www.researchgate.net\/figure\/Example-of-linear-SVM-regression-with-tube_fig7_316351306<\/center>","f420b78c":"Best Nonlinear SVM Regression:","b2a382e1":"# <span id=\"20\"><\/span>**Random Forest Regressor**\n#### [Return Contents](#0)\n<hr\/>","5b329846":"# **Predicting Audi Car Prices**\n<span id=\"0\"><\/span>\n1. [Overview](#1)\n1. [Importing Modules](#2)\n1. [Defining an Evaluation Table](#3)\n1. [Creating a Model Evaluation Function and Adjusted $R^{2}$ Function](#4)\n1. [Reading the Dataset](#5)\n1. [Take a quick look at the data set](#6)\n1. [Preprocessing and Visualize](#7)\n    * [Handling The Categorical Features](#8)\n    * [Visualizing](#9)\n    * [Scaling](#10)    \n    * [Splitting data into training set and test set](#11)\n1. [Linear Regression](#12)\n1. [Regularized Linear Models](#13)\n    * [Ridge Regression](#14)\n    * [Lasso Regression](#15)\n    * [Elastic Net](#16)\n1. [Polynomial Regression](#17)\n1. [Support Vector Machine (SVM)](#18)  \n1. [Decision Tree Regressor](#19)\n1. [Random Forest Regressor](#20)    \n1. [AdaBoost](#21)\n1. [Gradient Boosting](#22)\n1. [XGBoost (Extreme Gradient Boosting)](#23)\n1. [Voting Regressor](#24)\n1. [Evaluation Table](#25)\n1. [Conclusion](#26)\n1. [Resources](#27)","32dc9505":"Let's train a Gradient Boosting Regressor:","68173a90":" ## <span id=\"9\"><\/span>Visualizing\n #### [Return Contents](#0)\n<hr\/>","eb590af1":"# <span id=\"17\"><\/span>Polynomial Regression\n#### [Return Contents](#0)\n<hr\/>","2132734d":"Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso\u2019s regularization terms, and you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge Regression, and when r = 1, it is equivalent to Lasso Regression.\n\n<b>Elastic Net cost function<\/b>\n$$J(\\theta)=MSE(\\theta)+r\\alpha\\sum_{i=1}^{n}|\\theta_{i}|+\\frac{1-r}{2}\\alpha\\sum_{i=1}^{n}\\theta_{i}^2$$","83199c7b":"Gradient Boosting tries to fit the new predictor to the residual errors made by the previous predictor.For example, suppose you train a model(call this model model1) using an X training set and y targets. To train the next model(call this model model2.), you need to subtract the predictions made by the previous model(i.e,model1) from y, and our new y target values will be these values(y-model1 predictions).\n\nModel 2 is trained using the X training set and the new y target values(y - model1 predictions), and so on.\n\nTo make a prediction for a new instances,predictions of all models are summed up.\n\nThis is the technique used by Gradient Boosting.\n","da25b0da":"As you can see in the correlation matrix, there is a very strong relationship between diesel and petrol, let's remove one of the them:","c3976963":"# <span id=\"6\"><\/span> **Take a quick look at the data set**\n#### [Return Contents](#0)\n<hr\/>","568187d6":"We can use multiple regression models as a single model. The VotingRegressor class can be used for this.The prediction of the VotingRegressor model is the arithmetic mean of the prediction of each regression model. For example, if the prediction of linear regression is 25000 and the prediction of Rasso regression is 15000, the estimate of VotingRegressor is (25000 + 15000) \/ 2 = 20000.","9bf72d82":"Examining the linear relationship between 2 or more numerical variables is called linear regression. \n<br>\nInput variables are called independent variables(or features), and result variables are called dependent variables(or target).\n<br>\nUsually inputs are matrix, outputs are vector.\n<br>\nIf linear regression is created using an independent variable, it is called simple linear regression. If linear regression is created using two or more independent variables, it is called multiple linear regression.\n\n<b>Simple Linear Regression Equation<\/b>\n$$Y=\\beta_{0}+\\beta_{1}x_{1}$$\n\n<b>Multiple Linear Regression Equation<\/b>\n$$Y=\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+...+\\beta_{n}x_{n}$$\n\nLinear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the bias term (also called the intercept term).\n\n<b>Linear Regression model prediction<\/b>\n\n$$\\hat{y}=\\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+...+\\theta_{n}x_{n}$$\n\nIn this equation:\n* $\\hat{y}$ is the predicted value.\n* $n$ is the number of features.\n* $x_{i}$  is the $i^{th}$ feature value\n* $\\theta_{j}$ is the $j^{th}$  model parameter (including the bias term $\u03b8_{0}$ and the feature weights  $\u03b8_{1}$ ,  $\u03b8_{2}$ , \u22ef,  $\u03b8_{n}$ ).","04724602":"That\u2019s the Linear Regression model but how do we train it? Training a model means setting its parameters so that the model best fits the training set. For this purpose, we first need a measure of how well (or poorly) the model fits the training data.Most\ncommon performance measure of a regression model is the Root Mean Square Error (RMSE).Therefore, to train a Linear Regression model, we need to find the value of $\\theta$ that minimizes the RMSE.\n\n<b>Root Mean Square Error (RMSE)<\/b>\n$$RMSE(X, h) = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(\\theta^{T}x^{(i)}-y^{(i)})^{2}}$$\n\nWhen we find the model parameters that will minimize the RMSE, we train the model.So how do we find the model parameters that minimize the RMSE? Approaches for this purpose(This approachs will not be covered in this kernel.):\n* Normal Equation\n* Singular Value Decomposition (SVD)\n* Gradient Descent","bfefbf72":"# <span id=\"27\"><\/span>Resources","e6bf8fe1":"# <span id=\"12\"><\/span>Linear Regression\n#### [Return Contents](#0)\n<hr\/>","006e5674":"Least Absolute Shrinkage and Selection Operator Regression (usually simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function.\n\n<b>Lasso regression retularization term<\/b>\n$$|\\theta_{i}|$$\n\n<b>Lasso Regression cost function<\/b>\n$$J(\\theta)=MSE(\\theta)+\\alpha\\sum_{i=1}^{n}|\\theta_{i}|$$\n\nAn important characteristic of Lasso Regression is that it tends to eliminate the weights of the least important features (i.e., set them to zero). ","5a6bee7b":"# <span id=\"3\"><\/span> **Defining an Evaluation Table**\n#### [Return Contents](#0)\n<hr\/>","8a0d9d1a":"Now let's create a ridge regression model:","3853d48e":"Now let's evaluate model.","26536d3e":"# <span id=\"25\"><\/span>Evaluation Table","04eef987":"We created the model. Now let's evaluate our model.","013050f8":"If the scales of the features are very different, some features may suppress other features. This can adversely affect the performance of the model. It is therefore necessary to scale the features.","df33939a":"# <span id=\"4\"><\/span>**Creating a Model Evaluation Function and Adjusted $R^{2}$ Function**\n#### [Return Contents](#0)\n<hr\/>","3c60a83a":"Now let's create a linear regression model:","781d298d":"We created the model. Now let's evaluate our model.","cd42ed44":"Most machine learning algorithms operate with numbers. Therefore, it is necessary to convert categorical features into numbers. For this purpose, I convert categorical features to numbers using the pd.get_dummies() method.","f62168f8":"Now let's create a lasso regression model:","23a1cc8a":"# <span id=\"21\"><\/span>**AdaBoost**\n#### [Return Contents](#0)\n<hr\/>","84e35e55":"We created the model. Now let's evaluate our model.","fdb324a8":"# <span id=\"18\"><\/span>Support Vector Machine (SVM)\n#### [Return Contents](#0)\n<hr\/>","a11be679":"### Important Warning","98a2fe9f":"Let's train a AdaBoost Regressor:","c8ff36d2":"I created a copy of df so that my operations do not affect the actual data set:","4fb372fb":"We have 35 features. Since our new feature number for degree = 3 will be 8435 and this process will take a long time, I used the selected features instead of using all the features.","eee440b3":"# <span id=\"24\"><\/span>**Voting Regressor**\n#### [Return Contents](#0)\n<hr\/>","cba64e76":"Gradient Boosting Regressor with optimum n_estimators","b98548fc":"According to the table, the **\"Gradient Boosting Regressor with optimum n_estimators\"** model is the best model but looks a bit overfitting. It can be tried to solve the overfitting problem by giving more data to the model or by regularization the model.\n\n**Decision Tree Regressor** and **Random Forest Regressor** are severely overfitting in training set.So the generalization performance will be bad.Generalization performance can be improved by regularization models.\n\nAlso, the **Best Nonlinear SVM Regression** model seems to be a good model.\n\n# <center>Thank you for reading my kernel and If you liked it, please do not forget to <font color=\"blue\">UPVOTE <\/font>\ud83d\ude42 <\/center>","73e4b86c":"We created the linear regression model. Now let's evaluate our model.","73725ae2":"## <span id=\"14\"><\/span>Ridge Regression\n#### [Return Contents](#0)\n<hr\/>","a68855f8":"# <span id=\"1\"><\/span> **Overview**\n<hr\/>","c32cd35e":"Let's try nonlinear SVM regressor","0d031983":"## <span id=\"10\"><\/span>**Scaling**\n#### [Return Contents](#0)\n<hr\/>","93a4153e":"For the linear models, the main idea is to fit a straight line to our data.What if your data is more complex than a straight line? Surprisingly, you can use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called **Polynomial Regression**.\nWhile using polynomial transformation and deciding to degree, we should be very careful because it migh cause overfitting. ","7d8beb35":"## <span id=\"11\"><\/span> Splitting data into training set and test set\n#### [Return Contents](#0)\n<hr\/>","e269b068":"XGBoost also offers several nice features, such as automatically taking care of early stopping:","631ff671":"I've done some visualization and preprocessing in this section.","0bb56d97":"The $R^{2}$ increases when the number of features increase. Therefore, we may need a stronger evaluation metric to compare different models. We can use Adjusted $R^{2}$ for this purpose. Adjusted $R^{2}$ only increases if adding the feature decreases MSE. It is therefore a better metric than $R^{2}$.\n\nSo,I created a function to calculate the $R^{2}$ score, adjusted $R^{2}$ score, and Root Mean Squared Error in the model's training set and test set.","669a5a21":"Now let's create a linear svm regression model:","2f70c433":"Now let's create a Elastic Net regression model:","dd2723fa":"A Support Vector Machine (SVM) is a powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection.\n<br>\n<br>\nLinear SVM Regression tries to fit as many instances as possible on the street(represented by the parallel dashed lines) while limiting margin violations (i.e., instances off the street).The width of the street is controlled by a hyperparameter, \u03f5 (epsilon). ","607926b4":"### Note","c0a1fb1a":"I will train many models. So I created a dataframe to better see the model evaluation metrics. This dataframe includes **Root Mean Squared Error (RMSE)**, **R-squared**, **Adjusted R-squared**, **mean of the R-squared values obtained by the k-Fold Cross Validation** and **mean of the Root Mean Squared Error (RMSE) values obtained by the k-Fold Cross Validation**,which are the important metrics to compare different models.Usually having a R-squared value closer to one and smaller RMSE means a better fit.","6ac1611b":"A good way to reduce overfitting is to regularize the model.For a linear model, regularization is typically achieved by constraining the weights of the model.","54fa006e":"Previously, I tried various parameter values and found the most optimal values and then set the following values to obtain the optimum values I previously found.\n\nLet's try a few parameters for linear SVM regression with GridSearch and choose the best model:","f83ff702":"# <span id=\"19\"><\/span>**Decision Tree Regressor**\n#### [Return Contents](#0)\n<hr\/>","adc9516c":"Let's try a few parameters for Elastic Net regression with GridSearch and choose the best model:","544fca47":"### Important Warning","c0030326":"Decision Trees are versatile Machine Learning algorithms that can perform both classification and regression tasks, and even multioutput tasks. They are powerful algorithms, capable of fitting complex datasets.","f6860b0a":"I deleted the values I considered as outlier:","e9d7a4a4":"To explain Adaboost with an example: a predictor is trained in the training set, then predictions are made in the training set using this predictor, and more attention is paid to training instances where the predictor is undefitted. The weight of the training instances where the predictor is underfitting is increased. A new predictor is then trained in the training set with updated weights.This results in new predictors focusing more and more on the hard cases. This is the technique used by AdaBoost.","c463812d":"# <span id=\"5\"><\/span> **Reading the Dataset**\n#### [Return Contents](#0)\n<hr\/>","7883b2b1":"## <span id=\"16\"><\/span>Elastic Net\n#### [Return Contents](#0)\n<hr\/>","e121456c":"Before performing SVM Regression the training data should be scaled and centered.\nThis happens automatically because we are using StandartScaller.","f4ac7ac5":"# <span id=\"23\"><\/span>XGBoost (Extreme Gradient Boosting)\n#### [Return Contents](#0)\n<hr\/>","1fec8fb9":"## <span id=\"8\"><\/span>**Handling The Categorical Features**\n#### [Return Contents](#0)\n<hr\/>","631a12b9":"PolynomialFeatures(degree=d) transforms an array containing n features into an array containing $\\frac{(n+d)!}{d!n!}$ features, where n is the number of samples. \n<br>\nImplementation with scipy:","d9afde38":"Ridge Regression (also called Tikhonov regularization) is a regularized version of Linear Regression:a regularization term equal to $\\alpha\\sum_{i=i}^{n}\\theta_{i}^{2}$  is added to the cost function. \n<br>\nThe purpose of Ridge Regression is to keep model weights as small as possible.\n<br>\nThe hyperparameter $\\alpha$ controls how much you want to regularize the model:\n* if $\\alpha=0$,then Ridge Regression is just Linear Regression.  \n* if $\\alpha$  is very large, then all weights end up very close to zero.\n\n<b>Ridge Regression cost function<\/b>\n$$J(\\theta)=MSE(\\theta)+\\alpha\\frac{1}{2}\\sum_{i=1}^{n}\\theta_{i}^2$$","463c61c6":"We created the model. Now let's evaluate our model.","f9243068":"As you can see in the count plot on fuel type,let's delete the hybrid model because it is very few:","57c54246":"## <span id=\"15\"><\/span>Lasso Regression\n#### [Return Contents](#0)\n<hr\/>","e5846cc8":"Resources I used while writing this kernel:\n* https:\/\/www.kaggle.com\/burhanykiyakoglu\/predicting-house-prices\n* Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems 2nd Edition by Aurelien Geron","96ec04ca":"Now let's evaluate our model."}}