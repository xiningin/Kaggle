{"cell_type":{"9e67464f":"code","d0597576":"code","f46595c8":"code","46fc99b2":"code","36eb376d":"code","00871df6":"code","30851ceb":"code","a4b0fc36":"code","2881a19c":"code","f77e16b5":"code","d79afc9e":"code","fe929cb2":"code","8fe829fe":"code","78f80448":"code","0de97e54":"code","cb29ceeb":"code","031225e3":"code","a0f9a161":"code","ef2724df":"code","6e782a2c":"code","231bc49d":"code","7c95b9a2":"code","b3f046d1":"code","9da443e2":"code","34c63834":"code","a7072144":"code","b6faeffd":"code","372cbb3b":"code","97602aeb":"code","92f4cd7d":"code","882b1b1b":"code","56070b46":"code","882d26fe":"code","2ac046d4":"code","ccdc162e":"code","4a4d3f48":"code","f94da32b":"code","8347ddd4":"code","3fc7cb00":"code","09605397":"code","6b99db1d":"code","fcb3a1bb":"code","50b6c060":"code","4375d6fe":"code","05281a51":"code","50446f3d":"code","121ece29":"code","7ed61fb4":"code","56b6b2d7":"code","363a9810":"code","019eb282":"code","17e10b7c":"code","2ab992f3":"code","9a97b549":"code","49c3a935":"code","02b7c215":"code","d0247842":"code","9179e0c6":"code","c158d795":"code","db8cc2fe":"code","d211b51f":"code","2c3653bb":"code","48908bb1":"code","4e282e32":"code","4c89ce0b":"code","c3d38003":"code","155c3a8b":"code","61d34dfe":"code","17e77aec":"code","ce05b86e":"code","0a46e0dd":"code","9d4c4576":"code","8e60f385":"code","94957c70":"code","4bda210b":"code","8589e3e9":"code","aaf3c6a2":"code","da5e91c7":"code","2b626a83":"code","f29df089":"code","0bd89c4c":"code","15f01623":"code","3ee15162":"code","144be5a7":"code","2ce0adc2":"code","0c973faa":"code","9063a32f":"code","67ec235d":"code","f6868708":"code","eee6c73d":"code","8c9915fc":"code","ffc7bcbe":"code","baadc540":"code","2e38835c":"code","a5f92eb8":"code","59d0e72c":"markdown","50612e1d":"markdown","45dac14c":"markdown","55f6ee5f":"markdown","ba12001c":"markdown","23f6ff2f":"markdown","3a3c032e":"markdown","32352e62":"markdown","07b9e1ce":"markdown","698b74ef":"markdown","e887fd09":"markdown","a6ba8ff3":"markdown","f36e90ed":"markdown","d967d6b5":"markdown","930977ac":"markdown","2d7d89f9":"markdown","1036aaa3":"markdown","ca195036":"markdown","2c8a5c86":"markdown","f6abf243":"markdown","22b24658":"markdown","d2c594a7":"markdown","2655ec6c":"markdown","ffb9596f":"markdown","5d127962":"markdown","5fdcc886":"markdown","d3fe1c11":"markdown","08d51f21":"markdown","76a649bd":"markdown","28839bf7":"markdown","d8b3f352":"markdown","d00c7a31":"markdown","2ec4aa62":"markdown","7e0872f0":"markdown","164a4c64":"markdown","0a21b3f8":"markdown","ed2c2493":"markdown","ddcc7288":"markdown","caf614d9":"markdown","3367145d":"markdown","5f91a15e":"markdown","c28037a6":"markdown","f6f323af":"markdown","abf66d9f":"markdown","1ac04396":"markdown","1b616d09":"markdown","97679af4":"markdown","d2e2209f":"markdown","c2f7d453":"markdown","aeea2809":"markdown","6816c2fc":"markdown","9dc94a26":"markdown","8ac10a45":"markdown","513019e4":"markdown","54582148":"markdown","50dbc3d6":"markdown","6e9b3a09":"markdown","ffcb5963":"markdown","7091e1f3":"markdown","04728728":"markdown","203f1d71":"markdown","63226c7e":"markdown","6a18b6c3":"markdown","0b2ad4c9":"markdown","71301db5":"markdown","7302bd97":"markdown","fbefa3ce":"markdown","29f5c511":"markdown","c93a434b":"markdown","7b4155bc":"markdown","246ee6d4":"markdown","f1e574d3":"markdown"},"source":{"9e67464f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d0597576":"# Carregamento do Dataset:\ndf = pd.read_csv('\/kaggle\/input\/hmeq-data\/hmeq.csv')\ndf.shape","f46595c8":"# Visualizando os dados:\ndf.head()","46fc99b2":"# Verificando uma amostra aleat\u00f3ria\ndf.sample(5)","36eb376d":"# Verificando os tipos dos dados e os tamanhos\ndf.info()","00871df6":"# Verificando quantas observa\u00e7\u00f5es possuem esta caracter\u00edstica: \ndf[df['MORTDUE'].isnull() & df['VALUE'].isnull()]","30851ceb":"# Usando o comando drop para excluir os registros que possuem valores nulos nas 2 var\u00edaveis com dados \n#sobre o atual financiamento.\ndf.dropna(how='all', subset = ['MORTDUE', 'VALUE'], inplace=True)","a4b0fc36":"# conferindo se o comando de exclus\u00e3o deu certo: \ndf[df['MORTDUE'].isnull() & df['VALUE'].isnull()]","2881a19c":"# verificando a quantidade de casos enquadrados nesta situa\u00e7\u00e3o (nulo em MORTDUE ou VALUE:\ndf[df['MORTDUE'].isnull() | df['VALUE'].isnull()]","f77e16b5":"# imputando o valor da Propriedade pelo valor da d\u00edvida:\ndf['VALUE'] = df.apply(lambda row: row['MORTDUE'] if np.isnan(row['VALUE']) else row['VALUE'],axis=1)","d79afc9e":"# imputando o valor da d\u00edvida pelo valor da propriedade: \ndf['MORTDUE'] = df.apply(lambda row: row['VALUE'] if np.isnan(row['MORTDUE']) else row['MORTDUE'],axis=1)","fe929cb2":"# Conferindo que n\u00e3o existem mais registros com valores nulos nas colunas MORTDUE e VALUE:\ndf[df['MORTDUE'].isnull() | df['VALUE'].isnull()]","8fe829fe":"# Verificando os primeiros dados ap\u00f3s efeito da imputa\u00e7\u00e3o\ndf.head(10)","78f80448":"# Verificando como ficou o Dataset ap\u00f3s esta primeira imputa\u00e7\u00e3o:\ndf.info()","0de97e54":"# Verificando a quantidade de dados nulos:\ndf[df['REASON'].isnull()]","cb29ceeb":"# Verificando as classes da vari\u00e1vel 'REASON' e a frequ\u00eancia de cada um delas.\ndf['REASON'].value_counts()","031225e3":"# Verificando as propor\u00e7\u00e3o dos dados distribu\u00eddos entre as vari\u00e1veis 'REASON' e 'BAD':\ntotals=pd.crosstab(df['REASON'],df['BAD'],margins=True).reset_index()\npercentages = pd.crosstab(df['REASON'],\n   df['BAD']).apply(lambda row: row\/row.sum(),axis=1).reset_index()\ntotals\n\n","a0f9a161":"# Verificando os percentuais cruzados entre 'REASON' e 'BAD':\npercentages","ef2724df":"# Realizando a imputa\u00e7\u00e3o da vari\u00e1vel 'REASON':\ndf['REASON'].fillna('DebtCon', inplace=True)\n","6e782a2c":"# Verificando como ficou a distribui\u00e7\u00e3o de frequ\u00eancias da vari\u00e1vel 'REASON':\ndf['REASON'].value_counts()","231bc49d":"# Verificando como ficou o Dataset:\ndf.info()","7c95b9a2":"# Verificando as classes e frequ\u00eancia da vari\u00e1vel 'JOB':\ndf['JOB'].value_counts()","b3f046d1":"# Verificando os registros nulos na vari\u00e1vel 'JOB':\ndf[df['JOB'].isnull()]","9da443e2":"# Imputa\u00e7\u00e3o dos valores nulos da vari\u00e1vel 'JOB'pelo valor da classe 'Other':\ndf['JOB'].fillna('Other', inplace=True)","34c63834":"# Verificando como ficou a distribui\u00e7\u00e3o das classes da vari\u00e1vel 'JOB':\ndf['JOB'].value_counts()","a7072144":"#Verificando como ficou o Dataset:\ndf.info()","b6faeffd":"# Verificando a distribui\u00e7\u00e3o da vari\u00e1vel:\ndf['YOJ'].value_counts()","372cbb3b":"# Verificando os dados nulos da vari\u00e1vel 'YOJ':\ndf[df['YOJ'].isnull()]","97602aeb":"# Verificando o resumo de medidas est\u00edsiticas da vari\u00e1vel 'YOJ':\ndf['YOJ'].describe()","92f4cd7d":"# Verificando uma medida estat\u00edstica extra, a mediana:\ndf['YOJ'].median()","882b1b1b":"# Verificando a distribui\u00e7\u00e3o dos dados por meio do Histograma:\ndf['YOJ'].plot.hist(bins=50)","56070b46":"# Efetuando a imputa\u00e7\u00e3o pela Mediana: \ndf['YOJ'].fillna(7, inplace=True)","882d26fe":"# Verificando o dataset:\ndf.info()","2ac046d4":"# Verificando as classes e distribui\u00e7\u00e3o da 'DEROG':\ndf['DEROG'].value_counts()","ccdc162e":"# Verificando a quantidade de registros com dados missing na vari\u00e1vel 'DEROG': \ndf[df['DEROG'].isnull()]","4a4d3f48":"# Verificando as classes e distribui\u00e7\u00e3o da 'DELINQ':\ndf['DELINQ'].value_counts()","f94da32b":"# Verificando a quantidade de registros com dados missing na vari\u00e1vel 'DELINQ': \ndf[df['DELINQ'].isnull()]","8347ddd4":"# Verificando as classes e frequ\u00eancia da vari\u00e1vel 'CLAGE':\ndf['CLAGE'].value_counts()","3fc7cb00":"df[df['CLAGE'].isnull()]","09605397":"# Verificando o resumo de medidas est\u00edsiticas da vari\u00e1vel 'CLAGE':\ndf['CLAGE'].describe()","6b99db1d":"# Verificando a Mediana:\ndf['CLAGE'].median()","fcb3a1bb":"# Verificando a distribui\u00e7\u00e3o dos dados por meio do Histograma:\ndf['CLAGE'].plot.hist(bins=50)","50b6c060":"# Verificando as classes e distribui\u00e7\u00e3o entre estas para a vari\u00e1vel 'NINQ': \ndf['NINQ'].value_counts()","4375d6fe":"# Verificando os dados nulos para a vari\u00e1vel 'NINQ':\ndf[df['NINQ'].isnull()]","05281a51":"# Sum\u00e1rio estat\u00edstico para 'NINQ':\ndf['NINQ'].describe()","50446f3d":"# Verificando a Mediana:\ndf['NINQ'].median()","121ece29":"# Verificando o Histograma:\ndf['CLAGE'].plot.hist(bins=50)","7ed61fb4":"# Verificando classes e frequ\u00eancia para vari\u00e1vel 'CLNO':\ndf['CLNO'].value_counts()","56b6b2d7":"# Verificando os valores Nulos: \ndf[df['CLNO'].isnull()]","363a9810":"# Verificando as medidas estat\u00edsticas:\ndf['CLNO'].describe()","019eb282":"# Verificando a Mediana:\ndf['CLNO'].median()","17e10b7c":"# Verificando o Histograma:\ndf['CLNO'].plot.hist(bins=50)","2ab992f3":"# Verificando as classes e distribui\u00e7\u00e3o da vari\u00e1vel 'DEBTINC':\ndf['DEBTINC'].value_counts()","9a97b549":"# Verificando os valores Nulos: \ndf[df['DEBTINC'].isnull()]","49c3a935":"# Verificando o resumo estat\u00edstico:\ndf['DEBTINC'].describe()","02b7c215":"# Verificando a Mediana: \ndf['DEBTINC'].median()","d0247842":"# Verificando o histograma: \ndf['DEBTINC'].plot.hist(bins=50)","9179e0c6":"# Imputa\u00e7\u00e3o dos dados nulos:\ndf['DEROG'].fillna(0, inplace=True)\ndf['DELINQ'].fillna(0, inplace=True)\ndf['CLAGE'].fillna(173.48, inplace=True)\ndf['NINQ'].fillna(1, inplace=True)\ndf['CLNO'].fillna(20, inplace=True)\ndf['DEBTINC'].fillna(33.79, inplace=True)","c158d795":"# Verificando o dataset ap\u00f3s todo o tratamento de Dados 'Missing's':\ndf.info()","db8cc2fe":"# Importando \nimport seaborn as sns\n","d211b51f":"# Verificando inicialmente a Tabela Cruzada\n\ny = df['BAD'].astype(object) \ncount = pd.crosstab(index = y, columns=\"count\")\npercentage = pd.crosstab(index = y, columns=\"frequency\")\/pd.crosstab(index = y, columns=\"frequency\").sum()\npd.concat([count, percentage], axis=1)","2c3653bb":"# Plotando o gr\u00e1fico da Frequ\u00eancia da Vari\u00e1vel Resposta 'BAD'\nax = sns.countplot(x=y, data=df).set_title(\"Distribui\u00e7\u00e3o da Vari\u00e1vel Resposta 'BAD'\")","48908bb1":"# Plotando o Gr\u00e1fico de Barras Empilhadas mostrando a rela\u00e7\u00e3o entre a vari\u00e1vel 'BAD' e \"JOB\":\nJOB=pd.crosstab(df['JOB'],df['BAD'])\nJOB.div(JOB.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, title='JOB x BAD', figsize=(4,4))","4e282e32":"REASON=pd.crosstab(df['REASON'],df['BAD'])\nREASON.div(REASON.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, title='REASON x BAD', figsize=(4,4))","4c89ce0b":"sns.stripplot(x='BAD', y='MORTDUE', data=df, linewidth=1)","c3d38003":"sns.stripplot(x='BAD', y='DEBTINC', data=df, linewidth=1)","155c3a8b":"# Verificando as vari\u00e1veis do dataset:\ndf.info()","61d34dfe":"# Transformando as vari\u00e1veis 'REASON' e 'JOB' em 'dummies':\ndf = pd.get_dummies(df, columns=['REASON', 'JOB'])","17e77aec":"# Verificando como ficou o dataset ap\u00f3s o processo de transforma\u00e7\u00f5es em 'dummies':\ndf.info()","ce05b86e":"# Visualizando o dataset ap\u00f3s o processo de 'dummies':\ndf.head().T","0a46e0dd":"df.corr()","9d4c4576":"# Importando o pacote matplotlib\nimport matplotlib.pyplot as plt","8e60f385":"#Create Correlation matrix\ncorr = df.corr()\n#Plot figsize\nfig, ax = plt.subplots(figsize=(10,8))\n#Generate Color Map\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\n#Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\")\n#Apply xticks\nplt.xticks(range(len(corr.columns)), corr.columns);\n#Apply yticks\nplt.yticks(range(len(corr.columns)), corr.columns)\n#show plot\nplt.show()","94957c70":"# Importando o pacote do Sklearn:\nfrom sklearn.model_selection import train_test_split","4bda210b":"# Separando os dados de Treino, Valida\u00e7\u00e3o e Teste, usando a propor\u00e7\u00e3o 80\/20:\ntrain, test = train_test_split(df, test_size=0.20, random_state=42)\ntrain, valid = train_test_split(train, test_size=0.20, random_state=42)\ntrain.shape, valid.shape, test.shape","8589e3e9":"# Definindo colunas de entrada. Excluiremos a Vari\u00e1vel Resposta e as duas com colinearidade:\nfeats = [c for c in df.columns if c not in ['BAD', 'VALUE', 'REASON_HomeImp']]\nfeats","aaf3c6a2":"# Importando o pacote RandomForestClassifier necess\u00e1rio para rodar o modelo:\n\nfrom sklearn.ensemble import RandomForestClassifier","da5e91c7":"# Instanciando o modelo com 200 \u00e1rvores de decis\u00e3o\nrf = RandomForestClassifier(n_estimators=200, random_state=42) ","2b626a83":"# Treinando o Modelo: \nrf.fit(train[feats], train['BAD'])","f29df089":"# Fazendo as previs\u00f5es para os dados de Valida\u00e7\u00e3o:\npreds_val= rf.predict(valid[feats])\npreds_val","0bd89c4c":"# Importando o pacote necess\u00e1rio para verificarmos a acur\u00e1cio do modelo\n\nfrom sklearn.metrics import accuracy_score","15f01623":"# Verificando a predi\u00e7\u00e3o nos dados de Valida\u00e7\u00e3o:\naccuracy_score(valid['BAD'], preds_val)","3ee15162":"# Verificando a acur\u00e1cia do modelo nos dados de teste:\npreds_test = rf.predict(test[feats])\naccuracy_score(test['BAD'], preds_test)","144be5a7":"# avaliando a import\u00e2ncia de cada coluna (cada vari\u00e1vel de entrada)\n\npd.Series(rf.feature_importances_, index=feats).sort_values().plot.barh()","2ce0adc2":"#importando a biblioteca necess\u00e1ria para plotar o gr\u00e1fico de Matriz de Confus\u00e3o\n\nimport scikitplot as skplt","0c973faa":"# Gerando a Matriz de Confus\u00e3o \nskplt.metrics.plot_confusion_matrix(valid['BAD'], preds_val)","9063a32f":"# Fazendo uma c\u00f3pia do Dataset para a aplica\u00e7\u00e3o do outro modelo. Chamaremos de df1: \ndf1 = df.copy()","67ec235d":"# Separando o dataframe em dados de treino e Teste. N\u00e3o ser\u00e1 apartado dados para valida\u00e7\u00e3o visto que usaremos\n# a valida\u00e7\u00e3o cruzada onde faremos v\u00e1rias valida\u00e7\u00f5es com dados aleat\u00f3rios do dataset de treino.\n\n# Importando o train_test_split\nfrom sklearn.model_selection import train_test_split\n\n# Separando treino e teste\ntrain, test = train_test_split(df1, test_size=0.20, random_state=42)\n\n# N\u00e3o vamos mais usar o dataset de valida\u00e7\u00e3o\n\ntrain.shape, test.shape","f6868708":"# definindo colunas de entrada\n\nfeats = [c for c in df1.columns if c not in ['BAD', 'VALUE', 'REASON_HomeImp']]","eee6c73d":"# Importar o modelo\nfrom xgboost import XGBClassifier\n\n# Instanciar o modelo\nxgb = XGBClassifier(n_estimators=200, n_jobs=-1, random_state=42, learning_rate=0.05)","8c9915fc":"# Usando o Cross validation\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(xgb, train[feats], train['BAD'], n_jobs=-1, cv=5) # estimator= xgb\n\n# Definiremos 5 splits para realizar a valida\u00e7\u00e3o cruzada:\nscores, scores.mean() ","ffc7bcbe":"# Usando o XGB para treinamento e predi\u00e7\u00e3o \n\nxgb.fit(train[feats], train['BAD'])","baadc540":"# Fazendo predi\u00e7\u00f5es\npreds = xgb.predict(test[feats])","2e38835c":"# Medir o desempenho do modelo\nfrom sklearn.metrics import accuracy_score\n\naccuracy_score(test['BAD'], preds)","a5f92eb8":"pd.Series(xgb.feature_importances_, index=feats).sort_values().plot.barh()","59d0e72c":"O array acima mostra a acur\u00e1cia obtida em cada split de valida\u00e7\u00e3o. O \u00faltimo valor \u00e9 a m\u00e9dia encontrada: 0,912","50612e1d":"Com o algoritmo XGBoost percebemos diferen\u00e7as quanto \u00e0 import\u00e2ncia de cada uma das vari\u00e1veis para a predi\u00e7\u00e3o realizada. Verifica-se que a vari\u00e1vel que teve mais peso no modelo tamb\u00e9m foi a DEBTINC, por\u00e9m no XGBoost esta vem seguida DELINQ e DEROG.","45dac14c":"VARI\u00c1VEIS MAIS IMPORTANTES PARA O MODELO","55f6ee5f":"APURANDO A ACUR\u00c1CIA NOS DADOS DE TESTE:","ba12001c":"A classe 'Other' possui a maioria dos registros v\u00e1lidos.","23f6ff2f":"'BAD' versus 'JOB'\n\n","3a3c032e":"Este gr\u00e1fico relaciona a vari\u00e1vel 'BAD' com a 'DEBTINC'. A vari\u00e1vel 'DEBTINC recebe a raz\u00e3o entre o valor dos d\u00e9bitos e os rendimentos do financiado, ou seja, quanto maior o seu valor, maior \u00e9 endividamento do cliente. Graficamente percebe-se que nenhum clientes na classe que dos que conseguiram pagar seu empr\u00e9stimo possuem DEBTINC maior que 50. Por outro lado, \u00e9 grande a quantidade de inadimplentes com valores elevados nesta vari\u00e1vel. Portanto, provavelmente esta vari\u00e1vel ter\u00e1 grande impacto no modelo de predi\u00e7\u00e3o. ","32352e62":"MATRIZ DE CONFUS\u00c3O","07b9e1ce":"Visto a classe 'Other' possuir a maior parte dos registros, optou-se pela imputa\u00e7\u00e3o dos valores 'missing' pelo valor desta classe.","698b74ef":"A fim de conseguirmos previs\u00f5es acerca da possibilidade de 'default' ou n\u00e3o a partir dos dados do dataset, vamos utilizar dois algoritmos diferentes de \u00e1rvores de decis\u00e3o: Random Forest e o XGBoost, ambos modelos de Machine Learning. ","e887fd09":"Este gr\u00e1fico demonstra-se a import\u00e2ncia de cada uma das vari\u00e1veis para a predi\u00e7\u00e3o realizada. Verifica-se que a vari\u00e1vel que teve mais peso no modelo foi a DEBTINC, seguida de MORTDUE, DELINQ e LOAN.","a6ba8ff3":"Aqui se verifica que quase 10% da base de dados encontra-se nesta situa\u00e7\u00e3o (576 registros).","f36e90ed":"VARI\u00c1VEL 'JOB'","d967d6b5":"VARI\u00c1VEIS 'DEROG', 'DELINQ, 'CLAGE', 'NINQ', 'CLNO', DEBTINC':","930977ac":"## Por aluna matr\u00edcula 1931133087 ","2d7d89f9":"# 5.CONCLUS\u00c3O","1036aaa3":"Para NINQ usaremos a mediana, visto os dados estarem fortemente distribuidos \u00e0 esquerda da curva.","ca195036":"# 3. Verifica\u00e7\u00e3o das Correla\u00e7\u00f5es entre as vari\u00e1veis","2c8a5c86":"### 2.1.2 Imputa\u00e7\u00e3o de dados faltantes\n\n","f6abf243":"#    Previs\u00e3o de *Default* em Financiamento Banc\u00e1rio","22b24658":"Aqui verifica-se que a maior frequ\u00eancia \u00e9 da classe 'DebtCon'.","d2c594a7":"Para CLNO usaremos a mediana. Concentra\u00e7\u00e3o maior \u00e0 esquerda.","2655ec6c":"Tendo em vista a distribui\u00e7\u00e3o dos dados, para CLAGE usaremos a imputa\u00e7\u00e3o pela mediana","ffb9596f":"'BAD' versus 'REASON': \n\n","5d127962":"### 2.2.2. Verificando graficamente a rela\u00e7\u00e3o da vari\u00e1vel resposta 'BAD' e algumas vari\u00e1veis explicativas","5fdcc886":"Previamente \u00e0 aplica\u00e7\u00e3o dos modelos de Machine Learning escolhidos para a predi\u00e7\u00e3o, \u00e9 importante verificar a colinearidade entre as vari\u00e1veis explicativas, para que, com a devida exclus\u00e3o das vari\u00e1veis correlacionadas, podermos evitar o overfitting do modelo. \u00c9 o que ser\u00e1 realizado neste t\u00f3pico. ","d3fe1c11":"A base de dados conta com duas vari\u00e1veis do tipo 'object' e que dever\u00e3o ser tratadas para que se possa utiliz\u00e1-las no modelo de predi\u00e7\u00e3o: \"REASON\" e \"JOB\". \nA op\u00e7\u00e3o escolhida foi a transforma\u00e7\u00e3o em 'dummies, conforme \u00e9 efetuado a seguir:","08d51f21":"Conforme verifica-se acima, o dataset final, tratado e pronto para ser utilizado no modelo, contar\u00e1 com 19 vari\u00e1veis, todas num\u00e9ricas. ","76a649bd":"Analisando o Gr\u00e1fico da Matriz de Correla\u00e7\u00e3o, verifica-se que as Vari\u00e1veis 'MORTDUE' e 'VALUE' possuem forte correla\u00e7\u00e3o positiva (0,89). Para evitar distor\u00e7oes no modelo, optou-se pela exclus\u00e3o de uma delas: a 'VALUE'.\n\nDa mesma forma, ser\u00e1 exclu\u00edda da an\u00e1lise da vari\u00e1vel dummificada REASON_HomeImp.","28839bf7":"Verifica-se 267 registros com dado faltante na vari\u00e1vel 'JOB'.","d8b3f352":"Comparando o modelo de Random Forest e o XGBoost para os dados estudados, verifica-se que o primeiro possui leve vantagem em termos de acur\u00e1cia: 0,9157 contra 0,9115","d00c7a31":"Constata-se 505 registros com dados nulos na vari\u00e1vel 'YOJ';\nVisto a vari\u00e1vel ser cont\u00ednua, verificou-se a exist\u00eancia de v\u00e1rios dom\u00ednios, sem predomin\u00e2ncia de um deles. Desta forma partiu-se para a an\u00e1lise do seu resumo estat\u00edstico e de sua distribui\u00e7\u00e3o:","2ec4aa62":"Como apoio \u00e0 decis\u00e3o sobre imputar os dados missing da vari\u00e1vel 'REASON' pelo valor da classe com maior frequ\u00eancia, decidiu-se verificar em uma tabela cruzada se as classes desta vari\u00e1vel apresentam grande diferen\u00e7a de propor\u00e7\u00e3o na vari\u00e1vel resposta. Um despropor\u00e7\u00e3o indicaria tentar outro m\u00e9todo para imputa\u00e7\u00e3o visto a possibilidade de distor\u00e7\u00e3o na vari\u00e1vel resposta.  ","7e0872f0":"Verifica-se 242 com dado nulo na vari\u00e1vel 'REASON'.","164a4c64":"## 2.1. Tratamento dos dados Faltantes","0a21b3f8":"A base de dados ficou com 27 registros a menos, com nenhum valor nulo nas quatro primeiras colunas.","ed2c2493":"Finalmente, ap\u00f3s o t\u00e9rmino da an\u00e1lise das seis \u00faltimas vari\u00e1veis, procederemos a imputa\u00e7\u00e3o conforme a seguir: \nDEROG: por zero;\nDELINQ: por zero;\nCLAGE: pela Mediana;\nNINQ: pela Mediana;\nCLNO: pela Mediana:\nDEBTINC: pela M\u00e9dia.","ddcc7288":"Nesta visualiza\u00e7\u00e3o, verifica-se grande quantidade de colunas com dados 'missing'. Ali\u00e1s, somente a coluna com a vari\u00e1vel resposta 'BAD e a 'VALUE' n\u00e3o demandam tratamento.\nAl\u00e9m disso, h\u00e1 duas vari\u00e1veis do tipo 'object', ou seja, n\u00e3o num\u00e9ricas. \nLogo, o tratamento dos dados se destinar\u00e1 a solucionar estas duas situa\u00e7\u00f5es para que seja vi\u00e1vel a aplica\u00e7\u00e3o do modelo escolhido.","caf614d9":"## 2.2. AN\u00c1LISE EXPLORAT\u00d3RIA DAS VARI\u00c1VEIS EXPLICATIVAS E SUAS RELA\u00c7\u00d5ES COM A VARI\u00c1VEL RESPOSTA 'BAD'","3367145d":"Visto os dados n\u00e3o estarem muito dispersos, para a vari\u00e1vel DEBTINC usaremos a m\u00e9dia, como crit\u00e9rio para a imputa\u00e7\u00e3o dos dados faltantes.","5f91a15e":"Ser\u00e3o exclu\u00eddos os 27 registros que possuem esta caracteristica:","c28037a6":"### 2.2.1. Verificando graficamente a Distribui\u00e7\u00e3o da Vari\u00e1vel Resposta 'BAD': ","f6f323af":"A vari\u00e1vel DEBTINC \u00e9 cont\u00ednua sem classes predominante.","abf66d9f":"VARI\u00c1VEL 'REASON'","1ac04396":"O presente trabalho utiliza o dataset Home Equity DAtaset(HMEQ) dispon\u00edvel no Kaggle para, mediante modelo de Machine Learning, prever o Default ou n\u00e3o do financiamento banc\u00e1rio contratado pelo cliente.\n\nO Dataset HMEQ apresenta informa\u00e7\u00f5es de 5960 contratantes e de seus financiamentos banc\u00e1rios distribu\u00eddas nas 12 vari\u00e1veis abaixo e tamb\u00e9m a vari\u00e1vel resposta \"BAD\" que informa se o cliente deixou de honrar seu empr\u00e9stimo (default) ou n\u00e3o.\n\nAs vari\u00e1veis preditoras constantes da base de dados s\u00e3o:\n\n* LOAN: Valor da parcela (valor $);\n\n* MORTDUE: Valor devido na hipoteca existente (valor $);\n\n* VALUE: Valor corrente da propriedade financiada (valor $);\n\n* REASON: Destina\u00e7\u00e3o do cr\u00e9dito (se debt consolidatio= DebtCon ou home improvement= HomeImp;\n\n* JOB: Ocupa\u00e7\u00e3o. Possui sex classes: Other, ProfExe, Office, Mgr, Self e Sales;\n\n* YOJ: Anos no emprego atual (em anos);\n\n* DEROG: N\u00famero de relat\u00f3rios depreciativos;\n\n* DELINQ: N\u00famero de linhas de cr\u00e9dito inadimplentes;\n\n* CLAGE: Idade da linha de cr\u00e9dito mais antiga;\n\n* NINQ: N\u00famero de linhas de cr\u00e9dito recentes;\n\n* CLNO: N\u00famero de linhas de cr\u00e9dito;\n\n* DEBTIC : Raz\u00e3o D\u00edvida\/Rendimento.\n\nA vari\u00e1vel resposta BAD \u00e9 bin\u00e1ria e apresenta as seguintes classes: 0 - o cliente n\u00e3o ficou inadimplente e 1 - o cliente apresentou Default. Logo, \u00e9 esta vari\u00e1vel que conter\u00e1 as previs\u00f5es realizadas.\n","1b616d09":"## 4.1. Aplica\u00e7\u00e3o do Modelo de Random Forest","97679af4":"O gr\u00e1fico acima mostra a rela\u00e7\u00e3o entre a inadimpl\u00eancia e a finalidade do cr\u00e9dito. Verifica-se que a inadimpl\u00eancia \u00e9 ligeiramente menor para a finalidade \"DebtConf\".\n","d2e2209f":"Pode-se perceber que o \u00edndice de inadimpl\u00eancia ('default') \u00e9 de 19,70%","c2f7d453":"### 2.1.1. Exclus\u00e3o de observa\u00e7\u00f5es com informa\u00e7\u00f5es sobre o financiamento nulas.\n\nEstudando os dados, entendeu-se que as observa\u00e7\u00f5es onde as vari\u00e1veis 'MORTDUE' (valor devido) e 'VALUE' (valor da propriedade) est\u00e3o com dados nulos (em conjunto), devem ser exclu\u00eddas pois n\u00e3o agregam informa\u00e7\u00f5es relevantes para o estudo de predi\u00e7\u00e3o, podendo at\u00e9 distorcer, caso haja imputa\u00e7\u00e3o.\n\nDesta forma, inicialmente ser\u00e1 feito a verifica\u00e7\u00e3o das observa\u00e7\u00f5es com esta caracter\u00edstica e ap\u00f3s, sua exclus\u00e3o.","aeea2809":"## 2.3. Tranformando as vari\u00e1veis categ\u00f3ricas em 'dummies'","6816c2fc":"# 4. APLICA\u00c7\u00c3O DO MODELO\n","9dc94a26":"A an\u00e1lise da tabela cruzada mostrou pouco desbalanceamento entre classes da vari\u00e1vel resposta (0,18 e 0,21), ent\u00e3o optou-se pela imputa\u00e7\u00e3o usando a classe de maior frequ\u00eancia, 'DebtCon', para o tratamento dos dados missing.","8ac10a45":"Considerando os dados estarem mais distribu\u00eddos \u00e0 esquerda, usaremos a Mediana ao inv\u00e9s da M\u00e9dia para a imputa\u00e7\u00e3o dos valores missing da coluna \"YOJ\". ","513019e4":"O gr\u00e1fico acima mostra a rela\u00e7\u00e3o entre a inadimpl\u00eancia e o valor do financiamento. Pode-se constatar que com a exce\u00e7\u00e3o de 4 outlies com valores elevados de d\u00edvida, n\u00e3o existe grande diferen\u00e7a no valor m\u00e9dio do cr\u00e9dito do cliente insolvente para aquele que conseguiu honr\u00e1-lo;\n\n","54582148":"Tendo em vista a quase totalidade dos registros estarem com valor zero para a vari\u00e1vel 'DEROG' usaremos a imputa\u00e7\u00e3o por zero.","50dbc3d6":"## 4.2. Aplica\u00e7\u00e3o do Modelo XGBoost com Cross Validation","6e9b3a09":"VARI\u00c1VEL 'YOJ'","ffcb5963":"Com a realiza\u00e7\u00e3o do presente estudo, verificou-se que, ap\u00f3s a an\u00e1lise e tratamento realizados nos dados originais e aplica\u00e7\u00e3o do modelo de Machine Learnig Random Forest (com os par\u00e2metros utilizados acima) foi poss\u00edvel encontrar uma acur\u00e1cia na predi\u00e7\u00e3o nos dados de teste de 0,9154, ligeiramente melhor do que o modelo XGBoost usando com valida\u00e7\u00e3o cruzada. Desta forma, para os dados estudados, o modelo escolhido foi o Random Forest.","7091e1f3":"Encontramos uma acur\u00e1cia de 0,93 nos dados de valida\u00e7\u00e3o.","04728728":"# 1. Carregamento da Base de Dados HMEQ","203f1d71":"Tendo em vista, assim como a 'DEROG, que a quase totalidade dos registros est\u00e3o com valor zero para a vari\u00e1vel 'DELINQ' usaremos a imputa\u00e7\u00e3o por zero","63226c7e":"VARI\u00c1VEIS MAIS IMPORTANTES PARA O MODELO XGBoost","6a18b6c3":"# 2. Tratamento dos Dados","0b2ad4c9":"APLICA\u00c7\u00c3O DO XGBoost: \u00c1rvores encadeadas","71301db5":"#### VARI\u00c1VEIS MORTDUE E VALUE\nAinda existem muitos casos de valores nulos nas colunas 'MORTDUE' e 'VALUE'. Para imputar os valores destas colunas adotaremos o seguinte:\nCaso de nulo no valor da propriedade (VALUE): Ser\u00e1 imputado o valor da d\u00edvida atual (MORTDUE), garantindo assim que o valor da propriedade ser\u00e1 ao menos o valor da d\u00edvida.\nCaso de nulo no valor da d\u00edvida autal (MORTDUE): Ser\u00e1 imputado o valor da propriedade garantidora da opera\u00e7\u00e3o, considerando assim o valor da propriedade como valor estimado do financiamento.","7302bd97":"Na diagonal principal da Matriz de Confus\u00e3o encontram-se as predi\u00e7\u00f5es corretas realizadas pelo modelo aplicado. Verifica-se 760 acertos de n\u00e3o evento, ou seja, aqueles onde o modelo acertou que n\u00e3o haveria o Default e 126 acertos de evento, quando o modelo acertou a incorr\u00eancia do default. ","fbefa3ce":"A Acur\u00e1cia encontrada nos dados de teste foi de 0,915754.","29f5c511":"A Base de Dados possui 5960 observa\u00e7\u00f5es e 13 colunas","c93a434b":"'BAD'versus 'DEBTINC':","7b4155bc":"No gr\u00e1fico acima \u00e9 confrontada a inadimpl\u00eancia (vari\u00e1vel 'BAD') com o tipo de ocupa\u00e7\u00e3o ('JOB'). Verifica-se que o segmento voltado \u00e0 Vendas (Sales) possui o maior \u00edndice de inadimpl\u00eancia, cerca de 30%, sendo que o menor \u00edndice de default foi da classe 'Oficce'.","246ee6d4":"'BAD' versus 'MORTDUE'\n\n","f1e574d3":"Aqui tamb\u00e9m trata-se de vari\u00e1vel cont\u00ednua sem predomin\u00e2ncia de classe. \u00c9 necess\u00e1ria a an\u00e1lise de suas medidas estat\u00edsticas para a tomada de decis\u00e3o quanto a imputa\u00e7\u00e3o de dados nulos."}}