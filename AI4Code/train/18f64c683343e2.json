{"cell_type":{"dc1960d6":"code","d08c36b3":"code","1033edb8":"code","b8988138":"code","b1ab0368":"markdown","25f20cb7":"markdown","47b357c3":"markdown"},"source":{"dc1960d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d08c36b3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndata = pd.read_csv('..\/input\/Iris.csv')\nprint(data.columns)","1033edb8":"feature_names = [ 'PetalLengthCm', 'PetalWidthCm','SepalLengthCm', 'SepalWidthCm']\nX =data[feature_names]\ny=data['Species']\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nprint('Accuracy of Logistic regression classifier on training set: {:.2f}'\n     .format(logreg.score(X_train, y_train)))\nprint('Accuracy of Logistic regression classifier on test set: {:.2f}'\n     .format(logreg.score(X_test, y_test)))","b8988138":"from sklearn.neighbors import KNeighborsClassifier\n\nimport matplotlib.cm as cm\nfrom matplotlib.colors import ListedColormap, BoundaryNorm\nimport matplotlib.patches as mpatches\nimport matplotlib.patches as mpatches\ndf = pd.DataFrame(data, columns = ['PetalLengthCm', 'PetalWidthCm','SepalLengthCm', 'SepalWidthCm','Species'])\ndef score_to_numeric(x):\n    if x=='Iris-virginica':\n        return 3\n    if x=='Iris-setosa':\n        return 2\n    if x=='Iris-versicolor':\n        return 1\ndf['score_num'] = df['Species'].apply(score_to_numeric)   \n#df['score_num']\n\nfeature_names = [ 'PetalLengthCm', 'PetalWidthCm','SepalLengthCm', 'SepalWidthCm']\nX =data[feature_names]\ny=df['score_num']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\ndef plot_fruit_knn(X, y, n_neighbors):\n    X_mat = X[['PetalLengthCm', 'PetalWidthCm']].as_matrix()\n    y_mat = y.as_matrix()\n# Create color maps\n    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n    cmap_bold  = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n    clf = KNeighborsClassifier()\n    clf.fit(X_mat, y_mat)       \n    \n# Plot the decision boundary by assigning a color in the color map\n    # to each mesh point.\n    \n    mesh_step_size = .01  # step size in the mesh\n    plot_symbol_size = 30\n    \n    x_min, x_max = X_mat[:, 0].min() - 1, X_mat[:, 0].max() + 1\n    y_min, y_max = X_mat[:, 1].min() - 1, X_mat[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, mesh_step_size),\n                         np.arange(y_min, y_max, mesh_step_size))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n# Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n# Plot training points\n    plt.scatter(X_mat[:, 0], X_mat[:, 1],s=plot_symbol_size, c=y,  cmap=cmap_bold, edgecolor = 'black')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    patch0 = mpatches.Patch(color='#FF0000', label='Iris-versicolor')\n    patch1 = mpatches.Patch(color='#00FF00', label='Iris-setosa')\n    patch2 = mpatches.Patch(color='#0000FF', label='Iris-verginica')\n    plt.legend(handles=[patch0, patch1, patch2])\n    plt.xlabel('PetalLengthCm')\n    plt.ylabel('PetalWidthCm')\n    plt.title(\"3-Class classification \")\n    plt.subplots_adjust(bottom=0.1, right=1.9, top=1.5) \n    plt.show()\n\nplot_fruit_knn(X_train, y_train, n_neighbors=9)\n    \n    \n    ","b1ab0368":"Reference:\nSolving A Simple Classification Problem with Python\u200a\u2014\u200aFruits Lovers\u2019 Edition by \n**Susan Li**\nChanging the world, one article at a time. Sr. Data Scientist, Toronto Canada. Opinion=my own. \nDec 4, 2017\nhttps:\/\/towardsdatascience.com\/solving-a-simple-classification-problem-with-python-fruits-lovers-edition-d20ab6b071d2","25f20cb7":"**Using logistic regression as the model**","47b357c3":"**Plot and decision boundary of the classification**\nSteps involved\n\n1.Convert the categorical variables of the 3 classes in the database Iris-versicolor,Iris-verginica and Iris-setosa,into numeric.\n\n2.Split data into test and train.\n\n3.Knn classifier is used as ml algorithm.\n\n4.Visualizations are done using 2 selected features only-'PetalLengthCm', 'PetalWidthCm'\nand decision boundaries of classes  are plot.\n"}}