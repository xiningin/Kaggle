{"cell_type":{"6939207e":"code","f87bd76e":"code","ca3c26b4":"code","31c1e2e3":"code","a757a375":"code","376c20a5":"code","b40b4a71":"code","99ae26d8":"code","35dec52f":"code","55811cf9":"code","4fa5ac6b":"code","a93f052d":"code","e3c3cd45":"code","e2480676":"code","04ddada9":"code","1ceffca3":"code","d7e61235":"code","55a51424":"code","b34bef18":"code","8b53a131":"code","ac530bbe":"code","21232148":"code","01854a6d":"code","3365a3ef":"code","be99bf2d":"code","5f328b11":"code","1cc5916a":"code","ad74b65b":"code","6990a0ca":"code","4ade3b97":"code","692f63a9":"code","bb25b56b":"code","7477b153":"code","a1d0ad1c":"code","d0d56ca3":"code","ca870cc4":"code","2f3acc35":"code","a58fc65e":"code","e197d7c6":"code","51ddc7e0":"code","d28f6aa4":"code","526f0212":"code","56830626":"code","6028cf44":"code","59b242dc":"code","cb12b807":"code","761839b4":"code","91574b38":"code","cbd73f91":"code","5f613f61":"code","65b28d99":"code","a564febd":"code","84816643":"code","16f62c30":"code","a5aebd7c":"code","4538d9e2":"code","d73d78ae":"code","b12c7989":"code","2c714f81":"code","4c328a4d":"code","868f9457":"code","1e8a9f80":"code","4362194d":"code","f38dba50":"code","e34fbee5":"code","2e591704":"code","0b1507db":"code","47f56926":"code","f6f66fcd":"code","5da8d5b8":"code","7aeab560":"code","966d4624":"code","c0efa29a":"code","1866cae5":"code","ae6a2d13":"code","df3364f0":"code","5f06d3ca":"code","c0fe0065":"code","0687e04b":"code","43f3c234":"code","c06c7cc2":"code","d9298c1c":"code","01e3d9ab":"code","ac13b096":"code","aba566ee":"code","8f1126f5":"code","f5c10087":"code","1f7c8f84":"code","0e22698c":"code","1e435f2b":"code","5893127e":"code","f695a3b9":"code","7507d9fe":"code","158e300f":"code","8731cea5":"markdown","7e2f01ef":"markdown","e2f2bc75":"markdown","832caa03":"markdown","8bb45ab3":"markdown","99b4902b":"markdown","7723d242":"markdown","b4db6ee9":"markdown","45b0ae46":"markdown","294f4215":"markdown","769b34d5":"markdown","84480e9f":"markdown","ef53fd6c":"markdown","09cdae11":"markdown","01a40a1a":"markdown","2239624e":"markdown","aa0003e1":"markdown","40058a86":"markdown","3b4f3399":"markdown","eab38ae3":"markdown","84335e72":"markdown","af98a723":"markdown","b0730bb0":"markdown","d4e11b65":"markdown","e83023c7":"markdown","cc3d01fb":"markdown","12876f25":"markdown","4c6c31d3":"markdown","6805c744":"markdown","520c5b8f":"markdown","1726d2fd":"markdown","84365326":"markdown","e57eb13d":"markdown","008c0c68":"markdown","9238e4d0":"markdown","93ff7343":"markdown","6bde1b66":"markdown"},"source":{"6939207e":"!pip install langdetect\n!pip install scikit-plot\n!pip install nltk","f87bd76e":"### Ignore warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ca3c26b4":"### Packages de base\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nget_ipython().magic('matplotlib inline')\nimport time\nfrom itertools import chain \nfrom multiprocessing import Pool\nimport gc\nimport sys\nfrom scipy import sparse\nfrom random import sample \n\n### Packages pour NLP\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\nimport nltk \nstop = set(stopwords.words('english'))\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nstemmer = SnowballStemmer('english')\nimport nltk.tokenize as tokenize\nimport gensim\nfrom wordcloud import WordCloud\nfrom langdetect import detect\n\n### Packages pour les plots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nsia = SIA()\n\n### Packages ML\nfrom umap import UMAP\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.externals import joblib\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Dropout\nfrom keras.optimizers import Adam\nfrom sklearn.preprocessing import Normalizer\nimport scikitplot as skplt","31c1e2e3":"import os\nos.chdir('..\/input\/dataset') # Set working directory","a757a375":"df_train = pd.read_csv('train.txt', sep=\"\\t\",header =None)\ndf_train = pd.DataFrame(df_train[0].apply(lambda x: x.split(None, 1)).tolist(), columns=['label', 'text'])","376c20a5":"print('Shape of data :',df_train.shape)","b40b4a71":"df_train.describe(include='all')","99ae26d8":"df_train.info()","35dec52f":"df_train.head(20)","55811cf9":"df_train = df_train.drop_duplicates()","4fa5ac6b":"df_train['label'].unique()","a93f052d":"print(df_train['label'][0])\ndf_train['text'][0]","e3c3cd45":"print(df_train['label'][1])\ndf_train['text'][1]","e2480676":"print(df_train['label'][6])\ndf_train['text'][6]","04ddada9":"print(df_train['label'][14])\ndf_train['text'][14]","1ceffca3":"### Rename label\ndf_train['label_new'] = 'good'\ndf_train['label_new'][df_train['label']=='__label__1']='bad'\ndf_train.drop('label',axis = 1,inplace=True)","d7e61235":"sns.barplot(np.unique(df_train['label_new']),df_train.groupby('label_new').count().values[:,0])","55a51424":"df_train['len'] = df_train['text'].apply(lambda x: len(x.split()))","b34bef18":"plt.figure(figsize = (12, 7))\nsns.distplot(df_train['len'][df_train['label_new'] =='good'], hist = True, label = \"good\",)\nsns.distplot(df_train['len'][df_train['label_new'] =='bad'], hist = True, label = \"bad\")\nplt.legend(fontsize = 10)\nplt.title(\"Length Distribution by Class\", fontsize = 12)\nplt.show()","8b53a131":"sf_train_exclamation_mark = df_train[df_train['text'].apply(lambda x: '!' in x)]\nsns.barplot(np.unique(sf_train_exclamation_mark['label_new']),sf_train_exclamation_mark.groupby('label_new').count().values[:,0])\nplt.title('Exclamation_mark (n =' +str(sf_train_exclamation_mark.shape[0])+')')","ac530bbe":"sf_train_question_mark = df_train[df_train['text'].apply(lambda x: '?' in x)]\nsns.barplot(np.unique(sf_train_question_mark['label_new']),sf_train_question_mark.groupby('label_new').count().values[:,0])\nplt.title('Question_mark (n =' +str(sf_train_question_mark.shape[0])+')')","21232148":"sf_train_emotion_unhappy = df_train[df_train['text'].apply(lambda x: ':(' in x)]\nsns.barplot(np.unique(sf_train_emotion_unhappy['label_new']),sf_train_emotion_unhappy.groupby('label_new').count().values[:,0])\nplt.title('Emotion_unhappy (n =' +str(sf_train_emotion_unhappy.shape[0])+')')","01854a6d":"sf_train_emotion_happy = df_train[df_train['text'].apply(lambda x: ':)' in x)]\nsns.barplot(np.unique(sf_train_emotion_happy['label_new']),sf_train_emotion_happy.groupby('label_new').count().values[:,0])\nplt.title('Emotion_happy (n =' +str(sf_train_emotion_happy.shape[0])+')')","3365a3ef":"train_sentiment = pd.DataFrame.from_records(df_train['text'][0:10000].apply(lambda x: sia.polarity_scores(x)))\ntrain_sentiment['len'] = df_train['text'][0:10000].apply(len)\ntrain_sentiment['label'] = 'good'\ntrain_sentiment['label'][train_sentiment['neg']>train_sentiment['pos']] ='bad'\ntrain_sentiment['real_label'] = df_train['label_new'][0:10000]\ntrain_sentiment.head()","be99bf2d":"print(classification_report(df_train['label_new'][0:10000], train_sentiment['label']))","5f328b11":"accuracy_score(df_train['label_new'][0:10000], train_sentiment['label'])","1cc5916a":"lang_detection = df_train.iloc[0:10000]\nlang_detection['lang'] = df_train['text'][0:10000].map(detect,) ","ad74b65b":"lang_detection[lang_detection['lang'] != 'en']","6990a0ca":"lang_detection['lang'].value_counts()","4ade3b97":"### Clean memory\nn_obs = 10000\ndf_train = df_train.iloc[0:n_obs]\ndel sf_train_emotion_happy, sf_train_emotion_unhappy, sf_train_question_mark, sf_train_exclamation_mark, lang_detection\ngc.collect()","692f63a9":"from textblob import TextBlob\ndef lemmatize_with_postag(sentence):\n    sent = TextBlob(sentence)\n    tag_dict = {\"J\": 'a', \n                \"N\": 'n', \n                \"V\": 'v', \n                \"R\": 'r'}\n    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n    return \" \".join(lemmatized_list)\n\n### Function which define if a word is a number or not\n\ndef is_number(word):\n    try: \n        x = float(word)\n        return (x == x) and (x - 1 != x)\n    except Exception:\n        return False\n    \n### Tokenization using regular expression pattern which keep numbers, ?_mark, !_mark, :), :(     \ntokenizer= tokenize.RegexpTokenizer(r'[0-9]*\\.?[0-9]+|[a-zA-Z]+|[!]+|[:)]+|[:(]+|[?]+|[^[a-zA-Z]\\s]+|[^[a-zA-Z][0-9]]')\n\n### dictionary number to word\nnumber2word = {'0':'zero','1':'one','2':'two','3':'three','4':'four','5':'five','6':'six','7':'seven','8':'eight','9':'nine','.':'point'}\n\n### English Stemming \nstemmer = SnowballStemmer('english')\n\ndef text_process(mess):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Returns a list of the cleaned text\n    4. Keep ?, keep !\n    5. Keep . for decimal numbers\n    \"\"\"\n    mess = tokenizer.tokenize(mess.lower())\n    \n    word_list = []\n        \n    for word in mess:\n        if (word ==':') or (word.startswith(')')) or (word.startswith('(')): \n            next\n        elif word.startswith(':)') :\n            word_list.append('emotion_happy')\n        elif word.startswith(':(') :\n            word_list.append('emotion_unhappy')\n        elif word.startswith('?') : \n            word_list.append('question_mark')\n        elif word.startswith('!') :\n            word_list.append('exclamation_mark')\n        elif (word not in stopwords.words('english')) and (word not in ':.%...') and (is_number(word)==False):\n            word_list.append(lemmatize_with_postag(word))\n        elif is_number(word)==True:\n            b = [number2word.get(i) for i in list(word)]\n            word_list += b\n    return word_list\n\n","bb25b56b":"print('Pharse originale : \\n')\nprint(df_train['text'][32],'\\n')\nprint('Pharse transform\u00e9e: \\n')\nprint(' '.join(text_process(df_train['text'][32])),'\\n')\nprint('Liste de mots (Tokenization): \\n')\ntext_process(df_train['text'][32])","7477b153":"print('Pharse originale : \\n')\nprint(df_train['text'][979],'\\n')\nprint('Pharse transform\u00e9e: \\n')\nprint(' '.join(text_process(df_train['text'][979])),'\\n')\nprint('Liste de mots (Tokenization): \\n')\ntext_process(df_train['text'][32])","a1d0ad1c":"### These functions allow us to work in parallel\nnum_partitions = 10 #number of partitions to split dataframe\nnum_cores = 4 #number of cores on your machine\n\ndef vec_text_process(array):\n    return array.apply(text_process)\n\ndef parallelize_dataframe(df):\n    df_split = np.array_split(df, num_partitions)\n    pool = Pool(num_cores)\n    df = pd.Series(list(chain.from_iterable(pool.map(vec_text_process, df_split))))\n    pool.close()\n    pool.join()\n    return df\n\ndf_train_tokens = parallelize_dataframe(df_train['text'][0:n_obs])","d0d56ca3":"train_index = df_train.index.isin(sample(range(int(n_obs)), int(n_obs*0.7)))\nX_train = df_train_tokens[train_index]\nY_train = df_train['label_new'][train_index].reset_index(drop=True)\n\nX_test = df_train_tokens[~train_index].reset_index(drop=True)\nY_test = df_train['label_new'][~train_index].reset_index(drop=True)\n\n### Clear memory\n#del df_train, df_train_tokens\ngc.collect()","ca870cc4":"### Create a dictionary \ndf_train_dict = gensim.corpora.Dictionary(X_train)","2f3acc35":"### we draw word cloud on first 10000 reviews \ntext = ' '.join(df_train['text'].str.lower().values[0:10000])\nwordcloud = WordCloud(max_font_size=None, stopwords=stop, background_color='white',\n                      width=1200, height=1000).generate(text)\nplt.figure(figsize=(12, 8))\nplt.imshow(wordcloud)\nplt.title('Top words in review')\nplt.axis(\"off\")\nplt.show()","a58fc65e":"### Clear memory\ndel df_train, df_train_tokens\ngc.collect()","e197d7c6":"# fonction qui sort la dataframe de la fr\u00e9quence\ndef most_frequent_words(dictionary) : \n    df_tokens = pd.DataFrame.from_dict(dictionary.dfs, orient='index',columns=['counts'])\n    get_words = np.vectorize(dictionary.get)\n    df_tokens['word'] = get_words(df_tokens.index)\n    df_tokens.sort_values('counts',ascending=False,inplace = True)\n    return df_tokens\ndf_tokens = most_frequent_words(df_train_dict)","51ddc7e0":"print('The length of dictionary =', df_tokens.shape[0])","d28f6aa4":"df_tokens.reset_index(drop = True, inplace=True)\ndf_tokens.head()","526f0212":"fig, ax1 = plt.subplots(figsize=(14,8))\ng = sns.barplot(df_tokens['word'][0:50],y=df_tokens['counts'][0:50], ax=ax1)\nplt.xticks(rotation=90,fontsize=10)\nplt.title('The most 50 frequent words')\n\nax2 = ax1.twinx()\nax2.set_ylim(0,ax1.get_ylim()[1]\/X_train.shape[0])\nax2.set_ylabel('counts \/ documents')\nfig.tight_layout() \nplt.show()","56830626":"df_tokens['counts'].describe()","6028cf44":"print('Quantile 95% :', np.quantile(df_tokens['counts'], 0.95))\nprint('Quantile 97.5% :', np.quantile(df_tokens['counts'], 0.975))\nprint('Number of words appearing only 1 time =', sum(df_tokens['counts'] ==1 ))\nprint('Number of frequencies of the 10000th word =', df_tokens['counts'][9999])","59b242dc":"df_train_dict_1 = df_train_dict\ndf_train_dict_1.filter_extremes(keep_n=1000)\nX_train_corpus  = [df_train_dict_1.doc2bow(doc) for doc in X_train]\nX_train_visu = gensim.matutils.corpus2dense(X_train_corpus, num_terms=len(df_train_dict_1)).T\na = X_train_visu\/ X_train_visu.sum(axis =0)[None,:]","cb12b807":"plt.figure(figsize=(15,8))\n\nplt.imshow(a[np.argsort(Y_train),:].T,)","761839b4":"df_train_dict.filter_extremes(keep_n=10000)","91574b38":"X_train_corpus  = [df_train_dict.doc2bow(doc) for doc in X_train]\nX_train_tfidf = gensim.models.TfidfModel(X_train_corpus, df_train_dict)\n\n### transform data to spare matrix in order to reduce memory\nX_train_tfidf_full = gensim.matutils.corpus2csc(X_train_tfidf[X_train_corpus], num_terms=len(df_train_dict)).T","cbd73f91":"X_test_corpus  = [df_train_dict.doc2bow(doc) for doc in X_test]\nX_test_tfidf = gensim.models.TfidfModel(X_test_corpus, df_train_dict)\n\n### transform data to spare matrix in order to reduce memory\nX_test_tfidf_full = gensim.matutils.corpus2csc(X_test_tfidf[X_test_corpus], num_terms=len(df_train_dict)).T","5f613f61":"df_train_dict.filter_extremes(keep_n=1000)\nX_train_corpus_visu  = [df_train_dict.doc2bow(doc) for doc in X_train]\nX_train_tfidf_visu = gensim.models.TfidfModel(X_train_corpus_visu, df_train_dict)\nX_train_tfidf_visu = gensim.matutils.corpus2dense(X_train_tfidf_visu[X_train_corpus_visu], num_terms=len(df_train_dict)).T\nb = X_train_tfidf_visu\/ X_train_tfidf_visu.sum(axis =0)[None,:]\nplt.figure(figsize=(15,8))\nplt.imshow(b[np.argsort(Y_train),:].T)","65b28d99":"### For PCA, we try first with 200 components\np = time.time()\npca = PCA(n_components = 200)\nX_train_tfidf_pca = pca.fit_transform(X_train_tfidf_full.A)\n","a564febd":"pca_analysis = pd.DataFrame(columns = ['Number of components','Cumulative of variance ratio'])\npca_analysis['Number of components'] = np.arange(1,201)\npca_analysis['Cumulative of variance ratio'] = np.cumsum(pca.explained_variance_ratio_)\npca_analysis","84816643":"X_test_tfidf_pca = pca.transform(X_test_tfidf_full.A)\ntime_pca=time.time() - p","16f62c30":"p = time.time()\nlda_model_tfidf = gensim.models.LdaMulticore(X_train_tfidf[X_train_corpus], num_topics= 20, id2word=df_train_dict, passes=2, workers=4)\nX_train_lda_topic = lda_model_tfidf[X_train_tfidf[X_train_corpus]]\nX_train_tfidf_lda = sparse.lil_matrix((len(X_train_tfidf[X_train_corpus]),20), dtype=np.float64)\n\nfor i in range(len(X_train_tfidf[X_train_corpus])):\n    for j in X_train_lda_topic[i]:\n        X_train_tfidf_lda[i,j[0]] = j[1]\n\nX_test_lda_topic = lda_model_tfidf[X_test_tfidf[X_test_corpus]]\nX_test_tfidf_lda = sparse.lil_matrix((len(X_test_tfidf[X_test_corpus]),20), dtype=np.float64)\n\nfor i in range(len(X_test_tfidf[X_test_corpus])):\n    for j in X_test_lda_topic[i]:\n        X_test_tfidf_lda[i,j[0]] = j[1]\n        \ndel lda_model_tfidf, X_train_lda_topic, X_test_lda_topic\ntime_lda=time.time() - p","a5aebd7c":"p = time.time()\nrp_model_tfidf = gensim.models.RpModel(X_train_tfidf[X_train_corpus], id2word=df_train_dict, num_topics= 100)\nX_train_rp_topic = rp_model_tfidf[X_train_tfidf[X_train_corpus]]\nX_train_tfidf_rp = sparse.lil_matrix((len(X_train_tfidf[X_train_corpus]),100), dtype=np.float64)\n\nfor i in range(len(X_train_tfidf[X_train_corpus])):\n    for j in X_train_rp_topic[i]:\n        X_train_tfidf_rp[i,j[0]] = j[1]\n        \n\nX_test_rp_topic = rp_model_tfidf[X_test_tfidf[X_test_corpus]]\nX_test_tfidf_rp = sparse.lil_matrix((len(X_test_tfidf[X_test_corpus]),100), dtype=np.float64)\n\nfor i in range(len(X_test_tfidf[X_test_corpus])):\n    for j in X_test_rp_topic[i]:\n        X_test_tfidf_rp[i,j[0]] = j[1]\n        \ndel rp_model_tfidf, X_train_rp_topic, X_test_rp_topic\n\ntime_rp=time.time() - p","4538d9e2":"### n_components can be set between 1 to 100, but this technique consume a lot of time, so we choose only 20\np = time.time()\numap = UMAP(n_components=20, metric = 'cosine')\nX_train_tfidf_umap = umap.fit_transform(X_train_tfidf_full.A)\nX_test_tfidf_umap = umap.transform(X_test_tfidf_full.A)\ndel umap\ntime_umap = time.time()-p","d73d78ae":"train_sentiment['len'] = Normalizer().fit_transform(np.array(train_sentiment['len']).reshape(1,-1)).reshape(-1,1)\nsentiment = train_sentiment[['neg','neu','pos','compound','len']]\ndel train_sentiment\n\nX_train_tfidf_full = sparse.hstack((sentiment[0:int(n_obs*0.7)], X_train_tfidf_full))\nX_test_tfidf_full = sparse.hstack((sentiment[int(n_obs*0.7):n_obs], X_test_tfidf_full))\n\nX_train_tfidf_pca = np.concatenate([sentiment[0:int(n_obs*0.7)],X_train_tfidf_pca],axis =1)\nX_test_tfidf_pca = np.concatenate([sentiment[int(n_obs*0.7):n_obs],X_test_tfidf_pca],axis =1)\n\nX_train_tfidf_lda = sparse.hstack((sentiment[0:int(n_obs*0.7)], X_train_tfidf_lda))\nX_test_tfidf_lda = sparse.hstack((sentiment[int(n_obs*0.7):n_obs], X_test_tfidf_lda))\n\nX_train_tfidf_rp = sparse.hstack((sentiment[0:int(n_obs*0.7)], X_train_tfidf_rp))\nX_test_tfidf_rp = sparse.hstack((sentiment[int(n_obs*0.7):n_obs], X_test_tfidf_rp))\n\nX_train_tfidf_umap = np.concatenate([sentiment[0:int(n_obs*0.7)],X_train_tfidf_umap],axis =1)\nX_test_tfidf_umap = np.concatenate([sentiment[int(n_obs*0.7):n_obs],X_test_tfidf_umap],axis =1)\n\ndel sentiment","b12c7989":"### Clean memory\ndel X_train, X_test","2c714f81":"gc.collect()","4c328a4d":"dimension_reduction_comparison = pd.DataFrame(columns = ['Method','Computing time','Number of new features'])\ndimension_reduction_comparison['Method']=['Principal component analysis (PCA)','Random projection (RP)','Latent Dirichet Allocation (LDA)','Uniform Manifold Approximation and Projection (UMAP)']\ndimension_reduction_comparison['Computing time'] = [time_pca, time_lda, time_rp, time_umap]\ndimension_reduction_comparison['Number of new features'] =[200,20,100,20]\ndimension_reduction_comparison","868f9457":"### On full data\np = time.time()\nmodel_LR_full = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\nmodel_LR_full.fit(X_train_tfidf_full,Y_train)\nY_predict_LR_full = model_LR_full.predict(X_test_tfidf_full)\ntime_LR_full = time.time()-p","1e8a9f80":"### On PCA data\np = time.time()\nmodel_LR_pca = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\nmodel_LR_pca.fit(X_train_tfidf_pca,Y_train)\nY_predict_LR_pca = model_LR_pca.predict(X_test_tfidf_pca)\ntime_LR_pca = time.time()-p","4362194d":"### On LDA data\np = time.time()\nmodel_LR_lda = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\nmodel_LR_lda.fit(X_train_tfidf_lda,Y_train)\nY_predict_LR_lda = model_LR_lda.predict(X_test_tfidf_lda)\ntime_LR_lda = time.time()-p","f38dba50":"### On RP data\np = time.time()\nmodel_LR_rp = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\nmodel_LR_rp.fit(X_train_tfidf_rp,Y_train)\nY_predict_LR_rp = model_LR_rp.predict(X_test_tfidf_rp)\ntime_LR_rp = time.time()-p","e34fbee5":"### On UMAP data\np = time.time()\nmodel_LR_umap = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\nmodel_LR_umap.fit(X_train_tfidf_umap,Y_train)\nY_predict_LR_umap = model_LR_umap.predict(X_test_tfidf_umap)\ntime_LR_umap = time.time()-p","2e591704":"### On full data\np = time.time()\nmodel_SVM_full = SVC(kernel='linear', probability = True)\nmodel_SVM_full.fit(X_train_tfidf_full,Y_train)\nY_predict_SVM_full = model_SVM_full.predict(X_test_tfidf_full)\nY_predict_proba_SVM_full = model_SVM_full.predict_proba(X_test_tfidf_full)\ntime_SVM_full = time.time()-p","0b1507db":"### On PCA data\np = time.time()\nmodel_SVM_pca = SVC(kernel='linear', probability = True)\nmodel_SVM_pca.fit(X_train_tfidf_pca,Y_train)\nY_predict_SVM_pca = model_SVM_pca.predict(X_test_tfidf_pca)\nY_predict_proba_SVM_pca = model_SVM_pca.predict_proba(X_test_tfidf_pca)\ntime_SVM_pca = time.time()-p","47f56926":"### On LDA data\np = time.time()\nmodel_SVM_lda = SVC(kernel='linear')\nmodel_SVM_lda.fit(X_train_tfidf_lda,Y_train)\nY_predict_SVM_lda = model_SVM_lda.predict(X_test_tfidf_lda)\ntime_SVM_lda = time.time()-p","f6f66fcd":"### On RP data\np = time.time()\nmodel_SVM_rp = SVC(kernel='linear')\nmodel_SVM_rp.fit(X_train_tfidf_rp,Y_train)\nY_predict_SVM_rp = model_SVM_rp.predict(X_test_tfidf_rp)\ntime_SVM_rp = time.time()-p","5da8d5b8":"### On UMAP data\np = time.time()\nmodel_SVM_umap = SVC(kernel='linear')\nmodel_SVM_umap.fit(X_train_tfidf_umap,Y_train)\nY_predict_SVM_umap = model_SVM_umap.predict(X_test_tfidf_umap)\ntime_SVM_umap = time.time()-p","7aeab560":"### On full data\np = time.time()\nmodel_KNN_full = KNeighborsClassifier()\nmodel_KNN_full.fit(X_train_tfidf_full,Y_train)\nY_predict_KNN_full = model_KNN_full.predict(X_test_tfidf_full)\ntime_KNN_full = time.time()-p","966d4624":"### On PCA data\np = time.time()\nmodel_KNN_pca = KNeighborsClassifier()\nmodel_KNN_pca.fit(X_train_tfidf_pca,Y_train)\nY_predict_KNN_pca = model_KNN_pca.predict(X_test_tfidf_pca)\ntime_KNN_pca = time.time()-p","c0efa29a":"### On LDA data\np = time.time()\nmodel_KNN_lda = KNeighborsClassifier()\nmodel_KNN_lda.fit(X_train_tfidf_lda,Y_train)\nY_predict_KNN_lda = model_KNN_lda.predict(X_test_tfidf_lda)\ntime_KNN_lda = time.time()-p","1866cae5":"### On RP data\np = time.time()\nmodel_KNN_rp = KNeighborsClassifier()\nmodel_KNN_rp.fit(X_train_tfidf_rp,Y_train)\nY_predict_KNN_rp = model_KNN_rp.predict(X_test_tfidf_rp)\ntime_KNN_rp = time.time()-p","ae6a2d13":"### On UMAP data\np = time.time()\nmodel_KNN_umap = KNeighborsClassifier()\nmodel_KNN_umap.fit(X_train_tfidf_umap,Y_train)\nY_predict_KNN_umap = model_KNN_umap.predict(X_test_tfidf_umap)\ntime_KNN_umap = time.time()-p","df3364f0":"Y_train_NN = (Y_train=='good')\nY_test_NN = (Y_test=='good')","5f06d3ca":"### On full data\np = time.time()\nmodel_NN = Sequential()\nmodel_NN.add(Dense(128, activation = 'relu',input_shape=(X_train_tfidf_full.shape[1],)))\nmodel_NN.add(BatchNormalization())\nmodel_NN.add(Dropout(0.2)) # to avoid overfitting\nmodel_NN.add(Dense(1, activation = 'sigmoid'))\nmodel_NN.compile(loss = 'binary_crossentropy', optimizer = Adam(lr = 0.001, decay = 0.01), metrics = ['accuracy'])\nmodel_NN.fit(X_train_tfidf_full.A,Y_train_NN, validation_data=(X_test_tfidf_full.A, Y_test_NN), epochs = 5, batch_size = 128)\nY_predict_NN_full = np.where(model_NN.predict(X_test_tfidf_full.A)<0.5,'bad', 'good')\ntime_NN_full = time.time()","c0fe0065":"### On PCA data\np = time.time()\nmodel_NN = Sequential()\nmodel_NN.add(Dense(128, activation = 'relu',input_shape=(X_train_tfidf_pca.shape[1],)))\nmodel_NN.add(BatchNormalization())\nmodel_NN.add(Dropout(0.2)) # to avoid overfitting\nmodel_NN.add(Dense(1, activation = 'sigmoid'))\nmodel_NN.compile(loss = 'binary_crossentropy', optimizer = Adam(lr = 0.001, decay = 0.01), metrics = ['accuracy'])\nmodel_NN.fit(X_train_tfidf_pca,Y_train_NN, validation_data=(X_test_tfidf_pca, Y_test_NN), epochs = 5, batch_size = 128)\nY_predict_NN_pca = np.where(model_NN.predict(X_test_tfidf_pca)<0.5,'bad', 'good')\ntime_NN_pca = time.time()","0687e04b":"### On LDA data\np = time.time()\nmodel_NN = Sequential()\nmodel_NN.add(Dense(128, activation = 'relu',input_shape=(X_train_tfidf_lda.shape[1],)))\nmodel_NN.add(BatchNormalization())\nmodel_NN.add(Dropout(0.2)) # to avoid overfitting\nmodel_NN.add(Dense(1, activation = 'sigmoid'))\nmodel_NN.compile(loss = 'binary_crossentropy', optimizer = Adam(lr = 0.001, decay = 0.01), metrics = ['accuracy'])\nmodel_NN.fit(X_train_tfidf_lda.A,Y_train_NN, validation_data=(X_test_tfidf_lda.A, Y_test_NN), epochs = 5, batch_size = 128)\nY_predict_NN_lda = np.where(model_NN.predict(X_test_tfidf_lda.A)<0.5,'bad', 'good')\ntime_NN_lda = time.time()","43f3c234":"### On RP data\np = time.time()\nmodel_NN = Sequential()\nmodel_NN.add(Dense(128, activation = 'relu',input_shape=(X_train_tfidf_rp.shape[1],)))\nmodel_NN.add(BatchNormalization())\nmodel_NN.add(Dropout(0.2)) # to avoid overfitting\nmodel_NN.add(Dense(1, activation = 'sigmoid'))\nmodel_NN.compile(loss = 'binary_crossentropy', optimizer = Adam(lr = 0.001, decay = 0.01), metrics = ['accuracy'])\nmodel_NN.fit(X_train_tfidf_rp.A,Y_train_NN, validation_data=(X_test_tfidf_rp.A, Y_test_NN), epochs = 5, batch_size = 128)\nY_predict_NN_rp = np.where(model_NN.predict(X_test_tfidf_rp.A)<0.5,'bad', 'good')\ntime_NN_rp = time.time()","c06c7cc2":"### On UMAP data\np = time.time()\nmodel_NN = Sequential()\nmodel_NN.add(Dense(128, activation = 'relu',input_shape=(X_train_tfidf_umap.shape[1],)))\nmodel_NN.add(BatchNormalization())\nmodel_NN.add(Dropout(0.2)) # to avoid overfitting\nmodel_NN.add(Dense(1, activation = 'sigmoid'))\nmodel_NN.compile(loss = 'binary_crossentropy', optimizer = Adam(lr = 0.001, decay = 0.01), metrics = ['accuracy'])\nmodel_NN.fit(X_train_tfidf_umap,Y_train_NN, validation_data=(X_test_tfidf_umap, Y_test_NN), epochs = 5, batch_size = 128)\nY_predict_NN_umap = np.where(model_NN.predict(X_test_tfidf_umap)<0.5,'bad', 'good')\ntime_NN_umap = time.time()","d9298c1c":"def accuracy_score_vector(x) :\n    return [accuracy_score(Y_test, predictor) for predictor in x]\n\nperfomance_comparison =pd.DataFrame(columns = ['Machine Learning model','Reduction method','Accuracy score','Computing time'])\nperfomance_comparison['Machine Learning model'] = ['Logistic Regression']*5 + ['Support vector machine']*5+['K-nearest neighbors']*5+['Neural network']*5\nperfomance_comparison['Reduction method'] = ['Full matrix', 'PCA', 'LDA', 'RP', 'UMAP']*4\nperfomance_comparison['Accuracy score'] = accuracy_score_vector([\n    Y_predict_LR_full,Y_predict_LR_pca, Y_predict_LR_lda, Y_predict_LR_rp, Y_predict_LR_umap,\n    Y_predict_SVM_full,Y_predict_SVM_pca, Y_predict_SVM_lda, Y_predict_SVM_rp, Y_predict_SVM_umap,\n    Y_predict_KNN_full,Y_predict_KNN_pca, Y_predict_KNN_lda, Y_predict_KNN_rp, Y_predict_KNN_umap,\n    Y_predict_NN_full,Y_predict_NN_pca, Y_predict_NN_lda, Y_predict_NN_rp, Y_predict_NN_umap])\nperfomance_comparison['Computing time'] = [time_LR_full, time_LR_pca, time_LR_lda, time_LR_rp, time_LR_umap,\n                                           time_SVM_full, time_SVM_pca, time_SVM_lda, time_SVM_rp, time_SVM_umap,\n                                           time_KNN_full, time_KNN_pca, time_KNN_lda, time_KNN_rp, time_KNN_umap,\n                                           time_NN_full, time_NN_pca, time_NN_lda, time_NN_rp, time_NN_umap]\nperfomance_comparison['Computing time added dimension reduction time'] = perfomance_comparison['Computing time'] + np.array([0,time_pca, time_lda, time_rp, time_umap]*4)\nperfomance_comparison['Estimated computing time on all training set'] = perfomance_comparison['Computing time added dimension reduction time']\/n_obs*3600000\nperfomance_comparison\n","01e3d9ab":"print(classification_report(Y_test, Y_predict_LR_full))","ac13b096":"cm = confusion_matrix(Y_test,Y_predict_LR_full)\ncm = cm.astype('float')\/cm.sum(axis=1)[:, np.newaxis]\n#Visualization of the Confusion matrix\nax=plt.subplot(title='Confusion matrix of the Logistic Model on full data set');\nsns.heatmap(cm,annot=True, fmt='.4f', cmap='Blues', xticklabels=['good', 'bad'], yticklabels=['good', 'bad'])\nax.set_xlabel('LR Prediction');\nax.set_ylabel('True labels');","aba566ee":"Y_predict_proba_LR_full = model_LR_full.predict_proba(X_test_tfidf_full)\nskplt.metrics.plot_roc_curve(Y_test, Y_predict_proba_LR_full)\nplt.show()","8f1126f5":"print(classification_report(Y_test, Y_predict_LR_pca))","f5c10087":"cm = confusion_matrix(Y_test,Y_predict_LR_pca)\ncm = cm.astype('float')\/cm.sum(axis=1)[:, np.newaxis]\n#Visualization of the Confusion matrix\nax=plt.subplot(title='Confusion matrix of the Logistic Model on PCA data set');\nsns.heatmap(cm,annot=True, fmt='.4f', cmap='Blues', xticklabels=['good', 'bad'], yticklabels=['good', 'bad'])\nax.set_xlabel('LR Prediction');\nax.set_ylabel('True labels');","1f7c8f84":"Y_predict_proba_LR_pca = model_LR_pca.predict_proba(X_test_tfidf_pca)\nskplt.metrics.plot_roc_curve(Y_test, Y_predict_proba_LR_pca)\nplt.show()","0e22698c":"print(classification_report(Y_test, Y_predict_SVM_full))","1e435f2b":"cm = confusion_matrix(Y_test,Y_predict_SVM_full)\ncm = cm.astype('float')\/cm.sum(axis=1)[:, np.newaxis]\n#Visualization of the Confusion matrix\nax=plt.subplot(title='Confusion matrix of the Support Vector Machines on full data set');\nsns.heatmap(cm,annot=True, fmt='.4f', cmap='Blues', xticklabels=['good', 'bad'], yticklabels=['good', 'bad'])\nax.set_xlabel('SVM Prediction');\nax.set_ylabel('True labels');","5893127e":"Y_predict_proba_SVM_full = model_SVM_full.predict_proba(X_test_tfidf_full)\nskplt.metrics.plot_roc_curve(Y_test, Y_predict_proba_SVM_full)\nplt.show()","f695a3b9":"print(classification_report(Y_test, Y_predict_SVM_pca))","7507d9fe":"cm = confusion_matrix(Y_test,Y_predict_SVM_pca)\ncm = cm.astype('float')\/cm.sum(axis=1)[:, np.newaxis]\n#Visualization of the Confusion matrix\nax=plt.subplot(title='Confusion matrix of the Support Vector Machines on PCA data set');\nsns.heatmap(cm,annot=True, fmt='.4f', cmap='Blues', xticklabels=['good', 'bad'], yticklabels=['good', 'bad'])\nax.set_xlabel('SVM Prediction');\nax.set_ylabel('True labels');","158e300f":"Y_predict_proba_SVM_pca = model_SVM_pca.predict_proba(X_test_tfidf_pca)\nskplt.metrics.plot_roc_curve(Y_test, Y_predict_proba_SVM_pca)\nplt.show()","8731cea5":"\n##  III. Cr\u00e9ation de la matrice des features\nNous allons transformer notre texte en matrice num\u00e9rique pouvant etre impl\u00e9ment\u00e9 dans diff\u00e9rents algorithmes de machine learning pour faire de la classifacation.\n\nIl existe plusieurs m\u00e9thode de transformation du texte. Le plus simple d'entre eux est le Bag-of-words ou en ligne on aura les diff\u00e9rents commentaire et en colone chaque unique mot pr\u00e9sent dans le texte analis\u00e9.\n### III.1 Train test splitting\n","7e2f01ef":"Nous allons comparer les temps de calculs des diff\u00e9rentes m\u00e9thodes et s\u00e9lectionner celle qui nous donnera un meilleur rapport temps d'analyse et meilleur accuracy.","e2f2bc75":"#### IV.5.3.1 PCA data set\n","832caa03":"La figure ci-dessus donne un appercu de la distribution de nos labels dans le data train.<br> ( Distrubution \u00e9quilibr\u00e9e ) \n### I.2 Cr\u00e9ation de nouvelles variables\nUne fois cette premiere visualisation r\u00e9alis\u00e9, on se demande maintenant si la cr\u00e9ation de nouvelles variables par simple transformation de nos donn\u00e9es peut nous apporter des informations supl\u00e9mentaires. C'est pour quoi, on s'int\u00e9resse maintenant \u00e0 la distribution de la taille des textes par rapport aux diff\u00e9rents labels. ","8bb45ab3":"#### IV.5.3 Resultat de Logistic Regression\n#### IV.5.3.1 Toute la dictionnaire\n","99b4902b":"#### IV.5.4.1 PCA data set","7723d242":"## Conclusion\nApres avoir choisi de ne conserver que deux mod\u00e8les qui avaient l'accuracy la plus \u00e9lev\u00e9, nous venons d'effectuer deux crit\u00e8res de comparaison afin de d\u00e9finir le model le plus performant et prenant en compte bien \u00e9videmment le temps de calcul nous permettant de l'appliquer sur toute la base de donn\u00e9e.\u00e0 <br>\nAu vue des informations que nous apportent l'accuracy, la courbe ROC, et la matrice de confusion, nous choisissons pour la suite d'effectuer la pr\u00e9diction du avec le mod\u00e8le de **regression logistique**.\n","b4db6ee9":"Pour la suite, nous ferons la mod\u00e9lisation sur seulement 10000 observations. On choisira le meuilleur mod\u00e8le et l'addapte sur tout le jeu d'apprentisage, puis on sortira le r\u00e9sultat pr\u00e9vu sur la base de test.","45b0ae46":"### III.4 R\u00e9duction de dimention<br>\n\nA cette \u00e9tape, on souhaite r\u00e9duire la taille de notre matrice. En effet, le bag of words selectione tout les mots uniques dans l'enssemble de commentaires ce qui peut entrainer avec un trop grand nombre de zero car certaint mots n'apparaissent que dans une poign\u00e9 de commentaires.\n\nDeplus, une matrie de taille pleine peut entrainer des erreurs dans notre algorhytme c'est pour quoi\nil a \u00e9t\u00e9 normale de se questioner sur la viabilit\u00e9 de conserver ces mots dans notre matrice\n\nPour effectuer cette r\u00e9duction, nous testerons diff\u00e9rentes m\u00e9thodes qui se prete bien au traitement de texte. <br>\n\nLes quelques methodes impl\u00e9ment\u00e9 sont : \n\n1. Principal component analysis (PCA)\n2. Random projection (RP)\n3. Latent Dirichet Allocation (LDA) http:\/\/cs229.stanford.edu\/proj2017\/final-reports\/5163902.pdf\n4. Uniform Manifold Approximation and Projection (UMAP)\n\n#### III.4.1 PCA","294f4215":"On vient ici d'identifier plusieurs commentaires qui ne sont pas en anglais et on d\u00e9cide de les retirer de notre jeu de donn\u00e9es. Cependant, en terme de proportion cela ne repr\u00e9sente que 0,31% des commentaires.<br>\nOn a d\u00e9cid\u00e9 pour la suite on d\u00e9cide de ne pas retenir cette variable car la d\u00e9tection de langue demande enormement de temps de calcul pour une proportionalit\u00e9 relativement faible. ","769b34d5":"En ne prenant en compte que l'intensit\u00e9 d'apparution des mots sur le graphe cidessus, on se rend compte que la repr\u00e9sentation n'est pas tres net et difficile \u00e0 interpr\u00e9ter.<br>\nPour la suite, on va multiplier cette frequence par une variable permettant de d\u00e9finir le poid de chaque mots\n","84480e9f":"### IV.3 K-Neighbors Classifier","ef53fd6c":"Le gaphique ci-dessus illustre la densit\u00e9 de mots dans la matrice de Bage of Words<br>\nNous decidons de garder 10000 mots les plus pr\u00e9sentants dans la dictionnaire <br>","09cdae11":"## I. Importation des donn\u00e9es","01a40a1a":"Par ailleurs, on rattache a cette matrice  l'analyse des sentiment et les variable suppl\u00e9mentaire cr\u00e9e pr\u00e9c\u00e9demment.","2239624e":"#### III.4.5 Comparaison des m\u00e9thodes de la reduction de dimension ","aa0003e1":"#### III.4.4 UMAP","40058a86":"Il y a donc 2 classes \u00e0 pr\u00e9voir <br>\nOn obseve maintenant quelques pharses de mani\u00e8re compl\u00e8te","3b4f3399":"##  II. Preprocessing\nPour cette \u00e9tape, nous faisons les travaux comme suivant : <br>\n* Tokenizer les text -- https:\/\/www.debuggex.com\/cheatsheet\/regex\/python \n* Enlever les punctuation <br>\n* Enlever les stopwords <br>\n* Faire une lematisation avec part of speach -- https:\/\/www.machinelearningplus.com\/nlp\/lemmatization-examples-python\/","eab38ae3":"On n'observe qu'une l\u00e9g\u00e8re diff\u00e9rence entre les deux distributions. N\u00e9amoins, pour la suite on consid\u00e8re que cette diff\u00e9rence diff\u00e9rence serait suceptible de nous apporter des informations. On decide donc de conserver cette nouvelle variable pour l'\u00e9laboration de notre mod\u00e8le.\n##### I.2.A Certaines Poinctuations apportent elles des informations sur la distribution  des labels ?","84335e72":"L'accuracy trouv\u00e9 ce dessus n'est pas tres performante. Cependant on conserve la variable car on estime que cette statistique nous apportera des informations dans notre model. \n#### I.2.C Language Detection\n","af98a723":"##### <u>Remarque2<\/u>:\n\nOn observe qu'on a beaucoup plus de point d'exclamation dans les commentaires class\u00e9 positivement et plus de point d'int\u00e9rogation dans les messages class\u00e9 n\u00e9gativement.<br> Deplus, il y a une forte cor\u00e9lation entre l'apparution des emojis **happy** et le fait que le texte soit class\u00e9 positivement et inversement pour **unhappy**. \n\n#### I.2.B Sentiment Intensity Analysis (SIA)\n\nDans cette parti une analyserons l'intensit\u00e9 des sentiments des diff\u00e9rents commentaires par **VADER** (Valence Aware Dictionary and sEntiment Reasoner). Cette analyse ne sera \u00e9ffectu\u00e9 que sur 10000 observations par souci de temps de calcul. Notons que la variable \"label\" dans la table ci-dessous est la prediction de la SIA ","b0730bb0":"Le gaphique ci-dessus illustre le poid des mots dans la matrice de TF-IDF <br>\nPour la suite, on utilisera la matrice TF-IDF parce qu'on consid\u00e8re qu'elle est la version \"scaling\" du Bag of Words, et est plus addapt\u00e9e pour la plupart des algorithme de ML","d4e11b65":"Notre analyse portera sur exactement 3.600.000 commentaires correspondant aux observations dans la base d'apprentisage et de 2 colones texte & label.\n\n### I.1 Description des donn\u00e9es","e83023c7":"## IV. Machine Learning model","cc3d01fb":"Dans notre ACP on fait le choix  de 200  composante principale qui ne contribue que de 20% de la totalit\u00e9 de nos variable. \u00e0 premiere vue, ce n'est pas tr\u00e8s significatif mais pour des soucis de temps de calcul on accepte et regardera plus tard si on doit en r\u00e9cup\u00e9rer plus.","12876f25":"##### <u>Remarque 1<\/u>:\n\nLes textes \u00e9tudi\u00e9s sont  appriorie redig\u00e9s en anglais. En outre, ils comportent pas mal de ponctuations et quelsues emoji.<br>\nOn peut apriori apres lecture dire que le label 1  correspond aux commentaires plutot n\u00e9gatifs alors que ceux du label 2 sont positif. \n\n##### Rename label","4c6c31d3":"### IV.2 Support Vector Machines Classifier","6805c744":"#### IV.5.4 Resultat de SVM\n#### IV.5.4.1 Tous la dictionnaire\n","520c5b8f":"###  III.2  Frequence d'apparution des mots ","1726d2fd":"#### III.4.3 RP","84365326":"### IV.4 Artificial neural network","e57eb13d":"### IV.1 Logistic Regression ","008c0c68":"Apres analyse du tableau ci dessus,  nous s\u00e9lectionnons deux model avec un fort accuracy qui sont :\n* La logistique regression\n* Le SVM<br>\n\nces deux mod\u00e8les ont \u00e9t\u00e9 impl\u00e9ment\u00e9s sur  la full matrice et la matrice r\u00e9duite par PCA. Afin de d\u00e9terminer lequel des deux mod\u00e8le on choisira finalement  on observera la matrice de confusion et la courbe ROC","9238e4d0":"#### III.4.2 LDA","93ff7343":"   # <center> Natural Language Processing: Analyse de Sentiments <\/center> \n------------------------------------\n#### Authors :\n- Mohamed NIANG\n- Yao GNONSOU\n- Hoang Dung NGUYEN\n-----------------------------------\n## <center>Introduction<center>\nDans ce projet, nous fessons face \u00e0 un probleme de classification supervis\u00e9 sur des commentaires recolt\u00e9s sur amazon. <br>Le but ici est de traiter puis d'impl\u00e9menter diff\u00e9rent modeles afin d'en d\u00e9terminer le meilleur pour faire de la pr\u00e9diction sur de nouveaux commentaires.\n\nles principaux mod\u00e8les utilis\u00e9s dans ce TP sont : \n\n- **Na\u00efve Bayes Classifier**\n- **Support Vector Machine Classification**\n- **K-Nearest Neighbor Classification**\n- **Artificial Neural Networks**\n<br>","6bde1b66":"### IV.5 Comparaison des Resultats\n"}}