{"cell_type":{"8fef6173":"code","fcd5c80f":"code","3e2482c8":"code","6623f031":"code","84f4948b":"code","4eeba69b":"code","8b1e59fc":"code","13786ab0":"code","a8258f78":"code","4a25d047":"code","0d55cfb8":"code","901539f2":"code","1178b3a6":"code","1abc6dc2":"code","aa8d5a57":"code","62a4d5d9":"code","fc077f3c":"code","f0f970af":"code","1e163774":"code","5c2b3053":"code","24d80941":"code","7535187a":"code","e7c3d115":"code","ea9232f8":"code","94a3e5e2":"code","b2f021da":"code","0f731761":"code","e0bb507a":"code","1692bfdc":"code","6a8d8f6b":"code","f329b164":"code","d6976de9":"code","6139ccab":"code","874f5617":"code","b3499261":"code","271d3c2f":"code","c74753dc":"code","9cc893f0":"code","17cd2302":"code","00bfa484":"code","981f7f9f":"code","4bc35c71":"code","7777d9d0":"code","12d4f04c":"code","b8728044":"code","24bafc11":"code","7caee243":"code","bccf03e7":"code","00ef4e7f":"code","fc4e0323":"code","a9d3f114":"code","00f00c29":"code","e71680fa":"code","4b9388fe":"code","5545b3b2":"code","900890d5":"code","a0757a89":"markdown","d1854d75":"markdown","417d6b45":"markdown","d46238ed":"markdown","757c3a0f":"markdown","2bdf08c4":"markdown","37167079":"markdown","b25515e0":"markdown","a794526a":"markdown","0f9adb04":"markdown","0eec0d86":"markdown","637a3167":"markdown","0b4fcbe7":"markdown","c617a9df":"markdown","71c367e7":"markdown","f6778ec4":"markdown","52a3e973":"markdown","cbc6924e":"markdown","957d0b70":"markdown","1a9fe436":"markdown","e306da5d":"markdown"},"source":{"8fef6173":"# importing libraries\n\nimport os\nimport re\nimport json\nimport gc\nfrom itertools import repeat\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport glob\nfrom collections import defaultdict\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom tqdm.auto import tqdm\n\n\n%matplotlib inline\n#print(tf.__version__)\n#os.listdir('\/kaggle\/input\/coleridgeinitiative-show-us-the-data')\nimport warnings\nwarnings.simplefilter('ignore')","fcd5c80f":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","3e2482c8":"source_path = '..\/input\/coleridgeinitiative-show-us-the-data'","6623f031":"# Train data csv\ndf = pd.read_csv(f'{source_path}\/train.csv')\nsample_submission_df = pd.read_csv(f'{source_path}\/sample_submission.csv')\n#train = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')","84f4948b":"df.sample(3)\n","4eeba69b":"sample_submission_df.sample(3)","8b1e59fc":"# text of a publication from the json file and put it in the train dataframe\n\ndef get_text(filename, test=False):\n    if test:\n        df = pd.read_json('..\/input\/coleridgeinitiative-show-us-the-data\/test\/{}.json'.format(filename))\n    else:\n        df = pd.read_json('..\/input\/coleridgeinitiative-show-us-the-data\/train\/{}.json'.format(filename))\n    text = \" \".join(list(df['text']))\n    return text\n","13786ab0":"df['text'] = df['Id'].apply(get_text)\ndf.sample(5)","a8258f78":"df.info()","4a25d047":"# Unique characteristics of this dataset \n\nfor col in df.columns:\n    print(f\"{col}: {len(df[col].unique())}\")","0d55cfb8":"print(\"pub_title:\",len(df['pub_title'].unique()))\nprint(\"ID:\",len(df['Id'].unique()))\nprint(\"diff btw pub_title and Id:\",len(df['Id'].unique()) - len(df['pub_title'].unique()))\nprint(\"diff btw text - Id:\" ,len(df['Id'].unique()) - len(df['text'].unique()))","901539f2":"#cleaning publication title to small caps letters\n\ndf['cleaned_pub_title']= df.pub_title.apply(lambda x: clean_text(x))","1178b3a6":"# The new cleaned date columns as cleaned_label and cleaned_pub_titile\ndf.head()","1abc6dc2":"df['pub_title_len'] = df.cleaned_pub_title.apply(lambda x: len(x))","aa8d5a57":"# New column added to know the exact length of each pub_title\ndf","62a4d5d9":"#Checking for the lenght argmax() of pub_title\n\nprint(f'Length: {len(df.iloc[df.pub_title_len.argmax()].cleaned_pub_title)}')\nprint(f'Publication title: {df.iloc[df.pub_title_len.argmax()].cleaned_pub_title}')\n","fc077f3c":"#Checking for the lenght argmin() of pub_title\n\nprint(f'Length: {len(df.iloc[df.pub_title_len.argmin()].cleaned_pub_title)}')\nprint(f'Publication title: {df.iloc[df.pub_title_len.argmin()].cleaned_pub_title}')\n","f0f970af":"df['cl_label_len'] = df.cleaned_label.apply(lambda x: len(x))","1e163774":"#Checking for the lenght argmax() of cleaned_label\n\nprint(f'Length: {len(df.iloc[df.cl_label_len.argmax()].cleaned_label)}')\nprint(f'Publication title: {df.iloc[df.cl_label_len.argmax()].cleaned_label}')","5c2b3053":"#Checking for the lenght argmax() of cleaned_label\n\nprint(f'Length: {len(df.iloc[df.cl_label_len.argmin()].cleaned_label)}')\nprint(f'Publication title: {df.iloc[df.cl_label_len.argmin()].cleaned_label}')","24d80941":"df['dta_title_len'] = df.dataset_title.apply(lambda x: len(x))","7535187a":"df.sample(3)","e7c3d115":"tqdm.pandas()\n\ndf['text'] = df['text'].progress_apply(clean_text)\n","ea9232f8":"tqdm.pandas()\nsample_submission_df['text'] = sample_submission_df['Id'].progress_apply(get_text)","94a3e5e2":"sample_submission_df","b2f021da":"df['text_len'] = df.text.apply(lambda x: len(x))\ndf.sample(3)","0f731761":"df['Id'].value_counts().to_frame()","e0bb507a":"filt = df['Id'].str.contains(\"430aa11c-0283-411b-8edc-08f5df3db258\", na=False)\ndf.loc[filt, 'Id'].value_counts()","1692bfdc":"df1 = df.loc[filt]\n#df.loc[filt, 'cleaned_label'].to_frame()\ndf1[['Id', 'dataset_title', 'cleaned_label','text_len', 'pub_title_len']]","6a8d8f6b":"filte = df['Id']==\"335a83a5-0fc0-4820-881b-5e086deb2894\"\ndf.loc[filte]","f329b164":"from sklearn.model_selection import train_test_split\ndev, val = train_test_split(df, test_size=0.1, random_state=0)\nprint(\"Development: \", dev.shape)\nprint(\"Validation: \", val.shape)","d6976de9":"\ndef clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\ntemp_0 = [x.lower() for x in df['dataset_title'].unique()]\ntemp_1 = [x.lower() for x in df['cleaned_label'].unique()]\ntemp_2 =  [x.lower() for x in df['Id'].unique()]\nexisting_labels = set(temp_0 + temp_1 + temp_2)\nlabels = existing_labels\nid_list = []\nlables_list = []\ngt =[]\nfor index, row in tqdm(sample_submission_df.iterrows()):\n    sample_text = row['text']\n    row_id = row['Id']\n    temp_df = df[df['text'] == clean_text(sample_text)]\n    cleaned_labels = temp_df['cleaned_label'].to_list()\n    gt.append(cleaned_labels)\n    for known_label in existing_labels:\n        if known_label in sample_text.lower():\n            cleaned_labels.append(clean_text(known_label))\n    cleaned_labels = [clean_text(x) for x in cleaned_labels]\n    cleaned_labels = set(cleaned_labels)\n    lables_list.append(' | '.join(cleaned_labels))\n    id_list.append(row_id)\n","6139ccab":"submission = pd.DataFrame()\nsubmission['Id'] = id_list\nsubmission['prediction_string'] = lables_list\nsubmission['ground_truth'] = gt\n","874f5617":"submission","b3499261":"filt = df['Id'].str.contains('3f316b38-1a24-45a9-8d8c-4e05a42257c6', na = False)\ndf.loc[filt, 'cleaned_label'].to_frame()","271d3c2f":"df_shuffled = df.sample(frac = 1, random_state=42)# shuffle with random_state=42 for reproducibility\ndf_shuffled.sample(3)","c74753dc":"df_shuffled.info()","9cc893f0":"df_shuffled.describe()","17cd2302":"# let visualize  some random training samples\nimport random\nrandom_index = random.randint(0, len(df)-5)  # create random indexes not higher than the total number of samples\nprint(f\"The number of random samples are: {random_index}\\n\")\n\nfor row in df_shuffled[['Id', 'text_len']][random_index:random_index+4].itertuples(): # +10 control the printing of sample\n  _, Id, text_len = row\n  print(f\"text_len: {text_len}\", \"(matched)\") if text_len == text_len  else (\"no match\")\n  print(f\"Id:\\n{Id}\\n\")\n  print(\"---\\n\")","00bfa484":"from sklearn.model_selection import train_test_split\n\n#Use  train test split to split traing data into training a validation sets\ntrain_sentences, val_sentences,train_labels,val_labels = train_test_split(\n    df_shuffled.dataset_title.to_numpy(), \n    df_shuffled.cleaned_label.to_numpy(),\n    test_size = 0.2, # dedicate 10% of samples to validation set\n    random_state = 42 # random state for reproducibility\n)","981f7f9f":"#check the lenght or shape\nprint(train_sentences.shape, val_sentences.shape, train_labels.shape, val_labels.shape)","4bc35c71":"import tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\n#Use the default Textvectorization\n\ntext_vectorizer = TextVectorization(max_tokens = 10000, # how many words in the vocabulary (all of the different words in your text)\n                                    standardize = 'lower_and_strip_punctuation', # how to process text\n                                    split = 'whitespace',# how to split token\n                                    ngrams = None, #create group o n-word?\n                                    output_mode = 'int', #how to map tokens to numbers\n                                    output_sequence_length =  None, ## how long should the output sequence of tokens be?\n                                    pad_to_max_tokens = True)\n\ntext_vectorizer ","7777d9d0":"# find avg # of takens (words) in the training tweets\nround(sum([len(i.split()) for i in train_sentences])\/len(train_sentences))","12d4f04c":"max_vocab_length = 10000 #  max number of words to have in our vocabulary\nmax_length = 5 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n\ntext_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n                                    output_mode='int',\n                                    output_sequence_length=max_length)","b8728044":"# Fit the text vectorizer to the training text\ntext_vectorizer.adapt(train_sentences)\n","24bafc11":"# Create sample sentence and tokenize it\nsample_sentence = \"alzheimer s disease neuroimaging initiative adni\"\ntext_vectorizer([sample_sentence])","7caee243":"# Choose a random sentence from the training dataset and tokenize it\nrandom_sentence = random.choice(train_sentences)\nprint(f\"Original text:\\n{random_sentence}\\\n      \\n\\nVectorized version:\")\ntext_vectorizer([random_sentence])","bccf03e7":"# Get the unique words in the vocabulary\nwords_in_vocab = text_vectorizer.get_vocabulary()\ntop_10_words = words_in_vocab[:10] # most common tokens (notice the [UNK] token for \"unknown\" words)\nbottom_10_words = words_in_vocab[-10:] # least common tokens\nprint(f\"Number of words in vocab: {len(words_in_vocab)}\")\nprint(f\"Top 10 most common words: {top_10_words}\") \nprint(f\"Bottom 10 least common words: {bottom_10_words}\")","00ef4e7f":"from tensorflow.keras import layers\n\nembedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n                             output_dim=128, # set size of embedding vector\n                             embeddings_initializer=\"uniform\", # default, intialize randomly\n                             input_length=max_length) # how long is each input\n\nembedding","fc4e0323":"# Get a random sentence from training set\nrandom_sentence = random.choice(train_sentences)\nprint(f\"Original text:\\n{random_sentence}\\\n      \\n\\nEmbedded version:\")\n\n# Embed the random sentence (turn it into numerical representation)\nsample_embed = embedding(text_vectorizer([random_sentence]))\nsample_embed\nsample_embed [0][0]# look up a single token's embedding","a9d3f114":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\n\n#create a tokenization a modelling pipeline\nmodel_0 = Pipeline([\n                    ('tfidf', TfidfVectorizer()),#convert words to munbers using tfid\n                    ('clf', MultinomialNB())# model the text\n])\n\n#fit the pipeline to the training data\nmodel_0.fit(train_sentences, train_labels)","00f00c29":"muiltinomialB = model_0.score(val_sentences, val_labels)\nprint(f\" Our baseline model achieved an accuracy of: {muiltinomialB}\") #*100:.2f}%\")","e71680fa":"# How about make some predictions for our baseline model\n\nbaseline_preds = model_0.predict(val_sentences)\nbaseline_preds[:2]","4b9388fe":"# function evealuate: accuracy. precission, recall, f1_score\n\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef calculate_results(y_true, y_pred):\n    # Calculate model accuracy\n    model_accuracy = accuracy_score(y_true, y_pred) #* 100\n  # Calculate model precision, recall and f1 score using \"weighted\" average\n    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\",labels=np.unique(y_pred))\n    model_results = {\"accuracy\": model_accuracy,\n                  \"precision\": model_precision,\n                  \"recall\": model_recall,\n                  \"f1\": model_f1}\n    return model_results","5545b3b2":"# Get baseline results\nbaseline_results = calculate_results(y_true=val_labels,\n                                     y_pred=baseline_preds)\noutput = baseline_results","900890d5":"output","a0757a89":"### link to open an interactive dashboard to Viz the dataset \n\nhttps:\/\/jp-tok.dataplatform.cloud.ibm.com\/dashboards\/a2808023-27fa-4eee-9a33-80833d38d30f\/view\/5c38a50117b50fcd54ecc8e407917a067c622255b7bbd557d2d47b495d657497f36e1ac2c82d4b53de105366f6b8155e98","d1854d75":"### Creating an Embedding (layer)","417d6b45":"#### * There are 14,316 unique IDs in the dataset\n#### * There is a difference in total between ID and pub_title of 45, which means that pub_title is slightly smaller than the number of ID when we applied the unique counts. \n#### * There are a total of 45 unique dataset_title and 130 unique dataset_label. Meaning that a single dataset could have multiple labels throughout different publications.\n#### * The difference btw 'text' column and the 'Id' is equal to 15, meaning  that there are slight more IDs than text.","d46238ed":"# 4. Filtering Frq on 'ID' column to understand multiple pub in different datasets","757c3a0f":"#                     ![Screen%20Shot%202021-03-30%20at%202.51.58%20pm.png](attachment:Screen%20Shot%202021-03-30%20at%202.51.58%20pm.png)","2bdf08c4":"# 6. Simple Baseline\/Model ","37167079":"reference: https:\/\/www.kaggle.com\/harshsharma511\/start-to-end-easy-understanding-eda-model","b25515e0":"## 3.1.4 Exploring 'text' column","a794526a":"## 3.1 'pub_title' words exploration & analysis of words within","0f9adb04":"# 5. Split data for training and testing \/ Build Model","0eec0d86":"# 1. About the data","637a3167":"## 3.1.1 Exploring the length of cleaned_pub_title","0b4fcbe7":"# 3. Data Exploration and Manipulation","c617a9df":"![Screen Shot 2021-04-19 at 8.36.05 am.png](attachment:3106512c-6aa8-42d7-92e9-4b1f14e5f967.png)","71c367e7":"## 3.1.5 Lenght of publications (text_len) full publications","f6778ec4":"#### Checking results sample","52a3e973":"# 2. Some Quick General Visualizations \/ Interactions","cbc6924e":"## 3.1.3 Calculating the len dataset_title as dta_title_len","957d0b70":"## 3.1.2 Calculating lenght: cl_label_len on cleaned_label","1a9fe436":"#### Text Tokenization","e306da5d":"4 main pieces of data:\n\n* train.csv: The CSV file containing all the metadata of the publications, such as their title and the dataset they utilize. \n* train: The directory containing the actual publications that are referenced in train.csv in JSON format. \n* test: The directory containing the actual publications that will be used for testing purposes (thus, with no ground truth CSV file available). \n* sample_submission.csv: The CSV file containing all the publications IDs in the test set, for which we'll have to populate the prediction column."}}