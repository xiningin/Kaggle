{"cell_type":{"9ced4948":"code","2595d0dd":"code","6b208c9b":"code","c2d9a989":"code","a2e6a58d":"code","19724caf":"code","4b428caf":"code","5b0a16f3":"code","49d3fcf7":"code","6208942e":"code","e89c4768":"markdown","ea2b52f9":"markdown","f44ecee9":"markdown","20186624":"markdown","170613c9":"markdown","c5c9e07a":"markdown","e87a8c42":"markdown"},"source":{"9ced4948":"import transformers\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport pandas as pd \nimport torch\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport numpy as np\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","2595d0dd":"\nMAX_LEN = 512\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 4\nEPOCHS = 10\nBERT_PATH = \"..\/input\/bert-base-uncased\/\"\nMODEL_PATH = \"model.bin\"\nTRAINING_FILE = \"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\"\nTOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)","6b208c9b":"class BERTDataset:\n    def __init__(self, review, target):\n        self.review = review\n        self.target = target\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n\n    def __len__(self):\n        return len(self.review)\n\n    def __getitem__(self, item):\n        review = str(self.review[item])\n        review = \" \".join(review.split())\n\n        inputs = self.tokenizer.encode_plus(\n            review,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n\n        return {\n            \"ids\": torch.tensor(ids, dtype=torch.long),\n            \"mask\": torch.tensor(mask, dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n            \"targets\": torch.tensor(self.target[item], dtype=torch.float),\n        }","c2d9a989":"class BERTBaseUncased(nn.Module):\n    def __init__(self):\n        super(BERTBaseUncased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n        self.bert_drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768,1)\n\n    def forward(self, ids, mask, token_type_ids):\n        _, o2 = self.bert(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n        bo = self.bert_drop(o2)\n        output = self.out(bo)\n        return output","a2e6a58d":"def loss_fn(outputs, targets):\n    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n\ndef train_fn(data_loader, model, optimizer, device, scheduler):\n    model.train()\n\n    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets = d[\"targets\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n\n        optimizer.zero_grad()\n        outputs = model(\n            ids = ids,\n            mask = mask,\n            token_type_ids = token_type_ids\n        )\n\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        \n        optimizer.step()\n        scheduler.step()\n\n\ndef eval_fn(data_loader, model, device):\n    model.eval()\n    fin_outputs = []\n    fin_targets = []\n    with torch.no_grad():\n        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            outputs = model(\n                ids = ids,\n                mask = mask,\n                token_type_ids = token_type_ids\n            )\n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n    return fin_outputs, fin_targets","19724caf":"dfx = pd.read_csv(TRAINING_FILE).fillna(\"none\")\ndfx.sentiment = dfx.sentiment.apply(\n    lambda x: 1 if x == \"positive\" else 0\n)\ndf_train, df_valid_and_test = model_selection.train_test_split(\n    dfx,\n    test_size=0.1,\n    random_state=42,\n    stratify=dfx.sentiment.values\n)\n\ndf_train = df_train.reset_index(drop=True)\ndf_valid, df_test = model_selection.train_test_split(df_valid_and_test,test_size=0.5,random_state=42)\ndf_valid = df_valid.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\ntrain_dataset = BERTDataset(\n    review=df_train.review.values,\n    target=df_train.sentiment.values\n)\ntrain_data_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=TRAIN_BATCH_SIZE,\n    num_workers=4,\n)\n\nvalid_dataset = BERTDataset(\n    review=df_valid.review.values,\n    target=df_valid.sentiment.values\n)\n\nvalid_data_loader = torch.utils.data.DataLoader(\n    valid_dataset,\n    batch_size=VALID_BATCH_SIZE,\n    num_workers=1,\n)\ntest_dataset = BERTDataset(\n    review=df_test.review.values,\n    target=df_test.sentiment.values\n)\n\ntest_data_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=VALID_BATCH_SIZE,\n    num_workers=1,\n)\n","4b428caf":"df_train.shape, df_valid.shape, df_test.shape","5b0a16f3":"class_names = ['negative', 'positive']\nax = sns.countplot(dfx.sentiment)\nplt.xlabel('review sentiment')\nax.set_xticklabels(class_names);","49d3fcf7":"def run(train_data_loader, valid_data_loader):\n    \n    device = torch.device(\"cuda\")\n    model = BERTBaseUncased()\n    model.to(device)\n\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.001}\n    ]\n\n    num_training_steps = int(len(df_train) \/ TRAIN_BATCH_SIZE * EPOCHS)\n    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps\n    )\n\n    model = nn.DataParallel(model)\n\n    best_accuracy = 0\n    for epoch in range(EPOCHS):\n        train_fn(train_data_loader, model, optimizer, device, scheduler) \n        outputs, targets = eval_fn(valid_data_loader, model, device) \n        outputs = np.array(outputs) >= 0.5\n        accuracy = metrics.accuracy_score(targets, outputs)\n        print(f\"Accuracy Score = {accuracy}\")\n\n        if accuracy > best_accuracy:\n            torch.save(model.state_dict(), MODEL_PATH)\n            print(best_accuracy)\n            best_accuracy = accuracy\n\n\n","6208942e":"if __name__ == \"__main__\":\n    run(train_data_loader, valid_data_loader)","e89c4768":"# create a classifier that uses the BERT model","ea2b52f9":"# **import libraries**","f44ecee9":"# **Define constants**","20186624":"# **PyTorch dataset**","170613c9":"# **Training**","c5c9e07a":"# **Loss, Train and eval fonction**","e87a8c42":"# **Data preprocessing**"}}