{"cell_type":{"dfbc2fc0":"code","6fca5d3c":"code","74f51b28":"code","1cbc0624":"code","96618092":"code","94356dbd":"code","952aa4cf":"code","0cea7b4a":"code","08e87ebb":"code","daa058f0":"code","849a8667":"code","4a1ec51c":"code","04905722":"code","cbf05520":"code","78c4e08f":"code","1c78db7d":"code","40f7e833":"code","4891eddd":"code","f9ccf0bd":"code","19a7b663":"code","08dcdd33":"code","670bce29":"code","7e3d0e96":"code","e26ea21f":"code","6094e59f":"markdown","3e942aa0":"markdown","cd834cad":"markdown","075cf730":"markdown","d5253fae":"markdown","c5432751":"markdown","97dadc6b":"markdown","311079d5":"markdown","758486ba":"markdown","43a9c662":"markdown","31bbb892":"markdown","675089f6":"markdown","3d592419":"markdown"},"source":{"dfbc2fc0":"import numpy as np\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os,cv2\nfrom IPython.display import Image\nfrom tqdm import tqdm, tqdm_notebook\nfrom keras.preprocessing import image\nfrom keras import optimizers\nfrom keras import layers\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.applications.imagenet_utils import preprocess_input\nfrom keras.callbacks import ModelCheckpoint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom keras import regularizers\nfrom keras.preprocessing.image import ImageDataGenerator\nprint(os.listdir(\"..\/input\"))","6fca5d3c":"train_dir=\"..\/input\/train\/train\"\ntest_dir=\"..\/input\/test\/test\"\ntrain=pd.read_csv('..\/input\/train.csv')\n\ndf_test=pd.read_csv('..\/input\/sample_submission.csv')\ntrain.has_cactus=train.has_cactus.astype(str)","74f51b28":"train.tail()","1cbc0624":"train['has_cactus'].value_counts()\n","96618092":"train_datagen = ImageDataGenerator(featurewise_center=False, samplewise_center=False, \n                             featurewise_std_normalization=False, \n                             samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, \n                             rotation_range= 55, \n                             width_shift_range=0.2, \n                             height_shift_range=0.2, \n                             brightness_range=None, \n                             shear_range=0.0, zoom_range=0.0, \n                             channel_shift_range=0.0, \n                             fill_mode='nearest',\n                             cval=0.0, \n                             horizontal_flip=True, \n                             vertical_flip=False, \n                             rescale=1.\/255 , \n                             preprocessing_function=None, data_format=None, validation_split=0.0, dtype=None)","94356dbd":"valid_datagen = ImageDataGenerator(rescale=1.\/255)","952aa4cf":"batch_size = 128","0cea7b4a":"train_generator = train_datagen.flow_from_dataframe(train[:15000], directory=train_dir, x_col='id', y_col='has_cactus', \n                    target_size=(64, 64), color_mode='rgb', classes=None, \n                    class_mode='binary', batch_size=batch_size, \n                    shuffle=True, seed=None, \n                    save_to_dir=None, save_prefix='', save_format='png', \n                    subset=None, interpolation='nearest', drop_duplicates=True)","08e87ebb":"valid_generator = valid_datagen.flow_from_dataframe(train[15000:], directory=train_dir, x_col='id', y_col='has_cactus', \n                    target_size=(64, 64), color_mode='rgb', classes=None, \n                    class_mode='binary', batch_size=batch_size, \n                    shuffle=True, seed=None, \n                    save_to_dir=None, save_prefix='', save_format='png', \n                    subset=None, interpolation='nearest', drop_duplicates=True)","daa058f0":"model = Sequential()","849a8667":"model.add(layers.Conv2D(filters = 32, kernel_size = (3,3), input_shape = (64, 64, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D(pool_size = (2,2), padding = 'same'))\nmodel.add(layers.Dropout(0.3))\n\nmodel.add(layers.Conv2D(filters = 64, kernel_size= (3,3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D(pool_size = (2,2), padding = 'same'))\nmodel.add(layers.Dropout(0.3))\n\n\nmodel.add(layers.Conv2D(filters = 128, kernel_size= (3,3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D(pool_size = (2,2), padding = 'same'))\nmodel.add(layers.Dropout(0.3))\n\nmodel.add(layers.Conv2D(filters = 128, kernel_size= (3,3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D(pool_size = (2,2), padding = 'same'))\nmodel.add(layers.Dropout(0.3))\n\nmodel.add(layers.Conv2D(filters = 256, kernel_size= (3,3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D(pool_size = (2,2), padding = 'same'))\nmodel.add(layers.Dropout(0.3))\n\nmodel.add(layers.Flatten())\n\nmodel.add(layers.Dense(128, activation = 'relu'))\nmodel.add(layers.Dense(1, activation = 'sigmoid'))\n","4a1ec51c":"model.summary()","04905722":"optim = Adam(lr=0.0022, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)","cbf05520":"model.compile(optimizer = optim, loss='binary_crossentropy', metrics=['accuracy'])","78c4e08f":"filepath = \"best_model.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncall_backs_list = [checkpoint]\n","1c78db7d":"max_epochs = 60\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch = 100,\n    epochs = max_epochs,\n    validation_data = valid_generator,\n    callbacks = call_backs_list,\n    validation_steps = 50.\n)","40f7e833":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'g', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'g', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","4891eddd":"model.load_weights(\"best_model.hdf5\")","f9ccf0bd":"model.compile(optimizer = optim, loss='binary_crossentropy', metrics=['accuracy'])","19a7b663":"un_test_img=[]\ncount=0\nfor i in os.listdir(\"..\/input\/test\/test\/\"):\n    un_test_img.append(i)\n    count+=1\nun_test_image=[]\nfor i in tqdm(range(count)):\n    img = image.load_img('..\/input\/test\/test\/'+un_test_img[i], target_size=(64,64,3), grayscale=False)\n    img = image.img_to_array(img)\n    img = img\/255\n    un_test_image.append(img)\nun_test_img_array = np.array(un_test_image)","08dcdd33":"len(un_test_img)","670bce29":"output = model.predict_classes(un_test_img_array)","7e3d0e96":"submission_save = pd.DataFrame()\nsubmission_save['id'] = un_test_img\nsubmission_save['has_cactus'] = output\nsubmission_save.to_csv('submission.csv', header=True, index=False)","e26ea21f":"pd.read_csv('submission.csv')","6094e59f":"## Reading the Images\n\n* The main part is to handle the image data.\n* We need to take input of the image file\n* Decode the JPEG data into RGB channels\n* Then normalize it by dividing by 255.\n* We can do data-augmentation on top of that.","3e942aa0":"* First we need to compile the model.\n* Compiling the model has three parts, loss_function, optimizer and then the metrics.\n* We want to minimize the binary_cross_entropy loss, since we are dealing with binary classification problem.\n* Also we are using Adam optimizer.\n* You could try different optimizer such as RMSProp or SGD also. \n* https:\/\/keras.io\/optimizers\/ for details of all optimizers available\n* To learn more http:\/\/ruder.io\/optimizing-gradient-descent\/\n* It is not required to tune Nadam hyper-parameters so we can ignore them as of now.\n* We bother about accuracy score so we are using the metrics to be equalt to accuracy,","cd834cad":"## Making A CNN Model.\n\n### How to make a CNN Model ?\nBulding a CNN is just like building our brain to recognize patterns. We try to focus only on certain featurs of image to understand that it is an aerial cactus. E.g seeing the green colour and spikes, considering its thickness might mean that we are seeing an aerial cactus.\n\nSo we need to reduce the feature maps that are given in the image. This is done by Conv2D layer. We glide the 3x3 kernel over the entire image of size 256 x 256 multiple times. Thus we are reducing the dimensions of image while learning about them.\n\nWe are thus making an abstract identification and description of an image.\n\nSome points that made me think that we need this architecture: -\n\n* Because we have very high number of images we need a deep model.\n* We need connected layers of Conv2D. Which will extract features from a small 3x3 kernel.\n* Also we need to increase the number of filters as we go deep to extract more features.\n* We need to step-wize reduce the feature maps using MaxPooling operations.\n* Since we are training a large number of parameters, we need to take care about overfitting.\n* Also not to overfit we can include the Dropout layer. It has 0 parameters as well.\n* Then we need to memorize a few attributes of Aerial Cactus. Sound odd but think about it. While learning patterns we also need a small amount of rote memory to recognize the pattern. So we need a densely connected layer as well.\n* The activation function is relu in all Convulational layers.\n* Since we want to predict the probabilities our last layer should be Softmax Layer.\n","075cf730":"* Usually use batch_sizes in multiples of 2.\n* Fits well into the GPU","d5253fae":"* Let us get a brief idea about the dataset that is given.\n* We have total of 17500 cactii images here. \n* We have a medium sized data-set.\n* * Looks like we have a biased data-set. We have has_cactus for more data equal to  1.\n* This indicates that we have more images which have cactus.","c5432751":"* We can rotate the images by certain angle. (Using rotation_range)\n* We can also shift the images by a certain distance. (Using the width_shift_range and height_shift_range)\n* We can also try to horizontally flip the images.\n* I am not vertically flipping the image as it may mean an absurd view of aerial cactus.\n* Also finally we are rescaling it by a factor of 1.\/255 to normalize the pixel values.","97dadc6b":"* Seems like after 50 our model does not learn much.\n* We can try optimizing the learning rate, batch_size and also the epochs.\n* Since we have checkpointed the model at point where validation accuracy is higher, the extra training after 50 does not matter.","311079d5":"* Let us begin this task. We wll develop simple model. Beginner friendly code !!!! YAY !!!!\n* Doing the necessary imports.\n* We will be using Keras with Data Augementation\n* Let us build model from scratch instead of Transfer Learning","758486ba":"* Now we have to read the images with their labels which are in the train dataframe\n* To do this Keras provides us the way of loading from dataframe using flow_from_dataframe\n* Check it here https:\/\/keras.io\/preprocessing\/image\/\n* Also for a tutorial head here ->   https:\/\/medium.com\/@vijayabhaskar96\/tutorial-on-keras-imagedatagenerator-with-flow-from-dataframe-8bd5776e45c1","43a9c662":"* Let us have a quick summary of the model\n* So there are 1,754,114 parameters to be tuned.\n* Most of them come due to the fully connected layer.\n* The model satisfies to our need, let us train this.\n","31bbb892":"* Note while validating.\n* We do not want to check on augemented data.\n* We avoid checking on that by creating new validation generator and making use of it directly.","675089f6":"* Let us check point the model as well.\n* We need only the best model, that is one which reduces the loss significantly or improves accuracy.\n* We will store the weights of models in .hdf5 file.\n* Then load this model and make predictions.","3d592419":"* This also splits the data into two parts.\n* We have 15000 images in train part.\n* We have 2500 images in validation part."}}