{"cell_type":{"cdb65b4c":"code","09fef39e":"code","689a5444":"code","f1a484fd":"code","5a1e9aa9":"code","f44decac":"code","f643ef80":"code","76f3aace":"code","9135c878":"code","8a620bac":"code","7ab1dbe5":"code","79127ac4":"code","487445b3":"code","34c8acbd":"code","358c9190":"code","cb033ced":"code","74f0a5d3":"markdown"},"source":{"cdb65b4c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","09fef39e":"from pathlib import Path\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler","689a5444":"data_dir = '..\/input\/house-prices-advanced-regression-techniques'\ntrain_data = pd.read_csv(Path(data_dir, 'train.csv'))\ntest_data = pd.read_csv(Path(data_dir, 'test.csv'))","f1a484fd":"print(train_data.columns)\nprint(train_data.shape)\nprint(train_data.head())","5a1e9aa9":"# Deal with missing values\nfor col in ('Alley','Utilities','MasVnrType','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n            'BsmtFinType2','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond',\n           'PoolQC','Fence','MiscFeature'):\n    train_data[col]=train_data[col].fillna('None')\n    test_data[col]=test_data[col].fillna('None')\n    \n    \nfor col in ('Electrical','MSZoning','Exterior1st','Exterior2nd','KitchenQual','SaleType','Functional'):\n    train_data[col]=train_data[col].fillna(train_data[col].mode()[0])\n    test_data[col]=test_data[col].fillna(test_data[col].mode()[0])\n    \n    \n\nfor col in ('MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath',\n            'GarageYrBlt','GarageCars','GarageArea'):\n    train_data[col]=train_data[col].fillna(0)\n    test_data[col]=test_data[col].fillna(0)\n\n    \ntrain_data['LotFrontage']=train_data['LotFrontage'].fillna(train_data['LotFrontage'].mean())\ntest_data['LotFrontage']=test_data['LotFrontage'].fillna(test_data['LotFrontage'].mean())\n\n\n# train_data= train_data[train_data['GrLivArea'] < 4000]","f44decac":"features = [\n        'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n       'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n       'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n       'MoSold', 'YrSold', 'Neighborhood', 'SalePrice'\n]\n\ntrain_data = train_data.loc[:, features]\ntrain_data_price = train_data['SalePrice']\ntrain_data = train_data.loc[:, features[:len(features)-1]]\n\ntest_data_id = test_data['Id']\ntest_data = test_data.loc[:, features[:len(features)-1]]","f643ef80":"# Hot encode data with pandas\ntrain_data = pd.get_dummies(train_data)\ntest_data = pd.get_dummies(test_data)","76f3aace":"# Scale training and test sets with standard scaler \n# in order to have faster training (bigger gradient descent)\nscaler = MinMaxScaler()\ntrain_data_scaled = scaler.fit_transform(train_data.values)\ntrain_data = pd.DataFrame(train_data_scaled, columns=train_data.columns)\n\n\ntest_data_scaled = scaler.transform(test_data.values)\ntest_data = pd.DataFrame(test_data_scaled, columns=test_data.columns)","9135c878":"# Now we need to create our labels (y) and also create the validation data\ntrain_labels = train_data_price\n\ntrain_data, validation_data, train_labels, validation_labels = train_test_split(train_data, train_labels, test_size = 0.1, random_state=2)","8a620bac":"print(train_data.shape)\nprint(train_labels.shape)\nprint(validation_data.shape)\nprint(validation_labels.shape)","7ab1dbe5":"# Time to create the Deep Learning model\n\n# Prepare the DNN, something like this should do\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(64, kernel_initializer='lecun_normal', activation='selu', input_shape=(train_data.shape[1], )),\n    tf.keras.layers.Dense(64, kernel_initializer='lecun_normal', activation='selu'),\n    tf.keras.layers.Dense(64, kernel_initializer='lecun_normal', activation='selu'),\n    tf.keras.layers.Dense(64, kernel_initializer='lecun_normal', activation='selu'),\n    tf.keras.layers.Dense(64, kernel_initializer='lecun_normal', activation='selu'),\n    tf.keras.layers.Dense(64, kernel_initializer='lecun_normal', activation='selu'),\n    tf.keras.layers.Dense(1, kernel_initializer='normal', activation='linear')\n ])\n    \nmodel.summary()","79127ac4":"initial_learning_rate = 0.001\n\nlr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=10)\nearly_stop = tf.keras.callbacks.EarlyStopping(patience=100, restore_best_weights=True)\n\nmodel.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=initial_learning_rate), loss=tf.keras.losses.Huber())\n\nepochs = 1000\nhistory = model.fit(x=train_data, y=train_labels, \n                    batch_size=16,\n                    validation_data=(validation_data, validation_labels),\n                    epochs=epochs,\n                    callbacks=[\n                        early_stop,\n                        lr_scheduler\n                    ])","487445b3":"import matplotlib.pyplot as plt\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","34c8acbd":"from sklearn.metrics import mean_squared_error\n\npred = model.predict(validation_data)\nmse = mean_squared_error(pred, validation_labels)\nnp.sqrt(mse)","358c9190":"predictions = model.predict(test_data)","cb033ced":"print(predictions)\nsub_data = pd.DataFrame(predictions, columns=['SalePrice'])\nsub_data.insert(0, 'Id', test_data_id)\n\nsub_data.to_csv('\/kaggle\/working\/submission.csv', index=False)","74f0a5d3":"# Tensorflow"}}