{"cell_type":{"fa0f05d0":"code","e739cf2e":"code","d4aed7bf":"code","5d086035":"code","c1ca59aa":"code","af01390f":"code","a104d2c2":"code","e6c96f65":"code","f5943e40":"code","5f7717ae":"code","cb2b8231":"code","f74bc979":"code","36bc779c":"code","077d18ef":"code","43774cc0":"code","a291c943":"code","8735b112":"code","0ae42c83":"code","7319c8dc":"code","9ecb7987":"code","4315222b":"code","5f9dcbf1":"code","6cabbd77":"code","d63cd2ec":"code","b9ecd30d":"code","b2fcfea1":"code","66fe49ac":"code","06d82e2c":"code","4fe3d8ba":"code","1d194871":"code","a63d2c80":"code","60646c75":"code","74f14adf":"code","4530c8be":"code","490c73a5":"code","7f0b16b2":"code","dbc83167":"code","d4cf85ef":"code","07961acb":"code","d90eb726":"code","9b4d3100":"code","d4ede6f8":"code","aa3dffe8":"code","06dcdbdf":"code","f1c7df34":"code","d21e37a0":"code","097324f2":"code","b294cc62":"code","1462528f":"code","074add23":"markdown","e88b8021":"markdown","cc709f83":"markdown","8bd207d5":"markdown","5c5e6280":"markdown","7de77027":"markdown","02d6c65a":"markdown","f3b00132":"markdown","b4b7a329":"markdown","a1ea2cc5":"markdown","283693e6":"markdown","fba8d943":"markdown","7d4857e2":"markdown","b7a3d2c6":"markdown","d056ad08":"markdown","6b72c1ef":"markdown","a1b9abee":"markdown","1b95766e":"markdown","fcf3801a":"markdown","0820eae5":"markdown","4b74d93b":"markdown","99bb1b16":"markdown","522d6b27":"markdown"},"source":{"fa0f05d0":"import pandas as pd\nimport numpy as np\nimport nltk\nfrom bs4 import BeautifulSoup\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom snowballstemmer import TurkishStemmer\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB\nimport re\n\npd.options.display.max_colwidth = 280 # For showing big tweets\n%matplotlib inline\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(sklearn.__version__)\nprint(matplotlib.__version__)\nprint(np.__version__)\nprint(pd.__version__)\nprint(nltk.__version__)","e739cf2e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nturkcell_data = pd.read_csv(\"..\/input\/twitter-turkcell-data\/turkcell.csv\")\nturkcell_data = turkcell_data.drop(['created_at', 'date', 'time', 'username'], axis = 1)\nturkcell_data.columns = ['Tweets', 'Sentiment']\nturkcell_data.Sentiment.replace([0, 4], ['Negative', 'Positive'], inplace = True)\nturkcell_data.head()","d4aed7bf":"turkcell_data.describe()","5d086035":"turkcell_data['Sentiment'].value_counts()","c1ca59aa":"turkcell_data['TextSize'] = [len(t) for t in turkcell_data.Tweets]\nturkcell_data.head()","af01390f":"turkcell_data['TextSize'].mean()","a104d2c2":"plt.boxplot(turkcell_data.TextSize) # plot TextSize column\nplt.show()","e6c96f65":"target_tdS = Counter(turkcell_data.Sentiment)\n\nplt.figure(figsize=(16,8))\nplt.bar(target_tdS.keys(), target_tdS.values())\nplt.title(\"Dataset labels distribuition\")","f5943e40":"tweets_text = turkcell_data.Tweets.str.cat()\nemoticons = set(re.findall(r\" ([xX:;][-']?.) \", tweets_text))\nemoticons_count = []\nfor emot in emoticons:\n    emoticons_count.append((tweets_text.count(emot), emot))\nsorted(emoticons_count, reverse=True)","5f7717ae":"happy_emot = r\" ([xX;:]-?[dD)]|:-?[\\)]|[;:]|[:')][pP]) \"\nsad_emot = r\" (:'?[\/|\\(]) \"\nprint(\"Happy emoticons:\", set(re.findall(happy_emot, tweets_text)))\nprint(\"Sad emoticons:\", set(re.findall(sad_emot, tweets_text)))","cb2b8231":"#Tokenization of text\ntokenizer = ToktokTokenizer()\n\n#Setting Turkish stopwords\nstopword_list = open('..\/input\/turkce-stop-words\/turkce-stop-words', 'r').read().split()","f74bc979":"def most_used_words(text):\n    tokens = tokenizer.tokenize(text)\n    frequency_dist = nltk.FreqDist(tokens)\n    print(\"There is %d different words\" % len(set(tokens)))\n    return sorted(frequency_dist, key=frequency_dist.__getitem__, reverse=True)","36bc779c":"sorted(most_used_words(turkcell_data.Tweets.str.cat())[:150])","077d18ef":"mw = most_used_words(turkcell_data.Tweets.str.cat())\nmost_words = []\nfor w in mw:\n    if len(most_words) == 1000:\n        break\n    if w in stopword_list:\n        continue\n    else:\n        most_words.append(w)","43774cc0":"sorted(most_words[:150])","a291c943":"# Remove the html\ndef strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n# Remove the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\n# Remove the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    return text\n\nturkcell_data['Tweets'] = turkcell_data['Tweets'].apply(denoise_text)","8735b112":"def stemWord(text):\n    return text.lower()\n\n# Remove usernames\ndef removeUsernames(text):\n    return re.sub('@[^\\s]+', '', text)\n\n# Remove hashtags\ndef removeHashtags(text):\n    return re.sub(r'#[^\\s]+', ' ', text)\n\n# Remove punctuation\ndef removePunctuation(text):\n    return re.sub(r'[^\\w\\s]', ' ', text)\n\n# Remove single character\ndef singleCharacterRemove(text):\n    return re.sub(r'(?:^| )\\w(?:$| )', ' ', text)\n                  \n# Remove emoticon\ndef stripEmoji(text):\n    emoji = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n    return emoji.sub(r'', text)\n\ndef splitIntoStem(text):\n    text = stemWord(text)\n    text = removeUsernames(text)\n    text = removeHashtags(text)\n    text = removePunctuation(text)\n    text = singleCharacterRemove(text)\n    text = stripEmoji(text)\n    return text\n\n\nturkcell_data['Tweets'] = turkcell_data['Tweets'].apply(splitIntoStem)","0ae42c83":"# Remove pic.twitter\ndef picTwitter(text):\n    pic_pat = r'pic.[^ ]+'\n    text = re.sub(pic_pat, '', text)\n    return text\n    \nturkcell_data['Tweets'] = turkcell_data['Tweets'].apply(picTwitter)\n\n# Remove pic.twitter2\ndef picTwitter2(text):\n    pic_pat2 = r'com.[^ ]+'\n    text = re.sub(pic_pat2, '', text)\n    return text\n\nturkcell_data['Tweets'] = turkcell_data['Tweets'].apply(picTwitter2)","7319c8dc":"'''# Remove turkcell\ndef operator(text):\n    t = r'turkcell'\n    v = r'vodafone'\n    tt = r'turktelekom'\n    so = r'superonline'\n    sy = r'superyardim'\n    text = re.sub(t, '', text)\n    text = re.sub(v, '', text)\n    text = re.sub(tt, '', text)\n    return text\n\n\nturkcell_data['Tweets'] = turkcell_data['Tweets'].apply(operator)'''","9ecb7987":"turkcell_data['Tweets']","4315222b":"turkcell_data.head()","5f9dcbf1":"turkcell_data['TextSizeBeforeRemoveStopWords'] = [len(t) for t in turkcell_data.Tweets]\nturkcell_data.head()","6cabbd77":"# Set Stopwords to Turkish\nstop = set(stopword_list)\n\ndef remove_stopwords(text, is_lower_case = True):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text\n\nturkcell_data['Tweets'] = turkcell_data['Tweets'].apply(remove_stopwords)","d63cd2ec":"turkcell_data['TextSizeAfterRemoveStopWords'] = [len(t) for t in turkcell_data.Tweets]\nturkcell_data.head()","b9ecd30d":"turkStem = TurkishStemmer()\n\n# Stemming\ndef simple_stemmer(text):\n    ss = TurkishStemmer()\n    text = ' '.join([ss.stemWord(word) for word in text.split()])\n    return text\n\nturkcell_data['Tweets'] = turkcell_data['Tweets'].apply(simple_stemmer)","b2fcfea1":"turkcell_data['TextSizeStemming'] = [len(t) for t in turkcell_data.Tweets]\nturkcell_data.head()","66fe49ac":"turkcell_data.Sentiment.replace(['Negative', 'Positive'], [0, 1], inplace = True)\nturkcell_data.head()","06d82e2c":"sent = turkcell_data['Sentiment']\ntw = turkcell_data['Tweets']\n\n# Split the data \/ Ratio is 80:20\nX_train, X_test, y_train, y_test = train_test_split(tw, sent, test_size = 0.20, random_state= 1)\n\n\n# X_train is the tweets of training data, \n# X_test is the testing tweets which we have to predict \n# y_train is the sentiments of tweets in the traing data\n# y_test is the sentiments of the tweets which we will use to measure the accuracy of the model","4fe3d8ba":"# CountVectorizer for Bag of Words\ncv = CountVectorizer(min_df = 0, max_df = 1, binary = False, ngram_range = (1, 3))\n\n# Transformed train tweets\ncv_train_tweets = cv.fit_transform(X_train)\n\n# Transformed test tweets\ncv_test_tweets = cv.transform(X_test)\n\nprint('BoW_CV_Train:',cv_train_tweets.shape)\nprint('BoW_CV_Test:',cv_test_tweets.shape)","1d194871":"s = cv_train_tweets[1]\nprint(s)","a63d2c80":"ss = cv_test_tweets[1]\nprint(ss)","60646c75":"# TfidfVectorizer\ntv = TfidfVectorizer(min_df = 0, max_df = 1, use_idf = True, ngram_range = (1, 3))\n\n# Transformed train tweets\ntv_train_tweets = tv.fit_transform(X_train)\n\n# Transformed test tweets\ntv_test_tweets = tv.transform(X_test)\n\nprint('Tfidf_Train:',tv_train_tweets.shape)\nprint('Tfidf_Test:',tv_test_tweets.shape)","74f14adf":"s = tv_train_tweets[1]\nprint(s)","4530c8be":"ss = tv_test_tweets[1]\nprint(ss)","490c73a5":"# Training the Model\nlr = LogisticRegression(penalty = 'l2', max_iter = 500, C = 1.1, random_state = 42)\n\n# Fitting the model for Bag of Words\nlr_bow = lr.fit(cv_train_tweets, y_train)\nprint(lr_bow)\n\n# Fitting the model for TFIDF features\nlr_tfidf = lr.fit(tv_train_tweets, y_train)\nprint(lr_tfidf)","7f0b16b2":"# Predicting the model for Bag of Words\nlr_bow_predict = lr.predict(cv_test_tweets)\nprint(lr_bow_predict)\n\n# Predicting the model for TFIDF features\nlr_tfidf_predict = lr.predict(tv_test_tweets)\nprint(lr_tfidf_predict)","dbc83167":"# Accuracy score for Bag of Words\nlr_bow_score = accuracy_score(y_test, lr_bow_predict)\nprint(\"LR BoW Score :\",lr_bow_score)\n\n# Accuracy score for TFIDF features\nlr_tfidf_score = accuracy_score(y_test, lr_tfidf_predict)\nprint(\"LR TFIDF Score :\",lr_tfidf_score)","d4cf85ef":"# Classification report for Bag of Words\nlr_bow_report = classification_report(y_test, lr_bow_predict, target_names = ['Positive','Negative'])\nprint(lr_bow_report)\n\n# Classification report for TFIDF features\nlr_tfidf_report = classification_report(y_test, lr_tfidf_predict, target_names = ['Positive','Negative'])\nprint(lr_tfidf_report)","07961acb":"# Confusion matrix for Bag of Words\ncm_bow = confusion_matrix(y_test, lr_bow_predict, labels = [1,0])\nprint(cm_bow)\n\n# Confusion matrix for TFIDF features\ncm_tfidf = confusion_matrix(y_test, lr_tfidf_predict, labels = [1,0])\nprint(cm_tfidf)","d90eb726":"# Training the Linear SVM\nsvm = SGDClassifier(loss='hinge', max_iter=500, random_state=42)\n\n# Fitting the SVM for Bag of Words\nsvm_bow = svm.fit(cv_train_tweets, y_train)\nprint(svm_bow)\n\n# Fitting the SVM for TFIDF features\nsvm_tfidf = svm.fit(tv_train_tweets, y_train)\nprint(svm_tfidf)","9b4d3100":"# Predicting the model for Bag of Words\nsvm_bow_predict = svm.predict(cv_test_tweets)\nprint(svm_bow_predict)\n\n# Predicting the model for TFIDF features\nsvm_tfidf_predict = svm.predict(tv_test_tweets)\nprint(svm_tfidf_predict)","d4ede6f8":"# Accuracy score for Bag of Words\nsvm_bow_score = accuracy_score(y_test, svm_bow_predict)\nprint(\"SVM BoW Score :\",svm_bow_score)\n\n# Accuracy score for TFIDF features\nsvm_tfidf_score = accuracy_score(y_test, svm_tfidf_predict)\nprint(\"SVM TFIDF Score:\",svm_tfidf_score)","aa3dffe8":"# Classification report for Bag of Words \nsvm_bow_report = classification_report(y_test, svm_bow_predict, target_names = ['Positive','Negative'])\nprint(svm_bow_report)\n\n# Classification report for TFIDF features\nsvm_tfidf_report = classification_report(y_test, svm_tfidf_predict, target_names = ['Positive','Negative'])\nprint(svm_tfidf_report)","06dcdbdf":"# Confusion matrix for Bag of Words\ncm_bow = confusion_matrix(y_test, svm_bow_predict, labels = [1,0])\nprint(cm_bow)\n\n# Confusion matrix for TFIDF features\ncm_tfidf = confusion_matrix(y_test, svm_tfidf_predict, labels = [1,0])\nprint(cm_tfidf)","f1c7df34":"# Training the model\nmnb = MultinomialNB()\n\n# Fitting the NB for Bag of Words\nmnb_bow = mnb.fit(cv_train_tweets, y_train)\nprint(mnb_bow)\n\n# Fitting the NB for TFIDF features\nmnb_tfidf = mnb.fit(tv_train_tweets, y_train)\nprint(mnb_tfidf)","d21e37a0":"# Predicting the model for Bag of Words\nmnb_bow_predict = mnb.predict(cv_test_tweets)\nprint(mnb_bow_predict)\n\n# Predicting the model for TFIDF features\nmnb_tfidf_predict = mnb.predict(tv_test_tweets)\nprint(mnb_tfidf_predict)","097324f2":"# Accuracy score for Bag of Words\nmnb_bow_score = accuracy_score(y_test, mnb_bow_predict)\nprint(\"MNB BoW Score :\",mnb_bow_score)\n\n# Accuracy score for TFIDF features\nmnb_tfidf_score = accuracy_score(y_test, mnb_tfidf_predict)\nprint(\"MNB TFIDF Score :\",mnb_tfidf_score)","b294cc62":"# Classification report for Bag of Words\nmnb_bow_report = classification_report(y_test, mnb_bow_predict, target_names = ['Positive','Negative'])\nprint(mnb_bow_report)\n\n# Classification report for TFIDF features\nmnb_tfidf_report = classification_report(y_test, mnb_tfidf_predict, target_names = ['Positive','Negative'])\nprint(mnb_tfidf_report)","1462528f":"# Confusion matrix for Bag of Words\ncm_bow = confusion_matrix(y_test, mnb_bow_predict, labels = [1,0])\nprint(cm_bow)\n\n# Confusion matrix for TFIDF features\ncm_tfidf = confusion_matrix(y_test, mnb_tfidf_predict, labels = [1,0])\nprint(cm_tfidf)","074add23":"                                                            turkcell_data[0] Text Size\n\n| # | Original | Preprocess | RemoveStopWords | Stemming |\n|------|------|------|------|\n|TextSize| 55 | 55 | 34 | 30 |","e88b8021":"### Removing StopWords","cc709f83":"## <a class=\"anchor\" id=\"4.3.\">4.3. Text Stemming<\/a>","8bd207d5":"## <a class=\"anchor\" id=\"3.2.\">3.2. Emoticons Used in the Dataset<\/a>","5c5e6280":"# <a class=\"anchor\" id=\"6.\">6. Modelling the Dataset<\/a>","7de77027":"## <a class=\"anchor\" id=\"6.1.\">6.1. Logistic Regression<\/a>","02d6c65a":"## <a class=\"anchor\" id=\"5.1.\">5.1. Split Dataset Intro Train and Test<\/a>","f3b00132":"# <a class=\"anchor\" id=\"3.\">3. Loading Dataset<\/a>","b4b7a329":"# <a class=\"anchor\" id=0.>Contents<\/a>\n\n* [1. Introduction](#1.)\n    * [1.1. Import Liblaries](#1.1.)\n* [2. About Dataset](#2.)\n* [3. Loading Dataset](#3.)\n    * [3.1. Visualize The Dataset](#3.1.)\n    * [3.2. Emoticons Used in the Dataset](#3.2.)\n* [4. Normalization & Preprocessing & Stemming](#4.)\n    * [4.1. Text Normalization](#4.1.)\n    * [4.2. Preprocessing](#4.2.)\n    * [4.3. Removing Stopwords](#4.3.)\n    * [4.4. Text Stemming](#4.4.)\n* [5. Feature Extraction](#5.)\n    * [5.1. Split Dataset into Train and Test](#5.1.)\n    * [5.2. Bag of Words](#5.2.)\n    * [5.3. Term FrequencyxInverse Document Frequency \/ TFxIDF](#5.3.)\n* [6. Modelling the Dataset](#6.)\n    * [6.1. Logistic Regression](#6.1.)\n    * [6.2. SGDClassifier](#6.2.)\n    * [6.3. Naive Bayes](#6.3.)","a1ea2cc5":"# <a class=\"anchor\" id=\"2.\">2. About Dataset<\/a>\n\nThis work was done with 10700 tweets obtained via Twitter. The data were mined using the \"Turkcell\" keyword. (Turkcell: Turkcell is a converged telecommunication and technology services provider, founded and headquartered in Turkey.)\n\nTwitter scraping was carried out using Twitter Intelligence Tool (TWINT) and the Tweepy library.\n\n* Tweepy: https:\/\/www.tweepy.org\/\n* Twint: https:\/\/github.com\/twintproject\/twint","283693e6":"## <a class=\"anchor\" id=\"4.2.\">4.2. Preprocessing<\/a>","fba8d943":"### Labeling the sentiment text","7d4857e2":"# <a class=\"anchor\" id=\"4.\">4. Normalization & Preprocessing & Stemming<\/a>","b7a3d2c6":"## <a class=\"anchor\" id=\"3.1.\">3.1. Visualize the Dataset<\/a>","d056ad08":"### Information about data","6b72c1ef":"# <a class=\"anchor\" id=\"1.\">1. Introduction<\/a>\n\nSentiment analysis (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine. \n","a1b9abee":"## <a class=\"anchor\" id=\"5.3.\">5.3. Term FrequencyxInverse Document Frequency \/ TFxIDF<\/a>","1b95766e":"# <a class=\"anchor\" id=\"5.\">5. Feature Extraction<\/a>","fcf3801a":"## <a class=\"anchor\" id=\"1.1.\">1.1. Import Liblaries<\/a>","0820eae5":"## <a class=\"anchor\" id=\"5.2.\">5.2. Bag of Words<\/a>","4b74d93b":"## <a class=\"anchor\" id=\"6.2.\">6.2. SGDClassifier<\/a>","99bb1b16":"## <a class=\"anchor\" id=\"6.3.\">6.3. Naive Bayes<\/a>","522d6b27":"## <a class=\"anchor\" id=\"4.1.\">4.1. Text Normalization<\/a>"}}