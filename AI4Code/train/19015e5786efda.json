{"cell_type":{"3d3f92dc":"code","3db28547":"code","3cd119f3":"code","09fdf2c8":"code","550c8b31":"code","70993f2e":"code","f9d0191b":"code","2768bd3e":"code","627eee03":"code","b34ba535":"code","130772f2":"code","dc7f8d08":"code","1884ea66":"code","a675d30e":"code","c6ce5c0c":"code","b452cc3b":"code","cc4dfffe":"code","de56ea04":"code","e0d5835b":"code","d96db31d":"code","12f7223e":"code","0d7f091c":"code","b81dc617":"code","0d5c9503":"code","978dc166":"code","6aad1ca3":"markdown","09322dcf":"markdown","289fae97":"markdown","3664f6cf":"markdown","bf242c5c":"markdown"},"source":{"3d3f92dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3db28547":"sxd_processed_df = pd.read_csv('\/kaggle\/input\/stock-exchange-data\/indexProcessed.csv')\nsxd_processed_df.head()","3cd119f3":"sxd_indexData_df = pd.read_csv('\/kaggle\/input\/stock-exchange-data\/indexData.csv')\nsxd_indexData_df.head()","09fdf2c8":"sxd_indexInfo_df = pd.read_csv('\/kaggle\/input\/stock-exchange-data\/indexInfo.csv')\nsxd_indexInfo_df.head()","550c8b31":"sxd_Data_Info_merged_df = pd.merge(sxd_indexData_df,sxd_indexInfo_df, on = \"Index\" )\nsxd_Data_Info_merged_df.head()","70993f2e":"sxd_processed_Info_merged_df = pd.merge(sxd_processed_df,sxd_indexInfo_df, on = \"Index\" )\nsxd_processed_Info_merged_df.head()","f9d0191b":"sxd_Data_Info_merged_df.describe()","2768bd3e":"sxd_Data_Info_merged_df.head()","627eee03":"sxd_Data_Info_merged_df.isnull().sum()","b34ba535":"sxd_Data_Info_merged_df.head()","130772f2":"sxd_Data_Info_merged_df.fillna(method='ffill', inplace=True)","dc7f8d08":"sxd_Data_Info_merged_df.isnull().sum()","1884ea66":"sxd_processed_Info_merged_df.fillna(method='ffill', inplace=True)\nsxd_processed_Info_merged_df.isnull().sum()","a675d30e":"sxd_Data_Info_merged_df.describe()\n","c6ce5c0c":"sxd_Data_Info_merged_df.columns","b452cc3b":"sxd_Data_Info_merged_df.corr()","cc4dfffe":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np; np.random.seed(0)\n\nN_sxd_Data_Info_merged_df = sxd_Data_Info_merged_df.select_dtypes(include='number')\nN_sxd_Data_Info_merged_df.head()\nN_sxd_Data_Info_merged_df.corr()","de56ea04":"sns.pairplot(N_sxd_Data_Info_merged_df)","e0d5835b":"sxd_processed_Info_merged_df.corr()","d96db31d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np; np.random.seed(0)\n\nN_sxd_Processed_Info_merged_df = sxd_processed_Info_merged_df.select_dtypes(include='number')\nN_sxd_Processed_Info_merged_df.head()\nN_sxd_Processed_Info_merged_df.corr()","12f7223e":"sns.pairplot(N_sxd_Processed_Info_merged_df)","0d7f091c":"from sklearn.model_selection import train_test_split\n\n#splitting data into training data and testing data\nX_train1, X_test1, y_train1, y_test1 = train_test_split(\n    N_sxd_Processed_Info_merged_df.drop(['CloseUSD'], axis=1),\n    N_sxd_Processed_Info_merged_df.CloseUSD,\n    test_size= 0.6,  # 20% test data & 80% train data\n    random_state=0\n)\n\n#splitting data into training data and testing data\nX_train, X_test, y_train, y_test = train_test_split(\n    X_train1,\n    y_train1,\n    test_size= 0.6,  # 20% test data & 80% train data\n    random_state=0\n)","b81dc617":"N_sxd_Processed_Info_merged_df.describe()\ndataTypeSeries = N_sxd_Processed_Info_merged_df.dtypes\nprint('Data type of each column of Dataframe :')\nprint(dataTypeSeries)\n\ndataTypeSeriesX_train = X_train.dtypes\nprint('Data type of each column of Dataframe :')\nprint(dataTypeSeriesX_train)\n\ndataTypeSeriesX_test = X_test.dtypes\nprint('Data type of each column of Dataframe :')\nprint(dataTypeSeriesX_test)\n\ndataTypeSeriesy_train = y_train.dtypes\nprint('Data type of each column of Dataframe :')\nprint(dataTypeSeriesy_train)\n\ndataTypeSeriesy_test = y_test.dtypes\nprint('Data type of each column of Dataframe :')\nprint(dataTypeSeriesy_test) ","0d5c9503":"import numpy as np\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X_train, y_train)\ny_pred = reg.predict(X_test)\n\n# The coefficients\nprint('Coefficients: \\n', reg.coef_)\n# The mean squared error\nprint('Mean squared error: %.2f'\n      % mean_squared_error(y_test, y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint('Coefficient of determination: %.2f'\n      % r2_score(y_test, y_pred))\n\n# Plot outputs\nplt.scatter(y_test, y_pred,  color='black')\nplt.plot(y_test, y_pred, color='blue', linewidth=3)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()\n","978dc166":"import matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt1\nimport matplotlib.pyplot as plt2\nimport numpy as np \nimport pandas as pd\nimport matplotlib\nmatplotlib.rcParams.update({'font.size': 12})\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nrr = Ridge(alpha=0.01) \n# higher the alpha value, more restriction on the coefficients; low alpha > more generalization,\n# in this case linear and ridge regression resembles\nrr.fit(X_train, y_train)\nrr100 = Ridge(alpha=100) #  comparison with alpha value\nrr100.fit(X_train, y_train)\ntrain_score=lr.score(X_train, y_train)\ntest_score=lr.score(X_test, y_test)\nRidge_train_score = rr.score(X_train,y_train)\nRidge_test_score = rr.score(X_test, y_test)\nRidge_train_score100 = rr100.score(X_train,y_train)\nRidge_test_score100 = rr100.score(X_test, y_test)\nplt.plot(rr.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Ridge; $\\alpha = 0.01$',zorder=7) \nplt.xlabel('Coefficient Index',fontsize=16)\nplt.ylabel('Coefficient Magnitude',fontsize=16)\nplt.legend(fontsize=13,loc=4)\nplt.show()\nplt1.plot(rr100.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Ridge; $\\alpha = 100$') \nplt1.xlabel('Coefficient Index',fontsize=16)\nplt1.ylabel('Coefficient Magnitude',fontsize=16)\nplt1.legend(fontsize=13,loc=4)\nplt1.show()\nplt2.plot(lr.coef_,alpha=0.4,linestyle='none',marker='o',markersize=7,color='green',label='Linear Regression')\nplt2.xlabel('Coefficient Index',fontsize=16)\nplt2.ylabel('Coefficient Magnitude',fontsize=16)\nplt2.legend(fontsize=13,loc=4)\nplt2.show()","6aad1ca3":"Correlation Matrix for Index Info shows, columns are highly correlated.","09322dcf":"We can see the Various Co-efficients using Ridge methods of alpha.","289fae97":"Pairplot shows columns are highly correlated for Processed data.\nBut for CloseUSD the data is not highly correlated but does has random distribution.","3664f6cf":"Had to reduce volume of data, since getting memory issue","bf242c5c":"Pairplot shows that columns are highly related."}}