{"cell_type":{"fd50ce66":"code","da0e3050":"code","552664af":"code","3cfdac97":"code","45f73beb":"markdown","d38dc00d":"markdown"},"source":{"fd50ce66":"from scipy import linalg\nimport matplotlib","da0e3050":"def isotonic_regression(y, weight=None, y_min=None, y_max=None, callback=None):\n    \"\"\"Solve the isotonic regression model::\n\n        min sum w[i] (y[i] - y_[i]) ** 2\n\n        subject to y_min = y_[1] <= y_[2] ... <= y_[n] = y_max\n\n    where:\n        - y[i] are inputs (real numbers)\n        - y_[i] are fitted\n        - w[i] are optional strictly positive weights (default to 1.0)\n\n    Parameters\n    ----------\n    y : iterable of floating-point values\n        The data.\n\n    weight : iterable of floating-point values, optional, default: None\n        Weights on each point of the regression.\n        If None, weight is set to 1 (equal weights).\n\n    y_min : optional, default: None\n        If not None, set the lowest value of the fit to y_min.\n\n    y_max : optional, default: None\n        If not None, set the highest value of the fit to y_max.\n\n    Returns\n    -------\n    `y_` : list of floating-point values\n        Isotonic fit of y.\n\n    References\n    ----------\n    \"Active set algorithms for isotonic regression; A unifying framework\"\n    by Michael J. Best and Nilotpal Chakravarti, section 3.\n    \"\"\"\n    if weight is None:\n        weight = np.ones(len(y), dtype=y.dtype)\n    if y_min is not None or y_max is not None:\n        y = np.copy(y)\n        weight = np.copy(weight)\n        C = np.dot(weight, y * y) * 10  # upper bound on the cost function\n        if y_min is not None:\n            y[0] = y_min\n            weight[0] = C\n        if y_max is not None:\n            y[-1] = y_max\n            weight[-1] = C\n\n    active_set = [(weight[i] * y[i], weight[i], [i, ])\n                  for i in range(len(y))]\n    current = 0\n    counter = 0\n    while current < len(active_set) - 1:\n        value0, value1, value2 = 0, 0, np.inf\n        weight0, weight1, weight2 = 1, 1, 1\n        while value0 * weight1 <= value1 * weight0 and \\\n                        current < len(active_set) - 1:\n            value0, weight0, idx0 = active_set[current]\n            value1, weight1, idx1 = active_set[current + 1]\n            if value0 * weight1 <= value1 * weight0:\n                current += 1\n\n            if callback is not None:\n                callback(y, active_set, counter, idx1)\n                counter += 1\n\n        if current == len(active_set) - 1:\n            break\n\n        # merge two groups\n        value0, weight0, idx0 = active_set.pop(current)\n        value1, weight1, idx1 = active_set.pop(current)\n        active_set.insert(current,\n                          (value0 + value1,\n                           weight0 + weight1, idx0 + idx1))\n        while value2 * weight0 > value0 * weight2 and current > 0:\n            value0, weight0, idx0 = active_set[current]\n            value2, weight2, idx2 = active_set[current - 1]\n            if weight0 * value2 >= weight2 * value0:\n                active_set.pop(current)\n                active_set[current - 1] = (value0 + value2, weight0 + weight2,\n                                           idx0 + idx2)\n                current -= 1\n\n    solution = np.empty(len(y))\n    if callback is not None:\n        callback(y, active_set, counter+1, idx1)\n        callback(y, active_set, counter+2, idx1)\n    for value, weight, idx in active_set:\n        solution[idx] = value \/ weight\n    return solution","552664af":"import numpy as np\nimport pylab as pl\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.utils import check_random_state\n\n\ndef cb(y, active_set, counter, current):\n    solution = np.empty(len(y))\n    for value, weight, idx in active_set:\n        solution[idx] = value \/ weight\n    fig = matplotlib.pyplot.gcf()\n    fig.set_size_inches(9.5,6.5)\n\n    color = y.copy()\n    pl.scatter(np.arange(len(y)), solution, s=50, cmap=pl.cm.Spectral, vmin=50, c=color)\n    pl.scatter([np.arange(len(y))[current]], [solution[current]], s=200, marker='+', color='red')\n    pl.xlim((0, 40))\n    pl.ylim((50, 300))\n    pl.savefig('isotonic_%03d.png' % counter)\n    pl.show()\n\nn = 40\nx = np.arange(n)\nrs = check_random_state(0)\ny = rs.randint(-50, 50, size=(n,)) + 50. * np.log(1 + np.arange(n))\n\n###############################################################################\n# Fit IsotonicRegression and LinearRegression models\n\n\ny_ = isotonic_regression(y, callback=cb)","3cfdac97":"import pylab as pl\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.utils import check_random_state\n\nn = 100\ny = np.array([0]*50+[1]*50)\nrs = check_random_state(0)\nx = np.random.random(size=(n,)) #you can interpret it as the outputs of the SVM or any other model\n\nres = sorted(list(zip(x,y)), key = lambda x: x[0]) \nx = []\ny = []\nfor i,j in res:\n    x.append(i)\n    y.append(j)\nx= np.array(x)\ny= np.array(y)\n###############################################################################\n# Fit IsotonicRegression and LinearRegression models\n\nir = IsotonicRegression()\ny_ = ir.fit_transform(x, y)\n\nlr = LinearRegression()\nlr.fit(x[:, np.newaxis], y)  # x needs to be 2d for LinearRegression\n\n###############################################################################\n# plot result\n\nsegments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]\nlc = LineCollection(segments, zorder=0)\nlc.set_array(np.ones(len(y)))\nlc.set_linewidths(0.5 * np.ones(n))\n\nfig = pl.figure()\npl.plot(x, y, 'r.', markersize=12)\npl.plot(x, y_, 'g.-', markersize=12)\npl.plot(x, lr.predict(x[:, np.newaxis]), 'b-')\npl.gca().add_collection(lc)\npl.legend(('Data', 'Isotonic Fit', 'Linear Fit'), loc='lower right')\npl.title('Isotonic regression')\n\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(9.5,6.5)\npl.savefig('inverse_isotonic.png')\npl.show()","45f73beb":"# Isotonic regression :\n\nThere are situations when we need to find a regressor for a dataset of non-decreasing points which can present low-level oscillations (such as noise). A linear regression can easily achieve a very high score (considering that the slope is about constant), but it works like a denoiser, producing a line that can't capture the internal dynamics we'd like to model. For these situations, scikit-learn offers the class IsotonicRegression, which produces a piecewise interpolating function minimizing the functional:\n\n![](https:\/\/static.packt-cdn.com\/products\/9781785889622\/graphics\/B05169_01_90.png)\n\nIsotonic regression builds an increasing approximation of a function while maximizing the mean squared error on training data. In the case of linear regression, it minimizes the mean squared error. This kind of model is useful when you do not make any kind of assumption for target function such as linearity.\n\nAccording to sklearn Isotonic regression documentation, The class Isotonic Regression fits a non-decreasing real function to 1-dimensional data. Isotonic Regression produces a series of predictions for the training data which are the closest to the targets in terms of mean squared error. These predictions are interpolated for predicting to unseen data. The predictions of Isotonic Regression thus form a function that is piecewise linear:\n![](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_isotonic_regression_0011.png)\nFor better understanding you can visit:\nhttps:\/\/scikit-learn.org\/stable\/modules\/isotonic.html\n\n**If this helped you, some upvotes would be very much appreciated - that's where I get my motivation!**\n\nLet's see code implementation of Isotonic Regression : ","d38dc00d":"This algorithm was taken from scikit-learn v0.13 (the current is an equivalent Cython implementation), it just adds the callback argument"}}