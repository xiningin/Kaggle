{"cell_type":{"7500ab08":"code","cca278e7":"code","7b239e74":"code","ff4d0162":"code","c1201b3d":"code","f55174a9":"code","f4d06a68":"code","890159df":"code","14a95f64":"code","5bfc85c3":"code","7e729bfd":"code","20d72eba":"code","0ba51ef6":"code","1623a1ae":"code","07a62b5a":"code","5480098d":"code","4338a080":"code","b7223d60":"code","48cee47e":"markdown","dc4feb61":"markdown","13f78a3f":"markdown","e381602b":"markdown","e9beb256":"markdown","6300e3a9":"markdown","fb038422":"markdown","8a6009c2":"markdown","1f5e13c1":"markdown","1afc890c":"markdown","7d405abe":"markdown","75c30338":"markdown","a7c50e95":"markdown","b19b96c2":"markdown","dd4ad61e":"markdown","e7b3c521":"markdown","9ab3b7c0":"markdown","a92e06f3":"markdown","6f371fbb":"markdown","4079ed9d":"markdown","a99fa5c1":"markdown","48d3490e":"markdown","e4d19d70":"markdown","4f87640f":"markdown","a363d8e1":"markdown"},"source":{"7500ab08":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","cca278e7":"temp = pd.read_csv('..\/input\/temperature.csv', parse_dates=['datetime'])\ntemp = temp.set_index('datetime')\nprint('Dataset shape: {}'.format(temp.shape))\ntemp.head()","7b239e74":"all_std = temp.std(axis=0)\nmax_std = all_std.max()\ncity_max_std = temp.columns[all_std==max_std][0]\n\nprint('City with highest temperature variation: {} ({} degrees)'.format(city_max_std,round(max_std,2)))","ff4d0162":"data = temp[['San Francisco','Minneapolis']]\ndata.describe()","c1201b3d":"data = data-273.15\ndata.describe()","f55174a9":"_=data.plot(\n    figsize=(15,5),\n    subplots=False,\n    title='Temperature',\n    alpha=0.7\n)\n_=plt.xlabel('Date')\n_=plt.ylabel('Temperature')","f4d06a68":"SF_non_missing = data['San Francisco'].dropna()\nmax_date = SF_non_missing.index.max()\ndata = data[data.index <= max_date]","890159df":"print(data.isna().sum())","14a95f64":"data_mean = data.resample('D').mean()\ndata_min = data.resample('D').min()\ndata_max = data.resample('D').max()\nprint('Resample shape: {}'.format(data_mean.shape))\ndata_mean.describe()","5bfc85c3":"print('Missing data now?')\nprint(data_mean.isna().sum())","7e729bfd":"_=data_mean.plot(\n    figsize=(15,5),\n    subplots=False,\n    title='Temperature',\n    alpha=0.7\n)\n_=plt.fill_between(\n    x=data_mean.index,\n    y1=data_min['San Francisco'].values,\n    y2=data_max['San Francisco'].values,\n    alpha=0.3\n)\n_=plt.fill_between(\n    x=data_mean.index,\n    y1=data_min['Minneapolis'].values,\n    y2=data_max['Minneapolis'].values,\n    color='orange',\n    alpha=0.3\n)\n_=plt.xlabel('Date')\n_=plt.ylabel('Temperature')","20d72eba":"_=plt.hist(data_mean['San Francisco'], alpha=0.5, label='San Francisco')\n_=plt.hist(data_mean['Minneapolis'], alpha=0.5, label='Minneapolis')\n_=plt.legend()","0ba51ef6":"cut = data_mean.index[int(0.5*len(data_mean))]\nprint('Mean before {}:'.format(cut))\nprint(data_mean.loc[:cut].mean())\nprint('')\nprint('Mean after {}:'.format(cut))\nprint(data_mean.loc[cut:].mean())\nprint('')\nprint('---------------------------')\nprint('')\nprint('Std before {}:'.format(cut))\nprint(data_mean.loc[:cut].std())\nprint('')\nprint('Std after {}:'.format(cut))\nprint(data_mean.loc[cut:].std())","1623a1ae":"from statsmodels.tsa.stattools import adfuller\n\nresult = adfuller(data_mean['San Francisco'])\nprint('San Francisco')\nprint('--------------------------')\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n\nprint('\\n\\n')\n    \nresult = adfuller(data_mean['Minneapolis'])\nprint('Minneapolis')\nprint('--------------------------')\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))    ","07a62b5a":"import statsmodels.api as sm\nprint('San Francisco')\n_=sm.graphics.tsa.plot_acf(data_mean['San Francisco'])\nplt.show()\nprint('Minneapolis')\n_=sm.graphics.tsa.plot_acf(data_mean['Minneapolis'])\nplt.show()","5480098d":"import statsmodels.api as sm\nprint('San Francisco')\n_=sm.graphics.tsa.plot_acf(data_mean['San Francisco'], lags=365)\nplt.show()\nprint('Minneapolis')\n_=sm.graphics.tsa.plot_acf(data_mean['Minneapolis'], lags=365)\nplt.show()","4338a080":"print('San Francisco')\n_=sm.graphics.tsa.plot_pacf(data_mean['San Francisco'], lags=30)\nplt.show()\nprint('Minneapolis')\n_=sm.graphics.tsa.plot_pacf(data_mean['Minneapolis'], lags=30)\nplt.show()","b7223d60":"from statsmodels.tsa.seasonal import seasonal_decompose as sd\nsd_SF = sd(data_mean['San Francisco'], freq=365)\nsd_M = sd(data_mean['Minneapolis'], freq=365)\n\n_=plt.figure(figsize=(15,10))\nax1=plt.subplot(311)\n_=ax1.plot(sd_SF.trend, label='San Francisco', alpha=0.7)\n_=ax1.plot(sd_M.trend, label='Minneapolis', alpha=0.7)\n_=plt.legend()\nax2=plt.subplot(312)\n_=ax2.plot(sd_SF.seasonal, label='San Francisco', alpha=0.7)\n_=ax2.plot(sd_M.seasonal, label='Minneapolis', alpha=0.7)\n_=plt.legend()\nax3=plt.subplot(313)\n_=ax3.plot(sd_SF.resid, label='San Francisco', alpha=0.7)\n_=ax3.plot(sd_M.resid, label='Minneapolis', alpha=0.7)\n_=plt.legend()","48cee47e":"## Outliers\n\nSome times weird things happen and we end up with values that can mess up an entire model. One sensor can fail, for example, and measure a temperature of -10\u00ba in summer, which definitely is not normal. Other times you can see 10\u00ba in the summer, that's low, but maybe not an error, sometimes it can get cold, you can't remove this type of value, because there can be some reason to it. We have to be carefull how we treat outliers, we shouldn't remove it unless we know it's an error or is a one-time-thing that shouldn't affect our model.\n\nIt's not always easy to identify which points are outliers, and what to do with them, but a good place to start is to inspect the data to see if there are points with extremely high or low values. One good way to see this visually is to use histograms.\n","dc4feb61":"This could also have solved our problem with missing data, if there was more any row with non-missing data everyday. Let's check.","13f78a3f":"First thing I see is that there are missing data in both columns, I'll deal with that soon. Also, this temperature values are obviously not in a normal scale for day-to-day temperature. Since I live in Europe I'm going to transform the data from Kelvin to Celsius (sorry if you use Farenheit, but International Systemm baby).","e381602b":"## Cleaning the data\n\nAs we saw, there are clearly missing values, something that caughts my attention in the figure may be a reason for this. As I mentioned, the data for San Francisco finishes earlier, to work with both series at the same time I'm going to lose the final values in the Minneapolis series.\n\nTo do so I'm going to keep all the non-missing values of San Francisco and see the maximum date they reach. Then, we'll cut all data with date larger than that.","e9beb256":"## Visualizing the data\n\nLet's see what the temperature for these two cities looked like along the years","6300e3a9":"Let's see if we still have missing values.","fb038422":"No missing data now! This means that we have at least one value per day.\n\nIf this were not the case I would have used the values from the previus day. I like this solution for time series more than just dropping the row (is better to have the same temporal separation between all rows) or just using the mean value (since this would mess with the shape of the curve).\n\nLet's check how our data looks after resampling.","8a6009c2":"So, what do any of these values mean?\n* The ADF Statistic is the Augmented Dicken-Fuller score, the more negative this value is, the higher the certainty that we can reject out null hypothesis (the probability that the time series is stationary).\n* p-value is the level of confidance for the null hypothesis. A usual threshold for this value is 0.05, meaning that if *p_value <= 0.05* we can reject the null hypothesis.\n* The rest of the values are the critical values for a 99%, 95% and 90% confidence intervals respectively.\n\nSo, what all this means in this case is that we can reject the null hypothesis for San Francisco since *p* is lowe than 0.05, and also, the ADF score is lower than the limit for a 99% confidence interval. However, we fail to reject this hypothesis for Minneapolis, we could say it is stationarity with a confidence interval of 90%, but since the threshold was 95% (*p = 0.05*), we cannot reject it. This means we should difference the data before applying any model.\n\n\nSome references to understand this tests can be found here:\n* [Stationary Data Tests for Time Series Forecasting](https:\/\/pythondata.com\/stationary-data-tests-for-time-series-forecasting\/)\n* [How to Check if Time Series Data is Stationary with Python](https:\/\/machinelearningmastery.com\/time-series-data-stationary-python\/)","1f5e13c1":"Looking at this we can understand why we found the Minneapolis data to be non-stationary, there has been a clear increase in the temperature.\n\nWe can also find the trend by doing the moving average, I won't do that here since it's what the seasonal_decompose function does behing the scenes.","1afc890c":"Let's focus on the most important lags (the ones closer to the point), for example, data from one-year range.","7d405abe":"Here are some references about autocorrelation:\n* [Statsmodels reference to acf function](https:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.tsa.stattools.acf.html)\n* [Statsmodels reference to pacf function](https:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.tsa.stattools.pacf.html)\n* [A Gentle Introduction to Autocorrelation and Partial Autocorrelation](https:\/\/machinelearningmastery.com\/gentle-introduction-autocorrelation-partial-autocorrelation\/)","75c30338":"Let's look into each of the cities individually:\n* Values for San Francisco seem to follow a gaussian distribution with a small standard deviation, we can't see any outliers in this data.\n* For Minneapolis the curve is less perfect, with a high skewness to the right side (negative skewness). We cannot say that any of this points are outliers though, since there is quite a few of them, also, in the visual representation of the temperatures we could see that really low values are reached every year.\n\nI don't see any outliers and don't think I should remove any points.","a7c50e95":"Let's start by loading the data a having a quick look at its shape and first values. Since I already know this is a time series I'm going to set the datetime column as the index.","b19b96c2":"We can see that the values are pretty close for San Francisco, but further away for Minneapolis, they are still close enough for the time series to be stationary since we need to take into account the standard deviation.\n\nThis method doesn't prove or deny that our time series are stationary, it's just indicates that it can be.\n\nWe can also use a statistical test to see if the non-stationarity hypothesis should be rejected __Augmented Dickey-Fuller test__","dd4ad61e":"## Trend - Seasonality decomposition\n\nWe can think about time series as a composition of trend, seasonality and residuals (noise or other random behaviour). The composition of the time series from this components can be bouth aditive or multiplicative:\n* Additive: $data\\ =\\ Trend\\ +\\ Seasonality\\ +\\ Residuals$\n* Multiplicative: $data\\ =\\ Trend\\ \\cdot\\ Seasonality\\ \\cdot\\ Residuals$\n\nThe Statsmodels package offers a function to extract this 3 component at once: [Seasonal_decompose](https:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.tsa.seasonal.seasonal_decompose.html)\n\nThe decomposition here is easy because we know the period to be 365 days.","e7b3c521":"This is it for now. I'll start with some forecasting algorithms in future posts.\n\nI hope this can help some beginners like myself, thanks for reading! :)\n\n----------------------------\n\nI want to give credit to Jason Brownlee's blog [machinelearningmastery](http:\/\/machinelearningmastery.com), since is where I learnt most of what I know about time series data, he has many posts about this topic and his explanations and examples are really clear, I totally recommend it if you're getting started, or want to expand your knowladge about this or any other machine learning topic.","9ab3b7c0":"We can also check for partial autocorrelation, which calculates the correlation removing the effect of any other previous point (points closer to the new value). Here further away points lose importance, I'll focus on one-month range.","a92e06f3":"## Looking for stationarity and autocorrelation\n\nThs gaussian-shaped histagram we plotted earlier is a first clue the time series can be stationary.\n\nAnother clue is to compute some statistics on the time series in different time ranges and looking for a variation.","6f371fbb":"## Autocorrelation\n\nLast thing to check is if the data is autocorrelated. I want to use some autoregression methods to do forecasting in future posts, I can only do that if the data is autocorrelated (meaning that the value in a specific temporal point depends on previous values).\n\nThe statmodels library offers a great tool to check this. Everything outside of the shadowed area has a strong probability of being autocorrelated (over 95% confidence interval).","4079ed9d":"This dataset is composed by 36 columns, corresponding to 36 different cities, and 45253 rows, giving the value of the temperature every hour from late 2012 to late 2017.\n\nAs I said, one of the cities I'm going to use is the one with the highest variation in yearly temperature. Let's find out which one.","a99fa5c1":"Not bad, the weather seems quite cold in Minneapolis, maybe I wouldn't go there during the winter.\n\nAlso, there is a clear seasonality in the data with a period of a year. There is also more variation, I'll check later if this variation has something to do with some daily seasonality or it's more random.\n\nSomething I can see in this figure is that we have less data for San Francisco. The blue line doesn't get that close to 2018.","48d3490e":"The shadow around the curve shows the min-max values during that day, while the main line shows the mean value.\n\nNow both curves end at the same point, and we have less daily variation.","e4d19d70":"Let's subset the data now so it only contains values for San Francisco and Minneapolies and see some statistics.","4f87640f":"# Time Series Forecasting (1) - Initial Analysis\n\nLeet's start this series by defining what time series are. I opted for the simplest option and asked wikipedia and this is the answer:\n\n> A time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. - Wikipedia\n\nSo time series are basically like any other dataset but with two important characteristics:\n* The order of the data is important, we don't only care abour the values of the data but also **when** do those values happen.\n* We have information of the time in which each instance was observed: We either have a column with datetime information or we know that the data is equally space in time (one value every second, for example).\n\nHere I'm introducing the concept of time series and doing the initial analysis and tuning of the data; since Kaggle data is usually pretty clear this work is way easier than in real life, but the idea is to represent what would be the process, not to write complex code. The process I'm going to follow is this:\n1. Loading the data and subsetting (I won't be using the whole dataset here)\n2. Visualizing the data\n3. Cleaning the data: Are there missing values? Is all the data in the desired format?\n4. Looking at the statistics: Are there outliers?\n5. Looking for stationarity and autocorrelation\n\n**The Data**\n\nI'm using the [Historical Hourly Weather Data](https:\/\/www.kaggle.com\/selfishgene\/historical-hourly-weather-data\/home) from Kaggle's repository. This dataset contains ~5years of weather information for different cities, including information about the temperature, humidity, pressure, wind and description of the weather. I am going to focus on **temperature** data, since it's the property I find to be more intuitive.\n\nTo do the examples I'm going to use the temperature data for two cities, the chosen ones will be San Francisco (because I like it) and the city with the highest variation.\n\n\n--------------------------------\n\n## Loading the data\n\nI will be using Pandas through the entire notebook to handle the data and matplotlib for the visualization.\n","a363d8e1":"Ok, we still have to deal with the problem of missing data, but there is something I want to deal with first. My intention here is to study the yearly behaviour of the data, I'm not interested in the daily variation, so I'm going to resample the data into a daily frequency by taking the average, minimum and maximum of all the temperatures through that day."}}