{"cell_type":{"64d8bec3":"code","31a11426":"code","cc8d6fd9":"code","e39a47e5":"code","399c5e9c":"code","6b2d2ccf":"code","3d8652be":"code","9d8407b8":"code","be608973":"markdown","ad0798eb":"markdown","9c38cf9b":"markdown","100f429a":"markdown"},"source":{"64d8bec3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport numpy as np\nfrom PIL import Image\nimport keras\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\noutput_path = \"\/kaggle\/working\"","31a11426":"def generator_model():\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(input_dim=100, output_dim=1024))\n    model.add(keras.layers.Activation('tanh'))\n    model.add(keras.layers.Dense(128 * 7 * 7))\n    model.add(keras.layers.BatchNormalization())\n    model.add(keras.layers.Activation('tanh'))\n    model.add(keras.layers.Reshape((7, 7, 128), input_shape=(128 * 7 * 7,)))\n    model.add(keras.layers.UpSampling2D(size=(2, 2)))\n    model.add(keras.layers.Conv2D(64, (5, 5), padding='same'))\n    model.add(keras.layers.Activation('tanh'))\n    model.add(keras.layers.UpSampling2D(size=(2, 2)))\n    model.add(keras.layers.Conv2D(1, (5, 5), padding='same'))\n    model.add(keras.layers.Activation('tanh'))\n    return model","cc8d6fd9":"def discriminator_model():\n    model = keras.Sequential()\n    model.add(\n        keras.layers.Conv2D(64, (5, 5),\n                               padding='same',\n                               input_shape=(28, 28, 1))\n    )\n    model.add(keras.layers.Activation('tanh'))\n    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(keras.layers.Conv2D(128, (5, 5)))\n    model.add(keras.layers.Activation('tanh'))\n    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(keras.layers.Flatten())\n    model.add(keras.layers.Dense(1024))\n    model.add(keras.layers.Activation('tanh'))\n    model.add(keras.layers.Dense(1))\n    model.add(keras.layers.Activation('sigmoid'))\n    return model","e39a47e5":"def generator_containing_discriminator(g, d):\n    model = keras.Sequential()\n    model.add(g)\n    d.trainable = False\n    model.add(d)\n    return model","399c5e9c":"def combine_images(generated_images):\n    # \u751f\u6210\u56fe\u7247\u62fc\u63a5\n    num = generated_images.shape[0]\n    width = int(math.sqrt(num))\n    height = int(math.ceil(float(num)\/width))\n    shape = generated_images.shape[1:3]\n    image = np.zeros((height*shape[0], width*shape[1]), dtype=generated_images.dtype)\n    for index, img in enumerate(generated_images):\n        i = int(index\/width)\n        j = index % width\n        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = img[:, :, 0]\n    return image","6b2d2ccf":"def train(BATCH_SIZE):\n    # \u4e0b\u8f7d\u7684\u5730\u5740\u4e3a\uff1ahttps:\/\/s3.amazonaws.com\/img-datasets\/mnist.npz\n    # (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n    (X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n    X_train = (X_train.astype(np.float32) - 127.5) \/ 127.5\n    X_train = X_train[:, :, :, None]  # None\u5c063\u7ef4\u7684X_train\u6269\u5c55\u4e3a4\u7ef4\n    X_test = X_test[:, :, :, None]\n\n    d = discriminator_model()\n    g = generator_model()\n    d_on_g = generator_containing_discriminator(g, d)\n\n    d_optim = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n    g_optim = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n\n    g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n    d_on_g.compile(loss='binary_crossentropy', optimizer=g_optim)\n\n    d.trainable = True\n    d.compile(loss='binary_crossentropy', optimizer=d_optim)\n\n    for epoch in range(30):\n        print(\"Epoch is\", epoch)\n\n        print(\"Number of batches\", int(X_train.shape[0] \/ BATCH_SIZE))\n\n        for index in range(int(X_train.shape[0] \/ BATCH_SIZE)):\n\n            noise = np.random.uniform(-1, 1, size=(BATCH_SIZE, 100))\n\n            image_batch = X_train[index * BATCH_SIZE:(index + 1) * BATCH_SIZE]\n\n            generated_images = g.predict(noise, verbose=0)\n\n            X = np.concatenate((image_batch, generated_images))\n            y = [1] * BATCH_SIZE + [0] * BATCH_SIZE\n\n            d_loss = d.train_on_batch(X, y)  # (2*BATCH_SIZE,28,28,1) -> (2*BATCH_SIZE,1)\n\n            # \u968f\u673a\u751f\u6210\u7684\u566a\u58f0\u670d\u4ece\u5747\u5300\u5206\u5e03\n            noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n\n            # \u56fa\u5b9a\u5224\u522b\u5668\n            d.trainable = False\n\n            g_loss = d_on_g.train_on_batch(noise, [1] * BATCH_SIZE)  # (BATCH_SIZE,100) -> (BATCH_SIZE,28,28,1) -> (BATCH_SIZE,1)\n\n            # \u4ee4\u5224\u522b\u5668\u53ef\u8bad\u7ec3\n            d.trainable = True\n            # \u6bcf\u7ecf\u8fc7100\u6b21\u8fed\u4ee3\u8f93\u51fa\u4e00\u5f20\u751f\u6210\u7684\u56fe\u7247\n            if index % 300 == 0:\n                image = combine_images(generated_images)\n                image = image * 127.5 + 127.5\n                Image.fromarray(image.astype(np.uint8)).save(f\"{output_path}\/\" + str(epoch) + \"_\" + str(index) + \".png\")\n                print(\"batch %d d_loss : %f g_loss : %f\" % (index, d_loss, g_loss))\n\n            # \u6bcf100\u6b21\u8fed\u4ee3\u4fdd\u5b58\u4e00\u6b21\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u7684\u6743\u91cd\n            if index % 300 == 9:\n                g.save_weights(f'{output_path}\/generator.h5', True)\n                d.save_weights(f'{output_path}\/discriminator.h5', True)","3d8652be":"\ndef generate(BATCH_SIZE, nice=False):\n    # \u8bad\u7ec3\u5b8c\u6a21\u578b\u540e\uff0c\u53ef\u4ee5\u8fd0\u884c\u8be5\u51fd\u6570\u751f\u6210\u56fe\u7247\n    g = generator_model()\n    g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n    g.load_weights(f'{output_path}\/generator.h5')\n    if nice:\n        d = discriminator_model()\n        d.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n        d.load_weights(f'{output_path}\/discriminator.h5')\n        noise = np.random.uniform(-1, 1, (BATCH_SIZE*20, 100))\n        generated_images = g.predict(noise, verbose=1)\n        d_pret = d.predict(generated_images, verbose=1)\n        index = np.arange(0, BATCH_SIZE*20)\n        index.resize((BATCH_SIZE*20, 1))\n        pre_with_index = list(np.append(d_pret, index, axis=1))\n        pre_with_index.sort(key=lambda x: x[0], reverse=True)\n        nice_images = np.zeros((BATCH_SIZE,) + generated_images.shape[1:3], dtype=np.float32)\n        nice_images = nice_images[:, :, :, None]\n        for i in range(BATCH_SIZE):\n            idx = int(pre_with_index[i][1])\n            nice_images[i, :, :, 0] = generated_images[idx, :, :, 0]\n        image = combine_images(nice_images)\n    else:\n        noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n        generated_images = g.predict(noise, verbose=0)\n        image = combine_images(generated_images)\n    image = image*127.5+127.5\n    Image.fromarray(image.astype(np.uint8)).save(f\"{output_path}\/generated_image_{BATCH_SIZE}.png\")","9d8407b8":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1'\nd = discriminator_model()\ng = generator_model()\nd_on_g = generator_containing_discriminator(g, d)\n# print(g.summary())\n# print(d.summary())\nprint(d_on_g.summary())\ntrain(100)  # 100\u4e3abatch\u5927\u5c0f\uff0c\u53ef\u4ee5\u968f\u610f\u6307\u5b9a\u3002\ngenerate(1)  # 132\u4e3abatch\u5927\u5c0f\uff0c\u53ef\u4ee5\u968f\u610f\u6307\u5b9a\u3002\u8be5\u503c\u5927\u5c0f\u4e5f\u51b3\u5b9a\u4e86\u751f\u6210\u7684\u56fe\u7247\u4e2d\u542b\u6709\u591a\u5c11\u4e2a\u6570\u5b57\u3002\ngenerate(32)  # 32\u4e3abatch\u5927\u5c0f\uff0c\u53ef\u4ee5\u968f\u610f\u6307\u5b9a\u3002\u8be5\u503c\u5927\u5c0f\u4e5f\u51b3\u5b9a\u4e86\u751f\u6210\u7684\u56fe\u7247\u4e2d\u542b\u6709\u591a\u5c11\u4e2a\u6570\u5b57\u3002","be608973":"### \u751f\u6210\u5668\u7684\u67b6\u6784","ad0798eb":"### train","9c38cf9b":"### \u751f\u6210\u5668\u67b6\u6784 + \u5224\u522b\u5668\u67b6\u6784\u7ec4\u62fc\u63a5\u6210\u4e00\u4e2a\u5927\u7684\u795e\u7ecf\u7f51\u7edc","100f429a":"### \u5224\u522b\u5668\u67b6\u6784"}}