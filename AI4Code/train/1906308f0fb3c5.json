{"cell_type":{"eb402cef":"code","2c90769a":"code","96eff49e":"code","ac04dfcd":"code","d01d109d":"code","8bb38fdd":"code","34f1756b":"code","73cf9bee":"code","cf53907b":"code","12a157e6":"code","68fa2d68":"markdown","b2ffa060":"markdown","a009475a":"markdown","7593e4aa":"markdown","4cc09cac":"markdown","82b485e7":"markdown","20b769fc":"markdown","600175c4":"markdown","dfc9f5c0":"markdown"},"source":{"eb402cef":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgbm\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import precision_recall_curve\nfrom tqdm.notebook import tqdm\n\nsns.set(style='darkgrid', context='notebook', rc={\n    'figure.figsize': (16, 12),\n    'figure.frameon': False,\n    'legend.frameon': False,\n})\n\nrandom.seed(64)\nnp.random.seed(64)\ncv = StratifiedKFold(shuffle=True, random_state=64)\n\ndata_root = os.environ.get('KAGGLE_DIR', '..\/input')\ndf = pd.read_parquet(f'{data_root}\/tpsdec2021parquet\/train.pq')\ndf = df.loc[df.Cover_Type != 5]\nlabel_encoder = LabelEncoder()\nX = df.drop(columns=['Id', 'Cover_Type']).astype(np.float32).to_numpy()\ny = label_encoder.fit_transform(df.Cover_Type)\n\nX.shape, y.shape","2c90769a":"params = {\n    'objective': 'multiclass',\n    'metric': ['multi_error', 'multi_logloss'],\n    'first_metric_only': True,\n    'seed': 64,\n    'num_class': df.Cover_Type.nunique(),\n    'verbosity': -1,\n}\n\noof_proba = np.zeros((y.shape[0], df.Cover_Type.nunique()))\n\nfor train_idx, val_idx in tqdm(cv.split(X, y), total=cv.n_splits):\n    booster = lgbm.train(\n        params,\n        train_set=lgbm.Dataset(X[train_idx], label=y[train_idx]),\n        valid_sets=lgbm.Dataset(X[val_idx], label=y[val_idx]),\n        verbose_eval=20,\n        num_boost_round=100,\n        early_stopping_rounds=5,    \n    )\n    oof_proba[val_idx] = booster.predict(X[val_idx])","96eff49e":"probas = pd.DataFrame(\n    oof_proba[:, 0:2], columns=['Cover_Type=1', 'Cover_Type=2']\n).melt(var_name='class_', value_name='probability')\n\nsns.displot(\n    x=probas.probability, col=probas.class_, bins=50, kde=False\n);","ac04dfcd":"prec_0, _, thres_0 = precision_recall_curve(y == 0, oof_proba[:, 0])\nprec_1, _, thres_1 = precision_recall_curve(y == 1, oof_proba[:, 1])\n\nt_0 = thres_0[prec_0[:-1] > .995][0]\nt_1 = thres_1[prec_1[:-1] > .995][0]\n\nprint(f'Choose threshold={t_0:.4f} for Cover_Type=1, threshold={t_1:.4f} for Cover_Type=2')","d01d109d":"is_easy = (oof_proba[:, 0] > t_0) | (oof_proba[:, 1] > t_1)\nis_easy.mean()","8bb38fdd":"easy_X, easy_y = X[is_easy], y[is_easy]\n\nX_train, X_val, y_train, y_val = train_test_split(easy_X, easy_y, shuffle=True, random_state=64, test_size=.9)\n\nbooster = lgbm.train(\n    params,\n    train_set=lgbm.Dataset(X_train, label=y_train),\n    valid_sets=lgbm.Dataset(X_val, label=y_val),\n    verbose_eval=20,\n    num_boost_round=100,\n    early_stopping_rounds=5,    \n)","34f1756b":"hard_X, hard_y = X[~is_easy], y[~is_easy]\n\nX_train, X_val, y_train, y_val = train_test_split(hard_X, hard_y, shuffle=True, random_state=64, test_size=.1)\n\nbooster = lgbm.train(\n    params,\n    train_set=lgbm.Dataset(X_train, label=y_train),\n    valid_sets=lgbm.Dataset(X_val, label=y_val),\n    verbose_eval=20,\n    num_boost_round=100,\n    early_stopping_rounds=5,    \n)","73cf9bee":"np.mean(booster.predict(easy_X).argmax(axis=1) == easy_y)","cf53907b":"df.assign(\n    is_easy=is_easy\n).to_parquet('train.pq', index=False)","12a157e6":"sns.catplot(\n    data=df.assign(is_easy=is_easy).astype({'Cover_Type': 'string'}),\n    x='Cover_Type', kind='count', col='is_easy', sharey=False\n);","68fa2d68":"This booster does not have incredible performance on the easy data:","b2ffa060":"Now we have some probabilities. We're mostly interested in the first two classes, since they're so overrepresented. Let's plot their probability distributions:","a009475a":"This simple classifier considers about 80% of the data to be \"easy\". Let's check how easy it is:","7593e4aa":"Less than 1% validation error, from training on only 10% of the data.\n\nMy conclusion is that we don't need so *many* of the easy samples. I ended up throwing out 90% of the easy samples, using a similar method to this, which reduced the size of the data from ~4 million rows to ~1 million rows. This has not harmed the CV or LB score of my models as far as I can tell, and I'm obviously experimenting much faster now.\n\nJust to demonstrate how much harder the hard samples are, here's a similar experiment there, but training on 90% of the data:","4cc09cac":"TPS202112 - Do you need all that data?\n==\n\nIn this notebook, I will show that about 80% of the data in the training set is very uniform, and could probably be represented by *much* fewer samples. I will use a simple 3 step procedure to do this:\n\n1. Train a model and record the out of fold probability predictions.\n2. Flag the most confident Cover_Type=1 and Cover_Type=2 predictions as 'easy'\n3. Show that, given 10% of the easy samples, we can achieve accuracy of over 99% on the other 90% of the easy samples.\n\nI'll use lightgbm to do this right now, because it's convenient, and outputs probabilities and it's relatively fast to train something that isn't horrible.","82b485e7":"But then, that would be easy to fix by including some, but not all of the easy data in our training set.\n\nIn case you'd like to try playing around with this, I'm writing out the data again, together with the `is_easy` flag, so it's easy to experiment with different sampling strategies based on this:","20b769fc":"To load it, add this notebook as a data source to your notebook and run this code:\n\n```python\nimport pandas as pd\n\ndf = pd.read_parquet('..\/input\/tps202112-do-you-need-all-that-data\/train.pq')\n```\n\nYou should have the `is_easy` column there, which is going to be mostly `Cover_Type=1` and `Cover_Type=2`:","600175c4":"Note how many samples that have very high probability, especially in `Cover_Type=2`. Let's grab precision recall curves, so we can find the threshold for the classes where precision is 99.5%:","dfc9f5c0":"Going to get hold of some out of fold probabilities here. It's not important that these are fantastic, so we won't be tuning anything for accuracy here, we'll do mostly defaults:"}}