{"cell_type":{"eef8f5a3":"code","2b22481b":"code","9cf4460e":"code","47e04219":"code","169a7167":"code","02dfb670":"code","270eab1e":"code","0631540e":"code","decce758":"code","86e9bc11":"code","87c05c8e":"code","80f1f095":"code","69805b16":"code","40fc21cf":"code","78998716":"code","49983595":"code","eec2fd0a":"code","7ae544c5":"code","b44a6f49":"code","bb4ee5ac":"code","b938fffa":"code","91e7decc":"code","481940ef":"code","978ff7d3":"code","33842459":"code","e22eb2df":"code","9c73a572":"code","e26691c7":"code","efa2137f":"code","e369d5c6":"code","64f1e0d3":"code","a9b041a6":"code","376cf635":"code","22b4f7b4":"code","45f9b6ba":"code","df996be0":"code","11d40b74":"code","8bec4b4c":"code","b9ba255c":"code","a157f29c":"code","d04fae2f":"code","2ded0a3a":"code","2899ac00":"code","25f61a61":"code","8f7a343e":"code","4fc5b47a":"code","6dee093a":"code","076f7d44":"code","f0ea86b5":"code","c633aa49":"code","8a46d0ae":"code","876cae5b":"code","8d4bffe9":"code","976ed7a0":"code","403e4049":"code","ebae6e22":"code","d3e6908c":"code","9ec16c1f":"code","d3410387":"code","8721a2c2":"code","6c6dc483":"code","942d2651":"code","294296da":"code","6504b57e":"code","c0f85341":"code","3e11b549":"code","c385f7e6":"code","bdc4b84c":"code","48cf3f93":"code","8a608050":"markdown","caa00869":"markdown","423d0230":"markdown","723dbee7":"markdown","cec63ea3":"markdown","1162d34f":"markdown","ddf92565":"markdown","d2b6af61":"markdown","494196f3":"markdown","b50a6cd4":"markdown","67ab4232":"markdown","e7a0191e":"markdown","dbf7e9b6":"markdown","7ba6dfc4":"markdown","fc3e4e76":"markdown","7c74f127":"markdown","4eb0455f":"markdown","d96418c0":"markdown","392d0c1f":"markdown","6fd45247":"markdown","5bc9d76b":"markdown","34c4bc62":"markdown"},"source":{"eef8f5a3":"import pandas as pd \nimport numpy as np \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nimport scipy.stats as ss\nfrom statsmodels.formula.api import ols\nfrom scipy.stats import zscore\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%timeit \n%matplotlib inline\nsns.set_style('darkgrid')\n\nfont = {'family' : 'normal',\n        'weight' : 'bold',\n        'size'   : 15}\n\nplt.rc('font', **font)","2b22481b":"dftrain=pd.read_csv(\"..\/input\/train.csv\")\ndftest=pd.read_csv(\"..\/input\/test.csv\")\ntest=dftest.copy()","9cf4460e":"dftrain.info()","47e04219":"dftrain.head().T","169a7167":"dftrain.info()","02dfb670":"print('Number of null values :',dftrain.isnull().sum().sum())","270eab1e":"df = dftrain.copy()","0631540e":"print(\"Survival rates of MALES and FEMALES\")\nprint(\"**********************************\\n\\n\")\nmale1=df.loc[(df.Survived==1) &(df.Sex=='male'),:].count()\nprint(\"MALE:\\n\\n\",male1) \nfemale1=df.loc[(df.Survived==1) & (df.Sex=='female'),:].count()\nprint(\"\\nFEMALE:\\n\\n\",female1)","decce758":"print('*'*6,' NULL VALUES ',\"*\"*6,'\\n')\nprint(df.isnull().sum())","86e9bc11":"print(\"Mean age for each Pclass :\\n\\n\",df.groupby(\"Pclass\").Age.mean())","87c05c8e":"# Missing data\nimport missingno as msno\nmsno.bar(dftrain.sample(890),(28,10),color='red')\n\nplt.title('MISSING VALUES',fontsize=25)","80f1f095":"corr = dftrain.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(9, 7))\n    ax = sns.heatmap(corr,mask=mask,square=True,annot=True,fmt='0.2f',linewidths=.8,cmap=\"Accent\",robust=True)","69805b16":"# Stats and Visualisation of Survival Rate\nsns.factorplot(x=\"Sex\",col=\"Survived\", data=df , kind=\"count\",size=6, aspect=.7,palette=['crimson','lightblue'])\n\nmalecount=pd.value_counts((df.Sex == 'male') & (df.Survived==1))\nfemalecount=pd.value_counts((df.Sex=='female') & (df.Survived==1))\ntotalmale,totalfemale=pd.value_counts(df.Sex)","40fc21cf":"print(\"MALE survived \\n{} \\n\\n\\nFEMALE survived \\n{}\".format(malecount\/totalmale,femalecount\/totalfemale))","78998716":"#Clear representation of Ages of passengers and to which Class they belonged\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"Sex\",y=\"Age\",hue='Pclass',data=df,size=8 ,palette=['orange','brown','purple'])","49983595":"plt.figure(figsize=(10,10))\nsns.swarmplot(x=\"Sex\",y=\"Age\",hue='Survived',data=df,size=8,palette='viridis')","eec2fd0a":"sns.factorplot(x=\"Sex\", hue = \"Pclass\" , col=\"Survived\", data=df , kind=\"count\",size=7, aspect=.7,palette=['blue','green','yellow'])","7ae544c5":"pd.crosstab([df.Sex,df.Survived],df.Pclass, margins=True).style.background_gradient(cmap='autumn_r')","b44a6f49":"sns.factorplot(x=\"Survived\",col=\"Embarked\",data=df ,hue=\"Pclass\", kind=\"count\",size=8, aspect=.7,palette=['crimson','darkblue','purple'])","bb4ee5ac":"sns.factorplot(x=\"Sex\", y=\"Survived\",col=\"Embarked\",data=df ,hue=\"Pclass\",kind=\"bar\",size=7, aspect=.7)","b938fffa":"# Correlation Heatmap \ncontext1 = {\"female\":0 , \"male\":1}\ncontext2 = {\"S\":0 , \"C\":1 , \"Q\":2}\ndf['Sex_bool']=df.Sex.map(context1)\ndf[\"Embarked_bool\"] = df.Embarked.map(context2)\n\ncorr = df[['PassengerId', 'Survived', 'Pclass', 'Sex_bool', 'Age', 'SibSp',\n       'Parch', 'Fare' , 'Embarked_bool']].corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(9, 7))\n    ax = sns.heatmap(corr,mask=mask,square=True,annot=True,fmt='0.2f',linewidths=.8,cmap=\"jet\",robust=True)\n","91e7decc":"for x in [dftrain, dftest,df]:\n    x['Age_bin']=np.nan\n    for i in range(8,0,-1):\n        x.loc[ x['Age'] <= i*10, 'Age_bin'] = i\ndf[['Age','Age_bin']].head(20)","481940ef":"plt.figure(figsize=(20,20))\nsns.set(font_scale=1)\nsns.factorplot('Age_bin','Survived', col='Pclass' , row = 'Sex',kind=\"bar\", data=df)","978ff7d3":"df.describe().T","33842459":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\naxis1.set_title('Original Age values - Titanic')\naxis2.set_title('New Age values - Titanic')\n\n# plot original Age values\n# NOTE: drop all null values, and convert to int\ndftrain['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n\n# get average, std, and number of NaN values\naverage_age = dftrain[\"Age\"].mean()\nstd_age = dftrain[\"Age\"].std()\ncount_nan_age = dftrain[\"Age\"].isnull().sum()\n\n# generate random numbers between (mean - std) & (mean + std)\nrand_age = np.random.randint(average_age - std_age, average_age + std_age, size = count_nan_age)\n\n# fill NaN values in Age column with random values generated\nage_slice = dftrain[\"Age\"].copy()\nage_slice[np.isnan(age_slice)] = rand_age\n\n# plot imputed Age values\nage_slice.astype(int).hist(bins=70, ax=axis2)","e22eb2df":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\naxis1.set_title('Original Age values - Titanic')\naxis2.set_title('New Age values - Titanic')\n\n# plot original Age values\n# NOTE: drop all null values, and convert to int\ndftest['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n\n# get average, std, and number of NaN values\naverage_age = dftest[\"Age\"].mean()\nstd_age = dftest[\"Age\"].std()\ncount_nan_age = dftest[\"Age\"].isnull().sum()\n\n# generate random numbers between (mean - std) & (mean + std)\nrand_age = np.random.randint(average_age - std_age, average_age + std_age, size = count_nan_age)\n\n# fill NaN values in Age column with random values generated\nage_slice = dftest[\"Age\"].copy()\nage_slice[np.isnan(age_slice)] = rand_age\n\n# plot imputed Age values\nage_slice.astype(int).hist(bins=70, ax=axis2)","9c73a572":"for x in [dftrain, dftest , df]:\n    x['Fare_bin']=np.nan\n    for i in range(12,0,-1):\n        x.loc[ df['Fare'] <= i*50, 'Fare_bin'] = i\nfig, axes = plt.subplots(2,1)\nfig.set_size_inches(15, 12)\nsns.kdeplot(df.Age_bin , shade=True, color=\"green\" , ax= axes[0])\nsns.kdeplot(df.Fare , shade=True, color=\"orange\" , ax= axes[1])","e26691c7":"family_df = dftrain.loc[:,[\"Parch\", \"SibSp\", \"Survived\"]]\n\n# Create a family size variable including the passenger themselves\nfamily_df[\"Fsize\"] = family_df.SibSp + family_df.Parch + 1\n\nfamily_df.head()\n","efa2137f":"plt.figure(figsize=(15,5))\n\nplt.title(\"FAMILY_SIZE vs SURVIVAL\")\nsns.countplot(x='Fsize', hue=\"Survived\", data=family_df)","e369d5c6":"dftrain.info()","64f1e0d3":"dftest.info()","a9b041a6":"dftrain.loc[[61,829],\"Embarked\"] = 'C'","376cf635":"dftrain[\"Age\"] = age_slice\ndftrain=dftrain.drop('Age_bin',axis=1)\ndftrain.info()","22b4f7b4":"dftrain['Fsize']=family_df['Fsize']","45f9b6ba":"family_df_t= dftest.loc[:,[\"Parch\", \"SibSp\", \"Survived\"]]\n\n# Create a family size variable including the passenger themselves\nfamily_df_t[\"Fsize\"] = family_df_t.SibSp + family_df_t.Parch + 1\ndftest['Fsize']=family_df_t['Fsize']\nfamily_df_t.head()\ndftest['Fsize']=family_df_t['Fsize']","df996be0":"dftest.loc[[152],\"Fare\"] = 10","11d40b74":"family_df_tr= dftrain.loc[:,[\"Parch\", \"SibSp\", \"Survived\"]]\n\n# Create a family size variable including the passenger themselves\nfamily_df_tr[\"Fsize\"] = family_df_tr.SibSp + family_df_tr.Parch + 1\n\nfamily_df_tr.head()\ndftrain['Fsize']=family_df_tr['Fsize']","8bec4b4c":"import scipy.stats as stats\nfrom scipy.stats import chi2_contingency\n\nclass ChiSquare:\n    def __init__(self, dataframe):\n        self.df = dataframe\n        self.p = None #P-Value\n        self.chi2 = None #Chi Test Statistic\n        self.dof = None\n        \n        self.dfObserved = None\n        self.dfExpected = None\n        \n    def _print_chisquare_result(self, colX, alpha):\n        result = \"\"\n        if self.p<alpha:\n            result=\"{0} is IMPORTANT for Prediction\".format(colX)\n        else:\n            result=\"{0} is NOT an important predictor. (Discard {0} from model)\".format(colX)\n\n        print(result)\n        \n    def TestIndependence(self,colX,colY, alpha=0.05):\n        X = self.df[colX].astype(str)\n        Y = self.df[colY].astype(str)\n        \n        self.dfObserved = pd.crosstab(Y,X) \n        chi2, p, dof, expected = stats.chi2_contingency(self.dfObserved.values)\n        self.p = p\n        self.chi2 = chi2\n        self.dof = dof \n        \n        self.dfExpected = pd.DataFrame(expected, columns=self.dfObserved.columns, index = self.dfObserved.index)\n        \n        self._print_chisquare_result(colX,alpha)\n\n#Initialize ChiSquare Class\ncT = ChiSquare(dftrain)\n\n#Feature Selection\ntestColumns = ['Embarked','Cabin','Pclass','Age','Name','Fare','Fare_bin','Fsize']\nfor var in testColumns:\n    cT.TestIndependence(colX=var,colY=\"Survived\" )","b9ba255c":"# Make a copy of the titanic data frame\ndftrain['Title'] = dftrain['Name']\n# Grab title from passenger names\ndftrain[\"Title\"].replace(to_replace='(.*, )|(\\\\..*)', value='', inplace=True, regex=True)\n\nrare_titles = ['Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer']\ndftrain['Title'].replace(rare_titles, \"Rare title\", inplace=True)\n# Also reassign mlle, ms, and mme accordingly\ndftrain['Title'].replace([\"Mlle\",\"Ms\", \"Mme\"], [\"Miss\", \"Miss\", \"Mrs\"], inplace=True)","a157f29c":"cT = ChiSquare(dftrain)\n\n#Feature Selection\ntestColumns = ['Embarked','Cabin','Pclass','Age','Name','Fare','Fare_bin','Fsize','Title','SibSp','Parch']\nfor var in testColumns:\n    cT.TestIndependence(colX=var,colY=\"Survived\" )  ","d04fae2f":"dftest=dftest.drop(['Ticket','PassengerId'],axis=1)","2ded0a3a":"dftest['Title'] = dftest['Name']\n# Grab title from passenger names\n\ndftest[\"Title\"].replace(to_replace='(.*, )|(\\\\..*)', value='', inplace=True, regex=True)","2899ac00":"dftrain=dftrain.drop('Name',axis=1)","25f61a61":"dftrain.head().T","8f7a343e":"context1 = {\"female\":0 , \"male\":1}\ncontext2 = {\"S\":0 , \"C\":1 , \"Q\":2}\ncontext3= {\"Mr\":0 , \"Mrs\":1 , \"Miss\":2,'Master':3}\n\ndftrain['Sex_bool']=dftrain.Sex.map(context1)\ndftrain[\"Embarked_bool\"] = dftrain.Embarked.map(context2)\ndftrain['Title']=dftrain.Title.map(context3)","4fc5b47a":"dftrain=dftrain.drop(['PassengerId','Cabin','Ticket'],axis=1)\ndftrain=dftrain.drop(['Embarked','Sex'],axis=1)\ndftrain.head().T","6dee093a":"dftest.head().T","076f7d44":"dftest=dftest.drop(['Name','Sex','Embarked'],axis=1)","f0ea86b5":"for x in [dftrain, dftest,df]:\n    x['Age_bin']=np.nan\n    for i in range(8,0,-1):\n        x.loc[ x['Age'] <= i*10, 'Age_bin'] = i","c633aa49":"for x in [dftrain, dftest,df]:\n    x['Fare_bin']=np.nan\n    for i in range(12,0,-1):\n        x.loc[ x['Fare'] <= i*10, 'Fare_bin'] = i","8a46d0ae":"dftrain=dftrain.drop('Age',axis=1)\ndftest=dftest.drop('Age',axis=1)","876cae5b":"dftrain=dftrain.convert_objects(convert_numeric=True)","8d4bffe9":"def change_type(df):\n    float_list=list(df.select_dtypes(include=[\"float\"]).columns)\n    print(float_list)\n    for col in float_list:\n        df[col]=df[col].fillna(0).astype(np.int64)\n        \n    return df    \nchange_type(dftrain)   \ndftrain.dtypes","976ed7a0":"dftrain.head().T","403e4049":"x=dftrain.iloc[:,1:].values\ny=dftrain.iloc[:,0].values\nprint(dftrain.columns)\nprint(dftest.columns)\n\nX_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=101)","ebae6e22":"dftest=dftest.convert_objects(convert_numeric=True)\nchange_type(dftest)    \ndftest.dtypes","d3e6908c":"MLA = []\nZ = [LinearSVC() , DecisionTreeClassifier() , LogisticRegression() , KNeighborsClassifier() , GaussianNB() ,\n    RandomForestClassifier() , GradientBoostingClassifier()]\nX = [\"LinearSVC\" , \"DecisionTreeClassifier\" , \"LogisticRegression\" , \"KNeighborsClassifier\" , \"GaussianNB\" ,\n    \"RandomForestClassifier\" , \"GradientBoostingClassifier\"]\n\nfor i in range(0,len(Z)):\n    model = Z[i]\n    model.fit( X_train , y_train )\n    pred = model.predict(X_test)\n    MLA.append(accuracy_score(pred , y_test))\nMLA    ","9ec16c1f":"d = { \"Accuracy\" : MLA , \"Algorithm\" : X }\ndfm = pd.DataFrame(d)\ndfm","d3410387":"sns.barplot(dfm['Accuracy'],dfm['Algorithm'])","8721a2c2":"#Logistic Regression \nparams={'C':[1,100,0.01,0.1,1000],'penalty':['l2','l1']}\nlogreg=LogisticRegression()\ngscv=GridSearchCV(logreg,param_grid=params,cv=10)\n%timeit gscv.fit(x,y)","6c6dc483":"print(\"BEST PARAMS: \",gscv.best_params_)\nlogregscore=gscv.best_score_\nprint(\"BEST SCORE:\",logregscore)","942d2651":"print(\"SCORE:\",gscv.score(X_test,y_test))","294296da":"#KNN\nparam={'n_neighbors':[3,4,5,6,8,9,10],'metric':['euclidean','manhattan','chebyshev','minkowski'] }       \nknn = KNeighborsClassifier()\ngsknn=GridSearchCV(knn,param_grid=param,cv=10)\ngsknn.fit(x,y) \n\nprint(\"BEST PARAMS: \",gsknn.best_params_)\nprint(\"BEST SCORE:\",gsknn.best_score_)","6504b57e":"Survived=gsknn.predict(X_test)","c0f85341":"print(\"SCORE :\",gsknn.score(X_test,y_test))","3e11b549":"#Random Forest\nrfcv=RandomForestClassifier(n_estimators=500,max_depth=6)\nrfcv.fit(X_train,y_train)\nrfcv.predict(X_test)\nprint(\"SCORE:\", rfcv.score(X_test,y_test))","c385f7e6":"#Gradient Boosting\ngbcv=GradientBoostingClassifier(learning_rate=0.001,n_estimators=2000,max_depth=5)\ngbcv.fit(X_train,y_train)\ngbcv.predict(X_test)\nprint(\"SCORE:\",gbcv.score(X_test,y_test))","bdc4b84c":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, gscv.predict_proba(X_test)[:,1])\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, rfcv.predict_proba(X_test)[:,1])\nknn_fpr, knn_tpr, knn_thresholds = roc_curve(y_test, gsknn.predict_proba(X_test)[:,1])\ngbc_fpr, gbc_tpr, ada_thresholds = roc_curve(y_test, gbcv.predict_proba(X_test)[:,1])\n\nplt.figure(figsize=(9,9))\nlog_roc_auc = roc_auc_score(y_test, gscv.predict(X_test))\nprint (\"logreg model AUC = {} \" .format(log_roc_auc))\nrf_roc_auc = roc_auc_score(y_test, rfcv.predict(X_test))\nprint (\"random forest model AUC ={}\" .format(rf_roc_auc))\nknn_roc_auc = roc_auc_score(y_test, gsknn.predict(X_test))\nprint (\"KNN model AUC = {}\" .format(knn_roc_auc))\ngbc_roc_auc = roc_auc_score(y_test, gbcv.predict(X_test))\nprint (\"GBC Boost model AUC = {}\" .format(gbc_roc_auc))\n# Plot Logistic Regression ROC\nplt.plot(fpr, tpr, label='Logistic Regression')\n\n# Plot Random Forest ROC\nplt.plot(rf_fpr, rf_tpr, label='Random Forest')\n\n# Plot Decision Tree ROC\nplt.plot(knn_fpr, knn_tpr, label=' KnnClassifier')\n\n# Plot GradientBooseting Boost ROC\nplt.plot(gbc_fpr, gbc_tpr, label='GradientBoostingclassifier')\n\n# Plot Base Rate ROC\nplt.plot([0,1], [0,1],label='Base Rate',linestyle=\"--\")\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Graph')\nplt.legend(loc=\"lower right\")\n\nplt.arrow(0.5, 0.1, -0.12, 0.2, head_width=0.02, head_length=0.05, fc='k', ec='k')\nplt.show()","48cf3f93":"test_PassengerId = pd.read_csv('..\/input\/gender_submission.csv')['PassengerId']\nsubmission = pd.concat([pd.DataFrame(test_PassengerId), pd.DataFrame({'Survived':Survived})], axis=1)\nsubmission.to_csv('submission.csv', index=False)","8a608050":"Most of the people who died were from Passenger Class 3 irrespective of Gender","caa00869":"## Inferences from the above graph\n* Most passengers were aged from 20-40\n* Most passengers paid nearly 40 units $\/Rs\n\n### As the graph is left-skewed we can use log scale or sqrt scale to change this","423d0230":"The above graph makes it clear that most of the people were aged between 20-50","723dbee7":"<a id=\"1\"><\/a> \n# 1. Understanding the data at hand","cec63ea3":"The above stats show us survival of each class and its clear the ones in better class had a better chance of survival\n## Power of money","1162d34f":"<font color=\"red\" size=3>Please upvote this kernel if you like it. It motivates me to produce more quality content :)<\/font>","ddf92565":"The arrow is pointing towards `Base Rate`.  If any line is below it then model is performing in the worst manner possible.","d2b6af61":"Above we are creating boolean values for the model to understand ","494196f3":"<a id=\"4\"><\/a> \n# 4. Modelling","b50a6cd4":"It is clear from vizualisation that most of the survivors were children and women ","67ab4232":" Using Chi- Squared test at 5% we get the above results telling us which are important  features\n","e7a0191e":"Bigger the family lesser the chance of survival\n","dbf7e9b6":"<a id=\"3\"><\/a> \n# 3.Feature Engineering","7ba6dfc4":"Most of the embarkments were from class : S\n\nLeast embarkments were from class : Q","fc3e4e76":" In this notebook we will see how different people either survived or lost their lives who were present on the great RMS Titanic. \n \n \n<center><img src=\"https:\/\/www.printwand.com\/blog\/media\/2012\/01\/titanic-sinking.jpg\" width=\"500px\"><\/center>","7c74f127":"Making some changes in the titles ","4eb0455f":"\n<a class=\"anchor\" id=\"toc\"><\/a>\n<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 600px;\">\n<h1>Contents<\/h1>\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#1\">1 Understanding the data at hand<\/a><\/li>\n\n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#2\">2 Exploratory Data Analysis<\/a><\/li>\n    \n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#3\">3 Feature Engineering<\/a><\/li>\n    \n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#4\">4 Modelling<\/a><\/li>\n      <ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">    \n\n<\/ul>\n<\/div>","d96418c0":"<font color=\"red\" size=5><center>TITANIC: MACHINE LEARNING FROM DISASTER<\/center><\/font>","392d0c1f":"# 5.Tuning the parameters","6fd45247":"From above statistics it is clear that Women were given more preference than Men while evacuation  ","5bc9d76b":"<a id=\"2\"><\/a> \n# 2. Exploratory Data Analysis","34c4bc62":"## Inferences from the above heatmap\n*  PassengerId is a redundant column as its very much less related to all other attributes , we can remove it .\n* Also , Survived is related indirectly with Pclass and also we earlier proved that as Pclass value increases Survival decreases\n* Pclass and Age are also inversely related and can also be proven by the following cell that as Pclass decreases , the mean of the Age      increases , means the much of the older travellers are travelling in high class .\n* Pclass and fare are also highly inversely related as the fare of Pclass 1 would obviously be higher than corresponding Pclass 2 and 3 .\n* Also , people with lower ages or children are travelling with their sibling and parents more than higher aged people (following an                inverse relation) , which is quite a bit obvious .\n* Parch and SibSp are also highly directly related\n* Sex_bool and Survived people are highly inversely related , i.e. females are more likely to survive than men"}}