{"cell_type":{"9258891c":"code","bea84438":"code","67f23444":"code","80e4b0fc":"code","247ccaad":"code","799a0c56":"code","c6c02417":"code","6f8732f1":"code","6a8af7ad":"code","026a3bf3":"code","59241199":"code","7d9d1bba":"code","7a6f4212":"code","f618d0c9":"code","32535429":"code","df45d66a":"code","46545966":"code","736cf302":"code","c36ee452":"code","e12612d1":"code","402aef44":"code","dfa33163":"code","7243126c":"code","a674efb6":"code","4dcaa6b5":"code","49422dc7":"code","fdb3a034":"code","de4be661":"code","27b13df5":"code","cae16663":"code","5348ec39":"code","c20fb4b5":"code","501b3918":"code","364377f7":"code","29f9472f":"code","1aaf20b0":"code","aed9dc97":"code","0eb90f96":"code","1b9033d9":"code","e264c3db":"code","1a10c0e9":"code","0fa271bd":"code","53c06dab":"code","7e3db8aa":"code","d3af6ccf":"code","bf0d5272":"code","e6c2a750":"code","301aef3f":"code","deb1a34e":"code","1859e07c":"code","25c8f969":"code","1ac53b37":"code","bd25becc":"code","6eb1eb8d":"code","e4d7479a":"code","1ed1f7a4":"code","261fe529":"code","3d679f40":"code","ee635e22":"code","752af5ea":"code","6ac1f478":"code","2b89cd04":"code","09b277a3":"code","c74e8843":"code","80a680cb":"code","4423b67a":"code","8ea2b755":"code","035ee849":"code","0f0268c6":"code","92f318d7":"code","72a954a8":"code","3b4d663b":"code","fd512802":"code","b40a739d":"code","9dd58ca3":"code","cc561df5":"code","19cd5cf6":"code","6814799c":"code","fa6a2d58":"code","98c0d14c":"code","3b2cc63f":"code","5a2ef193":"code","208001be":"code","f8debd3d":"code","2d0b6eee":"code","209eba9c":"code","db0b003f":"code","57547cb9":"code","4c3d8751":"code","b336a94c":"markdown","c9305682":"markdown","c2fd46bc":"markdown","f8891aca":"markdown","f40fe725":"markdown","6a0d1345":"markdown","8479a26d":"markdown","b279726c":"markdown","1abd2a9d":"markdown","44508aa9":"markdown","1e49bee0":"markdown","82b8df11":"markdown","3d55d2bc":"markdown","5a23cc89":"markdown","de81d600":"markdown","a3cf77c0":"markdown","449e9649":"markdown","f187e75c":"markdown","8f6c5941":"markdown","630a58eb":"markdown","253b250f":"markdown","4fc934a5":"markdown","207dcb44":"markdown","79657bf5":"markdown","b6bbe624":"markdown","d0ac2b86":"markdown","02e0b650":"markdown","f012cbca":"markdown","5a4d861e":"markdown","f1da4436":"markdown","9302f9bf":"markdown","41c993b8":"markdown","4f8611ae":"markdown","1d2aba49":"markdown","5212464f":"markdown","9e072476":"markdown","e9ce17e6":"markdown","14b060d0":"markdown","24dd2ef3":"markdown","8127d814":"markdown","1820796c":"markdown","23ffd467":"markdown","516b7ee2":"markdown","e55fad2e":"markdown","cb462719":"markdown","86188d36":"markdown","046e2731":"markdown","260b7568":"markdown","0efccb15":"markdown","426a846c":"markdown","28385713":"markdown","f2ecf7a3":"markdown","e93710b1":"markdown"},"source":{"9258891c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\")\n\nimport seaborn as sns\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bea84438":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_PassengerId = test_df[\"PassengerId\"]","67f23444":"train_df.columns","80e4b0fc":"train_df.head()","247ccaad":"train_df.describe()","799a0c56":"train_df.info()","c6c02417":"def bar_plot(variable):\n    \"\"\"\n        input: variable ex: \"Sex\"\n        output: bar plot & value count\n    \"\"\"\n    # get feature\n    var = train_df[variable]\n    # count number of categorical variable(value\/sample)\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}: \\n {}\".format(variable,varValue))","6f8732f1":"category1 = [\"Survived\",\"Sex\",\"Pclass\",\"Embarked\",\"SibSp\", \"Parch\"]\nfor c in category1:\n    bar_plot(c)","6a8af7ad":"category2 = [\"Cabin\", \"Name\", \"Ticket\"]\nfor c in category2:\n    print(\"{} \\n\".format(train_df[c].value_counts()))","026a3bf3":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(train_df[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(variable))\n    plt.show()","59241199":"numericVar = [\"Fare\", \"Age\",\"PassengerId\"]\nfor n in numericVar:\n    plot_hist(n)","7d9d1bba":"# Plcass vs Survived\ntrain_df[[\"Pclass\",\"Survived\"]].groupby([\"Pclass\"], as_index = False).mean().sort_values(by=\"Survived\",ascending = False)","7a6f4212":"# Sex vs Survived\ntrain_df[[\"Sex\",\"Survived\"]].groupby([\"Sex\"], as_index = False).mean().sort_values(by=\"Survived\",ascending = False)","f618d0c9":"# Sibsp vs Survived\ntrain_df[[\"SibSp\",\"Survived\"]].groupby([\"SibSp\"], as_index = False).mean().sort_values(by=\"Survived\",ascending = False)","32535429":"# Parch vs Survived\ntrain_df[[\"Parch\",\"Survived\"]].groupby([\"Parch\"], as_index = False).mean().sort_values(by=\"Survived\",ascending = False)","df45d66a":"def detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","46545966":"train_df.loc[detect_outliers(train_df,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])]","736cf302":"# drop outliers\ntrain_df = train_df.drop(detect_outliers(train_df,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"]),axis = 0).reset_index(drop = True)","c36ee452":"train_df_len = len(train_df)\ntrain_df = pd.concat([train_df,test_df],axis = 0).reset_index(drop = True)","e12612d1":"train_df.head()","402aef44":"train_df.columns[train_df.isnull().any()]","dfa33163":"train_df.isnull().sum()","7243126c":"train_df[train_df[\"Embarked\"].isnull()]","a674efb6":"train_df.boxplot(column=\"Fare\",by = \"Embarked\")\nplt.show()","4dcaa6b5":"train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(\"C\")\ntrain_df[train_df[\"Embarked\"].isnull()]","49422dc7":"train_df[train_df[\"Fare\"].isnull()]","fdb3a034":"train_df[\"Fare\"] = train_df[\"Fare\"].fillna(np.mean(train_df[train_df[\"Pclass\"] == 3][\"Fare\"]))","de4be661":"train_df[train_df[\"Fare\"].isnull()]","27b13df5":"list1 = [\"SibSp\", \"Parch\", \"Age\", \"Fare\", \"Survived\"]\nsns.heatmap(train_df[list1].corr(), annot = True, fmt = \".2f\")\nplt.show()","cae16663":"g = sns.factorplot(x = \"SibSp\", y = \"Survived\", data = train_df, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","5348ec39":"g = sns.factorplot(x = \"Parch\", y = \"Survived\", kind = \"bar\", data = train_df, size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","c20fb4b5":"g = sns.factorplot(x = \"Pclass\", y = \"Survived\", data = train_df, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","501b3918":"g = sns.FacetGrid(train_df, col = \"Survived\")\ng.map(sns.distplot, \"Age\", bins = 25)\nplt.show()","364377f7":"g = sns.FacetGrid(train_df, col = \"Survived\", row = \"Pclass\", size = 2)\ng.map(plt.hist, \"Age\", bins = 25)\ng.add_legend()\nplt.show()","29f9472f":"g = sns.FacetGrid(train_df, row = \"Embarked\", size = 2)\ng.map(sns.pointplot, \"Pclass\",\"Survived\",\"Sex\")\ng.add_legend()\nplt.show()","1aaf20b0":"g = sns.FacetGrid(train_df, row = \"Embarked\", col = \"Survived\", size = 2.3)\ng.map(sns.barplot, \"Sex\", \"Fare\")\ng.add_legend()\nplt.show()","aed9dc97":"train_df[train_df[\"Age\"].isnull()]","0eb90f96":"sns.factorplot(x = \"Sex\", y = \"Age\", data = train_df, kind = \"box\")\nplt.show()","1b9033d9":"sns.factorplot(x = \"Sex\", y = \"Age\", hue = \"Pclass\",data = train_df, kind = \"box\")\nplt.show()","e264c3db":"sns.factorplot(x = \"Parch\", y = \"Age\", data = train_df, kind = \"box\")\nsns.factorplot(x = \"SibSp\", y = \"Age\", data = train_df, kind = \"box\")\nplt.show()","1a10c0e9":"sns.heatmap(train_df[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(), annot = True)\nplt.show()","0fa271bd":"index_nan_age = list(train_df[\"Age\"][train_df[\"Age\"].isnull()].index)\nfor i in index_nan_age:\n    age_pred = train_df[\"Age\"][((train_df[\"SibSp\"] == train_df.iloc[i][\"SibSp\"]) &(train_df[\"Parch\"] == train_df.iloc[i][\"Parch\"])& (train_df[\"Pclass\"] == train_df.iloc[i][\"Pclass\"]))].median()\n    age_med = train_df[\"Age\"].median()\n    if not np.isnan(age_pred):\n        train_df[\"Age\"].iloc[i] = age_pred\n    else:\n        train_df[\"Age\"].iloc[i] = age_med","53c06dab":"train_df[train_df[\"Age\"].isnull()]","7e3db8aa":"train_df[\"Name\"].head(10)","d3af6ccf":"name = train_df[\"Name\"]\ntrain_df[\"Title\"] = [i.split(\".\")[0].split(\",\")[-1].strip() for i in name]","bf0d5272":" train_df[\"Title\"].head(10)","e6c2a750":"sns.countplot(x=\"Title\", data = train_df)\nplt.xticks(rotation = 60)\nplt.show()","301aef3f":"# convert to categorical\ntrain_df[\"Title\"] = train_df[\"Title\"].replace([\"Lady\",\"the Countess\",\"Capt\",\"Col\",\"Don\",\"Dr\",\"Major\",\"Rev\",\"Sir\",\"Jonkheer\",\"Dona\"],\"other\")\ntrain_df[\"Title\"] = [0 if i == \"Master\" else 1 if i == \"Miss\" or i == \"Ms\" or i == \"Mlle\" or i == \"Mrs\" else 2 if i == \"Mr\" else 3 for i in train_df[\"Title\"]]\ntrain_df[\"Title\"].head(20)","deb1a34e":"sns.countplot(x=\"Title\", data = train_df)\nplt.xticks(rotation = 60)\nplt.show()","1859e07c":"g = sns.factorplot(x = \"Title\", y = \"Survived\", data = train_df, kind = \"bar\")\ng.set_xticklabels([\"Master\",\"Mrs\",\"Mr\",\"Other\"])\ng.set_ylabels(\"Survival Probability\")\nplt.show()","25c8f969":"train_df.drop(labels = [\"Name\"], axis = 1, inplace = True)","1ac53b37":"train_df.head()","bd25becc":"train_df = pd.get_dummies(train_df,columns=[\"Title\"])\ntrain_df.head()","6eb1eb8d":"train_df.head()","e4d7479a":"train_df[\"Fsize\"] = train_df[\"SibSp\"] + train_df[\"Parch\"] + 1","1ed1f7a4":"train_df.head()","261fe529":"g = sns.factorplot(x = \"Fsize\", y = \"Survived\", data = train_df, kind = \"bar\")\ng.set_ylabels(\"Survival\")\nplt.show()","3d679f40":"train_df[\"family_size\"] = [1 if i < 5 else 0 for i in train_df[\"Fsize\"]]","ee635e22":"train_df.head(10)","752af5ea":"sns.countplot(x = \"family_size\", data = train_df)\nplt.show()","6ac1f478":"g = sns.factorplot(x = \"family_size\", y = \"Survived\", data = train_df, kind = \"bar\")\ng.set_ylabels(\"Survival\")\nplt.show()","2b89cd04":"train_df = pd.get_dummies(train_df, columns= [\"family_size\"])\ntrain_df.head()","09b277a3":"train_df[\"Embarked\"].head()","c74e8843":"sns.countplot(x = \"Embarked\", data = train_df)\nplt.show()","80a680cb":"train_df = pd.get_dummies(train_df, columns=[\"Embarked\"])\ntrain_df.head()","4423b67a":"train_df[\"Ticket\"].head(20)","8ea2b755":"a = \"A\/5. 2151\"\na.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0]","035ee849":"tickets = []\nfor i in list(train_df.Ticket):\n    if not i.isdigit():\n        tickets.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0])\n    else:\n        tickets.append(\"x\")\ntrain_df[\"Ticket\"] = tickets","0f0268c6":"train_df[\"Ticket\"].head(20)","92f318d7":"train_df.head()","72a954a8":"train_df = pd.get_dummies(train_df, columns= [\"Ticket\"], prefix = \"T\")\ntrain_df.head(10)","3b4d663b":"sns.countplot(x = \"Pclass\", data = train_df)\nplt.show()","fd512802":"train_df[\"Pclass\"] = train_df[\"Pclass\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df, columns= [\"Pclass\"])\ntrain_df.head()","b40a739d":"train_df[\"Sex\"] = train_df[\"Sex\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df, columns=[\"Sex\"])\ntrain_df.head()","9dd58ca3":"train_df.drop(labels = [\"PassengerId\", \"Cabin\"], axis = 1, inplace = True)","cc561df5":"train_df.columns","19cd5cf6":"train_df.head(10)","6814799c":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","fa6a2d58":"train_df_len","98c0d14c":"test = train_df[train_df_len:]\ntest.drop(labels = [\"Survived\"],axis = 1, inplace = True)\n","3b2cc63f":"test.head()","5a2ef193":"train = train_df[:train_df_len]\nX_train = train.drop(labels = \"Survived\", axis = 1)\ny_train = train[\"Survived\"]","208001be":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nxgb_model = XGBClassifier(\n        objective = 'binary:logistic',\n        colsample_bytree = 0.8,\n        learning_rate = 0.3,\n        max_depth = 7,\n        min_child_weight = 3,\n        n_estimators = 100,\n        subsample = 0.6)\n\n# {'colsample_bytree': 0.5,\n#  'learning_rate': 0.1,\n#  'max_depth': 7,\n#  'min_child_weight': 3,\n#  'n_estimators': 100,\n#  'objective': 'binary:logistic',\n#  'subsample': 0.7}\n\n%time xgb_model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_train, y_train)], verbose=False)\ny_pred_xgb = xgb_model.predict(X_train)\na_xgb = accuracy_score(y_train, y_pred_xgb)\n\nprint(\"AS: \", a_xgb)","f8debd3d":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=xgb_model,X=X_train,y=y_train,cv=5)\nprint(\"Accuracy: {:.2f}%\".format(accuracies.mean()*100))","2d0b6eee":"#XGBoost hyper-parameter tuning\n#from sklearn.metrics import mean_squared_error\n#from sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\ndef hyperParameterTuning(x_train, y_train):\n    param_tuning = {\n        'learning_rate': [0.01, 0.1],\n        'max_depth': [3, 5, 7, 10],\n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.5, 0.7],\n        'colsample_bytree': [0.5, 0.7],\n        'n_estimators' : [100, 200, 500],\n        'objective': ['binary:logistic']\n    }\n\n    xgb_model = XGBClassifier()\n\n    gsearch = GridSearchCV(estimator = xgb_model,\n                           param_grid = param_tuning,                        \n                           #scoring = 'neg_mean_absolute_error', #MAE\n                           #scoring = 'neg_mean_squared_error',  #MSE\n                           cv = 5,\n                           n_jobs = -1,\n                           verbose = 1)\n\n    gsearch.fit(X_train,y_train)\n\n    return gsearch.best_params_","209eba9c":"hyperParameterTuning(X_train, y_train)","db0b003f":"from sklearn.ensemble import StackingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import StackingRegressor,StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nlayer_one_estimators = [\n                        ('rf_1', RandomForestClassifier(n_estimators=40, random_state=42)),\n                        ('knn_1', KNeighborsClassifier(n_neighbors=6))             \n                       ]\nlayer_two_estimators = [\n                        ('rf_2', RandomForestClassifier(n_estimators=40, random_state=42)),\n                        #('rf_2', RandomForestRegressor(n_estimators=20, random_state=42)),\n                        ('xg_2', XGBClassifier(\n        objective = 'binary:logistic',\n        colsample_bytree = 0.8,\n        learning_rate = 0.3,\n        max_depth = 7,\n        min_child_weight = 3,\n        n_estimators = 100,\n        subsample = 0.6)\n)\n\n                       ]\nlayer_two = StackingClassifier(estimators=layer_two_estimators, final_estimator=RandomForestClassifier())\n\n# Create Final model by \nclf2 = StackingClassifier(estimators=layer_one_estimators, final_estimator=layer_two)","57547cb9":"clf2.fit(X_train,y_train)\ny_pred_xgb = clf2.predict(X_train)\nac_xgb = accuracy_score(y_train, y_pred_xgb)\n\nprint(\"MAE: \", ac_xgb)","4c3d8751":"test_survived = pd.Series(xgb_model.predict(test), name = \"Survived\").astype(int)\nresults = pd.concat([test_PassengerId, test_survived],axis = 1)\nresults.to_csv(\"titanic.csv\", index = False)","b336a94c":"<a id = \"30\"><\/a><br>\n## XgBoost Fine Tuning\n\n**We are now using a XgBoost Classifier with Tuning using GridSearch CV**","c9305682":"# Introduction\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this contest, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\nThis Kaggle Getting Started Competition provides an ideal starting place for people who may not have a lot of experience in data science and machine learning.\"\n\n<font color = 'blue'>\nContent: \n\n1. [Load and Check Data](#1)\n1. [Variable Description](#2)\n    * [Univariate Variable Analysis](#3)\n        * [Categorical Variable](#4)\n        * [Numerical Variable](#5)\n1. [Basic Data Analysis](#6)\n1. [Outlier Detection](#7)\n1. [Missing Value](#8)\n    * [Find Missing Value](#9)\n    * [Fill Missing Value](#10)\n1. [Visualization](#11)\n    * [Correlation Between Sibsp -- Parch -- Age -- Fare -- Survived](#12)\n    * [SibSp -- Survived](#13)\n    * [Parch -- Survived](#14)\n    * [Pclass -- Survived](#15)\n    * [Age -- Survived](#16)\n    * [Pclass -- Survived -- Age](#17)\n    * [Embarked -- Sex -- Pclass -- Survived](#18)\n    * [Embarked -- Sex -- Fare -- Survived](#19)\n    * [Fill Missing: Age Feature](#20)\n1. [Feature Engineering](#21)\n    * [Name -- Title](#22)\n    * [Family Size](#23)\n    * [Embarked](#24)\n    * [Ticket](#25)\n    * [Pclass](#26)\n    * [Sex](#27)\n    * [Drop Passenger ID and Cabin](#28)\n1. [Modeling](#29)\n    * [Xgboost,Stacked Ensembels](#30)","c2fd46bc":"<a id = \"29\"><\/a><br>\n# Modeling","f8891aca":"* pclass is important feature for model training.","f40fe725":"<a id = \"4\"><\/a><br>\n## Categorical Variable","6a0d1345":"<a id = \"27\"><\/a><br>\n## Sex","8479a26d":"<a id = \"18\"><\/a><br>\n## Embarked -- Sex -- Pclass -- Survived","b279726c":"<a id = \"26\"><\/a><br>\n## Pclass","1abd2a9d":"<a id = \"6\"><\/a><br>\n# Basic Data Analysis\n* Pclass - Survived\n* Sex - Survived\n* SibSp - Survived\n* Parch - Survived","44508aa9":"<a id = \"10\"><\/a><br>\n## Fill Missing Value\n* Embarked has 2 missing value\n* Fare has only 1","1e49bee0":"<a id = \"25\"><\/a><br>\n## Ticket","82b8df11":"### K-Folds Cross Validation","3d55d2bc":"### Reference's\n\n1. EDA - https:\/\/www.kaggle.com\/kanncaa1\/dataiteam-titanic-eda","5a23cc89":"## Fine Tuning the Xgb Model","de81d600":"Age is not correlated with sex but it is correlated with parch, sibsp and pclass.","a3cf77c0":"Sex is not informative for age prediction, age distribution seems to be same.","449e9649":"<a id = \"5\"><\/a><br>\n## Numerical Variable","f187e75c":"<a id = \"2\"><\/a><br>\n# Variable Description\n1. PassengerId: unique id number to each passenger\n1. Survived: passenger survive(1) or died(0)\n1. Pclass: passenger class\n1. Name: name\n1. Sex: gender of passenger \n1. Age: age of passenger \n1. SibSp: number of siblings\/spouses\n1. Parch: number of parents\/children \n1. Ticket: ticket number \n1. Fare: amount of money spent on ticket\n1. Cabin: cabin category\n1. Embarked: port where passenger embarked (C = Cherbourg, Q = Queenstown, S = Southampton)\n","8f6c5941":"* age <= 10 has a high survival rate,\n* oldest passengers (80) survived,\n* large number of 20 years old did not survive,\n* most passengers are in 15-35 age range,\n* use age feature in training\n* use age distribution for missing value of age","630a58eb":"<a id = \"9\"><\/a><br>\n## Find Missing Value","253b250f":"<a id = \"13\"><\/a><br>\n## SibSp -- Survived","4fc934a5":"![1_5DEn2LeQsrwtWVNy3w6I5w.jpeg](attachment:1_5DEn2LeQsrwtWVNy3w6I5w.jpeg)","207dcb44":"<font size=\"+3\" color=purple ><b> <center><u>Titanic:EDA+Stacked Ensembels Accuracy 0.80%<\/u><\/center><\/b><\/font>","79657bf5":"<a id = \"21\"><\/a><br>\n# Feature Engineering","b6bbe624":"<a id = \"22\"><\/a><br>\n## Name -- Title","d0ac2b86":"Fare feature seems to have correlation with survived feature (0.26).","02e0b650":"![1_1ArQEf8OFkxVOckdWi7mSA.png](attachment:1_1ArQEf8OFkxVOckdWi7mSA.png)","f012cbca":"<a id = \"16\"><\/a><br>\n## Age -- Survived","5a4d861e":"### Generate titanic.csv","f1da4436":"<a id = \"1\"><\/a><br>\n# Load and Check Data ","9302f9bf":"## Congrats you have made your first Submission \ud83c\udf89\ud83c\udf8a","41c993b8":"<a id = \"8\"><\/a><br>\n# Missing Value\n* Find Missing Value\n* Fill Missing Value","4f8611ae":"<a id = \"12\"><\/a><br>\n## Correlation Between Sibsp -- Parch -- Age -- Fare -- Survived","1d2aba49":"<a id = \"20\"><\/a><br>\n## Fill Missing: Age Feature","5212464f":"<a id = \"14\"><\/a><br>\n## Parch -- Survived","9e072476":"<a id = \"24\"><\/a><br>\n## Embarked","e9ce17e6":"<font size=\"+3\" color=orange ><b> <center><u>Upvote If you like it!!\ud83d\udc4d\ud83d\ude01<\/u><\/center><\/b><\/font>","14b060d0":"<a id = \"19\"><\/a><br>\n## Embarked -- Sex -- Fare -- Survived","24dd2ef3":"* Having a lot of SibSp have less chance to survive.\n* if sibsp == 0 or 1 or 2, passenger has more chance to survive\n* we can consider a new feature describing these categories.","8127d814":"<a id = \"15\"><\/a><br>\n## Pclass -- Survived","1820796c":"<a id = \"28\"><\/a><br>\n## Drop Passenger ID and Cabin ","23ffd467":"<a id = \"23\"><\/a><br>\n## Family Size","516b7ee2":"Small familes have more chance to survive than large families.","e55fad2e":"<a id = \"7\"><\/a><br>\n# Outlier Detection","cb462719":"1st class passengers are older than 2nd, and 2nd is older than 3rd class. ","86188d36":"<a id = \"3\"><\/a><br>\n# Univariate Variable Analysis\n* Categorical Variable: Survived, Sex, Pclass, Embarked, Cabin, Name, Ticket, Sibsp and Parch\n* Numerical Variable: Fare, age and passengerId","046e2731":"## Stacked Ensembles ","260b7568":"* Female passengers have much better survival rate than males.\n* males have better surv\u015fval rate in pclass 3 in C.\n* embarked and sex will be used in training.","0efccb15":"<a id = \"17\"><\/a><br>\n## Pclass -- Survived -- Age","426a846c":"* Passsengers who pay higher fare have better survival. Fare can be used as categorical for training.","28385713":"* float64(2): Fare ve Age\n* int64(5): Pclass, sibsp, parch, passengerId and survived\n* object(5): Cabin, embarked, ticket, name and sex","f2ecf7a3":"* Sibsp and parch can be used for new feature extraction with th = 3\n* small familes have more chance to survive.\n* there is a std in survival of passenger with parch = 3","e93710b1":"<a id = \"11\"><\/a><br>\n# Visualization"}}