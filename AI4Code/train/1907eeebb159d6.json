{"cell_type":{"ea43548d":"code","753e975f":"code","5eebe268":"code","ec1b8039":"code","445fe606":"code","af60ea76":"code","b2f1b88e":"code","6d1be5b8":"code","ada51e10":"code","4957b850":"code","53c6a682":"code","71aebee7":"code","dba7b4ae":"code","00acca11":"code","11049cd8":"code","1f77c5e2":"code","dfd1fb2e":"code","b8a6f1d5":"code","e35b367c":"code","203e6549":"code","2448f57d":"code","4f84366c":"code","68474727":"code","5c29043c":"code","cebd3d60":"code","0906fc45":"code","342d0820":"code","7611fc99":"code","c32d6711":"code","a55877ef":"code","d2ced8b5":"code","af59f5ec":"code","f7197cfd":"code","d1e01546":"code","2ae8dfb7":"code","ff080108":"code","a3f83ac5":"code","17e14df2":"code","0a45e5f7":"code","bd99978a":"code","143c483c":"code","d6049c3e":"code","04698f5a":"code","b1397873":"code","e1eb74e5":"code","13b70481":"code","33c5ffec":"code","ce4ff13b":"code","880f1941":"code","f6df04a6":"code","aa85720a":"code","d0552530":"code","b42982d5":"code","70dd88c9":"code","59be645c":"code","088c78cf":"code","0b03b302":"code","dc187ecf":"code","c809afd3":"code","1f246c25":"code","1a03e004":"code","ce5bda96":"code","640cc767":"code","e5be54ef":"code","376e420e":"code","e913b9bd":"code","961a8a89":"code","f2c976fd":"code","138ebab1":"code","b04fc35b":"code","f3af87e9":"code","c52e7f6b":"code","c9f604d8":"code","4b96e56a":"code","00eaa105":"code","1c78f5cd":"markdown"},"source":{"ea43548d":"# Ignore warnings :\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Handle table-like data and matrices :\nimport numpy as np\nimport pandas as pd\nimport math \n\n\n\n# Modelling Algorithms :\n\n# Classification\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis , QuadraticDiscriminantAnalysis\n\n# Regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor \nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\n\n\n\n\n# Modelling Helpers :\nfrom sklearn.preprocessing import Imputer , Normalizer , scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import GridSearchCV , KFold , cross_val_score\n\n\n\n#preprocessing :\nfrom sklearn.preprocessing import MinMaxScaler , StandardScaler, Imputer, LabelEncoder\n\n#evaluation metrics :\n\n# Regression\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error \n\n# Classification\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  \n\n\n\n# Visualisation\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n#import missingno as msno\n\n\n\n# Configure visualisations\n%matplotlib inline\nmpl.style.use( 'ggplot' )\nplt.style.use('fivethirtyeight')\nsns.set(context=\"notebook\", palette=\"dark\", style = 'whitegrid' , color_codes=True)\nparams = { \n    'axes.labelsize': \"large\",\n    'xtick.labelsize': 'x-large',\n    'legend.fontsize': 20,\n    'figure.dpi': 150,\n    'figure.figsize': [25, 7]\n    \n}\nplt.rcParams.update(params)    \n%matplotlib inline\nfrom sklearn.metrics import r2_score","753e975f":"data = pd.read_csv(\"\/kaggle\/input\/diamonds\/diamonds.csv\")\ndata.head()","5eebe268":"data.shape","ec1b8039":"idsUnique = len(set(data['Unnamed: 0']))\nidsTotal = data.shape[0]\nidsdupe = idsTotal - idsUnique\nprint(idsdupe)\n#drop id col\n","445fe606":"data=data.drop(['Unnamed: 0'],axis=1)","af60ea76":"data.info()","b2f1b88e":"data_nas = data.isnull().sum()\ndata_nas = data_nas[data_nas>0]\ndata_nas.sort_values(ascending = False)","6d1be5b8":"data.describe()","ada51e10":"print(\"Number of rows with x == 0: {} \".format((data.x==0).sum()))\nprint(\"Number of rows with y == 0: {} \".format((data.y==0).sum()))\nprint(\"Number of rows with z == 0: {} \".format((data.z==0).sum()))\nprint(\"Number of rows with depth == 0: {} \".format((data.depth==0).sum()))","4957b850":"data[['x','y','z']] = data[['x','y','z']].replace(0,np.NaN)","53c6a682":"data.isnull().sum()","71aebee7":"data.dropna(inplace=True)","dba7b4ae":"data.shape","00acca11":"data.isnull().sum()","11049cd8":"categorical_features = data.select_dtypes(include=['object']).columns\ncategorical_features","1f77c5e2":"numerical_features = data.select_dtypes(exclude = [\"object\"]).columns\nnumerical_features","dfd1fb2e":"data_num = data[numerical_features]\ndata_cat = data[categorical_features]","b8a6f1d5":"data_num.head()","e35b367c":"data_cat.head()","203e6549":"data.describe()","2448f57d":"data['cut'].unique()","4f84366c":"sns.countplot(x='cut',data=data)","68474727":"g=sns.catplot(x='cut',y='price',data=data,kind='bar')","5c29043c":"data['cut'].value_counts()","cebd3d60":"data_cat['color'].unique()","0906fc45":"sns.countplot(x='color',data=data)","342d0820":"p=sns.catplot(x='color',y='price',data=data,kind='bar')\np","7611fc99":"data['color'].value_counts()","c32d6711":"data_cat['clarity'].unique()","a55877ef":"sns.countplot(x='clarity',data=data)","d2ced8b5":"sns.catplot(x='clarity',y='price',data=data,kind='bar')","af59f5ec":"data['clarity'].value_counts()","f7197cfd":"data_cat.head()","d1e01546":"diamond_onehot=data_cat.copy()","2ae8dfb7":"for i in range(data_cat.shape[1]):\n    diamond_onehot=pd.get_dummies(diamond_onehot,columns=[data_cat.columns[i]],prefix=[data_cat.columns[i]])\ndiamond_onehot","ff080108":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nsns.distplot(data['price'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(data['price'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Price distribution')\n\nfig = plt.figure()\nres = stats.probplot(data['price'], plot=plt)\nplt.show()","a3f83ac5":"data['price'] = np.log1p(data['price'] )\ny=data['price']\n\nsns.distplot(data['price'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(data['price'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Price distribution')\n\nfig = plt.figure()\nres = stats.probplot(data['price'], plot=plt)\nplt.show()","17e14df2":"data['price'] = np.log1p(data['price'] )\ny=data['price']\n\nsns.distplot(data['price'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(data['price'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Price distribution')\n\nfig = plt.figure()\nres = stats.probplot(data['price'], plot=plt)\nplt.show()","0a45e5f7":"data_num=data_num.drop(['price'],axis=1)","bd99978a":"from scipy.stats import skew \nskewness = data_num.apply(lambda x: skew(x))\nskewness.sort_values(ascending=False)","143c483c":"type(skewness)","d6049c3e":"skewness = skewness[abs(skewness)>0.1]\nskewness.index","04698f5a":"data_skew = data_num[skewness.index]\ndata_skew .columns","b1397873":"data_skew = np.log1p(data_skew )","e1eb74e5":"data_skew ","13b70481":"data_num","33c5ffec":"data_num1=data_num.drop(['carat', 'table', 'x', 'y', 'z'],axis=1)\ndata_numerical=pd.concat([data_skew ,data_num1],axis=1)\ndata_numerical","ce4ff13b":"data_final=pd.concat([data_numerical,diamond_onehot],axis=1)","880f1941":"data_normal=data_final","f6df04a6":"data_normal.describe()\ndata_final=pd.concat([data_normal,y],axis=1)","aa85720a":"data_final","d0552530":"print(\"Find most important features relative to target\")\ncorr = data_final.corr()\ncorr.sort_values([\"price\"], ascending = False, inplace = True)\ncorr\nprint(corr['price'])","b42982d5":"abc = corr['price'][abs(corr['price'])>0.05]\nabc.index","70dd88c9":"type(corr)","59be645c":"data_model=data_final[abc.index]\ndata_model","088c78cf":"data_model['volume'] = data_model['x']*data_model['y']*data_model['z']","0b03b302":"sns.jointplot(x='volume', y='price' , data=data_model, size=5)","dc187ecf":"data_model.drop(['x','y','z'], axis=1, inplace= True)","c809afd3":"data_model","1f246c25":"Data_out=data_model['price']\ninput_data=data_model.drop(['price'],axis=1)","1a03e004":"X_train, X_test, y_train, y_test = train_test_split(input_data,Data_out,test_size=0.2, random_state=66)","ce5bda96":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","640cc767":"X_train","e5be54ef":"# Collect all R2 Scores.\nR2_Scores = []\nmodels = ['Linear Regression' , 'AdaBoost Regression' , 'Ridge Regression' , 'GradientBoosting Regression',\n          'RandomForest Regression' ,\n         'KNeighbours Regression']","376e420e":"clf_lr = LinearRegression()\nclf_lr.fit(X_train , y_train)\naccuracies = cross_val_score(estimator = clf_lr, X = X_train, y = y_train, cv = 5,verbose = 1)\ny_pred = clf_lr.predict(X_test)\nprint('')\nprint('####### Linear Regression #######')\nprint('Score : %.4f' % clf_lr.score(X_test, y_test))\nprint(accuracies)\n\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred)**0.5\nr2 = r2_score(y_test, y_pred)\n\nprint('')\nprint('MSE    : %0.2f ' % mse)\nprint('MAE    : %0.2f ' % mae)\nprint('RMSE   : %0.2f ' % rmse)\nprint('R2     : %0.2f ' % r2)\n\nR2_Scores.append(r2)","e913b9bd":"clf_ar = AdaBoostRegressor(n_estimators=1000)\nclf_ar.fit(X_train , y_train)\naccuracies = cross_val_score(estimator = clf_ar, X = X_train, y = y_train, cv = 5,verbose = 1)\ny_pred = clf_ar.predict(X_test)\nprint('')\nprint('###### AdaBoost Regression ######')\nprint('Score : %.4f' % clf_ar.score(X_test, y_test))\nprint(accuracies)\n\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred)**0.5\nr2 = r2_score(y_test, y_pred)\n\nprint('')\nprint('MSE    : %0.2f ' % mse)\nprint('MAE    : %0.2f ' % mae)\nprint('RMSE   : %0.2f ' % rmse)\nprint('R2     : %0.2f ' % r2)\n\nR2_Scores.append(r2)","961a8a89":"clf_rr = Ridge(normalize=True)\nclf_rr.fit(X_train , y_train)\naccuracies = cross_val_score(estimator = clf_rr, X = X_train, y = y_train, cv = 5,verbose = 1)\ny_pred = clf_rr.predict(X_test)\nprint('')\nprint('###### Ridge Regression ######')\nprint('Score : %.4f' % clf_rr.score(X_test, y_test))\nprint(accuracies)\n\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred)**0.5\nr2 = r2_score(y_test, y_pred)\n\nprint('')\nprint('MSE    : %0.2f ' % mse)\nprint('MAE    : %0.2f ' % mae)\nprint('RMSE   : %0.2f ' % rmse)\nprint('R2     : %0.2f ' % r2)\n\nR2_Scores.append(r2)","f2c976fd":"clf_gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls',verbose = 1)\nclf_gbr.fit(X_train , y_train)\naccuracies = cross_val_score(estimator = clf_gbr, X = X_train, y = y_train, cv = 5,verbose = 1)\ny_pred = clf_gbr.predict(X_test)\nprint('')\nprint('###### Gradient Boosting Regression #######')\nprint('Score : %.4f' % clf_gbr.score(X_test, y_test))\nprint(accuracies)\n\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred)**0.5\nr2 = r2_score(y_test, y_pred)\n\nprint('')\nprint('MSE    : %0.2f ' % mse)\nprint('MAE    : %0.2f ' % mae)\nprint('RMSE   : %0.2f ' % rmse)\nprint('R2     : %0.2f ' % r2)\n\nR2_Scores.append(r2)","138ebab1":"clf_rf = RandomForestRegressor()\nclf_rf.fit(X_train , y_train)\naccuracies = cross_val_score(estimator = clf_rf, X = X_train, y = y_train, cv = 5,verbose = 1)\ny_pred = clf_rf.predict(X_test)\nprint('')\nprint('###### Random Forest ######')\nprint('Score : %.4f' % clf_rf.score(X_test, y_test))\nprint(accuracies)\n\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred)**0.5\nr2 = r2_score(y_test, y_pred)\n\nprint('')\nprint('MSE    : %0.2f ' % mse)\nprint('MAE    : %0.2f ' % mae)\nprint('RMSE   : %0.2f ' % rmse)\nprint('R2     : %0.2f ' % r2)","b04fc35b":"no_of_test=[100]\nparams_dict={'n_estimators':no_of_test,'n_jobs':[-1],'max_features':[\"auto\",'sqrt','log2']}\nclf_rf=GridSearchCV(estimator=RandomForestRegressor(),param_grid=params_dict,scoring='r2')\nclf_rf.fit(X_train,y_train)\nprint('Score : %.4f' % clf_rf.score(X_test, y_test))\npred=clf_rf.predict(X_test)\nr2 = r2_score(y_test, pred)\nprint('R2     : %0.2f ' % r2)\nR2_Scores.append(r2)","f3af87e9":"clf_knn = KNeighborsRegressor()\nclf_knn.fit(X_train , y_train)\naccuracies = cross_val_score(estimator = clf_knn, X = X_train, y = y_train, cv = 5,verbose = 1)\ny_pred = clf_knn.predict(X_test)\nprint('')\nprint('###### KNeighbours Regression ######')\nprint('Score : %.4f' % clf_knn.score(X_test, y_test))\nprint(accuracies)\n\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred)**0.5\nr2 = r2_score(y_test, y_pred)\n\nprint('')\nprint('MSE    : %0.2f ' % mse)\nprint('MAE    : %0.2f ' % mae)\nprint('RMSE   : %0.2f ' % rmse)\nprint('R2     : %0.2f ' % r2)","c52e7f6b":"n_neighbors=[]\nfor i in range (0,50,5):\n    if(i!=0):\n        n_neighbors.append(i)\nparams_dict={'n_neighbors':n_neighbors,'n_jobs':[-1]}\nclf_knn=GridSearchCV(estimator=KNeighborsRegressor(),param_grid=params_dict,scoring='r2')\nclf_knn.fit(X_train,y_train)\nprint('Score : %.4f' % clf_knn.score(X_test, y_test))\npred=clf_knn.predict(X_test)\nr2 = r2_score(y_test, pred)\nprint('R2     : %0.2f ' % r2)\nR2_Scores.append(r2)","c9f604d8":"compare = pd.DataFrame({'Algorithms' : models , 'R2-Scores' : R2_Scores})\ncompare.sort_values(by='R2-Scores' ,ascending=False)","4b96e56a":"sns.barplot(x='R2-Scores' , y='Algorithms' , data=compare)","00eaa105":"sns.factorplot(x='Algorithms', y='R2-Scores' , data=compare, size=6 , aspect=4)","1c78f5cd":"In this kernel I have tried to find out first which feature is catergorical and which feature is numerical. Then in categorical , using EDA I have divided it into ordinal and nominal. For ordinal I have mapped it seperately and for nominal I have used one hot encoding.\n\nMoving forward checking the skewness of the dependent variable, I have used log of the dependent variable. Taking the log reduces the skewness.\n\nFruther I have introduced a new feature called volume, using feature x,y,z and removed this 3 features. This new feature introduced has helped to reach better accuracy.\n\nFinally I have fed the data into model, and getting a accuracy of around 98%."}}