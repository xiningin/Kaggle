{"cell_type":{"869b3625":"code","55f0f57e":"code","03ef577c":"code","331e0636":"code","67118e4f":"code","ce254542":"code","2fd26018":"code","ea8a8bc6":"code","ddb28465":"code","00051ebd":"code","ad5e2713":"code","e026f3a4":"code","c498dcfa":"code","b9276ff9":"code","c0fd55ac":"code","8226b14d":"code","c18d9025":"code","d1eb8aea":"code","3c034352":"code","78e15715":"code","ad359313":"code","a9f11ad8":"code","93fff4f0":"code","57484c6d":"code","a629f723":"markdown","94a6563b":"markdown","628d58cc":"markdown","16377c5c":"markdown","2bb8e9a0":"markdown","e8baa143":"markdown","676ab5a6":"markdown"},"source":{"869b3625":"import pandas as pd \nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","55f0f57e":"path = '..\/input\/a-large-scale-fish-dataset\/Fish_Dataset\/Fish_Dataset'","03ef577c":"filepath = os.listdir(path)\nprint(filepath)","331e0636":"categories = ['Hourse Mackerel', 'Black Sea Sprat', 'Sea Bass', 'Red Mullet', 'Trout', 'Striped Red Mullet', 'Shrimp', 'Gilt-Head Bream', 'Red Sea Bream']","67118e4f":"image_path = []\nfish_category =  []\nfor i in categories:\n    for j in os.listdir(os.path.join(path+'\/'+i+'\/'+i)):\n        image_path.append(os.path.join(path+'\/'+i+'\/'+i+'\/'+j))\n        fish_category.append(i)\n    ","ce254542":"df = {'image_path':image_path,'category':fish_category}\ndf = pd.DataFrame(df)","2fd26018":"df.to_csv('train.csv')","ea8a8bc6":"import torch \nfrom torchvision import transforms as T\nfrom torchvision import models\nimport PIL \nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader","ddb28465":"class fishDataset(Dataset):\n    def __init__(self,dataframe,transform):\n        self.dataframe = dataframe\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.dataframe.image_path)\n    \n    def __getitem__(self,idx):\n        image = PIL.Image.open(self.dataframe.image_path[idx])\n        image = transform(image)\n        target = self.dataframe.category_number[idx]\n        return image,target","00051ebd":"train_transform = transform = T.Compose([T.Resize(256),T.CenterCrop(224),T.ToTensor(),T.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])])\nvalid_transform = transform = T.Compose([T.Resize(256),T.CenterCrop(224),T.ToTensor(),T.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])])","ad5e2713":"df_train = pd.read_csv('.\/train.csv')\ndf_train= df_train.sample(frac=1).reset_index(drop=True)","e026f3a4":"df_train.category.value_counts()","c498dcfa":"category_map={}\nfor i,j in enumerate(list(set(df_train.category))):\n    category_map[j]=i","b9276ff9":"df_train['category_number'] = df_train['category'].map(category_map)","c0fd55ac":"from sklearn.model_selection import train_test_split\ndf_train,df_valid = train_test_split(df_train,test_size=0.2,random_state=42)","8226b14d":"df_train.reset_index(drop=True,inplace=True)\ndf_valid.reset_index(drop=True,inplace=True)","c18d9025":"trainDataset = fishDataset(df_train,transform=train_transform)\nvalidDataset = fishDataset(df_valid,transform=valid_transform)","d1eb8aea":"image,label = next(iter(trainDataset))\nplt.imshow(np.transpose(np.array(image),(1,2,0)))\nplt.title(label)","3c034352":"trainLoader = DataLoader(trainDataset,batch_size=64,shuffle=True)\nvalidLoader = DataLoader(validDataset,batch_size=64,shuffle=False)","78e15715":"model = models.resnet18(pretrained=True)\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features,9)\nfor param in model.fc.parameters():\n    param.requires_grad = True\nprint(model)","ad359313":"if torch.cuda.is_available():\n    device=\"cuda\"\nelse:\n    device=\"cpu\"\nprint(device)","a9f11ad8":"optimizer = torch.optim.Adam(model.fc.parameters(),lr= 0.001)\ncriterion = nn.CrossEntropyLoss()","93fff4f0":"model.to(device)","57484c6d":"n_epochs = 20\nvalid_loss_min = np.Inf\nfor epoch in tqdm(range(1,n_epochs+1)):\n    train_loss = 0.0\n    valid_loss = 0.0\n    train_running_correct= 0.0\n    val_running_correct = 0.0\n    \n    model.train()\n    \n    for data,target in trainLoader:\n        data,target = data.to(device),target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output,target)\n        loss.backward()\n        optimizer.step()\n        train_loss+=loss.item()*data.size(0)\n        _,preds = torch.max(output.data,1)\n        train_running_correct += (preds==target).sum().item()\n    \n    model.eval()\n    \n    for data,target in validLoader:\n        data,target = data.to(device),target.to(device)\n        output = model(data)\n        loss = criterion(output,target)\n        valid_loss+=loss.item()*data.size(0)\n        _,preds =torch.max(output.data,1)\n        val_running_correct += (preds==target).sum().item()\n        \n    train_loss = train_loss\/len(trainLoader.sampler)\n    valid_loss = valid_loss\/len(validLoader.sampler)\n    train_accuracy = 100. * train_running_correct\/len(trainLoader.sampler)\n    valid_accuracy = 100. * val_running_correct\/len(validLoader.sampler)\n    print('Epoch: {} , Training Loss: {:.6f},Training Accuracy: {:.3f} ,Validation Loss: {:.6f}, Validation Accuracy: {:.3f}'.format(epoch,train_loss,train_accuracy,valid_loss,valid_accuracy))\n    \n    if valid_loss<=valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), 'model_fish.pt')\n        valid_loss_min = valid_loss\n\n        ","a629f723":"## Getting Path","94a6563b":"## Creating a image database","628d58cc":"## Building the fishDatasetClass","16377c5c":"## Training and Validation","2bb8e9a0":"## Importing pretrained model","e8baa143":"## Importing the deeplearning libraries","676ab5a6":"## Importing the libraries"}}