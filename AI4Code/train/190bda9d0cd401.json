{"cell_type":{"56b8fb43":"code","365b2ea2":"code","9241a989":"code","7fddc719":"code","01147ef8":"code","0aa841ef":"code","335c2547":"code","fec600bc":"code","ac919bbd":"code","622e3c64":"code","68cd097a":"code","5e79b221":"code","ef5a103d":"code","651a1e43":"code","3bd3c230":"code","2aeed8c1":"code","b2b5e58e":"code","642db26e":"code","6ee246d7":"code","ce7c8c74":"code","17ed9693":"code","537eaeb8":"code","81cc9542":"code","d1ca4e03":"code","58ebce3b":"code","d4a5bbdc":"code","94426e93":"code","6c405e10":"code","758128f8":"code","b75dc5e0":"code","8ef5b434":"code","2fd0325d":"code","bac6ccd1":"code","fba83c9f":"code","c80325de":"code","84b67b52":"code","ad6cd9c5":"code","a886cfa3":"code","d610b64c":"code","7b361267":"code","1545b16c":"code","ee93c2db":"code","6ef0476b":"code","e683b703":"code","fbf046fd":"markdown","ae271a96":"markdown","80adfc6b":"markdown","361d5a40":"markdown","110a1694":"markdown","fb068679":"markdown","170a217c":"markdown","28901b88":"markdown","283f2432":"markdown","015aaa5c":"markdown","6569b315":"markdown","97320aad":"markdown","3b04bf7a":"markdown","6c7d45b1":"markdown","6ebd9971":"markdown","cdd2ee41":"markdown","51eb576e":"markdown","da394414":"markdown","6d9cfad1":"markdown","b840aadc":"markdown","e5b86351":"markdown","644cd98e":"markdown","4330e48d":"markdown","edc61e3b":"markdown","f78ddd7e":"markdown","755c25bd":"markdown","d8b6648f":"markdown","e9c9eabc":"markdown","184701e4":"markdown","c3fd8baf":"markdown","8c394013":"markdown","4de0da72":"markdown","39f18f7a":"markdown","93aa4246":"markdown","d8319f02":"markdown","6e0ab67d":"markdown","1cce895c":"markdown","30bd1ede":"markdown","66581d5b":"markdown","f3809c26":"markdown","c1d61180":"markdown","71b20db4":"markdown","782c8975":"markdown","74c0e7e3":"markdown"},"source":{"56b8fb43":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","365b2ea2":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ngames = pd.read_csv(\"..\/input\/league-of-legends-diamond-ranked-games-10-min\/high_diamond_ranked_10min.csv\")\ngames.head()","9241a989":"games.isnull().sum()","7fddc719":"games[games==np.inf]=np.nan\ngames.isnull().sum()","01147ef8":"games.dtypes","0aa841ef":"games_blue=games.iloc[:,1:21] #drop ID and red team feature\ngames_blue.head()","335c2547":"f, ax=plt.subplots(figsize=(18,8))\nsns.heatmap(games_blue.corr(), annot=True, linewidth=0.5, fmt='.1f', ax=ax,cmap=\"YlGnBu\")","fec600bc":"#adding \"relative\" features\ngames_df = games_blue\nblueVision = games['blueWardsPlaced']-games['redWardsDestroyed']\nredVision = games['redWardsPlaced']-games['blueWardsDestroyed']\nblueKdRatio = games_df['blueKills']\/games_df['blueDeaths']\nredKdRatio = games['redKills']\/games['redDeaths']\ngames_df['blueRedKdDiff']= blueKdRatio - redKdRatio\ngames_df['blueVisionDiff']= blueVision - redVision","ac919bbd":"x=games_df.drop(['blueWins','blueWardsPlaced','blueWardsDestroyed','blueKills','blueDeaths','blueGoldPerMin',\n                'blueTotalExperience','blueTotalGold'],axis=1)\ny=games_df.blueWins","622e3c64":"#Move absolute of neagative values to a new features\nx['rGD_n']=0\nx['rGD_p']=0\nfor i in range(9879):\n    if(x.loc[i,'blueGoldDiff']<0):\n        x.loc[i,'rGD_n']=abs(x.loc[i,'blueGoldDiff'])\n    elif(x.loc[i,'blueGoldDiff']>=0):\n            x.loc[i,'rGD_p']=x.loc[i,'blueGoldDiff']\n            \n\nx['blueKdDiff_p']=0\nx['blueKdDiff_n']=0\n\nfor i in range(9879):\n    if(x.loc[i,'blueRedKdDiff']<0):\n        x.loc[i,'blueKdDiff_n']=abs(x.loc[i,'blueRedKdDiff'])\n    elif(x.loc[i,'blueRedKdDiff']>=0):\n            x.loc[i,'blueKdDiff_p']=x.loc[i,'blueRedKdDiff']\n\nx['blueVD_n']=0\nx['blueVD_p']=0\n\nfor i in range(9879):\n    if(x.loc[i,'blueVisionDiff']<0):\n        x.loc[i,'blueVD_n']=abs(x.loc[i,'blueVisionDiff'])\n    elif(x.loc[i,'blueVisionDiff']>=0):\n            x.loc[i,'blueVD_p']=x.loc[i,'blueVisionDiff']\n\nx=x.drop(['blueGoldDiff','blueExperienceDiff','blueRedKdDiff','blueVisionDiff'], axis=1)\n\n#check data\nx.head()","68cd097a":"#check infinity value\nx[x==np.inf]=np.nan\nx.isnull().sum()","5e79b221":"#fill nan with max value in there column\nx.fillna(x.max(axis=0), inplace=True)\nx.isnull().sum()","ef5a103d":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test=train_test_split(x,y,test_size = 0.2, random_state=1)","651a1e43":"#for train\/test split\nx_train = (x_train - x_train.min(axis=0)) \/ (x_train.max(axis=0) - x_train.min(axis=0))\nx_test = (x_test - x_test.min(axis=0)) \/ (x_test.max(axis=0) - x_test.min(axis=0))\n#for cross fold validation\nX = (x - x.min(axis=0)) \/ (x.max(axis=0) - x.min(axis=0))","3bd3c230":"#used to store accuracies of each algorithms\nacc={}","2aeed8c1":"#naive bayes\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(x_train, y_train)\nacc_gnb1=gnb.score(x_test,y_test)*100\nacc['GaussianNB_brfore_tuning'] = acc_gnb1\nprint('Accuracy of GNB:{:.2f}%'.format(acc_gnb1))","b2b5e58e":"from sklearn.model_selection import GridSearchCV\n\nnp.random.seed(999)\n\nnb_classifier = GaussianNB()\n\nparams_NB = {'var_smoothing': np.logspace(0,-9, num=150)}\n\ngs_NB = GridSearchCV(estimator=nb_classifier, \n                     param_grid=params_NB, \n                     cv=3,\n                     verbose=1, \n                     scoring='accuracy')\n\ngs_NB.fit(X, y);","642db26e":"gs_NB.best_params_","6ee246d7":"gs_NB.best_score_","ce7c8c74":"results_NB = pd.DataFrame(gs_NB.cv_results_['params'])\nresults_NB['test_score'] = gs_NB.cv_results_['mean_test_score']\n\nplt.plot(results_NB['var_smoothing'], results_NB['test_score'], marker = '.')    \nplt.xlabel('Var. Smoothing')\nplt.ylabel(\"Mean CV Score\")\nplt.title(\"NB Performance Comparison\")\nplt.show()","17ed9693":"gnb2 = GaussianNB(var_smoothing = 0.003338027673990301)\ngnb2.fit(x_train, y_train)\nacc_gnb2=gnb2.score(x_test,y_test)*100\nacc['GaussianNB_after_tuning'] = acc_gnb2\nprint('Accuracy of GNB after tuning:{:.2f}%'.format(acc_gnb2))","537eaeb8":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1, p=2)\n\nknn.fit(x_train, y_train)\nacc_knn1 = knn.score(x_test, y_test)*100\nacc['KNN_before_tuning'] = acc_knn1\nprint('Accuracy of knn:{:.2f}%'.format(acc_knn1))","81cc9542":"params_KNN = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7], \n              'p': [1, 2, 5]}\n\nfrom sklearn.model_selection import GridSearchCV\n\ngs_KNN = GridSearchCV(estimator=KNeighborsClassifier(), \n                      param_grid=params_KNN, \n                      cv=3,\n                      verbose=1,  # verbose: the higher, the more messages\n                      scoring='accuracy', \n                      return_train_score=True)","d1ca4e03":"gs_KNN.fit(X,y)","58ebce3b":"gs_KNN.best_params_","d4a5bbdc":"gs_KNN.best_score_","94426e93":"gs_KNN.cv_results_['mean_test_score']","6c405e10":"results_KNN = pd.DataFrame(gs_KNN.cv_results_['params'])\nresults_KNN['test_score'] = gs_KNN.cv_results_['mean_test_score']","758128f8":"results_KNN['metric'] = results_KNN['p'].replace([1,2,5], [\"Manhattan\", \"Euclidean\", \"Minkowski\"])\nresults_KNN","b75dc5e0":"%config InlineBackend.figure_format = 'retina'\nplt.style.use(\"ggplot\")\n\nfor i in [\"Manhattan\", \"Euclidean\", \"Minkowski\"]:\n    temp = results_KNN[results_KNN['metric'] == i]\n    plt.plot(temp['n_neighbors'], temp['test_score'], marker = '.', label = i)\n    \nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel(\"Mean CV Score\")\nplt.title(\"KNN Performance Comparison\")\nplt.show()","8ef5b434":"from sklearn.neighbors import KNeighborsClassifier\nknn2 = KNeighborsClassifier(n_neighbors=7, p=1)\n\nknn2.fit(x_train, y_train)\nacc_knn2 = knn2.score(x_test, y_test)*100\nacc['KNN_after_tuning'] = acc_knn2\nprint('Accuracy of knn after tuning:{:.2f}%'.format(acc_knn2))","2fd0325d":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nacc_dt1 = dt.score(x_test,y_test)*100\nacc['Decision_tree_before_tuning'] = acc_dt1\nprint('Accuracy of Decision tree before tuning:{:.2f}%'.format(acc_dt1))","bac6ccd1":"df_classifier = DecisionTreeClassifier(random_state=999)\n\nparams_DT = {'criterion': ['gini', 'entropy'],\n             'max_depth': [1, 2, 3, 4, 5, 6, 7, 8],\n             'min_samples_split': [2, 3]}\n\ngs_DT = GridSearchCV(estimator=df_classifier, \n                     param_grid=params_DT, \n                     cv=3,\n                     verbose=1, \n                     scoring='accuracy')\n\ngs_DT.fit(X, y);","fba83c9f":"gs_DT.best_params_","c80325de":"gs_DT.best_score_","84b67b52":"results_DT = pd.DataFrame(gs_DT.cv_results_['params'])\nresults_DT['test_score'] = gs_DT.cv_results_['mean_test_score']\nresults_DT.columns","ad6cd9c5":"for i in ['gini', 'entropy']:\n    temp = results_DT[results_DT['criterion'] == i]\n    temp_average = temp.groupby('max_depth').agg({'test_score': 'mean'})\n    plt.plot(temp_average, marker = '.', label = i)\n    \n    \nplt.legend()\nplt.xlabel('Max Depth')\nplt.ylabel(\"Mean CV Score\")\nplt.title(\"DT Performance Comparison\")\nplt.show()","a886cfa3":"from sklearn.tree import DecisionTreeClassifier\ndt2 = DecisionTreeClassifier(criterion = 'gini', max_depth=4, min_samples_split =2)\ndt2.fit(x_train,y_train)\nacc_dt2 = dt2.score(x_test,y_test)*100\nacc['Decision_tree_after_tuning'] = acc_dt2\nprint('Accuracy of Decision tree after tuning:{:.2f}%'.format(acc_dt2))","d610b64c":"acc_df = pd.DataFrame(acc.items(), columns=['Algorithm', 'acc_score'])\nacc_df.head()","7b361267":"plt.figure(figsize=(18,8))\nax = sns.barplot(x='Algorithm', y='acc_score', data = acc_df)","1545b16c":"y_NB = gnb2.predict(x_test)\ny_KNN = knn2.predict(x_test)\ny_DT = dt2.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm_nb = confusion_matrix(y_test,y_NB)\ncm_knn = confusion_matrix(y_test,y_KNN)\ncm_dt = confusion_matrix(y_test,y_DT)","ee93c2db":"sns.heatmap(cm_nb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})","6ef0476b":"sns.heatmap(cm_knn,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})","e683b703":"sns.heatmap(cm_dt,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})","fbf046fd":"<a id=052><\/a>\n## 5.2 Visualise the comparison of different parameters","ae271a96":"<a id=08><\/a>\n# 8. Comparison of different algorithm","80adfc6b":"#### Confusion matrix of gaussianNB","361d5a40":"<a id=053><\/a>\n## 5.3 Gaussian Naive bayes with hyperparameter","110a1694":"## Infinite value in x","fb068679":"<a id=05><\/a>\n# 5. Gaussian Naive Bayes","170a217c":"<a id=06><\/a>\n# 6. KNN","28901b88":"<a id=04><\/a>\n# 4. Machine learning preprocessing","283f2432":"## Train\/Test split","015aaa5c":"## Missing value","6569b315":"# League of Legends game result prediction\n## Goal:predicting result with data of first 10 minutes game.","97320aad":"(This is not a tutorial)","3b04bf7a":"<a id=064><\/a>\n## 6.4 KNN with hyperparameters","6c7d45b1":"## x and y","6ebd9971":"## Handle negative values","cdd2ee41":"<a id=063><\/a>\n## 6.3 Visualise the comparison of different parameters","51eb576e":"## Correlation between blue team features","da394414":"1. [Data Checking](#01)\n2. [Deeper look at blue team features](#02)\n3. [Feature Engineering](#03)\n4. [Machine learning preprocessing](#04)\n5. [Gaussian Naive Bayes](#05)\n > [5.1 Optimize Gaussian Naive Bayes](#051)\n\n > [5.2 Visualise the comparison of different parameters](#052)\n \n > [5.3 Gaussian Naive bayes with hyperparameter](#053)\n\n6. [KNN](#06)\n > [6.1 Optimize KNN](#061)\n\n > [6.2 Score of different parameters of KNN](#062)\n\n > [6.3 Visualise the comparison of different parameters](#063)\n \n > [6.4 KNN with hyperparameters](#064)\n\n7. [Decision Tree](#07)\n >[7.1 Optimize Decision Tree](#071)\n \n >[7.2 Visualise the comparison of different parameters](#072)\n \n >[7.3 Decision Tree with hyperparameters](#073)\n \n8. [Comparison of different algorithm](#08)\n9. [Confusion matrices of GaussianNB, KNN, Decision Tree with hyperparameters](#09)\n10. [Conclusion](#10)","6d9cfad1":"<a id=01><\/a>\n# 1. Data Checking","b840aadc":"For win or loose is a relative issue, simply using the data of one team doesn't make sense. ","e5b86351":"<a id=051><\/a>\n## 5.1 Optimize Gaussian Naive Bayes","644cd98e":"<a id=03><\/a>\n# 3. Feature Engineering","4330e48d":"<a id=073><\/a>\n## 7.3 Decision Tree with hyperparameters","edc61e3b":"## Infinite value","f78ddd7e":"<a id=062><\/a>\n## 6.2 Score of different parameters of KNN","755c25bd":"<a id=071><\/a>\n## 7.1 Optimize Decision Tree","d8b6648f":"<a id=09><\/a>\n# 9. Confusion matrices of GaussianNB, KNN, Decision Tree with hyperparameters","e9c9eabc":"With optimized Gaussian Naive bayes we have almost 74% test accuracy, which is somewhat good result, for this is only first 10 minutes of the game(the average game time is around 35 minutes).\n* How to improve the test accuracy?\n 1. Maybe use other models to train this data, such as SVM, Random forest, or unsupervised algorithms.\n 2. Select different features.","184701e4":"1. Gaussian Naive Bayes\n2. KNN\n3. Decision Tree","c3fd8baf":"<a id=10><\/a>\n# 10. Conclusion","8c394013":"#### Confusion matrix of Decision Tree","4de0da72":"Since negative values can affect the result of machine learning after standardize, we have to handle them.","39f18f7a":"<a id=07><\/a>\n# 7. Decision Tree","93aa4246":"## Data types","d8319f02":"## Classification Algorithms","6e0ab67d":"variables explanation:\n* blue(red)Vision: Sight that blue(red) actually have.\n* blue(red)KdRatio: blue(red)Kills\/blue(red)Kills","1cce895c":"#### Confusion matrix of KNN","30bd1ede":"## Standardize features","66581d5b":"I'm new to data science, still need to learn a lot.\nDon't be hesitate to share your advice or thought on this kernel, any opinion is a treasure.\nThank you!!","f3809c26":"<a id=061><\/a>\n## 6.1 Optimize KNN","c1d61180":"<a id=072><\/a>\n## 7.2 Visualise the comparison of different parameters","71b20db4":"Great! no missing value and infinite value.","782c8975":"# Import Libraries and Data","74c0e7e3":"<a id=02><\/a>\n# 2. Deeper look at blue team features"}}