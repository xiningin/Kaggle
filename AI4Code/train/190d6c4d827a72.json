{"cell_type":{"a76db6aa":"code","659f20fa":"code","848b46d7":"code","8de104a9":"code","262c4efa":"code","28ada64b":"code","fb4e9c2e":"code","f09d075a":"code","c35dca50":"code","080375a3":"code","12ae1495":"code","ca4ad4a1":"markdown","d06e97e0":"markdown","a3b6fc6c":"markdown","b3c6ee50":"markdown","8c2dfeab":"markdown","aa33e5d7":"markdown","49a0ac1a":"markdown","83dd58e8":"markdown","8074c266":"markdown","2183b35a":"markdown","6d27362c":"markdown","ef59f2b5":"markdown","20f7ad2e":"markdown","1a53b860":"markdown","e59a9fa6":"markdown"},"source":{"a76db6aa":"import math\nfrom scipy.io import arff\nfrom scipy.stats.stats import pearsonr\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\n\n# Formata\u00e7\u00e3o mais bonita para os notebooks\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (15,5)\n","659f20fa":"df_desharnais = pd.read_csv('..\/input\/02.desharnais.csv',  header=0)\ndf_desharnais.head()","848b46d7":"df_desharnais.info()","8de104a9":"df_desharnais.describe()","262c4efa":"df_desharnais.corr()","28ada64b":"colormap = plt.cm.viridis\nplt.figure(figsize=(10,10))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.set(font_scale=1.05)\nsns.heatmap(df_desharnais.drop(['id'], axis=1).astype(float).corr(),linewidths=0.1,vmax=1.0, square=True,cmap=colormap, linecolor='white', annot=True)","fb4e9c2e":"features = [ 'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n        'PointsNonAdjust', 'Adjustment', 'PointsAjust']\n\nmax_corr_features = ['Length', 'Transactions', 'Entities','PointsNonAdjust','PointsAjust']\n\nX = df_desharnais[max_corr_features]\ny = df_desharnais['Effort']","f09d075a":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=30)\n\nneigh = KNeighborsRegressor(n_neighbors=3, weights='uniform')\nneigh.fit(X_train, y_train) \nprint(neigh.score(X_test, y_test))","c35dca50":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=22)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))","080375a3":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=22)\n\nparameters = {'kernel':('linear', 'rbf'), 'C':[1,2,3,4,5,6,7,8,9,10], 'gamma':('auto', 'scale')}\n\nsvr = SVR()\nLinearSVC = GridSearchCV(svr, parameters, cv=3)\nLinearSVC.fit(X_train, y_train)\nprint(\"Best params hash: {}\".format(LinearSVC.best_params_))\nprint(LinearSVC.score(X_test, y_test))","12ae1495":"\n\nfor i, feature in enumerate(max_corr_features):\n    plt.figure(figsize=(18,6))\n\n    # Knn Regression Model \n    xs, ys = zip(*sorted(zip(X_test[feature], neigh.fit(X_train, y_train).predict(X_test))))\n    \n    # Linear Regression Model \n    model_xs, model_ys = zip(*sorted(zip(X_test[feature], model.fit(X_train, y_train).predict(X_test))))\n    \n    # Support Vector Machine\n    svc_model_xs, svc_model_ys = zip(*sorted(zip(X_test[feature], LinearSVC.fit(X_train, y_train).predict(X_test))))\n\n    plt.scatter(X_test[feature], y_test, label='Real data', lw=2,alpha= 0.7, c='k' )\n    plt.plot(model_xs, model_ys , lw=2, label='Linear Regression Model', c='cornflowerblue')\n    plt.plot(xs, ys , lw=2,label='K Nearest Neighbors (k=3)', c='yellowgreen')\n    plt.plot(svc_model_xs, svc_model_ys , lw=2,label='Support Vector Machine (Kernel=Linear)', c='gold')\n    \n    plt.xlabel(feature)\n    plt.ylabel('Effort')\n    plt.legend()\n    plt.show()","ca4ad4a1":"## 2) Linear Regression\n","d06e97e0":"## 1) Knn Regression","a3b6fc6c":"### This is a PROMISE Software Engineering Repository data set made publicly available in order to encourage repeatable, verifiable, refutable, and\/or improvable predictive models of software engineering.\n\nIf you publish material based on PROMISE data sets then, please\nfollow the acknowledgment guidelines posted on the PROMISE repository\nweb page http:\/\/promise.site.uottawa.ca\/SERepository .","b3c6ee50":"## 3) Support Vector Machine\n","8c2dfeab":"## Split  train\/test data","aa33e5d7":"_Making decisions with a highly uncertain level is a critical problem\nin the area of software engineering. Predicting software quality requires high\naccurate tools and high-level experience. Otherwise, AI-based predictive models\ncould be a useful tool with an accurate degree that helps on the prediction\nof software effort based on historical data from software development metrics.\nIn this study, we built a software effort estimation model to predict this effort\nusing a linear regression model. This statistical model was developed using a\nnon-parametric linear regression algorithm based on the K-Nearest Neighbours\n(KNN). So, our results show the possibility of using AI methods to predict the\nsoftware engineering effort prediction problem with an coefficient of determination\nof 76%_","49a0ac1a":"In this section, the correlations between attributes of Desharnais dataset and software effort are analyzed and applicability of the regression analysis is examined. The correlation between two variables is a measure of how well the variables are related. The most common measure of correlation in statistics is the Pearson Correlation (or the Pearson Product Moment Correlation - PPMC) which shows the linear relationship between two variables. \n\nPearson correlation coefficient analysis produces a result between `-1` and `1`. A result of `-1` means that there is a perfect negative correlation between the two values at all, while a result of `1` means that there is a perfect positive correlation between the two variables. \n\nResults between `0.5` and `1.0` indicate high correlation.Correlation coefficients are used in statistics to measure how strong a relationship is between two variables. There are several types of correlation coefficient. `Pearson\u2019s correlation` (also called Pearson\u2019s R) is a correlation coefficient commonly used in linear regression.","83dd58e8":"## Applying Pearson\u2019s correlation","8074c266":"## _Abstract_","2183b35a":"The K-Nearest Neighbor Regression is a simple algorithm that stores all available cases and predict the numerical target based on a similarity measure and it\u2019s been used in a statistical estimation and pattern recognition as non-parametric technique classifying correctly unknown cases calculating euclidean distance between data points. In fact our choice by K-Nearest Neighbor Regression was motivated by the absence of a detailed explanation about how effort attribute value is calculated on Desharnais dataset. In the K-Nearest Neighbor Regression we choose to specify only 3 neighbors for k-neighbors queries and uniform weights, that means all points in each neighborhood are weighted equally.","6d27362c":"In this study the following algorithms were used: Linear Regression and K-Nearest Neighbors Regression. The training of the regressors models were performed on 67% of the instances\n\n","ef59f2b5":"##  Results","20f7ad2e":"## Models Costruction\u00b6\n","1a53b860":"The regression analysis aims to verify the existence of a functional relationship between a variable with one or more variables, obtaining an equation that explains the variation of the dependent variable Y, by the variation of the levels of the independent variables. The training of the Linear Regression model consists of generating a regression for the target variable Y.","e59a9fa6":" The figure shows the linear model (blue line) prediction is fairly close to Knn model effort prediction (red line), predicting the numerical target based on a similarity measure.\n    According to the plot we observe that Linear Regression model (blue line) presents a better performance. Although Knn Regression model (red line) is fairly close to data points, the Linear Regression model shows a smaller mean squared error.  It is possible to observe that the lines of both models present a slight tendency to rise, which justifies their correlation with the increase in effort. Some metrics are also highlighted by the presence of outliers."}}