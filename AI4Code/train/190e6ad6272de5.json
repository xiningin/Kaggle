{"cell_type":{"4b4832c7":"code","cb654565":"code","d1b2742e":"code","5febd403":"code","92952b16":"code","91401e2d":"code","e549f7dd":"code","ef0423fc":"code","8c39abbd":"code","3dba2365":"code","1e0bcdab":"code","a4a7c876":"code","1138f417":"code","783d404c":"code","49c77c0a":"code","b096a03f":"code","91bbeb70":"code","5111b2ed":"code","3b3d58a0":"code","0ef1daee":"code","ede4f2fa":"code","8055a3c3":"code","041bc755":"code","6bb7260a":"code","e46a8944":"code","56b91ab1":"code","bfb89830":"code","fbf21f1c":"code","171be9cf":"code","42e7834c":"code","88e30758":"code","1058d685":"code","f714e7a1":"code","d53c2453":"code","eb29ef46":"markdown","0f064a9c":"markdown","a070e1a8":"markdown","9796a0be":"markdown","471959d4":"markdown","3351b986":"markdown","dbec62da":"markdown","4fe03296":"markdown","9061ecbf":"markdown","b59df71f":"markdown","fbfb3812":"markdown","06f8d913":"markdown","2b1ac0ef":"markdown","bbeb9d0c":"markdown","49bc15e5":"markdown","551c83d4":"markdown","8dc72d57":"markdown","44bf5934":"markdown","ef8ecd83":"markdown","1a37a0dd":"markdown","610e92e1":"markdown","c23cf364":"markdown"},"source":{"4b4832c7":"#importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nfrom pdpbox import pdp, get_dataset, info_plots\nimport shap\n\nfrom yellowbrick.classifier import ROCAUC, PrecisionRecallCurve, ClassificationReport\nfrom yellowbrick.model_selection import LearningCurve, ValidationCurve, learning_curve\n\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","cb654565":"df=pd.read_csv('\/kaggle\/input\/bank-marketing-dataset\/bank.csv')\nprint(df.shape)","d1b2742e":"#checking for duplicated rows and missing values\nprint(df.duplicated().sum())\nprint(df.isnull().sum().sum())","5febd403":"#checking types\ndf.dtypes.sort_values()","92952b16":"df.describe()","91401e2d":"for col in df.select_dtypes(include='object').columns:\n    print(col)\n    print(df[col].unique())","e549f7dd":"df.drop(\"duration\",axis=1, inplace=True)","ef0423fc":"#checking class balance\ndf.deposit.value_counts()\/df.deposit.count()","8c39abbd":"#I'm going to use StratifiedShuffleSplit to preserve the class proportions.\nsss=StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1)\nfor train_index, test_index in sss.split(df.drop(\"deposit\",axis=1), df.deposit):\n    traindf=df.loc[train_index] #to select only rows (with all columns) we dont need comma and colon.\n    testdf= df.loc[test_index]","3dba2365":"# setting my own palette\nmypalette = ['seagreen', 'indianred']\nsns.set_palette(mypalette)\nsns.palplot(sns.color_palette())","1e0bcdab":"# Scatterplots to search for linear and non-linear relationships and histograms.\nsns.pairplot(traindf, diag_kind='hist',  hue= 'deposit', height=1.5, \n             diag_kws={\"edgecolor\":\"k\", \"alpha\":0.5},\n             plot_kws={\"alpha\":0.5})\n","a4a7c876":"#Pearson\u2019s Correlations, which measures the strength of a linear relationship\nsns.heatmap(traindf.corr(method='pearson'), cmap=\"Greys\", annot=True)","1138f417":"#barplots showing the frequency of each category separated by label\nplt.figure(figsize=[12,14])\nfeatures=[\"marital\", \"education\", \"contact\", \"default\", \"housing\", \"loan\", \"poutcome\", \"month\"]\nn=1\nfor f in features:\n    plt.subplot(4,2,n)\n    sns.countplot(x=f, hue='deposit', edgecolor=\"black\", alpha=0.7, data=traindf)\n    sns.despine()\n    plt.title(\"Countplot of {}  by deposit\".format(f))\n    n=n+1\nplt.tight_layout()\nplt.show()\n\n\n    \nplt.figure(figsize=[14,4])\nsns.countplot(x='job', hue='deposit',edgecolor=\"black\", alpha=0.7, data=traindf)\nsns.despine()\nplt.title(\"Countplot of job by deposit\")\nplt.show()","783d404c":"#encoding target label\nLE=LabelEncoder()\ndf['deposit']=LE.fit_transform(df.deposit.values)\n\n#encoding categorical features\ndf=pd.get_dummies(df)","49c77c0a":"#partitioning again\nfor train_index, test_index in sss.split(df.drop(\"deposit\",axis=1), df.deposit):\n    traindf=df.loc[train_index]\n    testdf= df.loc[test_index]","b096a03f":"#partition x\/y\nxtrain=traindf.drop('deposit', axis=1)\nytrain=traindf.deposit\n\nxtest=testdf.drop('deposit', axis=1)\nytest=testdf.deposit","91bbeb70":"# pipeline combining transformers and estimator\npipe_knn= make_pipeline(StandardScaler(), KNeighborsClassifier())\n \n# grid searh to choose the best (combination of) hyperparameters\ngs_knn=GridSearchCV(estimator= pipe_knn,\n               param_grid={'kneighborsclassifier__n_neighbors':[4,5,6,7]},\n               scoring='accuracy',\n               cv=10)\n\n# nested cross validation combining grid search (inner loop) and k-fold cv (outter loop)\ngs_knn_scores = cross_val_score(gs_knn, X=xtrain, y=ytrain, cv=5,scoring='accuracy', n_jobs=-1)\n\n# fit, and fit with best estimator\ngs_knn.fit(xtrain, ytrain)\ngs_knn_best=gs_knn.best_estimator_\ngs_knn_best.fit(xtrain, ytrain)\n\nprint('Train Accuracy:   {0:.1f}%'.format(gs_knn.score(xtrain, ytrain)*100))\nprint('CV Mean Accuracy: {0:.1f}%'.format(np.mean(gs_knn_scores)*100))\nprint('Test Accuracy:    {0:.1f}%'.format(gs_knn.score(xtest, ytest)*100))","5111b2ed":"# pipeline combining transformers and estimator\npipe_svm= make_pipeline(StandardScaler(), SVC(random_state=1))\n\n# grid searh to choose the best (combination of) hyperparameters\nr=[0.1,1,10]\npg_svm=[{'svc__C':r, 'svc__kernel':['linear']},\n        {'svc__C':r, 'svc__gamma':r, 'svc__kernel':['rbf']}]\n\ngs_svm=GridSearchCV(estimator= pipe_svm,\n               param_grid= pg_svm,\n               scoring='accuracy',\n               cv=2)\n\n# nested cross validation combining grid search (inner loop) and k-fold cv (outter loop)\ngs_svm_scores = cross_val_score(gs_svm, X=xtrain, y=ytrain, cv=5,scoring='accuracy', n_jobs=-1)\n\n# fit, and fit with best estimator\ngs_svm.fit(xtrain, ytrain)\ngs_svm_best=gs_svm.best_estimator_\ngs_svm_best.fit(xtrain, ytrain)\n\nprint('Train Accuracy:   {0:.1f}%'.format(gs_svm.score(xtrain, ytrain)*100))\nprint('CV Mean Accuracy: {0:.1f}%'.format(np.mean(gs_svm_scores)*100))\nprint('Test Accuracy:    {0:.1f}%'.format(gs_svm.score(xtest, ytest)*100))","3b3d58a0":"rf= RandomForestClassifier(random_state=1)\n\n# grid searh to choose the best (combination of) hyperparameters\npg_rf={'n_estimators': [100,200,400],'max_depth': [20,40,50,60]}\n\ngs_rf=GridSearchCV(estimator= rf,\n               param_grid= pg_rf,\n               scoring='accuracy',\n               cv=2)\n\n# nested cross validation combining grid search (inner loop) and k-fold cv (outter loop)\ngs_rf_scores = cross_val_score(gs_rf, X=xtrain, y=ytrain, cv=5,scoring='accuracy', n_jobs=-1)\n\n# fit, and fit with best estimator\ngs_rf.fit(xtrain, ytrain)\ngs_rf_best=gs_rf.best_estimator_\ngs_rf_best.fit(xtrain, ytrain)\n\nprint('Train Accuracy:   {0:.1f}%'.format(gs_rf.score(xtrain, ytrain)*100))\nprint('CV Mean Accuracy: {0:.1f}%'.format(np.mean(gs_rf_scores)*100))\nprint('Test Accuracy:    {0:.1f}%'.format(gs_rf.score(xtest, ytest)*100))","0ef1daee":"# estimator\nxb= xgb.XGBClassifier(random_state=1)\n\n# grid searh to choose the best (combination of) hyperparameters\npg_xb={'n_estimators':[100,200,400], 'max_depth':[20,40,50]}\n\ngs_xb=GridSearchCV(estimator= xb,\n               param_grid= pg_xb,\n               scoring='accuracy',\n               cv=2)\n\n# nested cross validation combining grid search (inner loop) and k-fold cv (outter loop)\ngs_xb_scores = cross_val_score(gs_xb, X=xtrain, y=ytrain, cv=5,scoring='accuracy', n_jobs=-1)\n\n# fit, and fit with best estimator\ngs_xb.fit(xtrain, ytrain)\ngs_xb_best=gs_xb.best_estimator_\ngs_xb_best.fit(xtrain, ytrain)\n\nprint('Train Accuracy:   {0:.1f}%'.format(gs_xb.score(xtrain, ytrain)*100))\nprint('CV Mean Accuracy: {0:.1f}%'.format(np.mean(gs_xb_scores)*100))\nprint('Test Accuracy:    {0:.1f}%'.format(gs_xb.score(xtest, ytest)*100))","ede4f2fa":"# using random forest results: confusion_matrix and classification report\nypreds=gs_rf_best.predict(xtest)\nprint(confusion_matrix(ypreds ,ytest))\nprint(classification_report(ypreds ,ytest))","8055a3c3":"# using random forest results: confusion_matrix and classification report (yellowbrick)\n\nvisualizer_cr = ClassificationReport(gs_rf_best, classes=[\"no\", \"yes\"], support=True)\nvisualizer_cr.fit(xtrain, ytrain)\nvisualizer_cr.score(xtest, ytest)\nvisualizer_cr.show()","041bc755":"# using random forest results: precision recall curve\nvisualizer_pr = PrecisionRecallCurve(gs_rf_best)\nvisualizer_pr.fit(xtrain, ytrain)\nvisualizer_pr.score(xtest, ytest)\nvisualizer_pr.show()","6bb7260a":"# using random forest results: ROC curve\nvisualizer_roc = ROCAUC(gs_rf_best, classes=[\"no\", \"yes\"])\nvisualizer_roc.fit(xtrain, ytrain)\nvisualizer_roc.score(xtest, ytest)\nvisualizer_roc.show()","e46a8944":"# using random forest here to get feature importances\nimportances= gs_rf_best.feature_importances_\nfeature_importances= pd.Series(importances, index=xtrain.columns).sort_values(ascending=False)\nsns.barplot(x=feature_importances[0:10], y=feature_importances.index[0:10], palette=\"rocket\")\nsns.despine()\nplt.xlabel(\"Feature Importances\")\nplt.ylabel(\"Features\")\nplt.show()","56b91ab1":"# partial dependence plot of balance\npdp_data=pdp.pdp_isolate(model=gs_rf_best, dataset=xtrain, model_features=xtrain.columns, feature='balance')\npdp.pdp_plot(pdp_data, 'balance')\nplt.show()","bfb89830":"# partial dependence plot of age\npdp_data=pdp.pdp_isolate(model=gs_rf_best, dataset=xtrain, model_features=xtrain.columns, feature='age')\npdp.pdp_plot(pdp_data, 'age')\nplt.show()","fbf21f1c":"shap.initjs()","171be9cf":"#high-speed exact algorithm for tree ensemble methods\nexplainer = shap.TreeExplainer(gs_xb_best)\nshap_values = explainer.shap_values(xtrain)","42e7834c":"# first instance, feature values and its effects on prediction\nshap.force_plot(explainer.expected_value, shap_values[0,:], xtrain.iloc[0,:],matplotlib=True)","88e30758":"# all instances, feature values and its effects on prediction\n# shap.force_plot(explainer.expected_value, shap_values, xtrain)","1058d685":"shap.summary_plot(shap_values, xtrain)","f714e7a1":"shap.dependence_plot(\"balance\", shap_values, xtrain)","d53c2453":"shap.dependence_plot(\"age\", shap_values, xtrain)","eb29ef46":"I'm still trying to figure out why recall and precision seems to be inverted here. I would appreciate any help. ","0f064a9c":"# 9 - More interpretability: SHAP\n\nSHAP is a method based on coalitional game theory, also know as cooperative game theory.  \nSHAP method computes Shapley Values, basically speaking,  a measure of how much a feature value of an observation contributes to the prediction (marginally and on average, across all positive coalitions).\n\nLets make some plots based on this method:\n\n* Force plot shows feature values and its effects on prediction.    \n* The summary plot shows feature importances and feature effects. The features are ordered by their importance and the points are Shapley Values representing the **impact** of features values **on prediction**.  \n* The dependence plot shows the relationship between feature values and corresponding Shapley Values, in other words, feature values and their impact on prediction.","a070e1a8":"# 8 - Interpretability\nLet's check feature importances and some partial dependence plots using Random Forest (better accuracy until now)","9796a0be":"### Knowing the dataset ","471959d4":"# 6 - Gradient Boosting Decision Trees\n\nXGboost is an implementation of Gradient Boosting Decision Trees.\n\nBoosting is basically about combining simple models to build a stronger one. The predictors are trained sequentially, trying to learn from mistakes and trying to correct the previous model.  \nGradient Boosting is one of the most popular boosting methods. It works fitting new models to the errors of the previous models.\n","3351b986":"# 5 - Random Forest\n\nBefore Random Forest, let's remind something about Decision Trees.\nRoughly speaking, Decicion Trees split data making questions about feature values at each node. During this split there is the goal of maximizing the **information gain** at each split. Information gain can be understood as the difference between the impurity of a parent node and the sum of impurities of its child nodes. Thus, less impurity on child nodes, more information gain.\n\nRandom Forest is an ensemble of decision trees. Ensemble is a method that combines many models. The final prediction can be, for example, the average of them (regression) or the majority votes (classification). Random Forest consists in a group of decision trees built on samples of training dataset. More specifically, what happens is a sampling with replacement (bootstrap sampling, bagging ensemble  method).\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/7\/76\/Random_forest_diagram_complete.png)\n\n\nBut why \"random\"? Because at each split, instead of choosing the feature that maximize de information gain **among all features**, will be considered only a **random subset** of the features. This brings some diversity to the model.\n\nRandom Forest is easy, robust and can collaborate with interpretability. Some of the advantages of Random Forest are the possibility of check feature importance.","dbec62da":"# Classification: Review with Python\n\n\nMachine learning problems can be divided in 3 types: supervised, unsupervised and reinforcement learning.   \nWhile unsupervised learning is about searching for a structure in unlabeled data, supervised learning takes care of labeled data and works to predict and understand the label\/outcome.  \n2 common problems of supervised learning are classification and regression. While regression predicts continuous outputs, classification predicts categorical outputs. Some examples of classification are spam detection and churn prediction.  \n\n\nHere I intend to create a model to predict when a client will accept a term deposit.  \n\n1- Exploratory Data Analysis  \n2- Preprocessing  \n3- KNN  \n4- SVM  \n5- Random Forest  \n6- Boosting  \n7- Brief talk about other metrics  \n8- Interpretability","4fe03296":"### Plots","9061ecbf":"### Some checkings","b59df71f":"Earlier ages seem to be related with less chance of acceptance until close to 50 years, then the age increase seems to increase the chances of acceptance. ","fbfb3812":"# 2- Preprocessing","06f8d913":"Next topics will be about models, be aware that when the model works well on training but not on testing, this can indicates overfitting (high variance). The model is not generalizing well on unseen data, and it can be learning a pattern from the noise. It can be solved making the model simpler tuning the complexity (regularization) for example.","2b1ac0ef":"# 7 - Brief talk about other metrics\nBesides **accuracy**, we can also check the confusion matrix and the classification report.   \n**Precision** is about how good the model is to predict correctly as positive (get right) TP\/TP+FP  \n**Recall**   is about how good the model is to predict correctly the positives (reach them) TP\/FN+TP   \nThe Precision Recall Curve shows the tradeoff between them.   \nF1-score is a combination of precision and recall = 2.(Precision x Recall\/Precision + Recall)  \nThe ROC curve shows the relationship between True Positive Rate and False Positive Rate.  \nThe area under the ROC curve is also an important metric: the closer to 1, the better.  ","bbeb9d0c":"# 3 - KNN\n\nK-nearest neighbor is an instance-based and nonparametric algorithm. Nonparametric because it doesn't have a determinated number of parameters before training and instance based because it memorizes the training dataset. It's also considered a lazy learning due to this learning process.\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/e5\/KNN_detec.JPG)\n\nKNN works to find the K-nearest neighbors of a new sample, based on a distance metric (usually Euclidean distance), and make the prediction. In classification problem, the new point can be classified based on majority vote and in regression problems the prediction can be based on average of k-neighbors.\n\nThe choose of **K** parameter is important because small k can lead to overfitting.","49bc15e5":"Here we can see that the increase on balance,up to a point, seems to increase the chances of acceptance (term deposit) .","551c83d4":"Here we can see, for example, the positive effect of high values of age and high values of balance on the binary prediction. The dependence plots reinforce it.","8dc72d57":"I will cover logistic regression on a R notebook. I guess will be more confortable to check assuptions with R.  \nThis is a work under construction. I still want to work more on models and interpretability. Suggestions are welcome.","44bf5934":"# 4 - SVM\n\nSupport Vector Machine (SVM) can be used for classification, regression and even outlier detection.\n\nSVM works to find the hyperplane (decision boundary) that best separates training data. There are many ways of separating them but the best way is that one that maximizes the distance between hyperplane and the points closest to it. Like the red one on the graph. All this to avoid misclassification of a new instance.\n\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/b\/b5\/Svm_separating_hyperplanes_%28SVG%29.svg\/277px-Svm_separating_hyperplanes_%28SVG%29.svg.png)\nThe points closest to the hyperplane are called support vectors.\n\nWhat if it's not linearly separable? SVM can map the problem on a higher dimensional space, where it can be linearly separable. These methods are called **kernel** methods and Radial Basis Function (RBF) is an example of them.\n\nWhat about hyperparameters? \n\n**C** can control the penalty for misclassification and can help avoiding overfitting.\n* Larger C, more penalty, tighter margin, less misclassification on training, more variance. \n* Smaller C, less penalty, larger margin, more misclassification on training,less variance.\n\nIn case of a problem considered nonlinear, we have the **gamma** hyperparameter, which will control the importance of training points and can also help avoiding overfitting. Less gamma, softer decision boundary. Higher gamma, more \"fitted\" to training points and more likely to overfitting.","ef8ecd83":"According to documentation, this is the classic marketing bank dataset uploaded originally in the UCI Machine Learning Repository.\n\n**Source:**  \n[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014  \n\n### Attribute Information:\n\n**About client features:**  \n-age  \n-job (type of job)  \n-marital (marital status)    \n-education  \n-default (has credit in default? 'no','yes','unknown')  \n-balance  \n-housing (has housing loan?'no','yes','unknown')  \n-loan (has personal loan?'no','yes','unknown')  \n\n\n**About current campaign:**  \n-contact (communication type: 'cellular','telephone')  \n-duration (last contact duration, in seconds) Important note: the duration attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.  \n-day (last contact day of the week)  \n-month (last contact month of year)  \n-campaign (number of contacts performed during this campaign and for this client)  \n-deposit (has the client subscribed a term deposit?'yes','no')  \n\n**About previous contacts\/campaign:**  \n-pdays (number of days that passed by after the client was last contacted from a previous campaign, 999 means client was not previously contacted)  \n-previous (number of contacts performed before this campaign and for this client)  \n-poutcome (outcome of the previous marketing campaign: 'failure','nonexistent','success')","1a37a0dd":"# 1- Exploratory Data Analysis  \nTime to know the data. Time to get insights, to look distributions and relationship among variables and to identify potential problems like missing values, outliers and errors. \nAfter some informations about the dataset, I am going to make important checkings, the partition into train\/test and some plots.","610e92e1":"### Partitioning, creating a Test Set","c23cf364":"References:\n\n* Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow from Sebastian Raschka e Vahid Mirjalili\n* Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems from Aurelien Geron\n* Interpretable Machine Learning, A Guide for Making Black Box Models Explainable by Christoph Molnar\n* https:\/\/www.scikit-yb.org\/en\/latest\/"}}