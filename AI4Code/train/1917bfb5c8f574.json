{"cell_type":{"b2806cd0":"code","1d32b86d":"code","3238badb":"code","e4982d38":"code","c0aaa5b6":"code","08efa3de":"code","ee7e62ad":"code","6a96cb78":"markdown","48b59038":"markdown","48073257":"markdown","6359cd80":"markdown"},"source":{"b2806cd0":"import numpy as np\n\n\nclass Perceptron:\n    def __init__(self, max_iters=100):\n        self.max_iters = max_iters\n    \n    def fit(self, X, y):\n        # Bookkeeping.\n        X, y = np.asarray(X), np.asarray(y)\n        iters = 0\n        \n        # Insert a bias column.\n        X = np.concatenate((X, np.asarray([[1] * X.shape[0]]).T), axis=1)\n        \n        # Initialize random weights.\n        \u03c9 = np.random.random(X.shape[1])        \n        \n        # Train as many rounds as allotted, or until fully converged.\n        for _ in range(self.max_iters):\n            y_pred_all = []\n            for idx in range(X.shape[0]):\n                x_sample, y_sample = X[idx], y[idx]\n                y_pred = int(np.sum(\u03c9 * x_sample) >= 0.5)\n                if y_pred == y_sample:\n                    pass\n                elif y_pred == 0 and y_sample == 1:\n                    \u03c9 = \u03c9 + x_sample\n                elif y_pred == 1 and y_sample == 0:\n                    \u03c9 = \u03c9 - x_sample\n                \n                y_pred_all.append(y_pred)\n            \n            iters += 1\n            if np.equal(np.array(y_pred_all), y).all():\n                break\n                \n        self.iters, self.\u03c9 = iters, \u03c9\n        \n    def predict(self, X):\n        # Inject the bias column.\n        X = np.asarray(X)\n        X = np.concatenate((X, np.asarray([[1] * X.shape[0]]).T), axis=1)\n        \n        return (X @ self.\u03c9 > 0.5).astype(int)","1d32b86d":"clf = Perceptron()\nclf.fit([[1], [2], [3]], [0, 0, 1])","3238badb":"clf.iters","e4982d38":"clf.predict([[1], [2], [3]])","c0aaa5b6":"clf = Perceptron()\nclf.fit([[1], [2], [3]], [0, 1, 0])","08efa3de":"clf.iters","ee7e62ad":"clf.predict([[1], [2], [3]])","6a96cb78":"# Implementing a perceptron\n\n## Introduction\n\nIn this notebook I hand-implement a perceptron. The perceptron is the simplest possible neural network. It lacks any hidden layers, and uses binary threshold as its activation function.\n\nHow do you train a perceptron? Here's the example for a binary classification problem:\n\n1. Add an extra component with the value 1 to each input vector. This is the bias term.\n2. Pull the training samples, and run each one through the classifier.\n3. If the output is correct, leave the weights alone.\n4. If the output is incorrect, and a false negative (gives 0 when should give 1), add the input vector to the weights vector.\n5. If the output is incorrect, and a false positive (gives 1 when it should give 0), subtract the input vector from the weights vector.\n\nThe geometric interpretation is that the set of weights that boundarize a correct solution for any given training sample form a separating hyperplane. Lots of training samples draw lots of separating hyperplanes. The solution space of acceptable weights is the set of coordinates in the affine space constructed by the half-planes defined by the hyperplanes. This is a convex optimization problem!\n\n## Implementation","48b59038":"Completely correct!\n\nHere's an application to a dataset that is not binary distinguishable.","48073257":"As you can see it gets this one very, very wrong!","6359cd80":"## Application\n\nHere's an application to a trivial binary-separable dataset."}}