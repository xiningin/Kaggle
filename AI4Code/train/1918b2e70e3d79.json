{"cell_type":{"e74853a1":"code","9019c636":"code","907431e2":"code","80412f0f":"code","f2e576c9":"code","3914dd54":"code","9bab3ec1":"code","22265127":"code","04f0cbab":"code","ef848d15":"code","d2ab1114":"code","8f3a32d1":"code","720549ca":"code","794dfe49":"code","93078d32":"code","0539fe02":"code","8ca74217":"code","008846f8":"code","e79624a8":"code","9a1504bd":"markdown","f208935d":"markdown","7a8990f8":"markdown","dc40ecb6":"markdown","1273dd66":"markdown","2e3303b5":"markdown","6c6cfe95":"markdown","d7eaf6cd":"markdown","579335d2":"markdown","4bbd4fa3":"markdown","d0bff584":"markdown","4c92a0f7":"markdown","a6acdb35":"markdown","38f94688":"markdown","50217ba8":"markdown","30a0d420":"markdown","75f43f9c":"markdown"},"source":{"e74853a1":"# To begin this exploratory analysis, first import libraries and define functions and utilities to work with the data.\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# for beautiful plots and some types of graphs\nimport seaborn as sns","9019c636":"# IMPORTING THE DATA\n# There is 1 csv file in the current version of the dataset\n\nkBaseDataDirectory = \"\/kaggle\/input\"  # on Kaggle\n#kBaseDataDirectory = \".\/kaggle\/input\"  # when working offline with jupyter notebook\n\ndataset_files = []\n\n# This loop will import all dataset files in case we add more data in a next version of the dataset\nfor dirname, _, filenames in os.walk(kBaseDataDirectory):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        dataset_files.append(os.path.join(dirname, filename))\n","907431e2":"#######   Utility functions for statistics   #######\n\n### Helpers to filter dataframes\n\ndef helper_has_fields_compared_to(df, columns, target, what, operator):\n    \"\"\"\n    Helper to compare several columns to the same value.\n    \"\"\"\n    col = columns[0]\n    res = df[col] > target\n    for col in columns[1:]:\n        if operator == '>':\n            tmp = (df[col] > target)\n        elif operator == '>=':\n            tmp = (df[col] >= target)\n        elif operator == '<=':\n            tmp = (df[col] <= target)\n        elif operator == '<':\n            tmp = (df[col] < target)\n        elif operator == '==':\n            tmp = (df[col] == target)\n        elif operator == '!=':\n            tmp = (df[col] != target)\n        \n        # \n        if what == 'all':\n            res = res & tmp\n        elif what in ['any']:\n            res = res | tmp\n    return res\n\ndef helper_has_any_field_greater_than(df, columns, target):\n    \"\"\"Returns lines of the dataframe where any of value of the specified columns\n    is greater than the target.\n    \"\"\"\n    res = helper_has_fields_compared_to(df, columns, target, 'any', '>')\n    return res\n\ndef helper_has_all_field_greater_than(df, columns, target):\n    res = helper_has_fields_compared_to(df, columns, target, 'all', '>')\n    return res\n\n\n### Other utilities for stats\n\ndef frequency(data, probabilities=False, sort=False, reverse=False):\n    \"\"\"Returns the frequency distribution of elements.\n    This is a convenience method for effectif()'s most common use case, without all the more complicated parameters.\n    :param data: A collection of elements you want to count.\n    :param bool probabilities: Whether you want the result frequencies to sum up to 1. Default: False\n    \"\"\"\n    xis, nis = effectif(data, returnSplitted=True, frequencies=probabilities, sort=sort, reverse=reverse)\n    return xis, nis\n\n\ndef frequences(data, returnSplitted=True, hashAsString=False, universe=None, frequenciesOverUniverse=None):\n    \"\"\"\n    \"\"\"\n    if universe is None:\n        return effectif(data, returnSplitted, hashAsString, True)\n    else:\n        return effectifU(data, universe, returnSplitted, hashAsString, True, frequenciesOverUniverse)\n    \n\ndef effectif(data, returnSplitted=True, hashAsString=False, frequencies=False, inputConverter=None, sort=False, reverse=False):\n    \"\"\"calcule l'effectif\n    :param list data: une liste\n    :param bool hashAsString: whether we should convert the values in 'data' to\n                string before comparing them\n    :param function inputConverter: a callable function that is used to convert\n                the values within data into the class you want the values to be\n                compared as. When not provided, the identity function is used.\n                If used with parameter 'hashAsString', the hashed value will be\n                the one returned by this function.\n    :param bool sort: sort the result (only if returnSplitted). Shorthand for `sortBasedOn`\n    :param bool reverse: reverse the order (only if sort and returnSplitted). Shorthand for `sortBasedOn`\n    \"\"\"\n    inputConverter = (lambda x: x) if inputConverter is None else inputConverter\n    effs = {}\n    for val in data:\n        val = inputConverter(val)\n        key = str(val) if hashAsString else val\n        try:\n            effs[key] = effs[key]+1\n        except:\n            effs[key] = 1\n    \n    if frequencies:\n        tot = sum(effs.values())\n        for key in effs:\n            effs[key] = effs[key]\/tot\n    \n    if returnSplitted:\n        xis = list(effs.keys())\n        nis = list(effs.values())\n        if sort:\n            xis, nis = sortBasedOn(nis, xis, nis, reverse=reverse)\n        return xis, nis\n    \n    return effs\n","80412f0f":"# Distribution graphs (histogram\/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n\n# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth, segmentName=None):\n    filename = segmentName if segmentName else getattr(df, \"dataframeName\", segmentName)\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n\n# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","f2e576c9":"nRowsRead = None  # integer: How many rows to read. specify 'None' if want to read whole file\n# loading the 1 dataset file of the kernel\nmain_input_dataset_filepath = dataset_files[0]\ndf1 = pd.read_csv(os.path.join(kBaseDataDirectory, main_input_dataset_filepath), delimiter=',', nrows = nRowsRead)\n\ndf1.dataframeName = 'users.dataset.public.csv'\norig_df1 = df1.copy()\norig_df1.dataframeName = 'users.dataset.public.csv'\n\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows (i.e. users) and {nCol} columns (i.e. possible features)')\nprint('\\nThese columns are: \\n{}'.format(\" - \".join(list(df1.columns))))","3914dd54":"#orig_df1.head(10)\norig_df1.sample(10)","9bab3ec1":"#print('Columns: \\n{}'.format(\" - \".join(list(df1.columns))))\n\nuseless_columns = []\n# unused metadata are dropped\nuseless_columns += [\"identifierHash\", \"type\"]\n\n# Duplicate columns are dropped in favor of their siblings\nuseless_columns += [\"seniority\", \"seniorityAsYears\", \"civilityGenderId\", \"country\"]\n\ncolumns_unused_in_correlations = [\"hasProfilePicture\", \"daysSinceLastLogin\"]\n\n# Unless one specifically want to look at the difference between iOS users and Android users\n# For instance, \"Do iphone\/iOS users buy more than Android users, since iPhones are more pricey ?\"\n# However, keep in mind you should study the average users, and this version of the dataset is too small\n# to study and conclude straight. Contact the author of the dataset for more dataset entries ;)\nunused_mobile_app_columns = [\"hasIosApp\", \"hasAndroidApp\"]\n\n# this way we can have cleaner graphs if needed.\nall_unused_columns = useless_columns + unused_mobile_app_columns + columns_unused_in_correlations\n\n## Dropping columns inplace to conserve the df1.dataframeName member\ndf1.drop(useless_columns, axis=1, errors='ignore', inplace=True)\n#df1.drop(all_unused_columns, axis=1, errors='ignore', inplace=True)\n\n## Reordering few columns for better visualization and concordance\ncols = df1.columns.tolist()\n_i = cols.index(\"socialProductsLiked\")\n_ = cols.pop(_i); cols.insert(cols.index(\"productsWished\"), \"socialProductsLiked\")\n_i_country = cols.index(\"countryCode\"); cols.pop(_i_country); cols.insert(cols.index(\"language\")+1, \"countryCode\")\ndf1 = df1[cols]\n\n#print(df1.daysSinceLastLogin.describe(), \"\\n\\n\", df1.seniority.describe())\n\nprint(f\"New shape: {df1.shape[0]} users (rows), {df1.shape[1]} features (columns)\\n\")\nprint(\"Remaining columns: \\n{}\".format(\" - \".join(list(df1.columns))))\ndf1.sample(5)","22265127":"df1.describe()","04f0cbab":"### Hence, let's create specific subsets of users regarding all that.\n\n### ACTIVE USERS ###\n# By filtering out users that are completely passive from the dataframe\n# (i.e. users who did not sell\/upload\/buy\/like\/wish\/... any product), we can study the behavior of real active users.\n\n# using df1 instead of orig_df1 in case we had removed some rows after the initial import\n\nactive_df = df1[helper_has_any_field_greater_than(df1, ['socialProductsLiked', 'productsListed', 'productsSold',\n       'productsPassRate', 'productsWished', 'productsBought'], 0)]\nactive_df.dataframeName = \"Active Users\"\n\n\n### BUYING\/SELLING BEHAVIOUR ###\n\n### Buyers\nbuyers_df = df1[df1.productsBought > 0]\n#buyers_df.productsBought.describe()\nbuyers_df.dataframeName = \"Buyers\"\n\n### Prospecting Sellers (has sold a product or is still trying do his\/her first sale)\nsellers_df = df1[(df1.productsListed > 0) | (df1.productsSold > 0)]\nsellers_df.dataframeName = \"Prospecting Sellers\"\n\n### Successful sellers (at least 1 product sold)\nsuccessful_sellers_df = df1[df1.productsSold > 0]\nsuccessful_sellers_df.dataframeName = \"Successful sellers\"\n\n\n### SOCIAL INTERACTIONS ###\n\n# Users using the social media features of the service\n# Tracks anyone who willingly un\/-subscribed to someone.\n# And it also includes accounts that are either good (increased their followers)\n#   or bad (even basic followers unsubscribed from them).\n# Since each new account is automatically assigned 3 followers and 8 accounts to follow\n# I filter out those who differ from these default account settings.\nsocial_df = df1[ (df1['socialNbFollowers'] != 3) | (df1['socialNbFollows'] != 8) ]\n#asocial_users_df = df1[ (df1['socialNbFollows'] < 8) ]\n\n# Among those social users, filter only those active on products (selling, buying or wishing for articles, ...)\nmarket_social_df = social_df[helper_has_any_field_greater_than(social_df, ['socialProductsLiked', 'productsListed', 'productsSold',\n       'productsPassRate', 'productsWished', 'productsBought'], 0)]\n\n#print(socialUsers.shape, activeSocialUsers.shape)\n#socialUsers.head(20)\n\n\n### RESULTS \/ INFOS\nprint(f\"\"\"Out of the {orig_df1.shape[0]} users of the dataset sample, there are:\"\"\")\nprint()\nprint(f\"\"\"- {active_df.shape[0]} active users ({100*active_df.shape[0]\/df1.shape[0]:.3}%). Among these prospective buyers and sellers\"\"\")\n\nprint(f\"\"\"  - {active_df.shape[0] - sellers_df.shape[0]} are prospective buyers\"\"\")\nprint(f\"\"\"    among which {buyers_df.shape[0]} people actually bought products (at least 1)\"\"\")\nprint(f\"\"\"  - {sellers_df.shape[0]} are prospective sellers\"\"\")\nprint(f\"\"\"    among which {successful_sellers_df.shape[0]} are successful sellers (>= 1 product successfully sold)\"\"\")\nprint()\nprint(f\"\"\"- {social_df.shape[0]} people using social network features\"\"\")\nprint(f\"\"\"  such as following accounts or getting followers\"\"\")\nprint(\"\\nNote that among the above number of sellers, some may act as buyers and vice-versa\")","ef848d15":"print(f\"Active users: {active_df.shape[0]} records with {active_df.shape[1]} columns\")\nactive_df.sample(12)","d2ab1114":"#### Adding columns\n\n## Period in which the user has not completely dropped out (in nbr of days)\n## TODO: check if enough data for the formula\n# df1[\"activityDays\"] = df1.apply(lambda row: (row[\"seniority\"] - row[\"daysSinceLastLogin\"]), axis=1)","8f3a32d1":"#df1[\"seniorityAsMonths\"] = df1[\"seniorityAsMonths\"].apply(lambda x: int(x))\n\n#print(\"Months of seniority of the users in the dataset: \\n- {} months\".format(\" months\\n- \".join(map(str, df1[\"seniorityAsMonths\"].unique()))))\n\n#df1.head(10) #[\"seniorityAsMonths\"]\n#df1.seniorityAsMonths.describe()","720549ca":"### Distribution graphs (histogram\/bar graph) of sampled columns:\n\n#plotPerColumnDistribution(df1, 10, 5)","794dfe49":"plotCorrelationMatrix(df1, 8, \"All users\")","93078d32":"#correlation_df = df1.drop([\"hasProfilePicture\", \"hasAndroidApp\", \"hasIosApp\"], axis=1, errors=\"ignore\")\ncorrelation_df = active_df.drop([\"hasProfilePicture\", \"hasAndroidApp\", \"hasIosApp\"], axis=1, errors=\"ignore\")\ncorrelation_df.drop([\"seniority\", \"daysSinceLastLogin\"], axis=1, errors=\"ignore\", inplace=True)\nplotCorrelationMatrix(correlation_df, 8, \"Active Users\")\n","0539fe02":"plotCorrelationMatrix(buyers_df.drop(all_unused_columns, axis=1, errors=\"ignore\"), 8, \"Buyers\")\n\nprint(f\"\"\"In average, buyers buy {buyers_df.productsBought.sum() \/ buyers_df.shape[0] :.2f} products. Details are as follows:\"\"\")\nbuyers_df.productsBought.describe()","8ca74217":"dropout_after = lambda df, dayMax: dayMax - df.daysSinceLastLogin\nmax_last_login = df1.daysSinceLastLogin.max()\n","008846f8":"#df1 = df1[df1.daysSinceLastLogin <= df1.seniority]\n\nplt.title(f\"User retention: how users drop out over time [all user segments]\")\n#plt.xlabel(\"(present) <-- # days since last login  --> (past)\")\n#sns.kdeplot(np.array(df1.seniority), shade=True)\nplt.xlabel(\"(past) <-- # days before dropout  --> (present)\")\nplt.ylabel(\"density\")\nsns.kdeplot(np.array(dropout_after(df1, max_last_login)), shade=True)\nplt.show()\n","e79624a8":"plt.title(\"User retention - how people who bought at least one product dropout\")\n#plt.xlabel(\"(present) <-- # days since last login  --> (past)\")\n#sns.kdeplot(np.array(buyers_df.daysSinceLastLogin), shade=True)\nplt.xlabel(\"(past) <=  # days before dropout  => (present)\")\nplt.ylabel(\"density\")\nsns.kdeplot(np.array(dropout_after(buyers_df, max_last_login)), shade=True)\nsns.kdeplot(np.array(dropout_after(buyers_df, max_last_login)), shade=True)\nplt.show()\n\nplt.title(\"User retention - how sellers keep visiting a C2C site\")\n#plt.xlabel(\"(present) <-- # days since last login  --> (past)\")\nplt.xlabel(\"(past) <=  # days before dropout  => (present)\")\nplt.ylabel(\"density\")\nsns.kdeplot(np.array(dropout_after(sellers_df, max_last_login)), shade=True)\nplt.show()\n\nplt.title(\"User retention - buyers and sellers dropout curve\")\nsns.kdeplot(np.array(dropout_after(sellers_df,max_last_login)), shade=True)\nsns.kdeplot(np.array(dropout_after(buyers_df,max_last_login)), shade=True)\nplt.xlabel(\"(past) <=  # days before dropout  => (present)\")\nplt.ylabel(\"density\")\nplt.legend([\"sellers\", \"buyers\"])\nplt.show()","9a1504bd":"#### Removing duplicate colums\n\nAs you may have seen, **some columns describe the same thing** (they are precomputed in order to provide insights with bare eyes). \nFor instance, as described in the dataset description, the columns with `seniority` tell us how old the user account is.\nSince we are exploring the data using a kernel, I will *remove such duplicate columns and create my owns if need be*.","f208935d":"We see that even a one-time buyer can be expected to keep visiting you over a long period of time.\n\nWhile a lot of new users can cease using your site rapidly, the ones you get to keep and bring into action will be an asset for the future.\nOnce someone has made even the slightest step to engage with you and really use your platform (either by buying or uploading products to sell), you should put your efforts to support the relationship. It may be obvious, but here is a proof provided by actual data.\n\nFinally, we can observe the difference in the dropout curve between *buyers* and *sellers*. The latter seem to try the platform for a shorter amount of time before deciding whether or not they will turn to another one.\n\n> This means that a *crucial* step in C2C is ensuring \u2013among other things\u2013 that:\n> - your platform is easy to use for your sellers\/service providers\n> - you provide the actual tools they want or need (example: analytics, a not too restrictive way to interact with their prospective buyers, ...)\n\n\nWhereas *buyers* have mainly two distinct behavior: purchase once and then never come back (as denoted by the first litlle bump) or staying faithful to the platform for a long while (the spike at the end representing the present).\n","7a8990f8":"**_Buyers_ behaviour - Interesting fact**\n\n- So even among the buyers we observe that the `the number of accounts a user follows` and the number of `products that user bought` are not correlated.\n  \n  - This somehow might seem **counter-intuitive** since one might expect buyers to follow accounts they wish to purchase an article from, or follow accounts of sellers with products they might intend to buy.\n\n    One explaination would be that most users in general use to \"follow\" profiles that share tastes with. This could be to keep up with the trends, or just to get inspiration of styles from influencers and other people in the social network.\n\n    Regardless, considering only the ROI in developping a social network feature for your website, it might seem that giving your users the ability to follow inspiring users would not directly lead to an increase in the overall number of sales on your website.\n    Thus in order to use the business' funds to optimize the ROI, there might be better ways than implementing that aspect.\n    \n    However, as the previous result shows, from the point of view of individual sellers, sellers compete with each other to gain more followers, and the sellers who get the most followers are more likely to grab the buyers' attention when those are looking for something new to buy.\n    \n    So a feature to \"follow\" accounts may not yield buyers to buy more from a C2C website as a whole, but rather this would help motivate the sellers of the platform, since they will be able to see positive results from taking the time to improve their presence on the platform. Sellers are aslo the oil of a C2C business, so pampering them may be one of the factors that set apart successful C2C businesses.\n    \n  \n  - **_However_**, this weak correlation could be mitigated if we consider that maybe users with lower budgets might tend to \"dream\" more and follow more accounts, even though they cannot afford to buy from that much of their influencers.\n   \n    After all, clients cannot simply buy a product from each seller they follow. No one has an infinite amount of money to spend, not mentionning that some clients and prospects may even be on budget. In general, buyers bought `3` products in `average`, while the `median` stands at `1`.\n\n\nIn order to conclude whether or not a social network feature is worth it or not in terms of ROI, it would be necessary to look at other data and taking into account parameters specific to your business.\n\nFor instance, the prices of the products. If most products on a website are \"cheap\" (cheap compared to the standard budget people would allocate for your type of products) and there is a very weak correlation between `the number of accounts users follow` and the `number of products bought`, then it would lean towards the first hypothesis, since we could reasonably assume users have enough money to buy more products.\n\nAlso, it is worth noting that it would be necessary to normalize the relationship between those two columns using other sources of data. The idea is that, for example, maybe the correlation is roughly linear, but the coefficient of linearity is so low that it would appear as if the correlation is weak or non existent.\n","dc40ecb6":"## Exploratory Analysis\n","1273dd66":"### **Segmenting the users**\n\n#### **Studying non-passive people**\n\nCurrently, the dataset contains too many users who are inactive (users that only seem to have signed up to try the site out, maybe browse few articles and do nothing else\/drop out). The website actually strongly incitate users to sign up (otherwise they cannot browse the site for long), which may be a reason for such a proportion of passive users.\n\nSince the website is a C2C platform whose business model is to earn money when products are sold, I will define and call `active` users whose activity on the platform directly participates toward that end, i.e. prospective buyers and prospective sellers.\n\nFormally, **`active` users** are those that are either:\n\n- prospective buyers: those interacted with products of others with a like\/wishlist\/purchase\n- prospective sellers: those having at least one product for sale.\n\n\nAlso, I define as **`social` users** those accounts that interacted with other users by following accounts or getting followers.\n\n\nHowever, I **_exclude_** the following key metrics and will not create user segments related to:\n\n- having or not a profile picture\n- having or not installed any of the site's official app\n- browsing the site regularly (i.e. users who dropout late)\n\n\nThe dataset file contains active, social and inactive users. Also note that one can belong to multiple segments simultaneously, like a seller buying someone else's product.\n\nWe might want to study **_buyer_**-specific or **_seller_**-specific behaviour. Hence I will also filter these specific segments. \nWhile we're at it, let's also create a subset of **_social_** users so that their behavior can be studied.\nThe code below will create the aforementioned user segments.","2e3303b5":"For the curious eye, here is a statistical summary of the various numerical columns in the dataset so far.","6c6cfe95":"#### User retention\n\nFinally, a little parting gift: a graph showing user retention over time.\n\nA lot of users drop out quickly. The vast majority actually.\nThis is true for most website\/services\/apps: a lot of people try it once, maybe twice and then a lot of them drop out really quickly.\n\nHopefully, the following graph should give you a basis for comparison of your engagement vs dropout curve.","d7eaf6cd":"## Conclusion\n\n\n- Adding `wishlist` and `like` features on top of the basic `cart` feature allows your business to better understand your customers and their tastes. It should not be restricted to C2C business, but it could bring improvements on your products and services recommendations.\n\n- Sellers of a C2C website stand to gain a lot by having the means to create attractive product or service pages to drive more sales. And if your sellers sell more, your business can earn more too.\n\n- Adding a social network feature on a platform that connects sellers with buyers can greatly help your best sellers stand out. Seeing their efforts pay off will serve as rewards pushing them to invest more time and effort to build their community on *your* platform, which is for your benefit. After all, having sellers compete with one another yields to better products\/contents\/services for your customers and thus your reputation.\n  \n  And you should not forget that sellers are the oil that allow a C2C (or B2C) business to move forward. You should take interest in treating them well.\n\n- Finally, have a look at the example user retention curve to learn what kind of different behaviour to expect from buyers and sellers.\n","579335d2":"### Visualizing the data\n\nNow that our data is segmented, let's have a look at how the features correlate with one another.\n\nHere is a correlation matrix:","4bbd4fa3":"There are other interesting things we might note, like the fact that `socialNbFollows` (the nbr of accounts a user follows) and `productsBought` are not correlated at all, but it would be wiser to look at this correlation using only the segment of data containing *buyers*. Sellers may have a different behaviour, such as using the \"follow\" feature to get some follow-back instead of showing their appreciation of the other user's content.\n\nSo how about the correlation matrix for buyers ?\n","d0bff584":"Let's take a quick look at what the data looks like. More explainations about each column are available in the [dataset page](https:\/\/www.kaggle.com\/jmmvutu\/ecommerce-users-of-a-french-c2c-fashion-store) so I am not going to review those, especially since the columns of this dataset are self-explainatory. I will assume you have familiarized yourself with the information presented on the dataset project page.\n\nAs a brief recap, the table represented here concerns users of a C2C website. People can sell *and* buy products from one another.\nAs such, we can look at this dataset to study either buyers or sellers. They can also follow other users\/be followed and \"like\" products, just as in a. social network.\n\nEach row in the table represents a single user's statistics. Profiles have been anonymised for privacy concerns.","4c92a0f7":"### The dataset file","a6acdb35":"### Insights from the correlation matrix\n\nThe above matrix shows the following correlations:\n\n  **Buyers**\n- `productsBought` and `productsWished`\n- `productsBought` and `productsLiked` (though correlation is weaker than with `productsWished`).\n\n  This is interesting to note in terms of user experience. A significant enough number of users seem to put different meaning into `wishlist` and `like`. Proposing both options may help understanding a user's tastes and intentions better. These different lists of products can then be used to recommend more appropriate products.\n  - Thus this result should not only be limited to C2C platforms. Regular e-commerce shops (B2C) may also incorporate `like` and `wishlist` buttons to improve their knowledge of their users and prospects. By relying only on items added to the `cart`, a business would miss the greater potential of more specific targeting.\n  \n\n  **Sellers**\n- socialNbFollow***ers*** and `productsSold`\n- __socialNbFollow*ers*__ and `productsListed`\n\n  This shows that people who publish more are more likely to get followers. This is not an absolute rule, since the correlation is only around 0.5.\n  Practically, we can interpret it saying that some sellers may propose clothes that do not suit the dominant tastes.\n  \n  **How to use this insight from a business's point of view ?**\n  \n  Right of the bat, we cannot reach any clear conclusion using only this aggregated data, i.e. there are several ways this correlation could be interpreted, and one would need specifically collected data to test which hypothesis is the most likely.\n\n  For instance, the correlation between `followers` and `sold products` can be seen in two main ways (or a combination of both):\n  \n  1. a seller gets followers by selling products (and thus being granted a badge of \"trusted\" seller, which in turn will get him\/her to sell more).\n  \n  2. It is also possible that in C2C platforms, successful sellers first get subscribers and then, by showing them quality products, they earn the client's trust and turn cold prospects into their clients and fans.\n  \n    For a C2C business, the latter hypothesis would the most promising. It would imply that a C2C company could earn more by empowering their seller users to create attractive content \/ ad posts.\n    This could explain why successful C2C companies like [EBay](ebay.com) and [Ricardo](https:\/\/www.similarweb.com\/website\/ricardo.ch) allow users to customize their ads with styles \/ HTML, while others like [Vestiaire Collective](https:\/\/www.similarweb.com\/website\/vestiairecollective.com) and [Vide Dressing](https:\/\/www.similarweb.com\/website\/videdressing.com) tackle this problem by turning their users' product pictures into professional looking images. (\\*Example companies successful in Europe)\n\n\n\n**Social**\n\n- In the shoes of a seller, we can hypothesize that having followers DO really help a seller get sell more, as discussed above. **Hence, for an individual uploading products on a C2C website, it would not be enough to simply add products online.**\n  To maximize profit, a seller would also need to be proactive in getting followers, or in other words, advertise himself\/herself.\n  \n  This is the approach taken by C2C businesses like [Vinted](https:\/\/www.similarweb.com\/website\/vinted.com), whose business model is solely based on having sellers promote their products through *ad boosting* and they seem successful. The fact that *Vinted* does not charge *listing fees* nor *commission upon sale* is a good plus in their advantage since most of their competitors do charge commissions based on the article price.\n  \n  It seems that the idea that one has to advertise oneself in order to get more sales is deeply ingraved in people's minds. So much so that many sellers are willing to pay to increase their visibility. It seems to be the only \/ main revenue income for [*Vinted*](https:\/\/www.vinted.com\/how_it_works) and it seems they are doing well.\n  \n  This is especially true nowadays, since there are too many offers online. Sellers seem to think there are two ways to alleviate this issue: producing more quality content (better ad pictures, product descriptions, adequate categories, ...) or paying money to boost their products in the site's catalog.\n  It would be interesting to see where the market is leaning towards and which business model generates more revenue.\n  \n  Note that, the path *EBay* chose is to combine both approaches to maximize their revenue: they allow sellers to customize their ads (EBay charges some ad features like adding a subtitle) and they also allow sellers to boost the position of their ads in the listing. That gives EBay the benefit of both sources of revenues, which may explain why the company got more funds to develop itself.\n  It would be very instructive to have such companies share the results of both approaches, so that we might know what drives more revenue.\n\n","38f94688":"With these filtered data, let's have a look at the data without all the inactive users. This way there will be more variability in the data and hopefully we can start seeing more interesting things.\n\nHere is a sample of the `active` users.","50217ba8":"How to read this graph: The height of the curve represents the part of users whose last connection was N days ago.\n\nThe spike at ~700 shows that among all the users who signed up during the same week\/day, the vast majority of them dropped right away.\n","30a0d420":"## Introduction\n\nIn this kernel, I will explore a dataset I scraped from a **Customer-to-Customer (C2C)** second hand website in order to give you *insights* and some *benchmark* results about that kind of business. These results can also be extanded towards other domains of the e-commerce landscape, so be sure to follow along.\n\nHopefully, it will also help you keep hope by comparing your business with another one, especially the successful one studied here.\n\nFeel free to reach out in the comments or with direct messages.\nHave a nice walkthrough!","75f43f9c":"This correlation matrix may look a bit messy.\n\nSome aspects of the correlation matrix may be obvious or present lower interest, which is why I will remove less meaningful columns and create another correlation matrix.\n\n> For instance, we can briefly note that `daysSinceLastLogin` seems slightly negatively correlated to almost everything. Practically, it just means that the more often a user logs in (i.e. `daysSinceLastLogin` getting smaller) the more chances that user has some a good activity or social network presence on the site (i.e. the other key metrics getting bigger).\n\n\n> For instance, we see that the boolean feature `hasProfilePicture` correlates negatively to almost all the other features. But almost all the users of the site have a profile picture. That makes it almost useless."}}