{"cell_type":{"22749980":"code","5f484eb2":"code","fe0e3b73":"code","7047060e":"code","b0a337cc":"code","77cbf1bb":"code","59c3dd57":"code","dd1e946d":"code","e477050b":"code","d9a2fcec":"code","95f63c3b":"code","db38f95a":"code","6f027714":"code","fb372926":"code","ed04e605":"code","87d1013b":"code","40fa7dbb":"code","bfd286de":"code","25436915":"code","651bd0d4":"code","be834e79":"code","a2732ddb":"code","e85e1b80":"code","a0a8bf62":"code","917a9017":"code","56b763c7":"code","e68a173f":"code","bed54891":"code","653d6e97":"code","75e0ac72":"code","8755a594":"code","e5286179":"code","3c50258c":"code","d0608326":"code","58a524e3":"code","08a32e84":"code","09b6eb1e":"code","4f5fb541":"code","f0473134":"code","397639b5":"code","5889d998":"code","4504544e":"code","1590f22c":"code","971c672e":"code","8ad641b8":"code","02053d0c":"code","2afcd587":"code","721e7cb9":"code","ad20c6a8":"code","a2921def":"code","36f10f89":"code","611629c0":"code","f4e5204f":"code","3c36a8c5":"code","73d0aa29":"code","42024b25":"code","4a71a8d4":"code","f687519e":"code","8da11054":"code","b38a780c":"code","5120ac19":"code","10f7e17d":"code","362e8de9":"code","3a8cd44f":"code","4f8b917d":"code","81fc5355":"code","514b593b":"code","c852f370":"code","ed9b5cb2":"code","a3ec8143":"code","dde7daa2":"code","02df5f8f":"code","ca575213":"code","2b21e5d6":"code","f8b3c5b1":"code","fe944a99":"markdown","c1702421":"markdown","6133eecf":"markdown","e871a967":"markdown","ad508d1e":"markdown","f148631a":"markdown","abdeeb45":"markdown","48706c47":"markdown","78fdc95a":"markdown","1d9b49bc":"markdown","76f7ad6a":"markdown","400c1b52":"markdown","63b3a701":"markdown","36bee97d":"markdown","b093cdf9":"markdown","733e3a7d":"markdown","cef41428":"markdown","b652eb83":"markdown","9221ddc6":"markdown","ff17c2a5":"markdown","0f044327":"markdown","998ae702":"markdown","342c9b0d":"markdown","40508f92":"markdown","865e3850":"markdown","cf21ca1e":"markdown","e28501e0":"markdown","a26bc7f7":"markdown","777c0c9e":"markdown","4db6ee55":"markdown","ef10a323":"markdown","b48dfb85":"markdown"},"source":{"22749980":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5f484eb2":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","fe0e3b73":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","7047060e":"all_df = [train_df,test_df]","b0a337cc":"for df in all_df:\n    df.info()","77cbf1bb":"for df in all_df:\n    print(df.describe().T)","59c3dd57":"for df in all_df:\n    print(df.isna().sum())","dd1e946d":"sns.countplot(x = 'Survived',hue = 'Sex', data=train_df, palette='RdBu_r')","e477050b":"sns.countplot(hue = 'Pclass', x = 'Survived', data = train_df, palette=\"plasma_r\")","d9a2fcec":"g = sns.FacetGrid(train_df, col='Survived',height=5, aspect= 1.33)\ng.map(plt.hist, 'Age', bins=20)","95f63c3b":"plt.figure(figsize=(12,4))\n# Training Data\nsns.heatmap(data=train_df.isna(),cmap='viridis')","db38f95a":"plt.figure(figsize=(12,4))\n# Test Data\nsns.heatmap(data=test_df.isna(),cmap='viridis')","6f027714":"train_df[train_df['Embarked'].isna()]","fb372926":"train_df['Embarked'] = train_df['Embarked'].fillna('S')","ed04e605":"train_df['Embarked'].isna().sum()","87d1013b":"test_df[test_df['Fare'].isna()]","40fa7dbb":"test_df[['Fare','Pclass']].groupby('Pclass').mean()","bfd286de":"test_df['Fare'] = test_df['Fare'].fillna(12.45)","25436915":"test_df['Fare'].isna().sum()","651bd0d4":"plt.figure(figsize=(10,7))\nsns.boxplot(x = 'Pclass', y = 'Age', data = train_df)","be834e79":"def fill_age(cols):\n    age = cols[0]\n    pclass = cols[1]\n    \n    if pd.isnull(age):\n        \n        if pclass == 1:\n            age = 37\n        elif pclass == 2:\n            age = 29\n        else: \n            age = 24\n        \n    return age","a2732ddb":"for df in all_df:\n    df['Age'] = df[['Age','Pclass']].apply(fill_age, axis = 1)","e85e1b80":"for df in all_df:\n    print(df.isna().sum())","a0a8bf62":"train_df['Cabin'].unique()","917a9017":"cabin_letter= {'A':1,'B':2,'C':3,'D':4,'E':5,'F':6,'G':7,'N':8}\n\ncabin = train_df['Cabin'].apply(lambda x : str(x)[0].upper()).map(cabin_letter)","56b763c7":"sns.histplot(x = cabin,hue = train_df['Survived'])","e68a173f":"train_df.head()","bed54891":"for df in all_df:\n    df.drop('Cabin', axis = 1, inplace = True)","653d6e97":"title = train_df['Name'].apply(lambda x: (x.split(', ')[1:2][0]).split('.')[0]).rename('Title')\ntrain_df = pd.concat([train_df,title],axis=1)","75e0ac72":"title = test_df['Name'].apply(lambda x: (x.split(', ')[1:2][0]).split('.')[0]).rename('Title')\ntest_df = pd.concat([test_df,title],axis=1)","8755a594":"print(train_df.Title.unique())\nprint(test_df.Title.unique())","e5286179":"train_df = train_df.replace(['Don', 'Rev', 'Dr','Major', 'Lady', 'Sir','Col', 'Capt', 'the Countess','Jonkheer','Dona'],'Others')\ntrain_df = train_df.replace(['Mlle','Ms'], 'Miss')\ntrain_df = train_df.replace('Mme','Mrs')","3c50258c":"train_df[['Survived','Title']].groupby('Title', as_index=False).mean()","d0608326":"test_df = test_df.replace(['Don', 'Rev', 'Dr','Major', 'Lady', 'Sir','Col', 'Capt', 'the Countess','Jonkheer','Dona'],'Others')\ntest_df = test_df.replace(['Mlle','Ms'], 'Miss')\ntest_df = test_df.replace('Mme','Mrs')","58a524e3":"train_df.head()","08a32e84":"titles = {'Master':1, 'Miss':2, 'Mr':3, 'Mrs':4, 'Others':5}\n\nall_df=[train_df,test_df]\nfor df in all_df:\n    df['Title'] = df['Title'].map(titles)","09b6eb1e":"for df in all_df:\n    df.drop('Name', axis = 1, inplace = True)","4f5fb541":"for df in all_df:\n    df['Sex'] = df['Sex'].map({ 'female': 0, 'male': 1}).astype(int)","f0473134":"for df in all_df:\n    df['Family'] = df['SibSp'] + df['Parch'] + 1\n    \ntrain_df[['Family', 'Survived']].groupby(['Family'], as_index=False).mean().sort_values(by='Survived', ascending=False)","397639b5":"for df in all_df:\n    df['IsAlone'] = 0\n    df.loc[df['Family'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","5889d998":"for df in all_df:\n    df.drop(['SibSp','Parch','Family'], axis = 1, inplace = True)","4504544e":"train_df['Ticket'].describe()","1590f22c":"for df in all_df:\n    df.drop('Ticket',inplace=True, axis= 1)","971c672e":"grid = sns.FacetGrid(train_df, row='Embarked', height=5, aspect=1.33)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","8ad641b8":"embark = {'S': 0, 'C' : 1, 'Q':2}\n\nfor df in all_df:\n    df['Embarked'] = df['Embarked'].map(embark).astype(int)","02053d0c":"train_df.head()","2afcd587":"train_df['AgeBand'] = pd.cut(train_df['Age'], 4)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","721e7cb9":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","ad20c6a8":"train_df.head()","a2921def":"all_df = [train_df,test_df]\nfor df in all_df:    \n    df.loc[ df['Age'] <= 16, 'Age'] = 0\n    df.loc[(df['Age'] > 16) & (df['Age'] <= 32), 'Age'] = 1\n    df.loc[(df['Age'] > 32) & (df['Age'] <= 48), 'Age'] = 2\n    df.loc[(df['Age'] > 48) & (df['Age'] <= 64), 'Age'] = 3\n    df.loc[ df['Age'] > 64, 'Age'] = 4\ntrain_df.head()","36f10f89":"for df in all_df:    \n    df['Age'] = df['Age'].astype(int)","611629c0":"train_df.drop('AgeBand',axis = 1, inplace = True)","f4e5204f":"train_df['FareBand'] = pd.qcut(train_df['Fare'], 3)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","3c36a8c5":"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","73d0aa29":"for df in all_df:\n    df.loc[ df['Fare'] <= 7.91, 'Fare'] = 0\n    df.loc[(df['Fare'] > 7.91) & (df['Fare'] <= 14.454), 'Fare'] = 1\n    df.loc[(df['Fare'] > 14.454) & (df['Fare'] <= 31), 'Fare']   = 2\n    df.loc[ df['Fare'] > 31, 'Fare'] = 3\n    df['Fare'] = df['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)","42024b25":"test_df.head()","4a71a8d4":"train_df.head()","f687519e":"all_df = [train_df,test_df]\nfor df in all_df:\n    df['Age*Class'] = df.Age * df.Pclass","8da11054":"train_df.drop('PassengerId',axis =1 ,inplace = True)\nId = test_df.pop('PassengerId')","b38a780c":"for df in all_df:\n    print(df.head())","5120ac19":"from sklearn.model_selection import train_test_split\n\nX = train_df.drop(\"Survived\", axis=1)\ny = train_df[\"Survived\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","10f7e17d":"from sklearn.linear_model import LogisticRegression\n# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)\nacc_log","362e8de9":"from sklearn.svm import SVC\n# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, y_train) * 100, 2)\nacc_svc","3a8cd44f":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nacc_knn","4f8b917d":"from sklearn.naive_bayes import GaussianNB\n#Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)\nacc_gaussian","81fc5355":"from sklearn.linear_model import Perceptron\n\n# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, y_train) * 100, 2)\nacc_perceptron","514b593b":"from sklearn.svm import LinearSVC\n# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, y_train) * 100, 2)\nacc_linear_svc","c852f370":"from sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, y_train) * 100, 2)\nacc_sgd","ed9b5cb2":"from sklearn.ensemble import RandomForestClassifier\n# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nacc_random_forest","a3ec8143":"from sklearn.tree import DecisionTreeClassifier\n# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nacc_decision_tree","dde7daa2":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","02df5f8f":"from sklearn.ensemble import RandomForestClassifier\n# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X, y)\npredictions = random_forest.predict(test_df)","ca575213":"submission = pd.DataFrame({'PassengerId':Id,'Survived':predictions})\n\n#Visualize the first 5 rows\nsubmission.head()","2b21e5d6":"filename = 'submission.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","f8b3c5b1":"#!kaggle competitions submit -c titanic -f submission.csv -m 'RandomForestClassifier Continued Submission'","fe944a99":"Visulazing the missing data to make it easier to interpret.","c1702421":"Next is to clean the sex column into a numerical column.","6133eecf":"There seem to be a corrolation between age and Pclass. This makes sense, since one would assume with older people are more likely to be wealthier than young people.\nWe can use this fact in our advantage to fill the missing 'Age' column.","e871a967":"Looking up both passengers that are missing the embarked value. We find two females that are on first class, Mrs. George Nelson and her maid Miss Amelie . They both embraked at Southampton. THis can be found here: https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/martha-evelyn-stone.html\n\n\"Mrs Stone boarded the Titanic in Southampton on 10 April 1912 and was travelling in first class with her maid Amelie Icard. She occupied cabin B-28.\"","ad508d1e":"The only missing data left are the cabins. Taking a better look at the values of the cabins, we don't find much information other than the letter of the cabins and a numerical value. We can extract the letters and plot using them to check wether that could prove to be helpful.","f148631a":"The data set seems to be complete. Using what we have I was able to achieve a 76% accuracy using the Random Forest model. However to improve on it even further we can adjust the values of Fare and Age to represent a number of numerical values instead of their current shapes.","abdeeb45":"The following two plots confirm that ladies are first, and that the wealthy are more likely to survive in case of disaster. Sad if you ask me.","48706c47":"# Model","78fdc95a":"Joining both data set, to make it easier to edit both at the same time.","1d9b49bc":"Passenger id is just a numerical value that has no sense. We can drop it.","76f7ad6a":"Next is to work on the names in the data set. We see clearly that the names are in the following format: Name, 'Title'. Name. We can use that in our advantage to extract the titles.","400c1b52":"We can begin by dividing the age column into 4 or 5 categories to see what the best fit is.","63b3a701":"Now that we have dealt with the age, we are left with the fares. We can do the same thing and divide the prices to multiple columns. I assume that the prices can be divided into 3 (since there are 3 classes), however we can also check if we can seperate them with 4 and then choose as we find fit.","36bee97d":"As we thought, being with someone increases the chance of survival. We can use this column now and drop SibSp, Parch and Family","b093cdf9":"I believe using 5 is a better respresentation of the data. It shows the difference between kids and young adults, and as we saw earlier the 'Women and kids' first applies in this case. \n\nWe can divide the ages into the different categories ranging from 0 to 4.","733e3a7d":"No more small missing data, we can continue with our search.","cef41428":"The largest family did not survive unfortenately. However, we can see that being with someone increased the likelihood of survival. We can use that fact to divide the columns to alone or not.","b652eb83":"Filling the data and double checking if there still are any missing values.","9221ddc6":"We can now drop the name column.","ff17c2a5":"Looking at the embark, we find that there are 3 values either S,Q,C.\nJust a quick reminder, 0 is female and 1 is male.","0f044327":"Looking for NaN values in the datasets, we find that the Age is missing, however that can easily be predicted for. The cabins are missing a lot of values we might consider removin them directly. Embarked and Fare are also missing for 3 values. These can be looked up to see if we can find them online. If not, then we can just drop the values.","998ae702":"The tickets are most likely not useful since there are many repeating values (23.5%). They also cannot easily be conveyed into a numerical value that makes sense.","342c9b0d":"The upper line was just to convert the data in 'Age' to integers instead of floats. We can now drop the 'AgeBrand' column.","40508f92":"Next I will be applying multiple models to check which has the best output.","865e3850":"Mr Thomas was in third class. Taking the mean of the fare for the third class makes sense.","cf21ca1e":"Random Forest and Decision Tree have the best distribution. However Decision Tree is most likely to overfit. \n\nI will use Random Forest as the the predictor.","e28501e0":"Instead of using pd.cut, I use pd.qcut to get quantile-based discretization. We find that there is a 15% difference between 7->14 and 14->31. I will assume that there were differences between the second classes, that are causing this difference. It can be that the more expensive cabins have a better placement that causes their survival. Using these values we can divide the 'Fare' into 4 different categories running from 0 to 3.","a26bc7f7":"We can also get rid of two columns: SibSp and Parch, which esentially represent a family.","777c0c9e":"We begin by dividing the dataset to train.","4db6ee55":"Looking at the plot, and how many values are missing I believe getting rid of cabins is the best solution.","ef10a323":"Looking into the plot. We find that it doesn't matter where you are boarding, if you are a woman in first class you have a very high chance of survival.","b48dfb85":"Lastly I found this trick in another write-up. That person used both the Pclass and the Age to create a new column. I believe that is a great idea, since the older people in first class have a higher chance of survival."}}