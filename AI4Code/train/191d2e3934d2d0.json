{"cell_type":{"c54a8d28":"code","caed22b8":"code","2ecce3c6":"code","df10c26c":"code","04d09d5c":"code","73245cd4":"code","54efcf18":"code","50e24154":"code","49c2c4c0":"code","697cab1b":"code","6a718c09":"code","feb33dc2":"code","22e4aeb8":"code","58dd28fd":"code","fc589520":"code","ee6d614c":"code","77a254c5":"code","18313425":"code","c8d20974":"code","7e5f1f4a":"code","a40bbf28":"code","392cecf2":"code","590bbf7f":"code","7124fb1f":"code","f8ea207d":"code","e19e8332":"code","ba3d36c6":"code","acd44e30":"markdown","91873ff1":"markdown","4d832089":"markdown","df85f935":"markdown","342935d9":"markdown","84c51531":"markdown","d3e259a6":"markdown"},"source":{"c54a8d28":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # For creating plots\nimport matplotlib.ticker as mtick # For specifying the axes tick format \nimport matplotlib.pyplot as plt\nimport sklearn\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","caed22b8":"# loading into Panda dataframe\ndf = pd.read_csv(\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n# looking at the features we have\ndf.columns.values","2ecce3c6":"df.head()","df10c26c":"# Look at the unique data in each variable\nfor item in df.columns:\n    print(item)\n    print (df[item].unique())","04d09d5c":"# converting yes\/no variables to 1\/0 \n# for simplicity we combine \"no\" and \"no internet service\" to 0\ndf = pd.read_csv(\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\nfor i in [\"Partner\", \n          \"Dependents\", \n          \"PhoneService\", \n          \"OnlineSecurity\",\n          \"DeviceProtection\",\n          \"OnlineBackup\",\n          \"TechSupport\", \n          \"StreamingTV\", \n          \"StreamingMovies\", \n          \"PaperlessBilling\",\n          \"Churn\"]:\n    df[i].replace(to_replace='Yes', value=1, inplace=True)\n    df[i].replace(to_replace='No', value=0, inplace=True)\n    df[i].replace(to_replace='No internet service', value=0, inplace=True)\n    \ndf[\"gender\"].replace(to_replace='Female', value=1, inplace=True)\ndf[\"gender\"].replace(to_replace='Male', value=0, inplace=True)    \n    \n# creating dummy variables for categorial features; categorial features automatically dropped\ndummy = [\"MultipleLines\", \"InternetService\", \"Contract\", \"PaymentMethod\"]\ndf = pd.get_dummies(df,prefix=dummy, columns=dummy)\ndf.head()","73245cd4":"# View unique data in each column after converting to 1\/0\nfor item in df.columns:\n    print(item)\n    print (df[item].unique())","54efcf18":"df.dtypes","50e24154":"# Need to convert TotalCharges to a float, cannot be \"object\"\ndf[\"TotalCharges\"] = pd.to_numeric(df.TotalCharges, errors='coerce')\ndf.dtypes","49c2c4c0":"# checking for empty values\ndf.isnull().sum(axis = 0)","697cab1b":"# since its only 11 cases where TotalCharges is missing we simply drop those rows. \n# we could have replaced with a 0, but that's not correct.\ndf = df.dropna()\ndf.isnull().sum(axis = 0)","6a718c09":"# We can look at statistics for the variables\ndf.describe()","feb33dc2":"# We plot the distribution of each variable\ndf_plot = df.drop(columns=[\"customerID\"])\nitem = \"TotalCharges\"\nfor item in df_plot.columns:\n    df[item].plot(kind=\"hist\",bins=10, figsize=(12,6), title = item)\n    plt.show()","22e4aeb8":"# Calculate and display the correlation matrix\n# Note that the target \"Churn\" is one of the variables in the data set, and we see that it is not especially strongly correlated with a single feature\n# We see natural relationships in the data such as \"Streaming Movies\" being correlated with \"TotalCharges\"\ncorr = df.corr()\n\nfig = plt.figure(figsize = (20,10))\n# Plot the heatmap\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns,\n           center=0)","58dd28fd":"# Ranking features with highest correlation with Churn.\n# (Naive) idea is to remove columns with low correlation to simplify our feature set and reduce model training time.\n# By only looking at correlations between single variables we miss higher order effects and may discard variables that can help our model. \n# The variables with highest correlation to the target variable might not be the same as the most important features in our model.\n# The sign of the correlation depends on how the feature is formulated.\n# Since we do not have a lot of data and features, we do not remove any in this case.\ndf.corr()[\"Churn\"].sort_values(axis=0,ascending=False)","fc589520":"# We are happy with our data set and ready to train a classifier.\n# Split into test and training set, random selection of customers.\n\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df, test_size=0.3)\ntrain_x = train.drop(columns=[\"Churn\",\"customerID\"])\ntrain_y = train[[\"Churn\"]]\ntest_x = test.drop(columns=[\"Churn\",\"customerID\"])\ntest_y = test[[\"Churn\"]]","ee6d614c":"# Decision tree\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html\n# Have to set a max depth to avoid overtraining!\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nclf = tree.DecisionTreeClassifier(max_depth=5)\n#clf = RandomForestClassifier(n_estimators=40)\nclf = clf.fit(train_x, train_y)\npred = clf.predict(test_x)\n# The probability of each class is predicted, which is the fraction of training samples of the same class in a leaf\nprob = clf.predict_proba(test_x)","77a254c5":"# Calculating ROC AUC and plotting the ROC curve, \n# Calculate the fpr and tpr for all thresholds of the classification\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfpr, tpr, threshold = roc_curve(test_y, prob[:,1])\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend([round(roc_auc_score(test_y, prob[:,1]),2)]);\nplt.show()","18313425":"# The best way to think about hyperparameters is like the settings of an algorithm that can be adjusted to optimize performance.\n# Adapted from https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py\n\n# Utility function to report best scores\ndef report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")\n\n            \n# Specify parameters and distributions to sample from. \n# For now, I select only parameters that are related to the design of the tree. Ideally all parameters are included in the search space.\n# One should perform the search in several  iterations, starting from a very rough grid narrowing down each iteration.\n# I restrict the search space for max_depth to a rather shallow tree on purpose, so that we end up with a model which can be easily be interpreted in the end.\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html\nfrom scipy.stats import randint as sp_randint\nparam_dist = {\"max_depth\": np.arange(1,8),\n              \"min_samples_split\": np.arange(2,10),\n              \"min_samples_leaf\": np.arange(1,10),\n              \"max_features\": np.arange(1,10)\n             }\n\n# Run randomized search, print out results, and store the best parameters\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom time import time\nn_iter_search = 1000\nrandom_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search, cv=5)\nstart = time()\nrandom_search.fit(train_x, train_y)\nprint(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n      \" parameter settings.\" % ((time() - start), n_iter_search))\nreport(random_search.cv_results_)","c8d20974":"# We train the Decision Tree again with the best parameters.\nclf = tree.DecisionTreeClassifier(min_samples_split = 8,\n                                  min_samples_leaf = 1,\n                                  max_depth = 6,\n                                  max_features = 9\n                                 )\nclf = clf.fit(train_x, train_y)\npred = clf.predict(test_x)\n# The probability of each class is predicted, which is the fraction of training samples of the same class in a leaf\nprob = clf.predict_proba(test_x)","7e5f1f4a":"# Calculating ROC AUC and plotting the ROC curve, \n# Calculate the fpr and tpr for all thresholds of the classification\n# Ironically we get a slightly worse ROC AUC after Hyperparameter tuning.\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfpr, tpr, threshold = roc_curve(test_y, prob[:,1])\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend([round(roc_auc_score(test_y, prob[:,1]),2)]);\nplt.show()","a40bbf28":"# We start by looking at our tree model to see if it makes intuitive sense; which features are used in decisions.\nimport graphviz \ndot_data = tree.export_graphviz(clf, out_file=None,\n                                feature_names=train_x.columns.values[0:],\n                                class_names=[\"Non-Churn\",\"Churn\"],\n                                filled=True, rounded=True,\n                                special_characters=True)  \ngraph = graphviz.Source(dot_data)\ngraph","392cecf2":"# We can also look at the importance of each feature in the classifier.\n# The deeper our tree, the more features are used.\nplt.figure(figsize=(10,10))\nplt.barh(train_x.columns,clf.feature_importances_)\nplt.xticks(rotation=90)\nplt.show()","590bbf7f":"# We can also look at the \"confusion matrix\", which shows us the labeling made by our model for all the test customers.\n# The \"Confusion matrix\" is the source of the business case we build for operationalizing the model, using it to contact customers.\n# We also print out standard evaluation metrics for our model.\n# from sklearn documentation\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n    \ncnf_matrix = confusion_matrix(test_y, pred)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nclass_names = ['Not churned','churned']\n\nplot_confusion_matrix(cnf_matrix, classes=class_names,\n                      title='Confusion matrix, without normalization')\n\n# Plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()\n\nfrom sklearn.metrics import classification_report\neval_metrics = classification_report(test_y, pred, target_names=class_names)\nprint(eval_metrics)","7124fb1f":"# We compute the the gain curve which tells us what %y of Churners we correctly identify after inspecting %x of customers.\nresults = pd.DataFrame({'y': test_y[\"Churn\"], 'y_proba': prob[:,1]})\nresults = results.sort_values(by='y_proba', ascending=False).reset_index(drop=True)\nresults.index = results.index + 1\nresults.index = results.index \/ len(results.index) * 100\n\nsns.set_style('darkgrid')\nresults['Gains Curve'] = results.y.cumsum() \/ results.y.sum() * 100\nresults['Baseline'] = results.index\nbase_rate = test_y[\"Churn\"].sum() \/ len(test_y) * 100\nresults[['Gains Curve', 'Baseline']].plot(style=['-', '--', '--'])\npd.Series(data=[0, 100, 100], index=[0, base_rate, 100]).plot(style='--')\nplt.title('Cumulative Gains')\nplt.xlabel('% of Customers inspected')\nplt.ylabel(\"% of Churners identified\")\nplt.legend(['Gains Curve', 'Baseline', 'Ideal']);","f8ea207d":"# We can also plot the lift itself.\n# We see that we don't manage much more than 2.5, which is rather poor.\n# Some strange behavior around 0, not sure why... Otherwise looks good.\nsns.set_style('darkgrid')\nresults['Lift Curve']=(results.y.cumsum() \/ results.y.sum() * 100)\/results.index\n\nresults['Gains Curve'] = results.y.cumsum() \/ results.y.sum() * 100\nresults['Baseline'] = results.index\n\nresults[['Lift Curve']].plot(style=['-', '--', '--'])\nplt.title('Lift chart')\nplt.xlabel('% of Customers inspected')\nplt.ylabel(\"Lift\")\nplt.legend(['Lift Curve']);","e19e8332":"# Finally, getting ready for using the prediction in the real world. \n# We want to perform an action towards the customers that our model predicts as the most likely to churn.\n# If we select for example the top 10% of Churners that our model predicts we get a new confusion matrix for the business case for operationalization.\n\n# Creating a list [predicted churn probability, predicted churn 1\/0, actual churn] which we then sort by predicted churn probability\nlist = []\nfor i in range(len(test_y)):\n    list.append([prob[i][1],pred[i],test_y.iloc[i,0]])\nlist.sort(reverse=True)\n\n# Keeping the top x customers, setting predicted churn to 0 by hand for the remaining customers\n# E.g. we create a threshold for \"label = true\" by picking top x%\n# Note that this % must give a number larger than the number of guesses that have probability 0.5\n\nx = 0.10\ntop = []\nfor i in range(len(list)):\n    if i < len(test_y)*x:\n        top.append(list[i])\n    else:\n        top.append([list[i][0],0,list[i][2]]) #setting predicted churn to 0\n        \n# Creating our input lists for the function to plot confusion matrix\ntest_y_top = []\npred_top  = []\nfor i in range(len(top)):\n    test_y_top.append(top[i][2])\n    pred_top.append((top[i][1]))\n\n# Number of customers we guess churn and thus will contact\nsum(pred_top)","ba3d36c6":"# In the confusion matrix, we see that when we pick the top 10% of customers to contact, we guess 70% right, vs 25% by guessing randomly.\ncnf_matrix = confusion_matrix(test_y_top, pred_top)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nclass_names = ['Not churned','churned']\n\nplot_confusion_matrix(cnf_matrix, classes=class_names,\n                      title='Confusion matrix, without normalization')\n\n# Plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()\n\nfrom sklearn.metrics import classification_report\neval_metrics = classification_report(test_y_top, pred_top, target_names=class_names)\nprint(eval_metrics)","acd44e30":"## Experiment 2: Train & evaluate decision tree with hyperparameter search","91873ff1":"# Part 3: Model development\n### Goal: Train a machine learning algorithm ready to be evaluated\n### NB: If we had historical data so that each row had a customer number and a time stamp, and the target was \"event within time window\", we would have to ensure that our split is on customer number so that the same customer does not appear in test and training sets. If this split is not done, the same customer with virtually identical feature values and the same target value (churn yes\/no) would appear in both test and training set, resulting in overfitting.","4d832089":"## Experiment 1: Train & evaluate decision tree w\/o hyperparameter search\n### Benefit of starting with a simple decision tree is to have a transparent model, we can look at the decision made at each node.","df85f935":"# Part 4: Evaluation\n### Goal: Evaluate our model and plan for operationalization.","342935d9":"# Part 2: Feature selection & engineering\n\n### Goal: Understand and improve our features. Due to the simplicity of the data set in this exercise we do not actually perform any feature selection or engineering, we simply inspect our features.","84c51531":"### For more advanced feature selection see https:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html","d3e259a6":"# Part 1: Data prep\n### Goal: Clean up our data set so that we can use it to train a machine learning model."}}