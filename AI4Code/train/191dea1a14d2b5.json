{"cell_type":{"d27a6136":"code","15028663":"code","82927fee":"code","b1779e45":"code","a8733963":"code","5ad314da":"code","8579bc13":"code","2ea063f1":"code","a12110d2":"code","6e74ccd5":"code","ed76f9c7":"code","09c74b9d":"code","cd3720e2":"code","0469cc6b":"code","7ffee0f6":"code","2a503008":"code","6c89b5ad":"code","baf7d663":"code","088eac58":"code","446de2dd":"code","1aa26486":"code","85b2241c":"code","6a1f75d7":"code","594360eb":"code","331b3585":"code","b3e92b97":"code","8deb4a5e":"code","d7589a5f":"code","e991c8f3":"code","efae2cab":"code","22ece9f7":"code","3558ce2a":"code","5cf0a717":"code","ccdd0a02":"code","7ed6235e":"code","cb58d3b9":"code","5f2b78fb":"code","718c68a6":"code","9880a4f2":"code","8cb601ab":"code","545c1f41":"code","432dffbd":"code","a9f15500":"markdown","92a3583e":"markdown","1922f0f2":"markdown","aff07fc3":"markdown","1668547f":"markdown","91c95b06":"markdown","2f942b1b":"markdown","49c650c2":"markdown","bd7a4dc3":"markdown","fcfd636d":"markdown","2ea97451":"markdown","4ebdd1d1":"markdown","33ef63c3":"markdown","c67fe841":"markdown","dd9d60c8":"markdown","d621eb5c":"markdown","b91d22d4":"markdown","1ba6a241":"markdown","43595f00":"markdown","6ce1476a":"markdown","b2db6036":"markdown","fce99252":"markdown","2f2a4d82":"markdown","e6ba5fd4":"markdown","a8b0e3f0":"markdown","e336e456":"markdown","87fd7d11":"markdown","ac1b4cbc":"markdown","27bcc3f8":"markdown","1e468c27":"markdown","c69da03c":"markdown","36526151":"markdown","5d1892fe":"markdown","6ef49e65":"markdown","d0598934":"markdown","9505140d":"markdown","cee1328d":"markdown","2340bac5":"markdown"},"source":{"d27a6136":"!pip install PyPDF2","15028663":"import PyPDF2\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","82927fee":"filename = '..\/input\/manifesto\/manibook'\nopen_filename = open(filename, 'rb')\n\nind_manifesto = PyPDF2.PdfFileReader(open_filename)\n","b1779e45":"ind_manifesto.getDocumentInfo()","a8733963":"total_pages = ind_manifesto.numPages\ntotal_pages","5ad314da":"!pip install textract","8579bc13":"import textract   ","2ea063f1":"count = 0\ntext  = ''\n\n# Lets loop through, to read each page from the pdf file\nwhile(count < total_pages):\n    # Get the specified number of pages in the document\n    mani_page  = ind_manifesto.getPage(count)\n    # Process the next page\n    count += 1\n    # Extract the text from the page\n    text += mani_page.extractText()\n   ","a12110d2":"if text != '':\n    text = text\n    \nelse:\n    textract.process(open_filename, method='tesseract', encoding='utf-8', langauge='eng' )    ","6e74ccd5":"!pip install autocorrect","ed76f9c7":"from autocorrect import Speller\nfrom nltk.tokenize import word_tokenize\n\n\ndef to_lower(text):\n\n    \"\"\"\n    Converting text to lower case as in, converting \"Hello\" to  \"hello\" or \"HELLO\" to \"hello\".\n    \"\"\"\n    \n    # Specll check the words\n    spell  = Speller(lang='en')\n    \n    texts = spell(text)\n    \n    return ' '.join([w.lower() for w in word_tokenize(text)])\n\nlower_case = to_lower(text)\nprint(lower_case)","09c74b9d":"import nltk\nimport re\nimport string\nfrom nltk.corpus import stopwords, brown\nfrom nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom autocorrect import spell","cd3720e2":"def clean_text(lower_case):\n    # split text phrases into words\n    words  = nltk.word_tokenize(lower_case)\n    \n    \n    # Create a list of all the punctuations we wish to remove\n    punctuations = ['.', ',', '\/', '!', '?', ';', ':', '(',')', '[',']', '-', '_', '%']\n    \n    # Remove all the special characters\n    punctuations = re.sub(r'\\W', ' ', str(lower_case))\n    \n    # Initialize the stopwords variable, which is a list of words ('and', 'the', 'i', 'yourself', 'is') that do not hold much values as key words\n    stop_words  = stopwords.words('english')\n    \n    # Getting rid of all the words that contain numbers in them\n    w_num = re.sub('\\w*\\d\\w*', '', lower_case).strip()\n    \n    # remove all single characters\n    lower_case = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', lower_case)\n    \n    # Substituting multiple spaces with single space\n    lower_case = re.sub(r'\\s+', ' ', lower_case, flags=re.I)\n    \n    # Removing prefixed 'b'\n    lower_case = re.sub(r'^b\\s+', '', lower_case)\n    \n    \n    \n    # Removing non-english characters\n    lower_case = re.sub(r'^b\\s+', '', lower_case)\n    \n    # Return keywords which are not in stop words \n    keywords = [word for word in words if not word in stop_words  and word in punctuations and  word in w_num]\n    \n    return keywords","0469cc6b":"# Lemmatize the words\nwordnet_lemmatizer = WordNetLemmatizer()\n\nlemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in clean_text(lower_case)]\n\n# lets print out the output from our function above and see how the data looks like\nclean_data = ' '.join(lemmatized_word)\nprint(clean_data)","7ffee0f6":"import pandas as pd\n","2a503008":"df = pd.DataFrame([clean_data])\ndf.columns = ['script']\ndf.index = ['Itula']\ndf","6c89b5ad":"#  Counting the occurrences of tokens and building a sparse matrix of documents x tokens.\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\ncorpus = df.script\nvect = CountVectorizer(stop_words='english')\n\n# Transforms the data into a bag of words\ndata_vect = vect.fit_transform(corpus)","baf7d663":"feature_names = vect.get_feature_names()\ndata_vect_feat = pd.DataFrame(data_vect.toarray(), columns=feature_names)\ndata_vect_feat.index = df.index\ndata_vect_feat","088eac58":"data = data_vect_feat.transpose()\ndata","446de2dd":"import matplotlib.pyplot as plt\nimport seaborn as sn\n\n# Find the top 1000 words written in the manifesto\ntop_dict = {}\nfor c in data.columns:\n    top = data[c].sort_values(ascending=False)\n    top_dict[c]= list(zip(top.index, top.values))\n\n    \nfor x in list(top_dict)[0:100]:\n    print(\"key {}, value {} \".format(x,  top_dict[x]))\n    ","1aa26486":"# Look at the most common top words --> add them to the stop word list\nfrom collections import Counter\n\n# Let's first pull out the top 100 words for each comedian\nwords = []\nfor president in data:\n    top = [word for (word, count) in top_dict[president]]\n    for t in top:\n        words.append(t)\n\nprint(words[:10])","85b2241c":"from wordcloud import WordCloud, STOPWORDS\nimport imageio\nimport matplotlib.pyplot as plt\nimport nltk\n\n# Image used in which our world cloud output will be\nimg1 = imageio.imread(\"..\/input\/manifesto\/itula.jpeg\")\nhcmask1 = img1\n\n# Get 100 words based on the \nwords_except_stop_dist = nltk.FreqDist(w for w in words[:100]) \nwordcloud = WordCloud(stopwords=set(STOPWORDS),background_color='black',mask=hcmask1).generate(\" \".join(words_except_stop_dist))\nplt.imshow(wordcloud,interpolation = 'bilinear')\nfig=plt.gcf()\nfig.set_size_inches(10,12)\nplt.axis('off')\nplt.title(\"Top most common 100 words from Dr. Itula's Manifesto 2019\",fontsize=20)\nplt.tight_layout(pad=0)\nplt.savefig('Manifesto_top_100.jpeg')","6a1f75d7":"!pip install vaderSentiment","594360eb":"from collections import defaultdict\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nfrom textblob import TextBlob","331b3585":"blob = TextBlob(clean_data)\nblob.sentiment","b3e92b97":"# Importing all the necessary libraries\nfrom nltk.cluster.util import cosine_distance\nfrom nltk.tokenize import sent_tokenize\nimport numpy as np\nimport networkx as nx\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","8deb4a5e":"def sentence_similarity(sent1, sent2, stopwords=None):\n    if stopwords is None:\n        stopwords = []\n \n    sent1 = [w.lower() for w in sent1]\n    sent2 = [w.lower() for w in sent2]\n \n    all_words = list(set(sent1 + sent2))\n \n    vector1 = [0] * len(all_words)\n    vector2 = [0] * len(all_words)\n \n    # build the vector for the first sentence\n    for w in sent1:\n        if w in stopwords:\n            continue\n        vector1[all_words.index(w)] += 1\n \n    # build the vector for the second sentence\n    for w in sent2:\n        if w in stopwords:\n            continue\n        vector2[all_words.index(w)] += 1\n \n    return 1 - cosine_distance(vector1, vector2)\n \n# One out of 5 words differ => 0.8 similarity\nprint(sentence_similarity(\"This is a good sentence\".split(), \"This is a bad sentence\".split()))\n \n# One out of 2 non-stop words differ => 0.5 similarity\nprint(sentence_similarity(\"This is a good sentence\".split(), \"This is a bad sentence\".split(), stopwords.words('english')))\n \n# 0 out of 2 non-stop words differ => 1 similarity (identical sentences)\nprint(sentence_similarity(\"This is a good sentence\".split(), \"This is a good sentence\".split(), stopwords.words('english')))\n \n# Completely different sentences=> 0.0\nprint(sentence_similarity(\"This is a good sentence\".split(), \"I want to go to the market\".split(), stopwords.words('english')))\n","d7589a5f":"#print(sentences)\n\n# get the english list of stopwords\n#stop_words = stopwords.words('english')\n \ndef build_similarity_matrix(lower_case, stopwords=None):\n    # Create an empty similarity matrix\n    S = np.zeros([len(lower_case), len(lower_case)])\n \n \n    for idx1 in range(len(lower_case)):\n        for idx2 in range(len(lower_case)):\n            if idx1 == idx2:\n                continue\n \n            S[idx1][idx2] = sentence_similarity(lower_case[idx1], lower_case[idx2], stop_words)\n \n    # normalize the matrix row-wise\n    for idx in range(len(S)):\n        S[idx] \/= S[idx].sum()\n \n    return S","e991c8f3":"#len(sentences)\n#S = build_similarity_matrix(sentences, stop_words)    \n#S","efae2cab":"def generate_summary(lower_case, top_n=5):\n    # Remove all the stopwords in the document\n    stop_words = stopwords.words('english')\n    summarize_text = []\n    \n    \n    \n    #Read text and tokenize\n    #lower_case  = nltk.word_tokenize(lower_case)\n    \n   \n    \n    #Generate similarity matrix across sentences\n    sentence_similarity  = build_similarity_matrix((lower_case, stop_words))\n    \n    #Rank sentences in similarity matrix\n    sentence_similiraty_graph = nx.from_numpy_array(sentence_similarity)\n    scores = nx.pagerank(sentence_similiraty_graph)\n    \n    \n    #Sort the rank and pick top sentences\n    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(lower_case)), reverse=True)    \n    print(\"Indexes of top ranked_sentence order are \", ranked_sentence) \n    \n    for i in range(top_n):\n        summarize_text.append(' '.join(ranked_sentence[i][1]))\n        \n    #Output the summarized text\n    print('Summarized Text: \\n', '. '.join(summarize_text))\n       ","22ece9f7":"#generate_summary(lower_case, 2)","3558ce2a":"# imports\nfrom gensim.summarization.summarizer import summarize","5cf0a717":"# Print out our summarized text of the document which was converted to lower case, remember we could have opted to remove stopwords as well.\n\nprint(summarize(lower_case))","ccdd0a02":"# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\n# Plotting tools\nimport pyLDAvis\n#import graphlab as gl\n#import pyLDAvis.graphlab\nimport pyLDAvis.gensim  # don't skip this\n\n# Enable logging for gensim - optional\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)","7ed6235e":"data  = []\ndata.append(clean_text(lower_case))","cb58d3b9":"# This time we use spacy for lemmatizarion \nimport spacy\n\n# Second lemmatization of our data\ndef lemmatization(data, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_output = []\n    for sent in data:\n        doc = nlp(\" \".join(sent)) \n        texts_output.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_output\n\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# Lemmatize keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])","5f2b78fb":"# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n\n# View\nprint(corpus[:1])","718c68a6":"# LDA model\n\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, alpha='auto', num_topics=20, random_state=100,\n                                           update_every=1, passes=20, per_word_topics=True)\n","9880a4f2":"# Lets view the topics in our model\nprint(lda_model.print_topics())\ndoc_lda  = lda_model[corpus]","8cb601ab":"# Print model perplexity\nprint('\\nPerplexity:', lda_model.log_perplexity(corpus))\n\n\n# Coherence Score\n\ncoherence_model_lda = CoherenceModel(lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('Coherence Score:', coherence_lda)","545c1f41":"pyLDAvis.enable_notebook()\nvis_topics = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)","432dffbd":"vis_topics","a9f15500":"Lets define a function to genrate our summary of the whole document. Also note we will be calling other helper function to keep our summarization pipeline going.","92a3583e":"The `if` statement check if our document returned words from the loop above using the `extractText()` function. This is done since `PyPDF2` cannot read scanned documents. ","1922f0f2":"We can see that the polarity is **0.07** which means that the document is **neutral** and **0.47** subjectivity refers almost factual information in the document rather than public opinions, beliefs and so forth.","aff07fc3":"### **4. Clean our *to_lower_case* text variable and return it as a list of keywords.**\n\nFrom the printed text, it's apparent that our text contains unwanted characters such as spaces, punctuations `\\n` and so forth. \n\nLets break our text phrases into individual words using `word_tokenize()` function from the Naturalge Toolkit (nltk).","1668547f":"Loop throug all the pages in the document and extract the text from it ","91c95b06":"To get the document informtion  ussing the `getDocumentInfo()` function and check the number of pages in our document using the `numPages()` function. There are various useful functions one can use to check other things. See online documentation:[PyPDF2](https:\/\/pythonhosted.org\/PyPDF2\/index.html)","2f942b1b":"###  **5. Preprocess - Bag of Words model** \n\nThe bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision [Wikipedia](https:\/\/stackabuse.com\/python-for-nlp-sentiment-analysis-with-scikit-learn\/).\n\n\nIt is mostly used to extract features from text for used in modelling, such as machine learning algorithms.","49c650c2":"### 9.1 Building the topic model with LDA\n\n\nApart from that, `alpha` and `beta` are hyperparameters that affect sparsity of the topics. Both with default values to 1.0\/num_topics prior.\n\n`alpha` is the per-document topic distribution, in simple terms it is a matrix where each row is a document and each column is a topic.\n`beta` is the per-topic word distribution, also in simple terms it is a matrix where each row represents a topic and each column represents a word. \n\n\nAlso see online documentation for `gensim.gensim.models.ldamodel.LdaModel()` parameters","bd7a4dc3":"How do we make sense of this?\n\nFor topic `0` which is intrepreted as `'0.001*\"need\" + 0.001*\"people\" + 0.001*\"namibian\" + 0.001*\"government\" + 0.001*\"citizen\" + 0.001*\"youth\" + 0.001*\"benefit\" + 0.001*\"system\" + 0.001*\"provide\" + 0.001*\"make\"'`\n\n**What does it mean?**  it means that the top 10 keywords that are part of this topic are: `need`, `people`, `namibian`, `government`, `citizen`, `youth`, `benefit`, `system`, `provide` and `make`. The numbers before the words represent the weight of the specific word on that topic, example `need` in topic `0` weigh `0.001` and this is much for all the words top 10 words in topic `0`. \n\n\nCan we deduced what topic this could be by looking on the top 10 keyswords? like what topic could trigger one to talk about `need`, `people`, and so on? We can perhaps summarize it to **POLITICS - Namibian People**\n\nThis can be done for all the remaining topics to see wether we can come up with a close judgement of each topic.\n\n\n![titile](https:\/\/www.machinelearningplus.com\/wp-content\/uploads\/2018\/03\/Inferring-Topic-from-Keywords.png)","fcfd636d":"## **9. Topic modeling using LDA** \n\nTopic modeling can be seen as a type of statistical modeling for discovering the abstract 'topics' that are presented in a myriad of documents (it can be a single document). A topic  is considered as a collection of prevalent keywords that are typical representatives. Its through keywords in which one determines what the topic is all about.\n\n### What is LDA?\n$\\rightarrow$ Latent Dirichlet Allocation(LDA) is a popular algorithm for topic modeling with excellent implementations in the Python\u2019s `gensim`package. We will therefore use LDA  to classify text in our document to a particular topic. I works by building a topic per document model and words per topic model, from Dirichlet distribution models in statistics.","2ea97451":"## **8. Text Summarization of the Document**\n\n![title](https:\/\/miro.medium.com\/max\/1200\/1*GIVviyN9Q0cqObcy-q-juQ.png)\n\n\nText summarization is an important NLP task which is a way of producing a concise and fluent summary of a perticular textin an article, journal, book, comment review, etc while also preseving the key information and overall meaning. Its is divided into categories, i.e., **extraction** and **abstraction**. Extractive methods select a subset of existing words, phrases, or sentences in the original text to form a summary. In contrast, abstractive methods first build an internal semantic representation and then use natural language generation techniques to create a summary.\n\n\nBuild a similarity matrix $\\rightarrow$  generate rank based on matrix  $\\rightarrow$ pick top N sentences for summary.","4ebdd1d1":"From the outputs of our two previous codes, we got the **title** of the document, what OS was used to type the document, when the document was created and modified. And we also got the total number of pages in our document. ","33ef63c3":"Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of word id and word frequency in the document, i.e. [0,1] word_id 0 means the word id appers first in the document and word frequency it appears once in the document. For Human readable formart see `cell [87-89]` or else run the following code `[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]`.  ","c67fe841":"Applt the document-term matrix which is a mathematical matrix which decribes the frequency of words in a document","dd9d60c8":"Our vector representations show the frequency of words used in the document","d621eb5c":"Lets decide  which model we should use, between **TextBlob** and **VADER** for analysis of our text. We will therefore use TextBlob for its simplcity, and since VADER is specifically for analysis of social media data.  \n\n#### **7.1 TextBlob function - returns two properties**\n\n**Polarity:** a float value which ranges from [-1.0 to 1.0] where 0 indicates neutral, +1 indicates most positive statement and -1 rindicates  most negative statement.\n\n**Subjectivity:** a float value which ranges from [0.0 to 1.0] where 0.0 is most objective while 1.0 is most subjective. Subjective sentence expresses some personal opinios, views, beliefs, emotions, allegations, desires, beliefs, suspicions, and speculations where as objective refers to factual information.","b91d22d4":"### **10. Conclusion**\n\n\n* We have succefully analysed the document eventhough there are many models that can be use to throughly get an idea of the whole document.\n\n* The data cleaning process had issues in which there we characters that were not english and thus have tempered with our analysis, this can be improved.\n\n* Getting the top 100 most common words resonate well with me on a personal level looking at the state in which Namibia is and what that is needed to be done. This could be because of the word **youth** since I happen fall under that group, haha :-D\n\n* For sentiment analysis, we found the document to be neutral with almost factual information in the document rather than public opinions, beliefs and so forth.\n\n* The summarization of the document was quite challenging using the TextRank algorimth together with the implementaion of the similarity matrix, however it was easily done with the `gensim` package in one line of code, CAN YOU IMAGINE? Me neither :-;\n\n* Topic modelling  was successful, in which we could say that the document is focused on Namibian politics. \n\n**Hope you enjoyed reading this. I would appreciate if you leave your thoughts in the comments section. And dont forget to upvote if you liked it**","1ba6a241":"### **3. Lets extract the texts from the pdf file and print it**\n\nWe will use a `textract` package to extract our texts from the document.","43595f00":"* The coherance score is 0.27 and perpelexity is -7.88","6ce1476a":"Lets save our data into a dataframe so we can do our anal","b2db6036":"### **7. Sentiment Analysis of the Manifesto**\n\nThis is a set of Natural Language Processing (NLP) technique of analysing, identifying and categorizing opinions expressed in a piece of text, in order to determine whether the writer's attitude towards a particular topic, product, politics, services, brands etc. is positive, negative, or neutral. This data holds immense value in the fields of marketing analysis, public relations, product reviews, net promoter scoring, product feedback, and customer service, for example.","fce99252":"### **1. Imports**","2f2a4d82":"Lets create a dictionary and corpus needed for topic modeling which are the two crucial inputs in implementint the LDA topc model","e6ba5fd4":"# **Analysis of Independent Presidential Candidata Dr. Iitula's Manifesto for 2019  Election Campaign**","a8b0e3f0":"### 9.2 Model Perplexity and Coherence Score\n\n\nLets evaluate our model by computing the perplexity and topic coherence. Before that we need to understang what model perplexity and coherence is.\n**Perplexity** is an evaluation  metric on how probable (predictive likelihood) new unseen data is given the model that was learned earlier. **Topic coherence** measures a score on a single topic through measuring the degree of semantic similarity of all the high scoring words in a topic. Both model perplexity and topic score provide a convenient measure to judge how good a given topic model is.  ","e336e456":"### 9.3 Visualize topic's keywords","87fd7d11":"\n\nThis was our summarized text from the document. Note: there are various summarization packages like `TextTeaser`, `PyTeaser` , etc which one can use to accomplish this with only one line of code. ","ac1b4cbc":"And clear from seeing the printed text, we only extracted texts from page 2 till last pace. The reason being is that, those pages that we did not extract text from are in image based and we failed to to do. **SOMEONE CAN PERHAPS HELP!!!!.** ","27bcc3f8":"### **6. Getting the top 100 frequent words from the manifesto.**\n\nWe will try to get the top most common 100 words from our document and plot that into a wordcloud for visualization.","1e468c27":"We will use **unsupervised machine learning** approach to find the sentences similarity and rank them using the **cosine similarity** approach. Cosine simirality measures and calculates the similarity between two non-zero vectors of an inner product space by measuring the cosine angle between them.  One benefit of the **cosine similarity** approach is, there;s no need to train and build a model prior start using it for your project.","c69da03c":"### **2. Read our document**","36526151":"What can we deduce from graph output?\n\nOn the left it shows a buble in which it represnt a topic, also note we are supposed to have 20 bubles since we chose to have 20 topics in our model earlier on. The larger the buble, the more dominant that topic is. \n\n\nTo determine a good topic, the bubble on the left would be big and non-overlapping throught the chart and being in spread along all the quadrants in stead of being clustered. Also note we only have one big bubble which means that we classified our topic 1 very well, however if you look closely you will see a black dot which seem to have all the remaining topic clustered together and overlaping each other in one place.\n\n\n\nIf you move the cursor on the bubble, the words and bars on the right-hand side will update. These words are the salient keywords that form the selected topic, i.e, top 30 most relevant words. Clicking the  `Previous Topic` and `Next Topic`  buttons on the upper left, we again see the words on the right-hand side updating. However, there isn't much significant changes in most of the words in all the topics as they are all dominated by words like `need`, `youth`, `people`, `government`, `country` , `namibian` and so on... We can say \n\nWith the output, we can say that the topics in our document exacerbate around **politics**, **Namibian politics** to be specific. Thus we have successfully build a good looking topic model.","5d1892fe":"## **8.1 Gensim Package**\n\n\nLets summarize our text sing the `gensim` package which is a  module for topic modelling for humans.  Summarizing is based on ranks of text sentences using a variation of the TextRank algorithm. see online documentation [gensim](https:\/\/radimrehurek.com\/gensim\/summarization\/summariser.html) ","6ef49e65":"Create a vectors between two sntences and calculate the cosine angel between them.","d0598934":"Lets read our pdf for the manifesto using the `PdfFileReader()` function from the PyPDF2 which is a package for extracting document information such as **title, author, number of pages,....**, spliting documents page by page, merging page by page, etc. ","9505140d":"Been trying to implement the tetxrank algorithm, however we keep getting an error whenever we call the `generate_summary()` function. Feel free to clarify on the error in order to generate a summary using this algorithm. With that said we found a simplest way to do it using just one line of code as shown in the following section.","cee1328d":"If the above returns a false, then run the Optical Character Recognition (OCR) `textract` to convert scanned\/image based Pdf files to text. See `textract` online documentaion: [textract](https:\/\/textract.readthedocs.io\/en\/stable\/python_package.html).","2340bac5":"Lets print out our texts to see what it contains which was converted to lower case using the `lower()` function."}}