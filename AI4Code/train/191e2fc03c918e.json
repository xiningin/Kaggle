{"cell_type":{"1b38edb9":"code","722903e8":"code","bcb89155":"code","a0d72c2c":"code","5a418dc8":"code","0bdbc707":"code","b7f18da5":"code","12371edf":"markdown","8bfa736d":"markdown","31e80824":"markdown","b53b00b7":"markdown","0536d4de":"markdown"},"source":{"1b38edb9":"from keras.layers import Input, Dense\nfrom keras.models import Model\n\ninput_img = Input(shape=(784,))\nencoded = Dense(32, activation='relu')(input_img)  # encoding_dim = 32\ndecoded = Dense(784, activation='sigmoid')(encoded)\n\n# this model maps an input to its reconstruction\nautoencoder = Model(input_img, decoded)\n\n# get the encoder and decoder as seperate models\n# encoder\nencoder = Model(input_img, encoded)\n\n# decoder\nencoded_input = Input(shape=(32,))  # encoding_dim = 32\ndecoder_layer = autoencoder.layers[-1]\ndecoder = Model(encoded_input, decoder_layer(encoded_input))\n\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')","722903e8":"from keras.datasets import mnist\nimport numpy as np\n(x_train, _), (x_test, _) = mnist.load_data()\nx_train = x_train.astype('float32') \/ 255.\nx_test = x_test.astype('float32') \/ 255.\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))","bcb89155":"autoencoder.fit(x_train, x_train,\n                epochs=50,\n                batch_size=256,\n                validation_data=(x_test, x_test),\n                verbose=1)","a0d72c2c":"encoded_imgs = encoder.predict(x_test)\ndecoded_imgs = decoder.predict(encoded_imgs)","5a418dc8":"import matplotlib.pyplot as plt\n\nn = 10  # how many digits we will display\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","0bdbc707":"from keras import regularizers\n\nencoding_dim = 32\n\ninput_img = Input(shape=(784,))\n# add a Dense layer with a L1 activity regularizer\nencoded = Dense(encoding_dim, activation='relu',\n                activity_regularizer=regularizers.l1(10e-5))(input_img)\ndecoded = Dense(784, activation='sigmoid')(encoded)\n\nautoencoder = Model(input_img, decoded)","b7f18da5":"input_img = Input(shape=(784,))\nencoded = Dense(128, activation='relu')(input_img)\nencoded = Dense(64, activation='relu')(encoded)\nencoded = Dense(32, activation='relu')(encoded)\ndecoded = Dense(64, activation='relu')(encoded)\ndecoded = Dense(128, activation='relu')(decoded)\ndecoded = Dense(784, activation='sigmoid')(decoded)","12371edf":"## Demo\n### Single-layer\n\nThe simplest possible autoencoder is a single hidden layer of $n < \\text{# of input pixels}$ nodes. The output layer of this model mirrors the input layer in size. In the next few code cells we train just such a simple autoencoder and demonstrate its output.","8bfa736d":"### Deep\n\nYou can also use deeper autoencoders. These have all the benefits and tradeoffs you would expect of increasing the number of layers in your neural networks, namely: longer training times and less easily decoded representations, but more accurate reconstructions.\n\nA good default choice for a deep autoencoder is to scale the image down progressively, than scale it back up again, using the same number of nodes on each layer on either side of the \"most compressed\" representation at the center of the hidden layers.","31e80824":"We see here that the decoder trained in our autoencoder learns grainy (lossy) versions of the original images.\n\nThis form of simple one-layer autoencoder learns a representation of the underlying dataset that is very close to what is learned by PCA. You can thus think of this simple autoencoder as being a kind of stochastic approximation of the more deterministic PCA algorithm.","b53b00b7":"### Regularized single-layer\n\nThe idea is that an autoencoder ought to learn a spare representation of interesting features of the dataset, but when using standard convergence techniques more often than not it winds up learning how pixel maps moreso than interesting dataset features. One way to address this problem is to use regularization to introduce sparsity: make it so that only certain nodes fire on certain images, and the remainder stay silent. This way instead of learning \"how to assemble fuzzy patches\" into reconstructions of your inputs, the network will (hopefully) learn \"how to assemble interesting _features_\" into reconstructions of your input.\n\nYou can induce sparsity in your connections using [L1 regularization](https:\/\/www.kaggle.com\/residentmario\/l1-norms-versus-l2-norms). ","0536d4de":"## Autoencoders\n\n**Autoencoders** are a type of neural network which learns a sparse representation of an input. In other words, an autoencoder, once trained on appropriate training data, can be used to generate compressed copies of an input data point which preserve most of the information (features) in the input using significantly fewer bits of information. \n\nTechnically speaking, an autoencoder is any neural network with three components: an encoding function (which translates the data into a simpler space, e.g. a hidden layer with fewer nodes than on the input layer), a decoding function (which reverses this process, e.g. an output layer with equivalently many nodes as the input layer) and a distance metric (which measures loss as some distance between the original input and learned representation). The earliest type of autoencoders are [Restricted Boltzman Machines](https:\/\/www.kaggle.com\/residentmario\/restricted-boltzmann-machines-and-pretraining\/notebook), which are important enough to the history of deep learning that they get their own name cred.\n\nAutoencoders are the neural network equivalent of simpler variable compression techniques, like [PCA](https:\/\/www.kaggle.com\/residentmario\/dimensionality-reduction-and-pca-for-fashion-mnist\/) and [LDA](https:\/\/www.kaggle.com\/residentmario\/linear-discriminant-analysis-with-pokemon-stats). Autoencoders are practically used in deep learning applications for **pretraining**: determining the \"right\" weights for a neural network ahead of time, so that the algorithm doesn't have to work as hard to converge as it would if it had to start with completely random weights. For more on pretraining see [my notebook on RBMs](https:\/\/www.kaggle.com\/residentmario\/restricted-boltzmann-machines-and-pretraining\/notebook). In practice, the emergence of [faster convergence algorithms](https:\/\/www.kaggle.com\/residentmario\/neural-network-activation-functions) has obviated the need and utility for pretraining.\n\nThus autoencoders are no longer used in cutting edge deep learning work. They're also not notably better than simpler (usually information-theoretic) compression algorithms, like JPEG and MG3, in commercial applications. These days they're most used as a preprocessing step on high-dimensionality data before input to T-SNE.\n\nBut they are nevertheless historically and pedologically important.\n\nThis notebook based on [this blog post](https:\/\/blog.keras.io\/building-autoencoders-in-keras.html)."}}