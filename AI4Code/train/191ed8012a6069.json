{"cell_type":{"be2b9a9f":"code","4b1b0141":"code","6800b03c":"code","9750dc57":"code","96b9b1f8":"code","dcd7c963":"code","ff9a2032":"code","20451b57":"code","be8004c6":"code","1e95e021":"code","b8d04b92":"code","7ce66ab2":"code","b34a6f01":"code","00810b1e":"code","168d44e7":"code","5dfdddb6":"code","60c9cf6d":"code","7c92330a":"code","da29a21a":"code","cd9f174e":"code","674286e0":"code","433b5d0f":"code","127b99b6":"code","f504d4c0":"code","acfe6645":"code","cd66b93d":"code","56891f07":"code","b3d0a6d8":"code","226eb993":"code","a33c934e":"code","3eec7c18":"code","89787e6f":"code","866e57d4":"code","97d89062":"code","a7ad6a9e":"code","16979c88":"code","9fc01bbd":"code","ad7c6eb3":"code","91be5154":"code","8288c13e":"code","e3f61080":"code","796eac22":"code","e10e06b0":"code","638862e3":"code","84c9ef2b":"code","822f39d3":"code","77f18d4d":"code","7a37268c":"code","0a2512fb":"code","ba36d873":"code","f0468a6c":"code","6bb0ff28":"markdown","606c0dcf":"markdown","b1f8a0ba":"markdown","daf22326":"markdown","43296713":"markdown"},"source":{"be2b9a9f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4b1b0141":"import numpy as np\nimport pandas as pd\nimport datetime as dt\n\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder , RobustScaler\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dense,LSTM,GRU\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.layers import LeakyReLU , ReLU \nfrom keras.preprocessing.sequence import TimeseriesGenerator\n\nimport gc\nimport random\n\nfrom IPython import display as ipd\nfrom tqdm import tqdm\nimport xgboost as xgb\n\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler, LabelEncoder\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold,GroupKFold\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, f1_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"whitegrid\")\n\nimport optuna \nfrom optuna.visualization.matplotlib import plot_optimization_history\nfrom optuna.visualization.matplotlib import plot_param_importances","6800b03c":"def seeding(SEED, use_tf=False):\n    np.random.seed(SEED)\n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = str(SEED)\n    if use_tf:\n        tf.random.set_seed(SEED)\n    print('seeding done!!!')\n    \n## https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298201\ndef SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)   ","9750dc57":"RANDOM_SEED = 42\nTUNING = False\n\nseeding(RANDOM_SEED)\n\ntrain_path='..\/input\/petrichor-forecasting-given-time-series-problem\/Train_data_forecasting.csv'\nvalid_path= '..\/input\/petrichor-forecasting-given-time-series-problem\/Validation_test_data.csv'\ntest_path= '..\/input\/petrichor-forecasting-given-time-series-problem\/Test_data_forecasting.csv'","96b9b1f8":"df_train=pd.read_csv(train_path)\ndf_train.rename(columns = {'0':'O'}, inplace = True)\n\ndf_valid=pd.read_csv(valid_path)\ndf_valid.rename(columns = {'0':'O'}, inplace = True)\n\ndf_test=pd.read_csv(test_path)\n\ndf_train","dcd7c963":"df_valid","ff9a2032":"df_test","20451b57":"cols = list(df_train.columns)\nnull_cols = []\nfor column in cols:\n    if df_train[column].isnull().sum()!=0:\n        null_cols.append(column)","be8004c6":"for column in null_cols:\n    print(column ,'=' , df_train[column].isnull().sum())","1e95e021":"df_train = df_train.fillna(method='ffill', downcast='infer')\ndf_train.head(5)","b8d04b92":"df_train['Date'] = pd.to_datetime(df_train['Date'],format=\"%d\/%m\/%Y\")","7ce66ab2":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf_train['Time'] = le.fit_transform(df_train['Time'])","b34a6f01":"df_valid['Date'] = pd.to_datetime(df_valid['Date'],format=\"%d\/%m\/%Y\")\ndf_valid['Time'] = le.transform(df_valid['Time'])\n\ndf_test['Date'] = pd.to_datetime(df_test['Date'],format=\"%d\/%m\/%Y\")\ndf_test['Time'] = le.transform(df_test['Time'])","00810b1e":"df_concat = pd.concat([df_train,df_valid],axis=0,ignore_index=True)\n\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\n# # created scaler\n# scaler = MinMaxScaler()\n# # fit scaler on training dataset\n# arr = df_concat.iloc[:,2:].values\n# arr = scaler.fit_transform(arr)\n# df_concat.iloc[:,2:] = arr\n\n\ndf_concat = pd.concat([df_concat,df_test],axis=0,ignore_index=True)\ndf_concat['quarter'] = df_concat['Date'].dt.quarter\ndf_concat['month_encode'] = df_concat['Date'].dt.month\ndf_concat['year'] = df_concat['Date'].dt.year\ndf_concat","168d44e7":"category_cols = df_concat.columns.values[2:-3]\ncategory_cols","5dfdddb6":"df_organised = pd.DataFrame(columns=['Date','Time','category','quarter','month_encode','year','value'])\n\nDate=[]\nTime=[]\ncategory=[]\nquarter=[]\nmonth_encode=[]\nyear=[]\nvalue=[]\n\n\nfor i in range(17):\n    \n    for j in range(8208):\n        \n        Date.append(df_concat.iloc[j,0])\n        Time.append(df_concat.iloc[j,1])\n        category.append(category_cols[i])\n        quarter.append(df_concat.iloc[j,-3])\n        month_encode.append(df_concat.iloc[j,-2])\n        year.append(df_concat.iloc[j,-1])\n        value.append(df_concat.loc[j,category_cols[i]])    \n\ndf_organised['Date']= Date\ndf_organised['Time'] =  Time\ndf_organised['category']= category\ndf_organised['quarter']= quarter\ndf_organised['month_encode']= month_encode\ndf_organised['year']= year\ndf_organised['value']= value\n\ndf_organised","60c9cf6d":"import math","7c92330a":"for k in [1, 2, 3, 12]:\n    df_organised[f'sin{k}'] = np.sin(df_organised['year'] \/ 365 * 2 * math.pi * k)\n    df_organised[f'cos{k}'] = np.cos(df_organised['year'] \/ 365 * 2 * math.pi * k)   ","da29a21a":"df_organised['Date'] = pd.to_datetime(df_organised['Date'],format=\"%Y\/%m\/%d\")\n\ndf_train = df_organised[df_organised['Date']<'2005-02-17'] \n\ndf_valid = df_organised[(df_organised['Date']>='2005-02-17') & (df_organised['Date']<='2005-02-26')] \ndf_valid = df_valid.reset_index()\ndf_valid = df_valid.drop('index', axis=1)\n\ndf_test = df_organised[df_organised['Date']>'2005-02-26'] \ndf_test = df_test.reset_index()\ndf_test = df_test.drop('index', axis=1)\n\nle_category = LabelEncoder()\ndf_train['category'] = le_category.fit_transform(df_train['category'])\ndf_valid['category'] = le_category.transform(df_valid['category'])\ndf_test['category'] = le_category.transform(df_test['category'])","cd9f174e":"df_train","674286e0":"df_valid","433b5d0f":"df_test","127b99b6":"train = pd.concat([df_train,df_valid],axis=0,ignore_index=True)\n\ntarget = train['value']\ntrain.drop(['value','Date'], axis=1, inplace=True )\ntest = df_test.drop(['value','Date'], axis=1 )","f504d4c0":"# NUM_BOOST_ROUND = 1000\n# EARLY_STOPPING_ROUNDS = 20\n# VERBOSE_EVAL = 100\n    \n# def objective(trial, X, y):\n    \n#     param_grid = {\n#         'verbosity': 1,\n#         'objective': 'reg:squarederror', \n#         'eval_metric': 'rmse',\n#         'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.1),\n#         'eta': trial.suggest_float('eta', 0.1, 0.9),\n#         'max_depth': trial.suggest_int('max_depth', 50, 500),     \n#         'min_child_weight': trial.suggest_float('min_child_weight', 10, 100),\n#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n#         'gamma': trial.suggest_float('gamma', 0, 100),\n#         'subsample': trial.suggest_float('subsample', 0.5, 0.9),\n#         'lambda': trial.suggest_float('lambda', 1, 10),\n#         'alpha': trial.suggest_float('alpha', 0, 9),\n#     }    \n        \n#     X_train, X_valid, y_train, y_valid = train_test_split( X, y, test_size=0.25, random_state=RANDOM_SEED, shuffle=False)\n    \n#     dtrain = xgb.DMatrix(X_train, label=y_train)\n#     dvalid = xgb.DMatrix(X_valid, label=y_valid)\n#     model = xgb.train( param_grid, dtrain,\n#         num_boost_round = NUM_BOOST_ROUND,\n#         evals=[(dvalid, 'evals')], \n#         verbose_eval = VERBOSE_EVAL,\n#         early_stopping_rounds=EARLY_STOPPING_ROUNDS\n#     )   \n    \n#     oof_pred = model.predict(dvalid)\n#     oof_score = SMAPE(y_valid, oof_pred) \n#     print(f\"OOF SMAPE: {oof_score}\")\n#     return oof_score","acfe6645":"# N_TRIALS = 100\n\n# if TUNING:\n#     study = optuna.create_study(direction='minimize')\n#     objective_func = lambda trial: objective(trial, train, target)\n#     study.optimize(objective_func, n_trials=N_TRIALS)  # number of iterations\n\n#     print(\"Number of finished trials: {}\".format(len(study.trials)))\n#     print(\"Best trial:\")\n#     trial = study.best_trial\n#     print(\"  Value: {}\".format(trial.value))\n#     print(\"  Params: \")\n#     for key, value in trial.params.items():\n#         print(\"    {}: {}\".format(key, value))","cd66b93d":"# def run_train(X, y, run_params, splits, num_boost_round, verbose_eval, early_stopping_rounds ):\n#     scores = []\n#     models = []\n#     folds = StratifiedKFold(n_splits=splits)\n#     for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n#         print(f'Fold {fold_n+1} started')\n#         X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n#         y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        \n#         dtrain = xgb.DMatrix(X_train, label=y_train)\n#         dvalid = xgb.DMatrix(X_valid, label=y_valid)\n#         model = xgb.train( run_params, dtrain,\n#             num_boost_round = num_boost_round,\n#             evals=[(dvalid, 'evals')], \n#             verbose_eval = verbose_eval,\n#             early_stopping_rounds=early_stopping_rounds\n#         )   \n\n#         oof_pred = model.predict(dvalid)\n#         oof_score = SMAPE(y_valid, oof_pred) \n#         print(f\"OOF SMAPE: {oof_score}\")        \n        \n#         models.append(model)\n#         scores.append(oof_score)\n#     return scores, models\n\n\n# NUM_BOOST_ROUND = 2000\n# EARLY_STOPPING_ROUNDS = 100\n# VERBOSE_EVAL = 100\n# TOTAL_SPLITS = 5\n    \n# run_params = {\n#     'verbosity': 1,\n#     'objective': 'reg:squarederror', \n#     'eval_metric': 'rmse',\n#     'learning_rate': 0.01729433116660487,\n#     'eta': 0.4954283685809021,\n#     'max_depth': 476,\n#     'min_child_weight': 12.875223150484498,\n#     'colsample_bytree': 0.7890238951483045,\n#     'gamma': 96.89423371529557,\n#     'subsample': 0.8862703289885544,\n#     'lambda': 8.869246442053491,\n#     'alpha': 4.132837689865073,\n# }\n\n# FEATURES = [col for col in train.columns if col.endswith('enc')]\n# scores, models = run_train(train, target, run_params, TOTAL_SPLITS, NUM_BOOST_ROUND, \n#                                           VERBOSE_EVAL, EARLY_STOPPING_ROUNDS)\n\n# print('----------------------')\n# print(f'CV SMAPE mean score: {np.mean(scores)}, std: {np.std(scores)}.')\n# print('----------------------')","56891f07":"# y_pred = np.zeros(len(test))\n# for model in models:\n#     y_pred += model.predict(xgb.DMatrix(test)).reshape(-1)\n\n    \n# y_pred = y_pred \/ len(models)","b3d0a6d8":"# test['num_sold'] = np.round(y_pred).astype(int)\n# test.to_csv('submission_optuna_xgb.csv', index=False, float_format='%.6f')\n# test.head(20)","226eb993":"import lightgbm as lgb\ndef run_train(X, y, run_params, splits, num_boost_round, verbose_eval, early_stopping_rounds ):\n    scores = []\n    models = []\n    eval_results = {}  # to record eval results for plotting\n    folds = StratifiedKFold(n_splits=splits)\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print(f'Fold {fold_n+1} started')\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        model = lgb.train(\n            run_params, valid_names=[\"train\", \"valid\"], \n            train_set=lgb.Dataset(X_train, y_train ), \n            num_boost_round = num_boost_round,\n            valid_sets = [lgb.Dataset(X_valid, y_valid)],\n            callbacks=[lgb.log_evaluation(verbose_eval), \n               lgb.early_stopping(early_stopping_rounds, False, True),\n               lgb.record_evaluation(eval_result=eval_results)],\n        )\n\n        y_predicted = model.predict(X_valid)\n        score = SMAPE(y_valid, y_predicted)   \n        print(f'SMAPE: {score}')\n\n        models.append(model)\n        scores.append(score)\n    return scores, models, eval_results\n\n\nLEARNING_RATE = 0.075\nMAX_DEPTH = 20\nNUM_LEAVES = 350  \nTOTAL_SPLITS = 5\nNUM_BOOST_ROUND = 2000\nEARLY_STOPPING_ROUNDS = 50\nVERBOSE_EVAL = 100\n    \nrun_params = {\n    'verbose': -1, \n    'boosting_type': 'gbdt', \n    'objective': 'regression', \n    'metric': ['rmse'],\n    'learning_rate': LEARNING_RATE, \n    'num_leaves': NUM_LEAVES, \n    'max_depth': MAX_DEPTH, \n    'min_data_in_leaf': 200,\n    'max_bin': 250,\n    'lambda_l1': 100,\n    'lambda_l2': 95,\n    'feature_fraction': 0.97,\n    'bagging_fraction': 0.99,\n    'bagging_freq': 2,\n    'min_child_samples': 77,\n}\n\nFEATURES = [col for col in train.columns if col.endswith('enc')]\nscores, models, eval_results = run_train(train, target, run_params, TOTAL_SPLITS, NUM_BOOST_ROUND, \n                                          VERBOSE_EVAL, EARLY_STOPPING_ROUNDS)\nprint('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))","a33c934e":"ax = lgb.plot_metric(eval_results, metric='rmse')\nplt.show()","3eec7c18":"train","89787e6f":"test","866e57d4":"y_pred = np.zeros(len(test))\nfor model in models:\n    y_pred += model.predict(test).reshape(-1)\n                                                                        \ny_pred = y_pred \/ len(models)","97d89062":"test['num_sold'] = np.round(y_pred).astype(int)\ntest.to_csv('submission_optuna_lgbm.csv', index=False, float_format='%.6f')\ntest.head(20)","a7ad6a9e":"test","16979c88":"# plt.figure(figsize=(30,30))\n# for x in range(17):\n#   plt.subplot(6,3, x+1)\n#   sns.lineplot(data= df_train.iloc[-240:,2+x])\n#   plt.xticks(rotation=90)\n#   plt.title(x+1)","9fc01bbd":"# plt.figure(figsize=(30,30))\n# for x in range(17):\n#   plt.subplot(6,3, x+1)\n#   sns.lineplot(data= df_valid.iloc[:,2+x])\n#   plt.xticks(rotation=90)\n#   plt.title(x+1)","ad7c6eb3":"# plt.figure(figsize=(30,30))\n# for x in range(17):\n#   plt.subplot(6,3, x+1)\n#   sns.lineplot(data= df_concat.iloc[:,2+x])\n#   plt.xticks(rotation=90)\n#   plt.title(x+1)","91be5154":"from statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n#Ho: It is non stationary\n#H1: It is stationary\n\ndef adfuller_test(sales):\n    result=adfuller(sales)\n    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n    for value,label in zip(result,labels):\n        print(label+' : '+str(value) )\n    if result[1] <= 0.05:\n        print(\"strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data has no unit root and is stationary\")\n    else:\n        print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")","8288c13e":"for i in range(17):\n    adfuller_test(df_concat.iloc[:,2+i])\n    print()\n    print()","e3f61080":"import statsmodels.api as sm\n\ndecomposed_series = sm.tsa.seasonal_decompose(df_concat.iloc[:48,2].values, freq=5)\ndecomposed_series.plot()\nplt.figure(figsize=(60,60))\nplt.show()","796eac22":"# training_data = df_train.drop( ['Date','Time'] , axis=1 )\n# training_data","e10e06b0":"training_data","638862e3":"#LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n# normalize the dataset\nscaler = MinMaxScaler()\nscaler = scaler.fit(training_data)\ntraining_data = scaler.transform(training_data)","84c9ef2b":"length = 3\nbatch = 1\n\nn_features = training_data.shape[1]\nn_features","822f39d3":"generator = TimeseriesGenerator(data = training_data,\n                                targets = training_data,\n                                length = length,\n                                sampling_rate=1,\n                                stride=1,\n                                start_index=0,\n                                end_index=None,\n                                shuffle=False,\n                                reverse=False,\n                                batch_size=batch)","77f18d4d":"#Empty lists to be populated using formatted training data\ntrainX = []\ntrainY = []\n\nn_future = 1   # Number of days we want to look into the future based on the past days.\nn_past = 3 # Number of past days we want to use to predict the future.\n\n#Reformat input data into a shape: (n_samples x timesteps x n_features)\n#In my example, my df_for_training_scaled has a shape (12823, 5)\n#12823 refers to the number of data points and 5 refers to the columns (multi-variables).\nfor i in range(n_past, len(training_data) - n_future +1):\n    trainX.append(training_data[i - n_past:i, 0:training_data.shape[1]])\n    trainY.append(training_data[i + n_future - 1:i + n_future, 0])\n\ntrainX, trainY = np.array(trainX), np.array(trainY)\n\nprint('trainX shape == {}.'.format(trainX.shape))\nprint('trainY shape == {}.'.format(trainY.shape))","7a37268c":"#  define the Autoencoder model\n\n\n# Adding layers and definig the model\n# this model has 5 hidden layers \n\n# Initialisation\n\nearly_stop = EarlyStopping(monitor='val_loss',\n                        min_delta=0,\n                        patience= 5,\n                        verbose=1,  \n                        mode='auto',\n                        baseline=None,  \n                                               \n                        restore_best_weights=False)\n\nreg=Sequential()\n\n#Layer 1\nreg.add(LSTM(units=700,return_sequences=True,activation='LeakyReLU',input_shape=(trainX.shape[1],1)))\nreg.add(Dropout(0.2))\n#Layer 2\nreg=Sequential()\nreg.add(LSTM(units=500,return_sequences=True))\nreg.add(Dropout(0.2))\n#Layer 3\nreg=Sequential()\nreg.add(LSTM(units=300,return_sequences=True))\nreg.add(Dropout(0.2))\n#Layer 4\nreg=Sequential()\nreg.add(LSTM(units=200,return_sequences=True))\nreg.add(Dropout(0.2))\n#Layer 5\nreg=Sequential()\nreg.add(LSTM(units=200))\nreg.add(Dropout(0.2))\n\n# Final Output layer\nreg.add(Dense(units=1))\n\n# Compiling our neural network by choosing our loss functions and optimizer \n# Adam optimizer \nreg.compile(optimizer='adam',loss='mae')\n\n# fit the model\nhistory = reg.fit(trainX, trainY, epochs=50, batch_size=16, validation_split=0.1, verbose=1 )\n\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.legend()","0a2512fb":"df_valid","ba36d873":"df_test","f0468a6c":"prediction = model.predict(trainX[-3:])\nprediction","6bb0ff28":"## Model and train","606c0dcf":"#### Graphs","b1f8a0ba":"# Optuna LGB","daf22326":"### Looking At Trend, Seasonality and Residuality by Tweaking the Transposed Data\n","43296713":"## Tune\n"}}